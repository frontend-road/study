{"id":731310,"title":"在基础训练之外","content":"<p>训练 ChatGPT 的重头戏是在向其“展示”来自互联网、书籍等的大量现有文本，但事实证明训练还包括另一个（显然非常重要的）部分。</p><p>一旦根据被展示的原始文本语料库完成“原始训练”，ChatGPT 内部的神经网络就会准备开始生成自己的文本，根据提示续写，等等。尽管这些结果通常看起来合理，但它们很容易（特别是在较长的文本片段中）以“非类人”的方式“偏离正轨”。这不是通过对文本进行传统的统计可以轻易检测到的。但是，实际阅读文本的人很容易注意到。</p><p>构建 ChatGPT 的一个关键思想是，在“被动阅读”来自互联网等的内容之后添加一步：让人类积极地与 ChatGPT 互动，看看它产生了什么，并且在“如何成为一个好的聊天机器人”方面给予实际反馈。但是神经网络是如何利用这些反馈的呢？首先，仅仅让人类对神经网络的结果评分。然后，建立另一个神经网络模型来预测这些评分。现在，这个预测模型可以在原始网络上运行—本质上像损失函数一样—从而使用人类的反馈对原始网络进行“调优”。实践中的结果似乎对系统能否成功产生“类人”输出有很大的影响。</p><p>总的来说，有趣的是，“原本训练好的网络”似乎只需要很少的“介入”就能在特定方向上有效地进步。有人可能原本认为，为了让网络表现得好像学到了新东西，就必须为其训练算法、调整权重，等等。</p><!-- [[[read_end]]] --><p>但事实并非如此。相反，基本上只需要把东西告诉 ChatGPT 一次—作为提示的一部分—它就可以成功用其生成文本。再次强调，我认为这种方法有效的事实是理解 ChatGPT“实际上在做什么”以及它与人类语言和思维结构之间关系的重要线索。</p><p>它确实有些类人：至少在经过所有预训练后，你只需要把东西告诉它一次，它就能“记住”—至少记住足够长的时间来生成一段文本。这里面到底发生了什么事呢？也许“你可能告诉它的一切都已经在里面的某个地方了”，你只是把它引导到了正确的位置。但这似乎不太可能。更可能的是，虽然这些元素已经在里面了，但具体情况是由类似于“这些元素之间的轨迹”所定义的，而你告诉它的就是这条轨迹。</p><p>就像人类一样，如果 ChatGPT 接收到一些匪夷所思、出乎意料、完全不符合它已有框架的东西，它就似乎无法成功地“整合”这些信息。只有在这些信息基本上以一种相对简单的方式依赖于它已有的框架时，它才能够进行“整合”。</p><p>值得再次指出的是，神经网络在捕捉信息方面不可避免地存在“算法限制”。如果告诉它类似于“从这个到那个”等“浅显”的规则，神经网络很可能能够不错地表示和重现这些规则，并且它“已经掌握”的语言知识将为其提供一个立即可用的模式。但是，如果试图给它实际的“深度”计算规则，涉及许多可能计算不可约的步骤，那么它就行不通了。（请记住，它在每一步都只是在网络中“向前馈送数据”，除非生成新的标记，否则它不会循环。）</p><p>当然，神经网络可以学习特定的“不可约”计算的答案。但是，一旦存在可能性的组合数，这种“表查找式”的方法就不起作用了。因此，就像人类一样，神经网络此时需要使用真正的计算工具。（没错，Wolfram|Alpha 和 Wolfram 语言就非常适用，因为它们正是被构建用于“谈论世界中的事物”的，就像语言模型神经网络一样。）</p><br style=\"page-break-after:always\">","comments":[]}