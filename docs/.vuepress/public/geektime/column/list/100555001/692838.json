{"id":692838,"title":"21｜ 实战项目（四）：用ControlNet给你的创意上色","content":"<p>你好，我是南柯。</p><p>上一讲我们已经学习了ControlNet的算法原理，也了解了它在AI绘画中强大的控制能力。今天我们一起来完成ControlNet的实战任务。</p><p>在这一讲中，我们将通过写代码的方式使用ControlNet，一起完成后面这三个任务。</p><ol>\n<li>认识官方已经发布的ControlNet模型以及社区传播的第三方ControlNet模型。</li>\n<li>实现ControlNet论文中的控图生成任务，掌握ControlNet的基础能力。</li>\n<li>探索ControlNet的趣味生成功能，包括图像风格化、二维码生成、创意文字和线稿上色。</li>\n</ol><p>掌握了这些技巧，你也一定能够发挥创意，做出很多结构鲜明的作品。让我们开始吧！</p><h2>模型获取</h2><p>在Hugging Face上，我们不光可以获取到海量AI绘画基础模型，还能找到各种开发者训练的ControlNet模型。正式使用之前，我们先来认识下这些模型。</p><h3>官方发布的模型</h3><p>首先是ControlNet论文作者在ControlNet1.0和1.1中发布的22个模型。</p><p>ControlNet1.0的8个模型可以通过后面这个 <a href=\"https://huggingface.co/lllyasviel/ControlNet/tree/main/models\">Hugging Face链接</a>获取。我们可以看到ControlNet1.0各个模型的命名规则，以第一行的 “control_sd15_canny.pth” 为例，sd15表示用于训练这个ControlNet的基础模型是SD1.5，Canny便是ControlNet的控制条件是Canny算子，也就是提取原始图像的边缘。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/cf/b9/cfa0d623669b2be89042a3670af7b4b9.png?wh=1606x631\" alt=\"\"></p><p>关于前面图中的八个控制条件，我们依次认识下。</p><p>首先来看Canny条件，它使用图像处理的Canny算子来提取图像的边缘信息，可以用于通用物体的可控生成。</p><p><img src=\"https://static001.geekbang.org/resource/image/f6/45/f695f4ab223998bccd6b338093a10145.png?wh=1522x1177\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>接下来是HED条件。如果说前面提到的Canny算子是一种经典的、“上古的”边缘检测算法，HED则是一种基于深度学习方案的“现代化”边缘检测算法。HED条件可以看成Canny条件的升级版，适用于更复杂场景的精细化边缘提取。</p><p><img src=\"https://static001.geekbang.org/resource/image/be/4a/be0be2015594b8d94cb5e05ba8c59a4a.png?wh=1520x1177\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>下一个是MLSD条件，它使用 <a href=\"https://github.com/navervision/mlsd\">M-LSD算法</a> 提取图像的直线轮廓，可以用于各种建筑图像和室内装饰场景的可控生成。</p><p><img src=\"https://static001.geekbang.org/resource/image/7b/ec/7b069fb72bee97ba2456a93217043eec.png?wh=1833x848\" alt=\"\" title=\"图片来源：https://www.slideshare.net/ByungSooKo1/towards-lightweight-and-realtime-line-Segment-detection\"></p><p><img src=\"https://static001.geekbang.org/resource/image/11/a7/11282cb86fa4c42196287382eeea93a7.png?wh=1521x1176\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>接下来要说的是Scribble条件，能根据我们的随手涂鸦生成图像。这也是我个人最喜欢的控制条件，简单几笔就能生成出丰富的细节。</p><p><img src=\"https://static001.geekbang.org/resource/image/b8/e4/b86a4d892dc420074129dbb059c3a9e4.png?wh=1519x1184\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>下一个是Normal条件，Normal代表图像的法线，也就是与图像中物体表面切平面垂直的向量，通常用红、绿、蓝三个通道来表示法线向量在X、Y和Z方向的分量。从后面这个例子可以看出，Normal条件能够帮我们从2D图像中推断出3D几何信息，用于ControlNet的图像生成任务上。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/92/8e3f03e5f9c313cfa2f7b6df0360ea92.png?wh=1519x1168\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>接着是Depth条件。图像的Depth指的是图像的深度信息，可以理解为图像中每个像素距离我们的远近。和Normal条件一样，Depth条件也是包含3D信息的控制条件。</p><p><img src=\"https://static001.geekbang.org/resource/image/b9/f3/b98b51ee119f827b81b86cc27a65aff3.png?wh=1519x1171\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>接下来是OpenPose条件，它的思路是使用人体关键点算法提取肢体关键点，使用关键点的连线图作为控制条件。OpenPose条件经常用在通过ControlNet做换装的任务上。</p><p><img src=\"https://static001.geekbang.org/resource/image/0b/11/0b528cbaea8358177a48fa21f3b1b411.png?wh=1527x1178\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet\"></p><p>最后一个模型是Seg条件，作用是使用图像分割算法对原图进行分割处理，将分割的结果用作控制条件。</p><p><img src=\"https://static001.geekbang.org/resource/image/2f/fb/2f5602988db6392220cf160fbc4a4ffb.png?wh=1525x1173\" alt=\"\"></p><p>说完了ControlNet1.0的8个模型，我们再来看看ControlNet1.1的14个模型。这些模型可以通过后面的 <a href=\"https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main\">Hugging Face链接</a>获取。除了1.0中的8个控制条件，1.1中新增了指令级修图、Tile功能等能力，这些知识我们在上一讲中已经学过了。</p><p><img src=\"https://static001.geekbang.org/resource/image/d3/91/d3feafd1abfcb39f4894fa3942020891.png?wh=1742x769\" alt=\"\"></p><p>ControlNet 1.1模型的命名做出了规范，避免和1.0版产生混淆。</p><p>具体来说，可以看后面这张图，模型名称中的四个部分分别代表项目名、模型版本号、基础模型、控制方式。在模型版本号部分，你需要关注质量标签部分p、e、u三个字母的不同含义。p表示正式版（product-ready），e表示测试版（experimental），u表示未完成版（unfinished）。有时候我们还会看到f这个质量标签，它表示修复版（fixed）。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/26/4640c0a57df533a9aa0b0da801447e26.png?wh=1155x726\" alt=\"\" title=\"图片来源：https://github.com/lllyasviel/ControlNet-v1-1-nightly\"></p><p>值得一提的是，随着SDXL的大火，有很多<a href=\"https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0\">针对SDXL训练的ControlNet模型</a>也逐渐流行。稍后的代码实战中，我们就会用到基于SDXL的ControlNet模型。</p><h3>有趣的第三方模型</h3><p>除了上面的官方模型，开源社区中也涌现了很多有意思的模型。</p><p>Civitai社区中的 <a href=\"https://github.com/lllyasviel/ControlNet-v1-1-nightly\">QR Code Monster</a> 模型可以用于生成创意二维码。具体来说，我们使用Canny或者HED等边缘检测算法提取原始二维码的边缘，用作ControlNet的控制条件，然后写一个prompt进行创意二维码的生成。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/fe/8ed66f3365cf642f3c9dc790966e74fe.png?wh=2100x1009\" alt=\"\" title=\"图片来源：https://civitai.com/models/111006/qr-code-monster\"></p><p>Hugging Face中实现类似二维码功能的模型还有很多，比如我们稍后实战用的 <a href=\"https://huggingface.co/Nacholmo/controlnet-qr-pattern-v2\">qr-pattern-v2</a> 模型，可以先感受下它的二维码效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/yy/b1537b7f413276d21296740dae24yyyy.jpg?wh=2048x994\" alt=\"\" title=\"图片来源：https://huggingface.co/Nacholmo/controlnet-qr-pattern-v2\"></p><p>另一个要推荐的ControlNet模型专门用于<a href=\"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace\">控制表情生成</a>。通过提取人脸的关键点信息，并把这些信息用作控制条件，配合prompt就可以生成各种有意思的效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/86/0d/8613297be90f76c43ed1ded4f34db10d.png?wh=1045x1189\" alt=\"\" title=\"图片来源：https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace\"></p><p>你可以点开<a href=\"https://huggingface.co/models?p=1&sort=downloads&search=controlnet\">这个链接</a>，获取更多开源社区的ControlNet模型。</p><h2>经典技能</h2><p>了解了如何获取模型，现在我们可以使用这些模型完成AI绘画任务了。你可以点开我的 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson21/ControlNet%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.ipynb\">Colab链接</a>，和我一起感受ControlNet的各种趣味技能。</p><p>我们使用蒙娜丽莎作为输入图片，使用ControlNet的Canny控制条件进行AI绘画。</p><pre><code class=\"language-python\"># 导入需要使用的库\nimport cv2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # OpenCV库，用于图像处理\nfrom PIL import Image&nbsp; &nbsp;# PIL库中的Image模块，用于图像读取和处理\nimport numpy as np&nbsp; &nbsp; &nbsp; # NumPy库，用于数组和矩阵等的运算\nfrom diffusers import StableDiffusionControlNetPipeline\nfrom diffusers.utils import load_image\n\n# 使用蒙娜丽莎的图片用作原始图片提取Canny轮廓。你也可以替换为你自己的图片链接；或者上传本地图片到Colab。\nimage = load_image(\n    \"https://ice.frostsky.com/2023/08/26/2c809fbfcb030dd8a97af3759f37ee29.png\"\n)\n\n# 转化图像为NumPy类型的数组（矩阵），方便后续操作\nimage = np.array(image)\n\n# 设置Canny边缘检测算法的低阈值和高阈值\nlow_threshold = 100\nhigh_threshold = 200\n\n# 使用Canny算法进行边缘检测。图像中灰度超过阈值的点，OpenCV会标记为边缘点\nimage = cv2.Canny(image, low_threshold, high_threshold)\n\n# 因为Canny边缘检测后的输出是二维的，所以在第三维增加一个维度，以便转化为RGB图像\nimage = image[:, :, None]\n\n# 将单通道图像复制三次，生成三通道(RGB)图像\nimage = np.concatenate([image, image, image], axis=2)\n\n# 使用PIL库中的Image模块, 将NumPy的array类型转化为Image类型，方便后续图像保存与显示等操作\nCanny_image = Image.fromarray(image)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/db/fe/db9df5ce9770f5b65bef4afa22baa2fe.jpg?wh=1610x778\" alt=\"\"></p><p>然后依次加载SDXL模型和Canny控制条件对应的ControlNet模型。</p><pre><code class=\"language-python\">controlnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-Canny-sdxl-1.0-small\",\n    torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    torch_dtype=torch.float16,\n)\n</code></pre><p>之后，我们就可以使用蒙娜丽莎的Canny边缘轮廓，通过prompt引导模型生成不同的人像效果。你可以修改代码中的prompt描述，生成更多趣味效果。</p><pre><code class=\"language-python\"># 可以替换你的prompt\nprompt = [\"a photo of a girl, best quality, super photorealistic\"]*2\ngenerator = torch.manual_seed(1024)\n\n# 使用ControlNet完成AI绘画\ncontrolnet_conditioning_scale = 0.6\n\nimages = pipe(\n    prompt, num_inference_steps=50, negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * len(prompt),\n    image=Canny_image, controlnet_conditioning_scale=controlnet_conditioning_scale, generator=generator\n).images\n\nimage_grid(images, 1, 2)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/2a/c8/2aa9f529ca4fb88ffce5e881dcc831c8.png?wh=1600x776\" alt=\"\"></p><p>体验完Canny控制的强大，现在我们再来动手完成基于OpenPose条件的ControlNet绘画过程。想要使用OpenPose控制，我们首先需要提取原始图片的肢体关键点，代码是后面这样。</p><pre><code class=\"language-python\">from controlnet_aux import OpenPoseDetector\nfrom PIL import Image\n\n# 加载OpenPose提取模型\nmodel = OpenPoseDetector.from_pretrained(\"lllyasviel/ControlNet\")\n\n# 提取肢体姿态\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n)\npose = model(image)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/b5/5e/b5b00f5b9b655ecdde2e91683ba1a15e.jpg?wh=1024x512\" alt=\"p18\"></p><p>之后，我们依次加载SDXL模型和OpenPose条件的ControlNet模型，使用prompt要求模型根据原始图片的姿态，生成一组星球大战维德的形象效果。</p><pre><code class=\"language-python\"># 加载ControlNet-OpenPose\ncontrolnet = ControlNetModel.from_pretrained(\"thibaud/controlnet-OpenPose-sdxl-1.0\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, torch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\n    \n# 可以更换为你想使用的prompt\nprompt = \"Darth vader dancing in a desert, high quality\"\nnegative_prompt = \"low quality, bad quality\"\nimages = pipe(\n    prompt, \n    negative_prompt=negative_prompt,\n    num_inference_steps=25,\n    num_images_per_prompt=2,\n    image=pose.resize((1024, 1024)),\n    generator=torch.manual_seed(97),\n).images\nimage_grid(images, 1, 2)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/4a/c2/4a90bb910f952020yyecb12b985dacc2.png?wh=2048x1024\" alt=\"\"></p><p>通过上面两个例子，相信你已经感受到了ControlNet功能的强大之处。也推荐你课后探索下更多控制条件的生成效果。</p><h2>趣味玩法</h2><p>事实上，除了ControlNet1.0和1.1官方仓库中提供的用途外，ControlNet还可以帮助我们完成很多创意用法，比如图像风格化、二维码生成等等。我们逐一来进行代码实现。</p><h3>图像风格化</h3><p>在<a href=\"https://time.geekbang.org/column/article/688429\">第15讲</a>中，我们曾经探讨过，使用图生图来实现图像的风格化效果。具体做法就是通过重绘强度这个参数的控制，对输入图片加入少量步数的噪声，然后使用一个擅长生成风格图片的SD模型进行图片去噪。在ControlNet出现之前，大多数图片风格化效果都是采用类似这种方法实现的。</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/12/2e26487d752c26d2e6d01572f9fb6612.jpg?wh=4409x2305\" alt=\"\"></p><p>但是，这种风格化方法有一个缺陷，就是重绘强度这个参数的选择不好控制，可能要多次“抽卡”才能达到满意效果。如果重绘强度选择过小，最终生成的图片无法呈现目标风格效果。如果重绘强度选择过大，最终生成的图片就会丧失原始图片的构图，从而看不出与原图的关联。</p><p>ControlNet的出现弥补了这个缺陷。我们只需要提取原始图片的构图信息，比如使用Canny、HED、Depth等控制模块，便可以保证AI绘画效果与输入图片的构图一致。我们继续通过代码实现这个过程。你可以点开 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson21/ControlNet%E5%9B%BE%E7%89%87%E9%A3%8E%E6%A0%BC%E5%8C%96.ipynb\">Colab</a> 进行操作。</p><pre><code class=\"language-python\">from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nimport torch\nfrom transformers import pipeline\n\n# 提取图像的Depth信息\nimage = load_image(\n    \"https://ice.frostsky.com/2023/08/19/7a93c14f96e2ea8b3a4b6911fff134a1.png\"\n)\nDepth_estimator = pipeline('Depth-estimation')\nimage = Depth_estimator(image)['Depth']\nimage = np.array(image)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncontrol_image = Image.fromarray(image)\n\n# 加载ControlNet1.1的Depth模型\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_Depth\")\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n  \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet\n)\n\n# 使用ControlNet完成风格化\nprompt = [\"a praying cat, cartoon style, best quality, (8k, best quality, masterpiece:1.2)\"] * 2\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image, height = 512, width = 512).images\n</code></pre><p>后面的图片里，前两张图片分别展示了原始输入、我们提取的Depth信息。第二行则是我们ControlNet生成的效果。我们可以将文生图视作重绘强度为1.0的图生图，可以看到，利用ControlNet，我们可以在保持原有构图的前提下，将猫咪转换为卡通风格。</p><p><img src=\"https://static001.geekbang.org/resource/image/6c/7f/6ca864089502d1b8920c7b03f18c857f.jpg?wh=2048x1024\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/89/83/89d910541206fdb55313b97ee2784b83.png?wh=1024x512\" alt=\"\"></p><h3>二维码</h3><p>创意二维码生成是ControlNet非常亮眼的一个功能，比如我们这门课程的二维码，便是通过ControlNet来设计的。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/7a/6b6d3f2d5a921c28650b14803eae677a.jpg?wh=1536x768\" alt=\"\"></p><p>这样的二维码该如何设计呢？首先需要准备一个原始的二维码图像。比如对于我们的<a href=\"https://time.geekbang.org/column/intro/100555001?tab=catalog\">课程链接</a>，我们可以任意选择一个二维码生成工具，生成一张二维码。比如这里我使用<a href=\"https://classic.qrbtf.com/\">链接中</a>的工具，生成了两款原始二维码。</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/e9/e178fa8b427762yy1f7a5e2512f358e9.png?wh=1851x953\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/38/ba/383e293ce3e33b658420d075472434ba.jpg?wh=3000x1500\" alt=\"\"></p><p>搞定了原始二维码，我们便可以用ControlNet来发挥我们的创造力了。你可以点开<a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson21/ControlNet%E4%BA%8C%E7%BB%B4%E7%A0%81.ipynb\">Colab链接</a>和我一起操作。</p><p>首先加载生成二维码专用的ControlNet模型和对应的SD1.5基础模型。</p><pre><code class=\"language-python\">from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n\n# 加载二维码生成对应的ControlNet模型\ncontrolnet = ControlNetModel.from_pretrained(\"Nacholmo/controlnet-qr-pattern-v2\")\npipeline = StableDiffusionControlNetPipeline.from_pretrained(\n\t\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet\n)\n</code></pre><p>然后，我们使用原始二维码图像作为输入条件，并提供一个prompt用于引导AI绘画的生成过程。比如后面的代码中，使用我们课程的二维码作为控制条件，目标是生成两张创意二维码，分别是机器人风格和丛林风格。</p><pre><code class=\"language-python\"># 处理原始二维码图像，可以使用你的二维码链接，或者将本地二维码图片上传到Colab\n# 这里我们将二维码图片上传到图床获取图片链接\ninit_image = load_image(\"https://ice.frostsky.com/2023/08/19/38705b43b70b00af8d1aa23fd773102f.jpeg\")\n\n# 可以替换成你的prompt\nprompt = [\"robot costume, best quality, extremely detailed\", \"masterpiece, forest, jungle, trees, mist, fog, water, river, vines, photorealistic\"]\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1025)\n\n# 生成二维码\noutput = pipeline(\n    prompt,\n    init_image,\n    negative_prompt=[\"ugly, disfigured, low quality, blurry, nsfw\"] * len(prompt),\n    generator=generator,\n    num_inference_steps=20,\n)\n</code></pre><p>如果你想上传自己手里的原始二维码，可以进入<a href=\"https://imgloc.com/\">这个链接</a>，参考后面的示意图上传二维码图片，然后复制url链接，替换到后面截图位置的代码里。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/59/1cbfd30931300f85365a2bfa5a46ce59.png?wh=2452x1020\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/d4/e0/d4a134eb080185b4a9a516869f2860e0.png?wh=2628x476\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/ca/6a/ca2235d6e34cb22e25a54c6b3bc28d6a.png?wh=1540x324\" alt=\"\"></p><p>你可以点开图片查看我们生成的二维码效果，确认可以扫码成功便大功告成。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/6e/0d6889fd63d1dc355f3f490651edde6e.png?wh=1536x768\" alt=\"\"></p><h3>创意文字</h3><p>与二维码类似，我们还可以在照片中写入我们希望的文字。比如我们希望实现一张写着“极客”的创意图片。</p><p>注意，我们要做的并不是全新的字体设计，而是用已有的字体来生成创意文字。比如我们可以在文档中选择“手札体”写下目标文字，然后截图保存。将这张图片上传到Colab或者图床后，我们便可以生成创意文字了。你可以点开 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson21/ControlNet%E5%88%9B%E6%84%8F%E6%96%87%E5%AD%97.ipynb\">Colab链接</a>和我一起完成操作。</p><pre><code class=\"language-python\">image = load_image(\n    \"https://ice.frostsky.com/2023/08/19/f9a6a52b210fb8dbc0fe347efbb22d9f.png\"\n)\n\n# 将图像黑白颜色反转，得到的效果会更好\ninverted_image = ImageOps.invert(image)\ninverted_image\n</code></pre><p>对于创意字体这个任务，我们要使用Depth控制条件的ControlNet模型。这是因为Depth条件中包含3D信息，用于指导生成有3D感的图片效果更好。</p><p>在提取Depth信息前，我们需要将图片黑白通道反转，实测下来效果会更好。然后，我们使用黑白通道反转的图片，来提取Depth控制信息。</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/c9/e36db43f0fa7ac3da5fb8494f8c0a5c9.jpg?wh=1140x562\" alt=\"\"></p><pre><code class=\"language-python\">image = inverted_image\nDepth_estimator = pipeline('Depth-estimation')\nimage = Depth_estimator(image)['Depth']\nimage = np.array(image)\nimage = image[:, :, None]\n\n# 将单通道图像复制三次，生成三通道(RGB)图像\nimage = np.concatenate([image, image, image], axis=2)\ncontrol_image = Image.fromarray(image)\n</code></pre><p>你可以点开后面的图片，查看我们得到的Depth控制条件。</p><p><img src=\"https://static001.geekbang.org/resource/image/84/23/843284647be7188123aeba58c1e2f223.png?wh=570x562\" alt=\"\"></p><p>搞定了控制条件，我们就可以加载ControlNet1.1的Depth模型以及SD1.5基础模型，指定prompt要求模型生成指定场景，比如树叶、白云等，完成我们的创意字体生成任务。</p><pre><code class=\"language-python\"># 使用ControlNet1.1的Depth模型\ncheckpoint = \"lllyasviel/control_v11f1p_sd15_Depth\"\ncontrolnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n)\n\npipe.scHEDuler = UniPCMultistepScHEDuler.from_config(pipe.scHEDuler.config)\npipe.enable_model_cpu_offload()\n\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n</code></pre><p>你可以点开图片查看生成效果。可以看到，图像中的各个场景都呈现出“极客”的样式。当然，这里也可以选择其他和SD1.5亲缘关系接近的基础模型，实现更多有意思的效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/a7/19/a7a5cbc0141bf7e1f52c4dabbe669d19.png?wh=1136x1120\" alt=\"\"></p><h3>线稿上色</h3><p>现在我们再来尝试一下线稿上色的任务。如果你手中有现成的线稿图片，可以在Colab中直接使用你的线稿来进行AI绘画。如果手上没有线稿，可以挑选一张喜欢的动漫图片，使用<a href=\"https://flandredaisuki.github.io/Outline-Extractor/\">链接中的工具</a>进行线稿提取。你可以点开 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson21/ControlNet%E7%BA%BF%E7%A8%BF%E4%B8%8A%E8%89%B2.ipynb\">Colab链接</a>和我一起进行操作。</p><p>在这个任务中，我们使用最新的SDXL模型和它对应的ControlNet Canny模型。我们可以通过下面的代码进行模型加载。</p><pre><code class=\"language-python\"># 特殊说明：模型计算量较大，需要使用Colab的高RAM模式，否则Colab容易Crash\n# 高RAM模式需要点击「修改」-「笔记本设置」进行修改\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-Canny-sdxl-1.0-mid\",\n    torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    torch_dtype=torch.float16,\n)\npipe.enable_model_cpu_offload()\n</code></pre><p>之后我们需要读取图床的图片链接，或者读取上传到Colab的图片线稿。然后我们使用OpenCV的Canny轮廓提取函数获得图像的Canny边缘。后面图片展示的就是原始线稿和提取的Canny边缘。</p><pre><code class=\"language-python\"># 你可以根据实际需求替换为你自己的图片\nimage_original = load_image(\"https://ice.frostsky.com/2023/08/26/3995bd36b16e2c65d5e7a98ad04264d2.png\")\nimage_original\n\n# 提取Canny边缘\nimage = np.array(image_original)\nimage = cv2.Canny(image, 100, 200)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\nimage\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/08/ac/0815fa332a1fae8c3de8dbf8409926ac.jpg?wh=2560x960\" alt=\"\"></p><p>一切准备就绪后，我们写一个prompt描述目标图像，比如“一个黄头发、红眼睛、红衣服的卡通男孩”。利用SDXL的文生图能力，配合ControlNet的Canny轮廓控制，便可以给我们的线稿自动上色。后面代码展示的就是文生图的过程。</p><pre><code class=\"language-python\"># 结合ControlNet进行文生图\nprompt = \"a handsome cartoon boy, yellow hair, red eyes, red clothes\"\ngenerator = torch.manual_seed(1025)\nnegative_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, Normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n\ncontrolnet_conditioning_scale = 0.6 \n\nimages = pipe(\n    [prompt]*4, num_inference_steps=50, negative_prompt=[negative_prompt]*4, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale,generator = generator\n).images\n</code></pre><p>在前面的代码中，我们一口气生成了4张效果图，耐心等待一会，便得到了下面图像中的效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/5f/dcee9e7a06220e4678cfc7a232c7e05f.jpg?wh=2560x1920\" alt=\"\"></p><p>使用同样的方法，我们可以给下面图像中的房屋简笔画进行上色。这里我们用到的prompt描述是“一张高质量、有细节的专业图片”。你可以点开图片查看上色效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/08/ab/08e6c23864f59007654aa546186ab2ab.jpg?wh=800x600\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/a2/f4/a2e99c103057feb9830d2d1c0a1593f4.png?wh=1600x1200\" alt=\"\"></p><p>如果你手中有线稿，也期待你使用ControlNet为线稿实现染色效果。如果你有喜欢的动漫角色，也可以先提取线稿，然后试着为“他们”重新上色。</p><h2>总结时刻</h2><p>今天这一讲，我们一起完成了不少实战项目，一起认识了ControlNet各种控制条件的特性，还用ControlNet完成了一些有趣的创作。</p><p>首先我们了解了Canny、HED、Scribble、MLSD这几种控制条件，它们的思路就是用不同形式的轮廓线实现AI绘画的可控生成。Normal、Depth这两种控制条件，能够帮助我们从2D图像中推断出3D几何信息，生成的图片3D空间感更强。而Seg、OpenPose这两种条件，则分别从分割结果和人体姿态的角度控制构图。</p><p>然后我们以实战的形式，体验了如何使用Canny和OpenPose作为控制条件实现AI绘画效果，并使用ControlNet完成了图像风格化、创意二维码生成、创意文字生成和线稿上色等趣味用法。</p><p>实际上，ControlNet的能力还远不止于此。比如我们还可以用ControlNet完成动漫人物转真人、在人像照片中写入不违和的文字等任务。这些功能背后的原理都是ControlNet，不过需要更精心地选择控制条件、ControlNet模型和基础模型。</p><p><img src=\"https://static001.geekbang.org/resource/image/88/19/888ff087c36127611d760b97ebf70819.jpg?wh=3900x1993\" alt=\"\"></p><h2>思考题</h2><p>除了今天实战篇中学习的各种ControlNet用法，ControlNet还能实现哪些有趣的功能？欢迎你在Colab中实现出算法效果，并把你的Colab链接分享出来。</p><p>期待你在留言区和我交流讨论，也推荐你把今天的内容分享给身边更多朋友，和他一起探索ControlNet的多种玩法。</p>","comments":[{"had_liked":false,"id":380629,"user_name":"Toni","can_delete":false,"product_type":"c1","uid":3206957,"ip_address":"瑞士","ucode":"E6B2FACCC1E000","user_header":"https://static001.geekbang.org/account/avatar/00/30/ef/2d/757bb0d3.jpg","comment_is_top":false,"comment_ctime":1693908791,"is_pvip":false,"replies":[{"id":138663,"content":"这个想法非常有趣。QualityControlNet其实可以理解为一个美学打分模型，很多大厂优化SD模型的第一步就是训练美学打分模型，不过目的是从海量数据中进行筛选。回到你这个想法，我们可以引入美学打分模型和ChatGPT，美学模型负责形成分数反馈，ChatGPT负责优化文生图参数和prompt。凭借这种方式反复迭代，得到更好的效果。不过，这里需要给ChatGPT一些示例，让ChatGPT明白自己需要做啥。很有意思的idea，欢迎继续探讨~","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694267605,"ip_address":"北京","comment_id":380629,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"包括ControlNet 在内的AI绘画工具越来越多，但AI绘画结果需要人工抽取结果的痛点依在。\n\n现在AI 绘画的流程如下:\n1. 先有个想画的想法，\n2. 根据这个想法编出个prompt 输给AI 绘画工具，\n3. AI绘画工具生成多张图片比如说4张供人参考，\n4. 人们再依据上面步骤1中的想象，从出的4张图里面选出合意的，如不甚满意，就继续上面的步骤2改变prompt，然后再执行步骤3，让AI重新绘图，这过程可能会进行好几轮，比较费时。\n\n那么问题来了，有没有什么方法让上面的过程自动化? \n\n计算机的优势就是不知疲惫，虽然有时也宕机罢工，但总体来说比人能干。如果能将上面成图+判断的过程先交给计算机进行5轮，然后再由人类对出图给出评估，效率会提高很多，图的质量也会更高。\n\n解决上面痛点的思考之一是设计一个质量控制层 QualityControlNet，这个可训练的质量控制层应包含下面的一些功能:\n1. 有对图像质量评估的量化指标，可选艺术，技术，美学等几个方面做为评估参量，指标可调可变，\n2. 有对正反prompt修正反馈的能力，\n3. 要修正的图像+新的更改要求可以自动返回低维隐藏层，并启动重绘过程，\n4. 能自动求解最佳重绘参数，最佳去噪步骤等参量。\n\n达到上面要求中的一个或几个，对质量控制层 QualityControlNet 的一些想法，大家补充。","like_count":4,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627553,"discussion_content":"这个想法非常有趣。QualityControlNet其实可以理解为一个美学打分模型，很多大厂优化SD模型的第一步就是训练美学打分模型，不过目的是从海量数据中进行筛选。回到你这个想法，我们可以引入美学打分模型和ChatGPT，美学模型负责形成分数反馈，ChatGPT负责优化文生图参数和prompt。凭借这种方式反复迭代，得到更好的效果。不过，这里需要给ChatGPT一些示例，让ChatGPT明白自己需要做啥。很有意思的idea，欢迎继续探讨~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1694267606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380547,"user_name":"Geek_cbcfc8","can_delete":false,"product_type":"c1","uid":2532655,"ip_address":"北京","ucode":"A2141FDF6C7663","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/lJR3Ba9EuTLRSry9sajEeRcvfwuiaPDr41KicHYGxcsXnRcTxaTp3OHq24AebUR9MS016zSEmqAyws5iaQiaj5TDdQ/132","comment_is_top":false,"comment_ctime":1693807197,"is_pvip":false,"replies":[{"id":138668,"content":"你好，单图特征抽取是很有意思的话题，这块技术发展很迅猛。相信很快，人脸、风格、物体都可以通过一个提取模型进行处理，实现类似DreamBooth的效果。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694268565,"ip_address":"北京","comment_id":380547,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"腾讯发布了一个adapter https:&#47;&#47;github.com&#47;tencent-ailab&#47;IP-Adapter\n可以实现根据一张图抽取人脸，风格，服装等特征，用于image to image ","like_count":1,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627559,"discussion_content":"你好，单图特征抽取是很有意思的话题，这块技术发展很迅猛。相信很快，人脸、风格、物体都可以通过一个提取模型进行处理，实现类似DreamBooth的效果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694268565,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380614,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"河南","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1693894178,"is_pvip":false,"replies":[{"id":138612,"content":"对于问题1，你的问题比较像科幻小说，如果是画面模糊了并不知道是模糊成什么程度，全模糊了应该无法恢复。只是局部也许有可能，但是否还原成原始模样说不好；对于问题2，同样是理论上可以做，要看你想要什么样的名片。目前很多模型在图片上生成文字的水平并不高，我们课程里讲过了这点","user_name":"编辑回复","user_name_real":"编辑","uid":1501385,"ctime":1693918422,"ip_address":"北京","comment_id":380614,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"请教老师两个问题：\nQ1：彩票上面的数字，只有一个数字，其他的看不到了，不知道是被雨淋了还是被撕掉了。用WebUI可以恢复吗？或者其他某个模型？\nQ2：controlNet或其他模型可以创作名片吗？ ","like_count":0,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627240,"discussion_content":"对于问题1，你的问题比较像科幻小说，如果是画面模糊了并不知道是模糊成什么程度，全模糊了应该无法恢复。只是局部也许有可能，但是否还原成原始模样说不好；对于问题2，同样是理论上可以做，要看你想要什么样的名片。目前很多模型在图片上生成文字的水平并不高，我们课程里讲过了这点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693918422,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":8}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380534,"user_name":"Geek_cbcfc8","can_delete":false,"product_type":"c1","uid":2532655,"ip_address":"北京","ucode":"A2141FDF6C7663","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/lJR3Ba9EuTLRSry9sajEeRcvfwuiaPDr41KicHYGxcsXnRcTxaTp3OHq24AebUR9MS016zSEmqAyws5iaQiaj5TDdQ/132","comment_is_top":false,"comment_ctime":1693792685,"is_pvip":false,"replies":[{"id":138669,"content":"你好。21和24的实战篇都有使用ControlNet的XL模型，不过需要用付费的Colab资源。关于ControlNet x LoRA x SDXL的换装技术，我们已经列入加餐篇，会和大家详细探讨。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694268729,"ip_address":"北京","comment_id":380534,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师你好，controlnet xl版本已经出来了，能加餐一个的换装任务吗，把一些知识点串一下\n1、训练衣服或人物的lora模型\n2、基于controlnet进行衣服或模特的更换\n可应用到电商一键换装，或者帮组情侣实现AI婚纱摄影","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627560,"discussion_content":"你好。21和24的实战篇都有使用ControlNet的XL模型，不过需要用付费的Colab资源。关于ControlNet x LoRA x SDXL的换装技术，我们已经列入加餐篇，会和大家详细探讨。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694268730,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386346,"user_name":"刘蕾","can_delete":false,"product_type":"c1","uid":1899996,"ip_address":"斯洛伐克","ucode":"984D2C7E286DA1","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fd/dc/8c394a51.jpg","comment_is_top":false,"comment_ctime":1704626272,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师您好，这节课里面的蒙娜丽莎的那个例子，免费的CoLab资源是不是跑不了？ 价值SDXL（stable-diffusion-xl-base-1.0）的那句，跑了1个多小时还没跑完。","like_count":0}]}