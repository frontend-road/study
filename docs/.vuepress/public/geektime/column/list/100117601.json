[{"article_id":543579,"article_title":"开篇词｜系统学习移动端音视频开发技术，为业务赋能","article_content":"<p>你好，我是展晓凯，目前是快手音视频首席架构师，也是《音视频开发进阶指南：基于Android与iOS平台实践》一书的作者。欢迎你打开《快手·移动端音视频开发实战》专栏，从今天开始跟我一起系统地学习移动端音视频开发吧！</p><h2>我在音视频领域的探索</h2><p>迄今为止，我在移动互联网的音视频领域已经奋战十年了。回想起点，当时担任的是唱吧的音视频架构师，负责多条产品线核心的架构设计与开发工作，包括视频录播系统、唱吧直播间、视频合唱、视频特效、火星直播等等，一手搭建起了唱吧的整体音视频系统。</p><p>那段经历，让我逐渐积累起了自己的音视频技术图谱，也是从那个时候，我开始有意识地去沉淀经验，尝试分享给更多的人，《音视频开发进阶指南》就是在那段时间写出来的。</p><p>之后我又加入到StarMaker担任商业化技术负责人，又一次从零开始，这次是搭建StarMaker的在线KTV和直播系统。遇到的问题，也是一度让我很头痛，比如由于印度网络基建的落后，我们整个技术团队不得不自行建立直播源站，并将SRT协议引入到主播的上行链路中。</p><p>类似的突破还有很多，现在让我再去回溯那段经历的话，我感觉就一句话：“关关难过关关过”。但我很庆幸，在我入行之初就有这样的好机会。如果说唱吧给予我的是技术上施展拳脚的空间，那么StarMaker给予我的就是技术上的深度和广度。在这个阶段我充分认知到技术和业务像是连接在一起的齿轮，相互带动、相互促进。</p><!-- [[[read_end]]] --><p>后来，我也做了很多其他音视频相关的尝试。18年我加入了阿里巴巴的XLab，开始探索音乐赛道，我所在的团队开发并维护了唱鸭、鲸鸣、虾米音乐等产品。当时我们将弹唱的实时耳返做到了业界最佳并独创了音乐弹幕的交互形式，领先了竞品长达半年以上的技术窗口期。</p><p>对于一个技术人来说，利用过硬的技术为平台实现弯道超车是一件很酷的事儿，我也想把这样的事一直做下去，并依靠先进的团队实现业务上的更多可能。</p><p>于是，我选择在22年5月份加入快手，继续做音视频相关应用的架构与开发工作。基于快手强大的音视频能力，我们的技术可以很快赋能到各个业务中，让业务方快速地验证自己的想法，在业务所在的赛道跑到前方，拿到好的结果。</p><p>希望我的亲身经历也能给期待探索音视频领域的你一点启发吧，当然在这个过程中，我确实遇到了不少挑战，但也迎来了很多机遇。所以，我是非常鼓励你去实战中磨炼技能的，因为我就是这么过来的。</p><h2>为什么要系统地学习移动端音视频开发知识？</h2><p>在移动互联网音视频开发的这十年，我深感当下与十年前的互联网环境相比，已经发生了翻天覆地的变化。虽然各种点播云、视频云、RTC等基建成熟了很多，但是人工成本、买量成本也与日俱增，互联网用户的增量市场也已经渐渐匮乏，各大厂也都在做降本增效的事情。所以在这样的大背景下，<strong>技术人员如何能在业务中贡献出自己的价值变得更加重要了。</strong>大环境在赋予人机遇，只看你怎么做。</p><p>那我们作为技术人员应该怎么在业务中凸显出自己的价值呢？在我看来，至少要做到这3点。</p><ul>\n<li>首先，在业务的各个阶段，技术要能够快速满足业务的迭代需求，让业务先赢；</li>\n<li>其次，在完成业务迭代的同时要考虑中长期的变化，设计出合理的架构；</li>\n<li>最后，能做出一些技术沉淀以及流程机制的升级。</li>\n</ul><p>里面的每一项都离不开我们<strong>扎实的基本功和丰富的实战经验</strong>。现在市面上任何一款App其实都离不开音视频的能力，甚至说音视频能力已然是决定业务的关键因素之一。</p><p>这就需要大量的音视频技术人员参与其中，但目前音视频的人才储备仍旧不足。很多技术人员因为业务的需要被临时调去做音视频的项目；没有实际开发经验的小白直接上手参与项目开发。这种情况并不少见。</p><p>你可能会想：既然是这样，那音视频开发的入门门槛应该挺低的吧？事实恰恰相反，如果只有一些零散的音视频相关的技能，或者只做过一两个音视频相关的功能，还远远达不到入门的级别。因为音视频开发涉及的概念与技术繁多而复杂，零散的知识点虽然能够让我们“小步挪动”，但如果想要全面了解音视频开发整体的知识地图还是十分困难的。</p><h3>音视频开发知识图谱</h3><p>从我的经验来看，我们至少需要了解各种音视频基本概念，比如音视频的原始数据格式、压缩格式、封装格式、传输协议等，熟练使用常用的音视频框架，比如FFmpeg、libyuv、SOX、SoundTouch等，还要熟悉各个平台的音视频采集和渲染、音视频的编解码（软件/硬件）以及常用的滤镜处理；还有音视频的质量评测，比如视频质量评测PSNR、VMAF、无参考评测等。最后我们还要对各个场景有系统性的认知，包括但不限于录播、直播、RTC等场景的常用架构和最佳实践。</p><p>除此之外在移动平台上，还有一些与平台相关的加速库可以提高性能，比如vDsp、Neon指令集等，我们也需要了解。</p><p>当然最重要的还是开发者或者架构师本身自己的音视频架构能力，如何将音视频的能力与业务结合起来，设计出最符合当前业务阶段的架构并且快速地落地。</p><p><img src=\"https://static001.geekbang.org/resource/image/8f/dc/8f94bb332201c4c1d47ae96ee9e802dc.png?wh=1892x1481\" alt=\"图片\" title=\"图1 音视频开发知识图谱\"></p><p>听我说完，是不是有一种任重而道远的感觉？不过这并不是要劝退你，因为我们都清楚，行百里者半九十。相信在这个专栏的引导下，你这“百里路”会比别人行得更快、更轻松些。具体怎么做到这一点？我想并不难。</p><p>我们的专栏会以代码示例、项目实战的方式展开，带你系统地学习移动端的音视频开发，其中会涉及多平台、多项技术、多个场景。接下来我们不妨看看课程的具体安排吧！</p><h2>我是怎么设计这门课的？</h2><p>本专栏我们主要围绕直播与录播两个场景进行架构设计与实践，基本的思路就是从基础开始，拆分场景和核心技术，最终实操实练地去还原真实项目。</p><p><img src=\"https://static001.geekbang.org/resource/image/13/12/13946b8fd47fff453a0645b95c5a0812.png?wh=1920x597\" alt=\"图片\" title=\"图2 课程设计\"></p><p>当然，实操的过程是有一定难度的。你要理解直播场景所对应的核心流程是主播端和观众端，前者推流、后者拉流，核心体验是处理好秒开与内部的追赶逻辑。还要理解录播场景可以拆分为生产端和消费端，前者负责录制、编辑和导出上传，后者负责从CDN拉流进行播放。</p><p>里面的每一个技术细节都将影响你的业务，而我们要做的就是搞定它们，比如主播端要进行美颜、魔表处理，要根据网络情况做动态码率策略，观众端要能做主动的追赶逻辑等等；再比如生产端硬件编解码的使用、导出上传速度的优化、消费端要能够根据网络以及设备情况做多分辨率的切换等等。</p><p><strong>视频播放器项目：</strong>如果想要成功播放一个视频，就需要我们掌握音视频解码、音频渲染、视频渲染以及音视频同步等知识。所以针对于这些知识点，我都会系统地进行分析和讲解，这样在处理相关工作时，便于你做技术选型。学完移动端的音视频渲染后，我们即可产出一个视频播放器项目，这个项目会帮你把前面的知识点串联起来，同时还可以作为视频编辑器、导出器的先导知识。</p><p><img src=\"https://static001.geekbang.org/resource/image/d5/a4/d50f8c12c4d5e48d5c884195c73bdfa4.png?wh=1920x813\" alt=\"图片\" title=\"图3 视频播放器\"></p><p><strong>视频录制器</strong><strong>项目：</strong>如果想要把麦克风和摄像头采集的数据记录下来，就需要我们掌握音频采集、视频采集与渲染、音视频处理与编码等知识。学完这些相关知识后，我们最终会产出一个视频录制器项目，在这个项目中你还可以学到很多音视频架构相关的内容，可以帮助你在日后的工作中，做出更符合当下阶段的架构设计。</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/fd/9fbd4a650536ac0eeeefff3b470ca5fd.png?wh=1920x1066\" alt=\"图片\" title=\"图4 音频录制器\"></p><p><img src=\"https://static001.geekbang.org/resource/image/58/71/58d60679917076e7fb5e7e8a99393d71.png?wh=1920x1046\" alt=\"图片\" title=\"图5 视频录制器\"></p><p>这就是我们整个专栏的设计。希望你能跟着我的步伐，一步步地去实践，提升自己的开发能力与架构设计能力，更重要的是我希望你能通过这个专栏提升全链路思考能力，拥有更开阔的视野。最终让技术发挥更大的价值，为业务赋能。话不多说，让我们马上开始这一段能力升级之旅吧！</p>","neighbors":{"left":[],"right":{"article_title":"01｜iOS平台音频渲染（一）：使用AudioQueue渲染音频","id":543649}}},{"article_id":543649,"article_title":"01｜iOS平台音频渲染（一）：使用AudioQueue渲染音频","article_content":"<p>你好，我是展晓凯。</p><p>记得在开篇的时候我说过，我们最后的目标之一就是要实现一个视频播放器项目。而想要实现这个项目，需要我们先掌握音频渲染、视频渲染以及音视频同步等知识。所以今天我们就来迈出第一步——音频的渲染。</p><p>音频渲染相关的技术框架比较多，平台不同，需要用到的技术框架也不同。这节课我们就先来看一下iOS平台都有哪些音频框架可供我们选择，以及怎么在iOS平台做音频渲染。</p><p>我们先看一下图1，iOS平台的音频框架，里面比较高层次的音频框架有Media Player、AV Foundation、OpenAL和Audio Toolbox（AudioQueue），这些框架都封装了AudioUnit，然后提供了更高层次的、功能更精简、职责更加单一的API接口。这里你先简单地了解一下这些音频框架之间的关系，以及AudioUnit在整个音频体系中的作用，下节课我会给你详细地讲解AudioUnit框架。</p><p><img src=\"https://static001.geekbang.org/resource/image/42/7f/429c932f1b5fed302fa262bff76yy87f.png?wh=1744x704\" alt=\"\" title=\"图1 iOS平台的音频框架（图片来自苹果官网）\"></p><p>如果我们想要低开销地实现录制或播放音频的功能，就需要用到iOS音频框架中一个非常重要的接口——<strong>AudioQueue</strong>，<strong>它是实现录制与播放功能最简单的API接口</strong>，作为开发者的我们无需知道太多内部细节，就可以简单地完成播放PCM数据的功能，可以说是非常方便了。</p><!-- [[[read_end]]] --><p>在实际学习AudioQueue框架之前，我会先把AudioSession给你讲清楚，因为AudioSession是我们与系统对话的重要窗口，它能够向系统描述应用需要的音频能力，所以需要在学会使用AudioSession基础上，再去学习具体的框架。</p><h2>AVAudioSession</h2><p>在iOS的音视频开发中，使用具体API之前都会先创建一个会话，而音频这里的会话就是AVAudioSession，<strong>它以单例的形式存在，用于管理与获取iOS设备音频的硬件信息</strong>。我们可以使用以下代码来获取AudioSession的实例：</p><pre><code class=\"language-plain\">AVAudioSession *audioSession = [AVAudioSession&nbsp;sharedInstance];\n</code></pre><h3>基本设置</h3><p>获得AudioSession实例之后，就可以设置以何种方式使用音频硬件做哪些处理了，基本的设置如下所示：</p><ol>\n<li>根据我们需要硬件设备提供的能力来设置类别：</li>\n</ol><pre><code class=\"language-plain\">[audioSession&nbsp;setCategory:AVAudioSessionCategoryPlayback&nbsp;error:&amp;error];&nbsp;\n</code></pre><ol start=\"2\">\n<li>设置I/O的Buffer，Buffer越小说明延迟越低：</li>\n</ol><pre><code class=\"language-plain\">NSTimeInterval&nbsp;bufferDuration = 0.002;\n[audioSession&nbsp;setPreferredIOBufferDuration:bufferDuration&nbsp;error:&amp;error];&nbsp;\n</code></pre><ol start=\"3\">\n<li>设置采样频率，让硬件设备按照设置的采样率来采集或者播放音频：</li>\n</ol><pre><code class=\"language-plain\">double hwSampleRate = 44100.0;\n[audioSession&nbsp;setPreferredSampleRate:hwSampleRate error:&amp;error];\n</code></pre><ol start=\"4\">\n<li>当设置完毕所有参数之后就可以激活AudioSession了，代码如下：</li>\n</ol><pre><code class=\"language-plain\">[audioSession&nbsp;setActive:YES error:&amp;error];\n</code></pre><p>经过上述几个简单的调用，我们就完成了对AVAudioSession的设置。当我们使用具体API的时候，系统就会按照上述设置的参数进行播放或者回调给开发者进行处理。</p><h3>深入理解AudioSession</h3><p>除了上述基本的设置之外，我们再从以下几个层面深入理解一下AudioSession，在AVAudioSession设置Category的时候是有很多细节的，分别是Category和CategoryOptions，在某些场景下，它可能会产生奇效。</p><ol>\n<li>Category是向系统描述应用需要的能力，常用的分类如下：</li>\n</ol><ul>\n<li>AVAudioSessionCategoryPlayback：用于播放录制音乐或者其它声音的类别，如要在应用程序转换到后台时继续播放（锁屏情况下），在xcode中设置 UIBackgroundModes 即可。默认情况下，使用此类别意味着，应用的音频不可混合，激活音频会话将中断其它不可混合的音频会话。如要使用混音，则使用AVAudioSessionCategoryOptionMixWithOthers。</li>\n<li>AVAudioSessionCategoryPlayAndRecord&nbsp;: 同时需要录音（输入）和播放（输出）音频的类别，例如K歌、RTC场景。注意：用户必须打开音频录制权限（iPhone 麦克风权限）。</li>\n</ul><ol start=\"2\">\n<li>CategoryOptions是向系统设置类别的可选项，具体分类如下：</li>\n</ol><ul>\n<li>AVAudioSessionCategoryOptionDefaultToSpeaker：此选项只能在使用PlayAndRecord类别时设置。它用于保证在没有使用其他配件（如耳机）的情况下，音频始终会路由至扬声器而不是听筒。而如果类别设置的是Playback，系统会自动使用Speaker进行输出，无需进行此项设置。</li>\n<li>AVAudioSessionCategoryOptionAllowBluetooth：此选项代表音频录入和输出全部走蓝牙设备，仅可以为PlayAndRecord还有Record这两个类别设置这个选项，注意此时播放和录制的声音音质均为通话音质（16kHz），适用于RTC的通话场景，但不适用于K歌等需要高音质采集与播放的场景。</li>\n<li>AVAudioSessionCategoryOptionAllowBluetoothA2DP：此选项代表音频可以输出到高音质（立体声、仅支持音频输出不支持音频录入）的蓝牙设备中。如果使用Playback类别，系统将自动使用这个A2DP选项，如果使用PlayAndRecord类别，需要开发者自己手动设置这个选项，音频采集将使用机身内置麦克风（在需要高音质输出和输入的场景下可以设置成这种）。</li>\n</ul><ol start=\"3\">\n<li>监听音频焦点抢占，一般在检测到音频被打断的时候处理一些自己业务上的操作，比如暂停播放音频等，代码如下：</li>\n</ol><pre><code class=\"language-plain\">[[NSNotificationCenter&nbsp;defaultCenter] addObserver:self&nbsp;selector:@selector(audioSessionInterruptionNoti:)&nbsp;name:AVAudioSessionInterruptionNotification object:[AVAudioSession&nbsp;sharedInstance]];\n&nbsp;\n- (void)audioSessionInterruptionNoti:(NSNotification *)noti&nbsp;{&nbsp;&nbsp;&nbsp;\nAVAudioSessionInterruptionType type = [noti.userInfo[AVAudioSessionInterruptionTypeKey] intValue];\nif (type == AVAudioSessionInterruptionTypeBegan) {\n//Do Something\n}\n}\n</code></pre><ol start=\"4\">\n<li>监听声音硬件路由变化，当检测到插拔耳机或者接入蓝牙设备的时候，业务需要做一些自己的操作，代码如下：</li>\n</ol><pre><code class=\"language-plain\">[[NSNotificationCenter&nbsp;defaultCenter] addObserver:self selector:@selector(audioRouteChangeListenerCallback:) name:AVAudioSessionRouteChangeNotification&nbsp;object:nil];\n&nbsp;\n- (void)audioRouteChangeListenerCallback:(NSNotification*)notification {\nNSDictionary *interuptionDict = notification.userInfo;\nNSInteger&nbsp;routeChangeReason = [[interuptionDict&nbsp;valueForKey:AVAudioSessionRouteChangeReasonKey] integerValue];\nif (routeChangeReason==AVAudioSessionRouteChangeReasonNewDeviceAvailable ||&nbsp;routeChangeReason == AVAudioSessionRouteChangeReasonOldDeviceUnavailable ||&nbsp;routeChangeReason == AVAudioSessionRouteChangeReasonWakeFromSleep ) {\n//Do&nbsp;Something\n} else if (\nrouteChangeReason == AVAudioSessionRouteChangeReasonCategoryChange ||\nrouteChangeReason == AVAudioSessionRouteChangeReasonOverride) {\n//Do&nbsp;Something\n}\n}\n</code></pre><ol start=\"5\">\n<li>申请录音权限，首先判断授权状态，如果没有询问过，就询问用户授权，如果拒绝了就引导用户进入设置页面手动打开，代码如下：</li>\n</ol><pre><code class=\"language-plain\">AVAuthorizationStatus status = [AVCaptureDevice&nbsp;authorizationStatusForMediaType:AVMediaTypeAudio];\n&nbsp;&nbsp;&nbsp; if (status == AVAuthorizationStatusNotDetermined) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [[AVAudioSession&nbsp;sharedInstance] requestRecordPermission:^(BOOL granted) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //granted代表是否授权\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }];\n&nbsp;&nbsp;&nbsp; } else if (status == AVAuthorizationStatusRestricted || status == AVAuthorizationStatusDenied) {// 未授权\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //引导用户跳入设置页面\n&nbsp;&nbsp;&nbsp; } else {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 已授权\n&nbsp;&nbsp;&nbsp; }\n</code></pre><p><strong>注意：从iOS 10开始，所有访问任何设备麦克风的应用都必须静态声明其意图。</strong>为此，应用程序现在必须在其Info.plist文件中包含NSMicrophoneUsageDescription键，并为此密钥提供目的字符串。当系统提示用户允许访问时，这个字符串将显示为警报的一部分。如果应用程序尝试访问任何设备的麦克风而没有此键和值，则应用程序将终止。</p><p>现在，音频渲染第一步——会话创建就完成了，接下来就可以进入音频渲染框架的学习了，我们就先来看AudioQueue渲染音频的部分。</p><h2>AudioQueue详解</h2><p>iOS为开发者在AudioToolbox这个framework中提供了一个名为AudioQueueRef的类，AudioQueue内部会完成以下职责：</p><ul>\n<li>连接音频的硬件进行录音或者播放；</li>\n<li>管理内存；</li>\n<li>根据开发者配置的格式，调用编解码器进行音频格式转换。</li>\n</ul><p>接下来让我们一起看一下AudioQueue播放音频的结构图：</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/cd/5a73413d808694e8db31c6109681d2cd.jpg?wh=1920x645\" alt=\"图片\" title=\"图2 AudioQueue播放音频结构图\"></p><p>AudioQueue暴露给开发者的接口如下：</p><ul>\n<li>使用正确的音频格式、回调方法等参数，创建出AudioQueueRef对象；</li>\n<li>为AudioQueueRef分配Buffer，并将Buffer入队，启动AudioQueue；</li>\n<li>在AudioQueueRef的回调中填充指定格式的音频数据，并且重新入队；</li>\n<li>暂停、恢复等常规接口。</li>\n</ul><p>了解了AudioQueue的内部职责和暴露给开发者的接口之后，就让我们一起看一下AudioQueue的运行流程吧！</p><h3>AudioQueue运行流程</h3><p>AudioQueue的整体运行流程分为启动和运行阶段，启动阶段主要是应用程序配置和启动AudioQueue；运行阶段主要是AudioQueue开始播放之后回调给应用程序填充buffer，并重新入队，3个buffer周而复始地运行起来；直到应用程序调用AudioQueue的Pause或者Stop等接口。下图是一个详细的运行流程：</p><p><img src=\"https://static001.geekbang.org/resource/image/8d/c1/8d23534dddf478fc7b89ce8e2d2023c1.jpg?wh=1920x2543\" alt=\"图片\" title=\"图3 AudioQueue运行流程\"></p><h4>启动阶段</h4><ol>\n<li>配置AudioQueue：</li>\n</ol><pre><code class=\"language-plain\">AudioQueueNewInput(&amp;dataformat, playCallback, (__bridge void *)self, NULL, NULL, 0, &amp;queueRef);\n</code></pre><p>dataformat就是音频格式，后面我们会重点讲解，playCallback是当AudioQueue需要我们填充数据时的回调方法，函数返回值为OSStatus类型，如果为noErr则说明配置成功。</p><ol start=\"2\">\n<li>分配3个Buffer，并且依次灌到AudioQueue中：</li>\n</ol><pre><code class=\"language-plain\">for (int i = 0; i &lt; kNumberBuffers; i++) {\n  AudioQueueAllocateBuffer(queueRef, bufferBytesSize, &amp;buffers[i]);\n  AudioQueueEnqueueBuffer(queueRef, buffers[i], 0, NULL);\n}\n</code></pre><p>Buffer类型为AudioQueueBufferRef，是AudioQueue对外提供的数据封装，具体每个Buffer的大小是如何决定的，我会在后面与dataformat一起讲解。</p><ol start=\"3\">\n<li>调用Play方法进行播放：</li>\n</ol><pre><code class=\"language-plain\">AudioQueueStart(queueRef, NULL)\n</code></pre><h4>运行阶段</h4><p>启动完毕后，接下来就到运行阶段了，运行阶段主要分为4步：</p><ol>\n<li>AudioQueue启动之后会播放第一个buffer；</li>\n<li>当播放完第一个buffer之后，会继续播放第二个buffer，但是与此同时将第一个buffer回调给业务层由开发者进行填充，填充完毕重新入队；</li>\n<li>第二个buffer播放完毕后，会继续播放第三个buffer，与此同时会将第二个buffer回调给业务层由开发者进行填充，填充完毕重新入队；</li>\n<li>第三个buffer播放完毕后，会继续循环播放队列中的第一个buffer，也会将第三个buffer回调给业务层由开发者进行填充，填充完毕重新入队。</li>\n</ol><pre><code class=\"language-plain\">static void playCallback(void *aqData, AudioQueueRef&nbsp;inAQ, AudioQueueBufferRef&nbsp;inBuffer) {\n  KSAudioPlayer *player = (__bridge KSAudioPlayer *)aqData;\n  //TODO: Fill Data\n  AudioQueueEnqueueBuffer(player-&gt;queueRef, inBuffer, numPackets,&nbsp;player.mPacketDescs);\n}\n</code></pre><p>这样一来，整个AudioQueue的运行流程就讲解完了，还记得我们在前面说过AudioQueue内部会进行调用编解码器进行音频格式转换吗？接下来我们就详细介绍一下AudioQueue中的Codec运行流程。</p><h3>AudioQueue中Codec运行流程</h3><p>值得一提的是，AudioQueue的强大之处在于<strong>开发者可以不用关心播放的数据的编解码格式，它内部会帮助开发者将Codec的事情做好</strong>，所以这部分的流程我们是有必要单拎出来看一下的。</p><p><img src=\"https://static001.geekbang.org/resource/image/58/23/58bb49c9c742bd6955fb5a21aafe6e23.jpg?wh=1920x967\" alt=\"图片\" title=\"图4 Codec运行流程\"></p><p>如图所示，主要分为3个步骤：</p><ol>\n<li>开发者配置AudioQueue的时候告诉AudioQueue具体编码格式；</li>\n<li>开发者在回调函数中按照原始格式填充buffer；</li>\n<li>AudioQueue会自己采用合适的Codec将压缩数据解码成PCM进行播放。</li>\n</ol><p>介绍完Codec相关的流程，你可能还有一个疑问，就是数据格式以及音频数据到底应该如何设置以及填充呢？这个其实就是之前我们说要重点讲解的音频格式（dataformat），接下来我们就一起来学习一下吧！</p><h3>iOS平台的音频格式</h3><p>iOS平台的音频格式是ASBD（AudioStreamBasicDescription），用来描述音频数据的表示方式，结构体如下：</p><pre><code class=\"language-plain\">struct AudioStreamBasicDescription\n{\n&nbsp;&nbsp;&nbsp; AudioFormatID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mFormatID;\n    Float64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mSampleRate;    \n    UInt32              mChannelsPerFrame;    \n    UInt32              mFramesPerPacket;\n&nbsp;&nbsp;&nbsp; AudioFormatFlags&nbsp;&nbsp;&nbsp; mFormatFlags;\n&nbsp;&nbsp;&nbsp; UInt32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mBytesPerPacket;\n&nbsp;&nbsp;&nbsp; UInt32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mBytesPerFrame;\n&nbsp;&nbsp;&nbsp; UInt32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mBitsPerChannel;\n&nbsp;&nbsp;&nbsp; UInt32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mReserved;\n};\ntypedef struct AudioStreamBasicDescription&nbsp; AudioStreamBasicDescription;\n</code></pre><p>针对结构体中每个字段，我们需要配上一个实际的案例来逐个讲解一下，先看下面这个格式的配置：</p><pre><code class=\"language-plain\">UInt32 bytesPerSample = sizeof(Float32);\nAudioStreamBasicDescription&nbsp;asbd;\nbzero(&amp;asbd, sizeof(asbd));&nbsp;\nasbd.mFormatID = kAudioFormatLinearPCM;\nasbd.mSampleRate = _sampleRate;&nbsp;\nasbd.mChannelsPerFrame = channels;&nbsp;\nasbd.mFramesPerPacket = 1;&nbsp;\nasbd.mFormatFlags = kAudioFormatFlagsNativeFloatPacked | kAudioFormatFlagIsNonInterleaved;\nasbd.mBitsPerChannel = 8 * bytesPerSample;\nasbd.mBytesPerFrame = bytesPerSample;\nasbd.mBytesPerPacket = bytesPerSample;&nbsp;\n</code></pre><ul>\n<li>mFormatID这个参数是用来指定音频的编码格式，此处音频编码格式指定为PCM格式；</li>\n<li>接下来设置声音的采样率、声道数以及每个Packet有几个Frame这三个参数；</li>\n<li>mFormatFlags是用来描述声音表示格式的参数，代码中的第一个参数指定每个sample的表示格式是Float格式。这个类似于我们之前讲解的每个sample使用两个字节（SInt16）来表示；然后是后面的参数NonInterleaved，表面理解这个单词的意思是非交错的，其实对音频来说，就是左右声道是非交错存放的，实际的音频数据会存储在一个AudioBufferList结构中的变量mBuffers中。<strong>如果mFormatFlags指定的是NonInterleaved，那么左声道就会在mBuffers[0]里面，右声道就会在mBuffers[1]里面，而如果mFormatFlags指定的是Interleaved的话，那么左右声道就会交错排列在mBuffers[0]里面，</strong>理解这一点对后续的开发是十分重要的；</li>\n<li>接下来的mBitsPerChannel表示的是一个声道的音频数据用多少位来表示，前面我们已经知道每个采样使用Float来表示，所以这里就使用8乘以每个采样的字节数来赋值；</li>\n<li>最后是参数mBytesPerFrame和mBytesPerPacket的赋值，这里需要根据mFormatFlags的值来分配。如果在NonInterleaved的情况下，就赋值为bytesPerSample（因为左右声道是分开存放的）；但如果是Interleaved的话，那么就应该是bytesPerSample * channels（因为左右声道是交错存放的），这样才能表示一个Frame里面到底有多少个byte。</li>\n</ul><p>如果要播放的是一个MP3或者M4A的文件，这个ASBD应该如何确定呢？请看下面这个代码：</p><pre><code class=\"language-plain\">// 打开文件\nNSURL *fileURL = [NSURL URLWithString:filePath];\nOSStatus status = AudioFileOpenURL((__bridge CFURLRef)fileURL, kAudioFileReadPermission, kAudioFileCAFType, &amp;_mAudioFile);\nif (status != noErr) {\n  NSLog(@\"open file error\");\n}&nbsp;&nbsp; &nbsp;\n// 获取文件格式\nUInt32 dataFromatSize = sizeof(dataFormat);\nAudioFileGetProperty(_mAudioFile, kAudioFilePropertyDataFormat, &amp;dataFromatSize, &amp;dataFormat);\n</code></pre><p>第一步是用AudioFile打开文件，如果打开成功的话，直接获取出这个AudioFile的DataFormat就好了，是不是很简单呢？对于填充数据也比较简单，直接从AudioFile中读取原始数据就可以了。</p><p>学到这里可能你会有疑问，绕了一大圈，用AudioQueue就直接播放了一个音频文件，那我直接使用AVAudioPlayer或者AVPlayer来播放这个音频文件不更简单吗？的确是的，但我更想通过这个例子来告诉你：<strong>这就是iOS给开发者提供的强大的多媒体处理能力，而AudioQueue更适合开发者在一些更底层的数据处理的场景下使用。</strong></p><h2>小结</h2><p>最后，我们可以一起来回顾一下。<br>\n<img src=\"https://static001.geekbang.org/resource/image/bd/40/bd314e0090edf7c1636a5cd6d8a9cd40.png?wh=1725x751\" alt=\"\"><br>\n这节课我们对iOS的音频框架有了一个大致的了解。其中最重要的两个就是AudioQueue和AudioUnit。AudioQueue使用起来非常方便，它是实现录制与播放功能最简单的API接口，就算你不知道内部的细节，也可以简单地完成播放PCM数据的功能。所以如果你的输入是PCM，比如视频播放器场景、RTC等需要业务自己Mix或者处理PCM的场景，那么使用AudioQueue是非常适合的一种方式。</p><p>AudioUnit是iOS中最底层的音频框架，对音频能够实现更高程度的控制，所以也是我们的必学内容之一，下节课我会详细地讲一讲怎么使用AudioUnit实现音频的渲染，期待一下吧！</p><p>今天我通过代码带你创建并设置了AVAudioSession，还带你详细了解了AudioQueue的运行流程以及iOS平台的音频格式ASBD，希望你学完之后可以自己动手练一练，把今天学习的内容内化到自己的知识网络中。</p><h2>思考题</h2><p>学而不思则罔，最后我给你留一道思考题：你思考一下AudioQueue相比于AVPlayer或者AVAudioPlayer，它的灵活性或者说好处在哪儿呢？欢迎在评论区分享你的思考，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"开篇词｜系统学习移动端音视频开发技术，为业务赋能","id":543579},"right":{"article_title":"02｜iOS平台音频渲染（二）：使用 AudioUnit 渲染音频","id":543993}}},{"article_id":543993,"article_title":"02｜iOS平台音频渲染（二）：使用 AudioUnit 渲染音频","article_content":"<p>你好，我是展晓凯。</p><p>上节课我们学习了iOS平台的音频框架的第一部分，深入了解了AVAudioSession以及AudioQueue的使用方法，同时也学习了iOS音频格式的表示方法，就是ASBD。其中重点学习了AudioQueue渲染音频的方法。AudioQueue这个API其实是介于AVPlayer/AVAudioPlayer与Audio Unit之间的一个音频渲染框架。如果我们想对音频有更高层次的控制，而AudioQueue满足不了我们的开发需求的时候，我们应该使用哪个音频框架呢？</p><p><img src=\"https://static001.geekbang.org/resource/image/53/9a/5398e87e2955052a62f9dc5accc1b89a.png?wh=1744x704\" alt=\"图片\" title=\"图1 iOS平台的音频框架（图片来自苹果官网）\"></p><p>没错，就是AudioUnit。作为iOS最底层的音频框架，AudioUnit是音视频开发者必须要掌握的内容。我们在开发音频相关产品的时候，如果对音频有更高程度的控制、性能以及灵活性需求，或者想使用一些特殊功能（比如回声消除、实时耳返）的时候，就可以直接使用AudioUnit这一层的API，这些接口是面向C语言的。</p><p>随着iOS的API不断升级，AudioUnit还逐渐演变出了AUGraph与AVAudioEngine框架，它们可以为你的App融入更强大的音频多媒体能力。</p><p>正如苹果官方文档中描述的，AudioUnit提供了快速的音频模块化处理功能，如果是在以下场景中，更适合使用AudioUnit，而不是高层次的音频框架。</p><!-- [[[read_end]]] --><ul>\n<li>在VOIP的应用场景下，想使用低延迟的音频I/O；</li>\n<li>合成多路声音并且回放，比如游戏或者音乐合成器（弹唱、多轨乐器）的应用；</li>\n<li>使用AudioUnit里特有的功能，比如：均衡器、压缩器、混响器等效果器，以及回声消除、Mix两轨音频等；</li>\n<li>需要图状结构来处理音频时，可以使用iOS提供的AUGraph和AVAudioEngine的API接口，把音频处理模块组装到灵活的图状结构中。</li>\n</ul><p>既然AudioUnit这么强大，我们该怎么好好利用它呢？不要急，接下来我们就一起来看一下AudioUnit的使用方法。</p><h2>AudioUnit</h2><p>这部分我会从分类、创建、参数设置、构建处理框架四个方面来讲解，我们先看AudioUnit分为哪几类。</p><h3>AudioUnit的分类</h3><p>iOS根据AudioUnit的功能不同，将AudioUnit分成了5大类，了解AudioUnit的分类对于音频渲染和处理是非常重要的。这里我们会从全局视角来认识一下每个大类型（Type）以及大类型下面的子类型（SubType），并且还会介绍每个大类型下面子类型AudioUnit的用途，以及对应参数的意义。</p><ol>\n<li><strong>Effect Unit</strong></li>\n</ol><p>第一个大类型是kAudioUnitType_Effect，主要提供声音特效处理的功能。子类型及用途如下：</p><ul>\n<li>均衡效果器：子类型是kAudioUnitSubType_NBandEQ，主要作用是给声音的某一些频带增强或者减弱能量，这个效果器需要指定多个频带，然后为每个频带设置宽度以及增益，最终将改变声音在频域上的能量分布。</li>\n<li>压缩效果器：子类型是kAudioUnitSubType_DynamicsProcessor，主要作用是当声音较小的时候可以提高声音的能量，当声音能量超过了设置的阈值，可以降低声音的能量，当然我们要设置合适的作用时间和释放时间以及触发值，最终可以将声音在时域上的能量压缩到一定范围之内。</li>\n<li>混响效果器：子类型是kAudioUnitSubType_Reverb2，是对人声处理非常重要的效果器，可以想象我们在一个空房子中，有非常多的反射声和原始声音叠加在一起，可能从听感上会更有震撼力，但是同时也会使原始声音更加模糊，遮盖掉原始声音的一些细节，所以混响设置得大或小对不同的人来讲非常不一致，可以根据自己的喜好来设置。</li>\n</ul><p>Effect Unit下最常使用的就是这三种效果器，当然这个大类型下面还有很多种子类型的效果器，像高通（High Pass）、低通（Low Pass）、带通（Band Pass）、延迟（Delay）、压限（Limiter）等效果器，你可以自己使用一下，感受一下效果。</p><ol start=\"2\">\n<li><strong>Mixer Units</strong></li>\n</ol><p>第二个大类型是kAudioUnitType_Mixer，主要提供Mix多路声音的功能。子类型及用途如下。</p><ul>\n<li>3D Mixer：这个效果器在移动设备上无法使用，只能在OS X上使用，所以这里不介绍了。</li>\n<li>MultiChannelMixer：子类型是kAudioUnitSubType_MultiChannelMixer，这个效果器是我们重点介绍的对象，<strong>它是多路声音混音的效果器，可以接收多路音频的输入，还可以分别调整每一路音频的增益与开关，并将多路音频合并成一路</strong>，这个效果器在处理音频的图状结构中非常有用。</li>\n</ul><ol start=\"3\">\n<li><strong>I/O Units</strong></li>\n</ol><p>第三个大类型是kAudioUnitType_Output，它的用途就像它分类的名字一样，主要提供的就是I/O功能。子类型及用途如下：</p><ul>\n<li>RemoteIO：子类型是kAudioUnitSubType_RemoteIO，从名字上也可以看出，这是用来采集音频与播放音频的，当开发者在应用场景中要使用麦克风及扬声器的时候，都会用到这个AudioUnit。</li>\n<li>Generic Output：子类型是kAudioUnitSubType_GenericOutput，当开发者需要离线处理，或者说在AUGraph中不使用Speaker（扬声器）来驱动整个数据流，而是希望使用一个输出（可以放入内存队列或者进行磁盘I/O操作）来驱动数据流的话，就使用这个子类型。</li>\n</ul><ol start=\"4\">\n<li><strong>Format Converter Units</strong></li>\n</ol><p>第四个大类型是kAudioUnitType_FormatConverter，提供格式转换的功能，比如：采样格式由Float到SInt16的转换、交错和平铺的格式转换、单双声道的转换等，子类型及用途说明如下。</p><ul>\n<li>AUConverter：子类型是kAudioUnitSubType_AUConverter，这是我们要重点介绍的格式转换效果器，某些效果器对输入的音频格式有明确要求，比如3D Mixer Unit就必须使用UInt16格式的sample，或者开发者将音频数据后续交给一些其他编码器处理，又或者开发者想使用SInt16格式的PCM裸数据进行其他CPU上音频算法计算等场景下，就需要用到这个ConverterNode。</li>\n</ul><p>比较典型的场景是我们自定义的一个音频播放器，由FFmpeg解码出来的PCM数据是SInt16格式表示的，我们不可以直接让RemoteIO Unit播放，而是需要构建一个ConvertNode，将SInt16格式表示的数据转换为Float32表示的数据，然后再给到RemoteIO Unit，最终才能正常播放出来。</p><ul>\n<li>Time Pitch：子类型是kAudioUnitSubType_NewTimePitch，即变速变调效果器，这是一个比较意思的效果器，可以对声音的音高、速度进行更改，像Tom猫这样的应用场景就可以使用这个效果器来实现。</li>\n</ul><ol start=\"5\">\n<li><strong>Generator Units</strong></li>\n</ol><p>第五个大类型是kAudioUnitType_Generator，在开发中我们经常用它来提供播放器的功能。子类型及用途说明如下。</p><ul>\n<li>AudioFilePlayer：子类型是kAudioUnitSubType_AudioFilePlayer，在AudioUnit里面，如果我们的输入不是麦克风，而是一个媒体文件，要怎么办呢？当然也可以自己进行解码，通过转换之后给RemoteIO Unit播放出来。但其实还有一种更加简单、方便的方式，那就是使用AudioFilePlayer这个AudioUnit，其实数据源还是会调用AudioFile里面的解码功能，将媒体文件中的压缩数据解压成为PCM裸数据，最终再交给AudioFilePlayer Unit进行后续处理。</li>\n</ul><p>这里需要注意，<strong>我们必须在AUGraph初始化了之后，再去配置AudioFilePlayer的数据源以及播放范围等属性，否则会出现错误。</strong></p><h3>创建AudioUnit</h3><p>构建AudioUnit时，需要指定类型（Type）、子类型（Subtype）以及厂商（Manufacture）。</p><p>类型就是刚刚我们讲到的几个大类型；而子类型是这个大类型下面的小类型，比如Effect这个大类型下面有EQ、Compressor、limiter等子类型；厂商一般情况下比较固定，直接写成kAudioUnitManufacturer_Apple就好了。</p><p>利用以上这三个变量，开发者就可以完整描述出一个AudioUnit了，我们使用下面的代码创建一个RemoteIO类型的AudioUnit的描述。</p><pre><code class=\"language-plain\">AudioComponentDescription&nbsp;ioUnitDescription;\nioUnitDescription.componentType&nbsp;= kAudioUnitType_Output;\nioUnitDescription.componentSubType&nbsp;= kAudioUnitSubType_RemoteIO;\nioUnitDescription.componentManufacturer=kAudioUnitManufacturer_Apple;\nioUnitDescription.componentFlags&nbsp;= 0;\nioUnitDescription.componentFlagsMask&nbsp;= 0;\n</code></pre><p>上述代码构造了RemoteIO这个AudioUnit描述的结构体，那如何再使用这个描述来构造真正的AudioUnit呢？有两种方式：第一种方式是直接使用AudioUnit裸的创建方式；第二种方式则是使用AUGraph和AUNode（其实一个AUNode就是对AudioUnit的封装，可以理解为一个AudioUnit的Wrapper）方式来构建。下面我来介绍一下这两种方式。</p><ol>\n<li><strong>裸创建方式</strong></li>\n</ol><p>首先根据AudioUnit描述，找出实际的AudioUnit类型：</p><pre><code class=\"language-plain\">AudioComponent&nbsp;ioUnitRef = AudioComponentFindNext(NULL,&nbsp;&amp;ioUnitDescription);\n</code></pre><p>然后声明一个AudioUnit引用：</p><pre><code class=\"language-plain\">AudioUnit&nbsp;ioUnitInstance;\n</code></pre><p>最后根据类型创建出这个AudioUnit实例：</p><pre><code class=\"language-plain\">AudioComponentInstanceNew(ioUnitRef,&nbsp;&amp;ioUnitInstance);\n</code></pre><ol start=\"2\">\n<li><strong>AUGraph创建方式</strong></li>\n</ol><p>首先声明并且实例化一个AUGraph：</p><pre><code class=\"language-plain\">AUGraph&nbsp;processingGraph;\nNewAUGraph(&amp;processingGraph);\n</code></pre><p>然后利用AudioUnit的描述在AUGraph中按照描述增加一个AUNode：</p><pre><code class=\"language-plain\">AUNode&nbsp;ioNode;\nAUGraphAddNode(processingGraph,&nbsp;&amp;ioUnitDescription,&nbsp;&amp;ioNode);\n</code></pre><p>接下来打开AUGraph，其实打开AUGraph的过程也是间接实例化AUGraph中所有的AUNode的过程。<strong>注意，必须在获取AudioUnit之前打开整个Graph</strong>，否则我们不能从对应的AUNode里面获取到正确的AudioUnit。</p><pre><code class=\"language-plain\">AUGraphOpen(processingGraph);\n</code></pre><p>最后在AUGraph中的某个Node里面获得AudioUnit的引用：</p><pre><code class=\"language-plain\">AudioUnit&nbsp;ioUnit;\nAUGraphNodeInfo(processingGraph,&nbsp;ioNode, NULL,&nbsp;&amp;ioUnit);\n</code></pre><p>无论使用上面的哪一种方式，都可以创建出我们想要的AudioUnit，而具体应该使用哪一种方式，其实还是应该根据实际的应用场景来决定。结合我的实际工作经验，我认为<strong>使用AUGraph的结构可以在我们的应用中搭建出扩展性更高的系统</strong>，所以我推荐你使用第二种方式，我们整个音频处理系统就是使用的第二种方式来搭建的。</p><p></p><h3>RemoteIO详解</h3><p>AudioUnit创建好之后，就应该对其进行配置和使用了，因为我们后面的播放器项目会用到RemoteIO Unit，所以在这里我就以RemoteIO这个AudioUnit为例，详细讲解AudioUnit的使用。</p><p>RemoteIO这个AudioUnit是与硬件IO相关的一个Unit，它可以控制硬件设备的输入和输出（I代表Input，O代表Output）。输入端是麦克风（机身麦克风或者蓝牙耳机麦克风），输出端的话可能是扬声器（Speaker）或者耳机。如果要同时使用输入输出，即K歌应用中的耳返功能（用户在唱歌或者说话的同时，耳机中会将麦克风收录的声音播放出来，让用户自己能听到自己的声音），需要开发者将它们连接起来。</p><p><img src=\"https://static001.geekbang.org/resource/image/16/1c/16287030b548e0600f7e5fcd4d611f1c.jpg?wh=1920x1063\" alt=\"图片\" title=\"图2 RemoteIO\"></p><p>如图所示，RemoteIO Unit分为Element0和Element1，其中Element0控制输出端，Element1控制输入端，同时每个Element又分为Input Scope和Output Scope。如果开发者想要使用扬声器的播放声音功能，那么必须将这个Unit的Element0的OutputScope和Speaker进行连接。而如果开发者想要使用麦克风的录音功能，那么必须将这个Unit的Element1的InputScope和麦克风进行连接。使用扬声器的代码如下：</p><pre><code class=\"language-plain\">OSStatus status = noErr;\nUInt32 oneFlag = 1;\nUInt32 busZero = 0;//Element 0\nstatus = AudioUnitSetProperty(remoteIOUnit,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kAudioOutputUnitProperty_EnableIO,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kAudioUnitScope_Output,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; busZero,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;oneFlag,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sizeof(oneFlag));\nCheckStatus(status, @\"Could not Connect To Speaker\", YES);\n</code></pre><p>上面这段代码就是把RemoteIO Unit中Element0的OutputScope连接到Speaker上，连接过程会返回一个OSStatus类型的值，可以使用自定义的CheckError函数来判断错误并且打印Could not Connect To Speaker提示。具体的CheckError函数如下：</p><pre><code class=\"language-plain\">static void CheckStatus(OSStatus status, NSString *message, BOOL fatal)\n{\n&nbsp;&nbsp;&nbsp; if(status != noErr)\n&nbsp;&nbsp;&nbsp; {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; char fourCC[16];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *(UInt32 *)fourCC = CFSwapInt32HostToBig(status);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fourCC[4] = '\\0';\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if(isprint(fourCC[0]) &amp;&amp; isprint(fourCC[1]) &amp;&amp; isprint(fourCC[2]) &amp;&amp;\nisprint(fourCC[3]))\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NSLog(@\"%@: %s\", message, fourCC);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NSLog(@\"%@: %d\", message, (int)status);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if(fatal)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exit(-1);\n&nbsp;&nbsp;&nbsp; }\n</code></pre><p>连接成功之后，就应该给AudioUnit设置数据格式（ASBD）了，ASBD我们在前面已经详细讲过了，这里不再赘述。构造好合适的ASBD结构体，最终设置给AudioUnit对应的Scope（Input/Output），代码如下：</p><pre><code class=\"language-plain\">AudioUnitSetProperty(&nbsp;remoteIOUnit,kAudioUnitProperty_StreamFormat,&nbsp;\nkAudioUnitScope_Output, 1,&nbsp;&amp;asbd, sizeof(asbd));\n</code></pre><p>播放了解清楚了，那么接下来我们一起看下如何控制输入，我们通过一个实际的场景来学习这部分的内容。在K歌应用的场景中，会采集到用户的人声处理之后且立即给用户一个耳返（将声音在50ms之内输出到耳机中，让用户可以听到），那么如何让RemoteIO Unit利用麦克风采集出来的声音，经过中间效果器处理，最终输出到Speaker中播放给用户呢？</p><p>这里我来介绍一下，如何以AUGraph的方式将声音采集、处理以及声音输出的整个过程管理起来。</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/d9/f435eb739f6eec3039e6ebbe1180afd9.jpg?wh=1920x1096\" alt=\"图片\" title=\"图3 采集、处理及输出声音的过程\"></p><p>如上图所示，首先要知道数据可以从通道中传递是由最右端Speaker（RemoteIO Unit）来驱动的，它会向它的前一级AUNode去要数据（图中序号1），然后它的前一级会继续向上一级节点要数据（图中序号2），最终会从我们的RemoteIO Unit的Element1（即麦克风）中取得数据，这样就可以将数据按照相反的方向一级一级地传递下去（图中序号4、5），最终传递到RemoteIOUnit的Element0（即Speaker，图中序号6）就可以让用户听到了。</p><p>当然这时候你可能会想离线处理的时候是不能播放出来的，那么应该由谁来进行驱动呢？其实在离线处理的时候，应该使用Mixer Unit这个大类型下面的子类型为Generic Output的AudioUnit来做驱动端。那么这些AudioUnit或者说AUNode是如何进行连接的呢？有两种方式，第一种方式是直接将AUNode连接起来；第二种方式是通过回调把两个AUNode连接起来。下面我们分别来介绍下这两种方式。</p><ol>\n<li><strong>直接连接的方式</strong></li>\n</ol><pre><code class=\"language-plain\">AUGraphConnectNodeInput(mPlayerGraph, mPlayerNode, 0, mPlayerIONode, 0);\n</code></pre><p>这段代码是把Audio File Player Unit和RemoteIO Unit连接起来了，当RemoteIO Unit需要播放的数据的时候，就会调用AudioFilePlayer Unit来获取数据，最终数据会传递到RemoteIO中播放出来。</p><ol start=\"2\">\n<li><strong>回调的方式</strong></li>\n</ol><pre><code class=\"language-plain\">AURenderCallbackStruct&nbsp;renderProc;\nrenderProc.inputProc = &amp;inputAvailableCallback;\nrenderProc.inputProcRefCon = (__bridge void *)self;\nAUGraphSetNodeInputCallback(mGraph, ioNode, 0, &amp;finalRenderProc);\n</code></pre><p>这段代码首先构造了一个AURenderCallback的结构体，结构体中需要指定一个回调函数，然后设置给RemoteIO Unit，当这个RemoteIO Unit需要数据输入的时候就会回调这个回调函数，而回调函数的实现如下：</p><pre><code class=\"language-plain\">static OSStatus&nbsp;renderCallback(void *inRefCon, AudioUnitRenderActionFlags&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *ioActionFlags, const AudioTimeStamp *inTimeStamp, UInt32\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inBusNumber, UInt32 inNumberFrames, AudioBufferList *ioData)\n{\n&nbsp;&nbsp;&nbsp; OSStatus result = noErr;\n&nbsp;&nbsp;&nbsp; __unsafe_unretained&nbsp;AUGraphRecorder *THIS = (__bridge\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AUGraphRecorder *)inRefCon;\n&nbsp;&nbsp;&nbsp; AudioUnitRender(THIS-&gt;mixerUnit, ioActionFlags, inTimeStamp, 0,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inNumberFrames, ioData);\n&nbsp;&nbsp;&nbsp; return result;\n}\n</code></pre><p>这个回调函数中主要做两件事情，第一件事情是去Mixer Unit里面要数据，通过调用AudioUnitRender的方式来驱动Mixer Unit获取数据，得到数据之后放入ioData中，这也就填充了回调方法中的数据，从而实现了Mixer Unit和RemoteIO Unit的连接。</p><p>如果要播放一个音频文件，就自己构造一套AUGraph是十分不方便的，但我们这样做实际有两个目的：<strong>其一是为了让你体验在开发iOS平台的程序时，优先使用iOS平台自身提供的API的便捷性与重要性；其二是为了给后续的视频播放器项目打下基础。</strong>你可以好好学习一下这两个实例，充分感受iOS平台为开发者提供的强大的多媒体开发API。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><p>这节课我们重点学习了使用AudioUnit来渲染音频的方法。其实在iOS开发中除了这些底层的音频框架，还有一些常用的上层框架，了解这些框架的特点与适用场景能够帮助我们做技术选型，接下来我们一起简单看下iOS为开发者提供的各个层次的音频播放框架，以便以后在你的应用中做技术选型：</p><ul>\n<li>AVAudioPlayer：如果你要直接播放一个本地音频文件（无论是本地路径还是内存中的数据），使用AVAudioPlayer会是最佳选择；</li>\n<li>AVPlayer：如果是普通网络协议（HTTP、HLS）音频文件要直接播放，使用AVPlayer会是最佳选择；</li>\n<li>AudioQueue：但是如果你的输入是PCM（比如视频播放器场景、RTC等需要业务自己Mix或者处理PCM的场景），其实使用AudioQueue是适合的一种方式；</li>\n<li>AudioUnit：如果需要构造一个复杂的低延迟采集、播放、处理的音频系统，那么使用AudioUnit（实际实现可能是使用AUGraph或者AVAudioEngine框架）会是最佳选择。</li>\n</ul><p><strong>真正好的架构师应该像裁缝一样懂得量体裁衣</strong>，要了解清楚当前应用场景的现状和未来，然后根据自己的经验做出合理的技术选型，让开发的App可以快速、高质量地上线并且还有一定的扩展性，所以接下来的挑战就看你的了。</p><h2>思考题</h2><p>如果你想要实现一个K歌录制的功能，场景描述如下：</p><ol>\n<li>播放伴奏的同时，可以将人声进行录音；</li>\n<li>可以实时听到用户自己的人声耳返，并且要在耳返中加入混响效果器；</li>\n<li>可以调节伴奏与人声的音量；</li>\n</ol><p>请你思考一下，使用AUGraph如何构造出你的图状结构。欢迎在评论区分享你的思考，也欢迎把这节课分享给更多对音视频感兴趣的朋友，我们共同交流、共同进步。我们下节课再见！</p>","neighbors":{"left":{"article_title":"01｜iOS平台音频渲染（一）：使用AudioQueue渲染音频","id":543649},"right":{"article_title":"03｜Android平台音频渲染与技术选型","id":545000}}},{"article_id":545000,"article_title":"03｜Android平台音频渲染与技术选型","article_content":"<p>你好，我是展晓凯。</p><p>前两节课我们一起学习了iOS平台的音频渲染技术，深入地了解了AudioQueue和AudioUnit两个底层的音频框架，了解这些音频框架便于我们做技术选型，可以给我们的应用融入更强大的功能。那除了iOS平台外，Android平台的音视频开发也有着相当大的需求，所以这节课我们一起来学习Android平台的音频渲染技术。</p><p>由于Android平台的厂商与定制Rom众多，碎片化特别严重，所以系统地学习音频渲染是非常重要的。这节课我会先从音频渲染的技术选型入手，向你介绍Android系统上渲染音频方法的所有可能性，然后依次讲解常用技术选型的内部原理与使用方法。</p><h2>技术选型及其优缺点</h2><p>Android系统为开发者在SDK以及NDK层提供了多种音频渲染的方法，每一种渲染方法其实也是为不同的场景而设计的，我们必须要了解每一种方法的最佳实践是什么，这样在开发工作中才能如鱼得水地使用它们。</p><h3>SDK层的音频渲染</h3><p>Android系统在SDK层（Java层提供的API）为开发者提供了3套常用的音频渲染方法，分别是：MediaPlayer、SoundPool和AudioTrack。这三个API的推荐使用场景是不同的。</p><!-- [[[read_end]]] --><ul>\n<li>MediaPlayer适合在后台长时间播放本地音乐文件或者在线的流式媒体文件，相当于是一个端到端的播放器，可以播放音频也可以播放视频，它的封装层次比较高，使用方式也比较简单。</li>\n<li>SoundPool也是一个端到端的音频播放器，优点是：延时较低，比较适合有交互反馈音的场景，适合播放比较短的音频片段，比如游戏声音、按键声、铃声片段等，它可以同时播放多个音频。</li>\n<li>AudioTrack是直接面向PCM数据的音频渲染API，所以也是一个更加底层的API，提供了非常强大的控制能力，适合低延迟的播放、流媒体的音频渲染等场景，由于是直接面向PCM的数据进行渲染，所以一般情况下需要结合解码器来使用。</li>\n</ul><h3>NDK层的音频渲染</h3><p>Android系统在NDK层（Native层提供的API，即C或者C++层可以调用的API）提供了2套常用的音频渲染方法，分别是OpenSL ES和AAudio，它们都是为Android的低延时场景（实时耳返、RTC、实时反馈交互）而设计的，下面我们一起来看一下。</p><ul>\n<li>OpenSL&nbsp;ES：是Khronos Group开发的&nbsp;OpenSL ES™ API 规范的实现，专用于 Android低延迟高性能的音频场景，API接口设计会有一些晦涩、复杂，目前Google已经不推荐开发者把OpenSL&nbsp;ES用于新应用的开发了。但是在Android8.0系统以下以及一些碎片化的Android设备上它具有更好的兼容性，所以掌握这种音频渲染方法也是十分重要的。</li>\n<li>AAudio：专门为低延迟、高性能音频应用而设计的，API设计精简，是Google推荐的新应用构建音频的应用接口。掌握这种音频渲染方法，为现有应用中增加这种音频的渲染能力是十分有益的。但是它仅适合Android&nbsp;8.0及以上版本，并且在一些品牌的特殊Rom版本中适配性不是特别好。</li>\n</ul><p>NDK层的这两套音频渲染方法适用于不同的Android版本，可以应用在不同的场景中，因此了解这两种音频渲染的方法对我们的开发工作来说是非常必要的，一会儿我们会再就这两种方法深入展开讨论。</p><p>通过上述讲解，想必你已经了解了Android平台上所有的音频渲染的方法，而这里面最通用的渲染PCM的方法就是AudioTrack，那么接下来我们首先从AudioTrack开始讲起。</p><h2>AudioTrack</h2><p>由于AudioTrack是Android SDK层提供的最底层的音频播放API，因此只允许输入PCM裸数据。与MediaPlayer相比，对于一个压缩的音频文件（比如MP3、AAC等文件），它需要开发者自己来实现解码操作和缓冲区控制。由于这节课我们重点关注的是音频渲染的知识，所以这个部分我们只介绍如何使用AudioTrack来渲染音频PCM数据，对于缓冲区的控制机制会在播放器实战部分详细讲一下。</p><p>首先让我们一起来看一下AudioTrack的工作流程：</p><ol>\n<li>根据音频参数信息，配置出一个AudioTrack的实例。</li>\n<li>调用play方法，将AudioTrack切换到播放状态。</li>\n<li>启动播放线程，循环向AudioTrack的缓冲区中写入音频数据。</li>\n<li>当数据写完或者停止播放的时候，停止播放线程，并且释放所有资源。</li>\n</ol><p>根据上述AudioTrack的工作流程，我会给你详细地讲解流程中的每个步骤。</p><h3>第一步：配置AudioTrack</h3><p>我们先来看一下AudioTrack的参数配置，要想构造出一个AudioTrack类型的实例，得先了解一下它的构造函数原型，如下所示：</p><pre><code class=\"language-plain\">public AudioTrack(int streamType, int sampleRateInHz, int channelConfig,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int audioFormat, int bufferSizeInBytes, int mode);\n</code></pre><p>其中构造函数中的参数说明如下：</p><ul>\n<li>streamType，在Android手机上有多重音频管理策略，比如你按一下手机侧边的按键或者在系统设置中，可以看到有多个类型的音量管理，这其实就是不同音频策略的音量控制展示。当系统有多个进程需要播放音频的时候，管理策略会决定最终的呈现效果，该参数的可选值以常量的形式定义在类AudioManager中，主要包括：</li>\n</ul><pre><code class=\"language-plain\">STREAM_VOCIE_CALL：电话声音\nSTREAM_SYSTEM：系统声音\nSTREAM_RING：铃声\nSTREAM_MUSCI：音乐声\nSTREAM_ALARM：警告声\nSTREAM_NOTIFICATION：通知声\n</code></pre><ul>\n<li>sampleRateInHz，采样率，即播放的音频每秒钟会有多少次采样，可选用的采样频率列表为：8000、16000、22050、24000、32000、44100、48000等。采样率越高声音的还原度就越高，普通的语音通话可能16k的采样频率就够了，但是如果高保真的场景，比如：K歌、音乐、短视频、ASMR等，就需要44.1k以上的采样频率，你可以根据自己的应用场景进行合理的选择。</li>\n<li>channelConfig，声道数（通道数）的配置，可选值以常量的形式配置在类AudioFormat中，常用的是CHANNEL_IN_MONO（单声道）、CHANNEL_IN_STEREO（双声道）。因为现在大多数手机的麦克风都是伪立体声采集，考虑到性能，我建议你使用单声道进行采集，然后在声音处理阶段（比如混响、HRTF等效果器）转变为立体声的效果。</li>\n<li>audioFormat，这个参数是用来配置“数据位宽”的，即采样格式，可选值以常量的形式定义在类AudioFormat中，分别为ENCODING_PCM_16BIT（16bit）、ENCODING_PCM_8BIT（8bit）。注意，前者是可以保证兼容所有Android手机的，所以我建议你尽量选择前者。</li>\n<li>bufferSizeInBytes，它配置的是 AudioTrack 内部的音频缓冲区的大小，AudioTrack 类提供了一个帮助开发者确定这个 bufferSizeInBytes 的函数，原型如下：</li>\n</ul><pre><code class=\"language-plain\">int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat);&nbsp;\n</code></pre><p>在实际开发中，我建议你使用该函数计算出需要传入的bufferSizeInBytes，而不是自己手动计算。</p><ul>\n<li>mode，AudioTrack提供了两种播放模式，可选值以常量的形式定义在类AudioTrack中，一个是 MODE_STATIC，需要一次性将所有的数据都写入播放缓冲区，简单高效，通常用于播放铃声、系统提醒的音频片段；另一个是 MODE_STREAM，需要按照一定的时间间隔不间断地写入音频数据，理论上它可用于任何音频播放的场景。在实际开发中，我建议你采用MODE_STREAM这种播放模式。</li>\n</ul><p>讲到这里，我相信你可以根据自己的场景构造出一个AudioTrack实例来了，我们根据上面的工作流程继续进行下一步，接下来就是将AudioTrack切换到播放状态。</p><h3>第二步：将AudioTrack切换到播放状态</h3><p>其实切换到播放状态是非常简单的，需要先判断AudioTrack实例是否初始化成功，如果当前状态是初始化成功的话，那么就调用它的play方法，切换到播放状态，代码如下：</p><pre><code class=\"language-plain\">if (null != audioTrack &amp;&amp; audioTrack.getState() != AudioTrack.STATE_UNINITIALIZED) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n{\n&nbsp;&nbsp;&nbsp; audioTrack.play();\n}\n</code></pre><p>但是在切换为播放状态之后，需要开发者自己启动一个线程，用于向AudioTrack里面送入PCM数据，接下来我们一起来看如何开启播放线程。</p><h3>第三步：开启播放线程</h3><p>首先创建出一个播放线程，代码如下：</p><pre><code class=\"language-plain\">playerThread = new Thread(new&nbsp;PlayerThread(), \"playerThread\");\nplayerThread.start();\n</code></pre><p>接着我们来看这个线程中执行的任务，代码如下：</p><pre><code class=\"language-plain\">class PlayerThread implements Runnable {\n&nbsp;&nbsp;&nbsp; private short[] samples;\n&nbsp;&nbsp;&nbsp; public void run() {\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; samples = new short[minBufferSize];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while(!isStop) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int actualSize = decoder.readSamples(samples);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; audioTrack.write(samples, actualSize);\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; }\n&nbsp;&nbsp;&nbsp; }\n}\n</code></pre><p>线程中的minBufferSize的计算方式如下：</p><ol>\n<li>在初始化AudioTrack的时候获得的缓冲区大小为bufferSizeInBytes；</li>\n<li>换算为2个字节表示一个采样的大小，也就是除以2得到这个minBufferSize。</li>\n</ol><p>然后代码中的decoder是一个解码器实例，构建这个解码器实例比较简单，在这里我就不详细介绍了。现在我们假设已经构建成功，然后从解码器中拿到PCM采样数据，最后调用write方法写入AudioTrack的缓冲区中。循环往复地不断送入PCM数据，音频就能够持续地播放了。</p><p>这里有一点是需要额外注意的，就是这个write方法是阻塞的，比如：一般写入200ms的音频数据需要执行接近200ms的时间，所以这要求在这个线程中不应该做更多额外耗时的操作，比如IO、等锁。</p><h3>第四步：销毁资源</h3><p>当要停止播放（自动完成或者用户手动停止）的时候，就需要停止播放同时销毁资源，那么就需要首先停掉AudioTrack，代码如下：</p><pre><code class=\"language-plain\">if (null != audioTrack &amp;&amp; audioTrack.getState() != AudioTrack.STATE_UNINITIALIZED) &nbsp;&nbsp;\n{\n&nbsp;&nbsp;&nbsp; audioTrack.stop();\n}\n</code></pre><p>然后要停掉我们自己的播放线程：</p><pre><code class=\"language-plain\">isStop = true;\nif (null != playerThread) {\n&nbsp;&nbsp;&nbsp; playerThread.join();\n&nbsp;&nbsp;&nbsp; playerThread = null;\n}\n</code></pre><p>只有当线程停掉之后，才不会再有AudioTrack的使用，最后一步就是释放AudioTrack：</p><pre><code class=\"language-plain\">audioTrack.release();\n</code></pre><p>看完了SDK层的音频渲染方法，接下来我们继续看NDK层音频渲染的两种方法，先来介绍OpenSL&nbsp;ES吧！</p><h2>OpenSL&nbsp;ES</h2><p>OpenSL ES全称是Open Sound Library for Embedded Systems，即嵌入式音频加速标准。OpenSL ES是无授权费、跨平台、针对嵌入式系统精心优化的硬件音频加速的框架。它为嵌入式移动多媒体设备上的本地应用程序开发者提供了标准化、高性能、低响应时间的音频功能实现方法，并实现了软/硬件音频性能的直接跨平台部署，降低了执行难度，促进了高级音频市场的发展。</p><p><img src=\"https://static001.geekbang.org/resource/image/1b/43/1ba3480ff88d7d509782e1bc501fed43.png?wh=1920x1043\" alt=\"图片\" title=\"图1 OpenSL ES架构\"></p><p>图1描述了OpenSL ES的架构，在Android中，High Level Audio Libs是音频Java层API 输入输出，属于高级API。相对来说，OpenSL ES则是比较低级别的API，属于C语言API。在开发中，一般使用高级API就能完成，除非遇到性能瓶颈，比如低延迟耳返、低延迟声音交互反馈、语音实时聊天等场景，开发者可以直接通过C/C++开发。</p><h3>编译与链接</h3><p>我们这个专栏里使用的是OpenSL ES 1.0.1版本，因为这个版本是目前比较成熟并且通用的，Android系统2.3版本以上才支持这个版本，并且有一些高级功能，比如解码AAC，是在Andorid系统版本4.0以上才支持的。</p><p>在使用OpenSL ES的API之前，我们需要引入OpenSL ES的头文件，如下：</p><pre><code class=\"language-plain\">#include &lt;SLES/OpenSLES.h&gt;\n#include &lt;SLES/OpenSLES_Android.h&gt;\n</code></pre><p>由于是在Native层使用这个特性，所以要在编译脚本中引入对应的so库：</p><ol>\n<li>Android.mk这个Makefile文件中增加链接选项，以便在链接阶段使用系统提供的OpenSL ES的so库：</li>\n</ol><pre><code class=\"language-plain\">LOCAL_LDLIBS += -lOpenSLES\n</code></pre><ol start=\"2\">\n<li>Cmake的情况下，需要在CMakeLists.txt中增加链接选项，以便在链接阶段使用系统提供的OpenSL ES的so库：</li>\n</ol><pre><code class=\"language-plain\">target_link_libraries(audioengine\n        OpenSLES\n        )\n</code></pre><h3>OpenSL ES的对象和接口</h3><p>我们前面也提到了OpenSL ES提供的是基于C语言的API，设计者为了让开发更简单，所以以面向对象的方式为开发者提供接口，那基于C语言的API是如何设计面向对象的接口的呢？</p><p>OpenSL ES提供的是基于对象和接口的方式，采用面向对象的方法提供API接口，所以，我们先来看一下OpenSL ES里面对象和接口的概念。</p><ul>\n<li>对象：对象是对一组资源及其状态的抽象，每个对象都有一个在其创建时指定的类型，类型决定了对象可以执行的任务集，它类似于C++中类的概念。</li>\n<li>接口：接口是对象提供的一组特征的抽象，这些抽象会暴露给开发者一组方法和每个接口的类型功能，在代码中，接口的类型由接口ID标识。</li>\n</ul><p>我们需要理解的重点是，<strong>一个对象在代码中其实是没有实际的表示形式的，可通过接口来改变对象的状态以及使用对象提供的功能</strong>。对象可以有一个或者多个接口的实例，但是对于接口实例，肯定只属于一个对象。理解了OpenSL ES中对象和接口的概念，我们继续来看代码实例中是如何使用它们的吧！</p><h3>OpenSL ES的使用方法</h3><p>刚刚我们提到，对象是没有实际的代码表示形式的，对象的创建也是通过接口来完成的。通过相应的方法来获取对象，进而可以访问对象的其他接口方法或者改变对象的状态，具体的执行步骤如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/2f/f407f66fb5d5cf6e94c2c1811a65a52f.png?wh=2011x236\" alt=\"\" title=\"图2 OpenSL ES的执行步骤\"></p><ol>\n<li>创建一个引擎对象接口。引擎对象是OpenSL ES提供API的唯一入口，开发者需要调用全局函数slCreateEngine来获取SLObjectItf类型的引擎对象接口。</li>\n</ol><pre><code class=\"language-plain\">SLObjectItf&nbsp;engineObject;\nSLEngineOption&nbsp;engineOptions[] = { { (SLuint32)&nbsp;SL_ENGINEOPTION_THREADSAFE, &nbsp;(SLuint32) SL_BOOLEAN_TRUE } };\nslCreateEngine(&amp;engineObject, ARRAY_LEN(engineOptions), engineOptions, 0,&nbsp;0,&nbsp;0);\n</code></pre><ol start=\"2\">\n<li>实例化引擎对象，需要通过在上一步得到的引擎对象接口来实例化引擎对象，否则无法使用这个对象，其实在OpenSL ES的使用中，任何对象都需要使用接口来进行实例化，所以我们也封装出一个实例化对象的方法，代码如下：</li>\n</ol><pre><code class=\"language-plain\">RealizeObject(engineObject);\nSLresult&nbsp;RealizeObject(SLObjectItf object) {\n&nbsp;&nbsp;&nbsp; return (*object)-&gt;Realize(object, SL_BOOLEAN_FALSE);\n};\n</code></pre><ol start=\"3\">\n<li>获取这个引擎对象的方法接口，通过GetInterface方法，使用上一步已经实例化好的对象，获取对应的SLEngineItf类型的对象接口，这个接口将会是开发者使用所有其他API的入口。</li>\n</ol><pre><code class=\"language-plain\">SLEngineItf&nbsp;engineEngine;\n(*engineObject)-&gt;GetInterface(engineObject, SL_IID_ENGINE, &amp;engineEngine);\n</code></pre><ol start=\"4\">\n<li>创建需要的对象接口，通过调用SLEngineItf类型的对象接口的CreateXXX方法，来返回新的对象的接口，比如调用CreateOutputMix这个方法来获取出一个outputMixObject接口，或者调用CreateAudioPlayer这个方法来获取出一个audioPlayerObject接口，这里我们仅列出创建outputMixObject的接口代码，播放器接口的获取可以参考代码仓库中的代码：</li>\n</ol><pre><code class=\"language-plain\">SLObjectItf&nbsp;outputMixObject;\n(*engineEngine)-&gt;CreateOutputMix(engineEngine, &amp;outputMixObject, 0, 0, 0);\n</code></pre><ol start=\"5\">\n<li>实例化新的对象，任何对象接口获取出来之后，都必须要实例化，其实是和第二步（实例化引擎对象）一样的操作。</li>\n</ol><pre><code class=\"language-plain\">realizeObject(outputMixObject);\nrealizeObject(audioPlayerObject);\n</code></pre><ol start=\"6\">\n<li>对于某一些比较复杂的对象，需要获取新的接口来访问对象的状态或者维护对象的状态，比如在播放器audioPlayer或录音器audioRecorder中注册一些回调方法等，代码如下：</li>\n</ol><pre><code class=\"language-plain\">SLPlayItf&nbsp;audioPlayerPlay;\n(*audioPlayerObject)-&gt;GetInterface(audioPlayerObject, SL_IID_PLAY,&nbsp;\n&amp;audioPlayerPlay);\n//设置播放状态\n(*audioPlayerPlay)-&gt;SetPlayState(audioPlayerPlay, SL_PLAYSTATE_PLAYING);\n//设置暂停状态\n(*audioPlayerPlay)-&gt;SetPlayState(audioPlayerPlay, SL_PLAYSTATE_PAUSED);\n</code></pre><ol start=\"7\">\n<li>待使用完毕这个对象之后，要记得调用Destroy方法来销毁对象以及相关的资源。</li>\n</ol><pre><code class=\"language-plain\">destroyObject(audioPlayerObject);\ndestroyObject(outputMixObject);\nvoid AudioOutput::destroyObject(SLObjectItf&amp; object) {\n&nbsp;&nbsp;&nbsp; if (0 != object)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (*object)-&gt;Destroy(object);\n&nbsp;&nbsp;&nbsp; object = 0;\n}\n</code></pre><p>相较于其他音频接口（AudioTrack、AAudio），OpenSL&nbsp;ES的使用确实比较麻烦，但是如果你的面向对象思维比较好的话，按照它的套路写起来也会比较快。在后面课程中我们播放器实战的音频渲染部分也会使用OpenSL ES来构造，到时候你可以参考代码实例进行更深入的理解。</p><h2>Oboe</h2><p>学到这里，你是否会有一个疑问呢？就是如果要在NDK层构建一套适配性好同时面向未来的音频渲染框架，势必要将上面介绍的两种方法结合起来，同时也要有一定的策略来选择使用哪一种实现，而从零搭建一套这样的框架会比较复杂，那有没有一些开源的实现来完成这件事情呢？</p><p>有，那就是Oboe。</p><h3>Oboe介绍</h3><p>由于AAudio仅适用于Android 8.0系统以上，而OpenSL ES在某些设备上又没有Google给开发者提供的低延迟、高性能的能力，所以Google推出了自己的Oboe框架。Oboe使用和AAudio近乎一致的API接口为开发者封装了底层的实现，自动地根据当前Android系统来选择OpenSL&nbsp;ES还是AAudio，当然也给开发者提供了接口，开发者可以自由地选择底层的实现。由于Oboe的整体API接口以及设计思想与AAudio一致，所以我们这节课直接以Oboe为例来给你详细地讲解一下。</p><h3>集成Oboe到工程里</h3><ol>\n<li>在gradle文件中增添对Oboe的依赖：</li>\n</ol><pre><code class=\"language-plain\">dependencies {\n&nbsp;&nbsp;&nbsp; implementation 'com.google.oboe:oboe:1.6.1'\n}\n</code></pre><ol start=\"2\">\n<li>在CMake中增加头文件引用与库的链接：</li>\n</ol><pre><code class=\"language-plain\"># Find the Oboe package\nfind_package (oboe REQUIRED CONFIG)\n# Specify the libraries which our native library is dependent on, including Oboe\ntarget_link_libraries(native-lib log oboe::oboe)\n</code></pre><ol start=\"3\">\n<li>业务代码中引入必要的头文件：</li>\n</ol><pre><code class=\"language-plain\">#include &lt;oboe/Oboe.h&gt;\n</code></pre><p>至此Oboe就已经集成到工程里了，那我们如何在工程中使用它呢？</p><h3>在工程里使用Oboe</h3><ol>\n<li>创建AudioStream</li>\n</ol><ul>\n<li>首先，我们通过AudioStreamBuilder来创建Stream，AudioStreamBuilder是按照Builder设计模式设计的类，可以接连地设置多个参数。在下面的代码块中，我对参数进行了解释：</li>\n</ul><pre><code class=\"language-plain\">oboe::AudioStreamBuilder builder;\nbuilder.setPerformanceMode(oboe::PerformanceMode::LowLatency)\n&nbsp; -&gt;setDirection(oboe::Direction::Output)//播放的设置\n&nbsp; -&gt;setSharingMode(oboe::SharingMode::Exclusive)//独占设备，对应的是Shared\n&nbsp; -&gt;setChannelCount(oboe::ChannelCount::Mono)//单声道\n&nbsp; -&gt;setFormat(oboe::AudioFormat::Float);//格式采用Float，范围为[-1.0，1.0]，还有一种是I16，范围为[-32768, 32767]\n</code></pre><ul>\n<li>然后设置Callback，定义一个AudioStreamDataCallback的子类，重写onAudioReady方法来实现自己填充数据的逻辑，但这个方法不可以太耗时，否则会出现卡顿，最后基于这个类构建对象设置给Builder。</li>\n</ul><pre><code class=\"language-plain\">class MyCallback : public oboe::AudioStreamDataCallback {\npublic:\n&nbsp;&nbsp;&nbsp; oboe::DataCallbackResult\n&nbsp;&nbsp;&nbsp; onAudioReady(oboe::AudioStream *audioStream, void *audioData, int32_t numFrames) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auto *outputData = static_cast&lt;float *&gt;(audioData);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const float amplitude = 0.2f;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (int i = 0; i &lt; numFrames; ++i){\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; outputData[i] = ((float)drand48() - 0.5f) * 2 * amplitude;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return oboe::DataCallbackResult::Continue;\n&nbsp;&nbsp;&nbsp; }\n};\n&nbsp;\nMyCallback&nbsp;myCallback;\nbuilder.setDataCallback(&amp;myCallback);\n</code></pre><ul>\n<li>最终调用openStream来打开Stream，根据返回值来判断是否创建成功，如果创建失败了，可以使用convertToText来查看错误码。</li>\n</ul><pre><code class=\"language-plain\">oboe::Result result = builder.openStream(mStream);\nif (result != oboe::Result::OK) {\n  LOGE(\"Failed to create stream. Error: %s\",&nbsp;convertToText(result));\n}\n</code></pre><ol start=\"2\">\n<li>播放音频</li>\n</ol><p>调用了requestStart方法之后，就要在回调函数中填充数据。</p><pre><code class=\"language-plain\">mStream-&gt;requestStart();\n</code></pre><ol start=\"3\">\n<li>关闭AudioStream</li>\n</ol><pre><code class=\"language-plain\">mStream-&gt;close();\n</code></pre><p>至此Oboe渲染音频的方法我们就学完了，其实Oboe（AAudio）的接口设计是非常优雅的，开发者在使用的时候也都很得心应手，希望通过这节课的学习你可以在你的应用中加入Oboe的能力，给你的应用赋予Google最新的低延迟、高性能的能力。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/73/a9c7b54a5bfc724c2fd8bdfyyb8f2473.png?wh=3437x2473\" alt=\"\"></p><p>这节课我们重点学习了使用Java层的AudioTrack和Native层的OpenSL ES以及Oboe来渲染音频的方法。AudioTrack是最通用的渲染PCM的方法，今天我们详细地介绍了它的工作流程；然后我们又聚焦了Native层的OpenSL ES，它在高级音频市场占有非常重要的地位，今天通过代码实例，我们展示了OpenSL ES对象和接口的使用方法，希望你能通过今天的实战，掌握它的使用方法；最后我们学习了Oboe渲染音频的方法，Oboe可以根据当前Android系统选择合适的框架，在整个音频渲染体系中也是十分重要的存在。</p><p>其实在Android开发中除了这些底层的音频框架，还有其他的一些常用的上层框架，比如MediaPlayer、SoudPool，我们也做了简单的介绍。在系统地学习这些渲染音频的方法之后，相信你能够根据具体的开发场景，调用合适的音频框架去处理问题了。</p><h2>思考题</h2><p>如果你想要实现一个音频播放器功能，场景描述如下：</p><ol>\n<li>自己构建解码器来解码本地的MP3或者M4A文件；</li>\n<li>使用AudioTrack或者Oboe来渲染音频；</li>\n<li>可以随时暂停、停止这个播放器的播放。</li>\n</ol><p>请你设计出这个案例的整体架构图，同时标记清楚你设计的各个类的职责。欢迎在评论区分享你的思考，也欢迎把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"02｜iOS平台音频渲染（二）：使用 AudioUnit 渲染音频","id":543993},"right":{"article_title":"04｜移动平台的视频渲染（一）：OpenGL ES基础","id":545953}}},{"article_id":545953,"article_title":"04｜移动平台的视频渲染（一）：OpenGL ES基础","article_content":"<p>你好，我是展晓凯。今天我们来学习移动平台的视频渲染。</p><p>虽然我们的主题是视频渲染，但严格来讲其实是视频画面（不包含声音）的渲染。在音视频领域的大部分经典场景是离不开画面的渲染的，比如：视频播放器、视频编辑器、短视频录制、贴纸以及礼物的渲染等等，所以掌握视频画面渲染技术的重要性是不言而喻的。那视频渲染都有哪些技术可用呢？最常用的是哪一种呢？带着这些问题，让我们一起开始本节课的学习吧！</p><p>注意，这里我们所说的渲染并不单单指的是直接绘制到系统屏幕上，一些高级处理操作，比如滤镜、磨皮、瘦脸、贴纸等，也是渲染的一部分。</p><h2>视频渲染技术选型</h2><p>在开始学习移动平台的视频渲染技术之前，我们先探讨一下，渲染的源头是什么？前两节课我们一起学习了音频的渲染源是PCM数据，而视频的渲染源是YUV或者RGBA格式的数据，这种数据是描述画面最基础的格式，其中YUV常用在视频的原始格式中，RGBA常用在一些图像的原始格式上。</p><p>目前各个平台最终渲染到屏幕上的都是RGBA格式的，因为硬件对屏幕上的设计就是按照每个像素点分为四个子像素来实现的，所以YUV和RGBA之间是可以互相转换的。我们最终的实战案例是视频播放器，所以这节课我们重点学习的是如何将YUV数据渲染出来，在移动平台上都有哪些落地方案。</p><!-- [[[read_end]]] --><h3>平台上层的API</h3><p>Android平台在SDK层提供了Canvas和Paint的API接口，便于开发者将位图（Bitmap）绘制到系统的屏幕上，常用于一些复杂的自定义View的实现，也可以再结合SurfaceView或者TexureView，将绘制线程从主线程中独立出来进行渲染，满足基本的绘图实现。</p><p>iOS平台的SDK层也提供了Quartz 2D把位图（Bitmap）绘制到屏幕上，在一些自定义View以及获取snapshot的场景下使用。这种方式的优点是各自平台的开发者使用起来非常简单，无需了解更深层次的绘图原理就能做一些简单的图形图像处理，也可以利用系统提供的一些API进行各种变换、阴影以及渐变的处理。缺点也比较明显，需要各个平台独立书写自己平台代的码进行渲染绘制，并且当需要一些高级处理和更高性能的时候，就会捉襟见肘，比如视频的美颜、跟脸贴纸以及一些视频编辑的处理等。</p><h3>跨平台的OpenGL ES</h3><p>在移动设备上，各个平台提供了更加底层的图形绘制接口，就是众所周知的OpenGL ES。它天生就是为了跨平台而设计的，在跨平台方面具有独特的优势，由于是最底层的图形绘制接口，所以在性能以及图形的高级处理方面也都没有任何问题。缺点其实也很明显，就是掌握这项技术的学习成本是比较高的，不单单要了解OpenGL ES的各种概念，并且还要为不同平台创建自己的上下文环境，同时还得掌握OpenGL自己的编程语言GLSL的语法和运行机制。</p><h3>平台提供的最新技术</h3><p>Android在10.0系统之后开始支持Vulkan，Vulkan是用于高性能 3D 渲染的渲染方法，是Khronos Group组织开发的新一代API，用于替换掉OpenGL，所以天生也是跨平台的，支持Windows、Linux、Android，还有iOS和Mac平台，只不过在iOS平台上底层是MoltenVK来实现的。</p><p>由于这项渲染技术是OpenGL的下一代，所以渲染性能方面是非常好的，并且在模块化角度也更好一些。但是有两个缺点，一是对现有的渲染系统做大规模的迁移，没有特别友好的方案；另外一个就是对于普通的开发人员来讲，上手难度比较高。但总的来说，未来它将成为用于专业图形图像渲染引擎的底层技术。</p><p>同样iOS平台提供了Metal来作为OpenGL的替代者，在iOS12系统之后弃用 OpenGL ES，系统的一些框架全面改成默认 Metal 支持。由于Metal的设计和OpenGL比较类似，开发者上手难度并不大，绘制性能也有比较明显的提升，主要缺点是跨平台性比较差。</p><p>刚刚我们也提到了，渲染并不单单是要把画面绘制到屏幕上，更多的是需要构建一个跨平台、可扩展、高性能的渲染引擎。这对于后续的一些处理是非常关键的，最终我们选择的技术依然是OpenGL ES，虽然它上手难度会大一些，但是通过接下来的学习，我会带你彻底掌握这项技术，让我们一起来进入OpenGL ES的新世界吧！</p><h2>OpenGL ES</h2><p>OpenGL的全称是Open Graphics Library，它用于二维或三维图像的处理与渲染，是一个功能强大、调用方便的底层图形库。它定义了一套跨编程语言、跨平台编程的专业图形程序接口。而OpenGL ES（OpenGL for Embedded Systems）是它在嵌入式设备上的版本，这个版本是针对智能手机等嵌入式设备设计的，可以理解为是Open GL的一个子集。</p><p>目前，OpenGL的最新版本已经达到了3.0以上，由于兼容性以及现存渲染系统的架构，目前OpenGL ES 2.0还是使用最广泛的版本。所以我们就在2.0版本的基础上进行编程，来实现图像的处理与渲染。由于在视频应用这一场景下，绝大部分都是使用二维图像的处理与渲染，所以这里我们只讨论OpenGL ES的二维部分的内容，不涉及三维部分的介绍。其实除了专业的引擎开发者，大部分应用开发者只需要有二维部分的知识就可以完成我们的绝大部分工作了。</p><h3>上下文环境</h3><p>由于OpenGL是基于跨平台设计的，所以每个平台需要自己提供渲染的基础实现，用来抹平各个系统本身的差异。就像我们常说Java语言是跨平台的，实际上是JVM抹平了各个平台的差异一样。<strong>每个平台提供渲染的基础实现，我们称之为OpenGL的上下文环境。</strong>另外在OpenGL的设计中，OpenGL是不负责管理窗口的，窗口的管理也交由每个平台自己来完成。与上下文环境一样，窗口管理在每个平台也都有自己的实现。</p><p>具体来讲，作为OpenGL ES的上下文环境，iOS平台为开发者提供了EAGL，而Android平台（Linux平台）为开发者提供了EGL。所以如果想要让OpenGL程序运行在多个平台上，也要学会利用各个平台提供的API接口，为OpenGL ES创建出上下文环境。</p><p>除了了解OpenGL ES的各种上下文环境，这里我们还需要知道一个开源的跨平台多媒体开发库——SDL，它给开发人员提供了面向SDL的API编程，能够通过交叉编译这个库解决多平台下需要手动构建OpenGL上下文环境以及窗口管理的问题。但在移动端，这样的实现会让我们失去一部分更加灵活的控制，导致一些场景下的功能不能实现。因此我们这里不使用SDL，而是通过裸用各个平台的API来构建OpenGL的上下文环境。</p><p>上面介绍了OpenGL（ES）是什么以及它的上下文环境，下面我们一起来看一下用OpenGL（ES）能做什么。</p><h3>OpenGL（ES）的用途</h3><p>刚刚我们也提到了OpenGL（ES）是做图形图像处理的库，尤其运行在移动设备上，它有更好的性能与绘制效果。</p><p>要想了解OpenGL ES能做什么，就不得不提到一个开源项目——GPUImage。它的实现非常优雅、完备，尤其是在iOS平台，提供了视频录制、视频编辑、离线保存这些场景。</p><p>其中很重要的一部分就是内部滤镜的实现，在里面我们可以找到大部分图形图像处理Shader的实现，包括：</p><ul>\n<li>基础的图像处理算法：饱和度、对比度、亮度、色调曲线、灰度、白平衡等；</li>\n<li>图像像素处理的实现：锐化、高斯模糊等；</li>\n<li>视觉效果的实现：素描、卡通、浮雕效果等；</li>\n<li>各种混合模式的实现。</li>\n</ul><p>除此之外，开发者也可以自己去实现美颜滤镜效果、瘦脸效果以及粒子效果等等。</p><p>GPUImage框架是使用OpenGL ES中的GLSL书写出了上述的各种Shader。GLSL（OpenGL Shading Language）是OpenGL的着色器语言，也是2.0版本中最出色的功能。开发人员可以使用这种语言编写程序运行在GPU上来进行图像处理或渲染。使用GLSL写的代码最终可以编译、链接成为一个GLProgram。</p><blockquote>\n<p>注：Graphic Processor Unit是图形图像处理单元，可以把它理解成一种高并发的运算器。</p>\n</blockquote><p>一个GLProgram分为两部分，一是顶点着色器（Vertex Shader），二是片元着色器（Fragment Shader），这两部分分别完成各自在OpenGL 渲染管线中的功能。那它们是怎么工作的呢？我们一起来看一下。</p><h3>OpenGL渲染管线</h3><p>如果想要了解着色器并理解它们的工作机制，就要对OpenGL的渲染管线有深入的了解。在学习之前，我们还是先来了解几个概念。我们平时说的点、直线、三角形都是几何图元，是在顶点着色器中指定的，用这些几何图元创建的物体叫做模型，而<strong>根据这些模型创建并显示图像的过程就是我们所说的渲染。</strong>在渲染过程结束之后，图像就会以像素点的形式绘制到屏幕上了。</p><p>一张RGBA格式的图像在内存中是这样描述的：每四个Byte表示一个像素点的RGBA的数据，这些像素点可以被组织成一个大的一维数组，用来在内存中表述这张图片。那在显存中如何描述呢？</p><p>在显存中，这些像素点被组织成帧缓冲区（framebuffer），帧缓冲区保存了显卡为了控制屏幕上所有像素的颜色以及强度所需要的全部信息。理解了帧缓冲区的概念，接下来我们就可以进入OpenGL渲染管线的学习了，这个部分是学会使用OpenGL非常重要的环节。</p><p>OpenGL的渲染管线具体是指什么呢？其实就是<strong>OpenGL引擎把图像（内存中的RGBA数据）一步步渲染到屏幕上去的步骤</strong>。渲染管线主要分为以下几个阶段：</p><p><img src=\"https://static001.geekbang.org/resource/image/10/34/10490f4dec8ce4cac5572b6ff2096434.png?wh=1920x404\" alt=\"图片\" title=\"图1 OpenGL的渲染管线\"></p><p><strong>阶段一：指定几何对象</strong></p><p>几何对象，就是刚才我们说的几何图元，这里OpenGL引擎会根据开发者的指令去绘制几何图元。OpenGL（ES）提供给开发者的绘制方法glDrawArrays的第一个参数mode，就是指定绘制方式。绘制方式的可选值有点、线、三角形三种，分别应用于不同的场景中。</p><p>粒子效果的场景中，我们一般用点（GL_POINTS）来绘制；直线的场景中，我们主要用线（GL_LINES）来绘制；所有二维图形图像的渲染，都用三角形（GL_TRIANGLE_STRIP）来绘制。</p><p>我们根据不同的场景选择的绘制方式，就决定了OpenGL（ES）渲染管线的第一阶段怎么去绘制几何图元。</p><p><strong>阶段二：顶点变换</strong></p><p>不论阶段一指定的是哪种几何对象，所有的几何数据都要来到顶点处理阶段，顶点处理阶段的操作就是变换顶点的位置：一是根据模型视图和投影矩阵的计算来改变物体（顶点）坐标的位置；二是根据纹理坐标与纹理矩阵来改变纹理坐标的位置。这里的输出是用gl_Position来表示具体的顶点位置的，如果是用点（GL_POINTS）来绘制几何图元的话，还应该输出gl_PointSize。</p><p>如果涉及三维的渲染，这里还会处理光照计算和法线变换，因为这节课不会涉及三维图像的渲染，所以这里不做过多介绍。</p><p><strong>阶段三：图元组装</strong></p><p>在阶段二的时候，我们已经确定了物体坐标和顶点坐标，然后到这个阶段进行图元组装。这里我们根据应用程序送往图元的规则（如GL_POINTS 、GL_TRIANGLES 等）以及具体的纹理坐标，把纹理组装成图元。</p><p><strong>阶段四：栅格化操作</strong></p><p>由上一阶段传递过来的图元数据，在这里会被分解成更小的单元并对应帧缓冲区的各个像素。这些单元叫做“片元”，一个片元可能包含窗口颜色、纹理坐标等属性。片元的属性则是图元上顶点数据经过插值而确定的，这其实就是栅格化操作，这个操作能够确定每一个片元是什么。</p><p><strong>阶段五：片元处理</strong></p><p>通过纹理坐标取得纹理中相对应的片元像素值，根据自己的需要改变这个片元，比如调节饱和度、锐化等，最终输出的是一个四维向量gl_FragColor，我们用它来表示修改之后的片元像素。</p><p><strong>阶段六：帧缓冲操作</strong></p><p>帧缓冲操作是渲染管线的最后一个阶段，执行帧缓冲的写入等操作，OpenGL引擎负责把最终的像素值写入到帧缓冲区中。</p><p>这就是OpenGL渲染管线的六个步骤，从指定几何图元到帧缓冲区写入像素，图像就被OpenGL 引擎一步步地渲染到屏幕（FBO）上去了。</p><p>还记得我们之前提到过，OpenGL ES 2.0版本与之前的版本相比，最出色的功能就是开发者能够编写着色器来完成渲染管线的某个阶段。这里的“某个阶段”，实际上就是顶点变换阶段和片元处理阶段。开发者可以使用顶点着色器自己完成顶点变换阶段；也能使用片元（像素）着色器来完成片元处理阶段。</p><p>具体该如何书写顶点着色器和片元着色器呢？在专栏的第6讲我会详细地讲解GLSL语法以及内嵌函数，在这里先假设我们已经书写好了顶点着色器和片元着色器，现在需要通过OpenGL ES提供给开发者的接口来创建显卡可执行程序了。</p><h3>创建显卡执行程序</h3><p>假如我们已经完成了一组Shader的实例。那如何让显卡来运行这一组Shader呢？或者说如何用我们的Shader来替换掉OpenGL渲染管线中的那两个阶段呢？下面我们就来学习一下如何把Shader扔给OpenGL的渲染管线，这一部分叫做接口程序部分，就是利用OpenGL提供的接口来驱动OpenGL进行渲染。我们先来看一下图2，它描述了如何创建一个显卡的可执行程序（Program）。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/6e/2cf655052d6a9e2bd5472e26a7b7be6e.png?wh=1920x1137\" alt=\"图片\" title=\"图2 创建显卡可执行程序流程图\"></p><p>根据图示，我们来看一看如何一步步地创建出这个Program。首先看图的右半部分，也就是创建Shader的过程。</p><h4>创建Shader</h4><p>第一步是调用glCreateShader方法创建一个对象，作为Shader的容器，这个函数返回一个容器的句柄，函数原型如下：</p><pre><code class=\"language-plain\">GLuint glCreateShader(GLenum shaderType);&nbsp;\n</code></pre><p>函数原型中的参数shaderType有两种类型：</p><ul>\n<li>一是GL_VERTEX_SHADER，创建顶点着色器时开发者应传入的类型；</li>\n<li>二是GL_FRAGMENT_SHADER，创建片元着色器时开发者应传入的类型。</li>\n</ul><p>第二步就是给创建的这个Shader添加源代码，对应图片最右边的两个Shader Content，它们就是根据GLSL语法和内嵌函数书写出来的两个着色器程序，都是字符串类型。这个函数的作用就是把开发者的着色器程序加载到着色器句柄所关联的内存中，函数原型如下：</p><pre><code class=\"language-plain\">void glShaderSource(GLuint shader, int numOfStrings, const char **strings, int &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *lenOfStrings)\n</code></pre><p>最后一步就是来编译这个Shader，编译Shader的函数原型如下：</p><pre><code class=\"language-plain\">void glCompileShader(GLuint shader);\n</code></pre><p>编译完成之后，怎么知道这个Shader被编译成功了呢？我们可以使用下面这个函数来验证。</p><pre><code class=\"language-plain\">void glGetShaderiv (GLuint shader, GLenum pname, GLint* params);\n</code></pre><p>其中，第一个参数GLuint shader就是我们要验证的Shader句柄；第二个参数GLenum pname是我们要验证的这个Shader的状态值，我们一般在这里验证是否编译成功，这个状态值选择GL_COMPILE_STATUS。第三个参数GLint* params就是返回值，当返回值是1的时候，说明这个Shader编译成功了；如果是0，就说明这个shader没有被编译成功。</p><p>这个时候，我们就需要知道着色器代码中的哪一行出了问题，所以还需要开发者再次调用这个函数，只不过是获取这个Shader的另外一种状态，这个时候状态值应该选择GL_INFO_LOG_LENGTH，那么返回值返回的就是错误原因字符串的长度，我们可以利用这个长度分配出一个buffer，然后调用获取出Shader的InfoLog函数，原型如下：</p><pre><code class=\"language-plain\">void glGetShaderInfoLog(GLuint object, int maxLen, int *len, char *log);\n</code></pre><p>之后我们可以把InfoLog打印出来，帮助我们来调试实际的Shader中的错误。按照上面的步骤我们可以创建出Vertex Shader和Fragment Shader，那么接下来我们看图片中的左半部分，也就是如何通过这两个Shader来创建Program。</p><h4>创建Program</h4><p>首先我们创建一个程序的实例作为程序的容器，这个函数返回程序的句柄。函数原型如下：</p><pre><code class=\"language-plain\">GLuint glCreateProgram(void);\n</code></pre><p>紧接着将把上面部分编译的shader附加（Attach）到刚刚创建的程序中，调用的函数名称如下：</p><pre><code class=\"language-plain\">void glAttachShader(GLuint program, GLuint shader);\n</code></pre><p>其中，第一个参数GLuint program就是传入在上面一步返回的程序容器的句柄，第二个参数GLuint shader就是编译的Shader的句柄，当然要为每一个shader都调用一次这个方法才能把两个Shader都关联到Program中去。</p><p>当顶点着色器和片元着色器都被附加到程序中之后，最后一步就是链接程序，链接函数原型如下：</p><pre><code class=\"language-plain\">void glLinkProgram(GLuint program);\n</code></pre><p>传入参数就是程序容器的句柄，那么具体这个程序到底有没有链接成功呢？OpenGL也提供了一个函数用来检查这个程序的状态，函数原型如下：</p><pre><code class=\"language-plain\">glGetProgramiv (GLuint program, GLenum pname, GLint* params);\n</code></pre><p>第一个参数就是传入程序容器的句柄，第二个参数代表我们要检查这个程序的哪一个状态，这里面我们传入GL_LINK_STATUS，最后一个参数就是返回值。返回值是1则代表链接成功，如果返回值是0则代表链接失败。类似于编译Shader的操作，如果链接失败了，可以获取错误信息，以便修改程序。</p><p>获取错误信息时，也得先调用这个函数，但是第二个参数我们传递的是GL_INFO_LOG_LENGTH，代表获取出这个程序的InfoLog的长度，拿到长度之后我们分配出一个char*的内存空间以获取出InfoLog，函数原型如下：</p><pre><code class=\"language-plain\">void glGetProgramInfoLog(GLuint object, int maxLen, int *len, char *log);\n</code></pre><p>调用这个函数返回的InfoLog，可以作为Log打印出来，以便定位问题。</p><p>到这里我们就创建出了一个Program。回顾一下整个过程和C语言的编译和链接阶段非常相似，对比构造OpenGL Program，编译阶段做了编译顶点着色器和片元着色器，并且把两个着色器Attach到Program中，链接阶段就是链接这个Program。</p><p>创建显卡执行程序之后就是如何使用这个程序，其实使用这个构建出来的程序也很简单，调用glUseProgram这个方法即可，然后将一些内存中的数据与指令传递给这个程序，让这个程序运行起来。但是要想完全运行到手机上，还需要为OpenGL ES的运行提供一个上下文环境，下节课我们就来学习在两个平台上如何为OpenGL ES 创建上下文环境，但是你一定要记住我们刚刚创建好的这个Program，因为在接下来的课程中还会使用到。</p><h2>总结</h2><p>这节课我们从视频渲染的技术选型开始，给你介绍了视频渲染的实现手段和各自的优缺点，然后对OpenGL ES概念、用途、操作步骤等有了一个整体的了解。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/10/600e8cb4a0397c4d9c83a88bb061d410.png?wh=1920x1528\" alt=\"图片\"></p><p>其中我们重点学习了OpenGL ES的渲染管线以及渲染管线中留给开发者书写的顶点着色器和片元着色器两个阶段，最后学习使用OpenGL ES提供的接口API创建了一个显卡可执行程序，但是想让这个程序运行到显卡上，还需要一个OpenGL ES的上下文环境，关于构建OpenGL ES上下文环境的内容，我们会在下一节课中进行实际操作。期待一下吧！</p><h2>思考题</h2><p>学完了OpenGL ES基础内容，想必你已经对OpenGL ES有了比较深的了解。那我来考考你。OpenGL ES渲染管线分为哪几个步骤呢？哪些步骤是我们开发者可以自己去完成的？期待在评论区看到你的回答，也欢迎你把这节课分享给需要的朋友。我们下节课再见！</p>","neighbors":{"left":{"article_title":"03｜Android平台音频渲染与技术选型","id":545000},"right":{"article_title":"05｜移动平台的视频渲染（二）：OpenGL ES上下文环境搭建","id":546501}}},{"article_id":546501,"article_title":"05｜移动平台的视频渲染（二）：OpenGL ES上下文环境搭建","article_content":"<p>你好，我是展晓凯。今天我们来继续学习移动平台的视频渲染。</p><p><a href=\"https://time.geekbang.org/column/article/545953\">上一节课</a>，我们使用OpenGL ES提供给开发者的接口，创建出了一个GLProgram，但是如果让这个GLProgram运行起来还需要有一个上下文环境来支撑。由于OpenGL ES一开始就是为跨平台设计的，所以它本身并不承担窗口管理以及上下文环境构建的职责，这个职责需要由各自的平台来承担。</p><p>Android平台使用的是EGL，EGL是Khronos创建的一个框架，用来给OpenGL的输出与设备的屏幕搭建起一个桥梁。EGL的工作机制是双缓冲模式，也就是有一个Back Frame Buffer和一个Front Frame Buffer，正常绘制操作的目标都是Back Frame Buffer，渲染完毕之后，调用eglSwapBuffer这个API，会将绘制完毕的Back Frame Buffer与当前的Front Frame Buffer进行交换，然后显示出来。EGL承担了为OpenGL提供上下文环境以及窗口管理的职责。</p><p>iOS平台使用的是EAGL，与EGL的双缓冲机制比较类似，iOS平台也不允许我们直接渲染到屏幕上，而是渲染到一个叫renderBuffer的对象上。这个对象你可以理解为一个特殊的FrameBuffer，最终再调用EAGLContext的presentRenderBuffer方法，就可以将渲染结果输出到屏幕上去了。</p><!-- [[[read_end]]] --><p>下面我会分别对这两个平台的环境搭建进行详细地介绍。</p><h2>Android平台的环境搭建</h2><p>要在Android平台上使用OpenGL ES，最简单的方式是使用GLSurfaceView，因为不需要开发者搭建OpenGL ES的上下文环境以及创建OpenGL ES的显示设备，只需要遵循GLSurfaceView定义的接口，实现对应的逻辑即可。就像硬币有正反面一样，使用GLSurfaceView的缺点也比较明显，就是不够灵活，OpenGL ES很多核心用法，比如共享上下文，使用起来就会比较麻烦。</p><p>我们这个专栏的OpenGL ES上下文环境，会直接使用EGL提供的API，在Native层基于C++环境进行搭建。原因是在Java层进行搭建的话，对于普通的应用也许可以，但是对于要进行解码或使用第三方库的场景，比如人脸识别，又需要到Native层来实施。考虑到效率和性能，我们这里就直接使用Native层的EGL来搭建一个OpenGL ES的开发环境。</p><h3>引入头文件与so库</h3><p>那么如何在Native层使用EGL呢？我们必须要在CMake构建脚本（CMakeLists.txt）中加入EGL这个库，并在使用这个库的C++文件中引入EGL对应的头文件。</p><p>需要引用的头文件如下：</p><pre><code class=\"language-plain\">#include &lt;EGL/egl.h&gt;\n#include &lt;EGL/eglext.h&gt;\n</code></pre><p>需要引入的so库：</p><pre><code class=\"language-plain\">target_link_libraries(videoengine\n        # 引入系统的动态库\n        EGL\n        )\n</code></pre><p>这样我们就可以在Android的Native层中使用EGL了，不过要使用OpenGL ES给开发者提供的接口，还需要引入OpenGL ES对应的头文件与库。</p><p>需要包含的头文件：</p><pre><code class=\"language-plain\">#include &lt;GLES2/gl2.h&gt;\n#include &lt;GLES2/gl2ext.h&gt;\n</code></pre><p>需要引入的so库，注意这里使用的是OpenGL ES 2.0版本。</p><pre><code class=\"language-plain\">target_link_libraries(videoengine\n        # 引入系统的动态库\n        GLESv2\n        )\n</code></pre><p>至此，OpenGL ES的开发需要用到的头文件以及库文件就引入完毕了，下面我们来看一下如何使用EGL搭建出OpenGL的上下文环境以及实现窗口的管理。</p><h3>EGLDisplay作为绘制的目标</h3><p>EGL首先要解决的问题是，要告诉OpenGL ES绘制的目标在哪里，而EGLDisplay就是一个封装系统物理屏幕的数据类型，也就是绘制目标的一个抽象。开发者要调用eglGetDisplay这个方法来创建出EGLDisplay的对象，在调用这个方法的传参中，常量EGL_DEFAULT_DISPLAY会被传进这个方法中，每个厂商在自己的实现中都会返回默认的显示设备，代码如下：</p><pre><code class=\"language-plain\">EGLDisplay display;\nif ((display = eglGetDisplay(EGL_DEFAULT_DISPLAY)) == EGL_NO_DISPLAY) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglGetDisplay() returned error %d\", eglGetError());\n&nbsp;&nbsp;&nbsp; return false;\n}\n</code></pre><p>在获得了EGLDisplay的对象之后，就需要对这个对象做初始化工作，开发者需要调用EGL提供的eglInitialize方法来初始化这个对象，这个方法会返回一个bool值来代表函数运行结果。函数的第一个参数就是EGLDisplay对象，后面两个参数是这个函数为了返回EGL版本号而设计的，两个参数分别是Major和Minor的Version，比如EGL的版本号是1.0，那么Major返回1，Minor则返回0，如果我们不关心版本号，可以都传入0或者NULL，代码如下：</p><pre><code class=\"language-plain\">if (!eglInitialize(display, 0, 0)) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglInitialize() returned error %d\", eglGetError());\n&nbsp;&nbsp;&nbsp; return false;\n}\n</code></pre><p>一旦EGLDisplay初始化成功之后，它就可以将OpenGL ES的输出和设备的屏幕桥接起来，但是需要我们指定一些配置项，比如色彩格式、像素格式、OpenGL版本以及SurfaceType等，不同的系统以及平台使用的EGL标准是不同的，在Android平台下一般配置的代码如下所示：</p><pre><code class=\"language-plain\">EGLConfig config;\nconst EGLint attribs[] = {EGL_BUFFER_SIZE, 32,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_ALPHA_SIZE, 8,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_BLUE_SIZE, 8,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_GREEN_SIZE, 8,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_RED_SIZE, 8,&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EGL_RENDERABLE_TYPE, EGL_OPENGL_ES2_BIT,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_SURFACE_TYPE, EGL_WINDOW_BIT,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_NONE };\nif (!eglChooseConfig(display, attribs, &amp;config, 1, &amp;numConfigs)) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglChooseConfig() returned error %d\", eglGetError());\n&nbsp;&nbsp;&nbsp;&nbsp;return false;\n}\n</code></pre><p>配置选项这个函数也是返回一个bool值来代表配置状态，如果配置成功，则代表config会被正确初始化。</p><p><strong>EGLDisplay这个对象是EGL给开发者提供的最重要的入口</strong>，接下来我们要基于这个EGLDisplay对象还有EGLConfig配置来创建上下文了。</p><h3>EGLContext提供线程的上下文</h3><p>由于任何一条OpenGL ES指令（OpenGL ES提供给开发者的接口）都必须运行在自己的OpenGL上下文环境中，EGL提供EGLContext来封装上下文，可以按照如下代码构建出OpenGL ES的上下文环境。</p><pre><code class=\"language-plain\">EGLContext context;\nEGLint attributes[] = { EGL_CONTEXT_CLIENT_VERSION, 2, EGL_NONE };\nif (!(context = eglCreateContext(display, config, NULL,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eglContextAttributes))) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglCreateContext() returned error %d\", eglGetError());\n&nbsp;&nbsp;&nbsp; return false;\n}\n</code></pre><p>函数eglCreateContext的前两个参数就是我们刚刚创建的两个对象，第三个参数类型也是EGLContext类型的，一般会有两种用法。</p><ul>\n<li>如果想和已经存在的某个上下文共享OpenGL资源（包括纹理ID、frameBuffer以及其他的Buffer），则传入对应的那个上下文变量。</li>\n<li>如果目标仅仅是创建一个独立的上下文，不需要和其他OpenGL ES的上下文共享任何资源，则设置为NULL。</li>\n</ul><p>在一些场景下其实需要多个线程共同执行OpenGL ES的渲染操作，这种情况下就需要用到共享上下文，共享上下文的关键点就在这里，一定要记住。</p><p>成功创建出OpenGL ES的上下文，说明我们已经把OpenGL ES的绘制目标搞定了，但是这个绘制目标并没有渲染到我们某个View上，那如何将这个输出渲染到业务指定的View上呢？答案就在EGLSurface。</p><h3>EGLSurface将EGLDisplay与系统屏幕桥接起来</h3><p>EGLSurface实际上是一个FrameBuffer，开发者可以调用EGL提供的eglCreateWindowSurface创建一个可实际显示的Surface，调用eglCreatePbufferSuface可以创建一个OffScreen（离屏渲染，一般用户后台保存场景）的Surface，创建可实际显示的Surface代码如下：</p><pre><code class=\"language-plain\">EGLSurface surface = NULL;\nEGLint format;\nif (!eglGetConfigAttrib(display, config, EGL_NATIVE_VISUAL_ID,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;format)) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglGetConfigAttrib() returned error %d\", eglGetError());\n&nbsp;&nbsp;&nbsp; return surface;\n}\nANativeWindow_setBuffersGeometry(_window, 0, 0, format);\nif (!(surface = eglCreateWindowSurface(display, config, _window, 0))) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglCreateWindowSurface() returned error %d\", eglGetError());\n}\n</code></pre><p>上述代码中不得不提的就是_window这个参数，这里我需要重点向你解释一下，这个_window是ANativeWindow类型的对象，代表了本地业务层想要绘制到的目标View。在Android里面可以通过Surface（通过SurfaceView或TextureView来得到或者构建出Surface对象）去构建出ANativeWindow。但构建之前需要我们在使用的时候引用头文件。</p><pre><code class=\"language-plain\">#include &lt;android/native_window.h&gt;\n#include &lt;android/native_window_jni.h&gt;\n</code></pre><p>调用ANAtiveWindow的API接口如下：</p><pre><code class=\"language-plain\">ANativeWindow* window = ANativeWindow_fromSurface(env, surface);\n</code></pre><p>里面的env是JNI层的JNIEnv指针类型的变量，surface就是jobject类型的变量，是由Java层的Surface类型对象传递而来的。到这里我们就把EGLSurface和Java层的View（即设备的屏幕）连接起来了。</p><p>这里我们再补充一点，如果想做离屏渲染，也就是在后台使用OpenGL处理一些图像，就需要用到处理图像的Surface了，创建离屏Surface如下：</p><pre><code class=\"language-plain\">EGLSurface surface;\nEGLint PbufferAttributes[] = { EGL_WIDTH, width, EGL_HEIGHT, height, EGL_NONE,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EGL_NONE };\nif (!(surface = eglCreatePbufferSurface(display, config, PbufferAttributes))) {\n&nbsp;&nbsp;&nbsp; LOGE(\"eglCreatePbufferSurface() returned error %d\", eglGetError());\n}\n</code></pre><p>可以看到这个Surface并不会和任何一个业务View进行关联，进行离屏渲染的时候，就可以把这个Surface当作目标进行绘制。</p><p><img src=\"https://static001.geekbang.org/resource/image/7b/f3/7bbf36207fbb200f7de3636f88ea1bf3.png?wh=1920x860\" alt=\"图片\"></p><p>现在我们已经用EGL准备好了上下文环境与窗口管理，如上图所示，左侧我们为OpenGL ES准备好上下文环境，并且用EGLDisplay用来接收绘制的内容，右边通过EGLSurface连接好了设备的屏幕（Java层提供的SurfaceView或者TextureView）。那么接下来我们就具体看一下如何使用创建好的EGL环境进行工作。</p><h3>为绘制线程绑定上下文</h3><p>OpenGL ES需要开发者自己开辟一个新的线程，来执行OpenGL ES的渲染操作，还要求开发者在执行渲染操作前要为这个线程绑定上下文环境。EGL为绑定上下文环境提供了eglMakeCurrent这个接口。</p><pre><code class=\"language-plain\">eglMakeCurrent(display, eglSurface, eglSurface, context);\n</code></pre><p>在绑定了上下文环境以及窗口之后就可以执行RenderLoop循环了，每一次循环都是去调用OpenGL ES指令绘制图像。</p><p>还记得我们之前提到的EGL的双缓冲模式吗？EGL内部有两个帧缓冲区（frameBuffer），我们执行的渲染目标都是后台的frameBuffer。当渲染操作完成之后，要调用函数eglSwapBuffers进行前台frameBuffer和后台frameBuffer的交换。</p><pre><code class=\"language-plain\">eglSwapBuffers(display, eglSurface)\n</code></pre><p>执行上述函数之后，用户就可以在屏幕上看到刚刚渲染的图像了。</p><h3>销毁资源</h3><p>最后执行完所有的绘制操作之后，需要销毁资源。注意销毁资源也必须在这个独立的线程中，销毁显示设备（EGLSurface）的代码如下：</p><pre><code class=\"language-plain\">eglDestroySurface(display, eglSurface);\n</code></pre><p>销毁上下文（Context）代码如下：</p><pre><code class=\"language-plain\">eglDestroyContext(display, context);\n</code></pre><p>到这儿，在Android平台的Native层中，我们就使用EGL成功地把OpenGL ES的上下文环境搭建出来了，后面我们会用到这些知识。</p><h2>iOS平台的环境搭建</h2><p>iOS平台不允许开发者使用OpenGL ES直接渲染输出到屏幕上，而是使用Frame Buffer与Render Buffer相结合的方式来进行渲染。EAGL提供的方式是必须创建一个render Buffer，然后让OpenGL ES渲染到这个Render Buffer上面去。那这个Renderbuffer又是如何关联到业务层View上去的呢？</p><p>答案是RenderBuffer需要绑定一个CAEAGLLayer，而这个Layer实际上就可以一对一地关联到我们自定义的一个UIView。也就是开发者最后调用EAGLContext的presentRenderBuffer方法，这样就可以将渲染结果输出到屏幕上去了。实际上，在这个方法的内部实现中，EAGL也会执行类似于前面EGL中的swapBuffer的过程。具体使用步骤如下：</p><h3>自定义一个EAGLView</h3><p>我们先自定义一个继承自UIView的View，然后重写父类UIView的layerClass方法，并且一定要返回CAEAGLLayer 这个类型。</p><pre><code class=\"language-plain\">+ (Class) layerClass\n{\n&nbsp;&nbsp;&nbsp; return [CAEAGLLayer class];\n}\n</code></pre><p>接下来在这个View的初始化方法中，我们拿到layer并强制把类型转换为CAEAGLLayer类型的变量，然后给这个layer设置对应的参数。</p><pre><code class=\"language-plain\">- (id) initWithFrame:(CGRect)frame{\n&nbsp;&nbsp;&nbsp; if ((self = [super initWithFrame:frame])){\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CAEAGLLayer *eaglLayer = (CAEAGLLayer *)[self layer];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NSDictionary *dict = [NSDictionary dictionaryWithObjectsAndKeys:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [NSNumber numberWithBool:NO],\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kEAGLDrawablePropertyRetainedBacking,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kEAGLColorFormatRGB565,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kEAGLDrawablePropertyColorFormat,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nil];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [eaglLayer setOpaque:YES];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [eaglLayer setDrawableProperties:dict];\n&nbsp;&nbsp;&nbsp; }\n&nbsp;&nbsp;&nbsp; return self;\n}\n</code></pre><h3>构建EAGLContext</h3><p>像之前提到的，我们必须为每一个线程绑定OpenGL ES上下文，所以要开辟一个线程，开发者在iOS中开辟一个新线程的方法有很多，可以使用GCD，也可以使用NSOperationQueue，甚至裸用pthread也可以，反正必须在一个线程中执行创建上下文的操作。创建OpenGL ES的上下文代码如下：</p><pre><code class=\"language-plain\">EAGLContext* _context;\n_context = [[EAGLContext alloc]initWithAPI:kEAGLRenderingAPIOpenGLES2];\n</code></pre><p>创建成功以后就来绑定上下文。</p><pre><code class=\"language-plain\">[EAGLContext setCurrentContext:_context];\n</code></pre><p>执行成功之后就代表我们为这个线程绑定了刚刚创建好的上下文环境，也就是说我们已经建立好了EAGL与OpenGL ES的连接，接下来我们来建立另一端的连接。</p><h3>窗口管理</h3><p>这时，我们需要创建RenderBuffer并且把它绑定到前面自定义EAGLView的Layer上。首先是创建帧缓冲区。</p><pre><code class=\"language-plain\">glGenFramebuffers(1, &amp;_framebuffer);\n</code></pre><p>然后创建绘制缓冲区。</p><pre><code class=\"language-plain\">glGenRenderbuffers(1, &amp;renderbuffer);\n</code></pre><p>绑定帧缓冲区到渲染管线。</p><pre><code class=\"language-plain\">glBindFramebuffer(GL_FRAMEBUFFER, _framebuffer);\n</code></pre><p>绑定绘制缓存区到渲染管线。</p><pre><code class=\"language-plain\">glBindRenderbuffer(GL_RENDERBUFFER, _renderbuffer);\n</code></pre><p>为绘制缓冲区分配存储区，这里我们把CAEAGLLayer的绘制存储区作为绘制缓冲区的存储区。</p><pre><code class=\"language-plain\">[_context renderbufferStorage:GL_RENDERBUFFER fromDrawable:(CAEAGLLayer*)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layer]\n</code></pre><p>获取绘制缓冲区的像素宽度。</p><pre><code class=\"language-plain\">glGetRenderBufferParameteriv(GL_RENDER_BUFFER, GL_RENDER_BUFFER_WIDTH,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;_backingWidth);\n</code></pre><p>获取绘制缓冲区的像素高度。</p><pre><code class=\"language-plain\">glGetRenderBufferParameteriv(GL_RENDER_BUFFER, GL_RENDER_BUFFER_HEIGHT,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;_backingHeight);\n</code></pre><p>绑定绘制缓冲区到帧缓冲区。</p><pre><code class=\"language-plain\">glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_RENDERBUFFER,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _renderbuffer);\n</code></pre><p>检查Framebuffer的status。</p><pre><code class=\"language-plain\">GLenum status = glCheckFramebufferStatus(GL_FRAMEBUFFER);\nif(status != GL_FRAMEBUFFER_COMPLETE){\n&nbsp;&nbsp;&nbsp; //failed to make complete frame buffer object&nbsp;\n}\n</code></pre><p>完成以上操作，我们就把EAGL和我们的Layer（设备的屏幕）连接起来了，当我们绘制了一帧之后（当然绘制过程也必须在这个线程中），调用以下代码就可以将绘制的结果显示到屏幕上了。</p><pre><code class=\"language-plain\">[_context presentRenderbuffer:GL_RENDERBUFFER];\n</code></pre><p>这样我们就搭建好了iOS平台的OpenGL ES的上下文环境。这给我们之后要学的内容打好了基础。</p><p>这里Open GL上下文环境搭建的重点已经讲完了，但是学到这里你可能会有一些疑问，我们需要为每一个平台搭建自己的OpenGL ES上下文环境，感觉好复杂呀，并且整个搭建过程也都涉及平台相关的API接口，那有没有一个开源框架可以将上下文环境透明掉呢？这样开发者不就可以专注在自己的业务和重点算法攻坚上了吗？</p><p>答案是有的，就是我们接下来要学习的一个库——SDL。</p><h2>SDL的介绍与使用</h2><p>SDL可以给开发者提供面向libSDL的API编程，它的内部能解决多个平台的OpenGL上下文环境和窗口管理的问题。开发者只需要交叉编译这个库到各自的平台上，就可以达到一份代码运行到多个平台的目的了。FFmpeg中的ffplay工具就是基于libSDL开发的，SDL不单单可以渲染视频画面，也可以渲染音频。</p><p>但是对于移动开发者来讲，它也有一些缺点，比如使用SDL会牺牲一些更加灵活的控制，甚至某些场景下的功能实现不了。所以到底用不用SDL，你可以根据实际需求去考量，但了解这个库是十分有必要的。</p><p>我们这个部分的讲解是基于Mac + CLion的C++工程，最后学习怎么去使用sdl2这个库来构建OpenGL ES的上下文环境。</p><p>首先是安装sdl2库，我们可以通过命令行来安装。</p><pre><code class=\"language-plain\">brew install sdl2\n</code></pre><p>安装后，可以使用命令来查看安装列表和安装路径。</p><pre><code class=\"language-plain\">brew list 查看安装列表\nbrew list sdl2 查看sdl2安装路径\n</code></pre><p>如果安装路径是：/usr/local/Cellar/sdl2/2.0.14_1，需要在工程的CMakeLists.txt文件里，加入配置。</p><pre><code class=\"language-plain\">include_directories(/usr/local/Cellar/sdl2/2.0.14_1/include/)\nlink_directories(/usr/local/Cellar/sdl2/2.0.14_1/lib )\n</code></pre><p>在工程里加入SDL库和OpenGL库链接配置。</p><pre><code class=\"language-plain\">target_link_libraries(\n  SDL_OpenGL\n  SDL2\n  \"-framework OpenGL\"\n)\n</code></pre><p>加入头文件。</p><pre><code class=\"language-plain\">//SDL API\n#include \"SDL2/SDL.h\"\n//OpenGL API\n#include \"OpenGL/gl3.h\"\n</code></pre><p>接下来就是整个使用SDL渲染的整体流程，在讲解过程中我们参考这个流程结构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/6e/4b/6ed519cb2dc8cced3ee7396513fc4e4b.png?wh=1920x1074\" alt=\"图片\"></p><p>结合上图，我们来详细地讲解一下其中的关键流程。首先，我们需要创建窗口和OpenGL上下文。</p><pre><code class=\"language-plain\">SDL_Window *window = SDL_CreateWindow(\"hello OpenGL\", 0, 0, 300, 300, SDL_WINDOW_OPENGL);\nSDL_GLContext context = SDL_GL_CreateContext(window);\n</code></pre><p>进行循环渲染和更新窗口操作，然后退出。</p><pre><code class=\"language-plain\">while(true) {\n  drawFromOpenGL();&nbsp;//执行要做的渲染\n  SDL_GL_SwapWindow(window); //相当于执行了eglSwapBuffers操作\n  if (SDL_PollEvent(&amp;event) &amp;&amp; event.type == SDL_QUIT) {\n    //在点击窗口close时退出循环\n    break;\n  }\n  SDL_Delay(30); //单位ms,设置刷新频率\n}\n</code></pre><p>最后，释放资源。</p><pre><code class=\"language-plain\">SDL_GL_DeleteContext(context);\nSDL_DestroyWindow(window);\nSDL_Quit();\n</code></pre><p>按照上述的整个流程，我们就使用SDL构建起OpenGL ES的上下文环境了，其实这也回答了刚才我们提出的问题。如果你选择使用SDL来构建，那么你写的代码就是跨平台的，实际上是SDL内部将平台的差异性屏蔽掉了，让我们可以更方便地去书写自己的业务，这也是面向对象思想的一个重要特征，你也可以思考一下，把它用到平时的系统与架构设计中。</p><h2>总结</h2><p>这节课我们分别为Android和iOS平台构建出了OpenGL ES的上下文环境，也给你讲解了使用SDL的方法以及优势。</p><p><img src=\"https://static001.geekbang.org/resource/image/10/6c/1072af5f016e12b10874703310ac1a6c.png?wh=1920x895\" alt=\"图片\"></p><p>还记得在上一节课我们创建出来的显卡执行程序吗？接下来就是见证奇迹的时刻：让我们创建的显卡执行程序运行在构建出来的上下文环境上，最终我们可以通过OpenGL ES将YUV（或者RGBA）数据绘制到系统屏幕上。那么如何将显卡执行程序运行到平台的上下文环境中呢？我们下节课接着聊。</p><h2>思考题</h2><p>在今天学习中，我们知道了OpenGL ES的渲染需要在一个绑定了上下文的独立线程中进行，但是在实际的一些场景中，OpenGL ES的渲染需要分布在多个线程中，并且多个线程还需要共享这个上下文环境，那这个时候我们应该如何处理呢？欢迎在评论区留下你的思考，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"04｜移动平台的视频渲染（一）：OpenGL ES基础","id":545953},"right":{"article_title":"06｜移动平台的视频渲染（三）：OpenGL ES实操","id":547581}}},{"article_id":547581,"article_title":"06｜移动平台的视频渲染（三）：OpenGL ES实操","article_content":"<p>你好，我是展晓凯。今天我们来继续学习移动平台的视频渲染。</p><p>在<a href=\"https://time.geekbang.org/column/article/545953\">第4讲</a>中，我们学习了OpenGL ES的基础概念，并且用OpenGL ES提供给我们的接口构建出了一个GLProgram；<a href=\"https://time.geekbang.org/column/article/546501\">第5讲</a>，我们学习了如何在移动平台搭建OpenGL ES的上下文环境。有了之前的准备，我们终于迎来了收获成果的时刻——让GLProgram在上下文环境中跑起来。</p><h2>GLSL语法与内建函数</h2><p>在让GLProgram跑起来之前，我们还需要学习一下GLSL的语法和内建函数，这样才能够学会用GLSL书写着色器。</p><blockquote>\n<p>GLSL是OpenGL的着色器语言，开发者自己能够编写着色器来完成渲染管线的顶点变换阶段和片元处理阶段。——第4讲内容回顾</p>\n</blockquote><p>我们这个部分的目标就是实现一组着色器来完成增强对比度的功能，但是这组着色器还不能直接看到效果，因为着色器是需要运行到显卡中的，要想看到效果还得等这节课学完之后，所以不要着急，我们慢慢来。</p><p>前面我们已经粗略地介绍过GLSL是什么了，但是一直没有准确地给它下过定义，其实就是担心你看到它的定义之后，觉得难以理解，而学到这里我们已经了解了<a href=\"https://time.geekbang.org/column/article/545953\">渲染管线以及着色器的职责</a>，GLSL理解起来也就容易多了。</p><p>GLSL（OpenGL Shading Language）就是OpenGL为了实现着色器给开发人员提供的一种开发语言。接触一门新的编程语言，一般要先看一下它的数据类型和修饰符，然后再学习这种语言的内嵌函数，最终再构建一个完整的程序，让它跑起来。我们先来看一下基本的数据类型。</p><!-- [[[read_end]]] --><h3>GLSL的修饰符与数据类型</h3><p><img src=\"https://static001.geekbang.org/resource/image/43/3c/43605e35dc22794f5db5db21af9d743c.png?wh=1588x1326\" alt=\"图片\"></p><p>其实GLSL的语法跟C语言非常类似，所以我们学起来也会比较轻松。我们先来看一下<strong>GLSL中变量的修饰符</strong>，具体如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/17/1e/1703e04fd6c27993cbb8d9e94bfa9a1e.png?wh=1920x645\" alt=\"图片\" title=\"GLSL中变量的修饰符\"></p><p>接着我们看一下<strong>GLSL的基本数据类型：</strong>int、float、bool，这些都是和C语言一致的，有一点需要强调的就是，GLSL中的float是可以再加一个修饰符的，这个修饰符用来指定精度。修饰符的可选项有三种：</p><p><img src=\"https://static001.geekbang.org/resource/image/34/y7/349c929770ff5f73aa312edf86216yy7.png?wh=1920x485\" alt=\"图片\" title=\"float的修饰符\"></p><p>一般基础数据类型讲完之后，就应该讲数组了，在<strong>GLSL中就是向量类型</strong>（vec），向量类型是Shader中最常用的一个数据类型，因为在做数据传递的时候经常要传递多个参数，相较于写多个基本数据类型，使用向量类型更加简单。比如，通过OpenGL接口把物体坐标和纹理坐标传递到顶点着色器中，用的就是向量类型。每个顶点都是一个四维向量，在顶点着色器中利用这两个四维向量就能去做自己的运算，你可以看下它的声明方式。</p><pre><code class=\"language-plain\">attribute vec4 position;\n</code></pre><p>接下来是<strong>矩阵类型</strong>（matrix），矩阵类型在GLSL中同样也是一个非常重要的数据类型，在某些效果器的开发中，需要开发者自己传入一些矩阵类型的数据，用于像素计算。比如GPUImage中的怀旧效果器，就需要传入一个矩阵来改变原始的像素数据，你可以看一下它的声明方式。</p><pre><code class=\"language-plain\">uniform lowp mat4 colorMatrix;\n</code></pre><p>上面的代码表示的是一个4<em>4的浮点矩阵，如果是mat2的声明，代表的就是2</em>2的浮点矩阵，而mat3代表的就是3*3的浮点矩阵。OpenGL为开发者提供了以下接口，把内存中的数据（mColorMatrixLocation）传递给着色器。</p><pre><code class=\"language-plain\">glUniformMatrix4fv(mColorMatrixLocation, 1, false, mColorMatrix);\n</code></pre><p>其中，mColorMatrix是这个变量在接口程序中的句柄。这里一定要注意，上边的这个函数不属于GLSL部分，而是属于客户端代码，也就是说，我们调用这个函数来和着色器进行交互。</p><p>接下来就是<strong>纹理类型</strong>，在这节课的最后我们会介绍纹理应该如何加载以及渲染，这里我们先把关注点放在如何声明这个类型上，这个类型一般只在片元着色器中使用，下面GLSL代码是二维纹理类型的声明方式。</p><pre><code class=\"language-plain\">uniform sampler2D texSampler；\n</code></pre><p>那客户端如何写代码来把图像传递进来呢？首先我们需要拿到这个变量的句柄，定义为mGLUniformTexture，然后就可以给它绑定一个纹理，接口程序的代码如下：</p><pre><code class=\"language-plain\">glActiveTexture(GL_TEXTURE0);\nglBindTexture(GL_TEXTURE_2D, texId);\nglUniform1i(mGLUniformTexture, 0);\n</code></pre><p>注意，<strong>上述接口程序中的第一行代码激活的是哪个纹理句柄，在第三行代码中的第二个参数就需要传递对应的Index</strong>，就比如说代码中激活的纹理句柄是GL_TEXTURE0，对应的第三行代码中的第二个参数Index就是0，如果激活的纹理句柄是GL_TEXTURE1，那对应的Index就是1，句柄的个数在不同的平台不一样，但是一般都会在32个以上。</p><p>最后，我们来看一个比较特殊的<strong>传递类型</strong>，在GLSL中有一个特殊的修饰符就是varying，这个修饰符修饰的变量都是用来在顶点着色器和片元着色器之间传递参数的。最常见的使用场景就是在顶点着色器中修饰纹理坐标，顶点着色器会改变这个纹理坐标，然后把这个坐标传递到片元着色器，代码如下：</p><pre><code class=\"language-plain\">attribute vec2 texcoord;\nvarying vec2 v_texcoord;\nvoid main(void)\n{\n&nbsp;&nbsp;&nbsp; //计算顶点坐标\n&nbsp;&nbsp;&nbsp; v_texcoord = texcoord;\n}\n</code></pre><p>接着在片元着色器中也要声明同名的变量，然后使用texture2D方法来取出二维纹理中这个纹理坐标点上的纹理像素值，代码如下：</p><pre><code class=\"language-plain\">varying vec2 v_texcoord;\nvec4 texel = texture2D(texSampler, v_texcoord);\n</code></pre><p>取出了这个坐标点上的像素值，就可以进行像素变化操作了，比如说去提高对比度，最终将改变的像素值赋值给gl_FragColor。</p><h3>GLSL的内置变量与内嵌函数</h3><p>接下来我们看一下GLSL内置变量。常见的是两个Shader的输出变量，一个是顶点着色器的内置变量gl_position，它用来设置顶点转换到屏幕坐标的位置。</p><pre><code class=\"language-plain\">vec4 gl_posotion;\n</code></pre><p>另外一个内置变量用来设置每一个粒子矩形大小，一般是在粒子效果的场景下，需要为粒子设置绘制的半径大小时使用。</p><pre><code class=\"language-plain\">float gl_pointSize;\n</code></pre><p>其次是片元着色器的内置变量gl_FragColor，用来指定当前纹理坐标所代表的像素点的最终颜色值。</p><pre><code class=\"language-plain\">vec4 gl_FragColor;\n</code></pre><p>然后是GLSL内嵌函数部分，我们在这里只介绍常用的几个常用函数。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/29/e93a58f8c956ed8f0bf0805db008bd29.png?wh=1866x1094\" alt=\"图片\" title=\"GLSL的内嵌函数\"></p><p>其他函数，比如角度函数、指数函数、几何函数，这里我就不再赘述了，用到的时候你可以去<a href=\"https://www.khronos.org/opengles/sdk/docs/manglsl/docbook4/\">官方文档</a>中进行查询。</p><p>对于一种语言的语法来讲，剩下的就是控制流的部分了。GLSL的控制流与C语言非常类似，既可以使用for、while以及do-while实现循环，也可以使用if和if-else进行条件分支的操作，在后面的实践过程中，GLSL代码中都会用到这些控制流，这里我们就不再讲解了。</p><p>到这里， GLSL的语法部分已经讲完了，相信你已经可以书写出着色器了。现在我们再来思考一个问题：GLProgram是对什么进行绘制或者渲染呢？</p><p>没错，就是纹理，接下来让我们一起来学习一下。</p><h2>OpenGL ES的纹理</h2><p>OpenGL中的纹理用GLUint类型来表示，通常我们称之为Texture或者TextureID，可以用来表示图像、视频画面等数据。在我们这个专栏里，只需要处理二维的纹理，每个二维纹理都由许多小的片元组成，每一个片元我们可以理解为一个像素点。大多数的渲染过程，都是基于纹理进行操作的，最简单的一种方式就是从一个图像文件加载数据，然后上传到显存中构造成一个纹理，具体的方式后续我们会介绍。</p><h3>纹理坐标系</h3><p>为了访问到纹理中的每一个片元（像素点），OpenGL ES构造了纹理坐标空间，坐标空间的定义是从左下角的（0，0）到右上角的（1，1）。横轴维度称为S轴，左边是0，右边是1，纵轴维度称为T轴，下面是0，上面是1。按照这个规则就构成了左图所示的坐标系，可以看到上下左右四个顶点的坐标位置，而中间的位置就是（0.5，0.5）。</p><p>另外在这里不得不提的是计算机系统里的坐标空间，通常X轴称之为横轴，从左到右是0～1，Y轴称之为纵轴，是从上到下是0～1，如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/bd/60/bd8038285123c7da28150b7d41bfb360.png?wh=1920x1064\" alt=\"图片\" title=\"OpenGL二维纹理坐标和计算机图像二维纹理坐标对比\"></p><p>无论是计算机还是手机的屏幕坐标，X轴是从左到右是0～1，Y轴是从上到下是0～1，这种存储方式是和图片的存储是一致的。我们这里假设图片（Bitmap）的存储是把所有像素点存储到一个大数组中，数组的第一个像素点表示的就是图片左上角的像素点（即第一排第一列的像素点），数组中的第二个元素表示的是第一排第二列的第二个像素点，依此类推。</p><p>这样你会发现<strong>这种坐标其实是和OpenGL中的纹理坐标做了一个旋转180度</strong>，理解这一点是非常重要的，因为接下来我们学习如何从本地图片中加载一张纹理并且渲染到界面上的时候，就会用到纹理坐标和计算机系统的坐标的转换。</p><h3>纹理创建与绑定</h3><p>下面我们来看看如何加载一张图片作为OpenGL中的纹理。首先要在显卡中创建一个纹理对象，OpenGL ES提供了方法原型如下：</p><pre><code class=\"language-plain\">void glGenTextures (GLsizei n, GLuint* textures)\n</code></pre><p>这个方法中的第一个参数是需要创建几个纹理对象，第二个参数是一个数组（指针）的形式，函数执行之后会将创建好的纹理句柄放入到这个数组中。如果仅仅需要创建一个纹理对象的话，只需要声明一个GLuint类型的texId，然后将这个纹理ID取地址作为第二个参数，就可以创建出这个纹理对象，代码如下：</p><pre><code class=\"language-plain\">glGenTextures(1, &amp;texId);\n</code></pre><p>执行完上面这个指令之后，OpenGL引擎就会在显卡中创建出一个纹理对象，并且把这个纹理对象的句柄存储到texId这个变量中。</p><p>那接下来我们要对这个纹理对象进行操作，具体应该怎么做呢？OpenGL ES提供的都是类似于状态机的调用方式，也就是说在对某个OpenGL ES对象操作之前，先进行绑定操作，然后接下来所有操作的目标都是针对这个绑定的对象进行的。对于纹理ID的绑定调用代码如下：</p><pre><code class=\"language-plain\">glBindTexture(GL_TEXTURE_2D, texId);\n</code></pre><p>执行完上面这个指令之后，OpenGL ES引擎认为这个纹理对象已经处于绑定状态，那么接下来所有对于纹理的操作都是针对这个纹理对象的了，当我们操作完毕之后可以调用如下代码进行解绑：</p><pre><code class=\"language-plain\">glBindTexture(GL_TEXTURE_2D, 0);\n</code></pre><p>上面这行指令执行完毕之后，就代表我们不会对texId这个纹理对象做任何操作了，所以上面这行代码一般在一个GLProgram执行完成之后调用。</p><p>那一般对纹理的操作或者设置有哪些呢？</p><p>首先就是纹理的过滤方式，当纹理对象被渲染到物体表面上的时候，纹理的过滤方式指定纹理的放大和缩小规则。实际上，是OpenGL ES的绘制管线中将纹理的元素映射到片元这一过程中的映射规则，因为纹理（可以理解为一张图片）大小和物体（可以理解为手机屏幕的渲染区域）大小不太可能一致，所以要指定放大和缩小的时候应该具体确定每个片元（像素）是如何被填充的。</p><p>放大（magnification）规则的设置：</p><pre><code class=\"language-plain\">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);\n</code></pre><p>缩小（minification）规则的设置：</p><pre><code class=\"language-plain\">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);\n</code></pre><p>上述两个指令设置的过滤方式都是GL_LINEAR，这种过滤方式叫做双线性过滤，底层使用双线性插值算法来平滑像素之间的过渡部分，OpenGL的具体实现会使用四个邻接的纹理元素，并在它们之间用一个线性插值算法做插值，这种过滤方式是最常用的。</p><p>OpenGL还提供了GL_NEAREST的过滤方式，GL_NEAREST被称为最邻近过滤，底层为每个片段选择最近的纹理元素进行填充，缺点就是当放大的时候会丢失掉一些细节，会有很严重的锯齿效果。因为是原始的直接放大，相当于降采样。而当缩小的时候，因为没有足够的片段来绘制所有的纹理单元，也会丢失很多细节，是真正的降采样。</p><p>其实OpenGL还提供了另外一种技术，叫做MIP贴图，但是这种技术会占用更多的内存，优点是渲染也会更快。当缩小和放大到一定程度之后效果也比双线性过滤的方式更好，但是它对纹理的尺寸以及内存的占用是有一定限制的。不过，在处理以及渲染视频的时候不需要放大或者缩小这么多倍，所以在这种场景下MIP贴图并不适用。</p><p>综合对比这几种过滤方式，在使用纹理的过滤方式时我们一般都会选用双线性过滤的过滤方式（GL_LINEAR）。</p><p>接下来，我们看纹理对象的另外一个设置，也就是在纹理坐标系中的s轴和t轴超出范围的纹理处理规则，常见的代码设置如下：</p><pre><code class=\"language-plain\">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\n</code></pre><p>上述代码表示的含义就是，给这个纹理的s轴和t轴的坐标设置为GL_CLAMP_TO_EDGE类型，代表所有大于1的像素值都按照1这个点的像素值来绘制，所有小于0的值都按照0这个点的像素值来绘制。除此之外，OpenGL ES还提供了GL_REPEAT和GL_MIRRORED_REPEAT的处理规则，从名字也可以看得出来，GL_REPEAT代表超过1的会从0再重复一遍，也就是再平铺一遍，而GL_MIRRORED_REPEAT就是完全镜像地平铺一遍。</p><p>现在，我们创建出了一个纹理对象，并对这个纹理对象进行了一系列的设置，接下来就到关键的地方了，就是将内存中的一个图片数据上传到这个纹理对象中去。</p><h3>纹理的上传与下载</h3><p>假设我们有一张PNG类型的图片，我们需要将它解码为内存中RGBA裸数据，所以首先我们需要解码。可以采用跨平台（C++层）的方式，引用libpng这个库来进行解码操作，当然也可以采用各自平台的API进行解码。无论哪一种方式，最终都可以得到RGBA的数据。等拿到RGBA的数据之后，记为uint8_t数组类型的pixels。</p><p>接下来，就是要将PNG素材的内容放到这个纹理对象上面去了，如何上传到纹理上面去呢？代码如下：</p><pre><code class=\"language-plain\">glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GL_UNSIGNED_BYTE, pixels);\n</code></pre><p>执行上述指令的前提是我们已经绑定了某个纹理，OpenGL的大部分纹理一般只接受RGBA类型的数据。当然在视频场景下，考虑性能问题也会使用到GL_LUMINANCE类型，不过需要在片元着色器中，把YUV420P格式转换成RGBA格式。上述指令正确执行之后，RGBA的数组表示的像素内容会上传到显卡里面texId所代表的纹理对象中，以后要使用这个图片，直接使用这个纹理ID就可以了。</p><p>既然有内存数据上传到显存的操作，那么一定也会有显存的数据回传回内存的操作，这个应该如何实现呢？代码如下：</p><pre><code class=\"language-plain\">glReadPixels(0, 0, width, height, GL_RGBA, GL_UNSIGNED_BYTE, pixels);\n</code></pre><p>执行上述指令的前提是我们已经绑定了某个纹理，然后将绑定的这个纹理对象代表的内容拷贝回pixels这个数组中，这个拷贝会比较耗时，并且拷贝时间会和分辨率（width\\height）大小成正比。一般在实际的开发工作中要尽量避免这种内存和显存之间的数据拷贝与传输，而是使用各个平台提供的快速映射API去完成内存与显存的拷贝工作。</p><h2>物体坐标与纹理绘制</h2><p>通过刚刚的操作，我们已经准备好了一个纹理，那么如何把这张图片（纹理）绘制到屏幕上呢？在实际绘制之前，我们还需要了解一下一下OpenGL中的物体坐标系。</p><h3>物体坐标系</h3><p>如图所示，OpenGL规定物体坐标系中X轴从左到右是从-1到1变化的，Y轴从下到上是从-1到1变化的，物体的中心点是(0, 0)的位置。</p><p><img src=\"https://static001.geekbang.org/resource/image/97/5f/97e9ecff3314d71ffbb429d301b60a5f.png?wh=1920x1063\" alt=\"图片\" title=\"物体坐标系\"></p><p>接下来的任务就是将这个纹理绘制到物体（屏幕）上，首先要搭建好各自平台的OpenGL ES的环境，包括上下文与窗口管理，然后创建显卡可执行程序，最终让程序跑起来。</p><h3>纹理的绘制</h3><p>先来看一个最简单的顶点着色器（Vertex Shader），代码如下：</p><pre><code class=\"language-plain\">static char* COMMON_VERTEX_SHADER =\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"attribute vec4 position;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"attribute vec2 texcoord;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"varying vec2 v_texcoord;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"void main(void)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp; gl_Position = position;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp; v_texcoord = texcoord;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\";\n</code></pre><p>片元着色器（Fragment Shader），代码如下：</p><pre><code class=\"language-plain\">static char* COMMON_FRAG_SHADER =\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"precision highp float;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"varying highp vec2 v_texcoord;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"uniform sampler2D texSampler;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"void main() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; gl_FragColor = texture2D(texSampler, v_texcoord);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\";\n</code></pre><p>利用上面两个Shader创建好的这个Program，我们记为mGLProgId。接下来我们需要将这个Program中的重点属性以及常量的句柄寻找出来，以备后续渲染过程中向顶点着色器和片元着色器传递数据。</p><pre><code class=\"language-plain\">mGLVertexCoords = glGetAttribLocation(mGLProgId, \"position\");\nmGLTextureCoords = glGetAttribLocation(mGLProgId, \"texcoord\");\nmGLUniformTexture = glGetUniformLocation(mGLProgId, \"texSampler\");\n</code></pre><p>在这个例子里，我们要从Program的顶点着色器中读取两个attribute，并放置到全局变量的mGLVertexCoords与mGLTextureCoords中，从Program的片元着色器中读取出来的uniform会放置到mGLUniformTexture这个变量里。<br>\n所有准备工作都做好了之后，接下来进行真正的绘制操作。</p><p>首先，规定窗口大小：</p><pre><code class=\"language-plain\">glViewport(0, 0, screenWidth, screenHeight);\n</code></pre><p>函数中的参数screenWidth表示绘制View或者目标FBO的宽度，screenHeight表示绘制View或者目标FBO的高度。<br>\n然后使用显卡绘制程序：</p><pre><code class=\"language-plain\">glUseProgram(mGLProgId);\n</code></pre><p>设置物体坐标与纹理坐标：</p><pre><code class=\"language-plain\">GLfloat vertices[] = { -1.0f, -1.0f, 1.0f, -1.0f, -1.0f, 1.0f, 1.0f, 1.0f };\nglVertexAttribPointer(mGLVertexCoords, 2, GL_FLOAT, 0, 0, vertices);\nglEnableVertexAttribArray(mGLVertexCoords);\n</code></pre><p>设置纹理坐标：</p><pre><code class=\"language-plain\">GLfloat texCoords1[] = { 0.0f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 1.0f, 1.0f };\nGLfloat texCoords2[] = { 0.0f, 1.0f, 1.0f, 1.0f, 0.0f, 0.0f, 1.0f, 0.0f };\nglVertexAttribPointer(mGLTextureCoords, 2, GL_FLOAT, 0, 0,&nbsp; texCoords2);\nglEnableVertexAttribArray(mGLTextureCoords);\n</code></pre><p>代码中有两个纹理坐标数组，分别是texCoords1与texCoords2，最终我们使用的是texCoords2这个纹理坐标。因为我们的纹理对象是将一个RGBA格式的PNG图片上传到显卡上，其实上传上来本身就需要转换坐标系，这两个纹理坐标恰好就是做了一个上下的翻转，从而将计算机坐标系和OpenGL坐标系进行转换。对于第一次上传内存数据的场景纹理坐标一般都会选用texCoords2。但是如果这个纹理对象是OpenGL中的一个普通纹理对象的话，则需要使用texCoords1。</p><p>接下来，指定我们要绘制的纹理对象，并且将纹理句柄传递给片元着色器中的uniform常量：</p><pre><code class=\"language-plain\">glActiveTexture(GL_TEXTURE0);\nglBindTexture(GL_TEXTURE_2D, texId);\nglUniform1i(mGLUniformTexture, 0);\n</code></pre><p>执行绘制操作：</p><pre><code class=\"language-plain\">glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);\n</code></pre><p>上述这行指令执行成功之后，就相当于将最初内存中的PNG图片绘制到默认的FBO上去了，最终再通过各平台的窗口管理操作（Android平台的swapBuffer、iOS平台的renderBuffer），就可以让用户在屏幕上看到了。</p><p>当确定这个纹理对象不再使用了，则需要删掉它，执行代码是：</p><pre><code class=\"language-plain\">glDeleteTextures(1, &amp;texId);\n</code></pre><p>如果不调用这个方法，就会造成显存的泄露。同时调用删除纹理对象的时候，也必须要在OpenGL线程中进行操作。</p><p>除此之外，关于纹理的绘制我们还要额外注意一点：<strong>我们提交给OpenGL的绘图指令并不会马上送给图形硬件执行，而是会放到一个指令缓冲区中。</strong>考虑性能的问题，等缓冲区满了以后，这些指令会被一次性地送给图形硬件执行，指令比较少或比较简单的时候，是没办法填满缓冲区的，所以这些指令不能马上执行，也就达不到我们想要的效果。因此每次写完绘图代码，想让它立即完成效果的时候，就需要我们自己手动调用glFlush()或gLFinish()函数。</p><ul>\n<li>glFlush：将缓冲区中的指令（无论是否为满）立刻送给图形硬件执行，发送完立即返回；</li>\n<li>glFinish：将缓冲区中的指令（无论是否为满）立刻送给图形硬件执行，但是要等待图形硬件执行完后这些指令才返回。</li>\n</ul><p>在遇到绘图指令不能马上执行时，手动调用这两个函数可能会解决我们的问题。</p><h2>小结</h2><p><img src=\"https://static001.geekbang.org/resource/image/d6/b7/d604739781322a99ac3c76da398467b7.png?wh=1920x933\" alt=\"图片\"></p><p>到这里，我们整个移动平台视频渲染部分的内容就学完了。最后，我们一起来回顾一下吧！</p><p>我们从视频渲染的技术选型开始，给你介绍了几种视频渲染的技术手段并且分析了各自的优缺点，最终选择OpenGL ES作为我们这个部分的学习重点，但是要想完全弄懂OpenGL ES并实际上手操作，确实比较难。</p><p>所以这个部分我带着你从它的概念、用途开始，然后逐步了解它内部的运行机制和GLSL语法，并且创建出了一个GLProgram，继而在各个平台上搭建出OpenGL ES的上下文环境，最后让这个GLProgram运行到了搭建的上下文环境中。你可以结合这个思维导图来回顾一下我们这部分的重点内容。</p><p><img src=\"https://static001.geekbang.org/resource/image/0c/b9/0c494e8fcf68eec795266f84f5b831b9.png?wh=1920x1546\" alt=\"图片\"></p><h2>思考题</h2><p>打开<a href=\"https://github.com/BradLarson/GPUImage\">GPUImage</a>这个项目，阅读一下GPUImage这个框架，然后描述出这个框架的整体设计思路，你可以带着以下几个问题去思考。</p><ol>\n<li>GPUImage是如何构建上下文的？</li>\n<li>GPUImage中对于和OpenGL ES相关的API是如何抽象的？</li>\n<li>GPUImage核心的架构是如何设计的？</li>\n<li>GPUImage中的共享上下文是如何实现的？</li>\n<li>GPUImage中的TextureCache存在的意义是什么？</li>\n</ol><p>欢迎你在评论区留下自己的见解，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"05｜移动平台的视频渲染（二）：OpenGL ES上下文环境搭建","id":546501},"right":{"article_title":"07｜播放器项目实战（一）：场景分析与架构设计","id":548457}}},{"article_id":548457,"article_title":"07｜播放器项目实战（一）：场景分析与架构设计","article_content":"<p>你好，我是展晓凯。今天我们来学习如何写一个播放器。</p><p>前面我们分别学习了移动端音频的渲染和视频的渲染，现在是时候用一个完整的项目来将我们学习的知识串联起来了，所以从这节课开始，我们写一个视频播放器来实际操练一下。</p><p>播放器项目属于系统性比较强的项目，我会带着你从场景分析入手，然后进行架构设计与模块拆分，再到核心模块实现以及数据指标监控，最后还会向你介绍从这个基础的播放器如何扩展到其他业务场景，所以整体内容还是比较多的，我会分成三讲带着你学习。首先，我们一起进入播放器的场景分析与架构设计的部分吧。</p><h2>场景分析</h2><p>我们先来思考一下，播放器要提供哪些功能给用户？最基本的功能自然是从零开始播放视频，能听到声音、看到画面，并且声音和画面是要对齐的，然后还需要支持暂停和继续播放功能；另外，需要支持seek功能，即可以随意拖动到任意位置，并立即从这个位置继续播放；高级一点的也会支持切换音轨（如果视频中有多个音轨的话）、添加字幕等功能。</p><p>下面我们就先来实现最基本的功能，也就是播放器可以从头播放、暂停和继续的功能。如果直接让你实现这样一个项目，你可能会找不到任何头绪。但作为一个开发人员，我们需要具备把复杂的问题简单化，简单的问题条理化的能力，最终按照拆分得非常细的模块来逐个实现。那基于这个播放器项目，我们需要问自己几个问题：</p><!-- [[[read_end]]] --><ul>\n<li>输入是什么？</li>\n<li>输出是什么？</li>\n<li>要将输入转换为输出需要几个模块以及每个模块的职责是什么？</li>\n</ul><p>那接下来我们会逐一回答一下这几个问题，我们先了解一下播放器的输入是什么，它可以是本地硬盘上的一个媒体文件，格式有可能是FLV 、MP4、AVI、MOV等；也可以是网络上的一个媒体文件，网络传输协议有可能是HTTP、RTMP、HLS等协议，这样我们就确定了播放器的输入。</p><p>那接下来再看输出是什么，输出就是让用户可以听到、看到这个视频，也就是可以把视频中的音频播放出来，同时把视频画面渲染到屏幕上，并且让声音和画面同步播放出来，这样我们进一步确定了输出；最后一步我们根据输入和输出来拆分模块，并给模块分配合理的职责，其实就是需要将输入资源和掌握的音视频能力进行合理的规划和使用，来实现最终的输出结果，但这个问题相比前两个问题要复杂得多，我会带着你来慢慢分析。</p><h3>输入分析</h3><p>输入资源有可能是不同的协议，比如本地磁盘的文件（file）或者是HTTP、RTMP、HLS等协议，也有可能是不同的封装格式，比如MP4、FLV、MOV。这些封装格式里通常会有两个Stream（轨道/流），分别是音频流（轨道）和视频流（轨道）。每个轨道里面存储的都是压缩后的编码格式，音频一般为AAC、视频一般为H264。</p><p>对于这样的输入我们要将这两路流都解码为裸数据，等视频流和音频流都解码为裸数据之后，就可以用我们前面学习的音视频渲染方法去渲染了。但是如果在需要渲染一帧的时候再去做解码，那这一帧视频就有可能出现卡顿或者延迟，所以这里就需要用到视频播放器中的第一个线程——解码线程了，这个线程用来解析协议、处理解封装以及解码，最终把裸数据放到我们音频和视频的队列中，这个模块被称为<strong>输入模块</strong>。</p><h3>输出分析</h3><p>接下来我们看输出部分，输出部分由音频的输出和视频的输出两部分组成。不过可以确定的是，不论音频的输出还是视频的输出，都需要用一个独立的线程来管理，这两个线程会先去输入模块管理的队列中拿出音视频的裸数据，然后分别进行音视频的渲染，最终让用户听得到声音、看得到画面，这两个模块被称为<strong>音频输出模块</strong>和<strong>视频输出模块</strong>。</p><p>再来思考一件事情，输出模块都在各自的线程中，由于两个输出模块的播放频率以及线程控制没有任何关系，这就导致了另外一个问题：音画没有对齐。在上面我们规划的各个模块里，还没有一个模块的职责是负责音视频同步，所以需要再建立一个模块来负责相关的工作，这个模块就是<strong>音视频同步模块</strong>。</p><p>到这里，我们把模块都拆分完了，具体的模块分布如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/a3/8b32157027f571d4b4d53158f06cd4a3.png?wh=1920x918\" alt=\"图片\" title=\"架构设计图\"></p><p>左侧是输入模块，负责将多媒体文件处理成音视频裸数据；中间是音视频的队列负责存储音视频的裸数据；右侧中间是音视频同步模块，负责音视频的同步；音频输出与视频输出模块负责音视频的渲染。基于以上的模块拆分，我们就可以设计整体架构，然后为每个模块来做技术选型了。</p><h2>架构设计</h2><p>了解了具体的模块后，我们来整体看一下不同模块之间如何组装到一起。</p><p>音视频同步模块向外界暴露获取音频数据、视频数据的接口，这两个接口提供数据的同时要保持同步。音视频同步模块在内部组装输入模块，负责解码线程的调度。然后我们把音视频同步模块、音频输出模块、视频输出模块封装到调度器模块中，调度器模块会分别向音频输出模块和视频输出模块注册回调函数，调度器模块的回调函数中就调用音视频同步模块来获取音频数据和视频数据。</p><p>基于以上架构设计，我们可以进一步整理类图设计，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/4f/2cff86a464a7862e2a1a71f84d271b4f.png?wh=1920x1122\" alt=\"图片\" title=\"类图设计\"></p><p>我们可以详细地看一下类图设计中的各个模块。</p><ul>\n<li>VideoPlayerController：调度器模块的类，内部维护音视频同步模块、音频输出模块、视频输出模块，向上层业务暴露开始播放、暂停、继续播放、停止播放等接口；向音频输出模块和视频输出模块暴露两个获取裸数据的接口。</li>\n<li>AudioOutput：音频输出模块，在不同平台会有不同的实现，但是一般音频的渲染要放在单独的一个线程中进行，在运行过程中会调用注册过来的回调函数来获取音频数据。</li>\n<li>VideoOutput：视频输出模块，虽然我们统一使用OpenGL ES来渲染视频，但是前面也讲过，OpenGL ES在不同平台也会有自己的上下文环境，所以这里采用了Void类型的实现，当然，必须由我们主动开启一个线程来作为OpenGL ES的渲染线程，它会在运行过程中调用注册过来的回调函数，来获取视频的裸数据进行渲染。</li>\n<li>AVSynchronizer：音视频同步模块，用来组合输入模块及音频队列和视频队列，主要给它的客户端代码 VideoPlayerController这个调度器提供接口，接口包括开始、结束，还有最重要的获取音频数据和对应时间戳的视频帧等。此外，它也会维护一个解码线程，并且根据音视频队列的状态来暂停或者继续运行这个解码线程。</li>\n<li>AudioFrame：音频帧，这个结构体中记录了一段PCM Buffer以及这一帧的时间戳等信息。</li>\n<li>AudioFrameQueue：音频队列，主要用于存储音频帧，为它的客户端代码音视频同步模块提供压入和弹出操作，由于解码线程和声音播放线程会作为生产者和消费者同时访问这个队列，所以这个队列要确保具有线程安全性。</li>\n<li>VideoFrame：视频帧，这个结构体中记录了YUV数据以及这一帧数据的宽、高以及时间戳等信息。</li>\n<li>VideoFrameQueue：视频队列，主要用于存储视频帧，为它的客户端代码音视频同步模块提供压入和弹出操作，由于解码线程和视频播放线程会作为生产者和消费者同时访问这个队列中的元素，所以这个队列也要确保线程的安全性。</li>\n<li>VideoDecoder：输入模块，职责在前面已经分析了，由于还没有确定具体的技术实现，所以这里我们根据前面的分析写了三个实例变量，协议层解析器、格式解封装器还有解码器，并且它主要向AVSynchronizer暴露一些接口，如打开文件资源（网络或者本地）、关闭文件资源、解码出一定时间长度的音视频帧等。</li>\n</ul><p>到这儿，我们根据用户场景把视频播放器拆解成了各个模块，并且根据模块的调用关系画出了类图，那么接下来要做的事情就是来拆分每个模块的具体实现。</p><h2>每个模块的具体实现</h2><h3>输入模块</h3><p>从输入文件到最终得到裸数据，会经历解析协议、解封装、解码三个步骤。如果我们自己来写代码，处理不同的协议、不同的编解码格式（更专业地讲是各种解码器），会非常复杂也很不合理，要付出很大的开发与测试成本，并且最终效果也可能不会太理想。现在已经有一些成熟的技术可以供我们使用了，选择FFmpeg这个开源库来作为输入模块的技术选型是最合适不过的了。</p><p>FFmpeg中的libavformat模块可以处理各种不同的协议以及不同的封装格式，先用libavformat模块把文件解封装成每一路流，之后再进行解码。最简单的方式是直接使用FFmpeg的libavcodec模块来实现，但是如果需要更高性能的解码手段，我们可以使用Android和iOS平台各自的硬件解码器。</p><p>这节课暂时不考虑优化，只是先快速地实现一套方案，使用软件解码是一种好的选择，所以这节课我们使用FFmpeg的libavcodec模块来作为解码器的技术选型。</p><p>其实对于架构设计来说，没有最好的设计，只有最适合当前业务阶段的设计。放在这里来讲，就是硬件解码器对系统平台是有限制的，同时也会有一些兼容性问题，两个平台还需要分别去写代码做各自硬件解码器的实现，并且还要将硬件解码器的输出转换为可用于显示的视频帧数据结构。</p><p>因此我们这里选择使用软件解码器，它有更高的兼容性及更简单的API调用接口。另外考虑兼容性，以后可能需要硬件解码来提升性能，所以在设计解码模块的时候，我们可以更多地使用面向接口的设计，方便之后更加高效地替换实现。</p><h3>输出模块</h3><p>下面我们来看音频输出模块，我们知道音频渲染的技术选型有多种，让我们简单回顾一下。<br>\n<img src=\"https://static001.geekbang.org/resource/image/ac/b3/ac19d5c52c511e55fd82b35d19647fb3.png?wh=2126x666\" alt=\"\"><br>\n首先是Android平台，常用的就是Java层的AudioTrack和Native层的OpenSL ES。由于播放器的核心逻辑是在Native层，在AudioTrack和OpenSL ES之间，我们还是选择OpenSL ES，因为这样省去了JNI的数据传递，并且OpenSL ES在播放声音方面的延迟更低，缺点就是OpenSL ES提供的API比起AudioTrack不够友好，调试也不太方便，但是总体来衡量，还是OpenSL ES更合适些。</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/95/a6f8ca3dc1bc6046c5112yy522e4b495.jpg?wh=1254x444\" alt=\"\"></p><p>而iOS平台，比较常见的就是AudioQueue和AudioUnit，AudioQueue是更高层次的音频API，是建立在AudioUnit的基础之上的，提供的API更加简单，在这里选用AudioQueue其实也是可以的，但是我们最终选择了AudioUnit，首先是因为音频渲染过程中有可能存在音频格式的转换，这时使用AudioUnit会更加方便；其次我们也要为后续的录音、音效处理等打下使用AudioUnit的基础。所以这里我们最终选择AudioUnit作为实现方案。</p><p>然后是视频输出模块，技术选型肯定要选择OpenGL ES，因为不论在Android还是iOS平台我们都可以利用它高效地渲染视频。此外，在这里使用OpenGL ES还有一个好处，那就是扩展性。我们可以利用OpenGL ES处理图像的巨大优势，来对视频做一个后处理，比如增加去块滤波器、对比度等效果器，让用户感觉视频更加清晰。</p><p>前面我们已经学习了如何在Android平台和iOS平台搭建OpenGL ES的环境，在Android平台使用EGL来为OpenGL ES提供上下文环境，使用SurfaceView（TextureView）的Surface来构造显示对象，最终输出到SurfaceView（TextureView）上；在iOS平台使用EAGL来为OpenGL ES提供上下文环境，自己定义一个继承自UIView的View，使用EAGLLayer作为渲染对象，最终渲染到这个自定义的View上。</p><h3>音视频同步模块</h3><p>音视频同步模块中其实不会涉及任何平台相关的API，不过考虑到它要维护解码线程，因此使用PThread来创建线程会是一个好的选择，原因是两个平台都支持这种线程模型。此外，这个模块还需要维护两个队列，由于STL中提供的标准队列不能保证线程安全性，所以对于音视频队列，我们自己写一个保证线程安全的链表来实现。</p><p>音视频同步的策略一般分为三种：音频向视频同步；视频向音频同步；音频视频统一向外部时钟同步。具体操作我会在第9讲中进行详细地介绍，我们实现的播放器中的音视频对齐策略就选用业内常用的第二种方式，即视频向音频对齐的方式，而到代码实现阶段，音视频同步这块逻辑放到获取视频帧的方法里面就可以了。</p><h3>控制器模块</h3><p>最后是控制器，控制器需要把上述的三个模块合理地组装起来。在开始播放的时候，需要把资源的地址（有可能是本地的文件，也有可能是网络的资源文件）传递给AVSynchronizer。如果能够成功地打开文件，那么就去实例化VideoOutput和AudioOutput。</p><p>在实例化这两个类的同时，要传入回调函数，这两个回调函数又分别去调用AVSynchronizer里获取音频和视频帧的方法，这样就可以有序地组织多个模块，最终如果暂停、继续的指令调用下来，也相应地去调用各个模块对应的生命周期方法。</p><h2>小结</h2><p><img src=\"https://static001.geekbang.org/resource/image/fa/85/fac701bff764c01405f2b15373b85685.png?wh=1726x1866\" alt=\"\"></p><p>这节课我带你完成了视频播放器的场景分析和架构设计，学到这里我相信在你心里视频播放器的核心架构已经基本成型了。但是如果想要成为一个优秀的架构师，仅仅做到这些其实是不够的，我们必须在做完整个架构之后，再针对这个架构给出风险评估与部分测试用例，下面我们也逐一来分析一下。</p><p>首先是风险评估，由于我们最终做的项目是运行在移动平台上的，所以对于移动平台的设备碎片化（尤其是Android平台，碎片化更加严重）这一现象，必须要有足够的设备作为测试目标，以保证没有兼容性问题，设备所属的平台架构也应该覆盖到arm、armv7、arm64等平台。</p><p>然后是性能的评估，性能包括CPU消耗、内存占用、耗电量与发热量，其中一些风险点在这一期项目中可能无法完全解决，那我们就需要在架构设计中留出足够的扩展来应对这些风险。其实，目前来看最大的风险就是软件解码这部分，长期来看，需要有硬件解码的替代方案。</p><p>对于测试用例，我们要在以下几方面进行重点测试，首先是输入模块，包括协议层（网络资源、本地资源）、封装格式（FLV、MP4、MOV、AVI等等）、编码格式（H264、AAC、WAV）等；其次是音视频同步模块，应该在低网速的条件下观看网络资源的对齐程度，同时也要考虑蓝牙耳机的对齐程度，一些蓝牙耳机输出的Buffer很大；最后是两个输出模块，测试要覆盖iOS系统以及Android系统的大部分版本，保证应用运行的兼容性，在Top50的设备中音频与视频能够成功播放出来。</p><p>完成了风险评估和基本的测试用例，我们的架构就比较完善了，下节课我们会去具体实现各个模块。</p><h2>思考题</h2><p>这节课我们一起分析了播放器的基本场景，然后进行了架构设计与模块拆分，但是基于这个播放器架构可以扩展成为更多场景，比如：直播播放器、视频编辑器、离线保存器等，所以这节课留给你的思考题就是：如果要让你实现一个视频编辑器，你会如何基于播放器的基础架构进行扩展呢？视频编辑器核心需求如下：</p><ul>\n<li>可以对视频画面进行处理，比如：增加字幕、添加贴纸、增加一些主题蒙版效果等；</li>\n<li>可以给视频增加BGM音轨，并且可以调整音量等效果。</li>\n</ul><p>无需描述具体实现，把基于播放器架构的改动描述清楚即可。欢迎你把自己的思考过程写在评论区，我们一起讨论，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"06｜移动平台的视频渲染（三）：OpenGL ES实操","id":547581},"right":{"article_title":"08｜播放器项目实战（二）：底层核心模块的实现","id":549471}}},{"article_id":549471,"article_title":"08｜播放器项目实战（二）：底层核心模块的实现","article_content":"<p>你好，我是展晓凯。今天我们来学习实现播放器中的各个核心模块。</p><p><a href=\"https://time.geekbang.org/column/article/548457\">上一节课</a>我们设计了播放器的架构，架构中包含各种模块，其中解码模块、音视频播放模块以及画面播放模块是架构中的核心模块。为了让我们设计的架构快速落地，这节课我会带你来分别实现这三个核心模块。我们先来看看解码模块是如何实现的吧。</p><h2>解码模块的实现</h2><p><img src=\"https://static001.geekbang.org/resource/image/e9/35/e945e0dde7cfe05c5242cba0f95d2735.png?wh=1920x1078\" alt=\"图片\"></p><p>我们一起来构建输入模块，也就是来做类图中的VideoDecoder类的实现。这里我们使用FFmpeg这个开源库来完成输入模块的协议解析、封装格式拆分、解码操作等行为，你可以看一下FFmpeg在解码场景下的核心流程。<br>\n<img src=\"https://static001.geekbang.org/resource/image/08/f7/08119d9fbbedd74c730a7c643eff61f7.png?wh=2626x1591\" alt=\"\" title=\"FFmpeg在解码场景下的核心流程\"></p><p>整体的运行流程分为以下几个阶段：</p><ul>\n<li>建立连接、准备资源阶段：使用openInput方法向外提供接口。</li>\n<li>读取数据进行拆封装、解码、处理数据阶段：使用decodeFrames方法向外提供接口。</li>\n<li>释放资源阶段：使用releaseResource方法向外提供接口。</li>\n</ul><p>以上就是我们输入端的整体流程，其中第二个阶段是一个循环并且会放在单独的线程中来运行。接下来我们具体看一下这个类中最重要的几个接口是如何设计与实现的。</p><h3>openInput</h3><p>这个方法的职责是建立与媒体资源的连接通道，并且分配一些全局需要用到的资源，最后将建立连接通道与分配资源的结果返回给调用端。这个方法的实现主要分为三个核心部分。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/b4/10/b4ce66b6a73a2b74742d57d14e585810.png?wh=1732x1204\" alt=\"图片\"></p><ol>\n<li>建立连接与找出流信息</li>\n</ol><p>第一步就是与媒体资源建立连接，成功建立连接之后，就可以找出这个资源所包含的流的信息，比如声音轨的声道数、采样率、表示格式或者视频轨的宽、高、fps等。如果是本地资源，这个过程会很快，如果是网络资源，可能就需要一段时间了。找出流信息失败的话可以重试，具体的重试逻辑可以根据不同的业务场景进行设置，在这里我们一般会进行重试3次的策略。</p><p>find_stream_info这个函数的内部会发生实际的解码行为，所以解码的数据越多，花费的时间也会越长，对应得到的MetaData也会越准确。对此，我们一般通过设置probesize和max_analyze_duration这两个参数来改善秒开的时间。</p><p>probesize代表探测数据量，max_analyze_duration代表最大的解析数据的长度。如果达到了设置的值，还没有解析出对应的视频流和音频流的MetaData，那么find_stream_info这个函数就会返回失败，紧接着业务层可以提高这两个参数的数值，进入下一次重试，如果find_stream_info解析成功，就会将对应的流信息填充到对应的结构体中。</p><ol start=\"2\">\n<li>根据流信息打开解码器</li>\n</ol><p>找出对应的流信息之后，就要打开每个流的解码器了。如果声音有两路流，有的播放器允许切换，比如我们之前说的ffplay就支持带入参数进行选择，VLC播放器可以实时切换。我们这个项目中只选择第一个音频流。</p><ol start=\"3\">\n<li>分配解码后的数据存储对象</li>\n</ol><p>每个流都要分配一个AVFrame的结构体，用来存放解码之后的原始数据。对于音频流，可能还要额外分配一个重采样的上下文，将解码之后的音频格式进行重采样，把它变成业务层需要的PCM格式，这里我们只分配资源，具体的解码和转换行为下节课我们会讲到。</p><p>这三个步骤完成之后，整个初始化阶段就完成了。但如果是一个网络资源的话，一般我们需要设置一个超时时间，否则在弱网情况下会出现一些函数阻塞时间特别长的问题。在FFmpeg的设计中，寻找流信息阶段，还有实际的read_frame等阶段，它们的内部会在另外一个线程调用开发者设置的超时判断的回调函数，询问是否达到超时的条件。返回1则代表超时，FFmpeg会断开连接通道，阻塞的函数也会直接返回；返回0则代表不超时，会继续阻塞进行IO操作。</p><p>这个机制要想生效，需要在建立连接通道之前，将自己定义的函数给AVFormatContext的interrupt_callback赋值。一般在销毁资源的时候，直接让这个函数返回0，或者为弱网也提供一个超时的读取时间，这个设置是非常有用的，它可以<strong>保证你的解码模块不会因为一些阻塞的IO调用变得不可用。</strong></p><h3>decodeFrames</h3><p>我们再看decodeFrames这个方法的实现，这个接口主要负责将音视频压缩数据解码成裸数据，然后经过处理封装成自定义的结构体，最后返回给调用端。</p><p><img src=\"https://static001.geekbang.org/resource/image/da/f6/da82d66abff545cab7dca9bbdc3c48f6.png?wh=1920x1079\" alt=\"图片\"></p><p>如图所示，先调用read_frame读出一个压缩数据帧来，压缩数据在FFmpeg中使用AVPacket这个结构体来表示。在视频流中，一个AVPacket里面只包含一个视频帧，在音频流中，一个AVPacket里面有可能包含多个音频帧，所以我们需要根据AVPacket的类型决定它所走的解码流程，这里我们可以根据AVPacket里面的stream_index来判定它是音频类型还是视频类型。</p><p>视频部分只需要解码一次，就可以得到AVFrame的视频裸数据（一般是YUV格式）。音频部分需要判定这个AVPacket里面的压缩数据是否被全部消耗干净了，并以此作为一次解码结束的条件。</p><p>解码成功之后，需要提取出对应的裸数据填充到我们自定义的结构体中，并返回给外界调用端。为什么需要填充到自定义的结构体中呢？</p><p>原因是我们不希望向外界暴露Input这个模块内部所使用的技术细节，也就是说，不希望向调用者暴露内部使用的是FFmpeg的解码器库还是硬件解码器或者其他的解码器库等细节，所以解码之后，需要封装成AudioFrame和VideoFrame的自定义结构体。但是在将AVFrame中的数据封装成自定义数据结构的过程中，有可能会出现AVFrame中音频或者视频的表示方式和调用端预期的表示方式不一样的情况，那么就需要做个转换。</p><p>FFmpeg对于音频和视频的格式转换，分别提供了不同的API供开发者使用。</p><p><strong>音频的格式转换，FFmpeg提供libswresample库让开发者使用，一般称为重采样。</strong>开发者需要用原始音频格式和目标音频格式，来调用swr_alloc_set_opts方法，初始化重采样的上下文，其中音频格式用声道、采样率、表示格式来表示。初始化成功之后，就可以调用swr_convert方法，将解码器输出的AVFrame转换为目标音频格式类型的AVFrame，重采样之后的数据就是开发者需要的音频格式的数据了。使用完毕之后调用swr_free方法来释放掉重采样的上下文。</p><p><strong>视频帧的格式转换，FFmpeg提供libswscale库让开发者使用。</strong>一般情况下如果原始视频的裸数据表示格式不是YUV420P的话，那么就需要使用这个库来把非YUV420P格式的视频数据转换为YUV420P格式。</p><p>转换过程也很简单，开发者需要先根据源格式（包括视频宽、高、表示格式）和目标格式（包括视频宽、高、表示格式）调用sws_getCachedContext方法，来构造出转换视频的上下文。构造成功之后，就可以调用sws_scale方法，将解码器输出的AVFrame转换为目标格式的AVPicture类型的结构体。然后开发者就可以从AVPicture里取出对应的数据，封装到自定义的结构体中。最终使用完毕后调用sws_freeContext来释放掉这个转换上下文。</p><h3>releaseResource</h3><p>最后是销毁资源这个方法的实现，它的过程正好与打开流阶段相反。第一步是销毁音频相关的资源，包括分配的AVFrame以及音频解码器，另外如果分配了重采样上下文以及重采样输出的音频缓冲buffer，那么也要一并销毁掉。然后是销毁视频相关的资源，包括分配的AVFrame与视频解码器，另外如果分配了格式转换上下文与转换后的AVPicture，也要一并销毁掉。最后断开连接通道以及销毁掉上下文。</p><h2>音频播放模块的实现</h2><p><img src=\"https://static001.geekbang.org/resource/image/ec/84/ece11c976d62f6333d768016829aea84.png?wh=1920x1077\" alt=\"图片\"></p><p>下面，我们来一起学习音频播放模块的实现，也就是类图中的AudioOutput类的实现，这一部分对于Android和iOS平台的实现是不同的。前面我们已经详细地学习了多种音频渲染的方法，基础概念与实现代码这里就不再赘述了，这个部分我们会将重点放在AudioOutput的接口设计与内部实现上。</p><h3>Android平台的音频渲染</h3><p>Android平台我们选用OpenSL ES来渲染音频，建立AudioOutput这个类之后，按照之前的架构设计，第一步需要在这个类里面定义一个回调函数，来获取要播放的PCM数据buffer，回调函数如下：</p><pre><code class=\"language-plain\">typedef int(*audioPlayerCallback)(byte* , size_t, void* ctx);\n</code></pre><p>这个函数的第一个参数是需要外界填充PCM数据的缓冲区，第二个参数是这个缓冲区的大小，第三个参数是客户端代码自己填充的上下文对象，在C++中的回调函数是静态的，所以要传递一个上下文对象，以便回调回来的时候可以将这个上下文对象强制转换成为目标对象，用于访问对象中的属性以及方法。</p><p>接下来，让我们具体实现AudioOutput这个类中的几个接口方法。面向对象的特性之一就是封装，也就是将类内部的具体实现细节封装起来，向外暴露出接口，用来完成调用端想要这个类完成的行为。所以这几个接口不能暴露AudioOutput内部到底是选用哪种技术框架来实现的。</p><p>尽管我们现在是使用OpenSL ES来实现的音频播放，如果以后有一些特殊需求，可以换成Oboe或者其他的实现方式，但是暴露给<strong>外界的接口以及回调函数是不会变</strong>的。这对于整个系统的扩展性以及后期的维护是非常重要的，前面的解码模块不向客户端代码暴露AVFrame，而是暴露自定义的VideoFrame或者AudioFrame结构体，也是一样的道理。那就让我们一起来实现第一个接口。</p><ol>\n<li><strong>初始化以及填充数据流程</strong></li>\n</ol><p>传入参数就是声道数、采样率、表示格式、回调函数以及回调函数的上下文对象，返回值表示OpenSL ES是否可以正常完成初始化。核心流程里有一步是给audioPlayerBufferQueue设置回调函数，也就是当OpenSL ES需要数据进行播放的时候，会回调这个函数，由开发者来填充PCM数据。我们需要在此处就调用上面定义的回调函数来填充PCM数据，最后调用audioPlayerBufferQueue的Enqueue方法，把客户端代码填充过来的PCM数据放到OpenSL ES中的BufferQueue中去。</p><ol start=\"2\">\n<li><strong>暂停和继续播放</strong></li>\n</ol><p>在上面一步初始化OpenSL ES的时候，我们已经把audioPlayerObject中的play接口给拿出来了，对于暂停和继续播放，我们只需要设置playState就可以了。</p><pre><code class=\"language-plain\">int state = play ? SL_PLAYSTATE_PLAYING : SL_PLAYSTATE_PAUSED;\n(*audioPlayerPlay)-&gt;SetPlayState(audioPlayerPlay, state);\n</code></pre><ol start=\"3\">\n<li><strong>停止以及销毁资源</strong></li>\n</ol><p>首先应该暂停现在的播放，接下来最重要的一步是设置一个全局的状态，保证如果再有audioPlayerBufferQueue的回调函数要调用的时候，不需要再填充数据，最好再调用usleep方法来暂停一段时间（比如50ms），这样可以让buffer缓冲区里面的数据全部播放完毕，最终调用OpenSL ES的API销毁所有的资源，包括audioPlayerObject与outputMixObject。</p><h3>iOS平台的音频渲染</h3><p>在iOS平台，我们选用AudioUnit（AUGraph实际上封装的就是AudioUnit）来渲染音频，类似于Android平台的实现，我们也要设计一个回调函数用来获取要渲染的PCM数据。</p><p>在OC中回调函数的实现一般是定义一个协议（Protocol），由调用端去实现这个协议，重写协议里面定义的方法，下面来看这个协议的定义：</p><pre><code class=\"language-plain\">@protocol FillDataDelegate &lt;NSObject&gt;\n  - (NSInteger) fillAudioData:(SInt16*) sampleBuffer numFrames:(NSInteger)frameNum numChannels:(NSInteger)channels;\n@end\n</code></pre><p>第一个参数是要填充的PCM缓冲区，第二个参数是这个缓冲区包含的音频帧数目，第三个参数是声道数。调用端在实现中要按照帧的个数和声道数规则来填充这个缓冲区。比起C++的回调函数，OC语言的这种写法更加面向对象一些，调用端实现这个协议就意味着要承担这个协议所要求的职责，你可以自己体会一下。</p><p>接下来是这个类的初始化方法，传入声道数、采样率、采样的表示格式，以及实现上面填充数据协议的对象（id<filledelegate> fillAudioDelegate）这几个参数。</filledelegate></p><p>在这个方法的实现中，首先要构造一个AVAudioSession，然后给这个Session设置用途类型以及采样率等；接下来设置音频被中断的监听器，方便应用程序在特殊情况下可以给出相应的处理；最后就是核心流程构造AUGraph，用来实现音频播放。</p><p>这里需要注意的是，在AUGraph中需要添加一个ConvertNode，将调用端填充的SInt16格式音频数据转换为Float32格式的音频数据，Float32格式是RemoteIONode可以播放出来的。最后需要给ConvertNode配置一个InputCallback，在这个InputCallback的实现中调用Delegate的fillAudioData方法，让调用端来填充数据。</p><p>配置好整个AUGraph之后，调用AUGraphInitialize方法来初始化整个AUGraph。最终构造出来的AUGraph以及和客户端代码的调用关系如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/ca/1ee332b6bb5f9c21519ca3550c31d7ca.png?wh=1920x753\" alt=\"图片\" title=\"AUGraph和客户端代码的调用关系\"></p><p>接下来是play方法的实现，直接调用AUGraphStart方法启动这个AUGraph就可以了。一旦启动，RemoteIO这个AudioUnit就会启动播放，需要音频数据的时候，就向前一级AudioUnit（ConvertNode）去拉取数据，而ConvertNode则会去找到自己的InputCallback，在InputCallback的实现中，会去和delagate（调用端）要数据，然后就在实现这个Protocol的客户端代码中填充数据，最终就可以播放出来了。</p><p>至于pause方法，直接调用AUGraphStop方法就可以停掉AUGraph的运行，音频就会停止渲染。如果想恢复的话，重新调用play方法即可。</p><p>最后是销毁方法，在销毁方法中要先停掉AUGraph，然后调用AUGraphClose方法关闭这个AUGraph，并移除掉AUGraph里面所有的Node，最终调用DisposeAUGraph，这样就可以彻底销毁整个AUGraph了。</p><h2>视频（画面）播放模块的实现</h2><p><img src=\"https://static001.geekbang.org/resource/image/e8/1b/e8b513a2518735913681867f0137d21b.png?wh=1920x1078\" alt=\"图片\"></p><p>接下来我们介绍视频（画面）播放模块的实现，也就是类图中的VideoOutput类的实现，这部分的实现也是依赖于平台的，虽然底层都是使用的OpenGL ES，但是需要为不同平台构建自己的上下文环境以及窗口管理，具体细节我们<a href=\"https://time.geekbang.org/column/article/546501\">第5节课</a>已经学过了，所以这节课我们会把重点放在讲解VideoOutput的接口设计与内部实现上。</p><h3>Android平台的视频渲染</h3><p>根据我们之前的学习，无论在哪个平台使用OpenGL ES渲染视频画面，都需要单独开辟一个线程，并且为这个线程绑定一个OpenGL ES的上下文。</p><p>在Android平台上我们是在Native层进行OpenGL ES的开发工作的。第一件事就是选用线程模型，前面我们曾一起分析过各种线程模型的优缺点，这里就直接选用POSIX线程模型，即PThread。</p><p><span class=\"reference\">音视频同步模块中其实不会涉及任何平台相关的API，不过考虑到它要维护解码线程，因此使用PThread来创建线程会是一个好的选择，原因是两个平台都支持这种线程模型。——第7节课内容回顾</span></p><p>在VideoOutput中也是先定义一个回调函数，当VideoOutput这个模块需要渲染视频帧的时候，就调用这个回调函数拿到要渲染的视频帧进行真正的渲染，回调函数的代码原型如下：</p><pre><code class=\"language-plain\">typedef int (*getTextureCallback)(VideoFrame** texture, void* ctx);\n</code></pre><p>第一个参数是要获取的视频帧，第二个是回调函数的上下文，返回值为int类型，当成功获取到一帧视频帧后返回大于零的值，否则返回负值。初始化函数的原型如下：</p><pre><code class=\"language-plain\">bool initOutput(ANativeWindow* window, int screenWidth, int screenHeight, ITextureFrameCallback *pTextureFrameUploader,  void* ctx);\n</code></pre><p>第一个参数是ANativeWindow类型的对象，这个window实际上是通过从Java层传递过来的一个Surface构造的，而Surface就是从SurfaceView的SurfaceHolder中获取出来的。第二个和第三个参数则是绘制的View的宽和高，后面的参数就是获取视频帧的回调函数以及回调函数的上下文对象。</p><p>初始化函数的实现也比较简单，首先创建一个线程作为OpenGL ES的渲染线程，线程执行的第一个步骤就是初始化OpenGL ES环境，它会利用EGL构建出OpenGL ES的上下文，并且利用ANativeWindow构造出EGLDisplay作为显示目标，然后利用VertexShader和FragmentShader构造出一个GLProgram。</p><p>VertexShader代码如下：</p><pre><code class=\"language-plain\">static char* OUTPUT_VIEW_VERTEX_SHADER =\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"attribute vec4 vPosition;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"attribute vec4 vTexCords;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"varying vec2 yuvTexCoords;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"void main() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; yuvTexCoords = vTexCords.xy;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; gl_Position = vPosition;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\n\";\n</code></pre><p>VertexShader中直接将顶点赋值给gl_Position，然后将纹理坐标传递给FragmentShader，具体的FragmentShader代码如下：</p><pre><code class=\"language-plain\">static char* YUV_FRAME_FRAGMENT_SHADER =\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"varying highp vec2 yuvTexCoords;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"uniform sampler2D s_texture_y;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"uniform sampler2D s_texture_u;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"uniform sampler2D s_texture_v;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"void main(void)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; highp float y = texture2D(s_texture_y, yuvTexCoords).r;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; highp float u = texture2D(s_texture_u, yuvTexCoords).r - 0.5;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; highp float v = texture2D(s_texture_v, yuvTexCoords).r - 0.5;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; highp float r = y + 1.402 * v;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; highp float g = y - 0.344 * u - 0.714 * v;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; highp float b = y + 1.772 * u;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"&nbsp;&nbsp;&nbsp; gl_FragColor = vec4(r,g,b,1.0);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\";\n</code></pre><p>由于视频帧是YUV420P的数据格式表示的，所以在FragmentShader里面，需要把YUV的数据转换为RGBA格式的数据。先取出对应的YUV的数据，然后按照YUV到RGBA的计算公式将YUV格式转换为RGBA格式。由于UV的默认值是127，所以我们这里要减去0.5。在OpenGL ES的Shader中会把内存里0～255的整数数值换算为0.0-1.0的浮点数值。</p><p>接下来就是渲染方法，当客户端代码需要VideoOutput渲染视频帧的时候，VideoOutput模块会先利用回调函数获得视频帧，然后利用构造的GLProgram执行渲染操作，最终调用eglSwapBuffers方法将渲染的内容绘制到EGLDisplay上面去。</p><p>最后是销毁方法，这一步必须要在GL线程中进行，因为是在这个线程中创建的OpenGL上下文、EGLDisplay、GLProgram，渲染过程中还使用到了纹理对象、frameBuffer对象，所以必须在这个线程中来销毁这一系列的对象。</p><h3>iOS平台的视频渲染</h3><p><a href=\"http://time.geekbang.org/column/article/547581\">第6节课</a>我们已经学习过如何在iOS平台上使用OpenGL ES了，所以在接下来的实现中，我们书写一个继承自UIView的VideoOutput类，并重写父类的layerClass这个方法，返回CAEAGLLayer类型；然后在初始化方法中创建OpenGL 线程。线程模型我们采用NSOperationQueue来实现，这里我们会把所有OpenGL ES的操作都封装在NSOperationQueue中完成。</p><p>为什么要使用这种线程模型呢？由于一些低端设备执行一次OpenGL的绘制耗费的时间可能比较长，如果使用GCD的线程模型的话，就有可能导致DispatchQueue里面的绘制操作累积得越来越多，并且不能清空。如果使用NSOperationQueue的话，可以在检测到这个Queue里面的Operation的数量，当超过定义的阈值（Threshold）时，就会清空老的Operation，只保留最新的绘制操作。</p><p>iOS平台规定：App进入后台之后，就不可以再进行OpenGL ES的渲染操作。所以这里我们需要注册两个监听事件。</p><ul>\n<li>WillResignActiveNotification，当App从活跃状态转到非活跃状态的时候，或者说即将进入后台的时候系统会发出这个事件。</li>\n<li>DidBecomeActiveNotification，当App从后台到前台时系统会发出这个事件。</li>\n</ul><p>我们分别为这两个事件注册回调方法，然后设置一个全局变量enableOpenGLRendererFlag，在进入后台的监听事件中把它设置成NO，再回到前台的监听事件中，把它设置成YES。在OpenGL ES绘制过程中，应该先判定这个变量是否为YES，是YES就进行绘制，否则不进行绘制。</p><p>接下来我们看一下初始化方法的实现，首先给layer设置属性，然后初始化NSOperationQueue，并且直接将OpenGL ES的上下文以及GLProgram的构建作为一个Block（代码块）扔到这个Queue中。</p><p>这个Block中会先分配一个EAGLContext，然后为这个NSOperationQueue线程绑定这个刚创建好的上下文，然后创建frameBuffer和renderBuffer，并且把这个UIView的layer设置为renderBuffer的storage，再将frameBuffer和renderBuffer绑定起来，这样绘制过程中绘制到frameBuffer上的内容就相当于绘制到了renderBuffer上。最好使用前面提到的VertexShader和FragmentShader构造出GLProgram。</p><p>下面就到了关键的渲染方法，在发起绘制之前，我们需要判断当前OperationQueue里面的operation的数目，如果大于规定的阈值（一般为2或者3），就说明每一次绘制花费的时间较多，导致渲染队列积攒的数量越来越多了，我们应该删除最久的绘制操作，只保留与阈值个数对应的绘制操作数量，然后将本次绘制操作加入到绘制队列中。</p><p>由于在初始化的过程中已经给这个线程绑定了OpenGL ES的上下文，所以可以在这个线程中直接进行OpenGL ES的渲染操作。在绘制开始时，判定布尔型变量enableOpenGLRendererFlag的值，如果是YES，就绑定frameBuffer，然后使用GLProgram进行绘制，最后绑定renderBuffer，并调用EAGLContext的PresentRenderBuffer，将刚刚绘制的内容显示到layer上去，最终用户就可以在UIView中看到我们刚刚绘制的内容了。</p><p>最后是销毁方法，由于所有涉及OpenGL ES的操作都要放到绑定了上下文环境的线程中去操作，所以这个方法中对OpenGL ES的操作也要保证放到OperationQueue中去执行。在具体实现中，首先要把GLProgram释放掉，然后把frameBuffer和renderBuffer释放掉，最后解除本线程与OpenGL上下文之间的绑定。</p><p>UIView的dealloc方法主要负责回收所有的资源，首先移除所有的监听事件，然后清空OperationQueue里面未执行的操作，最后释放所有的资源。到这里这个VideoOutput就实现完毕了。</p><h2>小结</h2><p>最后，我们来整体回顾一下这节课的内容吧！</p><p>视频播放器几个底层模块的实现中，解码模块内部使用FFmpeg实现，但是对外界会隐藏内部的实现，提供统一的封装接口。音频播放模块也会隐藏内部的实现，提供一个获取PCM数据的方法来拉取要播放的数据。画面播放模块直接获取出YUV数据使用OpenGL ES进行绘制，需要注意的是所有OpenGL ES的操作必须要在构建的GL线程中进行操作。</p><p><img src=\"https://static001.geekbang.org/resource/image/5b/33/5bbbae5f2c12a60f1d6144c20a062b33.png?wh=1920x984\" alt=\"图片\"></p><p>这三个模块是视频播放器最底层的三个模块，而如何把这三个模块有机地结合起来让视频播放器流畅地运转起来，还需要我们一起用音视频同步模块以及中控模块把他们串联起来的，下节课我们会继续学习这两个模块的实现。</p><h2>思考题</h2><p>在解码模块中，为了方便你理解和掌握视频播放器流程，我把demux和decoder放到了一个线程中，但是这两个部分其实消耗的资源是不同的，demux是IO密集型的操作，decoder是CPU密集型的操作，所以如果让你去优化这部分的架构设计，应该如何做呢？</p><p>欢迎你把你的答案留在评论区，和我一起讨论，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"07｜播放器项目实战（一）：场景分析与架构设计","id":548457},"right":{"article_title":"09｜播放器项目实践（三）：让你的播放器跑起来","id":550165}}},{"article_id":550165,"article_title":"09｜播放器项目实践（三）：让你的播放器跑起来","article_content":"<p>你好，我是展晓凯。今天我们进入播放器项目实战最后一部分的学习。</p><p>前面两节课，我们从播放器的场景入手设计出了播放器的架构，然后一起实现了播放器的底层核心模块，包括音频渲染模块、视频渲染模块与视频解码模块。这节课我们要把这些模块串联在一起，让我们的播放器运行起来。</p><p>用播放器播放视频最重要的一点就是要保证<strong>音画对齐</strong>，架构设计中的AVSync模块就承担这一职责。</p><h2>AVSync模块的实现</h2><p><img src=\"https://static001.geekbang.org/resource/image/6d/4e/6d09fac5e835b1a6115cc560689aff4e.png?wh=1920x1077\" alt=\"图片\"></p><p>AVSync模块除了负责音视频的同步之外，还要维护一个解码线程，主要工作就是线程的创建、暂停、运行、销毁，就是我们架构类图中AVSynchronizer这个类。</p><p>这个类的实现分为两部分，第一部分是维护解码线程，第二部分就是音视频同步。主要接口与实现有以下四个。</p><ul>\n<li>提供<strong>初始化接口</strong>，内部实现为：使用外界传递过来的URI去实例化解码器模块，实例化成功之后，创建音频队列与视频队列，并且创建解码线程，将音视频解码后的数据放入队列中；</li>\n<li>提供<strong>获取音频数据接口</strong>，内部实现为：如果音频队列中有音频就直接去返回，同时要记录下这个音频帧的时间戳，如果音频队列中没有音频就返回静音数据；</li>\n<li>提供<strong>获取视频帧接口</strong>，内部实现为：返回与当前播放的音频帧时间戳对齐的视频帧。</li>\n<li>提供<strong>销毁接口</strong>，内部实现为：先停掉解码线程，然后销毁解码器，最后再销毁音视频队列。</li>\n</ul><!-- [[[read_end]]] --><h3>维护解码线程</h3><p>AVSync模块创建的解码线程扮演了<strong>生产者的角色</strong>，生产出来的数据根据类型分别存放到音频队列和视频队列中。而AVSync模块向外暴露的获取音频数据和视频帧的方法，扮演了消费者的角色，消费者会从音视频队列里面获取数据，这是一个标准的生产者-消费者模型。</p><p>由于是在Native层来维护线程，所以我们选用POSIX线程模型来创建一个解码线程。创建成功之后就让解码线程运行起来，解码音频帧和视频帧。解码出来的音视频帧封装为我们自定义的结构体AudioFrame和VideoFrame，然后把它们分别放入音视频队列中。解码线程内部代码如下：</p><pre><code class=\"language-plain\">while(isOnDecoding) {\n    isDecodingFrames = true;\n&nbsp;&nbsp;&nbsp; decodeFrames();\n    isDecodingFrames = false;\n    pthread_mutex_lock(&amp;videoDecoderLock);\n    pthread_cond_wait(&amp;videoDecoderCondition, &amp;videoDecoderLock);\n    pthread_mutex_unlock(&amp;videoDecoderLock);\n}\n</code></pre><p>上述代码表示，只要我们不销毁这个模块（销毁的时候会把isOnDecoding设置为false），就会一直在这个循环中。这个循环内部有一个条件锁，每执行一次循环之后，就会停在Wait的地方，等待Signal指令过来，才可以进行下一次解码操作，为什么要这样安排呢？</p><p>原因有两点，一是如果全部解码出来，内存中是放不下的，尤其是视频，占用内存太大了；二是用户随时可能停止播放，我们解码出来的音视频帧就都没用了，白白浪费了CPU和带宽资源。所以后台解码线程没必要一股脑把视频全部解码并放入队列中。</p><p>那decodeFrames方法执行结束之后就要进行Wait，那执行这个方法的条件是什么呢？</p><p>我们设置一个变量名字叫做max_bufferDuration，值设置为0.2s。每一次调用decodeFrames这个方法，都会将两个队列填充到max_bufferDuration的刻度之上，这个方法执行结束之后，解码线程就在Wait处等待Signal指令。那什么时候会得到Signal指令呢？</p><p>我们设置一个变量名字叫做min_bufferDuration，值设置为0.1s。消费者每一次消费数据之后，我们要判断队列里面的音视频缓冲长度是否在min_bufferDuration刻度以下，如果在这个刻度以下，就发送Signal指令，让生产者线程继续生产数据。</p><pre><code class=\"language-plain\">bool isBufferedDurationDecreasedToMin = bufferedDuration &lt;= minBufferedDuration;\nif (isBufferedDurationDecreasedToMin &amp;&amp; !isDecodingFrames) {\n&nbsp;&nbsp;&nbsp; int getLockCode = pthread_mutex_lock(&amp;videoDecoderLock);\n&nbsp;&nbsp;&nbsp; pthread_cond_signal(&amp;videoDecoderCondition);\n&nbsp;&nbsp;&nbsp; pthread_mutex_unlock(&amp;videoDecoderLock);\n}\n</code></pre><p>当生产线程收到Signal指令之后，就会进入下一轮的解码。伴随着生产者线程和消费者线程的协同工作，整个视频播放器也就运行起来了。</p><p>还需要注意的一点就是，<strong>在销毁这个模块的时候，需要先把isOnDecoding这个变量设置为false，然后再发送一次Signal指令，让解码线程有机会结束</strong>。否则，解码线程就有可能一直在这里等待，成为一个僵尸线程。</p><p>到这里，解码线程的工作模式和运行原理我们就学完了，下面我们来看AVSync模块的第二部分——音视频同步的设计逻辑。</p><h3>音视频同步</h3><p>音视频同步的策略一般分为三种：音频向视频同步、视频向音频同步，以及音频视频统一向外部时钟同步，在使用ffplay播放视频文件的时候，所指定的对齐方式就是上面所说的三种方式，下面我们来逐一分析一下，这三种对齐方式是如何实现的，以及各自的优缺点。</p><p><img src=\"https://static001.geekbang.org/resource/image/4b/bf/4bffcf5254f99400492cff6ecaa85fbf.png?wh=1920x639\" alt=\"图片\"></p><ol>\n<li>音频向视频同步</li>\n</ol><p>音频向视频同步，顾名思义，就是视频会维持一定的刷新频率，或者说根据渲染视频帧的时长来决定当前视频帧的渲染时长。而当我们向AudioOutput模块填充音频数据的时候，会和当前渲染的视频帧的时间戳进行比较。这个差值如果在阈值范围之内，就可以直接将这一帧音频帧填充给AudioOutput模块，让用户听到这个声音，如果不在阈值范围内，就需要做对齐操作。</p><p>如何做对齐操作呢？这就需要我们去调整音频帧了，也就是说，如果要填充的音频帧的时间戳比当前渲染的视频帧的时间戳小，那就需要跳帧操作，具体的跳帧操作可以是加快播放速度，也可以是丢弃掉一部分音频帧的实现。</p><p>如果音频帧的时间戳比当前渲染的视频帧的时间戳大，那么就需要等待，具体实现有两种，我们可以将音频的速度放慢，播放给用户听，也可以填充空数据给AudioOutput模块，进行播放。一旦视频的时间戳赶上了音频的时间戳，就可以将本帧音频帧的数据填充给AudioOutput模块了。</p><p>这种实现的优点就是视频可以每一帧都播放给用户看，画面可以说是最流畅的。但音频就会有丢帧或者插入静音帧的情况。所以这种对齐方式也会有一个比较大的缺点，就是音频有可能会加速或者跳变，也有可能会有静音数据或慢速播放。如果使用变速手段来实现，并且变速系数不太大的话，用户感知可能不太强，但是如果使用丢帧或者插入空数据来实现，用户的耳朵是可以明显感觉到卡顿的。</p><ol start=\"2\">\n<li>视频向音频同步</li>\n</ol><p>由于不论是哪个平台播放音频的引擎，都可以保证播放音频是线性的，即播放音频的时间长度与实际这段音频所代表的时间长度是一致的。由于音频线性渲染这一特性，当客户端代码跟我们要视频帧的时候，就会先计算出当前视频队列头部视频帧元素和当前音频播放帧时间戳的差值。</p><p>如果在阈值范围之内，就可以渲染这一帧视频帧；如果不在阈值范围之内的话就要进行对齐操作。如果当前队列头部视频帧的时间戳小于当前播放音频帧的时间戳的话，就进行跳帧操作；如果大于当前播放音频帧的时间戳，就等待一会儿（重复渲染上一帧或者不进行重复渲染）。这种对齐方式的优点是音频可以连续地播放，缺点就是视频有可能会跳帧，也有可能会重复播放，不会那么流畅。但是用户的眼睛是不太容易分辨轻微的丢帧和跳帧现象的。</p><ol start=\"3\">\n<li>统一向外部时钟同步</li>\n</ol><p>这种策略其实更像是上述两种对齐方式的合体，实现就是单独在外部维护一轨时钟，我们要保证这个外部时钟的更新是按照时间慢慢增加的，而我们获取音频数据和视频帧的时候，都要和这个外部时钟对齐。如果没有超过阈值，那么就直接渲染，如果超过阈值了我们就要进行对齐操作。</p><p>具体的对齐操作就是，使用上述两种方式里的对齐操作，用这些方式分别对齐音频和视频。优点是可以最大程度地保证音视频都不跳帧，缺点是如果控制不好外部时钟，极有可能出现音频和视频都跳帧的情况。</p><p>有研究表明，人的耳朵比眼睛要敏感得多，也就是说，一旦音频跳帧或者填充空数据，我们的耳朵是十分容易察觉到的，而视频有跳帧或者重复渲染的行为，我们的眼睛其实不太容易分辨出来，所以我们实现的播放器中就<strong>选用视频向音频对齐的方式</strong>。到代码实现阶段，音视频同步这部分的逻辑放到获取视频帧的方法里面就可以了。</p><p>到这里我们一起回忆一下，我们使用AVSync模块将解码模块包装了起来，而音频与视频渲染两个模块也已经准备好了，接下来我们需要书写一个中控模块，将这三个模块有机地串联起来，让我们的播放器可以跑起来。</p><h2>中控模块</h2><p><img src=\"https://static001.geekbang.org/resource/image/92/63/92b3cfed65850ff71a0a7cfb63c39463.png?wh=1920x1079\" alt=\"图片\"></p><p>中控模块就是架构类图中VideoPlayerController这个类，这个类就是将上面提到的各个模块有序地组织起来，让单独运行的各个模块可以协同配合起来。由于每个模块都有各自的线程在运行，所以这个类需要维护好各个模块的生命周期，否则容易产生多线程的问题。我会将这个类拆分为初始化、运行、销毁三个阶段给你讲解。</p><h3>初始化阶段</h3><p>虽然我们的项目叫做“视频”播放器，但即使客户端代码没提供渲染的View，播放器也应该能够播放出声音来，可以单独播放音频。在某些场景下是一个比较有用的功能，比如可以加速秒开，可以做一些画中画的功能。要想达到这样的目标，我们需要把播放器的初始化和渲染界面的初始化分开。所以，初始化阶段可以分为两部分，一部分是播放器的初始化，另外一部分是渲染界面的初始化。</p><p>首先我们来看播放器的初始化，因为在初始化的过程中需要和资源建立连接，并执行I/O操作，所以这里必须要开辟一个线程来做初始化操作，即利用PThread创建一个InitThread来执行。在这个线程中，先实例化AVSynchronizer这个对象，然后调用这个对象的init方法，来建立和媒体资源的连接通道。</p><p>如果打开连接失败，那么回调客户端打开资源失败；如果打开连接成功，就拿出音频流信息（Channel、SampleRate、SampleFormat）来初始化AudioOutput，并把这个对象注册给AudioOutput，用来提供音频数据。AudioOutput初始化成功后，就直接调用AVSync模块的start方法，以及AudioOutput的播放方法，并且将初始化成功回调给客户端，至此我们的播放器就可以正常地播放音频了。</p><p>接下来是渲染界面初始化的阶段，如果业务层在某个时机可以显示视频的画面了，那么就让Surface（Texture）View显示，按照Surface（Texture）View的生命周期，调用端会拿着Surface调用到JNI层，JNI层会把Surface构建成ANativeWindow，然后调用中控系统的initVideoOutput方法。这个方法内部用传递进来的ANativeWindow对象和界面的宽、高以及获取视频帧的回调函数来初始化VideoOutput对象。</p><p>通过以上两个步骤，中控系统就将AVSync、AudioOutput、VideoOutput连接起来了，接下来是运行阶段。</p><h3></h3><h3>运行阶段</h3><p>运行阶段我们也分为两个步骤，一是音频的渲染，二是视频的渲染，先来看音频渲染。由于在初始化阶段，我们已经通过调用AudioOutput的start方法开启了音频输出模块，所以当AudioOutput模块将自己缓冲区里面的音频数据播放完毕之后，就会立马通过回调方法让我们的中控模块填充音频数据，那中控模块填充音频数据的方法是如何实现的呢？</p><p>这次我们从结果出发来看这个方法的实现。</p><ol>\n<li>填充静音数据，当以下情况出现的时候，要填充静音数据给AudioOutput进行播放。</li>\n</ol><ul>\n<li>播放器状态是暂停状态；</li>\n<li>AVSync中的音频队列已经空了；</li>\n<li>AVSync已经被销毁了或者解码完毕了。</li>\n</ul><ol start=\"2\">\n<li>填充真实数据，当以上情况都不满足的情况下，就去AVSync模块中获取音频数据，等填充了音频数据之后，要做两件事情。</li>\n</ol><ul>\n<li>更新当前播放的时间戳，用于做后续的音画同步；</li>\n<li>给VideoOutput（视频输出模块）发送一个指令，让VideoOutput模块更新视频帧。</li>\n</ul><p>VideoOutput模块接收到这个指令之后，就可以调用回调方法来获取一帧视频帧，由于在初始化的时候，已经把中控系统注册给VideoOutput用来获取视频帧了，所以这里会调用中控模块获取视频帧的方法。这个方法会调用AVSync模块，来获取一个与音频匹配的视频帧，然后返回给视频播放模块，将最新的一帧视频帧更新到画面中。</p><p>运行阶段还有暂停和继续播放接口的实现，由于当前实现整个播放器是由音频播放模块来驱动的，所以只需要让音频播放模块暂停和继续就好了。</p><p>经过学习运行阶段的内容，我们知道整个视频播放的过程其实是由音频来驱动的，所以在销毁阶段要先停掉音频。首先调用AudioOutput对象的stop方法；然后要停掉AVSync模块，由于这个模块内部组合了输入模块，所以要把输入模块的连接给断开，输入模块中利用FFmpeg的超时设置可以快速断开连接，然后需要使用pthread_join这个排程的方法，等待解码线程运行结束，再把音频队列、视频队列以及解码器都给销毁掉。</p><p><img src=\"https://static001.geekbang.org/resource/image/ea/4e/ea348ffaeb66b9ec6a254d1cbd8a1f4e.png?wh=1846x584\" alt=\"图片\"></p><p>最后一步是停止VideoOutput模块，通过调用VideoOutput的销毁资源方法（里面会销毁frameBuffer、renderbuffer、Program等）来实现，最后再调用音频输出模块的销毁方法。这样就可以销毁所有的模块了。</p><h3>小结</h3><p><img src=\"https://static001.geekbang.org/resource/image/10/50/105d67db1b5739fe753b0086448ba750.png?wh=1920x998\" alt=\"图片\"></p><p>视频播放器项目我们已经实现完了，因为这个实战项目比较大，所以我们一起来回顾一下整个设计与开发阶段。我们从<a href=\"http://https://time.geekbang.org/column/article/548457\">第7节课</a>开始，一步步设计并实现这个播放器。</p><ul>\n<li>首先实现了输入模块，也叫做解码模块，输出的音频帧是AudioFrame，里面的主要数据就是PCM裸数据，输出的视频帧是VideoFrame，里面的主要数据就是YUV420P的裸数据。</li>\n<li>然后实现了音频播放模块，输入就是我们解码出来的AudioFrame，是SInt16表示一个sample格式的数据，输出就是输出到Speaker让用户直接听到声音。</li>\n<li>接着实现了视频播放模块，输入就是解码出来的VideoFrame，里面存放的是YUV420P格式的数据，在渲染过程中使用OpenGL ES的Program将YUV格式的数据转换为RGBA格式的数据，并最终显示到物理屏幕上。</li>\n<li>之后就是音视频同步模块了，它的工作主要由两部分组成，第一是负责维护解码线程，即负责输入模块的管理；另外一个就是音视频同步，向外部暴露填充音频数据的接口和获取视频帧的接口，保证提供出去的数据是同步的。</li>\n</ul><p>最后书写一个中控系统，负责将AVSync模块、AudioOutput模块、VideoOutput模块组织起来，最重要的就是维护这几个模块的生命周期，由于这里面存在多线程的问题，所以比较重要的就是在初始化、运行、销毁的各个阶段保证这几个模块可以协同有序地运行，同时中控系统向外暴露用户可以操作的接口，比如开始播放、暂停、继续、停止等接口。</p><p>整个播放器项目比较复杂，现在再回看<a href=\"https://time.geekbang.org/column/article/548457\">第7节课</a>的场景分析和架构设计，你是不是有了更清晰、更深刻的理解呢？</p><h2>思考题</h2><p>在这节课的实践中，VideoOutput渲染视频帧是用AudioOutput的运行来驱动的，但是每个平台渲染音频帧的间隔是不同的，如果遇到渲染音频帧间隔较大的设备，那么视频帧渲染就会出现掉帧的情况，那应该如何解决呢？</p><p>欢迎你把你的答案留在评论区，和我一起讨论，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"08｜播放器项目实战（二）：底层核心模块的实现","id":549471},"right":{"article_title":"10｜iOS平台音频采集：如何使用AudioQueue和AudioUnit采集音频？","id":551141}}},{"article_id":551141,"article_title":"10｜iOS平台音频采集：如何使用AudioQueue和AudioUnit采集音频？","article_content":"<p>你好，我是展晓凯。今天我们来一起学习iOS平台的音频采集。</p><p>iOS平台提供了多套API来采集音频，分别是AVAudioRecoder，AudioQueue以及AudioUnit。这三种方法各有优缺点，适用于不同的场景，我们一起看一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/76/5d/76f28e48a0bc736af8b03b26b3a0225d.png?wh=1920x587\" alt=\"图片\"></p><ul>\n<li>AVAudioRecorder，类似于AVAudioPlayer，属于端到端的API，存在于AVFoundation框架中。当我们想指定一个路径将麦克风的声音录制下来的时候，就可以使用这一个API。优点是简单易用，缺点是无法操控中间的数据。</li>\n<li>AudioQueue，之前我们使用AudioQueue渲染过音频，其实AudioQueue也可以录制音频，也是对AudioUnit的封装，它允许开发者获取、操控中间的数据<span class=\"reference\">（按照配置的数据格式）</span>。优点是灵活性较强，缺点是上手难度较高。</li>\n<li>AudioUnit，是音频最底层的API接口，之前我们使用AudioUnit渲染过音频，和AudioQueue一样，我们也可以使用它录制音频。当我们需要使用VPIO<span class=\"reference\">（VoiceProcessIO）</span>等处理音频的AudioUnit、需要使用实时耳返或在低延迟场景下，必须使用这一层的API。优点是灵活性最强，缺点是上手难度更高。</li>\n</ul><!-- [[[read_end]]] --><p>这节课我会重点讲解如何使用AudioQueue与AudioUnit来采集音频，但在学习这两个接口之前，我们还需要先设置并激活一下录制场景下AVAudioSession。</p><h2>设置AVAudioSession</h2><p>要想使用iOS的麦克风录音，首先要为App声明使用麦克风的权限，在工程目录下找到info.plist，然后在里面新增麦克风权限的声明。</p><pre><code class=\"language-plain\">&lt;key&gt;NSMicrophoneUsageDescription&lt;/key&gt;\n&lt;string&gt;microphoneDesciption&lt;/string&gt;\n</code></pre><p>这样添加完之后，就让系统知道了App要访问系统的麦克风权限。</p><p>接下来需要判断一下麦克风的授权情况。如果已经询问过了，就根据实际授权的情况进行处理，如果未授权可以引导用户跳转设置页面重新打开，代码如下：</p><pre><code class=\"language-plain\">[[UIApplication sharedApplication] openURL:[NSURL URLWithString:UIApplicationOpenSettingsURLString] options:@{} completionHandler:^(BOOL success) {\n}];\n</code></pre><p>如果没有询问过用户是否授权，就调用询问接口。</p><pre><code class=\"language-plain\">[[AVAudioSession sharedInstance] requestRecordPermission:^(BOOL granted) {\n  if (granted) {\n    //获得授权 &nbsp; &nbsp; &nbsp;\n  } else {\n    //未获得授权&nbsp; &nbsp;\n  }\n}];\n</code></pre><p>根据实际获得授权的情况，来决定是否继续开启录音流程。</p><p>当获得授权之后，就要开启一个音频会话了，即设置对应的AVAudioSession，代码如下：</p><pre><code class=\"language-plain\">[[AVAudioSession sharedInstance] setPreferredIOBufferDuration:AUDIO_RECORD_BUFFER_DURATION error:&amp;error];\n[[AVAudioSession sharedInstance] setPreferredSampleRate:48000 error:nil];\n[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayAndRecord&nbsp; error:nil];\n[[AVAudioSession sharedInstance] setActive:YES error:nil];\n</code></pre><p>代码的第一行是设置缓冲区的大小，一般设置得越大延迟越高，但是性能越好。如果需要实时耳返的话，一般设置5～8ms；不需要实时耳返的场景，设置23ms左右即可。第二行是设置采样率，目前的主流设备可以都设置成48k的采样率，兼容性是最好的；第三行是设置Category，如果是不戴耳机或戴有线耳机的情况，根据上面的代码设置就可以，但是如果是蓝牙耳机想保留高音质采集，就需要这样设置：</p><pre><code class=\"language-plain\">[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayAndRecord withOptions:AVAudioSessionCategoryOptionAllowBluetoothA2DP error:nil];\n</code></pre><p>这里的麦克风使用的是机身麦克风，如果想使用蓝牙本身的麦克风，设置代码如下：</p><pre><code class=\"language-plain\">[[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayAndRecord&nbsp; withOptions:AVAudioSessionCategoryOptionAllowBluetooth error:nil];\n</code></pre><p>最后，激活AVAudioSession之后，就可以使用具体的录音框架了，下面就让我们先来学习一下用AudioQueue来采集音频。</p><h2>如何使用AudioQueue采集音频？</h2><p>还记得我们<a href=\"https://time.geekbang.org/column/article/543649\">第1节课</a>使用AudioQueue来渲染音频的方法吗？和渲染音频相比，采集音频就相当于是一个相反的过程，我们先来看一下AudioQueue采集音频的结构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/10/6f/10f88a7a26ef4b3bd0f691296c53f76f.png?wh=1920x782\" alt=\"图片\" title=\"AudioQueue采集音频结构图\"></p><p>从结构图里，我们可以看到AudioQueue的输入是左侧的麦克风，输出是把BufferQueue里面填充好数据的buffer回调给业务层，让业务层自己处理或者执行IO操作。</p><p>下面我们再看一下AudioQueue录音的整个流程图。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/73/45a9cb9ac4bbb1d2ffd66b49b1cebd73.png?wh=1920x2035\" alt=\"图片\" title=\"AudioQueue录音流程图\"></p><ol>\n<li>AudioQueue把采集进来的音频填充到第一个buffer里。</li>\n<li>填满了第一个buffer之后，会把第一个buffer返回给业务层处理，同时把采集进来的音频填充到第二个buffer里。</li>\n<li>业务层把接收到的第一个buffer写到磁盘里。</li>\n<li>业务层把第一个buffer再返还给AudioQueue，也就是重新让buffer入队。</li>\n<li>AudioQueue填充好了第二个buffer回调给业务端，同时把麦克风采集到的数据写到第三个buffer里。</li>\n<li>业务层把第二个buffer写到磁盘上，同时把第二个buffer再返还给AudioQueue。</li>\n</ol><p>你可以再回顾一下<a href=\"https://time.geekbang.org/column/article/543649\">AudioQueue渲染音频</a>的数据流程，录制的数据运行流程恰恰是和音频渲染相反的一个过程：<strong>渲染音频是需要业务端来填充数据，然后给AudioQueue进行播放；采集音频是需要业务端把采集到的数据消费掉，然后再返回给AudioQueue来填充音频。</strong></p><p>在学习AudioQueue音频渲染的时候，我们了解了AudioQueue内部是可以自动做解码操作的，其实在采集音频过程中也有一个逆过程的处理，就是AudioQueue内部会根据我们指定的数据格式来执行压缩编码，你可以看一下流程图。</p><p><img src=\"https://static001.geekbang.org/resource/image/81/3f/81a4e7e86fe0d97ed67eb3yyf5556b3f.png?wh=1920x918\" alt=\"图片\"></p><p>在创建采集音频类型的AudioQueue的时候，要指定dataFormat<span class=\"reference\">（ASBD类型）</span>参数，而这个DataFormat里有一个mFormatID属性，当这个属性被指定为kAudioFormatMPEG4AAC的时候，AudioQueue内部就会自动把采集到的PCM数据编码为AAC类型的数据返回给业务层。</p><p>在实际使用过程中，你可以根据自己的需要来制定dataFormat，如果你需要PCM数据，那么这里就不需要使用AudioQueue的编码能力，我们要根据自己的使用场景来做配置。</p><p>在充分理解了AudioQueue的整体数据流程之后，我们再来看一下它的核心使用方法。第一步是创建采集音频类型的AudioQueue。</p><pre><code class=\"language-plain\">AudioQueueNewInput(&amp;dataformat, recoderCB, (__bridge void *)self, NULL, NULL, 0, &amp;queueRef);\n</code></pre><p>代码里的dataFormat代表期望AudioQueue采集音频之后返回给业务层的具体的音频格式，recoderCB代表AudioQueue填充好一个buffer之后回调给业务层的方法，这个方法的实现如下：</p><pre><code class=\"language-plain\">static void recoderCB(void *aqData, AudioQueueRef inAQ, AudioQueueBufferRef inBuffer, const AudioTimeStamp *timestamp, UInt32 inNumPackets, const AudioStreamPacketDescription *inPacketDesc) {\n&nbsp; &nbsp; //1 inBuffer-&gt;mAudioData 处理 &amp; IO\n    \n&nbsp; &nbsp; //2 重新入队\n&nbsp; &nbsp; AudioQueueEnqueueBuffer(inAQ, inBuffer, 0, NULL);\n}\n</code></pre><p>这个方法内部消费掉inBuffer里的音频数据，然后将这个buffer重新入队。</p><p>当创建好一个AudioQueue之后，一般要为这个AudioQueue分配3个buffer，然后依次入队。</p><pre><code class=\"language-plain\">for (int i = 0; i &lt; kNumberBuffers; i++) {\n  AudioQueueAllocateBuffer(queueRef, self.bufferBytesSize, &amp;buffers[i]);\n  AudioQueueEnqueueBuffer(queueRef, buffers[i], 0, NULL);\n}\n</code></pre><p>其中代码里的bufferBytesSize代表每个buffer的数据大小，这个可以根据dataFormat与期望的数据长度<span class=\"reference\">（比如50ms）</span>来计算。</p><p>最后调用Start方法来启动AudioQueue。</p><pre><code class=\"language-plain\">AudioQueueStart(mQueue, NULL)\n</code></pre><p>像Stop、Reset等方法这里我就不再赘述了。整体来看，AudioQueue的使用其实比较简单，核心就是配置好dataFormat，然后实现回调函数，再按照期望的dataFormat处理数据就可以了。</p><h2>如何使用AudioUnit采集音频？</h2><p>我们这个部分使用的AudioUnit接口，其实在专栏的<a href=\"https://time.geekbang.org/column/article/543993\">第2节课</a>已经详细地讲过了，所以这里我们就直接来看如何使用AudioUnit实现人声的采集，同时还会给耳机一个监听耳返。等完成这个功能之后，你就会感叹这在iOS平台上实现起来太简单了。</p><p>在激活AVAudioSession之后，我们就要构造一个AUGraph了，构造AUGraph的过程和第2节课渲染音频时构造的AUGraph几乎是一致的，这里就不再赘述了。由于这里我们要使用采集音频的能力，所以要启用RemoteIO这个AudioUnit的InputElement，代码如下：</p><pre><code class=\"language-plain\">static UInt32 kInputBus = 1;\nUInt32 flag = 1;\nAudioUnitSetProperty(ioUnit, kAudioOutputUnitProperty_EnableIO, kAudioUnitScope_Input, kInputBus, &amp;flag, sizeof(flag));\n</code></pre><p>RemoteIO这个AudioUnit其实是比较特别的，InputElement<span class=\"reference\">（kInputBus的值为1）</span>代表的是麦克风，而OutputElement代表的是扬声器，上述这行代码就是启用RemoteIOUnit的InputElement。</p><p>为了支持开发的App可以在后续扩展出Mix一轨伴奏这个功能，我们需要额外在AUGraph中增加MultiChannelMixer这个AudioUnit。由于每个AudioUnit的输入输出格式并不相同，所以这里还要使用AudioConvert这个AudioUnit来把输入的AudioUnit连接到MixerUnit上。最终让MixerUnit连接上RemoteIO的OutputElement，将声音送到耳机或者扬声器中。</p><p>这里需要注意，如果没有插耳机的情况下需要Mute<span class=\"reference\">（消音）</span>掉这一路，否则就会出现啸叫的现象。到这里我们就把AUGraph构建出来了，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/d9/36/d952faf11ee9b2c96b29b1b1a61ed736.png?wh=2145x803\" alt=\"图片\"></p><p>如何把采集的音频存储成为一个文件呢？</p><p>我们可以在RemoteIO这个节点的OutputElement增加一个回调，然后在回调方法中来拉取预期节点的数据，同时也可以去写文件，这种方式其实已经在音频渲染的时候使用过。首先给RemoteIO这个AudioUnit的OutputElement增加一个回调。</p><pre><code class=\"language-plain\">AURenderCallbackStruct finalRenderProc;\nfinalRenderProc.inputProc = &amp;renderCallback;\nfinalRenderProc.inputProcRefCon = (__bridge void *)self;\nstatus = AUGraphSetNodeInputCallback(_auGraph, _ioNode, 0, &amp;finalRenderProc);\n</code></pre><p>然后在上述的回调方法的实现中，把它的前一级MixerUnit的数据渲染出来，同时写文件。</p><pre><code class=\"language-plain\">static OSStatus renderCallback(void *inRefCon,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AudioUnitRenderActionFlags *ioActionFlags, const AudioTimeStamp &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *inTimeStamp, UInt32 inBusNumber, UInt32 inNumberFrames, AudioBufferList &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *ioData){\n&nbsp;&nbsp;&nbsp; OSStatus result = noErr;\n&nbsp;&nbsp;&nbsp; __unsafe_unretained AudioRecorder *THIS = (__bridge AudioRecorder *)inRefCon;\n&nbsp;&nbsp;&nbsp; AudioUnitRender(THIS-&gt;_mixerUnit, ioActionFlags,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inTimeStamp, 0, inNumberFrames, ioData);\n&nbsp;&nbsp;&nbsp; //Write To File\n&nbsp;&nbsp;&nbsp; return result;\n}\n</code></pre><p>关于写文件，我们会在第12节课使用AudioToolbox来给文件编码，但是这里我们使用一个更高级的API——ExtAudioFile 来写文件，其实这个ExtAudioFile内部封装了AudioToolbox里面的AudioConverterReference。iOS提供的这个API只需要我们设置好输入格式和输出格式以及输出文件路径和文件格式。</p><pre><code class=\"language-plain\">AudioStreamBasicDescription destinationFormat;\nCFURLRef destinationURL;\nresult = ExtAudioFileCreateWithURL(destinationURL, kAudioFileCAFType,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;destinationFormat, NULL, kAudioFileFlags_EraseFile, &amp;audioFile);\nresult = ExtAudioFileSetProperty(audioFile,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kExtAudioFileProperty_ClientDataFormat, sizeof(clientFormat),\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;clientFormat);\nUInt32 codec = kAppleHardwareAudioCodecManufacturer;\nresult = ExtAudioFileSetProperty(audioFile,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kExtAudioFileProperty_CodecManufacturer, sizeof(codec), &amp;codec);\n</code></pre><p>在需要给文件编码时，就直接写入数据。</p><pre><code class=\"language-plain\">ExtAudioFileWriteAsync(audioFile, inNumberFrames, ioData);\n</code></pre><p>在停止写入的时候调用关闭方法即可。</p><pre><code class=\"language-plain\">ExtAudioFileDispose(audioFile);\n</code></pre><p>注意，这里调用的是WriteAsync，就是异步的方式来写文件，这样它不会阻塞Remote IO这个线程。在停止写入的时候，我们关闭这个方法就可以了。</p><p>最终就可以得到我们想要的文件了，你可以从应用的沙盒中将保存的文件拿出来<span class=\"reference\">（在XCode中用Device或iExplorer取出文件）</span>，然后播放试听一下。</p><h2>小结</h2><p>最后，我们一起来回顾一下今天的内容。</p><p>在iOS平台采集音频数据，比较常用的就是AVAudioRecoder，AudioQueue以及AudioUnit三套接口。</p><ul>\n<li>AVAudioRecorder使用起来比较简单，如果是简单的录音，使用AVAudioRecorder就可以。但因为无法操控中间的数据，它提供不了更高级的能力支持。</li>\n<li>使用AudioQueue录制音频，灵活性比较强。如果只是获取内存中的录音数据，然后编码、输出，使用AudioQueue来采集音频会更适合。</li>\n<li>使用AudioUnit录制音频灵活性最强，如果要使用更多的音效处理以及实时的监听功能，那么使用AudioUnit会更方便一些。</li>\n</ul><p>后续我们视频录制器项目中，使用的都是AudioUnit，因为实现的场景不单单需要耳返，也需要音效的实时处理等功能。</p><h2>思考题</h2><p>你可以使用一下<span class=\"orange\">回森</span>这个App，这个App有一个特色的功能就是语音弹幕，它支持用户在观看音乐作品的同时演唱一段弹幕<span class=\"reference\">（启动音频采集）</span>，而从听到唱的过程中整个音乐作品播放是非常流畅的，这里我给你留两个问题：</p><ol>\n<li>这个App是如何实现启动录音的时候音乐作品可以流畅地播放的呢？</li>\n<li>使用蓝牙耳机的情况下，在保证录制高音质音频的同时，音乐作品如何保持声音的流畅性呢？</li>\n</ol><p>欢迎在评论区留下你的思考，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"09｜播放器项目实践（三）：让你的播放器跑起来","id":550165},"right":{"article_title":"11｜Android 平台音频采集：如何使用 AudioRecord 和 Oboe 采集音频？","id":551878}}},{"article_id":551878,"article_title":"11｜Android 平台音频采集：如何使用 AudioRecord 和 Oboe 采集音频？","article_content":"<p>你好，我是展晓凯。今天我们来一起学习Android平台的音频采集。</p><p>视频播放器是将一个视频文件通过解封装、解码、渲染等工作，让用户可以听到声音、看到画面，而视频录制器恰恰是一个逆向的过程，是将麦克风采集到的声音、摄像头采集到的画面通过编码、封装，最终得到一个视频文件。所以整个项目的前置知识包括音视频的采集、编码还有音视频同步等，上一节课我们一起学习了iOS平台的音频采集方法，这节课我们就一起来看Android平台给我们提供了哪些音频采集的方法吧。</p><h2>Android平台的音频采集技术选型</h2><p><img src=\"https://static001.geekbang.org/resource/image/f0/eb/f02ac0c612eyyeceebbc56898fc894eb.png?wh=1920x670\" alt=\"图片\"></p><h3>SDK层提供的采集方法</h3><p>Android SDK 提供了两套音频采集的API接口，分别是MediaRecorder 和 AudioRecord。前者是一个端到端的API，它可以直接把手机麦克风录入的音频数据进行编码压缩<span class=\"reference\">（如AMR、MP3等）</span>并存储成文件；而后者则更底层一些，可以让开发者更加灵活地控制麦克风采集到的PCM数据。</p><p>如果想简单地做一个录音机，并且录制出一个音频文件，首选肯定是MediaRecorder，而如果需要对音频做进一步的算法处理，或者需要采用第三方的编码库进行编码压缩，那么就需要使用AudioRecord了。我们的视频录制器场景显然更适合选用第二种方式，使用AudioRecord来采集音频。</p><!-- [[[read_end]]] --><h3>NDK层提供的采集方法</h3><p>Android NDK也提供了两套音频采集的API接口，就是OpenSL ES和AAudio，其中AAudio是在Android 8.0系统以上才可以使用的。这两个API接口也都属于底层接口，可以获取实时采集到的PCM数据。相比于AudioRecord，Native层提供的采集方法具有更低的延迟与更高的性能，但是由于Android设备与系统厂商碎片化的问题，兼容性比不上AudioRecord，比如某些品牌的手机上会有声音采集音量过小，采集的音频有杂音等问题。</p><p>如果在一些需要低延迟音频采集的场景比较适合使用这两个音频采集的方法，比如实时耳返场景、更低延迟的VOIP场景等。如果想为自己的工程构建音频采集框架，是需要同时支持SDK层AudioRecord采集和Native层采集的。而在Native层直接使用Google开源的Oboe框架是一个比较好的选择，因为它屏蔽掉了OpenSL ES与AAudio的内部实现细节，提供了统一的API接口。所以在这节课的最后一部分，我会以Oboe为例给你讲解Native层录制音频的方法。</p><h3>录音权限</h3><p>其实无论是SDK层还是NDK层的音频采集，Android平台都要求，在采集之前应用必须要向用户显示申请权限，所以这里我们还需要了解一下，应用如何在Android平台获取录音权限。</p><p>首先，需要在AndroidManifest.xml这个应用的配置文件中，增加权限的声明。</p><pre><code class=\"language-plain\">&lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\" /&gt;\n</code></pre><p>如果我们在获取到PCM数据之后，还要把数据写入SDCard中，就需要额外申请系统的文件权限。声明好了之后，就需要检查当前的授权状态。如果已经授权了，可以直接开启录音器采集音频。如果没有授权的话，就需要去申请授权。</p><pre><code class=\"language-plain\">if(ContextCompat.checkSelfPermission(this, android.Manifest.permission.RECORD_AUDIO)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; != PackageManager.PERMISSION_GRANTED){\n  //未授权，则进行申请权限\n  ActivityCompat.requestPermissions(this,new String[]{android.Manifest.permission.RECORD_AUDIO},1);\n}else {\n  //已经授权，可以开启录音器\n}\n</code></pre><p>申请授权的代码中，第一个参数传递的是一个Activity类型的对象，申请授权一般是用系统的弹窗来提示用户，应用索取录音权限，是否同意授权，用户的授权状态会回调给这个对象的方法onRequestPermissionsResult。</p><pre><code class=\"language-plain\">public static void requestPermission(@NonNull Activity activity, int requestCode, @NonNull String[] permissions,\n    @Nullable PermissionGrantCallback callback) {\n    //判断是否授权了\n}\n</code></pre><p>如果已获得授权一般requestCode是1，permissions里面包含了用户赋予应用的所有权限，可以在这里再判断一下用户是否给予了录音权限，如果确认授权了就可以执行录音操作，如果没有获得权限就可以给用户提示。</p><p>当我们取得内存中的PCM数据之后该如何处理呢？</p><p>其实在多媒体App中一般会进行声音特效处理，然后把它编码成一个AAC或者MP3文件。至于音频的处理部分这节课我们暂不涉及，音频的编码我们下节课会进行讲解，这里我们就简单地写到一个PCM文件中。在录制结束之后，可以从SD卡中拿出这个PCM文件，然后使用ffplay进行播放来试听效果。接下来我们就先看一下SDK层的AudioRecord采集音频吧。</p><h2>AudioRecord采集音频</h2><p>AudioRecord是Android平台兼容性最好的音频采集方法，熟练地使用它可以为你的App带来最稳定的音频采集能力。使用方法分为四个步骤。</p><p><img src=\"https://static001.geekbang.org/resource/image/22/a3/229f75c715e3c7e3064d15b9337c0fa3.png?wh=1920x2418\" alt=\"图片\"></p><ol>\n<li>配置参数，初始化内部的音频缓冲区</li>\n</ol><p>我们先来看一下AudioRecord的参数配置，AudioRecord是通过构造函数来进行配置的，构造函数原型如下：</p><pre><code class=\"language-plain\">public AudioRecord(int audioSource, int sampleRateInHz, int channelConfig, int\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; audioFormat, int bufferSizeInBytes).\n</code></pre><p>我们来看下构造函数中参数代表的含义，以及在各种场景下应该传递的值具体说明。</p><p>audioSource参数指定的是音频采集的输入源，可选值以常量的形式定义在类AudioSource（MediaRecorder的一个内部类）中，常用的可选值如下：</p><ul>\n<li>DEFAULT，代表使用默认的麦克风采集。</li>\n<li>CAMCORDER，使用和摄像头同方向的麦克风采集。</li>\n<li>VOICE_RECOGNITION，一般用于语音识别场景。</li>\n<li>MIC，代表使用手机的主麦克风作为采集的输入源。</li>\n<li>VOICE_COMMUNICATION，在VOIP场景下如果使用硬件AEC的话，可以设置这个参数。</li>\n</ul><p>sampleRateInHz，用来指定采用多大的采样频率来采集音频，现在用得最多的就是44100或者48000的采样频率。因为根据奈奎斯特采样定律，只有这两个采样频率在做AD/DA转换的时候才能还原出20K以上的音频。如果使用这个采样率初始化录音器失败的话，则可以使用16000的采样频率<span class=\"reference\">（就是我们经常说的16k的采样率）</span>来兜一下底。</p><p>channelConfig，这个参数用来指定录音器采集几个声道的声音，可选值以常量的形式定义在 AudioFormat 类中，常用的值包括：CHANNEL_IN_MONO单声道采集和CHANNEL_IN_STEREO立体声采集。</p><p>由于现在的移动设备都是伪立体声的采集，所以考虑到性能，一般按照单声道进行采集，然后在音频处理阶段转换为立体声效果。当然在某些需要立体声输入的场景<span class=\"reference\">（外接声卡输入/USB麦克风）</span>，可以设置为立体声。</p><p>audioFormat，就是基础概念里我们介绍过的采样表示格式，可选值以常量的形式定义在 AudioFormat 类中，常用的值包括：ENCODING_PCM_16BIT，使用16位或者2个字节来表示一个采样点，还有ENCODING_PCM_8BIT，代表使用8位或者1个字节来表示一个采样点。前者可以保证兼容大部分的Android手机，一般都采用16BIT。</p><p>bufferSizeInBytes，这是最难理解但又很重要的一个参数，用来指定AudioRecord内部音频缓冲区的大小。而具体的大小，不同厂商会有不同的实现，这个音频缓冲区越小，录音的延时就会越小。</p><p>AudioRecord 类提供了一个静态方法，帮助开发者来确定这个 bufferSizeInBytes 的函数，原型如下：</p><pre><code class=\"language-plain\">int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat);\n</code></pre><p>在实际开发中，一定要通过这个函数计算需要传入的 bufferSizeInBytes。</p><p>配置好AudioRecord之后，要检查一下当前AudioRecord的状态，因为有可能会因为权限或者参数的问题，使构造的AudioRecord状态异常。那如何检查录音的状态呢？我们可以通过AudioRecord的方法getState来获取当前状态，然后和AudioRecord.STATE_INITAILIZED进行比较。如果相等，才可以开启采集，否则应该提示给用户。</p><ol start=\"2\">\n<li>开始采集</li>\n</ol><p>走到这一阶段，说明已经创建好AudioRecord的实例了，并且状态也是正确的，接下来我们调用startRecording()方法来采集音频。</p><pre><code class=\"language-plain\">audioRecord.startRecording();\n</code></pre><ol start=\"3\">\n<li>提取数据</li>\n</ol><p>与AudioTrack的使用方法类似，读取音频录音器采集到的音频，需要开发者自己启动一个线程，不断地从 AudioRecord 的缓冲区将音频数据读出来。注意，这个过程一定要及时，否则控制台会打印“overrun”的错误，这个错误在音频开发中比较常见，意味着应用层没有及时地“取走”音频数据，导致内部的音频缓冲区溢出。读取录音器采集到的PCM数据的方法原型如下：</p><pre><code class=\"language-plain\">public int read(byte[] audioData, int offsetInBytes, int sizeInBytes)\n</code></pre><p>当然，也可以读取short数组类型的数据，因为这更符合音频采样点的表示，毕竟我们设置的是每个sample都是由两个字节来表示，方法原型如下：</p><pre><code class=\"language-plain\">public int read(short[] audioData, int offsetInShorts, int sizeInShorts)\n</code></pre><p>拿到数据之后，就可以通过Java层提供的FileOutputStream直接将数组写到文件中。</p><ol start=\"4\">\n<li>停止采集，释放资源</li>\n</ol><p>当我们想停止音频采集的时候，需要调用AudioRecord实例的stop方法，并且最终要对这个AudioRecord的实例调用release，来释放这个录音器，以便设备的其他应用可以正常使用录音器。一般通用的写法是通过调用个布尔型变量控制读取数据的线程结束，然后再停止和释放AudioRecord实例。最后还要关闭写入数据的文件，否则会有文件写出不完全的问题。</p><p>当拿到一个写出的PCM文件，我们应该如何播放呢？因为PCM文件就是一个二进制文件，如果不知道这个PCM文件的表示格式，是无法播放的。而我们这个PCM文件的表示格式是44100的采样率、单声道、16位表示一个采样点，那么使用ffplay来播放这个PCM文件的命令如下：</p><pre><code class=\"language-plain\">ffplay -f s16le&nbsp; -sample_rate 44100&nbsp; -channels 1 -i vocal.pcm\n</code></pre><p>而我们常见的WAV文件就是将PCM文件的表示格式放到WAV协议定义的一个头里面，我们也可以利用FFmpeg将PCM文件转换为WAV文件，然后使用PC上的系统播放器进行播放，转换命令行如下：</p><pre><code class=\"language-plain\">ffmpeg -f s16le -sample_rate 44100 -channels 1 -i vocal.pcm -acodec pcm_s16le vocal.wav\n</code></pre><h2>Oboe采集音频</h2><p>关于Oboe的介绍，以及如何集成Oboe到我们的工程中，我们<a href=\"https://time.geekbang.org/column/article/545000\">第3节课</a>使用Oboe来做音频渲染的时候已经讲过，你可以先回顾一下，这里我们就不再重复了。接下来，我们直接看如何使用Oboe采集音频。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/fa/50e04720a4474104027f198648c5cafa.png?wh=1035x747\" alt=\"图片\"></p><ol>\n<li>创建AudioStream</li>\n</ol><p>和音频的渲染一样，我们通过AudioStreamBuilder来创建Stream，不同点是Direction这里我们要设置成Input，代表我们要创建录音器。</p><pre><code class=\"language-plain\">oboe::AudioStreamBuilder builder;\nbuilder.setPerformanceMode(oboe::PerformanceMode::LowLatency)\n&nbsp; -&gt;setDirection(oboe::Direction::Input)//录制的设置\n&nbsp; -&gt;setSharingMode(oboe::SharingMode::Shared)\n&nbsp; -&gt;setChannelCount(oboe::ChannelCount::Mono)//单声道采集\n&nbsp; -&gt;setFormat(oboe::AudioFormat::I16);//格式采用I16，范围为[-32768, 32767]\n</code></pre><p>最终调用openStream来打开Stream，和音频渲染一样，也是根据返回值来判断是否创建成功，如果创建失败了，可以使用convertToText来查看错误码。</p><pre><code class=\"language-plain\">oboe::Result result = builder.openStream(mStream);\nif (result != oboe::Result::OK) {\n  LOGE(\"Failed to create stream. Error: %s\",&nbsp;convertToText(result));\n}\n</code></pre><ol start=\"2\">\n<li>录制音频</li>\n</ol><p>调用了requestStart方法，当采集到音频之后就会回调设置的Callback函数。</p><pre><code class=\"language-plain\">mStream-&gt;requestStart();\n</code></pre><ol start=\"3\">\n<li>启动一个线程来读取音频</li>\n</ol><pre><code class=\"language-plain\">oboe::ResultWithValue&lt;int32_t&gt; result = mInputStream-&gt;read(mInputBuffer.get(), numFrames, 0);\nif (!result) {\n    //结束\n} else {\n    int32_t framesRead = result.value();\n    //TODO 写入文件\n}\n</code></pre><p>在这里读取出一部分数据就可以写入文件了。</p><ol start=\"4\">\n<li>当停止录音的时候需要关闭AudioStream</li>\n</ol><pre><code class=\"language-plain\">mStream-&gt;requestStop();\nmStream-&gt;close();\n</code></pre><p>到这里，Oboe录制音频的方法我们也就讲完了，最终得到的PCM文件也可以使用ffplay进行播放，使用这个方法来检查一下你录音器是否录制成功了吧。</p><p>当然，在一些低性能手机上，写文件还是要在异步线程中去写，否则就会阻碍录制器的录制线程。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/66/77/66b65110b92012b25779f571063fa177.png?wh=1920x1796\" alt=\"图片\"></p><p>Android平台音频采集的技术选型，在SDK层和NDK层各有两套音频采集方法，考虑到我们的视频录制器的场景，我们选取了SDK层的AudioRecord和Native层的Oboe采集音频的方法。</p><ul>\n<li>AudioRecord采集音频主要分为配置参数，初始化内部音频缓冲区、采集、提取数据，以及停止采集，释放资源四步。</li>\n<li>使用Oboe录制音频也分为四步，分别是创建Audio Stream、录制、启动线程读取音频、停止录音时关闭Audio Stream。</li>\n</ul><p>每一步我都给出了相应的代码示例，相信你跟着我做，也能成功录制音频。通过音频的录制我们已经得到设备采集到的PCM声音了。但是PCM数据是非常大的，比如CD音质的PCM数据一般表示格式为：44100采样率、双声道、每个采样点用16位来表示，那1分钟CD音质的PCM数据大小就是10.09MB。</p><p>$$44100\\times16\\times2\\times60\\div8\\div1024=10.09MB$$</p><p>这么大的数据要想进行存储及网络传输是不现实的，需要对音频进行压缩编码，下节课我会带着你学习音频编码方面的内容。</p><h2>思考题</h2><p>在一些K歌的App，比如回森中有一个功能是实时耳返功能，它可以让带有线耳机的用户在演唱歌曲的同时，在耳机中听到自己的声音，要想实现这个功能肯定需要将采集到的音频立马播放出来。而这里面的难点就是低延迟，因为如果延迟高了用户跟着伴奏演唱是非常糟糕的体验。那如果让你来实现这个功能，应该怎么实现呢？欢迎在评论区分享你的思考，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p><p></p>","neighbors":{"left":{"article_title":"10｜iOS平台音频采集：如何使用AudioQueue和AudioUnit采集音频？","id":551141},"right":{"article_title":"12｜如何编码出一个AAC文件？","id":553115}}},{"article_id":553115,"article_title":"12｜如何编码出一个AAC文件？","article_content":"<p>你好，我是展晓凯。今天我们来一起学习移动平台的音频编码。</p><p>上节课，我们提到过CD音质的数据每分钟需要的存储空间约为10.1MB，如果仅仅是要放在存储设备<span class=\"reference\">（光盘、硬盘）</span>中，可能是可以接受的，但是一旦在线传输的话，这个数据量就太大了，所以我们必须对其进行压缩编码。</p><p>压缩编码的基本指标之一就是压缩比，压缩比通常小于1。压缩算法分为有损压缩和无损压缩。无损压缩就是解压后数据可以完全复原，比如我们常见的Zip压缩；在音视频领域我们用得比较多的是有损压缩，有损压缩指解压后的数据不能完全复原，会丢失一部分信息。压缩比越小，丢掉的信息越多，信号还原后失真越大。</p><p>根据不同的应用场景<span class=\"reference\">（从存储设备、传输网络环境、播放设备等角度综合考虑）</span>，我们可以选用不同的压缩编码算法，如WAV、MP3、AAC、OPUS等。压缩编码的原理实际上是压缩掉冗余信号。在音频中冗余信号指的是不容易被人耳感知到的信号，包含人耳听觉范围外的音频信号以及被掩蔽掉的音频信号等。</p><p>人耳听觉范围是20～20kHz，被掩蔽掉的音频信号指的是由于人耳的掩蔽效应不被察觉的信号，主要表现为频域掩蔽效应与时域掩蔽效应，无论在时域还是频域上，被掩蔽掉的声音信号都被认为是冗余信息，也不进行编码处理。</p><!-- [[[read_end]]] --><h2>常见的音频编码格式</h2><p>了解了音频编码的原理，下面我们来详细看一下几种常用的压缩编码格式。</p><ul>\n<li>WAV编码</li>\n</ul><p>PCM 脉冲编码调制是Pulse Code Modulation的缩写。前面我们提到了PCM大致的工作流程，而WAV编码的一种实现<span class=\"reference\">（有多种实现，但是都不会进行压缩操作）</span>就是在PCM数据格式的前面加上44个字节，分别用来描述PCM的采样率、声道数、数据格式等信息。</p><p>WAV编码的特点是音质非常好，几乎可以被所有软件播放，适用于多媒体开发的中间文件、保存音乐和音效素材。</p><ul>\n<li>MP3编码</li>\n</ul><p>MP3具有不错的压缩比，使用LAME编码<span class=\"reference\">（最常使用的一种MP3编码器）</span>的中高码率<span class=\"reference\">（320Kbps左右）</span>的MP3，听感上非常接近WAV源文件，当然在不同的应用场景下，我们自己应该调整合适的参数来达到最好的效果。</p><p>MP3编码的特点是音质在128Kbps以上表现还不错，压缩比比较高，大量软件和硬件都支持，兼容性最好，适用于高比特率下对兼容性有要求的音乐欣赏。</p><ul>\n<li>AAC编码</li>\n</ul><p>AAC是新一代的音频有损压缩技术，它通过一些附加的编码技术，如SBR、PS等，衍生出了LC-AAC、HE-AAC、HE-AACv2三种主要的编码规格<span class=\"reference\">（Profile）</span>。LC-AAC就是比较传统的AAC，主要用在中高码率的场景编码<span class=\"reference\">（$\\geq80Kbps$）</span>；HE-AAC，相当于AAC+SBR，主要用在中低码率场景编码<span class=\"reference\">（$\\leq80Kbps$）</span>；而HE-AACv2，相当于AAC+SBR+PS，主要用于双声道、低码率场景下的编码<span class=\"reference\">（$\\leq48Kbps$）</span>。</p><p>事实上，大部分AAC编码器的实现中，将码率设成小于等于48Kbps会自动启用PS技术，而大于48Kbps就不加PS，就相当于普通的HE-AAC。</p><p>AAC编码的特点是在小于128Kbps的码率下，表现优异，当下应用广泛，多用于 128Kbps以下的音频编码、视频中音频轨的编码以及音乐场景下的编码。</p><ul>\n<li>Opus编码</li>\n</ul><p>Opus集成了以语音编码为导向的SILK和低延迟编码为导向的CELT，所以它同时具有这两者的优点，可以无缝调节高低码率。在较低码率时，使用线性预测编码，在高码率时使用变换编码。</p><p>Opus具有非常低的算法延迟，最低可以做到5ms的延迟，默认为22.5 ms，非常适合用在低延迟语音通话场景。与MP3、AAC等常见格式相比，在低码率<span class=\"reference\">（64Kbps及以下）</span>场景下，Opus具有更好的音质表现，同时也有更低的延迟表现。WebRTC中采用的音频默认编码就是Opus编码，并且在Opus编码的协议中，开发者也可以加入自己的增强信息<span class=\"reference\">（类似于H264中的SEI）</span>用于一些场景功能的扩展。</p><p>Opus编码的特点是支持众多的帧长范围、码率范围、频率范围，内部有机制，来处理防止丢包策略，在低码率下依然能保持优异的音质。它主要适用于VOIP场景下的语音编码。</p><p><img src=\"https://static001.geekbang.org/resource/image/b5/bd/b590b60d01377b1d6b63088759ee11bd.png?wh=1920x708\" alt=\"图片\"></p><p>在移动平台上，无论是单独的音频编码，还是视频编码中的音频流的编码，用得最广泛的就是AAC这一编码格式。所以我们这节课来重点学习一下音频的AAC编码。</p><p>AAC编码的方式有两种，一种是使用软件编码，另一种是使用Android与iOS平台的硬件编码。软件编码的实现方式我们会基于FFmpeg来讲解，后期无论你想用什么格式，都可以自己配置编码库来实现，编码部分的代码是可以复用的。这节课的输入是我们前两节课用录音器采集下来的PCM文件，最终编码成一个AAC编码格式的M4A文件。</p><p>在学习使用软件编码器编码AAC之前，让我们先来系统了解AAC这种编码格式，底层编码原理与算法我们就不介绍了，但是站在应用层角度我们要学习一下它的编码规格和封装格式，让我们一起来看一下吧。</p><h2>AAC编码格式详解</h2><p>对于音视频应用层的开发者来讲，还是要掌握编码器本身的一些高级特性的，在AAC这个编码中，我们需要重点掌握的就是它的编码规格和封装格式。</p><h3>AAC的编码规格</h3><p>AAC编码器常用的编码规格有三种，分别是LC-AAC、HE-AAC、HE-AACv2，这三种编码规格以及使用的消除冗余的技术手段如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/ff/50/ffb29fce82cd718d7c2b9eda9c52b050.png?wh=1356x746\" alt=\"\"></p><p>其中LC-AAC的Profile是最基础的AAC的编码规格，它的复杂度最低，兼容性也是最好的，双声道音乐在128Kbps的码率下可以达到全频带<span class=\"reference\">（44.1kHz）</span>的覆盖。</p><p>在LC-AAC的基础上添加SBR技术，形成HE-AAC的编码规格，SBR全称是Spectral Band Replication，其实就是消除频域上的冗余信息，可以在降低码率的情况下保持音质。内部实现原理就是把频谱切割开，低频单独编码保存，来保留主要的频谱部分，高频单独放大编码保存以保留音质，这样就保证在降低码率的情况下，更大程度地保留了音质。</p><p>在HE-AAC的基础上添加PS技术，就形成了HE-AACv2的编码规格，PS全称是Parametric Stereo，其实就是消除立体声中左右声道之间的冗余信息，所以使用这个编码规格编码的源文件必须是<strong>双声道</strong>的。内部实现原理就是存储了一个声道的全量信息，然后再花很少的字节用参数描述另一个声道和全量信息声道有差异的地方，这样就达到了在HE-AAC基础上进一步提高压缩比的效果。</p><p>我们使用FFmpeg命令行，用不同的编码规格，来把同一个输入文件编码成为三个文件，然后使用可视化的音频分析软件Praat看一下它们的质量，我们先来看一下原始文件source.wav<span class=\"reference\">（双声道、采样率为44.1kHz）</span>导入到Praat中的频谱分布。</p><p><img src=\"https://static001.geekbang.org/resource/image/10/4c/1066fdffa2b4c0034c993af5244cf04c.png?wh=1265x616\" alt=\"图片\"></p><p>可以看到图片中高频的部分到了22050，根据奈奎斯特采样定律，编码之后频带分布到22050就是全频带分布<span class=\"reference\">（原始格式44.1kHz）</span>，使用下面命令来编码文件。</p><pre><code class=\"language-plain\">ffmpeg -i source.wav -acodec libfdk_aac -b:a 48K lc_aac.m4a\nffmpeg -i source.wav -acodec libfdk_aac -profile:a aac_he -b:a 48K he_aac.m4a\nffmpeg -i source.wav -acodec libfdk_aac -profile:a aac_he_v2 -b:a 48K he_v2_aac.m4a\n</code></pre><p>这三行命令使用的都是libfdk_aac编码器，但是用了不同编码器规格，编码出了三个M4A文件。接下来我们把三个文件放到Praat中，如下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/d5/7e42c6dbe9c0f650865913a73d6b39d5.png?wh=1260x1744\" alt=\"图片\"></p><p>可以看到lc_aac.m4a的频带分布到了10KHz就被截断了，对高频部分影响比较大；而he_aac这个文件的截止频率大约到16KHz以上，明显要比第一个好很多；再看第三个文件几乎达到了全频带覆盖，结合之前介绍的原理你就知道为什么这种编码规格可以达到全频带覆盖了。</p><p>AAC编码器的这三种编码规格之间的差异我们了解清楚之后，就可以根据自己的应用场景选择不同的编码规格和码率。</p><h3>AAC的封装格式</h3><p>我们日常生活中常见的AAC编码的封装格式有两种，一种是ADTS封装格式，可以简单理解为以AAC为后缀名的文件，另外一种是ADIF封装格式，可以简单地理解为以M4A为后缀名的文件。</p><p>ADIF全称是Audio Data Interchange Format，是AAC定义在MPEG4里面的格式，字面意思是交换格式，是将整个流的Meta信息<span class=\"reference\">（包括AAC流的声道、采样率、规格、时长）</span>写到头部，解码器只有解析了头部信息之后才可以解码具体的音频帧，像M4A封装格式、FLV封装格式、MP4封装格式都是这样的。</p><p>ADTS全称是Audio Data Transport Stream，是AAC定义在MPEG2里面的格式，含义就是传输流格式，特点就是从流中的任意帧位置都可以直接进行解码。这种格式实现的原理是在每一帧AAC原始数据块的前面都会加上一个头信息<span class=\"reference\">（ADTS+ES）</span>，形成一个音频帧，然后不断地写入文件中形成一个完整可播放的AAC文件。</p><p><img src=\"https://static001.geekbang.org/resource/image/e5/f6/e599901899e61a372e59d642f8c403f6.png?wh=1920x1027\" alt=\"图片\"></p><p>如图所示，ADTS头分为固定头和可变头两部分，各自需要28位来表示，要构造一个ADTS头其实就是分配好这7个字节，下面我们来分配一下这七个字节。</p><pre><code class=\"language-plain\">int adtsLength = 7;\nchar *packet = malloc(sizeof(char) * adtsLength);\npacket[0] = (char)0xFF;\npacket[1] = (char)0xF9;\n</code></pre><p>前12位表示同步字，固定为全1，表示为[11111111], [1111]；接下来的4位表示的是ID，我们这里是ADTS的封装格式ID，也就是1；Layer一般固定是00，protection_absent代表是否进行误码校验，这里我们填1，所以前2个字节就是 [11111111] [11111001]，也就是代码上的两个Char类型的数字了。</p><p>下面我们不再一一分析每一位是怎么构造的了，直接以字节来讲解。后边的字节是编码规格、采样率下标<span class=\"reference\">（注意是下标，而不是采样率）</span>、声道配置<span class=\"reference\">（注意是声道配置，而不是声道数）</span>、数据长度的组合<span class=\"reference\">（注意packetLen是原始数据长度加上ADTS头的长度）</span>，最后一个字节一般也是固定的代码，如下：</p><pre><code class=\"language-plain\">int profile = 2; // AAC LC\nint freqIdx = 4; // 44.1KHz\nint chanCfg = 2; // CPE\npacket[2] = (byte) (((profile - 1) &lt;&lt; 6) + (freqIdx &lt;&lt; 2) + (chanCfg &gt;&gt; 2));\npacket[3] = (byte) (((chanCfg &amp; 3) &lt;&lt; 6) + (packetLen &gt;&gt; 11));\npacket[4] = (byte) ((packetLen &amp; 0x7FF) &gt;&gt; 3);\npacket[5] = (byte) (((packetLen &amp; 7) &lt;&lt; 5) + 0x1F);\npacket[6] = (char) 0xFC;\n</code></pre><p>这里具体的编码Profile、采样率的下标以及声道数配置，可以点击<a href=\"https://wiki.multimedia.cx/index.php?title=MPEG-4_Audio#Channel_Configurations\">链接</a>查看相关的所有表示。一般编码器<span class=\"reference\">（Android的MediaCodec或者iOS的AudioToolbox）</span>编码出来的AAC原始数据块我们称为ES流，需要在前面加上ADTS的头，才可以形成可播放的AAC文件，下节课我们就会用到。对于这种ADTS的压缩音频帧，也可以直接使用FFmpeg封装成M4A格式的文件。</p><p>在FFmpeg中，有一个类型的Filter叫做bit stream filter，主要是应用在一些编码格式的转封装行为中。对于AAC编码上述的两种封装格式，FFmpeg提供了aac_adtstoasc类型的bit stream filter，用来把ADTS格式的压缩包<span class=\"reference\">（AVPacket）</span>转换成ADIF格式的压缩包<span class=\"reference\">（Packet）</span>。使用这个Filter可以很方便地完成AAC到M4A封装格式的转换，不用重新进行解码编码的操作，FFmpeg帮助开发者隐藏掉了实现细节，并且提供了更好的代码可读性。</p><p>了解了AAC的封装格式之后，我们就来学习如何使用FFmpeg来将PCM编码成AAC格式。</p><h2>使用软件编码器编码AAC</h2><p>我们用FFmpeg的API来编写的主要原因是，如果我们以后想使用别的编码格式，只需要调整相应的编码器ID或者编码器Name就可以了。原理就是FFmpeg帮我们透明掉了内部的细节，做了和各家编码器API对接的工作，给开发者暴露出了统一的面向FFmpeg API的接口。这里使用的编码器是libfdk_aac，既然要使用第三方库libfdk_aac，那么就必须在做交叉编译的时候，将libfdk_aac这个库编译到FFmpeg中去。</p><p>由于我们想书写一个同时运行在Andorid平台和iOS平台上编码器工具类，所以构造一个C++的类，叫做audio_encoder，向外暴露三个接口，分别是初始化、编码以及销毁方法。下面我会向你详细讲解每一个接口定义、职责描述以及内部实现<span class=\"reference\">（实现会根据FFmpeg版本不同稍有不同，FFmpeg5.0以上改动较大）</span>。</p><h3>初始化</h3><p>初始化接口定义如下：</p><pre><code class=\"language-plain\">int init(int bitRate, int channels, int sampleRate, int bitsPerSample,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; const char* aacFilePath, const char * codec_name);\n</code></pre><p>第一个参数是比特率，也就是最终编码出来的文件的码率，码率越高音质也就越好，对于双声道的音频，一般我们设置128Kb就可以了；接下来的参数是声道数、采样率和位深度；然后是最终编码的文件路径；最后是编码器的名字。注意，最后两个参数是有关联的，比如M4A文件要填入一个AAC的编码器<span class=\"reference\">（libfdk_aac）</span>名称、MP3文件要传入一个MP3编码器<span class=\"reference\">（lame）</span>的名称。</p><p>这个接口内部会拿着这些信息把编码器初始化，如果编码器初始化成功，则返回0，失败则返回小于0的值。接口内部的核心实现如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/ae/6b3d1722b702e3yy9a25b7244a4648ae.png?wh=1920x1202\" alt=\"图片\"></p><p>调用avformat_alloc_context方法分配出封装格式，然后调用avformat_alloc_output_context2 传入输出文件格式，分配出上下文，即分配出封装格。之后调用avio_open2方法将AAC的编码路径传入，相当于打开文件连接通道。这样就可以确定Muxer与Protocol了。</p><p>有了容器之后，就应该向容器中添加音频轨了，调用avformat_new_stream传入刚才的FormatContext构建出一个音频流<span class=\"reference\">（AVStream）</span>，接着要为这个Stream分配一个编码器，编码器是一个AVCodecContext类型的结构体，先调用avcodec_find_encoder_by_name函数，根据编码器名称找出对应的编码器，接着根据编码器分配出编码器上下文，然后给编码器上下文填充以下几个属性。</p><ul>\n<li>首先是codec_type，赋值为AVMEDIA_TYPE_AUDIO，代表音频类型；</li>\n<li>其次是bit_rate、sample_rate、channels等基本属性；</li>\n<li>然后是channel_layout，可选值是两个常量AV_CH_LAYOUT_MONO代表单声道、AV_CH_LAYOUT_STEREO代表立体声；</li>\n<li>最后也是最重要的sample_fmt，代表采样格式，使用的是AV_SAMPLE_FMT_S16，即用2个字节来表示一个采样点。</li>\n</ul><p>这样，我们就把AVCodecContext这个结构体构造完成了，然后还可以设置profile，这里可以设置FF_PROFILE_AAC_LOW。最后调用avcodec_open2来打开这个编码器上下文，接下来为编码器指定frame_size的大小，一般指定1024作为一帧的大小，现在我们就把音频轨以及这个音频轨里面编码器部分给打开了。</p><p>这里需要注意一下，某些编码器只允许特定格式的PCM作为输入源，比如对声道数、采样率、表示格式<span class=\"reference\">（比如lame编码器就不允许SInt16的表示格式）</span>是有要求的。这时候就需要构造一个重采样器，来将PCM数据转换为可适配编码器输入的PCM数据，就是前面讲过的需要将输入的声道、采样率、表示格式和输出的声道、采样率、表示格式，传递给初始化方法，然后分配出重采样上下文SwrContext。</p><p>接下来还要分配一个AVFrame类型的inputFrame，作为客户端代码输入的PCM数据存放的地方，这里需要知道inputFrame分配的buffer的大小，默认一帧大小是1024，所以对应的buffer<span class=\"reference\">（按照uint8_t类型作为一个元素来分配）</span>大小就应该是：</p><pre><code class=\"language-plain\">bufferSize = frame_size * sizeof(SInt16) * channels;\n</code></pre><p>也可以调用FFmpeg提供的方法av_samples_get_buffer_size，来帮助开发者计算，其实这个方法内部的计算公式就是上面所列的公式。如果需要重采样的处理的话，也需要额外分配一个重采样之后的AVFrame类型的swrFrame，作为最终得到结果的AVFrame。</p><p>在初始化方法的最后，需要调用FFmpeg提供的方法avformat_write_header将这个音频文件的Header部分写进去，然后记录一个标志isWriteHeaderSuccess，使其为true，因为后续在销毁资源的阶段，需要根据这个标志来判断是否调用write trailer方法写入文件尾部。</p><h3>编码方法</h3><p>编码接口定义如下：</p><pre><code class=\"language-plain\">void encode(byte* buffer, int size);\n</code></pre><p>传入的参数是uint8_t类型数组和它的长度，这个接口的职责就是将传递进来的PCM数据编码并写到文件中。接口内部实现就是将这个buffer填充入inputFrame，因为前面我们已经知道每一帧buffer需要填充的大小是多少了，所以这里可以利用一个while循环来做数据的缓冲，一次性填充到AVFrame中去。</p><p>调用avcodec_send_frame，当返回值大于0的时候，再调用avcodec_receive_packet来得到编码后的数据AVPacket，然后调用av_interleaved_write_frame方法，就可以将这个packet写到最终的文件中去。</p><h3>销毁方法</h3><p>接口定义如下：</p><pre><code class=\"language-plain\">void destroy();\n</code></pre><p>这个方法需要销毁前面分配的资源以及打开的连接通道。如果初始化了重采样器，那么就销毁重采样的数据缓冲区以及重采样上下文；然后销毁为输入PCM数据分配的AVFrame类型的inputFrame，再判断标志isWriteHeaderSuccess变量，决定是否需要填充duration以及调用方法av_write_trailer，然后关闭编码器和连接通道，最终释放FormatContext。</p><p>这个类写完之后，就可以集成到Android和iOS平台了，外界控制层需要初始化这个类，然后负责读写文件调用encode方法，最终调用销毁资源方法。</p><p>这节课涉及的代码比较多，后续我会把代码实例上传到GitHub上，你可以对着代码再进行练习一下。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/41/ce/41fb68f986803e354e81ae68fb50b0ce.png?wh=1136x1192\" alt=\"图片\"></p><p>一般我们采集得到的原始数据都会比较大，需要我们后期进行压缩编码。目前常用的编码格式有AAC、MP3、WAV、Opus几种，其中WAV格式编码是最常见的，MP3格式是兼容性最好的，而AAC在低码率<span class=\"reference\">（128Kb以下）</span>场景下，音质大大超过MP3。目前在音视频开发领域，用得最广泛的就是AAC编码格式。</p><p>这节课，我们用FFmpeg实现了一个编码AAC文件的工具类，并且这个音频编码的工具类不单单可以用到编码AAC格式中，同时支持后续的其他编码，比如WAV编码和MP3编码等。在Android和iOS平台上都提供了各自的硬件编码器用于音频编码，下节课我会给你讲一讲怎么使用这两个平台的硬件编码器来给音频编码，一起期待一下吧！</p><h2>思考题</h2><p>这节课我们一起学习了AAC的编码格式，并且一起书写了一个用FFmpeg来编码AAC的工具类，上节课我们也掌握了音频采集的方法，那如何将它们结合起来，做一个系统录音机呢？思考一下，描述出你的架构设计。</p><p>欢迎在评论区分享你的思考，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"11｜Android 平台音频采集：如何使用 AudioRecord 和 Oboe 采集音频？","id":551878},"right":{"article_title":"13｜如何使用硬件编码器来编码 AAC？","id":554444}}},{"article_id":554444,"article_title":"13｜如何使用硬件编码器来编码 AAC？","article_content":"<p>你好，我是展晓凯。今天我们一起学习使用移动平台的硬件编码器编码AAC。</p><p>上一节课，我们学习了AAC编码格式，还用FFmpeg书写了一个AAC编码工具类，这节课我们一起学习一下如何使用平台自身提供的硬件编码方法来给音频编码。因为两个平台的硬件编码器编码出来的是裸的ES流，如果要保存为可播放的AAC，还需要自己加上ADTS的头。</p><h2>Android平台的硬件编码器MediaCodec</h2><p>我们先来看如何使用Android平台提供的MediaCodec来编码AAC。MediaCodec是Android系统提供的硬件编码器，它可以利用设备的硬件来完成编码，大大提升了编码的效率，并且可以节省CPU，让你的App运行起来更加流畅。</p><p>但使用MediaCodec编码对Android系统是有要求的，必须是4.1以上的系统，Android的版本代号在Jelly_Bean以上。而且因为Android设备的碎片化太严重，所以兼容性方面不如软件编码好，你可以根据自己的实际情况决定是否使用Android平台的硬件编码能力。</p><p>下面我们来看MediaCodec使用方法。类似于软件编码提供的三个接口方法，这里也提供三个接口方法，分别完成初始化、编码数据和销毁编码器操作。</p><!-- [[[read_end]]] --><h3>初始化</h3><p>在初始化方法的实现中要构造一个MediaCodec实例出来，通过这个类的静态方法来构建。如果要构造一个AAC的Codec，代码如下：</p><pre><code class=\"language-plain\">MediaCodec mediaCodec = MediaCodec.createEncoderByType(\"audio/mp4a-latm\");\n</code></pre><p>上面这个方法，类似于前面讲到的FFmpeg中根据Codec的name来找出编码器，在构造出这个实例之后，就需要配置这个编码器了。配置编码器最重要的是传递一个MediaFormat类型的对象，这个对象里配置的是比特率、采样率、声道数以及编码AAC的Profile，此外，还要配置输入Buffer的最大值，代码如下：</p><pre><code class=\"language-plain\">MediaFormat encodeFormat = MediaFormat.createAudioFormat(MINE_TYPE, sampleRate, channels);\nencodeFormat.setInteger(MediaFormat.KEY_BIT_RATE, bitRate);\nencodeFormat.setInteger(MediaFormat.KEY_AAC_PROFILE, MediaCodecInfo.CodecProfileLevel.AACObjectLC);\nencodeFormat.setInteger(MediaFormat.KEY_MAX_INPUT_SIZE, 10 * 1024);\n</code></pre><p>然后将这个对象配置到编码器中。</p><pre><code class=\"language-plain\">mediaCodec.configure(encodeFormat, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);\n</code></pre><p>最后一个参数代表了我们要配置一个编码器，而非解码器<span class=\"reference\">（如果是解码器则传递为0）</span>。然后调用start方法就可以开启这个编码器了。</p><p>那接下来如何将PCM数据送给MediaCodec进行编码呢？我们先来看一下MediaCodec的工作原理图，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/10/d4/10701bd6e882b52cb33d10101138d3d4.png?wh=1440x694\" alt=\"图片\" title=\"MediaCodec的工作原理\n\"></p><p>左右两边的Client代表我们应用层书写代码的地方，左边Client元素代表需要应用层将PCM放到inputBuffers里某一个具体的buffer里去，右边的Client元素代表将编码之后的原始AAC数据从outputBuffers里一个具体的buffer中取出来。</p><p>开发者可以从MediaCodec实例中取出两个buffer来，一个是inputBuffers，用来存放输入的PCM数据<span class=\"reference\">（类似于FFmpeg编码的AVFrame）</span>；另外一个是outputBuffers，用来存放编码之后的原始AAC的数据<span class=\"reference\">（类似于FFmpeg编码的AVPacket）</span>。</p><pre><code class=\"language-plain\">ByteBuffer[] inputBuffers = mediaCodec.getInputBuffers();\nByteBuffer[] outputBuffers = mediaCodec.getOutputBuffers();\n</code></pre><p>初始化方法的核心实现完成了，接下来我们看一下编码方法的实现。</p><h3>编码</h3><p>首先，从MediaCodec里拿出一个空闲的InputBuffer，其实这里获取的不是一个buffer，而是一个buffer Index，然后从我们上面取出来的这个InputBuffers里面拿出buffer Index所代表的这个buffer，然后填充数据，并且把填充好的buffer送给Codec，就是调用Codec的codec.queueInputBuffer。</p><pre><code class=\"language-plain\">int bufferIndex = codec.dequeueInputBuffer(-1);\nif (inputBufferIndex &gt;= 0) {\n&nbsp;&nbsp;&nbsp; ByteBuffer inputBuffer = inputBuffers[bufferIndex];\n&nbsp;&nbsp;&nbsp; inputBuffer.clear();\n&nbsp;&nbsp;&nbsp; inputBuffer.put(data);\n&nbsp;&nbsp;&nbsp; long time = System.nanoTime();\n&nbsp;&nbsp;&nbsp; codec.queueInputBuffer(bufferIndex, 0, len, time, 0);\n}\n</code></pre><p>将PCM数据送给MediaCodec之后，那如何获取出编码之后的数据呢？我们只需要从Codec里面拿出一个编码好的buffer的Index，通过Index取出对应的outputBuffer，然后将数据读取出来就可以了。读取出来数据是裸的AAC的ES流，需要我们自己添加上ADTS头部，然后写文件，最后把这个outputBuffer放回待编码填充队列里面去。</p><pre><code class=\"language-plain\">BufferInfo info = new BufferInfo();\nint index = codec.dequeueOutputBuffer(info, 0);\nwhile (index &gt;= 0) {\n&nbsp;&nbsp;&nbsp; ByteBuffer outputBuffer = outputBuffers[index];\n&nbsp;&nbsp;&nbsp; if (outputAACDelegate != null) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int outPacketSize = info.size + 7;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; outputBuffer.position(info.offset);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; outputBuffer.limit(info.offset + info.size);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; byte[] outData = new byte[outPacketSize];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //添加ADTS 代码后面会贴上\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; addADTStoPacket(outData, outPacketSize);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; //将编码得到的AAC数据 取出到byte[]中 偏移量offset=7\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; outputBuffer.get(outData, 7, info.size);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; outputBuffer.position(info.offset);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; outputAACDelegate.outputAACPacket(outData);\n&nbsp;&nbsp;&nbsp; }\n&nbsp;&nbsp;&nbsp; codec.releaseOutputBuffer(index, false);\n&nbsp;&nbsp;&nbsp; index = mediaCodec.dequeueOutputBuffer(bufferInfo, 0);\n}\n</code></pre><p>上述代码中构造出了BufferInfo类型的对象，当从Codec的输出缓冲区拿取一个buffer的时候，Codec会把这个buffer的描述信息放到这个对象中，接下来我们读取数据以及确定数据大小都要根据这个对象的信息来处理。</p><p>这样周而复始地调用这个方法，就可以将PCM数据编码成AAC，并且交给Delegate把数据输出到文件中。在编码工作完成之后需要调用销毁方法，接下来我们就来看一下销毁方法。</p><h3>销毁</h3><p>使用完MediaCodec编码器之后，就需要停止运行并释放编码器。</p><pre><code class=\"language-plain\">if (null != mediaCodec) {\n&nbsp;&nbsp;&nbsp; mediaCodec.stop();\n&nbsp;&nbsp;&nbsp; mediaCodec.release();\n}\n</code></pre><p>这样整个类的核心实现我们就讲完了，外界调用端需要做的事情是先初始化这个类，然后读取PCM文件并调用这个类的编码方法，实现这个类的Delegate接口，在重写的方法中将输出带有ADTS头的AAC码流直接写文件，最终编码结束之后，调用这个类的停止编码方法。</p><h2>iOS平台的硬件编码器AudioToolbox</h2><p>在iOS平台上可使用AudioToolbox下面的Audio Converter Services来完成硬件编码，iOS平台提供的这个服务，看名字就知道它是一个转换服务，那么它能够提供哪几方面的转换呢？</p><ul>\n<li>PCM到PCM：可以转换位深度、采样率以及表示格式，也包括交错存储和平铺存储之间的转换，这与FFmpeg里的重采样器非常类似。</li>\n<li>PCM到压缩格式的转换：相当于一个编解码器，当然可以应用在编解码场景。</li>\n</ul><p>iOS平台在多媒体方面提供的API是非常强大的，并且兼容性也非常好。随着学习的深入，你会逐渐了解更多更好用的API，而这节课，我们利用的就是它所提供的编码服务，将PCM数据编码为AAC格式的数据。</p><p>AudioToolbox里编码出来的AAC数据也是裸数据，在写入文件之前也需要手动添加上ADTS头信息，最终写出的文件才可以被系统播放器播放，具体添加头信息的操作和Android平台的操作是一样的，这里就不重复了。类似于软件编码提供的三个接口方法，这里也提供三个接口方法，分别完成初始化、编码数据和销毁编码器操作。</p><p>在介绍这三个方法的实现之前，我们先定义一个Protocol，名称定为FillDataDelegate，需要调用端来实现这个Delegate，这里面定义了三个方法：</p><p>我们先看第一个方法的方法原型。</p><pre><code class=\"language-plain\">- (UInt32) fillAudioData:(uint8_t*) sampleBuffer bufferSize:(UInt32) bufferSize;\n</code></pre><p>当编码器<span class=\"reference\">（转换器）</span>需要编码一段PCM数据的时候，就通过调用这个方法来让实现这个Delegate的调用端来填充PCM数据。</p><p>再看第二个方法的方法原型。</p><pre><code class=\"language-plain\">- (void) outputAACPakcet:(NSData*) data presentationTimeMills:(int64_t)presentationTimeMills error:(NSError*) error;\n</code></pre><p>待编码器成功编码出一帧AAC的Packet之后，我们先给这段数据添加ADTS头，然后通过调用上面这个方法来让调用端输出编码后的数据。</p><p>最后一个方法的方法原型是onCompletion。</p><pre><code class=\"language-plain\">- (void) onCompletion;\n</code></pre><p>待编码<span class=\"reference\">（转换）</span>结束之后，调用上面这个方法来让调用端做资源销毁和关闭IO等操作。</p><p>接下来我们来看一下这三个接口的核心实现。</p><h3>初始化</h3><p>还记得之前我们讲过，在iOS平台提供的音视频API中，如果需要用到硬件Device相关的API，就需要配置各种Session。而如果要用到与配置相关API，一般就需要配置各种的Description来描述配置的信息。而在这里需要配置的Description就是我们<a href=\"http://time.geekbang.org/column/article/543649\">第一节课iOS平台的音频格式</a>里讲过的。这里需要分别配置一个input和一个output部分的Description，用来描述编码前后的音频格式。</p><p>首先是输入部分的Description的配置。</p><pre><code class=\"language-plain\">//构建InputABSD\nAudioStreamBasicDescription inASBD = {0};\nUInt32 bytesPerSample = sizeof (SInt16);\ninASBD.mFormatID = kAudioFormatLinearPCM;\ninASBD.mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagIsPacked;\ninASBD.mBytesPerPacket = bytesPerSample * channels;\ninASBD.mBytesPerFrame = bytesPerSample * channels;\ninASBD.mChannelsPerFrame = channels;\ninASBD.mFramesPerPacket = 1;\ninASBD.mBitsPerChannel = 8 * channels;\ninASBD.mSampleRate = inputSampleRate;\ninASBD.mReserved = 0;\n</code></pre><p>可以看到，这里输入的是PCM格式，表示格式是有符号整型并且是交错存储的，这一点十分关键，因为要按照设置的格式填充PCM数据，或者反过来说，客户端代码填充的PCM数据是什么格式的，就应该在这里配置给input，描述的mFormatFlags是什么。存储格式是指交错存储或非交错存储，输出或者输入数据都存储于AudioBufferList中的属性ioData中。假设声道是双声道的，对于交错存储<span class=\"reference\">（IsPacked）</span>来讲，对应的数据格式分布如下：</p><pre><code class=\"language-plain\">\tioData-&gt;mBuffers[0]: LRLRLRLRLRLR…\n</code></pre><p>而对于非交错的存储（NonInterleaved）来讲，数据格式分布如下：</p><pre><code class=\"language-plain\">\tioData-&gt;mBuffers[0]: LLLLLLLLLLLL…\n\tioData-&gt;mBuffers[1]: RRRRRRRRRRRR…\n</code></pre><p>这也就要求客户端代码，要按照配置的格式描述进行填充或者获取数据，否则就会出现不可预知的问题。由于我们提供的数据就是交错存储的，所以Description后续几个关键值都得乘以channels。这样输入的配置就已经书写好了，我们再来看一下输出部分的Description的配置。</p><pre><code class=\"language-plain\">//构造OutputABSD\nAudioStreamBasicDescription outASBD = {0};\noutASBD.mSampleRate = inASBD.mSampleRate;\noutASBD.mFormatID = kAudioFormatMPEG4AAC;\noutASBD.mFormatFlags = kMPEG4Object_AAC_LC;\noutASBD.mBytesPerPacket = 0;\noutASBD.mFramesPerPacket = 1024;\noutASBD.mBytesPerFrame = 0;\noutASBD.mChannelsPerFrame = inASBD.mChannelsPerFrame;\noutASBD.mBitsPerChannel = 0;\noutASBD.mReserved = 0;\n</code></pre><p>上面需要注意的是，mFormatID需要配置成AAC的编码格式， Profile配置为低运算复杂度的规格<span class=\"reference\">（LC）</span>，因为这样兼容性最好。另外，配置一帧数据的地方需要填写1024，这是AAC编码格式要求的帧大小。</p><p>到这里，输入和输出的Description就配置好了，接下来就需要构造一个编码器实例了，但是构造编码器实例也得从配置一个编码器描述开始。编码器的描述指定编码器类型是kAudioFormatMPEG4AAC，编码的实现方式使用兼容性更好的软件编码方式kAppleSoftwareAudioCodecManufacturer。通过这两个输入构造出一个编码器类的描述，它可以告诉iOS系统开发者到底想使用哪个编码器。</p><p><img src=\"https://static001.geekbang.org/resource/image/f0/05/f0f4dc77bc1a6d0cfc1d5592e382c805.png?wh=1920x900\" alt=\"图片\"></p><p>有了上述的三个Description<span class=\"reference\">（输入和输出数据的描述、编码器的描述）</span>，就可以构造出一个AudioConverterRef实例了，代码如下：</p><pre><code class=\"language-plain\">OSStatus status = AudioConverterNewSpecific(&amp;inABSD, &amp;outABSD, 1, codecDescription, &amp;_audioConverter);\n</code></pre><p>第三个参数1是指明要创建编码器的个数，最后一个参数就是我们想要构造的转码器实例对象，返回值是OSStatus类型的变量，如果返回的不是0，则表示出错了，成功的话返回常量noErr。</p><p>接下来就可以对这个转码实例设置比特率了。</p><pre><code class=\"language-plain\">AudioConverterSetProperty(_audioConverter, kAudioConverterEncodeBitRate, sizeof(bitRate), &amp;bitRate);\n</code></pre><p>要获取编码之后输出的AAC的Packet size最大值是多少，因为我们需要按照这个值来分配编码后数据的存储空间，从而让编码器输出到这个存储区域里面来，代码如下：</p><pre><code class=\"language-plain\">UInt32 size = sizeof(_aacBufferSize);\nAudioConverterGetProperty(_audioConverter, kAudioConverterPropertyMaximumOutputPacketSize, &amp;size, &amp;_aacBufferSize);\n_aacBuffer = malloc(_aacBufferSize * sizeof(uint8_t));\nmemset(_aacBuffer, 0, _aacBufferSize);\n</code></pre><p>到这里，初始化方法就结束了，这里面除了分配出编码器还有给编码器设置比特率之外，还根据输出的AAC的Packet大小分配了存储AAC的Packet的空间。接下来我们看编码方法的实现。</p><h3>编码</h3><p>第一步要用前面初始化好的_aacBuffer构造出一个AudioBufferList的结构体，作为编码器输出AAC数据的存储容器，代码如下：</p><pre><code class=\"language-plain\">AudioBufferList outAudioBufferList = {0};\noutAudioBufferList.mNumberBuffers = 1;\noutAudioBufferList.mBuffers[0].mNumberChannels = _channels;\noutAudioBufferList.mBuffers[0].mDataByteSize = _aacBufferSize;\noutAudioBufferList.mBuffers[0].mData = _aacBuffer;\n</code></pre><p>构造出了这个结构体之后，就可以去调用编码器的编码函数了。但是你可能会问，我们还没有拿到PCM数据，怎么调用编码器编码呢，数据源从哪里来呢？</p><p>AudioToolbox这里的设计是按照回调函数的方式获取数据源，而这个回调函数就设置在编码函数调用中，编码函数调用如下：</p><pre><code class=\"language-plain\">UInt32 ioOutputDataPacketSize = 1;\nOSStatus status = AudioConverterFillComplexBuffer(\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _audioConverter, inInputDataProc, (__bridge void *)(self),\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;ioOutputDataPacketSize, &amp;outAudioBufferList, NULL);\n</code></pre><p>函数中，第一个参数是编码器；第二个参数就是用来填充PCM数据的回调函数，第三个参数就是对象本身，一般回调函数都会传一个Context进去，便于调用本对象的方法；接下来的参数就是输出的AAC Packet的大小，还有编码之后的AAC Packet存放的容器；最后一个参数是输出AAC Packet的Description，一般填充为NULL。</p><p>接下来我们看一下这个回调函数的原型，以及在回调函数中我们如何填充PCM数据。回调函数的原型如下：</p><pre><code class=\"language-plain\">OSStatus inInputDataProc(AudioConverterRef inAudioConverter, UInt32\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *ioNumberDataPackets, AudioBufferList *ioData,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AudioStreamPacketDescription **outDataPacketDescription,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; void *inUserData)\n</code></pre><p>这个回调函数中的第一个参数是编码器的实例；第二个参数是需要填充多少个PCM数据的数量；第三个参数就是应用层要填充PCM数据的容器；第四个参数是填充输出Packet的Description，但是在这里不使用；最后一个参数就是上下文，即在调用编码函数<span class=\"reference\">（转换函数）</span>的时候传入的对象本身。这里我们直接将inUserData强制转换成这个类型的一个实例对象，就可以调用这个对象的方法了，代码如下：</p><pre><code class=\"language-plain\">AudioToolboxEncoder *encoder = (__bridge AudioToolboxEncoder *)(inUserData);\n[encoder fillAudioRawData:ioData ioNumberDataPackets:ioNumberDataPackets];\n</code></pre><p>在这个静态的回调函数里，通过上下文对象的强制类型转换，就可以得到对象本身，进而可以调用fillAudioRawData这个方法。</p><p>接下来我们看下这个方法的具体实现，首先根据需要填充的帧的数目、当前声道数以及表示格式，计算出需要填充的uint8_t类型的buffer大小。</p><pre><code class=\"language-plain\">int bufferLength = ioNumberDataPackets * channels * sizeof(short);\n</code></pre><p>根据公式算出来的bufferLength来分配出pcmBuffer，然后调用Delegate里面的fillAudioData:bufferSize:方法来填充数据，最后将Delegate中填充好的pcmBuffer放入ioData容器中并返回，这样就完成了给编码器提供PCM数据的回调函数。</p><p>编码函数执行结束之后，如果status是noErr，那编码好的AAC数据就存放在前面定义好的outAudioBufferList这个结构体里了。</p><p>从这个结构体的属性mBuffers[0].mData中，拿出AAC的原始Packet，添加上ADTS头信息，然后调用Delegate的方法outputAACPakcet，由Delegate输出AAC数据，这时候的AAC数据就是编码之后带有ADTS头信息的数据。最终如果输入数据为空，则代表结束，我们就可以调用Delegate的方法onCompletion，让调用端做一些资源关闭以及销毁操作了。</p><h3>销毁</h3><p>最后来看销毁接口的实现，我们先把分配的填充PCM数据的pcmBuffer释放掉，然后把分配的接收编码器输出的aacBuffer释放掉，最后释放掉编码器，代码如下：</p><pre><code class=\"language-plain\">if(_pcmBuffer) {\n&nbsp;&nbsp;&nbsp; free(_pcmBuffer);\n&nbsp;&nbsp;&nbsp; _pcmBuffer = NULL;\n}\nif(_aacBuffer) {\n&nbsp;&nbsp;&nbsp; free(_aacBuffer);\n&nbsp;&nbsp;&nbsp; _aacBuffer = NULL;\n}\nAudioConverterDispose(_audioConverter);\n</code></pre><p>到这里，这三个接口方法就全部实现了。</p><p>最后我们看集成阶段。首先，调用端需要实现这个类中定义的FillDataDelegate类型的Protocol，并且要重写里面的fillAudioData方法，以便给这个编码器类提供PCM数据。接着重写outputAACPacket方法来输出AAC的码流数据，重写onCompletion方法来关闭自己读写文件等操作；然后实例化我们的编码器，开启一个线程<span class=\"reference\">（使用GCD）</span>来调用编码方法；最终编码结束之后，在dealloc方法中调用结束编码的方法。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><p>这节课我们使用Android平台的硬件编码器MediaCodec和iOS平台的硬件编码器AudioToolbox来编码AAC，最终得到AAC编码格式的数据。相比于上节课我们用软件编码器编码AAC，使用硬件编码器可以有效提升我们的编码效率，节约CPU资源。</p><p>在iOS平台，你可以尝试使用硬件编码来实现速度的优化，当然也可以使用ExtAudioFile等API接口来操作。Android平台碎片化比较严重，可能会存在兼容性问题，所以我更推荐你直接使用软件编码，因为音频编码对于CPU的消耗或者计算性能要求并不会太高。</p><h2>思考题</h2><p>这两节课我们重点讲解了音频的AAC编码，那么我来考考你，AAC常用的编码规格中，HE-AAC和LC-AAC两种编码的每一帧音频帧的时长是多少呢？做一个实验来看看吧，欢迎在评论区分享你的答案，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"12｜如何编码出一个AAC文件？","id":553115},"right":{"article_title":"14 ｜ iOS平台如何采集视频画面？","id":556012}}},{"article_id":556012,"article_title":"14 ｜ iOS平台如何采集视频画面？","article_content":"<p>你好，我是展晓凯。今天我们一起来学习iOS平台的视频画面采集。</p><p>前面我们学习的音频采集与编码的方法，可以用来实现音频录制器的功能。但如果要完成视频录制器的功能我们还需要掌握视频采集与编码方面的内容，所以从今天开始我们来学习如何采集视频的画面。</p><p>采集到视频画面之后一般会给用户预览出来，这就要结合之前我们学过的<a href=\"https://time.geekbang.org/column/article/545953\">视频画面渲染</a>方面的知识，再加上视频的编码，这样就可以在用户点击录制的时候给视频画面编码并且存储到本地了。这节课我们就先来一起学习在iOS平台如何采集视频画面。</p><h2>视频框架ELImage架构设计</h2><p>在iOS平台使用Camera来采集视频画面的API接口比较简单，但要设计出一个优秀的、可扩展的架构，也不是一件容易的事情。所以这节课我会带你设计并实现出一个架构，这个架构基于摄像头采集驱动，中间可以支持视频特效处理，最终用OpenGL ES渲染到UIView上，且支持扩展插入编码节点。我们先来看一下整体的架构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/43/2f/43da58864042499a170ba75d96d9352f.png?wh=1694x636\" alt=\"图片\" title=\"架构图\"></p><p>左边第一个节点是用系统提供的Camera接口，采集出一帧内存中的图像，然后将这个图像上传到显存中成为YUV的纹理对象，最后将这个YUV格式的纹理重新渲染到一个RGBA的纹理上。接着将这个RGBA类型的纹理对象传到中间的Filters节点，这个节点内部会使用OpenGL ES来处理这个纹理对象，最后输出一个纹理对象到下面的节点。</p><!-- [[[read_end]]] --><p>下一级节点是GLImageView或将来扩展出来的组件VideoEncoder，拿到中间Filters节点输出之后，进行屏幕渲染或者编码操作。这样一帧图像就从采集、处理到最后预览让用户看到就完成了，并且可以满足我们之后做编码以及图像处理的需求。</p><p>继续看图，你会发现Camera和Filter这些节点是可以输出纹理对象的，也就是它们的目标纹理对象要作为后一级节点的输入纹理对象；另外，Filter、GLImageView以及VideoEncoder需要上一级节点提供输入纹理对象。通过以上两个特点，我们就可以抽象出两个规则。</p><ol>\n<li><strong>凡是需要输入纹理对象的，都是Input类型。</strong></li>\n<li><strong>凡是需要向后级节点输出纹理对象的，都是Output类型</strong>。</li>\n</ol><h3>ELImageInput</h3><p>基于规则一，我们可以定义出ELImageInput这样一个Protocol，因为需要别的组件给它输入纹理对象，所以这个Protocol里面定义了两个方法，第一个方法是设置输入的纹理对象。</p><pre><code class=\"language-plain\">- (void)setInputTexture:(ELImageTextureFrame *)textureFrame;\n</code></pre><p>节点中的Filter、GLImageView以及VideoEncoder都属于ELImageInput的类型，所以都应该实现这个方法，在这个方法的实现中应该将输入纹理对象保存为一个属性，等绘制的时候使用。此外，这些节点还有一个共同点，就是都需要做渲染操作，所以接下来第二个方法是执行渲染操作。</p><pre><code class=\"language-plain\">- (void)newFrameReadyAtTime:(CMTime)frameTime timimgInfo:(CMSampleTimingInfo)timimgInfo;\n</code></pre><p>这是上一级节点<span class=\"reference\">（实际上是一个Output节点）</span>处理完毕之后要调用的方法，在这个方法的实现中可以完成渲染操作。</p><h3>ELImageOutput</h3><p>基于规则二，我们再建立一个类——ELImageOutput，这个类可以向自己的后级节点输出目标纹理对象，其中Camera、Filter节点是需要继承自这个类。根据这个特点我们建立两个属性，一个是渲染目标的纹理对象，一个是后级节点列表，代码如下：</p><pre><code class=\"language-plain\">ELImageTextureFrame *outputTexture;\nNSMutableArray *targets;\n</code></pre><p>为什么后级节点是列表类型的呢？<br>\n因为后级节点可能有多个目标对象，比如Filter节点，既要输出给GLImageView，又要输出给VideoEncoder，而这个targets里面的对象，实际上就是之前定义的协议ELImageInput类型的对象，因为Output节点的后级肯定是一个Input类型的对象。既然有一个targets，就需要提供增加和删除目标节点的方法。</p><pre><code class=\"language-plain\">- (void)addTarget:(id&lt;ELImageInput&gt;)target;\n- (void)removeTarget:(id&lt;ELImageInput&gt;)target;\n</code></pre><p>我们用这两个方法来操作targets属性。<br>\n每一个真正继承这个类的节点，执行渲染过程结束之后，就会遍历targets里面所有的目标节点<span class=\"reference\">（即ELImageInput）</span>执行设置输出纹理对象的方法，然后执行下一个节点的渲染过程。代码如下：</p><pre><code class=\"language-plain\">//Do Render Work\nfor (id&lt;ELImageInput&gt; currentTarget in targets){\n&nbsp;&nbsp;&nbsp; [currentTarget setInputTexture:outputTextureFrame];\n&nbsp;&nbsp;&nbsp; [currentTarget newFrameReadyAtTime:frameTime timimgInfo:timimgInfo];\n}\n</code></pre><h3>ELImageProgram</h3><p>每一个节点的处理都是一个OpenGL的渲染过程，所以每个节点都需要建立一个GLProgram。我们不可能在每一个节点里面都去分别书写编译Shader、链接Program等代码，所以要先抽取出一个类，取名为ELImageProgram<span class=\"reference\">（EL是整个项目的前缀）</span>，把GLProgram的构建、查找属性、使用等这些操作以面向对象的形式封装起来，每一个节点都会组合这个类。</p><h3>ELImageTextureFrame</h3><p>每一个节点的输入都是一个纹理对象<span class=\"reference\">（实际上是一个纹理ID）</span>，使用GLProgram将这个纹理对象渲染到一个目标纹理对象的时候，还需要建立一个帧缓存对象<span class=\"reference\">（FBO）</span>，并且要将这个目标纹理对象Attach到这个帧缓存对象上。所以这里我们抽取一个类，取名为ELImageTextureFrame，将纹理对象和帧缓存对象的创建、绑定、销毁等操作，以面向对象的方式封装起来，让每一个节点使用起来都更加方便。</p><h3>ELImageContext</h3><p>要想使用OpenGL ES，必须有上下文以及关联的线程，之前我们也提到过iOS平台为OpenGL ES提供了EAGL作为OpenGL ES的上下文。后面我们书写编码器组件的时候，因为不希望它阻塞预览线程，所以需要单独开辟一个编码线程，也需要一个额外的OpenGL上下文，并且需要和渲染线程共享OpenGL上下文。只有这样，在编码线程中才可以正确访问到预览线程中的纹理对象、帧缓存对象。</p><p>所以我们抽取一个类，取名叫做ELImageContext，来封装EAGLContext和渲染线程。因为可能多个对象都要在调用线程和OpenGL ES的线程之间进行切换，所以需要给这个类书写一个静态方法，获得渲染线程的OpenGL上下文，并且提供静态方法可以得到这个dispatch_queue，让一些OpenGL ES的操作可以直接在这个线程中执行，具体的代码如下：</p><pre><code class=\"language-plain\">+ (void)useImageProcessingContext;\n{\n&nbsp;&nbsp;&nbsp; [[ELImageContext sharedImageProcessingContext] useAsCurrentContext];\n}\n- (void)useAsCurrentContext;\n{\n&nbsp;&nbsp;&nbsp; EAGLContext *imageProcessingContext = [self context];\n&nbsp;&nbsp;&nbsp; if ([EAGLContext currentContext] != imageProcessingContext)\n&nbsp;&nbsp;&nbsp; {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [EAGLContext setCurrentContext:imageProcessingContext];\n&nbsp;&nbsp;&nbsp; }\n}\n</code></pre><p>基于以上分析，我们画出了对应的类图。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/82/695b81a17036848ff70224cf4b365782.png?wh=1792x922\" alt=\"图片\"></p><p>GLImageView部分，你可以回顾之前<a href=\"https://time.geekbang.org/column/article/545953\">视频渲染方面</a>的内容，这里也就不再赘述了。第18节课我们将实现VideoEncoder，后面你可以自己去实现Filter，所以这节课我们只实现Camera。Camera的实现我会分两部分讲解，第一部分是摄像头的配置，第二部分是将摄像头采集到的YUV数据转换为纹理对象。</p><h2>摄像头配置</h2><p>我们一直反复强调，在iOS平台只要是与硬件相关的使用都要从会话开始配置，所以摄像头这里需要配置AVCaptureSession。</p><pre><code class=\"language-plain\">AVCaptureSession* captureSession;\ncaptureSession = [[AVCaptureSession alloc] init];\n</code></pre><p>然后需要配置出AVCaptureDeviceInput，这个对象代表了我们要使用哪个摄像头，比如使用前置摄像头。</p><pre><code class=\"language-plain\">AVCaptureDevice * captureDevice = nil;\nNSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];\nfor (AVCaptureDevice *device in devices) {\n&nbsp;&nbsp;&nbsp; if ([device position] == AVCaptureDevicePositionFront) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; captureDevice = device;\n&nbsp;&nbsp;&nbsp; }\n}\ncaptureInput = [[AVCaptureDeviceInput alloc] initWithDevice:captureDevice error:nil];\n</code></pre><p>接着要配置出AVCaptureVideoDataOutput，这个对象用来处理摄像头采集到的数据。</p><pre><code class=\"language-plain\">dispatch_queue_t dataCallbackQueue;\ndataCallbackQueue = dispatch_queue_create(\"dataCallbackQueue\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DISPATCH_QUEUE_SERIAL);\ncaptureOutput = [[AVCaptureVideoDataOutput alloc] init];\n[_captureOutput setSampleBufferDelegate:self queue:dataCallbackQueue];\n</code></pre><p>构建出captureOutput这个实例之后，要想获取摄像头采集的数据，就需要传入类型为AVCaptureVideoDataOutputSampleBufferDelegate的实例和一个dispatch_queue。</p><p>接下来，我们需要设置一下像素格式，默认使用<strong>YUVFullRange</strong>的表示格式，所谓的FullRange，表示YUV的取值范围是0到255；还有一种是<strong>YUVVideoRange</strong>的表示格式，为了防止溢出，我们把YUV的取值范围设置成16到235。Range的类型会决定YUV格式转换为RGBA格式时使用的矩阵，所以这里我们要根据支持的格式来设置，并且记录设置的格式，之后用来确定YUV到RGBA的转换矩阵。</p><p>接着将captureInput实例和captureOutput实例配置到CaptureSession中。</p><pre><code class=\"language-plain\">if ([self.captureSession canAddInput:self.captureInput]) {\n&nbsp;&nbsp;&nbsp; &nbsp;[self.captureSession addInput:self.captureInput];\n&nbsp; }\nif ([self.captureSession canAddOutput:self.captureOutput]) {\n&nbsp;&nbsp;&nbsp; &nbsp;[self.captureSession addOutput:self.captureOutput];\n}\n</code></pre><p>然后调用captureSession设置分辨率的方法，你可以看一下常见的分辨率以及设置代码。</p><pre><code class=\"language-plain\">NSString* highResolution = AVCaptureSessionPreset1280x720;\nNSString* lowResolution = AVCaptureSessionPreset640x480;\n[_captureSession setSessionPreset:[NSString stringWithString: highResolution]];\n</code></pre><p>接着调用CaptureSession的beginConfiguration方法，配置整个摄像头会话，最后取出captureOutput里面的AVCaptureConnection，来配置摄像头输出的方向，这是非常重要的，如果不配置这个参数，摄像头默认输出横向的图片，我们使用代码把它设置成纵向图片输出。</p><pre><code class=\"language-plain\">conn.videoOrientation = AVCaptureVideoOrientationPortrait;\n</code></pre><p>当然，也可以给CaptureInput设置帧率等信息，这里就不再赘述。</p><h2>摄像头采集数据处理</h2><p>前面我们已经实现了AVCaptureVideoDataOutputSampleBufferDelegate这个协议，重写了接收摄像头采集数据的方法，签名如下：</p><pre><code class=\"language-plain\">-(void) captureOutput:(AVCaptureOutput*)captureOutput\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fromConnection:(AVCaptureConnection*)connection\n</code></pre><p>这个方法会把具体是哪一个captureOutput以及connection返回过来，但是最重要的其实是CMSampleBuffer类型的sampleBuffer，这里面存储着摄像头采集到的图像。一个CMSampleBuffer由以下三部分组成：</p><ul>\n<li>CMTime，代表这一帧图像的时间。</li>\n<li>CMVideoFormatDescription，代表对这一帧图像格式的描述。</li>\n<li>CVPixelBuffer，代表这一帧图像的具体数据。</li>\n</ul><p>在这个回调函数里，我们需要完成从摄像头采集到图像渲染的全过程，而渲染部分会使用OpenGL ES来操作。</p><p>之前我们也提到过一个问题，iOS平台不允许App进入后台的时候还执行OpenGL渲染，通用的处理方式就是之前在播放器中用到的方式，分别注册applicationWillResignActive和applicationDidBecomeActive的通知，在这两个方法中将这个类中的shouldEnableOpenGL属性设置为NO和YES。然后在回调函数中判断这个变量，决定是否可以执行OpenGL的操作。</p><pre><code class=\"language-plain\">-(void) captureOutput:(AVCaptureOutput*)captureOutput didOutputSampleBuffer:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (CMSampleBufferRef)sampleBuffer fromConnection:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (AVCaptureConnection*)connection {\n&nbsp;&nbsp;&nbsp; if (self.shouldEnableOpenGL) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (dispatch_semaphore_wait(_frameRenderingSemaphore,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DISPATCH_TIME_NOW) != 0) {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return;\n&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CFRetain(sampleBuffer);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; runAsyncOnVideoProcessingQueue(^{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [self processVideoSampleBuffer:sampleBuffer];\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CFRelease(sampleBuffer);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dispatch_semaphore_signal(_frameRenderingSemaphore);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; });\n&nbsp;&nbsp;&nbsp; }\n}\n</code></pre><p>代码显示，首先要判断这个布尔型的变量，如果是NO的话，就不执行任何操作；如果是YES的话，要保证上一次渲染执行结束了<span class=\"reference\">（通过dispatch_semaphore的wait来确定）</span>才可以执行本次渲染操作。</p><p>执行渲染操作的时候首先要使用CFRetain锁定这个sampleBuffer，因为真正使用sampleBuffer的地方是在OpenGL ES线程中，只有这里Retain住才能保证sampleBuffer不被污染，等这一次OpenGL ES的渲染操作结束以后，再使用CFRelease释放这个sampleBuffer。最后给semaphore发一个signal指令。</p><p>接下来我们看一下真正的渲染操作，也就是方法processVideoSampleBuffer的实现，这个方法需要将sampleBuffer对象渲染成为一个纹理对象，然后调用后续的targets节点进行渲染。</p><p>我们先取出这个sampleBuffer中的图像数据，即它的属性CVPixelBuffer，然后我们需要确定这个CVPixelBuffer里面YUV转换成RGB的矩阵。我们根据两方面的内容来确定这个矩阵。</p><ul>\n<li>给摄像头配置的像素格式，是YUVFullRange还是YUVVideoRange；</li>\n<li>取出PixelBuffer里面的YUV转换类型，我们判断这个类型是ITU601还是ITU709格式，ITU601是SDTV的标准，而ITU709是HDTV的标准，因为标清与高清对应的YUV转换为RGB的矩阵是不同的。</li>\n</ul><p>这两方面共同决定了YUV转换成RGB的矩阵，其中只有ITU601分为YUVFullRange和YUVVideoRange两种，而ITU709就只有一种，根据组合从以下三个矩阵中选出合适的矩阵。</p><pre><code class=\"language-plain\">GLfloat colorConversion601Default[] = {\n&nbsp;&nbsp;&nbsp; 1.164,&nbsp; 1.164, &nbsp;&nbsp;1.164,\n&nbsp;&nbsp;&nbsp; 0.0, &nbsp;&nbsp;&nbsp;-0.392, &nbsp;2.017,\n&nbsp;&nbsp;&nbsp; 1.596, -0.813,&nbsp;&nbsp; 0.0,\n};\nGLfloat colorConversion601FullRangeDefault[] = {\n&nbsp;&nbsp;&nbsp; 1.0,&nbsp;&nbsp;&nbsp; 1.0,&nbsp;&nbsp;&nbsp; &nbsp;1.0,\n&nbsp;&nbsp;&nbsp; 0.0,&nbsp;&nbsp;&nbsp; -0.343, 1.765,\n&nbsp;&nbsp;&nbsp; 1.4,&nbsp;&nbsp;&nbsp; -0.711, 0.0,\n};\nGLfloat colorConversion709Default[] = {\n&nbsp;&nbsp;&nbsp; 1.164,&nbsp; 1.164, &nbsp;&nbsp;1.164,\n&nbsp;&nbsp;&nbsp; 0.0, &nbsp;&nbsp;&nbsp;-0.213, &nbsp;2.112,\n&nbsp;&nbsp;&nbsp; 1.793, -0.533,&nbsp;&nbsp; 0.0,\n};\n</code></pre><p>准备工作完成之后，接下来就进入真正渲染过程。</p><p>先绑定OpenGL ES的上下文，需要调用前面我们封装的ELImageContext绑定上下文的方法。然后创建这个节点的输出纹理对象，就是构建一个ELImageTextureFrame对象，并且激活这个纹理对象<span class=\"reference\">（代表这个渲染过程的目标就是这个纹理对象）</span>。</p><p>准备输入纹理，要将CVPixelBuffer中的YUV数据关联到两个纹理ID上。如果是在其他平台上，只能通过OpenGL ES提供的glTexImage2D方法，将内存中的数据上传到显卡的一个纹理ID上。但是这种内存和显存之间的数据交换效率是比较低的，在iOS平台的CoreVideo framework中提供了CVOpenGLESTextureCacheCreateTextureFromImage方法，可以使整个交换过程更加高效。</p><p>由于CVPixelBuffer内部数据是YUV数据格式的，所以可分配以下两个纹理对象分别存储Y和UV的数据。</p><pre><code class=\"language-plain\">CVOpenGLESTextureRef luminanceTextureRef = NULL;\nCVOpenGLESTextureRef chrominanceTextureRef = NULL;\n</code></pre><p>需要在使用CVPixelBuffer这块内存区域之前，先锁定这个对象，使用完毕之后解锁。以下代码可锁定这个PixelBuffer。</p><pre><code class=\"language-plain\">CVPixelBufferLockBaseAddress(pixelBuffer, 0);\n</code></pre><p>然后拿出这里面的Y通道部分的内容，上传到luminanceTexture里。</p><pre><code class=\"language-plain\">CVOpenGLESTextureCacheCreateTextureFromImage(\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kCFAllocatorDefault, coreVideoTextureCache, pixelBuffer,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, GL_TEXTURE_2D, GL_LUMINANCE, bufferWidth,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bufferHeight, GL_LUMINANCE, GL_UNSIGNED_BYTE, 0,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;luminanceTextureRef);\n</code></pre><p>代码中传入了pixelBuffer以及格式GL_LUMINANCE，还需要传入宽和高。这样这个API内部就知道访问pixelBuffer的哪部分数据了，里面还有一个非常重要的参数，就是纹理缓存，而创建的纹理会从这个纹理缓存中拿出来，纹理缓存的创建代码如下：</p><pre><code class=\"language-plain\">CVOpenGLESTextureCacheCreate(kCFAllocatorDefault, NULL, context, NULL, &amp;coreVideoTextureCache)\n</code></pre><p>可以看到，创建纹理缓存必须要传入OpenGL上下文，所以我们一般在ELImageContext中维护一个纹理缓存。使用Y通道的数据内容创建出来的纹理对象可以通过CVOpenGLESTextureGetName来获取出纹理ID。以下代码可以把UV通道部分上传到chrominanceTextureRef里。</p><pre><code class=\"language-plain\">CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault, coreVideoTextureCache, pixelBuffer,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NULL, GL_TEXTURE_2D, GL_LUMINANCE_ALPHA, bufferWidth/2,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bufferHeight/2, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, 1,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;chrominanceTextureRef)\n</code></pre><p>YUV420P格式规定，<strong>每四个像素会有一个U和一个V，用GL_LUMINANCE_ALPHA来表示UV部分，即U放到Luminance部分，V放到Alpha部分</strong>。理解这一点是非常重要的，因为这关乎后面在FragmentShader中如何拿到正确的YUV数据。</p><p>接下来，就是实际的渲染了，在渲染过程中需要注意两点，一是确定物体坐标和纹理坐标；二是在FragmentShader中，怎么把YUV转换成RGBA的表示格式。我们先来看如何确定物体坐标和纹理坐标，物体坐标其实是固定的，物体坐标如下：</p><pre><code class=\"language-plain\">GLfloat squareVertices[8] = {\n&nbsp;&nbsp;&nbsp; &nbsp; -1.0,&nbsp; -1.0, //物体左下角\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;1.0，&nbsp; -1.0，//物体右下角\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1.0， 1.0， //物体左上角\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，&nbsp; 1.0&nbsp; &nbsp;//物体右上角\n};\n</code></pre><p>而确定纹理坐标会麻烦一点，记得我们之前讲过，OpenGL纹理坐标系和计算机坐标系是不同的吗？所以默认情况下，纹理坐标如下：</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;0.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，0.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，0.0&nbsp;\n};\n</code></pre><p>进行旋转以及镜像的时候，都是根据这个纹理坐标来实施的。左边第一张图是前置摄像头采集到的图像。</p><p><img src=\"https://static001.geekbang.org/resource/image/55/67/55ba32b41ebb9c71e5f6bc4484563a67.png?wh=1656x520\" alt=\"图片\"></p><p>要想正确地显示，需要先按照顺时针旋转90度，由于是前置摄像头，还得做一个镜像处理，所以我们的纹理坐标如下：</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，0.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，0.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，1.0&nbsp;\n};\n</code></pre><p>如果是后置摄像头，那么我们应顺时针旋转90度。</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/1d/8b3b195d4337c405e20ca96f2b931a1d.png?wh=1388x544\" alt=\"图片\"></p><p>顺时针旋转90度的纹理坐标如下：</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp; &nbsp;&nbsp; 1.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，0.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，0.0&nbsp;\n};\n</code></pre><p>这里有一点需要特别注意的是，<strong>由于把纹理做了90度的旋转，所以目标纹理对象宽高和输入纹理（CVPixelBuffer）的宽高需要对调一下。</strong></p><p>上述纹理坐标是默认摄像头给出的图像纹理坐标，但前面我们给摄像头做过一个特殊的设置，就是给 AVCaptureConnection设置videoOrientation这个参数，摄像头默认是横向视频输出，当把这个参数设置为Portrait时，就要求摄像头按照竖直方向输出视频。这时候目标纹理对象的宽高和CVPixelBuffer的宽高一致了，那么后置摄像头采集出来的图像直接绘制即可。</p><p><img src=\"https://static001.geekbang.org/resource/image/63/66/63d39de16a151d2751658c0e1cdace66.png?wh=1340x556\" alt=\"图片\"></p><p>所对应的纹理坐标为：</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp; &nbsp;&nbsp;&nbsp; 0.0，1.0，\n&nbsp;&nbsp;&nbsp; &nbsp; 1.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0，0.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.0，0.0&nbsp;\n};\n</code></pre><p>而前置摄像头由于镜像的原因，所以绘制过程如下图所示。<br>\n<img src=\"https://static001.geekbang.org/resource/image/4f/29/4f4a14caa305cb4f9b3e7cb6f36a5529.png?wh=1374x538\" alt=\"图片\"></p><p>此时的纹理坐标正好和后置摄像头的每一个坐标的X点相反。</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，1.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0，0.0，\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0，0.0&nbsp;\n};\n</code></pre><p>确定物体坐标与纹理坐标之后，我们就来看一下如何在FragmentShader中，将YUV转换成RGBA。注意：这里是一定要转换为RGBA的，因为在OpenGL中通用的渲染纹理格式都是RGBA的，包括纹理处理、渲染到屏幕上以及最终编码器，都是以RGBA格式为基础进行转换和处理的。在播放器的部分我们已经做过一次YUV转RGB的操作，但是由于使用了CoreVideo这个framework下的快速上传，输入的纹理变得不一样了，所以我们来看下具体的FragmentShader。</p><pre><code class=\"language-plain\">varying highp vec2 textureCoordinate;\nuniform sampler2D luminanceTexture;\nuniform sampler2D chrominanceTexture;\nuniform mediump mat3 colorConversionMatrix;\nvoid main(){\n&nbsp;&nbsp;&nbsp; mediump vec3 yuv;\n&nbsp;&nbsp;&nbsp; lowp vec3 rgb;\n&nbsp;&nbsp;&nbsp; yuv.x = texture2D(luminanceTexture, textureCoordinate).r;\n&nbsp;&nbsp;&nbsp; yuv.yz = texture2D(chrominanceTexture, textureCoordinate).ra - vec2(0.5, 0.5);\n&nbsp;&nbsp;&nbsp; rgb = colorConversionMatrix * yuv;\n&nbsp;&nbsp;&nbsp; gl_FragColor = vec4(rgb, 1);\n}\n</code></pre><p>其中textureCoordinate是纹理坐标，而这两个sampler2D类型就是从CVPixelBuffer里面上传到显存中的纹理对象，3*3的矩阵就是前面根据像素格式，以及是否为FullRange选择的变换矩阵。我们重点看一下如何取出Y和UV。</p><ul>\n<li>由于luminanceTexture使用的是GL_LUMINANCE格式上传上来的纹理，所以这里使用texture2D函数拿出像素点之后，访问元素r就可以拿到Y通道的值了。</li>\n<li>而UV通道使用的是GL_LUMINANCE_ALPHA格式，通过texture2D取出像素点之后，访问元素r得到U的值，访问元素a得到V的值。但为什么UV值要减去0.5（换算为0-255就是减去127）？这是因为UV是色彩分量，当整张图片是黑白的时候，UV分量是默认值127，所以这里要先减去127，然后再转换成RGB，否则会出现色彩不匹配的错误。</li>\n</ul><p>最后使用传递进来的转换矩阵乘以YUV，得到这个像素点的RGBA表示格式，并赋值给gl_FragColor。</p><p>到这里，ELImageVideoCamera这个类的核心逻辑我们就学完了，最终这个节点会输出一个纹理ID，这个纹理ID可以直接渲染到ELImageView中让用户看到摄像头预览的效果。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><p>这节课我们设计并实现出一个ELImage视频框架的架构，包含ELImageInput、ELImageOutput、ELImageProgram、ELImageTextureFrame和ELImageContext五个基础类。然后基于这五个类可以扩展出众多组件。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/82/695b81a17036848ff70224cf4b365782.png?wh=1792x922\" alt=\"图片\"></p><p>我们利用框架核心完成了视频画面的采集，用的就是ELImageVideoCamera这个类。这个节点最终会输出一个RGBA类型的纹理，这个纹理可以给到后续的ELImageFilter节点做美颜处理，最终输出到ELImageView中，显示出来，而这个过程就是打开摄像头预览的过程。</p><p>ELImage这个核心框架很重要，我们需要好好“消化”这部分内容，因为这个视频框架后续会用到，并且基于这个视频框架，我们还会增加一些其他节点，比如ELImageVideoEncoder等，也就是当用户点击录制的时候，将纹理ID传入到Encoder节点就可以录制出视频文件来。</p><h2>思考题</h2><p>ELImage框架还可以扩展出许多其他的节点，组成复杂的场景，比如视频编辑器场景，如果让你书写一个ELImageMovie节点，然后与ELImageFilter以及ELImageView组合成视频编辑器，你将如何完成ELImageMovie的设计呢？在评论区中给出你的思考，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"13｜如何使用硬件编码器来编码 AAC？","id":554444},"right":{"article_title":"15 ｜ Android平台是如何采集视频画面的？","id":557500}}},{"article_id":557500,"article_title":"15 ｜ Android平台是如何采集视频画面的？","article_content":"<p>你好，我是展晓凯。今天我们来一起学习Android平台视频画面的采集。</p><p><a href=\"https://time.geekbang.org/column/article/556012\">上一节课</a>我们一起学习了iOS平台的视频画面采集，Android平台的采集相对来讲会更复杂一些，因为我们整个系统的核心部分都是在Native层构建的，所以这就会涉及JNI层的一些转换操作。不过不用担心，我会带着你一步步构建起整个系统。</p><h2>权限配置</h2><p>要想使用Android平台提供的摄像头，必须在配置文件里添加权限要求。</p><pre><code class=\"language-plain\">&lt;uses-permission android:name=\"android.permission.CAMERA\" /&gt;\n</code></pre><p>Android 6.0及以上的系统，需要动态申请权限。</p><pre><code class=\"language-plain\">if (ContextCompat.checkSelfPermission(MainActivity.this, android.Manifest.permission.CAMERA)!= PackageManager.PERMISSION_GRANTED){\n  //没有权限就在这里申请 \n  ActivityCompat.requestPermissions(MainActivity.this, new String[]{Manifest.permission.CAMERA}, CAMERA_OK); \n}else { \n  //说明已经获取到摄像头权限了 \n}\n</code></pre><!-- [[[read_end]]] --><p>随着Android系统的发展，摄像头API也有了非常多的变化，这里我们使用的是给Camera设置预览纹理的方式，而不是使用给Camera设置接收YUV数据回调的方式。这是因为得到纹理ID之后，进入我们的OpenGL ES渲染链路会更方便，同时视频滤镜处理、View的渲染也会方便一些。</p><h2>配置摄像头</h2><p>使用Android的摄像头采集数据，需要打开摄像头并进行一些配置，我们先来看打开摄像头的操作。</p><h3>打开摄像头</h3><p>Android平台提供的打开摄像头的接口如下：</p><pre><code class=\"language-plain\">public static Camera open(int cameraId)\n</code></pre><p>需要传入的参数就是摄像头的ID。我们知道，是先有的后置摄像头，后有的前置摄像头，甚至目前部分手机已经有了更多的辅助摄像头，所以摄像头的ID也是按先后顺序排列的，后置摄像头是0，前置摄像头是1，然后才是其他的摄像头，需要使用CameraInfo这个类里面的两个常量，CAMERA_FACING_BACK后置摄像头和CAMERA_FACING_FRONT前置摄像头</p><p>这个函数返回的就是一个摄像头的实例，如果返回的是NULL，或者抛出异常<span class=\"reference\">（因为不同厂商所给出的返回不一致）</span>，代表用户没有给这个应用授权，就不可以访问摄像头。拿到这个摄像头实例之后，要为这个摄像头实例做一些配置，参数的配置主要就是预览格式和预览的尺寸。先来看预览格式的设置。</p><h3>预览格式</h3><p>预览格式一般设置为NV21格式，实际上就是YUV420SP格式，即UV是交错<span class=\"reference\">（interleaved）</span>存放的，设置代码如下：</p><pre><code class=\"language-plain\">List&lt;Integer&gt; supportedPreviewFormats = parameters.getSupportedPreviewFormats();\nif (supportedPreviewFormats.contains(ImageFormat.NV21)) {\n&nbsp;&nbsp;&nbsp; parameters.setPreviewFormat(ImageFormat.NV21);\n} else {\n&nbsp;&nbsp;&nbsp; throw new CameraParamSettingException(\"视频参数设置错误:设置预览图像格式异常\");\n}\n</code></pre><p>代码显示，先取出摄像头支持的所有的预览格式，然后判断一下有没有我们要设定的格式，如果有，就设置进去；如果没有，就抛出异常，留给业务层处理。</p><h3>预览尺寸</h3><p>接下来是预览尺寸的设置，一般设置成1280<em>720，当然在某些特殊场景，也可以设置成640</em>480的分辨率，设置代码如下：</p><pre><code class=\"language-plain\">List&lt;Size&gt; supportedPreviewSizes = parameters.getSupportedPreviewSizes();\nint previewWidth = 640;//1280\nint previewHeight = 480;//720\nboolean isSupportPreviewSize = isSupportPreviewSize(\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; supportedPreviewSizes, previewWidth, previewHeight);\nif (isSupportPreviewSize) {\n&nbsp;&nbsp;&nbsp; parameters.setPreviewSize(previewWidth, previewHeight);\n} else {\n&nbsp;&nbsp;&nbsp; throw new CameraParamSettingException(\"视频参数设置错误:设置预览的尺寸异常\");\n}\n</code></pre><p>执行代码就会获取摄像头支持的所有分辨率列表，然后判断要设置的分辨率是否在支持的列表中，如果在的话就设置进去，否则抛出异常，留给业务层去处理。</p><p>配置完上面的参数之后，就要把这个参数设置给Camera这个实例了，代码如下：</p><pre><code class=\"language-plain\">try {\n&nbsp;&nbsp;&nbsp; mCamera.setParameters(parameters);\n} catch (Exception e) {\n&nbsp;&nbsp;&nbsp; throw new CameraParamSettingException(\"视频参数设置错误\");\n}\n</code></pre><p>在宽高的设置中，要注意宽是1280<span class=\"reference\">（或者640）</span>，高是720<span class=\"reference\">（或者480）</span>，这是因为摄像头默认采集出来的视频画面是横版的。在显示的时候，需要获取当前摄像头采集出来的画面的旋转角度，我们可以通过下面这段代码来获取旋转角度。</p><pre><code class=\"language-plain\">int degrees = 0;\nCameraInfo info = new CameraInfo();\nCamera.getCameraInfo(cameraId, info);\nif (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {\n&nbsp;&nbsp; &nbsp;degrees = (info.orientation) % 360;\n} else { // back-facing\n&nbsp;&nbsp;&nbsp; degrees = (info.orientation + 360) % 360;\n}\n</code></pre><p>根据不同的摄像头取出对应的CameraInfo，这个CameraInfo里的orientation属性表示的就是画面的旋转角度，不过要想正确地旋转还要再处理一下。如果是前置摄像头，就直接对360进行取模；如果是后置摄像头，要加上360度再取模360，从而得到要旋转的角度。这个角度对于将视频帧正确地显示到屏幕上是至关重要的，所以我们带着这个角度进入摄像头预览部分的学习吧。</p><h2>摄像头的预览</h2><p>摄像头的基础参数配置好之后，就可以接收摄像头采集的图像，然后将图像渲染到屏幕上了。OpenGL ES渲染图像的基本流程是，先把图像解码成RGBA格式，然后将RGBA格式的字节数组上传到一个纹理上，最终将纹理渲染到屏幕View上，而这里的渲染也会使用OpenGL ES实现。</p><h3>渲染链路</h3><p>整体渲染架构我们在Java层构造一个Surface(Texture)View，来显示渲染的结果，然后在Native层用EGL和OpenGL ES给这个SurfaceView做渲染。核心的连接点如下：</p><ul>\n<li>在Native层的OpenGL线程中，生成一个纹理ID并传递到Java层，Java层利用这个纹理ID构造出一个SurfaceTexture。</li>\n<li>把这个SurfaceTexture设置给Camera的预览纹理，然后调用Camera的开始预览方法。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/9a/37/9a85b7aa0208536735bb39b2cbe5f237.png?wh=1920x1001\" alt=\"图片\" title=\"开始预览\"></p><p>但是我们怎么能够知道摄像头捕捉到了一张新的图片呢？答案就是给这个SurfaceTexture设置视频帧可用监听者。当Camera设备采集到一帧内容的时候，就会回调这个Listener，你可以看一下代码。</p><pre><code class=\"language-plain\">mCameraSurfaceTexture = new SurfaceTexture(textureId);\ntry {\n&nbsp;&nbsp;&nbsp; mCamera.setPreviewTexture(mCameraSurfaceTexture);\n&nbsp;&nbsp;&nbsp; mCameraSurfaceTexture.setOnFrameAvailableListener(frameAvailableListener);\n&nbsp;&nbsp;&nbsp; mCamera.startPreview();\n} catch (Exception e) {\n&nbsp;&nbsp;&nbsp; throw new CameraParamSettingException(\"设置预览纹理错误\");\n}\n</code></pre><p>代码里的frameAvailableListener是继承自OnFrameAvailableListener的内部类的一个实例，这个内部类里需要重写onFrameAvailable方法。当摄像头采集到一帧图像后，就会调用这个方法，在方法中我们调用Native层的方法来渲染出摄像头刚刚捕捉的图像。</p><p>Native层的这个方法会转换到渲染线程中调用SurfaceTexture的updateTexImage方法<span class=\"reference\">（因为必须在OpenGL ES线程才可以调用这个方法，所以绕了一大圈）</span>。这个方法调用完毕之后，摄像头采集的视频帧就放到了Native层生成的纹理上去了，渲染线程就可以继续把这个纹理渲染到界面上去了。当摄像头再一次采集到一帧新图像的时候，就周而复始地执行上述过程，这样在设备屏幕上就可以流畅地预览摄像头采集的内容了。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/d0/744eyy84604f78a1bc065652fb6357d0.png?wh=1920x673\" alt=\"图片\" title=\"刷新预览\"></p><h3>渲染过程与OES纹理</h3><p>上面我们在Native层的OpenGL线程中生成一个纹理ID，然后传递到Java层，由Java层构造成一个SurfaceTexture类型的对象，并将Camera的PreviewCallback设置为这个SurfaceTexture对象。</p><p>还记得在摄像头配置阶段，我们给摄像头配置的视频频帧格式是NV21吗？也就是YUV420SP格式的，这个格式中width * height个像素点需要占用width * height * 3 / 2个字节数，即每一个像素点都会有一个Y放到数据存储的前width * height个数据中，每四个像素点共享一个UV放到后半部分进行交错存储。</p><p>而在OpenGL中使用的绝大部分纹理对象都是RGBA的格式，另外之前在讲播放器的时候，我们也讲过Luminance格式，但是那里面是开辟3个纹理来表示一张YUV的图片，而这里必须使用一个纹理ID来给Camera更新数据，那应该怎么把3个Luminance的纹理合并成一个纹理对象呢？</p><p>幸好OpenGL ES提供了一个扩展类型：GL_TEXTURE_EXTERNAL_OES，这种纹理在使用上会有一些特殊，比如，纹理需要绑定到类型GL_TEXTURE_EXTERNAL_OES上，而不是类型GL_TEXTURE_2D上，给纹理设置参数的时候也要使用GL_TEXTURE_EXTERNAL_OES类型，生成这种类型的纹理与设置参数的代码如下：</p><pre><code class=\"language-plain\">glGenTextures(1, &amp;texId);\nglBindTexture(GL_TEXTURE_EXTERNAL_OES, texId);\nglTexParameteri(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_MAG_FILTER, GL_LINEAR);\nglTexParameteri(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_MIN_FILTER, GL_LINEAR);\nglTexParameteri(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);\nglTexParameteri(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);\n</code></pre><p>在实际的渲染过程中绑定纹理的代码如下：</p><pre><code class=\"language-plain\">glActiveTexture(GL_TEXTURE0);\nglBindTexture(GL_TEXTURE_EXTERNAL_OES, texId);\nglUniform1i(uniformSamplers, 0);\n</code></pre><p>在OpenGL ES的shader中需要声明对此扩展的使用，指令如下：</p><pre><code class=\"language-plain\">#extension GL_OES_EGL_image_external : require\n</code></pre><p>在shader里也必须使用samplerExternalOES采样方式来声明纹理，FragmentShader中的代码如下：</p><pre><code class=\"language-plain\">static char* GPU_FRAME_FRAGMENT_SHADER =\n\"#extension GL_OES_EGL_image_external : require&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n\"precision mediump float;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n\"uniform samplerExternalOES yuvTexSampler;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n\"varying vec2 yuvTexCoords;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n\"void main() {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\"\n\"&nbsp;&nbsp;&nbsp; gl_FragColor = texture2D(yuvTexSampler, yuvTexCoords);\\n\"\n\"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\n\";\n</code></pre><p>从生成纹理到设置参数，再到真正渲染这一整个流程下来，我们就弄清楚了这种特殊格式的纹理的使用方法，接下来我们再看一下摄像头采集图像旋转角度的问题。</p><h3>纹理旋转角度</h3><p>在使用摄像头的时候，很容易在这个地方踩坑，比如手机摄像头预览的时候会出现倒立、镜像等问题，下面我就带你彻底地搞定它。</p><p>由于摄像头采集出来的视频都是横向的，比如摄像头的预览大小是640 * 480，其实摄像头采集出来的视频帧宽是640，高是480，并且图片也是横向采集的。如果要能正确地预览出来需要旋转一个90度或270度，具体旋转多少度需要在当前这个摄像头的CameraInfo里面获取。</p><p>不同的手机甚至是不同的系统都会不一样，并且如果是前置摄像头的话，还需要再做一个VFlip<span class=\"reference\">（垂直镜像）</span>用来修复镜像的问题。假设图像是横向采集出来的，就做竖直翻转，如果已经旋转过了，就做横向翻转。下面我们用实际的图片，来分别看一下前置摄像头和后置摄像头具体的渲染流程。先看一张实际摄像头要采集的物体。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/dc/5483e188bef86567c2f1f3aeeed100dc.png?wh=1696x578\" alt=\"图片\"></p><p>在使用<strong>前置摄像头</strong>去采集这个物体的时候，得到最左边的图片，这个时候在摄像头的CameraInfo里取出来的角度一般是270度。按照旋转角度，我们把图片顺时针旋转270度，得到中间的图片，再进行镜像处理，得到最右边的图片。最后我们在手机屏幕上看到的才是预期的图像。</p><p><img src=\"https://static001.geekbang.org/resource/image/af/c0/af629d66a6dcbe8592b5d82ba34730c0.png?wh=1916x560\" alt=\"图片\"></p><p>如果是<strong>后置摄像头</strong>，一般在摄像头的CameraInfo里取出来的角度是90度，当然这是ROM厂商定的，比如LG厂商的Nexus5X这个设备取出来的角度就是270度。不过无论是多少度，摄像头采集出来的图像，根据这个角度在旋转过后，就能得到一个正常的图像了，你可以看一下它的旋转流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/7f/f488a98ffyy778f2535dc4768620907f.png?wh=1692x580\" alt=\"图片\"></p><p>如果是LG厂商的Nexus5X或者HUAWEI厂商的Nexus6P这两款设备，图像的后置摄像头需要顺时针旋转270度，得到正常显示的图像。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/13/f88385f2591a1b34d2e03c606eb02813.png?wh=1680x584\" alt=\"图片\" title=\"Nexus 5/6P升级系统之后\n\"></p><h3>确定物体坐标与纹理坐标</h3><p>上述图像的旋转和镜像，在OpenGL ES中需要通过物体坐标和纹理坐标变换来实现。我们可以再回看一下之前物体坐标系和纹理坐标系的图。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/4b/451583e39a93e7b42ef62e38e7c9964b.png?wh=1920x865\" alt=\"图片\"></p><p>而我们这里的物体坐标也是一个通用的坐标。</p><pre><code class=\"language-plain\">GLfloat squareVertices[8] = {\n&nbsp;&nbsp;&nbsp; -1.0, -1.0, //物体左下角\n&nbsp;&nbsp;&nbsp; 1.0，-1.0，&nbsp; //物体右下角\n&nbsp;&nbsp;&nbsp; -1.0，1.0， &nbsp;//物体左上角\n&nbsp;&nbsp;&nbsp; 1.0，1.0&nbsp; &nbsp;&nbsp;&nbsp;//物体右上角\n};\n</code></pre><p>下面是不做任何旋转与镜像的OpenGL纹理坐标。</p><pre><code class=\"language-plain\">GLfloat textureCoordNoRotation[8] = {\n&nbsp;&nbsp;&nbsp; 0.0，0.0，//图像的左下角\n&nbsp;&nbsp;&nbsp; 1.0，0.0，//图像的右下角\n&nbsp;&nbsp;&nbsp; 0.0，1.0，//图像的左上角\n&nbsp;&nbsp;&nbsp; 1.0，1.0&nbsp; //图像的右上角\n};\n</code></pre><p>然后，给出顺时针旋转90度的纹理坐标，你可以想象一下，把OpenGL的纹理坐标系的图顺时针旋转90度，然后再把对应的左下、右下、左上、右上的坐标点写下来。</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp; &nbsp;1.0，0.0，//图像的右下角\n&nbsp;&nbsp;&nbsp; 1.0，1.0，//图像的右上角\n&nbsp;&nbsp;&nbsp; 0.0，0.0，//图像的左下角\n&nbsp;&nbsp;&nbsp; 0.0，1.0&nbsp; //图像的左上角\n};\n</code></pre><p>给出顺时针旋转180度的纹理坐标。</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp;&nbsp; 1.0，1.0，//图像的右上角\n&nbsp;&nbsp;&nbsp; 0.0，1.0，//图像的左上角\n&nbsp;&nbsp;&nbsp; 1.0，0.0，//图像的右下角\n&nbsp;&nbsp;&nbsp; 0.0，0.0&nbsp; //图像的左下角\n};\n</code></pre><p>然后，给出顺时针旋转270度的纹理坐标。</p><pre><code class=\"language-plain\">GLfloat textureCoords[8] = {\n&nbsp;&nbsp;&nbsp; 0.0，1.0，//图像的左上角\n&nbsp;&nbsp;&nbsp; 0.0，0.0，//图像的左下角\n&nbsp;&nbsp;&nbsp; 1.0，1.0，//图像的右上角\n&nbsp;&nbsp;&nbsp; 1.0，0.0&nbsp; //图像的右下角\n};\n</code></pre><p>还记得前面我们讲过计算机图像的坐标系与OpenGL的坐标系不同吗？它们的Y恰好是相反的，所以这里要把每一个纹理坐标做一个VFlip的变换<span class=\"reference\">（垂直镜像，即把每一个顶点的y值由0变为1或者由1变为0）</span>，这样就可以得到一个正确的图像旋转了。而我们的前置摄像头存在镜像的问题，这时候需要对每一个纹理坐标做一个HFlip变换<span class=\"reference\">（水平镜像，即把每一个顶点的x值由0变为1或者由1变为0）</span>，从而让图片在预览界面中看起来和在镜子中一样。</p><h3>自适应渲染</h3><p>上面的步骤其实就是一个特殊格式<span class=\"reference\">（OES）</span>的纹理经过旋转和渲染，变成了正常格式<span class=\"reference\">（RGBA）</span>的一个纹理，那接下来就可以把这个正常格式的纹理渲染到屏幕上去了。</p><p>但这里需要补充一句，由于这个纹理的宽和高实际上是摄像头捕捉过来的图像的高和宽<span class=\"reference\">（做了一个90度或者270度的旋转）</span>，而我们的目标是要渲染到SurfaceView上面去，但是如果Java层提供的SurfaceView的宽高和处理过后的这个纹理ID的宽高不一致，那么这一帧图像就会出现被压缩或者拉伸的问题，所以在渲染到屏幕上的时候，我们要做一个自适配，让纹理按照屏幕View的比例自动填充。</p><p>先来看纹理坐标，x从0.0到1.0就说明要把纹理的x轴方向全部都绘制到物体表面<span class=\"reference\">（整个SurfaceView）</span>上去，而如果我们只想绘制一部分，比如中间的一半，那么就可以将x轴的坐标写成0.25到0.75，类似的适配也可以应用到y轴上。但这个0.25和0.75是如何得出来的呢？</p><p>答案很简单，如果不想被拉伸，SurfaceView的宽高比例和纹理的宽高比例就应该是相同的。假设这一张纹理的宽为texWidth，高为texHeight，而物体表面的宽为screenWidth，高为screenHeight，就可以利用下面的公式来完成自动填充的坐标计算。</p><pre><code class=\"language-plain\">float textureAspectRatio = texHeight / texWidth;\nfloat viewAspectRatio = screenHeight / screenWidth;\nfloat xOffset = 0.0f;\nfloat yOffset = 0.0f;\nif(textureAspectRatio &gt; viewAspectRatio){\n&nbsp;&nbsp;&nbsp; //Update Y Offset\n&nbsp;&nbsp;&nbsp; int expectedHeight = texHeight*screenWidth/texWidth+0.5f;\n&nbsp;&nbsp;&nbsp; yOffset = (expectedHeight - screenHeight) / (2 * expectedHeight);\n} else if(textureAspectRatio &lt; viewAspectRatio){\n&nbsp;&nbsp;&nbsp; //Update X Offset\n&nbsp;&nbsp;&nbsp; int expectedWidth = texHeight * screenWidth / screenHeight + 0.5);\n&nbsp;&nbsp;&nbsp; xOffset = (texWidth - expectedWidth)/(2*texWidth);\n}\n</code></pre><p>计算得到的xOffset和yOffset在纹理坐标中分别替换掉0.0的位置，利用1.0-xOffset以及1.0-yOffset替换掉1.0的位置，最终得到一个纹理坐标矩阵。</p><pre><code class=\"language-plain\">GLfloat textureCoordNoRotation[8] = {\n&nbsp;&nbsp;&nbsp; xOffset，&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yOffset，\n&nbsp;&nbsp;&nbsp; 1.0 - xOffset，&nbsp;&nbsp;&nbsp;&nbsp; yOffset，\n&nbsp;&nbsp;&nbsp; xOffset，&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0 - yOffset，\n&nbsp;&nbsp;&nbsp; 1.0 - yOffset，&nbsp;&nbsp;&nbsp;&nbsp; 1.0 - yOffset&nbsp;\n};\n</code></pre><p>到这里，摄像头预览流程就可以随着摄像头所采集的图像一帧一帧地绘制下去了，也就实现了整个预览过程。</p><h3>切换摄像头与关闭预览</h3><p>当用户切换摄像头的时候，就可以给Native层发一个指令，Native层会在OpenGL线程中关闭当前摄像头，然后重新打开另外一个摄像头，并配置参数，然后设置预览的SurfaceTexture，最后调用开始预览方法，这样就可以切换成功了，用户看到的就是切换摄像头之后的预览画面。</p><p>最终关闭预览时，要先停掉整个渲染线程，然后关闭Camera，当然还要释放之前建立的SurfaceTexture，把摄像头的PreviewCallback设置为null，最终释放掉摄像头。</p><p><img src=\"https://static001.geekbang.org/resource/image/99/ec/994dd896159418becf7e6536395b20ec.png?wh=1920x524\" alt=\"图片\" title=\"结束预览\"></p><p>到这里，摄像头预览相关的操作就全都讲完了，这个部分的内容是十分重要的，对后面搭建整个录制视频的项目来说，是最基础的部分，所以一定要掌握。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下这节课的主要内容。这节课我们从权限配置开始，然后讲解了摄像头的配置与打开摄像头的操作，接着详细讲解了如何将摄像头采集到的图像一步步渲染到屏幕View上，整体流程如下：</p><ul>\n<li>在Native层创建一个OpenGL线程，并且构建出一个OES类型的纹理ID传递给Java层构造成一个SurfaceTexture，然后配置给摄像头。</li>\n<li>当摄像头采集到一帧图像之后，就会通过回调的方式告诉我们，我们将在OpenGL线程中调用update方法将这一帧图像更新到纹理ID上。</li>\n<li>接着按照旋转角度确定纹理矩阵，将OES的纹理渲染成为一个标准的RGBA的纹理。</li>\n<li>最后将RGBA类型的纹理在渲染到屏幕上。</li>\n</ul><p>如果你了解清楚整个过程，那么后面学习编码也会更加顺畅。</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/be/e3306e85566c35d565032124fde097be.png?wh=1898x1384\" alt=\"图片\"></p><h2>思考题</h2><p>这节课我们学习了摄像头预览的流程，那我来考考你，摄像头采集的出来的纹理类型是什么类型，它又是怎么被绘制到屏幕View上去的呢？欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"14 ｜ iOS平台如何采集视频画面？","id":556012},"right":{"article_title":"16 ｜视频编码的工作原理与H.264封装格式","id":559245}}},{"article_id":559245,"article_title":"16 ｜视频编码的工作原理与H.264封装格式","article_content":"<p>你好，我是展晓凯。今天我们来一起学习视频编码的工作原理与H.264的封装格式。</p><p>前两节课我们一起学习了iOS与Android平台的视频画面采集，但是采集下来的内容最终是需要保存到一个视频文件中的，所以就需要用到视频编码相关的知识。</p><p>还记得前面我们讨论的音频压缩方式吗？音频编码主要是去除冗余信息，从而达到数据量压缩的目的。那视频方面的编码，又是通过什么方式来压缩数据的呢？其实和音频编码类似，视频编码也是通过去找出冗余信息来压缩数据的。但相比于音频数据，视频数据有极强的相关性，也就是说有大量的冗余信息，包括空间上的冗余信息和时间上的冗余信息。接下来我们就一起看一下目前比较主流的视频压缩标准。</p><h2>编码标准介绍</h2><p>我想你一定知道你JPEG格式的图片吧，其实它就是ISO制定的JPEG的图像编码标准。对于视频，ISO同样也制定了标准，Motion JPEG就是MPEG，MPEG算法是适用于动态视频的压缩算法，它除了可以对单幅图像进行编码外，还可以利用图像序列中的相关性原则，去掉冗余信息，这样就可以大大提高视频的压缩比。</p><p>发展到现在，MPEG也已经经历过了好多代，版本一直在不断更新中，主要有这几个版本：Mpeg1，VCD用的就是它；Mpeg2，在DVD中使用；还有Mpeg4 AVC，现在的流媒体中使用最多的就是它了。</p><!-- [[[read_end]]] --><p>相较于ISO制定的MPEG的视频压缩标准，ITU-T制定的H.261、H.262、H.263、H.264一系列视频编码标准是另外一套体系。其中H.264汲取了以往标准制定中积累的经验，采用简洁的设计，使用的范围越来越广。</p><p>现在用得最多的就是H.264标准，H.264创造了多参考帧、多块类型、整数变换、帧内预测等新的压缩技术，使用了更精细的分象素运动矢量<span class=\"reference\">（1/4、1/8）</span>和新一代的环路滤波器，大大提高了压缩性能，系统更加完善。从H.264开始，它与Mpeg标准的AVC合并成为了一个，后续又衍生出了H.265、H.266等新一代的视频编码标准。</p><p>除了上述两个组织制定的标准之外，还有Google制定的VP系列的视频编码和我们国家AVS工作组制定的AVS编码标准，整体的图示如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/41/65/41166ca9a0e91958737abf4085144a65.png?wh=1920x705\" alt=\"图片\"></p><p>接下来我们会重点学习H.264这一视频编码的工作原理及它的封装格式。</p><h2>编码器的工作编码原理介绍</h2><p>由于人类眼睛的生理特点<span class=\"reference\">（视锥细胞对亮度的敏感程度强于颜色的敏感程度）</span>，所以出现了YUV420P的无损表示格式，但只是做到这样还是远远不够的，还应该使用有损的压缩方式，将YUV420P的原始数据压缩到更小，而压缩主要考虑到视频有以下两个特性。</p><ul>\n<li>一段视频中包含大量的视频帧，而相邻的视频帧之间几乎没有任何变化。</li>\n<li>一帧图像内部包含了许多使用相同或相似颜色的区域。</li>\n</ul><p>第一是去除时域上的冗余信息；第二是去除单张视频帧的空间冗余信息。那这两部分工作也是编码器工作的重点，这节课我就来介绍一下这两部分的通用实现手段，但是在不同的编码器中实现的方式又有所不同，编码出来的视频质量以及性能消耗也是不同的，而性能和质量也是开发者选用编码器的衡量指标。</p><h3>帧类型介绍</h3><p>在尝试着消除冗余信息<span class=\"reference\">（不论是时间的冗余信息还是空间的冗余信息）</span>之前，先来假设一个视频内容，一个帧率<span class=\"reference\">（fps）</span>为30的电影，下面是前四帧的内容：</p><p><img src=\"https://static001.geekbang.org/resource/image/70/f8/709aef5c1774e77cf0f5e261e5513af8.png?wh=460x122\" alt=\"图片\"></p><p>在这四帧视频帧中，我们可以看到大量的重复信息，比如蓝色的背景，它从第一帧到第四帧并没有任何变化，为了消除这些冗余信息，我们可以将视频帧的类型抽象为3种类型。</p><ul>\n<li>I Frame</li>\n</ul><p>I帧是一个仅包含当前帧信息的视频帧类型，所以它又被称为参考帧、关键帧。在解码过程中它不需要依赖任何帧就可以被解码出来，一个I帧看起来和一张静态图片非常类似，视频流或者视频文件中第一帧通常是I帧，并且会定期在其他帧类型中间插入I帧；</p><ul>\n<li>P Frame</li>\n</ul><p>P帧是前向参考帧，可以使用前面的I帧与P帧来呈现出当前视频帧，这也是解码器的解码规则，例如上图中第二帧视频帧与第一帧视频帧的变化只是小球向右前方移动了，所以我们可以基于第一帧视频帧计算出变化部分，作为第二帧视频帧的内容，这样第二帧视频帧所占用的空间就大大减少了。</p><ul>\n<li>B Frame</li>\n</ul><p>相比于P帧只参考前面的视频帧内容，B帧又增加了对后边视频帧的参考，这样可以提供更好的压缩，但是这对于编码器来讲，计算量增大的同时还增加了编码输出的延迟时间<span class=\"reference\">（因为需要参考后续过来的视频帧）</span>，所以在一些实时性要求较高的场景下通常不使用B帧。</p><p>这些帧类型共同组成了一个完整的视频，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/7b/cd/7be705a825b80c7c7fff6883b9edfecd.png?wh=894x150\" alt=\"图片\"></p><p>如果仅从存储或者网络带宽角度来衡量，我们可以认为I帧是最昂贵的，P帧会便宜一些，但是最便宜的是B帧。接下来我们看一下，这几种类型的帧是如何产生的。</p><h3>消除时间上的冗余信息</h3><p>我们先来一起讨论一下如何消除视频帧在时间上的冗余信息，比较成熟的技术就是帧间预测技术<span class=\"reference\">（inter-frame prediction）</span>，先来看这两帧视频帧。</p><p><img src=\"https://static001.geekbang.org/resource/image/fd/19/fd5b05ea17636b48f45114e5dae9f819.png?wh=406x204\" alt=\"图片\"></p><p>我们将去除掉时间上的冗余信息来达到使用更少的字节存储这两帧视频帧的目的，首先想到的就是这两帧视频帧相减，得到Diff值，就是我们要进行编码的东西，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/cc/49/cc5dfe676f5677c6088f90cf06f1be49.png?wh=350x312\" alt=\"图片\"></p><p>但其实还有一种更好的方法，可以使用更少的比特数来存储第二帧视频帧的内容，首先我们将每一帧视频帧<span class=\"reference\">（frame_0）</span>分为很多个部分，然后将这两帧视频帧中的每一部分进行匹配，这种算法我们可以看作是运动估计，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c0/07/c0a62654a38408e260f17a02c1cdb207.png?wh=442x222\" alt=\"图片\"></p><p>在第一帧变化到第二帧的过程中，我们可以估计第一帧视频帧中的黑色球从点（x=0, y=25）到点（x=7, y=26），而x和y组成的值的集合就叫做运动向量。我们可以更进一步来节省编码内容，即只对这两帧之间的运动矢量差进行编码，所以最终运动矢量是x=6（6-0)） y=1（26-25）。当然在真实的编码过程中，图中的小球会再被划分为N个部分，前面把它当作一部分只是为了方便理解。所以我们可以看到，当应用了运动估算算法之后，编码的数据要比简单地计算Diff值进行编码的数据要少得多。</p><p>我们可以使用FFmpeg工具来查看一个视频编码的运动矢量情况，命令如下：</p><pre><code class=\"language-plain\">ffplay -flags2 +export_mvs -vf codecview=mv=pf+bf+bb input.flv\n</code></pre><p>到这里，时间冗余信息我们可以靠运动估计算法给消除掉了，而要想使用运动估计，就需要一个帧作为参考帧，一般称之为I帧，那么I帧是如何进行压缩的呢？其实就是接下来我们要讲解的空间冗余信息的消除部分。</p><h3>消除空间上的冗余信息</h3><p>在一帧视频帧中我们可以看到大量的重复信息。</p><p><img src=\"https://static001.geekbang.org/resource/image/25/e8/25f1f6d2ca5904449565a1f36a54bbe8.png?wh=310x180\" alt=\"图片\"></p><p>从图中可以看到有大量的蓝色和白色组成，如果这是一帧I帧的话，我们就无法使用帧间预测技术来压缩它了。但我们还是有办法来压缩这张图片，因为它的重复信息还是比较多的。</p><p><img src=\"https://static001.geekbang.org/resource/image/14/9e/142e17837d89a38f425089cfa976679e.png?wh=208x306\" alt=\"图片\"></p><p>如图所示，我们将对标出来的红色块的部分进行编码，我们可以根据红色块周围的颜色趋势来预测当前部分的颜色值，比如可以预测，帧将继续垂直地传播颜色，这意味着未知像素的颜色将保持其邻居的值，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/83/d821cdfbfe1eb9f0f68ba54ebf831283.png?wh=274x218\" alt=\"图片\"></p><p>但是我们的预测有可能是错误的，所以我们需要使用另外一项技术，即帧内预测技术。我们使用真正的值减去预测的值可以得到一个矩阵，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/ca/34/ca2e9ac9c72dcyyff8c632e4b525da34.png?wh=894x274\" alt=\"图片\"></p><p>我们通过帧内预测技术实现了空间压缩，也简单了解了空间压缩的原理，但是一个编码器的内部工作机制还是非常复杂的，有非常多的复杂流程，最经典的就是DCT变换，它利用图像在低频部分的能量分布比较多，在高频比较少这一特点，将图像变换到频域上来进一步消除空间的冗余信息，这就是有损压缩最核心的一部分。除此之外，编码器还需要使用量化、熵编码等步骤共同来编码出最终的视频。如果你有兴趣了解更多的内部细节，可以参考libx264的<a href=\"https://www.videolan.org/developers/x264.html\">官方文档</a>。</p><h2>H264的NALU</h2><p>NALU的全称是Network Abstract Layer Unit，即网络抽象层，在H.264/AVC视频编码标准中，整个系统框架被分成了两层，即视频编码层面<span class=\"reference\">（VCL）</span>和网络抽象层面<span class=\"reference\">（NAL</span>）。前者负责有效表示视频数据的内容，而后者负责格式化数据并提供头信息，以保证数据适合各种信道和存储介质上的传输。我们平时的每帧数据就是一个NAL单元，也就是我们常说的NALU。一个完整的NALU由Header和Payload组成，其中Header部分占一个字节，每一位代表含义如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/2c/1e028c99df9604b7a4e73234f954022c.png?wh=1778x660\" alt=\"图片\"></p><p>其中第一位固定为0；第二位、第三位组成一个nal_ref_idc代表了这个NAL的重要性，取值范围是0～3，值越大表示当前NALU越重要，需要优先受到保护，比如IDR、SPS、PPS类型的NALU，这个值一定不是0；最后5位共同组成NALUType，判定规则就是“与”上0x1f得出的值再和下述数值做比较。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/23/5f68020098883f082b5d5bee60f66c23.png?wh=892x558\" alt=\"图片\"></p><p>我们经常使用的有IDR、SEI、SPS、PPS，IDR代表了关键帧，从这一帧开始不需要再参考前序的视频帧。SEI帧代表视频增强信息的帧，像之前风靡一时的视频答题应用就使用这种类型的帧来承载题目以及答案，通常用法是把这个帧加入到每一个关键帧的前面。</p><p>SPS和PPS保存了一组编码视频序列的全局参数，每一帧编码后的数据所依赖的参数保存在图像参数集里，里面的信息至关重要，如果其中的数据丢失或出现错误，那么解码过程很可能会失败。一般情况下SPS和PPS的NAL Unit处于整个码流的起始位置，初始化解码器的时候需要将这两者设置给解码器。你从图中也可以看到像SEI帧它的nal_ref_idc就是等于0的，而像IDR、SPS、PPS类型的nal_ref_idc就不能为0。</p><h2>H.264的封装格式</h2><p>在熟悉了H.264里面的NALU之后，接下来我们就再介绍一下H.264里面的封装格式，就像音频的AAC编码有两种封装格式，H.264的编码也有两种封装格式，一种是AVCC格式，一种是Annex-B格式。</p><h3>AVCC格式</h3><p>AVCC格式也叫AVC1格式，是属于MPEG-4标准定义的格式，通常用于存储MP4/FLV/MKV等文件，也可以用于RTMP直播中，格式如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/84/66/84f4b95b9941ayyc62f29c33a85f1a66.png?wh=1920x195\" alt=\"图片\"></p><p>最前面是extraData部分，这部分描述了整体编码的一些信息，主要部分是由sps和pps组成，后边跟的是许多个NALU，但是每一个NALU前面一般会使用4个字节来表示这个NALU的长度，然后紧接着跟着这个NALU。</p><h3>Annex-B格式</h3><p>Annex-B格式也叫MPEG-2 transport stream format格式，是属于MPEG-4标准定义的格式，通常用于ts流以及视频会议中，格式如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/54/5d/54c4ede90471200129dfb33668a11c5d.png?wh=1920x249\" alt=\"图片\"></p><p>其中startCode有可能是三个字节或者四个字节，会用0x001或者0x0001来表示，这个格式会以它作为NALU的分界点，SPS和PPS按照一个NALU的方式写在头部。</p><p>在FFmpeg中提供了一个bitstreamFilter叫做h264_mp4toannexb，在解码场景中可以方便地将AVCC格式的H.264转换为Annexb格式的H.264。当然在编码场景中，如果需要逆方向的格式转换，你也可以自己将startCode部分替换为NALULength。</p><h2>小结</h2><p>最后，我们可以一起来回顾一下。</p><ul>\n<li>各个组织的视频编码标准，包括ISO的Mpeg、ITU-T的H26x、Google的VPx还有AVS工作的AVS标准。</li>\n<li>视频编码器的工作原理，编码器需要消除掉视频帧里面的空间冗余信息和时间冗余信息，在这里引出了帧的类型，包括I帧、P帧和B帧。</li>\n<li>这节课的重点是H.264的NALU以及封装格式，在NALU部分我们一起学习了NALU的组成方式以及各种NALU Type，包括SPS、PPS、IDR、SEI等，在封装格式部分重点讲解了两种封装格式的区别和具体构造。</li>\n</ul><p>理解我们这节课的内容是非常重要的，因为在接下来的两节课我们会使用这节课学到的知识来开发代码，分别使用软件编码<span class=\"reference\">（基于FFmpeg）</span>以及硬件编码<span class=\"reference\">（Android与iOS平台本身提供的硬件编码接口）</span>的形式，来完成H.264的编码，这样就可以用<a href=\"http://time.geekbang.org/column/article/557500\">上一节课</a>采集到的YUV数据，生成一个H.264的数据了。</p><h2>思考题</h2><p>你可以思考一下，如果让你做一个推流的SDK，涉及网络的抖动，一定要做一些实时码率的变化，但是在码率变化的同时常用的一个做法是丢弃掉一部分高码率视频帧，在H.264中哪些视频帧可以丢弃，哪些又不可以丢弃呢？欢迎在评论区中告诉我你的答案，也欢迎你把这节课分享给更多对音视频感兴趣的朋友，我们一起交流、共同进步。下节课再见！</p>","neighbors":{"left":{"article_title":"15 ｜ Android平台是如何采集视频画面的？","id":557500},"right":[]}}]