{"id":614287,"title":"30｜辅助任务管理：任务优先级、去重与失败处理","content":"<p>你好，我是郑建勋。</p><p>这节课，让我们给系统加入一些辅助功能，把爬虫流程变得更完善一些。这些功能包括：爬虫最大深度、请求不重复、优先队列、以及随机的User-Agent。</p><h2>设置爬虫最大深度</h2><p>当我们用深度和广度优先搜索爬取一个网站时，为了防止访问陷入到死循环，同时控制爬取的有效链接的数量，一般会给当前任务设置一个最大爬取深度。最大爬取深度是和任务有关的，因此我们要在 Request 中加上MaxDepth 这个字段，它可以标识到爬取的最大深度。Depth 则表示任务的当前深度，最初始的深度为0。</p><pre><code class=\"language-plain\">type Request struct {\n\tUrl       string\n\tCookie    string\n\tWaitTime  time.Duration\n    Depth     int\n\tMaxDepth  int\n\tParseFunc func([]byte, *Request) ParseResult\n}\n</code></pre><p>那在异步爬取的情况下，我们怎么知道当前网站的深度呢？最好的时机是在采集引擎采集并解析爬虫数据，并将下一层的请求放到队列中的时候。以我们之前写好的ParseURL函数为例，在添加下一层的URL时，我们将Depth加1，这样就标识了下一层的深度。</p><pre><code class=\"language-plain\">func ParseURL(contents []byte, req *collect.Request) collect.ParseResult {\n\tre := regexp.MustCompile(urlListRe)\n\n\tmatches := re.FindAllSubmatch(contents, -1)\n\tresult := collect.ParseResult{}\n\n\tfor _, m := range matches {\n\t\tu := string(m[1])\n\t\tresult.Requesrts = append(\n\t\t\tresult.Requesrts, &amp;collect.Request{\n\t\t\t\tUrl:      u,\n\t\t\t\tWaitTime: req.WaitTime,\n\t\t\t\tCookie:   req.Cookie,\n\t\t\t\tDepth:    req.Depth + 1,\n\t\t\t\tMaxDepth: req.MaxDepth,\n\t\t\t\tParseFunc: func(c []byte, request *collect.Request) collect.ParseResult {\n\t\t\t\t\treturn GetContent(c, u)\n\t\t\t\t},\n\t\t\t})\n\t}\n\treturn result\n}\n</code></pre><!-- [[[read_end]]] --><p>最后一步，我们在爬取新的网页之前，判断最大深度。如果当前深度超过了最大深度，那就不再进行爬取。这部分的完整代码你可以查看分支<a href=\"https://github.com/dreamerjackson/crawler\">v0.1.7</a>。</p><pre><code class=\"language-plain\">func (r *Request) Check() error {\n\tif r.Depth &gt; r.MaxDepth {\n\t\treturn errors.New(\"Max depth limit reached\")\n\t}\n\treturn nil\n}\n\nfunc (s *Schedule) CreateWork() {\n\tfor {\n\t\tr := &lt;-s.workerCh\n\t\tif err := r.Check(); err != nil {\n\t\t\ts.Logger.Error(\"check failed\",\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\tcontinue\n\t\t}\n    ...\n\t}\n}\n</code></pre><h2>避免请求重复</h2><p>为了避免爬取时候的死循环，避免无效的爬取，我们常常需要检测请求是否重复，这时我们需要考虑3个问题：</p><ul>\n<li>用什么数据结构来存储数据才能保证快速地查找到请求的记录？</li>\n<li>如何保证并发查找与写入时，不出现并发冲突问题？</li>\n<li>在什么条件下，我们才能确认请求是重复的，从而停止爬取？</li>\n</ul><p>要解决第一个问题我们可以用一个简单高效的结构：哈希表。我们可以借助哈希表查找o(1)。另外，由于Go语言中的哈希表是不支持并发安全的，为了解决第二个问题，我们还需要在此基础上加一个互斥锁。而第三个问题我们需要在爬虫采取之前进行检查。</p><p>在解决上面的三个问题之前，我们先优化一下代码。我们之前的 <code>Request</code> 结构体会在每一次请求时发生变化，但是我们希望有一个字段能够表示一整个网站的爬取任务，因此我们需要抽离出一个新的结构Task作为一个爬虫任务，而 <code>Request</code> 则作为单独的请求存在。有些参数是整个任务共有的，例如Task中的Cookie、MaxDepth（最大深度）、WaitTime（默认等待时间）和RootReq（任务中的第一个请求）。</p><pre><code class=\"language-plain\">type Task struct {\n\tUrl         string\n\tCookie      string\n\tWaitTime    time.Duration\n\tMaxDepth    int\n\tRootReq     *Request\n\tFetcher     Fetcher\n}\n\n// 单个请求\ntype Request struct {\n\tTask      *Task\n\tUrl       string\n\tDepth     int\n\tParseFunc func([]byte, *Request) ParseResult\n}\n</code></pre><p>由于抽象出了Task，代码需要做对应的修改，例如我们需要把初始的Seed种子任务替换为Task结构。</p><pre><code class=\"language-plain\">for i := 0; i &lt;= 0; i += 25 {\n\t\tstr := fmt.Sprintf(\"&lt;https://www.douban.com/group/szsh/discussion?start=%d&gt;\", i)\n\t\tseeds = append(seeds, &amp;collect.Task{\n\t\t\t...\n\t\t  Url:      str,\n\t\t\tRootReq: &amp;collect.Request{\n\t\t\t\tParseFunc: doubangroup.ParseURL,\n\t\t\t},\n\t\t})\n\t}\n</code></pre><p>同时，在深度检查时，每一个请求的最大深度需要从Task字段中获取。</p><pre><code class=\"language-plain\">func (r *Request) Check() error {\n\tif r.Depth &gt; r.Task.MaxDepth {\n\t\treturn errors.New(\"Max depth limit reached\")\n\t}\n\treturn nil\n}\n</code></pre><p>完整代码你可以查看<a href=\"https://github.com/dreamerjackson/crawler\">v0.1.8</a>。</p><p>接下来，我们继续用一个哈希表结构来存储历史请求。由于我们希望随时访问哈希表中的历史请求，所以把它放在Request、Task中都不合适。 放在调度引擎中也不合适，因为调度引擎从功能上讲，应该只负责调度才对。所以，我们还需要完成一轮抽象，将调度引擎抽离出来作为一个接口，让它只做调度的工作，不用负责存储全局变量等任务。</p><p>所以我们就构建一个新的结构Crawler作为全局的爬取实例，将之前Schedule中的options迁移到Crawler中，Schedule只处理与调度有关的工作，并抽象为了Scheduler接口。</p><pre><code class=\"language-plain\">type Crawler struct {\n\tout chan collect.ParseResult\n\toptions\n}\n\ntype Scheduler interface {\n\tSchedule()\n\tPush(...*collect.Request)\n\tPull() *collect.Request\n}\n\ntype Schedule struct {\n\trequestCh chan *collect.Request\n\tworkerCh  chan *collect.Request\n\treqQueue  []*collect.Request\n\tLogger    *zap.Logger\n}\n</code></pre><p>在Scheduler中，Schedule方法负责启动调度器，Push方法会将请求放入到调度器中，而Pull方法则会从调度器中获取请求。我们也需要对代码做相应的调整，这里就不再赘述了，具体你可以参考<a href=\"https://github.com/dreamerjackson/crawler\">v0.1.9</a>。调度器抽象为接口后，如果我们有其他的调度器算法实现，也能够非常方便完成替换了。</p><p>现在，我们在Crawler中加入Visited哈希表，用它存储请求访问信息，增加VisitedLock来确保并发安全。</p><pre><code class=\"language-plain\">type Crawler struct {\n\tout         chan collect.ParseResult\n\tVisited     map[string]bool\n\tVisitedLock sync.Mutex\n\toptions\n}\n</code></pre><p>Visited中的Key是请求的唯一标识，我们现在先将唯一标识设置为URL + method方法，并使用MD5生成唯一键。后面我们还会为唯一标识加上当前请求的规则条件。</p><pre><code class=\"language-plain\">// 请求的唯一识别码\nfunc (r *Request) Unique() string {\n\tblock := md5.Sum([]byte(r.Url + r.Method))\n\treturn hex.EncodeToString(block[:])\n}\n</code></pre><p>接着，编写HasVisited方法，判断当前请求是否已经被访问过。StoreVisited方法用于将请求存储到Visited哈希表中。</p><pre><code class=\"language-plain\">func (e *Crawler) HasVisited(r *collect.Request) bool {\n\te.VisitedLock.Lock()\n\tdefer e.VisitedLock.Unlock()\n\tunique := r.Unique()\n\treturn e.Visited[unique]\n}\n\nfunc (e *Crawler) StoreVisited(reqs ...*collect.Request) {\n\te.VisitedLock.Lock()\n\tdefer e.VisitedLock.Unlock()\n\n\tfor _, r := range reqs {\n\t\tunique := r.Unique()\n\t\te.Visited[unique] = true\n\t}\n}\n</code></pre><p>最后在Worker中，在执行request前，判断当前请求是否已被访问。如果请求没有被访问过，将request放入Visited哈希表中。</p><pre><code class=\"language-plain\">func (s *Crawler) CreateWork() {\n\tfor {\n\t\tr := s.scheduler.Pull()\n\t\tif err := r.Check(); err != nil {\n\t\t\ts.Logger.Error(\"check failed\",\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\tcontinue\n\t\t}\n    // 判断当前请求是否已被访问\n\t\tif s.HasVisited(r) {\n\t\t\ts.Logger.Debug(\"request has visited\",\n\t\t\t\tzap.String(\"url:\", r.Url),\n\t\t\t)\n\t\t\tcontinue\n\t\t}\n    // 设置当前请求已被访问\n\t\ts.StoreVisited(r)\n\t  ...\n\t}\n}\n</code></pre><p>最后要注意的是，哈希表需要用make进行初始化，要不然在运行时访问哈希表会直接报错。（完整的代码位于<a href=\"https://github.com/dreamerjackson/crawler\">v0.2.0</a>）。</p><h2>设置优先队列</h2><p>我们要给项目增加的第三个功能就是优先队列。</p><p>爬虫任务的优先级有时并不是相同的，一些任务需要优先处理。因此，接下来我们就来设置一个任务的优先队列。优先队列还可以分成多个等级，不过在这里我将它简单地分为了两个等级，即优先队列和普通队列。优先级更高的请求会存储到priReqQueue优先队列中。</p><pre><code class=\"language-plain\">type Schedule struct {\n\trequestCh   chan *collect.Request\n\tworkerCh    chan *collect.Request\n\tpriReqQueue []*collect.Request\n\treqQueue    []*collect.Request\n\tLogger      *zap.Logger\n}\n</code></pre><p>在调度函数Schedule中，我们会优先从优先队列中获取请求。而在放入请求时，如果请求的优先级更高，也会单独放入优先级队列。</p><p>最后我们还修复了之前遗留的一个Bug，将变量req、ch放置到for循环外部，防止丢失请求的可能性。</p><pre><code class=\"language-plain\">func (s *Schedule) Schedule() {\n\tvar req *collect.Request\n\tvar ch chan *collect.Request\n\tfor {\n\t\tif req == nil &amp;&amp; len(s.priReqQueue) &gt; 0 {\n\t\t\treq = s.priReqQueue[0]\n\t\t\ts.priReqQueue = s.priReqQueue[1:]\n\t\t\tch = s.workerCh\n\t\t}\n\t\tif req == nil &amp;&amp; len(s.reqQueue) &gt; 0 {\n\t\t\treq = s.reqQueue[0]\n\t\t\ts.reqQueue = s.reqQueue[1:]\n\t\t\tch = s.workerCh\n\t\t}\n\t\tselect {\n\t\tcase r := &lt;-s.requestCh:\n\t\t\tif r.Priority &gt; 0 {\n\t\t\t\ts.priReqQueue = append(s.priReqQueue, r)\n\t\t\t} else {\n\t\t\t\ts.reqQueue = append(s.reqQueue, r)\n\t\t\t}\n\t\tcase ch &lt;- req:\n\t\t\treq = nil\n\t\t\tch = nil\n\t\t}\n\t}\n}\n</code></pre><p>执行后输出结果为：</p><pre><code class=\"language-plain\">{\"level\":\"INFO\",\"ts\":\"2022-11-05T21:40:18.339+0800\",\"caller\":\"crawler/main.go:19\",\"msg\":\"log init end\"}\n{\"level\":\"INFO\",\"ts\":\"2022-11-05T21:40:22.067+0800\",\"caller\":\"engine/schedule.go:163\",\"msg\":\"get result: &lt;https://www.douban.com/group/topic/278041246/&gt;\"}\n{\"level\":\"INFO\",\"ts\":\"2022-11-05T21:40:22.150+0800\",\"caller\":\"engine/schedule.go:163\",\"msg\":\"get result: &lt;https://www.douban.com/group/topic/278040957/&gt;\"}\n...\n</code></pre><p>完整的代码位于<a href=\"https://github.com/dreamerjackson/crawler\">v0.2.1</a>。</p><h2>设置随机 <code>User-Agent</code></h2><p>我们给项目增加的第四个功能是 <code>User-Agent</code> 随机性。 为了避免服务器检测到我们使用了同一个User-Agent，继而判断出是同一个客户端在发出请求，我们可以为发送的User-Agent加入随机性。这个操作的本质就是将浏览器的不同型号与不同版本拼接起来，组成一个新的User-Agent。</p><p>随机生成User-Agent的逻辑位于extensions/randomua.go中，里面枚举了不同型号的浏览器和不同型号的版本，并且通过排列组合产生了不同的User-Agent。</p><p>最后一步，我们要在采集引擎中调用GenerateRandomUA函数，将请求头设置为随机的User-Agent，如下所示：</p><pre><code class=\"language-plain\">func (b BrowserFetch) Get(request *spider.Request) ([]byte, error) {\n   ...\n   req.Header.Set(\"User-Agent\", extensions.GenerateRandomUA())\n   resp, err := client.Do(req)\n</code></pre><p>完整的代码你可以参考<a href=\"https://github.com/dreamerjackson/crawler\">v0.2.2分支</a>。</p><h2>进行失败处理</h2><p>在课程的最后，我们来看一看失败处理。</p><p>我们在爬取网站时，网络超时等诸多潜在风险都可能导致爬取失败。这时，我们可以对失败的任务进行重试。但是如果网站多次失败，那就没有必要反复重试了，我们可以将它们放入单独的队列中。为了防止失败请求日积月久导致的内存泄露，同时也为了在程序崩溃后能够再次加载这些失败网站，我们最后还需要将这些失败网站持久化到数据库或文件中。</p><p>这节课我们先完成前半部分，即失败重试。后半部分会在第32讲存储引擎中详细介绍。我们要在全局Crawler中存储failures哈希表，设置Key为请求的唯一键，用于快速查找。failureLock互斥锁用于并发安全。</p><pre><code class=\"language-plain\">type Crawler struct {\n   ...\n   failures    map[string]*collect.Request // 失败请求id -&gt; 失败请求\n   failureLock sync.Mutex\n}\n</code></pre><p>当请求失败之后，调用SetFailure方法将请求加入到failures哈希表中，并且把它重新交由调度引擎进行调度。这里我们为任务Task引入了一个新的字段Reload，标识当前任务的网页是否可以重复爬取。如果不可以重复爬取，我们需要在失败重试前删除Visited中的历史记录。</p><pre><code class=\"language-plain\">func (e *Crawler) SetFailure(req *collect.Request) {\n   if !req.Task.Reload {\n      e.VisitedLock.Lock()\n      unique := req.Unique()\n      delete(e.Visited, unique)\n      e.VisitedLock.Unlock()\n   }\n   e.failureLock.Lock()\n   defer e.failureLock.Unlock()\n   if _, ok := e.failures[req.Unique()]; !ok {\n      // 首次失败时，再重新执行一次\n      e.failures[req.Unique()] = req\n      e.scheduler.Push(req)\n   }\n   // todo: 失败2次，加载到失败队列中\n}\n</code></pre><p>如果失败两次，就将请求单独加载到失败队列中，并在后续进行持久化，这一步操作我们之后再进行。失败重试的完整代码位于<a href=\"https://github.com/dreamerjackson/crawler\">v0.2.3</a>。</p><h2>总结</h2><p>这节课，我们为爬虫系统增加了丰富的辅助任务，包括：设置爬虫的最大深度，避免了重复爬取、设置优先队列、设置随机的User-Agent，另外我们还进行了任务失败的处理。</p><p>随着爬虫系统的演进，我们还会有更加复杂的功能与需求。后面的内容，我们还会介绍限速器，并借助JS虚拟机实现更加灵活的爬虫系统。</p><h2>课后题</h2><p>学完这节课也给你留一道思考题。</p><p>之前我们实现了任务的优先队列，但我们目前只支持两种优先级。如果要支持多种优先级，你知道如何实现吗？</p><p>欢迎你在留言区与我交流讨论，我们下节课见。</p>","comments":[{"had_liked":false,"id":364697,"user_name":"Realm","can_delete":false,"product_type":"c1","uid":1081299,"ip_address":"浙江","ucode":"30CBEBE619D1A2","user_header":"https://static001.geekbang.org/account/avatar/00/10/7f/d3/b5896293.jpg","comment_is_top":false,"comment_ctime":1671349438,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"https:&#47;&#47;shimo.im&#47;docs&#47;5rk9dVyblnFzZLqx\n根据课程讲解以及原代码，自己理解整理的调度过程。\n","like_count":4},{"had_liked":false,"id":364633,"user_name":"Geek_crazydaddy","can_delete":false,"product_type":"c1","uid":2663289,"ip_address":"江苏","ucode":"11A6FF71825CA7","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEIqvYMQ1yscgB6xS4nDkoOuP6KiaCiaichQA1OiaQ9rFmNtT9icgrZxeH1WRn5HfiaibDguj8e0lBpo65ricA/132","comment_is_top":false,"comment_ctime":1671242576,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"把worker获取任务的channel换成channel切片，索引值就是优先级，然后用多个select按序监听这些channel，而且要加default，没读到就立即跳过？","like_count":2,"discussions":[{"author":{"id":1899599,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/fc/4f/0a452c94.jpg","nickname":"大毛","note":"","ucode":"93B18287F06706","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":636602,"discussion_content":"个人感觉这样不太好，假如我们设置有 100 个优先级，就需要创建 100 个 chan，我们需要自己去管理这 100 个 chan，去关注每一个 chan 的容量和阻塞情况，这比较麻烦。另一个问题是 select 监听多个 chan，假如有多个 chan 可以读取（写入），会随机执行一个，这又为优先级的控制增加了难度。\n\n优先级队列的实现有很多，并不一定要使用 chan 作为主要的数据结构。个人感觉使用堆+锁的方式自行设计也未尝不可","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1706530063,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"新疆","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":366345,"user_name":"顷","can_delete":false,"product_type":"c1","uid":1132878,"ip_address":"浙江","ucode":"096B6859C1FA80","user_header":"https://static001.geekbang.org/account/avatar/00/11/49/4e/8798cd01.jpg","comment_is_top":false,"comment_ctime":1673595916,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"if !req.Task.Reload { ... }  &#47;&#47; 这里我们为任务 Task 引入了一个新的字段 Reload，标识当前任务的网页是否可以重复爬取。如果不可以重复爬取，我们需要在失败重试前删除 Visited 中的历史记录。\n\n这里逻辑是不是反了？如果能重复爬，才需要再重新调度之前删掉记录吧。","like_count":0},{"had_liked":false,"id":364779,"user_name":"翡翠虎","can_delete":false,"product_type":"c1","uid":1448015,"ip_address":"广西","ucode":"2572E93C4C57A5","user_header":"https://static001.geekbang.org/account/avatar/00/16/18/4f/9e4d5591.jpg","comment_is_top":false,"comment_ctime":1671479755,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"用哈希表结构有什么好处？这样的话是不是就显得单机了？如果用类似redis这样的存储，加上布谷鸟算法，能够做到既省空间又支持多机协同，会不会更好？","like_count":0,"discussions":[{"author":{"id":1899599,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/fc/4f/0a452c94.jpg","nickname":"大毛","note":"","ucode":"93B18287F06706","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":636601,"discussion_content":"redis 也是哈希，用哈希的好处是查询的时间复杂度是 O(1)，这在去重的场景下通常是最优的选择","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1706529516,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"新疆","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}