{"id":1865,"title":"08 机器学习 | 简约而不简单：线性回归","content":"<p>数学中的线性模型可谓“简约而不简单”：它既能体现出重要的基本思想，又能构造出功能更加强大的非线性模型。在机器学习领域，线性回归就是这样一类基本的任务，它应用了一系列影响深远的数学工具。</p>\n<p>在数理统计中，回归分析是确定多种变量间相互依赖的定量关系的方法。<strong>线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数</strong>。在众多回归分析的方法里，线性回归模型最易于拟合，其估计结果的统计特性也更容易确定，因而得到广泛应用。而在机器学习中，回归问题隐含了输入变量和输出变量均可连续取值的前提，因而利用线性回归模型可以对任意输入给出对输出的估计。</p>\n<p>1875年，从事遗传问题研究的英国统计学家弗朗西斯·高尔顿正在寻找父代与子代身高之间的关系。在分析了1078对父子的身高数据后，他发现这些数据的散点图大致呈直线状态，即父亲的身高和儿子的身高呈正相关关系。而在正相关关系背后还隐藏着另外一个现象：矮个子父亲的儿子更可能比父亲高；而高个子父亲的儿子更可能比父亲矮。</p>\n<p>受表哥查尔斯·达尔文的影响，高尔顿将这种现象称为“<strong>回归效应</strong>”，即大自然将人类身高的分布约束在相对稳定而不产生两极分化的整体水平，并给出了历史上第一个线性回归的表达式：y = 0.516x + 33.73，式中的y和x分别代表以英寸为单位的子代和父代的身高。</p>\n<p>高尔顿的思想在今天的机器学习中依然保持着旺盛的生命力。假定一个实例可以用列向量${\\bf x} = (x_1; x_2; \\cdots, x_n)$表示，每个$x_i$代表了实例在第i个属性上的取值，线性回归的作用就是习得一组参数$w_i, i = 0, 1, \\cdots, n$，使预测输出可以表示为以这组参数为权重的实例属性的线性组合。如果引入常量$x_0 = 1$，线性回归试图学习的模型就是</p>\n<p>$$f({\\bf x}) = {\\bf w} ^ T {\\bf x} = \\sum\\limits_{i = 0}^{n} w_i \\cdot x_i $$</p>\n<p>当实例只有一个属性时，输入和输出之间的关系就是二维平面上的一条直线；当实例的属性数目较多时，线性回归得到的就是n+1维空间上的一个超平面，对应一个维度等于n的线性子空间。</p>\n<p>在训练集上确定系数$w_i$时，预测输出$f({\\bf x})$和真实输出$y$之间的误差是关注的核心指标。在线性回归中，这一误差是以<strong>均方误差</strong>来定义的。当线性回归的模型为二维平面上的直线时，均方误差就是预测输出和真实输出之间的<strong>欧几里得距离</strong>，也就是两点间向量的$L ^ 2$范数。而以使均方误差取得最小值为目标的模型求解方法就是<strong>最小二乘法</strong>，其表达式可以写成</p>\n<p>$${\\mathbf{w}}^* = \\mathop {\\arg \\min }\\limits_{\\mathbf{w}} \\sum\\limits_{k = 1} {{{({{\\mathbf{w}}^T}{{\\mathbf{x}}_k} - {y_k})}^2}} $$</p>\n<p>$$= \\mathop {\\arg \\min }\\limits_{\\mathbf{w}} \\sum\\limits_{k = 1} || y_k - \\mathbf{w}^T \\mathbf{x}_k ||^2 $$</p>\n<p>式中每个${\\bf x}_k$代表训练集中的一个样本。<strong>在单变量线性回归任务中，最小二乘法的作用就是找到一条直线，使所有样本到直线的欧式距离之和最小</strong>。</p>\n<p>说到这里，问题就来了：凭什么使均方误差最小化的参数就是和训练样本匹配的最优模型呢？</p>\n<!-- [[[read_end]]] -->\n<p>这个问题可以从概率论的角度阐释。线性回归得到的是统计意义上的拟合结果，在单变量的情形下，可能每一个样本点都没有落在求得的直线上。</p>\n<p>对这个现象的一种解释是回归结果可以完美匹配理想样本点的分布，但训练中使用的真实样本点是理想样本点和噪声叠加的结果，因而与回归模型之间产生了偏差，而每个样本点上噪声的取值就等于$y_k - f(\\mathbf{x}_k)$。</p>\n<p>假定影响样本点的噪声满足参数为$(0, \\sigma ^ 2)$的正态分布（还记得正态分布的概率密度公式吗？），这意味着噪声等于0的概率密度最大，幅度（无论正负）越大的噪声出现的概率越小。在这种情形下，对参数$\\mathbf{w}$的推导就可以用<strong>最大似然的方式</strong>进行，即在已知样本数据及其分布的条件下，找到使样本数据以最大概率出现的假设。</p>\n<p>单个样本$\\mathbf{x}_k$出现的概率实际上就是噪声等于$y_k - f(\\mathbf{x}_k)$的概率，而相互独立的所有样本同时出现的概率则是每个样本出现概率的乘积，其表达式可以写成</p>\n<p>$$p({{\\mathbf{x}}_1},{{\\mathbf{x}}_2}, \\cdots {{\\mathbf{x}}_k}, \\cdots |{\\mathbf{w}}) =$$</p>\n<p>$$ \\prod\\limits_k {\\frac{1}{{\\sqrt {2\\pi } \\sigma }}} \\exp [ - \\frac{1}{{2{\\sigma ^2}}}{({y_k} - {{\\mathbf{w}}^T}{{\\mathbf{x}}_k})^2}]$$</p>\n<p>而最大似然估计的任务就是让以上表达式的取值最大化。出于计算简便的考虑，上面的乘积式可以通过取对数的方式转化成求和式，且取对数的操作并不会影响其单调性。经过一番运算后，上式的最大化就可以等效为$\\sum\\limits_{k} (y_k - \\mathbf{w} ^ T \\mathbf{x}_k) ^ 2$的最小化。这不就是最小二乘法的结果么？</p>\n<p>因此，<strong>对于单变量线性回归而言，在误差函数服从正态分布的情况下，从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的</strong>。</p>\n<p>确定了最小二乘法的最优性，接下来的问题就是如何求解均方误差的最小值。在单变量线性回归中，其回归方程可以写成$y = w_1x + w_0$。根据最优化理论，将这一表达式代入均方误差的表达式中，并分别对$w_1$和$w_0$求偏导数，令两个偏导数均等于0的取值就是线性回归的最优解，其解析式可以写成</p>\n<p>$$ w_1 = \\dfrac{\\sum\\limits_{k = 1}^m y_k(x_k - \\frac{1}{m} \\sum\\limits_{k = 1}^m x_k)}{\\sum\\limits_{k = 1}^m x_k^2 - \\frac{1}{m} (\\sum\\limits_{k = 1}^m x_k) ^ 2}$$</p>\n<p>$$w_0 = \\dfrac{1}{m} \\sum\\limits_{k = 1}^m (y_k - w_1x_k)$$</p>\n<p><strong>单变量线性回归只是一种最简单的特例</strong>。子代的身高并非仅仅由父母的遗传基因决定，营养条件、生活环境等因素都会产生影响。当样本的描述涉及多个属性时，这类问题就被称为<strong>多元线性回归</strong>。</p>\n<p>多元线性回归中的参数$\\mathbf{w}$也可以用最小二乘法进行估计，其最优解同样用偏导数确定，但参与运算的元素从向量变成了矩阵。在理想的情况下，多元线性回归的最优参数为</p>\n<p>$$ {\\mathbf{w}}^* = (\\mathbf{X} ^ T  \\mathbf{X}) ^ {-1}  \\mathbf{X} ^ T  \\mathbf{y} $$</p>\n<p>式中的$\\mathbf{X}$是由所有样本${\\bf x} = (x_0; x_1; x_2; \\cdots, x_n)$的转置共同构成的矩阵。但这一表达式只在矩阵$(\\mathbf{X} ^ T  \\mathbf{X})$的逆矩阵存在时成立。在大量复杂的实际任务中，每个样本中属性的数目甚至会超过训练集中的样本总数，此时求出的最优解${\\mathbf{w}}^*$就不是唯一的，解的选择将依赖于学习算法的归纳偏好。</p>\n<p>但不论采用怎样的选取标准，存在多个最优解都是无法改变的事实，这也意味着过拟合的产生。更重要的是，在过拟合的情形下，微小扰动给训练数据带来的毫厘之差可能会导致训练出的模型谬以千里，模型的稳定性也就无法保证。</p>\n<p>要解决过拟合问题，常见的做法是正则化，即添加额外的惩罚项。<strong>在线性回归中，正则化的方式根据其使用惩罚项的不同可以分为两种，分别是“岭回归”和“LASSO回归”</strong>。</p>\n<p><strong>在机器学习中，岭回归方法又被称为“参数衰减”</strong>，于20世纪40年代由前苏联学者安德烈·季霍诺夫提出。当然，彼时机器学习尚未诞生，季霍诺夫提出这一方法的主要目的是解决矩阵求逆的稳定性问题，其思想后来被应用到正则化中，形成了今天的岭回归。</p>\n<p>岭回归实现正则化的方式是在原始均方误差项的基础上添加一个待求解参数的二范数项，即最小化的对象变为$|| y_k - \\mathbf{w}^T \\mathbf{x}_k || ^ 2 + || \\Gamma \\mathbf{w}|| ^ 2$，其中的$\\Gamma$被称为<strong>季霍诺夫矩阵</strong>，通常可以简化为一个常数。</p>\n<p>从最优化的角度看，二范数惩罚项的作用在于优先选择范数较小的$\\mathbf{w}$，这相当于在最小均方误差之外额外添加了一重关于最优解特性的约束条件，将最优解限制在高维空间内的一个球里。岭回归的作用相当于在原始最小二乘的结果上做了缩放，虽然最优解中每个参数的贡献被削弱了，但参数的数目并没有变少。</p>\n<p>LASSO回归的全称是“<strong>最小绝对缩减和选择算子</strong>”（Least Absolute Shrinkage and Selection Operator），由加拿大学者罗伯特·提布什拉尼于1996年提出。与岭回归不同的是，LASSO回归选择了待求解参数的一范数项作为惩罚项，即最小化的对象变为$|| y_k - \\mathbf{w}^T \\mathbf{x}_k || ^ 2 + \\lambda ||\\mathbf{w}||_1$，其中的$\\lambda$是一个常数。</p>\n<p><strong>与岭回归相比，LASSO回归的特点在于稀疏性的引入</strong>。它降低了最优解$\\mathbf{w}$的维度，也就是将一部分参数的贡献削弱为0，这就使得$\\mathbf{w}$中元素的数目大大小于原始特征的数目。</p>\n<p>这或多或少可以看作奥卡姆剃刀原理的一种实现：当主要矛盾和次要矛盾同时存在时，优先考虑的必然是主要矛盾。虽然饮食、环境、运动等因素都会影响身高的变化，但决定性因素显然只存在在染色体上。值得一提的是，<strong>引入稀疏性是简化复杂问题的一种常用方法，在数据压缩、信号处理等其他领域中亦有广泛应用</strong>。</p>\n<p>从概率的角度来看，最小二乘法的解析解可以利用正态分布以及最大似然估计求得，这在前文已有说明。岭回归和LASSO回归也可以从概率的视角进行阐释：岭回归是在$w_i$满足正态先验分布的条件下，用最大后验概率进行估计得到的结果；LASSO回归是在$w_i$满足拉普拉斯先验分布的条件下，用最大后验概率进行估计得到的结果。</p>\n<p><strong>但无论岭回归还是LASSO回归，其作用都是通过惩罚项的引入抑制过拟合现象，以训练误差的上升为代价，换取测试误差的下降</strong>。将以上两种方法的思想结合可以得到新的优化方法，在此就不做赘述了。</p>\n<p>今天我和你分享了机器学习基本算法之一的线性回归的基本原理，其要点如下：</p>\n<ul>\n<li>线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数；</li>\n<li>最小二乘法可用于解决单变量线性回归问题，当误差函数服从正态分布时，它与最大似然估计等价；</li>\n<li>多元线性回归问题也可以用最小二乘法求解，但极易出现过拟合现象；</li>\n<li>岭回归和LASSO回归分别通过引入二范数惩罚项和一范数惩罚项抑制过拟合。</li>\n</ul>\n<p>在深度学习大行其道的今天，巨量的参数已经成为常态。在参数越来越多，模型越来越复杂的趋势下，线性回归还能发挥什么样的作用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/c2/3d/c213a86d22def0da9a92fe3092605f3d.jpg?wh=1110*1122\" alt=\"\" /></p>\n","neighbors":{"left":{"article_title":"07 机器学习 | 数山有路，学海无涯：机器学习概论","id":1669},"right":{"article_title":"09 机器学习 | 大道至简：朴素贝叶斯方法","id":1866}},"comments":[{"had_liked":false,"id":1944,"user_name":"Maiza","can_delete":false,"product_type":"c1","uid":1007248,"ip_address":"","ucode":"D6911CDAC0D8C7","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/90/5f79859b.jpg","comment_is_top":false,"comment_ctime":1515200611,"is_pvip":false,"replies":[{"id":"384","content":"公式的出处都是前辈们的推导啊……后面我会再过一遍公式，看看有没有符号解释不清的地方","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1515597748,"ip_address":"","comment_id":1944,"utype":1}],"discussion_count":1,"race_medal":0,"score":"78824611939","product_id":100003101,"comment_content":"老师 每次看到公式的地方就跪了...<br>能麻烦给每个公式标明下出处，方便理解吗？<br>老师认为理所当然的事情对小白来说就是天书啊 。。。。。😂😂😂<br>","like_count":18,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415594,"discussion_content":"公式的出处都是前辈们的推导啊……后面我会再过一遍公式，看看有没有符号解释不清的地方","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1515597748,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":191293,"user_name":"Simon","can_delete":false,"product_type":"c1","uid":1914504,"ip_address":"","ucode":"A8A2E3E57BD029","user_header":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","comment_is_top":false,"comment_ctime":1584761265,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"27354565041","product_id":100003101,"comment_content":"“从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的。” 这是新收获","like_count":7},{"had_liked":false,"id":15180,"user_name":"wdf","can_delete":false,"product_type":"c1","uid":1168358,"ip_address":"","ucode":"179C28A57D41C6","user_header":"","comment_is_top":false,"comment_ctime":1530960402,"is_pvip":false,"replies":[{"id":"5175","content":"这是从贝叶斯角度出发的。假定参数本身已经服从正态&#47;拉普拉斯分布，那么在高斯噪声之下，用参数的似然概率乘以先验就可以得到后验。对后验概率取对数得到的结果和正则化的损失函数形式一致，所以对后验概率的最大化就是对正则化损失函数的最小化。<br>这相当于先假定参数符合特定条件，在此基础上再来计算最优参数。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1531113707,"ip_address":"","comment_id":15180,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23005796882","product_id":100003101,"comment_content":" 为什么说ringe对应的是正态，lasso是拉普拉斯分布","like_count":5,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":420274,"discussion_content":"这是从贝叶斯角度出发的。假定参数本身已经服从正态/拉普拉斯分布，那么在高斯噪声之下，用参数的似然概率乘以先验就可以得到后验。对后验概率取对数得到的结果和正则化的损失函数形式一致，所以对后验概率的最大化就是对正则化损失函数的最小化。\n这相当于先假定参数符合特定条件，在此基础上再来计算最优参数。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1531113707,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208313,"user_name":"nil","can_delete":false,"product_type":"c1","uid":1507193,"ip_address":"","ucode":"0F5D298C1CBB74","user_header":"https://static001.geekbang.org/account/avatar/00/16/ff/79/3b38c9e1.jpg","comment_is_top":false,"comment_ctime":1587339534,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18767208718","product_id":100003101,"comment_content":"概率论真是个厉害的工具，从另一个方面提供了看问题的视角，真所谓横看成林侧成峰，殊途同归。","like_count":4},{"had_liked":false,"id":176955,"user_name":"Jin","can_delete":false,"product_type":"c1","uid":1091562,"ip_address":"","ucode":"21409210CE292E","user_header":"https://static001.geekbang.org/account/avatar/00/10/a7/ea/75b48440.jpg","comment_is_top":false,"comment_ctime":1581235748,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14466137636","product_id":100003101,"comment_content":"老师，“回归”这个词的来历讲解有些不准确，我之前也不明白为啥叫回归，后来看了aurelien 的hands-on machine learning 这本书才知道，来源于统计学，就是您说的那个例子，但是解释不是您说的那样，而是因为高个子人的孩子个子没父母高，我查了词典regression就是有退化的含义。这样我记住了为啥叫回归。谢谢！","like_count":3},{"had_liked":false,"id":32192,"user_name":"haiker","can_delete":false,"product_type":"c1","uid":1131183,"ip_address":"","ucode":"4C4C799CACCF79","user_header":"https://static001.geekbang.org/account/avatar/00/11/42/af/8c37ca95.jpg","comment_is_top":false,"comment_ctime":1539481193,"is_pvip":false,"replies":[{"id":"18927","content":"LASSO的降维应该说是无心插柳，不是以降维为目的，但起到降维的效果。它和PCA这种直接降维的效果还是有区别的。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1545320445,"ip_address":"","comment_id":32192,"utype":1}],"discussion_count":2,"race_medal":1,"score":"10129415785","product_id":100003101,"comment_content":"LASSO 回归感觉也是在做特征降维，会不会和做完特征降维之后再做线性回归效果差不多呢？","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":426673,"discussion_content":"LASSO的降维应该说是无心插柳，不是以降维为目的，但起到降维的效果。它和PCA这种直接降维的效果还是有区别的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545320445,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210688,"discussion_content":"LASSO 有特征选择的效果","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584760962,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3410,"user_name":"Haley_Hu","can_delete":false,"product_type":"c1","uid":1050798,"ip_address":"","ucode":"00036407A508FC","user_header":"https://static001.geekbang.org/account/avatar/00/10/08/ae/e3649ed6.jpg","comment_is_top":false,"comment_ctime":1519446741,"is_pvip":false,"replies":[{"id":"665","content":"范数本身是个数学指标，用这个指标做正则化就是对应的正则化方法","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1519697459,"ip_address":"","comment_id":3410,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10109381333","product_id":100003101,"comment_content":"2范数是不是就指L2正则 1范数就指L1正则","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415892,"discussion_content":"范数本身是个数学指标，用这个指标做正则化就是对应的正则化方法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1519697459,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":1815,"user_name":"秦龙君","can_delete":false,"product_type":"c1","uid":1004181,"ip_address":"","ucode":"2085706DBABD5C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/52/95/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1514547138,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10104481730","product_id":100003101,"comment_content":"学习了。","like_count":2},{"had_liked":false,"id":32194,"user_name":"haiker","can_delete":false,"product_type":"c1","uid":1131183,"ip_address":"","ucode":"4C4C799CACCF79","user_header":"https://static001.geekbang.org/account/avatar/00/11/42/af/8c37ca95.jpg","comment_is_top":false,"comment_ctime":1539481581,"is_pvip":false,"replies":[{"id":"18928","content":"抑制过拟合几乎成为机器学习的核心问题了。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1545320530,"ip_address":"","comment_id":32194,"utype":1}],"discussion_count":1,"race_medal":1,"score":"5834448877","product_id":100003101,"comment_content":"引入抑制过拟合现象，以训练误差的上升为代价，换取测试误差的下降，训练误差和测试误差有时候是不是鱼和熊掌，不可兼得，训练误差太低可能就过拟合了，在测试集上效果就不好了。有些学者建议在训练集上训练的时候要等到稍微过拟合了再结束，因为提前结束的话，可能模型还没训练到位。","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":426675,"discussion_content":"抑制过拟合几乎成为机器学习的核心问题了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545320530,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":355395,"user_name":"早八很难不迟到","can_delete":false,"product_type":"c1","uid":2617586,"ip_address":"北京","ucode":"10116FDC6138EF","user_header":"https://static001.geekbang.org/account/avatar/00/27/f0/f2/d8624dce.jpg","comment_is_top":false,"comment_ctime":1661331771,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1661331771","product_id":100003101,"comment_content":"1. 最小二乘法在误差遵循正态分布的情况下能求得最优模型<br>2. 正则化有两种方式，二范数和一范数<br>3. 一范数正则化的特点能够有效减少参数数量，减少无用特征，保留主要特征","like_count":0},{"had_liked":false,"id":315231,"user_name":"深水蓝","can_delete":false,"product_type":"c1","uid":1637933,"ip_address":"","ucode":"3E3B195DE54DE1","user_header":"https://static001.geekbang.org/account/avatar/00/18/fe/2d/e23fc6ee.jpg","comment_is_top":false,"comment_ctime":1633760141,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1633760141","product_id":100003101,"comment_content":"请问老师 LASSO算子的公式末尾，λ∣∣w∣∣1​ 的这个下标 1 是什么意思啊？","like_count":0,"discussions":[{"author":{"id":1438419,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIoyo8nib1LwRHfCZGubJxuJicRbto8AIb0K6iacHda2Nvx7jpjdm5eZIgHax1dP7ibkDWM5ZV1eNkMkw/132","nickname":"leigk","note":"","ucode":"EC6198DFDAA63A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539568,"discussion_content":"L1范数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639754650,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280205,"user_name":"王  明","can_delete":false,"product_type":"c1","uid":1322189,"ip_address":"","ucode":"2737F5F4FD168C","user_header":"https://static001.geekbang.org/account/avatar/00/14/2c/cd/b88a0922.jpg","comment_is_top":false,"comment_ctime":1614133228,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1614133228","product_id":100003101,"comment_content":"“单个样本 xk​ 出现的概率实际上就是噪声等于 yk​−f(xk​) 的概率”<br>请问老师这句怎么理解啊？","like_count":0},{"had_liked":false,"id":280024,"user_name":"帝江","can_delete":false,"product_type":"c1","uid":1590610,"ip_address":"","ucode":"93CBA4E4D05DA5","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/B9vSOjMc2a86kYA8R5yDkVdFiaj2JeBZ1PuI9oUKhbnvuZwuibdUam6FTcGzDaiaFdk2GWJveUGhfCVpv4KaOdicoQ/132","comment_is_top":false,"comment_ctime":1614058224,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1614058224","product_id":100003101,"comment_content":"唉...听不懂...听不懂...学习这个章节需要什么基础知识啊?","like_count":0},{"had_liked":false,"id":165057,"user_name":"杨家荣","can_delete":false,"product_type":"c1","uid":1259241,"ip_address":"","ucode":"3DA65396C7F002","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132","comment_is_top":false,"comment_ctime":1577153832,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1577153832","product_id":100003101,"comment_content":"岭回归和 LASSO 回归分别通过引入二范数惩罚项和一范数惩罚项抑制过拟合。","like_count":0},{"had_liked":false,"id":122714,"user_name":"w 🍍","can_delete":false,"product_type":"c1","uid":1228290,"ip_address":"","ucode":"F2386C971EC58B","user_header":"https://static001.geekbang.org/account/avatar/00/12/be/02/43202976.jpg","comment_is_top":false,"comment_ctime":1565499535,"is_pvip":false,"replies":[{"id":"65096","content":"是的，感谢指出，一不小心就出来一个bug。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1577932104,"ip_address":"","comment_id":122714,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1565499535","product_id":100003101,"comment_content":"应该是”线性回归得到的就是 n+1 维空间上的一个超平面，对应一个维度等于 n 的线性子空间。“吧，还是我理解不对...<br>","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462370,"discussion_content":"是的，感谢指出，一不小心就出来一个bug。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577932104,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73573,"user_name":"全全","can_delete":false,"product_type":"c1","uid":1339616,"ip_address":"","ucode":"2D76AC91C82E47","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/AoXC8Dwxrs9SKJicxdoiav3pYibibjJeFm2dwDiaK4ly7Jk7ZtPh9J8JzyBibR3xBohvz4NvKlhibn8DlV0Grwv8FNhqg/132","comment_is_top":false,"comment_ctime":1551928214,"is_pvip":false,"replies":[{"id":"57734","content":"谢谢你的建议，因为这个专栏在定位时是音频为主，所以我在写稿时特意避免了图片的使用，现在看来是个败笔。<br>无穷范数的定义式涉及极限，不方便写，你找本书看一看。无穷范数最后的结果就是这个向量里最大的元素。不管1范数还是2范数做正则化项，都是所有元素共同来决定范数取值，损失函数一变化，所有的元素都在变化。结果在无穷范数这里，变成最大的家伙一言堂了。从表达式上说，用无穷范数做正则化相当于求无穷范数的无穷次方，这个数要么等于0要么等于无穷大，也起不到正则化的作用。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1573456753,"ip_address":"","comment_id":73573,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551928214","product_id":100003101,"comment_content":"天一老师，讲的真好！有些内容您可以加上一些图片帮助理解，比如讲特征值分解那里，变换是由矩阵引起的，特征值和特征向量，您用文字解释的非常明白，我又看维基百科里给了一个动图，觉得豁然开朗。还有这次课里那几个范数应用在回归正则化里，也有图片可以帮助理解。有时候学习这个东西，在我完全不懂的时候，解释的多明白都是天书的感觉。只有懂了，再看的时候就有共鸣，看出您文字里背后的意思。这是我的体会。<br>那么天一老师，我的困惑在于，用1范数可以做稀疏，同样是有棱有角的无穷范数，是不是也可以作为正则化的约束条件来实现稀疏呢？<br>可以么？用无穷范数来实现稀疏？直观上看是可以的，如果不可以是因为什么呢？希望老师指点<br>另外如果可以的话，那无穷范数实现的稀疏和1范数所实现的稀疏比较而言，有什么优缺点呢？<br>跟您学，有收获，真心求教！","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442164,"discussion_content":"谢谢你的建议，因为这个专栏在定位时是音频为主，所以我在写稿时特意避免了图片的使用，现在看来是个败笔。\n无穷范数的定义式涉及极限，不方便写，你找本书看一看。无穷范数最后的结果就是这个向量里最大的元素。不管1范数还是2范数做正则化项，都是所有元素共同来决定范数取值，损失函数一变化，所有的元素都在变化。结果在无穷范数这里，变成最大的家伙一言堂了。从表达式上说，用无穷范数做正则化相当于求无穷范数的无穷次方，这个数要么等于0要么等于无穷大，也起不到正则化的作用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573456753,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":61644,"user_name":"Snail@AI_ML","can_delete":false,"product_type":"c1","uid":1361881,"ip_address":"","ucode":"695C0F0F1C6E5C","user_header":"https://static001.geekbang.org/account/avatar/00/14/c7/d9/9fc367b5.jpg","comment_is_top":false,"comment_ctime":1547743300,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547743300","product_id":100003101,"comment_content":"和学习的课程相互印证之后，发现居然更糊涂了:这是上面的章节没有的现象，总结来说，本文更详细并做了一些拓展，比如线性回归的来历，正则化的应用等，课程只是告诉我们正则化的特点和应用，简单粗暴呢","like_count":0},{"had_liked":false,"id":54564,"user_name":"历尽千帆","can_delete":false,"product_type":"c1","uid":1270572,"ip_address":"","ucode":"67A48173352375","user_header":"https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg","comment_is_top":false,"comment_ctime":1545901094,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1545901094","product_id":100003101,"comment_content":"老师~我不太明白，为什么说LASSO回归的特点在于稀疏性的引入？我不太懂这里说的稀疏性是指的什么~","like_count":0,"discussions":[{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210686,"discussion_content":"这里稀疏性指某些属性的权重为0。有很多0，就是稀疏的意思。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1584760885,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":54132,"user_name":"历尽千帆","can_delete":false,"product_type":"c1","uid":1270572,"ip_address":"","ucode":"67A48173352375","user_header":"https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg","comment_is_top":false,"comment_ctime":1545795944,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1545795944","product_id":100003101,"comment_content":"您好，我没有明白，为什么引入常量 x0=1，后面的y=wx才成立呢？","like_count":0,"discussions":[{"author":{"id":1251257,"avatar":"https://static001.geekbang.org/account/avatar/00/13/17/b9/e8b3923c.jpg","nickname":"传世乐章","note":"","ucode":"30119DA76A693F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":41829,"discussion_content":"y= a0 + a1x1 + a2x2 + ...  + anxn   ，要想简化成求和公式，必须x0=1,这样a0在公式中才会写得通","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1572515205,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":45389,"user_name":"十八哥","can_delete":false,"product_type":"c1","uid":1027167,"ip_address":"","ucode":"C0130252F97814","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/5f/894761f8.jpg","comment_is_top":false,"comment_ctime":1543629652,"is_pvip":true,"replies":[{"id":"18937","content":"实际上线性回归只能给出每个属性的权重，当然可以人为地认定权重越大，属性越优。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1545323441,"ip_address":"","comment_id":45389,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1543629652","product_id":100003101,"comment_content":"假定给出美女的标准，数据化。问题提出如何确定美女在泳池中出现的概率？模型输入，地区、区域房价、游泳单价、时间维度、年龄参数等。此时我们可以用线性回归找到这些模型中那个变量是最优的，并且能给出排序。我的理解对吗？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":431014,"discussion_content":"实际上线性回归只能给出每个属性的权重，当然可以人为地认定权重越大，属性越优。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545323441,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":32191,"user_name":"haiker","can_delete":false,"product_type":"c1","uid":1131183,"ip_address":"","ucode":"4C4C799CACCF79","user_header":"https://static001.geekbang.org/account/avatar/00/11/42/af/8c37ca95.jpg","comment_is_top":false,"comment_ctime":1539481026,"is_pvip":false,"replies":[{"id":"18926","content":"超参数，需要提前确定，在不同的正则化超参数下计算出的模型参数也是不一样的。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1545320253,"ip_address":"","comment_id":32191,"utype":1}],"discussion_count":1,"race_medal":1,"score":"1539481026","product_id":100003101,"comment_content":"季霍诺夫矩阵是超参数要提前指定，还是参数，在训练过程中获得呢？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":426672,"discussion_content":"超参数，需要提前确定，在不同的正则化超参数下计算出的模型参数也是不一样的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545320253,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":26353,"user_name":"Howard.Wundt","can_delete":false,"product_type":"c1","uid":1202030,"ip_address":"","ucode":"A94474F731C816","user_header":"https://static001.geekbang.org/account/avatar/00/12/57/6e/ae14e53b.jpg","comment_is_top":false,"comment_ctime":1537575205,"is_pvip":false,"replies":[{"id":"10496","content":"Latex，学术写作排版神器","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1538282928,"ip_address":"","comment_id":26353,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1537575205","product_id":100003101,"comment_content":"老师的数学公式是用什么工具写的，可以具体分享一下吗？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":424602,"discussion_content":"Latex，学术写作排版神器","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1538282928,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":15181,"user_name":"wdf","can_delete":false,"product_type":"c1","uid":1168358,"ip_address":"","ucode":"179C28A57D41C6","user_header":"","comment_is_top":false,"comment_ctime":1530961488,"is_pvip":false,"replies":[{"id":"5176","content":"是的，文中考虑的就是多元的情形。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1531115270,"ip_address":"","comment_id":15181,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1530961488","product_id":100003101,"comment_content":"请问老师，如果是多元回归，假定噪声服从高斯分布极大似然估计和最小二乘法等价吗","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":420275,"discussion_content":"是的，文中考虑的就是多元的情形。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1531115270,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":10009,"user_name":"duchao_hit","can_delete":false,"product_type":"c1","uid":1056201,"ip_address":"","ucode":"9B0A03D9691724","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/mticjmrhDj380Cb21EEz89wa04HF5ia9ze1icZIKGM4JBgkXrVosAHrXFNeGKickHGXAiaF3PzPqwHiaaggsjokTDh3A/132","comment_is_top":false,"comment_ctime":1527217427,"is_pvip":true,"replies":[{"id":"3855","content":"这个表达式写的有问题，等式左侧应该是在给定参数w和输入x的条件下，输出y的概率，也就是p(y | x, w)","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1528378079,"ip_address":"","comment_id":10009,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1527217427","product_id":100003101,"comment_content":"老师，对误差的概率就等于在参数w下样本的条件概率觉得不是很理解","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":418306,"discussion_content":"这个表达式写的有问题，等式左侧应该是在给定参数w和输入x的条件下，输出y的概率，也就是p(y | x, w)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1528378079,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":10008,"user_name":"duchao_hit","can_delete":false,"product_type":"c1","uid":1056201,"ip_address":"","ucode":"9B0A03D9691724","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/mticjmrhDj380Cb21EEz89wa04HF5ia9ze1icZIKGM4JBgkXrVosAHrXFNeGKickHGXAiaF3PzPqwHiaaggsjokTDh3A/132","comment_is_top":false,"comment_ctime":1527217336,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1527217336","product_id":100003101,"comment_content":"老师，关于样本x的概率就等于误差的概率不是很理解。疑惑是y-wx=e，只能说p(y-wx)=p(e)，但不能说p(y-wx)=p(x|w)吧？","like_count":0},{"had_liked":false,"id":2401,"user_name":"刘滨","can_delete":false,"product_type":"c1","uid":1028174,"ip_address":"","ucode":"99BDE2701F50EB","user_header":"https://static001.geekbang.org/account/avatar/00/0f/b0/4e/937ee2ca.jpg","comment_is_top":false,"comment_ctime":1517022617,"is_pvip":false,"replies":[{"id":"538","content":"最小二乘定义了最优化的目标函数，梯度下降要找到最优化问题的最优解，两者大致是目的和手段的关系。<br>最小二乘是有解析解的，如果解析解难以求解，就可以用梯度下降这些数值方法。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1517546732,"ip_address":"","comment_id":2401,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1517022617","product_id":100003101,"comment_content":"老师，请问最小二乘法跟梯度下降方法有什么区别？这里可以用梯度下降方法吗","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415722,"discussion_content":"最小二乘定义了最优化的目标函数，梯度下降要找到最优化问题的最优解，两者大致是目的和手段的关系。\n最小二乘是有解析解的，如果解析解难以求解，就可以用梯度下降这些数值方法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1517546732,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":2184,"user_name":"wolfog","can_delete":false,"product_type":"c1","uid":1005815,"ip_address":"","ucode":"89BFEBE2E00B18","user_header":"https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg","comment_is_top":false,"comment_ctime":1516204127,"is_pvip":false,"replies":[{"id":"444","content":"感谢你的细心，但两个标号意思是不一样的哈：下标代表的是范数维度，上标表示的是平方操作，意思是2范数的平方。所以在上标下面再加一个表示2范数的下标2，是标准的写法。<br>所谓过拟合呢，就是用来描述模型的参数数目太多了，正则化项的作用就是通过不同范数控制参数的取值大小，如果把某些参数抑制为0，这个参数就消失了，也就起到了通过减少参数数目抑制过拟合的作用。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1516527824,"ip_address":"","comment_id":2184,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1516204127","product_id":100003101,"comment_content":"天一老师有段，开头是“LASSO回归的全称最小绝对缩减和选择算子”这一段的倒数第二行的2泛数项和一泛数项写法是否应该统一，要么数字都在右下角，要么都在右上角。今天后面这个惩罚项目不太了解为什么惩罚项目一加，就可以降低了他的过拟合。","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415663,"discussion_content":"感谢你的细心，但两个标号意思是不一样的哈：下标代表的是范数维度，上标表示的是平方操作，意思是2范数的平方。所以在上标下面再加一个表示2范数的下标2，是标准的写法。\n所谓过拟合呢，就是用来描述模型的参数数目太多了，正则化项的作用就是通过不同范数控制参数的取值大小，如果把某些参数抑制为0，这个参数就消失了，也就起到了通过减少参数数目抑制过拟合的作用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1516527824,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":1795,"user_name":"Andy","can_delete":false,"product_type":"c1","uid":1012037,"ip_address":"","ucode":"D69ED1BAF42262","user_header":"https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1514479032,"is_pvip":false,"replies":[{"id":"300","content":"这是由正态分布的特性决定的","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1514724195,"ip_address":"","comment_id":1795,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1514479032","product_id":100003101,"comment_content":"最小二乘的形式，为何跟极大似然估计后的形式一致呢？是巧合吗？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415536,"discussion_content":"这是由正态分布的特性决定的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1514724195,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210693,"discussion_content":"前提：正态分布，然后推导公式得到。文中乘积取对数，也是常见技巧。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584761164,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}