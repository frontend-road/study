{"id":718657,"title":"27｜模型工程（三）：低成本领域模型方案，小团队怎么做大模型？","content":"<p>你好，我是 Tyler。</p><p>在前两节课中，我们学习了如何通过 self-instruct 的方法获取训练数据，以及如何以较低成本训练模型。你对这两个内容掌握得如何？</p><p>今天这节课，我们将继续深入探讨这些算法的具体实现。我们将学习数据增强、全量训练和 LoRA（低秩适应）的低成本领域模型训练。</p><h2>为什么选 Alpaca 项目？</h2><p>为了帮助你快速直观地建立感性认识，我在众多的学习对象中选择了Alpaca这个开源项目。目前，许多领域专属模型的开发方法几乎都源自Alpaca，而且Alpaca的开源实现与工业界的需求紧密契合，可以说达到了工业级的入门标准。而且，Alpaca的全量参数训练和LoRA加速训练方法都得到了出色的开源项目支持。</p><p>我们可以通过研究 Alpaca 项目的原始代码来了解大语言模型的训练方法。在你掌握并灵活使用 Alpaca 之后，就可以逐渐上手工业级复杂大语言模型的开发和微调。好，现在我们正式开始 Alpaca 开源项目的学习。</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/76/a2ffa13131b1a08ce6a536e8660a4b76.png?wh=1641x633\" alt=\"图片\"></p><p>先来看一下数据生成算法的实现，Alpaca 模型是通过对 7B 的 LLaMA 模型使用 self-instruct 论文中的技术生成的 5.2 万条指令遵循数据。self-instruct 论文提出了一种新的生成数据增强方法，可以有效地提高大语言模型的性能。</p><!-- [[[read_end]]] --><p>目前 Alpaca 模型仍在发展中，存在一些局限性，例如安全性问题。不过，安全性问题是开源大语言模型的通用问题，因此在使用 Alpaca 模型时，我们需要采取相应的安全措施，例如使用安全的输入数据和训练方法，以及在部署模型时应采取相应的风控和内容安全策略。</p><p>下面我们开始 Alpaca 的具体实现学习，大致分为数据生成、模型训练和训练提效这几个环节。</p><h2>数据生成过程</h2><p>在<a href=\"https://time.geekbang.org/column/article/713908\">第25节课</a>我们学习了 Alpaca 的数据生成方法，Alpaca对self-instruct（自我教导）数据生成的方式进行了升级，提高了效率，降低了花费。这些升级包括用text-davinci-003（代替davinci）来生成指令数据，并且创建了一个新的提示词模版prompt.txt。</p><p>此外，他还采用了更大胆的批量解码方式，每一次会生成20条指令，大幅度降低了生成数据的费用。同时，数据生成流程也变得更简单了，取消了分类指令和非分类指令之间的差异，每个指令只生成一个实例，而不再生成2到3个实例。</p><p>这些优化成功生成了包含52000个实例的指令数据集，而成本只有不到500美元。初步研究还显示，与之前的self-instruct发布的数据相比，这样生成的这52000条数据更加多样化。</p><p>下一步，我们将带你生成这些数据，体会一下这种无中生有的快乐。</p><p>要运行这些代码，你需要使用你的OpenAI API密钥，设置环境变量OPENAI_API_KEY，然后用以下命令安装所需的依赖。</p><pre><code class=\"language-python\">$ pip install -r requirements.txt\n</code></pre><p>接下来，我们运行生成数据的指令。</p><pre><code class=\"language-python\">$ python -m generate_instruction generate_instruction_following_data\n</code></pre><p>这里generate_instruction_following_data 函数，是为了生成新的指令数据，确保新生成的指令文本不要和已有的指令文本过于相似。生成的数据可以用于微调预训练大语言模型。</p><p>下面我将为你解释一下这个函数的具体实现。步骤稍微有点多，需要你耐心一点，跟住我的节奏还是很容易理解的。</p><p>首先，它加载了种子任务数据，这是人工编写的指令数据，代码中将解析这些JSON格式的数据，提取指令、输入和输出信息。</p><pre><code class=\"language-python\">seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\nseed_instruction_data = [\n&nbsp; &nbsp; {\"instruction\": t[\"instruction\"], \"input\": t[\"instances\"][0][\"input\"], \"output\": t[\"instances\"][0][\"output\"]}\n&nbsp; &nbsp; for t in seed_tasks\n]\n</code></pre><p>接着，它创建了一个输出目录（如果不存在的话），用于存储生成的指令数据。</p><pre><code class=\"language-python\">os.makedirs(output_dir, exist_ok=True)\n</code></pre><p>之后我们需要初始化一些变量，其中包括生成请求的索引、存储机器生成指令的数据列表以及ROUGE评估器。</p><pre><code class=\"language-python\">request_idx = 0\nmachine_instruction_data = []\nscorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n</code></pre><p>这里还有一个进度条，用于可视化地追踪生成进度，如果已经有新的指令数据生成出来，就会更新进度条。</p><pre><code class=\"language-python\">progress_bar = tqdm.tqdm(total=num_instructions_to_generate)\nif machine_instruction_data:\n&nbsp; &nbsp; progress_bar.update(len(machine_instruction_data))\n</code></pre><p>接下来，将所有种子任务指令和已生成的机器指令数据变成 token，以便后续做相似性比较。</p><pre><code class=\"language-python\">all_instructions = [d[\"instruction\"] for d in seed_instruction_data] + [\n&nbsp; &nbsp; d[\"instruction\"] for d in machine_instruction_data\n]\nall_instruction_tokens = [scorer._tokenizer.tokenize(inst) for inst in all_instructions]\n</code></pre><p>之后，算法通过循环生成新的指令数据，直到达到指定的指令数据生成数量。</p><pre><code class=\"language-python\">while len(machine_instruction_data) &lt; num_instructions_to_generate:\n&nbsp; &nbsp; request_idx += 1\n&nbsp; &nbsp; # ... (以下是生成新指令的主要部分)\n\n</code></pre><p>接下来为每个生成请求创建一个输入文本批次，每个批次包含多个提示指令。</p><pre><code class=\"language-python\">batch_inputs = []\nfor _ in range(request_batch_size):\n&nbsp; &nbsp; # only sampling from the seed tasks\n&nbsp; &nbsp; prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n&nbsp; &nbsp; prompt = encode_prompt(prompt_instructions)\n&nbsp; &nbsp; batch_inputs.append(prompt)\n\n</code></pre><p>再然后我们就可以使用OpenAI的API向模型提出请求，生成新的指令数据。</p><pre><code class=\"language-python\">decoding_args = utils.OpenAIDecodingArguments(\n&nbsp; &nbsp; temperature=temperature,\n&nbsp; &nbsp; n=1,\n&nbsp; &nbsp; max_tokens=3072,\n&nbsp; &nbsp; top_p=top_p,\n&nbsp; &nbsp; stop=[\"\\n20\", \"20.\", \"20.\"],\n)\nrequest_start = time.time()\nresults = utils.openai_completion(\n&nbsp; &nbsp; prompts=batch_inputs,\n&nbsp; &nbsp; model_name=model_name,\n&nbsp; &nbsp; batch_size=request_batch_size,\n&nbsp; &nbsp; decoding_args=decoding_args,\n&nbsp; &nbsp; logit_bias={\"50256\": -100},\n)\nrequest_duration = time.time() - request_start\n\n</code></pre><p>再然后，处理生成的结果，提取新生成的指令数据。</p><pre><code class=\"language-python\">process_start = time.time()\ninstruction_data = []\nfor result in results:\n&nbsp; &nbsp; new_instructions = post_process_gpt3_response(num_prompt_instructions, result)\n&nbsp; &nbsp; instruction_data += new_instructions\n\n</code></pre><p>之后是比较关键的<strong>计算相似度环节</strong>。对每个新生成的指令数据，需要使用ROUGE评估方法，计算它与已有指令数据的相似性分数。如果相似度高于0.7，这个指令将不予保留。</p><pre><code class=\"language-python\">total = len(instruction_data)\nkeep = 0\nfor instruction_data_entry in instruction_data:\n&nbsp; &nbsp; # ... (以下是计算相似性分数的主要部分)\n\n</code></pre><p>最后，保存新生成的指令数据到文件，打印每个请求的持续时间、生成的指令数量和保留的指令数量。</p><pre><code class=\"language-python\">process_duration = time.time() - process_start\nprint(f\"Request {request_idx} took {request_duration:.2f}s, processing took {process_duration:.2f}s\")\nprint(f\"Generated {total} instructions, kept {keep} instructions\")\nutils.jdump(machine_instruction_data, os.path.join(output_dir, \"regen.json\"))\n</code></pre><p>到这里，generate_instruction_following_data 函数就完成了它的使命，我们也拿到了生成的指令微调数据。</p><h2>全量参数训练</h2><p>接下来，我们用上一步中生成的数据，来训练 Llama 模型。最终生成的数据存储在 <a href=\"https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json\">alpaca_data.json</a>  文件中，其中的数据集包含了 52000 条指令，每个指令由以下字段组成。</p><ul>\n<li>instruction：一个字符串，描述模型应该执行的任务，这52000条指令都是独一无二的。</li>\n<li>input：是一个可选的字段，用来表示指令的可选上下文或输入。大约40%的示例包含这个字段，例如，当指令是 “总结以下文章” 时，该字段的输入则是文章原文内容。</li>\n<li>output：一个字符串，由 text-davinci-003 生成的、作为指令数据的输出。</li>\n</ul><p>你可以看一下文稿后面的例子，这样更好理解。</p><pre><code class=\"language-python\">[\n    {\n        \"instruction\": \"Give three tips for staying healthy.\",\n        \"input\": \"\",\n        \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"\n    },\n    ...\n]\n</code></pre><p>如果你想要进行微调，首先需要使用以下指令安装必要的依赖。</p><pre><code class=\"language-plain\">$ pip install -r requirements.txt \n</code></pre><p>接下来，需要在拥有4个A100 80G GPU（FSDP full_shard 模式）的机器上，使用我们的数据集对LLaMA-7B进行微调。</p><p>我们使用后面的命令，就可以在 Python 3.10 上复现与论文中模型质量相似的结果。</p><p>首先修改以下配置内容。</p><ol>\n<li>第一要设置端口 &lt;your_random_port&gt;，这个端口用于多机多卡训练。</li>\n<li>第二个参数是原始预训练 llama 模型所在的路径 &lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt;。</li>\n<li>第三个参数是训练完成后模型保存的路径 &lt;your_output_dir&gt;。</li>\n</ol><pre><code class=\"language-python\">torchrun --nproc_per_node=4 --master_port=&lt;your_random_port&gt; train.py \\\n&nbsp; &nbsp; --model_name_or_path &lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt; \\\n&nbsp; &nbsp; --data_path ./alpaca_data.json \\\n&nbsp; &nbsp; --bf16 True \\\n&nbsp; &nbsp; --output_dir &lt;your_output_dir&gt; \\\n&nbsp; &nbsp; --num_train_epochs 3 \\\n&nbsp; &nbsp; --per_device_train_batch_size 4 \\\n&nbsp; &nbsp; --per_device_eval_batch_size 4 \\\n&nbsp; &nbsp; --gradient_accumulation_steps 8 \\\n&nbsp; &nbsp; --evaluation_strategy \"no\" \\\n&nbsp; &nbsp; --save_strategy \"steps\" \\\n&nbsp; &nbsp; --save_steps 2000 \\\n&nbsp; &nbsp; --save_total_limit 1 \\\n&nbsp; &nbsp; --learning_rate 2e-5 \\\n&nbsp; &nbsp; --weight_decay 0. \\\n&nbsp; &nbsp; --warmup_ratio 0.03 \\\n&nbsp; &nbsp; --lr_scheduler_type \"cosine\" \\\n&nbsp; &nbsp; --logging_steps 1 \\\n&nbsp; &nbsp; --fsdp \"full_shard auto_wrap\" \\\n&nbsp; &nbsp; --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n&nbsp; &nbsp; --tf32 True\n</code></pre><p>这个脚本也同样适用于OPT（Open Pre-trained Transformer Language Models）的微调。以下是微调OPT-6.7B的示例，你可以尝试一下 Meta 的另一款模型，看看和 Llama 有什么区别。</p><pre><code class=\"language-python\">torchrun --nproc_per_node=4 --master_port=&lt;your_random_port&gt; train.py \\\n&nbsp; &nbsp; --model_name_or_path \"facebook/opt-6.7b\" \\\n&nbsp; &nbsp; --data_path ./alpaca_data.json \\\n&nbsp; &nbsp; --bf16 True \\\n&nbsp; &nbsp; --output_dir &lt;your_output_dir&gt; \\\n&nbsp; &nbsp; --num_train_epochs 3 \\\n&nbsp; &nbsp; --per_device_train_batch_size 4 \\\n&nbsp; &nbsp; --per_device_eval_batch_size 4 \\\n&nbsp; &nbsp; --gradient_accumulation_steps 8 \\\n&nbsp; &nbsp; --evaluation_strategy \"no\" \\\n&nbsp; &nbsp; --save_strategy \"steps\" \\\n&nbsp; &nbsp; --save_steps 2000 \\\n&nbsp; &nbsp; --save_total_limit 1 \\\n&nbsp; &nbsp; --learning_rate 2e-5 \\\n&nbsp; &nbsp; --weight_decay 0. \\\n&nbsp; &nbsp; --warmup_ratio 0.03 \\\n&nbsp; &nbsp; --lr_scheduler_type \"cosine\" \\\n&nbsp; &nbsp; --logging_steps 1 \\\n&nbsp; &nbsp; --fsdp \"full_shard auto_wrap\" \\\n&nbsp; &nbsp; --fsdp_transformer_layer_cls_to_wrap 'OPTDecoderLayer' \\\n&nbsp; &nbsp; --tf32 True\n</code></pre><p>需要注意的是，<strong>这里提供的训练脚本是为了简化使用，让你可以轻松上手</strong>，所以没有经过特别的性能优化。如果要使用更多的GPU运行，你可以试着调整 gradient_accumulation_steps 的设置。</p><h2>LoRA低成本训练</h2><p>当然，不是所有的同学都有条件进行全量的训练实验，毕竟它需要8张A100的显卡，这时候我们可以运用上节课学到的LoRA（低秩适应）。</p><p>我们可以使用这种方法来复现斯坦福大学 Alpaca 项目的结果。在接下来要讲的这个项目中，也提供了一个 Instruct 模型，而且训练的质量与 text-davinci-003 类似，可以在树莓派上运行的（用于研究）、而且代码也可以轻松扩展到13b、30b 和 65b的模型。</p><p>除了训练代码，该项目还发布了已微调好的模型和 LoRA 权重以及推理的脚本。为了让微调的过程节省资源并且高效，该项目使用了 HuggingFace 的 PEFT 和 Tim Dettmers 的 bitsandbytes 来优化性能。</p><p>在默认的超参数设置下，LoRA 模型的输出与斯坦福 Alpaca 模型相当。不过进一步的调整可能会取得更好的性能。如果你对此感兴趣，不妨亲自尝试一下，并把你的结果在留言区里分享出来。</p><h3>本地设置</h3><p>我们首先在本地设置LoRA，首先安装必要的依赖。</p><pre><code class=\"language-python\">$ pip install -r requirements.txt\n</code></pre><p>如果 bitsandbytes 无法正常工作，可以考虑从源代码进行安装。</p><h3>训练（finetune.py）</h3><p>finetune.py 包含了基于 PEFT 微调 LLaMA 模型的示例，具体的用法是后面这样。</p><pre><code class=\"language-python\">python finetune.py \\\n&nbsp; &nbsp; --base_model 'decapoda-research/llama-7b-hf' \\\n&nbsp; &nbsp; --data_path 'yahma/alpaca-cleaned' \\\n&nbsp; &nbsp; --output_dir './lora-alpaca'\n</code></pre><p>下面我带你梳理一下 train 函数的内容，这是模型训练代码的核心部分，这里我们挑主要的代码逻辑讲解。我们使用了 Hugging Face Transformers 库中的 LlamaForCausalLM 类来加载基础模型，这里通过 base_model 参数来设置。</p><pre><code class=\"language-python\">&nbsp; &nbsp; model = LlamaForCausalLM.from_pretrained(\n&nbsp; &nbsp; &nbsp; &nbsp; base_model,\n&nbsp; &nbsp; &nbsp; &nbsp; load_in_8bit=True,\n&nbsp; &nbsp; &nbsp; &nbsp; torch_dtype=torch.float16,\n&nbsp; &nbsp; &nbsp; &nbsp; device_map=device_map,\n&nbsp; &nbsp; )\n</code></pre><p>接着，我们为 LoRA 方法创建配置，然后把它传到模型的设置中。</p><pre><code class=\"language-python\">&nbsp; &nbsp; config = LoraConfig(\n&nbsp; &nbsp; &nbsp; &nbsp; r=lora_r,\n&nbsp; &nbsp; &nbsp; &nbsp; lora_alpha=lora_alpha,\n&nbsp; &nbsp; &nbsp; &nbsp; target_modules=lora_target_modules,\n&nbsp; &nbsp; &nbsp; &nbsp; lora_dropout=lora_dropout,\n&nbsp; &nbsp; &nbsp; &nbsp; bias=\"none\",\n&nbsp; &nbsp; &nbsp; &nbsp; task_type=\"CAUSAL_LM\",\n&nbsp; &nbsp; )\n&nbsp; &nbsp; model = get_peft_model(model, config)\n</code></pre><p>然后，这里的代码检查了是否有恢复训练的检查点。如果有，它会加载已有的模型权重。</p><pre><code class=\"language-python\">&nbsp; &nbsp; if resume_from_checkpoint:\n&nbsp; &nbsp; &nbsp; &nbsp; # Check the available weights and load them\n&nbsp; &nbsp; &nbsp; &nbsp; checkpoint_name = os.path.join(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; resume_from_checkpoint, \"pytorch_model.bin\"\n&nbsp; &nbsp; &nbsp; &nbsp; )&nbsp; # Full checkpoint\n&nbsp; &nbsp; &nbsp; &nbsp; if not os.path.exists(checkpoint_name):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; checkpoint_name = os.path.join(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; resume_from_checkpoint, \"adapter_model.bin\"\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )&nbsp; # only LoRA model - LoRA config above has to fit\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; resume_from_checkpoint = (\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; False&nbsp; # So the trainer won't try loading its state\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; # The two files above have a different name depending on how they were saved, but are actually the same.\n&nbsp; &nbsp; &nbsp; &nbsp; if os.path.exists(checkpoint_name):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(f\"Restarting from {checkpoint_name}\")\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; adapters_weights = torch.load(checkpoint_name)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; set_peft_model_state_dict(model, adapters_weights)\n&nbsp; &nbsp; &nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(f\"Checkpoint {checkpoint_name} not found\")\n\n</code></pre><p>之后的这段代码用来检查是否在单机多卡（Multi-GPU）环境中运行，如果是，将模型配置为模型并行。</p><pre><code class=\"language-python\">&nbsp; &nbsp; if not ddp and torch.cuda.device_count() &gt; 1:\n&nbsp; &nbsp; &nbsp; &nbsp; # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n&nbsp; &nbsp; &nbsp; &nbsp; model.is_parallelizable = True\n&nbsp; &nbsp; &nbsp; &nbsp; model.model_parallel = True\n</code></pre><p>然后，这里创建了一个 Hugging Face Transformers 的 Trainer 对象，用于管理和执行训练过程，其中设置了训练参数、数据集和数据收集器。</p><pre><code class=\"language-python\">&nbsp; &nbsp; trainer = transformers.Trainer(\n&nbsp; &nbsp; &nbsp; &nbsp; model=model,\n&nbsp; &nbsp; &nbsp; &nbsp; train_dataset=train_data,\n&nbsp; &nbsp; &nbsp; &nbsp; eval_dataset=val_data,\n&nbsp; &nbsp; &nbsp; &nbsp; args=transformers.TrainingArguments(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; per_device_train_batch_size=micro_batch_size,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gradient_accumulation_steps=gradient_accumulation_steps,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; warmup_steps=100,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; num_train_epochs=num_epochs,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; learning_rate=learning_rate,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fp16=True,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logging_steps=10,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; optim=\"adamw_torch\",\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; evaluation_strategy=\"steps\" if val_set_size &gt; 0 else \"no\",\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; save_strategy=\"steps\",\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; eval_steps=200 if val_set_size &gt; 0 else None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; save_steps=200,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; output_dir=output_dir,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; save_total_limit=3,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; load_best_model_at_end=True if val_set_size &gt; 0 else False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ddp_find_unused_parameters=False if ddp else None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; group_by_length=group_by_length,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; report_to=\"wandb\" if use_wandb else None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; run_name=wandb_run_name if use_wandb else None,\n&nbsp; &nbsp; &nbsp; &nbsp; ),\n&nbsp; &nbsp; &nbsp; &nbsp; data_collator=transformers.DataCollatorForSeq2Seq(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n&nbsp; &nbsp; &nbsp; &nbsp; ),\n&nbsp; &nbsp; )\n</code></pre><p>最后，这里开始模型的训练，如果有提供的 checkpoint（检查点），则从该检查点恢复训练。</p><pre><code class=\"language-python\">&nbsp; &nbsp; trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n</code></pre><p>当然，你也可以基于后面的示例，根据具体的需要调整超参数。</p><pre><code class=\"language-python\">python finetune.py \\\n&nbsp; &nbsp; --base_model 'decapoda-research/llama-7b-hf' \\\n&nbsp; &nbsp; --data_path 'yahma/alpaca-cleaned' \\\n&nbsp; &nbsp; --output_dir './lora-alpaca' \\\n&nbsp; &nbsp; --batch_size 128 \\\n&nbsp; &nbsp; --micro_batch_size 4 \\\n&nbsp; &nbsp; --num_epochs 3 \\\n&nbsp; &nbsp; --learning_rate 1e-4 \\\n&nbsp; &nbsp; --cutoff_len 512 \\\n&nbsp; &nbsp; --val_set_size 2000 \\\n&nbsp; &nbsp; --lora_r 8 \\\n&nbsp; &nbsp; --lora_alpha 16 \\\n&nbsp; &nbsp; --lora_dropout 0.05 \\\n&nbsp; &nbsp; --lora_target_modules '[q_proj,v_proj]' \\\n&nbsp; &nbsp; --train_on_inputs \\\n&nbsp; &nbsp; --group_by_length\n</code></pre><p>至此，你已经完成基于 LoRA 的模型训练了，你可以直接将这里的模型用于在线的模型推理。</p><h3>推理（generate.py）</h3><p>当然，如果你没有领域定制训练的需求，只是想看看 Alpaca 默认实现的模型效果是什么样的，可以直接使用 generate.py 从 Hugging Face 模型库读取基础模型和 LoRA 权重，然后运行一个 Gradio 接口，用于对指定输入进行推理。</p><p>你可以使用后面这段示例代码使用模型，具体的权重文件版本可以使用 lora_weights 参数进行指定。</p><pre><code class=\"language-python\">python generate.py \\\n&nbsp; &nbsp; --load_8bit \\\n&nbsp; &nbsp; --base_model 'decapoda-research/llama-7b-hf' \\\n&nbsp; &nbsp; --lora_weights 'tloen/alpaca-lora-7b'\n</code></pre><p>是不是非常方便？你可以参考这节课学习的内容，课后亲手练习一下，尝试搭建起你自己的专属领域大模型，完成你在特定领域的具体任务。</p><h2>总结</h2><p>学到这里，我们做个总结吧。</p><p>在这节课中，我们学到了一系列方法和工具，它们可以帮助你以更低的成本进行模型训练、生成多样化的数据，我们还学习了如何通过 LoRA 来提高性能。如果你对这些技术和流程感兴趣，鼓励你深入学习和实践，以便能够训练出属于你自己的领域专属模型。</p><p>当然，目前的self-instruct和alpaca数据生成方法也存在一些局限性，正如我们在之前的课后习题中提到的，它们的数据质量受限于 GPT 知识的范围。因此，为了生成高质量的训练数据，超越 GPT 知识范围，我们可以考虑其他数据生成方法，比如结合低质量的大规模原始语料，利用大语言模型生成相对少量的高质量语料。</p><p>模型工程的任务主要包括两个方面：对大型语言模型的微调（从1-10）和构建全新的大型语言模型（从0-1）。至此，我们已经完成了模型训练的必修部分，也就是模型微调的内容。</p><h2>思考题</h2><p>根据前面学习的GPT系列原理的知识，想一想构建一个规模达到100B及以上的模型需要怎么做。</p><p>恭喜完成我们第 27 次的打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。</p>","neighbors":{"left":{"article_title":"26｜模型工程（二）：算力受限，如何为“无米之炊”？","id":717818},"right":{"article_title":"28｜总体回顾：工业级AI大模型系统的庐山真面目","id":719033}},"comments":[{"had_liked":false,"id":382665,"user_name":"顾琪瑶","can_delete":false,"product_type":"c1","uid":1179515,"ip_address":"上海","ucode":"71F3F122441915","user_header":"https://static001.geekbang.org/account/avatar/00/11/ff/7b/cbe07b5c.jpg","comment_is_top":false,"comment_ctime":1697777047,"is_pvip":false,"replies":[{"id":139523,"content":"你好，顾琪瑶！即使是偏向大模型的业务应用开发，我也建议根据课程的内容，学习微调能力。因为这样可以让你在技术原理上真正理解大模型能力的来源，帮助你更好地完成业务需求。","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1698378018,"ip_address":"北京","comment_id":382665,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"想问下老师, 在目前的行业内, 如果是偏向基于大模型的业务应用开发的话, 微调是必备技能吗, 还是可选的?","like_count":5,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630384,"discussion_content":"你好，顾琪瑶！即使是偏向大模型的业务应用开发，我也建议根据课程的内容，学习微调能力。因为这样可以让你在技术原理上真正理解大模型能力的来源，帮助你更好地完成业务需求。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698378018,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383151,"user_name":"周晓英","can_delete":false,"product_type":"c1","uid":1361053,"ip_address":"北京","ucode":"157378B0272243","user_header":"https://static001.geekbang.org/account/avatar/00/14/c4/9d/0f4ea119.jpg","comment_is_top":false,"comment_ctime":1698564748,"is_pvip":false,"replies":[{"id":139686,"content":"你好，周晓英。本节课主要学习的是大语言模型的训练方法，如果想训练自己的嵌入表征模型，可以使用第24课中的 sentence-bert（SBERT）算法。","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1699057994,"ip_address":"北京","comment_id":383151,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"老师您好，如果现有的Embedding模型无法完全满足需求，想训练自己领域的Ebedding模型，可以采用您文中的方法吗？","like_count":3,"discussions":[{"author":{"id":1361053,"avatar":"https://static001.geekbang.org/account/avatar/00/14/c4/9d/0f4ea119.jpg","nickname":"周晓英","note":"","ucode":"157378B0272243","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630940,"discussion_content":"谢谢老师","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1699089638,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630910,"discussion_content":"你好，周晓英。本节课主要学习的是大语言模型的训练方法，如果想训练自己的嵌入表征模型，可以使用第24课中的 sentence-bert（SBERT）算法。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1699057994,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382695,"user_name":"陈东","can_delete":false,"product_type":"c1","uid":2213995,"ip_address":"广西","ucode":"FCDE6D237CC621","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Ge7uhlEVxicQT73YuomDPrVKI8UmhqxKWrhtO5GMNlFjrHWfd3HAjgaSribR4Pzorw8yalYGYqJI4VPvUyPzicSKg/132","comment_is_top":false,"comment_ctime":1697814512,"is_pvip":true,"replies":[{"id":139808,"content":"你好，陈东！我的工作主要是负责AI大模型平台体系建设，构建基于大模型技术的应用生态。如果你对我在产业的洞察感兴趣，欢迎你多花时间看一下最后一章的内容，那里有很多一线经历的总结。","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1699373847,"ip_address":"北京","comment_id":382695,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"大模型在老师实践和工作中主要的作用？主要面对什么产业？产生什么价值？","like_count":1,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631191,"discussion_content":"你好，陈东！我的工作主要是负责AI大模型平台体系建设，构建基于大模型技术的应用生态。如果你对我在产业的洞察感兴趣，欢迎你多花时间看一下最后一章的内容，那里有很多一线经历的总结。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1699373847,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":390593,"user_name":"Geek_24ebc1","can_delete":false,"product_type":"c1","uid":1763236,"ip_address":"北京","ucode":"430073D39EAC36","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI64XUGp3cKv9AQZzWWArk0jx05TBycvZL68GsLaTtia9uoHJGae54Dgm2OfBPXc7j77Kiasu6FwcLg/132","comment_is_top":false,"comment_ctime":1715739039,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"想问下老师，领域大模型，都需要提供哪些输入信息？有没有可参考的链接或文案。例如：想做一个油气测井大模型，或者医疗大模型。","like_count":1},{"had_liked":false,"id":388287,"user_name":"R_R","can_delete":false,"product_type":"c1","uid":1336644,"ip_address":"北京","ucode":"88F9452B6DCA81","user_header":"https://static001.geekbang.org/account/avatar/00/14/65/44/a886955f.jpg","comment_is_top":false,"comment_ctime":1709801644,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"好文","like_count":0}]}