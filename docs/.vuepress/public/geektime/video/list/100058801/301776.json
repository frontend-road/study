{"id":301776,"title":"53｜DataStream & DataSet 与Table相互转换","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/geektime-Flink\">https://gitee.com/geektime-geekbang/geektime-Flink</a></p>","comments":[{"had_liked":false,"id":267299,"user_name":"碧雪天虹","can_delete":false,"product_type":"c3","uid":1258359,"ip_address":"","ucode":"313CC048C7E341","user_header":"https://static001.geekbang.org/account/avatar/00/13/33/77/0c593044.jpg","comment_is_top":false,"comment_ctime":1607673349,"is_pvip":false,"replies":[{"id":98226,"content":"没有，dataset后面的版本就不维护了，全部使用datastream批流一体了，建议关注下后续版本","user_name":"作者回复","user_name_real":"张利兵","uid":1119779,"ctime":1609242445,"ip_address":"","comment_id":267299,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100058801,"comment_content":"张老师, 课程里有专门讲 DateSet 的章节吗 ? 能用 Flink 搞定批计算就不上 Spark 了, 小公司人手少, 技术栈越精炼越好.","like_count":4,"discussions":[{"author":{"id":1119779,"avatar":"https://static001.geekbang.org/account/avatar/00/11/16/23/99c7ede5.jpg","nickname":"张利兵","note":"","ucode":"DBAE17970AB143","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511586,"discussion_content":"没有，dataset后面的版本就不维护了，全部使用datastream批流一体了，建议关注下后续版本","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609242445,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1057843,"avatar":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","nickname":"阿甘","note":"","ucode":"BC93175B70E05D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585237,"discussion_content":"可以用flink dataset替代spark批处理吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661412057,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"中国香港","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":327718,"user_name":"hujihu33","can_delete":false,"product_type":"c3","uid":1078729,"ip_address":"","ucode":"686860FE2DFABB","user_header":"https://static001.geekbang.org/account/avatar/00/10/75/c9/fb23007c.jpg","comment_is_top":false,"comment_ctime":1640249586,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100058801,"comment_content":"视频主要内容：\n1，将dataStream 和 dataset 转换为 表， \n      fromDatastream(stream), 对于schema会用默认的 f0， f1...的方式默认定义datastream的字段，字段类型按照stream的定义\n2，将dataStream 和 dataset 转换为 视图    tableenv.createTemporaryView(&quot;&quot;, stream)\n3,  将表转换为datastream 和 dataset ，  有2种模式 append mode  、 retract mode\n    append mode:仅当动态table仅当insert更改进行修改时\n    retract mode:任何情形都可以使用此模式，使用booleam值对insert 和 delete 操作进行标记\n4，将表转换为datastream  -》 datastream&lt;row&gt; dsrow = tableEnv.toAppendStream(table, row.class)\n    datastream&lt;Tuple2&lt;Boolean, row&gt;&gt; dsrow = tableEnv.toRetractStream(table, row.class) ---true insert , false delete\n5， 数据类型和table schema的映射  --- 2种方式  基于位置的映射，基于字段名称映射\n6，基于位置进行映射---基于位置  $(&quot;myLong&quot;), $(&quot;myInt&quot;)\n7，基于名称的映射--- 默认的字段 f0, f1...\n","like_count":0},{"had_liked":false,"id":258010,"user_name":"泊浮目","can_delete":false,"product_type":"c3","uid":1067981,"ip_address":"","ucode":"182A7CC2F6BDAB","user_header":"https://static001.geekbang.org/account/avatar/00/10/4b/cd/185e5378.jpg","comment_is_top":false,"comment_ctime":1604242160,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100058801,"comment_content":"想请教下张老师，为什么update一般会变成delete+insert？我的理解是避免依赖update之前的数据，以及避免单个消息体过大。不知道我理解的对嘛？","like_count":0}]}