[{"article_id":543605,"article_title":"开篇词｜想快速入门音视频技术，你得这么学","article_content":"<p>你好，我是刘歧，江湖人称“大师兄”。目前是快手音视频的首席架构师、FFmpeg的GSoC项目Mentor，也是《FFmpeg从入门到精通》一书的作者。</p><p>算到现在，我在互联网行业摸爬滚打已经有15年了。可能一部分人对于我的了解就是来自介绍FFmpeg的那本书。但其实刚开始我并不是做音视频相关工作的，我做的是机顶盒里图形库的维护，之后也参与过嵌入式平台的flashplayer的开发，中途又因为兴趣转技术去做了设备驱动。可能你觉得我走了一些弯路，最后才找到了自己的归属。实则不然，前面的经历都成为了我的积淀与助力。</p><p>也正是因为那次转技术，我才有幸进入这个行业，并在这个行业有了一些发展。所以如果你正在考虑从事这个行业，或想转入这个行业，不要犹豫，现在就是最好的时候。虽说前些年音视频技术都在持续地发展，但近两年疫情的影响导致音视频需求快速上涨，5G技术又给音视频提供了很好的硬件支持，很多活动转入线上，在线教育、视频会议、电商的直播带货等都迫切地需要音视频技术，所以<strong>今后的音视频开发行业充满了无限的可能</strong>。</p><h2>怎样快速进入音视频领域？</h2><p>我们都知道音视频领域需要的知识储备非常庞大，涉及各种音视频基础知识、常用的技术框架、不同的应用场景。所以我们首先要了解音视频领域有哪些基础概念与知识，比如：</p><!-- [[[read_end]]] --><ul>\n<li>什么是PCM？</li>\n<li>怎么计算音频的码率？</li>\n<li>帧率是什么？都有哪些种类？</li>\n<li>为什么会出现视频播放不了的情况？</li>\n<li>FFmpeg有哪些功能？具体应该怎么使用？</li>\n<li>…</li>\n</ul><p>这些音视频技术的基础知识，是所有音视频开发人员都需要掌握的。如果你现在还不是特别清楚，也不用担心，这些问题在我们的专栏里都能找到答案。</p><p>除了这些基础知识之外，如果你想要快速地入门音视频技术，就需要先找到一个突破口。在音视频领域这个突破口非FFmpeg莫属，FFmpeg是音视频平台及音视频系统开发工作中必不可少的一个组件库，也是我们掌握音视频编解码基础知识与流程的重要抓手。如果能够掌握FFmpeg的用法，那音视频方面的一些基本操作都难不住你。</p><p>但是想要掌握FFmpeg是有一定难度的，尤其是刚刚进入这个行业的时候，很有可能被那一千多页的官方文档吓到，所以你可以先听听我是怎么攻克FFmpeg这一难题的。</p><p>还记得2012我刚开始接触FFmpeg时，国内相关的资料非常少，需要自己去查官方文档。但官方文档是一块硬骨头，十分难啃。为了在一周内解决老板交给我的任务，就硬着头皮看下去了，结果在看完帮助文档最开始的一部分以后，我就发现了窍门，比如根据我想实现的功能，按文档索引线索查看就能快速找到对应信息。尽管“啃”官方文档有难度，但通过这样的“锻炼”我不但学到了技术知识，还学会了怎么用好帮助文档。</p><p>后来因为一个契机，我从2014年开始给FFmpeg项目贡献代码。</p><p>当时我在蓝汛从事CDN流媒体服务开发与支持，遇到了一个客户因为使用FFmpeg切片时hls_wrap参数设置得不太合理，导致客户端出现播放卡顿。但客户认为FFmpeg支持就是合理的。这时候我的职业操守告诉我一定要解决这个问题，所以我就开始参与到FFmpeg社区的交流中，通过不断地给FFmpeg解决bug、添加功能、提交patch，最后我成为了FFmpeg的项目维护者，之后我就按照规则删掉了这个hls_wrap。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/0e/0dc1552c25c31950214f203b31a8020e.png?wh=944x486\" alt=\"\"></p><p>就这样，我频繁地在社区中用代码和大家交流，自己也有了很大提升。</p><p>参与FFmpeg开源社区的交流能够给我们解决很多问题。因为有些时候我们自己改的代码不一定是最合理的，社区中的能人比较多，思考问题会比较全面，相比自己一个人做review，质量也会更有保证。所以在这个专栏里我也会教你如何参与到社区的交流中，创建自己专属的模块，乃至成为社区的开发者。相信通过学习你能够很快融入进去，与来自世界各地的开发者进行交流，探索更多的功能，并从中获得最前沿的信息。</p><p>当熟悉了FFmpeg之后，你就会发现你在做音视频处理，学习音视频各方面的知识都事半功倍了。因为大多数的知识是相通的，如果你能了解并熟练使用FFmpeg，那其他的工具对你来说也就不是什么难事了。</p><p>看完我的这些经验，你是不是对自己的学习路径更加清晰了呢？</p><ol>\n<li>了解音视频相关基础知识与概念；</li>\n<li>找到突破点，学会使用FFmpeg；</li>\n<li>知识迁移，做到举一反三。</li>\n</ol><p>做好这三点，你就能快速地入门音视频技术了。为了帮助你更好地去实施，我会这样来安排这门课程。<br>\n<img src=\"https://static001.geekbang.org/resource/image/e8/e8/e81540bf97afebe006833b0e9a2876e8.png?wh=3017x932\" alt=\"\"></p><ol>\n<li><strong>音视频基础概念</strong></li>\n</ol><p>首先，为了让你对音视频相关的基础概念有一个整体的了解，在专栏的第一部分我会专门讲解音视频相关的参数、视频转码相关的知识、直播行业技术的迭代，扫清你认知上的障碍。这部分是整个专栏的基础，同时也是我们入门音视频技术的一个基础。</p><ol start=\"2\">\n<li><strong>流媒体技术速成</strong></li>\n</ol><p>然后就到了实际操作应用的部分了，这一部分我会重点介绍几个工具，比如直播推流工具OBS、MP4专业工具，以及如何通过FFmpeg的基本用法深度挖掘FFmpeg更多的能力。完成第二部分的学习，你就会对音视频处理的常用工具有一个整体的认识，并且能够掌握如何自助查找FFmpeg的帮助信息，获得相关的音视频处理能力。</p><ol start=\"3\">\n<li><strong>FFmpeg API应用</strong></li>\n</ol><p>第三部分我会详细解读FFmpeg 的几个基础模块、关键结构体和常见的应用场景。学完第三部分，你会对FFmpeg的API接口有一个基本的认识，对FFmpeg常用的音视频处理上下文结构体有一个整体的了解，并且能够结合前面两部分内容做一些基本的音视频工具开发。</p><ol start=\"4\">\n<li><strong>FFmpeg社区“玩法”</strong></li>\n</ol><p>第四部分我会重点介绍FFmpeg开发者常用的工具，FFmpeg开发者平时参与社区交流的规则，如何为FFmpeg添加一个新的模块。之后遇到问题，你就可以参与FFmpeg官方社区的交流与讨论，甚至给社区回馈代码了。</p><p>音视频技术相关的工具怎么用，有什么技巧，我都会倾囊相授，为的就是让你少走一些弯路，减轻你的畏难情绪。相信按照这个学习路径一步步学习、实践，你不但会获得独立处理音视频相关操作的能力，还能借鉴专栏里的各种方法做更多探索。扎实的技术基础和解决问题的方法，是我希望通过专栏传递给你的。</p><p>因为音视频行业在持续发展中，各种生活场景逐渐线上化，比如未来的VR/AR技术、线上会议、远程看诊等都需要强大的音视频能力的加持；并且音视频技术受疫情、元宇宙、5G的影响，迭代速度很快，所以就需要我们有自己独立处理开发需求、独立思考探索的能力，主动地去追逐新技术。</p><p>技术迭代的速度也能从侧面展现出行业发展的态势，现在的音视频技术就如同东方冉冉升起的朝阳，会在未来很长一段时间内持续上升，散发光芒。你愿意成为那万丈光芒中的一缕吗？欢迎你加入到音视频开发的领域中，也希望你能够通过这个专栏掌握音视频的基础知识，打开通往音视频技术世界的大门。期待这一个多月的学习之旅，也期待看到你的成长与进步！</p>","neighbors":{"left":[],"right":{"article_title":"01｜如何从色彩格式、帧率等参数角度看视频图像？","id":541546}}},{"article_id":541546,"article_title":"01｜如何从色彩格式、帧率等参数角度看视频图像？","article_content":"<p>你好，我是刘歧。</p><p>在学习音视频开发之前，有一些知识是必备的，因为在学习的过程中我们会经常遇到这些概念，这也是我们学习音视频开发的基础，所以今天在专栏的第一讲我们就先来看一下那些“不得不”了解的视频基础知识。</p><p>在智能手机和社交媒体盛行的今天，我们每天都在接触各种各样的图像或视频，我们能感知到它们的色彩差异、清晰度、明暗对比等等，那这些画面是怎么形成并展示出来的呢？内部的机制与原理又是怎样的？今天我们就来一一揭秘。我们将从视频/图像的原始数据格式、视频逐行/隔行扫描、帧率、图像分辨率、色域等几方面入手，对视频基础知识做一个整体性的了解。</p><h2>视频、图像像素点数据格式</h2><p>当我们看视频时会看到很多图像，这些图像的展现形式我们中学学习几何课程的时候就接触过，是由一个个像素点组成的线，又由一条条线组成面，这个面铺在屏幕上展现出来的就是我们看到的图像。</p><p>这些图像有黑白的，也有彩色的。这是因为图像输出设备支持的规格不同，所以色彩空间也有所不同，不同的色彩空间能展现的色彩明暗程度，颜色范围等也不同。为了让你对色彩空间有一个基本的认识，这节课我将给你介绍一些常见的色彩格式，分别是：</p><p>• &nbsp;GRAY 色彩空间</p><p>• &nbsp;YUV 色彩空间</p><p>• &nbsp;RGB 色彩空间</p><!-- [[[read_end]]] --><p>• &nbsp;HSL 和 HSV 色彩空间</p><h3>GRAY 灰度模式表示</h3><p>在20世纪80、90年代，国内大多数家庭看的还是黑白电视。那个黑白电视的图像就是以GRAY的方式展现的图像，也就是Gray灰度模式。这一模式为8位展示的灰度，取值0至255，表示明暗程度，0为最黑暗的模式，255为最亮的模式，色彩表示范围如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/f2/77/f2dd26224cfbc38c3f5bc765abdf1b77.png?wh=300x59\" alt=\"图片\" title=\"图1 GARY灰度模式表示\"></p><p>由于每个像素点是用8位深展示的，所以一个像素点等于占用一个字节，一张图像占用的存储空间大小计算方式也比较简单：</p><p><img src=\"https://static001.geekbang.org/resource/image/09/96/0991e8250861857eca1b61608d7d7f96.png?wh=2270x172\" alt=\"\"></p><p>举个例子，如果图像为352x288的分辨率，那么一张图像占用的存储空间应该是352x288，也就是101376个字节大小。</p><h3>YUV 色彩表示</h3><p>在视频领域，通常以YUV的格式来存储和显示图像。其中Y表示视频的灰阶值，也可以理解为亮度值，而UV表示色彩度，如果忽略UV值的话，我们看到的图像与前面提到的GRAY相同，为黑白灰阶形式的图像。YUV最大的优点在于每个像素点的色彩表示值占用的带宽或者存储空间非常少。</p><p>你可以看一下这张图片，是原图与YUV的Y通道、U通道和V通道的图像示例：</p><p><img src=\"https://static001.geekbang.org/resource/image/01/9c/010e44f0ea9d55d50fe0b9149fcae39c.png?wh=1582x1196\" alt=\"图片\" title=\"图2 YUV图像示例\n\"></p><p>为节省带宽起见，大多数YUV格式平均使用的每像素位数都少于24位。主要的色彩采样格式有YCbCr 4：2：0、YCbCr 4：2：2、YCbCr 4：1：1和YCbCr 4：4：4。YUV的表示法也称为A：B：C表示法。</p><p>为了便于理解，下面我以352x288的图像大小为例，来详细介绍一下各个采样格式的区别。</p><ul>\n<li>YUV 4：4：4 格式</li>\n</ul><p>yuv444表示4比4比4的yuv取样，水平每1个像素（即1x1的1个像素）中y取样1个，u取样1个，v取样1个，所以每1x1个像素y占有1个字节，u占有1个字节，v占有1个字节，平均yuv444每个像素所占位数为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/45/53/451aa5319b93ffb752d97823107edb53.png?wh=1796x178\" alt=\"\"></p><p>那么352x288分辨率的一帧图像占用的存储空间为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/11/10/118b8649982e10d7eff013429ab1e110.png?wh=1766x146\" alt=\"\"></p><ul>\n<li>YUV 4：2：2 格式</li>\n</ul><p>yuv422表示4比2比2的yuv取样，水平每2个像素（即2x1的2个像素）中y取样2个，u取样1个，v取样1个，所以每2x1个像素y占有2个字节，u占有1个字节，v占有1个字节，平均yuv422每个像素所占位数为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/25/b6/25d2cc4be59e7a802af8c0ca359311b6.png?wh=1730x156\" alt=\"\"></p><p>那么352x288分辨率的一帧图像占用的存储空间为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/ab/6c/abd2dyy92616e22da06812d665ed0a6c.png?wh=1758x132\" alt=\"\"></p><ul>\n<li>yuv4：1：1 格式</li>\n</ul><p>yuv411表示4比1比1的yuv取样，水平每4个像素（即4x1的4个像素）中y取样4个，u取样1个，v取样1个，所以每4x1个像素y占有4个字节，u占有1个字节，v占有1个字节，平均yuv411每个像素所占位数为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/b7/f5/b7f2397063ed7f37cfe055963e620ff5.png?wh=1718x168\" alt=\"\"></p><p>那么352x288分辨率的一帧图像占用的存储空间为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/88/69/885b0cc56482e28a1096e5888a461569.png?wh=1744x150\" alt=\"\"></p><ul>\n<li>yuv4：2：0 格式</li>\n</ul><p>yuv420表示4比2比0的yuv取样，水平每2个像素与垂直每2个像素（即2x2的2个像素）中y取样4个，u取样1个，v取样1个，所以每2x2个像素y占有4个字节，u占有1个字节，v占有1个字节，平均yuv420每个像素所占位数为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/09/c8/0974460f6db4a486f43cd2b5b1a9c6c8.png?wh=1716x132\" alt=\"\"></p><p>那么352x288分辨率的一帧图像占用的存储空间为：<br>\n<img src=\"https://static001.geekbang.org/resource/image/d2/d0/d27da35fdc66728bcda9977b68258dd0.png?wh=1778x146\" alt=\"\"></p><p>为了方便理解YUV在内存中的存储方式，我们以宽度为6、高度为4的yuv420格式为例，一帧图像读取和存储在内存中的方式如图：</p><p><img src=\"https://static001.geekbang.org/resource/image/58/e4/58a9774ed3fdcd8ec7738cc5b6c5c7e4.png?wh=1127x418\" alt=\"图片\" title=\"图3 yuv420格式图像的读取和存储方式\"></p><h3>RGB 色彩表示</h3><p>三原色光模式（RGB color model），又称RGB颜色模型或红绿蓝颜色模型，是一种加色模型，将红（Red）、绿（Green）、蓝（Blue）三原色的色光按照不同的比例相加，来合成各种色彩光。</p><p>每象素24位编码的RGB值：使用三个8位无符号整数（0到255）表示红色、绿色和蓝色的强度。这是当前主流的标准表示方法，用于交换真彩色和JPEG或者TIFF等图像文件格式里的通用颜色。它可以产生一千六百万种颜色组合，对人类的眼睛来说，其中有许多颜色已经无法确切地分辨了。</p><p>使用每原色8位的全值域，RGB可以有256个级别的白-灰-黑深浅变化，255个级别的红色、绿色和蓝色以及它们等量混合的深浅变化，但是其他色相的深浅变化相对要少一些。</p><p>典型使用上，数字视频的RGB不是全值域的。视频RGB有比例和偏移量的约定，即 （16, 16, 16）是黑色，（235, 235, 235）是白色。例如，这种比例和偏移量就用在了CCIR 601的数字RGB定义中。</p><p>RGB常见的展现方式分为16位模式和32位模式（32位模式中主要用其中24位来表示RGB）。16位模式（RGB565、BGR565、ARGB1555、ABGR1555）分配给每种原色各为5位，其中绿色为6位，因为人眼对绿色分辨的色调更敏感。但某些情况下每种原色各占5位，余下的1位不使用或者表示Alpha通道透明度。</p><p>32位模式（ARGB8888），实际就是24位模式，余下的8位不分配到象素中，这种模式是为了提高数据处理的速度。同样在一些特殊情况下，在有些设备中或者图像色彩处理内存中，余下的8位用来表示象素的透明度（Alpha通道透明度）。</p><p>这就是我们所说的RGB图像色彩表示，你可以对照着RGB色彩分布直方图来理解。</p><p><img src=\"https://static001.geekbang.org/resource/image/05/a8/05030aa82b28aaf69d361faf0135b6a8.png?wh=420x385\" alt=\"图片\" title=\"图4 RGB色彩分布直方图\"></p><h3>HSL 与 HSV 色彩表示</h3><p>理解了RGB的色彩表示法，相信HSL和HSV对你来说就比较简单了。因为HSL和HSV是将RGB色彩模型中的点放在圆柱坐标系中的表示法，在视觉上会比RGB模型更加直观。</p><p>HSL，就是色相（Hue）、饱和度（ Saturation）、亮度（ Lightness）。HSV是色相（Hue）、饱和度（ Saturation）和明度（Value）。色相（H）是色彩的基本属性，就是平常我们所说的颜色名称，如红色、黄色等；饱和度（S）是指色彩的纯度，越高色彩越纯，低则逐渐变灰，取0～100%的数值；明度（V）和亮度（L），同样取0～100%的数值。</p><p>HSL和HSV二者都把颜色描述在圆柱坐标系里的点内，这个圆柱的中心轴取值为自底部的黑色到顶部的白色，而在它们中间的是灰色，绕这个轴的角度对应于“色相”，到这个轴的距离对应于“饱和度”，而沿着这个轴的高度对应于“亮度”、“色调”或“明度”。如图：</p><p><img src=\"https://static001.geekbang.org/resource/image/0b/09/0b9f69a9e9dc032eaf6e98e7ddea4909.png?wh=1024x1024\" alt=\"图片\" title=\"图5 HSL和HSV圆柱坐标系表示法\"></p><p>HSV色彩空间还可以表示为类似于上述圆柱体的圆锥体，色相沿着圆柱体的外圆周变化，饱和度沿着从横截面的圆心的距离变化，明度沿着横截面到底面和顶面的距离而变化。这种用圆锥体来表示HSV色彩空间的方式可能更加精确，有些图像在RGB或者YUV的色彩模型中处理起来并不精准，我们可以将图像转换为HSV色彩空间，再进行处理，效果会更好。例如图像的抠像处理，用圆锥体表示在多数情况下更实用、更精准。如图：</p><p><img src=\"https://static001.geekbang.org/resource/image/4c/y2/4cc53707630c3309ae172686f6957yy2.png?wh=1280x1024\" alt=\"图片\" title=\"图6 HSL和HSV圆锥体表示法\"></p><h2>图像的色彩空间</h2><p>了解了视频和图像的集中色彩表示方式，那我们是不是用相同的数据格式就能输出颜色完全一样的图像呢？不一定，你可以观察一下电视中的视频图像、电脑屏幕中的视频图像、打印机打印出来的视频图像，同一张图像会有不同的颜色差异，甚至不同的电脑屏幕看到的视频图像、不同的电视看到的视频图像，有时也会存在颜色差异，比如：</p><p><img src=\"https://static001.geekbang.org/resource/image/87/57/87c48e95940372ec867277daf476ca57.png?wh=1919x414\" alt=\"图片\" title=\"图7 图像显示的颜色差异\"></p><p>如果仔细观察的话，会发现右图的颜色比左图的颜色更深一些。之所以会出现这样的差异，主要是因为图像受到了色彩空间参数的影响。我们这里说的色彩空间也叫色域，指某种表色模式用所能表达的颜色构成的范围区域。而这个范围，不同的标准支持的范围则不同，下面，我们来看三种范围，分别为基于CIE模型表示的BT.601、BT.709和BT.2020范围。</p><p><img src=\"https://static001.geekbang.org/resource/image/91/3c/9137fa47925b35de69aa97651bd6b63c.png?wh=1920x795\" alt=\"图片\" title=\"图8 从左到右依次是BT.601、BT.709和BT.2020\"></p><p>色彩空间除了BT.601、BT.709和BT.2020以外，还有很多标准格式，具体的标准我就不在这里一一列举了，在用到的时候，可以使用参考标准（可参考标准：H.273）进行对比。当有人反馈偏色的问题时可以优先考虑是色彩空间的差异导致的，需要调整视频格式（Video Format）、色彩原色（Colour primaries）、转换特性（Transfer characteristics）和矩阵系数（Matrix coefficients）等参数。</p><p>现在我们对色彩的相关知识有了一些基本的了解。色彩格式是图像显示的基础，但是视频技术不仅仅需要知道色彩格式，想要理解视频图像的话，还需要弄清楚一些现象，比如有的视频图像运动的时候会有条纹，有的视频图像在运动的时候没有条纹，是什么原因呢？还有，我们在用一些工具导出电影视频的时候，一般会按照23.97fps的帧率导出，而很多公众号或者媒体在宣传支持60帧帧率，这又是为什么呢？接下来我们通过学习视频逐行、隔行扫描与帧率方面的内容，来一一揭秘。</p><h2>视频逐行、隔行扫描与帧率</h2><p>当我们看到一些老电视剧、老电影或者一些DV机拍摄的视频时，会发现视频中物体在移动时会出现条纹，这些条纹的存在主要是因为视频采用了隔行扫描的刷新方式。</p><h3>隔行扫描与逐行扫描</h3><p>隔行扫描（Interlaced）是一种将图像隔行显示在扫描式显示设备上的方法，例如早期的CRT电脑显示器。非隔行扫描的扫描方法，即逐行扫描（Progressive），通常从上到下地扫描每帧图像，这个过程消耗的时间比较长，占用的频宽比较大，所以在频宽不够时，很容易因为阴极射线的荧光衰减在视觉上产生闪烁的效应。而相比逐行扫描，隔行扫描占用带宽比较小。扫描设备会交换扫描偶数行和奇数行，同一张图像要刷两次，所以就产生了我们前面说的条纹。</p><p><img src=\"https://static001.geekbang.org/resource/image/70/78/7033730582dff921acc94e6fb9df9078.png?wh=188x142\" alt=\"图片\" title=\"图9 隔行扫描效果\"></p><p>早期的显示器设备刷新率比较低，所以不太适合使用逐行扫描，一般都使用隔行扫描。在隔行扫描的时候，我们常见的分辨率描述是720i、1080i，“i”就是Interlaced。现在我们看视频播放器相关广告和说明时，还经常会看到720p、1080p这样的说法，这个“p”又代表什么呢？</p><p>在如今这个时代的显示器和电视中，由于逐行扫描显示的刷新率的提高，使用者已经不会感觉到屏幕闪烁了。因此，隔行扫描技术逐渐被取代，逐行扫描越来越常见，也就是我们经常见到的720p、1080p。</p><p>当我们拿到隔行扫描/逐行扫描的数据后，总是会看到这样的参数：25fps、30fps、60fps等等。里面的fps是什么呢？</p><h3>帧率</h3><p>就是我们平时提到的帧率（FrameRate），指一秒钟刷新的视频图像帧数（Frames Per Second），视频一秒钟可以刷新多少帧，取决于显示设备的刷新能力。不同时代的设备，不同场景的视频显示设备，刷新的能力也不同，所以针对不同的场景也出现了很多种标准，例如：</p><ol>\n<li>NTSC标准的帧率是 30000/1001，大约为 29.97 fps；</li>\n<li>PAL标准的帧率是 25/1，为25 fps；</li>\n<li>QNTSC 标准的帧率是 30000/1001，大约为 29.97 fps；</li>\n<li>QPAL标准的帧率是 25/1，为25 fps；</li>\n<li>SNTSC标准的帧率是 30000/1001，大约为 29.97 fps；</li>\n<li>SPAL标准的帧率是 25/1，为25 fps；</li>\n<li>FILM标准的帧率是 24/1，为24 fps；</li>\n<li>NTSC-FILM标准的帧率是 24000/1001，大约为 23.976 fps。</li>\n</ol><p>如果用心观察的话，你会发现NTSC标准的分辨率都不是整除的帧率，分母都是1001，为什么会这样呢？</p><p>NTSC 制式的标准为了解决因为色度和亮度频率不同引起失真色差的问题，将频率降低千分之一，于是就看到了有零有整的帧率。我们在电影院看的电影的帧率，实际上标准的是 23.97 fps，所以我们可以看到给院线做视频后期制作的剪辑师们最终渲染视频的时候，大多数会选择23.97 fps的帧率导出。关于视频刷新帧率背后更详细的知识，如果感兴趣的话，你可以继续阅读一下《The Black Art of Video Game Console Design》。</p><p>说到这里，我们再来解答一下前面的问题，为什么有些公众号宣传自己的编码和设备支持60帧帧率呢？这是因为科技在进步，有些显示设备的刷新率更高了，为了让我们的眼球看着屏幕上的物体运动更流畅，所以定制了60帧，这也是为了宣传自己设备的功能更加先进、强大。但是在院线标准中，60fps刷新率的设备并没有大范围升级完毕，当前我们看的依然还是以film、ntsc-film标准居多。</p><h2>图像分辨率与比例</h2><p>最后我们来看一下另一个与图像相关的重要概念——分辨率。当人们在谈论流畅、标清、高清、超高清等清晰度的时候，其实主要想表达的是分辨率。它是衡量图像细节表现力的重要的技术参数。</p><p>除了分辨率之外，我们还需要结合视频的类型、场景等设置适合的码率（单位时间内传递的数据量）。随着视频平台竞争越来越激烈，网络与存储的开销越来越高，有了各种定制的参数设置与算法，在分辨率相同的情况下做了更深层的优化，比如极速高清、极致高清、窄带高清等。但是目前人们对流畅、标清、高清、超高清等清晰度的理解，其实普遍还是指分辨率。</p><p>一般，分辨率越高代表图像质量越好，越能看到图像的更多细节，文件也就会越大。分辨率通常由宽、高与像素点占用的位数组成，计算方式为图像的宽乘以高。在提到显示分辨率的时候，人们还常常会提到宽高比，即DAR。DAR是显示宽高比率（display aspect ratio），表示不同分辨率的图像的差别。</p><p><img src=\"https://static001.geekbang.org/resource/image/19/0e/19f244e3e85422ac3d208d55da383d0e.png?wh=420x71\" alt=\"图片\" title=\"图10 DAR不同分辨率的图像的差别\"></p><p>而分辨率在我们日常的应用中各家的档位定义均有不同，但是在国际的标准中还是有一个参考定义的，并且分辨率都有定义名称。为了方便理解，我们来看一下分辨率的示意图，如图：</p><p><img src=\"https://static001.geekbang.org/resource/image/b5/dc/b5499c40f19dffb4e03e7a1b7fb790dc.png?wh=1920x1013\" alt=\"图片\" title=\"图11 分辨率示意图\"></p><p>我们经常听到人们提到1080p、4K，其实它们还有更标准的称呼或者叫法，例如1080p我们又叫Full-HD，通常接近4K的分辨率我们叫4K也没太大问题，像有更标准的叫法，比如3840x2160 的分辨率应该是UHD-1。但是如果直接按标准叫法来叫的话，国内很多人可能不太习惯，为了便于区分，通常就直接说分辨率的宽乘以高的数值。因为4k的表述比较简洁，所以就可以模糊地说是4K了。</p><h2>小结</h2><p>好了，这就是今天的内容，最后我们来总结一下吧！</p><p>想要做好视频工作，就必然绕不开这些基础知识，今天我分别从视频图像像素点数据格式、视频逐行/隔行扫描、帧率、分辨率与比例、色域几个方面带你做了一个概览，这几个方面是组成视频基础最重要的几块基石。</p><p><img src=\"https://static001.geekbang.org/resource/image/70/f9/707b26430668337c198e9985202dcbf9.png?wh=1920x936\" alt=\"图片\"></p><p>音视频技术与计算机图形学在图像处理方面略有相似，在做视频技术的时候，会频繁地用到图像色彩相关的知识。所以这节课我详细地介绍了GARY、YUV、RGB、HSL/HSV四种色彩表示模式。但如果想要做好视频技术，仅仅知道一些图像色彩知识是万万不行的。因为视频是连续的图像序列，所以关于视频逐行/隔行扫描、帧的刷新频率等相关知识也必不可少。</p><p>图像序列裸数据占用的存储和带宽极高，为了降低存储和传输带宽，我们就需要做图像的数据压缩，图像压缩以有损压缩为主，加上图像本身色彩格式多样，所以难免会有偏色等问题，学完今天的课程你应该能想到这主要是色彩空间的差异导致的，这时候我们需要调整各项参数来解决问题。用户观看视频的时候还需要解码视频数据包，为图像色彩的像素点表示数据，所以我们就又需要用到图像与色彩技术了。</p><p>这一整套流程下来，你会发现里面囊括了今天我们学习的所有内容，希望这节课能够为你接下来的学习打下一个好的基础，我们一起加油吧！</p><h2>思考题</h2><p>最后我们来思考一个问题：我们常说的YUV与MP4、H.264、RTMP之间是什么样的关系呢？</p><p>欢迎在评论区留下你的答案和我讨论，相信通过深度的思考，能够让你对这节课的内容有更深刻的理解。欢迎你把这节课分享给需要的朋友，我们一起学习，一起探索。我们下节课再见！</p>","neighbors":{"left":{"article_title":"开篇词｜想快速入门音视频技术，你得这么学","id":543605},"right":{"article_title":"02｜音频从采集到输出涉及哪些关键参数？","id":544004}}},{"article_id":544004,"article_title":"02｜音频从采集到输出涉及哪些关键参数？","article_content":"<p>你好，我是刘歧。</p><p>上一节课我们学习了视频与图像相关的基础知识，相信你对视频/图像中的色彩表示方式、色域、帧率等相关概念已经有了一定的了解。在音视频技术开发与应用领域，除了视频与图像的知识外，我们还会接触到一些音频相关的知识，所以这节课我们会聚焦音频基础知识，为之后FFmpeg 音频相关内容的学习做好铺垫。</p><p>我们平常听到的自然界的声音，比如说鸟鸣、水流，其实是一种模拟信号，声音是振动产生的一种声波，通过气态、液态、固态的物理介质传播并能被人或动物感知的波动现象。声音的频率一般会以赫兹（Hz）表示，指每秒钟周期性振动的次数。而声音的强度单位则用分贝（dB）来表示。现如今我们在电脑上、Pad上、手机上听到的音乐、声音等音频信号，均为数字信号。</p><h2>音频采样数据格式</h2><p>介绍音频采样数据格式之前，我们需要先了解音频从采集一直到我们耳朵听到声音这个过程中都发生了什么，我们先看一下下面这张流程图：</p><p><img src=\"https://static001.geekbang.org/resource/image/ae/f2/aec34f3bc25b0ffcca2718eaa0b5b1f2.png?wh=1331x352\" alt=\"图片\"></p><p>首先我们说的话或者在自然界中听到的一些声音，比如鸟鸣，水流等，都是通过空气振动来传输的模拟信号，我们可以通过麦克风或者拾音器采集到声音的模拟信号，然后将模拟信号转换成数字信号，这个过程可以通过麦克风来做，也可以通过音频的转换器来做，转换成数字信号之后将数字信息存储起来，或者输出到扬声器，扬声器会根据数字信号产生一定频率的振动，然后通过空气传播模拟信号到我们的耳朵里面，我们就听到了对应的声音。</p><!-- [[[read_end]]] --><p>在这个流程里我们需要了解一个基本的操作，就是<strong>先采集到模拟信号，然后通过ADC（模数转换）将模拟信号转换成数字信号以后，再通过 PCM（Pulse Code Modulation）脉冲编码调制对连续变化的模拟信号进行采样、量化和编码转换成离散的数字信号，</strong>从而实现音频信号的采集。另外，也可以将采集的音频信号输出到扬声器、耳机之类的设备。</p><p>我们上面说的PCM文件就是未经封装的音频原始文件，或者叫做音频“裸数据”。不同的扬声器、耳机设备，甚至是声卡输出设备，对音频的裸数据支持的情况不一样，有的设备支持单精度浮点型数据、有的设备支持双精度浮点型数据、有的设备支持无符号型数据、有的设备支持有符号型数据。因为输出的数据类型的支持不同，所以PCM采样数据的格式在输出之前，需要转换一下。这些数据的格式我们通常称之为采样数据格式。</p><h2>音频采样频率</h2><p>音频PCM数据的输入和输出是需要有一个频率的，频率通常在我们听觉可接受的范围内，太高或者太低我们都听不见，通常我们人耳能够听到的频率范围是在20Hz～20kHz之间，为了保证音频不失真，音频的采样频率通常应该在40kHz以上，而理论上采样率大于40kHz的音频格式都可以称之为无损格式。现在一般的专业设备的采样频率为44100Hz（也称之为44.1kHz）。并且44.1kHz是专业音频中的最低采样率。当然要听到更高采样率，比如96kHz、192kHz采样频率中的细节的话，就取决于耳朵和对应的设备了。</p><p>下面我简单地介绍一下，在数字音频领域常用的采样率与对应的使用场景：</p><ul>\n<li>8000 Hz 主要是电话通信时用的采样率，对于传达人们说话时的声音已经足够了；</li>\n<li>11025 Hz、22050 Hz 主要是无线电广播用的采样率；</li>\n<li>44100 Hz 常用于音频 CD，MP3 音乐播放等场景；</li>\n<li>48000 Hz 常用于 miniDV、数字电视、DVD、电影和专业音频等设备中。</li>\n</ul><h2>音频声道及其布局</h2><p>当我们戴着耳机看电视剧、电影、听音乐、开会的时候，会发现左右耳朵听到的声音有时候会有一些差别，尤其是当我们看TVB港剧的时候，一只耳朵听到的是粤语，另一只耳朵听到的是普通话。因为视频里面的音频支持左声道右声道内容不同，所以有些时候我们看电影可以切换左声道右声道模式。</p><p>采集不同方位的声源，然后通过不同方位的扬声器播放出来就产生了不同的声道。其实我们常见的声道内容除了左声道、右声道，还有立体声等，当我们听到的音频声道比较多，比如听交响乐的时候，立体感会尤为明显，示意图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/26/d0/2633eb025d3243359794929b518f76d0.png?wh=1249x850\" alt=\"图片\"></p><p>实际上，音频的声道布局不仅仅是上图这么简单。音频技术发展至今，声道布局远比图片显示的复杂得多，在后面课程中我们会看到更多、更复杂的声道布局。</p><h2>音频采样位深度</h2><p>采样的位深度，也叫采样位深，它决定了声音的动态范围。平时，我们常见的16位（16bit）可以记录大概 96 分贝（96dB）的动态范围。也可以理解为每一个比特大约可以记录6dB的声音。同理，20bit可记录的动态范围大概是120dB，24bit就大概是144dB。</p><p>计算公式如下：</p><p>$$20 \\times math.log10(65535)$$</p><p>举个例子，我们假定0dB为峰值，那么音频的振动幅度就需要以向下延伸计算，所以音频可记录的动态范围就是“-96dB～0dB”。而24bit的高清音频的动态范围就是“-144dB～0dB”。由此可见，位深度较高时，有更大的动态范围可利用，可以记录更低电平的细节。</p><p><strong>但位深度并不是越大越好，也不是越小越好，不同的场景有不同的应用。</strong></p><p>44dB 属于人类可以接受的程度，55dB 会使人感觉到烦躁，60dB 会让人没有睡意，70dB 会令人精神紧张，85dB 长时间听会让人感觉刺耳，100dB 会使人暂时失去听觉，120dB 可以瞬间刺穿你的耳膜，160dB 会通过空气振波震碎玻璃，200dB 可以使人死亡。也就是说，其实如果真的能够让自己的声音达到一定分贝的话，武侠片里面的狮吼功是有可能成为现实的。</p><p>通常为了高保真，我们会选择使用32bit，甚至64bit来表示音频。而常规音频通话使用16bit来表示即可，当然条件有限的话，8bit也可以，但它是底线。因为8bit的音频表示，听起来有时候会比较模糊。</p><h2>音频的码率</h2><p>接下来我们看一下音频的码率。所谓码率，我们通常可以理解为按照某种频率计算一定数据量的单位，重点体现在“率”上面，我们常用的码率统计时间单位为秒，所以码率也就是一秒钟的数据量，通常我们用bps（bits per second）来表示，也就是每秒钟有多少位数据。而我们平时所说的码率，可以简单理解为每秒钟存储或传输的编码压缩后的数据量。</p><p>我们在很多音乐播放器软件中能看到音乐的格式，比如腾讯音乐里面的SQ、HQ、无损等，这些音乐的音频码率、采样率都是很高的。</p><p>那这个音频码率是怎么算出来的呢？其实这里有一个公式，可以帮助我们算出音频的码率。例如我们有一个双声道立体声、采样率是48000、采样位深是16位、时长为1分钟的音频，它的存储空间占用计算应该是：</p><p>$$声道数\\times采样率\\times采样位深\\times时长 = 2\\times48000\\times16\\times60 = 92160000 b = 11520000 B = 11.52 MB$$</p><p>码率应该是 ：</p><p>$$`92160000b\\div60s=1536000bps=1536kbps=1.536Mbps`$$</p><p>音频的码率可以间接地表示音频的质量，一般高清格式的码率更高。</p><h2>音频的编解码</h2><p>我们在传输音频文件的时候经常会看到文件名后面有MP3、AAC这样的后缀，其实这些都是音频编码的格式。因为音频在传输和存储时，如果直接存储PCM音频数据的话，消耗的带宽或者存储空间会比较多，所以我们为了节省传输带宽或者存储，通常会选择对音频数据做编码压缩处理。</p><p>我们在互联网上常见的音频编码有AAC、MP3、AC-3、OPUS，个别地方还可能会使用WMA，但是从兼容性来看，AAC和OPUS更出众一些。目前AAC应用于众多音乐播放器和音乐格式封装中，OPUS常见于语音通信中。当然还有很多其他的音频编码压缩的方法，这里我就不一一列举了。在后面课程中我们会接触到更多的音频压缩技术与方法。</p><p>不知道你有没有发现，现在使用AAC编码格式的次数越来越多了，为什么大家突然都开始用AAC做音频编码了，以前很火的MP3呢？其实MP3也还在用，只不过MP3的音频编码压缩方式相对于AAC来说，性价比低了一些。对比二者的高音质，AAC HEv2无论是从码率、清晰度还是音频的还原度来说，都比MP3更优秀。详细的对比数据，我们会在后面讲FFmpeg具体操作音频编码与解码的时候介绍到，这里你稍加了解即可。</p><h2>小结</h2><p>到这里，音频基础内容就讲完了，我们了解了音频的采样格式、音频采样率、音频声道及其布局、采样位深度，最后学会了计算音频码率，这些内容在后面我们都会频繁地用到，尤其是当我们做音频编码与解码的时候，需要考虑到这些参数的兼容情况。</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/a1/6fc968efe49636c453c7e5ff83837aa1.png?wh=1920x1237\" alt=\"图片\"></p><p>而在我们做音频压缩的时候，也需要考虑自己的音频用于哪些场景，比如做音频通话的话可以考虑使用OPUS，因为基于OPUS的音频，处理语音更方便一些，例如回声消除，降噪等。如果是做音乐压缩，我们可以考虑AAC，因为AAC支持的音质与硬件兼容性更好一些。如果还要效果更好，但不太要求兼容性的话，AC-3是一个不错的选择，因为杜比之类的音频，尤其是在全景声音乐压缩的场景下，使用AC-3做音频压缩效果更好，能够听到的细节会比AAC压缩的音频更多一些。</p><h2>思考题</h2><p>好了，这就是今天的全部内容，最后我给你留一道思考题吧！当我们播放一段PCM音频的时候，声音听上去比正常声音显得更尖更细，但是速度是正常的，是什么原因呢？</p><p>希望你能开动脑筋，欢迎你在评论区留下你的答案和我讨论，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"01｜如何从色彩格式、帧率等参数角度看视频图像？","id":541546},"right":{"article_title":"03｜如何做音视频的封装与转码？","id":544986}}},{"article_id":544986,"article_title":"03｜如何做音视频的封装与转码？","article_content":"<p>你好，我是刘歧。今天我们来讲讲做视频转码时需要具备的知识。</p><p>我们平时看视频、拍视频，还有传输视频的时候，经常出现播放不了，报错的情况。这主要是因为我们拿到的文件格式有很多不同的种类，比如RMVB、AVI、WMV、MP4、FLV等，而里面的视频编码格式也有很多种，比如Mpeg-4，所以播放器既要能够解析出对应的文件格式，又要能够对文件中的音视频流进行解码，只要有一个不支持就会导致视频播放出错。这个时候我们就需要给视频做一下转码。</p><p>视频转码主要涉及编码压缩算法（Encoding）、格式封装操作(Muxing)、数据传输(例如RTMP、RTP)、格式解封装（Demuxing）、解码解压缩算法（Decoding）几方面的操作。这些操作需要一个共识的协定，所以通常音视频技术都会有固定的参考标准，如封装格式标准、编解码操作标准、传输协议标准等等。</p><p>标准中没有写明的，通常兼容性不会太好。例如FLV参考标准协议中没有定义可以存储H.265视频压缩数据，如果我们自己将H.265的视频数据存储到FLV容器中，其他播放器不一定能够很好地播放这个视频。<strong>所以在我们将视频流、音频流写入到一个封装容器中之前，需要先弄清楚这个容器是否支持我们当前的视频流、音频流数据。</strong></p><!-- [[[read_end]]] --><p>如果你对音视频编解码、视频封装、MP4格式几方面的知识比较了解的话，那么解决这些问题就会游刃有余了，所以这节课我们来学习一下这几个知识点。</p><h2>音视频编解码</h2><p>我们先来看音视频编码方面的基础知识。就像前面两节课介绍的，音频的PCM数据、视频的YUV和RGB的图像数据直接存储或者通过带宽传输会比较消耗空间，那么为了节省存储，或者确保占用的带宽更少一些，就需要把音频、视频的数据压缩一下。</p><p>音频是连续的采样序列，而视频则是连续的图像序列，这些序列是有前后关系的，为了方便理解，我们用图像来举例，看上去会更直观一些。</p><p><img src=\"https://static001.geekbang.org/resource/image/40/ef/40c3a91cfbd4be7a1e67be25b7b2cfef.png?wh=1920x571\" alt=\"图片\" title=\"图1 连续的视频图像\"></p><p>我们有一个6帧的连续视频图像，每一帧都是宽100、高100的画幅，在每一帧的正中央都有一个字母在变化。遇到这种情况时，如果我们每一帧图像全都做传输或存储操作的话，占用的带宽或空间都会很大。</p><p>为了节省空间，我们可以来分析一下图像的规律。因为这6帧图像大范围是相同的，只有正中心的一小部分内容是变化的，所以我们分析后可以得到结论，刷新第一帧图像后，从第二帧开始，我们只要刷新正中心字母区域的内容即可。这个叫局部更新，只需要逐步更新A、B、C、D、E、F的区域就可以了，这样在传输内容的时候既节省带宽，又能在存储内容的时候节省画幅的数据存储空间。</p><p>而我们在看视频的时候可不仅仅是内容的更新，还涉及内容位置的运动等变化，所以视频内容更新的算法会更复杂一些。在做视频压缩的时候，就拿前面的这个例子来说，需要有一个参考帧，这里参考的是第一帧，后面每一帧都参考前面一帧做了局部更新。而我们的视频图像序列不能只做局部更新，因为里面的目标对象还会运动，所以我们不仅可以前向做参考，还有可以做双向参考的技术。在这个过程中就涉及了图像的类型，通常我们遇到的是这三类帧：I帧、P帧和B帧。</p><p>其中I帧作为关键帧，仅由帧内预测的宏块组成。而P帧代表预测帧，通过使用已经编码的帧进行运动估计，B帧则可以参考自己前后出现的帧。如果比较IBP帧包所占空间大小的话，通常是I帧＞P帧＞B帧，所以适当地增加B帧可以减少视频流占用的带宽或者存储空间。我们可以看一下，I、P、B帧在视频解码显示时的顺序。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/22/742c4cbd514eef4826cfbc139b92fa22.jpeg?wh=514x269\" alt=\"\" title=\"图2 I帧、P帧和B帧的显示顺序\"></p><p>从这个示意图里，我们可以看出，解码顺序是1423756，但是显示顺序却是1234。当编码中存在B帧的时候，因为解码需要双向参考帧，所以需要多缓存几帧作为参考数据，从而也就带来了一定的显示延迟。所以<strong>在实时直播场景下，参考标准中推荐的做法通常是不带B帧</strong>。</p><p>在视频编码时，因为图像的画面以及图像中对象运动的复杂程度比较高，为了保证清晰度，运动的图像组中通常也会包含更多的图像运动参考信息，所以压缩难度也提升了很多，压缩后的视频码率也就变得比常规图像更高一些，这个码率的波动通常时高时低，具有可变性，我们一般称之为可变码率（VBR）。</p><p>而在有些直播场景下，因为一个传输信号通道中会携带多条流，为了确保多条流在同一个信号通道中互不干扰，一般会要求编码时采用恒定码率的方式（CBR）。但是如果采用CBR的话，画质往往会有一些损耗，这也就是为什么我们现在在一些老式的电视直播场景中看到的画质偶尔会变得很差，比如交通广播电视场景。</p><h2>视频封装</h2><p>我们平时经常会听到有人说自己的视频是MP4格式的，有些人也会说自己的音乐格式是M4A，这些都是什么意思呢？</p><p>其实我们可以粗略地认为他们说的是<strong>封装格式</strong>，也就是常说的<strong>容器格式</strong>。在容器格式的内部会存储音频、视频的数据，这些数据我们可以称之为视频流、音频流。音视频流在容器中的存储形式有两种，既可以交错式存储，也可以是不同类型的流单独存储在自己的连续区域。</p><p>我们用两张图来说明一下问题。</p><p><img src=\"https://static001.geekbang.org/resource/image/41/92/41c042b5efe8fe9d41c95e3b505f8c92.png?wh=1920x1102\" alt=\"图片\" title=\"图3 音视频流的两种存储方式\"></p><p>这两种方式都比较常见，也都各有利弊，后面我们介绍具体的封装容器格式时会做详细地说明。</p><p>音视频编码后的数据流与封装的关系如果根据上面两张图来理解可能会略微抽象，我们如果以生活中的例子来说明的话，可以这么理解音视频编码数据流与封装的关系：我们有一碗稻米，可以理解为每一粒米都是一帧视频数据，例如 H.264。我们还有一碗小米，可以理解为每一粒小米都是一个音频采样，例如 AAC。我们把稻米和小米装到一个袋子里，这一袋装有稻米和小米的整体叫做音视频封装容器格式，例如MP4、FLV。</p><p>接下来，为了方便你理解，我们以互联网中最常见的封装格式——MP4为例，来详细地讲一讲音视频封装容器格式</p><h2>封装容器格式：MP4</h2><p>在互联网常见的格式中，跨平台最好用的应该是MP4文件，因为MP4文件既可以在PC平台的Flashplayer中播放，又可以在移动平台的Android、iOS等平台中播放，而且使用系统默认的播放器就能播放，因此我们说<strong>MP4格式是最常见的多媒体文件格式，</strong>接下来我们就来详细地看一下MP4这种文件格式。</p><p>MP4格式标准为ISO-14496 Part 12、ISO-14496 Part 14，标准内容并不是特别多，如果要了解MP4的格式信息，我们首先要清楚几个概念：</p><ol>\n<li>MP4文件由许多个Box与FullBox组成；</li>\n<li>每个Box由Header和Data两部分组成；</li>\n<li>FullBox则是Box的扩展，在Box结构的基础上，在Header中增加8bit位version标志和24bit位的flags标志；</li>\n<li>Header包含了整个Box的长度的大小（Size）和类型（Type），当Size等于0时，代表这个Box是文件中的最后一个Box；当Size等于1时说明Box长度需要更多的bits位来描述，在后面会定义一个64bits位的largesize用来描述Box的长度；当Type为uuid时，说明这个Box中的数据是用户自定义扩展类型；</li>\n<li>Data为Box的实际数据，可以是纯数据，也可以是更多的子Box；</li>\n<li>当一个Box中Data是一系列的子Box时，这个Box又可以称为Container Box。</li>\n</ol><p>而MP4文件中Box的组成，你可以仔细阅读一下<a href=\"http://mp4ra.org/#/references\">参考标准ISO-14496 Part 12</a>，我们这里就不再逐字解读了。但是有几个关键的Box我们需要知道。</p><p>MP4封装格式文件中，我们经常会遇到moov box与mdat box。我们存储音频与视频数据的索引信息，就是存在moov box中，音频和视频的索引信息在moov中分别存储在不同的trak里面，trak里面会保存对应的数据采样相关的索引信息，通过获得moov中的索引信息之后，根据索引信息我们才能从mdat中读取音视频数据，所以<strong>MP4文件中必不可少的是moov信息，如果缺少moov信息的话，这个点播文件将无法被成功打开</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/23/48/234eb61c78cd775c18434083d3a53048.png?wh=2418x670\" alt=\"\"></p><p>关于MP4封装里面能否存放我们当前想存的codec，还需要查找一下参考标准或者公共约定的标准，看看是否允许存放，你可以参考<a href=\"http://MP4ra.org/#/codecs\">MP4网站</a>里面的表格。</p><p>我们刚才介绍视频封装的时候有提到过，音视频数据存储的时候有交错存储的方式也有音频存在一个区域，视频存在一个区域的方式。其实两种方式都比较常见，但是需要注意一个问题，我们在存储音视频数据的时候，如果是顺序读取音视频数据的话，当然就是音视频数据交错存储比较好，这样会给内存、硬盘或者网络节省很多开销。</p><p>如果音视频分开，单独存放在各自的区域的话，为了更好地做音视频同步，通常会读取一段视频帧数据，再读取一段音频采样数据，这样势必会不断跳跃式地读取硬盘中的数据，或者不断地发起网络请求，如果是http请求的话，我们会看到大量的http range请求，给网络开销与服务器并发造成很大的压力。</p><p>当我们基于网络请求播放MP4点播文件时，如果moov box存储在mdat box后面的话，播放器需要先读取到MP4文件的moov以后，才能够开始播放MP4文件，而这个读取的动作，有些播放器是选择先下载全部MP4文件，有些则是需要先解析一下mdat，跳过mdat以后再读取moov，所以为了节省播放器操作，兼容性更好，通常我们需要将moov移到MP4文件的头部，节省播放MP4文件开始时间段的开销。</p><p>这里我们需要注意的是，这个moov的生成，一般需要先写入mdat中的音视频数据，这样我们就知道数据采样存储的位置和大小，然后才能写入到moov中，所以是先写入mdat后写入moov这样一个顺序。<strong>解决办法是生成文件之后做一个后处理，也就是将moov移动到mdat前面。</strong></p><h2>小结</h2><p>好了，这就是今天音视频封装与转码的相关内容，最后我们来回顾一下吧！</p><p>这节课我们对音视频编码的基础操作方式，视频封装的基本原理有了一个大致的了解，对于常见的点播视频文件MP4也有了一个基础的认识，并且了解了一些常见的MP4视频点播问题。其实，我们这节课在视频点播文件中选择去介绍MP4，主要还是因为MP4是最常用的一个，你理解起来也会更容易。当然也有一些其他的视频点播格式，在之后的课程还会涉及，例如MKV、FLV、MPEGTS等。</p><p>这里，我们只要记住一个重要的信息，<strong>在音视频技术领域，编解码、封装格式都有对应的参考标准，并且都是开放标准</strong>，我们要善于借助网上搜到的参考标准文档和详细的解析资料，如果你希望对音视频技术有更深的了解，除了学习我们这节课的基础知识外，还是需要自己动手实操，照着参考标准写一下对应的实现代码。</p><h2>思考题</h2><p>我们这节课讲解了MP4文件可以作为视频点播文件，那么MP4是否能够应用于直播场景中呢？怎么处理才能够支持直播？欢迎你在评论区留言与我交流，也欢迎你把这节课分享给身边需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"02｜音频从采集到输出涉及哪些关键参数？","id":544004},"right":{"article_title":"04｜直播行业的发展概况与技术迭代","id":545945}}},{"article_id":545945,"article_title":"04｜直播行业的发展概况与技术迭代","article_content":"<p>你好，我是刘歧。</p><p>上一节课我们了解了音视频编解码和封装基础，还讲解了MP4的容器格式。我们都知道MP4常见于视频点播场景，那MP4格式能用于直播场景吗？</p><p>其实稍微改一下容器格式是可以的，例如我们在做直播播放的时候常见的是fragment mp4格式，或者cmaf格式。如果我们想要观看HLS标准的直播，并且想要看比H.264码率更低的HEVC视频编码的直播，就需要在HLS子容器中支持fragment mp4格式。所以MP4的另一种封装形式是可以做直播的。</p><p>那么除了fragment mp4之外，我们还有哪些格式和协议可以做直播？这节课我们来了解一下直播在过去这十年之间发展的经过以及基本信息，以便我们对自己即将进入的直播场景有一个基本的了解。</p><h2>行业的演变</h2><p>2010年之前，直播技术的应用主要还集中在广播电视、广电IPTV、安防视频监控、视频会议以及个人媒体中心等领域，使用的传输协议主要为MMS、RTSP、DVB-C、DVB-T、DVB-S等。除了这些领域与协议，偶尔还会用RTMP与HTTP+FLV的方式做直播服务应用，例如一些广电领域的客户，还有像优酷这样的平台会在重大活动的直播中使用这种方式。当时RTMP与HTTP+FLV直播并未像后来这么火热，FMS、Wowza等流媒体服务器当时还是主流。</p><!-- [[[read_end]]] --><p>2010年之后，随着Adobe开放了RTMP协议，像CRtmpServer、libRTMP、Red5等支持RTMP协议的开源框架如雨后春笋般大量涌出，但是这些流媒体服务器还需要再做一部分开发才能真正建立起服务端的能力。2012年夏天，NginxRTMP的出现打破了这场僵局，它通过大幅降低RTMP服务器的建设门槛，将RTMP协议的直播发展带入快车道。</p><p>紧接着，2013年秋天，SimpleRtmpServer（SRS）的出现为RTMP服务器的建设带来了革命性的改变，SRS发展至今已经发布了4.0版本，它采用了比较前卫的协程方式（用StateThread做基础库）做任务控制，也可以理解为单进程单线程的工作方式，不需要考虑信号量和锁操作相关的问题，而如果需要提升更高的并发时，需要扩展多进程的方式，就可以借助第三方的能力来处理，例如Docker，自己启动多进程来控制多任务。</p><p>SRS原生支持的功能比较全，例如RTMP转HTTP+FLV、转HLS、转WebRTC等常用功能，运维操作方面支持Reload，项目文档健全。在互联网娱乐直播领域，由于RTMP可以保证从画面采集端至播放器端的整体延迟在3s左右，所以这个协议被广泛应用。</p><p>与此同时，支持动态多码率的解决方案HLS（HTTP Live Streaming ）与DASH（Dynamic Adaptive Streaming over HTTP）目前还在以草案的方式向前迭代，并未发布正式的参考标准。FFmpeg、Gstreamer等工具软件，也在跟随草案的更新进行功能迭代。当时，HLS主要用于移动端与电视盒中，DASH也开始逐步被一些服务商采用，比如YouTube、Vimeo、Akamai等。</p><p>同期动态多码率的解决方案除了HLS与DASH外，还有Adobe的HDS（HTTP Dynamic Streaming）。由于HDS的参考标准并不像HLS和DASH那样，由RFC或者ITU两个开放标准定制、组织、维护，而是Adobe自己在维护，所以推动起来并不是很顺畅，应用的客户也并不多。</p><p></p><p>从2013年开始，基于RTMP的直播平台开始逐渐发迹，呈现形式也主要是互联网的Web浏览器。基于RTMP协议的直播成为主流选择的原因如下：</p><ol>\n<li>Flash Player能够直接播放基于RTMP协议传输的音视频数据；</li>\n<li>Web浏览器已经默认内置Flash Player插件，不需要额外安装插件；</li>\n<li>RTMP推流、播放有很多开源实现，能够快速搭建起端到端的解决方案。</li>\n</ol><p>技术的成熟使PC端涌现出大量基于Web浏览器的视频秀场类直播、游戏类直播以及部分户外直播。与此同时，移动端直播App也开始零零散散地出现。</p><p>虽然安防监控、视频会议、广播电视与互联网行业也在使用直播技术，但是其应用领域、方式、场景差异很大。此时安防监控也在逐步接入互联网，例如通过家用的监控摄像头，用户可以用手机观看家里的实时画面。视频会议厂商在2013年还在加紧兼容WebRTC框架以提升其通用性，H.323/SIP+RTP的使用方案也还比较普遍，因此当时做RTC的服务商与平台并不多。</p><p>2015年，一个名为“17”的移动端App毫无征兆地成为了当时的爆款，但又迅速被App Store下架。自此事件开始，直播领域拉开了千播大战的序幕。各色各样的直播App开始被大力推广，其底层的技术原理大同小异，推流协议也仍然是RTMP，拉流协议均以RTMP或HTTP+FLV为主，各家App主要的差别在于起播时间，视频清晰度以及直播卡顿率。</p><p>伴随着千播大战的开场，基于PC端Web浏览器直播平台的用户流量开始逐渐被移动端直播App所蚕食，直播平台的形态也渐渐开始形成分化。此时的直播应用主要集中在以下四大领域：</p><ol>\n<li>广电单向发布直播流，如IPTV、电视盒、常用HLS、DASH、HDS这类端到端且画面延迟10s左右的直播流媒体协议；</li>\n<li>安防监控采集直播流，如海康、大华、水滴摄像头等，常用H.323/SIP+RTP等延迟50ms～500ms左右的协议；</li>\n<li>互联网/移动互联网互娱直播流，如秀场直播、游戏电竞直播、户外活动直播、街拍直播等，常用RTMP、HTTP+FLV等延迟3s左右的直播协议；</li>\n<li>视频会议直播流，如Polycom、Cisco等，常用H.323/SIP+RTP等协议。</li>\n</ol><p>我们可以看一下这四个领域的主要特点：</p><p><img src=\"https://static001.geekbang.org/resource/image/1b/9b/1b58ae0768bec867a2a6b76bdc93e79b.png?wh=1920x961\" alt=\"图片\"></p><p>这四个领域是当时直播应用的主要场景。但随着时间的推进，直播场景也变得越来越复杂了。</p><p>2016年，大量互娱直播平台逐渐增加了视频会议功能，也就是常说的“连麦”功能，使用场景也变得更复杂。实际上连麦采用的技术与视频会议几乎相同，但要兼顾互娱直播原有的场景，也就是一个直播间里，用户通过连麦进行实时交流，其他观众依然可以以原方式观看视频，例如歌曲合唱、实时采访、表演节目PK等。主播间也可以通过这种方式导入粉丝进行创收。当然，创新并没有止步于此，移动直播平台数量激增导致的“千播大战”让2016年被称为“直播元年”。</p><p></p><p>2017，大量的互娱直播平台又创新地增加了主播与观众互动的能力，例如冲顶大会、在线答题等功能。另一方面，直播创业的热潮开始消退，主要是由于用户流量再次向巨头靠拢，头部直播平台马太效应开始显现，随着流量的增加巨头也不断进行技术迭代和升级，进一步提升了用户的直播观看体验，从而加深了自己的护城河。</p><p></p><p>2018，PC互联网和移动互联网直播开始逐渐从单纯的秀场、游戏、户外直播演变出直播带货场景。而当年的互联网直播巨头快手发布了实时交互性更强的直播PK功能，直播的形态从单纯的观看转为互动，又从互动转为挖掘用户需求。</p><p>从2019年至今，尤其是在疫情期间直播电商的市场规模达到了近万亿，在巨大的音视频互联网体量下，直播电商市场规模还能达到100%以上的增长，说明<strong>音视频已经成为了互联网行业非常重要的一项基础设施。</strong></p><p>快手平台2019年至2020年举办了多次大型活动，从2019年的阅兵到跨年的春晚，在线数据一直在突破。快手春晚转播，最高在线人数创纪录，达到了2000多万人；2020年董明珠在快手直播卖货，一天卖了3.1个亿；周杰伦疫情期间从台湾连线，跨海峡两岸进行直播首秀，为快手用户提供了一个全新的云端演唱会的体验，直播观看人数约6800万，在一千零一夜活动中黄渤与周杰伦隔空唱奏“Mojito”，更是将直播同唱一首歌类型的实时直播技术做到了极致，打破了常规互联网直播的形态。</p><p>这就是从2008年起PC互联网/移动互联网环境直播的发展概况。</p><p><img src=\"https://static001.geekbang.org/resource/image/84/47/84f7a06714deyydd269fc33257ba8f47.png?wh=1920x1148\" alt=\"图片\"></p><h2>技术迭代</h2><p>通过了解直播行业的发展过程，我们发现，现在的直播应用和场景已经远超于十年前的范畴了，直播技术也一步步进化成如今完整的体系。</p><p>发展过程中有关技术部分的迭代包含了5个维度：音视频编码的迭代、音视频传输协议的迭代、音视频封装的迭代、音视频传输质量优化策略的迭代以及平台功能的迭代。接下来我们就从这五个维度出发系统地阐述直播技术的发展和演进过程。</p><h3>音视频编码的迭代</h3><p>自2008FLV正式支持H.264开始，H.264编码逐步被广泛应用，同期VP6、VC-1、H.263、mpeg4video、mpeg2video等编码方式一样比较火热，但只有H.264编码因为码率较低、图像质量高、容错和网络适应性更强，后续在国内脱颖而出。</p><p>2013年，PPLive和迅雷率先宣布采用H.265（即HEVC）进行视频压缩，在实现同等清晰度的前提下视频码率比H.264更低，这引起了直播行业的普遍关注。随后各大平台开始相继引入HEVC编码的视频直播流，但因为HEVC专利池问题的复杂性，又出现了一个比较诡异的情况：只有Safari浏览器支持HEVC的解码，而Chrome等浏览器不支持，这就导致时至今日依然有大量直播平台还在使用H.264的视频编码。</p><p>除了HEVC在发展，谷歌同期还推出并迭代了VP8、VP9视频编码，主要应用于YouTube等海外视频平台，在国内并未被大规模采用。在谷歌的WebRTC被大规模推广后，因为WebRTC最初对H.264的支持不太友好，VP9才勉强得到了一些发展。</p><p>2018年之后，谷歌联合一众巨头共同组建了开放媒体联盟（Alliance for Open Media）并推出了AV1视频编解码标准，同样在WebRTC中被大范围应用。</p><p>直到今天，<strong>视频编码标准中应用最广泛的依然是H.264、HEVC以及AV1</strong>，下一代视频编解码标准VVC已经正式发布，而AV2也在紧锣密鼓地制订中，业界很多企业也在用AI技术进行视频编解码方面的相关研究。相信在不久的将来，会出现更多优秀的视频编码标准。</p><p></p><p>相对于视频编码，音频编码标准从十年前单向直播领域的MP3发展为现在大范围应用的复杂音乐内容压缩编码标准的AAC。会议直播从G.711、G.729音频编码标准发展到更优秀的语音通话实时编码与处理标准Opus，再到谷歌刚刚发布的Lyra，音频编码也随着时代的发展与应用场景的演变在不断迭代。</p><p><img src=\"https://static001.geekbang.org/resource/image/1f/yy/1f5ef3bb5b01cd9faf3f34a9fdc26eyy.png?wh=1920x623\" alt=\"图片\"></p><p>近些年来编解码人才逐年递增，数字信号处理专家也逐渐成长起来，更多人参与到了音视频标准的制订中，打破了原来大一统的局面，视频编解码标准的发展随之进入了快车道，加上参与的公司与相关的组织逐年增加，今后用户和不同业务场景可以选择的音视频编码方案会更加丰富。</p><h3>音视频传输协议的迭代</h3><p>音视频编码的迭代给我们提供了更多的选择，能够在保证清晰度的前提下占用更少的空间。而音视频传输协议的迭代带给我们的则是更低的延时。</p><h4>低延时直播场景（延迟3s左右）</h4><p>2010年前，直播多采用RTSP传输协议，用户想要在浏览器中观看视频需要额外安装插件，所以多数用户选择在客户端观看直播。</p><p>当Adobe的RTMP传输协议开放后，涌现出很多基于RTMP协议的框架，例如librtmp、crtmpd、nginx-rtmp、simple-realtime-server等，加上OBS、FFmpeg等客户端开源套件的出现，在PC Web浏览器中可以直接使用默认安装的Flash Player插件来播放RTMP协议的直播内容，因而基于RTMP的应用开始广泛了起来，直播延迟也缩短至可以满足主播与观众的交流需求。</p><p>随着用户数量的增多，他们对直播回看的需求也随之增加，同时对首屏指标的要求也提高了，平台逐渐将播放协议从RTMP协议转变为HTTP协议，在建立连接时会节省很多Handshake相关操作，缩短首画显示的时间。</p><p>直播延迟的影响因素有很多，如果只考虑合适的传输协议，通常只能勉强达到秒开的效果，但速度依然较慢，加之个别地区会出现域名劫持等现象，低延迟直播场景的需求仍然无法被满足。于是在域名解析方面，业界又做了一些优化，主要分为三种：DNS、HTTPDNS、私有化协议预解析域名，可以预先将用户调度至直播所属的服务节点，无须在用户发起请求时再进行域名解析，从而节省了域名解析时间，进一步降低直播延迟。</p><p>整体来看，国内直播目前还是以RTMP、HTTP传输协议为主，为了降低延迟，主要采用DNS协议调度，并额外增加了HTTPDNS与私有化的协议解析域名进行调度，从而解决运营商的内容和流量劫持问题。之所以<strong>RTMP和HTTP会成为主流选择，主要还是因为传输协议比较通用，CDN厂商支持比较健全</strong>。除RTMP与HTTP外，还有一些介于封装和传输协议之间较为模糊的标准，如HLS和DASH。之后我们会具体介绍。</p><h4>视频会议场景（延迟50ms～500ms）</h4><p>在谷歌的WebRTC大规模应用之前，很多直播会议采用的是类似RTSP+RTCP+RTP或者H.323/SIP+RTP的方案，视频采集与播放均需要客户端开发解决，无法直接在Web浏览器中使用，因而门槛较高。</p><p>在WebRTC大规模应用后，应用视频会议技术的直播平台逐渐增多，只不过这些平台中大部分并不把它叫做直播会议，而是体现为连麦、PK等互动直播形式。</p><p>在互动直播出现前，大多数直播以秀场、竞技、体育等场景为主，主播与观众的交流方式主要是留言或者发送弹幕。而在互动直播出现后，诞生了在线抓娃娃、主播与观众连麦交流等全新的互动方式，后来火爆全球的Clubhouse也采用了同样的技术。</p><h3>视频封装的迭代</h3><p>201­­0年之前，直播流传输的通常是音视频的裸流，或者是常用的mpegts直播流。在Adobe开放RTMP后，FLV随即成为了主流的直播流封装格式，该格式常见于秀场直播、游戏竞技类直播，在移动端直播也比较常见。</p><p>在支持HEVC方面，国内由熊猫TV在2017年开始发起并联合多家公司共同定制了支持HEVC的FLV视频标准。当然，由于目前Adobe并未对FLV参考标准作出任何更新，所以现在来看熊猫TV发起的这个标准在实际应用中不会出现任何问题。但如果Adobe突然有一天开始继续维护FLV并更新了HEVC的支持，将出现比较大的风险，市面上大多数支持HEVC的FLV封装内容将无法播放。</p><p>在PC 直播平台与移动端直播出现的早期，HLS与DASH是同时存在的。HLS与DASH采用的是mpegts与fragment mp4的子封装格式，虽然HLS与DASH也支持动态多码率切换，但是延迟较高。近期推出的LLDASH与LLHLS都改善了延迟的状况，但由于依然需要在关键帧处切片，高延时问题并没有得到很好地解决，反而还会带来额外的协议方面的开销。</p><p>时至今日，FLV封装格式在国内依然被大范围应用，随着Flash Player播放器在浏览器中停止更新，PC 直播平台也开始逐渐转向HLS与DASH。但是<strong>FLV格式因为可以自行开发移动客户端的特性，在移动端仍有很大的用武之地。</strong></p><p></p><p>两年前，快手自研了一套动态多码率自适应切换清晰度的技术LAS（Live Adaptive Streaming），并于2020年开源。开源前该技术已经在快手大规模平稳运行多年，解决了HLS的高延迟问题，同时也解决了RTMP和HTTP+FLV的直播过程因网络质量变化，不能自动动态切换清晰度，导致直播卡顿率高等问题。</p><p></p><p>视频封装的迭代还在不断演进，而且愈加丰富。单从FFmpeg的Formats格式支持来看，十年间发展了不计其数的封装格式。由于环境差异，国内外侧重使用的封装技术略有差别，国内还是以FLV为主，其次是HLS/DASH及其衍生的封装格式，还有已经形成实际使用标准且有数亿用户的LAS。随着时间的发展，相信视频封装格式的演变将会越来越倾向于特定的业务与场景，从而可以针对不同场景使用不同的封装格式。</p><p><img src=\"https://static001.geekbang.org/resource/image/63/81/6381b0b4952cff93c989d198770e1581.png?wh=1920x1172\" alt=\"图片\"></p><h3>音视频传输质量优化策略的迭代</h3><p>在移动网络处于2.5G向3G升级的年代，移动端直播主要还是以HLS、RTSP传输为主，由于当时的网络基础建设并不太适合高清视频传输，再加上视频采集与编码设备能力较弱，所以直播清晰度普遍较低，且直播链路会出现卡顿。在之后的4到5年，也就是3G向4G升级的时代，主要是以自研TCP优化或者采用AppEx的TCP优化，来保证视频内容传输质量。</p><p>2016年，谷歌公布BBR网络传输优化算法并提供源代码以后，做TCP传输优化的专家越来越多，大量更优的网络传输质量优化算法随之出现。</p><p>还有很多直播平台基于UDP在不断地做一些直播传输质量优化的尝试，例如使用KCP替代TCP，再比如快手发布的KTP传输优化，都可以替代TCP传输来提升音视频传输质量。再后来，还有很多平台尝试用QUIC替代TCP传输，来提升音视频传输质量，降低音视频端到端的延迟。</p><p>对于主播推流端遇到弱网或网络上行质量较差的情况，各个平台也加大投入，来保障弱网服务质量，例如在直播推流时做动态切换帧率、动态切换码率、降低分辨率等操作，确保主播端推流不会卡顿，但是这种做法又提升了服务端处理录制兼容、转码兼容等问题的难度，这些问题都在努力解决中。</p><p>为了降低直播卡顿率，各直播平台还在不断地优化传输协议，从AppEx到BBR、KCP，再到QUIC，整个行业投入了大量的精力对网络进行针对性地优化，专门处理丢包、拥塞、抖动、慢启动等常见的网络问题，也有越来越多的链路传输优化专家开始着手进行更多的探索与深入研究。</p><h3>平台功能的迭代</h3><p>在互联网直播平台刚刚出现的时候，平台方主要关注音视频直播技术的功能性完善，随着音视频技术人员的增多以及直播平台的功能趋于同质化，大家比拼的点从功能完善性逐渐转变成了功能创新，例如设计冲顶大会、在线抓娃娃等新的直播场景。</p><p>创新很容易被复制，在创意逐渐干涸后，比拼的关键点从音视频流媒体技术本身转变为音视频平台服务质量。这对直播平台提出了更高的要求和挑战，因为提升QoS与QoE需要的不单单是音视频技术，还包括大数据、弹性计算、深度学习等。通过大数据可以及时发现用户体验的好坏，通过弹性计算可以实时调整相关服务，而通过深度学习则可以节省运维、运营成本。这些都需要大量的用户积累，长时间的技术投入以及深厚的技术功底，确保用户体验、降低卡顿率、提升清晰度、节省运维成本等工作也被纳入到直播技术中。</p><p></p><p>时至今日，要想提升服务质量，已经不单单需要功能的创新了。<strong>更重要的是要有大量的数据作为服务的支撑，以数据来驱动业务发展。</strong>如果没有健康的数据展现，我们所做的技术投入都有可能是徒劳，我们将无法准确评估QoE、QoS都是否得到了有效提升，也无法判断研发完成并投入使用的功能是否被大多数用户所喜爱。</p><h2>小结</h2><p>这节课，我们通过过去十多年直播的发展了解了直播技术的演变，从直播传输协议，音视频编解码标准、音视频封装容器格式、适用场景与直播内容延迟等属性，最后到直播平台的质量考量等方面的介绍。相信你对直播技术已经有了一个整体的认识。其实每个阶段音视频技术的发展，都会有一个公共的参考标准，而过去的几十年中，直播相关的参考标准一直在不断迭代和演进，我们可以通过RFC、ITU、MPEG等工作组中找到这些标准。</p><p>到这里，我们对音视频基础部分已经有了一个基本的认识。从下一节课开始，我们将重点讲解实际操作部分的内容了。</p><h2>思考题</h2><p>学完这节课的知识，我们一起来思考一个问题吧！如果我想自己搭建一个课程直播，自己推流，课堂里有一千个学生，这样的场景我该选用哪种技术呢？希望能够在评论区看到你思考后的成果，也希望你能把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"03｜如何做音视频的封装与转码？","id":544986},"right":{"article_title":"05｜如何使用 FFmpeg 与 OBS 进行直播推流？","id":546485}}},{"article_id":546485,"article_title":"05｜如何使用 FFmpeg 与 OBS 进行直播推流？","article_content":"<p>你好，我是刘歧。</p><p>前面四节课，我们介绍了音视频与直播相关的基础知识，那么接下来我们就要进入实战阶段了。学完这个部分，音视频处理的常用工具怎么用，你就能心中有数了。</p><p>前面我们虽然了解了什么是直播，直播服务器可以用到哪些开源项目。但直播推流到底怎么实现并没有详细展开，所以这节课我们重点讲讲怎么基于FFmpeg推直播流。如果你的业务场景用FFmpeg不太方便，我还提供了另一个方法——桌面工具OBS推流。相信学完之后，你就能轻松搞定推流。</p><p>首先，我们做直播推流的前提是要有直播服务器接收直播流，所以需要我们自己建设一个流媒体服务器。我们可以根据上一节课提到的开源直播服务器的<a href=\"https://github.com/ossrs/srs/wiki/v5_CN_Home\">官方文档</a>部署直播服务器，也可以挖掘自己当前使用的直播服务平台的服务器接收直播流。为了方便演示，我使用快手的直播云服务来接收我推的直播流。界面如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/d4/0f/d478c987355cff9dc4f1a7fc849b930f.png?wh=2634x1616\" alt=\"图片\"></p><h2>FFmpeg推流</h2><p>通常，推流服务器的管理界面会提供一个收流的RTMP服务器地址，还会提供一个直播流的流名称，也叫串流密钥。例如推流的RTMP服务器地址是rtmp://publish.x.com/live，串流密钥是stream，那么最后组成的推流地址就是rtmp://publish.x.com/live/stream。</p><!-- [[[read_end]]] --><p>如果使用FFmpeg推RTMP流的话，我们需要使用的输出格式为FLV，那么FFmpeg的输入就是-f flv rtmp://publish.x.com/live/stream。为了便于理解，我们自己模拟一个直播画面，然后推RTMP直播流，下面是完整的命令行。</p><pre><code class=\"language-plain\">ffmpeg -re -f lavfi -i testsrc=s=1280x720:r=25 -pix_fmt yuv420p -vcodec libx264 -f flv rtmp://publish.x.com/live/stream\n</code></pre><p>这条命令行理解起来比较简单，FFmpeg分两大部分，一部分是输入部分，也就是-i与-i的参数以及它之前的部分，另一部分就是-i与-i参数后面的部分，为输出部分。仔细划分一下，这条命令行的输入部分是-re -f lavfi -i testsrc=s=1280x720:r=25，输出部分是-pix_fmt yuv420p -vcodec libx264 -f flv rtmp://publish.x.com/live/stream。</p><h3>命令行参数</h3><p>输入部分的意思是使用FFmpeg的lavfi输入格式，也可以说输入的是lavfi设备。输入内容是testsrc，这个testsrc是输入lavfi格式的内容，lavfi的格式有很多内容，这些内容不是既有的内容，也不是某个文件，而是FFmpeg通过filter自己创建出来的。除了testsrc，还可以创建testsrc2、color、yuvall等图像内容（更多可创建的内容你可以查看<a href=\"https://ffmpeg.org/ffmpeg-filters.html#Video-Sources\">FFmpeg官方文档</a>）。输入的图像是25fps，也就是每秒钟会得到25帧图像，图像的宽是1280像素，高是720像素。</p><p>如果你足够细心的话，就会发现我还没有讲-re这个参数。它其实是控制获得图像频率的参数，用来控制输入包的读取速度，比如我们规定一秒钟输入25帧，它会把速度控制在25帧。因为在FFmpeg中数据是以最快的速度读完的，一般在高配的机器上读取速度会非常快。我们用FFmpeg自主生成的数据来模拟直播，就需要用-re来控制一下速度。但如果我们是读取摄像头还有RTSP等直播协议输入的数据，就可以不控制，因为对方输出也是控制频率的。</p><p>输出部分的意思是先把读取的图像像素点的颜色格式转成yuv420p格式，关于yuv420p格式，我们在前面的课程中是讲过的，这里为什么使用yuv420p呢？因为yuv420p在视频图像格式中是兼容性最好的，使用起来会比较稳定。</p><p>接下来编码器部分使用的视频编码器为libx264。libx264是一个第三方编码器，这里我们需要注意的是libx264的FFmpeg需要使用自由软件基金会的通用公共协议的License，也就是常说的GPL协议。作为开源软件发行版使用问题不大，如果商用的话可能需要考虑法律风险。</p><p>我们继续看输出部分，-f flv规定我们输出的封装格式为FLV，用-f指定封装格式以后，输出文件的文件名其实也不会有作用，因为FFmpeg会强制输出-f指定的输出格式。最后输出的文件是一个RTMP协议特征字符开头的URL，所以最终会将FLV格式的内容输出到FFmpeg RTMP协议内容中。</p><p>到这里，使用FFmpeg推流到RTMP服务器就可以执行了。我们点开快手，到我们自己的直播间看一下效果，从图中可以看到直播已经开始了，testsrc的画面已经出来了。</p><p><img src=\"https://static001.geekbang.org/resource/image/ee/42/ee0c74eda1fd5c8846ed041166b1ee42.png?wh=4449x3057\" alt=\"图片\"></p><p>有些人觉得安装FFmpeg太麻烦了，并且使用命令行也比较繁琐，记不住参数。刚入门的时候使用FFmpeg可能确实有些麻烦，会觉得参数太多。其实用着用着找到窍门后，你就会发现<strong>FFmpeg参数并不多，尤其是我们常用的能力部分，模块化做得很好</strong>，不同的编码器都有自己的参数可以配置，如果我们集中精力只关注我们自己使用的模块部分，参数并不多也很容易记住。</p><p>当然有些人可能想要采集自己的摄像头、桌面等外设，这些通过FFmpeg的-devices参数可以得到相关的设备信息。通过<a href=\"https://ffmpeg.org/ffmpeg-devices.html\">FFmpeg的设备相关的操作文档</a>指引信息，我们也可以自己用FFmpeg获得摄像头、桌面等外设图像，通过编码推流到直播服务器上。支持的外设比较多，这里就不展开说了，具体内容你可以参考文档自己挖掘一下。</p><h2>带界面的推流神器OBS</h2><p>如果你确实觉得FFmpeg太难、太麻烦，还有一种方式也能帮助你完成推流。那就是<strong>使用界面直播推流神器OBS，这种工具不用命令行，也可以帮你轻松搞定推流</strong>。</p><p>OBS是个桌面应用程序，首先我们需要从OBS官方网站下载OBS并安装上，安装后打开的界面比较直观。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/b7/df5ca1ced76500a1yy4aef10ce768fb7.png?wh=1079x751\" alt=\"图片\"></p><p>OBS的功能非常强大，这里我们主要介绍抓取本地窗口的场景。首先我们基于场景添加一个来源，来源可以选择窗口采集。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/9a/024b2354a6f205283573140c6f10919a.png?wh=1432x874\" alt=\"图片\"></p><p>然后选择一个窗口，比如我正在本地用播放器播放一个MP4的视频，我可以通过OBS抓取这个播放MP4视频的播放器窗口，我们预计将这个播放器正在播放的内容以直播的方式推流到RTMP服务器上。</p><p><img src=\"https://static001.geekbang.org/resource/image/5b/2d/5b65e7fc7cf66f3aa0e9767bf346102d.png?wh=819x912\" alt=\"图片\"></p><p>然后我们设置一下推流和编码器，点设置按钮。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/c1/e071438ce009c8a90d44e198b0c95dc1.png?wh=1077x749\" alt=\"图片\"></p><p>进入设置窗口后选推流选项，填入RTMP服务器地址，推流密钥。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/5d/26d5a553231fca1a849930b788b0a05d.png?wh=981x748\" alt=\"图片\"></p><p>为了让推流更流畅，我们还需要进入视频选项里面，设置一下视频的编码参数。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/ea/e2850d4ae8c2aa03ef309510d8f4deea.png?wh=981x748\" alt=\"图片\"></p><p>主要是分辨率和帧率，需要我们稍微调整一下，比如我的播放器播放的视频是720p，那我就可以选择720p对应的分辨率，关于分辨率我们在<a href=\"https://time.geekbang.org/column/article/543605\">第一节课</a>已经介绍过了，你可以一边回顾一边操作。</p><p>接下来我们设置一下音频编码参数。</p><p><img src=\"https://static001.geekbang.org/resource/image/c7/8b/c792edf5536cd244fyycb9146fd4b88b.png?wh=981x748\" alt=\"图片\"></p><p>音频编码我们可以根据自己的需要进行设置，比如是否采集桌面声音，是否采集麦克风声音等，我们这里选择默认的麦克风输入声音即可。</p><p>当然，如果我们推流的PC机有GPU卡，或者我们想直接用CPU软编码的话，可以进入高级设置项设置详细参数。</p><p><img src=\"https://static001.geekbang.org/resource/image/39/a1/391c7e0de8e59813fcf3b0079c197ea1.png?wh=977x744\" alt=\"图片\"></p><p>对应的可以设置的参数还是挺多的，比如我这里默认用的是x264编码器，那么可以设置码率是否为CBR，CBR主要是恒定码率，也就是在画面和视频图像运动场景比较复杂的时候，画质会显得模糊一些，如果用VBR的动态码率的话，在画质方面会根据图像运动场景等因素存储更多的参考帧相关的信息，码率会动态地拉升。</p><p>之后就需要设置关键帧间隔，这里我设置的1其实不太合理，一般我们设置2到5秒钟比较合理。关键帧间隔设置得合理的话，可以确保直播延迟低的同时画质更高，这个部分我们完全可以根据自己的考核标准操作。再往下看就是H.264编码标准里面的设置，例如快速编码，编码的profile设置，是否用低延迟模式等。</p><p>如果这些参数设置得不尽兴的话，了解了x264的更多详细参数后，你也可以自己选填更细节的参数。当然，如果想用GPU编码的话，也可以自由选择，比如我是苹果电脑，那么我可以选择使用videotoolbox进行视频编码，这样也可以节省一些我的CPU资源，电脑的风扇也会相对安静一些，编码性能大多数情况下会比CPU编码要高很多。</p><p><img src=\"https://static001.geekbang.org/resource/image/8f/00/8f7ba8247fd55bac74c86f98668a4200.png?wh=1076x747\" alt=\"图片\"></p><p>推流之后，我们就能在直播间里看到自己的直播流内容了。</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/3a/b3571d654785b39b74f1c7d9baa9c93a.png?wh=4421x3219\" alt=\"图片\"></p><h2>小结</h2><p>到这里，我们可以在零基础的情况下做直播推流，推一些我们自己的内容到直播服务器上面了。无论是使用FFmpeg还是OBS，都能够很好地作为推流客户端进行推流，使用FFmpeg推流的话一条命令行就可以搞定。如果你不喜欢用鼠标点来点去的话，推荐你使用FFmpeg推流；而用OBS的话，不需要下载安装FFmpeg，不需要点开终端自己输入各种参数，也不用特意去记这些参数就能轻松搞定推流，深度挖掘的话，甚至还可以实现导播等功能。</p><p>更多OBS的高级功能和黑科技，还是需要我们自己耐心地去一点点挖掘。在这里我通过简单的介绍引领你入门，目的是<strong>让你具备基础的直播推流能力</strong>。但师父领进门，修行在个人，还是需要你自己动手实际操作起来，相信你会有不小的收获。</p><h2>思考题</h2><p>通过刚刚的操作，我们推流成功了，而且也能在播放器端看到推流成功后的直播流了，那么我们怎么确认这个直播流是不是我们设置好的1280x720的分辨率，帧率是不是25fps呢？期待在评论区看到你的答案，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"04｜直播行业的发展概况与技术迭代","id":545945},"right":{"article_title":"06｜如何使用 ffprobe 分析音视频参数与内容？","id":547562}}},{"article_id":547562,"article_title":"06｜如何使用 ffprobe 分析音视频参数与内容？","article_content":"<p>你好，我是刘歧。</p><p><a href=\"https://time.geekbang.org/column/article/546485\">上一节课</a>我们学习了如何使用FFmpeg和OBS做直播推流，当时我给你留了一个思考题，怎么确认播放的流是我们自己推的那个直播流呢？想回答这个问题，就需要用到我们今天要学习的这个视频信息分析神器——ffprobe。它是FFmpeg提供的一个工具，能够用来分析音视频容器格式、音视频流信息、音视频包以及音视频帧等信息，在我们做音视频转码、故障分析时，这个工具能提供很大的帮助。</p><p>下面，我们就来看看怎么使用ffprobe来分析音视频相关信息。</p><h2>音视频容器格式分析</h2><p>当我们拿到一个视频文件、一个视频直播URL链接时，通常的操作是播放，或者是分析其音视频容器格式的信息。播放操作比较简单，这里就不说了，如果想要分析音视频容器格式的信息，我们应该怎么做呢？</p><p>其实使用<strong>ffprobe的-show_format</strong>参数就能得到对应的信息了，我们看一下输出的内容。</p><pre><code class=\"language-plain\">[FORMAT]\nfilename=/Users/liuqi/Movies/Test/ToS-4k-1920.mov\nnb_streams=2\nnb_programs=0\nformat_name=mov,mp4,m4a,3gp,3g2,mj2\nformat_long_name=QuickTime / MOV\nstart_time=0.000000\nduration=734.167000\nsize=738876331\nbit_rate=8051316\nprobe_score=100\nTAG:major_brand=qt\nTAG:minor_version=512\nTAG:compatible_brands=qt\nTAG:encoder=Lavf54.29.104\n[/FORMAT]\n</code></pre><!-- [[[read_end]]] --><p>我们来逐一地解读一下里面蕴含的信息。</p><p>从输出信息第二行的后缀可以看到，这是一个mov文件。nb_streams显示的是2，也就是说这个容器格式里有两个流；nb_programs是0，表示这里面不存在program信息，这个program信息常见于广电用的mpegts流里，比如某个卫视频道的节目；start_time是这个容器里正常的显示开始的时间0.00000；duration是这个容器文件的总时长734.167秒；接着就是size表示这个mov文件大小；bit_rate是这个文件的码率。</p><p>然后下面就是probe查找容器格式的得分，FFmpeg在probe这个文件中对应的得分是100，这个得分通常用来确定使用哪个容器模块来解析这个probe文件。最后就是对应容器的TAG。</p><p>其实分析容器格式的话，得到的信息并不是特别全，但是容器格式里面的这些信息是具有一定的参考价值的，如果想要获得容器格式里一些更详细的信息，还需要对容器格式里面的音视频流进行分析。</p><h2>音视频流分析</h2><p>音视频流的信息我们可以使用<strong>ffprobe的-show_streams</strong>获取到，我们看一下通过show_streams能得到哪些内容呢？我们以视频文件中的视频流为例来分析一下，如果你对音频流有兴趣，也可以按照我们分析视频流的方式分析一下音频流。</p><pre><code class=\"language-plain\">[STREAM]\nindex=0 //流的索引号\ncodec_name=h264 //流的编码名\ncodec_long_name=H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 //流的编码详细描述\nprofile=High //流的profile\ncodec_type=video //流的codec类型\ncodec_tag_string=avc1 // 流的codec tag 字符串\ncodec_tag=0x31637661 // 流的codec tag，也是字符串，只不过以16进制方式存储\nwidth=1920 //视频的宽，流内容部分\nheight=800 //视频的高，流内容部分\ncoded_width=1920 // 编码视频的宽，编解码时的部分，对齐的数据，显示时不用\ncoded_height=800 // 编码视频的高，编解码时的部分，对齐的数据，显示时不用\nhas_b_frames=2 // IPB 帧排列时两个P之间包含两个B\nsample_aspect_ratio=1:1 //像素点采样比例\ndisplay_aspect_ratio=12:5 // 显示图像比例\npix_fmt=yuv420p // 像素点格式\nlevel=40 // 与profile一起出现，对应的是参考标准中有对应的参数描述\ncolor_range=unknown //调色必备参数\ncolor_space=unknown //调色必备参数\ncolor_transfer=unknown  //调色必备参数\ncolor_primaries=unknown //调色必备参数\nfield_order=progressive // 隔行扫描逐行扫描标识\nr_frame_rate=24/1  // 实际帧率\navg_frame_rate=24/1 // 平均帧率\ntime_base=1/24 //时间基，通常和帧率有对应关系\nstart_pts=0 // 开始时间戳\nstart_time=0.000000 // 开始时间\nduration_ts=17620 //duration 时间戳\nduration=734.166667 // duration 时间\nbit_rate=7862427 // 码率\nmax_bit_rate=N/A // 最大码率\nbits_per_raw_sample=8 // 原始数据每个采样占位\nnb_frames=17620 // 总帧数\nextradata_size=42 // extradata 大小\nTAG:language=eng // 这个是TAG，主要是展示语种\nTAG:handler_name=VideoHandle // 句柄名\nTAG:vendor_id=FFMP // 生成MP4文件的工具\nTAG:encoder=libx264 // 视频编码器标识\n[/STREAM]\n</code></pre><p>从我们获取的这个视频流的内容中可以看到，音视频流的信息，用[STREAM]标签括起来了，标签里面的内容是我们要关注的。</p><p>我们可以根据codec_type来区分这个流是音频还是视频。通过codec_name来确定这个流是什么编码的，例如我们这个流是H.264编码的视频流。而这个流的profile是High profile，Level是40，也就是我们通常说的4.0。H.264编码不仅仅有High Profile，还有Main Profile、Baseline Profile，各种Profile还可以配合多种Level，不同组合标准的编码参数信息会有一些不同，应用场景也会有差别，更详细的信息你可以查看H.264的参考标准文档的附录A部分。</p><p>我们回来继续看这个流信息，注意这个codec的tag_string是avc1，因为视频编码会封装到容器中，不同的容器对tag_string的支持也不同，我们现在看到的是H.264，其实在HLS的参考标准里，fragment mp4里HEVC编码的tag_string就不只是avc1一种TAG了，HEVC编码被封装到fragment mp4的话还可以支持HVC1、HEV1。而在HLS的fragment mp4里，我们需要指定封装成HVC1的tag_string才可以被正常地播放出来，这个在我们排查问题时也是需要用到的。</p><p>接下来是宽高信息，是否包含B帧，每个GOP（图像组，可以理解为两个关键帧中间的数据，包含图像组的头部关键帧）之间包含多少个B帧。sample_aspect_ratio（SAR）数据采样宽高比，display_aspect_ratio（DAR）显示宽高比，这两个数值中间如果有差别的话，肯定有一个因素，就是像素点不是矩形的，不是1比1的单个像素点。这就产生了Pixel Aspect Ratio（PAR）像素宽高比。我们可以这么理解它们的关系：</p><p>$$DAR = SAR \\times PAR$$</p><p>我们可以看到pix_fmt是yuv420p，在<a href=\"https://time.geekbang.org/column/article/546485\">第5节课</a>中，我们学推流编码时像素点用的也是yuv420p，因为在视频领域它的兼容性更好。通常互联网上的视频流对应的像素点格式，最常见的也是yuv420p。</p><p>然后在视频流的信息里面，我们还可以看到一些视频色彩空间对应的信息，在示例代码中，我们看到没有解析正确的，全都是unkown。</p><p>视频的场编码我们可以看到字段是field_order=progressive，也就是逐行编码。关于场编码，我们<a href=\"https://time.geekbang.org/column/article/541546\">第1节课</a>说过，因为以前显示设备刷新能力有限，在编码和解码的时候需要支持顶场和底场的隔行扫描编解码，所以我们现在拿到的这个视频流不是隔行扫描了，而是逐行扫描。因为分辨率是1920x800的，所以我们可以简单地称之为1080p。</p><p>视频的start_time与容器格式中一样，是0.000000。duration则与容器格式里面的duration有些不同了，duration=734.166667，因为容器格式和视频流是两个信息存储空间，所以难免会有一些差别。视频码率是7862427，帧率共分为三部分，分别是：实际上最基础的帧率 r_frame_rate=24/1；实际上平均帧率 avg_frame_rate=24/1；时间基，通常称之为timebase，time_base=1/24。</p><p>为什么会有这么多帧率表示呢？</p><p>因为我们在给视频编码对应成视频流，封装到容器中的时候，会有多个地方记录视频帧信息，加上视频流有可能是可变帧率的，因此需要多个参考值。在解决问题的时候，我们可以把多个值提取出来，分析哪个更适合我们自己的使用场景，当然并不是固定某一个就肯定是准确的，帧率在复杂业务场景下很容易出现各种诡异的问题，FFmpeg能做的就是把各个节点的帧率信息尽量全面地提供给我们。</p><p>然后我们可以看到这个视频流里面一共包含了17620帧图像，这个数据有些容器不一定能拿得到，有些不一定能拿得准，所以我们除了通过show_streams获取这些信息之外，还可以单独对视频包进行逐包查看与分析，下面我们来看一下怎么用ffprobe参数来查看音视频包的信息。</p><h2>音视频包分析</h2><p>在我们拿到视频文件或视频流URL之后，为了更精准地分析问题，也会经常用到<strong>ffprobe的-show_packets</strong>参数，ffprobe的show_packets参数可以将音视频的所有包都列出来。</p><p>因为音频包和视频包放在一起交错显示，看上去会比较复杂并且费劲，我们可以通过使用select_streams v来只读取视频流的包。ffprobe默认输出内容是平铺模式的，所以一屏只能输出几个包的信息，为了一屏能够输出更多的信息，我们可以考虑使用-of来定制输出的信息格式，例如xml，那么输出的packet信息格式就是这样：</p><pre><code class=\"language-plain\">&lt;packets&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"0\" pts_time=\"0.000000\" dts=\"-2\" dts_time=\"-0.083333\" duration=\"1\" duration_time=\"0.041667\" size=\"5264\" pos=\"36\" flags=\"K_\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"4\" pts_time=\"0.166667\" dts=\"-1\" dts_time=\"-0.041667\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"5300\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"2\" pts_time=\"0.083333\" dts=\"0\" dts_time=\"0.000000\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"5313\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"1\" pts_time=\"0.041667\" dts=\"1\" dts_time=\"0.041667\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"6382\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"3\" pts_time=\"0.125000\" dts=\"2\" dts_time=\"0.083333\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"7417\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"8\" pts_time=\"0.333333\" dts=\"3\" dts_time=\"0.125000\" duration=\"1\" duration_time=\"0.041667\" size=\"17\" pos=\"8378\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"6\" pts_time=\"0.250000\" dts=\"4\" dts_time=\"0.166667\" duration=\"1\" duration_time=\"0.041667\" size=\"15\" pos=\"9407\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"5\" pts_time=\"0.208333\" dts=\"5\" dts_time=\"0.208333\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"9933\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"7\" pts_time=\"0.291667\" dts=\"6\" dts_time=\"0.250000\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"10913\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"12\" pts_time=\"0.500000\" dts=\"7\" dts_time=\"0.291667\" duration=\"1\" duration_time=\"0.041667\" size=\"17\" pos=\"11884\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"10\" pts_time=\"0.416667\" dts=\"8\" dts_time=\"0.333333\" duration=\"1\" duration_time=\"0.041667\" size=\"15\" pos=\"12907\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"9\" pts_time=\"0.375000\" dts=\"9\" dts_time=\"0.375000\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"13927\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"11\" pts_time=\"0.458333\" dts=\"10\" dts_time=\"0.416667\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"14450\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"16\" pts_time=\"0.666667\" dts=\"11\" dts_time=\"0.458333\" duration=\"1\" duration_time=\"0.041667\" size=\"17\" pos=\"15482\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"14\" pts_time=\"0.583333\" dts=\"12\" dts_time=\"0.500000\" duration=\"1\" duration_time=\"0.041667\" size=\"15\" pos=\"16480\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"13\" pts_time=\"0.541667\" dts=\"13\" dts_time=\"0.541667\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"17462\" flags=\"__\"/&gt;\n        &lt;packet codec_type=\"video\" stream_index=\"0\" pts=\"15\" pts_time=\"0.625000\" dts=\"14\" dts_time=\"0.583333\" duration=\"1\" duration_time=\"0.041667\" size=\"13\" pos=\"18478\" flags=\"__\"/&gt;\n</code></pre><p>从输出的内容中可以看到，show_packets里面包含了多个packet包，因为我们选择的是视频流，所以codec_type是视频codec，流的索引是0。后面就是pts与pts_time、dts与dts_time、duratrion和duration_time、包的size大小、包的数据所在文件的偏移位置，最后是查看这个数据包是Keyframe关键帧还是普通帧，是不是Discard包等。</p><p>Discard包通常会在解码或者播放的时候被丢弃，这个是需要注意的，分析问题的时候可能会遇到这种情况。在这些信息中，我们发现pts还有对应的pts_time，这是为什么呢？因为pts是跟stream的timebase做时间转换之前的数值，而pts_time这个值是与stream的timebase做时间转换之后的值，pts、dts、duration是int64的类型，pts_time、dts_time、duration_time是double浮点类型。</p><p>我们还可以看到，pts与dts数值的排列顺序有些不太一样，dts是顺序排列的，pts是前后跳的，这是因为编码的时候视频数据中有B帧，解码的时候可以顺序解码，但是显示的时候需要重新按照pts的顺序显示。用show_packet我们能看到pts与dts的顺序不是一一对应的，也能看到是否是keyframe，但无法确认packet是I帧、P帧还是B帧。</p><p>为了能够确认这些信息，除了show_packets之外，ffprobe还可以通过show_frames来查看帧类型等信息，接下来我们看一下音视频的帧分析。</p><h2>音视频帧分析</h2><p>前面我们介绍了容器格式分析、音视频流分析以及音视频包分析三种分析方法，其实以上三个维度的分析是可以拿到音视频帧以外的信息的。如果想要拿到音视频帧相关的信息，还是需要通过音视频帧的信息来分析的，例如可以分析出当前这个视频帧是I帧、P帧还是B帧。</p><p>我们这里还是用视频来举例，学完之后你可以根据这节课学到的技能用音频来练习。我们用<strong>ffprobe的-show_frames</strong>来看一下拿到的帧信息都有哪些。</p><pre><code class=\"language-plain\"> &lt;frames&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"1\" pts=\"0\" pts_time=\"0.000000\" pkt_dts=\"0\" pkt_dts_time=\"0.000000\" best_effort_timestamp=\"0\" best_effort_timestamp_time=\"0.000000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"36\" pkt_size=\"5264\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"I\" coded_picture_number=\"0\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"&gt;\n            &lt;side_data_list&gt;\n                &lt;side_data side_data_type=\"H.26[45] User Data Unregistered SEI message\"/&gt;\n            &lt;/side_data_list&gt;\n        &lt;/frame&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"1\" pts_time=\"0.041667\" pkt_dts=\"1\" pkt_dts_time=\"0.041667\" best_effort_timestamp=\"1\" best_effort_timestamp_time=\"0.041667\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"6382\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"3\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"2\" pts_time=\"0.083333\" pkt_dts=\"2\" pkt_dts_time=\"0.083333\" best_effort_timestamp=\"2\" best_effort_timestamp_time=\"0.083333\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"5313\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"2\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"3\" pts_time=\"0.125000\" pkt_dts=\"3\" pkt_dts_time=\"0.125000\" best_effort_timestamp=\"3\" best_effort_timestamp_time=\"0.125000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"7417\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"4\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"4\" pts_time=\"0.166667\" pkt_dts=\"4\" pkt_dts_time=\"0.166667\" best_effort_timestamp=\"4\" best_effort_timestamp_time=\"0.166667\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"5300\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"P\" coded_picture_number=\"1\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"5\" pts_time=\"0.208333\" pkt_dts=\"5\" pkt_dts_time=\"0.208333\" best_effort_timestamp=\"5\" best_effort_timestamp_time=\"0.208333\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"9933\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"7\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"6\" pts_time=\"0.250000\" pkt_dts=\"6\" pkt_dts_time=\"0.250000\" best_effort_timestamp=\"6\" best_effort_timestamp_time=\"0.250000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"9407\" pkt_size=\"15\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"6\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"7\" pts_time=\"0.291667\" pkt_dts=\"7\" pkt_dts_time=\"0.291667\" best_effort_timestamp=\"7\" best_effort_timestamp_time=\"0.291667\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"10913\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"8\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"8\" pts_time=\"0.333333\" pkt_dts=\"8\" pkt_dts_time=\"0.333333\" best_effort_timestamp=\"8\" best_effort_timestamp_time=\"0.333333\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"8378\" pkt_size=\"17\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"P\" coded_picture_number=\"5\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"9\" pts_time=\"0.375000\" pkt_dts=\"9\" pkt_dts_time=\"0.375000\" best_effort_timestamp=\"9\" best_effort_timestamp_time=\"0.375000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"13927\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"11\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"10\" pts_time=\"0.416667\" pkt_dts=\"10\" pkt_dts_time=\"0.416667\" best_effort_timestamp=\"10\" best_effort_timestamp_time=\"0.416667\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"12907\" pkt_size=\"15\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"10\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"11\" pts_time=\"0.458333\" pkt_dts=\"11\" pkt_dts_time=\"0.458333\" best_effort_timestamp=\"11\" best_effort_timestamp_time=\"0.458333\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"14450\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"12\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"12\" pts_time=\"0.500000\" pkt_dts=\"12\" pkt_dts_time=\"0.500000\" best_effort_timestamp=\"12\" best_effort_timestamp_time=\"0.500000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"11884\" pkt_size=\"17\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"P\" coded_picture_number=\"9\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"13\" pts_time=\"0.541667\" pkt_dts=\"13\" pkt_dts_time=\"0.541667\" best_effort_timestamp=\"13\" best_effort_timestamp_time=\"0.541667\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"17462\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"15\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"14\" pts_time=\"0.583333\" pkt_dts=\"14\" pkt_dts_time=\"0.583333\" best_effort_timestamp=\"14\" best_effort_timestamp_time=\"0.583333\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"16480\" pkt_size=\"15\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"14\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"15\" pts_time=\"0.625000\" pkt_dts=\"15\" pkt_dts_time=\"0.625000\" best_effort_timestamp=\"15\" best_effort_timestamp_time=\"0.625000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"18478\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"16\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"16\" pts_time=\"0.666667\" pkt_dts=\"16\" pkt_dts_time=\"0.666667\" best_effort_timestamp=\"16\" best_effort_timestamp_time=\"0.666667\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"15482\" pkt_size=\"17\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"P\" coded_picture_number=\"13\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"0\" pts=\"17\" pts_time=\"0.708333\" pkt_dts=\"17\" pkt_dts_time=\"0.708333\" best_effort_timestamp=\"17\" best_effort_timestamp_time=\"0.708333\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"24496\" pkt_size=\"13\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"B\" coded_picture_number=\"18\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        &lt;frame media_type=\"video\" stream_index=\"0\" key_frame=\"1\" pts=\"18\" pts_time=\"0.750000\" pkt_dts=\"18\" pkt_dts_time=\"0.750000\" best_effort_timestamp=\"18\" best_effort_timestamp_time=\"0.750000\" pkt_duration=\"1\" pkt_duration_time=\"0.041667\" pkt_pos=\"18961\" pkt_size=\"4554\" width=\"1920\" height=\"800\" pix_fmt=\"yuv420p\" sample_aspect_ratio=\"1:1\" pict_type=\"I\" coded_picture_number=\"17\" display_picture_number=\"0\" interlaced_frame=\"0\" top_field_first=\"0\" repeat_pict=\"0\" chroma_location=\"left\"/&gt;\n        \n</code></pre><p>从视频帧信息中可以看到，第一个keyframe中携带着side_data，是关于H.264的编码器相关的用户信息。也就是当我们使用开源的x264做视频编码的时候，会看到x264的标识信息。</p><p>这一段输出信息中帧信息有两个keyframe，也可以理解是一个GOP大小的帧，这个帧的个数与我们用show_packets参数时看到的packet的数量，应该是能匹配得上的，但差别是这里的pts是顺序排列的，也能看到对应的packet size和packet pos，而且这里还能看到帧的类型、图像宽高、像素点格式、宽高比等场编码一类的信息。show_packets与show_frames搭配一起分析音视频数据包，会精确很多。</p><p>从上面的信息中，我们能够发现一个共同的问题，就是我们经常会拿到一大堆我们可能并不需要的字段，为了精准地获得自己需要的字段，我们可以使用ffprobe中的-show_entries来指定自己想要的字段，例如配合show_packet，我想拿到packet的pts_time、dts_time和flags，那么就可以用show_packets与show_entries来操作。命令如下：</p><pre><code class=\"language-plain\">ffprobe -show_packets -select_streams v -of xml -show_entries packet=pts_time,dts_time,flags  ~/Movies/Test/ToS-4k-1920.mov\n</code></pre><p>然后输出的内容就会精简很多，如下所示：</p><pre><code class=\"language-plain\">    &lt;packets&gt;\n        &lt;packet pts_time=\"0.000000\" dts_time=\"-0.083333\" flags=\"K_\"/&gt;\n        &lt;packet pts_time=\"0.166667\" dts_time=\"-0.041667\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.083333\" dts_time=\"0.000000\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.041667\" dts_time=\"0.041667\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.125000\" dts_time=\"0.083333\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.333333\" dts_time=\"0.125000\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.250000\" dts_time=\"0.166667\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.208333\" dts_time=\"0.208333\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.291667\" dts_time=\"0.250000\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.500000\" dts_time=\"0.291667\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.416667\" dts_time=\"0.333333\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.375000\" dts_time=\"0.375000\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.458333\" dts_time=\"0.416667\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.666667\" dts_time=\"0.458333\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.583333\" dts_time=\"0.500000\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.541667\" dts_time=\"0.541667\" flags=\"__\"/&gt;\n        &lt;packet pts_time=\"0.625000\" dts_time=\"0.583333\" flags=\"__\"/&gt;\n</code></pre><p>关于音视频封装容器、音视频流、音视频包、音视频帧之间对应的关系，我们用图片来表示会更直观一些。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/70/2aa125f394e765d15250e52bc6589c70.png?wh=1418x666\" alt=\"图片\"></p><p>以MPEGTS封装为例，封装里面包含3个流，分别是视频流，音频流，字幕流，视频流中需要存储对应的视频编码参数信息，用来在解码器解码时使用，而视频、音频和字幕流在存储或者传输的时候是一一对应的，如果偏差太大的话会造成音视频不同步问题。</p><p><img src=\"https://static001.geekbang.org/resource/image/b8/47/b8be33dd1e3acf2e46ca0c93d7466447.png?wh=1698x624\" alt=\"图片\"></p><p>而我们刚才讲到的音视频包和音视频帧之间的对应关系，你可以理解为一个AVPacket对应一个AVFrame，对照着图片理解会更清晰。</p><h2>小结</h2><p>这节课我们学习使用ffprobe来分析音视频的容器格式、音视频的流、音视频的包、音视频的帧信息。拿到这些信息之后我们可以精确地分析音视频多个维度的信息，无论是做音视频转码、故障分析等，都会有很大的用处。</p><p>因为ffprobe的黑科技参数还有很多，更细节的内容，还需要你参考FFmpeg的官方手册中的<a href=\"https://ffmpeg.org/ffprobe-all.html\">ffprobe页面</a>，自己在使用过程中去探索、挖掘。</p><p><img src=\"https://static001.geekbang.org/resource/image/35/10/35a37b1cd76f05969171yyd1c6f0fb10.png?wh=1871x839\" alt=\"图片\"></p><h2>思考题</h2><p>如果你拿到一个电影视频，想做分片转码，根据学过的内容，你该怎么分片呢？欢迎在评论区留下你的思考，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"05｜如何使用 FFmpeg 与 OBS 进行直播推流？","id":546485},"right":{"article_title":"07 | 如何高效查找并使用FFmpeg常用参数？","id":548420}}},{"article_id":548420,"article_title":"07 | 如何高效查找并使用FFmpeg常用参数？","article_content":"<p>你好，我是刘歧。</p><p>我们讲述直播推流的时候曾简单介绍过<a href=\"https://time.geekbang.org/column/article/546485\">FFmpeg推直播流</a>的操作，但是并不是特别全面，遇到一些问题的时候我们还是无法很好地解决。有时候，我们想要使用FFmpeg的某个功能，但不知道怎么查找，甚至可能不知道FFmpeg是否包含我们需要的能力。那么这节课我们会更全面地介绍FFmpeg中常用的参数，还有遇到问题的时候如何确认FFmpeg是否可以达到我们预期的目的。</p><p>如果你是第一次使用FFmpeg，肯定会有很多疑惑，尤其是看到命令行的一堆参数之后。所以这节课我会一步一步引导你先学会使用FFmpeg，最后让你拥有自己深度挖掘FFmpeg里面各种黑科技的能力。先吃到“鱼”，然后学会“钓鱼”，之后你就可以自己慢慢收获各种“鱼”了。</p><h2>FFmpeg 输入输出组成</h2><p>FFmpeg的工作原理比较简单，它没有指定输出文件的参数。一般的工具都会带一个-o来指定输出文件，但FFmpeg不是，它不用-o指定输出，FFmpeg自己会分析命令行然后确定输出。例如我们输入这么一段命令：</p><pre><code class=\"language-plain\">ffmpeg -i i.mp4 a.mp4 -vcodec mpeg4 b.mp4\n</code></pre><p>这段命令会输出两个文件，分别是a.mp4和b.mp4。</p><pre><code class=\"language-plain\">(base) liuqi05:xx liuqi$ ls\na.mp4 b.mp4\n(base) liuqi05:xx liuqi$\n</code></pre><!-- [[[read_end]]] --><p>因为b.mp4的输出参数部分我指定了vcodec为mpeg4，所以b.mp4的视频编码采用的是mpeg4。而a.mp4的输出部分我没有指定任何信息，所以a.mp4使用的是mp4格式默认的视频编码，也就是H.264编码。我们一起来看一下命令行执行的输出信息。</p><pre><code class=\"language-plain\">Output #0, mp4, to 'a.mp4':\n  Metadata:\n    major_brand     : mp42\n    minor_version   : 512\n    compatible_brands: mp42iso2avc1mp41\n    encoder         : Lavf59.25.100\n  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1920x800 [SAR 1:1 DAR 12:5], q=2-31, 24 fps, 12288 tbn (default)\n    Metadata:\n      creation_time   : 2022-04-01T09:59:10.000000Z\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\n      encoder         : Lavc59.33.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n  Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n    Metadata:\n      creation_time   : 2022-04-01T09:59:10.000000Z\n      handler_name    : Stereo\n      vendor_id       : [0][0][0][0]\n      encoder         : Lavc59.33.100 aac\nOutput #1, mp4, to 'b.mp4':\n  Metadata:\n    major_brand     : mp42\n    minor_version   : 512\n    compatible_brands: mp42iso2avc1mp41\n    encoder         : Lavf59.25.100\n  Stream #1:0(und): Video: mpeg4 (mp4v / 0x7634706D), yuv420p(tv, bt709, progressive), 1920x800 [SAR 1:1 DAR 12:5], q=2-31, 200 kb/s, 24 fps, 12288 tbn (default)\n    Metadata:\n      creation_time   : 2022-04-01T09:59:10.000000Z\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\n      encoder         : Lavc59.33.100 mpeg4\n    Side data:\n      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n  Stream #1:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n    Metadata:\n      creation_time   : 2022-04-01T09:59:10.000000Z\n      handler_name    : Stereo\n      vendor_id       : [0][0][0][0]\n      encoder         : Lavc59.33.100 aac\nframe=  116 fps= 73 q=28.0 q=2.0 size=       0kB time=00:00:05.22 bitrate=   0.1kbits/s speed=3.28x\n</code></pre><p>可以看到，FFmpeg的输入可以是多个，输出也可以是多个，但是每一个输出都会根据输入的信息做一个参数复制。遇到不用于输出封装格式的默认codec，FFmpeg会转成默认的封装格式对应的编码。</p><p>如果想要指定编码，每个输出格式都需要输出对应的编码，如果不想重新编码，可以使用-vcodec copy，-acodec copy、-scodec copy这样的参数，来进行只转封装不转码（做解码后再编码）的操作。不做视频转码操作的话，会节省CPU的计算资源，CPU占用率会降低很多，但是如果输入的视频码率特别高的话，文件也会特别大，这种情况做一下转码还是有必要的。</p><p>FFmpeg的命令行参数分布通常是这样：</p><pre><code class=\"language-plain\">ffmpeg [第一个输入文件对应的解析参数] -i 第一个输入文件名 [第二个输入文件对应的解析参数 ] -i 第二个输入文件名 [如果有第三个文件输入] [-i] [如果有第三个文件] [第一个输出文件对应的参数] [第一个输出文件名] [第二个输出文件对应的参数] [第二个输出文件名] [第三个输出文件对应的参数] [第三个输出文件名]\n</code></pre><p>FFmpeg的输入输出组成看上去比较简单，但是参数和官方文档的使用说明还是太多，怎么办呢？没关系，多数的畏难情绪是因为不了解。接下来，我们看一下FFmpeg的参数组成，了解清楚它的参数内容，相信你的疑虑就会打消大半了。</p><h2>如何查找各个模块参数的帮助信息？</h2><p>当我们查看FFmpeg的帮助信息和官方的文档的时候，第一感觉是文档和参数太多了，一时间根本无从下手。其实FFmpeg官方文档和帮助信息中的内容并不多，主要取决于你怎么看、怎么用。比如，我们使用命令行的时候，能够看到FFmpeg参数的情况。</p><pre><code class=\"language-plain\">ffmpeg --help\n</code></pre><p>如果认真看帮助信息的话，你就会发现开头还有一部分关于查看帮助的方法。</p><pre><code class=\"language-plain\">Getting help:\n    -h      -- print basic options\n    -h long -- print more options\n    -h full -- print all options (including all format and codec specific options, very long)\n    -h type=name -- print all options for the named decoder/encoder/demuxer/muxer/filter/bsf/protocol\n    See man ffmpeg for detailed description of the options.\n</code></pre><p>查看帮助，我们可以看到基础版本，也就是–help，而–help还有两个常用参数，long和full。long这个参数，除了可以查看基础帮助信息之外，还能查看更多高级操作的帮助内容；full可以输出所有的帮助信息，可以全部打印出来查看。</p><p>下面还有个type=name的方式查看帮助信息，正如前面几节课我们讲到的FFmpeg一样，FFmpeg是包含了封装与解封装、编码与解码、滤镜以及传输协议模块的，所以我们可以通过type=name来过滤掉我们不需要的信息，只查看我们会用到的模块。例如我想查看FLV的封装，那就可以输入帮助操作。</p><pre><code class=\"language-plain\">ffmpeg -h muxer=flv\n\nMuxer flv [FLV (Flash Video)]:\n    Common extensions: flv.\n    Mime type: video/x-flv.\n    Default video codec: flv1.\n    Default audio codec: adpcm_swf.\nflv muxer AVOptions:\n  -flvflags          &lt;flags&gt;      E.......... FLV muxer flags (default 0)\n     aac_seq_header_detect              E.......... Put AAC sequence header based on stream data\n     no_sequence_end              E.......... disable sequence end for FLV\n     no_metadata                  E.......... disable metadata for FLV\n     no_duration_filesize              E.......... disable duration and filesize zero value metadata for FLV\n     add_keyframe_index              E.......... Add keyframe index metadata\n</code></pre><p>这样的话，我们得到的参数就比直接用ffmpeg --help full要简洁多了，只看到了我们想看到的部分。如果想要看编码对应的帮助信息，也可以用同样的方法，把muxer换成encoder就可以了。</p><pre><code class=\"language-plain\">ffmpeg -h encoder=libx264\n\nEncoder libx264 [libx264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10]:\n    General capabilities: dr1 delay threads\n    Threading capabilities: other\n    Supported pixel formats: yuv420p yuvj420p yuv422p yuvj422p yuv444p yuvj444p nv12 nv16 nv21 yuv420p10le yuv422p10le yuv444p10le nv20le gray gray10le\nlibx264 AVOptions:\n  -preset            &lt;string&gt;     E..V....... Set the encoding preset (cf. x264 --fullhelp) (default \"medium\")\n  -tune              &lt;string&gt;     E..V....... Tune the encoding params (cf. x264 --fullhelp)\n  -profile           &lt;string&gt;     E..V....... Set profile restrictions (cf. x264 --fullhelp)\n  -fastfirstpass     &lt;boolean&gt;    E..V....... Use fast settings when encoding first pass (default true)\n  -level             &lt;string&gt;     E..V....... Specify level (as defined by Annex A)\n  -passlogfile       &lt;string&gt;     E..V....... Filename for 2 pass stats\n  -wpredp            &lt;string&gt;     E..V....... Weighted prediction for P-frames\n  -a53cc             &lt;boolean&gt;    E..V....... Use A53 Closed Captions (if available) (default true)\n  -x264opts          &lt;string&gt;     E..V....... x264 options\n  -crf               &lt;float&gt;      E..V....... Select the quality for constant quality mode (from -1 to FLT_MAX) (default -1)\n  -crf_max           &lt;float&gt;      E..V....... In CRF mode, prevents VBV from lowering quality beyond this point. (from -1 to FLT_MAX) (default -1)\n  -qp                &lt;int&gt;        E..V....... Constant quantization parameter rate control method (from -1 to INT_MAX) (default -1)\n  -aq-mode           &lt;int&gt;        E..V....... AQ method (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     variance        1            E..V....... Variance AQ (complexity mask)\n     autovariance    2            E..V....... Auto-variance AQ\n     autovariance-biased 3            E..V....... Auto-variance AQ with bias to dark scenes\n  -aq-strength       &lt;float&gt;      E..V....... AQ strength. Reduces blocking and blurring in flat and textured areas. (from -1 to FLT_MAX) (default -1)\n  -psy               &lt;boolean&gt;    E..V....... Use psychovisual optimizations. (default auto)\n  -psy-rd            &lt;string&gt;     E..V....... Strength of psychovisual optimization, in &lt;psy-rd&gt;:&lt;psy-trellis&gt; format.\n  -rc-lookahead      &lt;int&gt;        E..V....... Number of frames to look ahead for frametype and ratecontrol (from -1 to INT_MAX) (default -1)\n  -weightb           &lt;boolean&gt;    E..V....... Weighted prediction for B-frames. (default auto)\n  -weightp           &lt;int&gt;        E..V....... Weighted prediction analysis method. (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     simple          1            E..V.......\n     smart           2            E..V.......\n  -ssim              &lt;boolean&gt;    E..V....... Calculate and print SSIM stats. (default auto)\n  -intra-refresh     &lt;boolean&gt;    E..V....... Use Periodic Intra Refresh instead of IDR frames. (default auto)\n  -bluray-compat     &lt;boolean&gt;    E..V....... Bluray compatibility workarounds. (default auto)\n  -b-bias            &lt;int&gt;        E..V....... Influences how often B-frames are used (from INT_MIN to INT_MAX) (default INT_MIN)\n  -b-pyramid         &lt;int&gt;        E..V....... Keep some B-frames as references. (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     strict          1            E..V....... Strictly hierarchical pyramid\n     normal          2            E..V....... Non-strict (not Blu-ray compatible)\n  -mixed-refs        &lt;boolean&gt;    E..V....... One reference per partition, as opposed to one reference per macroblock (default auto)\n  -8x8dct            &lt;boolean&gt;    E..V....... High profile 8x8 transform. (default auto)\n  -fast-pskip        &lt;boolean&gt;    E..V....... (default auto)\n  -aud               &lt;boolean&gt;    E..V....... Use access unit delimiters. (default auto)\n  -mbtree            &lt;boolean&gt;    E..V....... Use macroblock tree ratecontrol. (default auto)\n  -deblock           &lt;string&gt;     E..V....... Loop filter parameters, in &lt;alpha:beta&gt; form.\n  -cplxblur          &lt;float&gt;      E..V....... Reduce fluctuations in QP (before curve compression) (from -1 to FLT_MAX) (default -1)\n  -partitions        &lt;string&gt;     E..V....... A comma-separated list of partitions to consider. Possible values: p8x8, p4x4, b8x8, i8x8, i4x4, none, all\n  -direct-pred       &lt;int&gt;        E..V....... Direct MV prediction mode (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     spatial         1            E..V.......\n     temporal        2            E..V.......\n     auto            3            E..V.......\n  -slice-max-size    &lt;int&gt;        E..V....... Limit the size of each slice in bytes (from -1 to INT_MAX) (default -1)\n  -stats             &lt;string&gt;     E..V....... Filename for 2 pass stats\n  -nal-hrd           &lt;int&gt;        E..V....... Signal HRD information (requires vbv-bufsize; cbr not allowed in .mp4) (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     vbr             1            E..V.......\n     cbr             2            E..V.......\n  -avcintra-class    &lt;int&gt;        E..V....... AVC-Intra class 50/100/200/300/480 (from -1 to 480) (default -1)\n  -me_method         &lt;int&gt;        E..V....... Set motion estimation method (from -1 to 4) (default -1)\n     dia             0            E..V.......\n     hex             1            E..V.......\n     umh             2            E..V.......\n     esa             3            E..V.......\n     tesa            4            E..V.......\n  -motion-est        &lt;int&gt;        E..V....... Set motion estimation method (from -1 to 4) (default -1)\n     dia             0            E..V.......\n     hex             1            E..V.......\n     umh             2            E..V.......\n     esa             3            E..V.......\n     tesa            4            E..V.......\n  -forced-idr        &lt;boolean&gt;    E..V....... If forcing keyframes, force them as IDR frames. (default false)\n  -coder             &lt;int&gt;        E..V....... Coder type (from -1 to 1) (default default)\n     default         -1           E..V.......\n     cavlc           0            E..V.......\n     cabac           1            E..V.......\n     vlc             0            E..V.......\n     ac              1            E..V.......\n  -b_strategy        &lt;int&gt;        E..V....... Strategy to choose between I/P/B-frames (from -1 to 2) (default -1)\n  -chromaoffset      &lt;int&gt;        E..V....... QP difference between chroma and luma (from INT_MIN to INT_MAX) (default 0)\n  -sc_threshold      &lt;int&gt;        E..V....... Scene change threshold (from INT_MIN to INT_MAX) (default -1)\n  -noise_reduction   &lt;int&gt;        E..V....... Noise reduction (from INT_MIN to INT_MAX) (default -1)\n  -udu_sei           &lt;boolean&gt;    E..V....... Use user data unregistered SEI if available (default false)\n  -x264-params       &lt;dictionary&gt; E..V....... Override the x264 configuration using a :-separated list of key=value parameters\n</code></pre><p>比全部输出简洁多了，但是我们怎么确认我们使用的type名称是真实存在的呢？如果我直接输入ffmpeg -h encoder=helloworld，肯定是拿不到任何信息的，而且会返回错误。这个时候我们就需要从头部看FFmpeg的help信息了。</p><pre><code class=\"language-plain\">Print help / information / capabilities:\n-L                  show license\n-h topic            show help\n-? topic            show help\n-help topic         show help\n--help topic        show help\n-version            show version\n-buildconf          show build configuration\n-formats            show available formats\n-muxers             show available muxers\n-demuxers           show available demuxers\n-devices            show available devices\n-codecs             show available codecs\n-decoders           show available decoders\n-encoders           show available encoders\n-bsfs               show available bit stream filters\n-protocols          show available protocols\n-filters            show available filters\n-pix_fmts           show available pixel formats\n-layouts            show standard channel layouts\n-sample_fmts        show available audio sample formats\n-dispositions       show available stream dispositions\n-colors             show available color names\n-sources device     list sources of the input device\n-sinks device       list sinks of the output device\n-hwaccels           show available HW acceleration methods\n</code></pre><p>FFmpeg帮助信息中提供了一些查看各个模块的全部列表，例如我们如果想查看我们机器上的FFmpeg是否支持nvidia的H.264编码器的话，就可以通过参数ffmpeg -encoders来确认机器上当前安装的FFmpeg中是否涵盖了nvidia的H.264编码器。</p><pre><code class=\"language-plain\">ffmpeg -encoders|grep H.264\n</code></pre><p>我把所有输出信息通过管道用grep过滤了一遍，得到的输出信息是这样的。</p><pre><code class=\"language-plain\"> V....D libx264              libx264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (codec h264)\n V....D libx264rgb           libx264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 RGB (codec h264)\n V....D h264_videotoolbox    VideoToolbox H.264 Encoder (codec h264)\n</code></pre><p>从输出的内容中可以看到，我的机器上没有安装nvidia的H.264编码器，但是我机器上安装了lib264和h264_videotoolbox的编码器，所以可以直接查看libx264或者h264_videotoolbox的帮助信息。</p><p><strong>只要有针对性地查找帮助信息，就可以过滤掉很多我们不想看到的内容。</strong>另外，还有一小部分是常用的参数，接下来我们通过一些实际场景来学习一下。</p><h2>FFmpeg 公共基础参数</h2><p>当我们使用FFmpeg时，有一些贯穿FFmpeg各个组件的核心参数，在我们查看帮助信息时就可以看到，help不带参数的话就会输出基础帮助信息。</p><p>基础帮助信息中包含了这几部分：</p><ol>\n<li>公共操作部分</li>\n</ol><p>公共操作部分的参数主要包括把日志输出到日志文件的参数 -report，输出日志文件名的格式是ffmpeg-20220703-170736.log，也就是ffmpeg-年月日-时分秒.log的文件名格式。你可以使用命令行ffmpeg -report试验一下。</p><p>还可以将FFmpeg的日志输出到终端，通过-v参数来设置日志的级别，主要分为quiet、panic、fatal、error、warning、info、verbose、debug、trace9个级别，按先后顺序输出的内容越来越多。如果我们想看到全部的处理信息，用trace就可以。如果不特意设置的话，默认是info级别，所以我们看到的通常是info、warning、error以及更严重的错误信息。如果我们什么信息都不想看到，用quiet就能做到。</p><ol start=\"2\">\n<li>每个文件主要操作部分</li>\n</ol><p>文件主要操作部分的参数除了我们前面课程提到的-codec、-c、-f之外，还可以使用-ss来定位文件的开始时间，通过-t来规定输出文件时间长度。</p><p>如果我们想做分片转码的话，就可以使用-ss和-t两个参数分隔视频文件。另外，要注意-ss指定的位置最好是关键帧位置。比如说，我们可以通过ffprobe -show_packet input.mp4找到视频关键帧对应的pts位置，然后通过-ss定位到关键帧位置，-t设置为下一个关键帧减去当前关键帧位置的时间长度，这样就能完成切片了。</p><p>需要注意的是，-ss放在-i参数左侧来定位开始的位置会比放在右侧要快很多，但是放在-i左侧的话，定位通常不准确，但如果我们把-ss的时间点位设置为关键帧对应的点位，那定位就是准确的。另外，我们还可以使用-codec:v copy -an去掉音频，从而达到分离音视频的目的。</p><p>如果想给视频添加metadata信息的话，例如指定文件的标题等信息，可以通过-metadata参数来设置metadata。</p><p>我们通过命令行测一下效果。</p><pre><code class=\"language-plain\">ffmpeg -f lavfi -i testsrc=s=176x144 -metadata title=\"This Is A Test\" -t 2 out.mp4\n</code></pre><p>我们先确认一下输出的out.mp4文件是否包含了我们设定的Title“This Is A Test.”。</p><pre><code class=\"language-plain\">ffmpeg -i out.mp4\n\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from 'out.mp4':\n  Metadata:\n    major_brand     : isom\n    minor_version   : 512\n    compatible_brands: isomiso2avc1mp41\n    title           : This Is A Test\n    encoder         : Lavf58.77.100\n  Duration: 00:00:02.00, start: 0.000000, bitrate: 49 kb/s\n  Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 176x144 [SAR 1:1 DAR 11:9], 44 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : [0][0][0][0]\n</code></pre><p>从输出的信息中可以看到文件里带有我们设定的title信息。</p><ol start=\"3\">\n<li>视频操作部分</li>\n</ol><p>既然是做音视频处理，FFmpeg除了文件公共部分，还有视频操作公共部分的参数。比如说：</p><ul>\n<li>-r:v设置视频的帧率；</li>\n<li>-vb设置视频码率；</li>\n<li>-vframes设置视频输出的帧数；</li>\n<li>-aspect设置视频的宽高比；</li>\n<li>-vn关闭视频流处理操作，也就是屏蔽视频流；</li>\n<li>-vf给视频做简单滤镜处理，简单滤镜处理一般不支持多图层、多输入、多输出的滤镜。</li>\n</ul><ol start=\"4\">\n<li>音频操作部分</li>\n</ol><p>同样的，除了视频操作部分的参数，FFmpeg还有音频操作公共部分的参数，比如说：</p><ul>\n<li>-ar设置音频采样率；</li>\n<li>-ab设置音频码率；</li>\n<li>-aframes设置音频输出的帧数；</li>\n<li>-ac设置音频的声道数量</li>\n<li>-an关闭音频流处理操作，也就是屏蔽音频流</li>\n<li>-af给音频做简单滤镜处理，简单滤镜处理一般不支持多图层、多输入、多输出的滤镜；</li>\n<li>-vol设置音频的音量。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/bf/c8/bfe09d7f10e2a5401b4e12ce849686c8.png?wh=1132x1921\" alt=\"\"></p><h2>FFmpeg 公共高级参数</h2><p>前面我们讲到了一些基础操作部分，也提到了通过help long还可以查看到高级操作部分。高级操作部分是基于基础部分做的一些更高级的内容延伸。</p><p>-filter_complex参数可以将音视频混合在一条参数字符串里进行操作，也可以输入、输出多个视频流和音频流。如果滤镜字符串太长的话，一条命令行可能会遇到很多麻烦，例如命令行支持的输入内容长度有最长限制，从而导致无法输入完整的FFmpeg命令参数，这时我们可以考虑使用外挂filter脚本来处理，使用参数-filter_script能够解决这些问题。</p><p>-copytb参数设定timebase与输入的相同，确保时间戳不会跳变，当然这么做会有一定的风险，毕竟视频的MPEGTS封装格式与MP4的封装格式对应的timebase还是有差别的，如果强制使用相同的timebase则会引起时间戳间隔相关问题，严重的话甚至会引起丢帧。-force_key_frames在做编码的时候可以根据这个参数做强制关键帧设定，-force_key_frames支持表达式方式处理。FFmpeg的表达式处理可以参考<a href=\"https://ffmpeg.org/ffmpeg-utils.html#Expression-Evaluation\">FFmpeg官方文档</a>。</p><h2>小结</h2><p>最后，我们来对这节课的内容做一个回顾吧！</p><p>FFmpeg常用的参数主要分为输入输出组成、公共基础参数、高级参数几方面内容，这节课学习这些参数的用法除了让你了解FFmpeg常用的参数混个脸熟之外，更深层次的目的是消除你的畏难情绪。</p><p>FFmpeg作为一个必备的音视频开发工具，确实参数比较多，但入门之后你就会发现，里面的参数是分模块的，非常好记也非常好用，尤其是FFmpeg公共基础参数部分，我们把这部分内容分成了公共操作参数、每个文件主要的操作参数、音频和视频操作参数四部分，每个部分的参数功能都非常明确清晰。</p><p>除此之外，学会查看各个模块参数的帮助手册，就算是掌握了“钓鱼”的方法。我希望你可以好好地利用这些方法，真正地用起来，让这些参数知识、查找方法真正成为你趁手的工具。</p><h2><strong>思考题</strong></h2><p>最后，我给你留道思考题。使用FFmpeg的filter_complex和表达式，实现这个场景：前景是一个小图（也可以是logo），背景是一个视频；logo位置与视频的进度同步，在视频下方游动，也就是视频开始的时候logo在视频的最左下角，视频结束的时候在视频的最右下角，视频播放的时候按照视频进度比例从左下角向右下角移动。</p><p>我们应该怎么去实现这个场景呢？欢迎你把自己的想法写在评论区和我交流，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"06｜如何使用 ffprobe 分析音视频参数与内容？","id":547562},"right":{"article_title":"08｜FFmpeg 和它的朋友们是怎么处理 MP4 的？","id":549422}}},{"article_id":549422,"article_title":"08｜FFmpeg 和它的朋友们是怎么处理 MP4 的？","article_content":"<p>你好，我是刘歧。</p><p>我们日常看电视剧，录视频时，最常见的就是MP4格式了。你有没有想过，MP4格式为什么使用得这么广泛呢？</p><p>因为MP4标准非常灵活，可扩展性比较好，有很多常见的格式是基于MP4做了一些扩展，然后被应用到比较广的范围，比如CMAF、DASH、HLS。而且MP4的参考标准是一个开放的标准，我们通常以编号为ISO-14496-12来查找标准文档。因为MP4的使用范围比较广，我们在<a href=\"https://time.geekbang.org/column/article/544986\">第3节课</a>的时候，也着重讲了MP4封装容器格式，你可以回顾一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/68/6b/689b8f155c2ed9yy6dbb007fa474586b.png?wh=1920x1083\" alt=\"图片\" title=\"MP4扩展出来的格式\"></p><p>基于MP4的重要地位，我这节课来给你讲一讲，如何用FFmpeg、GPAC等工具生成与解析MP4。</p><p>尽管FFmpeg的目标是自己不去创造标准，但是难免会有一些工具或者用户会根据自己的臆测做一些定制或者修改，导致与我们公认的标准出现一些偏差。为了让MP4的标准性更好地得到验证，我们通常会选择使用多种工具，所以这节课除了给你介绍FFmpeg对MP4的mux与demux（封装与解封装）之外，我还会介绍一些其他的MP4相关的工具，例如MP4Box、Shaka- Packager。</p><p>在我们使用FFmpeg做音视频处理的时候，经常会使用FFmpeg生成MP4文件，或者使用FFmpeg输入MP4文件然后转换成其他格式。这里我们就先来了解一下FFmpeg对MP4都有哪些能力支持。这就需要用到<a href=\"https://time.geekbang.org/column/article/548420\">上节课</a>的知识了，你可以停下来先想一下我们应该怎么在FFmpeg中查找自己想要的帮助信息。</p><!-- [[[read_end]]] --><h3>用FFmpeg生成MP4文件</h3><p>首先，查看MP4的muxer对应的参数，输入ffmpeg -h muxer=mp4，看一下输出的内容。</p><pre><code class=\"language-plain\">Muxer mp4 [MP4 (MPEG-4 Part 14)]:\n    Common extensions: mp4.\n    Mime type: video/mp4.\n    Default video codec: h264.\n    Default audio codec: aac.\nmov/mp4/tgp/psp/tg2/ipod/ismv/f4v muxer AVOptions:\n  -movflags          &lt;flags&gt;      E.......... MOV muxer flags (default 0)\n     rtphint                      E.......... Add RTP hint tracks\n     empty_moov                   E.......... Make the initial moov atom empty\n     frag_keyframe                E.......... Fragment at video keyframes\n     frag_every_frame              E.......... Fragment at every frame\n     separate_moof                E.......... Write separate moof/mdat atoms for each track\n     frag_custom                  E.......... Flush fragments on caller requests\n     isml                         E.......... Create a live smooth streaming feed (for pushing to a publishing point)\n     faststart                    E.......... Run a second pass to put the index (moov atom) at the beginning of the file\n     omit_tfhd_offset              E.......... Omit the base data offset in tfhd atoms\n     disable_chpl                 E.......... Disable Nero chapter atom\n     default_base_moof              E.......... Set the default-base-is-moof flag in tfhd atoms\n     dash                         E.......... Write DASH compatible fragmented MP4\n     cmaf                         E.......... Write CMAF compatible fragmented MP4\n     frag_discont                 E.......... Signal that the next fragment is discontinuous from earlier ones\n     delay_moov                   E.......... Delay writing the initial moov until the first fragment is cut, or until the first fragment flush\n     global_sidx                  E.......... Write a global sidx index at the start of the file\n     skip_sidx                    E.......... Skip writing of sidx atom\n     write_colr                   E.......... Write colr atom even if the color info is unspecified (Experimental, may be renamed or changed, do not use from scripts)\n     prefer_icc                   E.......... If writing colr atom prioritise usage of ICC profile if it exists in stream packet side data\n     write_gama                   E.......... Write deprecated gama atom\n     use_metadata_tags              E.......... Use mdta atom for metadata.\n     skip_trailer                 E.......... Skip writing the mfra/tfra/mfro trailer for fragmented files\n     negative_cts_offsets              E.......... Use negative CTS offsets (reducing the need for edit lists)\n  -moov_size         &lt;int&gt;        E.......... maximum moov size so it can be placed at the begin (from 0 to INT_MAX) (default 0)\n  -rtpflags          &lt;flags&gt;      E.......... RTP muxer flags (default 0)\n     latm                         E.......... Use MP4A-LATM packetization instead of MPEG4-GENERIC for AAC\n     rfc2190                      E.......... Use RFC 2190 packetization instead of RFC 4629 for H.263\n     skip_rtcp                    E.......... Don't send RTCP sender reports\n     h264_mode0                   E.......... Use mode 0 for H.264 in RTP\n     send_bye                     E.......... Send RTCP BYE packets when finishing\n  -skip_iods         &lt;boolean&gt;    E.......... Skip writing iods atom. (default true)\n  -iods_audio_profile &lt;int&gt;        E.......... iods audio profile atom. (from -1 to 255) (default -1)\n  -iods_video_profile &lt;int&gt;        E.......... iods video profile atom. (from -1 to 255) (default -1)\n  -frag_duration     &lt;int&gt;        E.......... Maximum fragment duration (from 0 to INT_MAX) (default 0)\n  -min_frag_duration &lt;int&gt;        E.......... Minimum fragment duration (from 0 to INT_MAX) (default 0)\n  -frag_size         &lt;int&gt;        E.......... Maximum fragment size (from 0 to INT_MAX) (default 0)\n  -ism_lookahead     &lt;int&gt;        E.......... Number of lookahead entries for ISM files (from 0 to 255) (default 0)\n  -video_track_timescale &lt;int&gt;        E.......... set timescale of all video tracks (from 0 to INT_MAX) (default 0)\n  -brand             &lt;string&gt;     E.......... Override major brand\n  -use_editlist      &lt;boolean&gt;    E.......... use edit list (default auto)\n  -fragment_index    &lt;int&gt;        E.......... Fragment number of the next fragment (from 1 to INT_MAX) (default 1)\n  -mov_gamma         &lt;float&gt;      E.......... gamma value for gama atom (from 0 to 10) (default 0)\n  -frag_interleave   &lt;int&gt;        E.......... Interleave samples within fragments (max number of consecutive samples, lower is tighter interleaving, but with more overhead) (from 0 to INT_MAX) (default 0)\n  -encryption_scheme &lt;string&gt;     E.......... Configures the encryption scheme, allowed values are none, cenc-aes-ctr\n  -encryption_key    &lt;binary&gt;     E.......... The media encryption key (hex)\n  -encryption_kid    &lt;binary&gt;     E.......... The media encryption key identifier (hex)\n  -use_stream_ids_as_track_ids &lt;boolean&gt;    E.......... use stream ids as track ids (default false)\n  -write_btrt        &lt;boolean&gt;    E.......... force or disable writing btrt (default auto)\n  -write_tmcd        &lt;boolean&gt;    E.......... force or disable writing tmcd (default auto)\n  -write_prft        &lt;int&gt;        E.......... Write producer reference time box with specified time source (from 0 to 2) (default 0)\n     wallclock       1            E..........\n     pts             2            E..........\n  -empty_hdlr_name   &lt;boolean&gt;    E.......... write zero-length name string in hdlr atoms within mdia and minf atoms (default false)\n  -movie_timescale   &lt;int&gt;        E.......... set movie timescale (from 1 to INT_MAX) (default 1000)\n</code></pre><p>就像开头我说的那样，因为MP4的灵活性比较好，变种形态也比较多，为了方便使用，FFmpeg就增加了很多符合标准但又有可能用不到的参数，我们一会儿抽取一些例子来讲讲。</p><p>在讲例子之前，我们先看一下这个帮助信息里面的内容，帮助信息的头部给出了MP4的文件扩展名.mp4，视频默认用H.264编码，音频默认用AAC编码。</p><p>接下来我们来看一下FFmpeg封装MP4常用的参数有哪些。</p><p><img src=\"https://static001.geekbang.org/resource/image/47/a2/4788f2d19294d9f4b036bf0ed8042aa2.png?wh=2069x2864\" alt=\"图片\"></p><p>从参数的列表中可以看到，MP4的muxer支持的参数比较复杂，例如支持在视频关键帧处切片、支持设置moov容器的最大大小、支持设置encrypt加密等。下面我们用常见的参数来举几个例子。</p><h3>faststart参数</h3><p>正常情况下，FFmpeg生成moov是在mdat写完成之后，但我们可以通过参数faststart把moov容器移动到mdat前面，我们可以参考下面这个例子。</p><pre><code class=\"language-plain\">./ffmpeg -i input.flv -c copy -f mp4 output.mp4\n</code></pre><p>使用mp4info查看output.mp4的容器出现顺序。</p><p><img src=\"https://static001.geekbang.org/resource/image/59/bf/59b0386632994a0d4bba6yy7f7d958bf.png?wh=1728x1250\" alt=\"图片\"></p><p>可以看到图中moov容器是在mdat的下边，如果使用参数faststart，就会在生成上边的结构之后将moov移动到mdat前面。</p><pre><code class=\"language-plain\">./ffmpeg -i input.flv -c copy -f mp4 -movflags faststart output.mp4\n</code></pre><p>然后我们再使用mp4info查看MP4的容器顺序，可以看到moov被移动到了mdat前面。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/94/c9bcd9d90d3a4b2d41e6472d52a3de94.png?wh=1728x1250\" alt=\"图片\"></p><h3>DASH参数</h3><p>当生成DASH格式的时候，里面有一种特殊的MP4格式，其实我们也可以把它理解成MP4切片，可以通过DASH参数生成。</p><pre><code class=\"language-plain\">./ffmpeg -i input.flv -c copy -f mp4 -movflags dash output.mp4\n</code></pre><p>使用mp4info查看容器格式的信息，稍微有些特殊，我们来看一下图片。<br>\n<img src=\"https://static001.geekbang.org/resource/image/db/ed/db6ef849yya8855082932af6807e50ed.png?wh=1728x1254\" alt=\"图片\"></p><p>从图中可以看到，这个DASH格式的MP4文件存储的容器信息与常规的MP4格式有些差别，以sidx、moof与mdat这三种容器为主。</p><p>有些时候，我们在做MP4切片操作的时候，可能会出现一些问题，例如，editlist的时间戳相关计算变量与实际数据对应的时间戳有偏差，导致视频丢帧或音画略有不同步的现象。这个时候我们可以考虑把use_edilist操作项设置成忽略。</p><p><strong>MP4切片很常用，当我们做HLS/DASH直播、生成点播内容，还有做MP4上传云端实时转码等操作时，MP4切片都是比较常见的操作。</strong></p><p>通过这些例子，相信你已经对MP4生成部分的能力有一些了解了，剩余的能力我们可以课后自己一点点挖掘。接下来我们看一下FFmpeg的MP4文件解析操作。</p><h3>用FFmpeg解析MP4文件</h3><p>首先FFmpeg在解析MP4文件格式的时候，可能会因为MP4的内容生成得不标准产生一些奇奇怪怪的问题，例如前面我们提到的音视频不同步或者视频抖动等问题。FFmpeg针对出现的这类问题也做了格式上的兼容，但用户可能需要自己手动设置一些参数，定制一下，才可以解决这些问题。</p><p>FFmpeg这么做的主要原因是之前大部分用户使用时是正常的，异常素材只有少部分用户才会遇到。为了尽量不改变原有用户的使用习惯，只能通过新用户自己设置参数的形式来避免出现异常情况。我们看一下操作选项。</p><pre><code class=\"language-plain\">Demuxer mov,mp4,m4a,3gp,3g2,mj2 [QuickTime / MOV]:\n    Common extensions: mov,mp4,m4a,3gp,3g2,mj2,psp,m4b,ism,ismv,isma,f4v,avif.\nmov,mp4,m4a,3gp,3g2,mj2 AVOptions:\n  -use_absolute_path &lt;boolean&gt;    .D.V....... allow using absolute path when opening alias, this is a possible security issue (default false)\n  -seek_streams_individually &lt;boolean&gt;    .D.V....... Seek each stream individually to the closest point (default true)\n  -ignore_editlist   &lt;boolean&gt;    .D.V....... Ignore the edit list atom. (default false)\n  -advanced_editlist &lt;boolean&gt;    .D.V....... Modify the AVIndex according to the editlists. Use this option to decode in the order specified by the edits. (default true)\n  -ignore_chapters   &lt;boolean&gt;    .D.V.......  (default false)\n  -use_mfra_for      &lt;int&gt;        .D.V....... use mfra for fragment timestamps (from -1 to 2) (default auto)\n     auto            -1           .D.V....... auto\n     dts             1            .D.V....... dts\n     pts             2            .D.V....... pts\n  -use_tfdt          &lt;boolean&gt;    .D.V....... use tfdt for fragment timestamps (default true)\n  -export_all        &lt;boolean&gt;    .D.V....... Export unrecognized metadata entries (default false)\n  -export_xmp        &lt;boolean&gt;    .D.V....... Export full XMP metadata (default false)\n  -activation_bytes  &lt;binary&gt;     .D......... Secret bytes for Audible AAX files\n  -audible_key       &lt;binary&gt;     .D......... AES-128 Key for Audible AAXC files\n  -audible_iv        &lt;binary&gt;     .D......... AES-128 IV for Audible AAXC files\n  -audible_fixed_key &lt;binary&gt;     .D......... Fixed key used for handling Audible AAX files\n  -decryption_key    &lt;binary&gt;     .D......... The media decryption key (hex)\n  -enable_drefs      &lt;boolean&gt;    .D.V....... Enable external track support. (default false)\n  -max_stts_delta    &lt;int&gt;        .D......... treat offsets above this value as invalid (from 0 to UINT32_MAX) (default 4294487295)\n</code></pre><p>从内容中可以看到，FFmpeg的MP4 demuxer里提供了editlist相关的一些操作，比如ignore_editlist、-use_tfdt等。MP4切片时，可能会遇到切片的时间戳与editlist参考计算出来的时间戳对不上的问题，用户就可以使用-use_tfdt选项，来选择是否使用tfdt里面的时间戳。</p><p>FFmpeg为什么会给MP4的demuxer加这么多兼容性的参数呢？</p><p>因为可以生成和处理MP4文件的工具不止FFmpeg，还有其他的工具，例如GPAC、Shaka-Packager。工具多了，生成出来的MP4文件可能不统一，存在一定的差异，这个差异就可能引起兼容性问题。</p><h2>如何用GPAC生成MP4？</h2><p>其实要说生成MP4更接近参考标准的，就要数GPAC了，可能你之前并没有听说过GPAC，但是GPAC里的MP4Box工具你应该听说过。下面我们先来了解一下MP4Box。和使用FFmpeg类似，我们先看一下help信息。</p><pre><code class=\"language-plain\">MP4Box --help\n</code></pre><p>这是输出的内容。</p><pre><code class=\"language-plain\">MP4Box [option] input [option]\n\n\n\nGeneral Options:\n\n-h (string):                   print help\n\t* general: general options help\n\t* hint: hinting options help\n\t* dash: DASH segmenter help\n\t* import: import options help\n\t* encode: encode options help\n\t* meta: meta handling options help\n\t* extract: extraction options help\n\t* dump: dump options help\n\t* swf: Flash (SWF) options help\n\t* crypt: ISMA E&amp;A options help\n\t* format: supported formats help\n\t* live: BIFS streamer help\n\t* core: libgpac core options\n\t* all: print all the above help screens\n\t* opts: print all options\n\t* VAL: search for option named VAL (without - or --) in MP4Box, libgpac core and all filters\n\n-hx (string):                  look for given string in all possible options\n-nodes:                        list supported MPEG4 nodes\n-node (string):                get given MPEG4 node syntax and QP infolist\n-xnodes:                       list supported X3D nodes\n-xnode (string):               get given X3D node syntax\n-snodes:                       list supported SVG nodes\n-languages:                    list supported ISO 639 languages\n-boxes:                        list all supported ISOBMF boxes and their syntax\n-fstat:                        print filter session statistics (import/export/encrypt/decrypt/dashing)\n-fgraph:                       print filter session graph (import/export/encrypt/decrypt/dashing)\n-v:                            verbose mode\n-version:                      get build version\n---  INPUT:                    escape option if INPUT starts with - character\n</code></pre><p>帮助信息里有更详细的参数帮助信息，使用-h加对应的参数就可以得到，例如MP4Box -h dash，就可以查看dash切片帮助信息了。</p><p>我们使用 MP4Box查看一下MP4文件信息。</p><pre><code class=\"language-plain\"># MP4Box -info ~/Movies/Test/ToS-4k-1920.mov\n\n* Movie Info *\n\tTimescale 1000 - 2 tracks\n\tComputed Duration 00:12:14.167 - Indicated Duration 00:12:14.167\n\tFragmented File: no\n\tFile Brand qt   - version 512\n\t\tCompatible brands: qt\n\tCreated: UNKNOWN DATE\tModified: UNKNOWN DATE\nFile has no MPEG4 IOD/OD\n1 UDTA types: A9737772 (1)\n\nTrack # 1 Info - TrackID 1 - TimeScale 24\nMedia Duration 00:12:14.166 - Indicated Duration 00:12:14.166\nTrack has 1 edit lists: track duration is 00:12:14.167\nMedia Info: Language \"Undetermined (und)\" - Type \"vide:avc1\" - 17620 samples\nVisual Sample Entry Info: width=1920 height=800 (depth=24 bits)\nVisual Track layout: x=0 y=0 width=1920 height=800\nMPEG-4 Config: Visual Stream - ObjectTypeIndication 0x21\nAVC/H264 Video - Visual Size 1920 x 800\n\tAVC Info: 1 SPS - 1 PPS - Profile High @ Level 4\n\tNAL Unit length bits: 32\n\tPixel Aspect Ratio 1:1 - Indicated track size 1920 x 800\n\tChroma format YUV 4:2:0 - Luma bit depth 8 - chroma bit depth 8\n\tSPS#1 hash: DEC7C9D830854068543D5AE5BC84AA68081EC57C\n\tPPS#1 hash: 12874FF8439E10C45D6C9B519B94BDAADC9759BD\nSelf-synchronized\n\tRFC6381 Codec Parameters: avc1.640028\n\tAverage GOP length: 17 samples\n\tMax sample duration: 1 / 24\n\nTrack # 2 Info - TrackID 2 - TimeScale 44100\nMedia Duration 00:12:14.122 - Indicated Duration 00:12:14.122\nTrack has 1 edit lists: track duration is 00:12:14.123\nMedia Info: Language \"Undetermined (und)\" - Type \"soun:mp4a\" - 31616 samples\nMPEG-4 Config: Audio Stream - ObjectTypeIndication 0x40\nMPEG-4 Audio AAC LC (AOT=2 implicit) - 2 Channel(s) - SampleRate 44100\nSynchronized on stream 1\n\tRFC6381 Codec Parameters: mp4a.40.2\nAlternate Group ID 1\n\tAll samples are sync\n\tMax sample duration: 1024 / 44100\n</code></pre><p>其实从帮助信息中我们可以看到，MP4Box也可以切DASH和HLS，除了MP4Box，我们还可以尝试使用Shaka-Packager来做对应的操作。</p><h2>Shaka-Packager的HLS与DASH</h2><p>Shaka-Packager是Google的一个开源项目，除了用FFmpeg和GPAC做HLS、DASH之外，用Shaka-Packager其实也是个不错的选择，我们稍微了解一下。</p><p>例如，用Shaka-Packager做一个HLS视频流加密操作。</p><pre><code class=\"language-plain\">packager 'input=../in.mp4,stream=video,segment_template=output$Number$.m4s,playlist_name=video_playlist.m3u8,init_segment=init.mp4'\n--hls_master_playlist_output=\"master_playlist.m3u8\"\n--hls_base_url=\"http://127.0.0.1/\" --enable_raw_key_encryption\n--protection_scheme cbcs --keys\nkey=61616161616161616161616161616161:key_id=61616161616161616161616161616161\n--clear_lead 0\n</code></pre><p>这样就可以通过Shaka-Packager生成一个DRM的HLS了，输入一个MP4文件，视频流输出m4s，列表名为video_playlist.m3u8，mp4切片的初始文件名为init.mp4，master列表名为master_playlist.m3u8，使用raw_key方式加密内容，加密保护模式为cbcs模式，密钥是61616161616161616161616161616161，切片的init之后的第一个MP4切片文件不加密，从第二片开始加密。</p><p>Shaka-Packager也是一个很大的项目，更多的能力需要你自己慢慢去挖掘。<a href=\"https://shaka-project.github.io/shaka-packager/html/\">Shaka-Packager官方文档</a>的内容也比较全，你可以参考一下。</p><p>总的来说，Shaka-Packager和GPAC比FFmpeg要简单得多，使用的API操作也比较容易，所以如果不是多格式、多codec，对兼容性要求不高且音视频应用场景简单的话，Shaka-Packager和GPAC也是不错的选择。</p><h2>小结</h2><p>MP4格式因其开放性和灵活性，使用范围非常广泛，用MP4做切片后可以封装成HLS或者DASH做分发，日常的使用频率是比较高的。因此它是我们必须要掌握的一种格式。</p><p><img src=\"https://static001.geekbang.org/resource/image/0a/fd/0a1f3cc5b0d97efaeyy3f97c69e08bfd.png?wh=1920x835\" alt=\"图片\"></p><p>能够生成和解析MP4的工具有很多，FFmpeg就是其中之一。考虑兼容性的问题，FFmpeg给出了很多参数，其中有一些比较常用的参数，如faststart（把moov容器移动到mdat前面）、DASH（兼容DASH格式的MP4分片）等参数是需要我们掌握的。除了FFmpeg之外，在简单场景下我们还可以用GPAC或者Shaka-Packager做MP4的流媒体处理。</p><p>学完今天的内容，你就可以使用FFmpeg做一些MP4的日常处理了，如果你想要获得更多的能力支持，需要再深入研究一下这三个工具的参数，并且动手去操作一下。</p><h2>思考题</h2><p>一个视频流转成MP4文件时，如何用FFmpeg对视频流内容做加密呢？加密之后如何用FFmpeg解密并顺利地播放出来呢？欢迎在评论区留下你的思考，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"07 | 如何高效查找并使用FFmpeg常用参数？","id":548420},"right":{"article_title":"09｜如何使用 FFmpeg 与 Handbrake 做转码？","id":550128}}},{"article_id":550128,"article_title":"09｜如何使用 FFmpeg 与 Handbrake 做转码？","article_content":"<p>你好，我是刘歧。</p><p>学到这里，不知道你有没有发现一个事情：从开始到现在，我们讲得最多的就是FFmpeg的命令行工具和参数，很少讲到界面操作。其实在做视频转码时，大多数人除了经常用FFmpeg之外，还会用一些免费的带界面的转码工具，这节课我除了会教你如何使用FFmpeg转码之外，还会给你介绍一个非常好用的本地化转码工具Handbrake。</p><h2>如何使用FFmpeg转码？</h2><p>在我们专栏开篇的基础部分，我给你讲过音视频相关的图像色彩、编解码、封装（Mux）与解封装（Demux）的基本原理。<a href=\"https://time.geekbang.org/column/article/548420\">第7节课</a>，我又给你讲了如何高效地使用FFmpeg帮助信息和文档。在前面这些内容的基础上，我们来学以致用，讲一讲怎么用FFmpeg转码。</p><p>首先我们需要确定我们在转码的时候输出文件的应用场景。假如你是希望传到快手这样的内容发布平台，那么我们是需要转换成平台要求的转码规格的。既然要转码，就需要先看一下自己电脑系统的环境是否支持这一操作，比如使用CPU做转码，电脑会不会变得很慢，如果电脑上有GPU，使用GPU转码的话，CPU理所当然地会空出来，这样就不会影响我们继续使用电脑做其他的事情了。</p><p>我们先来了解一下怎么使用CPU做转码。</p><h3>用 CPU 转码</h3><!-- [[[read_end]]] --><p>使用CPU转码的话，通常是用CPU解码，然后用libx264、libx265、librav1e之类的编码器编码，也叫软编码。当然也有人用OpenH264或者其他自己定制的编码器，因为编码参数大多数是与编码的参考标准对应的，通用的或者常见的编码参数在libx264、libx265、librav1e里面相差无几，所以这里为了简洁一点，我使用FFmpeg与libx264来做软编码。我们先来回顾一下转码的基本操作流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/f3/54a1e6a1264b0c1a54f84c743b8a2ff3.png?wh=1142x492\" alt=\"图片\"></p><p>首先是读取文件后解析文件，然后对文件进行解封装，也就是demux。将解封装后的音视频流数据进行解码，得到原始数据，也就是我们<a href=\"https://time.geekbang.org/column/article/541546\">第1节课</a>讲的YUV数据或者PCM数据，然后用我们设置的目标编码对应的编码器进行编码，编码后的数据写入音频流或者视频流里，封装音频流或者视频流，写入文件里。</p><p>回顾完流程之后，我们用<a href=\"https://time.geekbang.org/column/article/548420\">第7节课</a>学到的知识查看一下我们能如何使用libx264。首先看一下libx264在FFmpeg里面支持的参数。</p><p>使用命令行ffmpeg -h encoder=libx264查看一下参数内容。</p><pre><code class=\"language-plain\">Encoder libx264 [libx264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10]:\n    General capabilities: dr1 delay threads\n    Threading capabilities: other\n    Supported pixel formats: yuv420p yuvj420p yuv422p yuvj422p yuv444p yuvj444p nv12 nv16 nv21 yuv420p10le yuv422p10le yuv444p10le nv20le gray gray10le\nlibx264 AVOptions:\n  -preset            &lt;string&gt;     E..V....... Set the encoding preset (cf. x264 --fullhelp) (default \"medium\")\n  -tune              &lt;string&gt;     E..V....... Tune the encoding params (cf. x264 --fullhelp)\n  -profile           &lt;string&gt;     E..V....... Set profile restrictions (cf. x264 --fullhelp)\n  -fastfirstpass     &lt;boolean&gt;    E..V....... Use fast settings when encoding first pass (default true)\n  -level             &lt;string&gt;     E..V....... Specify level (as defined by Annex A)\n  -passlogfile       &lt;string&gt;     E..V....... Filename for 2 pass stats\n  -wpredp            &lt;string&gt;     E..V....... Weighted prediction for P-frames\n  -a53cc             &lt;boolean&gt;    E..V....... Use A53 Closed Captions (if available) (default true)\n  -x264opts          &lt;string&gt;     E..V....... x264 options\n  -crf               &lt;float&gt;      E..V....... Select the quality for constant quality mode (from -1 to FLT_MAX) (default -1)\n  -crf_max           &lt;float&gt;      E..V....... In CRF mode, prevents VBV from lowering quality beyond this point. (from -1 to FLT_MAX) (default -1)\n  -qp                &lt;int&gt;        E..V....... Constant quantization parameter rate control method (from -1 to INT_MAX) (default -1)\n  -aq-mode           &lt;int&gt;        E..V....... AQ method (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     variance        1            E..V....... Variance AQ (complexity mask)\n     autovariance    2            E..V....... Auto-variance AQ\n     autovariance-biased 3            E..V....... Auto-variance AQ with bias to dark scenes\n  -aq-strength       &lt;float&gt;      E..V....... AQ strength. Reduces blocking and blurring in flat and textured areas. (from -1 to FLT_MAX) (default -1)\n  -psy               &lt;boolean&gt;    E..V....... Use psychovisual optimizations. (default auto)\n  -psy-rd            &lt;string&gt;     E..V....... Strength of psychovisual optimization, in &lt;psy-rd&gt;:&lt;psy-trellis&gt; format.\n  -rc-lookahead      &lt;int&gt;        E..V....... Number of frames to look ahead for frametype and ratecontrol (from -1 to INT_MAX) (default -1)\n  -weightb           &lt;boolean&gt;    E..V....... Weighted prediction for B-frames. (default auto)\n  -weightp           &lt;int&gt;        E..V....... Weighted prediction analysis method. (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     simple          1            E..V.......\n     smart           2            E..V.......\n  -ssim              &lt;boolean&gt;    E..V....... Calculate and print SSIM stats. (default auto)\n  -intra-refresh     &lt;boolean&gt;    E..V....... Use Periodic Intra Refresh instead of IDR frames. (default auto)\n  -bluray-compat     &lt;boolean&gt;    E..V....... Bluray compatibility workarounds. (default auto)\n  -b-bias            &lt;int&gt;        E..V....... Influences how often B-frames are used (from INT_MIN to INT_MAX) (default INT_MIN)\n  -b-pyramid         &lt;int&gt;        E..V....... Keep some B-frames as references. (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     strict          1            E..V....... Strictly hierarchical pyramid\n     normal          2            E..V....... Non-strict (not Blu-ray compatible)\n  -mixed-refs        &lt;boolean&gt;    E..V....... One reference per partition, as opposed to one reference per macroblock (default auto)\n  -8x8dct            &lt;boolean&gt;    E..V....... High profile 8x8 transform. (default auto)\n  -fast-pskip        &lt;boolean&gt;    E..V....... (default auto)\n  -aud               &lt;boolean&gt;    E..V....... Use access unit delimiters. (default auto)\n  -mbtree            &lt;boolean&gt;    E..V....... Use macroblock tree ratecontrol. (default auto)\n  -deblock           &lt;string&gt;     E..V....... Loop filter parameters, in &lt;alpha:beta&gt; form.\n  -cplxblur          &lt;float&gt;      E..V....... Reduce fluctuations in QP (before curve compression) (from -1 to FLT_MAX) (default -1)\n  -partitions        &lt;string&gt;     E..V....... A comma-separated list of partitions to consider. Possible values: p8x8, p4x4, b8x8, i8x8, i4x4, none, all\n  -direct-pred       &lt;int&gt;        E..V....... Direct MV prediction mode (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     spatial         1            E..V.......\n     temporal        2            E..V.......\n     auto            3            E..V.......\n  -slice-max-size    &lt;int&gt;        E..V....... Limit the size of each slice in bytes (from -1 to INT_MAX) (default -1)\n  -stats             &lt;string&gt;     E..V....... Filename for 2 pass stats\n  -nal-hrd           &lt;int&gt;        E..V....... Signal HRD information (requires vbv-bufsize; cbr not allowed in .mp4) (from -1 to INT_MAX) (default -1)\n     none            0            E..V.......\n     vbr             1            E..V.......\n     cbr             2            E..V.......\n  -avcintra-class    &lt;int&gt;        E..V....... AVC-Intra class 50/100/200/300/480 (from -1 to 480) (default -1)\n  -me_method         &lt;int&gt;        E..V....... Set motion estimation method (from -1 to 4) (default -1)\n     dia             0            E..V.......\n     hex             1            E..V.......\n     umh             2            E..V.......\n     esa             3            E..V.......\n     tesa            4            E..V.......\n  -motion-est        &lt;int&gt;        E..V....... Set motion estimation method (from -1 to 4) (default -1)\n     dia             0            E..V.......\n     hex             1            E..V.......\n     umh             2            E..V.......\n     esa             3            E..V.......\n     tesa            4            E..V.......\n  -forced-idr        &lt;boolean&gt;    E..V....... If forcing keyframes, force them as IDR frames. (default false)\n  -coder             &lt;int&gt;        E..V....... Coder type (from -1 to 1) (default default)\n     default         -1           E..V.......\n     cavlc           0            E..V.......\n     cabac           1            E..V.......\n     vlc             0            E..V.......\n     ac              1            E..V.......\n  -b_strategy        &lt;int&gt;        E..V....... Strategy to choose between I/P/B-frames (from -1 to 2) (default -1)\n  -chromaoffset      &lt;int&gt;        E..V....... QP difference between chroma and luma (from INT_MIN to INT_MAX) (default 0)\n  -sc_threshold      &lt;int&gt;        E..V....... Scene change threshold (from INT_MIN to INT_MAX) (default -1)\n  -noise_reduction   &lt;int&gt;        E..V....... Noise reduction (from INT_MIN to INT_MAX) (default -1)\n  -udu_sei           &lt;boolean&gt;    E..V....... Use user data unregistered SEI if available (default false)\n  -x264-params       &lt;dictionary&gt; E..V....... Override the x264 configuration using a :-separated list of key=value parameters\n</code></pre><p>从帮助信息中可以看到，libx264编码支持的图像色彩格式主要包括yuv420p、yuvj420p、yuv422p、yuvj422p、yuv444p、yuvj444p、nv12、nv16、nv21、yuv420p10le、yuv422p10le、yuv444p10le、nv20le、gray、gray10le，我们通常统一编码成yuv420p即可。如果确定播放器可以支持HDR的话，也可以考虑用yuv420p10le。但是如果想要在Web浏览器上正常播放出来的话，yuv420p是最稳定的格式。</p><p>为了解决设置编码参数时参数太多、太琐碎的问题，libx264提供了预置模板preset，在FFmpeg里默认用的是medium模板，也就是平衡画质与编码速度的最优选择。除了medium，还可以按照帮助信息里面的提示，通过使用x264 --fullhelp查看x264的其他preset，例如还有ultrafast、superfast、veryfast、faster、fast、slow、slower、veryslow、placebo。</p><p>除了preset模板，还有调优类型的模板tune，包括film、animation、grain、stillimage、psnr、ssim、fastdecode、zerolatency等不同的模版。</p><p><img src=\"https://static001.geekbang.org/resource/image/ee/09/ee8dd86b8e2c941a3b937acae374eb09.png?wh=1560x888\" alt=\"图片\"></p><p>不同的模板支持的参数也略有差别，比如视频编码想做画面延迟低的直播流的话，可以考虑设置tune为zerolatency。因为zerolatency模板里已经包含了低延迟编码的参数。</p><p><img src=\"https://static001.geekbang.org/resource/image/41/a2/414dd247876e469c13b6440d197374a2.png?wh=1856x806\" alt=\"图片\"></p><p>其中宏块树是一种视频编码结构，在编码时它可以增加slice处理的层数，降低视频编码的码率，但是复杂度会略有提升，所以耗时也会增加一些。你可以结合极客时间上<a href=\"https://time.geekbang.org/column/article/468091\">视频编码</a>及<a href=\"https://time.geekbang.org/column/article/463775\">帧内预测</a>相关的课程来理解，这里我就不展开说了。</p><p>slice的的意思是将一帧图像切成多个切片，然后将多个片放到多个线程里处理，从而达到并发处理的的目的。因为lookahead是0，不需要提前预存多个帧做缓存，也没有双向参考帧B帧，不需要预读多个帧做缓存，所以最大限度地降低了帧缓存引起的画面延迟。</p><p>除了以上两类模板，在给视频转码做转码的时候，有时也会被要求转成恒定码率的视频流，也就是我们常说的CBR，这个CBR可以通过参数nal-hrd cbr来设置，但是实际的码率不一定能够控制得很好，所以通常会搭配FFmpeg的maxrate、minrate与bufsize来精确地控制码率，一般bufsize控制比maxrate小大概1/3 ~ 1/2即可，达到的效果如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/ba/9ca113092d51469d046ef3115d846aba.png?wh=1920x990\" alt=\"图片\"></p><p>如果使用当前FFmpeg里面的libx264参数无法达到要求，但用x264没问题的话，我们就可以通过FFmpeg预留的x264opts来设置更多x264的参数，例如设置x264为OpenGOP模式，就需要使用参数-x264opts “open-gop=1”，来达到使用OpenGOP的编码模式的目的。</p><p>在同画质下，使用OpenGOP比CloseGOP码率更低一些，但是也可能会引入一些不稳定因素，例如视频切片的时候找不到关键帧，这一点需要我们注意。</p><p>说了这么多，接下来我们实际操练一下，使用FFmpeg命令行来做转码。</p><p>你先下载一个《大雄兔》或者《钢铁之泪》的电影，这两部电影是开放版权的，在互联网上能够搜到对应的视频文件，自己测试的时候可以随便用。先输入命令行：</p><pre><code class=\"language-plain\">ffmpeg -i ~/Movies/Test/ToS-4k-1920.mov -pix_fmt yuv420p -vcodec libx264 -nal-hrd cbr -tune zerolatency -preset superfast -maxrate 900k -minrate 890k -bufsize 300k -x264opts \"open-gop=1\" output.ts\n</code></pre><p>命令行执行后，输出的内容是这样的：</p><pre><code class=\"language-plain\">Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/Users/liuqi/Movies/Test/ToS-4k-1920.mov':\n  Metadata:\n    major_brand     : qt\n    minor_version   : 512\n    compatible_brands: qt\n    encoder         : Lavf54.29.104\n  Duration: 00:12:14.17, start: 0.000000, bitrate: 8051 kb/s\n  Stream #0:0[0x1](eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 1920x800 [SAR 1:1 DAR 12:5], 7862 kb/s, 24 fps, 24 tbr, 24 tbn (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : FFMP\n      encoder         : libx264\n  Stream #0:1[0x2](eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 182 kb/s (default)\n    Metadata:\n      handler_name    : SoundHandler\n      vendor_id       : [0][0][0][0]\nStream mapping:\n  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (libx264))\n  Stream #0:1 -&gt; #0:1 (aac (native) -&gt; mp2 (native))\nPress [q] to stop, [?] for help\n[libx264 @ 0x619000006480] CBR HRD requires constant bitrate\n[libx264 @ 0x619000006480] using SAR=1/1\n[libx264 @ 0x619000006480] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n[libx264 @ 0x619000006480] profile High, level 4.0, 4:2:0, 8-bit\nOutput #0, mpegts, to 'output.ts':\n  Metadata:\n    major_brand     : qt\n    minor_version   : 512\n    compatible_brands: qt\n    encoder         : Lavf59.25.100\n  Stream #0:0(eng): Video: h264, yuv420p(progressive), 1920x800 [SAR 1:1 DAR 12:5], q=2-31, 24 fps, 90k tbn (default)\n    Metadata:\n      handler_name    : VideoHandler\n      vendor_id       : FFMP\n      encoder         : Lavc59.33.100 libx264\n    Side data:\n      cpb: bitrate max/min/avg: 900000/0/0 buffer size: 300000 vbv_delay: N/A\n  Stream #0:1(eng): Audio: mp2, 44100 Hz, stereo, s16, 384 kb/s (default)\n    Metadata:\n      handler_name    : SoundHandler\n      vendor_id       : [0][0][0][0]\n      encoder         : Lavc59.33.100 mp2\nframe=  240 fps=0.0 q=39.0 Lsize=     778kB time=00:00:09.99 bitrate= 637.3kbits/s speed=11.5x\nvideo:218kB audio:469kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 13.097445%\n[libx264 @ 0x619000006480] frame I:3     Avg QP:16.91  size: 16651\n[libx264 @ 0x619000006480] frame P:237   Avg QP: 6.68  size:   733\n[libx264 @ 0x619000006480] mb I  I16..4: 71.5% 23.2%  5.3%\n[libx264 @ 0x619000006480] mb P  I16..4:  0.3%  0.7%  0.0%  P16..4:  2.3%  0.0%  0.0%  0.0%  0.0%    skip:96.7%\n[libx264 @ 0x619000006480] 8x8 transform intra:43.0% inter:46.2%\n[libx264 @ 0x619000006480] coded y,uvDC,uvAC intra: 19.4% 18.2% 3.9% inter: 0.1% 0.3% 0.0%\n[libx264 @ 0x619000006480] i16 v,h,dc,p: 60%  9% 24%  7%\n[libx264 @ 0x619000006480] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 12% 15% 27%  9%  6%  7%  9%  6%  8%\n[libx264 @ 0x619000006480] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 19% 29%  5%  5%  6%  8%  3%  5%\n[libx264 @ 0x619000006480] i8c dc,h,v,p: 78% 10% 10%  2%\n[libx264 @ 0x619000006480] Weighted P-Frames: Y:1.3% UV:0.0%\n[libx264 @ 0x619000006480] kb/s:178.99\n</code></pre><p>从输出的内容中可以看到，编码的帧类型里只有I帧和P帧，设置的CBR模式已经生效，你也可以尝试把输入文件改成直播推流。</p><p>但是需要注意的是，设置视频编码流为CloseGOP，关键帧间隔的-g设置成fps的一半即可，fps需要使用参数-r:v来设置，例如设置-r:v 为 30，就是 30 fps，那么-g可以设置为15，也就是每隔15帧会有一个关键帧，这样可以达到0.5秒钟一个关键帧。当然，实际上这么做会很浪费带宽，常规的秀场直播设置2~5秒钟一个关键帧就可以了。</p><p>如果我们平时用CPU转码的话，对CPU的消耗会比较高，转码的时候电脑做其他事情会比较慢，一般电脑上有GPU的话直接选择用GPU转码，这样可以节省一些CPU计算资源。</p><h3>用GPU转码</h3><p>用GPU转码之前，你需要先确认一下自己当前电脑里的GPU是否可以做转码，然后安装对应的音视频编解码环境（GPU相关的驱动、软件、开发库等）。</p><p>FFmpeg支持的硬件加速方案，按照各OS厂商、Chip厂商特定方案，还有行业联盟定义的标准来分的话，大致可以分成3类：</p><ul>\n<li>操作系统：包括Windows、Linux、macOS /iOS、Android等。</li>\n<li>Chip厂商的特定方案：包括Intel、AMD、Nvidia等。</li>\n<li>行业标准或事实标准：包括OpenMAX和OpenCL、Vulkan、OpenGL还有cuda等。</li>\n</ul><p>这只是一个粗略的分类，很多时候，这几者之间纵横交错，联系密切，之间的关系并非像列出的这般泾渭分明。</p><p>下面就是Windows环境下，在AMD、Intel、Nvidia 的GPU上用dxva2和d3d11va来解码，再使用厂商提供的编码器编码的例子。</p><ol>\n<li>AMD AMF</li>\n</ol><pre><code class=\"language-plain\">ffmpeg -hwaccel dxva2 -hwaccel_output_format dxva2_vld -i &lt;video&gt; -c:v h264_amf -b:v 2M -y out.mp4\nffmpeg -hwaccel d3d11va -hwaccel_output_format d3d11 -i &lt;video&gt; -c:v h264_amf -b:v 2M -y out.mp4\n</code></pre><ol start=\"2\">\n<li>Intel QSV</li>\n</ol><pre><code class=\"language-plain\">ffmpeg -hwaccel dxva2 -hwaccel_output_format dxva2_vld -i &lt;video&gt; -c:v h264_qsv -vf hwmap=derive_device=qsv,format=qsv -b:v 2M -y out.mp4\nffmpeg -hwaccel d3d11va -hwaccel_output_format d3d11 -i &lt;video&gt; -c:v h264_qsv -vf hwmap=derive_device=qsv,format=qsv -b:v 2M -y out.mp4\n</code></pre><ol start=\"3\">\n<li>Nvidia NVENC</li>\n</ol><pre><code class=\"language-plain\">ffmpeg -hwaccel d3d11va -hwaccel_output_format d3d11 -i &lt;video&gt; -c:v h264_nvenc -b:v 2M -y out.mp4\n</code></pre><p>比如我自己本机是苹果电脑，那么我使用videotoolbox做转码就可以。</p><pre><code class=\"language-plain\">./ffmpeg -vcodec h264_vda -i input.mp4 -vcodec h264_videotoolbox -b:v 2000k output.mp4\n</code></pre><p>更多GPU转码或音视频滤镜处理相关的操作，你可以通过GPU对应的开发者官方网站了解，当然，FFmpeg也提供了一些快速上手的<a href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro\">操作手册</a>，你也可以作为参考。<br>\n讲到这里，我们一直在用命令做转码，还要记好多参数，有没有一种更简单的方式来实现转码呢？当然有，接下来我们看一下带界面的转码工具——Handbrake。</p><h2>使用Handbrake转码</h2><p>我们打开Handbrake之后，先选择一个我们想要转码的视频文件，然后就会顺利打开Handbrake的界面。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/8d/2ccf112d0afdb95b3083810eaf0f958d.png?wh=1920x1251\" alt=\"图片\"></p><p>设置基础能力部分有很多预置模板，点“预设”的话，我们可以把参数调整为适配最终输出的格式。</p><p><img src=\"https://static001.geekbang.org/resource/image/88/1d/88861174a525dcd9e38305c73a57161d.png?wh=1640x1486\" alt=\"图片\"></p><p>不用预设的参数也可以自己定义，例如自己定制输出尺寸、滤镜以及视频编码参数，也有对应的使用FFmpeg做转码时相关参数的设置项。</p><p><img src=\"https://static001.geekbang.org/resource/image/81/65/817de4be011d0d613bd1df7fab7f8065.png?wh=1920x868\" alt=\"图片\"></p><p>如果选择使用x264编码的话，也有预设模板和调优模板。</p><p><img src=\"https://static001.geekbang.org/resource/image/14/e6/148dc8951fe92bd78f4f0c0c24124ee6.png?wh=1920x651\" alt=\"图片\"></p><p>由于是界面操作并且已经汉化，所以比较容易上手，你可以自己下载一个试试看，Handbrake设置还有更多黑科技，你可以在使用过程中慢慢挖掘。</p><h2>小结</h2><p>最后，我们总结一下这节课的内容吧！</p><p><img src=\"https://static001.geekbang.org/resource/image/88/35/88ac9e954ecf616315429756344e9535.png?wh=1920x1429\" alt=\"图片\"></p><p>给视频做转码时，我们最常用的工具就是FFmpeg，使用FFmpeg转码的时候，我们既可以选择使用软编码做转码，也可以使用硬编码做转码。</p><p>具体使用哪一种需要你根据自己的使用场景来决定，<strong>如果是CPU特别富裕的话，使用软编码是一个不错的选择。</strong>因为软编码对画质相关调优的参数自主可控性更高一些，硬编码因硬件设计比较固定，所以有些时候画质调优方面比较受限，例如对不同视频应用场景的高清低码率相关的调优，使用硬转码效果会比软转码差一些。同时因可控性受限，硬转码的码率普遍会比软转码的视频码率高。</p><p>除了使用FFmpeg来做软硬转码之外，我们还可以<strong>借助Handbrake这个界面工具</strong>，里面的参数设置我们可以使用它给出的模版，也可以自己根据需要去定义。</p><h2>思考题</h2><p>结合前面讲过的知识，你可以自己分析一下x264的编码参数，按照这样的参数输出视频编码。</p><ol>\n<li>每秒钟30fps的720p的视频。</li>\n<li>2秒钟一个关键帧（GOP）。</li>\n<li>每两个P帧（包括I帧）之间两个B帧。</li>\n</ol><p>欢迎在评论区留下你的答案，也欢迎你把这节课分享给需要的朋友。我们下节课再见！</p>","neighbors":{"left":{"article_title":"08｜FFmpeg 和它的朋友们是怎么处理 MP4 的？","id":549422},"right":{"article_title":"10 | FFmpeg 基础模块（一）：容器相关的 API 操作","id":551256}}},{"article_id":551256,"article_title":"10 | FFmpeg 基础模块（一）：容器相关的 API 操作","article_content":"<p>你好，我是刘歧。</p><p>学到这里，相信你对音视频的基本工作原理和基本操作已经有了一定的认识，并且能够通过命令行、参考标准文档等工具独立完成一些音视频工作了。课程到现在开始渐入佳境，我们要从工具用户逐渐转变成API用户了。毕竟还有很多工作是FFmpeg命令行操作起来不太方便的，比如直播连麦的动态画面拼接、连麦PK结束后画面比例变化等。</p><p>从这节课开始，我们会逐步分析作为API用户我们需要了解的FFmpeg中的重要模块，比如AVFormat模块、AVcodec模块、AVfilter模块、swscale模块、swresample模块。</p><p>在具体讲解如何使用FFmpeg的API之前，为了方便你查看API对应的代码，首先我会介绍一下FFmpeg的代码结构目录，我建议你先从<a href=\"https://ffmpeg.org/download.html\">FFmpeg的官方代码库</a>下载一份代码。</p><pre><code class=\"language-plain\">git clone git://source.ffmpeg.org/ffmpeg.git \n</code></pre><p>从目录中可以看到，FFmpeg目录中包含了FFmpeg库代码目录、构建工程目录、自测子系统目录等，具体内容如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/bd/8af20d33c9b1d3144dddf25cbfafe5bd.png?wh=1626x1140\" alt=\"图片\"></p><p>现在你知道FFmpeg的源代码目录中都包含了哪些内容，在之后使用FFmpeg的API做开发遇到问题时，就可以通过查看源代码来了解更多、更详细的内部实现了。</p><!-- [[[read_end]]] --><h2>AVFormat模块</h2><p>从FFmpeg的目录结构中可以看出，<strong>libavformat主要是用来做封装格式处理的模块</strong>，如果不做转码，只做切片或者封装格式转换的话，基本上用AVFormat模块就可以，下面我们来看一下AVFormat模块都有哪些常用接口提供给我们使用。</p><p>avformat_version、avformat_configuration、avformat_license 这三个接口都是用来调试的，确定使用的FFmpeg版本、编译配置信息以及License。因为FFmpeg本身是LGPL的，但是FFmpeg可以引入其他第三方库，比如libfdkaac是nonfree的，就有可能存在专利收费的法律风险。</p><p>如果引入了libx264这样的编码器，FFmpeg会自动切换成GPL的License，这个时候如果你想要基于FFmpeg做定制或者开发，就需要注意GPL的License法律风险，相关情况最好还是咨询一下开源License法律援助律师，尽量避免给自己的项目和公司带来不必要的麻烦。</p><blockquote>\n<p><span class=\"reference\">GPL 是 GNU 公共许可证的缩写。它通常会具有 “传染性”，当某一项目使用了 GPL 下的软件部分的话，那么该项目将被 “感染”变成了 GPL 协议下产品，也就是你需要将其开源和免费。LGPL 是&nbsp;GNU 宽松公共许可证的缩写，它是GPL的一个为主要为类库使用设计的开源协议。和GPL不同，LGPL 允许商业软件通过类库引用方式使用LGPL类库而不需要开源商业软件的代码。——<a href=\"https://my.oschina.net/u/4258525/blog/4542981\">关于开源许可 GPL 与 LGPL</a></span></p>\n</blockquote><h3>AVFormat前处理部分</h3><p>当我们做音视频内容处理的时候，首先接触到的应该是AVFormatContext模块相关的操作，也就是我们这里说的AVFormat部分，但是操作AVFormat的时候，会有一个前处理部分，主要包含网络初始化、模块遍历、申请上下文空间、打开文件，还有分析音视频流等操作。下面我们逐个了解一下AVFormat前处理部分的接口与作用。</p><ul>\n<li>avformat_network_init和avformat_network_deinit两个接口，是网络相关模块的初始化和撤销网络相关模块初始化。</li>\n<li>av_muxer_iterate和av_demuxer_iterate两个接口，是muxer和demuxer的遍历接口，如果你想查找自己需要的muxer或者demuxer是否在当前使用的FFmpeg库中，用这两个接口可以全面地查找。</li>\n<li>avformat_alloc_context和avformat_free_context 两个接口可以用来申请与释放AVFormatContext上下文结构。</li>\n<li>avformat_new_stream 接口用来创建新的AVStream。</li>\n<li>av_stream_add_side_data 接口用来向AVStream中添加新的side data信息，例如视频旋转信息，通常是可以存储在side data里面的。</li>\n<li>av_stream_new_side_data 接口用来申请新的side data。</li>\n<li>av_stream_get_side_data 接口用来获取side data。</li>\n<li>avformat_alloc_output_context2 接口用来申请将要输出的文件的AVFormatContext，可以通过avformat_free_context释放申请的AVFormatContext。</li>\n<li>av_find_input_format 接口可以根据传入的short_name来获得对应的AVFormat模块，例如MP4。</li>\n<li>avformat_open_input 接口主要用处是打开一个AVInputFormat，并挂在AVFormatContext模块上，这个接口里面会调用avformat_alloc_context，可以通过接口avformat_close_input来关闭和释放avformat_open_input里对应的alloc操作。</li>\n<li>av_find_best_stream 接口用来找到多个视频流或多个音频流中最优的那个流。</li>\n<li>avformat_find_stream_info 接口主要用来建立AVStream的信息，获得的信息大多数情况下是比较准确的。使用avformat_find_stream_info接口来获得AVStream信息的话，会比较消耗时间。因为里面需要通过try_decode进行解码操作，来获得更精准的AVStream信息，所以有些固定场景不使用avformat_find_stream_info，是为了节省时间方面的开销。</li>\n</ul><p>我们可以通过probesize、analyzeduration来设置读取的音视频数据的阈值，avformat_find_stream_info里面也会遍历这个阈值，所以通过设置probesize和analyzeduration也可以节省一些时间。</p><p>如果有多个类似AAC或者H264这样的codec的话，avformat_find_stream_info内部会使用最先遍历到的codec，其实我们可以在使用avformat_find_stream_info之前指定解码器，预期的结果会更准确一些。</p><h3>AVFormat读写处理部分</h3><p>看完AVFormat前处理部分的操作，接下来我们进入AVFormat读写处理的部分。</p><ul>\n<li>av_read_frame 接口用来从AVFormatContext中读取AVPacket，AVPacket里面存储的内容在<a href=\"https://time.geekbang.org/column/article/547562\">第6节课</a>的时候已经有讲过，这里就不重复讲解了。</li>\n<li>当拖动进度条的时候，我们可以调用avformat_seek_file<span class=\"reference\">（旧版是av_seek_frame）</span>接口，seek到自己想要指定的位置，但前提是对应的封装格式得支持精确seek，seek支持以下四种模式：</li>\n</ul><pre><code class=\"language-plain\">AVSEEK_FLAG_BACKWARD //往回seek\nAVSEEK_FLAG_BYTE //以字节数的方式seek\nAVSEEK_FLAG_ANY //可seek到任意帧\nAVSEEK_FLAG_FRAME //以帧数量的方式seek\n</code></pre><ul>\n<li>avformat_flush 接口主要是用来清空当前AVFormatContext中的buffer。</li>\n<li>avformat_write_header 接口主要用在“写”操作的开头部分，通常指传输协议的开始，写封装格式头部。avformat_write_header里会调用到avformat_init_output，通常avformat_write_header函数的最后一个参数可以传入Option，Option可以控制容器模块中的Option，关于如何查看封装容器格式的Option参数，我们<a href=\"https://time.geekbang.org/column/article/548420\">第7节课</a>的时候讲过，你可以回顾一下。</li>\n</ul><blockquote>\n<p><span class=\"reference\">写MP4文件有很多Option，可以通过ffmpeg -h muxer=mp4看到生成MP4的一些列参数，也就是Option。——第7节课内容回顾</span></p>\n</blockquote><ul>\n<li>avformat_init_output 接口主要用来做容器格式初始化部分的操作，例如打开文件，或者有一些容器格式内部的信息需要初始化的时候。</li>\n<li>av_interleaved_write_frame 接口支持在写入AVPacket的时候，根据dts时间戳交错写入数据。使用这个接口有一个需要注意的地方，就是数据会先写入到buffer里用来交错存储数据，这个buffer会不断变大，如果有必要的话，可以考虑自己调用avio_flush或者写NULL把buffer写到磁盘。</li>\n</ul><blockquote>\n<p><span class=\"reference\">我们在存储音视频数据的时候，如果是顺序读取音视频数据的话，音视频数据交错存储比较好，因为这样可以给内存、硬盘或者网络节省很多开销。——第3节课内容回顾</span></p>\n</blockquote><ul>\n<li>av_write_frame 接口是不按照交错的形式存储AVPacket，不过在写入文件的时候是直接写入到磁盘，不会有buffer，所以可以考虑自己先做交错再用这个接口，不过我一般选择使用av_interleaved_write_frame，因为比较方便，不需要自己做数据交错排列的操作。</li>\n<li>av_write_trailer 接口是写数据到封装容器的收尾部分。可以关闭和释放在此之前申请的内存，另外，MP4文件如果需要把moov移动到MP4文件头部，也是在这个接口里面完成的。</li>\n</ul><h2>小结</h2><p>FFmpeg中有很多重要的模块，比如AVFormat模块、AVcodec模块、AVfilter模块等。其中AVFormat是用来做封装格式处理的模块。这个模块的内部提供了很多常用的接口，比如前处理部分的avformat_find_stream_info等接口，读写处理部分的avformat_write_header、av_interleaved_write_frame等接口，了解这些接口的用途和可能出现的问题及解决办法，可以让我们在实践中更好地使用它们去做容器封装和解封装方面的操作。</p><p><img src=\"https://static001.geekbang.org/resource/image/5c/73/5c6f88a5324bc15c6a5d3354c6c16a73.png?wh=1920x2045\" alt=\"图片\"></p><p>关于AVFormat模块中API接口更多的使用方式，比如说参数相关的内容，你还需要多看一看avformat.h头文件中的注释和参数说明。如果你还是掌握不住这些接口的使用方式的话，也可以根据我的建议，先把源代码下来，去看一下API里实现的过程来加深理解。</p><h2>思考题</h2><p>我们介绍最后一个接口av_write_trailer的时候，提到它支持把MP4的moov移动到文件的头部，在FFmpeg的命令行参数里面使用的是-movflags faststart，那么如果我用API的话，需要在哪个接口里面传递这个参数呢？</p><p>欢迎在评论区留下你的答案，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"09｜如何使用 FFmpeg 与 Handbrake 做转码？","id":550128},"right":{"article_title":"11 | FFmpeg 基础模块（二）：AVIO、AVDictionary 与 AVOption","id":551890}}},{"article_id":551890,"article_title":"11 | FFmpeg 基础模块（二）：AVIO、AVDictionary 与 AVOption","article_content":"<p>你好，我是刘歧。</p><p><a href=\"https://time.geekbang.org/column/article/551256\">上一节课</a>我们了解了AVFormat中的API接口的功能，从实际操作经验看，这些接口是可以满足大多数音视频的mux与demux，或者说remux场景的。但是除此之外，在日常使用API开发应用的时候，我们还会遇到需要从自己定义的内存或文件中读写数据，然后套用在AVFormat中的场景。遇到这种场景的时候我们应该怎么办呢？使用AVIO就可以做到。</p><h2>AVIO</h2><p>我们先来认识一下AVIO。AVIO部分常见的接口看上去比较多，主要是为了方便读、写内容时做一些字节对齐与大小端定义的操作，了解了它内在的结构之后，你就会觉得清晰多了。下面我们来一一讲解一下。</p><p>当你想知道一个URL字符串是什么协议的时候，通过<strong>avio_find_protocol_name接口</strong>就能得到协议的名称，例如http、rtmp、rtsp等。</p><pre><code class=\"language-plain\">const char *avio_find_protocol_name(const char *url);\n</code></pre><p><strong>avio_alloc_context接口</strong>主要用来申请AVIOContext句柄，并且可以在申请的时候注册read_packet、write_packet与seek回调，然后可以将AVIOContext句柄挂载到AVFormatContext的pb上面。挂载完成后，在操作AVFormatContext的read_packet、write_packet、seek的时候，会调用这里注册过的回调接口，注册的时候如果把回调接口设置成NULL<span class=\"reference\">（空）</span>，就会使用AVIOContext子模块默认的流程。这里申请的AVIOContext可以通过<strong>avio_context_free</strong>来释放。</p><!-- [[[read_end]]] --><pre><code class=\"language-plain\">AVIOContext *avio_alloc_context(\n                  unsigned char *buffer,\n                  int buffer_size,\n                  int write_flag,\n                  void *opaque,\n                  int (*read_packet)(void *opaque, uint8_t *buf, int buf_size),\n                  int (*write_packet)(void *opaque, uint8_t *buf, int buf_size),\n                  int64_t (*seek)(void *opaque, int64_t offset, int whence));\n                  \nvoid avio_context_free(AVIOContext **s);\n</code></pre><p>下面这一系列的读写接口，从名字就可以看出来，其中w是写，r是读，l或者le代表小端方式读写，b或者be代表大端读写，8代表8位，16代表16位，24、32、64分别代表24位、32位和64位。至于是大端读写还是小端读写，你可以根据实际的参考标准的要求进行操作。然后是字符串操作，这个部分也可以区分大小端的读写。</p><pre><code class=\"language-plain\">void avio_w8(AVIOContext *s, int b);\nvoid avio_write(AVIOContext *s, const unsigned char *buf, int size);\nvoid avio_wl64(AVIOContext *s, uint64_t val);\nvoid avio_wb64(AVIOContext *s, uint64_t val);\nvoid avio_wl32(AVIOContext *s, unsigned int val);\nvoid avio_wb32(AVIOContext *s, unsigned int val);\nvoid avio_wl24(AVIOContext *s, unsigned int val);\nvoid avio_wb24(AVIOContext *s, unsigned int val);\nvoid avio_wl16(AVIOContext *s, unsigned int val);\nvoid avio_wb16(AVIOContext *s, unsigned int val);\nint avio_put_str(AVIOContext *s, const char *str);\nint avio_put_str16le(AVIOContext *s, const char *str);\nint avio_put_str16be(AVIOContext *s, const char *str);\n\nint avio_read(AVIOContext *s, unsigned char *buf, int size);\nint avio_read_partial(AVIOContext *s, unsigned char *buf, int size);\nint          avio_r8  (AVIOContext *s);\nunsigned int avio_rl16(AVIOContext *s);\nunsigned int avio_rl24(AVIOContext *s);\nunsigned int avio_rl32(AVIOContext *s);\nuint64_t     avio_rl64(AVIOContext *s);\nunsigned int avio_rb16(AVIOContext *s);\nunsigned int avio_rb24(AVIOContext *s);\nunsigned int avio_rb32(AVIOContext *s);\nuint64_t     avio_rb64(AVIOContext *s);\nint avio_get_str(AVIOContext *pb, int maxlen, char *buf, int buflen);\nint avio_get_str16le(AVIOContext *pb, int maxlen, char *buf, int buflen);\nint avio_get_str16be(AVIOContext *pb, int maxlen, char *buf, int buflen);\n</code></pre><p>当解析部分封装格式的时候，有一些字段暂时不用或者不需要解析，就可以使用<strong>avio_skip、avio_seek</strong>来跳过对应的字节，或者通过avio_seek定位到想去的字节处，如果想要知道文件读写之后当前的文件位置，可以通过<strong>avio_tell</strong>来获得。</p><pre><code class=\"language-plain\">int64_t avio_seek(AVIOContext *s, int64_t offset, int whence);\nint64_t avio_skip(AVIOContext *s, int64_t offset);\nstatic av_always_inline int64_t avio_tell(AVIOContext *s)\n</code></pre><p>AVIOContext句柄文件当前已经写入的内容的大小，可以通过<strong>avio_size</strong>来获得。</p><pre><code class=\"language-plain\">int64_t avio_size(AVIOContext *s);\n</code></pre><p>通过<strong>avio_feof</strong>可以判断当前位置是否是AVIOContext的EOF<span class=\"reference\">（文件末尾）</span>。</p><pre><code class=\"language-plain\">int avio_feof(AVIOContext *s);\n</code></pre><p>如果在操作AVIOContext写内容的时候内存不断增长，可以尝试用<strong>avio_flush</strong>把内容刷到目标文件中去。</p><pre><code class=\"language-plain\">void avio_flush(AVIOContext *s);\n</code></pre><p>当写入文件需要先临时放在内存中，最后按照自己的计划将内容刷到文件中的话，可以考虑使用<strong>avio_open_dyn_buf、avio_get_dyn_buf、avio_close_dyn_buf</strong>来操作。</p><pre><code class=\"language-plain\">int avio_open_dyn_buf(AVIOContext **s);\nint avio_get_dyn_buf(AVIOContext *s, uint8_t **pbuffer);\nint avio_close_dyn_buf(AVIOContext *s, uint8_t **pbuffer);\n</code></pre><p>比如操作HLS直播流的时候，考虑到fragment mp4文件的特殊性，我希望先把文件内容写入到内存中，确保写入的数据拿到音视频包完整的流信息数据，然后生成HLS列表时能够写入准确的流信息内容，我会调用avio_open_dyn_buf、avio_get_dyn_buf、avio_close_dyn_buf来解决问题。<br>\n再比如生成fragment mp4的HLS时，需要有一个fragment mp4的init头内容，这个init头部内容，通常可以用avio_open_dyn_buf、avio_get_dyn_buf、avio_close_dyn_buf来做临时缓存，并且定时刷新到init头中。</p><p><strong>avio_close与avio_closep</strong>几乎相同，用来释放申请的资源，但是在avio_closep里会调用avio_close，并清空AVIOContext句柄内容，然后置空。这样可以确保AVIOContext的操作安全，不会出现use-after-free的问题，所以有时候用avio_closep会更安全一些。</p><pre><code class=\"language-plain\">int avio_close(AVIOContext *s);\nint avio_closep(AVIOContext **s);\n</code></pre><p><strong>avio_open和avio_open2</strong>都是用来打开FFmpeg的输入输出文件的，它们之间的差别是avio_open2可以注册一个AVIOInterruptCB的callback做超时中断处理，而且可以在open的时候设置AVDictionary来操作AVIO目标对象的options。</p><pre><code class=\"language-plain\">int avio_open(AVIOContext **s, const char *url, int flags);\nint avio_open2(AVIOContext **s, const char *url, int flags, const AVIOInterruptCB *int_cb, AVDictionary **options);\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/77/47/773dc943233541714b0113c114118147.png?wh=1920x2084\" alt=\"图片\"></p><p>学完AVIO部分接口的用途和操作方式，就补齐了封装格式操作API方面的拼图。这是我们成为FFmpeg API用户的第一步。但你不要因此觉得成为API用户就可以不用FFmpeg的命令行了。</p><p>其实无论是FFmpeg的命令行还是各种API接口，都可以为我们所用，它们之间并不是割裂的。FFmpeg提供的命令行支持很多参数，这些参数不单单是提供给命令行用户的，API用户也可以使用。那具体API用户应该怎么去使用这些参数呢？</p><p>我们可以通过AVDictionary或者AVOption来设置参数，这两个API系列主要用来设置操作目标的format、codec、protocol的参数，最终达到与命令行使用参数一样的效果。因为AVDictionary和AVOption都是基础操作接口，之后我们学习的操作接口都会涉及参数设置，所以今天我们也详细地了解一下opt和dict的操作方法。</p><h2>AVDictionary 与 AVOption</h2><p>在使用 FFmpeg 命令行做封装、解封装、编解码、网络传输的时候，都会用到一些参数，比如我们录制MP4的时候，希望在录制完成之后把moov移动到文件头部，就需要添加一个参数‐movflags faststart。那么在使用FFmpeg的SDK时，就需要使用dict或opt的操作方式，来将参数传给 FFmpeg内部MP4的muxer模块。</p><p>同样是把moov移动到文件头部，使用dict和使用opt有什么区别呢？下面我用两个例子来说明这个问题。</p><ol>\n<li>通过opt操作设置参数</li>\n</ol><pre><code class=\"language-plain\">AVFormatContext *oc;\navformat_alloc_output_context2(&amp;oc, NULL, NULL, \"out.mp4\");\nav_opt_set(oc‐&gt;priv_data, \"movflags\", \"faststart\", 0); /* 直接设置容器对象的参数 */\navformat_write_header(oc, NULL);\nav_interleaved_write_frame(oc, pkt);\nav_write_trailer(oc);\n</code></pre><ol start=\"2\">\n<li>通过dict操作设置参数。</li>\n</ol><pre><code class=\"language-plain\">AVFormatContext *oc;\nAVDictionary *opt = NULL; /* 先定义一个AVDictionary变量 */\navformat_alloc_output_context2(&amp;oc, NULL, NULL, \"out.mp4\");\nav_dict_set(&amp;opt, \"movflags\", \"faststart\", 0); /* 将参数设置到AVDictionary变量中 */\navformat_write_header(oc, &amp;opt); /* 打开文件时传AVDictionary参数 */\nav_dict_free(&amp;opt); /* 使用完AVDictionary参数后立即释放以防止内存泄露 */\nav_interleaved_write_frame(oc, pkt);\nav_write_trailer(oc);\n</code></pre><p>这两种操作方式都可以将moov容器移动到MP4文件的头部，我们从操作的代码中看到， av_opt_set可以直接设置对应对象的参数，这样使用的话能够直接让设置的参数生效。而 av_dict_set可以把参数设置到AVDictionary变量中，放到AVDictionary里之后，可以复用到多个对象里，但是设置起来会稍微麻烦一些。二者各有优势，你可以通过个人的使用习惯而定。</p><p>除了av_opt_set与av_dict_set之外，opt与dict还有很多的操作接口可以使用，我们可以通过列表来了解一下。</p><ol>\n<li>opt接口列表</li>\n</ol><pre><code class=\"language-plain\">av_opt_set_int 只接受整数\nav_opt_set_double 只接受浮点数\nav_opt_set_q 只接受分子与分母，例如{1, 25}这样\nav_opt_set_bin 只接受二进制数据\nav_opt_set_image_size 只接受图像宽与高，例如1920，1080这样\nav_opt_set_video_rate 只接受分子与分母，例如{1, 25}这样\nav_opt_set_pixel_fmt 只接受枚举类型，例如AV_PIX_FMT_YUV420P\nav_opt_set_sample_fmt 只接受采样数据格式枚举类型，例如AV_SAMPLE_FMT_S16\nav_opt_set_channel_layout 只接受音频通道布局枚举类型，例如AV_CHANNEL_LAYOUT_5POINT0\nav_opt_set_dict_val 接受AVDictionary类型，例如设置metadata时候可以使用\nav_opt_set_chlayout 只接受音频通道布局枚举类型，例如AV_CHANNEL_LAYOUT_5POINT0\nav_opt_set_defaults 设置对象的默认值，例如hlsenc有自己对应的操作选项的默认值，全部设置对应的默认值\nav_opt_set_defaults2 设置对象的默认值，例如hlsenc有自己对应的操作选项的默认值，全部设置对应的默认值\nav_opt_set_from_string 解析key=value格式的字符串并设置对应的参数与值\n\nav_opt_next 获得opt操作的对象的下一个参数\nav_opt_get_int 获得对象参数的值为整数\nav_opt_get_double 获得对象参数的值为双精度浮点数\nav_opt_get_q 获得对象参数为分子分母数，例如{1, 25}这样\nav_opt_get_image_size 获得图像的宽和高，例如1920，1080这样\nav_opt_get_video_rate 获得视频的帧率，例如{1, 25}这样\nav_opt_get_pixel_fmt 获得视频的像素点格式枚举类型，例如AV_PIX_FMT_YUV420P\nav_opt_get_sample_fmt 获得音频的采样格式枚举类型，例如AV_SAMPLE_FMT_S16\nav_opt_get_channel_layout 获得音频的采样布局枚举类型，例如AV_CHANNEL_LAYOUT_5POINT0\nav_opt_get_dict_val 获得AVDictionary类型，通常是key-value方式\nav_opt_get_key_value 获得key=value类型\n</code></pre><p>使用opt中的这些接口进行操作时，可以精确地设置到参数值的类型，直接操作对象，比如某个封装格式模块、某个编解码模块，非常方便。</p><ol start=\"2\">\n<li>dirt接口列表</li>\n</ol><pre><code class=\"language-plain\">av_dict_count 获得dict参数的数量整数\nav_dict_parse_string 一次性解析多组key=value格式的字符串为dict\nav_dict_free 释放因设置dict申请的内存空间\nav_dict_copy 复制dict参数与值\nav_dict_get_string 获得dict的参数值为字符串，用key=value格式字符串获得到value\nav_dict_set_int 设置dict参数的值为整数\n</code></pre><p>和opt相比，dict的操作接口比较少，给人感觉比较简单。但是注意，使用dict这些接口操作对象后，通常只是设置了AVDictionary，并没有真正地设置具体对象。如果想让设置的参数生效，还需要在做封装格式open或编解码器open的时候，设置AVDictionary，并且需要仔细斟酌内存使用情况，通常需要自己调用av_dict_free做内存释放。</p><p>在日常使用API进行开发的时候，你可以使用opt与dict相关的接口，高效地设置对应的参数。当然想要获得这项能力，还需要你勤加练习。</p><h2>小结</h2><p><img src=\"https://static001.geekbang.org/resource/image/2d/77/2debcce3ce9a8360bdd31cf90a0dcd77.png?wh=1920x1348\" alt=\"图片\"></p><p>关于封装格式的API，除了<a href=\"https://time.geekbang.org/column/article/551256\">上节课</a>我们学习的AVFormat模块之外，还有AVIO，它主要应用于在内存中直接操作数据的场景中。</p><p>AVIO中包含很多常用的接口，比如用来查看协议名称的avio_find_protocol_name接口、用来申请AVIOContext句柄的avio_alloc_context接口，还有一系列的读写接口等。AVIO操作接口和我们标准文件的操作接口基本相似，可以在申请之后与FFmpeg的AVFormatContext的pb挂载，这样方便进入FFmpeg的AVFormat操作的内部流程。</p><p>除此之外，我们也应该合理利用FFmpeg命令行支持的参数，学会使用opt与dict相关的API操作，灵活调用FFmpeg命令行支持的参数，为我们使用API开发应用提供更强大的能力。</p><h2>思考题</h2><p>最后，我们来思考一个问题，在AVFormat模块中可以看到频繁出现的一个参数AVPacket，这个AVPacket属于AVFormat还是AVCodec呢？ 欢迎你在评论区留下你的答案，也欢迎你把这节课分享给感兴趣的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"10 | FFmpeg 基础模块（一）：容器相关的 API 操作","id":551256},"right":{"article_title":"12 | FFmpeg基础模块（三）：AVCodec","id":553087}}},{"article_id":553087,"article_title":"12 | FFmpeg基础模块（三）：AVCodec","article_content":"<p>你好，我是刘歧。</p><p>前面两节课我们学习了AVFormat、AVIO、dict和opt操作接口，做容器格式封装与解封装问题不大，但是如果要涉及音视频的编解码的话，就需要用到AVCodec部分的接口了。</p><p>AVCodec是存储编解码器信息的结构体，当我们使用编解码器的时候会用到AVCodec，而FFmpeg除了AVCodec结构体之外，还有一个AVCodecContext，是FFmpeg内部流程中处理编解码时，用来记录和存储上下文的结构体。关于AVCodecContext这个结构体的参数，如果你学习<a href=\"https://time.geekbang.org/column/article/548420\">第7节课FFmpeg常用参数</a>的时候，仔细阅读过帮助信息的话，那AVCodecContext这个结构体对你来说应该很好理解。</p><h2>AVCodec 接口</h2><p>在使用FFmpeg的编解码器之前，首先需要找到编解码器。</p><pre><code class=\"language-plain\">const AVCodec *avcodec_find_decoder(enum AVCodecID id);\nconst AVCodec *avcodec_find_decoder_by_name(const char *name);\nconst AVCodec *avcodec_find_encoder(enum AVCodecID id);\nconst AVCodec *avcodec_find_encoder_by_name(const char *name);\n</code></pre><!-- [[[read_end]]] --><p>如代码所示，找到编码器和解码器有两种方式，一种是通过AVCodecID来查找，一种是通过字符串来查找，字符串就是编码器或解码器的名称，例如libx264。</p><p>这里需要注意的是，如果编码器和解码器的find接口使用得没有问题，用avcodec_find_decoder查找编码器的话，在这里可能能找到AVCodec，但是在后续用来做编码的时候会报错。</p><p>通常我们也可以在做编码操作之前，调用接口av_codec_is_encoder来确认当前拿到的AVCodec是不是编码器，或者通过av_codec_is_decoder来确认是不是解码器。</p><p>找到AVCodec之后，最好不要直接使用，推荐的做法是与FFmpeg内部流程中的AVCodecContext建立关联。</p><pre><code class=\"language-plain\">AVCodecContext *avcodec_alloc_context3(const AVCodec *codec);\nvoid avcodec_free_context(AVCodecContext **avctx);\n</code></pre><p>从示例代码中可以看到，AVCodec与AVCodecContext可以通过avcodec_alloc_context3接口来申请并建立关联，因为涉及内存申请操作，所以用完之后需要使用avcodec_free_context释放资源。</p><p>申请完AVCodecContext上下文之后，接下来可以打开编码器或者解码器了。</p><pre><code class=\"language-plain\">int avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options);\n</code></pre><p>你应该已经发现了，这个avcodec_open2有三个参数，第一个是AVCodecContext，它是处理编解码时，用来记录和存储上下文的结构体，第三个参数是AVDictionary，这个参数用来设置AVCodec编码器或者解码器内部的参数，可以使用ffmpeg -h encoder=libx264查看libx264的内部可设置的参数，AVDictionary和AVOption的设置方式，我们在上一节课已经讲过了，这里就不过多介绍了。</p><p>好了，我们该说道说道这第二个参数了，前面我们不是已经在avcodec_alloc_context3将AVCodec与AVCodecContext建立过关联了吗，这里怎么还需要传递一个AVCodec呢？你想得没错，这里可以不传递了，设置为NULL就可以了。如果想要关闭编码器，推荐你使用avcodec_free_context来做一次释放，这样比较干净，因为avcodec_free_context里面已经有avcodec_close操作了。</p><h3>编码和解码的操作接口</h3><p>好了，说完编解码的前置操作，接下来进入正题，我们看一看编码和解码的操作接口。</p><pre><code class=\"language-plain\">int avcodec_send_packet(AVCodecContext *avctx, const AVPacket *avpkt);\nint avcodec_receive_frame(AVCodecContext *avctx, AVFrame *frame);\nint avcodec_send_frame(AVCodecContext *avctx, const AVFrame *frame);\nint avcodec_receive_packet(AVCodecContext *avctx, AVPacket *avpkt);\n</code></pre><p>这是两组接口，avcodec_send_packet与avcodec_receive_frame是用来做解码的组合，avcodec_send_frame与avcodec_receive_packet是用来做编码的组合。</p><p>大多数场景下，可以调用一次avcodec_send_packet，将AVPacket送到解码器里，然后avcodec_receive_frame读取一次AVFrame，但是稳妥起见，avcodec_receive_frame有时候会返回EAGAIN，所以我们还需要确认读全了AVframe，再做avcodec_receive_frame操作。</p><p>FFmpeg旧版本其实是用avcodec_decode_video2和avcodec_decode_audio4来做的音视频的解码，从2016年04月21日开始，FFmpeg新增了avcodec_send_packet和avcodec_receive_frame这样的组合解码与组合编码接口，主要是为了解决一个AVPacket中包含多个视频帧或者音频包的情况。</p><p>如果解码结束，给avcodec_send_packet写一个NULL的AVPacket包就可以了。编码的话，给avcodec_send_frame设置AVFrame为NULL就表示编码结束了。</p><h2>关键参数AVPacket</h2><p>在AVFormat和AVCodec之间有一个关键的参数，就是我们这几节课频繁见到的AVPacket。AVPacket的内容构建也有一系列的接口需要我们了解，构造AVPacket内容的时候用这些接口会非常方便，下面我来介绍一下。</p><p>如果你想使用AVPacket的话，可以通过av_packet_alloc来申请一个AVPacket。</p><pre><code class=\"language-plain\">AVPacket *av_packet_alloc(void);\n</code></pre><p>但这次申请的只是一个AVPacket的内存空间，里面的buf和data的内存空间不会被申请。如果想要申请buf和data的空间的话，可以考虑在av_packet_alloc之后使用av_new_packet来解决。</p><pre><code class=\"language-plain\">int av_new_packet(AVPacket *pkt, int size)\n</code></pre><p>当使用av_new_packet申请带buf和data的AVPacket的时候，需要给av_new_packet传递一个要申请的buf空间大小的值。</p><p>通过av_packet_alloc申请的AVPacket需要用av_packet_free来释放申请的内存空间。当然，av_new_packet申请的buf在av_packet_free里也会一并释放。</p><p>这个时候你可能会有个疑问。诶？不对啊，我如果按照AVIOContext的操作方式，自己从内存中读到一段数据，想挂到AVPacket做解码，这个时候如果用av_new_packet申请内存是不是不太对？你想的是对的，这个时候可以不用av_new_packet来申请buf或者data的内存空间，但是前面av_packet_alloc还是需要的，只是这里的buf或者data如果想要指向第三方data内存区域的话，最好还是使用av_packet_from_data。</p><pre><code class=\"language-plain\">int av_packet_from_data(AVPacket *pkt, uint8_t *data, int size);\n</code></pre><p>为什么推荐使用av_packet_from_data做data挂载，而不是直接把AVPacket的data、buf指到我们自己读到的data内存空间呢？</p><p>其实主要是因为你在使用FFmpeg的API，所以最好还是用FFmpeg提供的接口走FFmpeg自己内部的流程。并不是说不能自己手动处理，而是为了避免很多不必要的问题不这样做，比如你把data指向你自己申请的内存空间，那么很有可能会缺少data指向buf，然后那个buf是有PADDING空间预留的。</p><pre><code class=\"language-plain\">    pkt-&gt;buf = av_buffer_create(data, size + AV_INPUT_BUFFER_PADDING_SIZE,\n                                av_buffer_default_free, NULL, 0);\n</code></pre><p>这个AV_INPUT_BUFFER_PADDING_SIZE能解决很重要的问题，尤其是在后续做packet里面的data分析的时候，可能会出现crash。因为FFmpeg内部的parser在解析data的时候做了一些优化，但是会有一些额外的开销，FFmpeg的codec模块会预读一段数据，这个时候可能会因为内存越界出现crash错误。</p><pre><code class=\"language-plain\">/**\n * @ingroup lavc_decoding\n * Required number of additionally allocated bytes at the end of the input bitstream for decoding.\n * This is mainly needed because some optimized bitstream readers read\n * 32 or 64 bit at once and could read over the end.&lt;br&gt;\n * Note: If the first 23 bits of the additional bytes are not 0, then damaged\n * MPEG bitstreams could cause overread and segfault.\n */\n#define AV_INPUT_BUFFER_PADDING_SIZE 64\n</code></pre><p>说到对音视频流做parser，我们可以大概了解一下。</p><pre><code class=\"language-plain\">AVCodecParserContext *av_parser_init(int codec_id);\nint av_parser_parse2(AVCodecParserContext *s,\n                     AVCodecContext *avctx,\n                     uint8_t **poutbuf, int *poutbuf_size,\n                     const uint8_t *buf, int buf_size,\n                     int64_t pts, int64_t dts,\n                     int64_t pos);\n\nvoid av_parser_close(AVCodecParserContext *s);\n</code></pre><p>有些音视频的编码数据，是会把一部分数据相关的头信息存储在AVPacket的data中的，这个时候可以使用parser来做解析，获得相关的codec信息。如果你想要知道哪些codec有parser的话，可以在编译FFmpeg代码那一步就通过./configure --list-parsers来查看。</p><p>比如说H.264的数据，可以通过parser来得到编码数据的NALUnit信息，我们在<a href=\"https://time.geekbang.org/column/article/547562\">第6节课</a>的时候讲过ffprobe -show_frames可以看到音视频流的frames信息，parser解析出来有一些信息是在这个frames里面展示出来的。这些信息也主要用来传给解码器作为解码处理的一个参考。</p><h2>小结</h2><p>我们来回顾一下这节课我们都学到了哪些内容。</p><ul>\n<li>AVCodec中编解码相关的API接口：avcodec_send_packet与avcodec_receive_frame、avcodec_send_frame与avcodec_receive_packet，两组用来做编解码的组合接口。</li>\n<li>关键参数AVPacket，贯穿Codec与Format模块的始终，无论是处理已有的内存数据，还是按照FFmpeg内部框架流程建立的数据，都可以应对自如。</li>\n</ul><p>Codec和Format之间还有更多的可操作系统的方法，你可以参考FFmpeg提供的例子加深理解。还记得<a href=\"https://time.geekbang.org/column/article/551256\">第10节课</a>我推荐你下载FFmpeg的源代码吗？下载源代码以后，你可以在源代码目录的doc/examples目录下看到更全面的FFmpeg的API用例。</p><h2>思考题</h2><p>如果解码以后我想给视频添加一些特效，在AVCodec操作以后应该使用哪个结构体里面的内容呢？欢迎你在评论区分享你的想法，也欢迎你把这节课分享给对音视频感兴趣的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"11 | FFmpeg 基础模块（二）：AVIO、AVDictionary 与 AVOption","id":551890},"right":{"article_title":"13 | FFmpeg 有哪些常见的应用场景？","id":554456}}},{"article_id":554456,"article_title":"13 | FFmpeg 有哪些常见的应用场景？","article_content":"<p>你好，我是刘歧。</p><p>FFmpeg API 应用部分的前两节课，我们了解了AVFormat、AVCodec以及常用的操作接口，但是现在这些知识还是“各忙各的”的状态，好像没有真正地把图像与封装格式、传输协议给串起来，形成一个完整的音视频图形图像处理的链条，可能你都没空看FFmpeg源代码目录里面提供的例子。</p><p>别急，这节课我们就一起来看一看FFmpeg源代码里面的例子，主要是不转码只转封装、转码转封装和直播推流三个场景，通过分析这三个场景案例，加深一下你对API使用的理解。</p><h2>Remuxing</h2><p>在使用FFmpeg的API做开发之前，我们先来梳理一下想要做Remuxing的话都需要用到哪些结构体与模块，看一下基本的流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/a4/d7bac673dcbfc6949d9abae5ac6bc9a4.png?wh=1310x906\" alt=\"图片\"></p><ol>\n<li>打开输入文件和打开输出文件，我们可以理解为初始化操作。</li>\n<li>从输入文件中读取音视频数据包，将音视频数据包写入输出文件，我们可以把它理解为一个循环操作，直到遇到结束相关的操作信息才停止。</li>\n<li>关闭输出文件和输入文件，我们可以理解为收尾操作。</li>\n</ol><p>下面，我们逐步剖析一下。</p><p>初始化操作部分的代码大概会使用这些函数。</p><p>使用avformat_open_input、avformat_find_stream_info来打开输入文件，并根据输入文件中的音视频流信息建立音视频流，也就是AVStreams。</p><!-- [[[read_end]]] --><p>使用avformat_alloc_output_context2、avformat_new_stream和avformat_write_header 来打开输出文件，并建立音视频流，输出文件会用到AVOutputFormat，并建立封装格式操作的AVFormatContext，作为操作上下文的结构体，并且会尝试写入输出文件的封装格式头部信息。</p><p>从输入文件中读取音视频数据包，将音视频数据包写入输出文件会使用av_read_frame 函数，从输入文件中读取AVPacket音视频数据包，还会使用av_interleaved_write_frame函数，将读取到的音视频数据包写入输出文件。</p><p>然后是关闭输出文件和输入文件，使用av_write_trailer函数，在关闭输出文件之前做写封装收尾工作。使用avformat_free_context函数关闭输出文件，并释放因操作输出文件封装格式申请的资源。最后使用avformat_close_input关闭输入文件并释放相关的资源。</p><p>当然，除了以上这些操作之外，还有一些API是我们可以根据自己的需要使用的。详细的内容你可以看一下<a href=\"https://ffmpeg.org/doxygen/trunk/remuxing_8c-example.html\">示例代码</a>。其实在日常操作时，<strong>做remux主要还是用于收录一些音视频内容的场景中，用得更多的还是编码或者转码的操作。</strong>因为音视频的编码数据格式比较多，需要统一转成相同的编码，换句话说，就是将输入的音视频内容转成统一规格输出的场景，比收录场景更常见，下面我们来看一下转码场景的代码用例。</p><h2>Transcoding</h2><p>转码操作与转封装操作类似，就是多了解码和编码步骤，并且大多数情况下需要自己制定输出的编码参数与编码规格。和之前一样，我们先梳理一下转码的流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/46/a1051608199e8a3e8fc860b884f1c146.png?wh=1808x1234\" alt=\"图片\"></p><p>打开文件的操作，可以定义为open_input_file，这样将输入文件操作相关的代码放在一个函数里面比较清晰。</p><pre><code class=\"language-plain\">static int open_input_file(const char *filename)\n{\n    int ret;\n    unsigned int i;\n\n    ifmt_ctx = NULL;\n    if ((ret = avformat_open_input(&amp;ifmt_ctx, filename, NULL, NULL)) &lt; 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Cannot open input file\\n\");\n        return ret;\n    }\n\n    if ((ret = avformat_find_stream_info(ifmt_ctx, NULL)) &lt; 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Cannot find stream information\\n\");\n        return ret;\n    }\n\n    stream_ctx = av_calloc(ifmt_ctx-&gt;nb_streams, sizeof(*stream_ctx));\n    if (!stream_ctx)\n        return AVERROR(ENOMEM);\n\n    for (i = 0; i &lt; ifmt_ctx-&gt;nb_streams; i++) {\n        AVStream *stream = ifmt_ctx-&gt;streams[i];\n        const AVCodec *dec = avcodec_find_decoder(stream-&gt;codecpar-&gt;codec_id);\n        AVCodecContext *codec_ctx;\n        if (!dec) {\n            av_log(NULL, AV_LOG_ERROR, \"Failed to find decoder for stream #%u\\n\", i);\n            return AVERROR_DECODER_NOT_FOUND;\n        }\n        codec_ctx = avcodec_alloc_context3(dec);\n        if (!codec_ctx) {\n            av_log(NULL, AV_LOG_ERROR, \"Failed to allocate the decoder context for stream #%u\\n\", i);\n            return AVERROR(ENOMEM);\n        }\n        ret = avcodec_parameters_to_context(codec_ctx, stream-&gt;codecpar);\n        if (ret &lt; 0) {\n            av_log(NULL, AV_LOG_ERROR, \"Failed to copy decoder parameters to input decoder context \"\n                   \"for stream #%u\\n\", i);\n            return ret;\n        }\n        /* Reencode video &amp; audio and remux subtitles etc. */\n        if (codec_ctx-&gt;codec_type == AVMEDIA_TYPE_VIDEO\n                || codec_ctx-&gt;codec_type == AVMEDIA_TYPE_AUDIO) {\n            if (codec_ctx-&gt;codec_type == AVMEDIA_TYPE_VIDEO)\n                codec_ctx-&gt;framerate = av_guess_frame_rate(ifmt_ctx, stream, NULL);\n            /* Open decoder */\n            ret = avcodec_open2(codec_ctx, dec, NULL);\n            if (ret &lt; 0) {\n                av_log(NULL, AV_LOG_ERROR, \"Failed to open decoder for stream #%u\\n\", i);\n                return ret;\n            }\n        }\n        stream_ctx[i].dec_ctx = codec_ctx;\n\n        stream_ctx[i].dec_frame = av_frame_alloc();\n        if (!stream_ctx[i].dec_frame)\n            return AVERROR(ENOMEM);\n    }\n\n    av_dump_format(ifmt_ctx, 0, filename, 0);\n    return 0;\n}\n</code></pre><p>从代码中可以看到，除了在Remuxing中见过的函数之外，这里还使用了avcodec_find_decoder，通过CODECID查找要使用的解码器，当然，这里如果自己已经确定好音视频流是什么编码的话，也可以通过avcodec_find_decoder_by_name来指定解码器的名字。</p><p>然后用avcodec_alloc_context3申请AVCodecContext上下文，用avcodec_parameters_to_context将解析到的AVStream流信息中的AVCodecParameter复制到AVCodecContext对应字段中，方便后面解码的时候用。</p><p>使用avcodec_open2打开解码器，因为没有需要传的option，所以这里的avcodec_open2的option字段设置的是NULL，然后申请一个AVFrame，用来存储解码后的AVFrame数据。</p><p>然后是打开输出文件，这部分的代码也可以封装到一个函数里面，整体看上去会清晰很多。</p><pre><code class=\"language-plain\">static int open_output_file(const char *filename)\n{\n    AVStream *out_stream;\n    AVStream *in_stream;\n    AVCodecContext *dec_ctx, *enc_ctx;\n    const AVCodec *encoder;\n    int ret;\n    unsigned int i;\n\n    ofmt_ctx = NULL;\n    avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, NULL, filename);\n    if (!ofmt_ctx) {\n        av_log(NULL, AV_LOG_ERROR, \"Could not create output context\\n\");\n        return AVERROR_UNKNOWN;\n    }\n\n\n\n    for (i = 0; i &lt; ifmt_ctx-&gt;nb_streams; i++) {\n        out_stream = avformat_new_stream(ofmt_ctx, NULL);\n        if (!out_stream) {\n            av_log(NULL, AV_LOG_ERROR, \"Failed allocating output stream\\n\");\n            return AVERROR_UNKNOWN;\n        }\n\n        in_stream = ifmt_ctx-&gt;streams[i];\n        dec_ctx = stream_ctx[i].dec_ctx;\n\n        if (dec_ctx-&gt;codec_type == AVMEDIA_TYPE_VIDEO\n                || dec_ctx-&gt;codec_type == AVMEDIA_TYPE_AUDIO) {\n            /* in this example, we choose transcoding to same codec */\n            encoder = avcodec_find_encoder(dec_ctx-&gt;codec_id);\n            if (!encoder) {\n                av_log(NULL, AV_LOG_FATAL, \"Necessary encoder not found\\n\");\n                return AVERROR_INVALIDDATA;\n            }\n            enc_ctx = avcodec_alloc_context3(encoder);\n            if (!enc_ctx) {\n                av_log(NULL, AV_LOG_FATAL, \"Failed to allocate the encoder context\\n\");\n                return AVERROR(ENOMEM);\n            }\n\n            /* In this example, we transcode to same properties (picture size,\n             * sample rate etc.). These properties can be changed for output\n             * streams easily using filters */\n            if (dec_ctx-&gt;codec_type == AVMEDIA_TYPE_VIDEO) {\n                enc_ctx-&gt;height = dec_ctx-&gt;height;\n                enc_ctx-&gt;width = dec_ctx-&gt;width;\n                enc_ctx-&gt;sample_aspect_ratio = dec_ctx-&gt;sample_aspect_ratio;\n                /* take first format from list of supported formats */\n                if (encoder-&gt;pix_fmts)\n                    enc_ctx-&gt;pix_fmt = encoder-&gt;pix_fmts[0];\n                else\n                    enc_ctx-&gt;pix_fmt = dec_ctx-&gt;pix_fmt;\n                /* video time_base can be set to whatever is handy and supported by encoder */\n                enc_ctx-&gt;time_base = av_inv_q(dec_ctx-&gt;framerate);\n            } else {\n                enc_ctx-&gt;sample_rate = dec_ctx-&gt;sample_rate;\n                ret = av_channel_layout_copy(&amp;enc_ctx-&gt;ch_layout, &amp;dec_ctx-&gt;ch_layout);\n                if (ret &lt; 0)\n                    return ret;\n                /* take first format from list of supported formats */\n                enc_ctx-&gt;sample_fmt = encoder-&gt;sample_fmts[0];\n                enc_ctx-&gt;time_base = (AVRational){1, enc_ctx-&gt;sample_rate};\n            }\n\n            if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER)\n                enc_ctx-&gt;flags |= AV_CODEC_FLAG_GLOBAL_HEADER;\n\n            /* Third parameter can be used to pass settings to encoder */\n            ret = avcodec_open2(enc_ctx, encoder, NULL);\n            if (ret &lt; 0) {\n                av_log(NULL, AV_LOG_ERROR, \"Cannot open video encoder for stream #%u\\n\", i);\n                return ret;\n            }\n            ret = avcodec_parameters_from_context(out_stream-&gt;codecpar, enc_ctx);\n            if (ret &lt; 0) {\n                av_log(NULL, AV_LOG_ERROR, \"Failed to copy encoder parameters to output stream #%u\\n\", i);\n                return ret;\n            }\n\n            out_stream-&gt;time_base = enc_ctx-&gt;time_base;\n            stream_ctx[i].enc_ctx = enc_ctx;\n        } else if (dec_ctx-&gt;codec_type == AVMEDIA_TYPE_UNKNOWN) {\n            av_log(NULL, AV_LOG_FATAL, \"Elementary stream #%d is of unknown type, cannot proceed\\n\", i);\n            return AVERROR_INVALIDDATA;\n        } else {\n            /* if this stream must be remuxed */\n            ret = avcodec_parameters_copy(out_stream-&gt;codecpar, in_stream-&gt;codecpar);\n            if (ret &lt; 0) {\n                av_log(NULL, AV_LOG_ERROR, \"Copying parameters for stream #%u failed\\n\", i);\n                return ret;\n            }\n            out_stream-&gt;time_base = in_stream-&gt;time_base;\n        }\n\n    }\n    av_dump_format(ofmt_ctx, 0, filename, 1);\n\n    if (!(ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_NOFILE)) {\n        ret = avio_open(&amp;ofmt_ctx-&gt;pb, filename, AVIO_FLAG_WRITE);\n        if (ret &lt; 0) {\n            av_log(NULL, AV_LOG_ERROR, \"Could not open output file '%s'\", filename);\n            return ret;\n        }\n    }\n\n    /* init muxer, write output file header */\n    ret = avformat_write_header(ofmt_ctx, NULL);\n    if (ret &lt; 0) {\n        av_log(NULL, AV_LOG_ERROR, \"Error occurred when opening output file\\n\");\n        return ret;\n    }\n\n    return 0;\n}\n</code></pre><p>输出部分的话，需要设置的不是解码器，而是编码器，所以调用的是avcodec_find_encoder，然后设置编码器的AVCodecContext上下文，打开编码器。</p><p>因为是例子，所以这里大多数参数是从输入的AVStream的AVCodecParameter和AVCodecContext里面拿到的，相当于复制到输出的AVStream里面了。然后通过avio_open打开输出的AVFormatContext的pb句柄，也就是AVIOContext。然后在初始化操作的时候，写一下输出文件的文件头。</p><p>接着，设置原始数据操作相关的滤镜初始化，例如调色、调音色、放大、缩小等操作，这些操作可以在自己拿到解码后的数据后，用OpenGL等强大的库来完成，这里就不展开介绍了。</p><p>接下来就是循环操作：拿到AVPacket、解码、取原始数据、编码，再拿到AVPacket、再解码、再取原始数据、再编码，直到遇到退出相关的条件为止。</p><pre><code class=\"language-plain\">static int encode_write_frame(unsigned int stream_index, int flush)\n{\n    StreamContext *stream = &amp;stream_ctx[stream_index];\n    FilteringContext *filter = &amp;filter_ctx[stream_index];\n    AVFrame *filt_frame = flush ? NULL : filter-&gt;filtered_frame;\n    AVPacket *enc_pkt = filter-&gt;enc_pkt;\n    int ret;\n\n    av_log(NULL, AV_LOG_INFO, \"Encoding frame\\n\");\n    /* encode filtered frame */\n    av_packet_unref(enc_pkt);\n\n    ret = avcodec_send_frame(stream-&gt;enc_ctx, filt_frame);\n\n    if (ret &lt; 0)\n        return ret;\n\n    while (ret &gt;= 0) {\n        ret = avcodec_receive_packet(stream-&gt;enc_ctx, enc_pkt);\n\n        if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)\n            return 0;\n\n        /* prepare packet for muxing */\n        enc_pkt-&gt;stream_index = stream_index;\n        av_packet_rescale_ts(enc_pkt,\n                             stream-&gt;enc_ctx-&gt;time_base,\n                             ofmt_ctx-&gt;streams[stream_index]-&gt;time_base);\n\n        av_log(NULL, AV_LOG_DEBUG, \"Muxing frame\\n\");\n        /* mux encoded frame */\n        ret = av_interleaved_write_frame(ofmt_ctx, enc_pkt);\n    }\n\n    return ret;\n}\n\n\n\nwhile (1) {\n        if ((ret = av_read_frame(ifmt_ctx, packet)) &lt; 0)\n            break;\n        stream_index = packet-&gt;stream_index;\n        av_log(NULL, AV_LOG_DEBUG, \"Demuxer gave frame of stream_index %u\\n\",\n                stream_index);\n\n        if (filter_ctx[stream_index].filter_graph) {\n            StreamContext *stream = &amp;stream_ctx[stream_index];\n\n            av_packet_rescale_ts(packet,\n                                 ifmt_ctx-&gt;streams[stream_index]-&gt;time_base,\n                                 stream-&gt;dec_ctx-&gt;time_base);\n            ret = avcodec_send_packet(stream-&gt;dec_ctx, packet);\n            if (ret &lt; 0) {\n                av_log(NULL, AV_LOG_ERROR, \"Decoding failed\\n\");\n                break;\n            }\n\n            while (ret &gt;= 0) {\n                ret = avcodec_receive_frame(stream-&gt;dec_ctx, stream-&gt;dec_frame);\n                if (ret == AVERROR_EOF || ret == AVERROR(EAGAIN))\n                    break;\n                else if (ret &lt; 0)\n                    goto end;\n\n                stream-&gt;dec_frame-&gt;pts = stream-&gt;dec_frame-&gt;best_effort_timestamp;\n                ret = encode_write_frame(stream-&gt;dec_frame, stream_index);\n                if (ret &lt; 0)\n                    goto end;\n            }\n        }\n        av_packet_unref(packet);\n    }\n</code></pre><p>从代码里可以看到，通过av_read_frame循环读取AVPacket，然后调用avcodec_send_packet将AVPacket发送给解码器做解码，通过avcodec_receive_frame拿到解码后的AVFrame数据，然后通过编码器给AVFrame的数据编码，再写到输出文件里。这时候，写编码后的AVPacket数据用的是交错的方式。最后千万别忘了收尾工作，不然内存就泄露了。</p><p>关于转码相关的操作也有代码示例，通过访问<a href=\"https://ffmpeg.org/doxygen/trunk/transcoding_8c-example.html\">官方文档转码示例</a>就可以看到对应代码的完整版。到这里，Remuxing、Transcoding都介绍完了，如果想试试RTMP推流的话，我们可以继续看一下Muxing的例子，自己造一个数据，编码之后推流。</p><h2>推流</h2><p>因为前面两个例子涵盖了API的大部分接口了，其实做推流的话也比较简单，可以任选Remuxing或者Transcoding里的任何一个例子。</p><p>设置输出文件的时候，有一个avformat_alloc_output_context2操作，从我们Remuxing例子中可以看到，最后一个字段是输出文件名，这里可以改成RTMP的URL地址，关于URL的地址，我们<a href=\"https://time.geekbang.org/column/article/546485\">第5节课</a>讲过，这里就不再介绍了。</p><p>有一点需要注意，因为是推RTMP的直播流，所以输出格式要设置成FLV，否则会报错，报错内容是这样的：</p><pre><code class=\"language-plain\">[NULL @ 0x3dc1900] Unable to find a suitable output format for 'rtmp://127.0.0.1/live/stream'\n</code></pre><p>它提示找不到适合这个URL的输出格式，也就是说，我们需要指定输出格式，RTMP对应的输出格式是FLV，所以最后avformat_alloc_output_context2的第一个参数是输出的AVFormatContext，第二个参数可以设置成NULL交给FFmpeg自动查找，第三个参数设置为\"flv\"字符串，第四个参数就是我们输出的URL地址。</p><p>为了控制节奏，我们可以在循环av_read_frame操作的时候，在av_read_frame的下一句加上usleep(40000)来控制节奏，也就是sleep 40毫秒。最后，别忘了在头文件声明部分加上#include &lt;unistd.h&gt;，不然编译会报错。</p><h2>小结</h2><p>最后，我们来复习一下这节课的主要内容。</p><p>这节课，我们通过不转码只转封装、转码转封装和直播推流三个场景案例，详细地了解了FFmpeg中API的使用方法。现在我们可以在视频内容收录的场景中，使用API来自己独立实现收录音视频内容。如果需要将音视频文件或者直播流转一下码再输出的话，也可以通过转码的例子来完成。</p><p>灵活运用今天学到的这三个例子，基本上就可以满足大部分的场景了。当然，这些还是远远不够的，就像上节课说的，还是要从FFmpeg源代码的doc/examples目录下提供的例子入手，多改、多看、多学习，必要的话，以examples代码为入口跟踪代码进行学习，你会对FFmpeg的API有更深刻的理解的。</p><h2>思考题</h2><p>讲了这么多，你可以思考这么一个问题，如果想通过Remuxing将一个视频编码是H.264，音频编码是AAC编码的直播流，收录为MP4文件，并且MP4文件是moov在mdat的前面的话，该怎么改Remuxing这段代码呢？</p>","neighbors":{"left":{"article_title":"12 | FFmpeg基础模块（三）：AVCodec","id":553087},"right":{"article_title":"14｜如何在FFmpeg中定制一个自己专属的模块？","id":556024}}},{"article_id":556024,"article_title":"14｜如何在FFmpeg中定制一个自己专属的模块？","article_content":"<p>你好，我是刘歧。</p><p>通过前面13节课的学习，我们对FFmpeg整体的使用和架构已经有了一定的了解。接下来，我们一起来探索一下FFmpeg社区的“玩法”，了解一下FFmpeg常用的交流工具、反馈bug和贡献代码的渠道，以及定制专属板块的方法。这个部分，我会分成两讲给你介绍。这节课我们先来学习一下如何在FFmpeg中定制一个专属于自己的模块。定制模块的作用有很多，比如可以通过定制自己的私有格式，防止别人播放自己的视频。</p><p>在FFmpeg中添加模块，需要深入了解源代码架构。但FFmpeg源代码太多，我们需要找到一个突破口深入进去。下面，我们一步一步来解决这些问题。</p><p>首先，我们下载官方的源代码库，基于5.0分支做一个新分支kwai，作为我们源码的基础。</p><pre><code class=\"language-plain\">$ git clone git://source.ffmpeg.org/ffmpeg.git     # 下载源代码\n$ cd ffmpeg                                        # 进入源代码主目录\n$ git checkout remotes/origin/release/5.0          # 切换到5.0分支\nNote: switching to 'remotes/origin/release/5.0'.\n\n$ git checkout -b kwai                             # 开一个新分支，起名叫kwai\nSwitched to a new branch 'kwai'\n</code></pre><!-- [[[read_end]]] --><p>在源代码根目录中，执行命令ls |grep lib，可以列出相应模块的源代码目录。</p><pre><code class=\"language-plain\">$ ls | grep lib\nlibavcodec/\nlibavdevice/\nlibavfilter/\nlibavformat/\nlibavutil/\nlibpostproc/\nlibswresample/\nlibswscale/\n</code></pre><p>然后，可以用你喜欢的编辑器，打开整个源代码目录，浏览一下源代码的目录结构，找自己熟悉的内容看，比如你平时用H.264编码用得比较多，就可以在libavcodec目录下，找到很多h264开头的文件。</p><p>如果你写一个新的编解码模块的话，最简单的方式就是先找一个类似的模块，复制一个。不过，由于FFmpeg是一个庞大的项目，历史渊源也很深，里面会有各种代码判断，如if-else之类的，来应对实际使用环境中的问题，但这会干扰你的阅读。因此，这节课我们会通过最简单的例子，把问题讲清楚。</p><h2>编译可用版本</h2><p>我们先编译一个可用的版本。因为是自己实现模块，所以这里只需要最基本的编译就可以了，不需要编译大量的第三方模块。所以下面我们只使用了最简单的./configure，没有使用任何其它参数。</p><pre><code class=\"language-plain\">./configure\nmake -j4\n</code></pre><p>编译完成后，执行./ffmpeg -h，如果运行正常，说明我们编译成功了。我是在MacBook上编译的，如果没有MacBook的话，你也可以考虑使用Linux。使用下面的命令统计输出结果如下：</p><pre><code class=\"language-plain\">./ffmpeg -formats | wc -l          # 输出 388\n./ffmpeg -codecs | wc -l           # 输出 500\n./ffmpeg -filters | wc -l          # 输出 428\n./ffmpeg -protocols | wc -l        # 输出 58\n./ffmpeg -devices | wc -l          # 输出 9\n</code></pre><p>你也可以去掉其中的| wc -l，查看完整的输出，从这些输出的结果大概可以看出相应模块的数量<span class=\"reference\">（行数）</span>。</p><p>有了这个基础环境，下面我们就可以添加自己的模块了。如果你参考我在这节课里面贴的代码，同步做实验，可以在做完示例后，再次运行上面的命令，观察前后的差异。</p><h2>为FFmpeg添加自己的 AVFormat模块</h2><p>我们之前提到过很多次，AVForamt是各种音视频文件格式<span class=\"reference\">（包括网络文件格式）</span>的封装模块，要添加一个自己专属的AVForamt模块，需要先“发明”一种自己的文件格式，然后用代码实现，这里为了简洁一点儿，我们使用固定的编解码格式。</p><h3>kwai文件格式</h3><p>我们把新“发明”的文件格式命名为kwai，格式定义如下：</p><p>• 支持音视频，文件固定包含一个音频轨和一个视频轨。</p><p>• 音频固定为AAC，视频固定为H264。</p><p>• 音视频交错存储。</p><p>• 音视频数据块的长度，32位无符号整数，大端序，后面跟音视频数据。</p><p>• 长度字段最高位，音频为0，视频为1。</p><p>文件头定义如下：</p><p>• 4字节文件魔数，固定为kwai。</p><p>• 4字节版号，32位无符号整数，大端序。</p><p>• 4字节采样率，32位无符号整数，大端序。</p><p>• 1字节填充字符，无任何意义，固定为0。</p><p>• 1字节音频声道数。</p><p>• 2字节视频宽度，32位整数，大端序。</p><p>• 2字节视频高度，32位整数，大端序。</p><p>• 2字节帧率分子，16位整数，大端序。</p><p>• 2字节帧率分母，16位整数，大端序。</p><p>• 26字节其它文本信息，最后一字节为0，即NULL字符，主要是为了方便字符串处理。</p><p>这样定制内容，主要是为了演示不同长度整数的处理，外加一个额外的填充字符，同时也是为了数据对齐，人眼看起来比较直观，后面我们会在文件头的16进制数据表示时感受到这种效果。</p><h3>添加文件</h3><p>首先，我们在libavformat目录下创建两个文件：kwaienc.c和kwaidec.c，其中kwaienc.c对应文件编码，enc是encoding的缩写，也叫封装/mux。kwaidec.c对应文件解码，dec是decoding缩写，也叫解封装/demux。文件的具体内容我们后面再讲。</p><p>接下来在libavformat/Makefile中增加下面这段内容。</p><pre><code class=\"language-plain\">OBJS-$(CONFIG_kwai_DEMUXER)              += kwaidec.o\nOBJS-$(CONFIG_kwai_MUXER)                += kwaienc.o\n</code></pre><p>在libavformat/allformats.c中增加下面这段内容。</p><pre><code class=\"language-plain\">extern const AVOutputFormat ff_kwai_muxer;\nextern const AVInputFormat  ff_kwai_demuxer;\n</code></pre><p>执行命令./configure --list-muxers和./configure --list-demuxers可以列出所有的格式，如果能从输出结果中找到kwai，就说明添加成功了。</p><pre><code class=\"language-plain\">./configure --list-muxers\n./configure --list-demuxers\n</code></pre><p>然后重新执行configure。</p><pre><code class=\"language-plain\">./configure --enable-muxer=kwai --enable-demuxer=kwai\n</code></pre><p>到这里，我们的源文件和编译环境都准备好了。接下来，我们添加文件的封装格式。</p><h3>添加文件封装格式</h3><p>文件封装<span class=\"reference\">（就是AVOutputFormat）</span>是一个输出格式，它的输入端也是一个AVFormat<span class=\"reference\">（也就是AVInputFormat）</span>。下面，我们先注册我们的kwai封装文件格式，内容在kwaienc.c中。一般来说，分为这几步：</p><ol>\n<li>定义文件格式结构体</li>\n<li>准备参数</li>\n<li>定义一个类</li>\n<li>向FFmpeg注册文件格式</li>\n<li>实现回调函数</li>\n</ol><p>下面我们逐步讲解一下。</p><h4>定义文件格式结构体</h4><p>我们先来定义一个结构体，用来描述和存储文件的各种参数。其中，这个结构体的第一个成员必须是一个AVClass类型的指针，不需要特别处理，FFmpeg内部会用到。其它参数，你可以参考代码内的注释。</p><pre><code class=\"language-plain\">typedef struct kwaiMuxContext {\n    AVClass *class;         // AVClass指针\n    uint32_t magic;         // 文件魔数，用于标志文件的类型，很多文件类型如PNG和MP3都这么做\n    uint32_t version;       // 版本号，目前固定为1\n    uint32_t sample_rate;   // 音频采样率，常用的AAC格式采样率为44100\n    uint8_t channels;       // 声道数，一般为1或2\n    uint32_t width;         // 视频宽度\n    uint32_t height;        // 视频高度\n    AVRational fps;         // 帧率，这里用分数表示\n    char *info;             // 文件的其它描述信息，字符串\n} kwaiMuxContext;\n</code></pre><h4>准备参数</h4><p>准备一个AVOption结构体数组，用来存放kwai文件格式的相关参数。比如，我们下面定义了一个字符串格式的info参数和一个整数格式的version参数，最后用了一个空<span class=\"reference\">（NULL）</span>结构体元素结尾。这两个参数都可以在命令行上使用，后面我们会看到具体的用法。</p><p>FFmpeg会自动为这些参数申请存储空间。从下面的代码里我们可以看到，这些参数其实指向了我们上面定义的结构体，当在命令行上指定参数的时候，会修改相应的结构体指针。</p><pre><code class=\"language-plain\">static const AVOption options[] = {\n    { \"info\", \"kwai info\", offsetof(kwaiMuxContext, info), AV_OPT_TYPE_STRING,\n        {.str = NULL}, INT_MIN, INT_MAX, AV_OPT_FLAG_ENCODING_PARAM, \"kwaiflags\" },\n    { \"version\", \"kwai version\", offsetof(kwaiMuxContext, version), AV_OPT_TYPE_INT,\n        {.i64 = 1}, 0, 9, AV_OPT_FLAG_ENCODING_PARAM, \"kwaiflags\" },\n    { NULL },\n};\n</code></pre><h4>定义一个类</h4><p>定义一个AVClass结构体，指定kwai文件格式的名称，关联上面定义的参数等。</p><pre><code class=\"language-plain\">static const AVClass kwai_muxer_class = {\n    .class_name = \"kwai muxer\",\n    .item_name  = av_default_item_name,\n    .option     = options,\n    .version    = LIBAVUTIL_VERSION_INT,\n};\n</code></pre><h4>向FFmpeg注册文件格式</h4><p>有了上述内容，我们就可以向FFmpeg注册我们的kwai文件格式了。其中ff_kwai_muxer这个名字对应我们上面在allformats.c文件中添加的名字，这样FFmpeg在编译器中就可以找到我们定义的文件格式了。</p><pre><code class=\"language-plain\">const AVOutputFormat ff_kwai_muxer = {\n    .name              = \"kwai\",                 // 格式名称\n    .long_name         = NULL_IF_CONFIG_SMALL(\"kwai / kwai\"), // 长名称\n    .extensions        = \"kwai\",                 // 文件扩展名\n    .priv_data_size    = sizeof(kwaiMuxContext), // 私有数据内存大小\n    .audio_codec       = AV_CODEC_ID_AAC,        // 音频编码\n    .video_codec       = AV_CODEC_ID_H264,       // 视频编码\n    .init              = kwai_init,              // 初始化回调函数\n    .write_header      = kwai_write_header,      // 写文件头回调\n    .write_packet      = kwai_write_packet,      // 写文件内容回调\n    .write_trailer     = kwai_write_trailer,     // 写文件尾回调\n    .deinit            = kwai_free,              // 写文件结束后，释放内存回调\n    .flags             = 0,                      // 其它标志（略）\n    .priv_class        = &amp;kwai_muxer_class,      // 私有的文件结构体，指向我们上一步定义的内容\n};\n</code></pre><h4>实现回调函数</h4><p><img src=\"https://static001.geekbang.org/resource/image/8f/14/8f3a58623d5dbb8a819912c5ab667e14.png?wh=1920x529\" alt=\"图片\"></p><p>一切准备就绪，下面就是实现具体的回调函数了。</p><ol>\n<li>初始化函数</li>\n</ol><p>初始化函数在最初打开文件时调用，这个函数的输入参数是一个AVFormatContext结构体指针，由FFmpeg在打开文件时传入。这个结构体指针包含了文件的类型、流的数量<span class=\"reference\">（nb_streams）</span>和各种参数。根据这些参数，我们就可以完成kwai封装器的初始化工作了。</p><pre><code class=\"language-plain\">static int kwai_init(AVFormatContext *s)\n{\n    AVStream *st; // 音视频流，每一个stream代表一种类型，如音频流，视频流等\n    kwaiMuxContext *kwai = s-&gt;priv_data; // 指向私有的结构体，已初始化为默认值\n    printf(\"init nb_streams: %d\\n\", s-&gt;nb_streams);\n    if (s-&gt;nb_streams &lt; 2) { // 音视频流数量，我们只接受一个音频流和一个视频流的输入\n        return AVERROR_INVALIDDATA; // 简单出错处理\n    }\n    st = find_stream(s, AVMEDIA_TYPE_AUDIO); // 查找音频流，该函数后面解释\n    if (!st) return AVERROR_INVALIDDATA;     // 简单出错处理，如果找不到音频流则返回错误\n    kwai-&gt;sample_rate = st-&gt;codecpar-&gt;sample_rate; // 记住输入音频流的采样率\n    kwai-&gt;channels = st-&gt;codecpar-&gt;channels;       // 记住声道数\n    st = find_stream(s, AVMEDIA_TYPE_VIDEO); // 查找视频流\n    if (!st) return AVERROR_INVALIDDATA;     // 简单出错处理\n    kwai-&gt;width = st-&gt;codecpar-&gt;width;       // 记住视频宽度\n    kwai-&gt;height = st-&gt;codecpar-&gt;height;     // 记住视频高度\n    // kwai-&gt;fps = st-&gt;codecpar-&gt;fps;\n    kwai-&gt;fps = (AVRational){15, 1};         // 记住帧率\n    return 0;                                // 初始化正常返回0\n}\n</code></pre><p>在上面的代码中，我们用到了一个find_stream函数，它可以从kwai格式的输入参数中查找音视频流。</p><pre><code class=\"language-plain\">static AVStream *find_stream(AVFormatContext *s, enum AVMediaType type)\n{\n    int i = 0;\n    for (i = 0; i &lt; s-&gt;nb_streams; i++) { // 遍历所有输入流\n        AVStream *stream = s-&gt;streams[i];\n        if (stream-&gt;codecpar-&gt;codec_type == type) { // 找到第一个对应的类型（音频或视频），即返回对应的流\n            return stream;\n        }\n    }\n    return NULL;\n}\n</code></pre><ol start=\"2\">\n<li>写文件头</li>\n</ol><p>初始化完成后，下一步就是写文件头，其实写文件头函数也是一个回调函数，都是由FFmpeg的核心逻辑回调的，所以我们只需要照着输入输出格式定义好头文件就可以了。</p><pre><code class=\"language-plain\">static int kwai_write_header(AVFormatContext *s)\n{\n    kwaiMuxContext *kwai = s-&gt;priv_data;        // 获取我们的私有结构体\n    kwai-&gt;magic = MKTAG('K', 'W', 'A', 'I');    // 初始化魔数\n    // avio_wb32(s-&gt;pb, kwai-&gt;magic);\n    avio_write(s-&gt;pb, (uint8_t *)&amp;kwai-&gt;magic, 4); // 将该魔数写入文件，此时文件有4字节，内容为kwai\n    avio_wb32(s-&gt;pb, kwai-&gt;version);            // 以大端序写入版本号，占4字节\n    avio_wb32(s-&gt;pb, kwai-&gt;sample_rate);        // 以大端序写入采样率\n    avio_w8(s-&gt;pb, 0);                          // 写入一个字节占位符，无任何意义\n    avio_w8(s-&gt;pb, kwai-&gt;channels);             // 写入一个字节声道数\n    avio_wb16(s-&gt;pb, kwai-&gt;width);              // 写入宽度，2字节\n    avio_wb16(s-&gt;pb, kwai-&gt;height);             // 写入高度，2字节\n    avio_wb16(s-&gt;pb, kwai-&gt;fps.num);            // 写入帧率分子部分，2字节\n    avio_wb16(s-&gt;pb, kwai-&gt;fps.den);            // 字入帧率分母部分，2字节\n    char info[26] = {0};\n    if (kwai-&gt;info) {                           // 如果命令行上有info参数，则将其内容读到临时内在\n        strncpy(info, kwai-&gt;info, sizeof(info) - 1);\n    }\n    avio_write(s-&gt;pb, info, sizeof(info));       // 写入info字符串内容\n    return 0;\n}\n</code></pre><p>在上述代码中，info字符串占26个字节<span class=\"reference\">（包含结尾的NULL字符）</span>，这主要是为了使文件头部分正好是48个字节，对人眼比较友好。</p><ol start=\"3\">\n<li>写音视频数据</li>\n</ol><p>如果初始化和写文件头正常，FFmpeg就开始写音视频数据了。其中，输入参数除了AVFormatContext结构体指针外，还有一个AVPacket结构体指针，里面存放了具体要求的音视频数据。音视频会交错存储，首先写入当前数据的时间戳<span class=\"reference\">（64位的pts值）</span>，然后是以32位无符号整数表示的长度<span class=\"reference\">（其中视频的长度最高位置1）</span>，接着写入实际的音视频数据。</p><pre><code class=\"language-plain\">static int kwai_write_packet(AVFormatContext *s, AVPacket *pkt)\n{\n    // kwaiMuxContext *mov = s-&gt;priv_data;\n    uint32_t size = pkt-&gt;size;               // 获取数据大小\n\n    if (!pkt) {\n        return 1;\n    }\n\n    AVStream *st = s-&gt;streams[pkt-&gt;stream_index];  // 通过stream_index可以找到对应的流，是音频还是视频\n\n    if (st-&gt;codecpar-&gt;codec_type == AVMEDIA_TYPE_AUDIO) {\n        printf(\"Audio: %04d pts: %lld\\n\", size, pkt-&gt;pts); // 打印音频字节数和pts\n    } else if (st-&gt;codecpar-&gt;codec_type == AVMEDIA_TYPE_VIDEO) {\n        printf(\"Video: %04d pts: %lld\\n\", size, pkt-&gt;pts); // 打印视频字节数和pts\n        size |= (1 &lt;&lt; 31); // 如果是视频，将长度的最高位置1\n    } else {\n        return 0; // ignore any other types\n    }\n\n    avio_wb64(s-&gt;pb, pkt-&gt;pts);              // 写入8字节pts，大端序\n    avio_wb32(s-&gt;pb, size);                  // 写入4字节视频长度，大端序\n    avio_write(s-&gt;pb, pkt-&gt;data, pkt-&gt;size); // 写入实际的音视频数据\n\n    return 0;\n}\n</code></pre><ol start=\"4\">\n<li>文件结束处理</li>\n</ol><p>文件结束时，调用write_trailer回调写尾部数据，并调用deinit回调释放相应的内存。这里我们的封装器实现得比较简单，所以简单放两个空函数即可。</p><pre><code class=\"language-plain\">static int kwai_write_trailer(AVFormatContext *s)\n{\n    return 0;\n}\n\nstatic void kwai_free(AVFormatContext *s)\n{\n}\n</code></pre><p>到这里，我们的文件格式封装器就做好了。</p><ol start=\"5\">\n<li>编译运行</li>\n</ol><p>最后，直接执行make就可以编译了。不过，这时候由于我们没有实现解封装器，会提示ff_kwai_demuxer不存在。为了能“骗过”编译器，我们可以先在kwaidec.c里定义一下，临时代码如下所示：</p><pre><code class=\"language-plain\">#include \"avformat.h\"\nconst AVInputFormat  ff_kwai_demuxer;\n</code></pre><p>编译通过后，我们就可以使用熟悉的命令行来生成一个kwai类型的文件了。先找一个标准的MP4文件<span class=\"reference\">（如input.mp4）</span>作为输入，命令行如下：</p><pre><code class=\"language-plain\">./ffmpeg -i input.mp4 -bsf:v h264_mp4toannexb -info 'a simple test' out.kwai\n</code></pre><p>在上述命令中，我们使用了h264_mp4toannexb这个filter，它的主要作用是将MP4中的H264视频容器数据封装转换为传统的使用startcode分割的annexb比特流格式。因为大多数解码器只支持这种格式。此外，我们还使用-info参数增加了一些文本信息，输出文件名为out.kwai。</p><p>现在，地球上还没有任何一个播放器能播放我们生成的文件。下面，我们先来分析一下文件格式是否符合我们的预期。Windows平台可以使用一些16进制编辑器打开文件查看，Linux或macOS可以使用xxd命令来查看<span class=\"reference\">（这个命令一般会随vim编辑器一起安装）</span>。具体命令如下：</p><pre><code class=\"language-plain\">$ xxd out.kwai | head -n 4\n</code></pre><p><span class=\"reference\">注：head -n 4表示只查看输出的前4行。</span></p><p>输出结果是下面这段内容。</p><pre><code class=\"language-plain\">00000000: 6b77 6169 0000 0001 0000 ac44 0002 0280  kwai.......D....\n00000010: 01e0 000f 0001 6120 7369 6d70 6c65 2074  ......a simple t\n00000020: 6573 7400 0000 0000 0000 0000 0000 0000  est.............\n00000030: 0000 0000 0000 0000 0000 0017 de02 004c  ...............L\n</code></pre><p>其中，输出结果在横向上分为三部分，左侧是文件字节偏移量，以16进制表示；中间是实际的数据，每行有16个字节，两个16进制数字表示一个字节<span class=\"reference\">（取值范围为00~ff）</span>，两个字节为一组<span class=\"reference\">（即一个字）</span>；最后是文件内容可读的形式，如果是可读的字符，就会显示出来，否则以“.”表示。</p><p>对照着我们前面对kwai类型文件的定义以及代码实现，可以看到，上面的输出是符合预期的。从第一行看，最开始4个字节6b77 6169，对应的ASCII码是kwai，这是我们规定的文件头，也就是魔数。接下来4个字节0000 0001是版本号<span class=\"reference\">（此处为1）</span>；后面4个字节0000 ac44换成10进制是44100，即采样率；再后面一个字节的00是占位符，没有任何作用；占位符后的一个字节02表示声道数；两个字节0280表示视频宽度，转换成10进制是640。</p><p>第二行，前两个字节01e0是视频高度，即480；后面是帧率：000f和0001，即15/1。接下来就是我们命令行上用-info参数设置的字符串，此处是“a simple test”；后面全部以0填充，直到第三行行尾。我们精心设计了48个字节的文件头，就是为了让它在输出时正好占满3行。</p><p>从第四行开始，前8个字节的0是时间戳，它是一个64位整数，此处时间戳是从0开始的。接下来0000 0017表示后面音视频的长度，因为这个数的最高位是0<span class=\"reference\">（二进制最高位也是0）</span>，所以它后面应该是音频数据，占23<span class=\"reference\">（0x17换成10进制是23）</span>个字节。往后跳过23个字节就应该是下一组数据了，以此类推。</p><p>你可以在前面的命令行上加上-version 2，并修改相应的info看一下有什么变化，完整的命令行参考如下：</p><pre><code class=\"language-plain\">./ffmpeg -i input.mp4 -bsf:v h264_mp4toannexb -info 'another simple test' -version 2 out.kwai\n</code></pre><h2>小结</h2><p>这节课我们在FFmpeg中添加了一个自己专属的封装模块，如果你想只有你的播放器能够播放你自己的私有化音视频数据的话，这种操作是比较常见的。其实操作比较简单，主要是添加AVOutputFormat、AVIntputFormat、AVCodec这样的结构体，给结构体填充自己指定的内容即可。学习完这节课以后，你可以自己分析FFmpeg的各模块代码，跟着我的操作步骤添加一个自己的专属模块。</p><h2>思考题</h2><p>这节课我们学习的是添加一个muxer，那么你能否为muxer添加一个demuxer呢？ 欢迎你在评论区留言和我讨论，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"13 | FFmpeg 有哪些常见的应用场景？","id":554456},"right":{"article_title":"15｜如何参与到FFmpeg社区交流中？","id":557488}}},{"article_id":557488,"article_title":"15｜如何参与到FFmpeg社区交流中？","article_content":"<p>你好，我是刘歧。</p><p>我们在日常生活中经常会使用FFmpeg做一些音视频开发工作，但如果你在使用的过程中发现了一些自己解决不了的问题，你会怎么处理呢？</p><p>忽略问题或者想办法绕过去并不是一个好的选择，其实我们是可以通过参与FFmpeg社区的交流，来解决这些问题的。社区里有来自世界各地的专业能力很强的人，通过和他们交流，不仅可以解决工作中遇到的问题，还能够从交流中发现很多FFmpeg黑科技，拓宽自己的视野。</p><h2>角色介绍</h2><p>在交流过程中，我们会看到社区中主要有这么几类人。</p><ul>\n<li>贡献者<span class=\"reference\">（contributor）</span>：主要指给开源项目贡献代码或文档的人，还有参与code review并且review意见被采纳的人。</li>\n<li>维护者<span class=\"reference\">（maintainer）</span>：主要指对代码或文档有提交和维护责任的人。</li>\n<li>委员会<span class=\"reference\">（committee）</span>：主要指负责社区日常运作、流程管理这一类工作的人。</li>\n</ul><p>我们可以通过把代码提供给FFmpeg的方式成为代码贡献者，让全世界更多的音视频流媒体开发者和FFmpeg使用者用上我们的代码，放大我们个人的价值。如果你不知道该如何在音视频领域闯出一片天地，那么成为FFmpeg社区的开发者可能是一条不错的路。</p><p>那具体我们应该怎么参与到开发中呢？首先我们需要了解社区交流使用的工具。</p><!-- [[[read_end]]] --><h2>交流的工具</h2><p>参与FFmpeg开源社区交流，比较通用的沟通方式不是QQ、微信这一类社交软件。那用什么呢？其实说起来可能你会觉得比较落后，FFmpeg开源社区主要是通过邮件列表和IRC聊天室沟通。IRC聊天室比较即时，但是整体不如用邮件列表通用，因为邮件列表可以收发邮件，是最基础的通信工具，也是开发者人群覆盖面最全的工具，因为没有电子邮箱的开发者几乎为0。</p><h3>邮件列表</h3><p>通过FFmpeg官方网站<a href=\"https://ffmpeg.org/contact.html#MailingLists\">邮件列表页</a>我们可以看到，虽然命令行用户和API用户对FFmpeg来说都是用户，但是咨询问题用的邮件列表不一样。命令行用户是在ffmpeg-user列表里交流，API用户是在libav-user列表里交流，FFmpeg的开发者是在ffmpeg-devel列表里面交流。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/89/e048a58e71de74ba5319a4fa21b89689.png?wh=1920x1034\" alt=\"图片\"></p><p>这几个列表的特点比较明显，在libav-user列表里问问题、交流，几乎没什么人回复，主要是因为API用户自己业务场景或自己使用的方式出现了一些问题，别人也不太愿意看你的代码，所以自己挖的坑还是需要自己仔细研究怎么填。</p><p>ffmpeg-devel列表里，主要是以Patch为沟通的基础，在这里面提需求几乎没人理，反馈Bug也没人理，但是你修改了FFmpeg内部的代码的话，做成patch发到这个列表里，获得回应的概率会高一些。</p><p>ffmpeg-user里比较热闹，主要是命令行遇到问题，有些参数执行的效果有问题等。这个邮件列表里的用户比较多，相比另外两个，收到回复的概率更大。但是需要注意一点，<strong>不要“top-posting”</strong>，这是FFmpeg社区邮件列表里面沟通最基本的规则。“top-posting”是什么意思呢？就是在回复邮件的时候不要在邮件内容的最上面回复，推荐的做法是想要回复哪一句就在哪一句的下面新起一行回复。</p><p>如果对整个邮件都存在不同意见的话，可以在邮件的最下方回复，这么做除了能让大家看得更清晰之外，也方便归档。邮件列表是有每日归档的，归档时有统一的格式要求。所有的记录在<a href=\"https://lists.ffmpeg.org/pipermail/ffmpeg-devel/\">归档列表</a>里面都能找到。邮件列表的使用方法比较简单，先使用邮箱注册到邮件列表，然后在自己邮箱里确认注册成功就可以了。</p><p>正常参与社区交流的话，注册一个邮箱到邮件列表里还是有必要的，毕竟后面如果发patch到邮件列表的话，还是需要先注册邮箱才可以，否则patch发不出去。</p><h3>IRC</h3><p><img src=\"https://static001.geekbang.org/resource/image/a2/a0/a2af27289efe660e977f7e9480d766a0.png?wh=1920x1115\" alt=\"图片\"></p><p>FFmpeg 项目沟通除了邮件列表之外，还可以通过IRC即时聊天室沟通，只不过即时聊天室里的人比邮件列表里面的人少很多。</p><p>聊天室主要是分为用户聊天室和开发者聊天室，用户主要是命令行用户和API用户，开发者是指FFmpeg内部代码开发者。用户聊天室人会多一些，活跃度高一些，开发者聊天室用户少，所以活跃度没有那么高，并且大多是关于开发内容的临时性沟通，主要沟通还是在邮件列表里面，所以通常大家不怎么用IRC交流，除非特别着急的时候。如果你有兴趣的话，可以看一下<a href=\"https://ffmpeg.org/contact.html#IRCChannels\">IRC相关的参考链接</a>。</p><p>如果想要在FFmpeg项目中和大家沟通，需要先学会使用这两种交流工具。这是成为contributor的第一步，除了用工具进行日常的交流之外，成为contributor还需要给FFmpeg反馈Bug、贡献代码。接下来我们一起看一下FFmpeg 反馈Bug和贡献代码的渠道。</p><h2>Bug 反馈渠道</h2><p><img src=\"https://static001.geekbang.org/resource/image/10/86/10dcbf6a1294b37598e95e5f92855286.png?wh=1508x656\" alt=\"图片\"></p><p>FFmpeg的维护不是在GitHub上面，所以Bug反馈在GitHub上面找不到issue，FFmpeg是通过<a href=\"https://trac.ffmpeg.org/timeline\">trac</a>来管理Bug的。Bug也分很多种，有需求类Bug，也有阻塞型Bug。提Bug的格式也需要注意，主要是需要说清楚自己的环境、FFmpeg的版本和使用的参数，把FFmpeg执行命令的那一行到结束的所有的内容都贴到Bug说明里，不需要自己做内容剪切。不然，开发者们可能无法理解你的Bug诉求。</p><p>自己提了Bug以后，不一定能够及时得到解决。如果我们自己分析并解决了Bug，再把代码回馈给FFmpeg的话，是个参与FFmpeg开发不错的路径，而且还会降低使用风险。为什么我会这么说呢？因为FFmpeg是LGPL的License，如果不反馈修改过的代码的话，可能会涉及开源使用合规相关的法务问题，尽管FFmpeg目前不太追究法律问题了，但是还是要考虑一下合规性的。</p><h2>代码贡献渠道</h2><p>当前，给FFmpeg贡献代码采用的是向邮件列表发送patch的方式，发送patch到邮件列表后，邮件列表里面的开发者和维护者们会通过回邮件的方式做codec review，可能会提一些comments，他们做code review的时候，也是在邮件内容中对自己有意见的那一行或者那一部分做出回复，不会top-posting。</p><p>而patch是需要通过git format-patch来生成的，在发送patch之前，你需要自己验证一下代码格式是否符合标准，还记得<a href=\"https://time.geekbang.org/column/article/551256\">第10节课</a>我们git clone过FFmpeg的源代码吗？在源代码目录的tools目录下有一个<a href=\"https://ffmpeg.org/developer.html#Coding-Rules-1\">patcheck</a>，它可以辅助你检查代码是否符合FFmpeg的基本标准，在修改完代码之后，自己本地需要做一下FFmpeg自测，操作步骤如下：</p><ol>\n<li>make fate-rsync SAMPLES=fate-suite/</li>\n<li>make fate &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SAMPLES=fate-suite/</li>\n</ol><p>也可以在做configure编译配置的时候指定fate测试样本。</p><pre><code class=\"language-plain\">./configure --samples=fate-suite/\nmake fate-rsync\nmake fate\n</code></pre><p>在配置<span class=\"reference\">（configure）</span>的时候，添加一个–samples=fate-suite来指定测试样本下载的目录。</p><p>在make fate通过以后，才能确保修改的代码对原有的FFmpeg代码和能力没有太大的影响。如果make fate不通过的话，说明代码修改得还是不够好，会影响一些我们没有看到的逻辑。</p><p>发送patch到邮件列表的时候，我推荐你使用git send-email的方式来发送，这样可以按照标准格式将patch发送到邮件列表，如果打开文本复制粘贴到邮件里面的话，很容易出现乱码，下面这个人的操作就是错误的。</p><p><img src=\"https://static001.geekbang.org/resource/image/af/47/af8f8b3ae0d670933eb9db440cb7a347.png?wh=1456x1482\" alt=\"图片\"></p><p>这个patch发送得比较失败，内容乱码，不但复制粘贴了patch内容，还用的是富文本格式。patch本身是纯文本格式，富文本格式会出现乱码。虽然我们的语言环境是中文，但是FFmpeg项目是国际项目，所以全球哪里的人都有。试想一下如果人家发德文、法文、阿拉伯文、泰文、蒙文给我们，我们是否能看懂呢？所以把中文发给他们也是一样的道理。</p><p>而FFmpeg的邮件列表和其他工具基本上是联动的，如果我们发一个patch想确认是否正常的话，FFmpeg还提供了个<a href=\"https://patchwork.ffmpeg.org/\">patchwork工具</a>，在patchwork里也可以看到你的patch是否正常。因为FFmpeg支持的平台比较多，包括我们的龙芯，所以平台兼容性也在考虑范围之内。</p><p><img src=\"https://static001.geekbang.org/resource/image/34/f4/34c11b6e86ecfef9a3a2a465e04745f4.png?wh=1204x506\" alt=\"图片\"></p><p>当然，我们在修改完代码以后，做git commit的时候提交信息需要尽量全面地描述修改的原因、你的思考以及背后的逻辑，这样别人才能知道你为什么这样修改代码。</p><p>上面我说的这些FFmpeg开发者的规则，说难其实也不难，只要你平时保持良好的代码开发习惯，这些应该都不是问题。</p><p>按照正确的规则给FFmpeg贡献代码、提交patch之后，我们就有了成为贡献者的资格，如果你想顺着这条路径继续往前走，最后就是成为FFmpeg的维护者，但成为维护者的难度要比贡献者大得多。</p><h2>成为维护者</h2><p>要想成为维护者的话，首先需要达到几个关键的指标。</p><ol>\n<li>代码覆盖量达到一定的标准，就拿某个模块来说，如果这个模块代码有5000行，其中有超过50%的代码是你写的，那么你就有机会成为这个模块的维护者。</li>\n<li>在FFmpeg做Bugfix的patch、完善功能的patch以及性能优化的patch，达到一定数量以后，你就可以尝试申请成为FFmpeg的维护者。数量没有一个明确的量化值，但多多益善，你越活跃，邮件列表里的人们就会对你越熟悉，成为维护者之路才会越顺利。</li>\n</ol><h3>搭建本地验证环境</h3><p>当有人发patch到邮件列表里面的时候，patchwork会自动将patch放到自己的队列里面。如果想要成为维护者，可以考虑自己在本地搭建一个环境，从patchwork队列里将自己维护的模块或相关的patch自动下载到本地的，合并到自己本地的代码库里自动地make fate。</p><p>如果你希望成为维护者，patch的兼容性就是一个必要条件，所以在本地构建自动化回测的各个系统环境是必不可少的，一些基本的可自动化验证的环境也是必不可少的。你可以参考以下几个环境。</p><ol>\n<li>通过用QEMU模拟MIPS+Linux的环境。</li>\n</ol><pre><code class=\"language-plain\">../configure --target-exec='.../qemu-mips -cpu 74Kf -L/usr/mips-linux-gnu/' --samples=... --enable-gpl --cross-prefix=/usr/mips-linux-gnu/bin/ --cc='ccache mips-linux-gnu-gcc-4.4' --arch=mips --target-os=linux --enable-cross-compile --disable-pthreads --disable-mipsfpu --disable-iconv\n</code></pre><ol start=\"2\">\n<li>通过WINE模拟windows环境。</li>\n</ol><pre><code class=\"language-plain\">../configure  --cc='ccache i686-w64-mingw32-gcc'  --samples=... --arch=x86 --target-os=mingw32 --cross-prefix=i686-w64-mingw32- --enable-gpl --pkg-config=./pig-config --target_exec=wine\n</code></pre><ol start=\"3\">\n<li>X86 Linux环境</li>\n<li>X86 MacOS环境</li>\n<li>M1 MacOS环境</li>\n</ol><p>这些环境都make fate通过以后，相当于完成自动化review的第一步。</p><h3>本地验证代码</h3><p>新增的patch对代码的修改是否会引起内存泄露，这也是不可缺少的一步。你可以尝试使用AddressSanitizer或Valgrind做代码修改的内存操作异常检测。比如我本地用的是AddressSanitizer。</p><pre><code class=\"language-plain\">--extra-ldflags=' -O0 -g3 -fsanitize=address -Wno-error -fPIC -I/usr/local/include' --extra-ldflags='-O0 -g3 -fsanitize=address -Wno-error -fPIC '\n</code></pre><p>如果出现异常，比如内存操作不标准的话，执行FFmpeg做验证的时候会报错。</p><pre><code class=\"language-plain\">==58865==ERROR: AddressSanitizer: attempting free on address which was not malloc()-ed: 0x6130000013b8 in thread T0\n    #0 0x10cbf7639 in wrap_free+0xa9 (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x48639)\n    #1 0x10a4676f4 in av_free mem.c:251\n    #2 0x109433230 in mov_free movenc.c:6755\n    #3 0x109470296 in deinit_muxer mux.c:423\n    #4 0x109471671 in av_write_trailer mux.c:1281\n    #5 0x108e1eece in of_write_trailer ffmpeg_mux.c:533\n    #6 0x108e41c77 in transcode ffmpeg.c:4095\n    #7 0x108e410d2 in main ffmpeg.c:4242\n    #8 0x11069f51d in start+0x1cd (dyld:x86_64+0x551d)\n\n0x6130000013b8 is located 56 bytes inside of 304-byte region [0x613000001380,0x6130000014b0)\nallocated by thread T0 here:\n    #0 0x10cbf7c03 in wrap_posix_memalign+0xb3 (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x48c03)\n    #1 0x10a467536 in av_malloc mem.c:105\n    #2 0x10a4678a4 in av_mallocz mem.c:266\n    #3 0x10946f152 in avformat_alloc_output_context2 mux.c:122\n    #4 0x108e2272a in open_output_file ffmpeg_opt.c:2900\n    #5 0x108e20b4a in open_files ffmpeg_opt.c:3668\n    #6 0x108e20998 in ffmpeg_parse_options ffmpeg_opt.c:3724\n    #7 0x108e40ff7 in main ffmpeg.c:4225\n    #8 0x11069f51d in start+0x1cd (dyld:x86_64+0x551d)\n\nSUMMARY: AddressSanitizer: bad-free (libclang_rt.asan_osx_dynamic.dylib:x86_64h+0x48639) in wrap_free+0xa9\n==58865==ABORTING\n</code></pre><h3>本地验证 patch 代码风格</h3><p>因为每一个patch贡献者都有可能忽略了自己代码风格的问题，如果合并到FFmpeg代码库里，其他人在阅读代码的时候看到各种各样的代码风格会感觉很混乱，所以在合并patch之前需要检测一下代码风格是否符合FFmpeg本身的风格，通过patcheck就可以搞定。</p><pre><code class=\"language-plain\">[root@onvideo-liuqi05-01 ffmpeg]# ./tools/patcheck 0001-avfilter-vsrc_ddagrab-add-options-for-more-control-o.patch \npatCHeck 1e10.0\nThis tool is intended to help a human check/review patches. It is very far from\nbeing free of false positives and negatives, and its output are just hints of what\nmay or may not be bad. When you use it and it misses something or detects\nsomething wrong, fix it and send a patch to the ffmpeg-devel mailing list.\nLicense: GPL, Author: Michael Niedermayer\n\npossibly unused variables\npossibly never written:allow_fallback\npossibly constant     :allow_fallback\npossibly never written:force_fmt\npossibly constant     :force_fmt\n\nMissing changelog entry (ignore if minor change)\n[root@onvideo-liuqi05-01 ffmpeg]# \n</code></pre><p>这些准备工作都完成之后，你就可以根据我们上面说的两个关键指标去努力了。</p><h2>小结</h2><p>好了，最后我们来回顾一下今天学到的内容吧！</p><p><img src=\"https://static001.geekbang.org/resource/image/c0/10/c0df8d0f745784f2bc829b73c1101510.png?wh=1646x1764\" alt=\"图片\"></p><p>想要参与到社区交流中，我们需要熟知社区中的角色和职能、交流的规则，以及常用的工具。</p><ul>\n<li>FFmpeg社区中有3种主要角色：贡献者、维护者和委员会。</li>\n<li>常用的交流工具是邮件列表和即时聊天室IRC，其中邮件列表更通用一些。</li>\n<li>遇到Bug时需要通过<a href=\"https://trac.ffmpeg.org/timeline\">trac</a>反馈，解决了Bug以后可以用邮件列表发送patch的方式来反馈。</li>\n<li>想要成为官方维护者，我们需要在社区保持一定的活跃度，多贡献代码，提交patch。除此之外还要搭建本地验证环境，在本地验证代码和patch的代码风格。</li>\n</ul><p>最后，我还想强调一点，<strong>在参与社区交流和开发之前还是需要先观察，少发言，等到完全了解了基本操作规则和规范之后再交流也不迟。</strong>毕竟参与到FFmpeg社区交流以后，你就不是你自己了，而是代表我们的国家。所以需要慎之又慎，尽量职业化一些，因为在邮件列表和IRC里你的一言一行都会被归档记录下来，十年甚至二十年之后都是可以查到的。所以切记，<strong>多用代码交流，代码是FFmpeg社区交流最好的语言</strong>。</p><h2>思考题</h2><p>光说不练假把式，最后我希望你可以动手操作一下。</p><ol>\n<li>使用&nbsp;git &nbsp;send-email&nbsp; 发送一个&nbsp;patch到自己的邮箱里面。提示：首先你需要配置你的git send-email环境。</li>\n<li>在留言处总结一下你看到的FFmpeg社区里面的规则，包括官方开发者文档里写的还有你自己发现的。</li>\n</ol><p>欢迎你在评论区留下你的思考，也欢迎你把这节课分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"14｜如何在FFmpeg中定制一个自己专属的模块？","id":556024},"right":{"article_title":"结束语｜音视频技术更宠爱脚踏实地的人","id":559194}}},{"article_id":559194,"article_title":"结束语｜音视频技术更宠爱脚踏实地的人","article_content":"<p>你好，我是刘歧。</p><p>今天这节课，是我们专栏的最后一讲。感谢有你的一路相伴，每次看到在留言区认真地提问、回答问题的你，我都会感到无比欣慰，这说明你在认真地思考，这让我觉得自己正在做一件很有意义的事情，回顾我们来时的路，你会发现，在不知不觉中我们收获了很多知识。</p><p><img src=\"https://static001.geekbang.org/resource/image/b7/1f/b7a5bfbd31d54aabe22ed5147f14601f.png?wh=1920x1146\" alt=\"图片\"></p><p>从第一节课到现在，我已经尽我所能地将音视频技术相关的重要知识提炼出来，讲给你听了。从音视频相关的基础知识，到OBS、Handbrake等好用的音视频处理工具，再到FFmpeg命令行参数和API的应用，希望你能跟在我身后打开一扇又一扇音视频技术的大门。</p><p>但是门后的世界到底如何呢？之后还需要你自己在实践中继续摸索着前行。毕竟师父领进门，修行在个人，方法摆在你面前了，就看你愿不愿意去践行它。</p><p>记得之前一个做前端开发的同事，他在刚接触音视频流媒体开发的时候说过一句话：“这音视频技术对我们年轻人好像不太友好啊。”这话其实说对了一半，严格意义上来讲，其实是<strong>音视频技术更宠爱脚踏实地的人。</strong></p><p>因为音视频的参考标准比较多，需要有实实在在的技术积累，认认真真地将自己用到的标准研究一番，然后才能够略窥门径，耍小聪明的人不太适合做这门技术。这样的难度自然而然地筛掉了一部分人，所以真正从事这个领域的人确实不多。</p><!-- [[[read_end]]] --><p>当然，也许你会比较关注从事音视频技术的开发者的工资待遇会不会高一些？根据我的个人经验，这一行的工资很高，非常高，特别特别高。但前提是你要有<strong>良好的技术基础和相关的知识积累，</strong>这都需要我们花更多的时间去学习、去实践。</p><p>我们这个专栏虽然要结束了，但你未来的学习之路还很长。记得当时我们开篇的时候就说过，我们把FFmpeg当成进入音视频领域的突破口，这只是我们入门的第一课，除了FFmpeg之外，还我们还有很多软件可以参考，例如gstreamer、mediainfo、vlc等。但如果你想要在这个领域深耕，需要掌握的就不仅限于这样的软件工具了，还需要有一定的计算机应用技术基础。</p><p>我近几年在学习音视频技术的时候感觉越来越顺心应手了，其实也是得益于早些年积累的计算机基础知识。所以为了让你入门之后的路走得更顺利一些，我把我之前的经验总结成了几条建议，在这里分享给你。</p><ol>\n<li>选择一门合适的计算机程序设计语言作为入门音视频程序开发技术的语言，通过前面列举过的几个程序来看，C或者C++应该是首选，尽管JavaScript、Java、Python的用户规模很大，Rust、Go很时髦，但如果没有一个相对比较完整的开源项目做参考的话，学习起来难度还是比较大的。</li>\n<li>学一学Bash和Makefile，因为Linux、Macbook、Windows等编译环境的不同，在编译开源软件的时候会遇到千奇百怪的报错。有的人遇到错误的时候会比较慌，不知道从哪里入手排查，这时候其实只要认真地看一看错误提示，基本上是有解决办法的，因为错误提示会给出错误的原因。但是并不是所有的开源项目都很友好，所以还是需要自己去看一下Makefile或者构建项目的脚本，看看为什么会报错，这样才能从根本上解决问题。</li>\n<li>自己跟着<a href=\"https://linuxfromscratch.org/lfs/\">LFS</a>动手构建专属于自己的Linux发行版，这样可以加深对一个操作系统及其软件的理解，尤其是如何安装的问题。</li>\n<li>勤复习《高等数学》《高等代数》《离散数学》几本书的知识点，因为音视频处理会涉及一些算法优化，这时候就需要有一定的数学基础。尤其是当你有一天突然萌生了一个想法，“这个VVC编码器效率好像不太行，是不是哪一步消耗的时间比较多？我看看”，这时候如果你的数学不太好，可能就真的会从入门到放弃了。</li>\n<li>涉猎更多的开发技术，比如Linux内核程序设计与开发、Linux网络程序设计与开发、文件系统设计与开发、设备驱动程序开发等。</li>\n<li>一定要按照音视频的参考标准从main函数开始，写上一两个解析器或者解码器，哪怕性能没那么优秀，但至少你会对音视频的编解码、文件分析等操作有一个整体的感知，遇到问题后也会有一个大概的解决方向。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/56/fb/56e69e2e1576c96bb4a201a44b90cefb.png?wh=1920x745\" alt=\"图片\"></p><p>有了以上几点基本技能作为基础，以后你做音视频技术部分的开发与应用应该是会越来越顺的，解决问题也会越来越得心应手。坦率地讲，如果你真的把这几点付诸实践的话，就算不做音视频技术开发与应用，工资待遇也不会差很多，毕竟基础技术实力在那儿放着呢。</p><p>这两年网络中大肆贩卖35岁焦虑的文章比较多，但其实35岁的线与音视频技术领域的从业人员关系不大。从目前的状态看，我认识的音视频技术研发人员超过80%都是大于35岁的，甚至会感觉很多人的年纪远大于35岁，并不是因为他们年纪小但是长得老，而是因为他们工作的时间确实超过了十年，丰富的经验让他们有一种老成持重的感觉。</p><p>但即便他们已经有了完整的知识体系和丰富的经验，也还在持续地学习。前辈们实践与学习所花的时间并不比我们少，他们和我们一样勤奋，我们在进步的时候，他们依然在进步。所以想要在这个领域做出一番事业，是没有什么捷径可走的，前辈们尚且如此努力，我们有什么理由懈怠呢？</p><p>希望你选择了音视频技术之后，能够爱上这门技术，投入时间和精力去琢磨、去学习、去研究，脚踏实地，永不放弃地去追求极致。这个过程中你也许也会迷茫、困惑，但只要你坚持下去，时间就会给你答案。非淡泊无以明志，非宁静无以致远，仅此而已。</p><p>最后，文末有一份<a href=\"http://jinshuju.net/f/WIUBoM\">结课问卷</a>，希望你可以花两分钟的时间填写一下。我会认真倾听你对这个专栏的意见或建议，期待你的反馈！</p><p><a href=\"http://jinshuju.net/f/WIUBoM\"><img src=\"https://static001.geekbang.org/resource/image/06/d8/06692bb6f63818432f46e6297130bdd8.jpg?wh=1142x801\" alt=\"\"></a></p>","neighbors":{"left":{"article_title":"15｜如何参与到FFmpeg社区交流中？","id":557488},"right":[]}}]