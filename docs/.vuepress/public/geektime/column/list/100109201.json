[{"article_id":496850,"article_title":"开篇词 | 做性价比最高的自动化测试","article_content":"<p>你好，我是柳胜。很高兴通过这个专栏和你探讨自动化测试。</p><p>先简单介绍一下我自己，我在摩托罗拉和甲骨文做过开发工程师、测试工程师、测试经理和高级开发经理。从B端的企业应用测试到C端的云计算开发，我都做过。在甲骨文工作的13年时间里，我带领团队设计、开发了Automation Center框架，补全了视频会议二十多年来的自动化测试解决方案空白。</p><p>相比开发，我在自动化测试领域里得到的收获最多，教训也最多。因此，我把这些踩坑的血泪史总结沉淀，期待通过专栏的形式分享给你，帮助你在自动化测试的职业工作中，建立全局思维，找准工作的焦点，让手中的测试项目事半功倍。</p><h2>自动化测试的终点是什么？</h2><p>刚入门的自动化测试工程师，很容易陷入到工具和框架的汪洋大海里。的确，从代码静态扫描到单元测试、API测试、系统测试、性能测试，每个细分领域的工具都不少。</p><p>但是，工具之后呢？</p><p>从普通开发到架构师，软件开发的职业体系清晰可见。测试则不然，我观察过业界自动化测试人员的职业发展路线，有的公司有自动化测试架构师这个职位，有的公司设立了测试架构师，自动化架构算作测试架构师的职责之一，更多的公司根本就没有自动化测试的高级角色，各个产品线测试和开发混在一起玩。</p><!-- [[[read_end]]] --><p>我也问过测试业界不少朋友，你的公司为什么要自动化测试？预期的效果是什么？</p><p>答案是五花八门，有的说是节省手工开支，有的说是业界趋势，还有的说我们研发领导非常重视，干就是了。在团队中，基本认识都出现了这么大的差别，后面的乱象就出来了，项目投入不稳定，可多可少跟着感觉走。自动化测试人员严重缺乏表达自己工作价值的话语权，被迫和开发人员一起内卷技术工具。</p><p>经历了这些，你甚至怀疑自动化测试工作的尽头就是工具，随后一边“内卷”，一边在忙碌中更加迷茫。其实，<strong>要想成为高手，就必须要看到并解决更有价值的问题，对更高的结果负责</strong>，对应的职位和收入会随后而至，它可能叫自动化架构师，也可能叫测试专家，或什么ABC。</p><p>既然说到“更高的结果”，我们就有必要以终为始，先想想自动化测试的最终结果是什么？换个问法，自动化测试的最终交付价值是什么？</p><ul>\n<li>是自动化跑起来么？这个要求太初级了。</li>\n<li>是领导满意么？我见过很多案例，一个自动化测试项目因一个领导的支持而发起，但成也萧何，败也萧何，往往也因为换了一个领导，项目就半途而废。</li>\n<li>是100%自动化么？理想很丰满，现实很骨感，高度自动化并不会一定带来高质量，我也见过开发人员为了达到100%单元覆盖率，就写一个Test函数，把程序运行起来了事，有的连一个检查点都不做。</li>\n</ul><p>所有这一切，都让我深深意识到，无法清晰认知自动化测试的价值，测试工作就会举步维艰。在我看来，<strong>自动化测试项目的最终交付价值是它产生的效益，也就是投入回报率比ROI。一个成功的自动化测试项目必然是获得了高ROI的收益</strong>。自动化测试高手就是要做出成功的自动化测试项目。</p><h2>怎样成为高手？</h2><p>明确了目标，我们还需要找到高效的实现路径。</p><p><strong>我对自动化测试架构师的定义是，不仅仅是写代码让自动化测试跑起来，而且能够超脱于工具框架的层面，对测试需求和自动化ROI一起抽象建模，对自动化测试项目的最终ROI负责。</strong></p><p>为了达到这个目标，有两种学习路径。</p><p>第一是升级打怪型。</p><p>先是提高代码能力，学习编程、操作系统和数据库。这个的确很重要，尤其对于刚入门的朋友，首先搞一个Hello World程序，有个感官的体验，后面再逐步深入。</p><p>然后是工具能力，使用各种工具和框架，Xunit系列、API测试框架、系统测试工具，编写自动化测试案例，运行、出报告，之后和DevOps pipeline集成。</p><p>最后是架构能力，熟悉测试需求和技术架构，设计自动化测试整体方案和技术路线，能够选型工具和框架，搭建测试基础设施。</p><p><img src=\"https://static001.geekbang.org/resource/image/92/16/924b3cec32fba5d78f2e30c072ab1216.jpg?wh=1920x868\" alt=\"图片\" title=\"学习路径1：升级打怪型\n\"></p><p>这是我们熟知的自底向上学习路线，是认知方法的归纳法。见过的鱼多了，就知道鱼是什么了，逐渐积累捕鱼的经验。这个方法的优点是门槛低，缺点是耗时，周期长。</p><p>第二种是航海指南型。</p><p>和第一种方法相对应，另外一种认知学习方法是自顶向下，属于认知方法中的演绎法。从道术器三个层面从高到低推进，每一步都有自己的逻辑。</p><p>知道什么是鱼和它的游动规律，就相当于带着导航去捕鱼，甚至你还可以发明新的捕鱼工具。这个方法优点是速度快，但需要你自带脑袋瓜，跟着我一起思考。</p><p><img src=\"https://static001.geekbang.org/resource/image/b2/f7/b2785904147099d03e217520ed2a21f7.jpg?wh=1920x1080\" alt=\"图片\" title=\"学习路径2：航海指南型\"></p><p><strong>专栏中我们采取的学习方法是以自顶向下为主，自底向上为辅。</strong>通过整个专栏的学习，你将系统了解自动化测试的道、术、器，同时收获一种颠覆性的认知，跳出工具和框架的层面重新审视自动化测试设计，知道自己想要什么样的自动化测试架构。</p><p>自动化测试道的部分，主要是逻辑和常识，你不需要有工作经验和技术，也能听得懂，我期望你会和我一起演绎思考，就是说，这些方法如果应用到我的工作，会怎么样。</p><p>术的部分会涉及度量数据分析、代码逻辑和Job建模，这个也对应着软件开发里的数据、算法和建模。我会在GitHub上创建一个<a href=\"https://github.com/sheng-geek-zhuanlan/autmation-arch\">repo</a>（随课程进展陆续更新），放入专栏所讲到的整体代码和相关文件，希望你能动脑思考，动手运行代码。手脑结合，学习效果会更好。</p><p>器，也就是工具和框架。在第三讲里会列出业界主流工具框架以及选择策略和落地实践。在每个模块里也会穿插一些具体的落地案例，介绍相应的工具和代码相关例子。</p><p>不过，本专栏里具体工具和代码的篇幅不会超过20%。我这样克制，是因为这些东西网上搜搜也能免费获取。我会在最后附上全栈自动化测试工具列表，从单元测试到性能测试相关的网站地址。相信你通过自学，就能掌握个七七八八。</p><p>授之以鱼，不如授之以渔。我不希望你一下子扑到工具技术的茫茫大海里，等过几年之后，有一种学不完、学不精、用不好的绝望，这都是我曾经历过的。如果能再来一次，我更愿意早点了解鱼的规律，带着导航驶入大海，有方法地探索，最后满载而归。</p><h2>这门课如何安排</h2><p>课程一共分成了四大模块，分别是：价值篇、策略篇、设计篇与度量篇，自动化测试的道、术、器会贯穿其中。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/20/2cc1921668ebda517bf9ab098db23f20.jpg?wh=1920x845\" alt=\"图片\" title=\"专栏模块设计\"></p><p><strong>第一模块价值篇</strong>会带你重新审视自动化测试的基本概念和规律，掌握自动化测试效益的量化思维方法——<strong>投入产出比ROI模型</strong>。它是自动化测试项目成长的DNA，也是隐藏的命脉，在工作中紧紧抓住它，效果就来了：自动化测试使用的场景越来越丰富，越来越稳定和可信，交付发布的速度越来越快。</p><p>这些都可以帮你在述职报告中，用ROI的方式表达业绩，比如：“老板，我做的自动化测试案例，去年一年被n个场景使用，重复运行x次，发现bug y个，节省手工工作量z人月”。</p><p>ROI如何落地呢？我们会从立项、设计、代码、运营各个阶段逐步深入解读。</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/f3/dcba05528c731a2192abd4c31cee9af3.jpg?wh=1920x913\" alt=\"图片\" title=\"四测试阶段ROI落地示意图\"></p><p>通过这个模块的学习，你会获得一个新的思维角度，用它来审视自动测试项目，判断项目中哪些是过度工作，哪些是工作做得还不够。同时你也掌握了一门和管理者和团队沟通的语言，在述职、评审等交流环节，能把自己工作的价值表达清楚。</p><p>到了<strong>第二模块策略篇</strong>，我们会从一个订餐系统的例子出发，从单体应用升级到微服务集群，来观察测试需求的变化。针对微服务集群关系复杂、依赖多的挑战，建立起多层测试策略：单元测试、集成测试、接口测试、契约测试、UI测试和验收测试，通过逐层测试来全面验证需求。</p><p>每一种测试策略，我会用一讲的篇幅来讲述它的方法论和工具，能做什么、不能做什么，如何设计自动化测试案例。</p><p>在<strong>第三模块设计篇</strong>，我们一起推演模型设计。像开发的设计模式一样，自动化测试设计也应该有自己的方法论。</p><p>这里我提出的微测试Job模型，也是业界首创，会让你耳目一新。在这个Job模型里，没有TestSuite和TestCase的概念，也没有具体工具和框架的依赖，而是面向测试需求和自动化测试ROI要求设计。它可以帮你厘清测试的场景、工作流、需要代码实现的案例原子，同时内建自动化测试ROI需求，这正是自动化测试设计阶段需要关注和要做的事情，对吗？</p><p>基于Job模型的设计产出是一个XML的树形结构文档，描述了我们的自动化测试任务，基于这个需求，我们的工具选型就会更加科学。<strong>一个测试案例，A工具和B工具都能做自动化，那它们都可以去实现需求，将来它们被替换成C工具，对其他案例没有影响，我们的自动化测试设计也依然保持不变。这就保证自动化测试项目有了持续重构优化的能力，而不是走向腐化。</strong></p><p>同时，我给出了3个案例，分别针对领域业务型金融交易自动化测试设计，DevOps型持续集成pipeline设计，分布式型的复杂场景视频会议系统自动化测试设计，帮助你理解怎么使用Job模型来设计不同的自动化测试场景。</p><p>掌握了怎么把一个自动化测试项目送入到它的运行轨道，就可以坐享ROI的红利了么？不，还不够，你还要考虑怎么让这个项目始终可观测、可控，有反馈，这样就能保证这个项目始终在预定轨道上推进，即使有偏离，也能第一时间发现纠正回来。在<strong>第四模块度量篇</strong>，我还会提供一些度量模型和驱动改进的流程样例，供你参考实践。</p><p>另外，本专栏提出了不少新的方法论，3KU测试金字塔（第二讲），Job模型（第十七讲），而且是业界第一次出现，你刚看到不一定会很快适应。有时可以多看几遍，我相信你每次看都会有不同的收获。</p><p>马斯克最近说过一句话，我对此很有感触，也分享给你：</p><blockquote>\n<p>死亡对我们来说很重要，因为大多数时候人们不会改变主意，他们只是死去。如果人们长生不老，我们可能会成为一个非常僵化的社会，导致新想法无法成功。</p>\n</blockquote><p>我思故我在，思维的碰撞是痛苦的，也是快乐的。期待你加入我的自动化测试专栏，一起扬帆探索自动化测试的深海区！</p>","neighbors":{"left":[],"right":{"article_title":"01｜ROI价值内核：自动化测试的价值可以量化么？","id":496857}}},{"article_id":496857,"article_title":"01｜ROI价值内核：自动化测试的价值可以量化么？","article_content":"<p>你好，我是柳胜。</p><p>作为测试人员，我们都想做好自动化测试，但是每个行业都有自己的规律，也就是说常说的道，自动化测试也有自己的道。所以，在这个模块，我们的目标是了解自动化测试的道是什么，怎么能运用它让自己的测试工作更加有成效。</p><p>今天是价值篇的第一讲，我们先来弄清楚自动化测试的价值究竟是什么？看到这你可能有点困惑，自动化测试有那么多公司都在搞，自然是有价值的啊，有啥可讨论的呢？</p><p>其实这个问题非常关键，在开始工作之前，要把我们的工作价值想清楚，后续工作才能事半功倍。我列几个工作中我们频繁听到的问题，你会更有感触。</p><ul>\n<li>（上级沟通）“产品要上线了，QA人手紧，能不能搞一下测试自动化，减少点人手？”</li>\n<li>（调动人手）“什么？你还要再增加2个自动化测试开发工程师来完成这个项目，他们都要做什么？”</li>\n<li>（工作述职）“听说你开发了个什么自动化脚本，它给公司带来了什么价值？用量化的数据给我讲一讲！”</li>\n</ul><p>这样的问句是不是似曾相识？其实它们都指向了一个硬核问题“自动化测试项目的价值是什么？”</p><p>在这节课，我要和你捋一下，为什么要做自动化测试，并且带你找到度量它价值的方法。掌握了这些，就能对自己的工作目标更清楚、更有信心，别人问到的时候，我们也能讲清楚、说明白，得到了团队理解工作将事半功倍。</p><!-- [[[read_end]]] --><h2>为什么要搞自动化测试</h2><p>开篇词中我提了出海捕鱼的场景，不只自动化测试，整个测试工作就像织网一样，会有弹性的空间，网眼大了、小了，捕到多少鱼，这些都有不确定性，但是这个不确定性又关系到成本和收益这些敏感问题，这是测试工作的一个特点。</p><p>我曾经跟我的团队说过 “咱们做测试工作，甭管用什么方法和技术，目标就是用最小的成本，得出对软件质量最大的确定性结论”。</p><p>自动化测试也面临相同的困难，为了解决这样的不确定，我们有必要好好分析一下，自动化的成本和收益究竟怎么算？</p><p>如果感觉这样问还有些抽象，我们不妨换个问法，自动化测试实施之前和之后，自动化带来的改变是什么？为了进一步完善思路，我们结合一个更具体的例子来做推理、估算。</p><p>这个例子是：一个Web UI 订火车票的软件，成功订一张火车票这个测试案例，要做自动化所花费的成本、还有得到的收益，会是多少呢？</p><p>自动化实施之前，测试案例的执行要靠手工完成，一个工程师需要花费0.5个小时，运行完登录、订车票，查看数据库这样一个测试流程。</p><p>而自动化测试实施之后，流程可以用Selenium脚本自动完成，原先手工测试半个小时的工作量就省下来了。那么，省下来的这半个小时这就是自动化测试带来的价值。对不对？</p><p>对，但还不全面，我们衡量效益，不应该只看回报，还要看成本，我们要算上开发自动化测试花费的成本。为了开发Selenium订火车票的这个脚本，自动化测试工程师花费了1天的时间，合计就是8个小时。</p><p>现在，这个Selenium自动化测试案例，它的投资收益比ROI应该这么计算（产出和投入都用时间作为单位）：</p><p>产出/投入 = 0.5/8= 0.0625</p><p>结果不到7%。哇，投入了8个小时，才收获了0.5个小时。如果这是一项投资的话，那肯定是亏本的。哪个公司愿意做这样的买卖呢？</p><p>但是，上面的公式只计算了运行一次自动化测试案例的ROI。实际上，自动化测试案例开发出来后，肯定不止运行一次的。多运行一次，就会多节省下来一份工作量，如果用n来指代运行次数，t指代单次测试时间，现在的产出变成了n*t，n越大，产出就会越大。</p><p>那上面订火车票案例，运行多少次才能收回成本呢？1/0.0625=16，只要这个selenium脚本运行超过16次，我们就可以让ROI=1，收支平衡，收回成本了。</p><p><img src=\"https://static001.geekbang.org/resource/image/7f/82/7fa80bffaae0f230411b67310e9ca582.jpg?wh=1900x801\" alt=\"图片\" title=\"运行次数&amp;ROI关系示意图\"></p><p>太棒了，现在你就可以对公司说：“我的自动化测试收益是可以量化的，只要我保证开发出来的脚本，能运行超过16次，就是为公司省钱了。”</p><p>且慢，还有一笔账没有算，除了开发成本，还有维护成本。自动化测试开发出来后，还需要维护版本升级、诊断错误、优化结构等等的工作，这笔成本是需要持续投入的。</p><p>现在这个Selenium在它线上运营生命周期内的ROI计算公式，变成了后面这样：</p><p>产出/投入 =  0.5*N/(8+维护成本）</p><p>我们把它提炼成一个计算公式，就是：</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/00/cd34280bc70b3633e696a7ba16f9e300.jpg?wh=1920x868\" alt=\"图片\" title=\"ROI计算公式\"></p><p>这个公式很简单，但仔细揣摩可以推导出几个有意思的结论。</p><p><strong>1.ROI大于1就是赚了，小于1就是亏了。</strong>那么，给定一个测试案例，要不要对它做自动化，判断的依据是（自动化测试）预期ROI至少要大于1。</p><p><strong>2.自动化测试是一个长收益模式。</strong>在理想情况下，是一次性投入（投入为开发成本），之后每运行一次，就会增加一份产出。所以，时间越长，次数越多，收到的回报就会越大。</p><p>3.关于开发成本（包括开发成本d和维护成本m），类似估算软件开发工作量，代码行法、功能点法，我们也可以引入到估算开发工作量里，比较好掌握。<strong>但维护成本就有点模糊了，这里包含了多种可变因素，是自动化测试项目风险的主要来源。</strong></p><p>维护成本来自于多个地方。一段代码从产生以后，就会持续产生维护工作量，而且，因为存在架构腐化等问题，维护工作量增加速度是以非线性来增长的。</p><p>到了最后，一个陈旧的老破系统，加入一个新功能需要写10行代码，只要花5分钟。但是搞清楚这10行代码，应该加到哪个文件里，要花费3天时间。在这种时候，这个软件系统就已经不可维护了，它要寿终正寝了。自动化测试代码的维护成本更复杂，不仅面临着腐化的问题，还有被测产品更新带来的维护等等。</p><p>所以，在实践中，你看不到前面图里，那样简单漂亮的ROI直线，它会表现为一段曲线：自动化的ROI增长速度，要比运行次数增长慢一些，直到最后，每运行一次，花费的维护工作量，比节省的工作量还多，自动化就该退休了，也就是下线，重构完了再上线。</p><p><img src=\"https://static001.geekbang.org/resource/image/73/4d/73e3da46cce6542dceda6ddee0a5e54d.jpg?wh=1900x801\" alt=\"图片\" title=\"运行次数&amp;ROI关系示意图2.0版\"></p><p>这里我想提醒你注意，<strong>ROI模型提供的是一种自动化测试投资收益比的量化思路，方便我们明确哪些因素影响着自动化测试效益。</strong>不可能存在一个万能的公式，把参数往里一带，就会算出ROI的数字如果世间的事都这么简单，还需人类干什么。我们需要做的是尽可能量化，你对量化的了解越多，对自动化测试的理解就会更加深入全面。</p><h2>做不做自动化测试，能用数据说话么？</h2><p>从ROI的公式来看，自动化测试的收益取决于t和n，t指的是节省下来的手工工作量，还是比较容易理解的。在字面上，n是一份自动化测试案例重复运行的次数，那么在实践中，n是什么呢？</p><p>聪明的你可能已经猜到了，n是测试案例的稳定回归次数。软件的新功能开发出来后，第1次测试之后，第2次，第3次到第n次，都是对第1次的回归。它们都是重复的工作，应该被自动化替代。</p><p>你看，从ROI公式，我们很容易推导出业界熟知的经验<strong>“自动化测试是用来做回归测试的”</strong>。自动化是开发出来不会只运行一次，除非它的t特别大，实现了手工测试做不到的事情，比如单元测试、性能测试。</p><p>我们再回到回归测试，n作为回归测试次数，对自动化测试工作有什么启发呢？它的作用很大，因为它能帮助我们量化地去回答一个 “做不做自动化测试” 的关键问题</p><p>一个测试案例A做不做自动化测试？首先要看看它的n能有多大。</p><p>假设软件发布周期持续一年，每两周迭代一次，每次迭代都需要一次测试，那么在这一年里，需要回归测试次数至少是365/14=26次。如果还考虑一些紧急feature、patch的发布，那实际的回归次数要大于26次。这样，我们就能得到一个n的估算值，比如说30次。</p><p>得到估算值后，你的决策不再依赖直觉，而是有了可量化的思考逻辑。30次能不能收回来成本？能！那这个测试案例A就可以搞自动化。不能，你就面临亏本风险，自动化一顿操作猛如虎，测试工作还是苦，这是项目组里的每一个人都会感受到的。</p><p>刚才说的是基于软件发布周期不变的情况下，如何估算回归次数n。在实际工作中，自动化测试一旦做起来，带来的变化是：测试执行时间变快，软件发布周期缩短，又反过来增加回归次数n，自动化测试的收益也在增加。</p><p><strong>这里我们又得到一个结论：软件发布周期变短是自动化测试ROI提升的产物。</strong></p><p>总结来说，只要我们把注意力关注放在ROI上，后面的好处都会相继而来，测试质量提高了，发布周期缩短了，团队也更加有信心了。</p><h2>实践中，冒烟测试是你自动化的开始</h2><p>紧接着，咱们再考虑下一个问题，测试案例A需要30次回归，是不是在刚引入新功能A的第1次迭代，就开始运行自动化？我的答案是要根据情况来判断。</p><p>上面说到n是测试案例的稳定回归次数，注意<strong>稳定</strong>这两个字，它代表功能A已经稳固下来，不再变了。更精确地说，功能A即使有变化，但是变化规律已经可以被自动化测试吸纳。这种情况下，自动化测试运行才能发挥效益。</p><p>这里你可以看看后面这张图，画的是加特纳的技术成熟曲线，它也可以用来描述软件功能的发展过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/2f/e6/2fcb06174dca22b246630c5b5a5379e6.jpg?wh=1920x921\" alt=\"图片\" title=\"加特纳技术成熟度曲线\"></p><p>通过加特纳成熟度曲线可以看出，新功能在产生初期，一般是不稳定的，和它的预期有一个差距。经过几轮调整后，才会进入到一个平缓的阶段，这也是稳定回归测试的阶段。而不同类型的软件，它的功能成熟时间长短，变化剧烈程度可能是非常不一样。</p><p>有的软件是做标准化产品的，比如专业性强的B端财务软件，计税模块发布出来就很稳定，我们采取的策略是在第1个版本做计税模块的自动化。</p><p>有的互联网软件第1版是投放试验性的，我看过国外一个招聘网站经过产品设计，AB测试多轮后，打磨了x版才稳定下来简历模块，那么这时的策略是在x版本进行简历模块自动化。</p><p>还有的生命周期比较短的软件项目，虽然有迭代，但功能一直无法稳定，那可能需要考虑完全手工测试，根本不需要自动化测试。其实这些都可以通过ROI模型讲得通。</p><p>到这里，咱们总结一下我们通过ROI得出的三个核心观点：</p><p>1.自动化测试是用来做回归测试的。</p><p>2.自动化测试从哪里开始？实施顺序从ROI高到低，也就是（给定一个软件系统），优先做回归次数最高的那部分功能，先做自动化回归次数最高的案例，再做低的，直到ROI等于1的案例。在功能模块的初期，可以考虑先做手工测试。</p><p>3.自动化测试什么时候开始？功能模块稳定的时候。</p><p>实际上，有一个很好的测试实践可以匹配上面的要求，那就是冒烟测试。冒烟测试是测试用例的子集，用来验证系统中基础的、影响发布软件的功能。甄选冒烟测试的一个常用办法就是二八原则。</p><p>二八原则又叫帕累托原则，在因果关系中，仅有20%的因素会影响80%的结果。它在各个领域都有体现，比如在市场营销领域，80%的利润是由20%的用户创造的；在经济学里，80%的财富掌握在20%的人的手里。</p><p>在软件领域，80%用户，常用的是系统中20%的功能。冒烟测试覆盖的这部分20%功能，是常用的，一般也是核心的，最先被开发出来的。所以，它同时满足稳定和回归次数高两个特点。</p><p>进而我们就可以得到推论：<strong>在实践中，可以设定目标，冒烟测试100%自动化。</strong>这时，自动化测试就可以和手工测试配合，形成一个新版本发布+冒烟测试的简单流水线。</p><p>如图所示，先从代码管理工具比如CVS、Gitlab中的开发分支中拉取代码，build构建，做一轮冒烟测试。如果冒烟测试通过，开发分支可以merge到发布分支，如果冒烟测试失败，那开发人员必须修改代码，直到冒烟测试通过。</p><p>这个流水线Pipeline充分利用了自动化无人参与，执行速度快的特点，可以帮助开发人员在第一时间验证代码的正确。由于是每次分支归并都会调用冒烟测试，所以n的次数高，自动化测试的ROI也会高。</p><p><img src=\"https://static001.geekbang.org/resource/image/fe/62/fe138ec5e6513c05e111f72328414962.jpg?wh=1920x726\" alt=\"图片\" title=\"新版本发布+冒烟测试流程图解\"></p><h2>小结</h2><p>今天这一讲，我们通过一个投资产出视角来观察自动化测试，它的成本是什么，它的产出是什么，还学习了ROI的计算公式。</p><p>我们通过ROI的收益规律，不仅可以推导出自动化测试业界的已有共识，比如：“自动化是用来做回归测试”，“冒烟测试优先做自动化”……而且，我们还能挖掘出一些新的合理观点，比如：“ROI从高到低，来做自动化测试”。</p><p>这说明业界的实践已经有意或无意地践行ROI规律，因此可以说，<strong>ROI是一个自动化测试项目的隐式命脉</strong>。</p><p>同时，我们又详细介绍了ROI公式的因子n，测试案例的回归次数。在实践中，找到n来估算ROI，能帮你判断一个案例该不该做自动化。</p><p>ROI公式里，除了n还有其它因子。在后面的课程中，我们再一一介绍其它因子，像m维护成本，现在这个概念看起来还有点模糊，我还会帮你把它分解，直到可操作和可度量的粒度，让ROI的方法论更有效地指导你的工作。</p><h2>思考题</h2><p>学习ROI之后，你可以从开篇的三个问题里选择一个或多个，试着回答一下。</p><ul>\n<li>“产品要上线了，QA人手紧，能不能搞一下测试自动化，减少点人手?”</li>\n<li>“什么？你还要再增加2个自动化测试开发工程师来完成这个项目，怎么算出来的？”</li>\n<li>“听说你开发了个什么自动化脚本，它给公司带来了什么价值？用量化的数据给我讲一讲。”</li>\n</ul><p>欢迎你在留言区记录你的收获和思考。如果这一讲对你有启发，也可以推荐给身边更多同事、朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"开篇词 | 做性价比最高的自动化测试","id":496850},"right":{"article_title":"02｜3KU法则：如何找出最优自动化实施截面？","id":497405}}},{"article_id":497405,"article_title":"02｜3KU法则：如何找出最优自动化实施截面？","article_content":"<p>你好，我是柳胜。</p><p>上一讲我们提出了自动化测试ROI模型，在回归测试中的应用。回归测试是一个笼统的概念，单元测试、接口测试以及UI测试里都有回归测试，甚至性能测试也已经成为回归测试的一部分。</p><p>今天我们要关注一个具体场景，给你一个软件系统，作为自动化测试人员，你怎么找出测试截面，制定自动化测试方案？这些事可能你都做过，觉得并不稀奇，但既然我们已经学习了ROI思维，今天要再加上一个小目标，<strong>制定策略，能够让这个自动化测试设计获得尽可能大的ROI</strong>。换句话说，能干还不够，还要干得好，既要马儿跑，又要马儿少吃草。</p><p>有挑战不？那就跟我进入这一讲的学习，一起找到最佳策略吧。</p><h2>测试ROI金字塔</h2><p>在测试设计领域，经常提到的方法是分层。具体就是给定一个系统，结构上划分三个层级，单元在最小圈；服务包含多个单元，在中圈；而系统又包含多个服务，是外部的最大圈。结构图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/77/c3/7713a7081ac2723a2cfc35d3277b21c3.jpg?wh=1920x1050\" alt=\"图片\" title=\"软件结构圈图\"></p><p>相应地，我们的测试结构是在代码层做单元测试，服务层做接口测试，系统层做UI功能测试。</p><p>在实践中，这三种测试该怎么组合安排呢？迈克·科恩在2009年他的新书《敏捷成功之道》中首次提出了测试金字塔模型。单元测试自动化在金字塔底部，接口测试自动化在中部，而UI测试自动化在金字塔顶部。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/bd/68/bdyy34691cc7a36c8e50f13e3bbaca68.jpg?wh=1920x1050\" alt=\"图片\" title=\"分层测试金字塔\"></p><p>迈克·科恩讲到自动化测试工作量配比时，认为应该按照层面积分配。也就是说，单元测试案例的数目应该多于接口测试案例数目，接口测试案例数目应该多于UI测试自动化测试案例数目。</p><p>后来，金字塔模型又被业界发展，赋予了不同的测试策略，比如自底向上执行速度减慢，自顶向下业务属性减弱，技术属性增强。</p><p>但迈克·科恩没有解释，为什么各层工作量配比要按照测试金字塔分布？按照软件结构图，系统在最大圈，测试案例应该最多，而到了自动化测试金字塔，UI自动化测试案例却最少；单元测试在小圈，测试案例应该最少，但到了自动化测试金字塔，单元测试案例却最多。</p><p>为什么是金字塔？要是不去理解规律背后这个“为什么”，你就用不好这个规律。上一讲我们知道了“ROI其实是自动化测试的隐式命脉”，现在我们就利用ROI思维，分析一下测试金字塔规律。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/00/cd34280bc70b3633e696a7ba16f9e300.jpg?wh=1920x868\" alt=\"图片\" title=\"ROI公式\"></p><p>下面，我们分别看看每层的ROI。单元测试可以在开发人员每次code commit触发运行，回归频率高；接口测试在每轮集成测试运行，回归频率中；UI自动化测试在用户验收测试，回归频率低。</p><p>按照ROI模型，我们可以得出3种类型自动化测试的ROI排序，如下表：</p><p><img src=\"https://static001.geekbang.org/resource/image/71/08/713a743c26347d08d561d5a77ab27608.jpg?wh=3363x1379\" alt=\"\"></p><p>对照测试金字塔不难发现，实际上三类自动化测试的ROI是自底向上由高到低的。</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/af/7ebe91f53e1fc7a84e53a26d68c4baaf.jpg?wh=1920x1050\" alt=\"图片\" title=\"分层测试ROI金字塔\"></p><p>按照第一讲得出的规律“自动化测试顺序从ROI高到低”，我们优先投入精力做ROI最高的单元测试，再做ROI中的接口测试，最后完成UI测试。</p><p>现在就可以轻松解释迈克·科恩的金字塔了，因为ROI存在差异，所以按照高ROI大投入，中ROI中投入，低ROI小投入，工作量比例呈金字塔分布，底层面积最大，顶层面积最小。发现没？<strong>根源在于ROI，金字塔是表现出来的形态而已</strong>。</p><p>好，到这里，总结一下。各种软件理论学派，大致可以分为两种，一种是理论基础，讲的是做什么，比如软件测试定义、软件过程，另外一种是实践经验，讲的是该怎么做，比如金字塔模型。</p><p>实践和理论很大的不同就是在现实商业中，我们不可能完全按照理想来工作，而是要加入很多制约因素，其中最大的制约就是钱。明白这个道理，你就会知道为什么ROI是根源，你也会知道怎么能够在工作中做出业绩了，不是耍两个工具，忽悠一下领导就算成功，而是认认真真地去思考，踏踏实实地去提高ROI，直到边际效应ROI无法提高为止。</p><h2>寻找最优ROI策略</h2><p>刚才说了分层测试和各层ROI，业界也很认可这种分层理论，但实际落地时却存在问题：一批人做UI测试自动化，另外一批人去做接口测试，然后开发人员做单元测试。三路人马忙得不亦乐乎，都说自己贡献大，等到bug发生了泄漏到生产环境，又开始甩锅。</p><h3>分层测试为啥会“内卷”</h3><p>很明显，这是一个内卷的场景，让我们结合例子具体看看内卷发生在哪里？</p><p>以一个Web登录操作为例，用户在UI上输入用户名和密码，点击“登录”按钮。Selenium UI 自动化会这样实现：</p><pre><code class=\"language-plain\">@Test\npublic void login() {\n  WebDriver driver=new ChromeDriver();\n  driver.manage().window().maximize();\n  //打开页面\n  driver.get(\"https://www.example.com/users/sign_in\");\n  WebElement username=driver.findElement(By.id(\"user_email_Login\"));\n  WebElement password=driver.findElement(By.id(\"user_password\"));\n  WebElement login=driver.findElement(By.name(\"login\"));\n  //输入用户名\n  username.sendKeys(\"liusheng@example.com\");\n  //输入密码\n  password.sendKeys(\"123456\");\n  //点击登录按钮\n  login.click();\n}\n</code></pre><p>上面UI的操作被Web服务转化成Rest请求，进入到API网关，是这样的：</p><pre><code class=\"language-plain\">curl --location --request POST 'http://auth.example.com/auth/realms/Test/users' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--header 'username=liusheng@example.com' \\\n--header 'password=123456'\n</code></pre><p>在单元上执行的则是这样的代码：</p><pre><code class=\"language-plain\">public Future&lt;ResponseData&gt; login(String userName, String password) {\n    //入口参数检验\n    if (StringUtil.isBlank(userName)||StringUtil.isBlank(password)){\n      return new AsyncResult&lt;&gt;(ResponseData.error(\"账号密码不能为空\"));\n    }\n    //查询用户是否存在\n    List&lt;User&gt; userList = baseMapper.getUserInfo(userName);\n    if (CollectionUtils.isEmpty(userList)){\n        return new AsyncResult&lt;&gt;(ResponseData.error(\"账号不存在\"));\n    }\n    //验证账号密码是否正确\n    User user = userList.get(0);\n    String requestMd5 = SaltUtil.md5Encrypt(password, user.getSalt());\n    String dbMd5 = user.getPassword();\n    if (dbMd5 == null || !dbMd5.equalsIgnoreCase(requestMd5)) {\n        return new AsyncResult&lt;&gt;(ResponseData.error(\"账户密码不正确\"));\n    }\n    //生成 access token，并返回\n    String token = JwtTokenUtil.generateToken(user);\n    return new (ResponseData.success(token));\n}\n</code></pre><p>可以看到，一个请求，从浏览器页面发起，进入API网关，再传递到服务里的Login函数，经过了UI测试、API测试和单元测试三个测试截面。</p><p><img src=\"https://static001.geekbang.org/resource/image/0a/ce/0a1af39ca343083b14fe1472c5610cce.jpg?wh=1920x1045\" alt=\"图片\" title=\"三个测试截面示意图\"></p><p>三个测试截面测的是一个请求在不同层面上的形态，那么每一个截面都可以测试全部的案例，也可以测试部分的案例。就像3个人负责1个项目一样，如果没有经过事先的协调和安排，3个人可能做了重复的事情，造成浪费，也可能存在一件事3个人都没干，形成测试盲区。</p><h3>需求/策略矩阵</h3><p>这种“内卷”是不是一个问题？可能你会说没问题，各层独立测试能够加强质量保障。说这话的底气在于测试上的投入充足，不计内卷成本。实际上，在DevOps风行的今天，趋势是追求效果和效率。所以，在资源有限的条件下，我们需要在整体上看待分层测试的最优ROI。</p><p>咱们先看看测试需求是什么，用 <a href=\"https://zh.wikipedia.org/wiki/FURPS\">FURPS模型</a>来理一下需求。FURPS是用5个维度来描述一个软件的功能需求，FURPS这个单词对应着每个需求的英文首字母：</p><ul>\n<li>F=Function 功能</li>\n<li>U=Usability 易用性</li>\n<li>R=Reliability 可靠性</li>\n<li>P=Performance 性能</li>\n<li>S=Supportability 可支持性</li>\n</ul><p>把测试需求和测试类型组合在一起，就整合了后面这个矩阵表格：</p><p><img src=\"https://static001.geekbang.org/resource/image/44/21/443b6845a599a19d27704a3f89b44b21.jpg?wh=4000x1410\" alt=\"\" title=\"3KU测试矩阵\"></p><p>结合表格，可以看到UI测试、接口测试和单元测试每个截面的测试能力。</p><ul>\n<li>在UI层面上，功能性最强，所有测试需求都可以做。这个可以理解，因为软件本身就是满足用户需求，没有一个需求不可以从用户层面感受到。如果真的存在一个需求，用户却无法体验到，那根据<a href=\"https://zh.wikipedia.org/wiki/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80\">奥卡姆剃须刀原理</a>，这种用户无法体验到的需求就是无效的。</li>\n<li>接口层面上，功能性减弱，技术性增强。</li>\n<li>单元层面上，技术性最强，功能性主要体现在数据的处理，算法逻辑上。</li>\n</ul><h3>3KU整体策略</h3><p>好，有了需求/策略矩阵后，结合上面讲到的自动化测试ROI金字塔，我们的整体最优ROI策略就呼之欲出了。什么是整体最优ROI呢？</p><p>有3个Key（关键因素）：</p><ul>\n<li><strong>U</strong>seful:  每个测试需求都是有效的；</li>\n<li><strong>U</strong>ltimate:  每个测试需求的验证都在优先寻找自动化ROI高的层面去实现，如果不可行，按照ROI高到低回退，直到UI层；</li>\n<li><strong>U</strong>nique: 每个层面上验证的测试需求都和别的层面都不是重复的。<br>\n这样分配的工作，既不重复，又没遗漏，还遵循了ROI的原则。我管它叫<strong>3KU原则。</strong></li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/7e/af/7ebe91f53e1fc7a84e53a26d68c4baaf.jpg?wh=1920x1050\" alt=\"图片\" title=\"3KU测试金字塔\"></p><p>3KU策略该怎么执行呢?按照3KU策略，我们把表格里的测试需求，对照下面这三个问题，按顺序检查一遍：</p><p>1.能在单元测试验证么？<br>\n2.能在接口测试验证么？<br>\n3.能在UI测试验证么？</p><p>这样检查以后，就能得出各个需求的自动化实现截面了。</p><p>UI测试关注功能场景测试，易用性测试和可执行性测试；而接口测试关注不同数据的循环，接口的性能和错误恢复能力；单元测试关注算法的正确性和性能。</p><p>恭喜你看到这里，最后就是我们收割成果的环节了。我们又得出了一个满足3KU原则的自动化测试实施金字塔，各层有自己的关注点，又在整体上实现了互相配合补偿。</p><p><img src=\"https://static001.geekbang.org/resource/image/95/a7/950bdb1892c70d0598cd0657e2ca92a7.jpg?wh=1920x1064\" alt=\"图片\" title=\"3KU测试金字塔\"></p><p>在3KU测试金字塔下，每一个测试需求都会选择最大的ROI测试截面，通过这样的安排，实现了整体最优ROI的目标。对不对？</p><h2>小结</h2><p>这一讲，我们从ROI角度分析了一下分层测试的原理和在实践中的应用。先入为主地，分层理论上的分层测试的特性，必然会造成重叠和错失。这给测试从业者带来了挑战。但挑战也是机会，如何解决这个问题？</p><p>这就需要我们遵循回归到效益的原则，思考怎么用最少的资源干最多的事，能达到这个效果，就是好的实践。因此，我们提出了分层但协调实现整体最优ROI的解决方案，3KU测试矩阵和3KU测试金字塔。</p><p><img src=\"https://static001.geekbang.org/resource/image/44/21/443b6845a599a19d27704a3f89b44b21.jpg?wh=4000x1410\" alt=\"\" title=\"3KU测试矩阵\"></p><p><img src=\"https://static001.geekbang.org/resource/image/95/a7/950bdb1892c70d0598cd0657e2ca92a7.jpg?wh=1920x1064\" alt=\"图片\" title=\"3KU测试金字塔\"></p><p>沿着这个思路，各层做好自己具有优势能力的测试需求，比起全部需求系于端到端的测试上，更有效率和效益，<strong>分层是追求整体ROI的结果</strong>。之后的课程里我们还会反复提到ROI，最后你也会不由感叹，ROI是背后无形的大手，大道无形，无处不在。</p><h2>思考题</h2><p>1 软件大师马丁·福勒曾经说过：“在微服务时代，分层测试不再呈现金字塔形状。”这是为什么？试着用ROI来解释一下。</p><p>2 学完今天的内容，如果你是测试主管，你希望你的团队是全栈（一个人负责一个模块的所有层面测试），还是精细分工（一个人负责所有模块的一个层面测试）？有什么优劣?</p><p>欢迎你在留言区跟我交流互动，如果这一讲对你有启发，也推荐你分享给身边更多同事和朋友。</p>","neighbors":{"left":{"article_title":"01｜ROI价值内核：自动化测试的价值可以量化么？","id":496857},"right":{"article_title":"03｜工具选择：什么工具框架值得用？","id":498458}}},{"article_id":498458,"article_title":"03｜工具选择：什么工具框架值得用？","article_content":"<p>你好，我是柳胜。</p><p>工具选型评选会你应该不陌生，无论你是作为评审者，还是方案建议人。不过常常出现的场景就是，方案建议人讲了一通新工具A如何优秀强大，从问题分析到方案解决一应俱全。但参会专家并不是全都熟悉这个新工具，就会问出一堆类似“为什么你不用工具B”的问题。</p><p>结果也可想而知，懂A的不懂B，懂B的不懂C，懂C的不懂A，最后评审会咋决策？只好陷入一个依靠个人影响力来决策的模式。要破除这种死循环，就要找到一个量化模型。在我看来，量化数据才是团队有效交流的手段，也是团队达成共识的基础。</p><p>有了这样的模型，好处多多。评审阶段，可以更科学地预估某个工具的能力和风险；工具投入使用后，模型依旧能帮你持续观测，检验这个工具是否带来了预期价值。</p><p>今天我会带你一起推演出这个模型。学会推演方法，远比套用“模型”更重要。</p><h2>自动化测试案例的成本</h2><p>在开始量化评估之前，我们先回顾一下前面学过的ROI模型。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/00/cd34280bc70b3633e696a7ba16f9e300.jpg?wh=1920x868\" alt=\"图片\"></p><p>通过这个模型，我们得到一个重要结论：<strong>一个自动化测试案例的开发工作量，在给定条件下，什么经验的工程师用什么工具，需要多长时间完成，这是可以估算的定值。但维护工作量包含了多种可变因素，是自动化测试项目的风险所在。</strong></p><p>今天我们聚焦公式里的分母，也就是开发成本d和维护成本m。</p><!-- [[[read_end]]] --><p>先说开发成本。在软件开发中，估算开发规模的传统方法有功能点和代码行，前沿的方法也有用AST抽象语法树。那如何估算开发一个自动化案例需要多大工作量呢？</p><p>首先我们要看一下自动化测试案例是如何产生的，目前在业界，主要有5种方式。</p><h3>方法一：录制和回放方法</h3><p>第一代的自动化测试工具大多基于录制回放，像最早的WinRunner就是录制桌面UI应用的。目前代表工具就是Selenium IDE。</p><p>要生成测试代码很简单，开启浏览器Selenium的插件，打开测试的网页，比如http://www.baidu.com，输入关键字“极客时间”，点击搜索，就会自动生成测试脚本了，如下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/ae/39/aec0fcf088f853f9a1b684476052bc39.jpg?wh=1920x1364\" alt=\"图片\" title=\"自动生成测试脚本演示\"></p><p>通过录制产生自动化脚本的这种方法，优点是速度快、零编码，对测试人员技术要求低。缺点是规模一旦扩大，维护工作量几乎无法承受，比如和CICD集成、自定制报告、多环境支持等等。</p><h3>方法二：关键字驱动</h3><p>不过，录制回放产生的脚本，还是面向过程的一个个函数，还需要测试人员有一定代码基础，才能扩展和维护这些函数。</p><p>那么，有没有办法，让没有代码经验的人也能编辑、维护脚本呢？关键字驱动方式应运而生了，它增加了页面控件对象的概念，调用对象的方法就是操作对象运行，在这种机制下，对象、对象的行为、输入的数据和描述信息，这些内容都能用一个表格的形式呈现。业务人员只需要编辑表格，就能修改运行逻辑了，这就叫做<strong>关键字驱动</strong>。</p><p>下面是一张关键字驱动表格，编程就是编辑表格里的关键字和对象。关键字是一组预定义好的指令，编辑完表格后，框架会驱动这个表格，执行设定好的命令，来完成自动化测试。</p><p><img src=\"https://static001.geekbang.org/resource/image/4b/9c/4bbfd55d324e056f7e78d064526a509c.jpg?wh=3552x932\" alt=\"\" title=\"关键字驱动表格示例\"></p><p>相比录制回放，关键字驱动框架的优势在于降低了测试开发人员的技术要求。而且测试开发人员对代码还有了更多的逻辑控制能力，比如增加循环结构、wait time、log输出，只要框架提供足够丰富的关键字就行。</p><p>但你也不难看出，编辑表格的人和维护关键字仓库的人，并不是同一拨人。前者是对业务了解的测试人员，后者是技术能力强的开发人员，这样开发维护起来会增加难度。</p><h3>方法三：模块库开发</h3><p>随着软件技术的发展，自动化测试人员的技术水平也在提高，要解决的问题也更加复杂。比如自动化测试的代码怎么能够有效地复用，有没有好的扩展能力等等。这个扩展能力是二维的，分为水平功能扩展和垂直层级扩展。</p><p>水平功能扩展指的是测试功能增多，自动化测试代码就借鉴了软件的模块设计思维，一个应用的测试场景可以切分成多个功能模块。比如订餐的流程可以分成登录模块、下单模块、快递模块，模块之间通过调用关系连接起来，组成测试场景。</p><p><img src=\"https://static001.geekbang.org/resource/image/4c/60/4cd575d922bca8d498574d76b1824a60.jpg?wh=1920x396\" alt=\"图片\" title=\"订餐流程模块示意图\"></p><p>而在技术层面上，又可以垂直切分出功能案例库和通用库。比如页面的组件可以形成复用库、page对象、button对象、link对象等等，把和开发技术耦合的技术层封装在复用库里，而和测试相关的业务功能实现在功能案例库里。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/01/74b3b5bc5f60fd5bc37c58c73bfb1501.jpg?wh=1920x811\" alt=\"图片\" title=\"订餐流程垂直切分示意图\"></p><p>这样的设计，遵循了高内聚低耦合的软件设计思想，未来自动化测试规模扩展时也很方便。比方说增加一个支付功能，就可以把支付页面的对象写到复用库里，新创建一个支付测试案例，前面和下单模块衔接，后面和快递模块对接，就能跑起来了。</p><p><img src=\"https://static001.geekbang.org/resource/image/0e/96/0ed618bbb3ae23123c4a449eae470196.jpg?wh=1920x592\" alt=\"图片\" title=\"模块扩展示意图\"></p><p>模块库开发的优点是可扩展、可复用，困难是需要架构工作和代码工作，对自动化测试开发人员的代码能力要求也比较高，接近开发工程师。</p><h3>方法四：BDD混合框架</h3><p>还有一种方法是BDD混合框架。BDD全称是Behavior Drive Development，行为驱动开发，它通过Gherkin语法定义测试场景。</p><p>Gherkin语法包含一套类自然语言的关键字：when、given、then，given描述条件，when描述行为，then描述结果。这样一个场景的3要素：上下文、动作和结果就说明白了。</p><p>所以，Gherkin语法描述出来的测试场景，能够同时被非技术和技术人员理解。客户、需求人员、开发人员和测试人员从BDD案例各取所需，需求人员得到用户手册，开发人员得到Use Case，测试人员得到测试案例。这些都有BDD框架支持生成代码，比如Cucumber。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/64/6bf0e210588aeae67b9a59a2714c1064.jpg?wh=1920x1051\" alt=\"图片\" title=\"BDD方法示意图\"></p><p>BDD和关键字驱动框架的优缺点基本一样，不同的是，BDD不局限于测试框架，能够和软件流程集成在一起，也有相应的开发IDE插件，比如Eclipse plugin。</p><h3>方法五：更高ROI的探索，自动化前沿技术</h3><p>最后，我要说一下目前比较火，听起来也很酷的自动化前沿技术。</p><p>AI测试曾被寄予厚望，我们期待由它发展出一套自动化测试全栈解决方案，可以自动生成测试案例和代码，而且维护工作量为零。不能说这些全都是镜花水月，我相信目前AI只在一小部分测试领域里落地，比如图像识别的Applitools，可以用作图片验证；游戏领域里的监督学习，用来行为克隆等等。</p><p>我也曾看过一些AI根据规则自动生成测试案例的演示，但演示只是演示，它演示的方案需要的很多条件，现实还不具备，比如基于非常理想的数据模型等等。所以，我认为AI“落地”的定义是，它的形式是产品，而不是个人业余的项目或者一段开源代码。</p><p>相比AI测试，我更看好另外一种自动化生成测试的思路，就是基于规则化或可以模式化的业务场景，把案例的生成和代码生成一并自动化，形成一种可以量化的案例发现方案。我会在第5讲带你了解这个思路如何实现。</p><p>记住，<strong>测试工作是要能证明软件功能的成败，其方法论基石是确定论，而不是未知论</strong>。说得通俗一点，测试是在编网，虽然网会漏鱼，但我很确信只要投入人手和时间，就能把网编到什么程度，网住什么鱼。 而不是今天我捉到一条鱼，明天不知道鱼在哪里。这是我不认为AI能完全替代手工测试工作的原因。</p><p>好，业界已知的生成自动化测试方法，我们做了归类了解，接下来再分析不同类型的自动化测试开发成本和维护成本。</p><p>我给这个表格起一个名字，叫做<strong>案例DM分析表</strong>。D代表Development，M代表Mantainence。</p><p><img src=\"https://static001.geekbang.org/resource/image/9d/40/9d9a670b5db7b775b9990a4cae48fb40.jpg?wh=4000x2490\" alt=\"\" title=\"案例DM分析表\"></p><h2>工具的成本</h2><p>前面探讨的给定了类型工具的情况下，案例的开发、维护成本如何衡量。但别忘了，成本计算时，还有个不小的成本：工具和框架的成本。这个在选型阶段没有考虑的话，会给后面埋下很多隐患，带来不少让人头疼的问题，比如下面这些。</p><p>1.当你的团队开始上手案例开发时，发现框架只支持JavaScript，Perl，但你的团队都是熟悉Python和Java。在这种情况下，是培训已有团队还是招聘新的人才，还是弃用方案而选择新的框架？</p><p>2.当你需要升级框架时，高版本对低版本的兼容性。它有可能让你的原有测试案例不能工作，最糟糕的是，会出现一些奇奇怪怪的问题，每一个问题的诊断过程是一场对你毅力和耐心的考验。</p><p>3.当你的案例在运行的时候，遇到一个框架抛出来的exception，网上搜不到解决方案，去社区论坛提问，也没人响应，自己去看源代码又陷入浩若烟海的context中去，那种无力感蔓延开来，直让你怀疑人生。</p><p>4.当你的案例规模扩展，新的API案例需要调用PATCH原语，但你已使用了一年的API框架不支持PATCH，更悲催的是，你发现这个API框架已经停止更新，凋零的社区、隐身的支持人员、破败的代码，都是对你的毒打。</p><p>进坑容易出坑难。我见证过一个自动化测试团队选型工具草率，导致在2年内更换了4个工具。每次换工具，都会重写一遍自动化测试案例，人收获了教训离开公司另谋高就，但公司浪费了2年的时间和资源，之后还要换一拨人乐此不疲地重复这样的故事。</p><p>可以说，选型合适的框架是自动化测试架构设计中一个重要的选择，它通过开发成本和维护成本两个因子影响自动化测试ROI，ROI低于1的时候，这个项目就要over了。</p><p>那什么是ROI好的框架呢？</p><p>首先，框架要<strong>满足我们的测试需求</strong>。比如，UI框架能有对象识别能力，API框架能有http原语封装，对xml和Json的支持，单元测试框架能有mock能力。</p><p>其次，框架应该有<strong>广泛的同行用户、持续的更新、成熟的社区和积极的客户响应</strong>，这些也能帮我们降低维护框架的成本，获得更好的ROI。</p><p>把这些因素量化，就能得到一个工具四维成熟度表，你可以把它作为原始模型，用到你的工作里。</p><p><img src=\"https://static001.geekbang.org/resource/image/28/fd/28d25d4a9a860947983ed3b853ab2bfd.jpg?wh=3563x2011\" alt=\"\" title=\"工具四维成熟度表\"></p><h2>小结</h2><p>今天这一讲，我们从开发成本和维护成本的角度系统梳理了工具和框架的优劣势。根据脚本的生成方式的不同，可以划分出录制回放工具、关键字驱动工具、模块库开发工具、BDD混合工具和AI工具，针对自动化的不同类型和规模，脚本的开发和维护成本也会发生变化。</p><p>框架本身也会有维护成本，这里我们优先选择成熟主流的框架，会降低维护的成本。这里“成熟主流”，我们使用用户的数量、更新的频率、社区成熟度、开发语言与团队的适配度四个指标来对比度量。</p><p>我并没有给出一个精准计算工具框架ROI的公式，而是给出几个维度来帮你厘清工具和框架的选择思路。</p><p>记住，这里有两大原则：</p><p>1.按照团队的<strong>技术能力、项目的预期长短、将来扩展的规模大小</strong>，选择ROI最大的工具和框架；</p><p>2.当工具和框架带来的ROI无法升高时，就考虑按照原则1 <strong>重新评估</strong>，选择ROI更好的工具和框架。</p><p>所以，工具和框架的评估不是一劳永逸，而是在整个自动化测试生命周期内实时观测，如果有变要及时止损，让ROI持续提升。</p><h2>思考题</h2><p>在工作中，你的团队非常熟悉Python，所以选择某个支持Python框架。基于这样的原因做选择是不是遵循ROI原则？这样的选择会带来什么样的好处和坏处？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这一讲分享给更多同事和朋友，说不定就能帮他通过下一次的工具选型评审会。</p>","neighbors":{"left":{"article_title":"02｜3KU法则：如何找出最优自动化实施截面？","id":497405},"right":{"article_title":"04｜脚本复用：什么样的代码才值得写？","id":499382}}},{"article_id":499382,"article_title":"04｜脚本复用：什么样的代码才值得写？","article_content":"<p>你好，我是柳胜。</p><p>开发和测试团队我都带过，现在测试人员的代码能力越来越强了，已经接近开发人员，我看过一些测试牛人设计的测试模块和代码：MVC实现测试控制台、分布式多测试节点管理、Proxy对测试Interface默认实现……这些用到的技术栈和开发不相上下。</p><p>聊下来，很多的反馈是“能做起来，很庆幸有一个对测试技术很支持的领导”。这就是一个让人困惑的问题，为了开发自动化测试案例，你写了那么多代码，那每一行代码就应该存在它的价值，这个价值是客观的，而不是依赖主观的某个人的认知。不是么？</p><p>所以这一讲要关注的问题是，你写的每一行代码都有自动化测试的价值么？你能把它说出来，说清楚么？想清楚这些，你自然也会明白给定一个自动化测试的项目，哪些工作是overwork（过度工作），哪些是underwork（工作得还不够）。</p><h2>哪些代码值得写？</h2><p>在开始之前，我们再回顾一下自动化测试ROI模型。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/00/cd34280bc70b3633e696a7ba16f9e300.jpg?wh=1920x868\" alt=\"图片\"></p><p>一个案例转化成自动化测试后，我们的目标是它的投资回报率越高越好，在ROI公式里，回报也就是分子越高越好，成本也就是分母越低越好。</p><p>在第一讲我讲到过，n是自动化测试案例运行的次数，在回归测试里，n是回归迭代的次数，回归次数越高，n也就越大，这是从时间的角度上来看n。在这一讲，我们换个角度，从空间来看，也就是代码的复用率，我们有没有办法让代码的复用率升高？</p><!-- [[[read_end]]] --><h3>初始版本</h3><p>为了让你更容易理解，我们结合一段登录的脚本，一起探索一下怎样提高一个自动化测试案例的复用率。</p><p>脚本代码如下：</p><pre><code class=\"language-java\">@Test\npublic void login() {\n  WebDriver driver=new ChromeDriver();\n  driver.manage().window().maximize();\n  //打开页面\n  driver.get(\"https://www.example.com/users/sign_in\");\n  WebElement username=driver.findElement(By.id(\"user_name\"));\n  WebElement password=driver.findElement(By.id(\"user_password\"));\n  WebElement login=driver.findElement(By.text(\"登录\"));\n  //输入用户名\n  username.sendKeys(\"liusheng@example.com\");\n  //输入密码\n  password.sendKeys(\"123456\");\n  //点击登录按钮\n  login.click();\n}\n</code></pre><p>这段脚本实现的功能很简单，启动chrome浏览器，打开一个登录链接，在页面上输入用户名liushing@example.com，密码123456，点击“登录”按钮，完成登录。此处我省去了assert检查点。</p><p>现在，这段脚本每运行一次，就测试一次登录。它的n现在就等于1。我们想一下，这个脚本还能怎么提高它的复用率呢？</p><h3>提高复用率：一份代码，多浏览器运行</h3><p>可以看到，脚本运行的测试案例只在chrome上，但作为一个web应用，一般是要支持市面上主流的浏览器，看一下<a href=\"https://help.aliyun.com/document_detail/211434.html\">阿里云网站</a>支持12种浏览器，列表如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/32/5a/32647ecb58ae59f29f1e6aec994f6f5a.jpg?wh=1920x1130\" alt=\"图片\"></p><p>那么，有没有办法让我们的脚本能够一下子测试12种浏览器呢？此时我们需要修改脚本，支持调用多个浏览器driver：</p><pre><code class=\"language-java\">@Test\n@Iteration(Driver=ChromeDriver,FireFoxDriver.....)\npublic void login() {\n  //此处是伪代码，代表从Iteration数组里拿到的driver元素\n  WebDriver driver=new Iteration(\"driver\");\n  driver.manage().window().maximize();\n  //打开页面\n  driver.get(\"https://www.example.com/users/sign_in\");\n  WebElement username=driver.findElement(By.id(\"user_name\"));\n  WebElement password=driver.findElement(By.id(\"user_password\"));\n  WebElement login=driver.findElement(By.text(\"登录\"));\n  //输入用户名\n  username.sendKeys(\"liusheng@example.com\");\n  //输入密码\n  password.sendKeys(\"123456\");\n  //点击登录按钮\n  login.click();\n}\n</code></pre><p>上面是伪代码，代表Test会重复运行Iteration数组，driver的数目有多少个，就运行多少次，每次运行从iteration数组里取得driver的名字，交给脚本去启动相应的浏览器。</p><p>现在dirver有12个，我们就可以让一份脚本测试12个浏览器，获得了n1=12。</p><p>总结一下，最佳实践：一份代码，兼容多个浏览器。</p><h3>提高复用率：一份代码，多数据运行</h3><p>刚才已经迈出了第一步，不错，我们再继续看脚本，还有没有可以改进的地方。现在，我们的脚本只能测试一组用户数据，用户名liusheng，密码是123456。在测试方法论中，一个测试案例应该有多组测试数据，那合法的用户名的数据格式不止这么多，按照字符类型划分等价类，至少有5组：</p><p>1.ASCII字符<br>\n2.数字<br>\n3.特殊字符<br>\n4.拉丁文字符<br>\n5.中文字符<br>\n你如果有兴趣想知道为什么这5种字符是等价类，可以去研究一下字符集原理。</p><p>密码一般是数字，ASCII字符加特殊字符3种。我们至少可以开发出5*3=15种合法的用户名密码组合，作为测试用例。</p><pre><code class=\"language-java\">@Test\n@Iteration(UserPassword={xxxx,123456},{测试用户，Welcome1}....)\n@Iteration(Driver=ChromeDriver,FireFoxDriver.....)\npublic void login() {\n  //此处是伪代码，代表从Iteration数组里拿到的driver元素\n  WebDriver driver=new Iteration(\"driver\");\n  driver.manage().window().maximize();\n  //打开页面\n  driver.get(\"https://www.example.com/users/sign_in\");\n  WebElement username=driver.findElement(By.id(\"user_name\"));\n  WebElement password=driver.findElement(By.id(\"user_password\"));\n  WebElement login=driver.findElement(By.text(\"登录\"));\n  //输入用户名\n  username.sendKeys(UserPassword[0]);\n  //输入密码\n  password.sendKeys(UserPassword[1]);\n  //点击登录按钮\n  login.click();\n}\n</code></pre><p>上面的代码是伪代码，UserPassword数组有多少个元素，测试案例就会运行多少次。每次运行会从interation里取得数组的一个元素，一个元素就是一种用户名密码组合。</p><p>上面的UserPassword数组有15个元素，我们的测试案例就运行15次，现在n2=15，加上浏览器的12次，n=n1+n2=15+12=27次。</p><p>最佳实践：一份代码，多组测试数据。</p><h3>提高复用率：一份代码，多环境运行</h3><p>现在我们已经摸着道了，提高ROI，那就是让一份自动化测试程序，尽可能多复用在不同的测试场景中。这些测试场景本来就是有效的测试需求，转换成自动化也是一劳多得。</p><p>还有没有其他场景呢？当然有，举个例子，在我们的产品发布pipeline里，贯穿了从开发环境、测试环境、准生产环境到生产环境，由低向高的交付过程。</p><p>那你的测试脚本需要兼容每一个环境，在所有需要运行它的环境里都可以直接跑，不需要做任何修改。一份脚本，运行在dev，test，stage，productin 4种环境下，我们的n3=4, n=n1+n2+n3=15+12+4=31次</p><p>你可能会说，这个有难度呀，环境不同，不光是脚本url不同，里面的测试数据也不一样，测试配置也不一样，甚至timeout要求也不一样等等，不是那么容易实现的。那你可以想想，这种问题，是不是开发也会遇到，他们需要把服务部署运行在不同的环境下。开发是怎么做到的？</p><p>以SpringBoot为例，它提供了多profiles配置的功能，application.yml文件配置默认参数，application-dev.yml里放dev环境的配置参数，application-test.yml放test环境的配置参数，application-prod.yml里放production环境的配置参数。</p><p>当spring boot application启动时，会自动根据输入环境参数，加载相应的环境yml配置文件。这就实现了一份代码，多环境部署的场景。</p><p>我们的自动化测试也可以实现类似的机制。聪明的你，可以考虑自开发一个测试配置文件加载模块，代码不需要多，但会直接增加ROI。</p><p>最佳实践：一份代码，兼容多环境运行</p><h3>提高复用率：一份代码，多语言运行</h3><p>另外，如果你的产品支持多国语言，那么<strong>一份代码跑多国语言版本</strong>，也是一个会显著增加自动化测试ROI的好主意。</p><p>像上面的代码，当前只支持中文页面。假设我们的产品要求支持9种语言，那可以让页面控件加载不同语言的label text。</p><p>伪代码如下：</p><pre><code class=\"language-java\">@Test\n@Iteration(UserPassword={xxxx,123456},{测试用户，Welcome1}....)\n@Iteration(Driver=ChromeDriver,FireFoxDriver.....)\n@Iteration(Profiles=auto-dev.yml,auto-test.yml,auto-prod.yml....)\n@Iteration(Language=en,zh_CN,zh_TW, FR....)\npublic void login() {\n  //此处是伪代码，代表从Iteration数组里拿到的driver元素\n  WebDriver driver=new Iteration(\"driver\");\n  driver.manage().window().maximize();\n  //打开页面\n  driver.get(profile.getUrl());\n  WebElement username=driver.findElement(By.id(\"user_name\"));\n  WebElement password=driver.findElement(By.id(\"user_password\"));\n  WebElement login=driver.findElement(By.text(Label.getLoginText(language)));\n  //输入用户名\n  username.sendKeys(UserPassword[0]);\n  //输入密码\n  password.sendKeys(UserPassword[1]);\n  //点击登录按钮\n  login.click();\n}\n</code></pre><p>现在一份脚本经过了多浏览器、多数据、多环境和多语言4轮打磨，运行的次数n=n1+n2+n3+n4=12+15+4+9=40次。如果各个场景有关联关系，比如页面的语言和测试数据有耦合，英文页面的encoding和数据的charset有关联，那么两个场景的次数就是完全组合，采用乘法，15*9=135次。</p><p>而且，从脚本的变化可以看到，脚本第一版本里的hard code也一个个被消除了，取而代之的是数据驱动。<strong>消除hard code是提升ROI的结果</strong>。</p><p>当然，上面的代码都是伪代码，实际上为了支持多场景运行，付出的努力不止是增加循环那么简单。比如，支持多少种浏览器取决于框架，而不是脚本。像现在有一些新的基于JavaScript的测试框架，只支持chrome浏览器，开发人员很喜欢用它来验证功能，但从自动化测试角度来看，它的ROI就受限了，这些局限都应该框架选型时考虑进去。</p><h2>还有哪些工作值得做？</h2><p>第一讲提出ROI模型的时候，我就提过，成本里的维护工作量是一个不确定的风险。根据ROI金字塔模型，维护的工作量也是自底向上增加，不确定性增加。</p><p><img src=\"https://static001.geekbang.org/resource/image/07/f6/07aaf08df0b9a44987ed9a38faa412f6.jpg?wh=1920x1228\" alt=\"图片\"></p><p>维护工作量的不确定性是自动化测试的一个重要风险，所以我们有必要看一下维护的工作量都花在哪里了。</p><p>1.被测截面发生变化带来的维护工作量。比如UI自动化测试的产品页面发生了变化，API自动化测试的接口做了重构。</p><p>2.诊断自动化测试的工作量，如果把自动化测试结果分为真阳，假阳，真阴，假阴。那假阳和假阴都是需要诊断的。</p><p>诊断是要花费时间的，有这么几块：</p><p>1.从错误实际发生，到我们知道错误发生，这有一个<strong>通知</strong>的时间；<br>\n2.从开始诊断错误，到定位出错误，这要花费一个<strong>诊断</strong>的时间；<br>\n3.修复错误和验证修复方案，这也要时间，即<strong>修复</strong>时间；<br>\n4.修复上线后，跑出第一轮测试结果，证明完全恢复，这叫<strong>确认</strong>时间。</p><p><img src=\"https://static001.geekbang.org/resource/image/e8/f4/e85a5b985324f48fdbeef5de0d9178f4.jpg?wh=1920x459\" alt=\"图片\"></p><p>怎样能提高诊断的速度呢？从上面的分析，可以看到：</p><p>1.缩小通知时间，目标做到<strong>实时</strong>通知。一旦有错误出现，应该立刻有责任人被通知到，进入到诊断环节。</p><p>2.诊断越快越好，一旦确定为自动化测试脚本的问题，应该立即对团队做出说明，并将错误案例下线，避免持续出现相同错误，引起误解。</p><p>3.修复要彻底，一个案例持续不能给人信任的结果，将会打击团队的信心。我见过有的公司，自动化测试跑完一遍后，还要手工再去验证一遍，这样的自动化测试就失去了价值。</p><p>4.确认后，自动化测试快速恢复上线，开始使用。</p><p>你在这里，可以发挥技术能力，按照上面的方向努力，去降低自动化测试的维护成本，比如：</p><p>1.Log规范+ELK+Grafana实现告警实时传达。<br>\n2.检查点+日志+屏幕截图甚至视频，提高诊断效率。<br>\n3.高内聚低耦合的模块化设计，能够实现隔离错误，缩小影响范围，快速修复的效果。<br>\n4.自动化测试上下线的标准和流程的建立。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/26/3ffdb78yy842c96f530158b22e06d026.jpg?wh=1920x591\" alt=\"图片\"></p><h2>小结</h2><p>今天我最想和你传达的观点是：<strong>在提高收益的方向上，我们付出的每一份努力和尝试，都是值得的。</strong></p><p>其实自动化测试有个特点，它并没有一个显式的可验收交付目标。我们既可以写几行脚本就自动化一个案例，也可以实现一个支持控制台、多代理、持久化等功能的复杂系统。</p><p>但是别忘了自动化测试的本质，它是一个有业务需求的软件实现。这个业务需求以前没有人去讲清楚，学过这一讲，你就应该明白了，业务需求就是自动化测试ROI，想办法提高它是这个软件的唯一目的。</p><p>我讲到了提高代码ROI的通常的四种思路，一份代码，多浏览器运行，多数据运行，多环境运行，多语言运行，并用例子向你展示了，这个提供ROI的过程，也是消除代码Hard code，优化代码结构的过程。</p><p>另外，降低维护成本，也是提高ROI的一个非常有效的办法。我们分解了自动化测试的诊断时间，分为通知、定位、修复和确认四块时间，针对每一块时间，都有相应的办法去提高效率，这里我列出了一些常见办法，帮你在工作中找到更精准的目标，你可以保存下来做个参考。</p><p>总之，任何能够提高ROI的代码都是有价值的，反之，就是overwork。作为自动化测试架构师的你，在头脑里应该存在一个优先级从高到低的工作列表，先做哪块，再做哪块，按照ROI从高到低的顺序来安排。</p><p>你也应该能明白，在一个十几人的团队里，去做一个大规模的自动化测试项目，回报是追不回投入的，这样的项目即使能立项，最后也是死掉。反过来，如果你的领导愿意支持你去做一个复杂的系统，那他要么有一个推广提升的配套计划，要么是一个技术fans，像曾经的我一样，不尊重自动化测试的本质，也会败在ROI的规律手下。</p><h2>思考题</h2><p>回到这讲开篇的问题，思考一下你目前负责的自动化测试项目，是overwork还是underwork？</p><p>欢迎你在留言区和我交流互动，也推荐你把这讲内容推荐给身边的测试同学，一起精进技术。</p>","neighbors":{"left":{"article_title":"03｜工具选择：什么工具框架值得用？","id":498458},"right":{"article_title":"05｜Auto Gen Auto：所有测试工作即代码","id":499339}}},{"article_id":499339,"article_title":"05｜Auto Gen Auto：所有测试工作即代码","article_content":"<p>你好，我是柳胜。</p><p>我们前面用了4讲篇幅，讨论ROI模型和由此衍生出来的一套实践原则，从分层测试、选型思路和具体代码多个角度探索提升ROI的方法。</p><p>这些方法还都是基于常规的自动化测试开发流程，先有测试需求，再设计测试案例，然后做自动化。以登录测试为例，我画了一张流程图做说明。</p><p><img src=\"https://static001.geekbang.org/resource/image/e5/82/e524642fdd13d88e63de2900325f5e82.jpg?wh=1920x462\" alt=\"图片\"></p><p>自动化测试的开发成本，就是把测试需求转变成自动化测试代码这个过程花费的时间。在我们的图里，它是从左向右，所以我管它叫做<strong>水平开发成本</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/98/5fcyy4827fff8970ef6dc39aeb0ca598.jpg?wh=1920x709\" alt=\"图片\"></p><p>当登录功能测试需求发生变化时，就会重新走一遍这个流程，出现了多个版本的测试需求，也会带来多个版本的自动化测试案例。从下图可见，这个版本是自上向下增加，所以我管它叫做<strong>垂直维护成本</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/ab/e4/ab257d846526c1e731da9812c9cd08e4.jpg?wh=1920x781\" alt=\"图片\"></p><p>我们现在可以直观地看到开发成本和维护成本了。好，问题来了，有没有办法<strong>从流程上动手术，来降低这两个成本呢</strong>？</p><p>这就是我们今天要讲的Automation Generate Automation，也叫自动化产生自动化测试代码，为了方便起见，下面的篇幅用缩写Auto Gen Auto来指代。</p><h2>Auto Gen Auto 技术</h2><p>常规的自动化测试，是指用代码实现设计好的TestCase，而Auto Gen Auto的目的是让Test Case生成也自动化，如下图所示。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/b9/b0/b9650d372e55704a31431faa8f4cb6b0.jpg?wh=1920x770\" alt=\"图片\"></p><p>因为从测试需求到自动化测试案例是完全自动化的，每次需求改变的时候，只需运行一次Auto Gen Auto即可生成新的自动化案例，垂直维护成本为零。所以Auto Gen Auto技术如果能落地，ROI就会大大提高。</p><h3>从何处下手</h3><p>那Auto Gen Auto用在哪性价比更高呢？</p><p>业界熟知的测试方法是黑盒测试和白盒测试。白盒测试从测试案例设计开始，需要我们先了解代码逻辑结果，一个函数里有几个判断分支，处理那些数据。基于这些了解，再设计案例验证函数输出和达成代码覆盖率。</p><p>在白盒测试里，Auto Gen Auto不是啥稀奇事，XUnit框架都提供了不少开发IDE的plugin，可以扫描一个class的函数，直接产生test方法。开发人员只需补充少量代码，test方法就可以运转起来了。</p><p>与之对应的是黑盒测试，测试案例设计不基于产品代码，而是用户规格说明。比如，用户在订餐系统上完成一个订单，用户该怎么操作，下单成功后应该收到物流单号等等，设计这些测试案例的目的是验证业务能够完成，不需要去看代码。</p><p>今天，我们要关注的是<strong>在黑盒测试领域的Auto Gen Auto</strong>，这个更有挑战性，也更有探索价值。因为，作为测试人员花了大量时间来设计黑盒测试案例，而且还要手工维护这些测试案例的变化，这个过程要是都能自动化了，就会省去很大的重复又枯燥的工作量。</p><h3>如何实现</h3><p>怎么做到Auto Gen Auto呢？用代码生成代码，前提是测试需求得有一定的规则或模式，然后代码才能解析规则，根据规则生成最终的测试代码。</p><p>这个实现思路，在开发中是很常用的，比如Maven Archetype使用模版自动生成项目代码，Soap使用WSDL来生成调用桩等等，原理图如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/0f/cd/0f556ba335e4fef6286140819d804acd.jpg?wh=1920x901\" alt=\"图片\"></p><p>所以，要做Auto Gen Auto，我们的目标是先要找出测试需求里的这些规则，并把它们表达出来，放在一个规则文件里。我们看看下面的例子。</p><h2>测试等价类的规则</h2><p>远在天边，近在眼前，我们在测试案例设计中经常用到的等价类和边价值方法，就可以作为Auto Gen Auto的规则。</p><p>等价类是指某个输入域的子集合，在同一个子集合里的所有元素对于测试的效果都是等价的。</p><p>我们要测试一个订餐系统的用户名，首先要了解用户名上的约束。从长度上来看，假设用户名最大长度是255个字节，根据这个约束，至少能产生2个测试等价类：有效等价类是小于255字节的用户名，无效等价类是大于255字节的用户名。测试用户注册功能时，就可以用到这2个等价类了。</p><p>用同样的思路看用户名的另外一个约束，那就是字符类型的限制，假设用户名只能由英文字母和数字组成，根据这个约束，又可以产生多个等价类，中文字符、ASCII字符、数字、High ASCII等等。</p><p>看到没有？其实我们用等价类方法设计测试案例时，遵循的是<strong>等价类划分规则，设计出来的测试案例也与等价类一一对应</strong>。但手工做这些，工作量会很大，因为整理约束时会有遗漏，改变约束的时候，也容易忘了维护测试案例。</p><p>如果能让测试案例和等价类自动对应，然后依据规则动态产生测试案例，这些问题就会迎刃而解。不过，我们得先把这些约束规则外化表达出来，在这里，我用一个user-rule.yaml文件来表达这些规则。</p><pre><code class=\"language-yaml\">name: user name rules\n  appliedTestCase: register, login\n  rules:\n    lengthRule:\n      express: &lt;=255 chars\n    characterRule:\n      express: value&gt;=97 and value&lt;=122\n      express: value&gt;=48 and value&lt;=57\n</code></pre><p>为了让这个YAML文件能对代码友好，我把express表达式部分做了技术处理，ASCII值从97到122之间是a～z的字符，48到57是数字0～9。</p><p>然后，我们写一段代码，从这个YAML文件中直接把规则加载进来，在内存中形成一个分类树。</p><p><img src=\"https://static001.geekbang.org/resource/image/dd/9a/dd045b1924fd30754b50c7c51f90d69a.jpg?wh=6675x2935\" alt=\"\"></p><p>1个用户名，有2个约束，每种约束都取1次不同的等价类，那测试案例的组合总共有2 * 5= 10个测试案例。</p><p>如果对每一个等价类再加上权值，我们还可以根据权值，过滤掉部分权值偏小的测试案例。基于YAML可以生成以下测试案例，从案例名字，你可以看出用户名的取值规则：</p><p>TestUserNameLengthLessThan255<br>\nTestUserNameLengthBigThan255<br>\nTestUserNameAsciiValueLessThan48<br>\nTestUserNameAsciiValueBetween48And57<br>\nTestUserNameAsciiValueBetween57And97<br>\nTestUserNameAsciiValueBetween97And122<br>\nTestUserNameAsciiValueBigThan122</p><p>好，这里看到成果是很明显的。因为测试案例的生成是自动化的，所以，以后需求变化时，比如允许用户名出现中文，那就在user-rule.yaml里增加一条rule，测试案例也会自动被修改，测试案例维护工作量等于0。</p><p>到这里，我再画个图总结一下这个方案的实现思路。</p><p><img src=\"https://static001.geekbang.org/resource/image/19/3d/19146e8321dc45e65cedd9052661cf3d.jpg?wh=1920x889\" alt=\"图片\"></p><h2>业务的逻辑规则</h2><p>用等价类的规则表达小试牛刀后，我们尝到了甜头。看来，只要能把规则表达出来，生成测试案例这个工作就可以交给代码去做。我们再找一个更加实用的场景，来看看怎么落地。</p><p>在做API测试的时候，restAPI的接口一般是通过Open API规范来描述。在设计阶段，开发先定义要实现的API接口，Client要发送什么样的Request，Server要返回什么样的Response。</p><p>比如下面的user-restapi.yaml文件，就是遵循Open API规范，定义了一个根据name查询User的RestAPI。</p><pre><code class=\"language-yaml\">/api/users:\n    get:\n      description: 通过name查询用户.\n      parameters:\n        - username\n          type: string\n          description: 用户name\n      responses:\n        '200':\n          description: 成功返回符合查询条件的用户列表.\n          schema:\n            type: array\n            items:\n              $ref: '#/definitions/User'\n</code></pre><p>这个接口很简单，但它也声明了一个简单的契约，Client要想查询User，它需要向server发送一个http get请求，发送的url格式如下：</p><p>http://{host}:{port}/api/users?username=liusheng</p><p>而server如果查询到了User，它应该返回这样一个http status code为200的response，内容如下：</p><pre><code class=\"language-yaml\">{\n        \"items\": [\n          {\n            \"ID\": \"123456\",\n            \"name\": \"liusheng\",\n            \"age\": 18\n          }\n          ]\n}\n</code></pre><p>YAML文件里定义接口所用到的关键字，像get、description、parameters等等，它们都是Open API里定义好的，含义也是明确的，那么YAML表达出来的规则内容也是可以解析出来的。因此，我们同样可以根据规则内容，直接生成测试代码。</p><p>实际上，业界已经有了现成的工具，有Spring Clond Contract，也有<a href=\"https://github.com/OpenAPITools/openapi-generator\">OpenAPI generator</a>。</p><p>我们这就借用这个工具跑一下，把它下载到本地，运行如下命令行：</p><pre><code class=\"language-yaml\">java -jar openapi-generator-cli.jar generate\n&gt;&nbsp; &nbsp;-i user-restapi.yaml \\\n&gt;&nbsp; &nbsp;-g java \\\n&gt;&nbsp; &nbsp;--library rest-assured\n</code></pre><p>运行后就会生成一个基于RestAssure的测试项目。这个自动生成的项目里包含了API测试运行所需要的Client、Config、Model、API所有代码。</p><p><img src=\"https://static001.geekbang.org/resource/image/f0/d6/f0bd34e107d3bd7594fbea0bcda010d6.png?wh=582x522\" alt=\"图片\"></p><p>对照上图，UserApi.java里的testGetUserByName函数，就是根据YAML文件的API定义自动生成的测试代码，可以直接运行。</p><pre><code class=\"language-java\">@ApiOperation(value = \"Get user by user name\",\n        notes = \"通过name查询用户\")\n@ApiResponses(value = { \n        @ApiResponse(code = 200, message = \"成功返回符合查询条件的用户列表\") })\npublic GetUserByNameOper testGetUserByName() {\n    return new GetUserByNameOper(createReqSpec());\n}\n</code></pre><p>是不是很酷？一份契约就这样变成了可执行的测试代码，完全不需要任何开发工作量。</p><p>在解放生产力这件事上，优秀的工程师从不满足。上面只生成了一个测试案例，能不能生成多个测试案例，做更多的测试覆盖，让这个好点子物尽其用呢？</p><p>当然可以，按照等价类规则的思路，多个测试案例来自于多个约束，那我们可以在YAML文件中，加入username更多的约束描述。在user-restAPI.yaml文件里，username加两行属性，minLength是5，maxLength是255，代表用户名最小长度是5个字符，最大长度是255个字符。</p><pre><code class=\"language-yaml\">/api/users:\n    get:\n      description: 通过name查询用户\n      parameters:\n        - username\n          type: string\n          minLength: 5\n          maxLength: 255\n          description: 用户name\n         .............\n</code></pre><p>现在，我们就可以用等价类规则转变成测试案例的思路，解析YAML文件，把testGetUserByName分裂生成多个测试方法了：</p><p>testGetUserByNameLengthLessThan5<br>\ntestGetUserByNameLengthBetween5And255<br>\ntestGetUserByNameLengthBigThan255</p><p>OpenAPI generator这个开源工具就说到这，你可以根据具体需求灵活修改它，加入对YAML文件任何属性的解析，赋予它测试上的意义，使之成为强大的Auto Gen Auto工具，为你所用。</p><p>到这里，我们回顾一下，想要做好API测试0代码，自动生成的测试案例够多，有2个隐含的前提条件要满足：</p><p>1.API设计先行。在API<strong>设计阶段</strong>就要理清接口规则并把它表达出来。</p><p>2.在API接口的规则文件里，规则描述得越详细，可自动生成的测试案例就会越多。</p><p>在实践中，我看到很多公司忽视了API设计先行原则，开发团队写完代码，再生成YAML接口文件，然后交给测试人员开发API测试代码，这把流程恰好搞颠倒了。本来应该是根据接口设计文档来开发代码，开发和测试都能依据设计并行展开工作，这个做法也会促进团队在设计阶段考虑好设计。</p><p>但是，先写代码再产生接口文档，实际就会默许甚至鼓励开发人员先信马由缰写代码，写出来什么接口都算数。从API测试角度来看，测试需求就处在一个不稳定的状态，会带<strong>来高昂的自动化测试垂直维护成本</strong>。在第二模块里，你也会看到，设计先行对于开发人员内部协作也至关重要。</p><p>另外，有了一份定义完备详细的接口设计文档，Auto Gen Auto解决方案才可能实现。它不仅能够生成API test，还可以生成performance test等等。</p><p>有兴趣你可以自己研究一下这块的工具（相关工具有兴趣你可以点<a href=\"https://github.com/sheng-geek-zhuanlan/awesome-test-automation\">这里</a>了解），OpenAPI是其中的一个，我们在后面的课程还会提到Spring Cloud Contract，也是类似的解决方案。</p><h2>小结</h2><p>今天，我们一起学习了如何通过Auto Gen Auto技术，进一步降低我们的开发、维护成本。白盒测试中实现自动生成并不稀奇，所以我们把重心放在了更有挑战的黑盒测试领域。</p><p>想要自动生成测试案例，需要我们先洞察业务场景里的规则，并把它表达出来，形成文档，在团队里达成共识，作为<strong>开发和测试的契约</strong>。</p><p>我们先从黑盒测试里最熟悉的等价类案例设计方法开始，把用户名的命名规则整理清楚，并且把它表达在一个YAML文件里，将这个规则作为我们Auto Gen Auto的输入。然后经过加载，解析，最后输出测试案例。在这个过程中，我们关注的是实现思路，没有给出具体的实现方案，有兴趣你可以自己尝试一下。</p><p>在第二个例子里，我们着重看业界经验，以OpenAPI为例，它对RestAPI接口设计不仅有设计规范，还提供了一套工具集，我们使用了它的Generator生产了一个查询user的测试案例代码，你能直观感受到它的便利。而且，像OpenAPI generator代码都是开源的，你完全可以改造它，做适合自己项目的定制化扩展。</p><p>做Auto Gen Auto这样的项目，带来的ROI是非常高的，一次性开发工作量，不需要代码维护工作量。重要的话再说一遍：<strong>任何提高ROI的努力，都是值得去尝试的。</strong></p><h2>牛刀小试</h2><p>访问<a href=\"https://github.com/OpenAPITools/openapi-generator\">https://github.com/OpenAPITools/openapi-generator</a>，按照指令，运行generator命令，在Python，JavaScript，Java，Go四种语言里选一种生成代码。</p><p>欢迎你在留言区和我交流互动，也推荐你把这讲内容推荐给身边的测试同学，一起精进技术，提升效率。</p>","neighbors":{"left":{"article_title":"04｜脚本复用：什么样的代码才值得写？","id":499382},"right":{"article_title":"06｜左移&右移：测试如何在Dev和Ops领域大展身手？","id":501526}}},{"article_id":501526,"article_title":"06｜左移&右移：测试如何在Dev和Ops领域大展身手？","article_content":"<p>你好，我是柳胜。</p><p>前面的几讲，你已经学习了ROI模型，它开始于一颗带有ROI DNA的种子，不断生长，直到长成一棵大树。从树根到枝干、从规律到原则，它贯穿了一个自动化测试项目，从生到死的完整生命周期：设立目标，制定策略，选择框架到代码的实现，上线的度量，再到衰竭退出。</p><p>我把价值篇讲过的内容加以整理，得到了下面的树结构。上面的绿色部分是收益，下面的红色部分是成本。ROI模型树从左到右，是从根到枝干，从ROI理论到实践的延伸和具象化。</p><p><img src=\"https://static001.geekbang.org/resource/image/be/e2/bebf2e6673b64e11a0ff32703f7be0e2.jpg?wh=1920x1346\" alt=\"图片\" title=\"ROI模型树\"></p><p>ROI模型展开的这棵大树，告诉了我们健康的自动化测试项目长什么样子。但是还有一个现实的问题，我们怎么从这里到那里？换句话说，咱们怎么把自动化测试项目的节奏带起来，让它进入到一个不断提升ROI的轨道上去。</p><p>这里有一个很朴素的道理，好马是跑出来的，好钢是炼出来的。首先，要让自动化测试跑起来，增加它的运行次数，这是前提条件。在这个过程中，再修复它的问题，调整它的集合，提高它的可诊断性，整个项目就激活了。</p><p>所以，你要找到更多的土壤让自动化测试落地生长。如果你想在工作中推广自动化测试，哪些落地场景更容易出业绩呢？除了之前说过的回归测试领域，我们不妨把眼光从测试工作放宽到更多的领域，Dev和Ops领域，自动化测试在这些领域里一样可以发挥价值，我叫它自动化测试左移和自动化测试右移。</p><!-- [[[read_end]]] --><h2>自动化测试左移</h2><p>如果把软件的生命周期的一个个阶段，软件需求分析、软件设计、软件开发、单元测试、集成测试、系统测试，从左向右排列，开发活动在左侧，测试在后面也就是右侧。如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/dd/7e/dd3a0yy1cf966aa256381d98c874eb7e.jpg?wh=1920x596\" alt=\"图片\"></p><p>测试左移，就是说本来在生命周期后期的测试活动提前，在软件开发阶段就参与进来，能让软件质量内建到开发阶段，而不是在后期通过软件测试去发现。比如在需求分析阶段参与需求评审和规范制定，在软件设计阶段就开始测试案例设计。形象地看，是测试活动从右侧向左侧移动，即测试左移。</p><p><img src=\"https://static001.geekbang.org/resource/image/2d/98/2d6ac663a3887698a082bec930e03698.jpg?wh=1920x761\" alt=\"图片\"></p><p>测试左移后，当然也会带动自动化测试的变化。在传统模式下，按照软件生命周期顺序，自动化测试是这么安排的：编码完成之后运行单元测试，集成阶段运行接口测试，系统阶段运行UI自动化测试。</p><p><img src=\"https://static001.geekbang.org/resource/image/dd/b4/dd932dfef516dc7cd67faf0d94cf42b4.jpg?wh=1920x744\" alt=\"图片\"></p><p>这种流水线做法只能说中规中矩，那还有优化提升空间么？从图上看到，我们如果在编码阶段引入了bug，影响接口的bug要等到集成测试阶段才能发现，影响UI的bug要等到系统阶段才能发现。我们既然已经有了接口和UI自动化测试，可不可以把它们利用起来，尽早测试呢？</p><p>现在我提出一个新概念，自动化测试左移，在构建阶段建立一个冒烟测试集合的概念，包括单元测试和部分接口测试，甚至部分UI自动化测试，它们一起运行，来验证版本的每一次构建甚至代码的每一次提交。只要这个冒烟测试集合足够快和稳定，就可以被开发人员接受。</p><p><img src=\"https://static001.geekbang.org/resource/image/01/ee/0130fa2b9b66a43832a522dc39d528ee.jpg?wh=4563x1975\" alt=\"\"></p><p>自动化测试左移都有哪些好处呢？</p><p>最直观的好处是提早确认代码的变更，满足最终需求，尽早发现回归bug。软件测试领域有一个理论，叫做<strong>验证</strong>和<strong>确认，</strong>在每个软件阶段，都要做两种测试工作：第一是验证当前阶段做好了本阶段要求的事情。比如，编码阶段要把详细设计实现；第二是确认当前阶段实现的功能，可以满足最终的用户需求。</p><p>应用到具体场景里，在编码阶段做单元测试这叫验证，在编码阶段运行接口测试和UI自动化测试则是确认，都是有价值的测试活动。</p><p>此外，自动化测试就像一辆赛车，需要运行调试、持续保养维护，才能调整到最佳状态。</p><p>我见过一些团队，只在软件产品发布的时候，才把开发出来的UI自动化测试作为验收测试运行一次。这样UI自动化测试大概率会失败，测试人员不得不花时间一一解决。不难猜到，在整个发布周期里，UI功能都变化了很多，相应的漏洞更是不可胜数。久而久之，测试人员就对自动化测试产生了厌烦，把它看成摆设、负担，又重归手工测试的状态。</p><p>而自动化测试左移到开发的日常活动中，开发人员每天做一次code commit，做一次版本构建就会触发自动化测试，运行频率随之提高。一旦自动化测试运行失败，要么是发现了回归bug，要么是自动化测试需要维护了，问题发现得越早，修复越快，自动化测试就越健康，越稳定可用。在磨合调试的动态过程里，自动化测试越跑越稳定高效，团队也能实实在在体会到它的用处。</p><p>所以，自动化测试左移在结果上提高了ROI，丰富了运行场景，也锻炼了自动化测试项目和人马。火炼真金，大浪淘沙，方能走向成功。</p><h2>自动化测试右移</h2><p>既然有左移，那也存在右移。测试的右移是什么？测试阶段结束，产品就会上线，也就进入了线上运维阶段。所以测试右移是指<strong>测试活动介入线上运维，用户画像等工作</strong>。这里我说的自动化测试右移，意思是自动化测试也可以在生产环境里运行，起到一个自动检查监测的作用。</p><p>你可能会问，线上观测已经有了一套Ops流程和工具了，比如Newrelic和Splunk等，都能监测Web服务、API网关，数据库等全栈环境了。自动化测试线上运行的价值在哪里？</p><p>这是一个很好的问题。如果你想到了这一层面，说明离我们的右移的落地场景很近了。我们不可能把所有的自动化测试都搬到生产环境里，这样会造成和Ops团队的重叠浪费。</p><h3>部署后验证测试</h3><p>Ops工具看似很强大，能输出一堆软件服务的各种度量指标，告诉我们软件服务在生产环境里是健康运行的，但是有一个关键的事情，我们无法从Ops那里得知：那就是<strong>服务是不是按照客户的期望运行的，这对产品价值非常重要，但只有运行测试才能知道</strong>。</p><p>结合具体场景，我们分析一下这个问题是怎么产生和解决的。</p><p>线上升级常用的做法是红绿部署（也叫蓝绿部署）。红绿部署的机制是这样的，当准备升级软件服务时，保持原有的服务红色环境不变，部署一套新的服务绿色环境。在路由层面，把流量切换到绿色环境，完成软件的升级。这样做的好处是，软件升级对用户影响微小，风险也可控。</p><p><img src=\"https://static001.geekbang.org/resource/image/24/19/2436efdc7754257459ea9eaa5f787319.jpg?wh=2755x1288\" alt=\"\"></p><p>在这个红绿部署机制里，红环境代表是即将退役环境，绿色环境是健康即将上线环境，那么一个关键问题是，怎么确定环境是红的还是绿的？</p><p>环境是绿的，意味着对于用户来说一切正常。这个“正常”的标准，有2个含义，一是Ops的服务健康指标都正常，二是测试的结果也正常，这个测试集合在业内英文名叫Post Deployment Test，中文叫部署后验证测试。意思是在生产环境完成部署或升级后，需要验证业务功能正常运行。</p><p>通过Post Deployment Test的通过与否，来设定环境是绿色还是红色。像下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/9a/75/9a48f91415e62509044f7fae175de575.jpg?wh=1920x1129\" alt=\"图片\"></p><p>这样，Post Deployment Test结合部署的红绿机制，可以形成一个发布升级，环境切换的决策流程。我们又一次丰富了自动化测试运行的场景，让它变得“有用”。</p><h3>生产环境定时监测</h3><p>部署升级后，生产环境就开始运行了，直到下一次升级为止。在这段线上运行的时间里，是不是就不再需要测试了呢？</p><p>按照传统测试理论，测试的生命周期到正式发布为止，也有的到Beta测试为止，而部署到生产环境后，就进入了运维阶段，就是Ops工程师的事了，测试人员就不需要关注了。</p><p>但在云时代情况发生了变化。软件开发方不仅交付软件服务，而且也控制着服务器的运行环境。因此测试人员的责任从“在软件发布之前发现bug”变成了“在客户之前发现bug”。</p><p>有很多bug，在测试环境里是发现不了的，只有生产环境才能暴露。这些跟客户的行为、生产环境的数据、特定的错误扩散模式都有关系。只要我们在客户遇到bug之前发现它，测试工作仍然是有价值的。</p><p>这时我们可以建立一个机制，通过自动化测试来定时监测生产环境。每天定时触发自动化测试任务的运行，去检测生产环境的业务功能是否正常，然后生成测试报告。</p><p><img src=\"https://static001.geekbang.org/resource/image/1a/35/1a82912b949edf4e5dc180fffbf2f835.jpg?wh=1920x1174\" alt=\"图片\"></p><p>对于自动化测试生成的结果报告，我们还可以把它集成到Ops的Oncall流程里去。当自动化测试任务出了错误，触发Event时，就会进入到Oncall系统。Oncall系统会找出值守的测试人员，发送通知，让测试人员来处理测试错误，判断是不是线上出了bug。</p><p><img src=\"https://static001.geekbang.org/resource/image/b7/00/b7419b7a9e3f49934065061f3e644600.jpg?wh=1920x1114\" alt=\"图片\"></p><p>这个机制一旦建立起来，会大大锻炼你的测试人员队伍，因为你每天会收到各种告警，需要去处理，直到你真的把自动化测试的健壮性和稳定性，提升到一个“可信”的程度。</p><h2>小结</h2><p>这一讲，我们一起总结了自动化测试的ROI模型树，还有种种可以提高自动化测试收益的落地场景。从业界来说，有左移和右移这两个大领域，使得测试的工作不再局限于原先的那个圈圈里，而是向Dev和Ops方向延伸，并和它们融合。</p><p><img src=\"https://static001.geekbang.org/resource/image/be/e2/bebf2e6673b64e11a0ff32703f7be0e2.jpg?wh=1920x1346\" alt=\"图片\" title=\"ROI模型树\"></p><p>这个左移和右移已经不是在理念阶段了，而是业界正在发生的事情，参看<a href=\"https://lp.katalon.com/hubfs/download-content/report/The-State-of-Quality-2022.pdf\">2022世界软件质量报告</a>，有38%的公司已经在Ops领域开展测试了，而且这个数字还在增长。但是，要完成左移和右移并不容易。据我了解，很多公司还没有做好准备，一个重要的方面是意识上还没有转变。</p><p>今天，你学习了本模块的ROI模型之后，知道ROI是我们自动化测试工作成败的命脉，那你不妨换个思维角度，像做销售一样，向团队向公司积极布道你的自动化测试成果，让更多的角色来使用你的服务。</p><p>这块不局限于Dev和Ops团队，甚至可以是Product Manager团队，自动化测试代码调用是一个非常好的服务模式，一旦他们开始使用，就相当于你的服务有了客户，可以通过客户的反馈，持续打磨你的服务，<strong>这个服务的最后结果就是自动化测试持续输出“有用”和“可信”的结果</strong>。</p><p>在“有用”和“可信”基础上，如果再加上一个要素的话，我认为应该是“快速”，自动化测试服务的输出是当前的被测系统是否OK，没有人愿意忍受等几十分钟才能获得服务的结果。在Devops理念横行的今天，人们的耐心越来越低，你应该让自动化测试运行得精准、快速、高效。这给自动化测试提出了更大的挑战。</p><p>如何应对挑战，我们后续课程还会提出更有针对性的解决方案。是不是很期待下面的内容？那就准备好进入下一模块吧。</p><h2>思考题</h2><p>今天留两个题目，你可以任选一个有兴趣的聊聊想法。</p><p>1.什么是蓝绿部署？蓝代表什么？什么条件下变成绿？</p><p>2.学完这讲内容，你觉得自己手里负责的工作有没有左移或右移的空间呢，具体你想怎么做？</p><p>期待你在留言区跟我交流互动，如果这一讲对你有启发，也推荐你把它转发给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"05｜Auto Gen Auto：所有测试工作即代码","id":499339},"right":{"article_title":"07｜需求提炼（一）：单体应用要测什么？","id":502863}}},{"article_id":502863,"article_title":"07｜需求提炼（一）：单体应用要测什么？","article_content":"<p>你好，我是柳胜。</p><p>通过第一模块价值篇的学习，我们已经掌握了自动化测试效益的量化思维。理解思路还不够，我们在设计篇这个新模块，会把自动化测试里的各种类型、策略和实现技术都梳理一遍，把上个模块学到的知识应用起来，通过科学设计达到测试效益整体最优的目标。</p><p>为了让学习过程更接地气，尽可能贴近你日常的工作应用，我们这个模块会围绕一个具体的订餐系统项目，逐一分析它的需求、接口、用户场景，然后制定相应的自动化测试方案。掌握了这套推演逻辑，对单元测试、接口测试和系统测试，哪个层面的测试应该做什么、工作量分配比例是多少，你都能胸有成竹。</p><p>今天这一讲，我们的目标是整理出清晰、完整的测试需求，这是所有测试工作开始的第一步。有了这个基础，后面才能制定计划、设计自动化测试用例，完成测试代码开发。</p><h2>FoodCome的单体系统</h2><p>我们现在有一个名为FoodCome的应用。它刚开发出来的时候是一个单体系统。</p><p>这里要解释一下，什么是单体系统。一般的理解是，单体系统是一个整体，用一种语言开发，一次构建所有代码，产生一个部署实体，在运行态下是一个进程。比如常见的Web应用，就是一个war包。</p><p>这个FoodCome就是一个Web应用，它为用户提供点餐功能。用户可以通过手机下单点餐，订单生成后，餐馆可以接单，厨房制作完成，转给物流交付给用户。</p><!-- [[[read_end]]] --><p>为了分析测试需求，我们用六边形架构图方法来理清系统内外的交互接口。六边形架构法是把服务画成一个嵌套的六边形，最外层的大六边形是适配器层，代表了本系统和对外的所有交互。里层的六边形是领域业务层。适配器层负责对外交互，这个和业务关系不大，一般是通用的技术，主要是驱动、协议和基础设施，而领域层是业务逻辑的组织和实现。如果你对六边形架构不太熟悉，还可以参考<a href=\"https://www.jianshu.com/p/c2a361c2406c\">这里</a>了解。</p><p>为了简化，这里我只画出顾客下单和餐馆接单两个交互。</p><p><img src=\"https://static001.geekbang.org/resource/image/fa/d0/fac704c6fdf992540968669e1a468bd0.jpg?wh=1920x1326\" alt=\"图片\"></p><p>FoodCome是一个单体系统，它运行起来后，外层六边形上的接口有这么2种：</p><p>1.用户接口，用户有2种类型，一个是食客顾客，一个是餐馆业主。顾客通过手机下单，进入到FoodCome系统，而餐馆通过FoodCome的Web客户端可以查看和接受订单。</p><p>2.适配器接口，和第三方系统的集成接口。FoodCome集成了物流系统、通知系统和支付系统。顾客的订单通过支付系统完成支付后，餐馆开始加工，加工完毕后，食品通过物流系统快递给顾客。整个工作流，都会有状态的变更通知发送给用户。</p><p>2种接口明确了，我们再具体分析下测试需求都有哪些。</p><h2>用户接口的测试需求</h2><p>想定义测试需求，先要明确功能需求。功能需求是描述软件的功能，听着是不是像循环定义？想描述清楚软件的功能并不容易，这里我们借用迈克·凯恩提出的方法，一个软件软件功能需求要回答这三个问题：<strong>第一，这个功能存在的价值是什么？第二，软件是怎么实现这个价值的？第三，这个功能能给谁带来价值？</strong></p><h3>用户故事User Story</h3><p>刚才说的这三个问题，到后来就成了User Story的表达3要素，WHY为什么、WHAT是什么和WHO为谁，把这三个要素说明白了，这个功能也就表达出来了。</p><p>我们用FoodCome举个例子。“用户下单买食物”这么简单一句话，算不算一条功能需求呢？算，因为它包含了那3个基本要素，目的（WHY）、行为（WHAT）和人物（WHO）。用户是主体人物。他做了什么呢？下单。下单有什么价值呢？能得到食物。所以这就是一个最精简的功能需求表达。</p><p>细节是魔鬼，我们再来看个反例。相比来说，我相信不少人都会看到这样的需求：“用户使用username，password来登录FoodCome。”这是不是一条功能需求呢？</p><p>这句话里好像也包含了目的、行为和人物，但它并不是一个合格的功能需求。“使用username，password”和“登录FoodCome”这两个都是登录的行为，还是在重复WHAT，并没有说明登录的价值。</p><p>所以它的正确描述，应该是“用户登录FoodCome来购买食物”，或者“用户登录FoodCome来获得优惠券”，“购买食物”和“获得优惠券”是受登录保护的功能，也是能给用户带来价值的事情。</p><p>这个细微的差别，迈克·凯恩曾经这样说“A user story is a way of remembering that a piece of work why need to be done, without committing to actually doing it, or diving into the details too soon”。意思是，功能需求的主要目的是描述功能的商业价值逻辑，而不是刻画实现的细节。</p><p>如果我们用一个语句范式来组织前面三个要素，就是下面这样：</p><pre><code class=\"language-java\">As a&nbsp;&lt;type of user&gt;\nI want&nbsp;&lt;capability&gt;\nso that&nbsp;&lt;business value&gt;\n</code></pre><p>用了这样一组关键字，as a说明用户的角色，I want to后面是描述用户的行为，so that是获得的商业价值。<br>\nFoodCome的下单功能描述就变成了这样：</p><pre><code class=\"language-plain\">As a customer of foodcome,  \nI want to place an oder for food\nso that the food arrive in 30 minutes \n</code></pre><p>作为顾客，我希望能在FoodCome上下一个单，然后食物在30分钟内就来到我眼前，这就是FoodCome下单给我带来的价值。</p><p>用户下单之后，餐馆就会接单。FoodCome的接单功能描述如下：</p><pre><code class=\"language-plain\">As a restaurant of foodcome,  \nI want to accept an order from customer\nso that I can be paid after deliver the food \n</code></pre><p>那你可能会说，这个需求很笼统啊，作为开发人员不知道怎么实现，作为测试人员也不知道怎么测？好，这时就要进入BDD feature阶段了。</p><h3>测试需求BDD Feature</h3><p>BDD的全称叫做Behavior Drive Development，<strong>行为驱动开发模式</strong>。想达到驱动开发的程度，这个Behavior行为的定义就要足够细化，开发人员知道怎么去实现了，同样，测试人员也知道该怎么测试了。</p><p>BDD是怎么做的呢？它把User Story细化成一个或多个feature，每一个feature都是一个可测试的场景。</p><p>这个feature的文件书写也是有格式要求的，通过一个叫做Gherkins的语法关键字模版来写feature文件。</p><p>Gherkins提供的常见关键字有：</p><p><strong>Given:</strong>  用户场景的前提条件，可以是时间条件，也可以是另外一个用户场景的输出结果。</p><p><strong>When</strong>:  用户在这个场景里做的行为操作</p><p><strong>Then</strong>:   行为的输出结果</p><p><strong>And</strong>:  连接多个关键字</p><p>Gherkins还提供了更多其他关键字，你可以参看<a href=\"https://cucumber.io/docs/gherkin/reference\">这里</a>了解更多。</p><p>使用Gherkins语法，描述下单的Feature，是下面这样的：</p><pre><code class=\"language-plain\">Given a consumer\n  And a restaurant\n  And a delivery address/time that can be served by that restaurant\n  And an order total that meets the restaurant's order minimum\nWhen the consumer places an order for the restaurant\nThen consumer's credit card is authorized\n  And an order is created in the PENDING_ACCEPTANCE state\n  And the order is associated with the consumer\n</code></pre><p>我们可以看到，在Given、When、Then这些关键字的组合下，BDD feature比User Story丰满多了：一个用户下单的条件是什么，有地址的约束、信用卡的付款授权，下单之后的结果是什么，订单在系统里被创建，状态是Pending_Acceptance。</p><p>同样的，餐馆接单的feature文件如下：</p><pre><code class=\"language-java\">Given an order that is in the PENDING_ACCEPTANCE state\nWhen a restaurant accepts an order with a promise to prepare by a particular\n&nbsp; &nbsp; &nbsp;time\nThen the state of the order is changed to ACCEPTED\n&nbsp; And the order's promiseByTime is updated to the promised time\n</code></pre><p>Given、And、When、Then这些关键字，和编程语言里的If，else，then相似，所以Gherkins描述方法从自然语言向编程语言迈进了一步，更加详细精准，也更符合软件技术人员的习惯。</p><p>到这里，我们就通过BDD把可测试需求表达出来了。这个Feature要细化到什么程度呢？从测试角度来看，要达到可测试的程度，也就是说要能够通过feature来验收User Story，所以，feature在敏捷开发里又叫Accept Criteria，在传统测试里叫做验收标准。</p><p>我画了一张图，User Story和Feature的关系，你会看得更清楚一些</p><p><img src=\"https://static001.geekbang.org/resource/image/0b/68/0bdf40d5dcb13050bd3232e2b2580468.jpg?wh=1920x1185\" alt=\"图片\"></p><p>既然有了测试需求，建立了验收标准，后面再做验收测试就很轻松了，这个怎么做验收测试的问号，我先卖个关子，留到第16讲为你揭开答案。</p><h2>适配器接口的测试需求</h2><p>说完了功能需求如何表达，我们再来看看适配器接口。</p><p>FoodCome和3个外部服务有集成，分别是物流系统、通知系统和支付系统。和用户接口的强业务属性不同的是，适配器接口的技术属性强，因为适配器层走的都是协议和数据，这里我们的难点是，每个适配器接口用到的协议是不一样的，比如物联网用的MQTT，同步调用用restAPI，异步调用用Message Queue等等，那怎么把他们的测试需求梳理出来呢？</p><p>像使用Gherkins语法来表达功能需求一样，我们也可以通过一个契约的概念，理清集成点的内容。</p><p>一份契约的形成，包含以下几个要素：</p><ul>\n<li><strong>服务提供者</strong>：提供服务的一方</li>\n<li><strong>服务的消费者:</strong>调用服务的一方</li>\n<li><strong>交互风格</strong>: 双方交互的类型</li>\n<li><strong>契约的实现方式</strong>:  交互用到的具体协议和方法</li>\n<li><strong>契约内容</strong>： 交互的数据<br>\n在软件设计阶段，就应该定义契约，用什么服务、基于什么协议、消费者需要发送什么request，以及服务提供者返回什么response等等；在软件开发阶段，开发人员需要按照契约来实现代码；到了测试阶段，我们的任务就是验证双方是否按照之前定义的契约进行交互。</li>\n</ul><p>按照契约格式，可以整理出FoodCome的集成点，结果你可以参考下表：</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/81/e18114d6163ec3b7a096a2d46b618d81.jpg?wh=1920x850\" alt=\"图片\"></p><p>在实际工作中，你可以把这张表扩展和细化，加上版本信息，项目计划等等，让这个表更具有可操作性。</p><p>好，到这里，收割一下成果，对于单体系统，我们用六边形法找到了系统对外的交互点。这些交互点上，有不同类型的需求。根据测试点类型，我们采用不同方法来表达这些测试需求。</p><p>1.功能需求用gherkins语法表达出user story。</p><p>2.把第三方服务的集成点整理成契约。</p><p>其实测试需求不仅是功能需求和第三方服务集成这两种，还有安全性、兼容性、易用性和性能等方面的需求。不过我们这个专栏关注的是自动化测试，所以其他需求不做讨论。</p><h2>小结</h2><p>这一讲我们学了3种方法：六边形架构法、User Story三要素，Gherkins feature表达方法和接口契约。在订餐单体测试需求分析中，这些方法是如何应用的呢？我们一起回顾一下。</p><p>我们先用六边形架构法来从概要层面上观察系统的对外交互方式，也就是我们的测试点。然后根据测试点的不同类型提炼需求：对于功能性需求，用Gherkins表达方法来表达成用户故事；而接口性需求，用契约要素来厘清双方的“权利”和“义务”。</p><p>在实践中，这些测试需求不会那么理想，在设计阶段定义清楚了，就会一直不变，我们这里重点关注的是需求表达的格式化和文档化，做到这些，可以入库进行变更管理流程，作为我们下一步自动化测试设计和实现的基础。</p><p>另外，从这个过程中，你能感受到，想要把测试需求整理得清楚、完备，测试人员应该尽早尽多地参与到软件需求分析、软件设计等活动中去。这些能帮助我们更全面地理解“测什么”。</p><p>今天我们研究的还是一个单体应用，下一讲这个单体应用会演变成服务集群，测试需求的复杂度会加大，我们还要使用更多的方法来应对挑战，敬请期待。</p><h2>思考题</h2><p>今天我留两个思考题，你可以选择自己喜欢的题目说说看法。</p><p>1.思考一下你工作的软件系统，它们的测试需求在哪里？是怎么被表达的？怎么存储和变更的？</p><p>2.你遇到过什么奇葩的测试需求，如果换你来提交需求文档，你会如何改进它？</p><p>欢迎你在留言区跟我交流讨论，也推荐你把这一讲分享给身边的同事、朋友，说不定就能帮他解决需求整理的困惑。</p>","neighbors":{"left":{"article_title":"06｜左移&右移：测试如何在Dev和Ops领域大展身手？","id":501526},"right":{"article_title":"08｜需求提炼（二）：微服务集群要测什么？","id":503214}}},{"article_id":503214,"article_title":"08｜需求提炼（二）：微服务集群要测什么？","article_content":"<p>你好，我是柳胜。</p><p>随着互联网发展和软件场景普及，单体应用逐渐暴露出致命缺陷，比如过于庞大，大大增加系统的复杂度、交付速度变慢，协作困难、错误难以隔离、维护成本巨大等等。</p><p>同时，软件技术也在发展，出现了VMware、Docker和Kubernetes等轻量化部署方式，这使得拆分的困难变小，部署的成本降低。微服务架构诞生后，一个系统拆分成多个独立开发和运行的服务，这个服务不管大小，业界都管它叫微服务。它们也有一套服务治理的技术规范，用来保证部署和运行的可靠性和扩展性。</p><p>微服务集群的开发方式确实方便了用户需求快速实现和交付，但今天我们的关注点是，从测试角度看，微服务相比单体应用有什么不一样？有没有新的测试点？</p><p>这一讲我们将继续延续FoodCome的例子，看看它从单体应用变成微服务架构之后，给测试工作带来的变化和挑战。</p><h2>微服务架构下的FoodCome</h2><p>在上一讲，单体应用的FoodCome是这样的：<img src=\"https://static001.geekbang.org/resource/image/bd/a3/bddb6ed5729850bb7340033b437775a3.jpg?wh=1920x1369\" alt=\"图片\" title=\"FoodCome单体架构图\"></p><p>随着业务规模的扩大，开发人手增加，FoodCome被拆分成5个微服务，具体如下：</p><ul>\n<li>订单服务：处理用户下的订单；</li>\n<li>物流服务：Foodcome内部的物流管理，与外部物流对接；</li>\n<li>餐馆服务：管理餐馆的信息，参与订单的工作流；</li>\n<li>账户服务：管理订单里的顾客信息，和外部的支付系统对接。</li>\n<li>通知服务：产生消息通知用户，和外部的邮件系统对接。</li>\n</ul><!-- [[[read_end]]] --><p>由此组成的FoodCome的微服务架构如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/21/b1e740b327ea837f1278f1a44e754321.jpg?wh=1920x1030\" alt=\"图片\" title=\"FoodCome微服务架构图\"></p><p>在这个架构下，原先单体应用的对外接口保持不变，但是单体应用内部被5个独立的微服务取代。用户的订单请求先通过API网关到达订单服务，完成支付后，餐馆接单，再通过物流系统交付订单。</p><p>每个微服务实现自治，独立开发和发布部署，加快发布速度。而且增加新功能也很方便，比如登录鉴权，在这个图中再增加一个认证服务就可以，这是给客户带来的好处。</p><p>现在的问题是，这给测试带来哪些变化呢？分拆后，FoodCome系统变成了微服务集群，就像一部巨大机器，由多个零件组成，互相咬合，一起工作。作为测试人员，不但要验证每个零件是合格的，还要有办法预测它们组装起来的机器也能正常工作。</p><p>这里的测试难点是，微服务的数量增加，服务间的交互量也会剧增，相比单体系统，集成测试在微服务集群架构下更加关键。</p><p>要做集成测试，我们就先搞明白微服务间是怎么交互的。在微服务架构下，交互可以有多种风格，比如RPC远程过程调用、REST风格、Message Queue消息队列等等。根据交互的方法和风格，我把它们整理出一个表格，方便你理解。</p><p><img src=\"https://static001.geekbang.org/resource/image/6c/02/6cd53f0b8988bc03e77a996106f13302.jpg?wh=3323x1052\" alt=\"\"><br>\n在FoodCome采用了两种交互方式，RestAPI和Message Queue。</p><p>RestAPI用来处理实时性强的服务间交互，比如前端通过API网关调用订单服务来下订单。</p><p><img src=\"https://static001.geekbang.org/resource/image/d2/57/d231ac53f77f707c09f15d872af69d57.jpg?wh=1920x711\" alt=\"图片\"></p><p>Message Queue用来处理异步的交互，订单服务和通知服务之间通过Message Queue来交换信息.</p><p><img src=\"https://static001.geekbang.org/resource/image/2d/98/2d4c9f4fa823ab4cb0b877161a6f8f98.jpg?wh=1920x855\" alt=\"图片\"></p><p>下面我们来看一下这两种交互方式的具体实现，然后找出测试点。</p><h2>REST</h2><p>我们需要先知道Rest接口是怎么设计的，才能找出后面都要测什么。</p><h3>什么是REST</h3><p>REST是Representational State Transfer的缩写，叫做表现层状态转换。听起来挺拗口，但我一说你就能懂，它其实是一组松散的规范，不是严格的协议，也不是强制的标准。这个规范的目的就是让API的设计更加简单易懂。</p><p>它包含以下几个基本原则：</p><p>1.REST是基于HTTP协议的；<br>\n2.通过HTTP的URL暴露Resource资源；<br>\n3.通过HTTP的操作原语，提供对Resource的操作，GET、 POST、PUT、DELETE对应着增删改查的操作。</p><p>只要开发人员懂HTTP协议，按照上面的规则用REST风格表达他的API是很容易的。同样，另外一个开发人员看到REST API，也很快就能知道这些API是干什么用的，几乎不用看难懂的文档。</p><p>这是REST的优点，REST风格下设计的AP，学习成本非常低，所以互联网上有很多服务都是通过REST方式对外提供API，比如亚马逊的AWS云服务、Google的Document服务等等。</p><p>当然，现实中的REST，从2000年概念诞生到现在发展了20年，在上面的基本规范上又增加了很多内容，让REST接口具备自解释、可发现等优势，有兴趣你可以看RichardSon提出的 <a href=\"https://martinfowler.com/articles/richardsonMaturityModel.html\">REST四级成熟度模型</a>。</p><h3>Order Service的REST API设计</h3><p>遵循REST规范，Order Service的接口设计可以按照“名词-动词”的思路来捋清。</p><p>首先寻找名词，Order，它对应REST上的一个Resource资源：</p><pre><code class=\"language-plain\">http://api.foodcome.com/api/v1/orders\n</code></pre><p>再找到动词“下单”，它对应HTTP协议上的POST原语，对Orders资源发送POST请求就是下单：</p><pre><code class=\"language-plain\">POST http://api.foodcome.com/api/v1/orders\n</code></pre><p>之后将“查询订单”这个动词，转成HTTP协议上的GET原语，查询条件orderID以参数形式加在URL里：</p><pre><code class=\"language-plain\">GET http://api.foodcome.com/api/v1/orders?orderID=123456\n</code></pre><p>同样，修改订单使用PUT原语，删除订单使用DELETE原语。</p><p>我们再用同样的方法来把其他名词“顾客”和“餐馆”，转成Resource和操作：</p><pre><code class=\"language-plain\">http://api.foodcome.com/api/v1/customers\nhttp://api.foodcome.com/api/v1/restaurants\n</code></pre><h3>Order service的RestAPI规格定义</h3><p>不成熟的开发团队，经常是一边写代码，一边设计API，这样做的结果不难推测，一千个开发人员会写出一千个Order Service API，虽然他们都声称遵循了REST规范。</p><p>所以，好的实践是，开发团队需要先设计RestAPI，并把它表达出来，然后团队就可以进行评审，达成理解一致。</p><p>那表达的载体是什么呢？这里就要提到Interface Definition Language这个概念了，顾名思义，<strong>IDL是接口定义语言，它通过一种独立于编程语言的语法规则来描述API。</strong>不同类型的API，它的IDL是不一样的。</p><p>我们用REST主流的IDL，也就是OpenAPI的语法规范，来描述下订单的这个接口的参数，把请求和响应写在一个YAML文件里。</p><pre><code class=\"language-yaml\">\"/api/v1/orders\":\n&nbsp; &nbsp; post:\n&nbsp; &nbsp; &nbsp; consumes:\n&nbsp; &nbsp; &nbsp; - application/json\n&nbsp; &nbsp; &nbsp; produces:\n&nbsp; &nbsp; &nbsp; - application/json\n&nbsp; &nbsp; &nbsp; parameters:\n&nbsp; &nbsp; &nbsp; - in: body\n&nbsp; &nbsp; &nbsp; &nbsp; name: body\n&nbsp; &nbsp; &nbsp; &nbsp; description: order placed for Food \n&nbsp; &nbsp; &nbsp; &nbsp; required: true\n&nbsp; &nbsp; &nbsp; &nbsp; properties:\n          foodId:\n            type: integer\n          shipDate:\n            type: Date\n          status:\n            type: String\n            enum:\n            - placed\n            - accepted\n            - delivered\n&nbsp; &nbsp; &nbsp; responses:\n&nbsp; &nbsp; &nbsp; &nbsp; '200':\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; description: successful operation\n&nbsp; &nbsp; &nbsp; &nbsp; '400':\n          description: invalid order\n</code></pre><p>到这里，FoodCome服务间的REST接口规格说明书就生成了！</p><p>这个规格说明书定义了客户端和服务端之间的契约，顾客要下单的话，客户端应该向服务端\"api/v1/orders\"发送一个请求，里面包含了食品的代码、日期等等，而服务端成功则返回一个HTTP 200的响应，失败返回一个HTTP 400的响应。</p><h2>异步消息</h2><p>说完了同步常用的REST，我们再分析一下异步消息。</p><p>什么是异步消息呢？消息就是客户端和服务端交换的数据，而异步指的是调用的方式，客户端不用等到服务端处理完消息，就可以返回。等服务端处理完，再通知客户端。</p><p>异步消息在微服务集群的架构里，能够起到削峰、解耦的作用。比如FoodCome在订餐高峰时段，先把订单收下来，放到消息队列，排好队，等待餐馆一个个处理。所以异步消息是现在业界很常用的一种服务交互方式，它的技术原理是消息队列，技术实现是消息代理，有Kafka、RabbitMQ等等。</p><p>而开发人员在设计微服务时，首先要设计异步消息接口，定义好我的微服务什么时候往消息队列里放消息，放什么样的消息。同样，也要定义好取消息的时机和方法。</p><h3>异步消息接口设计</h3><p>好，那我们就来看一下订单服务是怎么设计它的异步消息接口的。</p><p>首先，要定义消息体，订单服务会向外发出三种消息OrderCreated、OrderUpdated、OrderCancelled。消息里包含了order ID、order items、order Status这些字段。</p><p>其次，还要说明这个消息发送到哪个channel里。Channel就是消息的队列，一个消息代理里可以有多个channel，每个channel有不同的功能。</p><p>因为order的消息有严格的时序，比如，OrderCancelled和OrderCreated这两个消息的顺序反了的话，会引起程序处理的混乱。所以，我们把这三种消息都发送到一个叫order的channel里。</p><p>如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/d9/f3f7b68948f2f64acc82c53ed833bfd9.jpg?wh=1920x1021\" alt=\"图片\"></p><h3>异步消息接口规格说明书</h3><p>好，下面就到关键环节了，对于测试人员来说，我们最关心的就是<strong>接口规格说明书</strong>，跟REST一样，消息队列也需要找到IDL来描述接口上的信息。</p><p>RestAPI的主流IDL是OpenAPI，相对应地，MessageAPI的IDL是 AsyncAPI。上面的Order消息接口，用AsyncAPI规范来定义，会是下面这个样子：</p><pre><code class=\"language-yaml\">asyncapi: 2.2.0\ninfo:\n&nbsp; title: 订单服务\n&nbsp; version: 0.1.0\nchannels:\n&nbsp; order:\n&nbsp; &nbsp; subscribe:\n&nbsp; &nbsp; &nbsp; message:\n&nbsp; &nbsp; &nbsp; &nbsp; description: Order created.\n&nbsp; &nbsp; &nbsp; &nbsp; payload:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: object\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; properties:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; orderID:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: Integer\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; orderStatus:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: string\n</code></pre><p>这段代码描述的是，订单服务在运行时会向Order channel输出OrderCreated消息，这个OrderCreated消息包含了2个字段，order的ID和order的状态。</p><h2>在设计阶段测试要做什么？</h2><p>刚刚我们花了不少篇幅分析API设计，如果你之前一直只做测试，也许会疑惑：“这些看起来是开发领域的知识啊，是不是跑题了？”其实我想说的是，API领域是开发、测试共同关注的。测试应该主动参与到这些领域的活动，才能让测试更加有效。</p><p>我曾经看到过两个微服务团队各自开发都很快，但是微服务一上线，就发现问题了，有的是接口就对不上，有的是数据类型不一致等等千奇百怪的问题，这些问题花了大把诊断时间不说，甚至会给客户带来损失。</p><p>单体应用基本没这些问题，它们是微服务集群的典型问题，那微服务集群的测试，该怎么避免这些问题呢？</p><p>有两个比较好的实践，推荐你尝试一下。</p><p>第一，测试设计先行原则。</p><p>测试设计先行，需要的是开发设计先行。开发不做设计，测试干着急也没法设计。怎么督促开发设计先行呢？一个关键指标是，它在设计阶段是否输出了接口规格说明书。<strong>对于开发工作来说，是需要去代码实现的开发需求。对于测试工作来说，它就是测试需求，需要根据它写测试案例。</strong></p><p>第二，找到合适的IDL来表达接口设计。</p><p>一份周密、高质量的测试需求，会是成功测试的开始。所以这个接口规格说明书不仅要有，还得规范，能指导我们生成测试案例。</p><p>怎么做到呢？让开发人员写一份Word文档？一千个开发人员能写出一千个规格说明。这时，IDL的价值就显现出来了，它提供一套规范和语法，像一门专用语言，能精准描述接口。而且它与编程语言无关，可以根据IDL做Java的实现，也可以是C++, JavaScript，Python等等。</p><p>OpenAPI和AsyncAPI是IDL族群里的2种。我这里列出一个常见的IDL列表，你可以看看你领域里的IDL是什么。</p><p><img src=\"https://static001.geekbang.org/resource/image/7f/4d/7ffae5be80f21c28d878156fd5e3664d.jpg?wh=1920x623\" alt=\"图片\"><br>\n找到了IDL，你和团队就可以一起商量怎么践行设计先行原则，使用IDL设计接口了。有了规格说明书之后，之后我们还会讲到，在测试阶段怎么运用它们设计、开发测试案例，敬请期待。</p><h2>小结</h2><p>这一讲，我们先分析了FoodCome升级成为微服务集群架构后，发生了哪些变化。其中最主要就是服务间的交互量大幅增加。Foodcome采用了2种交互方式，一是RestAPI同步接口，用来接收用户的订单；二是Message Queue的异步接口，用来处理用户的订单。</p><p>这两种API在设计中用到了不同的IDL：OpenAPI和AsyncAPI，产生出来的接口规格说明书是<strong>YAML文件</strong>。这个YAML文件对开发很重要，可以保证他们开发出来的微服务在集成时，能咬合在一起；对于测试来说也很重要，这是后续测试的测试需求。在后面的测试阶段，我们需要去验证，服务是不是遵循了接口。</p><p>当然微服务架构还有一些其他变化，比如服务的治理模式，分布式事务，可靠性的实现等等。我们本专栏关注和自动化相关的测试需求，其他变化可能需要开一个新专栏才能详细讨论。</p><p><img src=\"https://static001.geekbang.org/resource/image/08/61/0853ba4b25a66d1e21ebd2556a62bd61.jpg?wh=1990x1520\" alt=\"\"></p><h2>思考题</h2><p>在你的项目组里，能不能推行设计先行，你预想会遇到什么困难，要怎么应对呢？</p><p>欢迎你在留言区和我交流互动，也推荐你把这一讲分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"07｜需求提炼（一）：单体应用要测什么？","id":502863},"right":{"article_title":"09｜3KU法则：为一个订餐系统设计全栈测试方案","id":504644}}},{"article_id":504644,"article_title":"09｜3KU法则：为一个订餐系统设计全栈测试方案","article_content":"<p>你好，我是柳胜。</p><p>上一讲，我们找出了FoodCome订餐系统各截面的测试需求，今天我们就根据这些需求，完成测试设计，给这个订餐系统设计一个全栈的测试方案。我们前面讲到的很多思路和原则，都能在今天的课程里得以应用。</p><p>这个测试方案非常关键，它能回答自动化测试设计中的四大基本问题：做不做自动化？在哪里做、怎么做、怎么运行。这四个基本问题梳理清楚了，自动化测试项目就相当于有了骨架。</p><h2>FoodCome的订餐需求</h2><p>我们先列出前几讲提炼的测试需求，每项需求都过一遍上面的问题清单。让我们头脑中的测试方案，形成一个文档化的列表。</p><p>结合FoodCome订餐系统的例子，我们把测试需求整理如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/63/6f/63508b5588b2d7ed2bd06ef5d4efd46f.jpg?wh=1920x657\" alt=\"图片\" title=\"测试需求表1.0\"></p><p>订餐系统还有很多其他的测试需求，比如兼容性、安全性等等，因为本专栏的关注点是自动化测试，我在这里就不再列出来了。</p><h2>做不做自动化测试？</h2><p>有了文档化的测试需求列表后，我们在设计自动化测试方案时，需要先想清楚，这些需求做不做自动化测试？</p><p><strong>测试四象限法则</strong>能帮我们有效完成这个思考过程。这个测试四象限，是布雷·麦瑞克提出来的方法模型：根据需求的性质和等级2个维度，对测试需求进行分类。</p><p>一个维度是<strong>测试需求的性质</strong>，是技术性还是业务性的？通俗来说就是，如果这个需求越靠近程序员的思维，比如算法、接口、事务等等，它的技术性就越强；而越靠近用户的思维，比如工作流，场景等等，就是业务性越强。</p><!-- [[[read_end]]] --><p>另一个维度是<strong>测试需求的等级</strong>，也就是需求属于关键性的还是精益性的？你可以这样理解，关键性的需求指的是，对于用户显式而重要的需求。比方说，一个系统必须能下单，才能成为订餐系统。而精益性的需求指的是用户隐式的需求，没有直接表达出来，但也可能很重要，比如性能、可靠性等等。</p><p>好，明白了性质和等级这2个维度后，我们现在用这两个维度把测试需求列表过一遍，把它们填到象限里。</p><p><img src=\"https://static001.geekbang.org/resource/image/33/e0/33a6ee6dc3056306eb7a9ec27d6f04e0.jpg?wh=1920x1430\" alt=\"图片\" title=\"测试四象限\"></p><p>先看算法、接口、分布式事务测试，它们技术性强、也是关键需求，放在了第一象限，WebUI测试业务性强且属于关键需求，放在了第二象限，易用性测试放在第三象限，性能和可靠性放在第四象限。</p><p>针对每个象限，测试四象限法建议自动化测试实施策略如下：</p><ul>\n<li>第一象限里的测试需求是100%全部自动化；</li>\n<li>第二象限里的测试需求是自动化+手工；</li>\n<li>第三象限里的测试需求是手工测试；</li>\n<li>第四象限里的测试需求是通过工具和框架来执行，追求0代码。</li>\n</ul><p>四象限的策略你不必死记硬背，因为这些只是表象，底层逻辑还是ROI，学会了分析思路你自己也可以推导结论。</p><p>举例来说，第一象限里的算法和接口测试，因为它们验证的是关键功能，所以回归测试高，自动化测试收益就大。而技术性强，意味着这类测试不会因业务变化受太大影响，所以开发、维护的成本就低。因此，第一象限的测试需求可以100%自动化。至于其他象限的情况，你可以自己试着推演一下，同样符合ROI的规律。</p><p>好，到这里，通过四象限法则，我们已经有了一个自动化测试Yes or No的决定。更新一下表格，加入自动化测试Yes or No一列。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/71/322a2848b6ed0fef4991f5f1c4694871.jpg?wh=1920x992\" alt=\"图片\" title=\"测试需求表2.0\"></p><p>这张表是我们自动化测试方案迈出的第一步。现在，我们就能根据测试需求的性质和类型，判断它的测试方式是自动化测试还是手工测试了。</p><h2>在哪个层面做自动化测试？</h2><p>确定了测试方式，我们还要进一步考虑，这些测试需求的自动化测试是应该在哪个层面实现呢？在单元测试、接口测试还是UI自动化测试？</p><p>在专栏的<a href=\"https://time.geekbang.org/column/article/497405\">第二讲</a>里，我们学习过3KU测试矩阵和3KU测试金字塔，那就可以把它们应用到FoodCome的自动化测试设计了。</p><p>排除掉前面表格里提到的手工测试项，我们把其余内容填入到3KU测试矩阵里。</p><p><img src=\"https://static001.geekbang.org/resource/image/c7/55/c71a9cf6753e805b3133e4be46285d55.jpg?wh=1860x678\" alt=\"图片\" title=\"3KU测试矩阵\"></p><p>按照自动化测试寻求最大ROI实施层面原则，我们把上面的表格，转换成ROI自动化测试金字塔。</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/10/9ce694440dc1802fe7cd68c96e26b110.jpg?wh=1920x1074\" alt=\"图片\" title=\"ROI自动化测试金字塔\"></p><p>在ROI自动化测试金字塔里，我们又确定了两个信息。</p><p>第一，各个需求自动化测试实现的截面，单元测试截面上测试算法和分布式事务，接口测试层面来验证技术契约、服务接口和分布式事务，UI层面做用户下订单自动化测试和部分业务契约的验证。</p><p>第二，各部分自动化测试规模的配比，按照金字塔形状，单元测试案例最多，接口测试居中，UI自动化测试案例最少。</p><p>现在，我们再次更新表格，把在哪一层做自动化，还有工作量分配的比例加进去。</p><p><img src=\"https://static001.geekbang.org/resource/image/1a/4b/1a057d7fca983dd42707207115e0444b.jpg?wh=1920x837\" alt=\"图片\" title=\"测试需求表3.0\"></p><p>好，到这里，这个表格里，我们针对每个测试需求，已经做出了自动化测试方案，包括做不做自动化测试，在什么层面做自动化测试，做多少自动化测试。</p><p>下面，我们继续完善，把这个方案想周全。</p><h2>用什么工具做自动化测试？</h2><p>选择对了工具和框架，会让自动化测试事半功倍。这个“选对”的意思，就是工具必须适合你的项目、你的团队。</p><p>在<a href=\"https://time.geekbang.org/column/article/498458\">第三讲</a>“怎么选型自动化测试工具和框架”中，我已经和你分享了怎么选择工具的方法和原则。在工作量大的单元测试和接口测试，要选择成熟和支持模块化开发的工具，比如JUnit和Restassure；在工作量较小的UI测试，工具的稳定性最重要，其次追求效率。</p><p>在这里，我直接给结果，具体列出了FoodCome各个自动化测试技术和工具，你可以想想为什么选取它们。</p><p><img src=\"https://static001.geekbang.org/resource/image/00/fa/009d1455457c47ac454409217a92c0fa.jpg?wh=4131x1947\" alt=\"\" title=\"测试需求表4.0\"></p><h2>怎么运行自动化测试？</h2><p>在自动化测试方案里，除了做不做自动化测试，以及在哪个层面做。我们还要考虑清楚另外一个事，就是自动化测试开发出来后，它们在什么时候运行。</p><p>在第一模块里，我们讲ROI的时候，经常提到一个自动化测试的收益，其重要因子之一就是它的运行次数。所以，设计ROI高的自动化测试的运行场景是很关键的，而软件部署管线Deployment Pipeline就是重要的自动化测试运行场景之一。</p><p>那Deployment Pipleline是怎么设计的呢？先从概念说起，在2010年，Jez Humble 出版了《持续交付》一书，这里提出了部署管线的概念：</p><blockquote>\n<p>“部署管线是代码从开发人员的个人电脑到生产环境的自动化过程”。</p>\n</blockquote><p>为什么会叫管线呢？因为部署管线由一系列测试的阶段组成，每个阶段首尾相接，这就形成了一条流水线一样的管道。</p><p>部署管线通常是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/ea/f86f5a0da0c071b2a183b96ed36f1aea.jpg?wh=1920x604\" alt=\"图片\"></p><p>我们把FoodCome的自动化测试任务，填充到部署管线的各个阶段里去，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/d1/d4/d1yy204a9c8d3d6682e19545a5434dd4.jpg?wh=1920x850\" alt=\"图片\"></p><p>沿着部署管线发布的方向，也就是从左向右，自动化测试的运行速度由快变慢，而ROI也是由高到低。<strong>越靠近代码，活动越频繁，ROI就越高</strong>，而每一个关卡都会有失败的，最后能成功到达可部署生产环境的会是很少一部分，十次代码变更能有二次到生产环境。</p><p>所以，后面测试阶段得到运行的次数比较少，相应ROI也比较低。</p><p>这样的规律对我们工作有啥帮助呢？其实不难想到，我们要尽量把自动化测试放在管线的开始端，只要它具备条件，而且够快，这也是我们在<a href=\"https://time.geekbang.org/column/article/501526\">第六讲</a>里提到的自动化测试左移实践。</p><h2>小结</h2><p>今天，我们给FoodCome订餐系统整理了一套全栈自动化测试方案。这个方案里要回答自动化测试项目的四个问题，这些问题也是测试设计最基本的问题。那就是，给定一个测试需求，我们做不做自动化？具体在哪里做、怎么做、怎么运行。</p><p>1.要不要自动化？这里我们用到了测试需求2维度4象限法，帮助我们分析每个需求的性质和等级，遵循最佳实践来确定，它们需不需要自动化。</p><p>2.在哪个层面做自动化？分配多少工作量？这里我们用到了第一模块学到的3KU测试矩阵和3KU测试金字塔，从ROI的角度来回答这些问题。</p><p>3.用什么工具做自动化？每一个层面，我们都要使用工具来帮助我们做自动化。在这里，工具不选择最好的，但要根据团队和项目情况，选择最合适的、ROI产出最大的工具。</p><p>4.自动化测试怎么运行？应该物尽其用，我们投入工作量最大的单元测试和接口测试，它们应该在交付管线Pipeline里运行次数最频繁，这样才能收获好的ROI。</p><p><img src=\"https://static001.geekbang.org/resource/image/59/37/5993b6d561f18f16ee6eb24f8d691437.jpg?wh=1920x1080\" alt=\"图片\"></p><p>回答了这些基本问题后，我们自动化测试方案的骨架也就建立起来了。在实践中，你还可以结合团队的人手、项目的时间等等因素，来填充方案的血肉，比如设定任务的优先级等等。</p><p>接下来的课程里，我们还会进入到每一个测试点，看看怎么实现，敬请期待。</p><h2>思考题</h2><p>结合你的工作实践想一想，要怎么回答自动化测试设计中这四大问题？</p><p>欢迎你在留言区跟我交流，也推荐你把这一讲分享给更多同事、朋友，说不定就能帮Ta搞定下一次的测试方案设计。</p>","neighbors":{"left":{"article_title":"08｜需求提炼（二）：微服务集群要测什么？","id":503214},"right":{"article_title":"10｜单元测试（一）：原来测试可以这么高效","id":505695}}},{"article_id":505695,"article_title":"10｜单元测试（一）：原来测试可以这么高效","article_content":"<p>你好，我是柳胜。</p><p>提到单元测试，你可能第一反应是，这个不是归开发做么？作为测试的你，为什么不但要懂UI、接口测试，还要了解单元测试呢？学完今天的内容，你会有自己的答案。</p><p>先从一个常见的业务场景说起，开发同学在实现Order服务的时候，需要代码化一些业务逻辑。比如处理一个订单，要计算总价、优惠扣减、库存查验等等。</p><p>现在Order服务的开发想要测试自己写的这些代码是否预期运转，他最先想到的办法可能是，把Order服务构建完运行起来，向它发送HTTP Request “POST /api/v1/orders”, 然后检查返回的Response内容，看是不是订单已经如期生成。</p><p>这个方法当然也能达到测试目标，但是你已经学习过了3KU原则，就可以问开发人员一个问题 “同样的验证目标，能不能在ROI更高的单元测试阶段实现？”</p><p>你看，测试人员和开发人员的单元测试工作联系起来了，它们之前在实践中一直是不太交流的两个领域，现在需要相互配合，服务于测试的整体目标。</p><p>这一讲会用一个FoodCome系统里的一个Order服务的代码作为例子，帮你捋清楚单元测试能做什么，怎么做。</p><h2>制定单元测试策略</h2><p>我们先来看一下Order服务的内部结构，我画了一张Class关系图来展现内部的逻辑结构。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/a8/d5/a8b99e099953efc87cf3e34104d413d5.jpg?wh=1920x967\" alt=\"图片\"></p><p>我们先看图里的蓝色色块，通过这五个Class就能实现Order服务。</p><p>它们是这样分工的：<strong>OrderController</strong>接收Client发来的\"POST /api/v1/orders\" request,  交传给OrderService createOrder方法处理，再把生成的订单信息封装成Response返还给Client；</p><p><strong>OrderService</strong>是主要的业务逻辑类，它的createOrder完成了一个订单创建的所有工作，计算价格、优惠扣减，调用<strong>AccountClient</strong>做支付验证，调用<strong>RestaurantClient</strong>做餐馆库存查验。订单生成后，就交给OrderRepository去写DB生成订单记录。</p><p><strong>OrderRepository</strong>实现了和Order自带的数据库交互，读写操作。</p><p>知道了这些，还无法马上动工写单元测试代码，我们还需要考虑清楚后面这几件事。</p><h3>需要写多少个Test Class？</h3><p>这里我需要交代一个背景知识，那就是单元测试里的“单元”是什么？</p><p>如果你问不同的开发人员，可能会得到非常不一样的答案。在过程语言里，比如C、脚本语言，单元应该就是一个函数，单元测试就是调用这个函数，验证它的输出。而面向对象语言，C++或者Java，单元是一个Production Class。</p><p>FoodCome系统是Java面向对象语言开发的，包含5个Production Class，做单元测试，我们把规则设得简单一点，<strong>开发一个Test Class去测试一个Production Class，保持一对一的关系</strong>。</p><p>如下图所示，一个Test Class有多个Test Method。每个Method会Setup建立Production Class的上下文环境，Execute调用Production Class的Method，Assert验证输出，TearDown销毁上下文。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/77/e28060bbe8048fdc76ca6612d9915877.jpg?wh=1920x889\" alt=\"图片\"></p><h3>孤立型还是社交型？</h3><p>一个Test Class对应一个Production Class看起来简单明了，但理想虽然美好，现实却是复杂的。在实践中，很少有Production Class能独立运行。</p><p>我们拿出OrderSerevice这个Class的源代码看一下：</p><pre><code class=\"language-java\">public class OrderService {\n  //依赖注入OrderRepository, AccountClient, RestaurantClient\n  @Autowired\n  private OrderRepository orderRepository;\n  @Autowired\n  private AccountClient accountClient;\n  @Autowired\n  private RestaurantClient restaurantClient;\n  @Autowired\n  private OrderDomainEventPublisher orderAggregateEventPublisher;\n  public Order createOrder(OrderDetails orderDetails) {\n  //调用restaurantClient验证餐馆是否存在\n  Restaurant restaurant = restaurantRepository.findById(orderDetails.getRestaurantID())\n            .orElseThrow(() \n            -&gt; new RestaurantNotFoundException(orderDetails.getRestaurantID()));\n  //调用AccountClient验证客户支付信息是否有效\n  .............\n  //统计订餐各个条目，根据优惠策略，得出订单价格\n  float finalPrice = determineFinalPrice(orderDetails.getLineItems());\n  //生成订单\n  Order order = new Order(finalPrice,orderDetails);\n  //写入数据库\n  orderRepository.save(order);\n  return order;\n  }\n}\n</code></pre><p>看完代码，我们发现问题了，createOrder运行时，需要调用AccountClient、RestaurantClient和OrderRepository三个Class，如果它们不工作的话，createOrder就没法测试。</p><p>在单元测试里，这3个Class叫做Order Class的Dependency，这3个Class还会有各自的依赖，以此递归。到最后你会发现，想测试Order Class，需要整个服务都要运转起来。</p><p>我打个比方，方便你理解。就像本来你只想请一个朋友来派对，结果朋友还带来了他的朋友，以此类推，你最后发现，现场坐满了你不认识的人，自己的心情完全被毁掉了。</p><p>为了解决这个问题，有两种应对方法。</p><p>第一种孤立型，我只关注我的测试目标Class，而Dependency Class一律用Mock来替代，Mock只是模仿简单的交互。相当于我给派对立下一个规矩：客人不能带客，如果需要，就带个机器人来。</p><p><img src=\"https://static001.geekbang.org/resource/image/33/14/33d80dc58f0bd430184c686413cda914.jpg?wh=1920x864\" alt=\"图片\"></p><p>另一种是社交型，我还是关注我的测试目标Class，但是Depdency Class用真实的、已经实现好的Class。这就好比，我告诉大家，你们先玩，等你们派对结束，我最后再开个只有我自己在的派对。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/25/6561c6yy651cd072225d32ba42bf0c25.jpg?wh=1920x886\" alt=\"图片\"></p><p>社交型和独立型各有优缺点。</p><p><img src=\"https://static001.geekbang.org/resource/image/db/68/db7e538918edfae3fc78de89de85c768.jpg?wh=1920x639\" alt=\"图片\"></p><p>独立型的好处是确实独立，不受依赖影响，而且速度快，但是你要花费成本来开发Mock Class。</p><p>而社交型的好处是没有任何开发成本，但是有一个测试顺序的路径依赖，先测试依赖少的Class，最后才能测试依赖最多的那个Class。</p><p>在实践中，其实没有一定谁好的说法，就看怎么做，更加快捷方便。对于OrderService Class来说，我们两种策略都用。</p><p>我们用社交型处理Dependency OrderRepository，也就是先开发测试OrderRepository Class，再测试OrderService Class，为什么呢？OrderRepository和OrderService在同一个微服务内部，由同一个开发团队甚至同一个人开发，完全不用担心依赖会造成工作阻塞。</p><p>我们用孤立型处理Dependency AccountService和Restaurant Service，自己开发Mock service，因为这涉及到跨服务的依赖。等别的团队AccountService开发完，才能开始测自己的OrderService，这样的情况我们不能接受。</p><p>最后我们得出OrderService Class的测试策略图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/0c/44/0c4eaaece99bec7b2815ea7e4e3cfa44.jpg?wh=3370x1519\" alt=\"\"></p><p>到这里，我们已经大概捋清楚OrderService的单元测试要做哪些事了，可以分三步走。</p><p>1.开发一个OrderSerivceTest，来测试OrderService；<br>\n2.开发出来OrderRepository，作为OrderServiceTest的真实注入对象；<br>\n3.开发2个Mock Class：AccountClient和RestaurantClient，辅助OrderServiceTest运行。</p><p>“三步走”策略已定好，下面就撸起袖子加油干吧。</p><h2>OrderService的单元测试</h2><p>回到这一讲开头的那个问题 “同样的验证目标，能不能在ROI更高的单元测试阶段实现？”</p><h3>这个TestCase能不能在单元测试阶段做？</h3><p>OrderService是Order服务里业务逻辑最多的Class，因为它包办了创建订单的所有工作。所以测试创建订单可以粗略和测试OrderService划等号，那我们来研究一下OrderServiceTest怎么写。</p><p>我们需要创建一个名为OrderServiceTest的Class，在这个Test Class里，完成对OrderService对象的组装，它依赖的三个Class，通过构造函数的方式注入到OrderService对象里。</p><pre><code class=\"language-java\">public class OrderServiceTest {\n  //声明test需要用到的对象\n  private OrderService orderService;\n  private OrderRepository orderRepository;\n  private AccountClient accountClient;\n  private RestaurantClient restaurantClient;\n  @Before\n  public void setup() {\n    orderRepository = new OrderRespistory();  \n    //mock restaurantClient对象                    \n    restaurantClient = mock(RestaurantClient.class);\n    //mock accountClient对象\n    accountClient = mock(AccountClient.class);\n    //组装被测orderSerivce对象\n    orderService = new OrderService(orderRepository, restaurantClient，accountClient);\n  }\n  @Test\n  public void shouldCreateOrder() {\n    //组装订单内容\n    OrderDetails orderDetails = OderDetails.builder()\n        .customerID(customerID)\n        .restaurantID(restaurantID)\n        .addItem(CHICKEN)\n        .addItem(BEEF)\n        .build();\n    Order order = orderService.createOrder(orderDetails);\n    //验证order是否在数据库里创建成功\n    verify(orderRepository).save(same(order));                             \n  }\n}\n</code></pre><p>上面的测试代码基于Junit规范实现，为了帮你理解测试的主要逻辑，省去了一些关联度不高的代码。</p><p>OrderServiceTest运行时，会创建一个OrderService对象。我们先构造出订单内容OrderDetails，把它作为参数传递到createOrder的方法里。createOrder方法运行结束之后，预期结果是在数据库里生成一条订单记录。</p><p>这个下订单的TestCase如果通过UI来测，你需要打开页面手工、登录、输入订单信息、点击下订单按钮。内容如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/64/15/649baef02a7ed64ebd97c0534d607b15.jpg?wh=1920x1242\" alt=\"图片\"></p><p>下订单TestCase如果通过接口来测，你需要生成订单内容数据，然后发送一个POST请求到“/api/v1/orders”，内容如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/df/eb/df1564daa38c7bf4b08c92c9bfd9f4eb.jpg?wh=1920x737\" alt=\"图片\"></p><p>那我们来对比一下，同一个TestCase，在单元、接口和UI上运行的效果如何？</p><p><img src=\"https://static001.geekbang.org/resource/image/82/65/8209e0d25a7c06711138dce735ff2265.jpg?wh=1920x758\" alt=\"图片\"></p><p>单元测试能覆盖下订单功能的大部分业务逻辑，而且又早又快，是非常理想的自动化测试实施截面。这再次验证了在<a href=\"https://time.geekbang.org/column/article/497405\">第二讲</a>我们学过的3KU测试金字塔：“单元测试是ROI最高的自动化测试，自动化案例应该最多。”</p><h3>提高单元测试ROI</h3><p>既然单元测试又好又快，那么我们不妨把一些费力气的测试工作挪到这一层。哪些测试比较麻烦呢？</p><p>线上购物的场景就很典型，你在网购时一定用过优惠券，这些优惠券的使用条件十分复杂：要知道在什么时间、什么商品会有多大折扣，而且优惠券还存在叠加使用的情况，又有了各种规则，能把人搞晕。但用户可以晕，平台却不能晕，用了优惠券，最终结果还要非常精准地保证盈利，不能亏。</p><p>要是在UI层面测试优惠券，你需要重复运行大量的测试数据，来产生不同的优惠条件，这个代价是高昂的。</p><p>放在单元测试里，这个TestCase就好理解了。负责价格计算的是OrderService里的determineFinalPrice方法，在determineFinalPrice里需要考虑和计算各种优惠条件和规则，再输出一个最终价格。</p><p>因此，优惠券在单元测试里，就转换成了对determineFinalPrice方法的测试。</p><p>怎么测试这个方法呢？ 我们要构建多组OrderDetails数据，传到determineFinalPrice方法里去。这时，我们用JUnit测试框架里的DataProvider来完成这个工作：</p><pre><code class=\"language-java\">@DataProvider\npublic static Object[][] OrderDetails() {\n   return new Object[][]{\n      {1111,2222,\"佛跳墙\"},\n      {1112,2223, \"珍珠翡翠白玉汤\"}\n   };\n}\n\n@DataProvider(\"OrderDetails\")\n@Test\n  public void shouldCreateOrder(OrderDetails orderDetails) {\n    Order order = orderService.createOrder(orderDetails);\n    //验证order是否在数据库里创建成功\n    verify(orderRepository).save(same(order));                             \n  }\n</code></pre><p>对照代码可以看到，Test方法上加了一个注解@DataProvider，指定了Test方法的入口参数是来自于一个名为OrderDetails的数据源。OrderDetails是一个二维数组，存储了多条OrderDetails数据，有多少条数据，就会运行多少次Test方法。</p><p>这样，我们就不需要在UI上重复提交表单来做测试了，这个工作交给单元测试来做，在几毫秒内就完成上千条测试数据的测试了。Oh Yeah，原来单元测试可以这么Cool！</p><p>等从激动中缓过神来，你可能还发现了一个问题，不对？上面的Test方法好像没有验证输出的价格呀？</p><p>没错，这里有一个困难，determineFinalPrice方法实际上是实现了一个价格计算的算法，它会根据输入计算输出一个数值，算法怎么测？怎么验证它的输出是对的，还是错的？</p><p>这个领域业界的做法很不一样，有的会在测试代码里把算法又实现了一遍，然后得出一个数值作为预期值，跟开发代码算出来的数值做比对。</p><p>这种做法其实是错误的，<strong>因为测试代码里加入了被测单元的实现细节（算法逻辑）</strong>，你本来想验证产品线生产的产品A是不是合格，但你采用办法是让产品线再生产一个产品B，来比对A和B，这样验证没有意义，因为如果产品线本身就是有问题的，A和B都会是错的。</p><p>业界给这样的测试起了一个名，叫“领域知识泄漏”，你可以搜索一下“Domain Knowledge Leakage”，会发现各种各样的测试方法错误。</p><p>正确的做法是，你应该只关注产品A的合格标准，用标准来检查A就可以了。</p><p>在我们的情景里，很简单，把determineFinalPrice的输入参数和输出参数都写出来，作为常量。只要输入一个x1,x2,x3, 那就会得到y，然后就用x1,x2,x3,y这一组数据来测试determineFinalPrice方法就可以。</p><p>现在你可以思考一下，上面的代码应该怎么变动？相信你可以解决这个问题，也欢迎你在留言区晒一下你的“作业”。</p><h2><strong>小结</strong></h2><p>当今，业界一般都会把单元测试划到开发领域，接口测试和UI测试划到测试领域，这让两个领域很少交流。对于测试整体来说，这就存在着重叠和资源浪费。</p><p>做自动化测试的你，其实应该了解单元测试能做什么，都做了什么，甚至你应该设计好测试案例，让开发人员去实现。因为你是自动化测试的Owner和架构师，你应该对整体效益负责。</p><p>单元测试里很多内容：框架的接口、Mock的开发、Assert语句的使用等等，一本书都讲不完。今天我们通过FoodCome的代码里的一个OrderServiceTest的实现，学习了单元测试策略，以一个测试整体的视角来观察单元测试能做什么，在整个测试方案里的功能作用。</p><p>通过这一讲，你可以直观感受到，单元测试的ROI又高，速度又快。但在现实中，这是测试人员的一块短板，也是相对陌生的领域。所以，我<a href=\"https://time.geekbang.org/column/article/506638\">下一讲</a>还会继续单元测试的话题，谈谈怎样推动单元测试的“可测试性”，敬请期待。</p><h2>思考题</h2><p>去了解一下你开发团队里有没有做单元测试，有的话都做了什么。</p><p>欢迎你在留言区和我交流互动，也推荐你把这一讲分享给更多同事、朋友，说不定就能通过单元测试来解放双手，提高工作效率啦。</p>","neighbors":{"left":{"article_title":"09｜3KU法则：为一个订餐系统设计全栈测试方案","id":504644},"right":{"article_title":"11｜单元测试（二）：四象限法让你的单测火力全开","id":506638}}},{"article_id":506638,"article_title":"11｜单元测试（二）：四象限法让你的单测火力全开","article_content":"<p>你好，我是柳胜。</p><p>上一讲，我们写了OrderServiceTest测试类，来测试FoodCome的OrderService类。这样不管开发代码还是测试代码，都是简单清楚的。这是因为FoodCome的开发人员对代码有一个好的设计，实现了Controller、Service、Repository等Class的职能划分，OrderService类里专注订单管理，我们写出的OrderServiceTest才能集中火力测试。</p><p>但是好的设计不是天上掉下来的，有的团队在刚一开始写的代码结构性并不好，这有可能是项目的问题，需求不明确、赶进度，也有可能是开发人员的技术功底不扎实、抽象能力不够。所以，在软件生命周期内，需要持续重构，才能打磨出好的设计。</p><p>今天我们就用一个例子，来看看好的代码设计是怎么打磨出来的，而且，我们要从测试的角度来评估设计的效果，那就是单元测试容易写、覆盖率高、干净易懂，这又叫代码可测试性。</p><p>作为自动化测试架构师，你需要掌握观察和评估代码可测试性的能力，有能力推动你的开发团队做好代码设计，走向单元测试。</p><h2>从一个需求说起</h2><p>有一天，老板给FoodCome订餐系统提了一个需求，希望用户在页面上可以修改自己的邮箱地址。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/7a/45/7aa7d008a286da48897364228a21cc45.jpg?wh=1920x1284\" alt=\"图片\"></p><p>这个修改行为后端实现起来似乎很简单，修改邮箱就是更新数据库里的用户信息。但在FoodCome平台上有一个业务逻辑，邮箱地址域名如果是@foodcome.com，这说明是平台上注册的餐馆商家，否则，就是普通顾客。</p><p>这样，我们就需要在用户修改邮箱的时候，加入一个判断，如果邮箱地址的修改是从普通域到@FoodCome域，商家在平台里的数量就+1，否则，就-1。</p><p>这个需求看起来很简单，是吧。但是我要告诉你，即使这么简单的逻辑，也需要做好设计，否则，写出来的代码可能完全无法单元测试，不信就来看看吧。</p><h2>版本V1</h2><p>既然老板提了需求，自然要赶紧完成。开发人员小胜同学立刻在下班前完成了一个版本，新建了一个User类，在User类里有一个changeEmail的方法，负责修改邮箱地址。这是第一版代码<strong>User.java</strong>下的内容：</p><pre><code class=\"language-java\">public class User\n{\n    public int UserId { get; private set; }\n    public string Email { get; private set; }\n    public UserType Type { get; private set; }\n\n    public void ChangeEmail(int userId, string newEmail)\n    {\n        //查询出来用户的ID，email和type\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;object[]&nbsp;data&nbsp;=&nbsp;Database.GetUserById(userId);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n        UserId = userId;\n        Email = (string)data[1];\n        Type = (UserType)data[2];\n        //如果和修改前的email相同，直接返回\n        if (Email == newEmail)\n            return;\n        //从数据库里获得商家的数量\n        int numberOfRestaurtants = Database.GetRestaurant();\n        //判断用户要修改的类型是商家还是顾客\n        string emailDomain = newEmail.Split('@')[1];\n        bool isEmailRestaurant = emailDomain == \"foodcome.com\";\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UserType&nbsp;newType&nbsp;=&nbsp;isEmailRestaurant&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;?&nbsp;UserType.Restaurant&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;UserType.Customer;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n        //如果是修改成为商家，那么商家数量+1，如果是修改为顾客，商家数量-1  \n        if (Type != newType)\n        {\n            int delta = newType == UserType.Restaurant ? 1 : -1;\n            int newNumber = numberOfRestaurtants + delta;\n            Database.SaveRestaurant(newNumber);   \n            MessageBus.SendEmailChangedMessage(UserId, newEmail);                     \n        }\n        //提交用户信息修改，入库\n        Database.SaveUser(this);                                  \n           \n    }\n}\npublic enum UserType\n{\n    Restaurant = 1,\n    Employee = 2\n}\n</code></pre><p>上面的代码并不复杂，主要功能是判断邮件地址是否为FoodCome，然后更新数据库，发送通知到消息总线上。</p><p>那它的可测试性怎么样呢？</p><p>我们引入一个代码复杂度四象限法则来完成这个评估，它的原理是，用以下2个维度来评估代码：</p><p>1.领域复杂度，代码的领域复杂度越高，里面的业务逻辑就越多。<br>\n2.对外依赖度，依赖度越高，说明代码和外部的Class交互点多。</p><p>每个维度都有高低，2个维度的高低组合在一起就成了四个象限。</p><p>1.业务复杂、依赖又少，是业务逻辑集中的代码。比如业务算法。<br>\n2.业务简单、依赖又少，是一些简单代码，比如构造函数，数据对象等。<br>\n3.业务复杂、依赖多，是复杂度高的代码，测试起来也会比较困难。<br>\n4.业务简单、依赖多，是管理依赖的代码。比如消息总线。</p><p><img src=\"https://static001.geekbang.org/resource/image/b8/00/b828cec1b566e1df1ff6640fe5f32900.jpg?wh=1920x1547\" alt=\"图片\"></p><p>我们要把开发的代码放到四个象限里，每个象限对应不同的测试策略。那小胜同学的User Class，应该放到哪个象限呢？</p><p>User Class里包含了changeEmail的业务逻辑，就是根据Email的地址决定如何变更Restaurant的数量。同时，User Class里还有2个外部依赖：Database和MessageBus，这个依赖还是隐式的，没有声明在成员变量里，你得看代码才能找到2个外部依赖是怎么使用的。</p><p>显然，User class属于领域和依赖混合的代码，应该归属于<strong>复杂代码</strong>象限。</p><p>那么在这种情况下，怎么测试User Class呢？</p><p>为了调用changeEmail方法，你可能会想到Mock Database和MessageBus两个对象，但难点是，根本没有办法把Mock对象传递changeEmail方法里去。</p><p>所以，现在我们是没有办法单元测试User Class的，它的可测试性为0。</p><p>因此，小胜同学需要重构代码，把<strong>复杂代码</strong>象限的代码分解到<strong>领域代码</strong>象限和<strong>依赖代码</strong>象限。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/16/0257c4ff30f7be0e4fb0e3f14aa82e16.jpg?wh=1920x1571\" alt=\"图片\"></p><h2>版本V2</h2><p>下面来到第二个版本，小胜将业务逻辑和与外部依赖的交互分离，创建了一个新的UserController Class来专注管理外部依赖。</p><p><strong>UserController.java</strong>的代码如下：</p><pre><code class=\"language-java\">public class UserController\n{\n    //在controller里声明两个外部依赖\n    private readonly Database _database = new Database();\n    private readonly MessageBus _messageBus = new MessageBus();\n    //changeEmail方法里调用依赖\n    public void ChangeEmail(int userId, string newEmail)\n    {\n        object[] data = _database.GetUserById(userId);\n        string email = (string)data[1];\n        UserType type = (UserType)data[2];\n        var user = new User(userId, email, type);\n\n        int numberOfRestaurants = _database.GetRestauruant();\n        string companyDomainName = \"foodcome.com\";\n        //调用User的changeEmail，返回变化的商家数量。\n        int newNumberOfRestaurants = user.ChangeEmail(\n            newEmail, companyDomainName, numberOfRestaurants);\n        //调用依赖，变更数据库和发布消息\n        if(newNumberOfRestaurants!=numberOfRestaurants){\n          _database.SaveCompany(newNumberOfRestaurants);\n          _database.SaveUser(user);\n          _messageBus.SendEmailChangedMessage(userId, newEmail);\n        }\n    }\n}\n</code></pre><p>同时我们要修改User Class，它不负责任何依赖，只关注changeEmail的业务逻辑，根据输入参数，算出来新的商家的数量。</p><p><strong>User.java</strong>的代码如下：</p><pre><code class=\"language-java\">public class User\n{\n    public int ChangeEmail(string newEmail,\n    string companyDomainName, int numberOfRestaurants)\n   {\n    if (Email == newEmail)\n        return numberOfRestaurants;\n\n    string emailDomain = newEmail.Split('@')[1];\n    bool isEmailCorporate = emailDomain == companyDomainName;\n    UserType newType = isEmailCorporate\n        ? UserType.Restaurant\n        : UserType.Customer;\n    if (Type != newType)\n    {\n        int delta = newType == UserType.Employee ? 1 : -1;\n        int newNumber = numberOfRestaurants + delta;\n        numberOfRestaurants = newNumber;\n    }\n    return numberOfRestaurants;\n&nbsp;&nbsp;}\n}\n</code></pre><p>我们发现，相比版本1 ，版本2有了进步，多了UserController Class来专门负责管理外部交互，隐式依赖的Database和MessageBus变成了显式成员变量。User class专门负责业务逻辑，判断为餐馆还是客户。</p><p>我们再用复杂度四象限来观察现在的代码，它的分布情况变为下图这样。</p><p>可以看到，复杂代码消失了，原先一个User.java文件里的逻辑被分散到了两个文件里，一个是UserController.java，它专注于管理依赖，位于依赖代码象限；还有修改后的User.java，专注于业务逻辑实现，位于领域代码象限。</p><p><img src=\"https://static001.geekbang.org/resource/image/18/86/181afae377e2d60f74916703810a8386.jpg?wh=1920x1526\" alt=\"图片\"></p><p>现在可以测试了！</p><p>User Class只有一个changeEmail的方法，方法里只是对邮件地址做判断，并计算新的餐馆数量，将新值返回，不会涉及到操作数据库和消息总线。User Class可以测试了，因为不需要做任何Mock，我叫它<strong>简单测试</strong>。</p><p>而2个外部依赖Database和MessageBus都被显式地封装在UserController Class里，我们可以通过构造函数来传递Mock对象，UserController也是可以测试的，我叫它<strong>Mock测试</strong>。</p><p>但是，版本2依旧不够清爽，还存在两个问题。</p><ol>\n<li>\n<p>UserController里实例化了User class，这包含了业务逻辑（如何构造一个User对象），而我们希望UserController更为纯粹，专注依赖管理；</p>\n</li>\n<li>\n<p>User class和Restaurant数据也存在耦合，通过User&nbsp;Class的ChangeEmail函数返回了Restaurant的newNumofRestaurant，这和User的行为模型毫无关系。</p>\n</li>\n</ol><p>好，撸起袖子加油干，我们来完成第三个版本，一次性解决上面提到的问题。</p><h2>版本V3</h2><p>首先，使用UserFactory模式封装User实例化逻辑，如下：</p><pre><code class=\"language-java\">public class UserFactory\n{\n    public static User Create(object[] data)\n    {\n        Precondition.Requires(data.Length &gt;= 3);\n        int id = (int)data[0];\n        string email = (string)data[1];\n        UserType type = (UserType)data[2];\n\n        return new User(id, email, type);\n    }\n}\n</code></pre><p>可以看到UserFactory也是可以简单测试的✌️！这是阶段性胜利，可喜可贺。</p><p>然后，我们常见一个Restaurant Class用来封装关于Restaurant的业务逻辑，如下：</p><pre><code class=\"language-java\">public class Restaurant\n{\n    public string DomainName { get; private set; }\n    public int NumberOfRestaurant { get; private set; }\n\n    public void ChangeNumberOfRestaurants(int delta)\n    {\n        Precondition.Requires(NumberOfRestaurants + delta &gt;= 0);\n        NumberOfRestaurants += delta;\n    }\n    public bool IsEmailCorporate(string email)\n    {\n        string emailDomain = email.Split('@')[1];\n        return emailDomain == DomainName;\n    }\n}\n</code></pre><p>Restaurant Class也是可以简单测试的！</p><p>参考UserFactory的思路，再做一个RestaurantFactory，专门做Restaurant对象的构建，此处省略代码，实现起来其实并不复杂，有兴趣你可以自己写写看。</p><p>而UserController更关注与外部依赖的管理和交互，如下：</p><pre><code class=\"language-java\">public class UserController\n{\n    private readonly Database _database = new Database();\n    private readonly MessageBus _messageBus = new MessageBus();\n\n    public void ChangeEmail(int userId, string newEmail)\n    {\n        object[] userData = _database.GetUserById(userId);\n        User user = UserFactory.Create(userData);\n\n        object[] restaurantData = _database.GetRestaurant();\n        Restaurant restaurant = RestaurantFactory.Create(restaurantData);\n        user.ChangeEmail(newEmail, restaurant);\n        \n        _database.SaveUser(user);\n        //如果restaurant数量有变化，就写数据库，发送通知信息\n        if(restaurant.numberChanged()){\n          _database.SaveRestaurant(restaurant);\n          _messageBus.SendEmailChangedMessage(userId, newEmail);\n         }\n    }\n}\n</code></pre><p>UserController Class里没有业务逻辑，只有Database和MessageBus的管理和操作。在单元测试时，只需要用Mock替代，就OK了！UserController是可以轻松Mock测试的！</p><p>经过前面的操作，User Class变化如下：</p><pre><code class=\"language-java\">public class User\n{\n    public int UserId { get; private set; }\n    public string Email { get; private set; }\n    public UserType Type { get; private set; }\n\n    public void ChangeEmail(string newEmail, Restaurant restaurant)\n    {\n        if (Email == newEmail)\n            return;\n\n        UserType newType = company.IsEmailCorporate(newEmail)\n            ? UserType.Employee\n            : UserType.Customer;\n\n        if (Type != newType)\n        {\n            int delta = newType == UserType.Employee ? 1 : -1;\n            restaurant.ChangeNumberOfRestaurants(delta);\n        }\n    }\n}\n</code></pre><p>Oh Yeah, User Class是可以简单测试的！</p><p>现在我们再来看一下V3版本代码在复杂度四象限的位置，变成了这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/9b/8e678e078dd95b5ae0e2aa95f2ede29b.jpg?wh=1920x1521\" alt=\"图片\"></p><p>经过3个版本后，我们把代码重构完，复杂代码完全被消解掉了，分解到了其他3个象限。</p><p>现在，我们就可以为不同象限的代码，制定好单元测试策略和优先级了。我特意准备了一张总结表放在了后面。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/4e/8e17702d4ab6f9af4555cbc88837634e.jpg?wh=1920x889\" alt=\"图片\"></p><p>有了这张表，你就可以使用<a href=\"https://time.geekbang.org/column/article/505695\">上一讲</a>提到的测试方法来做业务逻辑测试和Mock测试了！</p><h2>小结</h2><p>向着提高代码可测试性的目标，复杂代码经过一步步的重构，被分解到了依赖管理和业务逻辑两个领域，如果也算上数据库代码的话，正好和软件架构的MVC模式一致。这不是凑巧，这恰恰说明好的代码结构等同于单元测试高覆盖率。如果你的项目，单元测试做不下去，测试覆盖率提不上来，那应该重构代码了。</p><p>反之也一样，如果开发人员声称做了非常好的设计，那么我们通过单元测试，就应该能直观感受到这个好的设计。期待今天这讲内容成为你开启开发领域大门的一把钥匙。</p><p><img src=\"https://static001.geekbang.org/resource/image/35/6b/35ddf28fc9c17585e7ef43c40f574f6b.jpg?wh=1920x999\" alt=\"图片\"></p><p>另外，结合今天的三个版本的代码例子，你也应该发现了，单元测试并不是眉毛胡子一把抓。</p><p>首先，你应该关注业务逻辑能否在单元测试里充分测试，这是跟我们自动化测试高度关联的地方。而Mock的目标，则是辅助业务逻辑能够更早更快地测试。以后，当开发人员骄傲地跟你说：“代码行覆盖率达到80%”的时候，你应该和他一起检查下，他到底测了什么。</p><p>单元测试看起来已经是个完美的测试方案了，那在单元测试层面还有什么做不到的事么，下一讲我们就来揭晓答案。</p><h2>思考题</h2><p>制定单元测试覆盖率100%的目标有价值么？如果让你制定目标，你会怎么做？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这讲内容分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"10｜单元测试（一）：原来测试可以这么高效","id":505695},"right":{"article_title":"12｜集成测试（一）：一条Happy Path扫天下","id":507443}}},{"article_id":507443,"article_title":"12｜集成测试（一）：一条Happy Path扫天下","article_content":"<p>你好，我是柳胜。</p><p>上一讲，我们学习了单元测试，在验证业务逻辑方面，它的优势在于速度又快，阶段又早。既然单元测试看起来是一个完美的自动化测试方案，那为什么还需要集成测试呢？</p><p>我在<a href=\"https://time.geekbang.org/column/article/497405\">第二讲</a>的3KU原则说过，测试需求首先要找ROI最高的截面来验证。在金字塔模型里，ROI最高的就是单元测试，如果无法实现，才回退到ROI第二高的截面，一直到ROI最低的端到端测试。</p><p>那集成测试存在的价值，一定是做得了单元测试层面做不到的事，否则，集成测试这个概念就没必要存在。那这些事具体有哪些呢？你要是能找到这些事，就找到了集成测试省力又见效的窍门。今天咱们就一起寻找这个答案。</p><h2>集成测试和单元测试</h2><p>上一讲我们学过了代码四象限法则，产品的代码按照业务相关性和依赖程度，可以划分到下面四个象限里。</p><p><img src=\"https://static001.geekbang.org/resource/image/53/cb/538461c119ff8ac736750f27ea60a7cb.jpg?wh=1920x1501\" alt=\"图片\"></p><p>那集成测试和单元测试分别应该归到第几象限呢？</p><p>集成测试，顾名思义，是验证本服务代码和其他进程的服务能不能一起配合工作。在上面的四象限里，集成测试的活动领域就在“依赖代码”象限，而单元测试的活动领域是在“领域代码”象限。</p><p>我再用图解的方式划分一下地盘，你会看得更清楚。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/45/3f6fc585e1af8bc0a110c83776781045.jpg?wh=1920x1203\" alt=\"图片\"></p><p>这张图里的信息量很大，展示了单元测试和集成测试的各自战场，我来跟你细说一下。</p><p>单元测试掌管领域代码的测试，这些领域代码只是负责数据计算，并不会触及外部依赖。像上一讲的changeEmail方法，只是计算出一个新的餐馆数目，单元测试只需要验证这个计算逻辑是否正确就好了。</p><!-- [[[read_end]]] --><p>那什么是单元测试测不了的呢？ 那就是依赖代码。在FoodCome的代码设计里，这些外部的依赖管理交给一个独立的Controller Class去做，它负责读写数据库、发送消息等等。这块就是集成测试的领域。</p><p>看到这里，你脑袋里可能会冒出这样一个问题：不对呀！单元测试也可以测试外部依赖，我们在前面讲过可以Mock外部依赖，如果我把Database、MessageBus都Mock了，那不就也可以做单元测试了么？</p><p>你能想到这一层，说明你已经关注概念背后真正的事情了。是的，如果所有的外部服务都Mock了，集成测试就变成了单元测试，往另外一个方向，如果所有的外部服务都是真实的，集成测试又变成了端到端的测试。<strong>集成测试就是处在单元测试和端到端测试中间的一个状态。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/e0/cf/e06c78d1995fbba0af903da9001302cf.jpg?wh=1920x733\" alt=\"图片\"></p><p>在这里，我们要关注<strong>Mock和Real的优劣势，集成测试怎么能做得更聪明一些，用最少的工作量，获得最大的测试效果</strong>。下面我们就展开来说一说。</p><h2>集成测试测什么？</h2><p>相比单元测试，集成测试有2个特点。</p><p>第一，集成测试运行速度慢，这个时间主要花在2个地方，第一个是准备集成测试环境的时间，你要先把依赖的外部服务启动起来，让环境处在一个健康状态；第二个是运行集成测试的时间，因为集成测试不像单元测试是进程内工作，它是跨进程通讯，除了计算时间，还要加上网络通讯时间等等。</p><p>第二，执行集成测试，要运行的代码量比单元测试要多。因为它走过的路径更长，从网络请求，到处理请求，再到网络返回结果，中间需要经历过n个代码单元，还有框架代码，库代码等等。</p><p>这两个特征告诉我们，集成测试是有比较大的成本的，并且它测试的代码逻辑和单元测试是有重叠的。</p><p>本着追求整体最大ROI效益的目标，集成测试和单元测试需要协同作战，保持一个平衡，这个平衡的原则是：</p><p>1.在单元测试阶段验证尽可能多的业务逻辑，这样能让集成测试关注在外部依赖上。</p><p>2.集成测试至少覆盖一条长路径案例，叫“Happy Path”。</p><h3>怎么挑选Happy Path</h3><p>Happy Path是指一条正常业务的测试案例，走尽可能多的外部依赖服务。比如，一条案例，同时走了Database和MessageBus。</p><p>针对<a href=\"https://time.geekbang.org/column/article/506638\">上一讲</a>提到的用户修改邮箱功能，我们有几个案例：</p><p>1.修改邮箱名从 a@foodcome.com到b@foodcome.com<br>\n2.修改邮箱名从 a@example.com到a@foodcome.com<br>\n3.修改邮箱名从 a@example.com到b@example.com</p><p>哪个案例是Happy Path呢？再回头看一下代码：</p><pre><code class=\"language-java\">public class UserController\n{\n   .............\n    public void ChangeEmail(int userId, string newEmail)\n    {\n        .....................\n        user.ChangeEmail(newEmail, restaurant);\n        _database.SaveUser(user);\n        //如果restaurant数量有变化，就写数据库，发送通知信息\n        if(restaurant.numberChanged()){\n          _database.SaveRestaurant(restaurant);\n          _messageBus.SendEmailChangedMessage(userId, newEmail);\n         }\n    }\n}\n</code></pre><p>我们不难发现案例2符合Happy Path，因为它触发了多次与2个外部依赖的交互，更新了Databse的用户信息和餐馆信息，还触发了消息总线发送一条通知出去。</p><p>你可能还想到一个疑问，如果我们找不到一个能触发全部外部依赖交互点的Happy Path，那怎么办？很简单，那就再加一条Happy Path。</p><h2>集成测试用Mock还是Real测试？</h2><p>集成测试领域一个有争议的话题，就是外部依赖是用Mock还是用真实的实例。在前面我们讲单元测试是“孤立型”还是“社交型”的时候，提到了Mock和Real两种方法都有优劣，都有适用的场景（可以回看<a href=\"https://time.geekbang.org/column/article/505695\">第十讲</a>）。</p><p>今天我们详细说说，选择Mock还是Real的方法。</p><p>首先要看外部依赖的特征，我把它划分成2种类型。</p><p>1.完全可控依赖<br>\n2.不可控依赖</p><p>什么是完全可控依赖呢？ 这个外部的服务被你的应用独享，你也能够控制它的开发和运维，那这个服务就是完全可控依赖的。一个典型的例子，就是数据库，在微服务模式下，每一个服务独享一个自己的数据库Schema。</p><p>那什么又是不可控依赖？与可控依赖相反，这个外部的服务不止你的应用调用，大家都得遵守一个协议或规范，和这个公共的外部服务交互。典型的例子，就是外部的支付系统，SMTP邮件通知服务等等。</p><p>与这两种类型相对应的Mock策略如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/17/07/1766011df1c922a898004190c0681a07.jpg?wh=1920x654\" alt=\"图片\"></p><p>为什么是这样的？ 完全可控依赖的服务，虽然是在你的应用之外的一个进程，但你可以把跟它的交互当作是你开发的内部实现。你可以升级数据库版本、修改表格结构、增加数据库函数，只要跟着应用的代码一起修改即可。</p><p>这种情况下，你可以把这个数据库和你的应用当作一个整体，没必要花力气做Mock，如果你脑子一抽做了Mock，就还要维护Mock的变化，恭喜进坑。</p><p>而不可控依赖服务就不一样了，它是公共的，你控制不了它，而且你跟它的交互还要遵守一个规范的契约。在这种情况下，做Mock就划算了，原因有二：<strong>第一，基于契约的Mock的维护成本比较低；第二，使用Mock可以保证你的应用持续重构，向后兼容</strong>。</p><p>分析到这，我们就能梳理出FoodCome的Mock策略了。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/98/26170c5b24f458f2ee3706f1a15a5f98.jpg?wh=1920x908\" alt=\"图片\"></p><h2>集成测试的实现</h2><p>找出了Happy Path，也定了Mock策略后，就可以动手写代码了。</p><p>根据2号案例，我们来创建一个测试方法，方法名为change_email_from_example_to_foodcome：</p><pre><code class=\"language-c#\">[Fact]\npublic void Changing_email_from_example_to_foodcome()\n{\n    // Arrange\n    var db = new Database(ConnectionString);                          \n    User user = CreateUser(                                           \n        \"a@example.com\", UserType.customer, db);                                              \n    var messageBusMock = new Mock&lt;IMessageBus&gt;();                     \n    var sut = new UserController(db, messageBusMock.Object);\n    // 调用changeEmail方法\n    string result = sut.ChangeEmail(user.UserId, \"b@foodcome.com\");\n    // 校验返回\n    Assert.Equal(\"OK\", result);\n    // 校验数据库里字段\n    object[] userData = db.GetUserById(user.UserId);                 \n    User userFromDb = UserFactory.Create(userData);                  \n    Assert.Equal(\"b@foodcome.com\", userFromDb.Email);                 \n    Assert.Equal(UserType.Restaurant, userFromDb.Type);                \n        messageBusMock.Verify(                                           \n        x =&gt; x.SendEmailChangedMessage(                              \n            user.UserId, \"b@foodcome.com\"),                           \n        Times.Once);                                                 \n}\n</code></pre><p>上面的代码完成了以下步骤，我特意分点列出来，方便你看清楚每一步。</p><p>1.创建真实的数据库连接对象；<br>\n2.创建MessageBus的Mock对象；<br>\n3.把2个依赖注入到被测UserController class里，调用changeEmail方法；<br>\n4.检验数据库里的User状态；<br>\n5.检验Mock的MessageBus里的消息。</p><h2>小结</h2><p>今天我们学习了和外部服务的集成测试的方法，在动手之前，我们要想明白测什么，用什么测，Mock还是Real。</p><p>测什么，怎么测，这就是集成测试方案要回答的问题，而且，这个方案的制定遵循3KU原则，也就是尽量不做重复的事，把精力和时间花在有价值的地方。</p><p>单元测试需要做好业务逻辑的验证，集成测试主要是测试与外部依赖的集成，集成又有2种策略，采用Mock还是Real真实的依赖，应该遵循<strong>能Real就Real的原则，不能Real的再采用Mock</strong>，如果一股脑Mock所有依赖，你会发现集成测试没测到什么有用的逻辑，都在Mock上，而真正集成时还是会遇到问题。</p><p>在集成测试案例的设计上，我提出了<strong>Happy Path</strong>，让你能用最少的工作量做最有效果的事情，这对于集成测试刚起步的项目来说，十分关键。在Mock策略上，也是遵循同样的原则，尽量把开发和维护Mock的工作量花在最有价值的外部依赖上。</p><h2>思考题</h2><p>在实际工作中，你有多个测试案例，怎么找出那条Happy Path？除了看代码，还有别的方法么？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这讲内容分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"11｜单元测试（二）：四象限法让你的单测火力全开","id":506638},"right":{"article_title":"13｜集成测试（二）：携手开发，集测省力又省心","id":508574}}},{"article_id":508574,"article_title":"13｜集成测试（二）：携手开发，集测省力又省心","article_content":"<p>你好，我是柳胜。</p><p>专栏里我一直强调这样一个观点，<strong>以全局ROI最高为导向，把各个测试类型综合考虑，而非割裂、独立地分析某一层的测试</strong>。不谋全局者，不足谋一域，测试领域如此，开发和测试领域协同也一样。比如前面咱们学过的单元测试，把单元测试做好，实际就能推动开发代码的结构优化。</p><p>集成测试也是一样，做好集成测试也需要开发的支持。在上一讲里，我提到了集成测试轻量化的想法：Mock服务和Happy Path。你学完了后，可能已经跃跃欲试，盘算着自己的项目该怎么做Mock，去哪里寻找那条Happy Path了。</p><p>但理论推演畅通无阻，现实挑战却障碍重重。很可能现实里你面临两难：做Mock成本高，需要修改很多开发代码，调用链条又长又复杂，根本理不出那条Happy Path。面临这样的困难，要么只能放弃ROI不高的集成测试，要么硬着头皮去设计和执行一些测试案例，而且这些案例你也不确定是否有效。</p><p>有没有第三种方法呢？有，这第三种方法不仅能让集成测试保持轻量和高效，而且还能让测试进入一个Bug越来越少的正反馈循环。但需要你主动介入代码世界，和开发人员一起推动集成测试的可测试性。后面的知识点不少，但只要你跟住我的思路，愿意跟着我思考，一定不虚此行。</p><!-- [[[read_end]]] --><h2>接口与Mock</h2><p>先从开发Mock说起，这是集成测试里一项必不可少的工作内容。怎么能又快又简单地开发Mock呢？带着这个问题，我们来看一下Mock的原理。</p><p>在面向对象语言里，Interface接口是个常被提及的概念。一个Interface接口定义了契约，而实现细节由继承类去完成。在代码里你经常会看到Interface-Class成对出现的情况：</p><pre><code class=\"language-c#\">public interface IMessageBus\npublic class MessageBus : IMessageBus\n\npublic interface IUserRepository\npublic class UserRepository : IUserRepository\n</code></pre><p>从设计的角度这叫做解耦，目的是<strong>分离定义和实现</strong>，有一个Interface，可以对应多个实现Class。那从集成测试的角度来看，Mock和Real不就是一个Interface的两个实现么？</p><p>这么说你可能还没理解，做个对比你就更清晰了，这个场景就像图灵实验。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/42/bb9b4954748179217baa116fdcyy4a42.jpg?wh=1920x1260\" alt=\"图片\"></p><p>在图灵实验里，人类C隔着一堵墙，分别跟B和A交流。当他分辨不出来哪个是人，哪个是电脑的时候，我们就说，电脑A已经具备了和人类B一样的人工智能了。</p><p>在集成测试里，就变成了下图这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/0c/fbyy7bbe3e8e3e61a2c54e3bb59e150c.jpg?wh=1920x1272\" alt=\"图片\"></p><p>C通过Interface定义的规范和A、B打交道，而Mock和Real都是对Interface MessageBus的实现。所以，C不能分辨出哪个是A，哪个是B，因为它们都遵循Interface MessageBus。</p><p>在Java语言里，MessageBus的Interface定义如下：</p><pre><code class=\"language-java\">public interface IMessageBus{\n    //发送消息\n\tpublic  void sendEmailChangeMessage(Srting userID,String changedEmail);\n\t//接收消息\n    public  Message fetchMessageById(Integer msgId);\n\t............................\n}\n</code></pre><p>有了Interface后，你可以借助工具生成Mock实现。</p><p>下面的例子是Mockito框架，直接通过Interface MessageBus生成Mock messageBus：</p><pre><code class=\"language-java\">import org.mockito.Mock;\nimport org.mockito.MockitoAnnotations;\npublic class FoodComeTest{\n  @Mock\n  private IMessageBus mockMessageBus;\n  public class InterfaceMethodMockingUsingMockAnnotationTest {\n    ........\n     user.changeEmail(userID,changedEmail);\n     mockMessageBus.sendEmailChangeMessage(userID,changedEmail);\n     ........\n  }\n}\n</code></pre><p>通过这个例子，我们看得出Interface的存在对于集成测试十分有用，有很多Mock框架支持通过Inteface生成Mock Class，这就大大降低了Mock的成本。</p><p>现在，我们得出了一条<strong>可测试性实践原则</strong>：开发人员在实现对外部服务访问的时候，应该要设立并通过Interface来完成访问，哪怕是一个Interface只有一个实现Class，也需要有Interface的存在。</p><h2>高效Happy Path</h2><p>很好，现在Mock的成本降低的原则找到了，咱们再接再厉，再分析下集成测试的另外一项重要工作。</p><p>还记得上一讲，我们提到过的Happy Path么？它是一条代码执行路径，如果你能找到有效的Happy Path，集成测试可以做得简单而高效。</p><p>但是，有没有Happy Path，你又能不能找到它，这很大程度取决于代码的结构。有的项目代码写得混乱，调用链复杂，甚至还有回路死循环，这样的代码你想集成测试，恐怕是走入迷宫难回头。</p><p>因此，从集成测试的可测试性角度，代码要满足三条要求：</p><p>1.<strong>结构：</strong>领域逻辑和依赖管理分离，清楚可见；<br>\n2.<strong>层次：</strong>调用链简洁而短，减少无效代码；<br>\n3.<strong>调用关系：</strong>调用链单向，避免回路的产生。</p><p>这三条要求具体怎么实现，我们一一来看。</p><h3>领域逻辑和依赖管理的分离</h3><p>之前讲单元测试时，我们已经学习了通过代码四象限法则，把代码分解到各个功能象限里去，我们以FoodCome为例做了练习，分解后的结果就是，我们建立Controller专门管理依赖；建立User类、Restaurant类，专门做业务逻辑相关的操作。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/9b/8e678e078dd95b5ae0e2aa95f2ede29b.jpg?wh=1920x1521\" alt=\"图片\"></p><p>不过明确了分工思路还不够，今天我们就展开说说代码层面的实操细节。</p><p>领域逻辑和依赖管理的边界不能只存在开发人员脑子里，否则，项目开发了一段时间后，这个边界就模糊了，甚至埋下项目未来走向腐化的隐患。要让这个边界持续可维护，它应该是可见的。</p><p>怎么让这个边界可见呢？最直观的做法就是，把领域逻辑和依赖管理的代码放在一个项目的不同的package下。</p><p>FoodCome划分了如下几个package：</p><ul>\n<li>Controller: 用来存放Controller类，还有外部依赖的Interface；</li>\n<li>Service:  用来存放Service类，Controller类调用Service，在Service里实现领域逻辑计算；</li>\n<li>Dao: Service类调用Dao类，实现对数据的持久化。</li>\n</ul><p>这样，通过不同package组织的代码，开发人员在后面修改的时候，会自然遵循这些规律，把不同功能的代码添加到相应的package里去。</p><h3>调用链条短而简洁</h3><p>再结合例子说说调用链的问题，在FoodCome应用里，调用关系是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/31/63/318d6700e73d7cbe77eff3c36757b363.jpg?wh=1920x768\" alt=\"图片\"></p><p>这个调用链非常简单有效，从Controller处理请求，到最后数据入库，就走了三个截面，分别是：Controller层，Service层和DAO层。</p><p>在很多项目实践中，调用链比这个复杂，有的刚开始简单，后来项目规模变大后，开发人员不断地往里“加货”。比如，有的开发人员特别喜欢加抽象层，加Interface，而且还会引用一堆软件的理论，说得有理有据，认为Interface可以增加将来的可扩展性。</p><p>这些理论没错，但这里我想提醒你，添加抽象层这件事需要保持一个度。在极限编程里还有一个YAGNI理论，YAGNI是英文“You aren’t gonna need it” 的缩写，YAGNI 原则指出，程序员应该在面临确凿的需求时，才要实现相应功能。</p><h3>调用链条单向，避免回路产生</h3><p>单向指的是，我们从Controler调Service，从Service调DAO，这是调用链的方向，而不能反着来。</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/ff/b3d8ce1c50ce1657078e363b0e1777ff.jpg?wh=1920x709\" alt=\"图片\"></p><p>回路指的是，存在两个或以上的Class，它们在调用时互相依赖。</p><p>比方说我们开发FoodCome，遇到这样的问题：在结账时需要检查详细列表，而产生详细列表时又需要先结账，这就成了一个死循环。</p><p>示例代码如下，实际调用路径比这个绕了好几道圈。</p><pre><code class=\"language-java\">public class CheckOutService\n{\n&nbsp; &nbsp; public void CheckOut(int orderId)\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; var service = new ReportGenerationService();\n&nbsp; &nbsp; &nbsp; &nbsp; service.GenerateReport(orderId, this);\n&nbsp; &nbsp; &nbsp; &nbsp; /* other code */\n&nbsp; &nbsp; }\n}\n\npublic class ReportGenerationService\n{\n&nbsp; &nbsp; public void GenerateReport(\n&nbsp; &nbsp; &nbsp; &nbsp; int orderId,\n&nbsp; &nbsp; &nbsp; &nbsp; CheckOutService checkOutService)\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; /* calls checkOutService when generation is completed */\n&nbsp; &nbsp; }\n}\n</code></pre><p>解决这样的问题，我们就需要理清工作流，从业务逻辑上彻底解除这样的回路依赖关系。</p><h2>集成测试神器</h2><p>前面，我们从查看代码的角度来寻找Happy Path，对代码的结构、层次和调用关系提出了一系列要求。</p><p>现在，我们再换个思路，能不能从<strong>结果</strong>上观测Happy Path呢？也就是说，我们有没有办法直观地看到这个调用链条呢？</p><h3>Happy Path探测神器</h3><p>你可以在头脑中想象这样一个场景：客户端向系统发起一个请求，就相当于客户要求物流公司把一个货物送到目的地，物流公司先把货物运送到A站点，然后A站点再运到B站点，以此类推，经过多个站点后，最后到达目的地仓库。</p><p>这个场景里，物流公司就是软件系统，货物经过的A站点、B站点就是一个个软件服务，而货物走过的ABCD路径，就是服务的调用链。</p><p>要想追溯物流的派送路径，那就得问物流公司的负责人，要想知道服务的调用链，我们就得问系统的管理员。在现实中，这个系统的管理员往往是生产环境的运维人员，它们可以<strong>借用各种运维工具，来观测和追踪生产环境的负载</strong>。</p><p>能做链路观测和追踪的运维工具有很多，有Zipkin、Dapper、Skywalking等。这里我们以SkyWalking为例，来看一下分布式追踪是怎么做的。</p><p>SkyWalking是Apache基金项目下的一个开源项目，用来做可观测性分析，提供分布式跟踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。</p><p>在Skywalking <a href=\"https://skywalking.apache.org\">官方网站</a>，你可以看到它的架构图：</p><p><img src=\"https://static001.geekbang.org/resource/image/47/7a/478582b42e61489f844e47b716c2c27a.jpg?wh=1920x949\" alt=\"图片\"></p><p>安装配置SkyWalking，你参照<a href=\"https://skywalking.apache.org/docs/skywalking-showcase/latest/readme\">这个文档</a>就能自学。这里我重点说说Skywalking是怎么工作的呢？</p><p>Skywalking需要在每一个服务上启用Skywalking的Agent，也就是图的左上方Tracing部分。</p><p>每当服务处理Inbound和Outbound请求时，Agent就把这些信息发送到SkyWalking的OAP服务器上去，OAP服务将这些信息经过链接、聚合、分析后，用户就能通过左下方的UI来查看这些调用信息组织形成的调用链条了。</p><p>SkyWalking UI上的功能很多，寻找Happy Path，我们可以使用下面这2个功能。</p><p>先看Topology功能下的服务拓扑图，通过拓扑图，可以看到所有的服务和它们的调用关系。</p><p><img src=\"https://static001.geekbang.org/resource/image/ba/55/bacb1aec464202821141yyaaa18d0355.jpg?wh=1920x845\" alt=\"图片\"></p><p>如果我们想观测一个特定的请求，看它走过了哪些路径，可以看Skywalking UI上的Trace Tree。</p><p><img src=\"https://static001.geekbang.org/resource/image/ca/39/cac202a1035e73554d8b0588968eff39.jpg?wh=1920x830\" alt=\"图片\"></p><p>Trace图里，左边导航栏里是一个个的请求，右边的Tree视图里展现了每个请求走过的调用链。这张页面截图主要是让你大概了解。清晰的调用关系，你可以参看下面这张逻辑图。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/b4/65f226413ec1381b6564e8784506c7b4.png?wh=1872x706\" alt=\"图片\"></p><p>有了调用Tree后，我们就可以运行两个测试案例A和B，使用白盒测试的路径覆盖法则，查看哪个测试案例能覆盖更多的路径，哪个就是Happy Path了！</p><p>如果你对白盒测试的路径覆盖方法不太熟悉，可以通过<a href=\"https://www.jianshu.com/p/8814362ea125\">这个资料</a>，了解一下白盒测试的基本理论。</p><h3>Docker构建随时可测的数据库</h3><p>上一讲，我们提到直接用真实的数据库实例来做集成测试。不过这个在实践中，你可能会遇到一系列问题，这个真实的数据库实例是生产环境么？如果不能用生产环境数据库，自己搭建数据库怎么能够保证真实模拟的测试的效果？</p><p>这些问题解决不了，集成测试的成本也会很高，甚至导致在实践中，集成测试无法开展。</p><p>应该怎么办呢？我先抛出开展集成测试的三个关键目标。</p><p>第一，集成测试的环境应该是独立的，和生产环境分离；</p><p>第二，集成测试的数据库上的数据库对象应该和生产环境保持同步；</p><p>第三，集成测试数据库实例的生成应该是快速的。</p><p>要实现第一条和第三条，我们可以采用Docker的方式来准备集成测试环境。Docker比较轻量，速度也快。但是要做到第二个目标，通过Docker构建一个和生产环境同样对象版本的环境，这就需要开发人员对数据库的对象、脚本的维护都能进行版本化管理。</p><p>这里我说说我的实践经验，希望你能灵活应用，帮助DBA和开发人员管理好数据库。</p><p>第一，<strong>版本化管理</strong>。所有的数据库对象、脚本、数据都要和应用代码一起进行版本化管理。结合自己的实践验证，我特意给你梳理了后面的自检清单。</p><p><img src=\"https://static001.geekbang.org/resource/image/0a/66/0a29abbd1b644cbf92f766699f458e66.jpg?wh=1920x1006\" alt=\"图片\"></p><p>这项工作的完成标准，就是你随时可以从版本库里，拉出来一个版本的应用代码和数据库脚本，部署成一个可运行的应用+数据库。</p><p>第二，跟应用代码升级一样，数据库的变更也要通过版本化的脚本来完成。</p><p>一些开发人员有一个不好的习惯，直接用Console等工具变更数据库，比如给表加一个字段、修改一个字段的长度等等。这些变更没有记录，之后数据库的维护、回滚、重建都会遇到困难。</p><p>正确的姿势是，开发人员应该写脚本来完成数据库的变更。如果变更数据库schema，那就写一段ALTER SQL，在开发环境测试通过后，把它提交到版本库里，<strong>和提交、合并应用代码是一样的流程</strong>。然后Ops通过部署，把应用代码和数据库变更，一起更新到生产环境。</p><p>好，如果你的开发人员和DBA完成了这些工作，那就可以享受集成测试的方便和敏捷了，你可以写一个Dockerfile，来描述Mysql数据库的生成步骤，比如这样：</p><pre><code class=\"language-dockerfile\">FROM mysql:5.6\nENV MYSQL_ROOT_PASSWORD liusheng12345\nCOPY sql/my_install.sql /tmp/\n# https://hub.docker.com/_/mysql \nCOPY sql/seed_data.sh /docker-entrypoint-initdb.d/\nCOPY cnf/my.cnf /etc/mysql/\n</code></pre><p>然后，你再运行一个Docker 命令：</p><pre><code class=\"language-json\">Docker build -t \"myintegration:1.0\" .\n</code></pre><p>这样一来，你的数据库实例就启动了，可以测试了！</p><h3>集成测试环境搭建神器</h3><p>通过前面的学习，我们发现Docker具有快速、轻量、低成本的特点，是我们搭建集成测试环境的理想技术方案。沿着这个方向，你可以学习Kubernetes、Helm、Docker Compose这些Docker部署技术，把它们应用在集成测试里，但这个学习过程也是个耗时又耗脑细胞的过程。</p><p>那有没有一种方案，能封装这些技术，让测试人员不用懂Docker，也能管理集成测试环境呢？</p><p>有痛点，就一定会有人去做解决方案。所以测试容器化应运而生，有Testcontainers、Arquillian等工具。我们这就来看一下Testcontainers是怎么做的。</p><p>Testcontainer实际就是一组Java Library，自动化测试开发人员通过调用Java代码，来生成Docker容器实例。</p><pre><code class=\"language-java\">public class OrderTest{\n  @Container\n  PostgreSQLContainer&lt;?&gt; postgreSQLContainer = new PostgreSQLContainer&lt;&gt;(\"postgres:latest\");\n \n  @Before\n  public void setup(){\n    String jdbcURL = postgreSQLContainer.getJdbcUrl();\n    String username =  postgreSQLContainer.getUsername();\n    String password = postgreSQLContainer.getPassword();\n    //使用刚创建的数据库容器作为测试数据库\n    OrderRepository.initializeDB(jdbcUrl,username,password);  \n  }\n  public void testCreateOrder(){\n    //创建订单\n    .........createOrder............\n    //从数据库容器获得订单数据，验证创建成功\n    ........ fetch order from containerized postgreSQL............\n  }\n}\n</code></pre><p>上面的Java代码，在new PostgreSQLContainer对象的时候，实际上创建并启动了一个用Postgres作为镜像的Docker容器，用这个Docker容器作为Order数据库，这样，创建的订单会被保存在容器里。</p><p>容器对象建好以后，在后面的Test方法，我们就可以连上这个容器对象，来验证里面的订单数据了。是不是很简单？甚至你根本不需要懂Docker是怎么启动的，就可以集成测试了。</p><p>利用容器代码化这个便捷方法，你还可以创建错误数据的数据库，做异常测试。</p><pre><code class=\"language-java\">@Container\npublic PostgreSQLContainer&lt;?&gt; goodPostgreSQLContainer = new PostgreSQLContainer&lt;&gt;(\"postgres:alpine\")\n            .withInitScript(\"db_init.sql\")\n            .waitingFor(Wait.forLogMessage(\"good database startup\", 1));\n@Container\npublic PostgreSQLContainer&lt;?&gt; badPostgreSQLContainer = new PostgreSQLContainer&lt;&gt;(\"postgres:alpine\")\n            .withInitScript(\"db_wrong.sql\")\n            .waitingFor(Wait.forLogMessage(\"bad database startup\", 1));\n</code></pre><p>TestContainer的解决方案不限于数据库，还有RabbitMQ、Kafaka消息中间件，Elastic search服务实例、Azure云服务等等。更多同类的解决方案，你还可以到<a href=\"https://www.testcontainers.org\">这个网站</a>上去看一下模块列表。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/28/260cff883d213be85ca940b51a192d28.jpg?wh=1507x1050\" alt=\"图片\"></p><h2>小结</h2><p>一口气学到这里不容易，给坚持学习的你点赞。今天围绕集成测试的可测试性这个主题，我们一起学习了不少开发知识。</p><p>开发需要定义Interface，这样测试程序就可以通过Interface快速生成可用的Mock对象。简单高效的Happy Path，实际要依赖<strong>设计合理、边界清楚、调用简捷的代码</strong>。</p><p>除了代码规范，我还介绍了一些链路观测、追踪工具。有了这些神器加持，你就能直观看到某个请求的调用路径，从结果上寻找Happy Path。</p><p>结合实践经验来看，搭建集成测试环境这项工作也不容忽视。想要快速搭建集成测试环境，还需要开发人员和DBA有效管理好数据库对象，所有的数据库对象、脚本、数据都要和应用代码一起进行版本化管理。</p><p>此外，测试环境Docker容器化也是一个必然的趋势，我们需要学会怎么利用Docker来容器化集成测试所需的外部实例，甚至我们的测试环境也可以被容器化，达到开箱即用的效果。</p><p>说句题外话，<a href=\"https://time.geekbang.org/column/article/506638\">第十一讲</a>和这一讲，是第二模块我认为最有价值的两篇内容，它们的出发点都是<strong>从测试推动开发</strong>。</p><p>也许现在的你，并没有决定未来选择什么方向发展，但我希望你可以掌握全栈知识，看到更多立足测试思维去解决问题的可能性，自由穿梭于测试和开发之间，这样你的工作前景也会越来越宽广。</p><h2>思考题</h2><p>分享一下你有哪些好办法来搭建集成测试环境？</p><p>欢迎你在留言区和我交流互动，也推荐你把今天这一讲分享给你的同事、朋友。</p>","neighbors":{"left":{"article_title":"12｜集成测试（一）：一条Happy Path扫天下","id":507443},"right":{"article_title":"14｜集成测试（三）：护航微服务集群迭代升级","id":509551}}},{"article_id":509551,"article_title":"14｜集成测试（三）：护航微服务集群迭代升级","article_content":"<p>你好，我是柳胜。</p><p>从第七讲开始，我们的FoodCome系统一步步演变。可以看到，当FoodCome从一个单体应用发展成一个服务集群的时候，它的内部服务，按功能可以划分出前端和后端、上游和下游等等</p><p>这就像传统社会走向现代化，开始分出第一产业、第二产业和第三产业，接着逐渐出现精细分工，产生了各种专业岗位，共同协作来完成整个社会的运转。这么复杂的社会，是靠什么协调不同的职业呢？靠的是大家都遵守法律和契约。</p><p>而在微服务集群的世界，也是一样的道理。各个服务之间通过契约来交互协作，整个系统就能运转起来。所以，契约就是微服务世界里一个重要的概念。契约是怎么用起来的呢？</p><p>这就绕不开两个关键问题，<strong>第一，契约的内容是什么？第二，谁来保障，怎么保障契约的履行？</strong>今天我们就带着这两个问题来学习服务的契约，学完这一讲之后，你就知道怎么做微服务的集成测试了。</p><h2>契约的内容</h2><p>在“微服务测什么”一讲中（<a href=\"https://time.geekbang.org/column/article/503214\">第八讲</a>），我们已经整理出来了订单服务的契约。我带你复习一下当时我们整理出来的两个接口规范，我把它们贴到了后面。</p><p>一个是RestAPI，完成用户下单的功能，OpenAPI接口定义如下：</p><pre><code class=\"language-yaml\">\"/api/v1/orders\":\n&nbsp; &nbsp; post:\n&nbsp; &nbsp; &nbsp; consumes:\n&nbsp; &nbsp; &nbsp; - application/json\n&nbsp; &nbsp; &nbsp; produces:\n&nbsp; &nbsp; &nbsp; - application/json\n&nbsp; &nbsp; &nbsp; parameters:\n&nbsp; &nbsp; &nbsp; - in: body\n&nbsp; &nbsp; &nbsp; &nbsp; name: body\n&nbsp; &nbsp; &nbsp; &nbsp; description: order placed for Food \n&nbsp; &nbsp; &nbsp; &nbsp; required: true\n&nbsp; &nbsp; &nbsp; &nbsp; properties:\n          foodId:\n            type: integer\n          shipDate:\n            type: Date\n          status:\n            type: String\n            enum:\n            - placed\n            - accepted\n            - delivered\n&nbsp; &nbsp; &nbsp; responses:\n&nbsp; &nbsp; &nbsp; &nbsp; '200':\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; description: successful operation\n&nbsp; &nbsp; &nbsp; &nbsp; '400':\n          description: invalid order\n</code></pre><!-- [[[read_end]]] --><p>还有一个是消息接口，它在处理完订单后，还要往消息队列的Order Channel里发布这样的消息，这样别的服务就能从Order Channel取到这个订单，再进行后续的处理。</p><p>AsyncAPI接口定义如下：</p><pre><code class=\"language-yaml\">asyncapi: 2.2.0\ninfo:\n&nbsp; title: 订单服务\n&nbsp; version: 0.1.0\nchannels:\n&nbsp; order:\n&nbsp; &nbsp; subscribe:\n&nbsp; &nbsp; &nbsp; message:\n&nbsp; &nbsp; &nbsp; &nbsp; description: Order created.\n&nbsp; &nbsp; &nbsp; &nbsp; payload:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: object\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; properties:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; orderID:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: Integer\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; orderStatus:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: string\n</code></pre><p>这两份契约的服务提供者是订单服务，消费者有两个，一个RestAPI契约的消费者，一个是消息契约的消费者。我画了一张图，你会看得更清楚些。</p><p><img src=\"https://static001.geekbang.org/resource/image/f0/d6/f03ace81d9081d8d61efa475b0cc19d6.jpg?wh=1920x1183\" alt=\"图片\"></p><h2>契约的游戏规则</h2><p>有了契约后，微服务的开发协作，就基于契约运转起来了，怎么运行呢，分成契约的<strong>建立、实现、验证</strong>三个阶段。</p><p>1.契约建立。契约双方，也就是服务提供者和消费者“坐在一起”，签订了一个契约，大家都同意遵守这个规则来做自己的开发。</p><p>2.契约的实现。订单服务按照契约来实现自己的服务接口，同时，API网关和通知服务、餐馆服务，它们都按照契约来实现自己的调用接口。</p><p>3.契约的验证，契约双方完成自己的工作后，然后再“坐在一起”完成集成，看看是不是履行了契约。</p><p>这个协作模型，跟我们现实里常见的债务合同很相似。合同签订的内容是，订单服务欠下了一笔债，到开发周期结束后，订单服务要按照合同约定的方式向调用者偿还这笔债。</p><p>但这还是个模型，想要真正落地实践，有两个问题需要考虑清楚。</p><p>第一个问题是监督机制。在契约建立日到履行日之间的这段时间里，有没有办法设置检查点来检查契约履行的进度和正确性，万一订单服务跑偏了，可以提前纠正。</p><p>第二个问题是检查办法，也就是如果要做检查，谁负责检查？</p><p>显然，这个检查的手段就是测试，那么谁来做这个测试呢？让服务者自测？</p><p>这个不太靠谱，最合适的办法，是让消费者去做测试，这就像在债务合同里，法律规定债权人要定时追讨债务，不履行追讨权超过一定时间，最终法院可能会不支持诉讼。这样做的目的是保证契约机制运转高效。</p><p>欠债还钱的现实世界，债权人推动着合同如期履行。按时交付的技术领域，消费者驱动着契约测试，那这个过程具体是怎么操作的呢？</p><h2>消费者驱动契约测试</h2><p>消费者驱动契约测试的玩法是这样的：消费者来主动去定义契约，开发测试脚本，然后把这个测试脚本交给服务者去跑，服务者要确定自己开发的代码能测试通过。这个过程相当于消费者完成了验收测试。</p><p>对于FoodCome来说，API网关负责编写RestAPI测试案例，通知服务和餐馆服务负责编写Message测试案例，如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/e5/be/e58fc28844c2950c627c5aa114615abe.jpg?wh=1920x1356\" alt=\"图片\"></p><h2>RestAPI的契约测试</h2><p>先来看一下RestAPI的契约测试怎么做。</p><p>首先你要明白，我们这种契约测试的场景处于<strong>开发阶段</strong>，契约测试案例的工具需要持续而快速地维护和验证契约。</p><p>所以，这个工具应该有高效的自动化能力，具体要满足这两个条件，首先要能解析契约，其次还能根据契约生成Test Class和Stub方便测试。</p><p>符合这两个条件的工具有不少，其中Pact和SpringCloud比较主流。今天我们就以Spring Cloud为例来看一下RestAPI契约测试怎么做的。</p><p>第一步，Spring Cloud先要加载契约，代码示例如下：</p><pre><code class=\"language-groovy\">org.springframework.cloud.contract.spec.Contract.make {\n    request {\n        method 'POST'\n        url '/api/v1/orders'\n    }\n    response {\n        status 200\n        headers {\n            header('Content-Type': 'application/json;charset=UTF-8')\n        }\n        body('''{\"orderId\" : \"1223232\", \"state\" : \"APPROVAL_PENDING\"}''')\n    }\n}\n</code></pre><p>第二步，根据契约，Build分别生成Stub和Test Class，其中Test Class给服务提供者，Stub给消费者，因为它们是同一份契约产生的，所以只要运行成功，就等同于双方都遵守了契约。</p><p>原理图是这样的，在订单服务项目下，运行Spring Cloud Contract Build，会在target/generated-test-sources目录下，自动产生一份ContractVerifierTest代码，供订单服务（也就是服务提供者）来测试自己的服务接口，也就是下图的右侧区域。</p><p>同时，SpringCloud Contract还提供一个sub-runner的Jar包，供消费者做集成测试的stub，这里对应着下图的左侧区域。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/97/753792656271befa2f074eebyy042897.jpg?wh=1920x1082\" alt=\"图片\"></p><p>服务者侧的集成测试代码示例如下：</p><pre><code class=\"language-java\">public abstract class ContractVerifierTest {\n  private StandaloneMockMvcBuilder controllers(Object... controllers) {\n    ...\n    return MockMvcBuilders.standaloneSetup(controllers)\n                     .setMessageConverters(...);\n  }\n  @Before\n  public void setup() {\n    //在开发阶段，Service和Repository还是用mock\n    OrderService orderService = mock(OrderService.class);                    ❶\n    OrderRepository orderRepository = mock(OrderRepository.class);\n    OrderController orderController =\n              new OrderController(orderService, orderRepository);\n  }\n  @Test\n  public void testOrder(){\n    when(orderRepository.findById(1223232L))                                 ❷\n            .thenReturn(Optional.of(OrderDetailsMother.CHICKEN_VINDALOO_ORDER));\n    ...\n    RestAssuredMockMvc.standaloneSetup(controllers(orderController));        ❸\n  }\n}\n</code></pre><p>这段代码的意思是，开发人员先写好OrderController代码，把接口代码写好，负责业务逻辑的OrderService和OrderRepository暂时用Mock来替代。而自动生成的ContractVerifierTest是来测试和验证OrderController的接口，不管将来OrderService和OrderRepository怎么实现和变化，只要保证OrderController接口不变，就可以。</p><p>消费者这一侧，这是在本地启动一个HTTP的Stub服务，在真实的订单服务没有完成之前，消费者可以和Stub做集成测试。具体代码如下：</p><pre><code class=\"language-java\">@RunWith(SpringRunner.class)\n@SpringBootTest(classes=TestConfiguration.class,\n        webEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =                                            \n         {\"com.foodcome.contracts\"},\n        workOffline = false)\n@DirtiesContext\npublic class OrderServiceProxyIntegrationTest {\n  @Value(\"${stubrunner.runningstubs.foodcome-order-service-contracts.port}\")  \n  private int port;\n  private OrderDestinations orderDestinations;\n  private OrderServiceProxy orderService;\n  @Before\n  public void setUp() throws Exception {\n    orderDestinations = new OrderDestinations();\n    String orderServiceUrl = \"http://localhost:\" + port;\n    orderDestinations.setOrderServiceUrl(orderServiceUrl);\n    orderService = new OrderServiceProxy(orderDestinations,               \n                                          WebClient.create());\n  }\n  @Test\n  public void shouldVerifyExistingCustomer() {\n    OrderInfo result = orderService.findOrderById(\"1223232\").block();\n    assertEquals(\"1223232\", result.getOrderId());\n    assertEquals(\"APPROVAL_PENDING\", result.getState());\n  }\n  @Test(expected = OrderNotFoundException.class)\n  public void shouldFailToFindMissingOrder() {\n    orderService.findOrderById(\"555\").block();\n  }\n}\n</code></pre><p>可以看到，只要契约不变，生成的服务端测试代码也是不变的。如果有一天，服务端在迭代开发中没有遵守契约，那么测试案例就会失败。</p><p>测试案例失败之后，服务端面临两个选择，要么修改自己的代码让契约测试通过，要么去修改契约，但是修改了契约后，消费者的测试又会失败。这样，我们就能以<strong>测试结果</strong>为准绳，让消费者和服务者始终保持同步。</p><h2>Message的契约测试</h2><p>Spring Cloud Contract也支持基于Message的契约，它和RestAPI的契约实现方法比较像，直接上原理图，你理解起来更直观。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/b8/0d0329a375100e55ab572082b4dff6b8.jpg?wh=1920x1292\" alt=\"图片\"></p><p>这里我画了一张图片，为你解读餐馆服务和订单服务通过契约做集成测试的内部原理。</p><p>还是同样的配方，熟悉的味道，一份契约产生服务者端集成测试代码和消费者集成测试代码。跟OpenAPI的原理类似，这里我同样把示例代码贴出来，供你参考。</p><p>服务端的集成测试代码如下：</p><pre><code class=\"language-java\">@RunWith(SpringRunner.class)\n@SpringBootTest(classes = MessagingBase.TestConfiguration.class,\n                webEnvironment = SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureMessageVerifier\npublic abstract class MessagingBase {\n  @Configuration\n  @EnableAutoConfiguration\n  @Import({EventuateContractVerifierConfiguration.class,\n           TramEventsPublisherConfiguration.class,\n           TramInMemoryConfiguration.class})\n  public static class TestConfiguration {\n    @Bean\n    public OrderDomainEventPublisher\n            OrderDomainEventPublisher(DomainEventPublisher eventPublisher) {\n      return new OrderDomainEventPublisher(eventPublisher);\n    }\n  }\n\n  @Autowired\n  private OrderDomainEventPublisher OrderDomainEventPublisher;\n  protected void orderCreated() {                                   \n     OrderDomainEventPublisher.publish(CHICKEN_VINDALOO_ORDER,\n          singletonList(new OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS)));\n  }\n}\n</code></pre><p>消费者端集成测试代码如下：</p><pre><code class=\"language-java\">@RunWith(SpringRunner.class)\n@SpringBootTest(classes= RestaurantEventHandlersTest.TestConfiguration.class,\n        webEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n        {\"foodcome-order-service-contracts\"},\n        workOffline = false)\n@DirtiesContext\npublic class RestaurantEventHandlersTest {\n  @Configuration\n  @EnableAutoConfiguration\n  @Import({RestaurantServiceMessagingConfiguration.class,\n          TramCommandProducerConfiguration.class,\n          TramInMemoryConfiguration.class,\n          EventuateContractVerifierConfiguration.class})\n  public static class TestConfiguration {\n    @Bean\n    public RestaurantDao restaurantDao() {\n      return mock(RestaurantDao.class);                                    \n     }\n  }\n  @Test\n  public void shouldHandleOrderCreatedEvent() throws ... {\n    stubFinder.trigger(\"orderCreatedEvent\");                                 \n     eventually(() -&gt; {                                                      \n       verify(restaurantDao).addOrder(any(Order.class), any(Optional.class));\n    });\n  }\n</code></pre><p>使用Pact也可以达到同样的效果，如果感兴趣，你可以研究一下。</p><h2>小结</h2><p>今天我们主要讲了微服务群内部之间的集成测试。</p><p>跟外部的服务集成测试不同，内部服务经常处在一个迭代开发的状态，可能一个服务变动了，就会导致别的服务不能工作。</p><p>为了解决这种问题，我们引入了<strong>消费者驱动契约测试</strong>的方法论。这个契约测试的特点是消费者把自己需要的东西写入契约，这样一份契约产生两份测试代码，分别集成到契约的服务端和消费端，服务端有任何违背契约的代码变更，会第一时间以测试失败的形式抛出。</p><p>为了让你深入理解契约测试的思想，学会怎样把这个方法论真正落地。我还带你一起实现了Spring Cloud的在RestAPI和Message两个方面的契约示例。有了这个基础，你可以结合自己面对的实际情况做调整，实现更契合自己项目的一套契约，集成测试做起来也会更得心应手。</p><p>当然了，Sping Cloud Contract还有更多的扩展使用，比如和OpenAPI的转换、Cotract的中央存储和签发等等，你有兴趣可以在这个领域继续深挖，也期待你通过留言区晒出自己的心得。</p><h2>牛刀小试</h2><p>这一讲中的契约是Groovy方式书写的，我们之前总结的契约是以YAML方式表现的，你可以在Spring Cloud Contract和Pact中任选其一，实现对yaml契约的加载。</p><p>欢迎你和我多多交流讨论，也推荐你把今天的内容分享给身边的朋友，和他共同进步。</p>","neighbors":{"left":{"article_title":"13｜集成测试（二）：携手开发，集测省力又省心","id":508574},"right":{"article_title":"15｜UI测试：如何让UI测试更轻快便捷？","id":510620}}},{"article_id":510620,"article_title":"15｜UI测试：如何让UI测试更轻快便捷？","article_content":"<p>你好，我是柳胜。</p><p>恭喜你坚持到这里，我们顺着测试金字塔底层的单元测试一步步向上，现在终于到了金字塔顶部。按照我们的整体设计，其实脏活累活已经在底层干得差不多了。</p><p>爬上塔顶不容易，应该是一身轻松，纵览风光了。可以想象，如果没有前面的整体设计，没有单元测试来夯实基础，把测试工作全都压到端到端测试，它必然会垮掉。</p><p>不过，既然需要金字塔顶部这个UI测试层，一定是它不可替代，做得了其他层力所不能及的事儿。今天咱们就来梳理下UI测试要做什么，怎么做才能收割更高的ROI。</p><p>UI全名叫做User Interface，在当下，User这个概念已经被扩展，甚至被滥用，我倒觉得，UI叫做PI（People Interface）更为准确，专指和人格用户交互的界面。</p><p>从UI这个角度，主要有三个测试点需要去关注：第一，用户的行为；第二，UI的布局；第三是用户的易用性。当然，根据具体业务的需求，还有其他的点，比如Globalization全球化、Accessibility亲和力等等。</p><h2>用户行为测试</h2><p>用户的行为，指的是用户通过操作UI，获得他想要的功能。在FoodCome里，用户通过WebUI填好订单信息，然后点击“下订单”按钮，就能完成下单功能。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/d7/81/d7cf1716877d300d129f26578a050181.png?wh=681x411\" alt=\"图片\"></p><p>分析一下就能知道，在这个过程里，有两部分代码逻辑参与了下单，一个是前端逻辑，就是HTML+JavaScript代码；另外一个就是后端逻辑，也就是我们前面讲过的RestAPI和DB。</p><p>既然后端逻辑我们在单元测试就测过了，而前后端集成我们也用契约测试测过了，那么UI测试的关键点，就在于前端逻辑有没有，又有多少？</p><p>这里要分两种情况。第一种，前端没有业务逻辑，就是简单的发送请求，接收响应并展现。这些工作都是通过浏览器的内嵌功能来完成的。比如FoodCome可以用一个HTML form来完成订单的提交过程：</p><pre><code class=\"language-xml\">&lt;form method=\"POST\" enctype=\"application/x-www-form-urlencoded\" action=\"/html/codes/html_form_handler.cfm\"&gt;\n&lt;p&gt;\n&lt;label&gt;餐馆名字\n&lt;input type=\"text\" name=\"restaurant_name\" required&gt;\n&lt;/label&gt;&nbsp;\n&lt;/p&gt;\n\n&lt;fieldset&gt;\n&lt;legend&gt;菜单&lt;/legend&gt;\n&lt;p&gt;&lt;label&gt; &lt;input type=\"checkbox\" value=\"no1\"&gt; 宫保鸡丁 &lt;/label&gt;&lt;/p&gt;\n&lt;p&gt;&lt;label&gt; &lt;input type=\"checkbox\" value=\"no2\"&gt; 佛跳墙 &lt;/label&gt;&lt;/p&gt;\n&lt;p&gt;&lt;label&gt; &lt;input type=\"checkbox\" value=\"no3\"&gt; 珍珠翡翠白玉汤 &lt;/label&gt;&lt;/p&gt;\n&lt;/fieldset&gt;\n&lt;p&gt;&lt;button&gt;下订单&lt;/button&gt;&lt;/p&gt;\n\n&lt;/form&gt;\n</code></pre><p>这也是业界提到过的Thin Client瘦客户端，客户端里没有或只有很少的业务逻辑。</p><p>瘦客户端怎么测？我的答案是，在单元测试和集成测试已经充分的情况下，瘦客户端只需找两三个典型业务场景测一下，甚至都不需要考虑UI自动化。因为主要的逻辑和路径都已经测过了嘛，你没必要再花时间重复。</p><p>与瘦客户端相对应的是胖客户端，也叫Rich Client，当然胖客户端里是嵌入了大量的业务逻辑。当今业界，胖客户端更加普遍，比如WebUI里嵌入了JavaScript来聚合后端的数据、画图、表格、排序等等，从这个角度，你也可以把Desktop客户端直接当作胖客户端来对待。</p><p>胖客户端该怎么测？ 要回答这个问题，我们需要首先思考一下“胖客户端是什么”。在微服务世界里，每个微服务实现自己的业务逻辑，向外提供服务，同时也是客户端去消费其他的服务。</p><p>而胖客户端是什么呢，它也有自己的业务逻辑，聚合数据、图形化都是它的业务逻辑。但是有一点特殊，胖客户端是微服务集群调用链条最早一个，它只会去调用别人，调用胖客户端的是终端用户。</p><p>从这个角度来看，胖客户端满足了提供服务，也消费其他服务的微服务特征，因此<strong>胖客户端本质上也是一个微服务。</strong></p><p>说到这里，胖客户端怎么测这个问题的答案就呼之欲出了。微服务该怎么测，胖客户端就该怎么测。什么意思呢？你还是要遵循测试3KU金字塔原则，胖客户端首先要做单元测试，再做集成测试，最后才是UI测试。</p><p>看到这里你可能会有点困惑，UI客户端还要分层，这是怎么回事呢？我拿WebUI的开发框架举个例子，你就明白了。</p><p>React是业界很常用的JavaScript开发框架，看看它是怎么实现下订单操作的：</p><pre><code class=\"language-javascript\">class FlavorForm extends React.Component {\n&nbsp; constructor(props) {\n&nbsp; &nbsp; super(props);\n&nbsp; &nbsp; this.handleChange = this.handleChange.bind(this);\n&nbsp; &nbsp; this.handleSubmit = this.handleSubmit.bind(this);\n&nbsp; }\n\n&nbsp; handleChange(event) {&nbsp; &nbsp; this.setState({value: event.target.value});&nbsp; }\n&nbsp; handleSubmit(event) {\n&nbsp; &nbsp; alert('Your order is: ' + this.state.value);\n&nbsp; &nbsp; event.preventDefault();\n&nbsp; }\n\n&nbsp; render() {\n&nbsp; &nbsp; return (\n&nbsp; &nbsp; &nbsp; &lt;form onSubmit={this.handleSubmit}&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;label&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Pick your favorite flavor:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;select value={this.state.value} onChange={this.handleChange}&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n            &lt;option value=\"no1\"&gt;宫保鸡丁&lt;/option&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;option value=\"no2\"&gt;佛跳墙&lt;/option&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;option value=\"no3\"&gt;珍珠翡翠白玉汤&lt;/option&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/select&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/label&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;input type=\"submit\" value=\"下订单\" /&gt;\n&nbsp; &nbsp; &nbsp; &lt;/form&gt;\n&nbsp; &nbsp; );\n&nbsp; }\n}\n</code></pre><p>上面一段简单React JavaScript代码有render来做数据的展现，有Handler来做数据的处理，还有props做数据的存储。其实React开发的前端功能，跟一个后端服务的MVC结构是类似的。</p><p><img src=\"https://static001.geekbang.org/resource/image/0c/13/0c19d928a328cf566yyb180295e53b13.jpg?wh=1920x1242\" alt=\"图片\"></p><p>看到没有？UI前端也有设计模式，也可以实现很多业务逻辑。所以，你可以把UI前端当做一个微服务来测试，既然是微服务，那就可以分层，做单元测试。</p><p>那前端的单元测试怎么做呢？和后端原理是一样的，该写Test方法写Test方法，该Assert就Assert，该Mock就Mock。只是前端开发框架有很多种，相对应地，单元测试框架也有多种，你需要找到匹配的那一对。我给你总结了一个表格，你也可以结合自己实践拓展、丰富它。</p><p><img src=\"https://static001.geekbang.org/resource/image/cb/77/cb48a99e45156d32cedc40ddc7410b77.jpg?wh=1920x714\" alt=\"图片\"></p><p>下面是使用Vue Test Utils来执行单元测试的例子，在订单页面，点击一个check Order按钮，验证页面上是否会显示“order validated”的消息。</p><pre><code class=\"language-javascript\">import { shallowMount } from '@vue/test-utils'\n\timport OrderToggle from '@/components/OrderToggle.vue'\n\timport Order from '@/components/Order.vue'\n\tdescribe('OrderToggle.vue', () =&gt; {\n\t  it('toggles msg passed to Order when Place Order button is clicked', () =&gt; {\n\t    const wrapper = shallowMount(OrderToggle)\n\t    const button = wrapper.find('#check-order')\n\t    button.trigger('click')\n\t    const OrderComponent = wrapper.find(Order)\n\t    expect(OrderComponent.props()).toEqual({msg: 'order validated'})  \n\t  })\n\t})\n</code></pre><p>你可以看到，JavaScript单元测试能测试数据逻辑，也能测试页面事件，模拟人的行为，发送一个个点击、输入事件。那么你可能还想问，前端JavaScript的单元测试做完，是不是就不需要额外的UI测试了呢？</p><p>这是一个好问题，不过完成之上，我们希望做得更加完美、更有效益。结合我们专栏里我不厌其烦给你提到的3KU原则，本着“做有效的，不做浪费的测试”的目标，单元测试做完了，UI上只做单元测试没做到的事情。</p><p>你可以思考一下符合这个条件的场景有没有，在哪里？</p><h2>页面的Layout布局测试</h2><p>相比API测试，UI的测试还有一个特殊的地方，不但要验证UI的控件画出来了，而且还要验证它们都在正确的位置上，这个验证就叫做UI布局测试。</p><p>你可以这样理解，API测试里，我们检查数据的时候，是一维的检查，而在UI测试里，数据的检查是二维的，有了x、y的坐标。这个复杂度一下子就上来了。</p><p>布局测试怎么做？有两种方案，咱们分别来看看。</p><p>一种是抓图方案，它是在运行UI自动化测试的时候，顺便调用captureScreen函数，对当前的UI抓屏，保存成图片。然后利用图片比较技术，去看页面的布局有没有发生变化。所以这个方案的技术关键点，就是<strong>位图比较</strong>。业界比较成熟的技术实现有Applitools、Sikuli。</p><p>比如，用Applitools的eyes类进行对比：</p><pre><code class=\"language-java\">driver = new ChromeDriver();\neyes = new CompareEyes();\n// 设置匹配级别为Layout布局\neyes.setMatchLevel(MatchLevel.LAYOUT);\neyes.setEnableComparison(true);\neyes.open(driver, appName, testName, viewPortSize);\neyes.switchToComparisonMode(driver);\n// 使用eyes对比当前窗口和已经保存的图片\neyes.check(\"/Users/sheng/Desktop/login.png\", Target.window());\neyes.close();\ndriver.quit();\n</code></pre><p>上面的代码是，启动Selenium Web Driver，加载页面，初始化eyes，然后调用eyes的check函数来实现图片的比较。</p><p>Applitools有AI的功能，在早期，测试人员手工地对它的比对结果进行确认或纠正，这相当于是训练了AI比对模型。这样使用一段时间后，它的比对会越来越智能，结果会越来越准确。</p><p>第二种是Layout规格说明书方案，什么意思呢？跟传统测试一样，需要先写一份Layout规格说明书，比如屏幕上在什么位置应该出现什么元素等等，应该有一个列表展示。</p><p>然后，自动化测试运行的时候，就把render出来的页面和规格说明书相比较，测试成功或失败。在这个领域里的工具也有很多种，Galen和Lineup是其中的代表。</p><p>比如说，下面我用Galen这个工具，演示的FoodCome系统login页面的Layout规格说明书LoginPage.spec：</p><pre><code class=\"language-java\">@objects\n\t    login-box           id  login-page\n\t    login-caption       css #login-page h2\n\t    username-textfield  css input[name='login.username']\n\t    password-textfield  css input[name='login.password']\n\t    login-button        css .button-login\n\t    cancel-button       css .button-cancel\n\t= Login box =\n\t    @on *\n\t        login-box:\n\t            centered horizontally inside content 1px\n\t            below menu 20 to 45px\n\t        login-caption:\n\t            height 20 to 35px\n\t            text is \"Login\"\n\t        username-textfield, password-textfield:\n\t            height 25 to 35 px\n\t        login-button, cancel-button:\n\t            height 40 to 50 px\n</code></pre><p>在这个spec里，描述了登录页面的布局，有6个页面对象：登录框、登录标题、用户名输入框，密码输入框、登录按钮和取消按钮，还说明了它们各自的样式和位置。</p><p>在运行测试的时候，当加载login页面的时候，会把展现出来的页面和LoginPage.spec进行匹配验证，匹配成功，说明Layout是按照预期加载的。</p><pre><code class=\"language-java\">public void loginPage_shouldLookGood_onDevice(TestDevice device) throws IOException {\n\tload(\"/\");\n\tgetDriver().findElement(By.xpath(\"//button[.='Login']\")).click();\n\tcheckLayout(\"/specs/loginPage.spec\", device.getTags())\n}\n</code></pre><h2>小结</h2><p>到这里，总结一下我们今天学习到的内容。</p><p>UI测试主要有三个关注点：第一，用户的行为；第二，UI的布局；第三，用户的易用性。</p><p>我并没有在正文介绍易用性，是因为这个关注点，最终指向的问题是：用户体验是一个“感觉好还是坏”。这是一个通过计算机技术，很难做回答的问题。所以，用户体验还是手工测试的方法，你可以考虑用探索性测试的策略来去发现易用性的问题，而这一讲我们重点讨论了前两个关注点，用户的行为和UI的布局。</p><p>从用户行为这个视角分析，UI测试的客户端，可以分为瘦客户端和胖客户端，瘦客户端的测试简单，你可以按照Happy Path的思路找出一两个案例来跑一下就可以了。而胖客户端包含了大量的业务逻辑，你应该用测试服务的方法来测试胖客户端，也要做单元测试。这一讲中针对JavaScript开发框架，列出了相应的单元测试框架，供你参考。</p><p>UI的布局测试也是一个特殊的领域，业界里有两种自动化思路，一个是基于图片，一个是基于Spec，两种方法都各有优势和劣势。我们可以根据项目目标和具体情况，采用其中一个，也可以把这两个思路都用上。</p><h2>牛刀小试</h2><p>说说你的项目中，UI前端有没有做单元测试？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这讲内容分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"14｜集成测试（三）：护航微服务集群迭代升级","id":509551},"right":{"article_title":"16｜概念重识：如何用3KU为端到端&验收测试赋能？","id":511982}}},{"article_id":511982,"article_title":"16｜概念重识：如何用3KU为端到端&验收测试赋能？","article_content":"<p>你好，我是柳胜。</p><p>看到这一讲标题里的“端到端测试”和“验收测试”，还有上一讲的“UI测试”，你可能还有点懵：在实践中，它们三个经常是一回事啊？</p><p>先给你理一理这三个概念。验收测试是指的客户视角，端到端的测试指的是测试方法，UI测试指的是测试发起的地方。听着不太一样，是吧？</p><p>可是我们为什么会觉得这些是一回事呢？因为在实践里，这三个测试概念常常指向同一个测试场景，即客户从UI端发起了一个对系统整体的可接受性测试。几年前的传统软件测试，这是成立的。但现在不一定了：客户不一定是UI用户，还有可能是API用户、SDK用户，端到端测试也不一定包括UI端。</p><p>这一讲，我们就用3KU法则重新审视一下这些测试概念，让我们的实践事半功倍。</p><h2>验收测试</h2><p>验收测试，相当于是一个契约履行。不同于建立在开发者之间的接口契约，验收契约建立在用户和系统之间。所以验收测试的前提条件有两条：第一，这个契约存在；第二，这个契约具有可测试性。</p><p>我们在<a href=\"https://time.geekbang.org/column/article/502863\">第七讲</a>“单体应用测什么”的时候，已经把FoodCome的契约表达出来了，就是<strong>用Ghkerkin语法描述出来的用户使用场景</strong>。</p><pre><code class=\"language-plain\">Given a consumer\n  And a restaurant\n  And a delivery address/time that can be served by that restaurant\n  And an order total that meets the restaurant's order minimum\nWhen the consumer places an order for the restaurant\nThen consumer's credit card is authorized\n  And an order is created in the PENDING_ACCEPTANCE state\n  And the order is associated with the consumer\n</code></pre><!-- [[[read_end]]] --><p>这样一段描述写在一个名为placeOrder.feature文件里，只要满足了Given的条件，做了When中定义的操作，就会得到Then里的结果。这就是契约的内容。</p><p>我之前和你说过Gherkins语法的好处，它的表达在自然语言和技术语言之间，需求人员理解起来不吃力，往前走一步又能成为测试案例，甚至自动化测试代码。</p><p>今天咱们就接着说说，这件事儿怎么实现。我介绍一个BDD自动化测试框架，它就是Cucumber。</p><p>Cucumber是支持行为驱动开发的软件工具。Cucumber的核心是它的Gherkin语言解析器，能够根据Feature文件直接生成自动化测试代码。详细情况，你可以参考Cucumber的<a href=\"https://cucumber.io\">官方网站</a>。</p><p>下面，我们用一个例子来说明一下Cucumber怎么使用。</p><p>具体操作步骤如下：</p><p>第一步，我们先生成一个测试项目工程，Cucumber可以支持多种开发语言，Ruby，Java，Javascript，.NET等。我们这里以Java为例，使用mvn来生成一个模版项目：</p><pre><code class=\"language-java\">mvn archetype:generate                      \\\n   -DarchetypeGroupId=io.cucumber           \\\n   -DarchetypeArtifactId=cucumber-archetype \\\n   -DarchetypeVersion=2.3.1.2               \\\n   -DgroupId=foodcometest                  \\\n   -DartifactId=foodcometest               \\\n   -Dpackage=foodcome                 \\\n   -Dversion=1.0.0-SNAPSHOT                 \\\n   -DinteractiveMode=false\n</code></pre><p>运行上面的命令，会生成一个空的Java项目，里面包含了Cucumber所需要的Library文件。</p><p>第二步，把上面的Feature文件，添加到项目路径：src/test/resources/foodcometest/placeorder.feature</p><p>接着是第三步，运行mvn命令：</p><pre><code class=\"language-plain\">mvn test\n</code></pre><p>遵循输出的指示，最终就可以自动生成一个测试Class文件了。</p><pre><code class=\"language-plain\">public class PlaceOrderTest ...  {\n  ...\n  @Given(\"A valid consumer\")\n  public void useConsumer() { ... }\n  @Given(\"a valid restaurant\")\n  public void useRestaurant() { ... }\n  @Given(\"a valid address\")\n  public void validAddres(String address) { ... }\n  @Given(\"a valid order\")\n  public void validAddres(Order orderDetails) { ... }\n  @When(\"I place an order\")\n  public void placeOrder() { ... }\n  @Then(\"the credit card should be authorized\")\n  public void authorizeCreditCard(Long creditCardNo) { ... }\n  @Then(\"order shoudl be Created with pending status\")\n  public void orderCreatedInPendingStatus()  { ... }\n}\n</code></pre><p>在Feature文件里，使用Given，When，Then关键字描述的步骤，对应着PlaceOrderTest的一个个函数，你通过函数名上的注解就可以看到这个对应关系，但是函数体还是TODO，需要你去实现。等你实现了这些函数，再运行Cucumber，它会按照Given，When，Then这个顺序来执行契约的验证了！</p><p>你可以看到这样操作的好处，自动化测试代码是紧紧贴合Feature文件的，如果契约变化了，那可以重新运行mvn命令，同步自动化测试代码。那么同时也意味着，自动化测试成功了，就代表契约验证通过，验收测试通过。</p><p>当然，上面说的只是一个Feature的测试，验收测试里还有一个关键问题，验收测试的范围应该有多大？我的建议是，<strong>签订了多少契约，就做多少验收测试</strong>。也就是说，用户显式表达了多少需求，就应该以这个为基准来做验收。</p><p>你可能会问，有些需求不一定是显式的，但又确实存在。比如一些业务异常路径、边角案例，甚至性能指标，这些也需要整体测试，该怎么测呢？这就要说到端到端的测试。</p><h2>端到端测试</h2><p>为什么要做端到端测试呢？我们在单元测试验证了业务逻辑，在集成测试验证了接口，现在终于要真刀真枪，拉上战场了。所有服务都一起上线，要看是不是能匹配得上。</p><p>所以，端到端测试（End-to-end testing）指的是一个功能从开始到结束，贯穿了整个系统的A服务、B服务，一直到N服务。通常这个功能是从哪里发起的呢？一般是UI端。它又在哪里结束呢？在某个服务模块，比如是数据库。</p><p>但是，根据3KU测试金字塔，在UI执行测试是ROI最小的。有没有办法找到一种ROI较高的端到端测试方法呢？在<a href=\"https://time.geekbang.org/column/article/497405\">第二讲</a>里，我提到过“<strong>分层是追求整体ROI的结果</strong> ”，反过来也是成立的，如果为了追求更高的ROI，你甚至可以创建出一个新的分层。</p><p>当时我就在想，UI测试ROI小，但好处是业务可见性强。如果下沉到代码层和接口层，ROI上来了，但是业务又模糊了。所以在接口测试层和系统测试层的中间地带，有没有可能找到一个业务可见性也比较强、ROI也比较高的测试层呢？</p><p>我很惊喜地发现，业界还真有人跟我一样在考虑这个问题了。Martin Fowler在他的网站提出了一种新的测试方法，叫做Subcutaneous Test，中文叫做<strong>皮下测试</strong>。</p><p>皮下测试，顾名思义，是当你要做的端到端测试在UI上测试很难做，或者能做但是成本很高，所以，选择紧贴着UI测试的下一层发起测试。在3KU测试金字塔里，它是在集成测试和UI测试中又加入了一层。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/5b/4671cce5c9bb86fabc726016fa222b5b.jpg?wh=1920x1108\" alt=\"图片\"></p><h3>皮下测试</h3><p>要想实现皮下测试，首先要找到皮层在哪里，看看请求链路，肉眼可见的皮层就是Http request，手机APP是皮上层，后台的订单服务是皮下层。</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/2e/c50744ea199518dcbdfc2a76d8d13a2e.jpg?wh=1920x986\" alt=\"图片\"></p><p>皮下测试就是模拟客户端发出的HTTP请求，直接发送到后端服务。这个用RestAssure测试框架就可以轻松做到：</p><pre><code class=\"language-java\">public class E2ERestTest {\n    @Test\n    public void shouldOrderBeCreated() throws Exception {\n        when()\n                .post(\"https://api.foodcome.com/api/v1/orders\", 443))\n        .then()\n                .statusCode(is(200))\n                .body(containsString(\"order created\"));\n    }\n}\n</code></pre><p>可以看出来，刚才做的皮下测试在执行方法上就是一个API测试。不过它跟正常的API测试相比，有两点不同：</p><p>第一，测试请求注入的地方是订单服务，也是UI端直接连接的入口，这个请求也贯穿了后端的服务A，服务B直到服务N，只是它绕过了ROI最小的UI端。</p><p>第二，发起的测试请求，目的是模拟UI端行为，而不是单纯地为了测试订单服务。</p><p>我给你分析一下，这个皮下测试的方法论原理：本来我们要端到端测试n个服务，但是由于第1个服务，也就是UI端的测试ROI非常低。在这里，怎么把复杂的问题简单化呢？可以利用动态规划的思维，我们把原先的测试工作量f(n)做了这样一个分解。</p><p>具体公式如下：<strong>f(n)=f(1)+f(n-1)</strong></p><p>f(n-1)因为绕过了UI，它的难度一下子降下来了。而f(1) 我们上一讲单独讲到了UI测试，可以用单元测试的方法降低UI测试的复杂度。所以，分而治之后，两部分难度都降下来了。皮下测试的价值也就出来了。</p><p>这里插一句提示，动态规划你不了解的话，可以通过后面<a href=\"https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/529408\">这个资料</a>了解一下。</p><p>我们说回正题，有哪些系统适合做皮下测试呢？ 刚才说到，自动化在UI层做不了，或者能做但是成本很大。比如一些带有画图功能的应用，在UI上有很多的曲线。像下面这样的，就适合皮下测试。</p><p><img src=\"https://static001.geekbang.org/resource/image/6c/89/6cfb23c331f5403bf4c1f2b559bac389.jpg?wh=1920x1138\" alt=\"图片\"></p><p>当然，皮下测试还有另外一个问题，就是这个离UI最近的这个“皮层”到底在哪里？</p><p>要找到这样一个合适的截面，需要结合你的项目而定。这个截面可能在网间请求，也可能就在客户端内部。但是原则就是，<strong>这个皮层离UI越近越好，能测试的逻辑最多越好，而且自动化实施的ROI越大越好</strong>。</p><h3>端到端测什么？</h3><p>皮下测试是一个准端到端的测试，本着3KU的原则，业务逻辑在单元测试已经验证过了，接口在集成测试也测过了，在端到端测试，我们的策略可以是 “Trust But Verify”，就是说信任前面阶段做的测试工作，但也要做一些基本的验证的工作。因此在策略上，端到端测试不会把全部的案例都走一遍。</p><p>那现在的端到端测什么？我们可以挑出一些测试案例，形成Workflow，作为端到端的测试案例。这个Workflow的选取原则可以参照集成测试的Happy Path。也就是说一个Workflow能够走过尽可能多的服务。</p><p>对FoodCome来说，客户下单-&gt;验证支付-&gt;餐馆接单-&gt;发送物流-&gt;通知客户，就是满足这样条件的一个Workflow。</p><h2>小结</h2><p>这一讲我们谈到了验收测试和端到端测试，这两种测试在业界经常被混在一起。还是那句话，如果它们是一回事，我们就没必要保留多余的概念。</p><p>经过我们的分析，实际上，这两者从测试角度和测试范围还是不一样的。验收测试是以客户视角，来验证是否按照契约交付，在这里我们用了Gherkins来表达契约，用Cucumber来生成测试代码，验收测试的范围是严格按照契约的内容来测试的。</p><p>对于端到端测试，我们依据3KU原则，提出了皮下测试的概念和实现方法，通过分析，这能够带来更高的ROI。</p><p>到这里，我们第二模块就结束了，我们把测试里的概念和策略都过了一遍，每种策略是什么样的，它在整体起到的什么作用。</p><p>你也可以看到，在3KU原则下，分层这个概念非常灵活：有的需求是在这个层测试，有的需求在那个层测试；甚至一个需求的一部分在这个层做，另外的部分在那个层做，只要ROI最高。</p><p>这就给自动化测试设计提出了挑战，原有的各层的测试方法就需要连接、兼容、打通，怎么做到呢？我们在第三模块设计篇，马上会引出微Job测试模型来解决这个问题。我们下一讲见吧。</p><h2><strong>思考题</strong></h2><p>UI测试、端到端测试还有验收测试的区别是什么？</p><p>欢迎你在留言区跟我交流互动，也推荐你把今天的内容分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"15｜UI测试：如何让UI测试更轻快便捷？","id":510620},"right":{"article_title":"17｜元数据模型（一）：小Job模型构建大蓝图","id":512680}}},{"article_id":512680,"article_title":"17｜元数据模型（一）：小Job模型构建大蓝图","article_content":"<p>你好，我是柳胜。</p><p>在价值篇我们学习了3KU整体最优分层模型。而到了策略篇，又结合一个具体的订餐系统，为你提供了一个测试设计实现案例，演示具体如何应用3KU法则。</p><p>不过推演下来，你有没有隐约感觉到，事情有点不对劲了？以前我们是先分层，再在每个层上设计自己的TestCase，UI上的TestCase的功能定义和粒度大小，可能和API层上的TestCase完全不一样。</p><p>现在，不同的测试需求，可能会落到不同的分层截面上去做自动化。甚至，一个测试需求的不同部分，也可以放在不同截面上。比如，你可以在API上测试createOrder，在UI上测试verifyOrder，只要这样做ROI最高。</p><p>所以，在3KU法则下，自动化测试设计是先定义TestCase，然后再判断它应该落在哪个层面。这跟我们以往熟悉的自动化测试设计大相径庭，需要我们在设计方法论上有新的突破。没错，现在我们的挑战是，需要找到一种新的概念指导我们做任务建模和设计，从而跨越各层的“TestCase”。</p><p>因为内容比较多，为了让你跟得上，我安排了两讲内容，带你<strong>找到一个测试设计的元数据模型，将各种类型和层次的测试纳入到这同一个模型里。一生二，二生三，三生万物，从这一个模型里，衍生出我们自动化测试中所需要的设计方法和策略、执行计划，效益度量甚至生死定夺等等一切</strong>。</p><!-- [[[read_end]]] --><p>“好大的口气”，看到这里，我猜你会这么说，不过，你不妨耐心看完后面的内容，相信会改变你以往的认知，换一个新的角度来审视自动化测试，能抓住最关键的东西做好设计。</p><h2>TestCase的设计态和运行态</h2><p>设计新模型前，我们有必要先分析一下，原有自动化测试设计有什么问题。我们可以从设计态和运行态复盘一下，自动化测试的产生过程是怎样的，借此发现问题。这里，我们延续之前那个订餐系统的例子讲解。</p><p>先看<strong>设计态。</strong>设计态下，自动化测试表现为测试案例，一个测试集合TestSuite包含了多个测试案例TestCase。TestCase测试案例里的信息包含以下元数据：测试名字、测试环境、测试步骤、测试结果等等。</p><p>对于订餐系统里“订餐”这个测试案例，就会这么写。</p><ul>\n<li>测试名字：下单</li>\n<li>测试环境：Web终端</li>\n<li>测试步骤：第一步，登陆订餐系统；第二步，选择餐品、收货地址；第三步，下订单</li>\n<li>预期结果：下单成功，产生物流配送记录</li>\n</ul><p>然后，我们对应到运行态。在<strong>运行态</strong>下，也就是自动化测试跑起来，就成为了一个运行的程序功能。这个程序功能有自己的运行环境、对外依赖和数据交互，我们可以用微服务的六边形架构方法（参看<a href=\"https://time.geekbang.org/column/article/502863\">第七讲</a>）描述订餐Job的运行态结构。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/69/df80f398d588e6084b23445934d4db69.jpg?wh=1920x1285\" alt=\"图片\" title=\"自动化测试案例：六边形模型架构\"></p><p>看到了没有，运行态的Job相比设计态的测试案例，多了不少信息，包括输入Inbound、输出Outbound、DAO，我们依次来看看。</p><ul>\n<li><strong>Inbound</strong>：本Job运行开始之前，接收的外部输入Input。</li>\n<li><strong>Outbound</strong>：本Job运行结束之后，对外的输出Output。用户在订餐系统完成下单后，会产生一个物流单号，比如123456。</li>\n<li><strong>DAO：</strong>Data Access Object,  本Job运行中需要持久化的信息。本次订餐测试运行结果的序列化，存储到数据库或文件里。</li>\n</ul><h2>原本的设计存在什么问题？</h2><p>分析完自动化测试的设计态和运行态长什么样，从内部和外部两个角度，我们很容易发现问题。</p><p>从内部看，传统的测试案例里的元素和自动化测试Job里的信息差别较大。用咱们现在流行的“元”概念来说，就是两者的元数据不一致。在基因上，它们是两个物种。</p><p>从外部看，传统测试案例之间关系比较松散，即使刚开始测试人员遵循规则来产生案例，但时间长了，也很难维护案例设计逻辑的一致性和完整性。而自动化测试Job从一开始就互相咬合，在生命周期中，遵循软件扩展和持续重构的规律：腐化或是优化。</p><p>这些问题导致了，自动化测试从设计态转化成运行态困难重重。反映到现实中，我们看到就是，测试案例设计和自动化测试实现脱节，貌合神离、各玩各的。造成的结果也很明显，就是自动化测试缺失了设计。代码一旦没有了设计，它的功能、效率、维护性、投产比都失去了追踪和控制。</p><p>那怎么解决这些问题呢？咱们开个脑洞，从软件设计的角度，重新审视一下自动化测试应该怎么做。</p><p>面向对象设计里，有Interface和Class概念，Interface定义行为表现，Class负责具体实现，Abstract Class提供代码重用。一个Interface，可以有多个实现；如果修改了Interface，那么Class也需要修改。</p><p>设计和实现解耦，但又保持一致，这个机制使得代码有内建生命力，有扩展能力而不会垮掉，对不对？</p><p>其实,我们可以把这个设计思路引入到测试模型里，测试案例设计就相当于定义Interface，暴露契约，要提供什么样的功能、达成什么目标，但是Interface不能实例化，更不能运行。</p><p>自动化测试Job就是实现了Interface的Class，有血有肉，能实例化也能跑。对一个测试案例，可以有不同的自动化实现，手工测试也相当于是测试案例的另外一个实现。如何定义这个Interface，就是我们要找的测试元数据模型。</p><p>好，说到这里，我们简单总结一下到现在的观点。</p><p>1.自动化测试和手工测试共用一套案例模型和设计方法。TestSuite和TestCase这些概念已经过时了，需要找到新的模型。</p><p>2.自动化测试设计本质是软件设计，精髓在于对测试场景的抽象和建模。这就像开发软件先设计Interface一样，而自动化测试工具和框架属于实现层面，用哪个取决于需求，而不能削足适履。</p><h2><strong>初探微测试Job模型</strong></h2><p>有了前面的推导，我们现在试图找到一个大一统的测试元数据模型，用它描述一个测试案例的设计态和运行态。为了区别于传统的测试案例，还有自动化测试脚本的叫法，我们管它叫<strong>自动化测试Job</strong>。</p><p>回顾一下运行态里的六边形，其实它已经初步勾勒出一个Job模型，但它还是像一个开发的通用模块，现在我们继续丰富它，<strong>添加更多的自动化测试基因，这个基因满足自动化测试的基本原则</strong>（你可以回顾<a href=\"https://time.geekbang.org/column/article/496857\">第一讲</a>），让它成为一个针对自动化测试的Job。</p><p>为了对齐自动化测试的术语习惯，我们把Inbound重命名为Input，把Outbound重命名为Output。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/eb/e92bdfde99ffc5e5ca03c81f8f1214eb.jpg?wh=1920x1285\" alt=\"图片\" title=\"自动化测试Job六边形模型架构\"></p><h3>模型属性</h3><p>先说结论，我们的自动化测试Job模型，除了上面的Input、Ouput和DAO 三个属性，下面还要加上Dependency、TestData、TestConfig、Document四个属性，一共七个属性。</p><p>这七个核心属性，其实是你在设计自动化测试案例时，需要考虑的七个方面，也是后续开发中要去实现的Interface（接口）。</p><p>这么多属性“扑面而来”，你也许有点应接不暇。别慌，下面我结合订餐系统例子，逐一给你解释。</p><p>对于自动化测试，我们需要创建一个依赖的概念，目的是通过阻断错误在Job链条上的传递、扩散，来缩短自动化测试的执行时间。Depedency的Job如果失败了，那么后续的Job没有必要再去运行。因此，我们要在后面的模型示意图中，用一个红色菱形描述这个<strong>Dependency。</strong>它的数据表达类型是List，存放1个或n个前置Job的名字。</p><p>结合具体例子，Depdency的使用场景更好理解。在下图，我们有2个Job，一个是登陆的Job，另外一个是下单的Job，让下单Job的前置依赖是登陆Job，登录成功后才要运行订餐案例，登陆失败则不必运行订餐案例。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/66/d7d533dee50603f28b6a7d4a59f38166.jpg?wh=1920x702\" alt=\"图片\" title=\"自动化测试Job模型Dependency\"></p><p>除了可以引入Dependency概念，帮我们缩短测试执行时间，要想提升测试ROI，还有什么着手点呢？</p><p>早在<a href=\"https://time.geekbang.org/column/article/499382\">第四讲</a>我们就说过，要提高自动化测试的ROI，一个方法就是增加自动化测试的运行次数。所以，从设计的角度来看，我们的期望是：一份代码，运行多次。这里的“多次”可以是多个场景、多个语言、多个客户端、多组数据，不管哪种，都是数据驱动的循环迭代。</p><p>所以，每一个自动化测试Job应该有一个自己的<strong>数据源TestData</strong>，它的数据表达类型是Hashmap，存放相似特征的N条KV（Key Value）数据记录，每一条KV记录触发一次Job运行。</p><p>看到这里，可能你会产生这样的疑问，在TestNG和Junit框架里，都有DataProvider机制，不就有了你说的TestData功能么？是，也不是，我们这里谈的是自动化测试Job的特征提取和建模，各个框架的DataProvider也好，DataValue也好，都是对模型的实现。</p><p>很好，我们又完成了一个TestData机制，让1份代码的产出效益翻n倍。</p><h2>课程小结</h2><p>为了设计一个更科学的模型<strong>，</strong>我们从传统自动化测试设计入手，通过传统自动化测试的设计态和运行态对比，看到了设计与实现之间的鸿沟。类似的鸿沟举不胜举，比如手工测试与自动化测试之间，测试分层之间。</p><p>从测试整体看，我们投入了这么多资源和精力，初心是编一张大网来捕捉bug的，但实际上却是各个小网工作，有重复、有遗漏，而这些问题不能度量，也无法提升。</p><p>为了编好大网，我们找到了一个统一的Job模型，用这个模型来设计我们各种类型各个层次的自动化测试，那我们不仅能明确大网有没有重复和遗漏，甚至网眼密度也可以统一。</p><p>这个Job模型包含七个属性，你在设计自动化测试的时候，需要把这七个属性想清楚。今天我们讲到了其中的5个属性，Input、Output、DAO、Dependency和TestData。</p><p>Input和Output要声明测试Job输入和输出是什么；DAO要解决自动化测试报告和Log的格式和持久化怎么做的问题；Dependency要回答的问题是，测试Job依赖的前置条件是什么，谁来提供？而TestData要回答的问题是，测试Job需要的测试数据是什么结构，有多少组？</p><p><img src=\"https://static001.geekbang.org/resource/image/3d/10/3da443024e8facbd7d7a6b160c16a710.jpg?wh=1920x1003\" alt=\"图片\" title=\"模型属性速记图\"></p><p>在下一讲，我们会介绍剩下两个属性，并继续分析自动化测试设计中你必须关注的其他问题。比如，怎么保证自动化测试健壮性，你可以自己先想一下，我们下一讲一起探讨。</p><h2>思考题</h2><p>说一下你设计自动化测试任务的时候，有哪些比较好的实践，怎么在团队里推行的？</p><p>欢迎你在留言区跟我交流讨论，也推荐你把这一讲分享给更多朋友。</p>","neighbors":{"left":{"article_title":"16｜概念重识：如何用3KU为端到端&验收测试赋能？","id":511982},"right":{"article_title":"18｜元数据模型（二）：小Job模型构建大蓝图","id":513617}}},{"article_id":513617,"article_title":"18｜元数据模型（二）：小Job模型构建大蓝图","article_content":"<p>你好，我是柳胜。</p><p>上一讲，我们分析了传统自动化测试设计态和运行态存在的鸿沟，并且提出了一个更科学的Job模型的设想。如下图所示，这个模型包含七个核心属性，其实也是后续在设计自动化测试案例时，我们需要考虑的七个方面，也是后续开发中要去实现的Interface（接口）。</p><p>模型属性我们上一讲开了个头，重点讲了Dependency和TestData，Dependency描述业务关联性、阻断错误、缩短执行时间，TestData可以实现一份代码多组数据运行，提升自动化测试的ROI。</p><p>今天咱们继续分析剩下的模型属性，勾勒整个Job模型的全貌，这样你就能进一步掌握自动化测试设计建模的利器了。</p><p><img src=\"https://static001.geekbang.org/resource/image/b7/59/b7b0596e823yya466ff87373b205e359.jpg?wh=1920x1310\" alt=\"图片\"></p><h2>模型属性</h2><p>自动化测试有一个令人头疼的问题——不稳定，经常失败，有没有办法通过设计攻克这个难题呢？</p><p>自动化测试Job像开发的微服务一样，都是独立的运行单元，我们可以通过给每个Job增加一个<strong>TestConfig</strong>，它的数据表达类型是HashMap，每一条记录代表一个配置，我们通过修改配置来控制Job的运行。结合实践，<strong>有三个关键配置，可以增强自动化测试的健壮性和诊断性，分别是：日志级别、超时时间以及重试次数</strong>。</p><p>Log是诊断程序运行问题最有力的工具，自动化测试也不例外，通过定义不同的log级别和相应输出信息，把它应用在不同环境下，可以形成一个自动化测试诊断策略。比如，Log level 如果是debug级别，抓取环境信息、屏幕截图、运行时堆栈，用于在自动化测试开发阶段做调试。Error级别记录出错trace和调用Job链状态，用在自动化测试生产运行环境。</p><!-- [[[read_end]]] --><p>我建议你在设计自动化测试Job的时候，一定要在TestConfig里加上<strong>Log_Level</strong>，用来提醒测试开发人员，在自动化开发中实现相应的Log信息的收集、分类和输出机制。</p><p>我们再看看超时问题，在自动化测试运行中，经常会因为各种奇怪的原因，导致运行不能返回，单元测试中阻塞调用不能返回，API测试中Http response超时，UI测试中页面刷不出来。</p><p>通常在技术层上不会无限等待，也有TimeOut的限制，比如http request的默认超时是120秒，我们这里关注的Timeout是执行整个Job的时间最大限值，从而保证自动化测试可以满足快速反馈的目标。</p><p>所以，我们应该在TestConfig里有一个Job的超时配置，<strong>TimeOut</strong>: 3min,  提醒自动化测试开发人员去做相应的代码实现。</p><p>定义了超时处理机制后，我们再来看看Retry重试，这是开发人员提高代码运行健壮性的常用手段，其实对于提高自动化测试的健壮性也很关键。</p><p>咱们在UI测试里经常遇到一些状况，比如在低环境里环境不稳定，有的页面刷一次出不来，第二次就没问题了。手工测试时测试人员通常会自己多刷两次，把功能测试完，这种不稳定性的问题，只要在生产环境里不重现即可。</p><p>那自动化程序怎么处理这种状况呢？我们可以加入一个<strong>Retry_Number</strong>的配置项，它的value是N。是什么意思呢？Job运行1次不成功，还可以尝试最多N-1次直到成功，或在N次失败后才返回失败状态。</p><p>在<strong>TestConfig</strong>中定义Log_Level、 TimeOut、 Retry_Number这些配置项，实际上是我们针对自动化测试的运行不稳定问题而设计的解决方案。TestConfig本身是一个HashMap，当然也可以加入其它你认为有用的配置项。至于怎么实现，那是代码阶段要去做的事情。</p><p>说完TestConfig后，别忘了我们还需要一个属性<strong>Document</strong>，来存放Job的描述信息。它的数据表达类型是一个对象，Document里可以存放Job的名字、测试执行步骤、测试环境信息等等。</p><p>到这里，我们的Job模型六边形，最终更新成了下面的样子。</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/f7/e6de0d6d56ea556f8584de2353e51ff7.jpg?wh=1920x1310\" alt=\"图片\" title=\"自动化测试Job七属性模型\"></p><p>讲到这里，你可能又有一个疑问：不对啊，之前学自动化测试都是在学框架，最重要的框架是不是忘了放进自动化测试Job模型里？</p><p>不，这正是我要强调的，在自动化测试Job模型里，我们关注的是设计，是如何满足测试需求和提高自动化的效益。而工具或框架，它是Job实现层面所关注的内容。</p><p>我们用Selenium也好，QTP也好，它们可以帮助我们实现设计意图，但不能喧宾夺主，倒逼我们按照它们的结构来设计，否则我们永远是一个工具使用者，而不能成为一个有设计思维的自动化测试人员。</p><p>这就相当于软件开发中，先定义了微服务实现业务接口，至于每个微服务选择哪种语言作为编程语言，选GO还是Java，这要交给开发人员根据经验和喜好权衡。</p><p>所以说，在Job模型里，七大属性是设计面的，而框架是Job实现面的。为了更好地理解，我们把刚才的六边形架构展成UML类关系图，就会看到框架在垂直下方。</p><p><img src=\"https://static001.geekbang.org/resource/image/83/4c/8388717b5b84d648073c0ff32ab1204c.jpg?wh=4435x2260\" alt=\"\" title=\"Job模型UML对象关系图\"></p><h2>模型层级</h2><p>听到这里，你也许对Job模型有一点点明白了，但困惑也不少。以前接触的都是TestSuite和TestCase这些概念，这个Job模型到底说的是啥？是一个TestCase还是一个大场景？</p><p>我们暂且忘记TestSuite和TestCase，你先想一下软件设计是怎么做的：一个系统的行为设计是为了解决一个大问题，把这个大问题分解成若干个小问题，对应着系统分解成服务，依次递归下钻一直到单元级别。大问题与小问题同构，系统与服务同构，这叫自顶向下的设计。</p><p>讲到这，是不是顿时开悟了？Job模型描述的是一个同构的测试任务，它既可以是一个小小的测试案例，也可以是一个大大的测试需求。这小和大之间，是通过组合编排来完成的。</p><p>Job之间到底是如何组合的？一生二，二生三，三生万物，这描述的是一个树形关系，软件架构如此，自动化测试Job也一样。</p><p>一个Job的定义是由父节点完成，Job的实现由它的子Job们完成。我们可以类比开发的Interface、abstract class和final class，理解根节点和子节点的用处。</p><p><img src=\"https://static001.geekbang.org/resource/image/c6/86/c6686eb4eaa6aeebb6237dbcf9912186.jpg?wh=1920x951\" alt=\"图片\" title=\"Job模型层级图\"></p><p>图中的根Job，是我们运行的自动化测试的任务，对应着interface。我们需要按照Job模型设计它的Input、Output、TestData、TestConfig等信息；叶子Job，这是已经实例化可运行的Job，相当于final class，也就是说一定有代码来负责这个Job的执行；在根Job与叶子Job之间的Job相当于abstract class，起到功能和接口划分的作用。</p><p>我们最终是要跑Job的，在Job跑起来的时候，会通过这个树形链接传递一些信息，我们定义以下四条规则。</p><p>1.一个Job的运行，实际上是运行其所有的子Job，并且Input和Output上也等价。也就是说，Job1 = Job11+Job12。 Job11+Job22是实现了Job1的定义。</p><p>2.Job的运行结果是其子Job们运行结果的与运算。这个很好理解，Job11和Job12全部成功，才能设定Job1成功，有一个失败，Job1就会失败。</p><p>3.子Job默认继承父Job的属性，除非自定义了属性去覆盖父Job上的同名属性。像TestConfig，比如父Job上已经定义了log级别为error，那么对其下子Job也适用，除非在子Job里定义不同的log级别。</p><p>4.一个Job的Depdency只能指向和它同一层次的Job。Job113可以一来Job112，但是不能依赖Job12。</p><p>这个层次模型，可以用自顶向下来去设计，也可以自底向上去组合。所以，根Job是相对的，不是绝对的，它完全可以和另外的Job组合在一起，生成一个更大的Job。</p><p>比如IT Ops开发的产品部署监控自动化Job对他们来说是终极任务，但是也可以和QA团队的测试自动化Job组合在一起，进行环境信息对接，形成一个部署测试一体自动化的Job，这就是我们常用的Pipeline。Pipeline本身就是一个有Input和Output的Job，Pipeline之间可以再对接组合成更多的场景。</p><h2>Job模型设计的四大优势</h2><p>好，到这里，我们已经把Job模型讲完了。明确了这个模型的设计之后，我们不妨进一步思考一下，Job模型是怎么帮助我们做好自动化测试的呢？我们可以从这四个方面来理解。</p><p><strong>第一，高可复用的自动化测试模块化设计。</strong>像软件设计逐层分解系统、子系统、服务和模块一样，自动化测试可以自顶向下分解一个大的任务，到中任务，再到小任务，直到可执行的代码级别；也可以自底向上聚合成新的测试场景，比如API测试可以和UI测试聚合生成跨端测试。</p><p><strong>第二，低成本的扩展和重构。</strong>Job任务的定义是基于自动化测试的需求，并不预先依赖于某个工具框架的实现，我们重构Job也更轻松了。更改一个Job里的实现代码，甚至更换工具从QTP切到Selenium，只要保持对外的Input和Output不变，那这就属于内部的事，不会影响到其他关联Job。这就叫高内聚，低耦合。</p><p><strong>第三，动态的生成执行计划。</strong>在Job执行的时候，根据我们定义的运行规则，就可以把树形图展成可执行的有向无环图，获得关键路径、关键Job，从而动态确定最优执行计划。</p><p><strong>第四，有助于后续的度量和改进。</strong>有了统一模型来做案例设计和自动化测试实现，我们就可以获得准确的度量数据了，比如自动化测试覆盖率、自动化测试ROI等等，可以持续驱动自动化测试效益的提升。</p><h2>课程小结</h2><p>我们通过两讲，一起推导建立了微测试Job模型。我这里再总结一下，Job模型包括属性和层级两个方面。</p><p>七个属性，其实是回答了自动化测试里，七个可以提高ROI的关键问题：</p><ul>\n<li>Input&amp;Output：测试Job的输入和输出是什么？</li>\n<li>DAO：测试Job应该用什么格式，怎么持久化自动化测试报告、日志、抓图等等。</li>\n<li>Depdedncy：测试Job的前置条件是什么，由谁来提供？</li>\n<li>TestData：测试Job的测试数据是什么结构，需要多少组？</li>\n<li>TestConfig：测试Job的运行时需不需要通过配置来调节？比如，健壮性、诊断性、环境信息等等。</li>\n<li>Document：测试Job的其它信息。</li>\n</ul><p>我准备了图解，方便你记忆：</p><p><img src=\"https://static001.geekbang.org/resource/image/f2/e3/f2481327e3d0dea9cdfcyy80fyy3a9e3.png?wh=1792x1142\" alt=\"图片\"></p><p>当你想清楚了这七大属性，就相当于Job的接口已经确定了。</p><p>但刚开始做自动化测试设计的时候，也是先想到一个大概要完成的任务，不可能一下子就到脚本实现的级别。就像软件设计，也是先有概要设计，再做详细设计。</p><p>微测试Job怎么赋能自动化测试开发人员做设计呢？那就是Job的层级能力，Job下面可以有子Job，一层层做细化，直到叶子节点Job，是可开发的自动化测试案例，至于怎么实现，那是自动化测试开发阶段需要考虑的事情。</p><p>你可能会问，这个Job模型真的好用么？可以用来做设计么？后面的课程里，我还会用几个实例，为你演示一下，怎么用Job模型来完成自动化测试的设计，敬请期待。</p><h2>练习题</h2><p>请用测试微Job七要素模型，描述一下你目前正在使用的测试案例。注意，不要代入工具和框架。</p><p>欢迎你在留言区跟我交流讨论，也推荐你把这一讲分享给更多朋友。</p>","neighbors":{"left":{"article_title":"17｜元数据模型（一）：小Job模型构建大蓝图","id":512680},"right":{"article_title":"19｜排兵布阵：自动化测试的编排、扩展与重构","id":514501}}},{"article_id":514501,"article_title":"19｜排兵布阵：自动化测试的编排、扩展与重构","article_content":"<p>你好，我是柳胜。</p><p>通过前面两讲，我们一起推导了一个自动化测试Job元数据模型。接下来，我不但会把这个Job模型用到自动化测试设计里，还要“点石成金”，让你充分体会到这个模型的强大力量。</p><p>在讲解设计过程之前，我想先和你聊聊，现在的自动化测试设计有什么问题。</p><p>如果你做过自动化测试开发，对现有设计一定不陌生。通常做法是创建一个TestSuite，它包含多个TestCase，每个TestCase完成一个测试任务，然后顺序跑下来，测试就执行完了。就像下面这样。</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/5f/c5c1d2c97ea26bb8a1d07724aa5b515f.jpg?wh=1920x962\" alt=\"图片\" title=\"传统做法\"></p><p>我相信，很多自动化测试开发人员刚入行的时候，就是这么学习和组织自动化测试案例的，一直到现在。</p><p>但你有没有想过，这种测试案例的组织方法是来源于单元测试。它的方法论是，有一个开发方法，就需要有一个对应的Test方法，两者一一对应。因为先有开发的代码存在，那么Test方法有多少个，都负责干什么事，这些都一眼到底，几乎不需要测试设计，往Test方法里填充代码就可以了。</p><p>这种自动化测试开发方法，我叫它<strong>“轻设计，重实现”</strong>的方法。不过，这种方法反过来也会潜移默化地影响自动化测试人员的思维方式。不想好自动化测试的设计，上来就先写代码，结果代码写出来又长又冗余。</p><p>而自动化测试已经发展了这么多年，早就不再局限于开发阶段的单元测试了，有API测试、UI测试等等。这些高层面的自动化测试案例，其设计往往以功能为入手点，对业务进行分解，进行自动化建模。在这个过程中，就不能像单元测试那样，有现成的开发方法做对标和参考了。</p><!-- [[[read_end]]] --><p>而在这样的背景下，自动化测试案例的设计，逐渐成长为一个非常重要的环节。所以，我们应该要走向“<strong>重设计，轻实现</strong>”的自动化测试开发方法。</p><p>所谓<strong>重设计</strong>，其实就是我们要像开发人员一样，去设计自动化测试案例，要考虑后面这三大关键问题：</p><p>1.概要设计。要写多少个TestJob，它们之间关系是什么？<br>\n2.详细设计。每个TestJob该怎么实现？<br>\n3.需要扩展和重构怎么办？</p><h2>概要设计：要写多少个Test Job？</h2><p>我们先从概要设计开始梳理。一说概要设计，你是不是马上想到的问题就是：要写多少个Test？</p><p>但其实这个问题无法直接解答。因为刚开始规划一个软件系统的自动化测试，我们只知道一个大概的场景需求，并不会一下子就知道，需要开发多少个Test Job。</p><p>比如下单功能，它是从Web UI下单，然后用户通过快递得到食物。为了去测试它，需要多少个Test Job？做多少个检查点？这些问题，需要从概要设计中梳理出答案。</p><p>那概要设计怎么做？一步步来。</p><p>第一步，我们先把大概的自动化测试需求梳理出来，建模成为测试Job，这里我们把测试Job模型再“召唤出来”，一一去对号入座。</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/f7/e6de0d6d56ea556f8584de2353e51ff7.jpg?wh=1920x1310\" alt=\"图片\" title=\"测试Job模型\"></p><h3>先完成第一个根Job</h3><p>Job的名字是“UserPlaceOrder”。Job的Input是什么呢？要想启动Job，需要有一个实例的URL，还有登录所需的User Name和Password。</p><p>接着往下分析，Job的Output是什么呢？用户得到食物。那用户得到食物的标准是什么？因为物流是Foodcome的第三方服务，FoodCome处理完食物的标准是：生成物流单号，并且用户收到了“食物快递发出”通知短信。那我们UserPlaceOrder的Job Output就是DeliveryTicketNo和DeliveryNotificationMessage两个信息。</p><p>Job的TestConfig是什么呢？这里我们需要考虑的一个Job参数就是Timeout。</p><p>从自动化测试的执行效率角度看，每一个Test Job都应该承诺在一定时间内完成，这样自动化测试的运行时间才能可控可预期。另外，从业务角度，每个业务都有<a href=\"https://zh.wikipedia.org/wiki/%E6%9C%8D%E5%8A%A1%E7%BA%A7%E5%88%AB%E5%8D%8F%E8%AE%AE\">SLA</a>的时间要求。</p><p>设想一下，我们对客户承诺，处理一个订餐的单子最长时间是30分钟，那么对应在Test Job的实现中，30分钟后，如果还没有输出DeliveryTicketNo和DeliveryNotificationMessage这两个信息，客户的承诺没有达到，Job应该就会失败。所以，我们在TestConfig里设定Timeout=30mins。</p><p>现在，UserPlaceOrder的Job我们就梳理出来了，画成六边形是这个样子：</p><p><img src=\"https://static001.geekbang.org/resource/image/99/53/999031947e107324d4cbb617e3bedb53.jpg?wh=1920x728\" alt=\"图片\"></p><p>它的XML表达是这样一个文档：</p><pre><code class=\"language-xml\">&lt;TestJob name=\"UserPlaceOrder\" description=\"创建订单\"&gt;\n  &lt;Input&gt;LoginURL&lt;/Input&gt;\n  &lt;Input&gt;UserName&lt;/Input&gt;\n  &lt;Input&gt;Password&lt;/Input&gt;\n  &lt;Output&gt;DeliveryTicketNo&lt;/Output&gt;\n  &lt;Output&gt;DeliveryNotifyMsg&lt;/Output&gt;\n  &lt;TestConfig&gt;Timeout:30mins&lt;/TestConfig&gt;\n&lt;/TestJob&gt;\n</code></pre><p>这个Job写出来后，你就完成了UserPlaceOrder这个Job的概要设计，也相当于开发完成了一个小型系统的概要设计。</p><p>对于开发来说，系统用一个模块就能完成，还是由多个微服务共同完成，这取决于业务复杂程度和实现能力。</p><p>那对于自动化测试设计来说，这个UserPlaceOrder是直接就可以开发了，还是需要继续细化成子Job，怎么决策呢？我们可以从<strong>复用性</strong>和<strong>实现能力</strong>两个角度，来决定是否拆分。</p><h3>根据复用性进行拆分</h3><p>先从复用的角度来看，登录是所有功能的前置条件，如果我们把登录测试任务提炼出来，是可以提高ROI的。按照这个思路，我们把UserPlaceOrder Job拆分成Login和PlaceOrder两个Job，像这样。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/3a/e205cc2ff8d14d448a9a5eb40e25d33a.jpg?wh=1920x938\" alt=\"图片\"></p><h3>根据实现能力拆分</h3><p>再琢磨一下，PlaceOrder这个Job包含了创建订单和验证订单两块逻辑。创建订单从Web UI上完成，而快递单号需要从数据库里查到，但是短信要从短信网关上得到，所以DeliveryTicketNo和DeliveryNotifMsg是两个技术上独立的实现的方法，它们应该被分割成两个Job，每个实现用不同的技术，这也遵循了<strong>面向对象设计单一职责的SRP原则</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/25/c10b0f0e81ec2fb8da9ebbyy0b4f7f25.jpg?wh=1920x1425\" alt=\"图片\"></p><p>在分解Job的时候，子Job要实现父Job的对外承诺，也就是Input和Output。而且在运行的时候，一个父Job的所有子Job都太运行通过，父Job才能通过。</p><p>这就是我们要创建一个VerifyOrder抽象Job的原因。因为它下面的2个子Job，即快递单号和通知短信都要成功，verifyOrder才算成功。</p><p>同样的道理，createOrder和verifyOrder都要成功，它们的父Job placeOrder才算成功。这样，我们的Job设计就咬合在一起了。</p><p>讲到这里，我们再回顾开头那个问题：要写多少个Test Job。是不是就有答案了？</p><p>作为测试架构师，或者自动化测试设计人员，你的首要任务是完成抽象Job的设计，把Input、Output、Testconfig、Testdata先定义好。之后要继续抽象再做拆分，还是按现在定义的Job实现接口，这个要交给你的自动化测试团队来决定。你其实不需要过多关心他们是怎么实现的，又用了多少个Job。</p><p>但是我们要注意，<strong> Job的拆分是有一个度，太大了，将来维护和诊断就会很费力气</strong>。你想想诊断一个1000行的代码模块和一个100代码模块，哪个更容易，就能明白了。拆分得太细了，就会让复杂度升高。</p><p>拆不拆分，你要遵循这两大原则：第一，提高复用性，也就是ROI；第二，方便独立实现和维护。</p><h2>详细设计：Test Job怎么实现？</h2><p>可能你注意到了，设计Job的过程，在思维上是一个笼统的想法逐渐细化的过程，反映在Job模型上，就是从一个Job生长成一棵Job树的过程。</p><p>你可以看看后面这张图，它能形象地描述这个过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/34/4b/34494bb8f326ca0625c5f5661yyeda4b.jpg?wh=1920x954\" alt=\"图片\"></p><p>设计完成后，这个Job树的每一个叶子，就是我们概要设计的成果，它是我们最终细化到可开发的实例Job，也就是传统的Test Case了。</p><p>这些实例Job，我们还要决定怎么来实现，放在金字塔的哪个层面做测试，具体用什么工具框架，详细设计做完之后，就能进入具体实现环节了。</p><p>按照第一模块讲的3KU法则（可以回顾<a href=\"https://time.geekbang.org/column/article/497405\">第二讲</a>），我们就可以确定每个Job要在金字塔的哪一层来完成。确定了这一点，工具和框架自然也就可以确定了。</p><p>对于我们的订餐系统，Login放在API层面，使用RestAssure框架；CreateOrder在API层面，使用RestAssure框架；DeliveryTicketNoVerify在DB层面，使用JDBC；DeliveryNotifMsg在SMS gateway层面，使用Java或Python。</p><p>我在六边形下面加了一个紫色的方块，来代表实现方法。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/c4/1e5c03d07cf625f9e5fefa96deaaeac4.jpg?wh=1920x1291\" alt=\"图片\"></p><h3></h3><p>到这里，我们就可以进入到实现阶段了。自动化测试Job就是一个个具体的模块，有输入、有输出、有实现方法，足以指导自动化测试开发人员去开发代码了。</p><p>而负责Job开发的自动化测试开发人员，并不需要了解整个Job蓝图，只要他按照接口去做，就可以把实体Job组装到场景里。这个跟开发的思维是一样的，每个开发人员遵循详细设计开发一个个模块，最后整个系统就会运转起来，也就是<strong>模块化开发</strong>。</p><h2>扩展和重构</h2><p>设计做得好不好，还要考察扩展和重构的能力。所以，我们要看一下，Job模型是怎么支持扩展和重构的。</p><h3>扩展</h3><p>假设未来有一天，我们发现前面的设计方案有瑕疵：比如，这个工作流没有走UI层面，没有验证UI上的功能表现。那我们就想加一个verifyOrderOnWebUI来验证订单，那怎么办呢？</p><p>我们还是先梳理verifyOrderOnWebUI的Input和Output，它所需要的Input用户名、订单号这些信息，是否已经存在，又是由哪个Job 输出的。明确了这些，只需要把verifyOrderOnWebUI的Depdency指向它即可。其实意思就是，前面的Job运行完了，有了这些信息了，verifyOrderWebUI就可以运行。</p><p>在这个具体的场景里，我们把verifyOrderOnUI就作为verifyOrder下的一个子Job，可以复用verifyOrder的Depdency和Input。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/82/e07be8a9204f4b27d63cd3a8e7331a82.jpg?wh=1920x1019\" alt=\"图片\"></p><h3>重构</h3><p>跟软件开发一样，自动化测试也需要修改、重构。在传统的自动化测试代码里，没有TestJob这个概念，往往是一个工具下面带着一群TestCase代码。</p><p>TestCase之间的依赖和数据交互这些逻辑会存在隐式的耦合，一换工具，全部的Test Case都要重新写，即使重写其中一个Test Case，也要倍加小心，说不定就会影响到其它的TestCase。</p><p>而有了TestJob之后，每一个TestJob都有清楚的接口，可以有自己独立的实现方法。这也符合面向对象设计中的单一职责SRP原则和开放-封闭OCP原则。我们可以把修改的影响，控制在TestJob的范围内。</p><p><img src=\"https://static001.geekbang.org/resource/image/13/db/13c987dab72847032afcee3aa1460cdb.jpg?wh=1920x1528\" alt=\"图片\"></p><p>这个修改可以是代码的修改，也可以是工具的替换。比如，有一天我们发现verifyOrderOnUI的web测试工具Selenium不好用了，想替换成Nightwatch，怎么办？很简单，这个替换仅限于这个Job，所以用Nightwatch重写一遍，只要保证Input和Output不变，就是对外承诺不变，我们心里就有底了。</p><h2>小结</h2><p>为了让你理解透彻，我再用类比的方式，为你概括一下微测试Job理论：一个微测试Job，就好比开发里的一个微服务，它是有独立的接口，配置和实现方法。运行这个微Job就是完成了测试任务，它会有自己的状态，通过、失败还是无法运行；同时，它会输出自己获得的数据，供其他Job使用。</p><p>我为你准备了一张图，帮你快速回顾微Job模型的设计流程，你可以保存下来做个参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/f1/35/f1273a8e298e6753dddbca1341243735.jpg?wh=1920x1080\" alt=\"图片\"></p><p>在我看来，这个微Job模型是一种新的设计思维。它给我们带来好处是，让自动化测试有了<strong>设计先行</strong>的理论基础，而且能够分解复杂的测试场景，降低和工具框架的耦合。</p><p>但这也带来了新的问题，虽然方法论上站住了，但落地的时候会有挑战，因为Job会比以前切得更碎，Job模型转成自动化测试案例的时候，这最后一公里路应该怎么走？下一讲我们再继续。</p><h2><strong>思考题</strong></h2><p>你的手工测试案例是怎么设计的呢？想想能不能也用Job模型来设计呢？</p><p>欢迎你在留言区跟我交流讨论，也推荐你把这一讲分享给更多朋友。</p>","neighbors":{"left":{"article_title":"18｜元数据模型（二）：小Job模型构建大蓝图","id":513617},"right":{"article_title":"20｜链接工具：要驱动工具，不要被工具驱动","id":514818}}},{"article_id":514818,"article_title":"20｜链接工具：要驱动工具，不要被工具驱动","article_content":"<p>你好，我是柳胜。</p><p>在上一讲的微测试Job设计方法论里，我们把工具打入了“地牢”，但你可能还是对它念念不忘。毕竟设计开发的最后一公里路，Job模型还是要转换成一个具体工具的自动化测试案例。</p><p>战略上我们都不愿束缚思想，被特定工具牢牢“绑票”，而战术上又要用好工具。因此，如何驱动工具实现Job模型这个问题，我们就必须解决，要是做不到，我们就不得不走回老路，看着那些工具稳坐C位。</p><p>我们不想看到这种情况，所以这一讲，我们继续深挖后面这两个问题：</p><p>1.主流的工具框架能不能被驱动？<br>\n2.这些工具框架怎么和微测试Job模型对接，执行Test Job？</p><p>解决完这些问题，微测试Job模型就算是可以落地了。</p><h2>Job模型往哪里放</h2><p>要想让Job模型落地，我们首先要找准它应该落在哪个地方。尤其在业界，自动化测试工具和技术可谓层出不穷、眼花缭乱。我们势必要理清它们之间的关系是什么，才能知道Job模型应该放在哪里。</p><p>按功能效用，我把各种自动化测试技术划分成了三个层面：框架层、工具层和Library层。</p><p>先来看<strong>框架层</strong>，这一层负责自动化测试的设计。其实它主要回答了设计的三个问题：测什么、怎么运行、结果是什么。</p><p>问题相同，解法各异。不同的测试理念，最终催生了面向这三个问题的不同答案，比如TDD、BDD、ATDD。我画了一张表格，帮你更直观地对比它们：</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/d2/c9/d2a62a5c69599c65aac7d058bb443bc9.jpg?wh=1920x607\" alt=\"图片\"><br>\n第二个层面就是<strong>工具层</strong>，这一层负责自动化测试的实现，把测试任务转换成代码，用各种工具跟被测试对象打交道。</p><p>比如，跟Web对象打交道的工具有Selenium、QTP，跟Windows对象打交道的工具有WinAppDriver等，跟API打交道的工具有SoapUI和RestAssure等。所以，工具的能力主要体现在对象的识别和控制能力上。</p><p>第三是<strong>Library层</strong>，主要提供了自动化测试运行的支持库。比如Mock库，Assert库等等。</p><p>我把业界一些常用工具，按照三层来做了一个归类，如下图，你这样看会更清楚一些。</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/39/e68484363dc5754d1bfd7b2ef76ef739.jpg?wh=1920x785\" alt=\"图片\"></p><p>以TDD为例，这三个层面的调用链条是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/c1/d07f99ff02791f2521311ab5183925c1.jpg?wh=1920x868\" alt=\"图片\"></p><p>显然，Job模型应该要放在框架层。我给它起了个名字JDD，加在这个表格里。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/yc/cdbfca934a07120df99050d4ae9aeyyc.jpg?wh=1920x590\" alt=\"图片\"></p><p>与TDD对比，JDD的实现会是这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/8c/b1324f2c6e890d0949198d0607ff3a8c.jpg?wh=1920x868\" alt=\"图片\"></p><p>在这个分工图里，JDD和TDD起到一样的功能，首先遍历找出所有的测试案例，计算出执行路径，交给工具层去执行。</p><p>在TDD里，对于TestRunner这两件事都比较简单，测试案例都有注解，执行路径一个个顺序执行就可以了，但在JDD里，对于JobRunner，要找出Job树的所有叶子结点，根据依赖关系，计算出执行路径。这件事涉及到算法和实现。我们需要进一步理一下该怎么做。</p><h2>Job模型怎么实现</h2><p>我们在把自动化测试技术进行分层后，从设计到实现，现在各个环节的分工更加清楚了。但实际应用中，你会发现这个层面是模糊的。有的工具遵循了框架和工具分离，比如像Selenium，RestAssure这些工具，它们可以和Junit集成、也可以和TestNG集成，而有的工具比如QTP、Squish，直接提供了一揽子解决方案，从框架到工具，到检查点都是玩自己的一套。</p><p>如果你的团队正在使用多个工具，又想要落地JDD，那就要理清这些工具的接口层面，然后才能用JDD来驱动它们。</p><h3>JobRunner的实现</h3><p>以Junit为例，我们先看看，现在最常用的TestRunner是怎么实现的？</p><p>测试人员要在Test Class里给method上加@Test注解，表示这个方法就是一个TestCase。</p><pre><code class=\"language-java\">public class OrderTest{\n  @Test\n  public void testCreateOrder1(){\n    //test logic 1\n  }\n  @Test\n  public void testCreateOrder2(){\n    //test logic 2\n  }\n\n}\n</code></pre><p>TestRunner的处理逻辑很简单，就是通过Java Reflection从Test Class里获得所有加了@Test注解的方法，每一个方法就是一个TestCase，运行方法就是运行TestCase。</p><pre><code class=\"language-java\">public class TestRunner extends Runner {\n    private Class testClass;\n    public TestRunner(Class testClass) {\n        super();\n        this.testClass = testClass;\n    }\n    @Override\n    public Description getDescription() {\n        return Description\n          .createTestDescription(testClass, \"My runner description\");\n    }\n    @Override\n    public void run(RunNotifier notifier) {\n        try {\n            Object testObject = testClass.newInstance();\n            for (Method method : testClass.getMethods()) {\n            //判断方法是否加了@Test的注解\n                if (method.isAnnotationPresent(Test.class)) {\n                    notifier.fireTestStarted(Description\n                      .createTestDescription(testClass, method.getName()));\n                    //调用方法运行\n                    method.invoke(testObject);\n                    notifier.fireTestFinished(Description\n                      .createTestDescription(testClass, method.getName()));\n                }\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n</code></pre><p>而微测试Job模型设计出来的Job，用一个XML文件表达出来是这样的。</p><pre><code class=\"language-xml\">&lt;TestJob name=\"FooJob\"&gt;\n  &lt;TestJob name=\"TestJob1\"&gt;\n    &lt;TestJob name=\"TestJob11\"/&gt;\n    &lt;TestJob name=\"TestJob12\" depends=\"TestJob11\"/&gt;\n    &lt;TestJob name=\"TestJob13\" depends=\"TestJob11\"/&gt;\n  &lt;/TestJob&gt;\n  &lt;TestJob name=\"TestJob2\" depends=\"TestJob1\"&gt;\n    &lt;TestJob name=\"TestJob21\"/&gt;\n    &lt;TestJob name=\"TestJob22\"/&gt;\n  &lt;/TestJob&gt;\n&lt;/TestJob&gt;\n</code></pre><p>为了让你看得更清楚，我又画了个树状图，这样形象一点。如图所示，它是这样一个树结构，在同一个父节点下的子节点之间，可以有依赖关系。</p><p><img src=\"https://static001.geekbang.org/resource/image/ff/6b/ff85a51b332a9456d09dd9696ab05c6b.jpg?wh=1920x1102\" alt=\"图片\"></p><p>那我们要运行FooJob，该怎么运行呢？</p><p>你可以看到，要想运行FooJob，实际上是运行它的两个子节点，TestJob1和TestJob2。但是TestJob2依赖于TestJob1，所以，我们就得先运行TestJob1，再运行TestJob2。运行TestJob1的策略又和运行FooJob的策略完全一样，递归下去，把TestJob1这棵子树运行完。TestJob2也按照同样的策略运行完。这时，FooJob就执行完，返回最终执行结果了。</p><p>运行FooJob，其实就是一个按照一定策略，去递归遍历FooJob树的过程。这个过程类似于树的深度优先DFS的前序遍历算法，只不过左右节点是靠依赖关系确定的，没有前置依赖的节点是最左节点，然后按照依赖关系从左到右排列。</p><p>JobRunner的伪代码实现如下：</p><pre><code class=\"language-java\">public class JobRunner{\n    private File jobXMLFile;\n    public TestRunner(File jobXMLFile) {\n        super();\n        this.jobXMLFile = jobXMLFile;\n    }\n    public void run() {\n        try {\n            TestJob testJob = TestJob.loadFile(jobXMLFile);\n            for(TestJob childJob:testJob.getChildJobs()){\n              //当前job有依赖Job\n              if(childJob.getDepends()!=null){\n                //先运行依赖Job\n                TestJob depdendedJob = childJob.getDepends();\n                dependedJob.run();\n              }else{\n                //当有Job没有依赖Job，可以运行它了\n                childJob.run();\n              }\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n</code></pre><p>JobRunner对于上面FooJob的运行顺序如下：</p><p>FooJob-&gt;TestJob1-&gt;TestJob11-&gt;TestJob12-&gt;TestJob13-&gt;TestJob2-&gt;TestJob21-&gt;TestJob22</p><p>好，JobRunner搞定了，自动化测试设计人员只管往Job树里加减Job，JobRunner会帮你理清执行的计划。如果你给每个Job增加一个权值，JobRunner甚至帮你理出一个冒烟执行链。这个怎么实现，我留给你课后思考，后面我们留言区里交流。</p><h3>Job的Input和Output</h3><p>说完了JobRunner，我们再来看看输入和输出，这是Job模型和TestCase的另外一个重要区别。因为Job运行之前会从外接收数据，运行之后，会向外输出数据。在这个过程中，它只关心自己的逻辑和对外的承诺，并不需要关心谁会使用它的Output。</p><p>你可以想象是这样的一个场景，有一条像传送带一样的数据通道，Job可以从这个数据通道上取东西，也可以放东西。</p><p><img src=\"https://static001.geekbang.org/resource/image/28/c0/28898e8ce1f6fdda6b7406e3125378c0.jpg?wh=1920x693\" alt=\"图片\"></p><p><strong>要实现Job Input和Output机制，核心就是实现这个数据通道。</strong></p><p>因为能提供数据通道功能的载体有很多种，这块我鼓励你广开思路：可能你先想到的就是Hashmap，那数据通道可以是内存里的一张Hashmap对象。数据都是Key Value方式存储在Hashmap里，读取Input就是Map.getValue(Key)，写入Output就是Map.put(Key,Value)。</p><p>这种方式的优点是简单，缺点是它有一个前提，就是所有的Job都在一个进程里。</p><p>如果我们想跨进程来读取Input和Output，可以采用最常见的文件方式。把数据放到文件系统的一个txt文件里，读取Input就是读文件操作，写入Output就是写文件操作。但文件方式也有局限，一个局限是并发写入需要引入文件锁的机制，另外一个局限是，本地文件也没办法应对分布式跨主机的Job。</p><p>当然，最强大的解决方案就是Message Queue，启动一个Message Queque的Broker服务，RabbitMQ或者Kafka，Job通过Queue Client来操作Queue里的消息。</p><p>总之，方案多多，所以我想给你的选择建议是，基于我们专栏的观点“做性价比最高的自动化测试”，不要过度工作，追求强大。强大的另一面就是复杂。所以，寻找合适、够用的方案就可以。</p><h3>驱动工具</h3><p>现在我们看剩下的最后一个问题：Job模型怎么驱动工具？</p><p>具体包括两种情况。第一种，对于那些和框架解耦的工具，像Selenium，RestAssure，它们的TestCase不需要变，我们只需更换调用框架就可以了。用自定义的JobRunner来替代传统的TestRunner，然后提供数据Channel的API，用来操作Input和Output，整个Job模型驱动就运转起来了。</p><pre><code class=\"language-java\">public class seleniumLoginTest{\n  String username=\"\";\n  String password=\"\";\n  @setup\n  public void setUp(){\n    //从DataChannel里获得input\n    username = DataChannel.getInput(\"username\");\n    password = DataChannel.getInput(\"password\")\n  }\n  @Test\n  public void login(){\n    //执行登录\n  }\n  @TearDown\n  public void tearDown(){\n    DataChannel.output(\"output1\",\"outputValue1\");\n    DataChannel.output(\"output2\",\"outputValue2\");\n  }\n}\n</code></pre><p>另外一种情况，对于那些和框架深度绑定的工具，比如QTP、Squish，这个时候我们要切割出它们的框架面和工具面。让工具面和新的JDD框架进行对接。<br>\n这个时候，你可以找它的驱动接口，去执行Job。比如QTP提供了Automation Object Model：</p><pre><code class=\"language-plain\">' A Sample Script to Demostrate AOM\nDim App 'As Application\nSet App = CreateObject(\"QuickTest.Application\")\nApp.Launch\nApp.Visible = True\nApp.loadTest(\"/user/sheng/myQTP/script\")\nApp.run()\nApp.close\n</code></pre><p>详细的接口信息，你可以从工具的官方网站得到，比如QTP的接口信息你可以查看<a href=\"https://admhelp.microfocus.com/uft/en/all/AutomationObjectModel/Content/AutomationIntro/Output/AutoObjModel.htm\">这个链接</a>。</p><h2>小结</h2><p>现在我们总结一下。现在常用的TestCase方式，对自动化测试设计能力的支持非常有限，它是一种弱设计。而Job模型则让自动化测试的设计更加强大丰富。</p><p>Job模型怎么落地呢？我们先把业界的自动化测试实现理念捋了一遍，分成了框架层、工具层和通用库，这样就清楚多了。Job模型落地在框架层，需要实现自己的JobRunner，对Job树进行遍历，生成执行路径。</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/8c/b1324f2c6e890d0949198d0607ff3a8c.jpg?wh=1920x868\" alt=\"图片\"></p><p>另外，Job的Input和Output是Job之间的重要交互方式，我们也找出了三种实现方法，分别是Hashmap，文件和Message Queue。</p><p>在Job模型下，自动化测试层次更加清楚，设计和实现的分工也更加清晰明确。</p><p>设计的产出结果是整理出Job树，Job树的表现形式是一个XML、Yaml或者Json文件，这就相当于开发用Swagger来表达OpenAPI的设计；而到了自动化测试实现阶段，才是工具登场的时候，利用工具实现Job模型，就像是开发去实现Swagger定义的API一样。这样就达到了分层清楚，驱动工具的效果。</p><p>后面三讲内容，我们就利用Job模型来讲解更多的实践案例，下一讲先从一个最简单的金融交易自动化测试开始，金融业务的特点是复杂，精准度要求高，Job模型能不能理清复杂的金融业务呢？敬请期待。</p><h2>思考题</h2><p>你的自动化测试设计目前是怎么做的？是以什么方式输出这个设计方案的？</p><p>欢迎你在留言区跟我交流讨论，也推荐你把这一讲分享给更多朋友。</p>","neighbors":{"left":{"article_title":"19｜排兵布阵：自动化测试的编排、扩展与重构","id":514501},"right":{"article_title":"21｜设计实战（一）： 一个金融交易业务的自动化测试设计","id":515297}}},{"article_id":515297,"article_title":"21｜设计实战（一）： 一个金融交易业务的自动化测试设计","article_content":"<p>你好，我是柳胜。</p><p>一提到金融业，可能你第一时间想到的就是钱。没错，存款、转账都是普通人最常接触的金融业务。不过在专业金融人士那里，钱的形态和流动方式也更加复杂，有交易、投资、期货、股票、基金等等金融模型。</p><p>从测试角度来看，金融业软件有两个特点：第一，对数据精准性要求非常高，不能出错，bug的成本极大；第二，软件的设计要求有很强的金融领域知识，也很复杂。</p><p>所以，金融行业对软件的交付质量要求高，对软件测试非常重视，自动化测试的投入也很大。而自动化测试设计的过程，其实也是对复杂业务梳理的过程。</p><p>挑战随之而来，用我们的Job模型，能不能让金融系统的复杂业务变得简单？今天我们就一起结合例子来看一看。</p><h2>一个转账案例</h2><p>我们来看后面这个转账案例。</p><p><img src=\"https://static001.geekbang.org/resource/image/14/b0/146f4098a2411060fee7aee3c0423cb0.jpg?wh=1920x636\" alt=\"图片\"></p><p>这是一个类型为正例的测试案例。金融业测试里，要设计很多的测试案例，正例是那些预期能够正常完成业务的测试案例，而反例是由于违背了业务规则，预期无法成功完成业务的测试案例。</p><p>在我们的测试案例里，交易账户都是正常的状态，交易额度也在安全额度之内，预期结果是能够正常完成转账交易，所以类型是正例。</p><h2>Job数据建模</h2><p>了解了案例情况，我们想用Job模型来为它建模，该怎么做呢？</p><p>金融业务里最关键的要素就是数据，我们可以从数据角度来理一下测试案例。</p><!-- [[[read_end]]] --><p>这里的数据有账户数据，包括转出账户<strong>A</strong>和转入账户<strong>B</strong>；还有交易数据，包括转账类型<strong>同行转账</strong>和转账金额<strong>4000</strong>；另外，还有配置数据，单笔转账限额<strong>5000</strong>和交易超时时间<strong>30分钟</strong>等。</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/0d/e14960ee123ed1cf08bfbfa5111cc00d.jpg?wh=1920x855\" alt=\"图片\"></p><p>我们Job模型有三个传递数据的地方，分别是JobInput、TestData和TestConfig。</p><p>Input是从Job外部传递进来的数据，TestData是Job自带的测试数据源，能够多组数据循环运行Job，TestConfig是Job会引用到的比较固定的配置数据。</p><p>想一想，账户数据和交易数据，需要放在JobInput里，还是TestData里呢？这个要看我们自动化测试整体设计。</p><p>如果我们每次测试运行都要创建新的测试账户，那转账Job就可以从Input里获得其他Job创建好的账户信息；反之，测试账户是存量的数据，那我们应该把它放在TestData里，实现“多份数据，一份代码”，这样才能提高转账Job的ROI。</p><p>而单笔转账限额和交易超时时间，我们把它放在TestConfig里。表格更新如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/5e/10/5eb82f438393149147da953799f88610.jpg?wh=1920x909\" alt=\"图片\"></p><p>Job模型建模如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/42/9ca49d1c9503692040a5052ed0562842.jpg?wh=1920x682\" alt=\"图片\"></p><h2>Job行为分解</h2><p>数据研究完了，我们再来看梳理一下测试案例的操作步骤。</p><p>1.登录系统<br>\n2.发起转账<br>\n3.验证转账<br>\n4.退出系统</p><p>登录系统和退出系统这两个行为是通用的，为了提高Job的ROI，我们可以把登录系统和退出系统独立出来2个Job。而发起转账和验证这2个步骤，是作为1个Job，还是分割成2个Job呢。这取决于转账和验证的步骤复杂度，在刚开始时，我们不要过度工作over work，可以把转账+验证先作为一个Job。</p><p>这样的话，转账Job就分解成了3个子Job：登录，转账和验证，退出。在Web上实现的话，我们选择用Selenium作为开发工具。</p><p><img src=\"https://static001.geekbang.org/resource/image/6a/15/6a2ab0312d834c699b66fa974e29aa15.jpg?wh=1920x1069\" alt=\"图片\"></p><p>因为子Job继承了父Job的属性，现在3个子Job应该可以从TestData里拿到账户信息，<strong>登录Job</strong>用A账户进行登录，<strong>转账验证Job</strong>使用A账户给B账户转账4000，<strong>退出Job</strong>退出A账户。</p><p>现在自动化测试开发人员就可以用Selenium工具进行开发了！</p><h2>Job扩展</h2><p>自动化测试开发人员开发出这3个Job后，就完成了转账自动化测试第一个版本。能实现跑起来的基础目标。</p><p>但要想让它稳定运行，我们还需要理清楚金融业务的关联性。比如，银行账户是有复杂的配置的，光账户状态就有几十种，比如时效、锁定、冻结、挂失等等。想要让转账测试运行成功，测试账户必须在正常的状态下，有转账的权限，而且还要有足够的余额等等这些条件。</p><p>这些都是转账业务成功运行的前提条件，如果不满足这些条件，直接开始运行转账Job，可能会遇到各种“错误”。但这些“错误”是正常的预期，所以，诊断它们只会浪费你的时间。</p><h3>数据验证</h3><p>怎么办？为了保证转账Job的稳定运行，我们可以增加一个<strong>ValidateJob</strong>来验证数据。让转账Job的依赖指向ValidateJob，如果ValidateJob运行通过，转账Job才会运行；否则，转账Job就不必运行。</p><p>ValidateJob负责做数据验证，我们可以用<a href=\"https://greatexpectations.io\">Great Expectation</a> 来完成这个验证。</p><p>Greate Expecation是一个Python的数据验证框架，它支持多种数据源，内嵌规则引擎。为了让你直观了解它的使用过程，我为你准备了后面的流程图。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/05/e052642764dba3c76a21182756b2dc05.jpg?wh=1920x961\" alt=\"图片\"></p><p>我们的账户存在MySQL数据库里，可以在greate_expectation.yaml里配置数据库信息：</p><pre><code class=\"language-yaml\">datasource_yaml = r\"\"\"\nname: my_mssql_datasource\nclass_name: Datasource\nexecution_engine:\n&nbsp; class_name: SqlAlchemyExecutionEngine\n&nbsp; connection_string: mssql+pyodbc://sheng.liu:Welcome1@&lt;HOST&gt;:&lt;PORT&gt;/&lt;DATABASE&gt;?driver=&lt;DRIVER&gt;&amp;charset=utf&amp;autocommit=true\n  #要验证数据集是Account表\n  table: account\n</code></pre><p>然后，就可以在Jupyter Notebook里开发数据验证代码了：</p><pre><code class=\"language-python\">#验证账户信息\nbatch.load(greate_expectation.yaml)\n#验证规则1:账户的State字段不为空，而且是Active状态\nbatch.expect_column_values_to_be_equal('State','Active')\n#验证规则2:账户余额必须大于4000\nbatch.expect_column_values_greater_than(\"balance\",4000)\n</code></pre><p>运行代码，Great Expectation会从数据库里读取账户数据，进行数据验证，2条规则都通过，整个Job运行成功，会产生下面的报告。<br>\n<img src=\"https://static001.geekbang.org/resource/image/55/85/553b5ea8ce8c11026b95803d83040a85.jpg?wh=1920x695\" alt=\"图片\"></p><p>现在，我们把Validate数据Job添加到Job设计图里，更新如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/40/f3/40739be914fb12409f3be5c89817f7f3.jpg?wh=1920x1069\" alt=\"图片\"></p><p>这样，每次转账Job运行之前，都会先运行Validate数据Job。只有数据都准备好了，转账Job才会运行。在这个机制的保护下，转账Job的运行结果成功或失败，就更可信了。</p><h3>反例测试</h3><p>有了ValidateJob之后，我们可以考虑反例测试了。这里反例的逻辑是这样的：如果账户数据是异常的情况下，转账Job是会失败的，要是交易依然成功，那就是一个安全漏洞。</p><p>因为这种漏洞在金融业务里很可能带来巨大的经济损失，所以，在金融测试里，反例测试也是很重要的测试案例。</p><p>不同的反例条件，转账执行失败的结果也是不一样的，它们有一个对应关系。 我列了一个条件-结果表格，这样你能一目了然。</p><p><img src=\"https://static001.geekbang.org/resource/image/0a/23/0a26db5a405ac2ed7083838849fa5923.jpg?wh=1920x776\" alt=\"图片\"></p><p>那自动化设计里，反例测试该怎么做？</p><p>你可能在第一时间想到的办法是，开发一个“转账Job-反例”，在这个反例Job里，把上面表格里的条件和结果，作为测试案例里的Given条件和Then预期输出。</p><p>这就引出了下一个问题，这个反例Job的数据从哪里来？其实，这种情况下，我们就可以利用Job模型的Input、Output机制，让ValidateJob验证数据之后，把数据通过Output分发到转账正例和转账反例的Job中去。</p><p>Greate expectation有一个Action的机制，根据规则匹配的结果，即可触发不同的Action，配置文件如下：</p><pre><code class=\"language-python\">validation_operators:\n&nbsp; &nbsp; action_list_operator:\n&nbsp; &nbsp; &nbsp; &nbsp; class_name: ActionListValidationOperator\n&nbsp; &nbsp; &nbsp; &nbsp; action_list:\n&nbsp; &nbsp; &nbsp; &nbsp; - name: StateNotSatisfied # 状态不符合\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; class_name: Output\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; output: \"State not satisified\"\n&nbsp; &nbsp; &nbsp; &nbsp; - name: BalanceNotSatisfied # 余额不符合\n          action:\n            class_name: Output\n            output: \"Balance not satisified\"&nbsp; \n</code></pre><p>现在我们再次重构Job设计树，就会得到下面这个设计结构。我们在跟转账正例Job平行的层级，再加上一个转账反例Job。不难看出，正例和反例的数据都来自于一个ValidateJob的输出。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/02/f3709f1c2367eb03690c443305a36a02.jpg?wh=1920x1156\" alt=\"图片\"></p><p>ValidateJob专门负责账户的合法性验证，使用Greate Expectation工具，内嵌规则引擎，来加载和解析账户数据，并把账户数据分组通过Output输出。然后，转账的正例Job拿到合法的数据，反例Job拿到非法的数据。</p><p>我给你打个比方，帮助你理解一下这个工作原理。其实这个场景就像生产线，第一个工人只管生产螺丝钉，他很专业，可以生产不同型号的螺丝钉，他不需要知道谁会使用这些螺丝钉。他需要做的就是，做好螺丝钉后，就把他们放到传送带上；而第二个工人是造飞机的，他会从传送带上拿到飞机螺丝钉，然后开始自己的任务；第三个工人是造汽车的，他拿到汽车的螺丝钉也开启自己的任务了。</p><p>这样分工，每个工人都在干自己的专业工作，又通过传送带和其它工人完成了协作。放在软件设计上，就满足了高内聚，低耦合的设计原则。</p><p>好，咱们接着推演后续可能会怎样发展。如果哪一天，ValidateJob里面越来越复杂了，一个人无法完成，需要分解给10个人去做。那么只需要保持ValidateJob接口不变，在ValidateJob下面分解成10个子Job就可以。</p><p>就像生产线上，生产螺丝钉的人手不够用了，他是招8个人还是10个人，分成几个小组，这些都是这个小组内部的事，只要这一组，还是跟以前一样，对外提供不同型号的螺丝钉就可以了。</p><p>好，让我们回顾一下整个过程。我们从一个银行转账正例案例开始，向下细化成3个Selenium Job，又平行引入了数据验证Job和反例Job。</p><p>到这里，你是不是对怎么应用Job模型更熟练了呢？我们利用这个模型，根据业务需要来扩展Job树。<strong>具体扩展方向有两种，一种是向下去下钻，将测试场景分解细化；另一种是向上聚合抽象，把场景归类合并到一起，形成一个更大的场景。</strong></p><p>最后，我们设计好了正例和反例Job，就能聚合一起，生成一个转账的正反测试Job。</p><p><img src=\"https://static001.geekbang.org/resource/image/56/5e/56a32700173fbf2e6e5f991ab75f655e.jpg?wh=1920x1634\" alt=\"图片\"></p><p>其实这个Job树并不是“终点”，我们还可以根据业务的复杂度再进行细化。</p><p>金融系统里，数据验证是一个精细的工作，确保无误。在手机端、Web端、API，甚至DB都要验证数据。根据这个需求，我们可以把转账+验证Job，进一步分解，分解后的情况如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/30/c49d0d3f10f03072cf40fc0085832a30.jpg?wh=1920x1483\" alt=\"图片\"></p><p>你可能还有疑惑，不知道要怎么判断，我们是否得到了一个足够细化的最终结果。其实判断起来也不难，终点就是业务逻辑梳理清楚后，这棵Job树的所有叶子结点，都明确了实现工具。这样，我们就可以推进分工，进入开发实现阶段了。</p><p>我在反例Job里，没有标注蓝色的实现工具，你可以考虑一下怎么细化。</p><h2>小结</h2><p>在今天的例子里，我们通过Job树的设计，梳理了金融业务的数据流动和验证。而且由于金融业务的特殊性，尤其是对数据精准性的要求。我还带你认识了一款数据验证框架<strong>Greate Expecation</strong>。它是基于Python语言的框架，能够从数据源里获得数据，进行规则验证，输出结果，而且在Jupyter Notebook里开发，交互性和效率都很好。</p><p>这整个的建模过程，经历了从整体到局部，从概要到细节的推演。其实，直到这棵Job树的叶子结点都有了选型好的工具、可开发的Job，我们的交易案例设计才算清楚了，这也是业务梳理完毕的信号。</p><p>每一个叶子结点的Job就是一个开发任务，有接口、有数据、有配置，开发工作任务列表就出来了。而且，通过Job之间的依赖关系，我们也可以得出一个有优先顺序的开发执行计划，谁在前，谁在后。</p><p>今天这一讲，为了让表达更形象，我们沿用了树形图来推演设计。在实际工作中，我们可以把这个设计保存在YAML或XML文件里，方便去做验证和二次开发，我会在设计篇最后一讲说说这块的实现思路。</p><p>下一讲，我们会继续锻炼自动化设计思维，探索如何利用Job模型实现自动化部署管线的设计，敬请期待。</p><h2>思考题</h2><p>你的项目里有没有涉及数据验证测试，你是怎么做的？</p><p>欢迎你在留言区和我交流互动，也推荐你把今天学的内容分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"20｜链接工具：要驱动工具，不要被工具驱动","id":514818},"right":{"article_title":"22｜设计实战（二）: 一个全周期自动化测试Pipeline的设计","id":516198}}},{"article_id":516198,"article_title":"22｜设计实战（二）: 一个全周期自动化测试Pipeline的设计","article_content":"<p>你好，我是柳胜。</p><p>说起交付流水线，你可能立马想到的是Jenkins或CloudBees这些工具，它们实现了从Code Build到最终部署到Production环境的全过程。</p><p>但Jenkins只是工具，一个Pipeline到底需要多少个Job，每个Job都是什么样的，这些问题Jenkins是回答不了的，需要使用工具的工程师去思考去设计。在实践中，通常是DevOps工程师来做这个设计。</p><p>既然我们学习了微测试Job模型，也知道了它能帮助我们去做自动化测试设计，那用这个模型，能不能帮我们做Pipeline设计呢？<strong>其实，Pipeline本质上也是一个自动化测试方案，只不过它解决的场景是把软件从代码端到生产端的自动化。</strong></p><p>掌握设计思维是测试工程师向测试架构师的必由之路，假设今天你需要设计一个Pipeline，把一个Example Service的代码，最终部署成为生产环境的一个服务进程。完成这个工作，你不仅能弄明白CICD的原理和实现，而且对自动化测试Job怎么集成到CICD，也将了如指掌。</p><p><img src=\"https://static001.geekbang.org/resource/image/fc/bd/fc734fc77fcac6773de057d5d7437dbd.jpg?wh=1920x791\" alt=\"图片\"></p><h2>Example Service的Pipeline的目标</h2><p>还是按照Job模型的设计原则，先理出Pipleline的第一个根Job。</p><!-- [[[read_end]]] --><p>Pipeline的起点在哪里呢？很显然，它从一个GitLab的repo开始，准确来说，要能访问Example Service的代码。怎么能访问代码呢？我们需要知道两个信息，代码仓库的URL地址和访问的token。</p><p>1.GitLab的代码仓库地址：<a href=\"http://81.70.254.64/yuzi/dockerbase.git\">http://gitlab.example.com/sheng/exampleservice.git</a></p><p>2.访问token：token-123456</p><p>Pipeline的终点到哪里呢？很简单，最后Example Service能够在生产环境运行起来，运行起来的标志，就是Example Service的工作URL：<a href=\"http://prod.example.com\">http://prod.example.com</a></p><p><img src=\"https://static001.geekbang.org/resource/image/c0/09/c05f9271d3a8dc72d8e16628d5690f09.jpg?wh=1920x791\" alt=\"图片\"></p><h2>划分Pipeline的阶段</h2><p>这个从代码转成生产环境服务的过程，至少要完成两次转换。</p><p>第一次转换是<strong>把代码转成可部署包</strong>，第二次是<strong>把可部署包转换成运行态服务</strong>。为了把好验证关的大门，我们还要在这两次转换中间再加入一个测试任务，那么Pipeline就可以分解成3个子Job，分别是开发Job、测试Job、部署Job，它们依次完成，全部运行通过，最后才能部署到生产环境。如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/bb/a698bd4c9cbe0912fe9f970c4de5fabb.jpg?wh=1920x791\" alt=\"图片\"></p><p>这三个Job，我们依次来看一下它们都需要干什么活。</p><p>首先，我们来看看DevJob。开发阶段测试Job，作为整个Pipleline的初始Job，Pipeline的Input就是DevJob的Input。</p><p>1.GitLab的代码仓库地址：<a href=\"http://81.70.254.64/yuzi/dockerbase.git\">http://gitlab.example.com/sheng/exampleservice.git</a><br>\n2.访问token： token-123456</p><p>DevJob要做什么呢？它前面连接了代码库，后面要对接集成测试，它的主要职责就是把代码变成可部署的Package。</p><p>Input有了，DevJob要输出什么呢？我们要知道，DevJob的输出会作为下一阶段Job的输入，设计DevJob的输出就跟接口设计一样，不同的接口都能让系统工作，但我们要选择最合适那一种。</p><p>比如，我们可以让DevJob输出一个Example Package的仓库地址，代表Package已经打好了，并上传到了仓库，或者我们输出单元测试报告；甚至，DevJob也可以什么都不输出，让下一个TestJob自己去按照双方约定好的Ftp服务器上去取包。</p><p>哪种方案更合适呢？</p><p>这里你要考虑两点：第一，我的输出别人是否用得到？像单元测试报告这种信息，是我DevJob内部的工作产物，别人不会感兴趣。按照面向对象设计原则，这些细节应该隐藏在DevJob内，不向外公布。</p><p>第二，我的输出是不是信息显式而充分的？像多个Job都遵循一个约定，去指定的FTP服务器上去取放Package，这个信息交互就是隐式的，没有在接口上体现出来。这样是有隐患的，如果之后各个Job变更重构的时候，就会出问题。</p><p>根据这个原则，我们让DevJob输出一个Example Package的仓库地址</p><p>DevPackageURL: <a href=\"http://nexus.exmaple.com/content/repositories/exampleservice.jar\">http://nexus.exmaple.com/content/repositories/exampleservice_1.0.1.jar</a></p><p>如果DevJob输出了这个URL，就代表DevJob已经运行通过，把代码变成了Package，而且把Package放到了组件管理平台Nexus上去，别人就可以用了。</p><p>你可能会问，拿到了Package URL还需要Nexus的用户名密码才能访问。那下一个Job从哪里获知用户名和密码呢？这时，就可以用到我们Job测试模型的TestConfig属性了。直接把用户名、密码放在TestConfig就可以了。</p><p><img src=\"https://static001.geekbang.org/resource/image/d1/82/d170896468d28932a5eb07745fe79482.jpg?wh=1920x755\" alt=\"图片\"></p><p>同样的思路，我们也能整理出TestJob的Input是DevPackageUrl，Output是ReleasePackageUrl：</p><p><img src=\"https://static001.geekbang.org/resource/image/22/b4/22ca2574e08a1622293e08bcfdc250b4.jpg?wh=1920x755\" alt=\"图片\"></p><p>DeployJob的Input是ReleasePackageUrl，作为Pipeline最末一个Job，Pipeline的输出就是DeployJob的输出：</p><p><img src=\"https://static001.geekbang.org/resource/image/85/b2/850eac96f93e65yy59d0124cac813db2.jpg?wh=1920x755\" alt=\"图片\"></p><p>推演到现在，整个Job树就成了后面这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/35/d08104f939e9ff47096974d761395935.jpg?wh=1920x713\" alt=\"图片\"></p><p>我们把Pipeline拆分成Dev、Test和Deploy三个Job后，就可以把它们分别指派给开发人员、测试人员、运维人员，由他们来推进下一阶段的设计了。至此，概要设计告一段落。</p><h2>各阶段Job的详细设计</h2><p>我们在上面的设计中规划了3个阶段，明确了作为Pipeline job的第一级子Job：DevJob、TestJob、DeployJob。但每个阶段要做什么具体任务，还需要进一步详细设计。</p><h3>DevJob</h3><p>我们先看DevJob，它的Output是一个可执行包。</p><p>怎么完成从代码到可执行包的转换呢？当然要进行构建，也就是Build。</p><p>BuildJob的模型很容易梳理，它的Input是codeRepoUrl，而Output是DevPackageUrl。BuildJob从代码库里抓取代码，构建，然后打包生成Package，上传到对象仓库，就完成了。</p><p>但build通过了后，是不是可以直接产生DevPackage呢？这里我分享一个“保险”措施，为了保证质量，我们可以在build之后，加上一个测试任务，也就是单元测试UnitTestJob。单元测试通过之后，再输出DevPackage。</p><p>UnitTestJob就是运行单元测试，单元测试成功还是失败，就是UnitTestJob的结果。所以，UnitTestJob的Job模型里，只设置一个Dependency就好了，指向BuildJob。BuildJob成功之后，才能运行UnitTestJob。这里的UnitTestJob，我们选用Junit开发框架就能实现。</p><p>现在DevJob被细分成了两个子Job：BuildJob和UnitTestJob。</p><p>按照Job模型的规则，子Job都运行成功，父Job才能成功。所以，BuildJob和UnitTestJob运行成功，DevJob才算成功，而DevJob成功了，后面的TestJob才能触发运行。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/cd/26d9e9dd80fbc153542b8553b28874cd.jpg?wh=1920x1038\" alt=\"图片\"></p><h3>TestJob</h3><p>经过第二模块里策略篇的学习，我们知道了测试的策略有单元测试、接口测试和系统测试。单元测试一般是开发人员来做，已经包含在DevJob里了。而接口测试和系统测试，就需要放在TestJob里。</p><p>先来看APITestJob的模型。APITestJob作为TestJob的第一个子Job，APITestJob的Input就是它的父Job TestJob的Input，也就是DevPackageUrl。有了这个信息，APITestJob就可以开发实现了，它的自动化工具可以选用RestAssure。</p><p>然后是E2ETestJob的模型，它的Dependency是APITestJob，而Output是ReleasePackageUrl。至于E2ETestJob的自动化技术，我们可以选用Selenium。</p><p>现在，可以把TestJob分解成了2个子Job，APTestJob和E2ETestJob，Job树如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/de/89/dede1f23e6df7yyfb231e2b5b2702f89.jpg?wh=1920x1038\" alt=\"图片\"></p><h3>DeploymentJob</h3><p>又下一城，咱们一鼓作气，继续来看DeploymentJob。</p><p>当Pipeline移动到DeploymentJob的时候，它会检查前置依赖TestJob的状态，只有成功了，DeploymentJob才会运行。在运行的时候，DeploymentJob会从Input获得一个ReleasePackageUrl，然后经过一系列部署操作，把Package部署到生产环境里开始运行，输出一个serviceUrl。</p><p>为了完成这个过程，简单的做法是，DeploymentJob不需要再细分子Job，你可以直接把它当作一个可执行Job，用部署脚本来实现。</p><h2>转成自动化测试开发计划</h2><p>搞定上面的设计后，现在Job树变成了这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/51/8b/51a0de00fc0c6e5e8e76e98a3b1b528b.jpg?wh=1920x1087\" alt=\"图片\"></p><p>现在每一个叶子结点，就是可执行的Job，需要自动化实现。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/78/741517d2175c0ee8a4e648e811638e78.jpg?wh=1920x693\" alt=\"图片\"></p><h2>变更和扩展</h2><p>在Pipeline搭建起来后，我们还要持续维护。这里有两种情形，一种是新增Job，还有一种是扩展Job。</p><p>先看新增Job的场景，我给你举个例子。比如开发人员有一天要引入Sonar，做代码质量检测，那我们该怎么做呢？</p><p>动手前，我们还是先分析一下。Sonar扫描，也是基于代码CodeRepoUrl，这个和DevJob的Input一致。这样，我们可以建立一个SonarJob，作为DevJob的子Job，可以复用DevJob的Input。</p><p>那SonarJob和BuildJob，还有UnitTestJob是什么关系呢？这就看我们想怎么定义它们的依赖关系了。也就是谁在前端，谁在末端。</p><p>一般来说，代码扫描，应该发生在代码变更的最早时刻。一旦有代码提交，就首先检查代码的质量，如果代码质量不符合预定标准，就终止Pipeline。实践里这样选择，是为了优先保证代码的质量。</p><p>基于这样的目标，我们就把SonarJob添加到DevJob下的第一个子Job，让BuildJob依赖SonarJob，而UnitTestJob保持不变，还是依赖BuildJob。</p><p>另外还有一种情形，就是某一个Job当时设计实现比较简单，后来需要扩展。比如DeploymentJob，刚开始我们用一段Shell脚本就可以轻松执行它。但随着Deployment要求提高，我们需要在Deployment结束后，还要执行测试任务，验证质量，然后再发布serviceUrl。面对这种情况，我们该怎么办？</p><p>其实这个场景，就跟开发一样，一旦代码规模扩大了，就需要分拆模块了。</p><p>在我们的Job树里，原先的DeploymentJob需要分拆了，拆分成ExecuteDeployment和TestDeployment，让TestDeployment依赖于ExecuteDeployment。之前的DeploymentJob变成一个抽象Job，它的脚本实现挪到ExecuteDeploymentJob里，在新TestDeployment里，加上测试任务即可。如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/96/8bc413d8d91f669927efde462fc77d96.jpg?wh=1920x1268\" alt=\"图片\"></p><h2>小结</h2><p>为了检验咱们的Job模型威力几何，今天我们再次锻炼设计思维，进行了部署Pipeline的设计。经过案例演练，你现在是不是对Job模型更加熟悉了呢？</p><p>咱们回顾一下关键流程。概要设计里，我们完成了抽象Job的定义，对应着Pipeline划分的三大阶段：开发阶段、测试阶段和部署阶段，然后逐层细化，一直到实例Job。而在详细设计里，我们不仅要确定实例Job的接口，而且还要确定它们的自动化测试实现技术。</p><p>做完设计，每一个叶子节点的Job都是要去开发实现的自动化测试任务。我们就可以整理出一个自动化测试开发任务列表了。</p><p>后面是Job树叶子工作表，供你复习回顾。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/78/741517d2175c0ee8a4e648e811638e78.jpg?wh=1920x693\" alt=\"图片\"></p><p>一个优秀模型，不光着眼于眼前的需求，还具备应对未来变化的能力。除了概要设计和详细设计，我们还推演了未来的应变策略。</p><p>当规模扩大时，原先的可执行Job的逻辑变得复杂的时候，我们依然可以根据Job模型，把可执行Job再进行分解成多个子Job。需要注意的是，我们遵循的原则是只有叶子结点Job才是可执行Job，不再进行分解的Job，而一旦Job下挂了子Job，那这个Job就变成了抽象Job，只负责接口的定义。</p><p>今天讲了Pipeline设计，也是对前面Job设计内容的复习巩固。接下来，我们就要进阶到另外一个难度级别的设计了，不仅有多端协作，还有分布式事务，这种场景怎么设计？我们下讲见。</p><h2>思考题</h2><p>你负责或者参与的Pipeline原先是怎么设计的？看完这一讲，你想到怎么优化它了么？</p><p>欢迎你在留言区跟我交流互动。如果觉得这一讲对你有启发，也推荐你把它分享给更多朋友、同事。</p>","neighbors":{"left":{"article_title":"21｜设计实战（一）： 一个金融交易业务的自动化测试设计","id":515297},"right":{"article_title":"23｜设计实战（三）: 一个分布式多端视频会议自动化测试设计","id":517078}}},{"article_id":517078,"article_title":"23｜设计实战（三）: 一个分布式多端视频会议自动化测试设计","article_content":"<p>你好，我是柳胜。</p><p>这几年疫情肆虐全球，远程办公越来越普遍了，视频会议的市场也变得更加火热。视频会议软件的质量，自然也需要测试保驾护航。</p><p>不过，视频会议软件自动化测试非常有挑战性，因为它有很多难点：第一，协作复杂；第二，分布式执行；第三，验证有技术难度。</p><p>今天，我们就来为视频会议软件设计一套自动化测试方案，用来测试视频会议的会议功能。</p><h2>场景还原</h2><p>我们先从视频会议的时序图开始分析，你就明白视频会议的自动化测试有多复杂了。这里我选择了一个迷你demo，但麻雀虽小，五脏俱全。假定会议用户有三位：使用Web端的用户A、桌面用户B，以及手机用户C。</p><p><img src=\"https://static001.geekbang.org/resource/image/71/23/71253a39b0e2dec88835edf5efcaee23.jpg?wh=1920x1192\" alt=\"图片\"></p><p>如图所示，Web用户A先在浏览器创建一个Meeting，发送邀请链接给桌面用户B和手机用户C，然后B和C加入会议，A、B、C三个人同时在线。</p><p>这时，用户A开始演示自己的屏幕，B和C要收到A演示的屏幕；A如果在聊天区里发送一个消息，B和C也要收到这个消息。</p><p>这样一个复杂的场景，我们既要测试视频会议的分布式协作能力，还要测试会议的准确度，另外还要保证效率性能。这三方面的测试目标，我用表格的方式来举例说明，你可以看看。</p><p><img src=\"https://static001.geekbang.org/resource/image/64/5c/644a891248a8a9ba0f5b0c8ac0d6c55c.jpg?wh=1920x691\" alt=\"图片\"></p><h2>视频会议测试概要设计</h2><p>那么，自动化测试该怎么设计和实现，才能达到上面的目标？我们先从概要设计开始分析。</p><!-- [[[read_end]]] --><p>还是遵循从顶向下的设计思路，我们先捋出来要完成视频会议测试，需要测试哪几个功能区。</p><p>首先是初始化，初始化的过程，是保证所有的客户端都进入到视频会议里；然后测试演示功能区，也就是完成一个客户端演示，其它客户端观看演示的过程；接着要测试聊天功能区，一个客户端发送聊天，其它客户端接收聊天；最后是退出，测试完毕后，各个客户端依次退出视频会议系统。</p><p>根据这个思路，我们先把视频会议Job切分成4个子Job，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/68/fbc44e98d4aa9d6bb6a04b72e04e4168.jpg?wh=1920x905\" alt=\"图片\"></p><p><strong>4个功能首尾衔接，按照依赖关系和重要程度，权值高的排在前面。</strong>如果权值高的失败了，后续的Job就没有必要运行了。根据Job模型层级原则，如果有任何一个子Job失败了，视频会议自动化Job就会失败。</p><p>为了支持分布式，我们还需要在Job模型里加入一个host属性，指明本Job运行的主机环境。</p><p>像这样，host=“10.0.0.1”，就代表着本Job会被发送到10.0.0.1这台机子上去运行。</p><p><img src=\"https://static001.geekbang.org/resource/image/20/8b/201b3d53eb2ee00136a97839d7ff268b.jpg?wh=1920x1113\" alt=\"图片\"></p><p>有了这个属性后，后面我们就好建模了。接下来，我们依次来看各个功能区怎么设计。</p><h2>初始化功能设计</h2><p>初始化Job，要完成的任务是A、B、C进入会议，同时在线。怎么设计呢？</p><p>我们还是先按操作顺序梳理一下A、B、C都做了什么。</p><p>首先，A要做一个CreateMeeting的操作，把它发到10.0.0.1主机上运行，输出一个MeetingUrl；然后，B和C从Input里取到MeetingUrl，在10.0.0.2主机上执行JoinMeetingDesktop，在10.0.0.3主机上执行JoinMeetingAndroid的Job。</p><p><img src=\"https://static001.geekbang.org/resource/image/bf/45/bf5545c0344834010348a8cc13b31b45.jpg?wh=1920x1215\" alt=\"图片\"></p><p>但有一个问题，初始化Job的运行结果是，A，B，C都加入了Meeting。怎么验证A、B和C都加入到会议里了呢？又由谁去做这个验证呢？</p><p>可以这样设计，等A、B、C都执行完操作以后，再运行一个VerifyJoin的Job，来验证ABC是否都已经成功加入Meeting了。</p><p>在Job树上会有这样的结构：把A、B、C聚合成一个ScheduleMeeting的Job，然后VerifyJoinJob的Dependency指向ScheduleMeeting Job。</p><p>在执行的时候，等到ScheduleMeeting Job执行完且结果为通过的情况下，VerifyJob会才会开始执行。如果A、B、C其中有一个失败了，ScheduleMeeting就会失败，VerifyJoin的Job也就没必要再去执行了。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/c0/a9b000199746d04dbb650fd8eaf87ac0.jpg?wh=1920x1627\" alt=\"图片\"></p><p>这种设计的好处是，<strong>整体自动化测试的健壮性很强</strong>。按照Job树的运行原理，运行顺序是这样的：CreateMeeting先运行，然后是JoinMeetingDesktop和JoinMeetingAndroid，最后再运行VerifyJoin。任何一个失败，后面都不必运行了。</p><p>不过，这种设计也有一个问题，就是自动化测试运行的效率。VerifyJoin的要等到ScheduleMeeting Job全部执行完了，才能执行，整个自动化测试任务执行时间是这4个Job执行时间的总和。</p><p>time(Total）=time(CreateMeeting)+time(JoinMeetingDesktop)+time(JoinMeetingAndroid)+time(VerifyJoin)</p><p>假设CreateMeeting花了30秒，JoinMeetingDesktop花了20秒，JoinMeetingAndroid花了25秒，VerifyJoin花了15秒。那根据这个公式，总共执行时间是 30+20+25+15 = 90秒，总共一分半的时间。</p><h3>优化执行时间</h3><p>怎么让执行时间缩短呢？面对这个问题，你可能会想到，有没有办法把串行执行变成并发执行呢？这样时间就缩短了。</p><p>没错，对于Job树上同一层级且没有依赖关系的子Job们，谁先执行、谁后执行都无所谓。为了加快执行速度，我们可以优化JobRunner的机制，用<strong>并发线程</strong>执行它们。</p><p>沿着这个思路，我们可以解除一些依赖关系，让执行效率得到提升，让VerifyJoin Job不再依赖于ScheduleMeeting Job，VerifyJoin和ScheduleMeeting同时启动。</p><p>VerifyJoinJob并不知道ScheduleMeeting是否开始，何时结束，它就是不停地轮询A、B、C是否都加入Meeting，如果得到确认就返回，否则会直到超时报错。没错，这里我们可以用上Job模型的TestConfig里的TimeOut参数，比如，我们给VerifyJoinJob设置TimeOut参数是3分钟。</p><p>在这个方案下，我们再算一下执行时间是多少。ScheduleMeeting的执行时间的计算公式如下：</p><p>executeTime(ScheduleMeeting)=executeTime(CreateMeeting)+ Max(executeTime(JoinMeetingDesktop), executeTime(JoinMeetingAndroid))30+Max(25,20)=55秒</p><p>而VerifyJoin Job是和ScheduleMeeting是并发执行的，只要ScheduleMeeting成功运行，ABC同时上线，VerifyJoin Job就会立即成功返回。</p><p>所以，和上面同样的Job，现在优化了JobRunner后，整个自动化测试的任务执行时间就缩短到了55秒。相比优化之前的90秒，自动化测试的运行速度加快了40%。</p><p>而且，健壮性也得到了保证，即使有错误发生，也不会超过3分钟。根据上面的描述，你可以脑补一下相应的Job树还有它的时间计算公式。</p><h2>演示功能设计</h2><p>演示功能又是一个需要多端协作的场景。这是个什么场景呢？简单来说，A开始演示PPT的时候，B和C都要能看到这张PPT。</p><p>我们可以用“初始化功能设计”的思路，来完成这个设计，A要执行presentPPT的Job，然后在B和C上分别执行viewPPT的Job，就可以完成这个任务了。</p><p><img src=\"https://static001.geekbang.org/resource/image/06/2d/065dca1d511b6fd0895bca85141d7f2d.jpg?wh=1920x1215\" alt=\"图片\"></p><p>但是，相比“初始化功能”，“演示功能”有两个特殊的地方需要考虑，一个是屏幕相似度，另一个是屏幕切换的流畅度。</p><h3>屏幕相似度</h3><p>如何验证演示的正确性？当A展示一张PPT的时候，怎么让程序验证，B和C看到的和A演示的是一样的PPT呢？</p><p>比如出现这样的屏幕是正常的。</p><p><img src=\"https://static001.geekbang.org/resource/image/c2/91/c277d037916ab6e87f3dae90a5450591.jpg?wh=1920x626\" alt=\"图片\"></p><p>但下面这样，C端出了问题，没有完整显示，下半部分有一块黑色区域。</p><p><img src=\"https://static001.geekbang.org/resource/image/97/07/971c136b2c75d8188a13e19832f1a907.jpg?wh=1920x617\" alt=\"图片\"></p><p>看到这里，你可能已经想到了，视频会议的屏幕演示，其实就是一帧帧的图片，我们可以采用位图比较的思路，来验证A，B，C所看到的屏幕是不是一致。</p><p>怎么实现位图比较呢？如果自开发的话，可以自己去写图像比较的算法，来计算两张图片之间的相似度。我给你列了一个表，里面提炼了几个经典算法的原理和优缺点。</p><p><img src=\"https://static001.geekbang.org/resource/image/e8/5e/e856ab2fd7df1fc67b03de02c765655e.jpg?wh=1920x704\" alt=\"图片\"></p><p>从上表可以看到，不同的算法，都有各自的优缺点和适用场景。而且，开发和维护这些算法的代码，也是一个不小的工作量。</p><p>那有没有智能对比图片的办法呢？图片识别和比较是AI擅长的领域，它们应用到测试领域就是AI测试。业界有两个比较成熟的两个AI测试工具，Applitools和Sikuli。</p><p>Applitools&nbsp;是一个AI 赋能的测试工具，通过视觉AI 进行智能功能和视觉测试。具体怎么实现呢？</p><p>A端先把自己的屏幕抓屏，保存成图片上传到Applitools服务器，形成一个<strong>基线</strong>。B和C也把自己的屏幕图片上传到同样的<strong>基线</strong>，Applitools会自动对同一基线的图片对比，得出结果。A、B、C端执行的代码如下：</p><pre><code class=\"language-java\">//创建基线 \"Image-PresentScreen1\"\neyes.open(\"Video Meeting\", \"Image-PresentScreen1 \", new RectangleSize(800, 600));\n//对屏幕抓图\nFile file = Screen.capture();\nBufferedImage img = ImageIO.read(file);\neyes.check(\"Image buffer\", Target.image(img));\n//关闭Eyes.\neyes.close();\n</code></pre><p>在Applitools的网站可以看到三条成功的记录，A和B和C上传的图片，Applitools经过对比是一致的，所以结果为成功。</p><p><img src=\"https://static001.geekbang.org/resource/image/18/a7/180f813b1f260b70d311d928cbc362a7.jpg?wh=1920x1023\" alt=\"图片\"></p><p>在图片上，还有两个图标，点赞和消赞，分别对应着Accept Diff和Reject Diff。当Applitools发现图片不匹配时，比如在图片中有动态的值（日期，时间，用户名等等），每次运行的值都不一样，你可以通过点击这点赞的图标Accept Diff，来反馈训练Applitools的图片识别算法，反馈次数越多，训练出的识别能力越精准。</p><p>Applitools很好用，但有一个限制，它需要图片上传到Applitool自己的官方服务器进行识别。如果我们对数据有安全的顾虑，Sikuli更适合。Sikuli由美国麻省理工学院研究开发，是一款基于视觉的测试工具，也提供了图片比较函数，代码示例如下：</p><pre><code class=\"language-java\">Screen screen=new Screen();\nPattern pa1=new Pattern(\"/Users/37397/Desktop/Screen Shot 2018-03-27 at 6.02.42 PM.png\");\nString img=screen.capture().save(\"/Users/1234/Desktop/Automation/Test_Sikuli/\", \"image\");\nFinder f1=new Finder(screen.capture().getImage());\nf1.find(pa1);\nif(f1.hasNext()){\n   Match m=f1.next();\n   System.out.println(\"Match found with \"+(m.getScore())+\"100\"+\"%\");\n   f1.destroy();\n}\n else{\n   System.out.println(\"No Match Found\");\n}\n</code></pre><h3>屏幕切换流畅度</h3><p>演示的流畅也是需要测试的。怎么度量演示的流畅？我们可以用延迟时间来评价。也就是当A切换屏幕之后，B和C上的画面也开始切换，这个时间差就是延迟时间。</p><p>现在的问题就变成了——怎么来截取这个时间差？这就涉及到一个分布式事务的概念，如果我们定义的Transaction是timeBetweenPresentAndView，那么Transaction Start是在A端开始Present的时候，而Transaction End是在B端看到PPT的时候。</p><p>分布式事务怎么实现？在Job模型里，我们可以用Input和Output来实现。A的Job在Present之后就立刻记录下当前的时间戳，输出一个TR_START_timeBetweenPresentAndView=1342629206455，而B的Job在看到PPT之后，也立刻记录下时间戳，输出一个TR_END_timeBetweenPresentAndView=1342629276772。</p><p>那它们怎么聚合成最后的事务呢？这时候，我们可以创建一个aggreateTransaction的Job，专门负责从Input里拿到这些数据，然后计算输出TR_timeBetweenPresentAndView的时间。</p><p><img src=\"https://static001.geekbang.org/resource/image/07/9d/07bb7223c7d2dc9d3a6c240fc78b1a9d.jpg?wh=1920x620\" alt=\"图片\"></p><p>聊天功能和演示功能类似，由A发送一个Message，B和C都要接收它。这里也有准确性和时效性的问题。解决方案和演示功能相似，你可以自己梳理一下Job树，巩固前面学到的内容。</p><h2>实现与扩展</h2><p>到这里，我们把刚才的设计整理一下，整体成果Job树如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/88/b1/88cde599cfcd1c26af0ed42cb768d9b1.jpg?wh=1920x890\" alt=\"图片\"></p><p>从图里可以看到，我们一共用15个Job完成了这个视频会议自动化测试场景的设计。接下来，我们对照Job树，继续推演需要实现的子Job和适合的工具。</p><h3>实现</h3><p>所有的叶子结点Job，都是后面测试实现阶段中，测试开发同学需要开发实现的实体Job，把它列成一个Job树叶子工作表：</p><p><img src=\"https://static001.geekbang.org/resource/image/c8/2f/c8fe5e9563fyy279266940e722d4f52f.jpg?wh=1920x839\" alt=\"图片\"></p><p>然后就到了每个Job选型工具的确定。工具的列表可以参考我为本专栏准备的 <a href=\"https://github.com/atinfo/awesome-test-automation/blob/master/java-test-automation.md\">Gitub地址</a>。选型遵循3KU法则，寻找合适的测试层面和成熟的工具。</p><p><img src=\"https://static001.geekbang.org/resource/image/ae/af/ae158cca355d8f23f0989c3455dd67af.jpg?wh=1920x824\" alt=\"图片\"></p><h3>扩展</h3><p>在刚才的Job设计中，视频会议的Desktop端是Windows，所以JoinMeetingDesktop Job的实现用WinAppDriver。</p><p>如果未来有一天，我们的视频会议有了Mac端，这个时候Job设计怎么变化？</p><p>我们可以想一下，不论是Mac还是Windows、Linux，它们都只是视频会议的不同载体，但上面实现的业务是一样的。也就是说，我们上面基于业务做的Job设计依然有效。</p><p>这时，可以把JoinMeetingDesktop变成抽象Job，增加3个子Job：JoinMeetingWindows、JoinMeetingMac、JoinMeetingLinux，它们是对JoinMeetingDesktop的实现。当JoinMeetingDesktop运行的时候，会把子Job都运行一遍，Windows、Linux、Mac就都得到了测试验证。</p><p><img src=\"https://static001.geekbang.org/resource/image/1a/7e/1a01495793cf2a01b5ee4057bc93937e.jpg?wh=1920x1300\" alt=\"图片\"></p><h2>小结</h2><p>今天我们通过视频会议的Job设计，把一个分布式多端协作的交互场景，分解成一个个简单可开发任务，并且分析了子任务的Input和Output。其实，这个分解过程就是我们的自动化测试设计要做的工作。不难发现，用好我们的Job模型，这个分解工作就能迎刃而解了。</p><p>不过，之前我们的Job模型不支持分布式，所以，我们对Job模型又做了扩展，增加了Host属性，支持分派Job到不同的主机上运行。在Host属性可以增加多台主机名，那么就可以实现一个Job发送到多台机子上运行。</p><p>还是按照Job设计的思路，我们分而治之，把视频会议自动化的整个任务分解成了四个功能区：初始化Meeting，演示功能区，聊天功能区和退出Meeting。然后，我们再对每个功能下钻细化，直到可执行的实例Job。</p><p>视频会议自动化测试中，还会涉及一些具体的技术实现，比如图像比较，自开发算法和使用主流工具两种思路，我都为你做了简要分析。</p><p>自开发算法的问题是，开发打磨出一个可用的图像相似度算法会耗费人力，说白了就是投产比并不划算。</p><p>所以，我们可以借鉴业界已有的技术，比较成熟的两款工具是Applitools和Sikuli。我用一个例子来介绍Applitools AI识别图像的实现思路，你也可以把它应用在你的工作中去。</p><p>在设计过程中，我们也发现了执行效率可优化的空间。没有依赖关系的Job们，我们可以通过<strong>并发线程</strong>来执行它们，只有当存在依赖关系时，才需要顺序执行。通过这样一个机制上的优化，我们就能把视频会议自动化的执行时间缩短将近一半。</p><p>到这里，基于Job模型的自动化测试设计思路和案例就讲完了，相信你收获了一种全新的设计思路和实现方法。但在收获的同时，关于怎么落地，你一定还会有不少疑问，我完全能够理解。</p><p><strong>一个新的设计方法，需要很多配套的实现。</strong>比如，围绕着OpenAPI设计就有一个工具生态圈：有OpenAPI规范来书写接口文档，有Swagger来可视化展现接口，有OpenAPI generator来自动生成代码。下一讲，我们就一起学习一下基于Job模型的框架实现和代码例子，敬请期待。</p><h2>思考题</h2><p>今天讲的视频会议的自动化测试场景设计，你觉得还有没有什么重要功能，是我们没有进行测试设计的？怎么用Job模型来完成这个功能呢？</p><p>欢迎你在留言区和我交流互动，也推荐你把今天学的内容分享给更多同事、朋友，一起应用Job模型来完成各种自动化测试设计。</p>","neighbors":{"left":{"article_title":"22｜设计实战（二）: 一个全周期自动化测试Pipeline的设计","id":516198},"right":{"article_title":"24｜启动运行：基于Job模型的框架实现和运行","id":517969}}},{"article_id":517969,"article_title":"24｜启动运行：基于Job模型的框架实现和运行","article_content":"<p>你好，我是柳胜。</p><p>今天是设计篇的最后一讲，从第十七讲到现在，我们围绕Job元数据模型讲了很多内容。毕竟，这个Job模型设计，是一个新的设计方法论，需要从模型推演、设计方法、不同场景的设计案例等各个方面来学习理解。虽然过程中有点像盲人摸象一样，但我们最后终于勾勒出了一个全面的理解。</p><p>不过，一种新的设计方法想要生根发芽，不光要在理论层面讲得通，还要让使用者轻松快速地掌握它，并且学以致用。沿着这个思路，今天我们把基于Job模型的框架实现梳理一遍，明确都需要实现什么，以及怎么实现。</p><p>经历了这个思考过程，对你把Job模型在团队里推广和使用大有帮助，而且对你去设计、推广自己的框架也同样适用。一个设计方法要在工作里落地，它就必须要和日常的设计活动，开发维护活动结合在一起，有3个关键场景：</p><p>1.自动化测试设计的评审和交流（设计文档化）；<br>\n2.自动化测试设计和实现的一致性维护（设计代码化）；<br>\n3.自动化测试的报告呈现（结果可视化）。</p><h2>Job设计文档化</h2><p>我们先从设计评审和交流说起，这离不开一份清晰易读的设计文档。在软件技术迅猛发展的今天，开发人员可以用Swagger来表达API的接口设计，运维人员可以用YAML来表达部署的方案，但<strong>测试人员却没有一个表达自动化设计的文档格式，真是遗憾</strong>。</p><!-- [[[read_end]]] --><p>而现在，有了Job模型后，作为自动化测试人员，你可以骄傲地说，我的自动化测试也有设计文档！</p><p>文档化怎么做呢？在前面几讲的案例中，我们都是用画六边形的方法来表达Job树的设计。这种方法有点像画思维导图，一边想，一边展开，帮助你迅速完成设计。但设计是最终要输出成文档的，尤其我们要版本化管理设计文档的时候。</p><p>这个设计文档用什么格式比较好呢？我们需要把Job树图转成可读的数据文件，Job树是一个树形结构，那我们继续思考，树形结构适合用什么格式来表达？业界里的树形结构数据表达格式已经非常多了，XML、Json、YAML都能胜任。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/de/bcdf661a2f885ab48550e2ac19dabdde.jpg?wh=1920x627\" alt=\"图片\"></p><p>用XML作为载体，展现视频会议（视频会议案例可以回顾<a href=\"https://time.geekbang.org/column/article/517078\">第二十三讲</a>）的Job设计树，也就是TestJobFile.xml的例子如下：</p><pre><code class=\"language-xml\">&lt;TestJob name=\"VideoMeeting\" description=\"Meeting Full automation\"&gt;\n  &lt;TestJob name=\"ScheduleMeeting\"&gt;\n    &lt;TestJob name=\"createMeeting\" host=\"10.0.0.1\" driver=\"Selenium\"&gt;\n      &lt;JobOutput name=\"MEETING_URL\"/&gt;\n    &lt;/TestJob&gt;\n    &lt;TestJob name=\"JoinMeetingDesktop\" host=\"10.0.0.2\" depends=\"createMeeting\" driver=\"WinappDriver\"&gt;\n      &lt;JobInput name=\"MEETING_URL\"/&gt;\n    &lt;/TestJob&gt;\n    &lt;TestJob name=\"JoinMeetingAnroid\" host=\"10.0.0.3\" depends=\"createMeeting\" driver=\"appium\"&gt;\n      &lt;JobInput name=\"MEETING_URL\"/&gt;\n    &lt;/TestJob&gt;\n  &lt;/TestJob&gt;\n  &lt;TestJob name=\"VerifyJoin\" depends=\"ScheduleMeeting\" driver=\"Restassure\"&gt;\n  &lt;/TestJob&gt;\n  &lt;TestJob name=\"PresentMeeting\"&gt;\n    &lt;TestJob name=\"presentPPT\" host=\"10.0.0.1\" driver=\"Selenium\"&gt;\n      &lt;JobOutput name=\"TR_START_TimeBetweenPresentAndView\"/&gt;\n    &lt;/TestJob&gt;\n    &lt;TestJob name=\"viewPPT\" host=\"10.0.0.2\" driver=\"WinappDriver\"&gt;\n      &lt;JobOutput name=\"TR_END_TimeBetweenPresentAndView\"/&gt;\n    &lt;/TestJob&gt;\n    &lt;TestJob name=\"viewPPT\" hot=\"10.0.0.3\" driver=\"appium\"&gt;\n      &lt;JobOutput name=\"TR_END_TimeBetweenPresentAndView\"/&gt;\n     &lt;/TestJob&gt;\n  &lt;/TestJob&gt;\n</code></pre><p>当然，你用YAML和Json也可以达到同样的表达效果。</p><p>有了设计文档后，自动化测试设计就有了自己的表达方式。这就像你和别人聊天的时候，都在说流行的名词“DevOps”，但有人认为DevOps是CICD，有人认为Devops是微服务架构，你甚至无法确定你们讨论的是同一件事。</p><p>这个时候怎么办？你需要定义一个内容的骨架，从三个方面：人员组织、流程、技术栈来谈论DevOps。这样，团队的表达方式统一了，交流就会高效了。</p><p>所以，能让一份设计文档被不同的人理解，还需要有一致的格式规范。这个格式规范的目的是<strong>让所有人都用同样的关键词</strong>，比如Input、Output、TestData等，都遵守同样的约束，比如同级依赖，实例对抽象的实现等等。</p><p>怎么来验证这些格式规范？ 你可能已经想到了，XML和YAML用schema验证，Json用JsonValidator来验证。</p><p>下面我们结合例子来看看，如何用一个TestJobSchema.xsd文件，来验证TestJobFile.xml是否遵循了关键词使用规范。</p><pre><code class=\"language-xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;xsd:schema xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" \n        targetNamespace=\"http://www.example.com/jobSchema\" \n        xmlns=\"http://www.example.com/jobSchema\" \n        elementFormDefault=\"qualified\"&gt;\n        &lt;xsd:element name=\"TestJob\"&gt;\n            &lt;xsd:complexType&gt;\n                        &lt;xsd:choice minOccurs=\"0\" maxOccurs=\"unbounded\"&gt;\n                            &lt;xsd:element name=\"TestJob\" type=\"TestJobType\" minOccurs=\"1\" maxOccurs=\"unbounded\"/&gt;\n                            &lt;xsd:group ref=\"TestJobGeneralElements\"/&gt; \n                            &lt;xsd:group ref=\"TestJobElementsGroup\"/&gt;                 \n                        &lt;/xsd:choice&gt;   \n                        &lt;xsd:attributeGroup ref=\"TestJobGeneralAttributes\"/&gt;                        \n                &lt;/xsd:complexType&gt;          \n        &lt;/xsd:element&gt;\n        &lt;xsd:attributeGroup name=\"TestJobGeneralAttributes\"&gt;\n            &lt;xsd:attribute name=\"name\" type=\"xsd:string\" use=\"required\"/&gt;\n            &lt;xsd:attribute name=\"description\" type=\"xsd:string\" use=\"required\"/&gt;\n            &lt;xsd:attribute name=\"depends\" type=\"xsd:string\" use=\"optional\"/&gt; \n            &lt;xsd:attribute name=\"host\" type=\"xsd:string\" use=\"optional\"/&gt; \n            &lt;xsd:attribute name=\"timeout\" type=\"xsd:nonNegativeInteger\" use=\"optional\"/&gt; \n      &lt;/xsd:attributeGroup&gt;\n      &lt;xsd:group name=\"TestJobGeneralElements\"&gt;\n          &lt;xsd:sequence&gt;\n            &lt;xsd:element name=\"JobInput\" type=\"JobInputType\" minOccurs=\"0\" maxOccurs=\"unbounded\"/&gt;  \n            &lt;xsd:element name=\"JobOutput\" type=\"JobOutputType\" minOccurs=\"0\" maxOccurs=\"unbounded\"/&gt;   \n            &lt;xsd:element name=\"TestData\" type=\"TestDataType\" minOccurs=\"0\" maxOccurs=\"unbounded\"/&gt; \n            &lt;/xsd:sequence&gt;      \n        &lt;/xsd:group&gt;    \n&lt;/xsd:schema&gt;\n</code></pre><p>有了Schema文件后，就能保证不同的人设计出来的Job都遵循同样的规范。然后，就可以在团队里开展设计评审、版本升级维护等等自动化测试设计相关的活动了。这样，自动化测试设计就在团队激活了！</p><h2>Job设计代码化</h2><p>按照软件生命周期，设计完成后，工作重心就需要转移到开发阶段了。传统开发模式里，这个过程一般是手工的，在设计阶段设计好多少个模块，每个模块的出入口是什么，再交由开发人员根据设计文档来实现代码。</p><p>这种模式在软件迭代初期是奏效的，因为设计和代码是一致的。但随着时间推移，迭代次数增加，不断修改bug，团队成员也换了几波后，代码和设计就会产生越来越大的偏差，这就是我们常说的“<strong>架构腐化</strong>”问题。</p><p>自动化测试开发本质上也是软件开发活动，当然也存在设计和代码脱节的风险。如果不解决掉这个问题，自动化测试设计最后还是会成为形式工程。试想一下，设计做了，文档也有了，但代码就是不按照设计来做，长此以往，谁还会注重自动化测试设计呢？</p><p>怎么办？解决架构腐化是个棘手的问题，幸运的是，现在业界也认识到了这一点，推出了很多方法论和技术。其中一个方法，就是通过设计直接生成代码来保持两者的一致性。</p><p>我在<a href=\"https://time.geekbang.org/column/article/511982\">第十六讲</a>里讲过Cucumber框架，利用Cucumber，测试设计的Feature文件可以直接转化成Java代码，Feature文件里的关键字Given，When，Then以注解的方式加在函数声明上。这就是设计转成代码的方法之一。</p><p>那TestJob该怎么做，才能让设计和代码保持一致呢？</p><h3>Auto Gen Auto：注解驱动机制</h3><p>首先我们可以采用Cucumber的思路，把Job模型的关键字与代码概念做一个映射。比如Java代码，Job结构和Class结构，有以下的对应关系。</p><p><img src=\"https://static001.geekbang.org/resource/image/3e/34/3eff1a2d4e40036c870f62675e2d6534.jpg?wh=1920x947\" alt=\"图片\"></p><p>然后，再用上<a href=\"https://time.geekbang.org/column/article/499339\">第五讲</a>里Auto Gen Auto提到的思路，把模版自动化生成测试代码的方法，用到这里。</p><p>如下图所示，由Job Parser来解析Job树，然后将解析后的数据注入到代码模版里，就可以实现Job设计树到代码的自动转换了。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/78/75741ef381eeec4a7a0e12ca11a74178.jpg?wh=1920x969\" alt=\"图片\"></p><p>模版技术你可以使用Mustache（链接在<a href=\"https://en.wikipedia.org/wiki/Mustache_(template_system)\">这里</a>）。</p><p>大致过程是这样的。首先，我们定义好后缀名为TestJob.mustache的代码模版文件，如下所示：</p><pre><code class=\"language-plain\">package {{package}};\nimport java.util.*;\n@dependency({{dependency}})\npublic class {{classname}} exetnds TestJob{\n  {{#Inputs}}\n  @Input\n  public String {{input}};\n  {{/inputs}}\n  {{#Outputs}}\n  @Output\n  public String {{output}};\n  {{/Outputs}}\n  public run(){\n    //TODO\n  }\n}\n</code></pre><p>然后通过Job Parser解析Job文件，向Mustache模版注入变量值，就可以生成代码了! 你可以脑补一下生成的代码。<br>\n这种Auto Gen Auto方式解决了从Job设计到自动化测试代码的转换过程，自动化测试开发人员只需填充测试逻辑的实现代码即可。</p><p>Job怎么跑呢？这个时候JobRunner需要解析上面代码里的@Dependency、@Input、@Output的一个个注解，进行计算，得出执行路径，然后在运行时把数据注入到框架里。整个运行过程是由注解驱动的。</p><p>其实，业界流行的开发框架Spring Boot，就是这种注解驱动的机制，你可以作为参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/63/38/635fa66373a33f7941e2a8867928bc38.jpg?wh=1920x682\" alt=\"图片\"></p><p>这个机制的优点是强大，扩展性强，但另外一面是，对Job Parser和Job Runner的开发技术要求高。再说直白点，就是这样的自动化测试框架开发投产比会比较低，只有稳定的测试组织，才能负担得起这样的开发投入和回报周期。</p><p>另外还有一种实现思路更轻量，Job设计文件就是可运行的文件，我们来看看。</p><h3>设计文件即运行文件</h3><p>如果你用过Apache Ant这个构建管理工具的话，你一定对编写Build.xml不会陌生。编写build.xml就是做任务设计，先创建Compile、Package、Test的Task，声明它们的调用关系，然后运行下面的命令：</p><pre><code class=\"language-plain\">ant -f build.xml \n</code></pre><p>运行命令后，build.xml里定义的task就启动运行了。<br>\n对于TestJobFile.xml，我们也可以采用相同的思路来处理。如果想让TestJobFile.xml里的TestJob能够启动运行，就需要把实例Job的运行细节填充进去。</p><p>比如一个通过QTP工具实现的实体Job，它要包含JobOutput、TestData数据源的类型和位置路径，还有引用的Library文件和运行的测试脚本路径。像下面这样：</p><pre><code class=\"language-xml\">&lt;QTP&nbsp;&nbsp;name=\"QTP_ScriptMode\"&nbsp;description=\"QTP Script mdoe\"&nbsp;depends=\"Java_Init\"&gt;\n&nbsp;&nbsp;&lt;JobOutput name=\"TicketNo\"/&gt;\n  &lt;TestData&nbsp;type=\"xls\"&nbsp;location=\"testscripts\\QTP\\UILabelData\\data_UI_Flight.xls\"/&gt;\"\n  &lt;Lib&nbsp;location=\"common\\lib_app\\lib_flightdemo_login.vbs\"/&gt;\n&nbsp;&nbsp;&lt;Lib&nbsp;location=\"common\\lib_app\\lib_utility.vbs\"/&gt;\n&nbsp;&nbsp;&lt;Run&nbsp;path=\"testscripts\\QTP\\QTP_ScriptMode\"&gt;\n&lt;/QTP&gt;\n</code></pre><p>而一个Selenium实现的实体Job，要包含Testdata，还有Java的ClassPath、TestCaseClass和TestMethod。</p><pre><code class=\"language-xml\">&lt;Selenium&nbsp;name=\"selenium_demo\"&nbsp;description=\"Test calc\"&nbsp;&nbsp;depends=\"\"&nbsp;&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;&lt;TestData&nbsp;type=\"xml\"&nbsp;&nbsp;location=\"selenium\\config.xml\"/&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;&lt;JobInput&nbsp;name=\"$number1\"/&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;&lt;ClassPath&nbsp;location=\"selenium\\selenium-java-client-driver_self_extended_oracle.jar\"/&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;&lt;ClassPath&nbsp;location=\"selenium\\my.jar\"/&gt;\n    &lt;ClassPath&nbsp;location=\" selenium\\selJava.jar\"/&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;&lt;SelTestCase&nbsp;path=\"selJava\"&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;SelTest&nbsp;name=\"testSelJava \"/&gt;\n&nbsp;&nbsp;&nbsp;&nbsp; &lt;/SelTestCase&gt;\n&lt;/Selenum&gt;\n</code></pre><p>通过代码我们可以看到，每种工具的实体Job都有一个Job定义的格式，那么，我们每完成一个Job的开发，就把这个Job的运行信息更新到TestJobFile.xml里去。</p><p>像Ant运行build.xml,  Maven运行pom.xml一样，用JobRunner直接运行TestJobFile.xml, 运行命令如下，这样就能启动自动化测试执行了。</p><pre><code class=\"language-plain\">JobRunner -f TestJobFile.xml \n</code></pre><p>这样做主要有两个好处。第一，设计文件就是运行文件。Job开发完，需要更新设计文件，直接就可以运行，不存在设计和开发脱节的问题；第二，每个实体Job的运行都由JobRunner来驱动。这样一来，JobRunner就能判断出，Job有没有遵循它在设计文件里的承诺的Output，一旦Job的实现有了偏差，立刻就能发现。</p><p>我把TestJobFile.xml样例更新到了专栏的GitHub里，你可以点击<a href=\"https://github.com/sheng-geek-zhuanlan/autmation-arch/tree/master/module3\">这里</a>查看。</p><p>当然这个思路也有缺点，当TestJob定义过多的时候，这个TestJobFile.xml就会过于臃肿，难以维护。不过这个时候，也有解决办法，我们可以让TestJobFile支持引用和嵌套，就像build.xml和Maven里的pom.xml一样。</p><h2>Job运行报告</h2><p>文档化和代码化的问题说完了，我们再来聊聊测试报告。测试报告是自动化测试最终的结果呈现。怎样利用Job模型来优化测试报告，让报告更好地服务各方人员呢？</p><p>我们先看一下传统的自动化测试报告是什么样的，它们包含什么信息，又是怎么展现的。Junit框架产生的报告是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/58/97/584b5bee8ec157891d24d0c256f81a97.jpg?wh=1920x1228\" alt=\"图片\"></p><p>发现了么？它报告的结构跟TestCase结构是一样的。在设计时，一个TestCase下有多个Test，运行产生报告后，结构保持不变。单个Test变成详细报告，有结果状态信息、检查点，还有Log输出；而TestCase变成汇总报告，聚合Test的执行结果，有成功率、失败率和执行时间等等。</p><p>再来看一下Jira产生的测试报告：</p><p><img src=\"https://static001.geekbang.org/resource/image/12/bb/1287fbb5266dfe852b5b0d115ff951bb.jpg?wh=1920x1113\" alt=\"图片\"></p><p>相比Junit报告，Jira的测试报告又丰富了一些信息。多了一个执行历史的维度，能看到一个TestCase执行的历史记录；还有TestCase和需求的关联关系，从这里就能得出测试对需求的覆盖度。</p><p>看了Junit和Jira的测试报告后，你会发现传统的测试报告里的数据定义不一样，展现风格也不一样。为什么会有这么大差别呢？</p><p>测试报告形式多样，但不管测试报告有多少种，有一个基本常识是不变的：<strong>测试报告是给人看的</strong>。只不过，不同角色的人，希望从测试报告里得到的信息是不一样的。</p><p>测试管理者想要确定发布的信心，在一个发布周期内测试对需求的覆盖度，关键Bug的数量。</p><p>测试经理关注测试设计的质量和测试执行的效率，比如测试案例发现Bug的数量、自动化测试执行的时间和假失败的次数等等。</p><p>自动化测试开发人员想要快速诊断错误，自然关注点就放在了出错的Log、抓图，还有出错时的现场信息等等。而从测试报告里，开发人员也希望得到错误定位到开发模块的信息，比如出错时间、服务端的执行链条、服务端log等等。</p><p>你能看到，不同角色的关注点不同，而且这些关注点随着项目的进展，也会发生变化。而我们通过Jira和Junit的例子看到，传统的测试报告数据和格式都很固定。</p><p>那么，Job模型能不能满足这些需求呢？既能产生多样数据的报告，又能给不同的角色看，还能够灵活扩展。</p><p>听起来有点复杂和烧脑，不过，如果把这个问题当作一个数据工程来处理，就能把复杂问题简单化，拆解成更容易解决的问题。我们把它分解成两个简单问题：第一，<strong>如何获得这些数据</strong>；第二，<strong>这些数据如何聚合和展现，为每个角色提供感兴趣的信息</strong>。</p><p>先解决第一个问题，也就是数据来源。Job模型在设计态和运行态下都是一棵Job树，所有的数据也都依附在这棵Job树上。</p><p>那数据就有两个来源：一个来源是在设计态下加到Job模型里的数据。比如测试Job和需求的关联，这在Job设计时，可以作为Job的一个属性RequirementID添加到Job模型里；另一个来源，是在运行态下采集到的数据。比如执行的检查点、测试Log、屏幕抓图等等，采集到这些数据之后，也可以添加到Job树里。</p><p>你可以想象这样的一个场景，Job树从设计态到运行态，是这棵树上的果实越来越多的过程。这里的果实就是数据，刚开始时只有设计数据，后来运行时数据也加进来了。参看下图，更有助于你加深印象。</p><p><img src=\"https://static001.geekbang.org/resource/image/3d/91/3dff8d185d218705001e0089d463bb91.jpg?wh=1920x834\" alt=\"图片\"></p><p>运行完毕后，这棵Job树就结满了果实。虽然你不知道谁会要什么样的水果拼盘，但你能通过根Job能遍历所有的果子，能做什么样的菜，自然就能心里有数了。</p><p>在每个Job节点执行相应的聚合算法，就可以生成不同层级的报告了。抽象Job可以生成汇总报告，而实例Job生成诊断报告。</p><p>现在，我们再把Job的设计态、运行态、结果态理一遍。通过下图，你会更清楚地看到这个数据采集到聚合的过程。</p><p>现在，你可能会问，我有了数据之后，到底怎么展现呢？这就涉及到数据工程的最后一个环节——数据可视化了。</p><p>传统的测试报告，会通过HTML展现。我们的TestJobFile.xml是XML文件，最后结果态Job树也是一个XML文件，你可以考虑通过XML+XSLT，来实现数据的处理和可变样式呈现。</p><p>如今的数据工程发展日新月异，自动化测试报告可以看作是数据工程的一个细分环节。这块的解决方案有很多，比如Grafana、Kibana、Tabelau等等。我在下一模块“数据度量”最后一讲里为你分享这个问题的实现思路和方法，敬请期待。</p><h2>小结</h2><p>为了把Job设计方法落地到你的日常工作活动中，我们用一讲的篇幅，依次讲解了将设计文档化、代码化和生成运行报告的方法。</p><p>怎么去评审和交流Job设计方案？这需要我们把Job设计文档化，而且要让评审高效，我们还要考虑好用什么数据格式呈现。</p><p>XML，YAML还是Json？这三种方式都能通用互相转换，找出适合你的团队的那一种。另外，还要能自动验证设计文档的合规性，这里的可用方法有Schema验证，还有自开发Validator代码的办法。</p><p>怎么能让Job设计和代码实现保持一致，不发生脱节呢？这里我们要考虑两个方向，一个是从设计到代码，另外一个是从代码到设计。</p><p>我们探讨了Auto Gen Auto的实现思路，还了解了一个模版利器<strong>Mustache</strong>，你可以用它产生任何类型的代码：Java、Go，JavaScript，甚至HTML等等。</p><p>另外还有一个轻量的思路，就是设计和执行合二为一，我们参考了Ant、Maven的运行机制，把TestJobFile.xml作为JobRunner的一个执行配置文件，直接就能跑起来。这样既能验证设计，又能验证代码。</p><p>最后，测试报告作为自动化测试最重要的成果，怎么能把价值充分展现呢？我们从传统报告里分析了需求之后，加以突破升级，用数据工程的思维理了一下自动化测试运行过程。</p><p><strong>这个过程，本质上是一个设计数据、采集数据、聚合数据和展现数据的数据工程。</strong>用好Job模型的设计树、运行树、结果树，就能满足这个要求，设计树里设计数据，运行树采集数据，结果树聚合数据。</p><p>通过今天的学习，想必你也感受到了，在自动化测试领域，创新并落地一个新的设计方法，要比实现某个垂直领域的工具自动化要复杂很多，要考虑的点也多，也需要不小的开发工作量。</p><p>感谢你学习完这一模块。为了奖励你的耐心，也为了节省Job模型在你项目落地的时间，我把Job模型框架的代码上传到 <a href=\"https://github.com/sheng-geek-zhuanlan/JobFramework\">GitHub</a> 里，你可以参考，按需取用。</p><h2><strong>思考题</strong></h2><p>在你的项目里，自动化测试报告都有哪些角色要看？报告里都展现了哪些数据，是什么格式呈现的？</p><p>欢迎你在留言区跟我交流互动。如果觉得今天讲的方法对你有启发，也推荐你分享给更多朋友、同事，一起学习进步。</p>","neighbors":{"left":{"article_title":"23｜设计实战（三）: 一个分布式多端视频会议自动化测试设计","id":517078},"right":{"article_title":"25｜找准方向：如何建立有效的测试度量体系？","id":518825}}},{"article_id":518825,"article_title":"25｜找准方向：如何建立有效的测试度量体系？","article_content":"<p>你好，我是柳胜。</p><p>提到数据度量、数据驱动这些词，我们并不陌生。无论是管理领域用来追踪业绩的KPI、OKR，还是商业中用数据来做最优决策的Business Intelligence，都用到了数据驱动的方法。</p><p>而在我们IT领域，管理大师彼得·格鲁克就曾说过：“You can’t improve what you cannot measure”，意思是如果你不能度量一件事情，你就不知道怎么去优化改进它。</p><p>既然数据驱动方法论已经深入人心，自动化测试领域也自然不会错过它。我们遇到的很多困境，其根源都要归结到数据不可见上，或者数据虽然可见，我们却不知道如何科学利用。比如常见的困难就有：管理者对自动化测试没什么信心，所以投入并不稳定；测试设计者也不清楚拿到了资源，自动化测试应该投入到哪个领域，分别投入多少才合适。</p><p>所以，在度量篇我们的目标就是把数据度量方法用起来，这样才能真正通过数据驱动测试的改进和提升。</p><p>前面Job模型的学习中，我们通常都是自顶向下，按照现状、目的、任务拆解，一直细化到实现层面。这节课思路也一样，自动化测试数据度量的关键问题如下：</p><p>1.为什么需要数据度量？<br>\n2.围绕目标，怎么建立有效的测试度量体系？<br>\n3.测试度量指标设计要遵循哪些原则？</p><!-- [[[read_end]]] --><p>解决了这几个问题，度量活动的整体脉络就有了，实际工作里才能有的放矢。</p><h2>为什么需要数据度量？</h2><p>在我看来，数据度量是自动化测试走向成熟阶段的标志。想要弄清楚为什么要花费时间精力，来做数据度量这件事，我们不妨追本溯源，从自动化测试的发展阶段说起。</p><p>如果你的团队自动化测试刚开始起步，有几个繁琐易错的测试场景需要自动化，比如，要测试在不同平台下安装软件客户端，要生成测试数据。</p><p>这时，你发愁的是，怎么帮助团队掌握如何使用工具，把手工案例转化成自动化测试案例。我管这个阶段叫做“<strong>自动化初识阶段</strong>”。</p><p>这个阶段的特点是这样的：自动化测试的需求是偶发的、临时起意的，一两个人负责自动化测试的实施，选型工具也没啥策略，开发出来的测试脚本能跑起来就行。</p><p>而随着产品规模扩大，测试案例越来越多，对自动化测试的需求也会增加，不再局限于一两个特定场景，开始建立冒烟测试自动化集合，多个团队小组都开始实施自动化测试。</p><p>到了这个时候，你会看到不同的测试需求，也看到了更多的自动化测试工具和框架，相应也会涉及到工具选型、自动化测试环境搭建的工作。我管这个阶段叫做“<strong>自动化乐观阶段</strong>”。</p><p>面对自动化测试的投入增大，领导的殷切期望，团队的学习热情，你可能心里会生出一点担忧“我们这样投入，自动化测试能带来质量提升么，能节约成本么”。</p><p>为了控制成本，你开始关注团队间工具能否通用、代码能否复用。不过，还是不清楚自动化测试做到什么程度才合适。</p><p>我再次引用加特纳技术成熟度曲线，它会告诉我们在自动化测试乐观阶段之后，会发生什么。</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/9b/d08c192d60b0e0274d6da907012b7a9b.jpg?wh=1920x995\" alt=\"图片\"></p><p>自动化测试乐观阶段的特点是，团队热情高，投入大，甚至有的测试组织都提出100%自动化的目标。但理想和现实总会有差距，当自动化实际的产出，没有达到对自动化测试预期的时候，我们就很容易进入到“自动化测试悲观阶段”，觉得自动化测试是糊弄人的，华而不实，还不如用一个自动化测试工程师的成本，去多招两个手工测试的人手。</p><p>这种认知的反差并不稀奇，往往就发生在同一个人身上，同一个测试组织里。前面乐观情绪有多高，后面悲观情绪就有多低落。</p><p>结合上面的曲线图，经历了乐观阶段和悲观阶段，我们怎么走向“自动化测试的成熟”阶段呢？这是一个很关键的门槛，推开了这扇大门，就进入到一个新的世界，对自动化测试的认识成熟理性，实践稳步提升，进入到一个正反馈的循环中。</p><p>其实，开启这扇大门的钥匙就是数据驱动。当乐观和悲观都解决不了问题的时候，数据才能让我们认清现实，从而找到解决问题之道。</p><p><img src=\"https://static001.geekbang.org/resource/image/af/1d/afb43b2f2d2e031966e64275d913ca1d.jpg?wh=1920x1204\" alt=\"图片\"></p><h2>度量什么？</h2><p>确立了数据度量的必要性，接下来就到了细化方向、建立指标的环节。</p><p>度量什么？这是一个好问题。建立一个度量指标，相当于在团队立了flag，如果这个flag立得不对，就会误导团队。</p><p>我也见过有些团队上来就立“军令状”，发誓单元测试覆盖率要达到80%，然后一顿操作猛如虎，开发测试全员加班加点，心中叫苦。也许后面覆盖率确实达到了定下的指标，但单元测试自上线起，从未捕捉到回归Bug，软件交付质量也不见提升。</p><p>可见度量flag的建立是一个技术活，最终要看指标对软件质量的提升有没有帮助。如果制定得不合理，就会竹篮打水一场空。怎么合理设立度量指标呢？我们需要先拆解软件质量包括哪些方面。</p><h3>交付质量度量</h3><p>软件质量体系，可以分为交付质量和内建质量。交付质量是结果，内建质量是过程。做好过程是为了得到好的结果。像我们前面做好设计，写好代码与测试案例，科学实施自动化测试，它们都是内建质量活动，最终都是为了提升交付质量。</p><p>所以，我们首先要把交付质量的度量先定下来，就相当于抛出一个锚，而内建度量就可以向它看齐了。</p><p>对于交付质量，我们最常用的度量指标就是生产环境Bug数量。这个也很简单直观，把软件交付给用户之后，用户发现的bug越多，说明质量越差，能做到0Bug，那说明质量最好。</p><p>但生产环境的Bug数量，跟发布多少功能以及发布速度都有关系。我用一个质量三角图来说明这三者之间的关系：</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/10/d039e7b398f981c34938e1b405216710.jpg?wh=1920x1043\" alt=\"图片\"></p><p>这三个因素中有一个因素发生变化，就会影响其他两个因素。比如，老板有一天说，为了抢占市场，我们这个产品要提前10天发布，这时三角形的时间发生了变化，那就要影响三角形的其它两边，要么缩小功能的范围，要么就要接受Bug增多的质量风险。</p><p>而我们交付质量的目标是在不损失其他两个因素的情况下，来提升质量。所以，我们在度量交付质量的时候，这三个因素都需要观测。我们后面就可以用这三个指标来代表交付质量。</p><p>1.交付速度，是版本迭代的速度，我们可以用发布的周期来度量。</p><p>2.交付范围，是新功能的规模，可以用新增代码行数来度量。</p><p>3.质量，用生产环境发现的Bug数量来度量。</p><p>从交付质量的这三个度量指标中，我们可以看到，自动化测试是能够提升这三个指标的。自动化测试的程度高，覆盖的功能越广，可以大面积回归，质量就能得到保证。而自动化测试执行速度也快，这样交付时间也会缩短。</p><p>交付质量的度量指标确定了后，相当于目标确定了。我们下面来看一下内建质量该怎么度量。</p><h3>内建质量度量</h3><p>交付质量是果，内建质量是因。而且是多因一果。软件研发又是一个复杂的活动，里面任何一个环节没做好，都有可能影响最终质量。</p><p>俄国文豪托尔斯泰曾经说过一句流传很广的话“幸福的家庭是相似的，不幸的家庭各有各的不幸”，换到软件质量领域也一样。你可以想想木桶理论。木桶是由多个木板组合而成的，木桶能装多少水，通常取决于短板。因为只要有一个短板，木桶的水就会流出来。</p><p>内建质量活动就相当于这些木桶板，内建质量的提升，就是要找出那块短板，把它加长。但内建质量的木板是很多的，大方面有人员、流程、技术栈，细节上有代码的实现，以及配置文件的读写等等。在这么多木板中，你怎么找出这块短板呢？</p><p>学过Job模型后，你应该掌握了一个把复杂问题简单化的解决思路。那就是先做划分，再定位实现。</p><p><img src=\"https://static001.geekbang.org/resource/image/9b/d8/9b6915247e7c915ca79313bdf67371d8.jpg?wh=1920x744\" alt=\"图片\"></p><p>内建质量可以划分成四个维度：需求质量、设计质量、开发质量和测试质量。每个维度展开以后，都是一组度量指标。</p><p>我们今天以测试质量为例，来研究一下这个领域的度量指标的设定。掌握这个思路，其他领域你也能触类旁通。</p><h2>测试活动的度量</h2><p>在设计测试活动度量之前，让我们以终为始，先想清楚，测试活动的结果输出是什么？什么样的输出能够促进交付质量的提升？换句话说就是，我们怎么评价一个测试活动是“很棒”的，还是“很差劲”的呢？</p><p>你可能想到，交付质量里有一个指标“生产环境发现的Bug数量”，把这个指标作为测试有效性的评价指标不就可以了么？</p><p>如果这样想，其实你就进入了一个“测试组织对最终质量负责”的思维误区。因为Bug的绝对数量增加，因素也很多。</p><p>比如，在这个Release 项目新加的功能多，开发代码变动大，引发Bug就多，这不是测试团队能控制的，既然不能控制，当然就没办法对它的结果负责，你也不能用它来驱动测试活动的提升。</p><p>怎么评价测试的有效性呢？这时我想提醒你，注意度量设计的一个原则，<strong>尽量不要以绝对数字来判断好和坏</strong>。</p><p>可是不用绝对数字，应该用什么呢？结合我的实践经验，为了衡量测试活动的有效性，可以设定以下指标，你可以做个参考：</p><ul>\n<li>Bug泄漏率=生产环境的Bug数量/Bug总数量</li>\n<li>冒烟测试Bug泄漏率=生产环境的关键Bug/关键Bug总数量</li>\n<li>测试需求覆盖率=被测试的需求条数/总需求条数</li>\n<li>测试执行效率=自动化测试的案例/总共测试案例</li>\n</ul><p>这几个指标都有一个共同的特点，它们都是用两个数字的比率来做度量，而不是用一个绝对数字。数字的比率可以消除绝对数字的笼统性，比如，影响Bug总数的因素很多，所以你很难用Bug总数来说事，但是把Bug归因到各个过程所占的比例，像是需求Bug比、设计Bug比、代码Bug比等等，就能说明一些问题了。</p><p>这个思路，不光用数字的比率，我们也可以用数字的差做度量，这种度量方法我会在第二十七讲里详细展开。</p><p>另外，上面的这四个指标之间也是有逻辑关系的，它评价测试的效果，也是从测试的范围（测试需求覆盖率），时间（执行效率），质量（Bug泄漏率），这三点支撑的一个度量体系。</p><p><strong>避免指标单一化</strong>，其实也是一个度量设计的原则，也就是说，不要从单一指标来评价。尤其有的组织习惯于把指标和绩效挂钩，可是实际上往往事与愿违，容易引发执行者不假思索地机械化工作，甚至弄虚作假。</p><p>自动化测试作为测试活动下面的一个子活动，我们该怎么度量呢？随着度量指标从上到下的细化，越细化，目标就越具体，也更加问题导向，便于落地。</p><p>到这里，测试活动的度量框架我们就建立了。度量篇的后面几节课，我会从单元测试度量设计，自动化测试度量设计，自动化测试关键问题可信性度量几个方面深入解析，带你系统掌握自动化测试的度量思路与方法。</p><h2>小结</h2><p>现在来回顾一下今天学习的内容，学完了正文，开篇的三大问题想必你也找到了答案。</p><p>第一个问题就是为什么要做度量？当团队和产品的规模和复杂度都在增大的时候，我们很难对它有清楚的把握，如果不加以梳理，就会走向混乱的败局。</p><p>怎么理呢？方法就是数据度量。只有由数据得来的信息，才能支撑下一步的行动，确保我们的决策有助于优化、提升产品，而不是做无用功、甚至越改越糟。</p><p>每个行业领域都服从这样的规律，自动化测试也不例外，要想从自动化测试盲目乐观和盲目悲观中走向成熟，那就要掌握数据驱动这个方法论。</p><p>为了更好地让你理解质量度量体系。我把它从头展开，从交付质量到内建质量，然后从内建质量再分解到各个活动中去。每一层级是上一级的实现，又是下一级的目标。这跟Job模型的思维非常相似。我把这个度量体系画出来，就是这个样子：</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/46/b3f67d8cfe86480161cc57b0591ea946.jpg?wh=1920x1138\" alt=\"图片\"></p><p>这里质量体系我们之所以用树形结构表达，是为了强调上下级的关系。根结点是交付质量，只要这个总目标不变，内建质量的度量点你可以根据组织和项目情况，灵活变化。</p><p>那怎么衡量你的度量是有效的呢？只要每一层的度量指标，都能促进上一层度量目标的积极变化就行。否则，就需要更改或寻找新的度量。</p><p>另外，在质量度量体系分解的过程中，需要遵循两个度量指标的设计原则：<strong>第一，避免绝对数字</strong>；<strong>第二，避免单一</strong>。这都来自于我和团队的实践经验，希望也能帮助你排忧解难。在后续的章节中，我还会陆续提到其它原则，到最后一讲，我还会为你再总结一下。</p><p>下一讲，我们要从单元测试的实际场景，去发现怎么定义合理的指标，敬请期待。</p><h2>思考题</h2><p>在你的组织中，有没有量化的指标？能否分享一下，你觉得好用的指标，或者不好用的指标呢？</p><p>欢迎你在留言区跟我交流互动。如果觉得今天讲的方法对你有启发，也推荐你分享给更多朋友、同事。</p>","neighbors":{"left":{"article_title":"24｜启动运行：基于Job模型的框架实现和运行","id":517969},"right":{"article_title":"26｜见微知著：单元测试度量避坑","id":519677}}},{"article_id":519677,"article_title":"26｜见微知著：单元测试度量避坑","article_content":"<p>你好，我是柳胜。</p><p>通过前面的学习，我们不难发现单元测试的 ROI 又高，速度又快。在我看来，单元测试是一块没有充分发挥价值的蓝海。可惜很多公司不重视单元测试，也不愿意投入；有的公司虽然做了单元测试，但发现效果不明显，时间久了，单元测试也就流于形式。</p><p>想要真正在团队、乃至公司推动单元测试，就要见到效果，进入到一个有反馈刺激的正循环里。一旦进入到这样的循环，哪怕起点再低，也能一步步优化提升，走向成熟。因此，在这个循环机制中，反馈尤为重要。那这个反馈来自哪里呢？没错，是合理有效的度量。</p><p>这一讲，我会结合例子带你一步步推导，如何用度量驱动单元测试的落地和提升。</p><h2>失效的单元测试覆盖率</h2><p>如何度量单元测试的效果？很多人会脱口而出——“单元测试覆盖率”。而且，还能讲出很多覆盖率的方法论，语句覆盖率、分支覆盖率、判定覆盖率等等。但是，单元测试的高覆盖率一定会有高的代码质量么？</p><p>我们先看看单元测试覆盖率是怎么产生的，看看它的原理是什么，然后再判断单元测试覆盖率这个指标有什么问题。</p><p>后面是一段代码例子。有这样一个名为add的函数，它的入口是a，b两个整形参数。如果a小于2，计算结果就是a+b的负数，a大于2，就返回a+b。看起来逻辑很简单，是吧？</p><!-- [[[read_end]]] --><pre><code class=\"language-java\">public service{\n  public static int add(int a, int b){\n    if(a&lt;2){\n      return (a+b)* -1;\n    }else{\n      return a + b;\n    }\n  }\n}\n</code></pre><p>好，如果现在咱们要测试这个add函数，要设计多少个测试案例呢？</p><p>按照代码覆盖率的思路来考虑这个问题，我们是不是要设计出2个案例：一个案例是走a&lt;2，一个是走a&gt;2。这样2个案例走2个分支，我们覆盖率达到100%。就能达到效果了，对吧？</p><p>我们这就来写2个Junit Test方法，代码例子如下：</p><pre><code class=\"language-java\">@Test\npublic testMethod1(){\n  Assert.assertEquals(Service.add(1,2),-3);\n}\n@Test\npublic testMethod2(){\n  Assert.assertEquals(Service.add(3,2),5);\n}\n</code></pre><p>运行一下Junit，全部通过。</p><p><img src=\"https://static001.geekbang.org/resource/image/64/1a/644e1f25f6a85fa955268766905dbb1a.jpg?wh=1539x656\" alt=\"图片\"></p><p>给出的报告是代码语句覆盖率100%：</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/fd/b02db7f8c1035d70cec68c681b2d75fd.jpg?wh=1920x778\" alt=\"图片\"></p><p>好，现在有了100%覆盖率，我们可以放心了么？</p><p>有一天，我们的团队来了一个新手开发人员，他在修复bug的时候，不小心把a小于2，改成了a&lt;=2，像下面这样：</p><pre><code class=\"language-java\">public service{\n  public static int add(int a, int b){\n    if(a &lt;= 2){\n      return (a+b)* -1;\n    }else{\n      return a + b;\n    }\n  }\n}\n</code></pre><p>这是一个错误，对不对？那我们的测试案例能不能捕捉到这个bug呢？</p><p>很不幸，我们的2个测试案例还是会通过，对不对？它们一个走了a&lt;2的分支，一个走了a&gt;2的分支。这两个分支上的逻辑依然没变，所以测试的结果是通过。</p><p>通过这个例子，我带你还原了一个“Bug泄漏”的场景。意思就是说，虽然我们的单元测试案例代码覆盖率达到了100%，但还是捕捉不到回归Bug。沿用我常用的织网捕鱼的例子，鱼（Bug）还是从网眼里漏出去了。</p><p>这看起来是一个很小的错误，但对于某些类型的产品却有可能会致命。比如，对数据精准度要求极高的金融产品来说，单元测试没捕捉到，集成测试和系统测试也错过了，这个算法的错误会呈现一个放大效应。经过一系列算法的计算，在UI上呈现的数据会大相径庭。</p><p><img src=\"https://static001.geekbang.org/resource/image/f5/39/f5d241448e9c9f86ca862dd03f545439.jpg?wh=1920x685\" alt=\"图片\"></p><p>从上图里不难看到，错误代码计算出的金融指数跟正确值差了二十多倍，这给客户带来的损失也是巨大的。</p><p>现在，我们清楚地看到了单元测试代码覆盖率这个指标是有问题的，即使做到100%，也会有Bug泄漏。怎么堵住这个泄漏问题呢？</p><p>你可能想到了这样的解决方案，在等价类基础上再增加边界值测试用例。对应到例子里就是，在边界2上，增加一个a=2的测试案例。这样刚才开发人员的错误，就能捕捉到了，这个测试案例的测试结果会失败。</p><p>这看起来是有效的。但我们对测试还是没有信心，因为今天是a=2出了问题，我们补上了，明天b=3再出问题，我们再补一个。每次都是亡羊补牢，被动地去完善我们的测试设计。</p><p>被动补救只能治标，不能治本。那有没有更有效、更主动的解决方法呢？这就需要我们先找到泄漏的根本原因。</p><p>这个泄漏的根本原因，其实是每个测试案例的检查点多少、数据设计合理与否，这些取决于测试开发人员的理解，经验甚至责任心，这个是用代码覆盖率衡量不出来的。我们沿用上一讲的质量三角图继续分析：</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/10/d039e7b398f981c34938e1b405216710.jpg?wh=1920x1043\" alt=\"图片\"></p><p>单元测试的代码覆盖率属于“范围”这个支撑点，可以衡量测试的范围。但是还少了两个支撑点，时间和质量。还记得前面我打的比方么？测试工程师设计案例捕捉Bug，就像织网捕鱼，能捕捉到多少鱼，不光取决于网的大小<strong>（覆盖率</strong>），网眼的密度也很关键。</p><p>网眼的密度，就相当于单元测试的质量。怎么度量它呢？我们继续往下看。</p><h2>如何度量单元测试的质量</h2><p>上一讲我们引入了Bug泄漏率这个指标度量自动化测试的质量。而单元测试的质量也是同样的思路，我们要度量的就是单元测试捕捉Bug的效果。</p><h3>单元测试Bug泄漏率</h3><p>在单元测试阶段，我们捕捉到了多少回归Bug？有多少Bug是应该由单元测试捕捉到，却泄漏到下一阶段的？这个效果可以用一个比率来度量，那就是单元测试Bug泄漏率。</p><p>为了突出重点，我列出两个公式辅助你理解：</p><p>单元测试泄漏的Bug = 本应该在单元测试捕捉到的Bug（实际没捕捉到）。</p><p>单元测试Bug泄漏率 = 单元测试泄漏的Bug/单元测试泄漏的Bug+单元测试捕捉到的Bug</p><p>这个泄漏率定义有点绕，绕的地方就在于如何定义“<strong>本应该在单元测试捕捉到</strong>”。因此，我们需要将泄漏的Bug按照它们的根源分类，分析一下Bug对应在测试的哪个层面，看看我们本应该在单元测试、API测试还是UI测试层面发现这个Bug。</p><p>做Bug归因的思考，是一个很好的实践。但是把归因后的Bug作为度量，违背了一个原则，<strong>度量数据的来源应该是来自未经人加工的数据</strong>。</p><p>想一想，为什么要遵循这个原则呢，背后的道理是什么？因为数据要驱动良性循环，而不是恶性循环。</p><p>如果单元测试的质量用单元测试Bug泄漏率来评价，那么负责单元测试的人，肯定不愿意把Bug归因于单元测试的泄漏。所以，在团队内部，就形成了一个互相甩锅的局面。你说是我的问题，我说是他的问题，最后得出的度量指标也不能让人心悦诚服。</p><p>理论是自洽的，落地是有问题的。单元测试能否脚踏实地推进，关键在于能否找出一个合理的自动的单元测试质量的度量方法。这样的方法有没有呢？这就要用到一个新方法——变异测试。</p><h3>变异测试覆盖率</h3><p>变异测试是什么呢？它是一种通过向代码植入错误，来度量测试案例有效性的测试方法。它的结果是变异测试覆盖率，可以用来量化测试质量。</p><p>明确了变异测试的概念，我们再理解变异测试覆盖率就更容易了。假如，我们的被测程序是P，我们的测试组件是T，正常情况下，用T测试P，结果是成功。现在我们向P植入一个变异M1，形成一个P1版本的程序，这时我们再运行T，应该会失败，对不对？因为有了变异嘛，T失败了，说明变异被捕捉到了，也叫杀死了变异。</p><p>如法炮制，我们也可以向P植入另外一个变异M2，形成P2，T如果还是成功，那就是变异被放过了，也就是变异存活了。</p><p>我们每植入一个变异M，就运行一次T，记录运行的结果。最后再来统计一下，用被杀死的变异数量除以总共植入的变异数量，这个量化的比率，就是变异测试覆盖率。</p><p>变异测试覆盖率越高，我们的T就越有效。我给你画了一张变异测试原理图，每个字母对应的含义如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/ab/5ac5b10cdf8cf951f64a6e30b32b4aab.jpg?wh=1920x1653\" alt=\"图片\"></p><p>刚才我给你描述了变异测试的过程，再结合上面的图解，我们发现相比传统测试，变异测试新增了两个环节：一个是<strong>前置环节</strong>用来产生变异，中间还是运行传统测试；然后多了一个<strong>后置环节</strong>，变异测试结果和正常测试结果对比，来判断变异是否杀死，然后统计多次变异的运行结果，产生变异测试报告和变异测试覆盖率。</p><p>把生成变异以及生成变异测试报告这两个环节都自动化了，就是变异测试框架。借助业界已有的变异测试框架，我们很轻松就能完成变异测试。</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/3e/a1b4293e9507dc68a67d2ecfd0be613e.jpg?wh=5510x1915\" alt=\"\"></p><p>下面我们用一个变异测试框架PItest，对我们上面的Java代码做变异测试。</p><p>PItest用法很简单，在项目的pom文件里加入Pitest的插件，然后运行mvn命令，就会自动产生变异，自动运行T产生报告。</p><p>下面是PItest的执行日志，PItest针对刚才我们的add函数，用了4种策略（看红框），产生了7个变异（看黄框），包括修改边界、修改运算符、反转条件、修改返回值。</p><p>前面我们说的程序员手误把a&lt;2变成a&lt;=2，就是我们PItest框架生成的7个变异之一，属于修改边界的情况。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/a9/0dcd93878772d3f896eca2f531f916a9.jpg?wh=1920x1304\" alt=\"图片\"></p><p>我们继续来看最后的结果报告。7个变异被杀死5个，变异测试覆盖率71%。</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/d7/b0786647a317e3d109f37dfc910252d7.png?wh=1660x160\" alt=\"图片\"></p><p><img src=\"https://static001.geekbang.org/resource/image/d6/cd/d644b3210d8e7eeeededae7a990cb8cd.jpg?wh=1920x1228\" alt=\"图片\"></p><p>存活下来的变异分别是Conditional Boundary和Math计算，PITest对这些变异规则都有介绍，你可以参考PItest官网网站，链接在<a href=\"https://pitest.org/quickstart/mutators\">这里</a>。</p><p>好，既然有了变异测试覆盖率量化指标，我们就有了目标。为了提高变异测试覆盖率，我们来增加测试案例，杀死变异。</p><p>现在，有2个存活的变异，一个是把a&lt;2修改成a&lt;=2, 另一个是乘运算改成了除运算。相应地，我们需要增加边界测试案例3，a=2，b=2，代码例子如下：</p><pre><code class=\"language-java\">@Test\npublic testMethod1(){\n  Assert.assertEquals(Service.add(1,2),-3);\n}\n@Test\npublic testMethod2(){\n  Assert.assertEquals(Service.add(3,2),5);\n}\n//***新增TestMethod3\n@Test\npublic void testMethod3(){\n  assertEquals(Service.add(2,2),4)\n}\n</code></pre><p>再次运行变异测试框架，变异测试覆盖率从71%上升到86%。<br>\n<img src=\"https://static001.geekbang.org/resource/image/1e/49/1efed0b235ac7690604c0b5aaaf03849.jpg?wh=1362x372\" alt=\"图片\"></p><p>现在有了3个测试案例。如果你仔细看一下这3个案例，会发现2号案例是3号案例的一个子集，如果把2号案例删掉，代码语句覆盖率100%，变异测试覆盖率还是86%，那么2号案例就是一个冗余的案例，可以把它删掉。</p><pre><code class=\"language-java\">@Test\npublic testMethod1(){\n  Assert.assertEquals(Service.add(1,2),-3);\n}\n//***新增TestMethod3\n@Test\npublic void testMethod3(){\n  assertEquals(Service.add(2,2),4)\n}\n</code></pre><p>现在我们还剩下一个变异，就是a+b乘以-1，被修改成了a+b除以-1。这个变动，我们现有的2个测试案例没有杀死它，那我们怎么办，有没有办法设计出新的测试案例呢？这个问题，我留给你课后思考。</p><p>说到这里，我们总结一下。今天这一讲，最重要的就是重新审视单元测试的有效性。</p><p>希望你看完以后，再说起单元测试质量的时候，不再停留于盲目追求把网织得很大（即追求高测试覆盖率），而是转向把网编织得足够密这个目标上来。而这个网的密度，我们可以用<strong>软件变异测试覆盖率</strong>这一方法来度量。</p><h2>小结</h2><p>今天，我们从单元测试覆盖率的例子入手，发现即便覆盖率高达100%，Bug还是会发生泄漏。更严重的是，这个泄漏率很难去度量。无法度量，自然就无法在组织层面驱动有效的单元测试。那么我们该拿什么指标，来度量单元测试的有效性呢？</p><p>为此，我们研究了一下变异测试的原理和实现。它的基本理论是，向代码植入n个错误，运行测试案例看捕获错误的效率，这就是变异测试覆盖率。而且，变异测试可以借助测试框架实现自动化。</p><p>我们引入了PITest测试框架，结合例子演示了如何自动地生成变异，运行测试，到得出变异测试覆盖率整个过程。</p><p>从例子里可以看到，变异测试覆盖率能够度量出测试案例设计的质量，这个质量不仅包含遗漏的测试案例，测试检查点，还能帮我们发现冗余重复的测试案例，测试检查点。</p><p>由此可见，变异测试无论从度量模型还是实现手段上都行得通，它满足了我们对单元测试质量度量设计的要求。不但能检测Bug泄漏，而且可以自动化，无需人工参与。</p><p>有了度量，也有实现方法，你就可以推动单元测试在组织中落地了！下一讲，我会带你重新审视ROI，并基于ROI规律为你分享度量设计的三种方法，敬请期待。</p><h2>思考题</h2><p>最后，给你留两道思考题：</p><p>1.变异测试的思路是否仅限于单元测试？可以用在其它测试场景中么？</p><p>2.“每一次代码迭代，就是一次变异”，这句话你怎么理解？</p><p>欢迎你在留言区跟我交流互动。如果觉得今天讲的方法对你有启发，也推荐你分享给更多朋友、同事。</p>","neighbors":{"left":{"article_title":"25｜找准方向：如何建立有效的测试度量体系？","id":518825},"right":{"article_title":"27｜眼见为实：如何用数据描述你的自动化测试ROI ？","id":520566}}},{"article_id":520566,"article_title":"27｜眼见为实：如何用数据描述你的自动化测试ROI ？","article_content":"<p>你好，我是柳胜。</p><p>在第一模块价值篇，我们反复提到“ROI是自动化测试项目的隐式命脉”。一个自动化测试项目，技术再牛，开发工作量再大，如果它不能产出效益，不被用起来，或者用得不好，那都是事关项目生死的问题。</p><p>所以我们需要有效监控当前的项目是否运行健康，观测目前的情况是在向好还是变差，这样才能及时调整，应对变化。不然拖到最后，往往就是项目解散人走空的下场。</p><p>这个观测的办法是什么呢？回到我们的主题上来，那就是数据。这一讲，我就和你聊聊如何用数据描述你的项目ROI。客观数据会告诉我们项目发生了什么，而且还能帮你统一团队的看法和认识。</p><h2>再提自动化测试ROI</h2><p>在前面的模块里，有一个同学在交流区提问说：“领导对自动化测试不支持投入，怎么办？”。这的确是个现实的问题，在其他领域也常见，就是一个团队对一件事情的认识和看法发生了分歧，这种分歧发生在平级的不同角色里，也发生在上下级。</p><p>比如，在做产品设计时，有人说UI页面设计成极简风格会受欢迎，另外有人说UI应该把尽可能多的功能暴露出来，用户才使用简单。各有各的说辞，谁也说服不了谁。那怎么达成共识呢？我的建议是“用数据说话”。</p><p>具体怎么实现呢？比如我们可以对两种解决方案做A/B测试，收集用户的使用数据。然后通过数据看看，在两种解决方案中，哪个更受用户的喜爱。</p><!-- [[[read_end]]] --><p>所以，今天讲的内容，不仅仅是技术层面的度量模型设计，也是帮你掌握沟通思路方法，这样你再跟其他人沟通自动化测试的问题时，也更容易达成共识。而团队达成一致，多人一心，其利断金，后面推动什么事，就都好办了。</p><h2></h2><p>怎么度量呢？我们再来回顾一下ROI计算公式。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/00/cd34280bc70b3633e696a7ba16f9e300.jpg?wh=1920x868\" alt=\"图片\"></p><p>下面我们要完成的工作就是，把这个ROI公式转换成度量数据。具体该怎么做？可以从这三方面入手：</p><p>1.定义度量集合<br>\n2.确定度量数据来源<br>\n3.度量的实现</p><h2>定义度量集合</h2><p>在ROI计算公式，我们有4个因子：手工运行的时间、运行的次数、开发成本、维护成本。</p><p>我们要用数据度量它们，比如维护的成本，首先要搞清楚的问题就是“自动化测试案例的维护工作量都花在了哪里？”。维护工作量花在多个地方，那这个问题的答案可能不止一个，每个答案都是一个度量点，多个度量点组合在一起就形成了一个集合，我们管它叫做<strong>度量的维度</strong>。</p><p>每一个ROI因子的度量都不是单个度量，而是一个维度。我们先画一张初版表格整理思路，后续再逐渐填充完善。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/91/02f35771fb85c3d77da25ea93acfd091.jpg?wh=1920x1169\" alt=\"图片\"></p><p>在维度一列里，我们列出的是一个领域（比如维护工作量）。现在，每个领域的问题和维度都找到了，怎么细化呢？我给你提供三个方法，完整建模法、关键问题法和最佳实践法。</p><h3>完整建模法</h3><p>我在<a href=\"https://time.geekbang.org/column/article/518825\">第二十五讲</a>说过：“内建质量度量就是找出木桶的那块短板”。完成这个工作，需要两步，第一，先找出木桶所有的木板；第二，拿尺子挨个量一下长短，找出最短的那块。</p><p>怎么找出木桶所有的木板呢？这就要用到完整建模法。</p><p>完整建模法，就相当于把木桶的木板按照一个既定的原则，划分成不同集合，然后每个集合再继续细分，直到明确每块木板。比如为了数清楚木板数量，我们先划分颜色，可以先成蓝色板和绿色板，再分别计数每一种颜色的板数。当然了，板子的类别还有很多分类角度，核心思路就是整体规划，再分而治之。</p><p>用这个思路来建模维护工作量，如果按照生命周期来划分，维护工作量就可以分为诊断、修改、验证和上线。如果按照工作量类型来分，就有产品变更维护，扩展自动化维护，问题诊断维护。这就像测试案例设计里的等价类方法，我们总能找到一个角度，把等价类设计得完备。</p><p>完整建模法比较适合用在你开始一个新项目，或者新接受一个工作的时候。因为你对情况不了解，通过这种方法，可以帮你按ROI因子梳理自动化测试工作的关注点，然后再分而治之。</p><p>不过完整建模法虽然拥有划分完备，不会出现重大遗漏的优势，但缺点也很明显，那就是会产生大量的度量指标。你可以想象，上面我们把维护工作量划分成诊断，修改，验证和上线，而其中的诊断工作量又划分成告警通知时间、Log搜索定位错误的速度、代码修改的效率等等，要做这么完备和大量的度量，这些都要花费大量的人力和时间，从投产比角度来说是不划算的。</p><h3>关键问题法</h3><p>上面说的完整建模法，是我们面对一个新木桶，通过摸索找短板。如果你已经发现木桶在漏水了，那这个时候你应该赶紧解决漏水的问题，找到短板在哪里。找没找对，以最后解没解决漏水为验证标准。</p><p>关键问题法看起来很简单，有问题，通过度量找短板，解决问题。但其实软件项目不像木桶那么直观，甚至有时你都看不到软件项目的木桶在漏水。所以，<strong>提出正确的问题，是关键问题法的重要一步</strong>。</p><p>举个例子，我们工作里一个常见的场景这样的：产品规模扩大，设计的测试案例也越来越多，成千上万。这时QA的工作负荷也越来越大，你需要考虑对测试案例进行重构，精简优化。这个时候我们就可以提出一个问题——“有没有低效的案例？哪些案例的设计是好的，哪些是冗余的？”</p><p>在自动化测试里，你也会遇到类似的问题。既有概要层面的问题，比如“我的自动化测试案例的健康状况是变好还是变坏呢？”，也有细节上的问题，像是“我的自动化测试对需求覆盖得怎么样？”。</p><p>像“怎么样”、“是多少”这类的问题，我把它们叫做<strong>单标</strong>问题，即一个度量数据就能回答单标的问题。比如数学考试，柳胜同学考了多少分，我们告诉他“75分”就可以了。</p><p>而“情况在变好还是变坏”、“工作投入重了还是轻了”，这类就属于<strong>对标</strong>问题，也就是通过一组相似数据对比的结果才能回答的问题。比如，上面柳胜同学在数学考试中得了75分，这个分数能说明什么意思呢，是考得好，还是考得差呢？</p><p>这时，就需要看75分在班级的排名，然后再看上一次数学考试的班级排名，把它和这次75分的班级排名做对比。这样，我们才能得出一个基本的判断，确认柳胜同学到底在数学上是进步了，还是退步了。在这个排名计算和历史对比过程中，我们消除了考卷难度变化的干扰因素，最终用排名变化来度量“变好还是变坏”的问题。</p><p>实践中，<strong>趋势比绝对值更有决策价值</strong>，这是数据度量的又一原则。对于自动化测试ROI度量来说，我们更专注的<strong>是ROI变化的趋势，而不是ROI一个绝对的数字。自动化测试变得更健康了，还是正在衰退，我们是不是在一个正向的积极循环里，这更重要。如果有衰退，我们能尽早采取相应的行动</strong>。</p><h3>最佳实践法</h3><p>随着度量经验越来越多，你可以总结一些行之有效的度量，并把它推广到其它项目中去，形成一个企业级的最佳实践度量。</p><p>比如，你发现你的项目木桶漏水的原因，经常是需求那块木板没做好，那你就可以把“什么是好的需求”这个度量推广到企业的其他项目里，来驱动其他项目的改进。</p><p>在第一模块第四讲里，我提到“一份代码，多环境运行，多浏览器运行”能够提高自动化测试ROI。这时，我们就可以用数据把它们表达出来，作为自动化测试ROI的度量之一。</p><p>怎么能度量出“一份代码是不是支持了多环境，多浏览器呢”？如果要去一行行检查代码，不仅有人工参与，而且工作量也非常大。这时，你可以换个角度来思考这个问题，如果代码支持多浏览器的话，我们应该会看到什么结果呢？</p><p>没错，我们应该看到一份脚本跑出来的多个浏览器测试报告。如果它支持firefox和chrome，就应该会跑出来firefox和chrome的测试报告。这样一来，“一份代码，多环境运行”的度量就转成了自动化测试报告的数据分析。</p><p>度量的原则相当重要，所有特征的度量设计都要遵循这些原则才是有效的。为了让你牢牢记住，这里我们再温习一遍：</p><p><img src=\"https://static001.geekbang.org/resource/image/f1/aa/f16c16decc15cc02ffc057c95c36eaaa.jpg?wh=1920x1100\" alt=\"图片\"></p><p>有了三个法宝（完整建模法、关键问题法和最佳实践法）后，我们就可以设计度量了，现在我们把度量的表格更新一下：</p><p><img src=\"https://static001.geekbang.org/resource/image/01/7b/01b96cef69740c6f83380cc2ed093d7b.jpg?wh=1920x2742\" alt=\"图片\"></p><h2>确定度量数据来源</h2><p>提出了度量之后，我们还要考虑这个数据能不能采集得到。能采集的话，具体是从哪里采集。</p><p>这时不要忘记我们刚才提到度量设计第三个原则，<strong>度量数据的来源应该是来自未经人加工的数据。</strong></p><p>按照这个标准，我们排除几个需要人工决策的度量，用红色标注。</p><p><img src=\"https://static001.geekbang.org/resource/image/db/0e/db0d120e9f9f9316f164afff980c740e.jpg?wh=1920x2418\" alt=\"图片\"></p><p>到这里，我们度量设计环节的主要内容就告一段落了。下面就是实现的环节，需要我们完成一个数据采集、聚合到展现的流程。这个流程更多是技术的实现过程，我会在这个模块的最后一讲，为你分享一个完整思路，为你演示如何搭建一个度量设计平台。</p><h2>小结</h2><p>今天，我们以自动化测试ROI为例，一起学习了度量的设计方法。</p><p>我分别给你分享了三种方法：<strong>完整建模法、关键问题法和最佳实践法</strong>。</p><p>完整建模法是按照一定规则从整体分解到细节。比如在第一讲里，我们来度量测试活动的时候，就用到了完整建模法，按照生命周期划分了测试设计、测试执行、测试自动化三部分。它的好处是不会有重大遗漏，但是问题也不小，就是抓不到重点。这适用于你刚接手一个新的项目，用完整建模法来理清大思路。</p><p>如果你对工作已经很熟悉了，想快速抓住重点，这时可以用关键问题法，用一个问题来驱动一个度量的建立，实现，直到驱动问题解决。这样就实现了优化和提升。问题可以分成两类，一种是单标问题，用一个数据来回答一个问题；另外一种是对标问题，是用一系列相似条件的数据来回答一个问题。</p><p>从数据驱动提升角度看，对标问题更有价值。因为看到项目变化的趋势才能有效决策，而且把一件事儿从差变成好，这个过程也是测试人员工作价值所在，这些都能通过对标度量来驱动，你不妨在自己的工作里试试看。</p><p>一个典型的应用场景就是内建质量和交付质量（可以回看<a href=\"https://time.geekbang.org/column/article/518825\">第二十五讲</a>）。当我们为内建质量做出种种努力和改善的时候，一定要定期观察交付质量的趋势，观测它是在提升，还是在下降。如果是在提升，那就说明内建质量的改善是找对了短板，方向是对的，应该继续坚持。如果在下降，那就说明短板没有找对，我们还需要继续用数据度量寻找短板。</p><p>而最佳实践法呢，是把业界一些已经证明行之有效的通用方法，通过度量表达出来，对工作进行驱动和改善。</p><p>有了这三种思路的加持，我们就能顺利完成自动化测试ROI的度量体系的建立。你应该已经感受到了，这个建立度量体系的过程，就是我们对问题的认识从模糊到清晰的过程。可以说“<strong>不能量化的工作，就无法在一个规模组织中真正落地</strong>” （记住，这是我说的）。</p><h2><strong>思考题</strong></h2><p>开动你的头脑，怎样用数据回答“自动化测试设计得好不好”这个问题？</p><p>欢迎你在留言区跟我交流互动。如果觉得今天讲的方法对你有启发，也推荐你分享给更多朋友、同事，我们一起优化测试度量设计。</p>","neighbors":{"left":{"article_title":"26｜见微知著：单元测试度量避坑","id":519677},"right":{"article_title":"28｜解决问题：如何保证自动化测试的可信性？","id":521124}}},{"article_id":521124,"article_title":"28｜解决问题：如何保证自动化测试的可信性？","article_content":"<p>你好，我是柳胜。</p><p>咱们今天讨论一个十分影响自动化测试ROI的具体问题，就是自动化测试的可信性。你可能听过健壮性，稳定性，但什么是自动化测试可信性呢？</p><p>我说一个场景你就明白了，当你的UI automation测试报告显示Login案例运行失败了，查了Log之后，发现是测试机上出了一个弹窗，转移了浏览器的焦点导致失败。这种情况下，就是自动化测试的失败 !=  产品的Bug，你白忙活了一通。</p><p>同样的，另外一个场景，你的API automation测试报告一直显示成功，直到有一天，生产环境发生了Bug，你检查API test的执行日志，才发现API的Response早就出了问题。一个字段的Default value从True变成了0，但是Automation没有检查这个字段，漏掉了Bug。这种情况下，自动化测试的成功 ！= 产品没有问题， Bug泄漏了。</p><p>通过这两个例子，你能够看出来，可信性指的是自动化测试报告是否直接反映产品真实的质量状态。如果它经常误导报告的使用者，那么自动化测试快速可靠的价值就会大打折扣。你也没有信心把自动化测试当做一个服务，提供给开发人员和运维人员去使用。</p><p>如果你的自动化测试项目有可信性不足的问题，那怎么办呢？学完今天的内容，你就知道怎么用度量驱动来解决这个问题了。</p><!-- [[[read_end]]] --><h2>可信性</h2><p>我们首先分析一下自动化测试报告失信是怎么发生的。我们把测试结果的成功和失败，跟产品功能的正确和错误，做一个组合矩阵。</p><p><img src=\"https://static001.geekbang.org/resource/image/06/d3/06396ffeea201324189162b19e742dd3.jpg?wh=1920x775\" alt=\"图片\"></p><p>结合矩阵我们可以看到四种情形：</p><p>1.产品功能出错的时候，自动化测试会检测出来也报错，这叫做True Positive，真阳。</p><p>2.产品功能出错的时候，但是自动化测试运行结果是成功，这叫做False Negative，假阴。</p><p>3.产品功能正确的时候，自动化测试也运行成功，这叫True Negative，真阴。</p><p>4.产品功能正确的时候，但自动化测试运行结果是失败，这叫False Positive，假阳。</p><p>真阴和真阳都是我们期望的，也就是说自动化测试的结果反映了产品功能质量状态。而假阳和假阴是我们要避免的，因为此时自动化测试的结果，歪曲了产品功能质量状态。</p><p>其中假阳的后果是浪费了自动化测试人员的维护时间，所以我管它叫做<strong>测试噪声</strong>。而假阴的后果是导致Bug没能被检测到，泄漏到了生产环境中，我管它叫做 <strong>Bug泄漏</strong>。</p><p>我们期望能够避免假阴和假阳，那应该怎么做呢？ 数据驱动的解决思路很简单，两步走：首先我们要能计算出来假阴和假阳的数量，然后找到降级它们的办法。</p><h2>度量假阴和假阳</h2><p>怎么度量假阴和假阳？ 有几种思路来实现。</p><h3>基于Bug的方式</h3><p>第一种，基于Bug的方式。</p><p>每一个自动化测试的失败会触发提交一个Bug，然后交给自动化测试人员诊断。如果是真的Bug，就移交给开发人员；如果是假阳的话，需要给Bug标记一个“Not a Bug”，最后我们统计“Not a Bug”的Bug数量作为假阳的数量。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/82/f8f949d8a6bdba1788ef89ceb1f13582.jpg?wh=1920x1777\" alt=\"图片\"></p><p>同样，假阴也可以用Bug来度量。我们通过Bug溯源，就可以知道Bug是在哪个阶段泄漏的，如果是自动化测试泄漏的话，那做一个标记“Automation Leakage”， 最后再统计所有带有“Automation Leakage”标记的Bug作为假阴的数量。这样，假阳和假阴的数量我们就都有了。</p><p><img src=\"https://static001.geekbang.org/resource/image/f5/c3/f5f25d89eda889218ee46939a50aa1c3.jpg?wh=1920x1626\" alt=\"图片\"></p><p>这种基于Bug的方式，从理论上看起来讲得通，但我们要是实践起来，就会发现其中的困难。有两个因素会导致你的统计不全面：第一是需要增加人工的工作量，第二是人的主观判断会影响到度量结果。</p><p>这两个因素会导致度量的收集不全面，不准确。比如，Bug不做标记，就不会被统计到；Bug标记错了，就会不准确，标记错误是很常见的，有的人会觉得Bug是UI自动化测试的泄漏，有的人会认为是API自动化测试的泄漏，不同人去做这个归类，都会得出不同的结果。</p><p>一旦推行设计了这样的度量方式，改进目标很可能最终落空，因为数据可能被扭曲，甚至发生数据造假的情况。实际上，上面说的基于Bug的方式，违背了我们讲过的一个度量原则——<strong>度量数据的来源应该是来自未经人加工的数据。</strong></p><p>所以，我们再接着寻找更合适的度量办法。</p><h3>Assert方式</h3><p>有没有办法能用代码去判断是假阳还是真阳呢？具体的场景就是，在自动化测试运行失败的时候，代码能自动地去识别这是一个产品的错误，还是自动化测试代码的问题。</p><p>方向找到了，使用关键问题法（可以回顾<a href=\"https://time.geekbang.org/column/article/520566\">第二十七讲</a>），我们可以提出这样的问题：“产品的错误和测试代码问题有什么区别？”。</p><p>下面我们通过一个自动化测试案例的代码示例，也就是Restassure工具开发来分析一下。</p><pre><code class=\"language-java\">@Test\npublic void searchMovieById() {\n    //加载测试数据，电影为哈利波特\n    TestData testMovie = TestData.loadJson(\"movie_harryPotter.json\");\n    //查询Movie API，验证Movie ID为123456\n    get(uri + \"/movie/123456\").then()\n      .assertThat()\n      .statusCode(HttpStatus.OK.value())\n      .body(\"id\", equalTo(123456))\n      .body(\"name\", equalTo(testMovie.getName()))\n      .body(\"Type\",testMovie.getType());\n}\n</code></pre><p>这段API 测试代码，是测试 GET \"/movie/123456\"得到的数据。对于这个结果数据，一共有四个验证点，HTTP状态代码是200，movie的id、name和type。为了执行测试，还加载了一个测试数据源movie_harryPotter.json。</p><p>searchMovieById函数一共有7行代码，每一行代码都有可能引起searchMovieById的运行失败。测试数据加载找不到文件，movie API返回的数据不是哈利波特的电影，或者返回的数据里没有Name字段，这些都可能是searchMovieById失败的原因。</p><p>我们来分析一下这些失败情况，哪些是假阳，哪些是真阳呢？</p><p>测试数据的加载失败是假阳，因为它是自动化测试代码本身的问题，需要自动化测试开发人员来解决；而返回数据不正确则是真阳，它代表着一个产品的错误，需要开发人员，环境人员和业务人员来共同处理。</p><p>我们再返回到代码来观察这些假阳和真阳的失败出处，就会发现一些特征：产品的错误是由Assert语句捕捉到的，它们是关键的测试逻辑，而Assert语句之外的代码，是辅助测试构建的，它们必须保证健壮和稳定。</p><p>到这里，我们就梳理出一条规则，可以区分出产品错误和代码健壮性问题：<strong>Assert检查点的失败，是产品错误，其它的失败都属于自动化测试代码健壮性的问题。</strong></p><p>基于这条规则，真阳率和假阳率的度量公式我们就找到了，即：</p><p>真阳率 =  Assert语句引起的失败次数/自动化测试失败的总数</p><p>除了真阳，就是假阳，那么假阳率 = 1-真阳率。</p><p>基于Assert语句的度量方法解决了Bug方式的人工污染问题。因为Assert语句是在开发代码写入的，抛出来的Assert Exception也可以被框架识别，它的数据采集是可以完全自动化的，不需要人工参与。但Assert度量方式需要基于一个规则，那就是测试人员把验证逻辑都用Assert语句来表达。</p><p>所以，如果我们以Assert语句的假阳率为度量指标，那么它就会起到一个效果：推动自动化测试开发人员做更多的Assert检查点。这不正是我们期望的么？ 通过度量驱动，把团队带入到一个积极的正向循环里。</p><p>好，假阳度量解决了，我们再来看看假阴的度量。</p><p>假阴的场景是：自动化测试运行成功了，但实际没有捕捉到它应该捕捉到的Bug。这种Bug泄漏情况，看起来只能是Bug发生了之后，才能度量假阴率。如果没有Bug发生，或者即使发生了，没有人去做标记归因，那么假阴率就始终为0。所以通过基于Bug的方式来衡量假阴，也是不靠谱的。</p><p>假阴该怎么度量呢？参照上面的假阳度量方式，我们也可以从代码层级上来度量假阴。</p><p>没有Bug怎么办？那就创造Bug来度量。听到这句话，你会不会感觉很熟悉呢？没错，在<a href=\"https://time.geekbang.org/column/article/519677\">第二十六讲</a>提到的变异测试里的每一个变异，就是一个制造出来的Bug。变异测试覆盖率的反向指标就是假阴率，变异测试覆盖率越高，假阴率就越低。</p><p>另外我再分享一个思路，既然度量的目标是为了提升自动化测试可信性，我们可以用最佳实践法来创建度量。在假阳度量分析中，我们知道产品的错误是由Assert语句发现的，那么Assert语句越多，产品的验证越完备，Bug的泄漏可能性也会降低。</p><p>基于这个逻辑，我们可以把验证点的数量/测试案例作为一个度量，就是每个测试案例中出现的验证点数量，来驱动自动化测试开发人员养成在测试案例多做检查点的习惯，最终达到减少假阴的目标。</p><p>建立度量是度量驱动循环中最重要的一环，合理有效的度量指标，就是自动化测试的前行路上的灯塔。有了灯塔指引方向，大船航行就不至于迷失偏航。接下来，我们就来看看“航行路线”，也就是完整度量过程的生命周期。</p><h2>建立完整度量驱动周期</h2><p>一个完整的度量周期可以分解成四个阶段：</p><p>1.度量定义：建立度量指标，设置目标<br>\n2.数据采集：收集原始数据，聚合成度量指标<br>\n3.问题分析：查看度量数据，并找出问题<br>\n4.优化改进：提出和应用解决办法</p><p>经过这四个阶段，一个循环周期就宣告完成。然后可以重新开始第一阶段“度量定义”，进入第二个循环周期，持续改进，直到度量达到目标。这时候，就可以考虑换一个新的度量指标来提升了。如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/58/c92f63a1bf3e0413571ed6c43e9dc058.jpg?wh=1920x1329\" alt=\"图片\"></p><p>就拿前面我们说的假阳度量为例。在第一个循环周期，我们先采集出来假阳率是多少，比如是40%。</p><p>接着我们从第二个循环周期开始，设置假阳率的目标是控制在15%，推动团队想尽办法，通过技术或流程，来降低假阳率。然后是第三个周期，如果得到假阳率是30%，那说明办法奏效了，继续努力，直到最终假阳目标达到15%。</p><p>为了保持成果，你可以把假阳率保留在观测列表上，这样，一旦有什么变动影响到了假阳率，我们可以快速得知，作出反应。</p><p>其中，第一步的度量设计和第四步的改进办法，需要测试架构师带领团队完成。而第二步和第三步，数据的采集和分析，应该自动化来完成，不需要人工的干预。自动化采集数据和分析如何实现，下一讲度量技术里我们再展开说。</p><h2>小结</h2><p>到这里，我们总结一下今天学习到的内容。</p><p>今天我们提出了一个自动化测试比较棘手的高发问题，就是如果自动化测试不能真实地反映产品的质量状态，应该如何解决。</p><p>为了抓住解决问题的重点。我们先对自动化测试的结果做了分析，得出了假阴，假阳，真阴，真阳四种结果。</p><p>其中，假阴和假阳是需要解决掉的问题；假阳相当于噪声，浪费自动化测试维护工作量。假阴是Bug泄漏，直接影响自动化测试的最终质量。</p><p>怎么减少假阴和假阳呢？当然，如果你是一个人做自动化测试，这就靠个人的责任心和技术能力就可以解决了。但如果你所在的是一个规模化的自动化测试团队，那就需要一个可见、透明且合理的方法来推动工作。这就是度量驱动。</p><p>但度量驱动的最关键的是设计一个合理的度量指标。针对假阳和假阴，我们探讨了两种实现方式，基于Bug的方式和基于Assert的方式。Bug是从结果的来度量，Assert是从实现过程来度量。而结果容易被人工污染，相比之下，Assert更具有优势，客观，无人干预，自动化。</p><p>有了度量后，就要驱动工作的优化和提升了。这里我把一个周期分解成了四个阶段：度量定义，数据采集，问题分析，优化改进。</p><p>在下一讲我会给你介绍度量技术的实现，让整个生命周期能够自动化运行，IT大佬李国庆对当当网的数据改造项目说过这样一句话“数据是最有价值的，观点是廉价的”。度量的实现，也是自动化测试工作价值的可视化。如何用数据向你的平级和领导描述自动化，比起单纯的观点更有说服力，敬请期待。</p><h2><strong>思考题</strong></h2><p>对于假阴和假阳的情况，你是怎么处理和提升的，有没有用到度量呢？如果用到了，请分享你的度量方法。</p><p>欢迎你在留言区跟我交流互动。如果觉得今天讲的方法对你有启发，也推荐你分享给更多朋友、同事。</p>","neighbors":{"left":{"article_title":"27｜眼见为实：如何用数据描述你的自动化测试ROI ？","id":520566},"right":{"article_title":"29｜落地实践：搭建可持续度量的技术平台","id":521792}}},{"article_id":521792,"article_title":"29｜落地实践：搭建可持续度量的技术平台","article_content":"<p>你好，我是柳胜。</p><p>前面我们花了不少篇幅来讨论度量体系的设计，在方法论上扫清了障碍。</p><p>理论讲得通，实践也要做得通。想要在工作中落地、出效果，那就一定要把度量可视化。什么是度量可视化呢？简单来说，就是让相关人员能直观看到度量展现出来的样子。</p><p>在正文开始之前，我先分享几个可视化的例子，帮你建立更直观的感受。就拿单标度量来说，它通常是一个数字值。比如冒烟自动化测试覆盖率38.14%，就可以展现成下面这样的数字仪表盘。</p><p><img src=\"https://static001.geekbang.org/resource/image/e8/af/e8e765ebc70bda5ef3fbd83c0a3517af.jpg?wh=1920x1053\" alt=\"图片\"></p><p>而对标度量是时间轴上的一条折线或柱状图。下面展现的是Bug泄漏率随着时间的变化趋势图：</p><p><img src=\"https://static001.geekbang.org/resource/image/18/9c/18650b2dbea4625cae7132f5920b2b9c.jpg?wh=1920x844\" alt=\"图片\"></p><p>为了比较两个数值的差异，我们也可以把相似的度量放在一张折线图里。</p><p>比如下面这张图，展现的是不同归因的Bug数量。Bug的根源有的来自于API，有的来自数据，还有的是需求等等，把它们放在一起对比，有助于你看出软件开发周期的薄弱环节。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/29/8e485fd6b5a2e1cdac75c017e770dc29.jpg?wh=1920x1138\" alt=\"图片\"></p><p>这几类可视化的图片还只是冰山一角。这一讲，我将为你揭秘，这种度量数据可视化的实现相关技术和工作原理是什么。此外，我还会和你分享一下如何在自己的团队里，快速搭建一个度量数据实时观测平台。</p><h2>ETLA数据全周期技术</h2><p>度量的工作无非就是收集、处理和展示数据，所以从本质上说，度量技术就是一个数据工程。</p><!-- [[[read_end]]] --><p>在业界，典型的数据处理过程就是ETL，ETL是怎么来的呢？这就要说到数据生命周期的前三个阶段：分别是提取数据（Extract）、聚合数据（Transform）以及持久化数据（Load）。</p><p>三个阶段的英文单词首字母组合起来，就是ETL，ETL完成了业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，用在度量技术里，也就是把分散、零乱、标准不一致的原始数据转换成统一的度量的过程。</p><p>但是度量生成后，最终还需要用图表形式实现可视化。所以，我们再给ETL加上一个A，A是Analyze的首字母，而ETLA就是我们今天要学习的<strong>全周期数据度量技术</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/36/72/366e679a8cd814b517a65b9eyydb7472.jpg?wh=1920x680\" alt=\"图片\"></p><p>数据生命周期的这四个阶段，每个阶段都有不同的技术工具，下面我们就一一地学习。</p><h3>数据提取（Extract）</h3><p>Extract是从数据源里提取数据，这个数据源可以是API，也可能是数据库，或者文件系统等等所有能提供数据的地方。</p><p>在整个软件生命周期里，从需求到设计，开发，测试再到生产运维，会产生大量的数据，我们来理一下有哪些数据跟软件质量相关的，它们应该怎么提取。</p><p>你可以参考我整理的数据源表格，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/49/e5/49bdf2118e327413eac1384b4b53bfe5.jpg?wh=1920x1232\" alt=\"图片\"></p><p>在Extract环节，我给你分享两条实践原则，它们都来自我的实践经验：</p><p><strong>第一，提取的数据要尽可能得多。</strong>因为在分析阶段，你可能会用到多种数据才能算出一个度量，不要等到用的时候，才想到临时提取。</p><p>尤其那些趋势的度量，它们需要一定周期的时间序列数据才能计算出来。比如季度环比Bug增长率，当前是第二季度，不仅需要第二季度的Bug数据，还需要第一季度的Bug数据才能做环比。如果是同比增长率，那就还需要一年前的Bug数据。所以，提前提取好数据，有备无患。</p><p><strong>第二，提取数据的过程不要人工污染。</strong>这个原则在前面已经说过了，人工的污染指的是个人的判断和决策加到数据里去了。比如说，在提取Bug的过程中，有的Bug没有标注发生的环境，你给它们加了一个默认值，都认作是测试环境，这就把Bug数据污染了。</p><h3>数据转换及加载（Transform &amp; Load）</h3><p>Transform和Load是解决度量数据聚合转换和持久化的问题。这两个环节的技术方案做不做，怎么做，都取决于你的场景需求。别忘了我们贯穿始终的最优ROI思维，要把做“必要”的工作当作目标，不要“过度”工作。</p><p>举一个例子，在Extract环节，我们已经把TestRail里的测试案例信息提取出来了，那还需要在Load环节，再设置一个新的数据库来保存它们么？</p><p>这时你需要考虑这样做的收益和付出的成本，根据 <a href=\"https://zh.m.wikipedia.org/zh-hans/CAP%E5%AE%9A%E7%90%86\">CAP原理</a>，如果技术方案里出现了分布式数据源，会大大增加系统的复杂度，你就必须在CAP里三选二，不能得三。要么是追求数据一致性，舍弃掉及时可用性，要么是为了及时可用性，你就得忍受数据可能不一致的状态。</p><p>所以，针对TestRail案例库，你要慎重考虑。如无必要，无需增加实体，这也符合我们早在<a href=\"https://time.geekbang.org/column/article/497405\">第二讲</a>就提到过的<a href=\"https://zh.m.wikipedia.org/zh/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80\">奥卡姆剃须刀原则</a>。所以，你可以把测试案例的数据聚合成度量，自动化测试案例百分比，冒烟测试百分比等等，不用Load，直接对接Analyze环节做可视化了。</p><p>不过在度量场景里，有一种情况，你不得不考虑增加一个度量数据库。是什么场景呢？</p><p>在度量分析里，我们要经常要做两种分析，横向分析和纵向分析。</p><p>横向分析是比较不同集合的相似度量数据。比如对两个项目的Bug泄漏率进行对比，这样能够启发我们寻找组织内的最佳实践。纵向分析则是比较同一个度量它的历史数据和当前数据。比如Bug的趋势，自动化测试假阳率的趋势。</p><p>我们不难发现，想要实现纵向分析，需要我们先记录时间轴上一个个数据点，最终整合为时间序列数据。</p><p>有些原始数据源只会保存最新的数据信息，比如我们获得“Open的Bug”趋势，昨天“Open状态的Bug”是100个，今天可能是120个。</p><p>如果昨天没有记录下100这个数字，今天你就无法追溯回去，这种情况，我们就应该把每次采集到的数据以时间序列的格式存进度量数据库里。</p><p>度量数据库存储的是时间序列数据，它的Json表达是这样的：</p><pre><code class=\"language-java\">{\n\"fleet\": \"JiraBugs\",\n\"tags\": \"FoodCome\",\n\"datapoints\":\n[\n  {\n    \"Bugs_OPEN\": 100\n    \"timestamp\": 1466734431563,\n  },\n  {\n    \"Bugs_OPEN\": 120\n    \"timestamp\": 1466734432763,\n  } \n]\n}\n</code></pre><p>时间序列数据库英文又叫TSDB（Time Series Database）,  它是一种非关系数据库，业界主流的有InfluxDB、Graphite、Prometheus、MongoDB等等。你可以参照这个列表来选择TSDB作为自己组织的度量数据库。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/d0/5402a2d6169aff2c9032e9ff648f08d0.jpg?wh=1920x860\" alt=\"图片\"></p><p>完整列表参考<a href=\"https://en.wikipedia.org/wiki/Time_series_database\">这里</a>。</p><h3>可视化（Analyze）</h3><p>做了这么多准备工作，我们现在终于到了最重要的环节，也就是度量数据的可视化。</p><p>这个环节可用工具不少。你既可以选择自开发可视化前端，也可以选择现成的工具，主流的工具有Grafana、Kibana、Tableau等等，它们都提供了基本的数据展现功能，像柱图、线图、点图和表格等等。</p><p>不过，它们在应用领域上有一些微小区别，Grafana有免费版，在DevOps领域有经验优势。像Amazon的内部服务监控Telemetry，Oracle Public Cloud的度量服务T2都是用Grafana作为可视前端的。而Tableau是商业版，在Business Inteligence上更为擅长。</p><p>对测试度量可视化需求来说，要展现数字度量表盘、线图、柱图等等，不管是Tableau、Kibana还是Grafana，都能胜任。其中Grafana具有轻量开源的特点，我们就看一下Grafana怎么实现数据可视化。</p><p>Grafana用Docker启动很简单，执行下面命令：</p><pre><code class=\"language-java\">docker run -d -p 3000:3000 --name=grafana -v /home/ec2-user/data/grafana:/var/lib/grafana -v /home/ec2-user/data/conf:/usr/share/grafana/conf grafana/grafana&nbsp;\n</code></pre><p>访问本机的3000端口，你就会看到Grafana的Home页面了。下面是演示图：<br>\n<img src=\"https://static001.geekbang.org/resource/image/d7/e4/d7414200d589fdcebf8ef276675a96e4.jpg?wh=1920x985\" alt=\"图片\"></p><p>Grafana是一个Web UI前端，自带了一个Sqlite小数据库。不过Sqllite不是存储度量数据的，而是用来放Grafana的访问账号和其它配置信息的。</p><p>那Grafana的度量数据源从哪里来呢？我们可以在数据源配置中心里，看看Grafana支持哪些数据源。</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/a5/a1bf37ffc27ddf25dd7829046fb3f7a5.jpg?wh=1920x1547\" alt=\"图片\"></p><p>这有一个数据源的列表，我们可以看到Grafana支持的数据源类型，有时间序列数据库Graphite、InlfuxDB等等，也有基于Log文件的数据源ElasticSearch等，还有关系型数据库MySQL、PostgresSQL、MS SQL Server，Cloud数据源Cloud Watch、S3等等。你可以看看<a href=\"https://grafana.com/docs/grafana/latest/datasources\">这个链接</a>。</p><p>有了数据源后，下一步就是把度量数据展现在Grafana上了。</p><p>在Grafana的世界里，一个度量会通过一个Panel来完成可视化。所以，Panel的定义要包含两个信息：度量的数据来自哪里，以及度量以什么方式展现。</p><p>我们沿用上面的JiraBugs做例子。在Panel的Query Setting里，我们可以设置查询SQL，来告诉Grafana数据怎么取，设置参数如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/9f/e0770yyc58a357d95d563ffe6a17c99f.jpg?wh=1920x914\" alt=\"图片\"></p><p>这样设置之后，存在InfluxDB里的JiraBugs的数据点就会被拉出来。因为每个Bugs_OPEN数据点都会有一个时间戳，所以上面的SQL会查出这样一个数据集，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/df/54/df0aa1aa8fe2057092002aae118db154.jpg?wh=1920x675\" alt=\"图片\"></p><p>这些时间序列的数据该怎么展现呢？我们只需在Grafana的Visualization下拉列表里，选择展现的样式：</p><p><img src=\"https://static001.geekbang.org/resource/image/99/d2/9938d2884d28327aa61790486d7788d2.jpg?wh=1920x2392\" alt=\"图片\"></p><p>这时，我们设定Panel的展现形式为Time Series折线图，Grafana就会把上面的时间序列数据画成一条折线图：</p><p><img src=\"https://static001.geekbang.org/resource/image/4a/fe/4a877e6821cf23edeac7b07f3150c8fe.jpg?wh=1920x774\" alt=\"图片\"></p><p>如果我们更换Panel的展现形式是Table，Grafana就会为我们画出一张表格：</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/e6/7e2bc455888c2f2a89fdd3b154035fe6.jpg?wh=1920x729\" alt=\"图片\"></p><p>到现在，相信你能体验到Grafana的强大功能了！</p><p>你只需准备好度量数据源，这个数据源可以是数据库，文件系统，API，云端数据，然后告诉Grafana这个数据源在哪里。Grafana会把它的时间序列数据拉出来，并根据你的定制，展现成各种格式的图表，这就完成了数据可视化。</p><p>而且，Grafana还内嵌了一些数据聚合函数，像Sum、Mean等运算。如果你想查看一个测试工程师平均每天处理多少个Bug，你可以把Bug先Sum在一起，再求Mean，这些计算都在Grafana侧前端完成的，而不需要更改度量数据库。</p><p>因此，你可以这样理解：<strong>Grafana也做了一部分ETLA的Transform工作</strong>。</p><p>学会了一个Panel的制作以后，你就可以如法炮制，制作多个Panel，放在一个Dashboard里。Dashboard是一个度量集合，用来反映一个组织或者一个项目的关注的所有度量。</p><p>比如，下面的Dashbaord反映了一个敏捷开发的Squad小组的活动状态，包括开发代码质量，自动化测试覆盖率，执行效率等等：</p><p><img src=\"https://static001.geekbang.org/resource/image/ce/62/cef7f2835494485caf64ab949d00c162.jpg?wh=1920x876\" alt=\"图片\"></p><p>上图的度量集合有的来自Bug数据库，有的来自静态代码扫描结果，还有来自Ops工具等等不同数据。通过这样一个Dashboard，能把这些度量组织在一起，你就能对这个项目的质量状态一目了然了！</p><p>能观测，我们还不满足，我们还要能够设定目标，用于发现短板，驱动优化和提升。这块怎么实现呢？</p><p>利用Grafana的告警功能，我们就能给度量设置阈值。比如我们设置好，当Bug泄漏率超过了20%，就需要告诉团队，需要采取行动了。</p><p>这个“告诉”的方式，可以是发一封邮件，也可以直接提交一个工单，结合你的团队习惯，使用团队成员都习惯的工作跟踪系统即可。详细使用方法，你可以参照Grafana Alerts<a href=\"https://grafana.com/docs/grafana/latest/alerting\">文档链接</a>。</p><h2>自动化测试报告展现</h2><p>上面我们学习了数据可视化的整个技术链条。只要有数据点和时间戳信息，就可以把它们放到度量数据库里，让Grafana生成各种各样的图表。</p><p>这种数据可视化方案跟传统的报告相比，有巨大的优势。第一，高度可定制化，图表的展现形式多种多样，聚合的函数也丰富。第二，动态观测，每次刷新Dashboard，都会获得新的数据，图表也会随之更新。</p><p>在我看来，自动化测试报告未来的样子就应该接近于这种数据可视化方案，在<a href=\"https://time.geekbang.org/column/article/517969\">第二十四讲</a>我曾经说过，传统自动化测试报告数据和格式都很固定，展示样式死板，自开发成本又高。</p><p>这里推荐你考虑把Grafana作为自动化测试报告的展现前端。这样一来，作为自动化测试开发人员，我们只需要关注数据，画图做表的工作都可以交给Grafana。</p><p>这个逻辑上是可行的。因为自动化测试会反复运行多次，每运行一次，就相当产生一个时间序列上的时间点。只不过，这个时间点对应的数据并不是一个简单的数据点，而是一个Json树形对象结构，包含了本次测试运行产生所有的结果数据。</p><p>逻辑上可行，实践中有没有落地呢？Jmeter的性能测试就可以用Grafana+influxDB+Jmeter的Docker容器编排，一站式完成。</p><p>实现思路是这样的：</p><p>1.Jmeter在BackendListener添加InfluxDBBackendListener，把结果数据传送给它；</p><p>2.InfluxDBBackendListener把数据写入InfluxDB实例；</p><p>3.Grafana连接InfluxDB，使用封装好的Dashboard模版来展现性能测试报告。</p><p>整个过程，不需要写任何代码，只要运行Jmeter，就可以在Grafana上看报告了！我给你准备了一个<a href=\"https://github.com/testsmith-io/jmeter-influxdb-grafana-docker\">GitHub</a>的链接，你可以看到docker-compose.yaml和Jmeter的运行脚本。</p><p>最后在Grafana上展现的性能测试报告如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/39/e63302626274b8cb0b247cfdfbc28e39.jpg?wh=1920x811\" alt=\"图片\"></p><p>那么剩下的问题是，我们UI自动化测试报告，API自动化测试报告，是不是也可以做成这样的Grafana Dashboard呢？是否可以做，可以的话要如何做，我把这个问题留给你思考一下。</p><h2>小结</h2><p>在这一讲，我们学习了度量技术平台的实现思路。在前端，技术平台展现的是度量图表，但背后是整个ETLA的工作流，这个工作流其实也是数据的流动：从数据的提取Extract，到数据的聚合Transform，数据的持久化Load，最后到数据可视化Analyze。</p><p>在数据生命周期ETLA的每个阶段都有相应的工具和技术，这一讲我给你分享了一个ETLA的落地技术方案。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/27/60f2775a2f6a7c863d5cd7204ae97b27.jpg?wh=1920x790\" alt=\"图片\"></p><p>在Extract环节，我给你准备了一个数据源列表，这些数据源是你将来展现质量度量的原始素材。你需要开发Data Injgestor去从这些数据源里拉取数据并转换成度量，存放到Load度量数据库里，或者直接连接Analyze。</p><p>在Load环节里，我们需要选择合适的持久化数据源，因为度量要有结构多样，历史对比的能力，所以这里推荐的是非关系型数据库MongoDB，时间序列数据库Influxdb等。</p><p>你也可以结合项目需求，采用其它类型的数据库，我就曾经用过MySQL来存储一些变更频繁的度量数据，比如Bug。</p><p>在数据可视化这个环节，我重点给你讲解了Grafana这个工具。它免费且功能强大，一个Dashboard上可以展现多个Panel，每个Panel对应一个度量。这样，你的团队就能在一个Dashboard里看到全面的质量数据。发现质量短板后，还可以通过设置度量阈值的方式，给团队设定提升目标，完成整个数据驱动的循环。</p><p>在度量数据可视化的实现过程中，我们也发现Grafana的数据可视化能力，同样可以用在自动化测试报告的展现中。</p><p>这里我用了一个Jmeter性能测试报告的例子，使用 <strong>Docker容器Jmeter+InfluxDB+Grafana</strong> 做到了运行测试、保存结果数据、展现报告的一站式解决方案。非常方便，你也可以动手试试看。</p><p>你还停留在HTML的自动化测试报告么，来试试Grafana强大的报表能力吧。</p><h2>思考题</h2><p>根据这一讲学到的内容，请你思考一下，你工作里的UI自动化测试报告、API自动化测试报告是不是也可以做成Grafana Dashboard呢？</p><p>欢迎你在留言区跟我交流互动。如果觉得今天讲的方法对你有帮助，也推荐你分享给更多朋友、同事。</p>","neighbors":{"left":{"article_title":"28｜解决问题：如何保证自动化测试的可信性？","id":521124},"right":{"article_title":"结束语｜测试有终点，成长无边界","id":522445}}},{"article_id":522445,"article_title":"结束语｜测试有终点，成长无边界","article_content":"<p>你好，我是柳胜。</p><p>首先，恭喜你完成了这门课程的学习，感谢你的一路相伴。不得不说，我们的专栏要结束了。不管是一门课程，还是一个项目，结束的时候都需要一个回顾的仪式，把学过的内容再次梳理和总结。跟开篇词不同的是，到这里我们已经翻过山涉过水，再次驻足回望，会看到不同的风景。</p><p>正如开篇词的标题“做性价比最高的自动化测试”，怎么通向这个目标？需要先有一个整体视角，不是割裂地去看待各个测试细分领域。这样，你才能知道把有限的精力投入到哪里，才能得到更高的ROI。</p><p>为了达到整体效果，我们不仅在水平方向做了测试左移和右移，分别切入到开发知识，数据知识和运维知识；还在垂直角度，依次从单元测试、API测试、UI测试、验收测试各个层面来分析它们各自的实现方法和优劣势。有了整体视角后，我们就可以运用ROI思维来分配自己的精力了，用Job模型做好设计，用自动化测试工具写好代码。</p><p>总结一下，在这个专栏里，<strong>我们一共覆盖了业界三大技术方向13个知识点。其中有6个开发知识点，3个运维知识点，4个数据工程知识点（AI单独作为数据工程下的一个知识点）</strong>。</p><p>方法论和模型也是专栏的特色。我一共讲了9个模型，其中<strong>微测试Job模型、3KU法则和3KU金字塔模型</strong>都是在业界首次提出的新模型。而质量度量体系，也是第一次对业界林林总总的度量做出的结构化整理。</p><!-- [[[read_end]]] --><p>除了学习方法论，我们当然也不能忘了落地和实践，我在讲解中为你穿插介绍了10款测试工具，覆盖单元测试、API测试、数据测试、UI测试和布局测试等等。</p><p>现在回顾开篇词里“航海指南型”的学习路径，你是不是又有了新的体会？</p><p><img src=\"https://static001.geekbang.org/resource/image/20/d9/2056005c907cf1e0abe5a4c4590295d9.png?wh=1920x1080\" alt=\"图片\"></p><p>如果把专栏学习比作我带你一同航海探险，那后面的总结图，就是为了庆祝我们完成这次冒险，我送上的航海图。我把重要知识点和它出现的章节做了梳理，方便你回顾（紫色的A、B、C对应三大技术方向，蓝色的D、E对应模型与工具）。</p><p><img src=\"https://static001.geekbang.org/resource/image/01/23/0192a1fc38dab85bb7b07e6bfefc4423.jpg?wh=6651x4802\" alt=\"\"></p><p>通过总结图你也能感受到，我们讲到的内容知识面很广，还要覆盖道术器三个维度。</p><p>还记得今年2月开启专栏项目时，一共规划了30讲，估计内容有15万字，还要提炼各种图和表格。面对这个工作量，我的心情是又期待又畏惧。</p><p>期待在于能把自己工作这么多年积累的经验心得，写下来分享出去。这对我而言，是一个梳理和再学习的过程。而畏惧来自于不确定自己能不能做好“传道授业解惑”的角色。</p><p>真正的高手，能把一个复杂的事情掰开了、揉碎了，以简单干脆的方式告诉别人“这xx就是这么回事”。可以说，这简直是对自己技术专业水平、知识理解水平、讲解表达能力和持续写作耐力的四重考验。</p><p>尽管下定了决心，准备了很久。但从项目启动到最终交付整个专栏，我也有种过五关、斩六将的感觉。从刚开始诚惶诚恐、怕自己写不出来，到后期写完了又持续整理删减，怕啰嗦了浪费同学时间。这里特别要说的就是你们在留言区的积极反馈，这给了我非常大的鼓励和启发。</p><p>我看到有朋友在留言区的反馈“一看就会，一上手就废”。这种情况，其实我在设计专栏的内容时就在思考，广度和深度该怎么配比，才能为你提供最好的内容。</p><p>这个问题确实困扰了我好多天。我先写出了Job模型、自动化测试价值这几讲来找感觉。有一天我在写ROI的时候，突然悟到了，“<strong>最好的内容</strong>”不就是给读者朋友做“<strong>最优ROI的知识分享</strong>”么？</p><p>领悟到这点以后我很激动，然后很快就理清了专栏的设计思路。我力求在专栏里分享的是你在网上找不到的知识，有启发的观点、能落地的思路、删繁就简的模型、更贴近业务的场景案例，以及我所积累的实践经验体会。而网上能很容易搜索到的知识，工具的使用技巧、代码的调试等等，不应该占用专栏宝贵的篇幅，给出链接就可以了。</p><p>相信这个搭配，能让你学到价值最大化的内容。而为了弥补“上手”的最后一公里路的问题，我会持续地更新本专栏的 <a href=\"https://github.com/sheng-geek-zhuanlan/awesome-test-automation\">GitHub仓库</a>，分享更多的免费又易上手的资料。</p><p><img src=\"https://static001.geekbang.org/resource/image/b5/5b/b52e1d2627486e0e46afea04ea73195b.jpg?wh=1920x1549\" alt=\"图片\"></p><p>在这篇“结束语”的留言区里，希望同学们都能“冒个泡”，分享一下在这个专栏中的收获和心路历程，还有将来的打算。也欢迎你继续跟我交流互动，我会定期回看留言动态。</p><h2>学习还未结束</h2><p>下面，我继续讲讲我的心里话。</p><p>专栏学习虽然告一段落，但测试从业者的成长却要“活到老学到老”。关于学习，我最想聊的一个话题就是<strong>跨界学习</strong>。为什么说跨界学习重要呢？</p><p>做了测试和开发这么多年，我越来越感受到测试无法成为一门独立于软件开发和运维的技术。换句话说：没有这样的好事，我只是学好了测试工具和技术，就能到一个企业把自动化测试做好。</p><p>一个优秀的自动化测试工程师，应该同时也是一个优秀的全栈技术工程师。</p><p>在这个专栏里，我也提到做好单元测试，需要先做好代码设计；做API测试，先了解API设计规范；做UI测试也需要了解界面开发技术。测试作为整个软件生命周期中靠后的那一环，如果我们不懂开发，不懂运维，只懂测试，“皮之不存，毛将焉附？”。你很可能会做出一个笨重又费力的自动化测试方案。</p><p>所以，我鼓励你多跨界学习，时不时看看开发的代码，了解他们是怎么修改你的Bug的；跟运维聊聊，观察下他们是怎么部署并监控服务，发现线上Bug的。</p><p>我有一个实践中的技巧，可以促进你的全栈学习。你不妨为自己的Bug设置一个必填属性，我把这个属性叫做Early possible detect In。</p><p>什么意思呢？每一个Bug，你都要考虑在理想情况下，它最早应该在哪个阶段可以发现。</p><p>这个阶段可能是在整个软件生命周期的任何一个阶段，需求、设计、代码、单测、集测和系统测试阶段，你都需要了解它们是怎么工作的。通过建立这样一个思维习惯，你不仅可以推动自己思考和跨界沟通，还能驱动优化提升整个测试团队的流程和技术。</p><h2>新的开始</h2><p>虽然我前面提到测试不能独立于其他工作，但并不是说测试和其他工作都一模一样。实际上，测试工作和开发工作有着本质上的不同：<strong>开发是基于确定性来实现确定性，测试是基于不确定性来寻找确定性</strong>。</p><p>怎么理解这句话呢？开发有需求分析、设计模式和开发框架，通过这些开发出软件，实现用户的需求。这都是基于确定的假设，确定的需求。</p><p>而测试呢？我们的目标是要验证最终的软件系统，是否满足用户的显式和隐式的需求。这个场景，乐观点说像寻找木桶之中的短板，悲观点可以说是“大海捞针”。</p><p>而对我们测试工程师的挑战就是，如何调度有限的资源，在茫茫大海中找出那根针。换个说法就是用最小的资源，赢得最大的质量信心。</p><p>这个问题的解就是测试的最优ROI，也可以说是测试里最有价值的那部分工作，不管它的外在呈现是一种测试策略，还是一个测试技术，还是测试工具什么的。</p><p><strong>希望你从今天开始，进入到一个新的思考角度，以ROI方式来看待我们测试的工作。一定会去喧嚣得平静，尽繁华见最真，看到我们工作最本质的东西。</strong></p><p>是要说声再见了。关于怎么做专栏，中间遇到了什么问题，我自己又收获了什么，这些更个人化的感受，我把这些写在了公众号里，你有兴趣可以点<a href=\"https://mp.weixin.qq.com/s/HvUQ9hppSioWoOdbwfM0HQ\">这里</a>了解。除了在专栏里与我留言交流，你也可以把这个公众号当作我们联系的另一架桥梁。</p><p>2022年，会是一个难忘的年度，有人经历着疫情的困扰，有人经历着工作的动荡，这两个多月的难忘时光，感谢你和我一起度过。</p><p>引用杨绛先生的一段话：</p><blockquote>\n<p>“每个人都会有一段异常艰难的时光，生活的压力，工作的失意，学业的压力，爱的惶惶不可终日，挺过来的，人生就会豁然开朗，挺不过来的，时间也会教你，怎么与它们握手言和，所以不必害怕，光明总在前方。”</p>\n</blockquote><p>与你共勉，我们相见在天涯！</p><p>最后，我还给你准备了一份<a href=\"https://jinshuju.net/f/a51zTF\">毕业问卷</a>，题目不多，两分钟左右就能填好，期待你能畅所欲言。</p><p><a href=\"https://jinshuju.net/f/a51zTF\"><img src=\"https://static001.geekbang.org/resource/image/58/0c/587d264b94f8c42104f9fb5635dcf30c.jpg?wh=1142x801\" alt=\"图片\"></a></p>","neighbors":{"left":{"article_title":"29｜落地实践：搭建可持续度量的技术平台","id":521792},"right":{"article_title":"期末测试｜来赴一场100分之约！","id":523162}}},{"article_id":523162,"article_title":"期末测试｜来赴一场100分之约！","article_content":"<p>你好，我是柳胜。</p><p>到这里，《自动化测试高手课》的正文内容就全部结束了。我特别给你准备了一套期末测试题，帮助你检验自己的学习效果。</p><p>这套测试题共有 20 道题目，包括 10 道单选题，10 道多选题，满分 100 分，提交试卷后，系统会自动评分。还等什么，点击下面的按钮开始答题吧!</p><p><a href=\"http://time.geekbang.org/quiz/intro?act_id=3748&exam_id=9556\"><img src=\"https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201\" alt=\"\"></a></p><p>最后，我很希望知道你学习这个专栏的感受。这里我准备了一份<a href=\"https://jinshuju.net/f/a51zTF\">毕业问卷</a>，题目不多，希望你可以花两分钟填一下。</p><p><a href=\"https://jinshuju.net/f/a51zTF\"><img src=\"https://static001.geekbang.org/resource/image/67/52/6703028241cb6a4b82ccfe596b5d0652.jpg?wh=1142x801\" alt=\"\"></a></p><!-- [[[read_end]]] -->","neighbors":{"left":{"article_title":"结束语｜测试有终点，成长无边界","id":522445},"right":[]}}]