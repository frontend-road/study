{"id":5030,"title":"10 | 那些在Netflix Prize中大放异彩的推荐算法","content":"<p>早在前几篇务虚的文章中，我就和你聊过了推荐系统中的经典问题，其中有一类就是评分预测。</p>\n<p>让我摸着自己的良心说，评分预测问题只是很典型，其实并不大众，毕竟在实际的应用中，评分数据很难收集到，属于典型的精英问题；与之相对的另一类问题行为预测，才是平民级推荐问题，处处可见。</p>\n<h2>缘起</h2>\n<p>评分预测问题之所以“虽然小众却十分重要”，这一点得益于十多年前 Netflix Prize 的那一百万美元的悬赏效应。</p>\n<p>公元2006年10月2号，对于很多人来说，这只是平凡了无新意的一天，但对于推荐系统从业者来说，这是不得了的一天，美国著名的光盘租赁商 Netflix 突然广发英雄帖，放下“豪”言，这个就是土豪的“豪”，凡是能在他们现有推荐系统基础上，把均方根误差降低10%的大侠，可以瓜分100万美元。消息一出，群贤毕至。</p>\n<p>Netflix放出的比赛数据，正是评分数据，推荐系统的问题模式也是评分预测，也就是为什么说，评价标准是均方根误差了。</p>\n<p>这一评分预测问题在一百万美元的加持下，催生出无数推荐算法横空出世，其中最为著名的就是一系列矩阵分解模型，而最最著名的模型就是SVD以及其各种变体。这些模型后来也经受了时间检验，在实际应用中得到了不同程度的开枝散叶。</p>\n<p>今天我就来和你细聊一下矩阵分解，SVD及其最有名的变种算法。</p>\n<h2>矩阵分解</h2>\n<h3>为什么要矩阵分解</h3>\n<p>聪明的你也许会问，好好的近邻模型，一会儿基于用户，一会儿基于物品，感觉也能很酷炫地解决问题呀，为什么还要来矩阵分解呢？</p>\n<p>刨除不这么做就拿不到那一百万的不重要因素之外，矩阵分解确实可以解决一些近邻模型无法解决的问题。</p>\n<p>我们都是读书人，从不在背后说模型的坏话，这里可以非常坦诚地说几点近邻模型的问题：</p>\n<ol>\n<li>物品之间存在相关性，信息量并不随着向量维度增加而线性增加；</li>\n<li>矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在。</li>\n</ol>\n<p>上述两个问题，在矩阵分解中可以得到解决。矩阵分解，直观上说来简单，就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵。</p>\n<p>具体说来就是，假设用户物品的评分矩阵A是m乘以n维，即一共有m个用户，n个物品。我们选一个很小的数k，这个k比m和n都小很多，比如小两个数量级这样，通过一套算法得到两个矩阵U和V，矩阵U的维度是m乘以k，矩阵V的维度是n乘以k。</p>\n<p>这两个矩阵有什么要求呢？要求就是通过下面这个公式复原矩阵A，你可以点击文稿查看公式。</p>\n<p>$$ U_{m\\times{k}}V_{n\\times{k}}^{T} \\approx A_{m\\times{n}}$$</p>\n<p>类似这样的计算过程就是矩阵分解，还有一个更常见的名字叫做SVD；但是，SVD和矩阵分解不能划等号，因为除了SVD还有一些别的矩阵分解方法。</p>\n<h3>1 基础的SVD算法</h3>\n<p>值得一说的是，SVD全称奇异值分解，属于线性代数的知识;然而在推荐算法中实际上使用的并不是正统的奇异值分解，而是一个伪奇异值分解（具体伪在哪不是本文的重点）。</p>\n<p>今天我介绍的SVD是由Netflix Prize中取得骄人成绩的Yehuda Koren提出的矩阵分解推荐算法。</p>\n<p>按照顺序，首先介绍基础的SVD算法，然后是考虑偏置信息，接着是超越评分矩阵增加多种输入，最后是增加时间因素。好，一个一个来。</p>\n<p>前面已经从直观上大致说了矩阵分解是怎么回事，这里再从物理意义上解释一遍。矩阵分解，就是把用户和物品都映射到一个k维空间中，这个k维空间不是我们直接看得到的，也不一定具有非常好的可解释性，每一个维度也没有名字，所以常常叫做隐因子，代表藏在直观的矩阵数据下面的。</p>\n<p>每一个物品都得到一个向量q，每一个用户也得到一个向量p。对于物品，与它对应的向量q中的元素，有正有负，代表着这个物品背后暗藏的一些用户关注的因素。</p>\n<p>对于用户，与它对应的向量p中的元素，也有正有负，代表这个用户在若干因素上的偏好。物品被关注的因素，和用户偏好的因素，它们的数量和意义是一致的，就是我们在矩阵分解之处人为指定的k。</p>\n<p>举个例子，用户u的向量是pu，物品i的向量是qi，那么，要计算物品i推荐给用户u的推荐分数，直接计算点积即可：</p>\n<p>$$ \\hat{r}_{ui} = p_{u}q_{i}^{T}$$</p>\n<p>看上去很简单，难在哪呢？难在如何得到每一个用户，每一个物品的k维向量。这是一个机器学习问题。按照机器学习框架，一般就是考虑两个核心要素：</p>\n<ol>\n<li>损失函数；</li>\n<li>优化算法。</li>\n</ol>\n<p>SVD的损失函数是这样定义的：</p>\n<p>$$ \\min_{q^{* },p^{* } } \\sum_{(u,i) \\in \\kappa }{(r_{ui} - p_{u}q_{i}^{T})^{2} + \\lambda (||q_{i}||^{2} + ||p_{u}||^{2})} $$</p>\n<p>理解SVD的参数学习过程并不是必须的，如果你不是算法工程师的话不必深究这个过程。</p>\n<p>由于这个公式略复杂，如果你正在听音频，就需要自己看一下图片。这个损失函数由两部分构成，加号前一部分控制着模型的偏差，加号后一部分控制着模型的方差。</p>\n<p>前一部分就是：用分解后的矩阵预测分数，要和实际的用户评分之间误差越小越好。</p>\n<p>后一部分就是：得到的隐因子向量要越简单越好，以控制这个模型的方差，换句话说，让它在真正执行推荐任务时发挥要稳定。这部分的概念对应机器学习中的过拟合，有兴趣可以深入了解。</p>\n<p>整个SVD的学习过程就是：</p>\n<ol>\n<li>准备好用户物品的评分矩阵，每一条评分数据看做一条训练样本；</li>\n<li>给分解后的U矩阵和V矩阵随机初始化元素值；</li>\n<li>用U和V计算预测后的分数；</li>\n<li>计算预测的分数和实际的分数误差；</li>\n<li>按照梯度下降的方向更新U和V中的元素值；</li>\n<li>重复步骤3到5，直到达到停止条件。</li>\n</ol>\n<p>过程中提到的梯度下降是优化算法的一种，想深入了解可以参见任何一本机器学习的专著。</p>\n<p>得到分解后的矩阵之后，实质上就是得到了每个用户和每个物品的隐因子向量，拿着这个向量再做推荐计算就简单了，哪里不会点哪里，意思就是拿着物品和用户两个向量，计算点积就是推荐分数了。</p>\n<!-- [[[read_end]]] -->\n<h3>2 增加偏置信息</h3>\n<p>到现在，你已经知道基础的SVD是怎么回事了。现在来多考虑一下实际情况，试想一下：有一些用户会给出偏高的评分，比如标准宽松的用户；有一些物品也会收到偏高的评分，比如一些目标观众为铁粉的电影，甚至有可能整个平台的全局评分就偏高。</p>\n<p>所以，原装的SVD就有了第一个变种：把偏置信息抽出来的SVD。</p>\n<p>一个用户给一个物品的评分会由四部分相加：</p>\n<p>$$\\hat{r}_{ui} = \\mu + b_{i} + b_{u} + p_{u}q_{i}^{T} $$</p>\n<p>从左至右分别代表：全局平均分、物品的评分偏置、用户评分的偏置、用户和物品之间的兴趣偏好。</p>\n<p>针对前面三项偏置分数，我在这里举个例子，假如一个电影评分网站全局平均分是3分，《肖申克的救赎》的平均分比全局平均分要高1分。</p>\n<p>你是一个对电影非常严格的人，你一般打分比平均分都要低0.5，所以前三项从左到右分别就是3，1，-0.5。如果简单的就靠这三项，也可以给计算出一个你会给《肖申克的救赎》打的分数，就是3.5。</p>\n<p>增加了偏置信息的SVD模型目标函数稍有改变：</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/2c/85/2c6abb8736688e2ca315ef6fc7b03985.png?wh=556*72\" alt=\"\" /></p>\n<p>和基本的SVD相比，要想学习两个参数：用户偏置和物品偏置。学习的算法还是一样的。</p>\n<h3>3 增加历史行为</h3>\n<p>探讨完增加偏执信息的SVD后，接着你再思考一个问题：有的用户评分比较少。事实上这很常见，相比沉默的大多数，主动点评电影或者美食的用户是少数。</p>\n<p>换句话说，显式反馈比隐式反馈少，那么能不能利用隐式反馈来弥补这一点呢？另外，再考虑多一点，对于用户的个人属性，比如性别等，是不是也可以加入到模型中来弥补冷启动的不足呢？</p>\n<p>是的，都是可以的，在SVD中结合用户的隐式反馈行为和属性，这套模型叫做SVD++。</p>\n<p>先说隐式反馈怎么加入，方法是：除了假设评分矩阵中的物品有一个隐因子向量外，用户有过行为的物品集合也都有一个隐因子向量，维度是一样的。把用户操作过的物品隐因子向量加起来，用来表达用户的兴趣偏好。</p>\n<p>类似的，用户属性，全都转换成0-1型的特征后，对每一个特征也假设都存在一个同样维度的隐因子向量，一个用户的所有属性对应的隐因子向量相加，也代表了他的一些偏好。</p>\n<p>综合两者，SVD++的目标函数中，只需要把推荐分数预测部分稍作修改，原来的用户向量那部分增加了隐式反馈向量和用户属性向量：</p>\n<p>$$ \\hat{r}_{ui} = \\mu + b_{i} + b_{u} + \\<br />\n(p_{u} + |N(u)|^{-0.5}\\sum_{i\\in{N(u)}}{x_{i}} + \\sum_{a\\in{A{u}}}{y_{a}})q_{i}^{T} $$<br />\n（滑动查看完整公式）</p>\n<p>学习算法依然不变，只是要学习的参数多了两个向量：x和y。一个是隐式反馈的物品向量，另一个用户属性的向量。</p>\n<p>这样一来，在用户没有评分时，也可以用他的隐式反馈和属性做出一定的预测。</p>\n<h2>4 考虑时间因素</h2>\n<p>截止到目前，我们还没有正视过一个人性：人是善变的。这个是一个广义的评价，我们在进步也是在变化，今天的我们和十年前的我们很可能不一样了。这是常态，因此，在SVD中考虑时间因素也变得顺理成章。</p>\n<p>在SVD中考虑时间因素，有几种做法：</p>\n<ol>\n<li>对评分按照时间加权，让久远的评分更趋近平均值；</li>\n<li>对评分时间划分区间，不同的时间区间内分别学习出隐因子向量，使用时按照区间使用对应的隐因子向量来计算；</li>\n<li>对特殊的期间，如节日、周末等训练对应的隐因子向量。</li>\n</ol>\n<h2>总结</h2>\n<p>至此，我们介绍了在Netflix Prize比赛中最为出众的模型：SVD及其一些典型的改进。改进方案分别是：</p>\n<ol>\n<li>考虑偏置信息；</li>\n<li>考虑隐式反馈和用户属性；</li>\n<li>考虑时间因素。<br />\n其实Netflix Prize比赛上诞生了很多其他优秀的算法，或者把一些已有的算法应用得到很好的效果，比如受限玻尔兹曼机用来融合多个模型，这个我会在后面的专栏文章中专门再讲。</li>\n</ol>\n<p>好了，最后我要给你留一个思考题，假如矩阵分解面对的数据不是评分数据，而是行为数据，那么今天讲到的损失函数是否依然有效呢？欢迎留言一起讨论。感谢你的收听，我们下次再见。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/87/b0/873b086966136189db14874181823fb0.jpg?wh=1110*549\" alt=\"\" /></p>\n","comments":[{"had_liked":false,"id":5202,"user_name":"Dan","can_delete":false,"product_type":"c1","uid":1064321,"ip_address":"","ucode":"9BA15ACF99ED5C","user_header":"","comment_is_top":false,"comment_ctime":1522836612,"is_pvip":false,"replies":[{"id":"1390","content":"用K-fold确定。","user_name":"作者回复","comment_id":5202,"uid":"1005376","ip_address":"","utype":1,"ctime":1522840961,"user_name_real":"刑无刀"}],"discussion_count":1,"race_medal":0,"score":"44472509572","product_id":100005101,"comment_content":"請問老師隱性因子k的個數通常如何決定？","like_count":10,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416634,"discussion_content":"用K-fold确定。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522840961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4412,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1522079335,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"18701948519","product_id":100005101,"comment_content":"行为数据对于每个用户每个物品已经不是一个数值。这时候预测的还是评分吗？我觉得数值处理过程和目标很可能不同，损失函数需要做一些修改。","like_count":4,"discussions":[{"author":{"id":1343949,"avatar":"","nickname":"赖春苹","note":"","ucode":"1B637D46549A21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":3532,"discussion_content":"如果是有限种行为的话，损失函数就可以换成softmax这类的多分类问题了吧~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564556521,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":90389,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1556507085,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14441408973","product_id":100005101,"comment_content":"老师，您好，我之前用过spark的ALS，我负责的项目中没有显示反馈，全部是隐式反馈，比如点击、点赞、收藏这种，所以我是对每种行为定义了分数，比如点击是3分，点赞4分，收藏或分享是5分这样，然后使用的矩阵分解，上线效果并不理想，我不知道这样是不是不合适？正好您也留了这个思考作业，所以希望您能指点下","like_count":4},{"had_liked":false,"id":92226,"user_name":"戏入蝶衣","can_delete":false,"product_type":"c1","uid":1304500,"ip_address":"","ucode":"F6378C86A90DBC","user_header":"https://static001.geekbang.org/account/avatar/00/13/e7/b4/9a6c44cd.jpg","comment_is_top":false,"comment_ctime":1557206442,"is_pvip":false,"replies":[{"id":"36018","content":"一般实际工作中也常常不分离出bias。","user_name":"作者回复","comment_id":92226,"uid":"1005376","ip_address":"","utype":1,"ctime":1559490317,"user_name_real":"刑无刀"}],"discussion_count":1,"race_medal":0,"score":"10147141034","product_id":100005101,"comment_content":"在最基础的svd模型里，如果不添加用户和物品的评分bias，会有什么影响？","like_count":3,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449184,"discussion_content":"一般实际工作中也常常不分离出bias。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1559490317,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68526,"user_name":"衬衫的价格是19美元","can_delete":false,"product_type":"c1","uid":1397631,"ip_address":"","ucode":"655F925451F772","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKVUskibDnhMt5MCIJ8227HWkeg2wEEyewps8GuWhWaY5fy7Ya56bu2ktMlxdla3K29Wqia9efCkWaQ/132","comment_is_top":false,"comment_ctime":1550539072,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10140473664","product_id":100005101,"comment_content":"1.用户物品评分矩阵中某个特定用户一般只给其中的部分物品有评分，那么如何计算该用户对未评分物品的推荐分呢？<br>2.通过分解用户物品评分矩阵得到隐式因子，这是隐藏在用户物品中的不为人直观理解的影响因子，却又能深刻揭示用户物品的关系<br>3.因此，从用户物品评分矩阵分解得到隐式因子是关键，一般用SVD方法","like_count":2,"discussions":[{"author":{"id":1349749,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6mh3xlaMoGtWjmVJh2LutdLcQcPbKNjRlVru3bx8ynPhgwuGhhdzTkwEMoXbvBtgkcDSfom1kZg/132","nickname":"夜雨声烦","note":"","ucode":"87D8DB1E32522A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":55864,"discussion_content":"svd第三步的改进增加历史行为，增加用户隐式行为和商品属性就是为了解决给未评分的用户的推荐吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574407150,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4460,"user_name":"愚公移山","can_delete":false,"product_type":"c1","uid":1059097,"ip_address":"","ucode":"9C6DAA10F89736","user_header":"","comment_is_top":false,"comment_ctime":1522116265,"is_pvip":false,"replies":[{"id":"1097","content":"就是认为每个隐式反馈对象和每个属性都是一个特征，都对应一个隐因子向量。也就是公式中的xi和ya。","user_name":"作者回复","comment_id":4460,"uid":"1005376","ip_address":"","utype":1,"ctime":1522121334,"user_name_real":"刑无刀"}],"discussion_count":1,"race_medal":0,"score":"10112050857","product_id":100005101,"comment_content":"老师，在SVD++分解中，用户的隐式反馈数据和用户属于是怎样加入到用户物品评分矩阵中的呢？损失函数应该需要这部分数据做监督训练的","like_count":2,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416371,"discussion_content":"就是认为每个隐式反馈对象和每个属性都是一个特征，都对应一个隐因子向量。也就是公式中的xi和ya。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522121334,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4324,"user_name":"185","can_delete":false,"product_type":"c1","uid":1058880,"ip_address":"","ucode":"F27A4189CA4864","user_header":"https://static001.geekbang.org/account/avatar/00/10/28/40/c8fad3f7.jpg","comment_is_top":false,"comment_ctime":1522024630,"is_pvip":false,"replies":[{"id":"1069","content":"可以这样处理，但又略有不同，下一篇会讲。","user_name":"作者回复","comment_id":4324,"uid":"1005376","ip_address":"","utype":1,"ctime":1522075806,"user_name_real":"刑无刀"}],"discussion_count":1,"race_medal":0,"score":"10111959222","product_id":100005101,"comment_content":"根据我的理解，损失函数对行为数据是有用的，例如购买物品的数量、观看或者收听的时长、每天打开app的次数等都是和评分类似的数据。<br>我理解的对吗？","like_count":2,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416298,"discussion_content":"可以这样处理，但又略有不同，下一篇会讲。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522075806,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":90617,"user_name":"sangyongjia","can_delete":false,"product_type":"c1","uid":1516130,"ip_address":"","ucode":"C856BB588090A3","user_header":"","comment_is_top":false,"comment_ctime":1556589336,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5851556632","product_id":100005101,"comment_content":"用户物品评分矩阵中某个特定用户一般只给其中的部分物品有评分，评分矩阵是极度稀疏的，但是在矩阵分解时需要使用到评分矩阵。问题是：这些未评分的位置如何填充呢？","like_count":1,"discussions":[{"author":{"id":1724132,"avatar":"https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLquWRFTIvMa7L6KsUZZuGVhJwQPhCnH1kNfvaalE9ZicsLMhX0CW7TuSmBNOOrGPJkG9Ul10sIh1g/132","nickname":"pennlio","note":"","ucode":"505C6160D64380","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":46253,"discussion_content":"可以用均值填充未知位置","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573135549,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4341,"user_name":"Skye","can_delete":false,"product_type":"c1","uid":1057533,"ip_address":"","ucode":"F58365BA5CE17E","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/fd/56c4fb54.jpg","comment_is_top":false,"comment_ctime":1522032244,"is_pvip":false,"replies":[{"id":"1064","content":"其实就是，每个隐式反馈对象ID都是特征，这些特征背后都有一个k维的隐因子向量。所有这些隐因子向量都是未知参数，同等地位被优化，所以都是随机初始化。","user_name":"作者回复","comment_id":4341,"uid":"1005376","ip_address":"","utype":1,"ctime":1522075214,"user_name_real":"刑无刀"}],"discussion_count":1,"race_medal":0,"score":"5816999540","product_id":100005101,"comment_content":"老师，我想问一下，SVD++对于隐式反馈数据，损失函数拟合的rui值是0吗？还有用户行为向量x和用户属性y这个迭代初始值是什么，加上这两个向量，可是损失函数拟合的还是评分，这两个向量好像有点捉摸不透，意义在哪，能否细讲一下","like_count":1,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416311,"discussion_content":"其实就是，每个隐式反馈对象ID都是特征，这些特征背后都有一个k维的隐因子向量。所有这些隐因子向量都是未知参数，同等地位被优化，所以都是随机初始化。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522075214,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284438,"user_name":"全","can_delete":false,"product_type":"c1","uid":1132314,"ip_address":"","ucode":"C478EAFACC4E3B","user_header":"https://static001.geekbang.org/account/avatar/00/11/47/1a/645ab65b.jpg","comment_is_top":false,"comment_ctime":1616249220,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1616249220","product_id":100005101,"comment_content":"1.物品之间存在相关性，信息量并不随着向量维度增加而线性增加；<br>2.矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在。<br><br>老师说的邻居模型这两个弊端真的没想通，可以举个例子吗","like_count":0},{"had_liked":false,"id":237077,"user_name":"小栗Aili","can_delete":false,"product_type":"c1","uid":1984292,"ip_address":"","ucode":"1B8700C22AA358","user_header":"https://static001.geekbang.org/account/avatar/00/1e/47/24/de13e1c6.jpg","comment_is_top":false,"comment_ctime":1595668246,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1595668246","product_id":100005101,"comment_content":"行为数据的话这几个方面因素依然存在，只是行为的话估计得有权重了","like_count":0},{"had_liked":false,"id":223844,"user_name":"大魔王","can_delete":false,"product_type":"c1","uid":1308552,"ip_address":"","ucode":"5EB049D18E122F","user_header":"https://static001.geekbang.org/account/avatar/00/13/f7/88/da243c77.jpg","comment_is_top":false,"comment_ctime":1591195219,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591195219","product_id":100005101,"comment_content":"老师用了矩阵分解就不能用协同过滤了吧？这俩是完全不一样的解法是吗？","like_count":0},{"had_liked":false,"id":193590,"user_name":"kissingurami","can_delete":false,"product_type":"c1","uid":1905163,"ip_address":"","ucode":"5399F825A4E083","user_header":"https://static001.geekbang.org/account/avatar/00/1d/12/0b/d2335912.jpg","comment_is_top":false,"comment_ctime":1584931274,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584931274","product_id":100005101,"comment_content":"行为数据(例如点击或不点击，收藏或不收藏...）可以使用均方差作为损失函数，来预测0，或者1；但是奖励和惩罚的力度不够，应该不如交叉熵(或者negitive log likelihood)效果好。不知道对不对？","like_count":0},{"had_liked":false,"id":173379,"user_name":"ljinshuan","can_delete":false,"product_type":"c1","uid":1353103,"ip_address":"","ucode":"9C2B65CFDCF1E0","user_header":"https://static001.geekbang.org/account/avatar/00/14/a5/8f/23e9c701.jpg","comment_is_top":false,"comment_ctime":1579515341,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1579515341","product_id":100005101,"comment_content":"老师我这样理解不知道对不对，假设有10w商品，1w用户。原始svd需要分解的矩阵是1w*10w，如果加入用户行为，比如点击行为。这个矩阵就变成了1w*20w？因为没个商品对象的行为都是一个特征？","like_count":0},{"had_liked":false,"id":173163,"user_name":"范蠡","can_delete":false,"product_type":"c1","uid":1110237,"ip_address":"","ucode":"E88D813DD9DE79","user_header":"https://static001.geekbang.org/account/avatar/00/10/f0/dd/318c6d52.jpg","comment_is_top":false,"comment_ctime":1579442715,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1579442715","product_id":100005101,"comment_content":"SVD 的损失函数公式，您能详细解释下吗？看不明白","like_count":0},{"had_liked":false,"id":158794,"user_name":"neohope","can_delete":false,"product_type":"c1","uid":1043475,"ip_address":"","ucode":"C0268F6E7E2B6E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/13/49e98289.jpg","comment_is_top":false,"comment_ctime":1575452359,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1575452359","product_id":100005101,"comment_content":"试着回答一下老师的问题：<br>由于行为数据是离散值，不是连续值，所以损失函数在没有修改的情况下，应该不适用。<br>而且，离散数据的话，好像也不适合梯度下降算法。<br>如果是离散数据，做聚类会不会更简单一些呢？","like_count":0},{"had_liked":false,"id":124736,"user_name":"Geek_86533a","can_delete":false,"product_type":"c1","uid":1610333,"ip_address":"","ucode":"6961C429E8953A","user_header":"","comment_is_top":false,"comment_ctime":1565955991,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565955991","product_id":100005101,"comment_content":"请问SVD++中的x和y如何生成，随机吗？","like_count":0},{"had_liked":false,"id":115070,"user_name":"FF","can_delete":false,"product_type":"c1","uid":1133758,"ip_address":"","ucode":"89DB3329AAEAB2","user_header":"https://static001.geekbang.org/account/avatar/00/11/4c/be/25919d4b.jpg","comment_is_top":false,"comment_ctime":1563466154,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1563466154","product_id":100005101,"comment_content":"偏置值，就是任性值哈哈哈","like_count":0},{"had_liked":false,"id":113701,"user_name":"北冥Master","can_delete":false,"product_type":"c1","uid":1014142,"ip_address":"","ucode":"EBCCEC79AFC5DF","user_header":"https://static001.geekbang.org/account/avatar/00/0f/79/7e/c38ac02f.jpg","comment_is_top":false,"comment_ctime":1563121605,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1563121605","product_id":100005101,"comment_content":"SVD++公式中，Nu -0.5,为什么是这个参数呢？","like_count":0,"discussions":[{"author":{"id":1609624,"avatar":"https://static001.geekbang.org/account/avatar/00/18/8f/98/7d1287d9.jpg","nickname":"韩 * *","note":"","ucode":"4F08BE3D081FFC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5202,"discussion_content":"应该是指数，开根号的意思","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566045478,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":109174,"user_name":"漂泊的小飘","can_delete":false,"product_type":"c1","uid":1222578,"ip_address":"","ucode":"25C0CA4887D8AD","user_header":"https://static001.geekbang.org/account/avatar/00/12/a7/b2/274a4192.jpg","comment_is_top":false,"comment_ctime":1561974900,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1561974900","product_id":100005101,"comment_content":"呃……学习AI的时候一直不知道奇异值分解的方法，原来是这样啊。。。感谢","like_count":0},{"had_liked":false,"id":96275,"user_name":"张贝贝","can_delete":false,"product_type":"c1","uid":1066004,"ip_address":"","ucode":"7CDBB643AD1492","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/icicSvapqLfCWmIofXILE3b20RVDicQvooGnbksVNgz7wSzEfCKtibhIVMwibf778E39fF9hAa1EFMCFyhgljkwicicXg/132","comment_is_top":false,"comment_ctime":1558374569,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558374569","product_id":100005101,"comment_content":"svd＋＋中的item向量q和隐士反馈的item向量x是独立的吗？","like_count":0},{"had_liked":false,"id":90390,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1556507180,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556507180","product_id":100005101,"comment_content":"老师，我还有个疑问就是基于矩阵分解的协同过滤和这节课讲的矩阵分解是不是就是一套算法啊？","like_count":0},{"had_liked":false,"id":65913,"user_name":"杜骞","can_delete":false,"product_type":"c1","uid":1396012,"ip_address":"","ucode":"F07A89B49876C4","user_header":"","comment_is_top":false,"comment_ctime":1549768909,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1549768909","product_id":100005101,"comment_content":"伪体现在伪逆吧","like_count":0},{"had_liked":false,"id":35205,"user_name":"nebula","can_delete":false,"product_type":"c1","uid":1143186,"ip_address":"","ucode":"80BD445D357787","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYWib1hZibEMrCawHUDp3VzEj5uRERX7I4sIjyOSKKNuV8XqSoNRbPTjvd1Bz3DniaLtGecvJDgg9xg/132","comment_is_top":false,"comment_ctime":1540458972,"is_pvip":false,"replies":[{"id":"13398","content":"SGD(随机梯度下降)本身就是可以在线更新的。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1541554117,"ip_address":"","comment_id":35205,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1540458972","product_id":100005101,"comment_content":"请问SVD分解，针对新用户、新物品，怎么做更新呢","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":427476,"discussion_content":"SGD(随机梯度下降)本身就是可以在线更新的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1541554117,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":32388,"user_name":"leoplodchang","can_delete":false,"product_type":"c1","uid":1057148,"ip_address":"","ucode":"9DF62572DD3842","user_header":"https://static001.geekbang.org/account/avatar/00/10/21/7c/932aca3d.jpg","comment_is_top":false,"comment_ctime":1539564134,"is_pvip":false,"replies":[{"id":"15568","content":"是的。损失函数还是均方根误差。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1543245610,"ip_address":"","comment_id":32388,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1539564134","product_id":100005101,"comment_content":"老师你好，我想请问下，在假如历史行为的那个章节中，你给出了r的公式，那么损失函数的公式是什么样的呢？直接将r套入么？损失函数的后一部分是什么样的呢？","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":426752,"discussion_content":"是的。损失函数还是均方根误差。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543245610,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":28884,"user_name":"蘑菇酱","can_delete":false,"product_type":"c1","uid":1251943,"ip_address":"","ucode":"9BED7325DC6550","user_header":"https://static001.geekbang.org/account/avatar/00/13/1a/67/4008d168.jpg","comment_is_top":false,"comment_ctime":1538185945,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1538185945","product_id":100005101,"comment_content":"物品之间存在相关性，信息量并不随着向量维度增加而线性增加；<br><br>老师 这句话能举个具体的例子吗","like_count":0},{"had_liked":false,"id":8644,"user_name":"lmmcuc","can_delete":false,"product_type":"c1","uid":1085777,"ip_address":"","ucode":"8C41CD20CF76DB","user_header":"","comment_is_top":false,"comment_ctime":1526351172,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1526351172","product_id":100005101,"comment_content":"老师，您讲关于行为数据的了吗","like_count":0},{"had_liked":false,"id":5145,"user_name":"shoxx","can_delete":false,"product_type":"c1","uid":1063875,"ip_address":"","ucode":"A5A2FD6139B0F2","user_header":"","comment_is_top":false,"comment_ctime":1522773486,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1522773486","product_id":100005101,"comment_content":"請問老師，最後一個預測函數中，將用戶屬性的向量用ya的方式加總，是出現在大神的哪篇論文中呢？目前我只看到xi的，還請指點迷津，不勝感激。","like_count":0},{"had_liked":false,"id":4501,"user_name":"想养猪的白","can_delete":false,"product_type":"c1","uid":1015922,"ip_address":"","ucode":"E71A8C9C5626E5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/80/72/15263013.jpg","comment_is_top":false,"comment_ctime":1522154981,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1522154981","product_id":100005101,"comment_content":"在要学习的空间内 向量p在向量q上的投影只能是1或0 ","like_count":0},{"had_liked":false,"id":4405,"user_name":"Skye","can_delete":false,"product_type":"c1","uid":1057533,"ip_address":"","ucode":"F58365BA5CE17E","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/fd/56c4fb54.jpg","comment_is_top":false,"comment_ctime":1522076763,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1522076763","product_id":100005101,"comment_content":"SVD++中隐式反馈数据在公式当中是不是只关注特征维度，不关注实际比如操作次数这些值？比如用户属性，我们只需人为确定与几个用户属性相关？","like_count":0},{"had_liked":false,"id":4337,"user_name":"Skye","can_delete":false,"product_type":"c1","uid":1057533,"ip_address":"","ucode":"F58365BA5CE17E","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/fd/56c4fb54.jpg","comment_is_top":false,"comment_ctime":1522030562,"is_pvip":false,"replies":[{"id":"1065","content":"不对。这里讲的全是评分预测，所以没有负样本。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1522075337,"ip_address":"","comment_id":4337,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1522030562","product_id":100005101,"comment_content":"行为次数可以作为评分，使用损失函数，但是基础的SVD模型缺少负样本，用户没有行为的记录不作为样本计算在内，在隐式反馈场景应该不行。SVD++加入了隐式反馈信息，在这个场景效果就好了。老师，这样理解对吗？","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416308,"discussion_content":"不对。这里讲的全是评分预测，所以没有负样本。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522075337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4325,"user_name":"jt120","can_delete":false,"product_type":"c1","uid":1058471,"ip_address":"","ucode":"245268E9108D54","user_header":"https://static001.geekbang.org/account/avatar/00/10/26/a7/177581b8.jpg","comment_is_top":false,"comment_ctime":1522025157,"is_pvip":false,"replies":[{"id":"1068","content":"后面两篇会讲。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1522075690,"ip_address":"","comment_id":4325,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1522025157","product_id":100005101,"comment_content":"行为和评分，只是y不同，公式一样","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416299,"discussion_content":"后面两篇会讲。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522075690,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4318,"user_name":"写点啥呢","can_delete":false,"product_type":"c1","uid":1065272,"ip_address":"","ucode":"C19032CF1C41BA","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/38/4f89095b.jpg","comment_is_top":false,"comment_ctime":1522021047,"is_pvip":false,"replies":[{"id":"1070","content":"训练样本是一部分用户对一部分物品的关系，要预测的是那部分还没有关系的。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1522075865,"ip_address":"","comment_id":4318,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1522021047","product_id":100005101,"comment_content":"请问老师，前面提到的SVD后利用有监督机器学习来得到用户和物品的隐因子向量，那么训练样本是如何获取的呢？有一点疑惑，训练样本已经是用户和物品之间的关系反应，还需要做机器学习么？","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416296,"discussion_content":"训练样本是一部分用户对一部分物品的关系，要预测的是那部分还没有关系的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522075865,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}