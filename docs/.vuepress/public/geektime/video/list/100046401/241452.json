{"id":241452,"title":"84 | 上层模型拼接：如何在语言模型基础上拼接更多的模型？","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/NLP\">https://gitee.com/geektime-geekbang/NLP</a></p>","comments":[{"had_liked":false,"id":222051,"user_name":"l1n4n","can_delete":false,"product_type":"c3","uid":1270566,"ip_address":"","ucode":"663026CE8D6909","user_header":"https://static001.geekbang.org/account/avatar/00/13/63/26/a2ce0b1c.jpg","comment_is_top":false,"comment_ctime":1590664189,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"请问双向CNN是？反向的CNN层就是把序列反转之后作为CNN的输入吗？这样的好处是？","like_count":2},{"had_liked":false,"id":348006,"user_name":"MapleTx","can_delete":false,"product_type":"c3","uid":2547188,"ip_address":"","ucode":"02551811A491FE","user_header":"https://static001.geekbang.org/account/avatar/00/26/dd/f4/d9db8ab7.jpg","comment_is_top":false,"comment_ctime":1654660599,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"请问下老师分层学习率（上层学习率更大，bert层学习率较小）也能达到和先冻结bert再一起微调类似的效果吗","like_count":0},{"had_liked":false,"id":348003,"user_name":"MapleTx","can_delete":false,"product_type":"c3","uid":2547188,"ip_address":"","ucode":"02551811A491FE","user_header":"https://static001.geekbang.org/account/avatar/00/26/dd/f4/d9db8ab7.jpg","comment_is_top":false,"comment_ctime":1654658601,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"学习率调整的时候，使用分层学习率能不能达到相同的效果呢，给bert里面的层很小的学习率，头部更大的学习率","like_count":0},{"had_liked":false,"id":237193,"user_name":"Geek_02623b","can_delete":false,"product_type":"c3","uid":1940233,"ip_address":"","ucode":"42B104E9C2D45A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9b/09/994ec468.jpg","comment_is_top":false,"comment_ctime":1595733301,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"请问王老师，现在好多论文中都把NSP去掉了，NSP去掉之后，CLS不是就没用了，原始的BERT是想让CLS既包含句子中token的信息又包含上下句的概念，直接取所有Token相应的隐变量这个是不是更好点？","like_count":0}]}