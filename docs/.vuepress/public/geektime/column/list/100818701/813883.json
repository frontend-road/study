{"id":813883,"title":"11｜性能提升：优化客户端访问和服务端读写性能","content":"<blockquote>\n<p><span class=\"reference\">本课程为精品小课，不标配音频</span></p>\n</blockquote><p>你好，我是文强。</p><p>到了第 10 课我们其实就完成了本次课程主体部分的开发。这节课我们主要来看一下如何提升集群的性能和可用性。</p><p>集群的性能提升可以分为客户端和服务端两部分，先来看下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/25/41/258890f646e2fc54166676eeaa4ece41.jpeg?wh=1920x1080\" alt=\"图片\"></p><p>这是 Placement Center 的读写示意图。从技术上分析，重点可以关注以下三个方面来提升性能和可用性。</p><ol>\n<li>\n<p>客户端基于连接池复用连接，避免连接频繁地创建、删除，从而提升性能。</p>\n</li>\n<li>\n<p>实现自动化重试机制，以解决当出现可恢复的异常时（比如网络抖动），可以自动进行重试，从而提升请求的成功率。</p>\n</li>\n<li>\n<p>均摊服务端多个节点之间的压力，由 Leader 节点负责写请求，所有节点负责读请求，避免 Leader 节点的单点瓶颈。</p>\n</li>\n</ol><p>在<a href=\"https://time.geekbang.org/column/article/809901\">第 5 课</a>我们基于 gRPC 框架实现的网络层的基础上，我们来看一下在 Rust 中如何实现 gPRC 的连接池。</p><h2>基于 mobc 库实现连接池</h2><p><img src=\"https://static001.geekbang.org/resource/image/33/bf/3381711ca19f6a929d7d95dc5efd13bf.jpeg?wh=1914x832\" alt=\"图片\"></p><p>如上图所示，连接池的原理本质上就是通过预先创建一批连接，并将可用连接保持在一定数量范围内。当客户端发起访问时，从连接池取出可用连接，从而避免每次创建/销毁连接产生的时间和资源开销。</p><p>从代码实现角度看，连接池的实现并不复杂，就是细节比较多，比如连接被动关闭时如何自动创建连接，如何保证连接不超过最大可用连接，空闲连接回收，连接心跳保持等等。</p><!-- [[[read_end]]] --><p>为了节省工作量，我们直接选择Rust 的连接池库 <a href=\"https://crates.io/crates/mobc\">mobc</a> 来实现我们 gRPC 的连接池。mobc 库本身就不展开细讲了，你可以直接参考<a href=\"https://crates.io/crates/mobc\">《官方文档》</a>。建议你先看完官方文档，再来看接下来的实现，会更容易理解。</p><p>从代码实现角度来看，基于 mobc 库来实现连接池主要包含下面两步：</p><ol>\n<li>\n<p>实现 mobc 中名为 Manager 的 trait。</p>\n</li>\n<li>\n<p>创建连接池，并将连接池变为一个全局可访问的变量。</p>\n</li>\n</ol><p>先来看第 1 点，我们通过 KvServiceManager 来实现 Manager trait。完整代码在<a href=\"https://github.com/robustmq/robustmq-geek/blob/main/src/clients/src/placement/kv/mod.rs\">《kv/mod.rs》</a>文件中，有兴趣可以去查看。</p><pre><code class=\"language-plain\">#[derive(Clone)]\npub struct KvServiceManager {\n    pub addr: String,\n}\n\n\nimpl KvServiceManager {\n    pub fn new(addr: String) -&gt; Self {\n        Self { addr }\n    }\n}\n\n\n#[tonic::async_trait]\nimpl Manager for KvServiceManager {\n    // 使用 Rust 类型别名的语法，将类型 KvServiceClient&lt;Channel&gt; 重命名为Connection\n    // 使用别名的好处是，你可以自定义connect方法的返回值，因为connect的返回值是 Self::Connection\n    // 在这里我们将KvServiceClient&lt;Channel&gt;重命名为Connection，因此connect方法的返回值就是KvServiceClient&lt;Channel&gt;\n    \n    type Connection = KvServiceClient&lt;Channel&gt;;\n    \n    // 如上\n    type Error = CommonError;\n  \n    // 实现 Manager trait 中的 connect 方法\n    // 该方法用来返回 GRPC KvService 可用连接\n    async fn connect(&amp;self) -&gt; Result&lt;Self::Connection, Self::Error&gt; {\n         // 创建一个 GRPC KvService 的连接\n         match KvServiceClient::connect(format!(\"http://{}\", self.addr.clone())).await {\n            Ok(client) =&gt; {\n                // 返回一个可用的 GRPC KvService 连接\n                return Ok(client);\n            }\n            Err(err) =&gt; return Err(CommonError::CommmonError(format!(\n                \"{},{}\",\n                err.to_string(),\n                self.addr.clone()\n            ))),\n        };\n    }\n\n\n    // 实现 Manager trait 中的 check 方法\n    // 该方法用来检查 GRPC KvService 连接是否可用\n    async fn check(&amp;self, conn: Self::Connection) -&gt; Result&lt;Self::Connection, Self::Error&gt; {\n        Ok(conn)\n    }\n}\n</code></pre><p>在上面的代码中，我们创建了 KvServiceManager 来实现 mobc 库中的 Manager trait，实现了 trait 中的 connect 和 check 方法，功能分别是创建一个可用的连接、检查连接是否可用。</p><p>再来看第 2 点，如何在多个线程中都可以使用这个连接池。从 Rust 代码来看，核心思路就是有一个全局的变量，并且在不同的线程中使用该变量。</p><p>因为我们肯定会有多个服务端节点和多个 gRPC Service，因此我们<strong>需要管理多个服务端节点及其对应的gRPC Service 的连接池</strong>。这里我们是通过一个名为 ClientPool 的结构体来管理这些连接池。</p><p>来看一下 ClientPool 的代码。</p><pre><code class=\"language-plain\">#[derive(Clone)]\npub struct ClientPool {\n    \n    // 定义每个连接池的最大连接数\n    max_open_connection: u64,\n\n\n    // 使用 DashMap 来存储每个服务端IP 和GRPC Service对应的连接池\n    placement_center_kv_service_pools: DashMap&lt;String, Pool&lt;KvServiceManager&gt;&gt;,\n}\n\n\nimpl ClientPool {\n\n\n    pub fn new(max_open_connection: u64) -&gt; Self {\n        Self {\n            max_open_connection,\n            placement_center_kv_service_pools: DashMap::with_capacity(2),\n        }\n    }\n    \n    pub async fn placement_center_kv_services_client(\n        &amp;self,\n        addr: String,\n    ) -&gt; Result&lt;Connection&lt;KvServiceManager&gt;, CommonError&gt; {\n        let module = \"KvServices\".to_string();\n        \n        // 根据模块和地址构建一个 key，唯一标识服务器和GRPC Service \n        let key = format!(\"{}_{}_{}\", \"PlacementCenter\", module, addr);\n        // 判断连接池是否存在\n        if !self.placement_center_kv_service_pools.contains_key(&amp;key) {\n            // 创建一个连接池\n            let manager = KvServiceManager::new(addr.clone());\n            let pool = Pool::builder()\n                .max_open(self.max_open_connection)\n                .build(manager);\n\n\n            // 将连接池存储在 map 中\n            self.placement_center_kv_service_pools\n                .insert(key.clone(), pool);\n        }\n\n\n        // 从 map 中取出目标 IP 对应的连接池\n        if let Some(poll) = self.placement_center_kv_service_pools.get(&amp;key) {\n            // 从连接池获取可用的连接\n            match poll.get().await {\n                Ok(conn) =&gt; {\n                    return Ok(conn);\n                }\n                Err(e) =&gt; {\n                    return Err(CommonError::NoAvailableGrpcConnection(\n                        module,\n                        e.to_string(),\n                    ));\n                }\n            };\n        }\n\n\n        return Err(CommonError::NoAvailableGrpcConnection(\n            module,\n            \"connection pool is not initialized\".to_string(),\n        ));\n    }\n}\n</code></pre><p>这段代码并不复杂，核心逻辑是通过 DashMap 缓存每个 IP 和 gRPC Service 组成的二元组对应的连接池。当需要访问某个服务器的 Service 时，就从 DashMap 中取出对应的连接池，获取可用连接，并访问服务。</p><p>接下来看一下连接池如何使用。</p><pre><code class=\"language-plain\"> // 初始化ClientPoll\n let client_poll: Arc&lt;ClientPool&gt; = Arc::new(ClientPool::new(5));\n\n\n // 从连接池获取可用连接\n match client_poll.placement_center_kv_services_client(addr).await {\n     Ok(client) =&gt; {\n        let key = \"mq\".to_string();\n        let value = \"robustmq\".to_string();\n        let request = tonic::Request::new(SetRequest {\n            key: key.clone(),\n            value: value.clone(),\n        });\n\n\n        let _ = client.set(request).await.unwrap();\n     }\n     Err(e) =&gt; {\n            \n     }\n}\n</code></pre><p>上面这段代码比较简单，就不展开讲了。不过这里有个细节问题，就是它执行一次就会返回。如果出现网络抖动错误，就会直接失败。从使用者的角度，我们会希望它能够自动进行重试<strong>。</strong></p><p>所以接下来我们来实现统一的自动重试机制。</p><h2>客户端实现统一的自动重试机制</h2><p>从原理上看，自动重试的思路不复杂，核心逻辑就是将上面的代码装到一个 loop { } 中，当请求成功时退出循环，如果失败则根据重试策略进行重试。伪代码如下所示：</p><pre><code class=\"language-plain\">  let mut times = 1;\n  loop{\n   // 初始化ClientPoll\n   let client_poll: Arc&lt;ClientPool&gt; = Arc::new(ClientPool::new(5));\n  \n   // 从连接池获取可用连接\n   match client_poll.placement_center_kv_services_client(addr).await {\n       Ok(client) =&gt; {\n          let key = \"mq\".to_string();\n          let value = \"robustmq\".to_string();\n          let request = tonic::Request::new(SetRequest {\n              key: key.clone(),\n              value: value.clone(),\n          });\n  \n          match client.set(request).await{\n            Ok(_) =&gt; break;\n            Err(e) =&gt; {\n                // 达到最大次数时，退出循环\n               if times &gt; retry_times() {\n                    break\n               }\n               // 重试次数 + 1\n               times = times + 1;  \n            }\n          }\n       }\n       Err(e) =&gt; {\n       }\n  }\n }\n</code></pre><p>这段代码也很好理解，但是有一个问题，就是因为<strong>上面只是client.set 的调用，我们还会有client.get/delete/exists的调用，那每一个都是像上面这样加一个 loop 循环和重试次数的判断吗？</strong>有没有更简单的方法呢？</p><p>答案肯定是不能每个方法都有loop + 重试策略，这样代码就太不优雅了，并且后续如果要改重试策略，那么就特别繁琐。</p><p>接下来我们以 Set 方法举例，来介绍一下我们的实现。</p><blockquote>\n<p>tips：我们这种写法不是唯一的写法，可能也不是最好的写法。你可以按照需求写一个自己的实现，和我们的实现对比，看思路有哪些差异。</p>\n</blockquote><p>从实现来看，会涉及到下面四个方法：</p><ol>\n<li>\n<p>placement_set：封装了连接池和重试机制的 Kv Service 的 Set 方法。它接受连接池 client_poll、服务端地址列表、请求参数来完成 Set 请求的调用。</p>\n</li>\n<li>\n<p>retry_call：重试策略的核心代码，统一封装了重试策略。</p>\n</li>\n<li>\n<p>kv_interface_call：因为 gRPC 的特性是，每一个 Service 都有一个独立的 Client，比如 KVService 就有一个 KVClient，因此就需要对每个 Service 的调用做一个分流。</p>\n</li>\n<li>\n<p>inner_set：封装 KVService Set 调用的统一逻辑。</p>\n</li>\n</ol><p>下面我们来看一下这四个方法的主要逻辑，完整代码你可以看<a href=\"https://github.com/robustmq/robustmq-geek/tree/main/src/clients/src/placement/kv\">《placement/kv》</a>。</p><ul>\n<li>placement_set</li>\n</ul><pre><code class=\"language-plain\">pub async fn placement_set(\n    client_poll: Arc&lt;ClientPool&gt;,\n    addrs: Vec&lt;String&gt;,\n    request: SetRequest,\n) -&gt; Result&lt;CommonReply, CommonError&gt; {\n    // 将SetRequest 转化为 vec 类型，递交给 retry_call 处理\n    let request_data = SetRequest::encode_to_vec(&amp;request);\n    match retry_call(\n        // 定义这次调用是 KvClient\n        PlacementCenterService::Kv,\n        // 定义这次调用的是KvClient 的 set 方法\n        PlacementCenterInterface::Set,\n        // client poll、addrs、request_data 请求方法\n        client_poll,\n        addrs,\n        request_data,\n    )\n    .await\n    {\n        // 将返回结果 decode 为CommonReply类型，并返回\n        Ok(data) =&gt; match CommonReply::decode(data.as_ref()) {\n            Ok(da) =&gt; return Ok(da),\n            Err(e) =&gt; return Err(CommonError::CommmonError(e.to_string())),\n        },\n        Err(e) =&gt; {\n            return Err(e);\n        }\n    }\n}\n</code></pre><ul>\n<li>retry_call</li>\n</ul><pre><code class=\"language-plain\">async fn retry_call(\n    service: PlacementCenterService,\n    interface: PlacementCenterInterface,\n    client_poll: Arc&lt;ClientPool&gt;,\n    addrs: Vec&lt;String&gt;,\n    request: Vec&lt;u8&gt;,\n) -&gt; Result&lt;Vec&lt;u8&gt;, CommonError&gt; {\n    let mut times = 1;\n    \n    loop {\n        let index = times % addrs.len();\n        let addr = addrs.get(index).unwrap().clone();\n        let result = match service {\n            // 执行 Kv Service 的方法\n            PlacementCenterService::Kv =&gt; {\n                kv_interface_call(\n                    interface.clone(),\n                    client_poll.clone(),\n                    addr.clone(),\n                    request.clone(),\n                )\n                .await\n            }\n        };\n\n\n        match result {\n            Ok(data) =&gt; {\n                return Ok(data);\n            }\n            Err(e) =&gt; {\n                error!(\n                    \"{:?}@{:?}@{},{},\",\n                    service.clone(),\n                    interface.clone(),\n                    addr.clone(),\n                    e\n                );\n                // 定义最大重试次数\n                if times &gt; retry_times() {\n                    return Err(e);\n                }\n                times = times + 1;\n            }\n        }\n        // 定义重试的退避策略\n        sleep(Duration::from_secs(retry_sleep_time(times) as u64)).await;\n    }\n}\n\n\n</code></pre><ul>\n<li>kv_interface_call</li>\n</ul><pre><code class=\"language-plain\">pub(crate) async fn kv_interface_call(\n    interface: PlacementCenterInterface,\n    client_poll: Arc&lt;ClientPool&gt;,\n    addr: String,\n    request: Vec&lt;u8&gt;,\n) -&gt; Result&lt;Vec&lt;u8&gt;, CommonError&gt; {\n    // 获取 Kv Client\n    match kv_client(client_poll.clone(), addr.clone()).await {\n        Ok(client) =&gt; {\n            // 执行对应的 set、delete、get、exists 方法\n            let result = match interface {\n                PlacementCenterInterface::Set =&gt; inner_set(client, request.clone()).await,\n                PlacementCenterInterface::Delete =&gt; inner_delete(client, request.clone()).await,\n                PlacementCenterInterface::Get =&gt; inner_get(client, request.clone()).await,\n                PlacementCenterInterface::Exists =&gt; inner_exists(client, request.clone()).await,\n                _ =&gt; return Err(CommonError::CommmonError(format!(\n                    \"kv service does not support service interfaces [{:?}]\",\n                    interface\n                ))),\n            };\n            // 返回结果\n            match result {\n                Ok(data) =&gt; return Ok(data),\n                Err(e) =&gt; {\n                    return Err(e);\n                }\n            }\n        }\n        Err(e) =&gt; {\n            return Err(e);\n        }\n    }\n}\n</code></pre><ul>\n<li>inner_set</li>\n</ul><pre><code class=\"language-plain\">pub(crate) async fn inner_set(\n    mut client: Connection&lt;KvServiceManager&gt;,\n    request: Vec&lt;u8&gt;,\n) -&gt; Result&lt;Vec&lt;u8&gt;, CommonError&gt; {\n    // 将请求 decode 为SetRequest \n    match SetRequest::decode(request.as_ref()) {\n        // 调用 kvCleint 的 set 方法\n        Ok(request) =&gt; match client.set(request).await {\n            Ok(result) =&gt; {\n                // 将返回值encode 为 vec，返回\n                return Ok(CommonReply::encode_to_vec(&amp;result.into_inner()));\n            }\n            Err(e) =&gt; return Err(CommonError::GrpcServerStatus(e)),\n        },\n        Err(e) =&gt; {\n            return Err(CommonError::CommmonError(e.to_string()));\n        }\n    }\n}\n</code></pre><p>可以看到，上面我们<strong>为了统一封装多个gRPC Service 的重试策略</strong>，流程分为了四步。从代码上看，核心思路是通过 match 来区分不同的 Client 和不同的方法，并进行调用。</p><p>当前这个实现的好处就是流程清晰，代码可读性比较强，不过代码看起来是比较繁琐的。所以我们在主项目 <a href=\"https://github.com/robustmq/robustmq\">RobustMQ</a> 中有一个更优雅的实现，但是这个优雅实现的代码可读性较差，使用了大量的泛型和 trait，有兴趣的话你也可以去参考一下。</p><p>聊完了客户端，我们来聊聊服务端的性能优化。</p><h2>Leader 写和所有节点可读</h2><p>从原理上看，集群中 Leader 节点同时负责写入和读取是为了解决数据一致性的问题。这种方式的好处是不管如何读写，数据都是准确的、最新的。缺点是 <strong>Leader 节点会成为集群的性能瓶颈，无法横向扩容。</strong></p><p>在服务端性能优化中，一个核心思考点是：<strong>服务端的性能是可以随着节点的横向扩容而增强的</strong>。为了解决横向扩容的问题，就需要把 Leader 的压力分摊到所有节点上。</p><p>从技术上看，元数据服务的业务特点是读写比例较低，也就是<strong>写少读多</strong>。因此我们可以先把读请求的压力从 Leader 分摊到所有节点上。</p><p>那代码上看要怎么实现呢？ 先来看架构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/6c/e23019e62739714711a3e05f75913c6c.jpeg?wh=1920x1080\" alt=\"图片\"></p><p>上面的核心思路是：</p><ol>\n<li>\n<p>客户端允许配置服务端地址列表，客户端会轮询挑选一台服务器进行访问。</p>\n</li>\n<li>\n<p>服务端判断标记每一个接口是读请求还是写请求。比如 KV 存储模型中 Set/Delete 是写请求，Get/Exists 是读请求。</p>\n</li>\n<li>\n<p>服务器会根据请求的类型进行处理。</p>\n<ol>\n<li>\n<p>如果是写请求，会先判断自己是否是 Leader，如果是则直接处理请求，如果自己是 Follwer，则将该请求转发到 Leader 进行处理；</p>\n</li>\n<li>\n<p>如果是读请求，则可以直接处理请求。</p>\n</li>\n</ol>\n</li>\n</ol><p>第 1 点在前面的客户端代码已经实现了。在上面举例的 Set 操作的placement_set和retry_call中。placement_set 接受的服务器地址 addrs 是一个 Vec 列表，即允许配置多个服务器地址。在retry_call中，当请求处理失败后，会更换一个新的服务器进行访问，以避免某个节点无法提供服务。主要起作用的代码是：</p><pre><code class=\"language-plain\">loop{\n  let index = times % addrs.len();\n  let addr = addrs.get(index).unwrap().clone();\n  ......\n  times = times + 1;\n}\n</code></pre><p>第 2 点标记接口是读还是写请求需要人工判断。比如 KvService 中有 Set/Get/Delete/Exists 四个请求。从接口功能来看，Set 和 Delete 会改变数据的内容，所以它属于写请求。而 Get 和 Exists 只是会读取数据，因此它属于读请求。</p><p>第 3 点的实现主要是代码逻辑的处理，完整代码在<a href=\"https://github.com/robustmq/robustmq-geek/blob/main/src/placement-center/src/server/grpc/services_kv.rs\">《services_kv.rs》</a>中，我们以 Set 和 Get 来分别讲一下写/读请求的处理。</p><p>先来看 Set 的代码：</p><pre><code class=\"language-plain\"> async fn set(&amp;self, request: Request&lt;SetRequest&gt;) -&gt; Result&lt;Response&lt;CommonReply&gt;, Status&gt; {\n        let req = request.into_inner();\n        // \n        if req.key.is_empty() || req.value.is_empty() {\n            return Err(Status::cancelled(\n                RobustMQError::ParameterCannotBeNull(\"key or value\".to_string()).to_string(),\n            ));\n        }\n\n\n        if !self.is_leader() {\n            let leader_addr = self.leader_addr();\n            match placement_set(self.client_poll.clone(), vec![leader_addr], req).await {\n                Ok(reply) =&gt; {\n                    return Ok(Response::new(reply));\n                }\n                Err(e) =&gt; {\n                    return Err(Status::cancelled(e.to_string()));\n                }\n            }\n        }\n\n\n        // Raft state machine is used to store Node data\n        let data = StorageData::new(StorageDataType::KvSet, SetRequest::encode_to_vec(&amp;req));\n        match self\n            .placement_center_storage\n            .apply_propose_message(data, \"set\".to_string())\n            .await\n        {\n            Ok(_) =&gt; return Ok(Response::new(CommonReply::default())),\n            Err(e) =&gt; {\n                return Err(Status::cancelled(e.to_string()));\n            }\n        }\n    }\n</code></pre><p>上面这段代码在上节课已经讲过，只是添加了下面这段代码：</p><pre><code class=\"language-plain\">        if !self.is_leader() {\n            let leader_addr = self.leader_addr();\n            match placement_set(self.client_poll.clone(), vec![leader_addr], req).await {\n                Ok(reply) =&gt; {\n                    return Ok(Response::new(reply));\n                }\n                Err(e) =&gt; {\n                    return Err(Status::cancelled(e.to_string()));\n                }\n            }\n        }\n        \n</code></pre><p>即判断当前节点是否是 Leader，如果不是Leader，则获取 Leader 的地址，并将请求转发到 Leader 节点进行处理。is_leader 和 leader_addr 的逻辑如下：</p><pre><code class=\"language-plain\">\n\n    pub fn is_leader(&amp;self) -&gt; bool {\n        return self.placement_cluster.read().unwrap().is_leader();\n    }\n\n\n    pub fn leader_addr(&amp;self) -&gt; String {\n        return self.placement_cluster.read().unwrap().leader_addr();\n    }\n</code></pre><p>结合<a href=\"https://time.geekbang.org/column/article/813851\">第 9 课</a>可以知道，Leader 信息是由 Raft 状态机来维护的。当 Leader 发生切换时，Raft 状态机就会触发 Leader 信息的变更。</p><p>也就是说如果是写请求，就通过is_leader判断本节点是否是 Leader，是的话就正常处理，否的话就通过leader_addr方法获取 Leader 地址，并通过我们在这节课前半部分开发的客户端将请求转发给 Leader 进行处理。</p><p>再来看一下 Get 的代码：</p><pre><code class=\"language-plain\"> \n    async fn get(&amp;self, request: Request&lt;GetRequest&gt;) -&gt; Result&lt;Response&lt;GetReply&gt;, Status&gt; {\n        let req = request.into_inner();\n        if req.key.is_empty() {\n            return Err(Status::cancelled(\n                RobustMQError::ParameterCannotBeNull(\"key\".to_string()).to_string(),\n            ));\n        }\n        \n        // 实例化KvStorage\n        let kv_storage = KvStorage::new(self.rocksdb_engine_handler.clone());\n        let mut reply = GetReply::default();\n        // 从 RocksDB 中获取对应 Key 的数据\n        match kv_storage.get(req.key) {\n            Ok(Some(data)) =&gt; {\n                reply.value = data;\n                return Ok(Response::new(reply));\n            }\n            Ok(None) =&gt; {}\n            Err(e) =&gt; return Err(Status::cancelled(e.to_string())),\n        }\n\n\n        return Ok(Response::new(reply));\n    }\n</code></pre><p>上面的代码比较简单，就是直接通过 KvStorage 从 RocksDB 中获取对应的 Key 数据。因为 Get 是读请求，则不需要进行判断转发的逻辑，直接进行逻辑处理即可。</p><p>到了这里，如下图所示，我们就完成了 Leader 负责写入、所有节点可读的特性了。</p><h2>总结</h2><blockquote>\n<p>tips：每节课的代码都能在项目 <a href=\"https://github.com/robustmq/robustmq-geek\">https://github.com/robustmq/robustmq-geek</a> 中找到源码，有兴趣的同学可以下载源码来看。</p>\n</blockquote><p>这节课我们讲了通过连接池来提高客户端访问服务端的性能，并通过合适的代码实现统一封装多个 RPC Service 的重试机制，从而实现所有客户端调用都有统一的重试策略。然后通过在区分读写请求，并针对写请求做判断，从而实现了 Leader 写和所有节点可读的特性。</p><p>我们在<a href=\"https://time.geekbang.org/column/intro/100552001?utm_campaign=geektime_search&utm_content=geektime_search&utm_medium=geektime_search&utm_source=geektime_search&utm_term=geektime_search\">《深入拆解消息队列47 讲》</a>中讲到过，集群性能的提升包括单节点性能的提升和集群能力的提升。单节点性能的提升主要是网络层、存储层、计算层的性能的提升，在元数据服务中就是我们第 5 课（网络层）和第 6 课（存储层）的部分。</p><p>而集群能力的提升核心就是允许水平扩展，Leader 写和所有节点可读只是第一步，因为集群的写入压力都集中在 Leader 上，如果写入请求太大，那么依旧会出问题。所以下一步就是实现 Raft Group 的能力，即第 9 课说的允许集群中有多个 Leader 的存在。Raft Group 的特性，课程中我们没有展开，欢迎在交流群中讨论。</p><h2>思考题</h2><p>这里是本节课推荐的相关 issue 的任务列表，请点击查看<a href=\"http://www.robustmq.com/docs/robustmq-tutorial-cn/%e8%b4%a1%e7%8c%ae%e6%8c%87%e5%8d%97/good-first-issue/\">《Good First Issue》</a>，任务列表会不间断地更新。另外欢迎给我的项目 <a href=\"https://github.com/robustmq/robustmq\">https://github.com/robustmq/robustmq</a> 点个 Star 啊！</p>","neighbors":{"left":{"article_title":"10｜分布式：实现集群化、多副本KV存储引擎","id":813869},"right":{"article_title":"结束语｜第一阶段课程结尾展望","id":813886}},"comments":[]}