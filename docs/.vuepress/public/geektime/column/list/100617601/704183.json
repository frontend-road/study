{"id":704183,"title":"10｜记忆：通过Memory记住客户上次买花时的对话细节","content":"<p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在默认情况下，无论是LLM还是代理都是无状态的，每次模型的调用都是独立于其他交互的。也就是说，我们每次通过API开始和大语言模型展开一次新的对话，它都不知道你其实昨天或者前天曾经和它聊过天了。</p><p>你肯定会说，不可能啊，每次和ChatGPT聊天的时候，ChatGPT明明白白地记得我之前交待过的事情。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/97/c9907bc695521228cdfb5d3f75c13897.png?wh=588x593\" alt=\"\"></p><p>的确如此，ChatGPT之所以能够记得你之前说过的话，正是因为它使用了<strong>记忆（Memory）机制</strong>，记录了之前的对话上下文，并且把这个上下文作为提示的一部分，在最新的调用中传递给了模型。在聊天机器人的构建中，记忆机制非常重要。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/de/e26993dd3957bfd2947424abb9de7cde.png?wh=1965x1363\" alt=\"\"></p><h2>使用ConversationChain</h2><p>不过，在开始介绍LangChain中记忆机制的具体实现之前，先重新看一下我们上一节课曾经见过的ConversationChain。</p><p>这个Chain最主要的特点是，它提供了包含AI 前缀和人类前缀的对话摘要格式，这个对话格式和记忆机制结合得非常紧密。</p><p>让我们看一个简单的示例，并打印出ConversationChain中的内置提示模板，你就会明白这个对话格式的意义了。</p><pre><code class=\"language-plain\">from langchain import OpenAI\nfrom langchain.chains import ConversationChain\n\n# 初始化大语言模型\nllm = OpenAI(\n&nbsp; &nbsp; temperature=0.5,\n&nbsp; &nbsp; model_name=\"gpt-3.5-turbo-instruct\"\n)\n\n# 初始化对话链\nconv_chain = ConversationChain(llm=llm)\n\n# 打印对话的模板\nprint(conv_chain.prompt.template)\n</code></pre><!-- [[[read_end]]] --><p>输出：</p><pre><code class=\"language-plain\">The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:\n</code></pre><p>这里的提示为人类（我们）和人工智能（text-davinci-003）之间的对话设置了一个基本对话框架：这是<strong>人类和</strong> <strong>AI</strong> <strong>之间的友好对话。AI</strong> <strong>非常健谈并从其上下文中提供了大量的具体细节。</strong> (The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. )</p><p>同时，这个提示试图通过说明以下内容来减少幻觉，也就是尽量减少模型编造的信息：</p><p><strong>“如果</strong> <strong>AI</strong> <strong>不知道问题的答案，它就会如实说它不知道。”</strong>（If the AI does not know the answer to a question, it truthfully says it does not know.）</p><p>之后，我们看到两个参数 {history} 和 {input}。</p><ul>\n<li><strong>{history}</strong> 是存储会话记忆的地方，也就是人类和人工智能之间对话历史的信息。</li>\n<li><strong>{input}</strong> 是新输入的地方，你可以把它看成是和ChatGPT对话时，文本框中的输入。</li>\n</ul><p>这两个参数会通过提示模板传递给 LLM，我们希望返回的输出只是对话的延续。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/7c/c11b24c318dbd762f13781e3e40f9b7c.png?wh=1592x1066\" alt=\"\"></p><p><strong>那么当有了</strong> <strong>{history}</strong> <strong>参数，以及</strong> <strong>Human</strong> <strong>和</strong> <strong>AI</strong> <strong>这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理</strong>。</p><p>下面就让我们来在ConversationChain中加入记忆功能。</p><h2>使用ConversationBufferMemory</h2><p>在LangChain中，通过ConversationBufferMemory（<strong>缓冲记忆</strong>）可以实现最简单的记忆机制。</p><p>下面，我们就在对话链中引入ConversationBufferMemory。</p><pre><code class=\"language-plain\">from langchain import OpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\n\n# 初始化大语言模型\nllm = OpenAI(\n&nbsp; &nbsp; temperature=0.5,\n&nbsp; &nbsp; model_name=\"gpt-3.5-turbo-instruct\")\n\n# 初始化对话链\nconversation = ConversationChain(\n&nbsp; &nbsp; llm=llm,\n&nbsp; &nbsp; memory=ConversationBufferMemory()\n)\n\n# 第一天的对话\n# 回合1\nconversation(\"我姐姐明天要过生日，我需要一束生日花束。\")\nprint(\"第一次对话后的记忆:\", conversation.memory.buffer)\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">第一次对话后的记忆: \nHuman: 我姐姐明天要过生日，我需要一束生日花束。\nAI:&nbsp; 哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。\n</code></pre><p>在下一轮对话中，这些记忆会作为一部分传入提示。</p><pre><code class=\"language-plain\"># 回合2\nconversation(\"她喜欢粉色玫瑰，颜色是粉色的。\")\nprint(\"第二次对话后的记忆:\", conversation.memory.buffer)\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">第二次对话后的记忆: \nHuman: 我姐姐明天要过生日，我需要一束生日花束。\nAI:&nbsp; 哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。\nHuman: 她喜欢粉色玫瑰，颜色是粉色的。\nAI:&nbsp; 好的，那我可以推荐一束粉色玫瑰的生日花束给你。你想要多少朵？我可以帮你定制一束，比如说十朵、二十朵或者更多？\n</code></pre><p>下面，我们继续对话，同时打印出此时提示模板的信息。</p><pre><code class=\"language-plain\"># 回合3 （第二天的对话）\nconversation(\"我又来了，还记得我昨天为什么要来买花吗？\")\nprint(\"/n第三次对话后时提示:/n\",conversation.prompt.template)\nprint(\"/n第三次对话后的记忆:/n\", conversation.memory.buffer)\n</code></pre><p>模型输出：</p><pre><code class=\"language-plain\">Human: 我姐姐明天要过生日，我需要一束生日花束。\nAI:&nbsp; 哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。\nHuman: 她喜欢粉色玫瑰，颜色是粉色的。\nAI:&nbsp; 好的，那我可以推荐一束粉色玫瑰的生日花束给你，你想要多少朵？\nHuman: 我又来了，还记得我昨天为什么要来买花吗？\nAI:&nbsp; 是的，我记得你昨天来买花是因为你姐姐明天要过生日，你想要买一束粉色玫瑰的生日花束给她。\n</code></pre><p>实际上，这些聊天历史信息，都被传入了ConversationChain的提示模板中的 {history} 参数，构建出了包含聊天记录的新的提示输入。</p><p>有了记忆机制，LLM能够了解之前的对话内容，这样简单直接地存储所有内容为LLM提供了最大量的信息，但是新输入中也包含了更多的Token（所有的聊天历史记录），这意味着响应时间变慢和更高的成本。而且，当达到LLM的令牌数（上下文窗口）限制时，太长的对话无法被记住（对于text-davinci-003和gpt-3.5-turbo，每次的最大输入限制是4096个Token）。</p><p>下面我们来看看针对Token太多、聊天历史记录过长的一些解决方案。</p><h2>使用ConversationBufferWindowMemory</h2><p>说到记忆，我们人类的大脑也不是无穷无尽的。所以说，有的时候事情太多，我们只能把有些遥远的记忆抹掉。毕竟，最新的经历最鲜活，也最重要。</p><p>ConversationBufferWindowMemory 是<strong>缓冲窗口记忆</strong>，它的思路就是只保存最新最近的几次人类和AI的互动。因此，它在之前的“缓冲记忆”基础上增加了一个窗口值 k。这意味着我们只保留一定数量的过去互动，然后“忘记”之前的互动。</p><p>下面看一下示例。</p><pre><code class=\"language-plain\">from langchain import OpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationBufferWindowMemory\n\n# 创建大语言模型实例\nllm = OpenAI(\n&nbsp; &nbsp; temperature=0.5,\n&nbsp; &nbsp; model_name=\"gpt-3.5-turbo-instruct\")\n\n# 初始化对话链\nconversation = ConversationChain(\n&nbsp; &nbsp; llm=llm,\n&nbsp; &nbsp; memory=ConversationBufferWindowMemory(k=1)\n)\n\n# 第一天的对话\n# 回合1\nresult = conversation(\"我姐姐明天要过生日，我需要一束生日花束。\")\nprint(result)\n# 回合2\nresult = conversation(\"她喜欢粉色玫瑰，颜色是粉色的。\")\n# print(\"\\n第二次对话后的记忆:\\n\", conversation.memory.buffer)\nprint(result)\n\n# 第二天的对话\n# 回合3\nresult = conversation(\"我又来了，还记得我昨天为什么要来买花吗？\")\nprint(result)\n</code></pre><p>第一回合的输出：</p><pre><code class=\"language-plain\">{'input': '我姐姐明天要过生日，我需要一束生日花束。', \n'history': '',\n 'response': ' 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？'}\n</code></pre><p>第二回合的输出：</p><pre><code class=\"language-plain\">{'input': '她喜欢粉色玫瑰，颜色是粉色的。', \n'history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\\nAI:&nbsp; 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？', \n'response': ' 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？'}\n</code></pre><p>第三回合的输出：</p><pre><code class=\"language-plain\">{'input': '我又来了，还记得我昨天为什么要来买花吗？', \n'history': 'Human: 她喜欢粉色玫瑰，颜色是粉色的。\\nAI:&nbsp; 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？', \n'response': '&nbsp; 当然记得，你昨天来买花是为了给你喜欢的人送一束粉色玫瑰花束，表达你对TA的爱意。'}\n</code></pre><p>在给定的例子中，设置 k=1，这意味着窗口只会记住与AI之间的最新的互动，即只保留上一次的人类回应和AI的回应。</p><p>在第三个回合，当我们询问“还记得我昨天为什么要来买花吗？”，由于我们只保留了最近的互动（k=1），模型已经忘记了正确的答案。所以，虽然它说记得，但只能模糊地说出“喜欢的人”，而没有说关键字“姐姐”。不过，如果（我是说如果哈）在第二个回合，模型能回答“我可以帮你<strong>为你姐姐</strong>找到…”，那么，尽管我们没有第一回合的历史记录，但凭着上一个回合的信息，模型还是有可能推断出昨天来的人买花的真实意图。</p><p>尽管这种方法不适合记住遥远的互动，但它非常擅长限制使用的Token数量。如果只需要记住最近的互动，缓冲窗口记忆是一个很好的选择。但是，如果需要混合远期和近期的互动信息，则还有其他选择。</p><h2>使用ConversationSummaryMemory</h2><p>上面说了，如果模型在第二轮回答的时候，能够说出“我可以帮你为你姐姐找到…”，那么在第三轮回答时，即使窗口大小 k=1，还是能够回答出正确答案。</p><p>这是为什么？</p><p>因为模型<strong>在回答新问题的时候，对之前的问题进行了总结性的重述</strong>。</p><p>ConversationSummaryMemory（<strong>对话总结记忆</strong>）的思路就是将对话历史进行汇总，然后再传递给 {history} 参数。这种方法旨在通过对之前的对话进行汇总来避免过度使用 Token。</p><p>ConversationSummaryMemory有这么几个核心特点。</p><ol>\n<li>汇总对话：此方法不是保存整个对话历史，而是每次新的互动发生时对其进行汇总，然后将其添加到之前所有互动的“运行汇总”中。</li>\n<li>使用LLM进行汇总：该汇总功能由另一个LLM驱动，这意味着对话的汇总实际上是由AI自己进行的。</li>\n<li>适合长对话：对于长对话，此方法的优势尤为明显。虽然最初使用的 Token 数量较多，但随着对话的进展，汇总方法的增长速度会减慢。与此同时，常规的缓冲内存模型会继续线性增长。</li>\n</ol><p>下面，我们来看看使用ConversationSummaryMemory的代码示例。</p><pre><code class=\"language-plain\">from langchain.chains.conversation.memory import ConversationSummaryMemory\n\n# 初始化对话链\nconversation = ConversationChain(\n&nbsp; &nbsp; llm=llm,\n&nbsp; &nbsp; memory=ConversationSummaryMemory(llm=llm)\n)\n</code></pre><p>第一回合的输出：</p><pre><code class=\"language-plain\">{'input': '我姐姐明天要过生日，我需要一束生日花束。', \n'history': '', \n'response': ' 我明白，你需要一束生日花束。我可以为你提供一些建议吗？我可以推荐一些花束给你，比如玫瑰，康乃馨，百合，仙客来，郁金香，满天星等等。挑选一束最适合你姐姐的生日花束吧！'}\n</code></pre><p>第二回合的输出：</p><pre><code class=\"language-plain\">{'input': '她喜欢粉色玫瑰，颜色是粉色的。', \n'history': \"\\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked the AI for advice on what type of flower bouquet to get for their sister's birthday, to which the AI provided a variety of suggestions.\", \n'response': ' 为了为你的姐姐的生日准备一束花，我建议你搭配粉色玫瑰和白色康乃馨。你可以在玫瑰花束中添加一些紫色的满天星，或者添加一些绿叶以增加颜色对比。这将是一束可爱的花束，让你姐姐的生日更加特别。'}\n</code></pre><p>第三回合的输出：</p><pre><code class=\"language-plain\">{'input': '我又来了，还记得我昨天为什么要来买花吗？', \n'history': \"\\n\\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked the AI for advice on what type of flower bouquet to get for their sister's birthday, to which the AI suggested pink roses and white carnations with the addition of purple aster flowers and green leaves for contrast. This would make a lovely bouquet to make the sister's birthday extra special.\",\n'response': ' 确实，我记得你昨天想买一束花给你的姐姐作为生日礼物。我建议你买粉红色的玫瑰花和白色的康乃馨花，再加上紫色的雏菊花和绿叶，这样可以让你的姐姐的生日更加特别。'}\n</code></pre><p>看得出来，这里的 <code>'history'</code>，不再是之前人类和AI对话的简单复制粘贴，而是经过了总结和整理之后的一个综述信息。</p><p>这里，我们<strong>不仅仅利用了LLM来回答每轮问题，还利用LLM来对之前的对话进行总结性的陈述，以节约Token数量</strong>。这里，帮我们总结对话的LLM，和用来回答问题的LLM，可以是同一个大模型，也可以是不同的大模型。</p><p>ConversationSummaryMemory的优点是对于长对话，可以减少使用的 Token 数量，因此可以记录更多轮的对话信息，使用起来也直观易懂。不过，它的缺点是，对于较短的对话，可能会导致更高的 Token 使用。另外，对话历史的记忆完全依赖于中间汇总LLM的能力，还需要为汇总LLM使用 Token，这增加了成本，且并不限制对话长度。</p><p>通过对话历史的汇总来优化和管理 Token 的使用，ConversationSummaryMemory 为那些预期会有多轮的、长时间对话的场景提供了一种很好的方法。然而，这种方法仍然受到 Token 数量的限制。在一段时间后，我们仍然会超过大模型的上下文窗口限制。</p><p>而且，总结的过程中并没有区分近期的对话和长期的对话（通常情况下近期的对话更重要），所以我们还要继续寻找新的记忆管理方法。</p><h2>使用ConversationSummaryBufferMemory</h2><p>我要为你介绍的最后一种记忆机制是ConversationSummaryBufferMemory，即<strong>对话总结缓冲记忆</strong>，它是一种<strong>混合记忆</strong>模型，结合了上述各种记忆机制，包括ConversationSummaryMemory 和 ConversationBufferWindowMemory的特点。这种模型旨在在对话中总结早期的互动，同时尽量保留最近互动中的原始内容。</p><p>它是通过max_token_limit这个参数做到这一点的。当最新的对话文字长度在300字之内的时候，LangChain会记忆原始对话内容；当对话文字超出了这个参数的长度，那么模型就会把所有超过预设长度的内容进行总结，以节省Token数量。</p><pre><code class=\"language-plain\">from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n\n# 初始化对话链\nconversation = ConversationChain(\n&nbsp; &nbsp; llm=llm,\n&nbsp; &nbsp; memory=ConversationSummaryBufferMemory(\n&nbsp; &nbsp; &nbsp; &nbsp; llm=llm,\n&nbsp; &nbsp; &nbsp; &nbsp; max_token_limit=300))\n</code></pre><p>第一回合的输出：</p><pre><code class=\"language-plain\">{'input': '我姐姐明天要过生日，我需要一束生日花束。', \n'history': '', \n'response': ' 哇，你姐姐要过生日啊！那太棒了！我建议你去买一束色彩鲜艳的花束，因为这样可以代表你给她的祝福和祝愿。你可以去你家附近的花店，或者也可以从网上订购，你可以看看有没有特别的花束，比如彩色玫瑰或者百合花，它们会更有特色。'}\n</code></pre><p>第二回合的输出：</p><pre><code class=\"language-plain\">{'input': '她喜欢粉色玫瑰，颜色是粉色的。', \n'history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\\nAI:&nbsp; 哇，你姐姐要过生日啊！那太棒了！我建议你去买一束色彩鲜艳的花束，因为这样可以代表你给她的祝福和祝愿。你可以去你家附近的花店，或者也可以从网上订购，你可以看看有没有特别的花束，比如彩色玫瑰或者百合花，它们会更有特色。', \n'response': ' 好的，那粉色玫瑰就是一个很好的选择！你可以买一束粉色玫瑰花束，这样你姐姐会很开心的！你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品'}\n</code></pre><p>第三回合的输出：</p><pre><code class=\"language-plain\">{'input': '我又来了，还记得我昨天为什么要来买花吗？', \n'history': \"System: \\nThe human asked the AI for advice on buying a bouquet for their sister's birthday. The AI suggested buying a vibrant bouquet as a representation of their wishes and blessings, and recommended looking for special bouquets like colorful roses or lilies for something more unique.\\nHuman: 她喜欢粉色玫瑰，颜色是粉色的。\\nAI:&nbsp; 好的，那粉色玫瑰就是一个很好的选择！你可以买一束粉色玫瑰花束，这样你姐姐会很开心的！你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品\", \n'response': ' 是的，我记得你昨天来买花是为了给你姐姐的生日。你想买一束粉色玫瑰花束来表达你的祝福和祝愿，你可以在花店里找到粉色玫瑰，也可以从网上订购，你可以根据你的预算，选择合适的数量。另外，你可以考虑添加一些装饰，比如细绳、彩带或者小礼品}\n</code></pre><p>不难看出，在第二回合，记忆机制完整地记录了第一回合的对话，但是在第三回合，它察觉出前两轮的对话已经超出了300个字节，就把早期的对话加以总结，以节省Token资源。</p><p>ConversationSummaryBufferMemory的优势是通过总结可以回忆起较早的互动，而且有缓冲区确保我们不会错过最近的互动信息。当然，对于较短的对话，ConversationSummaryBufferMemory也会增加Token数量。</p><p>总体来说，ConversationSummaryBufferMemory为我们提供了大量的灵活性。它是我们迄今为止的唯一记忆类型，可以回忆起较早的互动并完整地存储最近的互动。在节省Token数量方面，ConversationSummaryBufferMemory与其他方法相比，也具有竞争力。</p><h2>总结时刻</h2><p>好的，今天我给你介绍了一种对话链和四种类型的对话记忆机制，那么我们可以通过一个表格对这四种类型的记忆做一个整体比较。</p><p>四种记忆机制的比较如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/a0/c0/a06b5db35405b74yy317de917eacbdc0.jpg?wh=1660x640\" alt=\"\"></p><p>网上还有人总结了一个示意图，体现出了当对话轮次逐渐增加时，各种记忆机制对Token的消耗数量。意图向我们表达的是：有些记忆机制，比如说ConversationSummaryBufferMemory和ConversationSummaryMemory，在对话轮次较少的时候可能会浪费一些Token，但是多轮对话过后，Token的节省就逐渐体现出来了。</p><p>当然ConversationBufferWindowMemory对于Token的节省最为直接，但是它会完全遗忘掉K轮之前的对话内容，因此对于某些场景也不是最佳选择。</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/ea/c56yyd7eb61637687de448512yy426ea.png?wh=3676x1478\" alt=\"\" title=\"当对话轮次逐渐增加时，各种记忆机制对 Token 的消耗数量（估算）\"></p><h2>思考题</h2><ol>\n<li>在你的客服聊天机器人设计中，你会首先告知客户：“亲，我的记忆能力有限，只能记住和你的最近10次对话哦。如果我忘了之前的对话，请你体谅我。” 当有了这样的预设，你会为你的ChatBot选择那种记忆机制？</li>\n<li>尝试改变示例程序ConversationBufferWindowMemory中的k值，并增加对话轮次，看看记忆效果。</li>\n<li>尝试改变示例程序ConversationSummaryBufferMemory中的max_token_limit值，看看记忆效果。</li>\n</ol><p>期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2>延伸阅读</h2><ol>\n<li>代码，ConversationBufferMemory的<a href=\"https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/memory/buffer.py\">实现细节</a></li>\n<li>代码，ConversationSummaryMemory的<a href=\"https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/memory/summary.py\">实现细节</a></li>\n</ol>","neighbors":{"left":{"article_title":"09｜链（下）：想学“育花”还是“插花”？用RouterChain确定客户意图","id":703556},"right":{"article_title":"11｜代理（上）：ReAct框架，推理与行动的协同","id":707191}},"comments":[{"had_liked":false,"id":381654,"user_name":"在路上","can_delete":false,"product_type":"c1","uid":1402511,"ip_address":"广东","ucode":"6E31908EFE1107","user_header":"https://static001.geekbang.org/account/avatar/00/15/66/8f/02be926d.jpg","comment_is_top":false,"comment_ctime":1695610712,"is_pvip":false,"replies":[{"id":139018,"content":"很赞！深入源码分析，知其然，知其所以然。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695704071,"ip_address":"新加坡","comment_id":381654,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"我们可以通过源码来分析ConversationSummaryBufferMemory是如何实现长短期记忆的。首先要关注ConversationSummaryBufferMemory.save_context()方法，它将每轮对话的inputs和outputs成对加入memory，然后调用self.prune()方法。prune()方法会计算memory的当前token数，如果超过self.max_token_limit，则对超出的messages总结，调用的方法是self.predict_new_summary(pruned_memory, self.moving_summary_buffer)。总结时使用的PromptTemplate来自prompt.py的_DEFAULT_SUMMARIZER_TEMPLATE，_DEFAULT_SUMMARIZER_TEMPLATE的部分内容如下：\nEXAMPLE\n...\nCurrent summary:\n{summary}\nNew lines of conversation:\n{new_lines}\n也就是通过示例，让llm学习如何完成长期记忆的总结。\n\nConversationSummaryBufferMemory是在应用层平衡长短期记忆，我们也可以看看模型层是如何平衡长短期记忆的。RNN模型t时间步的隐藏层参数H_t计算公式为：H_t = phi(X_t*W_xh+H_(t-1)*W_hh+b_h)，X_t*W_xh表示短期记忆，H_(t-1)*W_hh表示长期记忆，平衡长短期记忆，就是给短期记忆和长期记忆加一个权重，来控制对整体记忆（H_t）的影响。现代循环神经网络GRU和LSTM的模型有区别，但是原理上都是为短期记忆和长期记忆加权重。","like_count":13,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628665,"discussion_content":"很赞！深入源码分析，知其然，知其所以然。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695704071,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3801776,"avatar":"","nickname":"Geek_42e319","note":"","ucode":"234FA8CA3A1013","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":634168,"discussion_content":"_DEFAULT_SUMMARIZER_TEMPLATE 这个prompt可以在外部被修改吗？这个prompt对中文很不友好，都按照英文summary，容易丢失中文关键信息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1703065105,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381676,"user_name":"iLeGeND","can_delete":false,"product_type":"c1","uid":1055475,"ip_address":"北京","ucode":"4055A628A6E97C","user_header":"https://static001.geekbang.org/account/avatar/00/10/1a/f3/41d5ba7d.jpg","comment_is_top":false,"comment_ctime":1695644899,"is_pvip":false,"replies":[{"id":139011,"content":"更新了，同学快去看看。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695701012,"ip_address":"新加坡","comment_id":381676,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"github代码是不是没更新呢","like_count":4,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628656,"discussion_content":"更新了，同学快去看看。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1695701012,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":389094,"user_name":"aloha66","can_delete":false,"product_type":"c1","uid":1469820,"ip_address":"广东","ucode":"60AF4685BF38A2","user_header":"https://static001.geekbang.org/account/avatar/00/16/6d/7c/e91866cf.jpg","comment_is_top":false,"comment_ctime":1711547625,"is_pvip":false,"replies":[{"id":141712,"content":"对于ConversationSummaryBufferMemory没有记住昨天买花原因的问题,可能有几个原因:\n\n(1)第二天的对话重新初始化了一个ConversationChain对象,没有复用第一天的对象,导致记忆丢失。需要确保两天使用的是同一个Chain和Memory对象。\n\n(2)设置的max_token_limit太小,无法存储较长的对话历史。可以尝试适当增大max_token_limit。\n\n(3)用来生成总结的prompt可能不够清晰,没有很好地提炼出买花的原因。可以调整summary_prompt,明确要求总结的要点。\n\n我建议依次排查以上原因,应该可以找出问题所在。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1712809464,"ip_address":"新加坡","comment_id":389094,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"为什么我的ConversationSummaryBufferMemory没记住我昨天为什么要来买花。\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.chains.conversation.memory import ConversationSummaryBufferMemory\nfrom util import base_url\n\n\n# 创建大语言模型实例\nllm = ChatOpenAI(temperature=0.5, base_url=base_url)\n\n\n# 初始化对话链\nconversation = ConversationChain(\n    llm=llm, memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=300)\n)\n\n\n# 第一天的对话\n# 回合1\nresult = conversation.invoke(&quot;我姐姐明天要过生日，我需要一束生日花束。&quot;)\nprint(&quot;回合1&quot;, result)\n# 回合2\nresult = conversation.invoke(&quot;她喜欢粉色玫瑰，颜色是粉色的。&quot;)\n# print(&quot;\\n第二次对话后的记忆:\\n&quot;, conversation.memory.buffer)\nprint(&quot;回合2&quot;, result)\n\n# 第二天的对话\n# 回合3\nresult = conversation.invoke(&quot;我又来了，还记得我昨天为什么要来买花吗？&quot;)\nprint(&quot;回合3&quot;, result)\n# 回复\n回合3 {&#39;input&#39;: &#39;我又来了，还记得我昨天为什么要来买花吗？&#39;, &#39;history&#39;: &#39;Human: 我姐姐明天要过生日，我需要一束生日花束。\\nAI:  \n哇，生日快乐给您的姐姐！您想要什么样的花束呢？玫瑰、郁金香、百合还是其他花卉？您可以选择她喜欢的颜色和花材，我可以帮您找到最合\n适的花束。您知道她喜欢的花吗？\\nHuman: 她喜欢粉色玫瑰，颜色是粉色的。\\nAI: 粉色玫瑰是一个很浪漫的选择！我会帮您找到一束粉色玫 \n瑰花束。您希望花束里还有其他花卉搭配吗？比如一些绿叶或者其他颜色的花朵？您还有其他要求吗？让我知道，我会为您找到完美的生日花束\n。&#39;, &#39;response&#39;: &#39;抱歉，我无法记住之前的对话内容。您可以告诉我您昨天为什么要来买花吗？我会尽力帮助您找到合适的花束。&#39;} ","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641602,"discussion_content":"对于ConversationSummaryBufferMemory没有记住昨天买花原因的问题,可能有几个原因:\n\n(1)第二天的对话重新初始化了一个ConversationChain对象,没有复用第一天的对象,导致记忆丢失。需要确保两天使用的是同一个Chain和Memory对象。\n\n(2)设置的max_token_limit太小,无法存储较长的对话历史。可以尝试适当增大max_token_limit。\n\n(3)用来生成总结的prompt可能不够清晰,没有很好地提炼出买花的原因。可以调整summary_prompt,明确要求总结的要点。\n\n我建议依次排查以上原因,应该可以找出问题所在。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712809464,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1722719,"avatar":"","nickname":"Geek_Mask","note":"","ucode":"8F12201D679C25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653345,"discussion_content":"我发现你把&#34;明天&#34;去掉他就能回答了，也就是说，他根据你的问问题的时态断定对话的昨天不是指今天。这个应该还是模型的能力问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1730717641,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382341,"user_name":"Webber","can_delete":false,"product_type":"c1","uid":2776202,"ip_address":"韩国","ucode":"F9662546472FA7","user_header":"https://static001.geekbang.org/account/avatar/00/2a/5c/8a/244128ff.jpg","comment_is_top":false,"comment_ctime":1697094432,"is_pvip":false,"replies":[{"id":139284,"content":"好问题。可以做到，大概这样，同学可以试试补全下面代码。\n\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.agents import Tool\nfrom langchain.agents import AgentType\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentExecutor\n\nmemory = ConversationBufferMemory() \n\nagent_executor = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\nagent_executor = AgentExecutor(\n    agent=agent, \n    tools=tools,\n    memory=memory\n)\n\n","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697425378,"ip_address":"瑞士","comment_id":382341,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，ConversationChain中可以加入memory机制，但是agents中怎么加入memory机制中呢。initialize_agent函数中的参数没有Chain类型，只是LLM类型。","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629597,"discussion_content":"好问题。可以做到，大概这样，同学可以试试补全下面代码。\n\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.agents import Tool\nfrom langchain.agents import AgentType\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.llms import OpenAI\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentExecutor\n\nmemory = ConversationBufferMemory() \n\nagent_executor = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\nagent_executor = AgentExecutor(\n    agent=agent, \n    tools=tools,\n    memory=memory\n)\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697425379,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1763638,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e9/36/96294f1d.jpg","nickname":"程序员在修行","note":"","ucode":"384F2D1FDB7345","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":636026,"discussion_content":"老师, 我是直接用的csv agent, 看起来不work. Google了一下午, 似乎没有解, 麻烦帮忙看下. 谢谢\nmemory = ConversationBufferMemory()\nagent = create_csv_agent(OpenAI(temperature=0), file_path, verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,memory=memory)\nprint(agent.memory)\nprint(agent.run(&#34;how many records we have&#34;));  # =&gt; There are 100 records in the dataframe.\nprint(agent.run(&#34;how many records we have&#34;));  # =&gt; 仍然执行了相同的路径来计算 看起来没有利用memory\nprint(agent.memory)  # =&gt; None","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705482198,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"日本","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381800,"user_name":"抽象派","can_delete":false,"product_type":"c1","uid":2599971,"ip_address":"广东","ucode":"6879F90CB702FC","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/YflLdCdbUAkfr9LPzF50EibDrMxBibPicQ5NNAETaPP0ytTmuR3h6QNichDMhDbR2XelSIXpOrPwbiaHgBkMJYOeULA/132","comment_is_top":false,"comment_ctime":1695882938,"is_pvip":false,"replies":[{"id":139087,"content":"是的，就不太准确了。以后的大模型会逐渐接收更长的Token窗口。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1696058644,"ip_address":"新加坡","comment_id":381800,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"如果对话的内容跨度比较广，是不是总结出来的就不太准确了？","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628866,"discussion_content":"是的，就不太准确了。以后的大模型会逐渐接收更长的Token窗口。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696058644,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386265,"user_name":"humor","can_delete":false,"product_type":"c1","uid":1181867,"ip_address":"浙江","ucode":"9B48C4C7BEC92C","user_header":"https://static001.geekbang.org/account/avatar/00/12/08/ab/caec7bca.jpg","comment_is_top":false,"comment_ctime":1704420874,"is_pvip":false,"replies":[{"id":140906,"content":"嗯嗯对的，同学对人类记忆的思考和补充非常深刻。其实我们人脑才是无穷无尽的宝藏。我们有不如机器的部分，也有远超机器的设定。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705065711,"ip_address":"瑞士","comment_id":386265,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，问个题外话\n\n说到记忆，我们人类的大脑也不是无穷无尽的。所以说，有的时候事情太多，我们只能把有些遥远的记忆抹掉。毕竟，最新的经历最鲜活，也最重要。\n\n你在文中说人类的记忆不是无穷无尽的，我们只能把遥远的记忆抹掉。但是我觉得人类的记忆可以认为是无穷无尽的，因为我们几乎不可能耗尽我们的记忆空间，大脑还有极强的可塑性，而且我们人类也没办法直接抹去遥远的记忆吧。人类会对于印象深刻的记忆或者经常重复的记忆保留很长时间，并不仅仅与时间有关，如果仅与时间有关的话，我们学习的意义就没了啊，因为我们就只记得刚学过的东西了🤔","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635710,"discussion_content":"嗯嗯对的，同学对人类记忆的思考和补充非常深刻。其实我们人脑才是无穷无尽的宝藏。我们有不如机器的部分，也有远超机器的设定。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705065712,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381669,"user_name":"阿斯蒂芬","can_delete":false,"product_type":"c1","uid":1024164,"ip_address":"广东","ucode":"61D5E3BDA4EBC5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg","comment_is_top":false,"comment_ctime":1695633610,"is_pvip":false,"replies":[{"id":139075,"content":"差不多一个意思哈。至于做汇总的模型是否更便宜，我不大清楚。给你一个很好的工具:https:&#47;&#47;gptforwork.com&#47;tools&#47;openai-chatgpt-api-pricing-calculator\n能看到所有模型的价格比较。gpt-3.5-turbo是最物美价廉的，比text-达文西便宜好多呢。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695872463,"ip_address":"新加坡","comment_id":381669,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"“汇总”的理念跟“摘要”的理念是一致的吗？所以是不是实际应用中，专门的“汇总”模型或许价格比Completions 和 chat  模型更便宜？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628807,"discussion_content":"差不多一个意思哈。至于做汇总的模型是否更便宜，我不大清楚。给你一个很好的工具:https://gptforwork.com/tools/openai-chatgpt-api-pricing-calculator\n能看到所有模型的价格比较。gpt-3.5-turbo是最物美价廉的，比text-达文西便宜好多呢。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695872464,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1024164,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg","nickname":"阿斯蒂芬","note":"","ucode":"61D5E3BDA4EBC5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628639,"discussion_content":"来交作业了。\n1. 保留最近的10轮对话，使用ConversationBufferWindowMemory，设置k=10即可\n2. ConversationBufferWindowMemory 中k越大，保留的完整对话历史越多，当然保留越多，也越容易触发tokens限制\n3. ConversationSummaryBufferMemory 的 max_token_limit 影响了其在保留最近和最远的历史对话的策略，这块 @在路上 的作业值得学习！\n\n记录一些上机过程中经验\n* 默认conversation使用的 memory facotry 就是 ConversationBufferMemory\n* 即使使用了ConversationBufferWindowMemory，打印的 memory.buffer 也是完整的记录，只是内部请求llm的时候应该是截断了（此处源码看的不深）\n* 因为换了台设备，重新写代码的时候发现没有 ConversationBufferWindowMemory 只有 ConversationalBufferWindowMemory，而且无法导入ConversationSummaryBufferMemory，看github最新代码，确认是自己本地问题，但查看langchain版本也是最新，无奈使用重新编译源码方式，果然奏效，与老师代码一致了。\n* 在使用 Summary 类的 memory 的时候，会发现时不时出现 history 是英文的情况，这是模型的处理，还是 LangChain 的处理呢？这里存疑。\n","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1695653735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628808,"discussion_content":"咱们先期学习LangChain的同学学的都特别认真，我相信这会给以后学习的同学做出榜样！\n既来之，则学之！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695872625,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381662,"user_name":"骨汤鸡蛋面","can_delete":false,"product_type":"c1","uid":1050002,"ip_address":"上海","ucode":"2AC141A523E710","user_header":"https://static001.geekbang.org/account/avatar/00/10/05/92/b609f7e3.jpg","comment_is_top":false,"comment_ctime":1695625142,"is_pvip":false,"replies":[{"id":139076,"content":"思路对的哈。只是这边我们一般不用“变量”这个词。我觉得同学的意思是，各种Chain的核心就是约定（预定）了很多提升模板的构建方法。这是LangChain给我们带来的一个核心机制。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695874555,"ip_address":"新加坡","comment_id":381662,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，可以认为不同的chain都对应一个prompt，约定了很多 变量，memory 这些机制都预定了自己可以提供哪些变量嘛？\n","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628811,"discussion_content":"思路对的哈。只是这边我们一般不用“变量”这个词。我觉得同学的意思是，各种Chain的核心就是约定（预定）了很多提升模板的构建方法。这是LangChain给我们带来的一个核心机制。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695874555,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393025,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1722517703,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"用以下通义千问代码代替OpenAI ，学习课程代码。\n旧代码：\n# 初始化大语言模型\n llm = OpenAI(\n     temperature=0.5,\n     model_name=&quot;text-davinci-003&quot;\n )\n\n新代码：\nfrom langchain_openai import ChatOpenAI\n# 初始化大语言模型\nllm = ChatOpenAI(\n    api_key=&quot;ｘｘｘｘ&quot;, 　# 此处用您的DASHSCOPE_API_KEY进行替换\n    base_url=&quot;https:&#47;&#47;dashscope.aliyuncs.com&#47;compatible-mode&#47;v1&quot;, # 填写DashScope base_url\n    model=&quot;qwen-plus&quot;\n    )\n\n但是使用ConversationSummaryBufferMemory 时，出现NotImplementedError: get_num_tokens_from_messages() is not presently implemented for model cl100k_base.  搜索了很多资料，没有找到简单的替代方法。可能是阿里云的某些特定模型不支持某些功能。","like_count":1},{"had_liked":false,"id":392502,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1721113751,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":2,"score":2,"product_id":100617601,"comment_content":"第10讲打卡~\n对于记忆系统，可以尝试结合ConversationSummaryBufferMemory+RAG的方案，即短期全量记忆+长期摘要记忆+向量数据库辅助记忆，这样的记忆方式可能更好地平衡性能、准确性和Token成本","like_count":1},{"had_liked":false,"id":389743,"user_name":"王凯","can_delete":false,"product_type":"c1","uid":1066040,"ip_address":"北京","ucode":"52FD9ED35AA506","user_header":"https://static001.geekbang.org/account/avatar/00/10/44/38/8e05dac1.jpg","comment_is_top":false,"comment_ctime":1713358131,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"langchian里的memory记忆机制面对多用户同时提问，是怎么区分不同的用户的历史记录的？","like_count":1},{"had_liked":false,"id":394376,"user_name":"刘双荣","can_delete":false,"product_type":"c1","uid":3870283,"ip_address":"北京","ucode":"FB42C207F24A18","user_header":"https://static001.geekbang.org/account/avatar/00/3b/0e/4b/ff4a21de.jpg","comment_is_top":false,"comment_ctime":1726653296,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"提到了4种会义记忆模式，","like_count":0},{"had_liked":false,"id":389687,"user_name":"onemao","can_delete":false,"product_type":"c1","uid":1969439,"ip_address":"新加坡","ucode":"1CB4101525C2D1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/rURvBicplInVqwb9rX21a4IkcKkITIGIo7GE1Tcp3WWU49QtwV53qY8qCKAIpS6x68UmH4STfEcFDJddffGC7lw/132","comment_is_top":false,"comment_ctime":1713252388,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"使用ConversationSummaryMemory, 后面AI的回复姐姐编程妹妹了😄","like_count":0},{"had_liked":false,"id":388608,"user_name":"大师兄","can_delete":false,"product_type":"c1","uid":1136897,"ip_address":"北京","ucode":"37273A01E3A205","user_header":"https://static001.geekbang.org/account/avatar/00/11/59/01/b8709bb9.jpg","comment_is_top":false,"comment_ctime":1710470537,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"请问ConversationSummaryBufferMemory在超过max_token_limit以后是将所有内容进行总结还是将超过max_token_limit的内容进行总结？","like_count":0,"discussions":[{"author":{"id":1052191,"avatar":"https://static001.geekbang.org/account/avatar/00/10/0e/1f/d0472177.jpg","nickname":"厉害了我的国","note":"","ucode":"CD0A54A1B998AA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":639440,"discussion_content":"    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -&gt; None:\n        &#34;&#34;&#34;Save context from this conversation to buffer.&#34;&#34;&#34;\n        super().save_context(inputs, outputs)\n        self.prune()\n\n    def prune(self) -&gt; None:\n        &#34;&#34;&#34;Prune buffer if it exceeds max token limit&#34;&#34;&#34;\n        buffer = self.chat_memory.messages\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\n        if curr_buffer_length &gt; self.max_token_limit:\n            pruned_memory = []\n            while curr_buffer_length &gt; self.max_token_limit:\n                pruned_memory.append(buffer.pop(0))\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\n            self.moving_summary_buffer = self.predict_new_summary(\n                pruned_memory, self.moving_summary_buffer\n            )\n\n看ConversationSummaryBufferMemory的源码，每次保存时，都会调用prune方法进行判断，如果messages长度大于max_token_limit，会按照从远到近的顺序依次把历史message放入到一个待总结的消息列表里，然后对这个消息列表做总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1710671122,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}