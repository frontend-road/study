[{"article_id":425450,"article_title":"开篇词 | 如何高效入门PyTorch？","article_content":"<p>你好，我是方远，欢迎你跟我一起学习PyTorch。</p><p>先做个自我介绍吧！我曾先后供职于百度和腾讯两家公司，任职高级算法研究员，目前在一家国际知名互联网公司Line China担任数据科学家，从事计算机视觉与自然语言处理相关的研发工作，每天为千万级别的流量提供深度学习服务。</p><p>想一想，我进入机器学习与深度学习的研究和应用领域已经有10年的时间了，这是个很有意思的过程。在人工智能快速发展的背景下，各种各样的深度学习框架层出不穷，有当下的主流，也有如今的新秀。</p><p>为什么我会这么说呢？这其实可以追溯到我的研究生时期。最早，我只是把PyTorch，打上了一个新秀的标签，记得那时候，深度学习的浪潮才刚刚兴起，传统的机器学习开始转到深度学习，但是我们能选的框架却十分有限。</p><p>当时在学术界流行的一个深度学习框架是Theano，可能有的同学都没有听说过它。这个框架就像是祖师爷般的存在，从2008年诞生之后的很长一段时间中，它都是深度学习开发和研究的行业标准。</p><p>为了复现论文中的算法，我开始学习Theano。接触之后，我发现它的声明式编程，无论是风格还是逻辑都十分奇特。而且那时候的学习资料很匮乏，只能啃官方的说明文档。如此一来，我觉得Theano十分晦涩难学，入门门槛非常高。</p><!-- [[[read_end]]] --><p>后来，我去到了互联网大厂的核心部门工作。那时学术界已经涌现出了很多深度学习方面的研究，而工业界才刚刚开始将深度学习技术落地。Google 的 TensorFlow 框架于2015年正式开源，而我们的团队也开始着手把深度学习技术应用于文本处理等方向。</p><p>2017年，Google发布了TensorFlow 1.0版本，到了2019年，又发布了2.0新版本。TensorFlow 1.x版本时期，TensorFlow框架拥有大量的用户。不过，问题也非常明显，主要的弊端就是<strong>框架环境配置不兼容，新老版本函数差异也很大，且编程困难</strong>。</p><p>但凡涉及版本更新，总会出现API变化，前后版本不兼容的问题。并且当我阅读别人代码的时候，TensorFlow 1.x的可读性也不是很高。这些问题都增加了我的学习成本。直到TensorFlow 2.x版本，TensorFlow逐渐借鉴了PyTorch的优点，进行了自我完善。</p><p>而与 TensorFlow 同一时期横空出世，也拥有众多用户的一个深度学习框架还有 Keras。Keras 的 API 对用户十分友好，使用起来很容易上手。如果有什么想法需要快速实验，看一看效果，那 Keras 绝对是不二的选择。</p><p>但是，高度模块化的封装也同样会带来弊端，看起来学习Keras似乎十分容易，但我很快就遇到了瓶颈。高度封装就意味着不够灵活，比如说如果需要修改一些网络底层的结构，Keras 所提供的接口就没有支持。在使用Keras的大多数时间里，我们主要都停留在调用接口这个阶段，很难真正学习到深度学习的内容。</p><p>直到 PyTorch 出现，随着使用它的人越来越多，其技术迭代速度跟生态发展速度都很迅猛。如果你在GitHub找到了一个PyTorch项目相关的开源代码，我们可以很容易移植到自己的项目中来，直接站在巨人的肩膀上看世界。</p><p>而且相比前面那些主流框架，<strong>PyTorch有着对用户友好的命令式编程风格</strong>。PyTorch设计得更科学，无需像TensorFlow那样，要在各种API之间切换，操作更加便捷。</p><p>PyTorch 的环境配置也很方便，各种开发版本都能向下兼容，不存在老版本的代码在新版本上无法使用的困扰，而且PyTorch跟NumPy的风格比较像，能轻易和Python生态集成起来，我们只需掌握NumPy和基本的深度学习概念即可上手，在网络搭建方面也是快捷又灵活。</p><p>另外，PyTorch 在debug代码的过程也十分方便，可以随时输出中间向量结果。用PyTorch就像在Python中使用print一样简单，只要把一个pdb断点扔进PyTorch模型里，直接就能用。</p><p>因为它的优雅灵活和高效可用，吸引了越来越多的人学习。如果还有人只把PyTorch当成一个新秀，觉得PyTroch不过是个“挑战者”，试图在TensorFlow主导的世界里划出一片自己的地盘。那么数据可以证明，这种想法已经时过境迁。事实上，PyTorch无论在学术界还是在工业界，都已经霸占了半壁江山。</p><p>从学术界来看，2019年之前，TensorFlow还是各大顶会论文选择的主流框架，而2019年之后，顶会几乎成了PyTorch的天下，此消彼长，PyTorch只用了一年的时间。</p><p><img src=\"https://static001.geekbang.org/resource/image/96/57/96afc1a6981ea2f8a2d847584ccb7c57.jpg?wh=900x559\" alt=\"\"></p><p>要知道，<strong>机器学习这个领域始终是依靠研究驱动的，工业界自然也不能忽视科学研究的成果</strong>。就拿我所在的团队来说，现在也已经逐步向PyTorch框架迁移，新开展的项目都会首选用PyTorch框架进行实现。</p><p>不得不说，<strong>PyTorch的应用范围已经逐渐扩大，同时也促进了其生态建设的发展</strong>。由于现在越来越多的开发者都在使用PyTroch，一旦我们的程序遇到了error或bug，很容易就可以在开发论坛上寻找到解决方案。</p><p><strong>总之，一旦你掌握了PyTorch，就相当于走上了深度学习、机器学习的快车道。</strong>以后学习其他深度学习框架也可以快速入门，融会贯通。</p><p>如果你即将或者已经进入了深度学习和机器学习相关领域，PyTorch能够帮你快速实现模型与算法的验证，快速完成深度学习模型部署，提供高并发服务，还可以轻松实现图像生成、文本分析、情感分析等有趣的实验。另外有很多算法相关的岗位，也同样会要求你熟练使用PyTorch等工具。</p><p>可以探索的方向还有很多，这里就不一一列举了。那么问题来了，既然PyTorch有这么多优点，我们要怎样快速上手呢？</p><p>只看原理好比空中楼阁，而直接实战对初学者来说又相对困难。因此我推荐的方法是，先理一个整体框架，有了整体认知之后，再通过实战练习巩固认知。</p><p>具体来说，我们要先把框架的基本语法大致了解一下，然后尽快融入到一个实际项目当中，看一看在实际任务中，我们是怎么基于框架去解决一个问题的。这个专栏，也正是沿着这样一个思路设计的。我在专栏里给你提供了丰富的代码和实战案例，可以帮助你快速上手PyTorch。</p><p><img src=\"https://static001.geekbang.org/resource/image/08/7a/08b96da4677066769fe3e6246f70237a.jpg?wh=1920x1418\" alt=\"图片\" title=\"专栏知识地图\"></p><p>通过这个专栏，你将会熟练使用 PyTorch 工具，解决自己的问题，这是这个专栏要实现的最基础的目标。</p><p>除了掌握工具用法之上，我希望交付的终点是让你获得分析问题的能力和解决问题的方法，让你懂得如何优化你自己的算法与模型。在学习经验方面，我希望这个专栏为你打开一扇窗，让你知道走深度学习这条路，需要有怎样的知识储备。</p><p>为了让你由入门到精通，我把专栏分成了三个递进的部分。</p><p><strong>基础篇</strong></p><p>简要介绍PyTorch的发展趋势与框架安装方法，以及 PyTorch的前菜——NumPy的常用操作。我们约定使用PyTorch 1.9.0 版本，以及默认你已经掌握了Python编程与简单的机器学习基础，不过你也不用太过担心，遇到新知识的我基本都会从0开始讲起的。</p><p><strong>模型训练篇</strong></p><p>想要快速掌握一个框架，就要从核心模块入手。在这个部分，会结合深度学习模型训练的一系列流程，为你详解自动求导机制、搭建网络、更新模型参数、保存与加载模型、训练过程可视化、分布式训练等等模块，带你具体看看PyTorch 能给我们提供怎样的帮助。通过这个部分的学习，你就能基于PyTorch搭建网络模型了。</p><p><strong>实战篇</strong></p><p>我们整个专栏都是围绕 PyTorch 框架在具体项目实践中的应用来讲的，所以最后我还会结合当下流行的图像与自然语言处理任务，串联前面两个模块的内容，为你深入讲解 PyTorch 如何解决实际问题。</p><p>总之，<strong>除了交付给你一个系统的PyTorch技术学习框架，我还希望给你传递我在深度学习这条路上的经验思考</strong>。</p><p>最后，给你一点建议，对于学习PyTorch来说，边学边查、边练边查是个很好的方法。因为在我们实际做项目的时候，肯定会遇到一些之前没有使用过的函数，自己去查的话可以很好地加强记忆。</p><p>当然，我也会尽心做好一个引路人，带你一步步实现课程目标，也期待你能以更加积极的状态投入到本次的学习之旅。现在就让我们一起探索PyTorch，打开深度学习的大门吧！</p>","neighbors":{"left":[],"right":{"article_title":"01 | PyTorch：网红中的顶流明星","id":425463}}},{"article_id":425463,"article_title":"01 | PyTorch：网红中的顶流明星","article_content":"<p>你好，我是方远。</p><p>从这节课开始，我们正式进入PyTorch基础篇的学习。</p><p>在基础篇中，我们带你了解PyTorch的发展趋势与框架安装方法，然后重点为你讲解NumPy和 Tensor的常用知识点。</p><p>掌握这些基础知识与技巧，能够让你使用 PyTorch 框架的时候更高效，也是从头开始学习机器学习与深度学习迈出的第一步。磨刀不误砍柴工，所以通过这个模块，我们的目标是做好学习的准备工作。</p><p>今天这节课，我们先从PyTorch的安装和常用编程工具说起，先让你对PyTorch用到的语言、工具、技术做到心里有数，以便更好地开启后面的学习之旅。</p><h2>PyTorch登场</h2><p>为什么选择 PyTorch 框架，我在开篇词就已经说过了。从19年起，无论是学术界还是工程界 PyTorch 已经霸占了半壁江山，可以说 PyTorch 已经是现阶段的主流框架了。</p><p>这里的Py我们不陌生，它就是Python，那Torch是什么？从字面翻译过来是一个“火炬”。</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/8d/8b83b03c5e25886e1c6fe5aed8572e8d.png?wh=480x141\" alt=\"图片\"></p><p>什么是火炬呢？其实这跟TensorFlow中的Tensor是一个意思，我们可以把它看成是<strong>能在GPU中计算的矩阵</strong>。</p><p>那PyTorch框架具体是怎么用的呢？说白了就是一个计算的工具。借助它，我们就能用计算机完成复杂的计算流程。</p><!-- [[[read_end]]] --><p>但是我们都知道，机器跟人类的“语言”并不相通，想要让机器替我们完成对数据的复杂计算，就得先把数据翻译成机器能够理解的内容。无论是图像数据、文本数据还是数值数据，都要转换成矩阵才能进行后续的变化和运算。</p><p>搞定了读入数据这一步，我们就要靠PyTorch搞定后面各种复杂的计算功能。这些所有的计算功能，包括了从前向传播到反向传播，甚至还会涉及其它非常复杂的计算，而这些计算统统要交给 PyTorch 框架实现。</p><p>PyTorch会把我们需要计算的矩阵传入到GPU（或CPU）当中，在GPU（或CPU）中实现各种我们所需的计算功能。<strong>因为GPU做矩阵运算比较快，所以在神经网络中的计算一般都首选使用GPU，但对于学习来说，我们用CPU就可以了</strong>。</p><p>而我们要做的就是，设计好整个任务的流程、整个网络架构，这样PyTorch才能顺畅地完成后面的计算流程，从而帮我们正确地计算。</p><h2>安装PyTorch及其使用环境</h2><p>在 PyTorch 安装之前，还有安装 Python3 以及 pip 这些最基础的操作，这些你在网上随便搜一下就能找到，相信你可以独立完成。</p><p>这里我直接从安装 PyTorch开始说，PyTorch 安装起来非常非常简单，方法也有很多，这里我们先看看最简单的方法：使用 pip 安装。</p><h3>使用pip安装PyTorch</h3><p>CPU版本安装：</p><pre><code class=\"language-plain\"># Linux\npip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n# Mac &amp; Windows\npip install torch torchvision torchaudio\n</code></pre><p>GPU版本安装：（默认CUDA 11.1 版本）</p><pre><code class=\"language-plain\"># Linux &amp; Windows\npip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre><p>我们只需要将上面的命令复制到计算机的命令行当中，即可实现快速安装。</p><p>这里有两个版本，一个GPU版本，一个CPU版本。建议你最好选择安装GPU版本，也就是说我们的硬件设备中最好有英伟达独立显卡。用GPU训练深度学习模型是非常快速的，所以在实际项目中都是使用GPU来训练模型。</p><p>但是如果说大家手里没有供开发使用的英伟达GPU显卡的话，那么安装CPU版本也是可以的，在学习过程中，CPU也足够让我们的小实验运行起来。</p><p>另外，安装GPU版本前，需要安装对应版本的CUDA工具包。我们可以到<a href=\"https://developer.nvidia.com/cuda-downloads\">英伟达官网</a>，选择相应操作系统的CUDA工具包，进行下载与安装。硬件设备中无英伟达显卡的，可以略过这部分。</p><p>目前 PyTorch 的稳定版本是 1.9.0，后续如果 PyTorch 的版本升级更新了，我们再将命令中的版本号稍作修改就可以了。</p><h3>其它方法安装PyTorch</h3><p><a href=\"https://pytorch.org/get-started/locally/\">这里</a>是PyTorch的官网，在页面如下图所示的位置，我们可以看到有一些配置选项和安装命令。</p><p><img src=\"https://static001.geekbang.org/resource/image/31/7d/31c693e2433525636f302a6e066c137d.png?wh=1624x624\" alt=\"图片\"></p><p>我们可以根据页面上的指引，依次选择PyTorch的版本、你的操作系统、安装方式、编程语言以及计算平台，然后根据最下方的执行命令进行安装即可。</p><p>值得注意的是，<strong>Mac的操作系统只能安装CPU版本</strong>。我尝试下来最简单的方式，还是使用pip来安装。</p><h3>验证是否安装成功</h3><p>你在终端中输入“python”，就可以进入到Python交互模式。</p><p>首先输入如下代码，如果没有报错，就意味着PyTorch已经顺利安装了。</p><pre><code class=\"language-python\">import torch\n</code></pre><p>接下来，输入下面的代码，如果输出结果是“True”，意味着你可以使用 GPU。 这行代码的意思是检测GPU是否可用。</p><pre><code class=\"language-python\">torch.cuda.is_available()\n</code></pre><p>这里你也许会有疑问，为什么我安装的明明是 GPU 版本，但是代码却返回了“False”，显示GPU不可用呢？</p><p>对于这个问题，我们依次按照下面的步骤进行检查。</p><p>1.检查计算机上是否有支持CUDA的GPU。</p><p>首先查看电脑的显卡型号以及是否有独立显卡，如果没有以“NVIDIAN”名称开头的独立显卡，则不能支持CUDA，因此GPU不可用。</p><p>然后，你可以在<a href=\"https://developer.nvidia.com/zh-cn/cuda-gpus#compute\">这个页面</a>查询GPU是否支持CUDA。如果你的GPU型号在页面的列表中，则表示你的计算机搭载了能够利用 CUDA 加速应用的现代 GPU，否则GPU也不可用。</p><p>若GPU支持CUDA，你还需要确保已经完成了上面介绍过的CUDA工具包的安装。</p><p>2.检查显卡驱动版本。</p><p>在终端中输入“nvidia-smi”命令，会显示出显卡驱动的版本和CUDA的版本，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/58/4b/587018eab4418fb70fc2ee76a72fcd4b.jpg?wh=587x401\" alt=\"图片\"></p><p>如果显卡驱动的版本过低，与CUDA版本不匹配，那么GPU也不可用，需要根据显卡的型号更新显卡驱动。</p><p>我用表格的方式，帮你梳理了CUDA版本与GPU驱动版本的对应关系，你可以根据自己计算机驱动的情况对照查看。例如，CUDA 11.1支持的 Linux驱动程序是450.80.02以上。<br>\n<img src=\"https://static001.geekbang.org/resource/image/34/ff/34007081a277979659f81019a5d559ff.jpg?wh=3168x1648\" alt=\"\"></p><p>我们可以在<a href=\"https://www.nvidia.com/Download/index.aspx?lang=cn\">这里</a>下载并安装显卡驱动程序。</p><p>3.检查PyTorch版本和CUDA版本是否匹配？</p><p>PyTorch版本同样与CUDA版本有对应关系，我们可以在<a href=\"https://pytorch.org/get-started/previous-versions/\">这个页面</a>查看它们之间的对应关系。如果两者版本不匹配，可以重新安装对应版本的PyTorch，或者升级CUDA工具包。</p><h3>使用Docker</h3><p>通过Docker使用PyTorch也非常简单，连安装都不需要，但是前提是你需要熟悉有关Docker的知识。</p><p>如果你会熟练地使用Docker，我推荐后面这个网页链接供你参考，<a href=\"https://hub.docker.com/r/pytorch/pytorch/tags\">这里</a>有很多的PyTorch的Docker镜像，你可以找到自己需要的镜像，然后拉取一个镜像到你的服务器或者本地，直接启动就可以了，无需额外的环境配置。</p><h2>常用编程工具</h2><p>在使用PyTorch进行编程之前，我们先来看看几个常用的编程工具，但是并不要求你必须使用它们，你可以根据自己的喜好自由选择。</p><h3>Sublime Text</h3><p>Sublime Text是一个非常轻量且强大的文本编辑工具，内置了很多快捷的功能，对于我们开发来说非常便捷。</p><p><img src=\"https://static001.geekbang.org/resource/image/5e/9f/5e9506c5188f8b0ac687f7b14f271e9f.jpg?wh=1920x1260\" alt=\"图片\"></p><p>例如，它可以自动为项目中的类、方法和函数生成索引，让我们可以跟踪代码。具体就是通过它的goto anything功能，根据一些关键字，查找到项目中的对应的代码行。另外，它能支持的插件功能也很丰富。</p><h3>PyCharm</h3><p>PyCharm&nbsp;作为一款针对&nbsp;Python&nbsp;的编辑器，配置简单、功能强大，使用起来省时省心，对初学者十分友好。它拥有一般 IDE 所具备的功能，比如：语法高亮、项目管理、代码跳转、代码补全、调试、单元测试、版本控制等等。</p><p><img src=\"https://static001.geekbang.org/resource/image/76/87/76293c15f3c202e99213a4c3c5d09887.jpg?wh=1904x1032\" alt=\"\"></p><h3>Vim</h3><p>Vim是Linux系统中的文本编辑工具，非常方便快捷，并且很强大。我们在项目中经常用到它。</p><p>在我们的项目中，经常是需要登录到服务器上进行开发的，服务器一般都是Linux系统，不会有Sublime Text与PyCharm，所以，我们用Vim打开代码，直接去进行编辑就可以了。</p><p>对于没有接触过Linux，或者一直习惯使用IDE来编程开发的同学，初步接触的时候，可能觉得Vim不是很方便，但实际上，Vim包含了丰富的快捷键，对于Shell与Python的开发来说非常高效。</p><p>但是Vim的缺点正如刚才所说，你需要去学习它的使用方法，有一点点门槛，但是只要你学会了，我保证你将对它爱不释手（这里也推荐有需要的同学去看看隔壁的<a href=\"https://time.geekbang.org/column/intro/100055801\">《Vim 实用技巧必知必会》</a>专栏）。</p><h3>Jupyter Notebook&amp;Lab</h3><p>Jupyter Notebook 是一个开源的Web应用，这也是我最想推荐给你的一个工具。它能够让你创建和分享包含可执行代码、可视化结构和文字说明的文档。</p><p><strong>在后面的课程，如果涉及图片生成或结果展示，我们也会使用到 Jupyter Notebook，这里推荐你先安装好</strong>。</p><p>简而言之，Jupyter Notebook是以网页的形式打开，可以在网页页面中直接编写代码和运行代码，代码的运行结果也会直接在代码块下显示。比如在编程过程中需要编写说明文档，可以在同一个页面中直接编写，便于及时说明、解释。</p><p>而 Jupyter Lab 可以看做是 Jupyter Notebook 的终极进化版，它不但包含了Jupyter Notebook所有功能，并且集成了操作终端、打开交互模式、查看csv文件及图片等功能。</p><p><img src=\"https://static001.geekbang.org/resource/image/fe/a4/fe289768b33b8aa2ce53e743f5d14fa4.png?wh=1322x914\" alt=\"图片\"></p><p>Jupyter Notebook在我们的深度学习领域非常活跃。在实验测试阶段，相比用py文件来直接编程，还是Jupyter Notebook方便一些。在项目结束之后如果要书写项目报告，我觉得用Jupyter也比较合适。</p><h4>使用pip安装Jupyter</h4><p>通过pip安装 Jupyter Notebook的命令如下。</p><pre><code class=\"language-plain\">pip install jupyter\n</code></pre><p>通过pip安装Jupyter Lab的命令如下。</p><pre><code class=\"language-plain\">pip install jupyterlab\n</code></pre><h4>启动Jupyter</h4><p>完成安装，就可以启动了。我们直接在终端中，执行下面的命令，就可以启动  Jupyter Notebook。</p><pre><code class=\"language-shell\">jupyter notebook\n</code></pre><p>启动 Jupyter Lab需要在终端执行如下命令。</p><pre><code class=\"language-plain\">jupyter lab\n</code></pre><p>不管在macOS系统里，还是在Windows系统，通过以上任意一种方式启动成功之后，浏览器都会自动打开Jupyter Notebook或者Jupyter Lab的开发环境（你可以回顾下“Jupyter Notebook &amp; Lab”那个例子里的界面）。</p><h4>运行Jupyter Notebook</h4><p>进入到 Jupyter Notebook 的界面，我们尝试新建一个Python的Notebook。具体操作方法如下图所示。点击“New”下拉菜单，然后点击“Python 3”选项，来创建一个新的Python Notebook。</p><p><img src=\"https://static001.geekbang.org/resource/image/06/27/0609b9a2d166a967ab23764903117b27.jpg?wh=1626x668\" alt=\"\"></p><p>上面我们已经讲过了 PyTorch 的安装方法，我们可以执行下面这段代码，来看看 PyTorch 是否已经安装成功。</p><pre><code class=\"language-python\">import torch\ntorch.__version__\n</code></pre><p>点击运行按钮，我们可以看到代码执行的结果，输出了当前安装的 PyTorch 的版本，即PyTorch 1.9.0 的GPU版本。这说明 PyTorch 框架已经安装成功。</p><p><img src=\"https://static001.geekbang.org/resource/image/68/4f/6804a69022ce814b32990ba94a74474f.jpg?wh=1092x490\" alt=\"\"></p><h2>小结</h2><p>恭喜你完成了这节课的学习。</p><p>今天，我们一起了解了PyTorch 框架的用途，简单来说就是能利用GPU帮我们搞定深度学习中一系列复杂运算的框架。</p><p>想要用好这个工具，我们就得设计好整个任务的流程、整个网络架构，这样PyTorch才能实现各种各样复杂的计算功能。</p><p>之后我们学习了PyTorch 框架的安装方法，我还给你推荐了一些深度学习编程的常用工具。<strong>其中我最推荐的工具就是 Jupyter Notebook，这个工具在深度学习领域里常常会用到，后面课程里涉及图片生成或者结果展示的环节，我们也会用到它</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/ba/c9/babe27a41b8fb46699475bd7b0c521c9.jpg?wh=2944x1837\" alt=\"\"></p><p>课程的准备工作就是这些，让我们一起动手，配置好环境，并选择一个你觉得顺手的开发工具，正式开始PyTorch的探索之旅吧！</p><p>千里之行，始于足下。我在下节课等你，如果你有什么问题，也可以通过留言区和我交流。</p><p>我是方远，我们下一讲见！</p>","neighbors":{"left":{"article_title":"开篇词 | 如何高效入门PyTorch？","id":425450},"right":{"article_title":"02 | NumPy（上）：核心数据结构详解","id":426126}}},{"article_id":426126,"article_title":"02 | NumPy（上）：核心数据结构详解","article_content":"<p>你好，我是方远。</p><p>通过前面两节课，我们已经对PyTorch有了初步的了解，你是不是迫不及待想要动手玩转PyTorch了？先别着急，我们有必要先品尝一道“前菜”，它就是NumPy。</p><p>为什么我们要先拿下NumPy呢？我相信，无论你正在从事或打算入门机器学习，不接触NumPy几乎不可能。现在的主流深度学习框架PyTorch与TensorFlow中最基本的计算单元Tensor，都与NumPy数组有着类似的计算逻辑，可以说掌握了NumPy对学习这两种框架都有很大帮助。</p><p>另外，NumPy还被广泛用在Pandas，SciPy等其他数据科学与科学计算的Python模块中。而我们日常用得越来越多的人脸识别技术（属于计算机视觉领域），其原理本质上就是先把图片转换成NumPy的数组，然后再进行一系列处理。</p><p>为了让你真正吃透NumPy，我会用两节课的内容讲解NumPy。这节课，我们先介绍NumPy的数组、数组的关键属性以及非常重要的轴的概念。</p><h2>什么是NumPy</h2><p>NumPy是用于Python中科学计算的一个基础包。它提供了一个多维度的数组对象（稍后展开），以及针对数组对象的各种快速操作，例如排序、变换，选择等。NumPy的安装方式非常简单，可以使用Conda安装，命令如下：</p><!-- [[[read_end]]] --><pre><code class=\"language-plain\">conda install numpy\n</code></pre><p>或使用pip进行安装，命令如下：</p><pre><code class=\"language-plain\">pip install numpy\n</code></pre><h2>NumPy数组</h2><p>刚才所说的数组对象是NumPy中最核心的组成部分，这个数组叫做ndarray，是“N-dimensional array”的缩写。其中的N是一个数字，指代维度，例如你常常能听到的1-D数组、2-D数组或者更高维度的数组。</p><p>在NumPy中，数组是由numpy.ndarray 类来实现的，它是NumPy的核心数据结构。我们今天的内容就是围绕它进行展开的。</p><p>学习一个新知识，我们常用的方法就是跟熟悉的东西做对比。NumPy数组从逻辑上来看，与其他编程语言中的数组是一样的，索引也是从0开始。而Python中的列表，其实也可以达到与NumPy数组相同的功能，但它们又有差异，做个对比你就能体会到NumPy数组的特点了。</p><p>1.Python中的列表可以动态地改变，而NumPy数组是不可以的，它在创建时就有固定大小了。改变Numpy数组长度的话，会新创建一个新的数组并且删除原数组。<br>\n2.NumPy数组中的数据类型必须是一样的，而列表中的元素可以是多样的。<br>\n3.NumPy针对NumPy数组一系列的运算进行了优化，使得其速度特别快，并且相对于Python中的列表，同等操作只需使用更少的内存。</p><h2>创建数组</h2><p>好，那就让我们来看看NumPy数组是怎么创建的吧？</p><p>最简单的方法就是把一个列表传入到np.array()或np.asarray()中，这个列表可以是任意维度的。np.array()属于深拷贝，np.asarray()则是浅拷贝，它们的区别我们下节课再细讲，这里你有个印象就行。</p><p>我们可以先试着创建一个一维的数组，代码如下。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;import numpy as np\n&gt;&gt;&gt;#引入一次即可\n\n&gt;&gt;&gt;arr_1_d = np.asarray([1])\n&gt;&gt;&gt;print(arr_1_d)\n[1]\n</code></pre><p>再创建一个二维数组：</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d = np.asarray([[1, 2], [3, 4]])\n&gt;&gt;&gt;print(arr_2_d)\n[[1 2]\n [3 4]]\n</code></pre><p>你也可以试试自己创建更高维度的数组。</p><h3>数组的属性</h3><p>作为一个数组，NumPy有一些固有的属性，我们今天来介绍非常常用且关键的数组维度、形状、size与数据类型。</p><h4>ndim</h4><p>ndim表示数组维度（或轴）的个数。刚才创建的数组arr_1_d的轴的个数就是1，arr_2_d的轴的个数就是2。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_1_d.ndim\n1\n&gt;&gt;&gt;arr_2_d.ndim\n2\n</code></pre><h4>shape</h4><p>shape表示数组的维度或形状， 是一个整数的元组，元组的长度等于ndim。</p><p>arr_1_d的形状就是（1，）（一个向量）， arr_2_d的形状就是(2, 2)（一个矩阵）。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_1_d.shape\n(1,)\n&gt;&gt;&gt;arr_2_d.shape\n(2, 2)\n</code></pre><p>shape这个属性在实际中用途还是非常广的。比如说，我们现在有这样的数据(B, W, H, C)，熟悉深度学习的同学肯定会知道，这代表一个batch size 为B的（W，H，C）数据。</p><p>现在我们需要根据（W，H，C）对数据进行变形或者其他处理，这时我们可以直接使用input_data.shape[1:3]获取到数据的形状，而不需要直接在程序中硬编程、直接写好输入数据的宽高以及通道数。</p><p>在实际的工作当中，我们经常需要对数组的形状进行变换，就可以使用arr.reshape()函数，在不改变数组元素内容的情况下变换数组的形状。但是你需要注意的是，<strong>变换前与变换后数组的元素个数需要是一样的</strong>，请看下面的代码。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d.shape\n(2, 2)\n&gt;&gt;&gt;arr_2_d\n[[1 2]\n [3 4]]\n# 将arr_2_d reshape为(4，1)的数组\n&gt;&gt;&gt;arr_2_d.reshape((4，1))\narray([[1],\n       [2],\n       [3],\n       [4]])\n</code></pre><p>我们还可以使用np.reshape(a, newshape, order)对数组a进行reshape，新的形状在newshape中指定。</p><p>这里需要注意的是，reshape函数有个<strong>order参数</strong>，它是指以什么样的顺序读写元素，其中有这样几个参数。</p><ul>\n<li>‘C’：默认参数，使用类似C-like语言（行优先）中的索引方式进行读写。</li>\n<li>‘F’：使用类似Fortran-like语言（列优先）中的索引方式进行读写。</li>\n<li>‘A’：原数组如果是按照‘C’的方式存储数组，则用‘C’的索引对数组进行reshape，否则使用’F’的索引方式。</li>\n</ul><p>reshape的过程你可以这样理解，首先需要根据指定的方式(‘C’或’F’)将原数组展开，然后再根据指定的方式写入到新的数组中。</p><p>这是什么意思呢？先看一个简单的2维数组的例子。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;a = np.arange(6).reshape(2,3)\narray([[0, 1, 2],\n&nbsp; &nbsp; &nbsp; &nbsp;[3, 4, 5]])\n</code></pre><p>我们要将数组a，按照’C’的方式reshape成(3,2)，可以这样操作。首先将原数组展开，对于‘C’的方式来说是行优先，最后一个维度最优先改变，所以展开结果如下，序号那一列代表展开顺序。<br>\n<img src=\"https://static001.geekbang.org/resource/image/46/e1/46dc5efc0fc1ff8yya419d459349cde1.jpg?wh=1185x621\" alt=\"图片\"></p><p>所以，reshape后的数组，是按照0，1，2，3，4，5这个序列进行写入数据的。reshape后的数组如下表所示，序号代表写入顺序。</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/1e/a2e4259d27eae29196616dece4b46d1e.jpg?wh=1240x615\" alt=\"图片\"></p><p>接下来，再看看将数组a，按照’F’的方式reshape成(3,2)要如何处理。</p><p>对于行优先的方式，我们应该是比较熟悉的，而‘F’方式是列优先的方式，这一点对于没有使用过列优先的同学来说，可能比较难理解一点。</p><p>首先是按列优先展开原数组，列优先意味着最先变化的是数组的第一个维度。下表是展开后的结果，序号是展开顺序，这里请注意下<strong>坐标的变换方式</strong>（第一个维度最先变化）。</p><p><img src=\"https://static001.geekbang.org/resource/image/fe/72/fe21a81ab58523edc0d1a84f15yyf372.jpg?wh=1185x621\" alt=\"\"></p><p>所以，reshape后的数组，是按照0，3，1，4，2，5这个序列进行写入数据的。reshape后的数组如下表所示，序号代表写入顺序，为了显示直观，我将相同行以同样颜色显示了。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/6b/26dbe3e14fded552bd8a0515858a476b.jpg?wh=1227x606\" alt=\"图片\"></p><p>这里我给你留一个小练习，你可以试试对多维数组的reshape吗？</p><p>不过，大部分时候还是使用’C’的方式比较多，也就是行优先的形式。至少目前为止我还没有使用过’F’与’A’的方式。</p><h4>size</h4><p>size，也就是数组元素的总数，它就等于shape属性中元素的乘积。</p><p>请看下面的代码，arr_2_d的size是4。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d.size\n4\n</code></pre><h4>dtype</h4><p>最后要说的是dtype，它是一个描述数组中元素类型的对象。使用dtype属性可以查看数组所属的数据类型。</p><p>NumPy中大部分常见的数据类型都是支持的，例如int8、int16、int32、float32、float64等。dtype是一个常见的属性，在创建数组，数据类型转换时都可以看到它。</p><p>首先我们看看arr_2_d的数据类型：</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d.dtype\ndtype('int64')\n</code></pre><p>你可以回头看一下刚才创建arr_2_d的时候，我们并没有指定数据类型，如果没有指定数据类型，NumPy会自动进行判断，然后给一个默认的数据类型。</p><p>我们再看下面的代码，我们在创建arr_2_d时，对数据类型进行了指定。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d = np.asarray([[1, 2], [3, 4]], dtype='float')\n&gt;&gt;&gt;arr_2_d.dtype\ndtype('float64')\n</code></pre><p>数组的数据类型当然也可以改变，我们可以使用astype()改变数组的数据类型，不过改变数据类型会创建一个新的数组，而不是改变原数组的数据类型。</p><p>请看后面的代码。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d.dtype\ndtype('float64')\n&gt;&gt;&gt;arr_2_d.astype('int32')\narray([[1, 2],\n&nbsp; &nbsp; &nbsp; &nbsp;[3, 4]], dtype=int32)\n&gt;&gt;&gt;arr_2_d.dtype\ndtype('float64')\n# 原数组的数据类型并没有改变\n&gt;&gt;&gt;arr_2_d_int = arr_2_d.astype('int32')\n&gt;&gt;&gt;arr_2_d_int.dtype\ndtype('int32')\n</code></pre><p>但是，我想提醒你，<strong>不能通过直接修改数据类型来修改数组的数据类型</strong>，这样代码虽然不会报错，但是数据会发生改变，请看下面的代码：</p><pre><code class=\"language-plain\">&gt;&gt;&gt;arr_2_d.dtype\ndtype('float64')\n&gt;&gt;&gt;arr_2_d.size\n4\n&gt;&gt;&gt;arr_2_d.dtype='int32'\n&gt;&gt;&gt;arr_2_d\narray([[         0, 1072693248,          0, 1073741824],\n       [         0, 1074266112,          0, 1074790400]], dtype=int32)\n</code></pre><p>1个float64相当于2个int32，所以原有的4个float32，会变为8个int32，然后直接输出这个8个int32。</p><h3>其他创建数组的方式</h3><p>除了使用np.asarray或np.array来创建一个数组之外，NumPy还提供了一些按照既定方式来创建数组的方法，我们只需按照要求，提供一些必要的参数即可。</p><h4>np.ones() 与np.zeros()</h4><p>np.ones()用来创建一个全1的数组，必须参数是指定数组的形状，可选参数是数组的数据类型，你可以结合下面的代码进行理解。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;np.ones()\nTraceback (most recent call last):\n&nbsp; File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: ones() takes at least 1 argument (0 given)\n# 报错原因是没有给定形状的参数\n&gt;&gt;&gt;np.ones(shape=(2,3))\narray([[1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp;[1., 1., 1.]])\n&gt;&gt;&gt;np.ones(shape=(2,3), dtype='int32')\narray([[1, 1, 1],\n&nbsp; &nbsp; &nbsp; &nbsp;[1, 1, 1]], dtype=int32)\n</code></pre><p>创建全0的数组是np.zeros()，用法与np.ones()类似，我们就不举例了。</p><p>那这两个函数一般什么时候用呢？例如，如果需要初始化一些权重的时候就可以用上，比如说生成一个2x3维的数组，每个数值都是0.5，可以这样做。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;np.ones((2, 3)) * 0.5\narray([[0.5, 0.5, 0.5],\n&nbsp; &nbsp; &nbsp; &nbsp;[0.5, 0.5, 0.5]]\n</code></pre><h4>np.arange()</h4><p>我们还可以使用np.arange([start, ]stop, [step, ]dtype=None)创建一个在[start, stop)区间的数组，元素之间的跨度是step。</p><p>start是可选参数，默认为0。stop是必须参数，区间的终点，请注意，刚才说的区间是一个<strong>左闭右开区间</strong>，所以数组并不包含stop。step是可选参数，默认是1。</p><pre><code class=\"language-plain\"># 创建从0到4的数组\n&gt;&gt;&gt;np.arange(5)\narray([0, 1, 2, 3, 4])\n# 从2开始到4的数组\n&gt;&gt;&gt;np.arange(2, 5)\narray([2, 3, 4])\n# 从2开始，到8的数组，跨度是3\n&gt;&gt;&gt;np.arange(2, 9, 3)\narray([2, 5, 8])\n</code></pre><h4>np.linspace()</h4><p>最后，我们也可以用np.linspace（start, stop, num=50, endpoint=True, retstep=False, dtype=None）创建一个数组，具体就是创建一个从开始数值到结束数值的等差数列。</p><ul>\n<li>start：必须参数，序列的起始值。</li>\n<li>stop：必须参数，序列的终点。</li>\n<li>num：序列中元素的个数，默认是50。</li>\n<li>endpoint：默认为True，如果为True，则数组最后一个元素是stop。</li>\n<li>retstep：默认为False，如果为True，则返回数组与公差。</li>\n</ul><pre><code class=\"language-plain\"># 从2到10，有3个元素的等差数列\n&gt;&gt;&gt;np.linspace(start=2, stop=10, num=3)\n</code></pre><p>np.arange与np.linspace也是比较常见的函数，比如你要作图的时候，可以用它们生成x轴的坐标。例如，我要生成一个$y=x^{2}$的图片，x轴可以用np.linespace()来生成。</p><pre><code class=\"language-plain\">import numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.arange(-50, 51, 2)\nY = X ** 2\n\nplt.plot(X, Y, color='blue')\nplt.legend()\nplt.show()\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/0c/b4/0c752f2b6777a95d8a373649e4a3a2b4.jpg?wh=1800x1146\" alt=\"图片\"></p><h2>数组的轴</h2><p>这是一个非常重要的概念，也是NumPy数组中最不好理解的一个概念。它经常出现在np.sum()、np.max()这样关键的聚合函数中。</p><p>我们用这样一个问题引出，同一个函数如何根据轴的不同来获得不同的计算结果呢？比如现在有一个(4,3)的矩阵，存放着4名同学关于3款游戏的评分数据。</p><pre><code class=\"language-plain\">&gt;&gt;&gt;interest_score = np.random.randint(10, size=(4, 3))\n&gt;&gt;&gt;interest_score\narray([[4, 7, 5],\n&nbsp; &nbsp; &nbsp; &nbsp;[4, 2, 5],\n&nbsp; &nbsp; &nbsp; &nbsp;[7, 2, 4],\n&nbsp; &nbsp; &nbsp; &nbsp;[1, 2, 4]])\n</code></pre><p>第一个需求是，计算每一款游戏的评分总和。这个问题如何解决呢，我们一起分析一下。<br>\n数组的轴即数组的维度，它是从0开始的。对于我们这个二维数组来说，有两个轴，分别是代表行的0轴与代表列的1轴。如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/de/e14a4f5d6ba946665b7ccf01c58a2dde.jpg?wh=1233x790\" alt=\"图片\"></p><p>我们的问题是要计算每一款游戏的评分总和，也就是沿着0轴的方向进行求和。所以，我们只需要在求和函数中指定沿着0轴的方向求和即可。</p><pre><code class=\"language-plain\">&gt;&gt;&gt; np.sum(interest_score, axis=0)\narray([16, 13, 18])\n</code></pre><p>计算方向如绿色箭头所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/3a/80/3a6bd04c4708d3635e9c92092612e380.jpg?wh=1207x812\" alt=\"图片\"></p><p>第二个问题是要计算每名同学的评分总和，也就是要沿着1轴方向对二维数组进行操作。所以，我们只需要将axis参数设定为1即可。</p><pre><code class=\"language-plain\">&gt;&gt;&gt; np.sum(interest_score, axis=1)\narray([16, 11, 13,&nbsp; 7])\n</code></pre><p>计算方向如绿色箭头所示。<br>\n<img src=\"https://static001.geekbang.org/resource/image/d6/b4/d60ed120c370e376253bee7b362590b4.jpg?wh=1196x790\" alt=\"图片\"></p><p>二维数组还是比较好理解的，那多维数据该怎么办呢？你有没有发现，其实当axis=i时，就是按照第i个轴的方向进行计算的，或者可以理解为第i个轴的数据将会被折叠或聚合到一起。</p><p>形状为(a, b, c)的数组，沿着0轴聚合后，形状变为(b, c)；沿着1轴聚合后，形状变为(a, c)；</p><p>沿着2轴聚合后，形状变为(a, b)；更高维数组以此类推。</p><p>接下来，我们再看一个多维数组的例子。对数组a，求不同维度上的最大值。</p><pre><code class=\"language-plain\">&gt;&gt;&gt; a = np.arange(18).reshape(3,2,3)\n&gt;&gt;&gt; a\narray([[[ 0,&nbsp; 1,&nbsp; 2],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 3,&nbsp; 4,&nbsp; 5]],\n\n&nbsp; &nbsp; &nbsp; &nbsp;[[ 6,&nbsp; 7,&nbsp; 8],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 9, 10, 11]],\n\n&nbsp; &nbsp; &nbsp; &nbsp;[[12, 13, 14],\n&nbsp; &nbsp; &nbsp; &nbsp; [15, 16, 17]]])\n</code></pre><p>我们可以将同一个轴上的数据看做同一个单位，那聚合的时候，我们只需要在同级别的单位上进行聚合就可以了。<br>\n如下图所示，绿框代表沿着0轴方向的单位，蓝框代表着沿着1轴方向的单位，红框代表着2轴方向的单位。</p><p><img src=\"https://static001.geekbang.org/resource/image/0a/b9/0af604dc4661e5512515781bbd7be3b9.jpg?wh=977x838\" alt=\"图片\"></p><p>当axis=0时，就意味着将三个绿框的数据聚合在一起，结果是一个（2，3）的数组，数组内容为：$$\\begin{matrix}<br>\n[ \\ [(max(a_{000},a_{100},a_{200}), max(a_{001},a_{101},a_{201}), max(a_{002},a_{102},a_{202}))], \\\\\\<br>\n[(max(a_{010},a_{110},a_{210}), max(a_{011},a_{111},a_{211}), max(a_{012},a_{112},a_{212}))] \\ ] \\<br>\n\\end{matrix}$$</p><p>代码如下：</p><pre><code class=\"language-plain\">&gt;&gt;&gt; a.max(axis=0)\narray([[12, 13, 14],\n&nbsp; &nbsp; &nbsp; &nbsp;[15, 16, 17]])\n</code></pre><p>当axis=1时，就意味着每个绿框内的蓝框聚合在一起，结果是一个（3，3）的数组，数组内容为：<br>\n$$\\begin{matrix}<br>\n[ \\ [(max(a_{000},a_{010}), max(a_{001},a_{011}), max(a_{002},a_{012}))], \\\\\\<br>\n[(max(a_{100},a_{110}), max(a_{101},a_{111}), max(a_{102},a_{112}))],  \\\\\\<br>\n[(max(a_{200},a_{210}), max(a_{201},a_{211}), max(a_{202},a_{212}))], \\ ] \\<br>\n\\end{matrix}<br>\n$$</p><p>代码如下：</p><pre><code class=\"language-plain\">&gt;&gt;&gt; a.max(axis=1)\narray([[ 3,&nbsp; 4,&nbsp; 5],\n&nbsp; &nbsp; &nbsp; &nbsp;[ 9, 10, 11],\n&nbsp; &nbsp; &nbsp; &nbsp;[15, 16, 17]])\n</code></pre><p>当axis=2时，就意味着每个蓝框中的红框聚合在一起，结果是一个（3，2）的数组，数组内容如下所示：<br>\n$$\\begin{matrix}<br>\n[ \\ [(max(a_{000},a_{001},a_{002}), max(a_{010},a_{011},a_{012}))], \\\\\\<br>\n[(max(a_{100},a_{101},a_{102}), max(a_{110},a_{111},a_{112}))], \\\\\\<br>\n[(max(a_{200},a_{201},a_{202}), max(a_{210},a_{211},a_{212}))], \\ ] \\\\\\<br>\n\\end{matrix}<br>\n$$</p><p>代码如下：</p><pre><code class=\"language-plain\">&gt;&gt;&gt; a.max(axis=2)\narray([[ 2,&nbsp; 5],\n&nbsp; &nbsp; &nbsp; &nbsp;[ 8, 11],\n&nbsp; &nbsp; &nbsp; &nbsp;[14, 17]])\n</code></pre><p>axis参数非常常见，不光光出现在刚才介绍的sum与max，还有很多其他的聚合函数也会用到，例如min、mean、argmin（求最小值下标）、argmax（求最大值下标）等。</p><h2>小结</h2><p>恭喜你完成了这节课的学习。其实你只要有一些其他语言的编程基础，学Numpy还是非常容易的。这里我想再次强调一下为什么NumPy这道前菜必不可少。</p><p>其实Numpy的很多知识点是与PyTorch融会贯通的，例如PyTorch中的Tensor。而且Numpy在机器学习中常常被用到，很多模块都要基于NumPy展开，尤其是在数据的预处理和膜后处理中。</p><p>NumPy是用于Python中科学计算的一个基础包。它提供了一个多维度的数组对象，以及针对数组对象的各种快速操作。为了让你有更直观的体验，我们学习了创建数组的四种方式。</p><p>其中你重点要掌握的方法，就是如何使用<strong>np.asarray</strong>创建一个数组。这里涉及数组属性（ndim、shape、dtype、size）的灵活使用，特别是数组的形状变化与数据类型转换。</p><p>最后，我为你介绍了数组轴的概念，我们需要在数组的聚合函数中灵活运用它。虽然这个概念十分常用，但却不好理解，建议你根据我课程里的例子仔细揣摩一下，从2维数组一步步推理到多维数组，根据轴的不同，数组聚合的方向是如何变化的。</p><p>下一节课，我们要继续学习NumPy中常用且重要的功能。</p><h2>每课一练</h2><p>在刚才用户对游戏评分的那个问题中，你能计算一下每位用户对三款游戏的打分的平均分吗？</p><p>欢迎你在留言区记录你的疑问或者收获，也推荐你把这节课分享给你的朋友。</p>","neighbors":{"left":{"article_title":"01 | PyTorch：网红中的顶流明星","id":425463},"right":{"article_title":"03 | NumPy（下）：深度学习中的常用操作","id":426801}}},{"article_id":426801,"article_title":"03 | NumPy（下）：深度学习中的常用操作","article_content":"<p>你好，我是方远。</p><p>通过上节课的学习，我们已经对NumPy数组有了一定的了解，正所谓实践出真知，今天我们就以一个图像分类的项目为例，看看NumPy的在实际项目中都有哪些重要功能。</p><p>我们先从一个常见的工作场景出发，互联网教育推荐平台，每天都有千万量级的文字与图片的广告信息流入。为了给用户提供更加精准的推荐，你的老板交代你设计一个模型，让你把包含各个平台Logo（比如包含极客时间Logo）的图片自动找出来。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/5d/1ecb3ccdd0b408b0350e255f7e0c875d.png?wh=318x116\" alt=\"图片\"></p><p>想要解决这个图片分类问题，我们可以分解成数据加载、训练与模型评估三部分（其实基本所有深度学习的项目都可以这样划分）。其中数据加载跟模型评估中，就经常会用到NumPy数组的相关操作。</p><p>那么我们先来看看数据的加载。</p><h2>数据加载阶段</h2><p>这个阶段我们要做的就是把训练数据读进来，然后给模型训练使用。训练数据不外乎这三种：图片、文本以及类似二维表那样的结构化数据。</p><p>不管使用PyTorch还是TensorFlow，或者是传统机器学习的scikit-learn，我们在读入数据这一块，都会先把数据转换成NumPy的数组，然后再进行后续的一系列操作。</p><p>对应到我们这个项目中，需要做的就是把训练集中的图片读入进来。对于图片的处理，我们一般会使用Pillow与OpenCV这两个模块。</p><!-- [[[read_end]]] --><p>虽然Pillow和OpenCV功能看上去都差不多，但还是有区别的。在PyTorch中，很多图片的操作都是基于Pillow的，所以当使用PyTorch编程出现问题，或者要思考、解决一些图片相关问题时，要从Pillow的角度出发。</p><p>下面我们先以单张图片为例，将极客时间的那张Logo图片分别用Pillow与OpenCV读入，然后转换为NumPy的数组。</p><h3>Pillow方式</h3><p>首先，我们需要使用Pillow中的下述代码读入上面的图片。</p><pre><code class=\"language-plain\">from PIL import Image\nim = Image.open('jk.jpg')\nim.size\n输出: 318, 116\n</code></pre><p>Pillow是以二进制形式读入保存的，那怎么转为NumPy格式呢？这个并不难，我们只需要利用NumPy的asarray方法，就可以将Pillow的数据转换为NumPy的数组格式。</p><pre><code class=\"language-plain\">import numpy as np\n\nim_pillow = np.asarray(im)\n\nim_pillow.shape\n输出：(116, 318, 3)\n</code></pre><h3>OpenCV方式：</h3><p>OpenCV的话，不再需要我们手动转格式，它直接读入图片后，就是以NumPy数组的形式来保存数据的，如下面的代码所示。</p><pre><code class=\"language-plain\">import cv2\nim_cv2 = cv2.imread('jk.jpg')\ntype(im_cv2)\n输出：numpy.ndarray\n\nim_cv2.shape\n输出：(116, 318, 3)\n</code></pre><p>结合代码输出可以发现，我们读入后的数组的最后一个维度是3，这是因为图片的格式是RGB格式，表示有R、G、B三个通道。对于计算视觉任务来说，绝大多数处理的图片都是RGB格式，如果不是RGB格式的话，要记得事先转换成RGB格式。<br>\n这里有个地方需要你关注，Pillow读入后通道的顺序就是R、G、B，而OpenCV读入后顺序是<strong>B、G、R</strong>。</p><p>模型训练时的通道顺序需与预测的通道顺序要保持一致。也就是说使用Pillow训练，使用OpenCV读入图片直接进行预测的话，不会报错，但结果会不正确，所以大家一定要注意。</p><p>接下来，我们就验证一下Pillow与OpenCV读入数据通道的顺序是否如此，借此引出有关Numpy数组索引与切片、合并等常见问题。</p><p>怎么验证这条结论呢？只需要将R、G、B三个通道的数据单独提取出来，然后令另外两个通道的数据全为0即可。</p><p>这里我给你说说为什么这样做。RGB色彩模式是工业界的一种颜色标准，RGB分别代表红、绿、蓝三个通道的颜色，将这三种颜色混合在一起，就形成了我们眼睛所能看到的所有颜色。</p><p>RGB三个通道各有256个亮度，分别用数字0到255表示，数字越高代表亮度越强，数字0则是代表最弱的亮度。在我们的例子中，如果一个通道的数据再加另外两个全0的通道（相当于关闭另外两个通道），最终图像以红色格调（可以先看一下后文中的最终输出结果）呈现出来的话，我们就可以认为该通道的数据是来源于R通道，G与B通道的证明同样可以如此。</p><p>好，首先我们提取出RGB三个通道的数据，这可以从数组的索引与切片说起。</p><h3>索引与切片</h3><p>如果你了解Python，那么索引和切片的概念你应该不陌生。</p><p>就像图书目录里的索引，我们可以根据索引标注的页码快速找到需要的内容，而Python</p><p>里的索引也是同样的功能，它用来定位数组中的某一个值。而切片意思就相当于提取图书中从某一页到某一页的内容。</p><p>NumPy数组的索引方式与Python的列表的索引方式相同，也同样支持切片索引。</p><p>这里需要你注意的是在NumPy数组中经常会出现用冒号来检索数据的形式，如下所示：</p><pre><code class=\"language-plain\">im_pillow[:, :, 0]\n</code></pre><p>这是什么意思呢？我们一起来看看。“：”代表全部选中的意思。我们的图片读入后，会以下图的状态保存在数组中。</p><p><img src=\"https://static001.geekbang.org/resource/image/20/01/20ayy454079771245f44f983b2130e01.jpg?wh=1920x1391\" alt=\"\"></p><p>上述代码的含义就是取第三个维度索引为0的全部数据，换句话说就是，取图片第0个通道的所有数据。</p><p>这样的话，通过下面的代码，我们就可以获得每个通道的数据了。</p><pre><code class=\"language-plain\">im_pillow_c1 = im_pillow[:, :, 0]\nim_pillow_c2 = im_pillow[:, :, 1]\nim_pillow_c3 = im_pillow[:, :, 2]\n</code></pre><p>获得了每个通道的数据，接下来就需要生成一个3维全0数组，全0数组的形状除了最后一维为2，其余两维要与im_pillow的形状相同。</p><p>全0数组你还记得怎么生成吗？可以自己先思考一下，生成的代码如下所示。</p><pre><code class=\"language-plain\">zeros = np.zeros((im_pillow.shape[0], im_pillow.shape[1], 2))\nzeros.shape\n输出：(116, 318, 2)\n</code></pre><p>然后，我们只需要将全0的数组与im_pillow_c1、im_pillow_c2、im_pillow_c3进行拼接，就可以获得对应通道的图像数据了。</p><h3>数组的拼接</h3><p>刚才我们拿到了单独通道的数据，接下来就需要把一个分离出来的数据跟一个全0数组拼接起来。如下图所示，红色的可以看作单通道数据，白色的为全0数据。</p><p><img src=\"https://static001.geekbang.org/resource/image/ee/c1/eedf20bf55a6c0521309d7c102719bc1.jpg?wh=1920x899\" alt=\"图片\"></p><p>NumPy数组为我们提供了np.concatenate((a1, a2, …), axis=0)方法进行数组拼接。其中，a1，a2, …就是我们要合并的数组；axis是我们要沿着哪一个维度进行合并，默认是沿着0轴方向。</p><p>对于我们的问题，是要沿着2轴的方向进行合并，也是我们最终的目标是要获得下面的三幅图像。<br>\n<img src=\"https://static001.geekbang.org/resource/image/68/7e/68bcd8107bf71ef876a339350a10c77e.jpg?wh=1603x653\" alt=\"\"></p><p>那么，我们先将im_pillow_c1与全0数组进行合并，生成上图中最左侧的数组，有了图像的数组才能获得最终图像。合并的代码跟输出结果如下：</p><pre><code class=\"language-plain\">im_pillow_c1_3ch = np.concatenate((im_pillow_c1, zeros), axis=2)\n---------------------------------------------------------------------------\nAxisError                                 Traceback (most recent call last)\n&lt;ipython-input-65-90bba90337ff&gt; in &lt;module&gt;\n----&gt; 1 im_pillow_c1_3ch = np.concatenate((im_pillow_c1, zeros), axis=2)\n&lt;__array_function__ internals&gt; in concatenate(*args, **kwargs)\nAxisError: axis 2 is out of bounds for array of dimension 2\n</code></pre><p>看到这里你可能很惊讶，竟然报错了？错误的原因是在2维数组中，axis如果等于2的话会越界。</p><p>我们看看im_pillow_c1与zeros的形状。</p><pre><code class=\"language-plain\">im_pillow_c1.shape\n输出：(116, 318)\nzeros.shape\n输出：(116, 318, 2)\n</code></pre><p>原来是我们要合并的两个数组维度不一样啊。那么如何统一维度呢？将im_pillow_c1变成(116, 318, 1)即可。</p><h4>方法一：使用np.newaxis</h4><p>我们可以使用np.newaxis让数组增加一个维度，使用方式如下。</p><pre><code class=\"language-plain\">im_pillow_c1 = im_pillow_c1[:, :, np.newaxis]\nim_pillow_c1.shape\n输出：(116, 318, 1)\n</code></pre><p>运行上面的代码，就可以将2个维度的数组转换为3个维度的数组了。<br>\n这个操作在你看深度学习相关代码的时候经常会看到，只不过PyTorch中的函数名unsqueeze(), TensorFlow的话是与NumPy有相同的名字，直接使用tf.newaxis就可以了。</p><p>然后我们再次将im_pillow_c1与zeros进行合并，这时就不会报错了，代码如下所示：</p><pre><code class=\"language-plain\">im_pillow_c1_3ch = np.concatenate((im_pillow_c1, zeros), axis=2)\nim_pillow_c1_3ch.shape\n输出：(116, 318, 3)\n</code></pre><h4>方法二：直接赋值</h4><p>增加维度的第二个方法就是直接赋值，其实我们完全可以生成一个与im_pillow形状完全一样的全0数组，然后将每个通道的数值赋值为im_pillow_c1、im_pillow_c2与im_pillow_c3就可以了。我们用这种方式生成上图中的中间与右边图像的数组。</p><pre><code class=\"language-plain\">im_pillow_c2_3ch = np.zeros(im_pillow.shape)\nim_pillow_c2_3ch[:,:,1] = im_pillow_c2\n\nim_pillow_c3_3ch = np.zeros(im_pillow.shape)\nim_pillow_c3_3ch[:,:,2] = im_pillow_c3\n</code></pre><p>这样的话，我们就可以将三个通道的RGB图片打印出来了。<br>\n关于绘图，你可以使用matplotlib进行绘图，它是NumPy的绘图库。如果你需要绘图，可以在<a href=\"https://matplotlib.org/stable/gallery/index.html\">这个网站</a>上找到各种各样的例子，然后根据它提供的代码进行修改，具体如何绘图我就不展开了。</p><p>说回我们的通道顺序验证问题，完成前面的操作后，你可以用下面的代码将原图、R通道、G通道与B通道的4幅图打印出来，你看是不是RGB顺序的呢？</p><pre><code class=\"language-plain\">from matplotlib import pyplot as plt\nplt.subplot(2, 2, 1)\nplt.title('Origin Image')\nplt.imshow(im_pillow)\nplt.axis('off')\nplt.subplot(2, 2, 2)\nplt.title('Red Channel')\nplt.imshow(im_pillow_c1_3ch.astype(np.uint8))\nplt.axis('off')\nplt.subplot(2, 2, 3)\nplt.title('Green Channel')\nplt.imshow(im_pillow_c2_3ch.astype(np.uint8))\nplt.axis('off')\nplt.subplot(2, 2, 4)\nplt.title('Blue Channel')\nplt.imshow(im_pillow_c3_3ch.astype(np.uint8))\nplt.axis('off')\nplt.savefig('./rgb_pillow.png', dpi=150)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/12/10/1264b5c889bfyy5e0d00668cfa205110.png?wh=900x600\" alt=\"图片\"></p><h3>深拷贝（副本）与浅拷贝（视图）</h3><p>刚才我们通过获取图片通道数据的练习，不过操作确实比较繁琐，介绍这些方法也主要是为了让你掌握切片索引和数组拼接的知识点。</p><p>其实我们还有一种更加简单的方式获得三个通道的BGR数据，只需要将图片读入后，直接将其中的两个通道赋值为0即可。代码如下所示：</p><pre><code class=\"language-plain\">from PIL import Image\nimport numpy as np\n\nim = Image.open('jk.jpg')\nim_pillow = np.asarray(im)\nim_pillow[:,:,1:]=0\n输出：\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-146-789bda58f667&gt; in &lt;module&gt;\n      4 im = Image.open('jk.jpg')\n      5 im_pillow = np.asarray(im)\n----&gt; 6 im_pillow[:,:,1:-1]=0\nValueError: assignment destination is read-only\n</code></pre><p>运行刚才的代码，报错提示说数组是只读数组，没办法进行修改。那怎么办呢？我们可以使用copy来复制一个数组。<br>\n说到copy()的话，就要说到浅拷贝与深拷贝的概念，<a href=\"https://time.geekbang.org/column/article/426126\">上节课</a>我们说到创建数组时就提过，np.array()属于深拷贝，np.asarray()则是浅拷贝。</p><p>简单来说，浅拷贝或称视图，指的是与原数组共享<strong>数据</strong>的数组，请注意，只是数据，没有说共享形状。视图我们通常使用view()来创建。常见的切片操作也会返回对原数组的浅拷贝。</p><p>请看下面的代码，数组a与b的数据是相同的，形状确实不同，但是修改b中的数据后，a的数据同样会发生变化。</p><pre><code class=\"language-plain\">a = np.arange(6)\nprint(a.shape)\n输出：(6,)\nprint(a)\n输出：[0 1 2 3 4 5]\n\nb = a.view()\nprint(b.shape)\n输出：(6,)\nb.shape = 2, 3\nprint(b)\n输出：[[0 1 2]\n [3 4 5]]\nb[0,0] = 111\nprint(a)\n输出：[111   1   2   3   4   5]\nprint(b)\n输出：[[111   1   2]\n [  3   4   5]]\n</code></pre><p>而深拷贝又称副本，也就是完全复制原有数组，创建一个新的数组，修改新的数组不会影响原数组。深拷贝使用copy()方法。</p><p>所以，我们将刚才报错的程序修改成下面的形式就可以了。</p><pre><code class=\"language-plain\">im_pillow = np.array(im)\nim_pillow[:,:,1:]=0\n</code></pre><p>可别小看深拷贝和浅拷贝的区别。这里讲一个我以前遇到的坑吧，我曾经要开发一个部署在手机端的人像分割模型。</p><p>为了提高模型的分割效果，我考虑了新的实验方法——将前一帧的数据也作为当前帧的输入进行考虑，训练阶段没有发生问题，但是在调试阶段发现模型的效果非常差。</p><p>后来经过研究，我才发现了问题的原因。原因是我为了可视化分割效果，我将前一帧的数据进行变换打印出来。同时，我错误的采用了浅拷贝的方式，将前一帧的数据传入当前帧，所以说传入到当前帧的数据是经过变化的，而不是原始的输出。</p><p>这时再传入当前帧，自然无法得到正确结果。当时因为这个坑，差点产生要放弃这个实验的想法，后面改成深拷贝才解决了问题。</p><p>好了，讲到这里，你是否可以用上述的方法对OpenCV读取图片读入通道顺序进行一下验证呢？</p><h2>模型评估</h2><p>在模型评估时，我们一般会将模型的输出转换为对应的标签。</p><p>假设现在我们的问题是将图片分为2个类别，包含极客时间的图片与不包含的图片。模型会输出形状为(2, )的数组，我们把它叫做probs，它存储了两个概率，我们假设索引为0的概率是包含极客时间图片的概率，另一个是其它图片的概率，它们两个概率的和为1。如果极客时间对应的概率大，则可以推断该图片为包含极客时间的图片，否则为其他图片。</p><p>简单的做法就是判断probs[0]是否大于0.5，如果大于0.5，则可以认为图片是我们要寻找的。</p><p>这种方法固然可以，但是如果我们需要判断图片的类别有很多很多种呢？</p><p>例如，有1000个类别的ImageNet。也许你会想到遍历这个数组，求出最大值对应的索引。</p><p>那如果老板让你找出概率最大的前5个类别呢？有没有更简单点的方法？我们继续往下看。</p><h3>Argmax Vs Argmin：求最大/最小值对应的索引</h3><p>NumPy的argmax(a, axis=None)方法可以为我们解决求最大值索引的问题。如果不指定axis，则将数组默认为1维。</p><p>对于我们的问题，使用下述代码即可获得拥有最大概率值的图片。</p><pre><code class=\"language-plain\">np.argmax(probs)\n</code></pre><p>Argmin的用法跟Argmax差不多，不过它的作用是获得具有最小值的索引。</p><h3>Argsort：数组排序后返回原数组的索引</h3><p>那现在我们再把问题升级一下，比如需要你将图片分成10个类别，要找到具有最大概率的前三个类别。</p><p>模型输出的概率如下：</p><pre><code class=\"language-plain\">probs = np.array([0.075, 0.15, 0.075, 0.15, 0.0, 0.05, 0.05, 0.2, 0.25])\n</code></pre><p>这时，我们就可以借助argsort(a, axis=-1, kind=None)函数来解决该问题。np.argsort的作用是对原数组进行从小到大的排序，返回的是对应元素在原数组中的索引。<br>\nnp.argsort包括后面这几个关键参数：</p><ul>\n<li>a是要进行排序的原数组；</li>\n<li>axis是要沿着哪一个轴进行排序，默认是-1，也就是最后一个轴；</li>\n<li>kind是采用什么算法进行排序，默认是快速排序，还有其他排序算法，具体你可以看看数据结构的排序算法。</li>\n</ul><p>我们还是结合例子来理解，你可以看看下面的代码，它描述了我们使用argsort对probs进行排序，然后返回对应坐标的全过程。</p><pre><code class=\"language-plain\">probs_idx_sort = np.argsort(-probs)  #注意，加了负号，是按降序排序\nprobs_idx_sort\n输出：array([8, 7, 1, 3, 0, 2, 5, 6, 4])\n#概率最大的前三个值的坐标\nprobs_idx_sort[:3]\n输出：array([8, 7, 1])\n</code></pre><h2>小结</h2><p>恭喜你，完成了这一节课的学习。这一节介绍了一些常用且重要的功能。几乎在所有深度学习相关的项目中，你都会常常用到这些函数，当你阅读别人的代码的时候也会经常看到。</p><p>让我们一起来复习一下今天学到的这些函数，我画了一张表格，给你总结了它们各自的关键功能和使用要点。</p><p><img src=\"https://static001.geekbang.org/resource/image/29/yd/296b503a7c2fb89987695035c0184yyd.jpg?wh=1709x798\" alt=\"\"></p><p>我觉得NumPy最难懂的还是上节课的轴，如果你把轴的概念理解清楚之后，理解今天的内容会更加轻松。理解了原理之后，关键还是动手练习。</p><h2>每课一练</h2><p>给定数组scores，形状为（256，256，2），scores[: , :, 0] 与scores[:, :, 1]对应位置元素的和为1，现在我们要根据scores生产数组mask，要求scores通道0的值如果大于通道1的值，则mask对应的位置为0，否则为1。</p><p>scores如下，你可以试试用代码实现：</p><pre><code class=\"language-plain\">scores = np.random.rand(256, 256, 2)\nscores[:,:,1] = 1 - scores[:,:,0]\n</code></pre><p>欢迎你在留言区记录你的疑问或者收获，也推荐你把这节课分享给你的朋友。</p>","neighbors":{"left":{"article_title":"02 | NumPy（上）：核心数据结构详解","id":426126},"right":{"article_title":"04 | Tensor：PyTorch中最基础的计算单元","id":427460}}},{"article_id":427460,"article_title":"04 | Tensor：PyTorch中最基础的计算单元","article_content":"<p>在上节课中，我们一起学习了NumPy的主要使用方法和技巧，有了NumPy我们可以很好地处理各种类型的数据。而在深度学习中，数据的组织则更进一步，从数据的组织，到模型内部的参数，都是通过一种叫做<strong>张量</strong>的数据结构进行表示和处理。</p><p>今天我们就来一块儿了解一下张量（Tensor），学习一下Tensor的常用操作。</p><h2>什么是Tensor</h2><p>Tensor是深度学习框架中极为基础的概念，也是PyTroch、TensorFlow中最重要的知识点之一，它是一种数据的存储和处理结构。</p><p>回忆一下我们目前知道的几种数据表示：</p><ol>\n<li>标量，也称Scalar，是一个只有大小，没有方向的量，比如1.8、e、10等。</li>\n<li>向量，也称Vector，是一个有大小也有方向的量，比如(1,2,3,4)等。</li>\n<li>矩阵，也称Matrix，是多个向量合并在一起得到的量，比如[(1,2,3),(4,5,6)]等。</li>\n</ol><p>为了帮助你更好理解标量、向量和矩阵，我特意准备了一张示意图，你可以结合图片理解。<br>\n<img src=\"https://static001.geekbang.org/resource/image/a8/85/a85883cc14171ff5361346dd65776085.jpg?wh=1920x1090\" alt=\"\"></p><p>不难发现，几种数据表示其实都是有着联系的，标量可以组合成向量，向量可以组合成矩阵。那么，我们可否将它们看作是一种数据形式呢？</p><p>答案是可以的，这种统一的数据形式，在PyTorch中我们称之为<strong>张量(Tensor)</strong>。从标量、向量和矩阵的关系来看，你可能会觉得它们就是不同<strong>“维度”</strong>的Tensor，这个说法对，也不全对。</p><!-- [[[read_end]]] --><p>说它不全对是因为在Tensor的概念中，我们更愿意使用Rank（秩）来表示这种<strong>“维度”</strong>，比如标量，就是Rank为0阶的Tensor；向量就是Rank为1阶的Tensor；矩阵就是Rank为2阶的Tensor。也有Rank大于2的Tensor。当然啦，你如果说维度其实也没什么错误，平时很多人也都这么叫。</p><p>说完Tensor的含义，我们一起看一下Tensor的类型，以及如何创建Tensor。</p><h2>Tensor的类型、创建及转换</h2><p>在不同的深度学习框架下，Tensor呈现的特点大同小异，我们使用它的方法也差不多。这节课我们就以PyTorch中的使用方法为例进行学习。</p><h3>Tensor的类型</h3><p>在PyTorch中，Tensor支持的数据类型有很多种，这里列举较为常用的几种格式：</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/08/e6af6a3b2172ee08db8c564146ae2108.jpg?wh=1680x933\" alt=\"图片\"></p><p>一般来说，torch.float32、torch.float64、torch.uint8和torch.int64用得相对较多一些，但是也不是绝对，还是要根据实际情况进行选择。这里你有个印象就行，后面课程用到时我还会进一步讲解。</p><h3>Tensor的创建</h3><p>PyTorch对于Tensor的操作已经非常友好了，你可以通过多种不同的方式创建一个任意形状的Tensor，而且每种方式都很简便，我们一起来看一下。</p><h4>直接创建</h4><p>首先来看直接创建的方法，这也是最简单创建的方法。我们需要用到下面的torch.tensor函数直接创建。</p><pre><code class=\"language-python\">torch.tensor(data, dtype=None, device=None,requires_grad=False)\n</code></pre><p>结合代码，我们看看其中的参数是什么含义。<br>\n我们从左往右依次来看，首先是data，也就是我们要传入模型的数据。PyTorch支持通过list、 tuple、numpy array、scalar等多种类型进行数据传入，并转换为tensor。</p><p>接着是dtype，它声明了你需要返回一个怎样类型的Tensor，具体类型可以参考前面表格里列举的Tensor的8种类型。</p><p>然后是device，这个参数指定了数据要返回到的设备，目前暂时不需要关注，缺省即可。</p><p>最后一个参数是requires_grad，用于说明当前量是否需要在计算中保留对应的梯度信息。在PyTorch中，只有当一个Tensor设置requires_grad为True的情况下，才会对这个Tensor以及由这个Tensor计算出来的其他Tensor进行求导，然后将导数值存在Tensor的grad属性中，便于优化器来更新参数。</p><p>所以，你需要注意的是，把requires_grad设置成true或者false要灵活处理。<strong>如果是训练过程就要设置为true，目的是方便求导、更新参数。而到了验证或者测试过程，我们的目的是检查当前模型的泛化能力，那就要把requires_grad设置成Fasle，避免这个参数根据loss自动更新</strong>。</p><h4>从NumPy中创建</h4><p>还记得之前的课程中，我们一同学习了NumPy的使用，在实际应用中，我们在处理数据的阶段多使用的是NumPy，而数据处理好之后想要传入PyTorch的深度学习模型中，则需要借助Tensor，所以PyTorch提供了一个从NumPy转到Tensor的语句：</p><pre><code class=\"language-python\">torch.from_numpy(ndarry)\n</code></pre><p>有时候我们在开发模型的过程中，需要用到一些特定形式的矩阵Tensor，比如全是0的，或者全是1的。这时我们就可以用这个方法创建，比如说，先生成一个全是0的NumPy数组，然后转换成Tensor。但是这样也挺麻烦的，因为这意味着你要引入更多的包（NumPy），也会使用更多的代码，这会增加出错的可能性。<br>\n不过你别担心，PyTorch内部已经提供了更为简便的方法，我们接着往下看。</p><h4>创建特殊形式的Tensor</h4><p>我们一块来看一下后面的几个常用函数，它们都是在PyTorch模型内部使用的。</p><ul>\n<li>创建零矩阵Tensor：零矩阵顾名思义，就是所有的元素都为0的矩阵。</li>\n</ul><pre><code class=\"language-python\">torch.zeros(*size, dtype=None...)\n</code></pre><p>其中，我们用得比较多的就是size参数和dtype参数。size定义输出张量形状的整数序列。<br>\n这里你可能注意到了，在函数参数列表中我加入了省略号，这意味着torch.zeros的参数有很多。不过。咱们现在是介绍零矩阵的概念，形状相对来说更重要。其他的参数（比如前面提到的requires_grad参数）与此无关，现阶段我们暂时不关注。</p><ul>\n<li>创建单位矩阵Tensor：单位矩阵是指主对角线上的元素都为1的矩阵。</li>\n</ul><pre><code class=\"language-python\">torch.eye(size, dtype=None...)\n</code></pre><ul>\n<li>创建全一矩阵Tensor：全一矩阵顾名思义，就是所有的元素都为1的矩阵。</li>\n</ul><pre><code class=\"language-python\">torch.ones(size, dtype=None...)\n</code></pre><ul>\n<li>创建随机矩阵Tensor：在PyTorch中有几种较为经常使用的随机矩阵创建方式，分别如下。</li>\n</ul><pre><code class=\"language-python\">torch.rand(size)\ntorch.randn(size)\ntorch.normal(mean, std, size)\ntorch.randint(low, high, size）\n</code></pre><p>这些方式各自有不同的用法，你可以根据自己的需要灵活使用。</p><ul>\n<li>torch.rand用于生成数据类型为浮点型且维度指定的随机Tensor，随机生成的浮点数据在 <strong>0~1 区间均匀分布</strong>。</li>\n<li>torch.randn用于生成数据类型为浮点型且维度指定的随机Tensor，随机生成的浮点数的取值满足<strong>均值为 0、方差为 1 的标准正态分布</strong>。</li>\n<li>torch.normal用于生成数据类型为浮点型且维度指定的随机Tensor，<strong>可以指定均值和标准差</strong>。</li>\n<li>torch.randint用于生成随机整数的Tensor，其内部填充的是在[low,high)均匀生成的随机整数。</li>\n</ul><h3></h3><h3>Tensor的转换</h3><p>在实际项目中，我们接触到的数据类型有很多，比如Int、list、NumPy等。为了让数据在各个阶段畅通无阻，不同数据类型与Tensor之间的转换就非常重要了。接下来我们一起来看看int、list、NumPy是如何与Tensor互相转换的。</p><ul>\n<li>Int与Tensor的转换：</li>\n</ul><pre><code class=\"language-python\">a = torch.tensor(1)\nb = a.item()\n</code></pre><p>我们通过torch.Tensor将一个数字（或者标量）转换为Tensor，又通过item()函数，将Tensor转换为数字（标量），item()函数的作用就是将Tensor转换为一个python number。</p><ul>\n<li>list与tensor的转换：</li>\n</ul><pre><code class=\"language-python\">a = [1, 2, 3]\nb = torch.tensor(a)\nc = b.numpy().tolist()\n</code></pre><p>在这里对于一个list a，我们仍旧直接使用torch.Tensor，就可以将其转换为Tensor了。而还原回来的过程要多一步，需要我们先将Tensor转为NumPy结构，之后再使用tolist()函数得到list。</p><ul>\n<li>NumPy与Tensor的转换：</li>\n</ul><p>有了前面两个例子，你是否能想到NumPy怎么转换为Tensor么？对，我们仍旧torch.Tensor即可，是不是特别方便。</p><ul>\n<li>CPU与GPU的Tensor之间的转换：</li>\n</ul><pre><code class=\"language-python\">CPU-&gt;GPU: data.cuda()\nGPU-&gt;CPU: data.cpu()\n</code></pre><h2>Tensor的常用操作</h2><p>好，刚才我们一起了解了Tensor的类型，如何创建Tensor，以及如何实现Tensor和一些常见的数据类型之间的相互转换。其实Tensor还有一些比较常用的功能，比如获取形状、维度转换、形状变换以及增减维度，接下来我们一起来看看这些功能。</p><h3>获取形状</h3><p>在深度学习网络的设计中，我们需要时刻对Tensor的情况做到了如指掌，其中就包括获取Tensor的形式、形状等。</p><p>为了得到Tensor的形状，我们可以使用shape或size来获取。两者的不同之处在于，shape是torch.tensor的一个属性，而size()则是一个torch.tensor拥有的方法。</p><pre><code class=\"language-python\">&gt;&gt;&gt; a=torch.zeros(2, 3, 5)\n&gt;&gt;&gt; a.shape\ntorch.Size([2, 3, 5])\n&gt;&gt;&gt; a.size()\ntorch.Size([2, 3, 5])\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/8d/24/8d5a9954126f8a737e58ba2f8afdc624.jpg?wh=1920x1080\" alt=\"图片\"></p><p>知道了Tensor的形状，我们就能知道这个Tensor所包含的元素的数量了。具体的计算方法就是直接将所有维度的大小相乘，比如上面的Tensor a所含有的元素的个数为2<em>3</em>5=30个。这样似乎有点麻烦，我们在PyTorch中可以使用numel()函数直接统计元素数量。</p><pre><code class=\"language-python\">&gt;&gt;&gt; a.numel()\n30\n</code></pre><h3>矩阵转秩(维度转换）</h3><p>在PyTorch中有两个函数，分别是permute()和transpose()可以用来实现矩阵的转秩，或者说交换不同维度的数据。比如在调整卷积层的尺寸、修改channel的顺序、变换全连接层的大小的时候，我们就要用到它们。</p><p>其中，用permute函数可以对任意高维矩阵进行转置，但只有 tensor.permute() 这个调用方式，我们先看一下代码：</p><pre><code class=\"language-python\">&gt;&gt;&gt; x = torch.rand(2,3,5)\n&gt;&gt;&gt; x.shape\ntorch.Size([2, 3, 5])\n&gt;&gt;&gt; x = x.permute(2,1,0)\n&gt;&gt;&gt; x.shape\ntorch.Size([5, 3, 2])\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/02/84/025985c8635f3896d45d15e1ea381c84.jpg?wh=1920x1080\" alt=\"图片\"></p><p>有没有发现，原来的Tensor的形状是[2,3,5]，我们在permute中分别写入原来索引位置的新位置，x.permute(2,1,0)，2表示原来第二个维度现在放在了第零个维度；同理1表示原来第一个维度仍旧在第一个维度；0表示原来第0个维度放在了现在的第2个维度，形状就变成了[5,3,2]</p><p>而另外一个函数transpose，不同于permute，它每次只能转换两个维度，或者说交换两个维度的数据。我们还是来看一下代码：</p><pre><code class=\"language-python\">&gt;&gt;&gt; x.shape\ntorch.Size([2, 3, 4])\n&gt;&gt;&gt; x = x.transpose(1,0)\n&gt;&gt;&gt; x.shape\ntorch.Size([3, 2, 4])\n</code></pre><p>需要注意的是，经过了transpose或者permute处理之后的数据，变得不再连续了，什么意思呢？</p><p>还是接着刚才的例子说，我们使用torch.rand(2,3,4)得到的tensor，在内存中是连续的，但是经过transpose或者permute之后呢，比如transpose(1,0)，内存虽然没有变化，但是我们得到的数据“看上去”是第0和第1维的数据发生了交换，现在的第0维是原来的第1维，所以Tensor都会变得不再连续。</p><p>那你可能会问了，不连续就不连续呗，好像也没啥影响吧？这么想你就草率了，我们继续来看看Tensor的形状变换，学完以后你就知道Tensor不连续的后果了。</p><h3>形状变换</h3><p>在PyTorch中有两种常用的改变形状的函数，分别是view和reshape。我们先来看一下view。</p><pre><code class=\"language-python\">&gt;&gt;&gt; x = torch.randn(4, 4)\n&gt;&gt;&gt; x.shape\ntorch.Size([4, 4])\n&gt;&gt;&gt; x = x.view(2,8)\n&gt;&gt;&gt; x.shape\ntorch.Size([2, 8])\n</code></pre><p>我们先声明了一个[4, 4]大小的Tensor，然后通过view函数，将其修改为[2, 8]形状的Tensor。我们还是继续刚才的x，再进行一步操作，代码如下：</p><pre><code class=\"language-python\">&gt;&gt;&gt; x = x.permute(1,0)\n&gt;&gt;&gt; x.shape\ntorch.Size([8, 2])\n&gt;&gt;&gt; x.view(4, 4)\nTraceback (most recent call last):\n&nbsp; File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n</code></pre><p>结合代码可以看到，利用permute，我们将第0和第1维度的数据进行了变换，得到了[8, 2]形状的Tensor，在这个新Tensor上进行view操作，忽然就报错了，为什么呢？其实就是因为view不能处理内存不连续Tensor的结构。<br>\n那这时候要怎么办呢？我们可以使用另一个函数，reshape：</p><pre><code class=\"language-python\">&gt;&gt;&gt; x = x.reshape(4, 4)\n&gt;&gt;&gt; x.shape\ntorch.Size([4, 4])\n</code></pre><p>这样问题就迎刃而解了。其实reshape相当于进行了两步操作，先把Tensor在内存中捋顺了，然后再进行view操作。</p><h3>增减维度</h3><p>有时候我们需要对Tensor增加或者删除某些维度，比如删除或者增加图片的几个通道。PyTorch提供了squeeze()和unsqueeze()函数解决这个问题。</p><p>我们先来看squeeze()。如果dim指定的维度的值为1，则将该维度删除，若指定的维度值不为1，则返回原来的Tensor。为了方便你理解，我还是结合例子来讲解。</p><pre><code class=\"language-python\">&gt;&gt;&gt; x = torch.rand(2,1,3)\n&gt;&gt;&gt; x.shape\ntorch.Size([2, 1, 3])\n&gt;&gt;&gt; y = x.squeeze(1)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 3])\n&gt;&gt;&gt; z = y.squeeze(1)\n&gt;&gt;&gt; z.shape\ntorch.Size([2, 3])\n</code></pre><p>结合代码我们可以看到，我们新建了一个维度为[2, 1, 3]的Tensor，然后将第1维度的数据删除，得到y，squeeze执行成功是因为第1维度的大小为1。然而在y上我们打算进一步删除第1维度的时候，就会发现删除失败了，这是因为y此刻的第1维度的大小为3，suqeeze不能删除。<br>\nunsqueeze()：这个函数主要是对数据维度进行扩充。给指定位置加上维数为1的维度，我们同样结合代码例子来看看。</p><pre><code class=\"language-python\">&gt;&gt;&gt; x = torch.rand(2,1,3)\n&gt;&gt;&gt; y = x.unsqueeze(2)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 1, 1, 3])\n</code></pre><p>这里我们新建了一个维度为[2, 1, 3]的Tensor，然后在第2维度插入一个维度，这样就得到了一个[2,1,1,3]大小的tensor。</p><h2>小结</h2><p>之前我们学习了NumPy相关的操作，如果把NumPy和Tensor做对比，就不难发现它们之间有很多共通的内容，共性就是两者都是数据的表示形式，都可以看作是科学计算的通用工具。但是NumPy和Tensor的用途是不一样的，NumPy不能用于GPU加速，Tensor则可以。</p><p>这节课我们一同学习了Tensor的创建、类型、转换、变换等常用功能，通过这几个功能，我们就可以对Tensor进行最基本也是最常用的操作，这些都是必须要牢记的内容。</p><p>此外，在实际上，真正的项目实战中还有个非常多的操作种类，其中较为重要的是<strong>数学计算操作</strong>，比如加减乘除、合并、连接等。但是这些操作如果一个一个列举出来，数量极其繁多，你也会感觉很枯燥，所以在后续的课程中，咱们会在具体的实战环节来学习相关的数学操作。</p><p>下一节课的内容，咱们会对Tensor的变形、切分等高级操作进行学习，这是一个很好玩儿的内容，敬请期待。</p><h2>每课一练</h2><p>在PyTorch中，有torch.Tensor()和torch.tensor()两种函数，它们的区别是什么呢？</p><p>欢迎你在留言区和我交流，也推荐你把今天的内容分享给更多同事和朋友。</p>","neighbors":{"left":{"article_title":"03 | NumPy（下）：深度学习中的常用操作","id":426801},"right":{"article_title":"05 | Tensor变形记：快速掌握Tensor切分、变形等方法","id":428186}}},{"article_id":428186,"article_title":"05 | Tensor变形记：快速掌握Tensor切分、变形等方法","article_content":"<p>你好，我是方远。</p><p>上节课我们一起学习了Tensor的基础概念，也熟悉了创建、转换、维度变换等操作，掌握了这些基础知识，你就可以做一些简单的Tensor相关的操作了。</p><p>不过，要想在实际的应用中更灵活地用好Tensor，Tensor的连接、切分等操作也是必不可少的。今天这节课，咱们就通过一些例子和图片来一块学习下。虽然这几个操作比较有难度，但只要你耐心听我讲解，然后上手练习，还是可以拿下的。</p><h2>Tensor的连接操作</h2><p>在项目开发中，深度学习某一层神经元的数据可能有多个不同的来源，那么就需要将数据进行组合，这个组合的操作，我们称之为<strong>连接</strong>。</p><h3>cat</h3><p>连接的操作函数如下。</p><pre><code class=\"language-python\">torch.cat(tensors, dim = 0, out = None)\n</code></pre><p>cat是concatnate的意思，也就是拼接、联系的意思。该函数有两个重要的参数需要你掌握。</p><p>第一个参数是tensors，它很好理解，就是若干个我们准备进行拼接的Tensor。</p><p>第二个参数是dim，我们回忆一下Tensor的定义，Tensor的维度（秩）是有多种情况的。比如有两个3维的Tensor，可以有几种不同的拼接方式（如下图），dim参数就可以对此作出约定。</p><p><img src=\"https://static001.geekbang.org/resource/image/61/3c/61bd88f3yy8d0ca07799f36540d3473c.jpg?wh=1285x862\" alt=\"图片\"></p><p>看到这里，你可能觉得上面画的图是三维的，看起来比较晦涩，所以咱们先从简单的二维的情况说起，我们先声明两个3x3的矩阵，代码如下：</p><!-- [[[read_end]]] --><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.ones(3,3)\n&gt;&gt;&gt; B=2*torch.ones(3,3)\n&gt;&gt;&gt; A\ntensor([[1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.]])\n&gt;&gt;&gt; B\ntensor([[2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.]])\n</code></pre><p>我们先看看dim=0的情况，拼接的结果是怎样的：</p><pre><code class=\"language-python\">&gt;&gt;&gt; C=torch.cat((A,B),0)\n&gt;&gt;&gt; C\ntensor([[1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.]])\n\n</code></pre><p>你会发现，两个矩阵是按照“行”的方向拼接的。</p><p>我们接下来再看看，dim=1的情况是怎样的：</p><pre><code class=\"language-python\">&gt;&gt;&gt; D=torch.cat((A,B),1)\n&gt;&gt;&gt; D\ntensor([[1., 1., 1., 2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 2., 2., 2.]])\n</code></pre><p>显然，两个矩阵，是按照“列”的方向拼接的。那如果Tensor是三维甚至更高维度的呢？其实道理也是一样的，dim的数值是多少，两个矩阵就会按照相应维度的方向链接两个Tensor。</p><p>看到这里你可能会问了，cat实际上是将多个Tensor在已有的维度上进行连接，那如果想增加新的维度进行连接，又该怎么做呢？这时候就需要stack函数登场了。</p><h3>stack</h3><p>为了让你加深理解，我们还是结合具体例子来看看。假设我们有两个二维矩阵Tensor，把它们“堆叠”放在一起，构成一个三维的Tensor，如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/9d/66/9d991a0d571e2733ba15d67566f65166.jpg?wh=1160x770\" alt=\"图片\"></p><p>这相当于原来的维度（秩）是2，现在变成了3，变成了一个立体的结构，增加了一个维度。你需要注意的是，这跟前面的cat不同，cat中示意图的例子，原来就是3维的，cat之后仍旧是3维的，而现在咱们是<strong>从2维变成了3维</strong>。</p><p>在实际图像算法开发中，咱们有时候需要将多个单通道Tensor（2维）合并，得到多通道的结果（3维）。而实现这种增加维度拼接的方法，我们把它叫做stack。</p><p>stack函数的定义如下：</p><pre><code class=\"language-python\">torch.stack(inputs, dim=0)\n</code></pre><p>其中，inputs表示需要拼接的Tensor，dim表示新建立维度的方向。</p><p>那stack如何使用呢？我们一块来看一个例子：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.arange(0,4)\n&gt;&gt;&gt; A\ntensor([0, 1, 2, 3])\n&gt;&gt;&gt; B=torch.arange(5,9)\n&gt;&gt;&gt; B\ntensor([5, 6, 7, 8])\n&gt;&gt;&gt; C=torch.stack((A,B),0)\n&gt;&gt;&gt; C\ntensor([[0, 1, 2, 3],\n        [5, 6, 7, 8]])\n&gt;&gt;&gt; D=torch.stack((A,B),1)\n&gt;&gt;&gt; D\ntensor([[0, 5],\n        [1, 6],\n        [2, 7],\n        [3, 8]])\n</code></pre><p>结合代码，我们可以看到，首先我们构建了两个4元素向量A和B，它们的维度是1。然后，我们在dim=0，也就是“行”的方向上新建一个维度，这样维度就成了2，也就得到了C。而对于D，我们则是在dim=1，也就是“列”的方向上新建维度。</p><h2>Tensor的切分操作</h2><p>学完了连接操作之后，我们再来看看连接的逆操作：<strong>切分</strong>。</p><p>切分就是连接的逆过程，有了刚才的经验，你很容易就会想到，切分的操作也应该有很多种，比如切片、切块等。没错，切分的操作主要分为三种类型：chunk、split、unbind。</p><p>乍一看有不少，其实是因为它们各有特点，适用于不同的使用情景，让我们一起看一下。</p><h3>chunk</h3><p>chunk的作用就是将Tensor按照声明的dim，进行尽可能平均的划分。</p><p>比如说，我们有一个32channel的特征，需要将其按照channel均匀分成4组，每组8个channel，这个切分就可以通过chunk函数来实现。具体函数如下：</p><pre><code class=\"language-python\">torch.chunk(input, chunks, dim=0)\n</code></pre><p>我们挨个来看看函数中涉及到的三个参数：</p><p>首先是input，它表示要做chunk操作的Tensor。</p><p>接着，我们看下chunks，它代表将要被划分的块的数量，而不是每组的数量。请注意，<strong>chunks必须是整型</strong>。</p><p>最后是dim，想想这个参数是什么意思呢？对，就是按照哪个维度来进行chunk。</p><p>还是跟之前一样，我们通过几个代码例子直观感受一下。我们从一个简单的一维向量开始：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([1,2,3,4,5,6,7,8,9,10])\n&gt;&gt;&gt; B = torch.chunk(A, 2, 0)\n&gt;&gt;&gt; B\n(tensor([1, 2, 3, 4, 5]), tensor([ 6,&nbsp; 7,&nbsp; 8,&nbsp; 9, 10]))\n</code></pre><p>这里我们通过chunk函数，将原来10位长度的Tensor A，切分成了两个一样5位长度的向量。（注意，B是两个切分结果组成的tuple）。</p><p>那如果chunk参数不能够整除的话，结果会是怎样的呢？我们接着往下看：</p><pre><code class=\"language-python\">&gt;&gt;&gt; B = torch.chunk(A, 3, 0)\n&gt;&gt;&gt; B\n(tensor([1, 2, 3, 4]), tensor([5, 6, 7, 8]), tensor([ 9, 10]))\n</code></pre><p>我们发现，10位长度的Tensor A，切分成了三个向量，长度分别是4，4，2位。这是怎么分的呢，不应该是3，3，4这样更为平均的方式么？</p><p>想要解决问题，就得找到规律。让我们再来看一个更大一点的例子，将A改为17位长度。</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])\n&gt;&gt;&gt; B = torch.chunk(A, 4, 0)\n&gt;&gt;&gt; B\n(tensor([1, 2, 3, 4, 5]), tensor([ 6,&nbsp; 7,&nbsp; 8,&nbsp; 9, 10]), tensor([11, 12, 13, 14, 15]), tensor([16, 17]))\n</code></pre><p>17位长度的Tensor A，切分成了四个分别为5，5，5，2位长度的向量。这时候你就会发现，其实在计算每个结果元素个数的时候，chunk函数是先做除法，然后再向上取整得到每组的数量。</p><p>比如上面这个例子，17/4=4.25，向上取整就是5，那就先逐个生成若干个长度为5的向量，最后不够的就放在一块，作为最后一个向量（长度2）。</p><p>那如果chunk参数大于Tensor可以切分的长度，又要怎么办呢？我们实际操作一下，代码如下：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([1,2,3])\n&gt;&gt;&gt; B = torch.chunk(A, 5, 0)\n&gt;&gt;&gt; B\n(tensor([1]), tensor([2]), tensor([3]))\n</code></pre><p>显然，被切分的Tensor只能分成若干个长度为1的向量。</p><p>由此可以推论出二维的情况，我们再举一个例子， 看看二维矩阵Tensor的情况 ：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.ones(4,4)\n&gt;&gt;&gt; A\ntensor([[1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.]])\n&gt;&gt;&gt; B = torch.chunk(A, 2, 0)\n&gt;&gt;&gt; B\n(tensor([[1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.]]), \ntensor([[1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.]]))\n</code></pre><p>还是跟前面的cat一样，这里的dim参数，表示的是第dim维度方向上进行切分。</p><p>刚才介绍的chunk函数，是按照“切分成确定的份数”来进行切分的，那如果想按照“每份按照确定的大小”来进行切分，该怎样做呢？PyTorch也提供了相应的方法，叫做split。</p><h3>split</h3><p>split的函数定义如下，跟前面一样，我们还是分别看看这里涉及的参数。</p><pre><code class=\"language-python\">torch.split(tensor, split_size_or_sections, dim=0)\n</code></pre><p>首先是tensor，也就是待切分的Tensor。</p><p>然后是split_size_or_sections这个参数。当它为整数时，表示将tensor按照每块大小为这个整数的数值来切割；当这个参数为列表时，则表示将此tensor切成和列表中元素一样大小的块。</p><p>最后同样是dim，它定义了要按哪个维度切分。</p><p>同样的，我们举几个例子来看一下split的具体操作。首先是split_size_or_sections是整数的情况。</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.rand(4,4)\n&gt;&gt;&gt; A\ntensor([[0.6418, 0.4171, 0.7372, 0.0733],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.0935, 0.2372, 0.6912, 0.8677],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.5263, 0.4145, 0.9292, 0.5671],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.2284, 0.6938, 0.0956, 0.3823]])\n&gt;&gt;&gt; B=torch.split(A, 2, 0)\n&gt;&gt;&gt; B\n(tensor([[0.6418, 0.4171, 0.7372, 0.0733],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.0935, 0.2372, 0.6912, 0.8677]]), \ntensor([[0.5263, 0.4145, 0.9292, 0.5671],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.2284, 0.6938, 0.0956, 0.3823]]))\n</code></pre><p>在这个例子里，我们看到，原来4x4大小的Tensor A，沿着第0维度，也就是沿“行”的方向，按照每组2“行”的大小进行切分，得到了两个2x4大小的Tensor。</p><p>那么问题来了，如果split_size_or_sections不能整除对应方向的大小的话，会有怎样的结果呢？我们将代码稍作修改就好了：</p><pre><code class=\"language-python\">&gt;&gt;&gt; C=torch.split(A, 3, 0)\n&gt;&gt;&gt; C\n(tensor([[0.6418, 0.4171, 0.7372, 0.0733],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.0935, 0.2372, 0.6912, 0.8677],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.5263, 0.4145, 0.9292, 0.5671]]), \ntensor([[0.2284, 0.6938, 0.0956, 0.3823]]))\n</code></pre><p>根据刚才的代码我们就能发现，原来，PyTorch会尽可能凑够每一个结果，使得其对应dim的数据大小等于split_size_or_sections。如果最后剩下的不够，那就把剩下的内容放到一块，作为最后一个结果。</p><p>接下来，我们再看一下split_size_or_sections是列表时的情况。刚才提到了，当split_size_or_sections为列表的时候，表示将此tensor切成和列表中元素大小一样的大小的块，我们来看一段对应的代码：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.rand(5,4)\n&gt;&gt;&gt; A\ntensor([[0.1005, 0.9666, 0.5322, 0.6775],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.4990, 0.8725, 0.5627, 0.8360],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.3427, 0.9351, 0.7291, 0.7306],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7939, 0.3007, 0.7258, 0.9482],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7249, 0.7534, 0.0027, 0.7793]])\n&gt;&gt;&gt; B=torch.split(A,(2,3),0)\n&gt;&gt;&gt; B\n(tensor([[0.1005, 0.9666, 0.5322, 0.6775],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.4990, 0.8725, 0.5627, 0.8360]]), \ntensor([[0.3427, 0.9351, 0.7291, 0.7306],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7939, 0.3007, 0.7258, 0.9482],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7249, 0.7534, 0.0027, 0.7793]]))\n</code></pre><p>这部分代码怎么解释呢？其实也很好理解，就是将Tensor A，沿着第0维进行切分，每一个结果对应维度上的尺寸或者说大小，分别是2（行），3（行）。</p><h3>unbind</h3><p>通过学习前面的几个函数，咱们知道了怎么按固定大小做切分，或者按照索引index来进行选择。现在我们想象一个应用场景，如果我们现在有一个3 channel图像的Tensor，想要逐个获取每个channel的数据，该怎么做呢？</p><p>假如用chunk的话，我们需要将chunks设为3；如果用split的话，需要将split_size_or_sections设为1。</p><p>虽然它们都可以实现相同的目的，但是如果channel数量很大，逐个去取也比较折腾。这时候，就需要用到另一个函数：unbind，它的函数定义如下：</p><pre><code class=\"language-python\">torch.unbind(input, dim=0)\n</code></pre><p>其中，input表示待处理的Tensor，dim还是跟前面的函数一样，表示切片的方向。</p><p>我们结合例子来理解：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.arange(0,16).view(4,4)\n&gt;&gt;&gt; A\ntensor([[ 0,&nbsp; 1,&nbsp; 2,&nbsp; 3],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 4,&nbsp; 5,&nbsp; 6,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 8,&nbsp; 9, 10, 11],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 13, 14, 15]])\n&gt;&gt;&gt; b=torch.unbind(A, 0)\n&gt;&gt;&gt; b\n(tensor([0, 1, 2, 3]), \ntensor([4, 5, 6, 7]), \ntensor([ 8,&nbsp; 9, 10, 11]), \ntensor([12, 13, 14, 15]))\n</code></pre><p>在这个例子中，我们首先创建了一个4x4的二维矩阵Tensor，随后我们从第0维，也就是“行”的方向进行切分 ，因为矩阵有4行，所以就会得到4个结果。</p><p>接下来，我们看一下：如果从第1维，也就是“列”的方向进行切分，会是怎样的结果呢：</p><pre><code class=\"language-python\">&gt;&gt;&gt; b=torch.unbind(A, 1)\n&gt;&gt;&gt; b\n(tensor([ 0,&nbsp; 4,&nbsp; 8, 12]), \ntensor([ 1,&nbsp; 5,&nbsp; 9, 13]), \ntensor([ 2,&nbsp; 6, 10, 14]), \ntensor([ 3,&nbsp; 7, 11, 15]))\n</code></pre><p>不难发现，这里是按照“列”的方向进行拆解的。所以，<strong>unbind是一种降维切分的方式</strong>，相当于删除一个维度之后的结果。</p><h2>Tensor的索引操作</h2><p>你有没有发现，刚才我们讲的chunk和split操作，我们都是将数据整体进行切分，并获得全部结果。但有的时候，我们只需要其中的一部分，这要怎么做呢？一个很自然的想法就是，直接告诉Tensor我想要哪些部分，这种方法我们称为索引操作。</p><p>索引操作有很多方式，有提供好现成API的，也有用户自行定制的操作，其中最常用的两个操作就是index_select和masked_select，我们分别去看看用法。</p><h3>index_select</h3><p>这里就需要index_select这个函数了，其定义如下：</p><pre><code class=\"language-python\">torch.index_select(tensor, dim, index)\n</code></pre><p>这里的tensor、dim跟前面函数里的一样，不再赘述。我们重点看一看index，它表示从dim维度中的哪些位置选择数据，这里需要注意，index<strong>是torch.Tensor类型</strong>。</p><p>还是跟之前一样，我们来看几个示例代码：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.arange(0,16).view(4,4)\n&gt;&gt;&gt; A\ntensor([[ 0,&nbsp; 1,&nbsp; 2,&nbsp; 3],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 4,&nbsp; 5,&nbsp; 6,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 8,&nbsp; 9, 10, 11],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 13, 14, 15]])\n&gt;&gt;&gt; B=torch.index_select(A,0,torch.tensor([1,3]))\n&gt;&gt;&gt; B\ntensor([[ 4,&nbsp; 5,&nbsp; 6,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 13, 14, 15]])\n&gt;&gt;&gt; C=torch.index_select(A,1,torch.tensor([0,3]))\n&gt;&gt;&gt; C\ntensor([[ 0,&nbsp; 3],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 4,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 8, 11],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 15]])\n</code></pre><p>在这个例子中，我们先创建了一个4x4大小的矩阵Tensor A。然后，我们从第0维选择第1（行）和3（行）的数据，并得到了最终的Tensor B，其大小为2x4。随后我们从Tensor A中选择第0（列）和3（列）的数据，得到了最终的Tensor C，其大小为4x2。</p><p>怎么样，是不是非常简单？</p><h3>masked_select</h3><p>刚才介绍的indexed_select，它是基于给定的索引来进行数据提取的。但有的时候，我们还想通过一些判断条件来进行选择，比如提取深度学习网络中某一层中数值大于0的参数。</p><p>这时候，就需要用到PyTorch提供的masked_select函数了，我们先来看它的定义：</p><pre><code class=\"language-python\">torch.masked_select(input, mask, out=None)&nbsp;\n</code></pre><p>这里我们只需要关心前两个参数，input和mask。</p><p>input表示待处理的Tensor。mask代表掩码张量，也就是满足条件的特征掩码。这里你需要注意的是，mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。</p><p>你是不是还感觉有些云里雾里？让我来举一个例子，你看了之后，一下子就能明白。</p><p>你在平时的练习中有没有想过，如果我们让Tensor和数字做比较，会有什么样的结果？比如后面这段代码，我们随机生成一个5位长度的Tensor A：</p><pre><code>&gt;&gt;&gt; A=torch.rand(5)\n&gt;&gt;&gt; A\ntensor([0.3731, 0.4826, 0.3579, 0.4215, 0.2285])\n&gt;&gt;&gt; B=A&gt;0.3\n&gt;&gt;&gt; B\ntensor([ True,  True,  True,  True, False])\n</code></pre><p>在这段代码里，我们让A跟0.3做比较，得到了一个新的Tensor，内部每一个数值表示的是A中对应数值是否大于0.3。</p><p>比如第一个数值原来是0.3731，大于0.3，所以是True；最后一个数值0.2285小于0.3，所以是False。</p><p>这个新的Tensor其实就是一个掩码张量，它的每一位表示了一个判断条件是否成立的结果。</p><p>然后，我们继续写一段代码，看看基于掩码B的选择是怎样的结果 ：</p><pre><code class=\"language-python\">&gt;&gt;&gt; C=torch.masked_select(A, B)\n&gt;&gt;&gt; C\ntensor([0.3731, 0.4826, 0.3579, 0.4215])\n</code></pre><p>你会发现，C实际上得到的就是：A中“<strong>满足B里面元素值为True的”</strong>对应位置的数据。</p><p>好了，这下你应该知道了masked_select的作用了吧？其实就是我们根据要筛选的条件，得到一个掩码张量，然后用这个张量去提取Tensor中的数据。</p><p>根据这个思路，上面的例子就可以简化为：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.rand(5)\n&gt;&gt;&gt; A\ntensor([0.3731, 0.4826, 0.3579, 0.4215, 0.2285])\n&gt;&gt;&gt; C=torch.masked_select(A, A&gt;0.3)\n&gt;&gt;&gt; C\ntensor([0.3731, 0.4826, 0.3579, 0.4215])\n</code></pre><p>是不是非常简单呢？</p><h2>小结</h2><p>恭喜你完成了这节课的学习。这节课，我们一同学习了Tensor里更加高级的操作，包括Tensor之间的连接操作，Tensor内部的切分操作，以及基于索引或者筛选条件的数据选择操作。</p><p>当然了，在使用这些函数的时候，你最需要关注的就是边界的数值大小，具体来说就是维度和大小相关的参数，一定要提前仔细计算好，要不然就会产生错误的结果。</p><p>结合众多的例子，我相信你一定可以拿下这些操作。</p><p>这里我特意给你梳理了一张表格，总结归纳了Tensor中的主要函数跟用法。不过这些参数咱们也不用死记硬背，我们在使用的时候，根据需要灵活查询相关的参数列表即可。<br>\n<img src=\"https://static001.geekbang.org/resource/image/d1/ba/d195706087f784c8e1e1c7c7b25a22ba.jpg?wh=3020x2455\" alt=\"\"><br>\n通过这两节课，我们搞懂了Tensor的一系列操作，在以后的项目中，你就可以游刃有余地对Tensor进行各种花式操作了，加油!</p><h2>每课一练</h2><p>现在有个Tensor，如下：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([[4,5,7], [3,9,8],[2,3,4]])\n&gt;&gt;&gt; A\ntensor([[4, 5, 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [3, 9, 8],\n&nbsp; &nbsp; &nbsp; &nbsp; [2, 3, 4]])\n</code></pre><p>我们想提取出其中第一行的第一个，第二行的第一、第二个，第三行的最后一个，该怎么做呢？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这节课分享给更多同事、朋友！</p>","neighbors":{"left":{"article_title":"04 | Tensor：PyTorch中最基础的计算单元","id":427460},"right":{"article_title":"06 | Torchvision（上）：数据读取，训练开始的第一步","id":429048}}},{"article_id":429048,"article_title":"06 | Torchvision（上）：数据读取，训练开始的第一步","article_content":"<p>你好，我是方远。</p><p>今天起我们进入模型训练篇的学习。如果将模型看作一辆汽车，那么它的开发过程就可以看作是一套完整的生产流程，环环相扣、缺一不可。这些环节包括<strong>数据的读取、网络的设计、优化方法与损失函数的选择以及一些辅助的工具等</strong>。未来你将尝试构建自己的豪华汽车，或者站在巨人的肩膀上对前人的作品进行优化。</p><p>试想一下，如果你对这些基础环节所使用的方法都不清楚，你还能很好地进行下去吗？所以通过这个模块，我们的目标是先把基础打好。通过这模块的学习，对于PyTorch都为我们提供了哪些丰富的API，你就会了然于胸了。</p><p>Torchvision 是一个和 PyTorch 配合使用的 Python 包，包含很多图像处理的工具。我们先从数据处理入手，开始PyTorch的学习的第一步。这节课我们会先介绍Torchvision的常用数据集及其读取方法，在后面的两节课里，我再带你了解常用的图像处理方法与Torchvision其它有趣的功能。</p><h2>PyTorch中的数据读取</h2><p>训练开始的第一步，首先就是数据读取。PyTorch为我们提供了一种十分方便的数据读取机制，即使用Dataset类与DataLoader类的组合，来得到数据迭代器。在训练或预测时，数据迭代器能够输出每一批次所需的数据，并且对数据进行相应的预处理与数据增强操作。</p><!-- [[[read_end]]] --><p>下面我们分别来看下Dataset类与DataLoader类。</p><h3>Dataset类</h3><p>PyTorch中的Dataset类是一个抽象类，它可以用来表示数据集。我们通过继承Dataset类来自定义数据集的格式、大小和其它属性，后面就可以供DataLoader类直接使用。</p><p>其实这就表示，无论使用自定义的数据集，还是官方为我们封装好的数据集，其本质都是继承了Dataset类。而在继承Dataset类时，至少需要重写以下几个方法：</p><ul>\n<li>__init__()：构造函数，可自定义数据读取方法以及进行数据预处理；</li>\n<li>__len__()：返回数据集大小；</li>\n<li>__getitem__()：索引数据集中的某一个数据。</li>\n</ul><p>光看原理不容易理解，下面我们来编写一个简单的例子，看下如何使用Dataset类定义一个Tensor类型的数据集。</p><pre><code class=\"language-python\">import torch\nfrom torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n&nbsp; &nbsp; # 构造函数\n&nbsp; &nbsp; def __init__(self, data_tensor, target_tensor):\n&nbsp; &nbsp; &nbsp; &nbsp; self.data_tensor = data_tensor\n&nbsp; &nbsp; &nbsp; &nbsp; self.target_tensor = target_tensor\n&nbsp; &nbsp; # 返回数据集大小\n&nbsp; &nbsp; def __len__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; return self.data_tensor.size(0)\n&nbsp; &nbsp; # 返回索引的数据与标签\n&nbsp; &nbsp; def __getitem__(self, index):\n&nbsp; &nbsp; &nbsp; &nbsp; return self.data_tensor[index], self.target_tensor[index]\n</code></pre><p>结合代码可以看到，我们定义了一个名字为MyDataset的数据集，在构造函数中，传入Tensor类型的数据与标签；在__len__函数中，直接返回Tensor的大小；在__getitem__函数中返回索引的数据与标签。</p><p>下面，我们来看一下如何调用刚才定义的数据集。首先随机生成一个10*3维的数据Tensor，然后生成10维的标签Tensor，与数据Tensor相对应。利用这两个Tensor，生成一个MyDataset的对象。查看数据集的大小可以直接用len()函数，索引调用数据可以直接使用下标。</p><pre><code class=\"language-python\"># 生成数据\ndata_tensor = torch.randn(10, 3)\ntarget_tensor = torch.randint(2, (10,)) # 标签是0或1\n\n# 将数据封装成Dataset\nmy_dataset = MyDataset(data_tensor, target_tensor)\n\n# 查看数据集大小\nprint('Dataset size:', len(my_dataset))\n'''\n输出：\nDataset size: 10\n'''\n\n# 使用索引调用数据\nprint('tensor_data[0]: ', my_dataset[0])\n'''\n输出:\ntensor_data[0]:  (tensor([ 0.4931, -0.0697,  0.4171]), tensor(0))\n'''\n</code></pre><h3>DataLoader类</h3><p>在实际项目中，如果数据量很大，考虑到内存有限、I/O速度等问题，在训练过程中不可能一次性的将所有数据全部加载到内存中，也不能只用一个进程去加载，所以就需要多进程、迭代加载，而DataLoader就是基于这些需要被设计出来的。</p><p>DataLoader是一个迭代器，最基本的使用方法就是传入一个Dataset对象，它会根据参数 batch_size的值生成一个batch的数据，节省内存的同时，它还可以实现多进程、数据打乱等处理。</p><p>DataLoader类的调用方式如下：</p><pre><code class=\"language-python\">from torch.utils.data import DataLoader\ntensor_dataloader = DataLoader(dataset=my_dataset, # 传入的数据集, 必须参数\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=2,&nbsp; &nbsp; &nbsp;  # 输出的batch大小\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;shuffle=True,&nbsp; &nbsp; &nbsp;  # 数据是否打乱\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_workers=0)&nbsp; &nbsp;   # 进程数, 0表示只有主进程\n\n# 以循环形式输出\nfor data, target in tensor_dataloader:&nbsp;\n&nbsp; &nbsp; print(data, target)\n'''\n输出:\ntensor([[-0.1781, -1.1019, -0.1507],\n        [-0.6170,  0.2366,  0.1006]]) tensor([0, 0])\ntensor([[ 0.9451, -0.4923, -1.8178],\n        [-0.4046, -0.5436, -1.7911]]) tensor([0, 0])\ntensor([[-0.4561, -1.2480, -0.3051],\n        [-0.9738,  0.9465,  0.4812]]) tensor([1, 0])\ntensor([[ 0.0260,  1.5276,  0.1687],\n        [ 1.3692, -0.0170, -1.6831]]) tensor([1, 0])\ntensor([[ 0.0515, -0.8892, -0.1699],\n        [ 0.4931, -0.0697,  0.4171]]) tensor([1, 0])\n'''\n&nbsp;\n# 输出一个batch\nprint('One batch tensor data: ', iter(tensor_dataloader).next())\n'''\n输出:\nOne batch tensor data:  [tensor([[ 0.9451, -0.4923, -1.8178],\n        [-0.4046, -0.5436, -1.7911]]), tensor([0, 0])]\n'''\n</code></pre><p>结合代码，我们梳理一下DataLoader中的几个参数，它们分别表示：</p><ul>\n<li>dataset：Dataset类型，输入的数据集，必须参数；</li>\n<li>batch_size：int类型，每个batch有多少个样本；</li>\n<li>shuffle：bool类型，在每个epoch开始的时候，是否对数据进行重新打乱；</li>\n<li>num_workers：int类型，加载数据的进程数，0意味着所有的数据都会被加载进主进程，默认为0。</li>\n</ul><h2>什么是Torchvision</h2><p>PyTroch官方为我们提供了一些常用的图片数据集，如果你需要读取这些数据集，那么无需自己实现，只需要利用Torchvision就可以搞定。</p><p>Torchvision 是一个和 PyTorch 配合使用的 Python 包。它不只提供了一些常用数据集，还提供了几个已经搭建好的经典网络模型，以及集成了一些图像数据处理方面的工具，主要供数据预处理阶段使用。简单地说，Torchvision 库就是<strong>常用数据集+常见网络模型+常用图像处理方法</strong>。</p><p>Torchvision的安装方式同样非常简单，可以使用conda安装，命令如下：</p><pre><code class=\"language-plain\">conda install torchvision -c pytorch\n</code></pre><p>或使用pip进行安装，命令如下：</p><pre><code class=\"language-plain\">pip install torchvision\n</code></pre><p>Torchvision中默认使用的图像加载器是PIL，因此为了确保Torchvision正常运行，我们还需要安装一个Python的第三方图像处理库——Pillow库。Pillow提供了广泛的文件格式支持，强大的图像处理能力，主要包括图像储存、图像显示、格式转换以及基本的图像处理操作等。</p><p>使用conda安装Pillow的命令如下：</p><pre><code class=\"language-python\">conda&nbsp;install&nbsp;pillow\n</code></pre><p>使用pip安装Pillow的命令如下：</p><pre><code class=\"language-python\">pip install pillow\n</code></pre><h2>利用Torchvision读取数据</h2><p>安装好Torchvision之后，我们再来接着看看。Torchvision库为我们读取数据提供了哪些支持。</p><p>Torchvision库中的<code>torchvision.datasets</code>包中提供了丰富的图像数据集的接口。常用的图像数据集，例如MNIST、COCO等，这个模块都为我们做了相应的封装。</p><p>下表中列出了<code>torchvision.datasets</code>包所有支持的数据集。各个数据集的说明与接口，详见链接<a href=\"https://pytorch.org/vision/stable/datasets.html\">https://pytorch.org/vision/stable/datasets.html</a>。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/44/5fa4d9067fa79b140d9e7646e7f28544.jpg?wh=1920x1162\" alt=\"图片\"></p><p>这里我想提醒你注意，<code>torchvision.datasets</code>这个包本身并不包含数据集的文件本身，它的工作方式是先从网络上把数据集下载到用户指定目录，然后再用它的加载器把数据集加载到内存中。最后，把这个加载后的数据集作为对象返回给用户。</p><p>为了让你进一步加深对知识的理解，我们以MNIST数据集为例，来说明一下这个模块具体的使用方法。</p><h3>MNIST数据集简介</h3><p>MNIST数据集是一个著名的手写数字数据集，因为上手简单，在深度学习领域，手写数字识别是一个很经典的学习入门样例。</p><p>MNIST数据集是NIST数据集的一个子集，MNIST 数据集你可以通过<a href=\"http://yann.lecun.com/exdb/mnist/\">这里</a>下载。它包含了四个部分，我用表格的方式为你做了梳理。</p><p><img src=\"https://static001.geekbang.org/resource/image/b6/9d/b6f465e8d27ca7f7abc27932da46309d.jpg?wh=1920x1157\" alt=\"图片\"></p><p>MNIST数据集是ubyte格式存储，我们先将“训练集图片”解析成图片格式，来直观地看一看数据集具体是什么样子的。具体怎么解析，我在后面数据预览再展开。</p><p><img src=\"https://static001.geekbang.org/resource/image/08/15/08977ccc74a3d2055434174e545d0515.jpg?wh=1920x844\" alt=\"图片\"></p><h3>数据读取</h3><p>接下来，我们看一下如何使用Torchvision来读取MNIST数据集。</p><p>对于<code>torchvision.datasets</code>所支持的所有数据集，它都内置了相应的数据集接口。例如刚才介绍的MNIST数据集，<code>torchvision.datasets</code>就有一个MNIST的接口，接口内封装了从下载、解压缩、读取数据、解析数据等全部过程。</p><p>这些接口的工作方式差不多，都是先从网络上把数据集下载到指定目录，然后再用加载器把数据集加载到内存中，最后将加载后的数据集作为对象返回给用户。</p><p>以MNIST为例，我们可以用如下方式调用：</p><pre><code class=\"language-python\"># 以MNIST为例\nimport torchvision\nmnist_dataset = torchvision.datasets.MNIST(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=True,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\n</code></pre><p><code>torchvision.datasets.MNIST</code>是一个类，对它进行实例化，即可返回一个MNIST数据集对象。构造函数包括包含5个参数：</p><ul>\n<li>root：是一个字符串，用于指定你想要保存MNIST数据集的位置。如果download是Flase，则会从目标位置读取数据集；</li>\n<li>download：是布尔类型，表示是否下载数据集。如果为True，则会自动从网上下载这个数据集，存储到root指定的位置。如果指定位置已经存在数据集文件，则不会重复下载；</li>\n<li>train：是布尔类型，表示是否加载训练集数据。如果为True，则只加载训练数据。如果为False，则只加载测试数据集。<strong>这里需要注意，并不是所有的数据集都做了训练集和测试集的划分，这个参数并不一定是有效参数，具体需要参考官方接口说明文档</strong>；</li>\n<li>transform：用于对图像进行预处理操作，例如数据增强、归一化、旋转或缩放等。这些操作我们会在下节课展开讲解；</li>\n<li>target_transform：用于对图像标签进行预处理操作。</li>\n</ul><p>运行上述的代码，我们可以得到下图所示的效果。从图中我们可以看出，程序首先去指定的网址下载了MNIST数据集，然后进行了解压缩等操作。如果你再次运行相同的代码，则不会再有下载的过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/8a/dcec80c2aa0e63f5450c85b7cda5c88a.png?wh=1920x1387\" alt=\"图片\"></p><p>看到这，你可能还有疑问，好奇我们得到的mnist_dataset是什么呢？</p><p>如果你用type函数查看一下mnist_dataset的类型，就可以得到<code>torchvision.datasets.mnist.MNIST</code> ，而这个类是之前我们介绍过的Dataset类的派生类。相当于<code>torchvision.datasets</code> ，它已经帮我们写好了对Dataset类的继承，完成了对数据集的封装，我们直接使用即可。</p><p>这里我们主要以MNIST为例，进行了说明。其它的数据集使用方法类似，调用的时候你只要需要将类名“MNIST”换成其它数据集名字即可。</p><p>对于不同的数据集，数据格式都不尽相同，而<code>torchvision.datasets</code>则帮助我们完成了各种不同格式的数据的解析与读取，可以说十分便捷。而对于那些没有官方接口的图像数据集，我们也可以使用以<code>torchvision.datasets.ImageFolder</code>接口来自行定义，在图像分类的实战篇中，就是使用ImageFolder进行数据读取的，你可以到那个时候再看一看。</p><h3>数据预览</h3><p>完成了数据读取工作，我们得到的是对应的mnist_dataset，刚才已经讲过了，这是一个封装了的数据集。</p><p>如果想要查看mnist_dataset中的具体内容，我们需要把它转化为列表。（如果IOPub&nbsp;data&nbsp;rate超限，可以只加载测试集数据，令train=False）</p><pre><code class=\"language-python\">mnist_dataset_list = list(mnist_dataset)\nprint(mnist_dataset_list)\n</code></pre><p>执行结果如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/12/9c7838a309b6e9ffa8yy33a44b00d312.png?wh=1920x434\" alt=\"图片\"></p><p>从运行结果中可以看出，转换后的数据集对象变成了一个元组列表，每个元组有两个元素，第一个元素是图像数据，第二个元素是图像的标签。</p><p>这里图像数据是PIL.Image.Image类型的，这种类型可以直接在Jupyter中显示出来。显示一条数据的代码如下。</p><pre><code class=\"language-python\">display(mnist_dataset_list[0][0])\nprint(\"Image label is:\", mnist_dataset_list[0][1])\n</code></pre><p>运行结果如下图所示。可以看出，数据集mnist_dataset中的第一条数据是图片手写数字“7”，对应的标签是“7”。</p><p><img src=\"https://static001.geekbang.org/resource/image/21/c3/211289da00fc13fd21f72573aee049c3.png?wh=1466x242\" alt=\"图片\"></p><p>好，如果你也得到了上面的运行结果，说明你的操作没问题，恭喜你成功完成了读取操作。</p><h2>小结</h2><p>恭喜你完成了这节课的学习。我们已经迈出了模型训练的第一步，学会了如何读取数据。</p><p>今天的重点就是掌握<strong>两种读取数据的方法，也就是自定义和读取常用图像数据集</strong>。</p><p>最通用的数据读取方法，就是自己定义一个Dataset的派生类。而读取常用的图像数据集，就可以利用PyTorch提供的视觉包Torchvision。</p><p>Torchvision库为我们读取数据提供了丰富的图像数据集的接口。我用手写数字识别这个经典例子，给你示范了如何使用Torchvision来读取MNIST数据集。</p><p><code>torchvision.datasets</code>继承了Dataset 类，它在预定义许多常用的数据集的同时，还预留了数据预处理与数据增强的接口。在下一节课中，我们就会接触到这些数据增强函数，并学习如何进行数据增强。</p><h2>每课一练</h2><p>在PyTorch中，我们要定义一个数据集，应该继承哪一个类呢？</p><p>欢迎你在留言区和我交流互动，也推荐你把这节课内容分享给更多的朋友、同事，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"05 | Tensor变形记：快速掌握Tensor切分、变形等方法","id":428186},"right":{"article_title":"07 | Torchvision（中）：数据增强，让数据更加多样性","id":429826}}},{"article_id":429826,"article_title":"07 | Torchvision（中）：数据增强，让数据更加多样性","article_content":"<p>你好，我是方远。</p><p>上一节课，我们一同迈出了训练开始的第一步——数据读取，初步认识了Torchvision，学习了如何利用Torchvision读取数据。不过仅仅将数据集中的图片读取出来是不够的，在训练的过程中，神经网络模型接收的数据类型是Tensor，而不是PIL对象，因此我们还需要对数据进行预处理操作，比如图像格式的转换。</p><p>与此同时，加载后的图像数据可能还需要进行一系列图像变换与增强操作，例如裁切边框、调整图像比例和大小、标准化等，以便模型能够更好地学习到数据的特征。这些操作都可以使用<code>torchvision.transforms</code>工具完成。</p><p>今天我们就来学习一下，利用Torchvision如何进行数据预处理操作，如何进行图像变换与增强。</p><h2>图像处理工具之torchvision.transforms</h2><p>Torchvision库中的<code>torchvision.transforms</code>包中提供了常用的图像操作，包括对Tensor 及PIL Image对象的操作，例如随机切割、旋转、数据类型转换等等。</p><p>按照<code>torchvision.transforms</code> 的功能，大致分为以下几类：数据类型转换、对PIL.Image 和 Tensor进行变化和变换的组合。下面我们依次来学习这些类别中的操作。</p><!-- [[[read_end]]] --><h3>数据类型转换</h3><p>在上一节课中，我们学习了读取数据集中的图片，读取到的数据是PIL.Image的对象。而在模型训练阶段，需要传入Tensor类型的数据，神经网络才能进行运算。</p><p>那么如何将PIL.Image或Numpy.ndarray格式的数据转化为Tensor格式呢？这需要用到<code>transforms.ToTensor()</code> 类。</p><p>而反之，将Tensor 或 Numpy.ndarray 格式的数据转化为PIL.Image格式，则使用<code>transforms.ToPILImage(mode=None)</code> 类。它则是ToTensor的一个逆操作，它能把Tensor或Numpy的数组转换成PIL.Image对象。</p><p>其中，参数mode代表PIL.Image的模式，如果mode为None（默认值），则根据输入数据的维度进行推断：</p><ul>\n<li>输入为3通道：mode为’RGB’；</li>\n<li>输入为4通道：mode为’RGBA’；</li>\n<li>输入为2通道：mode为’LA’;</li>\n<li>输入为单通道：mode根据输入数据的类型确定具体模式。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/d3/0c/d3013753ef85937a39b64ef8f556df0c.jpg?wh=318x116\" alt=\"图片\"></p><p>说完用法，我们来看一个具体的例子加深理解。以极客时间的LOGO图片（文件名为：jk.jpg）为例，进行一下数据类型的相互转换。具体代码如下。</p><pre><code class=\"language-python\">from PIL import Image\nfrom torchvision import transforms&nbsp;\n\n\n\nimg = Image.open('jk.jpg')&nbsp;\ndisplay(img)\nprint(type(img)) # PIL.Image.Image是PIL.JpegImagePlugin.JpegImageFile的基类\n'''\n输出:&nbsp;\n&lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt;\n'''\n\n# PIL.Image转换为Tensor\nimg1 = transforms.ToTensor()(img)\nprint(type(img1))\n'''\n输出:&nbsp;\n&lt;class 'torch.Tensor'&gt;\n'''\n\n# Tensor转换为PIL.Image\nimg2 = transforms.ToPILImage()(img1)&nbsp; #PIL.Image.Image\nprint(type(img2))\n'''\n输出:&nbsp;\n&lt;class 'PIL.Image.Image'&gt;\n'''\n</code></pre><p>首先用读取图片，查看一下图片的类型为PIL.JpegImagePlugin.JpegImageFile，这里需要注意<strong>，PIL.JpegImagePlugin.JpegImageFile类是PIL.Image.Image类的子类</strong>。然后，用<code>transforms.ToTensor()</code> 将PIL.Image转换为Tensor。最后，再将Tensor转换回PIL.Image。</p><h3>对 PIL.Image 和 Tensor 进行变换</h3><p><code>torchvision.transforms</code> 提供了丰富的图像变换方法，例如：改变尺寸、剪裁、翻转等。并且这些图像变换操作可以接收多种数据格式，不仅可以直接对PIL格式的图像进行变换，也可以对Tensor进行变换，无需我们再去做额外的数据类型转换。</p><p>下面我们依次来看一看。</p><h4>Resize</h4><p>将输入的 PIL Image 或 Tensor 尺寸调整为给定的尺寸，具体定义为：</p><pre><code class=\"language-python\">torchvision.transforms.Resize(size, interpolation=2)\n</code></pre><p>我们依次看下相关的参数：</p><ul>\n<li>size：期望输出的尺寸。如果 size 是一个像 (h, w) 这样的元组，则图像输出尺寸将与之匹配。如果 size 是一个 int 类型的整数，图像较小的边将被匹配到该整数，另一条边按比例缩放。</li>\n<li>interpolation：插值算法，int类型，默认为2，表示 PIL.Image.BILINEAR。</li>\n</ul><p>有关Size中是tuple还是int这一点请你一定要注意。</p><p>让我说明一下，在我们训练时，通常要把图片resize到一定的大小，比如说128x128，256x256这样的。如果直接给定resize后的高与宽，是没有问题的。但如果设定的是一个int型，较长的边就会按比例缩放。</p><p>在resize之后呢，一般会接一个crop操作，crop到指定的大小。对于高与宽接近的图片来说，这么做问题不大，但是高与宽的差距较大时，就会crop掉很多有用的信息。关于这一点，我们在后续的图像分类部分还会遇到，到时我在详细展开。</p><p>我们还是以极客时间的LOGO图片为例，一起看一下Resize的效果。</p><pre><code class=\"language-python\">from PIL import Image\nfrom torchvision import transforms&nbsp;\n\n# 定义一个Resize操作\nresize_img_oper = transforms.Resize((200,200), interpolation=2)\n\n# 原图\norig_img = Image.open('jk.jpg')&nbsp;\ndisplay(orig_img)\n\n# Resize操作后的图\nimg = resize_img_oper(orig_img)\ndisplay(img)\n</code></pre><p>首先定义一个Resize操作，设置好变换后的尺寸为(200, 200)，然后对极客时间LOGO图片进行Resize变换。<br>\n原图以及Resize变换后的效果如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/56/09/5611e53aaed88bb079909992db5c6d09.jpg?wh=1232x505\" alt=\"图片\"></p><h4>剪裁</h4><p><code>torchvision.transforms</code>提供了多种剪裁方法，例如中心剪裁、随机剪裁、四角和中心剪裁等。我们依次来看下它们的定义。</p><p>先说中心剪裁，顾名思义，在中心裁剪指定的 PIL Image 或 Tensor，其定义如下：</p><pre><code class=\"language-python\">torchvision.transforms.CenterCrop(size)\n</code></pre><p>其中，size表示期望输出的剪裁尺寸。如果 size 是一个像 (h, w) 这样的元组，则剪裁后的图像尺寸将与之匹配。如果&nbsp;size&nbsp;是&nbsp;int&nbsp;类型的整数，剪裁出来的图像是&nbsp;(size, size)&nbsp;的正方形。</p><p>然后是随机剪裁，就是在一个随机位置剪裁指定的 PIL Image 或 Tensor，定义如下：</p><pre><code class=\"language-python\">torchvision.transforms.RandomCrop(size, padding=None)\n</code></pre><p>其中，size代表期望输出的剪裁尺寸，用法同上。而padding表示图像的每个边框上的可选填充。默认值是 None，即没有填充。通常来说，不会用padding这个参数，至少对于我来说至今没用过。</p><p>最后要说的是FiveCrop，我们将给定的 PIL Image  或 Tensor ，分别从四角和中心进行剪裁，共剪裁成五块，定义如下：</p><pre><code>torchvision.transforms.FiveCrop(size)\n</code></pre><p>size可以是int或tuple，用法同上。<br>\n掌握了各种剪裁的定义和参数用法以后，我们来看一下这些剪裁操作具体如何调用，代码如下。</p><pre><code class=\"language-python\">from PIL import Image\nfrom torchvision import transforms&nbsp;\n\n# 定义剪裁操作\ncenter_crop_oper = transforms.CenterCrop((60,70))\nrandom_crop_oper = transforms.RandomCrop((80,80))\nfive_crop_oper = transforms.FiveCrop((60,70))\n\n# 原图\norig_img = Image.open('jk.jpg')&nbsp;\ndisplay(orig_img)\n\n# 中心剪裁\nimg1 = center_crop_oper(orig_img)\ndisplay(img1)\n# 随机剪裁\nimg2 = random_crop_oper(orig_img)\ndisplay(img2)\n# 四角和中心剪裁\nimgs = five_crop_oper(orig_img)\nfor img in imgs:\n&nbsp; &nbsp; display(img)\n</code></pre><p>流程和Resize类似，都是先定义剪裁操作，然后对极客时间LOGO图片进行不同的剪裁。<br>\n具体剪裁效果如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/b5/60ca577c5f08eef4ca727c1f0aac9cb5.jpg?wh=1384x896\" alt=\"图片\"></p><h4>翻转</h4><p>接下来，我们来看一看翻转操作。<code>torchvision.transforms</code>提供了两种翻转操作，分别是：以某一概率随机水平翻转图像和以某一概率随机垂直翻转图像。我们分别来看它们的定义。</p><p>以概率p随机水平翻转图像，定义如下：</p><pre><code class=\"language-plain\">torchvision.transforms.RandomHorizontalFlip(p=0.5)\n</code></pre><p>以概率p随机垂直翻转图像，定义如下：</p><pre><code class=\"language-plain\">torchvision.transforms.RandomVerticalFlip(p=0.5)\n</code></pre><p>其中，p表示随机翻转的概率值，默认为0.5。<br>\n这里的随机翻转，是为数据增强提供方便。如果想要必须执行翻转操作的话，将p设置为1即可。</p><p>以极客时间的LOGO图片为例，图片翻转的代码如下。</p><pre><code class=\"language-python\">from PIL import Image\nfrom torchvision import transforms&nbsp;\n\n# 定义翻转操作\nh_flip_oper = transforms.RandomHorizontalFlip(p=1)\nv_flip_oper = transforms.RandomVerticalFlip(p=1)\n\n# 原图\norig_img = Image.open('jk.jpg')&nbsp;\ndisplay(orig_img)\n\n# 水平翻转\nimg1 = h_flip_oper(orig_img)\ndisplay(img1)\n# 垂直翻转\nimg2 = v_flip_oper(orig_img)\ndisplay(img2)\n</code></pre><p>翻转效果如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/84/0dc2543bb7bdfyy7803c353f2030f184.jpg?wh=1386x675\" alt=\"图片\"></p><h3>只对Tensor进行变换</h3><p>目前版本的Torchvision（v0.10.0）对各种图像变换操作已经基本同时支持 PIL Image 和 Tensor 类型了，因此只针对Tensor的变换操作很少，只有4个，分别是LinearTransformation（线性变换）、Normalize（标准化）、RandomErasing（随机擦除）、ConvertImageDtype（格式转换）。</p><p>这里我们重点来看最常用的一个操作：标准化，其他3个你可以查阅官方文档。</p><h4>标准化</h4><p>标准化是指每一个数据点减去所在通道的平均值，再除以所在通道的标准差，数学的计算公式如下：</p><p>$$output=(input-mean)/std$$</p><p>而对图像进行标准化，就是对图像的每个通道利用均值和标准差进行正则化。这样做的目的，是<strong>为了保证数据集中所有的图像分布都相似，这样在训练的时候更容易收敛，既加快了训练速度，也提高了训练效果</strong>。</p><p>让我来解释一下：首先，标准化是一个常规做法，可以理解为无脑进行标准化后再训练的效果，大概率要好于不进行标准化。</p><p>我把极客时间的LOGO读入后，所有像素都减去50，获得下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/3c/a1/3c3f30cee39ec09cc08fa91b4925e3a1.png?wh=640x234\" alt=\"图片\"></p><p>对于我们人来说是可以分辨出，这也是极客时间的LOGO。但是计算机（也就是卷积神经网络）就不一定能分辨出来了，因为卷积神经网络是通过图像的像素进行提取特征的，这两张图片像素的数值都不一样，凭什么还让神经网络认为是一张图片？</p><p>而标准化后的数据就会避免这一问题，标准化后会将数据映射到同一区间中，一个类别的图片虽说有的像素值可能有差异，但是它们分布都是类似的分布。</p><p><code>torchvision.transforms</code>提供了对Tensor进行标准化的函数，定义如下。</p><pre><code class=\"language-plain\">torchvision.transforms.Normalize(mean, std, inplace=False)\n</code></pre><p>其中，每个参数的含义如下所示：</p><ul>\n<li>mean：表示各通道的均值；</li>\n<li>std：表示各通道的标准差；</li>\n<li>inplace：表示是否原地操作，默认为否。</li>\n</ul><p>以极客时间的LOGO图片为例，我们来看看以(R, G, B)均值和标准差均为(0.5, 0.5, 0.5)来标准化图片后，是什么效果。</p><pre><code class=\"language-python\">from PIL import Image\nfrom torchvision import transforms&nbsp;\n\n# 定义标准化操作\nnorm_oper = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\n# 原图\norig_img = Image.open('jk.jpg')&nbsp;\ndisplay(orig_img)\n\n# 图像转化为Tensor\nimg_tensor = transforms.ToTensor()(orig_img)\n\n# 标准化\ntensor_norm = norm_oper(img_tensor)\n\n# Tensor转化为图像\nimg_norm = transforms.ToPILImage()(tensor_norm)\ndisplay(img_norm)\n</code></pre><p>上面代码的过程是，首先定义了均值和标准差均为(0.5, 0.5, 0.5)的标准化操作，然后将原图转化为Tensor，接着对Tensor进行标准化，最后再将Tensor转化为图像输出。</p><p>标准化的效果如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/f5/05/f58f3662e60501e02b31b12fa9f4e905.jpg?wh=1244x515\" alt=\"\"></p><h3>变换的组合</h3><p>其实前面介绍过的所有操作都可以用 Compose 类组合起来，进行连续操作。</p><p>Compose类是将多个变换组合到一起，它的定义如下。</p><pre><code class=\"language-plain\">torchvision.transforms.Compose(transforms)\n</code></pre><p>其中，transforms是一个Transform对象的列表，表示要组合的变换列表。<br>\n我们还是结合例子动手试试，如果我们想要将图片变为200*200像素大小，并且随机裁切成80像素的正方形。那么我们可以组合Resize和RandomCrop变换，具体代码如下所示。</p><pre><code class=\"language-python\">from PIL import Image\nfrom torchvision import transforms&nbsp;\n\n# 原图\norig_img = Image.open('jk.jpg')&nbsp;\ndisplay(orig_img)\n\n# 定义组合操作\ncomposed = transforms.Compose([transforms.Resize((200, 200)),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transforms.RandomCrop(80)])\n\n# 组合操作后的图\nimg = composed(orig_img)\ndisplay(img)\n</code></pre><p>运行的结果如下表所示，也推荐你动手试试看。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/c1/6b3ce280815cff443734c9b8180fc6c1.jpg?wh=1046x505\" alt=\"图片\"></p><h2>结合datasets使用</h2><p>Compose类是未来我们在实际项目中经常要使用到的类，结合<code>torchvision.datasets</code>包，就可以在读取数据集的时候做图像变换与数据增强操作。下面让我们一起来看一看。</p><p>还记得<a href=\"https://time.geekbang.org/column/article/429048\">上一节课</a>中，在利用<code>torchvision.datasets</code> 读取MNIST数据集时，有一个参数“transform”吗？它就是用于对图像进行预处理操作的，例如数据增强、归一化、旋转或缩放等。这里的“transform”就可以接收一个<code>torchvision.transforms</code>操作或者由Compose类所定义的操作组合。</p><p>上节课中，我们在读取MNIST数据集时，直接读取出来的图像数据是PIL.Image.Image类型的。但是遇到要训练手写数字识别模型这类的情况，模型接收的数据类型是Tensor，而不是PIL对象。这时候，我们就可以利用“transform”参数，使数据在读取的同时做类型转换，这样读取出的数据直接就可以是Tensor类型了。</p><p>不只是数据类型的转换，我们还可以增加归一化等数据增强的操作，只需要使用上面介绍过的Compose类进行组合即可。这样，在读取数据的同时，我们也就完成了数据预处理、数据增强等一系列操作。</p><p>我们还是以读取MNIST数据集为例，看下如何在读取数据的同时，完成数据预处理等操作。具体代码如下。</p><pre><code class=\"language-python\">from torchvision import transforms\nfrom torchvision import datasets\n\n# 定义一个transform\nmy_transform = transforms.Compose([transforms.ToTensor(),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transforms.Normalize((0.5), (0.5))\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ])\n# 读取MNIST数据集 同时做数据变换\nmnist_dataset = datasets.MNIST(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=my_transform,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\n\n# 查看变换后的数据类型\nitem = mnist_dataset.__getitem__(0)\nprint(type(item[0]))\n'''\n输出：\n&lt;class 'torch.Tensor'&gt;\n'''\n</code></pre><p>当然，MNIST数据集非常简单，根本不进行任何处理直接读入的话，效果也非常好，但是它确实适合学习来使用，你可以在利用它进行各种尝试。</p><p>我们下面先来看看，在图像分类实战中使用的transform，可以感受一下实际使用的transforms是什么样子：</p><pre><code class=\"language-python\">transform = transforms.Compose([\n&nbsp; &nbsp; transforms.RandomResizedCrop(dest_image_size),\n&nbsp; &nbsp; transforms.RandomHorizontalFlip(),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n&nbsp; &nbsp; ])\n</code></pre><p>这也是我在项目中使用的transform。数据增强的方法有很多，不过根据我的经验来看，并不是用的越多，效果越好。</p><h2>小结</h2><p>恭喜你完成了这节课的学习，我来给你做个总结。</p><p><strong>今天的重点内容就是<code>torchvision.transforms</code>工具的使用。包括常用的图像处理操作，以及如何与<code>torchvision.datasets</code>结合使用</strong>。</p><p>常用的图像处理操作包括数据类型转换、图像尺寸变化、剪裁、翻转、标准化等等。Compose类还可以将多个变换操作组合成一个Transform对象的列表。</p><p><code>torchvision.transforms</code>与<code>torchvision.datasets</code>结合使用，可以在数据加载的同时进行一系列图像变换与数据增强操作，不仅能够直接将数据送入模型训练，还可以加快模型收敛速度，让模型更好地学习到数据特征。</p><p>当然，我们在实际的项目中会有自己的数据，而不会使用torchvision.datasets中提供的公开数据集，我们今天讲的<code>torchvision.transforms</code> 同样可以在我们自定义的数据集中使用，关于这一点，我会在图像分类的实战中继续讲解。</p><p>下节课中，我们会介绍Torchvision中其他有趣的功能。包括经典网络模型的实例化与其他有用的函数。</p><h2>每课一练</h2><p>Torchvision中 transforms 模块的作用是什么？</p><p>欢迎你在留言区跟我交流讨论，也欢迎你把这节课分享给自己的朋友，和他一起尝试一下Torchvision的各种功能。</p>","neighbors":{"left":{"article_title":"06 | Torchvision（上）：数据读取，训练开始的第一步","id":429048},"right":{"article_title":"08 | Torchvision（下）：其他有趣的功能","id":431420}}},{"article_id":431420,"article_title":"08 | Torchvision（下）：其他有趣的功能","article_content":"<p>你好，我是方远。</p><p>在前面的课程中，我们已经学习了Torchvision的数据读取与常用的图像变换方法。其实，Torchvision除了帮我们封装好了常用的数据集，还为我们提供了深度学习中各种经典的网络结构以及训练好的模型，只要直接将这些经典模型的类实例化出来，就可以进行训练或使用了。</p><p>我们可以利用这些训练好的模型来实现图片分类、物体检测、视频分类等一系列应用。</p><p>今天，我们就来学习一下经典网络模型的实例化与Torchvision中其他有趣的功能。</p><h2>常见网络模型</h2><p>Torchvision中的各种经典网络结构以及训练好的模型，都放在了<code>torchvision.models</code>模块中，下面我们来看一看<code>torchvision.models</code> 具体为我们提供了什么支持，以及这些功能如何使用。</p><h3>torchvision.models模块</h3><p><code>torchvision.models</code> 模块中包含了常见网络模型结构的定义，这些网络模型可以解决以下四大类问题：图像分类、图像分割、物体检测和视频分类。图像分类、物体检测与图像分割的示意图如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/42/b3/4211c2d8cd27db3e903e6125122f47b3.jpg?wh=1920x1204\" alt=\"\"></p><p>图像分类，指的是单纯把一张图片判断为某一类，例如将上图左侧第一张判断为cat。目标检测则是说，首先检测出物体的位置，还要识别出对应物体的类别。如上图中间的那张图，不仅仅要找到猫、鸭子、狗的位置，还有给出给定物体的类别信息。</p><!-- [[[read_end]]] --><p>我们看一下图里最右侧的例子，它表示的是分割。分割即是对图像中每一个像素点进行分类，确定每个点的类别，从而进行区域划分。</p><p>在早期的Torchvision版本中，<code>torchvision.models</code>模块中只包含了图片分类中的一部分网络，例如AlexNet、VGG系列、ResNet系列、Inception系列等。这里你先有个印象就行，具体网络特点，我后面会在图像分类中详细讲解。</p><p>到了现在，随着深度学习技术的不断发展，人工智能应用更为广泛，<code>torchvision.models</code>模块中所封装的网络模型也在不断丰富。比如在当前版本（v0.10.0）的Torchvision中，新增了图像语义分割、物体检测和视频分类的相关网络，并且在图像分类中也新增了GoogLeNet、ShuffleNet以及可以使用于移动端的MobileNet系列。这些新模型，都能让我们站在巨人的肩膀上看世界。</p><h3>实例化一个GoogLeNet网络</h3><p>如果我们直接把一个网络模型的类实例化，就会得到一个网络模型。而这个网络模型的类可以是我们自己定义的结构，也可以是按照经典模型的论文设计出来的结构。其实你自己按照经典模型的论文写一个类，然后实例化一下，这和从Torchvision中直接实例化一个网络效果是相同的。</p><p>下面我们就以 GoogLeNet 网络为例，来说说如何使用<code>torchvision.models</code>模块实例化一个网络。</p><p>GoogLeNet是Google推出的基于Inception模块的深度神经网络模型。你可别小看这个模型，GoogLeNet获得了2014年的ImageNet竞赛的冠军，并且相比之前的AlexNet、VGG等结构能更高效地利用计算资源。</p><p>GoogLeNet 也被称为Inception V1，在随后的两年中它一直在改进，形成了Inception V2、Inception V3等多个版本。</p><p>我们可以使用<strong>随机初始化的权重，创建一个GoogLeNet模型</strong>，具体代码如下：</p><pre><code class=\"language-python\">import torchvision.models as models\ngooglenet = models.googlenet()\n</code></pre><p>这时候的 GoogLeNet 模型，相当于只有一个实例化好的网络结构，里面的参数都是随机初始化的，需要经过训练之后才能使用，并不能直接用于预测。<br>\n<code>torchvision.models</code>模块除了包含了定义好的网络结构，还为我们提供了预训练好的模型，我们可以<strong>直接导入训练好的模型来使用</strong>。导入预训练好的模型的代码如下：</p><pre><code class=\"language-python\">import torchvision.models as models\ngooglenet = models.googlenet(pretrained=True)\n</code></pre><p>可以看出，我们只是在实例化的时候，引入了一个参数“pretrained=True”，即可获得预训练好的模型，因为所有的工作<code>torchvision.models</code>模块都已经帮我们封装好了，用起来很方便。<br>\n<code>torchvision.models</code>模块中所有预训练好的模型，都是在ImageNet数据集上训练的，它们都是由PyTorch 的<code>torch.utils.model_zoo</code>模块所提供的，并且我们可以通过参数&nbsp;pretrained=True&nbsp;来构造这些预训练模型。</p><p>如果之前没有加载过带预训练参数的网络，在实例化一个预训练好的模型时，模型的参数会被下载至缓存目录中，下载一次后不需要重复下载。这个缓存目录可以通过环境变量TORCH_MODEL_ZOO来指定。当然，你也可以把自己下载好的模型，然后复制到指定路径中。</p><p>下图是运行了上述实例化代码的结果，可以看到，GoogLeNet的模型参数被下载到了缓存目录/root/.cache/torch下面。</p><p><img src=\"https://static001.geekbang.org/resource/image/16/49/16975c6f4071ee1dacc9a41a28f93c49.png?wh=1920x270\" alt=\"图片\"></p><p><code>torchvision.models</code>模块也包含了Inception V3和其他常见的网络结构，在实例化时，只需要修改网络的类名，即可做到举一反三。<code>torchvision.models</code>模块中可实例化的全部模型详见这个<a href=\"https://pytorch.org/vision/stable/models.html\">网页</a>。</p><h3>模型微调</h3><p>完成了刚才的工作，你可能会疑惑，实例化了带预训练参数的网络有什么用呢？其实它除了可以直接用来做预测使用，还可以基于它做网络模型的微调，也就是“fine-tuning”。</p><p>那什么是“fine-tuning”呢？</p><p>举个例子，假设你的老板给布置了一个有关于图片分类的任务，数据集是关于狗狗的图片，让你区分图片中狗的种类，例如金毛、柯基、边牧等等。</p><p>问题是数据集中狗的类别很多，但数据却不多。你发现从零开始训练一个图片分类模型，但这样模型效果很差，并且很容易过拟合。这种问题该如何解决呢？于是你想到了使用迁移学习，可以用已经在ImageNet数据集上训练好的模型来达成你的目的。</p><p>例如上面我们已经实例化的GoogLeNet模型，只需要使用我们自己的数据集，重新训练网络最后的分类层，即可得到区分狗种类的图片分类模型。这就是所谓的“fine-tuning”方法。</p><p><strong>模型微调</strong>，简单来说就是先在一个比较通用、宽泛的数据集上进行大量训练得出了一套参数，然后再使用这套预训练好的网络和参数，在自己的任务和数据集上进行训练。使用经过预训练的模型，要比使用随机初始化的模型训练<strong>效果更好</strong>，<strong>更容易收敛</strong>，并且<strong>训练速度更快</strong>，在小数据集上也能取得比较理想的效果。</p><p>那新的问题又来了，为什么模型微调如此有效呢？因为我们相信同样是处理图片分类任务的两个模型，网络的参数也具有某种相似性。因此，把一个已经训练得很好的模型参数迁移到另一个模型上，同样有效。即使两个模型的工作不完全相同，我们也可以在这套预训练参数的基础上，经过微调性质的训练，同样能取得不错的效果。</p><p>ImageNet数据集共有1000个类别，而狗的种类远远达不到1000类。因此，加载了预训练好的模型之后，还需要根据你的具体问题对模型或数据进行一些调整，通常来说是调整输出类别的数量。</p><p>假设狗的种类一共为10类，那么我们自然需要将GoogLeNet模型的输出分类数也调整为10。对预训练模型进行调整对代码如下：</p><pre><code class=\"language-python\">import torch\nimport torchvision.models as models\n\n# 加载预训练模型\ngooglenet = models.googlenet(pretrained=True)\n\n# 提取分类层的输入参数\nfc_in_features = googlenet.fc.in_features\nprint(\"fc_in_features:\", fc_in_features)\n\n# 查看分类层的输出参数\nfc_out_features = googlenet.fc.out_features\nprint(\"fc_out_features:\", fc_out_features)\n\n# 修改预训练模型的输出分类数(在图像分类原理中会具体介绍torch.nn.Linear)\ngooglenet.fc = torch.nn.Linear(fc_in_features, 10)\n'''\n输出：\nfc_in_features: 1024\nfc_out_features: 1000\n'''\n</code></pre><p>首先，你需要加载预训练模型，然后提取预训练模型的分类层固定参数，最后修改预训练模型的输出分类数为10。根据输出结果，我们可以看到预训练模型的原始输出分类数是1000。</p><h2>其他常用函数</h2><p>之前在<code>torchvision.transforms</code>中，我们学习了很多有关于图像处理的函数，Torchvision还提供了几个常用的函数，make_grid和save_img，让我们依次来看一看它们又能实现哪些有趣的功能。</p><h3>make_grid</h3><p>make_grid 的作用是将若干幅图像拼成在一个网格中，它的定义如下。</p><pre><code class=\"language-plain\">torchvision.utils.make_grid(tensor, nrow=8, padding=2)&nbsp;\n</code></pre><p>定义中对应的几个参数含义如下：</p><ul>\n<li>tensor：类型是Tensor或列表，如果输入类型是Tensor，其形状应是 (B x C x H x W)；如果输入类型是列表，列表中元素应为相同大小的图片。</li>\n<li>nrow：表示一行放入的图片数量，默认为8。</li>\n<li>padding：子图像与子图像之间的边框宽度，默认为2像素。</li>\n</ul><p>make_grid函数主要用于展示数据集或模型输出的图像结果。我们以MNIST数据集为例，整合之前学习过的读取数据集以及图像变换的内容，来看一看make_grid函数的效果。</p><p>下面的程序利用make_grid函数，展示了MNIST的测试集中的32张图片。</p><pre><code class=\"language-python\">import torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# 加载MNIST数据集\nmnist_dataset = datasets.MNIST(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transforms.ToTensor(),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\n# 取32张图片的tensor\ntensor_dataloader = DataLoader(dataset=mnist_dataset,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=32)\ndata_iter = iter(tensor_dataloader)\nimg_tensor, label_tensor = data_iter.next()\nprint(img_tensor.shape)\n'''\n输出：torch.Size([32, 1, 28, 28])\n'''\n# 将32张图片拼接在一个网格中\ngrid_tensor = torchvision.utils.make_grid(img_tensor, nrow=8, padding=2)\ngrid_img = transforms.ToPILImage()(grid_tensor)\ndisplay(grid_img)\n</code></pre><p>结合代码我们可以看到，程序首先利用<code>torchvision.datasets</code>加载MNIST的测试集，然后利用DataLoader类的迭代器一次获取到32张图片的Tensor，最后利用make_grid函数将32张图片拼接在了一幅图片中。<br>\nMNIST的测试集中的32张图片，如下图所示，这里我要特别说明一下，因为MNIST的尺寸为28x28，所以测试集里的手写数字图片像素都比较低，但这并不影响咱们动手实践。你可以参照我给到的示范，自己动手试试看。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/yy/bb73d6bdf49fc876d983cfa48569dcyy.png?wh=242x122\" alt=\"图片\"></p><h3>save_img</h3><p>一般来说，在保存模型输出的图片时，需要将Tensor类型的数据转化为图片类型才能进行保存，过程比较繁琐。Torchvision提供了save_image函数，能够直接将Tensor保存为图片，即使Tensor数据在CUDA上，也会自动移到CPU中进行保存。</p><p>save_image函数的定义如下。</p><pre><code class=\"language-python\">torchvision.utils.save_image(tensor, fp, **kwargs)\n</code></pre><p>这些参数也很好理解：</p><ul>\n<li>tensor：类型是Tensor或列表，如果输入类型是Tensor，直接将Tensor保存；如果输入类型是列表，则先调用make_grid函数生成一张图片的Tensor，然后再保存。</li>\n<li>fp：保存图片的文件名；</li>\n<li>**kwargs：make_grid函数中的参数，前面已经讲过了。</li>\n</ul><p>我们接着上面的小例子，将32张图片的拼接图直接保存，代码如下。</p><pre><code class=\"language-python\"># 输入为一张图片的tensor 直接保存\ntorchvision.utils.save_image(grid_tensor, 'grid.jpg')\n\n# 输入为List 调用grid_img函数后保存\ntorchvision.utils.save_image(img_tensor, 'grid2.jpg', nrow=5, padding=2)\n</code></pre><p>当输入为一张图片的Tensor时，直接保存，保存的图片如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/yy/bb73d6bdf49fc876d983cfa48569dcyy.png?wh=242x122\" alt=\"图片\"></p><p>当输入为List时，则会先调用make_grid函数，make_grid函数的参数直接加在后面即可，代码中令nrow=5，保存的图片如下所示。这时我们可以看到图片中，每行中有5个数字，最后一行不足的数字，已经自动填充了空图像。</p><p><img src=\"https://static001.geekbang.org/resource/image/21/a6/21435a2115ca4704bc51496f8a1c8da6.png?wh=152x212\" alt=\"图片\"></p><h2>小结</h2><p>恭喜你完成了这节课的学习。至此，Torchvision的全部内容我们就学完了。</p><p>今天的重点内容是<code>torchvision.models</code>模块的使用，包括如何实例化一个网络与如何进行模型的微调。</p><p><code>torchvision.models</code>模块为我们提供了深度学习中各种经典的网络结构以及训练好的模型，我们不仅可以实例化一个随机初始化的网络模型，还可以实例化一个预训练好的网络模型。</p><p>模型微调可以让我们在自己的小数据集上快速训练模型，并取得比较理想的效果。但是我们需要根据具体问题对预训练模型或数据进行一些修改，你可以灵活调整输出类别的数量，或者调整输入图像的大小。</p><p>除了模型微调，我还讲了两个Torchvision中有趣的函数，make_grid和save_img，我还结合之前我们学习过的读取数据集以及图像变换的内容，为你做了演示。相信Torchvision工具配合PyTorch使用，一定能够使你事半功倍。</p><h2>每课一练</h2><p>请你使用<code>torchvision.models</code>模块实例化一个VGG 16网络。</p><p>欢迎你在留言区跟我交流讨论，也推荐你把这节课分享给更多的同事、朋友。</p><p>我是方远，我们下节课见！</p>","neighbors":{"left":{"article_title":"07 | Torchvision（中）：数据增强，让数据更加多样性","id":429826},"right":{"article_title":"09 | 卷积（上）：如何用卷积为计算机“开天眼”？","id":432042}}},{"article_id":432042,"article_title":"09 | 卷积（上）：如何用卷积为计算机“开天眼”？","article_content":"<p>你好，我是方远。</p><p>现在刷脸支付的场景越来越多，相信人脸识别你一定不陌生，你有没有想过，在计算机识别人脸之前，我们人类是如何判断一个人是谁的呢？</p><p>我们眼睛看到人脸的时候，会先将人脸的一些粗粒度特征提取出来，例如人脸的轮廓、头发的颜色、头发长短等。然后这些信息会一层一层地传入到某一些神经元当中，每经过一层神经元就相当于特征提取。我们大脑最终会将最后的特征进行汇总，类似汇总成一张具体的人脸，用这张人脸去大脑的某一个地方与存好的人名进行匹配。</p><p>那落实到我们计算机呢？其实这个过程是一样的，在计算机中进行特征提取的功能，就离不开我们今天要讲的卷积。</p><p>可以说，没有卷积的话，深度学习在图像领域不可能取得今天的成就。 那么，就让我们来看看什么是卷积，还有它在PyTorch中的实现吧。</p><h2>卷积</h2><p>在使用卷积之前，人们尝试了很多人工神经网络来处理图像问题，但是人工神经网络的参数量非常大，从而导致非常难训练，所以计算机视觉的研究一直停滞不前，难以突破。</p><p>直到卷积神经网络的出现，它的两个优秀特点：稀疏连接与平移不变性，这让计算机视觉的研究取得了长足的进步。什么是稀疏连接与平移不变性呢？简单来说，就是稀疏连接可以让学习的参数变得很少，而平移不变性则不关心物体出现在图像中什么位置。</p><!-- [[[read_end]]] --><p>稀疏连接与平移不变性是卷积的两个重要特点，如果你想从事计算机视觉相关的工作，这两个特点必须该清楚，但不是本专栏的重点，这里就不展开了，有兴趣你可以自己去了解。</p><p>下面我们直接来看看卷积是如何计算的。</p><h3>最简单的情况</h3><p>我们先看最简单的情况，输入是一个4x4的特征图，卷积核的大小为2x2。</p><p>卷积核是什么呢？其实就是我们卷积层要学习到的参数，就像下图中红色的示例，下图中的卷积核是最简单的情况，只有一个通道。</p><p><img src=\"https://static001.geekbang.org/resource/image/ac/5d/ac84c162ee165d535fcbf465572faf5d.jpg?wh=1075x728\" alt=\"图片\"></p><p>输入特征与卷积核计算时，计算方式是卷积核与输入特征按位做乘积运算然后再求和，其结果为输出特征图的一个元素，下图为计算输出特征图第一个元素的计算方式：</p><p><img src=\"https://static001.geekbang.org/resource/image/78/20/787c8b346de00dayyd7e2d3504c33320.jpg?wh=1561x891\" alt=\"图片\"></p><p>完成了第一个元素的计算，我们接着往下看，按以从左向右，从上至下的顺序进行滑动卷积核，分别与输入的特征图进行计算，请看下图，下图为上图计算完毕之后，向右侧滑动一个单元的计算方式：</p><p><img src=\"https://static001.geekbang.org/resource/image/5y/b5/5yy249d2f1221e21a1bdc7d8756f4fb5.jpg?wh=1544x862\" alt=\"图片\"></p><p>第一行第三个单元的计算以此类推。说完了同一行的移动，我们再看看，第一行计算完毕，向下滑动的计算方式是什么样的。</p><p><img src=\"https://static001.geekbang.org/resource/image/cf/bb/cf4aa3yy8ac31b06f153f9090d3bcebb.jpg?wh=1552x929\" alt=\"图片\"></p><p>第一行计算完毕之后，卷积核会回到行首，然后向下滑动一个单元，再重复以上从左至右的滑动计算。</p><p>这里我再给你补充一个知识点，什么是步长？</p><p>卷积上下左右滑动的长度，我们称为步长，用stride表示。上述例子中的步长就是1，根据问题的不同，会取不同的步长，但通常来说步长为1或2。不管是刚才说的最简单的卷积计算，还是我们后面要讲的标准卷积，都要用到这个参数。</p><h3>标准的卷积</h3><p>好啦，前面只是最简单的情况，现在我们将最简单的卷积计算方式延伸到标准的卷积计算方式。</p><p>我们先将上面的例子描述为更加通用的形式，输入的特征有m个通道，宽为w，高为h；输出有n个特征图，宽为$w^{\\prime}$，高为$h^{\\prime}$；卷积核的大小为kxk。</p><p>在刚才的例子中m、n、k、w、h、$w^{\\prime}$、$h^{\\prime}$的值分别为1、1、2、4、4、3、3。而现在，我们需要把一个输入为(m，h，w)的输入特征图经过卷积计算，生成一个输出为(n, $h^{\\prime}$, $w^{\\prime}$)的特征图。</p><p>那我们来看看可以获得这个操作的卷积是什么样子的。输出特征图的通道数由<strong>卷积核的个数决定</strong>的，所以说卷积核的个数为n。根据卷积计算的定义，<strong>输入特征图有m个通道，所以每个卷积核里要也要有m个通道</strong>。所以，我们的需要n个卷积核，每个卷积核的大小为(m, k, k)。</p><p>为了帮你更好地理解刚才所讲的内容，我画了示意图，你可以对照一下：</p><p><img src=\"https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080\" alt=\"\"></p><p>结合上面的图解可以看到，卷积核1与全部输入特征进行卷积计算，就获得了输出特征图中第1个通道的数据，卷积核2与全部输入特征图进行计算获得输出特征图中第2个通道的数据。以此类推，最终就能计算n个输出特征图。</p><p>在开篇的例子中，输入只有1个通道，现在有多个通道了，那我们该如何计算呢？其实计算方式类似，输入特征的每一个通道与卷积核中对应通道的数据按我们之前讲过的方式进行卷积计算，也就是输入特征图中第i个特征图与卷积核中的第i个通道的数据进行卷积。这样计算后会生成<strong>m</strong>个特征图，然后将这m个特征图按对应位置求和即可，求和后m个特征图合并为输出特征中一个通道的特征图。</p><p>我们可以用后面的公式表示当输入有多个通道时，每个卷积核是如何与输入进行计算的。</p><p>$Output_i$表示计算第i个输出特征图，i的取值为1到n；</p><p>$kernel_k$表示1个卷积核里的第k个通道的数据；</p><p>$input_k$表示输入特征图中的第k个通道的数据；</p><p>$bias_k$为偏移项，我们在训练时一般都会默认加上；</p><p>$\\star$为卷积计算；</p><p>$$Output_i = \\sum_{k=0}^{m}kernel_k \\star input_k + bias_i, \\space \\space \\space \\space i=1,2,…,n$$</p><p>我来解释一下为什么要加bias。就跟回归方程一样，如果不加bias的话，回归方程为y=wx不管w如何变化，回归方程都必须经过原点。如果加上bias的话，回归方程变为y=wx+b，这样就不是必须经过原点，可以变化的更加多样。</p><p>好啦，卷积计算方式的讲解到这里就告一段落了。下面我们看看在卷积层中有关卷积计算的另外一个重要参数。</p><h3>Padding</h3><p>让我们回到开头的例子，可以发现，输入的尺寸是4x4，输出的尺寸是3x3。你有没有发现，输出的特征图变小了？没错，在有多层卷积层的神经网络中，特征图会越来越小。</p><p>但是，有的时候我们为了让特征图变得不是那么小，可以对特征图进行补零操作。这样做主要有两个目的：</p><p>1.有的时候需要输入与输出的特征图保持一样的大小；<br>\n2.让输入的特征保留更多的信息。</p><p>这里我举个例子，带你看看，一般什么情况下会希望特征图变得不那么小。</p><p>通过刚才的讲解我们知道，如果不补零且步长（stride）为1的情况下，当有多层卷积层时，特征图会一点点变小。如果我们希望有更多层卷积层来提取更加丰富的信息时，就可以让特征图变小的速度稍微慢一些，这个时候就可以考虑补零。</p><p>这个补零的操作就叫做padding，padding等于1就是补一圈的零，等于2就是补两圈的零，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/99/08/99dc22a96df665e93e881yy3cf358d08.jpg?wh=1520x792\" alt=\"图片\"></p><p>在Pytorch中，padding这个参数可以是字符串、int和tuple。</p><p>我们分别来看看不同参数类型怎么使用：当为字符串时只能取$^{\\prime}valid^{\\prime}$与$^{\\prime}same^{\\prime}$。当给定整型时，则是说要在特征图外边补多少圈0。如果是tuple的时候，则是表示在特征图的行与列分别指定补多少零。</p><p>我们重点看一下字符串的形式，相比于直接给定补多少零来说，我认为字符串更加常用。其中，$^{\\prime}valid^{\\prime}$就是没有padding操作，就像开头的例子那样。$^{\\prime}same^{\\prime}$则是让输出的特征图与输入的特征图获得相同的大小。</p><p>那当padding为same时，到底是怎么计算的呢？我们继续用开篇的例子说明，现在padding为$^{\\prime}same^{\\prime}$了。</p><p><img src=\"https://static001.geekbang.org/resource/image/yy/9c/yy51a5c4a35ffa8a06e7d7415aba339c.jpg?wh=1417x736\" alt=\"图片\"></p><p>当滑动到特征图最右侧时，发现输出的特征图的宽与输入的特征图的宽不一致，它会自动补零，直到输出特征图的宽与输入特征图的宽一致为止。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/52/c7/52f8d0ba39b49e9a2736c1a0afb38cc7.jpg?wh=1463x720\" alt=\"图片\"></p><p>高的计算和宽的计算同理，当计算到特征图的底部时，发现输出特征图的高与输入特征图的高不一致时，它同样会自动补零，直到输入和输出一致为止，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/81/c48614da6c7bbcd5abdaf942ea45b481.jpg?wh=1476x735\" alt=\"图片\"></p><p>完成上述操作，我们就可以获得与输入特征图有相同高、宽的输出特征图了。理论讲完了，我们还是要学以致用，在实践中深入体会。在下面的练习中，我们会实际考察一下当padding为same时，是否像我们说的这样计算。</p><h2>PyTorch中的卷积</h2><p>卷积操作定义在torch.nn模块中，torch.nn模块为我们提供了很多构建网络的基础层与方法。</p><p>在torch.nn模块中，关于今天介绍的卷积操作有nn.Conv1d、nn.Conv2d与nn.Conv3d三个类。</p><p>请注意，我们上述的例子都是按照nn.Conv2d来介绍的，nn.Conv2d也是用的最多的，而nn.Conv1d与nn.Conv3d只是输入特征图的维度有所不一样而已，很少会被用到。</p><p>让我们先看看创建一个nn.Conv2d需要哪些必须的参数：</p><pre><code class=\"language-python\"># Conv2d类\nclass torch.nn.Conv2d(in_channels,&nbsp;\n                      out_channels,&nbsp;\n                      kernel_size,&nbsp;\n                      stride=1,&nbsp;\n                      padding=0,&nbsp;\n                      dilation=1,&nbsp;\n                      groups=1,&nbsp;\n                      bias=True,&nbsp;\n                      padding_mode='zeros',&nbsp;\n                      device=None,&nbsp;\n                      dtype=None)\n\n</code></pre><p>我们挨个说说这些参数。首先是跟通道相关的两个参数：in_channels是指输入特征图的通道数，数据类型为int，在标准卷积的讲解中in_channels为m；out_channels是输出特征图的通道数，数据类型为int，在标准卷积的讲解中out_channels为n。</p><p>kernel_size是卷积核的大小，数据类型为int或tuple，需要注意的是只给定卷积核的高与宽即可，在标准卷积的讲解中kernel_size为k。</p><p>stride为滑动的步长，数据类型为int或tuple，默认是1，在前面的例子中步长都为1。</p><p>padding为补零的方式，注意<strong>当padding为’valid’或’same’时，stride必须为1</strong>。</p><p>对于kernel_size、stride、padding都可以是tuple类型，当为tuple类型时，第一个维度用于height的信息，第二个维度时用于width的信息。</p><p>bias是否使用偏移项。</p><p>还有两个参数：dilation与groups，具体内容下节课我们继续展开讲解，你先有个印象就行。</p><h3>验证same方式</h3><p>接下来，我们做一个练习，验证padding为same时，计算方式是否像我们所说的那样。过程并不复杂，一共三步，分别是创建输入特征图、设置卷积以及输出结果。</p><p>先来看第一步，我们创建好例子中的（4，4，1）大小的输入特征图，代码如下：</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\ninput_feat = torch.tensor([[4, 1, 7, 5], [4, 4, 2, 5], [7, 7, 2, 4], [1, 0, 2, 4]], dtype=torch.float32)\nprint(input_feat)\nprint(input_feat.shape)\n\n# 输出：\ntensor([[4., 1., 7., 5.],\n        [4., 4., 2., 5.],\n        [7., 7., 2., 4.],\n        [1., 0., 2., 4.]])\ntorch.Size([4, 4])\n</code></pre><p>第二步，创建一个2x2的卷积，根据刚才的介绍，输入的通道数为1，输出的通道数为1，padding为’same’，所以卷积定义为：</p><pre><code class=\"language-python\">conv2d = nn.Conv2d(1, 1, (2, 2), stride=1, padding='same', bias=True)\n# 默认情况随机初始化参数\nprint(conv2d.weight)\nprint(conv2d.bias)\n# 输出：\nParameter containing:\ntensor([[[[ 0.3235, -0.1593],\n          [ 0.2548, -0.1363]]]], requires_grad=True)\nParameter containing:\ntensor([0.4890], requires_grad=True)\n</code></pre><p>需要注意的是，默认情况下是随机初始化的。一般情况下，我们不会人工强行干预卷积核的初始化，但是为了验证今天的例子，我们对卷积核的参数进行干预。请注意下面代码中卷积核的注释，代码如下：</p><pre><code class=\"language-python\">conv2d = nn.Conv2d(1, 1, (2, 2), stride=1, padding='same', bias=False)\n# 卷积核要有四个维度(输入通道数，输出通道数，高，宽)\nkernels = torch.tensor([[[[1, 0], [2, 1]]]], dtype=torch.float32)\nconv2d.weight = nn.Parameter(kernels, requires_grad=False)\nprint(conv2d.weight)\nprint(conv2d.bias)\n# 输出：\nParameter containing:\ntensor([[[[1., 0.],\n          [2., 1.]]]])\nNone\n</code></pre><p>完成之后就进入了第三步，现在我们已经准备好例子中的输入数据与卷积数据了，下面只需要计算一下，然后输出就可以了，代码如下：</p><pre><code class=\"language-python\">output = conv2d(input_feat)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/var/folders/pz/z8t8232j1v17y01bkhyrl01w0000gn/T/ipykernel_29592/2273564149.py in &lt;module&gt;\n----&gt; 1 output = conv2d(input_feat)\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1051             return forward_call(*input, **kwargs)\n   1052         # Do not call functions when jit is used\n   1053         full_backward_hooks, non_full_backward_hooks = [], []\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py in forward(self, input)\n    441 \n    442     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 443         return self._conv_forward(input, self.weight, self.bias)\n    444 \n    445 class Conv3d(_ConvNd):\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)\n    437                             weight, bias, self.stride,\n    438                             _pair(0), self.dilation, self.groups)\n--&gt; 439         return F.conv2d(input, weight, bias, self.stride,\n    440                         self.padding, self.dilation, self.groups)\n    441 \nRuntimeError: Expected 4-dimensional input for 4-dimensional weight[1, 1, 2, 2], but got 2-dimensional input of size [4, 4] instead\n</code></pre><p>结合上面代码，你会发现这里报错了，提示信息是输入的特征图需要是一个4维的，而我们的输入特征图是一个4x4的2维特征图。这是为什么呢？<br>\n请你记住，<strong>Pytorch输入tensor的维度信息是(batch_size, 通道数，高，宽)</strong>，但是在我们的例子中只给定了高与宽，没有给定batch_size（在训练时，不会将所有数据一次性加载进来训练，而是以多个批次进行读取的，每次读取的量成为batch_size）与通道数。所以，我们要回到第一步将输入的tensor改为(1,1,4,4)的形式。</p><p>你还记得我在之前的讲解中提到过怎么对数组添加维度吗？</p><p>在Pytorch中unsqueeze()对tensor的维度进行修改。代码如下：</p><pre><code class=\"language-python\">input_feat = torch.tensor([[4, 1, 7, 5], [4, 4, 2, 5], [7, 7, 2, 4], [1, 0, 2, 4]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nprint(input_feat)\nprint(input_feat.shape)\n# 输出：\ntensor([[[[4., 1., 7., 5.],\n          [4., 4., 2., 5.],\n          [7., 7., 2., 4.],\n          [1., 0., 2., 4.]]]])\ntorch.Size([1, 1, 4, 4])\n</code></pre><p>这里，unsqueeze()中的参数是指在哪个位置添加维度。<br>\n好，做完了修改，我们再次执行代码。</p><pre><code class=\"language-python\">output = conv2d(input_feat)\n输出：\ntensor([[[[16., 11., 16., 15.],\n          [25., 20., 10., 13.],\n          [ 9.,  9., 10., 12.],\n          [ 1.,  0.,  2.,  4.]]]])\n          \n</code></pre><p>你可以看看，跟我们在例子中推导的结果一不一样？</p><h2>总结</h2><p>恭喜你完成了今天的学习。今天所讲的卷积非常重要，它是各种计算机视觉应用的基础，例如图像分类、目标检测、图像分割等。</p><p>卷积的计算方式是你需要关注的重点。具体过程如下图所示，输出特征图的通道数由<strong>卷积核的个数决定</strong>的，下图中因为有n个卷积核，所以输出特征图的通道数为n。<strong>输入特征图有m个通道，所以每个卷积核里要也要有m个通道</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080\" alt=\"\"></p><p>其实卷积背后的理论比较复杂，但在PyTorch中实现却很简单。在卷积计算中涉及的几大要素：输入通道数、输出通道数、步长、padding、卷积核的大小，分别对应的就是PyTorch中nn.Conv2d的关键参数。所以，就像前面讲的那样，我们要熟练用好nn.Conv2d()。</p><p>之后，我还带你做了一个验证same方式的练习，动手跑跑代码会帮你形成直观印象，快速掌握这部分内容。</p><p>当然，对于卷积来说不光光有今天介绍的这种比较标准的卷积，还有各种变形。例如，今天没有讲到的dilation参数与groups参数，基于这两个参数实现的卷积操作，我会在下一节课中为展开，敬请期待。</p><h2>每课一练</h2><p>请你想一想，padding为’same’时，stride可以为1以外的数值吗？</p><p>欢迎你在留言区记录你的疑问或收获，也推荐你把这节课分享给更多朋友、同事。</p><p>我是方远，我们下节课见！</p>","neighbors":{"left":{"article_title":"08 | Torchvision（下）：其他有趣的功能","id":431420},"right":{"article_title":"10 | 卷积（下）：如何用卷积为计算机“开天眼”？","id":433801}}},{"article_id":433801,"article_title":"10 | 卷积（下）：如何用卷积为计算机“开天眼”？","article_content":"<p>你好，我是方远。</p><p>经过上一节课的学习，相信你已经对标准的卷积计算有所了解。虽然标准卷积基本上可以作为主力Carry全场，但是人们还是基于标准卷积，提出了一些其它的卷积方式，这些卷积方式在应对不同问题时能够发挥不同的作用，这里我为你列举了一些。</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/4b/8a3f6bba138f36c1b31e52aeb3e2604b.jpg?wh=1920x977\" alt=\"图片\"></p><p>在上一节课中，我们学习了conv2d的in_channels、out_channels、kernel_size、stride、padding与bias参数。</p><p>其中，PyTorch中conv2d中剩余的两个参数，它们分别对应着两种不同的卷积，分别是深度可分离卷积和空洞卷积，让我们一起来看看。</p><h2>深度可分离卷积（Depthwise Separable Convolution）</h2><p>我们首先看看依托groups参数实现的深度可分离卷积。</p><p>随着深度学习技术的不断发展，许多很深、很宽的网络模型被提出，例如，VGG、ResNet、SENet、DenseNet等，这些网络利用其复杂的结构，可以更加精确地提取出有用的信息。同时也伴随着硬件算力的不断增强，可以将这些复杂的模型直接部署在服务器端，在工业中可以落地的项目中都取得了非常优秀的效果。</p><p>但这些模型具有一个通病，就是速度较慢、参数量大，这两个问题使得这些模型无法被直接部署到移动终端上。而移动端的各种应用无疑是当今最火热的一个市场，这种情况下这些深而宽的复杂网络模型就不适用了。</p><!-- [[[read_end]]] --><p>因此，很多研究将目光投入到寻求更加轻量化的模型当中，这些轻量化模型的要求是速度快、体积小，精度上允许比服务器端的模型稍微降低一些。</p><p>深度可分离卷积就是谷歌在MobileNet v1中提出的一种轻量化卷积。<strong>简单来说，深度可分离卷积就是我们刚才所说的在效果近似相同的情况下，需要的计算量更少</strong>。接下来，我们先来看看深度可分离卷积是如何计算的，然后再对比一下计算量到底减少了多少。</p><p>深度可分离卷积（Depthwise Separable Convolution）由 <strong>Depthwise</strong>（DW）和 <strong>Pointwise</strong>（PW）这两部分卷积组合而成的。</p><p>我们先来复习一下标准卷积，然后再来讲解一下获得同样输出特征图的深度可分离卷积是如何工作的。</p><p>你还记得下面这张图吗？这是我们<a href=\"https://shimowendang.com/docs/SU6kWF8uTdkYIL7q\">上节课</a>讲到的标准卷积计算方式，它描述的是：输入m个尺寸为h, w的特征图，通过卷积计算获得n个通道尺寸为$h^{\\prime}$与$w^{\\prime}$的特征图的计算过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080\" alt=\"\"></p><p>我们将特征图与一个卷积核计算的过程展开一下，请看下图。一个卷积核中的m个卷积分别与输入特征图的m个通道数据进行卷积计算，生成一个中间结果，然后m个中间结果按位求和，最终就能获得n个输出特征图中的一个特征图。<br>\n<img src=\"https://static001.geekbang.org/resource/image/6a/3d/6abd2bd18e767b7f0f01cab2fdc7023d.jpeg?wh=1920x1080\" alt=\"\"></p><h3>Depthwise（DW）卷积</h3><p>那什么是DW卷积呢？DW卷积就是有m个卷积核的卷积，每个卷积核中的通道数为1，这m个卷积核分别与输入特征图对应的通道数据做卷积运算，所以DW卷积的输出是有<strong>m</strong>个通道的特征图。通常来说，DW卷积核的大小是3x3的。</p><p>DW卷积的过程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/e9/6baeb5b36cea5c04555ea6186de577e9.jpeg?wh=1920x1080\" alt=\"\"></p><h3>Pointwise（PW）卷积</h3><p>通常来说，深度可分离卷积的目标是轻量化标准卷积计算的，所以它是可以来替换标准卷积的，这也意味着原卷积计算的输出尺寸是什么样，替换后的输出尺寸也要保持一致。</p><p>所以，在深度可分离卷积中，我们最终要获得一个具有n个通道的输出特征图，而刚才介绍的DW卷积显然没有达到，并且DW卷积也忽略了输入特征图通道之间的信息。</p><p>所以，在DW之后我们还要加一个PW卷积。PW 卷积也叫做逐点卷积。PW卷积的主要作用就是将DW输出的m个特征图结合在一起考虑，再输出一个具有n个通道的特征图。</p><p>在卷积神经网络中，我们经常可以看到使用1x1的卷积，1x1的卷积主要作用就是升维与降维。所以，在DW的输出之后的PW卷积，就是n个卷积核的1x1的卷积，每个卷积核中有m个通道的卷积数据。</p><p>为了帮你理解刚才我描述的这个过程，我还是用图解的方式为你描述一下，你可以对照下图看一看：</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/17/7e2c1a874d7b467bc96d4243813f3017.jpeg?wh=1920x1080\" alt=\"\"></p><p>经过这样的DW与PW的组合，我们就可以获得一个与标准卷积有同样输出尺寸的轻量化卷积啦。既然是轻量化，那么我们下面就来看看，深度可分离卷积的计算量相对于标准卷积减少了多少呢？</p><h3>计算量</h3><p>我们的原问题是有m个通道的输入特征图，卷积核尺寸为kxk，输出特征图的尺寸为$(n, h^{\\prime}, w^{\\prime})$，那么标准的卷积的计算量为：</p><p>$$k \\times k \\times m \\times n \\times h^{\\prime} \\times w^{\\prime}$$</p><p>我们是怎么得出这个结果的呢？你可以从输出特征图往回思考。</p><p><img src=\"https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080\" alt=\"\"></p><p>上图输出特征图中每个点的数值是由n个卷积核与输入特征图计算出来的吧，这个计算量是$k \\times k \\times m \\times n$，那输出特征图有多少个点？没错，一共有$h^{\\prime} \\times w^{\\prime}$个。所以，我们自然就得出上面的计算方式了。</p><p>如果采用深度可分离卷积，DW的计算量为：$k \\times k \\times m \\times h^{\\prime} \\times w^{\\prime}$，而PW的计算量为：$1 \\times 1 \\times m \\times n \\times h^{\\prime} \\times w^{\\prime}$。</p><p>我们不难得出标准卷积与深度可分离卷积计算量的比值为：</p><p>$$\\frac {k \\times k \\times m \\times h^{\\prime} \\times w^{\\prime} + 1 \\times 1 \\times m \\times n \\times h^{\\prime} \\times w^{'}}{k \\times k \\times m \\times n \\times h^{\\prime} \\times w^{\\prime}}$$</p><p>$$= \\frac {1}{n} + \\frac {1}{k \\times k}$$</p><p>所以，深度可分离卷积的计算量大约为普通卷积计算量的$\\frac {1}{k^2}$。</p><p>那深度可分离卷积落实到PyTorch中是怎么实现的呢？</p><h3>PyTorch中的实现</h3><p>在PyTorch中实现深度可分离卷积的话，我们需要分别实现DW与PW两个卷积。我们先看看DW卷积，实现DW卷积的话，就会用到nn.Conv2d中的groups参数。groups参数的作用就是控制输入特征图与输出特征图的分组情况。</p><p>当groups等于1的时候，就是我们上一节课讲的标准卷积，而groups=1也是nn.Conv2d的默认值。</p><p>当groups不等于1的时候，会将输入特征图分成groups个组，每个组都有自己对应的卷积核，然后分组卷积，获得的输出特征图也是有groups个分组的。需要注意的是，<strong>groups不为1的时候，groups必须能整除in_channels和out_channels</strong>。</p><p>当groups等于in_channels时，就是我们的DW卷积啦。</p><p>好，下面我们一起动手操作一下，看看如何实现一个DW卷积。首先我们来生成一个三通道的5x5输入特征图，然后经过深度可分离卷积，输出一个4通道的特征图。</p><p>DW卷积的实现代码如下：</p><pre><code class=\"language-plain\">import torch\nimport torch.nn as nn\n\n# 生成一个三通道的5x5特征图\nx = torch.rand((3, 5, 5)).unsqueeze(0)\nprint(x.shape)\n# 输出：\ntorch.Size([1, 3, 5, 5])\n# 请注意DW中，输入特征通道数与输出通道数是一样的\nin_channels_dw = x.shape[1]\nout_channels_dw = x.shape[1]\n# 一般来讲DW卷积的kernel size为3\nkernel_size = 3\nstride = 1\n# DW卷积groups参数与输入通道数一样\ndw = nn.Conv2d(in_channels_dw, out_channels_dw, kernel_size, stride, groups=in_channels_dw)\n</code></pre><p>你需要注意以下几点内容：</p><p>1.DW中，输入特征通道数与输出通道数是一样的；<br>\n2.一般来讲，DW的卷积核为3x3；<br>\n3.DW卷积的groups参数与输出通道数是一样的。</p><p>好啦，DW如何实现我们已经写好了，接下来就是PW卷积的实现。其实PW卷积的实现就是我们上一节课介绍的标准卷积，只不过卷积核为1x1。需要注意的是，PW卷积的groups就是默认值了。</p><p>具体代码如下所示：</p><pre><code class=\"language-plain\">in_channels_pw = out_channels_dw\nout_channels_pw = 4\nkernel_size_pw = 1\npw = nn.Conv2d(in_channels_pw, out_channels_pw, kernel_size_pw, stride)\nout = pw(dw(x))\nprint(out.shape)\n</code></pre><p>好了，groups以及深度可分离卷积就讲完了，接下来我们看看最后一个dilation参数，它是用来实现空洞卷积的。</p><h2>空洞卷积</h2><p>空洞卷积经常用于图像分割任务当中。图像分割任务的目的是要做到pixel-wise的输出，也就是说，对于图片中的每一个像素点，模型都要进行预测。</p><p>对于一个图像分割模型，通常会采用多层卷积来提取特征的，随着层数的不断加深，感受野也越来越大。这里有个新名词——“感受野”，这个我稍后再解释。我们先把空洞卷积的作用说完。</p><p>但是对于图像分割模型有个问题，经过多层的卷积与pooling操作之后，特征图会变小。为了做到每个像素点都有预测输出，我们需要对较小的特征图进行上采样或反卷积，将特征图扩大到一定尺度，然后再进行预测。</p><p>要知道，从一个较小的特征图恢复到一个较大的特征图，这显然会带来一定的信息损失，特别是较小的物体，这种损失是很难恢复的。那问题来了，能不能既保证有比较大的感受野，同时又不用缩小特征图呢？</p><p>估计你已经猜到了，空洞卷积就是解决这个问题的杀手锏，它最大的优点就是不需要缩小特征图，也可以获得更大的感受野。</p><h3>感受野</h3><p>现在让我来解释一下什么是感受野。感受野是计算机视觉领域中经常会看到的一个概念。</p><p>因为伴随着不断的pooling（这是卷积神经网络中的一种操作，通常是在一定区域的特征图内取最大值或平均值，用最大值或平均值代替这个区域的所有数据，pooling操作会使特征图变小）或者卷积操作，在卷积神经网络中不同层的特征图是越来越小的。</p><p>这就意味着在卷积神经网络中，相对于原图来说，不同层的特征图，其计算区域是不一样的，这个区域就是感受野。感受野越大，代表着包含的信息更加全面、语义信息更加抽象，而感受野越小，则代表着包含更加细节的语义信息。</p><p>光说理论不容易理解，我们还是结合例子看一看。请看下图，原图是5x5的图像，第一层卷积层为3x3，这时它输出的感受野就是3，因为输出的特征图中每个值都是由原图中3x3个区域计算而来的。</p><p><img src=\"https://static001.geekbang.org/resource/image/ed/f1/edb2bd243872364fe032yy7fb7999bf1.jpg?wh=1920x886\" alt=\"图片\"></p><p>再看下图，卷积层2也为3x3的卷积，输出为2x2的特征图。这时卷积层2的感受野就会变为5（输入特征图中蓝色加橘黄色部分）。</p><p><img src=\"https://static001.geekbang.org/resource/image/2f/yy/2f0e685e8894db8dd8919dee8400e6yy.jpg?wh=1920x647\" alt=\"图片\"></p><p>配合图解，我相信你很容易就能明白感受野的含义了。</p><h3>计算方式</h3><p>好，那么我们再来看看空洞卷积具体是如何计算的。</p><p>用语言来描述空洞卷积的计算方式比较抽象，我们不妨看一下它的动态示意图（这个<a href=\"https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\">GitHub</a>中有各种卷积计算的动态图，非常直观，我们借助它来学习一下空洞卷积）。</p><p>首先，我们先来看看上节课讲的标准卷积是如何计算的。</p><p><img src=\"https://static001.geekbang.org/resource/image/77/63/77643b049ac3cd241980b151e0f32063.gif?wh=395x449\" alt=\"图片\"></p><p>对照上图，下面的蓝图为输入特征图，滑动的阴影为卷积核，绿色的为输出特征图。</p><p>然后我们再对照一下的空洞卷积示意图。</p><p><img src=\"https://static001.geekbang.org/resource/image/49/53/4959201e816888c6648f2e78cccfd253.gif?wh=395x381\" alt=\"图片\"></p><p>结合示意图我们会发现，计算方式与普通卷积一样，只不过是将卷积核以一定比例拆分开来。实现起来呢，就是用0来充填卷积核。</p><p>这个分开的比例，我们一般称之为扩张率，就是Conv2d中的dilation参数。</p><p>dilation参数默认为1，同样也是可以为int或者tuple。当为tuple时，第一位代表行的信息，第二位代表列的信息。</p><h2>总结</h2><p>恭喜你，完成了今天的学习。今天我们在实现PyTorch卷积操作的同时，学习了两个特殊的卷积，深度可分离卷积与空洞卷积。</p><p>对于空洞卷积，你最需要掌握的是感受野这个概念，以及空洞卷积的计算方式。感受野就是能在原始图像中反应的区域。</p><p>深度可分离卷积主要用于轻量化的模型，而空洞卷积主要用于图像分割任务中。</p><p>这里分享一下我的经验：如果说你需要轻量化你的模型，让你的模型变得更小、更快，你可以考虑将卷积层替换为深度可分离卷积。如果你在做图像分割项目的话，可以考虑将网络靠后的层替换为空洞卷积，看看效果是否能有所提高。</p><p>最后，我们再来总结一下PyTorch中卷积操作的各个重要参数，我用表格的方式帮你做了总结归纳，你可以把它作为自己的工具包，时常翻看。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/bf/c4e0f1fbf77dd63ccb2ab36ce9e0fdbf.jpg?wh=1920x1002\" alt=\"图片\"></p><h2>每课一练</h2><p>随机生成一个3通道的128x128的特征图，然后创建一个有10个卷积核且卷积核尺寸为3x3（DW卷积）的深度可分离卷积，对输入数据进行卷积计算。</p><p>欢迎你在留言区跟我交流互动，也推荐你把今天的内容分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"09 | 卷积（上）：如何用卷积为计算机“开天眼”？","id":432042},"right":{"article_title":"11 | 损失函数：如何帮助模型学会“自省”？","id":435553}}},{"article_id":435553,"article_title":"11 | 损失函数：如何帮助模型学会“自省”？","article_content":"<p>你好，我是方远。</p><p>在前面的课程中，我们一同拿下了深度学习实战所需的预备基础知识，包括PyTorch的基础操作、NumPy、Tensor的特性跟使用方法等，还一起学习了基于Torchvision的数据相关操作与特性。恭喜你走到这里，基础打好以后，我们距离实战关卡又进了一步。</p><p>有了基础预备知识，我们就要开始学习深度学习的几个重要的概念了。</p><p>一个深度学习项目包括了模型的设计、损失函数的设计、梯度更新的方法、模型的保存与加载、模型的训练过程等几个主要模块。每个模块都在整个深度学习项目搭建中意义重大，我特意为你画了一个示意图，方便你整体把握它们的功能。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/fc/d76e19dd8d8a5a1bfdb4f4b1a17078fc.jpg?wh=1896x910\" alt=\"图片\" title=\"深度学习项目核心模块\"></p><p>这节课咱们先从损失函数开始说起。损失函数是一把衡量模型学习效果的尺子，甚至可以说，训练模型的过程，实际就是优化损失函数的过程。如果你去面试机器学习岗位，常常会被问到前向传播、神经网络等内容，其实这些知识的考察都不可避免地会涉及到损失函数的相关概念。</p><p>今天，我就从识别劳斯莱斯这个例子，带你了解损失函数的工作原理和常见类型。</p><h2>一个简单的例子</h2><p>回想一下我们学习新知识的大致过程，比如现在让你背一个单词，我举一个夸张的例子：</p><p><strong>Pneumonoultramicroscopicsilicovolcanoconiosis（矽肺病）。</strong></p><!-- [[[read_end]]] --><p>为了背会这个单词，你要反复地去看去记，第一次可能记住了开头的几个字母，第二次又记住了中间的几个字母，第三次又记住了结尾的几个字母，然后不断地反复学习，才能掌握这个单词的准确组成。为了检验你的学习成果，老师还会让你默写单词，跟标准拼写进行对照。</p><p>刚才的例子用的是自然语言，那么如果视觉问题呢？比如我现在给你一个劳斯莱斯汽车的照片，让你记住，这就是这辈子都买不起的劳斯莱斯。</p><p><img src=\"https://static001.geekbang.org/resource/image/6c/19/6cf904ede85961a462c90cb555545e19.png?wh=1080x802\" alt=\"图片\"></p><p>你会怎么去记住它呢？对，你会下意识去寻找最具有代表性的内容，比如车前脸的方形格栅、车前面的立起来的小金人，方方正正的车体等。</p><p>等你以后见到了有了具有以上特征的汽车，你就知道，它是你要躲远点的劳斯莱斯了。不过呢，如果这些特征发生了变化，你又要犹豫或者怀疑它是不是别的品牌的汽车了。</p><p>其实，模型的学习也是一样的，模型最开始的时候就是一张白纸，它什么都不知道。我们作为研发人员，就要不断地给模型提供要学习的数据。</p><p>模型拿到数据之后就要有一个非常重要的环节：把模型自己的判断结果和数据真实的情况做比较。如果偏差或者差异特别大，那么模型就要去纠正自己的判断，用某种方式去减少这种偏差，然后反复这个过程，直到最后模型能够对数据进行正确的判断。</p><p>衡量这种偏差的方式很重要，也是模型学习进步的关键所在。这种减少偏差的过程，我们称之为<strong>拟合</strong>。接下来我们一同看看拟合的几种情况。</p><h2>过拟合与欠拟合</h2><p>我们先来学习第一组概念，也就是过拟合和欠拟合。为了方便你理解，我们结合函数曲线的例子来看看。</p><p>首先假设在一个二维坐标系中有若干个点，我们需要让一个函数（模型）通过学习去尽可能地拟合这些点。那么拟合的结果都有哪几种可能呢？我们看看下面的图片：</p><p><img src=\"https://static001.geekbang.org/resource/image/9e/c3/9e8d67c31fa00051c29a369f4135bdc3.jpg?wh=1068x636\" alt=\"图片\"></p><p>在第一张图中，蓝色的曲线是我们学习到的第一个模型函数（H1）。我们发现，H1好像没有很好地学习到这些点的拟合，或者说，函数跟样本点的拟合效果较差，只有一个大致符合的趋势。这种情况，我们称之为“欠拟合”。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/e2/d70c67323bc970e8d52610e02fdddee2.jpg?wh=1044x640\" alt=\"图片\"></p><p>既然有“欠”就有“过”，我们继续看第二张图。</p><p>在这张图中，红色的曲线是我们学习到的第二个模型函数（H2），在这个结果上，我们看到函数曲线可以很好地拟合所有的点。</p><p>但是，这里存在两个问题：第一，曲线对应的函数有点太过复杂了，不像H1那样简单明了；第二，如果我们在H2的曲线附近再增加一个点，这条H2对应的曲线就很难去拟合好。这种情况就叫做“过拟合”，实在是太过了。</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/e9/d8c507e14c00e5d7c399a7063b3642e9.jpg?wh=1082x626\" alt=\"图片\"></p><p>那么我们再来看第三张图，这张图的曲线就比较靠谱了，这个函数不是太复杂，同时也能较好拟合绝大部分的点。</p><p>看到这里，你可能会有疑惑，为什么我们会如此的在意“复杂”这个问题呢？其实你可以这样想，有这样两个函数：$y1=3x^2 + 2$，$y2=3x^7 + 7x^6 + 6x^2 + 4x+18$。y1无论是从可解释性上，还是在简洁程度、计算量方面，都要比y2好得多。</p><p>越复杂的函数，在实际工作中就需要越多的计算资源和时间消耗。当然了，我们也不能一味的追求简单，否则就会欠拟合。</p><h2>损失函数与代价函数</h2><p>过拟合和欠拟合的概念实际上就是模型的表现效果。接下来，我们再来看看损失函数和代价函数，这组概念就是我们刚才说用来衡量“偏差”、“效果”的方法。</p><p>我们还是延续之前的思路，用函数举例子。假设刚才的二维空间中，任意一个点对应的真实函数为F(x)。我们通过模型的学习拟合出来的函数为f(x)。根据刚才提到的学习过程，我们会知道F(x)和f(x)之间存在一个误差，我们定义为L(x)，于是有：</p><p>$$<br>\nL(x)=(F(x)-f(x))^{2}<br>\n$$</p><p>这里F(x)和f(x)的差距我们做了一个平方和，是为了保证两者的误差是一个正值，方便后续的计算。当然，你也可以做成绝对值的形式，后面课程里我们还会讲到梯度更新，那时你就会发现，平方和要比绝对值更为方便。这里你先有个印象就好，让我们言归正传。</p><p>有了L(x)，我们就有了一个评价拟合函数表现效果“好坏”的度量指标，这个指标函数我们称作<strong>损失函数（loss fuction)</strong>。根据公式可知，损失函数越小，拟合函数对于真实情况的拟合效果就越好。这里有一点需要你注意，损失函数的种类有很多种，L(x)只是我们学习到的第一个损失函数。</p><p>接下来，我们将数据从刚才的任意一个点，扩大到所有的点，那么这些点实际上就是一个训练集合。把集合所有的点对应的拟合误差做平均，就会得到如下公式：</p><p>$$<br>\n\\frac{1}{N} \\sum_{i=0}^{N}(F(x)-f(x))^{2}<br>\n$$</p><p>这个函数叫做<strong>代价函数（cost function）</strong>，即在训练样本集合上，所有样本的拟合误差的平均值。代价函数我们也称作经验风险。</p><p>其实，在实际的应用中，我们并不会严格区分损失函数和代价函数。你只需要知道，损失函数是单个样本点的误差，代价函数是所有样本点的误差。明白了这些，你哪怕混着叫，也没什么问题。</p><h2>常见损失函数</h2><p>在了解了损失函数的定义之后，我们来看一下常用的损失函数都有哪些。</p><p>其实，严格来说，损失函数的种类是无穷多的。这是因为损失函数是用来度量模型拟合效果和真实值之间的差距，而度量方式要根据问题的特点或者需要优化的方面具体定制，所以损失函数的种类是无穷无尽的。</p><p>作为初学者，我推荐你从一些常用的损失函数做开始学习。今天我们一块来看看5种最基本的损失函数。</p><p><strong>0-1损失函数</strong></p><p>假定我们要一个判断类型的问题，比如让模型判断用户输入的文字是不是数字。那么模型判断的结果只有两种：是和不是。</p><p>于是，我们很容易就会想到一个最为简单的评估方式：如果模型预测对了，损失函数的值就为0，因为没有误差；如果模型预测错了，那么损失函数的值就为1。这就是最简单的<strong>0-1损失函数</strong>，这个函数的公式表示如下：</p><p>$$<br>\nL(F(x), f(x)) = \\left\\{\\begin{matrix}<br>\n0 &amp; if F(x) \\ne f(x)\\\\\\<br>\n1 &amp; if F(x) =  f(x)<br>\n\\end{matrix}\\right.<br>\n$$</p><p><img src=\"https://static001.geekbang.org/resource/image/80/67/805e3ce996e95132392643d1b6140a67.jpg?wh=2958x654\" alt=\"\"></p><p>其中，F(x)是输入数据的真实类别，f(x)是模型预测的类别。是不是很简单？</p><p>但是，0-1损失函数的使用频率是非常少的，这是为什么呢？因为模型训练中经常用到的梯度更新和反向传播都需要能够求导的损失函数，可是0-1损失函数的导数值是0（常数的导数为0），所以它应用不多。</p><p>尽管如此，我们也一定要了解0-1损失函数，因为它是最简单的损失函数，有着很重要的意义。</p><p><strong>平方损失函数</strong></p><p>前面讲损失函数的定义时，我们曾举了一个例子$L(x)=(F(x)-f(x))^{2}$，这个函数的正式名称叫做平方损失函数。有时候，我们会在损失函数中加入一个1/2的系数，这是为了求导的时候能够跟平方项的系数约掉。</p><p>平方损失函数是可求导的损失函数中最简单的一种，它直接度量了模型拟合结果和真实结果之间的距离。在实际项目中，很多简单的问题，比如手写分类、花卉识别等，都可以使用这种简单的损失函数。</p><p><strong>均方差损失函数和平均绝对误差损失函数</strong></p><p>在正式讲解均方差损失函数之前，我们先补充一个重要的背景知识：机器学习分为有监督学习和无监督学习两大类。</p><p>其中有监督学习是从标签化训练数据集中，推断出函数的机器学习任务，也就是说：模型通过标注好的数据，就像一个学生（模型）一样，被老师（数据）“指导”和“监督”着去学习。有监督学习问题主要可以划分为两类，分类和回归。其中回归问题是根据数据预测一个数值。</p><p>而均方误差（Mean Squared Error，MSE）是回归问题损失函数中最常用的一个，也称作L2损失函数。它是预测值与目标值之间差值的平方和。它的定义如下：</p><p>$$<br>\nM S E=\\frac{\\sum_{i=1}^{n}\\left(s_{i}-y_{i}^{p}\\right)^{2}}{n}<br>\n$$</p><p>其中s为目标值的向量表示，y为预测值的向量表示。</p><p>细心的你会发现，平方损失函数好像也是差不多一个样子呀？没错，这两种形式本质上是等价的。只是MSE计算得到的值是把整个样本的误差做了平均，也就是加起来之后除了一个n。误差平方和以及均方差的公式中有系数1/2，这是为了求导后，系数被约去。</p><p>而平均绝对误差损失函数（Mean Absolute Error, MAE）是另一种常用于回归问题的损失函数，它的目标是度量真实值和预测值差异的绝对值之和，定义如下：</p><p>$$<br>\nM A E=\\frac{\\sum_{i=1}^{n}\\left|y_{i}-y_{i}^{p}\\right|}{n}<br>\n$$</p><h3>交叉熵损失函数</h3><p>接下来，我们再了解一下交叉熵损失函数。</p><p>熵这个概念有的小伙伴可能有些陌生，跟刚才一样，让我们先来简单了解一下什么是熵。熵最开始是物理学中的一个术语，它表示了一个系统的混乱程度或者说无序程度。如果一个系统越混乱，那么它的熵越大。</p><p>后来，信息论创始人香农把这个概念引申到信道通信的过程中，开创了信息论，所以这里的熵又称为信息熵。信息熵的公式化可以表示为：</p><p>$$<br>\nH§=-\\sum_{i} p\\left(x_{i}\\right) \\log p\\left(x_{i}\\right)<br>\n$$</p><p>其中，x表示随机变量，与之相对应的是所有可能输出的集合。P(x)表示输出概率函数。变量的不确定性越大，熵也就越大，把变量搞清楚所需要的信息量也就越大。</p><p>当我们将函数变为如下格式，将log p改为log q，即：</p><p>$$<br>\n-\\sum_{i=1}^{n} p\\left(x_{i}\\right) \\log \\left(q\\left(x_{i}\\right)\\right)<br>\n$$</p><p>其中，𝑝(𝑥)表示真实概率分布，𝑞(𝑥)表示预测概率分布。这个函数就是交叉熵损失函数（Cross entropy loss）。也就意味着，这个公式同时衡量了真实概率分布和预测概率分布两方面。所以，这个函数实际上就是通过衡量并不断去尝试缩小两个概率分布的误差，使预测的概率分布尽可能达到真实概率分布。</p><h3>softmax损失函数</h3><p>softmax是深度学习中使用非常频繁的一个函数。在某些场景下，一些数值大小范围分布非常广，而为了方便计算，或者使梯度更好的更新（后续我们还会学习梯度更新），我们需要把输入的这些数值映射为0-1之间的实数，并且归一化后能够保证几个数的和为1。</p><p>它的公式化表示为：<br>\n$$<br>\nS_{j}=\\frac{e^{a_{j}}}{\\sum_{k=1}^{T} e^{a_{k}}}<br>\n$$</p><p>回到刚才的交叉熵损失函数，公式中的q(xi)，也就是预测的概率分布，如果我们换成softmax方式的表示，即：</p><p>$$\\sum_{i=1}^{n}p(x_i)log(S_i)$$</p><p>之后我们就得到了一个成为softmax损失函数（softmax loss）的新函数，也称为softmax with cross-entropy loss，它是交叉熵损失函数的一个特例。</p><p>损失函数的种类非常多，这里我选择了最常用的几种。咱们在后续的实战环节，将会遇到更多的损失函数，到时候我再为你详细展开。</p><h2>小结</h2><p>这节课我们一同学习了损失函数的原理。对于模型来说，损失函数就是一个衡量其效果表现的尺子，有了这把尺子，模型就知道了自己在学习过程中是否有偏差，以及偏差到底有多大，从而做到“三省吾身”。</p><p>今天所讲的公式虽然数量不少，但并不需要你背下来。我想提醒你的是，这些公式有必要先过一遍，有了基本的理解，才能知道原理。否则，没有这些公式做基础，后面你根本无法区分不同的损失函数。</p><p>在实际的研发中，损失函数的设定是非常重要的，其地位甚至比得上模型网络设计。因为如果没有好的损失函数做指导的话，一切的功夫都白做了。就比如我们做最简单的手写体识别，损失函数每次计算模型和真实值的区别，通过这个损失函数，我们的模型才能知道自己学对了还是学错了，才能真正的有效学习。</p><p>后面咱们就要开始学习如何通过损失函数来更新模型参数的方法了，这也是非常有意思的一个话题，敬请期待。</p><h2>每课一练</h2><p>损失函数的值越小越好么？</p><p>欢迎你在留言区跟我交流互动，也推荐你把今天的内容分享给更多同事，朋友。</p><p>我是方远，我们下节课见！</p>","neighbors":{"left":{"article_title":"10 | 卷积（下）：如何用卷积为计算机“开天眼”？","id":433801},"right":{"article_title":"12 | 计算梯度：网络的前向与反向传播","id":436564}}},{"article_id":436564,"article_title":"12 | 计算梯度：网络的前向与反向传播","article_content":"<p>你好，我是方远。</p><p>在上节课，我们一同学习了损失函数的概念以及一些常用的损失函数。你还记得我们当时说的么：模型有了损失函数，才能够进行学习。那么问题来了，模型是如何通过损失函数进行学习的呢？</p><p>在接下来的两节课中，我们将会学习前馈网络、导数与链式法则、反向传播、优化方法等内容，掌握了这些内容，我们就可以将模型学习的过程串起来作为一个整体，彻底搞清楚怎样通过损失函数训练模型。</p><p>下面我们先来看看最简单的前馈网络。</p><h2>前馈网络</h2><p>前馈网络，也称为前馈神经网络。顾名思义，是一种“往前走”的神经网络。它是最简单的神经网络，其典型特征是一个单向的多层结构。简化的结构如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/27/2c89c49e8a4d46724113874c2e8d8f27.jpg?wh=1241x790\" alt=\"图片\"></p><p>结合上面的示意图，我带你具体看看前馈网络的结构。这个图中，你会看到最左侧的绿色的一个个神经元，它们相当于第0层，一般适用于接收输入数据的层，所以我们把它们叫做<strong>输入层</strong>。</p><p>比如我们要训练一个y=f(x)函数的神经网络，x作为一个向量，就需要通过这个绿色的输入层进入模型。那么在这个网络中，输入层有5个神经元，这意味着它可以接收一个5维长度的向量。</p><p>结合图解，我们继续往下看，网络的中间有一层红色的神经元，它们相当于模型的“内部”，一般来说对外不可见，或者使用者并不关心的非结果部分，我们称之为<strong>隐藏层</strong>。在实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模型能够学习知识的关键部分。</p><!-- [[[read_end]]] --><p>在图的右侧，蓝色的神经元是网络的最后一层。模型内部计算完成之后，就需要通过这一层输出到外部，所以也叫做<strong>输出层</strong>。</p><p>需要说明的是，神经元之间的连线，表示神经元之间连接的权重，通过权重就会知道网络中每个节点的重要程度。</p><p>那么现在我们回头再来看看前馈神经网络这个名字，是不是就很好理解了。在前馈网络中，数据从输入层进入到隐藏层的第一层，然后传播到第二层，第三层……一直到最后通过输出层输出。数据的传播是单向的，无法后退，只能前行。</p><h2>导数、梯度与链式法则</h2><p>既然有了前向的数据传播，自然也会有反向的数据传播过程。</p><p>说到反向传播，我们常常还会把梯度下降、链式法则这些词挂在嘴边。不过初次接触的话，这些生词你直接搜定义，常常还是一头雾水。其实并不是这些概念很复杂，而是你的学习路径有问题。</p><p>所以，接下来我会带你重温高数学过的导数、偏导数，搞懂这些前置知识，你就能<strong>对反向传播所需的知识做一个回顾，也能更好地理解反向传播的原理</strong>。</p><h3>导数</h3><p>导数，也叫做导函数值。</p><p>还记得高中数学我们曾学习过的斜率么？例如一个函数$F=2x^2$，它的导数F’=4x。其实斜率就是一种特殊情况下的导数。</p><p>更普遍的情况也很容易推导，我们以F=3x为例，在x=3的时候，函数值为3x=3*3=9。现在我们给x一个非常小的增量Δx，那么就有了F(x+Δx)=3(x+Δx)，也就是说函数值也有了一个非常小的增量，我们记为Δy。</p><p>当函数值增量Δy与变量x的增量Δx的比值，在Δx趋近于0时，如果极限a存在，我们就称a为函数F(x)在x处的导数。</p><p><img src=\"https://static001.geekbang.org/resource/image/f1/a7/f1224d68a66eba8503f82b165aa117a7.jpg?wh=1251x979\" alt=\"图片\"></p><p>需要注意的是，<strong>Δx一定要趋近于0，而且极限a是要存在的</strong>。不过在这节课里，极限的定义以及如何去判断极限并非是核心内容，感兴趣的小伙伴有空可以自己查阅相关的内容。</p><p>对照下面的公式，你会对导数的理解更加清晰，高中数学的斜率其实就是一种特殊的导数。导数我们一般采用如下的方式做描述：</p><p>$$<br>\nf^{\\prime}\\left(x_{0}\\right)=\\lim _{\\Delta x \\rightarrow 0} \\frac{\\Delta y}{\\Delta x}=\\lim _{\\Delta x \\rightarrow 0} \\frac{f\\left(x_{0}+\\Delta x\\right)-f\\left(x_{0}\\right)}{\\Delta x}<br>\n$$</p><p>这里面lim就是极限的意思。另外，函数y关于x的导数也可以记为$\\frac{\\partial y}{\\partial x}$。</p><h3>偏导数</h3><p>细心的小伙伴看到这里可能就会有疑问了，有的函数不止一个变量呀，比如z=3x+2y，这个函数中就同时存在了x和y两种变量，那该怎么求它们的导数呢？</p><p>别着急，这时我们就要让偏导数登场了。<strong>偏导数其实就是保持一个变量变化，而所有其他变量恒定不变的求导过程</strong>。</p><p>还是刚才的原理，假设有个函数z=f(x,y)，当我们要求x方向的导数的时候，就可以给x一个非常小的增量Δx，同时保持y不变。反之，如果要求y方向的导数，则需要给y一个非常小的增量Δy，而x保持不变。于是就能得出如下的偏导数描述公式：</p><p>$$<br>\n\\frac{\\partial}{\\partial x_{j}} f\\left(x_{0}, x_{1}, \\ldots, x_{n}\\right)=\\lim _{\\Delta x \\rightarrow 0} \\frac{\\Delta y}{\\Delta x}=\\lim _{\\Delta x \\rightarrow 0} \\frac{f\\left(x_{0}, \\ldots, x_{j}+\\Delta x, \\ldots, x_{n}\\right)-f\\left(x_{0}, \\ldots, x_{j}, \\ldots, x_{n}\\right)}{\\Delta x}<br>\n$$</p><p>上面的公式，看上去很复杂，其实仔细看，你就会发现只有$x_{j}$这个变量有一个小小的Δx，也就是说在x的某一个维度(j)增加了一个小的增量。</p><p>我们举个具体的例子来加深理解。比如对于函数$z=x^{2}+y^{2}$，$\\frac{\\partial z}{\\partial x}=2 x$表示函数z在x上的导数，$\\frac{\\partial z}{\\partial y}=2 y$表示函数z在y上的导数。</p><h3>梯度</h3><p>当我们了解了导数和偏导数的概念之后，那么梯度的概念就会非常容易理解了。函数所有偏导数构成的向量就叫做梯度。是不是非常简单呢？</p><p>我们一般使用$\\nabla f$来表述函数的梯度。它的描述公式为：</p><p>$$<br>\n\\nabla f(x)=\\left[\\frac{\\partial f}{\\partial x_{1}}, \\frac{\\partial f}{\\partial x_{2}}, \\ldots, \\frac{\\partial f}{\\partial x_{i}}\\right]<br>\n$$</p><p>关于梯度，后面这个结论你一定要牢记：<strong>梯度向量的方向即为函数值增长最快的方向</strong>。</p><p>这是一个非常重要的结论，它贯穿了整个深度学习的全过程。模型要学习知识，就要用最快最好的方式来完成，其实就是需要借助梯度来进行。不过，这个结论涉及的证明过程以及数学知识点非常多，这里你只需要记住结论就够了。</p><h3>链式法则</h3><p>深度学习的整个学习过程，其实就是一个更新网络节点之前权重的过程。这个权重就是刚才咱们在前馈网络中示意图中看到的节点之间的连线，权重我们一般使用w来进行表示。</p><p>回忆一下上节课我们提到的损失函数，模型就是<strong>通过不断地减小损失函数值的方式来进行学习</strong>的。让损失函数最小化，通常就要采用<strong>梯度下降</strong>的方式，即：每一次给模型的权重进行更新的时候，都要<strong>按照梯度的反方向</strong>进行。</p><p>为什么呢？因为梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向。</p><p>上面这个自然段的内容非常非常核心，为了确保你学会，我们换个方式再说一次：<strong>模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习</strong>。</p><p>好，我们具体来看一个公式加深理解，假设我们把损失函数表示为：</p><p>其中，Wij表示第i层的第j个节点对应的权重值。则其梯度向量▽H为：</p><p>$$<br>\n\\left[\\frac{\\partial H}{\\partial w_{11}}, \\quad \\frac{\\partial H}{\\partial w_{12}}, \\ldots, \\quad \\frac{\\partial H}{\\partial w_{i j}}, \\ldots, \\quad \\frac{\\partial H}{\\partial w_{m n}}\\right]<br>\n$$</p><p>看到这里，你发现了什么问题？对，感觉这个公式好复杂啊，令人头秃。就比如第一项，w11跟H的关系我哪知道呀，中间隔了那么多层。</p><p>这时候，就需要链式法则隆重登场了：“<strong>两个函数组合起来的复合函数，导数等于里面函数代入外函数值的导数，乘以里面函数之导数</strong>。”这个法则包括了两种形式：</p><p>$$<br>\n\\text { 1. } \\frac{\\mathrm{d} y}{\\mathrm{~d} x}=f^{\\prime}(g(x)) g^{\\prime}(x)<br>\n$$<br>\n$$<br>\n\\text { 2. } \\frac{\\mathrm{d} y}{\\mathrm{~d} x}=\\frac{\\mathrm{d} y}{\\mathrm{~d} u} \\cdot \\frac{\\mathrm{d} u}{\\mathrm{~d} x}<br>\n$$</p><p>可能这时候的你仍旧还很懵，不过没关系，我们通过一个更具体的例子再解释一下，你就知道该如何去计算了。</p><p>假设我们手中有函数$f(x)=\\cos \\left(x^{2}-1\\right)$。我们可以把函数分解为：</p><p>$$<br>\n\\text { 1. } f(x)=\\cos (x)<br>\n$$</p><p>$$<br>\n\\text { 2. } g(x)=x^{2}-1<br>\n$$</p><p>$\\mathrm{g}(\\mathrm{x})$的导数$g^{\\prime}(x)=2 x$，$\\mathrm{f}(\\mathrm{x})$的导数$f^{\\prime}(x)=-\\sin (x)$，则$f^{\\prime}(x)=f^{\\prime}(g(x)) g^{\\prime}(x)=-\\sin \\left(x^{\\wedge} 2-1\\right) 2 x$，相当于各自求导后再相乘。</p><p>说到这，你是不是有点感觉了？这个部分需要你结合公式和我提供的例子仔细看一看，相信你一定可以搞定它。</p><h2>反向传播</h2><p>了解了前面的导数、偏导数、梯度、链式法则，反向传播必备的前置知识我们就搞定了。接下来正式进入反向传播的学习，你会发现前面咱们花的这些功夫都没有白费。</p><p>反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。</p><p>反向传播的主要原理是：</p><ul>\n<li>前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一致。</li>\n<li>计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差<strong>通过某种方式反向传播</strong>，即从输出层向隐藏层传递并最后到达输入层。</li>\n<li>迭代：在反向传播的过程中，<strong>根据误差不断地调整模型的参数值</strong>，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。</li>\n</ul><p>其中最重要的环节有两个：一是通过某种方式反向传播；二是根据误差不断地调整模型的参数值。</p><p>这两个环节，我们统称为<strong>优化方法</strong>，一般而言，多采用梯度下降的方法。这里就要使用到导数、梯度和链式法则相关的知识点，梯度下降我们将在下节课详细展开。</p><p>反向传播的数学推导以及证明过程是非常复杂的，在实际的研发过程中反向传播的过程已经被PyTorch、TensorFlow等深度学习框架进行了完善的封装，所以我们不需要手动去写这个过程。不过作为深度学习的研发人员，你还是需要深入了解这个过程的运转方式，这样才能搞清楚深度学习中模型具体是如何学习的。</p><h2>小结</h2><p>这节课我们一块学习了前馈网络这种最简单的神经网络。</p><p>虽然前馈网络很简单，但是它的思想贯穿了整个深度学习的过程，是非常重要的概念。同时我们又学习了导数、梯度和链式法则，这几个内容是模型做反向传播从而学习知识的最重要知识点，也是深度学习的内在核心内容，你一定要牢牢掌握。</p><p>最后，我们初步了解了反向传播的大致过程和概念，这为我们后面正式学习如何计算反向传播奠定了基础。</p><p>今天的内容里，我尽可能将相关数学知识点进行了简化，保留了最核心的内容。但实际上在深度学习的研究中，涉及的数学知识点非常多，如果感兴趣，你可以在课后查阅更多的相关资料，不断进步。</p><p>下节课，我会带你学习优化函数，学会了优化函数之后，我们就可以正式开始计算反向传播的过程了。</p><h2>每课一练</h2><p>深度学习都是基于反向传播的么？</p><p>欢迎你在留言区跟我交流互动，也推荐你把今天的内容分享给更多同事、朋友。</p><p>我是方远，我们下节课见！</p>","neighbors":{"left":{"article_title":"11 | 损失函数：如何帮助模型学会“自省”？","id":435553},"right":{"article_title":"13 | 优化方法：更新模型参数的方法","id":438639}}},{"article_id":438639,"article_title":"13 | 优化方法：更新模型参数的方法","article_content":"<p>你好，我是方远。</p><p>在上节课中，我们共同了解了前馈网络、导数、梯度、反向传播等概念。但是距离真正完全了解神经网络的学习过程，我们还差一个重要的环节，那就是优化方法。只有搞懂了优化方法，才能做到真的明白反向传播的具体过程。</p><p>今天我们就来学习一下优化方法，为了让你建立更深入的理解，后面我还特意为你准备了一个例子，把这三节课的所有内容串联起来。</p><h2>用下山路线规划理解优化方法</h2><p>深度学习，其实包括了三个最重要的核心过程：模型表示、方法评估、优化方法。我们上节课学习的内容，都是为了优化方法做铺垫。</p><p>优化方法，指的是一个过程，这个过程的目的就是，寻找模型在所有可能性中达到评估效果指标最好的那一个。我们举个例子，对于函数f(x)，它包含了一组参数。</p><p>这个例子中，优化方法的目的就是<strong>找到能够使得f(x)的值达到最小值</strong>对应的权重。换句话说，优化过程就是找到一个状态，这个状态能够让模型的损失函数最小，而这个状态就是<strong>模型的权重</strong>。</p><p>常见的优化方法种类非常多，常见的有梯度下降法、牛顿法、拟牛顿法等，涉及的数学知识也更是不可胜数。同样的，PyTorch也将优化方法进行了封装，我们在实际开发中直接使用即可，节省了大量的时间和劳动。</p><p>不过，为了更好地理解深度学习特别是反向传播的过程，我们还是有必要对一些重要的优化方法进行了解。我们这节课要学习的梯度下降法，也是深度学习中使用最为广泛的优化方法。</p><!-- [[[read_end]]] --><p>梯度下降其实很好理解，我给你举一个生活化的例子。假期你跟朋友去爬山，到了山顶之后忽然想上厕所，需要尽快到达半山腰的卫生间，这时候你就需要规划路线，该怎么规划呢？</p><p>在不考虑生命危险的情况下，那自然是怎么快怎么走了，能跳崖我们绝不走平路，也就是说：越陡峭的地方，就越有可能快速到达目的地。</p><p>所以，我们就有了一个送命方案：每走几步，就改变方向，这个方向就是朝着当前最陡峭的方向，即坡度下降最快的方向行走，并不断重复这个过程。这就是梯度下降的最直观的表示了。</p><p>在上节课中我们曾说过：<strong>梯度向量的方向即为函数值增长最快的方向，梯度的反方向则是函数减小最快的方向</strong>。</p><p>梯度下降，就是梯度在深度学习中最重要的用途了。下面我们用相对严谨的方式来表述梯度下降。</p><p>在一个多维空间中，对于任何一个曲面，我们都能够找到一个跟它相切的超平面。这个超平面上会有无数个方向（想想这是为什么？），但是这所有的方向中，肯定有一个方向是<strong>能够使函数下降最快</strong>的方向，这个方向就是梯度的反方向。每次优化的目标就是沿着这个最快下降的方向进行，就叫做梯度下降。</p><p>具体来说，在一个三维空间曲线中，任何一点我们都能找到一个与之相切的平面（更高维则是超平面），这个平面上就会有无穷多个方向，<strong>但是只有一个使曲线函数下降最快的梯度</strong>。再次叨叨一遍：每次优化就沿着梯度的反方向进行，就叫做梯度下降。使什么函数下降最快呢？答案就是损失函数。</p><p>这下你应该将几个知识点串联起来了吧：<strong>为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值</strong>。这两节课的最终目的，就是让你牢牢记住这句话。</p><p>我们继续回到刚才的例子。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/3b/697af8ec29ae8fd9d54ce07f206de83b.png?wh=1920x986\" alt=\"图片\"></p><p>图中红色的线路，是一个看上去还不错的上厕所的路线。但是我们发现，还有别的路线可选。不过，下山就算是不要命地跑，也得讲究方法。</p><p>就比如，步子大小很重要，太大的话你可能就按照上图中的黄色路线跑了，最后跑到了别的山谷中（函数的局部极小值而非整体最小值）或者在接近和远离卫生间的来回震荡过程中，结果可想而知。但是如果步伐太小了，则需要的时间就很久，可能你还没走到目的地，就坚持不住了（蓝色路线）。</p><p>在算法中，这个步子的大小，叫做学习率（learning rate）。因为步长的原因，理论上我们是不可能精确地走到目的地的，而是最后在最小值的某一个范围内不断地震荡，也会存在一定的误差，不过这个误差是我们可以接受的。</p><p>在实际的开发中，如果损失函数在一段时间内没有什么变化，我们就认为是到达了需要的“最低点”，就可以认为模型已经训练收敛了，从而结束训练。</p><h2>常见的梯度下降方法</h2><p>我们搞清楚了梯度下降的原理之后，下面具体来看几种最常用的梯度下降优化方法。</p><h2>1.批量梯度下降法（Batch Gradient Descent，BGD）</h2><p>线性回归模型是我们最常用的函数模型之一。假设对于一个线性回归模型，y是真实的数据分布函数，$h_\\theta(x) = \\theta_1x_1 + \\theta_2x_2 + … + \\theta_nx_n$是我们通过模型训练得到的函数，其中θ是h的参数，也是我们要求的权值。</p><p>损失函数J(θ)可以表述为如下公式：</p><p>$$<br>\n\\operatorname{cost}=J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{i}\\right)-y^{i}\\right)^{2}<br>\n$$</p><p>在这里，m表示样本数量。既然要想损失函数的值最小，我们就要使用到梯度，还记得我们反复说的“<strong>梯度向量的方向即为函数值增长最快的方向</strong>”么？让损失函数以最快的速度减小，就得用梯度的反方向。</p><p>首先我们对J(θ)中的θ求偏导数，这样就可以得到每个θ对应的梯度：</p><p>$$<br>\n\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}=-\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{i}\\right)-y^{i}\\right) x_{j}^{i}<br>\n$$<br>\n得到了每个θ的梯度之后，我们就可以按照下降的方向去更新每个θ，即：</p><p>$$<br>\n\\theta_{j}^{\\prime}=\\theta_{j}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{i}\\right)-y^{i}\\right) x_{j}^{i}<br>\n$$</p><p>其中α就是我们刚才提到的学习率。更新θ之后，我们就得到了一个更新之后的损失函数，它的值肯定就会更小，那么我们的模型就更加接近于真实的数据分布了。</p><p>在上面的公式中，你注意到了m这个数了吗？没错，这个方法是当所有的数据都经过了计算之后再整体除以它，即把所有样本的误差做平均。这里我想提醒你，在实际的开发中，往往有百万甚至千万数量级的样本，那这个更新的量就很恐怖了。所以就需要另一个办法，随机梯度下降法。</p><h2>2.随机梯度下降（Stochastic Gradient Descent，SGD）</h2><p>随机梯度下降法的特点是，每计算一个样本之后就要更新一次参数，这样参数更新的频率就变高了。其公式如下：</p><p>$$<br>\n\\theta_{j}^{\\prime}=\\theta_{j}-\\alpha\\left(h_{\\theta}\\left(x^{i}\\right)-y^{i}\\right) x_{j}^{i}<br>\n$$</p><p>想想看，每训练一条数据就更新一条参数，会有什么好处呢？对，有的时候，我们只需要训练集中的一部分数据，就可以实现接近于使用全部数据训练的效果，训练速度也大大提升。</p><p>然而，鱼和熊掌不可兼得，SGD虽然快，也会存在一些问题。就比如，训练数据中肯定会存在一些错误样本或者噪声数据，那么在一次用到该数据的迭代中，优化的方向肯定不是朝着最理想的方向前进的，也就会导致训练效果（比如准确率）的下降。最极端的情况下，就会导致模型无法得到全局最优，而是陷入到局部最优。</p><p>世间安得两全法，有的时候舍弃一些东西，我们才能获得想要的。<strong>随机梯度下降方法选择了用损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了最终总体的优化效率的提高</strong>。</p><p>当然这个过程中增加的迭代次数，还是要远远小于样本的数量的。</p><p>那如果想尽可能折衷地去协调速度和效果，该怎么办呢？我们很自然就会想到，每次不用全部的数据，也不只用一条数据，而是用“一些”数据，这就是接下来我们要说的小批量梯度下降。</p><h2>3.小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</h2><p>Mini-batch的方法是目前主流使用最多的一种方式，它每次使用一个固定数量的数据进行优化。</p><p>这个固定数量，我们称它为batch size。batch size较为常见的数量一般是2的n次方，比如32、128、512等，越小的batch size对应的更新速度就越快，反之则越慢，但是更新速度慢就不容易陷入局部最优。</p><p>其实具体的数值设成为多少，也需要根据项目的不同特点，采用经验或不断尝试的方法去进行设置，比如图像任务batch size我们倾向于设置得稍微小一点，NLP任务则可以适当的大一些。</p><p>基于随机梯度下降法，人们又提出了包括momentum、nesterov momentum等方法，这部分知识同学们有兴趣点击<a href=\"https://ruder.io/optimizing-gradient-descent/\">这里</a>可以自行查阅。</p><h2>一个简单的抽象例子</h2><p>我们通过三节课（第11到13节课），分别学习了损失函数、反向传播和优化方法（梯度下降）的概念。这三个概念也是深度学习中最为重要的内容，其核心意义在于能够让模型真正做到不断学习和完善自己的表现。</p><p>那么接下来我们将通过一个简单的抽样例子把三节课的内容汇总起来。需要注意的是，下面的例子不是一个能够运行的例子，而是旨在让我们更加明确一个最基本的PyTorch训练过程都需要哪些步骤，你可以当这是一次军训。有了这个演示例子，以后我们上战场，也就是实现真正可用的例子也会事半功倍。</p><p>在一个模型中，我们要设置如下几个内容：</p><ul>\n<li>模型定义。</li>\n<li>损失函数定义。</li>\n<li>优化器定义。</li>\n</ul><p>通过下面的代码，我们来一块了解一下，上面三个内容在实际开发中应该怎么组合。当然，这个代码是一个抽象版本，目的是帮你快速领会思路。具体的代码填充，还是要根据实际项目来修改。</p><pre><code class=\"language-python\">import LeNet #假定我们使用的模型叫做LeNet，首先导入模型的定义类\nimport torch.optim as optim #引入PyTorch自带的可选优化函数\n...\nnet = LeNet() #声明一个LeNet的实例\ncriterion = nn.CrossEntropyLoss() #声明模型的损失函数，使用的是交叉熵损失函数\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n# 声明优化函数，我们使用的就是之前提到的SGD，优化的参数就是LeNet内部的参数，lr即为之前提到的学习率\n\n#下面开始训练\nfor epoch in range(30): #设置要在全部数据上训练的次数\n  \n&nbsp; &nbsp;&nbsp;for i, data in enumerate(traindata):\n&nbsp; &nbsp; &nbsp; &nbsp; #data就是我们获取的一个batch size大小的数据\n  \n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;inputs, labels = data #分别得到输入的数据及其对应的类别结果\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;#&nbsp;首先要通过zero_grad()函数把梯度清零，不然PyTorch每次计算梯度会累加，不清零的话第二次算的梯度等于第一次加第二次的\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;optimizer.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; # 获得模型的输出结果，也即是当前模型学到的效果\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;outputs = net(inputs)\n&nbsp; &nbsp; &nbsp; &nbsp; # 获得输出结果和数据真正类别的损失函数\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;loss = criterion(outputs, labels)\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;# 算完loss之后进行反向梯度传播，这个过程之后梯度会记录在变量中\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;# 用计算的梯度去做优化\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n...\n</code></pre><p>这个抽象框架是不是非常清晰？我们先设置好模型、损失函数和优化函数。然后针对每一批（batch）数据，求得输出结果，接着计算损失函数值，再把这个值进行反向传播，并利用优化函数进行优化。<br>\n别看这个过程非常简单，但它是深度学习最根本、最关键的过程了，也是我们通过三节课学习到的最核心内容了。</p><h2>总结</h2><p>这节课，我们学习了优化方法以及梯度下降法，并通过一个例子将损失函数、反向传播、梯度下降做了串联。至此，我们就能够在给定一个模型的情况下，训练属于我们自己的深度学习模型了，恭喜你耐心看完。</p><p>当你想不起来梯度下降原理的时候，不妨回顾一下我们下山路线规划的例子。我们的目标就是设置合理的学习率（步伐），尽可能接近咱们的目的地（达到较理想的拟合效果）。用严谨点的表达说，就是正文里咱们反复强调的：<strong>为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/3b/697af8ec29ae8fd9d54ce07f206de83b.png?wh=1920x986\" alt=\"图片\"></p><p>这里我再带你回顾一下这节课的要点：</p><ul>\n<li>模型之所以使用梯度下降，其实是通过优化方法不断的去修正模型和真实数据的拟合差距。</li>\n<li>常用的三种梯度方法包括批量、随机和小批量，一般来说我们更多采用<strong>小批量梯度下降</strong>。</li>\n<li>最后我们通过一个抽象的框架，汇总了训练一个模型所需要的几个关键内容，如损失函数、优化函数等，这部分内容是深度学习最关键的过程，建议你重点关注。</li>\n</ul><h2>每课一练</h2><p>batch size越大越好吗？</p><p>欢迎你在留言区记录你的疑问或收获，也推荐你把这节课分享给更多的同事、朋友。</p><p>我是方远，我们下节课见！</p>","neighbors":{"left":{"article_title":"12 | 计算梯度：网络的前向与反向传播","id":436564},"right":{"article_title":"加餐 | 机器学习其实就那么几件事","id":440393}}},{"article_id":440393,"article_title":"加餐 | 机器学习其实就那么几件事","article_content":"<p>你好，我是方远。</p><p>通过前面的学习，我们知道，PyTorch是作为一种机器学习或深度学习的实现工具出现的，因此学习PyTorch的时候，免不了会碰到一些机器学习中的相关概念和名词。</p><p>在专栏前期调研和上线之后，我收到了不少反馈、留言，希望可以在专栏里介绍一下机器学习的基本知识。</p><p>今天这次加餐，我们就一起来看看什么是机器学习，它是怎么分类的，都有哪些常见名词。在补充了这些基础知识之后，我还会和你聊聊模型训练的本质是什么，你可以把它当作专栏更新过半的期中总结。</p><p>好，让我们正式开始今天的学习。</p><h2>人工智能、机器学习与深度学习</h2><p>说到人工智能、机器学习还有深度学习这三个词，我们虽然很眼熟，但三者的关系总是理不清。</p><p>其实，这三者的关系是一种包含的关系。人工智能包含机器学习，而机器学习又包含深度学习。</p><p><img src=\"https://static001.geekbang.org/resource/image/15/2d/15102e9da587b21792c0a624da0ba32d.jpg?wh=1920x1254\" alt=\"图片\" title=\"人工智能、机器学习与深度学习关系示意图\"></p><p>人工智能的概念其实很早就有了，不过受到技术能力的限制，很少进入到人们的视线当中。当你在网络上搜索人工智能的概念时，可能每一条搜索结果都是用大段文字来解释。归根结底，人工智能的本质就是人们想让计算机像人一样思考，来帮助人们解决一些重复、繁重的工作。</p><p>人工智能的应用主要包括以下这几项：</p><ul>\n<li>专家系统</li>\n<li>自然语言处理</li>\n<li>计算机视觉</li>\n<li>语音系统</li>\n<li>机器人</li>\n<li>其他</li>\n</ul><!-- [[[read_end]]] --><p>其中自然语言处理、计算机视觉与语音系统是现在大热的几个方向，从招聘信息中就可以看出来，例如去检索大厂的计算机视觉工程师、自然语言处理工程师等。这些领域中的问题，本质上都可以用传统的机器学习来解决，但依然是受到技术能力的限制，一直处于瓶颈。</p><p>近十年，随着深度学习的发展，人们在这三个领域的研究中取得了长足的进步。越来越多的人工智能产品得以落地，让我们的生活变得更加便利、快捷。我们专栏所讲的PyTorch也活跃于这些领域当中。</p><h3>机器学习（深度学习）</h3><p>深度学习起源于机器学习中的人工神经网络，所以从工作机制上讲机器学习与深度学习是完全一致的，接下来我们就看看什么是机器学习与深度学习的分类与工作流程。（下文简称机器学习，省略深度学习）。</p><p>正如前文所说，机器学习的目的是让机器能够像人一样思考。那么我们可以先想想看，人类是根据什么来思考问题的呢？很显然，我们思考问题时，通常会根据以往的一些经验对当前的问题作出判断。</p><p>与人类归纳总结经验的过程类似，机器学习的主要目的是<strong>把人类归纳经验的过程，转化为由计算机自己来归纳总结的过程</strong>。</p><p>其中，人类积累的历史经验，在机器学习中体现为大量的数据。</p><p>比如在图像分类的过程中我们给计算机提供了大量的图片，总结归纳这个过程，就是机器学习的训练过程，即计算机处理图片并“学习”其中潜在特征的过程。最终的“规律”，则体现为机器学习中的模型，模型也是我们机器学习任务中最终的一个产出。</p><p>所以说，<strong>机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法</strong>。</p><h3>有监督学习 Vs 无监督学习</h3><p>刚才我们说到，机器学习需要训练出模型。机器学习中的模型基本上可以分为有监督学习与无监督学习两大类，当然，基于这两大分类，下面还有很多小的细分类别，我们先不做讨论。</p><p>这里我们先弄清楚，什么是有监督学习与无监督学习呢？</p><p>有监督学习与无监督学习最明显的区别就是，在训练的时候是否会使用数据真实的标签。为了让你快速理解，这里我结合一个人脸识别的例子来解释一下。</p><p>首先来看有监督学习，我们现在要训练一个人脸识别模型，来自动识别人脸是A还是B。那么，在训练的时候就要给模型看大量标记为A的A照片以及标记为B的B照片，让模型学习谁是A，谁是B。只有经过这样的训练之后，当我们进行预测的时候，模型才能正确判断出这张人脸图片是A还是B。</p><p>再来说说无监督学习，我们手机的相册中有这样的功能，它能自动把某一个人的照片汇聚在一起，但其实手机并不知道汇集到一起的照片是谁。这背后的模型训练原理是怎样的呢？其实训练的时候是把一堆图片给模型看，但是模型并不知道这些图片真实对应的标签，而是模型自己探索这些图片中的潜在特征。</p><p>大多数我们可以体验到的深度学习应用，都属于有监督学习，例如人脸识别、图像分类、手势识别、人像分割、情感分析等。而最近几年特别流行的GAN就属于无监督学习。</p><h2>常见名词讲解</h2><p>我们在专栏中出现了很多专业的术语，在这里我们就一起汇总一下，解释一下都是什么意思。为了不让你觉得这部分像教科书那样照本宣科，所以我决定用一个例子把这些名词给串联起来。</p><p>我们就像开篇所说的那样，<strong>机器学习的本质就是让机器像人一样的思考</strong>，所以，我就用学习这个专栏的过程来解释机器学习中的一些术语。</p><h3>训练集与验证集</h3><p>在训练时使用的数据我们称之为训练集。评估模型时使用的数据称之为评估集、验证集或测试集。</p><p>通过这个专栏的学习，会让你从无到有地掌握有关PyTorch的知识，在专栏结束的时候，我们还设置了期末测试题，用来帮助你衡量一下自己的学习成果。</p><p>那么，这个专栏的内容就相当于<strong>训练集，<strong>测试题就是</strong>验证集</strong>（或称测试集）。训练集是用来训练模型的，而验证集是用来评估模型的。</p><p>在模型训练的时候，要注意训练集与验证集一定是来自同一问题的不同数据。就像专栏学习的是PyTorch，但是后面是Python的测试题，那显然不能反映出你真实的学习成果。</p><h3>Epoch与Step</h3><p>用所有数据训练一遍就是一个<strong>Epoch</strong>，也就是把专栏学习一遍就叫做一个Epoch。</p><p>但受到硬件设备的限制，训练时不会一次性的读入所有数据，而是一次读入一部分进行训练，就像我们每周一、三、五更新一篇内容，然后你相应的去学习一部分内容一样。这里的“每次”就是对应的<strong>Step</strong>这个概念。那每次读入的数据量就是<strong>batch_size</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/1c/dcd68825fee0fc135dfe27a9b528711c.jpg?wh=1920x446\" alt=\"图片\" title=\"对应关系\"></p><h2>模型训练本质</h2><p>刚才我们通过一个例子理顺了不少机器学习的关键名词。其实专栏更新到现在，我们已经讲完了使用PyTorch做模型训练的大部分内容了，恭喜你坚持到这里。</p><p>其实我刚开始接触机器学习的时候，总是被它的那些算法弄得晕头转向，有一个阶段一直是摸不清头脑的迷茫状态。有的算法即使看明白了，我也不知道该如何使用。</p><p>所幸坚持学习了一段时间后，我慢慢发现，机器学习其实就那么几件事，可谓万变不离其宗。接下来让我们一起回顾一下机器学习乃至深度学习开发的几个重要环节。</p><p>首先看看机器学习开发的几个步骤，这我在之前的专栏也有提及，记不清的部分你可以温习回顾。</p><p>1.数据处理：主要包括数据清理、数据预处理、数据增强等。总之，就是构建让模型使用的训练集与验证集。<br>\n2.模型训练：确定网络结构，确定损失函数与设置优化方法。<br>\n3.模型评估：使用各种评估指标来评估模型的好坏。</p><p>你现在可以想想，基本没有项目的开发能离开这三步吧。无论是深度学习中的深度模型还是机器学习中的浅层模型，它们的开发基本都离不开这三步。</p><p>然后，我们再来看看其中的模型训练部分。各种模型纵有千万种变化，但是依然离不开以下几步：</p><p>1.模型结构设计：例如，机器学习中回归算法、SVM等，深度学习中的VGG、ResNet、SENet等各种网络结构，再或者你自己设计的自定义模型结构。<br>\n2.给定损失函数：损失函数衡量的是当前模型预测结果与真实标签之间的差距。<br>\n3.给定优化方法：与损失函数搭配，更新模型中的参数。</p><p>你现在再想想，是不是基本所有模型的训练都离不开这三步呢？其实上面讲的这6点，都来源于咱们前面讲过的内容。学习前面的内容就好比学会如何制造汽车的零部件，将这些零件组装起来就是完成了一辆汽车的完整生产，而这一步是我们后面要继续研究的。</p><p>这里面变化最多的就是模型结构了，这一点除了多读读论文，看看相关博客来扩充知识面之外，没有什么捷径可走。然后呢，我们也不要小瞧了损失函数，不同的损失函数有不同的侧重点，当你模型训练处于瓶颈很难提升，或者解决不了现有问题的话，可以考虑考虑调整一下损失函数。</p><p>在我看来，<strong>模型训练的本质就是确定网络结构、设定损失函数与优化方法</strong>。接下来，我们将一起学习如何将前面学习的各个环节组装起来，完成一个完整的模型训练。</p><p>欢迎你在留言区跟我交流讨论，咱们一起继续加油。</p>","neighbors":{"left":{"article_title":"13 | 优化方法：更新模型参数的方法","id":438639},"right":{"article_title":"14 | 构建网络：一站式实现模型搭建与训练","id":442442}}},{"article_id":442442,"article_title":"14 | 构建网络：一站式实现模型搭建与训练","article_content":"<p>你好，我是方远。</p><p>前面我们花了不少时间，既学习了数据部分的知识，还研究了模型的优化方法、损失函数以及卷积计算。你可能感觉这些知识还有些零零散散，但其实我们不知不觉中，已经拿下了模型训练的必学内容。</p><p>今天这节课，也是一个中期小练习，是我们检验自己学习效果的好时机。我会带你使用PyTorch构建和训练一个自己的模型。</p><p>具体我是这么安排的，首先讲解搭建网络必备的基础模块——nn.Module模块，也就是如何自己构建一个网络，并且训练它，换句话说，就是搞清楚VGG、Inception那些网络是怎么训练出来的。然后我们再看看如何借助Torchvision的模型作为预训练模型，来训练我们自己的模型。</p><h2>构建自己的模型</h2><p>让我们直接切入主题，使用PyTorch，自己构建并训练一个线性回归模型，来拟合出训练集中的走势分布。</p><p>我们先随机生成训练集X与对应的标签Y，具体代码如下：</p><pre><code class=\"language-python\">import numpy as np\nimport random\nfrom matplotlib import pyplot as plt\n\nw = 2\nb = 3\nxlim = [-10, 10]\nx_train = np.random.randint(low=xlim[0], high=xlim[1], size=30)\n\ny_train = [w * x + b + random.randint(0,2) for x in x_train]\n\nplt.plot(x_train, y_train, 'bo')\n</code></pre><!-- [[[read_end]]] --><p>上述代码中生成的数据，整理成散点图以后，如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/f2/11/f2d24e9e7ea5737a78032b686282ca11.jpg?wh=900x621\" alt=\"图片\"></p><p>熟悉回归的同学应该知道，我们的回归模型为：$y = wx+b$。这里的x与y，其实就对应上述代码中的x_train与y_train，而w与b正是我们要学习的参数。</p><p>好，那么我们看看如何构建这个模型。我们还是先看代码，再具体讲解。</p><pre><code class=\"language-python\">import torch\nfrom torch import nn\n\nclass LinearModel(nn.Module):\n&nbsp; def __init__(self):\n&nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; self.weight = nn.Parameter(torch.randn(1))\n&nbsp; &nbsp; self.bias = nn.Parameter(torch.randn(1))\n\n&nbsp; def forward(self, input):\n&nbsp; &nbsp; return (input * self.weight) + self.bias\n</code></pre><p>通过上面这个线性回归模型的例子，我们可以引出构建网络时的重要几个知识点。</p><p>1.<strong>必须继承nn.Module类</strong>。</p><p>2.<strong>重写__init__()方法</strong>。通常来说要把有需要学习的参数的层放到构造函数中，例如，例子中的weight与bias，还有我们之前学习的卷积层。我们在上述的__init__()中使用了nn.Parameter()，它主要的作用就是作为nn.Module中可训练的参数使用。</p><p>3.<strong>forward()是必须重写的方法</strong>。看函数名也可以知道，它是用来定义这个模型是如何计算输出的，也就是前向传播。对应到我们的例子，就是获得最终输出y=weight * x+bias的计算结果。对于一些不需要学习参数的层，一般来说可以放在这里。例如，BN层，激活函数还有Dropout。</p><h3>nn.Module模块</h3><p>nn.Module是所有神经网络模块的基类。当我们自己要设计一个网络结构的时候，就要继承该类。也就说，其实Torchvison中的那些模型，也都是通过继承nn.Module模块来构建网络模型的。</p><p>需要注意的是，<strong>模块本身是callable的，当调用它的时候，就是执行forward函数，也就是前向传播</strong>。</p><p>我们还是结合代码例子直观感受一下。请看下面的代码，先创建一个LinearModel的实例model，然后model(x)就相当于调用LinearModel中的forward方法。</p><pre><code class=\"language-python\">model = LinearModel()\nx = torch.tensor(3)\ny = model(x)\n</code></pre><p>在我们之前的课程里已经讲过，模型是通过前向传播与反向传播来计算梯度，然后更新参数的。我想学到这里，应该没有几个人会愿意去写反向传播和梯度更新之类的代码吧。</p><p>这个时候PyTorch的优点就体现出来了，当你训练时，PyTorch的求导机制会帮你自动完成这些令人头大的计算。</p><p>除了刚才讲过的内容，关于初始化方法__init__，你还需要关注的是，<strong>必须调用父类的构造方法才可以</strong>，也就是这行代码：</p><pre><code class=\"language-python\">super().__init__()\n</code></pre><p>因为在nn.Module的__init__()中，会初始化一些有序的字典与集合。这些集合用来存储一些模型训练过程的中间变量，如果不初始化nn.Module中的这些参数的话，模型就会报下面的错误。</p><pre><code class=\"language-python\">AttributeError: cannot assign parameters before Module.__init__() call\n</code></pre><h3>模型的训练</h3><p>我们的模型定义好之后，还没有被训练。要想训练我们的模型，就需要用到损失函数与优化方法，这一部分前面课里（如果你感觉陌生的话，可以回顾11～13节课）已经学过了，所以现在我们直接看代码就可以了。</p><p>这里选择的是MSE损失与SGD优化方法。</p><pre><code class=\"language-python\">model = LinearModel()\n# 定义优化器\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\ny_train = torch.tensor(y_train, dtype=torch.float32)\nfor _ in range(1000):\n    input = torch.from_numpy(x_train)\n    output = model(input)\n    loss = nn.MSELoss()(output, y_train)\n    model.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre><p>经过1000个Epoch的训练以后，我们可以打印出模型的weight与bias，看看是多少。</p><p>对于一个模型的<strong>可训练</strong>的参数，我们可以通过named_parameters()来查看，请看下面代码。</p><pre><code class=\"language-python\">for parameter in model.named_parameters():\n&nbsp; print(parameter)\n# 输出：\n('weight', Parameter containing:\ntensor([2.0071], requires_grad=True))\n('bias', Parameter containing:\ntensor([3.1690], requires_grad=True))\n</code></pre><p>可以看到，weight是2.0071，bias是3.1690，你再回头对一下我们创建训练数据的w与b，它们是不是一样呢？</p><p>我们刚才说过，继承一个nn.Module之后，可以定义自己的网络模型。Module同样可以作为另外一个Module的一部分，被包含在网络中。比如，我们要设计下面这样的一个网络：</p><p><img src=\"https://static001.geekbang.org/resource/image/25/c6/2574b93463fd3dbd0e97661d6a06ffc6.jpg?wh=1372x689\" alt=\"\"></p><p>观察图片很容易就会发现，在这个网络中有大量重复的结构。上图中的3x3与2x2的卷积组合，按照我们开篇的讲解的话，我们需要把每一层卷积都定义到__init__()，然后再在forward中定义好执行方法就可以了，例如下面的伪代码：</p><pre><code class=\"language-python\">class CustomModel(nn.Module):\n&nbsp; def __init__(self):\n&nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')\n&nbsp; &nbsp; self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; self.conv_m_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')\n&nbsp; &nbsp; self.conv_m_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; self.conv_n_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')\n&nbsp; &nbsp; self.conv_n_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')\n\n&nbsp; def forward(self, input):\n&nbsp; &nbsp; x = self.conv1_1(input)\n&nbsp; &nbsp; x = self.conv1_2(x)\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; x = self.conv_m_1(x)\n&nbsp; &nbsp; x = self.conv_m_2(x)\n&nbsp; &nbsp; ...&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; x = self.conv_n_1(x)\n&nbsp; &nbsp; x = self.conv_n_2(x)\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; return x\n</code></pre><p>其实这部分重复的结构完全可以放在一个单独的module中，然后，在我们模型中直接调用这部分即可，具体实现你可以参考下面的代码：</p><pre><code class=\"language-python\">class CustomLayer(nn.Module):\n  def __init__(self, input_channels, output_channels):\n    super().__init__()\n    self.conv1_1 = nn.Conv2d(in_channels=input_channels, out_channels=3, kernel_size=3, padding='same')\n    self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=output_channels, kernel_size=2, padding='same')\n    \n  def forward(self, input):\n    x = self.conv1_1(input)\n    x = self.conv1_2(x)\n    return x\n    \n</code></pre><p>然后呢，CustomModel就变成下面这样了：</p><pre><code class=\"language-python\">class CustomModel(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.layer1 = CustomLayer(1，1)\n    ...\n    self.layerm = CustomLayer(1，1)\n    ...\n    self.layern = CustomLayer(1，1)\n  \n  def forward(self, input):\n    x = self.layer1(input)\n    ...\n    x = self.layerm(x)\n    ...    \n    x = self.layern(x)\n    ...\n    return x\n</code></pre><p>熟悉深度学习的同学，一定听过残差块、Inception块这样的多层的一个组合。你没听过也没关系，在图像分类中我还会讲到。这里你只需要知道，这种多层组合的结构是类似的，对于这种组合，我们就可以用上面的代码的方式实现。</p><h3>模型保存与加载</h3><p>我们训练好的模型最终的目的，就是要为其他应用提供服务的，这就涉及到了模型的保存与加载。</p><p>模型保存与加载的话有两种方式。PyTorch模型的后缀名一般是pt或pth，这都没有关系，只是一个后缀名而已。我们接着上面的回归模型继续讲模型的保存与加载。</p><h4>方式一：只保存训练好的参数</h4><p>第一种方式就是只保存训练好的参数。然后加载模型的时候，你需要通过代码加载网络结构，然后再将参数赋予网络。</p><p>只保存参数的代码如下所示：</p><pre><code class=\"language-python\">torch.save(model.state_dict(), './linear_model.pth')\n</code></pre><p>第一个参数是模型的state_dict，而第二个参数要保存的位置。</p><p>代码中的state_dict是一个字典，在模型被定义之后会自动生成，存储的是模型<strong>可训练</strong>的参数。我们可以打印出线性回归模型的state_dict，如下所示：</p><pre><code class=\"language-python\">model.state_dict()\n输出：OrderedDict([('weight', tensor([[2.0071]])), ('bias', tensor([3.1690]))])\n</code></pre><p>加载模型的方式如下所示：</p><pre><code class=\"language-python\"># 先定义网络结构\nlinear_model = LinearModel()\n# 加载保存的参数\nlinear_model.load_state_dict(torch.load('./linear_model.pth'))\nlinear_model.eval()\nfor parameter in linear_model.named_parameters():\n&nbsp; print(parameter)\n输出：\n('weight', Parameter containing:\ntensor([[2.0071]], requires_grad=True))\n('bias', Parameter containing:\ntensor([3.1690], requires_grad=True))\n</code></pre><p>这里有个model.eval()需要你注意一下，因为有些层（例如，Dropout与BN）在训练时与评估时的状态是不一样的，当进入评估时要执行model.eval()，模型才能进入评估状态。这里说的评估不光光指代评估模型，也包括模型上线时候时的状态。</p><h4>方式二：保存网络结构与参数</h4><p>相比第一种方式，这种方式在加载模型的时候，不需要加载网络结构了。具体代码如下所示：</p><pre><code class=\"language-python\"># 保存整个模型\ntorch.save(model, './linear_model_with_arc.pth')\n# 加载模型，不需要创建网络了\nlinear_model_2 = torch.load('./linear_model_with_arc.pth')\nlinear_model_2.eval()\nfor parameter in linear_model_2.named_parameters():\n&nbsp; print(parameter)\n# 输出：\n('weight', Parameter containing:\ntensor([[2.0071]], requires_grad=True))\n('bias', Parameter containing:\ntensor([3.1690], requires_grad=True))\n</code></pre><p>这样操作以后，如果你成功输出了相应数值，而且跟之前保存的模型的参数一致，就说明加载对了。</p><h2>使用Torchvison中的模型进行训练</h2><p>我们前面说过，Torchvision提供了一些封装好的网络结构，我们可以直接拿过来使用。但是并没有细说如何使用它们在我们的数据集上进行训练。今天，我们就来看看如何使用这些网络结构，在我们自己的数据上训练我们自己的模型。</p><h3>再说微调</h3><p>其实，Torchvision提供的模型最大的作用就是当作我们训练时的预训练模型，用来加速我们模型收敛的速度，这就是所谓的微调。</p><p>对于微调，最关键的一步就是之前讲的<strong>调整最后全连接层输出的数目</strong>。Torchvision中只是对各大网络结构的复现，而不是对它们进行了统一的封装，所以在修改全连接层时，不同的网络有不同的修改方法。</p><p>不过你也别担心，这个修改并不复杂，你只需要打印出网络结构，就可以知道如何修改了。我们接下来以AlexNet为例带你尝试一下如何微调。</p><p>前面讲Torchvision的时候其实提到过一次微调，那个时候说的是固定整个网络的参数，只训练最后的全连接层。今天我再给你介绍另外一种微调的方式，那就是修改全连接层之后，整个网络都重新开始训练。只不过，这时候要使用预训练模型的参数作为初始化的参数，这种方式更为常用。</p><p>接下来，我们就看看如何使用Torchvision中模型进行微调。</p><p>首先，导入模型。代码如下：</p><pre><code class=\"language-python\">import torchvision.models as models\nalexnet = models.alexnet(pretrained=True)\n</code></pre><p>这一步如果你不能“科学上网”的话，可能会比较慢。你可以先根据命令中提示的url手动下载，然后使用今天讲的模型加载的方式加载预训练模型，代码如下所示：</p><pre><code class=\"language-python\">import torchvision.models as models\nalexnet = models.alexnet()\nalexnet.load_state_dict(torch.load('./model/alexnet-owt-4df8aa71.pth'))\n</code></pre><p>为了验证加载是否成功，我们让它对下图进行预测：<br>\n<img src=\"https://static001.geekbang.org/resource/image/66/20/666181b44c23e9075debe0daf6126b20.jpg?wh=1920x1867\" alt=\"图片\"></p><p>代码如下：</p><pre><code class=\"language-python\">from PIL import Image\nimport torchvision\nimport torchvision.transforms as transforms\n\nim = Image.open('dog.jpg')\n\ntransform = transforms.Compose([\n&nbsp; &nbsp; transforms.RandomResizedCrop((224,224)),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n&nbsp; &nbsp; ])\n\ninput_tensor = transform(im).unsqueeze(0)\nalexnet(input_tensor).argmax()\n输出：263\n</code></pre><p>运行了前面的代码之后，对应到ImageNet的类别标签中可以找到，263对应的是Pembroke（柯基狗），这就证明模型已经加载成功了。<br>\n这个过程中有两个重点你要留意。</p><p>首先，因为Torchvision中所有图像分类的预训练模型，它们都是在ImageNet上训练的。所以，输入数据需要是3通道的数据，也就是shape为(B, 3, H, W)的Tensor，B为batchsize。我们需要使用均值为[0.485, 0.456, 0.406]，标准差为[0.229, 0.224, 0.225]对数据进行正规化。</p><p>另外，从理论上说，大部分的经典卷积神经最后采用全连接层（也就是机器学习中的感知机）进行分类，这也导致了网络的输入尺寸是固定的。但是，在Torchvision的模型可以接受任意尺寸的输入的。</p><p>这是因为Torchvision对模型做了优化，有的网络是在最后的卷积层采用了全局平均，或者采用的是全卷积网络。这两种方式都可以让网络接受在最小输入尺寸基础之上，任意尺度的输入。这一点，你现在可能认识得还不够清楚，不过别担心，以后我们学习完图像分类理论之后，你会理解得更加透彻。</p><p>我们回到微调这个主题。正如刚才所说，训练一个AlexNet需要的数据必须是三通道数据。所以，在这里我使用了CIFAR-10公开数据集举例。</p><p>CIFAR-10数据集一共有60000张图片构成，共10个类别，每一类包含6000图片。每张图片为32x32的RGB图片。其中50000张图片作为训练集，10000张图片作为测试集。</p><p>可以说CIFAR-10是非常接近真实项目数据的数据集了，因为真实项目中的数据通常是RGB三通道数据，而CIFAR-10同样是三通道数据。</p><p>我们用之前讲的make_grid方法，将CIFAR-10的数据打印出来，代码如下：</p><pre><code class=\"language-python\">cifar10_dataset = torchvision.datasets.CIFAR10(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transforms.ToTensor(),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\n# 取32张图片的tensor\ntensor_dataloader = DataLoader(dataset=cifar10_dataset,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=32)\ndata_iter = iter(tensor_dataloader)\nimg_tensor, label_tensor = data_iter.next()\nprint(img_tensor.shape)\ngrid_tensor = torchvision.utils.make_grid(img_tensor, nrow=16, padding=2)\ngrid_img = transforms.ToPILImage()(grid_tensor)\ndisplay(grid_img)\n</code></pre><p>请注意，上述代码中的transform，我为了打印图片只使用了transform.ToTensor()输出图片，结果如下所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/f0/f2/f0b5f4ed8c24fbb81c7b7c71a907a6f2.jpg?wh=546x546\" alt=\"\"></p><p>这里我特别说明一下，因为这个训练集的数据都是32x32的，所以你现在看到的就是原图效果，图片大小并不影响咱们的学习。</p><p>下面我们要做的是修改全连接层，直接print就可以打印出网络结构，代码如下：</p><pre><code class=\"language-python\">print(alexnet)\n输出：\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n</code></pre><p>可以看到，最后全连接层输入是4096个单元，输出是1000个单元，我们要把它修改为输出是10个单元的全连接层（CIFR10有10类）。代码如下：</p><pre><code class=\"language-python\"># 提取分类层的输入参数\nfc_in_features = alexnet.classifier[6].in_features\n\n# 修改预训练模型的输出分类数\nalexnet.classifier[6] = torch.nn.Linear(fc_in_features, 10)\nprint(alexnet)\n输出：\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n</code></pre><p>这时，你可以发现输出就变为10个单元了。</p><p>接下来就是在CIFAR-10上，使用AlexNet作为预训练模型训练我们自己的模型了。首先是数据读入，代码如下：</p><pre><code class=\"language-python\">transform = transforms.Compose([\n&nbsp; &nbsp; transforms.RandomResizedCrop((224,224)),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n&nbsp; &nbsp; ])\ncifar10_dataset = torchvision.datasets.CIFAR10(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transform,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\ndataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=32,&nbsp; &nbsp; &nbsp; &nbsp;# 输出的batch大小\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;shuffle=True,&nbsp; &nbsp; &nbsp; &nbsp;# 数据是否打乱\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_workers=2)&nbsp; &nbsp; &nbsp; # 进程数, 0表示只有主进程\n</code></pre><p>这里需要注意的是，我更改了transform，并且将图片resize到224x224大小。这个尺寸是Torchvision中推荐的一个最小训练尺寸。模型就是我们修改后的AlexNet，之后的训练跟我们之前讲的是一样的。<br>\n先定义优化器，代码如下：</p><pre><code class=\"language-python\">optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n</code></pre><p>然后开始模型训练，是不是感觉后面的代码很眼熟，没错，它跟我们之前讲的一样：</p><pre><code class=\"language-python\"># 训练3个Epoch\nfor epoch in range(3):\n&nbsp; &nbsp; for item in dataloader:&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; output = alexnet(item[0])\n&nbsp; &nbsp; &nbsp; &nbsp; target = item[1]\n&nbsp; &nbsp; &nbsp; &nbsp; # 使用交叉熵损失函数\n&nbsp; &nbsp; &nbsp; &nbsp; loss = nn.CrossEntropyLoss()(output, target)\n&nbsp; &nbsp; &nbsp; &nbsp; print('Epoch {}, Loss {}'.format(epoch + 1 , loss))\n&nbsp; &nbsp; &nbsp; &nbsp; #以下代码的含义，我们在之前的文章中已经介绍过了\n&nbsp; &nbsp; &nbsp; &nbsp; alexnet.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n</code></pre><p>这里用到的微调方式，就是所有参数都需要进行重新训练。</p><p>而第一种方式（固定整个网络的参数，只训练最后的全连接层），只需要在读取完预训练模型之后，将全连接层之前的参数全部锁死即可，也就是让他们无法训练，我们模型训练时，只训练全连接层就行了，其余一切都不变。代码如下所示：</p><pre><code class=\"language-python\">alexnet = models.alexnet()\nalexnet.load_state_dict(torch.load('./model/alexnet-owt-4df8aa71.pth'))\nfor param in alexnet.parameters():\n    param.requires_grad = False\n</code></pre><p>说到这里，我们的模型微调就讲完了，你可以自己动手试试看。</p><h2>总结</h2><p>今天的内容，主要是围绕如何自己搭建一个网络模型，我们介绍了nn.Module模块以及围绕它的一些方法。</p><p>根据这讲我分享给你的思路，之后如果你有什么想法时，就可以快速搭建一个模型进行训练和验证。</p><p>其实，实际的开发中，我们很少会自己去构建一个网络，绝大多数都是直接使用前人已经构建好的一些经典网络，例如，Torchvision中那些模型。当你去看一些还没有被封装到PyTorch的模型的时候，今天所学的内容就能够帮你直接借鉴前人的工作结果，训练属于自己的模型。</p><p>最后，我再结合自己的学习研究经验，给有兴趣了解更多深度学习知识的同学提供一些学习线索。目前我们只讲了卷积层，对于一个网络还有很多其余层，比如Dropout、Pooling层、BN层、激活函数等。Dropout函数、Pooling层、激活函数相对比较好理解，BN层可能稍微复杂一些。</p><p>另外，细心的小伙伴应该发现了，我们在打印AlexNet网络结构中的时候，它的一部分是使用nn.Sequential构建的。<strong>nn.Sequential是一种快速构建网络的方式</strong>，有了这节课的知识作储备，弄懂这个方式你会觉得非常简单，也推荐你去看看。</p><h2>每课一练</h2><p>请你自己构建一个卷积神经网络，基于CIFAR-10，训练一个图像分类模型。因为还没有学习图像分类原理，所以我先帮你写好了网络的结构，需要你补全数据读取、损失函数(交叉熵损失)与优化方法（SGD）等部分。</p><pre><code class=\"language-python\">class MyCNN(nn.Module):\n&nbsp; &nbsp; def __init__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n&nbsp; &nbsp; &nbsp; &nbsp; # conv1输出的特征图为222x222大小\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = nn.Linear(16 * 222 * 222, 10)\n\n&nbsp; &nbsp; def forward(self, input):\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.conv1(input)\n&nbsp; &nbsp; &nbsp; &nbsp; # 进去全连接层之前，先将特征图铺平\n&nbsp; &nbsp; &nbsp; &nbsp; x = x.view(x.shape[0], -1)\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.fc(x)\n&nbsp; &nbsp; &nbsp; &nbsp; return x\n</code></pre><p>欢迎你在留言区和我交流讨论。如果这节课对你有帮助，也推荐你顺手分享给更多的同事、朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"加餐 | 机器学习其实就那么几件事","id":440393},"right":{"article_title":"15 | 可视化工具：如何实现训练的可视化监控？","id":444252}}},{"article_id":444252,"article_title":"15 | 可视化工具：如何实现训练的可视化监控？","article_content":"<p>你好，我是方远。欢迎来到第15节课的学习。</p><p>上节课中，我们以线性回归模型为例，学习了模型从搭建到训练的全部过程。在深度学习领域，模型训练是一个必须的环节，而在训练过程中，我们常常需要对模型的参数、评价指标等信息进行可视化监控。</p><p>今天我们主要会学习两种可视化工具，并利用它们实现训练过程的可视化监控。</p><p>在TensorFlow中，最常使用的可视化工具非Tensorboard莫属，而TensorboardX工具使得PyTorch也享受到Tensorboard的便捷功能。另外，FaceBook也为PyTorch开发了一款交互式可视化工具Visdom，它可以对实时数据进行丰富的可视化，帮助我们实时监控实验过程。</p><p>让我们先从TensorboardX说起。</p><h2>TensorboardX</h2><p>Tensorboard是TensorFlow的一个附加工具，用于记录训练过程的模型的参数、评价指标与图像等细节内容，并通过Web页面提供查看细节与过程的功能，用浏览器可视化的形式展现，帮助我们在实验时观察神经网络的训练过程，把握训练趋势。</p><p>既然Tensorboard工具这么方便，TensorFlow外的其它深度学习框架自然也想获取Tensorboard的便捷功能，于是，TensorboardX应运而生。</p><!-- [[[read_end]]] --><h3>安装</h3><p>安装 Tensorboard很容易，我们可以使用pip进行安装，命令如下：</p><pre><code class=\"language-python\">pip install tensorboard\n</code></pre><p>如果你已经安装过TensorFlow，那么就无需额外安装Tensorboard了。</p><p>接下来，我们需要安装 TensorboardX。这里需要注意的是，PyTorch 1.8之后的版本自带TensorboardX，它被放在<code>torch.utils.tensorboard</code>中，因此无需多余配置。</p><p>如果你用的是PyTorch 1.8之前的版本，TensorboardX安装起来也非常简单。我们依然使用pip命令安装：</p><pre><code class=\"language-plain\">pip install tensorboardX\n</code></pre><h3>使用与启动</h3><p>为了使用TensorboardX，我们首先需要创建一个SummaryWriter的实例，然后再使用<code>add_scalar</code>方法或<code>add_image</code>方法，将数字或图片记录到SummaryWriter实例中。</p><p>SummaryWriter类的定义如下：</p><pre><code class=\"language-python\">torch.utils.tensorboard.writer.SummaryWriter(log_dir=None)\n</code></pre><p>其中的log_dir表示保存日志的路径，默认会保存在“runs/当前时间_主机名”文件夹中。</p><p>实例创建好之后，我们来看<code>add_scalar</code>方法，这个方法用来记录<strong>数字常量</strong>，它的定义如下：</p><pre><code class=\"language-python\">add_scalar(tag, scalar_value, global_step=None, walltime=None)\n</code></pre><p>根据定义，我们依次说说其中的参数：</p><ul>\n<li>tag：字符串类型，表示数据的名称，不同名称的数据会使用不同曲线展示；</li>\n<li>scalar_value：浮点型，表示要保存的数值；</li>\n<li>global_step：整型，表示训练的step数；</li>\n<li>walltime：浮点型，表示记录发生的时间，默认为time.time()。</li>\n</ul><p>我们一般会使用<code>add_scalar</code>方法来记录训练过程的loss、accuracy、learning rate等数值的变化，这样就能直观地监控训练过程。</p><p><code>add_image</code>方法用来记录单个图像数据（需要Pillow库的支持），它的定义如下：</p><pre><code class=\"language-python\">add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')\n</code></pre><p>tag、global_step和walltime的含义跟<code>add_scalar</code>方法里一样，所以不再赘述，我们看看其他新增的参数都是什么含义。</p><ul>\n<li>img_tensor：PyTorch的Tensor类型或NumPy的array类型，表示图像数据；</li>\n<li>dataformats：字符串类型，表示图像数据的格式，默认为“CHW”，即Channel x Height x Width，还可以是“CHW”、“HWC”或“HW”等。</li>\n</ul><p>我们来看一个例子加深理解，具体代码如下。</p><pre><code class=\"language-python\">from torch.utils.tensorboard import SummaryWriter\n# PyTorch 1.8之前的版本请使用：\n# from tensorboardX import SummaryWriter\nimport numpy as np\n\n# 创建一个SummaryWriter的实例\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n    \nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter.add_image('my_image', img, 0)\nwriter.close()\n</code></pre><p>我给你梳理一下这段代码都做了什么。</p><p>首先创建一个SummaryWriter的实例，这里注意，PyTorch 1.8之前的版本请使用“from tensorboardX import SummaryWriter”，PyTorch 1.8之后的版本请使用“from torch.utils.tensorboard import SummaryWriter”。</p><p>然后，我们随机生成一些随机数，用来模拟训练与预测过程中的Loss和Accuracy，并且用<code>add_scalar</code>方法进行记录。最后生成了一个图像，用<code>add_image</code>方法来记录。</p><p>上述代码运行后，会在当前目录下生成一个“runs”文件夹，里面存储了我们需要记录的数据。</p><p>然后，我们在当前目录下执行下面的命令，即可启动Tensoboard。</p><pre><code class=\"language-python\">tensorboard --logdir=runs\n</code></pre><p>启动后，在浏览器中输入“<a href=\"http://127.0.0.1:6006/\">http://127.0.0.1:6006/</a>”（Tensorboard的默认端口为6006），即可对刚才我们记录的数据进行可视化。</p><p>Tensorboard的界面如下图所示。图片中右侧部分就是刚刚用<code>add_scalar</code>方法记录的Loss和Accuracy。你看，Tensorboard已经帮我们按照迭代step绘制成了曲线图，可以非常直观地监控训练过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/33/e8/3393e81d9a34c51cdecd4a99e460dde8.png?wh=1920x1360\" alt=\"图片\"></p><p>在“IMAGES”的标签页中，可以显示刚刚用<code>add_image</code>方法记录的图像数据，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/86/3a/86bcefa7e732ccceef230a888b3f3a3a.png?wh=1432x1114\" alt=\"图片\"></p><h3>训练过程可视化</h3><p>好，进行到这里，我们已经装好了TensorboardX并启动，还演示了这个工具如何使用。</p><p>那么如何在我们实际的训练过程中来进行可视化监控呢？我们用上节课构建并训练的线性回归模型为例，来进行实践。</p><p>下面的代码上节课讲过，作用是定义一个线性回归模型，并随机生成训练集X与对应的标签Y。</p><pre><code class=\"language-python\">import random\nimport numpy as np\nimport torch\nfrom torch import nn\n\n# 模型定义\nclass LinearModel(nn.Module):\n&nbsp; def __init__(self):\n&nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; self.weight = nn.Parameter(torch.randn(1))\n&nbsp; &nbsp; self.bias = nn.Parameter(torch.randn(1))\n\n&nbsp; def forward(self, input):\n&nbsp; &nbsp; return (input * self.weight) + self.bias\n\n# 数据\nw = 2\nb = 3\nxlim = [-10, 10]\nx_train = np.random.randint(low=xlim[0], high=xlim[1], size=30)\ny_train = [w * x + b + random.randint(0,2) for x in x_train]\n</code></pre><p>然后我们在训练的过程中，加入刚才讲过的SummaryWriter实例与<code>add_scalar</code>方法，具体的代码如下。</p><pre><code class=\"language-python\"># Tensorboard\nfrom torch.utils.tensorboard import SummaryWriter\n\n# 训练\nmodel = LinearModel()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\ny_train = torch.tensor(y_train, dtype=torch.float32)\n\nwriter = SummaryWriter()\n\nfor n_iter in range(500):\n    input = torch.from_numpy(x_train)\n    output = model(input)\n    loss = nn.MSELoss()(output, y_train)\n    model.zero_grad()\n    loss.backward()\n    optimizer.step()\n    writer.add_scalar('Loss/train', loss, n_iter)\n</code></pre><p>通过上面这段代码，我们记录了训练过程中的Loss的变换过程。具体的趋势如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/48/df021d742b09180fb267652fca440d48.png?wh=750x656\" alt=\"图片\"></p><p>可以看到Loss是一个下降的趋势，说明随着训练过程，模型越来越拟合我们的训练数据了。进行到这里，我们已经走完了利用TensorboardX工具，实现训练可视化监控的整个过程。</p><p>TensorboardX除了包括上述的常用方法之外，还有许多其他方法如<code>add_histogram</code>、<code>add_graph</code>、<code>add_embedding</code>、<code>add_audio</code> 等，感兴趣的同学可以参考<a href=\"https://pytorch.org/docs/stable/tensorboard.html\">[官方文档]</a>。相信参考已经学习过的两个add方法，你一定能够举一反三，很快熟练调用其它的方法。</p><h2>Visdom</h2><p>Visdom是Facebook开源的一个专门用于PyTorch的交互式可视化工具。它为实时数据提供了丰富的可视化种类，可以在浏览器中进行查看，并且可以很容易地与其他人共享可视化结果，帮助我们实时监控在远程服务器上进行的科学实验。</p><h3>安装与启动</h3><p>Visdom的安装非常简单，可直接使用pip进行安装，具体的命令如下：</p><pre><code class=\"language-python\">pip install visdom\n</code></pre><p>执行安装命令后，我们可以执行以下命令启动Visdom：</p><pre><code class=\"language-python\">python -m visdom.server\n</code></pre><p>Visdom的默认端口是8097，如果需要修改，可以使用-p选项。</p><p>启动成功后，在浏览器中输入“<a href=\"http://127.0.0.1:8097/\">http://127.0.0.1:8097/</a>”，进入Visdom的主界面。</p><p>Visdom的主界面如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/99/a8/99d3a7d1ayya17cb625a6f8a567e2aa8.jpg?wh=1920x460\" alt=\"图片\"></p><p>请你注意，Visdom的使用与Tensorboard稍有不同。Tensorboard是在生成记录文件后，启动可视化界面。而Visdom是先启动可视化界面，当有数据进入Visdom的窗口时，会实时动态地更新并绘制数据。</p><h3>快速上手</h3><p>下面我们就来动手试一下，看看Visdom如何绘制数据。</p><p>具体过程分四步走：首先，我们需要将窗口类Visdom实例化；然后，利用line()方法创建一个线图窗口并初始化；接着，利用生成的一组随机数数据来更新线图窗口。最后，通过image()方法来绘制一张图片。</p><p>上述过程的具体代码如下。</p><pre><code class=\"language-python\">from visdom import Visdom\nimport numpy as np\nimport time\n\n# 将窗口类实例化\nviz = Visdom()&nbsp;\n# 创建窗口并初始化\nviz.line([0.], [0], win='train_loss', opts=dict(title='train_loss'))\n\nfor n_iter in range(10):\n&nbsp; &nbsp; # 随机获取loss值\n&nbsp; &nbsp; loss = 0.2 * np.random.randn() + 1\n&nbsp; &nbsp; # 更新窗口图像\n&nbsp; &nbsp; viz.line([loss], [n_iter], win='train_loss', update='append')\n&nbsp; &nbsp; time.sleep(0.5)\n\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n# 可视化图像\nviz.image(img)\n</code></pre><p>可以看出，使用过程与Tensorboard基本一致，只是函数调用上的不同。<br>\n绘制线图的结果如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c0/13/c0057dd5935b0ed2f6606a8c148f4f13.gif?wh=368x336\" alt=\"图片\"></p><p>对应的绘制图片结果如下。可以看出，Visodm绘制数据时，是动态更新的。</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/88/a66f16a1be56456c009da3873c949888.jpg?wh=676x482\" alt=\"图片\"></p><h3>训练可视化监控</h3><p>同样地，我们学习可视化工具的使用主要是为了监控我们的训练过程。我们还是以构建并训练的线性回归模型为例，来进行实践。</p><p>Visdom监控训练过程大致分为三步：</p><ul>\n<li>实例化一个窗口；</li>\n<li>初始化窗口的信息；</li>\n<li>更新监听的信息。</li>\n</ul><p>定义模型与生成训练数据的过程跟前面一样，我就不再重复了。在训练过程中实例化并初始化Visdom窗口、实时记录Loss的代码如下。</p><pre><code class=\"language-python\"># Visdom\nfrom visdom import Visdom\nimport numpy as np\n\n# 训练\nmodel = LinearModel()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\ny_train = torch.tensor(y_train, dtype=torch.float32)\n\n# 实例化一个窗口\nviz = Visdom(port=8097)\n# 初始化窗口的信息\nviz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\n\nfor n_iter in range(500):\n    input = torch.from_numpy(x_train)\n    output = model(input)\n    loss = nn.MSELoss()(output, y_train)\n    model.zero_grad()\n    loss.backward()\n    optimizer.step()\n    # 更新监听的信息\n    viz.line([loss.item()], [n_iter], win='train_loss', update='append')\n</code></pre><p>在Visdom的界面中，我们可以看到Loss的变化趋势如下图所示。Visdom不会像Tensorboard自动对曲线进行缩放或平滑，因此可以看到50轮之后，由于Loss值变化范围比较小，图像的抖动趋势被压缩得非常不明显。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/e0/54665114801a49e36a0211b5b3dbdce0.jpg?wh=1007x763\" alt=\"图片\"></p><h2>小结</h2><p>这节课我带你学习了两种可视化工具：TensorboardX和Visdom。</p><p>相信通过一节课的讲解和练习，这两种可视化工具如何安装、启动，还有如何用它们绘制线图和图片这些基本的操作，相信你都已经掌握了。</p><p>学习使用可视化工具的主要目的，<strong>是为了帮助我们在深度学习模型的训练过程中，实时监控一些数据，例如损失值、评价指标等等</strong>。对这些数据进行可视化监控，可以帮助我们感知各个参数与指标的变化，实时把握训练趋势。因此，如何将可视化工具应用于模型训练过程中，是我们学习的重点。</p><p>TensorboardX和Visdom还有其它诸如绘制散点图、柱状图、热力图等等多种多样的功能，如果你感兴趣，可以参考官方文档，类比我们今天学习的方法动手试一试，经过练习，一定可以熟练使用它们。</p><h2>每课一练</h2><p>参考Visdom快速上手中的例子，现在需要生成两组随机数，分别表示Loss和Accuracy。在迭代的过程中，如何用代码同时绘制出Loss和Accuracy两组数据呢？</p><p>欢迎记录你的思考或疑惑，也推荐你把今天学到的可视化工具分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"14 | 构建网络：一站式实现模型搭建与训练","id":442442},"right":{"article_title":"16｜分布式训练：如何加速你的模型训练？","id":445886}}},{"article_id":445886,"article_title":"16｜分布式训练：如何加速你的模型训练？","article_content":"<p>你好，我是方远。</p><p>在之前的课程里，我们一起学习了深度学习必备的内容，包括构建网络、损失函数、优化方法等，这些环节掌握好了，我们就可以训练很多场景下的模型了。</p><p>但是有的时候，我们的模型比较大，或者训练数据比较多，训练起来就会比较慢，该怎么办呢？这时候牛气闪闪的分布式训练登场了，有了它，我们就可以极大地加速我们的训练过程。</p><p>这节课我就带你入门分布式训练，让你吃透分布式训练的工作原理，最后我还会结合一个实战项目，带你小试牛刀，让你在动手过程中加深对这部分内容的理解。</p><h2>分布式训练原理</h2><p>在具体介绍分布式训练之前，我们需要先简要了解一下为什么深度学习要使用GPU。</p><p>在我们平时使用计算机的时候，程序都是将进程或者线程的数据资源放在内存中，然后在CPU进行计算。通常的程序中涉及到了大量的if else等分支逻辑操作，这也是CPU所擅长的计算方式。</p><p>而在深度学习中，模型的训练与计算过程则没有太多的分支，基本上都是矩阵或者向量的计算，而这种暴力又单纯的计算形式非常适合用GPU处理，GPU 的整个处理过程就是一个流式处理的过程。</p><p>但是再好的车子，一个缸的发动机也肯定比不过12个缸的，同理单单靠一个GPU，速度肯定还是不够快，于是就有了多个GPU协同工作的办法，即分布式训练。分布式训练，顾名思义就是训练的过程是分布式的，重点就在于后面这两个问题：</p><!-- [[[read_end]]] --><p>1.谁分布了？答案有两个：数据与模型。<br>\n2.怎么分布？答案也有两个：单机多卡与多机多卡。</p><p>也就是说，为了实现深度学习的分布式训练，我们需要采用单机多卡或者多机多卡的方式，让分布在不同GPU上的数据和模型协同训练。那么接下来，我们先从简单的单机单卡入手，了解一下GPU的训练过程。</p><h3>单机单卡</h3><p>想象一下，如果让你把数据或者模型推送到GPU上，需要做哪几步操作呢？让我们先从单GPU的情况出发。</p><p>第一步，我们需要知道手头有多少GPU。PyTorch中使用torch.cuda.is_available()函数来判断当前的机器是否有可用的GPU，而函数torch.cuda.device_count()则可以得到目前可用的GPU的数量。</p><p>第二步，获得GPU的一个实例。例如下面的语句：</p><pre><code class=\"language-python\">device&nbsp;=&nbsp;torch.device(\"cuda:0\"&nbsp;if&nbsp;torch.cuda.is_available()&nbsp;else&nbsp;\"cpu\")\n</code></pre><p>这里torch.device代表将torch.Tensor分配到的设备，是一个设备对象实例，也就是GPU。其中cuda: 0表示我们使用的是第一块GPU。当然你也可以不用声明“:0”，默认就从第一块开始。如果没有GPU（torch.cuda.is_available()），那就只能使用CPU了。</p><p>第三步，将数据或者模型推到GPU上去，这个过程我们称为<strong>迁移</strong>。</p><p>在PyTorch中，这个过程的封装程度非常高，换句话说，我们只需要保证即将被推到GPU的内容是张量（Tensor）或者模型（Module），就可以用to()函数快速进行实现。例如：</p><pre><code class=\"language-python\">data = torch.ones((3, 3))\nprint(data.device)\n# Get: cpu\n\n# 获得device\ndevice = torch.device(\"cuda: 0\")\n\n# 将data推到gpu上\ndata_gpu = data.to(device)\nprint(data_gpu.device)\n# Get: cuda:0\n\n</code></pre><p>在上面这段代码中，我们首先创建了一个常规的张量data，通过device属性，可以看到data现在是在CPU上的。随后，我们通过to()函数将data迁移到GPU上，同样也能通过device属性看到data确实已经存在于GPU上了。</p><p>那么对于模型，是否也是一样的操作呢？答案是肯定的，我们接下来看一个例子：</p><pre><code class=\"language-python\">net = nn.Sequential(nn.Linear(3, 3))\nnet.to(device)\n</code></pre><p>这里仍旧使用to()函数即可。</p><p>单机单卡的模式，相当于有一批要处理加工的产品，只分给了一个工人和一台机器来完成，这种情况下数量少了还可以，但是一旦产品太多了，就得加人、加机器才能快速交工了。</p><p>深度学习也是一样，在很多场景中，比如推荐算法模型、语言模型等，往往都有着百万、千万甚至上亿的训练数据，这样如果只用一张卡的话肯定是搞不定了。于是就有了单机多卡和多机多卡的解决方案。</p><h3>单机多卡</h3><p>那么，在PyTorch中，单机多卡的训练是如何进行的呢？其实PyTorch提供了好几种解决方案，咱们先看一个最简单也是最常用的办法：nn.DataParallel()。其具体定义如下：</p><pre><code class=\"language-python\">torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n</code></pre><p>在这里，module就是你定义的模型，device_ids即为训练模型时用到的GPU设备号，output_device表示输出结果的device，默认为0也就是第一块卡。</p><p>我们可以使用nvidia-smi命令查看GPU使用情况。如果你足够细心就会发现，使用多个卡做训练的时候，output_device的卡所占的显存明显大一些。</p><p>继续观察你还会发现，使用DataParallel时，数据的使用是并行的，每张卡获得的数据都一样多，但是输出的loss则是所有的卡的loss都会在第output_device块GPU进行计算，这导致了output_device卡的负载进一步增加。</p><p><img src=\"https://static001.geekbang.org/resource/image/7f/08/7f8e9a83fa6a91yyf931565c55f0a708.png?wh=1130x522\" alt=\"图片\"></p><p>就这么简单？对，就这么简单，只需要一个DataParallel函数就可以将模型分发到多个GPU上。但是我们还是需要了解这内部的运行逻辑，因为只有了解了这个逻辑，在以后的开发中遇到了诸如<strong>时间计算、资源预估、优化调试问题</strong>的时候，你才可以更好地运用GPU，让多GPU的优势真正发挥出来。</p><p>在模型的前向计算过程中，数据会被划分为多个块，被推送到不同的GPU进行计算。但是不同的是，模型在每个GPU中都会复制一份。我们看一下后面的代码：</p><pre><code class=\"language-python\">class ASimpleNet(nn.Module):\n    def __init__(self, layers=3):\n        super(ASimpleNet, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(3, 3, bias=False) for i in range(layers)])\n    def forward(self, x):\n        print(\"forward batchsize is: {}\".format(x.size()[0]))\n        x = self.linears(x)\n        x = torch.relu(x)\n        return x\n        \nbatch_size = 16\ninputs = torch.randn(batch_size, 3)\nlabels = torch.randn(batch_size, 3)\ninputs, labels = inputs.to(device), labels.to(device)\nnet = ASimpleNet()\nnet = nn.DataParallel(net)\nnet.to(device)\nprint(\"CUDA_VISIBLE_DEVICES :{}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n\nfor epoch in range(1):\n    outputs = net(inputs)\n\n# Get:\n# CUDA_VISIBLE_DEVICES : 3, 2, 1, 0\n# forward batchsize is: 4\n# forward batchsize is: 4\n# forward batchsize is: 4\n# forward batchsize is: 4\n\n</code></pre><p>在上面的程序中，我们通过CUDA_VISIBLE_DEVICES得知了当前程序可见的GPU数量为4，而我们的batch size为16，输出每个GPU上模型forward函数内部的print内容，验证了每个GPU获得的数据量都是4个。这表示，DataParallel 会自动帮我们将数据切分、加载到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。</p><h3>多机多卡</h3><p>多机多卡一般都是基于集群的方式进行大规模的训练，需要涉及非常多的方面，咱们这节课只讨论最基本的原理和方法。在具体实践中，你可能还会遇到其它网络或环境等问题，届时需要具体问题具体解决。</p><h4>DP与DDP</h4><p>刚才我们已经提到，对于单机多卡训练，有一个最简单的办法：DataParallel。其实PyTorch的数据并行还有一个主要的API，那就是DistributedDataParallel。而<strong>DistributedDataParallel也是我们实现多机多卡的关键API</strong>。</p><p>DataParallel简称为DP，而DistributedDataParallel简称为DDP。我们来详细看看DP与DDP的区别。</p><p>先看DP，DP是单进程控制多GPU。从之前的程序中，我们也可以看出，DP将输入的一个batch数据分成了n份（n为实际使用的GPU数量），分别送到对应的GPU进行计算。</p><p>在网络前向传播时，模型会从主GPU复制到其它GPU上；在反向传播时，每个GPU上的梯度汇总到主GPU上，求得梯度均值更新模型参数后，再复制到其它GPU，以此来实现并行。</p><p>由于主GPU要进行梯度汇总和模型更新，并将计算任务下发给其它GPU，所以主GPU的负载与使用率会比其它GPU高，这就导致了GPU负载不均衡的现象。</p><p>再说说DDP，DDP多进程控制多GPU。系统会为每个GPU创建一个进程，不再有主GPU，每个GPU执行相同的任务。DDP使用分布式数据采样器（DistributedSampler）加载数据，确保数据在各个进程之间没有重叠。</p><p>在反向传播时，各GPU梯度计算完成后，各进程以广播的方式将梯度进行汇总平均，然后每个进程在各自的GPU上进行梯度更新，从而确保每个GPU上的模型参数始终保持一致。由于无需在不同GPU之间复制模型，DPP的传输数据量更少，因此速度更快。</p><p><strong>DistributedDataParallel既可用于单机多卡也可用于多机多卡</strong>，它能够解决DataParallel速度慢、GPU负载不均衡等问题。因此，官方更推荐使用DistributedDataParallel来进行分布式训练，也就是接下来要说的DDP训练。</p><h4>DDP训练</h4><p>DistributedDataParallel主要是为多机多卡而设计的，不过单机上也同样可以使用。</p><p>想要弄明白DPP的训练机制，我们先要弄明白这几个分布式中的概念：</p><ul>\n<li>group：即进程组。默认情况下，只有一个组，即一个world。</li>\n<li>world_size ：表示全局进程个数。</li>\n<li>rank：表示进程序号，用于进程间通讯，表示进程优先级。rank=0的主机为主节点。</li>\n</ul><p>使用DDP进行分布式训练的具体流程如下。接下来，我们就按步骤分别去实现。</p><p><img src=\"https://static001.geekbang.org/resource/image/27/7d/2730a8d7e7e1fe21574918a2dc48c67d.jpg?wh=1920x1009\" alt=\"图片\"></p><p>第一步，初始化进程组。我们使用init_process_group函数来进行分布式初始化，其定义如下：</p><pre><code class=\"language-python\">torch.distributed.init_process_group(backend, init_method=None,, world_size=-1, rank=-1, group_name='')\n</code></pre><p>我们分别看看定义里的相关参数：</p><ul>\n<li>backend：是通信所用的后端，可以是“nccl”或“gloo”。一般来说，nccl用于GPU分布式训练，gloo用于CPU进行分布式训练。</li>\n<li>init_method：字符串类型，是一个url，用于指定进程初始化方式，默认是 “env://”，表示从环境变量初始化，还可以使用TCP的方式或共享文件系统&nbsp;。</li>\n<li>world_size：执行训练的所有的进程数，表示一共有多少个节点（机器）。</li>\n<li>rank：进程的编号，也是其优先级，表示当前节点（机器）的编号。</li>\n<li>group_name：进程组的名字。</li>\n</ul><p>使用nccl后端的代码如下。</p><pre><code class=\"language-python\">torch.distributed.init_process_group(backend=\"nccl\")\n</code></pre><p>完成初始化以后，第二步就是模型并行化。正如前面讲过的，我们可以使用DistributedDataParallel，将模型分发至多GPU上，其定义如下：</p><pre><code class=\"language-python\">torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0）\n</code></pre><p>DistributedDataParallel的参数与DataParallel基本相同，因此模型并行化的用法只需将DataParallel函数替换成DistributedDataParallel即可，具体代码如下。</p><pre><code class=\"language-python\">net = torch.nn.parallel.DistributedDataParallel(net)\n</code></pre><p>最后就是创建分布式数据采样器。在多机多卡情况下，分布式训练数据的读取也是一个问题，不同的卡读取到的数据应该是不同的。</p><p>DP是直接将一个batch的数据划分到不同的卡，但是多机多卡之间进行频繁的数据传输会严重影响效率，这时就要用到分布式数据采样器DistributedSampler，它会为每个子进程划分出一部分数据集，从而使DataLoader只会加载特定的一个子数据集，以避免不同进程之间有数据重复。</p><p>创建与使用分布式数据采样器的代码如下。</p><pre><code class=\"language-python\">train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\ndata_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n</code></pre><p>结合代码我给你解读一下。<br>\n首先，我们将train_dataset送到了DistributedSampler中，并创建了一个分布式数据采样器train_sampler。</p><p>然后在构造DataLoader的时候,&nbsp;参数中传入了一个sampler=train_sampler，即可让不同的进程节点加载属于自己的那份子数据集。也就是说，使用DDP时，不再是从主GPU分发数据到其他GPU上，而是各GPU从自己的硬盘上读取属于自己的那份数据。</p><h1></h1><p>为什么要使用分布式训练以及分布式训练的原理我们就讲到这里。相信你已经对数据并行与模型并行都有了一个初步的认识。</p><h2>小试牛刀</h2><p>下面我们将会讲解一个<a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">官方的ImageNet的示例</a>，以后你可以把这个小项目当做分布式训练的一个模板来使用。</p><p>这个示例可对使用DP或DDP进行选配，下面我们就一起来看核心代码。</p><pre><code class=\"language-python\">if args.distributed:\n     if args.dist_url == \"env://\" and args.rank == -1:\n         args.rank = int(os.environ[\"RANK\"])\n     if args.multiprocessing_distributed:\n         # For multiprocessing distributed training, rank needs to be the\n         # global rank among all the processes\n         args.rank = args.rank * ngpus_per_node + gpu\n     dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                             world_size=args.world_size, rank=args.rank)\n</code></pre><p>这里你可以重点关注示例代码中的“args.distributed”参数，args.distributed为True，表示使用DDP，反之表示使用DP。</p><p>我们来看main_worker函数中这段针对DDP的初始化代码，如果使用DDP，那么使用init_process_group函数初始化进程组。ngpus_per_node表示每个节点的GPU数量。</p><p>我们再来看main_worker函数中的这段逻辑代码。</p><pre><code class=\"language-python\">if not torch.cuda.is_available():\n&nbsp; &nbsp; print('using CPU, this will be slow')\nelif args.distributed:\n&nbsp; &nbsp; # For multiprocessing distributed, DistributedDataParallel constructor\n&nbsp; &nbsp; # should always set the single device scope, otherwise,\n&nbsp; &nbsp; # DistributedDataParallel will use all available devices.\n&nbsp; &nbsp; if args.gpu is not None:\n&nbsp; &nbsp; &nbsp; &nbsp; torch.cuda.set_device(args.gpu)\n&nbsp; &nbsp; &nbsp; &nbsp; model.cuda(args.gpu)\n&nbsp; &nbsp; &nbsp; &nbsp; # When using a single GPU per process and per\n&nbsp; &nbsp; &nbsp; &nbsp; # DistributedDataParallel, we need to divide the batch size\n&nbsp; &nbsp; &nbsp; &nbsp; # ourselves based on the total number of GPUs we have\n&nbsp; &nbsp; &nbsp; &nbsp; args.batch_size = int(args.batch_size / ngpus_per_node)\n&nbsp; &nbsp; &nbsp; &nbsp; args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n&nbsp; &nbsp; &nbsp; &nbsp; model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n&nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; model.cuda()\n&nbsp; &nbsp; &nbsp; &nbsp; # DistributedDataParallel will divide and allocate batch_size to all\n&nbsp; &nbsp; &nbsp; &nbsp; # available GPUs if device_ids are not set\n&nbsp; &nbsp; &nbsp; &nbsp; model = torch.nn.parallel.DistributedDataParallel(model)\nelif args.gpu is not None:\n&nbsp; &nbsp; torch.cuda.set_device(args.gpu)\n&nbsp; &nbsp; model = model.cuda(args.gpu)\nelse:\n&nbsp; &nbsp; # DataParallel will divide and allocate batch_size to all available GPUs\n&nbsp; &nbsp; if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n&nbsp; &nbsp; &nbsp; &nbsp; model.features = torch.nn.DataParallel(model.features)\n&nbsp; &nbsp; &nbsp; &nbsp; model.cuda()\n&nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; model = torch.nn.DataParallel(model).cuda()\n</code></pre><p>这段代码是对使用CPU还是使用GPU、如果使用GPU，是使用DP还是DDP进行了逻辑选择。我们可以看到，这里用到了DistributedDataParallel函数或DataParallel函数，对模型进行并行化。<br>\n并行化之后就是创建分布式数据采样器，具体代码如下。</p><pre><code class=\"language-python\">if args.distributed:\n&nbsp; &nbsp; train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\nelse:\n&nbsp; &nbsp; train_sampler = None\n\ntrain_loader = torch.utils.data.DataLoader(\n&nbsp; &nbsp; train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n&nbsp; &nbsp; num_workers=args.workers, pin_memory=True, sampler=train_sample\n</code></pre><p>这里需要注意的是，<strong>在建立Dataloader的过程中，如果sampler参数不为None，那么shuffle参数不应该被设置</strong>。</p><p>最后，我们需要为每个机器节点上的每个GPU启动一个进程。PyTorch提供了torch.multiprocessing.spawn函数，来在一个节点启动该节点所有进程，具体的代码如下。</p><pre><code class=\"language-python\"> ngpus_per_node = torch.cuda.device_count()\n if args.multiprocessing_distributed:\n     # Since we have ngpus_per_node processes per node, the total world_size\n     # needs to be adjusted accordingly\n     args.world_size = ngpus_per_node * args.world_size\n     # Use torch.multiprocessing.spawn to launch distributed processes: the\n     # main_worker process function\n     mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n else:\n     # Simply call main_worker function\n     main_worker(args.gpu, ngpus_per_node, args)\n</code></pre><p>对照代码我们梳理一下其中的要点。之前我们提到的main_worker函数，就是每个进程中，需要执行的操作。ngpus_per_node是每个节点的GPU数量（每个节点GPU数量相同），如果是多进程，ngpus_per_node * args.world_size则表示所有的节点中一共有多少个GPU，即总进程数。<br>\n一般情况下，进程0是主进程，比如我们会在主进程中保存模型或打印log信息。</p><p>当节点数为1时，实际上就是单机多卡，所以说DDP既可以支持多机多卡，也可以支持单机多卡。</p><p>main_worker函数的调用方法如下。</p><pre><code class=\"language-python\">main_worker(args.gpu, ngpus_per_node, args)\n</code></pre><p>其中，args.gpu表示当前所使用GPU的id。而通过mp.spawn调用之后，会为每个节点上的每个GPU都启动一个进程，每个进程运行main_worker(i, ngpus_per_node, args)，其中i是从0到ngpus_per_node-1。<br>\n模型保存的代码如下。</p><pre><code class=\"language-python\">if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n         and args.rank % ngpus_per_node == 0):\n     save_checkpoint({\n         'epoch': epoch + 1,\n         'arch': args.arch,\n         'state_dict': model.state_dict(),\n         'best_acc1': best_acc1,\n         'optimizer' : optimizer.state_dict(),\n     }, is_best)\n</code></pre><p>这里需要注意的是，使用DDP意味着使用多进程，如果直接保存模型，每个进程都会执行一次保存操作，此时只使用主进程中的一个GPU来保存即可。</p><p>好，说到这，这个示例中有关分布式训练的重点内容我们就讲完了。</p><h2>小结</h2><p>恭喜你走到这里，这节课我们一起完成了分布式训练的学习，最后咱们一起做个总结。</p><p>今天我们不但学习了为什么要使用分布式训练以及分布式训练的原理，还一起学习了一个分布式训练的实战项目。</p><p>在分布式训练中，主要有DP与DDP两种模式。其中DP并不是完整的分布式计算，只是将一部分计算放到了多张GPU卡上，在计算梯度的时候，仍然是“一卡有难，八方围观”，因此DP会有负载不平衡、效率低等问题。而DDP刚好能够解决DP的上述问题，并且既可以用于单机多卡，也可以用于多机多卡，因此它是更好的分布式训练解决方案。</p><p>你可以将今天讲解的示例当做分布式训练的一个模板来使用。它包括了DP与DPP的完整使用过程，并且包含了如何在使用DDP时保存模型。不过这个示例中的代码里其实还有更多的细节，建议你留用课后空余时间，通过精读代码、查阅资料，多动手、多思考来巩固今天的学习成果。</p><h2>每课一练</h2><p>在torch.distributed.init_process_group(backend=“nccl”)函数中，backend参数可选哪些后端，它们分别有什么区别？</p><p>推荐你好好研读今天的分布式训练demo，也欢迎你记录自己的学习感悟或疑问，我在留言区等你。</p>","neighbors":{"left":{"article_title":"15 | 可视化工具：如何实现训练的可视化监控？","id":444252},"right":{"article_title":"17 | 图像分类（上）：图像分类原理与图像分类模型","id":446645}}},{"article_id":446645,"article_title":"17 | 图像分类（上）：图像分类原理与图像分类模型","article_content":"<p>你好，我是方远，欢迎来到图像分类的学习。</p><p>通过前面的学习，我们已经掌握了PyTorch有关深度学习的不少知识。为了避免纸上谈兵，我们正式进入实战环节，分别从计算机视觉与自然语言处理这两个落地项目最多的深度学习应用展开，看看业界那些常见深度学习应用都是如何实现的。</p><p>完成这个模块的学习以后，我想你不仅仅会巩固之前学习的内容，还会进一步地落实到细分的领域去看待问题、解决问题。</p><p>说到计算机视觉，<strong>很常见的一种应用方向就是图像分类</strong>。关于图像分类，其实离我们并不遥远。你有没有发现，现在很多智能手机，照相的时候都会自动给照片内容打上标签。</p><p>举个例子，你看后面的截图，就是我用手机拍照的时候，手机自动对摄像头的内容进行了识别，打上了“多云”这个标签。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/7c/75e6ec9c616da2c5c5907e0d11184d7c.jpeg?wh=1920x886\" alt=\"图片\"></p><p>然后你会发现，手机还能根据识别到的内容，为你推荐一些美化的方案。那这是怎么做到的呢？其实这就是卷积神经网络最常用、最广泛且最基本的一个应用：图像分类。</p><p>今天咱们就来一探究竟，看看图像分类到底是怎么一回事。我会用两节课的篇幅，带你学习图像分类。这节课我们先学习理论知识，掌握图像分类原理和常见的卷积神经网络。下节课，我们再基于今天学到的原理，一块完成一个完整的图像分类项目实践。</p><!-- [[[read_end]]] --><h2>图像分类原理</h2><p>我们还是“书接上文”，沿用第3节课NumPy的那个例子。现在线上每天都有大量的图片被上传，老板交代你设计一个模型，把有关极客时间Logo的图片自动找出来。</p><p>把这个需求翻译一下就是：建立一个图像分类模型，提供自动识别有极客时间Logo图片的功能。</p><p>我们来梳理一下这个模型的功能，我们这个模型会接收一张图片，然后会输出一组概率，分别是该图片为Logo的概率与该图片为其他图片的概率，从而通过概率来判断这张图片是Logo类还是Other类，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/68/f4b226497cb6aae5e0dcde4f65e46a68.png?wh=1392x604\" alt=\"图片\"></p><h3>感知机</h3><p>我们将上面的模型进一步拆分，看看如何才能获得这样的一组输出。</p><p>其中输入的图片，就是输入X，将其展开后，可以获得输入X为$X={x_1, x_2, … , x_n}$，而模型可以看做有两个节点，每个节点都会有一个输出，分别代表着对输入为Logo和Other的判断，但这里的输出暂时还不是概率，只是模型输出的一组数值。这一部分内容如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/03/29/0322747253dbbffe80a92004ea12be29.png?wh=1732x660\" alt=\"图片\"></p><p>上图这个结构其实就是感知机了，中间绿色的节点叫做神经元，是感知机的最基本组成单元。上图中的感知机只有中间一层（绿色的神经元），如果有多层神经元的话，我们就称之为多层感知机。</p><p>那什么是神经元呢？神经元是关于输入的一个线性变换，每一个输入x都会有一个对应的权值，上图中的y的计算方式为：</p><p>$$y_i=\\delta(w_{i1}x_{1} + w_{i2}x_{2} + … + w_{i_n}x_{n} + b_i), \\space \\space \\space  i=1,2$$</p><p>其中，$w_{i1}, w_{i2}, …, w_{in}$是神经元的权重，$b_i$为神经元的偏移项。权重与偏移项都是通过模型学习到的参数。$\\delta$为激活函数，激活函数是一个可选参数。</p><p>那如何将一组数值，也就是$y_{1}$与$y_{2}$转换为一组对应的概率呢？这个时候Softmax函数就要登场了。它的作用就是将一组数值转换为对应的概率，概率和为1。</p><p>Softmax的计算公式如下：</p><p>$$\\delta(x_j) = \\frac{e^{x_j}}{\\sum_{j=1}^{m}e^{x_j}}$$</p><p>请看下面的代码，我们用Softmax函数对原始的输入y做个转化，将y中的数值转化为一组对应的概率：</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\n# 2个神经元的输出y的数值为\ny = torch.randn(2)\nprint(y)\n输出：tensor([0.2370, 1.7276])\nm = nn.Softmax(dim=0)\nout = m(y)\nprint(out)\n输出：tensor([0.1838, 0.8162])\n</code></pre><p>你看，经过Softmax之后，原始的输出y是不是转换成一组概率，并且概率的和为1呢。原始y中最大的y具有最大的概率。</p><p>当然，Softmax也不是每一个问题都会使用。我们根据问题的不同可以采用不同的函数，例如，有的时候也会使用sigmoid激活函数，sigmoid激活函数是将1个数值转换为0到1之间的概率。</p><p>现在，我们将上述的过程补充到前面的模型里，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/92/32/92ec877yy99cd31b5c9c9fc46f78c832.png?wh=1708x608\" alt=\"图片\"></p><h3>全连接层</h3><p>其实，上面那张示意图，就是图像的分类原理了。其中绿色那一层。在卷积神经网络中称为<strong>全连接层，Full Connection Layer，简称fc层。一般都是放在网络的最后端</strong>，用来获得最终的输出，也就是各个类别的概率。</p><p>因为全连接层中的神经元的个数是固定的，所以说在有全连接层的网络中，输入图片是必须固定尺寸的。而现实里我们线上收集到的图片会有不同的尺寸，所以需要先把图片尺寸统一起来，PyTorch才能进一步处理。</p><p>我们假设将前面的输入图片resize到128x128，然后看看全连接层推断的过程在PyTorch中是如何实现的。</p><pre><code class=\"language-python\">x = torch.randint(0, 255, (1, 128*128), dtype=torch.float32)\nfc = nn.Linear(128*128, 2)\ny = fc(x)\nprint(y)\n输出：tensor([[&nbsp; 72.1361, -120.3565]], grad_fn=&lt;AddmmBackward&gt;)\n# 注意y的shape是(1, 2)\noutput = nn.Softmax(dim=1)(y)\nprint(output)\n输出：tensor([[1., 0.]], grad_fn=&lt;SoftmaxBackward&gt;)\n</code></pre><p>结合代码不难看出，PyTorch中全连接层用nn.Linear来实现。我们分别看看里面的重要参数有哪些：</p><ul>\n<li>in_features：输入特征的个数，在本例中为128x128；</li>\n<li>out_features：输出的特征数，在本例中为2；</li>\n<li>bias：是否需要偏移项，默认为True。</li>\n</ul><p>全连接层的输入，也不是原始图片数据，而是经过多层卷积提取的特征。</p><p>前面我们曾说过，有的网络是可以接收任意尺度的输入的。在上文中的设计中，全连接层的输入x1到xn是固定的，数目等于最后一层特征图所有元素的数目。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/af/5b/af7f9971ea5564d93c0a0089d3f5d75b.png?wh=1490x678\" alt=\"图片\"></p><p>我们将上述结构稍作调整，就可以接收任意尺度的输入了。只需要在最后的特征图后面加一个全局平均即可，也就是将每个特征图进行求平均，用平均值代替特征图，这样无论输入的尺度是多少，进入全连接层的数据量都是固定的。</p><p>如下图所示，黄色的圈就是全局平均的结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/e0/e0a62554422d28601af056809873d8e0.png?wh=1760x730\" alt=\"图片\"></p><p>我们下一节课介绍的EfficientNet就是采用这种方式，使得网络可以使用任意尺度的图片进行训练。</p><h2>卷积神经网络</h2><p>其实刚才说的多层感知机就是卷积神经网络的前身，由于自身的缺陷（参数量大、难以训练），使其在历史上有段时间一直是停滞不前，直到卷积神经网络的出现，打破了僵局。</p><p>卷积神经网络的最大作用就是提取出输入图片的丰富信息，然后再对接上层的一些应用，比如前面提到的图片分类。把卷积神经网络应用到图像分类原理中，得到的模型如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/1a/8bbc16d51yydca581cb1d88274ec161a.png?wh=1728x664\" alt=\"图片\"></p><p>你需要注意的是示意图中各个层的定义，不同层有不同的名称。</p><p>在上图中，<strong>整个模型或者网络的重点全都在卷积神经网络那块，所以这也是我们的工作重点</strong>。</p><p>那如何找到一个合适的卷积神经网络呢？在实际工作中，我们几乎不会自己去设计一个神经网络网的（因为不可控的变量太多），而是直接选择一些大神设计好的网络直接使用。那网络模型那么多，我们如何验证大神们提出的网络确实是可靠、可用的呢？</p><h3>ImageNet</h3><p>在业界中有个标杆——ImageNet，大家都用它来评价提出模型的好与坏。</p><p>ImageNet本身包含了一个非常大的数据集，并且从2010年开始，每年都会举办一次著名的ImageNet 大规模视觉识别挑战赛（The ImageNet Large Scale Visual Recognition Challenge ，ILSVRC），比赛包含了图像分类、目标检测与图像分割等任务。</p><p>其中，图像分类比赛使用的数据集是一份有1000个类别的庞大数据集，只要能在这个比赛中脱颖而出的模型，都是我们所说的经典网络结构，这些网络在实际项目中基本都是我们的首选。</p><p>从2012年开始，伴随着深度学习的发展，几乎每一年都有非常经典的网络结构诞生，下表为历年来ImageNet上Top-5的错误率。</p><p><img src=\"https://static001.geekbang.org/resource/image/da/73/da4f8fe982d066b8541f63231d257c73.jpg?wh=1920x818\" alt=\"\"></p><p>你可能会有疑问，了解这么多网络模型真的有必要么？</p><p>我想说的是，磨刀不误砍柴工，<strong>机器学习这个领域始终是依靠研究驱动的。</strong>工作当中，我们很少从0到1自创一个网络模型，常常是在经典设计基础上做一些自定义配置，所以你最好对这些经典网络都有所了解。</p><p>接下来，我们就挑选几个经典的神经网络来看看。</p><h3>VGG</h3><p><a href=\"https://arxiv.org/abs/1409.1556\">VGG</a>取得了ILSVRC 2014比赛分类项目的第2名和定位项目的第1名的优异成绩。</p><p>当年的VGG一共提供了A到E6种不同的VGG网络（字母不同，只是表示层数不一样）。VGG19的效果虽说最好，但是综合模型大小等指标，在实际项目中VGG16用得更加多一点。具体的网络结构你可以看看论文。</p><p>我们来看看VGG突破的一些重点：</p><ol>\n<li>证明了随着模型深度的增加，模型效果也会越来越好。</li>\n<li>使用较小的3x3的卷积，代替了AlexNet中的11x11、7x7以及5x5的大卷积核。</li>\n</ol><p>关于第二点，VGG中将5x5的卷积用2层3x3的卷积替换；将7x7的卷积用3层3x3的卷积替换。这样做首先可以减少网络的参数，其次是可以在相同感受野的前提下，加深网络的层数，从而提取出更加多样的非线性信息。</p><h3>GoogLeNet</h3><p>2014年分类比赛的冠军是<a href=\"https://arxiv.org/abs/1409.4842\">GoogLeNet</a>（VGG同年）。GoogLeNet的核心是Inception模块。这个时期的Inception模块是v1版本，后续还有v2、v3以及v4版本。</p><p>我们先来看看GoogLeNet解决了什么样的问题。研究人员发现，对于同一个类别的图片，主要物体在不同图片中，所占的区域大小均有不同，例如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/a7/c4bed5998c8yy9d4e4661c8a5520fba7.jpg?wh=2561x992\" alt=\"\"></p><p>如果使用AlexNet或者VGG中标准的卷积的话，每一层只能以相同的尺寸的卷积核来提取图片中的特征。</p><p>但是正如上图所示，很可能物体以不同的尺寸出现在图片中，那么能否以不同尺度的卷积来提取不同的特征呢？沿着这个想法，Inception模块应运而生，如下图示：</p><p><img src=\"https://static001.geekbang.org/resource/image/91/a1/911eee05256f145209fae76d3yy23fa1.png?wh=2718x1298\" alt=\"\" title=\"图片来源：https://arxiv.org/abs/1409.4842\"></p><p>结合图示我们发现，这里是将原来的相同尺寸卷积提取特征的方式拆分为，使用1x1、3x3、5x5以及3x3的max pooling同时进行特征提取，然后再合并到一起。这样就做到了以<strong>多尺度的方式</strong>提取图片中的特征。</p><p>作者为了降低网络的计算成本，将上述的Inception模块做了一步改进，在3x3、5x5之前与pooling之后添加了1x1卷积用来降维，从而获得了Inception模块的最终形态。</p><p><img src=\"https://static001.geekbang.org/resource/image/8f/21/8fd81403acd0d70fb5ae4a857177ee21.png?wh=2700x1312\" alt=\"\" title=\"图片来源：https://arxiv.org/abs/1409.4842\"></p><p>这里有个额外的小知识点，如果是面试，经常会被问到为什么采用1x1的卷积或者1x1卷积的作用。1x1卷积的作用就是用来升维或者降维的。</p><p>GooLeNet就是由以上的Inception模块构成的一个22层网络。别看网络层数有22层，但是它参数量却比AlexNet与VGG都要少，这带来的优势就是，搭建起来的模型就很小，占的存储空间也小。具体的网络结构你可以参考它的论文。</p><h3>ResNet</h3><p>ResNet中文意思是残差神经网络。在2015年的ImageNet比赛中，模型的分类能力首次超越人眼，1000类图片top-5的错误率降低到3.57%。</p><p>在<a href=\"https://arxiv.org/abs/1512.03385\">论文</a>中作者给出了18层、34层、50层、101层与152层的ResNet。101层的与152层的残差神经网络效果最好，但是受硬件设备以及推断时间的限制，50层的残差神经网络在实际项目中更为常用。</p><p>具体的网络结构你感兴趣的话可以自己看看论文全文，这里我着重带你看看这个网络的主要突破点。</p><h4>网络退化问题</h4><p>虽说研究已经证明，随着网络深度的不断增加，网络的整体性能也会提升。如果只是单纯的增加网络，就会引起以下两个问题：第一，模型容易过拟合；第二，产生梯度消失、梯度爆炸的问题。</p><p>虽然随着研究的不断发展，以上两个问题都可以被解决掉，但是ResNet网络的作者发现，以上两个问题被规避之后，简单的堆叠卷积层，依然不能获得很好的效果。</p><p>为了验证刚才的观点，作者做了这样的一个实验。通过搭建一个普通的20层卷积神经网络与一个56层的卷积神经网络，在CIFAR-10数据集上进行了验证。无论训练集误差还是测试集误差，56层的网络均高于20层的网络。下图来源于论文。</p><p><img src=\"https://static001.geekbang.org/resource/image/85/75/8503d95991270ea2d4a3ff80622af375.png?wh=2784x966\" alt=\"\" title=\"图片来源：https://arxiv.org/abs/1512.03385\"></p><p>出现这样的情况，作者认为这是网络退化造成的。</p><p>网络退化是指当一个网络可以开始收敛时，随着网络层数的增加，网络的精度逐渐达到饱和，并且会迅速降低。这里精度降低的原因并不是过拟合造成的，因为如果是过拟合，上图中56层的在训练集上的精度应该高于20层的精度。</p><p>作者认为这一现象并不合理，假设20层是一个最优的网络，通过加深到56层之后，理论上后面的36层是可以通过学习到一个恒等映射的，也就是说理论上不会学习到一个比26层还差的网络。所以，作者猜测网络不能很容易地学习到恒等映射(恒等映射就是f(x)=x)。</p><h4>残差学习</h4><p>正如刚才所说，从网络退化问题中可以发现，通过简单堆叠卷积层似乎很难学会到恒等映射。为了改善网络退化问题，论文作者何凯明提出了一种深度残差学习的框架。</p><p>因为网络不容易学习到恒等映射，所以就让它强制添加一个恒等映射，如下图所示（下图来源于论文）。</p><p><img src=\"https://static001.geekbang.org/resource/image/27/3b/27c8c4a22782ab29e77c36d0131f5e3b.png?wh=2034x806\" alt=\"\" title=\"图片来源：https://arxiv.org/abs/1512.03385\"></p><p>具体实现是通过一种叫做shortcut connection的机制来完成的。在残差神经网络中shortcut connection就是恒等变换，就是上图中带有x identity的那条曲线，包含shortcut connection的几层网络我们称之为残差块。</p><p>残差块被定义为如下形式：</p><p>$$y = F(x, W_i) + x$$</p><p>F可以是2层的卷积层。也可以是3层的卷积层。最后作者发现，通过残差块，就可以训练出更深、更加优秀的卷积神经网络了。</p><h2>小结</h2><p>恭喜你完成了这节课的学习，让我们回顾一下这节课的主要内容。</p><p>首先我们从多层感知机说起，带你认识了这个卷积神经网络的前身。之后我们一起推导出了图像分类原理的基础模型。<strong>你需要注意的是，整个模型或者网络的重点全都在卷积神经网络那块，所以这也是我们的工作重点</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/1a/8bbc16d51yydca581cb1d88274ec161a.png?wh=1728x664\" alt=\"图片\"></p><p>之后我们结合业界标杆ImageNet的评选情况，一起学习了一些经典的网络结构：VGG、GoogLeNet、ResNet。这里为了让你快速抓住重点，我是从每个网络解决了什么问题，各自有什么突破点展开的。也建议你课余时间多读读相关论文，做更为详细深入的了解。</p><p>纵观网络结构的发展，我们不难发现，一直都是长江后浪推前浪，一代更比一代强。掌握了这些网络结构，你就是深度学习未来的弄潮儿。下节课我们再一起实践一个图像分类项目，加深你对图像分类的理解，敬请期待。</p><h2>思考题</h2><p>欢迎推荐一下近几年来，你自己觉得比较不错的神经网络模型。</p><p>欢迎你在留言区跟我交流互动，也推荐你把这节课分享给更多的同事、朋友。</p>","neighbors":{"left":{"article_title":"16｜分布式训练：如何加速你的模型训练？","id":445886},"right":{"article_title":"18 | 图像分类（下）：如何构建一个图像分类模型?","id":447503}}},{"article_id":447503,"article_title":"18 | 图像分类（下）：如何构建一个图像分类模型?","article_content":"<p>你好，我是方远。欢迎来到第18节课的学习。</p><p>我相信经过上节课的学习，你已经了解了图像分类的原理，还初步认识了一些经典的卷积神经网络。</p><p>正所谓“纸上得来终觉浅，绝知此事要躬行”，今天就让我们把上节课的理论知识应用起来，一起从数据的准备、模型训练以及模型评估，从头至尾一起来完成一个完整的图像分类项目实践。</p><p>课程代码你可以从<a href=\"https://github.com/syuu1987/geekTime-image-classification\">这里</a>下载。</p><h2>问题回顾</h2><p>我们先来回顾一下问题背景，我们要解决的问题是，在众多图片中自动识别出极客时间Logo的图片。想要实现自动识别，首先需要分析数据集里的图片是啥样子的。</p><p>那我们先来看一张包含极客时间Logo的图片，如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/2c/1d221d4d170c54625dc8d124bcc6df2c.jpeg?wh=1242x2209\" alt=\"图片\"></p><p>你可以看到，Logo占整张图片的比例还是比较小的，所以说，如果这个项目是真实存在的，目标检测其实更加合适。不过，我们可以将问题稍微修改一下，修改成自动识别极客时间宣传海报，这其实就很适合图像分类任务了。</p><h2>数据准备</h2><p>相比目标检测与图像分割来说，图像分类的数据准备还是比较简单的。在图像分类中，我们只需要将每个类别的图片放到指定的文件夹里就行了。</p><p>下图是我的图片组织方式，文件夹就是图片所属的类别。</p><p><img src=\"https://static001.geekbang.org/resource/image/cf/8e/cf664db8d071979583a7cec69a45168e.png?wh=922x334\" alt=\"图片\"></p><p>logo文件夹中存放的是10张极客时间海报的图片。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/27/460af80104ec4550ff1b745a1f9f6627.png?wh=1516x704\" alt=\"图片\"></p><p>而others中，理论上应该是各种其它类型的图片，但这里为了简化问题，我这个文件夹中存放的都是小猫的图片。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/e6/b0/e6275aac026ce5d626c1e6ebb1fde9b0.png?wh=1494x480\" alt=\"图片\"></p><h2>模型训练</h2><p>好啦，数据准备就绪，我们现在进入模型训练阶段。</p><p>今天我想向你介绍一个在最近2年非常受欢迎的一个网络——EfficientNet。它为我们提供了B0～B7，一共8个不同版本的模型，这8个版本有着不同的参数量，在同等参数量的模型中，它的精度都是首屈一指的。因此，这8个版本的模型可以解决你的大多数问题。</p><h3>EfficientNet</h3><p>我先给你解读一下<a href=\"https://arxiv.org/pdf/1905.11946.pdf\">EfficientNet</a>的这篇论文，这里我着重分享论文的核心思路还有我的理解，学有余力的同学可以在课后自行阅读原文。</p><p>EfficientNet一共有B0到B7，8个模型，参数量由少到多，精度也越来越高，具体你可以看看后面的评价指标。</p><p>在之前的那些网络，要么从网络的深度出发，要么从网络的宽度出发来优化网络的性能，但从来没有人将这些方向结合在一起考虑。<strong>而EfficientNet就做了这样的尝试，它探索了网络深度、网络宽度、图像分辨率之间的最优组合</strong>。</p><p>EfficientNet利用一种复合的缩放手段，对网络的深度depth、宽度width和分辨率resolution同时进行缩放（按照一定的缩放规律），来达到精度和运算复杂度FLOPS的权衡。</p><p>但即使只探索这三个维度，搜索空间仍然很大，所以作者规定只在B0（作者提出的EfficientNet的一个Baseline）上进行放大。</p><p>首先，作者比较了单独放大这三个维度中的任意一个维度效果如何。得出结论是放大网络深度或网络宽度或图像分辨率，均可提升模型精度，但是越放大，精度增加越缓慢，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/7f/64/7ff4750599323623bb148ed8b2222b64.png?wh=1920x591\" alt=\"图片\"></p><p>然后，作者做了第二个实验，尝试在不同的r（分辨率），d（深度）组合下变动w（宽度），得到下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/de/46/dec67f3868ddcc44e503yy13a09c1e46.png?wh=1734x1310\" alt=\"图片\"></p><p>结论是，得到更高的精度以及效率的关键是平衡网络宽度，网络深度，图像分辨率三个维度的缩放倍率(d, r, w)。</p><p>因此，作者提出了混合维度放大法，该方法使用一个$\\phi$（混合稀疏）来决定三个维度的放大倍率。</p><p>深度depth：$d = \\alpha ^{\\phi}$</p><p>宽度width：$w = \\beta ^{\\phi}$</p><p>分辨率resolution: $r = \\gamma ^{\\phi}$</p><p>$$s.t. \\space \\alpha\\cdot\\beta^2\\cdot\\gamma^2 \\approx2 \\space \\space \\alpha \\geq1,\\beta \\geq1,\\gamma \\geq1$$</p><p>第一步，固定$\\phi$为1，也就是计算量为2倍，使用网格搜索，得到了最佳的组合，也就是$\\alpha=1.2, \\beta = 1.1, \\gamma = 1.15$。</p><p>第二步，固定$\\alpha=1.2, \\beta = 1.1, \\gamma = 1.15$，使用不同的混合稀疏$\\phi$，得到了B1~B7。</p><p>整体评估效果如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/03/14/037cd03be0995f97caa71ba079078814.png?wh=1682x1324\" alt=\"图片\"></p><p>从评估结果上可以看到，EfficientNet的各个版本均超过了之前的一些经典卷积神经网络。</p><p>EfficientNet v2也已经被提出来了，有时间的话你可以自己去看看。</p><p>我们不妨借助一下EfficientNet的<a href=\"https://github.com/lukemelas/EfficientNet-PyTorch\">GitHub</a>，它里面有训练ImageNet的demo(demo/imagenet/main.py)，接下来我们一起看看它的核心代码，然后精简一下代码，把它运行起来(Torchvision也提供了EfficientNet的模型，课后你也可以自己试一试)。</p><p>这里我们再回顾一下，之前说的机器学习3件套：</p><p>1.数据处理<br>\n2.模型训练（构建模型、损失函数与优化方法）<br>\n3.模型评估</p><p>接下来我们就挨个看看这些步骤。你需要先把<a href=\"https://github.com/lukemelas/EfficientNet-PyTorch\">https://github.com/lukemelas/EfficientNet-PyTorch</a>给克隆下来，我们只使用efficientnet_pytorch中的内容，它包含着模型的网络结构。</p><p>之后我们来创建一个叫做geektime的项目文件夹，然后把efficientnet_pytorch放进去。</p><p>在开始之前，我先把程序需要的参数给你列一下，在下面的讲解中，我们就直接使用这些参数了。当你在实现今天代码的时候，需要将这些参数补充到代码中（可以使用argparsem模块）。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/7a/df24d6aa865b645f1d2aa50716e7d17a.jpg?wh=1739x1027\" alt=\"图片\"></p><p>好，下面让我们正式开始动手。</p><h3>加载数据</h3><p>首先是数据加载的环节，我们创建一个dataset.py文件，用来存储与数据有关的内容。dataset.py如下（我省略了模块的引入）。</p><pre><code class=\"language-python\">\n# 作者给出的标准化方法\ndef _norm_advprop(img):\n&nbsp; &nbsp; return img * 2.0 - 1.0\n\ndef build_transform(dest_image_size):\n&nbsp; &nbsp; normalize = transforms.Lambda(_norm_advprop)\n\n&nbsp; &nbsp; if not isinstance(dest_image_size, tuple):\n&nbsp; &nbsp; &nbsp; &nbsp; dest_image_size = (dest_image_size, dest_image_size)\n&nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; dest_image_size = dest_image_size\n\n&nbsp; &nbsp; transform = transforms.Compose([\n&nbsp; &nbsp; &nbsp; &nbsp; transforms.RandomResizedCrop(dest_image_size),\n&nbsp; &nbsp; &nbsp; &nbsp; transforms.RandomHorizontalFlip(),\n&nbsp; &nbsp; &nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; &nbsp; &nbsp; normalize\n&nbsp; &nbsp; ])\n\n&nbsp; &nbsp; return transform\n\ndef build_data_set(dest_image_size, data):\n&nbsp; &nbsp; transform = build_transform(dest_image_size)&nbsp;\n&nbsp; &nbsp; dataset=datasets.ImageFolder(data, transform=transform, target_transform=None)&nbsp;\n\n&nbsp; &nbsp; return dataset\n\n</code></pre><p>这部分代码完成的工作是，通过build_data_set构建数据集。这里我们使用了torchvision.datasets.ImageFolder来创建Dataset。ImageFolder能将按文件夹形式的组织数据生成到一个Dataset。</p><p>在这个例子中，我传入的训练集路径为’./data/train’，你可以看看开篇的截图。</p><p>ImageFolder会自动的将同一文件夹内的数据打上一个标签，也就是说logo文件夹的数据，ImageFolder会认为是来自同一类别，others文件夹的数据，ImageFolder会认为是来自另外一个类别。</p><p>我们这个精简版只构建了训练集的Dataset，当你看Efficient官方代码的时候，在验证集的构建过程中，你需要留意一下验证集的<a href=\"https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/examples/imagenet/main.py#L240-L245\">transforms</a>。</p><p>我认为，这里这么做是有点问题的，原因是Resize中size参数如果是个tuple类型，则直接按照size的尺寸进行resize。如果是一个int的时候，如果图片的height大于width，则按照(size * height/width, size)进行resize。</p><p>在作者的原始程序中，imag_size是个int，而不是tuple。所以按照这种先resize再crop的方式处理一下，对长宽比比较大的图片来说，效果不是很好。</p><p>让我们实际验证一下这个想法，我将开篇的例子（也就是那张海报图）的image_size设定为224后，用上述的方式进行处理后，获得下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/e2/a93417ee476234249d0e69fb5c5f04e2.jpg?wh=224x224\" alt=\"图片\"></p><p>你看，是不是缺少了很多信息？</p><p>所以，如果在我们的例子中使用作者的程序，就需要做一下修改。把这里的代码逻辑修改为如果image_size不是tuple，先将image_size转换为tuple，并且也不需要crop了。代码如下所示：</p><pre><code class=\"language-python\">if not isinstance(image_size, tuple):\n&nbsp; &nbsp; image_size = (image_size, image_size)\nelse:\n&nbsp; &nbsp; image_size = image_size\n\ntransform = transforms.Compose([\n&nbsp; &nbsp; transforms.Resize(image_size, interpolation=Image.BICUBIC),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; normalize,\n])\n</code></pre><p>训练的主程序我们定义在main.py中，在main.py中的main()中，进行数据的加载，如下所示。</p><p>然后，我们通过for循环一个一个Epoch的调用train方法进行训练就可以了。</p><pre><code class=\"language-python\"># 省略了一些模块的引入\nfrom efficientnet import EfficientNet\nfrom dataset import build_data_set\n\ndef main():\n    # part1: 模型加载 (稍后补充)\n    # part2: 损失函数、优化方法(稍后补充)&nbsp; &nbsp; \n    train_dataset = build_data_set(args.image_size, args.train_data)\n\n&nbsp; &nbsp; train_loader = torch.utils.data.DataLoader(\n&nbsp; &nbsp; &nbsp; &nbsp; train_dataset,&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; batch_size=args.batch_size,\n&nbsp; &nbsp; &nbsp; &nbsp; shuffle=True,\n&nbsp; &nbsp; &nbsp; &nbsp; num_workers=args.workers,\n&nbsp; &nbsp; &nbsp; &nbsp; )\n\n&nbsp; &nbsp; for epoch in range(args.epochs):\n&nbsp; &nbsp; &nbsp; &nbsp; # 调用train函数进行训练，稍后补充\n&nbsp; &nbsp; &nbsp; &nbsp; train(train_loader, model, criterion, optimizer, epoch, args)\n        # 模型保存&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; if epoch % args.save_interval == 0:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if not os.path.exists(args.checkpoint_dir):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; os.mkdir(args.checkpoint_dir)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; torch.save(model.state_dict(), os.path.join(args.checkpoint_dir,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'checkpoint.pth.tar.epoch_%s' % epoch))\n</code></pre><h3>创建模型</h3><p>接下来，我们来看看如何创建模型，这一步我们直接使用作者给出的Efficient模型。在上面代码注释中的part1部分，用下述代码即可加载EfficientNet模型。</p><pre><code class=\"language-python\">&nbsp; &nbsp; args.classes_num = 2\n    if args.pretrained:\n&nbsp; &nbsp; &nbsp; &nbsp; model = EfficientNet.from_pretrained(args.arch, num_classes=args.classes_num,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; advprop=args.advprop)\n&nbsp; &nbsp; &nbsp; &nbsp; print(\"=&gt; using pre-trained model '{}'\".format(args.arch))\n&nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; print(\"=&gt; creating model '{}'\".format(args.arch))\n&nbsp; &nbsp; &nbsp; &nbsp; model = EfficientNet.from_name(args.arch, override_params={'num_classes': args.classes_num})\n    # 有GPU的话，加上cuda()\n    #mode.cuda()\n</code></pre><p>这段代码是说，如果pretrained model参数为True，则自动下载并加载pretrained model后进行训练，否则是使用随机数初始化网络。<br>\nfrom_pretrained与from_name中，都需要修改一下num_classes，将EfficientNet的全连接层修改我们项目对应的类别数，这里的args.classes_num为2（logo类与others类）。</p><h4>模型微调</h4><p>模型微调在<a href=\"https://time.geekbang.org/column/article/431420\">第8节课</a>和<a href=\"https://time.geekbang.org/column/article/442442\">第14节课</a>时说过，这个概念比较重要，我们一起再复习一下。</p><p>Pretrained model一般是在ImageNet（也有可能是COCO或VOC，都是公开数据集）上训练过的模型，我们可以直接把它在ImageNet上训练好的模型参数直接拿过来，在其基础上训练我们自己的模型，这就是模型微调。</p><p>所以说，<strong>如果有Pretrained model，我们一定会使用Pretrained model进行训练，收敛速度会快</strong>。</p><p>使用Pretrained model的时候要注意一点，在ImageNet上训练后的全连接层一共有1000个节点，所以使用Pretrained model的时候只使用全连接层以外的参数。</p><p>在上述代码的EfficientNet.from_pretrained中，会<s>通</s>调用load_pretrained_weights函数，调用之前num_classes已经被修改为2（logo与others），所以说传入load_pretrained_weights的load_fc参数为False，也就是说不会加载全连接层的参数。load_pretrained_weights的调用如下所示：</p><pre><code class=\"language-python\">load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000), advprop=advprop)\n</code></pre><p>load_pretrained_weights函数中包含下面这段代码，就像刚才所说，如果不加载全连接层，则删除_fc的weight与bias：</p><pre><code class=\"language-python\">if load_fc:\n    ret = model.load_state_dict(state_dict, strict=False)\n    assert not ret.missing_keys, 'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\nelse:\n&nbsp; &nbsp; state_dict.pop('_fc.weight')\n&nbsp; &nbsp; state_dict.pop('_fc.bias')\n&nbsp; &nbsp; ret = model.load_state_dict(state_dict, strict=False)\n</code></pre><h3>设定损失函数与优化方法</h3><p>最后要做的就是设定损失函数与优化方法了，我们将下面的代码补充到part2部分：</p><pre><code class=\"language-python\">criterion = nn.CrossEntropyLoss() # 有GPU的话加上.cuda()\n\noptimizer = torch.optim.SGD(model.parameters(), args.lr,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; momentum=args.momentum,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; weight_decay=args.weight_decay)\n</code></pre><p>到这里，我们就完成训练的所有准备了，只要再补充好train函数就可以了，代码如下。下面的代码的原理我们在<a href=\"https://time.geekbang.org/column/article/438639\">第13节课</a>中已经讲过了，记不清的可以去回顾一下。</p><pre><code class=\"language-python\">def train(train_loader, model, criterion, optimizer, epoch, args):\n&nbsp; &nbsp; # switch to train mode\n&nbsp; &nbsp; model.train()\n\n&nbsp; &nbsp; for i, (images, target) in enumerate(train_loader):\n&nbsp; &nbsp; &nbsp; &nbsp; # compute output\n&nbsp; &nbsp; &nbsp; &nbsp; output = model(images)\n&nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(output, target)\n&nbsp; &nbsp; &nbsp; &nbsp; print('Epoch ', epoch, loss)\n\n&nbsp; &nbsp; &nbsp; &nbsp; # compute gradient and do SGD step\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n</code></pre><p>不过在我的程序里，保存了若干个Epoch的模型，我们应该怎么选择呢？这就要说到模型的评估环节。</p><h2>模型评估</h2><p>对于分类模型的评估来说，有很多评价指标，例如准确率、精确率、召回率、F1-Score等。其中，<strong>我认为最直观、最有说服力的就是精确率与召回率</strong>，这也是我在项目中观察的主要是指标。下面我们依次来看看。</p><h3>混淆矩阵</h3><p>在讲解精确率与召回率之前，我们先看看混淆矩阵这个概念。其实精确率与召回率就是通过它计算出来的。下表就是一个混淆矩阵，正例就是logo类，负例就是others类。</p><p><img src=\"https://static001.geekbang.org/resource/image/57/8b/5756d1fe45493d69ayy534da3d20088b.jpg?wh=1920x847\" alt=\"图片\"></p><p>根据预测结果和真实类别的组合，一共有四种情况：</p><p>1.TP是说真实类别为Logo，模型也预测为Logo；<br>\n2.FP是说真实类别为Others，但模型预测为Logo；<br>\n3.FN是说真实类别为Logo，但模型预测为Others；<br>\n4.TN是说真实类别为Others，模型也预测为Others；</p><p>精确率的计算方法为：</p><p>$$precision = \\frac{TP}{ (TP + FP)}$$</p><p>召回率的计算方式为：</p><p>$$recall = \\frac{TP}{(TP + FN)}$$</p><p>精确率与召回率分别衡量了模型的不同表现，精确率说的是，如果模型认为一张图片是Logo类，那有多大概率是Logo类。而召回率衡量的是，在整个验证集中，模型能找到多少Logo图片。</p><p>那问题来了，怎样根据这两个指标来选择模型呢？业务需求不同，我们侧重的指标就不一样。</p><p>比如在我们的这个项目中，如果老板允许一部分Logo图片没有被识别，但是模型必须非常准，模型说一张图片是Logo类，那图片真实类别就有非常大的概率是Logo类图片，那应该侧重的就是精确率；如果老板希望把线上Logo类尽可能地识别出来，允许一部分图片被误识别，那应该侧重的就是召回率。</p><p>在计算精确率与召回率的时候，给你分享一下我的经验。在实际项目中，我习惯把模型对每张图片的预测结果保存到一个txt中，这样可以比较直观地筛选一些模型的badcase，并且验证集如果非常大，又需要调整的时候，直接更改txt就可以了，不需要再次让模型预测整个验证集。</p><p>下面是txt文件的一部分，分别记录了logo类的概率、others类的概率、真实类别是否为logo、真实类别是否为others、预测类别是否为logo、预测类别是否为ohters、图片名。</p><p>14.jpeg是开篇例子的那张图片，模型认为它是Logo的概率是0.58476，others类的概率是0.41524。</p><pre><code class=\"language-python\">...\n0.64460 0.35540 1 0 1 0 ./data/val/logo/13.jpeg\n0.58476 0.41524 1 0 1 0 ./data/val/logo/14.jpeg\n...\n</code></pre><p>下图是我训练了10个Epoch的B0模型，在验证集(这里我用训练集充当了一下验证集)上的评价效果。<br>\n<img src=\"https://static001.geekbang.org/resource/image/95/00/95a4b9f3e9eddb32b3bc30e85dfa2500.png?wh=966x730\" alt=\"图片\"></p><p>通过混淆矩阵可以看到，整个验证集一共有8+0张图片被预测为logo类，所以logo类的精确率为8 / (8 + 0 ) = 1；logo类一共有8+2张图片，有两张预测错了，所以召回率为8 / (8 +2) = 0.8。</p><p>others类别的计算类似，你可以自己算算看。</p><h2>小结</h2><p>恭喜你，完成了今天的学习任务。今天我们一起完成了一个图像分类项目的实践。虽然项目规模较小，但是在真实项目中的每一个环节都包含在内了，可以说是麻雀虽小，五脏俱全。</p><p>下面我们回顾一下每个环节上的关键要点和实操经验。</p><p><strong>数据准备其实是最关键的一步，数据的质量直接决定了模型好坏</strong>。所以，在开始训练之前你应该对你的数据集有十足的了解才可以。例如，验证集还是否可以反映出训练集、数据中有没有脏数据、数据分布有没有偏等等。</p><p>完成数据准备之后就到了模型训练，图像分类任务其实基本上都是采用主流的卷积神经网络了，很少对模型结构做一些更改。</p><p>最后的模型评估环节要侧重业务场景，看业务上需要高精确还是高召回，然后再对你的模型做调整。</p><h2>思考题</h2><p>老板希望你的模型能尽可能的把线上所有极客时间的海报都找到，允许一些误召回。训练模型的时候你应该侧重精确率还是召回率？</p><p>推荐你动手实现一下今天的Demo，也欢迎你把这节课分享给更多的同事、朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"17 | 图像分类（上）：图像分类原理与图像分类模型","id":446645},"right":{"article_title":"19 | 图像分割（上）：详解图像分割原理与图像分割模型","id":450898}}},{"article_id":450898,"article_title":"19 | 图像分割（上）：详解图像分割原理与图像分割模型","article_content":"<p>你好，我是方远。</p><p>在前两节课我们完成了有关图像分类的学习与实践。今天，让我们进入到计算机视觉另外一个非常重要的应用场景——图像分割。</p><p>你一定用过或听过腾讯会议或者Zoom之类的产品吧?在进行会议的时候，我们可以选择对背景进行替换，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/30/82/3066670d30116f462e54fd50376f5882.png?wh=1282x878\" alt=\"图片\" title=\"图片来源：https://tech.qq.com/a/20200426/002647.htm\"></p><p>在华为手机中也曾经有过人像留色的功能。</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/ec/b33ecbc167f2bbf66902cb35cc9e3eec.png?wh=1272x862\" alt=\"图片\" title=\"图片来源：https://www.sohu.com/a/294693393_264578\"></p><p>这些应用背后的实现都离不开今天要讲的图像分割。</p><p>我们同样用两节课的篇幅进行学习，这节课主攻分割原理，下节课再把这些技能点活用到实战上，从头开始搭建一个图像分割模型。</p><h2>图像分割</h2><p>我们不妨用对比的视角，先从概念理解一下图像分割是什么。图像分类是将一张图片自动分成某一类别，而图像分割是需要将图片中的每一个像素进行分类。</p><p>图像分割可以分为语义分割与实例分割，两者的区别是语义分割中只需要把每个像素点进行分类就可以了，不需要区分是否来自同一个实例，而实例分割不仅仅需要对像素点进行分类，还需要判断来自哪个实例。</p><p>如下图所示，左侧为语义分割，右侧为实例分割。我们这两节课都会以语义分割来展开讲解。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/81/75d04920aa9208d0108fd4e35332e281.png?wh=1622x540\" alt=\"图片\"></p><h2>语义分割原理</h2><p>语义分割原理其实与图像分类大致类似，主要有两点区别。首先是分类端（这是我自己起的名字，就是经过卷积提取特征后分类的那一块）不同，其次是网络结构有所不同。先看第一点，也就是分类端的不同。</p><!-- [[[read_end]]] --><h3>分类端</h3><p>我们先回想一下图像分类的原理。你可以结合下面的示意图做理解。</p><p><img src=\"https://static001.geekbang.org/resource/image/39/8a/39a58yy024069706401e741b63bd808a.jpg?wh=1699x914\" alt=\"图片\"></p><p>输入图片经过卷积层提取特征后，最终会生成若干特征图，然后在这些特征图之后会接一个全连接层（上图中红色的圆圈），全连接层中的节点数就对应着要将图片分为几类。我们将全连接层的输出送入到softmax中，就可以获得每个类别的概率，然后通过概率就可以判断输入图片属于哪一个类别了。</p><p>在图像分割中，同样是利用卷积层来提取特征，最终生成若干特征图。只不过最后生成的特征图</p><p>的数目对应着要分割成的类别数。举一个例子，假设我们想要将输入的小猫分割出来，也就是说，这个图像分割模型有两个类别，分别是小猫与背景，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/66/325b9e7e91200c1e9fba3e6a985dbc66.jpg?wh=1392x875\" alt=\"图片\"></p><p>最终的两个特征图中，通道1代表的小猫的信息，通道2对应着背景的信息。</p><p>这里我给你再举一个例子，来说明一下如何判断每个像素的类别。假设，通道1中（0,0）这个位置的输出是2，通道2中（0,0）这个位置的输出是30。</p><p>经过softmax转为概率后，通道1（0, 0）这个位置的概率为0，而对应通道2中(0,0)这个位置的概率为1，我们通过概率可以判断出，在（0，0）这个位置是背景，而不是小猫。</p><h3>网络结构</h3><p>在分割网络中最终输出的特征图的大小要么是与输入的原图相同，要么就是接近输入。</p><p>这么做的原因是，我们要对原图中的每个像素进行判断。当输出特征图与原图尺寸相同时，可以直接进行分割判断。当输出特征图与原图尺寸不相同时，需要将输出的特征图resize到原图大小。</p><p>如果是从一个比较小的特征图resize到一个比较大的尺寸的时候，必定会丢失掉一部分信息的。所以，输出特征图的大小不能太小。</p><p>这也是图像分割网络与图像分类网络的第二个不同点，在图像分类中，经过多层的特征提取，最后生成的特征图都是很小的。而<strong>在图像分割中，最后生成的特征图通常来说是接近原图的</strong>。</p><p>前文也说过，图像分割网络也是通过卷积进行提取特征的，按照之前的理论特征提取后，特征图尺寸是减小的。如果说把特征提取看做Encoder的话，那在图像分割中还有一步是Decoder。</p><p>Decoder的作用就是对特征图尺寸进行还原的过程，将尺寸还原到一个比较大的尺寸。这个还原的操作对应的就是上采样。而在上采样中我们通常使用的是转置卷积。</p><h4>转置卷积</h4><p>接下来我就带你研究一下转置卷积的计算原理，这也是这节课的重点内容。</p><p>我们看下面图这个卷积计算，padding为0，stride为1。</p><p><img src=\"https://static001.geekbang.org/resource/image/ac/c7/ac5dfca8d13e88b3e78fb3f8b34016c7.jpg?wh=1520x865\" alt=\"图片\"></p><p>从之前的学习我们可以知道，卷积操作是一个多对一的运算，输出中的每一个y都与输入中的4个x有关。其实，转置卷积从逻辑上是卷积的一个逆过程，而<strong>不是卷积的逆运算。</strong></p><p>也就是说，转置卷积并不是使用上图中的输出Y与卷积核Kernel来获得上图中的输入X，转置卷积只能还原出一个与输入特征图<strong>尺寸</strong>相同的特征图。</p><p>我们将转置卷积中的卷积核用k’表示，那么一个y会与四个k’进行还原，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/9d/2d/9d24b6a621f2b74648248dbce723a52d.jpg?wh=1593x745\" alt=\"图片\"></p><p>还原尺寸的过程如下所示，下图中每个还原后的结果都对应着原始3x3的输入。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/f1/1c18d2dbc330062410fddbcc911f78f1.jpg?wh=1920x1080\" alt=\"\"></p><p>通过观察你可以发现，有些部分是重合的，对于重合部分把它们加起来就可以了，最终还原后的特征图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/41/01/41833998c1510d0a6c1fe239a0557101.jpg?wh=1920x1110\" alt=\"图片\"></p><p>将上图的结果稍作整理，整理为下面的结果，也没有做什么特殊处理，只是补了一些零：</p><p><img src=\"https://static001.geekbang.org/resource/image/b7/0f/b7078cdd1ce155d1bb853ab2a88c000f.jpg?wh=1920x1163\" alt=\"\"></p><p>上面的结果，我们又可以通过下面的卷积获得：<br>\n<img src=\"https://static001.geekbang.org/resource/image/64/bb/640099fe3138893e0a9f6941c0d49bbb.jpg?wh=1920x980\" alt=\"\"></p><p>你有没有发现一件很神奇的事情，转置卷积计算又变回了卷积计算。</p><p>所以，我们一起梳理一下，转置卷积的计算过程如下：</p><p>1.对输入特征图进行补零操作。<br>\n2.将转置卷积的卷积核上下、左右变换作为新的卷积核。<br>\n3.利用新的卷积核在1的基础上进行<strong>步长为1</strong>，<strong>padding为0</strong>的卷积操作。</p><p>我们先来看一下，PyTorch中转置卷积以及它的主要参数，再根据参数解释一下第一步1是如何补零的。</p><pre><code class=\"language-python\">class torch.nn.ConvTranspose2d(in_channels, \n                               out_channels, \n                               kernel_size, \n                               stride=1, \n                               padding=0,\n                               groups=1,\n                               bias=True,\n                               dilation=1)\n</code></pre><p>其中，in_channels、out_channels、kernel_size、groups、bias以及dilation与我们之前讲卷积时的参数含义是一样的（你可以回顾卷积的第<a href=\"https://time.geekbang.org/column/article/432042\">9</a>、<a href=\"https://time.geekbang.org/column/article/433801\">10</a>两节课），这里我们就不赘述了。</p><p>首先，我们看一下stride。因为转置卷积是卷积的一个逆向过程，所以这里的stride指的是在原图上的stride。</p><p>在我们刚才的例子里，stride是等于1的，如果等于2时，按照同样的套路，可以转换为如下的卷积变换。 同时，我们也可以得到结论，上文中第一步，补零的操作是，在输入的特征图的行与列之间补stride-1个行与列的零。</p><p><img src=\"https://static001.geekbang.org/resource/image/39/ca/3913cdcc21afe55ccb8511951c4512ca.jpg?wh=1920x1011\" alt=\"图片\"></p><p>再来看padding操作，padding是指要在输入特征图的周围补dilation * (kernel_size - 1) - padding圈零。这里用到了dliation参数，但是通常在转置卷积中dilation、groups参数使用的比较少。</p><p>以上就是转置卷积的补零操作了，图片和文字双管齐下，我相信你一定能够理解它。</p><p>通过上述的讲解，我们可以推导出输出特征图尺寸与输入特征图尺寸的关系：</p><p>$$h_{out} = (h_{in} - 1) * stride[0] -  padding[0] + kernel\\_size[0]$$</p><p>$$w_{out} = (w_{in} - 1) * stride[1] -  padding[1] + kernel\\_size[1]$$</p><p>下面，我们借助代码来验证一下，我们讲的转置卷积是否是向我们所说的那样计算。</p><p>现在有特征图input_feat:</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport numpy as np\ninput_feat = torch.tensor([[[[1, 2], [3, 4]]]], dtype=torch.float32)\ninput_feat\n输出：\ntensor([[[[1., 2.],\n          [3., 4.]]]])\n</code></pre><p>卷积核k：</p><pre><code class=\"language-python\">kernels = torch.tensor([[[[1, 0], [1, 1]]]], dtype=torch.float32)\nkernels\n输出：\ntensor([[[[1., 0.],\n          [1., 1.]]]])\n          \n</code></pre><p>stride为1，padding为0的转置卷积：</p><pre><code class=\"language-python\">convTrans = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=1, padding=0, bias = False)\nconvTrans.weight=nn.Parameter(kernels)\n</code></pre><p>按照我们刚才讲的，第一步是补零操作，输入的特征图补零后为：</p><p>$$input\\_feat = \\begin{bmatrix}<br>\n0 &amp; 0 &amp; 0 &amp; 0 \\\\\\<br>\n0 &amp; 1 &amp; 2 &amp; 0 \\\\\\<br>\n0 &amp; 3 &amp; 4 &amp; 0 \\\\\\<br>\n0 &amp; 0 &amp; 0 &amp; 0\\\\\\<br>\n\\end{bmatrix} $$</p><p>然后再与变换后的卷积核：<br>\n$$\\begin{bmatrix}<br>\n1 &amp; 1  \\\\\\<br>\n0 &amp; 1<br>\n\\end{bmatrix}$$<br>\n做卷积运算后，获得输出：<br>\n$$output = \\begin{bmatrix}<br>\n1 &amp; 2 &amp; 0  \\\\\\<br>\n4 &amp; 7 &amp; 2  \\\\\\<br>\n3 &amp; 7 &amp; 4<br>\n\\end{bmatrix} $$</p><p>我们再看看代码的输出，如下所示：</p><pre><code class=\"language-python\">convTrans(input_feat)\n输出：\ntensor([[[[1., 2., 0.],\n          [4., 7., 2.],\n          [3., 7., 4.]]]], grad_fn=&lt;SlowConvTranspose2DBackward&gt;)\n</code></pre><p>你看看是不是一样呢？</p><h3>损失函数</h3><p>说完网络结构，我们再开启图像分割里的另一个话题：损失函数。</p><p>在图像分割中依然可以使用在图像分类中经常使用的交叉熵损失。在图像分类中，一张图片有一个预测结果，预测结果与真实值就可以计算出一个Loss。而在图像分割中，真实的标签是一张二维特征图，这张特征图记录着每个像素的真实分类结果。在分割中，含有像素类别的特征图，我们一般称为Mask。</p><p>我们结合一张小猫图片的例子解释一下。对于下图中的小猫进行标记，标记后会生成它的GT，这个GT就是一个Mask。</p><p>GT是Ground Truth的缩写，在图像分割中我们经常使用这个词。在图像分类中与之对应的就是数据的真实标签，在图像分割中则GT是每个像素的真实分类，如下面的例子所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpg?wh=1024x640\" alt=\"图片\"></p><p>GT如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/1a/a0/1a35623ceccb0750cd8058568d847fa0.png?wh=1024x640\" alt=\"图片\"></p><p>那在我们模型预测的Mask中，每个位置都会有一个预测结果，这个预测结果与GT中的Mask做比较，然后会生成一个Loss。</p><p>当然，在图像分割中不光有交叉熵损失可以用，还可以用更加有针对性的Dice Loss，下节课我再继续展开。</p><h4>公开数据集</h4><p>刚才我们也看到了，图像分割的数据标注还是比较耗时的，具体如何标注一张语义分割所需要的图片，下节课我们再一起通过实践探索。</p><p>除此之外业界也有很多比较有权威性且质量很高的公开数据集。最著名的就是COCO了，链接如下：<a href=\"https://cocodataset.org/#detection-2016\">https://cocodataset.org/#detection-2016</a>。一共有80个类别，超过2万张图片。感兴趣的话，课后你可以尝试着使用它训练来看看。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/f7/6027c11940b8e0205b182505a371c0f7.png?wh=1598x348\" alt=\"图片\"></p><h2>小结</h2><p>恭喜你完成了今天的学习。</p><p>今天我们首先明确了语义分割要解决的问题是什么，它可以对图像中的每个像素都进行分类。</p><p>然后我们对比图像分类原理，说明了语义分割的原理。它与图像分类主要有两个不同点：</p><p>1.在分类端有所不同，在图像分类中，经过卷积的特征提取后，最后会以若干个神经元的形式作为输出，每个神经元代表着对一个类别的判断情况。而语义分割，则是会输出若干的特征图，每个特征图代表着对应类别判断。</p><p>2.在图像分类的网络中，特征图是不断减小的。但是在语义分割的网络中，特征图还会有decoder这一步，它是将特征图进行放大的过程。实现decoder的方式称为上采样，在上采样中我们最常使用的就是转置卷积。</p><p>对于转置卷积，我们除了要知道它是怎么计算的之外，最重要的是要记住<strong>它不是卷积的逆运算，只是能将特征图的大小进行放大的一种卷积运算</strong>。</p><h2>每课一练</h2><p>对于本文的小猫分割问题，最终只输出1个特征图是否可以？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这节课分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"18 | 图像分类（下）：如何构建一个图像分类模型?","id":447503},"right":{"article_title":"20 | 图像分割（下）：如何构建一个图像分割模型？","id":455415}}},{"article_id":455415,"article_title":"20 | 图像分割（下）：如何构建一个图像分割模型？","article_content":"<p>你好，我是方远。</p><p>在上一节课中，我们掌握了图像分割的理论知识，你是不是已经迫不及待要上手体验一下，找找手感了呢？</p><p>今天我们就从头开始，来完成一个图像分割项目。项目的内容是，对图片中的小猫进行语义分割。为了实现这个项目，我会引入一个简单但实用的网络结构：UNet。通过这节课的学习，你不但能再次体验一下完整机器学习的模型实现过程，还能实际训练一个语义分割模型。</p><p>课程代码你可以从<a href=\"https://github.com/syuu1987/geekTime-semantic-segmentation/tree/main\">这里</a>下载。</p><h2>数据部分</h2><p>我们还是从机器学习开发三件套：数据、训练、评估说起。首先是数据准备部分，我们先对训练数据进行标记，然后完成数据读取工作。</p><h3>分割图像的标记</h3><p>之前也提到过，图像分割的准备相比图像分类的准备更加复杂。那我们如何标记语义分割所需要的图片呢？在图像分割中，我们使用的每张图片都要有一张与之对应的Mask，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpg?wh=1024x640\" alt=\"图片\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/1a/a0/1a35623ceccb0750cd8058568d847fa0.png?wh=1024x640\" alt=\"图片\"></p><p>上节课我们说过，Mask就是含有像素类别的特征图。结合这里的示例图片，我们可以看到，Mask就是原图所对应的一张图片，它的每个位置都记录着原图每个位置对应的像素类别。对于Mask的标记，我们需要使用到Labelme工具。</p><p>标记的方法一共包括七步，我们挨个看一下。</p><p>第一步，下载安装<a href=\"https://github.com/wkentaro/labelme\">Labelme</a>。我们按照Github中的安装方式进行安装即可。如果安装比较慢的话，你可以使用国内的镜像（例如清华的）进行安装。</p><!-- [[[read_end]]] --><p>第二步，我们要将需要标记的图⽚放到⼀个⽂件夹中。这里我是将所有猫的图片放入到cats文件夹中了。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/04/f3c8cc99959c74f363ec290558a51d04.png?wh=1722x882\" alt=\"图片\"></p><p>第三步，我们事先准备好⼀个label.txt的⽂件，⾥⾯每⼀⾏写好的需要标记的类别。我的label.txt如下：</p><pre><code class=\"language-python\">__ignore__\n_background_\ncat\n</code></pre><p>这里我要提醒你的是，前两行最好这么写。不这样写的话，使用label2voc.py转换就会报错，但label2voc.py不是唯一的数据转换方式（还可以使用labelme_json_to_dataset，但推荐你使用label2voc.py）。从第三行开始，表示要标记的类别。</p><p>第四步，执行后面的这条命令，就会自动启动Labelme。</p><pre><code class=\"language-python\">labelme --labels labels.txt --nodata\n</code></pre><p>第五步，点我们击左侧的Open Dir，选择第二步中的文件夹，就会自动导入需要标记的图片。在右下角选择需要标记的文件后，会自动显示出来，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/81/0d37591417e44d51a21bac11f409c381.png?wh=1780x1470\" alt=\"图片\"></p><p>第六步：点击左侧的Create Polygons。就可以开始标注了。标记的方式就是将小猫沿着它的边界给圈出来，当形成一个闭环的时候，Labelme会自动提示你输入类别，我们选择cat类即可。</p><p>标记成功后，结果如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/88/y6/888dc5b576ccc9629cd1f3fd2d9cbyy6.png?wh=1676x1438\" alt=\"图片\"></p><p>当标记完成后，我们需要保存一下，保存之后会生成标记好的json文件。如下所示：</p><pre><code class=\"language-python\">fangyuan@geektime data $ ls cats\n1.jpeg&nbsp; 1.json&nbsp; 10.jpeg 10.json 2.jpeg&nbsp; 3.jpeg&nbsp; 4.jpeg&nbsp; 4.json\n</code></pre><p>第七步，执行下面的代码，将标记好的数据转换成Mask。</p><pre><code class=\"language-python\">python label2voc.py cats cats_output --label label.txt&nbsp;\n</code></pre><p>上面代码里用到的label2voc.py，你可以通过后面这个链接获取它：<a href=\"https://github.com/wkentaro/labelme/blob/main/examples/semantic_segmentation/labelme2voc.py\">https://github.com/wkentaro/labelme/blob/main/examples/semantic_segmentation/labelme2voc.py</a>。</p><p>其中，cats为标记好的数据，cats_output为输出文件夹。在cats_output下会自动生成4个文件夹，我们只需要两个文件夹，分别是JPEGImages（训练原图）与SegmentationClassPNG（转换后的Mask）。</p><p>到此为止，我们的数据就准备好了。我一共标记了8张图片，如下所示。当然了，在实际的项目中需要大量标记好的图片，这里主要是为了方便演示。</p><p><img src=\"https://static001.geekbang.org/resource/image/7c/1d/7ca1ecafd3ce893610c5eb89yy7ca51d.png?wh=686x285\" alt=\"图片\"></p><p><img src=\"https://static001.geekbang.org/resource/image/53/67/53e088956a41de56ea1010af8a2a6d67.png?wh=703x324\" alt=\"图片\"></p><p>到此为止，标记工作宣告完成。</p><h3>数据读取</h3><p>完成了标记工作之后，我们就要用PyTorch把这些数据给读入进来了，我们把数据相关的写在dataset.py中。具体还是和之前讲的一样，要继承Dataset类，然后实现__init__、__len__和__getitem__方法。</p><p>dataset.py的代码如下所示，我已经在代码中写好注释了，相信结合注释你很容易就能领会意思。</p><pre><code class=\"language-python\">import os\nimport torch\nimport numpy as np\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image \n\n\nclass CatSegmentationDataset(Dataset):\n    \n    # 模型输入是3通道数据\n    in_channels = 3\n    # 模型输出是1通道数据\n    out_channels = 1\n\n    def __init__(\n        self,\n        images_dir,\n        image_size=256,\n    ):\n\n        print(\"Reading images...\")\n        # 原图所在的位置\n        image_root_path = images_dir + os.sep + 'JPEGImages'\n        # Mask所在的位置\n        mask_root_path = images_dir + os.sep + 'SegmentationClassPNG'\n        # 将图片与Mask读入后，分别存在image_slices与mask_slices中\n        self.image_slices = []\n        self.mask_slices = []\n        for im_name in os.listdir(image_root_path):\n            # 原图与mask的名字是相同的，只不过是后缀不一样\n            mask_name = im_name.split('.')[0] + '.png' \n\n            image_path = image_root_path + os.sep + im_name\n            mask_path = mask_root_path + os.sep + mask_name\n\n            im = np.asarray(Image.open(image_path).resize((image_size, image_size)))\n            mask = np.asarray(Image.open(mask_path).resize((image_size, image_size)))\n            self.image_slices.append(im / 255.)\n            self.mask_slices.append(mask)\n\n    def __len__(self):\n        return len(self.image_slices)\n\n    def __getitem__(self, idx):\n\n        image = self.image_slices[idx] \n        mask = self.mask_slices[idx] \n\n        # tensor的顺序是（Batch_size, 通道，高，宽）而numpy读入后的顺序是(高，宽，通道)\n        image = image.transpose(2, 0, 1)\n        # Mask是单通道数据，所以要再加一个维度\n        mask = mask[np.newaxis, :, :]\n\n        image = image.astype(np.float32)\n        mask = mask.astype(np.float32)\n\n        return image, mask\n</code></pre><p>然后，我们的训练代码写在train.py中，train.py中的main函数为主函数，在main中，我们会调用data_loaders来加载数据。代码如下所示：</p><pre><code class=\"language-python\">import torch\n\nfrom torch.utils.data import DataLoader \nfrom torch.utils.data import DataLoader\nfrom dataset import CatSegmentationDataset as Dataset\n\ndef data_loaders(args):\n    dataset_train = Dataset(\n        images_dir=args.images,\n        image_size=args.image_size,\n    )\n\n    loader_train = DataLoader(\n        dataset_train,\n        batch_size=args.batch_size,\n        shuffle=True,\n        num_workers=args.workers,\n    )\n\n    return loader_train\n\n# args是传入的参数\ndef main(args):\n    loader_train = data_loaders(args)\n</code></pre><p>以上就是数据处理的全部内容了。接下来，我们再来看看模型训练部分的内容。</p><h2>模型训练</h2><p>我们先来回忆一下，模型训练的老三样，分别是网络结构、损失函数和优化方法。</p><p>先从网络结构说起，今天我要为你介绍一个叫做UNet的语义分割网络。</p><h3>网络结构：UNet</h3><p><a href=\"https://arxiv.org/pdf/1505.04597.pdf\">UNet</a>是一个非常实用的网络。它是一个典型的Encoder-Decoder类型的分割网络，网络结构非常简单，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/11/b9/1196c6fcff2fe8c601f608b01bf82ab9.jpg?wh=1920x1130\" alt=\"图片\" title=\"图片来自论文：https://arxiv.org/pdf/1505.04597.pdf\"></p><p>它的网络结构虽然简单，但是效果并不“简单”，我在很多项目中都用它与一些主流的语义分割做对比，而UNet都取得了非常好的效果。</p><p>整体网络结构跟论文中给出的示意图一样，我们重点去关注几个实现细节。</p><p>第一点，图中横向蓝色的箭头，它们都是重复的相同结构，都是由两个3x3的卷积层组合而成的，在每层卷积之后会跟随一个BN层与ReLU的激活层。按照<a href=\"https://time.geekbang.org/column/article/442442\">第14节课</a>讲的，这一部分重复的组织是可以单独提取出来的。我们先来创建一个unet.py文件，用来定义网络结构。</p><p>现在unet.py中创建Block类，它是用来定义刚才所说的重复的卷积块：</p><pre><code class=\"language-python\">class Block(nn.Module):\n\n&nbsp; &nbsp; def __init__(self, in_channels, features):\n&nbsp; &nbsp; &nbsp; &nbsp; super(Block, self).__init__()\n\n&nbsp; &nbsp; &nbsp; &nbsp; self.features = features\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv1 = nn.Conv2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; in_channels=in_channels,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; out_channels=features,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kernel_size=3,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; padding='same',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv2 = nn.Conv2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; in_channels=features,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; out_channels=features,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kernel_size=3,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; padding='same',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n\n&nbsp; &nbsp; def forward(self, input):\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.conv1(input)\n&nbsp; &nbsp; &nbsp; &nbsp; x = nn.BatchNorm2d(num_features=self.features)(x)\n&nbsp; &nbsp; &nbsp; &nbsp; x = nn.ReLU(inplace=True)(x)\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.conv2(x)\n&nbsp; &nbsp; &nbsp; &nbsp; x = nn.BatchNorm2d(num_features=self.features)(x)\n&nbsp; &nbsp; &nbsp; &nbsp; x = nn.ReLU(inplace=True)(x)\n\n&nbsp; &nbsp; &nbsp; &nbsp; return x\n</code></pre><p>这里需要注意的是，同一个块内，特征图的尺寸是不变的，所以padding为same。</p><p>第二点，就是绿色向上的箭头，也就是上采样的过程。这块的实现就是采用上一节课所讲的转置卷积来实现的。</p><p>最后一点，我们现在是要对小猫进行分割，也就是说一共有两个类别——猫与背景。对于二分类的问题，我们可以直接输出一张特征图，然后通过概率来进行判断是正例（猫）还是负例（背景），也就是下面代码中的第71行。同时，下述代码也补全了unet.py中的所有代码。</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\nclass Block(nn.Module):\n    ...\nclass UNet(nn.Module):\n\n&nbsp; &nbsp; def __init__(self, in_channels=3, out_channels=1, init_features=32):\n&nbsp; &nbsp; &nbsp; &nbsp; super(UNet, self).__init__()\n\n&nbsp; &nbsp; &nbsp; &nbsp; features = init_features\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_encoder_1 = Block(in_channels, features)\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_encoder_2 = Block(features, features * 2)\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_encoder_3 = Block(features * 2, features * 4)\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_encoder_4 = Block(features * 4, features * 8)\n\n&nbsp; &nbsp; &nbsp; &nbsp; self.bottleneck = Block(features * 8, features * 16)\n\n&nbsp; &nbsp; &nbsp; &nbsp; self.upconv4 = nn.ConvTranspose2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; features * 16, features * 8, kernel_size=2, stride=2\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_decoder_4 = Block((features * 8) * 2, features * 8)\n&nbsp; &nbsp; &nbsp; &nbsp; self.upconv3 = nn.ConvTranspose2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; features * 8, features * 4, kernel_size=2, stride=2\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_decoder_3 = Block((features * 4) * 2, features * 4)\n&nbsp; &nbsp; &nbsp; &nbsp; self.upconv2 = nn.ConvTranspose2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; features * 4, features * 2, kernel_size=2, stride=2\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv_decoder_2 = Block((features * 2) * 2, features * 2)\n&nbsp; &nbsp; &nbsp; &nbsp; self.upconv1 = nn.ConvTranspose2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; features * 2, features, kernel_size=2, stride=2\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; self.decoder1 = Block(features * 2, features)\n\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv = nn.Conv2d(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; in_channels=features, out_channels=out_channels, kernel_size=1\n&nbsp; &nbsp; &nbsp; &nbsp; )\n\n&nbsp; &nbsp; def forward(self, x):\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_1_1 = self.conv_encoder_1(x)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_1_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_1_1)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_2_1 = self.conv_encoder_2(conv_encoder_1_2)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_2_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_2_1)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_3_1 = self.conv_encoder_3(conv_encoder_2_2)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_3_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_3_1)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_4_1 = self.conv_encoder_4(conv_encoder_3_2)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_encoder_4_2 = nn.MaxPool2d(kernel_size=2, stride=2)(conv_encoder_4_1)\n\n&nbsp; &nbsp; &nbsp; &nbsp; bottleneck = self.bottleneck(conv_encoder_4_2)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_4_1 = self.upconv4(bottleneck)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_4_2 = torch.cat((conv_decoder_4_1, conv_encoder_4_1), dim=1)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_4_3 = self.conv_decoder_4(conv_decoder_4_2)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_3_1 = self.upconv3(conv_decoder_4_3)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_3_2 = torch.cat((conv_decoder_3_1, conv_encoder_3_1), dim=1)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_3_3 = self.conv_decoder_3(conv_decoder_3_2)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_2_1 = self.upconv2(conv_decoder_3_3)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_2_2 = torch.cat((conv_decoder_2_1, conv_encoder_2_1), dim=1)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_2_3 = self.conv_decoder_2(conv_decoder_2_2)\n\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_1_1 = self.upconv1(conv_decoder_2_3)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_1_2 = torch.cat((conv_decoder_1_1, conv_encoder_1_1), dim=1)\n&nbsp; &nbsp; &nbsp; &nbsp; conv_decoder_1_3 = self.decoder1(conv_decoder_1_2)\n\n&nbsp; &nbsp; &nbsp; &nbsp; return torch.sigmoid(self.conv(conv_decoder_1_3))\n</code></pre><p>到这里，网络结构我们就搭建好了，然后我们来我看看损失函数。</p><h3>损失函数：Dice Loss</h3><p>这里我们来看一下语义分割中常用的损失函数，Dice Loss。</p><p>想要知道这个损失函数如何生成，你需要先了解一个语义分割的评价指标（但更常用的还是后面要讲的的mIoU），它就是Dice系数，常用于计算两个集合的相似度，取值范围在0-1之间。</p><p>Dice系数的公式如下。</p><p>$$Dice=\\frac{2|P\\cap G|}{|P|+|G|}$$</p><p>其中，$|P\\cap G|$是集合P与集合G之间交集元素的个数，$|P|$和$|G|$分别表示集合P和G的元素个数。分子的系数2，这是为了抵消分母中P和G之间的共同元素。对语义分割任务而言，集合P就是预测值的Mask，集合G就是真实值的Mask。</p><p>根据Dice系数我们就能设计出一种损失函数，也就是Dice Loss。它的计算公式非常简单，如下所示。</p><p>$$Dice Loss=1-\\frac{2|P\\cap G|}{|P|+|G|}$$</p><p>从公式中可以看出，当预测值的Mask与GT越相似，损失就越小；当预测值的Mask与GT差异度越大，损失就越大。</p><p>对于二分类问题，GT只有0和1两个值。当我们直接使用模型输出的预测概率而不是使用阈值将它们转换为二值Mask时，这种损失函数就被称为Soft Dice Loss。此时，$|P\\cap G|$的值近似为GT与预测概率矩阵的点乘。</p><p>定义损失函数的代码如下。</p><pre><code class=\"language-python\">import torch.nn as nn\n\nclass DiceLoss(nn.Module):\n&nbsp; &nbsp; def __init__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; super(DiceLoss, self).__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.smooth = 1.0\n\n&nbsp; &nbsp; def forward(self, y_pred, y_true):\n&nbsp; &nbsp; &nbsp; &nbsp; assert y_pred.size() == y_true.size()\n&nbsp; &nbsp; &nbsp; &nbsp; y_pred = y_pred[:, 0].contiguous().view(-1)\n&nbsp; &nbsp; &nbsp; &nbsp; y_true = y_true[:, 0].contiguous().view(-1)\n&nbsp; &nbsp; &nbsp; &nbsp; intersection = (y_pred * y_true).sum()\n&nbsp; &nbsp; &nbsp; &nbsp; dsc = (2. * intersection + self.smooth) / (\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y_pred.sum() + y_true.sum() + self.smooth\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; return 1. - dsc\n</code></pre><p>其中，self.smooth是一个平滑值，这是为了防止分子和分母为0的情况。</p><h3>训练流程</h3><p>最后，我们将模型、损失函数和优化方法串起来，看下整体的训练流程，训练的代码如下。</p><pre><code class=\"language-python\">def main(args):\n&nbsp; &nbsp; makedirs(args)\n    # 根据cuda可用情况选择使用cpu或gpu\n&nbsp; &nbsp; device = torch.device(\"cpu\" if not torch.cuda.is_available() else args.device)\n    # 加载训练数据\n&nbsp; &nbsp; loader_train = data_loaders(args)\n    # 实例化UNet网络模型\n&nbsp; &nbsp; unet = UNet(in_channels=Dataset.in_channels, out_channels=Dataset.out_channels)\n&nbsp; &nbsp; # 将模型送入gpu或cpu中\n    unet.to(device)\n    # 损失函数\n&nbsp; &nbsp; dsc_loss = DiceLoss()\n    # 优化方法\n&nbsp; &nbsp; optimizer = optim.Adam(unet.parameters(), lr=args.lr)\n\n&nbsp; &nbsp; loss_train = []\n&nbsp; &nbsp; step = 0\n    # 训练n个Epoch\n&nbsp; &nbsp; for epoch in tqdm(range(args.epochs), total=args.epochs):\n&nbsp; &nbsp; &nbsp; &nbsp; unet.train()\n&nbsp; &nbsp; &nbsp; &nbsp; for i, data in enumerate(loader_train):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; step += 1\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; x, y_true = data\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; x, y_true = x.to(device), y_true.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y_pred = unet(x)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loss = dsc_loss(y_pred, y_true)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loss_train.append(loss.item())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (step + 1) % 10 == 0:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print('Step ', step, 'Loss', np.mean(loss_train))\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loss_train = []\n&nbsp; &nbsp; &nbsp; &nbsp; torch.save(unet, args.weights + '/unet_epoch_{}.pth'.format(epoch))\n</code></pre><p>需要注意的点，我都在注释中进行了说明，你可以自己看一看。其实就是我们一直说的模型训练的那几件事情：数据加载、构建网络以及迭代更新网络参数。</p><p>我用训练数据训练了若干个Epoch，同时也保存了若干个模型，保存为pth格式。到这里就完成了模型训练的整个环节，我们可以使用保存好的模型进行预测，来看看分割效果如何。</p><h2>模型预测</h2><p>现在我们要用训练生成的模型来进行语义分割，看看结果是什么样子的。</p><p>模型预测的代码如下。</p><pre><code class=\"language-python\">import torch\nimport numpy as np\n\nfrom PIL import Image\n\nimg_size = (256, 256)\n# 加载模型\nunet = torch.load('./weights/unet_epoch_51.pth')\nunet.eval()\n# 加载并处理输入图片\nori_image = Image.open('data/JPEGImages/6.jpg')\nim = np.asarray(ori_image.resize(img_size))\nim = im / 255.\nim = im.transpose(2, 0, 1)\nim = im[np.newaxis, :, :]\nim = im.astype('float32')\n# 模型预测\noutput = unet(torch.from_numpy(im)).detach().numpy()\n# 模型输出转化为Mask图片\noutput = np.squeeze(output)\noutput = np.where(output&gt;0.5, 1, 0).astype(np.uint8)\nmask = Image.fromarray(output, mode='P')\nmask.putpalette([0,0,0, 0,128,0])\nmask = mask.resize(ori_image.size)\nmask.save('output.png')\n</code></pre><p>这段代码也很好理解。首先，用torch.load函数加载模型。接着加载一张待分割的图片，并进行数据预处理。然后将处理好的数据送入模型中，得到预测值output。最后将预测值转化为可视化的Mask图片进行保存。</p><p>输入图片也就是待分割的图片，如下左图所示。最终的输出，即可视化的Mask图片如下右图所示。</p><table>\n<thead>\n<tr>\n<th style=\"text-align:left\"><img src=\"https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpeg?wh=1024x640\" alt=\"图片\"></th>\n<th style=\"text-align:left\"><img src=\"https://static001.geekbang.org/resource/image/fb/61/fbfecd56d8c31589890fcd05c7995461.png?wh=1024x640\" alt=\"图片\"></th>\n</tr>\n</thead>\n<tbody></tbody>\n</table><p>在将预测值转化为Mask图片的过程中，最终预测值的概率卡了0.5的阈值，超过阈值的像素点，在output矩阵中的值为1，表示猫的区域，没有超过阈值的像素点，在output矩阵中的值为0，表示背景区域。</p><p>为了将output矩阵输出为可视化的图像，我们使用Image.fromarray函数，将Numpy的array转化为Image格式，并将模式设置为“P”，即调色板模式。然后用putpalette函数来给Image对象上色。</p><p>其中，putpalette函数的参数是一个列表：[0, 0, 0, 0, 128, 0]，列表前三个数表示值为0的像素的RGB（[0, 0, 0]表示黑色），列表后三个数表示值为1的像素的RGB（[0, 128, 0]表示绿色）。这样，我们保存的Mask图片，黑色部分即为背景区域，绿色部分即为猫的区域。</p><p>不过，这样分开的轮廓图，可能无法让我们很直观地看出语义分割的效果。所以我们将原图和Mask合成一张图片来看看效果。具体的代码如下。</p><pre><code class=\"language-python\">image = ori_image.convert('RGBA')\nmask = mask.convert('RGBA')\n# 合成\nimage_mask = Image.blend(image, mask, 0.3)\nimage_mask.save(\"output_mask.png\")\n</code></pre><p>首先，我们将原图image和Mask图片都转换为’RGBA’带透明度的模式。然后使用Image.blend函数将两张图片合成一张图片，最后一个参数0.3表示Mask图片透明度为30%，原图的透明度为70%。<br>\n最终的结果如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/4d/7b/4d804527a87cc92aab8173da85f0ff7b.png?wh=1024x640\" alt=\"图片\"></p><p>这样我们就可以直观地看出哪些地方预测得不准确了。</p><h2>模型评估</h2><p>在语义分割中，常用的评价指标是mIoU。mIoU全称为mean Intersection over Union，即平均交并比。交并比是真实值和预测值的交集和并集之比。</p><p>真实值就是我们刚刚用labelme标注的Mask，也是Ground Truth（GT）。如下左图所示。</p><p>预测值就是模型预测出的Mask，用Prediction表示。如后面右图所示。</p><table>\n<thead>\n<tr>\n<th style=\"text-align:left\"><img src=\"https://static001.geekbang.org/resource/image/61/0b/61afb79172dfa0bd652f237fd1c5bd0b.png?wh=1024x640\" alt=\"图片\"></th>\n<th style=\"text-align:left\"><img src=\"https://static001.geekbang.org/resource/image/c3/9d/c31ec50a2a67262728a9fd8e84a1729d.png?wh=1024x640\" alt=\"图片\"></th>\n</tr>\n</thead>\n<tbody></tbody>\n</table><p>交集是指真实值与预测值的交集，如下图黄色区域所示。并集是指真实值与预测值的并集，如下图蓝色区域所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/53/29/53e81fae1ceb5f21a269c3a461c6b129.png?wh=1024x640\" alt=\"图片\"></p><p>通过上面几个图，我们很容易就能理解mIoU了。mIoU的公式如下所示。</p><p>$$mIoU=\\frac{1}{k}\\sum_{i=1}^{k}{\\frac{P\\cap G}{P\\cup G}}$$</p><p>其中，k为所有类别数，在我们的例子中，只有“cat”一类，因此k为1，我们通常不将背景计算到mIoU中；P为预测值；G是真实值。</p><h2>小结</h2><p>恭喜你，完成了今天的学习任务。这节课我们一起完成了一个图像分割项目的实践。</p><p>首先，我带你了解了图像分割的数据准备，需要使用Labelme工具为图像做标记。数据质量的好坏决定了最终模型的质量，所以你要对数据的标注好好把握。在使用Labelme标记完成之后，我们可以使用label2voc.py将json转换为Mask。</p><p>之后我们学习了一种非常高效且实用的模型–UNet，并使用PyTorch实现了其网络结构。</p><p>然后，我为你讲解了图像分割的评估指标mIoU和损失函数Dice Loss。</p><p>mIoU的公式如下：</p><p>$$mIoU=\\frac{1}{k}\\sum_{i=1}^{k}{\\frac{P\\cap G}{P\\cup G}}$$</p><p>mIoU主要是从预测结果与GT的重合度这一角度，来衡量分割模型的好与坏的，它是图像分割中经常使用的评价指标。</p><p>最后，我们使用训练好的模型进行预测，并对分割结果进行了可视化绘制。相信通过之前学习的图像分类项目与今天学习的图像分割项目，对于图像处理，你会获得更深层次的理解。</p><h2>每课一练</h2><p>你可以根据今天的内容，自己动手试试建立一个图像分割模型，然后用一张图片来测一下效果如何。</p><p>欢迎你在留言区跟我交流讨论，也推荐你把今天的内容分享给更多同事、朋友，我们下节课见。</p><h1></h1>","neighbors":{"left":{"article_title":"19 | 图像分割（上）：详解图像分割原理与图像分割模型","id":450898},"right":{"article_title":"21 | NLP基础（上）：详解自然语言处理原理与常用算法","id":460504}}},{"article_id":460504,"article_title":"21 | NLP基础（上）：详解自然语言处理原理与常用算法","article_content":"<p>你好，我是方远。</p><p>在之前的课程中，我们一同学习了图像分类、图像分割的相关方法，还通过实战项目小试牛刀，学完这部分内容，相信你已经对深度学习图像算法有了一个较为深入的理解。</p><p>然而在实际的项目中，除了图像算法，还有一个大的问题类型，就是文字或者说语言相关的算法。这一类让程序理解人类语言表达的算法或处理方法，我们统称为自然语言处理（Natural Language Processing, NLP）。</p><p>这节课，我们先来学习自然语言处理的原理和常用算法，通过这一部分的学习，以后你遇到一些常见的NLP问题，很容易就能想出自己的解决办法。不必担心原理、算法的内容太过理论化，我会结合自己的经验从实际应用的角度，为你建立对NLP的整体认知。</p><h2>NLP的应用无处不在</h2><p>NLP研究的领域非常广泛，凡是跟语言学有关的内容都属于NLP的范畴。一般来说，较为多见的语言学的方向包括：词干提取、词形还原、分词、词性标注、命名实体识别、语义消歧、句法分析、指代消解、篇章分析等方面。</p><p>看到这里，你可能感觉这些似乎有点太学术、太专业了，涉及语言的结构、构成甚至是性质方面的研究了。没错，这些都是NLP研究在语言学中的应用方面，就会给人一种比较偏研究的感觉。</p><!-- [[[read_end]]] --><p>实际上，NLP还有很多的研究内容是侧重“处理”和“应用”方面的，比如我们常见的就有：机器翻译、文本分类、问答系统、知识图谱、信息检索等等。</p><p><img src=\"https://static001.geekbang.org/resource/image/7a/69/7ac2b2fac77ac640e0bf67785b677769.jpg?wh=1920x1416\" alt=\"图片\"></p><p>我举一个例子，你就知道自然语言处理有多么重要了。平时我们经常会用搜索引擎，当你打开网页、在搜索框中输入自己想要了解的关键词之后，搜索引擎的后台算法逻辑就要开始一整套非常复杂的算法逻辑，这其中包括几个比较重要的方面，我们不妨结合例子来看看。</p><p>在搜索引擎的输入框中，输入“亚洲的动wu”文本，显示的内容如下图所示。别看只是一次简单的检索动作，搜索系统要完成的工作可不少。</p><p><img src=\"https://static001.geekbang.org/resource/image/17/f6/171aae97ee0b55eb1ce229d9b8e751f6.png?wh=1920x991\" alt=\"图片\"></p><p>首先，搜索引擎要对你输入的内容（query）进行解析，这就涉及到了之前提到的分词、命名实体识别、语义消歧等内容，当然还涉及到了query纠错，因为你错误地输入了拼音而非汉字，需要改写成正确的形式。</p><p>通过一系列的算法之后，系统识别出你的需求是：寻找动物相关的搜索结果，这些结果的限定条件是它们要生活在亚洲。</p><p>接着，系统就开始在数据库（或者是存储的集群中）搜索相关的实体，这些实体的查询和限制条件的过滤，就涉及信息检索、知识图谱等内容。</p><p>最后，细心的同学对照搜索结果会发现，有的时候搜索引擎除了提供严格匹配的检索结果之外，还会提供一些相关内容的扩展结果，比如广告、新闻、视频等。而且很多搜索引擎的扩展搜索结果页都是个性化的，也就是根据用户的特点行为提供推荐，这些让我们的搜索结果更加丰富，体验更好。</p><p>仅仅只有这些了么？不，远远没有，因为刚才的这个过程，只是针对你这一个用户的一次检索所需要完成的一部分工作而已。更多的工作，实际是用户开始使用搜索引擎之前的构建准备阶段。</p><p>为了构建搜索引擎，就需要对存储的内容进行解析，这就包括了篇章理解、文本处理、图片识别、音视频算法等环节，对每一个网页（内容）进行特征的提取，构建检索库、知识库等，这个工作量就会非常的大，涉及的面也非常广泛。</p><p>由此可见，NLP的应用真的深入到了互联网业务的方方面面，掌握了NLP的相关算法将会使我们的竞争力变得更强。接下来，针对自然语言处理的“应用”方面，我们一起聊聊NLP中文场景下的一些重要内容。</p><h2>NLP的几个重要内容</h2><p>想要让程序对文本内容进行理解，我们需要解决几个非常基础和重要的内容，分别是分词、文本表示以及关键词提取。</p><h3>分词</h3><p>中文跟英文最大的不同在于，英文是由一个个单词构成的，单词与单词之间有空格隔断。但是中文不一样，中文单词和单词之间除了标点符号没有别的隔断。这就给程序理解文本带来了一定的难度，分词的需求也应运而生。</p><p>尽管现在的深度学习已经对分词的依赖越来越小，可以通过Word Embedding等方式对字符（token）级的文字进行表示，但是分词的地位不会降低，单词、词组级别的文本表示仍旧有非常多的应用场景。</p><p>因为我们的学习重在快速上手和实战应用，所以为了降低你的学习成本，这个专栏里我不会专门深入讲解各种分词算法细节，而是侧重于带你理解其特点，并教你学习如何用相应的工具包实现分词过程。</p><p>目前网络上已经有了很多的开源或者免费的NLP分词工具，比如jieba、HanLP、THULAC等，包括腾讯、百度、阿里等公司也有相应的商业付费工具。</p><p>贫穷使人理智，我们今天使用免费的jieba分词来做一个分词的例子，链接你可以从<a href=\"https://pypi.org/project/jieba\">这里</a>获取。安装这个工具非常简单，只需要使用pip即可。</p><pre><code class=\"language-python\">pip install jieba\n</code></pre><p>jieba的使用也很方便，我来演示一下：</p><pre><code class=\"language-python\">import jieba\ntext = \"极客时间棒呆啦\"\n# jieba.cut得到的是generator形式的结果\nseg = jieba.cut(text)  \nprint(' '.join(seg)) \n\n# Get： 极客 时间 棒呆 啦\n</code></pre><p>其实除了分词，jieba还提供了词性标注的结果（pos）：</p><pre><code class=\"language-python\">import jieba.posseg as posseg\ntext = \"一天不看极客时间我就浑身难受\"\n# 形如pair('word, 'pos')的结果\nseg = posseg.cut(text)  \nprint([se for se in seg]) \n# Get [pair('一天', 'm'), pair('不', 'd'), pair('看', 'v'), pair('极客', 'n'), pair('时间', 'n'), pair('我', 'r'), pair('就', 'd'), pair('浑身', 'n'), pair('难受', 'v')]\n</code></pre><p>是不是非常简单？搞定了分词，我们接下来就要开始对文本进行表示了。</p><h3>文本表示的方法</h3><p>在深度学习被广泛采用之前，很多传统机器学习算法结合自身的特点，使用了各种各样的文本表示。</p><p>最经典的就是独热（One-hot）表示法了。在这种方法中，假定所有的文字一共有N个单词（也可以是字符），我们可以将每个单词赋予一个单独的序号id，那么对于任意一个单词，我们都可以采用一个N位的列表（向量）对其进行表示。在这个表示中，只需要将这个单词对应序号id的位置为1，其他位置为0即可。</p><p>我还是举个例子来帮你加深理解。比方说，我们词典大小为10000，“极客”这个单词的序号id为666，那么我们就需要建立一个10000长度的向量，并将其中的第666位置为1，其余位为0。如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/0c/fbe6c8a89087cfa15a01689ae0f28a0c.jpg?wh=1920x602\" alt=\"图片\"></p><p>这时候你就会发现，在UTF-8编码中，中文字符有两万多个，词语数量更是天文数字，那么我们仅用字符的方式，每个字就需要两万多维度的数据来表示。推算一下，如果有一篇一万字的文章，这个数据量就很可怕了。</p><p>为了进一步的压缩数据的体积，可以只使用一个向量表示文章中所有的单词，例如前面的例子，我们仍旧建立一个10000维的向量，把文章中出现过的所有单词的对应位置置为1，其余为0。</p><p><img src=\"https://static001.geekbang.org/resource/image/fa/91/fa4377a9886cff7179f6f6b12e9b6b91.jpg?wh=1920x602\" alt=\"图片\"></p><p>这样看上去，数据体积就少了很多，还有没有其他办法进一步缩减空间的占用呢？有的，例如count-based表示方法。</p><p>在这种方法中，我们采用v={index1: count1, index2: count2,…, index n: count n}的形式，对每一个出现的单词的序号id以及出现过的次数进行统计，这样一来，“极客时间”我们只需要两个k-v对的dict即可表示: {3:1, 665:1}。</p><p>这种表示方法在SVM、树模型等多个算法包中被广泛采用，因为客观来说，<strong>它确实能够大幅度地压缩空间的占用，生成起来也非常方便</strong>。但是你会发现前面这几种方法，不能表述单词的语序信息。</p><p>举个例子，“我/喜欢/你”和“你/喜欢/我”两个截然不同的意思，用前面的方法做分词的话，却会得到相同的表示结果。这时候如果搞错了，其实却是单相思的话，那岂不是很苦涩？</p><p>好在现在深度学习的使用推动了Word Embedding的发展，基本上我们都会采用该方法进行文本表示。但是还是刚才的话，这并不意味着传统的文本表示方法就过时了。在一些小规模、轻量级的文本处理场景中，它们的作用仍旧非常大。</p><p>关于文本表示中Word Embedding的部分，咱们在后续课程再展开讲解，这也是NLP深度学习的核心内容之一。</p><p>让我们回到刚才的传统文本表示方法，为了实现对单词顺序信息的记录，该怎么办呢？这时我们要解决NLP中的一个重要问题：关键词的提取。</p><h3>关键词的提取</h3><p>关键词，顾名思义，就是能够表达文本中心内容的词语。关键词提取在检索系统、推荐系统等应用中有着极重要的地位。它是文本数据挖掘领域的一个分支，所以在摘要生成、文本分类/聚类等领域中也是非常基础的环节。</p><p>关键词提取，主要分为有监督和无监督的方法，一般来说，我们采用无监督的方法较多一些，这是因为它不需要人工标注的语料，只需要对文本中的单词按照位置、频率、依存关系等信息进行判断，就可以实现关键词的识别和提取。</p><p>无监督方法一般有三种类型，基于统计特征的方法、基于词图模型的方法，以及基于主题模型的方法，我们分别来看看。</p><h4>基于统计特征的方法</h4><p>这种类型的方法最为经典的就是TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）。该方法最核心的思想非常简单：一个单词在文件中出现的次数越多，它的重要性越高；但它在语料库中出现的频率越高，它的重要性反而越小。</p><p>什么意思呢？就比如说我们有10篇文章，其中有2篇财经文章、5篇科技、3篇娱乐，对于单词“股票”，它在财经文章中的次数肯定非常多，但是在娱乐和科技中就非常少，这就意味着“股票”这个词就能够更好的“区分”文章的类别，那它的重要性自然也就非常高了。</p><p>在TF-IDF中，词频（TF）表示关键字在文本中出现的频率。而逆向文件频率 (IDF)&nbsp;是由<strong>包含该词语的文件的数目</strong>除以<strong>总文件数目</strong>得到的，一般情况下还会取对数对结果进行缩放。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/bc/d7da8269b3664f82e539e7e217730bbc.jpg?wh=1920x602\" alt=\"图片\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/e2/9d/e2259d8c5f87664689f3c0a433ed7e9d.png?wh=1920x602\" alt=\"图片\"></p><p>你可以自己先想想，这里为什么分母要加1呢？这是为了避免分母为0的情况。得到了TF和IDF之后，我们将两者相乘，就得到了TF-IDF了。</p><p>通过TF-IDF不难看出，基于统计的方法的特点在于，<strong>对单词出现的次数以及分布进行数学上的统计，从而发现相应的规律和重要性（权重），并以此作为关键词提取的依据</strong>。</p><p>跟分词一样，关键词的提取目前也有很多集成工具包，比如NLTK（Natural Language Toolkit），它是一个非常著名的自然语言处理工具包，是NLP研究领域常用的Python库。我们仍旧可以使用pip install nltk命令来进行安装。</p><p>使用NLTK来计算TF-IDF非常简单，代码如下：</p><pre><code class=\"language-python\">from nltk import word_tokenize\nfrom nltk import TextCollection\n\nsents=['i like jike','i want to eat apple','i like lady gaga']\n# 首先进行分词\nsents=[word_tokenize(sent) for sent in sents]\n\n# 构建语料库\ncorpus=TextCollection(sents)\n\n# 计算TF\ntf=corpus.tf('one',corpus)\n\n# 计算IDF\nidf=corpus.idf('one')\n\n# 计算任意一个单词的TF-IDF\ntf_idf=corpus.tf_idf('one',corpus)\n</code></pre><p>你可以执行前面这段代码，看看tf_idf等于多少？</p><h4><strong>基于词图模型的关键词提取</strong></h4><p>前面基于统计的方法采用的是对词语的频率计算的方式，但我们还可以有其他的提取思路，那就是基于词图模型的关键词提取。</p><p>在这种方法中，我们首先要构建文本一个图结构，用来表示语言的词语网络。然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，即关键词。</p><p>该类方法中最经典的就是TextRank算法了，它脱胎于更为经典的网页排序算法PageRank。关于PageRank算法，你可以参考这个wiki（<a href=\"https://en.wikipedia.org/wiki/PageRank\">戳我</a>）。戳完PageRank之后，你就会知道，PageRank算法的核心内容有两点：</p><ul>\n<li>\n<p>如果一个网页被很多其他网页链接到的话，就说明这个网页比较重要，也就是PageRank值会相对较高。</p>\n</li>\n<li>\n<p>如果一个PageRank值很高的网页，链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高。<br>\n而TextRank就非常好理解了。它跟PageRank的区别在于：</p>\n</li>\n<li>\n<p>用句子代替网页</p>\n</li>\n<li>\n<p>任意两个句子的相似性可以采用类似网页转换概率的概念计算，但是也稍有不同，TextRank用归一化的句子相似度代替了PageRank中相等的转移概率，所以在TextRank中，所有节点的转移概率不会完全相等。</p>\n</li>\n<li>\n<p>利用矩阵存储相似性的得分，类似于PageRank的矩阵M。</p>\n</li>\n<li>\n<p>TextRank的基本流程如下图所示。</p>\n</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/44/43/44c7160354da4fcc7a93ed5f7e1bfc43.jpg?wh=1920x793\" alt=\"图片\"></p><p>看上去蛮复杂的，不过没有关系，刚才提到的jieba也有了相应的集成算法。在jieba中，我们可以使用如下的函数进行提取：</p><pre><code class=\"language-python\">jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False)\n</code></pre><p>其中sentence是待处理的文本，topK是选择最重要的K个关键词，基本上你用好这两个参数就足够了。</p><h4>基于主题模型的关键词提取</h4><p>最后一种关键词提取方法就是基于主题模型的关键词提取。</p><p>主题模型，这个名字看起来就高端了很多，实际上它也是一种基于统计的模型，只不过它会“发现”文档集合中出现的抽象的“主题”，并用于挖掘文本中隐藏的语义结构。</p><p>LDA（Latent Dirichlet Allocation）文档主题生成模型，是最典型的基于主题模型的算法。有关LDA的算法的介绍，你随便搜索一下网络资料就能找到，我就不展开说了。而咱们在这节课中，将会利用已经集成好的工具包gensim来实现使用这个模型，代码也非常简单，我们一起来看一下：</p><pre><code class=\"language-python\">from gensim import corpora, models\nimport jieba.posseg as jp\nimport jieba\n\ninput_content = [line.strip() for line open ('input.txt', 'r')]\n# 老规矩，先分词\nwords_list = []\nfor text in texts:\n  words = [w.word for w in jp.cut(text)]\n  words_list.append(words)\n\n# 构建文本统计信息, 遍历所有的文本，为每个不重复的单词分配序列id，同时收集该单词出现的次数\ndictionary = corpora.Dictionary(words_list)\n\n# 构建语料，将dictionary转化为一个词袋。\n# corpus是一个向量的列表，向量的个数就是文档数。你可以输出看一下它内部的结构是怎样的。\ncorpus = [dictionary.doc2bow(words) for words in words_list]\n\n# 开始训练LDA模型\nlda_model = models.ldamodel.LdaModel(corpus=corpus, num_topics=8, id2word=dictionary, passes=10)\n</code></pre><p>在训练环节中，num_topics代表生成的主题的个数。id2word即为dictionary，它把id都映射成为字符串。passes相当于深度学习中的epoch，表示模型遍历语料库的次数。</p><h2><strong>小结</strong></h2><p>在这节课中，我带你一同了解了自然语言处理的应用场景以及三个经典的NLP基础问题。</p><p>NLP的三大经典问题包括分词、文本表示、关键词提取，正是因为这三个问题太过经典和基础，所以现在已经有了大量的集成工具供我们直接使用。</p><p>但我还是那句话，有了工具，并不意味着我们不需要理解它内部的原理，学习要知其然，更需要知其所以然，这样在实际的工作中遇到问题的时候，我们才能游刃有余地解决。</p><p>细心的你可能已经发现了，今天的课程里我们针对不同的问题使用了不同的工具包，分词我们使用了jieba，关键词的提取我们使用了gensim和NLTK，所以我希望你在课后有空的时候，也去了解一下这三个工具的具体使用和更多功能，因为它们真的很强大。</p><p>在文本表示方法中，我们留了一个小尾巴，也就是Word Embedding。随着深度学习的越来越广泛使用，词嵌入（Word Embedding）的方法也有了越来越多的算法和工具来实现。在后续的课程中，我会通过BERT的实战开发来向你介绍Word Embedding的训练生成和使用。</p><h2>每课一练</h2><p>TF-IDF有哪些缺点呢？你不妨结合它的计算过程做个梳理。</p><p>期待你在留言区跟我交流互动，也推荐你把这节课分享给身边对NLP感兴趣的同事、朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"20 | 图像分割（下）：如何构建一个图像分割模型？","id":455415},"right":{"article_title":"22 | NLP基础（下）：详解语言模型与注意力机制","id":461691}}},{"article_id":461691,"article_title":"22 | NLP基础（下）：详解语言模型与注意力机制","article_content":"<p>你好，我是方远。</p><p>在上节课中，我们一同了解了NLP任务中的几个经典问题，这些方法各有千秋，但是我们也发现，有的方法并不能很好地将文本中单词、词组的顺序关系或者语义关系记录下来，也就是说，不能很好地量化表示，也不能对语言内容不同部分的重要程度加以区分。</p><p>那么，有没有一种方法，可以把语言变成一种数学计算过程，比如采用概率、向量等方式对语言的生成和分析加以表示呢？答案当然是肯定的，这就是这节课我们要讲到的语言模型。</p><p>那如何区分语言不同部分的重要程度呢？我会从深度学习中最火热的注意力机制这一角度为你讲解。</p><h2>语言模型</h2><p>语言模型是根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。很多NLP任务中，都涉及到一个问题：对于一个确定的概念或者表达，判断哪种表示结果是最有可能的。</p><p>我们结合两个例子体会一下。</p><p>先看第一个例子，翻译文字是：今天天气很好。可能的结果是： res1 = Today is a fine day.  res2 = Today is a good day.那么我们最后要的结果就是看概率P(res1)和P(res2)哪个更大。</p><p>再比如说，问答系统提问：我什么时候才能成为亿万富翁。可能的结果有：ans1 = 白日做梦去吧。ans2 = 红烧肉得加点冰糖。那么，最后返回的答案就要选择最贴近问题内容本身的结果，这里就是前一个答案。</p><!-- [[[read_end]]] --><p>对于上面例子中提到的问题，我们很自然就会联想到，可以使用概率统计的方法来建立一个语言模型，这种模型我们称之为统计语言模型。</p><h3>统计语言模型</h3><p>统计语言模型的原理，简单来说就是计算一句话是自然语言（也就是一个正常句子）的概率。多年以来，专家学者构建出了非常多的语言模型，其中最为经典的就是基于马尔可夫假设n-gram语言模型，它也是被广泛采用的模型之一。</p><p>接下来我们先从一个简单的抽象例子出发，来了解一下什么是统计语言模型。</p><p>给定一个句子S=w1,w2,w3,…,wn，则生成该句子的概率为：p(S)=p(w1,w2,w3,w4,w5,…,wn)，再由链式法则我们可以继续得到：p(S)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)。那么这个p(S)就是我们所要的统计语言模型。</p><p>那么问题来了，你会发现从p(w1,w2,w3,w4,w5,…,wn)到p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)无非是一个概率传递的过程，有一个非常本质的问题并没有被解决，那就是语料中数据必定存在稀疏的问题，公式中的很多部分是没有统计值的，那就成了0了，而且参数量真的实在是太大了。</p><p>怎么办呢？我们观察一下后面这句话：“我们本节课将会介绍统计语言模型及其定义”。其中“定义”这个词，是谁的定义呢？是“其”的。那“其”又是谁呢？是前面的“语言模型”的。于是我们发现，<strong>对于文本中的一个词，它出现的概率，很大程度上是由这个单词前面的一个或者几个单词决定的，</strong>这就是马尔可夫假设。</p><p>有了马尔可夫假设，我们就可以把前面的公式中的p(wn|w1,w2,…,wn-1)进一步简化，简化的程度取决于你认为一个单词是由前面的几个单词所决定的，如果只由前面的一个单词决定，那它就是p(wn|wn-1)，我们称之为bigram。如果由前面两个单词决定，则变为p(wn|wn-2wn-1)，我们称之为trigram。</p><p>当然了，如果你认为单词的出现仅由其本身决定的，与其他单词无关，就变成了最简单的形式：p(wn)，我们称之为unigram（一元模型）。</p><p>那么现在我们知道了，基于马尔可夫链的统计语言模型，其核心就在于基于统计的条件概率。为了计算一个句子的生成概率，我们只需要统计每个词及其前面n个词的共现条件概率，再经过简单的乘法计算就可以得到最终结果了。</p><h3>神经网络语言模型</h3><p>ngram模型一定程度上减少了参数的数量，但是如果n比较大，或者相关语料比较少的时候，数据稀疏问题仍然不能得到很好地解决。</p><p>这就好比我们把水浒传的文本放入模型中进行统计训练，最后却问模型林冲和潘金莲的关系，这就很难回答了。</p><p>因为基于ngram的统计模型实在是收集不到两者共现的文本。这种稀疏问题靠统计肯定不行了。那么怎么办呢？这时候就轮到神经网络语言模型闪亮登场了。</p><p>其实从本质上说，神经网络语言模型也是通过ngram来进行语言的建模，但是神经网络的学习不是通过计数统计的方法，而是通过神经网络内部神经元针对数据不断更新。</p><p>具体是怎么做的呢？首先我们要定义一个向量空间，假定这个空间是一百维的，这就意味着，对于每个单词，我们可以用一个一百维的向量对其进行表示，比如V(中国)=[0.2821289, 0.171265, 0.12378123,…,0.172364]。</p><p>这样，对于任意两个单词，我们可以用距离计算的方式来评价它们之间的联系。比如我们使用cosin距离计算“中国”和“北京”两个单词的距离，就大概率要比“中国”和“西瓜”的距离要近得多。</p><p>这样做有什么好处呢？首先，词与词之间的距离可以作为两个词之间相似性的度量。其次，向量空间隐含了很多的数学计算，比如经典的V(国王)-&nbsp;V(皇后)&nbsp;= V(男人) - V(女人)&nbsp;，这让词语之间有了更多的语义上的关联。</p><p>除了维度，为了确定向量空间，我们还需要确定这个空间有多少个“点”，也就是词语的数量有多少。一般来说，我们是将语料库中出现超过一定阈值次数的单词保留，把这些留下来的单词的数量，作为空间点的量了。</p><p>我们具体看看实际的操作过程中是怎么做的。我们只需要建立一个M*N大小的矩阵，并随机初始化里面的每一个数值，其中M表示的是词语的数量，N表示词语的维度。我们把这样矩阵叫做词向量矩阵。</p><p><img src=\"https://static001.geekbang.org/resource/image/d9/b3/d96a7d9yy8961e7175d2a421160065b3.jpg?wh=1920x1219\" alt=\"图片\"></p><p>既然是随机初始化的，那么就意味着这个向量空间不能作为我们的语言模型使用。下面我们就要想办法让这个矩阵学到内容。如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/f2/33/f2a74ede4e9d3bca29c38580b1260d33.jpg?wh=1920x1205\" alt=\"图片\"></p><p>刚才我们说过，神经网络语言模型也是通过ngram来进行语言建模的。假定我们的ngram长度为n，那么我们就从词向量矩阵中找到对应的前n-1个词的向量，经过若干层神经网络（包括激活函数），将这n-1个词的向量映射到对应的条件概率分布空间中。最后，模型就可以通过参数更新的方式，学习词向量的映射关系参数，以及上下文单词出现的条件概率参数了。</p><p>简单来说就是，我们使用n-1个词，预测第n个词，并利用预测出来的词向量跟真实的词向量做损失函数并更新，就可以不断更新词向量矩阵，从而获得一个语言模型。这种类型的神经网络语言模型我们称之为<strong>前馈网络语言模型</strong>。</p><p>除了前馈网络语言模型，还有一种叫做基于LSTM的语言模型。下节课，我们将会通过LSTM完成情感分析任务的项目，进一步细化LSTM神经网络语言模型的训练过程，同时也会用到前面提到的词向量矩阵。</p><p>现在，我们回过头来比较一下统计语言模型和神经网络语言模型的区别。<strong>统计语言模型的本质是基于词与词共现频次的统计，而神经网络语言模型则是给每个词分别赋予了向量空间的位置作为表征，从而计算它们在高维连续空间中的依赖关系</strong>。相对来说，神经网络的表示以及非线性映射，更加适合对自然语言进行建模。</p><h2>注意力机制</h2><p>如果你足够细心就会发现，在前面介绍的神经网络语言模型中，我们似乎漏掉了一个点，那就是，对于一个由n个单词组成的句子来说，不同位置的单词，重要性是不一样的。因此，我们需要让模型“注意”到那些相对更加重要的单词，这种方式我们称之为注意力机制，也称作Attention机制。</p><p>既然是机制，它就不是一个算法，准确来说是一个构建网络的思路。关于注意力机制最经典的论文就是大名鼎鼎的<a href=\"https://arxiv.org/abs/1706.03762\">《Attention Is All You Need》</a>，如果你有兴趣的话，可以自行阅读这篇论文。</p><p>因为我们的专栏是以动手实践为主，重在实际应用各种机器学习的理论，所以不会对其内部的数学原理进行过多剖析，但是我们还是要知道它是怎么运作的。</p><p>我们从一个例子入手，比如“我今天中午跑到了肯德基吃了仨汉堡”。这句话中，你一定对“我”、“肯德基”、“仨”、“汉堡”这几个词比较在意，不过，你是不是没注意到“跑”字？</p><p>其实Attention机制要做的就是这件事：找到最重要的关键内容。它对网络中的输入（或者中间层）的不同位置，给予了不同的注意力或者权重，然后再通过学习，网络就可以逐渐知道哪些是重点，哪些是可以舍弃的内容了。</p><p>在前面的神经网络语言模型中，对于一个确定的单词，它的向量是固定的，但是现在不一样了，因为Attention机制，对于同一个单词，在不同语境下它的向量表达是不一样的。</p><p>下面这张图是Attention机制和RNN结合的例子。其中红色框中的是RNN的展开模式，我们可以看到，我/爱/极/客四个字的向量沿着绿色箭头的方向传递，每个字从RNN节点出来之后都会有一个隐藏状态h，也就是输入节点上面的蓝色方框，在这个过程中每个状态的权重是一样的，不分大小。</p><p>而蓝色框就是Attention机制所加入的部分，其中的每个α就是每个状态h的权重，有了这个权重，就可以将所有的状态h，加权汇总到softmax中，然后求和得到最终输出C。这个C就可以为后续的RNN判断权重，提供更多的计算依据。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/54/652e45cc9bbf0643f8fb2b3d8e692e54.jpg?wh=1920x1080\" alt=\"图片\"></p><p>你看，这个注意力机制的原理其实很简单，但是也很巧妙。只需要增加很少的参数，就可以让模型自己弄清楚谁重要谁次要。那么下面我们来看一下抽象化之后的Attention，如下图，这张图想必你应该在很多attention的相关介绍中见过了。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/72/1e496a1d9359774c05bf8452955b9172.jpg?wh=1920x844\" alt=\"图片\"></p><p>在这里输入是query(Q), key(K), value(V)，输出是attention value。跟刚才Attention与RNN结合的图类比，query就是上一个时间节点传递进来的状态Zt-1，而这个Zt-1就是上一个时间节点输出的编码。key就是各个隐藏状态h，value也是隐藏状态h（h1, h2…hn）。模型通过Q和K的匹配公式计算出权重，再同V结合就可以得到输出，这就相当于计算得到了当前的输出和所有输入的匹配度，公式如下：</p><p>$$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}(\\operatorname{sim}(Q, K)) V$$</p><p>Attention目前主要有两种，一种是soft attention，一种是hard attention。hard attention关注的是当前词附近很小的一个区域，而soft attention则是关注了更大更广的范围，也更为常用。</p><p>作为应用者，你了解到Attention的基本原理就足够使用了，因为现在已经有了很多基于Attention的预训练模型可以直接使用。如果你想了解更深入，可以跟我在留言区和交流群中进一步讨论。</p><h2>小结</h2><p>恭喜你完成了今天的学习任务。</p><p>今天我们一起学习了语言模型的基本原理，了解了注意力机制如何给模型赋能，让模型更加“善解人意”，提高它抓取文本重点内容的能力。</p><p>通过语言模型，我们可以将语言文字变成可以计算的形式，让文字之间有了更为直接的关联。有了注意力机制，我们可以让模型了解到哪些是应该被更加关注的内容，从而提高语言模型的效果。如果你对注意力机制的数学原理感兴趣，并需要更深层次的专项学习，推荐你阅读<a href=\"https://arxiv.org/abs/1706.03762\">《Attention Is All You Need》</a>这篇论文。</p><p>至此我们通过两节课了解了NLP任务中的基础问题和重要内容，接下来的课程中，我们即将迎来动手操作环节。</p><p>首先是基于LSTM的情感分析项目，通过这个项目，你可以了解语言模型的构建方法，并可以实现一个由情感感知能力构成的模型。之后，我们还会使用目前火热的Bert模型来构建一个效果非常给力的文本分类模型，敬请期待。</p><h2>每课一练</h2><p>词向量的长度多少比较合适呢？越长越好吗？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这节课分享给更多同事、朋友。</p>","neighbors":{"left":{"article_title":"21 | NLP基础（上）：详解自然语言处理原理与常用算法","id":460504},"right":{"article_title":"23 | 情感分析：如何使用LSTM进行情感分析？","id":462524}}},{"article_id":462524,"article_title":"23 | 情感分析：如何使用LSTM进行情感分析？","article_content":"<p>你好，我是方远。</p><p>欢迎来跟我一起学习情感分析，今天我们要讲的就是机器学习里的文本情感分析。文本情感分析又叫做观点提取、主题分析、倾向性分析等。光说概念，你可能会觉得有些抽象，我们一起来看一个生活中的应用，你一看就能明白了。</p><p>比方说我们在购物网站上选购一款商品时，首先会翻阅一下商品评价，看看是否有中差评。这些评论信息表达了人们的各种情感色彩和情感倾向性，如喜、怒、哀、乐和批评、赞扬等。像这样根据评价文本，由计算机自动区分评价属于好评、中评或者说差评，背后用到的技术就是情感分析。</p><p>如果你进一步观察，还会发现，在好评差评的上方还有一些标签，比如“声音大小合适”、“连接速度快”、“售后态度很好”等。这些标签其实也是计算机根据文本，自动提取的主题或者观点。</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/6f/ef69caa72565c50d98b63e20f499ea6f.jpg?wh=2572x2473\" alt=\"\"></p><p>情感分析的快速发展得益于社交媒体的兴起，自2000年初以来，情感分析已经成长为自然语言处理（NLP）中最活跃的研究领域之一，它也被广泛应用在个性化推荐、商业决策、舆情监控等方面。</p><p>今天这节课，我们将完成一个情感分析项目，一起来对影评文本做分析。</p><h2>数据准备</h2><p>现在我们手中有一批影评数据（IMDB数据集），影评被分为两类：正面评价与负面评价。我们需要训练一个情感分析模型，对影评文本进行分类。</p><!-- [[[read_end]]] --><p>这个问题本质上还是一个文本分类问题，研究对象是电影评论类的文本，我们需要对文本进行二分类。下面我们来看一看训练数据。</p><p>IMDB（Internet Movie Database）是一个来自互联网电影数据库，其中包含了50000条严重两极分化的电影评论。数据集被划分为训练集和测试集，其中训练集和测试集中各有25000条评论，并且训练集和测试集都包含50%的正面评论和50%的消极评论。</p><h3>如何用Torchtext读取数据集</h3><p>我们可以利用Torchtext工具包来读取数据集。</p><p>Torchtext是一个包含<strong>常用的文本处理工具</strong>和<strong>常见自然语言数据集</strong>的工具包。我们可以类比之前学习过的Torchvision包来理解它，只不过，Torchvision包是用来处理图像的，而Torchtext则是用来处理文本的。</p><p>安装Torchtext同样很简单，我们可以使用pip进行安装，命令如下：</p><pre><code class=\"language-plain\">pip install torchtext\n</code></pre><p>Torchtext中包含了上面我们要使用的IMDB数据集，并且还有读取语料库、词转词向量、词转下标、建立相应迭代器等功能，可以满足我们对文本的处理需求。</p><p>更为方便的是，Torchtext已经把一些常见对文本处理的数据集囊括在了<code>torchtext.datasets</code>中，与Torchvision类似，使用时会自动下载、解压并解析数据。</p><p>以IMDB为例，我们可以用后面的代码来读取数据集：</p><pre><code class=\"language-python\"># 读取IMDB数据集\nimport torchtext\ntrain_iter = torchtext.datasets.IMDB(root='./data', split='train')\nnext(train_iter)\n</code></pre><p>torchtext.datasets.IMDB函数有两个参数，其中：</p><ul>\n<li>root：是一个字符串，用于指定你想要读取目标数据集的位置，如果数据集不存在，则会自动下载；</li>\n<li>split：是一个字符串或者元组，表示返回的数据集类型，是训练集、测试集或验证集，默认是&nbsp;(‘train’, ‘test’)。<br>\ntorchtext.datasets.IMDB函数的返回值是一个迭代器，这里我们读取了IMDB数据集中的训练集，共25000条数据，存入了变量train_iter中。</li>\n</ul><p>程序运行的结果如下图所示。我们可以看到，利用next()函数，读取出迭代器train_iter中的一条数据，每一行是情绪分类以及后面的评论文本。“neg”表示负面评价，“pos”表示正面评价。</p><p><img src=\"https://static001.geekbang.org/resource/image/e4/e6/e4625437cafc8bb29851fb57a9b3e8e6.png?wh=1920x616\" alt=\"图片\"></p><h3>数据处理pipelines</h3><p>读取出了数据集中的评论文本和情绪分类，我们还需要将文本和分类标签处理成向量，才能被计算机读取。处理文本的一般过程是先分词，然后根据词汇表将词语转换为id。</p><p>Torchtext为我们提供了基本的文本处理工具，包括分词器“tokenizer”和词汇表“vocab”。我们可以用下面两个函数来创建分词器和词汇表。</p><p>get_tokenizer函数的作用是创建一个分词器。将文本喂给相应的分词器，分词器就可以根据不同分词函数的规则完成分词。例如英文的分词器，就是简单按照空格和标点符号进行分词。</p><p>build_vocab_from_iterator函数可以帮助我们使用训练数据集的迭代器构建词汇表，构建好词汇表后，输入分词后的结果，即可返回每个词语的id。</p><p>创建分词器和构建词汇表的代码如下。首先我们要建立一个可以处理英文的分词器tokenizer，然后再根据IMDB数据集的训练集迭代器train_iter建立词汇表vocab。</p><pre><code class=\"language-python\"># 创建分词器\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\nprint(tokenizer('here is the an example!'))\n'''\n输出：['here', 'is', 'the', 'an', 'example', '!']\n'''\n\n# 构建词汇表\ndef yield_tokens(data_iter):\n&nbsp; &nbsp; for _, text in data_iter:\n&nbsp; &nbsp; &nbsp; &nbsp; yield tokenizer(text)\n\nvocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"&lt;pad&gt;\", \"&lt;unk&gt;\"])\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\nprint(vocab(tokenizer('here is the an example &lt;pad&gt; &lt;pad&gt;')))\n'''\n输出：[131, 9, 40, 464, 0, 0]\n'''\n</code></pre><p>在构建词汇表的过程中，yield_tokens函数的作用就是依次将训练数据集中的每一条数据都进行分词处理。另外，在构建词汇表时，用户还可以利用specials参数自定义词表。</p><p>上述代码中我们自定义了两个词语：“&lt;pad&gt;”和“&lt;unk&gt;”，分别表示占位符和未登录词。顾名思义，未登录词是指没有被收录在分词词表中的词。由于每条影评文本的长度不同，不能直接批量合成矩阵，因此需通过截断或填补占位符来固定长度。</p><p>为了方便后续调用，我们使用分词器和词汇表来建立数据处理的pipelines。文本pipeline用于给定一段文本，返回分词后的id。标签pipeline用于将情绪分类转化为数字，即“neg”转化为0，“pos”转化为1。</p><p>具体代码如下所示。</p><pre><code class=\"language-python\"># 数据处理pipelines\ntext_pipeline = lambda x: vocab(tokenizer(x))\nlabel_pipeline = lambda x: 1 if x == 'pos' else 0\n\nprint(text_pipeline('here is the an example'))\n'''\n输出：[131, 9, 40, 464, 0, 0 , ... , 0]\n'''\nprint(label_pipeline('neg'))\n'''\n输出：0\n'''\n</code></pre><p>通过示例的输出结果，相信你很容易就能理解文本pipeline和标签pipeline的用法了。</p><h3>生成训练数据</h3><p>有了数据处理的pipelines，接下来就是生成训练数据，也就是生成DataLoader。</p><p>这里还涉及到一个变长数据处理的问题。我们在将文本pipeline所生成的id列表转化为模型能够识别的tensor时，由于文本的句子是变长的，因此生成的tensor长度不一，无法组成矩阵。</p><p>这时，我们需要限定一个句子的最大长度。例如句子的最大长度为256个单词，那么超过256个单词的句子需要做截断处理；不足256个单词的句子，需要统一补位，这里用“/<pad>”来填补。</pad></p><p>上面所说的这些操作，我们都可以放到collate_batch函数中来处理。</p><p>collate_batch函数有什么用呢？它负责在DataLoad提取一个batch的样本时，完成一系列预处理工作：包括生成文本的tensor、生成标签的tensor、生成句子长度的tensor，以及上面所说的对文本进行截断、补位操作。所以，我们将collate_batch函数通过参数collate_fn传入DataLoader，即可实现对变长数据的处理。</p><p>collate_batch函数的定义，以及生成训练与验证DataLoader的代码如下。</p><pre><code class=\"language-python\"># 生成训练数据\nimport torch\nimport torchtext\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\nfrom torchtext.data.functional import to_map_style_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef collate_batch(batch):\n&nbsp; &nbsp; max_length = 256\n&nbsp; &nbsp; pad = text_pipeline('&lt;pad&gt;')\n&nbsp; &nbsp; label_list, text_list, length_list = [], [], []\n&nbsp; &nbsp; for (_label, _text) in batch:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;label_list.append(label_pipeline(_label))\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;processed_text = text_pipeline(_text)[:max_length]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;length_list.append(len(processed_text))\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;text_list.append((processed_text+pad*max_length)[:max_length])\n&nbsp; &nbsp; label_list = torch.tensor(label_list, dtype=torch.int64)\n&nbsp; &nbsp; text_list = torch.tensor(text_list, dtype=torch.int64)\n&nbsp; &nbsp; length_list = torch.tensor(length_list, dtype=torch.int64)\n&nbsp; &nbsp; return label_list.to(device), text_list.to(device), length_list.to(device)\n\ntrain_iter = torchtext.datasets.IMDB(root='./data', split='train')\ntrain_dataset = to_map_style_dataset(train_iter)\nnum_train = int(len(train_dataset) * 0.95)\nsplit_train_, split_valid_ = random_split(train_dataset,&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[num_train, len(train_dataset) - num_train])\ntrain_dataloader = DataLoader(split_train_, batch_size=8, shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(split_valid_, batch_size=8, shuffle=False, collate_fn=collate_batch)\n</code></pre><p>我们一起梳理一下这段代码的流程，一共是五个步骤。</p><p>1.利用torchtext读取IMDB的训练数据集，得到训练数据迭代器；<br>\n2.使用to_map_style_dataset函数将迭代器转化为Dataset类型；<br>\n3.使用random_split函数对Dataset进行划分，其中95%作为训练集，5%作为验证集；<br>\n4.生成训练集的DataLoader；<br>\n5.生成验证集的DataLoader。</p><p>到此为止，数据部分已经全部准备完毕了，接下来我们来进行网络模型的构建。</p><h2>模型构建</h2><p>之前我们已经学过卷积神经网络的相关知识。卷积神经网络使用固定的大小矩阵作为输入（例如一张图片），然后输出一个固定大小的向量（例如不同类别的概率），因此适用于图像分类、目标检测、图像分割等等。</p><p>但是除了图像之外，还有很多信息，其大小或长度并不是固定的，例如音频、视频、文本等。我们想要处理这些序列相关的数据，就要用到时序模型。比如我们今天要处理的文本数据，这就涉及一种常见的时间序列模型：循环神经网络（Recurrent Neural Network，RNN）。</p><p>不过由于RNN自身的结构问题，在进行反向传播时，容易出现梯度消失或梯度爆炸。<strong>LSTM网络</strong>在RNN结构的基础上进行了改进，通过精妙的门控制将短时记忆与长时记忆结合起来，<strong>一定程度上解决了梯度消失与梯度爆炸的问题</strong>。</p><p>我们使用LSTM网络来进行情绪分类的预测。模型的定义如下。</p><pre><code class=\"language-python\"># 定义模型\nclass LSTM(torch.nn.Module):\n&nbsp; &nbsp; def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dropout_rate, pad_index=0):\n&nbsp; &nbsp; &nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n&nbsp; &nbsp; &nbsp; &nbsp; self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropout=dropout_rate, batch_first=True)\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n&nbsp; &nbsp; &nbsp; &nbsp; self.dropout = torch.nn.Dropout(dropout_rate)\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; def forward(self, ids, length):\n&nbsp; &nbsp; &nbsp; &nbsp; embedded = self.dropout(self.embedding(ids))\n&nbsp; &nbsp; &nbsp; &nbsp; packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True,&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; enforce_sorted=False)\n&nbsp; &nbsp; &nbsp; &nbsp; packed_output, (hidden, cell) = self.lstm(packed_embedded)\n&nbsp; &nbsp; &nbsp; &nbsp; output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n&nbsp; &nbsp; &nbsp; &nbsp; if self.lstm.bidirectional:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n&nbsp; &nbsp; &nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hidden = self.dropout(hidden[-1])\n&nbsp; &nbsp; &nbsp; &nbsp; prediction = self.fc(hidden)\n&nbsp; &nbsp; &nbsp; &nbsp; return prediction\n</code></pre><p>网络模型的具体结构，首先是一个Embedding层，用来接收文本id的tensor，然后是LSTM层，最后是一个全连接分类层。其中，bidirectional为True，表示网络为双向LSTM，bidirectional为False，表示网络为单向LSTM。</p><p>网络模型的结构图如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/a8/f4013742ab70b0dc405948f07198cfa8.jpg?wh=619x404\" alt=\"图片\"></p><h2>模型训练与评估</h2><p>定义好网络模型的结构，我们就可以进行模型训练了。首先是实例化网络模型，参数以及具体的代码如下。</p><pre><code class=\"language-python\"># 实例化模型\nvocab_size = len(vocab)\nembedding_dim = 300\nhidden_dim = 300\noutput_dim = 2\nn_layers = 2\nbidirectional = True\ndropout_rate = 0.5\n\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)\nmodel = model.to(device)\n</code></pre><p>由于数据的情感极性共分为两类，因此这里我们要把output_dim的值设置为2。<br>\n接下来是定义损失函数与优化方法，代码如下。在之前的课程里也多次讲过了，所以这里不再重复。</p><pre><code class=\"language-python\"># 损失函数与优化方法\nlr = 5e-4\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion = criterion.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n</code></pre><p>计算loss的代码如下。</p><pre><code class=\"language-python\">import tqdm\nimport sys\nimport numpy as np\n\ndef train(dataloader, model, criterion, optimizer, device):\n&nbsp; &nbsp; model.train()\n&nbsp; &nbsp; epoch_losses = []\n&nbsp; &nbsp; epoch_accs = []\n&nbsp; &nbsp; for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n&nbsp; &nbsp; &nbsp; &nbsp; (label, ids, length) = batch\n&nbsp; &nbsp; &nbsp; &nbsp; label = label.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; ids = ids.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; length = length.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; prediction = model(ids, length)\n&nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(prediction, label) # loss计算\n&nbsp; &nbsp; &nbsp; &nbsp; accuracy = get_accuracy(prediction, label)\n        # 梯度更新\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n&nbsp; &nbsp; &nbsp; &nbsp; epoch_losses.append(loss.item())\n&nbsp; &nbsp; &nbsp; &nbsp; epoch_accs.append(accuracy.item())\n&nbsp; &nbsp; return epoch_losses, epoch_accs\n\ndef evaluate(dataloader, model, criterion, device):\n&nbsp; &nbsp; model.eval()\n&nbsp; &nbsp; epoch_losses = []\n&nbsp; &nbsp; epoch_accs = []\n&nbsp; &nbsp; with torch.no_grad():\n&nbsp; &nbsp; &nbsp; &nbsp; for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (label, ids, length) = batch\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label = label.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ids = ids.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; length = length.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prediction = model(ids, length)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(prediction, label) # loss计算\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; accuracy = get_accuracy(prediction, label)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; epoch_losses.append(loss.item())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; epoch_accs.append(accuracy.item())\n&nbsp; &nbsp; return epoch_losses, epoch_accs\n</code></pre><p>可以看到，这里训练过程与验证过程的loss计算，分别定义在了train函数和evaluate函数中。主要区别是训练过程有梯度的更新，而验证过程中不涉及梯度的更新，只计算loss即可。<br>\n模型的评估我们使用ACC，也就是准确率作为评估指标，计算ACC的代码如下。</p><pre><code class=\"language-python\">def get_accuracy(prediction, label):\n&nbsp; &nbsp; batch_size, _ = prediction.shape\n&nbsp; &nbsp; predicted_classes = prediction.argmax(dim=-1)\n&nbsp; &nbsp; correct_predictions = predicted_classes.eq(label).sum()\n&nbsp; &nbsp; accuracy = correct_predictions / batch_size\n&nbsp; &nbsp; return accuracy\n</code></pre><p>最后，训练过程的具体代码如下。包括计算loss和ACC、保存losses列表和保存最优模型。</p><pre><code class=\"language-python\">n_epochs = 10\nbest_valid_loss = float('inf')\n\ntrain_losses = []\ntrain_accs = []\nvalid_losses = []\nvalid_accs = []\n\nfor epoch in range(n_epochs):\n&nbsp; &nbsp; train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n&nbsp; &nbsp; valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n&nbsp; &nbsp; train_losses.extend(train_loss)\n&nbsp; &nbsp; train_accs.extend(train_acc)\n&nbsp; &nbsp; valid_losses.extend(valid_loss)\n&nbsp; &nbsp; valid_accs.extend(valid_acc)&nbsp;\n&nbsp; &nbsp; epoch_train_loss = np.mean(train_loss)\n&nbsp; &nbsp; epoch_train_acc = np.mean(train_acc)\n&nbsp; &nbsp; epoch_valid_loss = np.mean(valid_loss)\n&nbsp; &nbsp; epoch_valid_acc = np.mean(valid_acc)&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; if epoch_valid_loss &lt; best_valid_loss:\n&nbsp; &nbsp; &nbsp; &nbsp; best_valid_loss = epoch_valid_loss\n&nbsp; &nbsp; &nbsp; &nbsp; torch.save(model.state_dict(), 'lstm.pt') &nbsp;&nbsp;\n&nbsp; &nbsp; print(f'epoch: {epoch+1}')\n&nbsp; &nbsp; print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n&nbsp; &nbsp; print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n</code></pre><p>我们还可以利用保存下来train_losses列表，绘制训练过程中的loss曲线，或使用<a href=\"https://time.geekbang.org/column/article/444252\">第15课</a>讲过的可视化工具来监控训练过程。<br>\n至此，一个完整的情感分析项目已经完成了。从数据读取到模型构建与训练的方方面面，我都手把手教给了你，希望你能以此为模板，独立解决自己的问题。</p><h2>小结</h2><p>恭喜你，完成了今天的学习任务。今天我们一起完成了一个情感分析项目的实践，相当于是对自然语言处理任务的一个初探。我带你回顾一下今天学习的要点。</p><p>在数据准备阶段，我们可以使用PyTorch提供的文本处理工具包Torchtext。想要掌握Torchtext也不难，我们可以类比之前详细介绍过的Torchvision，不懂的地方再对应去<a href=\"https://pytorch.org/text/stable/index.html\">查阅文档</a>，相信你一定可以做到举一反三。</p><p><strong>模型构建时，要根据具体的问题选择适合的神经网络。卷积神经网络常被用于处理图像作为输入的预测问题；循环神经网络常被用于处理变长的、序列相关的数据。而LSTM相较于RNN，能更好地解决梯度消失与梯度爆炸的问题</strong>。</p><p>在后续的课程中，我们还会讲解两大自然语言处理任务：文本分类和摘要生成，它们分别包括了判别模型和生成模型，相信那时你一定会在文本处理方面有更深层次的理解。</p><h2>每课一练</h2><p>利用今天训练的模型，编写一个函数predict_sentiment，实现输入一句话，输出这句话的情绪类别与概率。</p><p>例如：</p><pre><code class=\"language-python\">text = \"This film is terrible!\"\npredict_sentiment(text, model, tokenizer, vocab, device)\n'''\n输出：('neg', 0.8874172568321228)\n'''\n</code></pre><p>欢迎你在留言区跟我交流互动，也推荐你把今天学到的内容分享给更多朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"22 | NLP基础（下）：详解语言模型与注意力机制","id":461691},"right":{"article_title":"24 | 文本分类：如何使用BERT构建文本分类模型？","id":464152}}},{"article_id":464152,"article_title":"24 | 文本分类：如何使用BERT构建文本分类模型？","article_content":"<p>你好，我是方远。</p><p>在第22节课我们一起学习了不少文本处理方面的理论，其实文本分类在机器学习领域的应用也非常广泛。</p><p>比如说你现在是一个NLP研发工程师，老板啪地一下甩给你一大堆新闻文本数据，它们可能来源于不同的领域，比如体育、政治、经济、社会等类型。这时我们就需要对文本分类处理，方便用户快速查询自己感兴趣的内容，甚至按用户的需要定向推荐某类内容。</p><p>这样的需求就非常适合用PyTorch + BERT处理。为什么会选择BERT呢？因为BERT是比较典型的深度学习NLP算法模型，也是业界使用最广泛的模型之一。接下来，我们就一起来搭建这个文本分类模型，相信我，它的效果表现非常强悍。</p><h2>问题背景与分析</h2><p>正式动手之前，我们不妨回顾一下历史。文本分类问题有很多经典解决办法。</p><p>开始时就是最简单粗暴的关键词统计方法。之后又有了基于贝叶斯概率的分类方法，通过某些条件发生的概率推断某个类别的概率大小，并作为最终分类的决策依据。尽管这个思想很简单，但是意义重大，时至今日，贝叶斯方法仍旧是非常多应用场景下的好选择。</p><p>之后还有支持向量机（SVM），很长一段时间，其变体和应用都在NLP算法应用的问题场景下占据统治地位。</p><p>随着计算设备性能的提升、新的算法理论的产生等进步，一大批的诸如随机森林、LDA主题模型、神经网络等方法纷纷涌现，可谓百家争鸣。</p><!-- [[[read_end]]] --><p>既然有这么多方法，为什么这里我们这里推荐选用BERT呢？</p><p>因为在很多情况下，尤其是一些复杂场景下的文本，像BERT这样具有强大处理能力的工具才能应对。比如说新闻文本就不好分类，因为它存在后面这些问题。</p><p>1.<strong>类别多</strong>。在新闻资讯App中，新闻的种类是非常多的，需要产品经理按照统计、实用的原则进行文章分类体系的设计，使其类别能够覆盖所有的文本，一般来说都有50种甚至以上。不过为了让你把握重点，咱们先简化问题，假定文本的分类体系已经确定。</p><p>2.<strong>数据不平衡</strong>。不难理解，在新闻中，社会、经济、体育、娱乐等类别的文章数量相对来说是比较多的，占据了很大的比例；而少儿、医疗等类别则相对较少，有的时候一天也没有几篇对应的文章。</p><p>3.<strong>多语言。</strong>一般来说，咱们主要的语言除了中文，应该是大多数人只会英语了，不过为了考虑到新闻来源的广泛性，咱们也假定这批文本是多语言的。</p><p>刚才提到了，因为Bert是比较典型的深度学习NLP算法模型，也是业界使用最广泛的模型之一。如果拿下这么有代表性的模型，以后你学习和使用基于Attention的模型你也能举一反三，比如GPT等。</p><p>想要用好BERT，我们需要先了解它有哪些特点。</p><h2>BERT原理与特点分析</h2><p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。作为一种基于Attention方法的模型，它最开始出现的时候可以说是抢尽了风头，在文本分类、自动对话、语义理解等十几项NLP任务上拿到了历史最好成绩。</p><p>在<a href=\"https://time.geekbang.org/column/article/461691\">第22节课</a>（如果不熟悉可以回看），我们已经了解了Attention的基本原理，有了这个知识做基础，我们很容易就能快速掌握BERT的原理。</p><p>这里我再快速给你回顾一下，BERT的理论框架主要是基于论文《Attention is all you need》中提出的Transformer，而后者的原理则是刚才提到的Attention。<strong>其最为明显的特点，就是摒弃了传统的RNN和CNN逻辑，有效解决了NLP中的长期依赖问题。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/57/e7/57129ea84051eaf5985535dcb97c1fe7.jpg?wh=1920x1269\" alt=\"图片\" title=\"图片来源：https://arxiv.org/abs/1706.03762\"></p><p>在BERT中，它的输入部分，也就是图片的左边，其实是由N个多头Attention组合而成。多头Attention是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，这有助于网络捕捉到更丰富的特征或者信息。（具体原理，一定要查阅<a href=\"https://arxiv.org/abs/1706.03762\">《Attention is all you need》</a>哦）。</p><p>结合上图我们要注意的是，BERT采用了基于MLM的模型训练方式，即Mask Language Model。因为BERT是Transformer的一部分，即encoder环节，所以没有decoder的部分（其实就是GPT）。</p><p>为了解决这个问题，MLM方式应运而生。它的思想也非常简单，就是在<strong>训练之前，随机将文本中一部分的词语（token）进行屏蔽（mask），然后在训练的过程中，使用其他没有被屏蔽的token对被屏蔽的token进行预测</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/ae/84/aeed42d94750436f1dyye31f92c96584.jpg?wh=1920x700\" alt=\"图片\"></p><p>用过Word2Vec的小伙伴应该比较清楚，在Word2Vec中，对于同一个词语，它的向量表示是固定的，这也就是为什么会有那个经典的“<em>国王-男人+女人=皇后</em>”计算式了。</p><p>但是有一个问题，“苹果”这个词，有可能是水果的苹果，也可能是电子产品的品牌，如果还是用同一个向量表示，这样就有可能产生偏差。而在BERT中则不一样，根据上下文的不同，对于同一个token给出的词向量是动态变化的，更加灵活。</p><p>此外，BERT还有多语言的优势。在以前的算法中，比如SVM，如果要做多语言的模型，就要涉及分词、提取关键词等操作，而这些操作要求你对该语言有所了解。像阿拉伯文、日语等语言，咱们大概率是看不懂的，这会对我们最后的模型效果产生极大影响。</p><p>BERT则不需要担心这个问题，通过基于字符、字符片段、单词等不同粒度的token覆盖并作WordPiece，能够覆盖上百种语言，甚至可以说，只要你能够发明出一种逻辑上自洽的语言，BERT就能够处理。有关WordPiece的介绍，你可以通过<a href=\"https://paperswithcode.com/method/wordpiece\">这里</a>做拓展阅读。</p><p>好，说了这么多，集高效、准确、灵活再加上用途广泛于一体的BERT，自然而然就成为了咱们的首选，下面咱们开始正式构建一个文本分类模型。</p><h2>安装与准备</h2><p>工欲善其事，必先利其器，在开始构建模型之前，我们要安装相应的工具，然后下载对应的预先训练好的模型，同时还要了解数据的格式。</p><h3>环境准备</h3><p>因为咱们要做的是一个基于PyTorch 的BERT模型，那么就要安装对应的python包，这里我选择的是hugging face的PyTorch版本的Transformers包。你可以通过pip命令直接安装。</p><pre><code>pip install Transformers\n</code></pre><h3>模型准备</h3><p>安装之后，我们打开Transformers的<a href=\"https://github.com/huggingface/transformers\">git页面</a>，并找到如下的文件夹。</p><pre><code class=\"language-plain\">src/Transformers/models/BERT\n</code></pre><p>从这个文件夹里，我们需要找到两个很重要的文件，分别是convert_BERT_original_tf2_checkpoint_to_PyTorch.py和modeling_BERT.py文件。</p><p>先来看第一个文件，你看看名字，是不是就能猜出来，它大概是用来做什么的了？没错，就是用来将原来通过TensorfFlow预训练的模型转换为PyTorch的模型。</p><p>然后是modeling_BERT.py文件，这个文件实际上是给了你一个使用BERT的范例。</p><p>下面，咱们开始准备模型，打开<a href=\"https://github.com/tensorflow/models/tree/master/official/nlp/bert\">这个地址</a>，你会发现在这个页面中，有几个预训练好的模型。</p><p><img src=\"https://static001.geekbang.org/resource/image/f7/a9/f7429816e9c736d99be4b55c67bac6a9.png?wh=1920x956\" alt=\"图片\"></p><p>对照这节课的任务，我们选择的是“BERT-Base, Multilingual Cased”的版本。从GitHub的介绍可以看出，这个版本的checkpoint支持104种语言，是不是很厉害？当然，如果你没有多语言的需求，也可以选择其他版本的，它们的区别主要是网络的体积不同。</p><p>转换完模型之后，你会发现你的本地多了三个文件，分别是config.json、pytorch_model.bin和vocab.txt。我来分别给你说一说。</p><p><img src=\"https://static001.geekbang.org/resource/image/a8/00/a85bfecfb02108cf7e46d5bef74efe00.jpg?wh=1920x228\" alt=\"图片\"></p><p>1.config.json：顾名思义，该文件就是BERT模型的配置文件，里面记录了所有用于训练的参数设置。</p><p>2.PyTorch_model.bin：模型文件本身。</p><p>3.vocab.txt：词表文件。尽管BERT可以处理一百多种语言，但是它仍旧需要词表文件用于识别所支持语言的字符、字符串或者单词。</p><h3>格式准备</h3><p>现在模型准备好了，我们还要看看跟模型匹配的格式。BERT的输入不算复杂，但是也需要了解其形式。在训练的时候，我们输入的数据不能是直接把词塞到模型里，而是要转化成后面这三种向量。</p><p>1.<strong>Token embeddings</strong>：词向量。这里需要注意的是，Token embeddings的第一个开头的token一定得是“[CLS]”。[CLS]作为整篇文本的语义表示，用于文本分类等任务。</p><p>2.<strong>Segment embeddings</strong>。这个向量主要是用来将两句话进行区分，比如问答任务，会有问句和答句同时输入，这就需要一个能够区分两句话的操作。不过在咱们此次的分类任务中，只有一个句子。</p><p>3.<strong>Position embeddings</strong>。记录了单词的位置信息。</p><h2>模型构建</h2><p>准备工作已经一切就绪，我们这就来搭建一个基于BERT的文本分类网络模型。这包括了<strong>网络的设计、配置、以及数据准备，这个过程也是咱们的核心过程</strong>。</p><h3>网络设计</h3><p>从上面提到的modeling_BERT.py文件中，我们可以看到，作者实际上已经给我们提供了很多种类的NLP任务的示例代码，咱们找到其中的“BERTForSequenceClassification”，这个分类网络我们可以直接使用，它也是最最基础的BERT文本分类的流程。</p><p>这个过程包括了利用<strong>BERT得到文本的embedding表示</strong>、<strong>将embedding放入全连接层得到分类结果</strong>两部分。我们具体看一下代码。</p><pre><code class=\"language-python\">class BERTForSequenceClassification(BERTPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels//类别标签数量\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)//还记得Dropout是用来做什么的吗？对，可以一定程度防止过拟合。\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)//BERT输出的embedding传入一个MLP层做分类。\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]//这个就是经过BERT得到的中间输出。\n\n        pooled_output = self.dropout(pooled_output)//对，就是为了减少过拟合和增加网络的健壮性。\n        logits = self.classifier(pooled_output)//多层MLP输出最后的分类结果。\n\n</code></pre><p>对照前面的代码，可以发现，接收到输入信息之后，BERT返回了一个outputs，outputs包括了模型计算之后的全部结果，不仅有每个token的信息，也有整个文本的信息，这个输出具体包括以下信息。</p><p>last_hidden_state是模型最后一层输出的隐藏层状态序列。shape是(batch_size, sequence_length, hidden_size)。其中hidden_size=768，这个部分的状态，就相当于利用sequence_length * 768维度的矩阵，记录了整个文本的计算之后的每一个token的结果信息。</p><p>pooled_output，代表序列的第一个token的最后一个隐藏层的状态。shape是(batch_size, hidden_size)。所谓的第一个token，就是咱们刚才提到的[CLS]标签。</p><p>除了上面两个信息，还有hidden_states、attentions、cross attentions。有兴趣的小伙伴可以去查一下，它们有何用途。</p><p>通常的任务中，我们用得比较多的是last_hidden_state对应的信息，我们可以用pooled_output = outputs[1]来进行获取。</p><p>至此，我们已经有了经过BERT计算的文本向量表示，然后我们将其输入到一个linear层中进行分类，就可以得到最后的分类结果了。<strong>为了提高模型的表现，我们往往会在linear层之前，加入一个dropout层，这样可以减少网络的过拟合的可能性，同时增强神经元的独立性</strong>。</p><h3>模型配置</h3><p>设计好网络，我们还要对模型进行配置。还记得刚才提到的config.json文件么？这里面就记录了BERT模型所需的所有配置信息，我们需要对其中的几个内容进行调整，这样模型就能知道我们到底是要做什么事情了。</p><p>后面这几个字段我专门说一下。</p><ul>\n<li>id2label：这个字段记录了类别标签和类别名称的映射关系。</li>\n<li>label2id：这个字段记录了类别名称和类别标签的映射关系。</li>\n<li>num_labels_cate：类别的数量。</li>\n</ul><h2>数据准备</h2><p>模型网络设计好了，配置文件也搞定了，下面我们就要开始数据准备这一步了。这里的数据准备是指将文本转换为BERT能够识别的形式，即前面提到的三种向量，在代码中，对应的就是input_ids、token_type_ids、attention_mask。</p><p>为了生成这些数据，我们需要在git中找到“src/Transformers/data/processors/utils.py”文件，在这个文件中，我们要用到以下几个内容。</p><p>1.InputExample：它用于记录单个训练数据的文本内容的结构。</p><p>2.DataProcessor：通过这个类中的函数，我们可以将训练数据集的文本，表示为多个InputExample组成的数据集合。</p><p>3.get_features：用于把InputExample数据转换成BERT能够理解的数据结构的关键函数。我们具体来看一下各个数据都怎么生成的。</p><p>input_ids记录了输入token对应在vocab.txt的id序号，它是通过如下的代码得到的。</p><pre><code class=\"language-python\">input_ids = tokenizer.encode(\n\t                example.text_a,\n\t                add_special_tokens=True,\n\t                max_length=min(max_length, tokenizer.max_len),\n\t            )\n</code></pre><p>而attention_mask记录了属于第一个句子的token信息，通过如下代码得到。</p><pre><code class=\"language-python\">attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\nattention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n</code></pre><p>另外，不要忘记记录文本类别的信息（label）。你可以自己想想看，能否按照utils.py文件中的声明方式，构建出对应的label信息呢？</p><h2>模型训练</h2><p>到目前为止，我们有了网络结构定义（BERTForSequenceClassification）、数据集合（get_features），现在就可以开始编写实现训练过程的代码了。</p><h3>选择优化器</h3><p>首先我们来选择优化器，代码如下。我们要对网络中的所有权重参数进行设置，这样优化器就可以知道哪些参数是要进行优化的。然后我们将参数list放到优化器中，BERT使用的是AdamW优化器。</p><pre><code class=\"language-plain\">param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\noptimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n</code></pre><p>这部分的代码，主要是为了选择一个合适咱们模型任务的优化器，并将网络中的参数设定好学习率。</p><h3>构建训练过程逻辑</h3><p>训练的过程逻辑是非常简单的，只需要两个for循环，分别代表epoch和batch，然后在最内部增加<strong>一个训练核心语句，<strong>以及</strong>一个梯度更新语句</strong>，这就足够了。可以看到，PyTorch在工程代码的实现上，封装得非常完善和简练。</p><pre><code class=\"language-plain\">for epoch in trange(0, args.num_train_epochs):\n  model.train()//一定别忘了要把模型设置为训练状态。\n  for step, batch in enumerate(tqdm(train_dataLoader, desc='Iteration')):\n    step_loss = training_step(batch)//训练的核心环节\n    tr_loss += step_loss[0]\n    optimizer.step()\n    optimizer.zero_grad()\n</code></pre><h3>训练的核心环节</h3><p>训练的核心环节，你需要关注两个部分，分别是<strong>通过网络得到预测输出</strong>，也就是logits，以及<strong>基于logits计算得到的loss</strong>，loss是整个模型使用梯度更新需要用到的数据。</p><pre><code class=\"language-plain\">def training_step(batch):\n  input_ids, token_type_ids, attention_mask, labels = batch\n  input_ids = input_ids.to(device)//将数据发送到GPU\n  token_type_ids = token_type_ids.to(device)\n  attention_mask = attention_mask.to(device)\n  labels = labels_voc.to(device)\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\n  logits = model(input_ids,\n        token_type_ids=token_type_ids,&nbsp;\n        attention_mask=attention_mask,&nbsp;\n        labels=labels)\n  loss_fct = BCEWithLogitsLoss()\n  loss = loss_fct(logits.view(-1, num_labels_cate), labels.view(-1, num_labels_cate).float())\n  loss.backward()\n</code></pre><p>至此，咱们已经快速构建出了一个BERT分类器所需的所有关键代码。但是仍旧有一些小小的环节需要你来完善，比如training_step代码块中的device，是怎么得到的呢？回顾一下咱们之前学习的内容，相信你一定可以做得到。</p><h2>小结</h2><p>恭喜你完成了这节课的学习，尽管现在GitHub上已经有了很多已经封装得非常完善的BERT代码，你也可以很快实现一个最基本的NLP算法流程，但是我仍希望你能够抽出时间，好好看一下Transformer中的模型代码，这会对你的技术提升有非常大的助益。</p><p>这节课我们学习了如何用PyTorch快速构建一个基本的文本分类模型，想要实现这个过程，你需要了解BERT的预训练模型的获取以及转化、分类网络的设计方法、训练过程的编写。整个过程不难，但是却可以让你快速上手，了解PyTorch在NLP方面如何应用。</p><p>除了技术本身，业务方面的考虑我们也要注意。比如新闻文本的多语言、数据不平衡等问题，模型有时不能解决所有的问题，因此你还需要学习一些<strong>数据预处理的技巧</strong>，这包括很多技术和算法方面的内容。</p><p>即使我列出一份长长的学习清单，也可能会挂一漏万，所以数据预处理方面的知识我建议你重点关注以下内容：建议你需要花一些时间去学习NumPy和Pandas的使用，这样才能更加得心应手地处理数据；你还可以多学习一些常见的数据挖掘算法（比如决策树、KNN、支持向量机等）；另外，深度学习的广泛使用，其实仍旧非常需要传统机器学习算法的背后支撑，也建议你多多了解。</p><h2>思考题</h2><p>BERT处理文本是有最大长度要求的（512），那么遇到长文本，该怎么办呢？</p><p>也欢迎你在留言区记录你的疑问或者收获，也推荐你把这节课分享给你的朋友。</p>","neighbors":{"left":{"article_title":"23 | 情感分析：如何使用LSTM进行情感分析？","id":462524},"right":{"article_title":"25 | 摘要：如何快速实现自动文摘生成？","id":464870}}},{"article_id":464870,"article_title":"25 | 摘要：如何快速实现自动文摘生成？","article_content":"<p>你好，我是方远。</p><p>当我们打开某个新闻APP或者某个网站时，常常被这样的标题所吸引：“震惊了十亿人”、“一定要读完，跟你的生命有关！”等。但是当我们点进去却发现都是标题党，实际内容大相径庭！这时候你可能会想，如果有一种工具能帮助我们提炼文章的关键内容，那我们就不会再受到标题党的影响了。其实想要实现这个工具并不复杂，用自动文摘技术就能解决。</p><p>自动文摘充斥着我们生活的方方面面，它可用于热点新闻聚合、新闻推荐、语音播报、APP消息Push、智能写作等场景。今天我们要讲的这个自然语言处理任务，就是自动文摘生成。</p><h2>问题背景</h2><p>自动文摘技术，就是自动提炼出一些句子来概括整篇文章的大意，用户通过读摘要就可以了解到原文要表达的意思。</p><h3>抽取与生成</h3><p>自动文摘有两种解决方案：一种是抽取式（Extractive）的，就是从原文中提取一些关键的句子，组合成一篇摘要；另外一种是生成式（Abstractive）的，也是这节课我们重点要讲的内容，这种方式需要计算机通读原文后，在理解整篇文章内容的基础上，使用简短连贯的语言将原文的主要内容表达出来，即会产生原文中没有出现的词和句子。</p><p>现阶段，抽取式的摘要目前已经相对成熟，但是抽取质量及内容流畅度都不够理想。随着深度学习的研究，生成式摘要的质量和流畅度都有很大提升，但目前也受到原文本长度过长、抽取内容不佳等限制，生成的摘要与人工摘要相比，还有相当的差距。</p><!-- [[[read_end]]] --><p>语言的表达方式多种多样，机器生成的摘要可能和人工摘要并不相同，那么如何衡量自动摘要的好坏呢？这就涉及到摘要的评价指标。</p><h3>评价指标</h3><p>评价自动摘要的效果通常使用 <strong>ROUGE</strong>（Recall Oriented Understudy for Gisting Evaluation）评价。</p><p>ROUGE评价法参考了机器翻译自动评价方法，并且考虑了N-gram共同出现的程度。这个方法具体是这样设计的：首先由多个专家分别生成人工摘要，构成标准摘要集；然后对比系统生成的自动摘要与人工生成的标准摘要，通过统计二者之间重叠的基本单元（n元语法、词序或词对）的数目，来评价摘要的质量。通过与多专家人工摘要的对比，提高评价系统的稳定性和健壮性。</p><p>ROUGE主要包括以下4种评价指标：</p><p>1.ROUGE-N，基于n-gram的共现统计；<br>\n2.ROUGE-L，基于最长公共子串；<br>\n3.ROUGE-S，基于顺序词对统计；<br>\n4.ROUGE-W，在ROUGE-L的基础上，考虑串的连续匹配。&nbsp;<br>\n了解了自动文摘的种类与评价指标，下面我们再来认识一个用于自动文摘生成的模型——BART。它的名字和上节课讲过的BERT非常像，我们先来看看它有哪些特点。</p><h2>BART原理与特点分析</h2><p>BART的全称是Bidirectional and&nbsp;Auto-Regressive&nbsp;Transformers（双向自回归变压器）。它是由 Facebook AI 在2019年提出的一个新的预训练模型，结合了双向Transformer和自回归Transformer，在文本生成相关任务中达到了SOTA的结果。你可以通过这个链接查看<a href=\"https://arxiv.org/abs/1910.13461\">相关论文</a>。</p><p>我们已经熟知了论文《Attention is all you need》中提出的Transformer。Transformer左半边为Encoder，右半边为Decoder。Encoder和Decoder的结构分别如下图（a）、（b）所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/c7/02d7541cd7b6f0a8b8c35efb3e4d74c7.jpg?wh=1920x901\" alt=\"图片\" title=\"图片来源：https://arxiv.org/abs/1910.13461\"></p><p>Encoder负责将原始文本进行self-attention，并获得句子中每个词的词向量，最经典的 Encoder架构就是上节课所学习的BERT，但是<strong>单独Encoder结构不适用于文本生成任务</strong>。</p><p>Decoder的输入与输出之间错开一个位置，这是为了模拟文本生成时，不能让模型看到未来的词，这种方式称为Auto-Regressive（自回归）。例如GPT等<strong>基于Decoder结构</strong>的模型通常适用于做文本生成任务，但是<strong>无法学习双向的上下文语境信息</strong>。</p><p>BART模型就是将Encoder和Decoder结合在一起的一种sequence-to-sequence结构，它的主要结构如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/99/a663e08e28803d6059aae93fea1a0699.png?wh=1898x778\" alt=\"图片\" title=\"图片来源：https://arxiv.org/abs/1910.13461\"></p><p>BART模型的结构看似与Transformer没什么不同，主要区别在于BART的预训练阶段。首先在Encoder端使用多种噪声对原始文本进行<strong>破坏</strong>，然后再使用Decoder <strong>重建</strong>原始文本。</p><p>由于BART本身就是在sequence-to-sequence的基础上构建并且进行预训练，它天然就适合做序列生成的任务，例如：问答、文本摘要、机器翻译等。在生成任务上获得进步的同时，在一些文本理解类任务上它也可以取得很好的效果。</p><p>下面我们进入实战阶段，利用BART来实现自动文摘生成。</p><h2>快速文摘生成</h2><p>这里我们还是使用hugging face的Transformers工具包。具体的安装过程，上一节课已经介绍过了。</p><p>Transformers工具包为快速使用自动文摘生成模型提供了pipeline API。pipeline聚合了文本预处理步骤与训练好的自动文摘生成模型。利用Transformers的pipeline，我们只需短短几行代码，就可以快速生成文本摘要。</p><p>下面是一个使用pipeline生成文摘的例子，代码如下。</p><pre><code class=\"language-python\">from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\")\n\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.&nbsp; Her next court appearance is scheduled for May 18.\n\"\"\"\n\nprint(summarizer(ARTICLE, max_length=130, min_length=30))\n'''\n输出:\n[{'summary_text': ' Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in\nthe first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and\n2002 . At one time, she was married to eight men at once, prosecutors say .'}]\n'''\n</code></pre><p>第3行代码的作用是构建一个自动文摘的pipeline，pipeline会自动下载并缓存训练好的自动文摘生成模型。这个自动文摘生成模型是BART模型在CNN/Daily Mail数据集上训练得到的。</p><p>第5~22行代码是待生成摘要的文章原文。第24行代码是针对文摘原文自动生成文摘，其中参数max_length和min_length限制了文摘的最大和最小长度，输出的结果如上面代码注释所示。</p><p>如果你不想使用Transformers提供的预训练模型，而是想使用自己的模型或其它任意模型也很简单。具体代码如下。</p><pre><code class=\"language-python\">from transformers import BartTokenizer, BartForConditionalGeneration\n\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\ninputs = tokenizer([ARTICLE], max_length=1024, return_tensors='pt')\n\n# 生成文摘\nsummary_ids = model.generate(inputs['input_ids'], max_length=130, early_stopping=True)\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint(summary)\n</code></pre><p>流程是一共包括四步，我们分别看一下。<br>\n第一步是实例化一个BART的模型和分词器对象。BartForConditionalGeneration类是BART模型用于摘要生成的类，BartTokenizer是BART的分词器，它们都有from_pretrained()方法，可以加载预训练模型。</p><p>from_pretrained()函数需要传入一个字符串作为参数，这个字符串可以是本地模型的路径，也可以是上传到Hugging Face模型库中的模型名字。</p><p>这里“facebook/bart-large-cnn”是Facebook利用CNN/Daily Mail数据集训练的BART模型，模型具体细节你可以参考<a href=\"https://huggingface.co/facebook/bart-large-cnn\">这里</a>。</p><p>接下来是第二步，对原始文本进行分词。我们可以利用分词器对象tokenizer对原始文本ARTICLE进行分词，并得到词语id的Tensor。return_tensors='pt’表示返回值是PyTorch的Tensor。</p><p>第三步，使用generate()方法生成摘要。其中参数max_length限制了生成摘要的最大长度，early_stopping表示生成过程是否可提前停止。generate()方法的输出是摘要词语的id。</p><p>最后一步，利用分词器解码得到最终的摘要文本。利用tokenizer.decode()函数，将词语id转换为词语文本。其中参数skip_special_tokens表示是否去掉“<pad>”、\"&lt;\\s&gt;\"等一些特殊token。</pad></p><h2>Fine-tuning BART</h2><p>下面我们来看一看如何用自己的数据集来训练BART模型。</p><h3>模型加载</h3><p>模型加载部分和之前讲的一样，不再过多重复。这里我们要利用BartForConditionalGeneration类的from_pretrained()函数，加载一个BART模型。</p><p>模型加载的代码如下。这里我们会在Facebook训练好的摘要模型上，继续Fine-tuning。</p><pre><code class=\"language-python\">from transformers import BartTokenizer, BartForConditionalGeneration\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\nmodel = BartForConditionalGeneration.from_pretrained('facebook/facebook/bart-large-cnn')\n</code></pre><h3>数据准备</h3><p>接下来，是数据准备。我们先来回顾一下，之前学习过的读取文本数据集的方式。在<a href=\"https://time.geekbang.org/column/intro/100093301\">第6课</a>中，我们学习过使用PyTorch原生的的Dataset类读取数据集；在<a href=\"https://time.geekbang.org/column/article/462524\">第23课</a>中，我们学习了使用Torchtext工具<code>torchtext.datasets</code>来读取数据集。今天，我们还要学习一种新的数据读取工具：Datasets库。</p><p>Datasets库也是由hugging face团队开发的，旨在轻松访问与共享数据集。官方的文档在<a href=\"https://huggingface.co/docs/datasets/index.html\">这里</a>，有兴趣了解更多的同学可以去看看。</p><p>Datasets库的安装同样非常简单。可以使用pip安装：</p><pre><code class=\"language-python\">pip install datasets\n</code></pre><p>或使用conda进行安装：</p><pre><code class=\"language-python\">conda install -c huggingface -c conda-forge datasets\n</code></pre><p>Datasets库中同样包括常见数据集，而且帮我们封装好了读取数据集的操作。我们来看一个读取IMDB数据集（第23课讲过）的训练数据的示例：</p><pre><code class=\"language-python\">import datasets\ntrain_dataset = datasets.load_dataset(\"imdb\", split=\"train\")\nprint(train_dataset.column_names)\n'''\n输出：\n['label', 'text']\n'''\n</code></pre><p>用load_dataset()函数来加载数据集，它的参数是数据集的名字或本地文件的路径，split参数用于指定加载训练集、测试集或验证集。</p><p>我们还可以从不止一个csv文件中加载数据：</p><pre><code class=\"language-python\">data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\ndataset = load_dataset(\"namespace/your_dataset_name\", data_files=data_files)\nprint(datasets)\n'''\n示例输出：(实际输出与此不同)\n{train: Dataset({\n    features: ['idx', 'text', 'summary'],\n    num_rows: 3668\n})\ntest: Dataset({\n    features: ['idx', 'text', 'summary'],\n    num_rows: 1725\n})\n}\n'''\n</code></pre><p>通过参数data_files指定训练集、测试集或验证集所需加载的文件路径即可。<br>\n我们可以使用map()函数来对数据集进行一些预处理操作，示例如下：</p><pre><code class=\"language-python\">def add_prefix(example):\n&nbsp; &nbsp; example['text'] = 'My sentence: ' + example['text']\n&nbsp; &nbsp; return example\nupdated_dataset = dataset.map(add_prefix)\nupdated_dataset['train']['text'][:5]\n'''\n示例输出：\n['My sentence: Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n\"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n]\n'''\n</code></pre><p>我们首先定义了一个add_prefix()函数，其作用是为数据集的“text”字段加上一个前缀“My sentence: ”。然后调用数据集dataset的map方法，可以看到输出中“text”字段的内容前面都增加了指定前缀。</p><p>下面我们来看一看，使用自定义的数据集fine-tuning BART模型应该怎么做。具体的代码如下：</p><pre><code class=\"language-python\">from transformers.modeling_bart import shift_tokens_right\n\ndataset = ... # Datasets的对象，数据集需有'text'和'summary'字段，并包含训练集和验证集\n\ndef convert_to_features(example_batch):\n&nbsp; &nbsp; input_encodings = tokenizer.batch_encode_plus(example_batch['text'], pad_to_max_length=True, max_length=1024, truncation=True))\n&nbsp; &nbsp; target_encodings = tokenizer.batch_encode_plus(example_batch['summary'], pad_to_max_length=True, max_length=1024, truncation=True))\n&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; labels = target_encodings['input_ids']\n&nbsp; &nbsp; decoder_input_ids = shift_tokens_right(labels, model.config.pad_token_id)\n&nbsp; &nbsp; labels[labels[:, :] == model.config.pad_token_id] = -100\n&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; encodings = {\n&nbsp; &nbsp; &nbsp; &nbsp; 'input_ids': input_encodings['input_ids'],\n&nbsp; &nbsp; &nbsp; &nbsp; 'attention_mask': input_encodings['attention_mask'],\n&nbsp; &nbsp; &nbsp; &nbsp; 'decoder_input_ids': decoder_input_ids,\n&nbsp; &nbsp; &nbsp; &nbsp; 'labels': labels,\n&nbsp; &nbsp; }\n\n&nbsp; &nbsp; return encodings\n\ndataset = dataset.map(convert_to_features, batched=True)\ncolumns = ['input_ids', 'labels', 'decoder_input_ids','attention_mask',]&nbsp;\ndataset.set_format(type='torch', columns=columns)\n</code></pre><p>首先需要加载自定义的数据集，你要注意的是，这个数据集需要包含原文和摘要两个字段，并且包含训练集和验证集。加载数据集的方法可以用我们刚刚讲过的load_dataset()函数。</p><p>由于加载的数据需要经过一系列预处理操作，比如通过分词器进行分词等等的处理后，才能送入到模型中，因此我们需要定义一个函数convert_to_features()来处理原文和摘要文本。</p><p>convert_to_features()函数中的主要操作就是调用tokenizer来将文本转化为词语id。需要注意的是，代码第10行中有一个shift_tokens_right()函数，它的作用就是我们在原理中介绍过的Auto-Regressive，目的是将Decoder的输入向后移一个位置。</p><p>然后我们需要调用dataset.map()函数来对数据集进行预处理操作，参数batched=True表示支持在batch数据上操作。</p><p>最后再利用set_format()函数生成选择训练所需的数据字段，并生成PyTroch的Tensor。到这里，数据准备的工作就告一段落了。</p><h3>模型训练</h3><p>做好了前面的准备工作，最后我们来看模型训练部分。Transformers工具已经帮我们封装了用于训练文本生成模型的Seq2SeqTrainer类，无需我们自己再去定义损失函数与优化方法了。</p><p>具体的训练代码如下。</p><pre><code class=\"language-python\">from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\ntraining_args = Seq2SeqTrainingArguments(\n&nbsp; &nbsp; output_dir='./models/bart-summarizer',# 模型输出目录\n&nbsp; &nbsp; num_train_epochs=1, # 训练轮数\n&nbsp; &nbsp; per_device_train_batch_size=1,&nbsp;# 训练过程bach_size\n&nbsp; &nbsp; per_device_eval_batch_size=1, # 评估过程bach_size\n&nbsp; &nbsp; warmup_steps=500, # 学习率相关参数\n&nbsp; &nbsp; weight_decay=0.01,&nbsp;# 学习率相关参数\n&nbsp; &nbsp; logging_dir='./logs', # 日志目录\n)\n\ntrainer = Seq2SeqTrainer(\n&nbsp; &nbsp; model=model,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n&nbsp; &nbsp; args=training_args,&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; train_dataset=dataset['train'],&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; eval_dataset=dataset['validation']&nbsp; &nbsp;\n)\n\ntrainer.train()\n</code></pre><p>首先我们要定义一个训练参数的对象，关于训练的相关参数都通过Seq2SeqTrainingArguments类进行定义。然后再实例化一个Seq2SeqTrainer类的对象，将模型和训练数据作为参数传入其中。最后调用train()方法，即可一键开始训练。</p><h2>小结</h2><p>恭喜你完成了今天的学习任务，同时也完成了PyTorch的全部学习内容。</p><p>这节课我们先一起了解了BART模型的原理与特点，这个模型是一个非常实用的预训练模型，能够帮助我们实现文本摘要生成。然后我们结合实例，学习了如何用PyTorch快速构建一个自动文摘生成项目，包括利用Transformers的pipeline快速生成文本摘要和Fine-tuning BART模型。</p><p>因为BART模型具有自回归Transformer的结构，所以它不只可以用于摘要生成，还适用于其它文本生成类的项目，例如机器翻译、对话生成等。相信理解了它的基本原理与模型Fine-tuning的基本流程，你可以很容易地利用BART完成文本生成类的任务，期待你举一反三，亲手做更多的实验。</p><p>通过实战篇的学习，我们一共探讨、实现了2个图像项目和3个自然语言处理项目。如何基于 PyTorch 搭建自己的深度学习网络，相信你已经了然于胸了。当我们解决实际的问题时，首先要从原理出发，选择适合的模型，PyTorch只是一个工具，辅助我们实现自己需要的网络。</p><p>除了自动摘要外，其他四个项目的共通思路都是把问题转化为分类问题。图像、文本分类不必细说，图像分割其实是判别一个像素是属于哪一个类别，情感分析则是判别文本是积极类还是消极类。而自动摘要则是生成模型，通常是基于sequence-to-sequence的结构来实现。</p><p>这些是我通过一系列的实战训练，最终希望你领会到的模型搭建思路。</p><h2>思考题</h2><p>自从2018年BERT被提出以来，获得了很大的成功，学术界陆续提出了各类相关模型，例如我们今天学习的BART。请你查一查还有哪些BERT系列的模型，并阅读相关论文，自行学习一下它们的原理与特点。</p><p>欢迎你在留言区和我交流互动，也推荐你把这节课转发给更多同事、朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"24 | 文本分类：如何使用BERT构建文本分类模型？","id":464152},"right":{"article_title":"用户故事 | Tango：师傅领进门，修行在个人","id":469029}}},{"article_id":469029,"article_title":"用户故事 | Tango：师傅领进门，修行在个人","article_content":"<p>你好，我是Tango。</p><p>很高兴能有机会来分享我对这个专栏的学习体验。先做个自我介绍，我是一个工作了11年的非科班出身程序员（大专日语专业）。目前在NTTDATA（中国）数据信息技术有限公司工作，出于对编程的兴趣便加入了咱们开发者大军。</p><p>如果你平时关注部落或者InfoQ写作平台的话，可能对我的头像有点印象。从2017年购买的第一门课到现在，我已经累计学习了153门课程，其中学完的有130门课。作为一个文科生，在没有遇到极客时间之前，我都是在某宝上找资源自学或者买相关的图书，但是那种学习效果并不是很理想。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/25/02e5c92ed2baa7c73fcaff5b099b1725.png?wh=357x387\" alt=\"图片\"></p><p>随着近些年机器学习的大热，我也开始接触这一块的内容。说实话，想入门机器学习这个领域还是很辛苦的一件事，不单要完成逻辑思维层面的转换，更需要补充很多基础知识。</p><p>之前我买过很多书，但是看起来总是很费劲。而网上能找到的资料，要么通篇数学公式，让我这种数学知识都还给老师的同学扼腕叹息，要么一笔带过原理，一直堆砌代码片段。总之，学习下来极其痛苦，也很难抓到重点是什么。所以当看到咱们这个专栏上线，就第一时间入手了。</p><p>相比之前阅读的纸质图书，我觉得通过专栏学习还是有不少优点的。</p><p>首先，老师沉淀的经验都来自于实际项目，这样我们接触到的知识便是最有用的部分。</p><!-- [[[read_end]]] --><p>其次，因为专栏形式是音频配合图文，可以很好地增加记忆。比方说，通勤路上或者其他零散时间我会听听音频，而有了整块儿时间还会回看图文内容，复习之前所学。</p><p>最后还有一点我尤其看重，就是专栏提供的互动功能，可以在专栏课程下面还有社群（主要是微信群）跟老师、同学互动。三人行，必有我师，很多时候，技术学习需要良好的交流、讨论氛围。</p><p>在业余时间，我也参加过开源社区的活动，目前在OpenVINO中文社区做志愿者，而OpenVINO就是做机器学习推理的，这让我对如何利用PyTorch来训练模型更加感兴趣。</p><p>这个专栏从基础理论到实战篇，每一篇都是干货满满，这要比我在网上看的视频，买的书要好很多，但伴随着知识的密集和难度的增加，如何做到能更好地掌握专栏的内容，让学习效果达到最好，也成为了一个亟待解决的问题。</p><h2>我的学习方法</h2><p>我梳理了一下自己的学习方法，主要是这样五步：学习、复习、归纳总结、复盘和进一步持续学习。</p><p><img src=\"https://static001.geekbang.org/resource/image/d3/47/d3905a7a2c33d3ff0d71c158f3314c47.jpg?wh=1920x816\" alt=\"图片\"></p><p>先说说初步学习，我用的日常听音频+周末整体理解的方式。每周3篇的更新频率，要学习的内容还是很多的，如果只是听音频基本上收获是很少的，所以我习惯用周末的时间，将专栏中的代码写一遍，重新理解一下文章中的内容，尤其是文章中的代码，更值得仔细研读。</p><p>好多小伙伴看到专栏不是视频课程就不想加入，其实我整个学习下来，感觉图文专栏的学习效率会更高一些。</p><p>之后就是复习，和软技能类的专栏不同，我们如果只在通勤路上或者做家务的时候听一听，那学习效果就会大打折扣。这类需要大量动手实践类的专栏，是需要反复学习、动手实践、消化理解后进行归纳总结的。</p><p>那怎样归纳总结呢？将专栏中的知识点归纳成文章发布在InfoQ写作平台，这是我比较推荐的一个方法。我之前的笔记有一部分写在了本地MarkDown文档里，后来发现，有的时候需要查找时还是很不方便，所以慢慢就转到了InfoQ写作平台上面。这样不但可以随时可以查看自己的笔记，还可以分享给他人。</p><p>说完方法，我想还想聊聊有什么内容值得归纳总结。在我看来，除了专栏中的知识点，微信群和专栏的留言区老师的答疑也是很大的宝藏，很值得整理出来。上面提到的总结内容，等到我把专栏讲解内容消化之后，我会一起公布在InfoQ写作平台上，也欢迎小伙伴围观。</p><p>除了文字的输出，为了实践“费曼学习法”，检验自己的学习效果，并将自己掌握的内容和其他人分享，我有时候还会到B站直个播。直播过程中有问题或者细节想不起来了，还会重新去看专栏，或者去网上搜索一下。这也就是为啥我的直播总是“翻车”。</p><p><strong>编程的课程在学习时，很容易出现一种错觉，眼睛觉得学会了，可实际动手写的时候，又好像感觉之前的内容没学到位</strong>。有了这个直播写代码的过程，我觉得会让学习变得轻松有意思一些，也能够查漏补缺。</p><p>最后还有一个持续学习的问题，很多专栏虽然完结了，但是评论区的内容还是不断出现新内容以及新的知识点，那么如何实时跟踪专栏的评论区内容的更新呢？我采用了自动化的方式，自己写一个工具去定时跟踪。比如一个星期去把专栏留言以及老师回复的内容抓取一下，然后利用下一周的时间整理一下。</p><p>另外，专栏毕竟篇幅有限，很多内容没有办法在专栏中事无巨细的交代。如果在工作或者项目中遇到了，则需要自己动手去查找，这也是一个持续学习的过程。</p><h2>学习收获与建议</h2><p>在我看来，学习这门课程绝对是一个正确的选择。因为通过学习方远老师的这个专栏，我不但很好地掌握了PyTorch的不少重要知识，还了解一些常用数学公式的定义，也算意外之喜。</p><p>老师会用Python代码来解释公式的代码逻辑，我们都知道，Python的代码相对容易理解，对入门同学来说，这大大降低了学习成本。</p><p>通过整个专栏的学习，我基本已经掌握了PyTorch的基本运用。在整个学习过程中，还结交了很多一起学习的小伙伴。老师关于VGG，GoogLeNet以及ResNet的讲解简洁明了，这对想了解机器视觉领域算法的新手有很大的帮助。</p><p>在整个专栏的学习过程中，印象最深的地方就是老师在讲完理论知识点后，便会用实际生活中的例子来做联系，就拿基础部分的NumPy相关的内容来说，老师用一个章节讲了需要掌握的知识点后，在下一个章节中就利用了上个章节中的内容，用极客时间的Logo做了一个实际可操作的Demo。这对新手来说非常友好，可以很快地将所学的内容运用起来，学起来很过瘾。</p><p>在后面的学习中，我了解到原来NumPy是不可以用来GPU加速的，而Tensor却是可以的。这个知识点我之前却从来未了解过，学到这个对我后面的训练起到了很大的帮助，我也重构了部分之前的代码，效果显著。</p><p>在学习卷积相关的章节时老师很贴心地整理一份文档：</p><p><img src=\"https://static001.geekbang.org/resource/image/fd/y0/fd8fd1f9c24f2324d439c2e571ec9yy0.png?wh=1920x1002\" alt=\"图片\"></p><p>这个文档在我的项目中也帮我节省了一些查找资料的时间。</p><p>在接触咱们这门课之前，参加大会的时候，经常听到别人提到<strong>多机多卡</strong>的方式进行模型训练，可是怎么操作并不太清楚。在学完咱们这个专栏，我已经掌握了如何搭建分布式的训练环境，等我的显卡到了，我就要开始动手试试了。</p><p>现在暂时先将专栏的<a href=\"https://time.geekbang.org/column/article/445886\">第16课</a>收藏了起来，以下是老师给的关键代码块：</p><pre><code class=\"language-python\">\nif args.distributed:\n&nbsp; &nbsp; &nbsp;if args.dist_url == \"env://\" and args.rank == -1:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;args.rank = int(os.environ[\"RANK\"])\n&nbsp; &nbsp; &nbsp;if args.multiprocessing_distributed:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# For multiprocessing distributed training, rank needs to be the\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# global rank among all the processes\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;args.rank = args.rank * ngpus_per_node + gpu\n&nbsp; &nbsp; &nbsp;dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;world_size=args.world_size, rank=args.rank)\n</code></pre><p>另外，我在学完第一遍后尝试用PyTorch做了一个日文识别的项目，目前还在编写中，以下代码片段是手写体数字识别（MNIST）中的部分内容:</p><p><img src=\"https://static001.geekbang.org/resource/image/cb/8d/cb2bc488f84476d6c36b1640f770818d.png?wh=1920x1037\" alt=\"图片\"></p><p>整个训练次数设置为20回：</p><p><img src=\"https://static001.geekbang.org/resource/image/4c/71/4cc32bb7d062e338be83838502bcf271.png?wh=1453x558\" alt=\"图片\"></p><p>在学完方老师的专栏后，想利用PyTorch来实现一下，看看OpenVINO结合PyTorch的效果如何。目前还在学习阶段，等后面可以展示的时候，我会将GitHub的地址放在评论区或者社群中。</p><p>从代码上看，感觉要比TF更加容易，API的变动也没有TF的那么大，很适合用来做学习。而且后期转换成OpenVINO所支持的形式也是很方便的，只需使用OpenVINO的model optimizer将ONNX转换为IR形式即可。</p><p>除了前面的启发，好用的工具也会大大提升工作效率。专栏里介绍的TensorboardX 和 Visdom工具（<a href=\"https://time.geekbang.org/column/article/444252\">15讲</a>）就很不错，可以更好地在可视化深度学习模型的训练过程中，实时监控一些数据，例如损失值、评价指标等等，我之前一直是自己在notebook中查看的，现在用到了课程里讲到的工具，可视化方便了很多，也节省了很多时间。</p><p>最后实战篇的内容也值得反复琢磨。比方说，在学习本专栏之前一直比较困惑，面对不同风格的PDF文件，如何才能准确提取出我需要的内容。在学完课程后（<a href=\"https://time.geekbang.org/column/article/446645\">17</a> ~ <a href=\"https://time.geekbang.org/column/article/447503\">18</a>讲），给了我很大启发，我可以先将PDF（不可编辑的版本）转换成图片，然后按照规则先训练好目标分类，再按照不同的分类进行图像识别从而提取出我所需要的内容（<a href=\"https://time.geekbang.org/column/article/450898\">19</a> ~ <a href=\"https://time.geekbang.org/column/article/455415\">20</a>讲）。</p><p>说了这么多，还是很希望你也和我一样，一起深入学习专栏，一起动手尝试实验。如果你和我一样已经加入到了学习队伍中，希望你在学习一遍后能有所收获，欢迎和我一起来N刷这门课程，一定会有不同的收获，如果你在实际项目中有什么问题，可以一起在评论区或者社群中积极讨论。</p><p>以上便是我分享的内容了，感谢你的阅读，如果能对你有所帮助，那是我最大的荣幸，如果有不足的地方，也欢迎留言提出，我们一起进步。极客时间，让学习成为习惯。</p>","neighbors":{"left":{"article_title":"25 | 摘要：如何快速实现自动文摘生成？","id":464870},"right":{"article_title":"答疑篇｜思考题答案集锦","id":483203}}},{"article_id":483203,"article_title":"答疑篇｜思考题答案集锦","article_content":"<p>你好，我是编辑宇新。春节将至，给你拜个早年。</p><p>距离我们的专栏更新结束，已经过去不少时间啦。方远老师仍然会在工作之余，回到专栏里转一转，看看同学最新的学习动态。大部分的疑问，老师都在留言区里做了回复。</p><p>除了紧跟更新的第一批同学，也很开心看到有更多新朋友加入到这个专栏的学习中。课程的思考题，为了给你留足思考和研究的时间，我们选择用加餐的方式，把所有参考答案一次性发布出来。</p><p>这里要提醒一下，建议你先自己思考和练习后，再来对答案。每节课都有超链接，方便你跳转回顾。</p><h2><a href=\"https://time.geekbang.org/column/article/426126\">第2节课</a></h2><p>题目：在刚才用户对游戏评分的那个问题中，你能计算一下每位用户对三款游戏打分的平均分吗？</p><p>答案：</p><pre><code class=\"language-plain\">&gt;&gt;&gt;interest_score.mean(axis=1)\n</code></pre><h2><a href=\"https://time.geekbang.org/column/article/42680\">第3节课</a></h2><p>题目：给定数组scores，形状为（256，256，2），scores[: , :, 0] 与scores[:, :, 1]对应位置元素的和为1，现在我们要根据scores生产数组mask，要求scores通道0的值如果大于通道1的值，则mask对应的位置为0，否则为1。</p><p>scores如下，你可以试试用代码实现：</p><pre><code class=\"language-plain\">scores = np.random.rand(256, 256, 2)\nscores[:,:,1] = 1 - scores[:,:,0]\n</code></pre><!-- [[[read_end]]] --><p>答案：</p><pre><code class=\"language-plain\">mask = np.argmax(scores, axis=2)\n</code></pre><h2><a href=\"https://time.geekbang.org/column/article/427460\">第4节课</a></h2><p>题目：在PyTorch中，有torch.Tensor()和torch.tensor()两种函数，它们的区别是什么呢？</p><p>答案：torch.Tensor()<strong>是Pytorch中的类</strong>，其实它是torch.FloatTensor()的别名，使用torch.Tensor()会调用Tensor类的构造函数，生成float类型的张量；</p><p>而torch.tensor()<strong>是Pytorch的函数</strong>，函数原型是torch.tensor(data, dtype…)，其中data可以是scalar，list，tuple等不同的数据结构形式。</p><h2><a href=\"https://time.geekbang.org/column/article/428186\">第5节课</a></h2><p>题目：现在有个Tensor，如下。</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([[4,5,7], [3,9,8],[2,3,4]])\n&gt;&gt;&gt; A\ntensor([[4, 5, 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [3, 9, 8],\n&nbsp; &nbsp; &nbsp; &nbsp; [2, 3, 4]])\n</code></pre><p>我们想提取出其中第一行的第一个，第二行的第一第二个，第三行的最后一个，该怎么做呢？</p><p>答案：</p><pre><code class=\"language-python\">&gt;&gt;&gt; B=torch.Tensor([[1,0,0], [1,1,0],[0,0,1]]).type(torch.ByteTensor)\n&gt;&gt;&gt; B\ntensor([[1, 0, 0],\n&nbsp; &nbsp; &nbsp; &nbsp; [1, 1, 0],\n&nbsp; &nbsp; &nbsp; &nbsp; [0, 0, 1]], dtype=torch.uint8)\n&gt;&gt;&gt; C=torch.masked_select(A,B)\n&gt;&gt;&gt; C\ntensor([4, 3, 9, 4])\n</code></pre><p>我们只需要创建一个形状跟A一样的Tensor，然后将对应位置的数值置为1，然后再把Tensor转换成torch.ByteTensor类型得到B，最后跟之前masked_select一样的操作就OK啦。</p><h2><a href=\"https://time.geekbang.org/column/article/429048\">第6节课</a></h2><p>题目：在PyTorch中，我们要定义一个数据集，应该继承哪一个类呢？</p><p>答案：torch.utils.data.Dataset</p><h2><a href=\"https://time.geekbang.org/column/article/429826\">第7节课</a></h2><p>题目：Torchvision中 transforms 模块的作用是什么？</p><p>答案：常用的图像操作，例如随机切割、旋转、Tensor 与 Numpy 和 PIL Image 的数据类型转换等。</p><h2><a href=\"https://time.geekbang.org/column/article/431420\">第8节课</a></h2><p>题目：请你使用<code>torchvision.models</code>模块实例化一个VGG 16网络。</p><p>答案：</p><pre><code class=\"language-python\">import torchvision.models as models\nvgg16 = models.vgg16(pretrained=True)\n</code></pre><h2><a href=\"https://time.geekbang.org/column/article/432042\">第9节课</a></h2><p>题目：请你想一想，padding为’same’时，stride可以为1以外的数值吗？</p><p>答案：不可以。</p><h2><a href=\"https://time.geekbang.org/column/article/433801\">第10节课</a></h2><p>题目：随机生成一个3通道的128x128的特征图，然后创建一个有10个卷积核且卷积核尺寸为3x3（DW卷积）的深度可分离卷积，对输入数据进行卷积计算。</p><p>答案：</p><pre><code class=\"language-plain\">import torch\nimport torch.nn as nn\n\n# 生成一个三通道的128x128特征图\nx = torch.rand((3, 128, 128)).unsqueeze(0)\n# DW卷积groups参数与输入通道数一样\ndw = nn.Conv2d(x.shape[1], x.shape[1], 3, 1, groups=x.shape[1])\npw = nn.Conv2d(x.shape[1], 10, 1, 1)\nout = pw(dw(x))\nprint(out.shape)\n</code></pre><h2><a href=\"https://time.geekbang.org/column/article/435553\">第11节课</a></h2><p>题目：损失函数的值越小越好么？</p><p>答案：不是的，咱们在这节课中学习的损失函数，实际上是模型在训练数据上的平均损失，这种损失函数我们称作为经验风险。实际上，还有一个方面也是我们在实际工作中需要考虑的，那就是模型的复杂度：一味追求经验风险的最小化，很容易使得模型过拟合（可回顾一下前文内容）。</p><p>所以，还需要对模型的复杂度进行约束，我们称之为结构风险。实际研发场景中<strong>，最终的损失函数是由经验风险和结构风险共同组成的，我们要求的是两者之和的最小化</strong>。</p><h2><a href=\"https://time.geekbang.org/column/article/436564\">第12节课</a></h2><p>题目：深度学习都是基于反向传播的么？</p><p>答案：不是的，主流的深度学习模型是基于反向传播和梯度下降的，但是一些非梯度下降的二阶优化算法也是存在的，比如拟牛顿法等。不过计算代价非常大，用的就比较少了。而且一般而言，工业界基本都采用基于反向传播和梯度下降的方式。</p><h2><a href=\"https://time.geekbang.org/column/article/438639\">第13节课</a></h2><p>题目：batch size越大越好吗？</p><p>答案：不是的。较大的batch_size容易使模型收敛在局部最优点，特别小则容易受噪声影响。</p><h2><a href=\"https://time.geekbang.org/column/article/442442\">第14节课</a></h2><p>题目：请你自己构建一个卷积神经网络，基于CIFAR-10，训练一个图像分类模型。因为还没有学习图像分类原理，所以我先帮你写好了网络的结构，需要你补全数据读取、损失函数(交叉熵损失)与优化方法（SGD）等部分。</p><pre><code class=\"language-python\">class MyCNN(nn.Module):\n&nbsp; &nbsp; def __init__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n&nbsp; &nbsp; &nbsp; &nbsp; # conv1输出的特征图为222x222大小\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = nn.Linear(16 * 222 * 222, 10)\n\n&nbsp; &nbsp; def forward(self, input):\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.conv1(input)\n&nbsp; &nbsp; &nbsp; &nbsp; # 进去全连接层之前，先将特征图铺平\n&nbsp; &nbsp; &nbsp; &nbsp; x = x.view(x.shape[0], -1)\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.fc(x)\n&nbsp; &nbsp; &nbsp; &nbsp; return x\n</code></pre><p>答案：</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\ntransform = transforms.Compose([\n&nbsp; &nbsp; transforms.RandomResizedCrop((224,224)),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n&nbsp; &nbsp; ])\ncifar10_dataset = torchvision.datasets.CIFAR10(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transform,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\ndataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=32,&nbsp; &nbsp; &nbsp; &nbsp;# 输出的batch大小\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;shuffle=True,&nbsp; &nbsp; &nbsp; &nbsp;# 数据是否打乱\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_workers=2)&nbsp; &nbsp; &nbsp; # 进程数, 0表示只有主进程\n\nclass MyCNN(nn.Module):\n&nbsp; &nbsp; def __init__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n&nbsp; &nbsp; &nbsp; &nbsp; # conv1输出的特征图为222x222大小\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = nn.Linear(16 * 222 * 222, 10)\n\n&nbsp; &nbsp; def forward(self, input):\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.conv1(input)\n&nbsp; &nbsp; &nbsp; &nbsp; # 进去全连接层之前，先将特征图铺平\n&nbsp; &nbsp; &nbsp; &nbsp; x = x.view(x.shape[0], -1)\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.fc(x)\n&nbsp; &nbsp; &nbsp; &nbsp; return x\n&nbsp; &nbsp;&nbsp;\ncnn = MyCNN()\noptimizer = torch.optim.SGD(cnn.parameters(), lr=1e-5, weight_decay=1e-2, momentum=0.9)\n\n# 训练3个Epoch\nfor epoch in range(3):\n&nbsp; &nbsp; for step, (images, target) in enumerate(dataloader):\n&nbsp; &nbsp; &nbsp; &nbsp; output = cnn(images)\n\n&nbsp; &nbsp; &nbsp; &nbsp; loss = nn.CrossEntropyLoss()(output, target)\n&nbsp; &nbsp; &nbsp; &nbsp; print('Epoch: {} Step: {} Loss: {}'.format(epoch + 1 , step, loss))\n&nbsp; &nbsp; &nbsp; &nbsp; cnn.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n</code></pre><h2><a href=\"https://time.geekbang.org/column/article/444252\">第15节课</a></h2><p>题目：参考Visdom快速上手中的例子，现在需要生成两组随机数，分别表示Loss和Accuracy。在迭代的过程中，如何用代码同时绘制出Loss和Accuracy两组数据呢？</p><p>答案：</p><pre><code class=\"language-python\">from visdom import Visdom\nimport numpy as np\nimport time\n\n# 实例化窗口\nviz = Visdom(port=6006)&nbsp;\n# 初始化窗口参数\nviz.line([[0.,0.]], [0.], \n         win='train', \n         opts=dict(title='loss&amp;acc', legend=['loss','acc'])\n         )\n\nfor step in range(10):\n&nbsp; &nbsp; loss = 0.2 * np.random.randn() + 1\n&nbsp; &nbsp; acc = 0.1 * np.random.randn() + 0.5\n&nbsp; &nbsp; # 更新窗口数据\n&nbsp; &nbsp; viz.line([[loss, acc]], [step], win='train', update='append')\n&nbsp; &nbsp; time.sleep(0.5)\n</code></pre><p>运行结果如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/18/7eecfc47a74d6fa319d45eb6092dec18.jpg?wh=1920x1075\" alt=\"图片\"></p><h2><a href=\"https://time.geekbang.org/column/article/445886\">第16节课</a></h2><p>题目：在torch.distributed.init_process_group(backend=“nccl”)函数中，backend参数可选哪些后端，它们分别有什么区别？</p><p>答案：backend参数指定的通信后端，包括NCCL、MPI、gloo。NCCL是Nvidia提供的官方多卡通信框架，相对比较高效；MPI也是高性能计算常用的通信协议，但是需要自己安装MPI实现框架，例如OpenMPI；gloo是内置通信后端，但不够高效。</p><h2><a href=\"https://time.geekbang.org/column/article/447503\">第18节课</a></h2><p>题目：老板希望你的模型能尽可能的把线上所有极客时间的海报都找到，允许一些误召回。训练模型的时候你应该侧重精确率还是召回率？</p><p>答案：侧重召回率。</p><h2><a href=\"https://time.geekbang.org/column/article/450898\">第19节课</a></h2><p>题目：对于这节课里讲的小猫分割问题，最终只输出1个特征图是否可以？</p><p>答案：可以的，因为小猫分割是一个二分类问题，可以将输出的特征图使用sigmoid函数将输出的数值转换为一个概率，从而进行判断。</p><h2><a href=\"https://time.geekbang.org/column/article/455415\">第20节课</a></h2><p>题目：图像分割的评价指标都有什么？</p><p>答案：mIoU和Dice系数。</p><h2><a href=\"https://time.geekbang.org/column/article/460504\">第21节课</a></h2><p>题目：TF-IDF有哪些缺点呢？你不妨结合它的计算过程做个梳理。</p><p>答案：TF-IDF认为文本频率小的单词就越重要，也就是区分性越强，但是实际上很多情况下，这并不正确。比如一篇财经类文章有一句“股价就跟火箭一样上了天”，这里的“火箭”就会变得非常重要，显然是错误的。怎么办呢？一般我们会对词频做一个条件过滤，比如超过多少次。也会对TF-IDF的公式进行改进，具体改进方法，如果有兴趣的话，你可以借助网络查找相应的文章。</p><h2><a href=\"https://time.geekbang.org/column/article/461691\">第22节课</a></h2><p>题目：词向量的长度多少比较合适呢？越长越好吗？</p><p>答案：不是的，越长的词向量尽管可以更加精细的表示词语的空间位置，但是也会带来计算量的暴涨、数据稀疏等问题，一般来说我们较多的选择64、128、256这样的长度，具体是多少，要靠实验来不断的确定。有的论文给出的建议是n&gt;8.33logN，具体是否可行，还是要结合实际情况来敲定。</p><h2><a href=\"https://time.geekbang.org/column/article/462524\">第23节课</a></h2><p>题目：利用今天训练的模型，编写一个函数predict_sentiment，实现输入一句话，输出这句话的情绪类别与概率。</p><p>例如：</p><pre><code class=\"language-python\">text = \"This film is terrible!\"\npredict_sentiment(text, model, tokenizer, vocab, device)\n'''\n输出：('neg', 0.8874172568321228)\n'''\n</code></pre><p>答案：参考代码如下。</p><pre><code class=\"language-python\"># 预测过程\ndef predict_sentiment(text, model, tokenizer, vocab, device):\n&nbsp; &nbsp; tokens = tokenizer(text)\n&nbsp; &nbsp; ids = [vocab[t] for t in tokens]\n&nbsp; &nbsp; length = torch.LongTensor([len(ids)])\n&nbsp; &nbsp; tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n&nbsp; &nbsp; prediction = model(tensor, length).squeeze(dim=0)\n&nbsp; &nbsp; probability = torch.softmax(prediction, dim=-1)\n&nbsp; &nbsp; predicted_class = prediction.argmax(dim=-1).item()\n&nbsp; &nbsp; predicted_probability = probability[predicted_class].item()\n&nbsp; &nbsp; predicted_class = 'neg' if predicted_class == 0 else 'pos'\n&nbsp; &nbsp; return predicted_class, predicted_probability\n\n# 加载模型\nmodel.load_state_dict(torch.load('lstm.pt'))\n\ntext = \"This film is terrible!\"\npredict_sentiment(text, model, tokenizer, vocab, device)\n</code></pre><h2><a href=\"https://time.geekbang.org/column/article/464152\">第24节课</a></h2><p>题目：Bert处理文本是有最大长度要求的（512），那么遇到长文本，该怎么办呢？</p><p>答案：这是一个非常开放的问题，设置为最大512主要还是兼顾了效率问题，但还是有非常多的解决办法，比如我们之前提到过的关键词提取。或者分别从开头、中间、结尾选择一定长度的内容做运算。不过这些都是比较简单的办法。你还有更好的办法吗？欢迎留言给我。</p><h2><a href=\"https://time.geekbang.org/column/article/464870?\">第25节课</a></h2><p>题目：自2018年BERT被提出以来，获得了很大的成功，学术界陆续提出了各类相关模型，例如我们今天学习的BART。请你查一查还有哪些BERT系列的模型，并阅读相关论文，自行学习一下它们的原理与特点。</p><p>答案：</p><p><a href=\"https://arxiv.org/abs/1906.08237\">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p><p><a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></p><p><a href=\"https://arxiv.org/abs/1909.11942\">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></p><p>最后，再次祝愿你虎年快乐，学习进步，工作顺利！</p>","neighbors":{"left":{"article_title":"用户故事 | Tango：师傅领进门，修行在个人","id":469029},"right":{"article_title":"结束语｜人生充满选择，选择与努力同样重要","id":465858}}},{"article_id":465858,"article_title":"结束语｜人生充满选择，选择与努力同样重要","article_content":"<p>你好，我是方远。</p><p>固定的打招呼开头，但是这节课却是咱们这个专栏的最后一节课了。感谢你一路坚持学习到这里，现在有没有感觉要解放了？哈哈。</p><p>不过解放之前，让我们一起快速回顾一下这门课程咱们学了什么，我也会像电影点映后的发布会那样，顺便揭秘一下，我为什么要这样讲。</p><p>咱们的课程核心是PyTorch实战，但是实战就跟习武一样，得先挑几件趁手的兵器，要不只能秒送人头。</p><p>正所谓“工欲善其事，必先利其器”，所以在最开始的时候，我们并没有上来就聊PyTorch本身，而是讲了如何使用NumPy工具，还一起研究了Tensor的数据结构。</p><p>如果你是一个算法工程师，很容易就会发现，在非常多的场景下都有NumPy的出现，因为无论从数据操作的便捷性、友好性还是通用性，NumPy都是强大到没朋友。就算你以后不用PyTorch开发，甚至不做深度学习开发，NumPy也是你躲不开的宿命。所以，我希望你在以后的学习中，对于NumPy以及Pandas的内容还是要多多关注。</p><p>而我们之所以要花不少篇幅来学习Tensor，也是因为它的通用性。不过咱们以前都习惯了dict、list、set这样的数据结构，以及NumPy中ndarray这样的通用数据处理格式，忽然转变操作数据的方法，肯定是有一个适应的过程，特别是像数据切片、数据变形、维度变化这样看不见、摸不着的操作，你感觉一个脑袋两个大，也是很正常的。</p><!-- [[[read_end]]] --><p>我想和你强调的是，Tensor玩转了之后，在以后的深度学习开发中，无论你使用PyTorch还是TensorFlow亦或是未来新出来的框架，都可以让你快速上手，从青铜迅速变王者。</p><p>掌握基础工具的使用，就如同选好了一把大宝剑，接下来就得学习训练模型所需要的招数和心法了。深度学习常用的内容比如卷积、损失函数、优化函数、梯度更新都是重中之重，甚至可以说是缺一不可了，咱们花了不少篇幅来细致地讲解其中的知识点，相信你现在应该也到了张口就来的地步了。</p><p>为了聚焦重点，其他类似全连接、池化等简单的结构咱们则是简要带过，你有兴趣可以课后了解，毕竟咱们要快速上手的。既然是实战，咱们就要从实际、客观、高效的角度去借鉴前人的工作成果，于是例如Torchvision、可视化工具、分布式训练方法等内容可以让我们少绕弯路，直达目标。</p><p>兵器也有了，武功也会了，就得出门下山历练了，否则光说不做，就全成了嘴上功夫。每一个成就的解锁，都需要我们挑战一个个艰难的任务，所以我特别为你安排了图像和文本算法任务这两个大Boss。</p><p>细心的同学不难发现，在每个任务开始之前，我都会向你介绍这个大Boss的背景，比如在NLP部分，我先向你介绍了NLP领域的几大内容、常用算法等，这样你才能更好地理解任务的目的和解决思路。</p><p>当你完成了整个课程，就相当于完成了习武历练的过程，从此以后，你就可以独立完成基于PyTorch的深度学习Pipeline了，是不是非常棒？</p><p><img src=\"https://static001.geekbang.org/resource/image/08/7a/08b96da4677066769fe3e6246f70237a.jpg?wh=1920x1418\" alt=\"图片\"></p><p>但是这就是全部吗？<strong>不是，还差得远。</strong></p><p>通过这样一个PyTorch的专栏，最根本的目的不是让你知道如何使用它，而是希望你借助它高效便捷地了解深度学习。这个<a href=\"https://pytorch.org/docs/stable/index.html\">链接</a>是PyTorch的官方文档。你会发现这里面的内容浩如烟海，但是我们没有必要把它所有的函数功能都学会，它是一个工具，仅此而已。</p><p>所以，临别之际，对于即将踏上AI之途的小伙伴，我有几句话想对你说，简单概括是“五个保持”。</p><p>第一，保持好奇。人工智能是一个更新迭代非常快的领域，以前很火的内容，可能没过多久就过时或者很少使用了，比如之前的RNN之于现在的Attention。所以你一定要多看论文，每年的顶会论文都是最好的学习资料。</p><p>后面我列出了CV和NLP领域的顶会，供你参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/e5/6047de426d3c10c4cbe988bd9b8526e5.jpg?wh=1920x933\" alt=\"图片\"></p><p>看完这份清单，有心人自然知道去搜搜这些会议的时间。会议结束后，你就可以自行查找各种论文的分析介绍了，当然还是建议你尽可能看原版。不过哪怕你看不懂长篇大论的英文论文，没有关系，中文版本的论文分析介绍也不少，这样也可以提高。</p><p>除了这些会议，还有不少是比较综合性的人工智能会议，比如IEEE、ICLR等等，你同样可以按需关注。</p><p>第二，保持平和。作为过来人，我想说，在以后的深度学习开发过程中，你会见识到各种各样奇葩的结果。</p><p>比如明明训练过程中各项指标都好好的，一到预测环节就崩盘的情况。这都算常见的。再比如两张差不多的图片，只因为其中一张多了几个色块或者形状，导致最后的结果大相径庭，这种case查起来就非常的“蓝瘦香菇”了。又或者你会因为业务提供的数据资源有限而苦恼，要么量太少、要么太不平衡，这些都是很难用技术的思路去解决的。</p><p>所以你要多多参与项目，多接触不同的场景，慢慢的……你就习惯了哈哈。当然，这不意味着破罐子破摔，而是随着历练的增多，你终会找到或者学到解决这些困难的办法。其实每一个深度学习算法工程师的成长，都是靠着一个又一个的狗血问题一路走来的。</p><p>第三，保持谦逊。诚然，算法工程师特别是深度学习算法工程师，目前是IT领域第一梯队的存在，也是好多IT人羡慕的工作，但是一定要记住山外有山，要多向别人取经，多向前人借鉴，才能让自己一直有足够的竞争力。</p><p>第四，保持童心。实不相瞒，我打小就想当超级英雄拯救世界，一直到今天也这样。这并不可笑，反而这是很真实的自我。保持童心，可以让你时时刻刻充满天马行空式的想象，而<strong>AI领域从业者最大的限制恰恰不是技术，而是想象力</strong>。</p><p>有了想象力，你可以开发仿生的阿尔法狗，你可以开发堪比艺术家的创作AI，你可以心血来潮搞一个大变脸AI把自己无缝放入任何好莱坞大片中。在AI的世界，你可以改变世界，就可以成为真正的超级英雄。</p><p>还有一条，保持活力。这与技术无关，码农的秃顶、肥胖、腰间盘突出、前列腺炎、啤酒肚、油腻、格子衫……这些已经在网络上被黑了无数次了。其实这一方面是他人的刻板印象，一方面可能真的是咱们的现状。</p><p>我建议你跟我一样，能够经常锻炼，参加体育活动，工作之余来几个俯卧撑，下班之后撸撸铁，周六、日的时候约上三五好友，骑骑车、打打球。因为你的人生中，工作只占了非常非常非常小的一个比例，多出去走走，发现更大更好的世界。</p><p><img src=\"https://static001.geekbang.org/resource/image/51/72/51d923cfaba3e9f1d736d2bd965f2072.jpg?wh=6068x3734\" alt=\"\"></p><p>五条“保持”我就说完了，还记得开篇词，我是如何高大上地介绍我自己的吗？其实还有一部分我没有说，现在我也愿意跟你分享出来。</p><p>作为一名80后，其实小的时候电脑这玩意儿并没有那么普及。一般都是家里条件非常好的同学，才会有一台大头机。所以，那时候每周一节的微机课就成为了我们最期待的、事实上的“游戏课”。电脑对于我来说，就是打打游戏，查查资料，看看电影，仅此而已。</p><p>面临高考报志愿这个人生抉择的时候，我听了家长和老师的话，选了制造、自动化、土木相关的超热门专业志愿，分数差了几分没上去，调剂到了计算机，也算是阴差阳错。但之后随着对编程的深入了解，我越来越觉得自己喜欢上了编程，喜欢上了算法。或许是命中注定，我就应该走这条路。</p><p>再后来我发现，这个时代是永远在快马加鞭发展着的：人工智能将影响所有人的生活。所以，我选择了AI方向。每个方向都有它独到的魅力，而AI的魅力在于它是没有边界的，就像绿灯侠一样。就像我前面说的那样，<strong>AI很多时候没有技术限制，唯一能限制你的，只有你的想象力</strong>。</p><p>当然，学习和提升自己是痛苦且枯燥的，这个过程充满了复杂的公式，难缠的优化方法，还有玄学一般的调参经历。但是这种自我蜕变是极具成就感的：我参与完成的搜索引擎推荐算法项目，时至今日仍然每天在为数亿用户服务；我主导的资讯产品文本算法项目，同样为数以千万计的用户更高效地获取信息而默默运转着；我参加的多模态算法项目，为无数的儿童与青少年提供了纯净的网络环境……</p><p>好了，最后一课的鸡汤终于要登场了，请你记住：</p><p><strong>阴差阳错是一种选择</strong></p><p><strong>命中注定也是一种选择</strong></p><p><strong>自我蜕变更是一种选择</strong></p><p><strong>人生充满了选择，选择与努力同样重要</strong></p><p>时间过得真快，快到让我觉得第一节课也就是几天前的事情。时间过得好慢，慢到使我迫不及待地希望在更多的课程中与你一同进步。</p><p>下课，再会。</p><p>最后，我希望你可以填一下后面这张<a href=\"https://jinshuju.net/f/fUxWMD\">毕业问卷</a>，说说你学习这个专栏的感受。</p>","neighbors":{"left":{"article_title":"答疑篇｜思考题答案集锦","id":483203},"right":[]}}]