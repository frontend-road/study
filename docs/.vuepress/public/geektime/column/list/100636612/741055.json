{"id":741055,"title":"第 4 章 分布式协同(1)","content":"<p>分布式系统引入了应用和服务的分布式部署，会根据需求将应用和服务部署在不同服务器或者不同网络环境中。第 3 章介绍了这些应用和服务是如何相互调用和通信的，本章主要介绍它们是如何协同工作的。当多个应用服务访问同一个资源时会出现互斥现象，为了避免互斥，以及保证数据的一致性，引入了互斥算法；互斥算法是理论基础，在实际工作中我们会利用分布式锁来解决互斥问题；当多个应用服务共同完成一个任务时会出现分布式事务问题，我们会介绍分布式事务的原则和解决方案；应用和服务在集群部署情况下通常会有主从（Master/Slave）之分，主服务器用来写入、读取数据，从服务器（Slave）用来读取数据，当主服务器出现故障时，通常需要选举新的主服务器；最后再看看 ZooKeeper 作为分布式服务协同系统的最佳实践，是如何工作的。总结一下，本章要介绍的内容如下。</p>\n<ul>\n<li>分布式系统的特性与互斥问题</li>\n<li>分布式锁</li>\n<li>分布式事务</li>\n<li>分布式选举</li>\n<li>ZooKeeper——分布式系统的实践</li>\n</ul>\n<h2 id=\"nav_point_78\">4.1　分布式系统的特性与互斥问题</h2>\n<p>想象一下，有两个小孩都想玩玩具，但是玩具只有一个，因此每次只有一个小孩可以玩，且这个小孩玩的时候另一个小孩只能等待。只有当第一个小孩玩完以后，第二个小孩才能得到玩具。在这个例子中，两个孩子争夺的玩具叫作临界资源，他们争夺玩具的动作叫作竞态，每次只有一个孩子能获得玩具的性质叫作互斥。互斥是指每次只允许规定数量的进程进入临界区，超出数量外的进程则无法进入，只能先等待前面的进程完成操作。单机模式下，进程都部署在同一台设备中，但是到了分布式系统中，这些进程部署在不同的网络节点，因此分布式系统的进程互斥需要考虑以下三方面特性。</p><!-- [[[read_end]]] -->\n<ul>\n<li><strong>分布式系统的互联网特性</strong>：分布式系统中的各台服务器是通过网络连接起来的，竞争临界资源的进程不一定都部署在同一台服务器上，进程之间的通信依赖于网络，在请求临界资源时会由于网络原因出现延迟、丢包等情况。</li>\n<li><strong>分布式系统没有统一时钟</strong>：一个进程在请求临界资源时，可能有其他进程正在使用该资源，因此这个进程会进入等待状态。如果等待时间太长，该进程就会因为长时间无法获得临界资源的使用权而饥饿。所以这里有必要记录获取临界资源所需的时长，以保证使用资源和等待资源的时间在一定范围内。可是分布式系统中的进程分布在不同的服务器上，每个服务器都有自己的物理时钟，这样它们之间很难进行时钟同步，因此需要一个全局的时钟机制对分布式系统中的时钟进行统一管理。</li>\n<li><strong>服务器网络出现故障</strong>：分布式系统中的服务器，以及每台服务器中运行的进程都可能出现故障，当有服务器或进程出现故障时，系统中的其他服务器或进程该如何感知到。针对这个问题，系统提供了检测故障的机制，从而提高了系统整体的稳定性。</li>\n</ul>\n<p>鉴于以上多个进程争夺临界资源的互斥操作和分布式系统的特性，就有了分布式互斥算法。在考虑和选择分布式互斥算法时需要遵循以下几个标准。</p>\n<ul>\n<li><strong>互斥性</strong>：分布式系统中的临界资源在任意时刻，都只能被一个进程访问，不能出现多个进程同时访问同一资源的情况。</li>\n<li><strong>无饥饿</strong>：每个申请访问临界资源的进程都能够如愿以偿，即便等待也不会出现无限等待的情况，最终肯定能够访问到资源。这里需要考虑资源的访问时长，以及当访问资源的进程出现故障，不能及时释放资源时该怎么办。</li>\n<li><strong>无死锁</strong>：死锁是指两个进程分别持有两个资源，而又同时等待对方手中的资源，使得双方以及其他进程都无法获取这两个进程持有的临界资源。</li>\n<li><strong>公平性</strong>：算法应提供一种机制保证所有进程都能请求临界资源，并且有机会获取该资源。</li>\n</ul>\n<p>说完了分布式互斥算法的特性以后，下面介绍三类互斥算法。</p>\n<h3 id=\"nav_point_79\">4.1.1　集中互斥算法</h3>\n<p>前面例子讲的是两个小孩争夺同一个玩具，当一个孩子玩的时候另一个孩子只能等待，如果此时还有其他几个孩子也都想玩这个玩具，该如何处理？等待的孩子一多，就存在先来后到的关系，谁也说不清谁先谁后，要是有一个老师对等待的孩子们进行排队，然后让孩子们按照排队顺序玩玩具，问题不就解决了吗，这就是集中互斥算法的思想。集中互斥算法的核心是增加一个全局协调者，由该协调者帮助进程访问并使用临界资源。如图 4-1 所示，当节点进程 1 和节点进程 2 都想访问临界资源时，首先向协调者发起申请，协调者会将这两个节点进程放到自己本地的顺序链表中，这个链表就像一个等待队列，节点进程和申请访问资源的时间戳都记录在其中。各节点进程会按照自己在顺序链表中的排列顺序访问临界资源，从而保证了互斥性。另外，如果一个节点进程长时间使用资源而不释放，或者由于内部、网络故障无法正常工作，协调者都可以将其从顺序链表中移除，让后面排队的节点进程得到资源的访问权限，这也保证了进程访问的无饥饿。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00383.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-1　集中互斥算法示意图</strong></p>\n<p>了解了集中互斥算法的基本方式以后，再来看看其具体的调用流程。图 4-2 模拟了节点进程 1 和节点进程 2 通过协调者访问临界资源的流程，具体为如下几步。</p>\n<p>(1) 节点进程 1 率先向协调者发起访问临界资源的请求，其请求时间戳记作时间戳 1。协调者会在本地维护一个针对此临界资源的链表，其中记录着发出请求的节点进程和请求时间戳。</p>\n<p>(2) 此时节点进程 2 也向协调者发出访问临界资源的请求，由于请求时间比节点进程 1 稍晚一点，所以请求时间戳记作时间戳 2，同样协调者会在临界资源访问链表中插入这条请求记录。由于此次请求的时间晚于节点进程 1，因此节点进程 2 排在节点进程 1 后面。</p>\n<p>(3) 协调者通过临界资源链表中各节点进程的排列顺序，选择第一个节点，使之获得访问临界资源的权限。这里的第一个节点是节点进程 1，因此就给节点进程 1 返回临界资源。</p>\n<p>(4) 节点进程 1 接收到协调者返回的可访问临界资源的消息，就会去访问临界资源。访问完毕后，节点进程 1 主动通知协调者已访问完毕。</p>\n<p>(5) 协调者接收到消息后，查看链表，发现节点进程 1 后面排着的是节点进程 2，于是通知节点进程 2 临界资源可用了。</p>\n<p>(6) 节点进程 2 接收到协调者的通知后，开始访问临界资源，同样使用完毕后会通知协调者。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00384.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-2　集中互斥算法流程</strong></p>\n<p>不难看出，在集中互斥算法中，每个节点进程要想获取一次临界资源的访问权限，都需要进行三次消息交互。</p>\n<p>第一次，向协调者发送访问临界资源的请求。</p>\n<p>第二次，协调者通过链表中进程的先后次序，选择排在最前面的节点进程，向其发送资源可用的消息。</p>\n<p>第三次，节点进程在使用完临界资源后通知协调者。</p>\n<p>集中互斥算法引入了协调者，解决资源互斥问题，其中协调者需要面对所有节点进程的资源请求，在高并发系统中有可能成为系统瓶颈，同时可能出现单点故障。因此这种算法需要考虑协调者的可用性和可扩展性。</p>\n<h3 id=\"nav_point_80\">4.1.2　基于许可的互斥算法</h3>\n<p>基于许可的互斥算法的思想是当进程访问临界资源时，先向分布式系统中的其他节点发送请求，请求获取临界资源的使用权限，等得到所有节点或者部分节点的同意后，方可访问临界资源。延续前面的例子，就是小孩在玩玩具之前先征求在场所有小朋友的意见，等所有人都同意以后才开始玩。Lamport 算法是许可算法的一种，该算法中提到了逻辑时间戳的概念，解决了分布式系统中没有统一时间戳的问题。由于分布式系统没有统一的时钟，因此无法知道各个进程申请访问临界资源的先后顺序。在集中互斥算法中是通过协调者记录先后顺序的，而在 Lamport 算法中，每个节点进程都会维护一个逻辑时钟，当系统启动时，所有节点上的进程都会对这个时钟进行初始化，每当节点进程向其他节点进程发起临界资源访问申请的时候，就会将这个逻辑时间戳加 1。同样，每个节点进程在接收到来自其他节点进程的申请请求时，也会更新自己的逻辑时钟，具体地，对自己逻辑时钟与发出请求的节点的逻辑时钟进行比较，选出最大值后加 1。在 Lamport 算法中，根据逻辑时钟的大小来选择由哪个节点进程获取临界资源的访问权限：逻辑时钟最小的节点将首先获得临界资源的使用权；如果逻辑时钟相同，就让节点编号较小的进程使用资源。同时每个节点进程都会维护一个请求队列，这个队列按照时间戳的优先级对各节点进程的请求进行排序，时间戳越小的节点进程排在越前面。</p>\n<p>Lamport 算法对时间戳的计算过程总结如下：</p>\n<ul>\n<li>每个节点进程在初始化时，时间戳为 0；</li>\n<li>当节点进程发起资源使用申请时，将其自带的时间戳加 1；</li>\n<li>发起申请的节点进程将节点编号和时间戳一起发送给分布式系统中的其他节点；</li>\n<li>其他节点接收到申请消息以后，更新本地的时间戳，更新公式是 <strong>Max（本地时间戳，消息中的时间戳）＋ 1</strong>。</li>\n</ul>\n<p>节点进程对系统中其他节点进程发送请求时，会使用三种消息类型，分别是 REQUEST、REPLY 和 RELEASE。每种消息都包含发送节点的 ID 和逻辑时钟（时间戳）。Lamport 算法的示意图如图 4-3 所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00385.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-3　Lamport 算法示意图</strong></p>\n<p>具体过程如下。</p>\n<p>(1) 节点进程 1 完成对时间戳的初始化以后，需要访问临界资源，于是通过 REQUEST 消息将节点 ID 和时间戳发送给节点进程 2 和节点进程 3。</p>\n<p>(2) 节点进程 2 和节点进程 3 接收到请求以后，根据 Lamport 算法更新本地的时间戳，同时将节点进程 1 的请求添加到本地的资源访问链表中。这样做的目的是如果此时还有其他节点也在申请访问资源，那么根据时间戳算法，其他节点的时间戳比节点进程 1 大，因此在链表中排在节点进程 1 后面。排队以后，节点进程 2 和节点进程 3 会发送 REPLY 消息给节点进程 1，也就是许可节点进程 1 对临界资源的访问。</p>\n<p>(3) 节点进程 1 接到许可信息以后，对临界资源进行访问。</p>\n<p>(4) 节点进程 1 访问完资源以后，会向节点进程 2 和节点进程 3 发出 RELEASE 消息，这两个进程接收到消息以后，会把节点进程 1 从链表中删除，此时如果链表中还排列着其他节点进程的请求，则会重复第 (2) 步。</p>\n<p>按照 Lamport 算法的思路，在一个含 <em>n</em> 节点的分布式系统中，访问一次临界资源需要发送 3 (<em>n</em>－1) 次消息，其中 REQUEST、REPLY 和 RELEASE 消息各发送 <em>n</em>－1 条。发送过多的消息容易导致网络堵塞，因此 Richard&amp;Agrawal 算法对 Lamport 算法进行了改进，将 Lamport 算法中的 RELEASE 消息和 REPLY 消息合并为一个，这样消息的发送次数就从 3(<em>n</em>－1) 降到了 2(<em>n</em>－1)。在 Richard&amp;Agrawal 算法中，节点进程访问临界资源时，发送 REQUEST 消息到其他节点进程，其他节点进程接收到请求后做以下动作：如果自己没有访问临界资源，就直接返回 REPLY 消息，允许发送请求的节点进程访问；如果自己正在访问临界资源，就对自己和发送请求的节点的时间戳进行排序，若后者的时间戳排在前面，则不回复任何消息，否则返回 REPLY 消息，允许其访问临界资源。请求节点接收到 REPLY 消息以后对临界资源进行访问，访问完成后向请求访问链表中的请求节点发送 REPLY 消息，链表中的节点根据优先级发送下一轮的资源访问请求。</p>\n<p>由上面两个算法的原理可以看出，基于许可的互斥算法是一个会征求每个进程意见的公平算法，但随着系统中访问资源的进程增加，其他通信成本也会变多。因此，这种算法可以用在临界资源使用频率较低且系统规模较小的场景。</p>\n<h3 id=\"nav_point_81\">4.1.3　令牌环互斥算法</h3>\n<p>孩子们围坐一圈按照一定方向（顺时针、逆时针）传递一个令牌，拿到令牌的孩子就可以玩玩具，玩完以后将令牌传给旁边的孩子，以此类推。获得令牌的孩子拥有玩玩具的权限，玩完以后将令牌往下传递，这就是令牌环互斥算法的思想。如图 4-4 所示，令牌环互斥算法中的所有节点进程构成一个环结构，每个节点进程都有一个唯一 ID 作为标识，且都会记录对应前驱节点和后继节点的地址。令牌作为访问临界资源的许可证，会按照一定方向（顺时针、逆时针）在节点进程之间传递，收到令牌的节点进程有权访问临界资源，访问完成后将令牌传送给下一个进程；若拿到令牌的节点进程不需要访问临界资源，则直接把令牌传递给下一个节点进程。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00386.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-4　令牌环互斥算法示意图</strong></p>\n<p>令牌环互斥算法的优点显而易见：令牌具有唯一性，每个时刻只有一个节点进程可以访问临界资源，能够解决分布式系统中临界资源的互斥问题；由于令牌在环中传递，因此每个节点进程都有获得资源访问权的机会，不会出现饥饿的现象，保证了公平性；在令牌环中，一个进程访问临界资源最多需要 <em>n</em>－1 条消息（<em>n</em> 是环上的节点进程数量），也就是令牌在环内循环一周发送的消息数。不过该算法同样存在缺点，就是令牌一旦丢失，恢复起来会比较困难，令牌的唯一性导致该算法容错性比较低。而且在令牌环中，如果节点进程发生变动（例如有节点进程加入或者退出），则会导致整个令牌环重构，在重构过程中节点进程是不能访问临界资源的。即使环内的节点进程对临界资源访问频率较低，令牌也会不断地在环中传递，这样的行为会造成较大的资源浪费。因此，令牌环互斥算法适用于系统规模较小，系统中临界资源使用频率较高、使用时间较短的场景。</p>\n<p>通过 4.1.1 节、4.1.2 节、4.1.3 节对三类分布式系统中互斥算法的描述，相信大家对这种算法的实现原理和应用场景已经有了理解。如表 4-1 所示，我们对三种算法的描述、优点、缺点、应用场景做了一个对比。</p>\n<p><strong>表 4-1　对比三种分布式互斥算法</strong></p>\n<table width=\"90%\" border=\"1\">\n<thead>\n<tr>\n<th><p>&nbsp;</p></th>\n<th><p>集中互斥算法</p></th>\n<th><p>基于许可的互斥算法</p></th>\n<th><p>令牌环互斥算法</p></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><p><strong>算法描述</strong></p></td>\n<td><p>协调者负责处理节点进程的请求，请求按照排队顺序访问临界资源</p></td>\n<td><p>先征求系统中其他节点进程的意见，得到同意以后方可访问临界资源</p></td>\n<td><p>系统中的所有节点进程组成一个环，通过令牌传递的方式，轮流访问临界资源</p></td>\n</tr>\n<tr>\n<td><p><strong>优点</strong></p></td>\n<td><p>实现简单、通信高效</p></td>\n<td><p>可用性高</p></td>\n<td><p>单个进程通信效率高</p></td>\n</tr>\n<tr>\n<td><p><strong>缺点</strong></p></td>\n<td><p>依赖协调者的可用性</p></td>\n<td><p>通信成本高、复杂度高</p></td>\n<td><p>令牌单点故障、环重构、资源使用频率低导致的无用通信</p></td>\n</tr>\n<tr>\n<td><p><strong>应用场景</strong></p></td>\n<td><p>在保证协调者可用性的情况下，广泛适用于分布式场景</p></td>\n<td><p>临界资源使用频率比较低、分布式系统规模相对较小的场景</p></td>\n<td><p>分布式系统规模较小、进程对资源访问频率较高的场景</p></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"nav_point_82\">4.2　分布式锁</h2>\n<p>在 4.1 节中，我们介绍了分布式系统中的节点进程在访问临界资源时，是如何协调多个节点进程对资源的访问顺序的，当具有访问权限的节点进程使用临界资源时，其他节点进程必须等待，直到该进程释放资源。那么在实际开发过程中，如何让节点进程产生顺序？在访问资源时又如何设置节点进程的访问权限？这就是这一节要讲的内容。如果说上一节讲的是分布式互斥的理论，那么这一节就是介绍分布式互斥算法的最佳实践，即如何实现分布式锁。另外，分布式应用面对的多是高并发、大流量场景，因此在实际应用中通常会使用集中互斥算法，同时保证协调者的可用性。</p>\n<p>接下来通过一个熟悉的场景带大家进入分布式锁的世界。相信大家都有网上购物的经历，每逢双 11 打折促销季，商家都会推出各种不同的秒杀活动，吸引消费者的眼球。秒杀活动对于消费者而言，秒杀的是商品，而对秒杀系统而言，秒杀的就是商品库存。商品的购买量一旦超过库存量就不能再卖了。在秒杀系统中，为了应对高并发的下单请求，通常会对订单服务进行水平扩展，扩展订单服务和系统能够承载多少下单请求密切相关。但是，无论有多少个订单服务，针对扣减库存的记录都只有一条。假设商品库存量只有 100，秒杀时同时到达了 200 个下单请求，并且都对库存进行扣减，则库存量瞬间就会变成  100。如果用户的下单量超过库存量后还允许下单，就是“超卖”，在设计系统时需要避免这点。为了保证不出现“超卖”，会在“库存”上加一把锁，每次只有一个进程可以得到这把锁，然后扣减库存。由于订单服务是分布式的，因此加的这把锁又叫作分布式锁。在并发量更高的情况下，也可以对库存记录进行分段，进一步提高并发量。我们将从以下四个方面讲述分布式锁。</p>\n<ul>\n<li>分布式锁的由来和定义</li>\n<li>通过 Redis 缓存实现分布式锁</li>\n<li>通过 ZooKeeper 实现分布式锁</li>\n<li>分布式分段加锁</li>\n</ul>\n<h3 id=\"nav_point_83\">4.2.1　分布式锁的由来和定义</h3>\n<p>通常来讲，在消费者下订单时也会对库存进行扣减，此时订单服务会更新库存变量，其实就是将其值减 1。如果有两个用户同时对同一商品下单，就会造成对同一商品库存进行扣减的情况。我们将库存称作临界资源，扣减库存的动作称为竞态。切换到在进程内，竞态可以理解为两个线程（两个用户请求）争夺临界资源，解决办法是在这个资源上加一把锁。如图 4-5 所示，线程 B 先到达，于是让其持有这把锁，并访问临界资源，之后线程 A 到达时由于没有锁，就进入等待队列，等线程 B 访问完毕并释放锁以后，线程 A 持有锁，可以访问临界资源。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00387.jpeg\" alt=\"\" width=\"55%\" style=\"width: 55%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-5　进程内对临界资源的竞态操作</strong></p>\n<p>为了面对高并发的下单请求，对订单服务做了水平扩展，因此订单服务通常是分散部署的。原来是进程内的多线程对临界资源产生的竞态，现在变成了分布式应用系统中的多个服务（进程）对临界资源的竞态。如图 4-6 所示，对订单服务进行了水平扩展，将其从原来的一个扩展为两个，分别是订单服务 A 和 B，这两个服务可能会同时扣减库存。由于是不同的服务或者进程，它们不知道对方的存在，因此共同访问的临界资源应该独立于服务，保存在一个公共的存储区域中，让水平扩展的订单服务都可以访问到。另外，可以通过锁机制，保证多服务并发请求时的竞态不会造成超卖情况，这和解决进程内竞态的方式相同。通过给临界资源加上一把锁，可以让并发操作变成串行的方式。这个锁就是分布式锁，其实现方式多种多样，比如通过数据库、Redis 缓存、ZooKeeper 实现。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00388.jpeg\" alt=\"\" width=\"55%\" style=\"width: 55%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-6　分布式锁示意图</strong></p>\n<p>用数据库实现分布式锁比较简单，就是创建一张锁表。要锁住临界资源并对其访问时，在锁表中增加一条记录即可；删除某条记录就可释放相应的临界资源。数据库对临界资源做了唯一性约束，如果有访问临界资源的请求同时提交到数据库，数据库会保证只有一个请求能够得到锁，然后只有得到锁的这个请求才可以访问临界资源。由于此类操作属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、对性能要求较高的场景中使用得并不多，这里不做详细介绍。本书重点介绍的是通过 Redis 和 ZooKeeper 实现分式锁。</p>\n<h3 id=\"nav_point_84\">4.2.2　通过 Redis 缓存实现分布式锁</h3>\n<p>前面提到库存作为临界资源会遭遇高并发的请求访问，为了提高效率，可以将库存信息放到缓存中。以流行的 Redis 为例，用其存放库存信息，当多个进程同时请求访问库存时会出现资源争夺现象，也就是分布式程序争夺唯一资源。为了解决这个问题，需要实现分布式锁。</p>\n<p>如图 4-7 所示，假设有多个扣减服务用于响应用户的下单请求，这些服务接收到请求后会去访问 Redis 缓存中存放的库存信息，每接收一次用户请求，就将 Redis 中存放的库存量减去 1。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00389.jpeg\" alt=\"\" width=\"83%\" style=\"width: 83%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-7　通过 Redis 缓存实现分布式锁</strong></p>\n<p>一个进程持有锁后，就可以访问 Redis 中的库存资源，且在其访问期间其他进程是不能访问的。如果该进程长期没有释放锁，就会造成其他进程饥饿，因此需要考虑锁的过期时间，设置超时时间。</p>\n<p>这里的超时时间需要考虑两方面问题。</p>\n<ul>\n<li>资源本身的超时时间，一旦资源被使用一段时间后还没有被释放，Redis 就会自动释放该资源，给其他服务使用。</li>\n<li>服务访问资源的超时时间，如果一个服务访问资源超过一段时间，那么不管这个服务是否处理完，都要马上释放资源给其他服务使用。</li>\n</ul>\n<p>如何设置两种超时时间需要根据具体情况具体考虑，也有可能只选择其中一种进行设置。由于下单服务中的扣减操作属于核心操作，因此会用到第二种方式。如果到达规定时间，下单服务还没有处理完库存资源，就重新申请资源并且延长持有锁的时间。</p>\n<p>说完了通过 Redis 缓存实现分布式锁的原理，再来看看如何实践。</p>\n<p>这里以 Redisson 为例介绍如何实现分布式锁，Redisson 是架设在 Redis 基础上的一个 Java 驻内存数据网格（In-Memory Data Grid）。Redisson 在基于 NIO 的 Netty 框架上，充分利用了 Redis 键值数据库提供的一系列优势，在 Java 实用工具包中常用接口的基础上，为使用者提供了一系列具有分布式特性的常用工具类。</p>\n<p>下面的代码通过 Redisson 在 Java 中实现 Redis 分布式锁：</p>\n<pre class=\"code-rows\"><code>public void testLock(){\n    Config config = new Config();\n    config.useClusterServers().addNodeAddress\n        (\"redis://192.168.1.1:7002\");\n    RedissonClient redisson = Redisson.create(config);    ①\n    RLock lock = redisson.getLock(\"lockName\");    ②\n    try{\n        //lock.lock(10, TimeUnit.SECONDS);   ③\n        boolean result = lock.tryLock(5, 10, TimeUnit.SECONDS);    ④\n        //若返回 true，表示获取锁成功，可以执行后面的业务逻辑\n        if(result){\n            ⑤\n        }\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    } finally {\n        lock.unlock();   ⑥\n    }\n}</code></pre>\n<p>下面分析一下上述代码。</p>\n<p>① 先配置 Redis Server 的地址信息，然后新建一个 Redisson 的客户端 <code>redisson</code>，让其与 Redis Server 通信。</p>\n<p>② <code>redisson</code> 的 <code>getLock</code> 方法需要传入参数 <code>Key</code>。这个 <code>Key</code> 是需要锁住的临界资源，在秒杀系统中就是库存，上面代码中输入的是 <code>\"lockName\"</code>，可以根据业务将之设置为某商品的库存 ID，例如 <code>\"iphoneStock\"</code>。</p>\n<p>③ 加锁 10 s 以后自动解锁，无须调用 <code>unlock</code> 方法手动解锁。这部分代码被注释起来，如果需要可以使用。</p>\n<p>④ 然后通过 <code>lock</code> 中的 <code>tryLock</code> 方法传入几个参数。分别是 <code>watiTime</code>（等待时间）、<code>leaseTime</code>（过期时间）、<code>TimeUnit</code>（时间单位）。我们这里输入的是 <code>5</code>、<code>10</code> 和 <code>TimeUnit.SECONDS</code>，表示尝试加锁，最多等待 5 s，上锁 10 s 以后自动解锁。如果 <code>tryLock</code> 方法的返回值为 <code>true</code>，表示获取锁成功，可以执行后面扣减库存的业务；如果返回 <code>false</code>，表示不可以继续执行下面的逻辑，需过段时间再通过 <code>tryLock</code> 方法访问锁，看其是否释放了（可以加入 <code>While</code> 循环）。</p>\n<p>⑤ 这里是获取锁成功后执行的具体业务逻辑，例如扣减库存等。</p>\n<p>⑥ 持有锁的进程执行完毕以后通过 <code>unlock</code> 方法释放锁，这时其他进程就可以获取锁了。</p>\n<p>这段代码看上去比较简单，由于 Redisson 已经做了很好的封装，因此我们只需要设置临界资源的 <code>Key</code> 以及过期时间就好了。如图 4-8 所示，我们一起来看看 Redisson 内部是如何实现加锁流程的。</p>\n<ul>\n<li>进程在申请锁时，会先判断“需要锁定的临界资源的 <code>Key</code> 是否存在？”。</li>\n<li>如果 <code>Key</code> 不存在，就走右边的流程，获取这个 <code>Key</code> 也就是临界资源，并且保存请求 ID，因为会有很多进程来请求这个 <code>Key</code>，为了区别这些进程需要保存访问进程的请求 ID。同时保存 <code>Key</code> 的过期时间，后面就是执行扣减库存业务了。</li>\n<li>如果存在 <code>Key</code>，说明已经有进程获取了这个 <code>Key</code> 的使用权，也就是有进程正在扣减库存了，那么走左边的流程。判断请求 ID 是否存在，即这次的请求 ID 和已经保存的请求 ID 是否一样。</li>\n<li>如果一样，说明是同一个进程（客户端）再次请求扣减库存，这种情况会被 Redisson 认作请求重入，也就是同一个请求再次获取同一临界资源，这时 Redisson 会将其请求重入+1，并且重新设置过期时间，也就是延长这个请求的处理时间。</li>\n<li>如果不一样，说明是另外一个进程（客户端）发来的请求，由于之前的进程还没有释放临界资源，因此只返回 <code>Key</code> 的生存时间，告诉这个进程前面的进程还有多久才能释放临界资源，也就是还需要等待多久。</li>\n</ul>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00390.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-8　Redisson 内部实现加锁流程</strong></p>\n<h3 id=\"nav_point_85\">4.2.3　通过 ZooKeeper 实现分布式锁</h3>\n<p>上面提到了使用 Redis 缓存实现分布式锁，使同时访问临界资源的进程由并行执行变为串行执行。按照同样的思路，ZooKeeper 中的 DataNode 也可以保证两个进程的访问顺序是串行的，两个库存扣减进程会在 ZooKeeper 上建立顺序的 DataNode，DataNode 的顺序就是进程访问临界资源的顺序，这样避免了多个进程同时访问临界资源，起到了锁的作用。这一节我们首先回忆一下 ZooKeeper 实现分布式锁的原理，再从代码实现的角度来分析。</p>\n<p>在 ZooKeeper 中建立一个 Locker 的 DataNode 节点，在此节点下面建立子 DataNode 来保证先后顺序。即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。细心的朋友会发现这部分内容在第一章提到过，先通过图 4-9 来温习一下整个过程。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00391.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-9　通过 ZooKeeper 实现分布式锁</strong></p>\n<p>整个过程具体如下。</p>\n<p>(1) 当库存服务 A 想要访问库存时，需要先申请锁，于是在 ZooKeeper 的 Locker 节点下面新建一个 DataNode1 节点，表明可以扣减库存。</p>\n<p>(2) 库存服务 B 在服务 A 后面申请库存的访问权限，由于申请锁操作排在服务 A 后面，因此节点会按照次序建立在 DataNode1 下面，为 DataNode2。</p>\n<p>(3) 库存服务 A 在申请锁成功以后访问库存资源，并完成扣减。这段时间内库存服务 B 一直等待，直到库存服务 A 扣减完毕，ZooKeeper 中 Locker 下面的 DataNode1 节点被删除。</p>\n<p>(4) DataNode1 被删除后，DataNode2 作为序号最靠前的节点，对应的库存服务 B 能够访问并扣减库存。</p>\n<p>由图 4-9 可知，ZooKeeper 实现分布式锁的基本原理是按照顺序建立 DataNode 节点。下面来看一下代码实现，本着不重复造轮子的原则，与 4.2.2 节 一样，我们找了 Curator 框架作为代码级别的实施基础。相关代码如下：</p>\n<pre class=\"code-rows\"><code>String connectString = \"192.168.1.1:2181,192.168.1.1:2182,192.168.1.1:2183\";   ①\nString dataNodePath = \"/DataNode\";   ②\nRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 4);   ③\nCuratorFramework client = CuratorFrameworkFactory.newClient(connectString,\n    60000, 15000, retryPolicy);   ④\nclient.start();   ⑤\nfinal InterProcessLock lock = new InterProcessSemaphoreMutex(client,\n    dataNodePath);    ⑥\nnew Thread(new Runnable() {\n    @Override\n    public void run() {\n        try {\n            lock.acquire();   ⑦\n            ⑧\n            lock.release();    ⑨\n        } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }).start();\nCloseableUtils.closeQuietly(client);   ⑩</code></pre>\n<p>Curator 是 Netflix 公司的一个开源的 ZooKeeper 客户端框架。它对分布式锁做了封装，提供了现成的 API 供我们使用。除了分布式锁之外，它还提供了 leader 选举、分布式队列等常用功能。上述代码实施大致分为两部分：初始化参数配置和锁的应用。下面简要解释一下这些代码。</p>\n<p>① 设置 ZooKeeper 服务的地址。地址以“IP 地址∶端口号”的形式填写。如果针对的是集群，则通过逗号分割每个地址。</p>\n<p>② 填写 DataNode 节点的路径。库存扣减请求会按照顺序存放在该路径下面。</p>\n<p>③ 重试策略，设置初始休眠时间为 <code>1000</code>（ms），最大重试次数为 <code>4</code>。</p>\n<p>④ 创建一个客户端，传入 ZooKeeper 服务器的地址和重试策略。设置 session 超时时间为 <code>60000</code>（ms），链接超时时间为 <code>15000</code>（ms）。</p>\n<p>⑤ 创建与 ZooKeeper 的会话，之后可以通过这个会话获取 <code>dataNodePath</code> 中的信息，即库存扣减请求。</p>\n<p>⑥ 创建锁 <code>lock</code>，这是一种不可重入的互斥锁（<code>InterProcessSemaphoreMutex</code>），即使是同一个线程，也无法在持有锁的情况下再次获取锁。</p>\n<p>⑦ 获取锁，此后就可以进行业务操作了。<code>acquire</code> 方法会在 <code>dataNodePath</code> 下面创建临时的时序节点，执行该方法时，它会比较创建的临时节点和父节点下面其他节点的时序，如果创建的临时节点的时序最小，临时节点就获得锁，否则启动一个监听器监听排在它前面的节点。监听器线程通过 <code>object.wait()</code> 方法等待，当前面一个节点有事件产生时（例如排在前面的库存扣减请求执行完毕，节点会触发删除事件），监听器会收到这个事件通知，然后判断当前节点是不是最小节点。如果是最小的节点，就触发线程 <code>notifyAll()</code>，获得锁并且执行后面的扣减操作。</p>\n<p>⑧ 获得锁以后执行的扣减库存的具体逻辑，在这里省略了。</p>\n<p>⑨ <code>release()</code> 方法用于释放锁，删除时序节点，同时排在后面的节点的监听器会被触发，然后查看自己是不是最小的节点，以确定是否可以获得锁。</p>\n<p>⑩ 最后，关闭与 ZooKeeper 的连接。</p>\n<h3 id=\"nav_point_86\">4.2.4　分布式分段加锁</h3>\n<p>前面讲的是分布式锁的原理与实现。通过 Redis 缓存和 ZooKeeper 实现分布式锁依据的都是把并行执行转换成串行执行的思路。现在假设处理一次下单扣减等逻辑需要 20ms，那么同时有 500 个扣减请求串行执行的话，就需要 20ms×500 =10 000ms，也就是 10 s。如果并发数量再高一点，即使可以将订单服务水平扩展成很多个，使用队列做缓冲，也需要很久才能完成。实际上，有了前几节分布式锁的基础，我们可以将原理中的临界资源——库存由一个分成多个，然后将分得的库存段放到临界资源中，例如库存量为 500，将其分成 50 份，每份放 10 个库存，并从 1 到 50 标号，每个号码中就放 10 个库存。当高并发来临时，订单服务按序或者随机请求 1 到 10 号库存段，如果请求的库存段没有被锁，就获取锁并进行扣减操作；如果请求的库存段被其他请求锁住了，就换一个库存段进行扣减。这样在无形中提高了并发量，可以用在秒杀系统中。</p>\n<p>如图 4-10 所示，扣减库存请求 1 获取了库存段 1 的资源后，扣减库存请求 2 再获取库存段 1 时会发现这部分库存资源已经被锁住了，于是找库存段 2 获取资源，发现这部分库存资源并没有被锁住，于是执行扣减操作。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00392.jpeg\" alt=\"\" width=\"68%\" style=\"width: 68%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-10　分布式分段加锁示意图</strong></p>\n<h2 id=\"nav_point_87\">4.3　分布式事务</h2>\n<p>4.1 节和 4.2 节分别提到了分布式系统中的互斥问题及其解决方案——分布式锁。互斥问题讨论的是多个进程对同一个临界资源进行操作的问题，本节将要讨论的是同一个进程对多个临界资源进行操作的问题。以银行转账为例，从 A 账户中转出 100 元钱，分别转入 B 账户 30 元、C 账户 70 元。这个例子中的转账操作涉及对三个临界资源的操作，分别是 A 账户、B 账户和 C 账户。这个转账操作要么全部成功，也就是 A 账户减少 100 元、B 账户增加 30 元、C 账户增加 70 元，要么就全部失败，即 A 账户的钱没有减少、B 账户和 C 账户中的钱没有增加，并不会出现中间状态。我们把具有这些特性的操作称作事务，事务是指访问和更新数据使所做操作的集合，要么全部成功，要么全部失败，并且保证并发执行的事务彼此互不干扰。</p>\n<p>随着分布式系统和微服务的兴起，如何在分布式环境和微服务架构下实现分布式事务成为我们需要解决的问题。如图 4-11 所示，即使在单个服务对单个资源进行操作的单体架构时代，也会遇到单个服务跨多个资源进行操作的现象。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00393.jpeg\" alt=\"\" width=\"55%\" style=\"width: 55%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-11　单体架构时代的资源访问</strong></p>\n<p>到了分布式系统和微服务时代，除了保留单体服务时代的资源访问方式以外，还引入了跨服务、跨资源的访问方式。如图 4-12 所示，分布式架构时代存在跨服务的资源调用，并且服务之间存在依赖关系，服务对资源也存在一对一或者一对多的调用情况。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00394.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-12　分布式架构时代的资源访问</strong></p>\n<p>本节将从单体架构开始介绍分布式事务，主要介绍单体事务的 ACID 特性：原子性（Atomicity）、一致性（Consistency）、隔离性或独立性（Isolation）和持久性（Durability）。</p>\n<p>出现分布式架构以后，服务和应用分布到了不同网络节点上。在这种环境下，要想保证整个系统的 ACID 特性就很难了。于是引入 CAP 理论：一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）。但 CAP 理论同时至多只能支持两个属性，无法三全其美，且高并发系统追求的往往是可用性，因此对 CAP 理论进行进一步扩充，出现了 BASE 理论：基本可用性（Basically Available）、软状态（Soft state）、最终一致性（Eventually consistent）。分布式事务的理论变化过程遵循的核心思想是分布式系统的引入事务无法做到强一致，但每个应用、服务需要根据具体业务达到最终一致性，这也是从 ACID 特性的刚性事务，到 CAP 理论以及 BASE 理论的柔性事务的发展过程。然后，根据分布式事务的原理派生出 DTP 分布式事务模型，在实践中不断打磨后，产生了 2PC 解决方案（两段提交）以及 TCC 解决方案（Try、Confirm、Cancel）。按照以上思路，本节对以下几个方面展开描述。</p>\n<ul>\n<li>ACID 理论</li>\n<li>CAP 理论</li>\n<li>BASE 理论</li>\n<li>DTP 模型</li>\n<li>分布式事务 2PC 解决方案</li>\n<li>分布式事务 TCC 解决方案</li>\n</ul>\n<h3 id=\"nav_point_88\">4.3.1　ACID 理论</h3>\n<p>正如这节开头提到的，分布式事务的处理源于单机架构的事务处理方式。因此单体事务的特性 ACID 也是分布式事务的基本特性，其具体含义如下。</p>\n<ul>\n<li><strong>原子性</strong>。就像上面转账例子中提到的，事务的最终状态只有两种：全部执行成功和全部不执行，并不会出现中间状态。事务操作过程中只要有一个步骤不成功，就会导致整个事务操作回滚（取消），相当于事务没有执行。</li>\n<li><strong>一致性</strong>。一致性是指事务执行前后数据的一致性。正如转账例子中提到的，从 A 账户转出 100 元，B 账户中到账 30 元，C 账户中到账 70 元。完成这个事务操作以后，A 账户减少的钱数与 B、C 账户增加的钱数总和应该是一样的，都是 100 元。这就是所操作数据的大小在操作前后并不会发生变化，只是会从一个状态转变为另一个状态，转账例子中是钱从一个账户转到了其他账户。</li>\n<li><strong>隔离性</strong>。继续以转账为例，假设系统中同时出现了多个转账操作，隔离性就是指这些操作可以同时进行，并且不会互相干扰。简单来说，即一个事务的内部操作对数据状态进行的更改不会影响其他事务。</li>\n<li><strong>持久性</strong>。这个特性的字面意思很容易理解，是指事务操作完成以后，此操作对数据状态的更新会被永久保存下来。更新后的数据会固化到硬件存储资源（例如数据库）上，即使系统发生故障或者网络出现问题，只要能够访问硬件存储资源，就一定能够获取这次事务操作后的数据状态。</li>\n</ul>\n<p>ACID 中的一致性指的是强一致性，换句话说就是事务中的所有操作都执行成功后，才会提交最终结果，从而保证数据一致性。</p>\n<h3 id=\"nav_point_89\">4.3.2　CAP 理论</h3>\n<p>4.3.1 节提到了单体架构中事务处理的 ACID 特性。在出现分布式架构以后，服务和应用分布在不同的网络节点上。在这种环境下，想保证整个系统的 ACID 特性很难，于是引入 CAP 理论。</p>\n<ul>\n<li><p><strong>一致性</strong>。分布式系统中的一致性是指所有数据在同一时刻具有同样的值。如图 4-13 所示，业务代码往数据库 01 节点写入 A 记录，然后数据库 01 把 A 记录同步到数据库 02，业务代码之后从数据库 02 中读出的记录也是 A，那么数据库 01 和数据库 02 中存放的数据就是一致的。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00395.jpeg\" alt=\"\" width=\"60%\" style=\"width: 60%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-13　CAP 中的一致性</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>可用性</strong>。可用性是指在分布式系统中，即使一部分节点出现故障，系统仍然可以响应用户的请求。如图 4-14 所示，数据库 01 和数据库 02 中都存放着记录 A，当数据库 01 挂掉时，业务代码就不能从中获取数据了，但可以从数据库 02 中获取记录 A。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00396.jpeg\" alt=\"\" width=\"55%\" style=\"width: 55%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-14　CAP 中的可用性</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>分区容错性</strong>。假设两个数据库节点分布在两个区，而这两个区的通信发生了问题，因此无法达成数据一致，这就是分区问题，此时需要从一致性和可用性之间做出选择。是选择一致性（C），等待两个区的数据同步了再去获取数据，还是选择可用性（A），只获取其中一个区的数据？</p>\n</li>\n</ul>\n<p>如图 4-15 所示，业务代码对两个节点的通信失败，往数据库 01 写入记录 A 时，需要锁住数据库 02 中的记录 A，不让其他业务代码修改此纪录，直到数据库 01 修改完成。一致性和可用性在此刻是矛盾的，不能兼得。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00397.jpeg\" alt=\"\" width=\"50%\" style=\"width: 50%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-15　CAP 中的分区容错性</strong></p>\n<p>从原则上讲，CAP 理论无法兼顾三个特性，在同一时刻只能保证其中两个，抛弃另一个。下面分别分析三种情况。</p>\n<ul>\n<li><strong>保证一致性、可用性，放弃分区容错性</strong>。在分布式系统中，服务和应用分布在不同的网络节点中，如果这些网络设施无法做到 100% 稳定，就会出现不同程度的网络不连通情况。因此如果放弃分区容错性，就等于放弃使用分布式系统，这不在本书的讨论范围内。</li>\n<li><strong>保证一致性、分区容错性，放弃可用性</strong>。保证一致性和分区容错性，说明这个分布式系统对数据一致性的要求比较高，宁可放弃可用性也要保证系统正常工作，这种情况适合应用在金融领域，即使长时间不响应用户，也要保证交易数据的一致性。当网络分区发生问题，导致数据无法同步时，也要牺牲系统的可用性，让用户一直等待，直到完成事务的整个操作。牺牲可用性虽然会给用户带来使用体验上的不满意，但是最大程度地保障了用户数据的一致性。</li>\n<li><strong>保证可用性、分区容错性，放弃一致性</strong>。保证可用性和分区容错性，说明这个分布式系统非常重视用户体验。这种应用场景多是 ToC 端的应用，例如电商网站。这类应用在承受大流量、高并发的同时，还要保证高可用性。当网络分区发生问题时，允许某些非核心数据暂时不一致，以此换取对用户操作的高响应。</li>\n</ul>\n<h3 id=\"nav_point_90\">4.3.3　BASE 理论</h3>\n<p>由于 CAP 理论导致一个应用同时至多只能支持两个特性，无法三全其美，且高并发系统追求的往往是可用性，因此对 CAP 理论进行进一步扩充，产生了 BASE 理论。</p>\n<ul>\n<li><strong>基本可用性</strong>。基本可用性是指不会因为某个节点出现问题就影响用户请求，即使在流量激增的情况下，也会考虑通过限流降级的办法保证用户请求是可用的。比如电商系统在流量激增时，会将资源向核心业务倾斜，其他业务则降级处理。</li>\n<li><p><strong>软状态</strong>。如果一条数据存在多个副本，则允许副本之间数据同步的延迟，能够容忍较短时间内的数据不一致。其中，数据同步正在进行但还没有完成的状态就称为软状态。如图 4-16 所示，业务代码往数据库 01 写入记录 A 后，数据库 02 中的记录 B 并没有立即同步这个记录 A，而是以延迟同步的方式实现。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00398.jpeg\" alt=\"\" width=\"60%\" style=\"width: 60%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-16　延迟同步</strong></p>\n</li>\n<li><p><strong>最终一致性</strong>。最终一致性是相对于强一致性来定义的，后者是要保证所有的数据都一致，为实时同步，而前者会容忍一小段时间的数据不一致，过了这小段时间后数据保证一致即可。有以下几种一致性。</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<ol>\n<li><p><strong>因果一致性（Causal consistency）</strong></p>\n<p>如图 4-17 所示，进程 1 和进程 2 都对变量 <code>x</code> 进行操作，进程 1 写入变量 <code>x</code>，进程 2 读取变量 <code>x</code>，然后计算 <code>x+2</code>。其中进程 1 和进程 2 的操作就存在因果关系，进程 2 的计算依赖于进程 1 写入的变量 <code>x</code>，如果进程 1 没有写入这个 <code>x</code>，进程 2 就无法完成计算。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00399.jpeg\" alt=\"\" width=\"70%\" style=\"width: 70%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-17　两个进程对同一变量进行操作</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>读己之所写（Read your writes）</strong></p>\n<p>如图 4-18 所示，进程 1 写入变量 <code>x</code> 之后，可以获取自己写入的这个变量值。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00400.jpeg\" alt=\"\" width=\"50%\" style=\"width: 50%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-18　进程写入值的同时获取此值</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>会话一致性（session consistency）</strong></p>\n<p>如图 4-19 所示，如果一个会话实现了读己之所写，那么数据更新后，客户端只要在同一个会话中，就可以看到这个更新的值。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00401.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-19　多进程在同一会话中能够看到相同的值</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>单调写一致性（monotonic write consistency）</strong></p>\n<p>如图 4-20 所示，进程 1 有三个写操作，进程 2 有两个写操作。当这两个进程同时请求系统时，系统会保证按照进程中操作的先后顺序来执行。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00402.jpeg\" alt=\"\" width=\"88%\" style=\"width: 88%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-20　多进程多操作通过队列方式执行</strong></p>\n</li>\n</ol>\n<h3 id=\"nav_point_91\">4.3.4　DTP 模型</h3>\n<p>聊完了分布式事务的理论以后，需要有对应的方式实现这些理论。The Open Group 组织由 X 公司和 Open 公司合并而成，该组织制定了一个叫作 X/Open Distributed Transaction Processing（以下简称 DTP）的模型，也就是分布式事务处理模型。DTP 模型用来定义分布式事务的处理方式，也可以将其称为一种基础软件架构，该架构提供了应用程序处理多个临界资源的标准，定义了多个应用程序协同工作于全局事务的模式。DTP 模型包括下面四部分内容。</p>\n<ul>\n<li><strong>应用程序（AP，Application Program）</strong>。应用程序具备两方面的功能：定义完成整个事务需要的所有操作（一个或者多个操作）、根据这些操作访问对应的资源。</li>\n<li><strong>资源管理器（RM，Resource Manager）</strong>。资源管理器用来管理共享资源，例如数据库实例。它同样具备两方面功能：一方面是提供访问资源所需的接口，并且支持对资源进行事务处理；另一方面是配合事务管理器完成全局事务，当应用程序在一个事务中需要操作多个资源时，资源管理器会在事务管理器注册多个事务，然后事务管理器根据注册的这些事务建立一个全局事务。全局事务中的每个事务都对应资源管理器的一个模型实例，通过资源管理器对资源进行操作来实现事务管理器中定义的全局事务信息。</li>\n<li><strong>事务管理器（TM，Transaction Manager）</strong>。事务管理器为应用程序提供注册事务的接口，针对应用程序的操作注册全局事务，然后在这个全局事务中定义原子事务（原子事务可以理解为应用程序事务中不能再拆分的操作）。同时通过 XA 协议与资源管理器直接通信，根据原子事务告诉资源管理器要操作哪些资源。通过存储、管理全局事务的内容，指挥资源管理器完成提交和回滚操作（commit 和 rollback）。</li>\n<li><strong>XA（Extended Architecture）</strong>。资源管理器和事务管理器之间通信遵循的协议规范，事务管理器通过它来管理和控制资源管理器上的原子事务访问资源。</li>\n</ul>\n<p>DTP 模型的事务处理流程如图 4-21 所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00403.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-21　DTP 模型的事务处理流程</strong></p>\n<p>其中每个步骤的操作分别如下。</p>\n<p>(1) 应用程序针对自身的业务要求在事务管理器中注册对应的操作，图 4-21 中假设应用程序的事务包含两个操作，这两个操作在事务管理器中就显示为两个原子事务，分别是事务 1 和事务 2，这两个原子事务包含在全局事务中被统一管理。</p>\n<p>(2) 事务管理器通过 XA 协议将原子事务与资源管理器上的原子事务对应起来。资源管理器有多个，每个资源管理器分别控制 1 个原子事务访问相应的资源。</p>\n<p>(3) 完成上面两步操作后，应用程序就可以根据自身业务直接访问资源管理器中的资源了，同时事务管理器中定义的全局事务会判断其原子事务的完成情况，以此决定是提交事务还是回滚事务。</p>\n<h3 id=\"nav_point_92\">4.3.5　分布式事务 2PC 解决方案</h3>\n<p>可以看出 DTP 定义了分布式事务的处理模型，可以针对这个模型提出分布式事务的解决方案，2PC 就是其中一种。2PC 的全称为两阶段提交（Two Phase Commitment Protocol），是 DTP 模型的最佳实践，解决了在分布式服务或数据库场景下，同一事务对多个节点进行操作的数据一致性问题。2PC 在一定程度上遵守 ACID 理论的刚性事务的要求，保证了强一致性。2PC 中有两个概念，一个是事务协调者，对应 DTP 模型中的事务管理器，用来协调事务，所有事务什么时候准备好、什么时候可以提交都由它来协调和管理；另一个是事务参与者，对应 DTP 模型中的资源管理器，主要负责处理具体事务、管理需要处理的资源。例如事务参与者可以处理订票业务，扣款业务。</p>\n<p>2PC 最佳实践分为两个阶段。</p>\n<p>第一阶段（准备阶段）：如图 4-22 所示，事务协调者（事务管理器）给每个事务参与者（资源管理器）发送准备（prepare）消息，目的是询问大家是不是都准备好了，马上就要执行事务了。事务参与者会根据自身业务和资源情况进行检查，然后给出反馈。检查过程根据业务内容的不同而不同，例如订票业务需要检查是否有剩余票、扣款业务需要检查余额是否足够。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00404.jpeg\" alt=\"\" width=\"75%\" style=\"width: 75%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-22　2PC 第一阶段</strong></p>\n<p>只有检查通过，才能返回就绪（ready）信息。否则，事务将终止，并且等待下次询问。由于检查过程需要完成一些操作，因此需要写 redo 日志和 undo 日志，以便事务失败重试，或者失败回滚时使用。</p>\n<p>第二阶段（提交阶段）：如图 4-23 所示，如果事务协调者接收到事务参与者检查失败或者超时的消息，会给其发送回滚（rollback）消息，否则发送提交（commit）消息。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00405.jpeg\" alt=\"\" width=\"75%\" style=\"width: 75%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-23　2PC 第二阶段</strong></p>\n<p>针对以上两种情况，处理过程分别如下。</p>\n<p>情况 1：只要有一个事务参与者反馈未就绪（no ready），事务协调者就会回滚事务。</p>\n<p>a) 事务协调者向所有事务参与者发出回滚请求。</p>\n<p>b) 事务参与者使用第一阶段中 undo 日志里的信息执行回滚操作，并且释放整个事务期间占用的资源。</p>\n<p>c) 各事务参与者向事务协调者反馈应答（ack）消息，表示完成操作。</p>\n<p>d) 事务协调者接收到所有事务参与者反馈的应答消息，即完成了事务回滚。</p>\n<p>情况 2：当所有事务参与者均反馈就绪（ready）消息时，事务协调者会提交（commit）事务。</p>\n<p>a) 事务协调者向所有事务参与者发出正式提交事务的请求。</p>\n<p>b) 事务参与者执行提交（commit）操作，并释放整个事务期间占用的资源。</p>\n<p>c) 各事务参与者向事务协调者反馈应答（ack）消息，表示完成操作。</p>\n<p>d) 事务协调者接收到所有事务参与者反馈的应答（ack）消息，即完成了事务提交。</p>\n<h3 id=\"nav_point_93\">4.3.6　分布式事务 TCC 解决方案</h3>\n<p>随着大流量、高并发业务场景的出现，对系统可用性的要求变得越来越高，这时 CAP 理论和 BASE 理论逐渐进入人们的视野，柔性事务成为分布式事务的主要实现方式，TCC 作为补偿事务也位列其中。</p>\n<p>TCC（Try-Confirm-Cancel）的核心思想是对于每个资源的原子操作，应用程序都需要注册一个与此操作对应的确认操作和补偿（撤销）操作。其中确认操作负责在原子操作执行成功时进行事务提交，补偿操作负责在原子操作执行失败时对事务进行回滚。TCC 分为三个阶段。</p>\n<ul>\n<li><strong>Try 阶段</strong>：负责对业务系统以及要操作的对象进行检测和资源预留。</li>\n<li><strong>Confirm 阶段</strong>：负责对业务系统做确认提交。如果 Try 阶段执行成功，表明针对资源的操作已经准备就绪，此时执行 Confirm 便会提交对资源的操作。也就是说当资源准备好时，只用提交该操作执行就好了。</li>\n<li><strong>Cancel 阶段</strong>：负责在业务执行错误，需要回滚时执行业务取消操作，此时就需要释放 Try 阶段预留的资源了。换句话说，是在资源操作执行失败的情况下，根据之前预留的资源情况进行回滚。</li>\n</ul>\n<p>接下来通过一个例子帮助大家理解。</p>\n<p>如图 4-24 所示，假设有一个转账服务，需要把 A 银行中 A 账户的 100 元分别转到 B 银行的 B 账户和 C 银行的 C 账户，这三个银行的转账服务各不相同，因此这次转账服务就形成了一次分布式事务。我们来看看如何用 TCC 的方式完成这个服务。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00406.jpeg\" alt=\"\" width=\"90%\" style=\"width: 90%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-24　转账服务</strong></p>\n<p>首先是 Try 阶段，主要检测资源是否可用，例如检测账户余额是否足够，缓存、数据库、队列是否可用等。这个阶段并不执行具体的转账逻辑。如图 4-25 所示，从 A 账户转出钱之前要检查该账户的总金额是否大于 100，并且记录总金额和转出金额。对于 B 账户和 C 账户，需要知道账户原有的总金额和转入金额，从而可以计算转帐后的总金额。这里的交易数据库除了设有总金额字段，还要有转出金额或者转入金额字段，供 Cancel 阶段回滚时使用。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00407.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-25　Try 阶段</strong></p>\n<p>如果 Try 阶段执行成功，就进入 Confirm 阶段，执行具体的转账逻辑。如图 4-26 所示，从 A 账户转出 100 元成功，剩余金额为 220－100＝120，把这个剩余金额写入总金额中保存，并且把交易状态设置为转账成功。B 账户和 C 账户分别设置总金额为 50＋30＝80 和 60＋70＝130，也把交易状态设置为转账成功。现在，整个事务完成。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00408.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-26　Confirm 阶段</strong></p>\n<p>如果 Try 阶段没有执行成功，那么 A、B、C 银行的转账服务都要做回滚的操作。如图 4-27 所示，A 账户需要把扣除的 100 元加回，所以总金额为 120＋100＝220。B 账户和 C 账户需要把从总金额中减去入账金额，分别是 80－30＝50 和 130－70＝60。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00409.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-27　Cancel 阶段</strong></p>\n<ul>\n<li><p><strong>TCC 接口实现</strong></p>\n<p>这里应注意，需要针对每个服务分别实现 Try、Confirm、Cancel 三个阶段的代码，例如上面转账服务涉及的检查资源、执行业务、回滚业务操作。如图 4-28 所示，在 A 银行转账服务中需要分别实现 Try、Confirm、Cancel 三种接口，来对应三种不同的阶段。目前有很多开源的架构可以借鉴，例如 ByteTCC、TCC-Transaction。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00410.jpeg\" alt=\"\" width=\"80%\" style=\"width: 80%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-28　TCC 接口实现</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>TCC 可靠性</strong></p>\n<p>TCC 通过记录事务处理日志来保证可靠性。一旦执行 Try、Confirm、Cancel 操作时服务挂掉或者出现异常，TCC 便会提供重试机制。如图 4-29 所示，A、B、C 银行转账服务在执行 Try、Confirm、Cancel 操作时都会记录日志，当服务或者服务器挂掉进行重启恢复时，就可以从日志中读取对应的操作阶段，对事务操作进行重试。另外如果服务存在异步情况，则可以采用消息队列的方式保持通信事务的一致。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00411.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-29　TCC 可靠性</strong></p>\n</li>\n</ul>\n","comments":[]}