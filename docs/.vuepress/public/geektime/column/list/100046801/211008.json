{"id":211008,"title":"08 | 一个几乎每个系统必踩的坑儿：访问数据库超时","content":"<p>你好，我是李玥。</p><p>每一个创业公司，它的系统随着公司的发展一起成长的过程中，都难免会发生一些故障或者是事故，严重的会影响业务。搞技术的同学管这个叫：坑儿，分析解决问题的过程，称为：填坑儿。而访问数据库超时这个坑儿，是我见过的被踩的次数最多的一个坑儿，并且这个坑儿还在被不停地踩来踩去。</p><p>今天这节课，我和你分享一个典型的数据库超时案例。我也希望你通过和我一起分析这个案例，一是，吸取其中的经验教训，日后不要再踩类似的坑儿；二是，如果遇到类似的问题，你能掌握分析方法，快速地解决问题。最重要的是，学习存储系统架构设计思想，在架构层面限制故障对系统的破坏程度。</p><h2>事故排查过程</h2><p>我们一起来看一下这个案例。</p><p>每一个做电商的公司都梦想着做社交引流，每一个做社交的公司都梦想着做电商将流量变现。我的一个朋友他们公司做社交电商，当年很有前途的一个创业方向，当时也是有很多创业公司在做。</p><p>有一天他找到我，让我帮他分析一下他们系统的问题。这个系统从圣诞节那天晚上开始，每天晚上固定十点多到十一点多这个时段，大概瘫痪一个小时左右的时间，过了这个时段系统自动就恢复了。系统瘫痪时的现象就是，网页和App都打不开，请求超时。</p><p>这个系统的架构是一个非常典型的小型创业公司的微服务架构。系统的架构如下图：</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/b7/18/b7edc46baa597b4bd6a25ee5c744b318.png?wh=802*641\" alt=\"\"></p><p>整个系统托管在公有云上，Nginx作为前置网关承接前端所有请求，后端按照业务，划分了若干个微服务分别部署。数据保存在MySQL中，部分数据用Memcached做了前置缓存。数据并没有按照微服务最佳实践的要求，做严格的划分和隔离，而是为了方便，存放在了一起。</p><p>这样的存储设计，对于一个业务变化极快的创业公司来说，是合理的。因为它的每个微服务，随时都在随着业务改变，如果做了严格的数据隔离，反而不利于应对需求变化。</p><p>听了我朋友对问题的描述，我的第一反应是，每天晚上十点到十一点这个时段，是绝大多数内容类App的访问量高峰，因为这个时候大家都躺在床上玩儿手机。初步判断，这个故障是和访问量有关系的，看下面这个系统每天的访问量的图，可以印证这个判断。</p><p><img src=\"https://static001.geekbang.org/resource/image/b6/f7/b6bd0e5d44075011680003338ff4bef7.png?wh=813*293\" alt=\"\"></p><p><strong>基于这个判断，排查问题的重点应该放在那些服务于用户访问的功能上。</strong>比如说，首页、商品列表页、内容推荐这些功能。</p><p>在访问量峰值的时候，请求全部超时，随着访问量减少，<strong>系统能自动恢复，基本可以排除后台服务被大量请求打死的可能性</strong>，因为如果进程被打死了，一般是不会自动恢复的。排查问题的重点应该放在MySQL上。观察下面这个MySQL的CPU利用率图，发现问题：</p><p><img src=\"https://static001.geekbang.org/resource/image/c7/d7/c73f64774a451cc6ce74d6b99535f0d7.png?wh=747*459\" alt=\"\"></p><p>从监控图上可以看出来，故障时段MySQL的CPU利用率一直是100%。这种情况下，MySQL基本上处于一个不可用的状态，执行所有的SQL都会超时。</p><p>MySQL这种CPU利用率高的现象，绝大多数情况都是由慢SQL导致的，所以我们优先排查慢SQL。MySQL和各大云厂商提供的RDS都能提供慢SQL日志，分析慢SQL日志，是查找类似问题原因最有效的方法。</p><p>一般来说，慢SQL的日志中，会有这样一些信息：SQL、执行次数、执行时长。通过分析慢SQL找问题，并没有什么标准的方法，主要还是依靠经验。</p><p>首先，你需要知道的一点是，当数据库非常忙的时候，它执行任何一个SQL都很慢。所以，并不是说，慢SQL日志中记录的这些慢SQL都是有问题的SQL。大部分情况下，导致问题的SQL只是其中的一条或者几条。不能简单地依据执行次数和执行时长进行判断，但是，单次执行时间特别长的SQL，仍然是应该重点排查的对象。</p><p>通过分析这个系统的慢SQL日志，首先找到了一个特别慢的SQL。</p><p>这个SQL支撑的功能是一个红人排行榜，这个排行榜列出粉丝数最多的TOP10红人。</p><pre><code>select fo.FollowId as vid, count(fo.id) as vcounts\nfrom follow fo, user_info ui\nwhere fo.userid = ui.userid\nand fo.CreateTime between\nstr_to_date(?, '%Y-%m-%d %H:%i:%s')\nand str_to_date(?, '%Y-%m-%d %H:%i:%s')\nand fo.IsDel = 0\nand ui.UserState = 0\ngroup by vid\norder by vcounts desc\nlimit 0,10\n</code></pre><p><strong>这种排行榜的查询，一定要做缓存</strong>。在这个案例中，排行榜是新上线的功能，可能忘记做缓存了，通过增加缓存可以有效地解决问题。</p><p>给排行榜增加了缓存后，新版本立即上线。本以为问题就此解决了，结果当天晚上，系统仍然是一样的现象，晚高峰各种请求超时，页面打不开。</p><p>再次分析慢SQL日志，排行榜的慢SQL不见了，说明缓存生效了。日志中的其他慢SQL，查询次数和查询时长分布的都很均匀，也没有看出明显写的有问题的SQL。</p><p>回过头来再看MySQL CPU利用率这个图。</p><p><img src=\"https://static001.geekbang.org/resource/image/c3/1e/c330355300eca211e5b1fad50709e91e.png?wh=786*446\" alt=\"\"></p><p>把这个图放大后，发现一些规律：</p><ol>\n<li>CPU利用率，以20分钟为周期，非常规律的波动；</li>\n<li>总体的趋势与访问量正相关。</li>\n</ol><p>那我们是不是可以猜测一下，对MySQL的CPU利用率的“贡献”来自两部分：红线以下的部分，是正常处理日常访问请求的部分，它和访问量是正相关的。红线以上的部分，来自某一个以20分钟为周期的定时任务，和访问量关系不大。</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/2f/2ebd674e2f5ef41065ca8eb3589eb62f.png?wh=764*417\" alt=\"\"></p><p>排查整个系统，没有发现有以20分钟为周期的定时任务，继续扩大排查范围，排查周期小于20分钟的定时任务，最终定位了问题。</p><p>App的首页聚合了非常多的内容，像精选商品、标题图、排行榜、编辑推荐等等。这些内容包含了很多的数据库查询。当初设计的时候，给首页做了一个整体的缓存，缓存的过期时间是10分钟。但是需求不断变化，首页需要查询的内容越来越多，导致查询首页的全部内容越来越慢。</p><p>通过检查日志发现，刷新一次缓存的时间竟然要15分钟。缓存是每隔10分钟整点刷一次，因为10分钟内刷不完，所以下次刷新就推迟到了20分钟之后，这就导致了上面这个图中，红线以上每20分钟的规律波形。</p><p>由于缓存刷新慢，也会很多请求无法命中缓存，请求直接穿透缓存打到了数据库上面，这部分请求给上图红线以下的部分，做了很多“贡献”。</p><p>找到了问题原因，做针对性的优化，问题很快就解决了。新版本上线之后，再没有出现过“午夜宕机”。</p><p><img src=\"https://static001.geekbang.org/resource/image/68/7f/6886630263c150d8af3b5a2ff97eb67f.png?wh=805*406\" alt=\"\"></p><p>对比优化前后MySQL的CPU利用率，可以明显地看出优化效果。</p><h2>如何避免悲剧重演</h2><p>到这里问题的原因找到了，问题也圆满解决了。单从这个案例来看，问题的原因在于，开发人员犯了错误，编写的SQL没有考虑数据量和执行时间，缓存的使用也不合理。最终导致在忙时，大量的查询打到MySQL上，MySQL繁忙无法提供服务。</p><p>作为系统的开发人员，对于这次事故，我们可以总结两点经验：</p><p>第一，在编写SQL的时候，一定要小心谨慎地仔细评估。先问自己几个问题：</p><ul>\n<li>你的SQL涉及到的表，它的数据规模是多少？</li>\n<li>你的SQL可能会遍历的数据量是多少？</li>\n<li>尽量地避免写出慢SQL。</li>\n</ul><p>第二，能不能利用缓存减少数据库查询次数？在使用缓存的时候，还需要特别注意的就是缓存命中率，要尽量避免请求命中不了缓存，穿透到数据库上。</p><p>以上两点，是开发人员需要总结的问题。不过你想没想过，谁能保证，整个团队的所有开发人员以后不再犯错误？保证不了吧？那是不是这种的悲剧就无法避免了呢？</p><p>其实，还是有办法的。不然，那些大厂，几万开发人员，每天会上线无数的Bug，系统还不得天天宕机？而实际情况是，大厂的系统都是比较稳定的，基本上不会出现全站无法访问这种情况。</p><p>靠的是什么？靠的是架构。</p><p>优秀的系统架构，可以在一定程度上，减轻故障对系统的影响。针对这次事故，我给这个系统在架构层面，提了两个改进的建议。</p><p>第一个建议是，上线一个定时监控和杀掉慢SQL的脚本。这个脚本每分钟执行一次，检测上一分钟内，有没有执行时间超过一分钟（这个阈值可以根据实际情况调整）的慢SQL，如果发现，直接杀掉这个会话。</p><p>这样可以有效地避免一个慢SQL拖垮整个数据库的悲剧。即使出现慢SQL，数据库也可以在至多1分钟内自动恢复，避免数据库长时间不可用。代价是，可能会有些功能，之前运行是正常的，这个脚本上线后，就会出现问题。但是，这个代价还是值得付出的，并且，可以反过来督促开发人员更加小心，避免写出慢SQL。</p><p>第二个建议是，做一个简单的静态页面的首页作为降级方案，只要包含商品搜索栏、大的品类和其他顶级功能模块入口的链接就可以了。在Nginx上做一个策略，如果请求首页数据超时的时候，直接返回这个静态的首页作为替代。这样后续即使首页再出现任何的故障，也可以暂时降级，用静态首页替代。至少不会影响到用户使用其他功能。</p><p>这两个改进建议都是非常容易实施的，不需要对系统做很大的改造，并且效果也立竿见影。</p><p>当然，这个系统的存储架构还有很多可以改进的地方，比如说对数据做适当的隔离，改进缓存置换策略，做数据库主从分离，把非业务请求的数据库查询迁移到单独的从库上等等，只是这些改进都需要对系统做比较大的改动升级，需要从长计议，在系统后续的迭代过程中逐步地去实施。</p><h2>小结</h2><p>这节课，我和你一起分析了一个由于慢SQL导致的全站故障的案例。在“破案”的过程中，有一些很有用的经验，这些经验对于后续你自己“破案”时会非常有用。比如说：</p><ol>\n<li>根据故障时段在系统忙时，推断出故障是跟支持用户访问的功能有关。</li>\n<li>根据系统能在流量峰值过后自动恢复这一现象，排除后台服务被大量请求打死的可能性。</li>\n<li>根据CPU利用率曲线的规律变化，推断出可能和定时任务有关。</li>\n</ol><p>在故障复盘阶段，除了对故障问题本身做有针对性的预防和改进以外，更重要的是，在系统架构层面进行改进，让整个系统更加健壮，不至于因为某一个小的失误，就导致全站无法访问。</p><p>我给系统提出的第一个自动杀慢SQL的建议，它的思想是：系统的关键部分要有自我保护机制，避免外部的错误影响到系统的关键部分。第二个首页降级的建议，它的思想是：当关键系统出现故障的时候，要有临时的降级方案，尽量减少故障带来的影响。</p><p>这些架构上的改进，虽然并不能避免故障，但是可以很大程度上减小故障的影响范围，减轻故障带来的损失，希望你能仔细体会，活学活用。</p><h2>思考题</h2><p>课后请你想一下，以你个人的标准，什么样的SQL算是慢SQL？如何才能避免写出慢SQL？欢迎你在留言区与我交流互动。</p><p>感谢你的阅读，如果你觉得今天的内容对工作有所帮助，也欢迎把它分享给你的朋友。</p>","comments":[{"had_liked":false,"id":187475,"user_name":"冯玉鹏","can_delete":false,"product_type":"c1","uid":1442088,"ip_address":"","ucode":"BA4B53F072B82F","user_header":"https://static001.geekbang.org/account/avatar/00/16/01/28/134da154.jpg","comment_is_top":false,"comment_ctime":1584124664,"is_pvip":false,"replies":[{"id":"72776","content":"👍👍👍👍👍","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1584409215,"ip_address":"","comment_id":187475,"utype":1}],"discussion_count":4,"race_medal":0,"score":"452555690744","product_id":100046801,"comment_content":"老师，慢SQL 我感觉也没有个人标准，个人的标准也要分场景，业务复杂度等；如果作为常规的用户业务系统，超过1秒就是慢SQL；但是如果是类似生成报表的服务，选择在业务低峰期，从库执行等策略，时间长点也不是不能接受。<br>避免慢SQL：第一点肯定想到的是合适的索引，毕竟SQL执行速度的快慢关键还是语句需要扫描数据的行数，如尽量不要使用 对where 条件列进行计算的做法让MySQL查询优化器不知道怎么选择索引，特定业务 可以设置联合索引让需要查询返回的列都在索引中避免回表操作。<br>第二：排序也是可能完成慢SQL的因素，尤其是数据量大，需要使用外部排序的时候又可以与磁盘IO性能扯上关系等，常见的问题还有limit  m,n  m很大又无法使用索引的时候<br>第三：多表联合查询的时候，尽量使用小表驱动大表。<br>第四：避免大事务，这也是发生死锁常见的雷区，尽量减小事务粒度，尽量注意不同事务对表操作的顺序一致，大事务其实也包含着批量操作的隐式事务，如一个update 影响100万行数据。<br><br>第五：见过的关于架构方面的慢SQL问题  1～数据量到达一定规模后，单机性能容易受限导致数据库响应慢；2～读写分离，从库提供读服务，错误的认为从库只需要提供查询服务采用了达不到性能指标的机器，其实是主库承受的数据更新压力，从库一个不落的都要承受，还要更多的提供查询服务<br>","like_count":105,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":487114,"discussion_content":"👍👍👍👍👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584409215,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2622081,"avatar":"https://static001.geekbang.org/account/avatar/00/28/02/81/c85aec72.jpg","nickname":"S","note":"","ucode":"32421518824037","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":573170,"discussion_content":"美团统一 100ms 为慢 sql 的阈值，搞笑","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653231232,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1282639,"avatar":"https://static001.geekbang.org/account/avatar/00/13/92/4f/ff04156a.jpg","nickname":"天天向上","note":"","ucode":"D0914D4FD82272","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546592,"discussion_content":"谢谢分享 准备借鉴下作为团队数据库SQL编程指引。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642344952,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1187478,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1e/96/c735ad6b.jpg","nickname":"滩涂曳尾","note":"","ucode":"40F650F2A419D4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":285805,"discussion_content":"可以的哥","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592958660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":188788,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1584415844,"is_pvip":true,"replies":[{"id":"72994","content":"我们例子里面当时它采用的就是人肉冷备，主节点出问题的时候人肉切换到备用节点上。<br><br>其实更合理的做法是做通过负载均衡器或者域名把流量均匀的打到多个NGINX节点上，配合探活机制，当某个节点有问题的时候，自动摘掉这个节点。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1584496059,"ip_address":"","comment_id":188788,"utype":1}],"discussion_count":2,"race_medal":0,"score":"74598859876","product_id":100046801,"comment_content":"上面那个小型创业公司的微服务架构，想知道有关 Nginx 的主备是怎么实现的？","like_count":17,"discussions":[{"author":{"id":1033240,"avatar":"https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJkwbyTYtSCx6Qc7cQPnnRWv38Jybh3etziaPmuP8gHcgS6FMxcdftrKgWiamH6fc2iciaicDKDVEwcEibQ/132","nickname":"sami","note":"","ucode":"9A66FCA00D8A37","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":207258,"discussion_content":"阿里的slb可以做到吧\n自己实现的话vip+keep alive","likes_number":6,"is_delete":false,"is_hidden":false,"ctime":1584483593,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":487526,"discussion_content":"我们例子里面当时它采用的就是人肉冷备，主节点出问题的时候人肉切换到备用节点上。\n\n其实更合理的做法是做通过负载均衡器或者域名把流量均匀的打到多个NGINX节点上，配合探活机制，当某个节点有问题的时候，自动摘掉这个节点。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1584496059,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":225768,"user_name":"小唐","can_delete":false,"product_type":"c1","uid":1018986,"ip_address":"","ucode":"B2A471AAF109E6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/6a/ec181c50.jpg","comment_is_top":false,"comment_ctime":1591850497,"is_pvip":false,"replies":[{"id":"84544","content":"1. 一般“服务被打死”，比较常见的情况是内存溢出、栈溢出或者进程直接挂掉，这些情况都是不能自动恢复的。<br><br>2. 案例中用的是Memcached，刷新的策略也是根据不同业务有不同的策略。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1592919175,"ip_address":"","comment_id":225768,"utype":1}],"discussion_count":2,"race_medal":0,"score":"57426425345","product_id":100046801,"comment_content":"请问老师，我有两个问题请教。1. 为什么后台服务被大量请求打死的话无法自动恢复呢？<br>2. 案例中用的什么cache，怎么refresh的？","like_count":13,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":497984,"discussion_content":"1. 一般“服务被打死”，比较常见的情况是内存溢出、栈溢出或者进程直接挂掉，这些情况都是不能自动恢复的。\n\n2. 案例中用的是Memcached，刷新的策略也是根据不同业务有不同的策略。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1592919175,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":3206657,"avatar":"https://static001.geekbang.org/account/avatar/00/30/ee/01/2ecf51b0.jpg","nickname":"Catcher","note":"","ucode":"5BEFBEE3D367A5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":591334,"discussion_content":"服务没有自动重启机制吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1666515624,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":497984,"ip_address":"北京"},"score":591334,"extra":""}]}]},{"had_liked":false,"id":202033,"user_name":"Halo","can_delete":false,"product_type":"c1","uid":1158603,"ip_address":"","ucode":"12CFC7DFF250AF","user_header":"https://static001.geekbang.org/account/avatar/00/11/ad/cb/3391d24c.jpg","comment_is_top":false,"comment_ctime":1585890520,"is_pvip":true,"replies":[{"id":"75536","content":"必须可行啊。但要注意一下配置中心的高可用。别出现因为配置中心宕机，导致不能熔断了。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1585907846,"ip_address":"","comment_id":202033,"utype":1}],"discussion_count":2,"race_medal":0,"score":"48830530776","product_id":100046801,"comment_content":"老师，我现在想做一个Mysql的本地熔断方案。就是监控对每一个表的操作语句，通过机器数量在配置中心配置每个服务的访问频次、访问时间等。比如Mysql的TPS是4000，我们有10台机器，平均下来每个服务的上限为400&#47;s。碰到超限、或者超慢的情况就熔断、告警。可以整体监控，也可以对热点表进行监控，这种方案是否可行？","like_count":11,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490488,"discussion_content":"必须可行啊。但要注意一下配置中心的高可用。别出现因为配置中心宕机，导致不能熔断了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585907846,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366126,"discussion_content":"这个描述是不是有点问题，没说明服务的种类个数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617964120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":193723,"user_name":"leslie","can_delete":false,"product_type":"c1","uid":1324255,"ip_address":"","ucode":"798E7C1CC98CC2","user_header":"https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg","comment_is_top":false,"comment_ctime":1584954902,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"40239660566","product_id":100046801,"comment_content":"合理使用日志系统、通过合理监控获取必要时信息和做报警提醒，每天定时排查日志中记录的问题；例：cpu使用率、IO使用率等。<br>指定一套完善的开发规范：严禁开发去写，上线之前做code review;规范制度+code review+架构审核，基本能避免大多数问题的发生。","like_count":9},{"had_liked":false,"id":187885,"user_name":"美美","can_delete":false,"product_type":"c1","uid":1148422,"ip_address":"","ucode":"44CC95C45AF345","user_header":"https://static001.geekbang.org/account/avatar/00/11/86/06/72b01bb7.jpg","comment_is_top":false,"comment_ctime":1584265426,"is_pvip":false,"replies":[{"id":"72785","content":"20%左右那个是闲时的图，忙时依然是100%....","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1584410516,"ip_address":"","comment_id":187885,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27354069202","product_id":100046801,"comment_content":"个人感觉，算不算慢SQL首先取决于这条SQL有没有正确的命中索引。如果可以正确的命中索引，那么从业务上是否正确。如果业务匹配，且正常命中索引，那应该不算是慢查询。<br>看完本章还有一点疑惑，就是当第一个慢查询SQL处理完成后，MySQL的CPU使用率已经降到了20%以下。那么即便会有周期性的SQL执行，但是以这个利用率不足以整体导致服务不可用吧。","like_count":6,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":487281,"discussion_content":"20%左右那个是闲时的图，忙时依然是100%....","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584410516,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":188237,"user_name":"Regis","can_delete":false,"product_type":"c1","uid":1435632,"ip_address":"","ucode":"3911E4EDE27F4E","user_header":"https://static001.geekbang.org/account/avatar/00/15/e7/f0/d0bf3a5f.jpg","comment_is_top":false,"comment_ctime":1584330073,"is_pvip":false,"replies":[{"id":"72791","content":"后面有一节课是专门讲对象存储的，敬请期待。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1584410760,"ip_address":"","comment_id":188237,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23059166553","product_id":100046801,"comment_content":"老师讲的非常好，给出了查找问题的思路和解决问题思路，对于项目经验少的很有用。后续课程里面有没有涉及对于视频类的大文件存储方式和使用方式的课程？这部分数据知道使用对象存储进行存储比较好，但是对于有这种大文件的存储的系统架构方面还是不清晰，老师要是了解给指导一下可以吗","like_count":5,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":487359,"discussion_content":"后面有一节课是专门讲对象存储的，敬请期待。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584410760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":187479,"user_name":"myrfy","can_delete":false,"product_type":"c1","uid":1169401,"ip_address":"","ucode":"2814BAE5D70098","user_header":"","comment_is_top":false,"comment_ctime":1584140074,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23058976554","product_id":100046801,"comment_content":"慢SQL要以业务场景来区分。例如做即时通讯或者消息类等有实时性要求的，可能2秒就算慢查询了，但是读从库做大数据分析的场景，可能跑一个小时也不算慢。另外，对于请数量大的时候，如果存在多个请求会加锁，即使一个查询是毫秒级别的，上百个查询访问一个热数据加锁也会有很大的问题，所以，没有慢查询的具体标准，影响到业务，拖慢了服务的，就算慢查询。","like_count":5},{"had_liked":false,"id":187568,"user_name":"观弈道人","can_delete":false,"product_type":"c1","uid":1016905,"ip_address":"","ucode":"F3BB619A33C605","user_header":"https://static001.geekbang.org/account/avatar/00/0f/84/49/47d48fd0.jpg","comment_is_top":false,"comment_ctime":1584159239,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14469061127","product_id":100046801,"comment_content":"作者文中描述的问题可以理解成就是缓存更新慢，导致的缓存穿透","like_count":3,"discussions":[{"author":{"id":1386818,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/42/43d4b1a8.jpg","nickname":"烫烫烫","note":"","ucode":"C06018670DE76A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":330530,"discussion_content":"是缓存击穿","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1606641692,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207521,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1587099763,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10177034355","product_id":100046801,"comment_content":"请问老师，杀掉慢 SQL 的脚本应该怎样写呢？","like_count":2},{"had_liked":false,"id":204357,"user_name":"Yezhiwei","can_delete":false,"product_type":"c1","uid":1005157,"ip_address":"","ucode":"31E8E33688CBEC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/56/65/22a37a8e.jpg","comment_is_top":false,"comment_ctime":1586391256,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10176325848","product_id":100046801,"comment_content":"第一个建议杀掉慢SQL主要还是对慢查询的SQL吧？如果是 update SQL 会带来更多的问题哈","like_count":2,"discussions":[{"author":{"id":1945605,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/b0/05/c9da834e.jpg","nickname":"小人物大希望","note":"","ucode":"8EF313AA26D4B8","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":231461,"discussion_content":"update实际上也是个事务","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586810457,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":187756,"user_name":"发条橙子 。","can_delete":false,"product_type":"c1","uid":1259218,"ip_address":"","ucode":"ED076F4534FFED","user_header":"https://static001.geekbang.org/account/avatar/00/13/36/d2/c7357723.jpg","comment_is_top":false,"comment_ctime":1584228525,"is_pvip":false,"replies":[{"id":"72799","content":"这两个问题我在后续的课程中都会讲到一些解决的经验和方法。<br><br>这种大查询，首先肯定是要用缓存，但要根据实际情况选择合适的缓存更新策略。<br><br>数据量特别大的统计分析，一般选择放到其它分析型数据库或者数据仓库中去执行，或者使用流计算来解决。<br>","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1584411506,"ip_address":"","comment_id":187756,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10174163117","product_id":100046801,"comment_content":"老师 针对两个问题排查后处理的方法能不能给个思路 <br><br>1. 缓存热点数据 ： 因为使用连表查询等复杂语句在数据量大的时候会产生慢差 。是否该考虑修改查询语句或者上搜索（es &#47; 阿里open search ) 然后再加一道缓存 缓存的读写策略采用旁路策略。<br><br>2. 像这种定时任务应该大部分公司都会有很多，一般都是放到凌晨来执行 ，经常会有人问当数据量大的时候 这种定时任务是否可行。 所以像数据量非常大（京东这种级别数据） 定时任务扫表是否还可行 有没有其他的解决思路<br><br>希望老师给些思路 谢谢🙏","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":487230,"discussion_content":"这两个问题我在后续的课程中都会讲到一些解决的经验和方法。\n\n这种大查询，首先肯定是要用缓存，但要根据实际情况选择合适的缓存更新策略。\n\n数据量特别大的统计分析，一般选择放到其它分析型数据库或者数据仓库中去执行，或者使用流计算来解决。\n","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1584411506,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1158873,"avatar":"https://static001.geekbang.org/account/avatar/00/11/ae/d9/7a732188.jpg","nickname":"ROCKETsFORWARD","note":"","ucode":"D171E33E546FD2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":205013,"discussion_content":"比较消耗系统资源的定时器任务应该另外部署到一台服务上吧，不然影响主业务。数据量很大的话应该都是考虑用Hadoop来做MapReduce了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584248393,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1259218,"avatar":"https://static001.geekbang.org/account/avatar/00/13/36/d2/c7357723.jpg","nickname":"发条橙子 。","note":"","ucode":"ED076F4534FFED","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1158873,"avatar":"https://static001.geekbang.org/account/avatar/00/11/ae/d9/7a732188.jpg","nickname":"ROCKETsFORWARD","note":"","ucode":"D171E33E546FD2","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":205714,"discussion_content":"不是影响操作系统的资源 主要是定时任务处理后更新数据都会打到数据库上 数据库资源会是瓶颈。所以单独部署机器用于计算也没能解决根本问题。所以在想大厂是不是有其他思路来处理的","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1584334361,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":205013,"ip_address":""},"score":205714,"extra":""}]}]},{"had_liked":false,"id":187550,"user_name":"webmin","can_delete":false,"product_type":"c1","uid":1047014,"ip_address":"","ucode":"98B0CA882454E8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f9/e6/47742988.jpg","comment_is_top":false,"comment_ctime":1584156712,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"10174091304","product_id":100046801,"comment_content":"1. 以你个人的标准，什么样的 SQL 算是慢 SQL？<br>SQL的快慢是个相对标准，与数据量和设备性能有相关性，需要建立监控机制，了解SQL在正常情况下的执行时间的基线是多少，偏离基线超过阀值时可以认为是慢了。<br>2. 如何才能避免写出慢 SQL？<br>抓大放小，针对查询量大和查询大数据集的SQL<br>2.1 先通过SQL执行计划看看是否使用了与预想一至的索引和SQL的执行路径是否与预想的一至；（需要在生产库上看执行计划，现在的DB大都是用成本法对SQL进行优化，数据量不一样会导致执行路径不同）<br>2.2 利用好测试DB，在无法模拟生产数据量的情况下，也需要按一定比例在测试DB在灌入数据，通过实际执行测量执行时间。","like_count":2},{"had_liked":false,"id":330246,"user_name":"沐","can_delete":false,"product_type":"c1","uid":1326537,"ip_address":"","ucode":"678F6A0C778CDD","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/c9/a1e6a307.jpg","comment_is_top":false,"comment_ctime":1641881267,"is_pvip":false,"discussion_count":0,"race_medal":1,"score":"5936848563","product_id":100046801,"comment_content":"作为系统开发人员，<br>一是在编写SQL的时候，一定要仔细评估<br>1、SQL涉及到的标，数据规模是多少<br>2、SQl会遍历到的数据量<br>3、尽量避免写出慢SQL<br>二是能不能利用缓存减少数据库查询次数，在使用缓存的时候要注意缓存的命中率，避免缓存穿透<br>同时要在架构层面，减轻故障对系统的影响<br>一是上线定时监控和杀掉慢SQL的脚本<br>二是简单的静态首页作为降级方案","like_count":1},{"had_liked":false,"id":250135,"user_name":"而立斋","can_delete":false,"product_type":"c1","uid":1087258,"ip_address":"","ucode":"5FED6E9E148195","user_header":"https://static001.geekbang.org/account/avatar/00/10/97/1a/389eab84.jpg","comment_is_top":false,"comment_ctime":1600947726,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5895915022","product_id":100046801,"comment_content":"重构:用自己的话，重述内容<br><br>对于一次系统高峰时段出现的问题，从排查分析到解决，到复盘总结，过程的一次演练。<br>根据出现的时间段，分析出是用户请求超时导致的结果，进而对系统中的慢sql进行分析，分析出慢sql之后进行修复，从数据库cpu使用率上分析出定时任务的存在，并分析出定时任务的周期，至此问题解决。但在复盘的时候从架构的层次进行了更为本质的分析，并给出数据库慢sql的预处理模式，数据库分离的建议以及页面降级预案。","like_count":1,"discussions":[{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366134,"discussion_content":"系统降级方案\n1. 定时杀慢sql\n2. 简单的静态页面的首页\n\n存储架构\n1. 数据做适当的隔离；\n2. 改进缓存置换策略\n3. 做数据库主从分离，把非业务请求的数据库查询迁移到单独的从库上等","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617968304,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":238860,"user_name":"Monday","can_delete":false,"product_type":"c1","uid":1250907,"ip_address":"","ucode":"77B9BACC783598","user_header":"https://static001.geekbang.org/account/avatar/00/13/16/5b/83a35681.jpg","comment_is_top":false,"comment_ctime":1596345021,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5891312317","product_id":100046801,"comment_content":"总感觉这篇文章在哪里听过读过，折腾了好久，终于找到出处了，卖桃者说 2020-03-20期推荐过这篇好文。哈哈，现在再次读来，备感亲切","like_count":1},{"had_liked":false,"id":189030,"user_name":"小袁","can_delete":false,"product_type":"c1","uid":1811495,"ip_address":"","ucode":"3F5D8721F577D9","user_header":"https://static001.geekbang.org/account/avatar/00/1b/a4/27/15e75982.jpg","comment_is_top":false,"comment_ctime":1584448487,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"5879415783","product_id":100046801,"comment_content":"不仅仅要扫描长时间执行的sql语句，还要扫描长事务。有些python库不开启自动提交，会导致长事务占据表的元数据锁，从而导致更多的问题。","like_count":1,"discussions":[{"author":{"id":2296382,"avatar":"https://static001.geekbang.org/account/avatar/00/23/0a/3e/5b1b1b75.jpg","nickname":"流年","note":"","ucode":"8B579C6E466CB4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":324953,"discussion_content":"元数据锁只在表结构有变动时才会加吧，sql更新加的是record锁","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605194740,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1009518,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/67/6e/f5ee46e8.jpg","nickname":"海滨","note":"","ucode":"F1B94D2DB944DC","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":273567,"discussion_content":"哈哈，同道中人pymysql库就比较坑，默认不开启自动提交","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590472179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":187498,"user_name":"肥low","can_delete":false,"product_type":"c1","uid":1043480,"ip_address":"","ucode":"A158AFAAB8C742","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/18/bf7254d3.jpg","comment_is_top":false,"comment_ctime":1584147693,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5879114989","product_id":100046801,"comment_content":"老师好 你讲的例子好像就发生在昨天，慢SQL的话执行时间超过1秒就算是了，长的主要原因有: 语句复杂，比如各联表，group by我就被搞死过一次; 还有就是没有建立合适的索引，总而言之，如果对数据库如MySQL BTree有较深入的理解的话，肯定不会写出这么慢的SQL来","like_count":1,"discussions":[{"author":{"id":1464525,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLNMoXfvZA04aaY1FAprFtjad1rYvic1wib2XmyjdAuyLg9XHdlrAI9s5Qxu75icH6165P3k5UulQMag/132","nickname":"Steven","note":"","ucode":"FA28281AFC10F3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":214614,"discussion_content":"讲的非常不错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585216309,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":360371,"user_name":"Catcher","can_delete":false,"product_type":"c1","uid":3206657,"ip_address":"北京","ucode":"5BEFBEE3D367A5","user_header":"https://static001.geekbang.org/account/avatar/00/30/ee/01/2ecf51b0.jpg","comment_is_top":false,"comment_ctime":1666515780,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666515780","product_id":100046801,"comment_content":"老师，想问下问什么缓存更新慢的监控指标是这样的？更新缓存15分钟，使用缓存5分钟，低谷期也会是一条直线呀？而不是图中这样，低谷期很短，基本上就是上升回落上升回落","like_count":0},{"had_liked":false,"id":350722,"user_name":"不卷怎么搞钱","can_delete":false,"product_type":"c1","uid":2206043,"ip_address":"","ucode":"F01CA83CF99047","user_header":"https://static001.geekbang.org/account/avatar/00/21/a9/5b/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1657117976,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1657117976","product_id":100046801,"comment_content":"老师好，在业务中想要杀死慢sql是不是mybatis的xml文件设置tiomeout就可以了","like_count":0},{"had_liked":false,"id":340494,"user_name":"Geek_fe19fb","can_delete":false,"product_type":"c1","uid":2941998,"ip_address":"","ucode":"23C751D4696C7D","user_header":"","comment_is_top":false,"comment_ctime":1648864110,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1648864110","product_id":100046801,"comment_content":"我们系统中有大量的导出，打印、下载  数据量大的时候一个SQL动辄就要执行一分钟以上  就很难受","like_count":0,"discussions":[{"author":{"id":1798204,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/70/3c/a002615b.jpg","nickname":"严丹","note":"","ucode":"96FA634D06C3FD","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":580659,"discussion_content":"有什么优化方案吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1658307905,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3020156,"avatar":"https://static001.geekbang.org/account/avatar/00/2e/15/7c/9c51ef90.jpg","nickname":"nicheng","note":"","ucode":"560A87C3E6CAC3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":575717,"discussion_content":"知己啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1655050754,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335192,"user_name":"sotondolphin","can_delete":false,"product_type":"c1","uid":2306394,"ip_address":"","ucode":"136444B47ACDC5","user_header":"","comment_is_top":false,"comment_ctime":1645385365,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1645385365","product_id":100046801,"comment_content":"我们鉴定慢sql 有三个指标：<br>1. 看这个sql扫描的行数是否符合实际预期<br>2. 看这个sql的锁时间是不是很长或者发生了未预料到的锁<br>3. 看这个sql的执行时间是否过长","like_count":0},{"had_liked":false,"id":319115,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1635593030,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1635593030","product_id":100046801,"comment_content":"思考题：以我的理解和实际工作中的经验，我觉得慢SQL有以下一些特征：<br>1. 对大数据量的表进行操作<br>2.2个以上大数据量的表做关联<br>3.输出列过多<br>4.在输出列上过多使用自定义函数<br>5.在where子句中大量使用非索引的字段，或使用一些使索引无效的函数<br>6.语句中有大量的嵌套子查询<br>要避免慢SQL：<br>1.要对SQL做优化分析<br>2.尽可能使用用索引字段<br>3.尽可能对大数据量的表进行关联操作<br>4.采用分批处理的方式来减少每次操作的数据量。","like_count":0},{"had_liked":false,"id":311188,"user_name":"注意力$","can_delete":false,"product_type":"c1","uid":1142316,"ip_address":"","ucode":"7FB3399A1EAB72","user_header":"https://static001.geekbang.org/account/avatar/00/11/6e/2c/e2f3cfc0.jpg","comment_is_top":false,"comment_ctime":1631095422,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1631095422","product_id":100046801,"comment_content":"老师，请问当数据库非常忙的时候，它执行任何一个 SQL 都很慢。 如果这个时候发现CPU和内存都不是瓶颈，这个时候怎么判断数据库很忙？这个怎么解释","like_count":0},{"had_liked":false,"id":260014,"user_name":"Geek_128543","can_delete":false,"product_type":"c1","uid":2291767,"ip_address":"","ucode":"09E3391C057CC1","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLWA1zUQkKWzjnOmeCtZKN1s7X8qrp81ZEyYPsStot84pW0fgE5etMeNmbUdwxTicjmbWOXZxf9EFw/132","comment_is_top":false,"comment_ctime":1604908086,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1604908086","product_id":100046801,"comment_content":"首页缓存过期时间要设置一个随机值，不然会造成缓存雪崩。","like_count":0},{"had_liked":false,"id":245789,"user_name":"FuriousEric","can_delete":false,"product_type":"c1","uid":1138576,"ip_address":"","ucode":"0A66DA938976F7","user_header":"https://static001.geekbang.org/account/avatar/00/11/5f/90/711efc88.jpg","comment_is_top":false,"comment_ctime":1599053554,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1599053554","product_id":100046801,"comment_content":"架构上有自我保护机制这点学习了。这个例子中，我觉得应该打开代码层面的数据库模块的日志开关，例如mybatis有拦截器可以记sql语句和数量，应该能根据sql语句看到异常的sql(首页请求没命中缓存），或者选取2个时间段，一个有问题，一个没问题，把同类sql按总数量大小从大到小用表格比一下，应该也能发现问题。日志是非常重要的一环。","like_count":0},{"had_liked":false,"id":241444,"user_name":"lcf枫","can_delete":false,"product_type":"c1","uid":1144171,"ip_address":"","ucode":"D51E8F68BD41CA","user_header":"https://static001.geekbang.org/account/avatar/00/11/75/6b/fd685164.jpg","comment_is_top":false,"comment_ctime":1597296757,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1597296757","product_id":100046801,"comment_content":"老师，上线一个定时监控和杀掉慢 SQL 的脚本。<br>这个是开启慢查询落DB，然后拉起定时任务去查询DB，发现了，kill 掉相应的session吗？<br>之前有看到mysql 可以设置最长的查询时间，不知道老师指的是哪种方式？","like_count":0},{"had_liked":false,"id":239001,"user_name":"DZZ","can_delete":false,"product_type":"c1","uid":1516167,"ip_address":"","ucode":"C8E4C4B089BCE2","user_header":"https://static001.geekbang.org/account/avatar/00/17/22/87/e7bd2acf.jpg","comment_is_top":false,"comment_ctime":1596413146,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1596413146","product_id":100046801,"comment_content":"老师好 现实中还有些老系统里有很多烂SQL，动辄几个表关联，或者几百万几千万的数据做关联查询，尝试了很多SQL优化的方案都不是很好，但是又无法从架构上拆分这些SQL，这种如何处理呢","like_count":0},{"had_liked":false,"id":237115,"user_name":"runner","can_delete":false,"product_type":"c1","uid":1062591,"ip_address":"","ucode":"26F98B2E82574A","user_header":"https://static001.geekbang.org/account/avatar/00/10/36/bf/95893da5.jpg","comment_is_top":false,"comment_ctime":1595681709,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1595681709","product_id":100046801,"comment_content":"老师，类似的慢sql是否可以通过全链路的压力测试提前暴露，根据压测发现系统瓶颈？","like_count":0},{"had_liked":false,"id":236918,"user_name":"锐锐精灵","can_delete":false,"product_type":"c1","uid":1252535,"ip_address":"","ucode":"22F9EB1EE29B31","user_header":"https://static001.geekbang.org/account/avatar/00/13/1c/b7/e0aa5512.jpg","comment_is_top":false,"comment_ctime":1595584405,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1595584405","product_id":100046801,"comment_content":"慢sql 要运维监控吧","like_count":0},{"had_liked":false,"id":220983,"user_name":"😚 46","can_delete":false,"product_type":"c1","uid":1433535,"ip_address":"","ucode":"EED0EBBBF80A43","user_header":"https://static001.geekbang.org/account/avatar/00/15/df/bf/96b50d1e.jpg","comment_is_top":false,"comment_ctime":1590372016,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1590372016","product_id":100046801,"comment_content":"遇到过一个程序只在工作日跑，周末没有任何请求。<br>数据库默认配置8小时无活动自动销毁，而程序的连接池仍然持有被销毁的连接去请求数据库时报错。更有意思的是，已经配置了获取连接检验有效性，但是仍然出现上述问题😂","like_count":0,"discussions":[{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366136,"discussion_content":"mysql数据库8小时无连接自动关闭\n\n获取连接好像得要有操作","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617968532,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219789,"user_name":"hy","can_delete":false,"product_type":"c1","uid":1236818,"ip_address":"","ucode":"666F4C56399C3B","user_header":"https://static001.geekbang.org/account/avatar/00/12/df/52/92389851.jpg","comment_is_top":false,"comment_ctime":1590084357,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1590084357","product_id":100046801,"comment_content":"那个缓存失效的问题也遇到过，同样时首页，缓存过期时间一致，导致缓存雪崩，后面把首页中各种数据的缓存时间随机。通过埋点数据找到热点数据，热点数据不设过期时间。","like_count":0},{"had_liked":false,"id":204857,"user_name":"闫冬","can_delete":false,"product_type":"c1","uid":1109691,"ip_address":"","ucode":"1725E869D5A3D3","user_header":"https://static001.geekbang.org/account/avatar/00/10/ee/bb/7afd6824.jpg","comment_is_top":false,"comment_ctime":1586483082,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1586483082","product_id":100046801,"comment_content":"确实开发人员在开发的时候要考虑数据量  包括压测 能用缓存还是要用缓存 另外也需要加适当的监控 一个是对慢sql的监控 一个是从服务端做缓存 包括负载均衡等","like_count":0},{"had_liked":false,"id":201218,"user_name":"菠萝吹雪—Code","can_delete":false,"product_type":"c1","uid":1650378,"ip_address":"","ucode":"A5B2FC661EE17D","user_header":"https://static001.geekbang.org/account/avatar/00/19/2e/ca/469f7266.jpg","comment_is_top":false,"comment_ctime":1585734751,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1585734751","product_id":100046801,"comment_content":"这一节真的很棒","like_count":0},{"had_liked":false,"id":193875,"user_name":"R","can_delete":false,"product_type":"c1","uid":1087403,"ip_address":"","ucode":"DD268315F54DB2","user_header":"https://static001.geekbang.org/account/avatar/00/10/97/ab/dc5a47f3.jpg","comment_is_top":false,"comment_ctime":1584976111,"is_pvip":true,"replies":[{"id":"73923","content":"一般还是要执行完成，下一个周期的任务就不再执行了。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1585025714,"ip_address":"","comment_id":193875,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1584976111","product_id":100046801,"comment_content":"老师好，我想问一下如果定时任务执行时间到了，但是数据还没执行完，这时候该怎么处理？","like_count":0,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":488688,"discussion_content":"一般还是要执行完成，下一个周期的任务就不再执行了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585025714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366137,"discussion_content":"这会取决于定时器的原理\n1. 有的会多开进程，仍然会有多个结果\n2. 有的会等待本次任务结束后，再次执行","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617968672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":190458,"user_name":"刘楠","can_delete":false,"product_type":"c1","uid":1120773,"ip_address":"","ucode":"9F19D44CBEE039","user_header":"https://static001.geekbang.org/account/avatar/00/11/1a/05/f154d134.jpg","comment_is_top":false,"comment_ctime":1584662406,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584662406","product_id":100046801,"comment_content":"这个真的有用，方法学会了，","like_count":0},{"had_liked":false,"id":188548,"user_name":"星星滴蓝天","can_delete":false,"product_type":"c1","uid":1465990,"ip_address":"","ucode":"2F2F56F93AD828","user_header":"https://static001.geekbang.org/account/avatar/00/16/5e/86/40877404.jpg","comment_is_top":false,"comment_ctime":1584367550,"is_pvip":false,"discussion_count":3,"race_medal":0,"score":"1584367550","product_id":100046801,"comment_content":"我们这种一般先上读写分离，死从库不死主库","like_count":0,"discussions":[{"author":{"id":1811495,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/a4/27/15e75982.jpg","nickname":"小袁","note":"","ucode":"3F5D8721F577D9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":206627,"discussion_content":"从库死了业务也会停，还不如想办法让从库别死。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584421924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1386818,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/42/43d4b1a8.jpg","nickname":"烫烫烫","note":"","ucode":"C06018670DE76A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1811495,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/a4/27/15e75982.jpg","nickname":"小袁","note":"","ucode":"3F5D8721F577D9","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":330532,"discussion_content":"也不会停吧，只是读请求全部打到主库上了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606641760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":206627,"ip_address":""},"score":330532,"extra":""},{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1386818,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/42/43d4b1a8.jpg","nickname":"烫烫烫","note":"","ucode":"C06018670DE76A","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":366139,"discussion_content":"一主一从，从库死了，停止同步也会影响主库\n两从只能死一个，而且主从同步数据，需要等待从库的一个确认，增加了响应时间","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617968772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":330532,"ip_address":""},"score":366139,"extra":""}]}]}]}