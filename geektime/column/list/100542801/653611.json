{"id":653611,"title":"09｜数据存储：如何将爬取到的数据存入数据库中？","content":"<p>你好，我是黄鸿波。</p><p>上节课，我们使用Scrapy框架已经能够爬取了新浪网的新闻数据，并且，我们也做了相应的翻页爬取功能。</p><p>这节课，我们就在上一节课的程序中做一个补充，加入参数传递和数据库存储相关功能（使用MongoDB数据库进行存储）。</p><h2>Python中的pymongo库</h2><p>如果要想在Python中操作MongoDB数据库，首先我们要了解一下pymongo这个库。</p><p>pymongo准确来讲是一个连接Python和MongoDB数据库的一个中间的驱动程序，这个程序可以使Python能够非常方便地使用和操作MongoDB数据库。在Python中，我们可以使用pip install pymongo的方式来安装。</p><p>接下来，我们就在我们的cmd环境中来安装我们的pymongo库。首先，我们使用下面的命令切换到我们的anaconda环境中。</p><pre><code>activate scrapy_recommendation\n</code></pre><p>如果你是Linux或者Mac用户，则需要把命令改成下面这样。</p><pre><code>conda activate scrapy_recommendation\n</code></pre><p>紧接着，我们使用下面的命令来安装pymongo。</p><pre><code>pip install pymongo\n</code></pre><p>安装完成之后，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/f7/30/f70dd8653c160486e21a8c0ab25d4430.png?wh=1443x417\" alt=\"\"></p><p>接着，我们就可以尝试着在我们的Python环境中使用它。</p><p>想要在Python环境中使用pymongo库，我们需要经过下面四个步骤。</p><!-- [[[read_end]]] --><ol>\n<li>导入pymongo库。</li>\n<li>连接MongoDB数据库。</li>\n<li>选择数据表。</li>\n<li>对数据表进行增删改查的操作。</li>\n</ol><p>接下来我们就从上面四个步骤入手，写一个简单的例子。</p><p>首先是导入pymongo这个库，在导入这个库的时候，我们直接使用import进行导入即可。接下来，我们正式对MongoDB数据库进行连接和操作。</p><p>一般来讲对于任何Python程序，如果我们想要做成工程化的形式都会先建立一个类，然后再在类中重写它的__init__函数，将我们需要初始化的内容在这里进行初始化。对于数据库来说，一般我们需要初始化的部分就是数据库的连接、选择，以及对于数据表的选择。</p><p>如果我们想要连接数据库就需要一系列的参数，包括数据库所在的服务器IP地址、端口号、用户名、密码以及数据库名。需要注意的是，MongoDB可以设置用户名密码，也可以使用默认的用户名密码。当使用默认的用户名密码时，我们就不需要输入它们，因此这里会分成有用户名密码和无用户名密码两种方式。</p><p>另外，MongoDB数据库的连接需要遵循MongoDB的通讯协议，格式如下，我给你画了一个表格简要解析了这行代码，你可以对照着进行学习。</p><pre><code>client = 'mongodb://' + user + ':' + pwd + '@' + host + &quot;:&quot; + str(port) + &quot;/&quot; + db\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/cf/28/cfb528d0a2e851027946ca690775a428.jpg?wh=2160x1808\" alt=\"\"></p><p>在上面的代码中可以有user和pwd也可以没有。因此，整个MongoDB连接的拼接程序就可以写成以下函数。这个函数我们需要放到连接数据库的函数中，作为数据库连接的一部分。</p><pre><code>def _splicing(host, port, user, pwd, db):\n\tclient = 'mongodb://' + host + &quot;:&quot; + str(port) + &quot;/&quot;\n\tif user != '':\n\t\tclient = 'mongodb://' + user + ':' + pwd + '@' + host + &quot;:&quot; + str(port) + &quot;/&quot;\n\t\tif db != '':\n\t\t\tclient += db\n\treturn client\n</code></pre><p>在使用pymongo进行数据库连接时，我们使用pymongo中的MongoClient类，这时需要创建一个MongoDB的客户端，然后用客户端来连接MongoDB数据库，整个类就会变成下面这个样子。</p><pre><code>import pymongo\n \nclass MongoDB(object):\n    def __init__(self, db):\n        mongo_client = self._connect('127.0.0.1', 27017, '', '', db)\n        self.db_scrapy = mongo_client['scrapy_data']\n        self.collection_test = self.db_scrapy['test_collections']\n \n    def _connect(self, host, port, user, pwd, db):\n        mongo_info = self._splicing(host, port, user, pwd, db)\n        mongo_client = pymongo.MongoClient(mongo_info, connectTimeoutMS=12000, connect=False)\n        return mongo_client\n \n    @staticmethod\n    def _splicing(host, port, user, pwd, db):\n        client = 'mongodb://' + host + &quot;:&quot; + str(port) + &quot;/&quot;\n        if user != '':\n            client = 'mongodb://' + user + ':' + pwd + '@' + host + &quot;:&quot; + str(port) + &quot;/&quot;\n            if db != '':\n                client += db\n        return client\n</code></pre><p>我们再来整体解读一下这段代码。实际上这段代码就是一段MongoDB的连接类，一般为了方便使用会把它单独拿出来，并在我们项目的目录中新建一个名为dao的目录，并且文件命名为mongo_db.py，在这里我的目录结构就是sina\\sina\\dao\\mongo_db.py。</p><p>在这段代码中，我们在__init__函数中初始化了几个变量，分别是用于连接的mongo_client、初始化数据库scrapy_data以及一个测试collection为test_collection。</p><h2>在Scrapy中对数据进行处理和保存</h2><p>现在我们已经写了一个简单的MongoDB数据库的连接类，接下来，我们要在Scrapy中使用它来进行数据的存取工作。在Scrapy中，管道的功能是负责处理Spider中获取到的Item，并进行后期处理（比如对数据进行分析、过滤、存储等）。因此，我们在做数据存储和处理的部分就应该写在Scrapy框架的pipelines.py文件中，这个文件的初始代码如下。</p><pre><code>from itemadapter import ItemAdapter\n \nclass SinaPipeline:\n    def process_item(self, item, spider):\n        return item\n</code></pre><p>这段代码非常简单，就是定义了一个默认的pipeline，然后默认带了一个process_item函数，我们处理数据就是在这个函数中来进行处理。</p><p>接下来我们就要来<strong>这个文件进行改造，将pymongo加入到这个文件中，并把爬虫爬取到的数据存入进去。</strong></p><p>在改造之前我们先来看一下上面的process_item这个函数，我们可以看到在这个函数的参数传递部分一共传入了2个值，分别是Item和spider，这个Item实际上就是我们爬取的数据。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/b8/02ce18db3937d494e05ddcbdd60ee1b8.png?wh=1766x1039\" alt=\"\"></p><p>我们还是先来看看这张经典的流程图。在上一节课中，我们在爬虫代码的parse_namedetail()函数中yield出去了一个Item，这个yield出去的Item实际上会被传入到Item Pipeline中，而这个Item Pipeline实际上就是对应着我们刚刚的pipelines.py文件。</p><p>在pipelines.py文件中有一个函数process_item()，这里所传入的Item参数，实际上就是从爬虫文件中yield出来的Item。清楚了这个逻辑之后你会发现，实际上这里我们所拿到的数据就是爬虫爬取回来的数据。这个时候要做的就是接收这个数据，然后将这个数据给存入MongoDB数据库中。</p><p>如果想在pipelines.py文件中使用MongoDB数据库，我们首先要做的就是导入我们写好的pymongo连接的类，并选择collection然后将我们需要的数据插入进去。 因此在顶部我们需要使用import来导入我们的类。</p><pre><code>from .dao.mongo_db import MongoDB\n</code></pre><p>在这里，我们在包的前面加了个“.”，说明这个是在当前目录下的。导入MongoDB数据库之后，我们就可以去重写我们的SinaPipeline这个类了。因为代码比较简单，因此我们直接上代码。</p><pre><code>from .dao.mongo_db import MongoDB\n \nclass SinaPipeline:\n    def __init__(self):\n        self.mongo = MongoDB(db='scrapy_data')\n        self.collection = self.mongo.db_scrapy['content_ori']\n \n    def process_item(self, item, spider):\n        result_item = dict(item)\n        self.collection.insert_one(result_item)\n        return item\n</code></pre><p>对于比较好的写法，任何一个类最好都有一个__init__函数，把我们需要初始化的内容都放在这里。因此，在这里我们重写了__init__函数，并把MongoDB的连接以及把要插入的collection进行了定义。</p><p>这里我们选择的数据库为scrapy_data，并且选择一个collection为“content_ori”。之前我们讲过，在MongoDB中如果已经有一个collection了，选择的时候就会直接去用。如果没有这个collection，则会自动创建collection。目前在我们的数据库中并没有名为“content_ori”这个collection，会自动创建。</p><p>接下来做的事情就是把process_item()这个函数进行重写，重写这个函数很简单，只需把数据转换成MongoDB所需要的bson类型，然后插入即可。</p><p>MongoDB中的bson类型实际上和Python中的dict格式一样，因此，我们就将Item转换成dict类型，并赋值给result_item这个变量，然后调用self.collection.insert_one()这个函数来进行数据的插入。</p><h2>将pipeline与爬虫程序进行关联</h2><p>这个时候是不是就可以将数据插入到数据库中了呢？理论上是的，但是实际上还差了一步。目前我们需要的代码都已经完成了，但是我们还缺少最重要的一步，那就是<strong>将pipeline和我们的爬虫程序进行关联，这个关联的操作在settings.py文件中进行。</strong></p><p>我们打开settings文件，发现这里面有很多已经写好的代码，并且都是配置文件，并且都加了注释，我们挑一些重要的来对这个文件做一个简单的解析。</p><pre><code># Scrapy settings for sina project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n \nBOT_NAME = 'sina'\n \nSPIDER_MODULES = ['sina.spiders']\nNEWSPIDER_MODULE = 'sina.spiders'\n \n \n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'sina (+http://www.yourdomain.com)'\n \n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n \n# Configure maximum concurrent requests performed by Scrapy (default: 16)\n#CONCURRENT_REQUESTS = 32\n \n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n \n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n \n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n \n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n \n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'sina.middlewares.SinaSpiderMiddleware': 543,\n#}\n \n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    'sina.middlewares.SinaDownloaderMiddleware': 543,\n#}\n \n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n \n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    'sina.pipelines.SinaPipeline': 300,\n#}\n \n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n \n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n</code></pre><p>在这个文件中，第10行的BOT_NAME定义了我们项目的名字。第12行定义了爬虫的Scrapy搜索spider的模块列表，在我们这里的列表中只有一个内容，那就是“sina.spiders”，说明目前我们只有这个一个爬虫文件。而第13行则是使用genspider命令创建新的spider的模块。</p><p>在第20行中，我们可以看到有一个ROBOTSTXT_OBEY参数，这个参数选择是否采用robots.txt策略，还记得robots协议吗？实际上就是在这里配置的。如果这里为True，那么爬虫就会遵守robots.txt的规则。</p><p>在第23行中，我们会发现一个已经被注释掉的CONCURRENT_REQUESTS这个参数，这个参数实际上指的是我们的爬虫的下载器，也就是scrapy downloader并发请求数的最大值。这个参数开启了之后，我们就可以利用多线程进行爬取，这个值一般建议设置为cpu的核心数。</p><p>在第28行中，我们可以看到一个DOWNLOAD_DELAY参数，这个参数实际上是告诉爬虫，我们的下载器在下载页面时，两个页面之间的等待间隔。这个参数主要是为了限制爬虫的速度，减轻服务器的压力。</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/5b/1de319535af069a2d46e7fa5e412a65b.jpg?wh=2820x1880\" alt=\"\"></p><p>上面这些就是相对比较重要的一些参数。但在这个设置文件中，并没有默认把爬虫和pipeline绑定在一起的参数。没关系，我们可以自己来加。我们可以在NEWSPIDER_MODULE参数下面增加一个ITEM_PIPELINES参数，并把我们的pipeline的路径赋值上去，这个时候我们的前后文就变成了这样。</p><pre><code>BOT_NAME = 'sina'\n \nSPIDER_MODULES = ['sina.spiders']\nNEWSPIDER_MODULE = 'sina.spiders'\n \nITEM_PIPELINES = {\n    'sina.pipelines.SinaPipeline': 300\n}\n</code></pre><p>然后我们再运行main.py这个文件，然后等待爬虫的结束。</p><p>当爬虫结束爬取工作之后，我们来验证一下数据是否已经存入到MongoDB数据库中。这个时候，我们可以在cmd命令中输入mongo，进入到MongoDB的控制台，然后输入下面的代码。</p><pre><code>use scrapy_data\n</code></pre><p>再输入下面这行代码。</p><pre><code>db.content_ori.find()\n</code></pre><p>然后你会发现，我们已经将爬取到的数据存入到了我们的数据库中。</p><p><img src=\"https://static001.geekbang.org/resource/image/22/6d/220845a37ee709aa8d53a50c6346ef6d.png?wh=3814x1241\" alt=\"\"></p><p>不过这样进行数据的查询起来太费劲了，我推荐你去下载一个名叫robo3T的软件（一款免费的专门针对MongoDB的数据库可视化软件）。安装之后创建一个默认的数据库连接，就可以看到我们的数据了。</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/12/1d31352ac753b2b9016e8b5abda9f412.png?wh=3840x2098\" alt=\"\"></p><p>到这里，我们就已经完成了数据的爬取和存储工作。</p><h2>总结</h2><p>到现在为止，我们本节课的代码已经学完了，关于爬虫的部分也已经完成，我来对这节课的内容做一个简单的总结。</p><ol>\n<li>我们应该知道什么是pymongo库，以及如何安装和使用它。</li>\n<li>你需要了解如何在scrapy中对数据进行处理和保存（一般是在pipelines文件中操作）。</li>\n<li>一定要记住，最后还要在settings.py文件下加入我们的pipelines相关的内容。</li>\n<li>我们也要熟悉settings.py文件下的各个配置。</li>\n</ol><p>明天就是五一假期了，在这里提前祝你五一假期玩得开心！我们的课程将会在五一期间停更（5.8日更新），一方面是希望你在这段时间里整体过一遍数据篇的内容，为接下来的召回篇做准备；另一方面这段时间看到了很多关于课程的认真反馈，我会在这期间整体盘一盘接下来的内容，做一些正文或者后续加餐的补充。如果你对于这个课程有任何进一步的建议，欢迎留言，你的建议是我不断改进的动力。</p><h2>课后题</h2><p>最后给你布置两个小作业。</p><ol>\n<li>自己使用Scrapy爬取新浪网的数据，并存入到数据库中。</li>\n<li>改造spider中的输入部分，增加翻页和增量爬取的内容（重难点）。</li>\n</ol>","comments":[{"had_liked":false,"id":373748,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1683118123,"is_pvip":false,"replies":[{"id":136922,"content":"同学你好，这个我之前也遇到过，主要是由于字符集的问题，一般这种方法copy到记事本，然后用空格来重新进行缩进，就可以解决。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685035197,"ip_address":"广东","comment_id":373748,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"第07课，创建爬虫程序出错的问题解决了，解决方法如下：\nE:\\Javaweb\\study\\recommendsys，这个目录下面有env和project两个子目录，Anaconda安装在env下面，虚拟环境在Anaconda的安装目录下面。爬虫项目在project目录下面。然后执行“scrapy genspider sina_spider sina.com.cn”时候报错了几次。\n后来在E盘下创建geekbang，和老师的目录一样，又创建了一个新的虚拟环境，照着老师的流程做下来，就成功创建了爬虫程序。\n问题解决了，不知道具体原因，感觉是目录问题。对于这个问题，老师如果有看法就告诉我一下，比如目录位置有一些坑。","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619389,"discussion_content":"同学你好，这个我之前也遇到过，主要是由于字符集的问题，一般这种方法copy到记事本，然后用空格来重新进行缩进，就可以解决。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685035197,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373689,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1683017985,"is_pvip":false,"replies":[{"id":136908,"content":"不错，可以推广给同学们。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685029805,"ip_address":"广东","comment_id":373689,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"第07课，创建爬虫程序出错的问题解决了，解决方法如下：\nE:\\Javaweb\\study\\recommendsys，这个目录下面有env和project两个子目录，Anaconda安装在env下面，虚拟环境在Anaconda的安装目录下面。爬虫项目在project目录下面。然后执行“scrapy genspider sina_spider sina.com.cn”时候报错了几次。\n后来在E盘下创建geekbang，和老师的目录一样，又创建了一个新的虚拟环境，照着老师的流程做下来，就成功创建了爬虫程序。\n问题解决了，不知道具体原因，感觉是目录问题。对于这个问题，老师如果有看法就告诉我一下，比如目录位置有一些坑。\n运行main.py后，遇到两个问题：\nQ1：ROBOTSTXT_OBEY = True时爬取失败\n创建main.py后，settings.py中，ROBOTSTXT_OBEY = True。 运行后报告：\n.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file。\n该错误导致另外一个错误：\nrobotstxt.py&quot;, line 15, in decode_robotstxt\n    robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;)\nUnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte\n网上建议ROBOTSTXT_OBEY 改成 False，好像是成功了。\nQ2：成功信息和老师的不同，不确定是否成功。\nA 专栏上的成功信息很少，我这里的信息非常多，是log设置不同吗？（我用PyCharm4.5）。\nB 专栏上的成功信息，有两个链接：\nGet https:&#47;&#47;news.sina.com.cn&#47;robots.txt\nGet https:&#47;&#47;news.sina.com.cn&#47;china\n但我的输出信息中并没有这两个链接，还没有成功吗？\n部分信息如下：\n2023-05-02 12:56:00 [scrapy.core.engine] INFO: Spider opened\n2023-05-02 12:56:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages&#47;min), scraped 0 items (at 0 items&#47;min)\n2023-05-02 12:56:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2023-05-02 12:56:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; from &lt;GET http:&#47;&#47;sina.com.cn&#47;&gt;\n2023-05-02 [engine] DEBUG: Crawled (200) &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; (referer: None)\n2023-05-02 [scrapy.core.engine] INFO: Closing spider (finished)\n2023-05-02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619366,"discussion_content":"不错，可以推广给同学们。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685029805,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373618,"user_name":"GhostGuest","can_delete":false,"product_type":"c1","uid":2871611,"ip_address":"上海","ucode":"A6785E3304276B","user_header":"https://static001.geekbang.org/account/avatar/00/2b/d1/3b/a94459d2.jpg","comment_is_top":false,"comment_ctime":1682802180,"is_pvip":false,"replies":[{"id":136918,"content":"同学你好，感谢你的指正。这里我确实说的有些问题。\nCONCURRENT_REQUESTS是Scrapy爬虫框架中的一个参数，表示同时发送请求数量的上限。该参数的默认值为16，可以根据具体环境和需求进行调整。例如，将CONCURRENT_REQUESTS设置为32可以在同一时间内发送更多的请求，加快爬取速度。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685031049,"ip_address":"广东","comment_id":373618,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"第 23 行中 CONCURRENT_REQUESTS 参数解释有误, 这个参数是设置线程数，并不是多线程的开关，文中描述开启就可以利用多线程进行爬取有点歧义了，默认就是多线程爬取的，这个参数只是设置并发量","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619382,"discussion_content":"同学你好，感谢你的指正。这里我确实说的有些问题。\nCONCURRENT_REQUESTS是Scrapy爬虫框架中的一个参数，表示同时发送请求数量的上限。该参数的默认值为16，可以根据具体环境和需求进行调整。例如，将CONCURRENT_REQUESTS设置为32可以在同一时间内发送更多的请求，加快爬取速度。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685031049,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373586,"user_name":"19984598515","can_delete":false,"product_type":"c1","uid":2267288,"ip_address":"贵州","ucode":"0CF2C50ECB7D6A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/mhSYbmpwSzVIEDu714dQuicXCf4ssKQ3LictIW6VoCFZ17EdanhRnhHEHmReiatJBrkUsfkXl4FsWU1JkoHqDiaxKA/132","comment_is_top":false,"comment_ctime":1682707136,"is_pvip":false,"replies":[{"id":136674,"content":"同学你好，我争取下周放上去。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683878185,"ip_address":"广东","comment_id":373586,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师你好，什么时候能有完整源码呢","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617793,"discussion_content":"同学你好，我争取下周放上去。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683878185,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373578,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1682676004,"is_pvip":false,"replies":[{"id":136675,"content":"是的，流程的整个调用是在settings.py里，主要是依靠爬虫名来调用整个流程，scrapy的框架流程一般是没办法改的。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683878246,"ip_address":"广东","comment_id":373578,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"调用流程是在哪里定义的？比如，对于pipelines.py文件，框架会自动调用它，假如我改变文件名字，应该就无法正常运行了；如果知道流程调用关系在哪里定义，就可以在那里修改文件名字。 （是settings.py吗？）","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617794,"discussion_content":"是的，流程的整个调用是在settings.py里，主要是依靠爬虫名来调用整个流程，scrapy的框架流程一般是没办法改的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683878246,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373558,"user_name":"GAC·DU","can_delete":false,"product_type":"c1","uid":1385403,"ip_address":"北京","ucode":"7847FBE1C13740","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","comment_is_top":false,"comment_ctime":1682657695,"is_pvip":false,"replies":[{"id":136673,"content":"同学你好，如果测试通过的话，我觉得这么写没问题。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683878165,"ip_address":"广东","comment_id":373558,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"把已经爬过的新闻标题，用Redis set集合存储，作为增量过滤器，如果标题不在集合中则抓取数据并保存数据，同时将新的标题放到集合中。\n\n测试成功，部分代码如下：\nredis：\nclass RedisDB:\n    def __init__(self):\n        self.host = &quot;ip地址&quot;\n        self.port = 6379\n        self.passwd = &quot;密码&quot;\n        self.db = 2\n        self.pool = redis.ConnectionPool(host=self.host, port=self.port, password=self.passwd, db=self.db, decode_responses=True)\n        self.client = redis.Redis(connection_pool=self.pool)\n\n    def add(self, key, value):\n        return self.client.sadd(key, value)\n\n    def exists(self, key, value):\n        return self.client.sismember(key, value)\n\nspider部分:\nif self.redis.exists(&quot;sina&quot;, items[&quot;news_title&quot;]):\n    continue\nself.redis.add(&quot;sina&quot;, items[&quot;news_title&quot;])\n\n那里有问题，请老师指正，谢谢。","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617792,"discussion_content":"同学你好，如果测试通过的话，我觉得这么写没问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683878165,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395040,"user_name":"Baird","can_delete":false,"product_type":"c1","uid":3972559,"ip_address":"上海","ucode":"FC70CBB11730FE","user_header":"https://static001.geekbang.org/account/avatar/00/3c/9d/cf/96713901.jpg","comment_is_top":false,"comment_ctime":1729158301,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"增量爬取：\npipelines.py\n新增一个方法获取MongoDB集合中的最新时间\ndef get_last_crawl_time(self):\n    latest_entry = self.collection.find_one(\n        sort=[(&quot;times&quot;, -1)],  # 按 times 字段降序排序\n        projection={&quot;times&quot;: 1} \n    )\n    if latest_entry:\n        return latest_entry[&quot;times&quot;]\n    return None\nsina_spider.py\n初始化一个sinaPipeline实例，调用get_last_crawl_time获取上次爬取新闻的最后时间\ndef __init__(self):\n    self.start_urls = [&#39;https:&#47;&#47;news.sina.com.cn&#47;china&#47;&#39;]\n    self.option = Options()\n    self.option.add_argument(&#39;--no-sandbox&#39;)\n    self.option.add_argument(&#39;--blink-settings=imagesEnabled=false&#39;)\n    self.pipeline = SinaPipeline()\n    self.last_crawl_time = self.pipeline.get_last_crawl_time()\n循环中判断时间是否小于上次爬取的新闻日期，是的话跳过这次循环\nif &#39;分钟前&#39; in eachtime:\n    minute = int(eachtime.split(&#39;分钟前&#39;)[0])\n    t = datetime.datetime.now() - datetime.timedelta(minutes=minute)\n    t2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)\nelse:\n    if &#39;年&#39; not in eachtime:\n        eachtime = str(today.year) + &#39;年&#39; + eachtime\n    t1 = re.split(&#39;[年月日:]&#39;, eachtime)\n    t2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),\n                           minute=int(t1[4]))\n\nif  t2 &lt;= self.last_crawl_time:\n    continue","like_count":0},{"had_liked":false,"id":385255,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1702357543,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"有微信群了吗？我遇到了一些问题，pymongo连不上mongo数据库","like_count":0},{"had_liked":false,"id":373747,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1683118105,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"运行main.py后，遇到两个问题：\nQ1：ROBOTSTXT_OBEY = True时爬取失败\n创建main.py后，settings.py中，ROBOTSTXT_OBEY = True。 运行后报告：\n.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file。\n该错误导致另外一个错误：\nrobotstxt.py&quot;, line 15, in decode_robotstxt\n    robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;)\nUnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte\n网上建议ROBOTSTXT_OBEY 改成 False，好像是成功了。\nQ2：成功信息和老师的不同，不确定是否成功。\nA 专栏上的成功信息很少，我这里的信息非常多，是log设置不同吗？（我用PyCharm4.5）。\nB 专栏上的成功信息，有两个链接：\nGet https:&#47;&#47;news.sina.com.cn&#47;robots.txt\nGet https:&#47;&#47;news.sina.com.cn&#47;china\n但我的输出信息中并没有这两个链接，还没有成功吗？\n部分信息如下：\n2023-05-02 12:56:00 [scrapy.core.engine] INFO: Spider opened","like_count":0}]}