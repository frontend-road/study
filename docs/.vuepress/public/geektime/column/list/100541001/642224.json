{"id":642224,"title":"04｜新时代模型性能大比拼，GPT-3到底胜在哪里？","content":"<p>你好，我是徐文浩。</p><p>前面两讲，我带你体验了OpenAI通过API提供的GPT-3.5系列模型的两个核心接口。一个是获取一段文本的Embedding向量，另一个则是根据提示语，直接生成一段补全的文本内容。我们用这两种方法，都可以实现零样本（zero-shot）或者少样本下的情感分析任务。不过，你可能会提出这样两个疑问。</p><ol>\n<li>Embedding不就是把文本变成向量吗？我也学过一些自然语言处理，直接用个开源模型，比如Word2Vec、Bert之类的就好了呀，何必要去调用OpenAI的API呢？</li>\n<li>我们在这个情感分析里做了很多投机取巧的工作。一方面，我们把3分这种相对中性的评价分数排除掉了；另一方面，我们把1分2分和4分5分分别合并在了一起，把一个原本需要判断5个分类的问题简化了。那如果我们想要准确地预测多个分类，也会这么简单吗？</li>\n</ol><p>那么，这一讲我们就先来回答第一个问题。我们还是拿代码和数据来说话，就拿常见的开源模型来试一试，看看能否通过零样本学习的方式来取得比较好的效果。第二个问题，我们下一讲再来探讨，看看能不能利用Embedding进一步通过一些机器学习的算法，来更好地处理情感分析问题。</p><h2>什么是预训练模型？</h2><!-- [[[read_end]]] --><p>给出一段文本，OpenAI就能返回给你一个Embedding向量，这是因为它的背后是GPT-3这个超大规模的预训练模型（Pre-trained Model）。事实上，GPT的英文全称翻译过来就是“生成式预训练Transformer（Generative Pre-trained Transformer）”。</p><p>所谓预训练模型，就是虽然我们没有看过你想要解决的问题，比如这里我们在情感分析里看到的用户评论和评分。但是，我可以拿很多我能找到的文本，比如网页文章、维基百科里的文章，各种书籍的电子版等等，<strong>作为理解文本内容的一个学习资料</strong>。</p><p>我们不需要对这些数据进行人工标注，只根据这些文本前后的内容，来习得文本之间内在的关联。比如，网上的资料里，会有很多“小猫很可爱”、“小狗很可爱”这样的文本。小猫和小狗后面都会跟着“很可爱”，那么我们就会知道小猫和小狗应该是相似的词，都是宠物。同时，一般我们对于它们的情感也是正面的。这些隐含的内在信息，在我们做情感分析的时候，就带来了少量用户评论和评分数据里缺少的“常识”，这些“常识”也有助于我们更好地预测。</p><p>比如，文本里有“白日依山尽”，那么模型就知道后面应该跟“黄河入海流”。文本前面是“今天天气真”，后面跟着的大概率是“不错”，小概率是“糟糕”。这些文本关系，最后以一堆参数的形式体现出来。对于你输入的文本，它可以根据这些参数计算出一个向量，然后根据这个向量，来推算这个文本后面的内容。</p><p>可以这样来理解：<strong>用来训练的语料文本越丰富，模型中可以放的参数越多，那模型能够学到的关系也就越多。类似的情况在文本里出现得越多，那么将来模型猜得也就越准。</strong></p><p>预训练模型在自然语言处理领域并不是OpenAI的专利。早在2013年，就有一篇叫做Word2Vec的经典论文谈到过。它能够通过预训练，根据同一个句子里一个单词前后出现的单词，来得到每个单词的向量。而在2018年，Google关于BERT的论文发表之后，整个业界也都会使用BERT这样的预训练模型，把一段文本变成向量用来解决自己的自然语言处理任务。在GPT-3论文发表之前，大家普遍的结论是，BERT作为预训练的模型效果也是优于GPT的。</p><h2>Fasttext、T5、GPT-3模型效果大比拼</h2><p>今天我们就拿两个开源的预训练模型，来看看直接用它们对文本进行向量化，是不是也能取得和OpenAI的API一样好的效果。</p><p>第一个是来自Facebook的Fasttext，它继承了Word2Vec的思路，能够把一个个单词表示成向量。第二个是来自Google的T5，T5的全称是Text-to-Text Transfer Trasnformer，是适合做迁移学习的一个模型。所谓迁移学习，也就是它推理出来向量的结果，常常被拿来再进行机器学习，去解决其他自然语言处理问题。通常很多新发表的论文，会把T5作为预训练模型进行微调和训练，或者把它当作Benchmark来对比、评估。</p><h3>Fasttext效果测试</h3><p>我们先来试一下Fasttext，在实际运行代码之前，我们需要先安装Fasttext和Gensim这两个Python包。</p><p>我在下面列出了通过pip安装对应Python包的代码，如果你使用的是PIP或者其他的Python包管理工具，你就换成对应的命令就好了。因为这一讲加载的AI模型需要的内存比较大，为了快速运行也需要对应的GPU资源，推荐你可以使用Google Colab来实验对应的代码。</p><pre><code class=\"language-bash\">%pip install gensim\n%pip install fasttext\n</code></pre><p>然后，我们要把Fasttext对应的模型下载到本地。因为这些开源库和对应的论文都是Facebook和Google这样的海外公司发布的，效果自然是在英语上比较好，所以我们就下载对应的英语模型，名字叫做 “cc.en.300.bin”。同样的，对应模型的下载链接，我也放在<a href=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\">这里</a>了。</p><p>下载之后解压，然后把文件放在和Notebook相同的目录下，方便我们接下来运行代码。</p><p>这里我们拿来测试效果的数据集还是和<a href=\"https://time.geekbang.org/column/article/642179\">第 02 讲</a>一样，用的是 2.5w 条亚马逊食物评论的数据。</p><p><img src=\"https://static001.geekbang.org/resource/image/12/46/1288595dd50b5aa60250daa94f5fd346.png?wh=1258x415\" alt=\"\"></p><p><span class=\"reference\">注：各种语言的 <a href=\"https://fasttext.cc/docs/en/crawl-vectors.html\">Fasttext模型</a>。</span></p><p>代码的逻辑也不复杂，我们先利用Gensim这个库，把Facebook预训练好的模型加载进来。然后，我们定义一个获取文本向量的函数。因为Fasttext学到的是单词的向量，而不是句子的向量。同时，因为我们想要测试一下零样本学习的效果，不能再根据拿到的评论数据进一步训练模型了。所以我们把一句话里每个单词的向量，加在一起平均一下，把得到的向量作为整段评论的向量。这个方法也是当年常用的<strong>一种将一句话变成向量的办法</strong>。我们把这个操作定义成了 get_fasttext_vector 这个函数，供后面的程序使用。</p><pre><code class=\"language-python\">import gensim\nimport numpy as np\n# Load the FastText pre-trained model\nmodel = gensim.models.fasttext.load_facebook_model('cc.en.300.bin')\n\ndef get_fasttext_vector(line):\n    vec = np.zeros(300) # Initialize an empty 300-dimensional vector\n    for word in line.split():\n        vec += model.wv[word]\n    vec /= len(line.split()) # Take the average over all words in the line\n    return vec\n</code></pre><p>而对应的零样本学习，我们还是和<a href=\"https://time.geekbang.org/column/article/642179\">第 02 讲</a>一样，将需要进行情感判断的评论分别与 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话进行向量计算，算出它们之间的余弦距离。</p><p><strong>离前一个近，我们就认为是正面情感，离后一个近就是负面情感。</strong></p><pre><code class=\"language-python\">positive_text = \"\"\"Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone\"\"\"\nnegative_text = \"\"\"First, these should be called Mac - Coconut bars, as Coconut is the #2 ingredient and Mango is #3.  Second, lots of people don't like coconut.  I happen to be allergic to it.  Word to Amazon that if you want happy customers to make things like this more prominent.  Thanks.\"\"\"\n\npositive_example_in_fasttext = get_fasttext_vector(positive_text)\nnegative_example_in_fasttext = get_fasttext_vector(negative_text)\n\npositive_review_in_fasttext = get_fasttext_vector(\"An Amazon review with a positive sentiment.\")\nnegative_review_in_fasttext = get_fasttext_vector('An Amazon review with a negative sentiment.')\n\nimport numpy as np\n\ndef get_fasttext_score(sample_embedding):\n  return cosine_similarity(sample_embedding, positive_review_in_fasttext) - cosine_similarity(sample_embedding, negative_review_in_fasttext)\n\npositive_score = get_fasttext_score(positive_example_in_fasttext)\nnegative_score = get_fasttext_score(negative_example_in_fasttext)\n\nprint(\"Fasttext好评例子的评分 : %f\" % (positive_score))\nprint(\"Fasttext差评例子的评分 : %f\" % (negative_score))\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">Fasttext好评例子的评分 : -0.000544\nFasttext差评例子的评分 : 0.000369\n</code></pre><p>我们从亚马逊食物评论的数据集里，选取了一个用户打5分的正面例子和一个用户打1分的例子试了一下。结果非常不幸，通过这个零样本学习的方式，这两个例子，程序都判断错了。</p><p>不过，仔细想一下，这样的结果也正常。因为这里的整句向量就是把所有单词的向量平均了一下。这意味着，<strong>可能会出现我们之前说过的单词相同顺序不同的问题。</strong></p><p>“not good, really bad” 和 “not bad, really good”，在这个情况下，意思完全不同，但是向量完全相同。更何况，我们拿来做对比的正面情感和负面情感的两句话，只差了positive/negative这样一个单词。不考虑单词的顺序，而只考虑出现了哪些单词，并且不同单词之间还平均一下。这种策略要是真的有很好的效果，你反而要担心是不是哪里有Bug。</p><h3>T5效果测试</h3><p>Fasttext出师不利，毕竟Word2Vec已经是10年前的技术了，可以理解。那么，我们来看看和GPT一样使用了现在最流行的Transformer结构的T5模型效果怎么样。</p><p>T5模型的全称是Text-to-Text Transfer Transformer，翻译成中文就是“文本到文本的迁移Transformer”，也就是说，这个模型就是为了方便预训练之后拿去“迁移”到别的任务上而创造出来的。当时发表的时候，它就在各种数据集的评测上高居榜首。</p><p>T5最大的模型也有110亿个参数，也是基于Transformer，虽然比起GPT-3的1750亿小了不少，但是对硬件的性能要求也不低。所以，我们先测试一下T5-Small这个小模型看看效果。</p><p>同样的，在实际运行代码之前，我们也需要安装对应的Python包。这里我们分别安装了SentencePiece和PyTorch。在安装PyTorch的时候，我一并安装了Torchvision，后面课程会用到。</p><pre><code class=\"language-python\">conda install transformers -c conda-forge\nconda install pytorch torchvision -c pytorch\nconda install sentencepiece\n</code></pre><p>代码也不复杂，我们先加载预训练好的T5模型的分词器（Tokenizer），还有对应的模型。然后，我们定义了一个get_t5_vector函数，它会接收一段你的文本输入，然后用分词器来分词把结果变成一个序列，然后让模型的编码器部分对其进行编码。编码后的结果，仍然是分词后的一个词一个向量，我们还是把这些向量平均一下，作为整段文本的向量。</p><p>不过要注意，虽然同样是平均，但是和前面Fasttext不一样的是，这里每个词的向量，随着位置以及前后词的不同，编码出来的结果是不一样的。所以<strong>这个平均值里，仍然包含了顺序带来的语义信息。</strong></p><p>这段代码执行的过程可能会有点慢。因为第一次加载模型的时候，Transformer库会把模型下载到本地并缓存起来，整个下载过程会花一些时间。</p><pre><code class=\"language-python\">from transformers import T5Tokenizer, T5Model\nimport torch\n\n# load the T5 tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('t5-small', model_max_length=512)\nmodel = T5Model.from_pretrained('t5-small')\n\n# set the model to evaluation mode\nmodel.eval()\n\n# encode the input sentence\ndef get_t5_vector(line):\n    input_ids = tokenizer.encode(line, return_tensors='pt', max_length=512, truncation=True)\n    # generate the vector representation\n    with torch.no_grad():\n        outputs = model.encoder(input_ids=input_ids)\n        vector = outputs.last_hidden_state.mean(dim=1)\n    return vector[0]\n</code></pre><p>有了模型和通过模型获取的向量数据，我们就可以再试一试前面的零样本学习的方式，来看看效果怎么样了。我们简单地把之前获取向量和计算向量的函数调用，都换成新的get_t5_vector，运行一下就能看到结果了。</p><pre><code class=\"language-python\">positive_review_in_t5 = get_t5_vector(\"An Amazon review with a positive sentiment.\")\nnegative_review_in_t5 = get_t5_vector('An Amazon review with a negative sentiment.')\n\ndef test_t5():\n  positive_example_in_t5 = get_t5_vector(positive_text)\n  negative_example_in_t5 = get_t5_vector(negative_text)\n\n  def get_t5_score(sample_embedding):\n    return cosine_similarity(sample_embedding, positive_review_in_t5) - cosine_similarity(sample_embedding, negative_review_in_t5)\n\n  positive_score = get_t5_score(positive_example_in_t5)\n  negative_score = get_t5_score(negative_example_in_t5)\n\n  print(\"T5好评例子的评分 : %f\" % (positive_score))\n  print(\"T5差评例子的评分 : %f\" % (negative_score))\n\ntest_t5()\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">T5好评例子的评分 : -0.010294\nT5差评例子的评分 : -0.008990\n</code></pre><p>不幸的是，结果还是不太好，两个例子都被判断成了负面情绪，而且好评的分数还更低一点。不过别着急，会不会是我们用的模型太小了呢？毕竟T5论文里霸占各个排行榜的是110亿个参数的大模型，我们这里用的是T5-Small这个同样架构下的小模型，参数数量只有6000万个。</p><p>110亿个参数要花太多时间了，我们不妨把模型放大一下，试试有2.2亿个参数的 T5-Base这个模型？试用起来也很简单，我们就直接把上面模型的名字从 T5-small 改成 T5-base 就好了，其他代码不需要动，重新运行一遍。</p><pre><code class=\"language-python\">tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)\nmodel = T5Model.from_pretrained('t5-base')\n\n# set the model to evaluation mode\nmodel.eval()\n\n# encode the input sentence\ndef get_t5_vector(line):\n    input_ids = tokenizer.encode(line, return_tensors='pt', max_length=512, truncation=True)\n    # generate the vector representation\n    with torch.no_grad():\n        outputs = model.encoder(input_ids=input_ids)\n        vector = outputs.last_hidden_state.mean(dim=1)\n    return vector[0]\n\npositive_review_in_t5 = get_t5_vector(\"An Amazon review with a positive sentiment.\")\nnegative_review_in_t5 = get_t5_vector('An Amazon review with a negative sentiment.')\n\ntest_t5()\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">T5好评例子的评分 : 0.010347\nT5差评例子的评分 : -0.023935\n</code></pre><p>这一次，结果似乎是我们想要的了，好评被判定为正面情感，而差评被判定为负面情感。不过，也许我们只是运气好，在这一两个例子上有效果呢？所以，接下来让我们把整个数据集里，1分2分的差评和4分5分的好评都拿出来看一看。在OpenAI的API拿到的Embedding里，它的准确率能够达到95%以上，我们看看用这个有2.2亿个参数的T5-base模型能有什么样的结果。</p><p>对应的代码也不复杂，基本上和<a href=\"https://time.geekbang.org/column/article/642179\">第 02 讲</a>里OpenAI给到的Embedding代码是类似的。无非是通过pandas，根据评论的Text字段，全部计算一遍T5下的Embedding，然后存到DataFrame的t5_embedding 里去。</p><p>同样的，我们还是要通过T5的模型，来获得 “An Amazon review with a positive sentiment.” 以及 “An Amazon review with a negative sentiment.” 这两句话的Embedding。然后，我们用刚刚计算的用户评论的Embedding和这两句话计算一下余弦距离，来判断这些评论是正面还是负面的。</p><p>最后，通过Scikit-learn的分类报告的类库把评估的报告结果打印出来。</p><pre><code class=\"language-python\">import pandas as pd\nfrom sklearn.metrics import classification_report\n\ndatafile_path = \"data/fine_food_reviews_with_embeddings_1k.csv\"\n\ndf = pd.read_csv(datafile_path)\n\ndf[\"t5_embedding\"] = df.Text.apply(get_t5_vector)\n# convert 5-star rating to binary sentiment\ndf = df[df.Score != 3]\ndf[\"sentiment\"] = df.Score.replace({1: \"negative\", 2: \"negative\", 4: \"positive\", 5: \"positive\"})\n\nfrom sklearn.metrics import PrecisionRecallDisplay\ndef evaluate_embeddings_approach():\n    def label_score(review_embedding):\n        return cosine_similarity(review_embedding, positive_review_in_t5) - cosine_similarity(review_embedding, negative_review_in_t5)\n\n    probas = df[\"t5_embedding\"].apply(lambda x: label_score(x))\n    preds = probas.apply(lambda x: 'positive' if x&gt;0 else 'negative')\n\n    report = classification_report(df.sentiment, preds)\n    print(report)\n\n    display = PrecisionRecallDisplay.from_predictions(df.sentiment, probas, pos_label='positive')\n    _ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\nevaluate_embeddings_approach()\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">              precision    recall  f1-score   support\n    negative       0.60      0.90      0.72       136\n    positive       0.98      0.90      0.94       789\n    accuracy                           0.90       925\n   macro avg       0.79      0.90      0.83       925\nweighted avg       0.93      0.90      0.91       925\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/ac/af/ac3b30d430fb855f1382bc845b6798af.png?wh=576x453\" alt=\"图片\"></p><p>结果显示，使用T5的效果也还可以，考虑所有样本的准确率也能达到90%。但是，在比较困难的差评的判断里，它的表现要比直接用OpenAI给到的Embedding要差很多，整体的精度只有60%。我们去看整体模型的准确率的话，OpenAI的Embedding能够到达96%，还是比这里的90%要好上一些的。</p><pre><code class=\"language-python\">            precision    recall  f1-score   support\n    negative       0.98      0.73      0.84       136\n    positive       0.96      1.00      0.98       789\n    accuracy                           0.96       925\n   macro avg       0.97      0.86      0.91       925\nweighted avg       0.96      0.96      0.96       925\n</code></pre><p>这里我重新贴一下使用OpenAI的Embedding取得的效果，你可以做个对比。</p><p>当然，这个分数也还不错，也能作为一个合格的情感分析分类器的基准线了。毕竟，我们这里采用的是零样本分类的方法，没有对需要分类的数据做任何训练，使用的完全是预训练模型给出来的向量，直接根据距离做的判断。所以，看起来大一点的预训练模型的确有用，能够取得更好的效果。而且，当你因为成本或者网络延时的问题，不方便使用OpenAI的API的时候，如果只是要获取文本的Embedding向量，使用T5这样的开源模型其实效果也还不错。</p><h2>小结</h2><p>最后，我们来复习一下这一讲的内容。</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/af/a2d8bc53ca126473543868e262d42baf.jpg?wh=2844x1762\" alt=\"\"></p><p>这一讲我们一起使用 Fasttext、T5-small 和 T5-base 这三个预训练模型，做了零样本分类测试。在和之前相同的食物评论的数据集上，使用只学习了单词向量表示的Fasttext，效果很糟糕。当我们换用同样基于Transformer的T5模型的时候，T5-small这个6000万参数的小模型其实效果也不好。但是当我们用上2.2亿参数的T5-base模型的时候，结果还可以。不过，还是远远比不上直接使用OpenAI的API的效果。可见，模型的大小，即使是对情感分析这样简单的问题，也能产生明显的差距。</p><h2>课后练习</h2><ol>\n<li>我们在尝试使用T5-base这个模型之后，下了个判断认为大一点的模型效果更好。不过，其实我们并没有在整个数据集上使用T5-small这个模型做评测，你能试着修改一下代码，用T5-small测试一下整个数据集吗？测试下来的效果又是怎样的呢？</li>\n<li>我们使用Fasttext的时候，把所有的单词向量平均一下，用来做情感分析效果很糟糕。那么什么样的分类问题，可以使用这样的方式呢？给你一个小提示，你觉得什么样的文本分类，只关心出现的单词是什么，而不关心它们的顺序？</li>\n</ol><p>期待你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>","neighbors":{"left":{"article_title":"03｜巧用提示语，说说话就能做个聊天机器人","id":642197},"right":{"article_title":"05｜善用Embedding，我们来给文本分分类","id":643889}},"comments":[{"had_liked":false,"id":371464,"user_name":"Daniel","can_delete":false,"product_type":"c1","uid":2046024,"ip_address":"北京","ucode":"8AF73DE4F435B6","user_header":"https://static001.geekbang.org/account/avatar/00/1f/38/48/557629d9.jpg","comment_is_top":false,"comment_ctime":1679973669,"is_pvip":false,"replies":[{"id":135721,"content":"BERT是 \n白日依山尽，_____，欲穷千里目\n\nGPT是\n白日依山尽，_____\n\n一个是完形填空，一个是续写。GPT没法看到后面的东西，所以在很多语义理解的指标上不如BERT。\n\n但是很多真实的使用场景你看不到后面的东西，所以从AGI的路线上，很多人觉得GPT才是正确路径。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680581943,"ip_address":"上海","comment_id":371464,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"BERT：BERT 基于 Transformer 的（Encoder）。BERT 使用双向（bidirectional）的自注意力机制，可以同时捕捉文本中的前后上下文信息。\nGPT：GPT 基于 Transformer 的（Decoder）。GPT 使用单向（unidirectional）的自注意力机制，只能捕捉文本中的前文（left context）信息。\n能否请老师详细讲一下，这两者的差别？","like_count":61,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612220,"discussion_content":"BERT是 \n白日依山尽，_____，欲穷千里目\n\nGPT是\n白日依山尽，_____\n\n一个是完形填空，一个是续写。GPT没法看到后面的东西，所以在很多语义理解的指标上不如BERT。\n\n但是很多真实的使用场景你看不到后面的东西，所以从AGI的路线上，很多人觉得GPT才是正确路径。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1680581943,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371516,"user_name":"stg609","can_delete":false,"product_type":"c1","uid":1073025,"ip_address":"浙江","ucode":"FB70A75A891BB8","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/81/1c614f4a.jpg","comment_is_top":false,"comment_ctime":1680005311,"is_pvip":false,"replies":[{"id":135777,"content":"1. 都是 GPT 家族的模型， ada, babbage, curie, davinci 模型从小到大\n2. ada模型应该要小得多，所以便宜","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680588847,"ip_address":"上海","comment_id":371516,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师能说说\n1.  davinci, ada 等模型与gpt3的关系吗？\n2. gpt3有1750亿参数，那是不是 ada 也有这么大量的参数","like_count":8,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612288,"discussion_content":"1. 都是 GPT 家族的模型， ada, babbage, curie, davinci 模型从小到大\n2. ada模型应该要小得多，所以便宜","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680588847,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371415,"user_name":"Roy Liang","can_delete":false,"product_type":"c1","uid":1098898,"ip_address":"广东","ucode":"1DF5FC831A35DA","user_header":"https://static001.geekbang.org/account/avatar/00/10/c4/92/338b5609.jpg","comment_is_top":false,"comment_ctime":1679911265,"is_pvip":false,"replies":[{"id":135532,"content":"1. 往后看一讲，第5讲里会具体解释这些指标的含义。\n2. 对，主题分类不太关注语序","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1679968796,"ip_address":"上海","comment_id":371415,"utype":1}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"1. 小数据集验证结果是这样，但是门外汉其实不懂表格里什么意思\n              precision    recall  f1-score   support\n\n    negative       0.25      0.99      0.40       136\n    positive       1.00      0.48      0.65       789\n\n    accuracy                           0.56       925\n   macro avg       0.62      0.74      0.52       925\nweighted avg       0.89      0.56      0.61       925\n\n2. 可能适合新闻分类、垃圾邮件分类等不关心词语次序的场景吧","like_count":6,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611154,"discussion_content":"1. 往后看一讲，第5讲里会具体解释这些指标的含义。\n2. 对，主题分类不太关注语序","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1679968796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1294597,"avatar":"https://static001.geekbang.org/account/avatar/00/13/c1/05/fd1d47b6.jpg","nickname":"空间","note":"","ucode":"2C83162E9E914A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647824,"discussion_content":"直接点“解释代码”，这里内嵌的AI就可以解释清楚了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720659387,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1811277,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/a3/4d/59390ba9.jpg","nickname":"排骨","note":"","ucode":"A413CF46211E1F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":645600,"discussion_content":"试着问下gpt看下他怎么回答","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1716424216,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2063962,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/7e/5a/da39f489.jpg","nickname":"Ethan New","note":"","ucode":"9CA2EF39E58030","race_medal":4,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611086,"discussion_content":"老哥，改代码的哪个地方呀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1679923139,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371372,"user_name":"Geek_61af67","can_delete":false,"product_type":"c1","uid":3572011,"ip_address":"北京","ucode":"09A8DC7C3686C0","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKPJb1k8zia02BAFmutyQ3jmlNpI9libqhtmNp6bTlJAYEj083ViaM048yuMHKs8na5TvLIkRDFRibiaZA/132","comment_is_top":false,"comment_ctime":1679849841,"is_pvip":false,"replies":[{"id":135550,"content":"tags的确不太关注顺序","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1679970818,"ip_address":"上海","comment_id":371372,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"不关心顺序的话，对tags进行分析会不会比较合适？","like_count":5,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611181,"discussion_content":"tags的确不太关注顺序","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1679970818,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373100,"user_name":"qingtama","can_delete":false,"product_type":"c1","uid":1069042,"ip_address":"北京","ucode":"765040243ECE01","user_header":"https://static001.geekbang.org/account/avatar/00/10/4f/f2/6ac3bdcf.jpg","comment_is_top":false,"comment_ctime":1682042185,"is_pvip":false,"replies":[{"id":136491,"content":"不能，输出的向量维度没有那么大。\n\n是指所谓的 transformer模型里面的各种变量参数有2.2亿个。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683087112,"ip_address":"上海","comment_id":373100,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请问老师，这里的2.2亿参数，可以理解成向量所在的维度是2.2亿个吗？","like_count":4,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616727,"discussion_content":"不能，输出的向量维度没有那么大。\n\n是指所谓的 transformer模型里面的各种变量参数有2.2亿个。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683087112,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371865,"user_name":"HXL","can_delete":false,"product_type":"c1","uid":2631455,"ip_address":"北京","ucode":"97298EEC95BFCA","user_header":"https://static001.geekbang.org/account/avatar/00/28/27/1f/42059b0f.jpg","comment_is_top":false,"comment_ctime":1680446072,"is_pvip":false,"replies":[{"id":135783,"content":"预训练模型，顾名思义，就是“预先训练好的模型”\n也就是这个模型，用了别的很多很多数据训练好了。可能和我们现在要解决的问题的数据有关，也有可能没有关系。\n\n但是因为 预训练模型 通常通过海量的数据训练的，它多少对你现在要解决的问题的知识是有了解和帮助的。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680590129,"ip_address":"上海","comment_id":371865,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"还是没明白什么是 预训练模型","like_count":4,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612300,"discussion_content":"预训练模型，顾名思义，就是“预先训练好的模型”\n也就是这个模型，用了别的很多很多数据训练好了。可能和我们现在要解决的问题的数据有关，也有可能没有关系。\n\n但是因为 预训练模型 通常通过海量的数据训练的，它多少对你现在要解决的问题的知识是有了解和帮助的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680590129,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372025,"user_name":"王昊翔Harry","can_delete":false,"product_type":"c1","uid":3573206,"ip_address":"英国","ucode":"D4B5FDFF6E1A50","user_header":"https://static001.geekbang.org/account/avatar/00/36/85/d6/0221579f.jpg","comment_is_top":false,"comment_ctime":1680604430,"is_pvip":false,"replies":[{"id":135886,"content":"我在 https:&#47;&#47;github.com&#47;xuwenhao&#47;geektime-ai-course 放了可以在Colab运行的Notebook代码，只要通过 pip 安装好依赖的包就能够运行。\n","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680974340,"ip_address":"日本","comment_id":372025,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"这一套流程有没有在Colab友好的。本身没有编程经验该怎么着手？","like_count":3,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612882,"discussion_content":"我在 https://github.com/xuwenhao/geektime-ai-course 放了可以在Colab运行的Notebook代码，只要通过 pip 安装好依赖的包就能够运行。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680974340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"日本","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372796,"user_name":"摩西","can_delete":false,"product_type":"c1","uid":1004110,"ip_address":"广东","ucode":"4E5736D525B765","user_header":"https://static001.geekbang.org/account/avatar/00/0f/52/4e/c7c5b256.jpg","comment_is_top":false,"comment_ctime":1681634892,"is_pvip":false,"replies":[{"id":136258,"content":"transformers 是指一种深度学习的基础模型架构，huggingface的transformers库，相当于为这类模型架构的开发、部署、试用定义了一个通用的接口形式。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681794428,"ip_address":"上海","comment_id":372796,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"刚接触机器学习，基础比较薄弱，请问老师 Transformer 是指什么？这里的transformer 跟 huggingface 中的transformer是相同的内容吗？","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":614460,"discussion_content":"transformers 是指一种深度学习的基础模型架构，huggingface的transformers库，相当于为这类模型架构的开发、部署、试用定义了一个通用的接口形式。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681794428,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371793,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1680332849,"is_pvip":false,"replies":[{"id":135733,"content":"👍 \n\nhttps:&#47;&#47;github.com&#47;xuwenhao&#47;geektime-ai-course\n\n可以看一些我放在线上的Notebook是否从上到下按步骤执行是否可以。\n文章里面对执行顺序做了一些假设。\n\n","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680583114,"ip_address":"上海","comment_id":371793,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"fasttext 无法安装成功，程序没有调试。\nT5的程序经过修改，可以安装使用。需要修改了以下句子：\n一、将fasttext中的句子：\npositive_text = &quot;&quot;&quot;Wanted to save some to bring to ……\nnegative_text = &quot;&quot;&quot;First, these shoul……\n完整黏贴到T5程序开始中。\n二、增加：from sklearn.metrics.pairwise import cosine_similarity\n三、将调用cosine_similarity 函数参数使用 NumPy 的 reshape()由一维数组转换为二维数组,具体如下:\ndef get_t5_score(sample_embedding):\n    return cosine_similarity(sample_embedding.reshape(1, -1), positive_review_in_t5.reshape(1, -1)) - cosine_similarity(sample_embedding.reshape(1, -1), negative_review_in_t5.reshape(1, -1))\n\ndef evaluate_embeddings_approach():\n    def label_score(review_embedding):\n        return cosine_similarity(review_embedding.reshape(1, -1), positive_review_in_t5.reshape(1, -1)) - cosine_similarity(review_embedding.reshape(1, -1), negative_review_in_t5.reshape(1, -1))\n\n四、将datafile_path = “fine_food_reviews_with_embeddings_1k.csv的地址” #按你电脑实际路径填写\n五、减少数量量，将fine_food_reviews_with_embeddings_1k.csv 的数据减少到100个，运行速度快很多","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612233,"discussion_content":"👍 \n\nhttps://github.com/xuwenhao/geektime-ai-course\n\n可以看一些我放在线上的Notebook是否从上到下按步骤执行是否可以。\n文章里面对执行顺序做了一些假设。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680583114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374326,"user_name":"Geek_93970d","can_delete":false,"product_type":"c1","uid":1452167,"ip_address":"北京","ucode":"52AC308BEC7737","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","comment_is_top":false,"comment_ctime":1683904127,"is_pvip":false,"replies":[{"id":136861,"content":"pip install fasttext 不行么？","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684752362,"ip_address":"上海","comment_id":374326,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"load_facebook_model 这个函数就得卡好半天，模型文件6.7G，加载很慢。另外 fasttext 既然用的是 gensim 里的，那就不用单独安装了吧？安装也安装不上，PackagesNotFoundError。","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619044,"discussion_content":"pip install fasttext 不行么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684752363,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374333,"user_name":"王石磊","can_delete":false,"product_type":"c1","uid":1905809,"ip_address":"土耳其","ucode":"CD8E0C5CF120E1","user_header":"https://static001.geekbang.org/account/avatar/00/1d/14/91/794687ef.jpg","comment_is_top":false,"comment_ctime":1683937415,"is_pvip":false,"replies":[{"id":136834,"content":"看一下哪里有问题？T5-Base没有那么差，这个看起来像T5-Small乃至更小的模型的结果。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684744842,"ip_address":"上海","comment_id":374333,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"参考文中的用例，用T5-base 推理的结果如下，准确度为56%，这大概是什么原因呢？ \nprecision    recall  f1-score   support\n\n    negative       0.25      0.99      0.40       136\n    positive       1.00      0.48      0.65       789\n\n    accuracy                           0.56       925\n   macro avg       0.62      0.74      0.52       925\nweighted avg       0.89      0.56      0.61       925\n","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619003,"discussion_content":"看一下哪里有问题？T5-Base没有那么差，这个看起来像T5-Small乃至更小的模型的结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684744842,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373205,"user_name":"Geek_40e894","can_delete":false,"product_type":"c1","uid":3604039,"ip_address":"江苏","ucode":"36D9102F832C8E","user_header":"","comment_is_top":false,"comment_ctime":1682241155,"is_pvip":false,"replies":[{"id":136454,"content":"应该是够的呀，Colab的机器有12G内存，看看是否有别的什么已经占了太多内容 restart jupyter runtime一下？","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683082849,"ip_address":"上海","comment_id":373205,"utype":1}],"discussion_count":3,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"colab Load the FastText pre-trained model 内存不够，崩溃","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616676,"discussion_content":"应该是够的呀，Colab的机器有12G内存，看看是否有别的什么已经占了太多内容 restart jupyter runtime一下？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683082849,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3556486,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLNWRTm9x25GZc5iaG09soGTsS6le9Ykv8aktXs3wzxakPIic0HmFS7CNKnhnicXNibTfcQxAia7w6jiaxQ/132","nickname":"Geek_a496d7","note":"","ucode":"AF011554DB7A14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":638754,"discussion_content":"fasttest在colab上确实会崩，本地跑吧，这个例子反正不用open ai，本地也没网络问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1709876746,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1052643,"avatar":"https://static001.geekbang.org/account/avatar/00/10/0f/e3/c49aa508.jpg","nickname":"鲸鱼","note":"","ucode":"71437C1C601040","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621819,"discussion_content":"我的colab也是在加载模型时崩溃，内存从1g一路涨到12g，然后崩溃。试了很多次都是这样。是不是模型太大了？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1687676480,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373202,"user_name":"buffett","can_delete":false,"product_type":"c1","uid":1145599,"ip_address":"北京","ucode":"6606929852E095","user_header":"https://static001.geekbang.org/account/avatar/00/11/7a/ff/df4eab46.jpg","comment_is_top":false,"comment_ctime":1682239012,"is_pvip":false,"replies":[{"id":136453,"content":"不是，本地加载模型了","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683082794,"ip_address":"上海","comment_id":373202,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"from transformers import T5Tokenizer, T5Model，这个代码调用T5模型的也是通过通过api请求吗？是maas的方式吗？还是本地load了模型了","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616675,"discussion_content":"不是，本地加载模型了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683082794,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373089,"user_name":"意无尽","can_delete":false,"product_type":"c1","uid":1234489,"ip_address":"重庆","ucode":"DA179626C46B81","user_header":"https://static001.geekbang.org/account/avatar/00/12/d6/39/6b45878d.jpg","comment_is_top":false,"comment_ctime":1682008682,"is_pvip":false,"replies":[{"id":136523,"content":"1. Fasttext没有语序信息，只有单个单词的文本信息\n2. 模型规模也有比较大差距","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683093903,"ip_address":"上海","comment_id":373089,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师，有点不是很理解的，就是文中讲的向量，可不可以这么理解：Fasttext 和 GPT3 的一个差别是 Fasttext 使用到的上下文比较少，所以很多场景下缺乏语义，导致判断失败，而 GPT3 包含的很多，所以更精准？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616764,"discussion_content":"1. Fasttext没有语序信息，只有单个单词的文本信息\n2. 模型规模也有比较大差距","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1683093903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372382,"user_name":"yanger2004","can_delete":false,"product_type":"c1","uid":1009644,"ip_address":"上海","ucode":"0A2CD03EF31052","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erms9qcIFYZ4npgLYPu1QgxQyaXcj64ZBicNVeBRWcYUpCZ9p0BGsrEcX8heibMLCV4Gde4P9pf7PjA/132","comment_is_top":false,"comment_ctime":1681082110,"is_pvip":false,"replies":[{"id":135935,"content":"一般来说是这样的。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681128773,"ip_address":"美国","comment_id":372382,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"如果用完整的110亿参数来测试，准确率应该会更高吧","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613104,"discussion_content":"一般来说是这样的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681128773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372122,"user_name":"海滨","can_delete":false,"product_type":"c1","uid":1009518,"ip_address":"浙江","ucode":"F1B94D2DB944DC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/6e/f5ee46e8.jpg","comment_is_top":false,"comment_ctime":1680760348,"is_pvip":false,"replies":[{"id":135899,"content":"挺多的 flan-t5, albert 等等一系列，需要看你具体想要解决的任务是什么\n一般来说，大家喜欢用 flan-t5 作为最常用的模型","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680975137,"ip_address":"日本","comment_id":372122,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"除了T5之外，有其他做NLP任务效果更好的大模型推荐吗？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612895,"discussion_content":"挺多的 flan-t5, albert 等等一系列，需要看你具体想要解决的任务是什么\n一般来说，大家喜欢用 flan-t5 作为最常用的模型","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680975137,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"日本","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371604,"user_name":"Warren","can_delete":false,"product_type":"c1","uid":1186273,"ip_address":"广东","ucode":"7518ED2E07AAF7","user_header":"https://static001.geekbang.org/account/avatar/00/12/19/e1/a7fbc963.jpg","comment_is_top":false,"comment_ctime":1680094121,"is_pvip":true,"replies":[{"id":135745,"content":"感谢，我让编辑修正一下","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680586046,"ip_address":"上海","comment_id":371604,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"\nconda install transformer -c conda-forge 这个命令中的transformer要改成transformers","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612251,"discussion_content":"感谢，我让编辑修正一下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680586046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371387,"user_name":"X丶楓","can_delete":false,"product_type":"c1","uid":1079760,"ip_address":"北京","ucode":"F5402FBD150BC1","user_header":"https://static001.geekbang.org/account/avatar/00/10/79/d0/63ac112e.jpg","comment_is_top":false,"comment_ctime":1679892646,"is_pvip":false,"replies":[{"id":135537,"content":"相信大部分人还没有排队拿到 gpt-4 的API\n对于实际应用开发和效果比较来说，gpt-3已经足够惊艳了\nAPI上本身也没有什么分别","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1679969452,"ip_address":"上海","comment_id":371387,"utype":1}],"discussion_count":4,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"现在最新的是gpt4，不应该说gpt4么？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611161,"discussion_content":"相信大部分人还没有排队拿到 gpt-4 的API\n对于实际应用开发和效果比较来说，gpt-3已经足够惊艳了\nAPI上本身也没有什么分别","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1679969452,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1009368,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/66/d8/e0c20492.jpg","nickname":"牛不白","note":"","ucode":"B8DF5D88D9B7A7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611071,"discussion_content":"大部分人都没有 GPT4 的权限.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1679918412,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2354198,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKib5EEL89P7Mpz2ib8QYmUwibiblzeziacHYCeIRCWv43l8E0FPPLtoDibsJ5xO56XSGLBaOFoSIA6dQ5A/132","nickname":"115121","note":"","ucode":"1DDB6FCA0788A5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611012,"discussion_content":"gpt4没开放个人账户应用啊，调用的接口是gpt3或者3.5的吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1679904023,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1182967,"avatar":"https://static001.geekbang.org/account/avatar/00/12/0c/f7/d6547adb.jpg","nickname":"努力努力再努力","note":"","ucode":"F71E16290CB58C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":610993,"discussion_content":"估计是之前的稿子","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1679898283,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371408,"user_name":"小神david","can_delete":false,"product_type":"c1","uid":1206959,"ip_address":"北京","ucode":"F593F5ECEC58BA","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epmAicDUiaUdtLhVwSs6fT0yx69ibWy6ia46ZD4vblGtyee8QFz71icKZJkzccAFG3zHnMngSz7WeGBtKw/132","comment_is_top":false,"comment_ctime":1679906352,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"鉴黄鉴暴","like_count":5},{"had_liked":false,"id":372310,"user_name":"Steven","can_delete":false,"product_type":"c1","uid":1253652,"ip_address":"辽宁","ucode":"3FE64459842015","user_header":"https://static001.geekbang.org/account/avatar/00/13/21/14/423a821f.jpg","comment_is_top":false,"comment_ctime":1680956335,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"Fasttext 效果测试时碰到异常：\nDLL load failed while importing _imaging\n\n更新 pillow 版本，然后重启 Notebook 即可\npip install --upgrade pillow","like_count":2},{"had_liked":false,"id":373820,"user_name":"王尼玛","can_delete":false,"product_type":"c1","uid":3579262,"ip_address":"北京","ucode":"0F48004336502A","user_header":"https://static001.geekbang.org/account/avatar/00/36/9d/7e/83524ccd.jpg","comment_is_top":false,"comment_ctime":1683201202,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"mac 安装pytorch  pip3 install torch torchvision torchaudio  ","like_count":1},{"had_liked":false,"id":372287,"user_name":"Steven","can_delete":false,"product_type":"c1","uid":1253652,"ip_address":"辽宁","ucode":"3FE64459842015","user_header":"https://static001.geekbang.org/account/avatar/00/13/21/14/423a821f.jpg","comment_is_top":false,"comment_ctime":1680943283,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"Win10 下 fasttext 安装（Linux 环境未尝试）：\n1，从 https:&#47;&#47;www.lfd.uci.edu&#47;~gohlke&#47;pythonlibs&#47;#fasttext 下载fasttext‑0.9.2‑cp310‑cp310‑win_amd64.whl 文件\n2，执行 pip install fasttext‑0.9.2‑cp310‑cp310‑win_amd64.whl 安装","like_count":1,"discussions":[{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617843,"discussion_content":"感谢分享，不过 fasttext 好像不需要安装","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683904591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391146,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1717417467,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":2,"score":4,"product_id":100541001,"comment_content":"第4讲打卡","like_count":0},{"had_liked":false,"id":389049,"user_name":"Prometheus","can_delete":false,"product_type":"c1","uid":2881916,"ip_address":"福建","ucode":"C99BF7E0EE8BF8","user_header":"https://static001.geekbang.org/account/avatar/00/2b/f9/7c/75d613e8.jpg","comment_is_top":false,"comment_ctime":1711445281,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"FileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-2-75fd75f5cd90&gt; in &lt;cell line: 4&gt;()\n      2 import numpy as np\n      3 # Load the FastText pre-trained model\n----&gt; 4 model = gensim.models.fasttext.load_facebook_model(&#39;cc.en.300.bin&#39;)\n      5 \n      6 def get_fasttext_vector(line):","like_count":0},{"had_liked":false,"id":381055,"user_name":"胡贺鹏","can_delete":false,"product_type":"c1","uid":2398372,"ip_address":"广东","ucode":"D03369FEEDCA92","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wr3qiaXYeAaLHyPBArO8ThtgMns5icVkv7SWasTbGeic8QsIkAIC2XCG7ibTTibdicLB6WCibjj5b50kcV7rPKxqVW8Dw/132","comment_is_top":false,"comment_ctime":1694615441,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"AttributeError: &#39;DataFrame&#39; object has no attribute &#39;Text&#39; 出现这个报错怎么处理呢？","like_count":0},{"had_liked":false,"id":380098,"user_name":"Geek_d54869","can_delete":false,"product_type":"c1","uid":1953765,"ip_address":"北京","ucode":"CF55462EF65B9E","user_header":"","comment_is_top":false,"comment_ctime":1692957327,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"很想看看bert运行的效果指标 对比一下","like_count":0},{"had_liked":false,"id":377780,"user_name":"Qweasd","can_delete":false,"product_type":"c1","uid":3003302,"ip_address":"广东","ucode":"1157FC1EE1E9B4","user_header":"https://static001.geekbang.org/account/avatar/00/2d/d3/a6/cb3d35fc.jpg","comment_is_top":false,"comment_ctime":1689176365,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"也没有GitHub上的代码","like_count":0},{"had_liked":false,"id":377419,"user_name":"silentyears","can_delete":false,"product_type":"c1","uid":1061748,"ip_address":"北京","ucode":"6E137BFEB874CA","user_header":"https://static001.geekbang.org/account/avatar/00/10/33/74/d9d143fa.jpg","comment_is_top":false,"comment_ctime":1688469938,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"请问老师，所谓的大模型就是指参数多吗？\n预训练模型学习的数据越多，参数会变得越多吗？文中所说的用来训练的语料文本越丰富，模型中可以放的参数越多是什么意思？ 和采用的模型ada, babbage, curie, davinci 决定的参数有什么关系？\n谢谢\n","like_count":0},{"had_liked":false,"id":376659,"user_name":"呼呼","can_delete":false,"product_type":"c1","uid":1038798,"ip_address":"四川","ucode":"5D7B57C05B5D8C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d9/ce/4528cb4b.jpg","comment_is_top":false,"comment_ctime":1687172131,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"我试了下T5-base模型。如果是中文的话，对应国内的好评和差评，得分都是0。是不是T5模型不支持中文？","like_count":0},{"had_liked":false,"id":375705,"user_name":"乀囗囗丿","can_delete":false,"product_type":"c1","uid":3577874,"ip_address":"上海","ucode":"2409C87DB3D7BB","user_header":"https://static001.geekbang.org/account/avatar/00/36/98/12/91d22998.jpg","comment_is_top":false,"comment_ctime":1685884387,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"T5-small的一个异常的解决\n异常：SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: &#47;t5-small&#47;resolve&#47;main&#47;spiece.model (Caused by SSLError(SSLEOFError(8, &#39;EOF occurred in violation of protocol (_ssl.c:997)&#39;)))\n解决如下：\n1、conda install requests==2.27.1\n2、在源代码基础上增加如下代码：\nimport os\nos.environ[&#39;CURL_CA_BUNDLE&#39;] = &#39;&#39;\n参考链接：https:&#47;&#47;stackoverflow.com&#47;questions&#47;75110981&#47;sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce","like_count":0},{"had_liked":false,"id":375243,"user_name":"牧海","can_delete":false,"product_type":"c1","uid":2014489,"ip_address":"广东","ucode":"2152440A1A1D8C","user_header":"https://static001.geekbang.org/account/avatar/00/1e/bd/19/ad8fe585.jpg","comment_is_top":false,"comment_ctime":1685106547,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100541001,"comment_content":"---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[11], line 3\n      1 # %load &#47;Users&#47;zongxi.lzx&#47;gensim.py\n      2 #!&#47;usr&#47;bin&#47;env python3\n----&gt; 3 import gensim\n      4 import numpy as np\n      5 # Load the FastText pre-trained model\n\nFile ~&#47;gensim.py:5\n      3 import numpy as np\n      4 # Load the FastText pre-trained model\n----&gt; 5 model = gensim.models.fasttext.load_facebook_model(&#39;cc.en.300.bin&#39;)\n      7 def get_fasttext_vector(line):\n      8     vec = np.zeros(300) # Initialize an empty 300-dimensional vector\n\nAttributeError: partially initialized module &#39;gensim&#39; has no attribute &#39;models&#39; (most likely due to a circular import)\n\n老师，报这个错是什么原因呢？","like_count":0},{"had_liked":false,"id":374077,"user_name":"Geek_b7449f","can_delete":false,"product_type":"c1","uid":2957535,"ip_address":"新加坡","ucode":"07E7023A3ECD5A","user_header":"https://static001.geekbang.org/account/avatar/00/2d/20/df/55bacf31.jpg","comment_is_top":false,"comment_ctime":1683565401,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100541001,"comment_content":"1、课后题的 Embedding\n              precision    recall  f1-score   support\n\n    negative       0.25      0.99      0.40       136\n    positive       1.00      0.48      0.65       789\n\n    accuracy                           0.56       925\n   macro avg       0.62      0.74      0.52       925\nweighted avg       0.89      0.56      0.61       925\n\n2、个人认为应该属于文章分类，词库分类等场景更适合这种只关注单词，而非词序的。","like_count":0}]}