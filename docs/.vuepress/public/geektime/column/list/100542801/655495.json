{"id":655495,"title":"10｜数据加工：如何将原始数据做成内容画像？","content":"<p>你好，我是黄鸿波。</p><p>在前面的课程中，我们已经能够使用scrapy爬取想要的数据集，下面我们更进一步，把数据集处理成内容画像。这节课我会从内容画像的定义出发，带你了解内容画像的作用，紧接着，我们把原始的数据做成内容画像，直到最基础的画像已经能够正常写入到MongoDB数据库。</p><h2>内容画像的定义与作用</h2><p>从通俗的角度来说，内容画像实际上就是内容的一系列标签，我们在各个维度上给用户打上各种各样的标签，就组成了内容画像。由于内容在各个维度上被打上了不同的标签，因此，我们就可以在不同的维度上对内容进行分类。</p><p>内容的来源一般分成官方、用户和互联网（例如爬虫爬取），不同的来源肯定就会使得内容的形式、质量等都有比较大的区别。</p><p>从标签和分类的角度来讲，我们可以将内容标签呈现出漏斗式。也就是说，从一个大而广的分类到垂直领域，再到细分领域，最后到关键词这个级别。在这个漏斗中，每一个层级都可以作为画像中的一个标签或者一个特征，到实际的模型中再根据需求进行取出，从而进行模型的训练。</p><p><img src=\"https://static001.geekbang.org/resource/image/56/9a/56d6cfbf29e401710056c5706abc039a.jpg?wh=1800x1616\" alt=\"\"></p><p>如果把内容画像平铺开来，实际上我们得到的就应该是一个大的标签库。从这个标签库中随意抓出一个标签，就能找到这个标签所对应的内容的列表。当把标签进行各种组合时，就会产生不同的列表。从理论上来讲，组合的条件越多，所描述和刻画的标签也就越精细，所对应的内容也就更加具体，这对于判断用户的喜好来说是非常重要的。</p><!-- [[[read_end]]] --><p>内容画像是一个推荐系统推荐效果的核心所在，当我们在构建各种推荐算法和模型的时候，需要使用到各种各样的特征，而这些特征一般来讲都是从内容画像中获取到的各个标签，然后经过一系列的处理，得到我们需要的信息，从而进行推荐算法的模型构建和推理。</p><p>推荐系统包含内容画像和用户画像两个大的画像。实际上，用户画像也可以简单理解为内容画像中多个内容的集合。根据前面的讲解我们可以知道，用户画像里一般包含着一系列的用户行为，这些行为中很大一部分就是用户所浏览的内容信息。而一个用户之所以能够点击这些内容，或者能够对这些内容感兴趣，也都是因为这些内容的标签。</p><p>对于用户来讲，这些内容的标签可以是显性的也可以是隐性的。显性指很多产品明确标记了内容标签、关键词或者其他的信息；隐性指有些标签并没有单独以标签的形式标记出来，而是用户根据自己的判断得到的，比如说标题里面的关键词、摘要里面的关键短语等。实际上，这些标签都是真实存在的，而这些标签就组成了一个个的内容画像。</p><p>另外我们可以通过内容画像找到内容之间是否有共同标签，以及内容之间是否有一定的相似性。在实际推荐系统的运行过程中，就可以利用这些相似性给用户推荐相似的文章。</p><h2>把原始的数据做成内容画像</h2><p>下面我们来把原始数据做成内容画像。</p><p>我们可以将原始文本数据粗略分为<strong>非结构化数据</strong>和<strong>结构化数据</strong>。我们在处理不同类型数据时所用的方法略有区别，但最终想要达到的目的都是相同的，那就是提取出它们的标签。</p><p>非结构化数据是指数据结构不规则或者不完整，没有预定义的数据模型。目前大部分的原始数据都是非结构化数据，它广义上包括了文档、文本、图片、音频、视频等等。这节课主要指的是文本信息，更确切地来讲，就是我们从新浪网上爬取的内容数据。</p><p>这种非结构化文本一般包含了关键词提取、命名实体识别、文本分类、Word Embedding等，我给你画了一个表格，里面有这些技术对应的作用，你可以对照进行学习。</p><p><img src=\"https://static001.geekbang.org/resource/image/04/e1/04c2f89a8a88b9cfc9e00bdb312c76e1.jpg?wh=3000x1334\" alt=\"\"></p><p>下面我们把之前使用scrapy爬取的数据制作成内容画像，然后存储到MongoDB数据库中。</p><p>首先我们拿MongoDB数据库中的任意一条数据为例。</p><p><img src=\"https://static001.geekbang.org/resource/image/58/ee/588a4051dc1yy08946cc3b5000f96aee.png?wh=2836x246\" alt=\"\"></p><p>在这条数据中，一共有5个字段，分别是“_id”、“type”、“title”、“times”和“desc”。在这里我们可以发现，除了“_id”这个字段以外，其他的数据都是爬虫爬取回来的。这里“_id”这个字段，实际上就是MongoDB为这条数据建立的一个id，并且这条id在MongoDB数据库中是唯一的。</p><p>想要把我们的内容做成画像，我会从下面几个角度来考虑。</p><ol>\n<li>这个文章的标题包含了什么关键词？</li>\n<li>这个文章的内容包含了什么关键词？</li>\n<li>这个文章有多少个字，是长文还是短文？</li>\n<li>是否需要附上一个初始的热度？</li>\n<li>这个文章的类型是什么？</li>\n</ol><p>当然，我们所需要考虑的不仅仅只有这些，不过前期可以先按照这几个角度来做成一个简单的内容画像。</p><p>一般来讲，内容画像不属于爬虫，它属于推荐系统的一部分。因此，在这里需要创建一个新的项目，用来专门做推荐系统与模型相关的处理，包括数据处理、模型搭建、训练等等。到了后面服务端内容的时候，我们也会再建立一个与服务端相关的项目。</p><p>这里需要注意的是，建立项目的时候，你需要选择一个anaconda的环境，一般来说，我对于anaconda环境的管理原则可以分成以下三个思路。</p><p><strong>按项目。</strong>每建立一个项目就建立一个新的anaconda环境，并且环境名与项目名进行对应或者相关。</p><p><strong>按环境相关性。</strong>所谓的按环境相关性就是指你要把你的常用的一类放在一个环境里，比如说某个环境就是和TensorFlow相关的库，或者某个环境就是和pytorch相关的库，不同的版本建立不同的库，这样的话可以做到环境的隔离，也相对比较通用。</p><p><strong>按项目通用性。</strong>如果项目里面用不到太多太复杂的包，可以建立一个通用的anaconda环境，然后装一些通用的库（比如说numpy、sklearn等），一些简单的机器学习和科学计算的方法都可以使用这个包来做。</p><p>在这个项目里，因为要用到的环境相对比较复杂，所以我就新建立一个环境，将其命名为“recommendation-class”，使用的Python环境为3.7，因此，我们在cmd环境下，输入下面这行代码。</p><pre><code>conda create -n recommendation-class python==3.7\n</code></pre><p>创建完成后，使用下面的命令来进行激活环境。</p><pre><code>activate recommendation-class\n</code></pre><p>这个时候，暂时可以先不用安装需要的库，我建议先在pycharm中创建一个recommendation-class的项目，然后关联我们的环境，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/4e/c46641dc1680cc1ca6dc1e190b79164e.png?wh=1197x970\" alt=\"\"></p><p>当项目创建好之后，就会出现下面的界面。</p><p><img src=\"https://static001.geekbang.org/resource/image/13/21/1367466fcb4855f8a9ae49c349f86d21.png?wh=3840x2101\" alt=\"\"></p><p>现在我们真正进入到推荐系统的代码开发部分。在开发代码之前来先对目录做一个规划，看看我们到底都需要哪些目录。</p><p>我先把最基本的一级目录列出来。</p><p><img src=\"https://static001.geekbang.org/resource/image/e8/21/e882e2babf7aa01d43b520f565258d21.png?wh=525x647\" alt=\"\"></p><p>我认为，一个最基本的推荐系统目录至少要包含表格里的这六个部分。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/99/2ca46d8a9af4fd6d2030e8bc5a5c3b99.jpg?wh=2790x1844\" alt=\"\"></p><p>接下来要做的是制作一个内容画像，我在models目录下建立一个label目录，专门来放各种画像，然后再在lables目录建立一个content_label.py文件，专门用来编写处理与内容画像相关的代码，最后目录结构如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/74/55/74e9eb8529b296248c2807cf5f2f5255.png?wh=526x467\" alt=\"\"></p><p>接下来我们要做的就是在这里面处理内容画像，处理的简单流程如下。</p><ol>\n<li>从MongoDB中获取数据。</li>\n<li>将获取到的数据进行取关键词、字数获取、其他信息获取的操作。</li>\n<li>给内容画像设置一个默认的点赞、收藏、阅读的数量。</li>\n<li>设置一个默认热度，以后可以做热度改变。</li>\n<li>将这些内容做成画像，插入到MongoDB数据库，数据库的collection为“content_labels”。</li>\n</ol><p>下面我们就来一步一步实现它。</p><p>首先，从MongoDB中导入数据，建立数据库连接。建立数据库连接的代码实际上和scrapy里面连接MongoDB的代码是一套，所以可以直接把scrapy项目中dao目录下的mongo_db.py文件直接拿过来，然后根据目前的项目，稍微做一下改动，代码如下。</p><pre><code>import pymongo\n \nclass MongoDB(object):\n    def __init__(self, db):\n        mongo_client = self._connect('localhost', 27017, '', '', db)\n        self.db_scrapy = mongo_client['scrapy_data']\n        self.db_recommendation = mongo_client['recommendation']\n \n    def _connect(self, host, port, user, pwd, db):\n        mongo_info = self._splicing(host, port, user, pwd, db)\n        mongo_client = pymongo.MongoClient(mongo_info, connectTimeoutMS=12000, connect=False)\n        return mongo_client\n \n    @staticmethod\n    def _splicing(host, port, user, pwd, db):\n        client = 'mongodb://' + host + &quot;:&quot; + str(port) + &quot;/&quot;\n        if user != '':\n            client = 'mongodb://' + user + ':' + pwd + '@' + host + &quot;:&quot; + str(port) + &quot;/&quot;\n            if db != '':\n                client += db\n        return client\n</code></pre><p>可以看到，在这里我只是做了一个非常小的改动，就是在最上边的数据库中添加了一个“recommendation”。后面所有在推荐系统中涉及MongoDB数据库的操作，都会基于“recommendation”这个库来操作。但是我仍然保留了“scrapy_data”这个库，因为读取原始数据还是要基于这个库来读取。</p><p>有了这个MongoDB连接类，就可以将其导入到content_label.py文件中，然后直接在我们的content_label.py文件中使用。</p><p>接下来，我们来看content_label.py这个文件的代码。我们先来做一个简单的内容画像，其画像的字段全部来自已知数据。</p><pre><code>from dao.mongo_db import MongoDB\nfrom datetime import datetime\n\nclass ContentLabel(object):\n   def __init__(self):\n       self.mongo_scrapy = MongoDB(db='scrapy_data')\n       self.mongo_recommendation = MongoDB(db='recommendation')\n       self.scrapy_collection = self.mongo_scrapy.db_scrapy['content_ori']\n       self.content_label_collection = self.mongo_recommendation.db_recommendation['content_label']\n\n   def get_data_from_mongodb(self):\n       datas = self.scrapy_collection.find()\n       return datas\n   \n   def make_content_labels(self):\n       datas = self.get_data_from_mongodb()\n       for data in datas:\n           content_collection = dict()\n           content_collection['describe'] = data['desc']\n           content_collection['type'] = data['type']\n           content_collection['title'] = data['title']\n           content_collection['news_date'] = data['times']\n           content_collection['hot_heat'] = 10000\n           content_collection['likes'] = 0\n           content_collection['read'] = 0\n           content_collection['collections'] = 0\n           content_collection['create_time'] = datetime.utcnow()\n           print(content_collection)\n\nif __name__ == '__main__':\n   content = ContentLabel()\n   content.make_content_labels()\n</code></pre><p>这是一个很简单的画像的例子，当然，目前我还没有把它们插入到MongoDB数据库中，仅仅是使用print()函数将它们打印了出来。</p><p>我们来稍微解析一下这段代码。在这段代码中首先在上面引入了MongoDB的连接类，又引入了一个datetime库，你可以利用这个库来获取到当前时间并赋值到创建时间这个字段中。</p><pre><code>content_collection['create_time'] = datetime.utcnow()\n</code></pre><p>在__init__()函数的定义中，我们主要是定义了与数据库连接相关的变量，主要有MongoDB连接scrapy库、MongoDB连接recommendation库（注意，这个库目前数据库里还没有，需要在数据库手动创建），以及对应的两个collection，分别是“scrapy_data”库的“content_ori”和“recommendation”库的“content_label”（这个collection目前也没有，当运行程序后会自动创建）。</p><pre><code>self.scrapy_collection = self.mongo_scrapy.db_scrapy['content_ori']\n       self.content_label_collection = self.mongo_recommendation.db_recommendation['content_label']\n</code></pre><p>紧接着，我们创建了一个名为“get_data_from_mongodb”的函数，这个函数主要是从scrapy库中获取到原始数据，然后将原始数据进行返回。</p><pre><code>   def get_data_from_mongodb(self):\n       datas = self.scrapy_collection.find()\n       return datas\n</code></pre><p>你可以看到，在这个函数里目前虽然只有一行数据，但是我还是单独给作为一个函数拿了出来。这样做的好处是能够很好地进行解耦，这个函数就是用作获取原始数据，职责单一。</p><p>当然，因为目前数据量比较少，所以我是直接获取了所有的数据。但是当数据量非常大的时候（比如有几万或者几十万甚至上百万数据），必须采用分页的形式来获取数据，这样可以减少数据库的开销以及程序卡顿。</p><p>然后到了内容画像中最重要的一个函数：make_content_labels()函数。我们会在这个函数中进行数据的组装，并把组装后的数据作为内容画像存储在MongoDB数据库中。</p><p>MongoDB所使用的数据格式叫BSON，BSON是一种类似于JSON的数据类型。在Python中，Dict类型和JSON类型可以相互转换。因此我们在这里新建了一个Dict()类型的变量，名为“content_collection”，然后再将所需要的内容给填充进去。我们首先从原始数据中获取到基本的信息，例如内容、类型、标题和新闻时间，然后放入到字典中。</p><pre><code>   def make_content_labels(self):\n       datas = self.get_data_from_mongodb()\n       for data in datas:\n           content_collection = dict()\n           content_collection['describe'] = data['desc']\n           content_collection['type'] = data['type']\n           content_collection['title'] = data['title']\n           content_collection['news_date'] = data['times']\n</code></pre><p>接着，我又新建了一些初始化的特征。例如在这里我设定了初始化的热度为10000，这是为了以后可以通过热度值来进行排序。然后我又新建了点赞、阅读、收藏这三个值，初始值为0。最后我又定义了一个创建时间作为这条数据的创建时间，它可以用作后面一些模型和算法的特征。</p><pre><code>           content_collection['hot_heat'] = 10000\n           content_collection['likes'] = 0\n           content_collection['read'] = 0\n           content_collection['collections'] = 0\n</code></pre><p>然后，我使用print先将数据打印出来看一下，得到如下结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/9a/53/9a2yybf687b001ac6640c65bbf2d0e53.png?wh=3811x1189\" alt=\"\"></p><p>如果出现这个结果，至少说明我们的程序目前是可以正常跑通的。接下来，我们就可以再向里面加点东西。</p><p>我们可以往里面再加入一个统计字数的功能。在Python中，字数可以使用正则化加上Unicode编码的方式来进行统计。我们知道，实际上汉字也是Unicode编码中的一部分。在Unicode编码中，常用汉字的编码是从4E00到9FA5，因此将这一部分的内容统计出来即可。你可以在程序中引入re库来进行正则表达式的统计，这段函数可以写成下面的形式。</p><pre><code>def get_words_nums(self, contents):\n\tch = re.findall('([\\u4e00-\\u9fa5])', contents)\n\tnum = len(ch)\n\treturn num\n</code></pre><p>在这段函数中传入的是文本内容，也就是我们爬取到的文章正文，然后使用re.findall()函数来取得内容中Unicode编码在4E00到9FA5之间的文字。这个时候会得到一个list列表，列表里面只包含常用的中文文字，之后用len()函数对这些文字的数量进行统计，就得到了需要的中文字数。</p><p>然后，我们在make_content_labels()中加入它的函数调用即可。</p><pre><code>content_collection['keywords'] = keywords\n</code></pre><p>现在我们可以加上MongoDB插入的部分，看看能不能正常写入到数据库中。这一步其实非常简单，只需要在make_content_labels()函数的最后加入下面这行代码即可。</p><pre><code>self.content_label_collection.insert_one(content_collection)\n</code></pre><p>我们来运行一下这段程序，运行后刷新一下MongoDB数据库，就得到了如下界面。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/43/d75b9116415cf68bcb75b4d9bf5dc043.png?wh=3837x1976\" alt=\"\"></p><p>到这里就说明最基础的画像已经能够正常写入到MongoDB数据库了，在接下来的课程中我们会继续完善这个内容画像，并将其作为模型和算法的特征。</p><p>实际上这里还有一个非常关键的特征没有加上，那就是关键词。也就是提取文章和标题中的关键词，然后再加入到我们的画像中作为画像的一部分。这一部分的内容我想留作一个作业让你课后去完成，我会在GitHub里公布我们的代码和思路。</p><h2>总结</h2><p>我们来总结一下这节课的内容，学完这节课，希望你能够记住以下要点。</p><ol>\n<li>知道内容画像是什么，它在推荐系统中有着举足轻重的地位。</li>\n<li>了解非结构化文本内容画像的生成处理方式，比如文本分类、文本聚类、关键词提取等等。</li>\n<li>熟悉如何使用Python配合MongoDB来做一个简单的内容画像。</li>\n</ol><h2>课后题</h2><p>最后依旧是课后题环节，给你留了两道课后题。</p><ol>\n<li>完成并理解课程中的代码，再想想还可以挖掘哪些特征并找一个特征加入到的内容画像中。</li>\n<li>在原有代码的基础上加入关键词的特征，关键词提取限定使用TF-IDF或者TextRank，也可以两者结合使用。</li>\n</ol>","comments":[{"had_liked":false,"id":374036,"user_name":"Geek_ccc0fd","can_delete":false,"product_type":"c1","uid":1461544,"ip_address":"广东","ucode":"DB53D576AEC020","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","comment_is_top":false,"comment_ctime":1683527608,"is_pvip":false,"replies":[{"id":136921,"content":"特征穿越问题主要是因为在训练模型时，使用了一些在未来才能得到的标签或特征，导致模型在实际使用时表现不佳。在用户画像中，由于画像数据是最新的，而训练样本是过去一段时间的数据，因此会遇到特征穿越问题。\n解决这个问题的方法主要有以下几种：\n1. 对历史数据进行特征工程，将历史数据中的一些特征转化为对未来数据有预测能力的特征。这样就可以使用历史数据中的特征来预测未来的画像标签，避免了特征穿越问题。\n2. 将画像标签作为新的训练样本特征，同时使用前面的历史数据特征作为输入，来训练模型。这种方法可以在模型中加入画像标签的影响，提高模型的预测准确性。同时，还可以使用 rolling window 等方法，避免将未来数据引入模型中。\n3. 对数据进行时间切片，将过去一段时间的数据作为一个时间窗口，来训练一个对应的模型。然后使用该模型来预测下一个时间窗口的画像标签。这样就可以保证模型只使用过去的数据，不受未来数据的影响。\n综上，面对特征穿越问题，我们需要针对具体场景来选择合适的解决方案。需要考虑的因素包括数据量、数据质量、特征工程复杂度、计算资源等等。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685035130,"ip_address":"广东","comment_id":374036,"utype":1}],"discussion_count":5,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"关于画像有个问题想请教一下老师：\n我们训练样本一般是过去一段时间的数据，但是画像数据保存的最新的画像标签，这里如果直接使用样本关联画像标签的话会发生特征穿越问题，这里实际工作中是如何处理的呢？","like_count":2,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619388,"discussion_content":"特征穿越问题主要是因为在训练模型时，使用了一些在未来才能得到的标签或特征，导致模型在实际使用时表现不佳。在用户画像中，由于画像数据是最新的，而训练样本是过去一段时间的数据，因此会遇到特征穿越问题。\n解决这个问题的方法主要有以下几种：\n1. 对历史数据进行特征工程，将历史数据中的一些特征转化为对未来数据有预测能力的特征。这样就可以使用历史数据中的特征来预测未来的画像标签，避免了特征穿越问题。\n2. 将画像标签作为新的训练样本特征，同时使用前面的历史数据特征作为输入，来训练模型。这种方法可以在模型中加入画像标签的影响，提高模型的预测准确性。同时，还可以使用 rolling window 等方法，避免将未来数据引入模型中。\n3. 对数据进行时间切片，将过去一段时间的数据作为一个时间窗口，来训练一个对应的模型。然后使用该模型来预测下一个时间窗口的画像标签。这样就可以保证模型只使用过去的数据，不受未来数据的影响。\n综上，面对特征穿越问题，我们需要针对具体场景来选择合适的解决方案。需要考虑的因素包括数据量、数据质量、特征工程复杂度、计算资源等等。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685035131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1187130,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","nickname":"未来已来","note":"","ucode":"3A21ACFD53CB9C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617625,"discussion_content":"画像分长期、短期、实时的，长期是多久看经验","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683719302,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1461544,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","nickname":"Geek_ccc0fd","note":"","ucode":"DB53D576AEC020","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1187130,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","nickname":"未来已来","note":"","ucode":"3A21ACFD53CB9C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617769,"discussion_content":"还是没有理解，有没有参考资料，非常感谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683856741,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":617625,"ip_address":"广东","group_id":0},"score":617769,"extra":""}]},{"author":{"id":1187130,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","nickname":"未来已来","note":"","ucode":"3A21ACFD53CB9C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617550,"discussion_content":"把画像落到 hive，用时间字段做分区","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683643130,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1461544,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","nickname":"Geek_ccc0fd","note":"","ucode":"DB53D576AEC020","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1187130,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","nickname":"未来已来","note":"","ucode":"3A21ACFD53CB9C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617576,"discussion_content":"每天做一份全量画像嘛？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683684080,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":617550,"ip_address":"广东","group_id":0},"score":617576,"extra":""}]}]},{"had_liked":false,"id":374038,"user_name":"Geek_ccc0fd","can_delete":false,"product_type":"c1","uid":1461544,"ip_address":"广东","ucode":"DB53D576AEC020","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","comment_is_top":false,"comment_ctime":1683529875,"is_pvip":false,"replies":[{"id":136919,"content":"这个错在哪里呢，我测试过了，没有什么问题。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685034874,"ip_address":"广东","comment_id":374038,"utype":1}],"discussion_count":5,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"统计字数那里赋的代码是不是搞错了","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619386,"discussion_content":"这个错在哪里呢，我测试过了，没有什么问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685034874,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":3,"child_discussions":[{"author":{"id":1461544,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","nickname":"Geek_ccc0fd","note":"","ucode":"DB53D576AEC020","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":619410,"discussion_content":"我的理解是调用统计字数的方法方法赋值，eg:content_collection[&#39;words_num&#39;] = self.get_words_nums(data[&#39;desc&#39;])，但是老师给的代码是赋值keywords，content_collection[&#39;keywords&#39;] = keywords，感觉有点断掉了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1685073118,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":619386,"ip_address":"广东","group_id":0},"score":619410,"extra":""},{"author":{"id":1112019,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/d3/2bbc62b2.jpg","nickname":"alexliu","note":"","ucode":"DD65983BBC9CD4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":619906,"discussion_content":"最后的截图里面也没有keywords关键字作为key值，应该是words_num","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685609528,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":619386,"ip_address":"上海","group_id":0},"score":619906,"extra":""},{"author":{"id":1258320,"avatar":"https://static001.geekbang.org/account/avatar/00/13/33/50/ddbf536c.jpg","nickname":"虎","note":"","ucode":"F723B951D7EDCF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":620739,"discussion_content":"keywords 在哪里？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1686492802,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":619386,"ip_address":"加拿大","group_id":0},"score":620739,"extra":""}]},{"author":{"id":1187130,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","nickname":"未来已来","note":"","ucode":"3A21ACFD53CB9C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617552,"discussion_content":"需要加下：keywords = self.get_words_nums(data[&#39;desc&#39;])","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1683643232,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383274,"user_name":"Abigail","can_delete":false,"product_type":"c1","uid":2604437,"ip_address":"澳大利亚","ucode":"ABB59CE5209E67","user_header":"https://static001.geekbang.org/account/avatar/00/27/bd/95/882bd4e0.jpg","comment_is_top":false,"comment_ctime":1698802797,"is_pvip":false,"replies":[{"id":139999,"content":"是的，这个是下载robo 3T的网址","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699859979,"ip_address":"广东","comment_id":383274,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"Robo 3T is now Studio 3T\n\nhttps:&#47;&#47;studio3t.com&#47;download&#47; ","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631633,"discussion_content":"是的，这个是下载robo 3T的网址","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699859979,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378896,"user_name":"MWF","can_delete":false,"product_type":"c1","uid":2846589,"ip_address":"广东","ucode":"2436883403543A","user_header":"https://static001.geekbang.org/account/avatar/00/2b/6f/7d/3abf607b.jpg","comment_is_top":false,"comment_ctime":1690947475,"is_pvip":false,"replies":[{"id":140003,"content":"https:&#47;&#47;github.com&#47;ipeaking&#47;recommendation\n\nhttps:&#47;&#47;github.com&#47;ipeaking&#47;scrapy_sina","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860237,"ip_address":"广东","comment_id":378896,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"请问github地址是什么？","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631637,"discussion_content":"https://github.com/ipeaking/recommendation\n\nhttps://github.com/ipeaking/scrapy_sina","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860237,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378888,"user_name":"MWF","can_delete":false,"product_type":"c1","uid":2846589,"ip_address":"广东","ucode":"2436883403543A","user_header":"https://static001.geekbang.org/account/avatar/00/2b/6f/7d/3abf607b.jpg","comment_is_top":false,"comment_ctime":1690942349,"is_pvip":false,"replies":[{"id":140019,"content":"https:&#47;&#47;github.com&#47;ipeaking&#47;recommendation\n\nhttps:&#47;&#47;github.com&#47;ipeaking&#47;scrapy_sina","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860442,"ip_address":"广东","comment_id":378888,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"请问能否把每一讲的代码（包括网络上爬取到的数据）都上传到github供大家下载呢，因为不是每个人都会从头到尾跟进每一节，比如我主要想学习画像部分的内容，那么没安装数据库以及爬虫相关插件，就无法得到数据进行后面的内容了。","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631653,"discussion_content":"https://github.com/ipeaking/recommendation\n\nhttps://github.com/ipeaking/scrapy_sina","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860442,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374207,"user_name":"GhostGuest","can_delete":false,"product_type":"c1","uid":2871611,"ip_address":"上海","ucode":"A6785E3304276B","user_header":"https://static001.geekbang.org/account/avatar/00/2b/d1/3b/a94459d2.jpg","comment_is_top":false,"comment_ctime":1683709175,"is_pvip":false,"replies":[{"id":136683,"content":"同学你好，谢谢你的指正，已修改。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683879208,"ip_address":"广东","comment_id":374207,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"文稿中热度设置错了，代码写的一万，文稿写的一千","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617805,"discussion_content":"同学你好，谢谢你的指正，已修改。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683879209,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374061,"user_name":"翡翠虎","can_delete":false,"product_type":"c1","uid":1448015,"ip_address":"广西","ucode":"2572E93C4C57A5","user_header":"https://static001.geekbang.org/account/avatar/00/16/18/4f/9e4d5591.jpg","comment_is_top":false,"comment_ctime":1683544450,"is_pvip":false,"replies":[{"id":136682,"content":"是的，这里只是举了个例子，实际上可以有更多特征。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683879191,"ip_address":"广东","comment_id":374061,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"除了关键词外，我感觉文章类型（文本分类）、国家地区也可以作为特征之一","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617804,"discussion_content":"是的，这里只是举了个例子，实际上可以有更多特征。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683879191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":396469,"user_name":"Geek_bc9832","can_delete":false,"product_type":"c1","uid":2948431,"ip_address":"北京","ucode":"30D50E13AB2DE6","user_header":"","comment_is_top":false,"comment_ctime":1734590483,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":2,"product_id":100542801,"comment_content":"文章中words_num的代码好像写错了，写成keywords了","like_count":0},{"had_liked":false,"id":386211,"user_name":"moonfeeling","can_delete":false,"product_type":"c1","uid":1158550,"ip_address":"河南","ucode":"85A2EAFF2ABA36","user_header":"https://static001.geekbang.org/account/avatar/00/11/ad/96/249d1643.jpg","comment_is_top":false,"comment_ctime":1704338360,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"关键词提取可参考这篇文章实现：https:&#47;&#47;mp.weixin.qq.com&#47;s&#47;Vd58Hxiocx9BkcKvnGS7ng","like_count":0},{"had_liked":false,"id":386201,"user_name":"moonfeeling","can_delete":false,"product_type":"c1","uid":1158550,"ip_address":"河南","ucode":"85A2EAFF2ABA36","user_header":"https://static001.geekbang.org/account/avatar/00/11/ad/96/249d1643.jpg","comment_is_top":false,"comment_ctime":1704329438,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师好，请问下文章和标题中的关键词提取部分的代码在哪里找呢？您给的github中源码好像没有","like_count":0},{"had_liked":false,"id":385264,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1702365365,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100542801,"comment_content":"def get_words_nums(self, contents):\n  ch = re.findall(&#39;([\\u4e00-\\u9fa5])&#39;, contents)\n  num = len(ch)\n  return num\n\n这个函数的入参应该是 data[&#39;desc&#39;] ，即文章正文，具体代码如下：\nword_nums = self.get_words_nums(data[&#39;desc&#39;])\ncontent_collection[&#39;word_num&#39;] = word_nums","like_count":0},{"had_liked":false,"id":374151,"user_name":"Geek_ea1710","can_delete":false,"product_type":"c1","uid":3241284,"ip_address":"陕西","ucode":"AE93FAF7C96B8D","user_header":"","comment_is_top":false,"comment_ctime":1683644720,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100542801,"comment_content":"已看","like_count":0}]}