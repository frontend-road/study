{"id":731297,"title":"导读序 奇事·奇人·奇书","content":"<h2 id=\"nav_point_2\">奇事</h2><p>本书的主题——ChatGPT 可谓奇事。</p><p>从 2022 年 11 月发布到现在半年多的时间，ChatGPT 所引起的关注、产生的影响，可能已经超越了信息技术历史上的几乎所有热点。</p><p>它的用户数 2 天达到 100 万，2 个月达到 1 亿，打破了 TikTok 之前的纪录。而在 2023 年 5 月它上架苹果应用商店后，也毫无悬念地冲上了免费 App 榜榜首。</p><p>许多人平生第一次接触如此高智能、知错能改的对话系统。虽然它很多时候会非常自信、“一本正经地胡说八道”，甚至连简单的加减法也算不对，但如果你提示它错了，或者让它一步步地来，它就会很“灵”地变得非常靠谱，有条不紊地列出做事情的步骤，然后得出正确答案。对于一些复杂的任务，你正等着看它的笑话呢，它却会不紧不慢地给你言之成理的回答，让你大吃一惊。</p><p>众多业界专家也被它征服：原本不看好它甚至在 2019 年微软投资 OpenAI 的决策中投了反对票的盖茨，现在将 ChatGPT 与 PC、互联网等相提并论；英伟达 CEO 黄仁勋称它带来了 AI 的“iPhone 时刻”；OpenAI 的山姆·阿尔特曼（Sam Altman）将它比作印刷机；谷歌 CEO 孙达尔·皮柴（Sundar Pichai）说它是“火和电”……这些说法都与腾讯创始人马化腾认为 ChatGPT“几百年不遇”的观点不谋而合，总之都是说它开启了新时代。阿里巴巴 CEO 张勇的看法是：“所有行业、应用、软件、服务，都值得基于大模型能力重做一遍。”以马斯克为代表的很多专家更是因为 ChatGPT 的突破性能力可能对人类产生威胁，呼吁应该暂停强大 AI 模型的开发。</p><!-- [[[read_end]]] --><p>在刚刚结束的 2023 智源大会上，山姆·阿尔特曼很自信地说 AGI（artificial general intelligence，通用人工智能）很可能在十年之内到来，需要全球合作解决由此带来的各种问题。而因为共同推动深度学习从边缘到舞台中央而获得图灵奖的三位科学家，意见却明显不同：</p><ul> <li class=\"第1级无序列表\">杨立昆（Yann LeCun）明确表示 GPT 代表的自回归大模型存在本质缺陷，需要围绕世界模型另寻新路，所以他对 AI 的威胁并不担心；</li> <li class=\"第1级无序列表\">约书亚·本吉奥（Yoshua Bengio）虽然也不认同单靠 GPT 路线就能通向 AGI（他看好将贝叶斯推理与神经网络结合），但承认大模型存在巨大潜力，从第一性原理来看也没有明显的天花板，因此他在呼吁暂停 AI 开发的公开信上签了字；</li> <li class=\"第1级无序列表\">压轴演讲的杰弗里·辛顿（Geoffrey Hinton）显然同意自己的弟子伊尔亚·苏茨克维（Ilya Sutskever）提出的“大模型能学到真实世界的压缩表示”的观点，他意识到具备反向传播机制（通俗地说就是内置“知错能改”机制）而且能轻易扩大规模的人工神经网络的智能可能会很快超过人类，因此他也加入到呼吁抵御 AI 风险的队伍中来。</li> </ul><p>以 ChatGPT 为代表的人工神经网络的逆袭之旅，在整个科技史上也算得上跌宕起伏。它曾经在流派众多的人工智能界内部屡受歧视和打击。不止一位天才先驱以悲剧结束一生：1943 年，沃尔特·皮茨（Walter Pitts）在与沃伦·麦卡洛克（Warren McCulloch）共同提出神经网络的数学表示时才 20 岁，后来因为与导师维纳失和而脱离学术界，最终因饮酒过度于 46 岁辞世；1958 年，30 岁的弗兰克·罗森布拉特（Frank Rosenblatt）通过感知机实际实现了神经网络，而 1971 年，他在 43 岁生日那天溺水身亡；反向传播的主要提出者大卫·鲁梅尔哈特（David Rumelhart）则正值盛年（50 多岁）就罹患了罕见的不治之症，1998 年开始逐渐失智，最终在与病魔斗争十多年后离世……</p><p>一些顶级会议以及明斯基这样的学术巨人都曾毫不客气地反对甚至排斥神经网络，逼得辛顿等人不得不先后采用“关联记忆”“并行分布式处理”“卷积网络”“深度学习”等中性或者晦涩的术语为自己赢得一隅生存空间。</p><p>辛顿自己从 20 世纪 70 年代开始，坚守冷门方向几十年。从英国到美国，最后立足曾经的学术边陲加拿大，他在资金支持匮乏的情况下努力建立起一个人数不多但精英辈出的学派。</p><p>直到 2012 年，他的博士生伊尔亚·苏茨克维等在 ImageNet 比赛中用新方法一飞冲天，深度学习才开始成为 AI 的显学，并广泛应用于各个产业。2020 年，他又在 OpenAI 带队，通过千亿参数的 GPT-3 开启了大模型时代。</p><p>ChatGPT 自己的身世也极富有戏剧性。</p><p>2015 年，30 岁的山姆·阿尔特曼和 28 岁的格雷格·布罗克曼（Greg Brockman）与马斯克联手，召集了 30 岁的苏茨克维等多位 AI 顶级人才，共同创立 OpenAI，希望在谷歌、Facebook 等诸多巨头之外，建立中立的 AI 前沿科研阵地，并且雄心勃勃地把构建与人类水平相当的人工智能作为自己的目标。那时候，媒体报道基本上都以马斯克支持成立了一家非营利 AI 机构为标题，并没有多少人看好 OpenAI。甚至连苏茨克维这样的灵魂人物，在加入前也经历了一番思想斗争。</p><p>前三年，他们在强化学习、机器人、多智能体、AI 安全等方面多线出击，的确没有取得特别有说服力的成果。以至于主要赞助人马斯克对进展不满意，动了要来直接管理的念头。在被理事会拒绝后，他选择了离开。</p><p>2019 年 3 月，山姆·阿尔特曼开始担任 OpenAI 的 CEO，并在几个月内完成了组建商业公司、获得微软 10 亿美元投资等动作，为后续发展做好了准备。</p><p>在科研方面，2014 年，富兰克林·欧林工程学院本科毕业两年的亚历克·拉德福德（Alec Radford）加入 OpenAI，开始发力。作为主要作者，他在苏茨克维等的指导下，连续完成了 PPO（2017）、GPT-1（2018）、GPT-2（2019）、Jukebox（2020）、ImageGPT（2020）、CLIP（2021）和 Whisper（2022）等多项开创性工作。尤其是 2017 年关于情感神经元的工作，开创了“预测下一个字符”的极简架构结合大模型、大算力、大数据的技术路线，对后续的 GPT 产生了关键影响。</p><p>GPT 的发展也不是一帆风顺的。</p><p>从下页图中可以清晰地看到，GPT-1 的论文发表之后，OpenAI 这种有意为之的更加简单的 Eecoder-Only 架构（准确地讲是带自回归的 Encoder-Decoder 架构）并没有得到太多关注，风头都被谷歌几个月之后发布的 BERT（Encoder-Only 架构，准确地讲是 Encoder-非自回归的 Decoder 架构）抢去了。随后，出现了一系列 <i>xx</i>BERT 类的很有影响的工作。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00052.jpeg\" alt=\"{%}\"></p><p><strong>（大模型进化树，出自 Amazon 杨靖锋等 2023 年 4 月的论文“Harnessing the Power of LLMs in Practice”）</strong></p><p>今天，BERT 论文的引用数已经超过 6.8 万，比 GPT-1 论文的不到 6000 仍然高了一个数量级。两篇论文的技术路线不同，无论是学术界还是工业界，几乎所有人当时都选择了 BERT 阵营。</p><p>2019 年 2 月发布的 GPT-2 将最大参数规模提升到 15 亿级别，同时使用了更大规模、更高质量和更多样的数据，模型开始展现很强的通用能力。当时令 GPT-2 登上技术社区头条的，还不是研究本身（直到今天，它的论文引用数也只有 6000 出头，远不如 BERT），而是 OpenAI 出于安全考虑，一开始只开源了最小的 3.45 亿参数模型，引起轩然大波。社区对 OpenAI 不“Open”的印象，就始自这里。</p><p>这前后，OpenAI 还做了规模对语言模型能力影响的研究，提出了“规模法则”（scaling law），确定了整个组织的主要方向：大模型。为此，OpenAI 将强化学习、机器人等其他方向都砍掉了。难能可贵的是，大部分核心研发人员选择了留下。他们改变自己的研究方向，放弃小我，集中力量做大事——很多人转而做工程和数据等方面的工作，或者围绕大模型重新定位自己的研究方向（比如强化学习就在 GPT 3.5 以及之后的演进中发挥了重大作用）。这种组织上的灵活性，也是 OpenAI 能成功的重要因素。</p><p>2020 年，GPT-3 横空出世，NLP（natural language processing，自然语言处理）小圈子里的一些有识之士开始意识到 OpenAI 技术路线的巨大潜力。在中国，北京智源人工智能研究院联合清华大学等高校推出了 GLM、CPM 等模型，并积极在国内学术界推广大模型理念。从上页关于大模型进化树的图中可以看到，2021 年之后，GPT 路线已经完全占据上风，而 BERT 这一“物种”的进化树几乎停止了。</p><p>2020 年底，OpenAI 的两位副总达里奥·阿莫迪（Dario Amodei）和丹妮拉·阿莫迪（Daniela Amodei）（同时也是兄妹）带领 GPT-3 和安全团队的多位同事离开，创办了 Anthropic。达里奥·阿莫迪在 OpenAI 的地位非同一般：他是伊尔亚·苏茨克维之外，技术路线图的另一个制定者，也是 GPT-2 和 GPT-3 项目以及安全方向的总负责人。而随他离开的，有 GPT-3 和规模法则论文的多位核心人员。</p><p>一年后，Anthropic 发表论文“A General Language Assistant as a Laboratory for Alignment”，开始用聊天助手研究对齐问题，此后逐渐演变为 Claude 这个智能聊天产品。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00053.jpeg\" alt=\"{%}\"></p><p>2022 年 6 月，论文“Emergent Abilities of Large Language Models”发布，第一作者是仅从达特茅斯学院本科毕业两年的谷歌研究员 Jason Wei（今年 2 月，他在谷歌的“精英跳槽潮”中去了 OpenAI）。他在论文中研究了大模型的涌现能力，这类能力在小模型中不存在，只有模型规模扩大到一定量级才会出现——也就是我们熟悉的“量变会导致质变”。</p><p>当年 11 月中旬，本来一直在研发 GPT-4 的 OpenAI 员工收到管理层的指令：所有工作暂停，全力推出一款聊天工具，原因是有竞争。两周后，ChatGPT 诞生。这之后的事情已经载入史册。</p><p>业界推测，OpenAI 管理层应该是得知了 Anthropic Claude 的进展，意识到这一产品的巨大潜力，决定先下手为强。这展现出核心人员超强的战略判断力。要知道，即使是 ChatGPT 的核心研发人员也不知道为什么该产品推出后会这么火（“我爸妈终于知道我在干什么了”），他们在自己试用时完全没有惊艳的感觉。</p><p>2023 年 3 月，在长达半年的“评估、对抗性测试和对模型及系统级缓解措施的迭代改进”之后，GPT-4 发布。微软研究院对其内部版本（能力超出公开发布的线上版本）研究的结论是：“在所有这些任务中，GPT-4 的表现与人类水平接近得惊人……鉴于 GPT-4 的广度和深度，我们认为它可以合理地被视为 AGI 系统早期（但仍然不完整）的版本。”</p><p>此后，国内外的企业和科研机构纷纷跟进，几乎每周就有一个甚至多个新模型推出。但在综合能力上，OpenAI 仍然一骑绝尘，唯一可以与之抗衡的，是 Anthropic。</p><p>很多人会问：为什么中国没有产生 ChatGPT？其实正确的问题（prompt）应该是：为什么全世界只有 OpenAI 能做出 ChatGPT？他们成功的原因是什么？</p><p>对此的思考，到今天仍有意义。</p><p>ChatGPT，真奇事也。</p><h2 id=\"nav_point_3\">奇人</h2><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00054.jpeg\" alt=\"{%}\"></p><p>本书作者斯蒂芬·沃尔弗拉姆（Stephen Wolfram）可谓奇人。</p><p>他虽然并不是马斯克那种在大众层面妇孺皆知的科技名人，但在科技极客小圈子里名气很大，被称为“在世的最聪明的人”。谷歌的创始人之一谢尔盖·布林（Sergey Brin）在大学期间曾经慕名到沃尔弗拉姆的公司实习。而搜狗和百川智能的创始人王小川更是他有名的铁杆粉丝，“带着崇敬和狂热的心……关注和追随多年”。</p><p>沃尔弗拉姆小时候是出名的神童。因为他不屑于看学校推荐的“蠢书”，而且算术不好，所以一开始老师们还以为“这孩子不行”。</p><p>结果人家 13 岁就自己写了几本物理书，其中之一名为《亚原子粒子物理》。</p><p>他 15 岁在 <em>Australian Journal of Physics</em> 上发表了一篇正儿八经的高能物理论文“Hadronic Electrons?”，提出了一种新形式的高能电子–强子耦合。这篇论文还被引用了 5 次。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00055.jpeg\" alt=\"{%}\"></p><p>在英国的伊顿公学、牛津大学等名校，沃尔弗拉姆都是“晃”了几年，也不怎么上课。他厌恶已经被人解决的问题，结果没毕业就“跑”了。最后，20 岁的他在美国加州理工学院直接拿了博士学位，导师是大名鼎鼎的费曼。随后他留校，成为加州理工学院的教授。</p><p>1981 年．沃尔弗拉姆荣获第一届麦克阿瑟奖（俗称“天才奖”），是最年轻的获奖者。同一批获奖的其他人都是各学科的大家，包括 1992 年诺贝尔文学奖得主沃尔科特。</p><p>他很快对纯物理失去了兴趣。1983 年，他转到普林斯顿高等研究院，开始研究元胞自动机，希望找到更多自然和社会现象的底层规律。这一转型产生了巨大影响。他成为复杂系统这一学科的开创者之一，有人认为他做出了诺贝尔奖级的工作。20 多岁的他也的确与诺贝尔奖得主盖尔曼、菲利普·安德森（正是他在 1972 年发表的文章“More is Different”中提出了 emergency，即涌现这一概念）等一起参与了圣塔菲研究所的早期工作，并在 UIUC 创立了复杂系统研究中心。他还创办了学术期刊 <em>Complex Systems</em>。</p><p>为了更方便地做与元胞自动机相关的计算机实验，他开发了数学软件 Mathematica（这个名字是他的好友乔布斯取的），进而创办了软件公司 Wolfram Research，转身成为一名成功的企业家。Mathematica 软件的强大，可以从本书后面对 ChatGPT 解读时高度抽象和清晰的语法中直观地感受到。说实话，这让我动了想认真学一下该软件和相关技术的念头。</p><p>1991 年，沃尔弗拉姆又返回研究状态，开始“昼伏夜出”，每天深夜埋头做实验、写作长达十年，出版了 1000 多页的巨著《一种新科学》（<em>A New Kind of Science</em>）。书中的主要观点是：万事皆计算，宇宙中的各种复杂现象，不论是人产生的还是自然中自发的，都可以用一些规则简单地计算和模拟。Amazon 上书评的说法可能更好懂：“伽利略曾宣称自然界是用数学的语言书写的，但沃尔弗拉姆认为自然界是用编程语言（而且是非常简单的编程语言）书写的。”而且这些现象或者系统，比如人类大脑的工作和气象系统的演化，在计算方面是等效的，具有相同的复杂度，这称为“计算等价原理”。</p><p>这本书很畅销，因为它的语言很通俗，又有近千幅图片，但是也受到了学术界尤其是物理界人士的很多批评。这些批评主要集中在书中的理论并不是原创的（图灵关于计算复杂性的工作、康威的生命游戏等都与此类似），而且缺乏数学严谨性，因此很多结论很难经得住检验（比如自然选择不是生物复杂性的根本原因，图灵公司出版的图书《量子计算公开课》的作者斯科特·阿伦森也指出沃尔弗拉姆的方法无法解释量子计算中非常核心的贝尔测试的结果）。</p><p>而沃尔弗拉姆回应批评的方式是推出 Wolfram|Alpha 知识计算引擎，它被很多人认为是第一项真正实用的人工智能技术。它结合了知识和算法，用户采用自然语言发出命令，系统即可直接返回答案。全世界的用户都可以通过网页、Siri、Alexa 甚至 ChatGPT 插件来使用这个强大的系统。</p><p>如果我们从以 ChatGPT 为代表的神经网络的角度来看沃尔弗拉姆的理论，就会发现一种暗合关系：GPT 底层的自回归架构，与很多机器学习模型不同，的确可以归类为“规则简单的计算”，而且其能力也是通过量变的累积涌现出来的。</p><p>沃尔弗拉姆经常为好莱坞的科幻电影做技术支持，用 Mathematica 和 Wolfram 编程语言生成一些逼真的效果，比较著名的包括《星际穿越》里的黑洞引力透镜效应，以及《降临》里掌握以后就能够超越时空的神奇外星语言，非常富有想象力。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00056.jpeg\" alt=\"{%}\"></p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00057.jpeg\" alt=\"{%}\"></p><p>他当年最终离开学术界，和与普林斯顿同事不和有关。老师费曼写信劝他：“你不会理解普通人的想法的，他们对你来说只是傻瓜。”</p><p>我行我素，活出了精彩的人生。</p><p>斯蒂芬·沃尔弗拉姆真奇人也。</p><h2 id=\"nav_point_4\">奇书</h2><p>奇事 + 奇人，本书当然可谓奇书了。</p><p>像斯蒂芬·沃尔弗拉姆这样的大神能动手为广大读者极为关注的主题写一本通俗读物，这本身就是一个奇迹。</p><p>他 40 年前从纯物理转向复杂系统的研究，就是想解决人类智能等现象的第一性原理，因此有很深的积累。因为他交游广泛，与杰弗里·辛顿、伊尔亚·苏茨克维、达里奥·阿莫迪等关键人物都有交流，所以有第一手资料，保证了技术的准确性。难怪本书出版后，OpenAI 的 CEO 称之为“对 ChatGPT 原理最佳的解释”。</p><p>全书包括两篇文章，篇幅很短，但是把关于 ChatGPT 的最重要的点都讲到了，而且讲得通俗透彻。</p><p>我在图灵社区发起了“ChatGPT 共学营”，与各种技术水平、专业背景的同学有很多交流，发现要理解大模型，正确建立一些核心概念是非常关键的。没有这些支柱，即使你是资深的算法工程师，认知也可能会有很大的偏差。</p><p>比如，GPT 技术路线的一大核心理念，是用最简单的自回归生成架构来解决无监督学习问题，也就是利用无须人特意标注的原始数据，学习其中对世界的映射。自回归生成架构，就是书中讲得非常通俗的“只是一次添加一个词”。这里特别要注意的是，选择这种架构并不是为了做生成任务，而是为了理解或者学习，是为了实现模型的通用能力。在 2020 年之前甚至之后的几年里，业界很多专业人士想当然地以为 GPT 是搞生成任务的，所以选择了无视。殊不知 GPT-1 论文的标题就是“通过生成式预训练改进语言理解”（“Improving Language Understanding by Generative Pre-Training”）。</p><p>再比如，对于没有太多技术背景或者机器学习背景的读者来说，了解人工智能最新动态时可能遇到的直接困难，是听不懂总是出现的基本概念“模型”“参数（在神经网络中就是权重）”是什么意思，而且这些概念很难讲清楚。本书中，大神作者非常贴心地用直观的例子（函数和旋钮）做了解释（参见“什么是模型”一节）。</p><p>关于神经网络的几节图文并茂，相信对各类读者更深刻地理解神经网络及其训练过程的本质，以及损失函数、梯度下降等概念都很有帮助。</p><p>作者在讲解中也没有忽视思想性，比如下面的段落很好地介绍了深度学习的意义：</p><blockquote> <p>“深度学习”在 2012 年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。</p> <p>换句话说，有时候用神经网络解决复杂问题比解决简单问题更容易——这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。</p> </blockquote><p>而下面这一段讲清楚了端到端学习的价值：</p><blockquote> <p>在神经网络的早期发展阶段，人们倾向于认为应该“让神经网络做尽可能少的事”。例如，在将语音转换为文本时，人们认为应该先分析语音的音频，再将其分解为音素，等等。但是后来发现，（至少对于“类人任务”）最好的方法通常是尝试训练神经网络来“解决端到端的问题”，让它自己“发现”必要的中间特征、编码等。</p> </blockquote><p>掌握这些概念的“why”，有益于理解 GPT 的大背景。</p><p>嵌入这个概念无论对从事大模型研发的算法研究者、基于大模型开发应用的程序员，还是想深入了解 GPT 的普通读者，都是至关重要的，也是“ChatGPT 的中心思想”，但是它比较抽象，不是特别容易理解。本书“‘嵌入’的概念”一节是我见过的对这一概念最好的解释，通过图、代码和文字这三种解读方式，让大家都能掌握。当然，后文中“意义空间和语义运动定律”一节还有多张彩图，可以进一步深化这一概念。“‘嵌入’的概念”一节最后还介绍了什么是标记（token），并举了几个直观的英文例子。</p><p>接下来对 ChatGPT 工作原理和训练过程的介绍也通俗而不失严谨。不仅把 Transformer 这个比较复杂的技术讲得非常细致，而且如实告知了目前理论上并没有搞清楚为什么这样就有效果。</p><p>第一篇最后结合作者的计算不可约理论，将对 ChatGPT 的理解上升到一个高度，与伊尔亚·苏茨克维在多个访谈里强调的“GPT 的大思路是通过生成来获取世界模型的压缩表示”异曲同工。</p><p>在我看来，下面这一段落是非常引人深思的：</p><blockquote> <p>产生“有意义的人类语言”需要什么？过去，我们可能认为人类大脑必不可少。但现在我们知道，ChatGPT 的神经网络也可以做得非常出色……我强烈怀疑 ChatGPT 的成功暗示了一个重要的“科学”事实：有意义的人类语言实际上比我们所知道的更加结构化、更加简单，最终可能以相当简单的规则来描述如何组织这样的语言。</p> </blockquote><p>语言是严肃思考、决策和沟通的工具。从孩子的成长过程来看，相比感知、行动，语言应该是智能中更难的任务。但 ChatGPT 很可能已经攻破了其中的密码，正如 Wolfram 说的“它也在某种意义上‘钻研’到了，不必考虑可能的不同措辞，就能‘以语义上有意义的方式组织语言’的地步”。这确实预示着未来我们通过计算语言或者其他表示方式，有可能进一步大幅提升整体的智能水平。</p><p>由此推广开来，人工智能的进展有可能在各学科产生类似的效应：以前认为很难的课题，其实换个角度来看并不是那么难的。加上 GPT 这种通用智能助手的“加持”，“一些任务从基本不可能变成了基本可行”，最终使全人类的科技水平达到新高度。</p><p>本书的第二篇介绍了 ChatGPT 和 Wolfram|Alpha 系统的对比与结合，有较多实例。如果说 GPT 这种通用智能更像人类，而大部分人类其实是天生不擅长精确计算和思考的，那么未来通用模型与专用模型的结合，应该也是前景广阔的发展方向。</p><p>稍有遗憾的是，本书只重点讲了 ChatGPT 的预训练部分，而没有过多涉及后面也很重要的几个微调步骤：监督微调（supervised fine-tuning，SFT）、奖励建模和强化学习。这方面比较好的学习资料是 OpenAI 创始成员、前 Tesla AI 负责人安德烈·卡帕斯（Andrej Karpathy）2023 年 5 月在微软 Build 大会上的演讲“State of GPT”。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00058.jpeg\" alt=\"{%}\"></p><p>在本书包含的两篇之外，沃尔弗拉姆还有一篇关于 ChatGPT 的文章“Will AIs Take All Our Jobs and End Human History—or Not? Well, It's Complicated...”，在更高层次上和更大范围内思考了 ChatGPT 的意义和影响。它也是《一种新科学》一书的延伸，充分体现了沃尔弗拉姆的思考深度。</p><p>关于 AI 能力的上限，他认为，根据“计算等价原理”，ChatGPT 这种通用人工智能的出现证明了“（人类）本质上没有任何特别的东西——事实上，在计算方面，我们与自然中许多系统甚至是简单程序基本上是等价的”。因此，曾经需要人类努力完成的事情，会逐渐自动化，最终能通过技术免费完成。很多人认为是人类特有的创造力或原创力、情感、判断力等，AI 应该也能够拥有。最终，AI 也会逐步发展出自己的世界。这是一种新的生态，可能有自己的宪章，人类需要适应，与之共存共荣。</p><p>那么，人类还剩下些什么优势呢？</p><p>根据“计算不可约性原理”（即“总有一些计算是没有捷径来加速或者自动化的”，作者认为这是思考 AI 未来的核心），复杂系统中总是存在无限的“计算可约区”，这正是人类历史上能不断出现科学创新、发明和发现的空间。所以，人类会不断向前沿进发，而且永远有前沿可以探索。同时，“计算不可约性原理”也决定了，人类、AI、自然界和社会等各种计算系统具有根本的不可预测性，始终存在“收获惊喜的可能”。人类可贵的，是有内在驱动力和内在体验，能够内在地定义目标或者意义，从而最终定义未来。</p><p>我们又应该怎么做呢？</p><p>沃尔弗拉姆给出了如下建议。</p><blockquote> <ul> <li class=\"第2级无序列表\">最高效的方式是发掘新的可能性，定义对自己有价值的东西。</li> <li class=\"第2级无序列表\">从现在的回答问题转向学会如何提出问题，以及如何确定哪些问题值得提出。也就是从知识执行转向知识战略。</li> <li class=\"第2级无序列表\">知识广度和思维清晰度将很重要。</li> <li class=\"第2级无序列表\">直接学习所有详细的知识已经变得不必要了：我们可以在更高的层次上学习和工作，抽象掉许多具体的细节。“整合”，而不是专业化。尽可能广泛、深入地思考，尽可能多地调用知识和范式。</li> <li class=\"第2级无序列表\">学会使用工具来做事。过去我们更倚重逻辑和数学，以后要特别注意利用计算范式，并运用与计算直接相关的思维方式。</li> </ul> </blockquote><p>的确，GPT 可能对我们的工作、学习和生活方式产生巨大的影响，需要我们转换思维方式，需要新型的学习和交流方式。这正是我在图灵社区发起“ChatGPT 共学营”的初衷。共学营是一个“课 + 群 + 书”的付费学习社区，这里不仅有我和众多专家的分享（开放和闭门直播课），有来自不同背景、不同行业、不同专业的同学每天在一起交流（微信群包含几千名优秀同学），还有系统的知识沉淀（电子书和知识库）。共学营中还提供了本书的导读课，以及“State of GPT”演讲的视频和中文精校文图，欢迎大家加入。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00059.jpeg\" alt=\"\" width=\"35%\" style=\"width: 35%\"></p><blockquote> <p style=\"text-align: right\">刘江<br>图灵公司联合创始人、前总编，曾任北京智源人工智能<br>研究院副院长、美团技术学院院长</p> </blockquote><br style=\"page-break-after:always\">","neighbors":{"left":[],"right":{"article_title":"前言","id":731298}},"comments":[]}