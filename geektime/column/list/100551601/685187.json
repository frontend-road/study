{"id":685187,"title":"23｜延迟消息：怎么在 Kafka 上支持延迟消息？","content":"<p>你好，我是大明。今天我们来讨论一个在消息队列面试中非常热的话题——延迟消息。</p><p>延迟消息在 Kafka 面试里面是非常热门的，其他消息队列多少也会问，但是不如 Kafka 问得频繁。因为 Kafka 不支持延迟消息是大家都知道的。但是偏偏 Kafka 又用得多，很多业务场景也要求使用延迟消息，因此面试的时候延迟消息就成了一个重要的考察点。</p><p>和之前的面试点不同，能把延迟消息的方案讲得清楚透彻的人就不多，更不要说刷亮点了。那么今天我就带你深入延迟消息，并且给出一个非常高级的方案。这个方案会涉及到分库分表，所以能帮助你把这些知识融会贯通。</p><h2>延迟队列和延迟消息</h2><p>延迟队列是一种特殊的队列。它里面的每个元素都有一个过期时间，当元素还没到过期时间的时候，如果你试图从队列里面获取一个元素，你会被阻塞。当有元素过期的时候，你就会拿到这个过期的元素。你可以这样想，你拿到的永远是最先过期的那个元素。</p><p>很多语言本身就提供了延迟队列的实现，比如说在 Java 里面的 DelayQueue。</p><p>这节课我们讨论的就是一种特殊形态的延迟队列，或者说是基于消息队列的延迟队列，也叫做延迟消息。具体来说，延迟消息是指消息不是立刻被消费的，而是在经过一段时间之后，才会被消费。在到时间之前，这个消息一直都被存储在消息队列的服务器上。上一节课，我就举了订单超时取消的例子，它就用到了延迟消息。</p><!-- [[[read_end]]] --><h2>支持延迟消息的其他消息队列</h2><p>目前，大部分云厂商版本的消息队列都支持了延迟消息，不过我们这里讨论的是原生支持的消息队列。如果你本身使用了今天我们介绍的这些中间件，那么你可以直接使用这些内容来面试。如果你使用的是 Kafka，那么你在面试的时候要注意结合着这些消息队列，对比着面试。</p><p>RabbitMQ 有插件支持延迟消息功能，而 RocketMQ 和 Kafka 则只能自己研发。</p><h3>RabbitMQ</h3><p>RabbitMQ 有一个延迟消息的插件 rabbitmq_delayed_message_exchange，只需要启用这个插件就可以使用延迟消息。这个插件的基本原理也比较简单，就是实现了一个 exchange。这个 exchange 控制住了消息什么时候会被真的投递到队列里。</p><p><img src=\"https://static001.geekbang.org/resource/image/47/56/476cfa5ff52291c158ed891641cd8f56.png?wh=1920x882\" alt=\"图片\"></p><p>如上图所示，消息会先暂时存储在 exchange 里面。它使用的是 Mnesia 来存储，如果你不知道 Mnesia 是什么，就直观地把它看作一个基于文件的数据库。</p><p>当延迟的时间满足条件之后，这些存储的数据就会被投递到真正的消息队列里面。紧接着消费者就可以消费到这个消息了。你可以从这里得到一个启发，就是如果要实现一个延迟队列，是可以借助数据库的。</p><p>那么这个插件本身也是有很多限制的，在它的官网主页里面就有说明，其中有两个最突出的限制。</p><ul>\n<li>消息在真的被投递到目标消息队列之前，是存放在接收到了这个消息的服务端本地的 Mnesia 里面。也就是说，如果这个时候还没有刷新磁盘，那么消息就会丢失；如果这个节点不可用了，那么消息也同样会丢失。</li>\n<li>不支持高并发、大数据量。显然，现实中很多场景都是要在高并发大数据量场景下使用延迟消息的，比如说前面说的订单超时取消。因此这个缺点也限制了这个插件被广泛使用。</li>\n</ul><p>除了这个插件，开发者也可以自己手动实现延迟消息。这就要利用到 RabbitMQ 的 ttl 功能和所谓的死信队列了。</p><p>死信队列是一种逻辑上的概念，也就是说它本身只是一个普通的队列。而死信的意思是指过期的无法被消费的消息，这些消息会被投送到这个死信队列。</p><p>简单来说，就是开发者准备一个队列 delay_queue，为这个 delay_queue 设置过期时间，这个 delay_queue 不需要消费者。然后把你真实的业务 biz_queue 绑定到这个 delay_queue，作为它的死信队列。</p><p><img src=\"https://static001.geekbang.org/resource/image/13/aa/1347e67374197cf37fa0a75a3930b2aa.png?wh=1920x827\" alt=\"图片\"></p><p>生产者发送消息到 delay_queue，因为没有消费者，所以消息会过期。过期之后的消息被转发到死信队列，也就是 biz_queue 里面。这时候消费者就能拿到消息了。</p><p>这种方案并没有插件的那两个缺点。但是 ttl 的设置是在队列级别上，也就是你一个 delay_queue 的延时是固定的，不能做到随机。比如说我这一条消息延后三分钟，另外一条消息延后五分钟，这是不可能的。因此，你可能需要创建很多不同 ttl 的 delay_queue 才能满足业务需要。</p><h2>面试准备</h2><p>在准备延迟消息面试的时候，你需要先弄清楚公司内部的一些情况。</p><ul>\n<li>你所在公司有没有使用延迟消息的场景？</li>\n<li>你所在公司的消息中间件支不支持延迟消息？</li>\n<li>延迟消息的 QPS 有多高，消息量有多大？延迟时间有多长？</li>\n</ul><p>然后，在面试中聊到下面这些话题的时候，你可以尝试把话题引导到这里。</p><ul>\n<li>如果你介绍你的业务的时候，提到了需要延迟消息的场景，就像我举的订单超时取消的例子，那么你就可以深入讨论延迟消息是怎么实现的。</li>\n<li>如果你聊到了 Kafka，在介绍它的优缺点的时候，你说一说它的缺点，就是不支持延迟队列，那么面试官就会问你如果要 Kafka 支持延迟队列应该怎么做。</li>\n<li>如果你聊到了你们公司使用的消息队列，但并不是 Kafka，那么你同样可以尝试引导到这里，比如说强调当初你们 MQ 技术选型的时候，考虑到 Kafka 不支持延迟消息，所以最终没有选择 Kafka。</li>\n</ul><h2>基础思路</h2><p>这里我给出了很多比较简单的方案，你可以从这些方案里选择一个作为公司的方案，其他方案你就作为自己了解但是没有实践过的方案。</p><h3>利用定时任务调度</h3><p>利用定时任务来实现延迟消息是最好、最简单的办法。对于一个延迟消息来说，一个延迟到 30 分钟后才可以被消费的消息，也可以认为是 30 分钟后才可以发送。也就是说，你可以设定一个定时任务，这个任务会在 30 分钟后把消息发送到消息服务器上。</p><p><img src=\"https://static001.geekbang.org/resource/image/c7/67/c70beb212c1773a0d6ae4cdbec908067.png?wh=1920x938\" alt=\"图片\"></p><p>所以你可以这么介绍这个方案。</p><blockquote>\n<p>最简单的做法就是利用定时任务，最好是解决了持久化的分布式任务平台。那么业务发送者就相当于注册一个任务，这个任务就是在 30 分钟之后发送一条消息到 Kafka 上。之后业务消费者就能够消费了。</p>\n</blockquote><p>同时你还可以阐述一下这个方案的缺点。</p><blockquote>\n<p>这个方案的最大缺点是支撑不住高并发。这是因为绝大多数定时任务中间件都没办法支撑住高并发、大数据的定时任务调度，所以只有应用规模小，延迟消息也不多的话，才可以考虑使用这个方案。如果想要支持高并发、大数据的延迟方案，还是要考虑利用消息队列。</p>\n</blockquote><p>最后一句话是为了让你把话题重新拉回消息队列这里，并且阐述下面这个方案。</p><h3>分区设置不同延迟时间</h3><p>这种方案应该算是很简单，而且也很好用的方案。你可以看一下它的基本架构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/96/27/966ba2212123df97e89a64cee30c6f27.png?wh=1920x1026\" alt=\"图片\"></p><p>这里关键的角色是 delay_topic 和延迟消费组。</p><ul>\n<li>delay_topic 里面的分区被用来接收不同延迟时间的消息。比如说在上图中分成了 p0、p1、p2 三个分区，分别用于接收延迟时间为 1min、3min和 10min 的消息。</li>\n<li>延迟消费组会创建出和分区数量一样的消费者，每一个消费者消费一个分区。消费者每次读取一个消息，等延迟足够长的时间之后，就会转发给 biz_topic。</li>\n</ul><p>因此对于业务发送者来说，他们需要根据自己的延迟时间来选择正确的分区。而业务消费者则是对整个过程是无感的，也就是说他们并不知道中间有延迟消费者在做转发的事情。</p><p>所以你可以简单介绍这个方案。</p><blockquote>\n<p>我们公司用的方案是比较简单的，也就是创建了一个 delay_topic，这个topic 有 N 个分区，每个分区设定了不同的延迟时间。然后我们创建了一个消费组去消费这个 delay_topic，每个分区有一个独立的消费者。每个消费者在读取到一条消息之后，就会根据消息里面的延迟时间来等待一段时间。等待完之后，再把消息发送到业务方真正的 topic 上。</p>\n</blockquote><p>注意，这个 N 很灵活，我给你两个选择。</p><ul>\n<li>5个分区：延迟时间分别是 1min、3min、5min、10min、30min。</li>\n<li>10个分区：延迟时间分别是 1min、3min，5min、10min、15min、30min、60min、90min、120min、180min。</li>\n</ul><p>总体来说 N 就是一个根据业务来设计的东西。</p><p>接下来就是你刷亮点的地方了，第一个就是 rebalance 问题。</p><h4>rebalance 问题</h4><p>在这个方案中，因为消费者睡眠了，睡眠期间不会消费消息，所以 Kafka 就会判定这个消费者已经崩溃了，就会触发 rebalance。发生 rebalance 之后，等消费者再恢复过来，就不知道又会被分配到哪个分区，那么之前的睡眠就可以认为是白睡了。</p><p><img src=\"https://static001.geekbang.org/resource/image/5e/d3/5edc1d0f01fc70cf287b5eaa3c0be0d3.png?wh=1920x1026\" alt=\"图片\"></p><p>为了避免这个问题，就需要确保在睡眠期间不会触发 rebalance。因此需要利用 Kafka 的暂停（Pause）功能，在睡眠结束之后，再恢复（Resume）。注意，Kafka 的暂停功能相当于拉取 0 条数据，并不是说不拉数据，也就是说还是会发起 Poll 调用。</p><p>所以整体逻辑就是这样的：</p><ol>\n<li>拉取一条消息，假如说 offset = N，查看剩余的延迟时间 t。</li>\n<li>暂停消费，睡眠一段时间 t。</li>\n<li>睡眠结束之后，恢复消费，继续从 offset = N 开始消费。</li>\n</ol><p>如果面试官没有做过类似的事情的话，可能想不到这里还有坑，所以我建议你主动补充说明，关键词是 <strong>rebalance</strong>。</p><blockquote>\n<p>这个方案里面，要注意一个 rebalance 的问题。因为每次我们拉取到消息之后，都要根据消息的剩余延迟时间，睡眠一段时间。在这段时间之内，Kafka 会认为消费者已经崩溃了，从而触发 rebalance。等睡眠结束之后，重新消费，就不一定还是消费原本的那个分区了。<br>\n&nbsp;<br>\n所以为了避免这个问题，在睡眠之前，要暂停消费，恢复之后重新消费。重新消费的时候，要注意把 offset 重置为之前拉取那个消息的 offset。</p>\n</blockquote><p>这个时候，你还可以进一步刷亮点，就是暂停的时候还是会发出 Poll 调用，只不过不会真的把数据从服务端拉到消费者那里。</p><blockquote>\n<p>Kafka 的暂停消费，并不是不再发起 Poll 请求，而是 Poll 了但是不会真的拉消息。这样可以让 Kafka 始终认为消费者还活着。</p>\n</blockquote><p>这个 rebalance 问题还是不太容易想到，但是一致性问题，面试官就很容易想到了。</p><h4>一致性问题</h4><p>前面说的最后几个步骤是从服务端拉取到消息，然后转发到 biz_topic 里面。那么你应该意识到这里面涉及到了一个关键问题，是先提交消息，还是先转发？</p><p>如果是先提交，那么就会出现消息提交了，但是还来不及转发 biz_topic 就宕机的情况，这显然不能容忍。</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/be/b1dd9f1cf49c9262961b24ccfc93d9be.png?wh=1920x763\" alt=\"图片\"></p><p>但是如果先转发 biz_topic，然后提交。那如果提交之前宕机了，后面恢复过来，又会转发一次。好在多发一次并不是什么问题，因为你可以要求业务消费者确保自己的逻辑是幂等的。</p><p><img src=\"https://static001.geekbang.org/resource/image/84/70/847d5581d890acc4ab0705a3e2fb1a70.png?wh=1920x796\" alt=\"图片\"></p><p>其实，即便是非延迟消息，还是需要消费者保证自己的逻辑是幂等的。因为本身发送者就存在重复发送的可能，不然没办法确保消息一定投递到 消息服务器上。</p><p>面试官很可能会问这个一致性的问题，那么你就可以抓住关键词<strong>后提交</strong>来回答。</p><blockquote>\n<p>一致性的问题解决起来需要业务方的配合。我们这个的逻辑是到了延迟时间，就先转发 biz_topic，然后再提交。也就是说，如果在转发 biz_topic 之后，提交失败了，下一次就还可以重试，那么 biz_topic 就可能收到两条同样的消息。在这种场景下，就只能要求消费者做到幂等。当然，即便不用延迟消息，消费者最好也要做到幂等的。因为发送方为了确保发送成功，本身就可能重试。</p>\n</blockquote><p>这个回答可能把话题引申到如何做幂等，后面会有一个高级方案，这里我就不多说了。</p><h4>优缺点分析</h4><p>在回答完前面的两个亮点之后，你要分析一下这个方案的优缺点。</p><blockquote>\n<p>这个方案最大的优点就是足够简单，对业务方的影响很小，业务方只需要根据自己的延迟时间选择正确的分区就可以了。</p>\n</blockquote><p>不过这个方案也有两个突出的缺点，就是<strong>延迟时间必须预先设定好、分区间负载不均匀</strong>。</p><blockquote>\n<p>这个方案的缺点其实还挺严重的。<strong>第一个是延迟时间必须预先设定好</strong>，比如只能允许延迟 1min、3min 或者 10min 的消息，不支持随机延迟时间。在一些业务场景下，需要根据具体业务数据来计算延迟时间，那么这个就不适用了。<strong>第二个是分区之间负载不均匀</strong>。比如很多业务可能只需要延迟 3min，那么 1min 和 10min 分区的数据就很少。这会进一步导致一个问题，<strong>就是负载高的分区会出现消息积压的问题</strong>。<br>\n&nbsp;<br>\n在这里，很多解决消息积压的手段都无法使用，所以只能考虑多设置几个延迟时间相近的分区，比如说在 3min 附近设置 2min30s，3min30s 这种分区来分摊压力。</p>\n</blockquote><p>这里你会把话题引导到消息积压如何解决的问题上，这个我们在后面第25讲会讲到。同时你又可以把话题引导到亮点方案里，就是如何实现随机延迟时间。</p><h2>基于 MySQL 的亮点方案</h2><p>如果在实践中，我会劝你放弃支持随机延迟时间。因为绝大多数情况下，业务是用不着非得随机延迟时间的，他们完全可以通过调整业务来适配固定的几个延迟时间。不过在面试中，你还是需要利用一下支持随机延迟时间来进一步加深面试官对你的印象。</p><p>你可以看一下基于 MySQL 方案的基本架构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/39/02/39c3c328e69d641ede4496df15921f02.png?wh=1920x1038\" alt=\"图片\"></p><p>这个方案里面的关键点是创建一个 delay_topic，业务发送者把消息发送到这个 topic 里面，消息里面带上了需要延迟的时间。里面有一个延迟消费者，它会消费 delay_topic 里面的消息，转储到数据库中。还有一个延迟发送者，它会轮询数据库里的消息，把已经可以转发出去的消息转发到真正的 biz_topic 上。发送完之后，延迟发送者把数据库的状态更新成已发送。最后业务消费者消费 biz_topic。</p><p>这个方案在落地的时候，要想支撑住高并发，还是一个比较麻烦的事情，所以<strong>怎么支撑住高并发就是我们的关注点。</strong></p><p>在前面的基本架构里面，最明显的性能瓶颈就是 MySQL，因为这个场景是一个写密集的场景。所以要想撑住高并发就要想办法提高 MySQL 的性能。当然最佳的策略还是换一个存储结构，比如说换 TiDB 或者 Elasticsearch。不过你要是回答换一种存储结构，那就没办法刷出亮点了。使用 MySQL 的话，你就可以从分区表、表交替、分库分表、批量操作几个方案里面选择。</p><h3>分区表</h3><p>最简单的优化方案，就是在 MySQL 上应用分区。因为延迟消息是一个时效性很强的数据，也就是说，你完全可以按照发送时间，也就是延迟之后具体的发送时间点来分区。在并发不是很高的时候，你可以按照周来分区。在并发很高的时候，你可以按照天来分区。历史分区可以及时清理掉，因为用不上了。</p><p>你这么回答：</p><blockquote>\n<p>要想提高 MySQL 的性能，一个比较简单的做法是使用分区表，比如说根据并发量选择按月分、按周分、按天分。历史分区就可以直接清理掉。</p>\n</blockquote><p>与分区表类似的解决方案，还有表交替方案。</p><h3>表交替</h3><p>表交替要复杂一点，但是也很好用。表交替的意思是你准备两个表，然后交替写、交替查询。比如说你今天用 tab_0，明天用 tab_1。当你用 tab_1 的时候你就可以直接清空（TRUNCATE）tab_0 的数据，反过来也是这样。这种按天交替的方案对延迟时间是有限制的，延迟时间不能超过一天。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/b5/45ff4ff8be3946a632da084fd5785fb5.png?wh=1920x1025\" alt=\"图片\"></p><blockquote>\n<p>还可以考虑使用表交替方案，也就是说准备两个表 tab_0、tab_1。那么最开始的时候可以读写 tab_0，然后换成读写 tab_1。每次交替的时候，都可以把之前使用的数据 TRUNCATE 掉。TRUNCATE 本身很快，所以没什么性能问题。</p>\n</blockquote><h3>分库分表</h3><p>前面两个方案实际上就能撑住比较高的并发了，但是要想进一步提高并发能力，就只能考虑分库分表了，毕竟单一的库再怎么分区或者交替都存在写瓶颈。</p><blockquote>\n<p>如果并发确实非常高，那么就只能考虑采用分库分表的方案了。这里分库分表也很简单，只需要按照 biz_topic 的名字来分库分表就可以了。而且每一张表可以叠加前面的分区表和交替表的方案，进一步提高性能。</p>\n</blockquote><p>不过这个方案也有一定的隐患，第一个问题就是不同 topic 的并发度不一样，比如说 biz_topic_1 的并发只有 100，而 biz_topic_2 的并发有 10000，那么按照 biz_topic 来分，就会出现不同库不同表的压力差异很大的问题。</p><p>在这种情况下，你可以回答关键词<strong>轮询插入</strong>。</p><blockquote>\n<p>如果不考虑消息有序性的问题，那么也可以考虑轮询。比如说分库分表是 4 * 8 = 32 张表，那么就可以要求每一个延迟消费者，轮流往这些表里插入数据。因为延迟消息有一个很显著的特点，就是查找的时候只会按照发送时间来找，所以随机插入都没问题。</p>\n</blockquote><p>这个你可能难以理解，我给你举个例子，比如说我有一个消息发送给 biz_topic_1，要求是一分钟后发出去。那么不管这个消息被存在哪个表，延迟发送者都可以找出来，然后转发到 biz_topic_1。</p><p><img src=\"https://static001.geekbang.org/resource/image/6a/fe/6ae7d6fd200bb1eb1f7d9e90cd3c58fe.png?wh=1920x831\" alt=\"图片\"></p><p>当然你的回答也引到了另一个亮点，就是消息有序性。</p><blockquote>\n<p>要想做到延迟消息有序性，有一个比较简单的方案。在分库分表的时候，确保同一个 biz_topic 的消息都落到同一张表里面。并且最开始发送到 delay_topic 上也是按照 biz_topic 的名字来选择分区的，那么就可以保证延迟消息转发到 biz_topic 上跟它们被发送到 delay_topic 上是一样的。</p>\n</blockquote><p><img src=\"https://static001.geekbang.org/resource/image/53/e5/53b3c70cd46cab2f17a0f981e50e57e5.png?wh=1920x1022\" alt=\"图片\"></p><p>到这里，你还剩下一个可以同时在延迟消费者和延迟发送者上使用的策略：批量操作。</p><h3>批量操作</h3><p>这个也非常简单，而且性能会有大幅提升。简单来说，就是批量插入、批量更新。</p><blockquote>\n<p>我们还可以利用批量操作来减轻 MySQL 的压力。对于延迟消费者来说，它可以消费了一批数据之后再批量插入到数据库里面，然后再提交这一批消息。对于延迟发送者来说，当发送了一批数据之后，再批量把这些消息更新为已发送。</p>\n</blockquote><p>那么你可能注意到，这个批量操作会使数据一致性问题更加严重。</p><p>但是这个方案类似于分区设置不同延时时间方案，只需要消费者做到幂等就可以了。本质上，这里的一致性问题要么是因为延迟消费者重复消费，要么就是因为延迟发送者重复发送。但不管是哪个原因，消费者幂等都可以解决问题。</p><h2>面试思路总结</h2><p>最后我们来总结一下这节课的内容，消息队列有两个基本的延迟消息解决方案：<strong>利用定时任务调度和分区设置不同延迟时间</strong>。在为分区设置不同的延迟时间时，你要注意rebalance和一致性的问题。</p><p>最后我给出了一个基于MySQL的亮点方案。你要记住里面的四个关键点：<strong>分区表、表交替、分库分表、批量操作</strong>。这里我强调一下分库分表，因为本身它也是一个高级话题，所以你把延迟消息和分库分表结合在一起去面试，优势显著。</p><p>当然，如果你本身不是使用 Kafka 的，这些方案也是有参考价值的。比如说你用 RabbitMQ，但是 RabbitMQ 已有的插件缺陷太多，你完全可以使用我后面描述的 MySQL 方案做一个类似的插件出来。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/62/3ff0d0d408e4520a1e2c0e9a40303562.jpg?wh=3169x3008\" alt=\"\"></p><h2>思考题</h2><ul>\n<li>在基础思路里面，我讲了一个不同分区设置不同延迟时间的方案，你觉得这个地方可以考虑换成不同 topic 设置不同延迟时间的方案吗？如果可以的话，topic 的分区该怎么设置？延迟消费者又该怎么办？</li>\n<li>我在最后亮点方案的分库分表里面还提到了解决消息有序性的一种方案，你能试着改造其他解决方案来满足消息有序的要求吗？</li>\n</ul><p>欢迎你把自己的想法分享到评论区，也欢迎你把这节课的内容分享给需要的朋友，邀他一起学习，我们下节课再见！</p>","comments":[{"had_liked":false,"id":379226,"user_name":"第四死神","can_delete":false,"product_type":"c1","uid":1801548,"ip_address":"北京","ucode":"DF8CC6F9863B28","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erxia5dpTeXMHUg3NlQmLzqw0qBic4kRjKBsNuT9ib9VD717dckYvU63S07KPkydMULG2wEcibJXmtTBA/132","comment_is_top":false,"comment_ctime":1691575537,"is_pvip":true,"replies":[{"id":138211,"content":"感谢提醒，倒是没想到！！！！果然是一年不学习，立马变垃圾。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1691754717,"ip_address":"广东","comment_id":379226,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"开源版本在2022年7月20日 增加了 [RIP-43] Support Timing Messages with Arbitrary Time Delay","like_count":10,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625493,"discussion_content":"感谢提醒，倒是没想到！！！！果然是一年不学习，立马变垃圾。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1691754718,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380296,"user_name":"陈斌","can_delete":false,"product_type":"c1","uid":1367048,"ip_address":"沙特阿拉伯","ucode":"B639AB5F6AA03D","user_header":"https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg","comment_is_top":false,"comment_ctime":1693320927,"is_pvip":false,"replies":[{"id":138542,"content":"盲生，你发现了华点，哈哈哈哈。是的，正常来说，并发很高的话，你可能一秒轮询就拿到几千条消息，这个时候你肯定来不及发完。‘\n\n优化的方法就是多线程，扫出来之后直接多线程来发送。这样的话发送比较快，就可以扫描更加频繁。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1693488600,"ip_address":"广东","comment_id":380296,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"基于 MySQL 的亮点方案 中，延时消息的延时精度，取决于延时生产者的轮询频率。如果业务有要求在业务低谷期时延时精度要在0.5s内，那么轮询频率会高于每秒两次，那么就会限制并发上限。即并发越高，我感觉该方案的时间精度可能会越低。","like_count":3,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626923,"discussion_content":"盲生，你发现了华点，哈哈哈哈。是的，正常来说，并发很高的话，你可能一秒轮询就拿到几千条消息，这个时候你肯定来不及发完。‘\n\n优化的方法就是多线程，扫出来之后直接多线程来发送。这样的话发送比较快，就可以扫描更加频繁。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1693488600,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2051563,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/4d/eb/2ee44294.jpg","nickname":"DarkPrince","note":"","ucode":"B1138082EDA3E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":644628,"discussion_content":"是的，这个不是大问题。还可以将即将到期的一批数据用时间轮放到内存里提前，对MySQL压力也不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1715390914,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":626923,"ip_address":"北京","group_id":0},"score":644628,"extra":""}]}]},{"had_liked":false,"id":384602,"user_name":"建涛涛涛、ᕕ( ᐛ )ᕗ","can_delete":false,"product_type":"c1","uid":1344168,"ip_address":"广东","ucode":"CD818A89F502FE","user_header":"https://static001.geekbang.org/account/avatar/00/14/82/a8/b419d69a.jpg","comment_is_top":false,"comment_ctime":1701228058,"is_pvip":false,"replies":[{"id":140346,"content":"有一个 Kafka 在前面挡着，可以保护住 mysql。直接用的话，mysql 不一定撑得住那么大的并发。\n\n另外一方面则是，客户端那边可能不希望和 mysql 打交道，而是希望说直接和 Kafka 打交道，然后 Kafka 后面你们怎么搞，就随便。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1701846830,"ip_address":"广东","comment_id":384602,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"请问一下老师， kafka + mysql 做随机时间的方案里， 既然都上mysql了。 还要kafka做什么？ \n只用 mysql, 然后轮询不就搞定了吗。","like_count":2,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633122,"discussion_content":"有一个 Kafka 在前面挡着，可以保护住 mysql。直接用的话，mysql 不一定撑得住那么大的并发。\n\n另外一方面则是，客户端那边可能不希望和 mysql 打交道，而是希望说直接和 Kafka 打交道，然后 Kafka 后面你们怎么搞，就随便。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701846830,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386696,"user_name":"一弦一柱思华年","can_delete":false,"product_type":"c1","uid":3720570,"ip_address":"广东","ucode":"36F8086A275AA7","user_header":"https://static001.geekbang.org/account/avatar/00/38/c5/7a/c03cb56e.jpg","comment_is_top":false,"comment_ctime":1705368808,"is_pvip":false,"replies":[{"id":141056,"content":"首先纠正一个小细节，就是如果要是采用的是同一个分区固定延迟时间或者同一个 topic 固定延迟时间，那么你可以预期，如果第一个消息需要延迟一小时，后续的消息必然也是要延迟一小时的。因此你一小时到时之后，消费了第一条消息，第二条消息应该就差不多要到期了。你可以理解为流水，睡眠只是等第一滴水到来，之后都是源源不断的。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1705927331,"ip_address":"广东","comment_id":386696,"utype":1}],"discussion_count":5,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"这里有个问题，关于delay-topic的：延迟消费者是每拉取一条数据就立马根据剩余时间睡眠吗，那这样的话消费速度远远跟不上生产速度吧。。假如我延迟时间是1小时，如果生产量稍微大一点，比如1000条消息，那后面的消息岂不是要等1000小时才能消费到，这样的话不是早就过期了吗，恐怕严重不符合用户使用预期。因此是否需要结合批量读来实现呢：根据业务场景并发量来看一次性拉取多少条合适，然后在延迟消费者里开启多线程睡眠为每条消息定时转发","like_count":1,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":636303,"discussion_content":"首先纠正一个小细节，就是如果要是采用的是同一个分区固定延迟时间或者同一个 topic 固定延迟时间，那么你可以预期，如果第一个消息需要延迟一小时，后续的消息必然也是要延迟一小时的。因此你一小时到时之后，消费了第一条消息，第二条消息应该就差不多要到期了。你可以理解为流水，睡眠只是等第一滴水到来，之后都是源源不断的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705927331,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":3,"child_discussions":[{"author":{"id":1138926,"avatar":"https://static001.geekbang.org/account/avatar/00/11/60/ee/b536678b.jpg","nickname":"傲然绝唳","note":"","ucode":"BD7365DD44589B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":641393,"discussion_content":"好像还是解决不了，比如我延迟时间1个小时，第一条消息0点发的，第二条消息0点30分发的，第一条消息可以正常延迟到1点消费，第二个消息要在什么时候消费呢？立马消费的话就只延迟了半个小时，等待1个小时后消费的话就要到2点，延迟了1个半小时","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712587465,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":636303,"ip_address":"北京","group_id":0},"score":641393,"extra":""},{"author":{"id":3666974,"avatar":"https://static001.geekbang.org/account/avatar/00/37/f4/1e/597f218a.jpg","nickname":"花卷","note":"","ucode":"473EB741F7A420","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1138926,"avatar":"https://static001.geekbang.org/account/avatar/00/11/60/ee/b536678b.jpg","nickname":"傲然绝唳","note":"","ucode":"BD7365DD44589B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641795,"discussion_content":"我理解是这条消息的生产时间距离这个1小时延迟的剩余时间，这样才能做到消费者源源不断消费吧 ","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1713013912,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":641393,"ip_address":"四川","group_id":0},"score":641795,"extra":""},{"author":{"id":3904710,"avatar":"","nickname":"Geek_4364a5","note":"","ucode":"16377D39F49AB2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1138926,"avatar":"https://static001.geekbang.org/account/avatar/00/11/60/ee/b536678b.jpg","nickname":"傲然绝唳","note":"","ucode":"BD7365DD44589B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646953,"discussion_content":"我也是有同样的困惑, 感觉好像是有问题的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719136340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":641393,"ip_address":"广东","group_id":0},"score":646953,"extra":""}]},{"author":{"id":1241197,"avatar":"https://static001.geekbang.org/account/avatar/00/12/f0/6d/3e570bb8.jpg","nickname":"一打七","note":"","ucode":"D242C5EF70C176","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652279,"discussion_content":"我这里也想不通","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1728603313,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386011,"user_name":"Geek_728b54","can_delete":false,"product_type":"c1","uid":3203559,"ip_address":"北京","ucode":"27F70307CAEDE8","user_header":"","comment_is_top":false,"comment_ctime":1703838117,"is_pvip":false,"replies":[{"id":140740,"content":"这是有可能的。但是你可以手动指定分区，并且在代码里面直接写死如果不是你预期的分区，你就忽略。那么在一个节点宕机之后，即便这个分区被分配给了别的消费者，因为别的消费者不会处理这个分区的消息，所以只有等到你本来宕机的节点重新恢复之后，这个分区的消息才会被处理。\n","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1704188525,"ip_address":"广东","comment_id":386011,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"老师好，在“分区设置不同延迟时间”方案中，如果其中一个消费者宕机和重启，这时候会不会触发整体的重平衡协议？","like_count":1,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":634899,"discussion_content":"这是有可能的。但是你可以手动指定分区，并且在代码里面直接写死如果不是你预期的分区，你就忽略。那么在一个节点宕机之后，即便这个分区被分配给了别的消费者，因为别的消费者不会处理这个分区的消息，所以只有等到你本来宕机的节点重新恢复之后，这个分区的消息才会被处理。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704188525,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379213,"user_name":"humor","can_delete":false,"product_type":"c1","uid":1181867,"ip_address":"浙江","ucode":"9B48C4C7BEC92C","user_header":"https://static001.geekbang.org/account/avatar/00/12/08/ab/caec7bca.jpg","comment_is_top":false,"comment_ctime":1691559302,"is_pvip":false,"replies":[{"id":138161,"content":"不需要关注。你延迟消息动不动就延迟几十秒几十分钟，时钟不同步误差都是纳秒级，约等于没有。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1691564611,"ip_address":"广东","comment_id":379213,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"延迟消息体里应该存放的是消息应该发送的具体时间戳吧，那会不会有时钟不同步的问题，比如消息发送者和延迟消息组的时钟不一致，导致延迟消息被提前或者延后发送。这个问题需要关注吗，怎么处理呢？","like_count":1,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625307,"discussion_content":"不需要关注。你延迟消息动不动就延迟几十秒几十分钟，时钟不同步误差都是纳秒级，约等于没有。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1691564611,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":388957,"user_name":"智商未成年","can_delete":false,"product_type":"c1","uid":1672749,"ip_address":"广东","ucode":"B39D13750DB255","user_header":"https://static001.geekbang.org/account/avatar/00/19/86/2d/90c206fe.jpg","comment_is_top":false,"comment_ctime":1711266866,"is_pvip":false,"replies":[{"id":141994,"content":"消费另一个分区的数据，又得拿出来看看，然后再次根据延时睡眠。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1715231578,"ip_address":"广东","comment_id":388957,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"你好，我想问下 &quot;发生 rebalance 之后，等消费者再恢复过来，就不知道又会被分配到哪个分区，那么之前的睡眠就可以认为是白睡了。&quot;\n这个白睡了是怎么理解呢，消费者睡眠结束后，消费另一个分区的数据，感觉对延时消费逻辑没影响，消费其他分区的数据时，还是根据剩余的延迟时间进行睡眠。","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":644416,"discussion_content":"消费另一个分区的数据，又得拿出来看看，然后再次根据延时睡眠。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1715231578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384975,"user_name":"sheep","can_delete":false,"product_type":"c1","uid":2770150,"ip_address":"广东","ucode":"DAC2036F08CE27","user_header":"https://static001.geekbang.org/account/avatar/00/2a/44/e6/2c97171c.jpg","comment_is_top":false,"comment_ctime":1701864159,"is_pvip":false,"replies":[{"id":140767,"content":"对，有一致性问题。比如说你启动 goroutine 之后，我节点和 Broker 失去联系了，一样会 rebalance。除非你说，我先提交，然后开 Goroutine 延迟处理，但是这样就会丢失。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1704190279,"ip_address":"广东","comment_id":384975,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"有个问题，“分区设置不同延迟时间”这里，消费者获取到指定消息之后，要延迟一定时间后，才能继续处理并提交信息吗。那这里，消费者获取到这个消息后，我启动一个协程来进行延迟和处理，是不是就能解决rebalance问题了呢？但是会有数据一致性的问题吧？比如这个协程处理异常或者服务宕机了，此时消息还没有处理完毕，就先前已经被提交了？","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":634928,"discussion_content":"对，有一致性问题。比如说你启动 goroutine 之后，我节点和 Broker 失去联系了，一样会 rebalance。除非你说，我先提交，然后开 Goroutine 延迟处理，但是这样就会丢失。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704190279,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381438,"user_name":"Geek_9affad","can_delete":false,"product_type":"c1","uid":3720930,"ip_address":"山东","ucode":"769E6EF2EA2EB5","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/B7en9NzmQO6FFgIK8H5vibcLET3ibqGeHjnTzncfZib4VqIkaM5hTxu1LybK12cYe6TbceTppRbjJS7Yh0AKldteA/132","comment_is_top":false,"comment_ctime":1695202729,"is_pvip":false,"replies":[{"id":139043,"content":"可以的。Redis 高可用的话，可以不用 MySQL 的","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1695728816,"ip_address":"广东","comment_id":381438,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"mysql的方案可以替换成 Redis中有序集合的score来完成吧","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628710,"discussion_content":"可以的。Redis 高可用的话，可以不用 MySQL 的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695728816,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1744257,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/9d/81/d748b7eb.jpg","nickname":"千锤百炼领悟之极限","note":"","ucode":"224B5CF2101716","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641045,"discussion_content":"用Redis有序集合的话，前置的Kafka delay-topic 就可以省掉了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712117808,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2420294,"avatar":"https://static001.geekbang.org/account/avatar/00/24/ee/46/7d65ae37.jpg","nickname":"木几丶","note":"","ucode":"FFDB958DA64F8C","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1744257,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/9d/81/d748b7eb.jpg","nickname":"千锤百炼领悟之极限","note":"","ucode":"224B5CF2101716","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652567,"discussion_content":"作为接入层还是很有必要的，一是以普通topic来接收客户端消息，屏蔽实现细节，二是方便扩展，后面不想用Redis了或者引入多个存储比较容易，这跟我们做前置网关一个道理。要是省掉这一层，消息队列都没必要了，直接用Redis不好吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729155457,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":641045,"ip_address":"福建","group_id":0},"score":652567,"extra":""}]}]},{"had_liked":false,"id":381277,"user_name":"itschenxiang","can_delete":false,"product_type":"c1","uid":1519547,"ip_address":"广东","ucode":"7D90194AC52435","user_header":"https://static001.geekbang.org/account/avatar/00/17/2f/bb/f663ac5a.jpg","comment_is_top":false,"comment_ctime":1694962133,"is_pvip":false,"replies":[{"id":139047,"content":"1. 对的，因为你同一个分区，延迟时间都是一样的。\n2. 这个我倒是没尝试过，但是你可以试试，看起来应该没啥问题。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1695728979,"ip_address":"广东","comment_id":381277,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"老师你好。我这里有两个疑问哈：\n1）分区设置不同延迟时间-rebalance 问题中，提出“暂停消费，睡眠一段时间 t”，这里前提是先消费到的消息先达到超时时间点吗？\n2）分区设置不同延迟时间-一致性问题，这里是不是可以使用kafka的extract once，使用事务来进行转发控制呢，感觉这就是典型的使用场景？","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628714,"discussion_content":"1. 对的，因为你同一个分区，延迟时间都是一样的。\n2. 这个我倒是没尝试过，但是你可以试试，看起来应该没啥问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695728979,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1519547,"avatar":"https://static001.geekbang.org/account/avatar/00/17/2f/bb/f663ac5a.jpg","nickname":"itschenxiang","note":"","ucode":"7D90194AC52435","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628188,"discussion_content":"* exactly","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695048920,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380419,"user_name":"Geek8004","can_delete":false,"product_type":"c1","uid":2328971,"ip_address":"中国香港","ucode":"B3828F6414BDB0","user_header":"","comment_is_top":false,"comment_ctime":1693538256,"is_pvip":false,"replies":[{"id":138605,"content":"不是遍历。它扫描就是用下次发送时间在下一秒的。每次就是取出来一秒，当然你也可以取出来十秒，但是要小心时间准确度。\n\n性能的话，其实也没那么差，你保持一个线程扫描一张表，然后扫出来的数据可以开启多线程处理。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1693896340,"ip_address":"广东","comment_id":380419,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"mysql这个亮点方案,延迟发送者去高频率扫表,假如有16个topic对应16个库,每个库8张表,假设每个表每天产生50万数据,每次遍历索引或者是扫全表,性能会不会很差呀,大概要多久呀","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627213,"discussion_content":"不是遍历。它扫描就是用下次发送时间在下一秒的。每次就是取出来一秒，当然你也可以取出来十秒，但是要小心时间准确度。\n\n性能的话，其实也没那么差，你保持一个线程扫描一张表，然后扫出来的数据可以开启多线程处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693896340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380418,"user_name":"Geek8004","can_delete":false,"product_type":"c1","uid":2328971,"ip_address":"中国香港","ucode":"B3828F6414BDB0","user_header":"","comment_is_top":false,"comment_ctime":1693536558,"is_pvip":false,"replies":[{"id":138604,"content":"就是我提到的，当你要处理这个消息的时候，你要先确保它还是处于一种未支付的状态。\n\n最简单的做法就是加分布式锁，检测状态，然后决定要不要丢弃消息。高级一点的就是用乐观锁。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1693896260,"ip_address":"广东","comment_id":380418,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"老师,秒杀场景中,用户创建了未付款的订单,发了一条30min超时未支付取消订单的消息.在设置不同的延时分区这个方案里面,假如此时用户30分钟已经已经支付了,订单状态已经变成了已支付.但是30分钟到期了未付款的消息收到了,此时应该怎么办? 是不是用状态机? 状态只能往大的方向更改? 还是应该怎么处理呢?例如去把MQ里面的未支付的超时消息删除? 我觉得应该是判断当前订单的状态,如果是已支付或者已取消,那业务方就丢弃这个超时未支付的消息?","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627212,"discussion_content":"就是我提到的，当你要处理这个消息的时候，你要先确保它还是处于一种未支付的状态。\n\n最简单的做法就是加分布式锁，检测状态，然后决定要不要丢弃消息。高级一点的就是用乐观锁。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693896260,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380323,"user_name":"Geek8004","can_delete":false,"product_type":"c1","uid":2328971,"ip_address":"中国香港","ucode":"B3828F6414BDB0","user_header":"","comment_is_top":false,"comment_ctime":1693376400,"is_pvip":false,"replies":[{"id":138537,"content":"不是在 Kafka 上设定的，而是你自己设计的时候，比如说你规定，分区 0 就是延迟 1分钟。然后要求生产者把延迟一分钟的发到分区 0。Kafka 没这种功能。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1693488198,"ip_address":"广东","comment_id":380323,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"每个分区设定了不同的延迟时间,比如kafka中这个参数在哪里设置?没操作过这种给分区设置不同的参数?","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626918,"discussion_content":"不是在 Kafka 上设定的，而是你自己设计的时候，比如说你规定，分区 0 就是延迟 1分钟。然后要求生产者把延迟一分钟的发到分区 0。Kafka 没这种功能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693488198,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380065,"user_name":"tyro","can_delete":false,"product_type":"c1","uid":1955472,"ip_address":"广东","ucode":"D35A80546E7A2F","user_header":"https://static001.geekbang.org/account/avatar/00/1d/d6/90/2b949521.jpg","comment_is_top":false,"comment_ctime":1692920416,"is_pvip":false,"replies":[{"id":138557,"content":"1. 不同业务需要不同的延迟时间。比如说我订单超时取消是 10 分钟，但是我别的业务可能就是延迟半小时。设置 2min30 就是你说的理由\n2. 你同一个分区都是一样的延迟消息，所以你可以拉一条就提交一条。\n3. 可以用 topic 的，并且也没多少问题。你并发极高就应该用 topic。\n4. 这有点绕。要点是这个表要根据你的发送时间来交替，而不是接收时间，并且要禁止你的延迟时间很长。比如说，禁止你的延迟时间超过一个小时。举个例子，当 23:39 的时候，你收到一个消息，延迟半小时，这个时候这条数据要写入 tab_1，但是此刻你读的，还是 tab_0。然后等 00:00 一过，你就可以清空 tab_0了。注意这个时候，00:00，因为你限制延迟时间不能超过 1 小时，也就是，你当下用的是 tab_1 的时候，过来的消息是不可能晚于 01:00 的，因此就不会用到 tab_0。\n5. 对的，无法兼顾。又要有序，又要延迟，是很难处理的。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1693490263,"ip_address":"广东","comment_id":380065,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"老师本章的内容我有几个疑问：\nQ1. “不同分区设置不同的延迟时间”中，比如3min的分区。消费者每次拉取消息后pause的时间，是不是这条消息的生产时间距离这个3min延迟的剩余时间？这样才能做到消费者源源不断消费吧？应该不是每个消息，不管三七二十一的都pause 3min吧？不知道我理解的对不对？后面说的消息积压的问题，应该也是因为吞吐量过高，单个消费者转发都转发不过来导致的积压吧？所以才需要设置2min30，3min30多个接近的分区分摊压力？\nQ2 针对Q1，如果我想进一步使用批量操作拉取和消费消息，这时候提交offset是不是只能这一批消息全部满足延迟并进行转发成功后，才能提交offset并拉取下一批消息？\nQ3 思考题1中，我觉得把延迟阶梯从分区换成topic好像更好一点啊，每个topic可以有多个分区多个消费者了，消费能力更高。balance问题也不会导致业务股长，只是单纯的性能问题？那为什么文章不使用topic方案呢？\nQ4 交替表方案中，比如我一天为单位交替写。有一批30min延迟的消息刚好在23:59这种要替换表到tab1的临界时间写到tab0来。那我替换表的时候，这些消息怎么办？这时候也不能truncate吧？就算不truncate，下次切换消费已经是24h后了？\nQ5 在分库分表方案中，为了平衡不同biz_topic的负载，提出了使用轮询插入的方案。后面又说为了满足消息的有序性，有回到biz_topic分库分表了？所以这是两个角度的trade-off，无法兼顾嘛？\n望老师解惑，谢谢🙏","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626939,"discussion_content":"1. 不同业务需要不同的延迟时间。比如说我订单超时取消是 10 分钟，但是我别的业务可能就是延迟半小时。设置 2min30 就是你说的理由\n2. 你同一个分区都是一样的延迟消息，所以你可以拉一条就提交一条。\n3. 可以用 topic 的，并且也没多少问题。你并发极高就应该用 topic。\n4. 这有点绕。要点是这个表要根据你的发送时间来交替，而不是接收时间，并且要禁止你的延迟时间很长。比如说，禁止你的延迟时间超过一个小时。举个例子，当 23:39 的时候，你收到一个消息，延迟半小时，这个时候这条数据要写入 tab_1，但是此刻你读的，还是 tab_0。然后等 00:00 一过，你就可以清空 tab_0了。注意这个时候，00:00，因为你限制延迟时间不能超过 1 小时，也就是，你当下用的是 tab_1 的时候，过来的消息是不可能晚于 01:00 的，因此就不会用到 tab_0。\n5. 对的，无法兼顾。又要有序，又要延迟，是很难处理的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693490263,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379871,"user_name":"不吃辣👾","can_delete":false,"product_type":"c1","uid":1333649,"ip_address":"浙江","ucode":"B25E0725B5E85F","user_header":"https://static001.geekbang.org/account/avatar/00/14/59/91/fa2d8bb2.jpg","comment_is_top":false,"comment_ctime":1692630266,"is_pvip":false,"replies":[{"id":138408,"content":"使用 mysql 来转储的时候。不需要 sleep 一天啊，两条消息都丢到数据库里面，记录一个预期发送时间，然后轮询。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1692892855,"ip_address":"广东","comment_id":379871,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"想不通最后mysql的亮点方案中 延迟消费者到底还要不要延迟，假如生产者向delay_topic先发了一条需要延迟一天的消息，紧接着发送了一条需要延迟10s的消息。这种情况下延迟消费者要睡一天，后面的需要延迟10s的得不到转存到数据库中了。延迟消费者怎么体现延迟呢？","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626423,"discussion_content":"使用 mysql 来转储的时候。不需要 sleep 一天啊，两条消息都丢到数据库里面，记录一个预期发送时间，然后轮询。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1692892855,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379862,"user_name":"不吃辣👾","can_delete":false,"product_type":"c1","uid":1333649,"ip_address":"浙江","ucode":"B25E0725B5E85F","user_header":"https://static001.geekbang.org/account/avatar/00/14/59/91/fa2d8bb2.jpg","comment_is_top":false,"comment_ctime":1692626583,"is_pvip":false,"replies":[{"id":138407,"content":"这是最保险的做法。\n\n不过如果你消息比如说都是统一延迟三十分钟，那么它们自然就会落到同一个分区上，那么你就可以有多个分区。但是如果消息又要有序，又是随机延迟时间，就比较尴尬了。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1692892808,"ip_address":"广东","comment_id":379862,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"考虑延迟消息有序性的话，topic 的分区只能有一个。","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626422,"discussion_content":"这是最保险的做法。\n\n不过如果你消息比如说都是统一延迟三十分钟，那么它们自然就会落到同一个分区上，那么你就可以有多个分区。但是如果消息又要有序，又是随机延迟时间，就比较尴尬了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1692892809,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379483,"user_name":"Geek_18dfaf","can_delete":false,"product_type":"c1","uid":1543018,"ip_address":"浙江","ucode":"CFC27E78220E2D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqw0R25Bt0iahFhEHfnxmzr9iaZf0eLsDQtFUJzgGkYwHTqicU9TydMngrJ4yL7D50awD2VibHBAdqplQ/132","comment_is_top":false,"comment_ctime":1692007674,"is_pvip":false,"replies":[{"id":138254,"content":"不是的。因为你可以预期生产者那边源源不断产生消息，那么消费者这边也会有源源不断的消息。对于消费者来说，它取出来第一条消息，看看要睡多久。\n除非你说你的 QPS 很低很低，半小时产生一条，不然是不会睡那么久的","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1692078241,"ip_address":"广东","comment_id":379483,"utype":1}],"discussion_count":6,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"5 个分区：延迟时间分别是 1min、3min、5min、10min、30min，针对这里有个疑问，\n总不能在消费端 睡30min后再提交到正常的消费队列吧","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625690,"discussion_content":"不是的。因为你可以预期生产者那边源源不断产生消息，那么消费者这边也会有源源不断的消息。对于消费者来说，它取出来第一条消息，看看要睡多久。\n除非你说你的 QPS 很低很低，半小时产生一条，不然是不会睡那么久的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1692078241,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2986043,"avatar":"https://static001.geekbang.org/account/avatar/00/2d/90/3b/791d0f5e.jpg","nickname":"进击的和和","note":"","ucode":"8978AF077FA6AD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626680,"discussion_content":"你好作者 这里能举个例子吗?不是很明白","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693275635,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"","child_discussion_number":4,"child_discussions":[{"author":{"id":1132327,"avatar":"https://static001.geekbang.org/account/avatar/00/11/47/27/bff28e1d.jpg","nickname":"赵旭","note":"","ucode":"C08CE16441E116","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2986043,"avatar":"https://static001.geekbang.org/account/avatar/00/2d/90/3b/791d0f5e.jpg","nickname":"进击的和和","note":"","ucode":"8978AF077FA6AD","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626902,"discussion_content":"比如一个队列是30分钟的延迟队列，现在时间是8:00:00，一条消息延迟半个小时就是8:30:00，下一条消息8:00:01产生的，消费时间是8:30:01。消费者那边8点开始取数据，取到第一条消费时间为8:30:00，那就sleep 1800s,到8:30:00在实际消费，消费完取下一条8:30:01消费，只需要sleep 1s就行了","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1693477865,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":626680,"ip_address":"北京","group_id":0},"score":626902,"extra":""},{"author":{"id":2986043,"avatar":"https://static001.geekbang.org/account/avatar/00/2d/90/3b/791d0f5e.jpg","nickname":"进击的和和","note":"","ucode":"8978AF077FA6AD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1132327,"avatar":"https://static001.geekbang.org/account/avatar/00/11/47/27/bff28e1d.jpg","nickname":"赵旭","note":"","ucode":"C08CE16441E116","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626955,"discussion_content":"谢谢哈 就是每个消息里还保存了生产时间哈","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1693529119,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":626902,"ip_address":"四川","group_id":0},"score":626955,"extra":""},{"author":{"id":2721761,"avatar":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","nickname":"范飞扬","note":"","ucode":"A665DF46833A81","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1132327,"avatar":"https://static001.geekbang.org/account/avatar/00/11/47/27/bff28e1d.jpg","nickname":"赵旭","note":"","ucode":"C08CE16441E116","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629427,"discussion_content":"终于懂了，感谢同学","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697039960,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":626902,"ip_address":"广东","group_id":0},"score":629427,"extra":""}]}]},{"had_liked":false,"id":379245,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1691589477,"is_pvip":false,"replies":[{"id":138215,"content":"1. 有很多的，什么 xxj-job 之类的。然后很多公司也有自研。\n2. 一个 topic 上有多个分区，然后你可以有多个 topic。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1691755160,"ip_address":"广东","comment_id":379245,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"请教老师两个问题：\nQ1：分布式任务平台，有开源的吗？或者商用的第三方平台？\nQ2：基础思路部分的“分区”不是“topic”吗？我一直是理解成topic的，但思考题里面又要区分“分区”和”topic”。 那“分区”到底是指什么？","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625498,"discussion_content":"1. 有很多的，什么 xxj-job 之类的。然后很多公司也有自研。\n2. 一个 topic 上有多个分区，然后你可以有多个 topic。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1691755160,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379232,"user_name":"humor","can_delete":false,"product_type":"c1","uid":1181867,"ip_address":"浙江","ucode":"9B48C4C7BEC92C","user_header":"https://static001.geekbang.org/account/avatar/00/12/08/ab/caec7bca.jpg","comment_is_top":false,"comment_ctime":1691579620,"is_pvip":false,"replies":[{"id":138212,"content":"主要是表交替的时候，你每张表的数据可以很快被清理掉。相比你一张表从头存到尾，的确要好一些。但是相比分库分表或者分区表，就不是很好。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1691754813,"ip_address":"广东","comment_id":379232,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"表交替可以支撑高并发的原理是什么呢？不管用不用表交替，在同一时刻，都是只会读写一张表吧，瓶颈都在一张表上","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625494,"discussion_content":"主要是表交替的时候，你每张表的数据可以很快被清理掉。相比你一张表从头存到尾，的确要好一些。但是相比分库分表或者分区表，就不是很好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1691754814,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379203,"user_name":"3.0的A7","can_delete":false,"product_type":"c1","uid":1211991,"ip_address":"北京","ucode":"23C5F02B45CE39","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/57/8c1051b6.jpg","comment_is_top":false,"comment_ctime":1691547153,"is_pvip":false,"replies":[{"id":138160,"content":"你们是不是用的云服务版本？我记得很多云厂商都二开了，搞出来了延迟队列。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1691564575,"ip_address":"广东","comment_id":379203,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"rocketmq不支持延迟队列吗？是不是开源版的不支持\n我们前两年就用到了rocketmq的延迟队列啊，没有做特殊开发。","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625306,"discussion_content":"你们是不是用的云服务版本？我记得很多云厂商都二开了，搞出来了延迟队列。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1691564575,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1195258,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3c/fa/e2990931.jpg","nickname":"文敦复","note":"","ucode":"B8F4A6BD5D7805","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":630044,"discussion_content":"开源版本支持延时队列的。开源版本只支持固定几个刻度的延迟，云上支持自定义延迟。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698053064,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":625306,"ip_address":"四川","group_id":0},"score":630044,"extra":""}]}]},{"had_liked":false,"id":388141,"user_name":"天天有吃的","can_delete":false,"product_type":"c1","uid":1604355,"ip_address":"福建","ucode":"6267FE8E68DEE5","user_header":"https://static001.geekbang.org/account/avatar/00/18/7b/03/03583011.jpg","comment_is_top":false,"comment_ctime":1709482245,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100551601,"comment_content":"RocketMQ不是很早就支持延迟消息了吗，4.9？18个level 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h","like_count":1},{"had_liked":false,"id":395039,"user_name":"木几丶","can_delete":false,"product_type":"c1","uid":2420294,"ip_address":"福建","ucode":"FFDB958DA64F8C","user_header":"https://static001.geekbang.org/account/avatar/00/24/ee/46/7d65ae37.jpg","comment_is_top":false,"comment_ctime":1729154821,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100551601,"comment_content":"我们的外部存储用的Redis+时间轮，ES兜底，延迟消费者消费后，写入Redis和ES，入轮，然后用时间轮推进。常规情况下，使用Redis zset+时间轮，Redis故障用ES兜底和恢复","like_count":0},{"had_liked":false,"id":394543,"user_name":"彭俊","can_delete":false,"product_type":"c1","uid":1054541,"ip_address":"广东","ucode":"FBEDBCCF22F1D0","user_header":"https://static001.geekbang.org/account/avatar/00/10/17/4d/7e13ec93.jpg","comment_is_top":false,"comment_ctime":1727100489,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100551601,"comment_content":"能否 消费者消费消息后，自己落db延迟处理呢","like_count":0},{"had_liked":false,"id":392675,"user_name":"Geek_bf3d13","can_delete":false,"product_type":"c1","uid":3688361,"ip_address":"浙江","ucode":"96F6762B465396","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ALtd8gWcgYyibSlHQgtwztDSApbwh47vlQoatCicU55svnC3jhciaBGfuzBia70ukicN5TJjcBsHBLkZRTSSMCcn48jM02lYsNq8cmSibMYen8Wrg/132","comment_is_top":false,"comment_ctime":1721530740,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100551601,"comment_content":"还有一个一致性问题，就是如何保证延迟消费者不丢失数据，延迟消费者具体要怎么实现","like_count":0},{"had_liked":false,"id":391334,"user_name":"白菜炒五花肉","can_delete":false,"product_type":"c1","uid":1519828,"ip_address":"浙江","ucode":"0D4CBD221C2880","user_header":"https://static001.geekbang.org/account/avatar/00/17/30/d4/6eb8f5af.jpg","comment_is_top":false,"comment_ctime":1718023316,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100551601,"comment_content":"老师，kafka 消费的时候，一次性拉取多条消息，如果是不同分区的消息，sleep 的时间怎么决定","like_count":0},{"had_liked":false,"id":389898,"user_name":"瀚海","can_delete":false,"product_type":"c1","uid":2062203,"ip_address":"上海","ucode":"E64C22F3F6D285","user_header":"https://static001.geekbang.org/account/avatar/00/1f/77/7b/338c4617.jpg","comment_is_top":false,"comment_ctime":1713830403,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100551601,"comment_content":"方案过于复杂     rocktmq的延迟消息是怎么实现的呢","like_count":0,"discussions":[{"author":{"id":1744257,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/9d/81/d748b7eb.jpg","nickname":"千锤百炼领悟之极限","note":"","ucode":"224B5CF2101716","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648002,"discussion_content":"RocketMQ的延迟消息是通过消息的延迟级别（delay level）来实现的，类似文章中“分区设置不同延迟时间”这个方案。\n\n具体实现是在发送消息时，可以指定消息的延迟级别，RocketMQ会根据指定的级别将消息存储在对应的延迟队列中，然后在延迟时间到达后再投递给消费者。延迟级别的设置可以在RocketMQ的配置中进行调整。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720925999,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}