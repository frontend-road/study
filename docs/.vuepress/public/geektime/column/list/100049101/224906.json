{"id":224906,"title":"07 | 同事老打脸说数据有问题，该怎么彻底解决？","content":"<p>你好，我是郭忆。</p><p>上一节课，我带你从模型设计层面，逐步将分散、杂乱、烟囱式的小数仓整合成了可复用、可共享的数据中台，数据研发效能提升了一倍。那是不是交付数据足够快，使用数据的人就满意了？ 当然不是，来看发生在我身边的一件事儿。</p><p>供应链部门运营陈英俊（化名）每天上班第一件事情，就是打开供应链辅助决策系统（数据产品），根据系统上给出的商品库存数据、区域下单数据，制订商品的采购计划，然后发送给供货商。可是今天当他准备工作时，突然发现系统中部分商品库存数据显示为0，他第一时间将问题反馈给了数据部门。与此同时，与他一样无法工作的还有供应链部门的其他50多名运营。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/7d/bcd45f4e938efe5e215e505f3e4f0b7d.jpg?wh=1142*555\" alt=\"\" title=\"图1 ADS层商品库存表加工链路图\"></p><p>接到投诉后，负责库存域的数据开发郝建（化名）立即开始定位，首先就“商品库存”指标的产出表ads_wms_sku_stock_1d进行排查，确认它产出任务没有问题，是这个任务的上游输入表数据有问题，引发下游表数据异常。</p><p>从数据血缘图中你可以看到，ads_wms_sku_stock_1d上游有20多张表，郝建逐层校验是哪个表的数据出现问题，结果锁定在了dwd_wms_inbound_stock_df。这张表的产出任务在前一天有一次线上变更，任务代码存在漏洞，对部分商品入库数据格式解析异常，但是没有将异常抛出，导致产出数据表dwd_wms_inbound_stock_df 数据异常，进而影响了所有下游表。</p><!-- [[[read_end]]] --><p>排查问题用了近3个小时。</p><p>既然问题定位清楚，就要开始修复的流程。修改好代码后，郝建重新跑了dwd_wms_inbound_stock_df的产出任务，确认数据没有问题，然后需要重跑该任务下游链路上的5个任务（图中红色表的产出任务）。</p><p>运行完任务用了5个小时。</p><p>经过数据验证，确认没有问题，此时已经过去了将近9个小时。对于像陈英俊这样的运营来说，一天都无法工作。如果你是陈英俊， 对数据会满意吗？<strong>所以，光快还不够，还要保证质量。</strong></p><p>当然，这个例子暴露出这样几个问题：</p><ul>\n<li>数据部门晚于业务方发现数据异常，被投诉后才发现问题。</li>\n<li>出现问题后，数据部门无法快速定位到数据异常的根源，排查用了较长的时间。</li>\n<li>故障出现在数据加工链路的上游顶端，出现问题没有第一时间报警处理，导致问题修复时，所有下游链路上的任务都要运行，修复时间成本非常高。</li>\n</ul><p><strong>这些问题最终导致了数据长时间不可用。</strong>那如何解决这些问题，确保数据高质量的交付呢？ 首先，你要了解产生这些问题的根源，毕竟认识问题才能解决问题。</p><h2>数据质量问题的根源</h2><p>在网易电商业务数据中台构建之初，我对数据团队一年内，记录在案的321次数据质量事件做了逐一分析，对这些事件的原因进行了归纳，主要有下面几类。这里多说一句，如果你想改进数据质量，不妨也对过去踩过的坑做一次复盘，归一下类，看看问题都出在哪里，然后制定针对性的改进计划。</p><p><img src=\"https://static001.geekbang.org/resource/image/91/68/9191c7dd526022341edaf98c5be2be68.jpg?wh=1142*557\" alt=\"\"></p><h4>业务源系统变更</h4><p>数据中台的数据来源于业务系统，而源系统变更一般会引发3类异常情况，</p><p><strong>首先是源系统数据库表结构变更。</strong>例如业务系统新版本发布上线，对数据库进行了表结构变更，增加了一个字段，同时对部分字段的类型、枚举值进行了调整。这种表结构变更没有通知到数据团队，导致数据同步任务或者数据清洗任务异常，进而影响了下游数据产出。</p><p><img src=\"https://static001.geekbang.org/resource/image/5d/2c/5d679674ecb108d152251d1777c8722c.jpg?wh=1142*512\" alt=\"\" title=\"图2 日志服务器扩容误操作引发数据异常\"></p><p><strong>第二个是源系统环境变更。</strong>我经常在大促期间见到这种情况，其中的典型是前端用户行为埋点日志量暴增，系统管理员紧急对服务器进行扩容，上线了5台新的服务器，但是没有配置这5台服务器的日志同步任务，结果导致数据侧少了这5台服务器的数据，最终影响了数据计算结果的准确性。</p><p><strong>最后一个是源系统日志数据格式异常。</strong>这种情况通常出现在前后端埋点日志中。业务系统发布上线引入埋点BUG，导致IP格式出现了不符合约定的格式（比如，我们一般约定的IP格式是166.111.4.129，结果出现了166.111.4.null），最终也会导致计算结果错误。</p><h4>数据开发任务变更</h4><p>这种情况在数据质量事件中占到了60%以上，而它大多数是由于数据开发的纰漏引发的，来看几个你比较熟悉的例子：</p><ul>\n<li>任务发布上线，代码中引用的测试库没有修改为线上库，结果污染了线上数据；</li>\n<li>任务发布上线，代码中使用了固定分区，没有切换为“${azkaban.flow.1.days.ago}”，导致数据异常；</li>\n<li>前面例子中，数据格式处理错误，代码忽略了异常，导致数据错误；</li>\n<li>任务配置异常，它通常表现在任务没有配置依赖，前一个任务没有运行完，后一个任务就开始运行，输入数据不完整，导致下游数据产出错误。</li>\n</ul><h4>物理资源不足</h4><p><img src=\"https://static001.geekbang.org/resource/image/c4/89/c45309ce7dcc381aab2c5f4ca770f589.jpg?wh=1142*470\" alt=\"\" title=\"图3  Hadoop多队列任务提交\"></p><p>在多租户下，Hadoop生态的大数据任务（MR，Hive，Spark）一般运行在yarn管理的多个队列上（调度器为CapacityScheduluer），每个队列都是分配了一定大小的计算资源（CPU、内存）。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/41/c1e5cdb298ff76bc0b68dce656190541.jpg?wh=1142*555\" alt=\"\" title=\"图4 物理资源不足导致任务延迟\"></p><p>我展示了两种常见的物理资源不足，导致任务延迟产出的情况。</p><h4>基础设施不稳定</h4><p>从数量上来看，这类异常不算多，但影响却是全局性的。我们曾经在大促期间，碰到了一个<a href=\"https://issues.apache.org/jira/browse/HDFS-10453\">Hadoop 2.7 NameNode的BUG，</a>造成HDFS整个服务都停止读写，最终通过临时补丁的方式才修复。</p><p>总的来说，出现问题并不可怕，可怕的是，我们没有及时发现问题，尽快恢复服务，举一反三地通过流程和技术手段，降低问题出现的概率。所以接下来我们就来看一看，如何提高数据质量？</p><h2>如何提高数据质量？</h2><p>我认为，要想提升数据质量，最重要的就是“早发现，早恢复”：</p><ul>\n<li>早发现，是要能够先于数据使用方发现数据的问题，尽可能在出现问题的源头发现问题，这样就为“早恢复”争取到了大量的时间。</li>\n<li>早恢复，就是要缩短故障恢复的时间，降低故障对数据产出的影响。</li>\n</ul><p>那具体如何做到这两个早呢？我总结了一套数据质量建设的方法，包括这样几个内容。</p><h4>添加稽核校验任务</h4><p>在数据加工任务中，对产出表按照业务规则，设计一些校验逻辑，确保数据的完整性、一致性和准确性，这是提升数据质量最行之有效的方法。</p><p>通常建议你在数据产出任务运行结束后，启动稽核校验任务对数据结果进行扫描计算，判断是否符合规则预期。如果不符合，就根据提前设定的强弱规则，触发不同的处理流程。</p><p>如果是强规则，就立即终止任务加工链路，后续的任务不会执行，并且立即发出电话报警，甚至我们要求，关键任务还要开启循环电话报警，直到故障被认领；如果是弱规则，任务会继续执行。但是存在风险，这些风险会通过邮件或者短信的方式，通知到数据开发，由人来进一步判断风险严重程度。</p><p><img src=\"https://static001.geekbang.org/resource/image/06/ea/06243ebbbfa6b46fc61803c84cb8edea.jpg?wh=1142*900\" alt=\"\" title=\"图5 稽核校验执行流程图\"></p><p>那具体要加哪些稽核规则呢？</p><ul>\n<li>\n<p><strong>完整性规则。</strong>主要目的是确保数据记录是完整的，不丢失。常见的稽核规则有表数据量的绝对值监控和波动率的监控（比如表波动超过20%，就认为是异常）。还有主键唯一性的监控，它是判断数据是否有重复记录的监控规则，比较基础。除了表级别的监控，还有字段级别的监控（比如字段为0、为NULL的记录）。</p>\n</li>\n<li>\n<p><strong>一致性规则。</strong>主要解决相关数据在不同模型中一致性的问题。商品购买率是通过商品购买用户数除以商品访问uv计算而来的，如果在不同的模型中，商品购买用户数是1W、商品访问uv10W，商品购买率20%，那这三个指标就存在不一致。</p>\n</li>\n<li>\n<p><strong>准确性规则。</strong>主要解决数据记录正确性的问题。常见的稽核规则有，一个商品只能归属在一个类目，数据格式是不是正确的IP格式，订单的下单日期是还没有发生的日期等等。</p>\n</li>\n</ul><p>它们是强规则还是弱规则，取决于业务对上述异常的容忍度（比如涉及到交易、支付跟钱相关的，一般都会设置为强规则，对于一些偏行为分析的，一般都是弱规则）。</p><h4>建立全链路监控</h4><p>在06讲中，我强调数据中台的模型设计是分层的，确保中间结果可以被多个模型复用。</p><p>不过这会导致数据加工的链路变长，加工链路的依赖关系会非常复杂，最终当下游表上的某个指标出现问题，排查定位问题的时间都会比较长。<strong>所以，我们有必要基于数据血缘关系，建立全链路数据质量监控。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/4e/02/4e2f5518a2273a0295352eae7f2a4c02.jpg?wh=1142*383\" alt=\"\" title=\"图6 全链路数据质量监控\"></p><p>从这个图中你可以看到，业务系统的源数据库表是起点，经过数据中台的数据加工链路，产出指标“黑卡会员购买用户数”，数据应用是链路的终点。</p><p>对链路中每个表增加稽核校验规则之后，当其中任何一个节点产出的数据出现异常时，你能够第一时间发现，并立即修复，做到早发现、早修复。另外，即使是使用方反馈经营分析上的黑卡会员购买用户数，相较于昨天数据大幅下降超过30%，你也可以快速判定整个指标加工链路上节点是否运行正常，产出任务是否有更新，提高了问题排查速度。</p><h4>通过智能预警，确保任务按时产出</h4><p>在数据质量问题中，我提到会存在物理资源不足，导致任务产出延迟的情况。在网易，所有数据中台产出的指标要求6点前产出。为了实现这个目标，我们需要对指标加工链路中的每个任务的产出时间进行监控，基于任务的运行时间和数据血缘，对下游指标产出时间进行实时预测，一旦发现指标无法按时产出，则立即报警，数据开发可以终止一些低优先级的任务，确保核心任务按时产出。</p><h4>通过应用的重要性区分数据等级，加快恢复速度</h4><p>稽核校验会消耗大量的资源，所以只有核心任务才需要。核心任务的定义是核心应用（使用范围广、使用者管理级别高）数据链路上的所有任务。</p><h4>规范化管理制度</h4><p>讲到这儿，你可能会问：数据质量取决于稽核规则的完善性，如果数据开发没有添加，或者添加的规则不全，是不是就达不到早发现、早恢复？</p><p>这个问题戳中了要害，就是规则的完备性如何来保障。在我看来，这不仅仅是一个技术问题，也涉及管理。在网易，我们会制定一些通用的指导规则（比如，所有数据中台维护的表都需要添加主键唯一性的监控规则），但这些通用规则往往与业务关系不大。如果涉及业务层面，就要由数据架构师牵头，按照主题域、业务过程，对每个表的规则进行评审，决定这些规则够不够。</p><p>那我想建议你，如果要做稽核校验，可以通过组建数据架构师团队，由这个团队负责核心表的规则审核，确保规则的完备性。</p><p>那么当你按照这几个方法建立了数据质量体系之后，要如何验证体系是否有效呢?</p><h2>如何衡量数据质量？</h2><p>做数据治理，我一直奉行“效果可量化”的原则，否则这个治理做到什么程度，很难衡量。那么如何评价数据质量是否有改进呢？除了故障次数，你还可以有这样几个指标。</p><ul>\n<li>\n<p>4点半前数据中台核心任务产出完成率。这个指标是一个综合性指标，如果任务异常，任务延迟，强稽核规则失败，都会导致任务无法在规定时间前产出。</p>\n</li>\n<li>\n<p>基于稽核规则，计算表级别的质量分数。根据表上稽核规则的通过情况，为每个表建立质量分数，对于分数低的表，表负责人要承担改进责任。</p>\n</li>\n<li>\n<p>需要立即介入的报警次数，通常以开启循环报警的电话报警次数为准。对于核心任务，任务异常会触发循环电话报警，接到报警的数据开发需要立即介入。</p>\n</li>\n<li>\n<p>数据产品SLA。每个数据产品上所有指标有没有在9点产出，如果没有，开始计算不可用时间，整体可以按照不同数据产品的重要性进行折算，99.8%是数据产品一个相对比较好的SLA。</p>\n</li>\n</ul><p>不过，技术和规范最终需要依靠产品来帮助落地，在网易内部，有一个数据质量中心的 产品，通过介绍这个产品，我希望能给你一个参考，如何去设计一个数据质量中心，或者在选型的时候，数据质量中心必须具备的功能。</p><h2>数据质量中心</h2><p>数据质量中心（以下简称DQC）的核心功能是稽核校验和基于数据血缘的全链路数据质量监控。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/b1/f39b371750e39eabde620ced05d0f0b1.jpg?wh=1142*690\" alt=\"\"></p><p>DQC的首页是质量大屏，提供了稽核规则的数量、表的覆盖量以及这些规则的执行通过情况。通过这些数据，你就能跟你的老板讲清楚，目前数据质量水平建设如何？目标是多少？距离目标还有多少差距。</p><p><img src=\"https://static001.geekbang.org/resource/image/70/ef/702ec67c9cb0f5b572a64af4289c31ef.jpg?wh=1142*681\" alt=\"\"></p><p>在DQC 中创建稽核规则非常简单，DQC 内置了大量的基础规则，例如IP 字段格式校验，主键唯一性校验，表行数波动率校验，同时还提供了自定义SQL的方式，允许业务层面的规则创建，例如我们前面提到的一致性规则中，两个指标相除等于第三个指标，就可以通过自定义SQL解决。</p><p><img src=\"https://static001.geekbang.org/resource/image/dd/c6/dd914160281cd1872eedb0717fb6c6c6.jpg?wh=1142*732\" alt=\"\"></p><p>DQC 还提供了全链路监控的功能，覆盖了从数据导入、数据加工、模型产出、指标、到数据应用的完整链路。绿色节点代表数据正常，蓝色节点代表数据正在产出中，红色节点代表数据异常，灰色节点代表产出任务还未被调度到。通过这个监控，大幅提高了问题发现和定位的速度。</p><p>所以你可以发现，一个好用的DQC， 必须要具备的功能就是质量度量、稽核规则管理以及全链路监控。</p><h2>课堂总结</h2><p>本节课，我从数据质量问题的根源入手，带你分析了背后的原因，和五种提高数据质量的方法，在课程结束前，我再强调几个重点：</p><ul>\n<li>数据质量治理必须要做到全链路，从业务系统的数据源到指标所在的应用，这样可以提前发现问题，将故障消灭在摇篮中；</li>\n<li>根据应用的优先级和全链路血缘关系，圈定核心任务，要确保核心任务的稽核规则全覆盖，优先保障核心任务的按时产出，在资源紧缺时，有必要停止非核心任务；</li>\n<li>稽核规则的完备性，可以通过数据架构师团队对每个域下的核心表进行评审的方式保障，同时问题回溯和复盘，也可以不断地完善。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/33/6c/33c40b8caedcd08edf50efeddbe6f26c.jpg?wh=1142*857\" alt=\"\"></p><h2>思考时间</h2><p>我提到数据完整性可以通过数据记录的波动率来监控，如果超过20%的波动，应该被视为异常。但是你有没有想过，这种也存在误判的情况，尤其是在双十一大促期间，大概率数据稽核规则都是异常的，此时你又该怎么办？</p><p>最后，感谢你的阅读，如果这节课让你有所收获，也欢迎你将它分享给更多的人。</p>","comments":[{"had_liked":false,"id":207417,"user_name":"吴科🍀","can_delete":false,"product_type":"c1","uid":1112547,"ip_address":"","ucode":"8F2C317887A323","user_header":"https://static001.geekbang.org/account/avatar/00/10/f9/e3/2529c7dd.jpg","comment_is_top":false,"comment_ctime":1587084932,"is_pvip":false,"replies":[{"id":"77730","content":"你说的都是大实话。<br><br>我真心觉得数据开发真是苦逼的职位，白天忙需求，晚上接报警，因为指标业务口径不一致、数据质量问题天天被人怼，处于弱势地位，工具产品也不到位，所以需要数据中台来拯救他们。<br><br>感谢你的阅读，期待与你在留言区再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587305514,"ip_address":"","comment_id":207417,"utype":1}],"discussion_count":5,"race_medal":0,"score":"96076365444","product_id":100049101,"comment_content":"数据部门作为下游系统，上游业务系统变更，往往不能第一时间通知，第二天跑批是才发现，早晨五点起来处理太平常不过了。<br>资源抢占也时常发生，分析师临时加了一个任务跑全量的数据，还加到了资源主列队中，第二天所有跑批都延迟了。<br>数据部门，要规范各种数据相关的变更才能解决。实际中，我们数据部门都是弱势的。<br>😂","like_count":22,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492116,"discussion_content":"你说的都是大实话。\n\n我真心觉得数据开发真是苦逼的职位，白天忙需求，晚上接报警，因为指标业务口径不一致、数据质量问题天天被人怼，处于弱势地位，工具产品也不到位，所以需要数据中台来拯救他们。\n\n感谢你的阅读，期待与你在留言区再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587305514,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1609720,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ0SDra4ZSspknscVIV8ZKQia684AjyzVaBiceT84p0Vic4GhrW6pKMkbDWBSwqbrdbaksr1nqKbF7rg/132","nickname":"Geek_b1b6b1","note":"","ucode":"927923A36F22B1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377402,"discussion_content":"不应该做资源隔离吗？线上任务跑的资源应该独享一个资源池吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622628665,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1336073,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoiaP1gptuBzj28sTEeoBdfgbr15YM8FXzuibzTdsBLTTpsOZ3TP3d28EURAP2HfQpus6Bz7CibNQyPw/132","nickname":"Geek_456ed3","note":"","ucode":"A8A793A4CC9A2D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":240435,"discussion_content":"首先数据应当有完整测试环境，上游上线之前肯定要走测试环境，可以暴露大部分问题。对于某些直接上线正式环境的情况，可以考虑在下班后跑一下当天的数据，这样也可以提早感知上游变化。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587363520,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1945987,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/b1/83/c6dd1322.jpg","nickname":"你好","note":"","ucode":"F6ED43EF5FD164","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1336073,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoiaP1gptuBzj28sTEeoBdfgbr15YM8FXzuibzTdsBLTTpsOZ3TP3d28EURAP2HfQpus6Bz7CibNQyPw/132","nickname":"Geek_456ed3","note":"","ucode":"A8A793A4CC9A2D","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":249662,"discussion_content":"对于执行时间，性能问题，测试环境和生产环境差别太大，参考意义不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587951416,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":240435,"ip_address":""},"score":249662,"extra":""}]},{"author":{"id":1945778,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/zgnl95XDvXnicWoGibeAdicWrBYj8UgYGAayxcrJcaFLSbENX402AsVCtRibg3G1tuWrpZwymicribdug9PaHMeGK1Xw/132","nickname":"Geek_f071bc","note":"","ucode":"B98A0BD2B4620A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":239217,"discussion_content":"上游业务系统变更。我个人觉得可以在st层加监控，做t,t-1上游元数据的check发现。\n资源的问题，可以为数据申请专门的队列。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587285205,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207664,"user_name":"大北","can_delete":false,"product_type":"c1","uid":1201843,"ip_address":"","ucode":"A1125F0CA4E75D","user_header":"https://static001.geekbang.org/account/avatar/00/12/56/b3/29c814af.jpg","comment_is_top":false,"comment_ctime":1587133180,"is_pvip":true,"replies":[{"id":"77724","content":"赞， 确实基于AI去判断稽核失败到底是正常失败还是异常失败，是一条可行的路，至少可以降低人接入的频率。Good!<br><br>感谢你的阅读，期待与你再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587304918,"ip_address":"","comment_id":207664,"utype":1}],"discussion_count":1,"race_medal":0,"score":"44536806140","product_id":100049101,"comment_content":"DQC 全链路监控功能不错，还需要在各个节点上增加执行时间和时长，监控各节点执行时长也很重要。<br>节日促销异常问题：可以将监控的所有指标作为数据，通过机器学习训练出稽核规则模型，这样每天每次任务跑完的指标，由稽核规则模型判别这次任务是否为异常。","like_count":10,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492199,"discussion_content":"赞， 确实基于AI去判断稽核失败到底是正常失败还是异常失败，是一条可行的路，至少可以降低人接入的频率。Good!\n\n感谢你的阅读，期待与你再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587304918,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207577,"user_name":"leslie","can_delete":false,"product_type":"c1","uid":1324255,"ip_address":"","ucode":"798E7C1CC98CC2","user_header":"https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg","comment_is_top":false,"comment_ctime":1587111861,"is_pvip":false,"replies":[{"id":"77736","content":"感谢你的肯定和鼓励，看到能够引发你的一些思考，我真的觉得这篇文章还是很有意义的。<br><br>再来谈谈我在课后留的这个问题。其实稽核监控的规则设定，都是根据日常流量的正常波动范围来设定的，如果遇到大规模的引流或者重大促销，必然会不适用，所以如果不调整，大促期间，稽核监控基本全部会触发报警，也失去了早发现早恢复的用途。<br><br>所以一般在大促期间，我们一般会根据历史大促的经验，预留一些Buffer，来调整稽核监控。<br><br>感谢你的阅读，期待与你再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587306322,"ip_address":"","comment_id":207577,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14472013749","product_id":100049101,"comment_content":"数据规则，各自为战。最近刚碰到数据问题，历史遗留下来的数据库设计造成的坑；虽然之前指定了一堆相应的规则，可是提交层还是出了不少隐患。<br>提交到平台其实是一个不错的方式与方法：多方面皆有隐患，今天的课程倒是引发了一些思路和操作方式。<br>现在来回答今天的问题：误判的原因是源自对于某些表的数据量或者说设计上做复杂了，过于精细有时反而会细致过度；例如我们经常会看到云数据库的分析报告，正确率是站在365天的基础上而非每年的数个双11类似场景的，这其实就是需要对于项目的理解，数据的设计越复杂在特殊场景的适用性越难准确；这其实是一个蛮纠结的事情。<br>谢谢老师今天的分享，期待后续的分享。","like_count":3,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492175,"discussion_content":"感谢你的肯定和鼓励，看到能够引发你的一些思考，我真的觉得这篇文章还是很有意义的。\n\n再来谈谈我在课后留的这个问题。其实稽核监控的规则设定，都是根据日常流量的正常波动范围来设定的，如果遇到大规模的引流或者重大促销，必然会不适用，所以如果不调整，大促期间，稽核监控基本全部会触发报警，也失去了早发现早恢复的用途。\n\n所以一般在大促期间，我们一般会根据历史大促的经验，预留一些Buffer，来调整稽核监控。\n\n感谢你的阅读，期待与你再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587306322,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":295882,"user_name":"Geek_b1b6b1","can_delete":false,"product_type":"c1","uid":1609720,"ip_address":"","ucode":"927923A36F22B1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ0SDra4ZSspknscVIV8ZKQia684AjyzVaBiceT84p0Vic4GhrW6pKMkbDWBSwqbrdbaksr1nqKbF7rg/132","comment_is_top":false,"comment_ctime":1622628902,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10212563494","product_id":100049101,"comment_content":"不知道老师还能不能看到，有没有什么办法可以实时来监测数据的异常？而不是一定等到跑任务出指标的时候才能发现数据异常？等到跑指标才出问题，大概率这个数据产出会延迟了。","like_count":2,"discussions":[{"author":{"id":1066409,"avatar":"https://static001.geekbang.org/account/avatar/00/10/45/a9/3d48d6a2.jpg","nickname":"Lorry","note":"","ucode":"BD4754D0F1D786","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":581158,"discussion_content":"流计算，实时同步的时候设置规则","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1658560523,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219328,"user_name":"keke","can_delete":false,"product_type":"c1","uid":1077379,"ip_address":"","ucode":"BE28B8110DF861","user_header":"https://static001.geekbang.org/account/avatar/00/10/70/83/36ab65ec.jpg","comment_is_top":false,"comment_ctime":1589989415,"is_pvip":true,"replies":[{"id":"81540","content":"你好，你说的确实是一种可行的方法，不过，一般可以通过横向对比数据之间的一致性趋势来进行数据的初步判断。<br><br>感谢你的留言~祝好~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1590410577,"ip_address":"","comment_id":219328,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10179924007","product_id":100049101,"comment_content":"给这种大促活动单独设定一套稽查规则，随时切换～","like_count":2,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495766,"discussion_content":"你好，你说的确实是一种可行的方法，不过，一般可以通过横向对比数据之间的一致性趋势来进行数据的初步判断。\n\n感谢你的留言~祝好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590410577,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211347,"user_name":"你好","can_delete":false,"product_type":"c1","uid":1945987,"ip_address":"","ucode":"F6ED43EF5FD164","user_header":"https://static001.geekbang.org/account/avatar/00/1d/b1/83/c6dd1322.jpg","comment_is_top":false,"comment_ctime":1587951008,"is_pvip":false,"replies":[{"id":"78864","content":"你好，通过azkaban中任务执行的后置检查条件执行的，在任务执行后，会自动开始执行稽核校验任务，如果任务失败的话，调度系统会根据任务的执行结果，中断任务流的执行。<br><br>感谢你的提问，期待下次与你再会~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1588071057,"ip_address":"","comment_id":211347,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10177885600","product_id":100049101,"comment_content":"老师，对于DQC中的规则控制里的强弱规则，是怎么控制调度流程继续或者终止的呢？怎么实现的。。。谢谢老师","like_count":2,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493256,"discussion_content":"你好，通过azkaban中任务执行的后置检查条件执行的，在任务执行后，会自动开始执行稽核校验任务，如果任务失败的话，调度系统会根据任务的执行结果，中断任务流的执行。\n\n感谢你的提问，期待下次与你再会~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588071057,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207419,"user_name":"JohnT3e","can_delete":false,"product_type":"c1","uid":1063982,"ip_address":"","ucode":"CF4AAAC933529C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132","comment_is_top":false,"comment_ctime":1587084974,"is_pvip":false,"replies":[{"id":"77729","content":"看来你也遇到了，哈哈。希望07数据质量的介绍对你有所帮助，能够解决你当前面临的问题~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587305323,"ip_address":"","comment_id":207419,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10177019566","product_id":100049101,"comment_content":"关于全链路监控深有体会。如果问题不早发现，而最终由业务反馈的话，可能会面临整个数据流程的数据回滚和重新加工，进而引发连锁反应，导致更多任务超时或者错误。","like_count":2,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492117,"discussion_content":"看来你也遇到了，哈哈。希望07数据质量的介绍对你有所帮助，能够解决你当前面临的问题~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587305323,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":233211,"user_name":"雷传盛","can_delete":false,"product_type":"c1","uid":1948935,"ip_address":"","ucode":"45F8D9FBCCAD5B","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLG1PhySnTqAaDnFrYB2vPNHAh6MzWrIdFoZYI7ME9BxX0iaqFsoc1ANkFy646YhfoC6p0NVLt1pvQ/132","comment_is_top":false,"comment_ctime":1594261398,"is_pvip":false,"replies":[{"id":"87130","content":"你好，目前没有看到有很成熟的开源工具可以实现，我们自己有提供DQC，数据质量中心，如果感兴趣，可以搜索网易大数据，进一步了解~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1595248713,"ip_address":"","comment_id":233211,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5889228694","product_id":100049101,"comment_content":"郭老师，有什么数据质量管理工具可以实现的么？","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500944,"discussion_content":"你好，目前没有看到有很成熟的开源工具可以实现，我们自己有提供DQC，数据质量中心，如果感兴趣，可以搜索网易大数据，进一步了解~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595248713,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062864,"avatar":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","nickname":"aof","note":"","ucode":"5815D63C4926BC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":303397,"discussion_content":"开源的有Apache Griffin，但是也不太好用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599229535,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219919,"user_name":"Weehua","can_delete":false,"product_type":"c1","uid":1170152,"ip_address":"","ucode":"91C0C2699D4EE4","user_header":"https://static001.geekbang.org/account/avatar/00/11/da/e8/d49dfa94.jpg","comment_is_top":false,"comment_ctime":1590118562,"is_pvip":false,"replies":[{"id":"81520","content":"HI， 你好，WeehuaZheng，<br><br>我们的调度系统是基于开源的azkaban 进行的二次开发，至于你提到的flow之间的依赖，我们支持跨flow的job之间的依赖设置，这个属于自研的feature，基本原理是在job运行前，会周期性的检查前置依赖节点运行状态，只有依赖job 运行完成后，才能被执行。这个可以避免你说的flow级别的依赖木桶效应。<br><br>感谢你的提问，欢迎你有任何数据建设中的问题，在留言区与我们分享~祝好~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1590406517,"ip_address":"","comment_id":219919,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5885085858","product_id":100049101,"comment_content":"azkaban.flow.1.days.ago，看到这个参数，请问郭老师，你们网易的任务调度系统是用azkaban吗？还是在此基础上做了二次开发？我们团队2018-2019就是用azkaban做调度的，为了方便设置前后依赖，我们采用了flow依赖，每一层是一个flow，比如ods_flow，dwd_flow，但这样会存在木桶原理，每个flow的结束时间就是最长的那个任务，当有几千个任务的时候，任务运行非常不稳定，每天凌晨人肉值班，非常辛苦。","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495995,"discussion_content":"HI， 你好，WeehuaZheng，\n\n我们的调度系统是基于开源的azkaban 进行的二次开发，至于你提到的flow之间的依赖，我们支持跨flow的job之间的依赖设置，这个属于自研的feature，基本原理是在job运行前，会周期性的检查前置依赖节点运行状态，只有依赖job 运行完成后，才能被执行。这个可以避免你说的flow级别的依赖木桶效应。\n\n感谢你的提问，欢迎你有任何数据建设中的问题，在留言区与我们分享~祝好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590406517,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219756,"user_name":"外星人","can_delete":false,"product_type":"c1","uid":1132861,"ip_address":"","ucode":"D8469B13F2AB37","user_header":"https://static001.geekbang.org/account/avatar/00/11/49/3d/4ac37cc2.jpg","comment_is_top":false,"comment_ctime":1590074257,"is_pvip":false,"replies":[{"id":"81535","content":"你好，数据质量的稽核监控，本质上也是提交了一个数据校对任务到yarn上执行，必然会涉及到计算资源的消耗。所以并不是每个表都需要稽核监控，只有核心链路的表才需要稽核监控。<br><br>那问题来了，什么是核心链路的表，这个主要取决于应用，只有影响到下游核心应用的表，才是核心表，这些表需要添加稽核监控。比如，老板每天都要看的KPI报表，上游产出链路的所有的表，都需要添加稽核监控~<br><br>感谢你的提问，你的问题挺好的，相信很多人都有这样的困惑~祝好~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1590409249,"ip_address":"","comment_id":219756,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5885041553","product_id":100049101,"comment_content":"额外起任务做数据校验和资源额外使用，怎么做衡量啊？","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495935,"discussion_content":"你好，数据质量的稽核监控，本质上也是提交了一个数据校对任务到yarn上执行，必然会涉及到计算资源的消耗。所以并不是每个表都需要稽核监控，只有核心链路的表才需要稽核监控。\n\n那问题来了，什么是核心链路的表，这个主要取决于应用，只有影响到下游核心应用的表，才是核心表，这些表需要添加稽核监控。比如，老板每天都要看的KPI报表，上游产出链路的所有的表，都需要添加稽核监控~\n\n感谢你的提问，你的问题挺好的，相信很多人都有这样的困惑~祝好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590409249,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":217177,"user_name":"zhuxueyu","can_delete":false,"product_type":"c1","uid":1943187,"ip_address":"","ucode":"D4FA805528C55C","user_header":"","comment_is_top":false,"comment_ctime":1589432517,"is_pvip":false,"replies":[{"id":"80312","content":"你好， 这部分还是要从流程规范的层面来推动业务系统的改造。<br><br>举个例子来说吧，对于日志格式的规范，要与业务系统定义好日志的格式，我们有一个日志管理平台，可以定义日志格式，然后监测不符合规范的日志，出现问题，第一时间发现，推动业务系统改造。","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1589455784,"ip_address":"","comment_id":217177,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5884399813","product_id":100049101,"comment_content":"来自传统企业的科技公司的大数据部门。<br>公司数据质量问题的根因更多是：<br>1、业务终端人员操作不规范，导致数据错误、数据不一致等<br>2、某一类系统设计欠缺，导致缺少核心字段等<br>诸如上面两种根因，单纯是从数据层面建立全链路监控，只能监测到数据的表象问题。但整个过程最令人头疼往往是要解决上游终端&#47;系统的规范性操作、合理设计等问题","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495063,"discussion_content":"你好， 这部分还是要从流程规范的层面来推动业务系统的改造。\n\n举个例子来说吧，对于日志格式的规范，要与业务系统定义好日志的格式，我们有一个日志管理平台，可以定义日志格式，然后监测不符合规范的日志，出现问题，第一时间发现，推动业务系统改造。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589455784,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1943187,"avatar":"","nickname":"zhuxueyu","note":"","ucode":"D4FA805528C55C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":266158,"discussion_content":"其实我感觉难得不是定规范、做监控。难的是发现业务端的问题了，如何推动让业务端改。不过，当然，推动解决也是要从ROI的角度先评估","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589470533,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2706240,"avatar":"https://static001.geekbang.org/account/avatar/00/29/4b/40/9efb571a.jpg","nickname":"何翔","note":"","ucode":"47C8B00760FC50","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1943187,"avatar":"","nickname":"zhuxueyu","note":"","ucode":"D4FA805528C55C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384841,"discussion_content":"同意，我也遇到同样的问题了，每次排查到底发下都是业务数据有问题，然而业务那边几乎很少会配合着去做修改","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626767940,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":266158,"ip_address":""},"score":384841,"extra":""}]}]},{"had_liked":false,"id":211079,"user_name":"XiangJiawei","can_delete":false,"product_type":"c1","uid":1324641,"ip_address":"","ucode":"AAC11557A3477B","user_header":"https://static001.geekbang.org/account/avatar/00/14/36/61/8863e18c.jpg","comment_is_top":false,"comment_ctime":1587891754,"is_pvip":true,"replies":[{"id":"78999","content":"你好，你说的是一个可行的探索方向。<br><br>感谢你的阅读~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1588160541,"ip_address":"","comment_id":211079,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882859050","product_id":100049101,"comment_content":"我提到数据完整性可以通过数据记录的波动率来监控，如果超过 20% 的波动，应该被视为异常。但是你有没有想过，这种也存在误判的情况，尤其是在双十一大促期间，大概率数据稽核规则都是异常的，此时你又该怎么办？<br>--考虑把历史的波动率的数据进行统计，通过机器学习等方式进行训练，提前预测双十一等情况下波动率的变化，把预测值与实际值进行对比分析。","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493190,"discussion_content":"你好，你说的是一个可行的探索方向。\n\n感谢你的阅读~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588160541,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209967,"user_name":"叫我小名","can_delete":false,"product_type":"c1","uid":1945684,"ip_address":"","ucode":"C480373CE60989","user_header":"","comment_is_top":false,"comment_ctime":1587642839,"is_pvip":false,"replies":[{"id":"78975","content":"你好，我来介绍一下我们这边的方案。<br><br>你说的这是一个很典型的协作问题导致的数据质量问题，而且在一般的数据团队中还很常见。<br><br>我们所有数据库的变更都是经过工单系统提交，由DBA审核后，自动执行的。我们在工单系统中增加了一个环节，就是如果涉及到表结构变更，会根据数据血缘，找到它影响到数据中台原始数据的表的负责人，并在负责人确认以后，才能继续执行下去，这样就保证了业务系统数据库变更能够通知到数据负责人了。<br><br>这里，依赖全链路数据血缘，这里不仅仅是数据中台内的大数据加工的血缘，还要包括数据导入和导出任务的血缘，所以你才能建立业务系统源数据库源表到数据中台某张hive表的血缘关系。这个可以通过数据传输中心中，把血缘关系传递给元数据中心实现。<br><br>感谢你的阅读~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1588157630,"ip_address":"","comment_id":209967,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882610135","product_id":100049101,"comment_content":"有这样一个场景，上游业务系统变更（表结构变更等），怎么自动通知到数据人员这边","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492900,"discussion_content":"你好，我来介绍一下我们这边的方案。\n\n你说的这是一个很典型的协作问题导致的数据质量问题，而且在一般的数据团队中还很常见。\n\n我们所有数据库的变更都是经过工单系统提交，由DBA审核后，自动执行的。我们在工单系统中增加了一个环节，就是如果涉及到表结构变更，会根据数据血缘，找到它影响到数据中台原始数据的表的负责人，并在负责人确认以后，才能继续执行下去，这样就保证了业务系统数据库变更能够通知到数据负责人了。\n\n这里，依赖全链路数据血缘，这里不仅仅是数据中台内的大数据加工的血缘，还要包括数据导入和导出任务的血缘，所以你才能建立业务系统源数据库源表到数据中台某张hive表的血缘关系。这个可以通过数据传输中心中，把血缘关系传递给元数据中心实现。\n\n感谢你的阅读~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588157630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207831,"user_name":"小桥流水","can_delete":false,"product_type":"c1","uid":1011273,"ip_address":"","ucode":"C1582AE2FB5299","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6e/49/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1587192695,"is_pvip":true,"replies":[{"id":"77758","content":"这两个指标，对于预防一些大面积的故障，还是有帮助的，例如基础设施层面的故障。不过可能粒度还是太粗了，最好是能够到任务粒度的，才能起到预警的作用。如果只是衡量结果，我觉得只要一个基线任务产出时间完成率就够了。<br><br>感谢你的阅读，期待与你再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587307528,"ip_address":"","comment_id":207831,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5882159991","product_id":100049101,"comment_content":"考虑增加2个指标<br>1、不同时间点的任务完成率<br>2、当天累计任务完成率","like_count":1,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492257,"discussion_content":"这两个指标，对于预防一些大面积的故障，还是有帮助的，例如基础设施层面的故障。不过可能粒度还是太粗了，最好是能够到任务粒度的，才能起到预警的作用。如果只是衡量结果，我觉得只要一个基线任务产出时间完成率就够了。\n\n感谢你的阅读，期待与你再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587307528,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1011273,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/6e/49/abb7bfe3.jpg","nickname":"小桥流水","note":"","ucode":"C1582AE2FB5299","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":239812,"discussion_content":"1、不同时间点任务完成率 及当天累计任务完成率 是面向业务数据化管理指标\n2、单指标完成时间阀值设置及监控，是预警及优化范畴，可以考虑 对阀值设置红黄蓝标准，利于界定何时优化处理此事。\n以上是结合郭老课件的感悟，已着手落地实施。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587308724,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":359654,"user_name":"个经","can_delete":false,"product_type":"c1","uid":2995234,"ip_address":"浙江","ucode":"E0F34F57919902","user_header":"https://static001.geekbang.org/account/avatar/00/2d/b4/22/117c1f31.jpg","comment_is_top":false,"comment_ctime":1665730373,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1665730373","product_id":100049101,"comment_content":"DQC 还提供了全链路监控的功能——这里相关的图片是以产出的作业为维度往上分析的，如果是中间层级的发生问题，如何第一时间定位到影响性呢，界面是如何展示的呢","like_count":0},{"had_liked":false,"id":326661,"user_name":"Geek_daa2ef","can_delete":false,"product_type":"c1","uid":2869555,"ip_address":"","ucode":"4CB151E405DD02","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKgibawVqWlar8rwOnXqH5tfheUfnIMXaavQvwoSvibibo6aRDmzK70hxRGxgVJBwvyRsfuNLibiaW7ORA/132","comment_is_top":false,"comment_ctime":1639622192,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1639622192","product_id":100049101,"comment_content":"老师您好，我是初学者。我想问下：稽核的强弱规则怎么判定?","like_count":0},{"had_liked":false,"id":326000,"user_name":"spencer","can_delete":false,"product_type":"c1","uid":1399304,"ip_address":"","ucode":"BBB22B77DCE987","user_header":"https://static001.geekbang.org/account/avatar/00/15/5a/08/467b3e34.jpg","comment_is_top":false,"comment_ctime":1639301185,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1639301185","product_id":100049101,"comment_content":"稽查的任务会不会占用很多时间呢？如果是主键唯一值检验，要数据量很大的情况下怎么优化？这个稽查任务不会影响6点之前算指标的SLA么","like_count":0},{"had_liked":false,"id":303382,"user_name":"何翔","can_delete":false,"product_type":"c1","uid":2706240,"ip_address":"","ucode":"47C8B00760FC50","user_header":"https://static001.geekbang.org/account/avatar/00/29/4b/40/9efb571a.jpg","comment_is_top":false,"comment_ctime":1626763570,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626763570","product_id":100049101,"comment_content":"老师您好，我现在就经常遇到项目上反馈数据不对的问题，但是每次经过排查后，发现都是业务数据有问题，比如一个月报表数据，某个商品正确的分类应该是A，但是业务数据中它的分类是B，这样就会导致报表数据不对，我每次遇到这种情况都是逐一排查数据，然后发现是业务数据有问题，然后修改业务数据，最后重新汇总月报表数据。我感觉现在这种方式并不是很好，而且都是些重复且很耗时的工作，而且对于自己也没什么提升，只会让自己陷入错误的业务数据“海洋”之中。请问老师对于这种情况有什么好的解决方案吗？","like_count":0},{"had_liked":false,"id":293162,"user_name":"fish","can_delete":false,"product_type":"c1","uid":1096768,"ip_address":"","ucode":"B5FC04A69E0E64","user_header":"https://static001.geekbang.org/account/avatar/00/10/bc/40/12086a85.jpg","comment_is_top":false,"comment_ctime":1621249895,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1621249895","product_id":100049101,"comment_content":"1. 提前设置好屏蔽规则，对常规的监控先移除<br>2. 设置新的检查点，前后的数据结合下来对得上就行，比如前面算出来了a, 可以倒推出b，而现在算出来的指标c，c跟b对得上就行","like_count":0},{"had_liked":false,"id":274236,"user_name":"Geek_f9c361","can_delete":false,"product_type":"c1","uid":2393258,"ip_address":"","ucode":"6C214750F201EF","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL8AP4vaWEiaZYQIBmn9n9eXJh8dkzluxMjMyMl1CbOcRzianpVXu5bWkCPJyj2sTfxHhpYOMOVTEjA/132","comment_is_top":false,"comment_ctime":1610935316,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1610935316","product_id":100049101,"comment_content":"我理解的数据湖，最标准的线路应该是数据备份直接接入ods层，然后通过数据解析任务进入dwd层。再通过数据汇总任务进入dws层，然后通过数据指标任务进入ads层。<br>从ods数据中通过纬度完善任务进入dim层。<br><br>1、现实可能为了利用资源，单源的ods和dwd或者dim构建集中成为一个任务。但是数据质量应该包含数据湖的各个阶段吧？<br>2、数据湖是应用的数据无法完全标准化，只能存储原数据。然后，根据需要才解析部分数据，生成汇总任务。毕竟，数据部门是弱势部门，公司产品部门一句没人能说清楚，数据你先存着，用了再分析还是很常见的。<br>那么，关于数据湖elt转etl分析，有什么比较好的实践吗？","like_count":0},{"had_liked":false,"id":261296,"user_name":"Geek_7673f7","can_delete":false,"product_type":"c1","uid":2116498,"ip_address":"","ucode":"0989DE86AB57E3","user_header":"","comment_is_top":false,"comment_ctime":1605261239,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1605261239","product_id":100049101,"comment_content":"想请教下假如中小企业不存在DQC的情况下，有什么好点的方式去保证数据的质量？需要数据测试介入吗？","like_count":0},{"had_liked":false,"id":255019,"user_name":"好好学习","can_delete":false,"product_type":"c1","uid":1762191,"ip_address":"","ucode":"9D44D9530D9A1D","user_header":"https://static001.geekbang.org/account/avatar/00/1a/e3/8f/77b5a753.jpg","comment_is_top":false,"comment_ctime":1603251463,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1603251463","product_id":100049101,"comment_content":"讲的简直太棒了. 很多都是自己遇到过的问题.并且给了可以落地的方案. 棒棒棒.","like_count":0},{"had_liked":false,"id":254295,"user_name":"杨杰","can_delete":false,"product_type":"c1","uid":1131823,"ip_address":"","ucode":"74817EA9499843","user_header":"https://static001.geekbang.org/account/avatar/00/11/45/2f/b0b0dd74.jpg","comment_is_top":false,"comment_ctime":1603097000,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1603097000","product_id":100049101,"comment_content":"关于数据质量，我们现实中碰到一个很头疼的问题：<br>业务系统是类似于ERP的系统，假设在这个系统里面有一个订单的表，里面有一个订单的状态（比如有效，作废等），由于设计源于，系统中对于状态的变更并没有明确的日志记录这些变化。这样就导致一个问题：对这些数据不同的时间做ETL将会导致ODS层的数据不一致，进而影响相关的统计数据。<br>请问这种情况一般怎么处理？","like_count":0,"discussions":[{"author":{"id":1445320,"avatar":"https://static001.geekbang.org/account/avatar/00/16/0d/c8/fdfd768b.jpg","nickname":"李志坡","note":"","ucode":"CCABC7E5CC5421","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":350808,"discussion_content":"数据按照状态分层","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614026756,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":241260,"user_name":"追梦","can_delete":false,"product_type":"c1","uid":1183831,"ip_address":"","ucode":"54C6E76E8FE033","user_header":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","comment_is_top":false,"comment_ctime":1597227909,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1597227909","product_id":100049101,"comment_content":"老师，数据部门作为下游系统，上游业务系统变更，往往不能第一时间通知；这个您这边有什么成熟的经验呢？我们要求过上游变更业务需要通知数据中心，但遇到很多困难1、变成比较频繁 2、什么样的变更需要通知呢？","like_count":0},{"had_liked":false,"id":217126,"user_name":"weiyunhao","can_delete":false,"product_type":"c1","uid":1788210,"ip_address":"","ucode":"75AEC913A1BD5D","user_header":"https://static001.geekbang.org/account/avatar/00/1b/49/32/e6616646.jpg","comment_is_top":false,"comment_ctime":1589423164,"is_pvip":false,"replies":[{"id":"81524","content":"这个是每日的意思，属于增量数据。","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1590407687,"ip_address":"","comment_id":217126,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1589423164","product_id":100049101,"comment_content":"郭老师，您好。df、di对应的是每天分区的数据是全量的和每天分区的数据是增量的。那么比如说1d代表是最近一天的数据，是增量还是全量呢？","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495045,"discussion_content":"这个是每日的意思，属于增量数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590407687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1788210,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/49/32/e6616646.jpg","nickname":"weiyunhao","note":"","ucode":"75AEC913A1BD5D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":265707,"discussion_content":"在dm、dwd、dws、ads中都有1d、1w、2d这种操作吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589429157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211530,"user_name":"summer","can_delete":false,"product_type":"c1","uid":1857933,"ip_address":"","ucode":"D3B6E73D4EB4CD","user_header":"https://static001.geekbang.org/account/avatar/00/1c/59/8d/b5406e81.jpg","comment_is_top":false,"comment_ctime":1587978969,"is_pvip":false,"replies":[{"id":"78969","content":"你好，其实问题的关键就在于环比和同比没有什么参考性，因为今天和昨天的数据相差的实在是太多了。<br><br>其实有的同学，提出用AI的方式，来让机器训练，预测。其实基于AIops做预测，当然是一条可以探索的路。另外，其实可以横向做多个表的对比，比如相关表数据量的增长幅度来加一下佐证。<br><br>感谢你的阅读~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1588156502,"ip_address":"","comment_id":211530,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587978969","product_id":100049101,"comment_content":"感觉这个问题就看你的波动率对比的基准值是什么啦，用一些统计的方法同环比结合","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493286,"discussion_content":"你好，其实问题的关键就在于环比和同比没有什么参考性，因为今天和昨天的数据相差的实在是太多了。\n\n其实有的同学，提出用AI的方式，来让机器训练，预测。其实基于AIops做预测，当然是一条可以探索的路。另外，其实可以横向做多个表的对比，比如相关表数据量的增长幅度来加一下佐证。\n\n感谢你的阅读~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588156502,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210695,"user_name":"Olivia_饶","can_delete":false,"product_type":"c1","uid":1779565,"ip_address":"","ucode":"CD2CEC066C914E","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/AoXC8Dwxrs9cia2MFhHjvnjSJMbs0CmxqGR7qJQpwbWJ3M55VKiatlNRN5LF1X2uYTA42nbn3GicJd6JaKy8zaVbA/132","comment_is_top":false,"comment_ctime":1587809178,"is_pvip":false,"replies":[{"id":"79000","content":"你好<br>1. 一般是表的负责人，也就是这表的产出任务的负责人添加稽核校验规则，如果负责人认为，数据有问题，需要终止任务，那下游任务即使再运行，也没用，因为依赖的数据已经有问题了。<br>2. 这个每个人负责管理好自己的表，我们对质量的打分，是具体到每一个表的，如果是上游的输入表导致你的问题，那由上游表的人承担责任。<br>3. 我们会对每个表的数据质量进行打分，打分的规则是根据稽核监控通过率来衡量的。当然，事故也会进行回溯，确认责任人，参与者往往是涉及表的开发，主题域的负责人，数据负责人。评审的主要目的，就是要完善稽核监控，让犯过的错误不再犯。<br><br>感谢你的提问~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1588160744,"ip_address":"","comment_id":210695,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587809178","product_id":100049101,"comment_content":"受益很多,有几个问题想请教:<br>1.谁可以配置数据质量规则?如果所有人都能配置强弱阻塞规则,那么是不是会导致下游任务会有问题,从这个角度是不是数据表管理员可以配置数据质量规则,那么接着第二个问题:<br>2.打通全链路数据质量监控,如果只能数据表管理员配置规则,但是他可能不是所有上游表的数据表管理员,这样怎么才能做到全链路数据质量监控?<br>3.关于数据质量的管理,当出现数据质量问题后,数据质量问题怎么管,怎么解决,需要什么角色参与?如何形成闭环?有没有更详细的关于数据质量管理办法的介绍?比如从数据质量规则配置,制定打分体系,再到出现数据质量问题如何解决闭环的一整套管理办法?<br>谢谢您的时间~","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493087,"discussion_content":"你好\n1. 一般是表的负责人，也就是这表的产出任务的负责人添加稽核校验规则，如果负责人认为，数据有问题，需要终止任务，那下游任务即使再运行，也没用，因为依赖的数据已经有问题了。\n2. 这个每个人负责管理好自己的表，我们对质量的打分，是具体到每一个表的，如果是上游的输入表导致你的问题，那由上游表的人承担责任。\n3. 我们会对每个表的数据质量进行打分，打分的规则是根据稽核监控通过率来衡量的。当然，事故也会进行回溯，确认责任人，参与者往往是涉及表的开发，主题域的负责人，数据负责人。评审的主要目的，就是要完善稽核监控，让犯过的错误不再犯。\n\n感谢你的提问~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588160744,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210379,"user_name":"流沙","can_delete":false,"product_type":"c1","uid":1040222,"ip_address":"","ucode":"D34D30A1494C63","user_header":"https://static001.geekbang.org/account/avatar/00/0f/df/5e/f709457f.jpg","comment_is_top":false,"comment_ctime":1587733051,"is_pvip":false,"replies":[{"id":"79003","content":"你好，稽核规则，可以有一些基本的规则是必须要添加的，比如主键唯一性的规则，表字段波动率的。其他的要根据业务来添加~<br><br>感谢你的阅读~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1588160977,"ip_address":"","comment_id":210379,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587733051","product_id":100049101,"comment_content":"这几条稽核规则都是建立在一定的前提条件下的吧，比如和其他字段存在逻辑上的关联。但是对于一项非常独立的数据，有没有办法凭空建立起来一些规则呢？","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493009,"discussion_content":"你好，稽核规则，可以有一些基本的规则是必须要添加的，比如主键唯一性的规则，表字段波动率的。其他的要根据业务来添加~\n\n感谢你的阅读~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588160977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208153,"user_name":"Geek_f071bc","can_delete":false,"product_type":"c1","uid":1945778,"ip_address":"","ucode":"B98A0BD2B4620A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/zgnl95XDvXnicWoGibeAdicWrBYj8UgYGAayxcrJcaFLSbENX402AsVCtRibg3G1tuWrpZwymicribdug9PaHMeGK1Xw/132","comment_is_top":false,"comment_ctime":1587285094,"is_pvip":false,"replies":[{"id":"77765","content":"你好，每个有稽核规则的表，会计算质量分数。质量分数的计算主要是结合规则是否成功，规则的数量，规则是强规则还是弱规则。比如我们约定，强规则失败一次扣4分，弱规则失败一个扣2分，然后我们就可以计算某一天的某个表的质量分。<br><br>你可能会有疑问，那加的规则越多，不是风险就越大了么？ 确实会有这样的问题，但是规则的完备性不是由数据开发单方面决定的，是经过由数据架构师，域负责人组成的评审小组确定的，所以会确保每个表的稽核规则尽可能的完备，另外事故回溯也会完善稽核规则。<br><br>感谢你的阅读，期待与你再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587308114,"ip_address":"","comment_id":208153,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587285094","product_id":100049101,"comment_content":"老师 ，基于稽核规则，计算表级别的质量分数？能举个例子吗，这个质量分数怎么算？","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492358,"discussion_content":"你好，每个有稽核规则的表，会计算质量分数。质量分数的计算主要是结合规则是否成功，规则的数量，规则是强规则还是弱规则。比如我们约定，强规则失败一次扣4分，弱规则失败一个扣2分，然后我们就可以计算某一天的某个表的质量分。\n\n你可能会有疑问，那加的规则越多，不是风险就越大了么？ 确实会有这样的问题，但是规则的完备性不是由数据开发单方面决定的，是经过由数据架构师，域负责人组成的评审小组确定的，所以会确保每个表的稽核规则尽可能的完备，另外事故回溯也会完善稽核规则。\n\n感谢你的阅读，期待与你再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587308114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207799,"user_name":"刘明","can_delete":false,"product_type":"c1","uid":1967372,"ip_address":"","ucode":"D35E61048B8E30","user_header":"","comment_is_top":false,"comment_ctime":1587182204,"is_pvip":false,"replies":[{"id":"77755","content":"你好，要把数据质量稽核监控，纳入到数据研发的标准流程中，不能因为时间紧张，就忽视了，这样导致交付后会出现更大的问题。我在第12讲流程协作与规范中，会讲到数据研发的标准化流程，期望对你有所帮助。感谢你的阅读~期待与你再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587307427,"ip_address":"","comment_id":207799,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587182204","product_id":100049101,"comment_content":"数据中台之后，稽核监控应当是比较重要的问题了。但是在任务紧的情况下，这些质量方面的投入往往又会被忽视","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492243,"discussion_content":"你好，要把数据质量稽核监控，纳入到数据研发的标准流程中，不能因为时间紧张，就忽视了，这样导致交付后会出现更大的问题。我在第12讲流程协作与规范中，会讲到数据研发的标准化流程，期望对你有所帮助。感谢你的阅读~期待与你再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587307427,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207578,"user_name":"volcano","can_delete":false,"product_type":"c1","uid":1276806,"ip_address":"","ucode":"FE3D36D7CE79E9","user_header":"https://static001.geekbang.org/account/avatar/00/13/7b/86/ef5581e3.jpg","comment_is_top":false,"comment_ctime":1587111945,"is_pvip":false,"replies":[{"id":"77766","content":"ODS 层的数据缺失，可以通过在数据传输工具中做校验来完成，读取了多少行记录，这个在数据传输工具中是可以知道的。<br><br>感谢你的阅读~期待与你再次相遇~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587308166,"ip_address":"","comment_id":207578,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587111945","product_id":100049101,"comment_content":"完整性规则中<br>如何确定业务库数据到ODS数据是不缺失的呢？有没有具体的稽核方案<br>业务库是不会允许select count()这种需要大量扫表的行为","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492176,"discussion_content":"ODS 层的数据缺失，可以通过在数据传输工具中做校验来完成，读取了多少行记录，这个在数据传输工具中是可以知道的。\n\n感谢你的阅读~期待与你再次相遇~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587308166,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207459,"user_name":"天草二十六","can_delete":false,"product_type":"c1","uid":1360712,"ip_address":"","ucode":"3165EE3007527B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c3/48/3a739da6.jpg","comment_is_top":false,"comment_ctime":1587089220,"is_pvip":false,"replies":[{"id":"77727","content":"感谢你的认可和鼓励，这确实是我们在实践过程中总结沉淀出来的一套方法，拿出来分享给大家，希望对你有所帮助~","user_name":"作者回复","user_name_real":"郭忆","uid":"1319022","ctime":1587305221,"ip_address":"","comment_id":207459,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587089220","product_id":100049101,"comment_content":"总结平常的数据质量问题很到位，对数据治理有帮助，大多数小公司都值得一学","like_count":0,"discussions":[{"author":{"id":1319022,"avatar":"https://static001.geekbang.org/account/avatar/00/14/20/6e/ad4dac0a.jpg","nickname":"郭忆","note":"","ucode":"496AEB07A40140","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492136,"discussion_content":"感谢你的认可和鼓励，这确实是我们在实践过程中总结沉淀出来的一套方法，拿出来分享给大家，希望对你有所帮助~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587305221,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}