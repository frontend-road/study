{"id":741056,"title":"第 4 章 分布式协同(2)","content":"<h2 id=\"nav_point_94\">4.4　分布式选举</h2>\n<p>本章内容主要介绍分布式系统中的协同问题。前面已经介绍了分布式互斥和分布式事务，前者讨论的是多个应用调用单个资源的问题，后者讨论的是单个、多个应用调用不同资源的问题。解决这些问题时，我们介绍了一些对应的算法（例如集中互斥算法、DTP 分布式模型），并在这些算法中引入事务协调者的角色。事务协调者虽然可以协助应用程序访问资源，但也会引出问题，就是事务协调者本身的可用性，一旦由于网络或者自身服务器问题造成事务协调者不可用，算法就无法实现。因此，我们将针对事务协调者搭建对应的集群，采用主从互备的方式增强其可用性。在主从互备中，会选择一个主节点作为信息更新的入口，其他从节点作为信息的读取端。信息的更新在主节点上完成，更新后主节点再将信息同步到其他从节点上，保证从节点上的信息与主节点的一致。一旦由于故障导致主节点下线，就需要在其他从节点中选举一个作为新的主节点，代替旧主节点工作。如果把主节点称为领导者，从节点称为跟随者，那么从多个跟随者中选举领导者的过程就是本节要讲的分布式选举。分布式选择是分布式系统中事务协调者可用性的表现，这一节会介绍分布式选举的几个算法，分别是 Bully 算法、Raft 算法、ZAB 算法。</p><!-- [[[read_end]]] -->\n<h3 id=\"nav_point_95\">4.4.1　Bully 算法</h3>\n<p>Bully 算法的原则是在存活的节点中，选取节点 ID 最大（或者最小）的节点为主节点。当然 Bully 算法也适用于含主从节点的分布式系统，其分为三种消息类型。</p>\n<ul>\n<li>Election 消息：此消息用于发起选举。</li>\n<li>Alive 消息：对 Election 消息的回复。</li>\n<li>Victory 消息：竞选成功的主节点向其他节点发送的消息，告诉其他节点自己是主节点。</li>\n</ul>\n<p>在使用 Bully 算法的集群中，每个节点都知道其他节点的 ID，选举过程分如下几步。</p>\n<ul>\n<li>集群中所有节点都判断自己的 ID 是否是存活节点中 ID 最大的，如果是，就向其他节点发送 Victory 消息，声明自己是主节点。</li>\n<li>否则，向比自己 ID 大的所有节点发送 Election 消息，并等待回复。</li>\n<li>在给定的时间范围内，如果当前节点没有接收到其他节点回复的 Alive 消息，就认为自己是主节点，同时向其他节点发送 Victory 消息，声明自己是主节点；若接收到比自己 ID 大的节点回复的 Alive 消息，就等待其他节点发送 Victory 消息。</li>\n<li>若当前节点接收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知这些节点，我的 ID 更大，需要重新选举。</li>\n</ul>\n<p>接下来通过例子直观地看看 Bully 算法是如何实现选举的。如图 4-30 所示，假设集群中有 4 个节点，ID 从小到大排列分别是 1、2、3、4，这四个节点互相连接，并且每个节点都知道其他节点的 ID，这里把这四个节点分别称作 N1、N2、N3、N4。由于 N4 节点的 ID 最大，因此在集群初始化的时候，选其作为主节点，其他三个节点为从节点。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00412.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-30　四个节点在集群中互相连接</strong></p>\n<p>如果 N4 节点突然遇到故障死机了，最早发现这个情况的是 N1 节点，那么 N1 节点会向比自己 ID 大的节点（N2、N3 和 N4）发起选举，即发送 Election 消息，如图 4-31 所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00413.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-31　N1 节点向其他三个节点发起选举</strong></p>\n<p>由于 N4 节点已经死机，如图 4-32 所示，因此只有 N2 和 N3 节点向 N1 节点回复了 Alive 消息，表示都比 N1 节点的 ID 大，因此需要重新发起选举。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00414.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-32　N2 和 N3 节点的 ID 大于 N1 节点</strong></p>\n<p>如图 4-33 所示，N2 向集群中比自己 ID 大的两个节点（N3 和 N4）发送了 Election 消息，从而重新发起选举。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00415.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-33　N2 节点重新发起选举</strong></p>\n<p>N4 节点由于死机同样无法回复 N2 节点的消息，如图 4-34 所示，N3 节点接收到 N2 节点的消息以后发现自己的 ID 比 N2 节点的大，于是回复 Alive 消息。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00416.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-34　N3 节点回复 Alive 消息</strong></p>\n<p>此时，轮到 N3 节点发起选举了，如图 4-35 所示，N3 节点向比自己 ID 大的 N4 节点发送了 Election 消息。很明显 N4 已经死机，无法回复 N3 节点，于是在等待一定时间后 N3 节点顺利成为了主节点。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00417.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-35　N3 节点向 N4 节点发起选举</strong></p>\n<p>作为新主节点的 N3 需要向其他节点声明自己的身份，于是向全网其他节点发送 Victory 消息，如图 4-36 所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00418.jpeg\" alt=\"\" width=\"40%\" style=\"width: 40%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-36　N3 作为主节点向全网其他节点发送 Victory 消息</strong></p>\n<h3 id=\"nav_point_96\">4.4.2　Raft 算法</h3>\n<p>Raft 算法是一种投票选举算法，遵从少数服从多数的原则，规定在一个周期内获得票数最多的节点为主节点。该算法将集群节点划分为三种角色。</p>\n<ul>\n<li><strong>Leader</strong>：领导者，也就是主节点，在一个集群中同一时刻只能有一个领导者，负责协调和管理其他从节点。</li>\n<li><strong>Candidate</strong>：候选者，集群中的每个节点都有可能成为候选者，只有先成为候选者才有机会被选为领导者。</li>\n<li><strong>Follower</strong>：跟随者，它会跟随领导者，不可以发起选举。</li>\n</ul>\n<p>如图 4-37 所示，集群初始化时，所有节点都是跟随者，在没有领导者的情况下，这些跟随者自然接收不到领导者的消息，经过一定时间后就会转换为候选者。候选者通过投票参与选举，如果接收到半数以上的投票就会转换为领导者。如果由于网络分区，网络中出现了更大任期（Term）领导者，那么旧的领导者会退为跟随者。候选者如果发现了新的领导者或者新的任期，会转换为跟随者。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00419.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-37　Raft 算法选举状态图</strong></p>\n<p>图 4-37 对 Raft 算法的三个角色之间的转换进行了简述，通过下面几步详细描述 Raft 选举的流程。</p>\n<p>(1) 在集群初始化阶段，各节点加入集群中，此时所有节点均为跟随者，也就是说在最初是没有领导者的。</p>\n<p>(2) 集群中如果存在领导者，就会给每个跟随者发送心跳包表示自己存在，但此时集群中并没有领导者，因此需要进行选举。</p>\n<p>(3) 集群中所有节点都从跟随者转换为候选者，同时向其他节点发送选举请求。这里有一个选举超时控制机制（Election Timeout），用来控制节点从跟随者转换为候选者的时间，一般选取 150ms 和 300ms 之间的随机值。设置这个值的目的是避免多个跟随者同时转换为候选者。如果跟随者在选举超时控制指定的时间范围内没有接收到来自领导者的心跳包，就转换为候选者发起选举。</p>\n<p>(4) 由于选举超时控制的时间是一定范围内的随机数，因此其他节点接收到的选举请求是有先后顺序的，接收请求的节点回复请求发起者，是否同意其成为领导者。需要注意，在每轮选举中，一个节点只能投出一张票。</p>\n<p>(5) 如果有候选者获得超过一半节点的投票，那么这个候选者就会成为领导者并且宣布自己的任期。任期可以理解为一个累加的数字，例如集群中第一次选举出来的领导者，其任期为 1。领导者宣布任期以后，其他节点会转变为跟随者。领导者节点会向跟随者节点定期发送心跳包，检测对方是否存活。通常来说心跳包会按照一定时间间隔发送，跟随者接收到来自领导者的心跳包以后，会回复一个消息表示已经接收到。</p>\n<p>(6) 当领导者出现网络延迟或者死机时，无法发送心跳包，跟随者如果在选举超时控制容忍的时间内没有接收到心跳包就会发起选举。这表明之前领导者节点的任期到了，如果之前的任期为 1，那么再选举出来的领导者的任期就是 2。当跟随者发起新选举时，当前的领导者节点会降级为跟随者，和其他跟随者一样参与新一轮选举。</p>\n<blockquote>\n<p><strong>注意</strong></p>\n<p>在 Raft 算法中，如果某一次选举时出现了平票的现象，也就是两个节点获得了一样多的票数，就将任期加 1 并重新发起一次选举，直到某个节点获得更多的票数。</p>\n</blockquote>\n<p>一旦集群中选举出了领导者，客户端的写入操作就会针对领导者展开，领导者再与跟随者同步信息完成最终值的修改。图 4-38 描绘了客户端通过领导者对集群中其他节点上的信息进行更新的过程。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00420.jpeg\" alt=\"\" width=\"65%\" style=\"width: 65%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-38　Raft 算法中客户端更新数据的过程——日志复制</strong></p>\n<p>具体过程如下。</p>\n<p>(1) 客户端将更新信息发送给领导者，此时领导者会在本地的日志中建立一个 Entry，记录这次修改。注意这个 Entry 是未提交（Uncommitted）的，即还没有进行提交操作，也就是说还没有领导者节点更新信息。</p>\n<p>(2) 领导者将客户端的更新信息分别提交到两个追随者中，说白了就是将这次修改的日志副本发送给两个跟随者。</p>\n<p>(3) 跟随者接收到更新信息后，会给领导者发送确认信息。直到接收到半数以上的确认信息后，领导者才会将客户端更新的 Entry 提交到本地的日志中保存，意思是这次更改已经发送给跟随者，并且得到半数以上的确认。与此同时，领导者还会向集群内的所有跟随者发送广播，声明这次更改已经提交了。上述整个将更新信息通知全网跟随者，并且得到确认的过程称为日志复制。</p>\n<p>(4) 最后，领导者告知客户端 Entry 已经提交。</p>\n<p>上面介绍的日志复制过程不仅在更新信息时会发生，在集群初始化，选举新领导者时也会进行。领导者会将自身保存的状态信息复制到集群的其他跟随者中，这个状态信息叫作 AppendEntries。值得注意，通常是在领导者向跟随者发送心跳包的时候发送 AppendEntries。</p>\n<p>集群中的各个节点可能位于不同的网络中，网络环境不同，就会存在网络分区的情况。如图 4-39 所示，由于网络问题导致集群分成了两个独立的网络，上面一个网络由领导者 1、跟随者 2、跟随者 3 组成，下面一个网络由领导者 4、跟随者 5 组成。这两个网络中的节点由于都感知不到对方的存在，因此分别在自己的网络中选举出领导者 1 和领导者 4。原本处于一个网络的集群，由于网络问题变成了两个独立的网络，并且产生了两个领导者。一个集群出现两个领导者，会破坏数据的一致性。比如当客户端告诉领导者更新信息的时候，两个领导者会分别在自己所处的网络中进行日志复制，这会造成两个网络中的数据不一致，特别是当网络问题解决后，所有节点都能互相感知到对方，此时数据又将如何处理？</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00421.jpeg\" alt=\"\" width=\"90%\" style=\"width: 90%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-39　网络分区造成的脑裂</strong></p>\n<p>从图 4-39 可以发现，两个分区领导者的选举有先后之分，因此对应的任期是不一样的。领导者 1 对应的任期为 2，而领导者 4 的任期为 1，显然领导者 1 较新。</p>\n<p>假设此时有两个客户端，分别是客户端 1 和客户端 2，它们并不知道出现了脑裂的状况，依旧在往集群中更新数据。客户端 1 向领导者 1 更新 Value=5 以后，领导者 1 通过日志复制的方式通知其他跟随者，并获得了超过半数的确认，因此信息为 Committed 状态，Value=5 更新成功。然后客户端 2 向领导者 4 发起 Value=8 的更新信息，由于其无法获得网络中超过半数的节点的支持，因此此次更新一直都处于 Uncommitted 状态，如图 4-40 所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00422.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-40　脑裂造成数据更新不一致</strong></p>\n<p>如图 4-41 所示，此时网络恢复正常，节点之间均可以互相感知到对方。在做心跳检测的时候，领导者 4 发现另外一个领导者 1 的任期比自己的大，因此转换为跟随者 4，然后跟随者 4 和跟随者 5 同时回滚之前的更新信息 Value=8。当领导者 1 发送心跳给跟随者 4 和跟随者 5 时，会将 Value=5 的日志复制信息一并发送，在得到确认以后，跟随者 4 和跟随者 5 的信息都修改为 Value=5。通过这种方式解决了脑裂问题。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00423.jpeg\" alt=\"\" width=\"68%\" style=\"width: 68%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-41　网络恢复以后，根据任期大小找到领导者并且同步数据</strong></p>\n<h3 id=\"nav_point_97\">4.4.3　ZAB 算法</h3>\n<p>ZAB（ZooKeeper Atomic Broadcast）是 ZooKeeper 原子消息广播协议，其设计目的是保证集群中数据的一致性。ZooKeeper 使用一个主进程处理客户端的请求，这个主进程和 Raft 算法中的领导者很像；同时还采用 ZAB 的原子广播协议，将数据的状态变化广播给集群中的其他节点，这点和 Raft 算法中的日志复制相类似。</p>\n<p>ZAB 算法中所有来自客户端的事务请求都由一个全局服务器来协调处理，这样的服务器叫作领导者，集群中的其他服务器叫作跟随者，此外还引入了观察者（Observer）。领导者负责将客户端的事务请求转换成一个提议（Proposal），并将该提议分发给集群中的所有跟随者，之后领导者需要等待所有跟随者的反馈信息，得到超过半数跟随者的反馈以后，领导者会再次向所有跟随者发布消息，告知提议已经半数通过了，并提交和提议对应的事务请求。</p>\n<p>ZAB 算法的选举过程会涉及四种状态，分别如下。</p>\n<ul>\n<li><strong>Looking 状态</strong>：即选举状态，此时集群中不存在领导者，所有节点进入选举状态。</li>\n<li><strong>Leading 状态</strong>：即领导状态，此时集群中已经选出领导者，它可向其他节点广播和同步信息。</li>\n<li><strong>Following 状态</strong>：即跟随状态，此时集群中已经选出领导者，其他节点进入跟随状态并且跟随领导者。</li>\n<li><strong>Observing 状态</strong>：即观察状态，当前节点为观察者，保持观望并且没有投票权和选举权。</li>\n</ul>\n<p>说完了四种状态，再来看看选举过程。ZAB 算法在选举投票的过程中，每个节点都会记录三元组信息 <code>(ServerID, ZXID, epoch)</code>。其中 <code>ServerID</code> 表示节点 ID；<code>ZXID</code> 表示处理的事务 ID，它越大表示处理的事务越新；<code>epoch</code> 表示选举轮数，一般用逻辑时钟表示，选举的轮数越多这个数字越大。每个节点通过二元组 <code>(vote_serverID, vote_zxid)</code> 来表明投票给哪个节点，其中 <code>vote_serverID</code> 表示被投票节点的 <code>ServerID</code>，<code>vote_zxid</code> 表示投票节点的事务 ID。选举的原则是 <code>ZXID</code> 最大的节点成为领导者，若 <code>ZXID</code> 相同，则 <code>ServerID</code> 大的节点成为领导者。</p>\n<p>下面以含 3 个服务器的集群为例，每个服务器就是一个节点，集群初始化的选举过程如下。</p>\n<ul>\n<li><p><strong>第一步</strong>：集群初始化时，由于所有节点都没有感知到领导者的存在，因此发起选举，进行第一轮投票，即 <code>epoch=1</code>。由于此时还没有处理任何事物，因此 3 个节点的 <code>ZXID</code> 都为 <code>0</code>。此时每个节点都会推选自己作为领导者，并向集群中的其他 2 个节点广播投票信息。如图 4-42 所示，按照投票的二元组 <code>(vote_serverID, vote_zxid)</code>，所有节点都会将自己的节点 ID 和事务 ID 发送给彼此。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00424.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-42　集群初始化，所有服务器节点进行投票</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>第二步</strong>：三个节点交换信息以后发现，<code>ZXID</code> 都是 <code>0</code>，难分伯仲。于是比较 <code>ServerID</code>，将 <code>ServerID</code> 较大的节点推选为领导者，所以节点 1 和节点 2 会更改投票信息。如图 4-43 所示，节点 1 和节点 2 将票改成 <code>(3,0)</code>，然后发送给集群中的其他节点，也就是选举节点 3 作为领导者。此时节点 3 的 ID 是最大的，因此没有参与投票。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00425.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-43　将票投向 <code>ServerID</code> 较大的服务器</strong></p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>第三步</strong>：集群中所有服务器都把票投给了节点 3，因此节点 3 当选为集群的领导者。节点 3 进入 Leading 状态，节点 1 和节点 2 作为跟随者进入 Following 状态。</p>\n</li>\n</ul>\n<p>完成第三步以后，领导者会向其他服务器同步信息，如果有客户端的事务请求要处理，还会发送提议信息。</p>\n<p>从选举的整个过程来看，ZAB 算法从开始选举到完成信息同步会经历如下三个阶段。</p>\n<ul>\n<li><strong>发现阶段</strong>：该阶段要求集群必须选举出一个领导者，这个领导者要维护集群中的可用跟随者的列表，从而保证与跟随者节点之间通信的顺利进行。</li>\n<li><strong>同步阶段</strong>：既然发现阶段已经能够保证领导者与跟随者之间的通信，那么在这个阶段，领导者就需要同步自身保存的数据与跟随者节点中的数据，这个和 Raft 算法中的日志复制是一个意思。这个过程会用到 4.3.2 节的 CAP 理论。</li>\n<li><strong>广播阶段</strong>：领导者可以接收客户端需要处理的事务请求，并以提议的形式广播给所有跟随者。</li>\n</ul>\n<p>至此，我们介绍了三种选举算法的机制和原理。实际上，选举算法的初衷是解决集群中如何选择领导者的问题。由于在分布式互斥和分布式事务中都会引入事务协调者，来协调分布式进程或者节点之间的关系，在解决协调问题的同时又对事务协调者的可用性提出了要求，因此通过集群的方式提高协调者的可用性。在集群中谁是领导者、谁是跟随者，以及如何选举领导者都是选举算法要解决的问题。这里我们把上面介绍的三种算法做一个总结和比较，如表 4-1 所示。</p>\n<p><strong>表 4-1　总结和比较三种选举算法</strong></p>\n<table width=\"90%\" border=\"1\">\n<thead>\n<tr>\n<th><p>&nbsp;</p></th>\n<th><p>Bully 算法</p></th>\n<th><p>Raft 算法</p></th>\n<th><p>ZAB 算法</p></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><p>选举原则</p></td>\n<td><p>节点 ID 最大的节点作为领导者</p></td>\n<td><p>少数服从多数，得到票数过半的节点当选领导者</p></td>\n<td><p><code>ZXID</code> 最大的节点成为领导者；若 <code>ZXID</code> 相同，则 <code>ServerID</code> 大的节点是领导者</p></td>\n</tr>\n<tr>\n<td><p>参与节点</p></td>\n<td><p>主从节点</p></td>\n<td><p>领导者（Leader）<br />候选者（Candidate）<br />跟随者（Follower）</p></td>\n<td><p>领导者（Leader）<br />跟随者（Follower）</p></td>\n</tr>\n<tr>\n<td><p>选举过程</p></td>\n<td><p>从节点发现领导者无响应就发起选举，选节点 ID 最大的</p></td>\n<td><p>节点从跟随者转换为候选者参加竞选，每个节点都对候选者进行投票，票数过半的候选者当选为领导者</p></td>\n<td><p>节点处在 Looking 状态时参与竞选，通过比较 <code>ZXID</code>，<code>ServerID</code> 来选择最佳的节点作为领导者</p></td>\n</tr>\n<tr>\n<td><p>所需时间</p></td>\n<td><p>短</p></td>\n<td><p>较短</p></td>\n<td><p>较长</p></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"nav_point_98\">4.5　ZooKeeper——分布式系统的实践</h2>\n<p>本章主要介绍分布式系统中服务以及应用的协同，既有多个服务同时操作同一个资源的情况，也有一个服务操作多个资源的情况。为了保证数据一致性和分布式事务引入了协调者，为了协调者的可用性引入了集群的主从复制机制，当主节点死机的时候会启动选举机制选出新的主节点，保证整个集群的可用性。说完了分布式协同的理论，再来看看分布式系统的最佳实践，ZooKeeper 作为分布式系统的最佳实践在很多项目中被用到。ZooKeeper 是一个分布式的、开放源码的分布式应用程序协调服务，是 Google 公司 Chubby 一个开源实现，也是 Hadoop 和 HBase 的重要组件。作为向分布式系统提供一致性服务的软件，ZooKeeper 的功能包括配置维护、域名服务、分布式同步、组服务等。下面以一个简单的例子作为切入点对 ZooKeeper 进行介绍。</p>\n<h3 id=\"nav_point_99\">4.5.1　从一个简单的例子开始</h3>\n<p>在分布式系统中经常会遇到一种情况，就是多个应用程序同时读取一个配置。例如 A、B 两个应用程序都去读取配置 C 中的内容，如果 C 中的内容出现变化，会同时通知 A 和 B。</p>\n<p>一般的做法是 A、B 以固定频率询问 C 的变化，或者使用观察者模式来监听 C，等发现变化以后再更新自身。那么 ZooKeeper 如何协调这种场景。</p>\n<p>如图 4-44 所示，ZooKeeper 会建立一个 ZooKeeper 服务器，暂且称之为 ZServer，用它来存放 C 的值。另外，分别为 A，B 两个应用程序生成两个客户端，称作 ClientA 和 ClientB。这两个客户端均连接到 ZServer 上，并获取其中存放着的 C。ZServer 中保存 C 值的地方称为 Znode。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00426.jpeg\" alt=\"\" width=\"75%\" style=\"width: 75%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-44　ClientA 和 ClientB 通过 ZServer 获取 C 的值</strong></p>\n<h3 id=\"nav_point_100\">4.5.2　Znode 的原理与使用</h3>\n<p>在上面的例子中，ClientA 和 ClientB 需要读取 C 的值。这个 C 作为树的叶子节点存放在 ZooKeeper 的 Znode 中。通常来说，为了提高效率，Znode 是存放在内存中的。</p>\n<p>Znode 的数据模型是一棵树（Znode Tree）。就像我们从图 4-44 中看到的一样，树中的每个节点都可以存放数据，并且每个节点下面都可以存放叶子节点。</p>\n<p>ZooKeeper 客户端以 / 作为访问路径来访问数据。例如通过路径 /RootNode/C 来访问 C 变量。为了方便客户端调用，ZooKeeper 会暴露一些命令，命令与其含义的对照关系如图 4-45 所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00427.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-45　ZooKeeper 命令与含义的对照关系</strong></p>\n<p>作为存储媒介，Znode 分为持久节点和临时节点。</p>\n<ul>\n<li><strong>持久节点（PERSISTENT）</strong>：该数据节点自创建后，就一直存在于 ZooKeeper 服务器上，除非执行删除操作（delete）清除该节点。</li>\n<li><p><strong>临时节点（EPHEMERAL）</strong>：该数据节点的生命周期和客户端会话（ClientSession）绑定在一起。如果客户端会话丢失了，那么该节点会自动清除。</p>\n<p>如果把临时节点看成资源，那么当客户端和服务器产生会话并生成临时节点后，一旦客户端与服务器中断联系，节点资源会从 Znode 中被删除。</p>\n</li>\n<li><p><strong>顺序节点（SEQUENTIAL）</strong>：﻿给 Znode 节点分配一个唯一且单调递增的整数。如果有多个客户端同时在服务器 /tasks 上申请节点，服务器会根据客户端申请的先后顺序，将数字追加到/tasks/task 后面。</p>\n</li>\n</ul>\n<p>此时有三个客户端申请节点资源，就在 /tasks 下面建立三个顺序节点，分别是 /tasks/task1、/tasks/task2、/tasks/task3。</p>\n<p>顺序节点对处理分布式事务非常有帮助，当多个客户端协同工作时，可以让这些客户端按一定的顺序执行。如果对前两类节点和顺序节点进行组合，就有产生四种节点类型，分别是持久节点、持久顺序节点、临时节点、临时顺序节点。</p>\n<h3 id=\"nav_point_101\">4.5.3　Watcher 原理与使用</h3>\n<p>4.5.1 节说到 ZooKeeper 用 Znode 来存放数据，并且把 C 的值存储在里面。那如果 C 被更新了，该如何通知 ClientA 和 ClientB 呢？</p>\n<p>ZooKeeper 客户端会在指定的节点（/RootNote/C）上注册一个 Watcher，当 Znode 上的 C 被更新时，服务端就会通知 ClientA 和 ClientB。</p>\n<p>通过以下三步来实现，如图 4-46 所示：</p>\n<p>(1) 客户端注册 Watcher；</p>\n<p>(2) 服务端处理 Watcher；</p>\n<p>(3) 客户端回调 Watcher。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00428.jpeg\" alt=\"\" width=\"70%\" style=\"width: 70%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-46　Watcher 的注册、处理、回调</strong></p>\n<p>下面详细介绍实现 ZNode 数据更新的三个步骤。</p>\n<ol>\n<li><p><strong>客户端注册 Watcher</strong></p>\n<p>ZooKeeper 客户端创建 Watcher 的实例对象。下面的代码使用 ZooKeeper 创建客户端：</p>\n<pre class=\"code-rows\"><code>Public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher);</code></pre>\n<p>同时这个 Watcher 会保存在客户端本地，一直用于和服务端会话。客户端可以通过 <code>getData</code>、<code>getChildren</code> 和 <code>exist</code> 方法来向服务端注册 Watcher，其中 <code>getData</code> 方法的代码如下：</p>\n<pre class=\"code-rows\"><code>Public byte getData(String path, Boolean watch, Stat stat);</code></pre>\n<p>整个注册过程分为 4 个步骤，如图 4-47 所示。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00429.jpeg\" alt=\"\" width=\"32%\" style=\"width: 32%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-47　客户端注册 Watcher 的简图</strong></p>\n<p>需要注意在客户端发送 Watcher 到服务端进行注册时，会将这个要发送的 Watcher 保存在本地的 ZKWatchManager 中。这样做的好处是服务端注册成功以后，就不用将 Watcher 的具体内容回传给客户端了，客户端只需在接收到服务端响应以后，从本地的 ZKWatchManager 中获取 Watcher 的信息进行处理即可。</p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>服务端处理 Watcher</strong></p>\n<p>服务端接收到客户端的请求以后，交给 FinalRequestProcessor 进程处理，这个进程会去 Znode 中获取对应的数据，同时会把 Watcher 加入到 WatchManager 中。这样下次这节点上的数据被更改了以后，就会通知注册 Watcher 的客户端了。图 4-48 展示了服务端处理客户端 Watcher 请求的过程。</p>\n<p class=\"p-img\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00430.jpeg\" alt=\"\" width=\"60%\" style=\"width: 60%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-48　服务端处理 Watcher 的过程</strong></p>\n<p>图 4-48 中的过程如下。</p>\n<p>(1) 客户端生成 Watcher 发起注册请求。</p>\n<p>(2) 服务端获取 Watcher 注册请求，从 DataTree 中对应的 Znode 里获取数据。</p>\n<p>(3) 客户端接收服务的返回数据，服务端将 Watcher 添加到 WatchManager 中。</p>\n<p>&nbsp;</p>\n</li>\n<li><p><strong>客户端回调 Watcher</strong></p>\n<p>客户端在响应客户端 Watcher 注册以后，会发送 WathcerEvent 事件。作为客户端，有对应的回调函数来接收这个消息，这里通过 <code>readResponse</code> 方法统一处理：</p>\n<pre class=\"code-rows\"><code>Class SendThread extends Thread{\n    Void readResponse(ByteBuffer message) throws IOException{\n\n    }\n}</code></pre>\n<p>SendTread 在接收到服务端的通知以后，会将事件通过 <code>EventThread.queueEvent</code> 发送给 EventThread。正如前面提到的，在客户端注册时，就已经将 Watcher 的具体内容保存在 ZKWatchManager 中一份了。所以 EventTread 通过 EventType 就可以知道哪个 Watcher 被响应了（数据发生了变化），然后从 ZKWatchManager 取出具体 Watcher 放到 waitingEvent 队列中，等待处理。</p>\n<p>最后，由 EventThread 中的 <code>processEvent</code> 方法依次处理数据更新的响应。</p>\n</li>\n</ol>\n<h3 id=\"nav_point_102\">4.5.4　Version 的原理与使用</h3>\n<p>介绍完了 Watcher 机制，回头再来谈谈 Znode 的版本——Version。如图 4-49 所示，客户端 ClientD 尝试修改 C 的值，此时其他两个客户端会接收到通知，然后进行后续的业务处理。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00431.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-49　ClientD 尝试修改 C 的值</strong></p>\n<p>如图 4-50 所示，在分布式系统中，会出现这么一种情况，在 ClientD 对 C 值进行写入操作的同时，另一个 ClientE 也对 C 进行写入。很明显这两个客户端会去竞争 C 资源，通常这种情况需要对 C 加锁。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00432.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-50　ClientD 和 ClientE 竞争同一个资源</strong></p>\n<p>针对上面这种情况，引入 Znode 版本的概念。版本用来保证分布式数据的原子性操作，此信息保存在 Znode 的 Stat 对象中。如图 4-51 所示，有三种版本类型，分别是 Version、CVersion 和 AVersion。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00433.jpeg\" alt=\"\" width=\"65%\" style=\"width: 65%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-51　版本类型</strong></p>\n<p>本例只关注“数据节点内容的版本号”，也就是 Version。</p>\n<p>如果将 ClientD 和 ClientE 对 C 进行写入操作视作一个事务，那么在执行写入操作之前，两个事务会分别获取节点上的值，即节点保存的数据和节点的版本号 Version。</p>\n<p>以乐观锁为例，对数据的写入操作会分成三个阶段——数据读取、写入校验和数据写入。假如 C 中保存的数据是 1，Version 是 0。此时 ClientD 和 ClientE 都获取了这两个信息。如果 ClientD 先进行写入操作，那么在进行写入校验时，会发现之前获得的 Version 和节点保存的 Version 是相同的，都是 0，因此直接执行数据写入。</p>\n<p>写入以后，Version 由原来的 0 变成了 1。因此当 ClientE 进行写入校验时，发现自己持有的 Version=0 和节点当前保存的 Version=1 不一样。于是，ClientE 写入失败，重新获取节点数据和 Version，并再次尝试写入。</p>\n<p>除上述方案外，ClientE 还可以利用 Znode 的有序性。如图 4-52 所示，在 C 下面建立多个有序的子节点，每当有客户端准备写入数据时，就创建一个临时的有序节点。节点的顺序根据 FIFO 算法而定，保证先申请写入的客户端排在前面。每个节点都会有一个序号，序号按照节点的申请次序依次递增。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00434.jpeg\" alt=\"\" width=\"85%\" style=\"width: 85%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-52　ClientD、ClientE 分别建立子 Znode</strong></p>\n<p>在这种方案下，每个客户端在执行修改 C 的操作时，都要检查有没有比自己序号小的节点，如果有就进入等待状态，直到比自己序号小的节点修改完毕，自己才能修改。这样保证了事务处理的顺序性。</p>\n<h3 id=\"nav_point_103\">4.5.5　会话的原理与使用</h3>\n<p>说完 Version 的概念，例子从原来的 ClientA 和 ClientB 扩充到了 ClientD 和 ClientE，这些客户端都会和 ZooKeeper 的服务端进行通信，或者读取数据，或者修改数据。</p>\n<p>我们将客户端和服务端完成通信的连接称作会话。ZooKeeper 的会话有 Connecting、Connected、Reconnecting、Reconnected 和 Close 这几种状态。服务端有专门的进程来管理这些状态，客户端在初始化时就会根据配置自动连接服务器，从而建立会话。客户端连接服务器时，会话处于 Connecting 状态。</p>\n<p>一旦连接完成，会话就进入 Connected 状态。如果出现延迟或者短暂失联，客户端会自动重连，Reconnecting 和 Reconnected 状态随即产生。如果连接长时间超时，或者客户端断开与服务器的连接，ZooKeeper 就会将会话以及该会话创建的临时数据节点清理掉，并且关闭和客户端的连接。</p>\n<p>Session 作为会话实体，用来代表客户端会话，其包括以下 4 个属性。</p>\n<ul>\n<li><code>SessionID</code>：用来全局唯一识别会话。</li>\n<li><code>TimeOut</code>：会话超时时间，客户端在创造 Session 实例时，会设置此属性。</li>\n<li><code>TickTime</code>：下次的会话超时时间点。这在下面要讲的分桶策略中会用到。</li>\n<li><code>isClosing</code>：当服务端如果检测到会话超时失效了，会通过设置这个属性将会话关闭。</li>\n</ul>\n<p>会话是客户端与服务器之间的连接，在服务器端由 SessionTracker 管理会话。SessionTracker 的一个工作就是将超时会话清除掉，于是分桶策略登场了。</p>\n<p>每个会话在生成时都会定义超时时间，通过计算当前时间＋超时时间就可以得到会话的过期时间。SessionTracker 并非实时监听会话超时，它是按照一定时间周期来监听的。也就是说，如果没有到达 SessionTracker 的监听时间，即使有会话过期，SessionTracker 也不会去清除。由此，引入会话超时计算公式，也就是 <code>TickTime</code> 的计算公式：</p>\n<pre class=\"code-rows\"><code>TickTime =（（当前时间＋超时时间）/检查时间间隔+1）×检查时间间隔</code></pre>\n<p>如图 4-53 所示，将 <code>TickTime</code> 值计算出来以后，SessionTracker 会把对应的会话按照这个时间放在对应的时间轴上面，并且在对应的 <code>TickTime</code> 检查会话是否过期。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00435.jpeg\" alt=\"\" width=\"95%\" style=\"width: 95%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-53　计算会话的下次过期时间点</strong></p>\n<p>每当客户端与服务器连接成功时，都会进行激活操作，并且每隔一段时间客户端会向服务器发送心跳检测。如图 4-54 所示，服务器接收到激活或者心跳检测后，会重新计算会话过期时间，根据分桶策略进行重新调整，把会话从老区块放到新区块中去。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00436.jpeg\" alt=\"\" width=\"95%\" style=\"width: 95%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-54　重新计算过期时间并且根据分桶策略调整</strong></p>\n<p>对于超时的会话，SessionTracker 会完成如下清理工作。</p>\n<p>(1) 标记会话状态为“已关闭”，也就是设置 isClosing 为 True。</p>\n<p>(2) 发起“会话关闭”的请求，让关闭操作在整个集群生效。</p>\n<p>(3) 接收集需要清理的临时节点。</p>\n<p>(4) 添加“节点删除”的事务变更。</p>\n<p>(5) 删除临时节点。</p>\n<p>(6) 移除会话。</p>\n<p>(7) 关闭客户端与服务端的连接。</p>\n<p>会话关闭以后，客户端就无法从服务端获取、写入数据了。</p>\n<h3 id=\"nav_point_104\">4.5.6　服务群组</h3>\n<p>4.5.5 节讲到了客户端如何通过会话与服务端保持联系，以及服务端是如何管理客户端会话的。我们继续思考一下，那么多服务端都依赖一个 ZooKeeper 服务器，如果服务器挂掉，那客户端岂不是无法工作了。为了提高 ZooKeeper 服务器的可靠性，引入了服务器集群的概念，如图 4-55 所示，从原来的单个服务器，扩充成多个服务器共同提供服务，这样即使其中某一台服务器挂了，其他的也可以顶替其工作。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00437.jpeg\" alt=\"\" width=\"60%\" style=\"width: 60%\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-55　ZooKeeper 的服务器集群</strong></p>\n<p>这样看起来效果不错，但又出现了新的问题，此时存在多个 ZooKeeper 服务器，那么客户端请求发给哪个呢？各服务器之间又该如何同步数据？当一个服务器挂掉时其他服务器如何替代？这里引入两个概念：领导者服务器和跟随者服务器。</p>\n<ul>\n<li>领导者服务器，是事务请求（写操作）的唯一调度者和处理者，保证集群事务处理的顺序性；也是集群内部服务器的调度者，它是整个集群的老大，其他服务器接收到事务请求后都会转交给它，由它协调处理。</li>\n<li>跟随者服务器，处理非事务请求（读操作），把事务请求转发给领导者服务器。参与选举领导者服务器的投票和事务请求 Proposal 的投票。</li>\n</ul>\n<p>领导者作为集群老大，是如何产生的呢？ZooKeeper 有仲裁机制，按照少数服从多数的原则，通过选举产生领导者。因此集群中服务器的个数一般都选奇数，例如 1、3、5，当然这只是个建议。选举和仲裁都有相关的算法，一起来看看吧。</p>\n<p>当众多服务器同时启动时，它们都不知道谁是领导者，因此都进入 Looking 状态，在网络中寻找领导者。寻找的过程即投票的过程，每个服务器都会将服务器 ID 和事务 ID 作为投票信息发送给网络中的其他服务器。假设称投票信息为 VOTE，VOTE 表示为 <code>(ServerID, ZXID)</code>。</p>\n<p><code>ServerID</code> 是服务器注册的 ID，随着服务器的启动顺序自动增加，启动越靠后的服务器 <code>ServerID</code> 越大；<code>ZXID</code> 是服务器处理事务的 ID，随着事务的增加自动增加，同样靠后提交的事务 <code>ZXID</code> 也会大一些。</p>\n<p>其他的服务器接收到 VOTE 信息以后会和自己的 VOTE 信息做比较，如果前者中的 <code>ZXID</code> 比自己的大，就把自己的 VOTE 信息修改成接收到的 VOTE。如果两个 <code>ZXID</code> 一样大，就比较 <code>ServerID</code>，将较大的那个 <code>ServerID</code> 作为自己 VOTE 信息中的 <code>ServerID</code>，转发给其他服务器。</p>\n<p>下面来个具体的例子，有三个服务器 S1、S2 和 S3，它们的 VOTE 信息分别是 <code>(1,6)</code>、<code>(2,5)</code>、<code>(3,5)</code>。</p>\n<p>三个服务器分别把自己的 VOTE 信息发给其他两个，S2 和 S3 接收到 S1 的 VOTE 以后发现 <code>ZXID</code> 为 <code>6</code>，比自己持有的 <code>ZXID</code> 要大，因此将自己的 VOTE 修改为 <code>(1,6)</code> 并投出去，最后 S1 被选为领导者。S1 作为领导者，如果因为某种原因挂掉或者长时间没有响应请求，其他服务器也会进入 Looking 状态，开启投票仲裁模式，寻找下一个领导者。选出的新领导者会通过广播的方式将 Znode 上的数据同步给其他跟随者。</p>\n<p>有了领导者，整个服务器集群就有了领袖，它可以处理客户端的事务请求。ZooKeeper 服务器可以将客户端的请求发送给集群中任意一个服务器，无论是哪个服务器都会将事务请求转交给领导者。领导者在将数据写入 Znode 之前，会向 ZooKeeper 集群的其他跟随者发送广播消息。这里的广播用到了 ZAB 协议（ZooKeeper Atomic Broadcast Protocol），这是 Paxos 协议的实践，说白了就是一个两段提交。</p>\n<p>如图 4-56 所示，ZooKeeper 通过以下方式实现两段提交。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00438.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-56　ZooKeeper 实现两段提交</strong></p>\n<p>具体方式如下。</p>\n<p>(1) 当集群领导者接收到提交信息以后，向所有跟随者发送一个 PROPOSAL。</p>\n<p>(2) 当跟随者接收到 PROPOSAL 后，返回给领导者一个 ACK 消息，表示接收到 PROPOSAL，并且准备好了。</p>\n<p>(3)领导者进行仲裁，如果接收到数量过半的跟随者发送的 ACK 消息（包括领导者自己），就发送消息通知跟随者进行提交。接下来跟随者开始干活，将数据写入到 Znode 中。</p>\n<p>集群选举出了领导者，领导者接收到客户端的请求以后，便可以协调跟随者工作了。</p>\n<p>当客户端很多，特别是这些客户端都请求读操作时，ZooKeeper 服务器如何处理这么多的请求呢？由此引入观察者的概念。</p>\n<p>观察者和跟随者基本一致，对于非事务请求（读操作），可以直接返回节点中的信息（数据是从领导者中同步过来的）；对于事务请求（写操作），则会转交给领导者做统一处理。观察者的存在就是为了解决大量客户端的读请求，它和跟随者的区别是观察者不参与选举领导者的仲裁投票。最后通过图 4-57 来看看 ZooKeeper 的几个组件，以及领导者、跟随者和观察者之间的关系。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100014/image00439.jpeg\" alt=\"{%}\" /></p>\n<p class=\"ebook-image-title\"><strong>图 4-57　观察者加入领导者和跟随者的大家庭</strong></p>\n<h2 id=\"nav_point_105\">4.6　总结</h2>\n<p>分布式系统中应用之间或者服务之间的协同是本章的介绍重点。本章中，我们从分布式系统的特征和互斥问题入手，介绍了分布式互斥的三种算法：集中互斥算法、基于许可的互斥算法和令牌环互斥算法。用分布式锁的方式实现互斥，通过 Redis 缓存和通过 ZooKeeper 是最为普遍的两种实现分布式锁的方式。针对一个应用操作多个资源的情况，提出了分布式事务的概念，由刚性事务到柔性事务提出了 ACID 理论、CAP 理论和 BASE 理论。DTP 作为分布式事务的模型定义了事务的基本处理方式和协议，2PC 和 TCC 两种解决方案就是 DTP 最佳实践的产物。在分布式互斥和事务的实践中引入了协调者的角色，目的是协调和管理资源与进程，为了提高协调者的可用性对其进行集群部署，提供了主从互备。集群中如何选举主服务器，就引出了分布式选举的算法：Bully 算法、Raft 算法和 ZAB 算法。最后通过一个简单的例子给大家简要介绍了实现 ZooKeeper 的基本原理和组件。</p>\n\n<br style=\"page-break-after:always\" />","neighbors":{"left":{"article_title":"第 4 章 分布式协同(1)","id":741055},"right":{"article_title":"第 5 章 分布式计算","id":741057}},"comments":[]}