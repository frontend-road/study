{"id":222439,"title":"01 | 日志段：保存消息文件的对象是怎么实现的？","content":"<p>你好，我是胡夕。</p><p>今天，我们开始学习Kafka源代码分析的第一模块：日志（Log）、日志段（LogSegment）以及索引（Index）源码。</p><p>日志段及其相关代码是Kafka服务器源码中最为重要的组件代码之一。你可能会非常关心，在Kafka中，消息是如何被保存和组织在一起的。毕竟，<strong>不管是学习任何消息引擎，弄明白消息建模方式都是首要的问题</strong>。因此，你非常有必要学习日志段这个重要的子模块的源码实现。</p><p>除此之外，了解日志段也有很多实际意义，比如说，你一定对Kafka底层日志文件00000000000000012345.log的命名感到很好奇。学过日志段之后，我相信这个问题一定会迎刃而解的。</p><p>今天，我会带你详细看下日志段部分的源码。不过在此之前，你需要先了解一下Kafka的日志结构。</p><h2>Kafka日志结构概览</h2><p>Kafka日志在磁盘上的组织架构如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/72/4b/72fb27cb49e41a61524322ab6bd1cb4b.jpg?wh=4000*2250\" alt=\"\"></p><p>日志是Kafka服务器端代码的重要组件之一，很多其他的核心组件都是以日志为基础的，比如后面要讲到的状态管理机和副本管理器等。</p><p>总的来说，Kafka日志对象由多个日志段对象组成，而每个日志段对象会在磁盘上创建一组文件，包括消息日志文件（.log）、位移索引文件（.index）、时间戳索引文件（.timeindex）以及已中止（Aborted）事务的索引文件（.txnindex）。当然，如果你没有使用Kafka事务，已中止事务的索引文件是不会被创建出来的。图中的一串数字0是该日志段的起始位移值（Base Offset），也就是该日志段中所存的第一条消息的位移值。</p><!-- [[[read_end]]] --><p>一般情况下，一个Kafka主题有很多分区，每个分区就对应一个Log对象，在物理磁盘上则对应于一个子目录。比如你创建了一个双分区的主题test-topic，那么，Kafka在磁盘上会创建两个子目录：test-topic-0和test-topic-1。而在服务器端，这就是两个Log对象。每个子目录下存在多组日志段，也就是多组.log、.index、.timeindex文件组合，只不过文件名不同，因为每个日志段的起始位移不同。</p><h2>日志段代码解析</h2><p>阅读日志段源码是很有必要的，因为日志段是Kafka保存消息的最小载体。也就是说，消息是保存在日志段中的。然而，官网对于日志段的描述少得可怜，以至于很多人对于这么重要的概念都知之甚少。</p><p>但是，不熟悉日志段的话，如果在生产环境出现相应的问题，我们是没有办法快速找到解决方案的。我跟你分享一个真实案例。</p><p>我们公司之前碰到过一个问题，当时，大面积日志段同时间切分，导致瞬时打满磁盘I/O带宽。对此，所有人都束手无策，最终只能求助于日志段源码。</p><p>最后，我们在LogSegment的shouldRoll方法中找到了解决方案：设置Broker端参数log.roll.jitter.ms值大于0，即通过给日志段切分执行时间加一个扰动值的方式，来避免大量日志段在同一时刻执行切分动作，从而显著降低磁盘I/O。</p><p>后来在复盘的时候，我们一致认为，阅读LogSegment源码是非常正确的决定。否则，单纯查看官网对该参数的说明，我们不一定能够了解它的真实作用。那，log.roll.jitter.ms参数的具体作用是啥呢？下面咱们说日志段的时候，我会给你详细解释下。</p><p>那话不多说，现在我们就来看一下日志段源码。我会重点给你讲一下日志段类声明、append方法、read方法和recover方法。</p><p>你首先要知道的是，日志段源码位于 Kafka 的 core 工程下，具体文件位置是 core/src/main/scala/kafka/log/LogSegment.scala。实际上，所有日志结构部分的源码都在 core 的 kafka.log 包下。</p><p>该文件下定义了三个 Scala 对象：</p><ul>\n<li>LogSegment class；</li>\n<li>LogSegment object；</li>\n<li>LogFlushStats object。LogFlushStats 结尾有个 Stats，它是做统计用的，主要负责为日志落盘进行计时。</li>\n</ul><p>我们主要关心的是 <strong>LogSegment class 和 object</strong>。在 Scala 语言里，在一个源代码文件中同时定义相同名字的 class 和 object 的用法被称为伴生（Companion）。Class 对象被称为伴生类，它和 Java 中的类是一样的；而 Object 对象是一个单例对象，用于保存一些静态变量或静态方法。如果用 Java 来做类比的话，我们必须要编写两个类才能实现，这两个类也就是LogSegment 和 LogSegmentUtils。在 Scala 中，你直接使用伴生就可以了。</p><p>对了，值得一提的是，Kafka 中的源码注释写得非常详细。我不打算把注释也贴出来，但我特别推荐你要读一读源码中的注释。比如，今天我们要学习的日志段文件开头的一大段注释写得就非常精彩。我截一个片段让你感受下：</p><p><span class=\"orange\">A segment of the log. Each segment has two components: a log and an index. The log is a FileRecords containing the actual messages. The index is an OffsetIndex that maps from logical offsets to physical file positions. Each segment has a base offset which is an offset &lt;= the least offset of any message in this segment and &gt; any offset in any previous segment.</span></p><p>这段文字清楚地说明了每个日志段由两个核心组件构成：日志和索引。当然，这里的索引泛指广义的索引文件。另外，这段注释还给出了一个重要的事实：每个日志段都有一个起始位移值（Base Offset），而该位移值是此日志段所有消息中最小的位移值，同时，该值却又比前面任何日志段中消息的位移值都大。看完这个注释，我们就能够快速地了解起始位移值在日志段中的作用了。</p><h3>日志段类声明</h3><p>下面，我分批次给出比较关键的代码片段，并对其进行解释。首先，我们看下 LogSegment 的定义：</p><pre><code>class LogSegment private[log] (val log: FileRecords,\n                               val lazyOffsetIndex: LazyIndex[OffsetIndex],\n                               val lazyTimeIndex: LazyIndex[TimeIndex],\n                               val txnIndex: TransactionIndex,\n                               val baseOffset: Long,\n                               val indexIntervalBytes: Int,\n                               val rollJitterMs: Long,\n\tval time: Time) extends Logging { … }\n</code></pre><p>就像我前面说的，一个日志段包含<strong>消息日志文件</strong>、<strong>位移索引文件</strong>、<strong>时间戳索引文件</strong>、<strong>已中止事务索引文件</strong>等。这里的 FileRecords 就是实际保存 Kafka 消息的对象。专栏后面我将专门讨论 Kafka 是如何保存具体消息的，也就是 FileRecords 及其家族的实现方式。同时，我还会给你介绍一下社区在持久化消息这块是怎么演进的，你一定不要错过那部分的内容。</p><p>下面的 lazyOffsetIndex、lazyTimeIndex 和 txnIndex 分别对应于刚才所说的 3 个索引文件。不过，在实现方式上，前两种使用了延迟初始化的原理，降低了初始化时间成本。后面我们在谈到索引的时候再详细说。</p><p>每个日志段对象保存自己的起始位移 <strong>baseOffset</strong>——这是非常重要的属性！事实上，你在磁盘上看到的文件名就是baseOffset的值。每个LogSegment对象实例一旦被创建，它的起始位移就是固定的了，不能再被更改。</p><p>indexIntervalBytes 值其实就是 Broker 端参数 log.index.interval.bytes 值，它控制了<strong>日志段对象新增索引项的频率</strong>。默认情况下，日志段至少新写入 4KB 的消息数据才会新增一条索引项。而 rollJitterMs 是日志段对象新增倒计时的“扰动值”。因为目前 Broker 端日志段新增倒计时是全局设置，这就是说，在未来的某个时刻可能同时创建多个日志段对象，这将极大地增加物理磁盘 I/O 压力。有了 rollJitterMs 值的干扰，每个新增日志段在创建时会彼此岔开一小段时间，这样可以缓解物理磁盘的 I/O 负载瓶颈。</p><p>至于最后的 time 参数，它就是用于统计计时的一个实现类，在 Kafka 源码中普遍出现，我就不详细展开讲了。</p><p>下面我来说一些重要的方法。</p><p>对于一个日志段而言，最重要的方法就是写入消息和读取消息了，它们分别对应着源码中的 append 方法和 read 方法。另外，recover方法同样很关键，它是Broker重启后恢复日志段的操作逻辑。</p><h3>append方法</h3><p>我们先来看append 方法，了解下写入消息的具体操作。</p><pre><code>def append(largestOffset: Long,\n             largestTimestamp: Long,\n             shallowOffsetOfMaxTimestamp: Long,\n             records: MemoryRecords): Unit = {\n    if (records.sizeInBytes &gt; 0) {\n      trace(s&quot;Inserting ${records.sizeInBytes} bytes at end offset $largestOffset at position ${log.sizeInBytes} &quot; +\n            s&quot;with largest timestamp $largestTimestamp at shallow offset $shallowOffsetOfMaxTimestamp&quot;)\n      val physicalPosition = log.sizeInBytes()\n      if (physicalPosition == 0)\n        rollingBasedTimestamp = Some(largestTimestamp)\n\n      ensureOffsetInRange(largestOffset)\n\n      // append the messages\n      val appendedBytes = log.append(records)\n      trace(s&quot;Appended $appendedBytes to ${log.file} at end offset $largestOffset&quot;)\n      // Update the in memory max timestamp and corresponding offset.\n      if (largestTimestamp &gt; maxTimestampSoFar) {\n        maxTimestampSoFar = largestTimestamp\n        offsetOfMaxTimestampSoFar = shallowOffsetOfMaxTimestamp\n      }\n      // append an entry to the index (if needed)\n      if (bytesSinceLastIndexEntry &gt; indexIntervalBytes) {\n        offsetIndex.append(largestOffset, physicalPosition)\n        timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestampSoFar)\n        bytesSinceLastIndexEntry = 0\n      }\n      bytesSinceLastIndexEntry += records.sizeInBytes\n    }\n  }\n</code></pre><p>append 方法接收 4 个参数，分别表示待写入消息批次中消息的<strong>最大位移值</strong>、<strong>最大时间戳</strong>、<strong>最大时间戳对应消息的位移</strong>以及<strong>真正要写入的消息集合</strong>。下面这张图展示了 append 方法的完整执行流程：</p><p><img src=\"https://static001.geekbang.org/resource/image/67/5c/6700570d3052fcadda54767ed8dc385c.jpg?wh=3515*2250\" alt=\"\"></p><p><strong>第一步：</strong></p><p>在源码中，首先调用 log.sizeInBytes 方法判断该日志段是否为空，如果是空的话， Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。</p><p><strong>第二步：</strong></p><p>代码调用 ensureOffsetInRange 方法确保输入参数最大位移值是合法的。那怎么判断是不是合法呢？标准就是看它与日志段起始位移的差值是否在整数范围内，即 largestOffset - baseOffset的值是不是介于 [0，Int.MAXVALUE] 之间。在极个别的情况下，这个差值可能会越界，这时，append 方法就会抛出异常，阻止后续的消息写入。一旦你碰到这个问题，你需要做的是升级你的 Kafka 版本，因为这是由已知的 Bug 导致的。</p><p><strong>第三步：</strong></p><p>待这些做完之后，append 方法调用 FileRecords 的 append 方法执行真正的写入。前面说过了，专栏后面我们会详细介绍 FileRecords 类。这里你只需要知道它的工作是将内存中的消息对象写入到操作系统的页缓存就可以了。</p><p><strong>第四步：</strong></p><p>再下一步，就是更新日志段的最大时间戳以及最大时间戳所属消息的位移值属性。每个日志段都要保存当前最大时间戳信息和所属消息的位移信息。</p><p>还记得 Broker 端提供定期删除日志的功能吗？比如我只想保留最近 7 天的日志，没错，当前最大时间戳这个值就是判断的依据；而最大时间戳对应的消息的位移值则用于时间戳索引项。虽然后面我会详细介绍，这里我还是稍微提一下：<strong>时间戳索引项保存时间戳与消息位移的对应关系</strong>。在这步操作中，Kafka会更新并保存这组对应关系。</p><p><strong>第五步：</strong></p><p>append 方法的最后一步就是更新索引项和写入的字节数了。我在前面说过，日志段每写入 4KB 数据就要写入一个索引项。当已写入字节数超过了 4KB 之后，append 方法会调用索引对象的 append 方法新增索引项，同时清空已写入字节数，以备下次重新累积计算。</p><h3>read 方法</h3><p>好了，append 方法我就解释完了。下面我们来看read方法，了解下读取日志段的具体操作。</p><pre><code>def read(startOffset: Long,\n           maxSize: Int,\n           maxPosition: Long = size,\n           minOneMessage: Boolean = false): FetchDataInfo = {\n    if (maxSize &lt; 0)\n      throw new IllegalArgumentException(s&quot;Invalid max size $maxSize for log read from segment $log&quot;)\n\n    val startOffsetAndSize = translateOffset(startOffset)\n\n    // if the start position is already off the end of the log, return null\n    if (startOffsetAndSize == null)\n      return null\n\n    val startPosition = startOffsetAndSize.position\n    val offsetMetadata = LogOffsetMetadata(startOffset, this.baseOffset, startPosition)\n\n    val adjustedMaxSize =\n      if (minOneMessage) math.max(maxSize, startOffsetAndSize.size)\n      else maxSize\n\n    // return a log segment but with zero size in the case below\n    if (adjustedMaxSize == 0)\n      return FetchDataInfo(offsetMetadata, MemoryRecords.EMPTY)\n\n    // calculate the length of the message set to read based on whether or not they gave us a maxOffset\n    val fetchSize: Int = min((maxPosition - startPosition).toInt, adjustedMaxSize)\n\n    FetchDataInfo(offsetMetadata, log.slice(startPosition, fetchSize),\n      firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)\n  }\n</code></pre><p>read 方法接收 4 个输入参数。</p><ul>\n<li>startOffset：要读取的第一条消息的位移；</li>\n<li>maxSize：能读取的最大字节数；</li>\n<li>maxPosition ：能读到的最大文件位置；</li>\n<li>minOneMessage：是否允许在消息体过大时至少返回第一条消息。</li>\n</ul><p>前3个参数的含义很好理解，我重点说下第 4 个。当这个参数为 true 时，即使出现消息体字节数超过了 maxSize 的情形，read 方法依然能返回至少一条消息。引入这个参数主要是为了确保不出现消费饿死的情况。</p><p>下图展示了 read 方法的完整执行逻辑：</p><p><img src=\"https://static001.geekbang.org/resource/image/61/45/61c97ee41b52e63e771cf5503e0ee345.jpg?wh=2778*2024\" alt=\"\"></p><p>逻辑很简单，我们一步步来看下。</p><p>第一步是调用 translateOffset 方法定位要读取的起始文件位置 （startPosition）。输入参数 startOffset 仅仅是位移值，Kafka 需要根据索引信息找到对应的物理文件位置才能开始读取消息。</p><p>待确定了读取起始位置，日志段代码需要根据这部分信息以及 maxSize 和 maxPosition 参数共同计算要读取的总字节数。举个例子，假设 maxSize=100，maxPosition=300，startPosition=250，那么 read 方法只能读取 50 字节，因为 maxPosition - startPosition = 50。我们把它和maxSize参数相比较，其中的最小值就是最终能够读取的总字节数。</p><p>最后一步是调用 FileRecords 的 slice 方法，从指定位置读取指定大小的消息集合。</p><h3>recover 方法</h3><p>除了append 和read 方法，LogSegment 还有一个重要的方法需要我们关注，它就是 recover方法，用于<strong>恢复日志段</strong>。</p><p>下面的代码是 recover 方法源码。什么是恢复日志段呢？其实就是说， Broker 在启动时会从磁盘上加载所有日志段信息到内存中，并创建相应的 LogSegment 对象实例。在这个过程中，它需要执行一系列的操作。</p><pre><code>def recover(producerStateManager: ProducerStateManager, leaderEpochCache: Option[LeaderEpochFileCache] = None): Int = {\n    offsetIndex.reset()\n    timeIndex.reset()\n    txnIndex.reset()\n    var validBytes = 0\n    var lastIndexEntry = 0\n    maxTimestampSoFar = RecordBatch.NO_TIMESTAMP\n    try {\n      for (batch &lt;- log.batches.asScala) {\n        batch.ensureValid()\n        ensureOffsetInRange(batch.lastOffset)\n\n        // The max timestamp is exposed at the batch level, so no need to iterate the records\n        if (batch.maxTimestamp &gt; maxTimestampSoFar) {\n          maxTimestampSoFar = batch.maxTimestamp\n          offsetOfMaxTimestampSoFar = batch.lastOffset\n        }\n\n        // Build offset index\n        if (validBytes - lastIndexEntry &gt; indexIntervalBytes) {\n          offsetIndex.append(batch.lastOffset, validBytes)\n          timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestampSoFar)\n          lastIndexEntry = validBytes\n        }\n        validBytes += batch.sizeInBytes()\n\n        if (batch.magic &gt;= RecordBatch.MAGIC_VALUE_V2) {\n          leaderEpochCache.foreach { cache =&gt;\n            if (batch.partitionLeaderEpoch &gt; 0 &amp;&amp; cache.latestEpoch.forall(batch.partitionLeaderEpoch &gt; _))\n              cache.assign(batch.partitionLeaderEpoch, batch.baseOffset)\n          }\n          updateProducerState(producerStateManager, batch)\n        }\n      }\n    } catch {\n      case e@ (_: CorruptRecordException | _: InvalidRecordException) =&gt;\n        warn(&quot;Found invalid messages in log segment %s at byte offset %d: %s. %s&quot;\n          .format(log.file.getAbsolutePath, validBytes, e.getMessage, e.getCause))\n    }\n    val truncated = log.sizeInBytes - validBytes\n    if (truncated &gt; 0)\n      debug(s&quot;Truncated $truncated invalid bytes at the end of segment ${log.file.getAbsoluteFile} during recovery&quot;)\n\n    log.truncateTo(validBytes)\n    offsetIndex.trimToValidSize()\n    // A normally closed segment always appends the biggest timestamp ever seen into log segment, we do this as well.\n    timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestampSoFar, skipFullCheck = true)\n    timeIndex.trimToValidSize()\n    truncated\n  }\n</code></pre><p>我依然使用一张图来说明 recover 的处理逻辑：</p><p><img src=\"https://static001.geekbang.org/resource/image/eb/6c/eb5bd324685ee393e8a3072fc4b4276c.jpg?wh=3136*2188\" alt=\"\"></p><p>recover 开始时，代码依次调用索引对象的 reset 方法清空所有的索引文件，之后会开始遍历日志段中的所有消息集合或消息批次（RecordBatch）。对于读取到的每个消息集合，日志段必须要确保它们是合法的，这主要体现在两个方面：</p><ol>\n<li>该集合中的消息必须要符合 Kafka 定义的二进制格式；</li>\n<li>该集合中最后一条消息的位移值不能越界，即它与日志段起始位移的差值必须是一个正整数值。</li>\n</ol><p>校验完消息集合之后，代码会更新遍历过程中观测到的最大时间戳以及所属消息的位移值。同样，这两个数据用于后续构建索引项。再之后就是不断累加当前已读取的消息字节数，并根据该值有条件地写入索引项。最后是更新事务型Producer的状态以及Leader Epoch缓存。不过，这两个并不是理解Kafka日志结构所必需的组件，因此，我们可以忽略它们。</p><p>遍历执行完成后，Kafka 会将日志段当前总字节数和刚刚累加的已读取字节数进行比较，如果发现前者比后者大，说明日志段写入了一些非法消息，需要执行截断操作，将日志段大小调整回合法的数值。同时， Kafka 还必须相应地调整索引文件的大小。把这些都做完之后，日志段恢复的操作也就宣告结束了。</p><h2>总结</h2><p>今天，我们对Kafka日志段源码进行了重点的分析，包括日志段的append方法、read方法和recover方法。</p><ol>\n<li>append方法：我重点分析了源码是如何写入消息到日志段的。你要重点关注一下写操作过程中更新索引的时机是如何设定的。</li>\n<li>read方法：我重点分析了源码底层读取消息的完整流程。你要关注下Kafka计算待读取消息字节数的逻辑，也就是maxSize、maxPosition和startOffset是如何共同影响read方法的。</li>\n<li>recover方法：这个操作会读取日志段文件，然后重建索引文件。再强调一下，<strong>这个操作在执行过程中要读取日志段文件</strong>。因此，如果你的环境上有很多日志段文件，你又发现Broker重启很慢，那你现在就知道了，这是因为Kafka在执行recover的过程中需要读取大量的磁盘文件导致的。你看，这就是我们读取源码的收获。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/15/80/158bed3c92e7205fc450bb8b2d136480.jpg?wh=2521*1875\" alt=\"\"></p><p>这三个方法是日志段对象最重要的功能。你一定要仔细阅读它们，尽量做到对源码中每行代码的作用都了然于心。没有什么代码是读一遍不能理解的，如果有，那就再多读几遍。另外，我希望你特别关注下append和read方法，它们将是后面我们讨论日志对象时重点会用到的两个方法。毕竟，读写日志是Kafka最常用的操作，而日志读取底层调用的就是日志段的这两个方法。</p><h2>课后讨论</h2><p>如果你查看日志段源码的话，你会发现，还有一个比较重要的方法我没有提到，那就是truncateTo方法，这个方法会将日志段中的数据强制截断到指定的位移处。该方法只有20几行代码，我希望你可以自己去阅读下，然后思考这样一个问题：如果指定的位移值特别特别大，以至于超过了日志段本身保存的最大位移值，该方法对执行效果是怎么样的？</p><p>欢迎你在留言区畅所欲言，跟我交流讨论，也欢迎你把文章分享给你的朋友。</p>","neighbors":{"left":{"article_title":"重磅加餐 | 带你快速入门Scala语言","id":230848},"right":{"article_title":"02 | 日志（上）：日志究竟是如何加载日志段的？","id":224795}},"comments":[{"had_liked":false,"id":206985,"user_name":"东风第一枝","can_delete":false,"product_type":"c1","uid":1402697,"ip_address":"","ucode":"0CD0F62E90DAD8","user_header":"https://static001.geekbang.org/account/avatar/00/15/67/49/864dba17.jpg","comment_is_top":true,"comment_ctime":1586968947,"is_pvip":false,"replies":[{"id":"77311","content":"对的：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587002315,"ip_address":"","comment_id":206985,"utype":1}],"discussion_count":5,"race_medal":0,"score":"9.2233720770964992e+18","product_id":100050101,"comment_content":"offsetIndex.truncateTo(offset) 和 timeIndex.truncateTo(offset)中有相同的一部分代码:<br><br>\t  &#47;* There are 3 cases for choosing the new size<br>       * 1) if there is no entry in the index &lt;= the offset, delete everything<br>       * 2) if there is an entry for this exact offset, delete it and everything larger than it<br>       * 3) if there is no entry for this offset, delete everything larger than the next smallest<br>       *&#47;<br>      val newEntries =<br>        if(slot &lt; 0)<br>          0<br>        else if(relativeOffset(idx, slot) == offset - baseOffset)<br>          slot<br>        else<br>          slot + 1<br>\t\t  <br><br>我觉得这应该解释了老师的提问：如果指定的位移值特别特别大，并且没有可以截断的消息时，会截断比下一个消息的位移大的所有的消息。代码里面用了solt+1来表示。不知道我理解的对不对。","like_count":10,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491971,"discussion_content":"对的：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587002315,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1703222,"avatar":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","nickname":"Roger宇","note":"","ucode":"CBA23C01409349","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":258470,"discussion_content":"您好，不太理解您说的这个trucateTo方法，您说的这个方法是截断索引文件的吧？老师让我们看的不是log.truncateTo方法吗？“：如果指定的位移值特别特别大，并且没有可以截断的消息时，会截断比下一个消息的位移大的所有的消息”，请问了老师在哪里说到了这点呢？我没有看到，是后面的章节吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588689213,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1965728,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/fe/a0/b1212e2b.jpg","nickname":"🙊顾小顾","note":"","ucode":"FC7974A351586C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250058,"discussion_content":"没有明白欸。这个和超过日志长度报异常是不是矛盾的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587988334,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1402697,"avatar":"https://static001.geekbang.org/account/avatar/00/15/67/49/864dba17.jpg","nickname":"东风第一枝","note":"","ucode":"0CD0F62E90DAD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":244999,"discussion_content":"会保留的，这个截断是一次性操作，不会影响到后面的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587646567,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1179580,"avatar":"https://static001.geekbang.org/account/avatar/00/11/ff/bc/368b9f80.jpg","nickname":"灰机","note":"","ucode":"46F6CFC2DFAA2B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":244705,"discussion_content":"大佬 请问一下 truncateToEntries(newEntries) 这个位置是截取到这个消息的位置， 会截断比下一个消息的位移大的所有的消息 那么这个“下一个消息” 会保留吗？ 感谢～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587622653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210947,"user_name":"胡夕","can_delete":false,"product_type":"c1","uid":1288090,"ip_address":"","ucode":"5709A689B6683B","user_header":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","comment_is_top":true,"comment_ctime":1587868748,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"9.2233720685074002e+18","product_id":100050101,"comment_content":"你好，我是胡夕。我来公布上节课的“课后讨论”题答案啦～<br><br>上节课，咱们重点学习了如何构建Kafka工程和搭建源码阅读环境。课后我让你去尝试寻找kafka-console-producer.sh脚本对应的Java类是哪个文件。现在我给出答案：如果我们打开这个SHELL脚本，可以清楚地发现它调用了kafka.tools.ConsoleProducer类。这个类位于core包的src&#47;main&#47;scala&#47;kafka&#47;tools下，文件名是ConsoleProducer.scala。<br><br>怎么样，你找到了吗？我们可以一起讨论下。","like_count":7},{"had_liked":false,"id":206000,"user_name":"趣","can_delete":false,"product_type":"c1","uid":1613430,"ip_address":"","ucode":"A14AFC6CC197A9","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIHREGPzjYqiaJc42UQ1Sg0tknO4ib4JnsRvXcS4kO6cLLft1UR8qKxMQqUjibDl09xPxdKJIgAXXyEg/132","comment_is_top":false,"comment_ctime":1586771067,"is_pvip":false,"replies":[{"id":"76988","content":"谢谢，后面我们一起讨论~","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1586827761,"ip_address":"","comment_id":206000,"utype":1}],"discussion_count":1,"race_medal":0,"score":"53126378619","product_id":100050101,"comment_content":"一口气读完了这篇文章，感觉老师讲得特别清楚，现在我很有信心学好源码。谢谢老师，期待后续的更新。","like_count":12,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491640,"discussion_content":"谢谢，后面我们一起讨论~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586827761,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208519,"user_name":"Eternal","can_delete":false,"product_type":"c1","uid":1188023,"ip_address":"","ucode":"EA6FE7CC98F740","user_header":"https://static001.geekbang.org/account/avatar/00/12/20/b7/bdb3bcf0.jpg","comment_is_top":false,"comment_ctime":1587375016,"is_pvip":false,"replies":[{"id":"78144","content":"是的。出现map failed + 超多分区的情况除了调整ulimit -n之外，最好调整下vm.max_map_count~","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587519883,"ip_address":"","comment_id":208519,"utype":1}],"discussion_count":3,"race_medal":0,"score":"48832015272","product_id":100050101,"comment_content":"一个分区partion对应一个Log对象<br>一个Log对象对应一个日志目录<br>一个日志目录有多个LogSegment，一个Logment概念下有4个文件，多一个LogSegment下的文件都是在一个目录下的，也就是LogSegment只是一个概念不是真的文件目录<br><br>一个LogSegment就有4个物理文件，那么集群有文件=4 * LogSegment*partion*副本因子*topic。当topic和分区数很多的时候，系统的文件句柄就不够用了，好像系统默认是65535<br>所有我们对topic的数量和分区的数量需要有一个合理的规划。<br><br>我有一个疑问：我们公司之前遇到过类似的问题，业务方创建了非常多的topic和分区，然后一次broker重启的时候，重启失败，原因是Map failed 内存映射失败，这个和老师文中讲的“ Broker 在启动时会从磁盘上加载所有日志段信息到内存中，并创建相应的 LogSegment 对象实例” 好像是一样的，不止我理解的对不对：","like_count":11,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492487,"discussion_content":"是的。出现map failed + 超多分区的情况除了调整ulimit -n之外，最好调整下vm.max_map_count~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587519883,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1125656,"avatar":"https://static001.geekbang.org/account/avatar/00/11/2d/18/918eaecf.jpg","nickname":"后端进阶","note":"","ucode":"480F48F5378307","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":307356,"discussion_content":"你好，4 * LogSegment*partion*副本因子*topic，好像是漏了一个参数，因为一个日志目录有可能包含多个LogSegment，是不是还要乘以一个 n（n >=1）？","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1600609519,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1188023,"avatar":"https://static001.geekbang.org/account/avatar/00/12/20/b7/bdb3bcf0.jpg","nickname":"Eternal","note":"","ucode":"EA6FE7CC98F740","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1125656,"avatar":"https://static001.geekbang.org/account/avatar/00/11/2d/18/918eaecf.jpg","nickname":"后端进阶","note":"","ucode":"480F48F5378307","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":307422,"discussion_content":"是的，至少一个logsegment，随着数据量增加，logsegmemt数量也增加","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600647281,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":307356,"ip_address":""},"score":307422,"extra":""}]}]},{"had_liked":false,"id":212637,"user_name":"小崔","can_delete":false,"product_type":"c1","uid":1104958,"ip_address":"","ucode":"4E537416787752","user_header":"https://static001.geekbang.org/account/avatar/00/10/dc/3e/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1588167887,"is_pvip":false,"replies":[{"id":"79136","content":"1. 都可以的。Kafka支持两种时间戳策略，CREATE和APPEND。前者是producer端生成的，后者是指broker端使用本机时间覆盖producer端生成的时间戳<br>2. 由消费者角色而定。假设不考虑事务型consumer。<br><br>如果是普通consumer，那么你只能读到HW以下的消息，那么maxPosition就是HW的物理位置——这是假设读取起始位移与hw在同一个段上，否则maxPosition就是整个段的字节数；<br>同理，如果是follower，那么最多可以读到LEO，maxPosition就是LEO的物理位置——同样是假设读取起始位移与hw在同一个段上","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588400964,"ip_address":"","comment_id":212637,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31652938959","product_id":100050101,"comment_content":"1.时间戳，是broker接收到消息时设置的，还是生产者发送时设置的？<br>2.read方法里的maxSize我理解可以由消费者设定，但是maxPosition由谁设定？又有什么用呢？防止物理位置越界么？","like_count":7,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493570,"discussion_content":"1. 都可以的。Kafka支持两种时间戳策略，CREATE和APPEND。前者是producer端生成的，后者是指broker端使用本机时间覆盖producer端生成的时间戳\n2. 由消费者角色而定。假设不考虑事务型consumer。\n\n如果是普通consumer，那么你只能读到HW以下的消息，那么maxPosition就是HW的物理位置——这是假设读取起始位移与hw在同一个段上，否则maxPosition就是整个段的字节数；\n同理，如果是follower，那么最多可以读到LEO，maxPosition就是LEO的物理位置——同样是假设读取起始位移与hw在同一个段上","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588400964,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208646,"user_name":"灰机","can_delete":false,"product_type":"c1","uid":1179580,"ip_address":"","ucode":"46F6CFC2DFAA2B","user_header":"https://static001.geekbang.org/account/avatar/00/11/ff/bc/368b9f80.jpg","comment_is_top":false,"comment_ctime":1587395709,"is_pvip":false,"replies":[{"id":"78140","content":"大致意思是说producer写入4KB的消息数据后，会为当前日志段对象的索引文件中新写入一条OffsetIndex索引项。索引文件就是***.index。4KB是由Broker端参数log.index.interval.bytes决定","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587519102,"ip_address":"","comment_id":208646,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27357199485","product_id":100050101,"comment_content":"老师您好，我想问一下 “日志段至少新写入 4KB 的消息数据才会新增一条索引项” 这个是写在OffsetIndex 中么？ 对应物理位置是***.index中么？ 这个新增一条索引项不是很理解，希望老师看到后给予解答。 感谢","like_count":6,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492560,"discussion_content":"大致意思是说producer写入4KB的消息数据后，会为当前日志段对象的索引文件中新写入一条OffsetIndex索引项。索引文件就是***.index。4KB是由Broker端参数log.index.interval.bytes决定","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587519102,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":245683,"user_name":"李奕慧","can_delete":false,"product_type":"c1","uid":1201005,"ip_address":"","ucode":"0D8871ED38859C","user_header":"https://static001.geekbang.org/account/avatar/00/12/53/6d/c3828950.jpg","comment_is_top":false,"comment_ctime":1599026612,"is_pvip":false,"replies":[{"id":"90394","content":"可能发生截断、日志段被删除等情况，因此最好重建索引文件","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1599095098,"ip_address":"","comment_id":245683,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18778895796","product_id":100050101,"comment_content":"不明白为啥 recover 需要清空索引文件(这里的索引文件指的是 xxx.index 文件吗？)","like_count":4,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504957,"discussion_content":"可能发生截断、日志段被删除等情况，因此最好重建索引文件","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599095098,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":213957,"user_name":"Roger宇","can_delete":false,"product_type":"c1","uid":1703222,"ip_address":"","ucode":"CBA23C01409349","user_header":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","comment_is_top":false,"comment_ctime":1588599510,"is_pvip":false,"replies":[{"id":"79372","content":"目前producer端是可以自行指定任意时间戳的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588743777,"ip_address":"","comment_id":213957,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18768468694","product_id":100050101,"comment_content":" if (largestTimestamp &gt; maxTimestampSoFar) <br>—————- <br>请想问一下，既然是要追加的日志，而一个日志段段消息应该是顺序追加（推测），那为什么需要这个if 判断呢？请问老师，在什么情况下会出现需要追加的消息集合中最大时间戳小于等于当前日志段已经见到的时间戳的最大值，即 largestTimestamp &lt;= maxTimestampSoFar的情况呢？","like_count":4,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493912,"discussion_content":"目前producer端是可以自行指定任意时间戳的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588743777,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209555,"user_name":"灰机","can_delete":false,"product_type":"c1","uid":1179580,"ip_address":"","ucode":"46F6CFC2DFAA2B","user_header":"https://static001.geekbang.org/account/avatar/00/11/ff/bc/368b9f80.jpg","comment_is_top":false,"comment_ctime":1587566275,"is_pvip":false,"replies":[{"id":"78224","content":"可能出现这种情况：日志文件写入了消息的部分字节然后broker宕机。磁盘是块设备，它可不能保证消息的全部字节要么全部写入，要么全都不写入。因此Kafka必须有机制应对这种情况，即校验+truncate","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587603685,"ip_address":"","comment_id":209555,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18767435459","product_id":100050101,"comment_content":"先回答老师提出的问题：log.truncateTo(validBytes) validBytes为要清到的大小，在truncate方法中首先会获取当前LogSegment的文件大小，当validByte&gt; 该文件大小的时候会throws 一个异常 异常为&quot;打算清空 xxx.log 到validBytes 这么大 但是失败了，日志段大小为当前LogSegement大小&quot;,如果validByte小于当前log byte 调用fileChannel.truncate(validBytes)进行直接截断, 同时返回截断的大小有多少字节。<br>请教：老师 我看到源码中val truncated = log.sizeInBytes - validBytes  validBytes为从logFile中的bactch 批量读取message的字节数累加实现，而log.sizeInByte同样不是log文件读取出来的么？ 为什么会出现需要truncate呢？ 能否举例说明什么时候LogFile中写入了异常数据呢  这个不是很理解。","like_count":4,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492830,"discussion_content":"可能出现这种情况：日志文件写入了消息的部分字节然后broker宕机。磁盘是块设备，它可不能保证消息的全部字节要么全部写入，要么全都不写入。因此Kafka必须有机制应对这种情况，即校验+truncate","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587603685,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":332232,"user_name":"Insomnia","can_delete":false,"product_type":"c1","uid":1751214,"ip_address":"","ucode":"5986A48988D6E3","user_header":"https://static001.geekbang.org/account/avatar/00/1a/b8/ae/085484e7.jpg","comment_is_top":false,"comment_ctime":1643102099,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10233036691","product_id":100050101,"comment_content":"胡老师, 对 Kafka 的顺序读写有点疑惑:<br>看了源码中写 LogSegment 的方法本质也是调用了 FileChannel.write() 方法, 但是 PageCache 的落盘是由 OS 来操作的, 这一点并不能保证数据会落在磁盘的连续扇区上? 我不知道这个理解对不对? 如果不保证扇区的连续性, 那么读的时候预读 PageCache 的红利不就没有了么?","like_count":1,"discussions":[{"author":{"id":2826174,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/1f/be/402a04e5.jpg","nickname":"康宁","note":"","ucode":"00EC7266C1619B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":590795,"discussion_content":"PageCache的数据是会严格按照顺序落盘的，它只是OS加上去的一层缓存，不影响文件顺序","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1666077796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272184,"user_name":"张三丰","can_delete":false,"product_type":"c1","uid":1155275,"ip_address":"","ucode":"3A6215A40B3B21","user_header":"https://static001.geekbang.org/account/avatar/00/11/a0/cb/aab3b3e7.jpg","comment_is_top":false,"comment_ctime":1609983999,"is_pvip":false,"replies":[{"id":"98756","content":"索引项就是&lt;offset，物理文件位置&gt;对","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1610089573,"ip_address":"","comment_id":272184,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10199918591","product_id":100050101,"comment_content":"老师请问这个索引项是什么？ 用来做什么的？<br><br>indexIntervalBytes 值其实就是 Broker 端参数 log.index.interval.bytes 值，它控制了日志段对象新增索引项的频率。默认情况下，日志段至少新写入 4KB 的消息数据才会新增一条索引项。","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513192,"discussion_content":"索引项就是&amp;lt;offset，物理文件位置&amp;gt;对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610089573,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2826174,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/1f/be/402a04e5.jpg","nickname":"康宁","note":"","ucode":"00EC7266C1619B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":590797,"discussion_content":"其实就是个稀疏索引，简单理解为，每隔4K数据就建立一个索引，是为将来数据查询时使用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1666077903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":230665,"user_name":"云超","can_delete":false,"product_type":"c1","uid":1581783,"ip_address":"","ucode":"63DFDDAB37A04F","user_header":"https://static001.geekbang.org/account/avatar/00/18/22/d7/ae0ed31f.jpg","comment_is_top":false,"comment_ctime":1593444449,"is_pvip":false,"replies":[{"id":"85261","content":"这句话的意思是说作者会假设调用append方法的时候已经持有锁了。事实上也是这样的，它被调用时线程都持有了Partition对象的leaderIsrUpdateLock锁","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593506937,"ip_address":"","comment_id":230665,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10183379041","product_id":100050101,"comment_content":"老师，我在看append方法的时候，在注释中看到有这么一句，It is assumed this method is being called from within a lock. 我英语不太好，翻译过来大概的意思是  假设方法是从锁中调用的。我想问一下，为什么会说假设方法是从锁中调用的？或者说，这一句注释是想要告诉我们些什么呢？","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499984,"discussion_content":"这句话的意思是说作者会假设调用append方法的时候已经持有锁了。事实上也是这样的，它被调用时线程都持有了Partition对象的leaderIsrUpdateLock锁","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593506937,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1581783,"avatar":"https://static001.geekbang.org/account/avatar/00/18/22/d7/ae0ed31f.jpg","nickname":"云超","note":"","ucode":"63DFDDAB37A04F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":297868,"discussion_content":"好的，我明白了，谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597074919,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208731,"user_name":"我是个bug","can_delete":false,"product_type":"c1","uid":1370035,"ip_address":"","ucode":"0666969CC4D839","user_header":"https://static001.geekbang.org/account/avatar/00/14/e7/b3/687b120e.jpg","comment_is_top":false,"comment_ctime":1587430197,"is_pvip":false,"replies":[{"id":"78136","content":"你基本上可以认为它们两个是一样的。前提是消息中时间戳是单调增加的，这也符合我们常见的使用场景。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587518938,"ip_address":"","comment_id":208731,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10177364789","product_id":100050101,"comment_content":"请问，append 方法的 largestOffset 和 shallowOffsetOfMaxTimestamp 参数，有可能相等吗？什么时候相等，什么时候不相等？","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492597,"discussion_content":"你基本上可以认为它们两个是一样的。前提是消息中时间戳是单调增加的，这也符合我们常见的使用场景。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587518938,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207333,"user_name":"drapeau🏖","can_delete":false,"product_type":"c1","uid":1405331,"ip_address":"","ucode":"83BB32485995FC","user_header":"https://static001.geekbang.org/account/avatar/00/15/71/93/448cec84.jpg","comment_is_top":false,"comment_ctime":1587049290,"is_pvip":false,"replies":[{"id":"77462","content":"很多操作系统的书对页缓存都有介绍。简单来说，page cache是最重要的一类操作系统（特别是Linux系统）disk cache。相关介绍可以看看《Understanding the Linux Kernel》","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587088866,"ip_address":"","comment_id":207333,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10176983882","product_id":100050101,"comment_content":"希望老师以后能讲讲页缓存这个东西","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492088,"discussion_content":"很多操作系统的书对页缓存都有介绍。简单来说，page cache是最重要的一类操作系统（特别是Linux系统）disk cache。相关介绍可以看看《Understanding the Linux Kernel》","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587088866,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207218,"user_name":"奔跑小电驴","can_delete":false,"product_type":"c1","uid":1670951,"ip_address":"","ucode":"36950F0FB59F58","user_header":"https://static001.geekbang.org/account/avatar/00/19/7f/27/9c5ac353.jpg","comment_is_top":false,"comment_ctime":1587026006,"is_pvip":false,"replies":[{"id":"77467","content":"Kafka中的变量命名总体来说还是友好的，确实也存在一些变量不知其意。还是要结合它在源码中的作用来看，特别是使用它的方法。也许在这些方法中有对应的info或warn log说明了它的用法。<br>我举个例子，比如LogCleaner组件中有个变量叫uncleanablePartitions，注释里写它保存不可被clean的分区，但光看注释我们也不知道不可被clean要满足什么条件。这个时候你就看哪个方法往这个Map中添加分区，一查源码发现是markPartitionUncleanable方法，可这个方法没有一行注释！<br>没问题，我们继续看，tryCleanFilthiestLog方法会调用markPartitionUncleanable。tryCleanFilthiestLog里面写了到底哪些分区不能被clean，因为有一行warn日志","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587089510,"ip_address":"","comment_id":207218,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10176960598","product_id":100050101,"comment_content":"kafka用的也不算少，但冷不丁看源码还是有难度，具体难在有些变量不是特别清楚是做什么的，导致理解整个流程就有点困难，这块老师有什么好的方法嘛~~","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492046,"discussion_content":"Kafka中的变量命名总体来说还是友好的，确实也存在一些变量不知其意。还是要结合它在源码中的作用来看，特别是使用它的方法。也许在这些方法中有对应的info或warn log说明了它的用法。\n我举个例子，比如LogCleaner组件中有个变量叫uncleanablePartitions，注释里写它保存不可被clean的分区，但光看注释我们也不知道不可被clean要满足什么条件。这个时候你就看哪个方法往这个Map中添加分区，一查源码发现是markPartitionUncleanable方法，可这个方法没有一行注释！\n没问题，我们继续看，tryCleanFilthiestLog方法会调用markPartitionUncleanable。tryCleanFilthiestLog里面写了到底哪些分区不能被clean，因为有一行warn日志","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587089510,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":329714,"user_name":"行走的飞翔者","can_delete":false,"product_type":"c1","uid":1072105,"ip_address":"","ucode":"A017E4C901A208","user_header":"https://static001.geekbang.org/account/avatar/00/10/5b/e9/829bb26f.jpg","comment_is_top":false,"comment_ctime":1641490065,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5936457361","product_id":100050101,"comment_content":"“专栏后面我将专门讨论 Kafka 是如何保存具体消息的，也就是 FileRecords 及其家族的实现方式。同时，我还会给你介绍一下社区在持久化消息这块是怎么演进的，你一定不要错过那部分的内容。”请问这块查了全部专栏也没发现讲？","like_count":1},{"had_liked":false,"id":260020,"user_name":"珍妮•玛仕多","can_delete":false,"product_type":"c1","uid":1806720,"ip_address":"","ucode":"6E1E5564F979B3","user_header":"https://static001.geekbang.org/account/avatar/00/1b/91/80/bc38f890.jpg","comment_is_top":false,"comment_ctime":1604909270,"is_pvip":true,"replies":[{"id":"94911","content":"看着眼熟。是这个帖子吗？https:&#47;&#47;www.zhihu.com&#47;question&#47;429659943&#47;answer&#47;1569256477","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1605423599,"ip_address":"","comment_id":260020,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5899876566","product_id":100050101,"comment_content":"胡老师，为什么代码调用 ensureOffsetInRange 方法确保输入参数最大位移值是合法的。那怎么判断是不是合法呢？标准就是看它与日志段起始位移的差值是否在整数范围内，即 largestOffset - baseOffset 的值是不是介于 [0，Int.MAXVALUE] 之间。而不是long的最大值，offset不是long类型的吗？<br>","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":509090,"discussion_content":"看着眼熟。是这个帖子吗？https://www.zhihu.com/question/429659943/answer/1569256477","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605423599,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1806720,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/91/80/bc38f890.jpg","nickname":"珍妮•玛仕多","note":"","ucode":"6E1E5564F979B3","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":325752,"discussion_content":"对哒，都是我","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605423660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":251412,"user_name":"三颗豆子","can_delete":false,"product_type":"c1","uid":2008638,"ip_address":"","ucode":"632CE3E7563666","user_header":"https://static001.geekbang.org/account/avatar/00/1e/a6/3e/3d18f35a.jpg","comment_is_top":false,"comment_ctime":1601536516,"is_pvip":false,"replies":[{"id":"92153","content":"非常佩服您的钻研精神！具体的上下文我简单看了下，发现了这么一句：<br>So in 5), the offset will be 2 x &#39;a&#39;. That is, we never recycle offsets. They keep increasing.<br><br>因此，400万是这么计算得来的：9223372036854775807 &#47; 1024^4 &#47; 2","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1602207736,"ip_address":"","comment_id":251412,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5896503812","product_id":100050101,"comment_content":"胡哥，请问下offset是单调递增的，我当时有个疑问是如果溢出怎么办，后来看到知乎上有个回答是这封邮件流（http:&#47;&#47;mail-archives.apache.org&#47;mod_mbox&#47;kafka-users&#47;201111.mbox&#47;%3CCAFbh0Q3wXeivW9G+xre5eN8-SLVprdNLWyKmd+CcW6n=A=ZSHQ@mail.gmail.com%3E）提到如果每天写1TB数据，大约可以用400万天，但是邮件没有上下文，所以我自己按照：<br>单个分区，每条消息X字节，每天写入1024*1024*1024*1024字节，写400万天，总共写入Long.max=9223372036854775807条消息，算出来X=(1024*1024*1024*1024*4000000)÷(9223372036854775807)=0.47，但是一条Kafka消息最少都要14字节的结构（V0格式），所以它这个400万天是怎么算出来的，是不是太少了呢？<br>谢谢。","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506482,"discussion_content":"非常佩服您的钻研精神！具体的上下文我简单看了下，发现了这么一句：\nSo in 5), the offset will be 2 x &amp;#39;a&amp;#39;. That is, we never recycle offsets. They keep increasing.\n\n因此，400万是这么计算得来的：9223372036854775807 / 1024^4 / 2","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602207736,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":240957,"user_name":"张子涵","can_delete":false,"product_type":"c1","uid":1743397,"ip_address":"","ucode":"57C509EA32B916","user_header":"https://static001.geekbang.org/account/avatar/00/1a/9a/25/3e5e942b.jpg","comment_is_top":false,"comment_ctime":1597132672,"is_pvip":false,"replies":[{"id":"89110","content":"������  Kafka的注释写得很不错，千万别忘记看","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1597213036,"ip_address":"","comment_id":240957,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5892099968","product_id":100050101,"comment_content":"* If the given offset is larger than the largest message in this segment, do nothing.      虽然刚开始阅读源码不是特别通畅，部分代码还不明白为什么，但是这个truncate to 方法的注释我看明白了，如果指定的位移值特别特别大，超过了日志段本身保存的最大位移值，则什么也不会做， do nothing : )","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":503605,"discussion_content":"������  Kafka的注释写得很不错，千万别忘记看","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597213036,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211070,"user_name":"🙊顾小顾","can_delete":false,"product_type":"c1","uid":1965728,"ip_address":"","ucode":"FC7974A351586C","user_header":"https://static001.geekbang.org/account/avatar/00/1d/fe/a0/b1212e2b.jpg","comment_is_top":false,"comment_ctime":1587890995,"is_pvip":false,"replies":[{"id":"78565","content":"简单来说，日志段对象可能被多个线程操作。比如Kafka的多个I&#47;O线程就可能同时操作同一个日志段对象。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587951511,"ip_address":"","comment_id":211070,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882858291","product_id":100050101,"comment_content":"maxTimestampSoFar等值被@volatile修饰，为何要这么设置<br>每一个topic分区对应一个Segment，这个会被多线程的访问吗？比如spark streaming的kafkaUtil.directStream,其会周期性的获取Kafka中每个topic的每个partition中的最新offsets,写入的时候应该也是，理解起来应该是单线程的操作","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493187,"discussion_content":"简单来说，日志段对象可能被多个线程操作。比如Kafka的多个I/O线程就可能同时操作同一个日志段对象。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587951511,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210376,"user_name":"Roger宇","can_delete":false,"product_type":"c1","uid":1703222,"ip_address":"","ucode":"CBA23C01409349","user_header":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","comment_is_top":false,"comment_ctime":1587732092,"is_pvip":false,"replies":[{"id":"78473","content":"如果为空，则没有可以append的了，直接结束。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587868273,"ip_address":"","comment_id":210376,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5882699388","product_id":100050101,"comment_content":"“首先调用 records.sizeInBytes 方法判断该日志段是否为空，如果是空的话， Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。”<br>-----------------------<br>想请问下老师，这段的如果为空的处理逻辑是不是不在append中啦？if record大小不为空就结束append方法啦。","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493008,"discussion_content":"如果为空，则没有可以append的了，直接结束。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587868273,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1061277,"avatar":"https://static001.geekbang.org/account/avatar/00/10/31/9d/daad92d2.jpg","nickname":"Stony.修行僧","note":"","ucode":"0F2368F7D93E4A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":345452,"discussion_content":"是不是笔误啊“首先调用 records.sizeInBytes 方法判断该日志段是否为空，如果是空的话， Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。”\n应该改为“如果不是空，Kafka需要记录。。。。。“","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611721818,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209531,"user_name":"灰机","can_delete":false,"product_type":"c1","uid":1179580,"ip_address":"","ucode":"46F6CFC2DFAA2B","user_header":"https://static001.geekbang.org/account/avatar/00/11/ff/bc/368b9f80.jpg","comment_is_top":false,"comment_ctime":1587564344,"is_pvip":false,"replies":[{"id":"78225","content":"1. magic是消息格式版本，总有3个版本。V2是最新版本<br>2. leader epoch是controller分配给分区leader副本的版本号。每个消息批次都要有对应的leader epoch。Kafka会记录每个分区leader不同epoch对应的首条消息的位移。比如leader epoch=0时producer写入了100条消息，那么cache会记录&lt;0, 0&gt;，之后leader变更，epoch增加到1，之后producer又写入了200条消息，那么cache会记录&lt;1, 100&gt;。epoch主要用于做日志截断时保证一致性用的，单纯依赖HW值可能出现各种不一致的情况。这是社区对于HW值的一个修正机制","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587604399,"ip_address":"","comment_id":209531,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882531640","product_id":100050101,"comment_content":"老师，有两个问题想请教一下，<br>1.kafka 的magic 我看在源码中有体现，这个具体指代kafka版本还是什么？<br>2.LeaderEpoch这个epoch 是什么含义？ 在recover方法中会cache.assign batch.parittionLeaderEpoch 这里不是很懂。希望老师赐教。","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492824,"discussion_content":"1. magic是消息格式版本，总有3个版本。V2是最新版本\n2. leader epoch是controller分配给分区leader副本的版本号。每个消息批次都要有对应的leader epoch。Kafka会记录每个分区leader不同epoch对应的首条消息的位移。比如leader epoch=0时producer写入了100条消息，那么cache会记录&amp;lt;0, 0&amp;gt;，之后leader变更，epoch增加到1，之后producer又写入了200条消息，那么cache会记录&amp;lt;1, 100&amp;gt;。epoch主要用于做日志截断时保证一致性用的，单纯依赖HW值可能出现各种不一致的情况。这是社区对于HW值的一个修正机制","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587604399,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208176,"user_name":"张珮磊想静静","can_delete":false,"product_type":"c1","uid":1084732,"ip_address":"","ucode":"2E582ED7BB178E","user_header":"https://static001.geekbang.org/account/avatar/00/10/8d/3c/9025c2ca.jpg","comment_is_top":false,"comment_ctime":1587291570,"is_pvip":true,"replies":[{"id":"77762","content":"为某个具有大量分区的主题设置了segment.ms参数。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587307720,"ip_address":"","comment_id":208176,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5882258866","product_id":100050101,"comment_content":"老师开始说的您公司碰到过大面积日志段同时间切分的线上问题，还是有点不太明白，什么情况会出现这样的现象","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492369,"discussion_content":"为某个具有大量分区的主题设置了segment.ms参数。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587307720,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1188023,"avatar":"https://static001.geekbang.org/account/avatar/00/12/20/b7/bdb3bcf0.jpg","nickname":"Eternal","note":"","ucode":"EA6FE7CC98F740","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":240525,"discussion_content":"@张珮磊想静静\n在broker端设置，segment.ms：即使log的分块文件没有达到需要删除、压缩的大小，一旦log 的时间达到这个上限，就会强制新建一个log分块文件；默认设置的是7天","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587371581,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206986,"user_name":"曾轼麟","can_delete":false,"product_type":"c1","uid":1451391,"ip_address":"","ucode":"D418371AC11270","user_header":"https://static001.geekbang.org/account/avatar/00/16/25/7f/473d5a77.jpg","comment_is_top":false,"comment_ctime":1586969029,"is_pvip":false,"replies":[{"id":"77303","content":"FileRecords和MemoryRecords是非常重要的两个类，我建议好好研读下：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587000715,"ip_address":"","comment_id":206986,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5881936325","product_id":100050101,"comment_content":"今天阅读源码发现收益良多，先回答老师的问题，如果超过了日志段本身保存的最大位移值或者targetSize小于0（怀疑是溢出），会抛出一个KafkaException来中断程序执行。<br><br>其次我发现read方法提到的：maxSize，maxPosition，minOneMessage几个参数，刚好可用对应上了kafka-clients消费者的配置项：max.poll.records，max.partition.fetch.bytes。其中kafka官方的api中也提到KafkaConsumer.poll()方法在主动拉取的时候，如果单条消息超过最大值能保证至少返回一条消息，也就是老师提到的消费者饥饿的情况。<br><br>同时我发现kafka里面存在着Records接口其对应多种记录类型比如FileRecords，MemoryRecords等等顺着这个应该可以发现消息的各种形态。而且在FileRecords中使用了JDK提供的FileChannel，FileChannel是可以实现0拷贝技术的","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491972,"discussion_content":"FileRecords和MemoryRecords是非常重要的两个类，我建议好好研读下：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587000715,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1399488,"avatar":"https://static001.geekbang.org/account/avatar/00/15/5a/c0/e20eb855.jpg","nickname":"Tomcat","note":"","ucode":"58A9D44991EDB7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":249453,"discussion_content":"您好，我是一个kafka初学者，如果把minOneMessage设置为flase了，并且单条消息体也大于可读取的最大值之后，从文章中的源码看，如果日志段长度不够的话，它会读到日志段结尾，那可以保证它是一条完整的消息吗？还是会抛出异常","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587923091,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206860,"user_name":"jeffery","can_delete":false,"product_type":"c1","uid":1219972,"ip_address":"","ucode":"35E2DAA386FB86","user_header":"https://static001.geekbang.org/account/avatar/00/12/9d/84/171b2221.jpg","comment_is_top":false,"comment_ctime":1586945876,"is_pvip":false,"replies":[{"id":"77318","content":"Java语言基础是必需的，Scala无需太多基础。一个比较好的经验是多读源码注释+测试用例。<br><br>测试用例代码简单，而且对于源码解释是直接的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587003136,"ip_address":"","comment_id":206860,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5881913172","product_id":100050101,"comment_content":"没有语言基础怎么弯道超车撸源码，能分享下经验吗？再此谢过了","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491926,"discussion_content":"Java语言基础是必需的，Scala无需太多基础。一个比较好的经验是多读源码注释+测试用例。\n\n测试用例代码简单，而且对于源码解释是直接的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1188023,"avatar":"https://static001.geekbang.org/account/avatar/00/12/20/b7/bdb3bcf0.jpg","nickname":"Eternal","note":"","ucode":"EA6FE7CC98F740","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":240523,"discussion_content":"以前没有关注到测试用例这个点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587371385,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206809,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1586939188,"is_pvip":true,"replies":[{"id":"77320","content":"也可以的。很多Java工程师都是直接撸Tomcat源码，即使没有用过。只要是优秀的框架代码都有学习的价值：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587003213,"ip_address":"","comment_id":206809,"utype":1}],"discussion_count":1,"race_medal":1,"score":"5881906484","product_id":100050101,"comment_content":"目前没有用过 kafka ,一上来就源码这样真的好吗？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491910,"discussion_content":"也可以的。很多Java工程师都是直接撸Tomcat源码，即使没有用过。只要是优秀的框架代码都有学习的价值：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003213,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206292,"user_name":"xinxin","can_delete":false,"product_type":"c1","uid":1285617,"ip_address":"","ucode":"52C5D688B10968","user_header":"https://static001.geekbang.org/account/avatar/00/13/9d/f1/05da9c09.jpg","comment_is_top":false,"comment_ctime":1586835648,"is_pvip":false,"replies":[{"id":"77345","content":"谢谢鼓励，一起加油","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587004937,"ip_address":"","comment_id":206292,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5881802944","product_id":100050101,"comment_content":"学习了，很棒的源码课程","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491743,"discussion_content":"谢谢鼓励，一起加油","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587004937,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206186,"user_name":"每天晒白牙","can_delete":false,"product_type":"c1","uid":1004698,"ip_address":"","ucode":"A1B102CD933DEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","comment_is_top":false,"comment_ctime":1586820041,"is_pvip":false,"replies":[{"id":"77002","content":"密度大是好事，说明讲的不算太宽泛：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1586828882,"ip_address":"","comment_id":206186,"utype":1}],"discussion_count":1,"race_medal":1,"score":"5881787337","product_id":100050101,"comment_content":"文章知识密度大","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491704,"discussion_content":"密度大是好事，说明讲的不算太宽泛：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586828882,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":355452,"user_name":"freesocean","can_delete":false,"product_type":"c1","uid":1529210,"ip_address":"中国香港","ucode":"CAD4C80CF569D3","user_header":"https://static001.geekbang.org/account/avatar/00/17/55/7a/d44df1d6.jpg","comment_is_top":false,"comment_ctime":1661398310,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1661398310","product_id":100050101,"comment_content":"课后思考，看了truncateTo源码，该方法注释其实说的很清楚<br>1.该方法不会检查传入参数的边界问题<br>2.只有当targetSize 小于底层FileChannel 大小才会截断<br>具体代码也很好理解：如果不在范围内，直接抛异常<br> if (targetSize &gt; originalSize || targetSize &lt; 0)<br>            throw new KafkaException(&quot;Attempt to truncate log segment &quot; + file + &quot; to &quot; + targetSize + &quot; bytes failed, &quot; +<br>                    &quot; size of this log segment is &quot; + originalSize + &quot; bytes.&quot;);","like_count":0},{"had_liked":false,"id":346418,"user_name":"清风","can_delete":false,"product_type":"c1","uid":1542078,"ip_address":"","ucode":"AB2D169746BC23","user_header":"https://static001.geekbang.org/account/avatar/00/17/87/be/7466bf26.jpg","comment_is_top":false,"comment_ctime":1653124261,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1653124261","product_id":100050101,"comment_content":"所以，网上说从第二个是segment开始，每个segment文件的名字是上一个segment<br>的最后一条消息的位移值，都是胡扯，这样说就是没有理解baseoffset的意义，也没有看官方文档，也没有真正的实操","like_count":0},{"had_liked":false,"id":318346,"user_name":"卡巴斯妞","can_delete":false,"product_type":"c1","uid":2721187,"ip_address":"","ucode":"5FEECDD3837B41","user_header":"https://static001.geekbang.org/account/avatar/00/29/85/a3/46648484.jpg","comment_is_top":false,"comment_ctime":1635251449,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1635251449","product_id":100050101,"comment_content":"我想问个问题，在消费过程事务比较长，Kafka出现一个问题，反复消费陷入循环，有建议的方案么？<br>","like_count":0},{"had_liked":false,"id":315329,"user_name":"huolang","can_delete":false,"product_type":"c1","uid":1346708,"ip_address":"","ucode":"FDC4A6B6151C5E","user_header":"https://static001.geekbang.org/account/avatar/00/14/8c/94/5282994c.jpg","comment_is_top":false,"comment_ctime":1633835256,"is_pvip":false,"replies":[{"id":"114433","content":"绝大部分时间两者是相同的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1634004303,"ip_address":"","comment_id":315329,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1633835256","product_id":100050101,"comment_content":"老师，append方法中的largestOffset和shallowOffsetOfMaxTimestamp有什么不同？最大位移的消息对应的时间戳不是最大的吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527936,"discussion_content":"绝大部分时间两者是相同的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634004303,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314108,"user_name":"傻傻的帅","can_delete":false,"product_type":"c1","uid":1668617,"ip_address":"","ucode":"14A795523A682E","user_header":"https://static001.geekbang.org/account/avatar/00/19/76/09/62a10668.jpg","comment_is_top":false,"comment_ctime":1632840538,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1632840538","product_id":100050101,"comment_content":"If the given offset is larger than the largest message in this segment, do nothing.","like_count":0},{"had_liked":false,"id":294387,"user_name":"黄振游","can_delete":false,"product_type":"c1","uid":1858989,"ip_address":"","ucode":"3927C81CAEDBE2","user_header":"","comment_is_top":false,"comment_ctime":1621924255,"is_pvip":false,"replies":[{"id":"107172","content":"这个是写新的索引项","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1622353688,"ip_address":"","comment_id":294387,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1621924255","product_id":100050101,"comment_content":"bytesSinceLastIndexEntry &gt; indexIntervalBytes 时是不是需要创建新的日志文件了，但在这之前记录已经log.append(records)。没太明白，麻烦老师解答下。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520633,"discussion_content":"这个是写新的索引项","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622353688,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292477,"user_name":"QAQ","can_delete":false,"product_type":"c1","uid":2307937,"ip_address":"","ucode":"6E47215CBB81F5","user_header":"https://static001.geekbang.org/account/avatar/00/23/37/61/51e10a30.jpg","comment_is_top":false,"comment_ctime":1620834525,"is_pvip":false,"replies":[{"id":"106082","content":"append过程中就会判断是否需要roll out","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1621146055,"ip_address":"","comment_id":292477,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620834525","product_id":100050101,"comment_content":"老师，你好，这里一直进行append插入日志操作，什么情况下应该生成一个新的段呢？总不至于一个文件无限大，那也不方便文件数据的过期删除","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519845,"discussion_content":"append过程中就会判断是否需要roll out","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621146055,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288585,"user_name":"邀月对影","can_delete":false,"product_type":"c1","uid":1204313,"ip_address":"","ucode":"A3482EDF73EB0D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/QicTra5HbNEwGxeG49XibcUibB82I2hpBnp8tviaiaicvFAojuMtRiaLCyl6syzzoS546H2hJibNAQ3h9XM097iapiaamcEQ/132","comment_is_top":false,"comment_ctime":1618551851,"is_pvip":false,"replies":[{"id":"105290","content":"个人感觉影响不大","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1619600122,"ip_address":"","comment_id":288585,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1618551851","product_id":100050101,"comment_content":"老师，想问下append方法的第24行代码offsetIndex.append(largestOffset, physicalPosition) 这个地方largestOffset对应的物理地址physicalPosition为啥是追加消息之前的地址，而不是消息追加之后的地址呢？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518674,"discussion_content":"个人感觉影响不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619600122,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283879,"user_name":"肖恒","can_delete":false,"product_type":"c1","uid":1861000,"ip_address":"","ucode":"50B1118691B222","user_header":"","comment_is_top":false,"comment_ctime":1615970964,"is_pvip":false,"replies":[{"id":"103539","content":"通常都是<br>","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1616722048,"ip_address":"","comment_id":283879,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1615970964","product_id":100050101,"comment_content":"请教下，最大时间对应的 offset 是否是这一批次消息中最大的 offset 了？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517171,"discussion_content":"通常都是\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616722048,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280657,"user_name":"Geek_198fea","can_delete":false,"product_type":"c1","uid":2446899,"ip_address":"","ucode":"094A1837F4F4FC","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/tANdoqYOVToHzTGNooHaWKotbZLKwIpcvg47edEbH6cRHQF5GZtm8Ay66MVA8YgxU5Z5XcFkibt1h2iadP288m9g/132","comment_is_top":false,"comment_ctime":1614306878,"is_pvip":false,"replies":[{"id":"102236","content":"LogSegment的append read就是负责向.log读写文件的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1614822336,"ip_address":"","comment_id":280657,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1614306878","product_id":100050101,"comment_content":"大佬你好，我想问一下，就LogSegment.scala中的append和read方法，在Log.scala也有同样的方法，而在物理磁盘中，partition就是对于不同目录，LogSegment(是把.index.log等文件合起来叫这个名字嘛)包含了.index,.log等文件，那么.log，.index文件中的数据读写是由那块的源码负责的呢？LogSegment的append,read方法实现的又是那块的读写功能呢。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516170,"discussion_content":"LogSegment的append read就是负责向.log读写文件的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614822336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":271043,"user_name":"是男人就开巴巴托斯","can_delete":false,"product_type":"c1","uid":1352006,"ip_address":"","ucode":"C65873BBB28D05","user_header":"https://static001.geekbang.org/account/avatar/00/14/a1/46/3136ac25.jpg","comment_is_top":false,"comment_ctime":1609380613,"is_pvip":false,"replies":[{"id":"98422","content":"read方法会读取底层消息，它是调用你说的那个方法来实现的，算是个上层方法把：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1609721553,"ip_address":"","comment_id":271043,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1609380613","product_id":100050101,"comment_content":"read 方法似乎并不真正读取消息， 只把要读取所有分区的文件名， 起始位移， 读取长度封装成response，然后交给 SocketServer的processor去完成实际的FileChannel-&gt;SocketChannel的zerocopy.<br>老师帮看看是不是这样","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512826,"discussion_content":"read方法会读取底层消息，它是调用你说的那个方法来实现的，算是个上层方法把：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609721553,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268528,"user_name":"Shane_ミ木","can_delete":false,"product_type":"c1","uid":1537249,"ip_address":"","ucode":"01396B247CC8D8","user_header":"https://static001.geekbang.org/account/avatar/00/17/74/e1/623ff8d8.jpg","comment_is_top":false,"comment_ctime":1608249274,"is_pvip":false,"replies":[{"id":"97816","content":"总字节数信息是单独保存的一个数据，这里比较的目的是将两者的数值对齐。总字节数不一定是真实正确的，一切以真实累加起来的数据为准。<br><br>截断就是直接调整物理文件位置。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1608775400,"ip_address":"","comment_id":268528,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608249274","product_id":100050101,"comment_content":"recover最后日志截断没太看懂<br><br>&quot;前者比后者大&quot;意思是日志段总大小比累加值大？累加不是用日志信息刚刚遍历计算的吗？为啥还会不一样？<br><br>如果累加比真实小，那为啥还截断？具体怎么截断？索引截断也不太明白","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511958,"discussion_content":"总字节数信息是单独保存的一个数据，这里比较的目的是将两者的数值对齐。总字节数不一定是真实正确的，一切以真实累加起来的数据为准。\n\n截断就是直接调整物理文件位置。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608775400,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268521,"user_name":"Shane_ミ木","can_delete":false,"product_type":"c1","uid":1537249,"ip_address":"","ucode":"01396B247CC8D8","user_header":"https://static001.geekbang.org/account/avatar/00/17/74/e1/623ff8d8.jpg","comment_is_top":false,"comment_ctime":1608247343,"is_pvip":false,"replies":[{"id":"97592","content":"1. 对的<br>2. 只是查找一个起始读取点。不一定非要是那条消息本身","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1608516810,"ip_address":"","comment_id":268521,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608247343","product_id":100050101,"comment_content":"老师 我有两个问题<br>1，时戳索引保存时戳和位移的关系， 那位移索引是位移和物理位置的对性关系吗？<br>2，＞4k才写位移索引，查找的时候怎么用？按位移就近？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511956,"discussion_content":"1. 对的\n2. 只是查找一个起始读取点。不一定非要是那条消息本身","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608516810,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":258103,"user_name":"flyCoder","can_delete":false,"product_type":"c1","uid":1074897,"ip_address":"","ucode":"82FB7B60775978","user_header":"https://static001.geekbang.org/account/avatar/00/10/66/d1/8664c464.jpg","comment_is_top":false,"comment_ctime":1604299317,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1604299317","product_id":100050101,"comment_content":"截取到  val slot = largestLowerBoundSlotFor(idx, offset, IndexSearchType.KEY) +1","like_count":0},{"had_liked":false,"id":257716,"user_name":"flyCoder","can_delete":false,"product_type":"c1","uid":1074897,"ip_address":"","ucode":"82FB7B60775978","user_header":"https://static001.geekbang.org/account/avatar/00/10/66/d1/8664c464.jpg","comment_is_top":false,"comment_ctime":1604075934,"is_pvip":true,"replies":[{"id":"93967","content":"private和Java中的private是类似的，后面的[log]表示它虽然是private，但是在log包中可见。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1604283472,"ip_address":"","comment_id":257716,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1604075934","product_id":100050101,"comment_content":"class LogSegment 后面跟 private[log]  是scala 的什么语法?，还有def translateOffset前面跟private[log] 的语法，没怎么看懂","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508427,"discussion_content":"private和Java中的private是类似的，后面的[log]表示它虽然是private，但是在log包中可见。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604283472,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":251409,"user_name":"三颗豆子","can_delete":false,"product_type":"c1","uid":2008638,"ip_address":"","ucode":"632CE3E7563666","user_header":"https://static001.geekbang.org/account/avatar/00/1e/a6/3e/3d18f35a.jpg","comment_is_top":false,"comment_ctime":1601532629,"is_pvip":false,"replies":[{"id":"92154","content":"具体的原因不得知，也许就是一种保守的做法：） ","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1602208087,"ip_address":"","comment_id":251409,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1601532629","product_id":100050101,"comment_content":"请问一下Kafka的文件为什么要做左侧补零的设计呢？<br>另外，为什么是20位呢，offset最大是Long.MAX_VALUE=9223372036854775807（19位）？<br>谢谢。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506481,"discussion_content":"具体的原因不得知，也许就是一种保守的做法：） ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602208087,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249364,"user_name":"后端进阶","can_delete":false,"product_type":"c1","uid":1125656,"ip_address":"","ucode":"480F48F5378307","user_header":"https://static001.geekbang.org/account/avatar/00/11/2d/18/918eaecf.jpg","comment_is_top":false,"comment_ctime":1600610805,"is_pvip":false,"replies":[{"id":"91472","content":"是的。那就要做相应的优化","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1600656452,"ip_address":"","comment_id":249364,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1600610805","product_id":100050101,"comment_content":"假设消息大小比 4kb 还大，是否可以认为每次append都会伴随索引文件的append？这时是不是应该适当调大log.index.interval.bytes来优化？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505906,"discussion_content":"是的。那就要做相应的优化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600656452,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":245679,"user_name":"李奕慧","can_delete":false,"product_type":"c1","uid":1201005,"ip_address":"","ucode":"0D8871ED38859C","user_header":"https://static001.geekbang.org/account/avatar/00/12/53/6d/c3828950.jpg","comment_is_top":false,"comment_ctime":1599025708,"is_pvip":false,"replies":[{"id":"90395","content":"指的是索引文件里面的内容","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1599095137,"ip_address":"","comment_id":245679,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1599025708","product_id":100050101,"comment_content":"索引项指的是位移索引文件吗 (0000.index)?","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504956,"discussion_content":"指的是索引文件里面的内容","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599095137,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":243964,"user_name":"鲁·本","can_delete":false,"product_type":"c1","uid":1209939,"ip_address":"","ucode":"F1DEB30C21B48E","user_header":"https://static001.geekbang.org/account/avatar/00/12/76/53/21d62a23.jpg","comment_is_top":false,"comment_ctime":1598341935,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1598341935","product_id":100050101,"comment_content":"val mapping = translateOffset(offset) 返回null <br>val bytesTruncated = if (mapping == null) 0 else log.truncateTo(mapping.position)<br>所以 就是什么都做.","like_count":0},{"had_liked":false,"id":240823,"user_name":"云超","can_delete":false,"product_type":"c1","uid":1581783,"ip_address":"","ucode":"63DFDDAB37A04F","user_header":"https://static001.geekbang.org/account/avatar/00/18/22/d7/ae0ed31f.jpg","comment_is_top":false,"comment_ctime":1597074812,"is_pvip":false,"replies":[{"id":"88984","content":"嗯嗯，可能是我没说清楚。Kafka有组参数（log.segment.ms和topic级别的segment.ms，如果是compact参数，那么就是log.cleaner.max.compaction.lag.ms和max.compaction.lag.ms）用于配置日志段多久之后被删除。计算的方式就是用日志段中消息的最大时间戳减去rollingBasedTimestamp的值，如果超过了配置的阈值就执行删除。rollingBasedTimestamp就是做这个用的。因此当Kafka在写入日志段第一条消息时会给rollingBasedTimestamp进行赋值。<br><br>不知道这么说是否清楚了些。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1597111949,"ip_address":"","comment_id":240823,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1597074812","product_id":100050101,"comment_content":"rollingBasedTimestamp 老师，你在文章里面有说这个值是用作为‘新增日志段倒计时的依据‘，我理解不了这句话的意思，老师能举个例子说一下它的实际用途吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":503569,"discussion_content":"嗯嗯，可能是我没说清楚。Kafka有组参数（log.segment.ms和topic级别的segment.ms，如果是compact参数，那么就是log.cleaner.max.compaction.lag.ms和max.compaction.lag.ms）用于配置日志段多久之后被删除。计算的方式就是用日志段中消息的最大时间戳减去rollingBasedTimestamp的值，如果超过了配置的阈值就执行删除。rollingBasedTimestamp就是做这个用的。因此当Kafka在写入日志段第一条消息时会给rollingBasedTimestamp进行赋值。\n\n不知道这么说是否清楚了些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597111949,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1581783,"avatar":"https://static001.geekbang.org/account/avatar/00/18/22/d7/ae0ed31f.jpg","nickname":"云超","note":"","ucode":"63DFDDAB37A04F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":298076,"discussion_content":"清楚了，谢谢老师。我说怎么给该参数只赋了一次值，后面也没对改值进行操作了，没想到它的具体用途，想了半天没想明白，老师这么一说就懂了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597161615,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":233279,"user_name":"Shine","can_delete":false,"product_type":"c1","uid":1184853,"ip_address":"","ucode":"BF3DB5ADF2B153","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/ydVBBkofXDqCyP7pdwkicHZ9xtyEEuZvzrrkeWcnQjZ1ibEgG60eLotQTsKJFpWibuf6e7G9r0I1xaribUAQibPMl7g/132","comment_is_top":false,"comment_ctime":1594277935,"is_pvip":false,"replies":[{"id":"86095","content":"删除了日志段文件就意味着丢失数据了。能否删除就看你的需求了。如果一定要删除的话，那么按照文件名中位移值从小到大的顺序开始删除。另外把配套的索引文件也一并删除，之后重启broker","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1594305936,"ip_address":"","comment_id":233279,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1594277935","product_id":100050101,"comment_content":"老师，有很多日志段文件，导致重启慢，要怎么做呢？可以删日志段文件吗？怎么合理删呢？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500971,"discussion_content":"删除了日志段文件就意味着丢失数据了。能否删除就看你的需求了。如果一定要删除的话，那么按照文件名中位移值从小到大的顺序开始删除。另外把配套的索引文件也一并删除，之后重启broker","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594305936,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1184853,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/ydVBBkofXDqCyP7pdwkicHZ9xtyEEuZvzrrkeWcnQjZ1ibEgG60eLotQTsKJFpWibuf6e7G9r0I1xaribUAQibPMl7g/132","nickname":"Shine","note":"","ucode":"BF3DB5ADF2B153","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":290109,"discussion_content":"谢谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594346586,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":231370,"user_name":"...","can_delete":false,"product_type":"c1","uid":1037826,"ip_address":"","ucode":"95B6293E8274C8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d6/02/dfcf14be.jpg","comment_is_top":false,"comment_ctime":1593658388,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1593658388","product_id":100050101,"comment_content":"If the given offset is larger than the largest message in this segment, do nothing.<br>如果指定的位移值特别特别大，以至于超过了日志段本身保存的最大位移值，该方法什么也不做","like_count":0},{"had_liked":false,"id":230148,"user_name":"magict4","can_delete":false,"product_type":"c1","uid":1043789,"ip_address":"","ucode":"CB6F063D881AAC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/4d/1d1a1a00.jpg","comment_is_top":false,"comment_ctime":1593279036,"is_pvip":false,"replies":[{"id":"85031","content":"1.  recordBatch 的地址信息 = recordBatch中首条消息的位移<br>2. 同理，二分查找是搜寻offset维度的，也不是record batch维度<br>3. 再次同理：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593388515,"ip_address":"","comment_id":230148,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593279036","product_id":100050101,"comment_content":"老师，你好！<br><br>我对 translateOffset 有点疑惑，不知道我的理解是否准确。<br>1. translateOffset 返回的是 recordBatch 的地址信息，而非具体某个 record 的地址信息？<br>2. 实现上，先根据二分法，粗略定位到某个地址。因为不是所有 recordBatch 都被 index 了。所以这里二分法没法精确定位到被请求 recordBatch 的地址。<br>3. 最后开始线性扫描，找到被请求的 recordBatch 的地址。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499782,"discussion_content":"1.  recordBatch 的地址信息 = recordBatch中首条消息的位移\n2. 同理，二分查找是搜寻offset维度的，也不是record batch维度\n3. 再次同理：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593388515,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":229827,"user_name":"yes","can_delete":false,"product_type":"c1","uid":1386201,"ip_address":"","ucode":"612BF6884ED6CC","user_header":"https://static001.geekbang.org/account/avatar/00/15/26/d9/f7e96590.jpg","comment_is_top":false,"comment_ctime":1593157734,"is_pvip":false,"replies":[{"id":"85037","content":"嗯，确实。看到这句了吗：firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size<br><br>上层调用者根据这个来判断是否读取完整了？","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593390015,"ip_address":"","comment_id":229827,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593157734","product_id":100050101,"comment_content":"老师我对这个fetchSize有些疑问，首先经过translateOffset的筛选保证了起始offset的那条消息一定在文件中，不然就return null了，如若设了minOneMessage，则通过adjustedMaxSize保证至少拉取第一条消息。此时又来了个fetchSize ？ 并且可能取到比adjustedMaxSize更小的size。<br>我的理解是 ：通过以上的操作保证adjustedMaxSize最小值是一条消息的size。那么怎么可能会还存在比它更小的size呢？假设取得比它小，那么获取到的内容不就不完整了？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499658,"discussion_content":"嗯，确实。看到这句了吗：firstEntryIncomplete = adjustedMaxSize &amp;lt; startOffsetAndSize.size\n\n上层调用者根据这个来判断是否读取完整了？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593390015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":229130,"user_name":"吃饭饭","can_delete":false,"product_type":"c1","uid":1231549,"ip_address":"","ucode":"95CFA07CDA2957","user_header":"https://static001.geekbang.org/account/avatar/00/12/ca/bd/a51ae4b2.jpg","comment_is_top":false,"comment_ctime":1592902518,"is_pvip":false,"replies":[{"id":"84611","content":"这行代码是向索引文件写入一个索引项。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1592966605,"ip_address":"","comment_id":229130,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1592902518","product_id":100050101,"comment_content":"有个地方没看明白，kafka.log.LogSegment#append 方法的这行代码 offsetIndex.append(largestOffset, physicalPosition) 是干了些什么事？如果字节数大于默认的 4KB 时是创建新的 LogSegment 文件吗？还是只是更新当前文件的索引值和物理索引值？Scala 看的很蒙圈了 ：）","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499345,"discussion_content":"这行代码是向索引文件写入一个索引项。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592966605,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1231549,"avatar":"https://static001.geekbang.org/account/avatar/00/12/ca/bd/a51ae4b2.jpg","nickname":"吃饭饭","note":"","ucode":"95CFA07CDA2957","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":285839,"discussion_content":"嗯，谢谢啦，我再研究一下，貌似看跑偏了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592967351,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228106,"user_name":"雕","can_delete":false,"product_type":"c1","uid":1892202,"ip_address":"","ucode":"5D14752E1450B0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLSEtGul3OLHfbkbq5qOywnsCmZ68icDaFcvghXyvmAicTUtLr9q18dmJHH3ZWq3QLGcaicHo1ARn5gA/132","comment_is_top":false,"comment_ctime":1592557476,"is_pvip":false,"replies":[{"id":"84345","content":"日志段有个log.roll.ms参数，控制日志段切分的频率","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1592794544,"ip_address":"","comment_id":228106,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1592557476","product_id":100050101,"comment_content":"老师，“Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。”中作为后面新增日志段倒计时的依据这个怎么理解，还有日志段的划分依据是什么？是存储消息的容量还是时间？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498922,"discussion_content":"日志段有个log.roll.ms参数，控制日志段切分的频率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592794544,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":223111,"user_name":"csyangchsh","can_delete":false,"product_type":"c1","uid":1002939,"ip_address":"","ucode":"8604F5C839710B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4d/bb/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1591003557,"is_pvip":false,"replies":[{"id":"82334","content":"嗯嗯，这块之前是正确的，后面改错了。我要改回来，多谢您的提醒：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1591094042,"ip_address":"","comment_id":223111,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1591003557","product_id":100050101,"comment_content":"老师，在讲解append方法第一步的描述 “在源码中，首先调用 records.sizeInBytes 方法判断该日志段是否为空，如果是空的话， Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。” 中 records.sizeInBytes 似乎是笔误，应该是 log.sizeInBytes()？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":497015,"discussion_content":"嗯嗯，这块之前是正确的，后面改错了。我要改回来，多谢您的提醒：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591094042,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219374,"user_name":"Roger宇","can_delete":false,"product_type":"c1","uid":1703222,"ip_address":"","ucode":"CBA23C01409349","user_header":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","comment_is_top":false,"comment_ctime":1590020252,"is_pvip":false,"replies":[{"id":"81033","content":"个人感觉没有什么特殊之处，就是社区自己定义的一个阈值。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1590022403,"ip_address":"","comment_id":219374,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1590020252","product_id":100050101,"comment_content":"老师，请问日志段没写入4kb增加一个人index entry，这个4kb是否有更深的考量？因为感觉这个4kb好像和操作系统内存分页的大小一样啊？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495787,"discussion_content":"个人感觉没有什么特殊之处，就是社区自己定义的一个阈值。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590022403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219103,"user_name":"Light","can_delete":false,"product_type":"c1","uid":1271854,"ip_address":"","ucode":"DDFEF340A9433B","user_header":"https://static001.geekbang.org/account/avatar/00/13/68/2e/e9d09fac.jpg","comment_is_top":false,"comment_ctime":1589945147,"is_pvip":false,"replies":[{"id":"80980","content":"嗯，具体什么才算第一步不重要。重要的是了解代码的作用就行：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1589974422,"ip_address":"","comment_id":219103,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1589945147","product_id":100050101,"comment_content":"append方法的第一步描述有点疑问：<br>“records.sizeInBytes 方法判断该日志段是否为空”,这个判断是对要写入消息的大小进行判断吧，当待写入消息内容不为空的时候才开始append；<br>val physicalPosition = log.sizeInBytes()<br>if (physicalPosition == 0)<br>    rollingBasedTimestamp = Some(largestTimestamp)<br>这段代码才是第一步做的事情？<br>","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495684,"discussion_content":"嗯，具体什么才算第一步不重要。重要的是了解代码的作用就行：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589974422,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":217344,"user_name":"小廖","can_delete":false,"product_type":"c1","uid":1103444,"ip_address":"","ucode":"BAABB5D93BA5BE","user_header":"https://static001.geekbang.org/account/avatar/00/10/d6/54/75625218.jpg","comment_is_top":false,"comment_ctime":1589467189,"is_pvip":false,"replies":[{"id":"80473","content":"你认为这就是offsetOfMaxTimestampSoFar的setter方法","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1589524436,"ip_address":"","comment_id":217344,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1589467189","product_id":100050101,"comment_content":"def offsetOfMaxTimestampSoFar_=(offset: Long): Unit = _offsetOfMaxTimestampSoFar = Some(offset)<br><br>LogSegment 的这行代码有点看不懂啊","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495116,"discussion_content":"你认为这就是offsetOfMaxTimestampSoFar的setter方法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589524436,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215020,"user_name":"霍云Obsidian","can_delete":false,"product_type":"c1","uid":1178275,"ip_address":"","ucode":"1E5818D5C01576","user_header":"https://static001.geekbang.org/account/avatar/00/11/fa/a3/1b29f975.jpg","comment_is_top":false,"comment_ctime":1588870752,"is_pvip":false,"replies":[{"id":"79689","content":"没太明白您的问题。bytesSinceLastIndexEntry判断之后会被清0，这个append方法是一次性将records的所有字节写入到日志段。这样每次写入时如果总字节数超过了4KB，那么就新增一条索引项，之后将bytesSinceLastIndexEntry清0。每次都是这样的逻辑。不知道是否解答了您的疑问","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588984335,"ip_address":"","comment_id":215020,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1588870752","product_id":100050101,"comment_content":"      &#47;&#47; append an entry to the index (if needed)<br>      if (bytesSinceLastIndexEntry &gt; indexIntervalBytes) {<br>        offsetIndex.append(largestOffset, physicalPosition)<br>        timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestampSoFar)<br>        bytesSinceLastIndexEntry = 0<br>      }<br>      bytesSinceLastIndexEntry += records.sizeInBytes<br><br>关于这段代码我有个疑问，records.sizeInBytes当大于4K的时候（比如8K），那么从代码上看下一次判断的时候只是判断是否&gt;4K，那么这种情况是只用一个index表示8K吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494279,"discussion_content":"没太明白您的问题。bytesSinceLastIndexEntry判断之后会被清0，这个append方法是一次性将records的所有字节写入到日志段。这样每次写入时如果总字节数超过了4KB，那么就新增一条索引项，之后将bytesSinceLastIndexEntry清0。每次都是这样的逻辑。不知道是否解答了您的疑问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588984335,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2107375,"avatar":"https://static001.geekbang.org/account/avatar/00/20/27/ef/a7f94eda.jpg","nickname":"Jerry You","note":"","ucode":"679A44B505482F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":328921,"discussion_content":"这里可能就涉及到了kafka消息体结构的概念了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606273870,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2107375,"avatar":"https://static001.geekbang.org/account/avatar/00/20/27/ef/a7f94eda.jpg","nickname":"Jerry You","note":"","ucode":"679A44B505482F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":328920,"discussion_content":"看到这有个疑问， 索引项虽然记录了消息的位移，但是每条消息的长度信息在哪记录的？ 在log文件的每条消息体里面么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606273817,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214817,"user_name":"mengjun","can_delete":false,"product_type":"c1","uid":1108936,"ip_address":"","ucode":"3CA20F7C8FB28F","user_header":"https://static001.geekbang.org/account/avatar/00/10/eb/c8/278ee25c.jpg","comment_is_top":false,"comment_ctime":1588823866,"is_pvip":false,"replies":[{"id":"79560","content":"你指的写入中间出现问题是指什么问题呢？如果是因为网络抖动导致某些字节丢失或写入错误字节，会出现消息集合CRC校验值发生变更，这个检查不是在log这一层级执行的，而是在底层的消息集合或消息批次这个level执行的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588845591,"ip_address":"","comment_id":214817,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588823866","product_id":100050101,"comment_content":"老师您好，<br><br>对于 recover 方法，有一段代码有疑问。<br><br>val truncated = log.sizeInBytes - validBytes<br>    if (truncated &gt; 0)<br>      debug(s&quot;Truncated $truncated invalid bytes at the end of segment ${log.file.getAbsoluteFile} during recovery&quot;)<br><br>上面这几行代码说的是: 如果log.sizeInBytes 和 validBytes 不一样的话，那么end of segment是有invalid bytes. <br><br>那我的问题是，有没有可能在File (Log segment) 中间写入的时候出现问题？如果有可能的话，那么recover 好像并没有处理这种情况？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494210,"discussion_content":"你指的写入中间出现问题是指什么问题呢？如果是因为网络抖动导致某些字节丢失或写入错误字节，会出现消息集合CRC校验值发生变更，这个检查不是在log这一层级执行的，而是在底层的消息集合或消息批次这个level执行的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588845591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":212129,"user_name":"北冥有鱼","can_delete":false,"product_type":"c1","uid":1592243,"ip_address":"","ucode":"1690734A1061F4","user_header":"https://static001.geekbang.org/account/avatar/00/18/4b/b3/51bb33f2.jpg","comment_is_top":false,"comment_ctime":1588064436,"is_pvip":false,"replies":[{"id":"78879","content":"我们专栏阅读Trunk分支代码进行阅读，至少是2.4，2.5版本的代码了。FileMessageSet是很老的概念了","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588083527,"ip_address":"","comment_id":212129,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588064436","product_id":100050101,"comment_content":"老师，kafka是哪个版本的？我这边的数据是放在FileMessageSet中的","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493435,"discussion_content":"我们专栏阅读Trunk分支代码进行阅读，至少是2.4，2.5版本的代码了。FileMessageSet是很老的概念了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588083527,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211252,"user_name":"Tomcat","can_delete":false,"product_type":"c1","uid":1399488,"ip_address":"","ucode":"58A9D44991EDB7","user_header":"https://static001.geekbang.org/account/avatar/00/15/5a/c0/e20eb855.jpg","comment_is_top":false,"comment_ctime":1587923370,"is_pvip":false,"replies":[{"id":"78563","content":"1. 所有offset字眼的变量都是位移值，不是物理位置。当然这里的startOffsetAndSize是一个pojo类，同时包含offset、物理文件位置和大小<br>2.1 比如消息体超过了max.partition.fetch.bytes值。或者这么说，Broker中保存的消息是1MB，有个consumer，每次只能读取不超过500KB的数据<br>2.2 不存在消息不完整的情况，至少对于consumer而言也是不可见的。因为consumer只能读到HW以下的数据，而HW以下的数据都是已提交的，不可能是不完整的<br>3. 后面那一大堆话我没太明白什么意思。不过当minOneMessage=false，代码也不会从另一个日志段读取消息","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587951325,"ip_address":"","comment_id":211252,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1587923370","product_id":100050101,"comment_content":"胡老师看完read方法之后我有几个疑问您可以给我解释下吗？<br>1、val startOffsetAndSize = translateOffset(startOffset)<br>\tstartOffsetAndSize这里Offset代表的是物理文件的位置，Size指的是物理文件的大小，我说的对吗？<br>2、即使出现消息体字节数超过了 maxSize 的情形，read 方法依然能返回至少一条消息。<br>   （1）、消息体字节数超过了 maxSize（能读取的最大字节数）的情形指的是哪种？这两个概念分别指什么？<br>   （2）、如果我的第一个大问题是对的，那么从源码可以看出如果minOneMessage设为true的时候<br>   会读到一个日志段的最大物理位置，我这样理解正确吗？（但是我觉得会存在一条消息不完整的情况，它是如何规避这个问题的？我的答案是kafka在写日志段的时候，即使超过了日志段的大小，<br>   它也不会立即分割一个新的日志段，而是把一条完整的消息写到当前的这个日志段之后再分割一个新的日志段。这样解释正确吗？）。当minOneMessage为false的时候，消息体字节数超过了maxSize它会从另一个日志段中去读取消息。这样理解对吗？<br>3、但是看完这行val fetchSize: Int = min((maxPosition - startPosition).toInt, adjustedMaxSize) 这行代码表示的是待读取的总字节数和日志段可读的字节数中取小的值。<br><br>我觉得我对我第二个问题的解答是有很大问题的，胡老师能帮我解答一下吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493230,"discussion_content":"1. 所有offset字眼的变量都是位移值，不是物理位置。当然这里的startOffsetAndSize是一个pojo类，同时包含offset、物理文件位置和大小\n2.1 比如消息体超过了max.partition.fetch.bytes值。或者这么说，Broker中保存的消息是1MB，有个consumer，每次只能读取不超过500KB的数据\n2.2 不存在消息不完整的情况，至少对于consumer而言也是不可见的。因为consumer只能读到HW以下的数据，而HW以下的数据都是已提交的，不可能是不完整的\n3. 后面那一大堆话我没太明白什么意思。不过当minOneMessage=false，代码也不会从另一个日志段读取消息","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587951325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1399488,"avatar":"https://static001.geekbang.org/account/avatar/00/15/5a/c0/e20eb855.jpg","nickname":"Tomcat","note":"","ucode":"58A9D44991EDB7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":249770,"discussion_content":"胡老师，2.2的言外之意是不是把消息从Broker拉到之后并不会马上去解析，而是Broker会把消息提交到consumer，然后consumer再去解析成咱们可以认识的消息","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587961485,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211049,"user_name":"🙊顾小顾","can_delete":false,"product_type":"c1","uid":1965728,"ip_address":"","ucode":"FC7974A351586C","user_header":"https://static001.geekbang.org/account/avatar/00/1d/fe/a0/b1212e2b.jpg","comment_is_top":false,"comment_ctime":1587888674,"is_pvip":false,"replies":[{"id":"78572","content":"不需要循环判断啊。一旦超过了4KB，直接写索引，然后清空计数器。仅此而已~","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587952381,"ip_address":"","comment_id":211049,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587888674","product_id":100050101,"comment_content":"当已写入字节数超过了 4KB 之后，append 方法会调用索引对象的 append 方法新增索引项，同时清空已写入字节数，以备下次重新累积计算. 字节数不会一次性超过4KB吗，我看没有一个循环的判断","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493180,"discussion_content":"不需要循环判断啊。一旦超过了4KB，直接写索引，然后清空计数器。仅此而已~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587952381,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209976,"user_name":"尘枫","can_delete":false,"product_type":"c1","uid":1093372,"ip_address":"","ucode":"79698689EDD97A","user_header":"https://static001.geekbang.org/account/avatar/00/10/ae/fc/d647a3e4.jpg","comment_is_top":false,"comment_ctime":1587644836,"is_pvip":false,"replies":[{"id":"78368","content":"这个类里面保存了Broker端的参数定义和默认值等信息，同时定义了很多实用方法。您具体的问题是什么呢？","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587712572,"ip_address":"","comment_id":209976,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587644836","product_id":100050101,"comment_content":"老师能讲讲KafkaConfigs吗， 在看AbstractConfig时，看不明白","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492902,"discussion_content":"这个类里面保存了Broker端的参数定义和默认值等信息，同时定义了很多实用方法。您具体的问题是什么呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587712572,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209830,"user_name":"灰机","can_delete":false,"product_type":"c1","uid":1179580,"ip_address":"","ucode":"46F6CFC2DFAA2B","user_header":"https://static001.geekbang.org/account/avatar/00/11/ff/bc/368b9f80.jpg","comment_is_top":false,"comment_ctime":1587621905,"is_pvip":false,"replies":[{"id":"78371","content":"&quot;那未完整写入的消息(脏消息) 不也会累加到一起吗？&quot;  --- 不会的。能读到的都是已提交的完整的消息。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587712891,"ip_address":"","comment_id":209830,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587621905","product_id":100050101,"comment_content":"老师您好。validBytes为从logFile中的bactch 批量读取message的字节数累加实现，当消息batch 写入一部分的时候如果出现broker宕机的话，会写入一部份消息，那么logFile大小会大于validBytes大小，但是validBytes不也是会扫描当前file的message吗？ 那未完整写入的消息(脏消息) 不也会累加到一起吗？ 还是说 logbatch会验证消息的完整性之后再读取呢？ 感谢老师","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492875,"discussion_content":"&amp;quot;那未完整写入的消息(脏消息) 不也会累加到一起吗？&amp;quot;  --- 不会的。能读到的都是已提交的完整的消息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587712891,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209052,"user_name":"huldar","can_delete":false,"product_type":"c1","uid":1396865,"ip_address":"","ucode":"4B2C41B3A6B824","user_header":"https://static001.geekbang.org/account/avatar/00/15/50/81/e9c5a274.jpg","comment_is_top":false,"comment_ctime":1587482892,"is_pvip":false,"replies":[{"id":"78133","content":"这是Scala中的setter和getter写法。特别是setter写法与Java很不一样。比如：<br>private var _value = .....<br>def value = _value &#47;&#47; getter<br>def value_= (newVal:Int) = _value = newVal  &#47;&#47; setter","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587518227,"ip_address":"","comment_id":209052,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587482892","product_id":100050101,"comment_content":"  def offsetOfMaxTimestampSoFar_=(offset: Long): Unit = _offsetOfMaxTimestampSoFar = Some(offset)<br>  def offsetOfMaxTimestampSoFar: Long = {<br>    if (_offsetOfMaxTimestampSoFar.isEmpty)<br>      _offsetOfMaxTimestampSoFar = Some(timeIndex.lastEntry.offset)<br>    _offsetOfMaxTimestampSoFar.get<br>  }<br>老师,这个是Scala的什么特性呢 ?没有看懂.谢谢","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492707,"discussion_content":"这是Scala中的setter和getter写法。特别是setter写法与Java很不一样。比如：\nprivate var _value = .....\ndef value = _value // getter\ndef value_= (newVal:Int) = _value = newVal  // setter","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587518227,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208840,"user_name":"thomas","can_delete":false,"product_type":"c1","uid":1016777,"ip_address":"","ucode":"9AB945308F1B50","user_header":"https://static001.geekbang.org/account/avatar/00/0f/83/c9/5d03981a.jpg","comment_is_top":false,"comment_ctime":1587447577,"is_pvip":true,"replies":[{"id":"78134","content":"对对，这里是笔误。是records.sizeInBytes不是log.sizeInBytes。感谢指正，我让编辑小姐姐修正下~","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587518431,"ip_address":"","comment_id":208840,"utype":1}],"discussion_count":5,"race_medal":0,"score":"1587447577","product_id":100050101,"comment_content":"第一步：在源码中，首先调用 log.sizeInBytes 方法判断该日志段是否为空，如果是空的话， Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。<br>--------------------------------&gt;<br>老师，我在append源码中只看到 records.sizeInBytes&gt;0, 没有log.sizeInBytes; 而且代码逻辑里只有为非空的逻辑块。请问是我哪里理解错了吗","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492643,"discussion_content":"对对，这里是笔误。是records.sizeInBytes不是log.sizeInBytes。感谢指正，我让编辑小姐姐修正下~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587518431,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1440394,"avatar":"https://static001.geekbang.org/account/avatar/00/15/fa/8a/d5eaa5c4.jpg","nickname":"Do！","note":"","ucode":"C80C0B4C9B6B2A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":245160,"discussion_content":"physicalPosition == 0应该是当前日志段为空的话","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587651753,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1703222,"avatar":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","nickname":"Roger宇","note":"","ucode":"CBA23C01409349","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1440394,"avatar":"https://static001.geekbang.org/account/avatar/00/15/fa/8a/d5eaa5c4.jpg","nickname":"Do！","note":"","ucode":"C80C0B4C9B6B2A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":257732,"discussion_content":"所以并没有写错，就应该是log.sizeInBytes","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588602159,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":245160,"ip_address":""},"score":257732,"extra":""},{"author":{"id":1211223,"avatar":"https://static001.geekbang.org/account/avatar/00/12/7b/57/a9b04544.jpg","nickname":"QQ怪","note":"","ucode":"1A39B8433D9208","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1703222,"avatar":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","nickname":"Roger宇","note":"","ucode":"CBA23C01409349","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300840,"discussion_content":"同感","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598280081,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":257732,"ip_address":""},"score":300840,"extra":""}]},{"author":{"id":1440394,"avatar":"https://static001.geekbang.org/account/avatar/00/15/fa/8a/d5eaa5c4.jpg","nickname":"Do！","note":"","ucode":"C80C0B4C9B6B2A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":245116,"discussion_content":"我对此也有疑问：源码中说的是physicalPosition == 0的话，Kafka 需要记录要写入消息集合的最大时间戳，并将其作为后面新增日志段倒计时的依据。并没有提到如果records.sizeInBytes=0才进行上面操作。所以physicalPosition == 0是什么意思呢？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587650776,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206879,"user_name":"Kvicii.Y","can_delete":false,"product_type":"c1","uid":1442588,"ip_address":"","ucode":"446BFA633569EA","user_header":"https://static001.geekbang.org/account/avatar/00/16/03/1c/c9fe6738.jpg","comment_is_top":false,"comment_ctime":1586950022,"is_pvip":false,"replies":[{"id":"77316","content":"val truncated = log.sizeInBytes - validBytes<br>validBytes是真实读取的字节数，以这个数为准","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587003031,"ip_address":"","comment_id":206879,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586950022","product_id":100050101,"comment_content":"1.“遍历执行完成后，Kafka 会将日志段当前总字节数和刚刚累加的已读取字节数进行比较，如果发现前者比后者大，说明日志段写入了一些非法消息，需要执行截断操作，将日志段大小调整回合法的数值”<br>这个难道不是已读取的比总字节数大才有问题吗？不是很理解<br>2.老师不去开班教英语可惜了🤣🤣🤣","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491933,"discussion_content":"val truncated = log.sizeInBytes - validBytes\nvalidBytes是真实读取的字节数，以这个数为准","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003031,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206734,"user_name":"Jonah","can_delete":false,"product_type":"c1","uid":1079507,"ip_address":"","ucode":"2C4799BD2FF0DE","user_header":"https://static001.geekbang.org/account/avatar/00/10/78/d3/1dc40aa2.jpg","comment_is_top":false,"comment_ctime":1586923468,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1586923468","product_id":100050101,"comment_content":"如果targetSize超过了originSize，该方法会报错KafkaException","like_count":0},{"had_liked":false,"id":206634,"user_name":"lujg","can_delete":false,"product_type":"c1","uid":1054963,"ip_address":"","ucode":"6CCCD89A29B06D","user_header":"https://static001.geekbang.org/account/avatar/00/10/18/f3/cd07e64c.jpg","comment_is_top":false,"comment_ctime":1586912408,"is_pvip":true,"replies":[{"id":"77333","content":"没错！","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587003852,"ip_address":"","comment_id":206634,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586912408","product_id":100050101,"comment_content":"课后讨论问题：指定的位移值大于日志段本身保存的最大位移值时，改方法不会有其他动作。在translateOffset方法中如果offset大于日志段最大位移时会返回null。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491845,"discussion_content":"没错！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003852,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206615,"user_name":"xinxin","can_delete":false,"product_type":"c1","uid":1285617,"ip_address":"","ucode":"52C5D688B10968","user_header":"https://static001.geekbang.org/account/avatar/00/13/9d/f1/05da9c09.jpg","comment_is_top":false,"comment_ctime":1586910773,"is_pvip":false,"replies":[{"id":"77334","content":"别急，专栏后面有专门讲解，索引部分的马上就来了~","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587003895,"ip_address":"","comment_id":206615,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586910773","product_id":100050101,"comment_content":"求助老师讲解一下更新索引项方法offsetIndex.append(largestOffset, physicalPosition)中的细节","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491842,"discussion_content":"别急，专栏后面有专门讲解，索引部分的马上就来了~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003895,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206019,"user_name":"进化论","can_delete":false,"product_type":"c1","uid":1124107,"ip_address":"","ucode":"2865A23392B9F9","user_header":"https://static001.geekbang.org/account/avatar/00/11/27/0b/12dee5ed.jpg","comment_is_top":false,"comment_ctime":1586774244,"is_pvip":true,"replies":[{"id":"76989","content":"在请求模块部分会有部分涉及","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1586827781,"ip_address":"","comment_id":206019,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1586774244","product_id":100050101,"comment_content":"老师会讲解kafka协议实现吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491643,"discussion_content":"在请求模块部分会有部分涉及","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586827781,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1433189,"avatar":"https://static001.geekbang.org/account/avatar/00/15/de/65/51147fb6.jpg","nickname":"北纬8℃","note":"","ucode":"4999F2E037C93C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":233411,"discussion_content":"老师老师能组建个读者群不，像俺这种kafka源码都下不下来的小白，希望能寻求帮助😂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586924042,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1453053,"avatar":"https://static001.geekbang.org/account/avatar/00/16/2b/fd/7013289d.jpg","nickname":"温故而知新可以为师矣","note":"","ucode":"FADCDFA9F9E5E8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1433189,"avatar":"https://static001.geekbang.org/account/avatar/00/15/de/65/51147fb6.jpg","nickname":"北纬8℃","note":"","ucode":"4999F2E037C93C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":286609,"discussion_content":"可能性是0","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593243887,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":233411,"ip_address":""},"score":286609,"extra":""}]}]}]}