{"id":320123,"title":"13 | 容器磁盘限速：我的容器里磁盘读写为什么不稳定?","content":"<p>你好，我是程远。今天我们聊一聊磁盘读写不稳定的问题。</p><p>上一讲，我给你讲了如何通过XFS Quota来限制容器文件系统的大小，这是静态容量大小的一个限制。</p><p>你也许会马上想到，磁盘除了容量的划分，还有一个读写性能的问题。</p><p>具体来说，就是如果多个容器同时读写节点上的同一块磁盘，那么它们的磁盘读写相互之间影响吗？如果容器之间读写磁盘相互影响，我们有什么办法解决呢？</p><p>接下来，我们就带着问题一起学习今天的内容。</p><h2>场景再现</h2><p>我们先用这里的<a href=\"https://github.com/chengyli/training/tree/master/filesystem/blkio\">代码</a>，运行一下 <code>make image</code> 来做一个带fio的容器镜像，fio在我们之前的课程里提到过，它是用来测试磁盘文件系统读写性能的工具。</p><p>有了这个带fio的镜像，我们可以用它启动一个容器，在容器中运行fio，就可以得到只有一个容器读写磁盘时的性能数据。</p><pre><code class=\"language-shell\">mkdir -p /tmp/test1\ndocker stop fio_test1;docker rm fio_test1\ndocker run --name fio_test1 --volume /tmp/test1:/tmp  registery/fio:v1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=/tmp/fio_test1.log\n</code></pre><!-- [[[read_end]]] --><p>上面的这个Docker命令，我给你简单地解释一下：在这里我们第一次用到了\"--volume\"这个参数。之前我们讲过容器文件系统，比如OverlayFS。</p><p>不过容器文件系统并不适合频繁地读写。对于频繁读写的数据，容器需要把他们到放到\"volume\"中。这里的volume可以是一个本地的磁盘，也可以是一个网络磁盘。</p><p>在这个例子里我们就使用了宿主机本地磁盘，把磁盘上的/tmp/test1目录作为volume挂载到容器的/tmp目录下。</p><p>然后在启动容器之后，我们直接运行fio的命令，这里的参数和我们<a href=\"https://time.geekbang.org/column/article/318173\">第11讲</a>最开始的例子差不多，只是这次我们运行的是write，也就是写磁盘的操作，而写的目标盘就是挂载到/tmp目录的volume。</p><p>可以看到，fio的运行结果如下图所示，IOPS是18K，带宽(BW)是70MB/s左右。</p><p><img src=\"https://static001.geekbang.org/resource/image/a8/54/a8a156d4a543bc02133751a14ba5a354.png?wh=1920*335\" alt=\"\"></p><p>好了，刚才我们模拟了一个容器写磁盘的性能。那么如果这时候有两个容器，都在往同一个磁盘上写数据又是什么情况呢？我们可以再用下面的这个脚本试一下：</p><pre><code class=\"language-shell\">mkdir -p /tmp/test1\nmkdir -p /tmp/test2\n\ndocker stop fio_test1;docker rm fio_test1\ndocker stop fio_test2;docker rm fio_test2\n\ndocker run --name fio_test1 --volume /tmp/test1:/tmp  registery/fio:v1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=/tmp/fio_test1.log &amp;\n\ndocker run --name fio_test2 --volume /tmp/test2:/tmp  registery/fio:v1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=/tmp/fio_test2.log &amp;\n</code></pre><p>这时候，我们看到的结果，在容器fio_test1里，IOPS是15K左右，带宽是59MB/s了，比之前单独运行的时候性能下降了不少。</p><p><img src=\"https://static001.geekbang.org/resource/image/cb/64/cb2f19b2da651b03521804e22f14b864.png?wh=1662*98\" alt=\"\"></p><p>显然从这个例子中，我们可以看到多个容器同时写一块磁盘的时候，它的性能受到了干扰。那么有什么办法可以保证每个容器的磁盘读写性能呢？</p><p>之前，我们讨论过用Cgroups来保证容器的CPU使用率，以及控制Memroy的可用大小。那么你肯定想到了，我们是不是也可以用Cgroups来保证每个容器的磁盘读写性能？</p><p>没错，在Cgroup v1中有blkio子系统，它可以来限制磁盘的I/O。不过blkio子系统对于磁盘I/O的限制，并不像CPU，Memory那么直接，下面我会详细讲解。</p><h2>知识详解</h2><h3>Blkio Cgroup</h3><p>在讲解blkio Cgroup 前，我们先简单了解一下衡量磁盘性能的<strong>两个常见的指标IOPS和吞吐量（Throughput）</strong>是什么意思，后面讲Blkio Cgroup的参数配置时会用到。</p><p>IOPS是Input/Output Operations Per Second的简称，也就是每秒钟磁盘读写的次数，这个数值越大，当然也就表示性能越好。</p><p>吞吐量（Throughput）是指每秒钟磁盘中数据的读取量，一般以MB/s为单位。这个读取量可以叫作吞吐量，有时候也被称为带宽（Bandwidth）。刚才我们用到的fio显示结果就体现了带宽。</p><p>IOPS和吞吐量之间是有关联的，在IOPS固定的情况下，如果读写的每一个数据块越大，那么吞吐量也越大，它们的关系大概是这样的：吞吐量=数据块大小*IOPS。</p><p>好，那么我们再回到blkio Cgroup这个概念上，blkio Cgroup也是Cgroups里的一个子系统。 在Cgroups v1里，blkio Cgroup的虚拟文件系统挂载点一般在\"/sys/fs/cgroup/blkio/\"。</p><p>和我之前讲过的CPU，memory Cgroup一样，我们在这个\"/sys/fs/cgroup/blkio/\"目录下创建子目录作为控制组，再把需要做I/O限制的进程pid写到控制组的cgroup.procs参数中就可以了。</p><p>在blkio Cgroup中，有四个最主要的参数，它们可以用来限制磁盘I/O性能，我列在了下面。</p><pre><code>blkio.throttle.read_iops_device\nblkio.throttle.read_bps_device\nblkio.throttle.write_iops_device\nblkio.throttle.write_bps_device\n</code></pre><p>前面我们刚说了磁盘I/O的两个主要性能指标IOPS和吞吐量，在这里，根据这四个参数的名字，估计你已经大概猜到它们的意思了。</p><p>没错，它们分别表示：磁盘读取IOPS限制，磁盘读取吞吐量限制，磁盘写入IOPS限制，磁盘写入吞吐量限制。</p><p>对于每个参数写入值的格式，你可以参考内核<a href=\"https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt\">blkio的文档</a>。为了让你更好地理解，在这里我给你举个例子。</p><p>如果我们要对一个控制组做限制，限制它对磁盘/dev/vdb的写入吞吐量不超过10MB/s，那么我们对blkio.throttle.write_bps_device参数的配置就是下面这个命令。</p><pre><code class=\"language-shell\">echo \"252:16 10485760\" &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device\n</code></pre><p>在这个命令中，\"252:16\"是 /dev/vdb的主次设备号，你可以通过 <code>ls -l /dev/vdb</code> 看到这两个值，而后面的\"10485760\"就是10MB的每秒钟带宽限制。</p><pre><code class=\"language-shell\"># ls -l /dev/vdb -l\nbrw-rw---- 1 root disk 252, 16 Nov  2 08:02 /dev/vdb\n</code></pre><p>了解了blkio Cgroup的参数配置，我们再运行下面的这个例子，限制一个容器blkio的读写磁盘吞吐量，然后在这个容器里运行一下fio，看看结果是什么。</p><pre><code class=\"language-shell\">mkdir -p /tmp/test1\nrm -f /tmp/test1/*\n\ndocker stop fio_test1;docker rm fio_test1\n\ndocker run -d --name fio_test1 --volume /tmp/test1:/tmp  registery/fio:v1 sleep 3600\n\nsleep 2\n\nCONTAINER_ID=$(sudo docker ps --format \"{{.ID}}\\t{{.Names}}\" | grep -i fio_test1 | awk '{print $1}')\n\necho $CONTAINER_ID\n\nCGROUP_CONTAINER_PATH=$(find /sys/fs/cgroup/blkio/ -name \"*$CONTAINER_ID*\")\n\necho $CGROUP_CONTAINER_PATH\n\n# To get the device major and minor id from /dev for the device that /tmp/test1 is on.\n\necho \"253:0 10485760\" &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.read_bps_device\n\necho \"253:0 10485760\" &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device\n\ndocker exec fio_test1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test1.log\n\ndocker exec fio_test1 fio -direct=1 -rw=read -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test1.log\n</code></pre><p>在这里，我的机器上/tmp/test1所在磁盘主次设备号是”253:0”，你在自己运行这组命令的时候，需要把主次设备号改成你自己磁盘的对应值。</p><p>还有一点我要提醒一下，不同数据块大小，在性能测试中可以适用于不同的测试目的。但因为这里不是我们要讲的重点，所以为了方便你理解概念，这里就用固定值。</p><p>在我们后面的例子里，fio读写的数据块都固定在4KB。所以对于磁盘的性能限制，我们在blkio Cgroup里就只设置吞吐量限制了。</p><p>在加了blkio Cgroup限制10MB/s后，从fio运行后的输出结果里，我们可以看到这个容器对磁盘无论是读还是写，它的最大值就不会再超过10MB/s了。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/b3/e26118e821a4b936521eacac924c7db3.png?wh=1492*112\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/0a/f5/0ae074c568161d24e57d37d185a47af5.png?wh=1502*100\" alt=\"\"></p><p>在给每个容器都加了blkio Cgroup限制，限制为10MB/s后，即使两个容器同时在一个磁盘上写入文件，那么每个容器的写入磁盘的最大吞吐量，也不会互相干扰了。</p><p>我们可以用下面的这个脚本来验证一下。</p><pre><code class=\"language-shell\">#!/bin/bash\n\nmkdir -p /tmp/test1\nrm -f /tmp/test1/*\ndocker stop fio_test1;docker rm fio_test1\n\nmkdir -p /tmp/test2\nrm -f /tmp/test2/*\ndocker stop fio_test2;docker rm fio_test2\n\ndocker run -d --name fio_test1 --volume /tmp/test1:/tmp  registery/fio:v1 sleep 3600\ndocker run -d --name fio_test2 --volume /tmp/test2:/tmp  registery/fio:v1 sleep 3600\n\nsleep 2\n\nCONTAINER_ID1=$(sudo docker ps --format \"{{.ID}}\\t{{.Names}}\" | grep -i fio_test1 | awk '{print $1}')\necho $CONTAINER_ID1\n\nCGROUP_CONTAINER_PATH1=$(find /sys/fs/cgroup/blkio/ -name \"*$CONTAINER_ID1*\")\necho $CGROUP_CONTAINER_PATH1\n\n# To get the device major and minor id from /dev for the device that /tmp/test1 is on.\n\necho \"253:0 10485760\" &gt; $CGROUP_CONTAINER_PATH1/blkio.throttle.read_bps_device\n\necho \"253:0 10485760\" &gt; $CGROUP_CONTAINER_PATH1/blkio.throttle.write_bps_device\n\nCONTAINER_ID2=$(sudo docker ps --format \"{{.ID}}\\t{{.Names}}\" | grep -i fio_test2 | awk '{print $1}')\necho $CONTAINER_ID2\n\nCGROUP_CONTAINER_PATH2=$(find /sys/fs/cgroup/blkio/ -name \"*$CONTAINER_ID2*\")\necho $CGROUP_CONTAINER_PATH2\n\n# To get the device major and minor id from /dev for the device that /tmp/test1 is on.\necho \"253:0 10485760\" &gt; $CGROUP_CONTAINER_PATH2/blkio.throttle.read_bps_device\n\necho \"253:0 10485760\" &gt; $CGROUP_CONTAINER_PATH2/blkio.throttle.write_bps_device\n\ndocker exec fio_test1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test1.log &amp;\n\ndocker exec fio_test2 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test2.log &amp;\n</code></pre><p>我们还是看看fio运行输出的结果，这时候，fio_test1和fio_test2两个容器里执行的结果都是10MB/s了。</p><p><img src=\"https://static001.geekbang.org/resource/image/67/a9/6719cc30a8e2933dae1ba6f96235e4a9.png?wh=1492*112\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/de/c8/de4be66c72ff9e4cdc5007fe71f848c8.png?wh=1482*104\" alt=\"\"></p><p>那么做到了这一步，我们是不是就可以认为，blkio Cgroup可以完美地对磁盘I/O做限制了呢？</p><p>你先别急，我们可以再做个试验，把前面脚本里fio命令中的 “-direct=1” 给去掉，也就是不让fio运行在Direct I/O模式了，而是用Buffered I/O模式再运行一次，看看fio执行的输出。</p><p>同时我们也可以运行iostat命令，查看实际的磁盘写入速度。</p><p>这时候你会发现，即使我们设置了blkio Cgroup，也根本不能限制磁盘的吞吐量了。</p><h3>Direct I/O 和 Buffered I/O</h3><p>为什么会这样的呢？这就要提到Linux的两种文件I/O模式了：Direct I/O和Buffered I/O。</p><p>Direct I/O 模式，用户进程如果要写磁盘文件，就会通过Linux内核的文件系统层(filesystem) -&gt; 块设备层(block layer) -&gt; 磁盘驱动 -&gt; 磁盘硬件，这样一路下去写入磁盘。</p><p>而如果是Buffered I/O模式，那么用户进程只是把文件数据写到内存中（Page Cache）就返回了，而Linux内核自己有线程会把内存中的数据再写入到磁盘中。<strong>在Linux里，由于考虑到性能问题，绝大多数的应用都会使用Buffered I/O模式。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/10/46/1021f5f7ec700f3c7c66cbf8e07b1a46.jpeg?wh=3200*1800\" alt=\"\"></p><p>我们通过前面的测试，发现Direct I/O可以通过blkio Cgroup来限制磁盘I/O，但是Buffered I/O不能被限制。</p><p>那通过上面的两种I/O模式的解释，你是不是可以想到原因呢？是的，原因就是被Cgroups v1的架构限制了。</p><p>我们已经学习过了v1 的CPU Cgroup，memory Cgroup和blkio Cgroup，那么Cgroup v1的一个整体结构，你应该已经很熟悉了。它的每一个子系统都是独立的，资源的限制只能在子系统中发生。</p><p>就像下面图里的进程pid_y，它可以分别属于memory Cgroup和blkio Cgroup。但是在blkio Cgroup对进程pid_y做磁盘I/O做限制的时候，blkio子系统是不会去关心pid_y用了哪些内存，哪些内存是不是属于Page Cache，而这些Page Cache的页面在刷入磁盘的时候，产生的I/O也不会被计算到进程pid_y上面。</p><p>就是这个原因，导致了blkio 在Cgroups v1里不能限制Buffered I/O。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/ba/32c69a6f69c4ce7f11c842450fe7d9ba.jpeg?wh=3200*1800\" alt=\"\"></p><p>这个Buffered I/O限速的问题，在Cgroup V2里得到了解决，其实这个问题也是促使Linux开发者重新设计Cgroup V2的原因之一。</p><h2>Cgroup V2</h2><p>Cgroup v2相比Cgroup v1做的最大的变动就是一个进程属于一个控制组，而每个控制组里可以定义自己需要的多个子系统。</p><p>比如下面的Cgroup V2示意图里，进程pid_y属于控制组group2，而在group2里同时打开了io和memory子系统 （Cgroup V2里的io子系统就等同于Cgroup v1里的blkio子系统）。</p><p>那么，Cgroup对进程pid_y的磁盘 I/O做限制的时候，就可以考虑到进程pid_y写入到Page Cache内存的页面了，这样buffered I/O的磁盘限速就实现了。</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/46/8ae3f3282b9f19720b764c696959bf46.jpeg?wh=3200*1800\" alt=\"\"></p><p>下面我们在Cgroup v2里，尝试一下设置了blkio Cgroup+Memory Cgroup之后，是否可以对Buffered I/O进行磁盘限速。</p><p>我们要做的第一步，就是在Linux系统里打开Cgroup v2的功能。因为目前即使最新版本的Ubuntu Linux或者Centos Linux，仍然在使用Cgroup v1作为缺省的Cgroup。</p><p>打开方法就是配置一个kernel参数\"cgroup_no_v1=blkio,memory\"，这表示把Cgroup v1的blkio和Memory两个子系统给禁止，这样Cgroup v2的io和Memory这两个子系统就打开了。</p><p>我们可以把这个参数配置到grub中，然后我们重启Linux机器，这时Cgroup v2的 io还有Memory这两个子系统，它们的功能就打开了。</p><p>系统重启后，我们会看到Cgroup v2的虚拟文件系统被挂载到了 /sys/fs/cgroup/unified目录下。</p><p>然后，我们用下面的这个脚本做Cgroup v2 io的限速配置，并且运行fio，看看buffered I/O是否可以被限速。</p><pre><code class=\"language-shell\"># Create a new control group\nmkdir -p /sys/fs/cgroup/unified/iotest\n\n# enable the io and memory controller subsystem\necho \"+io +memory\" &gt; /sys/fs/cgroup/unified/cgroup.subtree_control\n\n# Add current bash pid in iotest control group.\n# Then all child processes of the bash will be in iotest group too,\n# including the fio\necho $$ &gt;/sys/fs/cgroup/unified/iotest/cgroup.procs\n\n# 256:16 are device major and minor ids, /mnt is on the device.\necho \"252:16 wbps=10485760\" &gt; /sys/fs/cgroup/unified/iotest/io.max\ncd /mnt\n#Run the fio in non direct I/O mode\nfio -iodepth=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=./fio.test\n</code></pre><p>在这个例子里，我们建立了一个名叫iotest的控制组，并且在这个控制组里加入了io和Memory两个控制子系统，对磁盘最大吞吐量的设置为10MB。运行fio的时候不加\"-direct=1\"，也就是让fio运行在buffered I/O模式下。</p><p>运行fio写入1GB的数据后，你会发现fio马上就执行完了，因为系统上有足够的内存，fio把数据写入内存就返回了，不过只要你再运行”iostat -xz 10” 这个命令，你就可以看到磁盘vdb上稳定的写入速率是10240wkB/s，也就是我们在io Cgroup里限制的10MB/s。</p><p><img src=\"https://static001.geekbang.org/resource/image/81/c6/8174902114e01369193945891d054cc6.png?wh=1872*1036\" alt=\"\"></p><p>看到这个结果，我们证实了Cgoupv2 io+Memory两个子系统一起使用，就可以对buffered I/O控制磁盘写入速率。</p><h2>重点总结</h2><p>这一讲，我们主要想解决的问题是如何保证容器读写磁盘速率的稳定，特别是当多个容器同时读写同一个磁盘的时候，需要减少相互的干扰。</p><p>Cgroup V1的blkiio控制子系统，可以用来限制容器中进程的读写的IOPS和吞吐量（Throughput），但是它只能对于Direct I/O的读写文件做磁盘限速，对Buffered I/O的文件读写，它无法进行磁盘限速。</p><p><strong>这是因为Buffered I/O会把数据先写入到内存Page Cache中，然后由内核线程把数据写入磁盘，而Cgroup v1 blkio的子系统独立于memory 子系统，无法统计到由Page Cache刷入到磁盘的数据量。</strong></p><p>这个Buffered I/O无法被限速的问题，在Cgroup v2里被解决了。Cgroup v2从架构上允许一个控制组里有多个子系统协同运行，这样在一个控制组里只要同时有io和Memory子系统，就可以对Buffered I/O 作磁盘读写的限速。</p><p>虽然Cgroup v2 解决了Buffered I/O 磁盘读写限速的问题，但是在现实的容器平台上也不是能够立刻使用的，还需要等待一段时间。目前从runC、containerd到Kubernetes都是刚刚开始支持Cgroup v2，而对生产环境中原有运行Cgroup v1的节点要迁移转化成Cgroup v2需要一个过程。</p><h2>思考题</h2><p>最后呢，我给你留一道思考题。 其实这是一道操作题，通过这个操作你可以再理解一下 blkio Cgroup与 Buffered I/O的关系。</p><p>在Cgroup v1的环境里，我们在blkio Cgroup v1的例子基础上，把 fio 中\"direct=1\"参数去除之后，再运行fio，同时运行iostat查看实际写入磁盘的速率，确认Cgroup v1 blkio无法对Buffered I/O限速。</p><p>欢迎你在留言区分享你的收获和疑问。如果这篇文章给你带来了启发，也欢迎转发给你的朋友，一起学习和交流。</p>","neighbors":{"left":{"article_title":"12 | 容器文件Quota：容器为什么把宿主机的磁盘写满了？","id":318978},"right":{"article_title":"14 | 容器中的内存与I/O：容器写文件的延时为什么波动很大？","id":321330}},"comments":[{"had_liked":false,"id":268033,"user_name":"Geek2014","can_delete":false,"product_type":"c1","uid":2028957,"ip_address":"","ucode":"9EB356D8DF287E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","comment_is_top":false,"comment_ctime":1608034874,"is_pvip":false,"replies":[{"id":"97460","content":"@Geek2014， 是这样的。<br>","user_name":"作者回复","user_name_real":"CY","uid":"2070138","ctime":1608216290,"ip_address":"","comment_id":268033,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23082871354","product_id":100063801,"comment_content":"cgroup v2在buffered IO模式下能限制，是不是可以理解为：写入的1G数据对应的page cache属于该进程，内核同步该部分page cache时产生的IO会被计算在该进程的IO中","like_count":6,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511822,"discussion_content":"@Geek2014， 是这样的。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608216290,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269431,"user_name":"Alery","can_delete":false,"product_type":"c1","uid":1156557,"ip_address":"","ucode":"08F3F49181E67B","user_header":"https://static001.geekbang.org/account/avatar/00/11/a5/cd/3aff5d57.jpg","comment_is_top":false,"comment_ctime":1608643613,"is_pvip":false,"replies":[{"id":"97763","content":"如果一个容器用到多个磁盘（volume），并且都要做io限制，那么都要设置一遍。","user_name":"作者回复","user_name_real":"CY","uid":"2070138","ctime":1608719946,"ip_address":"","comment_id":269431,"utype":1}],"discussion_count":3,"race_medal":0,"score":"14493545501","product_id":100063801,"comment_content":"老师，假如一个容器跨多块磁盘是不是需要拿到每块磁盘的主次设备号都设置一遍iops限制？","like_count":3,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512271,"discussion_content":"如果一个容器用到多个磁盘（volume），并且都要做io限制，那么都要设置一遍。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608719946,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1580926,"avatar":"","nickname":"ch_ort","note":"","ucode":"B79746E687F29E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":575910,"discussion_content":"cgroup限制的是整个进程的IOPS，跟用了多少块磁盘没关系","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1655182147,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1156557,"avatar":"https://static001.geekbang.org/account/avatar/00/11/a5/cd/3aff5d57.jpg","nickname":"Alery","note":"","ucode":"08F3F49181E67B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":338403,"discussion_content":"老师我可能没描述清楚，我的意思是宿主机上的一个目录可能会跨越多个块设备，假如我把宿主机的这个目录挂在到容器中，这种情况是不是得对底层多个块设备分别做限制？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609256198,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269086,"user_name":"yayiyaya","can_delete":false,"product_type":"c1","uid":1615156,"ip_address":"","ucode":"38532C740FA186","user_header":"https://static001.geekbang.org/account/avatar/00/18/a5/34/6e3e962f.jpg","comment_is_top":false,"comment_ctime":1608518498,"is_pvip":true,"replies":[{"id":"97711","content":"iops也是blkio里的限制参数之一。我在例子中用了吞吐量来限制。","user_name":"作者回复","user_name_real":"CY","uid":"2070138","ctime":1608650330,"ip_address":"","comment_id":269086,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10198453090","product_id":100063801,"comment_content":"吞吐量 = 数据块大小 *IOPS。  如果限制了IOPS， 是不是也可以做到容器的写入磁盘也不会互相干扰了？","like_count":2,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512143,"discussion_content":"iops也是blkio里的限制参数之一。我在例子中用了吞吐量来限制。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608650330,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":271535,"user_name":"小羊","can_delete":false,"product_type":"c1","uid":1000877,"ip_address":"","ucode":"50239B7052BFD2","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epgKOrnIOAjzXJgb0f0ljTZLeqrMXYaHic1MKQnPbAzxSKgYxd7K2DlqRW8SibTkwV2MAUZ4OlgRnNw/132","comment_is_top":false,"comment_ctime":1609684510,"is_pvip":false,"replies":[{"id":"98646","content":"CentOS8上缺省用的还是Cgroup V1","user_name":"作者回复","user_name_real":"CY","uid":"2070138","ctime":1609941226,"ip_address":"","comment_id":271535,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5904651806","product_id":100063801,"comment_content":"centos8 上的podman 不是 默认cgroup v2了吗？我记得有一次报错查下来就是 v2不支持某个特性导致无法使用某个镜像。可能我搞错了？","like_count":1,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512952,"discussion_content":"CentOS8上缺省用的还是Cgroup V1","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609941226,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":267734,"user_name":"InfoQ_31da8ff7fcb1","can_delete":false,"product_type":"c1","uid":1485279,"ip_address":"","ucode":"AEF58B6E4F9E59","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIzVGyhMctYa2jumzLicZVLia0UCTqrWfiaY8pY4c3AbGH2tH5TxONcbicoXGdE3ia43TpXxbZWPZoS6Jg/132","comment_is_top":false,"comment_ctime":1607908725,"is_pvip":false,"replies":[{"id":"97321","content":"你的想法是让容器独享一个磁盘？这样不用device plugin, 用local volume就可以了。","user_name":"作者回复","user_name_real":"CY","uid":"2070138","ctime":1608041623,"ip_address":"","comment_id":267734,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5902876021","product_id":100063801,"comment_content":"现阶段在k8s对磁盘io进行限速可以尝试用device plugin + prestart hook么（仿照nvidia gpu的思路）？只是一个想法，想求证下","like_count":1,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511724,"discussion_content":"你的想法是让容器独享一个磁盘？这样不用device plugin, 用local volume就可以了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608041623,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1207457,"avatar":"https://static001.geekbang.org/account/avatar/00/12/6c/a1/80d83f0a.jpg","nickname":"Ellison","note":"","ucode":"A2FB94D4F6A332","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":540702,"discussion_content":"local pv就是一个整块的磁盘","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640141195,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028957,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","nickname":"Geek2014","note":"","ucode":"9EB356D8DF287E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":334938,"discussion_content":"可以说下你的思路，大家一起讨论下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608032246,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":356450,"user_name":"JianXu","can_delete":false,"product_type":"c1","uid":1033219,"ip_address":"上海","ucode":"2A61BDBB573BDC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c4/03/f753fda7.jpg","comment_is_top":false,"comment_ctime":1662338519,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1662338519","product_id":100063801,"comment_content":"CY 老师，今天看了这个才知道为什么我们希望去除大数据部门的专用独占硬件资源的工作模式需要cgroup v2。 那现在cgroup v2 在Kubernetes 上成熟度达到你的预期了吗？","like_count":0},{"had_liked":false,"id":352005,"user_name":"有识之士","can_delete":false,"product_type":"c1","uid":1120024,"ip_address":"","ucode":"23F5594193D200","user_header":"https://static001.geekbang.org/account/avatar/00/11/17/18/e4382a8e.jpg","comment_is_top":false,"comment_ctime":1658322073,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1658322073","product_id":100063801,"comment_content":"Step 1&#47;2 : FROM centos:8.1.1911<br> ---&gt; 470671670cac<br>Step 2&#47;2 : RUN yum install -y fio<br> ---&gt; Running in b64c47e09828<br>CentOS-8 - AppStream                             44  B&#47;s |  38  B     00:00<br>Failed to download metadata for repo &#39;AppStream&#39;<br>Error: Failed to download metadata for repo &#39;AppStream&#39;<br>The command &#39;&#47;bin&#47;sh -c yum install -y fio&#39; returned a non-zero code: 1<br>make: *** [image] Error 1    <br><br>Centos8于2021年年底停止了服务, 这个测试demo应该是有问题的，可以同步修改下？","like_count":0},{"had_liked":false,"id":311583,"user_name":"罗峰","can_delete":false,"product_type":"c1","uid":1218501,"ip_address":"","ucode":"5F3D6AF8F28322","user_header":"https://static001.geekbang.org/account/avatar/00/12/97/c5/84491beb.jpg","comment_is_top":false,"comment_ctime":1631321851,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1631321851","product_id":100063801,"comment_content":"发现cpu memory cgroup可以单独设置容器的资源使用量，但是io这个容器是共用一个控制组的配置参数(不知道是否每个容器可以使用不同的io控制组)吗？","like_count":0},{"had_liked":false,"id":311343,"user_name":"IOVE.-Minn","can_delete":false,"product_type":"c1","uid":1227784,"ip_address":"","ucode":"9C637EEEF64057","user_header":"https://static001.geekbang.org/account/avatar/00/12/bc/08/c43f85d9.jpg","comment_is_top":false,"comment_ctime":1631180636,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1631180636","product_id":100063801,"comment_content":"如何快速的查看cgroup的版本呢？","like_count":0},{"had_liked":false,"id":308525,"user_name":"罗峰","can_delete":false,"product_type":"c1","uid":1218501,"ip_address":"","ucode":"5F3D6AF8F28322","user_header":"https://static001.geekbang.org/account/avatar/00/12/97/c5/84491beb.jpg","comment_is_top":false,"comment_ctime":1629677215,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1629677215","product_id":100063801,"comment_content":"老师，你好，如果容器里面从云存储读取保存到本地，然后在写回到云存储，文件大小大概5G左右。这种不算频繁读写吧。因为使用的虚拟机，所以本地也是一个云存储挂载到虚拟机。之前出现过本地的磁盘被打满导致kubelet心跳上报失败。解决办法是本地采用贷款更高的物理磁盘阵列组raid。","like_count":0},{"had_liked":false,"id":302522,"user_name":"徐少文","can_delete":false,"product_type":"c1","uid":1670331,"ip_address":"","ucode":"8E35B10DA44EE3","user_header":"https://static001.geekbang.org/account/avatar/00/19/7c/bb/635a2710.jpg","comment_is_top":false,"comment_ctime":1626249501,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626249501","product_id":100063801,"comment_content":"老师，为什么我向io.max这个文件写内容的时候提示我设备不存在。但是我的设备确实是存在的，已经通过ls -l &#47;dev&#47;sda5验证了","like_count":0},{"had_liked":false,"id":268699,"user_name":"谢哈哈","can_delete":false,"product_type":"c1","uid":2326880,"ip_address":"","ucode":"5AADE70B5AFE27","user_header":"https://static001.geekbang.org/account/avatar/00/23/81/60/71ed6ac7.jpg","comment_is_top":false,"comment_ctime":1608295218,"is_pvip":false,"replies":[{"id":"97558","content":"@谢哈哈<br>这个是你的总结吗？","user_name":"作者回复","user_name_real":"CY","uid":"2070138","ctime":1608473999,"ip_address":"","comment_id":268699,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608295218","product_id":100063801,"comment_content":"cgoups V1下，blkio与buffer io是没什么关系的，一个是DIO模式写入磁盘，一个是通过page cache异步写入磁盘，在cgoups V2下，blkio 在非DIO的模式下包括了buffer io","like_count":0,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512012,"discussion_content":"@谢哈哈\n这个是你的总结吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608473999,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}