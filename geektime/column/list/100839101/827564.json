{"id":827564,"title":"11｜自己动手实现一个RAG应用","content":"<p>你好，我是郑晔！</p><p>上一讲，我们用了一讲的篇幅介绍了一下 RAG。你现在已经对 RAG 有了一个初步的了解，这一讲，我们就要动手实现一个 RAG 应用了。</p><p>我们知道 RAG 有两个核心的过程，一个是把信息存放起来的索引过程，一个是利用找到相关信息生成内容的检索生成过程。所以，我们这个 RAG 应用也要分成两个部分：<strong>索引和检索生成</strong>。</p><p>RAG 是为了让大模型知道更多的东西，所以，接下来要实现的 RAG 应用，用来增强的信息就是我们这门课程的内容，我会把开篇词做成一个文件，这样，我们就可以和大模型讨论我们的课程了。LangChain 已经提供了一些基础设施，我们可以利用这些基础设施构建我们的应用。</p><p>我们先从索引的过程开始！</p><h2>索引</h2><p>我们<a href=\"https://time.geekbang.org/column/article/827289\">上一讲</a>已经讲过了索引的基本过程，你可以先回顾一下，这样结合代码看起来就比较容易理解了：</p><p><img src=\"https://static001.geekbang.org/resource/image/bd/a2/bdba19e4bbcd2d5bc548cd23372a86a2.jpg?wh=2202x471\" alt=\"\"></p><p>下面是实现这个索引过程的代码：</p><pre><code class=\"language-python\">from langchain_community.document_loaders import TextLoader\n\nloader = TextLoader(\"introduction.txt\")\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma(\n    collection_name=\"ai_learning\",\n    embedding_function=OpenAIEmbeddings(),\n    persist_directory=\"vectordb\"\n)\nvectorstore.add_documents(splits)\n</code></pre><!-- [[[read_end]]] --><p>出于简化的目的，我这里直接从文本内容中加载信息源，而且选择了 Chroma 作为向量数据库，它对开发很友好，可以把向量数据存储在本地的指定目录下。</p><p>我们结合代码来看一下。首先是 TextLoader，它负责加载文本信息。</p><pre><code class=\"language-python\">loader = TextLoader(\"introduction.txt\")\ndocs = loader.load()\n</code></pre><p>这里的 TextLoader 属于 DocumentLoader。在 LangChain 中，有一个很重要的概念叫文档（Document），它包括文档的内容（page_content）以及相关的元数据（metadata）。所有原始信息都是文档，索引信息的第一步就是把这些文档加载进来，这就是 DocumentLoader 的作用。</p><p>除了这里用到的 TextLoader，LangChain 社区里已经实现了大量的 DocumentLoader，比如，从数据库里加载数据的 SQLDatabaseLoader，从亚马逊 S3 加载文件的 S3FileLoader。基本上，大部分我们需要的文档加载器都可以找到直接的实现。</p><p>拆分加载进来的文档是 TextSplitter 的主要职责。</p><pre><code class=\"language-python\">text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n</code></pre><p>虽然都是文本，但怎样拆分还是有讲究的，拆分源代码和拆分普通文本，处理方法就是不一样的。LangChain 社区里同样实现了大量的 TextSplitter，我们可以根据自己的业务特点进行选择。我们这里使用了 RecursiveCharacterTextSplitter，它会根据常见的分隔符（比如换行符）递归地分割文档，直到把每个块拆分成适当的大小。</p><p>做好基础的准备之后，就要把拆分的文档存放到向量数据库里了：</p><pre><code class=\"language-python\">vectorstore = Chroma(\n    collection_name=\"ai_learning\",\n    embedding_function=OpenAIEmbeddings(),\n    persist_directory=\"vectordb\"\n)\nvectorstore.add_documents(splits)\n</code></pre><p>LangChain 支持了很多的向量数据库，它们都有一个统一的接口：VectorStore，在这个接口中包含了向量数据库的统一操作，比如添加、查询之类的。这个接口屏蔽了向量数据库的差异，在向量数据库并不为所有程序员熟知的情况下，给尝试不同的向量数据库留下了空间。各个具体实现负责实现这些接口，我们这里采用的实现是 Chroma。</p><p>在 Chroma 初始化的过程中，我们指定了 Embedding 函数，它负责把文本变成向量。这里我们采用了 OpenAI 的 Embeddings 实现，你完全可以根据自己的需要选择相应的实现，LangChain 社区同样提供了大量的实现，比如，你可以指定 Hugging Face 这个模型社区中的特定模型来做 Embedding。</p><p>到这里，我们就完成了索引的过程，看上去还是比较简单的。为了验证我们索引的结果，我们可以调用 similarity_search 检索向量数据库的数据：</p><pre><code class=\"language-python\">vectorstore = Chroma(\n    collection_name=\"ai_learning\",\n    embedding_function=OpenAIEmbeddings(),\n    persist_directory=\"vectordb\"\n)\ndocuments = vectorstore.similarity_search(\"专栏的作者是谁？\")\nprint(documents)\n</code></pre><p>我们这里用的 similarity_search 表示的是根据相似度进行搜索，还可以使用 max_marginal_relevance_search，它会采用 MMR（Maximal Marginal Relevance，最大边际相关性）算法。这个算法可以在保持结果相关性的同时，尽量选择与已选结果不相似的内容，以增加结果的多样性。</p><h2>检索生成</h2><p>现在，我们已经为我们 RAG 应用准备好了数据。接下来，就该正式地构建我们的 RAG 应用了。我在之前的聊天机器上做了一些修改，让它能够支持 RAG，代码如下：</p><pre><code class=\"language-python\">from operator import itemgetter\nfrom typing import List\nimport tiktoken\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage, trim_messages\nfrom langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_chroma import Chroma\n\nvectorstore = Chroma(\n    collection_name=\"ai_learning\",\n    embedding_function=OpenAIEmbeddings(),\n    persist_directory=\"vectordb\"\n)\n\nretriever = vectorstore.as_retriever(search_type=\"similarity\")\n\ndef str_token_counter(text: str) -&gt; int:\n    enc = tiktoken.get_encoding(\"o200k_base\")\n    return len(enc.encode(text))\n\ndef tiktoken_counter(messages: List[BaseMessage]) -&gt; int:\n    num_tokens = 3\n    tokens_per_message = 3\n    tokens_per_name = 1\n    for msg in messages:\n        if isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"assistant\"\n        elif isinstance(msg, ToolMessage):\n            role = \"tool\"\n        elif isinstance(msg, SystemMessage):\n            role = \"system\"\n        else:\n            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n        num_tokens += (\n                tokens_per_message\n                + str_token_counter(role)\n                + str_token_counter(msg.content)\n        )\n        if msg.name:\n            num_tokens += tokens_per_name + str_token_counter(msg.name)\n    return num_tokens\n\ntrimmer = trim_messages(\n    max_tokens=4096,\n    strategy=\"last\",\n    token_counter=tiktoken_counter,\n    include_system=True,\n)\n\nstore = {}\n\ndef get_session_history(session_id: str) -&gt; BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    return store[session_id]\n\nmodel = ChatOpenAI()\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n            Context: {context}\"\"\",\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\ncontext = itemgetter(\"question\") | retriever | format_docs\nfirst_step = RunnablePassthrough.assign(context=context)\nchain = first_step | prompt | trimmer | model\n\nwith_message_history = RunnableWithMessageHistory(\n    chain,\n    get_session_history=get_session_history,\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n\nconfig = {\"configurable\": {\"session_id\": \"dreamhead\"}}\n\nwhile True:\n    user_input = input(\"You:&gt; \")\n    if user_input.lower() == 'exit':\n        break\n\n    if user_input.strip() == \"\":\n        continue\n\n    stream = with_message_history.stream(\n        {\"question\": user_input},\n        config=config\n    )\n    for chunk in stream:\n        print(chunk.content, end='', flush=True)\n    print()\n</code></pre><p>为了进行检索，我们需要指定数据源，这里就是我们的向量数据库，其中存放着我们前面已经索引过的数据：</p><pre><code class=\"language-python\">vectorstore = Chroma(\n    collection_name=\"ai_learning\",\n    embedding_function=OpenAIEmbeddings(),\n    persist_directory=\"vectordb\"\n)\n\nretriever = vectorstore.as_retriever(search_type=\"similarity\")\n</code></pre><p>这段代码引入了一个新的概念：<strong>Retriever</strong>。从名字不难看出，它就是充当 RAG 中的 R。Retriever 的核心能力就是根据文本查询出对应的文档（Document）。</p><p>为什么不直接使用向量数据库呢？因为 Retriever 并不只有向量数据库一种实现，比如，WikipediaRetriever 可以从 Wikipedia 上进行搜索。所以，一个 Retriever 接口就把具体的实现隔离开来。</p><p>回到向量数据库上，当我们调用 <code>as_retriever</code> 创建 Retriever 时，还传入了搜索类型（<code>search_type</code>），这里的搜索类型和前面讲到向量数据库的检索方式是一致的，这里我们传入的是 similarity，当然也可以传入 mmr。</p><p>文档检索出来，并不能直接就和我们的问题拼装到一起。这时，就轮到提示词登场了。下面是我们在代码里用到的提示词（改造自<a href=\"https://smith.langchain.com/hub/rlm/rag-prompt\">https://smith.langchain.com/hub/rlm/rag-prompt</a>）</p><pre><code class=\"language-plain\">You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nContext: {context}\n</code></pre><p>在这段提示词里，我们告诉大模型，根据提供的上下文回答问题，不知道就说不知道。这是一个提示词模板，在提示词的最后是我们给出的上下文（Context）。这里上下文是根据问题检索出来的内容。</p><p>有了这个提示词，再加上聊天历史和我们的问题，就构成了一个完整的提示词模板：</p><pre><code class=\"language-plain\">prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n            Context: {context}\"\"\",\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n</code></pre><p>好，我们已经理解了这一讲的新内容，接下来，就是把各个组件组装到一起，构成一条完整的链：</p><pre><code class=\"language-plain\">context = itemgetter(\"question\") | retriever | format_docs\nfirst_step = RunnablePassthrough.assign(context=context)\nchain = first_step | prompt | trimmer | model\n\nwith_message_history = RunnableWithMessageHistory(\n    chain,\n    get_session_history=get_session_history,\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n</code></pre><p>在这段代码里，我们首先构建了一个 context 变量，它也一条链。第一步是从传入参数中获取到 question 属性，也就是我们的问题，然后把它传给 retriever。retriever 会根据问题去做检索，对应到我们这里的实现，就是到向量数据库中检索，检索的结果是一个文档列表。</p><p>文档是 LangChain 应用内部的表示，要传给大模型，我们需要把它转成文本，这就是 format_docs 做的事情，它主要是把文档内容取出来拼接到一起：</p><pre><code class=\"language-python\">def format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre><p>这里补充几句实现细节。在 LangChain 代码里， <code>|</code> 运算符被用作不同组件之间的连接，其实现的关键就是大部分组件都实现了 Runnable 接口，在这个接口里实现了 <code>__or__</code> 和 <code>__ror__</code>。<code>__or__</code> 表示这个对象出现在<code>|</code> 左边时的处理，相应的 <code>__ror__</code> 表示这个对象出现在右边时的处理。</p><p>Python 在处理 <code>a | b</code> 这个表达式时，它会先尝试找 <code>a</code> 的 <code>__or__</code>，如果找不到，它会尝试找 <code>b</code> 的 <code>__ror__</code>。所以，在 context 的处理中， 来自标准库的 <code>itemgetter</code> 虽然没有实现<br>\n<code>__or__</code>，但 retriever 因为实现了 Runnable 接口，所以，它也实现了 <code>__ror__</code>。所以，这段代码才能组装出我们所需的链。</p><p>有了 context 变量，我们可以用它构建了另一个变量 first_step：</p><pre><code class=\"language-plain\">first_step = RunnablePassthrough.assign(context=context)\n</code></pre><p>还记得我们的提示词模板里有一个 context 变量吗？它就是从这里来的。</p><p>RunnablePassthrough.assign 这个函数就是在不改变链当前状态值的前提下，添加新的状态值。前面我们说了，这里赋给 context 变量的值是一个链，我们可以把它理解成一个函数，它会在运行期执行，其参数就是我们当前的状态值。现在你可以理解 itemgetter(“question”) 的参数是从哪来的了。这个函数的返回值会用来在当前的状态里添加一个叫 context 的变量，以便在后续使用。</p><p>其余的代码我们之前已经讲解过了，这里就不再赘述了。至此，我们拥有了一个可以运行的 RAG 应用，我们可以运行一下看看效果：</p><pre><code class=\"language-bash\">You:&gt; 专栏的作者是谁？\n专栏的作者是郑晔。\nYou:&gt; 作者还写过哪些专栏？\n作者郑晔还写过《10x程序员工作法》、《软件设计之美》、《代码之丑》和《程序员的测试课》这四个专栏。\n</code></pre><h2>总结时刻</h2><p>这一讲，我们自己动手实现了一个 RAG 应用，主要包括了索引和检索生成两个部分。</p><p>在 LangChain 中，有一些支持 RAG 应用的概念：</p><ul>\n<li>\n<p>文档（Document），表示各种信息。</p>\n</li>\n<li>\n<p>DocumentLoader，将文档加载进来。</p>\n</li>\n<li>\n<p>向量数据库（VectorStore），提供向量数据库的基础操作。</p>\n</li>\n<li>\n<p>Embedding，负责将文本向量化。</p>\n</li>\n<li>\n<p>TextSplitter，负责切分加载进来的文档。</p>\n</li>\n<li>\n<p>Retriever，负责检索文档，供生成使用。</p>\n</li>\n</ul><p>有了基础的概念支持，在实现 RAG 应用过程中，还需要写好提示词。在样例中，我提供了一个提示词，你也可以根据自己的需要调整提示词，以便更好地支持你的业务。</p><p>如果今天的内容你只能记住一件事，那请记住，<strong>LangChain 为 RAG 应用提供了基础的支持</strong>。</p><h2>练习题</h2><p>这一讲我们实现的 RAG 应用采用了我的开篇词作为原始信息，你可以尝试改造一下，把你的业务资料索引到向量数据库里。欢迎在留言区分享你的改造心得。</p>","comments":[{"had_liked":false,"id":395984,"user_name":"晴天了","can_delete":false,"product_type":"c1","uid":1359669,"ip_address":"北京","ucode":"5310E02F8DF1F0","user_header":"https://static001.geekbang.org/account/avatar/00/14/bf/35/0e3a92a7.jpg","comment_is_top":false,"comment_ctime":1732592945,"is_pvip":false,"replies":[{"id":143785,"content":"之所以选择 Chroma，就是因为它简单，指定目录就行，它就是操作文件，类似于 SQLite，不需要额外的安装。","user_name":"作者回复","user_name_real":"编辑","uid":1258861,"ctime":1732693133,"ip_address":"陕西","comment_id":395984,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"不需要先安装Chroma数据库吗? ","like_count":1,"discussions":[{"author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":654436,"discussion_content":"之所以选择 Chroma，就是因为它简单，指定目录就行，它就是操作文件，类似于 SQLite，不需要额外的安装。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1732693133,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":1222726,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a8/46/42c2e559.jpg","nickname":"煜寶","note":"","ucode":"F5A5BB5DAD4912","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":654696,"discussion_content":"还是有点天坑的啊老师…比较新的python比如3.9以上用这玩意儿必须要求本地的sqlite3版本是3.5以上，但是如果用的机器内置的版本较低的话就会有影响，安装了比较新的sqlite后新建的venv一直无法正常import到新的sqlite，重新构建编译python也不行…..","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1733273076,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":654436,"ip_address":"广东","group_id":0},"score":654696,"extra":""}]}]},{"had_liked":false,"id":396587,"user_name":"NierDaye","can_delete":false,"product_type":"c1","uid":1393961,"ip_address":"中国香港","ucode":"BC2E5739529CCE","user_header":"https://static001.geekbang.org/account/avatar/00/15/45/29/2478f7d0.jpg","comment_is_top":false,"comment_ctime":1735028099,"is_pvip":false,"replies":[{"id":143964,"content":"限制提示词长度是一个专门的工作，但一般来说，RAG不会太影响。","user_name":"作者回复","user_name_real":"编辑","uid":1258861,"ctime":1735130317,"ip_address":"陕西","comment_id":396587,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"经过Retriever加持的prompt，是否会容易超长？","like_count":0,"discussions":[{"author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":655519,"discussion_content":"限制提示词长度是一个专门的工作，但一般来说，RAG不会太影响。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1735130317,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":396227,"user_name":"lightwolf","can_delete":false,"product_type":"c1","uid":3229927,"ip_address":"上海","ucode":"9F4E0F742C174D","user_header":"https://static001.geekbang.org/account/avatar/00/31/48/e7/958b7e6c.jpg","comment_is_top":false,"comment_ctime":1733672540,"is_pvip":false,"replies":[{"id":143871,"content":"多谢补充","user_name":"作者回复","user_name_real":"编辑","uid":1258861,"ctime":1734078998,"ip_address":"陕西","comment_id":396227,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"关于embeddings的替换,可以参考这个文章: https:&#47;&#47;www.langchain.com.cn&#47;docs&#47;integrations&#47;vectorstores&#47;chroma&#47;","like_count":0,"discussions":[{"author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":655066,"discussion_content":"多谢补充","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1734078998,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395974,"user_name":"grok","can_delete":false,"product_type":"c1","uid":1341502,"ip_address":"美国","ucode":"4744AB3FA28FE2","user_header":"https://static001.geekbang.org/account/avatar/00/14/78/3e/f60ea472.jpg","comment_is_top":false,"comment_ctime":1732569324,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"本节代码在此：https:&#47;&#47;github.com&#47;groklab&#47;misc&#47;blob&#47;main&#47;geektime-llm-zhengye-column&#47;lec11.ipynb","like_count":2}]}