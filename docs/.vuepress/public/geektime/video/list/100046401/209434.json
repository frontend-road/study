{"id":209434,"title":"20 | Embedding简介：为什么Embedding更适合编码文本特征？","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/NLP\">https://gitee.com/geektime-geekbang/NLP</a></p>","comments":[{"had_liked":false,"id":189800,"user_name":". SWAgsWAGSwAGSWaG🐑","can_delete":false,"product_type":"c3","uid":1887020,"ip_address":"","ucode":"AF2A3AE7D1052E","user_header":"https://static001.geekbang.org/account/avatar/00/1c/cb/2c/02d98987.jpg","comment_is_top":false,"comment_ctime":1584552426,"is_pvip":false,"replies":[{"id":73124,"content":"一般来说是用已经训练好的embedding。","user_name":"作者回复","user_name_real":"王然","uid":1813673,"ctime":1584597203,"ip_address":"","comment_id":189800,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"老师，我对embedding的理解还是有些混淆。\nembedding可以理解为一个预训练的语言模型对吗？那我们具体应该怎么使用？\n按照之前自己在网上看的博客，我们的训练数据是不是都应该转换成embedding，才能输入模型进行训练呢？","like_count":2,"discussions":[{"author":{"id":1813673,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/ac/a9/fbcddd1f.jpg","nickname":"王然","note":"","ucode":"45A7F34120DF47","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":487798,"discussion_content":"一般来说是用已经训练好的embedding。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584597203,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":229526,"discussion_content":"word embedding，词嵌入，是词的一种向量化表示。好的词向量可以反映词义。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1586661194,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322161,"user_name":"EiLen🍖","can_delete":false,"product_type":"c3","uid":2760384,"ip_address":"","ucode":"998AADC238AACC","user_header":"https://static001.geekbang.org/account/avatar/00/2a/1e/c0/fa6afd6a.jpg","comment_is_top":false,"comment_ctime":1637211840,"is_pvip":false,"replies":[{"id":117741,"content":"我这里说的是数学本质。如果直接用one-hot后面接全连接数学一样。但是实现上就太差了～比如说你可以看看TF TPU当中的实现就是直接one-hot乘。建议GPU还是Embedding。","user_name":"作者回复","user_name_real":"编辑","uid":1813673,"ctime":1638448687,"ip_address":"","comment_id":322161,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"老师说的(直接在模型里训练embedding，对比直接one-hot没有任何优势)个人不是很认同，优势不就在于把one-hot的维度改变了方便后面的计算吗？😂","like_count":0,"discussions":[{"author":{"id":1813673,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/ac/a9/fbcddd1f.jpg","nickname":"王然","note":"","ucode":"45A7F34120DF47","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535469,"discussion_content":"我这里说的是数学本质。如果直接用one-hot后面接全连接数学一样。但是实现上就太差了～比如说你可以看看TF TPU当中的实现就是直接one-hot乘。建议GPU还是Embedding。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638448687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":258416,"user_name":"測試中……","can_delete":false,"product_type":"c3","uid":1187359,"ip_address":"","ucode":"E7E1B46CB6D8B1","user_header":"https://static001.geekbang.org/account/avatar/00/12/1e/1f/41e35a24.jpg","comment_is_top":false,"comment_ctime":1604418291,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"关于「词向量」补充一个视频（是同一个视频）：\n- bilibili：https:&#47;&#47;www.bilibili.com&#47;video&#47;BV19541157pe?from=search&amp;seid=13143938887158668822\n- YouTube：https:&#47;&#47;www.youtube.com&#47;watch?v=2fhChTLQDfY","like_count":7},{"had_liked":false,"id":258417,"user_name":"測試中……","can_delete":false,"product_type":"c3","uid":1187359,"ip_address":"","ucode":"E7E1B46CB6D8B1","user_header":"https://static001.geekbang.org/account/avatar/00/12/1e/1f/41e35a24.jpg","comment_is_top":false,"comment_ctime":1604418591,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"把「词」转化为「词向量」之后投到一个空间当中，这样计算机对于「词」的理解，就转变为计算机对于空间以及距离的理解。","like_count":3},{"had_liked":false,"id":186899,"user_name":"宋不肥","can_delete":false,"product_type":"c3","uid":1240126,"ip_address":"","ucode":"32B34AF579C91C","user_header":"https://static001.geekbang.org/account/avatar/00/12/ec/3e/885ec1d2.jpg","comment_is_top":false,"comment_ctime":1583949137,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"老师，这节课听的有点迷糊。对于“映射成数组是连续的”和“根据周围的词来推断它”这两句话感觉理解起来会有一点模糊。上学期之前我在实验室的任务是做遥感高光谱图像的降维，主要是用到了流形学习和图嵌入思想，感觉和nlp里面的嵌入有点类似。所以猜测一下这两句话的意思，通过编码把词转化为数字或映射为数组，也可视为高维空间中的一个个点，所以说是连续的。而通过周围的词来推断，即就是一个句子当高维中多个坐标值相同的时候，他们本身大概率也是非常靠近的点。同样高维空间中点的距离，也可以用来度量词之间的关系（相关度）。之前一直用的传统方法，刚转深度学习，nlp也是第一次接触。不知道理解的对不对，希望老师能指点一下。","like_count":2,"discussions":[{"author":{"id":1224793,"avatar":"https://static001.geekbang.org/account/avatar/00/12/b0/59/e46bc734.jpg","nickname":"水里捞鱼","note":"","ucode":"01AA841F3187BB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":604598,"discussion_content":"这门课其实不太适合作为“全部知识”去看的，可以根据主题再去找相关资料做补充，你稍微搜一下word embedding的解读文章，尤其是历史沿袭关系就很清楚了，比如关键词「distributed representation」","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1676374981,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":186870,"user_name":"人工智能混饭人","can_delete":false,"product_type":"c3","uid":1104354,"ip_address":"","ucode":"71CF953311BD81","user_header":"https://static001.geekbang.org/account/avatar/00/10/d9/e2/d75009ef.jpg","comment_is_top":false,"comment_ctime":1583941778,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"embedding让不同的词之间可以计算相似度","like_count":1},{"had_liked":false,"id":313417,"user_name":"sky","can_delete":false,"product_type":"c3","uid":2325199,"ip_address":"","ucode":"C7BA135845E1FF","user_header":"https://static001.geekbang.org/account/avatar/00/23/7a/cf/c42dd74e.jpg","comment_is_top":false,"comment_ctime":1632413433,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"预训练语言模型改变nlp 游戏规则，在实际使用中，确实如此。个人感觉，预训练模型的提出，不仅是采用了embedding 的思想，更重要的是复用了训练模型的架构","like_count":0},{"had_liked":false,"id":282891,"user_name":"大仙","can_delete":false,"product_type":"c3","uid":2439083,"ip_address":"","ucode":"A0AAA34D72D6BF","user_header":"https://static001.geekbang.org/account/avatar/00/25/37/ab/49e2d798.jpg","comment_is_top":false,"comment_ctime":1615452721,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"        # class torch.nn.Embedding(\n        # num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)\n        # 一个保存了固定字典和大小的简单查找表。\n        # num_embeddings (int) - 嵌入字典的大小\n        # embedding_dim (int) - 每个嵌入向量的大小\n        # padding_idx (int, optional) - 如果提供的话，输出遇到此下标时用零填充\n        # max_norm (float, optional) - 如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值\n        # norm_type (float, optional) - 对于max_norm选项计算p范数时的p\n        # scale_grad_by_freq (boolean, optional) - 如果提供的话，会根据字典中单词频率缩放梯度\n\n# 第一个参数是三。意味着就只能有三种数字。而我们tensor中包含的数字超过三个会报错。自行修改第一个参数观察embedding1 输出。你会发现对于tensor中相同的数字会被同一组值来表示\nembedding = torch.nn.Embedding(3, 3, padding_idx=0)\ntensor = torch.LongTensor([[2, 3, 5, 6, 7], [7, 5, 3, 8, 3]])\nembedding1 = embedding(tensor)\nprint(embedding1)\n","like_count":0}]}