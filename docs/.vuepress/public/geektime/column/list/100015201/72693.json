{"id":72693,"title":"49 | Custom Metrics: 让Auto Scaling不再“食之无味”","content":"<p>你好，我是张磊。今天我和你分享的主题是：Custom Metrics，让Auto Scaling不再“食之无味”。</p><p>在上一篇文章中，我为你详细讲述了 Kubernetes 里的核心监控体系的架构。不难看到，Prometheus 项目在其中占据了最为核心的位置。</p><p>实际上，借助上述监控体系，Kubernetes 就可以为你提供一种非常有用的能力，那就是 Custom Metrics，自定义监控指标。</p><p>在过去的很多 PaaS 项目中，其实都有一种叫作 Auto Scaling，即自动水平扩展的功能。只不过，这个功能往往只能依据某种指定的资源类型执行水平扩展，比如 CPU 或者 Memory 的使用值。</p><p>而在真实的场景中，用户需要进行Auto Scaling 的依据往往是自定义的监控指标。比如，某个应用的等待队列的长度，或者某种应用相关资源的使用情况。这些复杂多变的需求，在传统 PaaS项目和其他容器编排项目里，几乎是不可能轻松支持的。</p><p>而凭借强大的 API 扩展机制，Custom Metrics已经成为了 Kubernetes 的一项标准能力。并且，Kubernetes 的自动扩展器组件 Horizontal Pod Autoscaler （HPA）， 也可以直接使用Custom Metrics来执行用户指定的扩展策略，这里的整个过程都是非常灵活和可定制的。</p><!-- [[[read_end]]] --><p>不难想到，Kubernetes 里的 Custom Metrics 机制，也是借助Aggregator APIServer 扩展机制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes 里就会出现一个叫作<code>custom.metrics.k8s.io</code>的 API。而当你访问这个 URL 时，Aggregator就会把你的请求转发给Custom Metrics APIServer 。</p><p>而Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor。</p><p>比如，现在我们要实现一个根据指定 Pod 收到的 HTTP 请求数量来进行 Auto Scaling 的 Custom Metrics，这个 Metrics 就可以通过访问如下所示的自定义监控 URL 获取到：</p><pre><code>https://&lt;apiserver_ip&gt;/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/pods/sample-metrics-app/http_requests \n</code></pre><p>这里的工作原理是，当你访问这个 URL 的时候，Custom Metrics APIServer就会去 Prometheus 里查询名叫sample-metrics-app这个Pod 的http_requests指标的值，然后按照固定的格式返回给访问者。</p><p>当然，http_requests指标的值，就需要由 Prometheus 按照我在上一篇文章中讲到的核心监控体系，从目标 Pod 上采集。</p><p>这里具体的做法有很多种，最普遍的做法，就是让 Pod 里的应用本身暴露出一个/metrics API，然后在这个 API 里返回自己收到的HTTP的请求的数量。所以说，接下来 HPA 只需要定时访问前面提到的自定义监控 URL，然后根据这些值计算是否要执行 Scaling 即可。</p><p>接下来，我通过一个具体的实例，来为你讲解一下 Custom Metrics 具体的使用方式。这个实例的 GitHub 库<a href=\"https://github.com/resouer/kubeadm-workshop\">在这里</a>，你可以点击链接查看。在这个例子中，我依然会假设你的集群是 kubeadm 部署出来的，所以 Aggregator 功能已经默认开启了。</p><blockquote>\n<p>备注：我们这里使用的实例，fork 自 Lucas 在上高中时做的一系列Kubernetes 指南。</p>\n</blockquote><p><strong>首先</strong>，我们当然是先部署 Prometheus 项目。这一步，我当然会使用 Prometheus Operator来完成，如下所示：</p><pre><code>$ kubectl apply -f demos/monitoring/prometheus-operator.yaml\nclusterrole &quot;prometheus-operator&quot; created\nserviceaccount &quot;prometheus-operator&quot; created\nclusterrolebinding &quot;prometheus-operator&quot; created\ndeployment &quot;prometheus-operator&quot; created\n\n$ kubectl apply -f demos/monitoring/sample-prometheus-instance.yaml\nclusterrole &quot;prometheus&quot; created\nserviceaccount &quot;prometheus&quot; created\nclusterrolebinding &quot;prometheus&quot; created\nprometheus &quot;sample-metrics-prom&quot; created\nservice &quot;sample-metrics-prom&quot; created\n</code></pre><p><strong>第二步</strong>，我们需要把 Custom Metrics APIServer 部署起来，如下所示：</p><pre><code>$ kubectl apply -f demos/monitoring/custom-metrics.yaml\nnamespace &quot;custom-metrics&quot; created\nserviceaccount &quot;custom-metrics-apiserver&quot; created\nclusterrolebinding &quot;custom-metrics:system:auth-delegator&quot; created\nrolebinding &quot;custom-metrics-auth-reader&quot; created\nclusterrole &quot;custom-metrics-read&quot; created\nclusterrolebinding &quot;custom-metrics-read&quot; created\ndeployment &quot;custom-metrics-apiserver&quot; created\nservice &quot;api&quot; created\napiservice &quot;v1beta1.custom-metrics.metrics.k8s.io&quot; created\nclusterrole &quot;custom-metrics-server-resources&quot; created\nclusterrolebinding &quot;hpa-controller-custom-metrics&quot; created\n</code></pre><p><strong>第三步</strong>，我们需要为 Custom Metrics APIServer 创建对应的 ClusterRoleBinding，以便能够使用curl来直接访问 Custom Metrics 的 API：</p><pre><code>$ kubectl create clusterrolebinding allowall-cm --clusterrole custom-metrics-server-resources --user system:anonymous\nclusterrolebinding &quot;allowall-cm&quot; created\n</code></pre><p><strong>第四步</strong>，我们就可以把待监控的应用和 HPA 部署起来了，如下所示：</p><pre><code>$ kubectl apply -f demos/monitoring/sample-metrics-app.yaml\ndeployment &quot;sample-metrics-app&quot; created\nservice &quot;sample-metrics-app&quot; created\nservicemonitor &quot;sample-metrics-app&quot; created\nhorizontalpodautoscaler &quot;sample-metrics-app-hpa&quot; created\ningress &quot;sample-metrics-app&quot; created\n</code></pre><p>这里，我们需要关注一下 HPA 的配置，如下所示：</p><pre><code>kind: HorizontalPodAutoscaler\napiVersion: autoscaling/v2beta1\nmetadata:\n  name: sample-metrics-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: sample-metrics-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Object\n    object:\n      target:\n        kind: Service\n        name: sample-metrics-app\n      metricName: http_requests\n      targetValue: 100\n</code></pre><p>可以看到，<strong>HPA 的配置，就是你设置 Auto Scaling 规则的地方。</strong></p><p>比如，scaleTargetRef字段，就指定了被监控的对象是名叫sample-metrics-app的 Deployment，也就是我们上面部署的被监控应用。并且，它最小的实例数目是2，最大是10。</p><p>在metrics字段，我们指定了这个 HPA 进行 Scale 的依据，是名叫http_requests的 Metrics。而获取这个 Metrics 的途径，则是访问名叫sample-metrics-app的 Service。</p><p>有了这些字段里的定义， HPA 就可以向如下所示的 URL 发起请求来获取 Custom Metrics 的值了：</p><pre><code>https://&lt;apiserver_ip&gt;/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests\n</code></pre><p>需要注意的是，上述这个 URL 对应的被监控对象，是我们的应用对应的 Service。这跟本文一开始举例用到的 Pod 对应的 Custom Metrics URL 是不一样的。当然，<strong>对于一个多实例应用来说，通过 Service 来采集 Pod 的 Custom Metrics 其实才是合理的做法。</strong></p><p>这时候，我们可以通过一个名叫hey的测试工具来为我们的应用增加一些访问压力，具体做法如下所示：</p><pre><code>$ # Install hey\n$ docker run -it -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey\n\n$ export APP_ENDPOINT=$(kubectl get svc sample-metrics-app -o template --template {{.spec.clusterIP}}); echo ${APP_ENDPOINT}\n$ hey -n 50000 -c 1000 http://${APP_ENDPOINT}\n</code></pre><p>与此同时，如果你去访问应用 Service 的 Custom Metircs URL，就会看到这个 URL 已经可以为你返回应用收到的 HTTP 请求数量了，如下所示：</p><pre><code>$ curl -sSLk https://&lt;apiserver_ip&gt;/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests\n{\n  &quot;kind&quot;: &quot;MetricValueList&quot;,\n  &quot;apiVersion&quot;: &quot;custom-metrics.metrics.k8s.io/v1beta1&quot;,\n  &quot;metadata&quot;: {\n    &quot;selfLink&quot;: &quot;/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests&quot;\n  },\n  &quot;items&quot;: [\n    {\n      &quot;describedObject&quot;: {\n        &quot;kind&quot;: &quot;Service&quot;,\n        &quot;name&quot;: &quot;sample-metrics-app&quot;,\n        &quot;apiVersion&quot;: &quot;/__internal&quot;\n      },\n      &quot;metricName&quot;: &quot;http_requests&quot;,\n      &quot;timestamp&quot;: &quot;2018-11-30T20:56:34Z&quot;,\n      &quot;value&quot;: &quot;501484m&quot;\n    }\n  ]\n}\n</code></pre><p><strong>这里需要注意的是，Custom Metrics API 为你返回的 Value 的格式。</strong></p><p>在为被监控应用编写/metrics API 的返回值时，我们其实比较容易计算的，是该 Pod 收到的 HTTP request 的总数。所以，我们这个<a href=\"https://github.com/resouer/kubeadm-workshop/blob/master/images/autoscaling/server.js\">应用的代码</a>其实是如下所示的样子：</p><pre><code>  if (request.url == &quot;/metrics&quot;) {\n    response.end(&quot;# HELP http_requests_total The amount of requests served by the server in total\\n# TYPE http_requests_total counter\\nhttp_requests_total &quot; + totalrequests + &quot;\\n&quot;);\n    return;\n  }\n</code></pre><p>可以看到，我们的应用在/metrics 对应的 HTTP response 里返回的，其实是http_requests_total的值。这，也就是 Prometheus 收集到的值。</p><p>而 Custom Metrics APIServer 在收到对http_requests指标的访问请求之后，它会从Prometheus 里查询http_requests_total的值，然后把它折算成一个以时间为单位的请求率，最后把这个结果作为http_requests指标对应的值返回回去。</p><p>所以说，我们在对前面的 Custom Metircs URL 进行访问时，会看到值是501484m，这里的格式，其实就是milli-requests，相当于是在过去两分钟内，每秒有501个请求。这样，应用的开发者就无需关心如何计算每秒的请求个数了。而这样的“请求率”的格式，是可以直接被 HPA 拿来使用的。</p><p>这时候，如果你同时查看 Pod 的个数的话，就会看到 HPA 开始增加 Pod 的数目了。</p><p>不过，在这里你可能会有一个疑问，Prometheus 项目，又是如何知道采集哪些 Pod 的 /metrics API 作为监控指标的来源呢。</p><p>实际上，如果仔细观察一下我们前面创建应用的输出，你会看到有一个类型是ServiceMonitor的对象也被创建了出来。它的 YAML 文件如下所示：</p><pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: sample-metrics-app\n  labels:\n    service-monitor: sample-metrics-app\nspec:\n  selector:\n    matchLabels:\n      app: sample-metrics-app\n  endpoints:\n  - port: web\n</code></pre><p>这个ServiceMonitor对象，正是 Prometheus Operator 项目用来指定被监控 Pod 的一个配置文件。可以看到，我其实是通过Label Selector 为Prometheus 来指定被监控应用的。</p><h1>总结</h1><p>在本篇文章中，我为你详细讲解了 Kubernetes 里对自定义监控指标，即 Custom Metrics 的设计与实现机制。这套机制的可扩展性非常强，也终于使得Auto Scaling 在 Kubernetes 里面不再是一个“食之无味”的鸡肋功能了。</p><p>另外可以看到，Kubernetes 的 Aggregator APIServer，是一个非常行之有效的 API 扩展机制。而且，Kubernetes 社区已经为你提供了一套叫作 <a href=\"https://github.com/kubernetes-sigs/kubebuilder\">KubeBuilder</a> 的工具库，帮助你生成一个 API Server 的完整代码框架，你只需要在里面添加自定义 API，以及对应的业务逻辑即可。</p><h1>思考题</h1><p>在你的业务场景中，你希望使用什么样的指标作为 Custom Metrics ，以便对 Pod 进行 Auto Scaling 呢？怎么获取到这个指标呢？</p><p>感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。</p><p><img src=\"https://static001.geekbang.org/resource/image/96/25/96ef8576a26f5e6266c422c0d6519725.jpg?wh=1110*659\" alt=\"\"></p>","neighbors":{"left":{"article_title":"48 | Prometheus、Metrics Server与Kubernetes监控体系","id":72281},"right":{"article_title":"50 | 让日志无处可逃：容器日志收集与管理","id":73156}},"comments":[{"had_liked":false,"id":50322,"user_name":"虎虎❤️","can_delete":false,"product_type":"c1","uid":1086535,"ip_address":"","ucode":"157F261E80291A","user_header":"https://static001.geekbang.org/account/avatar/00/10/94/47/75875257.jpg","comment_is_top":false,"comment_ctime":1544952678,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"74559396710","product_id":100015201,"comment_content":"HPA 通过 HorizontalPodAutoscaler 配置要访问的 Custom Metrics, 来决定如何scale。<br>Custom Metric APIServer 的实现其实是一个Prometheus 的Adaptor，会去Prometheus中读取某个Pod&#47;Servicce的具体指标值。比如，http request的请求率。<br>Prometheus 通过 ServiceMonitor object 配置需要监控的pod和endpoints，来确定监控哪些pod的metrics。<br>应用需要实现&#47;metrics， 来响应Prometheus的数据采集请求。<br><br>留给自己的思考，Pod 的 metrics endpoint 如何对应到http_requests 这个指标的？<br>","like_count":18},{"had_liked":false,"id":220993,"user_name":"单朋荣","can_delete":false,"product_type":"c1","uid":1272662,"ip_address":"","ucode":"8AD121BEDD9675","user_header":"https://static001.geekbang.org/account/avatar/00/13/6b/56/37a4cea7.jpg","comment_is_top":false,"comment_ctime":1590374086,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"61719916230","product_id":100015201,"comment_content":"张老师，伸缩是不是写的简单了点，毕竟伸缩也是核心功能之一。主要想了解下，hpa伸缩的判定算法的替换方法，vpa预测算法的常见类型及优缺点，ca在大规模场景下实现的原理，ar的应用场景等；还有就是自定义伸缩（prometheus)时间戳优化原理、方法，及最后微服务下伸缩的结合方法等。可不可以从伸缩的数据分类、获取手段、处理算法、伸缩流程、调度算法、故障处理反馈机制等方面，再列几章讲一下，谢谢张老师！！","like_count":13,"discussions":[{"author":{"id":1269156,"avatar":"https://static001.geekbang.org/account/avatar/00/13/5d/a4/2e4b6d33.jpg","nickname":"素还真","note":"","ucode":"9ABA456355F7E3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":347881,"discussion_content":"我也希望，你懂得不少了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612350916,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":53378,"user_name":"William","can_delete":false,"product_type":"c1","uid":1241365,"ip_address":"","ucode":"4499F99B180102","user_header":"https://static001.geekbang.org/account/avatar/00/12/f1/15/8fcf8038.jpg","comment_is_top":false,"comment_ctime":1545635314,"is_pvip":true,"discussion_count":1,"race_medal":2,"score":"27315439090","product_id":100015201,"comment_content":"请问能否实现跨node的水平扩展?","like_count":6,"discussions":[{"author":{"id":1198816,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4a/e0/eff34583.jpg","nickname":"马超","note":"","ucode":"FAFF55DC5DD15C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":200144,"discussion_content":"cluster-autoscaler","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1583661027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":49591,"user_name":"DJH","can_delete":false,"product_type":"c1","uid":1256740,"ip_address":"","ucode":"2BDEF123B3DB6A","user_header":"https://static001.geekbang.org/account/avatar/00/13/2d/24/28acca15.jpg","comment_is_top":false,"comment_ctime":1544743045,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"18724612229","product_id":100015201,"comment_content":"请教一个问题：对于多POD的应用（如多副本的deployment），假设配置了根据CPU使用率进行自动水平伸缩（HPA），那么HPA执行水平伸缩的依据是各个POD中CPU使用率平均值还是最高值？另外HPA探测到多少次CPU高于设置值才会开始伸缩？CPU使用率探测的频率又是多久一次呢？","like_count":5,"discussions":[{"author":{"id":1198816,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4a/e0/eff34583.jpg","nickname":"马超","note":"","ucode":"FAFF55DC5DD15C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":200142,"discussion_content":"平均值，后面可以根据参数自己配置，这些都可以在K8S的官方文档HPA里有说明","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583660942,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":56791,"user_name":"剑走偏锋","can_delete":false,"product_type":"c1","uid":1095155,"ip_address":"","ucode":"A1A63BE0E3D08F","user_header":"https://static001.geekbang.org/account/avatar/00/10/b5/f3/8efcc2dd.jpg","comment_is_top":false,"comment_ctime":1546562167,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10136496759","product_id":100015201,"comment_content":"就为了做自定义业务指标的监控，我们也做了水晶桥(Crystal Bridge)项目开源在github上了。思路是自采通过annotations公开的promethus指标，然后推往prometheus GW，最后再由上层prometheus来采集。<br><br>今天这种让HPA通过自定义指标来完成扩容&#47;缩容操作的技术设计的确很棒，学习了，感谢。","like_count":3},{"had_liked":false,"id":162875,"user_name":"manatee","can_delete":false,"product_type":"c1","uid":1041112,"ip_address":"","ucode":"708D90E7A265BD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e2/d8/f0562ede.jpg","comment_is_top":false,"comment_ctime":1576627850,"is_pvip":false,"discussion_count":4,"race_medal":0,"score":"5871595146","product_id":100015201,"comment_content":"请问下老师，扩容好理解就是加容器，那缩容的话如何实现呢，怎么保证在删除容器的时候容器上的请求不受影响呢","like_count":1,"discussions":[{"author":{"id":1072245,"avatar":"https://static001.geekbang.org/account/avatar/00/10/5c/75/e831637c.jpg","nickname":"高阳","note":"","ucode":"7CCB7C0F7C9F3C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":348019,"discussion_content":"pod开始删除的时候，首先是sevice不再把流量负载到这个pod上，再执行pod的删除，这个过程由service控制","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1612405276,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1494505,"avatar":"https://static001.geekbang.org/account/avatar/00/16/cd/e9/bd4f997f.jpg","nickname":"放牛的程序猿","note":"","ucode":"4A5519623D3C5D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":265562,"discussion_content":"我觉得应用程序本身要支持grace down 机制","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1589417903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1443145,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eozpyAUaM6ra1hqeIsd4v0fulS4zVmxDM3LQyqGo0BFM141QpQnSib6oHdQricGrRxusp5rflGn54ew/132","nickname":"甜宝仙女的专属饲养员","note":"","ucode":"6D2039BA22551F","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556885,"discussion_content":"跟优雅停机一样吧，先断掉外来请求，再缓冲一段时间，也是尽可能保证所有的请求都处理完。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647566016,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1198816,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4a/e0/eff34583.jpg","nickname":"马超","note":"","ucode":"FAFF55DC5DD15C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":200143,"discussion_content":"有health check","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583660987,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":52151,"user_name":"stan","can_delete":false,"product_type":"c1","uid":1062266,"ip_address":"","ucode":"B48CDD63645B84","user_header":"https://static001.geekbang.org/account/avatar/00/10/35/7a/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1545320491,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5840287787","product_id":100015201,"comment_content":"请张老师帮忙解惑，对于多实例应用，采集 service暴露的指标才是正确的做法，这句怎么理解？采集每个pod对应的指标不好吗，service后面对应的api无法确认来自哪个pod吧？数据可能忽大忽小，如果采集到一个刚刚hpa的pod指标，数据可能更小，这样应该没有采集每个pod，然后平均值来的更精确吧？类似对于cpu 的hpa，就是采集的每个pod的指标然后做平均值","like_count":1,"discussions":[{"author":{"id":1007970,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/61/62/72296b09.jpg","nickname":"小雨","note":"","ucode":"A6332734063CC5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":211958,"discussion_content":"其实你实现下 Metrics 就明白了，暴露指标时也有域的概念。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584908635,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":346705,"user_name":"源子陌","can_delete":false,"product_type":"c1","uid":1943320,"ip_address":"","ucode":"20B47109705F8C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIpF5euTNx3GJRv2UprQHohnD9DStzjwgdFUA3x2w9o0OXldQZC1VWYuLPnVqM54XBib1G3c0AEpJA/132","comment_is_top":false,"comment_ctime":1653371848,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1653371848","product_id":100015201,"comment_content":"“所以说，我们在对前面的 Custom Metircs URL 进行访问时，会看到值是 501484m，这里的格式，其实就是 milli-requests，相当于是在过去两分钟内，每秒有 501 个请求。”——这个 501484m 是怎么折算成 501 QPS的，老师能再解释下嘛？","like_count":0},{"had_liked":false,"id":320448,"user_name":"陈斯佳","can_delete":false,"product_type":"c1","uid":1259323,"ip_address":"","ucode":"C236F874FC767A","user_header":"https://static001.geekbang.org/account/avatar/00/13/37/3b/495e2ce6.jpg","comment_is_top":false,"comment_ctime":1636336105,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1636336105","product_id":100015201,"comment_content":"第四十七课:Custom Metrics: 让Auto Scaling不再“食之无味”<br><br>很多 PaaS 项目中的Auto Scaling，即自动水平扩展的功能，只能依据某种指定的资源类型执行水平扩展，比如 CPU 或者 Memory 的使用值。凭借强大的 API 扩展机制，Custom Metrics 已经成为了 Kubernetes 的一项标准能力。并且，Kubernetes 的自动扩展器组件 Horizontal Pod Autoscaler （HPA）， 也可以直接使用 Custom Metrics 来执行用户指定的扩展策略，这里的整个过程都是非常灵活和可定制的。Kubernetes 里的 Custom Metrics 机制，也是借助 Aggregator APIServer 扩展机制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes 里就会出现一个叫作custom.metrics.k8s.io的 API。而当你访问这个 URL 时，Aggregator 就会把你的请求转发给 Custom Metrics APIServer 。而 Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor。","like_count":0},{"had_liked":false,"id":295695,"user_name":"罗峰","can_delete":false,"product_type":"c1","uid":1218501,"ip_address":"","ucode":"5F3D6AF8F28322","user_header":"https://static001.geekbang.org/account/avatar/00/12/97/c5/84491beb.jpg","comment_is_top":false,"comment_ctime":1622547090,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1622547090","product_id":100015201,"comment_content":"Hpa里面配置了对哪些deploy等进行水平伸缩、伸缩的范围，对应的指标的阈值。<br>指标是普罗主动拉取的，需要个servicemonitor告诉普罗哪个指标拉取哪个pod。<br>这样指标有了，需扩展的k8资源确定了，ha功能也就可以了。","like_count":0},{"had_liked":false,"id":289824,"user_name":"饮水","can_delete":false,"product_type":"c1","uid":2300785,"ip_address":"","ucode":"B8CA8D0A90310F","user_header":"https://static001.geekbang.org/account/avatar/00/23/1b/71/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1619191366,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1619191366","product_id":100015201,"comment_content":"你好，老师，KubeBuilder还能用来做api么，那这个api和operator有啥区别？","like_count":0},{"had_liked":false,"id":239897,"user_name":"小白","can_delete":false,"product_type":"c1","uid":1199969,"ip_address":"","ucode":"7ACE14C0C4AE61","user_header":"https://static001.geekbang.org/account/avatar/00/12/4f/61/00083e41.jpg","comment_is_top":false,"comment_ctime":1596687117,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1596687117","product_id":100015201,"comment_content":"HPA 定义时并没看到是访问custom.metrics.k8s.io接口的，Aggregator 怎么知道应该调用custom metrics API server?","like_count":0,"discussions":[{"author":{"id":1580926,"avatar":"","nickname":"ch_ort","note":"","ucode":"B79746E687F29E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":338444,"discussion_content":"sample-metrics-app","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609292434,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":225364,"user_name":"Mars","can_delete":false,"product_type":"c1","uid":1088584,"ip_address":"","ucode":"D7BAA9EBB7D89C","user_header":"https://static001.geekbang.org/account/avatar/00/10/9c/48/44965714.jpg","comment_is_top":false,"comment_ctime":1591743263,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591743263","product_id":100015201,"comment_content":"请教下老师 为什么knative要自己弄一个autoscaler。既然k8s已经有了autoscaler了，感觉没啥必要再造一个轮子。","like_count":0},{"had_liked":false,"id":143564,"user_name":"单朋荣","can_delete":false,"product_type":"c1","uid":1272662,"ip_address":"","ucode":"8AD121BEDD9675","user_header":"https://static001.geekbang.org/account/avatar/00/13/6b/56/37a4cea7.jpg","comment_is_top":false,"comment_ctime":1571732548,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1571732548","product_id":100015201,"comment_content":"Warning  FailedGetObjectMetric         1m (x13 over 7m)  horizontal-pod-autoscaler  unable to get metric http_requests: Service on default sample-metrics-app&#47;unable to fetch metrics from custom metrics API: the server could not find the metric http_requests for services<br>  Warning  FailedComputeMetricsReplicas  1m (x13 over 7m)  horizontal-pod-autoscaler  failed to get object metric value: unable to get metric http_requests: Service on default sample-metrics-app&#47;unable to fetch metrics from custom metrics API: the server could not find the metric http_requests for services<br>遇到一个问题，求解决思路。。<br>","like_count":0},{"had_liked":false,"id":121291,"user_name":"yzw","can_delete":false,"product_type":"c1","uid":1621418,"ip_address":"","ucode":"CE4A105807BE6A","user_header":"https://static001.geekbang.org/account/avatar/00/18/bd/aa/fa3ebfe9.jpg","comment_is_top":false,"comment_ctime":1565095015,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565095015","product_id":100015201,"comment_content":"老师，对于没有证书的kubernetes集群，修改prometheus的什么参数可以保证访问采用的是不安全方式呢？我的kubernetes集群是v1.11.2，prometheus是kube-prometheus:v0.1.0，谢谢解答","like_count":0},{"had_liked":false,"id":105623,"user_name":"suke","can_delete":false,"product_type":"c1","uid":1007753,"ip_address":"","ucode":"C0287C31A4F45B","user_header":"","comment_is_top":false,"comment_ctime":1561043864,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1561043864","product_id":100015201,"comment_content":"老师 我在自己的集群上实验了一下http_requests的监控，servicemonitor和相关的hpa，以及相关的权限绑定都部署了，pod里也实现了 &#47;meteics 接口 ，但是hpa的在线配置里提示service on xx xxxx&#47;object metrics are not yet supported，请问您大概知道我因为什么才导致的这个问题么，网上也没查到相关的解释","like_count":0},{"had_liked":false,"id":49911,"user_name":"DJH","can_delete":false,"product_type":"c1","uid":1256740,"ip_address":"","ucode":"2BDEF123B3DB6A","user_header":"https://static001.geekbang.org/account/avatar/00/13/2d/24/28acca15.jpg","comment_is_top":false,"comment_ctime":1544787878,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1544787878","product_id":100015201,"comment_content":"还有个问题请教一下，PVC属性里的读写属性ReadWriteMany指的是多个pod之间可以同时读写？同一个pod的多个容器之间算同时读写吗？","like_count":0,"discussions":[{"author":{"id":1834149,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEJnguccGyM8nvOb0N9B0kCnT9ennz1UmmWeKMhXAKUzTwvNoFC3QcPBmT7CCCujlvacXPHJgdVhBA/132","nickname":"小人物也有大梦想","note":"","ucode":"4BDDEE94107719","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":273014,"discussion_content":"同一个pod的容器是共享存储卷的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590392618,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":49640,"user_name":"vx:jiancheng_goon","can_delete":false,"product_type":"c1","uid":1218615,"ip_address":"","ucode":"FD34901B061825","user_header":"https://static001.geekbang.org/account/avatar/00/12/98/37/7f575aec.jpg","comment_is_top":false,"comment_ctime":1544749018,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1544749018","product_id":100015201,"comment_content":"请教一个问题，如何才能保证pod原地重启。不论是升级还是断电。","like_count":0,"discussions":[{"author":{"id":1272662,"avatar":"https://static001.geekbang.org/account/avatar/00/13/6b/56/37a4cea7.jpg","nickname":"单朋荣","note":"","ucode":"8AD121BEDD9675","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":272912,"discussion_content":"阿里巴巴有个原地重启项目-kurise项目。原地升级能力的实现，应该是借助的docker-shim和docker-rumtime？（记不清是不是这个了）解耦和，及可以暂时被接管的能力。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590374659,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1078160,"avatar":"https://static001.geekbang.org/account/avatar/00/10/73/90/9118f46d.jpg","nickname":"chenhz","note":"","ucode":"485420EA3282D4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":158732,"discussion_content":"可以考虑使用StatefulSet","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580618695,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}