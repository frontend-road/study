{"id":644795,"title":"09｜语义检索，利用Embedding优化你的搜索功能","content":"<p>你好，我是徐文浩。</p><p>在过去的8讲里面，相信你已经对Embedding和Completion接口非常熟悉了。Embedding向量适合作为一个中间结果，用于传统的机器学习场景，比如分类、聚类。而Completion接口，一方面可以直接拿来作为一个聊天机器人，另一方面，你只要善用提示词，就能完成合理的文案撰写、文本摘要、机器翻译等一系列的工作。</p><p>不过，很多同学可能会说，这个和我的日常工作又没有什么关系。的确，日常我们的需求里面，最常使用自然语言处理（NLP）技术的，是搜索、广告、推荐这样的业务。那么，今天我们就来看看，怎么利用OpenAI提供的接口来为这些需求提供些帮助。</p><h2>让AI生成实验数据</h2><p>在实际演示代码之前，我们需要一些可以拿来实验的数据。之前，我们都是在网上找一些数据集，或者直接使用一些机器学习软件包里面自带的数据集。但是，并不是所有时候我们都能很快找到合适的数据集。这个时候，我们也可以利用AI，我们直接让AI帮我们生成一些数据不就好了吗？</p><pre><code class=\"language-python\">import openai, os\n\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\nCOMPLETION_MODEL = \"text-davinci-003\"\n\ndef generate_data_by_prompt(prompt):\n    response = openai.Completion.create(\n        engine=COMPLETION_MODEL,\n        prompt=prompt,\n        temperature=0.5,\n        max_tokens=2048,\n        top_p=1,\n    )\n    return response.choices[0].text\n\nprompt = \"\"\"请你生成50条淘宝网里的商品的标题，每条在30个字左右，品类是3C数码产品，标题里往往也会有一些促销类的信息，每行一条。\"\"\"\ndata = generate_data_by_prompt(prompt)\n</code></pre><!-- [[[read_end]]] --><p>为了让数据和真实情况更加接近一点，我们可以好好设计一下我们的提示语。比如，我这里就指明了是淘宝的商品，品类是3C，并且标题里要包含一些促销信息。</p><p>我们把拿到的返回结果，按行分割，加载到一个DataFrame里面，看看结果会是怎么样的。</p><pre><code class=\"language-python\">import pandas as pd\n\nproduct_names = data.strip().split('\\n')\ndf = pd.DataFrame({'product_name': product_names})\ndf.head()\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">    product_name\n0\t1. 【限时特惠】Apple/苹果 iPhone 11 Pro Max 手机\n1\t2. 【超值折扣】Huawei/华为 P30 Pro 智能手机\n2\t3. 【热销爆款】OPPO Reno 10X Zoom 全网通手机\n3\t4. 【限量特价】Xiaomi/小米 9 Pro 5G 手机\n4\t5. 【限时促销】Apple/苹果 iPad Pro 11寸 平板\n</code></pre><p>可以看到，AI为我们生成了50条商品信息，并且每一个都带上了一些促销相关的标签。不过，在返回的结果里面，每一行都带上了一个标号，所以我们需要简单处理一下，去掉这个标号拿到一些干净的数据。</p><pre><code class=\"language-python\">df.product_name = df.product_name.apply(lambda x: x.split('.')[1].strip())\ndf.head()\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">     product_name\n0\t【限时特惠】Apple/苹果 iPhone 11 Pro Max 手机\n1\t【超值折扣】Huawei/华为 P30 Pro 智能手机\n2\t【热销爆款】OPPO Reno 10X Zoom 全网通手机\n3\t【限量特价】Xiaomi/小米 9 Pro 5G 手机\n4\t【限时促销】Apple/苹果 iPad Pro 11寸 平板\n</code></pre><p>我们可以如法炮制，再生成一些女装的商品名称，覆盖不同的品类，这样后面我们演示搜索效果的时候就会方便一点。</p><pre><code class=\"language-python\">clothes_prompt = \"\"\"请你生成50条淘宝网里的商品的标题，每条在30个字左右，品类是女性的服饰箱包等等，标题里往往也会有一些促销类的信息，每行一条。\"\"\"\nclothes_data = generate_data_by_prompt(clothes_prompt)\nclothes_product_names = clothes_data.strip().split('\\n')\nclothes_df = pd.DataFrame({'product_name': clothes_product_names})\nclothes_df.product_name = clothes_df.product_name.apply(lambda x: x.split('.')[1].strip())\nclothes_df.head()\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">    product_name\n0\t【新款】时尚百搭羊绒毛衣，暖洋洋温暖你的冬天\n1\t【热卖】复古气质翻领毛衣，穿出时尚感\n2\t【特价】轻薄百搭棉衣，舒适温暖，冬季必备\n3\t【限时】经典百搭牛仔外套，轻松搭配，时尚范\n4\t【全新】简约大气羊绒连衣裙，温柔优雅\n</code></pre><p>然后我们把这两个DataFrame拼接在一起，就是我们接下来用于做搜索实验的数据。</p><pre><code class=\"language-python\">df = pd.concat([df, clothes_df], axis=0)\ndf = df.reset_index(drop=True)\ndisplay(df)\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">\tproduct_name\n0\t【新款】Apple/苹果 iPhone 11 Pro Max 智能手机\n1\t【特惠】华为P30 Pro 8GB+256GB 全网通版\n2\t【限量】OnePlus 7T Pro 8GB+256GB 全网通\n3\t【新品】小米CC9 Pro 8GB+256GB 全网通版\n4\t【热销】三星Galaxy Note10+ 8GB+256GB 全网通\n...\t...\n92\t【优惠】气质小清新拼接百搭双肩斜挎包\n93\t【热卖】活力色彩精致小巧百搭女士单肩斜挎包\n94\t【特价】简约可爱原宿风时尚双肩斜挎包\n95\t【折扣】潮流小清新拼接百搭女士单肩斜挎包\n96\t【特惠】百搭潮流活力色彩拼色双肩斜挎\n</code></pre><p>不过如果你多试几次，你会发现AI有时候返回的条数没有50条，不过没有关系，这个基本不影响我们使用这个数据源。你完全可以多运行几次，获得足够你需要的数据。</p><h2>通过Embedding进行语义搜索</h2><p>那对于搜索问题，我们可以用GPT模型干些什么呢？像百度、阿里这样的巨头公司自然有很多内部复杂的策略和模型，但是对于大部分中小公司，特别是刚开始提供搜索功能的时候，往往是使用Elasticsearch这个开源项目。而Elasticsearch背后的搜索原理，则是先分词，然后再使用倒排索引。</p><p>简单来说，就是把上面的“气质小清新拼接百搭双肩斜挎包”这样的商品名称，拆分成“气质”“小清新”“拼接”“百搭”“双肩”“斜挎包”。每个标题都是这样切分。然后，建立一个索引，比如“气质”这个词，出现过的标题的编号，都按编号顺序跟在气质后面。其他的词也类似。</p><p>然后，当用户搜索的时候，比如用户搜索“气质背包”，也会拆分成“气质”和“背包”两个词。然后就根据这两个词，找到包含这些词的标题，根据出现的词的数量、权重等等找出一些商品。</p><p>但是，这个策略有一个缺点，就是如果我们有同义词，那么这么简单地去搜索是搜不到的。比如，我们如果搜“自然淡雅背包”，虽然语义上很接近，但是因为“自然”“淡雅”“背包”这三个词在这个商品标题里都没有出现，所以就没有办法匹配上了。为了提升搜索效果，你就得做更多的工程研发工作，比如找一个同义词表，把标题里出现的同义词也算上等等。</p><p>不过，有了OpenAI的Embedding接口，我们就可以把一段文本的语义表示成一段向量。而向量之间是可以计算距离的，这个我们在之前的情感分析的零样本分类里就演示过了。那如果我们把用户的搜索，也通过Embedding接口变成向量。然后把它和所有的商品的标题计算一下余弦距离，找出离我们搜索词最近的几个向量。那最近的几个向量，其实就是语义和这个商品相似的，而并不一定需要相同的关键词。</p><p>那根据这个思路，我们不妨用代码来试一试。</p><p>首先，我们还是要把随机生成出来的所有商品标题，都计算出来它们的Embedding，然后存下来。这里的代码，基本和之前通过Embedding进行分类和聚类一致，就不再详细讲解了。我们还是利用backoff和batch处理，让代码能够容错，并且快速处理完这些商品标题。</p><pre><code class=\"language-python\">from openai.embeddings_utils import get_embeddings\nimport openai, os, backoff\n\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\nembedding_model = \"text-embedding-ada-002\"\n\nbatch_size = 100\n\n@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef get_embeddings_with_backoff(prompts, engine):\n    embeddings = []\n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i+batch_size]\n        embeddings += get_embeddings(list_of_text=batch, engine=engine)\n    return embeddings\n\nprompts = df.product_name.tolist()\nprompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n\nembeddings = []\nfor batch in prompt_batches:\n    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n    embeddings += batch_embeddings\n\ndf[\"embedding\"] = embeddings\ndf.to_parquet(\"data/taobao_product_title.parquet\", index=False)\n</code></pre><p>然后，我们就可以定义一个search_product的搜索函数，接受三个参数，一个df代表用于搜索的数据源，一个query代表用于搜索的搜索词，然后一个n代表搜索返回多少条记录。而这个函数就干了这样三件事情。</p><ol>\n<li>调用OpenAI的API将搜索词也转换成Embedding。</li>\n<li>将这个Embedding和DataFrame里的每一个Embedding都计算一下余弦距离。</li>\n<li>根据余弦相似度去排序，返回距离最近的n个标题。</li>\n</ol><pre><code class=\"language-python\">from openai.embeddings_utils import get_embedding, cosine_similarity\n\n# search through the reviews for a specific product\ndef search_product(df, query, n=3, pprint=True):\n    product_embedding = get_embedding(\n        query,\n        engine=embedding_model\n    )\n    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .product_name\n    )\n    if pprint:\n        for r in results:\n            print(r)\n    return results\n\nresults = search_product(df, \"自然淡雅背包\", n=3)\n</code></pre><p>我们就拿刚才举的那个例子，使用“自然淡雅背包”作为搜索词，调用这个search_product函数，然后拿前3个返回结果。可以看到，尽管在关键词上完全不同，但是返回的结果里，的确包含了“小清新百搭拼色女士单肩斜挎包”这个商品。</p><p>输出结果：</p><pre><code class=\"language-python\">【新品】潮流简约可爱时尚双肩斜挎包\n【优惠】精致小巧可爱双肩斜挎包\n【新品】小清新百搭拼色女士单肩斜挎包\n</code></pre><p>注意，因为我们的商品标题都是随机生成的，所以你得到的数据集和搜索结果可能都和我不一样，你根据实际情况测试你想要的搜索词即可。</p><h2>利用Embedding信息进行商品推荐的冷启动</h2><p>Embedding的向量距离，不仅可以用于搜索，也可以用于<strong>商品推荐里的冷启动</strong>。主流的推荐算法，主要是依托于用户“看了又看”这样的行为信息。也就是如果有很多用户看了OPPO的手机，又去看了vivo的手机，那么在用户看OPPO手机的时候，我们就可以向他推荐vivo手机。但是，往往一个新的商品，或者新的平台，没有那么多相关的行为数据。这个时候，我们同样可以根据商品名称在语义上的相似度，来进行商品推荐。</p><p>我们这里的代码实现和上面的搜索例子基本一致，唯一的差别就是商品名称的Embedding是根据输入的商品名称，从DataFrame里找到的，而不是通过调用OpenAI的Embedding API获取。因为这个Embedding我们之前已经计算过一遍了，没有必要浪费成本再请求一次。</p><pre><code class=\"language-python\">def recommend_product(df, product_name, n=3, pprint=True):\n    product_embedding = df[df['product_name'] == product_name].iloc[0].embedding\n    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .product_name\n    )\n    if pprint:\n        for r in results:\n            print(r)\n    return results\n\nresults = recommend_product(df, \"【限量】OnePlus 7T Pro 8GB+256GB 全网通\", n=3)\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">【限量】OnePlus 7T Pro 8GB+256GB 全网通\n【新品】OnePlus 7T Pro 8GB+256GB 全网通\n【限量】荣耀V30 Pro 8GB+256GB 全网通版\n</code></pre><h2>通过FAISS加速搜索过程</h2><p>不过，上面的示例代码里面，还有一个问题。那就是每次我们进行搜索或者推荐的时候，我们都要把输入的Embedding和我们要检索的数据的所有Embedding都计算一次余弦相似度。例子里，我们检索的数据只有100条，但是在实际的应用中，即使不是百度、谷歌那样的搜索引擎，搜索对应的内容条数在几百万上千万的情况也不在少数。如果每次搜索都要计算几百万次余弦距离，那速度肯定慢得不行。</p><p>这个问题也有解决办法。我们可以利用一些向量数据库，或者能够快速搜索相似性的软件包就好了。比如，我比较推荐你使用Facebook开源的Faiss这个Python包，它的全称就是Facebook AI Similarity Search，也就是快速进行高维向量的相似性搜索。</p><p>我们把上面的代码改写一下，先把DataFrame里面计算好的Embedding向量都加载到Faiss的索引里，然后让Faiss帮我们快速找到最相似的向量，来看看效果怎么样。</p><p>当然，按照惯例我们还是需要先安装Faiss这个Python库。</p><pre><code class=\"language-python\">conda install -c conda-forge faiss\n</code></pre><p>把索引加载到Faiss里面非常容易，我们直接把整个的Embedding变成一个二维矩阵，整个加载到Faiss里面就好了。在加载之前，我们先要定义好Faiss索引的维度数，也就是我们Embedding向量的维度数。</p><pre><code class=\"language-python\">import faiss\nimport numpy as np\n\ndef load_embeddings_to_faiss(df):\n    embeddings = np.array(df['embedding'].tolist()).astype('float32')\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings)\n    return index\n\nindex = load_embeddings_to_faiss(df)\n</code></pre><p>而搜索Faiss也非常容易，我们把查询变成Embedding，然后再把Embedding转换成一个numpy的array向量，然后直接对刚才生成的索引 index 调用 search 方法，并且指定返回的结果数量就可以了。</p><p>返回拿到的，只有索引的index，也就是加载在Faiss里面的第几个索引。我们还是要根据这个，在DataFrame里面，反查到对应的是DataFrame里面的第几行，以及这一行商品的标题是什么，就能获得搜索的结果。</p><pre><code class=\"language-python\">def search_index(index, df, query, k=5):\n    query_vector = np.array(get_embedding(query, engine=embedding_model)).reshape(1, -1).astype('float32')\n    distances, indexes = index.search(query_vector, k)\n\n    results = []\n    for i in range(len(indexes)):\n        product_names = df.iloc[indexes[i]]['product_name'].values.tolist()\n        results.append((distances[i], product_names))    \n    return results\n\nproducts = search_index(index, df, \"自然淡雅背包\", k=3)\n\nfor distances, product_names in products:\n    for i in range(len(distances)):\n        print(product_names[i], distances[i])\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">【新品】潮流简约可爱时尚双肩斜挎包 0.22473168\n【优惠】精致小巧可爱双肩斜挎包 0.22881898\n【新品】小清新百搭拼色女士单肩斜挎包 0.22901852\n</code></pre><p>我们同样用“自然淡雅背包”这个搜索词，可以看到搜索结果和之前我们自己计算余弦距离排序的结果是一样的。</p><p>Faiss的原理，是通过ANN这样的近似最近邻的算法，快速实现相似性的搜索。如果你想进一步了解Faiss的原理，你也可以去问问ChatGPT。</p><p><img src=\"https://static001.geekbang.org/resource/image/ba/y2/bac528488936d4d2b5d6e22d983d6yy2.png?wh=688x652\" alt=\"图片\"></p><p>Faiss这个库能够加载的数据量，受限于我们的内存大小。如果你的数据量进一步增长，就需要选用一些向量数据库来进行搜索。比如OpenAI就推荐了 <a href=\"https://www.pinecone.io/\">Pinecone</a> 和 <a href=\"https://weaviate.io/\">Weaviate</a>，周围也有不少团队使用的是 <a href=\"https://milvus.io/\">Milvus</a> 这个国人开源的产品。</p><p>当然，无论是搜索还是推荐，使用Embedding的相似度都只是一种快速启动的方式。需要真正做到更好的效果，一定也需要投入更复杂的策略。比如根据用户行为的反馈，更好地排序搜索和推荐结果。但是，对于提供一个简单的搜索或者推荐功能来说，通过文本的Embedding的相似度，是一个很好的快速启动的方式。</p><h2>小结</h2><p>好了，相信经过这一讲，你已经有了快速优化你们现有业务里的推荐和搜索功能的思路了。这一讲里，我主要想教会你三件事情。</p><p>第一，是在没有合适的测试数据的时候，我们完全可以让AI给我们生成一些数据。既节约了在网上找数据的时间，也能根据自己的要求生成特定特征的数据。比如，我们就可以要求在商品标题里面有些促销相关的信息。</p><p>第二，是如何利用Embedding之间的余弦相似度作为语义上的相似度，优化搜索。通过Embedding的相似度，我们不要求搜索词和查询的内容之间有完全相同的关键字，而只要它们的语义信息接近就好了。</p><p>第三，是如何通过Faiss这样的Python库，或者其他的向量数据库来快速进行向量之间的检索。而不是必须每一次搜索都和整个数据库计算一遍余弦相似度。</p><p>通过对于我们自己的数据计算Embedding，然后索引起来，我们可以将外部的知识和信息引入到使用GPT模型的应用里来。后面，我们还会进一步学习如何利用这些外部知识，开发更加复杂的AI应用。</p><h2>课后练习</h2><p>搜索里面经常会遇到这样一个问题，同样的关键词有歧义。比如，我们搜索“小米手机”，返回结果里也许应该有“荣耀V30 Pro”，但是不应该返回“黑龙江优质小米”。你可以试一试用Embedding进行语义搜索，看看还会不会遇上这样的问题？</p><p>期待能在评论区看到你的分享，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>","neighbors":{"left":{"article_title":"导读｜从今天开始，让AI成为你的贴身助理","id":641726},"right":{"article_title":"10｜AI连接外部资料库，让Llama Index带你阅读一本书","id":645305}},"comments":[{"had_liked":false,"id":372466,"user_name":"廉烨","can_delete":false,"product_type":"c1","uid":1675946,"ip_address":"上海","ucode":"8081DA9EF0FBAE","user_header":"https://static001.geekbang.org/account/avatar/00/19/92/aa/360eefd3.jpg","comment_is_top":false,"comment_ctime":1681181901,"is_pvip":false,"replies":[{"id":135969,"content":"我们之前用过开源的t5-base，embedding也还不错\n你可以选择 flan-t5 系列，或者后面介绍开源平替的 sentence_transformers\n只是embedding的话，可以选的还是不少的","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681263046,"ip_address":"上海","comment_id":372466,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师，请问是否有开源的embedding组件，能够达到或接近openai embedding能力的？能够用于中文问答搜索","like_count":14,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613310,"discussion_content":"我们之前用过开源的t5-base，embedding也还不错\n你可以选择 flan-t5 系列，或者后面介绍开源平替的 sentence_transformers\n只是embedding的话，可以选的还是不少的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681263047,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371930,"user_name":"金","can_delete":false,"product_type":"c1","uid":1559128,"ip_address":"广东","ucode":"48D0B5CB8E5F9E","user_header":"https://static001.geekbang.org/account/avatar/00/17/ca/58/6fe1854c.jpg","comment_is_top":false,"comment_ctime":1680527017,"is_pvip":false,"replies":[{"id":135742,"content":"主要是如何运用现在的AI工具做应用开发。自然语言是其中的一部分，核心也不是自然语言处理知识，而是怎么利用好大语言模型直接开发应用。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680585944,"ip_address":"上海","comment_id":371930,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"这门课程主要讲nlp吗？","like_count":6,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612248,"discussion_content":"主要是如何运用现在的AI工具做应用开发。自然语言是其中的一部分，核心也不是自然语言处理知识，而是怎么利用好大语言模型直接开发应用。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1680585944,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373982,"user_name":"黄智荣","can_delete":false,"product_type":"c1","uid":1027823,"ip_address":"福建","ucode":"3C84C8654CCB11","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/ef/cbb8d881.jpg","comment_is_top":false,"comment_ctime":1683441985,"is_pvip":false,"replies":[{"id":136848,"content":"faiss其实还是很快的，百万量级的faiss similarity通过CPU计算也只需要10ms以内","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684748911,"ip_address":"上海","comment_id":373982,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"这个挺有意思的。不过这种性能应该会降低很多，就算有这种通过faiss计算。原来通过倒排索引，用很低的资源就可以实现，用这faiss 数据量一大，估计都要用显卡才可以，量大对显存要求也很高","like_count":4,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619027,"discussion_content":"faiss其实还是很快的，百万量级的faiss similarity通过CPU计算也只需要10ms以内","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1684748911,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372094,"user_name":"eagle","can_delete":false,"product_type":"c1","uid":1129632,"ip_address":"江苏","ucode":"D7601A2AFC1D17","user_header":"https://static001.geekbang.org/account/avatar/00/11/3c/a0/8b9d5aca.jpg","comment_is_top":false,"comment_ctime":1680707461,"is_pvip":false,"replies":[{"id":135881,"content":"类似 text-ada-embedding 之类的小模型不会改变\n升级往往是提供一个新模型\n\n特定模型也有带日期的快照版本，选取那些快照版本就好了。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680973977,"ip_address":"日本","comment_id":372094,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"过几天openAI的模型版本升级了，这些保存的embedding会失效吗？","like_count":4,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612877,"discussion_content":"类似 text-ada-embedding 之类的小模型不会改变\n升级往往是提供一个新模型\n\n特定模型也有带日期的快照版本，选取那些快照版本就好了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680973977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"日本","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372551,"user_name":"Yezhiwei","can_delete":false,"product_type":"c1","uid":1005157,"ip_address":"北京","ucode":"31E8E33688CBEC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/56/65/22a37a8e.jpg","comment_is_top":false,"comment_ctime":1681275485,"is_pvip":false,"replies":[{"id":135979,"content":"可以，但是我会建议反过来操作。把数据库的表信息和需求提供给大语言模型的上下文，然后让大语言模型自动生成查询的SQL，也许效果更好一些。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681287102,"ip_address":"上海","comment_id":372551,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请问老师可以把关系型数据库的结构化数据embedding 到向量数据库吗？比如财务报表的数据，然后通过自然语言的方式查询数据，谢谢","like_count":3,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613359,"discussion_content":"可以，但是我会建议反过来操作。把数据库的表信息和需求提供给大语言模型的上下文，然后让大语言模型自动生成查询的SQL，也许效果更好一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681287102,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":1005157,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/56/65/22a37a8e.jpg","nickname":"Yezhiwei","note":"","ucode":"31E8E33688CBEC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":613389,"discussion_content":"确实这么做了，用 ChatGPT 生成 SQL，也看到了别人的开源项目，个人觉得效果不是很好，目前的方法是优化各种情况的提问模板，这样的问题就是不断的增加模板，也不能保证各种情况都能覆盖到","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1681298533,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":613359,"ip_address":"北京","group_id":0},"score":613389,"extra":""},{"author":{"id":1180256,"avatar":"https://static001.geekbang.org/account/avatar/00/12/02/60/8b9572ac.jpg","nickname":"小风","note":"","ucode":"A5D04FF18880C0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":648084,"discussion_content":"老师，现在text2sql模型有啥好的推荐吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1721043964,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":613359,"ip_address":"浙江","group_id":0},"score":648084,"extra":""}]}]},{"had_liked":false,"id":373225,"user_name":"Jelly","can_delete":false,"product_type":"c1","uid":2861619,"ip_address":"广东","ucode":"877CC61AF0A7F3","user_header":"https://static001.geekbang.org/account/avatar/00/2b/aa/33/5e063968.jpg","comment_is_top":false,"comment_ctime":1682260366,"is_pvip":false,"replies":[{"id":136511,"content":"有几种做法\n1. 不只是用embedding来做文本召回，llama还提供了更多的数据结构，可以深入看一遍文档，试一下其他的indices\n2. 对于文本，通过 self-ask，先让OpenAI生成很多针对内容的问题。然后和问题做匹配。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683091598,"ip_address":"上海","comment_id":373225,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"当使用Llama Index导入一篇产品介绍的时候，问：本产品的特征是什么，向量匹配不准确。使用XXX的特征就没问题。当导入年报的时候问：2022年营收是多少？向量匹配也不准确，直接问：营收是多少就可以。请问怎么让用户问的问题更智能？","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616752,"discussion_content":"有几种做法\n1. 不只是用embedding来做文本召回，llama还提供了更多的数据结构，可以深入看一遍文档，试一下其他的indices\n2. 对于文本，通过 self-ask，先让OpenAI生成很多针对内容的问题。然后和问题做匹配。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1683091598,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374265,"user_name":"Joe Black","can_delete":false,"product_type":"c1","uid":1052528,"ip_address":"北京","ucode":"21FE222A286445","user_header":"https://static001.geekbang.org/account/avatar/00/10/0f/70/cdef7a3d.jpg","comment_is_top":false,"comment_ctime":1683797272,"is_pvip":false,"replies":[{"id":136852,"content":"embedding就是基础模型啊，只是不是拿来生成文本，只是用了最后一层Transformer里的输出向量。\n\n所以的确是越大的模型，embedding按道理应该越好。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684749353,"ip_address":"上海","comment_id":374265,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"embedding的实现是否也需要基础模型的知识沉淀呢？比如文字上虽然不相同，但是含义相近的句子生成的向量是相似的，这个是依靠模型之前学习的知识是吗？那这样自然就是越大的模型embedding的效果越好？可以这样理解吗？","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619032,"discussion_content":"embedding就是基础模型啊，只是不是拿来生成文本，只是用了最后一层Transformer里的输出向量。\n\n所以的确是越大的模型，embedding按道理应该越好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684749353,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372460,"user_name":"渔樵耕读","can_delete":false,"product_type":"c1","uid":1042735,"ip_address":"北京","ucode":"2C4E141C59381F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e9/2f/c5e5b809.jpg","comment_is_top":false,"comment_ctime":1681177069,"is_pvip":false,"replies":[{"id":135968,"content":"pip install faiss-cpu 或者 pip install faiss-gpu","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681262945,"ip_address":"上海","comment_id":372460,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请问有遇到安装faiss时提示：PackagesNotFoundError的没？改为pip install faiss 报错：\nERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\nERROR: No matching distribution found for faiss","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613309,"discussion_content":"pip install faiss-cpu 或者 pip install faiss-gpu","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681262945,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2238528,"avatar":"https://static001.geekbang.org/account/avatar/00/22/28/40/82d748e6.jpg","nickname":"小理想。","note":"","ucode":"EDC35A907570DB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":624392,"discussion_content":"我用pip install faiss-gpu好使了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1690531497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372028,"user_name":"xbc","can_delete":false,"product_type":"c1","uid":2187437,"ip_address":"海南","ucode":"03F1FD7B8CA32F","user_header":"https://static001.geekbang.org/account/avatar/00/21/60/ad/03351e6e.jpg","comment_is_top":false,"comment_ctime":1680606426,"is_pvip":false,"replies":[{"id":135887,"content":"👍","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680974348,"ip_address":"日本","comment_id":372028,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"cosine_similarity 也可以传入多个embeddings. \n\nscores = cosine_similarity(list[list[float]], list[float])\nindices = np.argsort(scores)[::-1]","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612883,"discussion_content":"👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680974348,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"日本","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371883,"user_name":"Oli张帆","can_delete":false,"product_type":"c1","uid":1338098,"ip_address":"北京","ucode":"6E60A370C3C14A","user_header":"https://static001.geekbang.org/account/avatar/00/14/6a/f2/db90fa96.jpg","comment_is_top":false,"comment_ctime":1680486837,"is_pvip":false,"replies":[{"id":135711,"content":"这个办法当然不是不可以。但是为什么要限制自己用那么小的内存呢？毕竟现在服务器的成本并不高啊。\n\n不用faiss，直接搞个云托管的向量数据库好了？大部分都有免费档位都比你自己的100条资源要多啊？","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680581212,"ip_address":"上海","comment_id":371883,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"考虑到我的服务器硬件资源有限（300MB的软限制，500MB的硬限制，现在已经占到了450MB），而且我已经在进行其他任务时使用了很多资源，当我在OpenAI之上构建AI机器人时，我考虑使用这个策略。请老师看看是否能行得通。\n\n每当用户发送请求时，我首先检测他的意图。我可以使用小的embeddings来帮助意图检测。甚至可以有其他方式，如缓存和预先一些意图让用户来点击，来加速意图检测。\n\n在我检测到用户意图后，我可以调用不同的embeddings库。例如，我的客服embeddings库将仅有50个项目，这对于余弦相似性来说非常快速和高效。我还可以将新闻embeddings库限制为最新的100篇文章，以便它可以轻松地通过余弦相似性处理。\n\n尽管这种方法可能不像搜索单个大型嵌入数据库那样准确，但如果我为用户提供足够的指导，您认为它能产生足够好的用户体验吗？例如，在我的界面中，我可以向用户显示他们当前正在讨论“最新的AI发展”或“客户服务”。然后，也可以允许用户快速地改变当前的话题。\n\n请老师看看，这个办法能不能跑通。还没有什么我没有想到的地方？","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612210,"discussion_content":"这个办法当然不是不可以。但是为什么要限制自己用那么小的内存呢？毕竟现在服务器的成本并不高啊。\n\n不用faiss，直接搞个云托管的向量数据库好了？大部分都有免费档位都比你自己的100条资源要多啊？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680581212,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1338098,"avatar":"https://static001.geekbang.org/account/avatar/00/14/6a/f2/db90fa96.jpg","nickname":"Oli张帆","note":"","ucode":"6E60A370C3C14A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612358,"discussion_content":"找到了关于ConePine的信息，这里也有如何与OpenAI整合的方式：https://docs.pinecone.io/docs/openai","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1680617767,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1338098,"avatar":"https://static001.geekbang.org/account/avatar/00/14/6a/f2/db90fa96.jpg","nickname":"Oli张帆","note":"","ucode":"6E60A370C3C14A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612353,"discussion_content":"感谢老师的指导，我去研究一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680615141,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373829,"user_name":"Devon","can_delete":false,"product_type":"c1","uid":1069991,"ip_address":"上海","ucode":"45442792ABEC73","user_header":"https://static001.geekbang.org/account/avatar/00/10/53/a7/83bf4578.jpg","comment_is_top":false,"comment_ctime":1683213999,"is_pvip":false,"replies":[{"id":136825,"content":"我觉得这个没有必要用Embedding吧，因为公司名称里面的“语义”信息很少，模糊搜索更多是基于字或者音的\n\n按照单个字，或者拼音，做个Trie Tree可能更合适？或者算一下按照字或者字的拼音的字符编辑距离来做搜索排序就好了。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684741660,"ip_address":"上海","comment_id":373829,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师，对于几十万个精准的中文公司名称或者单位地址进行向量化之后，预存进DataFrame作为之后的搜索源，使用时根据较为“脏”的公司名称或者单位地址进行匹配，在这样的操作中对于中文公司名称或者单位地址的向量化预处理，有什么推荐的方式或者模型吗？用OpenAI embedding的话，费用和时间会不会不太经济？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":618992,"discussion_content":"我觉得这个没有必要用Embedding吧，因为公司名称里面的“语义”信息很少，模糊搜索更多是基于字或者音的\n\n按照单个字，或者拼音，做个Trie Tree可能更合适？或者算一下按照字或者字的拼音的字符编辑距离来做搜索排序就好了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684741661,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372345,"user_name":"一叶","can_delete":false,"product_type":"c1","uid":3577168,"ip_address":"福建","ucode":"21E5455D0814E5","user_header":"https://static001.geekbang.org/account/avatar/00/36/95/50/01199ae9.jpg","comment_is_top":false,"comment_ctime":1681011160,"is_pvip":false,"replies":[{"id":135940,"content":"多久一个例子是推荐已经算过embedding的商品，所代码是先根据商品的index在faiss里找到它的向量呀\n\n这个向量是前面调用OpenAI已经计算过了的","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681129512,"ip_address":"美国","comment_id":372345,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师 在最后一个例子中\n\ndef search_index(index, df, query, k=5):\n\n这里面的 query 不用先进行向量化吗? 那么这个时候不是要调用到Openai?\n","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613110,"discussion_content":"多久一个例子是推荐已经算过embedding的商品，所代码是先根据商品的index在faiss里找到它的向量呀\n\n这个向量是前面调用OpenAI已经计算过了的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681129512,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1222233,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a6/59/1689ea0c.jpg","nickname":"金hb.Ryan 冷空氣駕到","note":"","ucode":"CAD363576696E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":618799,"discussion_content":"Search index 里*面调用了embedding哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684565099,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372344,"user_name":"一叶","can_delete":false,"product_type":"c1","uid":3577168,"ip_address":"福建","ucode":"21E5455D0814E5","user_header":"https://static001.geekbang.org/account/avatar/00/36/95/50/01199ae9.jpg","comment_is_top":false,"comment_ctime":1681010515,"is_pvip":false,"replies":[{"id":135938,"content":"用faiss或者上milvus我试过百万级商品毫无压力","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681129230,"ip_address":"美国","comment_id":372344,"utype":1}],"discussion_count":3,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师,这样的搜索效果的效率如何? 如果我是一个话术类搜索的,1万多个标题直接用内容来进行搜索的话可以?","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613107,"discussion_content":"用faiss或者上milvus我试过百万级商品毫无压力","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681129230,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":3577168,"avatar":"https://static001.geekbang.org/account/avatar/00/36/95/50/01199ae9.jpg","nickname":"一叶","note":"","ucode":"21E5455D0814E5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":613451,"discussion_content":"老师，faiss如果百万级，内存占用多少，我在想简单的就直接用faiss","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681314869,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":613107,"ip_address":"福建","group_id":0},"score":613451,"extra":""},{"author":{"id":1009518,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/67/6e/f5ee46e8.jpg","nickname":"海滨","note":"","ucode":"F1B94D2DB944DC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":3577168,"avatar":"https://static001.geekbang.org/account/avatar/00/36/95/50/01199ae9.jpg","nickname":"一叶","note":"","ucode":"21E5455D0814E5","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":614044,"discussion_content":"可以简单计算下，特征向量里一个数字 float 类型占用 4 个字节，如果特征向量是 1024 维度的，那么一条特征向量会占用 4KB 存储。一百万数据的话，差不多就是 4GB 的内存。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681645701,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":613451,"ip_address":"浙江","group_id":0},"score":614044,"extra":""}]}]},{"had_liked":false,"id":372238,"user_name":"幼儿编程教学","can_delete":false,"product_type":"c1","uid":1237199,"ip_address":"浙江","ucode":"F13F3150E6CAE9","user_header":"https://static001.geekbang.org/account/avatar/00/12/e0/cf/43f201f2.jpg","comment_is_top":false,"comment_ctime":1680866587,"is_pvip":false,"replies":[{"id":135861,"content":"不会，这个就是在你本机上计算两个向量的相似度而已","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680972406,"ip_address":"中国香港","comment_id":372238,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"请教老师， get_embedding函数会请求openai。那么cosine_similarity函数是否会请求openai接口？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612856,"discussion_content":"不会，这个就是在你本机上计算两个向量的相似度而已","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680972406,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"中国香港","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2031550,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/ff/be/a8750de7.jpg","nickname":"田会军","note":"","ucode":"7AE52133B18C0C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":615134,"discussion_content":"这个能做到语义分析吗，还是生硬的匹配相似度","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682062384,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"中国香港","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372911,"user_name":"aoe","can_delete":false,"product_type":"c1","uid":1121758,"ip_address":"浙江","ucode":"1C6201EDB4E954","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","comment_is_top":false,"comment_ctime":1681793057,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"原来商品搜索、推荐系统都已经使用上了 AI，确实强大","like_count":1},{"had_liked":false,"id":394406,"user_name":"Geek_78a551","can_delete":false,"product_type":"c1","uid":3784461,"ip_address":"北京","ucode":"CCADD2F766E193","user_header":"","comment_is_top":false,"comment_ctime":1726722053,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"taobao_product_title.parquet 你把所有相关中间文件放一个代码库里分享吧","like_count":0},{"had_liked":false,"id":388292,"user_name":"Jahng","can_delete":false,"product_type":"c1","uid":1325514,"ip_address":"广东","ucode":"5D5789EAAC4739","user_header":"https://static001.geekbang.org/account/avatar/00/14/39/ca/4a07bfd8.jpg","comment_is_top":false,"comment_ctime":1709809023,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"很好奇我们提示语要淘宝数据，gpt是怎么拿到这个数据的，是因为训练集里面有数据，还是联网拿的实时数据？不管是哪种，我后面都有非常多的疑问🤔️","like_count":0},{"had_liked":false,"id":380784,"user_name":"Geek_f0daf6","can_delete":false,"product_type":"c1","uid":3172264,"ip_address":"广东","ucode":"A8363E253E1C60","user_header":"","comment_is_top":false,"comment_ctime":1694150668,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"可以用羊驼2大模型吗","like_count":0},{"had_liked":false,"id":378619,"user_name":"小理想。","can_delete":false,"product_type":"c1","uid":2238528,"ip_address":"北京","ucode":"EDC35A907570DB","user_header":"https://static001.geekbang.org/account/avatar/00/22/28/40/82d748e6.jpg","comment_is_top":false,"comment_ctime":1690510477,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"taobao_product_title.parquet  老师这个文件可否发下呢？","like_count":0},{"had_liked":false,"id":378446,"user_name":"没角蜗牛","can_delete":false,"product_type":"c1","uid":1275377,"ip_address":"北京","ucode":"3AF5466D0D3E08","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/9wNL7qwjpZ1NQ6VIVNsZDyJrtkhUW5e7rpqMgR1qTsouamibLZC1oOv445jVUg1BOIJib4cHiaPGKBwrskyXn8xWw/132","comment_is_top":false,"comment_ctime":1690269648,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师您好，我想问几个问题：\n1： 文本的Embedding跟算法有关系，跟具体某个大模型没有关系吧？如果有关系，是什么关系\n2： 如果问题一跟大模型没关系，那我们只要知道Embedding算法就可以处理所有分本分类相关的工作，都不需要大模型了\n3： 如果Embedding跟大模型没关系，为啥不同的大模型处理文本分类的效果差距很大？\n\n利用 Embedding 之间的余弦相似度作为语义上的相似度，优化搜索。通过 Embedding 的相似度，我们不要求搜索词和查询的内容之间有完全相同的关键字，而只要它们的语义信息接近就好","like_count":0,"discussions":[{"author":{"id":3682659,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Wiba6ReQVp446vhcibQYiah5tdWPiaYAwsr6dty9ZTcZS1yXW1x9OQqPpkQbz2qyWMLPIl1T89rep1tXXEC9TefdKCjt51Crvrh2KmSO5cbeZFk/132","nickname":"Geek_e3264b","note":"","ucode":"D6388103876BD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":624693,"discussion_content":"跟某个大模型当然有关系，简单来看：\nEmbedding就是文本在模型空间中的位置表示\n不同的模型构造这个空间的方式都不是一样\n越大的模型构建空间越大，越有可能精确表示这个文本的位置\n","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1690858063,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":377140,"user_name":"猿鸽君","can_delete":false,"product_type":"c1","uid":1991951,"ip_address":"福建","ucode":"8562D8C5AD3D1E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/65/0f/7b9f27f2.jpg","comment_is_top":false,"comment_ctime":1687965914,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"老师好，请问下，数据量较大的情况下，为什么用向量数据库而不是faiss，像milvus不也是in-memory计算？而且为什么milvus对search返回的topk限制在了16348？faiss好像没看到有这种限制","like_count":0,"discussions":[{"author":{"id":3682659,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Wiba6ReQVp446vhcibQYiah5tdWPiaYAwsr6dty9ZTcZS1yXW1x9OQqPpkQbz2qyWMLPIl1T89rep1tXXEC9TefdKCjt51Crvrh2KmSO5cbeZFk/132","nickname":"Geek_e3264b","note":"","ucode":"D6388103876BD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":624694,"discussion_content":"milvus并不是纯粹的in-memory计算，在处理向量数据时不仅依赖于内存，还会使用磁盘存储来支持大规模数据的存储和查询","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1690858404,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":376599,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"美国","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1687062986,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"老师我们想用视频的字幕做检索 用什么技术比较好呀","like_count":0,"discussions":[{"author":{"id":3682659,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Wiba6ReQVp446vhcibQYiah5tdWPiaYAwsr6dty9ZTcZS1yXW1x9OQqPpkQbz2qyWMLPIl1T89rep1tXXEC9TefdKCjt51Crvrh2KmSO5cbeZFk/132","nickname":"Geek_e3264b","note":"","ucode":"D6388103876BD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":624695,"discussion_content":"用text-embedding-ada-002就好，当然真实情况真实考虑","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1690858468,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":376214,"user_name":"寻路人","can_delete":false,"product_type":"c1","uid":1195917,"ip_address":"江苏","ucode":"1711F740D4D60A","user_header":"https://static001.geekbang.org/account/avatar/00/12/3f/8d/a89be8f9.jpg","comment_is_top":false,"comment_ctime":1686551554,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"报错如下：Traceback (most recent call last):\n  File &quot;C:&#47;Users&#47;liuzq&#47;Desktop&#47;Source&#47;others&#47;pythonProject&#47;lzq&#47;aigc&#47;opt_search.py&quot;, line 82, in &lt;module&gt;\n    results = search_product(df, &quot;自然淡雅背包&quot;, n=3)\n  File &quot;C:&#47;Users&#47;liuzq&#47;Desktop&#47;Source&#47;others&#47;pythonProject&#47;lzq&#47;aigc&#47;opt_search.py&quot;, line 69, in search_product\n    df123[&quot;similarity&quot;] = df123.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n  File &quot;E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py&quot;, line 4630, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File &quot;E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 1025, in apply\n    return self.apply_standard()\n  File &quot;E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py&quot;, line 1076, in apply_standard\n    mapped = lib.map_infer(\n  File &quot;pandas\\_libs\\lib.pyx&quot;, line 2834, in pandas._libs.lib.map_infer\n  File &quot;C:&#47;Users&#47;liuzq&#47;Desktop&#47;Source&#47;others&#47;pythonProject&#47;lzq&#47;aigc&#47;opt_search.py&quot;, line 69, in &lt;lambda&gt;\n    df123[&quot;similarity&quot;] = df123.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n  File &quot;E:\\Anaconda3\\lib\\site-packages\\openai\\embeddings_utils.py&quot;, line 66, in cosine_similarity\n    return np.dot(a, b) &#47; (np.linalg.norm(a) * np.linalg.norm(b))\n  File &quot;&lt;__array_function__ internals&gt;&quot;, line 180, in dot\nValueError: shapes (1536,) and (6,1536) not aligned: 1536 (dim 0) != 6 (dim 0)","like_count":0,"discussions":[{"author":{"id":1637136,"avatar":"https://static001.geekbang.org/account/avatar/00/18/fb/10/b4cb2883.jpg","nickname":"袁作辉","note":"","ucode":"E758D30832F471","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621641,"discussion_content":"我运行没有问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1687406804,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"湖北","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":376038,"user_name":"梦典","can_delete":false,"product_type":"c1","uid":1203920,"ip_address":"美国","ucode":"0A6F91068A13E8","user_header":"https://static001.geekbang.org/account/avatar/00/12/5e/d0/e676ac19.jpg","comment_is_top":false,"comment_ctime":1686227090,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"基于AGI的向量实现语义搜索，真的震撼到我了。能否介绍一些这方面的论文？","like_count":0},{"had_liked":false,"id":375289,"user_name":"stg609","can_delete":false,"product_type":"c1","uid":1073025,"ip_address":"浙江","ucode":"FB70A75A891BB8","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/81/1c614f4a.jpg","comment_is_top":false,"comment_ctime":1685254135,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"老师，请问下，利用 Embedding 进行语义搜索是不是仅限于上下文与提问的关键字存在相关性，比如文中的例子。如果问题是一个需要进行聚合计算之类的问题，比如提问：&quot;一共有多少个气质背包?&quot;。这种问题还能利用向量相似性来给出答案吗？","like_count":0},{"had_liked":false,"id":375242,"user_name":"农民园丁","can_delete":false,"product_type":"c1","uid":1155913,"ip_address":"内蒙古","ucode":"6A91EBBC9DCE6C","user_header":"https://static001.geekbang.org/account/avatar/00/11/a3/49/4a488f4c.jpg","comment_is_top":false,"comment_ctime":1685105647,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"运行## 通过Embedding进行语义搜索代码出现如下错误：\nInvalidRequestError: Too many inputs. The max number of inputs is 1.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https:&#47;&#47;go.microsoft.com&#47;fwlink&#47;?linkid=2213926 for further questions\n\n------\n用的是azure openapi，请问可能是什么原因？","like_count":0},{"had_liked":false,"id":375241,"user_name":"农民园丁","can_delete":false,"product_type":"c1","uid":1155913,"ip_address":"立陶宛","ucode":"6A91EBBC9DCE6C","user_header":"https://static001.geekbang.org/account/avatar/00/11/a3/49/4a488f4c.jpg","comment_is_top":false,"comment_ctime":1685105622,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"运行## 通过Embedding进行语义搜索代码出现如下错误：\nInvalidRequestError: Too many inputs. The max number of inputs is 1.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https:&#47;&#47;go.microsoft.com&#47;fwlink&#47;?linkid=2213926 for further questions\n\n------\n用的是azure openapi，请问可能是什么原因？","like_count":0},{"had_liked":false,"id":373332,"user_name":"Allan","can_delete":false,"product_type":"c1","uid":1310388,"ip_address":"中国香港","ucode":"8DA4DBECC2C45C","user_header":"https://static001.geekbang.org/account/avatar/00/13/fe/b4/295338e7.jpg","comment_is_top":false,"comment_ctime":1682402881,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"授人以鱼不如授人以渔","like_count":0},{"had_liked":false,"id":372037,"user_name":"Ethan New","can_delete":false,"product_type":"c1","uid":2063962,"ip_address":"浙江","ucode":"9CA2EF39E58030","user_header":"https://static001.geekbang.org/account/avatar/00/1f/7e/5a/da39f489.jpg","comment_is_top":false,"comment_ctime":1680620819,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":4,"score":4,"product_id":100541001,"comment_content":"打卡学习","like_count":0}]}