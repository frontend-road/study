{"id":797799,"title":"17｜集群升级与数据备份","content":"<p>你好，我是雪飞。</p><p>刚开始使用 K8s 的时候，我们团队搭建了 V1.18 稳定版本的 K8s 集群，在上面部署了各种业务应用，这套环境长时间没有进行集群升级。随着业务迭代加快和系统安全需求提升，我们发现老版的 K8s 集群无法兼容一些新组件，因此必须要对集群升级。预期升级后的版本是 V1.26 版本，然而，由于版本跨度过大，升级时遇到了许多问题，花费了大量时间。因此，及时升级集群非常重要，避免留下技术债。</p><p>集群升级能够带来很多好处，比如确保集群能利用最新版本的新功能、性能提升和安全补丁，可以增强集群的稳定性和可靠性，同时提升兼容性和扩展性，满足不断变化的技术与业务需求。接下来，我们将深入了解 K8s 的升级过程。</p><h2>K8s 升级说明</h2><p>在 K8s 升级过程中，首先需要备份集群数据。由于所有 K8s 集群相关数据都存储在 etcd 数据库中，所以我们需要先对 etcd 中的数据进行整体备份。如果集群升级过程中出现意外，可以利用 etcd 的快照数据进行回退恢复。</p><p>接下来，按照顺序先升级管理节点，即 Master 节点，然后再升级工作节点，也就是其他Worker 节点。为避免业务服务中断，通常采用滚动升级方式，逐个或分批升级工作节点，确保部分节点仍能正常运行，从而保障节点上的 Pod 能持续对外提供服务。</p><!-- [[[read_end]]] --><p>在升级单个节点时，需要先清空节点上的 Pod 并设置节点为不可调度状态，待升级完成后再取消这一设置，确保整个集群的平稳过渡。</p><p><strong>注意：</strong>在整个升级过程中，需要更新 kubeadm、kubelet 和 kubectl 等工具和组件，K8s 建议这三个组件的版本在升级时保持一致。</p><h2>备份 etcd 数据</h2><p>在集群升级前，备份 etcd 数据至关重要，实际上，日常运维中也应该定期备份 etcd 数据，从而应对灾难恢复场景（如管理节点全部故障）。etcd 的数据备份文件包含所有 K8s 资源对象、配置参数、状态信息和其他数据，我们可以通过 etcdctl 工具导出数据备份，并将其加密存储在安全的服务器备份目录中，以确保在集群故障时能够进行集群恢复。下面我们来动手备份和恢复 etcd 数据。首先安装 etcdctl 工具。</p><h3>安装 etcdctl 命令工具</h3><p>由于 etcd 组件是部署在管理节点上，所以我们备份和恢复操作也是在管理节点上进行。在管理节点上执行安装 etcd 的命令，会自动安装 etcdctl 的命令工具。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# yum install -y etcd\n[root@k8s-master ~]# etcd --version   # 查看安装的etcd版本\netcd Version: 3.3.11\nGit SHA: 2cf9e51\nGo Version: go1.10.3\nGo OS/Arch: linux/amd64\n</code></pre><h3>备份 etcd 数据</h3><p>安装好 etcdctl 工具，执行 “etcdctl snapshot save” 命令，对 etcd 数据进行备份。</p><pre><code class=\"language-yaml\">[root@k8s-master ~]# ETCDCTL_API=3 etcdctl snapshot save snap.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key&nbsp;\nSnapshot saved at snap.db\n</code></pre><p>备份命令参数有点多，我来解释一下：</p><ul>\n<li><strong>ETCDCTL_API=3</strong>：设置环境变量，指定 etcdctl 工具使用的 API 版本为 3。</li>\n<li><strong>etcdctl snapshot save snap.db：</strong>执行全部数据的快照（快照就是一次数据备份）保存操作，快照文件命名为 snap.db。</li>\n<li><strong>–endpoints</strong>：指定集群 etcd 组件的 IP 和端口。由于备份命令就在管理节点执行，所以etcd 组件的 IP 就是本地 IP，端口号为 2379 端口，使用 HTTPS 协议。</li>\n<li><strong>–cacert</strong>：指定集群 etcd 组件的 CA 证书路径，用于验证身份。</li>\n<li><strong>–cert</strong>：指定集群 etcd 组件的证书路径。</li>\n<li><strong>–key</strong>：指定集群 etcd 组件的私钥路径。</li>\n</ul><p>部署 K8s 集群时，会自动安装 etcd 组件的证书，默认文件路径为：“/etc/kubernetes/pki/etcd/ca.crt”、“/etc/kubernetes/pki/etcd/server.crt”、“/etc/kubernetes/pki/etcd/server.key”。我们可以在管理节点中看到这三个文件。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# cd /etc/kubernetes/pki/etcd\n[root@k8s-master etcd]# ls\nca.crt&nbsp; ca.key&nbsp; healthcheck-client.crt&nbsp; healthcheck-client.key&nbsp; peer.crt&nbsp; peer.key&nbsp; server.crt&nbsp; server.key\n</code></pre><p>备份命令执行成功后，就可以在当前目录下看到名为 “snap.db” 的备份文件。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# ls\nsnap.db&nbsp;\n</code></pre><h3>恢复 etcd 数据</h3><p>如果集群管理节点出现故障，修复后可能导致 etcd 中存放的数据丢失，这时我们可以使用备份数据来恢复 etcd 数据，从而回退到备份时的集群工作状态。K8s 官方建议，在执行恢复数据操作前，应先停止所有系统核心组件，如&nbsp;Scheduler 和 Controller Manager 等，以确保它们不会依赖一些过时数据。以下是恢复 etcd 数据的具体步骤。</p><p><strong>第一步：暂停系统组件</strong></p><p>由于集群系统组件的部署文件都存放在 “/etc/kubernetes/manifests/” 目录中，并且以静态 Pod 的形式运行，因此我们只需要更改该目录的名称，让 K8s 找不到组件部署的 YAML 文件，从而会自动暂停这些系统组件运行。<strong>注意：</strong>修改目录之后，kubectl 命令也将无法使用。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# mv /etc/kubernetes/manifests /etc/kubernetes/manifests.bak\n</code></pre><p><strong>第二步：备份当前etcd 的数据目录</strong></p><p>etcd 数据文件位于 “/var/lib/etcd/”。在恢复数据前，需要将当前数据目录修改为另一目录，避免数据被直接覆盖。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# mv /var/lib/etcd/ /var/lib/etcd.bak\n</code></pre><p><strong>第三步：执行恢复命令</strong></p><p>在刚才备份文件 snap.db 所在目录执行 “etcdctl snapshot restore” 恢复数据命令，指定恢复的数据存放至 “/var/lib/etcd” 目录中。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# ETCDCTL_API=3 etcdctl snapshot restore snap.db --data-dir=/var/lib/etcd&nbsp;\n2024-07-03 09:35:12.892662 I | mvcc: restore compact to 3393178\n2024-07-03 09:35:12.901833 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32\n</code></pre><p><strong>第四步：重启系统组件</strong></p><p>备份数据恢复后，将集群系统组件的部署文件目录改回为 “/etc/kubernetes/manifests/”，这样所有系统组件会自动重新运行。最后重启所有节点上的 kubelet。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests\n[root@k8s-master ~]# systemctl restart kubelet&nbsp; # 所有节点重启 kubelet\n</code></pre><p>可以看到，所有集群系统组件 Pod 开始重新启动，稍等片刻，所有 Pod 加载正常。这时，集群状态恢复到 etcd 备份数据时的状态。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# kubectl get pod -n kube-system\nNAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY&nbsp; &nbsp;STATUS&nbsp; &nbsp; RESTARTS&nbsp; &nbsp; &nbsp; AGE\ncoredns-66f779496c-pqwpr&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;15d\ncoredns-66f779496c-x7xhf&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;15d\netcd-k8s-master&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;17d\nkube-apiserver-k8s-master&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10d\nkube-controller-manager-k8s-master&nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10d\nkube-proxy-84jp5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;1 (10d ago)&nbsp; &nbsp;10d\nkube-proxy-r2mwc&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10d\nkube-proxy-vchhh&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10d\nkube-scheduler-k8s-master&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10d\nmetrics-server-7f676b86dc-9lpsb&nbsp; &nbsp; &nbsp; 1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11d\n</code></pre><h2>升级控制面主节点</h2><p>我们掌握了 ectd 的数据备份，就可以在集群升级前做好数据备份，然后开始升级集群节点。</p><p>先从管理节点开始升级，确保升级版本不要跨度太大。通过 “kubectl get node” 命令查看当前我的集群节点版本为 V1.28.0，本次将升级到 V1.28.2 版本。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# kubectl get node\nNAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS&nbsp; &nbsp;ROLES&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE&nbsp; &nbsp;VERSION\nk8s-master&nbsp; &nbsp; Ready&nbsp; &nbsp; control-plane&nbsp; &nbsp;17d&nbsp; &nbsp;v1.28.0\nk8s-worker1&nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 15d&nbsp; &nbsp;v1.28.0\nk8s-worker2&nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 15d&nbsp; &nbsp;v1.28.0\n</code></pre><p>先升级 k8s-master 节点。在 k8s-master 节点上依次执行命令步骤如下：</p><pre><code class=\"language-bash\"># 1、查找最新版本号，目前能使用的最新版本是 1.28.2\n[root@k8s-master ~]# yum list --showduplicates kubeadm\nLoaded plugins: fastestmirror, langpacks\nLoading mirror speeds from cached hostfile\nInstalled Packages\nkubeadm.x86_64&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.28.0-0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;@kubernetes\nAvailable Packages\n.......\nkubeadm.x86_64&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.28.2-0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;kubernetes&nbsp;\n\n# 2、升级kubeadm\n[root@k8s-master ~]# yum install -y kubeadm-1.28.2-0\n\n# 3、驱逐节点上的Pod，且设置节点不可调度\n# --ignore-daemonsets 表示忽略 Daemonset 类型的 Pod\n[root@k8s-master ~]# kubectl drain k8s-master --ignore-daemonsets\nnode/k8s-master cordoned\nWarning: ignoring DaemonSet-managed Pods: calico-system/calico-node-qd57g, calico-system/csi-node-driver-fd2ws, kube-system/kube-proxy-84jp5\nnode/k8s-master drained\n\n# 4、检查集群是否可以升级，并获取可以升级的版本 \n[root@k8s-master ~]# kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n......\nYou can now apply the upgrade by executing the following command:\n&nbsp; &nbsp; &nbsp; &nbsp; kubeadm upgrade apply v1.28.11\n......\n\n# 5、执行升级\n[root@k8s-master ~]# kubeadm upgrade apply v1.28.2\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n.......\n\n# 6、升级 kubelet 和 kubectl\n[root@k8s-master ~]# yum install -y kubelet-1.28.2-0 kubectl-1.28.2-0\nLoaded plugins: fastestmirror, langpacks\nLoading mirror speeds from cached hostfile\n......\n\n# 7、重启kubelet\n[root@k8s-master ~]# systemctl daemon-reload\n[root@k8s-master ~]# systemctl restart kubelet\n\n# 8、取消设置节点不可调度，节点重新上线 \n[root@k8s-master ~]# kubectl uncordon k8s-master\nnode/k8s-master uncordoned\n</code></pre><p><strong>注意：</strong>在升级节点前，需要先清空节点上的所有 Pod 并设置为不可调度。升级完成后，再将节点重新设置为可调度状态。</p><p>主节点升级成功后，可以通过 “kubectl get node” 命令查看升级后的版本。我们看到主节点已经变成 1.28.2 版本。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# kubectl get node\nNAME          STATUS   ROLES           AGE   VERSION\nk8s-master    Ready    control-plane   17d   v1.28.2\nk8s-worker1   Ready    &lt;none&gt;          15d   v1.28.0\nk8s-worker2   Ready    &lt;none&gt;          15d   v1.28.0\n</code></pre><h2>升级工作节点</h2><p>下一步，我们开始升级工作节点。为了确保集群不会完全停止运行，通常会逐个依次升级工作节点，这样在升级过程中仍有正常的工作节点可以维持应用 Pod 的运行。</p><p>我们先升级 k8s-worker1 节点。升级成功后，再升级 k8s-worker2 节点。工作节点的升级步骤完全相同，只需要在不同节点上执行一系列命令。</p><p><strong>注意：</strong>驱逐节点和设置节点可调度时需要替换为每个节点自己的名称，而且这两步操作都在管理节点上通过 kubectl 命令操作。步骤如下：</p><pre><code class=\"language-bash\"># 1、升级 kubeadm\n[root@k8s-worker1 ~]# yum install -y kubeadm-1.28.2-0\n......\nUpdated:\n&nbsp; kubeadm.x86_64 0:1.28.2-0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\nComplete!\n\n# 2、驱逐节点上的Pod，且设置节点不可调度\n# 注意：在管理节点上执行\n# --ignore-daemonsets 表示忽略 Daemonset 类型的 Pod\n# --delete-emptydir-data 表示删除 Pod 用到的本地数据\n[root@k8s-master ~]# kubectl drain k8s-worker1 --ignore-daemonsets --delete-emptydir-data --force\nnode/k8s-worker1 cordoned\nWarning: ignoring DaemonSet-managed Pods: calico-system/calico-node-8g7nn, calico-system/csi-node-driver-hr6ng, kube-system/kube-proxy-vchhh; deleting Pods that declare no controller: default/my-nginx-resources-limit\n\n# 3、升级kubelet配置\n[root@k8s-worker1 ~]# kubeadm upgrade node\n[upgrade] Reading configuration from the cluster...\n[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks\n[preflight] Skipping prepull. Not a control plane node.\n[upgrade] Skipping phase. Not a control plane node.\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config2378199440/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[upgrade] The configuration for this node was successfully updated!\n[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.\n\n# 4、升级kubelet和kubectl\n[root@k8s-worker1 ~]# yum install -y kubelet-1.28.2-0 kubectl-1.28.2-0\n......\nUpdated:\n&nbsp; kubectl.x86_64 0:1.28.2-0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;kubelet.x86_64 0:1.28.2-0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n\nComplete!\n\n# 5、重启kubelet\n[root@k8s-worker1 ~]# systemctl daemon-reload\n[root@k8s-worker1 ~]# systemctl restart kubelet\n\n# 6、取消设置节点不可调度，节点重新上线。\n# 注意：在管理节点上执行\n[root@k8s-master ~]# kubectl uncordon k8s-worker1\nnode/k8s-worker1 uncordoned\n</code></pre><p>所有节点升级完成后，我们再查看节点信息，确认所有节点已更新至 1.28.2 版本。</p><pre><code class=\"language-bash\">[root@k8s-master ~]# kubectl get node\nNAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS&nbsp; &nbsp;ROLES&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE&nbsp; &nbsp;VERSION\nk8s-master&nbsp; &nbsp; Ready&nbsp; &nbsp; control-plane&nbsp; &nbsp;18d&nbsp; &nbsp;v1.28.2\nk8s-worker1&nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 16d&nbsp; &nbsp;v1.28.2\nk8s-worker2&nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 16d&nbsp; &nbsp;v1.28.2\n</code></pre><h2>故障回滚</h2><p>如果 kubeadm upgrade 在执行过程中失败，且没有自动回滚，比如因为节点意外关闭，这时，你可以直接再次运行 kubeadm upgrade 命令。这个命令具有幂等性，可以确保最终的状态与你的预期保持一致。</p><p>在升级过程中，kubeadm 会将数据备份到 “/etc/kubernetes/tmp” 目录下的以下文件夹：</p><ul>\n<li>kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;</li>\n<li>kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</li>\n</ul><p>kubeadm-backup-etcd 包含集群 etcd 组件数据的备份。如果 etcd 升级失败且自动回滚也无法修复，可以将此文件夹中的内容复制到 “/var/lib/etcd” 目录中进行手动恢复。</p><p>kubeadm-backup-manifests 包含集群组件的静态 Pod 的 YAML 文件的备份版本。如果升级失败且无法自动回滚，则可以将此文件夹中的内容复制到 “/etc/kubernetes/manifests” 目录中以实现手动恢复。</p><h2>小结</h2><p>今天，我给你介绍了 K8s 集群升级的重要性和具体步骤。集群升级可以保持集群技术的先进性和安全性，从而利用最新功能、性能改进和安全补丁，增强集群的稳定性、可靠性、兼容性和扩展性。</p><p>升级前需要备份 etcd 数据，先升级管理节点（Master Node），再升级工作节点（Worker Node），工作节点采用滚动升级的方式，逐个或分批升级，避免业务中断。</p><p>然后我带你动手完成了 etcd 组件的数据备份和数据恢复，需要先安装 etcdctl 命令工具，然后使用 “etcdctl snapshot save” 和 “etcdctl snapshot restore” 这两个命令进行数据备份和恢复。在数据恢复前需要先停止所有系统核心组件，确保它们不会依赖一些过时数据，等数据恢复完成再重启组件。</p><p>最后，我给你列出了管理节点和工作节点的升级步骤，按照操作步骤执行命令就可以完成集群节点的版本升级，需要注意升级过程中需要先驱逐节点上的 Pod，同时设置节点不可调度，升级完成后再取消设置。</p><h2>思考题</h2><p>这就是今天的全部内容，在 CKA 考试中会考到 etcd 备份和恢复命令相关的知识点。我给你留一道练习题。</p><p>写一个脚本，每天定时备份 etcd 数据。</p><p>相信经过动手实践，会让你对知识的理解更加深刻。</p>","comments":[{"had_liked":false,"id":392976,"user_name":"余泽锋","can_delete":false,"product_type":"c1","uid":1003207,"ip_address":"广东","ucode":"5AB1499746C003","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg","comment_is_top":false,"comment_ctime":1722357165,"is_pvip":false,"replies":[{"id":142753,"content":"你好，在日志那一课里大概介绍了一些排查方法，实际中故障有很多情况，在CKA里也只是考了kubelet，所以就没有展开讲了。","user_name":"作者回复","user_name_real":"编辑","uid":1209006,"ctime":1722742155,"ip_address":"北京","comment_id":392976,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100780501,"comment_content":"老师怎么没有故障排查","like_count":0,"discussions":[{"author":{"id":1209006,"avatar":"https://static001.geekbang.org/account/avatar/00/12/72/ae/cef8a848.jpg","nickname":"雪飞","note":"","ucode":"36BDFD1E35E9DA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649081,"discussion_content":"你好，在日志那一课里大概介绍了一些排查方法，实际中故障有很多情况，在CKA里也只是考了kubelet，所以就没有展开讲了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1722742155,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]}]}