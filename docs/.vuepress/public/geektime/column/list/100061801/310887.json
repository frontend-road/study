{"id":310887,"title":"19 |  在分布式环境中，Leader选举、互斥锁和读写锁该如何实现？","content":"<p>你好，我是鸟窝。</p><p>在前面的课程里，我们学习的并发原语都是在进程内使用的，也就是我们常见的一个运行程序为了控制共享资源、实现任务编排和进行消息传递而提供的控制类型。在接下来的这两节课里，我要讲的是几个分布式的并发原语，它们控制的资源或编排的任务分布在不同进程、不同机器上。</p><p>分布式的并发原语实现更加复杂，因为在分布式环境中，网络状况、服务状态都是不可控的。不过还好有相应的软件系统去做这些事情。这些软件系统会专门去处理这些节点之间的协调和异常情况，并且保证数据的一致性。我们要做的就是在它们的基础上实现我们的业务。</p><p>常用来做协调工作的软件系统是Zookeeper、etcd、Consul之类的软件，Zookeeper为Java生态群提供了丰富的分布式并发原语（通过Curator库），但是缺少Go相关的并发原语库。Consul在提供分布式并发原语这件事儿上不是很积极，而etcd就提供了非常好的分布式并发原语，比如分布式互斥锁、分布式读写锁、Leader选举，等等。所以，今天，我就以etcd为基础，给你介绍几种分布式并发原语。</p><p>既然我们依赖etcd，那么，在生产环境中要有一个etcd集群，而且应该保证这个etcd集群是7*24工作的。在学习过程中，你可以使用一个etcd节点进行测试。</p><!-- [[[read_end]]] --><p>这节课我要介绍的就是Leader选举、互斥锁和读写锁。</p><h1>Leader选举</h1><p>Leader选举常常用在主从架构的系统中。主从架构中的服务节点分为主（Leader、Master）和从（Follower、Slave）两种角色，实际节点包括1主n从，一共是n+1个节点。</p><p>主节点常常执行写操作，从节点常常执行读操作，如果读写都在主节点，从节点只是提供一个备份功能的话，那么，主从架构就会退化成主备模式架构。</p><p>主从架构中最重要的是如何确定节点的角色，也就是，到底哪个节点是主，哪个节点是从？</p><p><strong>在同一时刻，系统中不能有两个主节点，否则，如果两个节点都是主，都执行写操作的话，就有可能出现数据不一致的情况，所以，我们需要一个选主机制，选择一个节点作为主节点，这个过程就是Leader选举</strong>。</p><p>当主节点宕机或者是不可用时，就需要新一轮的选举，从其它的从节点中选择出一个节点，让它作为新主节点，宕机的原主节点恢复后，可以变为从节点，或者被摘掉。</p><p>我们可以通过etcd基础服务来实现leader选举。具体点说，我们可以将Leader选举的逻辑交给etcd基础服务，这样，我们只需要把重心放在业务开发上。etcd基础服务可以通过多节点的方式保证7*24服务，所以，我们也不用担心Leader选举不可用的问题。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/78/47/78010df8677171d9bf29c64d346d9647.jpg?wh=4000*1710\" alt=\"\"></p><p>接下来，我会给你介绍业务开发中跟Leader选举相关的选举、查询、Leader变动监控等功能。</p><p>我要先提醒你一句，如果你想运行我下面讲到的测试代码，就要先部署一个etcd的集群，或者部署一个etcd节点做测试。</p><p>首先，我们来实现一个测试分布式程序的框架：它会先从命令行中读取命令，然后再执行相应的命令。你可以打开两个窗口，模拟不同的节点，分别执行不同的命令。</p><p>这个测试程序如下：</p><pre><code>package main\n\n// 导入所需的库\nimport (\n    &quot;bufio&quot;\n    &quot;context&quot;\n    &quot;flag&quot;\n    &quot;fmt&quot;\n    &quot;log&quot;\n    &quot;os&quot;\n    &quot;strconv&quot;\n    &quot;strings&quot;\n\n    &quot;github.com/coreos/etcd/clientv3&quot;\n    &quot;github.com/coreos/etcd/clientv3/concurrency&quot;\n)\n\n// 可以设置一些参数，比如节点ID\nvar (\n    nodeID    = flag.Int(&quot;id&quot;, 0, &quot;node ID&quot;)\n    addr      = flag.String(&quot;addr&quot;, &quot;http://127.0.0.1:2379&quot;, &quot;etcd addresses&quot;)\n    electName = flag.String(&quot;name&quot;, &quot;my-test-elect&quot;, &quot;election name&quot;)\n)\n\nfunc main() {\n    flag.Parse()\n\n    // 将etcd的地址解析成slice of string\n    endpoints := strings.Split(*addr, &quot;,&quot;)\n\n    // 生成一个etcd的clien\n    cli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer cli.Close()\n\n    // 创建session,如果程序宕机导致session断掉，etcd能检测到\n    session, err := concurrency.NewSession(cli)\n    defer session.Close()\n\n    // 生成一个选举对象。下面主要使用它进行选举和查询等操作\n    // 另一个方法ResumeElection可以使用既有的leader初始化Election\n    e1 := concurrency.NewElection(session, *electName)\n\n    // 从命令行读取命令\n    consolescanner := bufio.NewScanner(os.Stdin)\n    for consolescanner.Scan() {\n        action := consolescanner.Text()\n        switch action {\n        case &quot;elect&quot;: // 选举命令\n            go elect(e1, *electName)\n        case &quot;proclaim&quot;: // 只更新leader的value\n            proclaim(e1, *electName)\n        case &quot;resign&quot;: // 辞去leader,重新选举\n            resign(e1, *electName)\n        case &quot;watch&quot;: // 监控leader的变动\n            go watch(e1, *electName)\n        case &quot;query&quot;: // 查询当前的leader\n            query(e1, *electName)\n        case &quot;rev&quot;:\n            rev(e1, *electName)\n        default:\n            fmt.Println(&quot;unknown action&quot;)\n        }\n    }\n}\n</code></pre><p>部署完以后，我们就可以开始选举了。</p><h2>选举</h2><p>如果你的业务集群还没有主节点，或者主节点宕机了，你就需要发起新一轮的选主操作，主要会用到<strong>Campaign和Proclaim</strong>。如果你需要主节点放弃主的角色，让其它从节点有机会成为主节点，就可以调用<strong>Resign</strong>方法。</p><p>这里我提到了三个和选主相关的方法，下面我来介绍下它们的用法。</p><p><strong>第一个方法是Campaign</strong>。它的作用是，把一个节点选举为主节点，并且会设置一个值。它的签名如下所示：</p><pre><code>func (e *Election) Campaign(ctx context.Context, val string) error\n</code></pre><p>需要注意的是，这是一个阻塞方法，在调用它的时候会被阻塞，直到满足下面的三个条件之一，才会取消阻塞。</p><ol>\n<li>成功当选为主；</li>\n<li>此方法返回错误；</li>\n<li>ctx被取消。</li>\n</ol><p><strong>第二个方法是Proclaim</strong>。它的作用是，重新设置Leader的值，但是不会重新选主，这个方法会返回新值设置成功或者失败的信息。方法签名如下所示：</p><pre><code>func (e *Election) Proclaim(ctx context.Context, val string) error\n</code></pre><p><strong>第三个方法是Resign</strong>：开始新一次选举。这个方法会返回新的选举成功或者失败的信息。它的签名如下所示：</p><pre><code>func (e *Election) Resign(ctx context.Context) (err error)\n</code></pre><p>这三个方法的测试代码如下。你可以使用测试程序进行测试，具体做法是，启动两个节点，执行和这三个方法相关的命令。</p><pre><code>var count int\n// 选主\nfunc elect(e1 *concurrency.Election, electName string) {\n    log.Println(&quot;acampaigning for ID:&quot;, *nodeID)\n    // 调用Campaign方法选主,主的值为value-&lt;主节点ID&gt;-&lt;count&gt;\n    if err := e1.Campaign(context.Background(), fmt.Sprintf(&quot;value-%d-%d&quot;, *nodeID, count)); err != nil {\n        log.Println(err)\n    }\n    log.Println(&quot;campaigned for ID:&quot;, *nodeID)\n    count++\n}\n// 为主设置新值\nfunc proclaim(e1 *concurrency.Election, electName string) {\n    log.Println(&quot;proclaiming for ID:&quot;, *nodeID)\n    // 调用Proclaim方法设置新值,新值为value-&lt;主节点ID&gt;-&lt;count&gt;\n    if err := e1.Proclaim(context.Background(), fmt.Sprintf(&quot;value-%d-%d&quot;, *nodeID, count)); err != nil {\n        log.Println(err)\n    }\n    log.Println(&quot;proclaimed for ID:&quot;, *nodeID)\n    count++\n}\n// 重新选主，有可能另外一个节点被选为了主\nfunc resign(e1 *concurrency.Election, electName string) {\n    log.Println(&quot;resigning for ID:&quot;, *nodeID)\n    // 调用Resign重新选主\n    if err := e1.Resign(context.TODO()); err != nil {\n        log.Println(err)\n    }\n    log.Println(&quot;resigned for ID:&quot;, *nodeID)\n}\n</code></pre><h2>查询</h2><p>除了选举Leader，程序在启动的过程中，或者在运行的时候，还有可能需要查询当前的主节点是哪一个节点？主节点的值是什么？版本是多少？不光是主从节点需要查询和知道哪一个节点，在分布式系统中，还有其它一些节点也需要知道集群中的哪一个节点是主节点，哪一个节点是从节点，这样它们才能把读写请求分别发往相应的主从节点上。</p><p>etcd提供了查询当前Leader的方法<strong>Leader</strong>，如果当前还没有Leader，就返回一个错误，你可以使用这个方法来查询主节点信息。这个方法的签名如下：</p><pre><code>func (e *Election) Leader(ctx context.Context) (*v3.GetResponse, error)\n</code></pre><p>每次主节点的变动都会生成一个新的版本号，你还可以查询版本号信息（<strong>Rev</strong>方法），了解主节点变动情况：</p><pre><code>func (e *Election) Rev() int64\n</code></pre><p>你可以在测试完选主命令后，测试查询命令（query、rev），代码如下：</p><pre><code>// 查询主的信息\nfunc query(e1 *concurrency.Election, electName string) {\n    // 调用Leader返回主的信息，包括key和value等信息\n    resp, err := e1.Leader(context.Background())\n    if err != nil {\n        log.Printf(&quot;failed to get the current leader: %v&quot;, err)\n    }\n    log.Println(&quot;current leader:&quot;, string(resp.Kvs[0].Key), string(resp.Kvs[0].Value))\n}\n// 可以直接查询主的rev信息\nfunc rev(e1 *concurrency.Election, electName string) {\n    rev := e1.Rev()\n    log.Println(&quot;current rev:&quot;, rev)\n}\n</code></pre><h2>监控</h2><p>有了选举和查询方法，我们还需要一个监控方法。毕竟，如果主节点变化了，我们需要得到最新的主节点信息。</p><p>我们可以通过Observe来监控主的变化，它的签名如下：</p><pre><code>func (e *Election) Observe(ctx context.Context) &lt;-chan v3.GetResponse\n</code></pre><p>它会返回一个chan，显示主节点的变动信息。需要注意的是，它不会返回主节点的全部历史变动信息，而是只返回最近的一条变动信息以及之后的变动信息。</p><p>它的测试代码如下：</p><pre><code>func watch(e1 *concurrency.Election, electName string) {\n    ch := e1.Observe(context.TODO())\n\n\n    log.Println(&quot;start to watch for ID:&quot;, *nodeID)\n    for i := 0; i &lt; 10; i++ {\n        resp := &lt;-ch\n        log.Println(&quot;leader changed to&quot;, string(resp.Kvs[0].Key), string(resp.Kvs[0].Value))\n    }\n}\n</code></pre><p>etcd提供了选主的逻辑，而你要做的就是利用这些方法，让它们为你的业务服务。在使用的过程中，你还需要做一些额外的设置，比如查询当前的主节点、启动一个goroutine阻塞调用Campaign方法，等等。虽然你需要做一些额外的工作，但是跟自己实现一个分布式的选主逻辑相比，大大地减少了工作量。</p><p>接下来，我们继续看etcd提供的分布式并发原语：互斥锁。</p><h1>互斥锁</h1><p>互斥锁是非常常用的一种并发原语，我专门花了4讲的时间，重点介绍了互斥锁的功能、原理和易错场景。</p><p>不过，前面说的互斥锁都是用来保护同一进程内的共享资源的，今天，我们要掌握的是分布式环境中的互斥锁。<strong>我们要重点学习下分布在不同机器中的不同进程内的goroutine，如何利用分布式互斥锁来保护共享资源。</strong></p><p>互斥锁的应用场景和主从架构的应用场景不太一样。<strong>使用互斥锁的不同节点是没有主从这样的角色的，所有的节点都是一样的，只不过在同一时刻，只允许其中的一个节点持有锁</strong>。</p><p>下面，我们就来学习下互斥锁相关的两个原语，即Locker和Mutex。</p><h2>Locker</h2><p>etcd提供了一个简单的Locker原语，它类似于Go标准库中的sync.Locker接口，也提供了Lock/UnLock的机制：</p><pre><code>func NewLocker(s *Session, pfx string) sync.Locker\n</code></pre><p>可以看到，它的返回值是一个sync.Locker，因为你对标准库的Locker已经非常了解了，而且它只有Lock/Unlock两个方法，所以，接下来使用这个锁就非常容易了。下面的代码是一个使用Locker并发原语的例子：</p><pre><code>package main\n\nimport (\n    &quot;flag&quot;\n    &quot;log&quot;\n    &quot;math/rand&quot;\n    &quot;strings&quot;\n    &quot;time&quot;\n\n    &quot;github.com/coreos/etcd/clientv3&quot;\n    &quot;github.com/coreos/etcd/clientv3/concurrency&quot;\n)\n\nvar (\n    addr     = flag.String(&quot;addr&quot;, &quot;http://127.0.0.1:2379&quot;, &quot;etcd addresses&quot;)\n    lockName = flag.String(&quot;name&quot;, &quot;my-test-lock&quot;, &quot;lock name&quot;)\n)\n\nfunc main() {\n    flag.Parse()\n    \n    rand.Seed(time.Now().UnixNano())\n    // etcd地址\n    endpoints := strings.Split(*addr, &quot;,&quot;)\n    // 生成一个etcd client\n    cli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer cli.Close()\n    useLock(cli) // 测试锁\n}\n\nfunc useLock(cli *clientv3.Client) {\n    // 为锁生成session\n    s1, err := concurrency.NewSession(cli)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer s1.Close()\n    //得到一个分布式锁\n    locker := concurrency.NewLocker(s1, *lockName)\n\n    // 请求锁\n    log.Println(&quot;acquiring lock&quot;)\n    locker.Lock()\n    log.Println(&quot;acquired lock&quot;)\n\n    // 等待一段时间\n    time.Sleep(time.Duration(rand.Intn(30)) * time.Second)\n    locker.Unlock() // 释放锁\n\n    log.Println(&quot;released lock&quot;)\n}\n</code></pre><p>你可以同时在两个终端中运行这个测试程序。可以看到，它们获得锁是有先后顺序的，一个节点释放了锁之后，另外一个节点才能获取到这个分布式锁。</p><h2>Mutex</h2><p>事实上，刚刚说的Locker是基于Mutex实现的，只不过，Mutex提供了查询Mutex的key的信息的功能。测试代码也类似：</p><pre><code>func useMutex(cli *clientv3.Client) {\n    // 为锁生成session\n    s1, err := concurrency.NewSession(cli)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer s1.Close()\n    m1 := concurrency.NewMutex(s1, *lockName)\n\n    //在请求锁之前查询key\n    log.Printf(&quot;before acquiring. key: %s&quot;, m1.Key())\n    // 请求锁\n    log.Println(&quot;acquiring lock&quot;)\n    if err := m1.Lock(context.TODO()); err != nil {\n        log.Fatal(err)\n    }\n    log.Printf(&quot;acquired lock. key: %s&quot;, m1.Key())\n\n    //等待一段时间\n    time.Sleep(time.Duration(rand.Intn(30)) * time.Second)\n\n    // 释放锁\n    if err := m1.Unlock(context.TODO()); err != nil {\n        log.Fatal(err)\n    }\n    log.Println(&quot;released lock&quot;)\n}\n</code></pre><p>可以看到，Mutex并没有实现sync.Locker接口，它的Lock/Unlock方法需要提供一个context.Context实例做参数，这也就意味着，在请求锁的时候，你可以设置超时时间，或者主动取消请求。</p><h1>读写锁</h1><p>学完了分布式Locker和互斥锁Mutex，你肯定会联想到读写锁RWMutex。是的，etcd也提供了分布式的读写锁。不过，互斥锁Mutex是在github.com/coreos/etcd/clientv3/concurrency包中提供的，读写锁RWMutex却是在github.com/coreos/etcd/contrib/recipes包中提供的。</p><p>etcd提供的分布式读写锁的功能和标准库的读写锁的功能是一样的。只不过，<strong>etcd提供的读写锁，可以在分布式环境中的不同的节点使用</strong>。它提供的方法也和标准库中的读写锁的方法一致，分别提供了RLock/RUnlock、Lock/Unlock方法。下面的代码是使用读写锁的例子，它从命令行中读取命令，执行读写锁的操作：</p><pre><code>package main\n\n\nimport (\n    &quot;bufio&quot;\n    &quot;flag&quot;\n    &quot;fmt&quot;\n    &quot;log&quot;\n    &quot;math/rand&quot;\n    &quot;os&quot;\n    &quot;strings&quot;\n    &quot;time&quot;\n\n    &quot;github.com/coreos/etcd/clientv3&quot;\n    &quot;github.com/coreos/etcd/clientv3/concurrency&quot;\n    recipe &quot;github.com/coreos/etcd/contrib/recipes&quot;\n)\n\nvar (\n    addr     = flag.String(&quot;addr&quot;, &quot;http://127.0.0.1:2379&quot;, &quot;etcd addresses&quot;)\n    lockName = flag.String(&quot;name&quot;, &quot;my-test-lock&quot;, &quot;lock name&quot;)\n    action   = flag.String(&quot;rw&quot;, &quot;w&quot;, &quot;r means acquiring read lock, w means acquiring write lock&quot;)\n)\n\n\nfunc main() {\n    flag.Parse()\n    rand.Seed(time.Now().UnixNano())\n\n    // 解析etcd地址\n    endpoints := strings.Split(*addr, &quot;,&quot;)\n\n    // 创建etcd的client\n    cli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer cli.Close()\n    // 创建session\n    s1, err := concurrency.NewSession(cli)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer s1.Close()\n    m1 := recipe.NewRWMutex(s1, *lockName)\n\n    // 从命令行读取命令\n    consolescanner := bufio.NewScanner(os.Stdin)\n    for consolescanner.Scan() {\n        action := consolescanner.Text()\n        switch action {\n        case &quot;w&quot;: // 请求写锁\n            testWriteLocker(m1)\n        case &quot;r&quot;: // 请求读锁\n            testReadLocker(m1)\n        default:\n            fmt.Println(&quot;unknown action&quot;)\n        }\n    }\n}\n\nfunc testWriteLocker(m1 *recipe.RWMutex) {\n    // 请求写锁\n    log.Println(&quot;acquiring write lock&quot;)\n    if err := m1.Lock(); err != nil {\n        log.Fatal(err)\n    }\n    log.Println(&quot;acquired write lock&quot;)\n\n    // 等待一段时间\n    time.Sleep(time.Duration(rand.Intn(10)) * time.Second)\n\n    // 释放写锁\n    if err := m1.Unlock(); err != nil {\n        log.Fatal(err)\n    }\n    log.Println(&quot;released write lock&quot;)\n}\n\nfunc testReadLocker(m1 *recipe.RWMutex) {\n    // 请求读锁\n    log.Println(&quot;acquiring read lock&quot;)\n    if err := m1.RLock(); err != nil {\n        log.Fatal(err)\n    }\n    log.Println(&quot;acquired read lock&quot;)\n\n    // 等待一段时间\n    time.Sleep(time.Duration(rand.Intn(10)) * time.Second)\n\n    // 释放写锁\n    if err := m1.RUnlock(); err != nil {\n        log.Fatal(err)\n    }\n    log.Println(&quot;released read lock&quot;)\n}\n</code></pre><h1>总结</h1><p>自己实现分布式环境的并发原语，是相当困难的一件事，因为你需要考虑网络的延迟和异常、节点的可用性、数据的一致性等多种情况。</p><p>所以，我们可以借助etcd这样成熟的框架，基于它提供的分布式并发原语处理分布式的场景。需要注意的是，在使用这些分布式并发原语的时候，你需要考虑异常的情况，比如网络断掉等。同时，分布式并发原语需要网络之间的通讯，所以会比使用标准库中的并发原语耗时更长。</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/23/a18a98aa9ac5de17373c953484ee4c23.jpg?wh=2250*1040\" alt=\"\"></p><p>好了，这节课就到这里，下节课，我会带你继续学习其它的分布式并发原语，包括队列、栅栏和STM，敬请期待。</p><h1>思考题</h1><ol>\n<li>如果持有互斥锁或者读写锁的节点意外宕机了，它持有的锁会不会被释放？</li>\n<li>etcd提供的读写锁中的读和写有没有优先级？</li>\n</ol><p>欢迎在留言区写下你的思考和答案，我们一起交流讨论。如果你觉得有所收获，也欢迎你把今天的内容分享给你的朋友或同事。</p>","neighbors":{"left":{"article_title":"18 | 分组操作：处理一组子任务，该用什么并发原语？","id":310443},"right":{"article_title":"20 | 在分布式环境中，队列、栅栏和STM该如何实现？","id":312590}},"comments":[{"had_liked":false,"id":263944,"user_name":"鸟窝","can_delete":false,"product_type":"c1","uid":1066613,"ip_address":"","ucode":"E49D44F9613F17","user_header":"https://static001.geekbang.org/account/avatar/00/10/46/75/d35c7623.jpg","comment_is_top":true,"comment_ctime":1606301181,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"9.2233720728207995e+18","product_id":100061801,"comment_content":"这一讲和下一讲的代码在 https:&#47;&#47;github.com&#47;smallnest&#47;distributed","like_count":8,"discussions":[{"author":{"id":1364628,"avatar":"https://static001.geekbang.org/account/avatar/00/14/d2/94/c7223eb8.jpg","nickname":"白志稳","note":"","ucode":"6D47A362797C6B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399085,"discussion_content":"原来我们用的rpcx，就是老师写的呀，太强了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632904667,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":266980,"user_name":"Kepler","can_delete":false,"product_type":"c1","uid":1214303,"ip_address":"","ucode":"0C9CA3DB8B3CF0","user_header":"https://static001.geekbang.org/account/avatar/00/12/87/5f/6bf8b74a.jpg","comment_is_top":false,"comment_ctime":1607562269,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"31672333341","product_id":100061801,"comment_content":"类似zookeeper 的分布式锁原理，节点宕机对应session 销毁，持有的锁会被释放","like_count":7},{"had_liked":false,"id":275215,"user_name":"K菌无惨","can_delete":false,"product_type":"c1","uid":2194764,"ip_address":"","ucode":"97A532D588FD49","user_header":"","comment_is_top":false,"comment_ctime":1611392313,"is_pvip":false,"replies":[{"id":"100027","content":"对","user_name":"作者回复","user_name_real":"鸟窝","uid":"1066613","ctime":1611635173,"ip_address":"","comment_id":275215,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5906359609","product_id":100061801,"comment_content":"老师, Locker是超时解锁是通过NewSession时添加WithTTL这个SessionOption来设置的吗","like_count":1,"discussions":[{"author":{"id":1066613,"avatar":"https://static001.geekbang.org/account/avatar/00/10/46/75/d35c7623.jpg","nickname":"鸟窝","note":"","ucode":"E49D44F9613F17","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514266,"discussion_content":"对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611635173,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263311,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1606098689,"is_pvip":false,"discussion_count":0,"race_medal":1,"score":"5901065985","product_id":100061801,"comment_content":"关于思考题，<br>如果持有互斥锁或者读写锁的节点意外宕机了，从调用接口来看，与当前节点启动的session有关系，节点宕机之后，感觉应该有与该session相关的处理，比如超时机制，所以它持有的锁会被释放。<br>etcd 提供的读写锁，按照rwmutex的实现写锁应该比读锁优先级高，但是在分布式环境下，如此实现的话，我想会增加复杂度和出问题的几率。<br><br>","like_count":1},{"had_liked":false,"id":338803,"user_name":"文武木子","can_delete":false,"product_type":"c1","uid":1015986,"ip_address":"","ucode":"348752BDECD65F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/80/b2/2e9f442d.jpg","comment_is_top":false,"comment_ctime":1647746041,"is_pvip":false,"discussion_count":0,"race_medal":5,"score":"1647746041","product_id":100061801,"comment_content":"Redis实现分布式锁大家用的多吗","like_count":0},{"had_liked":false,"id":305557,"user_name":"tianfeiyu","can_delete":false,"product_type":"c1","uid":1157786,"ip_address":"","ucode":"E65E6841AD5D7F","user_header":"https://static001.geekbang.org/account/avatar/00/11/aa/9a/92d2df36.jpg","comment_is_top":false,"comment_ctime":1628046900,"is_pvip":false,"replies":[{"id":"110579","content":"redis最常用的就是分布式锁","user_name":"作者回复","user_name_real":"鸟窝","uid":"1066613","ctime":1628121549,"ip_address":"","comment_id":305557,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1628046900","product_id":100061801,"comment_content":"老师，问一下您这边有用过 redis 相关的分布式的并发原语库吗","like_count":0,"discussions":[{"author":{"id":1066613,"avatar":"https://static001.geekbang.org/account/avatar/00/10/46/75/d35c7623.jpg","nickname":"鸟窝","note":"","ucode":"E49D44F9613F17","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524429,"discussion_content":"redis最常用的就是分布式锁","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628121549,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":300500,"user_name":"types","can_delete":false,"product_type":"c1","uid":2449777,"ip_address":"","ucode":"8B50927EF1804F","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLDUJyeq54fiaXAgF62tNeocO3lHsKT4mygEcNoZLnibg6ONKicMgCgUHSfgW8hrMUXlwpNSzR8MHZwg/132","comment_is_top":false,"comment_ctime":1625197235,"is_pvip":false,"replies":[{"id":"118835","content":"我认为你这里的说的主节点和从节点,leader指的都是应用程序的，而不是etcd的。<br>1.没办法获取，分布式锁不负责这个职责。你可以通过etcd的节点实现，这也是服务发现实现的逻辑<br>2.主节点","user_name":"作者回复","user_name_real":"编辑","uid":"1066613","ctime":1639808465,"ip_address":"","comment_id":300500,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1625197235","product_id":100061801,"comment_content":"关于leader选举，几个问题：<br>1. 如何获取从节点的信息？？<br>2. leader选举成功后， resign是只有主节点可以发起吗，还是从节点也可以发起resign","like_count":0,"discussions":[{"author":{"id":1066613,"avatar":"https://static001.geekbang.org/account/avatar/00/10/46/75/d35c7623.jpg","nickname":"鸟窝","note":"","ucode":"E49D44F9613F17","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539713,"discussion_content":"我认为你这里的说的主节点和从节点,leader指的都是应用程序的，而不是etcd的。\n1.没办法获取，分布式锁不负责这个职责。你可以通过etcd的节点实现，这也是服务发现实现的逻辑\n2.主节点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639808465,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263300,"user_name":"myrfy","can_delete":false,"product_type":"c1","uid":1169401,"ip_address":"","ucode":"2814BAE5D70098","user_header":"","comment_is_top":false,"comment_ctime":1606096165,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1606096165","product_id":100061801,"comment_content":"还没来得及去看etcd库的代码，盲猜一下。<br>第一个问题，我觉得要看场景，如果被锁住的资源可以被重新分配，我相信etcd能检测到持有锁的节点断开，concurrent包里应该有相关的实现把锁释放。但是，如果被锁住的资源非常重要，影响到整个系统的状态，必须要人工介入才能把破损的数据修复，那这个时候自动释放锁反而可能完成更大规模的损失。<br>第二个问题，还是得去看代码再说","like_count":0}]}