{"id":70211,"title":"42 | Kubernetes默认调度器调度策略解析","content":"<p>你好，我是张磊。今天我和你分享的主题是：Kubernetes默认调度器调度策略解析。</p><p>在上一篇文章中，我主要为你讲解了Kubernetes默认调度器的设计原理和架构。在今天这篇文章中，我们就专注在调度过程中Predicates和Priorities这两个调度策略主要发生作用的阶段。</p><p><span class=\"orange\">首先，我们一起看看Predicates。</span></p><p><strong>Predicates在调度过程中的作用，可以理解为Filter</strong>，即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度Pod的宿主机。</p><p>而在Kubernetes中，默认的调度策略有如下四种。</p><p><strong>第一种类型，叫作GeneralPredicates。</strong></p><p>顾名思义，这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources计算的就是宿主机的CPU和内存资源等是否够用。</p><p>当然，我在前面已经提到过，PodFitsResources检查的只是 Pod 的 requests 字段。需要注意的是，Kubernetes 的调度器并没有为 GPU 等硬件资源定义具体的资源类型，而是统一用一种名叫 Extended Resource的、Key-Value 格式的扩展字段来描述的。比如下面这个例子：</p><!-- [[[read_end]]] --><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: extended-resource-demo\nspec:\n  containers:\n  - name: extended-resource-demo-ctr\n    image: nginx\n    resources:\n      requests:\n        alpha.kubernetes.io/nvidia-gpu: 2\n      limits:\n        alpha.kubernetes.io/nvidia-gpu: 2\n</code></pre><p>可以看到，我们这个 Pod 通过<code>alpha.kubernetes.io/nvidia-gpu=2</code>这样的定义方式，声明使用了两个 NVIDIA 类型的 GPU。</p><p>而在PodFitsResources里面，调度器其实并不知道这个字段 Key 的含义是 GPU，而是直接使用后面的 Value 进行计算。当然，在 Node 的Capacity字段里，你也得相应地加上这台宿主机上 GPU的总数，比如：<code>alpha.kubernetes.io/nvidia-gpu=4</code>。这些流程，我在后面讲解 Device Plugin 的时候会详细介绍。</p><p>而PodFitsHost检查的是，宿主机的名字是否跟Pod的spec.nodeName一致。</p><p>PodFitsHostPorts检查的是，Pod申请的宿主机端口（spec.nodePort）是不是跟已经被使用的端口有冲突。</p><p>PodMatchNodeSelector检查的是，Pod的nodeSelector或者nodeAffinity指定的节点，是否与待考察节点匹配，等等。</p><p>可以看到，像上面这样一组GeneralPredicates，正是Kubernetes考察一个Pod能不能运行在一个Node上最基本的过滤条件。所以，GeneralPredicates也会被其他组件（比如kubelet）直接调用。</p><p>我在上一篇文章中已经提到过，kubelet在启动Pod前，会执行一个Admit操作来进行二次确认。这里二次确认的规则，就是执行一遍GeneralPredicates。</p><p><strong>第二种类型，是与Volume相关的过滤规则。</strong></p><p>这一组过滤规则，负责的是跟容器持久化Volume相关的调度策略。</p><p>其中，NoDiskConflict检查的条件，是多个Pod声明挂载的持久化Volume是否有冲突。比如，AWS EBS类型的Volume，是不允许被两个Pod同时使用的。所以，当一个名叫A的EBS Volume已经被挂载在了某个节点上时，另一个同样声明使用这个A Volume的Pod，就不能被调度到这个节点上了。</p><p>而MaxPDVolumeCountPredicate检查的条件，则是一个节点上某种类型的持久化Volume是不是已经超过了一定数目，如果是的话，那么声明使用该类型持久化Volume的Pod就不能再调度到这个节点了。</p><p>而VolumeZonePredicate，则是检查持久化Volume的Zone（高可用域）标签，是否与待考察节点的Zone标签相匹配。</p><p>此外，这里还有一个叫作VolumeBindingPredicate的规则。它负责检查的，是该Pod对应的PV的nodeAffinity字段，是否跟某个节点的标签相匹配。</p><p>在前面的第29篇文章<a href=\"https://time.geekbang.org/column/article/42819\">《PV、PVC体系是不是多此一举？从本地持久化卷谈起》</a>中，我曾经为你讲解过，Local Persistent Volume（本地持久化卷），必须使用nodeAffinity来跟某个具体的节点绑定。这其实也就意味着，在Predicates阶段，Kubernetes就必须能够根据Pod的Volume属性来进行调度。</p><p>此外，如果该Pod的PVC还没有跟具体的PV绑定的话，调度器还要负责检查所有待绑定PV，当有可用的PV存在并且该PV的nodeAffinity与待考察节点一致时，这条规则才会返回“成功”。比如下面这个例子：</p><pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: example-local-pv\nspec:\n  capacity:\n    storage: 500Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-storage\n  local:\n    path: /mnt/disks/vol1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - my-node\n</code></pre><p>可以看到，这个 PV 对应的持久化目录，只会出现在名叫 my-node 的宿主机上。所以，任何一个通过 PVC 使用这个 PV 的 Pod，都必须被调度到 my-node 上才可以正常工作。VolumeBindingPredicate，正是调度器里完成这个决策的位置。</p><p><strong>第三种类型，是宿主机相关的过滤规则。</strong></p><p>这一组规则，主要考察待调度 Pod 是否满足 Node 本身的某些条件。</p><p>比如，PodToleratesNodeTaints，负责检查的就是我们前面经常用到的Node 的“污点”机制。只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配的时候，这个 Pod 才能被调度到该节点上。</p><blockquote>\n<p>备注：这里，你也可以再回顾下第21篇文章<a href=\"https://time.geekbang.org/column/article/41366\">《容器化守护进程的意义：DaemonSet》</a>中的相关内容。</p>\n</blockquote><p>而NodeMemoryPressurePredicate，检查的是当前节点的内存是不是已经不够充足，如果是的话，那么待调度 Pod 就不能被调度到该节点上。</p><p><strong>第四种类型，是 Pod 相关的过滤规则。</strong></p><p>这一组规则，跟 GeneralPredicates大多数是重合的。而比较特殊的，是PodAffinityPredicate。这个规则的作用，是检查待调度 Pod 与 Node 上的已有Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系。比如下面这个例子：</p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-antiaffinity\nspec:\n  affinity:\n    podAntiAffinity: \n      requiredDuringSchedulingIgnoredDuringExecution: \n      - weight: 100  \n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: security \n              operator: In \n              values:\n              - S2\n          topologyKey: kubernetes.io/hostname\n  containers:\n  - name: with-pod-affinity\n    image: docker.io/ocpqe/hello-pod\n</code></pre><p>这个例子里的podAntiAffinity规则，就指定了这个 Pod 不希望跟任何携带了 security=S2 标签的 Pod 存在于同一个 Node 上。需要注意的是，PodAffinityPredicate是有作用域的，比如上面这条规则，就仅对携带了Key 是<code>kubernetes.io/hostname</code>标签的 Node 有效。这正是topologyKey这个关键词的作用。</p><p>而与podAntiAffinity相反的，就是podAffinity，比如下面这个例子：</p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity: \n      requiredDuringSchedulingIgnoredDuringExecution: \n      - labelSelector:\n          matchExpressions:\n          - key: security \n            operator: In \n            values:\n            - S1 \n        topologyKey: failure-domain.beta.kubernetes.io/zone\n  containers:\n  - name: with-pod-affinity\n    image: docker.io/ocpqe/hello-pod\n</code></pre><p>这个例子里的 Pod，就只会被调度到已经有携带了 security=S1标签的 Pod 运行的 Node 上。而这条规则的作用域，则是所有携带 Key 是<code>failure-domain.beta.kubernetes.io/zone</code>标签的 Node。</p><p>此外，上面这两个例子里的requiredDuringSchedulingIgnoredDuringExecution字段的含义是：这条规则必须在Pod 调度时进行检查（requiredDuringScheduling）；但是如果是已经在运行的Pod 发生变化，比如 Label 被修改，造成了该 Pod 不再适合运行在这个 Node 上的时候，Kubernetes 不会进行主动修正（IgnoredDuringExecution）。</p><p>上面这四种类型的Predicates，就构成了调度器确定一个 Node 可以运行待调度 Pod 的基本策略。</p><p><strong>在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动16个Goroutine，来并发地为集群里的所有Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。</strong></p><p>需要注意的是，在为每个 Node 执行 Predicates 时，调度器会按照固定的顺序来进行检查。这个顺序，是按照 Predicates 本身的含义来确定的。比如，宿主机相关的Predicates 会被放在相对靠前的位置进行检查。要不然的话，在一台资源已经严重不足的宿主机上，上来就开始计算 PodAffinityPredicate，是没有实际意义的。</p><p><span class=\"orange\">接下来，我们再来看一下 Priorities。</span></p><p>在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是0-10分，得分最高的节点就是最后被 Pod 绑定的最佳节点。</p><p>Priorities 里最常用到的一个打分规则，是LeastRequestedPriority。它的计算方法，可以简单地总结为如下所示的公式：</p><pre><code>score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2\n</code></pre><p>可以看到，这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。</p><p>而与LeastRequestedPriority一起发挥作用的，还有BalancedResourceAllocation。它的计算公式如下所示：</p><pre><code>score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10\n</code></pre><p>其中，每种资源的 Fraction 的定义是 ：Pod 请求的资源/节点上的可用资源。而 variance 算法的作用，则是计算每两种资源 Fraction 之间的“距离”。而最后选择的，则是资源 Fraction 差距最小的节点。</p><p>所以说，BalancedResourceAllocation选择的，其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。</p><p>此外，还有NodeAffinityPriority、TaintTolerationPriority和InterPodAffinityPriority这三种 Priority。顾名思义，它们与前面的PodMatchNodeSelector、PodToleratesNodeTaints和 PodAffinityPredicate这三个 Predicate 的含义和计算方法是类似的。但是作为 Priority，一个 Node 满足上述规则的字段数目越多，它的得分就会越高。</p><p>在默认 Priorities 里，还有一个叫作ImageLocalityPriority的策略。它是在 Kubernetes v1.12里新开启的调度规则，即：如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node 上，那么这些Node 的得分就会比较高。</p><p>当然，为了避免这个算法引发调度堆叠，调度器在计算得分的时候还会根据镜像的分布进行优化，即：如果大镜像分布的节点数目很少，那么这些节点的权重就会被调低，从而“对冲”掉引起调度堆叠的风险。</p><p>以上，就是 Kubernetes 调度器的 Predicates 和 Priorities 里默认调度规则的主要工作原理了。</p><p><strong>在实际的执行过程中，调度器里关于集群和 Pod 的信息都已经缓存化，所以这些算法的执行过程还是比较快的。</strong></p><p>此外，对于比较复杂的调度算法来说，比如PodAffinityPredicate，它们在计算的时候不只关注待调度 Pod 和待考察 Node，还需要关注整个集群的信息，比如，遍历所有节点，读取它们的 Labels。这时候，Kubernetes 调度器会在为每个待调度 Pod 执行该调度算法之前，先将算法需要的集群信息初步计算一遍，然后缓存起来。这样，在真正执行该算法的时候，调度器只需要读取缓存信息进行计算即可，从而避免了为每个 Node 计算 Predicates 的时候反复获取和计算整个集群的信息。</p><h2>总结</h2><p>在本篇文章中，我为你讲述了 Kubernetes 默认调度器里的主要调度算法。</p><p>需要注意的是，除了本篇讲述的这些规则，Kubernetes 调度器里其实还有一些默认不会开启的策略。你可以通过为kube-scheduler 指定一个配置文件或者创建一个 ConfigMap ，来配置哪些规则需要开启、哪些规则需要关闭。并且，你可以通过为 Priorities 设置权重，来控制调度器的调度行为。</p><h2>思考题</h2><p>请问，如何能够让 Kubernetes 的调度器尽可能地将 Pod 分布在不同机器上，避免“堆叠”呢？请简单描述下你的算法。</p><p>感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。</p>","neighbors":{"left":{"article_title":"41 | 十字路口上的Kubernetes默认调度器","id":69890},"right":{"article_title":"43 | Kubernetes默认调度器的优先级与抢占机制","id":70519}},"comments":[{"had_liked":false,"id":186303,"user_name":"芒果少侠","can_delete":false,"product_type":"c1","uid":1350159,"ip_address":"","ucode":"98D0BBB52BB80F","user_header":"https://static001.geekbang.org/account/avatar/00/14/9a/0f/da7ed75a.jpg","comment_is_top":false,"comment_ctime":1583822544,"is_pvip":true,"discussion_count":3,"race_medal":0,"score":"285051664080","product_id":100015201,"comment_content":"思考题答案，个人认为有三个解决思路<br>1. 为pod.yaml设置PreferredDuringSchedulingIgnoredDuringExecution（注意不是required），可以指定【不想和同一个label的pod】放在一起。调度器随后会根据node上不满足podAntiAffinity的pod数量打分，如果不想一起的pod数量越多分数越少。就能够尽量打散同一个service下多个副本pod的分布。<br>关于这一思路，k8s官网也给出了相同应用的例子。【preferredDuringSchedulingIgnoredDuringExecution 反亲和性的一个例子是 “在整个域内平均分布这个服务的所有 pod”（这里如果用一个硬性的要求是不可行的，因为您可能要创建比域更多的 pod）。】 -- https:&#47;&#47;k8smeetup.github.io&#47;docs&#47;concepts&#47;configuration&#47;assign-pod-node&#47;<br><br>2. SelectorSpreadPriority，kubernetes内置的一个priority策略。具体：与services上其他pod尽量不在同一个节点上，节点上同一个Service里pod数量越少得分越高。<br>3. 自定义策略，实现自己的负载均衡算法（一次性哈希等）。<br><br>参考资料：<br>0. PreferredDuringSchedulingIgnoredDuringExecution ：在调度期间尽量满足亲和性或者反亲和性规则，如果不能满足规则，POD也有可能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。<br>1. https:&#47;&#47;wilhelmguo.cn&#47;blog&#47;post&#47;william&#47;Kubernetes%E8%B0%83%E5%BA%A6%E8%AF%A6%E8%A7%A3<br>2. https:&#47;&#47;blog.fleeto.us&#47;post&#47;adv-scheduler-in-k8s&#47;<br>3. https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;56088355<br>4. https:&#47;&#47;kuboard.cn&#47;learning&#47;k8s-advanced&#47;schedule&#47;#filtering","like_count":66,"discussions":[{"author":{"id":3161045,"avatar":"","nickname":"Geek_2b95e9","note":"","ucode":"951F1ABD43892C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585022,"discussion_content":"mark","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661296872,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"吉林"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1340955,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/FheCgo4OvibofpdXVyWzev07tDHpqZ5CSArjLZ10kDMDwN7sUk3AHLUsuDUWk9KZEnSTWgXoLicn18UhsGgMfzrg/132","nickname":"ZeWe","note":"","ucode":"F02CBFC395527B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":554198,"discussion_content":"mark 学到了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646268163,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1006424,"avatar":"","nickname":"门窗小二","note":"","ucode":"0BF3780C247F22","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377655,"discussion_content":"mark","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622767025,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":49355,"user_name":"Dale","can_delete":false,"product_type":"c1","uid":1010056,"ip_address":"","ucode":"DD4717C06E417D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/69/88/528442b0.jpg","comment_is_top":false,"comment_ctime":1544670055,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"53084277607","product_id":100015201,"comment_content":"在工作中遇到这个问题，需要将pod尽量分散在不通的node上，通过kubernetes提供的反亲和性来解决的。从回答中老师希望从Priorities阶段中实现，我的想法如下：<br>1、首先在Predicates阶段，已经选择出来一组可以使用的node节点<br>2、在Priorities阶段，根据资源可用情况将node从大到小排序，加上node节点个数为m<br>3、根据pod中配置replicate个数n，在node列表中进行查找出资源可用的top n的node节点<br>4、如果node节点个数m不满足pod中的replicate个数，每次选择top m之后，重新计算node的资源可用情况，在选择top（n-m）的node，会存在node上有多个情况，最大程度上保证pod分散在不同的node上","like_count":12,"discussions":[{"author":{"id":1350159,"avatar":"https://static001.geekbang.org/account/avatar/00/14/9a/0f/da7ed75a.jpg","nickname":"芒果少侠","note":"","ucode":"98D0BBB52BB80F","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":201641,"discussion_content":"但是pod的出队方式是每次出队一个，应该没办法这样一次批量调度吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583817773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":44416,"user_name":"freeman","can_delete":false,"product_type":"c1","uid":1296242,"ip_address":"","ucode":"B766B91AFC3398","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/ZwGweFhVUTfOrrYRk6Dic1IBxFyj2ZgsI1UXQeic2B5uJFdjicsIicnKrJts9v7nGUTCQlSKNUpmvYULq5KjqWjU4g/132","comment_is_top":false,"comment_ctime":1543424787,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"35903163155","product_id":100015201,"comment_content":"首先筛选出满足资源的机器。如果可用节点大于等于需求副本集则一个node一份，顺序取node调度即可，如果node节点少于副本数量，则进行一次调度后，剩下的副本重复上面的事情。直到为每个副本找到对应node或者调度失败。","like_count":8,"discussions":[{"author":{"id":1293354,"avatar":"https://static001.geekbang.org/account/avatar/00/13/bc/2a/00a3d488.jpg","nickname":"gl328518397","note":"","ucode":"05AF4661EF0AAF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":18416,"discussion_content":"随机的取node怎么样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569056578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319629,"user_name":"陈斯佳","can_delete":false,"product_type":"c1","uid":1259323,"ip_address":"","ucode":"C236F874FC767A","user_header":"https://static001.geekbang.org/account/avatar/00/13/37/3b/495e2ce6.jpg","comment_is_top":false,"comment_ctime":1635895995,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27405699771","product_id":100015201,"comment_content":"第四十二课:Kubernetes默认调度器调度策略解析<br>Predicates在调度过程中的作用，可以理解为Filter，也就是按照调度策略从当前的集群所有节点中“过滤”出一些符合条件的节点来运行调度的 Pod。<br><br>默认的调度策略有三种，一种是GerenalPredicates，负责最基础的调度策略，比如计算宿主机CPU和内存资源等是否够用的PodFitsResource；还有检查宿主机名字是否和Pod的spec.nodeName一致的PodFitsHost。第二类是和Volume相关的过滤规则，比如NoDiskConflict是检查多个Pod申明挂载的持久化Volume是否有冲突。第三类是宿主机相关的过滤条件，主要考察待调度的Pod是否满足Node本身条件，比PodToleratesNodeTaints，负责检查Node 的“污点”taint机制，而 NodeMemoryPressurePredicate，检查的是当前节点的内存是不是已经不够充足，如果是的话，那么待调度 Pod 就不能被调度到该节点上。第四种类型是和Pod相关的过滤规则，这一组规则，跟 GeneralPredicates 大多数是重合的。而比较特殊的，是 PodAffinityPredicate。在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个 Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。<br><br>在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。Priorities 里最常用到的一个打分规则，是LeastRequestedPriority。这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。此外，还有 NodeAffinityPriority、TaintTolerationPriority 和 InterPodAffinityPriority 这三种 Priority。在默认 Priorities 里，还有一个叫作 ImageLocalityPriority 的策略。它是在 Kubernetes v1.12 里新开启的调度规则，即：如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node 上，那么这些 Node 的得分就会比较高。","like_count":6},{"had_liked":false,"id":167186,"user_name":"陈小白( ´･ᴗ･` )","can_delete":false,"product_type":"c1","uid":1334864,"ip_address":"","ucode":"A92C85374A711B","user_header":"https://static001.geekbang.org/account/avatar/00/14/5e/50/d2cdb05c.jpg","comment_is_top":false,"comment_ctime":1577710569,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"18757579753","product_id":100015201,"comment_content":"这个我们线上就遇到了，一开始编排好差不多100个系统，发现其中几台主机一堆pod ，内存cpu 都很吃紧，而其他的主机却十分空闲。重启也没用。另外还想请教老师一个问题，我们遇到主机内存不足的时候，经常出现docker  hang 住了，就是docker 的命令完全卡死，没反应，不知道老师有遇到过呢？","like_count":5,"discussions":[{"author":{"id":2439847,"avatar":"","nickname":"李德华","note":"","ucode":"9F6E4ED0725A1B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":564746,"discussion_content":"有几台主机一堆POD的问题解决了吗？怎么解决的呀？我也遇到这个问题了。现在是其中一台上pod的多，内存使用率高，其他几台比较空闲。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650327169,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1207457,"avatar":"https://static001.geekbang.org/account/avatar/00/12/6c/a1/80d83f0a.jpg","nickname":"Ellison","note":"","ucode":"A2FB94D4F6A332","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":351347,"discussion_content":"重启docker 或者docker stash 看看那个进程用的多资源","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614252386,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":56511,"user_name":"鱼自由","can_delete":false,"product_type":"c1","uid":1101806,"ip_address":"","ucode":"4EA108884BE545","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJFE8tbvsHARh0SaZk4aMBD4l3LQgicS44vPje09EWOES0ls4Q5vv4unsgJJZCm4P9ia8TYWa3kWsvA/132","comment_is_top":false,"comment_ctime":1546479807,"is_pvip":false,"replies":[{"id":"21227","content":"google一下 kube-scheduler policy。是一个配置文件或者配置yaml。","user_name":"作者回复","user_name_real":"Geek_6ef93d","uid":"1218095","ctime":1547185453,"ip_address":"","comment_id":56511,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18726348991","product_id":100015201,"comment_content":"老师，最后你提到为 Priorities 设置权重，请问，这个操作在哪里进行？","like_count":4,"discussions":[{"author":{"id":1218095,"avatar":"https://static001.geekbang.org/account/avatar/00/12/96/2f/876085fa.jpg","nickname":"张磊 Kubernetes","note":"","ucode":"16E29BDAB1F5BC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":435082,"discussion_content":"google一下 kube-scheduler policy。是一个配置文件或者配置yaml。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1547185453,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":44270,"user_name":"tyamm","can_delete":false,"product_type":"c1","uid":1314513,"ip_address":"","ucode":"092C3173AE0AF0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/c6wA5mBibxZWTgatn1xbfsTFZ42bvzYOvicyaOtMREicmaVFntPBwjvhnkPgQ4ZlJgagJSN2oxpvEwYSt5SGOiarzg/132","comment_is_top":false,"comment_ctime":1543396994,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18723266178","product_id":100015201,"comment_content":"老师，我有个疑问。这个课程后面会讲到如何搭建master节点高可用么？？","like_count":4},{"had_liked":false,"id":143982,"user_name":"王景迁","can_delete":false,"product_type":"c1","uid":1360656,"ip_address":"","ucode":"4CD9A1179AE084","user_header":"https://static001.geekbang.org/account/avatar/00/14/c3/10/3f18e402.jpg","comment_is_top":false,"comment_ctime":1571819983,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14456721871","product_id":100015201,"comment_content":"根据老师上面说的，默认的调度策略不是应该有四种类型吗？为什么文章开头说是三种类型？","like_count":3,"discussions":[{"author":{"id":1117803,"avatar":"https://static001.geekbang.org/account/avatar/00/11/0e/6b/9ee9f422.jpg","nickname":"wx","note":"","ucode":"7AA8F0FEFD1128","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385824,"discussion_content":"老师解释了,  第四种类型: 是 Pod 相关的过滤规则 &#34;跟 GeneralPredicates 大多数是重合的&#34;","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627288955,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":44147,"user_name":"Alex","can_delete":false,"product_type":"c1","uid":1246046,"ip_address":"","ucode":"68E92087D486E0","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/5e/818a8b1b.jpg","comment_is_top":false,"comment_ctime":1543373413,"is_pvip":false,"replies":[{"id":"15866","content":"这就成了predicate了，我们希望的是一个priority ","user_name":"作者回复","user_name_real":"Geek_6ef93d","uid":"1218095","ctime":1543417830,"ip_address":"","comment_id":44147,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14428275301","product_id":100015201,"comment_content":"podAntiAffinity:<br>          requiredDuringSchedulingIgnoredDuringExecution:<br>          - labelSelector:<br>              matchExpressions:<br>              - key: name<br>                operator: In<br>                values:<br>                - nginx-demo<br>            topologyKey: &quot;kubernetes.io&#47;hostname&quot;<br><br>我用了反亲和的特性让pod分到了不同的机器上，不知道是否回答了你的问题","like_count":3,"discussions":[{"author":{"id":1218095,"avatar":"https://static001.geekbang.org/account/avatar/00/12/96/2f/876085fa.jpg","nickname":"张磊 Kubernetes","note":"","ucode":"16E29BDAB1F5BC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430650,"discussion_content":"这就成了predicate了，我们希望的是一个priority ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543417830,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":46729,"user_name":"小朱","can_delete":false,"product_type":"c1","uid":1015550,"ip_address":"","ucode":"8D379085223EA9","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7e/fe/84914832.jpg","comment_is_top":false,"comment_ctime":1543974859,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5838942155","product_id":100015201,"comment_content":"请教一个问题，自己启动了另一个scheduler，某一个node上同时存在default-scheduler和second-scheduler调度的资源。但是scheduler只统计schedulerName是自己的pod，这样就和node上面kubelet统计的资源就出现了不一致，这种设计是为什么呢？","like_count":1},{"had_liked":false,"id":44328,"user_name":"周娄子","can_delete":false,"product_type":"c1","uid":1228039,"ip_address":"","ucode":"18A1E6926C095A","user_header":"https://static001.geekbang.org/account/avatar/00/12/bd/07/bf8c31fb.jpg","comment_is_top":false,"comment_ctime":1543409009,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5838376305","product_id":100015201,"comment_content":"写一个类似nginx一致性hash的算法","like_count":1},{"had_liked":false,"id":360304,"user_name":"浅陌","can_delete":false,"product_type":"c1","uid":2031603,"ip_address":"上海","ucode":"1C80224154E747","user_header":"https://static001.geekbang.org/account/avatar/00/1e/ff/f3/de2233f5.jpg","comment_is_top":false,"comment_ctime":1666413975,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666413975","product_id":100015201,"comment_content":"请问默认情况下，这些predicate的算法和priority的算法都会被执行吗","like_count":0},{"had_liked":false,"id":358642,"user_name":"zhoufeng","can_delete":false,"product_type":"c1","uid":1447741,"ip_address":"广东","ucode":"6F92F7866F9EB4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKq0oQVibKcmYJqmpqaNNQibVgia7EsEgW65LZJIpDZBMc7FyMcs7J1JmFCtp06pY8ibbcpW4ibRtG7Frg/132","comment_is_top":false,"comment_ctime":1664507701,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1664507701","product_id":100015201,"comment_content":"Predicates 单词字面意思是“谓词”，谁知道为啥调度算法要取这个名字，有什么说法吗","like_count":0},{"had_liked":false,"id":350212,"user_name":"窝窝头","can_delete":false,"product_type":"c1","uid":1063866,"ip_address":"","ucode":"5C2635ED6484F8","user_header":"https://static001.geekbang.org/account/avatar/00/10/3b/ba/3b30dcde.jpg","comment_is_top":false,"comment_ctime":1656662510,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1656662510","product_id":100015201,"comment_content":"避免堆叠的话，一个是将资源剩余量的节点权重增高，镜像较多的节点权重降低，避免pod都调度到一起去","like_count":0},{"had_liked":false,"id":322438,"user_name":"刘超","can_delete":false,"product_type":"c1","uid":2649310,"ip_address":"","ucode":"78B617AADA9035","user_header":"https://static001.geekbang.org/account/avatar/00/28/6c/de/1934abe6.jpg","comment_is_top":false,"comment_ctime":1637386479,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637386479","product_id":100015201,"comment_content":"你们好问个问题，关于POD选择调度NODE的预选、优选过程。 通过预选后，返回可调度NODE节点列表，再对筛选出来的的NODE进行&quot;优选打分&quot;，那么在哪里可以清晰的看到每个优选的NODE节点的得分呢？","like_count":0},{"had_liked":false,"id":306624,"user_name":"梁尔诗","can_delete":false,"product_type":"c1","uid":1171987,"ip_address":"","ucode":"19E1BCF01CAF3F","user_header":"https://static001.geekbang.org/account/avatar/00/11/e2/13/ae1c85ec.jpg","comment_is_top":false,"comment_ctime":1628648615,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1628648615","product_id":100015201,"comment_content":"都可以用反亲和性实现 required硬亲和为预选阶段执行  prefered为优选阶段执行","like_count":0},{"had_liked":false,"id":305631,"user_name":"Spark","can_delete":false,"product_type":"c1","uid":2336571,"ip_address":"","ucode":"5965AF24CBE056","user_header":"https://static001.geekbang.org/account/avatar/00/23/a7/3b/d4f90be0.jpg","comment_is_top":false,"comment_ctime":1628078766,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1628078766","product_id":100015201,"comment_content":"kubelet 在启动 Pod 前，会执行一个 Admit 操作来进行二次确认。这里二次确认的规则，就是执行一遍 GeneralPredicates。<br>原文这句话对应的源码我看了一下，好像没有podfitshost，podfitsnodes这类检查吧","like_count":0},{"had_liked":false,"id":298298,"user_name":"Ilovek8s","can_delete":false,"product_type":"c1","uid":1542450,"ip_address":"","ucode":"64DF0F7D0CF0B0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/a8PMLmCTCBa40j7JIy3d8LsdbW5hne7lkk9KOGQuiaeVk4cn06KWwlP3ic69BsQLpNFtRTjRdUM2ySDBAv1MOFfA/132","comment_is_top":false,"comment_ctime":1624005619,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1624005619","product_id":100015201,"comment_content":"Required Scheduling During Required Scheduling Execute","like_count":0},{"had_liked":false,"id":292513,"user_name":"long","can_delete":false,"product_type":"c1","uid":1939092,"ip_address":"","ucode":"4D5A4BFD5EDFDF","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/RRQerM7wWWB23jYmINBltjKhicIFTmWMQyGcy64PIRk7RO2vBhdnegLs3oFicVs65zIUxy0CsxyN15AexQUY3WqA/132","comment_is_top":false,"comment_ctime":1620868591,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1620868591","product_id":100015201,"comment_content":"张老师您好，请教一个问题：<br>请问schedule在调度打分的时候会考虑node当前的实际使用率吗？ 谢谢！","like_count":0,"discussions":[{"author":{"id":1199969,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4f/61/00083e41.jpg","nickname":"小白","note":"","ucode":"7ACE14C0C4AE61","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":374646,"discussion_content":"不会。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621301295,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292441,"user_name":"三颗豆子","can_delete":false,"product_type":"c1","uid":2008638,"ip_address":"","ucode":"632CE3E7563666","user_header":"https://static001.geekbang.org/account/avatar/00/1e/a6/3e/3d18f35a.jpg","comment_is_top":false,"comment_ctime":1620822526,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1620822526","product_id":100015201,"comment_content":"是不是找个函数，让Priority的权重设置为“该节点上POD数量越多，权重越低”这样就好了。","like_count":0},{"had_liked":false,"id":256944,"user_name":"求渔","can_delete":false,"product_type":"c1","uid":1103671,"ip_address":"","ucode":"6EABC5A0C703C9","user_header":"https://static001.geekbang.org/account/avatar/00/10/d7/37/d8c8acdf.jpg","comment_is_top":false,"comment_ctime":1603790630,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1603790630","product_id":100015201,"comment_content":"自定义Scoring插件中的priority算法， 使用各节点pod与平均节点pod数之间的方差的剩余百分比来打分","like_count":0},{"had_liked":false,"id":204133,"user_name":"Geek_c22199","can_delete":false,"product_type":"c1","uid":1441876,"ip_address":"","ucode":"1CE5B65513E360","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI2vn8hyjICTCletGs0omz28lhriaZKX2XX9icYzAEon2IEoRnlXqyOia2bEPP0j7T6xexTnr77JJic8w/132","comment_is_top":false,"comment_ctime":1586338891,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1586338891","product_id":100015201,"comment_content":"自定义的话，设置一些规则算法让“堆叠”量化起来","like_count":0},{"had_liked":false,"id":110213,"user_name":"东东","can_delete":false,"product_type":"c1","uid":1227580,"ip_address":"","ucode":"B60CCBF33AF1BE","user_header":"https://static001.geekbang.org/account/avatar/00/12/bb/3c/a8a909e4.jpg","comment_is_top":false,"comment_ctime":1562208559,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1562208559","product_id":100015201,"comment_content":"找到堆叠的原因，然后通过调整priorities相应的权重来避免“堆叠”","like_count":0},{"had_liked":false,"id":60696,"user_name":"北卡","can_delete":false,"product_type":"c1","uid":1218128,"ip_address":"","ucode":"2D947A61689FC6","user_header":"https://static001.geekbang.org/account/avatar/00/12/96/50/bde525b1.jpg","comment_is_top":false,"comment_ctime":1547532149,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547532149","product_id":100015201,"comment_content":"细看调度这一块，觉得k8s的设计真的蛮精彩的。","like_count":0},{"had_liked":false,"id":49090,"user_name":"tommyCmd","can_delete":false,"product_type":"c1","uid":1208393,"ip_address":"","ucode":"4ADBB1FA44668D","user_header":"https://static001.geekbang.org/account/avatar/00/12/70/49/d7690979.jpg","comment_is_top":false,"comment_ctime":1544602567,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1544602567","product_id":100015201,"comment_content":"最近也一直被这个问题困扰，讨论一个方案是把pod的qos都设为Guaranteed，希望老师能够解答困惑","like_count":0},{"had_liked":false,"id":44395,"user_name":"马希民","can_delete":false,"product_type":"c1","uid":1024226,"ip_address":"","ucode":"08E8D9045E991B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/e2/e683ce9a.jpg","comment_is_top":false,"comment_ctime":1543419430,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543419430","product_id":100015201,"comment_content":"想请教一下老师，在做优先级选择的时候节点的剩余资源是如何得到的？是通过已经分配在该节点上的pod声明limit的资源总和还是通过该节点当前的实际资源使用情况呢？按照调度器使用缓存的说法，似乎应该是前者，那么如果一个pod在声明的时候要求了远远高于本身所需的资源，就会造成这个节点资源的极大浪费？小白不太懂，老师多多指教","like_count":0},{"had_liked":false,"id":44157,"user_name":"快乐就好","can_delete":false,"product_type":"c1","uid":1205376,"ip_address":"","ucode":"5F4806B050FA46","user_header":"https://static001.geekbang.org/account/avatar/00/12/64/80/61107e24.jpg","comment_is_top":false,"comment_ctime":1543375733,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543375733","product_id":100015201,"comment_content":"问一下，我用二进制安装的kubernetes，在实现master的HA时，通过haproxy反代多个Master, 我测试手动注入kube-apiserver故障，进行一下调度和资源操作都是正常的，然后我恢复故障的apiserver，资源操作也都是正常，我用的kube-proxy 是ipvs, 我就发现node上通过ipvsadm查看流量转发情况到apiserver的其他两个master 节点都是有的，但是新恢复的apiserver上面是没有一点流量过来，是否在恢复前需要把故障节点etcd 信息给清了，再启动故障的服务，然后master 状态信息自动同步过来。","like_count":0},{"had_liked":false,"id":44114,"user_name":"wilder","can_delete":false,"product_type":"c1","uid":1007682,"ip_address":"","ucode":"C176379126D007","user_header":"https://static001.geekbang.org/account/avatar/00/0f/60/42/8a79c613.jpg","comment_is_top":false,"comment_ctime":1543369844,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543369844","product_id":100015201,"comment_content":"沙发","like_count":0}]}