{"id":615675,"title":"32｜存储引擎：数据清洗与存储","content":"<p>你好，我是郑建勋。</p><p>爬虫项目的一个重要的环节就是把最终的数据持久化存储起来，数据可能会被存储到MySQL、MongoDB、Kafka、Excel等多种数据库、中间件或者是文件中。</p><p>要达到这个目的，我们很容易想到使用接口来实现模块间的解耦。我们还要解决数据的缓冲区问题。最后，由于爬虫的数据可能是多种多样的，如何对最终数据进行合理的抽象也是我们需要面临的问题。</p><p>这节课，我们将书写一个存储引擎，用它来处理数据的存储问题。</p><h2>爬取结构化数据</h2><p>之前我们爬取的案例比较简单，像是租房网站的信息等。但是实际情况下，我们的爬虫任务通常需要获取结构化的数据。例如一本书的信息就包含书名、价格、出版社、简介、评分等。为了生成结构化的数据，我以<a href=\"https://book.douban.com/\">豆瓣图书</a>为例书写我们的任务规则。</p><p><strong>第一步，从首页中右侧获取热门标签的信息。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/73/65/73df9e84cc9937555326b92b4e448865.png?wh=1920x1257\" alt=\"图片\"></p><pre><code class=\"language-plain\">const regexpStr = `&lt;a href=\"([^\"]+)\" class=\"tag\"&gt;([^&lt;]+)&lt;/a&gt;`\nfunc ParseTag(ctx *collect.Context) (collect.ParseResult, error) {\n\tre := regexp.MustCompile(regexpStr)\n\n\tmatches := re.FindAllSubmatch(ctx.Body, -1)\n\tresult := collect.ParseResult{}\n\n\tfor _, m := range matches {\n\t\tresult.Requesrts = append(\n\t\t\tresult.Requesrts, &amp;collect.Request{\n\t\t\t\tMethod:   \"GET\",\n\t\t\t\tTask:     ctx.Req.Task,\n\t\t\t\tUrl:      \"&lt;https://book.douban.com&gt;\" + string(m[1]),\n\t\t\t\tDepth:    ctx.Req.Depth + 1,\n\t\t\t\tRuleName: \"书籍列表\",\n\t\t\t})\n\t}\n\treturn result, nil\n}\n</code></pre><!-- [[[read_end]]] --><p>进入标签页面后，我们可以进一步获取到图书的列表。</p><p><img src=\"https://static001.geekbang.org/resource/image/1b/ce/1b947f73808yy752af826b0bebf4d9ce.png?wh=1920x2229\" alt=\"图片\"></p><p>解析图片列表的代码如下：</p><pre><code class=\"language-plain\">const BooklistRe = `&lt;a.*?href=\"([^\"]+)\" title=\"([^\"]+)\"`\n\nfunc ParseBookList(ctx *collect.Context) (collect.ParseResult, error) {\n\tre := regexp.MustCompile(BooklistRe)\n\tmatches := re.FindAllSubmatch(ctx.Body, -1)\n\tresult := collect.ParseResult{}\n\tfor _, m := range matches {\n\t\treq := &amp;collect.Request{\n\t\t\tMethod:   \"GET\",\n\t\t\tTask:     ctx.Req.Task,\n\t\t\tUrl:      string(m[1]),\n\t\t\tDepth:    ctx.Req.Depth + 1,\n\t\t\tRuleName: \"书籍简介\",\n\t\t}\n\t\treq.TmpData = &amp;collect.Temp{}\n\t\treq.TmpData.Set(\"book_name\", string(m[2]))\n\t\tresult.Requesrts = append(result.Requesrts, req)\n\t}\n\n\treturn result, nil\n}\n</code></pre><p>注意，这里我获取到书名之后，将书名缓存到了临时的tmp结构中供下一个阶段读取。这是因为我们希望得到的某些信息是在之前的阶段获得的。在这里我将缓存结构定义为了一个哈希表，并封装了Get与Set两个函数来获取和设置请求中的缓存。</p><pre><code class=\"language-plain\">type Temp struct {\n\tdata map[string]interface{}\n}\n\n// 返回临时缓存数据\nfunc (t *Temp) Get(key string) interface{} {\n\treturn t.data[key]\n}\n\nfunc (t *Temp) Set(key string, value interface{}) error {\n\tif t.data == nil {\n\t\tt.data = make(map[string]interface{}, 8)\n\t}\n\tt.data[key] = value\n\treturn nil\n}\n</code></pre><p>最后，点击图书的详情页，可以看到图书的作者、出版社、页数、定价、得分、价格、简介等信息。</p><p><img src=\"https://static001.geekbang.org/resource/image/8c/23/8c0ba5f76yyffdc7ac5f11f7aab1f523.png?wh=1920x1880\" alt=\"图片\"></p><p>解析图书详细信息的代码如下：</p><pre><code class=\"language-plain\">var autoRe = regexp.MustCompile(`&lt;span class=\"pl\"&gt; 作者&lt;/span&gt;:[\\d\\D]*?&lt;a.*?&gt;([^&lt;]+)&lt;/a&gt;`)\nvar public = regexp.MustCompile(`&lt;span class=\"pl\"&gt;出版社:&lt;/span&gt;([^&lt;]+)&lt;br/&gt;`)\nvar pageRe = regexp.MustCompile(`&lt;span class=\"pl\"&gt;页数:&lt;/span&gt; ([^&lt;]+)&lt;br/&gt;`)\nvar priceRe = regexp.MustCompile(`&lt;span class=\"pl\"&gt;定价:&lt;/span&gt;([^&lt;]+)&lt;br/&gt;`)\nvar scoreRe = regexp.MustCompile(`&lt;strong class=\"ll rating_num \" property=\"v:average\"&gt;([^&lt;]+)&lt;/strong&gt;`)\nvar intoRe = regexp.MustCompile(`&lt;div class=\"intro\"&gt;[\\d\\D]*?&lt;p&gt;([^&lt;]+)&lt;/p&gt;&lt;/div&gt;`)\n\nfunc ParseBookDetail(ctx *collect.Context) (collect.ParseResult, error) {\n\tbookName := ctx.Req.TmpData.Get(\"book_name\")\n\tpage, _ := strconv.Atoi(ExtraString(ctx.Body, pageRe))\n\n\tbook := map[string]interface{}{\n\t\t\"书名\":  bookName,\n\t\t\"作者\":  ExtraString(ctx.Body, autoRe),\n\t\t\"页数\":  page,\n\t\t\"出版社\": ExtraString(ctx.Body, public),\n\t\t\"得分\":  ExtraString(ctx.Body, scoreRe),\n\t\t\"价格\":  ExtraString(ctx.Body, priceRe),\n\t\t\"简介\":  ExtraString(ctx.Body, intoRe),\n\t}\n\tdata := ctx.Output(book)\n\n\tresult := collect.ParseResult{\n\t\tItems: []interface{}{data},\n\t}\n\n\treturn result, nil\n}\n\nfunc ExtraString(contents []byte, re *regexp.Regexp) string {\n\n\tmatch := re.FindSubmatch(contents)\n\n\tif len(match) &gt;= 2 {\n\t\treturn string(match[1])\n\t} else {\n\t\treturn \"\"\n\t}\n}\n</code></pre><p>其中，书名是从缓存中得到的。这里仍然使用了正则表达式作为演示，你也可以改为使用更合适的CSS选择器。</p><p>完整的任务规则如下所示：</p><pre><code class=\"language-plain\">var DoubanBookTask = &amp;collect.Task{\n\tProperty: collect.Property{\n\t\tName:     \"douban_book_list\",\n\t\tWaitTime: 1 * time.Second,\n\t\tMaxDepth: 5,\n\t\tCookie:   \"xxx\"\n},\n\tRule: collect.RuleTree{\n\t\tRoot: func() ([]*collect.Request, error) {\n\t\t\troots := []*collect.Request{\n\t\t\t\t&amp;collect.Request{\n\t\t\t\t\tPriority: 1,\n\t\t\t\t\tUrl:      \"&lt;https://book.douban.com&gt;\",\n\t\t\t\t\tMethod:   \"GET\",\n\t\t\t\t\tRuleName: \"数据tag\",\n\t\t\t\t},\n\t\t\t}\n\t\t\treturn roots, nil\n\t\t},\n\t\tTrunk: map[string]*collect.Rule{\n\t\t\t\"数据tag\": &amp;collect.Rule{ParseFunc: ParseTag},\n\t\t\t\"书籍列表\":  &amp;collect.Rule{ParseFunc: ParseBookList},\n\t\t\t\"书籍简介\": &amp;collect.Rule{\n\t\t\t\tItemFields: []string{\n\t\t\t\t\t\"书名\",\n\t\t\t\t\t\"作者\",\n\t\t\t\t\t\"页数\",\n\t\t\t\t\t\"出版社\",\n\t\t\t\t\t\"得分\",\n\t\t\t\t\t\"价格\",\n\t\t\t\t\t\"简介\",\n\t\t\t\t},\n\t\t\t\tParseFunc: ParseBookDetail,\n\t\t\t},\n\t\t},\n\t},\n}\n</code></pre><p>在采集规则节点中，我们加入了一个新的字段 ItemFields 来表明当前输出数据的字段名，后面我们还会看到它的用途。</p><pre><code class=\"language-plain\">type Rule struct {\n\tItemFields []string\n\tParseFunc  func(*Context) (ParseResult, error) // 内容解析函数\n}\n</code></pre><p>上述代码位于<a href=\"https://github.com/dreamerjackson/crawler\">v0.2.6</a>中，执行程序后，输出结果如下：</p><pre><code class=\"language-plain\">{\"level\":\"INFO\",\"ts\":\"2022-11-19T11:19:23.720+0800\",\"caller\":\"crawler/main.go:16\",\"msg\":\"log init end\"}\n{\"level\":\"INFO\",\"ts\":\"2022-11-19T11:19:28.119+0800\",\"caller\":\"engine/schedule.go:301\",\"msg\":\"get result: &amp;{map[Data:map[书名:长安的荔枝 价格: 45.00元 作者:马伯庸 出版社: 得分: 8.5  简介:——陕西师范大学历史文化学院教授 于赓哲 页数:224] Rus://book.douban.com/subject/36104107/]}\"}\n</code></pre><p>现在我们就能够爬取结构化的图书信息了。</p><h2>数据存储</h2><h3>数据抽象</h3><p>爬取到足够的信息之后，为了将数据存储起来，首先我们需要完成对数据的抽象。在这里我将每一条要存储的数据都抽象为了DataCell结构。我们可以把DataCell想象为MySQL中的一行数据。</p><pre><code class=\"language-plain\">type DataCell struct {\n\tData map[string]interface{}\n}\n</code></pre><p>我们规定，DataCell中的Key为“Task”的数据存储了当前的任务名，Key为“Rule”的数据存储了当前的规则名，Key为“Url”的数据存储了当前的网址，Key为“Time”的数据存储了当前的时间。而最重要的Key为“Data”的数据存储了当前核心的数据，即当前书籍的详细信息。</p><p>在解析图书详细信息的规则中，我们定义“Data”对应的数据结构又是一个哈希表map[string]interface{}。在这个哈希表中，Key为“书名”“评分”等字段名，Value为字段对应的值。要注意的是，这里Data对应的Value不一定需要是map[string]interface{}，只要我们在后面能够灵活地处理不同的类型就可以了。</p><pre><code class=\"language-plain\">func (c *Context) Output(data interface{}) *collector.DataCell {\n\tres := &amp;collector.DataCell{}\n\tres.Data = make(map[string]interface{})\n\tres.Data[\"Task\"] = c.Req.Task.Name\n\tres.Data[\"Rule\"] = c.Req.RuleName\n\tres.Data[\"Data\"] = data\n\tres.Data[\"Url\"] = c.Req.Url\n\tres.Data[\"Time\"] = time.Now().Format(\"2006-01-02 15:04:05\")\n\treturn res\n}\n</code></pre><p>完成了数据的抽象之后，就可以将最终的数据存储到Items中，供我们专门的协程去处理了。</p><pre><code class=\"language-plain\">type ParseResult struct {\n\tRequesrts []*Request\n\tItems     []interface{}\n}\n</code></pre><h3>数据底层存储</h3><p>在之前我们一直有一个未完成项，就是在HandleResult方法中对解析后的数据进行存储，现在我们就可以将它处理完整了。现在我们要循环遍历Items，判断其中的数据类型，如果数据类型为DataCell，我们就要用专门的存储引擎将这些数据存储起来。（存储引擎是和每一个爬虫任务绑定在一起的，不同的爬虫任务可能会有不同的存储引擎。）</p><pre><code class=\"language-plain\">func (s *Crawler) HandleResult() {\n\tfor {\n\t\tselect {\n\t\tcase result := &lt;-s.out:\n\t\t\tfor _, item := range result.Items {\n\t\t\t\tswitch d := item.(type) {\n\t\t\t\tcase *collector.DataCell:\n\t\t\t\t\tname := d.GetTaskName()\n\t\t\t\t\ttask := Store.Hash[name]\n\t\t\t\t\ttask.Storage.Save(d)\n\t\t\t\t}\n\t\t\t\ts.Logger.Sugar().Info(\"get result: \", item)\n\t\t\t}\n\t\t}\n\t}\n}\n</code></pre><p>我选择使用比较常见的MySQL数据库作为这个示例的存储引擎。在这里，我创建了一个接口Storage作为数据存储的接口，Storage中包含了Save方法，任何实现了Save方法的后端引擎都可以存储数据。</p><pre><code class=\"language-plain\">type Storage interface {\n\tSave(datas ...*DataCell) error\n}\n</code></pre><p>不过我们还需要完成一轮抽象，因为后端引擎会处理的事务比较繁琐，它不仅仅包含了存储，还包含了缓存、对表头的拼接、数据的处理等。所以，我们要创建一个更加底层的模块，只进行数据的存储。</p><p>这个底层抽象的好处在于，我们可以比较灵活地替换底层的存储模块，我在这个例子中使用了原生的MySQL语句来与数据库交互。你也可以使用Xorm与Gorm这样的库来操作数据库。</p><p>新建一个文件夹mysqldb，设置操作数据库的接口DBer，里面的两个核心函数分别是CreateTable（创建表）以及Insert（插入数据）。</p><pre><code class=\"language-plain\">\ntype DBer interface {\n\tCreateTable(t TableData) error\n\tInsert(t TableData) error\n}\ntype Field struct {\n\tTitle string\n\tType  string\n}\ntype TableData struct {\n\tTableName   string\n\tColumnNames []Field       // 标题字段\n\tArgs        []interface{} // 数据\n\tDataCount   int           // 插入数据的数量\n\tAutoKey     bool\n}\n</code></pre><p>参数TableData包含了表的元数据，TableName为表名，ColumnNames包含了字段名和字段的属性 ，Args为要插入的数据，DataCount为插入数据的个数，AutoKey标识是否为表创建自增主键。<br>\n下面这段代码，我们使用option模式生成了SqlDB结构体，实现了DBer接口。Sqldb.OpenDB方法用于与数据库建立连接，需要从外部传入远程MySQL数据库的连接地址。</p><pre><code class=\"language-plain\">type Sqldb struct {\n\toptions\n\tdb *sql.DB\n}\n\nfunc New(opts ...Option) (*Sqldb, error) {\n\toptions := defaultOptions\n\tfor _, opt := range opts {\n\t\topt(&amp;options)\n\t}\n\td := &amp;Sqldb{}\n\td.options = options\n\tif err := d.OpenDB(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn d, nil\n}\n\nfunc (d *Sqldb) OpenDB() error {\n\tdb, err := sql.Open(\"mysql\", d.sqlUrl)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdb.SetMaxOpenConns(2048)\n\tdb.SetMaxIdleConns(2048)\n\tif err = db.Ping(); err != nil {\n\t\treturn err\n\t}\n\td.db = db\n\treturn nil\n}\n</code></pre><p>两个核心的方法CreateTable 与 Insert 会拼接 MySQL语句，并分别执行创建表与插入数据的从操作。</p><pre><code class=\"language-plain\">func (d *Sqldb) CreateTable(t TableData) error {\n\tif len(t.ColumnNames) == 0 {\n\t\treturn errors.New(\"Column can not be empty\")\n\t}\n\tsql := `CREATE TABLE IF NOT EXISTS ` + t.TableName + \" (\"\n\tif t.AutoKey {\n\t\tsql += `id INT(12) NOT NULL PRIMARY KEY AUTO_INCREMENT,`\n\t}\n\tfor _, t := range t.ColumnNames {\n\t\tsql += t.Title + ` ` + t.Type + `,`\n\t}\n\tsql = sql[:len(sql)-1] + `) ENGINE=MyISAM DEFAULT CHARSET=utf8;`\n\n\td.logger.Debug(\"crate table\", zap.String(\"sql\", sql))\n\n\t_, err := d.db.Exec(sql)\n\treturn err\n}\n\nfunc (d *Sqldb) Insert(t TableData) error {\n\tif len(t.ColumnNames) == 0 {\n\t\treturn errors.New(\"empty column\")\n\t}\n\tsql := `INSERT INTO ` + t.TableName + `(`\n\n\tfor _, v := range t.ColumnNames {\n\t\tsql += v.Title + \",\"\n\t}\n\n\tsql = sql[:len(sql)-1] + `) VALUES `\n\n\tblank := \",(\" + strings.Repeat(\",?\", len(t.ColumnNames))[1:] + \")\"\n\tsql += strings.Repeat(blank, t.DataCount)[1:] + `;`\n\td.logger.Debug(\"insert table\", zap.String(\"sql\", sql))\n\t_, err := d.db.Exec(sql, t.Args...)\n\treturn err\n}\n</code></pre><h3>存储引擎实现</h3><p>接下来，我们再看看如何实现存储引擎Storage。</p><pre><code class=\"language-plain\">type SqlStore struct {\n\tdataDocker  []*collector.DataCell //分批输出结果缓存\n\tcolumnNames []sqldb.Field         // 标题字段\n\tdb          sqldb.DBer\n\tTable       map[string]struct{}\n\toptions\n}\n\nfunc New(opts ...Option) (*SqlStore, error) {\n\toptions := defaultOptions\n\tfor _, opt := range opts {\n\t\topt(&amp;options)\n\t}\n\ts := &amp;SqlStore{}\n\ts.options = options\n\ts.Table = make(map[string]struct{})\n\tvar err error\n\ts.db, err = sqldb.New(\n\t\tsqldb.WithConnUrl(s.sqlUrl),\n\t\tsqldb.WithLogger(s.logger),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s, nil\n}\n</code></pre><p>SqlStore是对Storage接口的实现，SqlStore实现了option模式，同时它的内部包含了操作数据库的DBer接口。让我们来看看SqlStore如何实现DBer接口中的Save方法，它主要实现了三个功能：</p><ul>\n<li>循环遍历要存储的DataCell，并判断当前DataCell对应的数据库表是否已经被创建。如果表格没有被创建，则调用CreateTable创建表格。在存储数据时，getFields用于获取当前数据的表字段与字段类型，这是从采集规则节点的 <code>ItemFields</code> 数组中获得的。你可能想问，那我们为什么不直接用DataCell中Data对应的哈希表中的Key生成字段名呢？这一方面是因为它的速度太慢，另外一方面是因为Go中的哈希表在遍历时的顺序是随机的，而生成的字段列表需要顺序固定。</li>\n</ul><pre><code class=\"language-plain\">func getFields(cell *collector.DataCell) []sqldb.Field {\n\ttaskName := cell.Data[\"Task\"].(string)\n\truleName := cell.Data[\"Rule\"].(string)\n\tfields := engine.GetFields(taskName, ruleName)\n\n\tvar columnNames []sqldb.Field\n\tfor _, field := range fields {\n\t\tcolumnNames = append(columnNames, sqldb.Field{\n\t\t\tTitle: field,\n\t\t\tType:  \"MEDIUMTEXT\",\n\t\t})\n\t}\n\tcolumnNames = append(columnNames,\n\t\tsqldb.Field{Title: \"Url\", Type: \"VARCHAR(255)\"},\n\t\tsqldb.Field{Title: \"Time\", Type: \"VARCHAR(255)\"},\n\t)\n\treturn columnNames\n}\n</code></pre><ul>\n<li>如果当前的数据小于s.BatchCount，则将数据放入到缓存中直接返回（使用缓冲区批量插入数据库可以提高程序的性能）。</li>\n<li>如果缓冲区已经满了，则调用SqlStore.Flush()方法批量插入数据。</li>\n</ul><pre><code class=\"language-plain\">func (s *SqlStore) Save(dataCells ...*collector.DataCell) error {\n\tfor _, cell := range dataCells {\n\t\tname := cell.GetTableName()\n\t\tif _, ok := s.Table[name]; !ok {\n\t\t\t// 创建表\n\t\t\tcolumnNames := getFields(cell)\n\n\t\t\terr := s.db.CreateTable(sqldb.TableData{\n\t\t\t\tTableName:   name,\n\t\t\t\tColumnNames: columnNames,\n\t\t\t\tAutoKey:     true,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\ts.logger.Error(\"create table falied\", zap.Error(err))\n\t\t\t}\n\t\t\ts.Table[name] = struct{}{}\n\t\t}\n\t\tif len(s.dataDocker) &gt;= s.BatchCount {\n\t\t\ts.Flush()\n\t\t}\n\t\ts.dataDocker = append(s.dataDocker, cell)\n\t}\n\treturn nil\n}\n</code></pre><p>SqlStore.Flush()方法的实现如下：</p><pre><code class=\"language-plain\">func (s *SqlStore) Flush() error {\n\tif len(s.dataDocker) == 0 {\n\t\treturn nil\n\t}\n\targs := make([]interface{}, 0)\n\tfor _, datacell := range s.dataDocker {\n\t\truleName := datacell.Data[\"Rule\"].(string)\n\t\ttaskName := datacell.Data[\"Task\"].(string)\n\t\tfields := engine.GetFields(taskName, ruleName)\n\t\tdata := datacell.Data[\"Data\"].(map[string]interface{})\n\t\tvalue := []string{}\n\t\tfor _, field := range fields {\n\t\t\tv := data[field]\n\t\t\tswitch v.(type) {\n\t\t\tcase nil:\n\t\t\t\tvalue = append(value, \"\")\n\t\t\tcase string:\n\t\t\t\tvalue = append(value, v.(string))\n\t\t\tdefault:\n\t\t\t\tj, err := json.Marshal(v)\n\t\t\t\tif err != nil {\n\t\t\t\t\tvalue = append(value, \"\")\n\t\t\t\t} else {\n\t\t\t\t\tvalue = append(value, string(j))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tvalue = append(value, datacell.Data[\"Url\"].(string), datacell.Data[\"Time\"].(string))\n\t\tfor _, v := range value {\n\t\t\targs = append(args, v)\n\t\t}\n\t}\n\n\treturn s.db.Insert(sqldb.TableData{\n\t\tTableName:   s.dataDocker[0].GetTableName(),\n\t\tColumnNames: getFields(s.dataDocker[0]),\n\t\tArgs:        args,\n\t\tDataCount:   len(s.dataDocker),\n\t})\n}\n</code></pre><p>这段代码的核心是遍历缓冲区，解析每一个DataCell中的数据，将扩展后的字段值批量放入args参数中，并调用底层DBer.Insert方法批量插入数据（上述代码位于<a href=\"https://github.com/dreamerjackson/crawler\">v0.2.7分支</a>。）</p><h2>存储引擎验证</h2><p>接下来我们简单地验证下我们书写的存储引擎的正确性。首先为了方便起见，我们用Docker在后台启动一个MySQL数据库，将当前的数据库映射到本机的3326端口，设置root密码为123456。创建名为crawler的数据库。</p><pre><code class=\"language-plain\">docker run -d --name mysql-test -p 3326:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql\ndocker exec -it mysql-test sh\nCREATE DATABASE crawler;\nuse crawler;\n</code></pre><p>在main.go的启动参数中，创建sqlstorage并注入到Task当中。注意，这里WithSqlUrl的作用是传递MySQL的连接地址。</p><pre><code class=\"language-plain\">func main(){\n ...\n  var storage collector.Storage\n\tstorage, err = sqlstorage.New(\n\t\tsqlstorage.WithSqlUrl(\"root:123456@tcp(127.0.0.1:3326)/crawler?charset=utf8\"),\n\t\tsqlstorage.WithLogger(logger.Named(\"sqlDB\")),\n\t\tsqlstorage.WithBatchCount(2),\n\t)\n\tif err != nil {\n\t\tlogger.Error(\"create sqlstorage failed\")\n\t\treturn\n\t}\n\n\tseeds := make([]*collect.Task, 0, 1000)\n\tseeds = append(seeds, &amp;collect.Task{\n\t\tProperty: collect.Property{\n\t\t\tName: \"douban_book_list\",\n\t\t},\n\t\tFetcher: f,\n\t\tStorage: storage,\n\t})\n\n\ts := engine.NewEngine(\n\t\tengine.WithFetcher(f),\n\t\tengine.WithLogger(logger),\n\t\tengine.WithWorkCount(5),\n\t\tengine.WithSeeds(seeds),\n\t\tengine.WithScheduler(engine.NewSchedule()),\n\t)\n\n\ts.Run()\n}\n</code></pre><p>运行代码后，数据将存储到MySQL表的douban_book_list中。我们可以用多种与数据库交互的工具查看表中的数据。例如，我们这里使用的是DataGrip，使用地址、密码、和对应的Crawler Database，就可以连接到对应的数据库。</p><p><img src=\"https://static001.geekbang.org/resource/image/04/cf/04ee9f0b0998928d3de4aac18daf49cf.png?wh=1920x1620\" alt=\"图片\"></p><p>运行SHOW FULL COLUMNS FROM douban_book_list; 可以查看生成的表的字段和类型。</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/79/6f2b4d50db3119e984e24bcc77d9a779.png?wh=1920x433\" alt=\"图片\"></p><p>运行select * from  douban_book_list; 可以查看表中已经插入的数据。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/75/d75yy6c3955235c7040c8cb7df0e8975.png?wh=1920x197\" alt=\"图片\"></p><h2>总结</h2><p>好了，这节课，我们以存储数据为目标，实现了存储引擎。我们还以豆瓣图书的结构化数据为例，学习了如何对不同的数据进行抽象。我以MySQL为例，并使用了原生的SQL语句来从操作数据库，在这个过程中我们再次看到了接口的强大能力。当前的架构能够帮助我们比较容易地写一个新的存储引擎，例如把数据存储到Kafka，MongoDB、Excel中的存储引擎。如果我们希望在底层使用ORM库来操作数据库也会比较容易。</p><h2>课后题</h2><p>这节课的课后题是这样的：</p><p>在对不同的结构化信息进行抽象时，我们使用了map[string]interface{}来存储书籍的属性。那么我们有没有可能在数据输出时直接使用像Book这样的结构体，把数据直接传递给存储引擎来处理呢？</p><p>欢迎你在留言区与我交流讨论，我们下节课见。</p>","neighbors":{"left":{"article_title":"31｜规则引擎：自定义爬虫处理规则","id":614853},"right":{"article_title":"33｜固若金汤：限速器与错误处理","id":616904}},"comments":[{"had_liked":false,"id":370260,"user_name":"出云","can_delete":false,"product_type":"c1","uid":1477757,"ip_address":"广东","ucode":"5674C995C0E84F","user_header":"https://static001.geekbang.org/account/avatar/00/16/8c/7d/cae6b979.jpg","comment_is_top":false,"comment_ctime":1678622022,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"按文中的写法，SqlStore.Flush() 方法不能处理同一个Batch中存在不同Task的DataCell的情况。","like_count":0,"discussions":[{"author":{"id":3808122,"avatar":"","nickname":"Geek_2c2c44","note":"","ucode":"058A3078C656CF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635949,"discussion_content":"确实哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705393266,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}