{"id":432042,"title":"09 | 卷积（上）：如何用卷积为计算机“开天眼”？","content":"<p>你好，我是方远。</p><p>现在刷脸支付的场景越来越多，相信人脸识别你一定不陌生，你有没有想过，在计算机识别人脸之前，我们人类是如何判断一个人是谁的呢？</p><p>我们眼睛看到人脸的时候，会先将人脸的一些粗粒度特征提取出来，例如人脸的轮廓、头发的颜色、头发长短等。然后这些信息会一层一层地传入到某一些神经元当中，每经过一层神经元就相当于特征提取。我们大脑最终会将最后的特征进行汇总，类似汇总成一张具体的人脸，用这张人脸去大脑的某一个地方与存好的人名进行匹配。</p><p>那落实到我们计算机呢？其实这个过程是一样的，在计算机中进行特征提取的功能，就离不开我们今天要讲的卷积。</p><p>可以说，没有卷积的话，深度学习在图像领域不可能取得今天的成就。 那么，就让我们来看看什么是卷积，还有它在PyTorch中的实现吧。</p><h2>卷积</h2><p>在使用卷积之前，人们尝试了很多人工神经网络来处理图像问题，但是人工神经网络的参数量非常大，从而导致非常难训练，所以计算机视觉的研究一直停滞不前，难以突破。</p><p>直到卷积神经网络的出现，它的两个优秀特点：稀疏连接与平移不变性，这让计算机视觉的研究取得了长足的进步。什么是稀疏连接与平移不变性呢？简单来说，就是稀疏连接可以让学习的参数变得很少，而平移不变性则不关心物体出现在图像中什么位置。</p><!-- [[[read_end]]] --><p>稀疏连接与平移不变性是卷积的两个重要特点，如果你想从事计算机视觉相关的工作，这两个特点必须该清楚，但不是本专栏的重点，这里就不展开了，有兴趣你可以自己去了解。</p><p>下面我们直接来看看卷积是如何计算的。</p><h3>最简单的情况</h3><p>我们先看最简单的情况，输入是一个4x4的特征图，卷积核的大小为2x2。</p><p>卷积核是什么呢？其实就是我们卷积层要学习到的参数，就像下图中红色的示例，下图中的卷积核是最简单的情况，只有一个通道。</p><p><img src=\"https://static001.geekbang.org/resource/image/ac/5d/ac84c162ee165d535fcbf465572faf5d.jpg?wh=1075x728\" alt=\"图片\"></p><p>输入特征与卷积核计算时，计算方式是卷积核与输入特征按位做乘积运算然后再求和，其结果为输出特征图的一个元素，下图为计算输出特征图第一个元素的计算方式：</p><p><img src=\"https://static001.geekbang.org/resource/image/78/20/787c8b346de00dayyd7e2d3504c33320.jpg?wh=1561x891\" alt=\"图片\"></p><p>完成了第一个元素的计算，我们接着往下看，按以从左向右，从上至下的顺序进行滑动卷积核，分别与输入的特征图进行计算，请看下图，下图为上图计算完毕之后，向右侧滑动一个单元的计算方式：</p><p><img src=\"https://static001.geekbang.org/resource/image/5y/b5/5yy249d2f1221e21a1bdc7d8756f4fb5.jpg?wh=1544x862\" alt=\"图片\"></p><p>第一行第三个单元的计算以此类推。说完了同一行的移动，我们再看看，第一行计算完毕，向下滑动的计算方式是什么样的。</p><p><img src=\"https://static001.geekbang.org/resource/image/cf/bb/cf4aa3yy8ac31b06f153f9090d3bcebb.jpg?wh=1552x929\" alt=\"图片\"></p><p>第一行计算完毕之后，卷积核会回到行首，然后向下滑动一个单元，再重复以上从左至右的滑动计算。</p><p>这里我再给你补充一个知识点，什么是步长？</p><p>卷积上下左右滑动的长度，我们称为步长，用stride表示。上述例子中的步长就是1，根据问题的不同，会取不同的步长，但通常来说步长为1或2。不管是刚才说的最简单的卷积计算，还是我们后面要讲的标准卷积，都要用到这个参数。</p><h3>标准的卷积</h3><p>好啦，前面只是最简单的情况，现在我们将最简单的卷积计算方式延伸到标准的卷积计算方式。</p><p>我们先将上面的例子描述为更加通用的形式，输入的特征有m个通道，宽为w，高为h；输出有n个特征图，宽为$w^{\\prime}$，高为$h^{\\prime}$；卷积核的大小为kxk。</p><p>在刚才的例子中m、n、k、w、h、$w^{\\prime}$、$h^{\\prime}$的值分别为1、1、2、4、4、3、3。而现在，我们需要把一个输入为(m，h，w)的输入特征图经过卷积计算，生成一个输出为(n, $h^{\\prime}$, $w^{\\prime}$)的特征图。</p><p>那我们来看看可以获得这个操作的卷积是什么样子的。输出特征图的通道数由<strong>卷积核的个数决定</strong>的，所以说卷积核的个数为n。根据卷积计算的定义，<strong>输入特征图有m个通道，所以每个卷积核里要也要有m个通道</strong>。所以，我们的需要n个卷积核，每个卷积核的大小为(m, k, k)。</p><p>为了帮你更好地理解刚才所讲的内容，我画了示意图，你可以对照一下：</p><p><img src=\"https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080\" alt=\"\"></p><p>结合上面的图解可以看到，卷积核1与全部输入特征进行卷积计算，就获得了输出特征图中第1个通道的数据，卷积核2与全部输入特征图进行计算获得输出特征图中第2个通道的数据。以此类推，最终就能计算n个输出特征图。</p><p>在开篇的例子中，输入只有1个通道，现在有多个通道了，那我们该如何计算呢？其实计算方式类似，输入特征的每一个通道与卷积核中对应通道的数据按我们之前讲过的方式进行卷积计算，也就是输入特征图中第i个特征图与卷积核中的第i个通道的数据进行卷积。这样计算后会生成<strong>m</strong>个特征图，然后将这m个特征图按对应位置求和即可，求和后m个特征图合并为输出特征中一个通道的特征图。</p><p>我们可以用后面的公式表示当输入有多个通道时，每个卷积核是如何与输入进行计算的。</p><p>$Output_i$表示计算第i个输出特征图，i的取值为1到n；</p><p>$kernel_k$表示1个卷积核里的第k个通道的数据；</p><p>$input_k$表示输入特征图中的第k个通道的数据；</p><p>$bias_k$为偏移项，我们在训练时一般都会默认加上；</p><p>$\\star$为卷积计算；</p><p>$$Output_i = \\sum_{k=0}^{m}kernel_k \\star input_k + bias_i, \\space \\space \\space \\space i=1,2,…,n$$</p><p>我来解释一下为什么要加bias。就跟回归方程一样，如果不加bias的话，回归方程为y=wx不管w如何变化，回归方程都必须经过原点。如果加上bias的话，回归方程变为y=wx+b，这样就不是必须经过原点，可以变化的更加多样。</p><p>好啦，卷积计算方式的讲解到这里就告一段落了。下面我们看看在卷积层中有关卷积计算的另外一个重要参数。</p><h3>Padding</h3><p>让我们回到开头的例子，可以发现，输入的尺寸是4x4，输出的尺寸是3x3。你有没有发现，输出的特征图变小了？没错，在有多层卷积层的神经网络中，特征图会越来越小。</p><p>但是，有的时候我们为了让特征图变得不是那么小，可以对特征图进行补零操作。这样做主要有两个目的：</p><p>1.有的时候需要输入与输出的特征图保持一样的大小；<br>\n2.让输入的特征保留更多的信息。</p><p>这里我举个例子，带你看看，一般什么情况下会希望特征图变得不那么小。</p><p>通过刚才的讲解我们知道，如果不补零且步长（stride）为1的情况下，当有多层卷积层时，特征图会一点点变小。如果我们希望有更多层卷积层来提取更加丰富的信息时，就可以让特征图变小的速度稍微慢一些，这个时候就可以考虑补零。</p><p>这个补零的操作就叫做padding，padding等于1就是补一圈的零，等于2就是补两圈的零，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/99/08/99dc22a96df665e93e881yy3cf358d08.jpg?wh=1520x792\" alt=\"图片\"></p><p>在Pytorch中，padding这个参数可以是字符串、int和tuple。</p><p>我们分别来看看不同参数类型怎么使用：当为字符串时只能取$^{\\prime}valid^{\\prime}$与$^{\\prime}same^{\\prime}$。当给定整型时，则是说要在特征图外边补多少圈0。如果是tuple的时候，则是表示在特征图的行与列分别指定补多少零。</p><p>我们重点看一下字符串的形式，相比于直接给定补多少零来说，我认为字符串更加常用。其中，$^{\\prime}valid^{\\prime}$就是没有padding操作，就像开头的例子那样。$^{\\prime}same^{\\prime}$则是让输出的特征图与输入的特征图获得相同的大小。</p><p>那当padding为same时，到底是怎么计算的呢？我们继续用开篇的例子说明，现在padding为$^{\\prime}same^{\\prime}$了。</p><p><img src=\"https://static001.geekbang.org/resource/image/yy/9c/yy51a5c4a35ffa8a06e7d7415aba339c.jpg?wh=1417x736\" alt=\"图片\"></p><p>当滑动到特征图最右侧时，发现输出的特征图的宽与输入的特征图的宽不一致，它会自动补零，直到输出特征图的宽与输入特征图的宽一致为止。如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/52/c7/52f8d0ba39b49e9a2736c1a0afb38cc7.jpg?wh=1463x720\" alt=\"图片\"></p><p>高的计算和宽的计算同理，当计算到特征图的底部时，发现输出特征图的高与输入特征图的高不一致时，它同样会自动补零，直到输入和输出一致为止，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/81/c48614da6c7bbcd5abdaf942ea45b481.jpg?wh=1476x735\" alt=\"图片\"></p><p>完成上述操作，我们就可以获得与输入特征图有相同高、宽的输出特征图了。理论讲完了，我们还是要学以致用，在实践中深入体会。在下面的练习中，我们会实际考察一下当padding为same时，是否像我们说的这样计算。</p><h2>PyTorch中的卷积</h2><p>卷积操作定义在torch.nn模块中，torch.nn模块为我们提供了很多构建网络的基础层与方法。</p><p>在torch.nn模块中，关于今天介绍的卷积操作有nn.Conv1d、nn.Conv2d与nn.Conv3d三个类。</p><p>请注意，我们上述的例子都是按照nn.Conv2d来介绍的，nn.Conv2d也是用的最多的，而nn.Conv1d与nn.Conv3d只是输入特征图的维度有所不一样而已，很少会被用到。</p><p>让我们先看看创建一个nn.Conv2d需要哪些必须的参数：</p><pre><code class=\"language-python\"># Conv2d类\nclass torch.nn.Conv2d(in_channels,&nbsp;\n                      out_channels,&nbsp;\n                      kernel_size,&nbsp;\n                      stride=1,&nbsp;\n                      padding=0,&nbsp;\n                      dilation=1,&nbsp;\n                      groups=1,&nbsp;\n                      bias=True,&nbsp;\n                      padding_mode='zeros',&nbsp;\n                      device=None,&nbsp;\n                      dtype=None)\n\n</code></pre><p>我们挨个说说这些参数。首先是跟通道相关的两个参数：in_channels是指输入特征图的通道数，数据类型为int，在标准卷积的讲解中in_channels为m；out_channels是输出特征图的通道数，数据类型为int，在标准卷积的讲解中out_channels为n。</p><p>kernel_size是卷积核的大小，数据类型为int或tuple，需要注意的是只给定卷积核的高与宽即可，在标准卷积的讲解中kernel_size为k。</p><p>stride为滑动的步长，数据类型为int或tuple，默认是1，在前面的例子中步长都为1。</p><p>padding为补零的方式，注意<strong>当padding为’valid’或’same’时，stride必须为1</strong>。</p><p>对于kernel_size、stride、padding都可以是tuple类型，当为tuple类型时，第一个维度用于height的信息，第二个维度时用于width的信息。</p><p>bias是否使用偏移项。</p><p>还有两个参数：dilation与groups，具体内容下节课我们继续展开讲解，你先有个印象就行。</p><h3>验证same方式</h3><p>接下来，我们做一个练习，验证padding为same时，计算方式是否像我们所说的那样。过程并不复杂，一共三步，分别是创建输入特征图、设置卷积以及输出结果。</p><p>先来看第一步，我们创建好例子中的（4，4，1）大小的输入特征图，代码如下：</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\ninput_feat = torch.tensor([[4, 1, 7, 5], [4, 4, 2, 5], [7, 7, 2, 4], [1, 0, 2, 4]], dtype=torch.float32)\nprint(input_feat)\nprint(input_feat.shape)\n\n# 输出：\ntensor([[4., 1., 7., 5.],\n        [4., 4., 2., 5.],\n        [7., 7., 2., 4.],\n        [1., 0., 2., 4.]])\ntorch.Size([4, 4])\n</code></pre><p>第二步，创建一个2x2的卷积，根据刚才的介绍，输入的通道数为1，输出的通道数为1，padding为’same’，所以卷积定义为：</p><pre><code class=\"language-python\">conv2d = nn.Conv2d(1, 1, (2, 2), stride=1, padding='same', bias=True)\n# 默认情况随机初始化参数\nprint(conv2d.weight)\nprint(conv2d.bias)\n# 输出：\nParameter containing:\ntensor([[[[ 0.3235, -0.1593],\n          [ 0.2548, -0.1363]]]], requires_grad=True)\nParameter containing:\ntensor([0.4890], requires_grad=True)\n</code></pre><p>需要注意的是，默认情况下是随机初始化的。一般情况下，我们不会人工强行干预卷积核的初始化，但是为了验证今天的例子，我们对卷积核的参数进行干预。请注意下面代码中卷积核的注释，代码如下：</p><pre><code class=\"language-python\">conv2d = nn.Conv2d(1, 1, (2, 2), stride=1, padding='same', bias=False)\n# 卷积核要有四个维度(输入通道数，输出通道数，高，宽)\nkernels = torch.tensor([[[[1, 0], [2, 1]]]], dtype=torch.float32)\nconv2d.weight = nn.Parameter(kernels, requires_grad=False)\nprint(conv2d.weight)\nprint(conv2d.bias)\n# 输出：\nParameter containing:\ntensor([[[[1., 0.],\n          [2., 1.]]]])\nNone\n</code></pre><p>完成之后就进入了第三步，现在我们已经准备好例子中的输入数据与卷积数据了，下面只需要计算一下，然后输出就可以了，代码如下：</p><pre><code class=\"language-python\">output = conv2d(input_feat)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/var/folders/pz/z8t8232j1v17y01bkhyrl01w0000gn/T/ipykernel_29592/2273564149.py in &lt;module&gt;\n----&gt; 1 output = conv2d(input_feat)\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1051             return forward_call(*input, **kwargs)\n   1052         # Do not call functions when jit is used\n   1053         full_backward_hooks, non_full_backward_hooks = [], []\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py in forward(self, input)\n    441 \n    442     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 443         return self._conv_forward(input, self.weight, self.bias)\n    444 \n    445 class Conv3d(_ConvNd):\n~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)\n    437                             weight, bias, self.stride,\n    438                             _pair(0), self.dilation, self.groups)\n--&gt; 439         return F.conv2d(input, weight, bias, self.stride,\n    440                         self.padding, self.dilation, self.groups)\n    441 \nRuntimeError: Expected 4-dimensional input for 4-dimensional weight[1, 1, 2, 2], but got 2-dimensional input of size [4, 4] instead\n</code></pre><p>结合上面代码，你会发现这里报错了，提示信息是输入的特征图需要是一个4维的，而我们的输入特征图是一个4x4的2维特征图。这是为什么呢？<br>\n请你记住，<strong>Pytorch输入tensor的维度信息是(batch_size, 通道数，高，宽)</strong>，但是在我们的例子中只给定了高与宽，没有给定batch_size（在训练时，不会将所有数据一次性加载进来训练，而是以多个批次进行读取的，每次读取的量成为batch_size）与通道数。所以，我们要回到第一步将输入的tensor改为(1,1,4,4)的形式。</p><p>你还记得我在之前的讲解中提到过怎么对数组添加维度吗？</p><p>在Pytorch中unsqueeze()对tensor的维度进行修改。代码如下：</p><pre><code class=\"language-python\">input_feat = torch.tensor([[4, 1, 7, 5], [4, 4, 2, 5], [7, 7, 2, 4], [1, 0, 2, 4]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\nprint(input_feat)\nprint(input_feat.shape)\n# 输出：\ntensor([[[[4., 1., 7., 5.],\n          [4., 4., 2., 5.],\n          [7., 7., 2., 4.],\n          [1., 0., 2., 4.]]]])\ntorch.Size([1, 1, 4, 4])\n</code></pre><p>这里，unsqueeze()中的参数是指在哪个位置添加维度。<br>\n好，做完了修改，我们再次执行代码。</p><pre><code class=\"language-python\">output = conv2d(input_feat)\n输出：\ntensor([[[[16., 11., 16., 15.],\n          [25., 20., 10., 13.],\n          [ 9.,  9., 10., 12.],\n          [ 1.,  0.,  2.,  4.]]]])\n          \n</code></pre><p>你可以看看，跟我们在例子中推导的结果一不一样？</p><h2>总结</h2><p>恭喜你完成了今天的学习。今天所讲的卷积非常重要，它是各种计算机视觉应用的基础，例如图像分类、目标检测、图像分割等。</p><p>卷积的计算方式是你需要关注的重点。具体过程如下图所示，输出特征图的通道数由<strong>卷积核的个数决定</strong>的，下图中因为有n个卷积核，所以输出特征图的通道数为n。<strong>输入特征图有m个通道，所以每个卷积核里要也要有m个通道</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/62/12/62fd6c269cee17e778f8d5acf085be12.jpeg?wh=1920x1080\" alt=\"\"></p><p>其实卷积背后的理论比较复杂，但在PyTorch中实现却很简单。在卷积计算中涉及的几大要素：输入通道数、输出通道数、步长、padding、卷积核的大小，分别对应的就是PyTorch中nn.Conv2d的关键参数。所以，就像前面讲的那样，我们要熟练用好nn.Conv2d()。</p><p>之后，我还带你做了一个验证same方式的练习，动手跑跑代码会帮你形成直观印象，快速掌握这部分内容。</p><p>当然，对于卷积来说不光光有今天介绍的这种比较标准的卷积，还有各种变形。例如，今天没有讲到的dilation参数与groups参数，基于这两个参数实现的卷积操作，我会在下一节课中为展开，敬请期待。</p><h2>每课一练</h2><p>请你想一想，padding为’same’时，stride可以为1以外的数值吗？</p><p>欢迎你在留言区记录你的疑问或收获，也推荐你把这节课分享给更多朋友、同事。</p><p>我是方远，我们下节课见！</p>","comments":[{"had_liked":false,"id":319178,"user_name":"vcjmhg","can_delete":false,"product_type":"c1","uid":1526461,"ip_address":"","ucode":"B508D1E9B3F974","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132","comment_is_top":false,"comment_ctime":1635660690,"is_pvip":false,"replies":[{"id":"115818","content":"你好，vcjmhg。感谢你的留言，👍🏻👍🏻👍🏻👍🏻^^","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635860924,"ip_address":"","comment_id":319178,"utype":1}],"discussion_count":2,"race_medal":0,"score":"66060170130","product_id":100093301,"comment_content":"不可以，会出现ValueError: padding=&#39;same&#39; is not supported for strided convolutions。<br>原因分析：<br>1. 引入padding = &#39;same&#39;的目的实际上就是为了让输入特征保留更多的信息。而将stride设置成2，则是为了压缩特征，这和padding=&#39;same&#39; 的作用刚好相反<br>2. 假定允许在padding = &#39;same&#39;的情况下，将stride设置成2，则计算出来特征图，左侧和右侧的部分都会是0，提取到的这份部分特征是没有意义的。<br><br>此外我们在使用padding =  &#39;same&#39;时，一定要保证pytorch的版本大于等于1.9否则会出现RuntimeError: argument 1 (padding) must be tuple..这样的错误","like_count":15,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529534,"discussion_content":"你好，vcjmhg。感谢你的留言，👍🏻👍🏻👍🏻👍🏻^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635860924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1467446,"avatar":"","nickname":"clee","note":"","ucode":"122875102B23A8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":412891,"discussion_content":"没明白vcjmhg所说的第2点，为什么左侧和右侧的部分都会是0？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636302724,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":318940,"user_name":"Geek_8a391d","can_delete":false,"product_type":"c1","uid":2823414,"ip_address":"","ucode":"7A56B872B72D9F","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM5RJskfTOqjCBNJ7dH0Th9L4yPsC8bXSpEpnJTtIfIMx0ia6icOt6UluD0rxBh2vF4xibQKfxRGfr9EA/132","comment_is_top":false,"comment_ctime":1635487094,"is_pvip":false,"replies":[{"id":"115609","content":"你好，感谢你的留言。我刚接触卷积的时候跟你有相同的疑惑^^。卷积神经网络中的卷积参数都是通过训练得到的。<br>文中的提到的卷积是用在卷积神经网络中的，用来提取特征的。也有提前确定好的卷积参数，一般用在图像处理中，例如高斯blur等操作，都是事先确定好卷积参数的。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635493459,"ip_address":"","comment_id":318940,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23110323574","product_id":100093301,"comment_content":"老师您好，因为都是迁移学习从来没有思考过卷积的原理知识，一直有一个疑问，卷积核里面的值究竟是如何确定的呢？不同的卷积核会对卷积结果起到一个什么样的影响呢？我看很多模型会使用[-1 0 1]这样的形式，这样是不是对图像做了一个水平的分割呢？您在文中提到的这一个卷积核又起到什么作用？我们是通过训练得到的卷积核还是通过经验提前确定的呢？","like_count":5,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529437,"discussion_content":"你好，感谢你的留言。我刚接触卷积的时候跟你有相同的疑惑^^。卷积神经网络中的卷积参数都是通过训练得到的。\n文中的提到的卷积是用在卷积神经网络中的，用来提取特征的。也有提前确定好的卷积参数，一般用在图像处理中，例如高斯blur等操作，都是事先确定好卷积参数的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635493459,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":332834,"user_name":"天凉好个秋","can_delete":false,"product_type":"c1","uid":2842361,"ip_address":"","ucode":"C61A17A9591C24","user_header":"https://static001.geekbang.org/account/avatar/00/2b/5e/f9/96896116.jpg","comment_is_top":false,"comment_ctime":1643709633,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"5938676929","product_id":100093301,"comment_content":"请问为什么我跑代码会出现相同结果但是会报错呢？<br>UserWarning: Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created<br>dilation默认为1为奇数造成的吗？怎么才可以取消这个报错呢","like_count":1,"discussions":[{"author":{"id":2710847,"avatar":"https://static001.geekbang.org/account/avatar/00/29/5d/3f/ad1fed4a.jpg","nickname":"黑暗骑士","note":"","ucode":"2A1C89298DD0D1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552541,"discussion_content":"同学，请问你解决了吗？我也是报这个错误","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1645505469,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553715,"discussion_content":"这是一个警告，他会对原始的tensor进行一个拷贝，如果不影响性能的可以无视。\n出来这个警告的原因如下，假设stride=1，dilation=1。\n根据https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html给出的输出，输入特征图的计算公式，我们可以获得。\nH_o = H_in +2p-k+1. 因为H_o 与H_in相等，\n所以k=2p + 1 ，这时如果k为偶数时，需要补零","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646038570,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":332683,"user_name":"Chloe","can_delete":false,"product_type":"c1","uid":1004953,"ip_address":"","ucode":"C4848ED5B35752","user_header":"https://static001.geekbang.org/account/avatar/00/0f/55/99/4bdadfd3.jpg","comment_is_top":false,"comment_ctime":1643470394,"is_pvip":false,"replies":[{"id":"121742","content":"你好，Chloe，感谢留言。<br>谢谢你的认可。^^","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1644199658,"ip_address":"","comment_id":332683,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5938437690","product_id":100093301,"comment_content":"老师讲的很好呀，kernel的概念用二维图像一说，马上就明白了。谢谢老师解惑","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":549670,"discussion_content":"你好，Chloe，感谢留言。\n谢谢你的认可。^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644199659,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319123,"user_name":"🚶","can_delete":false,"product_type":"c1","uid":2034680,"ip_address":"","ucode":"527991CE01AF67","user_header":"https://static001.geekbang.org/account/avatar/00/1f/0b/f8/a9f695dc.jpg","comment_is_top":false,"comment_ctime":1635597422,"is_pvip":false,"replies":[{"id":"115724","content":"你好，感谢你的留言。嗯嗯 可以这样理解^^","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635689422,"ip_address":"","comment_id":319123,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5930564718","product_id":100093301,"comment_content":"是不是可以理解为每次训练都是为了确定合适的卷积核参数呢？","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529509,"discussion_content":"你好，感谢你的留言。嗯嗯 可以这样理解^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635689422,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":318951,"user_name":"lwg0452","can_delete":false,"product_type":"c1","uid":1853479,"ip_address":"","ucode":"0B58A1C80F9074","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKyEJx6dG2dMuMz6swdfjNuw3HMoEAbtxprfdBUAyRpLFayxmwEiaYLs224LuAdwWu55ENLgsI8U4w/132","comment_is_top":false,"comment_ctime":1635489247,"is_pvip":false,"replies":[{"id":"115646","content":"你好，lwg0452，感谢你的留言。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻。加油^^。如果可以的话，实现起来也比较麻烦，比较难判断输出特征图宽与高如何才能与输入的相同。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635586285,"ip_address":"","comment_id":318951,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5930456543","product_id":100093301,"comment_content":"padding 为’same’时，stride 不可以为 1 以外的数值（会报错）。<br>个人理解：假如可以的话，用文中的例子计算输出特征图只有左上部分不是0，图像边缘信息还是丢失了，没有意义。","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529441,"discussion_content":"你好，lwg0452，感谢你的留言。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻。加油^^。如果可以的话，实现起来也比较麻烦，比较难判断输出特征图宽与高如何才能与输入的相同。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635586285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":359302,"user_name":"chenyuping","can_delete":false,"product_type":"c1","uid":2821759,"ip_address":"北京","ucode":"4CDA32B255C724","user_header":"","comment_is_top":false,"comment_ctime":1665398590,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1665398590","product_id":100093301,"comment_content":"老师，您好，请问padding=1和same的效果是一样的吗？ <br>对这个我有一点疑惑，padding=1是补一圈0，然后same的时候我看的例子是在右边和下边补0。感觉两者效果不一样吧，因为padding=1的时候，算下来会比原来的长宽，各多出一个像素吧？<br><br>另外，为什么valid 和same 的时候 stride只能是1，这个没有想明白。<br>","like_count":0},{"had_liked":false,"id":353165,"user_name":"John(易筋)","can_delete":false,"product_type":"c1","uid":1180202,"ip_address":"北京","ucode":"BB4E58DD4B8F15","user_header":"https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg","comment_is_top":false,"comment_ctime":1659226837,"is_pvip":true,"replies":[{"id":"128549","content":"👍🏻，理解的很棒。<br>平移不变性我的理解跟你稍有不同。有限数据是很重要的，最好要保证数据的多样性，CNN本身就具有平移不变性的优点。两者结合才可以获得很好的效果。<br>","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1659509712,"ip_address":"北京","comment_id":353165,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1659226837","product_id":100093301,"comment_content":"平移不变性（translation invariant）指的是CNN对于同一张图及其平移后的版本，都能输出同样的结果。这对于图像分类（image classification）问题来说肯定是最理想的，因为对于一个物体的平移并不应该改变它的类别。而对于其它问题，比如物体检测（detection）、物体分割（segmentation）来说，这个性质则不应该有，原因是当输入发生平移时，输出也应该相应地进行平移。这种性质又称为平移等价性（translation equivalence）。<br>稀疏连接学生认为是stride，可以跳过一些计算，池化层可以缩小图片大小。<br><br>参数的学习需要数据，由于数据中平移的分布一般都比较不均匀，引入平移的数据增强（augmentation）肯定是必要的。其实裁切（crop）就是一种平移的数据增强方式，因为不同裁切方式对应的patch之间的变换就是平移。而且这种方式相比于平移更加自然，没有周围的黑边padding，因此更加常用。总结起来，就是CNN的平移不变性主要是通过数据学习来的，结构只能带来非常弱的平移不变性，而学习又依赖于数据增强中的裁切，裁切相当于一种更好的图像平移。<br><br>希望老师点评，wx: zgpeace<br>参考：https:&#47;&#47;www.zhihu.com&#47;question&#47;301522740&#47;answer&#47;531606623","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":582566,"discussion_content":"👍🏻，理解的很棒。\n平移不变性我的理解跟你稍有不同。有限数据是很重要的，最好要保证数据的多样性，CNN本身就具有平移不变性的优点。两者结合才可以获得很好的效果。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1659509712,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":347852,"user_name":"梁","can_delete":false,"product_type":"c1","uid":2035865,"ip_address":"","ucode":"F42357A654931C","user_header":"https://static001.geekbang.org/account/avatar/00/1f/10/99/40e067d4.jpg","comment_is_top":false,"comment_ctime":1654513323,"is_pvip":false,"replies":[{"id":"126796","content":"hi，梁，你好，谢谢你的留言。<br>在后续的实战中会有案例，你可以参考一下。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1654567238,"ip_address":"","comment_id":347852,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1654513323","product_id":100093301,"comment_content":"你好，老师，请问有无这些文章中的完整案例，直接看只言片语感觉比较吃力","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":575039,"discussion_content":"hi，梁，你好，谢谢你的留言。\n在后续的实战中会有案例，你可以参考一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654567238,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":345869,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1652667711,"is_pvip":false,"replies":[{"id":"126240","content":"👍🏻👍🏻👍🏻^^","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1652749179,"ip_address":"","comment_id":345869,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1652667711","product_id":100093301,"comment_content":"不能。<br>ValueError: padding=&#39;same&#39; is not supported for strided convolutions","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":572364,"discussion_content":"👍🏻👍🏻👍🏻^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652749179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":343120,"user_name":"Zeurd","can_delete":false,"product_type":"c1","uid":2919792,"ip_address":"","ucode":"7CE1E0F76C3313","user_header":"https://static001.geekbang.org/account/avatar/00/2c/8d/70/b0047299.jpg","comment_is_top":false,"comment_ctime":1650639972,"is_pvip":true,"replies":[{"id":"125352","content":"你好，Zeurd。感谢你的留言。<br>这是一个警告，他会对原始的tensor进行一个拷贝，如果不影响性能的可以无视。<br>出来这个警告的原因如下，假设stride=1，dilation=1。<br>根据https:&#47;&#47;pytorch.org&#47;docs&#47;stable&#47;generated&#47;torch.nn.Conv2d.html给出的输出，输入特征图的计算公式，我们可以获得。<br>H_o = H_in +2p-k+1. 因为H_o 与H_in相等，<br>所以k=2p + 1 ，这时如果k为偶数时，需要补零","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1650767906,"ip_address":"","comment_id":343120,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1650639972","product_id":100093301,"comment_content":"跑的时候会报Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created的错，是新版本需要先创副本么？要怎么操作呢？ ","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":566817,"discussion_content":"你好，Zeurd。感谢你的留言。\n这是一个警告，他会对原始的tensor进行一个拷贝，如果不影响性能的可以无视。\n出来这个警告的原因如下，假设stride=1，dilation=1。\n根据https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html给出的输出，输入特征图的计算公式，我们可以获得。\nH_o = H_in +2p-k+1. 因为H_o 与H_in相等，\n所以k=2p + 1 ，这时如果k为偶数时，需要补零","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650767906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":340106,"user_name":"jssfy","can_delete":false,"product_type":"c1","uid":1137238,"ip_address":"","ucode":"F16353CFE607B7","user_header":"https://static001.geekbang.org/account/avatar/00/11/5a/56/115c6433.jpg","comment_is_top":false,"comment_ctime":1648605573,"is_pvip":false,"replies":[{"id":"124425","content":"你好jssfy，感谢留言。<br>你再往下看看就应该能明白了。<br>你检索&quot;输入特征图有 m 个通道，所以每个卷积核里要也要有 m 个通道。&quot;这句话，你看下面的那些卷积核就是有多个通道的。<br>卷积核通道数与输入特征的通道数是一样的。<br>只有一个通道的原因是输入的特征只有一个通道。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1648690876,"ip_address":"","comment_id":340106,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1648605573","product_id":100093301,"comment_content":"卷积核是什么呢？其实就是我们卷积层要学习到的参数，就像下图中红色的示例，下图中的卷积核是最简单的情况，只有一个通道。<br><br>请问<br>1. 这里的通道只有一个怎么理解，玕没太明白?","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":559293,"discussion_content":"你好jssfy，感谢留言。\n你再往下看看就应该能明白了。\n你检索&#34;输入特征图有 m 个通道，所以每个卷积核里要也要有 m 个通道。&#34;这句话，你看下面的那些卷积核就是有多个通道的。\n卷积核通道数与输入特征的通道数是一样的。\n只有一个通道的原因是输入的特征只有一个通道。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648690876,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":336943,"user_name":"黑暗骑士","can_delete":false,"product_type":"c1","uid":2710847,"ip_address":"","ucode":"2A1C89298DD0D1","user_header":"https://static001.geekbang.org/account/avatar/00/29/5d/3f/ad1fed4a.jpg","comment_is_top":false,"comment_ctime":1646483655,"is_pvip":false,"replies":[{"id":"123172","content":"hello, 不好意思哈。没太理解你的意思。能再稍微详细点吗？","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1646630610,"ip_address":"","comment_id":336943,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1646483655","product_id":100093301,"comment_content":"老师好，<br>请问padding=&#39;same&#39;时，第一行和第一列都只进行了一次卷积运算，而其他部分都进行了两次，如何确保这一部分信息不被丢失呢？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":554819,"discussion_content":"hello, 不好意思哈。没太理解你的意思。能再稍微详细点吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646630610,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335401,"user_name":"黑暗骑士","can_delete":false,"product_type":"c1","uid":2710847,"ip_address":"","ucode":"2A1C89298DD0D1","user_header":"https://static001.geekbang.org/account/avatar/00/29/5d/3f/ad1fed4a.jpg","comment_is_top":false,"comment_ctime":1645505350,"is_pvip":false,"replies":[{"id":"122892","content":"这是一个警告，他会对原始的tensor进行一个拷贝，如果不影响性能的可以无视。<br>出来这个警告的原因如下，假设stride=1，dilation=1。<br>根据https:&#47;&#47;pytorch.org&#47;docs&#47;stable&#47;generated&#47;torch.nn.Conv2d.html给出的输出，输入特征图的计算公式，我们可以获得。<br>H_o = H_in +2p-k+1. 因为H_o 与H_in相等，<br>所以k=2p + 1 ，这时如果k为偶数时，需要补零","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1646038541,"ip_address":"","comment_id":335401,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1645505350","product_id":100093301,"comment_content":"老师，最后结果出不来，报警告<br><br>Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553714,"discussion_content":"这是一个警告，他会对原始的tensor进行一个拷贝，如果不影响性能的可以无视。\n出来这个警告的原因如下，假设stride=1，dilation=1。\n根据https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html给出的输出，输入特征图的计算公式，我们可以获得。\nH_o = H_in +2p-k+1. 因为H_o 与H_in相等，\n所以k=2p + 1 ，这时如果k为偶数时，需要补零","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646038541,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2710847,"avatar":"https://static001.geekbang.org/account/avatar/00/29/5d/3f/ad1fed4a.jpg","nickname":"黑暗骑士","note":"","ucode":"2A1C89298DD0D1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":554600,"discussion_content":"谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646483688,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":553714,"ip_address":""},"score":554600,"extra":""}]}]},{"had_liked":false,"id":325488,"user_name":"徐洲更","can_delete":false,"product_type":"c1","uid":1314643,"ip_address":"","ucode":"F8A323CB732D05","user_header":"https://static001.geekbang.org/account/avatar/00/14/0f/53/92a50f01.jpg","comment_is_top":false,"comment_ctime":1638975532,"is_pvip":true,"replies":[{"id":"118099","content":"对，是这样的。^^","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1639015762,"ip_address":"","comment_id":325488,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1638975532","product_id":100093301,"comment_content":"卷积核让我想到了传统图像处理中的HORRIS这些方法，有着类似的卷积运算。只不过，卷积神经网络是随机初始化卷积核里的数值，通过不断的训练，得到比较合适的参数，而传统的方法可能偏向于人工设计一些比较好的卷积核，用于某些特定的任务。不知道，这样子理解是否合适呢？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537261,"discussion_content":"对，是这样的。^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639015763,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322675,"user_name":"Allen","can_delete":false,"product_type":"c1","uid":1738275,"ip_address":"","ucode":"0A4EA62199D198","user_header":"https://static001.geekbang.org/account/avatar/00/1a/86/23/f1dbd526.jpg","comment_is_top":false,"comment_ctime":1637548046,"is_pvip":true,"replies":[{"id":"117177","content":"hello，Allen, 你好，感谢你的留言。<br>我用第14节课中的例子解释一下。<br><br>首先，nn.Parameter可以将一个tensor关联到一个module中，这样在module的前向&amp;#47;反向传播的过程中就可以更新这个tensor了。<br><br>你看下面代码中的，self.weight = torch.tensor([11.], requires_grad=True)，并没有使用nn.Parameter。<br>所以，他并不会被关联到LinearModel中。<br>import torch<br>from torch import nn<br><br>class LinearModel(nn.Module):<br>  def __init__(self):<br>    super().__init__()<br>    #self.weight = nn.Parameter(torch.randn(1))<br>    self.weight = torch.tensor([11.], requires_grad=True)<br>    print(self.weight)<br>    self.bias = nn.Parameter(torch.randn(1))<br><br>  def forward(self, input):<br>    return (input * self.weight) + self.bias<br><br>经过多轮训练后，我打印LinearModel的参数，<br>for parameter in model.named_parameters():<br>  print(parameter)<br>输出的是：<br>(&#39;bias&#39;, Parameter containing:<br>tensor([16.4081], requires_grad=True))<br><br>这里并没有weight参数。<br><br>当预测的时候：<br>model(torch.tensor(1))<br>输出的是：tensor([27.4081], grad_fn=&amp;lt;AddBackward0&amp;gt;)<br>恰巧27.4081 = 1 * 11. + 16.4081<br>","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1637567918,"ip_address":"","comment_id":322675,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637548046","product_id":100093301,"comment_content":"nn.Parameter这一行的用法老师能再详细解说一下吗？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532290,"discussion_content":"hello，Allen, 你好，感谢你的留言。\n我用第14节课中的例子解释一下。\n\n首先，nn.Parameter可以将一个tensor关联到一个module中，这样在module的前向&amp;#47;反向传播的过程中就可以更新这个tensor了。\n\n你看下面代码中的，self.weight = torch.tensor([11.], requires_grad=True)，并没有使用nn.Parameter。\n所以，他并不会被关联到LinearModel中。\nimport torch\nfrom torch import nn\n\nclass LinearModel(nn.Module):\n  def __init__(self):\n    super().__init__()\n    #self.weight = nn.Parameter(torch.randn(1))\n    self.weight = torch.tensor([11.], requires_grad=True)\n    print(self.weight)\n    self.bias = nn.Parameter(torch.randn(1))\n\n  def forward(self, input):\n    return (input * self.weight) + self.bias\n\n经过多轮训练后，我打印LinearModel的参数，\nfor parameter in model.named_parameters():\n  print(parameter)\n输出的是：\n(&#39;bias&#39;, Parameter containing:\ntensor([16.4081], requires_grad=True))\n\n这里并没有weight参数。\n\n当预测的时候：\nmodel(torch.tensor(1))\n输出的是：tensor([27.4081], grad_fn=&amp;lt;AddBackward0&amp;gt;)\n恰巧27.4081 = 1 * 11. + 16.4081\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637567918,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320552,"user_name":"咪西小悠","can_delete":false,"product_type":"c1","uid":2823411,"ip_address":"","ucode":"732E856780999D","user_header":"https://static001.geekbang.org/account/avatar/00/2b/14/f3/175dc943.jpg","comment_is_top":false,"comment_ctime":1636376038,"is_pvip":false,"replies":[{"id":"116255","content":"你好，咪西小悠。感谢你的留言。<br>恩 是的，是在1.9才有的。在Tensorflow中是很早就支持same与valid的参数了。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636422992,"ip_address":"","comment_id":320552,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636376038","product_id":100093301,"comment_content":"padding = &#39;same&#39;&#47;&#39;valid&#39; 是在pytorch 1.9之后的版本才支持的，我用的1.8版本会报错：conv2d(): argument &#39;padding&#39; (position 5) must be tuple of ints, not str","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530043,"discussion_content":"你好，咪西小悠。感谢你的留言。\n恩 是的，是在1.9才有的。在Tensorflow中是很早就支持same与valid的参数了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636422992,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319809,"user_name":"Geek_8cfc4c","can_delete":false,"product_type":"c1","uid":2156103,"ip_address":"","ucode":"83EF0B576530E2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7uYZvwhrLHsJICstGXaOvUNZnyq0aO7gF0OLicMyZAZFnRiaDyvM1lGxnEDP2VUMV3m6UjiazMmSNGQ/132","comment_is_top":false,"comment_ctime":1635949684,"is_pvip":false,"replies":[{"id":"116071","content":"你好，感谢你的留言。<br>是的，当m等于1的时候，就是开篇中介绍的最简单的情况，输入数据只有1个通道。所以就不会有按位求和这回事了。<br>当m不等于1的时候，就要按位求和了。<br><br>关于第二个问题，<br>所有输入的通道与1个大小为(m, k, k)的卷积核进行卷积计算后，生成n个输出特征图中的1个特征图。一共重复n次，就会生成n个(w&#39;, h&#39;)的特征图。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636117493,"ip_address":"","comment_id":319809,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1635949684","product_id":100093301,"comment_content":"老师你好：<br><br>文中这句没看懂……<br>&quot;这样计算后会生成 m 个特征图，然后将这 m 个特征图按对应位置求和即可，<br>求和后 m 个特征图合并为输出特征中一个通道的特征图。&quot;<br>假设m等于1，那么 带入原文就是<br>&quot;这样计算后会生成 1 个特征图，然后将这 1 个特征图按对应位置求和即可，<br>求和后 1 个特征图合并为输出特征中一个通道的特征图。&quot; 中的 &quot;按对应位置求和&quot; 指的是什么呢，<br>计算后的特征图不就已经是输出特征图了么？<br><br>另外，n个卷积核，每个大小都是(m, k, k), 那么输入图的每个通道都要经过n个卷积核计算，生成n个输出图，所以最后的形状是<br>(n, w&#39;, h&#39;)这么理解对么？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529775,"discussion_content":"你好，感谢你的留言。\n是的，当m等于1的时候，就是开篇中介绍的最简单的情况，输入数据只有1个通道。所以就不会有按位求和这回事了。\n当m不等于1的时候，就要按位求和了。\n\n关于第二个问题，\n所有输入的通道与1个大小为(m, k, k)的卷积核进行卷积计算后，生成n个输出特征图中的1个特征图。一共重复n次，就会生成n个(w&amp;#39;, h&amp;#39;)的特征图。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636117493,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}