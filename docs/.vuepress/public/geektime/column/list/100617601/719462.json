{"id":719462,"title":"23｜易速鲜花聊天客服机器人的开发（下）","content":"<p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>上节课，咱们的聊天机器人已经基本完成，这节课，我们要看一看如何把它部署到网络上。</p><h2>“聊天机器人”项目说明</h2><p>简单回顾一下这个项目的设计。</p><p><strong>第一步：</strong>通过LangChain的ConversationChain，实现一个最基本的聊天对话工具。</p><p><strong>第二步：</strong>通过LangChain中的记忆功能，让这个聊天机器人能够记住用户之前所说的话。</p><p><strong>第三步：</strong>通过LangChain中的检索功能，整合易速鲜花的内部文档资料，让聊天机器人不仅能够基于自己的知识，还可以基于易速鲜花的业务流程，给出专业的回答。</p><p><strong>第四步（可选）：</strong>通过LangChain中的数据库查询功能，用户可以输入订单号来查询订单状态，或者看看有没有存货等等。</p><p><strong>第五步：</strong>在网络上部署及发布这个聊天机器人，供企业内部员工和易速鲜花用户使用。</p><p>在上一个项目中，我们是通过 Flask 部署的人脉工具。Flask是一个通用的、微型的Web应用框架，非常适合创建各种Web应用程序，不仅仅局限于机器学习或数据科学项目。Flask为开发者提供了很高的灵活性，你可以自定义路由、模板、前端和后端的交互等等。对于初学者，Flask可能需要更长时间来学习，尤其是需要结合其他前端技术或数据库技术时。</p><!-- [[[read_end]]] --><p>不过，对于机器学习项目来说，我们还有其他部署方案。比如 Streamlit 和 Gradio，就为机器学习和数据科学应用提供了快速、专门化的解决方案。如果你的项目目标是快速展示和验证模型效果，那么 Streamlit 和 Gradio 是优秀的选择。这些框架提供了简单易用的 API 和丰富的可视化组件，让你可以用少量代码快速构建交互式应用程序，提高你的开发效率，也可以更好地展示工作成果。</p><p>下面，我就带着你用这两种机器学习部署框架来展示我们的聊天机器人。</p><h2>方案 1 ：通过 Streamlit 部署聊天机器人</h2><p>首先来看看Streamlit。这是一个挺有名的专门为数据科学家和机器学习工程师设计的开源Python库，它可以迅速地将Python脚本转化为交互式Web应用。</p><p><strong>Streamlit</strong> <strong>的一些主要特点和亮点包括：</strong></p><ul>\n<li>简易性：Streamlit 的真正魅力在于它的简单性，只需几行代码，你就可以为其数据或模型创建交互式应用。</li>\n<li>无需前端经验：与传统的Web开发框架相比，使用 Streamlit，你不需要深入了解HTML、CSS或JavaScript，所有交互都是通过Python代码来管理的。</li>\n<li>实时交互：当你更改代码或数据时，Streamlit 应用会实时更新，这为迭代和实验提供了极大的便利。</li>\n<li>内置组件：Streamlit 附带了许多内置的可视化和交互组件，如滑块、按钮、表格等，可以无缝集成到你的应用中。</li>\n<li>数据集可视化：除了基本的图形和图表，Streamlit 还支持其他数据可视化库，如 Plotly、Matplotlib 和 Altair，使你能够轻松地展示数据。</li>\n<li>设计简洁：Streamlit 的界面设计简洁而优雅，使得应用程序看起来既专业又时尚。</li>\n<li>部署和共享：尽管 Streamlit 专注于创建应用，但它也有与部署和分享相关的工具和整合，如 Streamlit Sharing，允许你免费托管其应用。</li>\n<li>社区与生态系统：Streamlit 拥有一个积极的开源社区，定期提供新的功能更新、组件和扩展。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/71/6a/718fbd049acdf8681185805384028f6a.jpg?wh=1448x679\" alt=\"\"></p><p>我们用下面的语句，安装Streamlit。</p><pre><code class=\"language-plain\">pip install streamlit\n</code></pre><p>然后，简单的几行代码，就可以做出一个网页版的小程序。</p><pre><code class=\"language-plain\">import streamlit as st\n\n# 设置标题\nst.title('平方计算器')\n\n# 创建一个滑块\nnumber = st.slider(\"Select a number:\", min_value=0, max_value=100)\n\n# 显示选中数字的平方\nst.write(f\"Square of {number} is {number ** 2}\")\n</code></pre><p>用 <code>streamlit run &lt;your_script_name&gt;.py</code>（注意，必须是streamlit run命令，而不是通过python命令来跑程序）来运行程序，就可以在浏览器中看到它。</p><pre><code class=\"language-plain\">streamlit run 01_SimpleStreamlit.py\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/22/ee/22e0fcf90978160e1f50d75fe9a33dee.jpg?wh=661x458\" alt=\"\"></p><p>此时，在 localhost 的 8501 端口，程序开始启动。</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/87/ef4e3a09a3df9776330b343e721fbe87.jpg?wh=1364x1189\" alt=\"\"></p><p>下面就通过 Streamlit 来重构聊天机器人。</p><pre><code class=\"language-plain\"># 导入所需的库\nimport os\nimport streamlit as st\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Qdrant\nfrom langchain.memory import ConversationSummaryMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import Docx2txtLoader\nfrom langchain.document_loaders import TextLoader\n\n# 设置OpenAI API密钥\nos.environ[\"OPENAI_API_KEY\"] = 'Your OpenAI Key' &nbsp;\n\n# ChatBot类的实现\nclass ChatbotWithRetrieval:\n\n&nbsp; &nbsp; def __init__(self, dir):\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 加载Documents\n&nbsp; &nbsp; &nbsp; &nbsp; base_dir = dir # 文档的存放目录\n&nbsp; &nbsp; &nbsp; &nbsp; documents = []\n&nbsp; &nbsp; &nbsp; &nbsp; for file in os.listdir(base_dir): \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; file_path = os.path.join(base_dir, file)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if file.endswith('.pdf'):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loader = PyPDFLoader(file_path)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif file.endswith('.docx') or file.endswith('.doc'):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loader = Docx2txtLoader(file_path)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif file.endswith('.txt'):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loader = TextLoader(file_path)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 文本的分割\n&nbsp; &nbsp; &nbsp; &nbsp; text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n&nbsp; &nbsp; &nbsp; &nbsp; all_splits = text_splitter.split_documents(documents)\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 向量数据库\n&nbsp; &nbsp; &nbsp; &nbsp; self.vectorstore = Qdrant.from_documents(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents=all_splits, # 以分块的文档\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; location=\":memory:\", &nbsp;# in-memory 存储\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; collection_name=\"my_documents\",) # 指定collection_name\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化LLM\n&nbsp; &nbsp; &nbsp; &nbsp; self.llm = ChatOpenAI()\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化Memory\n&nbsp; &nbsp; &nbsp; &nbsp; self.memory = ConversationSummaryMemory(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; llm=self.llm, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory_key=\"chat_history\", \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return_messages=True\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 设置Retrieval Chain\n&nbsp; &nbsp; &nbsp; &nbsp; retriever = self.vectorstore.as_retriever()\n&nbsp; &nbsp; &nbsp; &nbsp; self.qa = ConversationalRetrievalChain.from_llm(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.llm, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retriever=retriever, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory=self.memory\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n\n&nbsp; &nbsp; def chat_loop(self):\n&nbsp; &nbsp; &nbsp; &nbsp; print(\"Chatbot 已启动! 输入'exit'来退出程序。\")\n&nbsp; &nbsp; &nbsp; &nbsp; while True:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; user_input = input(\"你: \")\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if user_input.lower() == 'exit':\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(\"再见!\")\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 调用 Retrieval Chain &nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; response = self.qa(user_input)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(f\"Chatbot: {response['answer']}\")\n\n# Streamlit界面的创建\ndef main():\n&nbsp; &nbsp; st.title(\"易速鲜花聊天客服\")\n\n&nbsp; &nbsp; # Check if the 'bot' attribute exists in the session state\n&nbsp; &nbsp; if \"bot\" not in st.session_state:\n&nbsp; &nbsp; &nbsp; &nbsp; st.session_state.bot = ChatbotWithRetrieval(\"OneFlower\")\n\n&nbsp; &nbsp; user_input = st.text_input(\"请输入你的问题：\")\n&nbsp; &nbsp; \n&nbsp; &nbsp; if user_input:\n&nbsp; &nbsp; &nbsp; &nbsp; response = st.session_state.bot.qa(user_input)\n&nbsp; &nbsp; &nbsp; &nbsp; st.write(f\"Chatbot: {response['answer']}\")\n\nif __name__ == \"__main__\":\n&nbsp; &nbsp; main()\n</code></pre><p>以下是使用 Streamlit 进行的更改和添加功能的简要说明。</p><ol>\n<li>\n<p>界面创建：<br>\na.  <code>st.title(\"易速鲜花聊天客服\")</code>：设置 Web 应用程序的标题为“易速鲜花聊天客服”。<br>\n&nbsp;</p>\n</li>\n<li>\n<p>会话状态：<br>\na. 使用 <code>st.session_state</code> 来存储用户会话状态。这是 Streamlit 的一个特性，允许你在用户与应用程序交互时保存变量。<br>\nb. <code>if \"bot\" not in st.session_state</code>：检查是否已经有一个 bot 实例存在于 session state 中。如果没有，就创建一个新的 ChatbotWithRetrieval 实例，并将其保存到 session state。这样做的好处是可以避免在每次用户与应用程序交互时重新初始化机器人。<br>\n&nbsp;</p>\n</li>\n<li>\n<p>用户交互：<br>\na. <code>user_input = st.text_input(\"请输入你的问题：\")</code>：创建一个文本输入框供用户输入问题。当用户输入内容并提交后，代码会获取用户的输入，并使用聊天机器人的 qa 方法来获取响应。<br>\nb. <code>st.write(f\"Chatbot: {response['answer']}\")</code>：在应用程序界面上显示机器人的响应。<br>\n&nbsp;</p>\n</li>\n<li>\n<p>主函数中，当脚本被执行时，它将启动 Streamlit 服务器，并显示创建的 Web 应用程序。</p>\n</li>\n</ol><p>用 <code>streamlit run</code> 运行程序，就可以开始聊天了！</p><p><img src=\"https://static001.geekbang.org/resource/image/1f/49/1f380b60c8cdec362c9c8070c919d849.jpg?wh=3840x2076\" alt=\"\"></p><h2>方案2 ：通过 Gradio 部署聊天机器人</h2><p>与 Streamlit 不同，Gradio 界面更侧重于模型的交互，据说上手也更简单。这使得 Gradio 非常适合展示和测试机器学习模型。我在GitHub上看到很多新的开源LLM都是提供一个Gradio UI界面来进行测试的。相比之下， Streamlit 则提供了更丰富的 Web 应用开发功能。</p><p>到底有多简单，等会你看到 Gradio UI的界面，你就明白我的意思了。</p><p>下面，我们先安装这个包。</p><pre><code class=\"language-plain\">pip install gradio\n</code></pre><p>通过 Gradio 框架重构聊天机器人的程序代码如下：</p><pre><code class=\"language-plain\"># 导入所需的库\nimport os\nimport gradio as gr\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Qdrant\nfrom langchain.memory import ConversationSummaryMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import Docx2txtLoader\nfrom langchain.document_loaders import TextLoader\n\n# 设置OpenAI API密钥\nos.environ[\"OPENAI_API_KEY\"] = 'Your OpenAI Key' &nbsp;\n\nclass ChatbotWithRetrieval:\n&nbsp; &nbsp; def __init__(self, dir):\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 加载Documents\n&nbsp; &nbsp; &nbsp; &nbsp; base_dir = dir # 文档的存放目录\n&nbsp; &nbsp; &nbsp; &nbsp; documents = []\n&nbsp; &nbsp; &nbsp; &nbsp; for file in os.listdir(base_dir): \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; file_path = os.path.join(base_dir, file)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if file.endswith('.pdf'):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loader = PyPDFLoader(file_path)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif file.endswith('.docx') or file.endswith('.doc'):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loader = Docx2txtLoader(file_path)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif file.endswith('.txt'):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loader = TextLoader(file_path)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 文本的分割\n&nbsp; &nbsp; &nbsp; &nbsp; text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n&nbsp; &nbsp; &nbsp; &nbsp; all_splits = text_splitter.split_documents(documents)\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 向量数据库\n&nbsp; &nbsp; &nbsp; &nbsp; self.vectorstore = Qdrant.from_documents(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; documents=all_splits, # 以分块的文档\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; location=\":memory:\", &nbsp;# in-memory 存储\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; collection_name=\"my_documents\",) # 指定collection_name\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化LLM\n&nbsp; &nbsp; &nbsp; &nbsp; self.llm = ChatOpenAI()\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化Memory\n&nbsp; &nbsp; &nbsp; &nbsp; self.memory = ConversationSummaryMemory(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; llm=self.llm, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory_key=\"chat_history\", \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return_messages=True\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化对话历史\n&nbsp; &nbsp; &nbsp; &nbsp; self.conversation_history = \"\"\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 设置Retrieval Chain\n&nbsp; &nbsp; &nbsp; &nbsp; retriever = self.vectorstore.as_retriever()\n&nbsp; &nbsp; &nbsp; &nbsp; self.qa = ConversationalRetrievalChain.from_llm(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.llm, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; retriever=retriever, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory=self.memory\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )\n\n&nbsp; &nbsp; def get_response(self, user_input): &nbsp;# 这是为 Gradio 创建的新函数\n&nbsp; &nbsp; &nbsp; &nbsp; response = self.qa(user_input)\n&nbsp; &nbsp; &nbsp; &nbsp; # 更新对话历史\n&nbsp; &nbsp; &nbsp; &nbsp; self.conversation_history += f\"你: {user_input}\\nChatbot: {response['answer']}\\n\"\n&nbsp; &nbsp; &nbsp; &nbsp; return self.conversation_history\n\nif __name__ == \"__main__\":\n&nbsp; &nbsp; folder = \"OneFlower\"\n&nbsp; &nbsp; bot = ChatbotWithRetrieval(folder)\n\n&nbsp; &nbsp; # 定义 Gradio 界面\n&nbsp; &nbsp; interface = gr.Interface(\n&nbsp; &nbsp; &nbsp; &nbsp; fn=bot.get_response, &nbsp;# 使用我们刚刚创建的函数\n&nbsp; &nbsp; &nbsp; &nbsp; inputs=\"text\", &nbsp;# 输入是文本\n&nbsp; &nbsp; &nbsp; &nbsp; outputs=\"text\", &nbsp;# 输出也是文本\n&nbsp; &nbsp; &nbsp; &nbsp; live=False, &nbsp;# 实时更新，这样用户可以连续与模型交互\n&nbsp; &nbsp; &nbsp; &nbsp; title=\"易速鲜花智能客服\", &nbsp;# 界面标题\n&nbsp; &nbsp; &nbsp; &nbsp; description=\"请输入问题，然后点击提交。\" &nbsp;# 描述\n&nbsp; &nbsp; )\n&nbsp; &nbsp; interface.launch() &nbsp;# 启动 Gradio 界面\n</code></pre><p>以下是 Gradio 部分代码的详细解释。</p><ol>\n<li><code>get_response(self, user_input)</code>：这个新函数是为 Gradio 创建的，它接收用户输入作为参数，并返回机器人的响应。为了保持聊天历史连续性，此函数将每次的用户输入和机器人的响应添加到 conversation_history，并返回整个聊天历史。<br>\n&nbsp;</li>\n<li>使用 <code>gr.Interface()</code> 来定义 Gradio 界面。<code>fn=bot.get_response</code>：设置界面的主函数为刚刚创建的 get_response 函数。<code>live=False</code>：确保实时更新是关闭的，这意味着用户需要点击提交按钮来发送他们的问题，而不是一边打字，一边生成回答的流模式（比较适合展示生成式模型）。<br>\n&nbsp;</li>\n<li>启动 Gradio 界面。<code>interface.launch()</code>：调用这个方法会启动 Gradio 的 Web 服务器，并在默认的 Web 浏览器中打开一个新窗口，显示刚刚定义的界面，用户可以通过这个界面与机器人交互。</li>\n</ol><p>运行程序，聊天机器人在本地端口 7860 上启动。</p><p><img src=\"https://static001.geekbang.org/resource/image/28/26/28d3c772116bf4733ed55e937e476d26.jpg?wh=408x53\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/46/d8/460fa6854ffa68ef4bfa54056cb034d8.jpg?wh=2140x1499\" alt=\"\"></p><p>这里，输入输出窗口的配置更加清晰，而且相对于原来的只记录一轮对话的机器人，这里我们增加了历史对话信息记录功能。</p><h2>总结时刻</h2><p>Streamlit 和 Gradio 都是让数据科学家和开发者能够快速为机器学习模型创建 Web UI 的框架。</p><ul>\n<li>Streamlit 是为数据应用、仪表板和可视化设计的。它提供了多种小部件，使得用户可以与数据和模型进行交互。它非常 Pythonic，意味着它的使用方式非常自然，对于熟悉Python的人来说非常直观。</li>\n<li>Gradio 更多是为了展示和演示机器学习模型。它提供了一种快速的方法，使非技术用户也能与机器学习模型进行交互，无需编写复杂的代码。</li>\n</ul><p>以下是对它们特点进行的对比总结。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/87/654d95bdb96e627c2464b4cedd964e87.jpg?wh=1110x752\" alt=\"\"></p><p>无论选择哪个框架，你都可以在非常短的时间内为你的应用创建一个Web UI。至于选择哪个，更多取决于你的具体需求和个人喜好。</p><p>如果你用这些框架，在你熟悉的业务场景中，利用LangChain和LLM的能力开发出了更为酷炫的LangChain应用，不要忘记在留言区中和大家分享！说说思路就行，有具体代码实现更好。</p><h2>思考题</h2><ol>\n<li>我的易速鲜花Chatbot有很多不完美的地方，比如，检索功能的设计不够细致，UI不够美观，等等。请你在这个Repo的基础上，大刀阔斧地进行改进。</li>\n<li>请你用Flask框架设计自己的Chatbot UI，重构聊天机器人，实现更多、更完善的功能。</li>\n<li>请你回过头去看看<a href=\"https://time.geekbang.org/column/article/699400\">第01讲</a>我给你留的3道思考题。那时候，你不了解LangChain，现在你已经基本掌握了它的精髓，能否把第01讲的思考题重新回答一遍呢？应该很有趣吧！</li>\n</ol><p>期待在留言区看到你的成果分享，如果觉得内容对你有帮助，也欢迎分享给有需要的朋友！</p>","comments":[{"had_liked":false,"id":383236,"user_name":"Yimmy","can_delete":false,"product_type":"c1","uid":1177248,"ip_address":"北京","ucode":"E66EAF7050C74A","user_header":"https://static001.geekbang.org/account/avatar/00/11/f6/a0/8ea0bfba.jpg","comment_is_top":false,"comment_ctime":1698732671,"is_pvip":false,"replies":[{"id":139601,"content":"已经更新啦","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698770633,"ip_address":"瑞士","comment_id":383236,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师好，实践环节的代码，在github上没有看到（第20-23节课程的）","like_count":3,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630672,"discussion_content":"已经更新啦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698770634,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383217,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1698713323,"is_pvip":false,"replies":[{"id":139598,"content":"Streamlit 和 Gradio 主要设计用于创建数据应用和机器学习模型的交互式前端界面。它们为数据科学家和机器学习工程师提供了一个简单快速的方式来构建和共享他们的模型和数据分析的交互式界面。使用 Streamlit 和 Gradio 开发普通网站，理论上可以，但是不占优势。看法普通网站应选择类似于WordPress之类的建站工具对吧。\n","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698767897,"ip_address":"瑞士","comment_id":383217,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"请问：Streamlit和Gradio可以用来开发普通网站吗？","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630665,"discussion_content":"Streamlit 和 Gradio 主要设计用于创建数据应用和机器学习模型的交互式前端界面。它们为数据科学家和机器学习工程师提供了一个简单快速的方式来构建和共享他们的模型和数据分析的交互式界面。使用 Streamlit 和 Gradio 开发普通网站，理论上可以，但是不占优势。看法普通网站应选择类似于WordPress之类的建站工具对吧。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698767897,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383842,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1699788640,"is_pvip":false,"replies":[{"id":140182,"content":"要使LangChain对外部提供API接口，需要设置一个中间层服务是吧，这一块内容我不是太懂。有经验的同学希望给分享分享。\n下面是ChatGPT给我的东西，我同学随便看看：\n假设您的中间层服务提供了一个&#47;langchain的API端点，Java应用可以这样调用：\nString url = &quot;http:&#47;&#47;your-middle-layer-service.com&#47;langchain&quot;;\n&#47;&#47; 构建请求体，包含需要LangChain处理的数据\nHttpEntity&lt;MyRequestData&gt; request = new HttpEntity&lt;&gt;(new MyRequestData(...));\nRestTemplate restTemplate = new RestTemplate();\n&#47;&#47; 发送请求并接收响应\nResponseEntity&lt;MyResponseData&gt; response = restTemplate.postForEntity(url, request, MyResponseData.class);\n通过这种方式，您可以使Java应用通过中间层服务与LangChain进行交互，利用LangChain提供的高级语言模型功能。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1700581633,"ip_address":"瑞士","comment_id":383842,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，UI有了，那LangChain如何对外部提供api接口呢？如java项目（springboot框架实现的），java语言的项目又可以通过什么方式调用LangChain提供的api接口？","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632230,"discussion_content":"要使LangChain对外部提供API接口，需要设置一个中间层服务是吧，这一块内容我不是太懂。有经验的同学希望给分享分享。\n下面是ChatGPT给我的东西，我同学随便看看：\n假设您的中间层服务提供了一个/langchain的API端点，Java应用可以这样调用：\nString url = &#34;http://your-middle-layer-service.com/langchain&#34;;\n// 构建请求体，包含需要LangChain处理的数据\nHttpEntity&lt;MyRequestData&gt; request = new HttpEntity&lt;&gt;(new MyRequestData(...));\nRestTemplate restTemplate = new RestTemplate();\n// 发送请求并接收响应\nResponseEntity&lt;MyResponseData&gt; response = restTemplate.postForEntity(url, request, MyResponseData.class);\n通过这种方式，您可以使Java应用通过中间层服务与LangChain进行交互，利用LangChain提供的高级语言模型功能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700581633,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1129543,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/47/3809bf42.jpg","nickname":"Drol🐵","note":"","ucode":"9BAD3E5F9580DB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":637500,"discussion_content":"langChain官方提供了一个Restful API框架langServer，它提供了快速搭建Restful API项目的脚手架，可以很方便的将langChain写的方法暴露成Restful API，同时自动生成API的Swagger文档。还会提供一个简单的playground页面可以直接在浏览器进行问答。langServer的服务端部分只是集成了FastAPI和Pydantic所以不会增加学习负担。FastAPI是流行的Python WEB框架之一；Pydantic是Python的类型标注和校验框架，相当于TypeScript之于JavaScript。\n\nlangServer官方文档：https://python.langchain.com/docs/langserve","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1708570859,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383625,"user_name":"Monin","can_delete":false,"product_type":"c1","uid":3144503,"ip_address":"上海","ucode":"AA6C4C4F19FA14","user_header":"https://static001.geekbang.org/account/avatar/00/2f/fb/37/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1699362462,"is_pvip":false,"replies":[{"id":140084,"content":"https:&#47;&#47;help.openai.com&#47;en&#47;articles&#47;8550641-assistants-api\nhttps:&#47;&#47;platform.openai.com&#47;docs&#47;assistants&#47;overview\n我快速看了一下，功能很相似。OpenAI的API也越来越强大了。\n可以说，1. 开发者多了一个选择 2. LangChain生态位仍在，因为毕竟还有其它大模型和接口以及各种各样的工具。\n一个功能的实现，总有多种选择，就像PyTorch和TensorFlow。最终的优劣如何，我们拭目以待。\n我作为程序员，如果我只使用GPT, OpenAI API能直接做，我就不套壳。如果做着做着觉得麻烦，还是LangChain方便，我再换。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1699977955,"ip_address":"瑞士","comment_id":383625,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师  咨询下  openAi最近发布的Assistant API   是不是意味着langchain的价值被替代了  Assistant API的agent，retrieval都不错\n","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631800,"discussion_content":"https://help.openai.com/en/articles/8550641-assistants-api\nhttps://platform.openai.com/docs/assistants/overview\n我快速看了一下，功能很相似。OpenAI的API也越来越强大了。\n可以说，1. 开发者多了一个选择 2. LangChain生态位仍在，因为毕竟还有其它大模型和接口以及各种各样的工具。\n一个功能的实现，总有多种选择，就像PyTorch和TensorFlow。最终的优劣如何，我们拭目以待。\n我作为程序员，如果我只使用GPT, OpenAI API能直接做，我就不套壳。如果做着做着觉得麻烦，还是LangChain方便，我再换。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699977955,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3737073,"avatar":"https://static001.geekbang.org/account/avatar/00/39/05/f1/a00779a1.jpg","nickname":"xxyc","note":"","ucode":"66C9411D73AFE9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631503,"discussion_content":"这个问题我也想到了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699625410,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"日本","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392798,"user_name":"2xshu","can_delete":false,"product_type":"c1","uid":1188473,"ip_address":"北京","ucode":"71584CB9676EDF","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKsz8j0bAayjSne9iakvjzUmvUdxWEbsM9iasQ74spGFayIgbSE232sH2LOWmaKtx1WqAFDiaYgVPwIQ/132","comment_is_top":false,"comment_ctime":1721806067,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"# 向量数据库 \nself.vectorstore = Qdrant.from_documents(\n documents=all_splits, # 以分块的文档 \n embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入\n location=&quot;:memory:&quot;, # in-memory 存储 \ncollection_name=&quot;my_documents&quot;,) # 指定collection_name\n\n这个地方是用OpenAIEmbeddings来做的向量嵌入，这样会消耗openai的调用次数吧？有平替的方法吗？\n","like_count":0}]}