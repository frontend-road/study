{"id":130907,"title":"39 | 从0搭建基于Kafka的企业级实时日志流处理平台","content":"<p>你好，我是胡夕。今天我要和你分享的主题是：从0搭建基于Kafka的企业级实时日志流处理平台。</p><p>简单来说，我们要实现一些大数据组件的组合，就如同玩乐高玩具一样，把它们“插”在一起，“拼”成一个更大一点的玩具。</p><p>在任何一个企业中，服务器每天都会产生很多的日志数据。这些数据内容非常丰富，包含了我们的<strong>线上业务数据</strong>、<strong>用户行为数据</strong>以及<strong>后端系统数据</strong>。实时分析这些数据，能够帮助我们更快地洞察潜在的趋势，从而有针对性地做出决策。今天，我们就使用Kafka搭建一个这样的平台。</p><h2>流处理架构</h2><p>如果在网上搜索实时日志流处理，你应该能够搜到很多教你搭建实时流处理平台做日志分析的教程。这些教程使用的技术栈大多是Flume+Kafka+Storm、Spark Streaming或Flink。特别是Flume+Kafka+Flink的组合，逐渐成为了实时日志流处理的标配。不过，要搭建这样的处理平台，你需要用到3个框架才能实现，这既增加了系统复杂度，也提高了运维成本。</p><p>今天，我来演示一下如何使用Apache Kafka这一个框架，实现一套实时日志流处理系统。换句话说，我使用的技术栈是Kafka Connect+Kafka Core+Kafka Streams的组合。</p><!-- [[[read_end]]] --><p>下面这张图展示了基于Kafka的实时日志流处理平台的流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/09/51/09e4dc28ea338ee69b5662596c0b8751.jpg?wh=2625*1083\" alt=\"\"></p><p>从图中我们可以看到，日志先从Web服务器被不断地生产出来，随后被实时送入到Kafka Connect组件，Kafka Connect组件对日志进行处理后，将其灌入Kafka的某个主题上，接着发送到Kafka Streams组件，进行实时分析。最后，Kafka Streams将分析结果发送到Kafka的另一个主题上。</p><p>我在专栏前面简单介绍过Kafka Connect和Kafka Streams组件，前者可以实现外部系统与Kafka之间的数据交互，而后者可以实时处理Kafka主题中的消息。</p><p>现在，我们就使用这两个组件，结合前面学习的所有Kafka知识，一起构建一个实时日志分析平台。</p><h2>Kafka Connect组件</h2><p>我们先利用Kafka Connect组件<strong>收集数据</strong>。如前所述，Kafka Connect组件负责连通Kafka与外部数据系统。连接外部数据源的组件叫连接器（Connector）。<strong>常见的外部数据源包括数据库、KV存储、搜索系统或文件系统等。</strong></p><p>今天我们使用文件连接器（File Connector）实时读取Nginx的access日志。假设access日志的格式如下：</p><pre><code>10.10.13.41 - - [13/Aug/2019:03:46:54 +0800] &quot;GET /v1/open/product_list?user_key=****&amp;user_phone=****&amp;screen_height=1125&amp;screen_width=2436&amp;from_page=1&amp;user_type=2&amp;os_type=ios HTTP/1.0&quot; 200 1321\n</code></pre><p>在这段日志里，请求参数中的os_type字段目前有两个值：ios和android。我们的目标是实时计算当天所有请求中ios端和android端的请求数。</p><h3>启动Kafka Connect</h3><p>当前，Kafka Connect支持单机版（Standalone）和集群版（Cluster），我们用集群的方式来启动Connect组件。</p><p>首先，我们要启动Kafka集群，假设Broker的连接地址是localhost:9092。</p><p>启动好Kafka集群后，我们启动Connect组件。在Kafka安装目录下有个config/connect-distributed.properties文件，你需要修改下列项：</p><pre><code>bootstrap.servers=localhost:9092\nrest.host.name=localhost\nrest.port=8083\n</code></pre><p>第1项是指定<strong>要连接的Kafka集群</strong>，第2项和第3项分别指定Connect组件开放的REST服务的<strong>主机名和端口</strong>。保存这些变更之后，我们运行下面的命令启动Connect。</p><pre><code>cd kafka_2.12-2.3.0\nbin/connect-distributed.sh config/connect-distributed.properties\n</code></pre><p>如果一切正常，此时Connect应该就成功启动了。现在我们在浏览器访问localhost:8083的Connect REST服务，应该能看到下面的返回内容：</p><pre><code>{&quot;version&quot;:&quot;2.3.0&quot;,&quot;commit&quot;:&quot;fc1aaa116b661c8a&quot;,&quot;kafka_cluster_id&quot;:&quot;XbADW3mnTUuQZtJKn9P-hA&quot;}\n</code></pre><h3>添加File Connector</h3><p>看到该JSON串，就表明Connect已经成功启动了。此时，我们打开一个终端，运行下面这条命令来查看一下当前都有哪些Connector。</p><pre><code>$ curl http://localhost:8083/connectors\n[]\n</code></pre><p>结果显示，目前我们没有创建任何Connector。</p><p>现在，我们来创建对应的File Connector。该Connector读取指定的文件，并为每一行文本创建一条消息，并发送到特定的Kafka主题上。创建命令如下：</p><pre><code>$ curl -H &quot;Content-Type:application/json&quot; -H &quot;Accept:application/json&quot; http://localhost:8083/connectors -X POST --data '{&quot;name&quot;:&quot;file-connector&quot;,&quot;config&quot;:{&quot;connector.class&quot;:&quot;org.apache.kafka.connect.file.FileStreamSourceConnector&quot;,&quot;file&quot;:&quot;/var/log/access.log&quot;,&quot;tasks.max&quot;:&quot;1&quot;,&quot;topic&quot;:&quot;access_log&quot;}}'\n{&quot;name&quot;:&quot;file-connector&quot;,&quot;config&quot;:{&quot;connector.class&quot;:&quot;org.apache.kafka.connect.file.FileStreamSourceConnector&quot;,&quot;file&quot;:&quot;/var/log/access.log&quot;,&quot;tasks.max&quot;:&quot;1&quot;,&quot;topic&quot;:&quot;access_log&quot;,&quot;name&quot;:&quot;file-connector&quot;},&quot;tasks&quot;:[],&quot;type&quot;:&quot;source&quot;}\n</code></pre><p>这条命令本质上是向Connect REST服务发送了一个POST请求，去创建对应的Connector。在这个例子中，我们的Connector类是Kafka默认提供的<strong>FileStreamSourceConnector</strong>。我们要读取的日志文件在/var/log目录下，要发送到Kafka的主题名称为access_log。</p><p>现在，我们再次运行curl http: // localhost:8083/connectors， 验证一下刚才的Connector是否创建成功了。</p><pre><code>$ curl http://localhost:8083/connectors\n[&quot;file-connector&quot;]\n</code></pre><p>显然，名为file-connector的新Connector已经创建成功了。如果我们现在使用Console Consumer程序去读取access_log主题的话，应该会发现access.log中的日志行数据已经源源不断地向该主题发送了。</p><p>如果你的生产环境中有多台机器，操作也很简单，在每台机器上都创建这样一个Connector，只要保证它们被送入到相同的Kafka主题以供消费就行了。</p><h2>Kafka Streams组件</h2><p>数据到达Kafka还不够，我们还需要对其进行实时处理。下面我演示一下如何编写Kafka Streams程序来实时分析Kafka主题数据。</p><p>我们知道，Kafka Streams是Kafka提供的用于实时流处理的组件。</p><p>与其他流处理框架不同的是，它仅仅是一个类库，用它编写的应用被编译打包之后就是一个普通的Java应用程序。你可以使用任何部署框架来运行Kafka Streams应用程序。</p><p>同时，你只需要简单地启动多个应用程序实例，就能自动地获得负载均衡和故障转移，因此，和Spark Streaming或Flink这样的框架相比，Kafka Streams自然有它的优势。</p><p>下面这张来自Kafka官网的图片，形象地展示了多个Kafka Streams应用程序组合在一起，共同实现流处理的场景。图中清晰地展示了3个Kafka Streams应用程序实例。一方面，它们形成一个组，共同参与并执行流处理逻辑的计算；另一方面，它们又都是独立的实体，彼此之间毫无关联，完全依靠Kafka Streams帮助它们发现彼此并进行协作。</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/67/6fc7c835c993b48b1ef1558e02f24f67.png?wh=1950*1291\" alt=\"\"></p><p>关于Kafka Streams的原理，我会在专栏后面进行详细介绍。今天，我们只要能够学会利用它提供的API编写流处理应用，帮我们找到刚刚提到的请求日志中ios端和android端发送请求数量的占比数据就行了。</p><h3>编写流处理应用</h3><p>要使用Kafka Streams，你需要在你的Java项目中显式地添加kafka-streams依赖。我以最新的2.3版本为例，分别演示下Maven和Gradle的配置方法。</p><pre><code>Maven:\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n    &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;\n    &lt;version&gt;2.3.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre><pre><code>Gradle:\ncompile group: 'org.apache.kafka', name: 'kafka-streams', version: '2.3.0'\n</code></pre><p>现在，我先给出完整的代码，然后我会详细解释一下代码中关键部分的含义。</p><pre><code>package com.geekbang.kafkalearn;\n\nimport com.google.gson.Gson;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.kstream.TimeWindows;\nimport org.apache.kafka.streams.kstream.WindowedSerdes;\n\nimport java.time.Duration;\nimport java.util.Properties;\nimport java.util.concurrent.CountDownLatch;\n\npublic class OSCheckStreaming {\n\n    public static void main(String[] args) {\n\n\n        Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;os-check-streams&quot;);\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;);\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS, Serdes.StringSerde.class.getName());\n\n        final Gson gson = new Gson();\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        KStream&lt;String, String&gt; source = builder.stream(&quot;access_log&quot;);\n        source.mapValues(value -&gt; gson.fromJson(value, LogLine.class)).mapValues(LogLine::getPayload)\n                .groupBy((key, value) -&gt; value.contains(&quot;ios&quot;) ? &quot;ios&quot; : &quot;android&quot;)\n                .windowedBy(TimeWindows.of(Duration.ofSeconds(2L)))\n                .count()\n                .toStream()\n                .to(&quot;os-check&quot;, Produced.with(WindowedSerdes.timeWindowedSerdeFrom(String.class), Serdes.Long()));\n\n        final Topology topology = builder.build();\n\n        final KafkaStreams streams = new KafkaStreams(topology, props);\n        final CountDownLatch latch = new CountDownLatch(1);\n\n        Runtime.getRuntime().addShutdownHook(new Thread(&quot;streams-shutdown-hook&quot;) {\n            @Override\n            public void run() {\n                streams.close();\n                latch.countDown();\n            }\n        });\n\n        try {\n            streams.start();\n            latch.await();\n        } catch (Exception e) {\n            System.exit(1);\n        }\n        System.exit(0);\n    }\n}\n\n\nclass LogLine {\n    private String payload;\n    private Object schema;\n\n    public String getPayload() {\n        return payload;\n    }\n}\n</code></pre><p>这段代码会实时读取access_log主题，每2秒计算一次ios端和android端请求的总数，并把这些数据写入到os-check主题中。</p><p>首先，我们构造一个Properties对象。这个对象负责初始化Streams应用程序所需要的关键参数设置。比如，在上面的例子中，我们设置了bootstrap.servers参数、application.id参数以及默认的序列化器（Serializer）和解序列化器（Deserializer）。</p><p>bootstrap.servers参数你应该已经很熟悉了，我就不多讲了。这里的application.id是Streams程序中非常关键的参数，你必须要指定一个集群范围内唯一的字符串来标识你的Kafka Streams程序。序列化器和解序列化器设置了默认情况下Streams程序执行序列化和反序列化时用到的类。在这个例子中，我们设置的是String类型，这表示，序列化时会将String转换成字节数组，反序列化时会将字节数组转换成String。</p><p>构建好Properties实例之后，下一步是创建StreamsBuilder对象。稍后我们会用这个Builder去实现具体的流处理逻辑。</p><p>在这个例子中，我们实现了这样的流计算逻辑：每2秒去计算一下ios端和android端各自发送的总请求数。还记得我们的原始数据长什么样子吗？它是一行Nginx日志，只不过Connect组件在读取它后，会把它包装成JSON格式发送到Kafka，因此，我们需要借助Gson来帮助我们把JSON串还原为Java对象，这就是我在代码中创建LogLine类的原因。</p><p>代码中的mapValues调用将接收到的JSON串转换成LogLine对象，之后再次调用mapValues方法，提取出LogLine对象中的payload字段，这个字段保存了真正的日志数据。这样，经过两次mapValues方法调用之后，我们成功地将原始数据转换成了实际的Nginx日志行数据。</p><p>值得注意的是，代码使用的是Kafka Streams提供的mapValues方法。顾名思义，<strong>这个方法就是只对消息体（Value）进行转换，而不变更消息的键（Key）</strong>。</p><p>其实，Kafka Streams也提供了map方法，允许你同时修改消息Key。通常来说，我们认为<strong>mapValues要比map方法更高效</strong>，因为Key的变更可能导致下游处理算子（Operator）的重分区，降低性能。如果可能的话最好尽量使用mapValues方法。</p><p>拿到真实日志行数据之后，我们调用groupBy方法进行统计计数。由于我们要统计双端（ios端和android端）的请求数，因此，我们groupBy的Key是ios或android。在上面的那段代码中，我仅仅依靠日志行中是否包含特定关键字的方式来确定是哪一端。更正宗的做法应该是，<strong>分析Nginx日志格式，提取对应的参数值，也就是os_type的值</strong>。</p><p>做完groupBy之后，我们还需要限定要统计的时间窗口范围，即我们统计的双端请求数是在哪个时间窗口内计算的。在这个例子中，我调用了<strong>windowedBy方法</strong>，要求Kafka Streams每2秒统计一次双端的请求数。设定好了时间窗口之后，下面就是调用<strong>count方法</strong>进行统计计数了。</p><p>这一切都做完了之后，我们需要调用<strong>toStream方法</strong>将刚才统计出来的表（Table）转换成事件流，这样我们就能实时观测它里面的内容。我会在专栏的最后几讲中解释下流处理领域内的流和表的概念以及它们的区别。这里你只需要知道toStream是将一个Table变成一个Stream即可。</p><p>最后，我们调用<strong>to方法</strong>将这些时间窗口统计数据不断地写入到名为os-check的Kafka主题中，从而最终实现我们对Nginx日志进行实时分析处理的需求。</p><h3>启动流处理应用</h3><p>由于Kafka Streams应用程序就是普通的Java应用，你可以用你熟悉的方式对它进行编译、打包和部署。本例中的OSCheckStreaming.java就是一个可执行的Java类，因此直接运行它即可。如果一切正常，它会将统计数据源源不断地写入到os-check主题。</p><h3>查看统计结果</h3><p>如果我们想要查看统计的结果，一个简单的方法是使用Kafka自带的kafka-console-consumer脚本。命令如下：</p><pre><code>$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic os-check --from-beginning --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --property print.key=true --property key.deserializer=org.apache.kafka.streams.kstream.TimeWindowedDeserializer --property key.deserializer.default.windowed.key.serde.inner=org.apache.kafka.common.serialization.Serdes\\$StringSerde\n[android@1565743788000/9223372036854775807] 1522\n[ios@1565743788000/9223372036854775807] 478\n[ios@1565743790000/9223372036854775807] 1912\n[android@1565743790000/9223372036854775807] 5313\n[ios@1565743792000/9223372036854775807] 780\n[android@1565743792000/9223372036854775807] 1949\n[android@1565743794000/9223372036854775807] 37\n……\n</code></pre><p>由于我们统计的结果是某个时间窗口范围内的，因此承载这个统计结果的消息的Key封装了该时间窗口信息，具体格式是：<strong>[ios或android@开始时间/结束时间]</strong>，而消息的Value就是一个简单的数字，表示这个时间窗口内的总请求数。</p><p>如果把上面ios相邻输出行中的开始时间相减，我们就会发现，它们的确是每2秒输出一次，每次输出会同时计算出ios端和android端的总请求数。接下来，你可以订阅这个Kafka主题，将结果实时导出到你期望的其他数据存储上。</p><h2>小结</h2><p>至此，基于Apache Kafka的实时日志流处理平台就简单搭建完成了。在搭建的过程中，我们只使用Kafka这一个大数据框架就完成了所有组件的安装、配置和代码开发。比起Flume+Kafka+Flink这样的技术栈，纯Kafka的方案在运维和管理成本上有着极大的优势。如果你打算从0构建一个实时流处理平台，不妨试一下Kafka Connect+Kafka Core+Kafka Streams的组合。</p><p>其实，Kafka Streams提供的功能远不止做计数这么简单。今天，我只是为你展示了Kafka Streams的冰山一角。在专栏的后几讲中，我会重点向你介绍Kafka Streams组件的使用和管理，敬请期待。</p><p><img src=\"https://static001.geekbang.org/resource/image/fd/9f/fdb79f35b43cab31edb945b977cc609f.jpg?wh=2069*2560\" alt=\"\"></p><h2>开放讨论</h2><p>请比较一下Flume+Kafka+Flink方案和纯Kafka方案，思考一下它们各自的优劣之处。在实际场景中，我们该如何选择呢？</p><p>欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p>","neighbors":{"left":{"article_title":"38 | 调优Kafka，你做到了吗？","id":128184},"right":{"article_title":"40 | Kafka Streams与其他流处理平台的差异在哪里？","id":132096}},"comments":[{"had_liked":false,"id":175523,"user_name":"陈国林","can_delete":false,"product_type":"c1","uid":1438037,"ip_address":"","ucode":"83D12F3E79F197","user_header":"https://static001.geekbang.org/account/avatar/00/15/f1/55/8ac4f169.jpg","comment_is_top":false,"comment_ctime":1580738613,"is_pvip":false,"replies":[{"id":"68341","content":"棒棒：）","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1580864640,"ip_address":"","comment_id":175523,"utype":1}],"discussion_count":2,"race_medal":0,"score":"78890149941","product_id":100029201,"comment_content":"老师好，说下我这边的一些实践。19年一直在做容器日志平台，我们目前的方案是 Fluentd + Kafka + ELK。使用Fluentd做为容器平台的采集器实时采集数据，采集完之后数据写入Kafka通过Kafka进行解耦，使用Logstash消费后写入ES。这套方案目前在容器环境下应该可以说是标配","like_count":19,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":482776,"discussion_content":"棒棒：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580864640,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1169147,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d6/fb/837af7bf.jpg","nickname":"董永刚","note":"","ucode":"ADA792B226A6CD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363132,"discussion_content":"兄弟，交流一下吧 微信  dyg46255830","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617114807,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130687,"user_name":"老鱼","can_delete":false,"product_type":"c1","uid":1221974,"ip_address":"","ucode":"2C873D9E34CB00","user_header":"https://static001.geekbang.org/account/avatar/00/12/a5/56/d1f70c0d.jpg","comment_is_top":false,"comment_ctime":1567509889,"is_pvip":false,"replies":[{"id":"48970","content":"此时，生产者和消费者化身成这个大平台的小组件了。Connect中只有producer，将读取的日志行数据写入到Kafka源主题中。Streams中既有producer也有consumer：producer负责将计算结果实时写入到目标Kafka主题；consumer负责从源主题中读取消息供下游实时计算之用。","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567560017,"ip_address":"","comment_id":130687,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35927248257","product_id":100029201,"comment_content":"老师，上述Kafka Connect+Kafka Core+Kafka Streams例子中，生产者和消费者分别是什么？","like_count":8,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466067,"discussion_content":"此时，生产者和消费者化身成这个大平台的小组件了。Connect中只有producer，将读取的日志行数据写入到Kafka源主题中。Streams中既有producer也有consumer：producer负责将计算结果实时写入到目标Kafka主题；consumer负责从源主题中读取消息供下游实时计算之用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567560017,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214061,"user_name":"石栖","can_delete":false,"product_type":"c1","uid":1496443,"ip_address":"","ucode":"35600F645A479F","user_header":"https://static001.geekbang.org/account/avatar/00/16/d5/7b/c512da6a.jpg","comment_is_top":false,"comment_ctime":1588647850,"is_pvip":false,"replies":[{"id":"79344","content":"Kafka Streams提供了consume-process-produce的原子性操作，也就是端到端的EOS。如果你自己实现代价很高","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588725512,"ip_address":"","comment_id":214061,"utype":1}],"discussion_count":2,"race_medal":0,"score":"27358451626","product_id":100029201,"comment_content":"胡老师，对于Stream的处理和之前的topic-message，我感觉没什么大的区别，感觉流程是类似的。只不过是在以前的consumer中额外添加了producer的逻辑，把处理结果发送到另一个topic中。感觉不用这里说的stream也能实现一样的效果。我不是个明白本质的区别是什么，麻烦能解释一下吗？谢谢","like_count":6,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493943,"discussion_content":"Kafka Streams提供了consume-process-produce的原子性操作，也就是端到端的EOS。如果你自己实现代价很高","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588725512,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1496443,"avatar":"https://static001.geekbang.org/account/avatar/00/16/d5/7b/c512da6a.jpg","nickname":"石栖","note":"","ucode":"35600F645A479F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":258188,"discussion_content":"我看到其他地方的回复了，那么抛开提供的算子。是不是可以通过实现自己的producer和consumer，加上事物，可以达到同样的效果，只是会麻烦一点？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588661607,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":133283,"user_name":"ban","can_delete":false,"product_type":"c1","uid":1034204,"ip_address":"","ucode":"E523CE97E48266","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c7/dc/9408c8c2.jpg","comment_is_top":false,"comment_ctime":1568475245,"is_pvip":false,"replies":[{"id":"51246","content":"在Nginx日志机器上开启，因为目前File Connector只支持从本地文件读取","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1568593907,"ip_address":"","comment_id":133283,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18748344429","product_id":100029201,"comment_content":"老师，示例中开启Connect后启动读取的是本机的nginx日志，但如果nginx日志是在其他机器上面，那Connect是不是支持远程读的还是怎么样可以读取到其他机器的日志？","like_count":4,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467334,"discussion_content":"在Nginx日志机器上开启，因为目前File Connector只支持从本地文件读取","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568593907,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":186363,"user_name":"helloworld","can_delete":false,"product_type":"c1","uid":1105161,"ip_address":"","ucode":"1EECCA0F43E278","user_header":"https://static001.geekbang.org/account/avatar/00/10/dd/09/feca820a.jpg","comment_is_top":false,"comment_ctime":1583830225,"is_pvip":false,"replies":[{"id":"72116","content":"是的","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1583976309,"ip_address":"","comment_id":186363,"utype":1}],"discussion_count":3,"race_medal":0,"score":"14468732113","product_id":100029201,"comment_content":"写的不太明白啊, 难道每一个nginx服务器上都要部署kafka吗","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":486728,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583976309,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1079345,"avatar":"https://static001.geekbang.org/account/avatar/00/10/78/31/c7f8d1db.jpg","nickname":"Laputa","note":"","ucode":"64C157042CF138","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":268131,"discussion_content":"应该是每个nginx服务器都要部署Kafka Connect吧","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1589728294,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1030082,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b7/c2/196932c7.jpg","nickname":"南琛一梦","note":"","ucode":"6338D5428DB2B0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414694,"discussion_content":"对，单独部署Kafka Connect，作为一个新的java进程，这个进程作为Kafka Producer端，去抓取自己所在机器上磁盘的日志写入到Kafka中，Kafka Stream作为消费端去消费对应主题的消息，经过流处理分析后，将结果写入到另一个新的主题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636860584,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":181714,"user_name":"w","can_delete":false,"product_type":"c1","uid":1225358,"ip_address":"","ucode":"7A3A80C8F8213D","user_header":"https://static001.geekbang.org/account/avatar/00/12/b2/8e/0911bf35.jpg","comment_is_top":false,"comment_ctime":1582622640,"is_pvip":false,"replies":[{"id":"70433","content":"是单独的组件。需要单独部署。<br>kafka集群上启动单机模式的connect  --- 这是可以的","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1582680169,"ip_address":"","comment_id":181714,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14467524528","product_id":100029201,"comment_content":"老师我想问一下。Kafka Connect 是一个单独的组件么？类似agent一样，可以在目标采集机器（非kafka集群）上部署？那如果要用，岂不是每个业务机器都要装个kafka？<br>单机模式跟集群模式有啥区别呢？没太懂。比如kafka集群上启动单机模式的connect ？不行么？<br>最终操作，都往同一个topic里扔就好了?","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485092,"discussion_content":"是单独的组件。需要单独部署。\nkafka集群上启动单机模式的connect  --- 这是可以的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1582680169,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":171672,"user_name":"Ball","can_delete":false,"product_type":"c1","uid":1521451,"ip_address":"","ucode":"1EE949E68D84CA","user_header":"https://static001.geekbang.org/account/avatar/00/17/37/2b/b32f1d66.jpg","comment_is_top":false,"comment_ctime":1578989450,"is_pvip":false,"replies":[{"id":"66784","content":"新的java进程","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1579135225,"ip_address":"","comment_id":171672,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14463891338","product_id":100029201,"comment_content":"老师我有问题要请教下，添加 Connector 步骤里面是用 http REST 接口新建的，那新建的 Connector 是跑在 Broker 里面还是说又启动了一个新的 Java 进程执行 Connector？","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":481464,"discussion_content":"新的java进程","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1579135225,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":251609,"user_name":"timmy21","can_delete":false,"product_type":"c1","uid":1174860,"ip_address":"","ucode":"9D6DED247B1F38","user_header":"https://static001.geekbang.org/account/avatar/00/11/ed/4c/8674b6ad.jpg","comment_is_top":false,"comment_ctime":1601721023,"is_pvip":false,"replies":[{"id":"92149","content":"需要的。Kafka Streams使用特殊的repartition topics来保存shuffle后的数据","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1602206629,"ip_address":"","comment_id":251609,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10191655615","product_id":100029201,"comment_content":"老师如果有多个分区，并且消息写入是随机的。那么多个kafka streams实例在对os_type进行group_by统计时，需要相互之间传输数据进行shuffle操作吗？","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506562,"discussion_content":"需要的。Kafka Streams使用特殊的repartition topics来保存shuffle后的数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602206629,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285090,"user_name":"霍格沃兹小学徒","can_delete":false,"product_type":"c1","uid":1352674,"ip_address":"","ucode":"EB52B060CF9ED9","user_header":"https://static001.geekbang.org/account/avatar/00/14/a3/e2/439731b2.jpg","comment_is_top":false,"comment_ctime":1616602307,"is_pvip":true,"replies":[{"id":"103527","content":"可以为每个telnet的数据流编制一个key来管理","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1616720359,"ip_address":"","comment_id":285090,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5911569603","product_id":100029201,"comment_content":"老师 想咨询个问题 我这边如果日志不是来自于文件 而是来自于telenet的输出 需要怎么做日志实时分析。 而且需要有上万个telenet实例一起在输出 我需要分别分析每个实例按照某种统一规则","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517581,"discussion_content":"可以为每个telnet的数据流编制一个key来管理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616720359,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":276180,"user_name":"Geek_6e00ab","can_delete":false,"product_type":"c1","uid":2428239,"ip_address":"","ucode":"788EDD3C3E803E","user_header":"","comment_is_top":false,"comment_ctime":1611823082,"is_pvip":false,"replies":[{"id":"100452","content":"因为是二进制字节序列，需要反序列化","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1612143253,"ip_address":"","comment_id":276180,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5906790378","product_id":100029201,"comment_content":"为什么消费者取出来的值是乱码的","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514622,"discussion_content":"因为是二进制字节序列，需要反序列化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612143253,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":255738,"user_name":"大土豆","can_delete":false,"product_type":"c1","uid":1121636,"ip_address":"","ucode":"67445DC3EC9DB0","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/64/52a5863b.jpg","comment_is_top":false,"comment_ctime":1603419908,"is_pvip":true,"replies":[{"id":"93383","content":"不一定。离线计算通常是指延时比较高的batch computing，和用什么存储没有强绑定关系","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1603675466,"ip_address":"","comment_id":255738,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5898387204","product_id":100029201,"comment_content":"问个大数据相关的问题，一定要读取和写入都在hdfs上才是离线计算吗？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507822,"discussion_content":"不一定。离线计算通常是指延时比较高的batch computing，和用什么存储没有强绑定关系","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603675466,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228580,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1592743226,"is_pvip":false,"replies":[{"id":"84322","content":"依靠Kafka Streams找到Kafka Cluster上的彼此。这么说是不是好点：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1592789387,"ip_address":"","comment_id":228580,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5887710522","product_id":100029201,"comment_content":"“另一方面，它们又都是独立的实体，彼此之间毫无关联，完全依靠 Kafka Streams 帮助它们发现彼此并进行协作。”<br>————————————————————<br>完全依靠“kafka cluster”这个吧！？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499087,"discussion_content":"依靠Kafka Streams找到Kafka Cluster上的彼此。这么说是不是好点：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592789387,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":183959,"user_name":"Joypan","can_delete":false,"product_type":"c1","uid":1044182,"ip_address":"","ucode":"B7298241E7AAAF","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ee/d6/a9b34bd3.jpg","comment_is_top":false,"comment_ctime":1583169095,"is_pvip":false,"replies":[{"id":"71209","content":"hmmm..... 这个只能靠测试才能知道了：）","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1583197009,"ip_address":"","comment_id":183959,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5878136391","product_id":100029201,"comment_content":"connect与filebeat比单文件收集性能比较来说哪个更高？我们用filebeat单文件收集性能到瓶颈了，我们现在的做法是减小单文件的，大小，来提高并行度，不知道connect性能如何","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485834,"discussion_content":"hmmm..... 这个只能靠测试才能知道了：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583197009,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1707258,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJiaib792gktLoHOVhTSo4mhNnwtnObIwoeNSOxicesHa6ZKiassNgtwPCFdoMfgy0tgnhIiczYHTBE2mA/132","nickname":"雷锋","note":"","ucode":"D381E27E818C49","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331400,"discussion_content":"你单文件遇到瓶颈不是filebeat 的问题，问题在单文件.如果是日志直接设置rotation 其他场景另讨论","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606858571,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":333474,"user_name":"陈斌","can_delete":false,"product_type":"c1","uid":1367048,"ip_address":"","ucode":"B639AB5F6AA03D","user_header":"https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg","comment_is_top":false,"comment_ctime":1644377125,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1644377125","product_id":100029201,"comment_content":"Kafka Stream 能不能将消费者Topic的一个消息分解转换成生产者的多条消息，而且其中还涉及可以把消息数据存入Redis中？","like_count":1},{"had_liked":false,"id":322637,"user_name":"alex_lai","can_delete":false,"product_type":"c1","uid":1903459,"ip_address":"","ucode":"3057F2A593A6DB","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/m7fLWyJrnwEPoIefiaxusQRh6D1Nq7PCXA8RiaxkmzdNEmFARr5q8L4qouKNaziceXia92an8hzYa5MLic6N6cNMEoQ/132","comment_is_top":false,"comment_ctime":1637522743,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1637522743","product_id":100029201,"comment_content":"Kafka stream 的输出必须要返回到topic么？ 我可以直接用stream 的api输出到我的目标db么？","like_count":0},{"had_liked":false,"id":253396,"user_name":"L.","can_delete":false,"product_type":"c1","uid":1447456,"ip_address":"","ucode":"7F05DB20D35924","user_header":"https://static001.geekbang.org/account/avatar/00/16/16/20/ce768739.jpg","comment_is_top":false,"comment_ctime":1602726350,"is_pvip":false,"replies":[{"id":"92794","content":"使用KStream的to方法","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1603073900,"ip_address":"","comment_id":253396,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1602726350","product_id":100029201,"comment_content":"老师，如果要将处理后的结果写到另一个 kafka 集群的 topic，应该怎么做呢？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507062,"discussion_content":"使用KStream的to方法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603073900,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":243434,"user_name":"InfoQ_ad0a52af1586","can_delete":false,"product_type":"c1","uid":1625690,"ip_address":"","ucode":"8ACD132E908038","user_header":"","comment_is_top":false,"comment_ctime":1598104724,"is_pvip":false,"replies":[{"id":"89761","content":".windowedBy(TimeWindows.of(Duration.ofSeconds(2L)))<br><br>改长点","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1598232168,"ip_address":"","comment_id":243434,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1598104724","product_id":100029201,"comment_content":"这个案例运行了下没有两秒钟就输出，老师能怎么排查下是什么问题吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504284,"discussion_content":".windowedBy(TimeWindows.of(Duration.ofSeconds(2L)))\n\n改长点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598232168,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":241017,"user_name":"杨逸林","can_delete":false,"product_type":"c1","uid":1167233,"ip_address":"","ucode":"4BF3CF3E2F1AC7","user_header":"https://static001.geekbang.org/account/avatar/00/11/cf/81/96f656ef.jpg","comment_is_top":false,"comment_ctime":1597151775,"is_pvip":false,"replies":[{"id":"89157","content":"其实我一直有个想法：在学习任何大数据流式处理框架前，我们至少要对Java的Stream和Lambda表达式有一定的了解，这样我们才能更好地理解如何把操作算子组合在一起。比如要理解基本的有状态操作算子和无状态操作算子都有哪些等","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1597279193,"ip_address":"","comment_id":241017,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1597151775","product_id":100029201,"comment_content":"我看了有点多的 SpringCloud + Kafka Stream 整合的，按照 Spring 官方给的教学视频学，结果到头来还是只会改下字符串（从一个 Topic 监听数据，送到另一个 Topic 做数据清洗）。Kafka 官网都是像你写的 demo 一样的，其实还是有点懵的。。。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":503621,"discussion_content":"其实我一直有个想法：在学习任何大数据流式处理框架前，我们至少要对Java的Stream和Lambda表达式有一定的了解，这样我们才能更好地理解如何把操作算子组合在一起。比如要理解基本的有状态操作算子和无状态操作算子都有哪些等","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597279193,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1367048,"avatar":"https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg","nickname":"陈斌","note":"","ucode":"B639AB5F6AA03D","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":550399,"discussion_content":"Kafka 能不能将topic1的1条消息拆成多条消息写入topic2，并且将从topic1中获取到的消息部分数据存入Mysql数据库，这种场景能用Kafka Stream吗，代码应该怎么写？\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644509832,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":503621,"ip_address":""},"score":550399,"extra":""}]}]},{"had_liked":false,"id":233742,"user_name":"长脖子树","can_delete":false,"product_type":"c1","uid":1182802,"ip_address":"","ucode":"D9090EF67EEB1B","user_header":"https://static001.geekbang.org/account/avatar/00/12/0c/52/f25c3636.jpg","comment_is_top":false,"comment_ctime":1594433192,"is_pvip":true,"replies":[{"id":"86290","content":"Connect是可以集群搭建的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1594448062,"ip_address":"","comment_id":233742,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1594433192","product_id":100029201,"comment_content":"用过kafka connector，直接在集群上启动单机的connector连接mqtt，但感觉这种方案的扩展性不高","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501151,"discussion_content":"Connect是可以集群搭建的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594448062,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":229478,"user_name":"丁丁历险记","can_delete":false,"product_type":"c1","uid":1661704,"ip_address":"","ucode":"A43829E454C067","user_header":"https://static001.geekbang.org/account/avatar/00/19/5b/08/b0b0db05.jpg","comment_is_top":false,"comment_ctime":1593009895,"is_pvip":false,"replies":[{"id":"84789","content":"哈哈哈，有关搭建集群的内容在前面的课程中：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593163361,"ip_address":"","comment_id":229478,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1593009895","product_id":100029201,"comment_content":"说好的从0 开始的，kafka 集群怎么装。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499499,"discussion_content":"哈哈哈，有关搭建集群的内容在前面的课程中：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593163361,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2138087,"avatar":"https://static001.geekbang.org/account/avatar/00/20/9f/e7/050735bf.jpg","nickname":"Rain","note":"","ucode":"D183D7A32FA9A7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":302130,"discussion_content":"你好，关于这个集群安装我也不是很理解，能指以下看老师的哪篇文章吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598796423,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1134861,"avatar":"https://static001.geekbang.org/account/avatar/00/11/51/0d/fc1652fe.jpg","nickname":"James","note":"","ucode":"48B0F2A334D1C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":288959,"discussion_content":"老哥,是你自己跳着看的.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593942024,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207613,"user_name":"李先生","can_delete":false,"product_type":"c1","uid":1237614,"ip_address":"","ucode":"D9039715F7D290","user_header":"https://static001.geekbang.org/account/avatar/00/12/e2/6e/0a300829.jpg","comment_is_top":false,"comment_ctime":1587118544,"is_pvip":false,"replies":[{"id":"77823","content":"不确定完全理解了意思。听上去是否可以在推消息和发消息之间加一个队列解耦下？这样server只管向队列推消息就可以了。connect负责从队列取消息然后发出去。如果单机性能不够，还可以水平扩展成多机的情况","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587347965,"ip_address":"","comment_id":207613,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587118544","product_id":100029201,"comment_content":"胡哥：<br>开发的一个弹幕系统，分为server和connect，connect是专门维护socket链接和推消息的，可以横向扩展。但是目前遇到了系统瓶颈，单机就支持3000左右连接。在推消息的过程中，系统每次把消息给tcp就不管客户端是否能收到消息，但是操作系统tcp连接在发消息失败的时候会重试，所以在推消息的过程中消息就堆积了起来，导致单机连接数一直上不去。这种能指点一下吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492182,"discussion_content":"不确定完全理解了意思。听上去是否可以在推消息和发消息之间加一个队列解耦下？这样server只管向队列推消息就可以了。connect负责从队列取消息然后发出去。如果单机性能不够，还可以水平扩展成多机的情况","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587347965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":202718,"user_name":"冬风向左吹","can_delete":false,"product_type":"c1","uid":1066928,"ip_address":"","ucode":"376C45C5134F93","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/b0/a9b77a1e.jpg","comment_is_top":false,"comment_ctime":1586057002,"is_pvip":false,"replies":[{"id":"76092","content":"不是的，支持各种形式","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1586242877,"ip_address":"","comment_id":202718,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586057002","product_id":100029201,"comment_content":"connect只能支持json格式的数据吗","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490660,"discussion_content":"不是的，支持各种形式","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586242877,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":160673,"user_name":"丁丁历险记","can_delete":false,"product_type":"c1","uid":1661704,"ip_address":"","ucode":"A43829E454C067","user_header":"https://static001.geekbang.org/account/avatar/00/19/5b/08/b0b0db05.jpg","comment_is_top":false,"comment_ctime":1576017489,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1576017489","product_id":100029201,"comment_content":"终于等到了。","like_count":0},{"had_liked":false,"id":160273,"user_name":"leige","can_delete":false,"product_type":"c1","uid":1772022,"ip_address":"","ucode":"10A33DC83D642D","user_header":"","comment_is_top":false,"comment_ctime":1575902837,"is_pvip":false,"replies":[{"id":"61189","content":"不会生成多条记录，但是的确可能会被丢弃（如果late太多）","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1575938906,"ip_address":"","comment_id":160273,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1575902837","product_id":100029201,"comment_content":"胡老师，请问对于迟到的数据，os_check主题会生成多条记录吗？此时消费者应用程序应该如何处理？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":477321,"discussion_content":"不会生成多条记录，但是的确可能会被丢弃（如果late太多）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1575938906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":150394,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1573526558,"is_pvip":false,"replies":[{"id":"57967","content":"只是举个例子而已，没说一定只能用Gson做JSON的序列化","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1573609755,"ip_address":"","comment_id":150394,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1573526558","product_id":100029201,"comment_content":"老师，您在处理json串的时候为什么用Gson，而不用Alibaba的fastjson呢？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474145,"discussion_content":"只是举个例子而已，没说一定只能用Gson做JSON的序列化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573609755,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1030082,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b7/c2/196932c7.jpg","nickname":"南琛一梦","note":"","ucode":"6338D5428DB2B0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414696,"discussion_content":"还在用fastjson，多次被报出安全漏洞问题，所谓的快也不过是投机取巧后的结果。主流框架现在使用的都是jackson了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636860901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":137483,"user_name":"伟","can_delete":false,"product_type":"c1","uid":1552786,"ip_address":"","ucode":"527BE0265C20FD","user_header":"https://static001.geekbang.org/account/avatar/00/17/b1/92/4272e592.jpg","comment_is_top":false,"comment_ctime":1569751121,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1569751121","product_id":100029201,"comment_content":"老师，请教connect读取mysql数据库中，我的添加connector命令是curl -X POST http:&#47;&#47;l-lzw:8083&#47;connectors -H &quot;Content-Type: application&#47;json&quot; -d &#39;{&quot;name&quot;: &quot;mysql-connector&quot;,&quot;config&quot;: {&quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,&quot;tasks.max&quot;: &quot;1&quot;,&quot;database.hostname&quot;: &quot;lzw-mysql&quot;,&quot;database.port&quot;: &quot;3306&quot;,&quot;database.user&quot;: &quot;root&quot;,&quot;database.password&quot;: &quot;123456&quot;,&quot;database.server.id&quot;: &quot;1&quot;,&quot;database.server.name&quot;: &quot;pydata&quot;,&quot;database.whitelist&quot;: &quot;employee&quot;,&quot;topic.prefix&quot;: &quot;test-mysql-&quot;,&quot;database.history.kafka.bootstrap.servers&quot;: &quot;l-lzw:9092,l-lzw2:9092&quot;,&quot;database.history.kafka.topic&quot;: &quot;db.history.mysql&quot;}}&#39;<br><br>消费topic时返回如下：<br>{&quot;source&quot;:{&quot;version&quot;:&quot;0.9.5.Final&quot;,&quot;connector&quot;:&quot;mysql&quot;,&quot;name&quot;:&quot;pydata&quot;,&quot;server_id&quot;:0,&quot;ts_sec&quot;:0,&quot;gtid&quot;:null,&quot;file&quot;:&quot;mysql-bin.000004&quot;,&quot;pos&quot;:1021,&quot;row&quot;:0,&quot;snapshot&quot;:true,&quot;thread&quot;:null,&quot;db&quot;:null,&quot;table&quot;:null,&quot;query&quot;:null},&quot;databaseName&quot;:&quot;&quot;,&quot;ddl&quot;:&quot;SET character_set_server=latin1, collation_server=latin1_swedish_ci;&quot;}}<br><br>没有正常可能原因是什么？<br><br>","like_count":0,"discussions":[{"author":{"id":1030082,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b7/c2/196932c7.jpg","nickname":"南琛一梦","note":"","ucode":"6338D5428DB2B0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414697,"discussion_content":"看错误原因啊，先排查Source端任务是否正常创建了，创建了的话再看主题是否正常写入了消息。从数据的流向逐层排查，应该很容易发现问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636860975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":135890,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1569293012,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1569293012","product_id":100029201,"comment_content":"打卡，仅仅使用kafka这一个大数据组件就能实现一个企业级的实时日志流处理平台。<br>获取——存储——清洗——转存——展示","like_count":0,"discussions":[{"author":{"id":1030082,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b7/c2/196932c7.jpg","nickname":"南琛一梦","note":"","ucode":"6338D5428DB2B0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414698,"discussion_content":"这里仅仅是一个可行的尝试方案，具体性能如何要实际使用测试才知道。这个方案貌似用的人很少，真要使用的话，还是要做好充分测试才行，尤其是海量数据下处理性能问题，稳定性倒是还好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636861064,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131345,"user_name":"艺超(鲁鸣)","can_delete":false,"product_type":"c1","uid":1029436,"ip_address":"","ucode":"7F749FA543E0F1","user_header":"https://static001.geekbang.org/account/avatar/00/0f/b5/3c/967d7291.jpg","comment_is_top":false,"comment_ctime":1567728723,"is_pvip":false,"replies":[{"id":"49807","content":"在Nginx日志本地","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567730945,"ip_address":"","comment_id":131345,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1567728723","product_id":100029201,"comment_content":"请教一下，集群版的connector是说每个kafka节点都启动一个吗？还有它读取的nginx日志就在本地？谢谢","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466400,"discussion_content":"在Nginx日志本地","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567730945,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1029436,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b5/3c/967d7291.jpg","nickname":"艺超(鲁鸣)","note":"","ucode":"7F749FA543E0F1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7921,"discussion_content":"是说connector和nginx在一起吧，就是相当于一个生产端，不需要和kafka broker一起","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1567731157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130469,"user_name":"写点啥呢","can_delete":false,"product_type":"c1","uid":1065272,"ip_address":"","ucode":"C19032CF1C41BA","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/38/4f89095b.jpg","comment_is_top":false,"comment_ctime":1567471363,"is_pvip":false,"replies":[{"id":"48677","content":"这里的结束时间在代码中没有指定，因此默认值是Long.MAX_VALUE","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567473180,"ip_address":"","comment_id":130469,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1567471363","product_id":100029201,"comment_content":"请问胡老师，console-consumer输出的message，为什么结束时间是一个很大的整数？从开始时间看，它应该是millisecond epoch，原本以为结束时间应该也是开始时间+2 second，但是文章中的例子看着不像：<br><br>bin&#47;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic os-check --from-beginning --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --property print.key=true --property key.deserializer=org.apache.kafka.streams.kstream.TimeWindowedDeserializer --property key.deserializer.default.windowed.key.serde.inner=org.apache.kafka.common.serialization.Serdes\\$StringSerde<br>[android@1565743788000&#47;9223372036854775807] 1522<br>[ios@1565743788000&#47;9223372036854775807] 478<br>[ios@1565743790000&#47;9223372036854775807] 1912<br>[android@1565743790000&#47;9223372036854775807] 5313<br>[ios@1565743792000&#47;9223372036854775807] 780<br>[android@1565743792000&#47;9223372036854775807] 1949<br>[android@1565743794000&#47;9223372036854775807] 37<br>……<br>","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465969,"discussion_content":"这里的结束时间在代码中没有指定，因此默认值是Long.MAX_VALUE","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567473180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}