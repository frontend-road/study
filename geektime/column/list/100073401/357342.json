{"id":357342,"title":"09 | 调优一筹莫展，配置项速查手册让你事半功倍！（上）","content":"<p>你好，我是吴磊。</p><p>对于Spark性能调优来说，应用开发和配置项设置是两个最主要也最常用的入口。但在日常的调优工作中，每当我们需要从配置项入手寻找调优思路的时候，一打开Spark官网的Configuration页面，映入眼帘的就是上百个配置项。它们有的需要设置True或False，有的需要给定明确的数值才能使用。这难免让我们蒙头转向、无所适从。</p><p>所以我经常在想，如果能有一份Spark配置项手册，上面分门别类地记录着与性能调优息息相关的配置项就好了，肯定能省去不少麻烦。</p><p>那么，接下来的两讲，我们就来一起汇总这份手册。这份手册可以让你在寻找调优思路的时候，迅速地定位可能会用到的配置项，不仅有章可循，还能不丢不漏，真正做到事半功倍！</p><h2>配置项的分类</h2><p>事实上，能够显著影响执行性能的配置项屈指可数，更何况在Spark分布式计算环境中，计算负载主要由Executors承担，Driver主要负责分布式调度，调优空间有限，因此对Driver端的配置项我们不作考虑，<strong>我们要汇总的配置项都围绕Executors展开</strong>。那么，结合过往的实践经验，以及对官网全量配置项的梳理，我把它们划分为3类，分别是硬件资源类、Shuffle类和Spark SQL大类。</p><!-- [[[read_end]]] --><p>为什么这么划分呢？我们一一来说。</p><p><strong>首先，硬件资源类包含的是与CPU、内存、磁盘有关的配置项。</strong>我们说过，调优的切入点是瓶颈，定位瓶颈的有效方法之一，就是从硬件的角度出发，观察某一类硬件资源的负载与消耗，是否远超其他类型的硬件，而且调优的过程收敛于所有硬件资源平衡、无瓶颈的状态，所以掌握资源类配置项就至关重要了。这类配置项设置得是否得当，决定了应用能否打破瓶颈，来平衡不同硬件的资源利用率。</p><p><strong>其次，Shuffle类是专门针对Shuffle操作的。</strong>在绝大多数场景下，Shuffle都是性能瓶颈。因此，我们需要专门汇总这些会影响Shuffle计算过程的配置项。同时，Shuffle的调优难度也最高，汇总Shuffle配置项能帮我们在调优的过程中锁定搜索范围，充分节省时间。</p><p><strong>最后，Spark SQL早已演化为新一代的底层优化引擎。</strong>无论是在Streaming、Mllib、Graph等子框架中，还是在PySpark中，只要你使用DataFrame API，Spark在运行时都会使用Spark SQL做统一优化。因此，我们需要梳理出一类配置项，去充分利用Spark SQL的先天性能优势。</p><p>我们一再强调硬件资源的平衡才是性能调优的关键，所以今天这一讲，我们就先从硬件资源类入手，去汇总应该设置的配置项。在这个过程中，我会带你搞清楚这些配置项的定义与作用是什么，以及它们的设置能解决哪些问题，让你为资源平衡打下基础。下一讲，我们再来讲Shuffle类和Spark SQL大类。</p><h2>哪些配置项与CPU设置有关？</h2><p>首先，我们先来说说与CPU有关的配置项，<strong>主要包括spark.cores.max、spark.executor.cores和spark.task.cpus这三个参数</strong>。它们分别从集群、Executor和计算任务这三个不同的粒度，指定了用于计算的CPU个数。开发者通过它们就可以明确有多少CPU资源被划拨给Spark用于分布式计算。</p><p>为了充分利用划拨给Spark集群的每一颗CPU，准确地说是每一个CPU核（CPU Core），你需要设置与之匹配的并行度，并行度用spark.default.parallelism和spark.sql.shuffle.partitions这两个参数设置。对于没有明确分区规则的RDD来说，我们用spark.default.parallelism定义其并行度，spark.sql.shuffle.partitions则用于明确指定数据关联或聚合操作中Reduce端的分区数量。</p><p>说到并行度（Parallelism）就不得不提并行计算任务（Paralleled Tasks）了，这两个概念关联紧密但含义大相径庭，有不少同学经常把它们弄混。</p><p>并行度指的是分布式数据集被划分为多少份，从而用于分布式计算。换句话说，<strong>并行度的出发点是数据，它明确了数据划分的粒度</strong>。并行度越高，数据的粒度越细，数据分片越多，数据越分散。由此可见，像分区数量、分片数量、Partitions这些概念都是并行度的同义词。</p><p>并行计算任务则不同，它指的是在任一时刻整个集群能够同时计算的任务数量。换句话说，<strong>它的出发点是计算任务、是CPU，由与CPU有关的三个参数共同决定</strong>。具体说来，Executor中并行计算任务数的上限是spark.executor.cores与spark.task.cpus的商，暂且记为#Executor-tasks，整个集群的并行计算任务数自然就是#Executor-tasks乘以集群内Executors的数量，记为#Executors。因此，最终的数值是：#Executor-tasks * #Executors。</p><p>我们不难发现，<strong>并行度决定了数据粒度，数据粒度决定了分区大小，分区大小则决定着每个计算任务的内存消耗</strong>。在同一个Executor中，多个同时运行的计算任务“基本上”是平均瓜分可用内存的，每个计算任务能获取到的内存空间是有上限的，因此并行计算任务数会反过来制约并行度的设置。你看，这两个家伙还真是一对相爱相杀的冤家！</p><p>至于，到底该怎么平衡并行度与并行计算任务两者之间的关系，我们留到后面的课程去展开。这里，咱们只要记住和CPU设置有关配置项的含义、区别与作用就行了。</p><p><img src=\"https://static001.geekbang.org/resource/image/23/ff/234e6b62ff32394f99055c9385988aff.jpeg?wh=1711*783\" alt=\"\" title=\"与CPU有关的配置项\"></p><h2>哪些配置项与内存设置有关？</h2><p>说完CPU，咱们接着说说与内存管理有关的配置项。我们知道，在管理模式上，Spark分为堆内内存与堆外内存。</p><p>堆外内存又分为两个区域，Execution Memory和Storage Memory。要想要启用堆外内存，我们得先把参数spark.memory.offHeap.enabled置为true，然后用spark.memory.offHeap.size指定堆外内存大小。堆内内存也分了四个区域，也就是Reserved Memory、User Memory、Execution Memory和Storage Memory。</p><p>内存的基础配置项主要有5个，它们的含义如下表所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/67/50/67528832632c392d7e2e4c89a872b350.jpeg?wh=1920*872\" alt=\"\" title=\"与内存有关的配置项\"></p><p>简单来说，<strong>这些配置项决定了我们刚才说的这些区域的大小</strong>，这很好理解。工具有了，但很多同学在真正设置内存区域大小的时候还会有各种各样的疑惑，比如说：</p><ul>\n<li>内存空间是有限的，该把多少内存划分给堆内，又该把多少内存留给堆外呢？</li>\n<li>在堆内内存里，该怎么平衡User Memory和Spark用于计算的内存空间？</li>\n<li>在统一内存管理模式下，该如何平衡Execution Memory和Storage Memory？</li>\n</ul><p>别着急，接下来，咱们一个一个来解决。</p><h3>堆外与堆内的平衡</h3><p>相比JVM堆内内存，off heap堆外内存有很多优势，如更精确的内存占用统计和不需要垃圾回收机制，以及不需要序列化与反序列化。你可能会说：“既然堆外内存这么厉害，那我们干脆把所有内存都划分给它不就得了？”先别急着下结论，我们先一起来看一个例子。</p><p>用户表1记录着用户数据，每个数据条目包含4个字段，整型的用户ID、String类型的姓名、整型的年龄和Char类型的性别。如果现在要求你用字节数组来存储每一条用户记录，你该怎么办呢？</p><p><img src=\"https://static001.geekbang.org/resource/image/13/83/136d2f0618d374f2622e1985984ca783.jpeg?wh=1706*696\" alt=\"\" title=\"用户表1：简单数据模式\"></p><p>我们一起来做一下。首先，除姓名外其它3个字段都是定长数据类型，因此可以直接安插到字节数组中。对于变长数据类型如String，由于我们事先并不知道每个用户的名字到底有多长，因此，为了把name字段也用字节数组的形式存储，我们只能曲线救国：先记录name字段的在整个字节数组内的偏移量，再记录它的长度，最后把完整的name字符串安插在字节数组的末尾，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/51/2c/516c0e41e6757193533c8dfa33f9912c.jpg?wh=3549*1251\" alt=\"\" title=\"用字节数组存储用户记录\"></p><p>尽管存储String类型的name字段麻烦一些，但我们总算成功地用字节数组容纳了每一条用户记录。OK，大功告成！</p><p>你可能会问：“做这个小实验的目的是啥呢？”事实上，Spark开辟的堆外内存就是以这样的方式来存储应用数据的。<strong>正是基于这种紧凑的二进制格式，相比JVM堆内内存，Spark通过Java Unsafe API在堆外内存中的管理，才会有那么多的优势。</strong></p><p>不过，成也萧何败也萧何，字节数组自身的局限性也很难突破。比如说，如果用户表1新增了兴趣列表字段，类型为List[String]，如用户表2所示。这个时候，如果我们仍然采用字节数据的方式来存储每一条用户记录，不仅越来越多的指针和偏移地址会让字段的访问效率大打折扣，而且，指针越多，内存泄漏的风险越大，数据访问的稳定性就值得担忧了。</p><p><img src=\"https://static001.geekbang.org/resource/image/07/92/07d010f82edf0bec630fd9ed57ba8892.png?wh=1460*666\" alt=\"\" title=\"用户表2：复杂数据模式\"></p><p>因此，当数据模式（Data Schema）开始变得复杂时，Spark直接管理堆外内存的成本将会非常高。</p><p>那么，针对有限的内存资源，我们该如何平衡JVM堆内内存与off heap堆外内存的划分，我想你心中也该有了答案。<strong>对于需要处理的数据集，如果数据模式比较扁平，而且字段多是定长数据类型，就更多地使用堆外内存。相反地，如果数据模式很复杂，嵌套结构或变长字段很多，就更多采用JVM堆内内存会更加稳妥。</strong></p><h3>User Memory与Spark可用内存如何分配？</h3><p>接下来，我们再来说说User Memory。我们都知道，参数spark.memory.fraction的作用是明确Spark可支配内存占比，换句话说，就是在所有的堆内空间中，有多大比例的内存可供Spark消耗。相应地，1 - spark.memory.fraction就是User Memory在堆内空间的占比。</p><p>因此，<strong>spark.memory.fraction参数决定着两者如何瓜分堆内内存，它的系数越大，Spark可支配的内存越多，User Memory区域的占比自然越小</strong>。spark.memory.fraction的默认值是0.6，也就是JVM堆内空间的60%会划拨给Spark支配，剩下的40%划拨给User Memory。</p><p>那么，User Memory都用来存啥呀？需要预留那么大的空间吗？简单来说，User Memory存储的主要是开发者自定义的数据结构，这些数据结构往往用来协助分布式数据集的处理。</p><p>举个例子，还记得调度系统那一讲Label Encoding的例子吗？</p><pre><code>/**\n实现方式2\n输入参数：模板文件路径，用户兴趣字符串\n返回值：用户兴趣字符串对应的索引值\n*/\n \n//函数定义\nval findIndex: (String) =&gt; (String) =&gt; Int = {\n(filePath) =&gt;\nval source = Source.fromFile(filePath, &quot;UTF-8&quot;)\nval lines = source.getLines().toArray\nsource.close()\nval searchMap = lines.zip(0 until lines.size).toMap\n(interest) =&gt; searchMap.getOrElse(interest, -1)\n}\nval partFunc = findIndex(filePath)\n \n//Dataset中的函数调用\npartFunc(&quot;体育-篮球-NBA-湖人&quot;)\n\n</code></pre><p>在这个例子中，我们先读取包含用户兴趣的模板文件，然后根据模板内容构建兴趣到索引的映射字典。在对千亿样本做Lable Encoding的时候，这个字典可以快速查找兴趣字符串，并返回对应索引，来辅助完成数据处理。像这样的映射字典就是所谓的自定义数据结构，这部分数据都存储在User Memory内存区域。</p><p>因此，<strong>当在JVM内平衡Spark可用内存和User Memory时，你需要考虑你的应用中类似的自定义数据结构多不多、占比大不大？然后再相应地调整两块内存区域的相对占比</strong>。如果应用中自定义的数据结构很少，不妨把spark.memory.fraction配置项调高，让Spark可以享用更多的内存空间，用于分布式计算和缓存分布式数据集。</p><h2>Execution Memory该如何与Storage Memory平衡？</h2><p>最后，咱们再来说说，Execution Memory与Storage Memory的平衡。在内存管理那一讲，我给你讲了一个黄四郎地主招租的故事，并用故事中的占地协议类比了执行内存与缓存内存之间的竞争关系。执行任务与RDD缓存共享Spark可支配内存，但是,执行任务在抢占方面有更高的优先级。</p><p>因此通常来说，在统一内存管理模式下，spark.memory.storageFraction的设置就显得没那么紧要，因为无论这个参数设置多大，执行任务还是有机会抢占缓存内存，而且一旦完成抢占，就必须要等到任务执行结束才会释放。</p><p>不过，凡事都没有绝对，<strong>如果你的应用类型是“缓存密集型”，如机器学习训练任务，就很有必要通过调节这个参数来保障数据的全量缓存</strong>。这类计算任务往往需要反复遍历同一份分布式数据集，数据缓存与否对任务的执行效率起着决定性作用。这个时候，我们就可以把参数spark.memory.storageFraction调高，然后有意识地在应用的最开始把缓存灌满，再基于缓存数据去实现计算部分的业务逻辑。</p><p>但在这个过程中，<strong>你要特别注意RDD缓存与执行效率之间的平衡</strong>。为什么这么说呢？</p><p>首先，RDD缓存占用的内存空间多了，Spark用于执行分布式计算任务的内存空间自然就变少了，而且数据分析场景中常见的关联、排序和聚合等操作都会消耗执行内存，这部分内存空间变少，自然会影响到这类计算的执行效率。</p><p>其次，大量缓存引入的GC（Garbage Collection，垃圾回收）负担对执行效率来说是个巨大的隐患。</p><p>你还记得黄四郎要招租的土地分为托管田和自管田吗？托管田由黄四郎派人专门打理土地秋收后的翻土、整平等杂务，为来年种下一茬庄稼做准备。堆内内存的垃圾回收也是一个道理，JVM大体上把Heap堆内内存分为年轻代和老年代。年轻代存储生命周期较短、引用次数较低的对象；老年代则存储生命周期较长、引用次数高的对象。因此，像RDD cache这种一直缓存在内存里的数据，一定会被JVM安排到老年代。</p><p>年轻代的垃圾回收工作称为Young GC，老年代的垃圾回收称为Full GC。每当老年代可用内存不足时，都会触发JVM执行Full GC。在Full GC阶段，JVM会抢占应用程序执行线程，强行征用计算节点中所有的CPU线程，也就是“集中力量办大事”。当所有CPU线程都被拿去做垃圾回收工作的时候，应用程序的执行只能暂时搁置。只有等Full GC完事之后，把CPU线程释放出来，应用程序才能继续执行。这种Full GC征用CPU线程导致应用暂停的现象叫做“Stop the world”。</p><p>因此，Full GC对于应用程序的伤害远大于Young GC，并且GC的效率与对象个数成反比，对象个数越多，GC效率越差。这个时候，对于RDD这种缓存在老年代中的数据，就很容易引入Full GC问题。</p><p>一般来说，为了提升RDD cache访问效率，很多同学都会采用以对象值的方式把数据缓存到内存，因为对象值的存储方式避免了数据存取过程中序列化与反序列化的计算开销。我们在RDD/DataFrame/Dataset之上调用cache方法的时候，默认采用的就是这种存储方式。</p><p>但是，采用对象值的方式缓存数据，不论是RDD，还是DataFrame、Dataset，每条数据样本都会构成一个对象，要么是开发者自定义的Case class，要么是Row对象。换句话说，老年代存储的对象个数基本等于你的样本数。因此，当你的样本数大到一定规模的时候，你就需要考虑大量的RDD cache可能会引入的Full GC问题了。</p><p>基于上面的分析，我们不难发现，<strong>在打算把大面积的内存空间用于RDD cache之前，你需要衡量这么做可能会对执行效率产生的影响</strong>。</p><p>你可能会说：“我的应用就是缓存密集型，确实需要把数据缓存起来，有什么办法来平衡执行效率吗？”办法还是有的。</p><p><strong>首先，你可以放弃对象值的缓存方式，改用序列化的缓存方式，序列化会把多个对象转换成一个字节数组。</strong>这样，对象个数的问题就得到了初步缓解。</p><p><strong>其次，我们可以调节spark.rdd.compress这个参数。</strong>RDD缓存默认是不压缩的，启用压缩之后，缓存的存储效率会大幅提升，有效节省缓存内存的占用，从而把更多的内存空间留给分布式任务执行。</p><p>通过这两类调整，开发者在享用RDD数据访问效率的同时，还能够有效地兼顾应用的整体执行效率，可谓是两全其美。不过，有得必有失，尽管这两类调整优化了内存的使用效率，但都是以引入额外的计算开销、牺牲CPU为代价的。这也就是我们一直强调的：性能调优的过程本质上就是不断地平衡不同硬件资源消耗的过程。</p><h2>哪些配置项与磁盘设置有关？</h2><p>在存储系统那一讲，我们简单提到过spark.local.dir这个配置项，这个参数允许开发者设置磁盘目录，该目录用于存储RDD cache落盘数据块和Shuffle中间文件。</p><p>通常情况下，spark.local.dir会配置到本地磁盘中容量比较宽裕的文件系统，毕竟这个目录下会存储大量的临时文件，我们需要足够的存储容量来保证分布式任务计算的稳定性。不过，如果你的经费比较充裕，有条件在计算节点中配备足量的SSD存储，甚至是更多的内存资源，完全可以把SSD上的文件系统目录，或是内存文件系统添加到spark.local.dir配置项中去，从而提供更好的I/O性能。</p><h2>小结</h2><p>掌握硬件资源类的配置项是我们打破性能瓶颈，以及平衡不同硬件资源利用率的必杀技。具体来说，我们可以分成两步走。</p><p><strong>第一步，理清CPU、内存和磁盘这三个方面的性能配置项都有什么，以及它们的含义。</strong>因此，我把硬件资源类配置项的含义都汇总在了一个表格中，方便你随时查看。有了这份手册，在针对硬件资源进行配置项调优时，你就能够做到不重不漏。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/0a/c9cfdb17e3ec6e8d022ff8e91e4a170a.jpg?wh=1873*1255\" alt=\"\"></p><p><strong>第二步，重点理解这些配置项的作用，以及可以解决的问题。</strong></p><p>首先，对于CPU类配置项，我们要重点理解并行度与并行计算任务数的区别。并行度从数据的角度出发，明确了数据划分的粒度，并行度越高，数据粒度越细，数据越分散，CPU资源利用越充分，但同时要提防数据粒度过细导致的调度系统开销。</p><p>并行计算任务数则不同，它从计算的角度出发，强调了分布式集群在任一时刻并行处理的能力和容量。并行度与并行计算任务数之间互相影响、相互制约。</p><p>其次，对于内存类配置项，我们要知道怎么设置它们来平衡不同内存区域的方法。这里我们主要搞清楚3个问题就可以了：</p><ol>\n<li>在平衡堆外与堆内内存的时候，我们要重点考察数据模式。如果数据模式比较扁平，而且定长字段较多，应该更多地使用堆外内存。相反地，如果数据模式比较复杂，应该更多地利用堆内内存</li>\n<li>在平衡可支配内存和User memory的时候，我们要重点考察应用中自定义的数据结构。如果数据结构较多，应该保留足够的User memory空间。相反地，如果数据结构较少，应该让Spark享有更多的可用内存资源</li>\n<li>在平衡Execution memory与Storage memory的时候，如果RDD缓存是刚需，我们就把spark.memory.storageFraction调大，并且在应用中优先把缓存灌满，再把计算逻辑应用在缓存数据之上。除此之外，我们还可以同时调整spark.rdd.compress和spark.memory.storageFraction来缓和Full GC的冲击</li>\n</ol><h2>每日一练</h2><ol>\n<li>并行度设置过大会带来哪些弊端？</li>\n<li>在Shuffle的计算过程中，有哪些Spark内置的数据结构可以充分利用堆外内存资源？</li>\n<li>堆外与堆内的取舍，你还能想到其他的制约因素吗？</li>\n<li>如果内存资源足够丰富，有哪些方式可以开辟内存文件系统，用于配置spark.local.dir参数？</li>\n</ol><p>期待在留言区看到你的思考和答案，也欢迎你把这份硬件资源配置项手册分享给更多的朋友，我们下一讲见！</p>","comments":[{"had_liked":false,"id":290674,"user_name":"Joe","can_delete":false,"product_type":"c1","uid":1039661,"ip_address":"","ucode":"3C5DB1B75EC61C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dd/2d/9fd44ed5.jpg","comment_is_top":false,"comment_ctime":1619683100,"is_pvip":false,"replies":[{"id":"105343","content":"先来回答第二个问题，这个参数对所有的DF、DS、SQL都生效。<br><br>再来说第一个问题，例子蛮好的。先说答案，然后咱们再去分析：对于你这种情况，两张表都会Shuffle。当然，假设你的表都很大，超过广播阈值，能转换成Broadcast join那就另当别论了。<br><br>为什么呢？spark.sql.shuffle.partitions默认是200。 这部分其实需要一些“前置引用”的知识，这些知识其实是在22讲Physical Planning才会涉及，所以在这一讲这里会比较难理解。<br><br>是这样的，Spark SQL执行计划中的每一个节点，都有4个重要的属性，分别是：<br>节点要求的输入：<br>1）requiredChildDistribution<br>2）requiredChildOrdering<br>节点的输出：<br>3）outputPartitioning<br>4）outputOrdering<br><br>每个节点都有输出的分区情况、排序情况，也就是3）、4）；同时，每个节点对于自己的子节点，都有关于 分区和排序的要求，也就是1）、2）。<br><br>当子节点的分区与排序情况，不满足当前节点的输入要求时，Spark SQL就会在Physical planning阶段，强行插入一些中间节点，比如Exchange（Shuffle）。<br><br>回到你的问题，join要求子节点（df1、df2）的outputPartitioning是以joinKey为分区键，分成200个分区（因为spark.sql.shuffle.partitions默认值是200）。但是，你的df1，有3个分区；df2未知，因此，Spark SQL会在物理计划阶段给两个子节点，也就是df1、df2强行插入Exchange、也就是Shuffle。这两个Shuffle，会分别把df1、df2变成是有200个分区的分布式数据集。两个Shuffle做完之后，才会计算后面的Join。<br><br>所以，最终的分区，不是df1的3、也不是df2原来的分区，而是spark.sql.shuffle.partitions参数设定的值。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619694776,"ip_address":"","comment_id":290674,"utype":1}],"discussion_count":1,"race_medal":0,"score":"121878767388","product_id":100073401,"comment_content":"老师，针对spark.sql.shuffle.partitions的使用有一些疑问？<br><br>1. 如果df1.join(df2)，df1用的是hash partitioner并且分区数是3，这种情况在reduce端参数spark.sql.shuffle.partitions会生效吗？还是以df1的分区为准？<br><br>2. 这个参数是针对所有的Dataframe，DataSet和SQL，还是只有SQL生效？<br><br>","like_count":28,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519258,"discussion_content":"先来回答第二个问题，这个参数对所有的DF、DS、SQL都生效。\n\n再来说第一个问题，例子蛮好的。先说答案，然后咱们再去分析：对于你这种情况，两张表都会Shuffle。当然，假设你的表都很大，超过广播阈值，能转换成Broadcast join那就另当别论了。\n\n为什么呢？spark.sql.shuffle.partitions默认是200。 这部分其实需要一些“前置引用”的知识，这些知识其实是在22讲Physical Planning才会涉及，所以在这一讲这里会比较难理解。\n\n是这样的，Spark SQL执行计划中的每一个节点，都有4个重要的属性，分别是：\n节点要求的输入：\n1）requiredChildDistribution\n2）requiredChildOrdering\n节点的输出：\n3）outputPartitioning\n4）outputOrdering\n\n每个节点都有输出的分区情况、排序情况，也就是3）、4）；同时，每个节点对于自己的子节点，都有关于 分区和排序的要求，也就是1）、2）。\n\n当子节点的分区与排序情况，不满足当前节点的输入要求时，Spark SQL就会在Physical planning阶段，强行插入一些中间节点，比如Exchange（Shuffle）。\n\n回到你的问题，join要求子节点（df1、df2）的outputPartitioning是以joinKey为分区键，分成200个分区（因为spark.sql.shuffle.partitions默认值是200）。但是，你的df1，有3个分区；df2未知，因此，Spark SQL会在物理计划阶段给两个子节点，也就是df1、df2强行插入Exchange、也就是Shuffle。这两个Shuffle，会分别把df1、df2变成是有200个分区的分布式数据集。两个Shuffle做完之后，才会计算后面的Join。\n\n所以，最终的分区，不是df1的3、也不是df2原来的分区，而是spark.sql.shuffle.partitions参数设定的值。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619694776,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288605,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1618558063,"is_pvip":false,"replies":[{"id":"104747","content":"这块确实有点坑，只有你的计算中涉及Joins或是聚合，spark.sql.shuffle.partitions，这个参数的设置，才会影响Shuffle Reduce阶段的并行度。如果你的作业没有Joins或是聚合计算，那确实，这个参数设了也是摆设。<br><br>比如你仅仅是读Parquet，然后想通过这个参数调整并行度，确实是徒劳，这个时候，你确实只能自己用repartition或是Coalesce去做重分区。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618564728,"ip_address":"","comment_id":288605,"utype":1}],"discussion_count":4,"race_medal":0,"score":"70338034799","product_id":100073401,"comment_content":"关于spark.sql.shuffle.partitions 老师实际工作中我发现这个参数不管用。比如我把它设成2000，并去读parquet文件，大约几百个文件吧，但我看task数量只有80个，而且还一直在变，有时会更少。网上说spark-sql的并行度不管用，要自己手动repartition，是这样吗？task数量一直在变是因为不断地在做groupBy和join吗？那为什么task数量始终达不到我设的spark.sql.shuffle.partitions = 2000呢？","like_count":16,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518682,"discussion_content":"这块确实有点坑，只有你的计算中涉及Joins或是聚合，spark.sql.shuffle.partitions，这个参数的设置，才会影响Shuffle Reduce阶段的并行度。如果你的作业没有Joins或是聚合计算，那确实，这个参数设了也是摆设。\n\n比如你仅仅是读Parquet，然后想通过这个参数调整并行度，确实是徒劳，这个时候，你确实只能自己用repartition或是Coalesce去做重分区。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618564728,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":1333809,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/NicO4BNFaVbHiblKLfaLU2krpbibDniaodysvDQCcicRX7SzmdMtjsJmXm3tCjfCD0sHB1XI4Tak1M4Gp6ibibibt2rWRg/132","nickname":"蒙娜丽莎法师","note":"","ucode":"65FAF50A78609F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":543463,"discussion_content":"老师，这种情况是不是可以通过spark.default.parallelism来设置","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1641170697,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":518682,"ip_address":""},"score":543463,"extra":""},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1333809,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/NicO4BNFaVbHiblKLfaLU2krpbibDniaodysvDQCcicRX7SzmdMtjsJmXm3tCjfCD0sHB1XI4Tak1M4Gp6ibibibt2rWRg/132","nickname":"蒙娜丽莎法师","note":"","ucode":"65FAF50A78609F","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":563735,"discussion_content":"不可以，单纯读文件来说，有多线程的写法。没看错，spark代码里面可以写多线程。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650072111,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":543463,"ip_address":""},"score":563735,"extra":""}]},{"author":{"id":2621850,"avatar":"https://static001.geekbang.org/account/avatar/00/28/01/9a/d2831441.jpg","nickname":"康","note":"","ucode":"C3E85292E026D5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":578487,"discussion_content":"spark.files.maxPartitionBytes 这个参数可以，让spark读的时候就把任务切分，这样map节点的任务就上去了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656823591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289880,"user_name":"断笔画墨","can_delete":false,"product_type":"c1","uid":1675883,"ip_address":"","ucode":"5AEF9C3FFDB877","user_header":"https://static001.geekbang.org/account/avatar/00/19/92/6b/6f324fc8.jpg","comment_is_top":false,"comment_ctime":1619250378,"is_pvip":false,"replies":[{"id":"105148","content":"我理解数据读取效率和字段类型没什么关系，要提高并行计算效率，可以考虑使用API：<br>spark.read.jdbc(url, table, predicates, props)<br><br>其中url就是你的Oracle DB<br>table是表名<br>props是各种属性，比如权限信息，如user，password<br><br>predicates比较关键，是划分数据分区的谓词数组，比如[&quot;id between 1 and 100&quot;, &quot;id between 101 and 200&quot;,  ... &quot;id between 901 and 1000&quot;]，本质上就是用这些过滤条件把查询分成多个，每个查询的结果都是一个数据分片。这么做的好处，是可以提高Spark的并行度，但是要注意，同时发这么多查询请求给Oracle，要注意Oracle数据库本身的并发处理能力。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619317037,"ip_address":"","comment_id":289880,"utype":1}],"discussion_count":3,"race_medal":0,"score":"48863890634","product_id":100073401,"comment_content":"spark读取oracle表，oracle表结构没有数值类型，大部分都是varchar，数据量上亿，怎么在源端做高并发读取啊","like_count":11,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519041,"discussion_content":"我理解数据读取效率和字段类型没什么关系，要提高并行计算效率，可以考虑使用API：\nspark.read.jdbc(url, table, predicates, props)\n\n其中url就是你的Oracle DB\ntable是表名\nprops是各种属性，比如权限信息，如user，password\n\npredicates比较关键，是划分数据分区的谓词数组，比如[&amp;quot;id between 1 and 100&amp;quot;, &amp;quot;id between 101 and 200&amp;quot;,  ... &amp;quot;id between 901 and 1000&amp;quot;]，本质上就是用这些过滤条件把查询分成多个，每个查询的结果都是一个数据分片。这么做的好处，是可以提高Spark的并行度，但是要注意，同时发这么多查询请求给Oracle，要注意Oracle数据库本身的并发处理能力。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619317037,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392072,"discussion_content":"其实我很赞同楼上说的，首先并行读取一定是一个端到端的过程，比如spark从hive读数据，首先考虑的是hive的数据文件分布，数据块分布，然后使用task的纬度去并行的拉取数据，如果楼主要从oracle mysql等传统数据库读数据，那么一定要考虑这些数据库本身能否支持并行读取，甚至做到像hive那样数据块级别的并行读取，这才是真正的并行，如果只是靠着指定id范围来指定并行度，而这些数据在传统数据库中并不是这样分布的，读取时就会带来大量的随机读，严格的说这种并行式读取是一种假并行，有点自欺欺人了，所以我说真正的并行读取一定是一个端到端的过程，一定要考虑原始数据的分布或者它本身是否支持并行度。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1630811893,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2618579,"avatar":"https://static001.geekbang.org/account/avatar/00/27/f4/d3/0540ea8c.jpg","nickname":"寒娇娇爸","note":"","ucode":"39FC58CA8BF06F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":379079,"discussion_content":"这个问题不仅要考虑spark本身处理的过程，同时也要考虑从oracle获取数据的过程。1.为什么提问者不从oracle本身思考解决并发问题，并且把计算在Oracle内完成，例如alter table parallel xx parallel query，相关可以参考Oracle原厂文档关于大数据处理，这类舍近求远架构是否合理？2. Spark是能告知Oracle触发平行读取？3. 整个从oracle到spark的过程涉及到IO、CPU、网络带宽等诸多要素，如何做评估和度量？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623666826,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288377,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1618449321,"is_pvip":false,"replies":[{"id":"104697","content":"好问题，通常来说，这个参数都不需要动，默认就是1。回答你的问题，这个值，不能小于1。那么大于1是什么情况呢？就是你的task本身，是需要多线程操作的，比如一个线程把数据写到HDFS，另外一个线程，通过JDBC同时把数据塞进DBMS，诸如此类。<br><br>Spark.task.cpus这个参数的意思，在于Spark为这种特殊的多线程task提供了一种开放的可能，允许你去设置大于1的cpu core。但允许不代表你一定要这么做哈<br><br>特别注意的是，如果你的task没什么特别，但你还是设置的大于1的数值，那cpu就是白白浪费。spark只会去同时launch （spark.executor.cores &#47; spark.task.cpus）这么多的tasks。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618486485,"ip_address":"","comment_id":288377,"utype":1}],"discussion_count":1,"race_medal":0,"score":"48863089577","product_id":100073401,"comment_content":"Spark.task.cpus这个参数的设置，我之前理解就是一个cpu核运行一个task。难道还可以0.5个cpu或者多个cpu运行一个task吗？","like_count":11,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518608,"discussion_content":"好问题，通常来说，这个参数都不需要动，默认就是1。回答你的问题，这个值，不能小于1。那么大于1是什么情况呢？就是你的task本身，是需要多线程操作的，比如一个线程把数据写到HDFS，另外一个线程，通过JDBC同时把数据塞进DBMS，诸如此类。\n\nSpark.task.cpus这个参数的意思，在于Spark为这种特殊的多线程task提供了一种开放的可能，允许你去设置大于1的cpu core。但允许不代表你一定要这么做哈\n\n特别注意的是，如果你的task没什么特别，但你还是设置的大于1的数值，那cpu就是白白浪费。spark只会去同时launch （spark.executor.cores / spark.task.cpus）这么多的tasks。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618486485,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288036,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1618278724,"is_pvip":false,"replies":[{"id":"104591","content":"这个其实取决于你把对象放在哪里。<br><br>分两种情况来看哈~<br><br>1. 如果你用RDD封装这些自定义类型，比如RDD[Student]，那么，数据集消耗的是Execution memory。<br><br>2. 相反，如果你是在处理分布式数据集的函数中，new Student来辅助计算过程，那么这个对象，是放在User memory里面的。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618305794,"ip_address":"","comment_id":288036,"utype":1}],"discussion_count":3,"race_medal":0,"score":"40272984388","product_id":100073401,"comment_content":"Class Student是存在User Memory? new Student(&quot;小明&quot;)是存在Executor Memory？","like_count":9,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518496,"discussion_content":"这个其实取决于你把对象放在哪里。\n\n分两种情况来看哈~\n\n1. 如果你用RDD封装这些自定义类型，比如RDD[Student]，那么，数据集消耗的是Execution memory。\n\n2. 相反，如果你是在处理分布式数据集的函数中，new Student来辅助计算过程，那么这个对象，是放在User memory里面的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618305794,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":563738,"discussion_content":"你是不是背JVM八股文着魔了？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650072251,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367734,"discussion_content":"我提这个问题，可能受到Java虚拟机内存管理的影响了，哈哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618452748,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288373,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1618449132,"is_pvip":false,"replies":[{"id":"104698","content":"并行度和并发度是两个完全不同的概念，一个数据视角——并行度；一个是计算视角——并发度，这个我们本讲应该有过介绍哈~<br><br>另外你说的spark.executor.cores与v-core的对应关系是没问题的。但是cores也好，v-core也罢，物理上不见得对应的是一个物理CPU core，这个要看CPU的硬件配置，有些CPU只能起一个线程，不过大部分现代CPU都能起两个线程。所以spark.executor.cores与v-core，更准确的对应，是线程，而不是物理上的CPU core。这个地方其实有点绕，需要注意。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618486851,"ip_address":"","comment_id":288373,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31683220204","product_id":100073401,"comment_content":"老师是不是可以这样理解，spark-submit提交任务的时候申请的总cores数为10，yarn调度系统会分配10个v-core，如果集群资源充足，实际上一个v-core就是对应一个cpu核，如果资源不够，相当于就不是一对一，此时集群最大的task并行度并不是10，而是并发度为10。<br>以上理解对吗？","like_count":7,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518607,"discussion_content":"并行度和并发度是两个完全不同的概念，一个数据视角——并行度；一个是计算视角——并发度，这个我们本讲应该有过介绍哈~\n\n另外你说的spark.executor.cores与v-core的对应关系是没问题的。但是cores也好，v-core也罢，物理上不见得对应的是一个物理CPU core，这个要看CPU的硬件配置，有些CPU只能起一个线程，不过大部分现代CPU都能起两个线程。所以spark.executor.cores与v-core，更准确的对应，是线程，而不是物理上的CPU core。这个地方其实有点绕，需要注意。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618486851,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289041,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1618826382,"is_pvip":false,"replies":[{"id":"104895","content":"spark.executor.memoryOverhead ，在yarn、k8s部署模式下，container会预留一部分内存，形式是堆外，用来保证稳定性，主要存储nio buffer，函数栈等一些开销，所以你看名字：over head。这部分内存，他的目的是保持Spark作业运行时的稳定性。<br><br>这个failure，报这个错，说明overhead空间不足，系统必须的开销没有足够的空间～ 调大就行了～ 这部分内存和执行性能关系不大，所以咱们课程里没有提。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618883475,"ip_address":"","comment_id":289041,"utype":1}],"discussion_count":3,"race_medal":0,"score":"23093662862","product_id":100073401,"comment_content":"一个任务报错Container Killed by Yarn For Exceeding Memory Limits Consider boosting spark.yarn.executor.memoryOverhead<br>我按照提示增加spark.yarn.executor.memoryOverhead，任务的确执行通过。<br>请教老师spark.yarn.executor.memoryOverhead参数控制哪部分内存，主要负责什么？什么情况下会用到这部分内存。文章中关于内存的好像没有提到这部分。","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518801,"discussion_content":"spark.executor.memoryOverhead ，在yarn、k8s部署模式下，container会预留一部分内存，形式是堆外，用来保证稳定性，主要存储nio buffer，函数栈等一些开销，所以你看名字：over head。这部分内存，他的目的是保持Spark作业运行时的稳定性。\n\n这个failure，报这个错，说明overhead空间不足，系统必须的开销没有足够的空间～ 调大就行了～ 这部分内存和执行性能关系不大，所以咱们课程里没有提。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618883475,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368990,"discussion_content":"我的场景是hive on spark两个表join ，好奇这种场景为什么会用到预留的端外内存。而且这部分内存还是用来保存nio 函数栈开销的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618897628,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":563741,"discussion_content":"local buffer-poll、network buffer-poll 等自定义缓存都是需要内存开销的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650072425,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":368990,"ip_address":""},"score":563741,"extra":""}]}]},{"had_liked":false,"id":287837,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1618190223,"is_pvip":false,"replies":[{"id":"104549","content":"好问题，你可以这么来理解，凡是数据源，不管是Hive来的，还是HDFS来的，还是S3来的，不管是什么格式，比如Parquet、ORC，这些数据源，consume的都是执行内存，当然，如果他们被cache了，那就消耗Storage memory。<br><br>那些开发者自己自定义的类、类型、数据结构、Struct，等等，这些东西往往用来辅助完成对于刚刚说的那些数据源的处理，这些辅助性的类、类型、数据结构、Struct，才是自定义数据结构。他们消耗的，就是user memory。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618220739,"ip_address":"","comment_id":287837,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23093026703","product_id":100073401,"comment_content":"老师，这个自定义结构不是很懂，什么样的数据格式是自定义数据格式，我现在经常接触到的是spark-submit 提交sparksql这一块，用的是hive表，这个涉及自定义数据结构吗","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518436,"discussion_content":"好问题，你可以这么来理解，凡是数据源，不管是Hive来的，还是HDFS来的，还是S3来的，不管是什么格式，比如Parquet、ORC，这些数据源，consume的都是执行内存，当然，如果他们被cache了，那就消耗Storage memory。\n\n那些开发者自己自定义的类、类型、数据结构、Struct，等等，这些东西往往用来辅助完成对于刚刚说的那些数据源的处理，这些辅助性的类、类型、数据结构、Struct，才是自定义数据结构。他们消耗的，就是user memory。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618220739,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":308445,"user_name":"Sean","can_delete":false,"product_type":"c1","uid":2162751,"ip_address":"","ucode":"69234046BFD81B","user_header":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","comment_is_top":false,"comment_ctime":1629622495,"is_pvip":false,"replies":[{"id":"111711","content":"对的，总结的很到位~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629702595,"ip_address":"","comment_id":308445,"utype":1}],"discussion_count":2,"race_medal":0,"score":"18809491679","product_id":100073401,"comment_content":"根据老师的回回复,个人总结如下,不知是否正确:在不使用yarn,k8s模式下,完全没有必要启用off heap,而且在钨丝计划的加持下,可以理解为使用堆内内存,不会对任务有任何影响,但在使用yarn或k8s模式下,必须要开启off heap,否则会出现t Container Killed by Yarn For Exceeding Memory Limits Consider boosting spark.yarn.executor.memoryOverhead报错,需要调大spark.yarn.executor.memoryOverhead","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525495,"discussion_content":"对的，总结的很到位~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629702595,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392073,"discussion_content":"使用yarn或者k8s模式也没必要一定要开启offhead，这两个没有必然联系，如果实际报了overhead不够，你就调大就好了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1630813477,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286786,"user_name":"勿更改任何信息","can_delete":false,"product_type":"c1","uid":2028956,"ip_address":"","ucode":"575185C69C05A3","user_header":"","comment_is_top":false,"comment_ctime":1617594680,"is_pvip":false,"replies":[{"id":"104149","content":"不会的，用堆内更稳定，而且堆外默认是关闭的。没有必须使用堆外的场景，事实上，由于tungsten的优化，spark官方推荐使用堆内内存，在保证稳定性的同时，还可以大幅提升性能。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617614965,"ip_address":"","comment_id":286786,"utype":1}],"discussion_count":3,"race_medal":0,"score":"14502496568","product_id":100073401,"comment_content":"如果我把堆外内存关闭，会不会导致spark sql执行失败？也就是spark sql有没有必须使用堆外内存的场景？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518089,"discussion_content":"不会的，用堆内更稳定，而且堆外默认是关闭的。没有必须使用堆外的场景，事实上，由于tungsten的优化，spark官方推荐使用堆内内存，在保证稳定性的同时，还可以大幅提升性能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617614965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028957,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","nickname":"Geek2014","note":"","ucode":"9EB356D8DF287E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366215,"discussion_content":"但是老师你上一讲说tungsten是使用的Java unsafe api，这个时候就是使用的堆外内存吧？只是这时的堆外内存，不受spark.memory.offHeap.size的控制，而是spark.yarn.executor.memoryOverhead的控制？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617982090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2028957,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","nickname":"Geek2014","note":"","ucode":"9EB356D8DF287E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377050,"discussion_content":"先跟老弟说声抱歉，这么久才回复，折叠的留言不容易发现，后台也没有提醒，所以通常是定期回顾留言的时候，才会看到。是这样的，Tungsten优化机制能否享受的到，和是否开启堆外内存没有关系，即便你不用堆外，Tungsten的各种优化机制，你照样可以享受得到，具体可以参考Spark SQL和Tungsten那3讲。另外，spark.yarn.executor.memoryOverhead，这个参数主要是用来维护稳定性的，可以参考其他留言的回复，它跟性能调优没有关系哈~ 主要是用到Yarn、K8s调度，才需要考虑这个参数~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1622472098,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":366215,"ip_address":""},"score":377050,"extra":""}]}]},{"had_liked":false,"id":286475,"user_name":"铜镜","can_delete":false,"product_type":"c1","uid":2524884,"ip_address":"","ucode":"8D1E7234D5AA01","user_header":"https://static001.geekbang.org/account/avatar/00/26/86/d4/98216895.jpg","comment_is_top":false,"comment_ctime":1617334250,"is_pvip":false,"replies":[{"id":"104090","content":"四道题都答的不错～ 第三题，同样的操作，堆内也存在同样的问题。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617431232,"ip_address":"","comment_id":286475,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14502236138","product_id":100073401,"comment_content":"1.资源浪费,并行度设置过大会增加调度开销,CPU利用率会变低<br>2.不好意思没看明白这个是在问啥😁, int,char这种?还是dataframe这种?还是AppendOnlyMap这种?<br>3.如果需要频繁操作反序列化出来的实际值的话,使用堆外内存会大幅增加cpu开销<br>4.alluxio?<br>","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517999,"discussion_content":"四道题都答的不错～ 第三题，同样的操作，堆内也存在同样的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617431232,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291201,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620112704,"is_pvip":false,"replies":[{"id":"105486","content":"满分💯","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620117385,"ip_address":"","comment_id":291201,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10210047296","product_id":100073401,"comment_content":"并行度太高可能会造成任务调度耗时超过任务处理耗时，如果不进行后续分区合并，还有会造成小文件问题（比如写入Hive）","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519399,"discussion_content":"满分💯","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620117385,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287580,"user_name":"薛峰","can_delete":false,"product_type":"c1","uid":2546023,"ip_address":"","ucode":"DA8CAF1473D2B9","user_header":"https://static001.geekbang.org/account/avatar/00/26/d9/67/9bca6a6e.jpg","comment_is_top":false,"comment_ctime":1618038991,"is_pvip":false,"replies":[{"id":"104432","content":"有帮助就好哈～ 没事，以后面试机会多的是～ 😉","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618055754,"ip_address":"","comment_id":287580,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10207973583","product_id":100073401,"comment_content":"醍醐灌顶，真希望是在过去的面试之前看到这篇。","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518355,"discussion_content":"有帮助就好哈～ 没事，以后面试机会多的是～ 😉","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618055754,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325758,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1639118566,"is_pvip":false,"replies":[{"id":"118184","content":"不是的哈，DataFrame、RDD、Dataset，这些都是分布式数据集，自定义数据结构，指的是像数组、列表、map这些用户自定义，在分布式数据集当中引用的数据结构","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639149701,"ip_address":"","comment_id":325758,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5934085862","product_id":100073401,"comment_content":"老师问下自己定义的dataframe是不是也属于自定义数据结构（从而存储在user memory）？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537722,"discussion_content":"不是的哈，DataFrame、RDD、Dataset，这些都是分布式数据集，自定义数据结构，指的是像数组、列表、map这些用户自定义，在分布式数据集当中引用的数据结构","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639149701,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":297621,"user_name":"feihui","can_delete":false,"product_type":"c1","uid":1007294,"ip_address":"","ucode":"13F1D4A82BC650","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/be/9ea55f46.jpg","comment_is_top":false,"comment_ctime":1623674045,"is_pvip":true,"replies":[{"id":"108254","content":"听上去和你的描述并不冲突呀~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624009063,"ip_address":"","comment_id":297621,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5918641341","product_id":100073401,"comment_content":"老师描述的gc有歧义，并不是抢占线程，而是CPU资源，STW 是因为并发安全问题，","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521880,"discussion_content":"听上去和你的描述并不冲突呀~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624009063,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292107,"user_name":"Geek_01fccd","can_delete":false,"product_type":"c1","uid":2177406,"ip_address":"","ucode":"6D5D9C2882513F","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ia89MwaRSP6iaeQB4789roUntH7tia9EXyoDOnlibE8ABibAzFPiamP0SAU54NNTRiaVtkOtmLaWRH5OXbTOhUZl46scw/132","comment_is_top":false,"comment_ctime":1620694060,"is_pvip":false,"replies":[{"id":"105817","content":"对，通常来说，这个时候都是用SQL去做数据探索、数据分析，不需要User Memory的。<br><br>不过，并不绝对哈，比如你的SQL需要某个自定义数组来过滤数据条目，这个时候这种自定义数组，会去消耗User Memory。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620797992,"ip_address":"","comment_id":292107,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5915661356","product_id":100073401,"comment_content":"在hive on spark模式下，是否可以忽略user memory，设置spark.memory.fraction=0.9，这里应该不涉及用户自定义数据结构吧？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519681,"discussion_content":"对，通常来说，这个时候都是用SQL去做数据探索、数据分析，不需要User Memory的。\n\n不过，并不绝对哈，比如你的SQL需要某个自定义数组来过滤数据条目，这个时候这种自定义数组，会去消耗User Memory。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620797992,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1051904,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","nickname":"果子","note":"","ucode":"0A669B6DE26F21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388901,"discussion_content":"老师，如果我使用UDF的话，这块也是用户内存吧？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629032520,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287950,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1618232628,"is_pvip":false,"replies":[{"id":"104590","content":"比如，你用parallelize创建的RDD","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618305623,"ip_address":"","comment_id":287950,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5913199924","product_id":100073401,"comment_content":"老师你好，<br>对于“对于没有明确分区规则的 RDD 来说，我们用 spark.default.parallelism 定义其并行度。”<br>没有明确分区规则的 RDD是指什么，有什么场景属于没有分区规则的RDD？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518472,"discussion_content":"比如，你用parallelize创建的RDD","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618305623,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1418226,"avatar":"https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg","nickname":"Sampson","note":"","ucode":"BA78CA29A6D898","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545139,"discussion_content":"老师，我想就着这个问题再请教下，没有明确规则的RDD 是否包含我从各种数据库load 进来的dataframe 数据 ？还是说只有设置了分区规则的那个自定义类的才算呢 ？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641863354,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":358509,"user_name":"Geek_52bf8d","can_delete":false,"product_type":"c1","uid":2848485,"ip_address":"广东","ucode":"949E9DB1623217","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIFSW0PJiaSBqSAEBstv4OBkHtgTQgZpcLicueXlZoHiclia7pwSrntfnp5Yd8MYdhPF3Va2PsuhVHIibw/132","comment_is_top":false,"comment_ctime":1664371250,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1664371250","product_id":100073401,"comment_content":"老是，请求个问题，driver端内存，如果只是insert操作把数据写入表，那么driver端内存只要能存下广播变量差不错就可以了吧？给个1G2G足够了吧？","like_count":0},{"had_liked":false,"id":348170,"user_name":"Geek_73cee2","can_delete":false,"product_type":"c1","uid":2828185,"ip_address":"","ucode":"54848FF6BE74B1","user_header":"","comment_is_top":false,"comment_ctime":1654797167,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1654797167","product_id":100073401,"comment_content":"老师每个计算任务 是指每个stage还是同一个stage的不同分区对应的任务  还是其他的。。。有点学晕了","like_count":0},{"had_liked":false,"id":344323,"user_name":"我爱夜来香","can_delete":false,"product_type":"c1","uid":2609930,"ip_address":"","ucode":"10761E677EF05F","user_header":"https://static001.geekbang.org/account/avatar/00/27/d3/0a/92640aae.jpg","comment_is_top":false,"comment_ctime":1651468712,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1651468712","product_id":100073401,"comment_content":"老师,spark.sql.shuffle.partitions,这个是代表会有多少个reducer数量吧,其中一轮的并发量是不是可以理解为spark.sql.shuffle.partitions&#47;spark.executor.instance * spark.executor.cores","like_count":0},{"had_liked":false,"id":340790,"user_name":"夏国秦","can_delete":false,"product_type":"c1","uid":1205557,"ip_address":"","ucode":"8A0AB0EB22E999","user_header":"https://static001.geekbang.org/account/avatar/00/12/65/35/d0e0d2ee.jpg","comment_is_top":false,"comment_ctime":1649144567,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1649144567","product_id":100073401,"comment_content":"老师 请教一下 怎样才能够用scala去写spark写的很顺利 有不有好的学习资料推荐","like_count":0},{"had_liked":false,"id":337593,"user_name":"孙福","can_delete":false,"product_type":"c1","uid":2939356,"ip_address":"","ucode":"D5E75F0A962B72","user_header":"","comment_is_top":false,"comment_ctime":1646906294,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1646906294","product_id":100073401,"comment_content":"老师您好，请问spark.memory.offHeap.size与spark.yarn.executor.memoryOverhead区别是什么，这两个参数在spark中使用在什么场景","like_count":0},{"had_liked":false,"id":337592,"user_name":"孙福","can_delete":false,"product_type":"c1","uid":2939356,"ip_address":"","ucode":"D5E75F0A962B72","user_header":"","comment_is_top":false,"comment_ctime":1646905791,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1646905791","product_id":100073401,"comment_content":"老师，请问下，如果堆内和堆外内存同时开始，什么数据放堆内，什么数据放堆外，spark是怎么决定的？","like_count":0},{"had_liked":false,"id":308497,"user_name":"Sean","can_delete":false,"product_type":"c1","uid":2162751,"ip_address":"","ucode":"69234046BFD81B","user_header":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","comment_is_top":false,"comment_ctime":1629645563,"is_pvip":false,"replies":[{"id":"111721","content":"可以用repartition来做到~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629704213,"ip_address":"","comment_id":308497,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1629645563","product_id":100073401,"comment_content":"老师提到&quot;只有在计算中涉及Joins或是聚合，spark.sql.shuffle.partitions，这个参数的设置，才会影响Shuffle Reduce阶段的并行度。如果你的作业没有Joins或是聚合计算，这个参数是个摆设&quot;<br>那么比如在使用spark-sql跑任务时,sql没有涉及到join或聚合操作(可以理解为sql不会涉及到shuffle),那么spark.sql.shuffle.partitions的设置就不会生效,但是要增大并行度,正确的操作因该是怎么的呢?","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525507,"discussion_content":"可以用repartition来做到~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629704213,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390539,"discussion_content":"我在自己补充记录下,在spark2.4之前不支持sql设置,正确姿势:\nINSERT ... SELECT /*+ COALESCE(numPartitions) */ ...\nINSERT ... SELECT /*+ REPARTITION(numPartitions) */ ...","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1629882154,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390546,"discussion_content":"补充:\n1.没有明确分区规则的 RDD 来说，比如.reducebykey()没有明确,则使用 spark.default.parallelism 定义的并行度，如.reducebykey().(500),则使用500\n2.spark.sql.shuffle.partitions 用于明确指定数据关联或聚合操作中 Reduce 端的分区数量(对spark sql有效,joins,聚合才会触发)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629882938,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390307,"discussion_content":"好的 谢谢老师!","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629771889,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":304208,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1627293025,"is_pvip":false,"replies":[{"id":"110176","content":"没错，完全正确~ 赞老弟的学习方法，xMind我也常用，对于构建知识体系非常有帮助。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1627482255,"ip_address":"","comment_id":304208,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1627293025","product_id":100073401,"comment_content":"老师，有个问题不是很清楚，假如我自定义一个case class Student，然后将数据源RDD[Row]变为RDD[Student]，第一，RDD[Sstudent]的聚合操作是消耗了Execution Memory？第二，假如我将RDD[Student].cache起来了，是消耗了Storage Memory?第三，假如执行RDD[Sstudent].filter()，flilter里的自定义函数，我new了一个array或map来处理逻辑，那么这些array或map是不是消耗user memory呢？期待老师的回答，不然我的xmind无法产生了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523980,"discussion_content":"没错，完全正确~ 赞老弟的学习方法，xMind我也常用，对于构建知识体系非常有帮助。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627482255,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":298945,"user_name":"ctf小白","can_delete":false,"product_type":"c1","uid":1544726,"ip_address":"","ucode":"C0326909867DE4","user_header":"https://static001.geekbang.org/account/avatar/00/17/92/16/558ff49c.jpg","comment_is_top":false,"comment_ctime":1624406134,"is_pvip":false,"replies":[{"id":"108503","content":"Hive on Spark也是Spark的应用场景之一，所以咱们后续的一些优化方法，对于Hive on Spark也是适用的哈~ 关于内存报错这部分，可以特别关注CPU和内存那几讲~<br><br>另外对于SQL部分，后续的Spark SQL调优也会对Hive on Spark有帮助的，可以关注下","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624525995,"ip_address":"","comment_id":298945,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1624406134","product_id":100073401,"comment_content":"老师，想请教下，现在主要是在hive数仓的场景下使用spark，现在经常出现hive on mr可以顺利执行，hive on spark 各种内存报错执行任务失败，想问下老师针对hive on spark的场景，有什么调优的建议?谢谢老师","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522285,"discussion_content":"Hive on Spark也是Spark的应用场景之一，所以咱们后续的一些优化方法，对于Hive on Spark也是适用的哈~ 关于内存报错这部分，可以特别关注CPU和内存那几讲~\n\n另外对于SQL部分，后续的Spark SQL调优也会对Hive on Spark有帮助的，可以关注下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624525995,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1051904,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","nickname":"果子","note":"","ucode":"0A669B6DE26F21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":381018,"discussion_content":"老师，请教一下hive on spark写hql和直接使用sparksql，写spark.sql(&#39;&#39;)有什么区别吗？性能，稳定性等有区别吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624869846,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294902,"user_name":"闯闯","can_delete":false,"product_type":"c1","uid":1544692,"ip_address":"","ucode":"D32958F8EF26BD","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83errHypG6kuO0V0bRwp74rm8srjoQ4zXUBNNLMcY19uNdz8Ea3rOFuBJibXMHWePMwBYpGsyyxiav0ibw/132","comment_is_top":false,"comment_ctime":1622126670,"is_pvip":false,"replies":[{"id":"107274","content":"很有趣的问题，不过有两个细节需要老弟进一步帮忙确认，一个是，Spark的资源调度器，看上去是Yarn？另外一个，是这张表mydb.mytable，请问它是Hive表，还是什么表？<br><br>之所以问这两个问题，主要是帮助判断，消耗Container Extra memory的到底是什么操作。目前，从代码上，spark.read.format(&quot;orc&quot;).load(path).mode(SaveMode.Overwrite).insertInto(&quot;mydb.mytable&quot;)，很难看出来到底是哪个环节消耗了Container部分的堆外内存，所以需要老弟提供更多的信息。<br><br>另外，还想确认的是，除了这行代码，真的没有其他操作了吗？因为这行代码涉及到的计算其实并不多，需要消耗内存的地方也并不多，完全就是纯Map，把数据从一个地方读进来，内存中展开，再存出去。说到这里，有另外一个细节待确认，就是这张表mydb.mytable，它底下的文件系统，和Spark集群，是同属一个硬件集群吗？或者换句话说，物理上是隔离的吗？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1622470343,"ip_address":"","comment_id":294902,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1622126670","product_id":100073401,"comment_content":"请教个问题：21.3 GB of 20 GB physical memory used. Consider boosting spark.executor.memoryOverhead 参数调整前后还是报这个错误<br>背景：100个400M的文件<br>代码：orc文件复制到parquet表<br>spark.read.format(&quot;orc&quot;).load(path).mode(SaveMode.Overwrite).insertInto(&quot;mydb.mytable&quot;)<br>配置：<br>spark.driver.cores=1<br>spark.driver.memory=2G<br>--num-executors 2<br>--executor-cores 1 调整前是2<br>spark.executor.memory=12G 调整前是6G<br>spark.executor.memoryOverhead=8G 调整前是3G<br>spark.memory.fraction=0.8<br>spark.files.maxPartitionBytes=256M<br>spark.default.parallelism=300 不行然后调到 600，调整600后，每隔Task input size是60~90M，输出100M，按理来说这个配置每Executor(1个cpu core)就一个同时运行Task，仅仅60M的输入，内存应该是足够的<br>spark.sql.shuffle.partitions=300 不行然后调到 600<br>spark.dynamicAllocation.enabled=true<br>spark.dynamicAllocation.maxExecutors=16<br>spark.sql.autoBroadcastJoinThreshold=-1<br>spark.sql.adaptive.shuffle.targetPostShuffleInputSize=256MB","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520862,"discussion_content":"很有趣的问题，不过有两个细节需要老弟进一步帮忙确认，一个是，Spark的资源调度器，看上去是Yarn？另外一个，是这张表mydb.mytable，请问它是Hive表，还是什么表？\n\n之所以问这两个问题，主要是帮助判断，消耗Container Extra memory的到底是什么操作。目前，从代码上，spark.read.format(&amp;quot;orc&amp;quot;).load(path).mode(SaveMode.Overwrite).insertInto(&amp;quot;mydb.mytable&amp;quot;)，很难看出来到底是哪个环节消耗了Container部分的堆外内存，所以需要老弟提供更多的信息。\n\n另外，还想确认的是，除了这行代码，真的没有其他操作了吗？因为这行代码涉及到的计算其实并不多，需要消耗内存的地方也并不多，完全就是纯Map，把数据从一个地方读进来，内存中展开，再存出去。说到这里，有另外一个细节待确认，就是这张表mydb.mytable，它底下的文件系统，和Spark集群，是同属一个硬件集群吗？或者换句话说，物理上是隔离的吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622470343,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1264318,"avatar":"https://static001.geekbang.org/account/avatar/00/13/4a/be/7168674c.jpg","nickname":"锐意进取","note":"","ucode":"D38C42B6F61279","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":395665,"discussion_content":"这个问题很常见．","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632322108,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1329566,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","nickname":"慢慢卢","note":"","ucode":"853D399100D83B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":379492,"discussion_content":"问题解决了吗\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623926997,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1544692,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83errHypG6kuO0V0bRwp74rm8srjoQ4zXUBNNLMcY19uNdz8Ea3rOFuBJibXMHWePMwBYpGsyyxiav0ibw/132","nickname":"闯闯","note":"","ucode":"D32958F8EF26BD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377127,"discussion_content":"谢谢老师回复。\nSpark资源调度是YARN，看了下yarn.resourcemanager.hostname的配置域名有aws前缀\n；文件系统使用S3，不清楚这个是不是物理分离。\nmydb.mytable是S3上的Hive表。\n需求是，原来的orc每日快照分区Hive表复制为parquet格式的每日快照分区Hive表。\n做法：driver端遍历每一个分区目录, 对column进行cast类型转换，然后insert新的Hive表","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622515992,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287548,"user_name":"Geek2014","can_delete":false,"product_type":"c1","uid":2028957,"ip_address":"","ucode":"9EB356D8DF287E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","comment_is_top":false,"comment_ctime":1618023917,"is_pvip":false,"replies":[{"id":"104430","content":"不是的哈，用不用堆外，和这个api没关系，这个api既可以支配堆外、也可以用来支配堆内。主要还是看off heap的配置项有没有打开，这个是关键～<br><br>和yarn的overhead也没有关系哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618055152,"ip_address":"","comment_id":287548,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1618023917","product_id":100073401,"comment_content":"老师你上一讲说tungsten是使用的Java unsafe api，这个时候就是使用的堆外内存吧？只是这时的堆外内存，不受spark.memory.offHeap.size的控制，而是spark.yarn.executor.memoryOverhead的控制？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518343,"discussion_content":"不是的哈，用不用堆外，和这个api没关系，这个api既可以支配堆外、也可以用来支配堆内。主要还是看off heap的配置项有没有打开，这个是关键～\n\n和yarn的overhead也没有关系哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618055152,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":563746,"discussion_content":"你理解的有误，spark3.0 对内存模型进行了更新。这个文章主要对的是spark3","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650072926,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028957,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","nickname":"Geek2014","note":"","ucode":"9EB356D8DF287E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368849,"discussion_content":"unsafe api只能使用堆外内存吧，老师你说的是tungsten可以使用堆内，也可以使用堆外吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618844689,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2028957,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","nickname":"Geek2014","note":"","ucode":"9EB356D8DF287E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377045,"discussion_content":"对的，Tungsten可以同时管理堆外和堆内内存，而且实际上，unsafe api也可以同时管理堆外和堆内内存，这部分细节可以参考Tungsten那一讲（23讲）哈~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622471916,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":368849,"ip_address":""},"score":377045,"extra":""}]}]},{"had_liked":false,"id":286495,"user_name":"Geek_2e6a7e","can_delete":false,"product_type":"c1","uid":2027323,"ip_address":"","ucode":"BCDD3367AC16FD","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIyPPFIyvytj0LJrpHicVrTqibuLWLWcR5VqzArSHZicwJYC6gKrIF6GTxx4MakS6xiaxZBCw8icCPB8wQ/132","comment_is_top":false,"comment_ctime":1617346298,"is_pvip":true,"replies":[{"id":"104089","content":"坦白说driver没什么可调的，oom无非就是把driver memory调高。不管是因为结果集、数据结构，还是你说的这些，除了增大内存，没太多调优手段。<br><br>另外，了解一下，你的并发有多高？调度开销都能把内存撑爆？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617430903,"ip_address":"","comment_id":286495,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1617346298","product_id":100073401,"comment_content":"老师描述了executor端的内存管理，但是在生产环境中，当并发高使用spark sql时，spark driver(常驻)端的OOM问题是比较严重的，因为spark sql解析、dag划分、调度开销还是比较大的？这块老师有碰到相关问题么？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518010,"discussion_content":"坦白说driver没什么可调的，oom无非就是把driver memory调高。不管是因为结果集、数据结构，还是你说的这些，除了增大内存，没太多调优手段。\n\n另外，了解一下，你的并发有多高？调度开销都能把内存撑爆？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617430903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028957,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","nickname":"Geek2014","note":"","ucode":"9EB356D8DF287E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366217,"discussion_content":"有一种可能是因为分片过多，小文件过多导致block元数据过多，进而占用了大量driver端内存","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617982274,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2027323,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIyPPFIyvytj0LJrpHicVrTqibuLWLWcR5VqzArSHZicwJYC6gKrIF6GTxx4MakS6xiaxZBCw8icCPB8wQ/132","nickname":"Geek_2e6a7e","note":"","ucode":"BCDD3367AC16FD","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":364979,"discussion_content":"吞吐高，即spark sql每秒处理的sql条数。压测过，spark 3.0 吞吐能到80，之前的版本基本只能达到20多。怎么发现的呢，有些sql处理非常快，基本不消耗executor资源，甚至是空表，反而导致driver端OOM。3.0后这块还是有优化的，比如事件处理队列由单队列改成根据事件类型（包括垃圾回收事件）划分为多队列。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617675513,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286484,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1617339238,"is_pvip":false,"replies":[{"id":"104091","content":"先说第一题，spark ui看不到硬件资源消耗，需要结合第三方工具比如ganglia，或者其他的工具。或者你如果可以登陆worker节点，用系统命令也可以。<br><br>第二题，spark其实不会自行判断数据结构复杂与否，堆外与堆内，如果堆外有空闲，他会优先选择堆外。需要注意的是，同一个task，不能脚踏两条船，不能同时用堆外和堆内，假设你最开始选择堆外，跑着跑着发现堆外不够用，这个时候，即使堆内还有空闲，task还是会oom。<br><br>第三题，你同学说的对，你这个操作，不算rdd复用，不需要cache，不要滥用cache哈～ 你需要好好看看前面的rdd和单机思维那几篇🙂","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617431794,"ip_address":"","comment_id":286484,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1617339238","product_id":100073401,"comment_content":"这一讲有好多实操中不理解的问题，请老师不吝赐教。<br><br>1 老师多次强调要确保硬件资源的平衡。但我们应该怎么查作业是否平衡呢？比如在Yarn上，我能看到我的job占用的总内存和总v-core（顺便问一下，一个v-core是一个CPU的核吗？），但我应该从哪个界面看出CPU是太空闲还是太忙了？<br><br>2 开启堆外内存，Spark会把复杂的schema放到堆外内存吗？比如我读parquet表生成一个几亿条的DF，对它做了一些collect_list操作，那它会把简单的数据结构放堆里而把复杂的list放到堆外吗，要是反过来怎么办，岂不是更糟？<br><br>3 自己写了一个函数，传入一个sparkSession，返回一个DF，我在外面用个val变量去接，请问在函数返回的最后一步要cache一下吗？因为我觉得这算是两个变量复用一个RDD了，应该要cache()。但我同事说不用cache的，我俩究竟谁对？麻烦老师都回答一下吧~","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518003,"discussion_content":"先说第一题，spark ui看不到硬件资源消耗，需要结合第三方工具比如ganglia，或者其他的工具。或者你如果可以登陆worker节点，用系统命令也可以。\n\n第二题，spark其实不会自行判断数据结构复杂与否，堆外与堆内，如果堆外有空闲，他会优先选择堆外。需要注意的是，同一个task，不能脚踏两条船，不能同时用堆外和堆内，假设你最开始选择堆外，跑着跑着发现堆外不够用，这个时候，即使堆内还有空闲，task还是会oom。\n\n第三题，你同学说的对，你这个操作，不算rdd复用，不需要cache，不要滥用cache哈～ 你需要好好看看前面的rdd和单机思维那几篇🙂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617431794,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1789481,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg","nickname":"静心","note":"","ucode":"B80DE4B5C923D3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369101,"discussion_content":"老师，您回答的第二题有些疑问，还请您指导一下。\n例如：spark executor如果配置了堆内和堆外各4GB，executor cores配置为2。\n那么该executor运行的第一个task只会使用堆外内存？调度来的第二个task，哪怕堆外剩余几十MB，它也会用堆外内存，除非第二个task发现堆外不够用，才会使用堆内内存？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618929217,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}