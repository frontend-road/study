{"id":718402,"title":"20｜部署一个鲜花网络电商的人脉工具（上）","content":"<p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>从今天开始，我要用4节课的篇幅，带着你设计两个有趣而又实用的应用程序。设计这两个应用程序的目的，是为了让你能够把LangChain中的各个组件灵活地组合起来，融会贯通，并以此作为启发，在你熟悉的业务场景中，利用LangChain和LLM的能力，开发出更多、更强大的效率工具。</p><p>第一个应用程序，是用LangChain创建出一个专属于“易速鲜花”的网络人脉工具。光这么说，有些模糊，这个人脉工具长啥样？有些啥具体功能？</p><p>动手之前，让我先给你把这个所谓“人脉”工具的能力和细节说清楚。</p><h2>“人脉工具”项目说明</h2><p><strong>项目背景</strong>：易速鲜花电商网络自从创建以来，通过微信、抖音、小红书等自媒体宣传推广，短期内获得了广泛流量展示。目前，营销部门希望以此为契机，再接再厉，继续扩大品牌影响力。经过调研，发现很多用户会通过微博热搜推荐的新闻来购买鲜花赠送给明星、达人等，因此各部门一致认为应该联络相关微博大V，共同推广，带动品牌成长。</p><p>然而，发掘并选择适合于“鲜花推广”的微博大V有一定难度。营销部门员工表示，这个任务比找微信、抖音和小红书达人要难得多。他们都希望技术部门能够给出一个“人脉搜索工具”来协助完成这一目标。</p><!-- [[[read_end]]] --><p><strong>项目目标：</strong>帮助市场营销部门的员工找到微博上适合做鲜花推广的大V，并给出具体的联络方案。</p><h2>项目的技术实现细节</h2><p>这个项目的具体技术实现细节，这里简述如下。</p><p><strong>第一步：</strong>通过LangChain的搜索工具，以模糊搜索的方式，帮助运营人员找到微博中有可能对相关鲜花推广感兴趣的大V（比如喜欢玫瑰花的大V），并返回UID。</p><p><strong>第二步：</strong>根据微博UID，通过爬虫工具拿到相关大V的微博公开信息，并以JSON格式返回大V的数据。</p><p><img src=\"https://static001.geekbang.org/resource/image/00/5f/0049810d3cfe1aee633d29722ded8e5f.png?wh=1860x1612\" alt=\"\" title=\"找到热爱鲜花的微博大 V 谋求合作\"></p><p><strong>第三步：</strong>通过LangChain调用LLM，通过LLM的总结整理以及生成功能，根据大V的个人信息，写一篇热情洋溢的介绍型文章，谋求与该大V的合作。</p><p><strong>第四步：</strong>把LangChain输出解析功能加入进来，让LLM生成可以嵌入提示模板的格式化数据结构。</p><p><strong>第五步：</strong>添加HTML、CSS，并用Flask创建一个App，在网络上部署及发布这个鲜花电商人脉工具，供市场营销部门的人员使用。</p><p>在上面的5个步骤中，我们使用到很多LangChain技术，包括<strong>提示工程、模型、链、代理、输出解析</strong>等。</p><p>这节课我们先来实现项目的前两个部分。</p><h2>第一步：找到大 V</h2><p>因为咱们的项目需要用到很多工具，所以我创建了一个项目目录，叫做socializer_v0（项目每完成一步，我就创建一个新目录，并把版本号加1）。当第一个步骤“找到大 V”实现之后，项目中的文档结构如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/5d/0c/5dab492f802d34086975616d06708e0c.jpg?wh=152x270\" alt=\"\"></p><p>这里，主程序是findbigV.py。意思就是派程序来作为智能代理，找到喜欢鲜花的微博大V。</p><h2>主程序 findbigV.py</h2><p>主程序findbigV.py在第一步完成之后，是这样的。</p><pre><code class=\"language-plain\"># 设置OpenAI API密钥\nimport os\nos.environ[\"OPENAI_API_KEY\"] = ''\nos.environ[\"SERPAPI_API_KEY\"] = ''\n\n# 导入所取的库\nimport re\nfrom agents.weibo_agent import lookup_V\n\nif __name__ == \"__main__\":\n\n&nbsp; &nbsp; # 拿到UID\n&nbsp; &nbsp; response_UID = lookup_V(flower_type = \"牡丹\" )\n&nbsp; &nbsp; print(response_UID)\n\n&nbsp; &nbsp; # 抽取UID里面的数字\n&nbsp; &nbsp; UID = re.findall(r'\\d+', response_UID)[0]\n&nbsp; &nbsp; print(\"这位鲜花大V的微博ID是\", UID)\n</code></pre><p>这里，我们要搜到的，是一个热爱鲜花的大V的微博UID，而不是URL。</p><p><img src=\"https://static001.geekbang.org/resource/image/52/f1/520688ef98a70c3d3651420bcc26bef1.jpg?wh=1805x1292\" alt=\"\"></p><p>比如，上面这位喜欢牡丹花的大V，他的UID是6053338099。这些都是公开的信息。</p><p>为什么我们希望得到UID呢？因为我们可以通过这个ID，爬取他个人主页里的更多介绍信息，有利于进一步了解他。</p><h3>微博 Agent：查找大 V 的 ID</h3><p>下面，我们就来看看，文件agents\\weibo_agent.py中的lookup_V函数是如何实现这个搜寻UID的功能的。</p><pre><code class=\"language-plain\"># 导入一个搜索UID的工具\nfrom tools.search_tool import get_UID\n\n# 导入所需的库\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\n\n# 通过LangChain代理找到UID的函数\ndef lookup_V(flower_type: str) :\n&nbsp; &nbsp; # 初始化大模型\n&nbsp; &nbsp; llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n\n&nbsp; &nbsp; # 寻找UID的模板\n&nbsp; &nbsp; template = \"\"\"given the {flower} I want you to get a related 微博 UID.\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Your answer should contain only a UID.\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The URL always starts with https://weibo.com/u/\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for example, if https://weibo.com/u/1669879400 is her 微博, then 1669879400 is her UID\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; This is only the example don't give me this, but the actual UID\"\"\"\n&nbsp; &nbsp; # 完整的提示模板\n&nbsp; &nbsp; prompt_template = PromptTemplate(\n&nbsp; &nbsp; &nbsp; &nbsp; input_variables=[\"flower\"], template=template\n&nbsp; &nbsp; )\n\n&nbsp; &nbsp; # 代理的工具\n&nbsp; &nbsp; tools = [\n&nbsp; &nbsp; &nbsp; &nbsp; Tool(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; name=\"Crawl Google for 微博 page\",\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; func=get_UID,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; description=\"useful for when you need get the 微博 UID\",\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; ]\n\n&nbsp; &nbsp; # 初始化代理\n&nbsp; &nbsp; agent = initialize_agent(\n&nbsp; &nbsp; &nbsp; &nbsp; tools, \n&nbsp; &nbsp; &nbsp; &nbsp; llm, \n&nbsp; &nbsp; &nbsp; &nbsp; agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n&nbsp; &nbsp; &nbsp; &nbsp; verbose=True\n&nbsp; &nbsp; )\n\n&nbsp; &nbsp; # 返回找到的UID\n&nbsp; &nbsp; ID = agent.run(prompt_template.format_prompt(flower=flower_type))\n\n&nbsp; &nbsp; return ID\n</code></pre><p>这段代码的目的，是为了通过提供的花的类型（flower type）来查找与之相关的微博UID。其中使用了LangChain中的代理和工具。</p><p>这里有两点需要特别说明：</p><ol>\n<li>搜索UID的工具通过from tools.search_tool import get_UID导入，这个内容后面还会介绍。</li>\n<li>下面的提示模板说明，强调了需要的是UID，而不是URL。刚才说了，这是因为后续的爬虫工具需要一个特定的UID，来获取该微博大V的个人信息（公开）。然后我们会继续利用这些信息让LLM为我们写“勾搭”文案。</li>\n</ol><pre><code class=\"language-plain\">&nbsp; &nbsp; # 寻找UID的模板\n&nbsp; &nbsp; template = \"\"\"given the {flower} I want you to get a related 微博 UID.\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Your answer should contain only a UID.\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The URL always starts with https://weibo.com/u/\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for example, if https://weibo.com/u/1669879400 is her 微博, then 1669879400 is her UID\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; This is only the example don't give me this, but the actual UID\"\"\"\n</code></pre><h3>定制的 SerpAPI：getUID</h3><p>上面的程序只是调用了代理，但是没有给出具体的工具实现。现在我们来继续实现搜索大V的UID的功能。</p><pre><code class=\"language-plain\"># 导入一个搜索UID的工具\nfrom tools.search_tool import get_UID\n</code></pre><p>这个具体的实现，在代码 \\tools\\search_tool.py 中。</p><p>说到通过LangChain来搜索微博，相信你会马上想到已经多次使用过的SerpAPI。我们先来试一试标准的SerpAPI，看看它能否满足我们的需求。</p><pre><code class=\"language-plain\">from langchain.utilities import SerpAPIWrapper\n\ndef get_UID(flower: str):\n&nbsp; &nbsp; \"\"\"Searches for Linkedin or twitter Profile Page.\"\"\"\n&nbsp; &nbsp; search = SerpAPIWrapper()\n&nbsp; &nbsp; res = search.run(f\"{flower}\")\n&nbsp; &nbsp; return res\n</code></pre><p>写好了这段代码，第一步就可以说是完成了。下面我们跑一遍findbigV.py，看看程序会给出我们什么样的结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/d8/a22043515a4b9686c58ccedcda2075d8.jpg?wh=1336x670\" alt=\"\"></p><p>结果还好，不算太失望，SerpAPI找到了一个貌似喜欢牡丹花的大V，名叫戏精牡丹，搜到的信息也都是真实的。看起来他蛮适合为我们的牡丹花代言。然而，这个大V的微博ID肯定不是6。</p><p>中间哪里或许是出了点小问题。</p><p>像这样的错误，明显发生在LangChain内部，那你的 trouble_shooting 也只能通过Debug来解决。这里，我就忽略掉一长串的错误排查过程，直接指出问题的根本原因所在。</p><p>让我们把断点设置在SerpAPIWrapper类的_process_response中。</p><p><img src=\"https://static001.geekbang.org/resource/image/86/88/86238bae7452cde52f23e4d6ea3a1688.jpg?wh=1909x1398\" alt=\"\"></p><p>当程序进入 <code> if \"organic_results\" in res.keys()</code> 这段逻辑之后，我发现，它返回的总是一个snippet（摘要文字），而不是link（URL）。</p><p><img src=\"https://static001.geekbang.org/resource/image/38/be/38643a041b2f24ed9e8405a485580cbe.jpg?wh=1275x1473\" alt=\"\"></p><p>无论这背后的逻辑何在，这并不是我们所想要的。在Debug过程中，我们发现，新浪微博的UID，实际上包含在URL中，也就是 <a href=\"https://weibo.com/u/6053338099\">https://weibo.com/u/6053338099</a>。因此，如果我们不返回微博的简短说明（戏精牡丹，搞笑视频自媒体……），而是返回URL，会更有利于大模型提炼出UID。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/c0/2a78e4b1cc0734f1c5ce171a05f153c0.jpg?wh=1275x465\" alt=\"\"></p><p>如何做呢？直接修改LangChain的SerpAPIWrapper类的_process_response源代码肯定不是一个好办法。</p><p>因此，这里我们可以继承SerpAPIWrapper类，并构造一个CustomSerpAPIWrapper类，在这个类中，我们重构_process_response这个静态方法。</p><p>新的search_tool.py完整代码如下：</p><pre><code class=\"language-plain\"># 导入SerpAPIWrapper\nfrom langchain.utilities import SerpAPIWrapper\n\n# 重新定制SerpAPIWrapper，重构_process_response，返回URL\nclass CustomSerpAPIWrapper(SerpAPIWrapper):\n&nbsp; &nbsp; def __init__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; super(CustomSerpAPIWrapper, self).__init__()\n\n&nbsp; &nbsp; @staticmethod\n&nbsp; &nbsp; def _process_response(res: dict) -&gt; str:\n&nbsp; &nbsp; &nbsp; &nbsp; \"\"\"Process response from SerpAPI.\"\"\"\n&nbsp; &nbsp; &nbsp; &nbsp; if \"error\" in res.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; raise ValueError(f\"Got error from SerpAPI: {res['error']}\")\n&nbsp; &nbsp; &nbsp; &nbsp; if \"answer_box_list\" in res.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; res[\"answer_box\"] = res[\"answer_box_list\"]\n        '''删去很多无关代码'''\n&nbsp; &nbsp; &nbsp; &nbsp; snippets = []\n&nbsp; &nbsp; &nbsp; &nbsp; if \"knowledge_graph\" in res.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; knowledge_graph = res[\"knowledge_graph\"]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; title = knowledge_graph[\"title\"] if \"title\" in knowledge_graph else \"\"\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if \"description\" in knowledge_graph.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(knowledge_graph[\"description\"])\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for key, value in knowledge_graph.items():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; isinstance(key, str)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; and isinstance(value, str)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; and key not in [\"title\", \"description\"]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; and not key.endswith(\"_stick\")\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; and not key.endswith(\"_link\")\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; and not value.startswith(\"http\")\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(f\"{title} {key}: {value}.\")\n&nbsp; &nbsp; &nbsp; &nbsp; if \"organic_results\" in res.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; first_organic_result = res[\"organic_results\"][0]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if \"snippet\" in first_organic_result.keys():\n                # 此处是关键修改\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # snippets.append(first_organic_result[\"snippet\"])\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(first_organic_result[\"link\"]) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif \"snippet_highlighted_words\" in first_organic_result.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(first_organic_result[\"snippet_highlighted_words\"])\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif \"rich_snippet\" in first_organic_result.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(first_organic_result[\"rich_snippet\"])\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif \"rich_snippet_table\" in first_organic_result.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(first_organic_result[\"rich_snippet_table\"])\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elif \"link\" in first_organic_result.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(first_organic_result[\"link\"])\n&nbsp; &nbsp; &nbsp; &nbsp; if \"buying_guide\" in res.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(res[\"buying_guide\"])\n&nbsp; &nbsp; &nbsp; &nbsp; if \"local_results\" in res.keys() and \"places\" in res[\"local_results\"].keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(res[\"local_results\"][\"places\"])\n\n&nbsp; &nbsp; &nbsp; &nbsp; if len(snippets) &gt; 0:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return str(snippets)\n&nbsp; &nbsp; &nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return \"No good search result found\"\n\n# 获取与某种鲜花相关的微博UID的函数\ndef get_UID(flower: str):\n&nbsp; &nbsp; \"\"\"Searches for Linkedin or twitter Profile Page.\"\"\"\n&nbsp; &nbsp; # search = SerpAPIWrapper()\n&nbsp; &nbsp; search = CustomSerpAPIWrapper()\n&nbsp; &nbsp; res = search.run(f\"{flower}\")\n&nbsp; &nbsp; return res\n</code></pre><p>唯一的区别就是，我们在下面的逻辑中返回了link，而不是snippet。</p><pre><code class=\"language-plain\">&nbsp; &nbsp; &nbsp; &nbsp; if \"organic_results\" in res.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; first_organic_result = res[\"organic_results\"][0]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if \"snippet\" in first_organic_result.keys():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # snippets.append(first_organic_result[\"snippet\"])\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snippets.append(first_organic_result[\"link\"]) \n</code></pre><p>再次Debug，我们发现返回的snippets里面包含了URL信息，其中UID信息包含在URL中了。</p><p><img src=\"https://static001.geekbang.org/resource/image/06/5e/06da407d1f6d93eeaefa6e9f450cdf5e.jpg?wh=1837x1631\" alt=\"\"></p><p>此时运行主程序findbigV.py，会发现代理中返回了URL信息，并且经过进一步思考，提炼出了UID。</p><p><img src=\"https://static001.geekbang.org/resource/image/ab/6f/ab67de9be2b4f26be53c0e8af713f16f.jpg?wh=819x326\" alt=\"\" title=\"成功得到了大 V 的 UID\"></p><h2>第二步：爬取大 V 资料</h2><p>好的，第一步虽然是有磕有绊，但是经过了调整的CustomSerpAPIWrapper工具和代理，在LLM的帮助之下，总算是不辱使命，完成了找到UID的任务。</p><p>这位大V，看起来又喜欢牡丹，又喜欢搞笑。我们很想和他联络一下，也许他很适合为我们的牡丹花品牌代言。（到底是否适合，不必特别认真哈，总之搜索“牡丹”，Agent给了这个ID，就可以了。咱学的是LangChain，不是真的要找他代言）</p><p>不过，知己知彼，百战不殆。想要和他沟通，就得了解他更多。下面，我们将使用爬虫程序，通过UID来爬取他的更多信息。</p><h3>主程序 findbigV.py</h3><p>第二步完成之后，主程序代码如下：</p><pre><code class=\"language-plain\"># 设置OpenAI API密钥\nimport os\nos.environ[\"OPENAI_API_KEY\"] = 'Your OpenAI API Key'\nos.environ[\"SERPAPI_API_KEY\"] = 'Your SerpAPI Key'\n\n# 导入所取的库\nimport re\nfrom agents.weibo_agent import lookup_V\nfrom tools.general_tool import remove_non_chinese_fields\nfrom tools.scraping_tool import get_data\n\nif __name__ == \"__main__\":\n\n&nbsp; &nbsp; # 拿到UID\n&nbsp; &nbsp; response_UID = lookup_V(flower_type = \"牡丹\" )\n\n&nbsp; &nbsp; # 抽取UID里面的数字\n&nbsp; &nbsp; UID = re.findall(r'\\d+', response_UID)[0]\n&nbsp; &nbsp; print(\"这位鲜花大V的微博ID是\", UID)\n\n&nbsp; &nbsp; # 根据UID爬取大V信息\n&nbsp; &nbsp; person_info = get_data(UID)\n&nbsp; &nbsp; print(person_info)\n</code></pre><p>从第一步到第二步，我们主要是完成了一次微博信息的爬取。</p><h3>scraping_tool.py 中的 scrape_weibo 方法</h3><p>第二步中的关键逻辑是scraping_tool.py中的scrape_weibo方法，具体代码如下：</p><pre><code class=\"language-plain\"># 导入所需的库\nimport json\nimport requests\nimport time\n\n# 定义爬取微博用户信息的函数\ndef scrape_weibo(url: str):\n&nbsp; &nbsp; '''爬取相关鲜花服务商的资料'''\n&nbsp; &nbsp; headers = {\n&nbsp; &nbsp; &nbsp; &nbsp; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"Referer\": \"https://weibo.com\"\n&nbsp; &nbsp; }\n&nbsp; &nbsp; cookies = {\n&nbsp; &nbsp; &nbsp; &nbsp; \"cookie\": '''SINAGLOBAL=3762226753815.13.1696496172299; ALF=1699182321; SCF=AiOo8xtPwGonZcAbYyHXZbz9ixm97mWi0vHt_VvuOKB-u4-rcvlGtWCrE6MfMucpxiOy5bYpkIFNWTj7nYGcyp4.; _sc_token=v2%3A2qyeqD3cTZFNTl0sn3KAYe4fNqzMUEP-C7nxNsd_Q1r-vpYMlF2K3xc4vWNuLNBbp3RsohghkJdlSVN09cymVo5AKAm0V92004V8cSRe9O5v9B65jd4yiG_sATDeB06GnjiJulXUrEF_6XsHh1ozK6jvbTKEUIkF7v0_BlbX6IcWrPkwh6xL_WM_0YUV2v7CtNPwyxfbAjaWnG32TsxG_ftN3s5m7qfaRftU6iTOSnE%3D; XSRF-TOKEN=4o0E6jaUQ0BlN77az0sURTg3; PC_TOKEN=dcf0e7607f; login_sid_t=36ebf31f1b3694fb71e77e35d30f052f; cross_origin_proto=SSL; WBStorage=4d96c54e|undefined; _s_tentry=passport.weibo.com; UOR=www.google.com,weibo.com,login.sina.com.cn; Apache=7563213131783.361.1696667509205; ULV=1696667509207:2:2:2:7563213131783.361.1696667509205:1696496172302; wb_view_log=3440*14401; WBtopGlobal_register_version=2023100716; crossidccode=CODE-gz-1QP2Jh-13l47h-79FGqrAQgQbR8ccb7b504; SSOLoginState=1696667553; SUB=_2A25IJWfwDeThGeFJ6lsQ-SbNzjuIHXVr5gm4rDV8PUJbkNAbLUWtkW1NfJd_XHamKIzj5RlT_-RGMma6z3YQZUK3; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFDKvBlvg14YuHk_4c6MEH_5NHD95QNS024eK.ReK-NWs4DqcjZCJ8oIN.pSKzceBtt; WBPSESS=gyY2mn77F4p5VxWF2IB_yFR0phHVTNfaJAHAMprnW7MeUr-NHPZNyeeyKae3tHELlc_RbcI1XPSz-TjSJqWrIXs-yh1fwhxL4mSDrnpPZEogFt8ScF5NEwSqPGn7x2KMAgTHtWde-3MBm6orQ98PDA=='''\n&nbsp; &nbsp; }\n&nbsp; &nbsp; response = requests.get(url, headers=headers, cookies=cookies)\n&nbsp; &nbsp; time.sleep(3) &nbsp; # 加上3s 的延时防止被反爬\n&nbsp; &nbsp; return response.text\n\n# 根据UID构建URL爬取信息\ndef get_data(id):\n&nbsp; &nbsp; url = \"https://weibo.com/ajax/profile/detail?uid={}\".format(id)\n&nbsp; &nbsp; html = scrape_weibo(url)\n&nbsp; &nbsp; response = json.loads(html)\n\n&nbsp; &nbsp; return response\n</code></pre><p>我这段爬虫代码特别简洁，不需要过多的解释，唯一需要说明的部分是怎么找到你自己的Cookies。</p><blockquote>\n<p><span class=\"reference\">Cookie 是由服务器发送到用户浏览器的一小段数据，并可能在随后的请求中被回传。它的主要目的是让服务器知道用户的上下文信息或状态。在Web爬虫中，使用正确的Cookie可以模拟登录状态，从而获取到需要权限的网页内容。</span></p>\n</blockquote><p>首先，我是用QQ ID登录的微博，我发现通过这样的方式找到的Cookie能用得比较久。</p><p>然后，从我的浏览器中获取 Cookie，以下是简单步骤：</p><ol>\n<li>使用浏览器（如 Chrome、Firefox）访问微博并登录。</li>\n<li>登录后，右键单击页面并选择“检查”（Inspect）。</li>\n<li>打开开发者工具，点击 Network 选项卡。</li>\n<li>在页面上进行一些操作（如刷新页面），然后在 Network 选项卡下查看请求列表。</li>\n<li>选择任一请求项，然后在右侧的 Headers 选项卡中查找 Request Headers 部分。</li>\n<li>在这部分中，你应该可以看到一个名为 Cookie 的字段，这就是你需要的 Cookie 值。</li>\n</ol><p>将获取到的完整Cookie值复制（挺长的），并替换上述代码中的 <code>\"你的Cookie\"</code> 部分。</p><p><img src=\"https://static001.geekbang.org/resource/image/92/38/92ea6832ea69c8a1342a62180b7da538.jpg?wh=3842x1944\" alt=\"\"></p><blockquote>\n<p><span class=\"reference\">但请注意，微博的Cookie可能有过期时间，所以如果你发现一段时间后你的爬虫无法正常工作，你可能需要再次获取新的Cookie。同时，频繁地爬取或大量请求可能会导致你的账号被封禁，所以请谨慎使用爬虫。</span></p>\n</blockquote><p>此时，运行 findbigV.py，就得到了下面的输出。</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/94/e696b6dae6332a486763e29e9f310594.jpg?wh=1318x665\" alt=\"\"></p><h3>精简爬取输出</h3><p>最后一个步骤，是精简上面的输出，因为类似 <code>'word_color': '#FFEA8011', 'background_color': '#FF181818'</code> 这样的内容会占据很多Token空间，而且对于LLM总结整理信息，也没啥作用。</p><p>因此，我创建了一个额外的步骤，就是\\tools\\general_tool.py中的remove_non_chinese_fields函数。</p><pre><code class=\"language-plain\">import re\n\ndef contains_chinese(s):\n&nbsp; &nbsp; return bool(re.search('[\\u4e00-\\u9fa5]', s))\n\ndef remove_non_chinese_fields(d):\n&nbsp; &nbsp; if isinstance(d, dict):\n&nbsp; &nbsp; &nbsp; &nbsp; to_remove = [key for key, value in d.items() if isinstance(value, (str, int, float, bool)) and (not contains_chinese(str(value)))]\n&nbsp; &nbsp; &nbsp; &nbsp; for key in to_remove:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; del d[key]\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; for key, value in d.items():\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if isinstance(value, (dict, list)):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; remove_non_chinese_fields(value)\n&nbsp; &nbsp; elif isinstance(d, list):\n&nbsp; &nbsp; &nbsp; &nbsp; to_remove_indices = []\n&nbsp; &nbsp; &nbsp; &nbsp; for i, item in enumerate(d):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if isinstance(item, (str, int, float, bool)) and (not contains_chinese(str(item))):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; to_remove_indices.append(i)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; remove_non_chinese_fields(item)\n&nbsp; &nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; &nbsp; for index in reversed(to_remove_indices):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; d.pop(index)\n</code></pre><p>在findbigV.py中，调用这个函数，对爬虫的输出结果进行了精简。</p><pre><code class=\"language-plain\">&nbsp; &nbsp; # 移除无用的信息\n&nbsp; &nbsp; remove_non_chinese_fields(person_info)\n&nbsp; &nbsp; print(person_info)\n</code></pre><p>重新运行findbigV.py，结果如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/16/10/16aaa4d952428bb1960ecfee9df6df10.jpg?wh=1310x217\" alt=\"\"></p><p>此时，爬取的内容就只剩下了干货。</p><h2>总结时刻</h2><p>这节课我们完成了前两步的工作。分别是，找到适合推广某种鲜花的大V的微博UID，并且爬取了大V的资料。这为我们后续生成文本、进一步链接大V打下了良好的基础。</p><p>其中，我们用到了大量之前学习过的LangChain组件，具体包括：</p><ol>\n<li>用提示模板告诉大模型我们要找到内容（UID）。</li>\n<li>调用LLM。</li>\n<li>使用Chain。</li>\n<li>使用Agent。</li>\n<li>在Agent中，我们使用了一个Customized Tool，因为LangChain内置的SerpAPI Tool不能完全满足我们的需要。这给了我们一个好机会创建自己的“私人定制” Tool。</li>\n</ol><p>在下节课中，我们还要继续利用大模型的总结文本、生成文本的功能，来为我们撰写能够打动大V和咱易速鲜花合作的文案，我们还将利用Output Parser把文案解析成需要的格式，部署到网络服务器端。敬请期待！</p><h2>思考题</h2><ol>\n<li>如果Agent不返回UID，而是返回URL，是不是也能够完成这个任务？你可以尝试重构提示模板以及后续逻辑，返回URL，然后手动从URL中解析出UID。</li>\n<li>研究一下SerpAPIWrapper类的_process_response中的代码，看看这个方法具体是怎么设计的，用来实现了什么功能？</li>\n</ol><p>期待在留言区看到你的分享，如果觉得内容对你有帮助，也欢迎分享给有需要的朋友！</p>","comments":[{"had_liked":false,"id":386085,"user_name":"马里奥的马里奥","can_delete":false,"product_type":"c1","uid":1048423,"ip_address":"江苏","ucode":"6848D9E82B58E3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ff/67/6ffe3a52.jpg","comment_is_top":false,"comment_ctime":1704029662,"is_pvip":false,"replies":[{"id":140825,"content":"嗯嗯，谢谢同学的反馈。我这个是一个启发性的示例。并非落地产品。\n这就是典型的大模型开发过程中的结果不稳定的问题。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1704774754,"ip_address":"瑞士","comment_id":386085,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"2个问题\n1.虽然修改了获取link的方式，依然是会有错误，在我这里解析到的是 https:&#47;&#47;weibo.com&#47;p&#47;xxx后的这个xx数字；\n2.我尝试修改提示词，只搜索粉丝超过一百万的账号，似乎也不生效。","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635374,"discussion_content":"嗯嗯，谢谢同学的反馈。我这个是一个启发性的示例。并非落地产品。\n这就是典型的大模型开发过程中的结果不稳定的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704774754,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1469820,"avatar":"https://static001.geekbang.org/account/avatar/00/16/6d/7c/e91866cf.jpg","nickname":"aloha66","note":"","ucode":"60AF4685BF38A2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641376,"discussion_content":"所以有解决方案吗？这种不稳定的情况。我这里十条只有一条是正确的地址","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712573690,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384777,"user_name":"Liberalism","can_delete":false,"product_type":"c1","uid":1233947,"ip_address":"北京","ucode":"BD0A293B928668","user_header":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","comment_is_top":false,"comment_ctime":1701505744,"is_pvip":false,"replies":[{"id":140917,"content":"解决了么？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705069176,"ip_address":"瑞士","comment_id":384777,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"在爬取微博用户资料时报 400 错误，请问是哪里出了问题？？？？\n\ndef scrape_weibo(url: str):\n    &quot;&quot;&quot;爬取相关鲜花服务商的资料&quot;&quot;&quot;\n    headers = {\n        &quot;User-Agent&quot;: &quot;Mozilla&#47;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#47;537.36 (KHTML, like Gecko) &quot;\n                      &quot;Chrome&#47;89.0.4389.82 Safari&#47;537.36&quot;,\n        &quot;Referer&quot;: &quot;https:&#47;&#47;weibo.com&quot;}\n    cookies = {\n        &quot;cookie&quot;: &#39;&#39;&#39;SINAGLOBAL=6620500101466.929.1684124696145; \n        ULV=1701318517344:5:1:1:2994044278409.513.1701318517342:1695626334231; \n        SUB=_2A25IbH_LDeRhGedJ71EX9C3Kwz6IHXVrAP0DrDV8PUNbmtAGLWP8kW9NVhncgwwKDAJC-BEIEyWCj9aAZiYRqGGn; \n        SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WF1hJoCEpSf.bqAJzfD-UMO5JpX5KzhUgL.Fo2NShecShec1hz2dJLoI7peIgiLMJ8jIPS_qgRt; \n        ALF=1732854554; XSRF-TOKEN=Dr6hVUuVr5JfuRn3NHntHjk7; \n        WBPSESS=qWBqMfNAAnlnO3TmyuUMG1qvHg86Rz2Zv7YHVRzN1MJAsJSyBhxx0_AUvWsjCUhaTWJpEmAC3UJ6u4OKi_zznpkRnRQ8ciUPXXDldaaw58i1fvBUm_1oDKnjo6sGr9qeK-zdbnLVltKplYXJysV88A==&#39;&#39;&#39;\n    }\n    response = requests.get(url, headers=headers, cookies=cookies)\n\n    time.sleep(3)  # 加上3s 的延时防止被反爬\n    return response.text","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635723,"discussion_content":"解决了么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705069176,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384743,"user_name":"Liberalism","can_delete":false,"product_type":"c1","uid":1233947,"ip_address":"北京","ucode":"BD0A293B928668","user_header":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","comment_is_top":false,"comment_ctime":1701422532,"is_pvip":false,"replies":[{"id":140916,"content":"是么？同学这个问题后来解决了么？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705069164,"ip_address":"瑞士","comment_id":384743,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"def scrape_weibo(url: str):\n    &quot;&quot;&quot;爬取相关鲜花服务商的资料&quot;&quot;&quot;\n    headers = {\n        &#39;Accept&#39;: &#39;text&#47;html,application&#47;xhtml+xml,application&#47;xml;q=0.9,image&#47;webp,*&#47;*;q=0.8&#39;,\n        &#39;Cache-Control&#39;: &#39;max-age=0&#39;,\n        &#39;Connection&#39;: &#39;keep-alive&#39;,\n        &#39;Referer&#39;: &#39;http:&#47;&#47;www.baidu.com&#47;&#39;,\n        &#39;User-Agent&#39;: &#39;Mozilla&#47;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#47;537.36 (KHTML, like Gecko)&#39;\n    }\n    cookies = {\n        &quot;cookie&quot;: &#39;&#39;&#39;SINAGLOBAL=6620500101466.929.1684124696145; \n        ULV=1701318517344:5:1:1:2994044278409.513.1701318517342:1695626334231; \n        SUB=_2A25IbH_LDeRhGedJ71EX9C3Kwz6IHXVrAP0DrDV8PUNbmtAGLWP8kW9NVhncgwwKDAJC-BEIEyWCj9aAZiYRqGGn; \n        SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WF1hJoCEpSf.bqAJzfD-UMO5JpX5KzhUgL.Fo2NShecShec1hz2dJLoI7peIgiLMJ8jIPS_qgRt; \n        ALF=1732854554; XSRF-TOKEN=Dr6hVUuVr5JfuRn3NHntHjk7; \n        WBPSESS=qWBqMfNAAnlnO3TmyuUMG1qvHg86Rz2Zv7YHVRzN1MJAsJSyBhxx0_AUvWsjCUhaTWJpEmAC3UJ6u4OKi_zznpkRnRQ8ciUPXXDldaaw58i1fvBUm_1oDKnjo6sGr9qeK-zdbnLVltKplYXJysV88A==&#39;&#39;&#39;\n    }\n    response = requests.get(url, headers=headers, cookies=cookies)\n\n    time.sleep(3)  # 加上3s 的延时防止被反爬\n    return response.text\n\n\n爬取数据这里一直报错  400 ","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635722,"discussion_content":"是么？同学这个问题后来解决了么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705069164,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384679,"user_name":"fireshort","can_delete":false,"product_type":"c1","uid":1008312,"ip_address":"广东","ucode":"10550CA9C6C730","user_header":"https://static001.geekbang.org/account/avatar/00/0f/62/b8/0e1b655e.jpg","comment_is_top":false,"comment_ctime":1701332579,"is_pvip":false,"replies":[{"id":140921,"content":"主要展示的是模型主动去调用工具的功能。可以考虑如何加入更多自主判断的元素。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705070543,"ip_address":"瑞士","comment_id":384679,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"黄老师，这里的Agent感觉没有发挥作用，没有太多智能，LLM就加了“微博”去搜索。\nID = agent.run(prompt_template.format_prompt(flower=flower_type))\n改成\nID = get_UID(flower_type+&quot; 微博&quot;)\n得到一样的结果。\n\n\n","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635727,"discussion_content":"主要展示的是模型主动去调用工具的功能。可以考虑如何加入更多自主判断的元素。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705070543,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1172679,"avatar":"https://static001.geekbang.org/account/avatar/00/11/e4/c7/7f8be879.jpg","nickname":"山哥","note":"","ucode":"7C8585B021CAC4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635277,"discussion_content":"框架boy","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704689262,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"福建","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383248,"user_name":"starj","can_delete":false,"product_type":"c1","uid":1108791,"ip_address":"北京","ucode":"3546E42F1340B2","user_header":"https://static001.geekbang.org/account/avatar/00/10/eb/37/a2f4c9f8.jpg","comment_is_top":false,"comment_ctime":1698746740,"is_pvip":false,"replies":[{"id":139599,"content":"代码已更","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698770285,"ip_address":"瑞士","comment_id":383248,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"github上没有代码？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630669,"discussion_content":"代码已更","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698770285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1127727,"avatar":"https://static001.geekbang.org/account/avatar/00/11/35/2f/3d8f7b2e.jpg","nickname":"River.W♌","note":"","ucode":"70324ED4D52FCD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":632599,"discussion_content":"老师代码地址能发下么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701054238,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":630669,"ip_address":"北京","group_id":0},"score":632599,"extra":""}]}]},{"had_liked":false,"id":387733,"user_name":"卓丁","can_delete":false,"product_type":"c1","uid":1030687,"ip_address":"北京","ucode":"71B68238D304FE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ba/1f/7551f182.jpg","comment_is_top":false,"comment_ctime":1708488784,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"我遇到一个报错：\n    raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 2 column 3 (char 3)\n\n看着好像是 json.loads解析失败了。\n进一步看报错栈，是报 400 的状态码。\n\n于是我尝试将get_data方法所请求的url打印出来了一下。\n\n```\n# 根据UID构建URL爬取信息\ndef get_data(id):\n    url = &quot;https:&#47;&#47;weibo.com&#47;ajax&#47;profile&#47;detail?uid={}&quot;.format(id)\n    print(&quot;url-&gt;&quot;,url)\n    html = scrape_weibo(url)\n    print(html)\n    response = json.loads(html)\n\n    return response\n```\n\n结果如下：\n\n&gt; Finished chain.\n这位鲜花大V的微博ID是 100808\nurl-&gt; https:&#47;&#47;weibo.com&#47;ajax&#47;profile&#47;detail?uid=100808\n&lt;h2&gt;400 Bad Request&lt;&#47;h2&gt;\nTraceback (most recent call last):\n  File &quot;&#47;Users&#47;bawenmao&#47;PycharmProjects&#47;socializer_v0&#47;findbigV.py&quot;, line 23, in &lt;module&gt;\n    person_info = get_data(UID)\n                  ^^^^^^^^^^^^^\n  File &quot;&#47;Users&#47;bawenmao&#47;PycharmProjects&#47;socializer_v0&#47;tools&#47;scraping_tool.py&quot;, line 26, in get_data\n    response = json.loads(html)\n               ^^^^^^^^^^^^^^^^\n  File &quot;&#47;Library&#47;Frameworks&#47;Python.framework&#47;Versions&#47;3.11&#47;lib&#47;python3.11&#47;json&#47;__init__.py&quot;, line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&#47;Library&#47;Frameworks&#47;Python.framework&#47;Versions&#47;3.11&#47;lib&#47;python3.11&#47;json&#47;decoder.py&quot;, line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;&#47;Library&#47;Frameworks&#47;Python.framework&#47;Versions&#47;3.11&#47;lib&#47;python3.11&#47;json&#47;decoder.py&quot;, line 355, in raw_decode\n    raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n\n\n是不是所请求的那个url 本身不对；\n请教下老师，这是哪里的问题；","like_count":0,"discussions":[{"author":{"id":1378913,"avatar":"https://static001.geekbang.org/account/avatar/00/15/0a/61/ff5549d1.jpg","nickname":"洋","note":"","ucode":"863193F96C0E81","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":638285,"discussion_content":"应该是cookie设置的不对。找到detail接口复制cookie还有use-agent","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1709470507,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}