{"id":69978,"title":"13 | 同样的本质，为何Spark可以更高效？","content":"<p>上一期我们讨论了Spark的编程模型，这期我们聊聊<span class=\"orange\">Spark的架构原理</span>。和MapReduce一样，<strong>Spark也遵循移动计算比移动数据更划算这一大数据计算基本原则</strong>。但是和MapReduce僵化的Map与Reduce分阶段计算相比，Spark的计算框架更加富有弹性和灵活性，进而有更好的运行性能。</p><h2>Spark的计算阶段</h2><p>我们可以对比来看。首先和MapReduce一个应用一次只运行一个map和一个reduce不同，Spark可以根据应用的复杂程度，分割成更多的计算阶段（stage），这些计算阶段组成一个有向无环图DAG，Spark任务调度器可以根据DAG的依赖关系执行计算阶段。</p><p>还记得在上一期，我举了一个比较逻辑回归机器学习性能的例子，发现Spark比MapReduce快100多倍。因为某些机器学习算法可能需要进行大量的迭代计算，产生数万个计算阶段，这些计算阶段在一个应用中处理完成，而不是像MapReduce那样需要启动数万个应用，因此极大地提高了运行效率。</p><p>所谓DAG也就是有向无环图，就是说不同阶段的依赖关系是有向的，计算过程只能沿着依赖关系方向执行，被依赖的阶段执行完成之前，依赖的阶段不能开始执行，同时，这个依赖关系不能有环形依赖，否则就成为死循环了。下面这张图描述了一个典型的Spark运行DAG的不同阶段。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/c8/db/c8cf515c664b478e51058565e0d4a8db.png?wh=1296*712\" alt=\"\"></p><p>从图上看，整个应用被切分成3个阶段，阶段3需要依赖阶段1和阶段2，阶段1和阶段2互不依赖。Spark在执行调度的时候，先执行阶段1和阶段2，完成以后，再执行阶段3。如果有更多的阶段，Spark的策略也是一样的。只要根据程序初始化好DAG，就建立了依赖关系，然后根据依赖关系顺序执行各个计算阶段，Spark大数据应用的计算就完成了。</p><p>上图这个DAG对应的Spark程序伪代码如下。</p><pre><code>rddB = rddA.groupBy(key)\nrddD = rddC.map(func)\nrddF = rddD.union(rddE)\nrddG = rddB.join(rddF)\n</code></pre><p>所以，你可以看到Spark作业调度执行的核心是DAG，有了DAG，整个应用就被切分成哪些阶段，每个阶段的依赖关系也就清楚了。之后再根据每个阶段要处理的数据量生成相应的任务集合（TaskSet），每个任务都分配一个任务进程去处理，Spark就实现了大数据的分布式计算。</p><p>具体来看的话，负责Spark应用DAG生成和管理的组件是DAGScheduler，DAGScheduler根据程序代码生成DAG，然后将程序分发到分布式计算集群，按计算阶段的先后关系调度执行。</p><p>那么Spark划分计算阶段的依据是什么呢？显然并不是RDD上的每个转换函数都会生成一个计算阶段，比如上面的例子有4个转换函数，但是只有3个阶段。</p><p>你可以再观察一下上面的DAG图，关于计算阶段的划分从图上就能看出规律，当RDD之间的转换连接线呈现多对多交叉连接的时候，就会产生新的阶段。一个RDD代表一个数据集，图中每个RDD里面都包含多个小块，每个小块代表RDD的一个分片。</p><p>一个数据集中的多个数据分片需要进行分区传输，写入到另一个数据集的不同分片中，这种数据分区交叉传输的操作，我们在MapReduce的运行过程中也看到过。</p><p><img src=\"https://static001.geekbang.org/resource/image/d6/c7/d64daa9a621c1d423d4a1c13054396c7.png?wh=1316*772\" alt=\"\"></p><p>是的，这就是shuffle过程，Spark也需要通过shuffle将数据进行重新组合，相同Key的数据放在一起，进行聚合、关联等操作，因而每次shuffle都产生新的计算阶段。这也是为什么计算阶段会有依赖关系，它需要的数据来源于前面一个或多个计算阶段产生的数据，必须等待前面的阶段执行完毕才能进行shuffle，并得到数据。</p><p>这里需要你特别注意的是，<strong>计算阶段划分的依据是shuffle，不是转换函数的类型</strong>，有的函数有时候有shuffle，有时候没有。比如上图例子中RDD B和RDD F进行join，得到RDD G，这里的RDD F需要进行shuffle，RDD B就不需要。</p><p><img src=\"https://static001.geekbang.org/resource/image/4e/8b/4e5c79d1ad7152bc8ab8bc350cf6778b.png?wh=508*644\" alt=\"\"></p><p>因为RDD B在前面一个阶段，阶段1的shuffle过程中，已经进行了数据分区。分区数目和分区Key不变，就不需要再进行shuffle。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/25/4650b622d9c6ed5f65670482cc8ca325.png?wh=558*222\" alt=\"\"></p><p>这种不需要进行shuffle的依赖，在Spark里被称作窄依赖；相反的，需要进行shuffle的依赖，被称作宽依赖。跟MapReduce一样，shuffle也是Spark最重要的一个环节，只有通过shuffle，相关数据才能互相计算，构建起复杂的应用逻辑。</p><p>在你熟悉Spark里的shuffle机制后我们回到今天文章的标题，同样都要经过shuffle，为什么Spark可以更高效呢？</p><p>其实从本质上看，Spark可以算作是一种MapReduce计算模型的不同实现。Hadoop MapReduce简单粗暴地根据shuffle将大数据计算分成Map和Reduce两个阶段，然后就算完事了。而Spark更细腻一点，将前一个的Reduce和后一个的Map连接起来，当作一个阶段持续计算，形成一个更加优雅、高效的计算模型，虽然其本质依然是Map和Reduce。但是这种多个计算阶段依赖执行的方案可以有效减少对HDFS的访问，减少作业的调度执行次数，因此执行速度也更快。</p><p>并且和Hadoop MapReduce主要使用磁盘存储shuffle过程中的数据不同，Spark优先使用内存进行数据存储，包括RDD数据。除非是内存不够用了，否则是尽可能使用内存， 这也是Spark性能比Hadoop高的另一个原因。</p><h2>Spark的作业管理</h2><p>我在专栏上一期提到，Spark里面的RDD函数有两种，一种是转换函数，调用以后得到的还是一个RDD，RDD的计算逻辑主要通过转换函数完成。</p><p>另一种是action函数，调用以后不再返回RDD。比如<strong>count</strong>()函数，返回RDD中数据的元素个数；<strong>saveAsTextFile</strong>(path)，将RDD数据存储到path路径下。Spark的DAGScheduler在遇到shuffle的时候，会生成一个计算阶段，在遇到action函数的时候，会生成一个作业（job）。</p><p>RDD里面的每个数据分片，Spark都会创建一个计算任务去处理，所以一个计算阶段会包含很多个计算任务（task）。</p><p>关于作业、计算阶段、任务的依赖和时间先后关系你可以通过下图看到。</p><p><img src=\"https://static001.geekbang.org/resource/image/2b/d0/2bf9e431bbd543165588a111513567d0.png?wh=668*188\" alt=\"\"></p><p>图中横轴方向是时间，纵轴方向是任务。两条粗黑线之间是一个作业，两条细线之间是一个计算阶段。一个作业至少包含一个计算阶段。水平方向红色的线是任务，每个阶段由很多个任务组成，这些任务组成一个任务集合。</p><p>DAGScheduler根据代码生成DAG图以后，Spark的任务调度就以任务为单位进行分配，将任务分配到分布式集群的不同机器上执行。</p><h2>Spark的执行过程</h2><p>Spark支持Standalone、Yarn、Mesos、Kubernetes等多种部署方案，几种部署方案原理也都一样，只是不同组件角色命名不同，但是核心功能和运行流程都差不多。</p><p><img src=\"https://static001.geekbang.org/resource/image/16/db/164e9460133d7744d0315a876e7b6fdb.png?wh=596*286\" alt=\"\"></p><p>上面这张图是Spark的运行流程，我们一步一步来看。</p><p>首先，Spark应用程序启动在自己的JVM进程里，即Driver进程，启动后调用SparkContext初始化执行配置和输入数据。SparkContext启动DAGScheduler构造执行的DAG图，切分成最小的执行单位也就是计算任务。</p><p>然后Driver向Cluster Manager请求计算资源，用于DAG的分布式计算。Cluster Manager收到请求以后，将Driver的主机地址等信息通知给集群的所有计算节点Worker。</p><p>Worker收到信息以后，根据Driver的主机地址，跟Driver通信并注册，然后根据自己的空闲资源向Driver通报自己可以领用的任务数。Driver根据DAG图开始向注册的Worker分配任务。</p><p>Worker收到任务后，启动Executor进程开始执行任务。Executor先检查自己是否有Driver的执行代码，如果没有，从Driver下载执行代码，通过Java反射加载后开始执行。</p><h2>小结</h2><p>总结来说，Spark有三个主要特性：<strong>RDD的编程模型更简单，DAG切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效</strong>。这三个特性使得Spark相对Hadoop MapReduce可以有更快的执行速度，以及更简单的编程实现。</p><p>Spark的出现和流行其实也有某种必然性，是天时、地利、人和的共同作用。首先，Spark在2012年左右开始流行，那时内存的容量提升和成本降低已经比MapReduce出现的十年前强了一个数量级，Spark优先使用内存的条件已经成熟；其次，使用大数据进行机器学习的需求越来越强烈，不再是早先年那种数据分析的简单计算需求。而机器学习的算法大多需要很多轮迭代，Spark的stage划分相比Map和Reduce的简单划分，有更加友好的编程体验和更高效的执行效率。于是Spark成为大数据计算新的王者也就不足为奇了。</p><h2>思考题</h2><p>Spark的流行离不开它成功的开源运作，开源并不是把源代码丢到GitHub上公开就万事大吉了，一个成功的开源项目需要吸引大量高质量开发者参与其中，还需要很多用户使用才能形成影响力。</p><p>Spark开发团队为Spark开源运作进行了大量的商业和非商业活动，你了解这些活动有哪些吗？假如你所在的公司想要开源自己的软件，用于提升自己公司的技术竞争力和影响力，如果是你负责人，你应该如何运作？</p><p>欢迎你写下自己的思考或疑问，与我和其他同学一起讨论。</p>","neighbors":{"left":{"article_title":"12 | 我们并没有觉得MapReduce速度慢，直到Spark出现","id":69822},"right":{"article_title":"14 | BigTable的开源实现：HBase","id":70253}},"comments":[{"had_liked":false,"id":53884,"user_name":"vivi","can_delete":false,"product_type":"c1","uid":1316902,"ip_address":"","ucode":"17A9A72EC95315","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/oNyD409Hg3iaSaMpF3ibn35FstWN3AHnQaJf7LaIqkrsvHZcd8lYbOicDiaU1NcicoERz5kC3YKM4iaYsXpHBYEVW6yw/132","comment_is_top":false,"comment_ctime":1545748148,"is_pvip":false,"replies":[{"id":"19546","content":"👍🏻","user_name":"作者回复","comment_id":53884,"uid":"1007349","ip_address":"","utype":1,"ctime":1545786780,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"280718622388","product_id":100020201,"comment_content":"懂了原理，实战其实很简单，不用急着学部署啊，操作，原理懂了，才能用好，我觉得讲得很好","like_count":66,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":434278,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545786780,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":160064,"user_name":"张小喵","can_delete":false,"product_type":"c1","uid":1708090,"ip_address":"","ucode":"2CD5003020D751","user_header":"https://static001.geekbang.org/account/avatar/00/1a/10/3a/053852ee.jpg","comment_is_top":false,"comment_ctime":1575862570,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"160489652522","product_id":100020201,"comment_content":"把12|13反复读了4、5遍，以下是我的感悟<br>Spark再次理解：<br>\t1：Spark计算框架编程和运行速度比MapReduce更加的简单和快<br>\t编程更简单：Spark编程关注的数据是RDD，RDD是个抽象的，比较不好理解，我的理解RDD是一次计算阶段中 要操作的所有的数据的抽象，虽然它们是分片的，并且分布在HDFS的任意节点上，但是概念上，我们是针对这个抽象的RDD编程的。使用Scala编程，wordcount只需要三行代码，很简单是吧，但是背后的整体的计算过程是相当的复杂的。这样看我们在MapReduce中的编程要是关注所有的数据的，默认为map的数据输入的数据是整个要处理的数据，并不是说，其他的应用就不知道了，毕竟我们也可以在代码中感知到在文件中的偏移量这种东西。<br>\t速度更快：MapReduce暴力的把计算阶段分为两个阶段，Map和Reduce阶段， 如果一个应用的计算实现只有两个阶段，那么MapReduce计算框架的速度不会比Spark慢多少，慢的地方只是在于Spark是不经过落盘的操作的，直接在内存中存储，但是如果一个应用的计算阶段变得很多的话，比如机器学习中的迭代计算，那么使用MapReduce的就非常慢了，如果这个应用的计算分为10000个计算阶段，如果用MapReduce实现，就需要启动5000次相关的应用，速度很慢，并且编码会很麻烦，如果 是Spark，那么在一次应用就可以解决该问题。<br><br>Spark的多个计算阶段的理解：<br>\t相比于MapReduce只有两个计算阶段， Spark理论上可以有无限个计算阶段， 这也是Spark的速度的优势<br>\tSpark的计算阶段的表示中，DAG（有向无环图）是Spark的关键，DAG可以很好的表示每个计算阶段的关系，或者说依赖书序<br>\t那么DAG是谁生成的呢，是根据什么生成的呢？<br>\t\tDAG是有Spark计算框架根据用于所写的代码生成的，那怎么依据代码的什么生成的呢？<br>\t\t\t类比MapReduce的两个计算阶段，两个阶段之间的过度是什么？是shuffle，Spark也是根据Spark中的代码中的转换函数是否是有shuffle操作进行划分阶段的！<br>\t我的理解，Spark的每个计算阶段可以类比MapReduce中的Map阶段或者Reduce阶段。不同的是，Spark计算阶段关注的是RDD，但是又有相同的点，RDD中的数据组成也是一片一片的，Spark中的最小的任务也就是对于片的计算，原理和MapReduce一样，Spark中的片和MapReduce中的片是通一个东西，每个片都是分布于HDFS上的，对于每个片的计算大概率也是在片所在的计算节点的。所以Spark的计算也是分布式，并且原理和MapReduce是一样的。<br><br>Spark中RDD上的操作函数：RDD上的操作函数分为两种类型，一种是转换函数，另一种是action函数<br>\t转换函数：Spark编程中对于RDD的操作基本是用转换函数来完成的，转换函数是计算RDD，转换函数又分为两种<br>\t\t只是改变RDD内容的转换函数：类似map，filter函数，RDD本身物理上没有变化，所有的操作都是针对于当前的分片<br>\t\t会新生成RDD转换函数：类似于reduceByKey这种函数，会组合key生成新的RDD（所有的会生成新的RDD的函数都可以作为 计算阶段的分割函数吗？）<br>\taction函数：aciton函数对于RDD的操作没有返回值，或者说不会改变RDD的内容，比如rdd.saveToPath等等<br><br>Spark中一些关于应用的生命周期中的过程的名词、概念：<br>\tSpark中的RDD函数主要有两种，一是转换函数，调用转换函数可以返回一个RDD（产生新的RDD？&#47;会shuffle的函数分割计算阶段），RDD的计算逻辑主要是由转换函数完成<br><br>\t另一种是action函数，调用这种函数不返回RDD，DAGScheduler在遇到shuffle的时候生成一个新的计算阶段，在遇到action函数的时候，产生一个作业。（我理解可以类似于，一次MapReduce程序对应Spark中的一个作业）<br>\t在每个计算阶段都是针对RDD（包含很多片）的计算，每个分片Spark都会创建一个计算任务去处理，所以每个计算阶段会包含很多个计算任务","like_count":38,"discussions":[{"author":{"id":2836611,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/WN351R6WwfnlILLdnH4xsUTJCG2mYFRuibdNHBJFicTHXGiaR3lxBYGmagJhicibRWFPB7fGYnDrtV9GFQtwDo1d73A/132","nickname":"黄谦敏","note":"","ucode":"057CB1810E2366","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537425,"discussion_content":"你好科代表。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639057720,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":43871,"user_name":"落叶飞逝的恋","can_delete":false,"product_type":"c1","uid":1046429,"ip_address":"","ucode":"F9A95DB28BCF1E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f7/9d/be04b331.jpg","comment_is_top":false,"comment_ctime":1543315488,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"151867170848","product_id":100020201,"comment_content":"总结：Spark的优点就是能够动态根据计算逻辑的复杂度进行不断的拆分子任务，而实现在一个应用中处理所有的逻辑，而不像MapReduce需要启动多个应用进行计算。","like_count":35},{"had_liked":false,"id":44340,"user_name":"My dream","can_delete":false,"product_type":"c1","uid":1077733,"ip_address":"","ucode":"2FEFB344230C17","user_header":"https://static001.geekbang.org/account/avatar/00/10/71/e5/bcdc382a.jpg","comment_is_top":false,"comment_ctime":1543411543,"is_pvip":false,"discussion_count":7,"race_medal":0,"score":"91737724759","product_id":100020201,"comment_content":"老师，你讲的理论看的我头晕脑胀的，能不能讲点实战操作，搭建spark环境，通过案例来讲的话，对于我们这些初学学生来说是最直观，最容易弄明白它为什么会那么快，像你这样一味的讲理论，不讲实战，我们实在是吸收不了，理解不了你讲的这些知识点","like_count":21,"discussions":[{"author":{"id":1142315,"avatar":"https://static001.geekbang.org/account/avatar/00/11/6e/2b/301bd439.jpg","nickname":"RXTM","note":"","ucode":"A308B87945E44D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":8355,"discussion_content":"上来讲实战，才是什么都学不会。实战可以随时翻文档，原理不搞清楚，实战完了也不知道怎么回事。","likes_number":12,"is_delete":false,"is_hidden":false,"ctime":1567939009,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1544726,"avatar":"https://static001.geekbang.org/account/avatar/00/17/92/16/558ff49c.jpg","nickname":"ctf小白","note":"","ucode":"C0326909867DE4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":176013,"discussion_content":"可能有点实战经验会更容易理解这篇文章， 网上讲如何搭建这些的文章其实很多，但这样概括讲原理的真的很少，把重要知识点理出来的不多。我自己已经有过大数据的经验，现在再来看觉得讲的很好，大家有机会上手实践以后，再回来看也许会有更多体会","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1582014650,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2859880,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/a3/68/e913d823.jpg","nickname":"*其","note":"","ucode":"B1B0A47CEDECC3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":543341,"discussion_content":"实战+理论更适合，可以先用HDP+ambari搭建一个Hadoop集群，在课程上了解原理，再具体实战，毕竟实战也离不开理论指导，不然就只会使用工具了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641096369,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1077636,"avatar":"https://static001.geekbang.org/account/avatar/00/10/71/84/4b33817f.jpg","nickname":"50包邮解君愁","note":"","ucode":"BFC08A518F24B3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372946,"discussion_content":"哈哈隔两天有空就翻回来看一遍，这样间断地消化着理解，看两三遍就能理解的更好啦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620535702,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1339820,"avatar":"https://static001.geekbang.org/account/avatar/00/14/71/ac/8295e3e7.jpg","nickname":"书忆江南","note":"","ucode":"90776A7CE06D66","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":257323,"discussion_content":"可以看看《Spark权威指南》，这本书对入门者非常友好：\n有什么关于 Spark 的书推荐？ - 书忆江南的回答 - 知乎\nhttps://www.zhihu.com/question/23655827/answer/1039397401","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588558670,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1140643,"avatar":"https://static001.geekbang.org/account/avatar/00/11/67/a3/bdd14fdc.jpg","nickname":"lei Shi","note":"","ucode":"AA28B0567A68C9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":217150,"discussion_content":"实战的资料永远比理论的知识多","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585502772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1708090,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/10/3a/053852ee.jpg","nickname":"张小喵","note":"","ucode":"2CD5003020D751","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":73928,"discussion_content":"之前的每节课听得都挺好，但是这章节我听的也是云里雾里，我觉得好多的疑问点，没有一个实际的例子，不像之前讲MapReduce的时候，用Word count做例子串整个工作流程。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1575611750,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":44002,"user_name":"scorpiozj","can_delete":false,"product_type":"c1","uid":1031677,"ip_address":"","ucode":"C66EA76809F9BF","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bd/fd/3f5d5db5.jpg","comment_is_top":false,"comment_ctime":1543364252,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"91737677468","product_id":100020201,"comment_content":"移动计算比移动数据划算 总结的真好 很多设计仔细想一想都是围绕这个中心<br><br>关于开源<br>1 准备好详细的使用 api文档并提供示例<br>2 撰写设计思路 和竞品比较的优势 以及创新点<br>3 提前联系若干团队使用产品 并请他们提供真实的提高效率的数据报告<br>4 联系公关团队在知名技术论坛推广<br>5 成立团队 及时响应开发者疑问 需求和pr等","like_count":21},{"had_liked":false,"id":43879,"user_name":"纯洁的憎恶","can_delete":false,"product_type":"c1","uid":1130512,"ip_address":"","ucode":"5E9757DE6F45DF","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/10/b6bf3c3c.jpg","comment_is_top":false,"comment_ctime":1543316811,"is_pvip":true,"replies":[{"id":"15682","content":"引用楼下的评论回复<br><br>落叶飞逝的恋<br>总结：Spark的优点就是能够动态根据计算逻辑的复杂度进行不断的拆分子任务，而实现在一个应用中处理所有的逻辑，而不像MapReduce需要启动多个应用进行计算。","user_name":"作者回复","comment_id":43879,"uid":"1007349","ip_address":"","utype":1,"ctime":1543329670,"user_name_real":"李智慧"}],"discussion_count":4,"race_medal":0,"score":"48787957067","product_id":100020201,"comment_content":"这两天的内容对我来说有些复杂，很多知识点没有理解透。针对“而 Spark 更细腻一点，将前一个的 Reduce 和后一个的 Map 连接起来，当作一个阶段持续计算，形成一个更加优雅、高效地计算模型”。这句话中“将前一个的 Reduce 和后一个的 Map 连接起来”在细节上该如何理解，这也是明显的串行过程，感觉不会比传统的MapReduce快？是因为不同阶段之间有可能并行么？","like_count":11,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430533,"discussion_content":"引用楼下的评论回复\n\n落叶飞逝的恋\n总结：Spark的优点就是能够动态根据计算逻辑的复杂度进行不断的拆分子任务，而实现在一个应用中处理所有的逻辑，而不像MapReduce需要启动多个应用进行计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543329670,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028938,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/8a/dc9a23a1.jpg","nickname":"续费专用","note":"","ucode":"1B585A131B64B4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":364605,"discussion_content":"还是没明白+1 我理解spark更快是由于map和reduce在spark中是作为一个整体stage运行的，没有落盘，直接存储在内存中；但是是否启动多个应用是什么意思呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617533949,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1763486,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqwURicicejDP5IlicpyCYxlfzufmGLjvb07WwibTSjdH1V9WfyALhicDxF6l0pq1LgAtroWkxChetsO5Q/132","nickname":"Geek_4c72df","note":"","ucode":"361CE31F4976C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2028938,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/8a/dc9a23a1.jpg","nickname":"续费专用","note":"","ucode":"1B585A131B64B4","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":587734,"discussion_content":"我理解这里是说，假如两个mapReduce阶段是有依赖关系的。mapReduce会串行执行，起两个应用。而在spark会被划分到同一个stage执行，前一个结果出来后会直接执行下一个，中间结果存储在内存中，这样整体效率高很多。除非当前数据的计算过程结束，需要引入/join新数据进行shuffle而启动新的stage，否则可以一直计算，而不用重新启动在读取处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1663244902,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":364605,"ip_address":"北京"},"score":587734,"extra":""}]},{"author":{"id":2062252,"avatar":"","nickname":"常振华","note":"","ucode":"D61B40E1CCEFD5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":293035,"discussion_content":"还是没明白","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595419232,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":47870,"user_name":"ming","can_delete":false,"product_type":"c1","uid":1054341,"ip_address":"","ucode":"63135A86598E64","user_header":"https://static001.geekbang.org/account/avatar/00/10/16/85/e4d53282.jpg","comment_is_top":false,"comment_ctime":1544240908,"is_pvip":false,"replies":[{"id":"17355","content":"数据分布删掉，谢谢指正。","user_name":"作者回复","comment_id":47870,"uid":"1007349","ip_address":"","utype":1,"ctime":1544488868,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"31609011980","product_id":100020201,"comment_content":"老师，有一句话我不太理解，请老师指导。“DAGScheduler 根据代码和数据分布生成 DAG 图”。根据代码生产DAG图我理解，但是为什么生成DAG图还要根据数据分布生成，数据分布不同，生成的DAG图也会不同吗？","like_count":7,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":431919,"discussion_content":"数据分布删掉，谢谢指正。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1544488868,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":43991,"user_name":"周洋舟","can_delete":false,"product_type":"c1","uid":1252433,"ip_address":"","ucode":"EA2BC9E38FC35B","user_header":"https://static001.geekbang.org/account/avatar/00/13/1c/51/dc21bb08.jpg","comment_is_top":false,"comment_ctime":1543363007,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31608134079","product_id":100020201,"comment_content":"每一篇文章都认真的读了，有些东西还没真正的去在实际工作中体会到，但这种思维的启发还是受益匪浅。","like_count":7},{"had_liked":false,"id":71628,"user_name":"张飞","can_delete":false,"product_type":"c1","uid":1405598,"ip_address":"","ucode":"836F612B8E9C8A","user_header":"https://static001.geekbang.org/account/avatar/00/15/72/9e/69606254.jpg","comment_is_top":false,"comment_ctime":1551402844,"is_pvip":false,"replies":[{"id":"26258","content":"1 Spark的map和reduce的划分要更优雅一点，比如宽依赖和窄依赖，编程上看不出明显的map和reduce，这种优雅还有很多，多写一些spark和MapReduce程序就能感受到。<br><br>2 如果内存够用，Spark几乎总是使用内存。<br><br>3 可以这么理解。","user_name":"作者回复","comment_id":71628,"uid":"1007349","ip_address":"","utype":1,"ctime":1551662859,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"27321206620","product_id":100020201,"comment_content":"1.“而 Spark 更细腻一点，将前一个的 Reduce 和后一个的 Map 连接起来，当作一个阶段持续计算，形成一个更加优雅、高效地计算模型”，stage之间是串行的，即便前一个的reduce和后一个的map连接起来，也是要从前一个stage的计算节点的磁盘上拉取数据的，这跟mapreduce的计算是一样的，老师所说的高效在这里是怎么提现的呢？<br>2. spark的内存计算主要体现在shuffle过程，下一个stage拉取上一个stage的数据的时候更多的使用内存，在这里区分出与mapreduce计算的不同，别的还有什么阶段比mapreduce更依赖内存的吗？ <br>3.我是不是可以这样理解，如果只有map阶段的话，即便计算量很大，mapreduce与spark的计算速度上也没有太大的区别？<br>可能问题问的不够清晰，希望老师解答一下。","like_count":6,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441241,"discussion_content":"1 Spark的map和reduce的划分要更优雅一点，比如宽依赖和窄依赖，编程上看不出明显的map和reduce，这种优雅还有很多，多写一些spark和MapReduce程序就能感受到。\n\n2 如果内存够用，Spark几乎总是使用内存。\n\n3 可以这么理解。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551662859,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73489,"user_name":"海","can_delete":false,"product_type":"c1","uid":1349556,"ip_address":"","ucode":"49BD7F2F4603F0","user_header":"https://static001.geekbang.org/account/avatar/00/14/97/b4/2ec7a386.jpg","comment_is_top":false,"comment_ctime":1551916996,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23026753476","product_id":100020201,"comment_content":"对于hbase和高速发展的es，不知道您怎么看，他们的优缺点是什么？","like_count":5},{"had_liked":false,"id":43886,"user_name":"追梦小乐","can_delete":false,"product_type":"c1","uid":1035507,"ip_address":"","ucode":"82AB8A7F98389D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/cc/f3/5e4b0315.jpg","comment_is_top":false,"comment_ctime":1543318034,"is_pvip":false,"replies":[{"id":"15683","content":"每个任务一个进程<br><br>很多红色线条，每条线代表一个任务<br><br>cache理解成存储rdd的内存","user_name":"作者回复","comment_id":43886,"uid":"1007349","ip_address":"","utype":1,"ctime":1543329787,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"23018154514","product_id":100020201,"comment_content":"老师，我想请教几个问题：<br>1、“根据每个阶段要处理的数据量生成相应的任务集合（TaskSet），每个任务都分配一个任务进程去处理”，是一个任务集合TaskSet启动一个进程，taskSet里面的任务是用线程计算吗？还是每个TaskSet里面的任务启动一个进程？<br><br>2、task-time 图中红色密集的表示什么？<br><br>3、Spark 的执行过程 的图中 Executor 中的 Cache 是表示内存吗？task和Executor不是在内存中的吗？","like_count":5,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430537,"discussion_content":"每个任务一个进程\n\n很多红色线条，每条线代表一个任务\n\ncache理解成存储rdd的内存","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543329787,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1327497,"avatar":"https://static001.geekbang.org/account/avatar/00/14/41/89/77d3e613.jpg","nickname":"bookchan","note":"","ucode":"6C40EEAC767E25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":255152,"discussion_content":"图里面没有显示stage是并行执行的，真实情况是不是可以并行执行？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588382180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72687,"user_name":"落叶飞逝的恋","can_delete":false,"product_type":"c1","uid":1046429,"ip_address":"","ucode":"F9A95DB28BCF1E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f7/9d/be04b331.jpg","comment_is_top":false,"comment_ctime":1551694496,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"18731563680","product_id":100020201,"comment_content":"现在回过头看，Spark的编程模型其实类似Java8的Steam编程模型","like_count":4,"discussions":[{"author":{"id":1121459,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1c/b3/6d10ba4f.jpg","nickname":"Running Man","note":"","ucode":"4DD2F6AF71327E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":52099,"discussion_content":"函数式编程","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573995143,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":45037,"user_name":"白鸽","can_delete":false,"product_type":"c1","uid":1182064,"ip_address":"","ucode":"BDA54B54F653B4","user_header":"https://static001.geekbang.org/account/avatar/00/12/09/70/6b6382ca.jpg","comment_is_top":false,"comment_ctime":1543541069,"is_pvip":false,"replies":[{"id":"16227","content":"整个jar","user_name":"作者回复","comment_id":45037,"uid":"1007349","ip_address":"","utype":1,"ctime":1543676373,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"18723410253","product_id":100020201,"comment_content":"Executor 从 Diver 下载执行代码，是整个程序 jar包?还是仅 Executor 计算任务对应的一段计算程序（经SparkSession初始化后的）?","like_count":4,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430905,"discussion_content":"整个jar","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543676373,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":43667,"user_name":"多襄丸","can_delete":false,"product_type":"c1","uid":1074310,"ip_address":"","ucode":"1AA1497C5A293C","user_header":"https://static001.geekbang.org/account/avatar/00/10/64/86/f5a9403a.jpg","comment_is_top":false,"comment_ctime":1543279317,"is_pvip":false,"replies":[{"id":"15647","content":"有些问题不一定要得到答案或者回答出来，只是关注到了思考一下，就会有收获～","user_name":"作者回复","comment_id":43667,"uid":"1007349","ip_address":"","utype":1,"ctime":1543304645,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"18723148501","product_id":100020201,"comment_content":"啊、老师现在的提问都好大，我现在是老虎吃天无从下爪啊 ^_^","like_count":4,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430454,"discussion_content":"有些问题不一定要得到答案或者回答出来，只是关注到了思考一下，就会有收获～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543304645,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":119241,"user_name":"weiruan85","can_delete":false,"product_type":"c1","uid":1069949,"ip_address":"","ucode":"8CFD7B198759E3","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Ng7NIuMhg8E3U6DjwJlTKFcEYsTtFJkiag22G13JXSiaobpibfI6MicKg93VNqQnG7Rkvl2OfCsAaSksCAVbNDp8zw/132","comment_is_top":false,"comment_ctime":1564542316,"is_pvip":false,"replies":[{"id":"43887","content":"赞","user_name":"作者回复","comment_id":119241,"uid":"1007349","ip_address":"","utype":1,"ctime":1564645675,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"14449444204","product_id":100020201,"comment_content":"1.完备的技术说明文档是必须的，比如使用场景，常见问题，环境搭建，核心技术的原理等。<br>2.输出真实等使用案例，以及给解决实际问题带来等好处，比如如果没有我们的开源方案是怎么实现的，有了这个方案是怎么实现的，差异是什么<br>3.商业推广，找业界有名的公司站台，或者有名的技术大牛做宣传（头羊效应）<br>4.归根结底，还是得有开创性的技术，能解决现实中的某一类问题。","like_count":3,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":460802,"discussion_content":"赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564645675,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":59934,"user_name":"Yezhiwei","can_delete":false,"product_type":"c1","uid":1005157,"ip_address":"","ucode":"31E8E33688CBEC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/56/65/22a37a8e.jpg","comment_is_top":false,"comment_ctime":1547431924,"is_pvip":false,"replies":[{"id":"21593","content":"👍👍👍","user_name":"作者回复","comment_id":59934,"uid":"1007349","ip_address":"","utype":1,"ctime":1547523597,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"14432333812","product_id":100020201,"comment_content":"这里是学习过程中做的一些总结<br><br>https:&#47;&#47;mp.weixin.qq.com&#47;s&#47;OyPRXAu9hR1KWIbvc20y1g","like_count":3,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":436418,"discussion_content":"👍👍👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1547523597,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":119233,"user_name":"weiruan85","can_delete":false,"product_type":"c1","uid":1069949,"ip_address":"","ucode":"8CFD7B198759E3","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Ng7NIuMhg8E3U6DjwJlTKFcEYsTtFJkiag22G13JXSiaobpibfI6MicKg93VNqQnG7Rkvl2OfCsAaSksCAVbNDp8zw/132","comment_is_top":false,"comment_ctime":1564540429,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10154475021","product_id":100020201,"comment_content":"  客户：我想给数据库中的一张关键表添加几个索引，对生产有没有影响<br>  fey：为什么要加索引呢，是张什么表<br>  客户：系统中的流水表，查询比较慢<br>  fey：数据量大概多大<br>  客户：有5000万<br>  fey：存储了多久的数据呢<br>  客户：存了将近2年的数据了<br>  fey：按照业务应该保存多久的数据呢<br>  客户：3个月<br>  fey：那我们是不是应该先把历史数据进行归档，然后在添加索引呢。<br>  客户：对，可以先做数据的归档。<br>  ","like_count":2},{"had_liked":false,"id":90637,"user_name":"冰ྂ镇ྂ可ྂ乐ྂ","can_delete":false,"product_type":"c1","uid":1455996,"ip_address":"","ucode":"A19F24BA1178FB","user_header":"https://static001.geekbang.org/account/avatar/00/16/37/7c/b6bc6024.jpg","comment_is_top":false,"comment_ctime":1556593360,"is_pvip":false,"replies":[{"id":"32706","content":"可以这样理解。<br><br>MR模型本身不包含DAG，需要外部工具基于MR构建DAG实现复杂的计算，比如Hive。<br>Spark的计算模型本身就是包含DAG。","user_name":"作者回复","comment_id":90637,"uid":"1007349","ip_address":"","utype":1,"ctime":1557020405,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"10146527952","product_id":100020201,"comment_content":"之前讲mr时候也有提到生成dag，spark这里也是dag，二者的差异是mr中，map reduce为一组操作(可能没有reduce)的一个job job之间是依赖关系，而spark并非简单依照m r划分而是针对数据的处理情况，如果r后到下一个m是窄依赖，则属于同一个stage，属于一个流程，这样理解对吗？","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448604,"discussion_content":"可以这样理解。\n\nMR模型本身不包含DAG，需要外部工具基于MR构建DAG实现复杂的计算，比如Hive。\nSpark的计算模型本身就是包含DAG。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557020405,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":57874,"user_name":"尼糯米","can_delete":false,"product_type":"c1","uid":1282819,"ip_address":"","ucode":"04D1B63F3801AE","user_header":"","comment_is_top":false,"comment_ctime":1546933565,"is_pvip":false,"replies":[{"id":"20871","content":"√","user_name":"作者回复","comment_id":57874,"uid":"1007349","ip_address":"","utype":1,"ctime":1546939107,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"10136868157","product_id":100020201,"comment_content":"1、DAGScheduler根据应用构造执行的DAG：是不是一个应用便构造一个DAG<br>2、DAG划分出计算阶段<br>3、每个计算阶段，根据要处理的数据量，为每个数据分片生成一个对应的任务，这些任务组成一个任务集合<br>4、每个任务分配一个进程<br>5、分布式计算：某个计算阶段，其某个任务的进程在集群某处执行计算<br>6、执行action函数便生成作业：依据DAG计算，是不是便可以生成多个作业<br><br>我这么理解可以吗？","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":435803,"discussion_content":"√","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1546939107,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":49827,"user_name":"Richard","can_delete":false,"product_type":"c1","uid":1316758,"ip_address":"","ucode":"893F958B9DD161","user_header":"https://static001.geekbang.org/account/avatar/00/14/17/96/846fc11b.jpg","comment_is_top":false,"comment_ctime":1544776619,"is_pvip":false,"replies":[{"id":"18000","content":"SQL的执行计划和spark的dag，都是执行描述，可以用文本查看的，不包括执行代码。<br><br>worker下载应用程序的jar包，反射加载执行。第三模块spark源码优化有详细描述。","user_name":"作者回复","comment_id":49827,"uid":"1007349","ip_address":"","utype":1,"ctime":1544843570,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"10134711211","product_id":100020201,"comment_content":"“DAGScheduler 根据程序代码生成 DAG，然后将程序分发到分布式计算集群，按计算阶段的先后关系调度执行。Worker 收到任务后，启动 Executor 进程开始执行任务。Executor 先检查自己是否有 Driver 的执行代码，如果没有，从 Driver 下载执行代码，通过 Java 反射加载后开始执行。” 针对这一段话，我想多请教老师一些，我理解的DAGScheduler 根据程序代码生成 DAG，类似于关系型数据库优化器根据SQL生成执行计划，然后spark计算引擎根据这些计划去做计算，我的疑惑的是：DAG已经是根据代码生成的了，那Worker 还要从 Driver 下载执行代码去执行，我无法想象worker是如何执行代码的，能否帮忙解疑一下？","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":432784,"discussion_content":"SQL的执行计划和spark的dag，都是执行描述，可以用文本查看的，不包括执行代码。\n\nworker下载应用程序的jar包，反射加载执行。第三模块spark源码优化有详细描述。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1544843570,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":44117,"user_name":"felix","can_delete":false,"product_type":"c1","uid":1086157,"ip_address":"","ucode":"DF514D0BB9B508","user_header":"https://static001.geekbang.org/account/avatar/00/10/92/cd/d39e568c.jpg","comment_is_top":false,"comment_ctime":1543370071,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10133304663","product_id":100020201,"comment_content":"老师有了解过微软sql server analysis service的tabular吗，内存型列式存储的数据库，数据压缩比很高。如果光做数据库查询的话，比spark还快很多，因为spark还要把数据load进内存。我们自己写了一个服务来分布聚合，还用redis以维度为key做缓存。","like_count":2},{"had_liked":false,"id":43982,"user_name":"Jack Zhu","can_delete":false,"product_type":"c1","uid":1266238,"ip_address":"","ucode":"8425B570880F28","user_header":"https://static001.geekbang.org/account/avatar/00/13/52/3e/a0614b62.jpg","comment_is_top":false,"comment_ctime":1543361247,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10133295839","product_id":100020201,"comment_content":"1.确定做开源的商业模式，是羊毛出在猪身上，还是送小羊推大羊等等<br>2.确定组织投入人员<br>3.建立社区，定期together讨论，为迭代版本做计划<br>4.做一些必要商业推广和免费客户服务","like_count":2},{"had_liked":false,"id":43696,"user_name":"Riordon","can_delete":false,"product_type":"c1","uid":1127497,"ip_address":"","ucode":"E2F6855B5FE5F9","user_header":"https://static001.geekbang.org/account/avatar/00/11/34/49/6b27feb1.jpg","comment_is_top":false,"comment_ctime":1543281858,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10133216450","product_id":100020201,"comment_content":"为何Spark可以更高效？<br>1）MR使用多进程模型，Spark使用多线程模型，这样Spark构建一个可重用的资源池，而MR往往不会，MR会耗费更多的构建资源池的启动时间，Spark在进程内线程会竞争资源，导致Spark相比MR不稳定；<br>2）对于多次读取的中间结果可以Cache，避免多次读盘，对于迭代计算友好；<br>另外shuffle过程，貌似不管是MR还是Spark都是需要写盘的吧<br>","like_count":2},{"had_liked":false,"id":138558,"user_name":"星一","can_delete":false,"product_type":"c1","uid":1103829,"ip_address":"","ucode":"EF550131FAE1BF","user_header":"https://static001.geekbang.org/account/avatar/00/10/d7/d5/6fbf1070.jpg","comment_is_top":false,"comment_ctime":1570335236,"is_pvip":false,"replies":[{"id":"53684","content":"是的，比如复杂的SQL，就会生成多个job，也就是多个map reduce过程。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1570602715,"ip_address":"","comment_id":138558,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5865302532","product_id":100020201,"comment_content":"老师您好，请问一下，对于复杂的多阶段计算，Hadoop MapReduce是需要进行多次map reduce 过程吗，而且每次map和reduce之间一定会进行shuffle（主要使用磁盘进行存储），所以相比Spark才更加低效，不知道我理解的正确嘛，请指教","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":469570,"discussion_content":"是的，比如复杂的SQL，就会生成多个job，也就是多个map reduce过程。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570602715,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":109390,"user_name":"羊小看","can_delete":false,"product_type":"c1","uid":1488453,"ip_address":"","ucode":"90F58F80A75520","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132","comment_is_top":false,"comment_ctime":1562029301,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5856996597","product_id":100020201,"comment_content":"spark的有向无环图，将一个阶段的输出，直接给到另一个阶段的输入，不需要再次启动应用，避免多次访问HDFS，避免多次调度，尤其是机器学习，上万次迭代的场景，且优先使用内存存储中间结果。","like_count":1},{"had_liked":false,"id":69284,"user_name":"forever","can_delete":false,"product_type":"c1","uid":1142325,"ip_address":"","ucode":"AAA930B496D5A0","user_header":"https://static001.geekbang.org/account/avatar/00/11/6e/35/11008e02.jpg","comment_is_top":false,"comment_ctime":1550711839,"is_pvip":false,"replies":[{"id":"24788","content":"在内存计算。<br><br>shuffle肯定要移动数据的。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1550813208,"ip_address":"","comment_id":69284,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5845679135","product_id":100020201,"comment_content":"老师spark是内存计算，是把数据都拿到内存算吗，shuffle的时候，如果两个父rdd不在同一个work节点，是不是需要把不同节点的数据移动到其中一个节点内存里，做聚合。这是不是也是移动数据","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440041,"discussion_content":"在内存计算。\n\nshuffle肯定要移动数据的。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1550813208,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1020557,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/92/8d/0c820dfe.jpg","nickname":"马踏飞机747","note":"","ucode":"BE94F887A7D077","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":24757,"discussion_content":"借楼补充提问：如果计算过程中内存资源不够用了，是会利用磁盘进行中间过程的数据存储和读写吗？这个过程又是怎样的处理机制呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570254894,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67298,"user_name":"chenssy","can_delete":false,"product_type":"c1","uid":1074196,"ip_address":"","ucode":"CB5AF27229E2D1","user_header":"https://static001.geekbang.org/account/avatar/00/10/64/14/c980c239.jpg","comment_is_top":false,"comment_ctime":1550126553,"is_pvip":false,"replies":[{"id":"23911","content":"作为应用可以跳过MapReduce，但是作为学习，最好不要跳过。MapReduce更简单，更直接表达了大数据计算的核心思想。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1550201189,"ip_address":"","comment_id":67298,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5845093849","product_id":100020201,"comment_content":"老师你好，是否可以过滤掉MapReduce，直接学习 spark","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439136,"discussion_content":"作为应用可以跳过MapReduce，但是作为学习，最好不要跳过。MapReduce更简单，更直接表达了大数据计算的核心思想。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550201189,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":49887,"user_name":"林骥","can_delete":false,"product_type":"c1","uid":1005101,"ip_address":"","ucode":"2E9BFD8E5DF1DB","user_header":"https://static001.geekbang.org/account/avatar/00/0f/56/2d/e6b9dd0f.jpg","comment_is_top":false,"comment_ctime":1544785113,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5839752409","product_id":100020201,"comment_content":"希望能讲讲实际怎么操作。","like_count":1},{"had_liked":false,"id":48013,"user_name":"足迹","can_delete":false,"product_type":"c1","uid":1105779,"ip_address":"","ucode":"38134D1A6B8DC2","user_header":"https://static001.geekbang.org/account/avatar/00/10/df/73/4a4ce2b5.jpg","comment_is_top":false,"comment_ctime":1544321684,"is_pvip":false,"replies":[{"id":"17343","content":"实践中，spark用一般yarn启动分发，不需要自己安装","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1544488251,"ip_address":"","comment_id":48013,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5839288980","product_id":100020201,"comment_content":"老师，根据大数据技术，程序移动，数据不动的原理。是不是应该在每个DataNode节点上面安装Spark呢？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":431988,"discussion_content":"实践中，spark用一般yarn启动分发，不需要自己安装","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1544488251,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":43649,"user_name":"sunlight001","can_delete":false,"product_type":"c1","uid":1126975,"ip_address":"","ucode":"A72C4274D5DE8A","user_header":"https://static001.geekbang.org/account/avatar/00/11/32/3f/fa4ac035.jpg","comment_is_top":false,"comment_ctime":1543276117,"is_pvip":false,"replies":[{"id":"15648","content":"谢谢支持～","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1543304754,"ip_address":"","comment_id":43649,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5838243413","product_id":100020201,"comment_content":"老师，多出几本书吧，我保证全买☺，这个专栏真值！","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430445,"discussion_content":"谢谢支持～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1543304754,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":355588,"user_name":"唐方刚","can_delete":false,"product_type":"c1","uid":3046392,"ip_address":"广东","ucode":"93DA58C3DCCF1B","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ugib9sF9icd9dhibQoAA025hibbD5zgZTiaddLoeEH457hrkBBhtQK6qknTWt270rHCtBZqeqsbibtHghgjdkPx3DyIw/132","comment_is_top":false,"comment_ctime":1661514745,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1661514745","product_id":100020201,"comment_content":"不知道理解的对不对，spark比mr快的原因有两点：<br>一是多个stage在一个job中，mr需要拆分成多个job。有个疑问❓如果大家都是单job，spark和mr的速度是不是就差不多了？<br>二是spark的中间结果不需要落盘，这个中间结果是指的shufle吗？但是我在极客时间的另一个老师的spark教程里面看到说spark的shuffle结果也是要落盘的，而且也分为数据文件和索引文件。不知道哪个是对的？还是说spark优先使用内存，内存不够了才落盘。<br>希望老师能解惑，这个问题困扰很久了，而且一般大家只说spark是内存计算所以快，这个回答太简单了，还是没法解决心中的疑问","like_count":0},{"had_liked":false,"id":340961,"user_name":"李勇","can_delete":false,"product_type":"c1","uid":1303796,"ip_address":"","ucode":"7BC9DE501890D4","user_header":"https://static001.geekbang.org/account/avatar/00/13/e4/f4/96fa9deb.jpg","comment_is_top":false,"comment_ctime":1649247149,"is_pvip":true,"replies":[{"id":"124668","content":"原文：因为 RDD B 在前面一个阶段，阶段 1 的 shuffle 过程中，已经进行了数据分区。分区数目和分区 Key 不变，就不需要再进行 shuffle。","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1649296061,"ip_address":"","comment_id":340961,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1649247149","product_id":100020201,"comment_content":"老师，有个问题想咨询一下，在rddB和RDDF join的过程中，RDDB是窄依赖，RDDF是宽依赖。也就是说RDDB直接平移到下一个RDD中，而RDDF会对RDDB存在的分片做数据交叉。这里我有个问题，为什么是RDDF做数据平移而不是RDDB呢？有什么规则可遵循的么？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":560353,"discussion_content":"原文：因为 RDD B 在前面一个阶段，阶段 1 的 shuffle 过程中，已经进行了数据分区。分区数目和分区 Key 不变，就不需要再进行 shuffle。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649296061,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":330475,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1641990294,"is_pvip":true,"replies":[{"id":"120576","content":"对","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1642071590,"ip_address":"","comment_id":330475,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1641990294","product_id":100020201,"comment_content":"感觉Spark 这个Driver 跟之前的东西都不一样了， 这个Driver 是在Cluster 之外的感觉， Cluster Mannger 是 资源的 提供者。 Job 的管理和执行完全是在集群外的Driver 自己处理，不知道这里理解对不？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545887,"discussion_content":"对","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1642071590,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1066752,"avatar":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","nickname":"piboye","note":"","ucode":"7CFD8712857A85","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":546125,"discussion_content":"谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642172064,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":545887,"ip_address":""},"score":546125,"extra":""}]}]},{"had_liked":false,"id":330470,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1641989221,"is_pvip":true,"replies":[{"id":"120575","content":"driver在spark集群之外，不受spark集群管理","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1642071494,"ip_address":"","comment_id":330470,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641989221","product_id":100020201,"comment_content":"老师driver也是运行在spark集群中吗？还是只是客户端的机器上？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545886,"discussion_content":"driver在spark集群之外，不受spark集群管理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642071495,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312101,"user_name":"钱鹏 Allen","can_delete":false,"product_type":"c1","uid":2518863,"ip_address":"","ucode":"7E95E82C0717DA","user_header":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","comment_is_top":false,"comment_ctime":1631625281,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1631625281","product_id":100020201,"comment_content":"Spark的高效体现在减少直接对HDFS的访问和将使用内存存储。<br>Spark两类函数，转换函数从一个RDD另外一个，action函数不返回RDD，生成job.","like_count":0},{"had_liked":false,"id":307665,"user_name":"许灵","can_delete":false,"product_type":"c1","uid":1059926,"ip_address":"","ucode":"0296EC9929B570","user_header":"https://static001.geekbang.org/account/avatar/00/10/2c/56/ff7a9730.jpg","comment_is_top":false,"comment_ctime":1629193954,"is_pvip":true,"replies":[{"id":"111467","content":"k8s相当于yarn，只是做资源管理，数据访问还是spark自己控制的。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1629268325,"ip_address":"","comment_id":307665,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1629193954","product_id":100020201,"comment_content":"老师，这里有一个疑问: 如果spark用k8s部署，spark是怎么与hdfs协作的呢？如何做到移动计算而不是移动数据，hadoop也要用k8s部署吗？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525233,"discussion_content":"k8s相当于yarn，只是做资源管理，数据访问还是spark自己控制的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629268325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1059926,"avatar":"https://static001.geekbang.org/account/avatar/00/10/2c/56/ff7a9730.jpg","nickname":"许灵","note":"","ucode":"0296EC9929B570","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390816,"discussion_content":"老师补充问一下：这样是不是指，只要k8s的spark worknode与hadoop的datanode部署在同一个节点上就可以了？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630056576,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":258214,"user_name":"艾利特-G","can_delete":false,"product_type":"c1","uid":1195204,"ip_address":"","ucode":"8C4DA0ABE77CDC","user_header":"https://static001.geekbang.org/account/avatar/00/12/3c/c4/4ee2968a.jpg","comment_is_top":false,"comment_ctime":1604326952,"is_pvip":false,"replies":[{"id":"94245","content":"������","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1604629925,"ip_address":"","comment_id":258214,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1604326952","product_id":100020201,"comment_content":"我入职了Cloudera，虽然股票让人感觉沮丧，我觉得产品和技术还是挺好的。<br>关于开源产品如何提升影响力，我觉得要和公有云搞好关系以及当前最火的行业开源软件合作。当然，这也是现在看到k8s这么火之后的马后炮啦～","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508582,"discussion_content":"������","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604629925,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":245240,"user_name":"抱小星","can_delete":false,"product_type":"c1","uid":1504652,"ip_address":"","ucode":"BA7B0DAFDA4AF5","user_header":"https://static001.geekbang.org/account/avatar/00/16/f5/8c/82fb5890.jpg","comment_is_top":false,"comment_ctime":1598873733,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1598873733","product_id":100020201,"comment_content":"这篇我也看了很久，结合Spark流程边运行边看的，后面看懂一点了，讲的很好，就是RDD流程那块还要再讲清楚一点就好了。<br><br>上一节和这一节说：代码中的RDD没有经过shuffle，物理上就不会生成新的RDD<br><br>那么当RDD转换时，数据量不一样的时候，是否会生成新的RDD？<br>我不太明白<br>比如RDD D和E生成F，没有经过shuffle，是否物理上会生成新的RDD？<br>","like_count":0},{"had_liked":false,"id":200037,"user_name":"草裡菌","can_delete":false,"product_type":"c1","uid":1241514,"ip_address":"","ucode":"E11C630ABA4370","user_header":"https://static001.geekbang.org/account/avatar/00/12/f1/aa/c29def94.jpg","comment_is_top":false,"comment_ctime":1585535610,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585535610","product_id":100020201,"comment_content":"开源自己的软件，用于提升自己公司的技术竞争力和影响力，如果是你负责人，你应该如何运作？<br><br>1.明确我们开源软件与市面上主流的的核心优势。<br>2.如果自己是大厂，则可以用自己的品牌背书。<br>3.主动提供技术支持，找合作伙伴公司产品线，以更高的技术指标、更长的技术支持期等为条件，使用开源软件提供服务实现。成功后可作为宣传背书。<br>4.建立官方社区技术支持，Github仓库维护团队。<br>5.定期开展技术论坛。<br><br>瞎猜的↑","like_count":0},{"had_liked":false,"id":196101,"user_name":"Geek_b8928e","can_delete":false,"product_type":"c1","uid":1926597,"ip_address":"","ucode":"96E4ABE3F2F145","user_header":"","comment_is_top":false,"comment_ctime":1585235630,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585235630","product_id":100020201,"comment_content":"Spark 有三个主要特性：RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效","like_count":0},{"had_liked":false,"id":143259,"user_name":"半瓶醋","can_delete":false,"product_type":"c1","uid":1282746,"ip_address":"","ucode":"8C898E244D0417","user_header":"https://static001.geekbang.org/account/avatar/00/13/92/ba/9833f06f.jpg","comment_is_top":false,"comment_ctime":1571657825,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1571657825","product_id":100020201,"comment_content":"我发现有些点之前看的时候并不清楚，做了几个Demo后才真正的理解了老师的意思。之前老师讲的5-20-2很受益！","like_count":0},{"had_liked":false,"id":142692,"user_name":"李凯","can_delete":false,"product_type":"c1","uid":1304705,"ip_address":"","ucode":"FA57F4645D0639","user_header":"https://static001.geekbang.org/account/avatar/00/13/e8/81/6692218b.jpg","comment_is_top":false,"comment_ctime":1571460432,"is_pvip":false,"replies":[{"id":"55269","content":"每个任务一个进程，但是这个进程是否需要启动，要看Spark的部署方式。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1571626552,"ip_address":"","comment_id":142692,"utype":1}],"discussion_count":1,"race_medal":1,"score":"1571460432","product_id":100020201,"comment_content":"之后再根据每个阶段要处理的数据量生成相应的任务集合（TaskSet），每个任务都分配一个任务进程去处理，Spark 就实现了大数据的分布式计算。<br>我有点不明白，每个任务不应该是一个线程处理吗？难道你这个任务是指任务集合，所以要启动一个进城？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":471230,"discussion_content":"每个任务一个进程，但是这个进程是否需要启动，要看Spark的部署方式。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571626552,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":138562,"user_name":"星一","can_delete":false,"product_type":"c1","uid":1103829,"ip_address":"","ucode":"EF550131FAE1BF","user_header":"https://static001.geekbang.org/account/avatar/00/10/d7/d5/6fbf1070.jpg","comment_is_top":false,"comment_ctime":1570336780,"is_pvip":false,"replies":[{"id":"53683","content":"是的，后面spark性能优化还会继续详细讨论这些性能曲线。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1570602655,"ip_address":"","comment_id":138562,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1570336780","product_id":100020201,"comment_content":"老师您好，请教一下，上面关于作业、计算阶段、任务的依赖和时间关系图中，说到，两条粗黑线之间是一个作业，两条细线直接是一个计算阶段，那两个细线之间夹了一条粗黑线这个怎么算。。我的理解是分属两个作业的各一个计算阶段，不知道我的理解正确嘛","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":469572,"discussion_content":"是的，后面spark性能优化还会继续详细讨论这些性能曲线。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570602655,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":137193,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1569643231,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1569643231","product_id":100020201,"comment_content":"如果是我负责人，我会这样运作：<br>1：让我们的核心开发人员在极客时间开一个专栏，讲解类似的产品，然后不断的提及我们将要开源的项目，并大概讲解一下她的牛逼之处，好像正在这么做<br>2：首先，确认开源的项目确实已经在公司内提供了许多的价值久经考验，然后就先在公司内造势，使人人都是宣传大使<br>3：做好开源的前提工作，比如：开源那一部分、怎么达到目标、用例、介绍、特性、代码都准备好<br>4：花钱找微信微博自媒体知乎博客等地方宣传<br>5：做分享尤其是大型的互联网大会<br>6：开设使用教程音频视频文本<br>7：请大一点公司使用站台，请有影响力的技术大咖站台比如：智慧老师，有钱就好办事，如果公司小资金少，那就靠产品本身的能量加时间的积累","like_count":0},{"had_liked":false,"id":136593,"user_name":"鲁·本","can_delete":false,"product_type":"c1","uid":1209939,"ip_address":"","ucode":"F1DEB30C21B48E","user_header":"https://static001.geekbang.org/account/avatar/00/12/76/53/21d62a23.jpg","comment_is_top":false,"comment_ctime":1569477432,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1569477432","product_id":100020201,"comment_content":"一篇文章比人家一本书讲的都通透","like_count":0},{"had_liked":false,"id":105429,"user_name":"乐毅","can_delete":false,"product_type":"c1","uid":1348052,"ip_address":"","ucode":"6361ADC9B0E8D5","user_header":"https://static001.geekbang.org/account/avatar/00/14/91/d4/3785c799.jpg","comment_is_top":false,"comment_ctime":1560997906,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560997906","product_id":100020201,"comment_content":"Spark这个RDD感觉和神经网络很像啊","like_count":0},{"had_liked":false,"id":102689,"user_name":"利","can_delete":false,"product_type":"c1","uid":1346711,"ip_address":"","ucode":"561249904EEED5","user_header":"https://static001.geekbang.org/account/avatar/00/14/8c/97/f00593ab.jpg","comment_is_top":false,"comment_ctime":1560297347,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560297347","product_id":100020201,"comment_content":"你好，spark的文件系统用的啥？hdfs么","like_count":0},{"had_liked":false,"id":96986,"user_name":"周小桥","can_delete":false,"product_type":"c1","uid":1062408,"ip_address":"","ucode":"80875AE7675695","user_header":"https://static001.geekbang.org/account/avatar/00/10/36/08/e3ed94b8.jpg","comment_is_top":false,"comment_ctime":1558569975,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558569975","product_id":100020201,"comment_content":"聚合操作是它最大的缺点。","like_count":0},{"had_liked":false,"id":96979,"user_name":"周小桥","can_delete":false,"product_type":"c1","uid":1062408,"ip_address":"","ucode":"80875AE7675695","user_header":"https://static001.geekbang.org/account/avatar/00/10/36/08/e3ed94b8.jpg","comment_is_top":false,"comment_ctime":1558569329,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558569329","product_id":100020201,"comment_content":"拥有大厂的支持，是最好的推广。","like_count":0},{"had_liked":false,"id":85786,"user_name":"纳兰残德","can_delete":false,"product_type":"c1","uid":1134022,"ip_address":"","ucode":"2CCEA4539748B4","user_header":"https://static001.geekbang.org/account/avatar/00/11/4d/c6/d4e85081.jpg","comment_is_top":false,"comment_ctime":1555232791,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555232791","product_id":100020201,"comment_content":"spark on yarn的时候 执行过程中 哪些datanode可以启动executor 分别启动几个 应该是resourcemanager控制的吧？老师说的 应该是到了task层面，worker去认领task？不知道我是否理解有误。","like_count":0},{"had_liked":false,"id":80091,"user_name":"珠闪闪","can_delete":false,"product_type":"c1","uid":1300447,"ip_address":"","ucode":"45BE0D586A3839","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eov38ZkwCyNoBdr5drgX0cp2eOGCv7ibkhUIqCvcnFk8FyUIS6K4gHXIXh0fu7TB67jaictdDlic4OwQ/132","comment_is_top":false,"comment_ctime":1553604075,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1553604075","product_id":100020201,"comment_content":"之前几节课都能看懂，到spark这节突然感觉门槛很高，不好看懂入门；请问李老师，有什么方法更容易看懂，或者入门spark吗","like_count":0},{"had_liked":false,"id":63157,"user_name":"渔夫","can_delete":false,"product_type":"c1","uid":1019289,"ip_address":"","ucode":"6BF08FD8923E45","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/99/f886543d.jpg","comment_is_top":false,"comment_ctime":1548291374,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548291374","product_id":100020201,"comment_content":"我觉得开源成功的一个重要前提要有经典的使用案例，确实能解决当下的软件开发的一些痛点，那她就一定能成功——就像RoR之于Ruby","like_count":0},{"had_liked":false,"id":62158,"user_name":"eldon","can_delete":false,"product_type":"c1","uid":1368116,"ip_address":"","ucode":"3165FDE0FACB9A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLeKdLZTWmcxDE7AUnM90naTbDzynshqzILrQAweQXicGgvdg1gImWxeZabiay9LVLsnOCfjj2nZaBA/132","comment_is_top":false,"comment_ctime":1547964311,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547964311","product_id":100020201,"comment_content":"开源原来也会对公司有这么多好处，本学生受益匪浅","like_count":0},{"had_liked":false,"id":58817,"user_name":"Stream","can_delete":false,"product_type":"c1","uid":1044489,"ip_address":"","ucode":"CEBB83F7768736","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f0/09/fc34d3bd.jpg","comment_is_top":false,"comment_ctime":1547199997,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547199997","product_id":100020201,"comment_content":"就是应该这样讲原理，最好能更细节一些，比如原始数据是HDFS一个12个分区，DAG时描述仔细数据的流行和转化完的结果，这样更具体。至于操作直接看官网二下就会了，再就是不断的实践。","like_count":0},{"had_liked":false,"id":57803,"user_name":"木白","can_delete":false,"product_type":"c1","uid":1194009,"ip_address":"","ucode":"BEC459430B293C","user_header":"https://static001.geekbang.org/account/avatar/00/12/38/19/c8d72c61.jpg","comment_is_top":false,"comment_ctime":1546913957,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1546913957","product_id":100020201,"comment_content":"关于开源软件的运作，我没有接触过但是可以猜一下，不一定对。就像一个商业软件一样，首先需要积累原始用户，这个阶段就首先可以向正在使用相关技术的公司的员工推广，请他们来试用，这个推广可以是线上的，使用内测或者邀请码的形式在社区或者论坛邀请，也可以登门做试用宣讲，吸引用户。这批用户可能GitHub上进行相关讨论，然后吸引更多的用户。还可以赞助领域相关的学术会议。当用户量达到一定的规模、软件较为成熟、在实验室环境下能够解决一些问题的时候就可以召开自己的学术会议，吸引投稿，扩大影响力。等等等等。","like_count":0},{"had_liked":false,"id":57799,"user_name":"木白","can_delete":false,"product_type":"c1","uid":1194009,"ip_address":"","ucode":"BEC459430B293C","user_header":"https://static001.geekbang.org/account/avatar/00/12/38/19/c8d72c61.jpg","comment_is_top":false,"comment_ctime":1546913396,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1546913396","product_id":100020201,"comment_content":"我有少量的spark使用经验，并且也了解过本章所涉及到的原理，但是看起来还是有点费劲，所以我觉得如果真的是初学者的话，看起来只会更费劲。从最近两篇的留言来看我这种推断也不无道理。我觉得老师还是需要在某些地方更加详细一点，比如在举例子的时候用具体的数据去代替小方块所表示的数据分片。但是总体来说老师还是讲得很好的。","like_count":0},{"had_liked":false,"id":57166,"user_name":"王超","can_delete":false,"product_type":"c1","uid":1295135,"ip_address":"","ucode":"DFAD7F33369F64","user_header":"https://static001.geekbang.org/account/avatar/00/13/c3/1f/f1b88ff9.jpg","comment_is_top":false,"comment_ctime":1546670183,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1546670183","product_id":100020201,"comment_content":"请教老师，看到一个观点，以前移动计算比移动数据更好。但是现在有一些新的变化，比如数据的移动速度成本比几年前低很多，而Hadoop框架下计算资源（相对于存储资源）浪费较多，因为计算和存储资源是配比的，所以传统的Hadoop没有那么优了。请教老师是这样吗，以及更优的解决方案是什么？谢谢！","like_count":0},{"had_liked":false,"id":46706,"user_name":"ming","can_delete":false,"product_type":"c1","uid":1054341,"ip_address":"","ucode":"63135A86598E64","user_header":"https://static001.geekbang.org/account/avatar/00/10/16/85/e4d53282.jpg","comment_is_top":false,"comment_ctime":1543973174,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543973174","product_id":100020201,"comment_content":"spark相比mapreduce，1、使用多个map和reduce按照业务需求组合成更加高效、灵活的阶段。2、能使用内存交互数据的就不使用磁盘，速度更快。","like_count":0},{"had_liked":false,"id":46499,"user_name":"亚亚","can_delete":false,"product_type":"c1","uid":1215843,"ip_address":"","ucode":"C318200897EBAF","user_header":"https://static001.geekbang.org/account/avatar/00/12/8d/63/0ce0afee.jpg","comment_is_top":false,"comment_ctime":1543930781,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543930781","product_id":100020201,"comment_content":"spark的中间数据都存储在内存中，shuffle过程也是在内存中处理的，分布式情况下，内存也要实现分布式吧，比如redis集群，不然多个机器之间就不能直接通过内存进行数据传输，不知道spark是怎么处理的","like_count":0},{"had_liked":false,"id":45615,"user_name":"杰之7","can_delete":false,"product_type":"c1","uid":1297232,"ip_address":"","ucode":"F7DA2E21085332","user_header":"https://static001.geekbang.org/account/avatar/00/13/cb/50/66d0bd7f.jpg","comment_is_top":false,"comment_ctime":1543722374,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543722374","product_id":100020201,"comment_content":"通过本节的学习，了解了Spark的架构原理，Spark通过产生多阶段的有向无环图DAG，让前一个阶段的reduce和后一阶段的Map连接起来，有限减少对HDFS的访问，这样可以让计算速度加快。在Spark产生的时间点上，可以让Spark用内存存储，而不是Mapreduce的磁盘存储，加快了读写速度。编程模型上的优势，也让Spark更胜mapreduce。","like_count":0},{"had_liked":false,"id":45181,"user_name":"ray","can_delete":false,"product_type":"c1","uid":1056436,"ip_address":"","ucode":"76DAA3FACF49E2","user_header":"https://static001.geekbang.org/account/avatar/00/10/1e/b4/93966675.jpg","comment_is_top":false,"comment_ctime":1543557388,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543557388","product_id":100020201,"comment_content":"老师能否理论讲完后，有一些实践课。作为初学者，只是将理论感觉有点枯燥和心里没底。","like_count":0},{"had_liked":false,"id":43892,"user_name":"往事随风，顺其自然","can_delete":false,"product_type":"c1","uid":1235692,"ip_address":"","ucode":"F266EC6B143E38","user_header":"https://static001.geekbang.org/account/avatar/00/12/da/ec/779c1a78.jpg","comment_is_top":false,"comment_ctime":1543319160,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543319160","product_id":100020201,"comment_content":"Spark 的广播变量实现机制是什么","like_count":0},{"had_liked":false,"id":43887,"user_name":"追梦小乐","can_delete":false,"product_type":"c1","uid":1035507,"ip_address":"","ucode":"82AB8A7F98389D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/cc/f3/5e4b0315.jpg","comment_is_top":false,"comment_ctime":1543318123,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543318123","product_id":100020201,"comment_content":"老师，我想请教几个问题：<br>1、“根据每个阶段要处理的数据量生成相应的任务集合（TaskSet），每个任务都分配一个任务进程去处理”，是一个任务集合TaskSet启动一个进程，taskSet里面的任务是用线程计算吗？还是每个TaskSet里面的任务启动一个进程？<br><br>2、task-time 图中红色密集的表示什么？<br><br>3、Spark 的执行过程 的图中 Executor 中的 Cache 是表示内存吗？Cache、task和Executor之间的关系是什么？","like_count":0},{"had_liked":false,"id":43672,"user_name":"吴科🍀","can_delete":false,"product_type":"c1","uid":1112547,"ip_address":"","ucode":"8F2C317887A323","user_header":"https://static001.geekbang.org/account/avatar/00/10/f9/e3/2529c7dd.jpg","comment_is_top":false,"comment_ctime":1543279816,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543279816","product_id":100020201,"comment_content":"参加过spark meetingup，免费交流的平台。现在spark源码都被databricks这个商业公司把持了吧","like_count":0}]}