{"id":96256,"title":"15 | Spark SQL：Spark数据查询的利器","content":"<p>你好，我是蔡元楠。</p><p>上一讲中，我介绍了弹性分布式数据集的特性和它支持的各种数据操作。</p><p>不过在实际的开发过程中，我们并不是总需要在RDD的层次进行编程。</p><p>就好比编程刚发明的年代，工程师只能用汇编语言，到后来才慢慢发展出高级语言，如Basic、C、Java等。使用高级语言大大提升了开发者的效率。</p><p>同样的，Spark生态系统也提供很多库，让我们在不同的场景中使用。</p><p>今天，让我们来一起探讨Spark最常用的数据查询模块——Spark SQL。</p><h2>Spark SQL 发展历史</h2><p>几年前，Hadoop/MapReduce在企业生产中的大量使用，HDFS上积累了大量数据。</p><p>由于MapReduce对于开发者而言使用难度较大，大部分开发人员最熟悉的还是传统的关系型数据库。</p><p>为了方便大多数开发人员使用Hadoop，Hive应运而生。</p><p>Hive提供类似SQL的编程接口，HQL语句经过语法解析、逻辑计划、物理计划转化成MapReduce程序执行，使得开发人员很容易对HDFS上存储的数据进行查询和分析。</p><p>在Spark刚问世的时候，Spark团队也开发了一个Shark来支持用SQL语言来查询Spark的数据。</p><p>Shark的本质就是Hive，它修改了Hive的内存管理模块，大幅优化了运行速度，是Hive的10倍到100倍之多。</p><!-- [[[read_end]]] --><p>但是，Shark对于Hive的依赖严重影响了Spark的发展。Spark想要定义的是一个统一的技术栈和完整的生态，不可能允许有这样的外在依赖。</p><p>试想，如果Spark想发布新的功能还需要等Hive的更新，那么势必会很难执行。此外，依赖于Hive还制约了Spark各个组件的相互集成，Shark也无法利用Spark的特性进行深度优化。</p><p><img src=\"https://static001.geekbang.org/resource/image/68/75/68a739ff869d714a32c7760d1a439a75.png?wh=4758*2667\" alt=\"\"></p><p>所以，2014年7月1日，Spark团队就将Shark交给Hive进行管理，转而开发了SparkSQL。</p><p>SparkSQL摒弃了Shark的（将SQL语句转化为Spark RDD的）执行引擎，换成自己团队重新开发的执行引擎。</p><p>Spark SQL不仅将关系型数据库的处理模式和Spark的函数式编程相结合，还兼容多种数据格式，包括Hive、RDD、JSON文件、CSV文件等。</p><p>可以说，Spark SQL的问世大大加快了Spark生态的发展。</p><h2>Spark SQL的架构</h2><p>Spark SQL本质上是一个库。它运行在Spark的核心执行引擎之上。</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/13/3bdb29b1d697e3530d1efbd05e694e13.png?wh=4758*2667\" alt=\"\"></p><p>如上图所示，它提供类似于SQL的操作接口，允许数据仓库应用程序直接获取数据，允许使用者通过命令行操作来交互地查询数据，还提供两个API：DataFrame API和DataSet API。</p><p>Java、Python和Scala的应用程序可以通过这两个API来读取和写入RDD。</p><p>此外，正如我们在上一讲介绍的，应用程序还可以直接操作RDD。</p><p>使用Spark SQL会让开发者觉得好像是在操作一个关系型数据库一样，而不是在操作RDD。这是它优于原生的RDD API的地方。</p><p>与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了关于数据结构和正在执行的计算的更多信息。</p><p>在内部，Spark SQL使用这些额外的信息来执行额外的优化。虽然Spark SQL支持多种交互方式，但是在计算结果时均使用相同的执行引擎。</p><p>这种统一意味着开发人员可以轻松地在不同的API之间来回切换，基于这些API提供了表达给定转换的最自然的方式。</p><p>接下来让我们进一步了解DataSet和DataFrame。</p><h2>DataSet</h2><p>DataSet，顾名思义，就是数据集的意思，它是Spark 1.6新引入的接口。</p><p>同弹性分布式数据集类似，DataSet也是不可变分布式的数据单元，它既有与RDD类似的各种转换和动作函数定义，而且还享受Spark SQL优化过的执行引擎，使得数据搜索效率更高。</p><p>DataSet支持的转换和动作也和RDD类似，比如map、filter、select、count、show及把数据写入文件系统中。</p><p>同样地，DataSet上的转换操作也不会被立刻执行，只是先生成新的DataSet，只有当遇到动作操作，才会把之前的转换操作一并执行，生成结果。</p><p>所以，DataSet的内部结构包含了逻辑计划，即生成该数据集所需要的运算。</p><p>当动作操作执行时，Spark SQL的查询优化器会优化这个逻辑计划，并生成一个可以分布式执行的、包含分区信息的物理计划。</p><p>那么，DataSet和RDD的区别是什么呢？</p><p>通过之前的叙述，我们知道DataSet API是Spark SQL的一个组件。那么，你应该能很容易地联想到，DataSet也具有关系型数据库中表的特性。</p><p>是的，DataSet所描述的数据都被组织到有名字的列中，就像关系型数据库中的表一样。</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/e3/5a6fd6e91c92c166d279711bf9c761e3.png?wh=4758*2286\" alt=\"\"></p><p>如上图所示，左侧的RDD虽然以People为类型参数，但Spark框架本身不了解People类的内部结构。所有的操作都以People为单位执行。</p><p>而右侧的DataSet却提供了详细的结构信息与每列的数据类型。</p><p>这让Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。也就是说，DataSet提供数据表的schema信息。这样的结构使得DataSet API的执行效率更高。</p><p>试想，如果我们要查询People的年龄信息，Spark SQL执行的时候可以依靠查询优化器仅仅把需要的那一列取出来，其他列的信息根本就不需要去读取了。所以，有了这些信息以后在编译的时候能够做更多的优化。</p><p>其次，由于DataSet存储了每列的数据类型。所以，在程序编译时可以执行类型检测。</p><h2>DataFrame</h2><p>DataFrame可以被看作是一种特殊的DataSet。它也是关系型数据库中表一样的结构化存储机制，也是分布式不可变的数据结构。</p><p>但是，它的每一列并不存储类型信息，所以在编译时并不能发现类型错误。DataFrame每一行的类型固定为Row，他可以被当作DataSet[Row]来处理，我们必须要通过解析才能获取各列的值。</p><p>所以，对于DataSet我们可以用类似people.name来访问一个人的名字，而对于DataFrame我们一定要用类似people.get As [String] (“name”)来访问。</p><h2>RDD、DataFrame、DataSet对比</h2><p>学习Spark到现在，我们已经接触了三种基本的数据结构：RDD、DataFrame和DataSet。接下来你的表格中，你可以看到它们的异同点，思考一下怎样在实际工程中选择。</p><p><img src=\"https://static001.geekbang.org/resource/image/40/ef/40691757146e1b480e08969e676644ef.png?wh=4758*1843\" alt=\"\"></p><h3>发展历史</h3><p>从发展历史上来看，RDD API在第一代Spark中就存在，是整个Spark框架的基石。</p><p>接下来，为了方便熟悉关系型数据库和SQL的开发人员使用，在RDD的基础上，Spark创建了DataFrame API。依靠它，我们可以方便地对数据的列进行操作。</p><p>DataSet最早被加入Spark SQL是在Spark 1.6，它在DataFrame的基础上添加了对数据的每一列的类型的限制。</p><p>在Spark 2.0中，DataFrame和DataSet被统一。DataFrame作为DataSet[Row]存在。在弱类型的语言，如Python中，DataFrame API依然存在，但是在Java中，DataFrame API已经不复存在了。</p><h3>不变性与分区</h3><p>由于DataSet和DataFrame都是基于RDD的，所以它们都拥有RDD的基本特性，在此不做赘述。而且我们可以通过简单的 API在 DataFrame或 Dataset与RDD之间进行无缝切换。</p><h3>性能</h3><p>DataFrame和DataSet的性能要比RDD更好。</p><p>Spark程序运行时，Spark SQL中的查询优化器会对语句进行分析，并生成优化过的RDD在底层执行。</p><p>举个例子，如果我们想先对一堆数据进行GroupBy再进行Filter操作，这无疑是低效的，因为我们并不需要对所有数据都GroupBy。</p><p>如果用RDD API实现这一语句，在执行时它只会机械地按顺序执行。而如果用DataFrame/DataSet API，Spark SQL的Catalyst优化器会将Filter操作和GroupBy操作调换顺序，从而提高执行效率。</p><p>下图反映了这一优化过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/d6/42/d6162126bae14517aa163b3885c13a42.png?wh=4758*1693\" alt=\"\"></p><h3>错误检测</h3><p>RDD和DataSet都是类型安全的，而DataFrame并不是类型安全的。这是因为它不存储每一列的信息如名字和类型。</p><p>使用DataFrame API时，我们可以选择一个并不存在的列，这个错误只有在代码被执行时才会抛出。如果使用DataSet API，在编译时就会检测到这个错误。</p><h2>小结</h2><p>DataFrame和DataSet是Spark SQL提供的基于RDD的结构化数据抽象。</p><p>它既有RDD不可变、分区、存储依赖关系等特性，又拥有类似于关系型数据库的结构化信息。</p><p>所以，基于DataFrame和DataSet API开发出的程序会被自动优化，使得开发人员不需要操作底层的RDD API来进行手动优化，大大提升开发效率。</p><p>但是RDD API对于非结构化的数据处理有独特的优势，比如文本流数据，而且更方便我们做底层的操作。所以在开发中，我们还是需要根据实际情况来选择使用哪种API。</p><h2>思考题</h2><p>什么场景适合使用DataFrame API，什么场景适合使用DataSet API？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":96602,"user_name":":)","can_delete":false,"product_type":"c1","uid":1239198,"ip_address":"","ucode":"23D505949442B6","user_header":"https://static001.geekbang.org/account/avatar/00/12/e8/9e/6550a051.jpg","comment_is_top":false,"comment_ctime":1558457490,"is_pvip":false,"replies":[{"id":"48474","content":"很好的总结","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567373759,"ip_address":"","comment_id":96602,"utype":1}],"discussion_count":2,"race_medal":0,"score":"143292378258","product_id":100025301,"comment_content":"任何一个生命力的存在与发展都源于历史背景下的需求。<br>1.  hive sql虽好，但是限制了spark的发展，hive sql已经满足不了广发人民群众对spark sql的的强烈需求，我们需要进入社会主义社会，你却还是用石头打猎，，不行滴，，<br>2. RDD是spark这座大厦的基石，那块砖就是RDD。RDD赋予了灵活性，但带来了另一个问题就是复杂性。灵活性和复杂性是一对永久不变的矛盾。<br>3.Dataset与DataFrame。我现在要建个房子，你直接给我4面墙一个顶就够了，不要一块砖一块砖的给我，我不但很懒而且很笨。Dataset与DataFrame是根据通用结构化数据处理的建立在RDD之上的封装。带来的便易性但是降低了一定的灵活性。","like_count":32,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450930,"discussion_content":"很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567373759,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":10961,"discussion_content":"课代表请坐。\n①Shark的故事告诉我们，在进行技术选型时，不仅要考虑难易程度，还需考虑未来的路；\n②Sql的故事告诉我们，共识始终是重要的因素，和广大人民群众站在一起，而群众偏爱继承、规范、共识、简洁；\n③鱼和熊掌不可兼得么，做好分类就可以，饿坏了就给熊掌(dataset),随便两口就容易满足，小资的话就给鱼（RDD），轻挑复撮，慢慢品。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1568343547,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96644,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1558483806,"is_pvip":false,"replies":[{"id":"48501","content":"嗯，基本上说的没错，而且在需要类型检测的时候要用DataSet。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567384520,"ip_address":"","comment_id":96644,"utype":1}],"discussion_count":1,"race_medal":0,"score":"104637698910","product_id":100025301,"comment_content":"待处理数据的schema是静态的且对其完全掌控的情况下用DataSet，反之用DataFrame","like_count":25,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450950,"discussion_content":"嗯，基本上说的没错，而且在需要类型检测的时候要用DataSet。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567384520,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96612,"user_name":"涵","can_delete":false,"product_type":"c1","uid":1502742,"ip_address":"","ucode":"BB8575DB13F1E0","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg","comment_is_top":false,"comment_ctime":1558466445,"is_pvip":false,"replies":[{"id":"48502","content":"👍🏻👍🏻","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567384550,"ip_address":"","comment_id":96612,"utype":1}],"discussion_count":2,"race_medal":0,"score":"74572910477","product_id":100025301,"comment_content":"老师好，我猜想DataSet适用于每列类型程序都很确定时使用，而DataFrame适用于列的类型不确定，类似于generic的类型，拿到数据后根据数据类型再进行不同的处理。是否可以这样理解?","like_count":18,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450936,"discussion_content":"👍🏻👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567384550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382946,"discussion_content":"列的类型怎么会不确定的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625803815,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96737,"user_name":"Geek_9319","can_delete":false,"product_type":"c1","uid":1535449,"ip_address":"","ucode":"90C84C0048D73F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaELtsib7NQO2NaEubNbfbAqh8FQDibTuUon1QNHvbV1vD1Vo9vO8wymibTria6nIO3hakJNfrY1okseuRQ/132","comment_is_top":false,"comment_ctime":1558494968,"is_pvip":false,"replies":[{"id":"48500","content":"DataFrame可以看做是DataSet的无类型特例，认为每一行都是一个Row object。但是先提出的是DataFrame, 后来才发展出了DataSet并将两者统一。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567384409,"ip_address":"","comment_id":96737,"utype":1}],"discussion_count":2,"race_medal":0,"score":"27328298744","product_id":100025301,"comment_content":"到底 DataSet是DataFrame的特例？还是DataFrame 是DataSet的特例？","like_count":6,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450996,"discussion_content":"DataFrame可以看做是DataSet的无类型特例，认为每一行都是一个Row object。但是先提出的是DataFrame, 后来才发展出了DataSet并将两者统一。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567384409,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1518906,"avatar":"https://static001.geekbang.org/account/avatar/00/17/2d/3a/412825c9.jpg","nickname":"子系","note":"","ucode":"CF1149D8EE02F5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333744,"discussion_content":"应该理解为 dataset 继承自dataframe","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607608046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":125427,"user_name":"leesper","can_delete":false,"product_type":"c1","uid":1351104,"ip_address":"","ucode":"CC76320571BECA","user_header":"https://static001.geekbang.org/account/avatar/00/14/9d/c0/cb5341ec.jpg","comment_is_top":false,"comment_ctime":1566183286,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14451085174","product_id":100025301,"comment_content":"感觉这就是传说中的Schema-on-read，传统的数据仓库只能存储结构化数据，对于非结构化数据的，后来有了data lake","like_count":4},{"had_liked":false,"id":103446,"user_name":"珅剑","can_delete":false,"product_type":"c1","uid":1504220,"ip_address":"","ucode":"4290D5C140F80F","user_header":"https://static001.geekbang.org/account/avatar/00/16/f3/dc/80b0cd23.jpg","comment_is_top":false,"comment_ctime":1560436497,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14445338385","product_id":100025301,"comment_content":"DataSet在需要访问列中的某个字段时是非常方便的，但是如果要写一些适配性很强的函数时，如果使用DataSet，行的类型无法在编译时确定，可能是各种case class，这时候用DataFrame就可以比较好地解决问题","like_count":3},{"had_liked":false,"id":96902,"user_name":"朱同学","can_delete":false,"product_type":"c1","uid":1514233,"ip_address":"","ucode":"2EF7D5A051712C","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/f9/180f347a.jpg","comment_is_top":false,"comment_ctime":1558532244,"is_pvip":false,"replies":[{"id":"48473","content":"说的很对，DataFrame的功能可以看做是Dataset的子集。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567373683,"ip_address":"","comment_id":96902,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14443434132","product_id":100025301,"comment_content":"大部分情况下用dataset都没问题，限制因素主要是spark版本","like_count":4,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451055,"discussion_content":"说的很对，DataFrame的功能可以看做是Dataset的子集。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567373683,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":100610,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1559606156,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5854573452","product_id":100025301,"comment_content":"实际上如果只是因为hive的不足，为什么不用impala代替，性能也比sparklsql高","like_count":1,"discussions":[{"author":{"id":1970391,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/10/d7/65ff6e5d.jpg","nickname":"机智的李赛艇","note":"","ucode":"66794F6816DD26","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":344411,"discussion_content":"impala是mpp的，本身是有瓶颈的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611456411,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":97563,"user_name":"brv","can_delete":false,"product_type":"c1","uid":1254213,"ip_address":"","ucode":"DD94D8C2085EAB","user_header":"https://static001.geekbang.org/account/avatar/00/13/23/45/3addffe7.jpg","comment_is_top":false,"comment_ctime":1558695930,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5853663226","product_id":100025301,"comment_content":"dataset是在dateframe基础上发展起来的，那dateset是dateframe的特例","like_count":1},{"had_liked":false,"id":96661,"user_name":"RocWay","can_delete":false,"product_type":"c1","uid":1088024,"ip_address":"","ucode":"377CD114BABBF7","user_header":"https://static001.geekbang.org/account/avatar/00/10/9a/18/3596069c.jpg","comment_is_top":false,"comment_ctime":1558485850,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5853453146","product_id":100025301,"comment_content":"老师你好，请教个问题：既然dataframe没有存储具体的类型，那么是否可以认为dataframe具有动态语言的特性？也就是说当数据类型变化后，程序能够自动适应。实现起来可否这样：先判断某个名字的字段是否存在，再做出相应的动作？","like_count":1},{"had_liked":false,"id":247864,"user_name":"茂杨","can_delete":false,"product_type":"c1","uid":1181344,"ip_address":"","ucode":"8D8259E905DCA3","user_header":"https://static001.geekbang.org/account/avatar/00/12/06/a0/3da0e315.jpg","comment_is_top":false,"comment_ctime":1599896651,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1599896651","product_id":100025301,"comment_content":"越来越喜欢spark了, 有了dataframe和dataset，就可以用sql这种函数式语言做项目了","like_count":1},{"had_liked":false,"id":209104,"user_name":"娄江国","can_delete":false,"product_type":"c1","uid":1018479,"ip_address":"","ucode":"6C2AAE4E409286","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8a/6f/3d4f7e31.jpg","comment_is_top":false,"comment_ctime":1587504524,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1587504524","product_id":100025301,"comment_content":"如果做数据仓库用sparksql做etl，那么数据以什么格式存储最好呢？","like_count":0},{"had_liked":false,"id":201596,"user_name":"平安喜乐","can_delete":false,"product_type":"c1","uid":1102350,"ip_address":"","ucode":"B97D8944B0B035","user_header":"https://static001.geekbang.org/account/avatar/00/10/d2/0e/26bf35a4.jpg","comment_is_top":false,"comment_ctime":1585809848,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585809848","product_id":100025301,"comment_content":"使用StructField(filedName, StringType, nullable = true)<br>或者val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS(）<br>如果要查整个表的字段  怎么动态的填充数据字段对应的类型","like_count":0},{"had_liked":false,"id":157746,"user_name":"坤2021牛","can_delete":false,"product_type":"c1","uid":1676097,"ip_address":"","ucode":"A26B321F300785","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/cKeYVTKJCJhrfTCBkEGla1WA7W0S9FPZrTR3ovYJFhcKo7kl72gR9VibCufhHsjOar2Z6mZlFKb8VEkkDv9lqVA/132","comment_is_top":false,"comment_ctime":1575254736,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1575254736","product_id":100025301,"comment_content":"大部分情况都是用dataset和dataframe,rdd的用于操作细节的地方","like_count":0},{"had_liked":false,"id":154806,"user_name":"wy","can_delete":false,"product_type":"c1","uid":1064681,"ip_address":"","ucode":"41C1B304E7F032","user_header":"https://static001.geekbang.org/account/avatar/00/10/3e/e9/116f1dee.jpg","comment_is_top":false,"comment_ctime":1574569807,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1574569807","product_id":100025301,"comment_content":"dataframe是dataset的子集，类型是row，所以dataframe适合那些编译阶段无法确定字段数量和类型的查询，dataset适用那些编译阶段就明确知道字段详细详细信息的场景。","like_count":1},{"had_liked":false,"id":113096,"user_name":"王翔宇🍼","can_delete":false,"product_type":"c1","uid":1177925,"ip_address":"","ucode":"D7AA5EDEC8D612","user_header":"https://static001.geekbang.org/account/avatar/00/11/f9/45/eaa8beb0.jpg","comment_is_top":false,"comment_ctime":1562896564,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1562896564","product_id":100025301,"comment_content":"何时检测分析错误 的分析错误是指什么？","like_count":0,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2617,"discussion_content":"编译时错误的一种，比如错别字、命名不规范这些是语法错误，而调用函数参数和定义参数类型不一就是分析错误了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563788334,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":112702,"user_name":"西北偏北","can_delete":false,"product_type":"c1","uid":1043160,"ip_address":"","ucode":"64BD69C84EE6A1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132","comment_is_top":false,"comment_ctime":1562808103,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1562808103","product_id":100025301,"comment_content":"Rdd是spark最底层的核心数据结构，但直接使用rdd的门槛较高，你得知道那些地方可以优化，比如filter要写在group之前。如若不然，写出来的计算，性能其差。<br><br><br>并且rdd没有字段类型名称信息，字段的获取和处理难度较高。<br><br><br>推荐使用spark sql更顶层，提供了schema信息，提供了对查询的优化。","like_count":1},{"had_liked":false,"id":104196,"user_name":"linuxfans","can_delete":false,"product_type":"c1","uid":1193815,"ip_address":"","ucode":"0C0BEE82F8A409","user_header":"https://static001.geekbang.org/account/avatar/00/12/37/57/750c641d.jpg","comment_is_top":false,"comment_ctime":1560679855,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560679855","product_id":100025301,"comment_content":"Dataset具有类型信息，而Dataframe没有，所以当我们要处理类型化数据，例如数值型时，用Dataset更方便。理论上，Dataframe可以用在Dataset的所有地方，只是字段类型要在程序中自己处理。","like_count":0},{"had_liked":false,"id":99281,"user_name":"Geek_f406a1","can_delete":false,"product_type":"c1","uid":1453834,"ip_address":"","ucode":"62A15101047054","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/4lUeUo6LHfsHLYfKaMQXQiaZVyEsqY1nfXU6dP0wN1KCch7LDIZTCO4rJ5mq1SdqY9FibCGMsjFdknULmEQ4Octg/132","comment_is_top":false,"comment_ctime":1559186072,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1559186072","product_id":100025301,"comment_content":"请问作者 &quot;还兼容多种数据格式，包括 Hive、RDD、JSON 文件&quot; 这句话中hive是一种数据格式么？","like_count":0,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2619,"discussion_content":"严格的讲不是，hive 是一个数据库软件，这里指的是 spark 可以从这几个地方抽取数据，相当于都是数据源","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1563788411,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96900,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1558532019,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558532019","product_id":100025301,"comment_content":"1. 对于读写Hive时，适合用Dataframe，可以认为df就是一张表。<br>2. 对于要操作元组或者case class或者csv等文件时，适合用Dataset，因为是强类型的","like_count":0}]}