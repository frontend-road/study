{"id":806933,"title":"10｜MySQL如何快速导入导出数据？（下）","content":"<p>你好，我是俊达。</p><p>上一讲我介绍了mysqldump和MySQL Shell的Dump工具。使用mysqldump导出的，实际上是一个SQL文件，将这个文件直接拿到数据库中执行，就可以完成数据导入。MySQL Shell Dump工具将建表语句、表中的数据导出到了不同的文件中，而且数据以文本文件的形式存储，需要使用MySQL Shell配套的Load工具，或者使用Load Data命令导入数据。</p><p>这一讲我们来学习MySQL Shell Load工具的使用方法，以及导出和导入单个表数据的一些其他方法。</p><h2>MySQL Shell Load工具</h2><h3>使用load_dump导入</h3><p>MySQL Shell Dump导出的数据，可以用MySQL Shell Load工具导入。load_dump有两个参数，第一个参数是Dump文件的路径。第二个参数是一个字典，用来指定导入的各个选项。</p><pre><code class=\"language-go\">util.load_dump(\"/data/backup/db_backups\", {})\n</code></pre><p>load_dump默认会导入Dump路径下的所有文件。你可以使用includeSchemas、includeTables来指定需要导入的库和表，用excludeSchemas、excludeTables忽略指一些库和表。这里includeTables和excludeTables中表名的格式为\"db_name.table_name\"。</p><!-- [[[read_end]]] --><pre><code class=\"language-go\">util.load_dump(\"/data/backup/db_backups\", {\n    \"includeTables\":[\"employees.salaries\"]\n})\n</code></pre><p>将loadDdl设置为True，loadData设置为False，你可以只导入表结构，不导入数据。</p><pre><code class=\"language-go\">util.load_dump(\"employees_dump\", {\n    \"includeTables\":[\"employees.salaries\"], \n    \"loadDdl\":True,\n    \"loadData\":False\n})\n</code></pre><p>如果你只导出了一个库，还可以通过schema选项，数据导入到另外一个库中。如果指定的库不存在，load_dump会自动创建这个数据库。</p><pre><code class=\"language-go\">util.load_dump(\"/data/backup/backup_employees\", {\n\t\"schema\":\"employees_restore\"\n})\n</code></pre><p>如果Dump文件中包含了多个库，那么使用schema选项时，会报出下面这样的错误信息。</p><pre><code class=\"language-go\">ERROR: The 'schema' option can only be used when loading a single schema, but 2 will be loaded.\n</code></pre><h3>注意事项</h3><ol>\n<li>\n<p>load_dump工具使用LOAD DATA LOCAL INFILE命令导入数据，因此需要在目标库上将local_infile设置为ON，否则会报错“ERROR: The ‘local_infile’ global system variable must be set to ON in the target server, after the server is verified to be trusted.”。</p>\n</li>\n<li>\n<p>使用load_dump导入时，如果目标库中已经有同名的表，导入操作会报错。</p>\n</li>\n</ol><pre><code class=\"language-go\">ERROR: Schema `employees` already contains a table named employees\nERROR: One or more objects in the dump already exist in the destination database. You must either DROP these objects or exclude them from the load.\n</code></pre><p>你可以设置excludeTables，不导入这些表。或者设置ignoreExistingObjects，忽略已经存在的表。注意，设置ignoreExistingObjects只是不重新创建表，数据还是会重新导入的，表中已经存在数据，会被Dump文件中的数据覆盖。</p><ol start=\"3\">\n<li>load_dump将导入的进度记录在progressFile文件中。如果导入过程异常中断，下次继续导入时，会根据progressFile文件中的内容跳过已经完成的步骤。如果你想重新导入数据，可以将resetProgress设置为True。</li>\n</ol><pre><code class=\"language-go\">util.load_dump(\"/data/backup/backup_employees\", {\n\t\"schema\":\"employees_restore\",\n\t\"resetProgress\": True,\n})\n</code></pre><h3>Load选项</h3><p>下面的表格整理了Load工具支持的一部分参数，供你参考。完整的选项请参考<a href=\"https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-shell-utilities-load-dump.html\">官方文档</a>。</p><p><img src=\"https://static001.geekbang.org/resource/image/15/8c/1594612be99cf0ed9806d24da09cc78c.jpg?wh=1920x2177\" alt=\"图片\"></p><p>MySQL Shell Dump导出的文件，实际上还可以直接使用LOAD DATA命令导入，或者使用MySQL Shell的import_table导入，接下来我会依次介绍。</p><h2>单表数据导出</h2><p>MySQL Shell Dump导出的数据，实际上使用了比较常用的一种数据格式。在MySQL中，还有其他几个方法也能将数据导出成一样的格式，包括使用SELECT INTO OUTFILE和使用MySQL Shell的export_table功能。</p><h3>使用SELECT INTO OUTFILE</h3><p>使用SELECT INTO OUTFILE可以将数据导出到文本文件。不过使用这个功能时，需要先设置数据库参数secure_file_priv。修改secure_file_priv需要重启数据库，我们将参数加到配置文件中，重启数据库。</p><pre><code class=\"language-go\"># tail -1 /data/mysql01/my.cnf\nsecure_file_priv=''\n</code></pre><p>OUTFILE不能指向已经存在的文件，否则会报错。</p><pre><code class=\"language-go\">mysql&gt; select * from employees.employees limit 10 into outfile '/tmp/emp.txt';\nQuery OK, 10 rows affected (0.19 sec)\n</code></pre><p>SELECT INTO不加额外参数时，使用Tab分割字段，使用换行符分割记录。</p><pre><code class=\"language-go\"># head -5 /tmp/emp.txt\n10001\t1953-09-02\tGeorgi\tFacello\tM\t1986-06-26\n10002\t1964-06-02\tBezalel\tSimmel\tF\t1985-11-21\n10003\t1959-12-03\tParto\tBamford\tM\t1986-08-28\n10004\t1954-05-01\tChirstian\tKoblick\tM\t1986-12-01\n10005\t1955-01-21\tKyoichi\tMaliniak\tM\t1989-09-12\n</code></pre><p>你可以分别指定列分割符、行分割符、转义符。下面这个例子中，列分割符是逗号 <code>\",\"</code>，行分割符是换行符 <code>\"\\n\"</code>，字段的数据用引号引用起来。</p><pre><code class=\"language-go\">mysql&gt; select * from employees limit 10\n     into outfile '/tmp/emp1.txt'\n     character set utf8mb4\n     fields terminated by ','\n     optionally enclosed by '\"'\n     escaped by '\\\\'\n     lines terminated by '\\n';\nQuery OK, 10 rows affected (0.01 sec)\n</code></pre><p>这样导出的文件，就是非常常见的CSV格式。</p><pre><code class=\"language-go\"># cat /tmp/emp1.txt\n10001,\"1953-09-02\",\"Georgi\",\"Facello\",\"M\",\"1986-06-26\"\n10002,\"1964-06-02\",\"Bezalel\",\"Simmel\",\"F\",\"1985-11-21\"\n10003,\"1959-12-03\",\"Parto\",\"Bamford\",\"M\",\"1986-08-28\"\n10004,\"1954-05-01\",\"Chirstian\",\"Koblick\",\"M\",\"1986-12-01\"\n10005,\"1955-01-21\",\"Kyoichi\",\"Maliniak\",\"M\",\"1989-09-12\"\n</code></pre><p>实际场景中，字段中存储的数据中很可能也包含了列分割符、行分割符、引号、转义符，这会引起文件格式错乱吗？我们用一个例子来测试下。</p><pre><code class=\"language-go\">mysql&gt; create table emp2(\n    emp_no int,\n    emp_name varchar(60),\n    emp_intro varchar(100),\n    primary key (emp_no)\n) engine=InnoDB;\n\nmysql&gt; insert into emp2 values\n    (10001, '张三', '一生二,二生三,三生万物。'),\n    (10002, '李某', '引用一句名言:\"天行健\\n君子以自强不息\"'),\n    (10003, '陈某', 'D:\\\\Pictures\\\\myself.png');\n\nmysql&gt; select * from emp2;\n+--------+----------+------------------------------------------------------+\n| emp_no | emp_name | emp_intro                                            |\n+--------+----------+------------------------------------------------------+\n|  10001 | 张三     | 一生二,二生三,三生万物。                             |\n|  10002 | 李某     | 引用一句名言:\"天行健\n君子以自强不息\"                 |\n|  10003 | 陈某     | D:\\Pictures\\myself.png                               |\n+--------+----------+------------------------------------------------------+\n3 rows in set (0.00 sec)\n</code></pre><p>上面这个表的几行数据中，有逗号、双引号、换行符、反斜杠这些特殊的字符。将数据导出后，可以看到这些特殊字符都进行了转义处理。因此导入这些数据时，只要指定相同的参数，就不会有任何问题。</p><pre><code class=\"language-go\">mysql&gt; select * from emp2 limit 10\n     into outfile '/tmp/emp2.txt'\n     character set utf8mb4\n     fields terminated by ','\n     optionally enclosed by '\"'\n     escaped by '\\\\'\n     lines terminated by '\\n';\n\nQuery OK, 3 rows affected (0.00 sec)\n</code></pre><pre><code class=\"language-go\"># cat /tmp/emp2.txt\n10001,\"张三\",\"一生二,二生三,三生万物。\"\n10002,\"李某\",\"引用一句名言:\\\"天行健\\\n君子以自强不息\\\"\"\n10003,\"陈某\",\"D:\\\\Pictures\\\\myself.png\"\n</code></pre><p>SELECT INTO OUTFILE只能将数据导出在数据库服务器的目录中，使用起来并不是很方便。因为你可能并没有数据库服务器的权限，比如你可能使用了云数据库，无法访问底层操作系统。</p><h3>使用MySQL Shell export_table导出数据</h3><p>MySQL Shell提供了export_table功能，可以将表的数据导出到本地文件中。下面这个例子使用export_table导出emp2表。</p><pre><code class=\"language-go\">mysqlsh -u user_01 -h172.16.121.234 -psomepass --py --mysql\n\n MySQL Py &gt; util.export_table(\"employees.emp2\", \"/data/backup/emp2.csv\", {\n    \"linesTerminatedBy\": \"\\n\",\n    \"fieldsTerminatedBy\": \",\",\n    \"fieldsEnclosedBy\": \"\\\"\",\n    \"fieldsOptionallyEnclosed\": True,\n    \"fieldsEscapedBy\": \"\\\\\"\n    })\n\n\nInitializing - done\nGathering information - done\nRunning data dump using 1 thread.\nNOTE: Progress information uses estimated values and may not be accurate.\nStarting data dump\n100% (3 rows / ~3 rows), 0.00 rows/s, 0.00 B/s\nDump duration: 00:00:00s\nTotal duration: 00:00:00s\nData size: 170 bytes\nRows written: 3\nBytes written: 170 bytes\nAverage throughput: 170.00 B/s\n\nThe dump can be loaded using:\nutil.import_table(\"/data/backup/emp2.csv\", {\n    \"characterSet\": \"utf8mb4\",\n    \"fieldsEnclosedBy\": \"\\\"\",\n    \"fieldsEscapedBy\": \"\\\\\",\n    \"fieldsOptionallyEnclosed\": true,\n    \"fieldsTerminatedBy\": \",\",\n    \"linesTerminatedBy\": \"\\n\",\n    \"schema\": \"employees\",\n    \"table\": \"emp2\"\n})\n</code></pre><p>指定相同的参数后，使用export_table生成的文件和SELECT INTO OUTFILE基本一致。</p><pre><code class=\"language-go\"># cat ./employees_dump/emp2.csv\n10001,\"张三\",\"一生二\\,二生三\\,三生万物。\"\n10002,\"李某\",\"引用一句名言:\\\"天行健\\n君子以自强不息\\\"\"\n10003,\"陈某\",\"D:\\\\Pictures\\\\myself.png\"\n</code></pre><h2>单表数据导入</h2><p>前面讲到，load_dump底层实际上使用了LOAD DATA LOCAL INFILE命令来导入数据。我们也可以在MySQL客户端中直接使用LOAD DATA命令。</p><h3>使用Load Data导入数据</h3><p>LOAD DATA命令的基本格式如下：</p><pre><code class=\"language-go\">LOAD DATA\n    [LOCAL]\n    INFILE 'file_name'\n    [REPLACE | IGNORE]\n    INTO TABLE tbl_name\n    [CHARACTER SET charset_name]\n    [FIELDS\n        [TERMINATED BY 'string']\n        [[OPTIONALLY] ENCLOSED BY 'char']\n        [ESCAPED BY 'char']\n    ]\n    [LINES\n        [STARTING BY 'string']\n        [TERMINATED BY 'string']\n    ]\n    [IGNORE number {LINES | ROWS}]\n    [(col_name_or_user_var\n        [, col_name_or_user_var] ...)]\n    [SET col_name={expr | DEFAULT}\n        [, col_name={expr | DEFAULT}] ...]\n</code></pre><p>INFILE指定文件路径，如果不加LOCAL，那么文件需要存放在数据库服务器的指定路径下，并且登录用户需要有FILE权限。如果加上了LOCAL，那么文件需要在客户端所在的机器上。如果导入的数据和表里原有的数据有冲突，默认会报错，可以加上REPLACE，覆盖表中的数据，或者加上IGNORE，跳过冲突的数据。</p><p>你可以使用IGNORE忽略文件开头的几行内容。如果你的文件前几行是标题，使用IGNORE就很方便。</p><p>下面的例子中，我们使用LOAD DATA命令来导入之前生成的CSV文件。注意mysql命令行需要加上参数–local-infile。</p><pre><code class=\"language-go\">mysql -vvv -uuser_01 -h172.16.121.234 -pabc123 -psomepass --local-infile employees &lt;&lt;EOF\nload data local infile '/data/backup/emp2.csv' \nreplace into table emp3\ncharacter set utf8mb4\nfields terminated by ','\noptionally enclosed by '\"'\nescaped by '\\\\\\\\'\nlines terminated by '\\n';\nEOF\n</code></pre><p>导入后要检查命令的输出信息，如果有Warning，需要检查下产生warning的具体原因。</p><pre><code class=\"language-go\">Query OK, 3 rows affected (1.42 sec)\nRecords: 3  Deleted: 0  Skipped: 0  Warnings: 0\n\nBye\n</code></pre><p>我们来检查下导入的数据，没有发现什么问题。</p><pre><code class=\"language-go\">mysql&gt; select * from emp3;\n+--------+----------+------------------------------------------------------+\n| emp_no | emp_name | emp_intro                                            |\n+--------+----------+------------------------------------------------------+\n|  10001 | 张三     | 一生二,二生三,三生万物。                             |\n|  10002 | 李某     | 引用一句名言:\"天行健\n君子以自强不息\"                 |\n|  10003 | 陈某     | D:\\Pictures\\myself.png                               |\n+--------+----------+------------------------------------------------------+\n3 rows in set (0.00 sec)\n</code></pre><p>刚才的例子中，CSV文件中字段数量和顺序跟表里面的字段数量和顺序完全一致。如果文件和表里面字段数量或顺序不一致，应该怎么处理呢？</p><p>我们使用一个具体的例子来说明如何处理这种情况。</p><pre><code class=\"language-go\">create table emp4(\n    emp_name varchar(64),\n    emp_no int,\n    emp_intro varchar(100),\n    grade int,\n    primary key(emp_no)\n) engine=InnoDB;\n</code></pre><p>emp4这个表和刚才的emp2.csv文件的字段顺序不一样，字段数量也不一样。</p><p><img src=\"https://static001.geekbang.org/resource/image/30/4d/3031c7c3a5ca759a82671d42810a2a4d.jpg?wh=1076x348\" alt=\"图片\"></p><p>导入数据时，以CSV文件中字段顺序为准，指定字段列表。我们的例子中，第一列对应到emp_no字段，第二列对应到emp_name字段，第三列对应到变量@emp_intro。然后再使用SET，将表的grade字段设置成固定值10，将emp_intro字段设置为文件中emp_intro列的前缀。</p><pre><code class=\"language-go\">mysql -vvv -uuser_01 -h172.16.121.234 -pabc123 -psomepass --local-infile employees &lt;&lt;EOF\nload data local infile '/data/backup/emp2.csv' \nreplace into table emp4\ncharacter set utf8mb4\nfields terminated by ','\noptionally enclosed by '\"'\nescaped by '\\\\\\\\'\nlines terminated by '\\n'\n(emp_no, emp_name, @emp_intro)\nset grade = 10, emp_intro = substring(@emp_intro, 1, 5)\nEOF\n</code></pre><p>我们来看一下导入的数据是不是符合预期。</p><pre><code class=\"language-go\">mysql&gt; select * from emp4;\n+----------+--------+-----------------+-------+\n| emp_name | emp_no | emp_intro       | grade |\n+----------+--------+-----------------+-------+\n| 张三     |  10001 | 一生二,二       |    10 |\n| 李某     |  10002 | 引用一句名      |    10 |\n| 陈某     |  10003 | D:\\Pi           |    10 |\n+----------+--------+-----------------+-------+\n</code></pre><h3>MySQL Shell 并行导入</h3><p>使用Load Data命令导入一个文件时，数据库内部使用了单线程处理。服务端接收到LOAD DATA LOCAL INFILE命令后，向客户端访问文件内容。客户端依次读取文件的内容，通过网络发送到服务端，服务端将网络中读取到的数据解析成一行一行的记录，再调用存储引擎接口写入数据。</p><p>MySQL Shell的import_table工具提供了并行导入数据的功能，如果你的MySQL服务器配置比较高，CPU和IO性能都很好，使用并行导入可能能提高大表的导入速度。</p><p>MySQL Shell Dump导出的文件，也可以用import_table来导入。有一点需要注意，import_table可以直接导入zstd压缩过的文件，但是对于单个压缩文件是无法使用并行导入的。</p><p>下面这个例子中，我们先解压文件，再使用import_table来导入。我们将bytesPerChunk设置为1M，也就是每执行一次LOAD DATA命令，就发送1M的文件内容。threads设置为8。</p><pre><code class=\"language-plain\"># zstd -d employees@salaries@@0.tsv.zst\n\n# mysqlsh -u user_01 -h172.16.121.234 -psomepass --py --mysql\n\n MySQL Py &gt; util.import_table(\n     \"/data/backup/employees_dump/employees@salaries@@0.tsv\", \n     {   \n        \"schema\": \"employees\",  \n        \"table\": \"salaries_backup\",\n        \"bytesPerChunk\": \"1M\",\n        \"threads\":8\n    })\n</code></pre><p>到目标服务器上执行show processlist，可以看到有8个会话都在执行LOAD DATA命令。</p><pre><code class=\"language-plain\">*************************** 10. row ***************************\n     Id: 60\n   User: user_01\n   Host: mysql02:45028\n     db: employees\nCommand: Query\n   Time: 36\n  State: executing\n   Info: LOAD DATA LOCAL INFILE '/data/backup/employees_dump/employees@salaries@@0.tsv' INTO TABLE `employees\n*************************** 11. row ***************************\n     Id: 61\n   User: user_01\n   Host: mysql02:45026\n     db: employees\nCommand: Query\n   Time: 36\n  State: executing\n   Info: LOAD DATA LOCAL INFILE '/data/backup/employees_dump/employees@salaries@@0.tsv' INTO TABLE `employees\n\n......\n*************************** 17. row ***************************\n     Id: 67\n   User: user_01\n   Host: mysql02:45040\n     db: employees\nCommand: Query\n   Time: 36\n  State: executing\n   Info: LOAD DATA LOCAL INFILE '/data/backup/employees_dump/employees@salaries@@0.tsv' INTO TABLE `employees\n</code></pre><p>指定相应的参数后，import_table也能用来导入CSV格式的文件，可以看出，这些参数和LOAD DATA命令可以一一对应起来。</p><pre><code class=\"language-plain\">util.import_table(\n     \"/data/backup/emp2.csv\", \n     {   \n        \"schema\": \"employees\",  \n        \"table\": \"emp4\",\n  \n        \"linesTerminatedBy\": \"\\n\",\n        \"fieldsTerminatedBy\": \",\",\n        \"fieldsEnclosedBy\": '\"',\n        \"fieldsOptionallyEnclosed\": True,\n        \"fieldsEscapedBy\": \"\\\\\",\n        \"replaceDuplicates\": True,\n        \"columns\": [\"emp_no\", \"emp_name\", 1 ],\n        \"decodeColumns\": {\n            \"grade\":11,\n            \"emp_intro\":\"substring(@1, 1, 10)\",\n        }\n    })\n\n</code></pre><pre><code class=\"language-plain\">mysql&gt; select * from emp4;\n+----------+--------+----------------------------+-------+\n| emp_name | emp_no | emp_intro                  | grade |\n+----------+--------+----------------------------+-------+\n| 张三     |  10001 | 一生二,二生三,三生         |    11 |\n| 李某     |  10002 | 引用一句名言:\"天行         |    11 |\n| 陈某     |  10003 | D:\\Picture                 |    11 |\n+----------+--------+----------------------------+-------+\n</code></pre><h4>import_table选项</h4><p>我整理了import_table支持的部分选项，供你参考。完整的选项请参考<a href=\"https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-shell-utilities-parallel-table.html\">官方文档</a>。</p><p><img src=\"https://static001.geekbang.org/resource/image/e5/09/e5ffdea7316793cb8af235750223cd09.png?wh=1758x1536\" alt=\"图片\"></p><h2>总结</h2><p>这一讲我们探讨了MySQL数据导入导出的一些工具和方法，这都是官方提供的工具。mysqldump使用起来非常方便，但由于是单线程的，如果你的数据库特别大，导入数据可能会需要很长的时间。你需要注意，导出数据时是否会锁表，尤其是导出生产环境的数据库时，不要影响正常的业务访问。</p><p>MySQL Shell的Dump和Load工具能以多线程的方式运行，在导出和导入大量数据时有优势。当然，如果你需要复制整个数据库实例，使用物理备份的方式可能性能更好，后续的课程中，我们会分别介绍使用xtrabackup和clone插件来复制整个库的方法。</p><p>如果你需要将大量数据从别的数据库迁移到MySQL，一种可行的方法是先将源库的数据导出成CSV文件，然后再使用LOAD DATA或MySQL Shell的import_table导入数据。数据导出和导入时，还需要注意文本数据的字符集，导入数据后要检查是否有乱码产生。同时也要检查导入前后的数据量是否一样。</p><h2>思考题</h2><p>由于公司的策略，需要将一个核心业务系统的Oracle数据库迁移到MySQL。这个Oracle数据库大概有1T数据，迁移过程中，要尽可能缩短业务停机的时间，业务方能接受的最大停机时间在1～2小时之内。请你设计一个方案，将数据平滑地迁移到MySQL。你需要考虑全量数据如何迁移，业务运行期间新产生的数据如何迁移。</p><p>期待你的思考，欢迎在留言区中与我交流。如果今天的课程让你有所收获，也欢迎转发给有需要的朋友。我们下节课再见！</p>","neighbors":{"left":{"article_title":"09｜MySQL如何快速导入导出数据？（上）","id":804980},"right":{"article_title":"11｜表太大了，修改表结构太慢怎么解决？（上）","id":806938}},"comments":[{"had_liked":false,"id":394109,"user_name":"123","can_delete":false,"product_type":"c1","uid":2662872,"ip_address":"浙江","ucode":"5A343B568B9524","user_header":"https://static001.geekbang.org/account/avatar/00/28/a1/d8/42252c48.jpg","comment_is_top":false,"comment_ctime":1725851552,"is_pvip":false,"replies":[{"id":143092,"content":"如果源库是MySQL，可以按这个思路来做。","user_name":"作者回复","user_name_real":"编辑","uid":3898827,"ctime":1726057489,"ip_address":"浙江","comment_id":394109,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100799401,"comment_content":"思考题：\n\n可以通过Oracle工具多线程导出整个数据库实例，设置开启事务，短暂的获取全局锁后进行快照导出，将数据导出到指定目录中，并且每个库表的ddl和数据都是分开存放，方便后面的并行导入；\n\n导入过程中使用import_table，指定线程数，具体的线程数据还是要根据目标数据库的并发写性能来确定，尽可能的缩短时间;\n\n对于导出导入过程中的增量数据，可以在导出开始的时候记录binlog位置或GTID的全局事务id，导入增量数据的时候可以通过binlog位置和GTID来恢复增量数据，且导出数据的过程尽量在用户量少的时间操作，确保增量的数据尽可能少，同时来减少导入增量数据时业务库的停机时间；","like_count":2,"discussions":[{"author":{"id":3898827,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/7d/cb/fa3dae58.jpg","nickname":"俊达","note":"","ucode":"F79BF9651AD086","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651011,"discussion_content":"如果源库是MySQL，可以按这个思路来做。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1726057490,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394140,"user_name":"ls","can_delete":false,"product_type":"c1","uid":1001037,"ip_address":"上海","ucode":"C18E208B1DFDA7","user_header":"https://static001.geekbang.org/account/avatar/00/0f/46/4d/161f3779.jpg","comment_is_top":false,"comment_ctime":1725937885,"is_pvip":true,"replies":[{"id":143090,"content":"全量数据采用Oracle导出csv，然后再导入到MySQL，是一种比较高效的数据迁移方法。\n增量数据需要另外想办法，比如应用程序双写。","user_name":"作者回复","user_name_real":"编辑","uid":3898827,"ctime":1726057183,"ip_address":"浙江","comment_id":394140,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100799401,"comment_content":"也可以看数据变化程度，如果历史数据几乎不会改变，可以直接oracle 并发导出csv ，然后mysql  并发导入csv，最后检查数据","like_count":0,"discussions":[{"author":{"id":3898827,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/7d/cb/fa3dae58.jpg","nickname":"俊达","note":"","ucode":"F79BF9651AD086","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651008,"discussion_content":"全量数据采用Oracle导出csv，然后再导入到MySQL，是一种比较高效的数据迁移方法。\n增量数据需要另外想办法，比如应用程序双写。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1726057183,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394139,"user_name":"ls","can_delete":false,"product_type":"c1","uid":1001037,"ip_address":"上海","ucode":"C18E208B1DFDA7","user_header":"https://static001.geekbang.org/account/avatar/00/0f/46/4d/161f3779.jpg","comment_is_top":false,"comment_ctime":1725937804,"is_pvip":true,"replies":[{"id":143089,"content":"可以的。\nOGG是业界用得比较多的实时同步工具，对Oracle的支持比较强大。👍","user_name":"作者回复","user_name_real":"编辑","uid":3898827,"ctime":1726056829,"ip_address":"浙江","comment_id":394139,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100799401,"comment_content":"借用ogg 先实时同步 Oracle和MySQL的数据，切换窗口时间用来检测数据同步是否正确。","like_count":0,"discussions":[{"author":{"id":3898827,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/7d/cb/fa3dae58.jpg","nickname":"俊达","note":"","ucode":"F79BF9651AD086","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651007,"discussion_content":"可以的。\nOGG是业界用得比较多的实时同步工具，对Oracle的支持比较强大。👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1726056830,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394107,"user_name":"Geek_0126","can_delete":false,"product_type":"c1","uid":3952196,"ip_address":"浙江","ucode":"2916F7FB3F6D71","user_header":"https://static001.geekbang.org/account/avatar/00/3c/4e/44/49b29792.jpg","comment_is_top":false,"comment_ctime":1725850190,"is_pvip":false,"replies":[{"id":143091,"content":"如果源库是MySQL，实时增量的数据可以采用一些开源的方案来处理。\n如果源库是Oracle，实时增量数据可以采用一些商业化的产品来同步，或者在应用层双写。","user_name":"作者回复","user_name_real":"编辑","uid":3898827,"ctime":1726057320,"ip_address":"浙江","comment_id":394107,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100799401,"comment_content":"异构数据源之间的同步问题，就要借助同步工具了，例如DataX、cloudcanal等，可以先进行全量迁移，然后再开启增量同步。不过其中需要注意的细节太多了，比如数据类型及各种数据库对象之间的转换。","like_count":0,"discussions":[{"author":{"id":3898827,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/7d/cb/fa3dae58.jpg","nickname":"俊达","note":"","ucode":"F79BF9651AD086","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651009,"discussion_content":"如果源库是MySQL，实时增量的数据可以采用一些开源的方案来处理。\n如果源库是Oracle，实时增量数据可以采用一些商业化的产品来同步，或者在应用层双写。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1726057320,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]}]}