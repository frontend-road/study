{"id":360837,"title":"13 | 广播变量（二）：如何让Spark SQL选择Broadcast Joins？","content":"<p>你好，我是吴磊。</p><p>上一讲我们说到，在数据关联场景中，广播变量是克制Shuffle的杀手锏，用Broadcast Joins取代Shuffle Joins可以大幅提升执行性能。但是，很多同学只会使用默认的广播变量，不会去调优。那么，我们该怎么保证Spark在运行时优先选择Broadcast Joins策略呢？</p><p>今天这一讲，我就围绕着数据关联场景，从配置项和开发API两个方面，帮你梳理出两类调优手段，让你能够游刃有余地运用广播变量。</p><h2>利用配置项强制广播</h2><p>我们先来从配置项的角度说一说，有哪些办法可以让Spark优先选择Broadcast Joins。在Spark SQL配置项那一讲，我们提到过spark.sql.autoBroadcastJoinThreshold这个配置项。它的设置值是存储大小，默认是10MB。它的含义是，<strong>对于参与Join的两张表来说，任意一张表的尺寸小于10MB，Spark就在运行时采用Broadcast Joins的实现方式去做数据关联。</strong>另外，AQE在运行时尝试动态调整Join策略时，也是基于这个参数来判定过滤后的数据表是否足够小，从而把原本的Shuffle Joins调整为Broadcast Joins。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/69/89/697cccb272fc8863f40bb7c465f53c89.jpeg?wh=1785*348\" alt=\"\"></p><p>为了方便你理解，我来举个例子。在数据仓库中，我们经常会看到两张表：一张是订单事实表，为了方便，我们把它记成Fact；另一张是用户维度表，记成Dim。事实表体量很大在100GB量级，维度表很小在1GB左右。两张表的Schema如下所示：</p><pre><code>//订单事实表Schema\norderID: Int\nuserID: Int\ntrxDate: Timestamp\nproductId: Int\nprice: Float\nvolume: Int\n \n//用户维度表Schema\nuserID: Int\nname: String\nage: Int\ngender: String\n</code></pre><p>当Fact表和Dim表基于userID做关联的时候，由于两张表的尺寸大小都远超spark.sql.autoBroadcastJoinThreshold参数的默认值10MB，因此Spark不得不选择Shuffle Joins的实现方式。但如果我们把这个参数的值调整为2GB，因为Dim表的尺寸比2GB小，所以，Spark在运行时会选择把Dim表封装到广播变量里，并采用Broadcast Join的方式来完成两张表的数据关联。</p><p>显然，对于绝大多数的Join场景来说，autoBroadcastJoinThreshold参数的默认值10MB太低了，因为现在企业的数据体量都在TB，甚至PB级别。因此，想要有效地利用Broadcast Joins，我们需要把参数值调大，一般来说，2GB左右是个不错的选择。</p><p>现在我们已经知道了，<strong>使用广播阈值配置项让Spark优先选择Broadcast Joins的关键，就是要确保至少有一张表的存储尺寸小于广播阈值</strong>。</p><p>但是，在设置广播阈值的时候，不少同学都跟我抱怨：“我的数据量明明小于autoBroadcastJoinThreshold参数设定的广播阈值，为什么Spark SQL在运行时并没有选择Broadcast Joins呢？”</p><p>详细了解后我才知道，这些同学所说的数据量，<strong>其实指的是数据表在磁盘上的存储大小</strong>，比如用<code>ls</code>或是<code>du -sh</code>等系统命令查看文件得到的结果。要知道，同一份数据在内存中的存储大小往往会比磁盘中的存储大小膨胀数倍，甚至十数倍。这主要有两方面原因。</p><p>一方面，为了提升存储和访问效率，开发者一般采用Parquet或是ORC等压缩格式把数据落盘。这些高压缩比的磁盘数据展开到内存之后，数据量往往会翻上数倍。</p><p>另一方面，受限于对象管理机制，在堆内内存中，JVM往往需要比数据原始尺寸更大的内存空间来存储对象。</p><p>我们来举个例子，字符串“abcd”按理说只需要消耗4个字节，但是，JVM在堆内存储这4个字符串总共需要消耗48个字节！那在运行时，一份看上去不大的磁盘数据展开到内存，翻上个4、5倍并不稀奇。因此，如果你按照磁盘上的存储大小去配置autoBroadcastJoinThreshold广播阈值，大概率也会遇到同样的窘境。</p><p><strong>那么问题来了，有什么办法能准确地预估一张表在内存中的存储大小呢？</strong></p><p>首先，我们要避开一个坑。我发现，有很多资料推荐用Spark内置的SizeEstimator去预估分布式数据集的存储大小。结合多次实战和踩坑经验，咱们必须要指出，<strong>SizeEstimator的估算结果不准确</strong>。因此，你可以直接跳过这种方法，这也能节省你调优的时间和精力。</p><p>我认为比较靠谱的办法是：<strong>第一步，把要预估大小的数据表缓存到内存，比如直接在DataFrame或是Dataset上调用cache方法；第二步，读取Spark SQL执行计划的统计数据</strong>。这是因为，Spark SQL在运行时，就是靠这些统计数据来制定和调整执行策略的。</p><pre><code>val df: DataFrame = _\ndf.cache.count\n \nval plan = df.queryExecution.logical\nval estimated: BigInt = spark\n.sessionState\n.executePlan(plan)\n.optimizedPlan\n.stats\n.sizeInBytes\n</code></pre><p>你可能会说：“这种办法虽然精确，但是这么做，实际上已经是在运行时进行调优了。把数据先缓存到内存，再去计算它的存储尺寸，当然更准确了。”没错，采用这种计算方式，调优所需花费的时间和精力确实更多，但在很多情况下，尤其是Shuffle Joins的执行效率让你痛不欲生的时候，这样的付出是值得的。</p><h2>利用API强制广播</h2><p>既然数据量的预估这么麻烦，有没有什么办法，不需要配置广播阈值，就可以让Spark SQL选择Broadcast Joins？还真有，而且办法还不止一种。</p><p>开发者可以通过Join Hints或是SQL functions中的broadcast函数，来强制Spark SQL在运行时采用Broadcast Joins的方式做数据关联。下面我就来分别讲一讲它们的含义和作用，以及该如何使用。必须要说明的是，这两种方式是等价的，并无优劣之分，只不过有了多样化的选择之后，你就可以根据自己的偏好和习惯来灵活地进行开发。</p><h3>用Join Hints强制广播</h3><p>Join Hints中的Hints表示“提示”，它指的是在开发过程中使用特殊的语法，明确告知Spark SQL在运行时采用哪种Join策略。一旦你启用了Join Hints，不管你的数据表是不是满足广播阈值，Spark SQL都会尽可能地尊重你的意愿和选择，使用Broadcast Joins去完成数据关联。</p><p>我们来举个例子，假设有两张表，一张表的内存大小在100GB量级，另一张小一些，2GB左右。在广播阈值被设置为2GB的情况下，并没有触发Broadcast Joins，但我们又不想花费时间和精力去精确计算小表的内存占用到底是多大。在这种情况下，我们就可以用Join Hints来帮我们做优化，仅仅几句提示就可以帮我们达到目的。</p><pre><code>val table1: DataFrame = spark.read.parquet(path1)\nval table2: DataFrame = spark.read.parquet(path2)\ntable1.createOrReplaceTempView(&quot;t1&quot;)\ntable2.createOrReplaceTempView(&quot;t2&quot;)\n \nval query: String = “select /*+ broadcast(t2) */ * from t1 inner join t2 on t1.key = t2.key”\nval queryResutls: DataFrame = spark.sql(query)\n</code></pre><p>你看，在上面的代码示例中，只要在SQL结构化查询语句里面加上一句<code>/*+ broadcast(t2) */</code>提示，我们就可以强制Spark SQL对小表t2进行广播，在运行时选择Broadcast Joins的实现方式。提示语句中的关键字，除了使用broadcast外，我们还可以用broadcastjoin或者mapjoin，它们实现的效果都一样。</p><p>如果你不喜欢用SQL结构化查询语句，尤其不想频繁地在Spark SQL上下文中注册数据表，你也可以在DataFrame的DSL语法中使用Join Hints。</p><pre><code>table1.join(table2.hint(“broadcast”), Seq(“key”), “inner”)\n\n</code></pre><p>在上面的DSL语句中，我们只要在table2上调用hint方法，然后指定broadcast关键字，就可以同样达到强制广播表2的效果。</p><p>总之，Join Hints让开发者可以灵活地选择运行时的Join策略，对于熟悉业务、了解数据的同学来说，Join Hints允许开发者把专家经验凌驾于Spark SQL的优化引擎之上，更好地服务业务。</p><p>不过，Join Hints也有个小缺陷。如果关键字拼写错误，Spark SQL在运行时并不会显示地抛出异常，而是默默地忽略掉拼写错误的hints，假装它压根不存在。因此，在使用Join Hints的时候，需要我们在编译时自行确认Debug和纠错。</p><h3>用broadcast函数强制广播</h3><p>如果你不想等到运行时才发现问题，想让编译器帮你检查类似的拼写错误，那么你可以使用强制广播的第二种方式：broadcast函数。这个函数是类库org.apache.spark.sql.functions中的broadcast函数。调用方式非常简单，比Join Hints还要方便，只需要用broadcast函数封装需要广播的数据表即可，如下所示。</p><pre><code>import org.apache.spark.sql.functions.broadcast\ntable1.join(broadcast(table2), Seq(“key”), “inner”)\n</code></pre><p>你可能会问：“既然开发者可以通过Join Hints和broadcast函数强制Spark SQL选择Broadcast Joins，那我是不是就可以不用理会广播阈值的配置项了？”其实还真不是。我认为，<strong>以广播阈值配置为主，以强制广播为辅</strong>，往往是不错的选择。</p><p><strong>广播阈值的设置，更多的是把选择权交给Spark SQL，尤其是在AQE的机制下，动态Join策略调整需要这样的设置在运行时做出选择。强制广播更多的是开发者以专家经验去指导Spark SQL该如何选择运行时策略。</strong>二者相辅相成，并不冲突，开发者灵活地运用就能平衡Spark SQL优化策略与专家经验在应用中的比例。</p><h2>广播变量不是银弹</h2><p>不过，虽然我们一直在强调，数据关联场景中广播变量是克制Shuffle的杀手锏，但广播变量并不是银弹。</p><p>就像有的同学会说：“开发者有这么多选项，甚至可以强制Spark选择Broadcast Joins，那我们是不是可以把所有Join操作都用Broadcast Joins来实现？”答案当然是否定的，广播变量不能解决所有的数据关联问题。</p><p><strong>首先，从性能上来讲，Driver在创建广播变量的过程中，需要拉取分布式数据集所有的数据分片。</strong>在这个过程中，网络开销和Driver内存会成为性能隐患。广播变量尺寸越大，额外引入的性能开销就会越多。更何况，如果广播变量大小超过8GB，Spark会直接抛异常中断任务执行。</p><p><strong>其次，从功能上来讲，并不是所有的Joins类型都可以转换为Broadcast Joins。</strong>一来，Broadcast Joins不支持全连接（Full Outer Joins）；二来，在所有的数据关联中，我们不能广播基表。或者说，即便开发者强制广播基表，也无济于事。比如说，在左连接（Left Outer Join）中，我们只能广播右表；在右连接（Right Outer Join）中，我们只能广播左表。在下面的代码中，即便我们强制用broadcast函数进行广播，Spark SQL在运行时还是会选择Shuffle Joins。</p><pre><code>import org.apache.spark.sql.functions.broadcast\nbroadcast (table1).join(table2, Seq(“key”), “left”)\ntable1.join(broadcast(table2), Seq(“key”), “right”)\n</code></pre><h2>小结</h2><p>这一讲，我们总结了2种方法，让Spark SQL在运行时能够选择Broadcast Joins策略，分别是设置配置项和用API强制广播。</p><p><strong>首先，设置配置项主要是设置autoBroadcastJoinThreshold配置项。</strong>开发者通过这个配置项指示Spark SQL优化器。只要参与Join的两张表中，有一张表的尺寸小于这个参数值，就在运行时采用Broadcast Joins的实现方式。</p><p>为了让Spark SQL采用Broadcast Joins，开发者要做的，就是让数据表在内存中的尺寸小于autoBroadcastJoinThreshold参数的设定值。</p><p>除此之外，在设置广播阈值的时候，因为磁盘数据展开到内存的时候，存储大小会成倍增加，往往导致Spark SQL无法采用Broadcast Joins的策略。因此，我们在做数据关联的时候，还要先预估一张表在内存中的存储大小。一种精确的预估方法是先把DataFrame缓存，然后读取执行计划的统计数据。</p><p><strong>其次，用API强制广播有两种方法，分别是设置Join Hints和用broadcast函数。</strong>设置Join Hints的方法就是在SQL结构化查询语句里面加上一句“/*+ broadcast(某表) */”的提示就可以了，这里的broadcast关键字也可以换成broadcastjoin或者mapjoin。另外，你也可以在DataFrame的DSL语法中使用调用hint方法，指定broadcast关键字，来达到同样的效果。设置broadcast函数的方法非常简单，只要用broadcast函数封装需要广播的数据表就可以了。</p><p>总的来说，不管是设置配置项还是用API强制广播都有各自的优缺点，所以，<strong>以广播阈值配置为主、强制广播为辅</strong>，往往是一个不错的选择。</p><p>最后，不过，我们也要注意，广播变量不是银弹，它并不能解决所有的数据关联问题，所以在日常的开发工作中，你要注意避免滥用广播。</p><h2>每日一练</h2><ol>\n<li>除了broadcast关键字外，在Spark 3.0版本中，Join Hints还支持哪些关联类型和关键字？</li>\n<li>DataFrame可以用sparkContext.broadcast函数来广播吗？它和org.apache.spark.sql.functions.broadcast函数之间的区别是什么？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","comments":[{"had_liked":false,"id":287848,"user_name":"kingcall","can_delete":false,"product_type":"c1","uid":1056982,"ip_address":"","ucode":"508884DC684B5B","user_header":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","comment_is_top":false,"comment_ctime":1618194651,"is_pvip":false,"replies":[{"id":"104550","content":"Perfect x 2！两道题都是满分💯~<br><br>不过，第一题，我再追问一句，当然，这么追问有点过分，哈哈，毕竟咱们这节课还没有讲不同Join的实现方式（26讲会展开）。追问的问题是：<br>SHUFFLE_MERGE,<br>SHUFFLE_HASH,<br>SHUFFLE_REPLICATE_NL<br>这几个hints的作用和效果，分别是什么？他们分别适合哪些场景？不妨提前思考一下，等到了26讲，咱们再细聊哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618221062,"ip_address":"","comment_id":287848,"utype":1}],"discussion_count":3,"race_medal":0,"score":"104697409755","product_id":100073401,"comment_content":"第一题：可以参考JoinStrategyHint.scala <br>    BROADCAST,<br>    SHUFFLE_MERGE,<br>    SHUFFLE_HASH,<br>    SHUFFLE_REPLICATE_NL<br>第二题:本质上是一样的，sql 的broadcast返回值是一个Dataset[T]，而sparkContext.broadcast的返回值是一个Broadcast[T] 类型值，需要调用value方法，才能返回被广播出去的变量,所以二者在使用的使用的体现形式上，sparkContext.broadcast 需要你调用一次value 方法才能和其他DF 进行join,下面提供一个demo 进行说明<br><br>    import org.apache.spark.sql.functions.broadcast<br><br>    val transactionsDF: DataFrame = sparksession.range(100).toDF(&quot;userID&quot;)<br>    val userDF: DataFrame = sparksession.range(10, 20).toDF(&quot;userID&quot;)<br><br>    val bcUserDF = broadcast(userDF)<br>    val bcUserDF2 = sparkContext.broadcast(userDF)<br>    val dataFrame = transactionsDF.join(bcUserDF, Seq(&quot;userID&quot;), &quot;inner&quot;)<br>    dataFrame.show()<br>    val dataFrame2 = transactionsDF.join(bcUserDF2.value, Seq(&quot;userID&quot;), &quot;inner&quot;)<br>    dataFrame2.show()<br>","like_count":25,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518441,"discussion_content":"Perfect x 2！两道题都是满分💯~\n\n不过，第一题，我再追问一句，当然，这么追问有点过分，哈哈，毕竟咱们这节课还没有讲不同Join的实现方式（26讲会展开）。追问的问题是：\nSHUFFLE_MERGE,\nSHUFFLE_HASH,\nSHUFFLE_REPLICATE_NL\n这几个hints的作用和效果，分别是什么？他们分别适合哪些场景？不妨提前思考一下，等到了26讲，咱们再细聊哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618221062,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368010,"discussion_content":"追问的，正是想要问的。尤其SHUFFLE_REPLICATE_NL","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618538929,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375668,"discussion_content":"这块可以参考第26讲，那里会回答上面的问题~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621809394,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":368010,"ip_address":""},"score":375668,"extra":""}]}]},{"had_liked":false,"id":288584,"user_name":"YJ","can_delete":false,"product_type":"c1","uid":1368790,"ip_address":"","ucode":"D75733726E39C6","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaELOB7pvNfq404zOrBt7OfibficJmfaTHbd14w0Om7VRUakQWnEzmbbpJHGTmRYp0ibA31oJAZUVruatA/132","comment_is_top":false,"comment_ctime":1618551487,"is_pvip":false,"replies":[{"id":"104757","content":"好问题， 这种写法，Spark不会复用先前的广播变量，所以第二次的Broadcast会重复计算。<br><br>复用广播最保险的方式，是这种写法：<br>val bcSmallTable = sparkContext.broadcast(smallTable)<br>  <br>bigTableA.Join(bcSmallTable.value, ...);<br>bigTableB.Join(bcSmallTable.value, ...);  ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618585585,"ip_address":"","comment_id":288584,"utype":1}],"discussion_count":3,"race_medal":0,"score":"53158159039","product_id":100073401,"comment_content":"老师，我有一个问题。 <br>bigTableA.Join(broadcast(smallTable), ...);<br>bigTableB.Join(broadsast(smallTable), ...);<br>bigTableA.Join(bigTableB, ...);<br>这里 广播了的smallTable 会被第二个join重用吗？ 还是说会被广播两次？","like_count":13,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518673,"discussion_content":"好问题， 这种写法，Spark不会复用先前的广播变量，所以第二次的Broadcast会重复计算。\n\n复用广播最保险的方式，是这种写法：\nval bcSmallTable = sparkContext.broadcast(smallTable)\n  \nbigTableA.Join(bcSmallTable.value, ...);\nbigTableB.Join(bcSmallTable.value, ...);  ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618585585,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369117,"discussion_content":"对，你说的这些方法，都不会复用广播，val bcSmallTable = sparkContext.broadcast(smallTable)，这种方法是最稳妥的。调用value没有问题~","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1618930854,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1368790,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaELOB7pvNfq404zOrBt7OfibficJmfaTHbd14w0Om7VRUakQWnEzmbbpJHGTmRYp0ibA31oJAZUVruatA/132","nickname":"YJ","note":"","ucode":"D75733726E39C6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368309,"discussion_content":"追问一下，所以说使用spark sql的Broadcast()或hint或autoBroadcast都不会重用，一定要用spark core方法切实的广播。   另外val bcSmallTable = spaekContext.broadcast(smallTable).value;  再直接使用这个value去join会有问题吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618651163,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293327,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1621336548,"is_pvip":false,"replies":[{"id":"106615","content":"好问题~<br><br>这里我先说声抱歉，看到你这个问题，我意识到我原文没有说清楚。是这样的，我们分两种情况来说。为了叙述方便，我把你的数据集记为D，也就是磁盘中133.4MB，展开到内存是1000MB。<br><br>第一种情况，数据集D上不存在Cache，这个时候那D直接参与后续的计算，比如Join，由于Spark SQL无从判断D在内存中的大小，因此它会“偷懒”地直接拿磁盘大小作为判断依据，也就是你上面说的那种情况，广播阈值大于133.4MB，就可以广播，反之则不行。不得不说，这个“懒”偷的有点过分，不过现阶段Spark SQL就是这么做的，这个和Spark的版本没有关系，3.0中Spark SQL也是这么处理的。<br><br>第二种情况，数据集之上有Cache，这个时候，Spark SQL判定的依据，就是Cache中的存储大小，也就是1000MB，所以你不妨再做个实验，也就是用Cache过后的D再去做Join，这个时候，你会发现，广播阈值必须要大于1000MB才行。<br><br>总结下来，对于数据集大小的判定，Cache与否完全不同，不Cache则直接用磁盘存储大小，Cache过后则使用Cache中的存储大小。需要特别注意的是，第一种情况，也就是不加Cache的时候，Spark SQL简单粗暴地直接用磁盘存储大小，坦白讲是存在隐患的，也就是创建广播变量创建过程中Driver端的OOM问题，或是广播变量创建过程中的Time out问题。因此，即便在那种情况之下，Spark SQL会选择参考磁盘存储大小，但是我的建议还是事先估算好数据集在内存中的存储大小，做到有备无患。<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621691389,"ip_address":"","comment_id":293327,"utype":1}],"discussion_count":4,"race_medal":0,"score":"48865976804","product_id":100073401,"comment_content":"老师我做了一个测试，我的表数据是parquet存储，snappy压缩的，磁盘的存储大小为133.4M。我将广播变量的阈值调到了134M，它却可以自动广播；当我将阈值调到132M，则不会自动广播。<br>我用老师的方法做了一个数据在内存展开的预估，大概1000M左右，那么为什么我按照磁盘的大小设定广播阈值，它能够广播呢？","like_count":12,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520161,"discussion_content":"好问题~\n\n这里我先说声抱歉，看到你这个问题，我意识到我原文没有说清楚。是这样的，我们分两种情况来说。为了叙述方便，我把你的数据集记为D，也就是磁盘中133.4MB，展开到内存是1000MB。\n\n第一种情况，数据集D上不存在Cache，这个时候那D直接参与后续的计算，比如Join，由于Spark SQL无从判断D在内存中的大小，因此它会“偷懒”地直接拿磁盘大小作为判断依据，也就是你上面说的那种情况，广播阈值大于133.4MB，就可以广播，反之则不行。不得不说，这个“懒”偷的有点过分，不过现阶段Spark SQL就是这么做的，这个和Spark的版本没有关系，3.0中Spark SQL也是这么处理的。\n\n第二种情况，数据集之上有Cache，这个时候，Spark SQL判定的依据，就是Cache中的存储大小，也就是1000MB，所以你不妨再做个实验，也就是用Cache过后的D再去做Join，这个时候，你会发现，广播阈值必须要大于1000MB才行。\n\n总结下来，对于数据集大小的判定，Cache与否完全不同，不Cache则直接用磁盘存储大小，Cache过后则使用Cache中的存储大小。需要特别注意的是，第一种情况，也就是不加Cache的时候，Spark SQL简单粗暴地直接用磁盘存储大小，坦白讲是存在隐患的，也就是创建广播变量创建过程中Driver端的OOM问题，或是广播变量创建过程中的Time out问题。因此，即便在那种情况之下，Spark SQL会选择参考磁盘存储大小，但是我的建议还是事先估算好数据集在内存中的存储大小，做到有备无患。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621691389,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1504510,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/t1aR8h117KxusZQHQ9urp6hr3jMA9icnWR3tLlYZ5M1wbgXIqRTKfLHJ9iciaTgliaPhfV5s5fYrARMZySKHltMlUg/132","nickname":"Gti","note":"","ucode":"21995DFB1AAFB3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388225,"discussion_content":"如果是分布式的数据集，磁盘大小是怎么估算的？所有数据分片的大小之和？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628661961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1598796,"avatar":"https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg","nickname":"welldo","note":"","ucode":"D38E75364CD2E3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375457,"discussion_content":"请教一下, 我也做了类似的测试, 但是我看日志时,我不知道如何确认spark是否做了&#34;自动广播&#34;, 请问一下如何确认呢?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621669500,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1598796,"avatar":"https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg","nickname":"welldo","note":"","ucode":"D38E75364CD2E3","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375670,"discussion_content":"可以用explain或是Spark UI查看执行计划，其中会明确是否做了Broadcast join，搜索Broadcast exchange、Broadcast Hash Join关键字即可","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621809691,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":375457,"ip_address":""},"score":375670,"extra":""}]}]},{"had_liked":false,"id":309730,"user_name":"Geek1185","can_delete":false,"product_type":"c1","uid":2028954,"ip_address":"","ucode":"47BEE492EF4C1A","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9a/63dc81a2.jpg","comment_is_top":false,"comment_ctime":1630315076,"is_pvip":false,"replies":[{"id":"112651","content":"好问题~ 这主要是因为left join的计算逻辑。简单的回答是，广播左表、或者说基表，没用！<br><br>为什么这么说呢，你不妨仔细想想left join的原理。它是把inner join的结果，再加上不满足关联条件的结果。<br><br>如果是广播右表的话，左表的数据分区，能看到右表的全量数据，不管是满足关联条件和不满足关联条件，左表的数据分区都能立即得到答案。<br><br>但是，反过来不行，比如把左表广播了，右表“待在原地、保持不动”，那对于左表来说，对于每一个右表的数据分区来说，左表没有全局视角，它只能知道哪些右表分区的数据满足关联条件，但是，它不知道在全局情况下，到底有没有不满足关联条件、需要生成null的数据记录。<br><br>可能说的有点绕，你不妨仔细想想left join的计算原理，就能明白，在left join下，广播左表是没有用的；同理，right join，广播右表，也没有用。因为他们都不能实现left&#47;right join原本的计算逻辑~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630939940,"ip_address":"","comment_id":309730,"utype":1}],"discussion_count":2,"race_medal":0,"score":"27400118852","product_id":100073401,"comment_content":"为什么left join的时候不能广播左边的小表呢？几百行的表左连接几亿行的表（业务上要求即便没关联上也要保留左表的记录）。<br>就像为什么left join时，左表在on的谓词不能下推？<br>我不太明白，希望老师解答","like_count":7,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525986,"discussion_content":"好问题~ 这主要是因为left join的计算逻辑。简单的回答是，广播左表、或者说基表，没用！\n\n为什么这么说呢，你不妨仔细想想left join的原理。它是把inner join的结果，再加上不满足关联条件的结果。\n\n如果是广播右表的话，左表的数据分区，能看到右表的全量数据，不管是满足关联条件和不满足关联条件，左表的数据分区都能立即得到答案。\n\n但是，反过来不行，比如把左表广播了，右表“待在原地、保持不动”，那对于左表来说，对于每一个右表的数据分区来说，左表没有全局视角，它只能知道哪些右表分区的数据满足关联条件，但是，它不知道在全局情况下，到底有没有不满足关联条件、需要生成null的数据记录。\n\n可能说的有点绕，你不妨仔细想想left join的计算原理，就能明白，在left join下，广播左表是没有用的；同理，right join，广播右表，也没有用。因为他们都不能实现left/right join原本的计算逻辑~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630939940,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2955154,"avatar":"https://static001.geekbang.org/account/avatar/00/2d/17/92/0af520ef.jpg","nickname":"十月","note":"","ucode":"95877CD3753595","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":570723,"discussion_content":"“简单的回答是，广播左表、或者说基表，没用！” 这句话和正文中的“二来，在所有的数据关联中，我们不能广播基表。”这句话中的基表是不是误将驱动表写成基表了呢？基表通常不应该是较小的表么，并且老师在基础篇的第17讲中给出过定义：“在探讨关联机制的时候，我们又常常把左表称作是“驱动表”，而把右表称为“基表””，往老师解答","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1651892610,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":307596,"user_name":"周俊","can_delete":false,"product_type":"c1","uid":2723993,"ip_address":"","ucode":"018FF939002C0B","user_header":"https://static001.geekbang.org/account/avatar/00/29/90/99/24cccca5.jpg","comment_is_top":false,"comment_ctime":1629169113,"is_pvip":true,"replies":[{"id":"111548","content":"是这样的，每个广播的上限是8G，超过这个限制，Spark直接抛异常。所以，这个8G是以广播变量为粒度的。15个小表的话，只要他们各自不超过8G就没事。不过，话说，同时搞15个广播变量，Driver不一定受得了~ 当然，要看你的每个广播变量有多大了。<br><br>不过anyway，只要你Driver内存足够大，每个广播又不超过8G，就没事。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629368977,"ip_address":"","comment_id":307596,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10219103705","product_id":100073401,"comment_content":"老师，假设我有16张表需要连接，其余15张都是小表，如果我将15张小表都做成广播变量，假设他们的总数据量超过了8G，是不是会直接OOM呀，还是说只要每一个广播变量不超过8g,就不会有问题。","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525208,"discussion_content":"是这样的，每个广播的上限是8G，超过这个限制，Spark直接抛异常。所以，这个8G是以广播变量为粒度的。15个小表的话，只要他们各自不超过8G就没事。不过，话说，同时搞15个广播变量，Driver不一定受得了~ 当然，要看你的每个广播变量有多大了。\n\n不过anyway，只要你Driver内存足够大，每个广播又不超过8G，就没事。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629368977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294404,"user_name":"笨小孩","can_delete":false,"product_type":"c1","uid":1799778,"ip_address":"","ucode":"9C43C0BEA34F4C","user_header":"https://static001.geekbang.org/account/avatar/00/1b/76/62/74e6d2d1.jpg","comment_is_top":false,"comment_ctime":1621930061,"is_pvip":false,"replies":[{"id":"106839","content":"不会的~ 和语法没什么关系，主要是看表大小。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621952522,"ip_address":"","comment_id":294404,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10211864653","product_id":100073401,"comment_content":"老师你好  在SparkSql中使用类似with  as  这样的语法  会自动广播这张表嘛","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520641,"discussion_content":"不会的~ 和语法没什么关系，主要是看表大小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621952522,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288935,"user_name":"Jefitar","can_delete":false,"product_type":"c1","uid":1370659,"ip_address":"","ucode":"D7ED9F32ADA5B1","user_header":"https://static001.geekbang.org/account/avatar/00/14/ea/23/508f71e3.jpg","comment_is_top":false,"comment_ctime":1618793526,"is_pvip":false,"replies":[{"id":"104877","content":"具体细节可以参考这里哈：https:&#47;&#47;databricks.com&#47;blog&#47;2015&#47;04&#47;28&#47;project-tungsten-bringing-spark-closer-to-bare-metal.html<br><br>java.lang.String object internals:<br>OFFSET  SIZE   TYPE DESCRIPTION                    VALUE<br>     0     4        (object header)                ...<br>     4     4        (object header)                ...<br>     8     4        (object header)                ...<br>    12     4 char[] String.value                   []<br>    16     4    int String.hash                    0<br>    20     4    int String.hash32                  0<br>Instance size: 24 bytes (reported by Instrumentation API)","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618814044,"ip_address":"","comment_id":288935,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10208728118","product_id":100073401,"comment_content":"老师，有个问题，字符串“abcd”只需要消耗 4 个字节，为什么JVM 在堆内存储这 4 个字符串总共需要消耗 48 个字节？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518774,"discussion_content":"具体细节可以参考这里哈：https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\n\njava.lang.String object internals:\nOFFSET  SIZE   TYPE DESCRIPTION                    VALUE\n     0     4        (object header)                ...\n     4     4        (object header)                ...\n     8     4        (object header)                ...\n    12     4 char[] String.value                   []\n    16     4    int String.hash                    0\n    20     4    int String.hash32                  0\nInstance size: 24 bytes (reported by Instrumentation API)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618814044,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311133,"user_name":"魏海霞","can_delete":false,"product_type":"c1","uid":2438607,"ip_address":"","ucode":"961DED8B696936","user_header":"https://static001.geekbang.org/account/avatar/00/25/35/cf/aa4f9c65.jpg","comment_is_top":false,"comment_ctime":1631083578,"is_pvip":true,"replies":[{"id":"112912","content":"能把执行计划贴出来吗？（可以在join完成之后的DataFrame之上调用explain）<br><br>我想知道这4张表的关联顺序，讲道理，应该是:<br>((a inner join b) inner join c) left join d<br>这是我的理解，如果是这样的话，理论上应该都是Broadcast Join才对，可以把explain执行计划贴出来确认一下~<br><br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631345773,"ip_address":"","comment_id":311133,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5926050874","product_id":100073401,"comment_content":"老师您好，用sparksql开发，遇到一个写了hint也不走broadcast的情况。具体情况是这样的，A表是个大表,有20多亿条记录，b,c,d都是小表，表就几个字段，数据最多也就3000多条，select &#47;*+ broadcast(b,c,d) from a join b jion c left join d<br>执行计划里b c都用的是BroadcastHashJOIN,d表是SortMergeJoin。d表不走bhj的原因大概是什么？能给个思路吗？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526486,"discussion_content":"能把执行计划贴出来吗？（可以在join完成之后的DataFrame之上调用explain）\n\n我想知道这4张表的关联顺序，讲道理，应该是:\n((a inner join b) inner join c) left join d\n这是我的理解，如果是这样的话，理论上应该都是Broadcast Join才对，可以把explain执行计划贴出来确认一下~\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631345773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2438607,"avatar":"https://static001.geekbang.org/account/avatar/00/25/35/cf/aa4f9c65.jpg","nickname":"魏海霞","note":"","ucode":"961DED8B696936","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":393582,"discussion_content":"执行顺序就是老师写的关联顺序\n我的这个小表是一个压缩表，压缩策略是snappy,我们给它改成了文本表，然后它就走了broadcast了。虽然执行计划对了，但是原理我没有想明白","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631503207,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":331703,"user_name":"Sampson","can_delete":false,"product_type":"c1","uid":1418226,"ip_address":"","ucode":"BA78CA29A6D898","user_header":"https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg","comment_is_top":false,"comment_ctime":1642731920,"is_pvip":true,"replies":[{"id":"121207","content":"这个配置项spark.sql.autoBroadcastJoinThreshold，它的含义是，size不大于这个设置的数据集，可以被广播。从log来看，你的广播变量小于1G，所以没有问题~ 并不是说你设置多少，广播变量大小就是多少","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1642780693,"ip_address":"","comment_id":331703,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1642731920","product_id":100073401,"comment_content":"磊哥你好 ，请教一下，我在我的任务中设置的如下的参数提交Spark任务，<br>--master yarn --deploy-mode cluster --num-executors 20 --executor-cores 1 --executor-memory 5G --driver-memory 2G  --conf spark.yarn.executor.memoryOverhead=2048M --conf spark.sql.shuffle.partitions=30 --conf spark.default.parallelism=30 --conf spark.sql.autoBroadcastJoinThreshold=1024 <br><br>按照上文中讲到的我设置了广播变量的阀值是 1024 = 1G ，但是看任务运行中的日志 <br><br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai7:38123 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai9:38938 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai5:39487 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai7:46429 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai5:41456 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai7:40246 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai5:45320 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai4:41769 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai4:38896 (size: 14.5 KB, free: 2.5 GB)<br>storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on miai4:35351 (size: 14.5 KB, free: 2.5 GB)<br><br>并不是我设置的1G呀 ，这是为什么呢 ？<br><br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":547655,"discussion_content":"这个配置项spark.sql.autoBroadcastJoinThreshold，它的含义是，size不大于这个设置的数据集，可以被广播。从log来看，你的广播变量小于1G，所以没有问题~ 并不是说你设置多少，广播变量大小就是多少","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642780693,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":329626,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1641451962,"is_pvip":false,"replies":[{"id":"120202","content":"最早在哪里看到的，说实话我也忘了[允悲]，太久了。不过这个方法一定是切实可行的，日常的工作中验证无数次了哈~<br><br>需要提醒的是，要预估数据集的内存占用，需要先把数据集cache起来，否则的话，sizeInBytes给出的，实际上源文件在磁盘上的存储大小，切记切记","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1641722102,"ip_address":"","comment_id":329626,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641451962","product_id":100073401,"comment_content":"老师您好 请问<br>val plan = df.queryExecution.logicalval estimated: BigInt = spark.sessionState.executePlan(plan).optimizedPlan.stats.sizeInBytes<br>这个查看内存占用的方法是在哪里看到的呢？我在官方文档 <br>https:&#47;&#47;spark.apache.org&#47;docs&#47;2.4.0&#47;api&#47;scala&#47;index.html#org.apache.spark.sql.DataFrameNaFunctions 里把这些方法都搜了一遍没有搜到QAQ","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":544814,"discussion_content":"最早在哪里看到的，说实话我也忘了[允悲]，太久了。不过这个方法一定是切实可行的，日常的工作中验证无数次了哈~\n\n需要提醒的是，要预估数据集的内存占用，需要先把数据集cache起来，否则的话，sizeInBytes给出的，实际上源文件在磁盘上的存储大小，切记切记","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641722102,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":306674,"user_name":"猿鸽君","can_delete":false,"product_type":"c1","uid":1991951,"ip_address":"","ucode":"8562D8C5AD3D1E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/65/0f/7b9f27f2.jpg","comment_is_top":false,"comment_ctime":1628667185,"is_pvip":false,"replies":[{"id":"111186","content":"你说的是DataFrame.stat下面的那些统计函数吗？比如：<br>approxQuantile   bloomFilter   corr   countMinSketch   cov   crosstab   freqItems   sampleBy？<br><br>我这边3.0试了不需要SQLConf。<br><br>另外，如果你想获取和执行计划有关的信息，还可以用DataFrame.queryExecution.* 下面的那些函数，比如executedPlan、optimizedPlan、sparkPlan，等等。当然，这些都是些不同阶段的执行计划，没有你说的统计数据。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1628838176,"ip_address":"","comment_id":306674,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1628667185","product_id":100073401,"comment_content":"老师好。我们spark是2.2.0，sparksql是2.11。我想模拟读取 Spark SQL 执行计划的统计数据时。在调用stats时却需要传一个SQLConf类型的参数。请问这是版本的问题吗？有什么替代的方法？感谢","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524865,"discussion_content":"你说的是DataFrame.stat下面的那些统计函数吗？比如：\napproxQuantile   bloomFilter   corr   countMinSketch   cov   crosstab   freqItems   sampleBy？\n\n我这边3.0试了不需要SQLConf。\n\n另外，如果你想获取和执行计划有关的信息，还可以用DataFrame.queryExecution.* 下面的那些函数，比如executedPlan、optimizedPlan、sparkPlan，等等。当然，这些都是些不同阶段的执行计划，没有你说的统计数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628838176,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301207,"user_name":"臻果爸爸","can_delete":false,"product_type":"c1","uid":1021794,"ip_address":"","ucode":"03DC9615B024A4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/97/62/1a2d7825.jpg","comment_is_top":false,"comment_ctime":1625574701,"is_pvip":false,"replies":[{"id":"109179","content":"2个思路哈~<br><br>1）结合你说的现象，就是只有一个task是long running，那么大概率是有数据倾斜问题，因此不妨先看看，数据集在哪里发生data skew，然后定向地去除倾斜问题。数据倾斜的应对办法，我们在29讲数据倾斜那个部分有详细展开，不妨关注一下<br><br>2）既然定位到是GC问题，那么不妨通过JVM参数(spark.driver.extraJavaOptions中添加-XX:+PrintGCDetails)，来记录GC日志，然后通过一些可视化工具来检查GC行为，比如是新生代的多，还是老年代的多。就你这个Case来说，我猜测，大概率是老年代的多，那你就要看，是不是Cache加的太多了，还是其他什么东西占用内存太久了，没能及时清理。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1625801909,"ip_address":"","comment_id":301207,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1625574701","product_id":100073401,"comment_content":"spark sql执行时，有一个task一直running，但是执行耗时等sparkui参数都为0，只有gc时间一直在增加，想问下这个怎么排查？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522929,"discussion_content":"2个思路哈~\n\n1）结合你说的现象，就是只有一个task是long running，那么大概率是有数据倾斜问题，因此不妨先看看，数据集在哪里发生data skew，然后定向地去除倾斜问题。数据倾斜的应对办法，我们在29讲数据倾斜那个部分有详细展开，不妨关注一下\n\n2）既然定位到是GC问题，那么不妨通过JVM参数(spark.driver.extraJavaOptions中添加-XX:+PrintGCDetails)，来记录GC日志，然后通过一些可视化工具来检查GC行为，比如是新生代的多，还是老年代的多。就你这个Case来说，我猜测，大概率是老年代的多，那你就要看，是不是Cache加的太多了，还是其他什么东西占用内存太久了，没能及时清理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625801909,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":297210,"user_name":"闯闯","can_delete":false,"product_type":"c1","uid":1544692,"ip_address":"","ucode":"D32958F8EF26BD","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83errHypG6kuO0V0bRwp74rm8srjoQ4zXUBNNLMcY19uNdz8Ea3rOFuBJibXMHWePMwBYpGsyyxiav0ibw/132","comment_is_top":false,"comment_ctime":1623378758,"is_pvip":false,"replies":[{"id":"107956","content":"quote：“这说明是不是可以简化呢。”<br><br>具体指的是简化什么呢？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1623395609,"ip_address":"","comment_id":297210,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1623378758","product_id":100073401,"comment_content":"老师有个疑问，看了您的文章后，动手试了下：<br>df.queryExecution.optimizedPlan.stats.sizeInBytes<br>这段代码也是能够获取统计信息的。这说明是不是可以简化呢。看了源码发现，这个例子跟您的例子调用 optimizedPlan 是同一段代码","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521761,"discussion_content":"quote：“这说明是不是可以简化呢。”\n\n具体指的是简化什么呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623395609,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291792,"user_name":"Geek_01eb83","can_delete":false,"product_type":"c1","uid":1637240,"ip_address":"","ucode":"15E5A8D93BA470","user_header":"https://static001.geekbang.org/account/avatar/00/18/fb/78/dc5a1035.jpg","comment_is_top":false,"comment_ctime":1620485298,"is_pvip":false,"replies":[{"id":"105737","content":"能提供更多细节吗？比如示例代码。如果仅仅是在每一次sqlContext.sql()调用之后添加count，就可以提升执行性能，这个实在是解释不过去。如果count之前，分布式数据集加了Cache，还情有可原，但如果什么都没有，单纯靠加count提升性能，坦白说，我真的很好奇这是怎么做到的。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620656698,"ip_address":"","comment_id":291792,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1620485298","product_id":100073401,"comment_content":"老师，您好！请教一个问题，最近用DataFrame编写spark程序，程序中通过sqlContext.sql()的方式处理Hive上的数据，发现速度很慢（整个程序很长，用了很多次sqlContext.sql()，并且注册了临时表）。最后在每一步的sqlContext.sql()语句后面加上了count（也即是action算子），其他没有改动，这样整个程序快了很多。想麻烦问下这是什么原因？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519572,"discussion_content":"能提供更多细节吗？比如示例代码。如果仅仅是在每一次sqlContext.sql()调用之后添加count，就可以提升执行性能，这个实在是解释不过去。如果count之前，分布式数据集加了Cache，还情有可原，但如果什么都没有，单纯靠加count提升性能，坦白说，我真的很好奇这是怎么做到的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620656698,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1637240,"avatar":"https://static001.geekbang.org/account/avatar/00/18/fb/78/dc5a1035.jpg","nickname":"Geek_01eb83","note":"","ucode":"15E5A8D93BA470","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":376857,"discussion_content":"val df1 = sqlContext.sql().persist()\nval df2 = sqlContext.sql().persist()\nval df3 = df1.join(df2).persist()\nval df4 = sqlContext.sql().persist()\nval df = df3.join(df4)\n类似这种代码，里面加了挺多缓存的，用的是persist函数。\n        我的理解是，每个df下面加了count这种执行算子后，强制把DataFrame执行了，然后把结果缓存起来。但是，不太理解的是，为啥不加执行算子就慢？是不是不加执行算子后spark优化执行计划的时候出了问题？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622383687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1637240,"avatar":"https://static001.geekbang.org/account/avatar/00/18/fb/78/dc5a1035.jpg","nickname":"Geek_01eb83","note":"","ucode":"15E5A8D93BA470","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388752,"discussion_content":"有可能两者的执行计划有区别，需要具体对比下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628935204,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":376857,"ip_address":""},"score":388752,"extra":""}]}]},{"had_liked":false,"id":289204,"user_name":"耳东","can_delete":false,"product_type":"c1","uid":1266059,"ip_address":"","ucode":"70F69C219EF10B","user_header":"https://static001.geekbang.org/account/avatar/00/13/51/8b/29ed1c41.jpg","comment_is_top":false,"comment_ctime":1618915357,"is_pvip":false,"replies":[{"id":"104943","content":"先说join形式，left join，一定是大表放左边，小表放右边。right join，反过来，一定是小表放左边，大表放右边~ 这个是关联形式决定的，不然干嘛叫“左”、“右”连接呢？<br><br>再说Broadcast Joins，参与关联的两张表，要广播的话，一定是广播小表，原因虽然我们本讲没有直接说，不过结合上一讲Broadcast Join和Shuffle Joins的计算过程和原理，其实可以自行推导出来哈~<br><br>不妨反向思维，如果你强行广播大表，会发生什么？广播大表的收益是什么？广播大表的收益，是否大于原Shuffle Joins的执行性能？不妨想一想哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618917953,"ip_address":"","comment_id":289204,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1618915357","product_id":100073401,"comment_content":"在左连接（Left Outer Join）中，我们只能广播右表；在右连接（Right Outer Join）中，我们只能广播左表。  这段的意思是指在 left outer join时 大表放左边 ，小表放右边吗 ？为什么？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518849,"discussion_content":"先说join形式，left join，一定是大表放左边，小表放右边。right join，反过来，一定是小表放左边，大表放右边~ 这个是关联形式决定的，不然干嘛叫“左”、“右”连接呢？\n\n再说Broadcast Joins，参与关联的两张表，要广播的话，一定是广播小表，原因虽然我们本讲没有直接说，不过结合上一讲Broadcast Join和Shuffle Joins的计算过程和原理，其实可以自行推导出来哈~\n\n不妨反向思维，如果你强行广播大表，会发生什么？广播大表的收益是什么？广播大表的收益，是否大于原Shuffle Joins的执行性能？不妨想一想哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618917953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028954,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/9a/63dc81a2.jpg","nickname":"Geek1185","note":"","ucode":"47BEE492EF4C1A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":391146,"discussion_content":"不应定left join左边要放大表吧，比如我想看某个人群的运动轨迹，左表是人群（几千个），右表是轨迹（几百亿），没有轨迹的人也要保留，就用left join，那此时左表为什么不能广播呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630314860,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288550,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1618536616,"is_pvip":false,"replies":[{"id":"104752","content":"3.0之前只支持Broadcast hint，3.0之后支持的就多了：<br>BROADCAST<br>MERGE<br>SHUFFLE_HASH<br>SHUFFLE_REPLICATE_NL","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618566806,"ip_address":"","comment_id":288550,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1618536616","product_id":100073401,"comment_content":"Hint好像是spark2.4版本之后才会有的吧，我公司版本是2.2，但是我之前使用broadcast不管用，然后试了一下hint,居然生效了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518657,"discussion_content":"3.0之前只支持Broadcast hint，3.0之后支持的就多了：\nBROADCAST\nMERGE\nSHUFFLE_HASH\nSHUFFLE_REPLICATE_NL","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618566806,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}