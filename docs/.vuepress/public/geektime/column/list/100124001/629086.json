{"id":629086,"title":"53｜容器化实战：怎样搭建K8s爬虫集群？","content":"<p>你好，我是郑建勋。</p><p>上节课，我们介绍了 Kubernetes 架构和相关的原理。这节课让我们更进一步，将爬虫项目相关的微服务部署到 Kubernetes 中。</p><h2>安装 Kubernetes 集群</h2><p>首先，我们需要准备好Kubernetes的集群。部署 Kubernetes 集群的方式有很多种，典型的方式有下面几种：</p><ul>\n<li>Play with Kubernetes (PWK)</li>\n<li>Docker Desktop</li>\n<li>云厂商的k8s服务，例如Google Kubernetes Engine (GKE)</li>\n<li>kops</li>\n<li>kubeadm</li>\n<li>k3s</li>\n<li>k3d</li>\n</ul><p>其中，<a href=\"https://labs.play-with-k8s.com/\">PWK</a> 是试验性质的免费的Kubernetes集群，只要有Docker或者Github账号就可以在浏览器上一键生成Kubernetes集群。但是它有诸多限制，例如一次只能使用4个小时，并且有扩展性和性能等问题。所以PWK一般只用于教学或者试验。</p><p>之前，我们在Windows和Mac中用Docker Desktop安装包来安装了Docker，其实利用最新的Docker Desktop，我们还可以在本地生成Kubernetes集群。使用Docker Desktop生成Kubernetes集群非常简单，我们只需要点击Docker的鲸鱼图标，并且在Preferences中勾选Enable Kubernetes，然后点击下方的Apply & restart 就可以创建我们的 Kubernetes 集群了。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/dd/d8/dd6ea310014be369041fcb4ea7c120d8.png?wh=1920x1089\" alt=\"图片\"></p><p>最后，Docker Desktop还提供了切换Kubernetes Context的能力，我们点击docker-desktop，这样我们通过kubectl发送的命令就会传到Docker Desktop构建的Kubernetes集群中。</p><p><img src=\"https://static001.geekbang.org/resource/image/23/a1/23f4e08849555a3963b14a3e2457d3a1.png?wh=1920x1925\" alt=\"图片\"></p><p>上面的这个界面操作和下面的命令行操作在功能上是相同的。</p><pre><code class=\"language-plain\">» kubectl config use-context docker-desktop                                                                     jackson@jacksondeMacBook-Pro\nSwitched to context \"docker-desktop\".\n</code></pre><p>接着，我们执行kubectl get nodes可以看到当前集群为单节点的集群，版本为v1.25.2。</p><pre><code class=\"language-plain\">» kubectl get nodes                                                                                             jackson@jacksondeMacBook-Pro\nNAME             STATUS   ROLES           AGE   VERSION\ndocker-desktop   Ready    control-plane   25d   v1.25.2\n</code></pre><p>Docker Desktop生成的Kubernetes集群可以用于开发测试，但是它不能模拟多节点的Kubernetes集群。</p><p>在一些生产环境中，我们可能需要手动部署多节点的Kubernetes集群。例如我们要以Kubernetes为基座为某企业部署一套人脸识别系统，这时我们可以使用 <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\">kubeadm</a> 工具来安装Kubernetes集群。</p><p>kubeadm是 Kubernetes 1.4版本引入的命令行工具，它致力于简化集群的安装过程。如果要更精细地调整Kubernetes各组件服务的参数和安全设置，还可以用Kubernetes二进制文件的方式进行部署。kubeadm和二进制文件的安装方式你可以查看官方文档和《kubernetes权威指南》。</p><p>在另一些生产环境中，例如在私有云的场景下，如果为了应对高峰期而购入机器，容易导致机器闲置，带来资源浪费，这时我们可以借助云厂商的Kubernetes服务（Google GKE，Microsoft AKS，Amazon EKS，腾讯云，阿里云）搭建 Kubernetes 集群。以 <a href=\"https://cloud.google.com/kubernetes-engine\">GKE</a> 为例，GKE是运行在谷歌云平台上的Kubernetes托管服务，它可以为我们快速部署和管理生产级的Kubernetes 集群。要注意的是，部署在云厂商的Kubernetes集群一般都是需要付费的。</p><p>k3s是轻量级的Kubernetes集群，它通过删除Kubernetes中不必要的第三方存储驱动，删除与云厂商交互的代码，将Kubernetes组件合并到了不到100 MB的二进制文件中去运行，这就减少了二进制文件与内存占用的大小。这个方案适用于物联网等对资源吃紧的环境，也可以用于CI及开发环境。</p><p>而k3d是一个社区驱动的项目，它对k3s进行了封装，从而可以在Docker中创建单节点或多节点的 Kubernetes 集群。使用k3d，我们用一行指令就可以创建Kubernetes 集群。这节课，我们就用k3d搭建多节点、轻量级的Kubernetes集群，实现开发与测试的目的。</p><h2>安装 k3d</h2><p>k3d的安装方式比较简单，可以执行如下的脚本完成。</p><pre><code class=\"language-plain\">curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash\n</code></pre><p>如果想安装k3d的指定版本，可以使用下面的指令。</p><pre><code class=\"language-plain\">curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | TAG=v5.4.6 bash\n</code></pre><p>在Mac中，还可以使用brew进行安装，如下所示。其他的安装方式你可以查看<a href=\"https://k3d.io/v5.4.6/\">官方文档</a>。</p><pre><code class=\"language-plain\">brew install k3d\n</code></pre><p>安装完成后，执行 k3d version 可以查看k3d的版本信息和k3d依赖的k3s的版本。</p><pre><code class=\"language-plain\">» k3d version                                                                                                                   jackson@bogon\nk3d version v5.4.6\nk3s version v1.24.4-k3s1 (default)\n</code></pre><p>k3d的使用方法我推荐你阅读它的<a href=\"https://k3d.io/v5.4.6/\">官方文档</a>和k3d维护的一个交互式的<a href=\"https://github.com/k3d-io/k3d-demo\">demo项目</a>。这个demo项目可以完成从集群的创建到销毁，从Pod的创建到销毁的完整链路。你可以下载k3d-demo项目的代码，然后make demo，跟着它的提示一步步去学习。</p><pre><code class=\"language-plain\">git clone https://github.com/k3d-io/k3d-demo\nmake demo\n</code></pre><p>下面让我们来看看如何借助k3d搭建Kubernetes集群。</p><p>首先执行k3d cluster create命令，创建Kubernetes集群。</p><pre><code class=\"language-plain\">» k3d cluster create demo --api-port 6550 --servers 1 --c 3 --port 8080:80@loadbalancer --wait\nINFO[0000] portmapping '8080:80' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy]\nINFO[0000] Prep: Network\nINFO[0000] Created network 'k3d-demo'\nINFO[0000] Created image volume k3d-demo-images\nINFO[0000] Starting new tools node...\nINFO[0000] Starting Node 'k3d-demo-tools'\nINFO[0007] Creating node 'k3d-demo-server-0'\nINFO[0007] Creating node 'k3d-demo-agent-0'\nINFO[0007] Creating node 'k3d-demo-agent-1'\nINFO[0007] Creating node 'k3d-demo-agent-2'\nINFO[0007] Creating LoadBalancer 'k3d-demo-serverlb'\nINFO[0007] Using the k3d-tools node to gather environment information\nINFO[0007] Starting new tools node...\nINFO[0007] Starting Node 'k3d-demo-tools'\nINFO[0009] Starting cluster 'demo'\nINFO[0009] Starting servers...\nINFO[0009] Starting Node 'k3d-demo-server-0'\nINFO[0013] Starting agents...\nINFO[0014] Starting Node 'k3d-demo-agent-2'\nINFO[0014] Starting Node 'k3d-demo-agent-1'\nINFO[0014] Starting Node 'k3d-demo-agent-0'\nINFO[0019] Starting helpers...\nINFO[0019] Starting Node 'k3d-demo-serverlb'\nINFO[0026] Injecting records for hostAliases (incl. host.k3d.internal) and for 6 network members into CoreDNS configmap...\nINFO[0028] Cluster 'demo' created successfully!\n</code></pre><p>其中，运行参数api-port用于指定Kubernetes API server对外暴露的端口。servers用于指定master node的数量。agents用于指定worker node的数量。port用于指定宿主机端口到Kubernetes集群的映射。后面我们会看到，当我们访问宿主机8080端口时，实际上会被转发到集群的80端口。wait参数表示等待k3s集群准备就绪后指令才会返回。</p><p>创建好 Kubernetes 集群后，执行 k3d cluster list 可以看到当前创建好的demo集群信息。</p><pre><code class=\"language-plain\">» k3d cluster list  \nNAME   SERVERS   AGENTS   LOADBALANCER\ndemo   1/1       3/3      true\n</code></pre><p>要让kubectl客户端能够访问到我们刚创建好的Kubernetes集群，需要使用下面的指令，把当前集群的配置信息合并到kubeconfig文件中，然后切换Kubernetes Context，使kubectl能够访问到新建的集群。</p><pre><code class=\"language-plain\">k3d kubeconfig merge demo --kubeconfig-merge-default --kubeconfig-switch-context\n</code></pre><p>接下来，输入kubectl get nodes，可以看到当前集群的Master Node与Worker Node。</p><pre><code class=\"language-plain\">» kubectl get nodes                                       jackson@localhost\nNAME                STATUS   ROLES                  AGE     VERSION\nk3d-demo-server-0   Ready    control-plane,master   2m36s   v1.24.4+k3s1\nk3d-demo-agent-1    Ready    &lt;none&gt;                 2m31s   v1.24.4+k3s1\nk3d-demo-agent-0    Ready    &lt;none&gt;                 2m31s   v1.24.4+k3s1\nk3d-demo-agent-2    Ready    &lt;none&gt;                 2m31s   v1.24.4+k3s1\n</code></pre><h2>部署Worker Deployment</h2><p>创建好集群之后，我们要想办法将当前的爬虫项目部署到 Kubernetes 集群中。首先让我们书写一个crawl-worker.yaml文件，用它来创建Deployment资源，管理我们的容器crawl-worker。描述文件如下。</p><pre><code class=\"language-plain\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crawler-deployment\n  labels:\n    app: crawl-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: crawl-worker\n  template:\n    metadata:\n      labels:\n        app: crawl-worker\n    spec:\n      containers:\n      - name: crawl-worker\n        image: crawler:local\n        command:\n          - sh\n          - -c\n          - \"./crawler worker --podip=${MY_POD_IP}\"\n        ports:\n        - containerPort: 8080\n        env:\n          - name: MY_POD_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.podIP\n</code></pre><p>下面我们来挨个解释一下文件中的信息。</p><ul>\n<li>apiVersion：定义创建对象时使用的 Kubernetes API的版本。apiVersion的一般形式为 <code>&lt;GROUP_NAME&gt;/&lt;VERSION&gt;</code>。在这里，Deployment的API group为app，版本为v1。而Pod对象由于位于特殊的核心组，可以省略掉 <code>&lt;GROUP_NAME&gt;</code>。apiVersion的设计有助于Kubernetes对不同的资源进行单独的管理，也有助于开发者创建Kubernetes中实验性质的资源进行测试。</li>\n<li>kind 表示当前资源的类型，在这里我们定义的是Deployment对象。之前我们提到过，Deployment在Pod之上增加了自动扩容、自动修复和滚动更新等功能。</li>\n<li><a href=\"http://metadata.name\">metadata.name</a>：定义deployment的名字。</li>\n<li>metadata.labels：给 Deployment 的标签。</li>\n<li>spec：代表对象的期望状态。</li>\n<li>spec.replicas：代表创建和管理的 Pod 的数量。</li>\n<li>spec.selector：定义了 Deployment Controller 要管理哪些Pod。这里定义的通常是标签匹配的Pod，满足该标签的Pod会被纳入到Deployment Controller 中去管理。</li>\n<li>spec.template.metadata：定义了Pod的属性，在上例中，定义了标签属性crawl-worker。</li>\n<li>spec.template.spec.containers：定义了Pod中的容器属性。</li>\n<li><a href=\"http://spec.template.spec.containers.name\">spec.template.spec.containers.name</a>：定义了容器的名字。</li>\n<li>spec.template.spec.containers.image：定义了容器的镜像。</li>\n<li>spec.template.spec.containers.command：定义了容器的启动命令。注意，启动参数中的 <code>${MY_POD_IP}</code> 是从环境变量中获取的MY_POD_IP对应的值。</li>\n<li>spec.template.spec.container.ports：描述服务暴露的端口信息，方便开发者更好地理解服务，没有实际的作用。</li>\n<li>spec.template.spec.container.env：定义容器的环境变量。在这里，我们定义了一个环境变量MY_POD_IP，并且传递了一个特殊的值，即Pod的IP。并将该环境变量的值传递到了运行参数当中。</li>\n</ul><p>这里我将Pod的IP传入到程序中有一个妙处。我们之前在程序运行时手动传入了Worker的ID，这在开发环境中是没有问题的。但是在生产环境中，我们希望Worker能够动态扩容，这时就不能手动地指定ID了。我们需要让程序在启动后自动生成一个ID，并且这个ID在分布式节点中是唯一的。</p><p>一些同学可能会想到把时间当作唯一的ID，例如使用 <code>time.Now().UnixNano()</code> 来获取Unix时间戳。但是，程序获取到的时间仍然可能是重复的，虽然概率很小。另一些同学可能会想到利用MySQL的自增主键或者借助etcd等分布式组件来得到分布式ID。这当然是一种解决办法，不过却依赖了额外的外部服务。在这里，我选择了一种更为巧妙的方法，即借助Kubernetes中Pod的IP不重复的特性，将Pod的IP传递到程序中，借助唯一的Pod IP生成唯一的ID。代码如下所示。</p><pre><code class=\"language-plain\">WorkerCmd.Flags().StringVar(\n\t\t&amp;podIP, \"podip\", \"\", \"set worker id\")\n\nWorkerCmd.Flags().StringVar(\n\t\t&amp;workerID, \"id\", \"\", \"set worker id\")\n\nvar podIP string\n\nif workerID == \"\" {\n\t\tif podIP != \"\" {\n\t\t\tip := generator.GetIDbyIP(podIP)\n\t\t\tworkerID = strconv.Itoa(int(ip))\n\t\t} else {\n\t\t\tworkerID = fmt.Sprintf(\"%d\", time.Now().UnixNano())\n\t\t}\n\t}\n\n// generator/generator.go\nfunc GetIDbyIP(ip string) uint32 {\n\tvar id uint32\n\tbinary.Read(bytes.NewBuffer(net.ParseIP(ip).To4()), binary.BigEndian, &amp;id)\n\treturn id\n}\n</code></pre><p>现在workerID的默认值为空，如果没有传递id flag，也没有传递podip flag，这一般是线下开发场景，我们直接使用Unix时间戳来生成ID。如果没有传递id flag，但传递了podip flag，我们则要利用Pod IP的唯一性生成ID。</p><p>准备好程序代码之后，让我们生成Worker的镜像，并打上镜像tag：crawler:local。 要注意的是，这里我们并没有和之前一样将镜像变为crawler:latest。 这是因为对于crawler:latest的镜像，Kubernetes会在DockerHub中默认<a href=\"https://kubernetes.io/docs/concepts/containers/images/#imagepullpolicy-defaulting\">拉取最新的镜像</a>，这会导致镜像拉取失败，这里我们希望 Kubernetes 使用本地缓存的镜像。</p><pre><code class=\"language-plain\">docker image build -t crawler:local .\n</code></pre><p>接着，将镜像导入到k3d集群中。</p><pre><code class=\"language-plain\">k3d image import crawler:local -c demo\n</code></pre><p>准备好镜像之后，我们就可以创建 Kubernetes 中的 Deployment 资源，用它管理我们的Worker节点了。具体步骤是，执行kubectl apply，告诉 Kubernetes 我们希望创建的Deployment资源的信息。</p><pre><code class=\"language-plain\">kubectl apply -f crawl-worker.yaml\n</code></pre><p>接下来，输入kubectl get po，查看default namespace中Pod的信息。可以看到，Kubernetes已经为我们创建了3个Pod。</p><pre><code class=\"language-plain\">» kubectl get po\nNAME                                  READY   STATUS    RESTARTS   AGE\ncrawler-deployment-6744cc644b-2mm9r   1/1     Running   0          88s\ncrawler-deployment-6744cc644b-bgjqc   1/1     Running   0          88s\ncrawler-deployment-6744cc644b-84t9g   1/1     Running   0          88s\n</code></pre><p>使用kubectl logs 打印出某一个Pod的日志，可以看到Worker节点已经正常地运行了。</p><p>还要注意的是，当前Worker节点的启动依赖于MySQL和etcd这两个组件。和之前一样，我们是先将这两个组件在宿主机中Docker启动的，后续这两个组件可以部署到Kubernetes集群中。</p><pre><code class=\"language-plain\">» kubectl logs -f crawler-deployment-6744cc644b-2mm9r  \n{\"level\":\"INFO\",\"ts\":\"2022-12-24T07:50:09.301Z\",\"caller\":\"worker/worker.go:101\",\"msg\":\"log init end\"}\n{\"level\":\"INFO\",\"ts\":\"2022-12-24T07:50:09.301Z\",\"caller\":\"worker/worker.go:109\",\"msg\":\"proxy list: [&lt;http://192.168.0.105:8888&gt; &lt;http://192.168.0.105:8888&gt;] timeout: 3000\"}\n{\"level\":\"DEBUG\",\"ts\":\"2022-12-24T07:50:09.311Z\",\"caller\":\"worker/worker.go:152\",\"msg\":\"grpc server config,{RegistryAddress::2379 RegisterTTL:60 RegisterInterval:15 Name:go.micro.server.worker ClientTimeOut:10}\"}\n{\"level\":\"DEBUG\",\"ts\":\"2022-12-24T07:50:09.313Z\",\"caller\":\"worker/worker.go:223\",\"msg\":\"start http server listening on :8080 proxy to grpc server;:9090\"}\n2022-12-24 07:50:09  file=worker/worker.go:196 level=info Starting [service] go.micro.server.worker\n2022-12-24 07:50:09  file=v4@v4.9.0/service.go:96 level=info Server [grpc] Listening on [::]:9090\n2022-12-24 07:50:09  file=grpc@v1.2.0/grpc.go:913 level=info Registry [etcd] Registering node: go.micro.server.worker-1\n</code></pre><p>现在让我们来尝试从外部访问一下Worker节点。<br>\n由于网络存在隔离性，当前要想从外部访问Worker节点还没有那么容易。但是我们之前讲过，Pod之间是可以通过IP相互连接的，所以我们打算通过一个 Pod 容器访问Worker节点。由于当前我们生成的crawler容器内没有内置网络请求工具，所以在这里我们用kubectl run启动一个带有curl工具的镜像curlimages/curl，并命名为mycurlpod。</p><pre><code class=\"language-plain\">kubectl run mycurlpod --image=curlimages/curl -i --tty -- sh\n</code></pre><p>如下，我们仍然使用kubectl get pod 查看当前Worker Pod 的IP地址，-o wide 可以帮助我们得到更详细的Pod信息。</p><pre><code class=\"language-plain\">» kubectl get pod -o wide                                                                                                      jackson@bogon\nNAME                                  READY   STATUS    RESTARTS   AGE   IP           NODE               NOMINATED NODE   READINESS GATES\ncrawler-deployment-6744cc644b-2mm9r   1/1     Running   0          40m   10.42.4.9    k3d-new-agent-1    &lt;none&gt;           &lt;none&gt;\ncrawler-deployment-6744cc644b-bgjqc   1/1     Running   0          40m   10.42.5.10   k3d-new-agent-0    &lt;none&gt;           &lt;none&gt;\ncrawler-deployment-6744cc644b-84t9g   1/1     Running   0          40m   10.42.3.6    k3d-demo-agent-2   &lt;none&gt;           &lt;none&gt;\nmycurlpod                             1/1     Running   0          26m   10.42.4.10   k3d-new-agent-1    &lt;none&gt;           &lt;none&gt;\n</code></pre><p>接着进入到mycurlpod中，利用Worker节点的Pod IP地址进行访问，成功返回预期数据。</p><pre><code class=\"language-plain\">\n» curl -H \"content-type: application/json\" -d '{\"name\": \"john\"}' http://10.42.4.9:8080/greeter/hello\n{\"greeting\":\"Hello \"}\n</code></pre><h2>部署Worker Service</h2><p>不过到这里还不能放松警惕。我们上节课说过，Pod的IP会随时发生变化，为了有稳定的IP来访问我们的Worker与Master，我们要创建一个文件crawl-worker-service.yaml，用它来描述Service资源。Service默认的类型为ClusterIP，该类型的Service IP只能够在集群内部访问。</p><pre><code class=\"language-plain\">kind: Service\napiVersion: v1\nmetadata:\n  name: crawl-worker\n  labels:\n    app: crawl-worker\nspec:\n  selector:\n    app: crawl-worker\n  ports:\n  - port: 8080\n    name: http\n  - port: 9090\n    name: grpc\n</code></pre><p>描述Service资源与之前描述Deployment资源非常类似。</p><ul>\n<li>kind: Service：指的是当前的Kubernetes资源类型为Service。</li>\n<li>apiVersion: v1：代表apiVersion的版本是v1，由于Service是核心类型，因此省略掉了API GROUP的命名前缀。</li>\n<li><a href=\"http://metadata.name\">metadata.name</a>：当前Service的名字。</li>\n<li>metadata.labels：当前Service的标签。</li>\n<li>spec.selector：选择器，表示当前Service管理哪些后台服务，只有标签为 app: crawl-worker的Pod才会受到该Service的管理，这些Pod就是我们的Worker节点。</li>\n<li>spec.ports.port：当前Service监听的端口号。例如。port: 8080指的是当前Service会监听8080端口。默认情况下，当外部访问该Service的8080端口时，会将请求转发给后端服务相同的端口。</li>\n<li>spec.ports.port.name：描述了当前Service端口规则的名字。</li>\n</ul><p>接下来使用 kubectl apply 创建Service资源，并使用kubectl get service得到Service的Cluster-IP地址。</p><pre><code class=\"language-plain\">» kubectl  apply -f crawl-worker-service.yaml\n» kubectl get service \nNAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\ncrawl-worker   ClusterIP   10.43.245.115    &lt;none&gt;        8080/TCP,9090/TCP   13m\n</code></pre><p>同样，在包含curl工具的mycurlpod中，访问Service暴露的ClusterIP，这时请求会负载均衡到后端任意一个Worker节点中。</p><pre><code class=\"language-plain\">$ curl -H \"content-type: application/json\" -d '{\"name\": \"john\"}' http://10.43.245.115:8080/greeter/hello\n</code></pre><h2>部署 Master Deployment</h2><p>Master节点的部署和Worker节点的部署非常类似。如下所示，创建crawler-master.yaml文件，描述Deployment资源的信息。这里和Worker Deployment不同的主要是相关的名字和程序启动的命令。</p><pre><code class=\"language-plain\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crawler-master-deployment\n  labels:\n    app: crawl-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crawl-master\n  template:\n    metadata:\n      labels:\n        app: crawl-master\n    spec:\n      containers:\n      - name: crawl-master\n        image: crawler:local\n        command:\n          - sh\n          - -c\n          - \"./crawler master --podip=${MY_POD_IP}\"\n        ports:\n        - containerPort: 8081\n        env:\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n</code></pre><h2>部署 Master Service</h2><p>同样地，为了能够用固定地IP访问Master，我们需要创建Master Service。新建crawl-master-service.yaml文件，如下所示，port指的是Service监听的端口80，这是默认的HTTP端口，而targetPort指的是转发到后端服务器的端口，也就是Master服务的HTTP端口8081。</p><pre><code class=\"language-plain\">kind: Service\napiVersion: v1\nmetadata:\n  name: crawl-master\n  labels:\n    app: crawl-master\nspec:\n  selector:\n    app: crawl-master\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8081\n    name: http\n  - port: 9091\n    name: grpc\n</code></pre><p>接下来，我们还是利用kubectl apply创建Master Service。</p><pre><code class=\"language-plain\">» kubectl  apply -f crawl-master-service.yaml\n</code></pre><p>现在我们就可以和Worker一样，利用Service访问Masrer暴露的接口了。</p><h2>创建 Ingress</h2><p>下面让我们更进一步，尝试在宿主机中访问集群的Master服务。由于资源具有隔离性，之前我们一直都是在集群内从一个Pod中去访问另一个Pod。现在我们希望在集群外部使用HTTP访问Master服务。要实现这个目标，可以使用Ingress资源。</p><p>具体做法是，创建ingress.yaml，在ingress.yaml文件中书写相关的HTTP路由规则，根据不同的域名和URL将请求路由到后端不同的Service中。</p><pre><code class=\"language-plain\">apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: crawler-ingress\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: crawl-master\n                port:\n                  number: 80\n</code></pre><p>spec.http.paths中描述了Ingress的路由规则，我们对应上面这个例子解读一下。</p><ul>\n<li>spec.http.paths.path 表示URL匹配的路径。</li>\n<li>spec.http.paths.pathType 表示URL匹配的类型为前缀匹配。</li>\n<li><a href=\"http://spec.http.paths.backend.service.name\">spec.http.paths.backend.service.name</a> 表示路由到后端的Service的名字。</li>\n<li>spec.http.paths.backend.service.port 表示路由到后端的Service的端口。</li>\n</ul><p>其实要使用Ingress的功能，光是定义ingress.yaml中的路由规则是不行的，我们还需要Ingress Controller控制器来实现这些路由规则的逻辑。但是和Kubernetes中内置的Controller不同，Ingress Controller 是可以灵活选择的，比较有名的Ingress Controller包括Nginx、Kong等。如果你想使用这些Ingress Controller ，需要单独安装。</p><p>好在，k3d默认为我们内置了<a href=\"https://github.com/containous/traefik\">traefik</a> Ingress Controller，这样我们就不需要再额外安装了。只要创建Ingress资源，就可以直接访问它了。</p><pre><code class=\"language-plain\">» kubectl  apply -f ingress.yaml\n</code></pre><p>下一步，我们在宿主机中访问集群。在这里我们访问的是8080端口，因为我在创建集群时指定了端口的映射，所以当前8080端口的请求会转发到集群的80端口中。然后根据Ingress指定的规则，请求会被转发到后端的Master Service当中。</p><pre><code class=\"language-plain\">» curl -H \"content-type: application/json\" -d '{\"id\":\"zjx\",\"name\": \"douban_book_list\"}' http://127.0.0.1:8080/crawler/resource\n</code></pre><p>此外，Ingress还可以设置规则，让不同的域名走不同的域名规则，这里就不再赘述了。</p><pre><code class=\"language-plain\">» curl -H \"content-type: application/json\" -d '{\"id\":\"zjx\",\"name\": \"douban_book_list\"}' http://zjx.vx1131052403.com:8080/crawler/resource\n</code></pre><h2>创建 ConfigMap</h2><p>到这一步，我们的配置文件都是打包在镜像中的，如果想要修改程序的启动参数会非常麻烦。因此，我们可以借助Kubernetes中的ConfigMap资源，将配置挂载到容器当中，这样我们就可以更灵活地修改配置文件，而不必每一次都打包新的镜像了。</p><p>具体做法如下。我们创建一个ConfigMap资源，把它放到默认的namespace中。在Data下，对应地输入文件名config.toml和文件内容。</p><pre><code class=\"language-plain\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: crawler-config\n  namespace: default\ndata:\n  config.toml: |-\n    logLevel = \"debug\"\n    \n    [fetcher]\n    timeout = 3000\n    proxy = [\"http://192.168.0.105:8888\", \"http://192.168.0.105:8888\"]\n    \n    \n    [storage]\n    sqlURL = \"root:123456@tcp(192.168.0.105:3326)/crawler?charset=utf8\"\n    \n    [GRPCServer]\n    HTTPListenAddress = \":8080\"\n    GRPCListenAddress = \":9090\"\n    ID = \"1\"\n    RegistryAddress = \"192.168.0.105:2379\"\n    RegisterTTL = 60\n    RegisterInterval = 15\n    ClientTimeOut   = 10\n    Name = \"go.micro.server.worker\"\n    \n    [MasterServer]\n    RegistryAddress = \"192.168.0.105:2379\"\n    RegisterTTL = 60\n    RegisterInterval = 15\n    ClientTimeOut   = 10\n    Name = \"go.micro.server.master\"\n</code></pre><p>然后，修改crawler-master.yaml文件，将ConfigMap挂载到容器中。</p><pre><code class=\"language-plain\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crawler-master-deployment\n  labels:\n    app: crawl-master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crawl-master\n  template:\n    metadata:\n      labels:\n        app: crawl-master\n    spec:\n      containers:\n      - name: crawl-master\n        image: crawler:local\n        command:\n          - sh\n          - -c\n          - \"./crawler master --podip=${MY_POD_IP}  --config=/app/config/config.toml\"\n        ports:\n        - containerPort: 8081\n        env:\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        volumeMounts:\n        - name: crawler-config\n          mountPath: /app/config/\n      volumes:\n      - name: crawler-config\n        configMap:\n          name: crawler-config\n</code></pre><p>在这个例子中，spec.template.spec.volumes创建了一个存储卷crawler-config，它的内容来自于名为crawler-config的ConfigMap。接着，spec.template.spec.containers.volumeMounts表示将该存储卷挂载到容器的/app/config目录下，这样程序就能够顺利使用ConfigMap中的配置文件了。</p><h2>总结</h2><p>这节课，我们学习了如何安装Kubernetes集群，然后使用k3d工具搭建起了轻量级的Kubernetes集群，从而在开发环境中模拟了多节点的Kubernetes集群。</p><p>我们还书写了核心的YAML描述文件，创建了Kubernetes中的资源，包括Deployment、Pod、Service、Ingress、ConfigMap等，将我们的爬虫项目部署到了Kubernetes中。</p><p>如果你是第一次接触这些 Kubernetes 描述文件，可能会觉得非常繁琐，但是熟悉之后就会变得简单了。创建了这些资源之后，剩下服务的扩容、维护就可以交给强大的Kubernetes来管理了。</p><p>（这节课中的 YAML 文件在<a href=\"https://github.com/dreamerjackson/crawler\">最新代码分支</a>的Kubernetes文件夹中。 ）</p><h2>课后题</h2><p>学完这节课，请你思考下面这个问题。</p><p>我们这节课书写的YAML文件中的资源对象，都是Kubernetes中定义好的资源类型。那么我们可不可以创建一个自定义的类型，自己去控制它的生命周期呢？</p><p>欢迎你在留言区留下自己思考的结果，也可以把这节课分享给对这个话题感兴趣的同事和朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"52 | 容器海洋中的舵手：Kubernetes工作机制","id":628250},"right":{"article_title":"特别放送｜Go泛型：用法、原理与最佳实践","id":624629}},"comments":[{"had_liked":false,"id":387190,"user_name":"Geek_2c2c44","can_delete":false,"product_type":"c1","uid":3808122,"ip_address":"浙江","ucode":"058A3078C656CF","user_header":"","comment_is_top":false,"comment_ctime":1706682912,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"这个集群高度依赖etcd， 但是这里部署的etcd本身似乎好像没有很强的可用性保障？","like_count":0},{"had_liked":false,"id":370383,"user_name":"北庭","can_delete":false,"product_type":"c1","uid":3207225,"ip_address":"江苏","ucode":"4E975D92E7E706","user_header":"https://static001.geekbang.org/account/avatar/00/30/f0/39/b9097d7d.jpg","comment_is_top":false,"comment_ctime":1678777158,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"老师，在使用kubectl get node命令后出现了这样的错误：couldn&#39;t get current server API group list: Get &quot;https:&#47;&#47;host.docker.internal:6550&#47;api?timeout=32s&quot;: dial tcp 10.0.0.35:6550: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\n这是什么原因呢","like_count":0,"discussions":[{"author":{"id":1615572,"avatar":"https://static001.geekbang.org/account/avatar/00/18/a6/d4/8d50d502.jpg","nickname":"简简单单","note":"","ucode":"5DD625D84ED7D3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619841,"discussion_content":"这是 Docker 客户端在尝试连接到 Docker 服务端时发生的错误。具体地说，它表示 Docker 客户端无法获得当前 Docker API 可用的组列表。该错误通常是由网络连接问题引起的，可能是 Docker 服务端无法响应或 Docker 客户端无法连接到 Docker 服务端。\n\n解决此问题的步骤如下：\n\n确保 Docker 服务正在运行，并尝试重启 Docker 服务以确保其正在正确运行。\n确保 Docker 客户端和 Docker 服务端在同一个网络上，并尝试使用不同的网络尝试连接。\n检查防火墙设置，确保 Docker 客户端可以连接到 Docker 服务端的端口，并启用了相应的端口。\n尝试更改 Docker 客户端的连接地址或端口。\n如果以上步骤均未解决问题，则可能需要检查 Docker 配置，以确保其配置正确。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685544188,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":368597,"user_name":"Geek_7873ee","can_delete":false,"product_type":"c1","uid":3506474,"ip_address":"四川","ucode":"4B90EAB891A29F","user_header":"","comment_is_top":false,"comment_ctime":1676467274,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"master 只部署了一个节点，那代码中对master做的高可用，就不是看不出来效果了嘛","like_count":0}]}