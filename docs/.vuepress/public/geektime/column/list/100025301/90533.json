{"id":90533,"title":"02 | MapReduce后谁主沉浮：怎样设计下一代数据处理技术？","content":"<p>你好，我是蔡元楠。</p><p>在上一讲中，我们介绍了2014年之前的大数据历史，也就是MapReduce作为数据处理的默认标准的时代。重点探讨了MapReduce面对日益复杂的业务逻辑时表现出的不足之处，那就是：1. 维护成本高；2. 时间性能不足。</p><p>同时，我们也提到了2008年诞生在Google西雅图研发中心的FlumeJava，它成为了Google内部的数据处理新宠。</p><p>那么，为什么是它扛起了继任MapReduce的大旗呢？</p><p>要知道，在包括Google在内的硅谷一线大厂，对于内部技术选择是非常严格的，一个能成为默认方案的技术至少满足以下条件：</p><ol>\n<li>\n<p>经受了众多产品线，超大规模数据量例如亿级用户的考验；</p>\n</li>\n<li>\n<p>自发地被众多内部开发者采用，简单易用而受开发者欢迎；</p>\n</li>\n<li>\n<p>能通过内部领域内专家的评审；</p>\n</li>\n<li>\n<p>比上一代技术仅仅提高10%是不够的，必须要有显著的比如70%的提高，才能够说服整个公司付出技术迁移的高昂代价。就看看从Python 2.7到Python 3的升级花了多少年了，就知道在大厂迁移技术是异常艰难的。</p>\n</li>\n</ol><p>今天这一讲，我不展开讲任何具体技术。</p><p>我想先和你一起设想一下，假如我和你站在2008年的春夏之交，在已经清楚了MapReduce的现有问题的情况下，我们会怎么设计下一代大规模数据处理技术，带领下一个十年的技术革新呢？</p><!-- [[[read_end]]] --><h3>我们需要一种技术抽象让多步骤数据处理变得易于维护</h3><p>上一讲中我提到过，维护协调多个步骤的数据处理在业务中非常常见。</p><p><img src=\"https://static001.geekbang.org/resource/image/44/c7/449ebd6c5950f5b7691d34d13a781ac7.jpg?wh=4075*2658\" alt=\"\"></p><p>像图片中这样复杂的数据处理在MapReduce中维护起来令人苦不堪言。</p><p>为了解决这个问题，作为架构师的我们或许可以用有向无环图（DAG）来抽象表达。因为有向图能为多个步骤的数据处理依赖关系，建立很好的模型。如果你对图论比较陌生的话，可能现在不知道我在说什么，你可以看下面一个例子，或者复习一下极客时间的《数据结构与算法之美》。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/83/26072f95c409381f3330b77d93150183.png?wh=2888*3133\" alt=\"\"></p><p>西红柿炒鸡蛋这样一个菜，就是一个有向无环图概念的典型案例。</p><p>比如看这里面番茄的处理，最后一步“炒”的步骤依赖于切好的番茄、打好的蛋、热好的油。而切好的番茄又依赖于洗好的番茄等等。如果用MapReduce来实现的话，在这个图里面，每一个箭头都会是一个独立的Map或Reduce。</p><p>为了协调那么多Map和Reduce，你又难以避免会去做很多检查，比如：番茄是不是洗好了，鸡蛋是不是打好了。</p><p>最后这个系统就不堪重负了。</p><p>但是，如果我们用有向图建模，图中的每一个节点都可以被抽象地表达成一种通用的<strong>数据集</strong>，每一条边都被表达成一种通用的<strong>数据变换</strong>。如此，你就可以用<strong>数据集</strong>和<strong>数据变换</strong>描述极为宏大复杂的数据处理流程，而不会迷失在依赖关系中无法自拔。</p><h3>我们不想要复杂的配置，需要能自动进行性能优化</h3><p>上一讲中提到，MapReduce的另一个问题是，配置太复杂了。以至于错误的配置最终导致数据处理任务效率低下。</p><p>这种问题怎么解决呢？很自然的思路就是，如果人容易犯错，就让人少做一点，让机器多做一点呗。</p><p>我们已经知道了，得益于上一步中我们已经用有向图对数据处理进行了高度抽象。这可能就能成为我们进行自动性能优化的一个突破口。</p><p>回到刚才的番茄炒鸡蛋例子，哪些情况我们需要自动优化呢？</p><p>设想一下，如果我们的数据处理食谱上又增加了番茄牛腩的需求，用户的数据处理有向图就变成了这个样子了。</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/a7/dc07e6cccdcc892bf6dff9a288e7f3a7.jpg?wh=2871*2542\" alt=\"\"></p><p>理想的情况下，我们的计算引擎要能够自动发现红框中的两条数据处理流程是重复的。它要能把两条数据处理过程进行合并。这样的话，番茄就不会被重复准备了。</p><p>同样的，如果需求突然不再需要番茄炒蛋了，只需要番茄牛腩，在数据流水线的预处理部分也应该把一些无关的数据操作优化掉，比如整个鸡蛋的处理过程就不应该在运行时出现。</p><p>另一种自动的优化是计算资源的自动弹性分配。</p><p>比如，还是在番茄炒蛋这样一个数据处理流水线中，如果你的规模上来了，今天需要生产1吨的番茄炒蛋，明天需要生产10吨的番茄炒蛋。你发现有时候是处理1000个番茄，有时候又是10000个番茄。如果手动地去做资源配置的话，你再也配置不过来了。</p><p>我们的优化系统也要有可以处理这种问题的弹性的劳动力分配机制。它要能自动分配，比如100台机器处理1000个番茄，如果是10000个番茄那就分配1000台机器，但是只给热油1台机器可能就够了。</p><p>这里的比喻其实是很粗糙也不精准的。我想用这样两个例子表达的观点是，在数据处理开始前，我们需要有一个自动优化的步骤和能力，而不是按部就班地就把每一个步骤就直接扔给机器去执行了。</p><h3>我们要能把数据处理的描述语言，与背后的运行引擎解耦合开来</h3><p>前面两个设计思路提到了很重要的一个设计就是有向图。</p><p>用有向图进行数据处理描述的话，实际上<strong>数据处理描述语言</strong>部分完全可以和后面的<strong>运算引擎</strong>分离了。有向图可以作为<strong>数据处理描述语言</strong>和<strong>运算引擎</strong>的前后端分离协议。</p><p>举两个你熟悉的例子可能能更好理解我这里所说的前后端分离（client-server design）是什么意思：</p><p>比如一个网站的架构中，服务器和网页通过HTTP协议通信。</p><p><img src=\"https://static001.geekbang.org/resource/image/22/b4/22c92b5a9dd6e4d9fc07a8ac61fff2b4.png?wh=2550*758\" alt=\"\"></p><p>比如在TensorFlow的设计中，客户端可以用任何语言（比如Python或者C++）描述计算图，运行时引擎（runtime) 理论上却可以在任何地方具体运行，比如在本地，在CPU，或者在TPU。</p><p><img src=\"https://static001.geekbang.org/resource/image/f9/06/f9e2bb76a168469f572c91d0c5a0bf06.png?wh=2525*675\" alt=\"\"></p><p>那么我们设计的数据处理技术也是一样的，除了有向图表达需要<strong>数据处理描述语言</strong>和<strong>运算引擎</strong>协商一致，其他的实现都是灵活可拓展的。</p><p>比如，我的数据描述可以用Python描述，由业务团队使用；计算引擎用C++实现，可以由数据底层架构团队维护并且高度优化；或者我的数据描述在本地写，计算引擎在云端执行。</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/b8/d77857341e194bae59ce099e7d68c9b8.png?wh=2518*616\" alt=\"\"></p><h3>我们要统一批处理和流处理的编程模型</h3><p>关于什么是批处理和流处理概念会在后面的章节展开。这里先简单解释下，批处理处理的是有界离散的数据，比如处理一个文本文件；流处理处理的是无界连续的数据，比如每时每刻的支付宝交易数据。</p><p>MapReduce的一个局限是它为了批处理而设计的，应对流处理的时候不再那么得心应手。即使后面的Apache Storm、Apache Flink也都有类似的问题，比如Flink里的批处理数据结构用DataSet，但是流处理用DataStream。</p><p>但是真正的业务系统，批处理和流处理是常常混合共生，或者频繁变换的。</p><p>比如，你有A、B两个数据提供商。其中数据提供商A与你签订的是一次性的数据协议，一次性给你一大波数据，你可以用批处理。而数据提供商B是实时地给你数据，你又得用流处理。更可怕的事情发生了，本来是批处理的数据提供商A，突然把协议修改了，现在他们实时更新数据。这时候你要是用Flink就得爆炸了。业务需求天天改，还让不让人活了？！</p><p>因此，我们设计的数据处理框架里，就得有更高层级的数据抽象。</p><p>不论是批处理还是流处理的，都用统一的数据结构表示。编程的API也需要统一。这样不论业务需求什么样，开发者只需要学习一套API。即使业务需求改变，开发者也不需要频繁修改代码。</p><h3>我们要在架构层面提供异常处理和数据监控的能力</h3><p>真正写过大规模数据处理系统的人都深有感触：在一个复杂的数据处理系统中，难的不是开发系统，而是异常处理。</p><p>事实正是如此。一个Google内部调研表明，在大规模的数据处理系统中，90%的时间都花在了异常处理中。常常发生的问题的是，比如在之前的番茄炒鸡蛋处理问题中，你看着系统log，明明买了1000个鸡蛋，炒出来的菜却看起来只有999个鸡蛋，你仰天长叹，少了一个蛋到底去哪里了！</p><p>这一点和普通的软件开发不同。比如，服务器开发中，偶尔一个RPC请求丢了就丢了，重试一下，重启一下能过就行了。可如果在数据处理系统中，数据就是钱啊，不能随便丢。比如我们的鸡蛋，都是真金白银买回来的。是超市买回来数错了？是打蛋时候打碎了？还是被谁偷吃了？你总得给老板一个合理的交代。</p><p>我们要设计一套基本的数据监控能力，对于数据处理的每一步提供自动的监控平台，比如一个监控网站。</p><p>在番茄炒蛋系统中，要能够自动的记录下来，超市买回来是多少个蛋，打蛋前是多少个蛋，打完蛋是多少个蛋，放进锅里前是多少个蛋等等。也需要把每一步的相关信息进行存储，比如是谁去买的蛋，哪些人打蛋。这样出错后可以帮助用户快速找到可能出错的环节。</p><h2>小结</h2><p>通过上面的分析，我们可以总结一下。如果是我们站在2008年春夏之交来设计下一代大规模数据处理框架，一个基本的模型会是图中这样子的：</p><p><img src=\"https://static001.geekbang.org/resource/image/53/2e/53aa1aad08b11e6c2db5cf8bb584572e.png?wh=4909*3085\" alt=\"\"></p><p>但是这样粗糙的设计和思想实验离实现还是太远。你可能还是会感到无从下手。</p><p>后面的章节会给你补充一些设计和使用大规模数据处理架构的基础知识。同时，也会深入剖析两个与我们这里的设计理念最接近的大数据处理框架，Apache Spark和Apache Beam。</p><h2>思考题</h2><p>你现在在使用的数据处理技术有什么问题，你有怎样的改进设计？</p><p>欢迎你把自己的想法写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"01 | 为什么MapReduce会被硅谷一线公司淘汰？","id":90081},"right":{"article_title":"03 | 大规模数据处理初体验：怎样实现大型电商热销榜？","id":91125}},"comments":[{"had_liked":false,"id":87570,"user_name":"mjl","can_delete":false,"product_type":"c1","uid":1044391,"ip_address":"","ucode":"B84138FF626850","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ef/a7/6e8f3636.jpg","comment_is_top":true,"comment_ctime":1555636353,"is_pvip":false,"replies":[{"id":"31487","content":"谢谢你的留言！感觉活捉了一只技术大牛呢。<br><br>是的，Spark的话虽然原生Spark Streaming Model和Dataflow Model不一样，但是Cloudera Labs也有根据Dataflow Model的原理实现了Spark Dataflow使得Beam可以跑Spark runner。<br><br>而对于Flink来说的话，在0.10版本以后它的DataStream API就已经是根据Dataflow Model的思想来重写了。现在Flink也支持两套API，分别是DataStream版本的和Beam版本的。其实data Artisans一直都有和Google保持交流，希望未来两套Beam和Flink的API能达到统一。<br><br>最后赞一点批处理是流处理的子集，这个观点我在第一讲留言也提到过。<br><br>如果觉得有收获，欢迎分享给朋友！同时也欢迎你继续留言交流，一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555638543,"ip_address":"","comment_id":87570,"utype":1}],"discussion_count":2,"race_medal":0,"score":"9.2233722832235008e+18","product_id":100025301,"comment_content":"Unify platform和批流统一已经是主要趋势了，而我个人目前只对spark、flink有一定的了解。对于spark来说，无疑是很优秀的一个引擎，包括它的all in one的组件栈，structured streaming出来后的批流api的统一，目前在做的continues Mode。而flink，的确因为阿里的运营，在国内火了。但也展现了它的独有优势，更加贴近dataflow model的思想。同时，基于社区以及阿里、华为小伙伴的努力，flink的table&#47;sql 的api也得到的很大的增强，提供了流批统一的api。虽然底层然后需要分化为dataset和datastream以及runtime层的batchTask和StreamTask，但是现在也在rethink the stack，这个point在2019 SF 的大会也几乎吸引了所有人。但就现状而言，flink的确有着理念上的优势（流是批的超集），同时也有迅猛上升的趋势。","like_count":58,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447511,"discussion_content":"谢谢你的留言！感觉活捉了一只技术大牛呢。\n\n是的，Spark的话虽然原生Spark Streaming Model和Dataflow Model不一样，但是Cloudera Labs也有根据Dataflow Model的原理实现了Spark Dataflow使得Beam可以跑Spark runner。\n\n而对于Flink来说的话，在0.10版本以后它的DataStream API就已经是根据Dataflow Model的思想来重写了。现在Flink也支持两套API，分别是DataStream版本的和Beam版本的。其实data Artisans一直都有和Google保持交流，希望未来两套Beam和Flink的API能达到统一。\n\n最后赞一点批处理是流处理的子集，这个观点我在第一讲留言也提到过。\n\n如果觉得有收获，欢迎分享给朋友！同时也欢迎你继续留言交流，一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555638543,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":1,"child_discussions":[{"author":{"id":1368189,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJyos9KR4uD08N54vPmfOq6E63QGvZ9gVxTe7Vo94x0iclq05YQf1auJEG17zI6hORCn8plwdzxS1Q/132","nickname":"FooBee","note":"","ucode":"88C0ECA229858D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":549589,"discussion_content":"请问现在统一了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644117222,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":447511,"ip_address":""},"score":549589,"extra":""}]}]},{"had_liked":false,"id":87485,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1555609117,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"70275085853","product_id":100025301,"comment_content":"1. DAG是将一系列的转换函数连接起来，最后调用真正的计算函数<br>2. 番茄炒鸡蛋，炒牛腩这个例子，说明不要进行重复计算，要复用之前的计算。而且在计算之前过滤掉不必要的数据，最大限度的减少计算量。<br><br>现在使用的数据处理技术遇到的问题就是在两个大表进行关联的时候，没有太多的优化手段，自己能想到的就是增加计算资源（但是条件不允许），然后过滤掉两个表中不必要的数据，但其实优化之后还是会很慢。<br><br>希望老师能分享一下大表和大表join的优化手段！","like_count":17},{"had_liked":false,"id":87675,"user_name":"段斌","can_delete":false,"product_type":"c1","uid":1173578,"ip_address":"","ucode":"B5A85C6AF139B4","user_header":"https://static001.geekbang.org/account/avatar/00/11/e8/4a/c2a539a0.jpg","comment_is_top":false,"comment_ctime":1555657865,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"44505330825","product_id":100025301,"comment_content":"Q：你现在在使用的数据处理技术有什么问题，你有怎样的改进设计？<br><br>A：简单介绍下我自己的背景，以前有RDBMS时期的数据仓库经验，后来没有跟上Hadoop发展的节奏，逐渐转向了前台业务部门。现在在一家sdk行为数据采集与分析厂商做解决方案，但是公司有很多技术栈：storm+es，flink，spark，还有最新的kudu+carbandata，学习起来非常吃力。<br><br>基于本期的课程，我有几个问题：<br>1. storm+es实时大数据平台的出现是解决什么问题？优势是什么，未来是否会被其他技术栈取代？<br>2. 我们客户在用这个技术栈的产品，反馈查错成本很高，不知道在sdk采集时候数据没有收上来，还是在collector，或者是后面丢失。我想问数据监控是大数据普遍问题吗？业内是怎么解决的？<br>3. 咱们是否会介绍Apache Carbondata，这个技术栈的优势是什么？你怎么看待它的发展？","like_count":10},{"had_liked":false,"id":87533,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1555633370,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31620404442","product_id":100025301,"comment_content":"Flink在国内也是刚刚兴起火热起来，而Apache Beam也就是Google内部FlumeJava的开源版本早早就被设计出来。国内现在讲实时数据处理，批流统一还是比较推崇Flink和阿里的自主设计的Blink系统，接受Beam可能还需要几年的时间","like_count":7},{"had_liked":false,"id":87826,"user_name":"fy","can_delete":false,"product_type":"c1","uid":1152186,"ip_address":"","ucode":"EDB661C3A05910","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/jsMMDDzhbsTzhicsGZiaeV0PWSnAS0fBlb1r6CsuB32vr3hRwV9UubmfHQx45v7jtaXajPlQ8kQ17b3zpQzHmqVw/132","comment_is_top":false,"comment_ctime":1555725104,"is_pvip":false,"replies":[{"id":"31611","content":"谢谢肯定。期待你后面的留言。<br><br>也欢迎推荐给朋友！","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555739445,"ip_address":"","comment_id":87826,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27325528880","product_id":100025301,"comment_content":"虽然看的不是很懂，毕竟没搞过这方面的，但是每篇文章的知识点逻辑很清楚，学习中，这里尊称一句蔡老师，被老师的回复学者的留言给感动了。期待老师后面精彩的专栏","like_count":6,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447614,"discussion_content":"谢谢肯定。期待你后面的留言。\n\n也欢迎推荐给朋友！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555739445,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87779,"user_name":"Milittle","can_delete":false,"product_type":"c1","uid":1045455,"ip_address":"","ucode":"80E566639A8ABB","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f3/cf/851dab01.jpg","comment_is_top":false,"comment_ctime":1555689617,"is_pvip":true,"replies":[{"id":"31614","content":"你理解的很多！期待你后面的分享","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555739621,"ip_address":"","comment_id":87779,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27325493393","product_id":100025301,"comment_content":"讲真，这个思路太清晰了。讲到有向图那里，还没看到tf那段，我脑子里第一蹦出来的就是深度学习里面的符号式计算图，可以在图编译的时候进行优化。然后在计算时可以加速运算，还有就是，这里的计算图用拓扑排序结合优先级队列来执行计算。<br>我还是个学生，有不对的地方请指教，没在真实场景中体验过，但是感觉这些都是通用的，学会了，处处可以用到。期待后续课程给更多的启发，谢谢(*°∀°)=3","like_count":6,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447598,"discussion_content":"你理解的很多！期待你后面的分享","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555739621,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87504,"user_name":"大王叫我来巡山","can_delete":false,"product_type":"c1","uid":1099513,"ip_address":"","ucode":"1B8D0C701BC95E","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/f9/caf27bd3.jpg","comment_is_top":false,"comment_ctime":1555629239,"is_pvip":false,"replies":[{"id":"31475","content":"你说的对，WordCount就像helloworld，真实业务场景肯定会复杂很多。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555634414,"ip_address":"","comment_id":87504,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23030465719","product_id":100025301,"comment_content":"经历了mapreduce到spark, 从最具欺骗性的wordcount开始到发现很多业务本身并不适合mapreduce编程模型，一直做日志处理，现在在政府某部门同时处理批处理任务和流处理任务，老师的课太及时了，感觉对我们的业务模型革新会产生很大的影响，前两篇没有涉及技术，已经感觉醍醐灌顶了。","like_count":6,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447490,"discussion_content":"你说的对，WordCount就像helloworld，真实业务场景肯定会复杂很多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555634414,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87496,"user_name":"文洲","can_delete":false,"product_type":"c1","uid":1298996,"ip_address":"","ucode":"E2CD38F88A53CB","user_header":"https://static001.geekbang.org/account/avatar/00/13/d2/34/39a3edef.jpg","comment_is_top":false,"comment_ctime":1555627655,"is_pvip":false,"replies":[{"id":"31472","content":"并没有说flink比不上spark。。。这篇没有展开比较。只是带了一句话flink的流处理批处理api不一样。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555634299,"ip_address":"","comment_id":87496,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23030464135","product_id":100025301,"comment_content":"老师讲的很明白，有个疑问，现在都说flink是下一代实时流处理系统，这里按老师的对比来看，它还比不上spark，可否理解为flink主要还是专注实时流处理而spark有兼具批处理和流处理的优势","like_count":5,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447485,"discussion_content":"并没有说flink比不上spark。。。这篇没有展开比较。只是带了一句话flink的流处理批处理api不一样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555634299,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87544,"user_name":"wmg","can_delete":false,"product_type":"c1","uid":1070036,"ip_address":"","ucode":"BA4CED171B59E9","user_header":"https://static001.geekbang.org/account/avatar/00/10/53/d4/2ed767ea.jpg","comment_is_top":false,"comment_ctime":1555634032,"is_pvip":false,"replies":[{"id":"31479","content":"数据的索引查询和这里的数据处理是两个话题。关于怎么提高数据系统的查询效率，我在考虑要不要开一个存储系统高性能索引专栏。<br><br>我不知道你的查询有多复杂，简单处理的话，可以先试试建一些常见查询路径的索引表。写的时候往两张表写或者再做一些专门的建索引的数据处理系统。<br><br>","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555635575,"ip_address":"","comment_id":87544,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735503216","product_id":100025301,"comment_content":"现在使用较多的是hive和sparkSql，这两种技术多使用的都是类似关系数据库的数据模型，对于一些复杂的对象必须要通过建立多张表来存储，查询时可能需要多张表进行join，由于担心join的性能损耗，一般又会设计成大宽表，但这样又会浪费存储空间，数据一致性也得不到约束。想问一下老师，有没有支持类似mongodb这样的文档型数据模型的大数据处理技术？","like_count":4,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447500,"discussion_content":"数据的索引查询和这里的数据处理是两个话题。关于怎么提高数据系统的查询效率，我在考虑要不要开一个存储系统高性能索引专栏。\n\n我不知道你的查询有多复杂，简单处理的话，可以先试试建一些常见查询路径的索引表。写的时候往两张表写或者再做一些专门的建索引的数据处理系统。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555635575,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87972,"user_name":"monkeyking","can_delete":false,"product_type":"c1","uid":1196221,"ip_address":"","ucode":"4AF7ECF79C4379","user_header":"https://static001.geekbang.org/account/avatar/00/12/40/bd/acb9d02a.jpg","comment_is_top":false,"comment_ctime":1555774368,"is_pvip":false,"replies":[{"id":"31653","content":"谢谢你的提问！在留言区里的Dataflow指的是Google在2015发表的Dataflow Model数据处理模型。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555809474,"ip_address":"","comment_id":87972,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440676256","product_id":100025301,"comment_content":"老师，啥叫dataflow？从字面上来看好像和dag差不多，不知道我理解的是否正确，还请老师指正","like_count":3,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447669,"discussion_content":"谢谢你的提问！在留言区里的Dataflow指的是Google在2015发表的Dataflow Model数据处理模型。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555809474,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87875,"user_name":"来碗绿豆汤","can_delete":false,"product_type":"c1","uid":1070051,"ip_address":"","ucode":"B0AB63B8D9729F","user_header":"https://static001.geekbang.org/account/avatar/00/10/53/e3/39dcfb11.jpg","comment_is_top":false,"comment_ctime":1555738889,"is_pvip":false,"replies":[{"id":"31627","content":"这个是很典型的问题。后面学到了一些框架之后可以很方便用groupBy进行处理。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555772960,"ip_address":"","comment_id":87875,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440640777","product_id":100025301,"comment_content":"蔡老师好，看到您那么认真的回复读者问题，果断决定买了，跟着您一起学习。我们最近在打算重构一个项目，想听听您的建议。系统的输入是将近1TB的数据文件（好多个文件，不是一个大文件），内容就是一条条记录，描述了某个实体的某个属性。我们要做的就是把属于同一个实体的属性整合到一起，然后将数据按实体id排序输出，没100万个实体写一个文件。我现在的思路就是:第一步，把文件重新按照实体id切块，然后排序写成小文件，这一步可以是一个程序；第二步，对id属于某个范围内的（如1-100w）文件，归并排序，这一步一个程序；第三步，对排好序的文件按指定格式输出。因为以前没接触过大数据相关技术，所以现在是用java实现的demo版本，就像你课程里面所示，担心将来各个进程，线程之间，交互通信，异常处理，log处理，等一系列问题都需要自己维护会比较麻烦，想听听您的建议。","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447632,"discussion_content":"这个是很典型的问题。后面学到了一些框架之后可以很方便用groupBy进行处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555772960,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87625,"user_name":"Codelife","can_delete":false,"product_type":"c1","uid":1041421,"ip_address":"","ucode":"10458683978083","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLhMtBwGqqmyhxp5uaDTvvp18iaalQj8qHv6u8rv1FQXGozfl3alPvdPHpEsTWwFPFVOoP6EeKT4bw/132","comment_is_top":false,"comment_ctime":1555644007,"is_pvip":false,"replies":[{"id":"31511","content":"谢谢你的留言！我在第十讲中所讲解到的Lambda Architecture应该会对你们的架构设计有所帮助。希望能在那时继续看到你的留言。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555646997,"ip_address":"","comment_id":87625,"utype":1}],"discussion_count":2,"race_medal":0,"score":"14440545895","product_id":100025301,"comment_content":"我们是做车联网业务的，目前实时处理采用的storm，批处理采用的是MR,和您的文章中描述的一样，业务场景和算法中经常出现实时和批处理共存的情况，为了保证实时性，通常是定时执行MR任务计算出中间结果，再由storm任务调用，这样的坏处是MR任务并不及时，而且维护起来很麻烦，效果并不理想。希望能够从apache beam中学到东西","like_count":3,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447536,"discussion_content":"谢谢你的留言！我在第十讲中所讲解到的Lambda Architecture应该会对你们的架构设计有所帮助。希望能在那时继续看到你的留言。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555646997,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1022102,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/98/96/a2f4554e.jpg","nickname":"Jay Wu","note":"","ucode":"39C9EDDC71BADE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":3237,"discussion_content":"我们也是做车联网的，可以加一下wx以后可以交流一下吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564325519,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87985,"user_name":"小辉辉","can_delete":false,"product_type":"c1","uid":1189661,"ip_address":"","ucode":"9FF25E25C85350","user_header":"https://static001.geekbang.org/account/avatar/00/12/27/1d/1cb36854.jpg","comment_is_top":false,"comment_ctime":1555779640,"is_pvip":false,"replies":[{"id":"31655","content":"谢谢你的肯定！同时也欢迎你把专栏分享给朋友。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555809567,"ip_address":"","comment_id":87985,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145714232","product_id":100025301,"comment_content":"看完前三讲，让我对大数据又有一个更新的认识","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447674,"discussion_content":"谢谢你的肯定！同时也欢迎你把专栏分享给朋友。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555809567,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87950,"user_name":"电光火石","can_delete":false,"product_type":"c1","uid":1013160,"ip_address":"","ucode":"3AD33BB4AA940F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/75/a8/dfe4cade.jpg","comment_is_top":false,"comment_ctime":1555764949,"is_pvip":false,"replies":[{"id":"31628","content":"专栏里提到了这种应用在google有1000个，所以很难概括说这类应用google怎么处理的。你的方法既然能解决问题就好了。<br><br>关于流处理批处理后面的章节还会深入讨论。<br><br>","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555773299,"ip_address":"","comment_id":87950,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145699541","product_id":100025301,"comment_content":"我们做风控的，现在每天收的数据比较多，现在都是先通过spark streaming落地hive，然后每天批量跑任务做模型训练和分析，因为现在模型训练还是集中在离线训练，在线训练约束比较大，所以使用场景还比较小，训练处理的模型在通过导出到pmml在线预测，整个作为一个闭环。不知道google在这方面试怎么处理的？谢谢！<br><br>另外，老师有个理念很新颖（从处理的数据范围来看批处理和实时计算，批处理的数据是有界的，实时计算的数据是无界的），我一直从处理的间隔来看批处理和实时计算，觉得实时计算是批处理的一种，就是不断的把离线任务处理频率做个极限，就变成了实时计算。","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447662,"discussion_content":"专栏里提到了这种应用在google有1000个，所以很难概括说这类应用google怎么处理的。你的方法既然能解决问题就好了。\n\n关于流处理批处理后面的章节还会深入讨论。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555773299,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87801,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1555718281,"is_pvip":false,"replies":[{"id":"31612","content":"似乎和本篇内容并不相关。general的技术咨询我看看能不能让平台拉群讨论","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555739583,"ip_address":"","comment_id":87801,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10145652873","product_id":100025301,"comment_content":"老师我补充下我的三个问题:<br>1.这种两个流都是大数据量的一般业界用什么样计算模型去做匹配，目前我们是用spark写的匹配？<br>2.由于一个流（假设名字为A流）的数据时间跨度比较大，比如几天前的数据，这就要求另外一个流（假设为B流）必须有缓存，而从缓存中抽取B流数据时候，会根据时间和Ip的hash原一批数据和A流匹配，那这个缓存用什么比较好那？我们原来用Hbase后发现数据量大scan比较慢，改hdfs直接存储了，勉强可以用。<br>3.我们最终结果数据一天也有几百亿条，目前存Es里面的，业务要求查询性能分钟级别就可以接受，我们查询性能够了，但是索引数据存储空间占用大，入索引性能一般，我们想换个高压缩性能分钟级的存储系统，目前考虑用列式存储➕ SQL引擎来做，这种没经验是否合适？<br><br>谢谢老师耐心看完，有空指点下谢谢","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447607,"discussion_content":"似乎和本篇内容并不相关。general的技术咨询我看看能不能让平台拉群讨论","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555739583,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1068361,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","nickname":"明翼","note":"","ucode":"E77F86BEB3D5C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":130698,"discussion_content":"没有\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1578787344,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1032932,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c2/e4/ad418d90.jpg","nickname":"风","note":"","ucode":"2DEDB586E625C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":128604,"discussion_content":"想问下现在有技术群了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1578654169,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87557,"user_name":"王二","can_delete":false,"product_type":"c1","uid":1061909,"ip_address":"","ucode":"12615595909BD9","user_header":"https://static001.geekbang.org/account/avatar/00/10/34/15/53201a55.jpg","comment_is_top":false,"comment_ctime":1555635401,"is_pvip":false,"replies":[{"id":"31497","content":"都是很好的问题<br><br>1 SQL 就是一个统一的用户界面，后面的引擎可以是hive也可以是自己实现的别的<br>2 看到spark部分就知道了 :)<br>3 的确，监控部分我可以看看是否在后面的部分增加章节","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555643420,"ip_address":"","comment_id":87557,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145569993","product_id":100025301,"comment_content":"老师您好：<br>1.如您所说flink在批流处理上的不统一，目前社区各个厂商也在努力的实现这种基于SQL或其他引擎的一些统一，这块在您看来有什么好的建议。<br>2.后面文章使用spark举例，这里是用他的struct streaming还是spark streaming呢.<br>3.感同身受，流系统难的不在开发，在于监控，如何处理背压，如何根据负载动态调整资源，除过开源框架自带的一些监控外，后续在任务流监控这块是否有什么好的建议推荐。","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447503,"discussion_content":"都是很好的问题\n\n1 SQL 就是一个统一的用户界面，后面的引擎可以是hive也可以是自己实现的别的\n2 看到spark部分就知道了 :)\n3 的确，监控部分我可以看看是否在后面的部分增加章节","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555643420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87538,"user_name":"dylan","can_delete":false,"product_type":"c1","uid":1181340,"ip_address":"","ucode":"B6A9BD80B1BB4B","user_header":"https://static001.geekbang.org/account/avatar/00/12/06/9c/1ee94a9d.jpg","comment_is_top":false,"comment_ctime":1555633687,"is_pvip":false,"replies":[{"id":"31481","content":"容错性是很重要的问题，两个的容错性设计方案是不同的。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555635793,"ip_address":"","comment_id":87538,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145568279","product_id":100025301,"comment_content":"有没有这么一个问题,mapreduce在大规模数据处理时，由于每个计算阶段都会落盘，所以计算比较慢，但是数据完整，不会丢失；spark基于内存计算，会有数据丢失的风险？","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447499,"discussion_content":"容错性是很重要的问题，两个的容错性设计方案是不同的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555635793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312177,"user_name":"高景洋","can_delete":false,"product_type":"c1","uid":2717072,"ip_address":"","ucode":"532188513579E4","user_header":"","comment_is_top":false,"comment_ctime":1631673073,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5926640369","product_id":100025301,"comment_content":"问题：<br>1、数据处理流程太长<br>2、处理过程中采用分布式、高并发的方式<br>3、环节太多，需要把每个环节的产生的日志数据，都推kafka<br>4、用 elk 的方式，进行数据查询及问题查找<br><br>------痛点-------<br><br>5、在数据处理过程中，会产生大量的大文本内容，比如网页的html，内容大且多，即使使用消息压缩后推kafka的方式，也会将kafka的生产流量打爆<br>6、为了应对5的情况，我们不得不，有选择性的开一些任务的kafka推送。<br>   而未开kafka推送的那部分任务，数据产生异常时，又很难及时的对异常现场的数据内容作分析<br><br>------处理方式-----<br>7、为了降低kafka的生产压力，我们将消息大的数据，做了只有在数据处理过程中标记失败的数据才推kafka，这样既降低了kafka生产压力，又可以保证有异常时可以及时查看到异常内容<br><br>------其他------<br>8、数据处理流程中，产生的环节日志数据，是否有其他更高效的收集及展示方式？","like_count":2},{"had_liked":false,"id":87877,"user_name":"退而结网","can_delete":false,"product_type":"c1","uid":1126394,"ip_address":"","ucode":"48C99264CC3A5C","user_header":"https://static001.geekbang.org/account/avatar/00/11/2f/fa/191049df.jpg","comment_is_top":false,"comment_ctime":1555739973,"is_pvip":false,"replies":[{"id":"31626","content":"谢谢，欢迎把专栏分享给朋友","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555772760,"ip_address":"","comment_id":87877,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850707269","product_id":100025301,"comment_content":"在极客时间订阅了几个专栏了，有些专栏留言的问题很少得到专栏老师的回复，看了下蔡老师的留言回复，基本上都是有问必答。这两节专栏的内容真的很多干货，同学们的留言也给了我很多启发。作为大数据处理的半入门汉，希望能在这门课程中得到更多的收获，祝愿和大家共同进步！","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447634,"discussion_content":"谢谢，欢迎把专栏分享给朋友","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555772760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87783,"user_name":":)","can_delete":false,"product_type":"c1","uid":1239198,"ip_address":"","ucode":"23D505949442B6","user_header":"https://static001.geekbang.org/account/avatar/00/12/e8/9e/6550a051.jpg","comment_is_top":false,"comment_ctime":1555691831,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5850659127","product_id":100025301,"comment_content":"期待更新！","like_count":1},{"had_liked":false,"id":87780,"user_name":"咕噜男爵-Tony","can_delete":false,"product_type":"c1","uid":1219258,"ip_address":"","ucode":"8C841851AC767F","user_header":"https://static001.geekbang.org/account/avatar/00/12/9a/ba/f5eb2a66.jpg","comment_is_top":false,"comment_ctime":1555690174,"is_pvip":false,"replies":[{"id":"31613","content":"谢谢！","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555739593,"ip_address":"","comment_id":87780,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850657470","product_id":100025301,"comment_content":"喜欢上了这门课程，期待对Spark的深入剖析 ^_^!","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447599,"discussion_content":"谢谢！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555739593,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87584,"user_name":"Edwin","can_delete":false,"product_type":"c1","uid":1105199,"ip_address":"","ucode":"04DC152E82B835","user_header":"https://static001.geekbang.org/account/avatar/00/10/dd/2f/7f0d19a8.jpg","comment_is_top":false,"comment_ctime":1555638258,"is_pvip":false,"replies":[{"id":"31493","content":"谢谢你的留言！很好的总结！是的，框架一旦和内部系统Coupled了在一起之后想要开源会遇到很多困难。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555639810,"ip_address":"","comment_id":87584,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850605554","product_id":100025301,"comment_content":"Spark 和 Flink 都是通用的开源大规模处理引擎，目标是在一个系统中支持所有的数据处理以带来效能的提升。两者都有相对比较成熟的生态系统。是下一代大数据引擎最有力的竞争者。Spark 的生态总体更完善一些，在机器学习的集成和易用性上暂时领先。Flink 在流计算上有明显优势，核心架构和模型也更透彻和灵活一些。<br>社区版本的flink有些问题还待进一步完善，比如统一API层、状态管理机制、资源调度等，阿里内部的blink虽然对flink做了大量改造，但依赖内部环境能真正返璞社区还有很长一段路要走～","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447519,"discussion_content":"谢谢你的留言！很好的总结！是的，框架一旦和内部系统Coupled了在一起之后想要开源会遇到很多困难。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555639810,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87562,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1555635709,"is_pvip":false,"replies":[{"id":"31488","content":"谢谢你的提问！如果是不确定数据流入时间的话需要采用流处理框架去处理了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555638615,"ip_address":"","comment_id":87562,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5850603005","product_id":100025301,"comment_content":"请问不定时的来一批大量数据要处理是要用批框架还是流框架呢？","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447507,"discussion_content":"谢谢你的提问！如果是不确定数据流入时间的话需要采用流处理框架去处理了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555638615,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1612924,"avatar":"https://static001.geekbang.org/account/avatar/00/18/9c/7c/fdb85fde.jpg","nickname":"xuexiqiu","note":"","ucode":"D00BD255E0E0D3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":3901,"discussion_content":"我的看法是“不确定流入时间，但是每次流入是一个很大的batch的话”，需要批处理+一个scheduler框架。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564964389,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87503,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1555629234,"is_pvip":false,"replies":[{"id":"31478","content":"我觉得你总结的很好。<br><br>问题我似乎没有很明白，难点是在时间范围大还是缓存？","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555635261,"ip_address":"","comment_id":87503,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850596530","product_id":100025301,"comment_content":"课堂笔记：mr有三个重要问题，一是支持的计算粒度太大，描述麻烦；二是性能差，调优复杂；三是只支持批量处理不支持流处理。针对此设计了有向无环图，好处是描述简单，易在上面进行自动优化；描述和计算引擎分开了，解耦合，可以分别实现进一步提升性能；我一直觉得批量处理和流处理完全两回事，老师的视角是将两者融合，想了下，批量处理可以看做时间间隔很长的流，而流按照spark等时间窗口概念可以看做小范围的批量，这个设计可以这样想，设计起来可能很复杂。另外一个由于可以表达更复杂的事件转换，那好的监控异常处理就是必须的了，以上为总结。<br><br>关于遇到问题，我们曾经遇到过两个都是很大的流类的日志数据匹配处理问题，是TB级别，而且数据流不同步的一个流的数据跨度范围大，如何设计高效处理是个难题，在hbase缓存还是在hdfs缓存是个难题，不知道老师有何高见？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447489,"discussion_content":"我觉得你总结的很好。\n\n问题我似乎没有很明白，难点是在时间范围大还是缓存？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555635261,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87492,"user_name":"Liu C.","can_delete":false,"product_type":"c1","uid":1504607,"ip_address":"","ucode":"29F3988DD8128A","user_header":"https://static001.geekbang.org/account/avatar/00/16/f5/5f/217c6a14.jpg","comment_is_top":false,"comment_ctime":1555624649,"is_pvip":false,"replies":[{"id":"31482","content":"谢谢你的留言！我相信第10讲中的Lambda数据处理架构会对你有所帮助，期待那时候再次看到你的留言。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555635979,"ip_address":"","comment_id":87492,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850591945","product_id":100025301,"comment_content":"目前还在学校科研的现个丑：目前我所做的数据任务是下载科研论文和报告并对其中的文字数据进行分析，用的是自己写的工具，分为两部分：第一部分从网上下载所需数据，第二部分进行内容提取和分析。<br><br>目前这是个批处理式的系统：先运行下载工具之后再运行分析。这会导致没法及时更新最新出现的数据。解决方法是，加入一个定时查看的模块，在发现新添加数据时进行下载并处理update分析结果。<br><br>另一个缺点是，我没有log那些数据抓取失败的情况。可以添加一个这样的模块来协助优化系统。","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447482,"discussion_content":"谢谢你的留言！我相信第10讲中的Lambda数据处理架构会对你有所帮助，期待那时候再次看到你的留言。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555635979,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87491,"user_name":"涵","can_delete":false,"product_type":"c1","uid":1502742,"ip_address":"","ucode":"BB8575DB13F1E0","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg","comment_is_top":false,"comment_ctime":1555620534,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5850587830","product_id":100025301,"comment_content":"从实施经验看，批量处理往往是频率低，一次性数据量很大，响应无需即时，而流数据处理往往是高频率但小数据量，要求即时响应，很期待看到beam是如何将两种数据量级差异很大的场景统一起来，都可以处理的很好。","like_count":1},{"had_liked":false,"id":232009,"user_name":"鲁齐","can_delete":false,"product_type":"c1","uid":1710659,"ip_address":"","ucode":"C64B9FBECBC146","user_header":"","comment_is_top":false,"comment_ctime":1593830499,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1593830499","product_id":100025301,"comment_content":"利用有向图描述数据处理语言，就是抽象每个数据处理过程，使之对应一个运算引擎，这就是有向图的建模过程，不知道我的理解对吗？如果是这样，有向图是如何能与后台运算引擎产生实际连接的？希望老师帮忙解答下，谢谢~","like_count":0},{"had_liked":false,"id":186350,"user_name":"杰之7","can_delete":false,"product_type":"c1","uid":1297232,"ip_address":"","ucode":"F7DA2E21085332","user_header":"https://static001.geekbang.org/account/avatar/00/13/cb/50/66d0bd7f.jpg","comment_is_top":false,"comment_ctime":1583829316,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583829316","product_id":100025301,"comment_content":"最后那张图真是点金之作。非科班出身真是道路漫漫呀，学习下去的唯一动力也许就是热爱吧。关于老师讲的监控，有点像Excel中当中的宏，把处理过程记录下来，方便后续的维护与修改。","like_count":0},{"had_liked":false,"id":184794,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1583405692,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583405692","product_id":100025301,"comment_content":"实时数据吐过来不一定要实时处理吧，先存储一段时间在进行批处理也可以啊，在我们遇到的大部分业务，都是跑批的。实时的主要是OLTP需求。","like_count":0},{"had_liked":false,"id":184793,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1583405566,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583405566","product_id":100025301,"comment_content":"我们这边开发的是作业流，开发者定义作业关系，最终执行的不一定是一个DAG图，而且没有依赖关系的作业可以并发执行，至于数据集共享这一块感觉交给开发者定义更好吧，毕竟开发者更清楚我依赖的数据集，是全集依赖还是子集依赖开发者更清楚，自动分析能做一部分但是实际过程中更重要是开发者业务逻辑的优化。","like_count":0},{"had_liked":false,"id":120474,"user_name":"wong ka seng","can_delete":false,"product_type":"c1","uid":1338885,"ip_address":"","ucode":"C1A713082D0D64","user_header":"https://static001.geekbang.org/account/avatar/00/14/6e/05/d47cee18.jpg","comment_is_top":false,"comment_ctime":1564909738,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564909738","product_id":100025301,"comment_content":"大家好! 我是大规模数据处理的新手, 关於这一讲我有些想法: 文中提到在MAP REDUCE 的依赖关係中, 是一个DAG GRAPH, 我在想关於分配工作量的问题: 只要把工作分成不同大小, 然後把大的工作分给计算力较高的机器, 把小的工作分给计算力弱的机器, 这就应能把效率提到最高, 而分配的结果只要一个简单的程式就能解决: 把机器的性能都记录下来, 然後透过人手或机器估算把算法的覆杂度记下来( 例如排序是 O(N LOG N) ) , 最後分配程式就可应认各机器的性能分配工作量( 设每台机器i 所後的工作大小为Wi, 使得summation of Wi = O(N LOG N) )   ), 这样能否解决工作分配问题? 请指教!","like_count":0},{"had_liked":false,"id":120473,"user_name":"wong ka seng","can_delete":false,"product_type":"c1","uid":1338885,"ip_address":"","ucode":"C1A713082D0D64","user_header":"https://static001.geekbang.org/account/avatar/00/14/6e/05/d47cee18.jpg","comment_is_top":false,"comment_ctime":1564909730,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564909730","product_id":100025301,"comment_content":"大家好! 我是大规模数据处理的新手, 关於这一讲我有些想法: 文中提到在MAP REDUCE 的依赖关係中, 是一个DAG GRAPH, 我在想关於分配工作量的问题: 只要把工作分成不同大小, 然後把大的工作分给计算力较高的机器, 把小的工作分给计算力弱的机器, 这就应能把效率提到最高, 而分配的结果只要一个简单的程式就能解决: 把机器的性能都记录下来, 然後透过人手或机器估算把算法的覆杂度记下来( 例如排序是 O(N LOG N) ) , 最後分配程式就可应认各机器的性能分配工作量( 设每台机器i 所後的工作大小为Wi, 使得summation of Wi = O(N LOG N) )   ), 这样能否解决工作分配问题? 请指教!","like_count":0},{"had_liked":false,"id":94439,"user_name":"漫漫越","can_delete":false,"product_type":"c1","uid":1249222,"ip_address":"","ucode":"F9F1E3AC4CF692","user_header":"https://static001.geekbang.org/account/avatar/00/13/0f/c6/cf344f74.jpg","comment_is_top":false,"comment_ctime":1557811876,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557811876","product_id":100025301,"comment_content":"老师，您好，我的工作中还涉及不到这些技术，请问我该如何跟着专栏学习，或者怎样才能有所提高~","like_count":0},{"had_liked":false,"id":94239,"user_name":"很吵请安静","can_delete":false,"product_type":"c1","uid":1155651,"ip_address":"","ucode":"194DCC3D5288BA","user_header":"https://static001.geekbang.org/account/avatar/00/11/a2/43/96857244.jpg","comment_is_top":false,"comment_ctime":1557749685,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557749685","product_id":100025301,"comment_content":"您好，我是还没毕业的学生，打算以后从事数据算法相关的工作，对于该课程应该如何学习呢，我看本课程应该会讲很多架构上的知识，对于数据算法工程事来说，架构方面的知识需要学的很深吗？谢谢","like_count":0},{"had_liked":false,"id":93674,"user_name":"莫醒醒","can_delete":false,"product_type":"c1","uid":1039794,"ip_address":"","ucode":"8D8AA77A1ABA90","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dd/b2/4fe0944a.jpg","comment_is_top":false,"comment_ctime":1557556754,"is_pvip":false,"replies":[{"id":"33489","content":"谢谢你的留言！哈哈，StreamSQL吗？","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557639694,"ip_address":"","comment_id":93674,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557556754","product_id":100025301,"comment_content":"个人觉得 SQL 才是流批统一的最优选择！","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449727,"discussion_content":"谢谢你的留言！哈哈，StreamSQL吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557639694,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":92059,"user_name":"darren","can_delete":false,"product_type":"c1","uid":1027735,"ip_address":"","ucode":"386736C90F32CA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/97/ba512167.jpg","comment_is_top":false,"comment_ctime":1557181520,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557181520","product_id":100025301,"comment_content":"flink 的流处理和批处理api是不一样的，底层执行也不同，分别是stream task和batch task,为什么flink对外宣称自己是流批统一呢？未来朝这个方向在努力，但当前也没实现啊","like_count":0},{"had_liked":false,"id":90577,"user_name":"朱月俊","can_delete":false,"product_type":"c1","uid":1017707,"ip_address":"","ucode":"4DA0728B862FBD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/87/6b/0b6cd39a.jpg","comment_is_top":false,"comment_ctime":1556583581,"is_pvip":false,"replies":[{"id":"32542","content":"谢谢你的分享！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556653966,"ip_address":"","comment_id":90577,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1556583581","product_id":100025301,"comment_content":"除了map resuce之外，我们还使用了两种流处理技术，简单说就是将任务拆解成一个一个算子，中间勇气kafka连接；还有一个流处理技术就是先将原始数据进行包装，从而屏蔽一条数据和多条数据的区别，然后将数据进行下发，并且通过设计一套diff系统进行重复数据优化。当然，还有一套日志系统用于排错。","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448576,"discussion_content":"谢谢你的分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556653966,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":90285,"user_name":"Nam_楠","can_delete":false,"product_type":"c1","uid":1346282,"ip_address":"","ucode":"57ACDE190A7EA9","user_header":"https://static001.geekbang.org/account/avatar/00/14/8a/ea/f2139dd0.jpg","comment_is_top":false,"comment_ctime":1556466123,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556466123","product_id":100025301,"comment_content":"老师您好，我是在校大三学生，接触大数据生态有半年，在实验室项目中也使用过Hadoop相关技术。对大数据处理技术很感兴趣，计划毕业找数据研发相关岗位。<br><br>听了老师的课收获很多！也希望老师可以给我提一些大数据研发岗位的就业建议，比如技术学习推荐等？","like_count":0},{"had_liked":false,"id":89420,"user_name":"莫冰","can_delete":false,"product_type":"c1","uid":1088262,"ip_address":"","ucode":"0351B86F6DAFB7","user_header":"https://static001.geekbang.org/account/avatar/00/10/9b/06/29b4715f.jpg","comment_is_top":false,"comment_ctime":1556170875,"is_pvip":false,"replies":[{"id":"33008","content":"省略了牛腩的分支，因为这边想要表达的是番茄步骤是重复的","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1557220412,"ip_address":"","comment_id":89420,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1556170875","product_id":100025301,"comment_content":"番茄牛腩那张图，最左边的流程应该是画错了吧，依然是对番茄的处理？番茄牛腩没有牛腩哦 ：）","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448124,"discussion_content":"省略了牛腩的分支，因为这边想要表达的是番茄步骤是重复的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557220412,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89067,"user_name":"Chn.K","can_delete":false,"product_type":"c1","uid":1285191,"ip_address":"","ucode":"F82E8CE20C16FA","user_header":"https://static001.geekbang.org/account/avatar/00/13/9c/47/50cf2cab.jpg","comment_is_top":false,"comment_ctime":1556071111,"is_pvip":false,"replies":[{"id":"32093","content":"总结得很生动啊。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556224251,"ip_address":"","comment_id":89067,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1556071111","product_id":100025301,"comment_content":"有点像搭积木？我要搭个房子，我可以选这些积木，我要搭个火车，我又可以选择那些积木，这些积木的接口都是国际标准的。在完成搭房子搭火车过程中，我并不需要去做积木……","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448002,"discussion_content":"总结得很生动啊。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556224251,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88971,"user_name":"ECHO","can_delete":false,"product_type":"c1","uid":1101496,"ip_address":"","ucode":"E78B3B557D469C","user_header":"https://static001.geekbang.org/account/avatar/00/10/ce/b8/92178ccd.jpg","comment_is_top":false,"comment_ctime":1556036539,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556036539","product_id":100025301,"comment_content":"看回复时间比看文章还长，但收获很大","like_count":0},{"had_liked":false,"id":88715,"user_name":"dexiao10","can_delete":false,"product_type":"c1","uid":1016027,"ip_address":"","ucode":"6F7A97A076C1A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/80/db/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1555985347,"is_pvip":false,"replies":[{"id":"31798","content":"不是，底层完全不一样","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1556000945,"ip_address":"","comment_id":88715,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555985347","product_id":100025301,"comment_content":"不是这个行业的，看的有点不懂。<br>作者可以把 mapreduce 跟 apache beam对比一下，直观展示。<br>还有apqche beam只是对map reduce的一个封装吗，底层原理还是map reduce？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447874,"discussion_content":"不是，底层完全不一样","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556000945,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88652,"user_name":"long.mr","can_delete":false,"product_type":"c1","uid":1071784,"ip_address":"","ucode":"F808C4E62BBCF1","user_header":"https://static001.geekbang.org/account/avatar/00/10/5a/a8/f25ec64c.jpg","comment_is_top":false,"comment_ctime":1555979573,"is_pvip":false,"replies":[{"id":"31782","content":"不确定有没有正确理解这个问题，同样的数据处理还是在啊，只是换了工具","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555988092,"ip_address":"","comment_id":88652,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555979573","product_id":100025301,"comment_content":"老师，我理解虽然统一了流式和批量，但是，如果流式服务的输入没有对接上游数据的话，对于离线的发压端来讲，还是需要借助mapreduce吧。换句话说，目前Google是完全不用mapreduce了吗，如果用的话现在是用在哪个环节呢。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447856,"discussion_content":"不确定有没有正确理解这个问题，同样的数据处理还是在啊，只是换了工具","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555988092,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88628,"user_name":"渡码","can_delete":false,"product_type":"c1","uid":1348536,"ip_address":"","ucode":"8FD8B863D1DA0C","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/b8/6510592e.jpg","comment_is_top":false,"comment_ctime":1555976381,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555976381","product_id":100025301,"comment_content":"阿里贡献给flink的blink代码解决了批流统一的问题","like_count":0},{"had_liked":false,"id":88604,"user_name":"Tomcat","can_delete":false,"product_type":"c1","uid":1346364,"ip_address":"","ucode":"B270CEED693256","user_header":"https://static001.geekbang.org/account/avatar/00/14/8b/3c/0462eca7.jpg","comment_is_top":false,"comment_ctime":1555951152,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555951152","product_id":100025301,"comment_content":"我所在的行业是传统金融行业，现在我们的大数据平台是使用hive 为主，可是一旦遇到大表join 操作，将会运行很长很长的时间，甚至导致整个批处理都宕机！<br>在一天时间窗口都没有办法处理完批处理作业，新的技术方案总是没法审批通过，表示很无奈！","like_count":0},{"had_liked":false,"id":88600,"user_name":"趁早","can_delete":false,"product_type":"c1","uid":1031970,"ip_address":"","ucode":"949FB3AA250D80","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bf/22/26530e66.jpg","comment_is_top":false,"comment_ctime":1555950688,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555950688","product_id":100025301,"comment_content":"思路非常清晰","like_count":0},{"had_liked":false,"id":88459,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1555924558,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555924558","product_id":100025301,"comment_content":"IT类技术更新快，有什么网站之类可以了解前沿技术吗？比如大数据、开发、运维之类","like_count":0},{"had_liked":false,"id":87776,"user_name":"helloWorld","can_delete":false,"product_type":"c1","uid":1137301,"ip_address":"","ucode":"5436F2A13DECC6","user_header":"https://static001.geekbang.org/account/avatar/00/11/5a/95/866bccb7.jpg","comment_is_top":false,"comment_ctime":1555689043,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555689043","product_id":100025301,"comment_content":"看到有同学说blink是阿里自主设计的，其实不是，阿里也没有说是他们自主设计的，blink是阿里基于Flink的改进版，已经推回Flink社区了，阿里现在正在大力地发现Flink社区。","like_count":0},{"had_liked":false,"id":87724,"user_name":"YzmYU","can_delete":false,"product_type":"c1","uid":1447355,"ip_address":"","ucode":"C9FE6A877DFCB0","user_header":"https://static001.geekbang.org/account/avatar/00/16/15/bb/e2c24e25.jpg","comment_is_top":false,"comment_ctime":1555671480,"is_pvip":false,"replies":[{"id":"31615","content":"似乎并不是本篇提到的概念","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555740956,"ip_address":"","comment_id":87724,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555671480","product_id":100025301,"comment_content":"老师，想请问下计算粒度这个概念该怎么理解呢？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447578,"discussion_content":"似乎并不是本篇提到的概念","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555740956,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87649,"user_name":"尚科","can_delete":false,"product_type":"c1","uid":1165054,"ip_address":"","ucode":"F23A164954CA5A","user_header":"https://static001.geekbang.org/account/avatar/00/11/c6/fe/cf8b21ab.jpg","comment_is_top":false,"comment_ctime":1555649843,"is_pvip":false,"replies":[{"id":"31523","content":"没看明白，似乎和这篇内容并不相关？","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555654735,"ip_address":"","comment_id":87649,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555649843","product_id":100025301,"comment_content":"greenplum+sas脚本处理，用perl脚本做作业控制、调度、监控，经常出现greenplum集群机器出问题后作业延时，追数的问题","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447547,"discussion_content":"没看明白，似乎和这篇内容并不相关？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555654735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87606,"user_name":"Chelsea_alpaca","can_delete":false,"product_type":"c1","uid":1503872,"ip_address":"","ucode":"7B214A09F6D45F","user_header":"","comment_is_top":false,"comment_ctime":1555640444,"is_pvip":false,"replies":[{"id":"31504","content":"粗看起来没有问题。但这点信息有点少，可以发详细的文档可以仔细探讨。<br><br>","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555644092,"ip_address":"","comment_id":87606,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555640444","product_id":100025301,"comment_content":"现在在用Spark做parallel computing, 比如每一个executor上面来处理一个单独城市的数据，以及训练一个模型。不知道这是不是一个正确的用法…","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447529,"discussion_content":"粗看起来没有问题。但这点信息有点少，可以发详细的文档可以仔细探讨。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555644092,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87604,"user_name":"xiaowaner","can_delete":false,"product_type":"c1","uid":1437121,"ip_address":"","ucode":"D5DB6FBE90C3C5","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJrZb9pm07aictfkWebiaZPQqzuyQ2L3T6VJHMAvRYTuAicYnYP7YTNtoMumXdMibXWMMfdZVfYHic0BiaQ/132","comment_is_top":false,"comment_ctime":1555640338,"is_pvip":false,"replies":[{"id":"31501","content":"谢谢你的支持！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555643803,"ip_address":"","comment_id":87604,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555640338","product_id":100025301,"comment_content":"😄终于等到更新了。","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447528,"discussion_content":"谢谢你的支持！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555643803,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87494,"user_name":"Rainbow","can_delete":false,"product_type":"c1","uid":1259525,"ip_address":"","ucode":"248A7E2C05E4DE","user_header":"https://static001.geekbang.org/account/avatar/00/13/38/05/67aae6c8.jpg","comment_is_top":false,"comment_ctime":1555626514,"is_pvip":false,"replies":[{"id":"31476","content":"文章里写到了是这个意思","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555635040,"ip_address":"","comment_id":87494,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555626514","product_id":100025301,"comment_content":"flumejava第一次听说不是很了解，老师的意思google内部目前还是flumejava作为主流框架吗？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447484,"discussion_content":"文章里写到了是这个意思","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555635040,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87493,"user_name":"时间是最真的答案","can_delete":false,"product_type":"c1","uid":1183601,"ip_address":"","ucode":"B90F3EF769F865","user_header":"https://static001.geekbang.org/account/avatar/00/12/0f/71/9273e8a4.jpg","comment_is_top":false,"comment_ctime":1555626180,"is_pvip":false,"replies":[{"id":"31473","content":"这个目标应该是能达到的。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555634337,"ip_address":"","comment_id":87493,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555626180","product_id":100025301,"comment_content":"作为一个不会大数据开发的，学完这套课程能不能使用大数据技术做一些初级的工作呢？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447483,"discussion_content":"这个目标应该是能达到的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555634337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87483,"user_name":"微思","can_delete":false,"product_type":"c1","uid":1004349,"ip_address":"","ucode":"853C48AA183A7B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/53/3d/1189e48a.jpg","comment_is_top":false,"comment_ctime":1555607297,"is_pvip":true,"replies":[{"id":"31484","content":"谢谢！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555635990,"ip_address":"","comment_id":87483,"utype":1}],"discussion_count":1,"race_medal":1,"score":"1555607297","product_id":100025301,"comment_content":"👍赞！期待后续章节...","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447478,"discussion_content":"谢谢！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555635990,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]}]}