{"id":75421,"title":"27 | 大数据从哪里来？","content":"<p>大数据就是存储、计算、应用大数据的技术，如果没有数据，所谓大数据就是无源之水、无本之木，所有技术和应用也都无从谈起。可以说，数据在大数据的整个生态体系里面拥有核心的、最无可代替的地位。很多从事机器学习和人工智能的高校学者选择加入互联网企业，并不是贪图企业给的高薪，而是因为只有互联网企业才有他们做研究需要用到的大量数据。</p><p>技术是通用的，算法是公开的，只有数据需要自己去采集。因此数据采集是大数据平台的核心功能之一，也是大数据的来源。数据可能来自企业内部，也可能是来自企业外部，<span class=\"orange\">大数据平台的数据来源主要有数据库、日志、前端程序埋点、爬虫系统</span>。</p><h2>从数据库导入</h2><p>在大数据技术风靡之前，关系数据库（RDMS）是数据分析与处理的主要工具，我们已经在关系数据库上积累了大量处理数据的技巧、知识与经验。所以当大数据技术出现的时候，人们自然而然就会思考，能不能将关系数据库数据处理的技巧和方法转移到大数据技术上，于是Hive、Spark  SQL、Impala这样的大数据SQL产品就出现了。</p><p>虽然Hive这样的大数据产品可以提供和关系数据库一样的SQL操作，但是互联网应用产生的数据却还是只能记录在类似MySQL这样的关系数据库上。这是因为互联网应用需要实时响应用户操作，基本上都是在毫级完成用户的数据读写操作，通过前面的学习我们知道，大数据不是为这种毫秒级的访问设计的。</p><!-- [[[read_end]]] --><p>所以要用大数据对关系数据库上的数据进行分析处理，必须要将数据从关系数据库导入到大数据平台上。上一期我提到了，目前比较常用的数据库导入工具有Sqoop和Canal。</p><p>Sqoop是一个数据库批量导入导出工具，可以将关系数据库的数据批量导入到Hadoop，也可以将Hadoop的数据导出到关系数据库。</p><p>Sqoop数据导入命令示例如下。</p><pre><code>$ sqoop import --connect jdbc:mysql://localhost/db --username foo --password --table TEST\n</code></pre><p>你需要指定数据库URL、用户名、密码、表名，就可以将数据表的数据导入到Hadoop。</p><p>Sqoop适合关系数据库数据的批量导入，如果想实时导入关系数据库的数据，可以选择Canal。</p><p>Canal是阿里巴巴开源的一个MySQL binlog获取工具，binlog是MySQL的事务日志，可用于MySQL数据库主从复制，Canal将自己伪装成MySQL从库，从MySQL获取binlog。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/6d/f84e49e679c9444812200ba0b079ce6d.png?wh=1066*380\" alt=\"\"></p><p>而我们只要开发一个Canal客户端程序就可以解析出来MySQL的写操作数据，将这些数据交给大数据流计算处理引擎，就可以实现对MySQL数据的实时处理了。</p><h2>从日志文件导入</h2><p>日志也是大数据处理与分析的重要数据来源之一，应用程序日志一方面记录了系统运行期的各种程序执行状况，一方面也记录了用户的业务处理轨迹。依据这些日志数据，可以分析程序执行状况，比如应用程序抛出的异常；也可以统计关键业务指标，比如每天的PV、UV、浏览数Top  N的商品等。</p><p>Flume是大数据日志收集常用的工具。Flume最早由Cloudera开发，后来捐赠给Apache基金会作为开源项目运营。Flume架构如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/33/76/33e564d2c4f292584eab32c488f13a76.png?wh=958*426\" alt=\"\"></p><p>从图上看，Flume收集日志的核心组件是Flume Agent，负责将日志从数据源收集起来并保存到大数据存储设备。</p><p>Agent Source负责收集日志数据，支持从Kafka、本地日志文件、Socket通信端口、Unix标准输出、Thrift等各种数据源获取日志数据。</p><p>Source收集到数据后，将数据封装成event事件，发送给Channel。Channel是一个队列，有内存、磁盘、数据库等几种实现方式，主要用来对event事件消息排队，然后发送给Sink。</p><p>Sink收到数据后，将数据输出保存到大数据存储设备，比如HDFS、HBase等。Sink的输出可以作为Source的输入，这样Agent就可以级联起来，依据具体需求，组成各种处理结构，比如下图的结构。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/84/c95302bac6f6cff1653e65382f918b84.png?wh=1770*362\" alt=\"\"></p><p>这是一个日志顺序处理的多级Agent结构，也可以将多个Agent输出汇聚到一个Agent，还可以将一个Agent输出路由分发到多个Agent，根据实际需求灵活组合。</p><p><img src=\"https://static001.geekbang.org/resource/image/05/bf/057e3a89a22cc6a77c9c892b7cdd4ebf.png?wh=1254*848\" alt=\"\"></p><h2>前端埋点采集</h2><p>前端埋点数据采集也是互联网应用大数据的重要来源之一，用户的某些前端行为并不会产生后端请求，比如用户在一个页面的停留时间、用户拖动页面的速度、用户选中一个复选框然后又取消了。这些信息对于大数据处理，对于分析用户行为，进行智能推荐都很有价值。但是这些数据必须通过前端埋点获得，所谓前端埋点，就是应用前端为了进行数据统计和分析而采集数据。</p><p>事实上，互联网应用的数据基本都是由用户通过前端操作产生的，有些互联网公司会将前端埋点数据当作最主要的大数据来源，用户所有前端行为，都会埋点采集，再辅助结合其他的数据源，构建自己的大数据仓库，进而进行数据分析和挖掘。</p><p>对于一个互联网应用，当我们提到前端的时候，可能指的是一个App程序，比如一个iOS应用或者Android应用，安装在用户的手机或者pad上；也可能指的是一个PC Web前端，使用PC浏览器打开；也可能指一个H5前端，由移动设备浏览器打开；还可能指的是一个微信小程序，在微信内打开。这些不同的前端使用不同的开发语言开发，运行在不同的设备上，每一类前端都需要解决自己的埋点问题。</p><p>埋点的方式主要有手工埋点和自动化埋点。</p><p>手工埋点就是前端开发者手动编程将需要采集的前端数据发送到后端的数据采集系统。通常公司会开发一些前端数据上报的SDK，前端工程师在需要埋点的地方，调用SDK，按照接口规范传入相关参数，比如ID、名称、页面、控件等通用参数，还有业务逻辑数据等，SDK将这些数据通过HTTP的方式发送到后端服务器。</p><p>自动化埋点则是通过一个前端程序SDK，自动收集全部用户操作事件，然后全量上传到后端服务器。自动化埋点有时候也被称作无埋点，意思是无需埋点，实际上是全埋点，即全部用户操作都埋点采集。自动化埋点的好处是开发工作量小，数据规范统一。缺点是采集的数据量大，很多数据采集来也不知道有什么用，白白浪费了计算资源，特别是对于流量敏感的移动端用户而言，因为自动化埋点采集上传花费了大量的流量，可能因此成为卸载应用的理由，这样就得不偿失了。在实践中，有时候只是针对部分用户做自动埋点，抽样一部分数据做统计分析。</p><p>介于手工埋点和自动化埋点之间的，还有一种方案是可视化埋点。通过可视化的方式配置哪些前端操作需要埋点，根据配置采集数据。可视化埋点实际上是可以人工干预的自动化埋点。</p><p>就我所见，在很多公司前端埋点都是一笔糊涂账。很多公司对于数据的需求没有整体规划和统一管理，数据分析师、商业智能BI工程师、产品经理、运营人员、技术人员都会在数据采集这里插一脚，却没有专门的数据产品经理来统一负责数据采集的规划和需求工作。很多需要的数据没有采集，更多没用的数据却被源源不断地被采集存储起来。</p><p>不同于业务需求，功能和价值大多数时候都是实实在在的。数据埋点需求的价值很多时候不能直观看到，所以在开发排期上往往被当作低优先级的需求。而很多埋点也确实最后没起到任何作用，加剧了大家这种印象。老板觉得数据重要，却又看不到足够的回报，也渐渐心灰意冷。</p><p>所以专业的事情需要专业对待，从安排专业的人专门负责开始。</p><h2>爬虫系统</h2><p>通过网络爬虫获取外部数据也是公司大数据的重要来源之一。有些数据分析需要行业数据支撑，有些管理和决策需要竞争对手的数据做对比，这些数据都可以通过爬虫获取。</p><p>对于百度这样的公开搜索引擎，如果遇到网页声明是禁止爬虫爬取的，通常就会放弃。但是对于企业大数据平台的爬虫，常常被禁止爬取的数据才是真正需要的数据，比如竞争对手的数据。被禁止爬取的应用通常也会采用一些反爬虫技术，比如检查请求的HTTP头信息是不是爬虫，以及对参数进行加密等。遇到这种情况，需要多花一点技术手段才能爬到想要的数据。</p><h2>小结</h2><p>各种形式的数据从各种数据源导入到大数据平台，进行数据处理计算后，又将数据导出到数据库，完成数据的价值实现。输入的数据格式繁杂、数据量大、冗余信息多，而输出的数据则结构性更好，用更少的数据包含了更多的信息，这在热力学上，被称作熵减。</p><p>熵是表征系统无序状态的一个物理学参量，系统越无序、越混乱，熵越大。我们这个宇宙的熵一刻不停地在增加，当宇宙的熵达到最大值的时候，就是宇宙寂灭之时。虽然宇宙的熵在不停增加，但是在局部，或者某些部分、某些子系统的熵却可以减少。</p><p>比如地球，似乎反而变得更加有序，熵正在减少，主要原因在于这些熵在减少的系统在吸收外部能量，地球在吸收太阳的能量，实现自己熵的减少。大数据平台想要实现数据的熵的减少，也必须要吸收外部的能量，这个能量来自于工程师和分析师的算法和计算程序。</p><p>如果算法和程序设计不合理，那么熵可能就不会下降多少，甚至可能增加。所以大数据技术人员在审视自己工作的时候，可以从熵的视角看看，是不是输出了更有价值、更结构化的数据，是不是用更少量的数据包含了更多的信息。</p><p>人作为一个系统，从青壮到垂老，熵也在不停增加。要想减缓熵增的速度，必须从外部吸收能量。物质上，合理饮食，锻炼身体；精神上，不断学习，参与有价值的工作。那些热爱生活、好好学习、积极工作的人是不是看起来更年轻，而整日浑浑噩噩的人则老得更快。</p><h2>思考题</h2><p>前面提到，爬虫在采集数据的时候，可能会遇到对方的反爬虫策略，反爬虫策略有哪些？如何应对这些反爬虫策略？</p><p>欢迎你点击“请朋友读”，把今天的文章分享给好友。也欢迎你写下自己的思考或疑问，与我和其他同学一起讨论。</p>","neighbors":{"left":{"article_title":"26 | 互联网产品 + 大数据产品 = 大数据平台","id":74879},"right":{"article_title":"28 | 知名大厂如何搭建大数据平台？","id":75717}},"comments":[{"had_liked":false,"id":119405,"user_name":"白荣东","can_delete":false,"product_type":"c1","uid":1129170,"ip_address":"","ucode":"668C8B4EFA0674","user_header":"https://static001.geekbang.org/account/avatar/00/11/3a/d2/b9d6a45e.jpg","comment_is_top":false,"comment_ctime":1564579391,"is_pvip":false,"replies":[{"id":"43888","content":"强，灰常全面~","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1564645836,"ip_address":"","comment_id":119405,"utype":1}],"discussion_count":4,"race_medal":0,"score":"336572028479","product_id":100020201,"comment_content":"反爬虫策略：<br>网页时代，验证header&amp;签名，动态加载，反selenium&#47;phantomjs，ip封禁，有毒数据，动态爬虫阈值（过了阈值后依然允许爬一阵再封禁），各种验证码，云厂商反爬模式识别<br>app时代，ios和安卓的反逆向，比如安卓的加壳，代码混淆，强制登录token，账户管理，反抓包（ssl pin），包签名校验，反注入（监测），so，LLVM混淆，反Hook，异常账号识别，模式识别<br><br>应对这些反爬虫策略: 网页，从简单的header伪装，机器学习验证码，验证码打码平台，ip代理商，反动态抓取校验，阈值报警，多策略爬取校验<br>应对手机反爬：这个是逆向安全团队，加壳有脱壳，账号有养账号，短信打码平台，反抓包有xposed切面hook，反sslpinning，签名校验有调试关闭，so包有模拟环境调用，IDA调试。脚本精灵抓包。<br>当爬虫发现爬取收益小于爬取代价，又没法改变，无利可图的时候，就应该放弃。<br>","like_count":79,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":460890,"discussion_content":"强，灰常全面~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564645836,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1567940,"avatar":"https://static001.geekbang.org/account/avatar/00/17/ec/c4/19f85ada.jpg","nickname":"乃鱼同学","note":"","ucode":"50A96C24978057","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":292889,"discussion_content":"给高手点赞！\n反爬和反反爬，是一种博弈，用什么技术是手段。非常同意您的观点。最后博弈的是投入产出比。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595379062,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1519344,"avatar":"https://static001.geekbang.org/account/avatar/00/17/2e/f0/0bbb0df5.jpg","nickname":"乐天","note":"","ucode":"1DC138F7BD536E","race_medal":2,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":235462,"discussion_content":"全面，学习了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587041304,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1150270,"avatar":"https://static001.geekbang.org/account/avatar/00/11/8d/3e/239df6b2.jpg","nickname":"胡小发","note":"","ucode":"CB893FCC99AA1E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":41268,"discussion_content":"Mark","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1572392525,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":55536,"user_name":"REAL_MADIRD","can_delete":false,"product_type":"c1","uid":1012889,"ip_address":"","ucode":"9E327B5456739D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/74/99/dbdee494.jpg","comment_is_top":false,"comment_ctime":1546225544,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"74560669576","product_id":100020201,"comment_content":"利用熵增熵减原理来过好这一生","like_count":18},{"had_liked":false,"id":55448,"user_name":"纯洁的憎恶","can_delete":false,"product_type":"c1","uid":1130512,"ip_address":"","ucode":"5E9757DE6F45DF","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/10/b6bf3c3c.jpg","comment_is_top":false,"comment_ctime":1546181728,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"57380756576","product_id":100020201,"comment_content":"Sqoop适合离线批量导入关系数据库的数据，Canle适合实时导入关系数据库的数据。<br><br>flume是比较常用的大数据日志收集工具。<br><br>前端埋点采集。很多前端操作不会引发后端响应，但对于分析用户行为十分重要。<br><br>大数据计算的整个过程确实是熵大大降低的过程，因为很多不为人知、难以发觉的规律，被从海量数据中整理出来了。","like_count":13},{"had_liked":false,"id":130435,"user_name":"Kevin Zhang","can_delete":false,"product_type":"c1","uid":1468851,"ip_address":"","ucode":"E1E90A800662A2","user_header":"https://wx.qlogo.cn/mmopen/vi_32/DYAIOgq83ernluE4tN96owInv0MjviaOT7NG03nvavanW3LxWt00HROKfc5W2MICrCXXtQNticVpXgjUibdoHIcag/132","comment_is_top":false,"comment_ctime":1567466418,"is_pvip":false,"replies":[{"id":"48757","content":"需求和目标驱动。<br><br>你要做一件事，就要有清晰的目标和任务，为了达成任务需要什么的手段和工具，通过网络、专家、书籍各种渠道去了解各种可能的工具，因为目标清晰，所以什么样的工具是比较接近需求的，什么样的工具不太合适，可以很快做出判断，对于合适的，就进一步去学习和了解。<br><br>没有需求和目标怎么办？给自己创造一个。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1567494472,"ip_address":"","comment_id":130435,"utype":1}],"discussion_count":2,"race_medal":0,"score":"53107073970","product_id":100020201,"comment_content":"李老师，您知道的大数据框架很多，请问您从哪里第一次知道这些框架的？或者您有哪些获取资讯的渠道？","like_count":13,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465952,"discussion_content":"需求和目标驱动。\n\n你要做一件事，就要有清晰的目标和任务，为了达成任务需要什么的手段和工具，通过网络、专家、书籍各种渠道去了解各种可能的工具，因为目标清晰，所以什么样的工具是比较接近需求的，什么样的工具不太合适，可以很快做出判断，对于合适的，就进一步去学习和了解。\n\n没有需求和目标怎么办？给自己创造一个。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567494472,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1056668,"avatar":"https://static001.geekbang.org/account/avatar/00/10/1f/9c/6e37e32b.jpg","nickname":"simon","note":"","ucode":"77F8D34328603D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":37252,"discussion_content":"&#34;没有需求和目标怎么办？给自己创造一个。&#34;没办法的办法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571561077,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":55110,"user_name":"ヾ(◍°∇°◍)ﾉﾞ","can_delete":false,"product_type":"c1","uid":1044175,"ip_address":"","ucode":"89545632BDA56E","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJOBwR7MCVqwZbPA5RQ2mjUjd571jUXUcBCE7lY5vSMibWn8D5S4PzDZMaAhRPdnRBqYbVOBTJibhJg/132","comment_is_top":false,"comment_ctime":1546045729,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18725914913","product_id":100020201,"comment_content":"反爬虫技术：检查头浏览器信息；检查refer是否正常的流程链上的URL；对IP 或者 imei mac进行实时计算请求量高的；避免csrf攻击的办法也可以用在这里调用接口检查ID；针对通过无界面浏览器的爬取行为要进行行为分析 比如简单的操作步骤间隔时间等<br><br>应对策略：对于疯狂的爬虫封禁。想对付的竞争对手进行真假数据混合。消磨对手排查时间","like_count":4},{"had_liked":false,"id":68949,"user_name":"Creso","can_delete":false,"product_type":"c1","uid":1436020,"ip_address":"","ucode":"4DBEAA8096AED9","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJe0TJQ1CQXlLX52l3aMicKKRt4zKJVI5RZO3A88yombD7xMcySOn9ZWvbGfXHkmhxkK29CzV7Nx3Q/132","comment_is_top":false,"comment_ctime":1550632805,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14435534693","product_id":100020201,"comment_content":"1.请求头<br>2.ip地址<br>3.验证码<br>4.js加密<br>5.必须登录<br>6.真假数据混合<br>7.据说还有sql注入的，这个没有遇到过","like_count":3},{"had_liked":false,"id":189583,"user_name":"不记年","can_delete":false,"product_type":"c1","uid":1045945,"ip_address":"","ucode":"287E40C68356DC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f5/b9/888fe350.jpg","comment_is_top":false,"comment_ctime":1584531711,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5879499007","product_id":100020201,"comment_content":"针对http请求头，通过对token，session验证来反爬<br>针对ip，如果一个ip的行为异常，比如单位时间内请求书过高就封掉一段时间<br>通过验证码，像那种左滑就是<br>针对行为通过机器学习来预测一个请求是人发出的还是机器发出的<br><br>总之就是尽量为难机器<br>","like_count":1},{"had_liked":false,"id":177293,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1581343235,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5876310531","product_id":100020201,"comment_content":"阅过留痕<br>数据及数据采集是大数据的核心，也是计算机界的核心，没有数据计算机就没有了生产资料。计算机的一切操作无非是对数据的增删改查加上一些业务逻辑，以达到挖掘数据价值，提高社会运行效率的作用。","like_count":1},{"had_liked":false,"id":57690,"user_name":"hunterlodge","can_delete":false,"product_type":"c1","uid":1069755,"ip_address":"","ucode":"5B83A79E784161","user_header":"https://static001.geekbang.org/account/avatar/00/10/52/bb/225e70a6.jpg","comment_is_top":false,"comment_ctime":1546868823,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5841836119","product_id":100020201,"comment_content":"“数据埋点需求的价值很多时候不能直观看到，所以在开发排期上往往被当作低优先级的需求。而很多埋点也确实最后没起到任何作用，加剧了大家这种印象。老板觉得数据重要，却又看不到足够的回报，也渐渐心灰意冷。” <br>大实话，我们今年的一个大项目也做了很多埋点，目的也是便于分析项目的上线效果，然而采集的大量数据并没有有效利用起来","like_count":1},{"had_liked":false,"id":56086,"user_name":"杰之7","can_delete":false,"product_type":"c1","uid":1297232,"ip_address":"","ucode":"F7DA2E21085332","user_header":"https://static001.geekbang.org/account/avatar/00/13/cb/50/66d0bd7f.jpg","comment_is_top":false,"comment_ctime":1546392035,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5841359331","product_id":100020201,"comment_content":"通过这一节的学习，用煽减来看待大数据平台。<br><br>整个过程通过初始的数据获取，包括从数据库导入数据，有Sqoop,cancal的方式，日志系统导入数据，有Flume将数据库导入到HDFS中，SDK从前端埋点获取数据，及爬虫系统获取数据。<br><br>通过这些途径获取的数据经过大数据产品的数据处理返回给数据库处理过后的数据，这样得到的数据清洗有用的数据。<br><br>这就是一个大数据煽减的过程。<br><br>","like_count":1,"discussions":[{"author":{"id":2887719,"avatar":"","nickname":"Geek_220257","note":"","ucode":"54280A883207CA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":544782,"discussion_content":"sqoop工具用来批量从数据库导入数据，canal工具用来实时从数据库导入数据。Flume 接入日志系统","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641711271,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":339332,"user_name":"高志权","can_delete":false,"product_type":"c1","uid":2681599,"ip_address":"","ucode":"37F690BEDACE96","user_header":"","comment_is_top":false,"comment_ctime":1648039047,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1648039047","product_id":100020201,"comment_content":"厉害了","like_count":0},{"had_liked":false,"id":330754,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1642146744,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1642146744","product_id":100020201,"comment_content":"Sqoop 不是不怎么搞了吗？","like_count":0},{"had_liked":false,"id":321943,"user_name":"dog_brother","can_delete":false,"product_type":"c1","uid":1619597,"ip_address":"","ucode":"9F64D3C6D815FB","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83er6OV33jHia3U9LYlZEx2HrpsELeh3KMlqFiaKpSAaaZeBttXRAVvDXUgcufpqJ60bJWGYGNpT7752w/132","comment_is_top":false,"comment_ctime":1637114202,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1637114202","product_id":100020201,"comment_content":"从数据采集，引申到熵减，真是高！","like_count":0},{"had_liked":false,"id":175972,"user_name":"鹿鸣","can_delete":false,"product_type":"c1","uid":1816643,"ip_address":"","ucode":"F8DBFB42FD7762","user_header":"https://static001.geekbang.org/account/avatar/00/1b/b8/43/d1ca843f.jpg","comment_is_top":false,"comment_ctime":1580896497,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1580896497","product_id":100020201,"comment_content":"老师，可以讲下kylin 的应用场景吗。","like_count":0},{"had_liked":false,"id":114886,"user_name":"吴小智","can_delete":false,"product_type":"c1","uid":1310798,"ip_address":"","ucode":"C7C9F58B5C9F7B","user_header":"https://static001.geekbang.org/account/avatar/00/14/00/4e/be2b206b.jpg","comment_is_top":false,"comment_ctime":1563423455,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1563423455","product_id":100020201,"comment_content":"敲黑板：文章的小节是重点中的重点。","like_count":0},{"had_liked":false,"id":110297,"user_name":"TKbook","can_delete":false,"product_type":"c1","uid":1073829,"ip_address":"","ucode":"F6E0E99CC79059","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg","comment_is_top":false,"comment_ctime":1562224130,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1562224130","product_id":100020201,"comment_content":"可以用IP地址池、随机的user_agent等来应对反爬虫","like_count":0},{"had_liked":false,"id":90919,"user_name":"balabala","can_delete":false,"product_type":"c1","uid":1500924,"ip_address":"","ucode":"F965BC1723E167","user_header":"https://static001.geekbang.org/account/avatar/00/16/e6/fc/6c39b5fc.jpg","comment_is_top":false,"comment_ctime":1556723024,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556723024","product_id":100020201,"comment_content":"关于数据从哪里来这个问题，在当前有这么多自动化数据导入、数据处理手段的前提下，数据获取、整理、清洗仍然存在很多不可避免的dirty work，怎么样看待和处理遇到的这种dirty work？","like_count":0},{"had_liked":false,"id":62674,"user_name":"小老鼠","can_delete":false,"product_type":"c1","uid":1257460,"ip_address":"","ucode":"C663A0C863A515","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/f4/2dede51a.jpg","comment_is_top":false,"comment_ctime":1548128229,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548128229","product_id":100020201,"comment_content":"大数据获取不断地写磁盘会不会影响系统的性能？","like_count":0},{"had_liked":false,"id":61320,"user_name":"John","can_delete":false,"product_type":"c1","uid":1020861,"ip_address":"","ucode":"E4ADF8488953FB","user_header":"https://static001.geekbang.org/account/avatar/00/0f/93/bd/f3977ebb.jpg","comment_is_top":false,"comment_ctime":1547659794,"is_pvip":false,"replies":[{"id":"21817","content":"Sqoop是SQL操作，所以是通用的。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1547689189,"ip_address":"","comment_id":61320,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1547659794","product_id":100020201,"comment_content":"請問老師 MySQL的binlog用Canel 那麼另一個特別流行的postgresql該用什麼工具呢 謝謝","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":436831,"discussion_content":"Sqoop是SQL操作，所以是通用的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1547689189,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1128318,"avatar":"https://static001.geekbang.org/account/avatar/00/11/37/7e/219dd994.jpg","nickname":"liuyong","note":"","ucode":"D68555E8057F6E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":320854,"discussion_content":"postgresql 可以使用 debezium 导出","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1604489753,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1020861,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/93/bd/f3977ebb.jpg","nickname":"John","note":"","ucode":"E4ADF8488953FB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1128318,"avatar":"https://static001.geekbang.org/account/avatar/00/11/37/7e/219dd994.jpg","nickname":"liuyong","note":"","ucode":"D68555E8057F6E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":328847,"discussion_content":"多谢提醒 那个项目已经结束了 哈哈 看来若自己创业首选还是MySQL和postgresql","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606258433,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":320854,"ip_address":""},"score":328847,"extra":""}]}]},{"had_liked":false,"id":55089,"user_name":"萧杰","can_delete":false,"product_type":"c1","uid":1014762,"ip_address":"","ucode":"2024A5E13A8807","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7b/ea/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1546043405,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1546043405","product_id":100020201,"comment_content":"反爬虫如果企业单单是在http请求头上监听，可以用scarpy框架有支持很多类库，模拟真实用户浏览器渲染请求，现在我也发现电商网站在开始使用请求参数加密的方式，而作为一个爬虫者，从技术手段的角度怎么应对，请老师答疑解惑。","like_count":0}]}