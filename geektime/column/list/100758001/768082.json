{"id":768082,"title":"12｜Kubernetes HPA：云原生应用扩缩容管理之道","content":"<p>你好，我是潘野。</p><p>上一讲的末尾我讲了公有云上引起浪费的两个大类情况，其实云资源浪费是一个普遍存在的问题，无论是在公有云还是私有云环境中都可能发生，如何解决云资源浪费也是基础架构管理中非常重要的一个部分。</p><p>今天我们展开讲讲，哪些原因会造成云资源浪费，以及我们怎么利用好云原生技术来解决云资源的浪费问题，提升我们的资源使用率。</p><h2>哪些情况会造成资源浪费？</h2><p>云资源利用率低是指已分配的云资源没有被充分利用，导致闲置浪费。我们看看哪些情况会造成这个结果。</p><p><strong>首先是过度或错误配置。</strong>在创建云实例时，为了满足峰值需求，应用维护者往往会配置过多的资源。然而，实际应用中，资源需求通常会低于峰值，导致大部分时间资源处于闲置状态。</p><p>比如我就遇到过这种情况：开发申请了十台300G内存的机器做Redis Cluster，而上线之后，发现实际使用率只有20%，这就相当于8台机器被浪费了。</p><p>错误配置的现象也很常见，配置云资源时，由于缺乏经验或相关知识，可能会出现错误配置。例如应用程序比较消耗CPU，不太消耗内存，但是却申请了一个内存很大的机器，导致资源浪费。</p><p><strong>然后是资源预留问题。</strong>&nbsp;为了保证资源的可用性，应用维护者会预留一定量的资源保障应用做横向或者纵向扩展。然而，预留资源往往无法完全利用，导致资源闲置浪费。</p><!-- [[[read_end]]] --><p><strong>最后是不必要的服务，</strong>有时候我们可能会启用不必要的云服务，导致资源浪费。</p><h2>解决办法</h2><p>在云原生体系中，解决上面这些问题并不难做，我们解决的思路是让应用动态地调整资源使用。在流量高峰时期，又能及时动态扩容上去。当流量进入低峰时候，又能释放掉闲置资源。用一个词来总结就是<strong>动态扩缩容。</strong></p><p>动态扩缩容里主要包括两个层级的动态扩缩容。</p><p>第一个层级是应用本身<strong>水平扩展与纵向扩展。</strong></p><p>水平扩展是指通过增加节点数量来扩展应用的容量。例如，可以将一个应用部署在多个服务器上，每个服务器都运行应用的一部分。水平扩展可以有效地提高应用的处理能力，并可以应对更大的负载。</p><p>纵向扩展是指通过增加单个节点的资源来扩展应用的性能。例如，可以增加服务器的 CPU、内存或存储空间。纵向扩展可以提高应用的处理速度，并可以处理更复杂的任务。</p><p>在应用本身这个层级的动态扩缩容，我们可以使用Kubernetes HPA和VPA来实现应用层面的动态扩缩容。</p><p>第二个层级是Kubernetes集群本身容量的动态扩缩容，指根据集群的负载情况自动增加或减少节点数量，以满足应用的需求。这样做可以有效地提高资源利用率，并降低成本。</p><p>在这个层级的动态扩缩容中，可以使用Kubernetes 官方社区支持的集群自动扩缩容工具Cluster Autoscaler，它可以根据 Pod 的资源需求和节点的资源可用情况自动扩缩容集群。</p><p>今天我们主要讲解第一个层级，应用本身的<strong>水平扩展与纵向扩展。</strong></p><h2>HPA</h2><p>我们先来看HPA，也就是水平扩缩容。HPA 可以根据 CPU 利用率、内存利用率和其他自定义指标自动扩缩容 Pod 数量，以满足应用程序的负载需求。</p><p>HPA工作原理是这样的，控制器会定期（默认每 30 秒）查询目标资源的 Pod 的资源使用情况，并将其与 HPA 对象中指定的指标进行比较。如果资源使用情况超过或低于目标指标，HPA 控制器会根据扩缩容策略来扩缩容 Pod 数量。</p><p>Kubernetes默认支持根据容器的CPU和内存的使用率。我们结合例子来理解一下。</p><pre><code class=\"language-plain\">apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n&nbsp; name: web-app-hpa\nspec:\n&nbsp; scaleTargetRef:\n&nbsp; &nbsp; apiVersion: apps/v1\n&nbsp; &nbsp; kind: Deployment\n&nbsp; &nbsp; name: web-app\n&nbsp; minReplicas: 1\n&nbsp; maxReplicas: 10\n&nbsp; metrics:\n&nbsp; - type: Resource\n&nbsp; &nbsp; resource:\n&nbsp; &nbsp; &nbsp; name: cpu\n&nbsp; &nbsp; &nbsp; targetAverageUtilization: 80\n</code></pre><p>从上述配置可以看到，我们针对web-app这个deployment设置了最小副本数1，最大副本数10，在CPU平均使用率80%的时候开始扩容。</p><p><strong>基于CPU和内存指标这种配置非常简单，但是这在实际生产中并不够用。</strong>我们举个常见例子，比如在一个生产者消费者服务里，当消费者的消费速度不足，我们希望启动更多的消费者服务的时候，CPU和内存的使用率这种指标显然就不能满足需求了。</p><h2>怎么扩展HPA的能力</h2><p>现在的HPA支持了从其他的API中获取指标来进行扩容， Kubernetes API 中有一个资源叫 <code>apiservices</code> , 用于注册和管理非核心 API，非核心 API 是指 Kubernetes 核心代码之外的 API。这些 API 可以由第三方开发人员提供，也可以由 Kubernetes 社区提供。</p><p>HPA控制器会从 <code>apiservices</code> 管理的API中获取一些指标，然后根据<strong>定义好指标的阈值</strong>来触发一些扩缩容。 我们可以用 <code>kubectl get apiservices</code> 这个命令，查看集群现在支持的第三方API。</p><pre><code class=\"language-plain\">&gt;&gt;&gt; kubectl get apiservices.apiregistration.k8s.io\nv1beta1.custom.metrics.k8s.io&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; monitoring/prometheus-adapter&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;True&nbsp; &nbsp; &nbsp; &nbsp; 30h\nv1beta1.external.metrics.k8s.io&nbsp; &nbsp; &nbsp; &nbsp; addons-system/keda-operator-metrics-apiserver&nbsp; &nbsp;True&nbsp; &nbsp; &nbsp; &nbsp; 5h37m\nv1beta1.metrics.k8s.io&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;kube-system/metrics-server&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; True&nbsp; &nbsp; &nbsp; &nbsp; 26d\n</code></pre><ul>\n<li>对于资源指标，使用&nbsp;<code>metrics.k8s.io</code> <a href=\"https://kubernetes.io/zh-cn/docs/reference/external-api/metrics.v1beta1/\">API</a>， 一般由&nbsp;<a href=\"https://github.com/kubernetes-incubator/metrics-server\">metrics-server</a>&nbsp;提供。 它可以作为集群插件启动。</li>\n<li>对于自定义指标，使用&nbsp;<code>custom.metrics.k8s.io</code> <a href=\"https://kubernetes.io/zh-cn/docs/reference/external-api/metrics.v1beta1/\">API</a>。 它由其他“适配器（Adapter）” API 服务器提供。从上面的命令输出可以看出，这里是由 <code>prometheus-adapter</code> 来提供的。</li>\n<li>对于外部指标，将使用&nbsp;<code>external.metrics.k8s.io</code> <a href=\"https://kubernetes.io/zh-cn/docs/reference/external-api/metrics.v1beta1/\">API</a>。</li>\n</ul><p>接下来，我们来看看怎么使用Prometheus-Adapter来扩展HPA的能力。</p><h2>Prometheus-Adapter</h2><p>Prometheus-Adapter 是一款用于将 Prometheus 指标转换为 Kubernetes 自定义指标的工具。它可以用于将 Prometheus 监控的应用程序的指标暴露给 Kubernetes Horizontal Pod Autoscaler (HPA) 等工具。</p><p>Prometheus-Adapter 的工作过程要经历三个阶段。</p><ol>\n<li>Prometheus-Adapter 会定期（默认每 30 秒）从 Prometheus 服务器拉取指标数据。</li>\n<li>Prometheus-Adapter 会将拉取到的指标数据转换为 Kubernetes API Server 可以理解的形式。</li>\n<li>Prometheus-Adapter 会将转换后的指标数据暴露给 Kubernetes API Server。</li>\n</ol><p>假设我们有一个 Web 应用程序，我们希望通过Prometheus监控这个应用的性能指标。在这个前提下，我们就可以使用HPA配合Prometheus-Adapter来自动扩缩容 Pod 以满足应用程序的负载需求。</p><p>接下来，我们再通过一个例子，了解一下Prometheus-Adapter是怎么实现HPA扩展的。</p><p>首先，我们需要安装Prometheus和Prometheus-Adapter，这里你参照<a href=\"https://github.com/kubernetes-sigs/prometheus-adapter?tab=readme-ov-file#installation\">官方文档</a>安装即可。</p><p>然后，我们需要在Prometheus-Adapter的configmap中加上我们需要监控的指标，这里是从 Prometheus 查询 HTTP 请求总数，排除某些指标，用\"_qps \"后缀重命名，并计算 30 秒窗口内的查询率。</p><pre><code class=\"language-plain\">  custom:\n   - seriesQuery: '{__name__=~\"^http_requests.*_total$\",container!=\"POD\",namespace!=\"\",pod!=\"\"}'\n     resources:\n       overrides:\n         namespace: { resource: \"namespace\" }\n         pod: { resource: \"pod\" }\n     name:\n       matches: \"(.*)_total\"\n       as: \"${1}_qps\"\n     metricsQuery: sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}[30s])) by (&lt;&lt;.GroupBy&gt;&gt;)\n</code></pre><p>一切都成功的话，我们运行下面这条命令即可获得对应的输出。</p><pre><code class=\"language-plain\">kubectl get --raw '/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_qps'\n</code></pre><p>最后就是HPA的配置了，你可以参考后面的例子。</p><pre><code class=\"language-plain\">kind: HorizontalPodAutoscaler\napiVersion: autoscaling/v2\nmetadata:\n&nbsp; name: sample-httpserver\nspec:\n&nbsp; scaleTargetRef:\n&nbsp; &nbsp; apiVersion: apps/v1\n&nbsp; &nbsp; kind: Deployment\n&nbsp; &nbsp; name: sample-httpserver\n&nbsp; minReplicas: 1\n&nbsp; maxReplicas: 10\n&nbsp; behavior:\n&nbsp; &nbsp; scaleDown:\n&nbsp; &nbsp; &nbsp; stabilizationWindowSeconds: 30\n&nbsp; &nbsp; &nbsp; policies:\n&nbsp; &nbsp; &nbsp; &nbsp; - type: Percent\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; value: 100\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; periodSeconds: 15\n&nbsp; &nbsp; scaleUp:\n&nbsp; &nbsp; &nbsp; stabilizationWindowSeconds: 0\n&nbsp; &nbsp; &nbsp; policies:\n&nbsp; &nbsp; &nbsp; &nbsp; - type: Percent\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; value: 100\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; periodSeconds: 15\n&nbsp; metrics:\n&nbsp; &nbsp; - type: Pods\n&nbsp; &nbsp; &nbsp; pods:\n&nbsp; &nbsp; &nbsp; &nbsp; metric:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; name: http_requests_qps\n&nbsp; &nbsp; &nbsp; &nbsp; target:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; type: AverageValue\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; averageValue: 50000m\n</code></pre><p>这里的配置有三个需要注意的地方。</p><ol>\n<li>最小最大的副本数分别设置 1、10。</li>\n<li>为了测试效果的时效性，设置扩缩容的行为 behavior。</li>\n<li>指定指标 http_requests_qps、类型 Pods 以及目标值 50000m（它表示平均每个 pod 的 RPS 50 ）。比如以 300 的 RPS 访问，副本数就是 300/50=6 。</li>\n</ol><p>测试工具我们选用&nbsp;vegeta，因为它可以指定 RPS。</p><pre><code class=\"language-plain\"># 240\necho \"GET http://192.168.1.92:31617\" | vegeta attack -duration 60s -connections 10 -rate 240 | vegeta report\n# 120\necho \"GET http://192.168.1.92:31617\" | vegeta attack -duration 60s -connections 10 -rate 120 | vegeta report\n# 40\necho \"GET http://192.168.1.92:31617\" | vegeta attack -duration 60s -connections 10 -rate 40 | vegeta report\n</code></pre><p>我们describe下deployment，就可以看到这样的日志，horizontal-pod-autoscaler  New size: 2; reason: pods metric http_requests_qps above target。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/d5/50f495931f8bbfb8b322b7147a53abd5.jpg?wh=3314x697\" alt=\"\"></p><p>如果日志如上图所示，就说明配置成功了。</p><h2>HPA 的不足</h2><p>回顾刚才的过程，我们发现，基于Prometheus-adapter的HPA整体功能还是比较完全的，但也有几个不足之处。</p><p><strong>第一，反应慢半拍</strong>。HPA 是根据历史数据来扩缩容的，所以可能来不及应对突然增加的负载，导致服务中断。</p><p><strong>第二，服务抖动问题。</strong>HPA 频繁扩缩容 Pod 数量，可能会导致服务响应时间变长，影响用户体验。我在生产环境中遇到过用户明确不要启用自动扩容的情况，因为他们的应用不能很好的兼容水平扩展。</p><p><strong>第三，配置复杂。</strong> HPA的基础配置比较简单，但是要扩展它的能力就需要一定的Kubernetes知识，小白用户可能搞不定。比如Prometheus-Adapter依赖Prometheus，这要求我们先要维护一套Prometheus，同时配置上需要指定要转换的指标、转换规则等。如果配置不正确，可能会导致 HPA 无法正常工作。</p><p><strong>第四，应用场景有限。</strong>HPA 只适用于可以根据资源指标进行扩缩容的应用。不能根据事件来进行扩缩容。</p><h2>VPA</h2><p>前面我们学习的HPA用于水平扩展，那么垂直扩展VPA又适合用在什么样的场景，它又具备什么样的优势呢？</p><p>我们以 Prometheus 为例来讨论一下。刚开始集群应用的 Pod 数量比较少，一个 4 CPU、4GB 内存的 Prometheus Pod 就够用了。不过，随着集群规模的扩大，Pod 数量越来越多，Prometheus 需要处理大量涌入的指标，那么性能就会成为瓶颈。</p><p><strong>但是Prometheus不支持水平扩展，</strong>这时，VPA 就能派上用场了。它可以根据单个 Pod 的负载情况，自动扩充其 CPU 和内存资源，有效解决性能问题，同样类似的场景在DevOps常用的Jenkins上也存在。</p><p>所以Pod的纵向自动扩缩VPA具有以下优势。</p><ul>\n<li>不需要再预估Pod初始启动的时候要分多少的CPU和内存，一切交给VPA去处理。</li>\n<li>VPA会根据节点剩余的资源量来确定分配多少CPU和内存给Pod，最大化地利用节点的可分配资源。</li>\n<li>VPA可以自动调整Pod的CPU 和内存请求，无需手工干预。。</li>\n</ul><p><strong>但是我们在实际的生产中用上VPA的机会并不大，为什么呢？</strong>因为垂直扩缩容面对的问题比水平扩缩容要复杂很多。</p><p>首先，VPA不像HPA，HPA不会更改正在运行的Pod，而VPA会更新正在运行的Pod资源配置，这会导致Pod的重建和重启。而且Pod有可能被调度到其他的节点上，这有可能会中断应用程序，这点可以说是VPA落地的最大的阻碍。</p><p>其次，预测资源需求难度比较高。有的时候，应用负载在不断地变化，Pod需要多少资源可能不是线性增长的。这种情况下VPA 要想合理地扩缩容，就需要准确预测 Pod 的资源需求，这件事并不容易。</p><p>最后，有些应用程序并不支持VPA方式。例如基于 JVM 的工作负载是无法使用VPA的，因为对此类工作负载的实际内存用量并没有暴露出来。</p><p>所以社区对VPA的推进比较慢，2017 年社区开始讨论VPA的方案，但是一直到kubernetes 1.25这个版本，VPA才演进到1.0的版本。</p><h2>总结</h2><p>云资源浪费是一个普遍存在的问题，无论是在公有云还是私有云环境中都可能发生。今天，我们分析了造成云资源浪费的原因，主要是资源利用率低和资源浪费两种情况。</p><p>我们解决云资源浪费的关键手段是引入动态扩缩容。动态扩缩容是指根据应用的负载情况自动调整资源配置，以满足应用的需求。在Kubernetes 中常用的两种动态扩缩容工具是水平扩缩容 HPA 和垂直扩缩容 VPA。</p><p>这一讲里，我也结合例子，为你讲解了如何使用Prometheus-Adapter来扩展Kubernetes HPA的能力，支持更多的扩缩容指标。建议你参考课程里的讲解，课后自己动手练习实践一下。</p><p>但是HPA 和 VPA 并不是完美的解决方案，都存在一些局限。例如原生的HPA只支持基于CPU和内存的指标进行水平扩容，VPA可能会造成应用的中断等。我们需要根据实际需求选择合适的工具，并进行合理配置。</p><h2>思考题</h2><p>今天的课程里，我提到了HPA的一些不足，比如在真正流量突发的时候，有可能会遇到反应慢半拍的情况，无法及时跟上扩展，导致应用服务中断。你有哪些思路来解决这个问题呢？</p><p>欢迎在评论区与我讨论。如果这一讲对你有启发，也欢迎分享给身边更多朋友。</p>","comments":[{"had_liked":false,"id":389817,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"江苏","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1713528825,"is_pvip":false,"replies":[{"id":141853,"content":"Knative里有两种方式\n\n第一种叫serving，是根据访问请求来做的动态扩缩容\n第二种叫eventing，是事件驱动模型的serverless","user_name":"作者回复","user_name_real":"编辑","uid":1043450,"ctime":1713894549,"ip_address":"上海","comment_id":389817,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100758001,"comment_content":"事件驱动动态扩缩容方案用knatine方案吗？","like_count":0,"discussions":[{"author":{"id":1043450,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/eb/fa/5dc2bcae.jpg","nickname":"潘野","note":"","ucode":"5B60CCA6C00359","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":642912,"discussion_content":"Knative里有两种方式\n\n第一种叫serving，是根据访问请求来做的动态扩缩容\n第二种叫eventing，是事件驱动模型的serverless","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713894549,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":1043738,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","nickname":"cfanbo","note":"","ucode":"39D8D71453E575","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043450,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/eb/fa/5dc2bcae.jpg","nickname":"潘野","note":"","ucode":"5B60CCA6C00359","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":643145,"discussion_content":"那么在生产环境中，是不是同时用这两个实现cicd？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1714059126,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":642912,"ip_address":"浙江","group_id":0},"score":643145,"extra":""}]}]},{"had_liked":false,"id":389798,"user_name":"夜空中最亮的星","can_delete":false,"product_type":"c1","uid":1267566,"ip_address":"北京","ucode":"ADC3E7B6789955","user_header":"https://static001.geekbang.org/account/avatar/00/13/57/6e/b6795c44.jpg","comment_is_top":false,"comment_ctime":1713493302,"is_pvip":false,"replies":[{"id":141859,"content":"VPA基于历史数据，推荐资源配置，因此在初期可能不太准确。它没有定时提前的能力。","user_name":"作者回复","user_name_real":"编辑","uid":1043450,"ctime":1713895280,"ip_address":"上海","comment_id":389798,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100758001,"comment_content":"根据业务情况和历史数据 定时提前扩容。\nVPA 有什么示列吗？","like_count":0,"discussions":[{"author":{"id":1043450,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/eb/fa/5dc2bcae.jpg","nickname":"潘野","note":"","ucode":"5B60CCA6C00359","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":642918,"discussion_content":"VPA基于历史数据，推荐资源配置，因此在初期可能不太准确。它没有定时提前的能力。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713895280,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392027,"user_name":"Starduster","can_delete":false,"product_type":"c1","uid":1737043,"ip_address":"日本","ucode":"9C0613D5C0158D","user_header":"https://static001.geekbang.org/account/avatar/00/1a/81/53/857c999f.jpg","comment_is_top":false,"comment_ctime":1719788411,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100758001,"comment_content":"老师好，请问VPA做不到对pod资源限制的热更新吗？假设host资源足够，是不是只需要更改k8s记录的资源限制字段就行了，一定要重启pod才能生效吗？","like_count":1},{"had_liked":false,"id":396392,"user_name":"于加硕","can_delete":false,"product_type":"c1","uid":1445845,"ip_address":"福建","ucode":"723797D27971A4","user_header":"https://static001.geekbang.org/account/avatar/00/16/0f/d5/73ebd489.jpg","comment_is_top":false,"comment_ctime":1734263456,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100758001,"comment_content":"老师你好。微服务架构下，应用服务需要做容量规划么？还是统一用HPA来支撑流量变化？\n","like_count":0},{"had_liked":false,"id":393976,"user_name":"斯蒂芬.赵","can_delete":false,"product_type":"c1","uid":1200179,"ip_address":"山东","ucode":"AA0FF2DA654418","user_header":"https://static001.geekbang.org/account/avatar/00/12/50/33/9dcd30c4.jpg","comment_is_top":false,"comment_ctime":1725462497,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100758001,"comment_content":"首先，VPA 不像 HPA，HPA 不会更改正在运行的 Pod，而 VPA 会更新正在运行的 Pod 资源配置，这会导致 Pod 的重建和重启。而且 Pod 有可能被调度到其他的节点上，这有可能会中断应用程序，这点可以说是 VPA 落地的最大的阻碍\n\n\n\n\n这句话有毛病吧，VPA 不会中断正在运行的 pod。VPA 的策略是通过 gradual rolling updates 实现平滑地应用升级，避免影响 pod 的运行。","like_count":0}]}