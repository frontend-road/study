{"id":782557,"title":"12｜数据导入及清洗：ChatGPT在数据处理过程中的完整应用（上）","content":"<p><span class=\"orange\">点击“展开”查看“精华文字稿”</span></p><p>前面的课程中，我们一起走过了数据分析的完整流程，包括<strong>数据获取、清洗、处理、建模、结果呈现和业务价值发现</strong>这六个关键步骤。这里面建模部分相对复杂，到现在我们只是初步讨论，在课程的模块四“增强决策支持”这一章节中会进行详细解读。</p><p>我们在第一讲中讨论过，数据分析的核心目的是<strong>提炼出业务价值，任何不能为业务服务的分析都是无效的</strong>。因此，在深入探讨了每一个环节之后，现在我将把这些部分串联起来，以全面的视角展示ChatGPT如何在整个数据处理过程中提供支持。</p><p>为了让你更好理解，我举一个大家生活中常见的例子，个人贷款全周期数据分析，咱们一起来学习ChatGPT在整个数据分析过程的应用。</p><h2>数据获取</h2><p>来，先复习一下，数据处理之前最重要的工作是什么？对，得先明确分析目标。如果目标不清晰，我们无法判断数据的有效性，也很难高效开展后续分析。</p><p>咱们的案例是个人贷款的全周期数据分析，目标是什么呢？咱先对齐：<strong>优化贷款审批流程、提升客户满意度，并最终降低违约风险</strong>。</p><p>下面我给你提供一个示例数据表，并解释每个字段的含义。</p><p><a href=\"https://shimo.im/sheets/pmkxdbM0laSp9akN/MODOC/\">https://shimo.im/sheets/pmkxdbM0laSp9akN/MODOC/</a> 《12讲示例数据表》</p>","comments":[{"had_liked":false,"id":392124,"user_name":"Devon","can_delete":false,"product_type":"c3","uid":1069991,"ip_address":"上海","ucode":"45442792ABEC73","user_header":"https://static001.geekbang.org/account/avatar/00/10/53/a7/83bf4578.jpg","comment_is_top":false,"comment_ctime":1720005790,"is_pvip":false,"replies":[{"id":142780,"content":"你好，对于你提到的这两个问题，我们可以一一解答\nChatGPT与数据库连接:\n\nAPI连接: 如果你有技术支持，可以通过编程实现ChatGPT与数据库的直接连接。比如使用Python库（如psycopg2用于PostgreSQL，pymysql用于MySQL）来提取数据，然后将这些数据通过Langchain上传给ChatGPT，并得到ChatGPT的回复。\n形式如下：\nMySQL - pymysql - python逻辑 - Langchain - OpenAI API - ChatGPT\n\n如果数据量很大（如几十万行），可能会超出token限制，如何处理这种情况？\n数据采样:\n\n抽样法: 从数据集中抽取有代表性的小样本进行分析，比如使用随机采样、分层抽样等方法。这样可以大大减少需要处理的数据量，同时保留数据的代表性。\n分批处理: 将数据分成多个批次上传到ChatGPT，每批只处理一部分数据。虽然增加了处理的次数，但可以避免超出限制。 参考文档实现文本分割（文档转换器）“https:&#47;&#47;python.langchain.com.cn&#47;docs&#47;modules&#47;data_connection&#47;document_transformers&#47;”\n\n另一种方法是 聚合和总结:\n\n数据聚合: 在数据库中先进行数据聚合和汇总，生成较小的数据集。这可以是通过SQL中的GROUP BY等聚合函数来实现。\n降维与压缩: 使用数据降维技术（如PCA）或数据压缩方法，将数据规模缩小，同时保持数据特性。\n","user_name":"作者回复","user_name_real":"编辑","uid":1056235,"ctime":1723006376,"ip_address":"广东","comment_id":392124,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100769301,"comment_content":"老师好，请问当场景是需要从数据库取数的时候，我有几个衍生问题想要请教：\n1. AI可以帮助生成SQL，但是我还需要把SQL复制到其他可以跑SQL抽取数据的地方，然后把跑出来的数据再上传进ChatGPT是吗?\n2. 如果数据量很大，比如有几十万行，可能超出token限制，有什么解决办法吗？\n谢谢老师","like_count":0,"discussions":[{"author":{"id":1056235,"avatar":"https://static001.geekbang.org/account/avatar/00/10/1d/eb/b2123759.jpg","nickname":"尹会生","note":"","ucode":"D1093DBD093617","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649212,"discussion_content":"你好，对于你提到的这两个问题，我们可以一一解答\nChatGPT与数据库连接:\n\nAPI连接: 如果你有技术支持，可以通过编程实现ChatGPT与数据库的直接连接。比如使用Python库（如psycopg2用于PostgreSQL，pymysql用于MySQL）来提取数据，然后将这些数据通过Langchain上传给ChatGPT，并得到ChatGPT的回复。\n形式如下：\nMySQL - pymysql - python逻辑 - Langchain - OpenAI API - ChatGPT\n\n如果数据量很大（如几十万行），可能会超出token限制，如何处理这种情况？\n数据采样:\n\n抽样法: 从数据集中抽取有代表性的小样本进行分析，比如使用随机采样、分层抽样等方法。这样可以大大减少需要处理的数据量，同时保留数据的代表性。\n分批处理: 将数据分成多个批次上传到ChatGPT，每批只处理一部分数据。虽然增加了处理的次数，但可以避免超出限制。 参考文档实现文本分割（文档转换器）“https://python.langchain.com.cn/docs/modules/data_connection/document_transformers/”\n\n另一种方法是 聚合和总结:\n\n数据聚合: 在数据库中先进行数据聚合和汇总，生成较小的数据集。这可以是通过SQL中的GROUP BY等聚合函数来实现。\n降维与压缩: 使用数据降维技术（如PCA）或数据压缩方法，将数据规模缩小，同时保持数据特性。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723006377,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]}]}