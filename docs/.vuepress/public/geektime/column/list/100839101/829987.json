{"id":829987,"title":"16｜缓存：节省使用大模型的成本","content":"<p>你好，我是郑晔！</p><p>上一讲，我们讲了一种工程实践——长期记忆，它可以让我们的应用更加了解自己的用户。这一讲，我们继续，讨论另外一种工程实践——缓存。</p><h2>缓存</h2><blockquote>\n<p>计算机科学中只有两大难题：缓存失效和命名。<br>\n—— Phil Karlton</p>\n</blockquote><p>稍有经验的程序员对缓存都不陌生，在任何一个正式的工程项目上都少不了缓存的身影。硬件里面有缓存，软件里面也有缓存，缓存已经成了程序员的必修课。</p><p>我们为什么要使用缓存呢？主要就是为了减少访问低速服务的次数，提高访问速度。大模型显然就是一个低速服务，甚至比普通的服务还要慢。</p><p>为了改善大模型的使用体验，人们已经做出了一些努力，比如采用流式响应，提升第一个字出现在用户面前的速度。缓存，显然是另外一个可以解决大模型响应慢的办法。</p><p><img src=\"https://static001.geekbang.org/resource/image/d9/ed/d95af15a6475a62fe4eabe0692fd7ced.jpg?wh=3292x1270\" alt=\"\"></p><p>一个使用了缓存的大模型应用在接受到用户请求之后，会先到缓存中进行查询，如果命中缓存，则直接将内容返回给用户，如果没有命中，再去请求大模型生成相应的回答。</p><p>在这个架构中，关键点就是如果缓存命中，就直接将内容返回给用户，也就说明，在这种情况下无需访问大模型。我们使用大模型生成数据时，是根据请求和生成的内容计费的。</p><p>如果能够不请求大模型就给用户返回内容，我们就可以节省一次生成的费用，换言之，每次有效的缓存命中，就是在节省成本。所以，<strong>对大模型应用而言，缓存是至关重要的，一方面可以提升访问速度，另一方面，可以实实在在地节省成本</strong>。</p><!-- [[[read_end]]] --><h2>LangChain 中的缓存</h2><p>因为缓存在大模型应用开发中是一个普遍的需求，所以，LangChain 也为它提供了基础抽象。下面就是一段使用了缓存的代码：</p><pre><code class=\"language-python\">from time import time\n\nfrom langchain.globals import set_llm_cache\nfrom langchain_core.caches import InMemoryCache\nfrom langchain_openai import ChatOpenAI\n\nset_llm_cache(InMemoryCache())\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\nstart_time = time()\nresponse = model.invoke(\"给我讲个一句话笑话\")\nend_time = time()\nprint(response.content)\nprint(f\"第一次调用耗时: {end_time - start_time}秒\")\n\nstart_time = time()\nresponse = model.invoke(\"给我讲个一句话笑话\")\nend_time = time()\nprint(response.content)\nprint(f\"第二次调用耗时: {end_time - start_time}秒\")\n</code></pre><p>这段代码里只有一句是重点，就是设置大模型的缓存：</p><pre><code class=\"language-python\">set_llm_cache(InMemoryCache())\n</code></pre><p>下面是一次执行的结果，从结果上看，因为有缓存，第二次明显比第一次快得多。</p><pre><code class=\"language-python\">为什么鸡过马路？因为它想去对面看看有没有更好的“鸡”会！\n第一次调用耗时: 4.399242162704468秒\n为什么鸡过马路？因为它想去对面看看有没有更好的“鸡”会！\n第二次调用耗时: 0.00045371055603027344秒\n</code></pre><p>在 LangChain 里，缓存是一个全局选项，只要设置了缓存，所有的大模型都可以使用它。如果某个特定的大模型不需要缓存，可以在设置的时候关掉缓存：</p><pre><code class=\"language-python\">model = ChatOpenAI(model=\"gpt-4o-mini\", cache=False)\n</code></pre><p>当然，如果你不想缓存成为一个全局选项，只想针对某个特定进行设置也是可以的：</p><pre><code class=\"language-plain\">model = ChatOpenAI(model=\"gpt-4o-mini\", cache=InMemoryCache())\n</code></pre><p>LangChain 里的缓存是一个统一的接口，其核心能力就是把生成的内容插入缓存以及根据提示词进行查找。LangChain 社区提供了很多缓存实现，像我们在前面例子里用到的内存缓存，还有基于数据库的缓存，当然，也有我们最熟悉的 Redis 缓存。</p><p>虽然 LangChain 提供了许多缓存实现，但本质上说，只有两类缓存——精确缓存和语义缓存。精确缓存，只是在提示词完全相同的情况下才能命中缓存，它和我们理解的传统缓存是一致的，我们前面用来演示的内存缓存就是精确缓存。</p><h2>语义缓存</h2><p>但大模型应用的特点就决定了精确缓存往往是失效的。因为大模型应用通常采用的是自然语言交互，以自然语言为提示词，就很难做到完全相同。像前面我展示的那个例子，实际上是我特意构建的，才能保证精确匹配。所以，语义匹配就成了更好的选择。</p><p>语义匹配我们并不陌生，前面讲 RAG 时，我们讲过了基于向量的语义匹配。LangChain 社区提供了许多语义缓存的实现，在各种语义缓存中，我们最熟悉的应该是 Redis。</p><p>在大部分人眼中，Redis 应该属于精确匹配的缓存。Redis 这么多年也在不断地发展，有很多新功能不断地拓展出来，最典型的就是 Redis Stack，它就是在原本开源 Redis 基础上扩展了其它的一些能力。</p><p>比如，对 JSON 支持（RedisJSON），对全文搜索的支持（RediSearch），对时序数据的支持（RedisTimeSeries），对概率结构的支持（RedisBloom）。其中，支持全文搜索的 RediSearch 就可以用来实现基于语义的搜索。全文搜索，本质上也是语义搜索，而这个能力刚好就是我们在语义缓存中需要的。</p><p>你现在知道了，Redis 对于语义缓存的支持是基于 RediSearch 的。所以，要想使用语义缓存，我们需要使用安装了 RediSearch 的 Redis，一种方式是使用 Redis Stack：</p><pre><code class=\"language-bash\">docker run -p 6379:6379 redis/redis-stack-server:latest\n</code></pre><p>下面是一个使用 Redis 语义缓存的例子：</p><pre><code class=\"language-python\">from langchain.globals import set_llm_cache\nfrom langchain_community.cache import RedisSemanticCache\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\n\nRETURN_VAL_TYPE = Sequence[Generation]\n\ndef prompt_key(prompt: str) -&gt; str:\n    messages = json.loads(prompt)\n    result = [\"('{}', '{}')\".format(data['kwargs']['type'], data['kwargs']['content']) for data in messages if\n               'kwargs' in data and 'type' in data['kwargs'] and 'content' in data['kwargs']]\n    return ' '.join(result)\n\n\nclass FixedSemanticCache(BaseCache):\n    def __init__(self, cache: BaseCache):\n        self.cache = cache\n\n    def lookup(self, prompt: str, llm_string: str) -&gt; Optional[RETURN_VAL_TYPE]:\n        key = prompt_key(prompt)\n        return self.cache.lookup(key, llm_string)\n\n    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -&gt; None:\n        key = prompt_key(prompt)\n        return self.cache.update(key, llm_string, return_val)\n\n    def clear(self, **kwargs: Any) -&gt; None:\n        return self.cache.clear(**kwargs)\n\nset_llm_cache(\n    FixedSemanticCache(\n        RedisSemanticCache(redis_url=\"redis://localhost:6379\",\n                           embedding=OpenAIEmbeddings())\n    )\n)\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\nstart_time = time()\nresponse = model.invoke(\"\"\"请给我讲一个一句话笑话\"\"\")\nend_time = time()\nprint(response.content)\nprint(f\"第一次调用耗时: {end_time - start_time}秒\")\n\nstart_time = time()\nresponse = model.invoke(\"\"\"你能不能给我讲一个一句话笑话\"\"\")\nend_time = time()\nprint(response.content)\nprint(f\"第二次调用耗时: {end_time - start_time}秒\")\n</code></pre><p>我们先把注意力放在后面的核心代码上，在调用模型时，我们给出了两句并不完全相同的提示词。作为普通人，我们很容易看出，这两句话的意图是一样的。如果采用精确匹配，显然是无法命中的，但如果是语义匹配，则应该是可以命中的。</p><p>这里的语义缓存，我们采用了 RedisSemanticCache。在配置中，我们指定了 Redis 的地址和 Embedding 模型。LangChain 支持的 Redis 缓存有精确缓存和语义缓存两种，RedisCache 对应的是精确缓存，RedisSemanticCache 对应的是语义缓存。</p><p>最后说一下 FixedSemanticCache，其实，它是不应该存在的，它是为了解决 LangChain实现中的一个问题而写的。LangChain 在实现缓存机制的时候，会先把消息做字符串化处理，然后，再交给缓存去查找。</p><p>在转化成字符串的过程中，LangChain 目前的实现是把它转换成一个 JSON 字符串，这个 JSON 字符串里除了提示词本身外，还会有很多额外信息，也就是消息对象本身的信息。当提示词本身很小的时候，这个生成的字符串信噪比就很低，正是因为噪声过大，结果就是不同的提示词都能匹配到相同的内容上，所以，总是能够命中缓存。</p><p>这段代码是写在框架内部的，不论采用什么样的缓存实现都有这个问题。只不过，因为精确缓存要完全匹配得上，这个实现的问题不会暴露出来，但对于语义缓存来说，就是一个非常严重的问题了。</p><p>在 LangChain 还没有修复这个问题之前，FixedSemanticCache 就是一个临时解决方案。思路也很简单，既然信噪比太低，就把信息提取出来，在这个实现里，把提示词和消息类型从字符串中提取出来，作为存储到 Redis 里的键值。如果后续 LangChain 解决了这个问题，FixedSemanticCache 就可以去掉了。</p><p>下面是一次执行的结果，从结果上看，第二次比第一次快了很多，这说明缓存起了作用：</p><pre><code class=\"language-plain\">为什么计算机很冷？因为它们总是打开窗口！\n第一次调用耗时: 5.586452960968018秒\n为什么计算机很冷？因为它们总是打开窗口！\n第二次调用耗时: 0.579164981842041秒\n</code></pre><p>正如你在这里看到的，我们把 Redis 当作语义缓存，它起到了和我们之前讲到的向量存储类似的作用。实际上，LangChain 社区确实已经有了实现 VectorStore 接口的 Redis，也就是说，我们完全可以用 Redis 替换之前讲过的向量存储。事实上，这里的语义缓存底层就是用了这个实现了 VectorStore 接口的 Redis。</p><p>顺便说一下，Redis 社区在向量的支持上也在继续努力，有一个项目 <a href=\"https://www.redisvl.com/\">RedisVL</a>（Redis Vector Library）就是把 Redis 当作向量数据库，有兴趣的话你可以了解一下。</p><p>无论是前面用 InMemoryCache 介绍精确缓存，还是这里采用 Redis 来介绍语义缓存，都是为了降低大家理解的难度。</p><p>实际上，LangChain 社区已经集成了大量的缓存实现，其中，有我们已经耳熟能详的，比如基于 SQL 和 NoSQL 的实现，也有基于 Elasticsearch 这样搜索项目的实现，这些都是基于传统项目实现的，还有一些项目就是针对大模型应用设计的缓存项目，这其中最典型的当属 <a href=\"https://github.com/zilliztech/GPTCache\">GPTCache</a>。总之，如果需要在项目上采用缓存，不妨先去了解一下不同的缓存项目。</p><h2>总结时刻</h2><p>这一讲，我们又讲了一个非常重要的工程实践——缓存。对大模型应用而言，缓存是至关重要的，一方面可以提升访问速度，另一方面，可以实实在在地节省调用大模型的成本。</p><p>我们结合 LangChain 框架介绍了缓存的实现。缓存主要分成两大类，精确缓存和语义缓存。精确缓存，是在提示词完全相同的情况下才能命中缓存，而语义缓存则是语义类似的情况下就能命中缓存。对于聊天类型的应用而言，语义缓存的实用价值更大一些。</p><p>我以 Redis 为例给你介绍了语义缓存，你也知道了，很多项目都是支持语义缓存的，有的是基于已有的项目进行改造，比如 Redis 和 Elasticsearch，有的是专门为了大模型应用而设计的，比如，GPTCache。</p><p>如果今天的内容你只能记住一件事，那请记住，<strong>对大模型应用而言，缓存可以提升访问速度，降低访问成本</strong>。</p><h2>思考题</h2><p>LangChain 社区提供了<a href=\"https://python.langchain.com/docs/integrations/llm_caching/\">大量的缓存实现</a>，你可以去了解一下不同的缓存实现，看看哪个实现可以与你在用的基础设施结合起来。欢迎在留言区分享你的所得。</p>","neighbors":{"left":{"article_title":"15｜长期记忆：让大模型更了解你","id":829671},"right":{"article_title":"17｜集中接入：将大模型统一管理起来","id":830506}},"comments":[]}