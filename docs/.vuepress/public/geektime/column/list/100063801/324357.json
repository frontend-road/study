{"id":324357,"title":"18 | 容器网络配置（3）：容器中的网络乱序包怎么这么高？","content":"<p>你好，我是程远。这一讲，我们来聊一下容器中发包乱序的问题。</p><p>这个问题也同样来自于工作实践，我们的用户把他们的应用程序从物理机迁移到容器之后，从网络监控中发现，容器中数据包的重传的数量要比在物理机里高了不少。</p><p>在网络的前面几讲里，我们已经知道了容器网络缺省的接口是veth，veth接口都是成对使用的。容器通过veth接口向外发送数据，首先需要从veth的一个接口发送给跟它成对的另一个接口。</p><p>那么这种接口会不会引起更多的网络重传呢？如果会引起重传，原因是什么，我们又要如何解决呢？接下来我们就带着这三个问题开始今天的学习。</p><h2>问题重现</h2><p>我们可以在容器里运行一下 <code>iperf3</code> 命令，向容器外部发送一下数据，从iperf3的输出\"Retr\"列里，我们可以看到有多少重传的数据包。</p><p>比如下面的例子里，我们可以看到有162个重传的数据包。</p><pre><code class=\"language-shell\"># iperf3 -c 192.168.147.51\nConnecting to host 192.168.147.51, port 5201\n[  5] local 192.168.225.12 port 51700 connected to 192.168.147.51 port 5201\n[ ID] Interval           Transfer     Bitrate                        Retr    Cwnd\n[  5]   0.00-1.00   sec  1001 MBytes  8.40 Gbits/sec  162    192 KBytes\n…\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  9.85 GBytes  8.46 Gbits/sec  162             sender\n[  5]   0.00-10.04  sec  9.85 GBytes  8.42 Gbits/sec                  receiver\n \niperf Done.\n</code></pre><!-- [[[read_end]]] --><p><strong>网络中发生了数据包的重传，有可能是数据包在网络中丢了，也有可能是数据包乱序导致的。</strong>那么，我们怎么来判断到底是哪一种情况引起的重传呢？</p><p>最直接的方法就是用tcpdump去抓包，不过对于大流量的网络，用tcpdump抓包瞬间就会有几个GB的数据。可是这样做的话，带来的额外系统开销比较大，特别是在生产环境中这个方法也不太好用。</p><p>所以这里我们有一个简单的方法，那就是运行netstat命令来查看协议栈中的丢包和重传的情况。比如说，在运行上面的iperf3命令前后，我们都在容器的Network Namespace里运行一下netstat看看重传的情况。</p><p>我们会发现，一共发生了162次（604-442）快速重传（fast retransmits），这个数值和iperf3中的Retr列里的数值是一样的。</p><pre><code class=\"language-shell\">-bash-4.2# nsenter -t 51598 -n netstat -s | grep retran\n    454 segments retransmited\n    442 fast retransmits\n-bash-4.2# nsenter -t 51598 -n netstat -s | grep retran\n    616 segments retransmited\n    604 fast retransmits\n</code></pre><h2>问题分析</h2><h3>快速重传（fast retransmit）</h3><p>在刚才的问题重现里，我们运行netstat命令后，统计了快速重传的次数。那什么是快速重传（fast retransmit）呢？这里我给你解释一下。</p><p>我们都知道TCP协议里，发送端（sender）向接受端（receiver）发送一个数据包，接受端（receiver）都回应ACK。如果超过一个协议栈规定的时间（RTO），发送端没有收到ACK包，那么发送端就会重传（Retransmit）数据包，就像下面的示意图一样。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/4b/bc6a6059f49a5b61e95ba3705894b64b.jpeg?wh=3200*1800\" alt=\"\"></p><p>不过呢，这样等待一个超时之后再重传数据，对于实际应用来说太慢了，所以TCP协议又定义了快速重传 （fast retransmit）的概念。它的基本定义是这样的：<strong>如果发送端收到3个重复的ACK，那么发送端就可以立刻重新发送ACK对应的下一个数据包。</strong></p><p>就像下面示意图里描述的那样，接受端没有收到Seq 2这个包，但是收到了Seq 3–5的数据包，那么接收端在回应Ack的时候，Ack的数值只能是2。这是因为按顺序来说收到Seq 1的包之后，后面Seq 2一直没有到，所以接收端就只能一直发送Ack 2。</p><p>那么当发送端收到3个重复的Ack 2后，就可以马上重新发送 Seq 2这个数据包了，而不用再等到重传超时之后了。</p><p><img src=\"https://static001.geekbang.org/resource/image/21/5d/21935661dda5069f8dyy91e6cd5b295d.jpeg?wh=3200*1800\" alt=\"\"></p><p>虽然TCP快速重传的标准定义是需要收到3个重复的Ack，不过你会发现在Linux中常常收到一个Dup Ack（重复的Ack）后，就马上重传数据了。这是什么原因呢？</p><p>这里先需要提到 <strong>SACK</strong> 这个概念，SACK也就是选择性确认（Selective Acknowledgement）。其实跟普通的ACK相比呢，SACK会把接收端收到的所有包的序列信息，都反馈给发送端。</p><p>你看看下面这张图，就能明白这是什么意思了。</p><p><img src=\"https://static001.geekbang.org/resource/image/2d/55/2d61334e4066391ceeb90cac0bb25d55.jpeg?wh=3200*1800\" alt=\"\"></p><p>那有了SACK，对于发送端来说，在收到SACK之后就已经知道接收端收到了哪些数据，没有收到哪些数据。</p><p>在Linux内核中会有个判断（你可以看看下面的这个函数），大概意思是这样的：如果在接收端收到的数据和还没有收到的数据之间，两者数据量差得太大的话（超过了reordering*mss_cache），也可以马上重传数据。</p><p>这里你需要注意一下，<strong>这里的数据量差是根据bytes来计算的，而不是按照包的数目来计算的，所以你会看到即使只收到一个SACK，Linux也可以重发数据包。</strong></p><pre><code class=\"language-shell\">static bool tcp_force_fast_retransmit(struct sock *sk)\n{\n        struct tcp_sock *tp = tcp_sk(sk);\n \n        return after(tcp_highest_sack_seq(tp),\n                     tp-&gt;snd_una + tp-&gt;reordering * tp-&gt;mss_cache);\n}\n</code></pre><p>好了，了解了快速重传的概念之后，我们再来看看，如果netstat中有大量的\"fast retransmits\"意味着什么？</p><p>如果你再用netstat查看\"reordering\"，就可以看到大量的SACK发现的乱序包。</p><pre><code class=\"language-shell\">-bash-4.2# nsenter -t 51598 -n netstat -s  | grep reordering\n    Detected reordering 501067 times using SACK\n</code></pre><p><strong>其实在云平台的这种网络环境里，网络包乱序+SACK之后，产生的数据包重传的量要远远高于网络丢包引起的重传。</strong></p><p>比如说像下面这张图里展示的这样，Seq 2与Seq 3这两个包如果乱序的话，那么就会引起Seq 2的立刻重传。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/d6/a99709757a45279324600a45f7a44cd6.jpeg?wh=3200*1800\" alt=\"\"></p><h3>Veth接口的数据包的发送</h3><p>现在我们知道了网络包乱序会造成数据包的重传，接着我们再来看看容器的veth接口配置有没有可能会引起数据包的乱序。</p><p>在上一讲里，我们讲过通过veth接口从容器向外发送数据包，会触发peer veth设备去接收数据包，这个接收的过程就是一个网络的softirq的处理过程。</p><p>在触发softirq之前，veth接口会模拟硬件接收数据的过程，通过enqueue_to_backlog()函数把数据包放到某个CPU对应的数据包队列里（softnet_data）。</p><pre><code class=\"language-shell\">static int netif_rx_internal(struct sk_buff *skb)\n{\n        int ret;\n \n        net_timestamp_check(netdev_tstamp_prequeue, skb);\n \n        trace_netif_rx(skb);\n \n#ifdef CONFIG_RPS\n        if (static_branch_unlikely(&amp;rps_needed)) {\n                struct rps_dev_flow voidflow, *rflow = &amp;voidflow;\n                int cpu;\n \n                preempt_disable();\n                rcu_read_lock();\n \n                cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow);\n                if (cpu &lt; 0)\n                        cpu = smp_processor_id();\n \n                ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);\n \n                rcu_read_unlock();\n                preempt_enable();\n        } else\n#endif\n        {\n                unsigned int qtail;\n \n                ret = enqueue_to_backlog(skb, get_cpu(), &amp;qtail);\n                put_cpu();\n        }\n        return ret;\n}\n</code></pre><p>从上面的代码，我们可以看到，在缺省的状况下（也就是没有RPS的情况下），enqueue_to_backlog()把数据包放到了“当前运行的CPU”（get_cpu()）对应的数据队列中。如果是从容器里通过veth对外发送数据包，那么这个“当前运行的CPU”就是容器中发送数据的进程所在的CPU。</p><p>对于多核的系统，这个发送数据的进程可以在多个CPU上切换运行。进程在不同的CPU上把数据放入队列并且raise softirq之后，因为每个CPU上处理softirq是个异步操作，所以两个CPU network softirq handler处理这个进程的数据包时，处理的先后顺序并不能保证。</p><p>所以，veth对的这种发送数据方式增加了容器向外发送数据出现乱序的几率。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/3b/c11581ec8f390b13ebc89fdb4cc2043b.jpeg?wh=3200*1800\" alt=\"\"></p><h3>RSS和RPS</h3><p>那么对于veth接口的这种发包方式，有办法减少一下乱序的几率吗？</p><p>其实，我们在上面netif_rx_internal()那段代码中，有一段在\"#ifdef CONFIG_RPS\"中的代码。</p><p>我们看到这段代码中在调用enqueue_to_backlog()的时候，传入的CPU并不是当前运行的CPU，而是通过get_rps_cpu()得到的CPU，那么这会有什么不同呢？这里的RPS又是什么意思呢？</p><p>要解释RPS呢，需要先看一下RSS，这个RSS不是我们之前说的内存RSS，而是和网卡硬件相关的一个概念，它是Receive Side Scaling的缩写。</p><p>现在的网卡性能越来越强劲了，从原来一条RX队列扩展到了N条RX队列，而网卡的硬件中断也从一个硬件中断，变成了每条RX队列都会有一个硬件中断。</p><p>每个硬件中断可以由一个CPU来处理，那么对于多核的系统，多个CPU可以并行的接收网络包，这样就大大地提高了系统的网络数据的处理能力.</p><p>同时，在网卡硬件中，可以根据数据包的4元组或者5元组信息来保证同一个数据流，比如一个TCP流的数据始终在一个RX队列中，这样也能保证同一流不会出现乱序的情况。</p><p>下面这张图，大致描述了一下RSS是怎么工作的。</p><p><img src=\"https://static001.geekbang.org/resource/image/ea/33/ea365c0d44625cf89c746d91799f9633.jpeg?wh=3200*1800\" alt=\"\"></p><p>RSS的实现在网卡硬件和驱动里面，而RPS（Receive Packet Steering）其实就是在软件层面实现类似的功能。它主要实现的代码框架就在上面的netif_rx_internal()代码里，原理也不难。</p><p>就像下面的这张示意图里描述的这样：在硬件中断后，CPU2收到了数据包，再一次对数据包计算一次四元组的hash值，得到这个数据包与CPU1的映射关系。接着会把这个数据包放到CPU1对应的softnet_data数据队列中，同时向CPU1发送一个IPI的中断信号。</p><p>这样一来，后面CPU1就会继续按照Netowrk softirq的方式来处理这个数据包了。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/f9/e0413b74cbc1b5edcde5442fe94e11f9.jpeg?wh=3200*1800\" alt=\"\"></p><p>RSS和RPS的目的都是把数据包分散到更多的CPU上进行处理，使得系统有更强的网络包处理能力。在把数据包分散到各个CPU时，保证了同一个数据流在一个CPU上，这样就可以减少包的乱序。</p><p>明白了RPS的概念之后，我们再回头来看veth对外发送数据时候，在enqueue_to_backlog()的时候选择CPU的问题。显然，如果对应的veth接口上打开了RPS的配置以后，那么对于同一个数据流，就可以始终选择同一个CPU了。</p><p>其实我们打开RPS的方法挺简单的，只要去/sys目录下，在网络接口设备接收队列中修改队列里的rps_cpus的值，这样就可以了。rps_cpus是一个16进制的数，每个bit代表一个CPU。</p><p>比如说，我们在一个12CPU的节点上，想让host上的veth接口在所有的12个CPU上，都可以通过RPS重新分配数据包。那么就可以执行下面这段命令：</p><pre><code class=\"language-shell\"># cat /sys/devices/virtual/net/veth57703b6/queues/rx-0/rps_cpus\n000\n# echo fff &gt; /sys/devices/virtual/net/veth57703b6/queues/rx-0/rps_cpus\n# cat /sys/devices/virtual/net/veth57703b6/queues/rx-0/rps_cpus\nfff\n</code></pre><h2>重点小结</h2><p>好了，今天的内容讲完了，我们做个总结。我们今天讨论的是容器中网络包乱序引起重传的问题。</p><p>由于在容器平台中看到大部分的重传是快速重传（fast retransmits），我们先梳理了什么是快速重传。快速重传的基本定义是：<strong>如果发送端收到3个重复的ACK，那么发送端就可以立刻重新发送ACK对应的下一个数据包，而不用等待发送超时。</strong></p><p>不过我们在Linux系统上还会看到发送端收到一个重复的ACK就快速重传的，这是因为Linux下对SACK做了一个特别的判断之后，就可以立刻重传数据包。</p><p>我们再对容器云平台中的快速重传做分析，就会发现这些重传大部分是由包的乱序触发的。</p><p>通过对容器veth网络接口进一步研究，我们知道它可能会增加数据包乱序的几率。同时在这个分析过程中，我们也看到了Linux网络RPS的特性。</p><p><strong>RPS和RSS的作用类似，都是把数据包分散到更多的CPU上进行处理，使得系统有更强的网络包处理能力。它们的区别是RSS工作在网卡的硬件层，而RPS工作在Linux内核的软件层。</strong></p><p>在把数据包分散到各个CPU时，RPS保证了同一个数据流是在一个CPU上的，这样就可以有效减少包的乱序。那么我们可以把RPS的这个特性配置到veth网络接口上，来减少数据包乱序的几率。</p><p>不过，我这里还要说明的是，RPS的配置还是会带来额外的系统开销，在某些网络环境中会引起softirq CPU使用率的增大。那接口要不要打开RPS呢？这个问题你需要根据实际情况来做个权衡。</p><p>同时你还要注意，TCP的乱序包，并不一定都会产生数据包的重传。想要减少网络数据包的重传，我们还可以考虑协议栈中其他参数的设置，比如/proc/sys/net/ipv4/tcp_reordering。</p><h2>思考题</h2><p>在这一讲中，我们提到了Linux内核中的tcp_force_fast_retransmit()函数。那么你可以想想看，这个函数中的tp-&gt;recording和内核参数 /proc/sys/net/ipv4/tcp_reordering是什么关系？它们对数据包的重传会带来什么影响？</p><pre><code class=\"language-shell\">static bool tcp_force_fast_retransmit(struct sock *sk)\n{\n        struct tcp_sock *tp = tcp_sk(sk);\n \n        return after(tcp_highest_sack_seq(tp),\n                     tp-&gt;snd_una + tp-&gt;reordering * tp-&gt;mss_cache);\n}\n</code></pre><p>欢迎你在留言区分享你的思考或疑问。如果学完这一讲让你有所收获，也欢迎转发给你的同事、或者朋友，一起交流探讨。</p>","neighbors":{"left":{"article_title":"17｜容器网络配置（2）：容器网络延时要比宿主机上的高吗?","id":324122},"right":{"article_title":"19 | 容器安全（1）：我的容器真的需要privileged权限吗?","id":326253}},"comments":[{"had_liked":false,"id":270863,"user_name":"蒹葭","can_delete":false,"product_type":"c1","uid":1247976,"ip_address":"","ucode":"1625300C2A0D56","user_header":"https://static001.geekbang.org/account/avatar/00/13/0a/e8/ea77be2f.jpg","comment_is_top":false,"comment_ctime":1609305416,"is_pvip":false,"replies":[{"id":"98375","content":"RPS会对数据包重新计算hash, 然后把数据包重新分派到新的cpu对应的队列，之后还需要用IPI的中断通知新的cpu, 而在IPI中断之后，就需要再做一次softirq。这样对于高频率收包的情况下， softirq就会明显的增大。<br>在实际应用的时候，对于物理网络接口，如果已经有了RSS的情况，一般就不需要再打开RPS了。<br>","user_name":"作者回复","comment_id":270863,"uid":"2070138","ip_address":"","utype":1,"ctime":1609594506,"user_name_real":"CY"}],"discussion_count":1,"race_medal":0,"score":"53148912968","product_id":100063801,"comment_content":"RPS 的配置还是会带来额外的系统开销，在某些网络环境中会引起 softirq CPU 使用率的增大。 <br>老师请教一下这里的某些网络环境指的是什么网络环境？具体增大softirq CPU 使用率的原因是什么呢？","like_count":13,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512768,"discussion_content":"RPS会对数据包重新计算hash, 然后把数据包重新分派到新的cpu对应的队列，之后还需要用IPI的中断通知新的cpu, 而在IPI中断之后，就需要再做一次softirq。这样对于高频率收包的情况下， softirq就会明显的增大。\n在实际应用的时候，对于物理网络接口，如果已经有了RSS的情况，一般就不需要再打开RPS了。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609594506,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":273547,"user_name":"po","can_delete":false,"product_type":"c1","uid":1023905,"ip_address":"","ucode":"7DB36C278F34D7","user_header":"https://static001.geekbang.org/account/avatar/00/0f/9f/a1/d75219ee.jpg","comment_is_top":false,"comment_ctime":1610634812,"is_pvip":true,"replies":[{"id":"99535","content":"ovs和iptables的规则是独立的。<br>数据包在协议栈中传递的时候有专门的hook点来处理对应的规则，比如iptables规则在netfilter的pre, in, out等hooks点上处理，ovs规则在ovs相关的device上处理。","user_name":"作者回复","comment_id":273547,"uid":"2070138","ip_address":"","utype":1,"ctime":1610929339,"user_name_real":"CY"}],"discussion_count":2,"race_medal":0,"score":"18790503996","product_id":100063801,"comment_content":"网络插件(flanneld、calico、ovs)和iptables的关系有点乱，网络插件也有自己的转发表，比如ovs的流表和iptables的规则是什么关系呢？","like_count":5,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513627,"discussion_content":"ovs和iptables的规则是独立的。\n数据包在协议栈中传递的时候有专门的hook点来处理对应的规则，比如iptables规则在netfilter的pre, in, out等hooks点上处理，ovs规则在ovs相关的device上处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610929339,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1023905,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9f/a1/d75219ee.jpg","nickname":"po","note":"","ucode":"7DB36C278F34D7","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":343305,"discussion_content":"假如网络用的是flanneld和iptables，老师能介绍一下数据包从容器到外网的流程吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610989630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":270072,"user_name":"莫名","can_delete":false,"product_type":"c1","uid":1007254,"ip_address":"","ucode":"E28F2602BA25DD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/96/a03175bc.jpg","comment_is_top":false,"comment_ctime":1608892909,"is_pvip":false,"replies":[{"id":"97997","content":"@莫名，<br>其实除了RSS&#47;RPS外，你还可以去看一下RFS的概念，这个和这一讲的关系不大，并且逻辑要更加复杂，就没有在文档中说明了。<br><br>你测试iperf3的环境是什么？如果改了veth的rps_cpus配置，没有效果，那么还要再分析有没有其他情况造成了快速重传。","user_name":"作者回复","comment_id":270072,"uid":"2070138","ip_address":"","utype":1,"ctime":1609056159,"user_name_real":"CY"}],"discussion_count":5,"race_medal":0,"score":"18788762093","product_id":100063801,"comment_content":"赞，感谢老师，学习了新知识 RPS。<br><br>试了下，CONFIG_RPS、rps_cpus 开启时，iperf3 测试仍然存在快速重传情况，没有明显改观。是不是漏了什么？","like_count":5,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512507,"discussion_content":"@莫名，\n其实除了RSS/RPS外，你还可以去看一下RFS的概念，这个和这一讲的关系不大，并且逻辑要更加复杂，就没有在文档中说明了。\n\n你测试iperf3的环境是什么？如果改了veth的rps_cpus配置，没有效果，那么还要再分析有没有其他情况造成了快速重传。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609056159,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1007254,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/96/a03175bc.jpg","nickname":"莫名","note":"","ucode":"E28F2602BA25DD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337823,"discussion_content":"在容器中使用 iperf3。感谢指点，我有空再分析下。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1609080907,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2129632,"avatar":"https://static001.geekbang.org/account/avatar/00/20/7e/e0/ff046b55.jpg","nickname":"李兵","note":"","ucode":"291243E3AB889F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385732,"discussion_content":"不是还有内核TCP重传参数设置吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627235207,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2170645,"avatar":"https://static001.geekbang.org/account/avatar/00/21/1f/15/bacf9770.jpg","nickname":"白夜行","note":"","ucode":"6D9ED88EA1B8FB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":348257,"discussion_content":"我也是， 没发现什么变化。。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612493122,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1007254,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/96/a03175bc.jpg","nickname":"莫名","note":"","ucode":"E28F2602BA25DD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2170645,"avatar":"https://static001.geekbang.org/account/avatar/00/21/1f/15/bacf9770.jpg","nickname":"白夜行","note":"","ucode":"6D9ED88EA1B8FB","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":348278,"discussion_content":"容器和物理机上分别使用 iperf3 测试，观察了下，主要区别是 Cwnd 数值差别比较大。容器里面 Cwnd 比较大（拥塞控制相关），更容易发生重传，显式指定 socket 缓冲区大小为物理机上的平均 Cwnd 数值（iperf3 client 端 CWnd 列输出），例如：iperf3 -c 10.246.38.11 -w 400K，重传降为 0。","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1612497325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":348257,"ip_address":""},"score":348278,"extra":""}]}]},{"had_liked":false,"id":271573,"user_name":"大大","can_delete":false,"product_type":"c1","uid":1340730,"ip_address":"","ucode":"3A3DC9AC382651","user_header":"https://static001.geekbang.org/account/avatar/00/14/75/3a/a7596c06.jpg","comment_is_top":false,"comment_ctime":1609721994,"is_pvip":false,"replies":[{"id":"98651","content":"现成的自动配置的方法我不知道。我们是在自己实现的cni模块里对veth的rps做配置的。","user_name":"作者回复","comment_id":271573,"uid":"2070138","ip_address":"","utype":1,"ctime":1609944564,"user_name_real":"CY"}],"discussion_count":1,"race_medal":0,"score":"10199656586","product_id":100063801,"comment_content":"有深度，有什么方式自动配置veth的rps么","like_count":2,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512966,"discussion_content":"现成的自动配置的方法我不知道。我们是在自己实现的cni模块里对veth的rps做配置的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609944564,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":271005,"user_name":"上邪忘川","can_delete":false,"product_type":"c1","uid":1276588,"ip_address":"","ucode":"1C4459917B038D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIz9dKN1C8rKQoaVtmEGdzObhlia6zAfTsPYOm4ibz39VjTbu7Aia1LyeedHR26b6nxUtcCufpichcYgw/132","comment_is_top":false,"comment_ctime":1609344523,"is_pvip":false,"replies":[{"id":"98372","content":"一个F是4个bits, 总共三个F， 那么是4 * 3 = 12","user_name":"作者回复","comment_id":271005,"uid":"2070138","ip_address":"","utype":1,"ctime":1609592512,"user_name_real":"CY"}],"discussion_count":1,"race_medal":0,"score":"10199279115","product_id":100063801,"comment_content":"rps_cpus 是一个 16 进制的数，每个 bit 代表一个 CPU。那么12个CPU，为啥是FFF？请教一下","like_count":3,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512816,"discussion_content":"一个F是4个bits, 总共三个F， 那么是4 * 3 = 12","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609592512,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":270554,"user_name":"Geek2014","can_delete":false,"product_type":"c1","uid":2028957,"ip_address":"","ucode":"9EB356D8DF287E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","comment_is_top":false,"comment_ctime":1609156996,"is_pvip":false,"replies":[{"id":"98236","content":"物理机上的网卡和驱动一般都有RSS, 一般不需要RPS。<br>","user_name":"作者回复","comment_id":270554,"uid":"2070138","ip_address":"","utype":1,"ctime":1609249776,"user_name_real":"CY"}],"discussion_count":1,"race_medal":0,"score":"10199091588","product_id":100063801,"comment_content":"有一点不适很明白：“我们的用户把他们的应用程序从物理机迁移到容器之后，从网络监控中发现，容器中数据包的重传的数量要比在物理机里高了不少”<br><br>我理解，这个是因为容器的环境没有设置那个RPS导致的？也就是物理机器如果没设置也会有类似的问题吧","like_count":3,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512668,"discussion_content":"物理机上的网卡和驱动一般都有RSS, 一般不需要RPS。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609249776,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":270089,"user_name":"谢哈哈","can_delete":false,"product_type":"c1","uid":2326880,"ip_address":"","ucode":"5AADE70B5AFE27","user_header":"https://static001.geekbang.org/account/avatar/00/23/81/60/71ed6ac7.jpg","comment_is_top":false,"comment_ctime":1608898809,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10198833401","product_id":100063801,"comment_content":"内核参数tcp_reordering是快速重传的一个初始门限值，到达max tcp_reordering的内核参数大小就会触发快速重传，而tp-&gt;reordering是乱序的数据量bytes总大小，触发到达一定大小也会触发快速重传，两者没什么关系，但都是触发快速重传的一个必要条件","like_count":3,"discussions":[{"author":{"id":1144195,"avatar":"https://static001.geekbang.org/account/avatar/00/11/75/83/e6aa8443.jpg","nickname":"localhost","note":"","ucode":"415F609B78B72F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340680,"discussion_content":"按作者贴出来的内核代码，和你描述的不一样。tp->reodering肯定不是bytes总大小，否则计算的时候就不会去乘以mss了。另外tp->reordering是随丢包情况调整的，只是不能大于max_tcp_reordering，而不是个所谓初始门限值。如果到达max_tcp_reordering的内核参数大小就会触发快速重传，那么其实是很困难的，因为你拿ss命令去看当前的tp->reordering的大小，基本上都和max_tcp_reordering差距挺大的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610096735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269968,"user_name":"大拇哥","can_delete":false,"product_type":"c1","uid":1161483,"ip_address":"","ucode":"888C59A8045B5B","user_header":"https://static001.geekbang.org/account/avatar/00/11/b9/0b/cdf98d11.jpg","comment_is_top":false,"comment_ctime":1608864621,"is_pvip":true,"replies":[{"id":"98378","content":"@LWW<br>&gt;1 <br>对的，乱序重传，最终会影响到延时和吞吐量。<br><br>&gt;2<br>RPS会对数据包重新计算hash, 然后把数据包重新分派到新的cpu对应的队列，之后还需要用IPI的中断通知新的cpu, 而在IPI中断之后，就需要再做一次softirq。这样对于高频率收包的情况下， softirq就会明显的增大。<br>在实际应用的时候，对于物理网络接口，如果已经有了RSS的情况，一般就不需要再打开RPS了。","user_name":"作者回复","comment_id":269968,"uid":"2070138","ip_address":"","utype":1,"ctime":1609596894,"user_name_real":"CY"}],"discussion_count":2,"race_medal":0,"score":"10198799213","product_id":100063801,"comment_content":"1.这个网络包的乱序导致的重传最终造成的影响就是网络传输的延迟和吞吐量的下降？<br>2.这个问题你需要根据实际情况来做个权衡。这个实际情况是指？是说开启rps带来的softirq CPU增高和网络延迟的权衡吗还是说其它方面的权衡？ 这个感觉没有讲清楚，还请老师赐教。","like_count":2,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512468,"discussion_content":"@LWW\n&amp;gt;1 \n对的，乱序重传，最终会影响到延时和吞吐量。\n\n&amp;gt;2\nRPS会对数据包重新计算hash, 然后把数据包重新分派到新的cpu对应的队列，之后还需要用IPI的中断通知新的cpu, 而在IPI中断之后，就需要再做一次softirq。这样对于高频率收包的情况下， softirq就会明显的增大。\n在实际应用的时候，对于物理网络接口，如果已经有了RSS的情况，一般就不需要再打开RPS了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609596894,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1992293,"avatar":"","nickname":"Geek_2169f1","note":"","ucode":"149744A63A7F5D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337448,"discussion_content":"开启rps为啥softirq CPU会增高？请老师赐教","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608907370,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":309568,"user_name":"多选参数","can_delete":false,"product_type":"c1","uid":1248326,"ip_address":"","ucode":"B2294D80AB075F","user_header":"https://static001.geekbang.org/account/avatar/00/13/0c/46/dfe32cf4.jpg","comment_is_top":false,"comment_ctime":1630236556,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1630236556","product_id":100063801,"comment_content":"老师，您好。假如是正常的网络收包的话，应该是产生一次硬件中断，然后产生一次软中断吧？那么这里 veth 发送的时候，都已经是 skb 了，这个时候应该已经是软中断过程了，为什么还会再次产生一次软中断？希望老师问答","like_count":0},{"had_liked":false,"id":304407,"user_name":"鸠摩·智","can_delete":false,"product_type":"c1","uid":1211909,"ip_address":"","ucode":"6CD93CD1DB6955","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/05/431d380f.jpg","comment_is_top":false,"comment_ctime":1627392551,"is_pvip":false,"replies":[{"id":"111086","content":"可以根据节点上网络的情况来决定在哪一端配置rps.","user_name":"作者回复","comment_id":304407,"uid":"2070138","ip_address":"","utype":1,"ctime":1628691417,"user_name_real":"CY"}],"discussion_count":1,"race_medal":1,"score":"1627392551","product_id":100063801,"comment_content":"请教老师一个问题，设置veth pair的rps，在两端host和container都需要配置吗？","like_count":0,"discussions":[{"author":{"id":2070138,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/96/7a/8a14d008.jpg","nickname":"CY","note":"","ucode":"4AF98230985918","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524034,"discussion_content":"可以根据节点上网络的情况来决定在哪一端配置rps.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628691417,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272533,"user_name":"姜姜","can_delete":false,"product_type":"c1","uid":1120332,"ip_address":"","ucode":"1DAD620C8E9F31","user_header":"https://static001.geekbang.org/account/avatar/00/11/18/4c/e12f3b41.jpg","comment_is_top":false,"comment_ctime":1610114707,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1610114707","product_id":100063801,"comment_content":"“在RSS的作用下，每条 RX 队列都会有一个硬件中断，每个硬件中断可以由一个 CPU 来处理。在网卡硬件中，可以根据数据包的 4 元组或者 5 元组信息来保证同一个数据流，比如一个 TCP 流的数据始终在一个 RX 队列中。”<br><br>请问老师，这里每一个单独的RX队列的中断是对应一个固定的cpu来处理呢，还是任意一个空闲的cpu都可以来响应处理？<br>假设一个RX对应一个固定的cpu：因为RX中数据已经是有序的了，由这个cpu按序处理，我觉得就没有必要再开启RPS。<br>假设一个RX对应的cpu并不是固定的：开启RPS之后，由于每个cpu处理的时间不一样，重新hash到某个cpu的时候，可能前后两包数据就会乱序了，此时RPS并不能保证同一五元组的报文有序。<br>所以我觉得，只需要开启RSS就行，RPS作用不大，不知道我的理解是否有误？以及，一个RX队列是否对应一个固定的cpu呢？","like_count":0,"discussions":[{"author":{"id":2956746,"avatar":"","nickname":"Geek_4911b5","note":"","ucode":"EB3F19827ECCD2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":559573,"discussion_content":"1. RPS主要在发送端打开，就是docker内部需要，接收端如果有RSS就可以了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648820368,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}