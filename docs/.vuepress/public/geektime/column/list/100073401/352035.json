{"id":352035,"title":"01 | 性能调优的必要性：Spark本身就很快，为啥还需要我调优？","content":"<p>你好，我是吴磊。</p><p>在日常的开发工作中，我发现有个现象很普遍。很多开发者都认为Spark的执行性能已经非常强了，实际工作中只要按部就班地实现业务功能就可以了，没有必要进行性能调优。</p><p>你是不是也这么认为呢？确实，Spark的核心竞争力就是它的执行性能，这主要得益于Spark基于内存计算的运行模式和钨丝计划的锦上添花，以及Spark SQL上的专注与发力。</p><p>但是，真如大家所说，<strong>开发者只要把业务逻辑实现了就万事大吉了吗</strong>？这样，咱们先不急于得出结论，你先跟着我一起看两个日常开发中常见的例子，最后我们再来回答这个问题。</p><p>在数据应用场景中，ETL（Extract Transform Load）往往是打头阵的那个，毕竟源数据经过抽取和转换才能用于探索和分析，或者是供养给机器学习算法进行模型训练，从而挖掘出数据深层次的价值。我们今天要举的两个例子，都取自典型ETL端到端作业中常见的操作和计算任务。</p><h2>开发案例1：数据抽取</h2><p>第一个例子很简单：给定数据条目，从中抽取特定字段。这样的数据处理需求在平时的ETL作业中相当普遍。想要实现这个需求，我们需要定义一个函数extractFields：它的输入参数是Seq[Row]类型，也即数据条目序列；输出结果的返回类型是Seq[(String, Int)]，也就是（String, Int）对儿的序列；函数的计算逻辑是从数据条目中抽取索引为2的字符串和索引为4的整型。</p><!-- [[[read_end]]] --><p>应该说这个业务需求相当简单明了，实现起来简直是小菜一碟。在实际开发中，我观察到有不少同学一上来就迅速地用下面的方式去实现，干脆利落，代码写得挺快，功能也没问题，UT、功能测试都能过。</p><pre><code>//实现方案1 —— 反例\nval extractFields: Seq[Row] =&gt; Seq[(String, Int)] = {\n  (rows: Seq[Row]) =&gt; {\n    var fields = Seq[(String, Int)]()\n    rows.map(row =&gt; {\n        fields = fields :+ (row.getString(2), row.getInt(4))\n    })\n  fields\n  }\n}\n</code></pre><p>在上面这个函数体中，是先定义一个类型是Seq[(String, Int)]的变量fields，变量类型和函数返回类型完全一致。然后，函数逐个遍历输入参数中的数据条目，抽取数据条目中索引是2和4的字段并且构建二元元组，紧接着把元组追加到最初定义的变量fields中。最后，函数返回类型是Seq[(String, Int)]的变量fields。</p><p>乍看上去，这个函数似乎没什么问题。特殊的地方在于，尽管这个数据抽取函数很小，在复杂的ETL应用里是非常微小的一环，但在整个ETL作业中，它会在不同地方被频繁地反复调用。如果我基于这份代码把整个ETL应用推上线，就会发现ETL作业端到端的执行效率非常差，在分布式环境下完成作业需要两个小时，这样的速度难免有点让人沮丧。</p><p>想要让ETL作业跑得更快，我们自然需要做性能调优。可问题是我们该从哪儿入手呢？既然extractFields这个小函数会被频繁地调用，不如我们从它下手好了，看看有没有可能给它“减个肥、瘦个身”。重新审视函数extractFields的类型之后，我们不难发现，这个函数从头到尾无非是从Seq[Row]到Seq[(String, Int)]的转换，函数体的核心逻辑就是字段提取，只要从Seq[Row]可以得到Seq[(String, Int)]，目的就达到了。</p><p>要达成这两种数据类型之间的转换，除了利用上面这种开发者信手拈来的过程式编程，我们还可以用函数式的编程范式。函数式编程的原则之一就是尽可能地在函数体中避免副作用（Side effect），副作用指的是函数对于状态的修改和变更，比如上例中extractFields函数对于fields变量不停地执行追加操作就属于副作用。</p><p>基于这个想法，我们就有了第二种实现方式，如下所示。与第一种实现相比，它最大的区别在于去掉了fields变量。之后，为了达到同样的效果，我们在输入参数Seq[Row]上直接调用map操作逐一地提取特定字段并构建元组，最后通过toSeq将映射转换为序列，干净利落，一气呵成。</p><pre><code>//实现方案2 —— 正例\nval extractFields: Seq[Row] =&gt; Seq[(String, Int)] = {\n  (rows: Seq[Row]) =&gt; \n    rows.map(row =&gt; (row.getString(2), row.getInt(4))).toSeq\n}\n\n</code></pre><p>你可能会问：“两份代码实现无非是差了个中间变量而已，能有多大差别呢？看上去不过是代码更简洁了而已。”事实上，我基于第二份代码把ETL作业推上线后，就惊奇地发现端到端执行性能提升了一倍！从原来的两个小时缩短到一个小时。<strong>两份功能完全一样的代码，在分布式环境中的执行性能竟然有着成倍的差别。因此你看，在日常的开发工作中，仅仅专注于业务功能实现还是不够的，任何一个可以进行调优的小环节咱们都不能放过。</strong></p><h2>开发案例2：数据过滤与数据聚合</h2><p>你也许会说：“你这个例子只是个例吧？更何况，这个例子里的优化，仅仅是编程范式的调整，看上去和Spark似乎也没什么关系啊！”不要紧，我们再来看第二个例子。第二个例子会稍微复杂一些，我们先来把业务需求和数据关系交代清楚。</p><pre><code>/**\n(startDate, endDate)\ne.g. (&quot;2021-01-01&quot;, &quot;2021-01-31&quot;)\n*/\nval pairDF: DataFrame = _\n \n/**\n(dim1, dim2, dim3, eventDate, value)\ne.g. (&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;2021-01-15&quot;, 12)\n*/\nval factDF: DataFrame = _\n \n// Storage root path\nval rootPath: String = _ \n\n</code></pre><p>在这个案例中，我们有两份数据，分别是pairDF和factDF，数据类型都是DataFrame。第一份数据pairDF的Schema包含两个字段，分别是开始日期和结束日期。第二份数据的字段较多，不过最主要的字段就两个，一个是Event date事件日期，另一个是业务关心的统计量，取名为Value。其他维度如dim1、dim2、dim3主要用于数据分组，具体含义并不重要。从数据量来看，pairDF的数据量很小，大概几百条记录，factDF数据量很大，有上千万行。</p><p>对于这两份数据来说，具体的业务需求可以拆成3步：</p><ol>\n<li>对于pairDF中的每一组时间对，从factDF中过滤出Event date落在其间的数据条目；</li>\n<li>从dim1、dim2、dim3和Event date 4个维度对factDF分组，再对业务统计量Value进行汇总；</li>\n<li>将最终的统计结果落盘到Amazon S3。</li>\n</ol><p>针对这样的业务需求，不少同学按照上面的步骤按部就班地进行了如下的实现。接下来，我就结合具体的代码来和你说说其中的计算逻辑。</p><pre><code>//实现方案1 —— 反例\ndef createInstance(factDF: DataFrame, startDate: String, endDate: String): DataFrame = {\nval instanceDF = factDF\n.filter(col(&quot;eventDate&quot;) &gt; lit(startDate) &amp;&amp; col(&quot;eventDate&quot;) &lt;= lit(endDate))\n.groupBy(&quot;dim1&quot;, &quot;dim2&quot;, &quot;dim3&quot;, &quot;event_date&quot;)\n.agg(sum(&quot;value&quot;) as &quot;sum_value&quot;)\ninstanceDF\n}\n \npairDF.collect.foreach{\ncase (startDate: String, endDate: String) =&gt;\nval instance = createInstance(factDF, startDate, endDate)\nval outPath = s&quot;${rootPath}/endDate=${endDate}/startDate=${startDate}&quot;\ninstance.write.parquet(outPath)\n} \n</code></pre><p>首先，他们是以factDF、开始时间和结束时间为形参定义createInstance函数。在函数体中，先根据Event date对factDF进行过滤，然后从4个维度分组汇总统计量，最后将汇总结果返回。定义完createInstance函数之后，收集pairDF到Driver端并逐条遍历每一个时间对，然后以factDF、开始时间、结束时间为实参调用createInstance函数，来获取满足过滤要求的汇总结果。最后，以Parquet的形式将结果落盘。</p><p>同样地，这段代码从功能的角度来说没有任何问题，而且从线上的结果来看，数据的处理逻辑也完全符合预期。不过，端到端的执行性能可以说是惨不忍睹，在16台机型为C5.4xlarge AWS EC2的分布式运行环境中，基于上面这份代码的ETL作业花费了半个小时才执行完毕。</p><p>没有对比就没有伤害，在同一份数据集之上，采用下面的第二种实现方式，仅用2台同样机型的EC2就能让ETL作业在15分钟以内完成端到端的计算任务。<strong>两份代码的业务功能和计算逻辑完全一致，执行性能却差了十万八千里</strong>。</p><pre><code>//实现方案2 —— 正例\nval instances = factDF\n.join(pairDF, factDF(&quot;eventDate&quot;) &gt; pairDF(&quot;startDate&quot;) &amp;&amp; factDF(&quot;eventDate&quot;) &lt;= pairDF(&quot;endDate&quot;))\n.groupBy(&quot;dim1&quot;, &quot;dim2&quot;, &quot;dim3&quot;, &quot;eventDate&quot;, &quot;startDate&quot;, &quot;endDate&quot;)\n.agg(sum(&quot;value&quot;) as &quot;sum_value&quot;)\n \ninstances.write.partitionBy(&quot;endDate&quot;, &quot;startDate&quot;).parquet(rootPath)\n</code></pre><p>那么问题来了，这两份代码到底差在哪里，是什么导致它们的执行性能差别如此之大。我们不妨先来回顾第一种实现方式，嗅一嗅这里面有哪些不好的代码味道。</p><p>我们都知道，触发Spark延迟计算的Actions算子主要有两类：一类是将分布式计算结果直接落盘的操作，如DataFrame的write、RDD的saveAsTextFile等；另一类是将分布式结果收集到Driver端的操作，如first、take、collect。</p><p>显然，对于第二类算子来说，Driver有可能形成单点瓶颈，尤其是用collect算子去全量收集较大的结果集时，更容易出现性能问题。因此，在第一种实现方式中，我们很容易就能嗅到collect这里的调用，味道很差。</p><p>尽管collect这里味道不好，但在我们的场景里，pairDF毕竟是一份很小的数据集，才几百条数据记录而已，全量搜集到Driver端也不是什么大问题。</p><p>最要命的是collect后面的foreach。要知道，factDF是一份庞大的分布式数据集，尽管createInstance的逻辑仅仅是对factDF进行过滤、汇总并落盘，但是createInstance函数在foreach中会被调用几百次，pairDF中有多少个时间对，createInstance就会被调用多少次。对于Spark中的DAG来说，在没有缓存的情况下，每一次Action的触发都会导致整条DAG从头到尾重新执行。</p><p>明白了这一点之后，我们再来仔细观察这份代码，你品、你细品，目不转睛地盯着foreach和createInstance中的factDF，你会惊讶地发现：有着上千万行数据的factDF被反复扫描了几百次！而且，是全量扫描哟！吓不吓人？可不可怕？这么分析下来，ETL作业端到端执行效率低下的始作俑者，是不是就暴露无遗了？</p><p>反观第二份代码，factDF和pairDF用pairDF.startDate &lt; factDF.eventDate &lt;= pairDF.endDate的不等式条件进行数据关联。在Spark中，不等式Join的实现方式是Nested Loop Join。尽管Nested Loop Join是所有Join实现方式（Merge Join，Hash Join，Broadcast Join等）中性能最差的一种，而且这种Join方式没有任何优化空间，但factDF与pairDF的数据关联只需要扫描一次全量数据，仅这一项优势在执行效率上就可以吊打第一份代码实现。</p><h2>小结</h2><p>今天，我们分析了两个案例，这两个案例都来自数据应用的ETL场景。第一个案例讲的是，在函数被频繁调用的情况下，函数里面一个简单变量所引入的性能开销被成倍地放大。第二个例子讲的是，不恰当的实现方式导致海量数据被反复地扫描成百上千次。</p><p>通过对这两个案例进行分析和探讨，我们发现，对于Spark的应用开发，绝不仅仅是完成业务功能实现就高枕无忧了。<strong>Spark天生的执行效率再高，也需要你针对具体的应用场景和运行环境进行性能调优</strong>。</p><p>而性能调优的收益显而易见：一来可以节约成本，尤其是按需付费的云上成本，更短的执行时间意味着更少的花销；二来可以提升开发的迭代效率，尤其是对于从事数据分析、数据科学、机器学习的同学来说，更高的执行效率可以更快地获取数据洞察，更快地找到模型收敛的最优解。因此你看，性能调优不是一件锦上添花的事情，而是开发者必须要掌握的一项傍身技能。</p><p>那么，对于Spark的性能调优，你准备好了吗？生活不止眼前的苟且，让我们来一场说走就走的性能调优之旅吧。来吧！快上车！扶稳坐好，系好安全带，咱们准备发车了！</p><h2>每日一练</h2><ol>\n<li>日常工作中，你还遇到过哪些功能实现一致、但性能大相径庭的案例吗？</li>\n<li>我们今天讲的第二个案例中的正例代码，你觉得还有可能进一步优化吗？</li>\n</ol><p>期待在留言区看到你分享，也欢迎把你对开发案例的思考写下来，我们下节课见！</p>","neighbors":{"left":{"article_title":"开篇词 | Spark性能调优，你该掌握这些“套路”","id":352065},"right":{"article_title":"02 | 性能调优的本质：调优的手段五花八门，该从哪里入手？","id":352577}},"comments":[{"had_liked":false,"id":283519,"user_name":"Will","can_delete":false,"product_type":"c1","uid":1256770,"ip_address":"","ucode":"671FFBDF1A3E69","user_header":"https://static001.geekbang.org/account/avatar/00/13/2d/42/6e3be953.jpg","comment_is_top":false,"comment_ctime":1615806338,"is_pvip":false,"replies":[{"id":"102926","content":"没错，Broadcast joins可以进一步提升性能。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1615871880,"ip_address":"","comment_id":283519,"utype":1}],"discussion_count":5,"race_medal":0,"score":"66040315778","product_id":100073401,"comment_content":"第二个例子，可以利用map join，让小数据分发到每个worker上，这样不用shuffle数据","like_count":15,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517061,"discussion_content":"没错，Broadcast joins可以进一步提升性能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615871880,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2485585,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","nickname":"Geek_d794f8","note":"","ucode":"1E20DA4FF8B800","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361377,"discussion_content":"广播join默认是开启的吧","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616656103,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2485585,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","nickname":"Geek_d794f8","note":"","ucode":"1E20DA4FF8B800","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362527,"discussion_content":"需要配合相关的配置项哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616979523,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":361377,"ip_address":""},"score":362527,"extra":""}]},{"author":{"id":1269727,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq6dppX1EPFeNOiaYOEClwPrO71zlLoprY0YffxSIk7773sjo4CE5juo0ppIzgfJ7Y5eUbwYl1SkNg/132","nickname":"xujunwei","note":"","ucode":"E129638FD1481F","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":412184,"discussion_content":"PairDF 表比较小的时候会走Broadcast nested loop join 吗？，只是做单纯的做Broadcast join是不是不能提升效率，我觉得PairDF得转换成Hash Join 把日期展开把所有日期展开成（日期，List[(开始日期，结束日期)]）形式 做 等值join 可以进一步优化效率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636096320,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":403251,"discussion_content":"棒棒棒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634039944,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286138,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1617165761,"is_pvip":false,"replies":[{"id":"103886","content":"nlj是一种join实现方式哈，和hash join、sort merge join一样，是一种join实现机制。cartesian join是一种join形式，和inner、left、right对等。每一种join形式，都可以用多种实现机制来做、来实现～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617189962,"ip_address":"","comment_id":286138,"utype":1}],"discussion_count":4,"race_medal":0,"score":"61746707905","product_id":100073401,"comment_content":"请问磊哥，spark里面nested loop join和cartesian product jion有什么区别？","like_count":14,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517880,"discussion_content":"nlj是一种join实现方式哈，和hash join、sort merge join一样，是一种join实现机制。cartesian join是一种join形式，和inner、left、right对等。每一种join形式，都可以用多种实现机制来做、来实现～","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617189962,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1357290,"avatar":"https://static001.geekbang.org/account/avatar/00/14/b5/ea/e7d78a65.jpg","nickname":"不犹豫～不后悔","note":"","ucode":"CB9AABBC403BB1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":577034,"discussion_content":"多种join(left、right、inner、catesian)，每种join都有多种实现机制(nest、hash、sort merge)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1655896160,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":403254,"discussion_content":"简洁精辟，我要去查一下这几种Join的实现机制了，还没有学习过，加油","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634040337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1187106,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","nickname":"Fendora范东_","note":"","ucode":"63EE9DBEE08D70","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363562,"discussion_content":"join形式对应多种join实现方式，👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617237774,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290900,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1619842707,"is_pvip":false,"replies":[{"id":"105410","content":"赞👍，满分💯，无可挑剔~<br><br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619860714,"ip_address":"","comment_id":290900,"utype":1}],"discussion_count":2,"race_medal":0,"score":"48864482963","product_id":100073401,"comment_content":"1. 其实有很多，比如用foreach算子将数据写入到外部数据库，导致每条数据的写入都会建立连接，另外单条写入也比批量写入的性能差很多。建议使用foreachPartition()，每个分区建立一个连接，同时可以批量写入，性能会好很多。<br>2. 一般来讲，小表与大表的关联操作，首先要考虑Broadcast Join<br><br><br>另外，关于Nested Loop Join的原理：https:&#47;&#47;www.geeksforgeeks.org&#47;join-algorithms-in-database&#47;amp&#47;<br>","like_count":11,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519315,"discussion_content":"赞👍，满分💯，无可挑剔~\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619860714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1476427,"avatar":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","nickname":"tiankonghewo","note":"","ucode":"7A55A9C17DD9DF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532021,"discussion_content":"关于Nested Loop Join的原理,资料很赞,学习了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637501656,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283804,"user_name":"TaoInsight","can_delete":false,"product_type":"c1","uid":1318272,"ip_address":"","ucode":"81A31DD9630FEC","user_header":"https://static001.geekbang.org/account/avatar/00/14/1d/80/59f1886d.jpg","comment_is_top":false,"comment_ctime":1615943554,"is_pvip":false,"replies":[{"id":"102969","content":"这块能展开说说吗？具体怎么转化为等值join？可以举个例子哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1615958654,"ip_address":"","comment_id":283804,"utype":1}],"discussion_count":6,"race_medal":0,"score":"35975681922","product_id":100073401,"comment_content":"如果pai rDF的startDate和endDate范围有限，可以把日期范围展开，将非等值join转成等值join","like_count":8,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517152,"discussion_content":"这块能展开说说吗？具体怎么转化为等值join？可以举个例子哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615958654,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1318272,"avatar":"https://static001.geekbang.org/account/avatar/00/14/1d/80/59f1886d.jpg","nickname":"TaoInsight","note":"","ucode":"81A31DD9630FEC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358318,"discussion_content":"以文中的例子，如果pairDF的一条记录是(startDate, endDate) = (&#34;2021-01-01&#34;, &#34;2021-01-31&#34;)，explode成31条，(startDate, endDate, eventDate) = (&#34;2021-01-01&#34;, &#34;2021-01-31&#34;), &#34;2021-01-01&#34;),(&#34;2021-01-01&#34;, &#34;2021-01-31&#34;, &#34;2021-01-02&#34;), ... ,(&#34;2021-01-01&#34;, &#34;2021-01-31&#34;, &#34;2021-01-31&#34;), 这样就可以基于eventDate字段进行等值关联。\n数据会有N倍的膨胀，不过只要N不太大，也可以这么搞","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1615964176,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1029952,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b7/40/979af7cc.jpg","nickname":"敬彦辉","note":"","ucode":"99C18D514673FE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1318272,"avatar":"https://static001.geekbang.org/account/avatar/00/14/1d/80/59f1886d.jpg","nickname":"TaoInsight","note":"","ucode":"81A31DD9630FEC","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":379522,"discussion_content":"这个思路很给力哈！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623939308,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":358318,"ip_address":""},"score":379522,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358805,"discussion_content":"非常好的思路！这个是我之前没有想到的，赞~ N其实还好，pairDF就算膨胀个100倍，也能塞进广播变量，用Broadcast joins做等值join，应该是这个场景最佳的解决方案了。赞！👍 后面我会整理大家的回答，把一些好的思路整理进去，你的这个想法肯定算一个~","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1616053094,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1030156,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7BDPIqkNsYm2ibVUkhIAuniaBOcRIiaroNsKT7YasCRGkVxbba5gsZccBRkGs1rlXRJScvDMebqcSGw/132","nickname":"不算帅","note":"","ucode":"1907C992E4DF30","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558187,"discussion_content":"等值关联和非等值关联性能上有什么区别吗？\n是否像MySQL那样存在索引一说？\n按照我个人理解，这两种不都是全表扫描吗，从执行量上，非等值表达式只是比等值表达式多了一条执行语句的性能消耗。相比较楼主的方案还有数据量膨胀问题呢。\n\n是不是可以这么解释，等值JOIN操作实际上是将小表变成Map数据结构，广播到其他executors，等值查询是近似O(1)的操作。\n非等值查询因为需要传入比较参数，无法映射成Map,时间复杂度是O(小N)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648123664,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358806,"discussion_content":"多多参与后续课程的讨论哈~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616053130,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":298047,"user_name":"慢慢卢","can_delete":false,"product_type":"c1","uid":1329566,"ip_address":"","ucode":"853D399100D83B","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","comment_is_top":false,"comment_ctime":1623896604,"is_pvip":false,"replies":[{"id":"108256","content":"空task~ 其实这个好理解，比如你对原始数据集做过滤，原来的数据有1万条，过滤之后1条，但是filter会继承之前的partitioner，也就是分区和之前是一样的，但其实很多分区中都没有数据，也就是空task。你这个例子也一样，并行度是200，实际上就是reducer的partitioner会划分出200的分区，partitioner是固定的，分区是一定要划出来的，但是实际上数据只有一条，其他的都是空task，白白浪费调度资源。<br><br>这也是为什么Coalesce之后，要做filter，以及为什么AQE要做自动分区合并，道理其实都一样，都是为了避免空task白白浪费宝贵的CPU资源。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624009734,"ip_address":"","comment_id":298047,"utype":1}],"discussion_count":3,"race_medal":0,"score":"18803765788","product_id":100073401,"comment_content":"老师，我把第二个例子自己试了一遍，有个问题不理解：两个df都只有一条数据，sparkui上第二个stage有200个task，为什么shuffle之后的stage的task有200个，虽然说shuffle之后reduce默认并行度是200，但我只有一条数据，实际上只需要一个task啊，其他的task是怎么产生的？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521999,"discussion_content":"空task~ 其实这个好理解，比如你对原始数据集做过滤，原来的数据有1万条，过滤之后1条，但是filter会继承之前的partitioner，也就是分区和之前是一样的，但其实很多分区中都没有数据，也就是空task。你这个例子也一样，并行度是200，实际上就是reducer的partitioner会划分出200的分区，partitioner是固定的，分区是一定要划出来的，但是实际上数据只有一条，其他的都是空task，白白浪费调度资源。\n\n这也是为什么Coalesce之后，要做filter，以及为什么AQE要做自动分区合并，道理其实都一样，都是为了避免空task白白浪费宝贵的CPU资源。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624009734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387773,"discussion_content":"理解通常我们在filter之后要进行coalesce缩减分区,老师说coalesce之后做filter提前收缩资源在处理数据会不会也不太合理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628408027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404356,"discussion_content":"说的对，我这里写反了，应该是先filter，后coalesce，估计当时脑子懵，写反了，哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634292674,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":387773,"ip_address":""},"score":404356,"extra":""}]}]},{"had_liked":false,"id":283605,"user_name":"葛聂","can_delete":false,"product_type":"c1","uid":1008349,"ip_address":"","ucode":"2771A5888FCBE0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/6uQj0EmXTGstVxNicLWqJhGvY0tavWvxK5RwgxjaMK1rNB9Rf7kAdnzBnG7YOicHjTibOf6G6HEFwonRFXDN3Pp8A/132","comment_is_top":false,"comment_ctime":1615855078,"is_pvip":false,"replies":[{"id":"102925","content":"好问题，其实改动非常小，开销相比正例也不大，但这里的关键在于，这个函数会被反反复复调用上百次，累积下来，开销就上去了。所以，关键不在于点小不小，而是这个点，是不是瓶颈。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1615871596,"ip_address":"","comment_id":283605,"utype":1}],"discussion_count":3,"race_medal":0,"score":"18795724262","product_id":100073401,"comment_content":"Case 1为什么性能差一倍呢","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517090,"discussion_content":"好问题，其实改动非常小，开销相比正例也不大，但这里的关键在于，这个函数会被反反复复调用上百次，累积下来，开销就上去了。所以，关键不在于点小不小，而是这个点，是不是瓶颈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615871596,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2010843,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/cojb2AA3eM620kb7hj7YoG8k56TKsdCmVletmYKYwibickH5Ced8UyxicpY9icZEM2ZTcqyUaEk2PRmH1FVLtGTggw/132","nickname":"orangelin","note":"","ucode":"730CC9997C16F2","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360138,"discussion_content":"我这里的理解是 pairDF实际上只产生了一个job，但是他的遍历里调用了create函数，而该函数又调用了factDF，在没有做缓存的情况下，parDF的每条数据对于DAG来说都需要重新获取factDF,假设pariDF有1000条记录，那么每一条记录都需要一个job来生成factDF？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616377959,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2010843,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/cojb2AA3eM620kb7hj7YoG8k56TKsdCmVletmYKYwibickH5Ced8UyxicpY9icZEM2ZTcqyUaEk2PRmH1FVLtGTggw/132","nickname":"orangelin","note":"","ucode":"730CC9997C16F2","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":362531,"discussion_content":"没错，观察的很仔细，第一版的实现就会像你说的，每条都trigger一次action","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616979646,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":360138,"ip_address":""},"score":362531,"extra":""}]}]},{"had_liked":false,"id":284697,"user_name":"Elon","can_delete":false,"product_type":"c1","uid":1004727,"ip_address":"","ucode":"DB5BE4DA9A1690","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/b7/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1616419708,"is_pvip":false,"replies":[{"id":"103286","content":"是的，你说的没错。函数的副作用指的是对外部变量、外部环境的影响，内部状态的改变和转换不算。文中这块表述的不严谨哈，这里主要是想强调可变变量fields带来的计算开销。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616456083,"ip_address":"","comment_id":284697,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14501321596","product_id":100073401,"comment_content":"函数式的副作用指的是不修改入参吧？在函数内部是可以定义变量、修改变量的。因此fields变量在函数内部，应该不算副作用吧？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517435,"discussion_content":"是的，你说的没错。函数的副作用指的是对外部变量、外部环境的影响，内部状态的改变和转换不算。文中这块表述的不严谨哈，这里主要是想强调可变变量fields带来的计算开销。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616456083,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284021,"user_name":"fsc2016","can_delete":false,"product_type":"c1","uid":1255585,"ip_address":"","ucode":"5480F05703A974","user_header":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","comment_is_top":false,"comment_ctime":1616038643,"is_pvip":false,"replies":[{"id":"103038","content":"可以，没问题，接触过Spark就行。放心吧，原理部分会有大量的生活化类比和故事，尽可能地让你“边玩边学”。另外，咱们有微信群，有问题可以随时探讨～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616051117,"ip_address":"","comment_id":284021,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14500940531","product_id":100073401,"comment_content":"请问老师，这个课程需要哪些基础，我平时使用过pysaprk 做过一些机器学习相关数据处理练习，对于我这种使用spark不多的，可以消化吸收嘛","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517205,"discussion_content":"可以，没问题，接触过Spark就行。放心吧，原理部分会有大量的生活化类比和故事，尽可能地让你“边玩边学”。另外，咱们有微信群，有问题可以随时探讨～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616051117,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":315401,"user_name":"浩然","can_delete":false,"product_type":"c1","uid":1805991,"ip_address":"","ucode":"1F2FC08382D396","user_header":"https://static001.geekbang.org/account/avatar/00/1b/8e/a7/92d1b90e.jpg","comment_is_top":false,"comment_ctime":1633881616,"is_pvip":false,"replies":[{"id":"114409","content":"精辟！满分💯，一针见血","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633961985,"ip_address":"","comment_id":315401,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10223816208","product_id":100073401,"comment_content":"简单啊。那个时间区间的，罗列出来，广播一下就完事了。从nest loop到hash join的跨越。<br>我之前做Oracle优化的，所以第一反应是哈希join，第二反应是不等值到等值。","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527953,"discussion_content":"精辟！满分💯，一针见血","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633961985,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283544,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1615813246,"is_pvip":false,"replies":[{"id":"102928","content":"广播的思路很赞。不过二分查找这里值得商榷哈，咱们目的是过滤出满足条件的event date，然后和其他维度一起、分组聚合。这里关键不在于过滤和查找效率，关键在于大表的重复扫描，只要解决这个核心痛点，性能问题就迎刃而解。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1615873831,"ip_address":"","comment_id":283544,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10205747838","product_id":100073401,"comment_content":"可以先将pairdf collect到driver，再将数组按照startdate排序，然后再将其广播。然后在factdf.map里面实现一个方法来从广播的数组里面二分查找到eventdate所属的时间对子。最后就可以根据这个时间对子以及其他的维度属性进行分组聚合了","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517068,"discussion_content":"广播的思路很赞。不过二分查找这里值得商榷哈，咱们目的是过滤出满足条件的event date，然后和其他维度一起、分组聚合。这里关键不在于过滤和查找效率，关键在于大表的重复扫描，只要解决这个核心痛点，性能问题就迎刃而解。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615873831,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1184678,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJDgV2qia6eAL7Fb4egX3odViclRRwOlkfCBrjhU9lLeib90KGkIDjdddSibNVs47N90L36Brgnr6ppiag/132","nickname":"ddww","note":"","ucode":"2871112FC9B3F7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390486,"discussion_content":"我觉得老铁，没毛病，将小表，第一步:加入到广播变量。第二步:之后再提供一个查找算法，这不就进行了过滤操作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629860027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283526,"user_name":"Geek_92df49","can_delete":false,"product_type":"c1","uid":2284073,"ip_address":"","ucode":"F8AC23800BC89C","user_header":"","comment_is_top":false,"comment_ctime":1615807598,"is_pvip":false,"replies":[{"id":"102929","content":"兄弟我是作者哈，你说的没错，分组只需要前4个字段，但是你看最后，instances.write.partitionBy(&quot;endDate&quot;, &quot;startDate&quot;).parquet(rootPath)，需要用开始和结束时间这两个字段去做分区存储，因此，在前一步分组的时候，把这两个字段保留了下来。","user_name":"编辑回复","user_name_real":"王莹","uid":"1743279","ctime":1615875809,"ip_address":"","comment_id":283526,"utype":2}],"discussion_count":10,"race_medal":0,"score":"10205742190","product_id":100073401,"comment_content":"四个维度分组为什么要加上开始时间和结束时间？<br>.groupBy(&quot;dim1&quot;, &quot;dim2&quot;, &quot;dim3&quot;, &quot;event_date&quot;, &quot;startDate&quot;, &quot;endDate&quot;)","like_count":2,"discussions":[{"author":{"id":1743279,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/99/af/e4cc7374.jpg","nickname":"啊呜","note":"","ucode":"76E24AB808868D","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517064,"discussion_content":"兄弟我是作者哈，你说的没错，分组只需要前4个字段，但是你看最后，instances.write.partitionBy(&amp;quot;endDate&amp;quot;, &amp;quot;startDate&amp;quot;).parquet(rootPath)，需要用开始和结束时间这两个字段去做分区存储，因此，在前一步分组的时候，把这两个字段保留了下来。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615875809,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1460463,"avatar":"https://static001.geekbang.org/account/avatar/00/16/48/ef/4750cb14.jpg","nickname":"🚤","note":"","ucode":"95D7F773789B74","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357828,"discussion_content":"group by不加上 &#34;startDate&#34;, &#34;endDate&#34;,最后写数据的时候不能按照这两个字段分区","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1615876597,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1460463,"avatar":"https://static001.geekbang.org/account/avatar/00/16/48/ef/4750cb14.jpg","nickname":"🚤","note":"","ucode":"95D7F773789B74","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357860,"discussion_content":"正解~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615883821,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":357828,"ip_address":""},"score":357860,"extra":""}]},{"author":{"id":1370659,"avatar":"https://static001.geekbang.org/account/avatar/00/14/ea/23/508f71e3.jpg","nickname":"Jefitar","note":"","ucode":"D7ED9F32ADA5B1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357897,"discussion_content":"老师，课程几天一更啊？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615891684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1370659,"avatar":"https://static001.geekbang.org/account/avatar/00/14/ea/23/508f71e3.jpg","nickname":"Jefitar","note":"","ucode":"D7ED9F32ADA5B1","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358021,"discussion_content":"每周一、三、五更新哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615904610,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":357897,"ip_address":""},"score":358021,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357824,"discussion_content":"兄弟我是作者哈，你说的没错，分组只需要前4个字段，但是你看最后，instances.write.partitionBy(&#34;endDate&#34;, &#34;startDate&#34;).parquet(rootPath)，需要用开始和结束时间这两个字段去做分区存储，因此，在前一步分组的时候，把这两个字段保留了下来。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615875622,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1254213,"avatar":"https://static001.geekbang.org/account/avatar/00/13/23/45/3addffe7.jpg","nickname":"brv","note":"","ucode":"DD94D8C2085EAB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":358149,"discussion_content":"类似于hive的动态分区吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615941130,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":357824,"ip_address":""},"score":358149,"extra":""},{"author":{"id":1159758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/b2/4e/13a993a5.jpg","nickname":"超级达达","note":"","ucode":"153FCE3113AD11","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1254213,"avatar":"https://static001.geekbang.org/account/avatar/00/13/23/45/3addffe7.jpg","nickname":"brv","note":"","ucode":"DD94D8C2085EAB","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358294,"discussion_content":"是的，Spark的partitionBy可以根据具体数据内容去动态创建对应的分区，类似于Hive的动态分区功能。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1615961562,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":358149,"ip_address":""},"score":358294,"extra":""},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1159758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/b2/4e/13a993a5.jpg","nickname":"超级达达","note":"","ucode":"153FCE3113AD11","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358308,"discussion_content":"正解~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615963114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":358294,"ip_address":""},"score":358308,"extra":""}]},{"author":{"id":1179298,"avatar":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","nickname":"对方正在输入。。。","note":"","ucode":"7B0DEB4D9B43D2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357479,"discussion_content":"因为是要在每一条pairDf的数据条目基础之上筛选出factDf的数据聚合","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615811897,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284963,"user_name":"刘吉超","can_delete":false,"product_type":"c1","uid":1111513,"ip_address":"","ucode":"846F8EAB9AFAE9","user_header":"https://static001.geekbang.org/account/avatar/00/10/f5/d9/345d743d.jpg","comment_is_top":false,"comment_ctime":1616563526,"is_pvip":false,"replies":[{"id":"103381","content":"兄弟我是作者哈，第二份代码，我有几个疑问：<br>1. 两个import语句的作用是什么？<br>2. ads具体是什么内容？是RDD，还是数组，还是什么？<br>3. 没有看到哪里定义分布式数据集，所有计算看上去是基于(0 until ads.size())这个List，那么后续所有的计算，map，map里面的toMap，都是在driver计算的，如果你9T数据都在driver计算，那结果。。。<br>4. toMap之后，又加了个map，我理解是为了把value中的null替换为空字符串，如果是这样的话，map里面只处理value就好了，不用带着key<br><br>不知道我理解的对不对哈，期待老弟提供更多信息~","user_name":"编辑回复","user_name_real":"王莹","uid":"1743279","ctime":1616593457,"ip_address":"","comment_id":284963,"utype":2}],"discussion_count":2,"race_medal":0,"score":"5911530822","product_id":100073401,"comment_content":"我们每天有9T数据，用如下代码做ETL  json平铺，花很长时间<br>val adArr = ArrayBuffer[Map[String, String]]()<br>if (ads != null) {<br>  val adnum = ads.length<br>  for (i &lt;- 0 until adnum) {<br>    val addObj = ads.getJSONObject(i)<br>    val adMap = THashMap[String, String]()<br>    addObj.entrySet().foreach(x =&gt; adMap += (x.getKey -&gt; (if(x.getValue==null) &quot;&quot; else x.getValue.toString )  ))<br>    adArr += adMap.toMap<br>  }<br>}<br>基于老师讲的，避免副作用，改为如下代码<br>import org.apache.flink.streaming.api.scala._<br>import scala.collection.JavaConversions._<br> val adArr = (0 until ads.size()).map(i =&gt; ads.getJSONObject(i).toMap.map(entry =&gt; entry._1 -&gt; (if(entry._2==null) &quot;&quot; else entry._2.toString)))<br>尝试后没啥效果，希望老师指导一下","like_count":1,"discussions":[{"author":{"id":1743279,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/99/af/e4cc7374.jpg","nickname":"啊呜","note":"","ucode":"76E24AB808868D","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517527,"discussion_content":"兄弟我是作者哈，第二份代码，我有几个疑问：\n1. 两个import语句的作用是什么？\n2. ads具体是什么内容？是RDD，还是数组，还是什么？\n3. 没有看到哪里定义分布式数据集，所有计算看上去是基于(0 until ads.size())这个List，那么后续所有的计算，map，map里面的toMap，都是在driver计算的，如果你9T数据都在driver计算，那结果。。。\n4. toMap之后，又加了个map，我理解是为了把value中的null替换为空字符串，如果是这样的话，map里面只处理value就好了，不用带着key\n\n不知道我理解的对不对哈，期待老弟提供更多信息~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616593457,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361103,"discussion_content":"另外，我们有个读者微信群：“528*吴磊的 Spark 读者群”，那里讨论更方便，你可以问问运营怎么加入。或者你可以加我好友，搜rJunior或者“方块K”，我把你拉进去，你把更多信息贴给我，咱们一起看看瓶颈在哪儿，然后再看怎么优化。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616593626,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283889,"user_name":"LJK","can_delete":false,"product_type":"c1","uid":1199213,"ip_address":"","ucode":"12B2441099FF1D","user_header":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","comment_is_top":false,"comment_ctime":1615973157,"is_pvip":false,"replies":[{"id":"103037","content":"不一定的，action个数不是关键，关键是数据的访问效率。关于提升数据访问效率，咱们专栏后面的内容会讲哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616050952,"ip_address":"","comment_id":283889,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5910940453","product_id":100073401,"comment_content":"同一个application如果action多的话一定会影响效率吗？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517172,"discussion_content":"不一定的，action个数不是关键，关键是数据的访问效率。关于提升数据访问效率，咱们专栏后面的内容会讲哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616050952,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":360762,"user_name":"阿奎","can_delete":false,"product_type":"c1","uid":1048598,"ip_address":"广东","ucode":"2EAF87513224C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/00/16/bedf7932.jpg","comment_is_top":false,"comment_ctime":1666834074,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1666834074","product_id":100073401,"comment_content":"老师您好，我最近遇到一个奇怪的问题。我直接登录终端去执行spark submit任务可以执行成功，我在项目里是模仿的小海豚调度器在程序里去拼接的spark submit脚本然后生成的shell脚本，通过sudo -u  hadoop集群的租户  sh command去执行的，这种方式提交的任务如果任务数据量小可以执行成功，任务数据量大就会失败，提示applicationmaster 接受到了kill的信号","like_count":0},{"had_liked":false,"id":355389,"user_name":"唐方刚","can_delete":false,"product_type":"c1","uid":3046392,"ip_address":"上海","ucode":"93DA58C3DCCF1B","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ugib9sF9icd9dhibQoAA025hibbD5zgZTiaddLoeEH457hrkBBhtQK6qknTWt270rHCtBZqeqsbibtHghgjdkPx3DyIw/132","comment_is_top":false,"comment_ctime":1661330154,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1661330154","product_id":100073401,"comment_content":"弱弱的问个问题，val extractFields: Seq[Row] =&gt; Seq[(String, Int)] = {}，一般定义方法，冒号后面不是返回值么，怎么这里冒号后面参数和返回值都有","like_count":0,"discussions":[{"author":{"id":2775166,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/58/7e/1d5fadd4.jpg","nickname":"object","note":"","ucode":"F64CF320C1DA12","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585975,"discussion_content":"scala的语法，通俗来讲就把一个函数赋值成一个变量了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661922669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"吉林"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":349113,"user_name":"组织灵魂 王子健","can_delete":false,"product_type":"c1","uid":2957218,"ip_address":"","ucode":"94DB462CEAFAD3","user_header":"https://static001.geekbang.org/account/avatar/00/2d/1f/a2/0ac2dc38.jpg","comment_is_top":false,"comment_ctime":1655736251,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1655736251","product_id":100073401,"comment_content":"看到一个数据量大另一个数据量小，立刻马上就会想到广播变量。还有优化部分write用到了partitionBy这个细节必须注意","like_count":0},{"had_liked":false,"id":345108,"user_name":"Geek_32772e","can_delete":false,"product_type":"c1","uid":1446158,"ip_address":"","ucode":"2850EF64F4775F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoribOUgcicu1sOqZZVtPqpSDSS43vicxW0GesxQeBRjUC47CzulKSzYNj2aMg9YOZDdjPdAZxS3jNcQ/132","comment_is_top":false,"comment_ctime":1652028797,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1652028797","product_id":100073401,"comment_content":"这两个例子直接用sparkSQL不是更简单吗","like_count":0},{"had_liked":false,"id":342167,"user_name":"Spoon","can_delete":false,"product_type":"c1","uid":1959822,"ip_address":"","ucode":"2FF9193AD482C2","user_header":"https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg","comment_is_top":false,"comment_ctime":1650079810,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1650079810","product_id":100073401,"comment_content":"使用Boradcast NLJ这样可以节省Shuffle带来的开销~","like_count":0},{"had_liked":false,"id":323754,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1638152235,"is_pvip":false,"replies":[{"id":"117605","content":"老弟别着急~ <br><br>先把NLJ的算法复杂度忘掉，先不用管用什么Join实现机制，先关注下面这段代码：<br>&#47;&#47;实现方案1 —— 反例def createInstance(factDF: DataFrame, startDate: String, endDate: String): DataFrame = {val instanceDF = factDF.filter(col(&quot;eventDate&quot;) &gt; lit(startDate) &amp;&amp; col(&quot;eventDate&quot;) &lt;= lit(endDate)).groupBy(&quot;dim1&quot;, &quot;dim2&quot;, &quot;dim3&quot;, &quot;event_date&quot;).agg(sum(&quot;value&quot;) as &quot;sum_value&quot;)instanceDF}<br><br>factDF 是一份庞大的分布式数据集，尽管 createInstance 的逻辑仅仅是对 factDF 进行过滤、汇总并落盘，但是 createInstance 函数在 foreach 中会被调用几百次，pairDF 中有多少个时间对，createInstance 就会被调用多少次。对于 Spark 中的 DAG 来说，在没有缓存的情况下，每一次 Action 的触发都会导致整条 DAG 从头到尾重新执行。明白了这一点之后，我们再来仔细观察这份代码，盯着 foreach 和 createInstance 中的 factDF，你会惊讶地发现：有着上千万行数据的 factDF 被反复扫描了几百次！<br><br>性能差的关键在于：“有着上千万行数据的 factDF 被反复扫描了几百次！”，也就是说，同一份超大的分布式数据集，被磁盘I&#47;O了上百次。而正例代码中，这份数据集只会被I&#47;O一次。仅这一项区别，执行性能就高下立现了。","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1638284595,"ip_address":"","comment_id":323754,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1638152235","product_id":100073401,"comment_content":"老师我把所有课程看了一遍还是不太理解为什么例2 正例的性能更高（捂脸）；如果spark不能根据日期范围对fairDF做过滤提前去掉一部分的话 不管是fairDF left join factDF还是factDF left join fairDF还是反例的代码，感觉时间复杂度都是 size(fairDF)*size(factDF) 啊。。。虽然数据是分布在不同节点中但是NLJ的算法不是对于factDF中每条记录都遍历一遍fairDF吗？希望老师能解答一下嘤嘤嘤","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":534840,"discussion_content":"老弟别着急~ \n\n先把NLJ的算法复杂度忘掉，先不用管用什么Join实现机制，先关注下面这段代码：\n//实现方案1 —— 反例def createInstance(factDF: DataFrame, startDate: String, endDate: String): DataFrame = {val instanceDF = factDF.filter(col(&#34;eventDate&#34;) &gt; lit(startDate) &amp;&amp; col(&#34;eventDate&#34;) &lt;= lit(endDate)).groupBy(&#34;dim1&#34;, &#34;dim2&#34;, &#34;dim3&#34;, &#34;event_date&#34;).agg(sum(&#34;value&#34;) as &#34;sum_value&#34;)instanceDF}\n\nfactDF 是一份庞大的分布式数据集，尽管 createInstance 的逻辑仅仅是对 factDF 进行过滤、汇总并落盘，但是 createInstance 函数在 foreach 中会被调用几百次，pairDF 中有多少个时间对，createInstance 就会被调用多少次。对于 Spark 中的 DAG 来说，在没有缓存的情况下，每一次 Action 的触发都会导致整条 DAG 从头到尾重新执行。明白了这一点之后，我们再来仔细观察这份代码，盯着 foreach 和 createInstance 中的 factDF，你会惊讶地发现：有着上千万行数据的 factDF 被反复扫描了几百次！\n\n性能差的关键在于：“有着上千万行数据的 factDF 被反复扫描了几百次！”，也就是说，同一份超大的分布式数据集，被磁盘I/O了上百次。而正例代码中，这份数据集只会被I/O一次。仅这一项区别，执行性能就高下立现了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638284595,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536321,"discussion_content":"但是为什么正例中factDF只被I/O一次呢...","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638758644,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322622,"user_name":"tiankonghewo","can_delete":false,"product_type":"c1","uid":1476427,"ip_address":"","ucode":"7A55A9C17DD9DF","user_header":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","comment_is_top":false,"comment_ctime":1637503558,"is_pvip":false,"replies":[{"id":"117334","content":"老弟这是典型的“单机思维”哈~ 可以花些时间了解一下课程后面的内容~<br>这里说的扫描一次，指的是驱动表factDF，你可以先不考虑NLJ算法上的执行过程，而从分布式的角度，去考虑下哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637897949,"ip_address":"","comment_id":322622,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1637503558","product_id":100073401,"comment_content":"&quot;尽管 Nested Loop Join 是所有 Join 实现方式（Merge Join，Hash Join，Broadcast Join 等）中性能最差的一种，而且这种 Join 方式没有任何优化空间，但 factDF 与 pairDF 的数据关联只需要扫描一次全量数据&quot;, <br>对于这一段话,感觉解释的不好, Nested Loop Join 是两个for循环, 时间复杂度应该也是M*N, 如果factDF只需要扫码一遍全量数据,那么pairDF需要扫描的次数也不会少,<br>两个for循环,时间复杂度怎么都是M*N, 那么节约下来的时间,相比第一种foreach方式,应该是从磁盘中拉取的时间<br><br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533545,"discussion_content":"老弟这是典型的“单机思维”哈~ 可以花些时间了解一下课程后面的内容~\n这里说的扫描一次，指的是驱动表factDF，你可以先不考虑NLJ算法上的执行过程，而从分布式的角度，去考虑下哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637897949,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537697,"discussion_content":"这里说的扫描一次，指的是驱动表factD //   这句话能不能再通俗解析一下   ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639146041,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322610,"user_name":"tiankonghewo","can_delete":false,"product_type":"c1","uid":1476427,"ip_address":"","ucode":"7A55A9C17DD9DF","user_header":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","comment_is_top":false,"comment_ctime":1637498551,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637498551","product_id":100073401,"comment_content":"第二个例子,学习了","like_count":0},{"had_liked":false,"id":322023,"user_name":"萝莉巴索小布丁","can_delete":false,"product_type":"c1","uid":1221146,"ip_address":"","ucode":"00C285326B8C77","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/1a/f2c8988b.jpg","comment_is_top":false,"comment_ctime":1637139900,"is_pvip":false,"replies":[{"id":"116958","content":"老弟这是典型的“单机思维”哈~ 可以花些时间了解一下课程后面的内容~<br>这里说的扫描一次，指的是驱动表factDF，你可以先不考虑NLJ算法上的执行过程，而从分布式的角度，去考虑下哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637162403,"ip_address":"","comment_id":322023,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637139900","product_id":100073401,"comment_content":"老师，我看了下nested loop join的原理，查找次数应该为外层循环的次数呀，为什么你说只全表扫描一次呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530894,"discussion_content":"老弟这是典型的“单机思维”哈~ 可以花些时间了解一下课程后面的内容~\n这里说的扫描一次，指的是驱动表factDF，你可以先不考虑NLJ算法上的执行过程，而从分布式的角度，去考虑下哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637162403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":315319,"user_name":"雁归来🍁","can_delete":false,"product_type":"c1","uid":1698742,"ip_address":"","ucode":"FC3A7ABC05C27D","user_header":"https://static001.geekbang.org/account/avatar/00/19/eb/b6/2069455f.jpg","comment_is_top":false,"comment_ctime":1633830340,"is_pvip":false,"replies":[{"id":"114519","content":"我们一个个来说~<br><br>1. 半个小时是否正常，其实要看你集群规模的。不过像你所说，同等规模下，impala、hive比Spark快了一个数量级，这个说实话我挺困惑。一种猜测是，impala、hive用的是broadcast join，而Spark用的shuffle join。你可以考虑把50万、几万的数据表，都强制广播，然后用broadcast join去实现left join，然后再对比执行时间<br><br>2. Spark SQL确实不太适合大表Join大表的场景，这个我们在后面的28、29讲给出一些大表Join大表的tips和最佳实践，老弟不妨参考下~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634136834,"ip_address":"","comment_id":315319,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1633830340","product_id":100073401,"comment_content":"我有2个问题想请教一下老师：<br>第一个：我们现在生产用sparkSQL去做SQL分析，一张4千万数据左右的表去left join 一张50万的表和几张几万数据的表，要花费半个小时，这个正常吗？然后把得到的近800万的数据写入TIDB又花费半个小时，这个是不是正常的时效啊？，但是用MPP数据库如impala,hive等几分钟就全搞定了。<br>第二个：有5张700万左右数据的表做left join的话sparkSQL经常会OOM，然后调大内存后要跑1个小时才能出结果，是不是sparkSQL在大表关联大表方面性能很差啊？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527932,"discussion_content":"我们一个个来说~\n\n1. 半个小时是否正常，其实要看你集群规模的。不过像你所说，同等规模下，impala、hive比Spark快了一个数量级，这个说实话我挺困惑。一种猜测是，impala、hive用的是broadcast join，而Spark用的shuffle join。你可以考虑把50万、几万的数据表，都强制广播，然后用broadcast join去实现left join，然后再对比执行时间\n\n2. Spark SQL确实不太适合大表Join大表的场景，这个我们在后面的28、29讲给出一些大表Join大表的tips和最佳实践，老弟不妨参考下~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634136834,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":315225,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1633755604,"is_pvip":false,"replies":[{"id":"114420","content":"正反例其实都有Shuffle，这里的对比，其实和Shuffle无关哈~<br><br>这里的症结，其实是“单机思维”带来的问题，也就是像写单机代码那样，去实现分布式应用，从而导致同一份数据集被反反复复地扫描，从而导致执行效率低下~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633964078,"ip_address":"","comment_id":315225,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1633755604","product_id":100073401,"comment_content":"老师，案例二，正例中用的join，会做shuffle ,反例是做遍历，大数据下，会不会不做shuffle 更好？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527907,"discussion_content":"正反例其实都有Shuffle，这里的对比，其实和Shuffle无关哈~\n\n这里的症结，其实是“单机思维”带来的问题，也就是像写单机代码那样，去实现分布式应用，从而导致同一份数据集被反反复复地扫描，从而导致执行效率低下~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633964078,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":305343,"user_name":"边边爱学习","can_delete":false,"product_type":"c1","uid":2126965,"ip_address":"","ucode":"3B091043B0E1EC","user_header":"","comment_is_top":false,"comment_ctime":1627912587,"is_pvip":false,"replies":[{"id":"110612","content":"问题没看懂，哈哈，能具体说说嘛？<br><br>我的理解是，1）子查询那个SQL，外表table1是全表扫描，但是SQL执行下来没有非常慢？<br>2）inner join的SQL，外表table1是部分扫描，然后呢？<br><br>不知道我理解的对不对哈，所以问题是啥呢？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1628146640,"ip_address":"","comment_id":305343,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1627912587","product_id":100073401,"comment_content":"(1) select partition_col from table1 where partition_col in (select partition_col from table2 where partition_col between 1 and 10)<br>(2) select partition_col from table1 inner join (select partition_col from table2 where partition_col between 1 and 10) r on table1.partition_col = r.partition_col<br>（1）全表scan，没有非常慢<br>（2）指定分区scan<br>为啥会这样呢","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524357,"discussion_content":"问题没看懂，哈哈，能具体说说嘛？\n\n我的理解是，1）子查询那个SQL，外表table1是全表扫描，但是SQL执行下来没有非常慢？\n2）inner join的SQL，外表table1是部分扫描，然后呢？\n\n不知道我理解的对不对哈，所以问题是啥呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628146640,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2347318,"avatar":"https://static001.geekbang.org/account/avatar/00/23/d1/36/8c015107.jpg","nickname":"bee","note":"","ucode":"43CC23663FB156","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555884,"discussion_content":"老师我觉得他说得是为什么 第一条sql 对于table1 会全表扫描 第二条sql对于table1 来说只是扫描部分分区,  我觉得第二条sql进行了动态分区过滤 所以table1 会只扫描部分分区","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647092145,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284469,"user_name":"方得始终","can_delete":false,"product_type":"c1","uid":1229356,"ip_address":"","ucode":"6FBEDDACC32EA1","user_header":"https://static001.geekbang.org/account/avatar/00/12/c2/2c/900cb4f0.jpg","comment_is_top":false,"comment_ctime":1616286731,"is_pvip":false,"replies":[{"id":"103167","content":"上传到了B站：https:&#47;&#47;www.bilibili.com&#47;video&#47;BV1AX4y1G7Ks&#47;","user_name":"编辑回复","user_name_real":"王莹","uid":"1743279","ctime":1616295724,"ip_address":"","comment_id":284469,"utype":2}],"discussion_count":1,"race_medal":0,"score":"1616286731","product_id":100073401,"comment_content":"错过了老师上星期的直播，请问在哪里可以看到回放吗？","like_count":0,"discussions":[{"author":{"id":1743279,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/99/af/e4cc7374.jpg","nickname":"啊呜","note":"","ucode":"76E24AB808868D","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517360,"discussion_content":"上传到了B站：https://www.bilibili.com/video/BV1AX4y1G7Ks/","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616295724,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284370,"user_name":"方得始终","can_delete":false,"product_type":"c1","uid":1229356,"ip_address":"","ucode":"6FBEDDACC32EA1","user_header":"https://static001.geekbang.org/account/avatar/00/12/c2/2c/900cb4f0.jpg","comment_is_top":false,"comment_ctime":1616216828,"is_pvip":false,"replies":[{"id":"103154","content":"可以的，后面会统一放到git上，方便大家查阅。对，目前专栏里的代码示例都是Scala，主要是简洁，用很短的代码就能示意。如果习惯用pyspark开发，其实不用刻意去学Scala，API其实大同小异，区别不大。pyspark比较熟悉的话，Scala的代码也很容易看懂～ 另外，执行性能pyspark也不差，和Scala，Java相当。除了udf需要跨进程、有一些开销之外，整体还好。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616283901,"ip_address":"","comment_id":284370,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1616216828","product_id":100073401,"comment_content":"代码会集中放到GitHub上面吗？这样方便于以后学习查找。我主要用 pyspark, 这个课程只用Scala吗？请问有关于Scala spark的入门教程吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517322,"discussion_content":"可以的，后面会统一放到git上，方便大家查阅。对，目前专栏里的代码示例都是Scala，主要是简洁，用很短的代码就能示意。如果习惯用pyspark开发，其实不用刻意去学Scala，API其实大同小异，区别不大。pyspark比较熟悉的话，Scala的代码也很容易看懂～ 另外，执行性能pyspark也不差，和Scala，Java相当。除了udf需要跨进程、有一些开销之外，整体还好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616283901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387778,"discussion_content":"想问一下 专栏的这块代码git地址可以发一下吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628408923,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404361,"discussion_content":"https://github.com/wulei-bj-cn/potatoes.git","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634292901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":387778,"ip_address":""},"score":404361,"extra":""}]}]},{"had_liked":false,"id":284004,"user_name":"天渡","can_delete":false,"product_type":"c1","uid":1322829,"ip_address":"","ucode":"B4D031E05E266E","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJrZb9pm07aicqH4rErIanMN5owmsBO6rWa6VSkpFMFjVRKOKkNM6JEf2gvQ2g23xUiammg7PUykJFA/132","comment_is_top":false,"comment_ctime":1616035083,"is_pvip":false,"replies":[{"id":"103036","content":"广播的思路没问题～ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616050825,"ip_address":"","comment_id":284004,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616035083","product_id":100073401,"comment_content":"可否将小表进行broadcast，将reduce端join变为map端join。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517201,"discussion_content":"广播的思路没问题～ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616050825,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283720,"user_name":"Shockang","can_delete":false,"product_type":"c1","uid":1263546,"ip_address":"","ucode":"B871D731F6E6B1","user_header":"https://static001.geekbang.org/account/avatar/00/13/47/ba/d36340c1.jpg","comment_is_top":false,"comment_ctime":1615896430,"is_pvip":false,"replies":[{"id":"102978","content":"case1的副作用本身其实还好，主要是它引入的开销，就是你说的性能损耗。case2的广播思路没问题～ <br><br>coalesce这里有待商榷哈，正例里面已经没有filter，反例里面加coalesce也于事无补。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1615963203,"ip_address":"","comment_id":283720,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1615896430","product_id":100073401,"comment_content":"Case1里面除了老师讲的副作用外，我认为Scala在处理闭包时也会存在一定的性能损耗，Case2里面把大变量广播出去是一种常见的操作，另外，filter之后加上coalesce 也是比较常见的优化手段","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517127,"discussion_content":"case1的副作用本身其实还好，主要是它引入的开销，就是你说的性能损耗。case2的广播思路没问题～ \n\ncoalesce这里有待商榷哈，正例里面已经没有filter，反例里面加coalesce也于事无补。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615963203,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283666,"user_name":"rb@31","can_delete":false,"product_type":"c1","uid":1177418,"ip_address":"","ucode":"F5990804D55D96","user_header":"","comment_is_top":false,"comment_ctime":1615876121,"is_pvip":false,"replies":[{"id":"102935","content":"你好，课程详情页处有更新安排哦，每周一，三，五更新。预计更新到5月份","user_name":"编辑回复","user_name_real":"王莹","uid":"1743279","ctime":1615883214,"ip_address":"","comment_id":283666,"utype":2}],"discussion_count":1,"race_medal":0,"score":"1615876121","product_id":100073401,"comment_content":"另外，不知道老师的更新频率。大概多久能全部更新好？","like_count":0,"discussions":[{"author":{"id":1743279,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/99/af/e4cc7374.jpg","nickname":"啊呜","note":"","ucode":"76E24AB808868D","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517106,"discussion_content":"你好，课程详情页处有更新安排哦，每周一，三，五更新。预计更新到5月份","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615883214,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]}]}