{"id":462524,"title":"23 | 情感分析：如何使用LSTM进行情感分析？","content":"<p>你好，我是方远。</p><p>欢迎来跟我一起学习情感分析，今天我们要讲的就是机器学习里的文本情感分析。文本情感分析又叫做观点提取、主题分析、倾向性分析等。光说概念，你可能会觉得有些抽象，我们一起来看一个生活中的应用，你一看就能明白了。</p><p>比方说我们在购物网站上选购一款商品时，首先会翻阅一下商品评价，看看是否有中差评。这些评论信息表达了人们的各种情感色彩和情感倾向性，如喜、怒、哀、乐和批评、赞扬等。像这样根据评价文本，由计算机自动区分评价属于好评、中评或者说差评，背后用到的技术就是情感分析。</p><p>如果你进一步观察，还会发现，在好评差评的上方还有一些标签，比如“声音大小合适”、“连接速度快”、“售后态度很好”等。这些标签其实也是计算机根据文本，自动提取的主题或者观点。</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/6f/ef69caa72565c50d98b63e20f499ea6f.jpg?wh=2572x2473\" alt=\"\"></p><p>情感分析的快速发展得益于社交媒体的兴起，自2000年初以来，情感分析已经成长为自然语言处理（NLP）中最活跃的研究领域之一，它也被广泛应用在个性化推荐、商业决策、舆情监控等方面。</p><p>今天这节课，我们将完成一个情感分析项目，一起来对影评文本做分析。</p><h2>数据准备</h2><p>现在我们手中有一批影评数据（IMDB数据集），影评被分为两类：正面评价与负面评价。我们需要训练一个情感分析模型，对影评文本进行分类。</p><!-- [[[read_end]]] --><p>这个问题本质上还是一个文本分类问题，研究对象是电影评论类的文本，我们需要对文本进行二分类。下面我们来看一看训练数据。</p><p>IMDB（Internet Movie Database）是一个来自互联网电影数据库，其中包含了50000条严重两极分化的电影评论。数据集被划分为训练集和测试集，其中训练集和测试集中各有25000条评论，并且训练集和测试集都包含50%的正面评论和50%的消极评论。</p><h3>如何用Torchtext读取数据集</h3><p>我们可以利用Torchtext工具包来读取数据集。</p><p>Torchtext是一个包含<strong>常用的文本处理工具</strong>和<strong>常见自然语言数据集</strong>的工具包。我们可以类比之前学习过的Torchvision包来理解它，只不过，Torchvision包是用来处理图像的，而Torchtext则是用来处理文本的。</p><p>安装Torchtext同样很简单，我们可以使用pip进行安装，命令如下：</p><pre><code class=\"language-plain\">pip install torchtext\n</code></pre><p>Torchtext中包含了上面我们要使用的IMDB数据集，并且还有读取语料库、词转词向量、词转下标、建立相应迭代器等功能，可以满足我们对文本的处理需求。</p><p>更为方便的是，Torchtext已经把一些常见对文本处理的数据集囊括在了<code>torchtext.datasets</code>中，与Torchvision类似，使用时会自动下载、解压并解析数据。</p><p>以IMDB为例，我们可以用后面的代码来读取数据集：</p><pre><code class=\"language-python\"># 读取IMDB数据集\nimport torchtext\ntrain_iter = torchtext.datasets.IMDB(root='./data', split='train')\nnext(train_iter)\n</code></pre><p>torchtext.datasets.IMDB函数有两个参数，其中：</p><ul>\n<li>root：是一个字符串，用于指定你想要读取目标数据集的位置，如果数据集不存在，则会自动下载；</li>\n<li>split：是一个字符串或者元组，表示返回的数据集类型，是训练集、测试集或验证集，默认是&nbsp;(‘train’, ‘test’)。<br>\ntorchtext.datasets.IMDB函数的返回值是一个迭代器，这里我们读取了IMDB数据集中的训练集，共25000条数据，存入了变量train_iter中。</li>\n</ul><p>程序运行的结果如下图所示。我们可以看到，利用next()函数，读取出迭代器train_iter中的一条数据，每一行是情绪分类以及后面的评论文本。“neg”表示负面评价，“pos”表示正面评价。</p><p><img src=\"https://static001.geekbang.org/resource/image/e4/e6/e4625437cafc8bb29851fb57a9b3e8e6.png?wh=1920x616\" alt=\"图片\"></p><h3>数据处理pipelines</h3><p>读取出了数据集中的评论文本和情绪分类，我们还需要将文本和分类标签处理成向量，才能被计算机读取。处理文本的一般过程是先分词，然后根据词汇表将词语转换为id。</p><p>Torchtext为我们提供了基本的文本处理工具，包括分词器“tokenizer”和词汇表“vocab”。我们可以用下面两个函数来创建分词器和词汇表。</p><p>get_tokenizer函数的作用是创建一个分词器。将文本喂给相应的分词器，分词器就可以根据不同分词函数的规则完成分词。例如英文的分词器，就是简单按照空格和标点符号进行分词。</p><p>build_vocab_from_iterator函数可以帮助我们使用训练数据集的迭代器构建词汇表，构建好词汇表后，输入分词后的结果，即可返回每个词语的id。</p><p>创建分词器和构建词汇表的代码如下。首先我们要建立一个可以处理英文的分词器tokenizer，然后再根据IMDB数据集的训练集迭代器train_iter建立词汇表vocab。</p><pre><code class=\"language-python\"># 创建分词器\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\nprint(tokenizer('here is the an example!'))\n'''\n输出：['here', 'is', 'the', 'an', 'example', '!']\n'''\n\n# 构建词汇表\ndef yield_tokens(data_iter):\n&nbsp; &nbsp; for _, text in data_iter:\n&nbsp; &nbsp; &nbsp; &nbsp; yield tokenizer(text)\n\nvocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"&lt;pad&gt;\", \"&lt;unk&gt;\"])\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\nprint(vocab(tokenizer('here is the an example &lt;pad&gt; &lt;pad&gt;')))\n'''\n输出：[131, 9, 40, 464, 0, 0]\n'''\n</code></pre><p>在构建词汇表的过程中，yield_tokens函数的作用就是依次将训练数据集中的每一条数据都进行分词处理。另外，在构建词汇表时，用户还可以利用specials参数自定义词表。</p><p>上述代码中我们自定义了两个词语：“&lt;pad&gt;”和“&lt;unk&gt;”，分别表示占位符和未登录词。顾名思义，未登录词是指没有被收录在分词词表中的词。由于每条影评文本的长度不同，不能直接批量合成矩阵，因此需通过截断或填补占位符来固定长度。</p><p>为了方便后续调用，我们使用分词器和词汇表来建立数据处理的pipelines。文本pipeline用于给定一段文本，返回分词后的id。标签pipeline用于将情绪分类转化为数字，即“neg”转化为0，“pos”转化为1。</p><p>具体代码如下所示。</p><pre><code class=\"language-python\"># 数据处理pipelines\ntext_pipeline = lambda x: vocab(tokenizer(x))\nlabel_pipeline = lambda x: 1 if x == 'pos' else 0\n\nprint(text_pipeline('here is the an example'))\n'''\n输出：[131, 9, 40, 464, 0, 0 , ... , 0]\n'''\nprint(label_pipeline('neg'))\n'''\n输出：0\n'''\n</code></pre><p>通过示例的输出结果，相信你很容易就能理解文本pipeline和标签pipeline的用法了。</p><h3>生成训练数据</h3><p>有了数据处理的pipelines，接下来就是生成训练数据，也就是生成DataLoader。</p><p>这里还涉及到一个变长数据处理的问题。我们在将文本pipeline所生成的id列表转化为模型能够识别的tensor时，由于文本的句子是变长的，因此生成的tensor长度不一，无法组成矩阵。</p><p>这时，我们需要限定一个句子的最大长度。例如句子的最大长度为256个单词，那么超过256个单词的句子需要做截断处理；不足256个单词的句子，需要统一补位，这里用“/<pad>”来填补。</pad></p><p>上面所说的这些操作，我们都可以放到collate_batch函数中来处理。</p><p>collate_batch函数有什么用呢？它负责在DataLoad提取一个batch的样本时，完成一系列预处理工作：包括生成文本的tensor、生成标签的tensor、生成句子长度的tensor，以及上面所说的对文本进行截断、补位操作。所以，我们将collate_batch函数通过参数collate_fn传入DataLoader，即可实现对变长数据的处理。</p><p>collate_batch函数的定义，以及生成训练与验证DataLoader的代码如下。</p><pre><code class=\"language-python\"># 生成训练数据\nimport torch\nimport torchtext\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\nfrom torchtext.data.functional import to_map_style_dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef collate_batch(batch):\n&nbsp; &nbsp; max_length = 256\n&nbsp; &nbsp; pad = text_pipeline('&lt;pad&gt;')\n&nbsp; &nbsp; label_list, text_list, length_list = [], [], []\n&nbsp; &nbsp; for (_label, _text) in batch:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;label_list.append(label_pipeline(_label))\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;processed_text = text_pipeline(_text)[:max_length]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;length_list.append(len(processed_text))\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;text_list.append((processed_text+pad*max_length)[:max_length])\n&nbsp; &nbsp; label_list = torch.tensor(label_list, dtype=torch.int64)\n&nbsp; &nbsp; text_list = torch.tensor(text_list, dtype=torch.int64)\n&nbsp; &nbsp; length_list = torch.tensor(length_list, dtype=torch.int64)\n&nbsp; &nbsp; return label_list.to(device), text_list.to(device), length_list.to(device)\n\ntrain_iter = torchtext.datasets.IMDB(root='./data', split='train')\ntrain_dataset = to_map_style_dataset(train_iter)\nnum_train = int(len(train_dataset) * 0.95)\nsplit_train_, split_valid_ = random_split(train_dataset,&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[num_train, len(train_dataset) - num_train])\ntrain_dataloader = DataLoader(split_train_, batch_size=8, shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(split_valid_, batch_size=8, shuffle=False, collate_fn=collate_batch)\n</code></pre><p>我们一起梳理一下这段代码的流程，一共是五个步骤。</p><p>1.利用torchtext读取IMDB的训练数据集，得到训练数据迭代器；<br>\n2.使用to_map_style_dataset函数将迭代器转化为Dataset类型；<br>\n3.使用random_split函数对Dataset进行划分，其中95%作为训练集，5%作为验证集；<br>\n4.生成训练集的DataLoader；<br>\n5.生成验证集的DataLoader。</p><p>到此为止，数据部分已经全部准备完毕了，接下来我们来进行网络模型的构建。</p><h2>模型构建</h2><p>之前我们已经学过卷积神经网络的相关知识。卷积神经网络使用固定的大小矩阵作为输入（例如一张图片），然后输出一个固定大小的向量（例如不同类别的概率），因此适用于图像分类、目标检测、图像分割等等。</p><p>但是除了图像之外，还有很多信息，其大小或长度并不是固定的，例如音频、视频、文本等。我们想要处理这些序列相关的数据，就要用到时序模型。比如我们今天要处理的文本数据，这就涉及一种常见的时间序列模型：循环神经网络（Recurrent Neural Network，RNN）。</p><p>不过由于RNN自身的结构问题，在进行反向传播时，容易出现梯度消失或梯度爆炸。<strong>LSTM网络</strong>在RNN结构的基础上进行了改进，通过精妙的门控制将短时记忆与长时记忆结合起来，<strong>一定程度上解决了梯度消失与梯度爆炸的问题</strong>。</p><p>我们使用LSTM网络来进行情绪分类的预测。模型的定义如下。</p><pre><code class=\"language-python\"># 定义模型\nclass LSTM(torch.nn.Module):\n&nbsp; &nbsp; def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dropout_rate, pad_index=0):\n&nbsp; &nbsp; &nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n&nbsp; &nbsp; &nbsp; &nbsp; self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropout=dropout_rate, batch_first=True)\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n&nbsp; &nbsp; &nbsp; &nbsp; self.dropout = torch.nn.Dropout(dropout_rate)\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; def forward(self, ids, length):\n&nbsp; &nbsp; &nbsp; &nbsp; embedded = self.dropout(self.embedding(ids))\n&nbsp; &nbsp; &nbsp; &nbsp; packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True,&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; enforce_sorted=False)\n&nbsp; &nbsp; &nbsp; &nbsp; packed_output, (hidden, cell) = self.lstm(packed_embedded)\n&nbsp; &nbsp; &nbsp; &nbsp; output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n&nbsp; &nbsp; &nbsp; &nbsp; if self.lstm.bidirectional:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n&nbsp; &nbsp; &nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hidden = self.dropout(hidden[-1])\n&nbsp; &nbsp; &nbsp; &nbsp; prediction = self.fc(hidden)\n&nbsp; &nbsp; &nbsp; &nbsp; return prediction\n</code></pre><p>网络模型的具体结构，首先是一个Embedding层，用来接收文本id的tensor，然后是LSTM层，最后是一个全连接分类层。其中，bidirectional为True，表示网络为双向LSTM，bidirectional为False，表示网络为单向LSTM。</p><p>网络模型的结构图如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/a8/f4013742ab70b0dc405948f07198cfa8.jpg?wh=619x404\" alt=\"图片\"></p><h2>模型训练与评估</h2><p>定义好网络模型的结构，我们就可以进行模型训练了。首先是实例化网络模型，参数以及具体的代码如下。</p><pre><code class=\"language-python\"># 实例化模型\nvocab_size = len(vocab)\nembedding_dim = 300\nhidden_dim = 300\noutput_dim = 2\nn_layers = 2\nbidirectional = True\ndropout_rate = 0.5\n\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)\nmodel = model.to(device)\n</code></pre><p>由于数据的情感极性共分为两类，因此这里我们要把output_dim的值设置为2。<br>\n接下来是定义损失函数与优化方法，代码如下。在之前的课程里也多次讲过了，所以这里不再重复。</p><pre><code class=\"language-python\"># 损失函数与优化方法\nlr = 5e-4\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion = criterion.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n</code></pre><p>计算loss的代码如下。</p><pre><code class=\"language-python\">import tqdm\nimport sys\nimport numpy as np\n\ndef train(dataloader, model, criterion, optimizer, device):\n&nbsp; &nbsp; model.train()\n&nbsp; &nbsp; epoch_losses = []\n&nbsp; &nbsp; epoch_accs = []\n&nbsp; &nbsp; for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n&nbsp; &nbsp; &nbsp; &nbsp; (label, ids, length) = batch\n&nbsp; &nbsp; &nbsp; &nbsp; label = label.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; ids = ids.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; length = length.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; prediction = model(ids, length)\n&nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(prediction, label) # loss计算\n&nbsp; &nbsp; &nbsp; &nbsp; accuracy = get_accuracy(prediction, label)\n        # 梯度更新\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n&nbsp; &nbsp; &nbsp; &nbsp; epoch_losses.append(loss.item())\n&nbsp; &nbsp; &nbsp; &nbsp; epoch_accs.append(accuracy.item())\n&nbsp; &nbsp; return epoch_losses, epoch_accs\n\ndef evaluate(dataloader, model, criterion, device):\n&nbsp; &nbsp; model.eval()\n&nbsp; &nbsp; epoch_losses = []\n&nbsp; &nbsp; epoch_accs = []\n&nbsp; &nbsp; with torch.no_grad():\n&nbsp; &nbsp; &nbsp; &nbsp; for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (label, ids, length) = batch\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; label = label.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ids = ids.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; length = length.to(device)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prediction = model(ids, length)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(prediction, label) # loss计算\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; accuracy = get_accuracy(prediction, label)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; epoch_losses.append(loss.item())\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; epoch_accs.append(accuracy.item())\n&nbsp; &nbsp; return epoch_losses, epoch_accs\n</code></pre><p>可以看到，这里训练过程与验证过程的loss计算，分别定义在了train函数和evaluate函数中。主要区别是训练过程有梯度的更新，而验证过程中不涉及梯度的更新，只计算loss即可。<br>\n模型的评估我们使用ACC，也就是准确率作为评估指标，计算ACC的代码如下。</p><pre><code class=\"language-python\">def get_accuracy(prediction, label):\n&nbsp; &nbsp; batch_size, _ = prediction.shape\n&nbsp; &nbsp; predicted_classes = prediction.argmax(dim=-1)\n&nbsp; &nbsp; correct_predictions = predicted_classes.eq(label).sum()\n&nbsp; &nbsp; accuracy = correct_predictions / batch_size\n&nbsp; &nbsp; return accuracy\n</code></pre><p>最后，训练过程的具体代码如下。包括计算loss和ACC、保存losses列表和保存最优模型。</p><pre><code class=\"language-python\">n_epochs = 10\nbest_valid_loss = float('inf')\n\ntrain_losses = []\ntrain_accs = []\nvalid_losses = []\nvalid_accs = []\n\nfor epoch in range(n_epochs):\n&nbsp; &nbsp; train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n&nbsp; &nbsp; valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n&nbsp; &nbsp; train_losses.extend(train_loss)\n&nbsp; &nbsp; train_accs.extend(train_acc)\n&nbsp; &nbsp; valid_losses.extend(valid_loss)\n&nbsp; &nbsp; valid_accs.extend(valid_acc)&nbsp;\n&nbsp; &nbsp; epoch_train_loss = np.mean(train_loss)\n&nbsp; &nbsp; epoch_train_acc = np.mean(train_acc)\n&nbsp; &nbsp; epoch_valid_loss = np.mean(valid_loss)\n&nbsp; &nbsp; epoch_valid_acc = np.mean(valid_acc)&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; if epoch_valid_loss &lt; best_valid_loss:\n&nbsp; &nbsp; &nbsp; &nbsp; best_valid_loss = epoch_valid_loss\n&nbsp; &nbsp; &nbsp; &nbsp; torch.save(model.state_dict(), 'lstm.pt') &nbsp;&nbsp;\n&nbsp; &nbsp; print(f'epoch: {epoch+1}')\n&nbsp; &nbsp; print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n&nbsp; &nbsp; print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n</code></pre><p>我们还可以利用保存下来train_losses列表，绘制训练过程中的loss曲线，或使用<a href=\"https://time.geekbang.org/column/article/444252\">第15课</a>讲过的可视化工具来监控训练过程。<br>\n至此，一个完整的情感分析项目已经完成了。从数据读取到模型构建与训练的方方面面，我都手把手教给了你，希望你能以此为模板，独立解决自己的问题。</p><h2>小结</h2><p>恭喜你，完成了今天的学习任务。今天我们一起完成了一个情感分析项目的实践，相当于是对自然语言处理任务的一个初探。我带你回顾一下今天学习的要点。</p><p>在数据准备阶段，我们可以使用PyTorch提供的文本处理工具包Torchtext。想要掌握Torchtext也不难，我们可以类比之前详细介绍过的Torchvision，不懂的地方再对应去<a href=\"https://pytorch.org/text/stable/index.html\">查阅文档</a>，相信你一定可以做到举一反三。</p><p><strong>模型构建时，要根据具体的问题选择适合的神经网络。卷积神经网络常被用于处理图像作为输入的预测问题；循环神经网络常被用于处理变长的、序列相关的数据。而LSTM相较于RNN，能更好地解决梯度消失与梯度爆炸的问题</strong>。</p><p>在后续的课程中，我们还会讲解两大自然语言处理任务：文本分类和摘要生成，它们分别包括了判别模型和生成模型，相信那时你一定会在文本处理方面有更深层次的理解。</p><h2>每课一练</h2><p>利用今天训练的模型，编写一个函数predict_sentiment，实现输入一句话，输出这句话的情绪类别与概率。</p><p>例如：</p><pre><code class=\"language-python\">text = \"This film is terrible!\"\npredict_sentiment(text, model, tokenizer, vocab, device)\n'''\n输出：('neg', 0.8874172568321228)\n'''\n</code></pre><p>欢迎你在留言区跟我交流互动，也推荐你把今天学到的内容分享给更多朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"22 | NLP基础（下）：详解语言模型与注意力机制","id":461691},"right":{"article_title":"24 | 文本分类：如何使用BERT构建文本分类模型？","id":464152}},"comments":[{"had_liked":false,"id":328915,"user_name":"李雄","can_delete":false,"product_type":"c1","uid":2805175,"ip_address":"","ucode":"ABC36CFEEEE41B","user_header":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","comment_is_top":true,"comment_ctime":1640934421,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"9.223372042790699e+18","product_id":100093301,"comment_content":"如果是使用torchtext==0.8.1以下的版本建议看官网文档：<br>https:&#47;&#47;pytorch.org&#47;text&#47;0.8.1&#47;datasets.html","like_count":1},{"had_liked":false,"id":328917,"user_name":"李雄","can_delete":false,"product_type":"c1","uid":2805175,"ip_address":"","ucode":"ABC36CFEEEE41B","user_header":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","comment_is_top":false,"comment_ctime":1640935394,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5935902690","product_id":100093301,"comment_content":"import torch<br>import torchtext<br>torchtext.__version__<br>dir(torchtext)<br>from torchtext import data<br>dir(data)<br><br># 读取IMDB数据集<br><br><br># 读取IMDB数据集<br><br>TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)<br>LABEL = data.Field(sequential=False)<br><br># make splits for data 划分数据<br>train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL)<br><br># build the vocabulary<br>from torchtext.vocab import GloVe<br>TEXT.build_vocab(train, vectors=GloVe(name=&#39;6B&#39;, dim=300))<br>LABEL.build_vocab(train)<br><br># make iterator for splits<br>train_iter, test_iter = data.BucketIterator.splits(<br>    (train, test), batch_size=3, device=0)","like_count":1},{"had_liked":false,"id":326850,"user_name":"林于翔","can_delete":false,"product_type":"c1","uid":2810184,"ip_address":"","ucode":"BCFE3F7520F8A0","user_header":"https://static001.geekbang.org/account/avatar/00/2a/e1/48/3a981e55.jpg","comment_is_top":false,"comment_ctime":1639720582,"is_pvip":false,"replies":[{"id":"119009","content":"Hello，感谢留言。<br>我们首先看一下torch.nn.LSTM这个函数的输入与输出，详见文档：<br>https:&#47;&#47;pytorch.org&#47;docs&#47;stable&#47;generated&#47;torch.nn.LSTM.html?highlight=nn%20lstm#torch.nn.LSTM<br><br>torch.nn.LSTM的输入参数：<br>input_size： 表示的是输入的矩阵特征数<br>hidden_size： 表示的是输出矩阵特征数<br>num_layers 表示堆叠几层的LSTM，默认是1<br>bidirectional： 默认是false，代表不用双向LSTM，双向LSTM，也就是序列从左往右算一次，从右往左又算一次，这样就可以两倍的输出<br>batch_first： True 或者 False，因为nn.lstm()接受的数据输入是(序列长度，batch，输入维数)，这和我们cnn输入的方式不太一致，所以使用batch_first，我们可以将输入变成(batch，序列长度，输入维数)<br><br>torch.nn.LSTM的输出格式为：output,(h_n,c_n)<br>其中：<br>output的shape=(seq_length,batch_size,num_directions*hidden_size),它包含的是LSTM的最后一时间步的输出特征(h_t),ｔ是batch_size中每个句子的长度。<br>h_n.shape==(num_directions * num_layers,batch,hidden_size)<br>c_n.shape==h_n.shape<br>h_n包含的是句子的最后一个单词（也就是最后一个时间步）的隐藏状态，c_n包含的是句子的最后一个单词的细胞状态，所以它们都与句子的长度seq_length无关。<br>output[-1]与h_n是相等的，因为output[-1]包含的正是batch_size个句子中每一个句子的最后一个单词的隐藏状态。<br><br>if self.lstm.bidirectional:<br> hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))<br>这里面，hidden是torch.nn.LSTM的第二个输出h_n，hidden的shape是(2 * 2, batch, 300)。<br>因此，双向LSTM时，hidden[-1]表示从最后一个堆叠的从左到右的LSTM的的最后一个时间步向量，hidden[-2]表示最后一个堆叠的从右到左的LSTM最后一个时间步向量。将这两个向量拼接在一起，得到最后的hidden层。<br><br>而dim=-1表示torch.cat的拼接方式，拼接300(embedding_dim)那个维度，也就是将两个300维度的向量拼接成一个600维度的向量。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1639977839,"ip_address":"","comment_id":326850,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5934687878","product_id":100093301,"comment_content":"LSTM模型定义中:<br> if self.lstm.bidirectional:<br>            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))<br>这里不太理解，最后连接的不是最后一层hidden的第一个时间步和最后一个时间步嘛？另外dim为什么会是-1。","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":540168,"discussion_content":"Hello，感谢留言。\n我们首先看一下torch.nn.LSTM这个函数的输入与输出，详见文档：\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=nn%20lstm#torch.nn.LSTM\n\ntorch.nn.LSTM的输入参数：\ninput_size： 表示的是输入的矩阵特征数\nhidden_size： 表示的是输出矩阵特征数\nnum_layers 表示堆叠几层的LSTM，默认是1\nbidirectional： 默认是false，代表不用双向LSTM，双向LSTM，也就是序列从左往右算一次，从右往左又算一次，这样就可以两倍的输出\nbatch_first： True 或者 False，因为nn.lstm()接受的数据输入是(序列长度，batch，输入维数)，这和我们cnn输入的方式不太一致，所以使用batch_first，我们可以将输入变成(batch，序列长度，输入维数)\n\ntorch.nn.LSTM的输出格式为：output,(h_n,c_n)\n其中：\noutput的shape=(seq_length,batch_size,num_directions*hidden_size),它包含的是LSTM的最后一时间步的输出特征(h_t),ｔ是batch_size中每个句子的长度。\nh_n.shape==(num_directions * num_layers,batch,hidden_size)\nc_n.shape==h_n.shape\nh_n包含的是句子的最后一个单词（也就是最后一个时间步）的隐藏状态，c_n包含的是句子的最后一个单词的细胞状态，所以它们都与句子的长度seq_length无关。\noutput[-1]与h_n是相等的，因为output[-1]包含的正是batch_size个句子中每一个句子的最后一个单词的隐藏状态。\n\nif self.lstm.bidirectional:\n hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n这里面，hidden是torch.nn.LSTM的第二个输出h_n，hidden的shape是(2 * 2, batch, 300)。\n因此，双向LSTM时，hidden[-1]表示从最后一个堆叠的从左到右的LSTM的的最后一个时间步向量，hidden[-2]表示最后一个堆叠的从右到左的LSTM最后一个时间步向量。将这两个向量拼接在一起，得到最后的hidden层。\n\n而dim=-1表示torch.cat的拼接方式，拼接300(embedding_dim)那个维度，也就是将两个300维度的向量拼接成一个600维度的向量。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639977839,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":357168,"user_name":"John(易筋)","can_delete":false,"product_type":"c1","uid":1180202,"ip_address":"广东","ucode":"BB4E58DD4B8F15","user_header":"https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg","comment_is_top":false,"comment_ctime":1663025379,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1663025379","product_id":100093301,"comment_content":"最后一块代码跑出错<br>版本 torchtext<br>print(torchtext.__version__)  # 0.13.1<br>出错的代码<br>for epoch in range(n_epochs):<br>    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)<br><br>错误log<br>training...:   0%|          | 0&#47;2969 [00:00&lt;?, ?it&#47;s]<br>---------------------------------------------------------------------------<br>TypeError                                 Traceback (most recent call last)<br>&lt;ipython-input-23-3dd500bd7dd1&gt; in &lt;module&gt;<br>      8 <br>      9 for epoch in range(n_epochs):<br>---&gt; 10     train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)<br><br>      7     epoch_losses = []<br>      8     epoch_accs = []<br>----&gt; 9     for batch in tqdm.tqdm(dataloader, desc=&#39;training...&#39;, file=sys.stdout):<br>     10         (label, ids, length) = batch<br><br>~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;tqdm&#47;std.py in __iter__(self)<br>   1127 <br>   1128         try:<br>-&gt; 1129             for obj in iterable:<br><br>~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;dataloader.py in __next__(self)<br>    680                 self._reset()  # type: ignore[call-arg]<br>--&gt; 681             data = self._next_data()<br>    682             self._num_yielded += 1<br><br>~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;dataloader.py in _next_data(self)<br>    720         index = self._next_index()  # may raise StopIteration<br>--&gt; 721         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration<br>    722         if self._pin_memory:<br>~&#47;opt&#47;anaconda3&#47;lib&#47;python3.8&#47;site-packages&#47;torch&#47;utils&#47;data&#47;_utils&#47;fetch.py in fetch(self, possibly_batched_index)<br>     51             data = self.dataset[possibly_batched_index]<br>---&gt; 52         return self.collate_fn(data)<br><br>&lt;ipython-input-12-6fc8a353bed8&gt; in collate_batch(batch)<br>     10         length_list.append(len(processed_text))<br>---&gt; 11         text_list.append((processed_text + max_length)[:max_length])<br><br>TypeError: can only concatenate list (not &quot;int&quot;) to list","like_count":0},{"had_liked":false,"id":356973,"user_name":"John(易筋)","can_delete":false,"product_type":"c1","uid":1180202,"ip_address":"广东","ucode":"BB4E58DD4B8F15","user_header":"https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg","comment_is_top":false,"comment_ctime":1662770596,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1662770596","product_id":100093301,"comment_content":"torchtext.datasets.IMDB 改为如下可以正确运行<br><br># pip install torchtext<br># pip install torchdata<br>import torchtext<br># 读取IMDB数据集<br>train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)<br>train_iter = iter(train_iter)<br>next(train_iter)","like_count":0},{"had_liked":false,"id":350094,"user_name":"hallo128","can_delete":false,"product_type":"c1","uid":1212044,"ip_address":"","ucode":"3921D6E11CFCB1","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","comment_is_top":false,"comment_ctime":1656569678,"is_pvip":false,"replies":[{"id":"127408","content":"你好，感谢你的留言。<br>torchtext支持的分词器有：spacy, moses, toktok, revtok, subword。<br>spacy库是可以支持中文分词的。<br>可以尝试以下代码：<br>import spacy<br>spacy = spacy.load(&quot;zh_core_web_sm&quot;)<br>tokenizer = get_tokenizer(spacy)","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1656659124,"ip_address":"","comment_id":350094,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1656569678","product_id":100093301,"comment_content":"torchtext可以支持中文分词吗","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":578322,"discussion_content":"你好，感谢你的留言。\ntorchtext支持的分词器有：spacy, moses, toktok, revtok, subword。\nspacy库是可以支持中文分词的。\n可以尝试以下代码：\nimport spacy\nspacy = spacy.load(&#34;zh_core_web_sm&#34;)\ntokenizer = get_tokenizer(spacy)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656659124,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":350093,"user_name":"hallo128","can_delete":false,"product_type":"c1","uid":1212044,"ip_address":"","ucode":"3921D6E11CFCB1","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","comment_is_top":false,"comment_ctime":1656569331,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1656569331","product_id":100093301,"comment_content":"torchtext说明文档：https:&#47;&#47;pytorch.org&#47;text&#47;stable&#47;index.html","like_count":0},{"had_liked":false,"id":349147,"user_name":"Geek_a82ba7","can_delete":false,"product_type":"c1","uid":2365466,"ip_address":"","ucode":"69D1E55F93FC52","user_header":"","comment_is_top":false,"comment_ctime":1655774833,"is_pvip":false,"replies":[{"id":"127193","content":"您好，感谢你的留言。<br>看看是不是torchtext版本更新了？<br>我用的是0.10.1","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1655962304,"ip_address":"","comment_id":349147,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1655774833","product_id":100093301,"comment_content":"import torchtext<br>train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)<br>next(train_iter)<br>&#39;MapperIterDataPipe&#39; object is not an iterator<br>为什么说这个不是一个迭代器，查了很多资料都没有解决，老师能回答一下吗？<br>c","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":577186,"discussion_content":"您好，感谢你的留言。\n看看是不是torchtext版本更新了？\n我用的是0.10.1","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1655962304,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1212044,"avatar":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","nickname":"hallo128","note":"","ucode":"3921D6E11CFCB1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":578249,"discussion_content":"我也是同样的问题，版本：0.12.0，centos上。数据下载不下来，可以试下换datasets包来得到数据集\n\nimport datasets\ntrain_data, test_data = datasets.load_dataset(&#39;imdb&#39;, split=[&#39;train&#39;, &#39;test&#39;])","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656601668,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2365466,"avatar":"","nickname":"Geek_a82ba7","note":"","ucode":"69D1E55F93FC52","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":577328,"discussion_content":"我已经把我的版本降到了0.10.1，发现一直下载不下来数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656039387,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1507892,"avatar":"https://static001.geekbang.org/account/avatar/00/17/02/34/daade19d.jpg","nickname":"野蛮酋长","note":"","ucode":"B55BA52109CF00","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":2365466,"avatar":"","nickname":"Geek_a82ba7","note":"","ucode":"69D1E55F93FC52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":579081,"discussion_content":"print(train_iter)\nit = iter(train_iter)\nprint(next(it)) 就可以解决了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1657166944,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":577328,"ip_address":""},"score":579081,"extra":""}]}]},{"had_liked":false,"id":348387,"user_name":"吴十一","can_delete":false,"product_type":"c1","uid":1658932,"ip_address":"","ucode":"47232ED3225081","user_header":"https://static001.geekbang.org/account/avatar/00/19/50/34/172342fd.jpg","comment_is_top":false,"comment_ctime":1655043873,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1655043873","product_id":100093301,"comment_content":"def predict_sentiment(text, model, tokenizer, vocab, device):<br>    max_length = 256<br>    pad = text_pipeline(&#39;&lt;pad&gt;&#39;)<br>    processed_text = text_pipeline(text)[:max_length]<br>    ids, length = [],[]<br>    ids.append((processed_text+pad*max_length)[:max_length])<br>    length.append(len(processed_text))<br>    ids_t,length_t = torch.tensor(ids, dtype = torch.int64), torch.tensor(length, dtype = torch.int64)<br>    print(ids_t,length_t)<br>    ids_t.to(device)<br>    length_t.to(device)<br>    pred = model(ids_t,length_t)<br>    return pred<br>vocab_size = len(vocab)<br>embedding_dim = 300<br>hidden_dim = 300<br>output_dim = 2<br>n_layers = 2<br>bidirectional = True<br>dropout_rate = 0.5<br>device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<br>model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)<br>model = model.to(device)<br>model.load_state_dict(torch.load(&quot;.&#47;lstm.pt&quot;))<br>model.eval()<br>text = &quot;This film is terrible!&quot;<br>pred = predict_sentiment(text, model, tokenizer, vocab, device)<br>&#39;&#39;&#39;<br>输出：(&#39;neg&#39;, 0.8874172568321228)<br>&#39;&#39;&#39;<br>tips：一定要安装torchtext 稍微新点的版本，0.6.0 IDMB数据set跑报错，会一直要求用tensorflow v1 版本兼容","like_count":0},{"had_liked":false,"id":347833,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1654498269,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1654498269","product_id":100093301,"comment_content":"用自己的电脑训练一个端午节，结果如下：<br>(&#39;neg&#39;, 0.9985383749008179)","like_count":0},{"had_liked":false,"id":347831,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1654497549,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1654497549","product_id":100093301,"comment_content":"import torch<br>import torchtext<br><br>from LSTM import LSTM<br><br>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<br><br>def predict_sentiment(text, model, tokenizer, vocab, device):<br>    tokens = tokenizer(text)<br>    ids = [vocab[t] for t in tokens]<br>    length = torch.LongTensor([len(ids)])<br>    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)<br>    prediction = model(tensor, length).squeeze(dim=0)<br>    probability = torch.softmax(prediction, dim=-1)<br>    predicted_class = prediction.argmax(dim=-1).item()<br>    predicted_probability = probability[predicted_class].item()<br>    predicted_class_title = [&#39;neg&#39;, &#39;pos&#39;]<br>    return predicted_class_title[predicted_class], predicted_probability<br><br>if __name__ == &quot;__main__&quot;:<br>    text = &quot;This film is terrible!&quot;<br><br>    train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)<br>    # 创建分词器<br>    tokenizer = torchtext.data.utils.get_tokenizer(&#39;basic_english&#39;)<br><br><br>    # 构建词汇表<br>    def yield_tokens(data_iter):<br>        for _, text in data_iter:<br>            yield tokenizer(text)<br><br><br>    vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_iter), specials=[&quot;&lt;pad&gt;&quot;, &quot;&lt;unk&gt;&quot;])<br>    vocab.set_default_index(vocab[&quot;&lt;unk&gt;&quot;])<br><br>    # 加载模型<br>    vocab_size = len(vocab)<br>    embedding_dim = 300<br>    hidden_dim = 300<br>    output_dim = 2<br>    n_layers = 2<br>    bidirectional = True<br>    dropout_rate = 0.5<br>    model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate)<br>    model.load_state_dict(torch.load(&#39;.&#47;lstm.pt&#39;))<br>    model.to(device)<br>    model.eval()<br>    print(predict_sentiment(text, model, tokenizer, vocab, device))<br><br>参考：<br>https:&#47;&#47;github.com&#47;bentrevett&#47;pytorch-sentiment-analysis&#47;blob&#47;master&#47;2_lstm.ipynb","like_count":0},{"had_liked":false,"id":347496,"user_name":"Sarai青霞","can_delete":false,"product_type":"c1","uid":1519686,"ip_address":"","ucode":"C8BD6647DEAF06","user_header":"https://static001.geekbang.org/account/avatar/00/17/30/46/19f203cc.jpg","comment_is_top":false,"comment_ctime":1654076745,"is_pvip":false,"replies":[{"id":"126704","content":"你好，感谢留言。<br>python的多行注释是用三对单引号。<br><br>&#39;&#39;&#39;<br>输出：[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;an&#39;, &#39;example&#39;, &#39;!&#39;]<br>&#39;&#39;&#39; <br>这部分内容是代码中的注释，为了告诉你上述代码的运行结果是什么。<br><br>有的IDE运行时会输出多行注释，只需把注释删除即可，并不是断行出现问题。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1654160423,"ip_address":"","comment_id":347496,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1654076745","product_id":100093301,"comment_content":"方老师，我试了代码，输出显示：<br>&quot;\\n输出：[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;an&#39;, &#39;example&#39;, &#39;!&#39;]\\n&quot;<br>是断行出现问题了吗？<br>谢谢！","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":574582,"discussion_content":"你好，感谢留言。\npython的多行注释是用三对单引号。\n\n&#39;&#39;&#39;\n输出：[&#39;here&#39;, &#39;is&#39;, &#39;the&#39;, &#39;an&#39;, &#39;example&#39;, &#39;!&#39;]\n&#39;&#39;&#39; \n这部分内容是代码中的注释，为了告诉你上述代码的运行结果是什么。\n\n有的IDE运行时会输出多行注释，只需把注释删除即可，并不是断行出现问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654160423,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":347473,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1654066770,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1654066770","product_id":100093301,"comment_content":"import torchtext<br>train_iter = torchtext.datasets.IMDB(root=&#39;.&#47;data&#39;, split=&#39;train&#39;)<br>print(next(iter(train_iter)))<br>在PyCharm里面这样跑没问题，JupyterLab这样就跑不动了","like_count":0},{"had_liked":false,"id":344255,"user_name":"Ringcoo","can_delete":false,"product_type":"c1","uid":2979424,"ip_address":"","ucode":"89AA99AB7FE6FF","user_header":"https://static001.geekbang.org/account/avatar/00/2d/76/60/4be280d7.jpg","comment_is_top":false,"comment_ctime":1651399506,"is_pvip":false,"replies":[{"id":"125782","content":"hello，Ringcoo，你好。感谢你的留言。<br>当我们训练LSTM时，因为文本句子长短不一，如果想要进行批次化训练，就需要选择一个合适的长度来进行截断和填充。<br>torch.nn.utils.rnn.pad_packed_sequence()并不是一个rnn层，这个函数的目的是将LSTM返回的结果进行统一填充，得到长度相同的Tensor对象。<br><br>hidden和cell是LSTM的结构中的一部分。<br>LSTM神经元在时间维度上向后传递了两份信息:<br>(1)cell state;<br>(2)hidden state。<br>hidden state是cell state经过一个神经元和一道“输出门”后得到的，因此hidden state里包含的记忆，实际上是cell state衰减之后的内容。<br>另外，cell state在一个衰减较少的通道里沿时间轴传递，对时间跨度较大的信息的保持能力比hidden state要强很多。<br>因此，实际上hidden state里存储的，主要是“近期记忆”；cell state里存储的，主要是“远期记忆”。<br>cell state的存在，使得LSTM得以对长依赖进行很好地刻画。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1651717380,"ip_address":"","comment_id":344255,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1651399506","product_id":100093301,"comment_content":"老师好，请问<br>packed_output, (hidden, cell) = self.lstm(packed_embedded)       <br> output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)<br>为什么从lstm层出来以后 会有(hidden, cell) ，我不太理解，以及为什么lstm层出来以后还有在进RNN层，底下的流程图上明明是lstm层出来以后进入了fc全连接层。麻烦您讲解一下","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":570255,"discussion_content":"hello，Ringcoo，你好。感谢你的留言。\n当我们训练LSTM时，因为文本句子长短不一，如果想要进行批次化训练，就需要选择一个合适的长度来进行截断和填充。\ntorch.nn.utils.rnn.pad_packed_sequence()并不是一个rnn层，这个函数的目的是将LSTM返回的结果进行统一填充，得到长度相同的Tensor对象。\n\nhidden和cell是LSTM的结构中的一部分。\nLSTM神经元在时间维度上向后传递了两份信息:\n(1)cell state;\n(2)hidden state。\nhidden state是cell state经过一个神经元和一道“输出门”后得到的，因此hidden state里包含的记忆，实际上是cell state衰减之后的内容。\n另外，cell state在一个衰减较少的通道里沿时间轴传递，对时间跨度较大的信息的保持能力比hidden state要强很多。\n因此，实际上hidden state里存储的，主要是“近期记忆”；cell state里存储的，主要是“远期记忆”。\ncell state的存在，使得LSTM得以对长依赖进行很好地刻画。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1651717380,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":338056,"user_name":"赵心睿","can_delete":false,"product_type":"c1","uid":2860861,"ip_address":"","ucode":"9CD189483AAF0B","user_header":"","comment_is_top":false,"comment_ctime":1647259640,"is_pvip":false,"replies":[{"id":"123638","content":"你好，赵心睿，感谢留言。使用的是0.10.1。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1647398907,"ip_address":"","comment_id":338056,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1647259640","product_id":100093301,"comment_content":"请问使用的torchtext是哪个版本的呢？<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556484,"discussion_content":"你好，赵心睿，感谢留言。使用的是0.10.1。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647398907,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":329084,"user_name":"快乐小夜曲","can_delete":false,"product_type":"c1","uid":1602315,"ip_address":"","ucode":"0E257D2EAAA499","user_header":"https://static001.geekbang.org/account/avatar/00/18/73/0b/0f918fa2.jpg","comment_is_top":false,"comment_ctime":1641104936,"is_pvip":false,"replies":[{"id":"119902","content":"你好，快乐小夜曲，感谢你的留言。<br>在train函数中，已经对length进行了转换：length = length.to(device)<br>length是从外部传入的，所以这里无需显示对length再进行转换了。","user_name":"作者回复","user_name_real":"作者","uid":"2802608","ctime":1641258469,"ip_address":"","comment_id":329084,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641104936","product_id":100093301,"comment_content":"packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length.to(&#39;cpu&#39;), batch_first=True,  enforce_sorted=False)<br>这里length必须转为成cpu，否则会报错。","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":543645,"discussion_content":"你好，快乐小夜曲，感谢你的留言。\n在train函数中，已经对length进行了转换：length = length.to(device)\nlength是从外部传入的，所以这里无需显示对length再进行转换了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641258469,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":328914,"user_name":"李雄","can_delete":false,"product_type":"c1","uid":2805175,"ip_address":"","ucode":"ABC36CFEEEE41B","user_header":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","comment_is_top":false,"comment_ctime":1640934158,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1640934158","product_id":100093301,"comment_content":"我的版本太低了<br>","like_count":0},{"had_liked":false,"id":328898,"user_name":"李雄","can_delete":false,"product_type":"c1","uid":2805175,"ip_address":"","ucode":"ABC36CFEEEE41B","user_header":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","comment_is_top":false,"comment_ctime":1640929351,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1640929351","product_id":100093301,"comment_content":"我安装了torchtext==0.6跟老师的版本估计不一致<br>","like_count":0},{"had_liked":false,"id":328889,"user_name":"李雄","can_delete":false,"product_type":"c1","uid":2805175,"ip_address":"","ucode":"ABC36CFEEEE41B","user_header":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","comment_is_top":false,"comment_ctime":1640926670,"is_pvip":false,"replies":[{"id":"119903","content":"hello，我用的是torchtext 0.10.1。","user_name":"作者回复","user_name_real":"作者","uid":"2802608","ctime":1641258859,"ip_address":"","comment_id":328889,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1640926670","product_id":100093301,"comment_content":"请问这是那个版本的torchtext","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":543646,"discussion_content":"hello，我用的是torchtext 0.10.1。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641258859,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":324750,"user_name":"reatiny","can_delete":false,"product_type":"c1","uid":1084445,"ip_address":"","ucode":"1702FDE8CCFBE3","user_header":"https://static001.geekbang.org/account/avatar/00/10/8c/1d/4641c012.jpg","comment_is_top":false,"comment_ctime":1638603048,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1638603048","product_id":100093301,"comment_content":"我感觉模型部分数据流动不太明白，建议多一点解释","like_count":0}]}