{"id":357197,"title":"08 | 应用开发三原则：如何拓展自己的开发边界？","content":"<p>你好，我是吴磊。</p><p>从今天开始，我们就进入通用性能调优篇的学习了。这一篇，我们会从基本的开发原则、配置项、Shuffle以及硬件资源这四个方面，去学习一些通用的调优方法和技巧，它们会适用于所有的计算场景。</p><p>今天这一讲，我们先从应用开发的角度入手，去探讨开发阶段应该遵循的基础原则。<strong>如果能在开发阶段就打好基础、防患于未然，业务应用的执行性能往往会有个不错的起点</strong>。开发阶段就像学生时代的考卷，虽然有很难的拔高题，但只要我们稳扎稳打，答好送分的基础题，成绩往往不会太差。</p><p>这些“基础题”对应的就是工作中一些“常规操作”，比如Filter + Coalesce和用mapPartitions代替map，以及用ReduceByKey代替GroupByKey等等。我相信，你在日常的开发工作中肯定已经积累了不少。但是据我观察，很多同学在拿到这些技巧之后，都会不假思索地“照葫芦画瓢”。不少同学反馈：“试了之后怎么没效果啊？算了，反正能试的都试了，我也实在没有别的调优思路了，就这样吧”。</p><p>那么，这种情况该怎么办呢？我认为，最重要的原因可能是你积累的这些“常规操作”还没有形成体系。结合以往的开发经验，我发现这些“常规操作”可以归纳为三类：</p><!-- [[[read_end]]] --><ul>\n<li>坐享其成</li>\n<li>能省则省、能拖则拖</li>\n<li>跳出单机思维</li>\n</ul><p>话不多说，接下来，我就来和你好好聊一聊。</p><h2>原则一：坐享其成</h2><p>站在巨人的肩膀上才能看得更远，所以在绞尽脑汁去尝试各种调优技巧之前，我们应该尽可能地充分利用Spark为我们提供的“性能红利”，如钨丝计划、AQE、SQL functions等等。<strong>我把这类原则称作“坐享其成”，意思是说我们通过设置相关的配置项，或是调用相应的API去充分享用Spark自身带来的性能优势</strong>。</p><p>那么，我们都可以利用哪些现成的优势呢？</p><h3>如何利用钨丝计划的优势？</h3><p>首先，我们可以利用Databricks在2015年启动的“钨丝计划（Project Tungsten）”。它的优势是，可以通过对数据模型与算法的优化，把Spark应用程序的执行性能提升一个数量级。那这是怎么做到的呢？这就要从它的数据结构说起了。</p><p><strong>在数据结构方面，Tungsten自定义了紧凑的二进制格式。</strong>这种数据结构在存储效率方面，相比JVM对象存储高出好几个数量级。另外，由于数据结构本身就是紧凑的二进制形式，因此它天然地避免了Java对象序列化与反序列化引入的计算开销。</p><p>基于定制化的二进制数据结构，<strong>Tungsten利用Java Unsafe API开辟堆外（Off Heap Memory）内存来管理对象</strong>。堆外内存有两个天然的优势：一是对于内存占用的估算更精确，二来不需要像JVM Heap那样反复执行垃圾回收。</p><p>最后，在运行时，<strong>Tungsten用全阶段代码生成（Whol Stage Code Generation）取代火山迭代模型</strong>，这不仅可以减少虚函数调用和降低内存访问频率，还能提升CPU cache命中率，做到大幅压缩CPU idle时间，从而提升CPU利用率。</p><p><a href=\"https://databricks.com/session/spark-sql-another-16x-faster-after-tungsten\">Databricks官方对比实验</a>显示，开启Tungsten前后，应用程序的执行性能可以提升16倍！<strong>因此你看，哪怕<strong><strong>咱们</strong></strong>什么都不做，只要开发的业务应用能够利用到Tungsten提供的种种特性，Spark就能让应用的执行性能有所保障</strong>。对于咱们开发者来说，这么大的便宜，干吗不占呢？</p><h3>如何利用AQE的优势？</h3><p>除了钨丝计划，我们最应该关注Spark 3.0版本发布的新特性——AQE。AQE（Adaptive Query Execution）全称“自适应查询执行”，它可以在Spark SQL优化的过程中动态地调整执行计划。</p><p>我们知道，Spark SQL的优化过程可以大致分为语法分析、语义解析、逻辑计划和物理计划这几个环节。在3.0之前的版本中，Spark仅仅在编译时基于规则和策略遍历AST查询语法树，来优化逻辑计划，一旦基于最佳逻辑计划选定物理执行计划，Spark就会严格遵照物理计划的步骤去机械地执行计算。</p><p><strong>而AQE可以让Spark在运行时的不同阶段，结合实时的运行时状态，周期性地动态调整前面的逻辑计划，然后根据再优化的逻辑计划，重新选定最优的物理计划，从而调整运行时后续阶段的执行方式。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/4c/17/4cdd21d991c290a12e34d5dbfbdf1f17.jpg?wh=3969*1131\" alt=\"\" title=\"Spark SQL端到端优化流程\"></p><p>你可能会问：“听上去这么厉害，那AQE具体都有哪些改进呢？”AQE主要带来了3个方面的改进，分别是自动分区合并、数据倾斜和Join策略调整。我们一一来看。</p><p><strong>首先，自动分区合并很好理解</strong>，我们拿Filter与Coalesce来举例。分布式数据集过滤之后，难免有些数据分片的内容所剩无几，甚至为空，所以为了避免多余的调度开销，我们经常会用Coalesce去做手工的分区合并。</p><p>另外，在Shuffle的计算过程中，同样也存在分区合并的需求。</p><p><img src=\"https://static001.geekbang.org/resource/image/4a/a6/4a12bc05971799da422e942754c72fa6.jpg?wh=4989*1506\" alt=\"\" title=\"Spark支持AQE前后\"></p><p>以上图为例，我们可以看到，数据表原本有2个分区，Shuffle之后在Reduce阶段产生5个数据分区。由于数据分布不均衡，其中3个分区的数据量很少。对CPU来说，这3个小分区产生的调度开销会是一笔不小的浪费。在Spark支持AQE以前，开发者对此无能为力。现在呢，AQE会自动检测过小的数据分区，并对它们自动合并，根本不需要我们操心了。</p><p><strong>其次是数据倾斜（Data Skew），它在数据分析领域中很常见</strong>，如果处理不当，很容易导致OOM问题。</p><p>比方说，我们要分析每一个微博用户的历史行为。那么，不论是发博量还是互动频次，普通用户与头部用户（明星、网红、大V、媒体）会相差好几个数量级。这个时候，按照用户ID进行分组分析就会产生数据倾斜的问题，而且，同一Executor中的执行任务基本上是平均分配可用内存的。因此，一边是平均的内存供给，一边是有着数量级之差的数据处理需求，数据倾斜严重的Task报出OOM错误也就不足为怪了。</p><p>以往处理数据倾斜问题的时候，往往需要我们在应用中手动“加盐”，也就是强行给倾斜的Key添加随机前缀，通过把Key打散来均衡数据在不同节点上的分布。现在，在数据关联（Joins）的场景中，如果AQE发现某张表存在倾斜的数据分片，就会自动对它做加盐处理，同时对另一张表的数据进行复制。除此之外，开发者在自行盐化之前，还需要先统计每一个Key的倾斜情况再决定盐化的幅度。不过，自从有了AQE，这些糟心事交给它搞定就好了。</p><p><strong>最后，Join策略调整也不难理解。</strong>当两个有序表要进行数据关联的时候，Spark SQL在优化过程中总会选择Sort Merge Join的实现方式。但有一种情况是，其中一个表在排序前需要对数据进行过滤，过滤后的表小到足可以由广播变量容纳。这个时候，Broadcast Join比Sort Merge Join的效率更高。但是，3.0版本之前的优化过程是静态的，做不到动态切换Join方式。</p><p>针对这种情况，AQE会根据运行时的统计数据，去动态地调整Join策略，把之前敲定的Sort Merge Join改为Broadcast Join，从而改善应用的执行性能。</p><p>说了这么多，对于这些天然的优势，我们到底怎么才能利用好呢？首先，<strong>想要利用好Tungsten的优势，你只要抛弃RDD API，采用DataFrame或是Dataset API进行开发就可了</strong>，是不是很简单？</p><p>不过，<strong>AQE功能默认是关闭的，如果我们想要充分利用自动分区合并、自动数据倾斜处理和Join策略调整，需要把相关的配置项打开</strong>，具体的操作如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/75/bbc4f9f8a39990b2c85d9d9bc46e2e75.jpeg?wh=1369*419\" alt=\"\"></p><p>总的来说，通过钨丝计划和AQE，我们完全可以实现低投入、高产出，这其实就是坐享其成的核心原则。除此之外，类似的技巧还有用SQL functions或特征转换算子去取代UDF等等。我非常希望你能在开发过程中去主动探索、汇总这些可以拿来即用的技巧，如果有成果，也期待你在留言区分享。</p><h2>原则二：能省则省、能拖则拖</h2><p>在很多数据处理场景中，为了快速实现业务需求，我往往会对数据清洗、过滤、提取、关联和聚合等多种操作排列组合来完成开发。这些排列组合的执行性能参差不齐、有好有坏，那我们该怎么找到性能更好的实现方式呢？</p><p>这个时候，我们就可以使用第二个原则：<strong>“能省则省、能拖则拖”。省的是数据处理量</strong>，因为节省数据量就等于节省计算负载，更低的计算负载自然意味着更快的处理速度；<strong>拖的是Shuffle操作</strong>，因为对于常规数据处理来说，计算步骤越靠后，需要处理的数据量越少，Shuffle操作执行得越晚，需要落盘和分发的数据量就越少，更低的磁盘与网络开销自然意味着更高的执行效率。</p><p>实现起来我们可以分3步进行：</p><ul>\n<li><strong>尽量把能节省数据扫描量和数据处理量的操作往前推；</strong></li>\n<li><strong>尽力消灭掉Shuffle，省去数据落盘与分发的开销；</strong></li>\n<li><strong>如果不能干掉Shuffle，尽可能地把涉及Shuffle的操作拖到最后去执行</strong>。</li>\n</ul><p>接下来，我们再通过一个例子来对这个原则加深理解。</p><p>这次的业务背景很简单，我们想要得到两个共现矩阵，一个是物品、用户矩阵，另一个是物品、用户兴趣矩阵。得到这两个矩阵之后，我们要尝试用矩阵分解的方法去计算物品、用户和用户兴趣这3个特征的隐向量（Latent Vectors，也叫隐式向量），这些隐向量最终会用来构建机器学习模型的特征向量（Feature Vectors）。</p><p>基于这样的业务背景，代码需要实现的功能是读取用户访问日志，然后构建出这两个矩阵。访问日志以天为单位保存在Parquet格式的文件中，每条记录包含用户ID、物品ID、用户兴趣列表、访问时间、用户属性和物品属性等多个字段。我们需要读取日志记录，先用distinct对记录去重，然后用explode将兴趣列表展开为单个兴趣，接着提取相关字段，最后按照用户访问频次对记录进行过滤并再次去重，最终就得到了所需的共现矩阵。</p><p>拿到这样的业务需求之后，你会怎么实现呢？同学小A看完之后，二话不说就实现了如下的代码：</p><pre><code>val dates: List[String] = List(&quot;2020-01-01&quot;, &quot;2020-01-02&quot;, &quot;2020-01-03&quot;)\nval rootPath: String = _\n \n//读取日志文件，去重、并展开userInterestList\ndef createDF(rootPath: String, date: String): DataFrame = {\nval path: String = rootPath + date\nval df = spark.read.parquet(path)\n.distinct\n.withColumn(&quot;userInterest&quot;, explode(col(&quot;userInterestList&quot;)))\ndf\n}\n \n//提取字段、过滤，再次去重，把多天的结果用union合并\nval distinctItems: DataFrame = dates.map{\ncase date: String =&gt;\nval df: DataFrame = createDF(rootPath, date)\n.select(&quot;userId&quot;, &quot;itemId&quot;, &quot;userInterest&quot;, &quot;accessFreq&quot;)\n.filter(&quot;accessFreq in ('High', 'Medium')&quot;)\n.distinct\ndf\n}.reduce(_ union _)\n</code></pre><p>我们不妨来一起分析一下这段代码，其中主要的操作有4个：用distinct去重、用explode做列表展开、用select提取字段和用filter过滤日志记录。因为后3个操作全部是在Stage内完成去内存计算，只有distinct会引入Shuffle，所以我们要重点关注它。distinct一共被调用了两次，一次是读取日志内容之后去重，另一次是得到所需字段后再次去重。</p><p>首先，我们把目光集中到第一个distinct操作上：<strong>在createDF函数中读取日志记录之后，立即调用distinct去重</strong>。要知道，日志记录中包含了很多的字段，distinct引入的Shuffle操作会触发所有数据记录，以及记录中所有字段在网络中全量分发，但我们最终需要的是用户粘性达到一定程度的数据记录，而且只需要其中的用户ID、物品ID和用户兴趣这3个字段。因此，这个distinct实际上在集群中分发了大量我们并不需要的数据，这无疑是一个巨大的浪费。</p><p>接着，我们再来看第二个distinct操作：<strong>对数据进行展开、抽取、过滤之后，再对记录去重</strong>。这次的去重和第一次大不相同，它涉及的Shuffle操作所分发的数据记录没有一条是多余的，记录中仅包含共现矩阵必需的那几个字段。</p><p>这个时候我们发现，两个distinct操作都是去重，目的一样，但是第二个distinct操作比第一个更精准，开销也更少，所以我们可以去掉第一个distinct操作。</p><p>这样一来，我们也就消灭了一个会引入全量数据分发的Shuffle操作，这个改进对执行性能自然大有裨益。不过，按下葫芦浮起瓢，把第一个distinct干掉之后，紧随其后的explode就浮出了水面。尽管explode不会引入Shuffle，但在内存中展开兴趣列表的时候，它还是会夹带着很多如用户属性、物品属性等等我们并不需要的字段。</p><p>因此，我们得把过滤和列剪枝这些可以节省数据访问量的操作尽可能地往前推，把计算开销较大的操作如Shuffle尽量往后拖，从而在整体上降低数据处理的负载和开销。基于这些分析，我们就有了改进版的代码实现，如下所示。</p><pre><code>val dates: List[String] = List(&quot;2020-01-01&quot;, &quot;2020-01-02&quot;, &quot;2020-01-03&quot;)\nval rootPath: String = _\n \nval filePaths: List[String] = dates.map(rootPath + _)\n \n/**\n一次性调度所有文件\n先进行过滤和列剪枝\n然后再展开userInterestList\n最后统一去重\n*/\nval distinctItems = spark.read.parquet(filePaths: _*)\n.filter(&quot;accessFreq in ('High', 'Medium'))&quot;)\n.select(&quot;userId&quot;, &quot;itemId&quot;, &quot;userInterestList&quot;)\n.withColumn(&quot;userInterest&quot;, explode(col(&quot;userInterestList&quot;)))\n.select(&quot;userId&quot;, &quot;itemId&quot;, &quot;userInterest&quot;)\n.distinct\n</code></pre><p>在这份代码中，所有能减少数据访问量的操作如filter、select全部被推到最前面，会引入Shuffle的distinct算子则被拖到了最后面。经过实验对比，两版代码在运行时的执行性能相差一倍。因此你看，遵循“能省则省、能拖则拖”的开发原则，往往能帮你避开很多潜在的性能陷阱。</p><h2>原则三：跳出单机思维模式</h2><p>那么，开发者遵循上述的两个原则去实现业务逻辑，是不是就万事大吉、高枕无忧了呢？当然不是，我们再来看下面的例子。</p><p>为了生成训练样本，我们需要对两张大表进行关联。根据“能省则省、能拖则拖”原则，我们想把其中一张表变小，把Shuffle Join转换为Broadcast Join，这样一来就可以把Shuffle的环节省掉了。</p><p>尽管两张表的尺寸都很大，但右表的Payload只有一列，其他列都是Join keys，所以只要我们把Join keys干掉，右表完全可以放到广播变量里。但是，直接干掉Join keys肯定不行，因为左右表数据关联是刚需。那么，我们能否换个方式把它们关联在一起呢？</p><p>受Hash Join工作原理的启发，我们想到可以把所有的Join keys拼接在一起，然后用哈希算法生成一个固定长度的字节序列，把它作为新的Join key。这样一来，右表中原始的Join keys就可以拿掉，右表的尺寸也可以大幅削减，小到可以放到广播变量里。同时，新的Join key还能保证左右表中数据的关联关系保持不变，一举两得。</p><p>为了对拼接的Join keys进行哈希运算，我们需要事先准备好各种哈希算法，然后再转换左、右表。接到这样的需求之后，同学小A立马在右表上调用了map算子，并且在map算子内通过实例化Util类获取哈希算法，最后在拼接的Join keys上进行哈希运算完成了转换。具体的代码如下所示。</p><pre><code>import java.security.MessageDigest\n \nclass Util {\nval md5: MessageDigest = MessageDigest.getInstance(&quot;MD5&quot;)\nval sha256: MessageDigest = _ //其他哈希算法\n}\n \nval df: DataFrame = _\nval ds: Dataset[Row] = df.map{\ncase row: Row =&gt;\nval util = new Util()\nval s: String = row.getString(0) + row.getString(1) + row.getString(2)\nval hashKey: String = util.md5.digest(s.getBytes).map(&quot;%02X&quot;.format(_)).mkString\n(hashKey, row.getInt(3))\n}\n</code></pre><p>仔细观察，我们发现这份代码其实还有可以优化的空间。要知道，map算子所囊括的计算是以数据记录（Data Record）为操作粒度的。换句话说，分布式数据集涉及的每一个数据分片中的每一条数据记录，都会触发map算子中的计算逻辑。因此，我们必须谨慎对待map算子中涉及的计算步骤。很显然，map算子之中应该仅仅包含与数据转换有关的计算逻辑，与数据转换无关的计算，都应该提炼到map算子之外。</p><p>反观上面的代码，map算子内与数据转换直接相关的操作，是拼接Join keys和计算哈希值。但是，实例化Util对象仅仅是为了获取哈希函数而已，与数据转换无关，因此我们需要把它挪到map算子之外。</p><p>只是一行语句而已，我们至于这么较真吗？还真至于，这个实例化的动作每条记录都会触发一次，如果整个数据集有千亿条样本，就会有千亿次的实例化操作！差之毫厘谬以千里，一个小小的计算开销在规模化效应之下会被放大无数倍，演化成不容小觑的性能问题。</p><pre><code>val ds: Dataset[Row] = df.mapPartitions(iterator =&gt; {\nval util = new Util()\nval res = iterator.map{\ncase row=&gt;{\nval s: String = row.getString(0) + row.getString(1) + row.getString(2)\nval hashKey: String = util.md5.digest(s.getBytes).map(&quot;%02X&quot;.format(_)).mkString\n(hashKey, row.getInt(3)) }}\nres\n})\n</code></pre><p>类似这种忽视实例化Util操作的行为还有很多，比如在循环语句中反复访问RDD，用临时变量缓存数据转换的中间结果等等。这种<strong>不假思索地直入面向过程编程，忽略或无视分布式数据实体的编程模式，我们把它叫做单机思维模式。</strong>我们在RDD那一讲也说过，单机思维模式会让开发者在分布式环境中，无意识地引入巨大的计算开销。</p><p>但你可能会说：“单机思维模式随处可见，防不胜防，我们该怎么跳出去呢？”</p><p>冰冻三尺、非一日之寒，既然是一种思维模式，那么它自然不是一天、两天就能形成的，想要摆脱它自然也不是一件容易的事情。不过，关于跳出单机思维，我这里也有个小技巧要分享给你。当然，这可能需要一点想象力。</p><p>你还记得土豆工坊吗？每当你准备开发应用的时候，你都可以在脑海里构造一个土豆工坊，把你需要定义的分布式数据集，安置到工坊流水线上合适的位置。当你需要处理某个数据集的时候，不妨花点时间想一想，得到当前这种土豆形态都需要哪些前提。持续地在脑海里构造土豆工坊，可以持续地加深你对分布式计算过程的理解。假以时日，我相信你一定能摆脱单机思维模式！</p><h2>小结</h2><p>在日常的开发工作中，遵循这3个原则，不仅能让你的应用在性能方面有个好的起点，还能让你有意无意地去探索、拓展更多的调优技巧，从而由点及面地积累调优经验。</p><p>首先，遵循“坐享其成”的原则，你就可以通过设置相关的配置项，或是调用相应的API充分享用Spark自身带来的性能优势。比如，使用DataFrame或是Dataset API做开发，你就可以坐享Catalyst和Tungsten的各种优化机制。再比如，使用Parquet、ORC等文件格式，去坐享谓词下推带来的数据读取效率。</p><p>其次，如果你能够坚持“能省则省、能拖则拖”，尽量把节省数据扫描量和数据处理量的操作往前推，尽可能地把涉及Shuffle的操作拖延到最后去执行，甚至是彻底消灭Shuffle，你自然能够避开很多潜在的性能陷阱。</p><p>最后，在日常的开发工作中，我们要谨防单机思维模式，摆脱单机思维模式有利于培养我们以性能为导向的开发习惯。我们可以在开发应用的过程中运用想象力，在脑海中构造一个土豆工坊。把每一个分布式数据集都安插到工坊的流水线上。在尝试获取数据集结果的时候，结合我们在原理篇讲解的调度系统、存储系统和内存管理，去进一步想象要得到计算结果，整个工坊都需要做哪些事情，会带来哪些开销。</p><p>最后的最后，我们再来说说归纳这件事的意义和价值。我们之所以把各种开发技巧归纳为开发原则，一方面是遵循这些原则，你能在不知不觉中避开很多性能上的坑。但更重要的是，从这些原则出发，向外推演，<strong>我们往往能发现更多的开发技巧，从而能拓展自己的“常规操作”边界，做到举一反三，真正避免“调优思路枯竭”的窘境</strong>。</p><h2>每日一练</h2><ol>\n<li>针对我们今天讲的3个原则，你还能想到哪些案例？</li>\n<li>除了这3个原则外，你觉得是否还有其他原则需要开发者特别留意？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","comments":[{"had_liked":false,"id":308495,"user_name":"Sean","can_delete":false,"product_type":"c1","uid":2162751,"ip_address":"","ucode":"69234046BFD81B","user_header":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","comment_is_top":false,"comment_ctime":1629644676,"is_pvip":false,"replies":[{"id":"111719","content":"好问题，是这样的：<br>1. 谓词下推本身，不依赖于任何文件存储格式，它本身就是Spark SQL的优化策略，DataFrame里面如果包含filter一类的操作，他们就会尽可能地被推到执行计划的最下面。<br>2. 但是，谓词下推的效果，和文件存储格式有关。假设是CSV这种行存格式，那么谓词下推顶多是在整个执行计划的shuffle之前，降低数据量大小。但如果是orc、Parquet这种列存文件，谓词下推能直接推到文件扫描上去，直接在磁盘扫描阶段，就降低文件扫描量，降低i&#47;o开销，从而提升执行性能。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629704107,"ip_address":"","comment_id":308495,"utype":1}],"discussion_count":1,"race_medal":0,"score":"78939056004","product_id":100073401,"comment_content":"老师上文提到了&quot;使用 Parquet、ORC 等文件格式，去坐享谓词下推带来的数据读取效率&quot;,应该如何理解,莫非谓词下推依耐与于文件存储格式吗","like_count":19,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525505,"discussion_content":"好问题，是这样的：\n1. 谓词下推本身，不依赖于任何文件存储格式，它本身就是Spark SQL的优化策略，DataFrame里面如果包含filter一类的操作，他们就会尽可能地被推到执行计划的最下面。\n2. 但是，谓词下推的效果，和文件存储格式有关。假设是CSV这种行存格式，那么谓词下推顶多是在整个执行计划的shuffle之前，降低数据量大小。但如果是orc、Parquet这种列存文件，谓词下推能直接推到文件扫描上去，直接在磁盘扫描阶段，就降低文件扫描量，降低i/o开销，从而提升执行性能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629704107,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286126,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1617161289,"is_pvip":false,"replies":[{"id":"103888","content":"对，在这个场景下，先用distinct节省数据量更合适。咱们能省则省、能拖则拖是一般性原则哈。不过先用distinct其实还是遵循了能省则省的原则。get到核心思想就好，灵活应用～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617190202,"ip_address":"","comment_id":286126,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27386965065","product_id":100073401,"comment_content":"老师如果两个超大表，但是一张表重复数据很多，那是不是先做distinct，再join会好一些？毕竟虽然distinct会shuffle但最后join的数据量也是成倍减少的","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517876,"discussion_content":"对，在这个场景下，先用distinct节省数据量更合适。咱们能省则省、能拖则拖是一般性原则哈。不过先用distinct其实还是遵循了能省则省的原则。get到核心思想就好，灵活应用～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617190202,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292021,"user_name":"猿鸽君","can_delete":false,"product_type":"c1","uid":1991951,"ip_address":"","ucode":"8562D8C5AD3D1E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/65/0f/7b9f27f2.jpg","comment_is_top":false,"comment_ctime":1620646638,"is_pvip":false,"replies":[{"id":"105818","content":"这个PR描述的问题，是说：<br><br>当你读取Parquet文件的时候，Parquet filter（就是用谓词下推的时候，需要用到的功能）是需要做序列化的，在这个PR之前，序列化是在Driver端做得，但是，Driver的做法不是线程安全的，所以存在重复序列化的隐患，也就是你看到的报错。<br><br>这个PR解决的问题，就是把Parquet filters的序列化，从Dirver，挪到Executors，从而避免刚刚说的线程安全问题。<br><br>因此，如果你的业务问题，和这个PR的描述是一致的，那么，升级Spark（比如到2.3），就可以解决这个问题~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620798876,"ip_address":"","comment_id":292021,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18800515822","product_id":100073401,"comment_content":"https:&#47;&#47;github.com&#47;apache&#47;spark&#47;pull&#47;21086。老师您好，请问老师知道这个可能是什么原因导致的吗？我用2.2.0版本就会出这个错。通过spark ui也看不到有task failed。看起来就像被强制终止了。","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519639,"discussion_content":"这个PR描述的问题，是说：\n\n当你读取Parquet文件的时候，Parquet filter（就是用谓词下推的时候，需要用到的功能）是需要做序列化的，在这个PR之前，序列化是在Driver端做得，但是，Driver的做法不是线程安全的，所以存在重复序列化的隐患，也就是你看到的报错。\n\n这个PR解决的问题，就是把Parquet filters的序列化，从Dirver，挪到Executors，从而避免刚刚说的线程安全问题。\n\n因此，如果你的业务问题，和这个PR的描述是一致的，那么，升级Spark（比如到2.3），就可以解决这个问题~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620798876,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":297370,"user_name":"John.Xiong","can_delete":false,"product_type":"c1","uid":2004094,"ip_address":"","ucode":"5E46B21C68A7D5","user_header":"https://static001.geekbang.org/account/avatar/00/1e/94/7e/98c2a436.jpg","comment_is_top":false,"comment_ctime":1623488577,"is_pvip":false,"replies":[{"id":"108103","content":"好问题，你说的对，hash之后，确实存在哈希冲突的隐患，具体细节可以参考27讲提到的思路：https:&#47;&#47;time.geekbang.org&#47;column&#47;article&#47;373089。<br><br>简单来说，至少有两种办法来避免哈希冲突：<br>1）使用两种算法做哈希，最终把哈希值拼接在一起，同一个Value，经过两种不同哈希算法，得到完全一样的哈希值的概率，几乎为0。<br>2）不使用哈希的办法，而是给join keys每个字段维护一个字典，每个字段值在字典内对应一个唯一的整数。拿到每个字段指定的种整数，然后组装起来，作为新的join key。这个是 @Fendora范东_ 同学提供的方法，我觉得比哈希的方法更好，既缩短了Join Key，又不存在哈希冲突的问题。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1623747684,"ip_address":"","comment_id":297370,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14508390465","product_id":100073401,"comment_content":"老师，您说的两个表通过把多个字段拼接后hash成一个字段关联，但是hash不是只有碰撞问题吗？万一两个不同组合弄成了一个hash值不是会导致问题吗？我对碰撞不太熟悉，可能说的不对，请老师指教<br>","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521810,"discussion_content":"好问题，你说的对，hash之后，确实存在哈希冲突的隐患，具体细节可以参考27讲提到的思路：https://time.geekbang.org/column/article/373089。\n\n简单来说，至少有两种办法来避免哈希冲突：\n1）使用两种算法做哈希，最终把哈希值拼接在一起，同一个Value，经过两种不同哈希算法，得到完全一样的哈希值的概率，几乎为0。\n2）不使用哈希的办法，而是给join keys每个字段维护一个字典，每个字段值在字典内对应一个唯一的整数。拿到每个字段指定的种整数，然后组装起来，作为新的join key。这个是 @Fendora范东_ 同学提供的方法，我觉得比哈希的方法更好，既缩短了Join Key，又不存在哈希冲突的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623747684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291185,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620102102,"is_pvip":false,"replies":[{"id":"105484","content":"补充的非常好~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620116749,"ip_address":"","comment_id":291185,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14505003990","product_id":100073401,"comment_content":"其实老师总结的已经很全面了。这里推荐两个比较通用的调优小技巧：<br>1、 Spark默认使用的是Java serialization序列化方式，我们可以考虑使用Kryo serialization序列化的方式，不过会有一些限制，比如不是支持所有的序列化类型，需要手动注册要序列化的类。<br>2、 尽量使用占用空间小的数据结构。比如，能使用基本数据类型的就用基本数据类型，不要用对应的包装类（int——&gt;Integer），能用int的就不要用String，String占用的空间要大的多。","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519394,"discussion_content":"补充的非常好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620116749,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287201,"user_name":"Cohen","can_delete":false,"product_type":"c1","uid":2199749,"ip_address":"","ucode":"42E341920327CB","user_header":"https://static001.geekbang.org/account/avatar/00/21/90/c5/ee3d6247.jpg","comment_is_top":false,"comment_ctime":1617815010,"is_pvip":false,"replies":[{"id":"104281","content":"可以的，没问题～ 后面搞一个～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617843047,"ip_address":"","comment_id":287201,"utype":1}],"discussion_count":3,"race_medal":0,"score":"14502716898","product_id":100073401,"comment_content":"老师，能否弄个GitHub 配套代码案例","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518229,"discussion_content":"可以的，没问题～ 后面搞一个～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617843047,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375641,"discussion_content":"Github地址：https://github.com/wulei-bj-cn/potatoes.git","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1621782633,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404237,"discussion_content":"棒棒棒！标星星了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634263932,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286131,"user_name":"October","can_delete":false,"product_type":"c1","uid":1137879,"ip_address":"","ucode":"CEDA78F4A5F8B1","user_header":"https://static001.geekbang.org/account/avatar/00/11/5c/d7/e4673fde.jpg","comment_is_top":false,"comment_ctime":1617162762,"is_pvip":false,"replies":[{"id":"103887","content":"你说的没错，默认是关闭的，这块细节咱们在后面的配置项章节和内存视角会有详细展开～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617190046,"ip_address":"","comment_id":286131,"utype":1}],"discussion_count":4,"race_medal":0,"score":"14502064650","product_id":100073401,"comment_content":"享受Tungsten带来的堆外内存的红利时，除了使用dataframe或dataset API之外，还需要在sparkconf中开启堆外内存吧","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517878,"discussion_content":"你说的没错，默认是关闭的，这块细节咱们在后面的配置项章节和内存视角会有详细展开～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617190046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375642,"discussion_content":"可以的，Tungsten——钨丝计划，是一系列的优化机制，堆外内存只是其中之一，Tungsten主要包括二进制数据结构设计，缓存感知，全阶段代码生成等等。October这里特地问得是堆外的事情。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621783506,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1239815,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKkpApOjdIb81ZHxeAup1IGH97eaD8oiawlCtUJdvct1AP6UfmmpYlE6r25tNM5cgOCgM3oAzpic5Aw/132","nickname":"Sandy","note":"","ucode":"3097343BDAB831","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":378531,"discussion_content":"老师，spark sql api可以享受tungsten的堆外内存红利么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623272826,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":375642,"ip_address":""},"score":378531,"extra":""}]},{"author":{"id":1789481,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg","nickname":"静心","note":"","ucode":"B80DE4B5C923D3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369082,"discussion_content":"不开启堆外内存，仅仅使用sparksql api，就不能享受Tungsten内存管理器的优势了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618924317,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":300169,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1625031926,"is_pvip":false,"replies":[{"id":"108830","content":"好问题~ 这部分细节咱们在27讲“大表Join小表”有展开哈，不妨看一看。<br><br>简单来说，就是左右表的的Join Keys，都做相同的处理，比如文中提到的<br>1. 拼接Join Keys<br>2. 计算哈希值<br><br>就是左右两张表，都做同样的操作，这样，每张表都会多出来一个新的字段，比如把它叫做Key Hash，那么两张表就可以用新的Key Hash来做关联了~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1625035788,"ip_address":"","comment_id":300169,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5919999222","product_id":100073401,"comment_content":"磊哥好~ 请教个基础问题，文章有一段话：“用哈希算法生成一个固定长度的字节序列，把它作为新的 Join key”。我的理解是把右表的字段名用哈希算法形式拼接起来，我在想新的Join key怎么能跟左表的key保持关联关系呢？我在用join连接表的话，这个新的key起到作用关联的作用？<br><br>希望得到磊哥的解惑！~~","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522662,"discussion_content":"好问题~ 这部分细节咱们在27讲“大表Join小表”有展开哈，不妨看一看。\n\n简单来说，就是左右表的的Join Keys，都做相同的处理，比如文中提到的\n1. 拼接Join Keys\n2. 计算哈希值\n\n就是左右两张表，都做同样的操作，这样，每张表都会多出来一个新的字段，比如把它叫做Key Hash，那么两张表就可以用新的Key Hash来做关联了~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625035788,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":381396,"discussion_content":"感谢磊哥的解答^_^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625040027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286065,"user_name":"乐意至极","can_delete":false,"product_type":"c1","uid":2340308,"ip_address":"","ucode":"7AC6778D854482","user_header":"https://static001.geekbang.org/account/avatar/00/23/b5/d4/147abdaa.jpg","comment_is_top":false,"comment_ctime":1617148814,"is_pvip":false,"replies":[{"id":"103889","content":"看上去是shuffle fetch的过程中出了问题，总是没办法成功拉取远端数据，之所以时间长，是因为task总是retry，不过居然最后都试成功了。也就是你的task从那个host不停地拉数据、不停地失败、不停地重试，在第4次fail之前，总能成功。基于这个猜测，我觉得看看那台主机的文件系统。如果文件系统没问题，就要看那台主机的负载，需要double check下是否真的没有大gc、数据是不是真的没有倾斜。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617191136,"ip_address":"","comment_id":286065,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5912116110","product_id":100073401,"comment_content":"老师，你好。我在实际工作中遇到这个问题ERROR RetryingBlockFetcher: Exception while beginning fetch of 520 outstanding blocks (after 1 retries) java.io.IOException: Failed to connect to &lt;HOST&#47;IP&gt;:38000 持续了12小时<br>我有以下观察：<br>1，这个&lt;HOST&#47;IP&gt;上的Executor已经SUCCESS了<br>2，这个持续了12小时的task是process local<br>3，无长时间gc，也无明显倾斜<br><br>排查了很久。。希望老师能给点指点~","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517863,"discussion_content":"看上去是shuffle fetch的过程中出了问题，总是没办法成功拉取远端数据，之所以时间长，是因为task总是retry，不过居然最后都试成功了。也就是你的task从那个host不停地拉数据、不停地失败、不停地重试，在第4次fail之前，总能成功。基于这个猜测，我觉得看看那台主机的文件系统。如果文件系统没问题，就要看那台主机的负载，需要double check下是否真的没有大gc、数据是不是真的没有倾斜。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617191136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363649,"discussion_content":"先试试哈，有啥问题再讨论，如果有什么发现，也欢迎分享出来~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617253862,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2340308,"avatar":"https://static001.geekbang.org/account/avatar/00/23/b5/d4/147abdaa.jpg","nickname":"乐意至极","note":"","ucode":"7AC6778D854482","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363452,"discussion_content":"谢谢~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617197886,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":342124,"user_name":"狗哭","can_delete":false,"product_type":"c1","uid":2725399,"ip_address":"","ucode":"56E48A6FC1298A","user_header":"https://static001.geekbang.org/account/avatar/00/29/96/17/200c21f0.jpg","comment_is_top":false,"comment_ctime":1650021880,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1650021880","product_id":100073401,"comment_content":"老师您好，用临时变量缓存数据中间结果是怎样的呢？为什么会有影响呢？","like_count":0},{"had_liked":false,"id":336643,"user_name":"蔡其斌","can_delete":false,"product_type":"c1","uid":1113478,"ip_address":"","ucode":"FD29A8B0A29E06","user_header":"https://static001.geekbang.org/account/avatar/00/10/fd/86/09bf9657.jpg","comment_is_top":false,"comment_ctime":1646272311,"is_pvip":false,"replies":[{"id":"123249","content":"3.0的时候，AQE刚出来，社区比较谨慎，现在的3.2版本里面，AQE是默认打开了的~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1646725144,"ip_address":"","comment_id":336643,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1646272311","product_id":100073401,"comment_content":"为什么Spark AQE优化默认是关闭的","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555036,"discussion_content":"3.0的时候，AQE刚出来，社区比较谨慎，现在的3.2版本里面，AQE是默认打开了的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646725144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322736,"user_name":"tiankonghewo","can_delete":false,"product_type":"c1","uid":1476427,"ip_address":"","ucode":"7A55A9C17DD9DF","user_header":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","comment_is_top":false,"comment_ctime":1637571398,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637571398","product_id":100073401,"comment_content":"https:&#47;&#47;www.cnblogs.com&#47;tgzhu&#47;p&#47;15211820.html<br>join分析：shuffle hash join、broadcast hash join<br>这篇文章讲的不错","like_count":0},{"had_liked":false,"id":322729,"user_name":"tiankonghewo","can_delete":false,"product_type":"c1","uid":1476427,"ip_address":"","ucode":"7A55A9C17DD9DF","user_header":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","comment_is_top":false,"comment_ctime":1637569875,"is_pvip":false,"replies":[{"id":"117410","content":"好问题~<br><br>讲道理，Spark SQL的优化器，在任意情况下，都能帮助开发者自动优化SQL或是DataFrame写的应用。但是呢，现实情况是，优化器本身还没有那么智能，并不能cover所有的cases，所以说，保险起见，作为开发者，我们还是应该养成良好的开发习惯，遵循一些基本的开发原则~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637986762,"ip_address":"","comment_id":322729,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637569875","product_id":100073401,"comment_content":"两个distinct的疑问<br>spark sql在执行之前,不是有一个优化环节吗? 为什么这里不能自动优化掉?<br>filter应该是在优化范围内的,自动提升到最前边","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533821,"discussion_content":"好问题~\n\n讲道理，Spark SQL的优化器，在任意情况下，都能帮助开发者自动优化SQL或是DataFrame写的应用。但是呢，现实情况是，优化器本身还没有那么智能，并不能cover所有的cases，所以说，保险起见，作为开发者，我们还是应该养成良好的开发习惯，遵循一些基本的开发原则~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637986762,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":318804,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1635428838,"is_pvip":false,"replies":[{"id":"115677","content":"后面的课程还会有更多的代码案例，希望对老弟有所帮助~ <br><br>关于源码的讲解，看吧，等以后有机会，可以讲讲源码，现在杂务缠身，抽不出身来，哎","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635608955,"ip_address":"","comment_id":318804,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1635428838","product_id":100073401,"comment_content":"老师，看了你的课，受益匪浅，每篇文章下的评论都很有价值，值得反复琢磨，期待老师有更多代码优化案例，如果有机会看到老师出源码讲解学习的课程就更好了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529382,"discussion_content":"后面的课程还会有更多的代码案例，希望对老弟有所帮助~ \n\n关于源码的讲解，看吧，等以后有机会，可以讲讲源码，现在杂务缠身，抽不出身来，哎","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635608955,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317920,"user_name":"balabala","can_delete":false,"product_type":"c1","uid":2178875,"ip_address":"","ucode":"C2C2D7A702A27A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/o7oEtlsQ2VRiczVpDGO4GiaUFUT8p1nAbO2IUzWNM1nz3EHPkBJNicnO01Ah6X9oKWQ3Jiay7PUmxqRFNsu5ut2oEQ/132","comment_is_top":false,"comment_ctime":1635055201,"is_pvip":false,"replies":[{"id":"115580","content":"比如说：<br><br>val df: DataFrame = _ &#47;&#47; 某个转换过程的中间结果<br>val temp = df.cache<br><br>然后，在另一个分布式数据集（RDD、或是DataFrame）中，引用temp变量，把temp当作普通变量来用","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635429325,"ip_address":"","comment_id":317920,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1635055201","product_id":100073401,"comment_content":"老师你好。您在单机思维中提到“类似这种忽视实例化 Util 操作的行为还有很多，比如用临时变量缓存数据转换的中间结果等等”。请问可以举例一下吗？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529029,"discussion_content":"比如说：\n\nval df: DataFrame = _ // 某个转换过程的中间结果\nval temp = df.cache\n\n然后，在另一个分布式数据集（RDD、或是DataFrame）中，引用temp变量，把temp当作普通变量来用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635429325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286288,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1617240585,"is_pvip":false,"replies":[{"id":"103931","content":"yarn的node manager在创建executor的时候，会预留一部分资源给yarn自己，因此有一部分overhead，所以实际allocate给executor的内存，会比你指定的，要少一些，不过还好，没有少很多。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617276484,"ip_address":"","comment_id":286288,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617240585","product_id":100073401,"comment_content":"老师yarn上实际跑的资源总是和自己spark-submit里提交的资源不一样，会略小一些，这是为什么呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517935,"discussion_content":"yarn的node manager在创建executor的时候，会预留一部分资源给yarn自己，因此有一部分overhead，所以实际allocate给executor的内存，会比你指定的，要少一些，不过还好，没有少很多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617276484,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286207,"user_name":"布兰特","can_delete":false,"product_type":"c1","uid":1526892,"ip_address":"","ucode":"EE3316C188EC3C","user_header":"https://static001.geekbang.org/account/avatar/00/17/4c/6c/11fb0f1d.jpg","comment_is_top":false,"comment_ctime":1617191156,"is_pvip":true,"replies":[{"id":"103893","content":"一般的倾斜可以交给aqe，不过aqe处理倾斜本身也有局限性，这个我们后面aqe那一讲再展开哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617238602,"ip_address":"","comment_id":286207,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617191156","product_id":100073401,"comment_content":"所以，开启AQE之后，就不用手动处理数据倾斜了？完全的扔给Spark是嘛","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517907,"discussion_content":"一般的倾斜可以交给aqe，不过aqe处理倾斜本身也有局限性，这个我们后面aqe那一讲再展开哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617238602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286158,"user_name":"蠟筆小噺","can_delete":false,"product_type":"c1","uid":1264412,"ip_address":"","ucode":"694ABA92BC48C7","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJN8s3YnzyDRCeg73yzglRgQgk581uIY1FRFO01GibMro4Mbxk58rRgulZTKrSGnd8ZD6RHY8uQj2A/132","comment_is_top":false,"comment_ctime":1617172395,"is_pvip":false,"replies":[{"id":"103896","content":"为啥不都用dataframe呢？rdd开发框架，享受不到catalyst和tungsten的性能红利，最好都在dataframe api下去开发","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617239026,"ip_address":"","comment_id":286158,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617172395","product_id":100073401,"comment_content":"在非Shuffle部分用RDD，在遇到Shufle部分调用toDF转换为DataFrame，这种方式可取吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517887,"discussion_content":"为啥不都用dataframe呢？rdd开发框架，享受不到catalyst和tungsten的性能红利，最好都在dataframe api下去开发","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617239026,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}