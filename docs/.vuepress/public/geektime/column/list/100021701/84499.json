{"id":84499,"title":"36丨数据分析算法篇答疑","content":"<p>算法篇更新到现在就算结束了，因为这一模块比较难，所以大家提出了形形色色的问题。我总结了同学们经常遇到的问题，精选了几个有代表性的来作为答疑。没有列出的问题，我也会在评论区陆续解答。</p><h2>17-19篇：决策树</h2><h3>答疑1：在探索数据的代码中，print(boston.feature_names)有什么作用？</h3><p>boston是sklearn自带的数据集，里面有5个keys，分别是data、target、feature_names、DESCR和filename。其中data代表特征矩阵，target代表目标结果，feature_names代表data对应的特征名称，DESCR是对数据集的描述，filename对应的是boston这个数据在本地的存放文件路径。</p><p>针对sklearn中自带的数据集，你可以查看下加载之后，都有哪些字段。调用方法如下：</p><pre><code>boston=load_boston()\nprint(boston.keys())\n</code></pre><p>通过boston.keys()你可以看到，boston数据集的字段包括了[‘data’, ‘target’, ‘feature_names’, ‘DESCR’, ‘filename’]。</p><h3>答疑2：决策树的剪枝在sklearn中是如何实现的？</h3><p>实际上决策树分类器，以及决策树回归器（对应DecisionTreeRegressor类）都没有集成剪枝步骤。一般对决策树进行缩减，常用的方法是在构造DecisionTreeClassifier类时，对参数进行设置，比如max_depth表示树的最大深度，max_leaf_nodes表示最大的叶子节点数。</p><!-- [[[read_end]]] --><p>通过调整这两个参数，就能对决策树进行剪枝。当然也可以自己编写剪枝程序完成剪枝。</p><h3>答疑3：对泰坦尼克号的乘客做生存预测的时候，Carbin字段缺失率分别为77%和78%，Age和Fare字段有缺失值，是如何判断出来的？</h3><p>首先我们需要对数据进行探索，一般是将数据存储到DataFrame中，使用df.info()可以看到表格的一些具体信息，代码如下：</p><pre><code># 数据加载\ntrain_data = pd.read_csv('./Titanic_Data/train.csv')\ntest_data = pd.read_csv('./Titanic_Data/test.csv')\nprint(train_data.info())\nprint(test_data.info())\n</code></pre><p>这是运行结果：</p><pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.6+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\nPassengerId    418 non-null int64\nPclass         418 non-null int64\nName           418 non-null object\nSex            418 non-null object\nAge            332 non-null float64\nSibSp          418 non-null int64\nParch          418 non-null int64\nTicket         418 non-null object\nFare           417 non-null float64\nCabin          91 non-null object\nEmbarked       418 non-null object\ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\nNone\n</code></pre><p>你可以关注下运行结果中Carbin的部分，你能看到在训练集中一共891行数据，Carbin有数值的只有204个，那么缺失率为1-204/891=77%，同样在测试集中一共有418行数据，Carbin有数值的只有91个，那么缺失率为1-91/418=78%。</p><p>同理你也能看到在训练集中，Age字段有缺失值。在测试集中，Age字段和Fare字段有缺失值。</p><h3>答疑4：在用pd.read_csv时报错“UnicodeDecodeError utf-8 codec can’t decode byte 0xcf in position 15: invalid continuation byte”是什么问题？</h3><p>一般在Python中遇到编码问题，尤其是中文编码出错，是比较常见的。有几个常用的解决办法，你可以都试一下：</p><ol>\n<li>\n<p>将read_csv中的编码改为gb18030，代码为：data = pd.read_csv(filename, encoding = ‘gb18030’)。</p>\n</li>\n<li>\n<p>代码前添加# -<em>- coding: utf-8 -</em>-。</p>\n</li>\n</ol><p>我说一下gb18030和utf-8的区别。utf-8是国际通用字符编码，gb18030是新出的国家标准，不仅包括了简体和繁体，也包括了一些不常见的中文，相比于utf-8更全，容错率更高。</p><p>为了让编辑器对中文更加支持，你也可以在代码最开始添加# -<em>- coding: utf-8 -</em>- 的说明，再结合其他方法解决编码出错的问题。</p><h2>第20-21篇：朴素贝叶斯</h2><h3>答疑1：在朴素贝叶斯中，我们要统计的是属性的条件概率，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。这个我算的是3/5，跟老师的不一样，老师可以给一下详细步骤吗？</h3><p>不少同学都遇到了这个问题，我来统一解答下。</p><p>这里我们需要运用贝叶斯公式（我在文章中也给出了），即：</p><p><img src=\"https://static001.geekbang.org/resource/image/88/a1/88f2981f938fac38980f1325fe7046a1.png?wh=455*76\" alt=\"\"><br>\n假设A代表白棋子，B1代表A盒，B2代表B盒。带入贝叶斯公式，我们可以得到：</p><p><img src=\"https://static001.geekbang.org/resource/image/63/8a/633e213385195fb958520f513a3d9f8a.png?wh=564*121\" alt=\"\"><br>\n其中$P(B_{1})$代表A盒的概率，7个棋子，A盒有4个，所以$P(B_{1})$=4/7。</p><p>$P(B_{2})$代表B盒的概率，7个棋子，B盒有3个，所以$P(B_{2})$=3/7。</p><p>最终求取出来的是白色的棋子，那么它属于 A盒的概率$P(B_{1}|A)$= 2/3。</p><h2>22-23篇：SVM算法</h2><h3>答疑1：SVM多分类器是集成算法么？</h3><p>SVM算法最初是为二分类问题设计的，如果我们想要把SVM分类器用于多分类问题，常用的有一对一方法和一对多方法（我在文章中有介绍到）。</p><p>集成学习的概念你这样理解：通过构造和使用多个分类器完成分类任务，也就是我们所说的博取众长。</p><p>以上是SVM多分类器和集成算法的概念，关于SVM多分类器是否属于集成算法，我认为你需要这样理解。</p><p>在SVM的多分类问题中，不论是采用一对一，还是一对多的方法，都会构造多个分类器，从这个角度来看确实在用集成学习的思想，通过这些分类器完成最后的学习任务。</p><p>不过我们一般所说的集成学习，需要有两个基本条件：</p><ol>\n<li>\n<p>每个分类器的准确率要比随机分类的好，即准确率大于50%；</p>\n</li>\n<li>\n<p>每个分类器应该尽量相互独立，这样才能博采众长，否则多个分类器一起工作，和单个分类器工作相差不大。</p>\n</li>\n</ol><p>所以你能看出，在集成学习中，虽然每个弱分类器性能不强，但都可以独立工作，完成整个分类任务。而在SVM多分类问题中，不论是一对一，还是一对多的方法，每次都在做一个二分类问题，并不能直接给出多分类的结果。</p><p>此外，当我们谈集成学习的时候，通常会基于单个分类器之间是否存在依赖关系，进而分成Boosting或者Bagging方法。如果单个分类器存在较强的依赖关系，需要串行使用，也就是我们所说的Boosting方法。如果单个分类器之间不存在强依赖关系，可以并行工作，就是我们所说的Bagging或者随机森林方法（Bagging的升级版）。</p><p>所以，一个二分类器构造成多分类器是采用了集成学习的思路，不过在我们谈论集成学习的时候，通常指的是Boosing或者Bagging方法，因为需要每个分类器（弱分类器）都有分类的能力。</p><h2>26-27篇：K-Means</h2><h3>答疑1：我在给20支亚洲球队做聚类模拟的时候，使用K-Means算法需要重新计算这三个类的中心点，最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类。对中心点的重新计算不太理解。</h3><p>实际上是对属于这个类别的点的特征值求平均，即为新的中心点的特征值。</p><p>比如都属于同一个类别里面有10个点，那么新的中心点就是这10个点的中心点，一种简单的方式就是取平均值。比如文章中的足球队一共有3个指标，每个球队都有这三个指标的特征值，那么新的中心点，就是取这个类别中的这些点的这三个指标特征值的平均值。</p><h2>28-29篇：EM聚类</h2><h3>答疑1：关于EM聚类初始参数设置的问题，初始参数随机设置会影响聚类的效果吗。会不会初始参数不对，聚类就出错了呢？</h3><p>实际上只是增加了迭代次数而已。</p><p>EM算法的强大在于它的鲁棒性，或者说它的机制允许初始化参数存在误差。</p><p>举个例子，EM的核心是通过参数估计来完成聚类。如果你想要把菜平均分到两个盘子中，一开始A盘的菜很少，B盘的菜很多，我们只要通过EM不断迭代，就会让两个盘子的菜量一样多，只是迭代的次数多一些而已。</p><p>另外多说一句，我们学的这些数据挖掘的算法，不论是EM、Adaboost还是K-Means，最大的价值都是它们的思想。我们在使用工具的时候都会设置初始化参数，比如在K-Means中要选择中心点，即使一开始只是随机选择，最后通过迭代都会得到不错的效果。所以说学习这些算法，就是学习它们的思想。</p><h2>30-31篇：关联规则挖掘</h2><h3>答疑1：看不懂构造FP树的过程，面包和啤酒为什么会拆分呢？</h3><p>FP-Growth中有一个概念叫条件模式基。它在创建FP树的时候还用不上，我们主要通过扫描整个数据和项头表来构造FP树。条件模式基用于挖掘频繁项。通过找到每个项（item）的条件模式基，递归挖掘频繁项集。</p><h3>答疑2：不怎么会找元素的XPath路径。</h3><p>XPath的作用大家应该都能理解，具体的使用其实就是经验和技巧的问题。</p><p>我的方法就是不断尝试，而且XPath有自己的规则，绝大部分的情况下都是以//开头，因为想要匹配所有的元素。我们也可以找一些关键的特征来进行匹配，比如class='item-root’的节点，或者id='root’都是很好的特征。通过观察id或class，也可以自己编写XPath，这样写的XPath会更短。总之，都是要不断尝试，才能找到自己想要找的内容，寻找XPath的过程就是一个找规律的过程。</p><h3>答疑3：最小支持度可以设置小一些，如果最小支持度小，那么置信度就要设置得相对大一点，不然即使提升度高，也有可能是巧合。这个参数跟数据量以及项的数量有关。理解对吗？</h3><p>一般来说最小置信度都会大一些，比如1.0，0.9或者0.8。最小支持度和数据集大小和特点有关，可以尝试一些数值来观察结果，比如0.1，0.5。</p><h2>34-35篇：AdaBoost算法</h2><h3>答疑1：关于$Z_{k}$和$y_{i}$的含义</h3><p>第 k+1 轮的样本权重，是根据该样本在第 k 轮的权重以及第 k 个分类器的准确率而定，具体的公式为：</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/cd/1c812efcf6173652cf152f2ad25987cd.png?wh=532*89\" alt=\"\"><br>\n其中$Z_{k}$, $y_{i}$代表什么呢？</p><p>$Z_{k}$代表规范化因子，我们知道第K+1轮样本的权重为：</p><p><img src=\"https://static001.geekbang.org/resource/image/ce/c8/ce857425d5465209bf7cd2529e31e3c8.png?wh=496*82\" alt=\"\"><br>\n为了让样本权重之和为1，我们需要除以规范化因子$Z_{k}$，所以：</p><p><img src=\"https://static001.geekbang.org/resource/image/94/58/940d9b4b8889c668074e1dbaac275f58.png?wh=510*113\" alt=\"\"><br>\n$y_{i}$代表的是目标的结果，我在AdaBoost工作原理之后，列了一个10个训练样本的例子：</p><p><img src=\"https://static001.geekbang.org/resource/image/df/ed/df33bd6ee148b1333b531252e5a936ed.png?wh=1729*128\" alt=\"\"><br>\n你能看到通常我们把X作为特征值，y作为目标结果。在算法篇下的实战练习中，我们一般会把训练集分成train_X和train_y，其中train_X代表特征矩阵，train_y代表目标结果。</p><p>我发现大家对工具的使用和场景比较感兴趣，所以最后留两道思考题。</p><p>第一道题是，在数据挖掘的工具里，我们大部分情况下使用的是sklearn，它自带了一些数据集，你能列举下sklearn自带的数据集都有哪些么？我在第18篇使用print(boston.feature_names)来查看boston数据集的特征名称（数据集特征矩阵的index名称），你能查看下其他数据集的特征名称都是什么吗？列举1-2个sklearn数据集即可。</p><p>第二个问题是，对于数据挖掘算法来说，基础就是数据集。Kaggle网站之所以受到数据科学从业人员的青睐就是因为有众多比赛的数据集，以及社区间的讨论交流。你是否有使用过Kaggle网站的经历，如果有的话，可以分享下你的使用经验吗？如果你是个数据分析的新人，当看到Kaggle网站时，能否找到适合初学者的kernels么(其他人在Kaggle上成功运行的代码分享)？</p><p>欢迎你在评论区与我分享你的答案，也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事。</p><p></p>","neighbors":{"left":{"article_title":"35丨AdaBoost（下）：如何使用AdaBoost对房价进行预测？","id":84086},"right":{"article_title":"37丨数据采集实战：如何自动化运营微博？","id":84933}},"comments":[{"had_liked":false,"id":73222,"user_name":"志","can_delete":false,"product_type":"c1","uid":1307922,"ip_address":"","ucode":"805696CC72A0E2","user_header":"https://static001.geekbang.org/account/avatar/00/13/f5/12/a5383fff.jpg","comment_is_top":false,"comment_ctime":1551842519,"is_pvip":false,"replies":[{"id":"41436","content":"对的，Kaggle里很多数据集都不错，另外在专栏里也会讲到关于信用卡违约率分析和信用卡欺诈分析。下面整理了一些数据集，更多数据集，可以通过https:&#47;&#47;www.kaggle.com&#47;datasets 查找<br><br>Titanic: Machine Learning from Disaster<br>Titanic乘客生存预测<br>https:&#47;&#47;www.kaggle.com&#47;c&#47;titanic<br><br>House Prices-Advanced Regression Techniques<br>预测房价<br>https:&#47;&#47;www.kaggle.com&#47;c&#47;house-prices-advanced-regression-techniques<br><br>MNIST手写数字识别<br>https:&#47;&#47;www.kaggle.com&#47;scolianni&#47;mnistasjpg<br><br>Passenger Satisfaction<br>乘客满意度，提供了美国航空公司US Airline乘客满意度数据<br>https:&#47;&#47;www.kaggle.com&#47;johndddddd&#47;customer-satisfaction<br><br>Bike Sharing Demand<br>自行车共享数据库，用于预测自行车的共享需求<br>https:&#47;&#47;www.kaggle.com&#47;lakshmi25npathi&#47;bike-sharing-dataset<br><br>San Francisco Building Permits<br>5年时间，三藩市20万的建筑许可<br>https:&#47;&#47;www.kaggle.com&#47;aparnashastry&#47;building-permit-applications-data<br><br>San Francisco Crime Classification<br>12年时间的三藩市的犯罪记录<br>https:&#47;&#47;www.kaggle.com&#47;kaggle&#47;san-francisco-crime-classification","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563158902,"ip_address":"","comment_id":73222,"utype":2}],"discussion_count":1,"race_medal":0,"score":"96041123031","product_id":100021701,"comment_content":"Kaggle的Python数据分析入门教程：https:&#47;&#47;www.kaggle.com&#47;kanncaa1&#47;data-sciencetutorial-for-beginners<br><br>另外入门级别的kernels就是Titanic和房价预测：<br>1、https:&#47;&#47;www.kaggle.com&#47;c&#47;titanic<br>2、https:&#47;&#47;www.kaggle.com&#47;c&#47;house-prices-advanced-regression-techniques","like_count":23,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441978,"discussion_content":"对的，Kaggle里很多数据集都不错，另外在专栏里也会讲到关于信用卡违约率分析和信用卡欺诈分析。下面整理了一些数据集，更多数据集，可以通过https://www.kaggle.com/datasets 查找\n\nTitanic: Machine Learning from Disaster\nTitanic乘客生存预测\nhttps://www.kaggle.com/c/titanic\n\nHouse Prices-Advanced Regression Techniques\n预测房价\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n\nMNIST手写数字识别\nhttps://www.kaggle.com/scolianni/mnistasjpg\n\nPassenger Satisfaction\n乘客满意度，提供了美国航空公司US Airline乘客满意度数据\nhttps://www.kaggle.com/johndddddd/customer-satisfaction\n\nBike Sharing Demand\n自行车共享数据库，用于预测自行车的共享需求\nhttps://www.kaggle.com/lakshmi25npathi/bike-sharing-dataset\n\nSan Francisco Building Permits\n5年时间，三藩市20万的建筑许可\nhttps://www.kaggle.com/aparnashastry/building-permit-applications-data\n\nSan Francisco Crime Classification\n12年时间的三藩市的犯罪记录\nhttps://www.kaggle.com/kaggle/san-francisco-crime-classification","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563158902,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73768,"user_name":"小熊猫","can_delete":false,"product_type":"c1","uid":1257442,"ip_address":"","ucode":"7549BA17FFBAD4","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/e2/3640e491.jpg","comment_is_top":false,"comment_ctime":1551973443,"is_pvip":false,"replies":[{"id":"41437","content":"首先十个经典算法代表了十种数据挖掘思想，基于他们都有不少算法的变种和改进，对数据挖掘的影响是非常深远的。<br>另外这十大经典算法，解决的问题也不同，按照解决问题来划分的话：<br>分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART<br>聚类算法：K-Means，EM<br>关联分析：Apriori<br>连接分析：PageRank<br>所以这十大算法要解决的问题也不同，比如分类是一种有监督的学习方式，事先知道样本的类别，通过数据挖掘可以将不同类别的样本进行区别，从而对未知的物体进行分类。而聚类是一种无监督的学习方式，事先不知道样本的类别，而是通过相关属性分析，将具有类似属性的物体聚成一类。<br>所以对十大算法的理解，想要知道他们解决的是哪类问题。然后针对同一类问题，比如分类问题，也有不同种解法，比如C4.5，朴素贝叶斯，SVM，KNN等。<br>不同的算法实际上都有自己对这个问题分析的方式，很难说哪种算法更优，哪个算法不好。实际上这和我们的样本有很大关系，不同的样本属性，样本分布，特征值等，采用不同的算法结果都会有差别，最好的方式就是都做一遍，然后选择针对这个训练集&#47;测试集最优的算法。所以你能看到，在后面的练习中，我们往往都在采用多种算法。<br>另外我想说的是，关于算法的研究，这十大算法是根基，很多人都会在这些算法基础上提出自己的模型，就类似于研究生期间发表论文，都是在这些算法（会有这个算法相应的参考文献）的基础上进行的改进。同时，也会给出自己所采用的的数据集，然后针对这个数据集，采用传统方法和改进方法进行对比，得出结论。所以：算法是可以改进的，采用哪个适合和数据集也有关系，很多时候都会做一遍然后选择适合的。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563158955,"ip_address":"","comment_id":73768,"utype":2}],"discussion_count":2,"race_medal":0,"score":"53091580995","product_id":100021701,"comment_content":"老师可以总结一下，这十个算法的应用场景、优缺点吗","like_count":13,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442268,"discussion_content":"首先十个经典算法代表了十种数据挖掘思想，基于他们都有不少算法的变种和改进，对数据挖掘的影响是非常深远的。\n另外这十大经典算法，解决的问题也不同，按照解决问题来划分的话：\n分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART\n聚类算法：K-Means，EM\n关联分析：Apriori\n连接分析：PageRank\n所以这十大算法要解决的问题也不同，比如分类是一种有监督的学习方式，事先知道样本的类别，通过数据挖掘可以将不同类别的样本进行区别，从而对未知的物体进行分类。而聚类是一种无监督的学习方式，事先不知道样本的类别，而是通过相关属性分析，将具有类似属性的物体聚成一类。\n所以对十大算法的理解，想要知道他们解决的是哪类问题。然后针对同一类问题，比如分类问题，也有不同种解法，比如C4.5，朴素贝叶斯，SVM，KNN等。\n不同的算法实际上都有自己对这个问题分析的方式，很难说哪种算法更优，哪个算法不好。实际上这和我们的样本有很大关系，不同的样本属性，样本分布，特征值等，采用不同的算法结果都会有差别，最好的方式就是都做一遍，然后选择针对这个训练集/测试集最优的算法。所以你能看到，在后面的练习中，我们往往都在采用多种算法。\n另外我想说的是，关于算法的研究，这十大算法是根基，很多人都会在这些算法基础上提出自己的模型，就类似于研究生期间发表论文，都是在这些算法（会有这个算法相应的参考文献）的基础上进行的改进。同时，也会给出自己所采用的的数据集，然后针对这个数据集，采用传统方法和改进方法进行对比，得出结论。所以：算法是可以改进的，采用哪个适合和数据集也有关系，很多时候都会做一遍然后选择适合的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563158955,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1526343,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIfCY2mvbZ2Po4efYBhMJPacb9mlOicNI6Us4ph3ianrkGlUcop8ZlzN6QiaDrnvFcNeaAfwP7XAv5fw/132","nickname":"even","note":"","ucode":"2EDE76B1F80F33","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":287685,"discussion_content":"课时你还是没说不同算法的优缺点和应用场景，全都试一遍固然可以，但是我们需要一些经验分享好帮助我们做判断啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593509721,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73227,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1551843001,"is_pvip":false,"replies":[{"id":"64458","content":"很好的总结 感谢","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619527,"ip_address":"","comment_id":73227,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35911581369","product_id":100021701,"comment_content":"一、sklearn自带的小数据集（packageddataset）：sklearn.datasets.load_&lt;name&gt;<br><br>1)鸢尾花数据集：load_iris（）：用于分类任务的数据集<br>2)手写数字数据集：load_digits（）:用于分类任务或者降维任务的数据集<br>3)乳腺癌数据集load_breast_cancer（）：简单经典的用于二分类任务的数据集<br>4)糖尿病数据集：load_diabetes（）：经典的用于回归认为的数据集，值得注意的是，这10个特征中的每个特征都已经被处理成0均值，方差归一化的特征值。<br>5)波士顿房价数据集：load_boston（）：经典的用于回归任务的数据集<br>6)体能训练数据集：load_linnerud（）：经典的用于多变量回归任务的数据集。<br><br>体能训练数据集中的特征名称linnerud.feature_names为[&#39;Chins&#39;, &#39;Situps&#39;, &#39;Jumps&#39;]<br>鸢尾花数据集的特征名称iris.feature_names为[&#39;sepal length (cm)&#39;,&#39;sepal width (cm)&#39;,&#39;petal length (cm)&#39;,&#39;petal width (cm)&#39;]","like_count":8,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441982,"discussion_content":"很好的总结 感谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619527,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":79059,"user_name":"听妈妈的话","can_delete":false,"product_type":"c1","uid":1462417,"ip_address":"","ucode":"089D797A39C791","user_header":"https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg","comment_is_top":false,"comment_ctime":1553335092,"is_pvip":false,"replies":[{"id":"41438","content":"整理的不错 可以看看 https:&#47;&#47;www.kaggle.com&#47;learn&#47;overview","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563159029,"ip_address":"","comment_id":79059,"utype":2}],"discussion_count":1,"race_medal":0,"score":"18733204276","product_id":100021701,"comment_content":"https:&#47;&#47;www.kaggle.com&#47;learn&#47;overview 页面里有分类好的比较简单的kernel，可以fork kernel在kaggle上运行，也可以下载ipynb或者rmd文件在自己的电脑上运行。比较经典的kaggle竞赛有泰坦尼克预测，房价预测，数字识别等，刚起步时可以参考这些竞赛里的kernel.<br>另外，有一个开源组织ApacheCN有一些kaggle的培训，有很多相关的活动，也可以找同伴组队参加比赛。","like_count":4,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444342,"discussion_content":"整理的不错 可以看看 https://www.kaggle.com/learn/overview","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563159029,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":174482,"user_name":"Merlin","can_delete":false,"product_type":"c1","uid":1804229,"ip_address":"","ucode":"5D21F59EC5C9DA","user_header":"","comment_is_top":false,"comment_ctime":1580227882,"is_pvip":false,"replies":[{"id":"68179","content":"不错 加油","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1580722959,"ip_address":"","comment_id":174482,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14465129770","product_id":100021701,"comment_content":"尝试过kaggle上预测房价的项目，适合入门","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":482416,"discussion_content":"不错 加油","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580722959,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88843,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1556011809,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14440913697","product_id":100021701,"comment_content":"预减枝就是在划分子树的时候不能带来准确度的提升，就不划分。后减枝就是试着减掉每一个叶子节点，看准确度是否有提升。","like_count":3},{"had_liked":false,"id":73225,"user_name":"third","can_delete":false,"product_type":"c1","uid":1025114,"ip_address":"","ucode":"9A37408A834F0B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg","comment_is_top":false,"comment_ctime":1551842919,"is_pvip":false,"replies":[{"id":"64459","content":"kaggle给数据分析师提供了非常好的数据集","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619551,"ip_address":"","comment_id":73225,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14436744807","product_id":100021701,"comment_content":"import sklearn.datasets as db<br># help(db)#可以查看文档，有很多的数据集<br># 准备数据集<br>iris=db.load_iris()<br>print(iris.feature_names)<br>结果<br>[&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]<br><br>说来惭愧，到现在为止，都还没有注意到Kaggle的重要性。刚去看看了入门，发现这篇文章介绍的不错<br>http:&#47;&#47;www.360doc.com&#47;content&#47;18&#47;0106&#47;16&#47;44422250_719580875.shtml#<br><br>一些摘要<br>Kaggle成立于2010年，是一个进行数据发掘和预测竞赛的在线平台。从公司的角度来讲，可以提供一些数据，进而提出一个实际需要解决的问题；从参赛者的角度来讲，他们将组队参与项目，针对其中一个问题提出解决方案，最终由公司选出的最佳方案可以获得5K-10K美金的奖金。<br><br>除此之外，Kaggle官方每年还会举办一次大规模的竞赛，奖金高达一百万美金，吸引了广大的数据科学爱好者参与其中。从某种角度来讲，大家可以把它理解为一个众包平台，类似国内的猪八戒。但是不同于传统的低层次劳动力需求，Kaggle一直致力于解决业界难题，因此也创造了一种全新的劳动力市场——不再以学历和工作经验作为唯一的人才评判标准，而是着眼于个人技能，为顶尖人才和公司之间搭建了一座桥梁。","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441980,"discussion_content":"kaggle给数据分析师提供了非常好的数据集","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619551,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":166582,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1577508775,"is_pvip":false,"replies":[{"id":"63859","content":"可以使用picke工具<br>import pickle<br># fp_lr_model 是模型保存的文件位置<br>pickle.dump(lr_model, open(fp_lr_model, &#39;wb&#39;))<br>lr_model = pickle.load(open(fp_lr_model, &#39;rb&#39;))","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577594075,"ip_address":"","comment_id":166582,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10167443367","product_id":100021701,"comment_content":"有一个问题想请教一下老师，每次做算法模型训练，用训练集数据拟合一个模型后，如何把它保存下来，如果不保存拟合后的模型，每次要做新的预测时，难道都要用样本训练集重新拟合模型？","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":479548,"discussion_content":"可以使用picke工具\nimport pickle\n# fp_lr_model 是模型保存的文件位置\npickle.dump(lr_model, open(fp_lr_model, &amp;#39;wb&amp;#39;))\nlr_model = pickle.load(open(fp_lr_model, &amp;#39;rb&amp;#39;))","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577594075,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":80348,"user_name":"吃饭睡觉打窦窦","can_delete":false,"product_type":"c1","uid":1357253,"ip_address":"","ucode":"CF5D4397180D8F","user_header":"","comment_is_top":false,"comment_ctime":1553651096,"is_pvip":false,"replies":[{"id":"41439","content":"哈哈 需要和知识反复交朋友，其实很多需要推导的知识，往往需要学习多次，所以也很正常。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563159079,"ip_address":"","comment_id":80348,"utype":2}],"discussion_count":1,"race_medal":0,"score":"10143585688","product_id":100021701,"comment_content":"学校学了一遍，这里又学一遍，这才把东西学透点，但是我好奇为啥课堂上学不会呀?[滑稽]（老师是个海归）","like_count":2,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444828,"discussion_content":"哈哈 需要和知识反复交朋友，其实很多需要推导的知识，往往需要学习多次，所以也很正常。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563159079,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73440,"user_name":"小熊猫","can_delete":false,"product_type":"c1","uid":1257442,"ip_address":"","ucode":"7549BA17FFBAD4","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/e2/3640e491.jpg","comment_is_top":false,"comment_ctime":1551884202,"is_pvip":false,"replies":[{"id":"41434","content":"首先特征选择是数据挖掘(机器学习)中的重要问题之一，一般来说对于数据特征空间大的数据集来说，我们需简要对特征进行选择，也就是选取有代表性的特征，来降低特征空间的冗余度，提升算法的效率。<br>特征选择的过程，你可以理解是从m个特征中选择n个特征的过程，文章中从三个相关性大的特征只选择一个，目的是在于降低冗余信息，缩减特征维数。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563158837,"ip_address":"","comment_id":73440,"utype":2}],"discussion_count":1,"race_medal":0,"score":"10141818794","product_id":100021701,"comment_content":"老师 为什么三个相关性大的特征只选一个呢？原理是什么？","like_count":2,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442087,"discussion_content":"首先特征选择是数据挖掘(机器学习)中的重要问题之一，一般来说对于数据特征空间大的数据集来说，我们需简要对特征进行选择，也就是选取有代表性的特征，来降低特征空间的冗余度，提升算法的效率。\n特征选择的过程，你可以理解是从m个特征中选择n个特征的过程，文章中从三个相关性大的特征只选择一个，目的是在于降低冗余信息，缩减特征维数。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563158837,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73259,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1551846052,"is_pvip":false,"replies":[{"id":"41435","content":"完整代码在https:&#47;&#47;github.com&#47;cystanford&#47;text_classification","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563158878,"ip_address":"","comment_id":73259,"utype":2}],"discussion_count":1,"race_medal":0,"score":"10141780644","product_id":100021701,"comment_content":"在第21课朴素贝叶斯分类（下），对中文文档进行分类，老师可以提供完整代码吗？一直遇到对中文词组不支持的问题？","like_count":2,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441991,"discussion_content":"完整代码在https://github.com/cystanford/text_classification","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563158878,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73255,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1551845673,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10141780265","product_id":100021701,"comment_content":"在第21课朴素贝叶斯分类（下）：<br>在模块4:生成朴素贝叶斯分类器，特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 是如何获取的。老师并没有讲清楚。","like_count":2},{"had_liked":false,"id":282816,"user_name":"小晨","can_delete":false,"product_type":"c1","uid":1935063,"ip_address":"","ucode":"B7ADDC309C0D19","user_header":"https://static001.geekbang.org/account/avatar/00/1d/86/d7/46842f90.jpg","comment_is_top":false,"comment_ctime":1615428429,"is_pvip":false,"replies":[{"id":"102680","content":"感谢提醒。代表船舱的字段是Cabin，而不是carbin","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1615477988,"ip_address":"","comment_id":282816,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5910395725","product_id":100021701,"comment_content":"答疑3：carbin是卡宾枪 &#47;偷笑&#47;偷笑","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516846,"discussion_content":"感谢提醒。代表船舱的字段是Cabin，而不是carbin","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615477988,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":115890,"user_name":"周志翔","can_delete":false,"product_type":"c1","uid":1464176,"ip_address":"","ucode":"22866A9A9650E6","user_header":"","comment_is_top":false,"comment_ctime":1563762502,"is_pvip":false,"replies":[{"id":"42431","content":"对 有不少优质的kernel可以参考和学习","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1563782500,"ip_address":"","comment_id":115890,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5858729798","product_id":100021701,"comment_content":"我觉得在kaggle可以学到很多数据处理的方法，看厉害的人怎么做的，是个很不错的网站","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":459334,"discussion_content":"对 有不少优质的kernel可以参考和学习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563782500,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":111110,"user_name":"Hulk","can_delete":false,"product_type":"c1","uid":1052892,"ip_address":"","ucode":"8D6EDC6E370E21","user_header":"https://static001.geekbang.org/account/avatar/00/10/10/dc/96476998.jpg","comment_is_top":false,"comment_ctime":1562477358,"is_pvip":false,"replies":[{"id":"41440","content":"这里想说明的是KMeans计算的中心点，实际上是这个类别里所有点的 属性值的平均值。然后作为这个新的中心点的属性值。如果还不理解的话，可以加数据分析的微信群","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1563159152,"ip_address":"","comment_id":111110,"utype":2}],"discussion_count":2,"race_medal":0,"score":"5857444654","product_id":100021701,"comment_content":"K-Means的例子还是看不懂","like_count":1,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":457234,"discussion_content":"这里想说明的是KMeans计算的中心点，实际上是这个类别里所有点的 属性值的平均值。然后作为这个新的中心点的属性值。如果还不理解的话，可以加数据分析的微信群","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563159152,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1115232,"avatar":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","nickname":"Fan","note":"","ucode":"3BF28670FD9407","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385784,"discussion_content":"数据分析的微信群 在哪里？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627272069,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314002,"user_name":"宏伟","can_delete":false,"product_type":"c1","uid":2764948,"ip_address":"","ucode":"82EFC7D18CBFB8","user_header":"https://static001.geekbang.org/account/avatar/00/2a/30/94/12c20983.jpg","comment_is_top":false,"comment_ctime":1632797811,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1632797811","product_id":100021701,"comment_content":"陈老师，微软的PowerBI里已经集成了Python，可以安装anaconda，那么您课程里的十大算法是不是都可以在powerbi里的Python操作建模、拆分数据集、训练、验证、测试，乃至部署模型、进入实战呢？<br>之前看您讲Python的IDE时，没有提到anaconda的jupyter notebook。但黄佳老师（我就是从他的机器学习课程链接到您的课程的）讲，传统机器学习用notebook就可以了。<br>期盼回复。","like_count":0}]}