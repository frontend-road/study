{"id":249785,"title":"08 | 技术栈选型","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/geektime-distributed\">https://gitee.com/geektime-geekbang/geektime-distributed</a></p>","comments":[{"had_liked":false,"id":244338,"user_name":"Johar","can_delete":false,"product_type":"c3","uid":1101969,"ip_address":"","ucode":"834136A6F64CDC","user_header":"https://static001.geekbang.org/account/avatar/00/10/d0/91/89123507.jpg","comment_is_top":false,"comment_ctime":1598488437,"is_pvip":false,"replies":[{"id":90029,"content":"DBWriter可以和Aggregator做在一起，这样的确简单。但是这样也有问题，一个是Aggregator的逻辑会搞复杂(又要计算又要写DB)，第二个是DBWriter和Aggregator就耦合了，后续升级更新就要一起更新，不能单独更新DBWriter或者Aggregator。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1598614672,"ip_address":"","comment_id":244338,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师，DBWriter的功能也可以直接放在Aggregator里面，只有写入DB成功，才手动回复Ack给kafka，这样也可以保证数据的不丢失，所以方案中Queue和DBWriter是否添加的必要？","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504556,"discussion_content":"DBWriter可以和Aggregator做在一起，这样的确简单。但是这样也有问题，一个是Aggregator的逻辑会搞复杂(又要计算又要写DB)，第二个是DBWriter和Aggregator就耦合了，后续升级更新就要一起更新，不能单独更新DBWriter或者Aggregator。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598614672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228032,"user_name":"飞翔","can_delete":false,"product_type":"c3","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1592539354,"is_pvip":false,"replies":[{"id":84206,"content":"counting和query service都是无状态，可以部署多台做成集群，一般假定不会全部挂。\n\n缓存有很多HA技术，比方说redis支持主从+哨兵，还有cluster集群等，一般也假定不会挂。当然，redis是可以开启持久化的。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592667854,"ip_address":"","comment_id":228032,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师 您说counting-service 把请求储存在缓存当中 这里的缓存也应该是持久化的把  counting-service挂了 就会丢失","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498897,"discussion_content":"counting和query service都是无状态，可以部署多台做成集群，一般假定不会全部挂。\n\n缓存有很多HA技术，比方说redis支持主从+哨兵，还有cluster集群等，一般也假定不会挂。当然，redis是可以开启持久化的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592667854,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227938,"user_name":"Xg huang","can_delete":false,"product_type":"c3","uid":1016995,"ip_address":"","ucode":"9680FA95DE3553","user_header":"","comment_is_top":false,"comment_ctime":1592528628,"is_pvip":false,"replies":[{"id":84202,"content":"对，如果要对冷数据进行查询处理，肯定需要对查询服务做一些包装和扩展的。一般像阿里云oss，AWS S3等，都是有API可以调用的，基于这些API实现查询也不是很复杂，只要合理设计文件的存储结构。\n\n比方说像prometheus之类的时间序列数据库，就支持所谓远程存储，可以参考thanos(https:&#47;&#47;thanos.io&#47;)项目，它支持GCP, S3, Azure, Swift and Tencent COS等作为promethues的远程存储，相当于冷存储。\n","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592667624,"ip_address":"","comment_id":227938,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师，请教一下，冷数据的查询问题要怎样解决呢？用对象存储只能解决存储容量的问题，如果用户需要查询这些冷数据，需要再搭一个新的query service来解决吗？","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498847,"discussion_content":"对，如果要对冷数据进行查询处理，肯定需要对查询服务做一些包装和扩展的。一般像阿里云oss，AWS S3等，都是有API可以调用的，基于这些API实现查询也不是很复杂，只要合理设计文件的存储结构。\n\n比方说像prometheus之类的时间序列数据库，就支持所谓远程存储，可以参考thanos(https://thanos.io/)项目，它支持GCP, S3, Azure, Swift and Tencent COS等作为promethues的远程存储，相当于冷存储。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592667624,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1016995,"avatar":"","nickname":"Xg huang","note":"","ucode":"9680FA95DE3553","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":285223,"discussion_content":"明白了，谢谢老师","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1592786000,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227684,"user_name":"飞翔","can_delete":false,"product_type":"c3","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1592453137,"is_pvip":false,"replies":[{"id":83977,"content":"我之前就曾开发和开源过一个持久化的queue\nhttps:&#47;&#47;github.com&#47;bulldog2011&#47;bigqueue\n\n另外，你基于磁盘文件，自己实现一个持久化的queue，也不麻烦。\n\n还有，用嵌入式DB，比如sqlite&#47;berkeleyDB等，也可以实现简单queue的功能。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592498529,"ip_address":"","comment_id":227684,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师 inner queue 不是也是持久化queue 嘛 这个应该用什么开源组件呀","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498746,"discussion_content":"我之前就曾开发和开源过一个持久化的queue\nhttps://github.com/bulldog2011/bigqueue\n\n另外，你基于磁盘文件，自己实现一个持久化的queue，也不麻烦。\n\n还有，用嵌入式DB，比如sqlite/berkeleyDB等，也可以实现简单queue的功能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592498529,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":236609,"user_name":"漂泊的松鼠","can_delete":false,"product_type":"c3","uid":1194545,"ip_address":"","ucode":"281CF2FAA6F904","user_header":"https://static001.geekbang.org/account/avatar/00/12/3a/31/6a127efe.jpg","comment_is_top":false,"comment_ctime":1595477082,"is_pvip":false,"replies":[{"id":87610,"content":"一条一条消息发，和聚合一段时间再批量发，性能有数量级的差异。请继续看第三章消息队列的设计，里头有专门讲到这个问题。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1595690943,"ip_address":"","comment_id":236609,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"counting-service 的职责很奇怪，缓存了1分钟的数据，起线程同步到kafka，\n为什么不直接发给kafka，来一条发一条，用videoId做分区键\ncounting-service的职责，我认为是kafka的写入、数据转换消息封装，\n希望波波回复","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":502140,"discussion_content":"一条一条消息发，和聚合一段时间再批量发，性能有数量级的差异。请继续看第三章消息队列的设计，里头有专门讲到这个问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595690943,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":230142,"user_name":"jjn0703","can_delete":false,"product_type":"c3","uid":1076652,"ip_address":"","ucode":"83AF403AC7DFB7","user_header":"https://static001.geekbang.org/account/avatar/00/10/6d/ac/6128225f.jpg","comment_is_top":false,"comment_ctime":1593273744,"is_pvip":true,"replies":[{"id":85284,"content":"我之前公司主要采用CAT进行应用性能监控，CAT是支持JVM监控的，还支持线程监控。另外，Skywalking也是支持JVM监控的。\nhttps:&#47;&#47;github.com&#47;dianping&#47;cat\nhttps:&#47;&#47;github.com&#47;apache&#47;skywalking","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1593529783,"ip_address":"","comment_id":230142,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"不懂就问，老师课程好几次提到JVM垃圾回收监控，这块完全没接触过，请问业界都用什么来落地呢？有什么参考材料参考吗？","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499777,"discussion_content":"我之前公司主要采用CAT进行应用性能监控，CAT是支持JVM监控的，还支持线程监控。另外，Skywalking也是支持JVM监控的。\nhttps://github.com/dianping/cat\nhttps://github.com/apache/skywalking","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593529783,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":230129,"user_name":"zhou","can_delete":false,"product_type":"c3","uid":1087243,"ip_address":"","ucode":"E1CE8575B3F106","user_header":"https://static001.geekbang.org/account/avatar/00/10/97/0b/a943bcb3.jpg","comment_is_top":false,"comment_ctime":1593270482,"is_pvip":false,"replies":[{"id":85283,"content":"1. MQ的分区，比方说Kafka的分区，是直接支持高可用的。对于每一个分区，Kafka支持在不同Broker节点上复制多分数据拷贝。\n2. Consumer端的第二个缓冲队列，是需要考虑持久化的，这样数据才不会丢。当然内部队列都要监控，如果超过一定大小要告警。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1593529571,"ip_address":"","comment_id":230129,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"1.Counting Service 将请求缓存在内存中，后台以一分钟的频率将请求写入MQ中的某一个分区（约定相同的video_id前缀的请求，放入同一个分区），\n2.技术消费者从对应的分区中拉取消息缓存在内部队列当中，Aggregator从内存队列中拉取消息进行聚合运算每分钟的观看量，并且以一分钟为频率将结果写入内部的第二个缓冲队列\n3.DbWriter 从队列当中拉取结果，最终将结果写入DB中\n----------------------------\n老师我有几个问题\n1.如果放入不同分区，怎么保证高可用，若一个分区\n2.缓冲队列太多，是否有可能会造成数据丢失，且队列中的消息也被消费了的情况","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499772,"discussion_content":"1. MQ的分区，比方说Kafka的分区，是直接支持高可用的。对于每一个分区，Kafka支持在不同Broker节点上复制多分数据拷贝。\n2. Consumer端的第二个缓冲队列，是需要考虑持久化的，这样数据才不会丢。当然内部队列都要监控，如果超过一定大小要告警。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593529571,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}