{"id":186415,"title":"35 | MongoDB读写性能机制","content":"<p><strong>亲爱的学员：</strong><br>\n你好，在回答学员问题的过程中，我发现大家对于第二章的事务处理, 特别是隔离级别，以及第三章的分片集群有相对较多的问题。在这里我给大家推荐一些补充的学习材料可以从另外一个角度来加深一些这些概念的理解。<br>\nMongoDB事务的原子性<br>\n<a href=\"https://docs.mongodb.com/manual/core/write-operations-atomicity/\">https://docs.mongodb.com/manual/core/write-operations-atomicity/</a><br>\nMongoDB事务的隔离级别和一致性模型<br>\n<a href=\"https://docs.mongodb.com/manual/core/read-isolation-consistency-recency/\">https://docs.mongodb.com/manual/core/read-isolation-consistency-recency/</a><br>\n如果英文不太感冒，在MongoDB中文网站上有不少内容，比如分片相关的：<br>\n<a href=\"http://www.mongoing.com/?s=%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4\">http://www.mongoing.com/?s=分片集群</a></p><p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/geektime-mongodb-course\">https://gitee.com/geektime-geekbang/geektime-mongodb-course</a></p>","comments":[{"had_liked":false,"id":178087,"user_name":"黄智寿","can_delete":false,"product_type":"c3","uid":1249951,"ip_address":"","ucode":"43685DE62F720B","user_header":"https://static001.geekbang.org/account/avatar/00/13/12/9f/b6eb3471.jpg","comment_is_top":false,"comment_ctime":1581584365,"is_pvip":false,"replies":[{"id":69151,"content":"ticket一般只是个标志，如果发现用光，是因为你的物理资源不够，主要是IOPS，少数时候CPU。这个时候不是调高tickt，而是优化硬件资源。","user_name":"作者回复","user_name_real":"远航的TJ 唐建法","uid":1260017,"ctime":1581648364,"ip_address":"","comment_id":178087,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"这个ticket默认是128，一般调高这个值的时候需要参考哪些指标？比如物理机内存？cpu核数？节点数？有没有相关经验的最佳实践分享下？谢谢","like_count":4,"discussions":[{"author":{"id":1260017,"avatar":"https://static001.geekbang.org/account/avatar/00/13/39/f1/6d18a4d1.jpg","nickname":"远航的TJ 唐建法","note":"","ucode":"0719807390250A","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":483662,"discussion_content":"ticket一般只是个标志，如果发现用光，是因为你的物理资源不够，主要是IOPS，少数时候CPU。这个时候不是调高tickt，而是优化硬件资源。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1581648364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":174280,"user_name":"hal","can_delete":false,"product_type":"c3","uid":1359844,"ip_address":"","ucode":"98E625F7327FD9","user_header":"https://static001.geekbang.org/account/avatar/00/14/bf/e4/45758517.jpg","comment_is_top":false,"comment_ctime":1580117986,"is_pvip":false,"replies":[{"id":67796,"content":"oplog记录的是对数据库的逻辑操作， 在MongoDB里面用一个固定大小的普通集合来记录，和其他的数据一样，默认增删都是在内存里发生。 \n\njournal 和其他数据库类似，采用WriteAheadLog机制，用来提供对数据库写入操作的一致性和持久性保证。它记录的是要对数据物理区块修改的一些动作。\n\nJournal 日志和Oplog理论上都可以用来恢复，但是journal比较底层，直接操作存储区，写入和恢复效率要比oplog 日志高，所以通常数据库都会采用专门的journal来做crash recovery.","user_name":"作者回复","user_name_real":"远航的TJ 唐建法","uid":1260017,"ctime":1580251423,"ip_address":"","comment_id":174280,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"唐老师新年好，听完写入操作的过程，有点好奇，oplog和journal日志能不能合并呢，我的理解的话就是oplog是个定容的集合，存放的记录都是幂等操作，用于mongodb的复制集模式，从主节点复制到其他slave节点保证数据的一致，journal日志是用于mongodb crash之后恢复的一个日志。那crash恢复能不能也用oplog呢？","like_count":3,"discussions":[{"author":{"id":1260017,"avatar":"https://static001.geekbang.org/account/avatar/00/13/39/f1/6d18a4d1.jpg","nickname":"远航的TJ 唐建法","note":"","ucode":"0719807390250A","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":482357,"discussion_content":"oplog记录的是对数据库的逻辑操作， 在MongoDB里面用一个固定大小的普通集合来记录，和其他的数据一样，默认增删都是在内存里发生。 \n\njournal 和其他数据库类似，采用WriteAheadLog机制，用来提供对数据库写入操作的一致性和持久性保证。它记录的是要对数据物理区块修改的一些动作。\n\nJournal 日志和Oplog理论上都可以用来恢复，但是journal比较底层，直接操作存储区，写入和恢复效率要比oplog 日志高，所以通常数据库都会采用专门的journal来做crash recovery.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580251423,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1359844,"avatar":"https://static001.geekbang.org/account/avatar/00/14/bf/e4/45758517.jpg","nickname":"hal","note":"","ucode":"98E625F7327FD9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":155644,"discussion_content":"谢谢老师！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580271999,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215413,"user_name":"磐石","can_delete":false,"product_type":"c3","uid":1076068,"ip_address":"","ucode":"2C73D2DE8A7119","user_header":"https://wx.qlogo.cn/mmopen/vi_32/MwqUMVzhCEMSsnibTrLrWzrJkibib8NLGb7dtFCyH2Efxa273ibib80t5BjibZ5unMWUHI2VqwkASicvkvJCfpK9j8IbQ/0","comment_is_top":false,"comment_ctime":1588987972,"is_pvip":false,"replies":[{"id":80143,"content":"你需要对这些数据做pre-aggregation，按照时间颗粒度，获得分钟级平均数，小时级平均数，把他们事先存到库里，给前段展现用。这样的话你就不需要返回大量的原始数据再在应用里做计算了。","user_name":"作者回复","user_name_real":"远航的TJ 唐建法","uid":1260017,"ctime":1589319372,"ip_address":"","comment_id":215413,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"老师您好！最近做了个前端监控分析的系统，使用了MongoDB作为数据库，现在每天的存储记录达到300W+条，查询返回百万级数据这块不知道怎么优化，老师有什么建议吗？谢谢！","like_count":1,"discussions":[{"author":{"id":1260017,"avatar":"https://static001.geekbang.org/account/avatar/00/13/39/f1/6d18a4d1.jpg","nickname":"远航的TJ 唐建法","note":"","ucode":"0719807390250A","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494422,"discussion_content":"你需要对这些数据做pre-aggregation，按照时间颗粒度，获得分钟级平均数，小时级平均数，把他们事先存到库里，给前段展现用。这样的话你就不需要返回大量的原始数据再在应用里做计算了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589319372,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":182755,"user_name":"长脖子树","can_delete":false,"product_type":"c3","uid":1182802,"ip_address":"","ucode":"D9090EF67EEB1B","user_header":"https://static001.geekbang.org/account/avatar/00/12/0c/52/f25c3636.jpg","comment_is_top":false,"comment_ctime":1582860322,"is_pvip":true,"replies":[{"id":70940,"content":"\n\ninsert 特别是使用batch写的时候普通盘性能也可以挺快的。\n\n你要测试随机读写，就可以看出区别了","user_name":"作者回复","user_name_real":"远航的TJ 唐建法","uid":1260017,"ctime":1583027923,"ip_address":"","comment_id":182755,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"老师你好, 我在分别在 4核8G 普通硬盘和 SSD 上测试(使用的是阿里云),  只测试了 insert 性能, 使用的是mongodb默认配置, 单机没有复制集, 在普通云盘上最高 15000 次&#47;秒, 在 SSD 上最高 17000 次&#47;秒,  虽然测试中感觉 ssd 的insert 性能比普通云盘的insert 性能稳定, 但是实际差距并不是很大, 我想请问老师, 有哪些参数&#47;操作可以让 insert 性能更进一步\n具体IOPS等参数, 测试结果见链接 https:&#47;&#47;www.processon.com&#47;view&#47;link&#47;5e58867ee4b0cc44b5b17f21","like_count":0,"discussions":[{"author":{"id":1260017,"avatar":"https://static001.geekbang.org/account/avatar/00/13/39/f1/6d18a4d1.jpg","nickname":"远航的TJ 唐建法","note":"","ucode":"0719807390250A","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485436,"discussion_content":"\n\ninsert 特别是使用batch写的时候普通盘性能也可以挺快的。\n\n你要测试随机读写，就可以看出区别了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583027923,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1419723,"avatar":"https://static001.geekbang.org/account/avatar/00/15/a9/cb/a431bde5.jpg","nickname":"木头发芽","note":"","ucode":"657B381C5DA963","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":393961,"discussion_content":"你insert的时候得让普通硬盘的机械臂动起来,这样才能比较出与SSD盘的速度差距. 比如插入压测的时候轮流写不同的表的数据,而不是一直往一个表里顺序插入","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631672247,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":243736,"user_name":"Jast","can_delete":false,"product_type":"c3","uid":1500072,"ip_address":"","ucode":"8F563226BF2A3F","user_header":"https://static001.geekbang.org/account/avatar/00/16/e3/a8/249f6d53.jpg","comment_is_top":false,"comment_ctime":1598260671,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"老师您好，想问下一次性读取数据比较大有什么优化方案吗？一次取数据量都在50w以上。目前用spring mongo data 时间都要在2分钟","like_count":2},{"had_liked":false,"id":352041,"user_name":"刘金生","can_delete":false,"product_type":"c3","uid":1191973,"ip_address":"","ucode":"3388B5D8BF9355","user_header":"https://static001.geekbang.org/account/avatar/00/12/30/25/178759a2.jpg","comment_is_top":false,"comment_ctime":1658363075,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"老师，journal和oplog不在一个事务里，是如何本证如果oplog成功，而journal不成功造成的数据不一致性","like_count":0,"discussions":[{"author":{"id":1098505,"avatar":"https://static001.geekbang.org/account/avatar/00/10/c3/09/dc368335.jpg","nickname":"杰sir","note":"","ucode":"80BB56B3BFB71A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631103,"discussion_content":"同问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699320413,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":326242,"user_name":"Hoboson!","can_delete":false,"product_type":"c3","uid":1341692,"ip_address":"","ucode":"D593C3EB90E136","user_header":"https://static001.geekbang.org/account/avatar/00/14/78/fc/f7d59501.jpg","comment_is_top":false,"comment_ctime":1639448639,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"老师好：我们的数据库数据量16T增加一个second节点，同步数据报这个错误 ，这个影响同步吗 ，会不会最后失败，错误：Restarting oplog query due to error: ExceededTimeLimit: operation exceeded time limit. Last fetched optime (with hash): { ts: Timestamp 1639448007000|222, t: 33 }[3951692676946563652]. Restarts remaining: 3","like_count":0},{"had_liked":false,"id":217121,"user_name":"扬一场远远的风","can_delete":false,"product_type":"c3","uid":1357801,"ip_address":"","ucode":"AB47E3D2EAB8A8","user_header":"https://static001.geekbang.org/account/avatar/00/14/b7/e9/5400cdf3.jpg","comment_is_top":false,"comment_ctime":1589422743,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"请问一下，mongod执行应用端发送的请求是异步还是同步？另外ticket 不足操作会等待，这个等待队列的长度与mongostat 中看到的qr|qw是同一个吗？","like_count":0},{"had_liked":false,"id":202211,"user_name":"kylexy_0817","can_delete":false,"product_type":"c3","uid":1068372,"ip_address":"","ucode":"392DD9DD5E4B6E","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/54/9c214885.jpg","comment_is_top":false,"comment_ctime":1585924151,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100040001,"comment_content":"感觉MongoDB的机制有点和ElasticSearch类似，都是先操作缓存同时写日志，一个叫Oplog，一个叫transactonLog","like_count":0}]}