{"id":105324,"title":"31 | WordCount Beam Pipeline实战","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“WordCount Beam Pipeline实战”。</p><p>前面我们已经学习了Beam的基础数据结构PCollection，基本数据转换操作Transform，还有Pipeline等技术。你一定跃跃欲试，想要在实际项目中使用了。这一讲我们就一起学习一下怎样用Beam解决数据处理领域的教科书级案例——WordCount。</p><p>WordCount你一定不陌生，在<a href=\"https://time.geekbang.org/column/article/97658\">第18讲</a>中，我们就已经接触过了。WordCount问题是起源于MapReduce时代就广泛使用的案例。顾名思义，WordCount想要解决的问题是统计一个文本库中的词频。</p><p>比如，你可以用WordCount找出莎士比亚最喜欢使用的单词，那么你的输入是莎士比亚全集，输出就是每个单词出现的次数。举个例子，比如这一段：</p><pre><code>HAMLET\n\nACT I\n\nSCENE I\tElsinore. A platform before the castle.\n\n\t[FRANCISCO at his post. Enter to him BERNARDO]\n\nBERNARDO\tWho's there?\n\nFRANCISCO\tNay, answer me: stand, and unfold yourself.\n\nBERNARDO\tLong live the king!\n\nFRANCISCO\tBernardo?\n\nBERNARDO\tHe.\n\nFRANCISCO\tYou come most carefully upon your hour.\n\nBERNARDO\t'Tis now struck twelve; get thee to bed, Francisco.\n\nFRANCISCO\tFor this relief much thanks: 'tis bitter cold,\n\tAnd I am sick at heart.\n\nBERNARDO\tHave you had quiet guard?\n\nFRANCISCO\tNot a mouse stirring.\n\nBERNARDO\tWell, good night.\n\tIf you do meet Horatio and Marcellus,\n\tThe rivals of my watch, bid them make haste.\n\nFRANCISCO\tI think I hear them. Stand, ho! Who's there?\n</code></pre><p>在这个文本库中，我们用“the: 数字”表示the出现了几次，数字就是单词出现的次数。</p><pre><code>The: 3\nAnd: 3\nHim: 1\n...\n</code></pre><p>那么我们怎样在Beam中处理这个问题呢？结合前面所学的知识，我们可以把Pipeline分为这样几步：</p><ol>\n<li>用Pipeline IO读取文本库（参考<a href=\"https://time.geekbang.org/column/article/102578\">第27讲</a>）；</li>\n<li>用Transform对文本进行分词和词频统计操作（参考<a href=\"https://time.geekbang.org/column/article/101735\">第25讲</a>）；</li>\n<li>用Pipeline IO输出结果（参考<a href=\"https://time.geekbang.org/column/article/102578\">第27讲</a>）；</li>\n<li>所有的步骤会被打包进一个Beam Pipeline（参考<a href=\"https://time.geekbang.org/column/article/102182\">第26讲</a>）。</li>\n</ol><!-- [[[read_end]]] --><p>整个过程就如同下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c6/cd/c6b63574f6005aaa4a6aba366b0a5dcd.jpg?wh=1686*850\" alt=\"\"></p><h2>创建Pipeline</h2><p>首先，我们先用代码创建一个PipelineOptions的实例。PipelineOptions能够让我们对Pipeline进行必要的配置，比如配置执行程序的Runner，和Runner所需要的参数。我们在这里先采用默认配置。</p><p>记得第30讲中我们讲过，Beam Pipeline可以配置在不同的Runner上跑，比如SparkRunner，FlinkRunner。如果PipelineOptions不配置的情况下，默认的就是DirectRunner，也就是说会在本机执行。</p><p>Java</p><pre><code>PipelineOptions options = PipelineOptionsFactory.create();\n</code></pre><p>接下来，我们就可以用这个PipelineOptions去创建一个Pipeline了。一个Pipeline实例会去构建一个数据处理流水线所需要的数据处理DAG，以及这个DAG所需要进行的Transform。</p><p>Java</p><pre><code>Pipeline p = Pipeline.create(options);\n</code></pre><h2>应用Transform</h2><p>在上面的设计框图中，我们可以看到，我们需要进行好几种Transform。比如TextIO.Read、ParDo、Count去读取数据，操纵数据，以及存储数据。</p><p>每一种Transform都需要一些参数，并且会输出特定的数据。输入和输出往往会用PCollection的数据结构表示。简单回顾一下，PCollection是Beam对于数据集的抽象，表示任意大小、无序的数据，甚至可以是无边界的Streaming数据。</p><p>在我们这个WordCount例子中，我们的Transform依次是这样几个。</p><p>第一个Transform，是先要用TextIO.Read来读取一个外部的莎士比亚文集，生成一个PCollection，包含这个文集里的所有文本行。这个PCollection中的每个元素都是文本中的一行。</p><p>Java</p><pre><code>PCollection&lt;String&gt; lines = p.apply(TextIO.read().from(&quot;gs://apache-beam-samples/shakespeare/*&quot;));\n</code></pre><p>第二个Transform，我们要把文本行中的单词提取出来，也就是做分词（tokenization）。</p><p>这一步的输入PCollection中的每个元素都表示了一行。那么输出呢？输出还是一个PCollection，但是每个元素变成了单词。</p><p>你可以留意一下，我们这里做分词时，用的正则表达式[^\\p{L}]+，意思是非Unicode Letters所以它会按空格或者标点符号等把词分开。</p><p>Java</p><pre><code>PCollection&lt;String&gt; words = lines.apply(&quot;ExtractWords&quot;, FlatMapElements\n        .into(TypeDescriptors.strings())\n        .via((String word) -&gt; Arrays.asList(word.split(&quot;[^\\\\p{L}]+&quot;))));\n</code></pre><p>第三个Transform，我们就会使用Beam SDK提供的Count Transform。Count Transform会把任意一个PCollection转换成有key/value的组合，每一个key是原来PCollection中的非重复的元素，value则是元素出现的次数。</p><p>Java</p><pre><code>PCollection&lt;KV&lt;String, Long&gt;&gt; counts = words.apply(Count.&lt;String&gt;perElement());\n</code></pre><p>第四个Transform会把刚才的key/value组成的PCollection转换成我们想要的输出格式，方便我们输出词频。因为大部分的时候，我们都是想要把输出存储到另一个文件里的。</p><p>Java</p><pre><code>PCollection&lt;String&gt; formatted = counts.apply(&quot;FormatResults&quot;, MapElements\n    .into(TypeDescriptors.strings())\n    .via((KV&lt;String, Long&gt; wordCount) -&gt; wordCount.getKey() + &quot;: &quot; + wordCount.getValue()));\n</code></pre><p>最后一个Transform就是TextIO.Write用来把最终的PCollection写进文本文档。PCollection中的每一个元素都会被写为文本文件中的独立一行。</p><h2>运行Pipeline</h2><p>调用Pipeline的run()方法会把这个Pipeline所包含的Transform优化并放到你指定的Runner上执行。这里你需要注意，run()方法是异步的，如果你想要同步等待Pipeline的执行结果，需要调用waitUntilFinish()方法。</p><p>Java</p><pre><code>p.run().waitUntilFinish();\n</code></pre><h2>改进代码的建议</h2><p>代码看起来都完成了，不过，我们还可以对代码再做些改进。</p><h3>编写独立的DoFn</h3><p>在上面的示例代码中，我们把Transform都inline地写在了apply()方法里。</p><p>Java</p><pre><code>lines.apply(&quot;ExtractWords&quot;, FlatMapElements\n        .into(TypeDescriptors.strings())\n        .via((String word) -&gt; Arrays.asList(word.split(&quot;[^\\\\p{L}]+&quot;))));\n</code></pre><p>但是这样的写法在实际工作中很难维护。</p><p>一是因为真实的业务逻辑往往比较复杂，很难用一两行的代码写清楚，强行写成inline的话可读性非常糟糕。</p><p>二是因为这样inline的Transform几乎不可复用和测试。</p><p>所以，实际工作中，我们更多地会去继承DoFn来实现我们的数据操作。这样每个DoFn我们都可以单独复用和测试。</p><p>接下来，我们看看怎样用用DoFn来实现刚才的分词Transform？</p><p>其实很简单，我们继承DoFn作为我们的子类ExtracrtWordsFn，然后把单词的拆分放在DoFn的processElement成员函数里。</p><p>Java</p><pre><code>static class ExtractWordsFn extends DoFn&lt;String, String&gt; {\n    private final Counter emptyLines = Metrics.counter(ExtractWordsFn.class, &quot;emptyLines&quot;);\n    private final Distribution lineLenDist =\n        Metrics.distribution(ExtractWordsFn.class, &quot;lineLenDistro&quot;);\n\n    @ProcessElement\n    public void processElement(@Element String element, OutputReceiver&lt;String&gt; receiver) {\n      lineLenDist.update(element.length());\n      if (element.trim().isEmpty()) {\n        emptyLines.inc();\n      \n\n      // Split the line into words.\n      String[] words = element.split(“[^\\\\p{L}]+”, -1);\n\n      // Output each word encountered into the output PCollection.\n      for (String word : words) {\n        if (!word.isEmpty()) {\n          receiver.output(word);\n        }\n      }\n   }\n}\n</code></pre><h3>创建PTransform合并相关联的Transform</h3><p>PTransform类可以用来整合一些相关联的Transform。</p><p>比如你有一些数据处理的操作包含几个Transform或者ParDo，你可以把他们封装在一个PTransform里。</p><p>我们这里试着把上面的ExtractWordsFn和Count两个Transform封装起来。这样可以对这样一整套数据处理操作复用和测试。当定义PTransform的子类时，它的输入输出类型就是一连串Transform的最初输入和最终输出。那么在这里，输入类型是String，输出类型是KV&lt;String, Long&gt;。就如同下面的代码一样。</p><p>Java</p><pre><code>  /**\n   * A PTransform that converts a PCollection containing lines of text into a PCollection of\n   * formatted word counts.\n   *\n   * &lt;p&gt;This is a custom composite transform that bundles two transforms (ParDo and\n   * Count) as a reusable PTransform subclass. Using composite transforms allows for easy reuse,\n   * modular testing, and an improved monitoring experience.\n   */\n\n  public static class CountWords\n      extends PTransform&lt;PCollection&lt;String&gt;, PCollection&lt;KV&lt;String, Long&gt;&gt;&gt; {\n    @Override\n    public PCollection&lt;KV&lt;String, Long&gt;&gt; expand(PCollection&lt;String&gt; lines) {\n\n      // Convert lines of text into individual words.\n      PCollection&lt;String&gt; words = lines.apply(ParDo.of(new ExtractWordsFn()));\n\n      // Count the number of times each word occurs.\n      PCollection&lt;KV&lt;String, Long&gt;&gt; wordCounts = words.apply(Count.perElement());\n\n      return wordCounts;\n    }\n  }\n</code></pre><h3>参数化PipelineOptions</h3><p>刚才我们把输入文件的路径和输出文件的路径都写在了代码中。但实际工作中我们很少会这样做。</p><p>因为这些文件的路径往往是运行时才会决定，比如测试环境和生产环境会去操作不同的文件。在真正的实际工作中，我们往往把它们作为命令行参数放在PipelineOptions里面。这就需要去继承PipelineOptions。</p><p>比如，我们创建一个WordCountOptions，把输出文件作为参数output。</p><p>Java</p><pre><code>public static interface WordCountOptions extends PipelineOptions {\n    @Description(&quot;Path of the file to write to&quot;)\n    @Required\n    String getOutput();\n\n    void setOutput(String value);\n}\n</code></pre><p>完成上面两个方面的改进后，我们最终的数据处理代码会是这个样子：</p><p>Java</p><pre><code>public static void main(String[] args) {\n  WordCountOptions options =\n        PipelineOptionsFactory.fromArgs(args).withValidation().as(WordCountOptions.class);\n\n  Pipeline p = Pipeline.create(options);\n\n  p.apply(&quot;ReadLines&quot;, TextIO.read().from(options.getInputFile()))\n        .apply(new CountWords())\n        .apply(ParDo.of(new FormatAsTextFn()))\n        .apply(&quot;WriteCounts&quot;, TextIO.write().to(options.getOutput()));\n\n    p.run().waitUntilFinish();\n}\n</code></pre><h3>DoFn和PTransform的单元测试</h3><p>如同<a href=\"https://time.geekbang.org/column/article/103750\">第29讲</a>“如何测试Pipeline”中所讲的那样，我们用PAssert测试Beam Pipeline。具体在我们这个例子中，我一再强调要把数据处理操作封装成DoFn和PTransform，因为它们可以独立地进行测试。</p><p>什么意思呢？比如，ExtractWordsFn我们想要测试它能把一个句子分拆出单词，比如“\" some input words \"，我们期待的输出是[“some”, “input”, “words”]。在测试中，我们可以这样表达：</p><pre><code>/** Example test that tests a specific {@link DoFn}. */\n  @Test\n  public void testExtractWordsFn() throws Exception {\n    DoFnTester&lt;String, String&gt; extractWordsFn = DoFnTester.of(new ExtractWordsFn());\n\n    Assert.assertThat(\n        extractWordsFn.processBundle(&quot; some  input  words &quot;),\n        CoreMatchers.hasItems(&quot;some&quot;, &quot;input&quot;, &quot;words&quot;));\n    Assert.assertThat(extractWordsFn.processBundle(&quot; &quot;), CoreMatchers.hasItems());\n    Assert.assertThat(\n        extractWordsFn.processBundle(&quot; some &quot;, &quot; input&quot;, &quot; words&quot;),\n        CoreMatchers.hasItems(&quot;some&quot;, &quot;input&quot;, &quot;words&quot;));\n  }\n</code></pre><h2>小结</h2><p>这一讲我们应用前面学习的PCollection，Pipeline，Pipeline IO，Transform知识去解决了一个数据处理领域经典的WordCount问题。并且学会了一些在实际工作中改进数据处理代码质量的贴士，比如写成单独可测试的DoFn，和把程序参数封装进PipelineOptions。</p><h2>思考题</h2><p>文中提供了分词的DoFn——ExtractWordsFn，你能利用相似的思路把输出文本的格式化写成一个DoFn吗？也就是文中的FormatAsTextFn，把PCollection&lt;KV&lt;String, Long&gt;&gt; 转化成PCollection<string>，每一个元素都是<word> : <count>的格式。</count></word></string></p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"30 | Apache Beam实战冲刺：Beam如何run everywhere?","id":104253},"right":{"article_title":"32 | Beam Window：打通流处理的任督二脉","id":105707}},"comments":[{"had_liked":false,"id":151455,"user_name":"杰洛特","can_delete":false,"product_type":"c1","uid":1098146,"ip_address":"","ucode":"46D0574654F8AC","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqEacia8yO1dR5Tal9B7w8PzTRrViajlAvDph96OqcuBGe29icbXOibhibGmaBcO7BfpVia0Y8ksZwsuAYQ/132","comment_is_top":false,"comment_ctime":1573723064,"is_pvip":false,"discussion_count":4,"race_medal":0,"score":"23048559544","product_id":100025301,"comment_content":"前两章还在说不要使用任何 DoFnTester 进行测试，这边怎么又来写 DoFnTester 了？感觉这专栏像是很多人写了拼起来的，有很多前后矛盾的地方","like_count":6,"discussions":[{"author":{"id":1685428,"avatar":"https://static001.geekbang.org/account/avatar/00/19/b7/b4/f870dc5f.jpg","nickname":"是茜茜啊","note":"","ucode":"7A5EBB87C8FBFA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365545,"discussion_content":"这一章内容完全就是Apache Beam官方文档的中文翻译，甚至还不够官方文档详细...英语好的话可以直接看，给出了java, python, go三种语言的示例https://beam.apache.org/get-started/wordcount-example/","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1617835768,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1096652,"avatar":"https://static001.geekbang.org/account/avatar/00/10/bb/cc/fac12364.jpg","nickname":"xxx","note":"","ucode":"E79CEA70430449","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":544490,"discussion_content":"这个课确实好水……","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641539728,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1100750,"avatar":"https://static001.geekbang.org/account/avatar/00/10/cb/ce/d9e00eb5.jpg","nickname":"undefined","note":"","ucode":"768098DBDBE333","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365062,"discussion_content":"哈哈，亏了……","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617698729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1876212,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg","nickname":"之渊","note":"","ucode":"02B9299DBB4881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300492,"discussion_content":"我觉得意思是可以写单元测试。但是集成测试的时候不需要测试DoFntester的意思","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598147061,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":113158,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1562904508,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18742773692","product_id":100025301,"comment_content":"Beam的函数确实难用，不像spark和scala collection那样用起来直观. <br>sc.textFile(&quot;file:&#47;&#47;&#47;your-input.txt&quot;).flatMap(_.split(&quot;[^\\\\p{L}]+&quot;)).map((_,1)).reduceByKey(_+_).map(_.swap).sortByKey(false).map(_.swap).collect","like_count":4},{"had_liked":false,"id":113165,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1562906452,"is_pvip":false,"replies":[{"id":"41476","content":"谢谢你的提问！在Beam原生的Composite Transform中有一个叫Top Transform，只需要应用Top Transform然后传入一个自己实现的Comparator就好了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1563167102,"ip_address":"","comment_id":113165,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5857873748","product_id":100025301,"comment_content":"如何用Apache Beam求word count TopK问题呢？","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":458087,"discussion_content":"谢谢你的提问！在Beam原生的Composite Transform中有一个叫Top Transform，只需要应用Top Transform然后传入一个自己实现的Comparator就好了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563167102,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":113151,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1562903583,"is_pvip":false,"replies":[{"id":"41477","content":"谢谢你的提问！这个可以应用Beam中的Top Transform，实现一个Comparator Interface就可以了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1563167186,"ip_address":"","comment_id":113151,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5857870879","product_id":100025301,"comment_content":"如果要按word出现次数从大到小排序应该怎么写？","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":458078,"discussion_content":"谢谢你的提问！这个可以应用Beam中的Top Transform，实现一个Comparator Interface就可以了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563167186,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":111682,"user_name":"李孟","can_delete":false,"product_type":"c1","uid":1006768,"ip_address":"","ucode":"AD2349CB12F130","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5c/b0/77e5f8c8.jpg","comment_is_top":false,"comment_ctime":1562582513,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5857549809","product_id":100025301,"comment_content":"这还是比较重，spark求同样的需求几个函数就搞定了","like_count":1},{"had_liked":false,"id":243493,"user_name":"之渊","can_delete":false,"product_type":"c1","uid":1876212,"ip_address":"","ucode":"02B9299DBB4881","user_header":"https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg","comment_is_top":false,"comment_ctime":1598154596,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1598154596","product_id":100025301,"comment_content":"实例代码 https:&#47;&#47;gitee.com&#47;oumin12345&#47;daimademojihe&#47;blob&#47;e20d60b93113d2537f4bd2e7f38b23ac17d4c3c0&#47;cloudx&#47;bigdata&#47;src&#47;main&#47;java&#47;test&#47;beam&#47;WordCountBeam.java<br>从我这个新人角度来看。虽然代码可能看起来没有spark 那些那么简洁。但是编程思想就是全部都是transform 。而且都是 链式调用，apply(xx).apply(xx) 其实没有差到哪里去。<br>1. 上手难得大大减低。spark 太多算子了，什么并发算子，什么action类的啊，等等。概念太多了。而beam 帮我们自动优化了。就好像在写很传统的java 代码一样。而且也非常易于理解。<br>2. 如果要类比的话，就好像 Apache beam 就是mybatis ， 而 spark ,flink 就是 hibernate了。","like_count":0},{"had_liked":false,"id":233728,"user_name":"Hank_Yan","can_delete":false,"product_type":"c1","uid":1110662,"ip_address":"","ucode":"86899B561C502B","user_header":"https://static001.geekbang.org/account/avatar/00/10/f2/86/d689f77e.jpg","comment_is_top":false,"comment_ctime":1594427285,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1594427285","product_id":100025301,"comment_content":"还是spark方便一些。。。beam看了这么多节，只能感受到其出发点，问题抽象的独到之处，很难感受实际工作中会带来什么益处，文中例子不太容易领悟到这点。","like_count":0},{"had_liked":false,"id":194056,"user_name":"jeeeeeennnny","can_delete":false,"product_type":"c1","uid":1866565,"ip_address":"","ucode":"374B5E28287FB7","user_header":"https://static001.geekbang.org/account/avatar/00/1c/7b/45/871a64fb.jpg","comment_is_top":false,"comment_ctime":1585019832,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1585019832","product_id":100025301,"comment_content":"Sideinput 可以根据业务逻辑新增数据吗？","like_count":0,"discussions":[{"author":{"id":1866565,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/7b/45/871a64fb.jpg","nickname":"jeeeeeennnny","note":"","ucode":"374B5E28287FB7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":214955,"discussion_content":"还有一点不是很明白，如果想用JDBCIO读表数据，怎么设置批量5000条取，怎么根据表数据更新定时去update这个output Map。不知道可否有小伙伴指导一下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585266336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}