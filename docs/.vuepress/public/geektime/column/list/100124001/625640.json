{"id":625640,"title":"48 | 完善核心能力：Master请求转发与Worker资源管理","content":"<p>你好，我是郑建勋。</p><p>这节课，让我们继续优化Master服务，实现Master请求转发和并发情况下的资源保护，同时实现Worker对分配资源的监听。</p><h2>将Master请求转发到Leader</h2><p>首先我们需要考虑一下，当Master是Follower状态，同时还接收到了请求的情形。在之前的设计中，为了避免并发处理时可能出现的异常情况，我们只打算让Leader来处理请求。所以，当Master节点接收到请求时，如果当前节点不是Leader，我们可以直接报错，由客户端选择正确的Leader节点。如下所示。</p><pre><code class=\"language-plain\">func (m *Master) AddResource(ctx context.Context, req *proto.ResourceSpec, resp *proto.NodeSpec) error {\n\tif !m.IsLeader() {\n\t\treturn errors.New(\"no leader\")\n\t}\n}\n</code></pre><p>我们还可以采用另外一种更常见的方式：将接收到的请求转发给Leader。要实现这一点，首先所有Master节点要在Leader发生变更时，将当前最新的Leader地址保存到leaderID中。</p><pre><code class=\"language-plain\">func (m *Master) Campaign() {\n\tselect {\n\t\tcase resp := &lt;-leaderChange:\n\t\t\tm.logger.Info(\"watch leader change\", zap.String(\"leader:\", string(resp.Kvs[0].Value)))\n\t\t\tm.leaderID = string(resp.Kvs[0].Value)\n\t\t}\t\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase err := &lt;-leaderCh:\n\t\t\t\t\tm.leaderID = m.ID\n\t\t\tcase resp := &lt;-leaderChange:\n\t\t\t\tif len(resp.Kvs) &gt; 0 {\n\t\t\t\t\tm.logger.Info(\"watch leader change\", zap.String(\"leader:\", string(resp.Kvs[0].Value)))\n\t\t\t\t\tm.leaderID = string(resp.Kvs[0].Value)\n\t\t\t\t}\n\t\t}\n}\n</code></pre><!-- [[[read_end]]] --><p>在处理请求前，首先判断当前Master的状态，如果它不是Leader，就获取Leader的地址并完成请求的转发。注意，这里如果不指定Leader的地址，go-micro就会随机选择一个地址进行转发。</p><pre><code class=\"language-plain\">func (m *Master) AddResource(ctx context.Context, req *proto.ResourceSpec, resp *proto.NodeSpec) error {\n\tif !m.IsLeader() &amp;&amp; m.leaderID != \"\" &amp;&amp; m.leaderID != m.ID {\n\t\taddr := getLeaderAddress(m.leaderID)\n\t\tnodeSpec, err := m.forwardCli.AddResource(ctx, req, client.WithAddress(addr))\n\t\tresp.Id = nodeSpec.Id\n\t\tresp.Address = nodeSpec.Address\n\t\treturn err\n\t}\n\n\tnodeSpec, err := m.addResources(&amp;ResourceSpec{Name: req.Name})\n\tif nodeSpec != nil {\n\t\tresp.Id = nodeSpec.Node.Id\n\t\tresp.Address = nodeSpec.Node.Address\n\t}\n\treturn err\n}\n</code></pre><p>在转发时，我们使用了micro生成的GRPC客户端，这是通过在初始化时导入 micro GRPC client的插件实现的。SetForwardCli方法将生成的GRPC client 注入到了Master结构体中。</p><pre><code class=\"language-plain\">import (\n\tgrpccli \"github.com/go-micro/plugins/v4/client/grpc\"\n)\n\nfunc RunGRPCServer(m *master.Master, logger *zap.Logger, reg registry.Registry, cfg ServerConfig) {\n\tservice := micro.NewService(\n\t\t...\n\t\tmicro.Client(grpccli.NewClient()),\n\t)\n\n\tcl := proto.NewCrawlerMasterService(cfg.Name, service.Client())\n\tm.SetForwardCli(cl)\n}\n</code></pre><p>接下来，我们来验证一下服务是否能够正确地转发。</p><p>首先启动一个 Worker和一个Master服务，当前的 Leader会变成master2，IP地址为192.168.0.105:9091。</p><pre><code class=\"language-plain\">» go run main.go worker  --pprof=:9983\n» go run main.go master --id=2 --http=:8081  --grpc=:9091\n</code></pre><p>现在我们启动一个新的Master服务master3。</p><pre><code class=\"language-plain\">» go run main.go master --id=3 --http=:8082  --grpc=:9092 --pprof=:9982\n</code></pre><p>接着访问master3服务暴露的HTTP接口。虽然master3并不是Leader，但是访问master3添加资源时，操作仍然能够成功。</p><pre><code class=\"language-plain\">» curl  --request POST 'http://localhost:8082/crawler/resource' --header 'Content-Type: application/json' --data '{\"id\":\"zjx\",\"name\": \"task-forward\"}' \n{\"id\":\"go.micro.server.worker-1\",\"Address\":\"192.168.0.105:9090\"}\n</code></pre><p>同时，我们在Leader服务的日志中能够看到请求信息，验证成功。</p><pre><code class=\"language-plain\">{\"level\":\"INFO\",\"ts\":\"2022-12-29T17:23:55.792+0800\",\"caller\":\"master/master.go:198\",\"msg\":\"receive request\",\"method\":\"CrawlerMaster.AddResource\",\"Service\":\"go.micro.server.master\",\"request param:\":{\"id\":\"zjx\",\"name\":\"task-forward\"}}\n</code></pre><h2>资源保护</h2><p>由于Worker节点与Resource资源一直在动态变化当中，因此如果不考虑数据的并发安全，在复杂线上场景下，就可能出现很多难以解释的现象。</p><p>为了避免数据的并发安全问题，我们之前利用了通道来进行协程间的通信，但<strong>如果我们现在希望保护Worker节点与Resource资源，</strong><strong>其实当前场景下更好的方式是使用原生的互斥锁。</strong>这是因为我们只希望在关键位置加锁，其他的逻辑仍然是并行的。如果我们在读取一个变量时还要用通道来通信，代码会变得不优雅。</p><p>我们来看下使用原生互斥锁的操作是怎样的。如下，在Master中添加sync.Mutex互斥锁，用于资源的并发安全。</p><pre><code class=\"language-plain\">type Master struct {\n\t...\n\tID         string\n\trlock      sync.Mutex\n\n\toptions\n}\n</code></pre><p>我们可以在资源更新（资源加载与增删查改）、Worker节点更新、资源分配的阶段都加入互斥锁如下所示。</p><pre><code class=\"language-plain\">func (m *Master) DeleteResource(ctx context.Context, spec *proto.ResourceSpec, empty *empty.Empty) error {\n\tm.rlock.Lock()\n\tdefer m.rlock.Unlock()\n\n\tr, ok := m.resources[spec.Name]\n\t...\n}\n\nfunc (m *Master) AddResource(ctx context.Context, req *proto.ResourceSpec, resp *proto.NodeSpec) error {\n\t...\n\tm.rlock.Lock()\n\tdefer m.rlock.Lock()\n\tnodeSpec, err := m.addResources(&amp;ResourceSpec{Name: req.Name})\n\tif nodeSpec != nil {\n\t\tresp.Id = nodeSpec.Node.Id\n\t\tresp.Address = nodeSpec.Node.Address\n\t}\n\treturn err\n}\n\nfunc (m *Master) updateWorkNodes() {\n\tservices, err := m.registry.GetService(worker.ServiceName)\n\tif err != nil {\n\t\tm.logger.Error(\"get service\", zap.Error(err))\n\t}\n\n\tm.rlock.Lock()\n\tdefer m.rlock.Unlock()\n\t...\n\tm.workNodes = nodes\n}\n\nfunc (m *Master) loadResource() error {\n\tresp, err := m.etcdCli.Get(context.Background(), RESOURCEPATH, clientv3.WithPrefix(), clientv3.WithSerializable())\n\t...\n\tresources := make(map[string]*ResourceSpec)\n\tm.rlock.Lock()\n\tdefer m.rlock.Unlock()\n\tm.resources = resources\n}\n\nfunc (m *Master) reAssign() {\n\trs := make([]*ResourceSpec, 0, len(m.resources))\n\n\tm.rlock.Lock()\n\tdefer m.rlock.Unlock()\n\n\tfor _, r := range m.resources {...}\n\n\tfor _, r := range rs {\n\t\tm.addResources(r)\n\t}\n}\n</code></pre><p>当外部访问Leader的HTTP接口时，实际上服务端会开辟一个协程并发处理请求。通过使用互斥锁，我们消除了并发访问同一资源可能出现的问题。在实践中，需要合理地使用互斥锁，尽量让锁定的范围足够小，锁定的资源足够少，减少锁等待的时间。</p><h2>Worker单机模式</h2><p>接下来我们回到Worker。Worker可以有两种模式，集群模式与单机模式。我们可以在Worker中加一个flag来切换Worker运行的模式。</p><p>对于少量的任务，可以直接用单机版的Worker来处理，种子节点来自于配置文件。而对于集群版的Worker，任务将来自Master的分配。</p><p>要切换Worker模式只要判断一个flag值cluster即可做到。如下所示，在启动Worker时，如果cluster为false，代表为单机模式。如果cluster为true，代表是集群模式。</p><pre><code class=\"language-plain\">\nWorkerCmd.Flags().BoolVar(\n\t\t&amp;cluster, \"cluster\", true, \"run mode\")\nvar cluster bool\n\nfunc (c *Crawler) Run(cluster bool) {\n\tif !cluster {\n\t\tc.handleSeeds()\n\t}\n\n\tgo c.Schedule()\n\tfor i := 0; i &lt; c.WorkCount; i++ {\n\t\tgo c.CreateWork()\n\t}\n\tc.HandleResult()\n}\n</code></pre><h2>Worker集群模式</h2><p>在集群模式下，我们还需要书写Worker加载和监听etcd资源这一重要的功能。首先来看看初始化时的资源加载，在初始时，我们生成了etcd client，并注入到Crawler结构中。</p><pre><code class=\"language-plain\">endpoints := []string{e.registryURL}\ncli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})\nif err != nil {\n\treturn nil, err\n}\ne.etcdCli = cli\n</code></pre><h3>资源加载</h3><p>Crawler.loadResource方法用于从etcd中加载资源。我们调用etcd Get方法，获取前缀为<code>/resources</code> 的全量资源列表。解析这些资源，查看当前资源分配的节点是否为当前节点。如果分配的节点和当前节点匹配，意味着当前资源是分配给当前节点的，不是当前节点的资源将会被直接忽略。</p><pre><code class=\"language-plain\">func (c *Crawler) loadResource() error {\n\tresp, err := c.etcdCli.Get(context.Background(), master.RESOURCEPATH, clientv3.WithPrefix(), clientv3.WithSerializable())\n\tif err != nil {\n\t\treturn fmt.Errorf(\"etcd get failed\")\n\t}\n\n\tresources := make(map[string]*master.ResourceSpec)\n\tfor _, kv := range resp.Kvs {\n\t\tr, err := master.Decode(kv.Value)\n\t\tif err == nil &amp;&amp; r != nil {\n\t\t\tid := getID(r.AssignedNode)\n\t\t\tif len(id) &gt; 0 &amp;&amp; c.id == id {\n\t\t\t\tresources[r.Name] = r\n\t\t\t}\n\t\t}\n\t}\n\tc.Logger.Info(\"leader init load resource\", zap.Int(\"lenth\", len(resources)))\n\tc.rlock.Lock()\n\tdefer c.rlock.Unlock()\n\tc.resources = resources\n\tfor _, r := range c.resources {\n\t\tc.runTasks(r.Name)\n\t}\n\n\treturn nil\n}\n</code></pre><p>资源加载完毕后，分配给当前节点的任务会执行runTask方法，通过任务名从全局任务池中获取爬虫任务，调用t.Rule.Root()获取种子请求，并放入到调度器中执行。</p><pre><code class=\"language-plain\">func (c *Crawler) runTasks(taskName string) {\n\tt, ok := Store.Hash[taskName]\n\tif !ok {\n\t\tc.Logger.Error(\"can not find preset tasks\", zap.String(\"task name\", taskName))\n\t\treturn\n\t}\n\tres, err := t.Rule.Root()\n\n\tif err != nil {\n\t\tc.Logger.Error(\"get root failed\",\n\t\t\tzap.Error(err),\n\t\t)\n\t\treturn\n\t}\n\n\tfor _, req := range res {\n\t\treq.Task = t\n\t}\n\tc.scheduler.Push(res...)\n}\n</code></pre><h3>资源监听</h3><p>除了加载资源，在初始化时我们还需要开辟一个新的协程c.watchResource来监听资源的变化。</p><pre><code class=\"language-plain\">func (c *Crawler) Run(id string, cluster bool) {\n\tc.id = id\n\tif !cluster {\n\t\tc.handleSeeds()\n\t}\n\tgo c.loadResource()\n\tgo c.watchResource()\n\tgo c.Schedule()\n\tfor i := 0; i &lt; c.WorkCount; i++ {\n\t\tgo c.CreateWork()\n\t}\n\tc.HandleResult()\n}\n</code></pre><p>如下所示，我在watchResource函数中书写了一个监听新增资源的功能。watchResource借助etcd client 的Watch方法监听资源的变化。Watch返回值是一个通道，当etcd client监听到etcd中前缀为 <code>/resources</code> 的资源发生变化时，就会将信息写入到通道Watch中。通过通道返回的信息，不仅能够得到当前有变动的资源最新的值，还可以得知当前资源变动的事件是新增、更新还是删除。如果是新增事件，那就调用runTasks启动该资源对应的爬虫任务。</p><pre><code class=\"language-plain\">func (c *Crawler) watchResource() {\n\twatch := c.etcdCli.Watch(context.Background(), master.RESOURCEPATH, clientv3.WithPrefix())\n\tfor w := range watch {\n\t\tif w.Err() != nil {\n\t\t\tc.Logger.Error(\"watch resource failed\", zap.Error(w.Err()))\n\t\t\tcontinue\n\t\t}\n\t\tif w.Canceled {\n\t\t\tc.Logger.Error(\"watch resource canceled\")\n\t\t\treturn\n\t\t}\n\t\tfor _, ev := range w.Events {\n\t\t\tspec, err := master.Decode(ev.Kv.Value)\n\t\t\tif err != nil {\n\t\t\t\tc.Logger.Error(\"decode etcd value failed\", zap.Error(err))\n\t\t\t}\n\n\t\t\tswitch ev.Type {\n\t\t\tcase clientv3.EventTypePut:\n\t\t\t\tif ev.IsCreate() {\n\t\t\t\t\tc.Logger.Info(\"receive create resource\", zap.Any(\"spec\", spec))\n\n\t\t\t\t} else if ev.IsModify() {\n\t\t\t\t\tc.Logger.Info(\"receive update resource\", zap.Any(\"spec\", spec))\n\t\t\t\t}\n\t\t\t\tc.runTasks(spec.Name)\n\t\t\tcase clientv3.EventTypeDelete:\n\t\t\t\tc.Logger.Info(\"receive delete resource\", zap.Any(\"spec\", spec))\n\t\t\t}\n\t\t}\n\t}\n}\n</code></pre><p>现在让我们来验证一下新增资源的功能，启动Master与Worker节点。</p><pre><code class=\"language-plain\">» go run main.go master --id=3 --http=:8082  --grpc=:9092 --pprof=:9982\n» go run main.go worker  --pprof=:9983\n</code></pre><p>紧接着调用Master的添加资源接口。</p><pre><code class=\"language-plain\">» curl -H \"content-type: application/json\" -d '{\"id\":\"zjx\",\"name\": \"douban_book_list\"}' &lt;http://localhost:8082/crawler/resource&gt;         jackson@localhost\n{\"id\":\"go.micro.server.worker-1\", \"Address\":\"192.168.0.105:9090\"}\n</code></pre><p>可以看到，Worker日志中任务开始正常地执行了，验证成功。完整代码你可以查看<a href=\"https://github.com/dreamerjackson/crawler\">v0.4.1分支</a>。</p><pre><code class=\"language-plain\">{\"level\":\"DEBUG\",\"ts\":\"2022-12-30T21:06:56.743+0800\",\"caller\":\"doubanbook/book.go:77\",\"msg\":\"parse book tag,count: 47\"}\n{\"level\":\"DEBUG\",\"ts\":\"2022-12-30T21:07:00.532+0800\",\"caller\":\"doubanbook/book.go:108\",\"msg\":\"parse book list,count: 20 url: &lt;https://book.douban.com/tag/随笔&gt;\"}\n{\"level\":\"DEBUG\",\"ts\":\"2022-12-30T21:07:04.240+0800\",\"caller\":\"doubanbook/book.go:108\",\"msg\":\"parse book list,count: 20 url: &lt;https://book.douban.com/tag/散文&gt;\"}\n</code></pre><h3>资源删除</h3><p>接下来让我们继续看看如何删除一个爬虫任务。</p><p>我们需要在Watch的选项中设置 <code>clientv3.WithPrevKV()</code>，这样，当监听到资源的删除时，就能够获取当前删除的资源信息，接着就可以调用 <code>c.deleteTasks</code> 来删除任务了。</p><pre><code class=\"language-plain\">func (c *Crawler) watchResource() {\n\twatch := c.etcdCli.Watch(context.Background(), master.RESOURCEPATH, clientv3.WithPrefix(), clientv3.WithPrevKV())\n\tfor w := range watch {\n\t\tfor _, ev := range w.Events {\n\t\t\tswitch ev.Type {\n\t\t\t...\n\t\t\tcase clientv3.EventTypeDelete:\n\t\t\t\tspec, err := master.Decode(ev.PrevKv.Value)\n\t\t\t\tc.Logger.Info(\"receive delete resource\", zap.Any(\"spec\", spec))\n\t\t\t\tif err != nil {\n\t\t\t\t\tc.Logger.Error(\"decode etcd value failed\", zap.Error(err))\n\t\t\t\t}\n\t\t\t\tc.rlock.Lock()\n\t\t\t\tc.deleteTasks(spec.Name)\n\t\t\t\tc.rlock.Unlock()\n\t\t\t}\n\t\t}\n\t}\n}\n</code></pre><p>deleteTasks 会删除 <code>c.resources</code> 中存储的当前Task，并且将Task的Closed变量设置为true。</p><pre><code class=\"language-plain\">func (c *Crawler) deleteTasks(taskName string) {\n\tt, ok := Store.Hash[taskName]\n\tif !ok {\n\t\tc.Logger.Error(\"can not find preset tasks\", zap.String(\"task name\", taskName))\n\t\treturn\n\t}\n\tt.Closed = true\n\tdelete(c.resources, taskName)\n}\n</code></pre><p>我们在Task中设计了一个新的变量Closed用于标识当前的任务是否已经被删除了。这是因为被删除的任务可能现在还在运行当中，我们通过该变量确认它已经不再运行了。</p><p>在一些场景中，我们也可以将标识任务是否已经结束的变量设计为通道类型或者context.Context，然后与select语句结合起来实现多路复用。我们在HTTP标准库中也经常看到这种用法，它可以判断通道的事件与其他事件哪一个先发生。</p><pre><code class=\"language-plain\">type Task struct {\n\tVisited     map[string]bool\n\tVisitedLock sync.Mutex\n\n\t//\n\tClosed bool\n\n\tRule RuleTree\n\tOptions\n}\n</code></pre><p>不过我们这里使用一个标识任务是否关闭的bool类型就足够了。在任务流程的核心位置，我们都需要检测该变量。检测到任务关闭时，就不再执行后续的流程。</p><p>具体操作是在request.Check方法中，加入对任务是否关闭的判断。</p><pre><code class=\"language-plain\">func (r *Request) Check() error {\n\tif r.Depth &gt; r.Task.MaxDepth {\n\t\treturn errors.New(\"max depth limit reached\")\n\t}\n\n\tif r.Task.Closed {\n\t\treturn errors.New(\"task has Closed\")\n\t}\n\n\treturn nil\n}\n</code></pre><p>接着，在任务的采集和调度的两个核心位置检测任务的有效性。一旦发现任务已经被关闭，它所有的请求将不再被调度和采集。</p><pre><code class=\"language-plain\">func (c *Crawler) CreateWork() {\n\tfor {\n\t\treq := c.scheduler.Pull()\n\t\tif err := req.Check(); err != nil {\n\t\t\tc.Logger.Debug(\"check failed\",\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\n\t\t\tcontinue\n\t\t}\n}\n\nfunc (s *Schedule) Schedule() {\n\tvar ch chan *spider.Request\n\n\tvar req *spider.Request\n\n\tfor {\n\t\t...\n\t\t// 请求校验\n\t\tif req != nil {\n\t\t\tif err := req.Check(); err != nil {\n\t\t\t\tzap.S().Debug(\"check failed\",\n\t\t\t\t\tzap.Error(err),\n\t\t\t\t)\n\t\t\t\treq = nil\n\t\t\t\tch = nil\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n}\n</code></pre><p>下面让我们来验证一下任务的删除功能是否正常，首先启动一个Master和一个Worker服务。</p><pre><code class=\"language-plain\">» go run main.go master --id=3 --http=:8082  --grpc=:9092 --pprof=:9982\n» go run main.go worker  --pprof=:9983\n</code></pre><p>紧接着，调用Master的添加资源接口，可以看到爬虫任务是正常执行的。</p><pre><code class=\"language-plain\">» curl -H \"content-type: application/json\" -d '{\"id\":\"zjx\",\"name\": \"douban_book_list\"}' &lt;http://localhost:8082/crawler/resource&gt;         jackson@localhost\n{\"id\":\"go.micro.server.worker-1\", \"Address\":\"192.168.0.105:9090\"}\n</code></pre><p>然后，我们调用Master的删除资源接口，从Worker中的日志可以看到，Worker监听到了删除资源的事件。在日志打印出 <code>\"task has Closed\"</code> 的错误信息之后，删除的爬虫任务将不再运行。</p><pre><code class=\"language-plain\">{\"level\":\"INFO\",\"ts\":\"2022-12-31T14:06:33.528+0800\",\"caller\":\"engine/schedule.go:479\",\"msg\":\"receive delete resource\",\"spec\":{\"ID\":\"1609068436011356160\",\"Name\":\"douban_book_list\",\"AssignedNode\":\"go.micro.server.worker-1|192.168.0.105:9090\",\"CreationTime\":1672466784878532000}\n{\"level\":\"DEBUG\",\"ts\":\"2022-12-31T14:06:33.845+0800\",\"caller\":\"engine/schedule.go:336\",\"msg\":\"check failed\",\"error\":\"task has Closed\"}\n{\"level\":\"DEBUG\",\"ts\":\"2022-12-31T14:06:33.845+0800\",\"caller\":\"engine/schedule.go:269\",\"msg\":\"check failed\",\"error\":\"task has Closed\"}\n</code></pre><p>此后，当我们再次调用Master的添加资源接口时，爬虫任务又将恢复如初。删除功能验证成功。</p><h2>总结</h2><p>好了，这节课，我们设计了将Master请求转发到Leader的功能，让所有的Master都具备了接收请求的能力。此外，我们还使用了原生的互斥锁解决了并发安全问题。因为通道并不总是解决并发安全问题的最佳方式，在这里如果我们使用通道会减慢程序的并发性，使代码变得不优雅。</p><p>最后，我们还实现了在Worker集群模式下任务的加载与监听。在初始化时，我们通过加载etcd中属于当前节点的资源获取了全量的爬虫任务。我们还启动了对etcd资源的监听，实现了资源的动态添加和删除。至此，Master与Worker的核心功能与交互都已经能够正常工作了。</p><h2>课后题</h2><p>最后，还是给你留一道思考题。</p><p>在我们的设计中，默认一个爬虫任务是不能够被添加多次的。那有没有一种场景，可以让同一个爬虫任务添加多次，也就是让多个Worker可以同时执行同一个爬虫任务呢? 如果有这样的场景，我们应该如何修改设计？</p><p>欢迎你在留言区与我交流讨论，我们下节课再见！</p>","comments":[{"had_liked":false,"id":368906,"user_name":"Geek_crazydaddy","can_delete":false,"product_type":"c1","uid":2663289,"ip_address":"江苏","ucode":"11A6FF71825CA7","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEIqvYMQ1yscgB6xS4nDkoOuP6KiaCiaichQA1OiaQ9rFmNtT9icgrZxeH1WRn5HfiaibDguj8e0lBpo65ricA/132","comment_is_top":false,"comment_ctime":1676882755,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"watchResource里获取和删除任务时为啥都不判断任务是不是分配给当前worker了？","like_count":0,"discussions":[{"author":{"id":1393764,"avatar":"https://static001.geekbang.org/account/avatar/00/15/44/64/3380fc05.jpg","nickname":"Ares","note":"","ucode":"5E8D4387D35911","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648403,"discussion_content":"我也有这个疑问🤔 @郑老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1721544784,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2908917,"avatar":"https://static001.geekbang.org/account/avatar/00/2c/62/f5/339046cd.jpg","nickname":"嗨了","note":"","ucode":"72AECB49813E7F","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627227,"discussion_content":"我也觉得 @作者","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693903061,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":367962,"user_name":"Realm","can_delete":false,"product_type":"c1","uid":1081299,"ip_address":"浙江","ucode":"30CBEBE619D1A2","user_header":"https://static001.geekbang.org/account/avatar/00/10/7f/d3/b5896293.jpg","comment_is_top":false,"comment_ctime":1675768199,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100124001,"comment_content":"follow节点在收到资源变更请求，当请求到达grpc服务层时，通过注入进来的master grpc client，向master发起请求，参数不变，实现了转发功能，这个设计很赞！👍","like_count":0}]}