{"id":102182,"title":"26 | Pipeline：Beam如何抽象多步骤的数据流水线？","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“Pipeline：Beam如何抽象多步骤的数据流水线”。</p><p>在上两讲中，我们一起学习了Beam是如何抽象封装数据，以及如何抽象对于数据集的转换操作的。在掌握了这两个基本概念后，我们就可以很好地回答Beam编程模型里的4个维度What、Where、When、How中的第一个问题——What了。也就是，我们要做什么计算？想得到什么样的结果？</p><p><img src=\"https://static001.geekbang.org/resource/image/71/bb/71c8ace006d56d7f6fe93cbc56dc91bb.png?wh=1142*770\" alt=\"unpreview\"></p><p>这个时候你可能已经跃跃欲试，开始想用PCollection和Transform解决我们平常经常会使用到的批处理任务了。没有问题，那我们就先抛开Where、When和How这三个问题，由简至繁地讲起。</p><p>现在假设我们的数据处理逻辑只需要处理有边界数据集，在这个情况下，让我们一起来看看Beam是如何运行一套批处理任务的。</p><h2>数据流水线</h2><p>在Beam的世界里，所有的数据处理逻辑都会被抽象成<strong>数据流水线（Pipeline）</strong>来运行。那么什么是数据流水线呢？</p><p>Beam的数据流水线是对于数据处理逻辑的一个封装，它包括了从<strong>读取数据集</strong>，<strong>将数据集转换成想要的结果</strong>和<strong>输出结果数据集</strong>这样的一整套流程。</p><p>所以，如果我们想要跑自己的数据处理逻辑，就必须在程序中创建一个Beam数据流水线出来，比较常见的做法是在main()函数中直接创建。</p><!-- [[[read_end]]] --><p>Java</p><pre><code>PipelineOptions options = PipelineOptionsFactory.create();\nPipeline p = Pipeline.create(options);\n</code></pre><p>在创建Beam数据流水线的同时，我们必须给这个流水线定义一个<strong>选项</strong>（Options）。这个选项会告诉Beam，用户的Pipeline应该如何运行。例如，是在本地的内存上运行，还是在Apache Flink上运行？关于具体Beam选项的解释，我会在第30讲中展开讲解。</p><h2>Beam数据流水线的应用</h2><p>有了数据流水线这个抽象概念之后，我们就可以将PCollection和Transform应用在这个流水线里面了。</p><p><img src=\"https://static001.geekbang.org/resource/image/a5/94/a56f824d0dc8b3c1a777595b42c4b294.jpg?wh=1994*1302\" alt=\"\"></p><p>上图就是一个Beam的数据流水线，整个数据流水线包括了从读取数据，到经过了N个Transform之后输出数据的整个过程。</p><p>在<a href=\"https://time.geekbang.org/column/article/100666\">第24讲</a>中我们学习过PCollection的不可变性。也就是说，一个PCollection一经生成，我们就不能够再增加或者删除它里面的元素了。所以，在Beam的数据流水线中，每次PCollection经过一个Transform之后，流水线都会新创建一个PCollection出来。而这个新的PCollection又将成为下一个Transform的新输入。</p><p><img src=\"https://static001.geekbang.org/resource/image/47/4b/47e4856cfdcb771c135417741d4d044b.jpg?wh=1754*1088\" alt=\"\"></p><p>在上图的示例中，Beam数据流水线在经过Transform1读取了输入数据集之后，会创建出一个新的PCollection1，而经过了Transform2之后，数据流水线又会创建出新的PCollection2出来，同时PCollection1不会有任何改变。也就是说，在上面的例子中，除去最终的输出结果，数据流水线一共创建了3个不同的PCollection出来。</p><p>这种特性可以让我们在编写数据处理逻辑的时候，对同一个PCollection应用多种不同的Transfrom。</p><p>例如下图所示，对于PCollection1，我们可以使三个不同的Transform应用在它之上，从而再产生出三个不同的PCollection2、PCollection3和PCollection4出来。</p><p><img src=\"https://static001.geekbang.org/resource/image/ee/ef/eeb81605c09e4a6cc684176ef0a9c9ef.jpg?wh=2168*1040\" alt=\"\"></p><h2>Beam数据流水线的处理模型</h2><p>在了解完Beam数据流水线高度抽象的概念后，紧接着，我想和你介绍一下Beam数据流水线的处理模型，也就是数据流水线在运行起来之后，会发生些什么，它是如何处理我们定义好的PCollection和Transform的。</p><p>Beam数据流水线的底层思想其实还是动用了MapReduce的原理，在分布式环境下，整个数据流水线会启动N个Workers来同时处理PCollection。而在具体处理某一个特定Transform的时候，数据流水线会将这个Transform的输入数据集PCollection里面的元素分割成不同的Bundle，将这些Bundle分发给不同的Worker来处理。</p><p>Beam数据流水线具体会分配多少个Worker，以及将一个PCollection分割成多少个Bundle都是随机的。但Beam数据流水线会尽可能地让整个处理流程达到<strong>完美并行</strong>（Embarrassingly Parallel）。</p><p>我想举个几个例子让你更好地来理解这个概念。</p><p>假设在数据流水线的一个Transform里面，它的输入数据集PCollection是1、2、3、4、5、6这个6个元素。数据流水线可能会将这个PCollection按下图的方式将它分割成两个Bundles。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/1d/1ec163043a8e8e18928ed4771cac671d.jpg?wh=1642*716?wh=1642*716\" alt=\"\"></p><p>当然，PCollection也有可能会被分割成三个Bundles。</p><p><img src=\"https://static001.geekbang.org/resource/image/87/2b/87c924863790f3564949b416a98a6c2b.jpg?wh=1650*868\" alt=\"\"></p><p>那数据流水线会启用多少个Worker来处理这些Bundle呢？这也是任意的。还是以刚刚的PCollection输入数据集作为例子，如果PCollection被分割成了两个Bundles，数据流水线有可能会分配两个Worker来处理这两个Bundles。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/33/32cf33cae5a581b6b5d5739bfe775533.jpg?wh=1406*738?wh=1406*738\" alt=\"\"></p><p>甚至有可能只分配一个Worker来处理这两个Bundles。</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/29/d8d53d23ea0d507055e003cb2e07cb29.jpg?wh=1370*534\" alt=\"\"></p><p>在多步骤的Transforms中，一个Bundle通过一个Transform产生出来的结果会作为下一个Transform的输入。</p><p>之前刚刚讲过，在Beam数据流水线中，抽象出来的PCollection经过一个Transform之后，流水线都会新创建一个PCollection出来。同样的，Beam在真正运行的时候，每一个Bundle在一个Worker机器里经过Transform逻辑后，也会产生出来一个新的Bundle，它们也是具有不可变性的。像这种具有关联性的Bundle，必须在同一个Worker上面处理。</p><p>我现在来举例说明一下上面的概念。现在假设输入数据集如下图所示，它被分成了两个Bundles。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/1d/1ec163043a8e8e18928ed4771cac671d.jpg?wh=1642*716?wh=1642*716\" alt=\"\"></p><p>我们现在需要做两个Transforms。第一个Transform会将元素的数值减一；第二个Transform会对元素的数值求平方。整个过程被分配到了两个Workers上完成。</p><p><img src=\"https://static001.geekbang.org/resource/image/57/fd/574e866c6609c6551083d55ff534cffd.jpg?wh=1948*812\" alt=\"\"></p><p>过程就如上图所示，总共产生了6个不可变的Bundle出来，从Bundle1到Bundle3的整个过程都必须放在Worker1上完成，因为它们都具有关联性。同样的，从Bundle4到Bundle6的整个过程也都必须放在Worker2上完成。</p><h2>Beam数据流水线的错误处理</h2><p>在学习完Beam数据流水线底层的处理模型之后，你可能会有个疑问：既然Bundle都是放在分布式环境下处理的，要是其中一个步骤出错了，那数据流水线会做什么样的处理？接下来我会给你讲解一下Beam数据流水线的错误处理机制。</p><h3>单个Transform上的错误处理</h3><p>我们还是以单个Transform开始讲解。在一个Transform里面，如果某一个Bundle里面的元素因为任意原因导致处理失败了，则这整个Bundle里的元素都必须重新处理。</p><p>还是假设输入数据集如下图所示，被分成了两个Bundles。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/33/32cf33cae5a581b6b5d5739bfe775533.jpg?wh=1406*738?wh=1406*738\" alt=\"\"></p><p>Beam数据流水线分配了两个Worker来处理这两个Bundles。我们看到下图中，在Worker2处理Bundle2的时候，最后一个元素6处理失败了。</p><p><img src=\"https://static001.geekbang.org/resource/image/e4/91/e4e87019b6e646073a4234348c346091.jpg?wh=1338*602\" alt=\"\"></p><p>这个时候，即便Bundle2的元素5已经完成了处理，但是因为同一个Bundle里面的元素处理失败，所以整个Bundle2都必须拿来重新处理。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/7b/2c80f7616367535a4bae5d036d75ff7b.jpg?wh=1418*720\" alt=\"\"></p><p>重新处理的Bundle也不一定要在原来的Worker里面被处理，有可能会被转移到另外的Worker里面处理。如上图所示，需要重新被处理的Bundle2就被转移到Worker1上面处理了。</p><h3>多步骤Transform上的错误处理</h3><p>学习完单个Transform上的错误处理机制，我们再来看看在多步骤的Transform上发生错误时是如何处理的。</p><p>在多步骤的Transform上，如果处理的一个Bundle元素发生错误了，则这个元素所在的整个Bundle以及与这个Bundle有关联的所有Bundle都必须重新处理。</p><p>我们还是用上面的多步骤Transform来讲解这个例子。</p><p><img src=\"https://static001.geekbang.org/resource/image/93/25/939e3cf386d5ae416dd878743d98be25.jpg?wh=2126*858\" alt=\"\"></p><p>你可以看到，在Worker2中，处理Transform2逻辑的时候生成Bundle6里面的第一个元素失败了。因为Bundle4、Bundle5和Bundle6都是相关联的，所以这三个Bundle都会被重新处理。</p><h2>小结</h2><p>今天我们一起学习了Beam里对于数据处理逻辑的高度抽象数据流水线，以及它的底层处理模型。数据流水线是构建数据处理的基础，掌握了它，我们就可以根据自身的应用需求，构建出一套数据流水线来处理数据。</p><h2>思考题</h2><p>你能根据自己的理解重述一下在Beam的数据流水线中，当处理的元素发生错误时流水线的错误处理机制吗？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":105685,"user_name":"espzest","can_delete":false,"product_type":"c1","uid":1094470,"ip_address":"","ucode":"64B1A7DEA19348","user_header":"https://static001.geekbang.org/account/avatar/00/10/b3/46/ec914238.jpg","comment_is_top":false,"comment_ctime":1561075506,"is_pvip":false,"replies":[{"id":"38544","content":"谢谢你的提问！其实我们在第24讲中知道，PCollection是具有无序性的，所以最简单的做法bundle在处理完成之后可以直接append到结果PCollection中。<br><br>至于为什么需要重做前面的bundle，这其实也是错误处理机制的一个trade-off了。Beam希望尽可能减少persistence cost，也就是不希望将中间结果保持在某一个worker上。可以这么想，如果我们想要不重新处理前面的bundle，我们必须要将很多中间结果转换成硬盘数据，这样一方面增加很大的时间开销，另一方面因为数据持久化了在具体一台机器上，我们也没有办法再重新动态分配bundle到不同的机器上去了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561358471,"ip_address":"","comment_id":105685,"utype":1}],"discussion_count":1,"race_medal":0,"score":"61690617650","product_id":100025301,"comment_content":"bundle怎么聚合成pcollection？  一个bundle处理失败，为什么需要重做前面的bundle？","like_count":15,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454776,"discussion_content":"谢谢你的提问！其实我们在第24讲中知道，PCollection是具有无序性的，所以最简单的做法bundle在处理完成之后可以直接append到结果PCollection中。\n\n至于为什么需要重做前面的bundle，这其实也是错误处理机制的一个trade-off了。Beam希望尽可能减少persistence cost，也就是不希望将中间结果保持在某一个worker上。可以这么想，如果我们想要不重新处理前面的bundle，我们必须要将很多中间结果转换成硬盘数据，这样一方面增加很大的时间开销，另一方面因为数据持久化了在具体一台机器上，我们也没有办法再重新动态分配bundle到不同的机器上去了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561358471,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105655,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1561069077,"is_pvip":false,"replies":[{"id":"38535","content":"谢谢你的提问！其实文章中所讲到的随机分配并不是说像分配随机数那样将bundle随机分配出去给workers，只是说根据runner的不同，bundle的分配方式也会不一样了，但最终还是还是希望能最大化并行度。<br><br>至于完美并行的背后机制，Beam会在真正处理数据前先计算优化出执行的一个有向无环图，希望保持并行处理数据的同时，能够减少每个worker之间的联系。<br><br>Beam据我所知是有的，BEAM-7131 issue就有反应这个问题。<br><br>","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561357305,"ip_address":"","comment_id":105655,"utype":1}],"discussion_count":2,"race_medal":0,"score":"35920807445","product_id":100025301,"comment_content":"bundle随机分配会不会产生数据倾斜？完美并行背后的机制是？beam应该也有类似spark的persist方法缓存转换中间结果，防止出错恢复链太长吧？","like_count":8,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454757,"discussion_content":"谢谢你的提问！其实文章中所讲到的随机分配并不是说像分配随机数那样将bundle随机分配出去给workers，只是说根据runner的不同，bundle的分配方式也会不一样了，但最终还是还是希望能最大化并行度。\n\n至于完美并行的背后机制，Beam会在真正处理数据前先计算优化出执行的一个有向无环图，希望保持并行处理数据的同时，能够减少每个worker之间的联系。\n\nBeam据我所知是有的，BEAM-7131 issue就有反应这个问题。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357305,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1349519,"avatar":"https://static001.geekbang.org/account/avatar/00/14/97/8f/c091d4f3.jpg","nickname":"john","note":"","ucode":"CE4991BEF991BA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":344317,"discussion_content":"似乎可以借鉴Max-flow算法，每个节点的处理能力（cpu、内存、网络带宽）是已知参数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611405700,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105762,"user_name":"YZJ","can_delete":false,"product_type":"c1","uid":1260810,"ip_address":"","ucode":"88B2A495A33197","user_header":"https://static001.geekbang.org/account/avatar/00/13/3d/0a/5f2a9a4c.jpg","comment_is_top":false,"comment_ctime":1561081682,"is_pvip":false,"replies":[{"id":"38625","content":"谢谢你的提问！这个问题很好啊，运行大型的Beam pipeline遇到OOM也是有可能的。要配置底层资源的话要看Runner支不支持，像如果将Google Cloud Dataflow作为Runner的话，我们可以通过配置PipelineOption来达到目的。底层使用Spark的话我个人还没有使用过，不过应该是可以用SparkContextOptions来配置的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561400667,"ip_address":"","comment_id":105762,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31625852754","product_id":100025301,"comment_content":"老师请教个问题：PCollectionA transform PCollectionB, 假如PCollectionB 要比PCollectionA大很多倍，比如transform  是把PCollectionA 中每个字符串重复1000次，那PCollectionB 就要大1000倍，worker会不会有内存溢出问题? spark中可以配置executor 的core和memery来控制每个task内存用量，beam有类似机制吗?不然怎样让资源利用最优化呢?","like_count":7,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454801,"discussion_content":"谢谢你的提问！这个问题很好啊，运行大型的Beam pipeline遇到OOM也是有可能的。要配置底层资源的话要看Runner支不支持，像如果将Google Cloud Dataflow作为Runner的话，我们可以通过配置PipelineOption来达到目的。底层使用Spark的话我个人还没有使用过，不过应该是可以用SparkContextOptions来配置的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561400667,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":107355,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1561518291,"is_pvip":false,"replies":[{"id":"39220","content":"的确如此","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1561703872,"ip_address":"","comment_id":107355,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23036354771","product_id":100025301,"comment_content":"Beam的错误处理和RDD的真的很像，因为transformation都是lazy的，只有action才会触发计算，中间的转换过程都是被记录在DAG中的，这就导致中间某个transformation失败之后，需要往上追溯之前的转换，可以理解为是寻找父transformation，然后父transformation还要往上寻找父父transformation，直到没有父transformation为止，就像是类加载机制一样。但是如果能把中间结果保存在内存中，在失败重新计算时，就能提高计算的效率。","like_count":5,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":455483,"discussion_content":"的确如此","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561703872,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":106430,"user_name":"沈洪彬","can_delete":false,"product_type":"c1","uid":1458200,"ip_address":"","ucode":"F9911236D0BA1C","user_header":"https://static001.geekbang.org/account/avatar/00/16/40/18/cc3804e2.jpg","comment_is_top":false,"comment_ctime":1561299183,"is_pvip":false,"replies":[{"id":"38459","content":"谢谢留言！总结得不错！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561330178,"ip_address":"","comment_id":106430,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18741168367","product_id":100025301,"comment_content":"在 Beam 的数据流水线中，当处理的元素发生错误时流水线的的错误处理机制分两种情况<br><br>1.单个Transform上的错误处理<br>如果某个Bundle里元素处理失败，则整个Bundle里元素都必须重新处理<br><br>2.多步骤Transform上的错误处理<br>如果某个Bundle里元素处理失败，则整个Bundle里元素及与之关联的所有Bundle都必须重新处理","like_count":4,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":455084,"discussion_content":"谢谢留言！总结得不错！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561330178,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105666,"user_name":"onepieceJT2018","can_delete":false,"product_type":"c1","uid":1112182,"ip_address":"","ucode":"C8C214C3D5D285","user_header":"https://static001.geekbang.org/account/avatar/00/10/f8/76/3db69173.jpg","comment_is_top":false,"comment_ctime":1561072337,"is_pvip":true,"replies":[{"id":"38545","content":"谢谢你的提问！这个依赖worker1和worker2计算结果的Transform会一直等待。但是与此同时，worker2可以做其它的计算，甚至有可能worker1的计算如果一直出错，Beam会将这个bundle重新分配给worker2来计算。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561360149,"ip_address":"","comment_id":105666,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14445974225","product_id":100025301,"comment_content":"老师 想到一个问题啊 如果有个计算是 需要worker1 和 worker2 都算完的结果再计算 发生worker1 一直错误没通过 这时候worker2会一直傻傻等待嘛","like_count":3,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454762,"discussion_content":"谢谢你的提问！这个依赖worker1和worker2计算结果的Transform会一直等待。但是与此同时，worker2可以做其它的计算，甚至有可能worker1的计算如果一直出错，Beam会将这个bundle重新分配给worker2来计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561360149,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105724,"user_name":"常超","can_delete":false,"product_type":"c1","uid":1138665,"ip_address":"","ucode":"4AE7743B4ADF20","user_header":"https://static001.geekbang.org/account/avatar/00/11/5f/e9/95ef44f3.jpg","comment_is_top":false,"comment_ctime":1561078641,"is_pvip":false,"replies":[{"id":"38626","content":"谢谢你的提问！这个问题非常好啊，如果你所说的是stateful processing的话，那它的错误处理机制和stateful-less会不太一样。Stateful processing里的ParDo在处理每一个元素的时候会将它的state持久化，也就是保存到外部的storage中，下次需要用到这个元素的时候会再读取这个元素的state。如果是发生了错误的话，会有机制reclaim这些states或者是invalidate它们。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561401373,"ip_address":"","comment_id":105724,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10151013233","product_id":100025301,"comment_content":"&lt;在多步骤的 Transform 上，如果处理的一个 Bundle 元素发生错误了，则这个元素所在的整个 Bundle 以及与这个 Bundle 有关联的所有 Bundle 都必须重新处理。<br>如果upstream transform里状态有更新操作，重新处理已经成功的bundle会出现数据重复，可能导致状态更新不正确吧？<br>","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454786,"discussion_content":"谢谢你的提问！这个问题非常好啊，如果你所说的是stateful processing的话，那它的错误处理机制和stateful-less会不太一样。Stateful processing里的ParDo在处理每一个元素的时候会将它的state持久化，也就是保存到外部的storage中，下次需要用到这个元素的时候会再读取这个元素的state。如果是发生了错误的话，会有机制reclaim这些states或者是invalidate它们。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561401373,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":106016,"user_name":"Alpha","can_delete":false,"product_type":"c1","uid":1001861,"ip_address":"","ucode":"60CA15A25EC796","user_header":"https://static001.geekbang.org/account/avatar/00/0f/49/85/3f161d95.jpg","comment_is_top":false,"comment_ctime":1561127501,"is_pvip":false,"replies":[{"id":"38461","content":"谢谢留言！哈哈，需要表达的内容不一样了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561330292,"ip_address":"","comment_id":106016,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5856094797","product_id":100025301,"comment_content":"上一期讲到，PCollection 是有向图中的边，而 Transform 是有向图里的节点。这一期的图咋又变了呢","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454916,"discussion_content":"谢谢留言！哈哈，需要表达的内容不一样了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561330292,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":337902,"user_name":"哇哈哈","can_delete":false,"product_type":"c1","uid":1175537,"ip_address":"","ucode":"47453D1C96A1DD","user_header":"https://static001.geekbang.org/account/avatar/00/11/ef/f1/8b06801a.jpg","comment_is_top":false,"comment_ctime":1647152875,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1647152875","product_id":100025301,"comment_content":"流式数据也会分bundle吗？那不是变成spark streaming的微型批处理形式了？","like_count":0},{"had_liked":false,"id":330435,"user_name":"weiming","can_delete":false,"product_type":"c1","uid":1730052,"ip_address":"","ucode":"3EC2968FB08501","user_header":"https://static001.geekbang.org/account/avatar/00/1a/66/04/22190a73.jpg","comment_is_top":false,"comment_ctime":1641977205,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1641977205","product_id":100025301,"comment_content":"1. PCollection会被划分成多个Bundle（分配多少个是随机的），Bundle会被分配到Worker中处理（分配也是随机的），最终机制保障最大程度的完美并行。<br>2. 错误处理中有关联Bundle的概念（因为是同一个Worker处理），如果关联Bundle中的一个Bundle失败了，所有关联的Bundle全部重做，主要是考虑到数据持久化的成本。（通过重做消除持久化）<br>3. 注意有PCollection不可变性，引申到Bundle中的不可变性。 ","like_count":0},{"had_liked":false,"id":273384,"user_name":"旭东(Frank)","can_delete":false,"product_type":"c1","uid":1024486,"ip_address":"","ucode":"176FA629800062","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a1/e6/50da1b2d.jpg","comment_is_top":false,"comment_ctime":1610579414,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1610579414","product_id":100025301,"comment_content":"分而治之","like_count":0},{"had_liked":false,"id":205456,"user_name":"冯杰","can_delete":false,"product_type":"c1","uid":1950765,"ip_address":"","ucode":"61C92D62D49A66","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/qmdZbyxrRD5qQLKjWkmdp3PCVhwmWTcp0cs04s39pic2RcNw0nNKTDgKqedSQ54bAGWjAVSc9p4vWP8RJRKB6nA/132","comment_is_top":false,"comment_ctime":1586620445,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1586620445","product_id":100025301,"comment_content":"从您的描述中可以看出，数据的实际计算和容错都是以分区来进行的，原因在于ParDo模式下同一个Pc下不同的数据记录之间不存在依赖关系即可以完成计算。     在实际计算时，我们处理玩Trasform1得到Pc1，然后在接着计算transform2，那为什么不能以单条数据来并行呢？   即分区内的每一条数据独立完成所有的计算链，而不是要等同一个Pc下的数据都就绪后在执行下一个计算。<br>关于容错不以单条数据来设计，我倒是能理解，因为要这样做的话，我们必然需要为每条数据都记录他的计算关系，追溯它具体是从上游的哪一条数据来的，这会增加存储的压力。  而以分区来实现容错，我们只需要记录血缘即可，血缘关系太长，可以像Spark那样做一些持久化的操作。","like_count":0},{"had_liked":false,"id":117559,"user_name":"jimyth","can_delete":false,"product_type":"c1","uid":1053939,"ip_address":"","ucode":"BA30C4B9EDE10B","user_header":"https://wx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq4UrqDxicaxiatJOL4CDNZmlsMsn9FPChJUIxaQjTSP552UFZmY4tdlI1Ju0ZiaIFk1yk2A1DJ1qD8w/132","comment_is_top":false,"comment_ctime":1564078363,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564078363","product_id":100025301,"comment_content":"老师你好，既然PCollection 是无序的，请问一下怎么处理数据流中的先后依赖的问题，本节例子的 bound中的数据都是有序的分配的，实际计算过程中是不是会出现 1,3,5出现在一个 bound ;2,4,6 出现在一个 bound<br>您在 23 讲的例子中，ParDo 是针对单个元素的处理，怎么实现计算2 个元素的累加的呢？<br>例如下面是一组速度数据<br>时间                     速度<br>2019-07-26 00:00:00   10<br>2019-07-26 00:00:01   15<br>2019-07-26 00:00:02   20<br>2019-07-26 00:00:03   40<br>2019-07-26 00:00:04   70<br>我需要大概怎么计算加速度，","like_count":0},{"had_liked":false,"id":110295,"user_name":"chief","can_delete":false,"product_type":"c1","uid":1051099,"ip_address":"","ucode":"918B8F4CE9E49F","user_header":"https://static001.geekbang.org/account/avatar/00/10/09/db/19a7c51c.jpg","comment_is_top":false,"comment_ctime":1562223980,"is_pvip":false,"replies":[{"id":"40129","content":"谢谢你的留言！前后bundle保不保留这个还要看你的执行DAG需不需要还用到这个bundle。至于保留前后关系的话主要是用于在发生错误的情况下重新trace back到最开始的那个transform，这个信息从DAG中就可以找到了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1562266580,"ip_address":"","comment_id":110295,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1562223980","product_id":100025301,"comment_content":"老师您好，bundle经过Transform会产生新的bundle，那么是同时保留前后bundle数据还是在新生成的bundle中保留血缘关系？<br><br>","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":456805,"discussion_content":"谢谢你的留言！前后bundle保不保留这个还要看你的执行DAG需不需要还用到这个bundle。至于保留前后关系的话主要是用于在发生错误的情况下重新trace back到最开始的那个transform，这个信息从DAG中就可以找到了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562266580,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":106426,"user_name":"fy","can_delete":false,"product_type":"c1","uid":1152186,"ip_address":"","ucode":"EDB661C3A05910","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/jsMMDDzhbsTzhicsGZiaeV0PWSnAS0fBlb1r6CsuB32vr3hRwV9UubmfHQx45v7jtaXajPlQ8kQ17b3zpQzHmqVw/132","comment_is_top":false,"comment_ctime":1561297240,"is_pvip":false,"replies":[{"id":"38458","content":"谢谢你的留言！确实是可以直接学习，不过我也建议如果平时有时间的话，可以看看这些计算机的基础。像操作系统的话里面的一些调度算法或许可以给你平时的实际应用有一些启发。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561330150,"ip_address":"","comment_id":106426,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1561297240","product_id":100025301,"comment_content":"老师，有编程语言基础。我也去Beam看了看教程，请问这个可以直接学吧。还需要其他基础么，比如操作系统，计算机组成原理等","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":455082,"discussion_content":"谢谢你的留言！确实是可以直接学习，不过我也建议如果平时有时间的话，可以看看这些计算机的基础。像操作系统的话里面的一些调度算法或许可以给你平时的实际应用有一些启发。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561330150,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":106011,"user_name":"TJ","can_delete":false,"product_type":"c1","uid":1394641,"ip_address":"","ucode":"3EEAFFB8D3B7AD","user_header":"https://static001.geekbang.org/account/avatar/00/15/47/d1/15f1a8ce.jpg","comment_is_top":false,"comment_ctime":1561126280,"is_pvip":false,"replies":[{"id":"38433","content":"谢谢你的留言！如果我没有理解错你的问题的话，我想你说的“边界”指的就是在第23讲中讲到的Beam的统一模型层了。<br><br>其实Beam提供的是在Dataflow Model论文里面的一种批流统一处理的思想，数据处理引擎如果能够按照这个思想提供出相应的APIs的话那这个数据处理引擎就可以成为Beam的底层Runner。<br><br>最后一问的话是的，PCollection抽象就是Spark中的RDD。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561271919,"ip_address":"","comment_id":106011,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1561126280","product_id":100025301,"comment_content":"能否说一下Beam和底层执行系统的边界在哪里？那些功能由Beam提供,那些由底层如Spark提供?<br>如果底层是spark，是否PCollection就是RDD?","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454913,"discussion_content":"谢谢你的留言！如果我没有理解错你的问题的话，我想你说的“边界”指的就是在第23讲中讲到的Beam的统一模型层了。\n\n其实Beam提供的是在Dataflow Model论文里面的一种批流统一处理的思想，数据处理引擎如果能够按照这个思想提供出相应的APIs的话那这个数据处理引擎就可以成为Beam的底层Runner。\n\n最后一问的话是的，PCollection抽象就是Spark中的RDD。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561271919,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105910,"user_name":"JohnT3e","can_delete":false,"product_type":"c1","uid":1063982,"ip_address":"","ucode":"CF4AAAC933529C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132","comment_is_top":false,"comment_ctime":1561102998,"is_pvip":false,"replies":[{"id":"38468","content":"谢谢你的留言！你这样的理解其实也没有错，如果Beam优化器能优化合并掉一些步骤的话，那确实实际产生出来的bundle会比理论上可以产生出来的bundle要少。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561336969,"ip_address":"","comment_id":105910,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1561102998","product_id":100025301,"comment_content":"由于beam优化器，是不是实际产生的bundle要少于逻辑上的个数？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454863,"discussion_content":"谢谢你的留言！你这样的理解其实也没有错，如果Beam优化器能优化合并掉一些步骤的话，那确实实际产生出来的bundle会比理论上可以产生出来的bundle要少。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561336969,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105829,"user_name":"dancer","can_delete":false,"product_type":"c1","uid":1019036,"ip_address":"","ucode":"B8D5641A3AC490","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/9c/d48473ab.jpg","comment_is_top":false,"comment_ctime":1561090355,"is_pvip":false,"replies":[{"id":"38540","content":"谢谢你的提问！这个问题还是要看情况吧，如果多步骤的Transform都是ParDo的话，那确实可以按照你说的做法去做。不过当Transform涉及到Combine或者Flatten这种Transform的话， 那就必须等到这一阶段所有的Transform完成了之后才能够进行下一步的Transform了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561357749,"ip_address":"","comment_id":105829,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1561090355","product_id":100025301,"comment_content":"想问老师，一个bundle的数据必须要全部处理完之后才能进行第二个transform吗？如果部分数据经过transform1后就可以继续执行transform2，这样数据并行度会更高吧，为什么没有采用这种机制呢？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454829,"discussion_content":"谢谢你的提问！这个问题还是要看情况吧，如果多步骤的Transform都是ParDo的话，那确实可以按照你说的做法去做。不过当Transform涉及到Combine或者Flatten这种Transform的话， 那就必须等到这一阶段所有的Transform完成了之后才能够进行下一步的Transform了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357749,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]}]}