{"id":94976,"title":"14 | 弹性分布式数据集：Spark大厦的地基（下）","content":"<p>你好，我是蔡元楠。</p><p>上一讲我们介绍了弹性分布式数据集（RDD）的定义、特性以及结构，并且深入讨论了依赖关系（Dependencies）。</p><p>今天让我们一起来继续学习RDD的其他特性。</p><h2>RDD的结构</h2><p>首先，我来介绍一下RDD结构中其他的几个知识点：检查点（Checkpoint）、存储级别（ Storage Level）和迭代函数（Iterator）。</p><p><img src=\"https://static001.geekbang.org/resource/image/8c/1c/8cae25f4d16a34be77fd3e84133d6a1c.png?wh=1776*2263\" alt=\"\"></p><p>通过上一讲，你应该已经知道了，基于RDD的依赖关系，如果任意一个RDD在相应的节点丢失，你只需要从上一步的RDD出发再次计算，便可恢复该RDD。</p><p>但是，如果一个RDD的依赖链比较长，而且中间又有多个RDD出现故障的话，进行恢复可能会非常耗费时间和计算资源。</p><p>而检查点（Checkpoint）的引入，就是为了优化这些情况下的数据恢复。</p><p>很多数据库系统都有检查点机制，在连续的transaction列表中记录某几个transaction后数据的内容，从而加快错误恢复。</p><p>RDD中的检查点的思想与之类似。</p><p>在计算过程中，对于一些计算过程比较耗时的RDD，我们可以将它缓存至硬盘或HDFS中，标记这个RDD有被检查点处理过，并且清空它的所有依赖关系。同时，给它新建一个依赖于CheckpointRDD的依赖关系，CheckpointRDD可以用来从硬盘中读取RDD和生成新的分区信息。</p><!-- [[[read_end]]] --><p>这样，当某个子RDD需要错误恢复时，回溯至该RDD，发现它被检查点记录过，就可以直接去硬盘中读取这个RDD，而无需再向前回溯计算。</p><p>存储级别（Storage Level）是一个枚举类型，用来记录RDD持久化时的存储级别，常用的有以下几个：</p><ul>\n<li>\n<p>MEMORY_ONLY：只缓存在内存中，如果内存空间不够则不缓存多出来的部分。这是RDD存储级别的默认值。</p>\n</li>\n<li>\n<p>MEMORY_AND_DISK：缓存在内存中，如果空间不够则缓存在硬盘中。</p>\n</li>\n<li>\n<p>DISK_ONLY：只缓存在硬盘中。</p>\n</li>\n<li>\n<p>MEMORY_ONLY_2和MEMORY_AND_DISK_2等：与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。</p>\n</li>\n</ul><p>这就是我们在前文提到过的，Spark相比于Hadoop在性能上的提升。我们可以随时把计算好的RDD缓存在内存中，以便下次计算时使用，这大幅度减小了硬盘读写的开销。</p><p>迭代函数（Iterator）和计算函数（Compute）是用来表示RDD怎样通过父RDD计算得到的。</p><p>迭代函数会首先判断缓存中是否有想要计算的RDD，如果有就直接读取，如果没有，就查找想要计算的RDD是否被检查点处理过。如果有，就直接读取，如果没有，就调用计算函数向上递归，查找父RDD进行计算。</p><p>到现在，相信你已经对弹性分布式数据集的基本结构有了初步了解。但是光理解RDD的结构是远远不够的，我们的终极目标是使用RDD进行数据处理。</p><p>要使用RDD进行数据处理，你需要先了解一些RDD的数据操作。</p><p>在<a href=\"http://time.geekbang.org/column/article/94410\">第12讲</a>中，我曾经提过，相比起MapReduce只支持两种数据操作，Spark支持大量的基本操作，从而减轻了程序员的负担。</p><p>接下来，让我们进一步了解基于RDD的各种数据操作。</p><h2>RDD的转换操作</h2><p>RDD的数据操作分为两种：转换（Transformation）和动作（Action）。</p><p>顾名思义，转换是用来把一个RDD转换成另一个RDD，而动作则是通过计算返回一个结果。</p><p>不难想到，之前举例的map、filter、groupByKey等都属于转换操作。</p><h3>Map</h3><p>map是最基本的转换操作。</p><p>与MapReduce中的map一样，它把一个RDD中的所有数据通过一个函数，映射成一个新的RDD，任何原RDD中的元素在新RDD中都有且只有一个元素与之对应。</p><p>在这一讲中提到的所有的操作，我都会使用代码举例，帮助你更好地理解。</p><pre><code>rdd = sc.parallelize([&quot;b&quot;, &quot;a&quot;, &quot;c&quot;])\nrdd2 = rdd.map(lambda x: (x, 1)) // [('b', 1), ('a', 1), ('c', 1)]\n</code></pre><h3>Filter</h3><p>filter这个操作，是选择原RDD里所有数据中满足某个特定条件的数据，去返回一个新的RDD。如下例所示，通过filter，只选出了所有的偶数。</p><pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5])\nrdd2 = rdd.filter(lambda x: x % 2 == 0) // [2, 4]\n</code></pre><h3>mapPartitions</h3><p>mapPartitions是map的变种。不同于map的输入函数是应用于RDD中每个元素，mapPartitions的输入函数是应用于RDD的每个分区，也就是把每个分区中的内容作为整体来处理的，所以输入函数的类型是Iterator[T] =&gt; Iterator[U]。</p><pre><code>rdd = sc.parallelize([1, 2, 3, 4], 2)\ndef f(iterator): yield sum(iterator)\nrdd2 = rdd.mapPartitions(f) // [3, 7]\n</code></pre><p>在mapPartitions的例子中，我们首先创建了一个有两个分区的RDD。mapPartitions的输入函数是对每个分区内的元素求和，所以返回的RDD包含两个元素：1+2=3 和3+4=7。</p><h3>groupByKey</h3><p>groupByKey和SQL中的groupBy类似，是把对象的集合按某个Key来归类，返回的RDD中每个Key对应一个序列。</p><pre><code>rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 2)])\nrdd.groupByKey().collect()\n//&quot;a&quot; [1, 2]\n//&quot;b&quot; [1]\n</code></pre><p>在此，我们只列举这几个常用的、有代表性的操作，对其他转换操作感兴趣的同学可以去自行查阅官方的API文档。</p><h2>RDD的动作操作</h2><p>让我们再来看几个常用的动作操作。</p><h3>Collect</h3><p>RDD中的动作操作collect与函数式编程中的collect类似，它会以数组的形式，返回RDD的所有元素。需要注意的是，collect操作只有在输出数组所含的数据数量较小时使用，因为所有的数据都会载入到程序的内存中，如果数据量较大，会占用大量JVM内存，导致内存溢出。</p><pre><code>rdd = sc.parallelize([&quot;b&quot;, &quot;a&quot;, &quot;c&quot;])\nrdd.map(lambda x: (x, 1)).collect() // [('b', 1), ('a', 1), ('c', 1)]\n</code></pre><p>实际上，上述转换操作中所有的例子，最后都需要将RDD的元素collect成数组才能得到标记好的输出。</p><h3>Reduce</h3><p>与MapReduce中的reduce类似，它会把RDD中的元素根据一个输入函数聚合起来。</p><pre><code>from operator import add\nsc.parallelize([1, 2, 3, 4, 5]).reduce(add)  // 15\n</code></pre><h3>Count</h3><p>Count会返回RDD中元素的个数。</p><p>sc.parallelize([2, 3, 4]).count() // 3</p><h3>CountByKey</h3><p>仅适用于Key-Value pair类型的 RDD，返回具有每个 key 的计数的&lt;Key, Count&gt;的map。</p><pre><code>rdd = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 1)])\nsorted(rdd.countByKey().items()) // [('a', 2), ('b', 1)]\n</code></pre><p>讲到这，你可能会问了，为什么要区分转换和动作呢？虽然转换是生成新的RDD，动作是把RDD进行计算生成一个结果，它们本质上不都是计算吗？</p><p>这是因为，所有转换操作都很懒，它只是生成新的RDD，并且记录依赖关系。</p><p>但是Spark并不会立刻计算出新RDD中各个分区的数值。直到遇到一个动作时，数据才会被计算，并且输出结果给Driver。</p><p>比如，在之前的例子中，你先对RDD进行map转换，再进行collect动作，这时map后生成的RDD不会立即被计算。只有当执行到collect操作时，map才会被计算。而且，map之后得到的较大的数据量并不会传给Driver，只有collect动作的结果才会传递给Driver。</p><p>这种惰性求值的设计优势是什么呢？让我们来看这样一个例子。</p><p>假设，你要从一个很大的文本文件中筛选出包含某个词语的行，然后返回第一个这样的文本行。你需要先读取文件textFile()生成rdd1，然后使用filter()方法生成rdd2，最后是行动操作first()，返回第一个元素。</p><p>读取文件的时候会把所有的行都存储起来，但我们马上就要筛选出只具有特定词组的行了，等筛选出来之后又要求只输出第一个。这样是不是太浪费存储空间了呢？确实。</p><p>所以实际上，Spark是在行动操作first()的时候开始真正的运算：只扫描第一个匹配的行，不需要读取整个文件。所以，惰性求值的设计可以让Spark的运算更加高效和快速。</p><p>让我们总结一下Spark执行操作的流程吧。</p><p>Spark在每次转换操作的时候，使用了新产生的 RDD 来记录计算逻辑，这样就把作用在 RDD 上的所有计算逻辑串起来，形成了一个链条。当对 RDD 进行动作时，Spark 会从计算链的最后一个RDD开始，依次从上一个RDD获取数据并执行计算逻辑，最后输出结果。</p><h2>RDD的持久化（缓存）</h2><p>每当我们对RDD调用一个新的action操作时，整个RDD都会从头开始运算。因此，如果某个RDD会被反复重用的话，每次都从头计算非常低效，我们应该对多次使用的RDD进行一个持久化操作。</p><p>Spark的persist()和cache()方法支持将RDD的数据缓存至内存或硬盘中，这样当下次对同一RDD进行Action操作时，可以直接读取RDD的结果，大幅提高了Spark的计算效率。</p><pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5])\nrdd1 = rdd.map(lambda x: x+5)\nrdd2 = rdd1.filter(lambda x: x % 2 == 0)\nrdd2.persist()\ncount = rdd2.count() // 3\nfirst = rdd2.first() // 6\nrdd2.unpersist()\n</code></pre><p>在文中的代码例子中你可以看到，我们对RDD2进行了多个不同的action操作。由于在第四行我把RDD2的结果缓存在内存中，所以Spark无需从一开始的rdd开始算起了（持久化处理过的RDD只有第一次有action操作时才会从源头计算，之后就把结果存储下来，所以在这个例子中，count需要从源头开始计算，而first不需要）。</p><p>在缓存RDD的时候，它所有的依赖关系也会被一并存下来。所以持久化的RDD有自动的容错机制。如果RDD的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算。</p><p>持久化可以选择不同的存储级别。正如我们讲RDD的结构时提到的一样，有MEMORY_ONLY，MEMORY_AND_DISK，DISK_ONLY等。cache()方法会默认取MEMORY_ONLY这一级别。</p><h2>小结</h2><p>Spark在每次转换操作的时候使用了新产生的 RDD 来记录计算逻辑，这样就把作用在 RDD 上的所有计算逻辑串起来形成了一个链条，但是并不会真的去计算结果。当对 RDD 进行动作Action时，Spark 会从计算链的最后一个RDD开始，利用迭代函数（Iterator）和计算函数（Compute），依次从上一个RDD获取数据并执行计算逻辑，最后输出结果。</p><p>此外，我们可以通过将一些需要复杂计算和经常调用的RDD进行持久化处理，从而提升计算效率。</p><h2>思考题</h2><p>对RDD进行持久化操作和记录Checkpoint，有什么区别呢？</p><p>欢迎你把对弹性分布式数据集的疑问写在留言区，与我和其他同学一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"13 | 弹性分布式数据集：Spark大厦的地基（上）","id":94974},"right":{"article_title":"15 | Spark SQL：Spark数据查询的利器","id":96256}},"comments":[{"had_liked":false,"id":95438,"user_name":"锦","can_delete":false,"product_type":"c1","uid":1468298,"ip_address":"","ucode":"CB0EB4B68C468B","user_header":"https://static001.geekbang.org/account/avatar/00/16/67/8a/babd74dc.jpg","comment_is_top":false,"comment_ctime":1558056466,"is_pvip":false,"replies":[{"id":"48533","content":"👍🏻","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392382,"ip_address":"","comment_id":95438,"utype":1}],"discussion_count":4,"race_medal":0,"score":"263551061522","product_id":100025301,"comment_content":"区别在于Checkpoint会清空该RDD的依赖关系，并新建一个CheckpointRDD依赖关系，让该RDD依赖，并保存在磁盘或HDFS文件系统中，当数据恢复时，可通过CheckpointRDD读取RDD进行数据计算；持久化RDD会保存依赖关系和计算结果至内存中，可用于后续计算。","like_count":62,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450458,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392382,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":10952,"discussion_content":"课代表请坐。\n后优化与前优化的使用是spark设计的巧妙之处，惰性计算机制使得计算逻辑可以从后向前，由漏斗变线体，节省了运行开销；持久化persist与检查点checkpoint机制相当于我们已经确定这步要做，且做好了，以后都可以基于现成的，这也提高了运行效率。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1568342295,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1009644,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erms9qcIFYZ4npgLYPu1QgxQyaXcj64ZBicNVeBRWcYUpCZ9p0BGsrEcX8heibMLCV4Gde4P9pf7PjA/132","nickname":"yanger2004","note":"","ucode":"0A2CD03EF31052","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":574925,"discussion_content":"持久化也可以是硬盘的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654441144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1581245,"avatar":"https://static001.geekbang.org/account/avatar/00/18/20/bd/5656b5d7.jpg","nickname":"走刀口 💰","note":"","ucode":"C7E2B812A4A02C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362195,"discussion_content":"作者你这也发个👍🏻？他说的根本不对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616885247,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95413,"user_name":"RocWay","can_delete":false,"product_type":"c1","uid":1088024,"ip_address":"","ucode":"377CD114BABBF7","user_header":"https://static001.geekbang.org/account/avatar/00/10/9a/18/3596069c.jpg","comment_is_top":false,"comment_ctime":1558054452,"is_pvip":false,"replies":[{"id":"48537","content":"这位同学的理解是很准确的","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392564,"ip_address":"","comment_id":95413,"utype":1}],"discussion_count":1,"race_medal":0,"score":"113227204148","product_id":100025301,"comment_content":"主要区别应该是对依赖链的处理：<br>checkpoint在action之后执行，相当于事务完成后备份结果。既然结果有了，之前的计算过程，也就是RDD的依赖链，也就不需要了，所以不必保存。<br>但是cache和persist只是保存当前RDD，并不要求是在action之后调用。相当于事务的计算过程，还没有结果。既然没有结果，当需要恢复、重新计算时就要重放计算过程，自然之前的依赖链不能放弃，也需要保存下来。需要恢复时就要从最初的或最近的checkpoint开始重新计算。","like_count":27,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450448,"discussion_content":"这位同学的理解是很准确的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392564,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95367,"user_name":"涵","can_delete":false,"product_type":"c1","uid":1502742,"ip_address":"","ucode":"BB8575DB13F1E0","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg","comment_is_top":false,"comment_ctime":1558044070,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"61687586214","product_id":100025301,"comment_content":"从目的上来说，checkpoint用于数据恢复，RDD持久化用于RDD的多次计算操作的性能优化，避免重复计算。从存储位置上看checkpoint储存在外存中，RDD可以根据存储级别存储在内存或&#47;和外存中。","like_count":14},{"had_liked":false,"id":102539,"user_name":"珅剑","can_delete":false,"product_type":"c1","uid":1504220,"ip_address":"","ucode":"4290D5C140F80F","user_header":"https://static001.geekbang.org/account/avatar/00/16/f3/dc/80b0cd23.jpg","comment_is_top":false,"comment_ctime":1560246932,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31625018004","product_id":100025301,"comment_content":"1.设置checkpoint时需要指定checkpoint的存储目录，而持久化不管是直接调用cache还是通过persist指定缓存级别都不需要指定存储目录，由系统自己指定<br>2.checkpoint是将RDD去除依赖关系后将数据直接存储到磁盘，且一般是HDFS，带有备份，因此不容易丢失，恢复时直接获取checkpoint的数据；而持久化一般是直接cache到内存。数据容易丢失，即便是通过设置MEMORY_AND_DISK_2等缓存级别达到内存和磁盘都有备份，也会在每个备份中都缓存RDD的依赖关系，造成不必要的冗余","like_count":7},{"had_liked":false,"id":95503,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1558067518,"is_pvip":false,"replies":[{"id":"48546","content":"如果限定为单机处理，我觉得你的第二个思路是可行的，第一个行不通。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567393450,"ip_address":"","comment_id":95503,"utype":1}],"discussion_count":2,"race_medal":0,"score":"31622838590","product_id":100025301,"comment_content":"老师，我想问下，如果是linux 命令分析单机300G log日志，内存只有16G，怎搞？<br>如果用spark思想，，从io读很卡，直接内存爆了。<br>如果先分割日志为100份，再用shell，一下10个并发执行，最后结果合并。感觉还是有点慢。","like_count":7,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450485,"discussion_content":"如果限定为单机处理，我觉得你的第二个思路是可行的，第一个行不通。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567393450,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382810,"discussion_content":"怎么并发，并发也要30g","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625729796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95428,"user_name":"JohnT3e","can_delete":false,"product_type":"c1","uid":1063982,"ip_address":"","ucode":"CF4AAAC933529C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132","comment_is_top":false,"comment_ctime":1558055678,"is_pvip":false,"replies":[{"id":"48534","content":"这位同学的理解很准确","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392448,"ip_address":"","comment_id":95428,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23032892158","product_id":100025301,"comment_content":"两者区别在于依赖关系是否保留吧。checkpoint的话，检查点之前的关系应该丢失了，但其数据已经持久化了；而persist或者cache保留了这个依赖关系，如果缓存结果有丢失，可以通过这个关系进行rebuild。","like_count":5,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450453,"discussion_content":"这位同学的理解很准确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382835,"discussion_content":"依赖关系保存在哪里","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625736220,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95632,"user_name":"挖矿的小戈","can_delete":false,"product_type":"c1","uid":1503917,"ip_address":"","ucode":"2078A85139BD5D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/9chAb6SjxFiapSeicsAsGqzziaNlhX9d5aEt8Z0gUNsZJ9dICaDHqAypGvjv4Bx3PryHnj7OFnOXFOp7Ik21CVXEA/132","comment_is_top":false,"comment_ctime":1558106075,"is_pvip":false,"replies":[{"id":"48532","content":"理解的很对","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392341,"ip_address":"","comment_id":95632,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18737975259","product_id":100025301,"comment_content":"1. 前者：persist或者cache除了除了持久化该RDD外，还会保留该RDD前面的依赖关系<br>2. 后者：将该RDD保存到磁盘上，并清除前面的依赖关系<br>感觉后者的开销会大很多","like_count":4,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450539,"discussion_content":"理解的很对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392341,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95426,"user_name":"miwucc","can_delete":false,"product_type":"c1","uid":1326429,"ip_address":"","ucode":"7935BD907119AE","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/5d/ac666969.jpg","comment_is_top":false,"comment_ctime":1558055639,"is_pvip":false,"replies":[{"id":"48535","content":"并不是，checkpoint会将一些RDD的结果存入硬盘，但是不会保留依赖关系；缓存函数或者持久化处理会保留依赖关系，所以错误恢复会更方便。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392543,"ip_address":"","comment_id":95426,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14442957527","product_id":100025301,"comment_content":"手动调用缓存函数和checkpoint本质上是一样的吧。就是一个手动控制落盘时间，一个自动控制。","like_count":3,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450451,"discussion_content":"并不是，checkpoint会将一些RDD的结果存入硬盘，但是不会保留依赖关系；缓存函数或者持久化处理会保留依赖关系，所以错误恢复会更方便。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392543,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95401,"user_name":"廖师虎","can_delete":false,"product_type":"c1","uid":1485562,"ip_address":"","ucode":"1297068AE141DA","user_header":"https://static001.geekbang.org/account/avatar/00/16/aa/fa/3ad0a689.jpg","comment_is_top":false,"comment_ctime":1558053197,"is_pvip":false,"replies":[{"id":"48540","content":"翻译也是为了方便英文不好的同学理解，但是每个名次第一次出现我都会标出英文。<br>","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392778,"ip_address":"","comment_id":95401,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14442955085","product_id":100025301,"comment_content":"记不太清除了，checkpoint清除血缘关系，一般保存在类hdfs文件系统，目的是容错，缓存是保留血缘关系，并保存在本机，的目的是提高效率，High performance Spark书讲得很详细。<br><br>第一次遇到把driver翻译成驱动程序的，个人感觉还是保留Driver，Action为佳。","like_count":3,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450442,"discussion_content":"翻译也是为了方便英文不好的同学理解，但是每个名次第一次出现我都会标出英文。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392778,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95703,"user_name":"Peter","can_delete":false,"product_type":"c1","uid":1506885,"ip_address":"","ucode":"D2A0BB2F6A7236","user_header":"https://static001.geekbang.org/account/avatar/00/16/fe/45/c353f3da.jpg","comment_is_top":false,"comment_ctime":1558146043,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10148080635","product_id":100025301,"comment_content":"在计算过程中，对于一些计算过程比较耗时的 RDD，我们可以将它缓存至硬盘或 HDFS 中，标记这个 RDD 有被检查点处理过，并且清空它的所有依赖关系。同时，给它新建一个依赖于 CheckpointRDD 的依赖关系，CheckpointRDD 可以用来从硬盘中读取 RDD 和生成新的分区信息。<br>","like_count":2},{"had_liked":false,"id":95410,"user_name":"Steven","can_delete":false,"product_type":"c1","uid":1141382,"ip_address":"","ucode":"BDE4911550BDE6","user_header":"https://static001.geekbang.org/account/avatar/00/11/6a/86/f1876812.jpg","comment_is_top":false,"comment_ctime":1558054130,"is_pvip":false,"replies":[{"id":"48576","content":"你的观察很仔细，这里确实是笔误。持久化处理过的RDD只有第一次有action操作时才会从源头计算，之后就把结果存储下来。所以在这个例子中Count需要从源头开始计算，而first不需要。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567411611,"ip_address":"","comment_id":95410,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10147988722","product_id":100025301,"comment_content":"缓存了之后，第一个action还是需要从头计算的吧？ &quot;所以无论是 count 还是 first，Spark 都无需从头计算&quot;， 这句话是不是有误？","like_count":2,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450446,"discussion_content":"你的观察很仔细，这里确实是笔误。持久化处理过的RDD只有第一次有action操作时才会从源头计算，之后就把结果存储下来。所以在这个例子中Count需要从源头开始计算，而first不需要。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567411611,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1086295,"avatar":"https://static001.geekbang.org/account/avatar/00/10/93/57/3ffdfc8d.jpg","nickname":"vigo","note":"","ucode":"036CEE5F3FABE7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":25968,"discussion_content":"缓存之后，第一个action还从头计算的设计原因为什么呀？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570582006,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1086295,"avatar":"https://static001.geekbang.org/account/avatar/00/10/93/57/3ffdfc8d.jpg","nickname":"vigo","note":"","ucode":"036CEE5F3FABE7","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382840,"discussion_content":"因为这个时候还没有结果啊，所以要算一下，然后把这个结果保存下，后面的action就在这个基础上做","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625736876,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":25968,"ip_address":""},"score":382840,"extra":""}]}]},{"had_liked":false,"id":95403,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1558053256,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10147987848","product_id":100025301,"comment_content":"RDD的checkpoint会导致写入可靠存储的开销。这可能导致RDD被checkpoint的那些批次的处理时间增加。相反，checkpoint太过不频繁会导致血统链增长和任务大小增加。请问该如何设置合理的checkpoint时间间隔呢？","like_count":2},{"had_liked":false,"id":95402,"user_name":"jon","can_delete":false,"product_type":"c1","uid":1253287,"ip_address":"","ucode":"5768A34E292CAA","user_header":"https://static001.geekbang.org/account/avatar/00/13/1f/a7/d379ca4f.jpg","comment_is_top":false,"comment_ctime":1558053250,"is_pvip":false,"replies":[{"id":"48545","content":"👍🏻","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567393360,"ip_address":"","comment_id":95402,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10147987842","product_id":100025301,"comment_content":"checkpoint不会存储该rdd前面的依赖关系，它后面的rdd都依赖于它。<br>persist、 cache操作会存储依赖关系，当一个分区丢失后可以根据依赖重新计算。","like_count":2,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450443,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567393360,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95396,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1558052495,"is_pvip":false,"replies":[{"id":"48544","content":"这些机制对开发者并不是透明的，开发者可以手动调用checkpoint和cache方法来存储RDD。他们的主要区别是是否存储依赖关系。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567393349,"ip_address":"","comment_id":95396,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10147987087","product_id":100025301,"comment_content":"终于明白spark惰性求值的原理了。我理解对 RDD 进行持久化操作和记录 Checkpoint的区别是：前者是开发人员为了避免重复计算、减少长链路计算时间而主动去缓存中间结果，而后者是spark框架为了容错而提供的保存中间结果机制，它对开发人员是透明的，无感知的。","like_count":2,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450441,"discussion_content":"这些机制对开发者并不是透明的，开发者可以手动调用checkpoint和cache方法来存储RDD。他们的主要区别是是否存储依赖关系。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567393349,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":247852,"user_name":"茂杨","can_delete":false,"product_type":"c1","uid":1181344,"ip_address":"","ucode":"8D8259E905DCA3","user_header":"https://static001.geekbang.org/account/avatar/00/12/06/a0/3da0e315.jpg","comment_is_top":false,"comment_ctime":1599893024,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5894860320","product_id":100025301,"comment_content":"Spark的计算是按照Action为单位的，多次的转换在一起只为了组成一个公式，只有真正把数据赋值在公式中并写上等号了才去执行(Action)","like_count":1},{"had_liked":false,"id":136338,"user_name":"夜吾夜","can_delete":false,"product_type":"c1","uid":1293142,"ip_address":"","ucode":"FC8729883EAD62","user_header":"https://static001.geekbang.org/account/avatar/00/13/bb/56/41cbcda2.jpg","comment_is_top":false,"comment_ctime":1569412108,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5864379404","product_id":100025301,"comment_content":"我是否可以这样理解，使用checkpoint，而不是用持久化的RDD来进行数据恢复，是因为当从某一个节点进行回放时，checkpoint的路径比持久化RDD短，更能节省时间，但spark的这种机制也决定了它不支持确定性计算。","like_count":1},{"had_liked":false,"id":116076,"user_name":"refactor","can_delete":false,"product_type":"c1","uid":1058818,"ip_address":"","ucode":"EC8FF55FE6EC8E","user_header":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","comment_is_top":false,"comment_ctime":1563782213,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5858749509","product_id":100025301,"comment_content":"cache 和 checkpoint 区别：1.产生过程，cache 是 partition 分区计算完后就执行，而后者是要整个 rdd 计算完再去起新的 job 完成，成本更大；2.执行完后 cache 无论存在内存还是硬盘都会被清理，而 后者不会，除非手动清理；3.cache保存依赖关系，而后者删除所有依赖关系。4.读取一个同时被 cache 和 checkpoint 处理过的 rdd，会先读取前者。","like_count":1},{"had_liked":false,"id":95704,"user_name":"Peter","can_delete":false,"product_type":"c1","uid":1506885,"ip_address":"","ucode":"D2A0BB2F6A7236","user_header":"https://static001.geekbang.org/account/avatar/00/16/fe/45/c353f3da.jpg","comment_is_top":false,"comment_ctime":1558146120,"is_pvip":false,"replies":[{"id":"48539","content":"👍🏻","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392699,"ip_address":"","comment_id":95704,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5853113416","product_id":100025301,"comment_content":"在缓存 RDD 的时候，它所有的依赖关系也会被一并存下来。所以持久化的 RDD 有自动的容错机制。如果 RDD 的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算","like_count":1,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450565,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392699,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95370,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1558047467,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5853014763","product_id":100025301,"comment_content":"checkpoint用的不多，是不是可以对目前所有的rdd均缓存，rdd是针对特定rdd缓存","like_count":1},{"had_liked":false,"id":347800,"user_name":"yanger2004","can_delete":false,"product_type":"c1","uid":1009644,"ip_address":"","ucode":"0A2CD03EF31052","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erms9qcIFYZ4npgLYPu1QgxQyaXcj64ZBicNVeBRWcYUpCZ9p0BGsrEcX8heibMLCV4Gde4P9pf7PjA/132","comment_is_top":false,"comment_ctime":1654441338,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1654441338","product_id":100025301,"comment_content":"persist和cache什么区别？","like_count":0},{"had_liked":false,"id":252278,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1602222033,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1602222033","product_id":100025301,"comment_content":"是不是map&#47;reduce可以处理更大规模的数据啊？","like_count":0},{"had_liked":false,"id":195810,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1585216400,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585216400","product_id":100025301,"comment_content":"checkpoint会清空依赖关系，持久化不会","like_count":0},{"had_liked":false,"id":179371,"user_name":"余泽锋","can_delete":false,"product_type":"c1","uid":1003207,"ip_address":"","ucode":"5AB1499746C003","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg","comment_is_top":false,"comment_ctime":1581991837,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1581991837","product_id":100025301,"comment_content":"请问spark可以支持多表头的数据吗？","like_count":0},{"had_liked":false,"id":120269,"user_name":"露娜","can_delete":false,"product_type":"c1","uid":1289012,"ip_address":"","ucode":"CC3EF71EB30EE1","user_header":"https://static001.geekbang.org/account/avatar/00/13/ab/34/ef002163.jpg","comment_is_top":false,"comment_ctime":1564818111,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564818111","product_id":100025301,"comment_content":"如果一个DAG图中出现菱形结构，我持久化会提高效率吗？在一个Action的DAG血缘图里，有个父RDD被两个子RDD依赖。","like_count":0},{"had_liked":false,"id":106701,"user_name":"淹死的大虾","can_delete":false,"product_type":"c1","uid":1488430,"ip_address":"","ucode":"79DB76C60F8404","user_header":"https://static001.geekbang.org/account/avatar/00/16/b6/2e/096790e1.jpg","comment_is_top":false,"comment_ctime":1561374458,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1561374458","product_id":100025301,"comment_content":"持久化到内存就不说了，用完就没了。和持久化到硬盘比，checkpoint只需要按需在关键节点储存，持久化则所有操作节点都会","like_count":0},{"had_liked":false,"id":100644,"user_name":"Flash","can_delete":false,"product_type":"c1","uid":1236163,"ip_address":"","ucode":"E285075C9E0B02","user_header":"https://static001.geekbang.org/account/avatar/00/12/dc/c3/e4ba51d5.jpg","comment_is_top":false,"comment_ctime":1559610240,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559610240","product_id":100025301,"comment_content":"检查点是Spark的一个容错机制，应该是会自动缓存那些计算比较耗时的RDD，缓存的是进行计算动作的RDD。<br>持久化是Spark针对转换操作的RDD进行缓存，需要开发人员手动调用persist或cache方法。","like_count":0},{"had_liked":false,"id":99955,"user_name":"徐宁","can_delete":false,"product_type":"c1","uid":1208822,"ip_address":"","ucode":"5B38BF576B1749","user_header":"https://static001.geekbang.org/account/avatar/00/12/71/f6/ad0ad3df.jpg","comment_is_top":false,"comment_ctime":1559371730,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559371730","product_id":100025301,"comment_content":"看图一spark sql支持jdbc吗？老师能给个链接不？","like_count":0},{"had_liked":false,"id":97338,"user_name":"渡码","can_delete":false,"product_type":"c1","uid":1348536,"ip_address":"","ucode":"8FD8B863D1DA0C","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/b8/6510592e.jpg","comment_is_top":false,"comment_ctime":1558658108,"is_pvip":false,"replies":[{"id":"48538","content":"最主要的区别是是否保存依赖关系，checkpoint只存计算结果，不保留依赖关系；但是缓存化会保存依赖关系，方便错误恢复，开销相对比较小。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392671,"ip_address":"","comment_id":97338,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1558658108","product_id":100025301,"comment_content":"设计上，应用场景不同，checkpoint用来做故障恢复，cache为了避免重复计算。实现技术上我没做深入了解不清楚，猜测存储技术类似，可能checkpoint会额外增加用于故障恢复的信息","like_count":0,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451235,"discussion_content":"最主要的区别是是否保存依赖关系，checkpoint只存计算结果，不保留依赖关系；但是缓存化会保存依赖关系，方便错误恢复，开销相对比较小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392671,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96743,"user_name":"时间是最真的答案","can_delete":false,"product_type":"c1","uid":1183601,"ip_address":"","ucode":"B90F3EF769F865","user_header":"https://static001.geekbang.org/account/avatar/00/12/0f/71/9273e8a4.jpg","comment_is_top":false,"comment_ctime":1558495971,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558495971","product_id":100025301,"comment_content":"checkpoint用于数据回滚，持久化用于计算","like_count":0},{"had_liked":false,"id":95710,"user_name":"lwenbin","can_delete":false,"product_type":"c1","uid":1202109,"ip_address":"","ucode":"05C4CC6BE0B56C","user_header":"https://static001.geekbang.org/account/avatar/00/12/57/bd/acf40fa0.jpg","comment_is_top":false,"comment_ctime":1558146705,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558146705","product_id":100025301,"comment_content":"老师您好<br>上面filter first的例子里，假如Spark从最后一个 first RDD开始计算，通知前一个 filter RDD要给数据，请问filter是完全计算好了还是基于流式的有数据就给？我觉得肯定是有数据就给。不知道理解是不是对。如果有数据就给的话，这个应该是依赖于具体action的吧，比如first sum可以，但是min max这种不行。老师能否更深入的说一下这些处理流程啊？<br>谢谢！","like_count":0},{"had_liked":false,"id":95450,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1558057853,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558057853","product_id":100025301,"comment_content":"建议老师把动作操作统一修改为action操作，驱动程序改为driver端。","like_count":0}]}