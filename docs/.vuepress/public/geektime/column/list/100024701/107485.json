{"id":107485,"title":"47 | 接收网络包（上）：如何搞明白合作伙伴让我们做什么？","content":"<p>前面两节，我们分析了发送网络包的整个过程。这一节，我们来解析接收网络包的过程。</p><p>如果说网络包的发送是从应用层开始，层层调用，一直到网卡驱动程序的话，网络包的结束过程，就是一个反过来的过程，我们不能从应用层的读取开始，而应该从网卡接收到一个网络包开始。我们用两节来解析这个过程，这一节我们从硬件网卡解析到IP层，下一节，我们从IP层解析到Socket层。</p><h2>设备驱动层</h2><p>网卡作为一个硬件，接收到网络包，应该怎么通知操作系统，这个网络包到达了呢？咱们学习过输入输出设备和中断。没错，我们可以触发一个中断。但是这里有个问题，就是网络包的到来，往往是很难预期的。网络吞吐量比较大的时候，网络包的到达会十分频繁。这个时候，如果非常频繁地去触发中断，想想就觉得是个灾难。</p><p>比如说，CPU正在做某个事情，一些网络包来了，触发了中断，CPU停下手里的事情，去处理这些网络包，处理完毕按照中断处理的逻辑，应该回去继续处理其他事情。这个时候，另一些网络包又来了，又触发了中断，CPU手里的事情还没捂热，又要停下来去处理网络包。能不能大家要来的一起来，把网络包好好处理一把，然后再回去集中处理其他事情呢？</p><p>网络包能不能一起来，这个我们没法儿控制，但是我们可以有一种机制，就是当一些网络包到来触发了中断，内核处理完这些网络包之后，我们可以先进入主动轮询poll网卡的方式，主动去接收到来的网络包。如果一直有，就一直处理，等处理告一段落，就返回干其他的事情。当再有下一批网络包到来的时候，再中断，再轮询poll。这样就会大大减少中断的数量，提升网络处理的效率，这种处理方式我们称为<strong>NAPI</strong>。</p><!-- [[[read_end]]] --><p>为了帮你了解设备驱动层的工作机制，我们还是以上一节发送网络包时的网卡drivers/net/ethernet/intel/ixgb/ixgb_main.c为例子，来进行解析。</p><pre><code>static struct pci_driver ixgb_driver = {\n\t.name     = ixgb_driver_name,\n\t.id_table = ixgb_pci_tbl,\n\t.probe    = ixgb_probe,\n\t.remove   = ixgb_remove,\n\t.err_handler = &amp;ixgb_err_handler\n};\n\nMODULE_AUTHOR(&quot;Intel Corporation, &lt;linux.nics@intel.com&gt;&quot;);\nMODULE_DESCRIPTION(&quot;Intel(R) PRO/10GbE Network Driver&quot;);\nMODULE_LICENSE(&quot;GPL&quot;);\nMODULE_VERSION(DRV_VERSION);\n\n/**\n * ixgb_init_module - Driver Registration Routine\n *\n * ixgb_init_module is the first routine called when the driver is\n * loaded. All it does is register with the PCI subsystem.\n **/\n\nstatic int __init\nixgb_init_module(void)\n{\n\tpr_info(&quot;%s - version %s\\n&quot;, ixgb_driver_string, ixgb_driver_version);\n\tpr_info(&quot;%s\\n&quot;, ixgb_copyright);\n\n\treturn pci_register_driver(&amp;ixgb_driver);\n}\n\nmodule_init(ixgb_init_module);\n</code></pre><p>在网卡驱动程序初始化的时候，我们会调用ixgb_init_module，注册一个驱动ixgb_driver，并且调用它的probe函数ixgb_probe。</p><pre><code>static int\nixgb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev = NULL;\n\tstruct ixgb_adapter *adapter;\n......\n\tnetdev = alloc_etherdev(sizeof(struct ixgb_adapter));\n\tSET_NETDEV_DEV(netdev, &amp;pdev-&gt;dev);\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter-&gt;netdev = netdev;\n\tadapter-&gt;pdev = pdev;\n\tadapter-&gt;hw.back = adapter;\n\tadapter-&gt;msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\n\n\tadapter-&gt;hw.hw_addr = pci_ioremap_bar(pdev, BAR_0);\n......\n\tnetdev-&gt;netdev_ops = &amp;ixgb_netdev_ops;\n\tixgb_set_ethtool_ops(netdev);\n\tnetdev-&gt;watchdog_timeo = 5 * HZ;\n\tnetif_napi_add(netdev, &amp;adapter-&gt;napi, ixgb_clean, 64);\n\n\tstrncpy(netdev-&gt;name, pci_name(pdev), sizeof(netdev-&gt;name) - 1);\n\n\tadapter-&gt;bd_number = cards_found;\n\tadapter-&gt;link_speed = 0;\n\tadapter-&gt;link_duplex = 0;\n......\n}\n</code></pre><p>在ixgb_probe中，我们会创建一个struct net_device表示这个网络设备，并且netif_napi_add函数为这个网络设备注册一个轮询poll函数ixgb_clean，将来一旦出现网络包的时候，就是要通过它来轮询了。</p><p>当一个网卡被激活的时候，我们会调用函数ixgb_open-&gt;ixgb_up，在这里面注册一个硬件的中断处理函数。</p><pre><code>int\nixgb_up(struct ixgb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter-&gt;netdev;\n......\n    err = request_irq(adapter-&gt;pdev-&gt;irq, ixgb_intr, irq_flags,\n\t                  netdev-&gt;name, netdev);\n......\n}\n\n/**\n * ixgb_intr - Interrupt Handler\n * @irq: interrupt number\n * @data: pointer to a network interface device structure\n **/\n\nstatic irqreturn_t\nixgb_intr(int irq, void *data)\n{\n\tstruct net_device *netdev = data;\n\tstruct ixgb_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgb_hw *hw = &amp;adapter-&gt;hw;\n......\n\tif (napi_schedule_prep(&amp;adapter-&gt;napi)) {\n\t\tIXGB_WRITE_REG(&amp;adapter-&gt;hw, IMC, ~0);\n\t\t__napi_schedule(&amp;adapter-&gt;napi);\n\t}\n\treturn IRQ_HANDLED;\n}\n</code></pre><p>如果一个网络包到来，触发了硬件中断，就会调用ixgb_intr，这里面会调用__napi_schedule。</p><pre><code>/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&amp;softnet_data), n);\n\tlocal_irq_restore(flags);\n}\n\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n</code></pre><p>__napi_schedule是处于中断处理的关键部分，在他被调用的时候，中断是暂时关闭的，但是处理网络包是个复杂的过程，需要到延迟处理部分，所以____napi_schedule将当前设备放到struct softnet_data结构的poll_list里面，说明在延迟处理部分可以接着处理这个poll_list里面的网络设备。</p><p>然后____napi_schedule触发一个软中断NET_RX_SOFTIRQ，通过软中断触发中断处理的延迟处理部分，也是常用的手段。</p><p>上一节，我们知道，软中断NET_RX_SOFTIRQ对应的中断处理函数是net_rx_action。</p><pre><code>static __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&amp;softnet_data);\n    LIST_HEAD(list);\n    list_splice_init(&amp;sd-&gt;poll_list, &amp;list);\n......\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n......\n\t\tn = list_first_entry(&amp;list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &amp;repoll);\n\t}\n......\n}\n</code></pre><p>在net_rx_action中，会得到struct softnet_data结构，这个结构在发送的时候我们也遇到过。当时它的output_queue用于网络包的发送，这里的poll_list用于网络包的接收。</p><pre><code>struct softnet_data {\n\tstruct list_head\tpoll_list;\n......\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n......\n}\n</code></pre><p>在net_rx_action中，接下来是一个循环，在poll_list里面取出网络包到达的设备，然后调用napi_poll来轮询这些设备，napi_poll会调用最初设备初始化的时候，注册的poll函数，对于ixgb_driver，对应的函数是ixgb_clean。</p><p>ixgb_clean会调用ixgb_clean_rx_irq。</p><pre><code>static bool\nixgb_clean_rx_irq(struct ixgb_adapter *adapter, int *work_done, int work_to_do)\n{\n\tstruct ixgb_desc_ring *rx_ring = &amp;adapter-&gt;rx_ring;\n\tstruct net_device *netdev = adapter-&gt;netdev;\n\tstruct pci_dev *pdev = adapter-&gt;pdev;\n\tstruct ixgb_rx_desc *rx_desc, *next_rxd;\n\tstruct ixgb_buffer *buffer_info, *next_buffer, *next2_buffer;\n\tu32 length;\n\tunsigned int i, j;\n\tint cleaned_count = 0;\n\tbool cleaned = false;\n\n\ti = rx_ring-&gt;next_to_clean;\n\trx_desc = IXGB_RX_DESC(*rx_ring, i);\n\tbuffer_info = &amp;rx_ring-&gt;buffer_info[i];\n\n\twhile (rx_desc-&gt;status &amp; IXGB_RX_DESC_STATUS_DD) {\n\t\tstruct sk_buff *skb;\n\t\tu8 status;\n\n\t\tstatus = rx_desc-&gt;status;\n\t\tskb = buffer_info-&gt;skb;\n\t\tbuffer_info-&gt;skb = NULL;\n\n\t\tprefetch(skb-&gt;data - NET_IP_ALIGN);\n\n\t\tif (++i == rx_ring-&gt;count)\n\t\t\ti = 0;\n\t\tnext_rxd = IXGB_RX_DESC(*rx_ring, i);\n\t\tprefetch(next_rxd);\n\n\t\tj = i + 1;\n\t\tif (j == rx_ring-&gt;count)\n\t\t\tj = 0;\n\t\tnext2_buffer = &amp;rx_ring-&gt;buffer_info[j];\n\t\tprefetch(next2_buffer);\n\n\t\tnext_buffer = &amp;rx_ring-&gt;buffer_info[i];\n......\n\t\tlength = le16_to_cpu(rx_desc-&gt;length);\n\t\trx_desc-&gt;length = 0;\n......\n\t\tixgb_check_copybreak(&amp;adapter-&gt;napi, buffer_info, length, &amp;skb);\n\n\t\t/* Good Receive */\n\t\tskb_put(skb, length);\n\n\t\t/* Receive Checksum Offload */\n\t\tixgb_rx_checksum(adapter, rx_desc, skb);\n\n\t\tskb-&gt;protocol = eth_type_trans(skb, netdev);\n\n\t\tnetif_receive_skb(skb);\n......\n\t\t/* use prefetched values */\n\t\trx_desc = next_rxd;\n\t\tbuffer_info = next_buffer;\n\t}\n\n\trx_ring-&gt;next_to_clean = i;\n......\n}\n</code></pre><p>在网络设备的驱动层，有一个用于接收网络包的rx_ring。它是一个环，从网卡硬件接收的包会放在这个环里面。这个环里面的buffer_info[]是一个数组，存放的是网络包的内容。i和j是这个数组的下标，在ixgb_clean_rx_irq里面的while循环中，依次处理环里面的数据。在这里面，我们看到了i和j加一之后，如果超过了数组的大小，就跳回下标0，就说明这是一个环。</p><p>ixgb_check_copybreak函数将buffer_info里面的内容，拷贝到struct sk_buff *skb，从而可以作为一个网络包进行后续的处理，然后调用netif_receive_skb。</p><h2>网络协议栈的二层逻辑</h2><p>从netif_receive_skb函数开始，我们就进入了内核的网络协议栈。</p><p>接下来的调用链为：netif_receive_skb-&gt;netif_receive_skb_internal-&gt;__netif_receive_skb-&gt;__netif_receive_skb_core。</p><p>在__netif_receive_skb_core中，我们先是处理了二层的一些逻辑。例如，对于VLAN的处理，接下来要想办法交给第三层。</p><pre><code>static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct packet_type *ptype, *pt_prev;\n......\n\ttype = skb-&gt;protocol;\n......\n\tdeliver_ptype_list_skb(skb, &amp;pt_prev, orig_dev, type,\n\t\t\t       &amp;orig_dev-&gt;ptype_specific);\n\tif (pt_prev) {\n\t\tret = pt_prev-&gt;func(skb, skb-&gt;dev, pt_prev, orig_dev);\n\t}\n......\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype-&gt;type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n</code></pre><p>在网络包struct sk_buff里面，二层的头里面有一个protocol，表示里面一层，也即三层是什么协议。deliver_ptype_list_skb在一个协议列表中逐个匹配。如果能够匹配到，就返回。</p><p>这些协议的注册在网络协议栈初始化的时候， inet_init函数调用dev_add_pack(&amp;ip_packet_type)，添加IP协议。协议被放在一个链表里面。</p><pre><code>void dev_add_pack(struct packet_type *pt)\n{\n    struct list_head *head = ptype_head(pt);\n    list_add_rcu(&amp;pt-&gt;list, head);\n}\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n    if (pt-&gt;type == htons(ETH_P_ALL))\n        return pt-&gt;dev ? &amp;pt-&gt;dev-&gt;ptype_all : &amp;ptype_all;\n    else\n        return pt-&gt;dev ? &amp;pt-&gt;dev-&gt;ptype_specific : &amp;ptype_base[ntohs(pt-&gt;type) &amp; PTYPE_HASH_MASK];\n}\n</code></pre><p>假设这个时候的网络包是一个IP包，则在这个链表里面一定能够找到ip_packet_type，在__netif_receive_skb_core中会调用ip_packet_type的func函数。</p><pre><code>static struct packet_type ip_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.func = ip_rcv,\n};\n</code></pre><p>从上面的定义我们可以看出，接下来，ip_rcv会被调用。</p><h2>网络协议栈的IP层</h2><p>从ip_rcv函数开始，我们的处理逻辑就从二层到了三层，IP层。</p><pre><code>int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)\n{\n\tconst struct iphdr *iph;\n\tstruct net *net;\n\tu32 len;\n......\n\tnet = dev_net(dev);\n......\n\tiph = ip_hdr(skb);\n\tlen = ntohs(iph-&gt;tot_len);\n\tskb-&gt;transport_header = skb-&gt;network_header + iph-&gt;ihl*4;\n......\n\treturn NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING,\n\t\t       net, NULL, skb, dev, NULL,\n\t\t       ip_rcv_finish);\n......\n}\n</code></pre><p>在ip_rcv中，得到IP头，然后又遇到了我们见过多次的NF_HOOK，这次因为是接收网络包，第一个hook点是NF_INET_PRE_ROUTING，也就是iptables的PREROUTING链。如果里面有规则，则执行规则，然后调用ip_rcv_finish。</p><pre><code>static int ip_rcv_finish(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct net_device *dev = skb-&gt;dev;\n\tstruct rtable *rt;\n\tint err;\n......\n\trt = skb_rtable(skb);\n.....\n\treturn dst_input(skb);\n}\n\nstatic inline int dst_input(struct sk_buff *skb)\n{\n\treturn skb_dst(skb)-&gt;input(skb);\n\n</code></pre><p>ip_rcv_finish得到网络包对应的路由表，然后调用dst_input，在dst_input中，调用的是struct rtable的成员的dst的input函数。在rt_dst_alloc中，我们可以看到，input函数指向的是ip_local_deliver。</p><pre><code>int ip_local_deliver(struct sk_buff *skb)\n{\n\t/*\n\t *\tReassemble IP fragments.\n\t */\n\tstruct net *net = dev_net(skb-&gt;dev);\n\n\tif (ip_is_fragment(ip_hdr(skb))) {\n\t\tif (ip_defrag(net, skb, IP_DEFRAG_LOCAL_DELIVER))\n\t\t\treturn 0;\n\t}\n\n\treturn NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN,\n\t\t       net, NULL, skb, skb-&gt;dev, NULL,\n\t\t       ip_local_deliver_finish);\n}\n</code></pre><p>在ip_local_deliver函数中，如果IP层进行了分段，则进行重新的组合。接下来就是我们熟悉的NF_HOOK。hook点在NF_INET_LOCAL_IN，对应iptables里面的INPUT链。在经过iptables规则处理完毕后，我们调用ip_local_deliver_finish。</p><pre><code>static int ip_local_deliver_finish(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\t__skb_pull(skb, skb_network_header_len(skb));\n\n\tint protocol = ip_hdr(skb)-&gt;protocol;\n\tconst struct net_protocol *ipprot;\n\n\tipprot = rcu_dereference(inet_protos[protocol]);\n\tif (ipprot) {\n\t\tint ret;\n\t\tret = ipprot-&gt;handler(skb);\n......\n\t}\n......\n}\n</code></pre><p>在IP头中，有一个字段protocol用于指定里面一层的协议，在这里应该是TCP协议。于是，从inet_protos数组中，找出TCP协议对应的处理函数。这个数组的定义如下，里面的内容是struct net_protocol。</p><pre><code>struct net_protocol __rcu *inet_protos[MAX_INET_PROTOS] __read_mostly;\n\nint inet_add_protocol(const struct net_protocol *prot, unsigned char protocol)\n{\n......\n\treturn !cmpxchg((const struct net_protocol **)&amp;inet_protos[protocol],\n\t\t\tNULL, prot) ? 0 : -1;\n}\n\nstatic int __init inet_init(void)\n{\n......\n\tif (inet_add_protocol(&amp;udp_protocol, IPPROTO_UDP) &lt; 0)\n\t\tpr_crit(&quot;%s: Cannot add UDP protocol\\n&quot;, __func__);\n\tif (inet_add_protocol(&amp;tcp_protocol, IPPROTO_TCP) &lt; 0)\n\t\tpr_crit(&quot;%s: Cannot add TCP protocol\\n&quot;, __func__);\n......\n}\n\nstatic struct net_protocol tcp_protocol = {\n\t.early_demux\t=\ttcp_v4_early_demux,\n\t.early_demux_handler =  tcp_v4_early_demux,\n\t.handler\t=\ttcp_v4_rcv,\n\t.err_handler\t=\ttcp_v4_err,\n\t.no_policy\t=\t1,\n\t.netns_ok\t=\t1,\n\t.icmp_strict_tag_validation = 1,\n};\n\nstatic struct net_protocol udp_protocol = {\n\t.early_demux =\tudp_v4_early_demux,\n\t.early_demux_handler =\tudp_v4_early_demux,\n\t.handler =\tudp_rcv,\n\t.err_handler =\tudp_err,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n</code></pre><p>在系统初始化的时候，网络协议栈的初始化调用的是inet_init，它会调用inet_add_protocol，将TCP协议对应的处理函数tcp_protocol、UDP协议对应的处理函数udp_protocol，放到inet_protos数组中。</p><p>在上面的网络包的接收过程中，会取出TCP协议对应的处理函数tcp_protocol，然后调用handler函数，也即tcp_v4_rcv函数。</p><h2>总结时刻</h2><p>这一节我们讲了接收网络包的上半部分，分以下几个层次。</p><ul>\n<li>硬件网卡接收到网络包之后，通过DMA技术，将网络包放入Ring Buffer。</li>\n<li>硬件网卡通过中断通知CPU新的网络包的到来。</li>\n<li>网卡驱动程序会注册中断处理函数ixgb_intr。</li>\n<li>中断处理函数处理完需要暂时屏蔽中断的核心流程之后，通过软中断NET_RX_SOFTIRQ触发接下来的处理过程。</li>\n<li>NET_RX_SOFTIRQ软中断处理函数net_rx_action，net_rx_action会调用napi_poll，进而调用ixgb_clean_rx_irq，从Ring Buffer中读取数据到内核struct sk_buff。</li>\n<li>调用netif_receive_skb进入内核网络协议栈，进行一些关于VLAN的二层逻辑处理后，调用ip_rcv进入三层IP层。</li>\n<li>在IP层，会处理iptables规则，然后调用ip_local_deliver，交给更上层TCP层。</li>\n<li>在TCP层调用tcp_v4_rcv。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/a5/37/a51af8ada1135101e252271626669337.png\" alt=\"\"></p><h2>课堂练习</h2><p>我们没有仔细分析对于二层VLAN的处理，请你研究一下VLAN的原理，然后在代码中看一下对于VLAN的处理过程，这是一项重要的网络基础知识。</p><p>欢迎留言和我分享你的疑惑和见解 ，也欢迎可以收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。</p><p><img src=\"https://static001.geekbang.org/resource/image/8c/37/8c0a95fa07a8b9a1abfd394479bdd637.jpg\" alt=\"\"></p>","neighbors":{"left":{"article_title":"46 | 发送网络包（下）：如何表达我们想让合作伙伴做什么？","id":107209},"right":{"article_title":"48 | 接收网络包（下）：如何搞明白合作伙伴让我们做什么？","id":108227}},"comments":[]}