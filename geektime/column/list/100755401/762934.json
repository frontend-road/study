{"id":762934,"title":"10｜LLM辅助建模（二）：构造思维链","content":"<p>你好，我是徐昊，今天我们来继续学习AI时代的软件工程。</p><p>在上一节课，我们介绍了如何构建反馈循环，并使用大语言模型（Large Language Model，LLM）反馈模型的缺失，然后再使用LLM进行模型展开，完成对模型的验证。构造有效的反馈循环，并使用LLM对其加速，这是一种利用LLM加速知识学习的通用模式。</p><p>然而在实际的应用的过程中，很少会出现不提供业务上下文，仅仅通过LLM反馈辅助建模这样的情况，而且这样的做法实际效率也不高。今天，我们来讨论一下使用LLM辅助建模的其他方法。</p><h2>通过概念字典辅助建模</h2><p>我们知道在建模的过程中，最重要的就是发掘系统中的存在的概念，并建立概念与概念之间的关联。那么在实际建模的过程中，对于概念的提炼就是重中之重了。</p><p>通常在建模的过程中，我们会维护一个<strong>概念字典（Glossary）</strong>，其中包含对于系统非常重要的业务概念，以及对于这些概念的基本解释。举个例子，在上节课中，当我们收到LLM给出的反馈后，我们对于建模任务给出了这样的调整：</p><pre><code class=\"language-plain\">这是一个教学学籍管理系统。系统中应该包含以下的核心概念：\n- 教学计划：一系列相关课程和活动，这些课程和活动旨在培养特定领域的知识和技能。比如，计算机科学与技术学士学位教学计划，或是计算机科学与技术硕士学位教学计划\n- 录取通知：学生需要根据录取通知注册学籍。录取通知应该包含学生被录取的信息，如录取的教学计划\n- 学籍：当学生注册之后，学籍记录学生在校将按照哪个教学计划学习\n- 学生\n</code></pre><!-- [[[read_end]]] --><p>这实际就是一个<strong>概念字典</strong>。而在实际工作中，概念字典的构建并不需要完全依靠LLM的反馈。在建模的过程中，我们通过对领域或业务的理解、用户访谈，甚至是阅读用户故事，都不难发现其中涉及的一些概念，以及这些概念之间的关联。</p><p>因此，我们可以在建模开始就构造<strong>概念字典</strong>，并通过概念字典加速整个建模反馈的过程。那么我们可以修改我们的任务模版：</p><pre><code class=\"language-plain\">业务描述\n=======\n{context}\n\n系统中涉及概念的glossary如下：\n{glossary}\n\n任务\n====\n根据业务描述，为系统建立模型。可以添加你认为必要的实体和关系。并将模型表示为mermaid的class diagram\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/40/f6/406aed45e7c82941a0b957c4aec78cf6.jpg?wh=1263x1990\" alt=\"\"></p><p>同样，我们也可以使用LLM辅助我们提取这个<strong>概念字典</strong>。我们可以设计这样的任务模板：</p><pre><code class=\"language-plain\">用户故事\n======\n{story}\n\n任务\n===\n请根据用户故事中描述的业务场景，提取其中的业务概念，并给出每个概念的定义。\n结果以表格形式给出。\n</code></pre><p>通过一部分用户故事，让LLM理解业务的上下文，并从中提取我们提及的业务概念。仍然以上节课中，学生注册的用户故事为例：</p><blockquote>\n<p><strong>作为</strong>学校的教职员工<br>\n<strong>我希望</strong>学生可以根据录取通知将学籍注册到教学计划上<br>\n<strong>从而</strong>我可以跟踪他们的获取学位的进度</p>\n</blockquote><p>使用上面的任务模板，我们可以获得这样的概念字典：</p><p><img src=\"https://static001.geekbang.org/resource/image/16/12/16ce209a14735d1d1011de2bf8b79112.jpg?wh=1362x1035\" alt=\"\"></p><p>当然这里有些概念并不十分准确，比如<strong>学籍注册</strong>，显然是个分词错误。这些错误在反馈中也不难修正。</p><p>在得到<strong>概念字典</strong>之后，我们就可以将概念字典与任务模板结合，完成建模。然后再使用模型检查和模型展开，帮助我们验证模型的适用性。过程与之前的过程类似，就不在此重复了。我们需要重点强调的是，<strong>这个辅助建模过程的转变</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/f3/2ec17aefcd3c73369e7766417289e9f3.jpg?wh=1894x1080\" alt=\"\"></p><p>在最开始的时候，我们完全使用<strong>复杂认知模式（Complex）</strong>，依靠反馈循环辅助我们完成建模的过程。而当我们发现LLM给出的反馈，可以构成<strong>概念字典</strong>时。我们的关注点，就变成了如何有效地生成<strong>概念字典</strong>。也就是，我们从<strong>探测性的模型生成转向了有针对性的知识生成（Generated Knowledge）</strong>。这实际是一种认知提升的体现，我们也由此逐渐进入到<strong>庞杂认知模式（Complicated）。</strong></p><h2>围绕概念字典构造思维链</h2><p>一旦进入到<strong>庞杂认知模式</strong>，那么通过构造思维链（Chain of Thought）提高效率就是一个很自然的选择了。而针对建模而言，就是把建模方法转化为思维链，以提高模型的质量。</p><p>让我们举一个例子，如果我们目前要以Peter Coad的四色法（4 Colors Modeling）对这个系统进行建模，那么我们需要为LLM讲解四色法的四个基本原型：</p><blockquote>\n<p>在使用四色法建模时，我们将使用 4 个基本原型（Archetype）：Moment-interval、role、party-place-thing和description。<br>\n&nbsp;<br>\n在这四个原型中，最重要的原型是moment-interval，也就是某个时间或是一段时间。它代表的是出于业务或法律原因需要记录和跟踪的事情，是在某个时间或时间段内发生的事情。它帮助提醒我们在问题领域中寻找重要的时刻或时间段。<br>\n&nbsp;<br>\n比如，销售（Sale）是在某一时刻进行的（moment），这里重要的信息是销售的日期和时间；<br>\n比如，租赁（Rental）则发生在一段时间内，这个时间段就是从支付（checkout）到归还（return）；<br>\n比如，预订（Reservation）也是发生在一段时间内，这个时间段就是预订到使用、取消或过期。<br>\n&nbsp;<br>\n第二个重要原型是role，也就是角色。角色是人、地点或事物（party-place-thing）以何种方式参与到moment-interval中。<br>\n&nbsp;<br>\n比如，在销售（Sale）中，就会存在买家(buyer)和卖家(seller)两种角色。<br>\n&nbsp;<br>\n第三个原型是party-place-thing，是扮演不同role的人（个人或组织）、地点或事物。<br>\n&nbsp;<br>\n第四个原型是description，它是一种类似于目录条目的值对象，用以描述party-place-thing的具体数据。</p>\n</blockquote><p>并提供建模思路的CoT描述：</p><blockquote>\n<p>使用四色法建模时，步骤如下：<br>\n&nbsp;<br>\n首先需要寻找系统中的moment-interval，并梳理与它前后关联的其他moment-interval。比如，支付（payment）作为一个moment-interval，可能存在前置的moment-interval对象订单（order）<br>\n然后寻找参与到moment-interval中的role<br>\n之后再寻找可以扮演这些role的party-place-thing<br>\n最后寻找description对象</p>\n</blockquote><p>然后，我们可以修改建模的任务模板：</p><pre><code class=\"language-plain\">用户故事\n======\n{story}\n\n概念提取的方法\n============\n{modeling_method}\n\n任务\n===\n请根据上面描述的业务场景，按照提取概念的方法，提取其中的业务概念。并给出每个概念的定义。并以表格形式给出Glossary，并标注对应的archtype\n</code></pre><p>让GPT使用四色建模法得到的概念字典是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/97/bf/972c73b13bf05a261c1ca4fd7bcd35bf.jpg?wh=1385x1357\" alt=\"\"></p><p>这次生成的结果就要比之前好了很多，因为我们让它明确标记了建模的类型到底是什么类型。</p><p>剩下的步骤就类似了，我们需要将找到的概念字典放入建模模板，然后进行模型的检查环节。这里就不再赘述了。</p><p>当然我们也可以使用其他的建模方法，比如<strong>催化剂方法（Catalysis）</strong>、<strong>实体目标法</strong>、<strong>事件风暴法（Event Storming）</strong>等等。</p><p>关键在于寻找到合适的<strong>中间产物</strong>，利用知识生成让LLM辅助提取核心概念。比如，对于事件风暴法，除了实体外，可能还需要提取事件等。但整体思路上，与我们所举的例子，并没有什么不同。</p><h2>小结</h2><p>如果再看一下我们目前所用的流程，在提取领域字典时，需要用户故事作为业务上下文；在检查模型是否有概念遗漏时，也使用了用户故事作为业务上下文；最后在模型展开的时候，还是用的用户故事作为业务上下文。</p><p>在不同场景下，我们都使用了用户故事，但是目的不同。提取领域字典时，提供用户故事主要是为了建模，而做完备性检查和模型展开时，用户故事则用来验证模型的适用性。那么我们就能从机器学习的角度去理解我们现在所做的事情。也就是将用户故事划分为<strong>学习集合</strong>和<strong>测试集合。</strong></p><p>LLM先利用学习集合提取其中的模型，然后再用测试集合中的用户故事，验证模型的适用性。</p><p><img src=\"https://static001.geekbang.org/resource/image/34/74/34a1edfc011d6c60ce36cdf9b0f60674.jpg?wh=1948x1080\" alt=\"\"></p><p>在我们给出的例子中，我们并没有区分不同的用户故事，通篇都使用了相同的用户故事。但在实际工作中，我们通常使用 <strong>Epic用户故事</strong>提取模型，然后再利用更细粒度的用户故事对模型加以验证。</p><p>如果已经存在相对完善的用户故事，那么我们可以相对容易地利用LLM提取模型。<strong>这就为遗留系统提供了很大便利</strong>。而如果是从头开始建设的系统，那么有效地通过用户故事捕捉业务需求，就成了能利用好LLM的关键，这也是我们接下来要讲解的内容。</p><h2>思考题</h2><p>请为其他建模方法构造CoT，并对当前的例子建模。</p><p>欢迎你在留言区分享自己的思考或疑惑，我们会把精彩内容置顶供大家学习讨论。</p>","comments":[{"had_liked":false,"id":389159,"user_name":"听水的湖","can_delete":false,"product_type":"c1","uid":1501385,"ip_address":"北京","ucode":"B1759F90165D81","user_header":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","comment_is_top":true,"comment_ctime":1711696680,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"第二章学习反馈持续收集中，链接在这里：https:&#47;&#47;jinshuju.net&#47;f&#47;YzTEQa[社会社会] 期待你们的反馈！","like_count":1},{"had_liked":false,"id":389262,"user_name":"范飞扬","can_delete":false,"product_type":"c1","uid":2721761,"ip_address":"广东","ucode":"A665DF46833A81","user_header":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","comment_is_top":false,"comment_ctime":1711975563,"is_pvip":false,"replies":[{"id":141626,"content":"如果epic没有覆盖所有核心概念 epic是不合格的 因而只用epic就够了","user_name":"作者回复","user_name_real":"编辑","uid":2537798,"ctime":1712019672,"ip_address":"上海","comment_id":389262,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"原文：我们通常使用 Epic 用户故事提取模型。\n\n为什么呢？翻了一下User story applied，没什么收获。我想到两个原因：\n1、一开始粒度可以先粗一点，需要的时候再细化（比较敏捷？）\n2、用epic的粒度描述feature的话，story数量比较少，比较方便","like_count":2,"discussions":[{"author":{"id":2537798,"avatar":"https://static001.geekbang.org/account/avatar/00/26/b9/46/758ecf4a.jpg","nickname":"徐八叉","note":"","ucode":"DA6D1EB08A7396","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":640947,"discussion_content":"如果epic没有覆盖所有核心概念 epic是不合格的 因而只用epic就够了","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1712019672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":2,"child_discussions":[{"author":{"id":2721761,"avatar":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","nickname":"范飞扬","note":"","ucode":"A665DF46833A81","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2537798,"avatar":"https://static001.geekbang.org/account/avatar/00/26/b9/46/758ecf4a.jpg","nickname":"徐八叉","note":"","ucode":"DA6D1EB08A7396","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":640965,"discussion_content":"妙啊，谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712031021,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":640947,"ip_address":"浙江","group_id":0},"score":640965,"extra":""},{"author":{"id":1154294,"avatar":"https://static001.geekbang.org/account/avatar/00/11/9c/f6/eca921d9.jpg","nickname":"赫伯伯","note":"","ucode":"85722DBCB88E9C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2537798,"avatar":"https://static001.geekbang.org/account/avatar/00/26/b9/46/758ecf4a.jpg","nickname":"徐八叉","note":"","ucode":"DA6D1EB08A7396","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":641538,"discussion_content":"这个解释直击要害✊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712745075,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":640947,"ip_address":"河北","group_id":0},"score":641538,"extra":""}]}]},{"had_liked":false,"id":390210,"user_name":"6点无痛早起学习的和尚","can_delete":false,"product_type":"c1","uid":1703256,"ip_address":"北京","ucode":"33A8A1CDA103F9","user_header":"https://static001.geekbang.org/account/avatar/00/19/fd/58/1af629c7.jpg","comment_is_top":false,"comment_ctime":1714717057,"is_pvip":false,"replies":[{"id":141998,"content":"让子弹飞一会儿","user_name":"编辑回复","user_name_real":"编辑","uid":1501385,"ctime":1715237802,"ip_address":"北京","comment_id":390210,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"2024年05月03日14:17:27\n到了这里留言越来越少了","like_count":0,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":644426,"discussion_content":"让子弹飞一会儿","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1715237802,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":8,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":389281,"user_name":"范飞扬","can_delete":false,"product_type":"c1","uid":2721761,"ip_address":"浙江","ucode":"A665DF46833A81","user_header":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","comment_is_top":false,"comment_ctime":1712031760,"is_pvip":false,"replies":[{"id":141630,"content":"意思是把建模任务模版改为提取glossary的模板哦～要不蹲守一下其他同学的反馈，看看是否有歧义。","user_name":"编辑回复","user_name_real":"编辑","uid":1501385,"ctime":1712034946,"ip_address":"北京","comment_id":389281,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"勘误：\n\n原文：然后，我们可以修改建模的任务模板：用户故事======{story}概念提取的方法============{modeling_method}任务===请根据上面描述的业务场景，按照提取概念的方法，提取其中的业务概念。并给出每个概念的定义。并以表格形式给出Glossary，并标注对应的archtype\n\n======\n\n这个不是建模任务模板，这个是提取glossary的模板，建模模板会让生成mermiad","like_count":0,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":640971,"discussion_content":"意思是把建模任务模版改为提取glossary的模板哦～要不蹲守一下其他同学的反馈，看看是否有歧义。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712034947,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":8,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":389198,"user_name":"aoe","can_delete":false,"product_type":"c1","uid":1121758,"ip_address":"浙江","ucode":"1C6201EDB4E954","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","comment_is_top":false,"comment_ctime":1711805163,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"第二次学习收获：\n\n1. 将概念字典（Glossary）加入 CoT 中可以增强 AI 辅助建模\n2. 如果没有概念字典，可以通过用户故事让 AI 提取\n3. CoT 越完善 AI 生成的结果越准确，当缺少某项概念时，可以先通过 AI 获取，然后再加入 CoT，让 AI 越来越强","like_count":3},{"had_liked":false,"id":391302,"user_name":"范飞扬","can_delete":false,"product_type":"c1","uid":2721761,"ip_address":"广东","ucode":"A665DF46833A81","user_header":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","comment_is_top":false,"comment_ctime":1717909794,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"在文章最后突然来了个“领域字典”，这个领域字典和概念字典是同一个东西嘛？","like_count":0},{"had_liked":false,"id":391238,"user_name":"范飞扬","can_delete":false,"product_type":"c1","uid":2721761,"ip_address":"广东","ucode":"A665DF46833A81","user_header":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","comment_is_top":false,"comment_ctime":1717676101,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"先epic 然后用细粒度的用户故事验证。\n这和TDD很像，先AT再UT。","like_count":0},{"had_liked":false,"id":389311,"user_name":"术子米德","can_delete":false,"product_type":"c1","uid":1898023,"ip_address":"日本","ucode":"382EA7E2AF0B56","user_header":"https://static001.geekbang.org/account/avatar/00/1c/f6/27/c27599ae.jpg","comment_is_top":false,"comment_ctime":1712098007,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"10 | LLM辅助建模（二）：构造思维链\n🤔☕️🤔☕️🤔\n【R】建模中发掘概念，建立观念间关联，维护成概念词典（Glossary）。\n开始为复杂认知模式（Complex），当发现LLM给出的反馈，可以有效构成概念字典时，即从探测性模型生成、转向针对性的知识生成（Generated Knowledge），由此逐渐进入到庞杂认知模式（Complicated），提效就靠构造思维链（CoT）的发挥，再借助四色法、催化剂法、实体目标法、事件风暴法。\n【.I.】概念，最容易建立，是因为脑子里会自动浮现出来，最难以共识，即使看着相同的词典，在没有一起摩擦过几次之前，面对相同的名词，大概率9个人有11种以上的概念解释，包括某些人前后解释还矛盾的情况。容易和困难，同时在概念上表现得如此突出。\n概念，举个例子，立刻应付道，噢，这样呀，加点操作，嗯，知道啦，来个状态，啊，至于嘛，再来辨析分类抽象，这，清晰到繁复，繁复到简化，得多来几遍，才能体验到概念之魔力，也真正需要概念它老人家的存在。\n【Q】用户故事，主要来自社会化活动，逻辑上它不该来自LLM的生成？\n    —  by 术子米德@2024年4月2日","like_count":0},{"had_liked":false,"id":389284,"user_name":"范飞扬","can_delete":false,"product_type":"c1","uid":2721761,"ip_address":"浙江","ucode":"A665DF46833A81","user_header":"https://static001.geekbang.org/account/avatar/00/29/87/e1/b3edcc09.jpg","comment_is_top":false,"comment_ctime":1712034897,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100755401,"comment_content":"老师，我注意到建模的任务模板只有context 和 glossary 这两个 variable 。既然我们在提取glossary 的任务模板里有 modeling_method 这个variable，那么建模的任务模板是不是也加上modeling_method呢？还是这里面有什么不加的考量？","like_count":0,"discussions":[{"author":{"id":3837552,"avatar":"","nickname":"Geek_aa4629","note":"","ucode":"98385FEE90374B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":642134,"discussion_content":"提取Glossory的模板，是告诉LLM，从user story里，通过哪种modeling method（含CoT），来提取glossory。\n\n建模的模板，是通过业务描述（context） + glossory，完成建模并用mermaid展示，建模过程并没有用到user story。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713340179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}