{"id":787626,"title":"16｜从零开始，构建一个具有100M参数规模的Transformer模型","content":"<p>你好，我是独行。</p><p>前两节课，我从理论层面向你介绍了Transformer的架构原理，到现在为止，我们所有介绍基础理论知识的部分基本结束了。从这节课开始，我们会进入实战环节，对模型的设计、构建、预训练、微调、评估等进行全面的介绍，所以接下来的课程会越来越有意思。这节课，我们先来学习下，如何从0到1构建基于Transformer的模型。</p><h2>选择模型架构</h2><p>Transformer架构自从被提出以来，已经演化出多种变体，用来适应不同的任务和性能需求。以下是一些常见的Transformer架构方式，包括原始的设计以及后续的一些创新。</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/ba/e3b3f87c7cf8298f70f760d8eca979ba.jpg?wh=2082x964\" alt=\"图片\"></p><p>我们熟知的GPT系列模型，使用的是Decoder-only架构，Google的Bert模型使用的是Encoder-only架构。GPT系列模型之所以选择使用Decoder-only架构，主要出于以下几点考虑：</p><ol>\n<li>GPT是语言模型，根据给定的文本预测下一个单词，而解码器就是用来生成输出序列的。</li>\n<li>Decoder-only模型采用自回归方式进行训练，在生成每一个新词时，都会利用之前所有已生成的词作为上下文。这种自回归的特性使得GPT能够在生成文本时保持内容的连贯性和逻辑性。</li>\n<li>与编码器-解码器结构相比，Decoder-only架构简化了模型设计，专注于解码器的能力。</li>\n</ol><!-- [[[read_end]]] --><p>其他原因如并行性、长距离依赖、自注意力就不讲了，这是Transformer架构的通用特点，我们这节课就基于Decoder-only架构构建一个模型。</p><h2>构建模型</h2><p>模型设计比较复杂，需要先设计模型大概的结构，比如层数、多头注意力头数、隐藏层层数等，甚至词汇表大小都要有一个预估，根据这些参数，我们可以大概计算出模型的参数量，然后根据Scaling Law，计算出大概需要的运算量，进而评估训练成本。</p><p>Scaling Laws，简单理解就是随着模型大小、数据集大小和用于训练的计算浮点数的增加，模型的性能会提高。并且为了获得最佳性能，这三个因素必须同时放大。当不受其他两个因素的制约时，模型性能与每个单独的因素都有幂律关系，其中包含一个计算公式：浮点运算量（FLOPs）C、模型参数N以及训练的token数D之间存在关系：$C\\approx6ND$。不过需要注意，这个公式成立的前提是，我们的模型基于Decoder-only架构。</p><p>我们先来看看参数规模的计算方式：总参数量=嵌入层参数量+位置编码参数量+解码器层参数量+线性输出层参数量。</p><ol>\n<li><strong>嵌入层参数量</strong> = vocab_size * embed_size</li>\n</ol><p>vocab_size是指词汇表的大小，预训练数据集处理后会转换成词汇表，vocab_size就是这个词汇表的大小；embed_size是指词嵌入向量的维度数，简单理解就是每个词的特征数。</p><ol start=\"2\">\n<li><strong>位置编码参数量</strong> = embed_size</li>\n<li><strong>解码器层参数量</strong> =（自注意力机制参数量 + 前向馈网络参数量）* 层数</li>\n</ol><p>自注意力部分通常包括四个核心组件的参数。</p><ul>\n<li>查询矩阵 (Q)：embed_size * embed_size</li>\n<li>键矩阵 (K)：embed_size * embed_size</li>\n<li>值矩阵 (V)：embed_size * embed_size</li>\n<li>输出线性变换：embed_size * embed_size</li>\n</ul><p>所以自注意力机制参数量 = 4 * embed_size * embed_size，前馈网络参数量 = 2 * (embed_size * hidden_dim)，hidden_dim是指隐藏层层数。</p><ol start=\"4\">\n<li><strong>线性输出层参数量</strong> = vocab_size * embed_size</li>\n</ol><p><strong>总参数量N</strong> = vocab_size * embed_size+embed_size+（4 * embed_size * embed_size+2 * embed_size * hidden_dim）* 层数 + vocab_size * embed_size。公式大概就是这样的，我们先不计算最终结果，因为vocab_size取决于训练文本大小以及分词方式，所以参数量先放放，我们继续往下看。</p><p>我们接下来定义一个简单的Transformer模型，每行代码都有注释，你可以看下。</p><pre><code class=\"language-python\">import torch\nfrom torch import nn\n\n# 定义一个仅包含解码器的Transformer模型\nclass TransformerDecoderModel(nn.Module):\n&nbsp; &nbsp; def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers):\n&nbsp; &nbsp; &nbsp; &nbsp; super(TransformerDecoderModel, self).__init__()&nbsp; # 调用基类的初始化函数\n&nbsp; &nbsp; &nbsp; &nbsp; # 创建嵌入层，将词索引转换为嵌入向量\n&nbsp; &nbsp; &nbsp; &nbsp; self.embed = nn.Embedding(vocab_size, embed_size)\n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化位置编码，是一个可学习的参数\n&nbsp; &nbsp; &nbsp; &nbsp; self.positional_encoding = nn.Parameter(torch.randn(embed_size).unsqueeze(0))\n&nbsp; &nbsp; &nbsp; &nbsp; # 定义一个Transformer解码器层\n&nbsp; &nbsp; &nbsp; &nbsp; decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim)\n&nbsp; &nbsp; &nbsp; &nbsp; # 堆叠多个解码器层构成完整的解码器\n&nbsp; &nbsp; &nbsp; &nbsp; self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n&nbsp; &nbsp; &nbsp; &nbsp; # 定义输出层，将解码器输出转换回词汇空间\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = nn.Linear(embed_size, vocab_size)\n\n&nbsp; &nbsp; def forward(self, src):\n&nbsp; &nbsp; &nbsp; &nbsp; # 嵌入输入并添加位置编码\n&nbsp; &nbsp; &nbsp; &nbsp; src = self.embed(src) + self.positional_encoding\n&nbsp; &nbsp; &nbsp; &nbsp; # 生成源序列的掩码，用于屏蔽未来的信息\n&nbsp; &nbsp; &nbsp; &nbsp; src_mask = self.generate_square_subsequent_mask(src.size(0))\n&nbsp; &nbsp; &nbsp; &nbsp; # 通过解码器传递源数据和掩码\n&nbsp; &nbsp; &nbsp; &nbsp; output = self.transformer_decoder(src, src, src_mask)\n&nbsp; &nbsp; &nbsp; &nbsp; # 应用线性层输出最终的预测结果\n&nbsp; &nbsp; &nbsp; &nbsp; output = self.fc(output)\n&nbsp; &nbsp; &nbsp; &nbsp; return output\n\n&nbsp; &nbsp; def generate_square_subsequent_mask(self, sz):\n&nbsp; &nbsp; &nbsp; &nbsp; # 生成一个上三角矩阵，用于序列生成中遮蔽未来位置的信息\n&nbsp; &nbsp; &nbsp; &nbsp; mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n&nbsp; &nbsp; &nbsp; &nbsp; # 将掩码的非零位置设为无穷大，零位置设为0\n&nbsp; &nbsp; &nbsp; &nbsp; mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n&nbsp; &nbsp; &nbsp; &nbsp; return mask\n\n</code></pre><p>__init__方法类似于Java里类的构造函数，用来初始化属性。forward方法是前向传播的具体实现，generate_square_subsequent_mask方法用来生成掩码矩阵。这个类会在训练的时候进行实例化。在正式训练之前，我们先看下怎么准备数据。</p><h2>准备训练数据</h2><p>本次训练我找的是一个公开的<a href=\"https://drive.usercontent.google.com/download?id=1EdHUZIDpgcBoSqbjlfNKJ3b1t0XIUjbt&export=download&authuser=0&confirm=t&uuid=8062444c-9f4a-4d45-85b0-18c9433e2adb&at=APZUnTVIp-S0F7HyCRpkH7ILqqC3%3A1713443417964\">中文wiki数据集</a>。因为是自回归训练，所以不需要使用像翻译模型那种语料对，而是直接使用自然语言文本。单条文本格式如下：</p><pre><code class=\"language-python\">{\"id\": \"66\", \"url\": \"https://zh.wikipedia.org/wiki?curid=66\", \"title\": \"信息学\", \"text\": \"信息学\\n\\n信息学，旧称情报学（外来语），主要是指以信息为研究对象，利用计算机及其程序设计等技术为研究工具来分析问题、解决问题的学问，是以扩展人类的信息功能为主要目标的一门综合性学科。\\n\\n\\n作为一门新型的综合性学科，信息学的理论主要是建立在数学中的离散数学之上的。因为信息学所研究的对象信息本身即是离散体。在某些特定的条件下，信息学与计算机科学是等价的。\\n\"}\n</code></pre><p>不需要ID和URL这些，只保留text字段，所以先进行文本预处理。你可以看一下文件目录。</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/ec/e31809bb37504c2afc08c0e76ee971ec.png?wh=578x640\" alt=\"图片\"></p><p>循环遍历子目录，然后分别读取每个文件里的文本，再从json格式的数据里抽取text字段对应的数据，保存成sentence.txt文件。抽取完成的sentence.txt文件有1.2GB。你可以参考我给出的代码。</p><pre><code class=\"language-python\">import json\nimport os\n\nclass PrepareData():\n    @staticmethod\n    def prepare():\n        root_dir = 'data/wiki_zh'\n        ds = []\n        for dir_path, dir_names, file_names in os.walk(root_dir):\n            for file_name in file_names:\n                file_path = os.path.join(dir_path, file_name)\n                if \".\" in file_path:\n                    continue\n                with open(file_path, 'r') as file:\n                    for line in file:\n                        try:\n                            text = json.loads(line)[\"text\"]\n                            print(text)\n                            ds.append(text)\n                        except json.JSONDecodeError:\n                            print(\"格式不正确\")\n        print(len(ds))\n        with open('data/sentence.txt', 'w') as file:\n            for i in ds:\n                file.write(i + '\\n')\n        return ds\n\ndata_set = PrepareData.prepare()\nfor item in data_set:\n    print(item)\n\n</code></pre><p>数据处理好，我们就可以开始训练了。</p><h2>训练模型</h2><p>模型训练是非常复杂的过程，也是最消耗资源的过程。我们先定义一个数据集处理类，将sentence.txt里的内容逐行读入，使用jieba进行分词，转化成词汇表保存到本地。</p><pre><code class=\"language-python\"># 导入必需的库\nfrom torch.utils.data import Dataset\nimport torch\nimport jieba\nimport json\n\n# 定义TextDataset类，该类继承自PyTorch中的Dataset\nclass TextDataset(Dataset):\n&nbsp; &nbsp; # 初始化函数，filepath为输入文件路径\n&nbsp; &nbsp; def __init__(self, filepath):\n&nbsp; &nbsp; &nbsp; &nbsp; words = []&nbsp; # 创建一个空列表来存储所有单词\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 打开文件并读取每一行\n&nbsp; &nbsp; &nbsp; &nbsp; with open(filepath, 'r') as file:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for line in file:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 使用jieba库进行分词，并去除每行的首尾空白字符\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; words.extend(list(jieba.cut(line.strip())))\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 将所有单词转换为一个集合来去除重复，然后再转回列表形式，形成词汇表\n&nbsp; &nbsp; &nbsp; &nbsp; self.vocab = list(set(words))\n&nbsp; &nbsp; &nbsp; &nbsp; self.vocab_size = len(self.vocab)&nbsp; # 计算词汇表的大小\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 创建从单词到整数的映射和从整数到单词的映射\n&nbsp; &nbsp; &nbsp; &nbsp; self.word_to_int = {word: i for i, word in enumerate(self.vocab)}\n&nbsp; &nbsp; &nbsp; &nbsp; self.int_to_word = {i: word for i, word in enumerate(self.vocab)}\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 将映射关系保存为JSON文件\n&nbsp; &nbsp; &nbsp; &nbsp; with open('data/word_to_int.json', 'w') as f:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; json.dump(self.word_to_int, f, ensure_ascii=False, indent=4)\n&nbsp; &nbsp; &nbsp; &nbsp; with open('data/int_to_word.json', 'w') as f:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; json.dump(self.int_to_word, f, ensure_ascii=False, indent=4)\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 将所有单词转换为对应的整数索引，形成数据列表\n&nbsp; &nbsp; &nbsp; &nbsp; self.data = [self.word_to_int[word] for word in words]\n\n&nbsp; &nbsp; # 返回数据集的长度减1，这通常是因为在机器学习中可能需要使用当前数据点预测下一个数据点\n&nbsp; &nbsp; def __len__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; return len(self.data) - 1\n\n&nbsp; &nbsp; # 根据索引idx返回数据，这里用于返回模型训练时的输入序列和目标输出\n&nbsp; &nbsp; def __getitem__(self, idx):\n&nbsp; &nbsp; &nbsp; &nbsp; # 从数据中提取最多50个整数索引作为输入序列\n&nbsp; &nbsp; &nbsp; &nbsp; input_seq = torch.tensor(self.data[max(0, idx - 50):idx], dtype=torch.long)\n&nbsp; &nbsp; &nbsp; &nbsp; # 提取目标输出，即索引位置的单词\n&nbsp; &nbsp; &nbsp; &nbsp; target = torch.tensor(self.data[idx], dtype=torch.long)\n&nbsp; &nbsp; &nbsp; &nbsp; return input_seq, target&nbsp; # 返回一个元组包含输入序列和目标输出\n\n</code></pre><p>通过下面的代码加载数据集，并处理成DataLoader，DataLoader可以理解成数据迭代器，可以便利地进行数据加载、批次划分和数据打乱等操作。</p><pre><code class=\"language-python\"># 加载数据集\ndataset = TextDataset('data/sentence.txt')\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n\n</code></pre><p>初始化模型：</p><pre><code class=\"language-python\"># 初始化TransformerDecoderModel，设置特定的参数：\n# vocab_size - 数据集中的词汇表大小\n# embed_size - 嵌入层的维度（这里是512）\n# num_heads - 多头注意力机制中的注意力头数（这里是8）\n# hidden_dim - 变换器中前馈网络模型的维度（这里是2048）\n# num_layers - 模型中的层数（这里是6）\nmodel = TransformerDecoderModel(vocab_size=dataset.vocab_size, embed_size=512, num_heads=8, hidden_dim=2048, num_layers=6)\n\n# 将模型传送到定义的设备上（例如GPU或CPU），以便进行训练\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# 初始化优化器，这里使用Adam优化器，并设置学习率\n# model.parameters() - 从模型中获取参数\n# lr - 学习率（这里用变量learning_rate表示）\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# 初始化损失函数，这里使用交叉熵损失，适用于分类问题\ncriterion = nn.CrossEntropyLoss()\n\n</code></pre><p>前面讲的计算参数规模的几个变量，基本都在这里了，我们再来看看参数量计算公式：<strong>总参数量N</strong> = vocab_size * embed_size + embed_size +（4 * embed_size * embed_size + 2 * embed_size * hidden_dim）* 层数 + vocab_size * embed_size</p><p>代入变量：N = vocab_size * 512 + 512 +（4 * 512 * 512+2 * 512 * 2048）* 6 + vocab_size * 512，假设vocab_size = 100000（100000其实非常小，几个MB的训练数据，词汇量基本就能达到100000了），那么参数量N = 100000 * 512 + 512 +（4 * 512 * 512 + 2 * 512 * 2048）* 6+100000 * 512 = 121,274,880，这已经是1.2亿规模的参数了。GPT-3的训练数据大约570GB，Transformer层数为96。你可以想象一下，1750亿参数是怎么来的。按照我们这个计算方式，不止1750亿了，所以我猜测embed_size没有512，就是说一个单词不一定需要那么多维度去描述。</p><p>接下来我们开始训练。</p><pre><code class=\"language-python\"># 将模型设置为训练模式\nmodel.train()\n\n# 循环遍历所有的训练周期\nfor epoch in range(num_epochs):\n&nbsp; &nbsp; # 循环遍历数据加载器中的每个批次\n&nbsp; &nbsp; for i, (inputs, targets) in enumerate(dataloader):\n&nbsp; &nbsp; &nbsp; &nbsp; # 将输入数据转置，以符合模型的期望输入维度\n&nbsp; &nbsp; &nbsp; &nbsp; inputs = inputs.t()&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; # 在每次迭代前清空梯度\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; # 前向传播：计算模型对当前批次的输出\n&nbsp; &nbsp; &nbsp; &nbsp; outputs = model(inputs)\n&nbsp; &nbsp; &nbsp; &nbsp; # 选择输出的最后一个元素进行损失计算\n&nbsp; &nbsp; &nbsp; &nbsp; outputs = outputs[-1]&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; # 计算损失值\n&nbsp; &nbsp; &nbsp; &nbsp; loss = criterion(outputs, targets)\n&nbsp; &nbsp; &nbsp; &nbsp; # 反向传播：计算损失的梯度\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; # 更新模型的参数\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n&nbsp; &nbsp; &nbsp; &nbsp; # 每隔50步打印一次当前的训练状态\n&nbsp; &nbsp; &nbsp; &nbsp; if i % 50 == 0:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item()}')\n\n# 保存模型到指定路径\ntorch.save(model, model_path)\nprint('模型已保存到', model_path)\n\n</code></pre><p>训练过程太过于消耗资源，我的本地用的是A10-24G显卡，按照上面这个参数设置，准备了500M的训练文本，预估大概需要1个月时间才能跑完，所以为了快速出模型，我只截取了其中一部分训练数据，大概10M，用了半天时间进行训练。</p><h2>模型测试</h2><p>接下来加载训练好的模型，然后对输入语句进行分词等等，你可以参考下面的代码：</p><pre><code class=\"language-python\"># 导入所需库\nimport torch\nimport json\nimport jieba\n\ndef load_model(model_path):\n&nbsp; &nbsp; # 加载模型到CPU\n&nbsp; &nbsp; model = torch.load(model_path, map_location=torch.device('cpu'))\n&nbsp; &nbsp; # 设置为评估模式\n&nbsp; &nbsp; model.eval()\n&nbsp; &nbsp; return model\n\ndef load_vocab(json_file):\n&nbsp; &nbsp; \"\"\"从JSON文件中加载词汇表。\"\"\"\n&nbsp; &nbsp; # 读取词汇表文件\n&nbsp; &nbsp; with open(json_file, 'r') as f:\n&nbsp; &nbsp; &nbsp; &nbsp; vocab = json.load(f)\n&nbsp; &nbsp; return vocab\n\ndef predict(model, initial_seq, max_len=50):\n&nbsp; &nbsp; # 加载数字到单词的映射\n&nbsp; &nbsp; int_to_word = load_vocab('int_to_word.json')\n&nbsp; &nbsp; # 确保模型处于评估模式\n&nbsp; &nbsp; model.eval()\n&nbsp; &nbsp; # 关闭梯度计算\n&nbsp; &nbsp; with torch.no_grad():\n&nbsp; &nbsp; &nbsp; &nbsp; generated = initial_seq\n&nbsp; &nbsp; &nbsp; &nbsp; # 生成最多max_len个词\n&nbsp; &nbsp; &nbsp; &nbsp; for _ in range(max_len):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; input_tensor = torch.tensor([generated], dtype=torch.long)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; output = model(input_tensor)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; predicted_idx = torch.argmax(output[:, -1], dim=-1).item()\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; generated.append(predicted_idx)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 如果生成结束标记，则停止生成\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if predicted_idx == len(int_to_word) - 1:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break\n&nbsp; &nbsp; &nbsp; &nbsp; # 将生成的索引转换为单词\n&nbsp; &nbsp; &nbsp; &nbsp; return [int_to_word[str(idx)] for idx in generated]\n\ndef generate(model, input_sentence, max_len=50):\n&nbsp; &nbsp; # 使用结巴分词对输入句子进行分词\n&nbsp; &nbsp; input_words = list(jieba.cut(input_sentence.strip()))\n&nbsp; &nbsp; # 加载单词到数字的映射\n&nbsp; &nbsp; word_to_int = load_vocab('word_to_int.json')\n&nbsp; &nbsp; # 将单词转换为索引\n&nbsp; &nbsp; input_seq = [word_to_int.get(word, len(word_to_int) - 1) for word in input_words]\n&nbsp; &nbsp; # 生成文本\n&nbsp; &nbsp; generated_text = predict(model, input_seq, max_len)\n&nbsp; &nbsp; # 将生成的单词列表合并为字符串\n&nbsp; &nbsp; return \"\".join(generated_text)\n\ndef main():\n&nbsp; &nbsp; # 定义输入提示\n&nbsp; &nbsp; prompt = \"hello\"\n&nbsp; &nbsp; # 加载模型\n&nbsp; &nbsp; model = load_model('transformer_model.pth')\n&nbsp; &nbsp; # 生成文本\n&nbsp; &nbsp; completion = generate(model, prompt)\n&nbsp; &nbsp; # 打印生成的文本\n&nbsp; &nbsp; print(\"生成文本：\", completion)\n\nif __name__ == '__main__':\n&nbsp; &nbsp; # 主函数入口\n&nbsp; &nbsp; main()\n\n</code></pre><p>得到结果：</p><pre><code class=\"language-python\">生成文本： 珍珠港\n</code></pre><p>简直不忍直视，因为训练有限，所以得到这个结果在意料之中，不过基本流程都跑通了。想要模型性能好，是要一步一步调的，这也是我们说前提要有大量的计算资源的原因。</p><h2>小结</h2><p>这节课我带你手敲Transformer模型，总共加起来不到300行代码，实际上如果你阅读过GPT-2和BERT的模型构建代码，你会发现它们也没有多少行，所以模型构建本身其实并不复杂，模型的构建过程就是整个深度神经网络的架构过程，虽然有一点难度，但是没有想象的那么难，而我认为难点在于预训练过程，既吃训练资源又需要考虑训练效果，如何调整参数让训练效果更好是难点，这和传统CV小模型有点类似。</p><p>你也可以参考下面这个说明：</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/be/5a92e0bc261890b86d8d09c62e16b8be.png?wh=2010x724\" alt=\"图片\"></p><p>这是 <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 论文</a>里给出的不同参数规模下的设置，你可以根据自己的资源情况，选择适合自己的训练数据集以及设置合适的参数，比如embed_size，可以调小一点，隐藏层深度hidden_dim也可以适当调小一点，训练一个模型实际跑一遍，看看效果。</p><h2>思考题</h2><p>在这节课开头，我们提到过Scaling Law中的一个计算公式 $C\\approx 6ND$，你可以思考一下，按照我们上面提到的参数量（1.2亿）和训练数据集大小（1.2GB），算一下训练过程需要多少算力资源，如果换算成A100-80G显卡，需要多少张？欢迎你在评论区留下你的计算结果，如果你觉得这节课的内容对你有帮助的话，也欢迎你把这节课的内容分享给其他朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"15｜Transformer技术原理：为什么说Transformer是大模型架构的基石？（下）","id":787621},"right":{"article_title":"17｜模型解剖：探究模型内部到底是什么？","id":790282}},"comments":[{"had_liked":false,"id":392144,"user_name":"newCheng","can_delete":false,"product_type":"c1","uid":2438860,"ip_address":"北京","ucode":"E6E2819AFC6B29","user_header":"https://static001.geekbang.org/account/avatar/00/25/36/cc/f3bc7bbf.jpg","comment_is_top":false,"comment_ctime":1720077159,"is_pvip":false,"replies":[{"id":142554,"content":"自己配置的话，可以考虑3090显卡，24G显存，京东上二手的7000~8000左右，配一个好一点的电源就好。云平台的话推荐autodl，3090-24G，一个月800多一点，也比较划算","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1720144022,"ip_address":"浙江","comment_id":392144,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师，帮忙推荐一台深度学习的电脑配置呗，或者有没有合适的云平台？","like_count":6,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647525,"discussion_content":"自己配置的话，可以考虑3090显卡，24G显存，京东上二手的7000~8000左右，配一个好一点的电源就好。云平台的话推荐autodl，3090-24G，一个月800多一点，也比较划算","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720144022,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393989,"user_name":"石云升","can_delete":false,"product_type":"c1","uid":1024195,"ip_address":"广东","ucode":"78F1DD33EFD000","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg","comment_is_top":false,"comment_ctime":1725502106,"is_pvip":false,"replies":[{"id":143028,"content":"赞！","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1725529800,"ip_address":"浙江","comment_id":393989,"utype":1}],"discussion_count":1,"race_medal":1,"score":2,"product_id":100770601,"comment_content":"C ≈ 8.64 * 10^17 FLOPs\nA100-80G的理论峰值性能是312 TFLOPS，即每秒可以进行：\n312 * 10^12 = 3.12 * 10^14 FLOPs&#47;s\n理论上最快完成时间：\n时间 = 总FLOPs &#47; 每秒FLOPs\n= (8.64 * 10^17) &#47; (3.12 * 10^14)\n≈ 2,769 秒\n≈ 46 分钟\n\n所以，理论上使用一张A100-80G显卡，最快约46分钟就能完成训练。","like_count":3,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650737,"discussion_content":"赞！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725529801,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392103,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1719974687,"is_pvip":false,"replies":[{"id":142549,"content":"没错，加油！","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1720074427,"ip_address":"浙江","comment_id":392103,"utype":1}],"discussion_count":1,"race_medal":2,"score":2,"product_id":100770601,"comment_content":"第16讲打卡~\n这里简单补充一下GPT和Bert这两种模型的差异：GPT是Decoder-only架构，并且采用单向注意力机制，这意味着在生成文本时，它只考虑前面的上下文信息；而Bert是Encoder-only架构，并且采用双向注意力机制，也就是可以同时考虑上文和下文的信息。这两种结构的差别，也就决定了GPT和Bert有各自擅长的应用场景：GPT更擅长文本生成，也就是“续写”，即根据上文生成下文；而Bert可以被应用于更广泛的NLP任务中，如文本分类、情感分析、命名实体识别等。","like_count":3,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647468,"discussion_content":"没错，加油！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720074427,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392847,"user_name":"张 ·万岁！","can_delete":false,"product_type":"c1","uid":3923442,"ip_address":"北京","ucode":"C95598C751FCC1","user_header":"https://static001.geekbang.org/account/avatar/00/3b/dd/f2/3513c633.jpg","comment_is_top":false,"comment_ctime":1721922977,"is_pvip":false,"replies":[{"id":142734,"content":"没问题的，总算量C=2.88*10^17，你如果用14769张a100，那就是1秒钟搞定，如果用147张A100，那就100秒","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1722309080,"ip_address":"浙江","comment_id":392847,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"参数量1.2亿，要训练的词元数\n理论上，如果光看模型的内存，一张a100 80gb完全可以容纳得下来，因为120,000,000×4÷1,024÷1,024=457.763671875MB，绰绰有余。\n\n算力上来看的话，我没太懂这个问法。按照我的理解来算，12gb的训练文本一共大概800万条样本，每个样本50个词元，词元数D=4亿。再算上参数量1.2亿。那么总的计算量就是C=2.88*10^17。  英伟达的a100 80gb的32位浮点运算的峰值是19.5tflops，也就说1.95*10^13。\n那么有(2.88×10^(17))÷(1.95×10^(13))=14769.23\n也就说等于14769张a100的算力？\n总感觉我算的一点也不对。。。","like_count":1,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648852,"discussion_content":"没问题的，总算量C=2.88*10^17，你如果用14769张a100，那就是1秒钟搞定，如果用147张A100，那就100秒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1722309080,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392500,"user_name":"就在这儿💋","can_delete":false,"product_type":"c1","uid":1119877,"ip_address":"广东","ucode":"36644369AB020E","user_header":"https://static001.geekbang.org/account/avatar/00/11/16/85/b3af5d54.jpg","comment_is_top":false,"comment_ctime":1721112724,"is_pvip":false,"replies":[{"id":142693,"content":"直接用就可以，DataLoader是torch里的类，你想看源码的话直接去看torch的源码就行","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1721389058,"ip_address":"浙江","comment_id":392500,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师,有没有完整的代码文件呀,课程里的示例代码缺了一些内容, DataLoader的 collect_fn是没有的","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648338,"discussion_content":"直接用就可以，DataLoader是torch里的类，你想看源码的话直接去看torch的源码就行","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1721389058,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1008257,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/62/81/ad80f427.jpg","nickname":"Lane","note":"","ucode":"F70459D1BBD9F4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":654967,"discussion_content":"+1 我也跑不通\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1733897225,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392422,"user_name":"大宽","can_delete":false,"product_type":"c1","uid":1071011,"ip_address":"北京","ucode":"12970A21E939EA","user_header":"https://static001.geekbang.org/account/avatar/00/10/57/a3/4bcf1b59.jpg","comment_is_top":false,"comment_ctime":1720876981,"is_pvip":false,"replies":[{"id":142690,"content":"咱们课程里都有了呀，框架有好多种，pytorch，TensorFlow，百度飞桨等等","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1721381928,"ip_address":"浙江","comment_id":392422,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师 大模型训练都用哪些框架呢，pytorch 吗，还有其他工具不 ，或者咱国内的一些大模型都用什么框架训练的呢。老师可否整体介绍一下，一个模型从训练到推理 做成 api 需要的步骤及其对应的技术工具哈？","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648332,"discussion_content":"咱们课程里都有了呀，框架有好多种，pytorch，TensorFlow，百度飞桨等等","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1721381928,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392263,"user_name":"Lee","can_delete":false,"product_type":"c1","uid":1060230,"ip_address":"福建","ucode":"357C3D2F33B325","user_header":"https://static001.geekbang.org/account/avatar/00/10/2d/86/07a10be2.jpg","comment_is_top":false,"comment_ctime":1720441295,"is_pvip":false,"replies":[{"id":142595,"content":"模型训练时间依赖几个因素，包括数据集的大小、模型的复杂度、计算资源的性能等，没有通用的公式可以精确计算训练时间，你可以自己根据每一个epoch时间来评估整体需要的时间。","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1720604731,"ip_address":"浙江","comment_id":392263,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师  能说说训练时间怎么预估吗","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647786,"discussion_content":"模型训练时间依赖几个因素，包括数据集的大小、模型的复杂度、计算资源的性能等，没有通用的公式可以精确计算训练时间，你可以自己根据每一个epoch时间来评估整体需要的时间。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720604731,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392093,"user_name":"翔","can_delete":false,"product_type":"c1","uid":1952130,"ip_address":"上海","ucode":"89E97976EC2DF3","user_header":"","comment_is_top":false,"comment_ctime":1719931205,"is_pvip":false,"replies":[{"id":142532,"content":"有的，可能大家还没到这节课","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1719973739,"ip_address":"浙江","comment_id":392093,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"课程到这里，没人互动了，是不是都掉队了😟","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647386,"discussion_content":"有的，可能大家还没到这节课","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719973739,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392092,"user_name":"翔","can_delete":false,"product_type":"c1","uid":1952130,"ip_address":"上海","ucode":"89E97976EC2DF3","user_header":"","comment_is_top":false,"comment_ctime":1719931159,"is_pvip":false,"replies":[{"id":142526,"content":"CPU比较慢，你可以试一下，把模型网络定义的简单一点，训练数据少一点，CPU可能也可以","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1719973274,"ip_address":"浙江","comment_id":392092,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"训练模型必须要用显卡吗，跑一丢丢测试数据，用 cpu 不行吗","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647380,"discussion_content":"CPU比较慢，你可以试一下，把模型网络定义的简单一点，训练数据少一点，CPU可能也可以","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719973274,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":396634,"user_name":"Geek_f80ce5","can_delete":false,"product_type":"c1","uid":2360052,"ip_address":"北京","ucode":"8A6FE741BE5F07","user_header":"","comment_is_top":false,"comment_ctime":1735172327,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"这个源代码有吗？","like_count":0},{"had_liked":false,"id":396267,"user_name":"Lane","can_delete":false,"product_type":"c1","uid":1008257,"ip_address":"北京","ucode":"F70459D1BBD9F4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/62/81/ad80f427.jpg","comment_is_top":false,"comment_ctime":1733816028,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100770601,"comment_content":"collate_fn应该怎么实现呢","like_count":0},{"had_liked":false,"id":396266,"user_name":"Lane","can_delete":false,"product_type":"c1","uid":1008257,"ip_address":"北京","ucode":"F70459D1BBD9F4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/62/81/ad80f427.jpg","comment_is_top":false,"comment_ctime":1733813930,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100770601,"comment_content":"代码缺失的地方呢","like_count":0},{"had_liked":false,"id":396200,"user_name":"Geek_4d9162","can_delete":false,"product_type":"c1","uid":4039257,"ip_address":"河北","ucode":"9415AF51BF6371","user_header":"","comment_is_top":false,"comment_ctime":1733539305,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100770601,"comment_content":"老师  求助，中文 wiki 数据集下载页面打不开， 能否上传一个阿里云盘或者腾讯云盘的，百度网盘下载太慢了。","like_count":0}]}