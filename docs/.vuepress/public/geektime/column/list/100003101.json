[{"article_id":1291,"article_title":"开篇词 | 人工智能：新时代的必修课","article_content":"<p>你好，我是王天一，于北京邮电大学获得工学博士学位，现任贵州大学大数据与信息工程学院副教授，著有《人工智能革命》一书。在未来的几个月中，我将通过“人工智能基础课”这个专栏和你分享人工智能的基础知识，以帮助你更好地理解人工智能的内涵。</p>\n<p>2016年，AlphaGo的横空出世掀起了人工智能的新一轮热潮。在经历了近三十年的沉寂后，人工智能终于迎来了新的春天。自此，人工智能不仅以如火如荼之势赚足了政策的关注、资本的涌入、以及吃瓜群众的眼球，其技术进展更是以令人瞠目结舌的速度狂飙突进，悄无声息地改造着普通人的生活。</p>\n<p>人工智能是一把双刃剑，它既能推动经济社会的进步，也能潜移默化地改造人类，这种改造的威力甚至远胜于基因武器。关于人工智能对人类自身影响的论述虽然屡见不鲜，绝大多数却都不得要领。众多专家鼓吹的“超人工智能毁灭人类”的末世论调即使不是胡说八道也称得上危言耸听。无人超市和无人工厂的出现都在表明：人工智能真正的威胁在于使绝大多数人沦为机器的附庸。人工智能本质上是一种劳动工具，但当劳动工具本身已经强大到反客为主时，作为劳动者的人类便成了多余的角色，有降格为“亚人工智能”的风险。</p>\n<p>如何应对来势汹汹的人工智能？一个办法是专精于依赖创造力的领域，比如科学和艺术，但这对天赋的要求较高，显然并不适用于每一个人（更何况人工智能很可能重塑人类对于科学和艺术的品味）；另一种门槛更低的办法就是掌握核心技术，让人工智能回归“为我所用”的工具性，正如伟大的军事家孙武所言：“知己知彼，百战不殆”。</p>\n<p>这就是这个专栏诞生的原因：<strong>普及人工智能的基础知识，走出了解人工智能的第一步</strong>。人工智能复杂，但并不神秘。它建立在以线性代数和概率论为骨架的基础数学上，通过简单模型的组合实现复杂功能。在工程上，深度神经网络通常以其恒河沙数般的参数让人望而却步；可在理论上，其数学原理却具有更好的可解释性。从事年薪百万的高端研究固然需要非凡的头脑，但理解人工智能的基本原理绝非普通人遥不可及的梦想。</p>\n<p>人工智能的早期发展遵循的是符号主义学派的发展路径，但狭窄的应用领域让它在短暂的辉煌之后迅速走向沉寂。吸取了符号主义学派的教训，连接主义学派通过以工程技术手段模拟人脑神经系统的结构和功能来模拟人类智能。这种思路在一定程度上实现了人脑形象思维的功能，也成为今天人工智能的核心技术。</p>\n<p>正因如此，这个专栏将围绕机器学习与神经网络等核心概念展开，并结合当下火热的深度学习技术，勾勒出人工智能发展的基本轮廓与主要路径。出于可读性的考虑，我在专栏里不会使用大量复杂的数学公式，而是力图以通俗的语言解释清楚公式背后的道理，起到入门的作用。专栏的内容将分为以下几个模块：</p>\n<ol>\n<li><p><strong>数学基础</strong>。数学基础知识蕴含着处理智能问题的基本思想与方法，也是理解复杂算法的必备要素。这一模块我将介绍研究人工智能必备的数学基础知识，包括线性代数、概率论、最优化方法等。</p>\n</li>\n<li><p><strong>机器学习</strong>。机器学习的作用是从数据中习得学习算法，进而解决实际的应用问题，是人工智能的核心目标之一。这一模块我将介绍机器学习中的主要方法，包括线性回归、决策树、支持向量机、聚类等。</p>\n</li>\n<li><p><strong>人工神经网络</strong>。作为机器学习的一个分支，神经网络将认知科学引入机器学习当中，以模拟生物神经系统对真实世界的交互反映，并取得了良好的效果。这一模块我将介绍神经网络中的基本概念，包括多层神经网络、前馈与反向传播、自组织神经网络等。</p>\n</li>\n<li><p><strong>深度学习</strong>。简而言之，深度学习就是包含多个中间层的神经网络，数据的爆炸和计算力的飙升推动了深度学习的崛起。这一模块我将介绍深度学习的概念与实现，包括深度前馈网络、深度学习中的正则化、自动编码器等。</p>\n</li>\n<li><p><strong>神经网络实例</strong>。在深度学习框架下，一些神经网络已用于各种应用场景，并取得了不俗的效果。这一模块我将介绍几种神经网络实例，包括卷积神经网络、递归神经网络、深度信念网络等。</p>\n</li>\n<li><p><strong>深度学习之外的人工智能</strong>。深度学习既有优点也有局限，其他方向的人工智能研究正是有益的补充。这一模块我将介绍几种与深度学习无关的学习方法，包括马尔可夫随机场、迁移学习、集群智能等。</p>\n</li>\n<li><p><strong>应用实例</strong>。除了代替人类执行重复性的劳动之外，在诸多实际问题的处理中，人工智能也提供了有意义的尝试。这一模块我将介绍人工智能技术在几类实际任务中的应用，包括计算机视觉、语音识别、对话系统等。</p>\n</li>\n</ol>\n<p>世纪之交时曾流行一种说法：不懂计算机、外语和驾驶技术的人将成为21世纪的文盲。而在不久的将来，人工智能很可能成为检验文盲的新标尺。<strong>希望你能全面掌握这个专栏的内容，并以此为基础进行更加系统和深入的学习</strong>。也祝愿未来的你能够掌控人工智能，而不是被人工智能所掌控。</p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":[],"right":{"article_title":"01 数学基础 | 九层之台，起于累土：线性代数","id":1340}}},{"article_id":1340,"article_title":"01 数学基础 | 九层之台，起于累土：线性代数","article_content":"<p>“人工智能基础课”将从数学基础开始。必备的数学知识是理解人工智能不可或缺的要素，今天的种种人工智能技术归根到底都建立在数学模型之上，而这些数学模型又都离不开线性代数（linear algebra）的理论框架。</p>\n<p>事实上，线性代数不仅仅是人工智能的基础，更是现代数学和以现代数学作为主要分析方法的众多学科的基础。从量子力学到图像处理都离不开向量和矩阵的使用。而在向量和矩阵背后，线性代数的核心意义在于提供了⼀种看待世界的抽象视角：<strong>万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察</strong>。</p>\n<p>线性代数中最基本的概念是集合（set）。在数学上，集合的定义是由某些特定对象汇总而成的集体。集合中的元素通常会具有某些共性，因而可以用这些共性来表示。对于集合 { 苹果，橘子，梨 } 来说， 所有元素的共性是它们都是水果；对于集合 {牛，马，羊} 来说，所有元素的共性是它们都是动物。当然 { 苹果，牛 } 也可以构成一个集合，但这两个元素并没有明显的共性，这样的集合在解决实际问题中的作用也就相当有限。</p>\n<p>“苹果”或是“牛”这样的具体概念显然超出了数学的处理范围，因而集合的元素需要进行进一步的抽象——用数字或符号来表示。如此一来，集合的元素既可以是单个的数字或符号，也可以是多个数字或符号以某种方式排列形成的组合。</p>\n<p>在线性代数中，由单独的数a构成的元素被称为标量（scalar）：一个标量a可以是整数、实数或复数。如果多个标量${ a_1, a_2, \\cdots, a_n}$ 按一定顺序组成一个序列，这样的元素就被称为向量（vector）。显然，向量可以看作标量的扩展。原始的一个数被替代为一组数，从而带来了维度的增加，给定表示索引的下标才能唯一地确定向量中的元素。</p>\n<p>每个向量都由若干标量构成，如果将向量的所有标量都替换成相同规格的向量，得到的就是如下的矩阵（matrix）:</p>\n<p>$$\\left( {\\begin{array}{cc}<br />\n{{a_{11}}}&amp;{{a_{12}}}&amp;{{a_{13}}}\\cr<br />\n{{a_{21}}}&amp;{{a_{22}}}&amp;{{a_{23}}}\\cr<br />\n{{a_{31}}}&amp;{{a_{32}}}&amp;{{a_{33}}}<br />\n\\end{array}} \\right)$$</p>\n<p>相对于向量，矩阵同样代表了维度的增加，矩阵中的每个元素需要使用两个索引（而非一个）确定。同理，如果将矩阵中的每个标量元素再替换为向量的话，得到的就是张量（tensor）。直观地理解，张量就是高阶的矩阵。</p>\n<p>如果把三阶魔方的每一个小方块看作一个数，它就是个3×3×3的张量，3×3的矩阵则恰是这个魔方的一个面，也就是张量的一个切片。相比于向量和矩阵，张量是更加复杂，直观性也更差的概念。</p>\n<p>向量和矩阵不只是理论上的分析工具，也是计算机工作的基础条件。人类能够感知连续变化的大千世界，可计算机只能处理离散取值的二进制信息，因而来自模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。从这个角度看，<strong>线性代数是用虚拟数字世界表示真实物理世界的工具</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>在计算机存储中，标量占据的是零维数组；向量占据的是一维数组，例如语音信号；矩阵占据的是二维数组，例如灰度图像；张量占据的是三维乃至更高维度的数组，例如RGB图像和视频。</p>\n<p>描述作为数学对象的向量需要有特定的数学语言，范数和内积就是代表。范数（norm）是对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值。通用的$L ^ p$范数定义如下：</p>\n<p>$${\\left| {\\bf{x}} \\right|_p} = {\\left( {\\sum\\limits_i {{{\\left| {{x_i}} \\right|}^p}} } \\right)^{\\frac{1}{p}}}$$</p>\n<p>对⼀个给定向量，$L ^ 1$范数计算的是向量所有元素绝对值的和，$L ^ 2$范数计算的是通常意义上的向量长度，$L ^ {\\infty}$范数计算的则是向量中最大元素的取值。</p>\n<p>范数计算的是单个向量的尺度，内积（inner product）计算的则是两个向量之间的关系。两个相同维数向量内积的表达式为</p>\n<p>$$\\left\\langle {{\\bf{x,y}}} \\right\\rangle  = \\sum\\limits_i {{x_i} \\cdot {y_i}} $$</p>\n<p>即对应元素乘积的求和。内积能够表示两个向量之间的相对位置，即向量之间的夹角。一种特殊的情况是内积为0，即$\\left\\langle \\bf{x,y} \\right\\rangle = 0$。在二维空间上，这意味着两个向量的夹角为90度，即相互垂直。而在高维空间上，这种关系被称为正交（orthogonality）。如果两个向量正交，说明他们线性无关，相互独立，互不影响。</p>\n<p>在实际问题中，向量的意义不仅是某些数字的组合，更可能是某些对象或某些行为的特征。范数和内积能够处理这些表示特征的数学模型，进而提取出原始对象或原始行为中的隐含关系。</p>\n<p>如果有一个集合，它的元素都是具有相同维数的向量（可以是有限个或无限个）， 并且定义了加法和数乘等结构化的运算，这样的集合就被称为线性空间（linear space），定义了内积运算的线性空间则被称为内积空间（inner product space）。<strong>在线性空间中，任意一个向量代表的都是n维空间中的一个点；反过来， 空间中的任意点也都可以唯一地用一个向量表示</strong>。两者相互等效。</p>\n<p>在线性空间上点和向量的相互映射中，一个关键问题是参考系的选取。在现实生活中，只要给定经度、纬度和海拔高度，就可以唯一地确定地球上的任何一个位置，因而经度值、纬度值、高度值构成的三维向量(x, y, h)就对应了三维物理空间中的⼀个点。</p>\n<p>可是在直觉无法感受的高维空间中，坐标系的定义可就没有这么直观了。要知道，人工神经网络要处理的通常是数以万计的特征，对应着维度同样数以万计的复杂空间，这时就需要正交基的概念了。</p>\n<p>在内积空间中，一组两两正交的向量构成这个空间的正交基（orthogonal basis），假若正交基中基向量的$L ^ 2$范数都是单位长度1，这组正交基就是标准正交基（orthonormal basis）。正交基的作用就是给内积空间定义出经纬度。⼀旦描述内积空间的正交基确定了，向量和点之间的对应关系也就随之确定。</p>\n<p>值得注意的是，描述内积空间的正交基并不唯一。对二维空间来说，平面直角坐标系和极坐标系就对应了两组不同的正交基，也代表了两种实用的描述方式。</p>\n<p>线性空间的一个重要特征是能够承载变化。当作为参考系的标准正交基确定后，空间中的点就可以用向量表示。当这个点从一个位置移动到另一个位置时，描述它的向量也会发生改变。<strong>点的变化对应着向量的线性变换（linear transformation），而描述对象变化抑或向量变换的数学语言，正是矩阵</strong>。</p>\n<p>在线性空间中，变化的实现有两种方式：一是点本身的变化，二是参考系的变化。在第一种方式中，使某个点发生变化的方法是用代表变化的矩阵乘以代表对象的向量。可是反过来，如果保持点不变，而是换一种观察的角度，得到的也将是不同的结果，正所谓“横看成岭侧成峰，远近高低各不同”。</p>\n<p>在这种情况下，矩阵的作用就是对正交基进行变换。因此，对于矩阵和向量的相乘，就存在不同的解读方式：</p>\n<p>$${\\bf{Ax = y}}$$</p>\n<p>这个表达式既可以理解为向量x经过矩阵A所描述的变换，变成了向量y；也可以理解为一个对象在坐标系A的度量下得到的结果为向量x，在标准坐标系 I（单位矩阵：主对角线元素为1，其余元素为0）的度量下得到的结果为向量y。</p>\n<p>这表示矩阵不仅能够描述变化，也可以描述参考系本身。引用网络上一个精当的类比：表达式Ax就相当于对向量x做了一个环境声明，用于度量它的参考系是A。如果想用其他的参考系做度量的话，就要重新声明。<strong>而对坐标系施加变换的方法，就是让表示原始坐标系的矩阵与表示变换的矩阵相乘</strong>。</p>\n<p>描述矩阵的⼀对重要参数是特征值（eigenvalue）和特征向量（eigenvector）。对于给定的矩阵A，假设其特征值为λ，特征向量为x，则它们之间的关系如下：</p>\n<p>$${\\bf{Ax}} = \\lambda {\\bf{x}}$$</p>\n<p>正如前文所述，矩阵代表了向量的变换，其效果通常是对原始向量同时施加方向变化和尺度变化。可对于有些特殊的向量，矩阵的作用只有尺度变化而没有方向变化，也就是只有伸缩的效果而没有旋转的效果。对于给定的矩阵来说，这类特殊的向量就是矩阵的特征向量，特征向量的尺度变化系数就是特征值。</p>\n<p><strong>矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向</strong>。如果把矩阵所代表的变化看作奔跑的人，那么矩阵的特征值就代表了他奔跑的速度，特征向量代表了他奔跑的方向。但矩阵可不是普通人，它是三头六臂的哪吒，他的不同分身以不同速度（特征值）在不同方向（特征向量）上奔跑，所有分身的运动叠加在⼀起才是矩阵的效果。</p>\n<p>求解给定矩阵的特征值和特征向量的过程叫做特征值分解，但能够进行特征值分解的矩阵必须是n维方阵。将特征值分解算法推广到所有矩阵之上，就是更加通用的奇异值分解。</p>\n<p>今天我和你分享了人工智能必备的线性代数基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：</p>\n<ul>\n<li>线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态的特性；</li>\n<li>向量的实质是n维线性空间中的静止点；</li>\n<li>线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示；</li>\n<li>矩阵的特征值和特征向量描述了变化的速度与方向。</li>\n</ul>\n<p>线性代数之于人工智能如同加法之于高等数学。这样一个基础的工具集，你能想到它在人工智能中的哪些具体应用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e4/a2/e4111df16317c6c9a400ed9494c2f8a2.jpg\" alt=\"\" /></p>\n<p></p>\n","neighbors":{"left":{"article_title":"开篇词 | 人工智能：新时代的必修课","id":1291},"right":{"article_title":"02 数学基础 | 月有阴晴圆缺，此事古难全：概率论","id":1341}}},{"article_id":1341,"article_title":"02 数学基础 | 月有阴晴圆缺，此事古难全：概率论","article_content":"<p>除了线性代数之外，概率论（probability theory）也是人工智能研究中必备的数学基础。随着连接主义学派的兴起，概率统计已经取代了数理逻辑，成为人工智能研究的主流工具。在数据爆炸式增长和计算力指数化增强的今天，概率论已经在机器学习中扮演了核心角色。</p>\n<p>同线性代数一样，概率论也代表了一种看待世界的方式，<strong>其关注的焦点是无处不在的可能性</strong>。对随机事件发生的可能性进行规范的数学描述就是概率论的公理化过程。概率的公理化结构体现出的是对概率本质的一种认识。</p>\n<p>将同一枚硬币抛掷10次，其正面朝上的次数既可能一次没有，也可能全部都是，换算成频率就分别对应着0%和100%。频率本身显然会随机波动，但随着重复试验的次数不断增加，特定事件出现的频率值就会呈现出稳定性，逐渐趋近于某个常数。</p>\n<p>从事件发生的频率认识概率的方法被称为“频率学派”（frequentist probability），频率学派口中的“概率”，其实是一个可独立重复的随机实验中单个结果出现频率的极限。因为稳定的频率是统计规律性的体现，因而通过大量的独立重复试验计算频率，并用它来表征事件发生的可能性是一种合理的思路。</p>\n<p>在概率的定量计算上，频率学派依赖的基础是古典概率模型。在古典概率模型中，试验的结果只包含有限个基本事件，且每个基本事件发生的可能性相同。如此一来，假设所有基本事件的数目为n，待观察的随机事件A中包含的基本事件数目为k，则古典概率模型下事件概率的计算公式为</p>\n<p> $$P(A) = \\dfrac{k}{n} $$</p>\n<p>从这一基本公式就可以推导出复杂的随机事件的概率。</p>\n<p>前文中的概率定义针对都是单个随机事件，可如果要刻画两个随机事件之间的关系，这个定义就不够看了。在一场足球比赛中，球队1:0取胜和在0:2落后的情况下3:2翻盘的概率显然是不一样的。这就需要引入条件概率的概念。</p>\n<!-- [[[read_end]]] -->\n<p>条件概率（conditional probability）是根据已有信息对样本空间进行调整后得到的新的概率分布。假定有两个随机事件A和B，条件概率就是指事件A在事件B已经发生的条件下发生的概率，用以下公式表示</p>\n<p> $$P(A|B) = \\dfrac{P(AB)}{P(B)} $$</p>\n<p>上式中的$P(AB)$称为联合概率（joint probability），表示的是A和B两个事件共同发生的概率。如果联合概率等于两个事件各自概率的乘积，即$P(AB) = P(A) \\cdot P(B)$，说明这两个事件的发生互不影响，即两者相互独立。对于相互独立的事件，条件概率就是自身的概率，即$P(A|B) = P(A)$。</p>\n<p>基于条件概率可以得出全概率公式（law of total probability）。全概率公式的作用在于将复杂事件的概率求解转化为在不同情况下发生的简单事件的概率求和，即 </p>\n<p>$$P(A) = \\sum_{i = 1}^{N}P(A|B_i)\\cdot P(B_i)$$</p>\n<p>$$\\sum_{i = 1}^{N} P(B_i) = 1$$</p>\n<p>全概率公式代表了频率学派解决概率问题的思路，即先做出一些假设（$P(B_i)$），再在这些假设下讨论随机事件的概率（$P(A|B_i)$）。</p>\n<p>对全概率公式稍作整理，就演化出了求解“逆概率”这一重要问题。<strong>所谓“逆概率”解决的是在事件结果已经确定的条件下（$P(A)$），推断各种假设发生的可能性（$P(B_i|A)$）</strong>。由于这套理论首先由英国牧师托马斯·贝叶斯提出，因而其通用的公式形式被称为贝叶斯公式：</p>\n<p>$$P(B_i|A)=\\frac{P(A|B_i)\\cdot P(B_i)}{\\sum _{j=1}^{N} P(A|B_j)\\cdot P(B_j)}$$</p>\n<p>贝叶斯公式可以进一步抽象为贝叶斯定理（Bayes&#39; theorem）：</p>\n<p>$$P(H|D) = \\dfrac{P(D|H) \\cdot P(H)}{P(D)}$$</p>\n<p>式中的$P(H)$被称为先验概率（prior probability），即预先设定的假设成立的概率；$P(D|H)$被称为似然概率（likelihood function），是在假设成立的前提下观测到结果的概率；$P(H|D)$被称为后验概率（posterior probability），即在观测到结果的前提下假设成立的概率。</p>\n<p>从科学研究的方法论来看，贝叶斯定理提供了一种全新的逻辑。它根据观测结果寻找合理的假设，或者说根据观测数据寻找最佳的理论解释，其关注的焦点在于后验概率。概率论的贝叶斯学派（Bayesian probability）正是诞生于这种理念。</p>\n<p>在贝叶斯学派眼中，概率描述的是随机事件的可信程度。如果手机里的天气预报应用给出明天下雨的概率是85%，这就不能从频率的角度来解释了，而是意味着明天下雨这个事件的可信度是85%。</p>\n<p><strong>频率学派认为假设是客观存在且不会改变的，即存在固定的先验分布，只是作为观察者的我们无从知晓</strong>。因而在计算具体事件的概率时，要先确定概率分布的类型和参数，以此为基础进行概率推演。</p>\n<p>相比之下，<strong>贝叶斯学派则认为固定的先验分布是不存在的，参数本身也是随机数</strong>。换言之，假设本身取决于观察结果，是不确定并且可以修正的。数据的作用就是对假设做出不断的修正，使观察者对概率的主观认识更加接近客观实际。</p>\n<p>概率论是线性代数之外，人工智能的另一个理论基础，多数机器学习模型采用的都是基于概率论的方法。但由于实际任务中可供使用的训练数据有限，因而需要对概率分布的参数进行估计，这也是机器学习的核心任务。</p>\n<p><strong>概率的估计有两种方法：最大似然估计法（maximum likelihood estimation）和最大后验概率法（maximum a posteriori estimation），两者分别体现出频率学派和贝叶斯学派对概率的理解方式</strong>。</p>\n<p>最大似然估计法的思想是使训练数据出现的概率最大化，依此确定概率分布中的未知参数，估计出的概率分布也就最符合训练数据的分布。最大后验概率法的思想则是根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最可能的未知参数取值作为估计值。在估计参数时，最大似然估计法只需要使用训练数据，最大后验概率法除了数据外还需要额外的信息，就是贝叶斯公式中的先验概率。</p>\n<p>从理论的角度来说，频率学派和贝叶斯学派各有千秋，都发挥着不可替代的作用。但具体到人工智能这一应用领域，基于贝叶斯定理的各种方法与人类的认知机制吻合度更高，在机器学习等领域中也扮演着更加重要的角色。</p>\n<p>概率论的一个重要应用是描述随机变量（random variable）。根据取值空间的不同，随机变量可以分成两类：离散型随机变量（discrete random variable）和连续型随机变量（continuous random variable）。在实际应用中，需要对随机变量的每个可能取值的概率进行描述。</p>\n<p>离散变量的每个可能的取值都具有大于0的概率，取值和概率之间一一对应的关系就是离散型随机变量的分布律，也叫<strong>概率质量函数</strong>（probability mass function）。概率质量函数在连续型随机变量上的对应就是概率密度函数（probability density function）。</p>\n<p>需要说明的是，<strong>概率密度函数体现的并非连续型随机变量的真实概率，而是不同取值可能性之间的相对关系</strong>。对连续型随机变量来说，其可能取值的数目为不可列无限个，当归一化的概率被分配到这无限个点上时，每个点的概率都是个无穷小量，取极限的话就等于零。而概率密度函数的作用就是对这些无穷小量加以区分。虽然在$x \\rightarrow \\infty$时，$1 / x$和$2 / x$都是无穷小量，但后者永远是前者的2倍。这类相对意义而非绝对意义上的差别就可以被概率密度函数所刻画。对概率密度函数进行积分，得到的才是连续型随机变量的取值落在某个区间内的概率。</p>\n<p>定义了概率质量函数与概率密度函数后，就可以给出一些重要分布的特性。重要的离散分布包括两点分布、二项分布和泊松分布，重要的连续分布则包括均匀分布、指数分布和正态分布。</p>\n<ul>\n<li><strong>两点分布</strong>（Bernoulli distribution）：适用于随机试验的结果是二进制的情形，事件发生/不发生的概率分别为$p/(1 - p)$。任何只有两个结果的随机试验都可以用两点分布描述，抛掷一次硬币的结果就可以视为等概率的两点分布。</li>\n<li><strong>二项分布</strong>（Binomial distribution）：将满足参数为p的两点分布的随机试验独立重复n次，事件发生的次数即满足参数为(n,p)的二项分布。二项分布的表达式可以写成$P(X = k) = C^n_k \\cdot p ^ k \\cdot (1 - p) ^ {(n - k)}, 0 \\le k \\le n$。</li>\n<li><strong>泊松分布</strong>（Poisson distribution）：放射性物质在规定时间内释放出的粒子数所满足的分布，参数为$\\lambda$的泊松分布表达式为$P(X = k) = \\lambda ^ k \\cdot e ^ {-\\lambda} / (k!)$。当二项分布中的n很大且p很小时，其概率值可以由参数为$\\lambda = np$的泊松分布的概率值近似。</li>\n<li><strong>均匀分布</strong>（uniform distribution）：在区间(a, b)上满足均匀分布的连续型随机变量，其概率密度函数为1 / (b - a)，这个变量落在区间(a, b)内任意等长度的子区间内的可能性是相同的。</li>\n<li><strong>指数分布</strong>（exponential distribution）：满足参数为$\\theta$指数分布的随机变量只能取正值，其概率密度函数为$e ^ {-x / \\theta} / \\theta, x &gt; 0$。指数分布的一个重要特征是无记忆性：即P(X &gt; s + t | X &gt; s) = P(X &gt; t)。</li>\n<li><strong>正态分布</strong>（normal distribution）：参数为正态分布的概率密度函数为</li>\n</ul>\n<p>$$ f(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma} \\cdot e ^ {-\\frac{(x - \\mu) ^ 2}{2\\sigma ^ 2}}$$</p>\n<p>当$\\mu = 0, \\sigma = 1$时，上式称为标准正态分布。正态分布是最常见最重要的一种分布，自然界中的很多现象都近似地服从正态分布。</p>\n<p>除了概率质量函数/概率密度函数之外，另一类描述随机变量的参数是其数字特征。<strong>数字特征是用于刻画随机变量某些特性的常数，包括数学期望（expected value）、方差（variance）和协方差（covariance）</strong>。</p>\n<p>数学期望即均值，体现的是随机变量可能取值的加权平均，即根据每个取值出现的概率描述作为一个整体的随机变量的规律。方差表示的则是随机变量的取值与其数学期望的偏离程度。方差较小意味着随机变量的取值集中在数学期望附近，方差较大则意味着随机变量的取值比较分散。</p>\n<p>数学期望和方差描述的都是单个随机变量的数字特征，如果要描述两个随机变量之间的相互关系，就需要用到协方差和相关系数。协方差度量了两个随机变量之间的线性相关性，即变量Y能否表示成以另一个变量X为自变量的$aX+b$的形式。</p>\n<p>根据协方差可以进一步求出相关系数（correlation coefficient），相关系数是一个绝对值不大于1的常数，它等于1意味着两个随机变量满足完全正相关，等于-1意味着两者满足完全负相关，等于0则意味着两者不相关。需要说明的是，无论是协方差还是相关系数，刻画的都是线性相关的关系。如果随机变量之间的关系满足$Y = X ^ 2$，这样的非线性相关性就超出了协方差的表达能力。</p>\n<p>今天我和你分享了人工智能必备的概率论基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：</p>\n<ul>\n<li>概率论关注的是生活中的不确定性或可能性；</li>\n<li>频率学派认为先验分布是固定的，模型参数要靠最大似然估计计算；</li>\n<li>贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算；</li>\n<li>正态分布是最重要的一种随机变量的分布。</li>\n</ul>\n<p>在今天的机器学习中，大量任务是根据已有的数据预测可能出现的情况，因而贝叶斯定理得到了广泛应用。那么在生活中，你能想到哪些实例，是从已有的观测结果反过来推演假设成立的呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/60/87/60dfa8c61a5847be616f08d18b36a587.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"01 数学基础 | 九层之台，起于累土：线性代数","id":1340},"right":{"article_title":"03 数学基础 | 窥一斑而知全豹：数理统计","id":1498}}},{"article_id":1498,"article_title":"03 数学基础 | 窥一斑而知全豹：数理统计","article_content":"<p>在人工智能的研究中，数理统计同样不可或缺。基础的统计理论有助于对机器学习的算法和数据挖掘的结果做出解释，只有做出合理的解读，数据的价值才能够体现。<strong>数理统计（mathematical statistics）根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断</strong>。</p>\n<p>虽然数理统计以概率论为理论基础，但两者之间存在方法上的本质区别。概率论作用的前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断。</p>\n<p>用一句不严谨但直观的话讲：<strong>数理统计可以看成是逆向的概率论</strong>。用买彩票打个比方，概率论解决的是根据已知的摇奖规律判断一注号码中奖的可能性，数理统计解决的则是根据之前多次中奖/不中奖的号码记录以一定的精确性推测摇奖的规律，虽然这种尝试往往无功而返。</p>\n<p>在数理统计中，可用的资源是有限的数据集合，这个有限数据集被称为<strong>样本</strong>（sample）。相应地，观察对象所有的可能取值被称为<strong>总体</strong>（population）。数理统计的任务就是根据样本推断总体的数字特征。样本通常由对总体进行多次独立的重复观测而得到，这保证了不同的样本值之间相互独立，并且都与总体具有相同的分布。</p>\n<p>在统计推断中，应用的往往不是样本本身，而是被称为统计量的样本的函数。统计量本身是一个随机变量，是用来进行统计推断的工具。<strong>样本均值和样本方差是两个最重要的统计量</strong>：</p>\n<ul>\n<li>样本均值：${\\bar X} = \\dfrac{1}{n} \\sum\\limits_{i = 1}^{n} X_i$</li>\n<li>样本方差：$S ^ 2 = \\dfrac{1}{n - 1} \\sum\\limits_{i = 1}^{n} (X_i - {\\bar X}) ^ 2$</li>\n</ul>\n<p><strong>统计推断的基本问题可以分为两大类：参数估计（estimation theory）和假设检验（hypothesis test）</strong>。</p>\n<h2>参数估计</h2>\n<p>参数估计是通过随机抽取的样本来估计总体分布的方法，又可以进一步划分为<strong>点估计</strong>（point estimation）和<strong>区间估计</strong>（interval estimation）。在已知总体分布函数形式，但未知其一个或者多个参数时，借助于总体的一个样本来估计未知参数的取值就是参数的点估计。点估计的核心在于构造合适的统计量$\\hat \\theta$，并用这个统计量的观察值作为未知参数$\\theta$的近似值。<strong>点估计的具体方法包括矩估计法（method of moments）和最大似然估计法</strong>（maximum likelihood estimation）。</p>\n<!-- [[[read_end]]] -->\n<p>矩表示的是随机变量的分布特征，$k$阶矩的定义为随机变量的$k$次方的均值，即$E(X^k)$。<strong>矩估计法的思想在于用样本的$k$阶矩估计总体的$k$阶矩</strong>，其理论依据在于样本矩的函数几乎处处收敛于总体矩的相应函数，这意味着当样本的容量足够大时，几乎每次都可以根据样本参数得到相应总体参数的近似值。</p>\n<p>相对于基于大数定律的矩估计法，最大似然估计法源于频率学派看待概率的方式。<strong>对最大似然估计的直观理解是：既然抽样得到的是已有的样本值，就可以认为取到这一组样本值的概率较大，因而在估计参数$\\theta$的时候就需要让已有样本值出现的可能性最大</strong>。</p>\n<p>在最大似然估计中，似然函数被定义为样本观测值出现的概率，确定未知参数的准则是让似然函数的取值最大化，也就是微积分中求解函数最大值的问题。由于不同的样本值之间相互独立，因而似然函数可以写成若干概率质量函数/概率密度函数相乘的形式，并进一步转化为对数方程求解。</p>\n<p>矩估计法和最大似然估计法代表了两种推断总体参数的思路，但对于同一个参数，用不同的估计方法求出的估计量很可能存在差异，这就引出了如何对估计量进行评价的问题。在实际应用中，估计量的评价通常要考虑以下三个基本标准。</p>\n<ul>\n<li><strong>无偏性</strong>：估计量的数学期望等于未知参数的真实值；</li>\n<li><strong>有效性</strong>：无偏估计量的方差尽可能小；</li>\n<li><strong>一致性</strong>：当样本容量趋近于无穷时，估计量依概率收敛于未知参数的真实值。</li>\n</ul>\n<p>以上三个要求构成了对点估计量的整体判定标准。无偏性意味着给定样本值时，根据估计量得到的估计值可能比真实值更大，也可能更小。但如果保持估计量的构造不变，而是进行多次重新抽样，每次都用新的样本计算估计值，那么这些估计值与未知参数真实值的偏差在平均意义上等于0，这意味着不存在系统误差。</p>\n<p>虽然估计值与真实值之间的偏差不可避免，但个体意义上的偏差越小意味着估计的性能越精确，有效性度量的正是估计量和真实值之间的偏离程度。而偏离程度不仅仅取决于估计量的构造方式，还取决于样本容量的大小，一致性考虑的就是样本容量的影响。一致性表示的是随着样本容量的增大，估计量的值将稳定在未知参数的真实值上。不具备一致性的估计量永远无法将未知参数估计得足够精确，因而是不可取的。</p>\n<p>对估计量的判别标准涉及了估计误差的影响，这是和估计值同样重要的参量。在估计未知参数$\\theta$的过程中，除了求出估计量，还需要估计出一个区间，并且确定这个区间包含$\\theta$真实值的可信程度。<strong>在数理统计中，这个区间被称为置信区间（confidence interval），这种估计方式则被称为区间估计</strong>。</p>\n<p>置信区间可以用如下的方式直观解释：对总体反复抽样多次，每次得到容量相同的样本，则根据每一组样本值都可以确定出一个置信区间$(\\underline{\\theta},\\overline{\\theta})$，其上界和下界是样本的两个统计量，分别代表了置信上限和置信下限。</p>\n<p>每个置信区间都存在两种可能性：包含$\\theta$的真实值或不包含$\\theta$的真实值。如果对所有置信区间中包含$\\theta$真实值的比率进行统计，得到的比值就是置信水平。因此，区间估计相当于在点估计的基础上进一步提供了取值范围和误差界限，分别对应着置信区间和置信水平。</p>\n<h2>假设检验</h2>\n<p><strong>参数估计的对象是总体的某个参数，假设检验的对象则是关于总体的某个论断，即关于总体的假设</strong>。假设检验中的假设包含原假设$H_0$和备择假设$H_1$；检验的过程就是根据样本在$H_0$和$H_1$之间选择一个接受的过程。</p>\n<p>理想的情况是假设$H_0$($H_1$)为真并且这个假设被接受。但由于检验是基于样本做出的，错误的决策终归会出现，其形式可以分为两种：第I类错误对应假设$H_0$为真但是被拒绝的情况，也就是“弃真”类型的错误；第II类错误对应假设$H_0$不真但是被接受的情况，也就是“取伪”类型的错误。</p>\n<p>假设检验的思维方式建立在全称命题只能被证伪不能被证实的基础上。要证明原假设$H_0$为真，更容易的方法是证明备择假设$H_1$为假，因为只要能够举出一个反例就够了。但在假设检验中，反例并非绝对意义上对假设的违背，而是以小概率事件的形式出现。</p>\n<p>在数理统计中，发生概率小于1%的事件被称作小概率事件，在单次实验中被认为是不可能发生的。如果在一次观测得到的样本中出现了小概率事件，那么就有理由认为这不是真正意义上的小概率事件，原始的假设也就此被推翻。如果是备择假设被推翻，就意味着接受原假设；反之，如果是原假设被推翻，则意味着拒绝原假设。</p>\n<p>从数理统计的角度看，监督学习算法的任务就是在假设空间中搜索能够针对特定问题做出良好预测的假设。学习器通过对测试数据集的学习得到具有普适性的模型，这个模型适用于不属于测试集的新样本的能力被称为泛化能力。显然，泛化能力越强，学习器就越好。</p>\n<p><strong>假设检验的作用就在于根据学习器在测试集上的性能推断其泛化能力的强弱，并确定所得结论的精确程度，可以进一步推广为比较不同学习器的性能</strong>。由于度量学习器性能的常用指标是错误率，假设检验中的假设就是对学习器的泛化错误率的推断，推断的依据就是在测试数据集上的测试错误率。具体的检验方式有很多种，在此不做赘述。</p>\n<p>除了推断之外，对泛化性能的解释也是机器学习算法分析的重要内容。<strong>泛化误差的构成可以分为三部分：偏差（bias）、方差（variance）和噪声（noise）</strong>。</p>\n<p>偏差表示算法预测值和真实结果之间的偏离程度，刻画的是模型的欠拟合特性；方差表示数据的扰动对预测性能的影响，刻画的是模型的过拟合特性；噪声表示在当前学习任务上能够达到的最小泛化误差，刻画的是任务本身的难度。对任何实际的模型来说，偏差和方差都难以实现同时优化，反映出欠拟合与过拟合之间难以调和的矛盾。</p>\n<p>今天我和你分享了人工智能必备的数理统计基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：</p>\n<ul>\n<li>数理统计的任务是根据可观察的样本反过来推断总体的性质；</li>\n<li>推断的工具是统计量，统计量是样本的函数，是个随机变量；</li>\n<li>参数估计通过随机抽取的样本来估计总体分布的未知参数，包括点估计和区间估计；</li>\n<li>假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用于估计机器学习模型的泛化错误率。</li>\n</ul>\n<p>既然机器学习和数理统计关注的都是利用数据提取信息或者规律，机器学习中的很多算法也依赖于数理统计作为基础，那么如何看待两者之间的区别和联系呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/55/6d/553fd5c4498ba56b75a15fc99770dc6d.jpg\" alt=\"\" /></p>\n","neighbors":{"left":{"article_title":"02 数学基础 | 月有阴晴圆缺，此事古难全：概率论","id":1341},"right":{"article_title":"04 数学基础 | 不畏浮云遮望眼：最优化方法","id":1507}}},{"article_id":1507,"article_title":"04 数学基础 | 不畏浮云遮望眼：最优化方法","article_content":"<p>从本质上讲，<strong>人工智能的目标就是最优化：在复杂环境与多体交互中做出最优决策</strong>。几乎所有的人工智能问题最后都会归结为一个优化问题的求解，因而最优化理论同样是人工智能必备的基础知识。</p>\n<p><strong>最优化理论（optimization）研究的问题是判定给定目标函数的最大值（最小值）是否存在，并找到令目标函数取到最大值（最小值）的数值</strong>。如果把给定的目标函数看成连绵的山脉，最优化的过程就是判断顶峰的位置并找到到达顶峰路径的过程。</p>\n<p>要实现最小化或最大化的函数被称为<strong>目标函数（objective function）或评价函数</strong>，大多数最优化问题都可以通过使目标函数$f(x)$最小化解决，最大化问题则可以通过最小化$-f(x)$实现。</p>\n<p>实际的最优化算法既可能找到目标函数的全局最小值（global minimum），也可能找到局部极小值（local minimum），两者的区别在于全局最小值比定义域内所有其他点的函数值都小；而局部极小值只是比所有邻近点的函数值都小。</p>\n<p>理想情况下，最优化算法的目标是找到全局最小值。但找到全局最优解意味着在全局范围内执行搜索。还是以山峰做例子。全局最小值对应着山脉中最高的顶峰，找到这个顶峰最好的办法是站在更高的位置上，将所有的山峰尽收眼底，再在其中找到最高的一座。</p>\n<p>可遗憾的是，目前实用的最优化算法都不具备这样的上帝视角。它们都是站在山脚下，一步一个脚印地寻找着附近的高峰。但受视野的限制，找到的峰值很可能只是方圆十里之内的顶峰，也就是局部极小值。</p>\n<p>当目标函数的输入参数较多、解空间较大时，绝大多数实用算法都不能满足全局搜索对计算复杂度的要求，因而只能求出局部极小值。但在人工智能和深度学习的应用场景下，只要目标函数的取值足够小，就可以把这个值当作全局最小值使用，作为对性能和复杂度的折中。</p>\n<!-- [[[read_end]]] -->\n<p><strong>根据约束条件的不同，最优化问题可以分为无约束优化（unconstrained optimization）和约束优化（constrained optimization）两类</strong>。无约束优化对自变量$x$的取值没有限制，约束优化则把$x$的取值限制在特定的集合内，也就是满足一定的约束条件。</p>\n<p>线性规划（linear programming）就是一类典型的约束优化，其解决的问题通常是在有限的成本约束下取得最大的收益。约束优化问题通常比无约束优化问题更加复杂，但通过<strong>拉格朗日乘子</strong>（Lagrange multiplier）的引入可以将含有$n$个变量和$k$个约束条件的问题转化为含有$(n + k)$个变量的无约束优化问题。拉格朗日函数最简单的形式如下</p>\n<p>$$  L(x, y, \\lambda) = f(x, y) + \\lambda \\varphi(x, y) $$</p>\n<p>式中$f(x, y)$为目标函数，$\\varphi(x, y)$则为等式约束条件，$\\lambda$是拉格朗日乘数。从数学意义上讲，由原目标函数和约束条件共同构成的拉格朗日函数与原目标函数具有共同的最优点集和共同的最优目标函数值，从而保证了最优解的不变性。</p>\n<p><strong>求解无约束优化问题最常用的方法是梯度下降法（gradient descent）</strong>。直观地说，梯度下降法就是沿着目标函数值下降最快的方向寻找最小值，就像爬山时要沿着坡度最陡的路径寻找山顶一样。在数学上，梯度的方向是目标函数导数（derivative）的反方向。</p>\n<p>当函数的输入为向量时，目标函数的图象就变成了高维空间上的曲面，这时的梯度就是垂直于曲面等高线并指向高度增加方向的向量，也就携带了高维空间中关于方向的信息。而要让目标函数以最快的速度下降，就需要让自变量在负梯度的方向上移动。这个结论翻译成数学语言就是“<strong>多元函数沿其负梯度方向下降最快</strong>”，这也是梯度下降法的理论依据。</p>\n<p>在梯度下降算法中，另一个重要的影响因素是<strong>步长</strong>，也就是每次更新$f(\\mathbf{x})$时$\\mathbf{x}$的变化值。较小的步长会导致收敛过程较慢，当$f(\\mathbf{x})$接近最小值点时，步长太大反而会导致一步迈过最小值点，正所谓“过犹不及”。</p>\n<p>因而在梯度下降法中，步长选择的整体规律是逐渐变小的。这样的方式也符合我们的认识规律。在校对仪器时，不都是先粗调再微调么？</p>\n<p>以上是针对单个样本的梯度下降法，当可用的训练样本有多个时，样本的使用模式就分为两种。</p>\n<p>一种是<strong>批处理模式</strong>（batch processing），即计算出在每个样本上目标函数的梯度，再将不同样本的梯度进行求和，求和的结果作为本次更新中目标函数的梯度。在批处理模式中，每次更新都要遍历训练集中所有的样本，因而运算量较大。</p>\n<p>另一种模式叫做<strong>随机梯度下降法</strong>（stochastic gradient descent），它在每次更新中只使用一个样本，下一次更新再使用另外一个样本，在不断迭代的更新过程中实现对所有样本的遍历。有趣的是，事实表明当训练集的规模较大时，随机梯度下降法的性能更佳。</p>\n<p>梯度下降法只用到了目标函数的一阶导数（first-order derivative），并没有使用二阶导数（second-order derivative）。一阶导数描述的是目标函数如何随输入的变化而变化，二阶导数描述的则是一阶导数如何随输入的变化而变化，提供了关于目标函数曲率（curvature）的信息。曲率影响的是目标函数的下降速度。当曲率为正时，目标函数会比梯度下降法的预期下降得更慢；反之，当曲率为负时，目标函数则会比梯度下降法的预期下降得更快。</p>\n<p>梯度下降法不能利用二阶导数包含的曲率信息，只能利用目标函数的局部性质，因而难免盲目的搜索中。已知目标函数可能在多个方向上都具有增加的导数，意味着下降的梯度具有多种选择。但不同选择的效果显然有好有坏。</p>\n<p>遗憾的是，梯度下降法无法获知关于导数的变化信息，也就不知道应该探索导数长期为负的方向。由于不具备观察目标函数的全局视角，在使用中梯度下降法就会走出一些弯路，导致收敛速度变慢。而二阶导数所包含的全局信息能够为梯度下降的方向提供指导，进而获得更优的收敛性。</p>\n<p><strong>如果将二阶导数引入优化过程，得到的典型方法就是牛顿法（Newton&#39;s method）</strong>。在牛顿法中，目标函数首先被泰勒展开，写成二阶近似的形式（相比之下，梯度下降法只保留了目标函数的一阶近似）。此时再对二阶近似后的目标函数求导，并令其导数等于0，得到的向量表示的就是下降最快的方向。<strong>相比于梯度下降法，牛顿法的收敛速度更快</strong>。</p>\n<p>不管是利用一阶导数的梯度下降法，还是利用二阶导数的牛顿法，<strong>其寻找最小值点的基本思想都是先确定方向，再确定步长，因而统称为“线性搜索方法”（line search）</strong>。</p>\n<p>还有一类算法，<strong>其寻找最小值点的基本思路是先确定步长，以步长为参数划定一个区域，再在这个区域内寻找最快下降的方向。这类算法被称为“置信域方法”（trust region）</strong>。</p>\n<p>具体来说，置信域算法的运行过程如下：设定一个置信域半径$s$，并在以当前点为中心、以$s$为半径的封闭球形区域作为置信域，在置信域内寻找目标函数的二次近似模型的最优点，最优点和当前点之间的距离就是计算出来的备选位移。</p>\n<p>在备选位移上，如果目标函数的二次近似产生了充分的下降，就将当前点移动到计算出的最优点，则继续按此规则迭代计算下去，并可以适当增加$s$；如果目标函数的近似下降不够理想，则说明步子跨得太大，需要缩小$s$并计算出新的备选位移，直到满足终止条件。</p>\n<p>除了以上算法外，还有一类被称为“<strong>启发式算法</strong>”（heuristics）的最优化方法。启发式算法的灵感来源于20世纪50年代诞生的仿生学，它将生物进化等自然现象的机理应用于现实世界复杂问题的优化之中，并取得了不俗的效果。</p>\n<p>相对于传统的基于数学理论的最优化方法，启发式算法显得返璞归真。<strong>启发式算法的核心思想就是大自然中&quot;优胜劣汰&quot;的生存法则，并在算法的实现中添加了选择和突变等经验因素</strong>。</p>\n<p>事实上，搜索越多并不意味着智能越高，智能高的表现恰恰是能够善用启发式策略，不用经过大量搜索也能解决问题。启发式算法的实例包括模拟生物进化规律的遗传算法（genetic algorithm）、模拟统计物理中固体结晶过程的模拟退火算法（simulated annealing）、模拟低等动物产生集群智能的蚁群算法（ant colony optimization）等等。</p>\n<p>今天炙手可热的神经网络实际上也是一类启发式算法，它模拟的则是大脑中神经元竞争和协作的机制。关于各类启发式算法的论著较多，如果你感兴趣，可以查阅不同算法的原理及实现，受篇幅所限，在此不做赘述。</p>\n<p>今天我和你分享了人工智能必备的最优化方法基础，着重于抽象概念的解释而非具体的数学公式，其要点如下：</p>\n<ul>\n<li>通常情况下，最优化问题是在无约束情况下求解给定目标函数的最小值；</li>\n<li>在线性搜索中，确定寻找最小值时的搜索方向需要使用目标函数的一阶导数和二阶导数；</li>\n<li>置信域算法的思想是先确定搜索步长，再确定搜索方向；</li>\n<li>以人工神经网络为代表的启发式算法是另外一类重要的优化方法。</li>\n</ul>\n<p>在最优化方法中，基于数学定理的搜索式算法和基于仿生学原理的启发式算法，哪一种能够在人工智能的发展中发挥更大的作用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/13/8e/13a4991f9bc5b7c3717f47ea28b4d18e.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"03 数学基础 | 窥一斑而知全豹：数理统计","id":1498},"right":{"article_title":"05 数学基础 | 万物皆数，信息亦然：信息论","id":1573}}},{"article_id":1573,"article_title":"05 数学基础 | 万物皆数，信息亦然：信息论","article_content":"<p>近年来的科学研究不断证实，不确定性才是客观世界的本质属性。换句话说，上帝还真就掷骰子。不确定性的世界只能使用概率模型来描述，正是对概率的刻画促成了信息论的诞生。</p>\n<p>1948年，供职于美国贝尔实验室的物理学家克劳德·香农发表了著名论文《通信的数学理论》（A Mathematical Theory of Communication），给出了对信息这一定性概念的定量分析方法，标志着信息论作为一门学科的正式诞生。</p>\n<p>香农在《通信的数学理论》中开宗明义：“通信的基本问题是在一点精确地或近似地复现在另一点所选取的消息。消息通常有意义，即根据某种体系，消息本身指向或关联着物理上或概念上的特定实体。但消息的语义含义与工程问题无关，重要的问题是一条消息来自于一个所有可能的消息的集合。”</p>\n<p>这样一来，所有类型的信息都被抽象为逻辑符号，这拓展了通信任务的范畴与信息论的适用性，也将信息的传播和处理完全剥离。</p>\n<p><strong>信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>在生活中，信息的载体是消息，而不同的消息带来的信息即使在直观感觉上也是不尽相同的。比如，“中国男子足球队获得世界杯冠军”的信息显然要比“中国男子乒乓球队获得世界杯冠军”的信息要大得多。</p>\n<p>究其原因，国足勇夺世界杯是如假包换的小概率事件（如果不是不可能事件的话），发生的可能性微乎其微；而男乒夺冠已经让国人习以为常，丢掉冠军的可能性才是意外。因此，以不确定性来度量信息是一种合理的方式。不确定性越大的消息可能性越小，其提供的信息量就越大。</p>\n<p>香农对信息的量化正是基于以上的思路，他定义了“熵”这一信息论中最基本最重要的概念。“熵”这个词来源于另一位百科全书式的科学家约翰·冯诺伊曼，他的理由是没人知道熵到底是什么。虽然这一概念已经在热力学中得到了广泛使用，但直到引申到信息论后，<strong>熵的本质才被解释清楚，即一个系统内在的混乱程度</strong>。</p>\n<p>在信息论中，如果事件$A$发生的概率为$p(A)$，则这个事件的自信息量的定义为 </p>\n<p>$$ h(A) = - \\log_2 p(A) $$</p>\n<p>如果国足闯进世界杯决赛圈，1:1000的夺冠赔率是个很乐观的估计，用这个赔率计算出的信息量约为10比特；而国乒夺冠的赔率不妨设为1:2，即使在这样高的赔率下，事件的信息量也只有1比特。两者之间的差距正是其可能性相差悬殊的体现。</p>\n<p>根据单个事件的自信息量可以计算包含多个符号的信源的信息熵。信源的信息熵是信源可能发出的各个符号的自信息量在信源构成的概率空间上的统计平均值。如果一个离散信源$X$包含$n$个符号，每个符号$a_i$的取值为$p(a_i)$，则$X$的信源熵为</p>\n<p> $$H(X) = -\\sum\\limits_{i = 1}^n p(a_i) \\log_2 p(a_i)$$</p>\n<p>信源熵描述了信源每发送一个符号所提供的平均信息量，是信源总体信息测度的均值。当信源中的每个符号的取值概率相等时，信源熵取到最大值$\\log _2 n$，意味着信源的随机程度最高。</p>\n<p>在概率论中有<strong>条件概率</strong>的概念，将条件概率扩展到信息论中，就可以得到<strong>条件熵</strong>。如果两个信源之间具有相关性，那么在已知其中一个信源$X$的条件下，另一个信源$Y$的信源熵就会减小。条件熵$H(Y|X)$表示的是在已知随机变量$X$的条件下另一个随机变量$Y$的不确定性，也就是在给定$X$时，根据$Y$的条件概率计算出的熵再对$X$求解数学期望：</p>\n<p>$$H(Y|X) = \\sum_{i = 1}^ n p(x_i) H(Y|X = x_i) $$</p>\n<p>$$= -\\sum_{i = 1}^ n p(x_i) \\sum_{j = 1}^ m p(y_j|x_i) \\log_2 p(y_j|x_i) $$</p>\n<p>$$ = - \\sum_{i = 1}^ n \\sum_{j = 1}^ m p(x_i, y_j) \\log_2 p(y_j|x_i) $$ </p>\n<p><strong>条件熵的意义在于先按照变量$X$的取值对变量$Y$进行了一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照$X$的分布计算其数学期望</strong>。</p>\n<p>以上课为例，学生在教室中可以任意选择座位，那么可能出现的座位分布会很多，其信源熵也就较大。如果对座位的选择添加一个限制条件，比如男生坐左边而女生坐右边，虽然左边的座位分布和右边的座位分布依然是随机的，但相对于未加限制时的情形就会简单很多。这就是分类带来的不确定性的下降。</p>\n<p>定义了条件信息熵后，就可以进一步得到<strong>互信息</strong>的概念 </p>\n<p>$$I(X; Y) = H(Y) - H(Y|X)$$</p>\n<p>互信息等于$Y$的信源熵减去已知$X$时$Y$的条件熵，即由$X$提供的关于$Y$的不确定性的消除，也可以看成是$X$给$Y$带来的<strong>信息增益</strong>。互信息这个名称在通信领域经常使用，信息增益则在机器学习领域中经常使用，两者的本质是一样的。</p>\n<p><strong>在机器学习中，信息增益常常被用于分类特征的选择</strong>。对于给定的训练数据集$Y$，$H(Y)$表示在未给定任何特征时，对训练集进行分类的不确定性；$H(Y|X)$则表示了使用特征$X$对训练集$Y$进行分类的不确定性。信息增益表示的就是特征$X$带来的对训练集$Y$分类不确定性的减少程度，也就是特征$X$对训练集$Y$的区分度。</p>\n<p>显然，信息增益更大的特征具有更强的分类能力。但信息增益的值很大程度上依赖于数据集的信息熵$H(Y)$，因而并不具有绝对意义。为解决这一问题，研究者又提出了<strong>信息增益比</strong>的概念，并将其定义为$g(X, Y) = I(X; Y) / H(Y)$。</p>\n<p>另一个在机器学习中经常使用的信息论概念叫作“<strong>Kullback-Leibler散度</strong>”，简称<strong>KL散度</strong>。KL散度是描述两个概率分布$P$和$Q$之间的差异的一种方法，其定义为</p>\n<p>$$D_{KL}(P||Q) = \\sum_{i = 1}^n p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)}$$</p>\n<p><strong>KL散度是对额外信息量的衡量</strong>。给定一个信源，其符号的概率分布为$P(X)$，就可以设计一种针对$P(X)$的最优编码，使得表示该信源所需的平均比特数最少（等于该信源的信源熵）。</p>\n<p>可是当信源的符号集合不变，而符合的概率分布变为$Q(X)$时，再用概率分布$P(X)$的最优编码对符合分布$Q(X)$的符号编码，此时编码结果的字符数就会比最优值多一些比特。</p>\n<p>KL散度就是用来衡量这种情况下平均每个字符多用的比特数，也可以表示两个分布之间的距离。</p>\n<p><strong>KL散度的两个重要性质是非负性和非对称性</strong>。</p>\n<p>非负性是指KL散度是大于或等于0的，等号只在两个分布完全相同时取到。</p>\n<p>非对称性则是指$D_{KL}(P||Q) \\ne D_{KL}(Q||P)$，即用$P(X)$去近似$Q(X)$和用$Q(X)$去近似$P(X)$得到的偏差是不同的，因此KL散度并不满足数学意义上对距离的定义，这一点需要注意。</p>\n<p>事实上，$D_{KL}(P||Q)$ 和$D_{KL}(Q||P)$代表了两种不同的近似方式。要让$D_{KL}(P||Q)$最小，需要让$Q(X)$在$P(X)$不等于0的位置同样不等于0；要让$D_{KL}(Q||P)$最小，则需要让$Q(X)$在$P(X)$等于0的位置同样等于0。</p>\n<p>除了以上定义的指标之外，信息论中还有一个重要定理，叫作“<strong>最大熵原理</strong>”。<strong>最大熵原理是确定随机变量统计特性时力图最符合客观情况的一种准则。对于一个未知的概率分布，最坏的情况就是它以等可能性取到每个可能的取值</strong>。这个时候的概率分布最均匀，也就是随机变量的随机程度最高，对它进行预测也就最困难。</p>\n<p>从这个角度看，最大熵原理的本质在于在推断未知分布时不引入任何多余的约束和假设，因而可以得到最不确定的结果，预测的风险也就最小。投资理财中的名言“不要把所有鸡蛋放在同一个篮子里”，就可以视为最大熵原理的一个实际应用。</p>\n<p>将最大熵原理应用到分类问题上就可以得到<strong>最大熵模型</strong>。在分类问题中，首先要确定若干特征函数作为分类的依据。为了保证特征函数的有效性，其在模型真实分布$P(X)$上的数学期望和在由训练数据集推导出的经验分布$\\tilde P(X)$上的数学期望应该相等，即对给定特征函数数学期望的估计应该是个无偏估计量。</p>\n<p>这样一来，每一个特征函数就对应了一个约束条件。分类的任务就是在这些约束条件下，确定一个最好的分类模型。由于除了这些约束条件之外，没有任何关于分类的先验知识，因而需要利用最大熵原理，求解出不确定性最大的条件分布，即让以下函数的取值最大化</p>\n<p>$$H(p) = -\\sum\\limits_{x, y} \\tilde p(x) p(y|x) \\log_2 p(y|x) $$</p>\n<p>式中的$p(y|x)$就是分类问题要确定的目标条件分布。计算上式的最大值实质上就是一个约束优化问题，由特征函数确定的约束条件可以通过<strong>拉格朗日乘子</strong>的引入去除其影响，转化为无约束优化问题。从数学上可以证明，这个模型的解是存在且唯一的。</p>\n<p>今天我和你分享了人工智能必备的信息论基础，着重于抽象概念的解释而非数学公式的推导，其要点如下：</p>\n<ul>\n<li>信息论处理的是客观世界中的不确定性；</li>\n<li>条件熵和信息增益是分类问题中的重要参数；</li>\n<li>KL散度用于描述两个不同概率分布之间的差异；</li>\n<li>最大熵原理是分类问题中的常用准则。</li>\n</ul>\n<p>信息论建立在概率的基础上，但其形式并不唯一，除了香农熵外也有其他关于熵的定义。那么概率与信息之间的关系对人工智能有什么启示呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e2/e5/e248d05acca0ac225b043a775bb221e5.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"04 数学基础 | 不畏浮云遮望眼：最优化方法","id":1507},"right":{"article_title":"06 数学基础 | 明日黄花迹难寻：形式逻辑","id":1639}}},{"article_id":1639,"article_title":"06 数学基础 | 明日黄花迹难寻：形式逻辑","article_content":"<p>1956年召开的达特茅斯会议宣告了人工智能的诞生。在人工智能的襁褓期，各位奠基者们，包括约翰·麦卡锡、赫伯特·西蒙、马文·明斯基等未来的图灵奖得主，他们的愿景是让“具备抽象思考能力的程序解释合成的物质如何能够拥有人类的心智”。</p>\n<p>通俗地说，理想的人工智能应该具备抽象意义上的学习、推理与归纳能力，其通用性将远远强于解决国际象棋或是围棋这些具体问题的算法。</p>\n<p>要实现这样的人工智能，不可或缺的基础是形式逻辑。人工智能的早期研究者认为人类认知和思维的基本单元是符号，而认知过程就是对符号的逻辑运算，这样一来，人类抽象的逻辑思维就可以通过计算机中逻辑门的运算模拟，进而实现机械化的人类认知。</p>\n<p>反过来，形式逻辑也是智能行为的描述方式，任何能够将某些物理模式或符号转化成其他模式或符号的系统都有可能产生智能的行为，也就是人工智能。</p>\n<p>人工智能能够模拟智能行为的基础是具有知识，但知识本身也是抽象的概念，需要用计算机能够理解的方式表示出来。</p>\n<p><strong>在人工智能中，常用的知识表示方法包括数据结构和处理算法。数据结构用于静态存储待解决的问题、问题的中间解答、问题的最终解答以及解答中涉及的知识；处理算法则用于在已有问题和知识之间进行动态交互，两者共同构成完整的知识表示体系</strong>。</p>\n<p>在人工智能的研究中，用形式逻辑实现知识表示是一种普遍的方法。形式逻辑可谓包罗万象，其最简单的实例就是由古希腊哲学家亚里士多德提出并流传至今的<strong>三段论</strong>，它由两个前提和一个结论构成：</p>\n<!-- [[[read_end]]] -->\n<ul>\n<li>科学是不断发展的；</li>\n<li>人工智能是科学；</li>\n<li>所以，人工智能是不断发展的。</li>\n</ul>\n<p>亚里士多德的贡献不仅在于证明了人工智能的不断发展，更在于确定了在大前提和小前提的基础上推导出一个结论的形式化过程，这个过程完全摆脱了内容的限制。<strong>由此诞生的符号推理给数理逻辑的研究带来了深远的影响</strong>。</p>\n<p>在人工智能中应用的主要是<strong>一阶谓词逻辑</strong>。谓词逻辑是最基本的逻辑系统，也是形式逻辑的根本部分。谓词逻辑的一个特例是<strong>命题逻辑</strong>。在命题逻辑中，命题是逻辑处理的基本单位，只能对其真伪做出判断。</p>\n<p>但命题这种表示法的局限性在于无法把其描述对象的结构及逻辑特征反映出来，也不能体现出不同事物的共同特征。假如单独给定命题“老李是小李的父亲”，在没有上下文时就无法确定老李和小李之间的关系，这个命题的真伪也就没有意义。</p>\n<p>为了扩展形式逻辑的表示能力，在命题逻辑的基础上又诞生了谓词逻辑。谓词逻辑将命题拆分为个体词、谓词和量词，三者的意义如下：</p>\n<ul>\n<li>个体词是可以独立存在的具体或抽象的描述对象，比如前文例子中的“老李”和“小李”；</li>\n<li>谓词用于描述个体词的属性与相互关系，比如前文例子中的“是...的父亲”；</li>\n<li>量词用于描述个体词的数量关系，包括全称量词$\\forall$ 和存在量词$\\exists$。</li>\n</ul>\n<p>以上三种元素可以共同构成命题。不同的命题之间则可以用逻辑联结词建立联系，由简单命题形成复合命题。按照优先级由高到低排列，逻辑联结词包括以下五种。</p>\n<ul>\n<li><strong>否定</strong>($\\neg$)：复合命题$\\neg P$表示否定命题P的真值的命题，即“非P” 。</li>\n<li><strong>合取</strong>($\\wedge$)：复合命题$P \\wedge Q$表示命题P和命题Q的合取，即“P且Q”。</li>\n<li><strong>析取</strong>($\\vee$)：复合命题$P \\vee Q$表示命题P或命题Q的析取，即“P或Q”。</li>\n<li><strong>蕴涵</strong>($\\to$)：复合命题$P \\to Q$表示命题P是命题Q的条件，即“如果P，那么Q”。</li>\n<li><strong>等价</strong>($\\leftrightarrow$)：复合命题$P \\leftrightarrow Q$表示命题P和命题Q相互蕴涵，即“如果P，那么Q且如果Q，那么P”。</li>\n</ul>\n<p>在谓词逻辑中出现的不只有常量符号，变量符号也是合法的，同时还可以出现函数符号。变量和函数的引入拓展了谓词逻辑的表示范围，也提升了其普适性。<strong>谓词逻辑既可以用于表示事物的概念、状态、属性等事实性知识，也可以用于表示事物间具有确定因果关系的规则性知识</strong>。</p>\n<p>事实性知识通常使用析取与合取符号连接起来的谓词公式表示，规则性知识则通常使用由蕴涵符号连接起来的谓词公式来表示。在一般意义上，使用谓词逻辑进行知识表示的步骤如下：</p>\n<ul>\n<li>定义谓词及个体，确定每个谓词及每个个体的确切含义；</li>\n<li>根据所要表达的事物或概念，为每个谓词中的变量赋以特定的值；</li>\n<li>根据所要表达的知识的语义，用适当的逻辑联结词将各个谓词连接起来。</li>\n</ul>\n<p>经过以上步骤的处理后，抽象意义上的知识就能够转化为计算机可以识别并处理的数据结构。</p>\n<p>例如，如果要使用谓词逻辑对“所有自然数都是大于零的整数”进行知识表示，首先要将所有关系定义为相应的谓词。谓词N(x)表示x是自然数，P(x)表示x大于零，I(x)表示x是整数，再将这些谓词按照语义进行连接就可以得到谓词公式：</p>\n<p>$$ (\\forall x)(N(x) \\to P(x) \\wedge I(x)) $$</p>\n<p>使用形式逻辑进行知识表示只是手段，其目的是让人工智能在知识的基础上实现自动化的推理、归纳与演绎，以得到新结论与新知识。就现阶段而言，人类智能与人工智能的主要区别就体现在推理能力上。</p>\n<p>人类的判断方式绝非一头扎进浩如烟海的数据中学习，而是基于少量数据的特征进行归纳与推理，以得出的一般性规律作为判断的基础。在数字图像中稍微添加一点干扰就可以让神经网络将海龟误认为步枪，这点伎俩却不能欺骗具有思考能力的人类，其原因也在于此。</p>\n<p><strong>人工智能实现自动推理的基础是产生式系统</strong>。产生式系统以产生式的规则描述符号串来替代运算，把推理和行为的过程用产生式规则表示，其机制类似人类的认知过程，因而被早年间大多数专家系统所使用。</p>\n<p>产生式规则通常用于表示事物之间的因果关系，其基本形式为$P \\to Q$。它既可以用来表示在前提P下得到结论Q，也可以表示在条件P下实施动作Q。这里的P称为规则前件，它既可以是简单条件，也可以是由多个简单条件通过联结词形成的复合条件；Q则称为规则后件。</p>\n<p>当一组产生式规则相互配合、协同作用时，一个产生式规则生成的结论就可以为另一个产生式规则作为已知的前提或条件使用，以进一步解决更加复杂的问题，这样的系统就是产生式系统。</p>\n<p>一般说来，<strong>产生式系统包括规则库、事实库和推理机三个基本部分</strong>。</p>\n<p><strong>规则库</strong>是专家系统的核心与基础，存储着以产生式形式表示的规则集合，其中规则的完整性、准确性和合理性都将对系统性能产生直接的影响。</p>\n<p><strong>事实库</strong>存储的是输入事实、中间结果与最终结果，当规则库中的某条产生式的前提可与事实库中的某些已知事实匹配时，该产生式就被激活，其结论也就可以作为已知事实存储在事实库中。</p>\n<p><strong>推理机</strong>则是用于控制和协调规则库与事实库运行的程序，包括了推理方式和控制策略。</p>\n<p>具体而言，<strong>推理的方式可以分为三种：正向推理、反向推理和双向推理</strong>。</p>\n<p><strong>正向推理</strong>采用的是自底向上的方式，即从已知事实出发，通过在规则库中不断选择匹配的规则前件，得到匹配规则的后件，进而推演出目标结论。</p>\n<p><strong>反向推理</strong>采用的是自顶向下的方式，即从目标假设出发，通过不断用规则库中规则的后件与已知事实匹配，选择出匹配的规则前件，进而回溯已知事实。</p>\n<p><strong>双向推理</strong>则是综合利用正向推理和反向推理，使推理从自顶向下和自底向上两个方向进行，直到在某个中间点汇合，这种方式具有更高的效率。</p>\n<p><strong>自动推理虽然在数学定理的证明上显示出强大的能力，可解决日常生活中的问题时却远远谈不上智能，其原因在于常识的缺失</strong>。对于人类而言，常识的建立是通过社会化的成长过程实现的。</p>\n<p>可计算机没办法像人类一样在成长中达成理解，因而常识这一智能的先决条件只能以形式化的方式被灌输到硬盘与内存之中。这要求将一般成年人的知识和信念进行显式的表达，并加以灵活的组织和运用。</p>\n<p>可几乎尽人皆知的是，对常识性知识的表达和组织存在着难以想象的困难。从“张三捡起足球”和“张三在运动场上”这两个命题推断出“足球在运动场上”对人工智能来说就已经不可想象，更不用说道德观世界观这些复杂概念。</p>\n<p>没有对常识和信念的清晰表达，人工智能就必然陷入混乱的泥沼而无法自拔，获得通用性和适应性较强的智能行为也只能是痴人说梦。</p>\n<p>谈论人工智能中的形式逻辑，最终的不可回避的本质问题在于<strong>哥德尔不完备性定理</strong>。</p>\n<p>1900年，德国数学家大卫·希尔伯特在巴黎国际数学家代表大会上提出了20世纪23个最重要的数学问题，其中的第二问题便是算术公理系统的无矛盾性。</p>\n<p>1931年，奥地利数学家库尔特·哥德尔对这个问题给出了否定的答案，即<strong>第一不完备性定理</strong>：</p>\n<p>$$\\text{在任何包含初等数论的形式系统中，}$$</p>\n<p>$$\\text{都必定存在一个不可判定命题。} $$ </p>\n<p>通过这个定理，哥德尔证明了公理化系统的阿喀琉斯之踵在于对自指的无能为力，下面的语句就是个典型的自指语句：</p>\n<p> $$ \\text{本数学命题不可以被证明。} $$ </p>\n<p>首先，这个数学命题所讨论的对象不是别的，恰恰是它自己。“本数学命题”就是对整个命题的指代。其次，该命题给出了一个逻辑判断，即这条命题是不可以被证明的。</p>\n<p>哥德尔证明了这个命题既不能被证实也不能被证伪，无情戳破了数学公理化系统同时具备一致性和完备性的黄粱一梦。</p>\n<p><strong>不完备性定理对人工智能的影响在于对“认知的本质是计算”这一理论基础的理解</strong>。从“认知即计算”的角度出发，基于计算机的人工智能如果想要达到近似人类的思维能力，也必须建立起“自我”的概念，这就无疑会导致自指的出现，也将成为不完备性定理的活靶子。</p>\n<p>如果计算机能在运算中制造出一个代表自身的符号，那么哥德尔制造悖论的方式就可以在计算机中造出不可证实也不可证伪的飘渺幻境。</p>\n<p>在哥德尔不完备性定理的阴影下，基于图灵可计算概念的“认知可计算主义”研究纲领已经显示出其极大的局限。今天，依靠人工神经网络逐渐崛起的连接主义学派大放异彩，与此同时，以形式逻辑为依据的符号主义学派则已经走向没落。</p>\n<p>但抛开学术路径的不同，人类智能与人工智能的本质区别到底在哪里，也许这才是不完备性定理留给我们的最大谜团。</p>\n<p>今天我和你分享了人工智能必备的形式逻辑基础，以及采用形式逻辑进行自动推理的基本原理，其要点如下：</p>\n<ul>\n<li>如果将认知过程定义为对符号的逻辑运算，人工智能的基础就是形式逻辑；</li>\n<li>谓词逻辑是知识表示的主要方法；</li>\n<li>基于谓词逻辑系统可以实现具有自动推理能力的人工智能；</li>\n<li>不完备性定理向“认知的本质是计算”这一人工智能的基本理念提出挑战。</li>\n</ul>\n<p>虽然问题多多，但符号主义的思路依然是更接近于人类认知的思路。对形式逻辑的处理能否成为未来依赖小数据学习的人工智能的核心技术呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d4/59/d4e00273015088b3d813ddb59fc4f659.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"05 数学基础 | 万物皆数，信息亦然：信息论","id":1573},"right":{"article_title":"（课外辅导）数学基础 | 拓展阅读参考书","id":1807}}},{"article_id":1807,"article_title":"（课外辅导）数学基础 | 拓展阅读参考书","article_content":"<p>线性代数推荐两本国外的教材。</p>\n<p>其一是 <strong>Gilbert Strang</strong> 所著的 <strong>Introduction to Linear Algebra</strong>，英文版在2016年出到第五版，暂无中译本。这本通过直观形象的概念性解释阐述抽象的基本概念，同时辅以大量线性代数在各领域内的实际应用，对学习者非常友好。作者在麻省理工学院的OCW上开设了相应的视频课程，还配有习题解答、模拟试题等一系列电子资源。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/de/51/deab88e3a873a8c02e3fd385336b6851.jpg\" alt=\"\" /></p>\n<p>其二是 <strong>David C Lay</strong> 所著的 <strong>Linear Algebra and its Applications</strong>，英文版在2015年同样出到第五版，中译本名为<strong>《线性代数及其应用》</strong>，对应原书第四版。这本书通过向量和线性方程组这些基本概念深入浅出地介绍线代中的基本概念，着重公式背后的代数意义和几何意义，同样配有大量应用实例，对理解基本概念帮助很大。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/fd/4b/fd67a5515225cddafbc6bd159c21d24b.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/fc/fb/fcc6cfbc5daa261aaf472951963854fb.jpg\" alt=\"\" /></p>\n<p>概率论的基础读物可以选择 <strong>Sheldon M Ross</strong> 所著的<br />\n<strong>A First Course in Probability</strong>，英文版在2013年出到第九版（18年马上要出第十版），中译本名为<strong>《概率论基础教程》</strong>，对应原书第九版，也有英文影印本。这本书抛开测度，从中心极限定理的角度讨论概率问题，对概念的解释更加通俗，书中还包含海量紧密联系生活的应用实例与例题习题。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/3f/a9/3f8ba17f3adfedfdefbee6805c8f2ca9.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d3/79/d34838e4cd8abfc89aebfe781ce95179.jpg\" alt=\"\" /></p>\n<p>另一本艰深的读物是 <strong>Edwin Thompson Jaynes</strong> 所著的<br />\n<strong>Probability Theory: The Logic of Science</strong>，本书暂无中译本，影印本名为<strong>《概率论沉思录》</strong>也已绝版。这本书是作者的遗著，花费半个世纪的时间完成，从名字就可以看出是一部神书。作者从逻辑的角度探讨了基于频率的概率，贝叶斯概率和统计推断，将概率论这门偏经验的学科纳入数理逻辑的框架之下。如果读这本书，千万要做好烧脑的准备。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/de/d0/de211b6a95d6d1ad83b2ba6e518783d0.jpg\" alt=\"\" /></p>\n<p>数理统计的基础读物可以选择陈希孺院士所著的<strong>《数理统计学教程》</strong>。关于统计学是不是科学的问题依然莫衷一是，但它在机器学习中的重要作用毋庸置疑。陈老的书重在论述统计的概念和思想，力图传授利用统计观点去观察和分析事物的能力，这是非常难能可贵的。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/eb/c8/eb91566fe246daf53a0bd0c3a98363c8.jpg\" alt=\"\" /></p>\n<p>进阶阅读可以选择 <strong>Roger Casella</strong> 所著的 <strong>Statistical Inference</strong>，由于作者已于2012年辞世，2001年的第二版便成为绝唱。中译本名为<strong>《统计推断》</strong>，亦有影印本。本书包含部分概率论的内容，循循善诱地介绍了统计推断、参数估计、方差回归等统计学中的基本问题。</p>\n<p><img src=\"https://static001-test.geekbang.org/resource/image/a1/20/a129538c6273a679fe641c89c597db20.png\" alt=\"\" /></p>\n<p>最优化理论可以参考 <strong>Stephen Boyd</strong> 所著的 <strong>Convex Optimization</strong>，中译本名为<strong>《凸优化》</strong>。这本书虽然块头吓人，但可读性并不差，主要针对实际应用而非理论证明，很多机器学习中广泛使用的方法都能在这里找到源头。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/a8/66/a8aecbd9467ca6dfa607329e3c43ce66.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/19/da/194d1decc9774117f8dab99be9ee55da.jpg\" alt=\"\" /></p>\n<p>信息论书籍推荐 <strong>Thomas Cover</strong> 和 <strong>Jay A Thomas</strong><br />\n合著的 <strong>Elements of Information Theory</strong>，2006年出到第二版，中译本为<strong>《信息论基础》</strong>。这本书兼顾广度和深度，虽然不是大部头却干货满满，讲清了信息论中各个基本概念的物理内涵，但要顺畅阅读需要一定的数学基础。另外，本书偏重于信息论在通信中的应用。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/32/f0/32809f551ae31c7f5b376e6104324af0.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/4e/cb/4e13d7b7d12f16d70ac5f7b5dd813dcb.jpg\" alt=\"\" /></p>\n<p><strong>部分书目链接</strong>：</p>\n<p><a href=\"https://math.mit.edu/~gs/linearalgebra/linearalgebra5_Preface.pdf\">Introduction to Linear Algebra</a></p>\n<p><a href=\"http://www.zuj.edu.jo/download/linear-algebra-and-its-applications-david-c-lay-pdf/\">Linear Algebra and its Applications</a></p>\n<p><a href=\"http://julio.staff.ipb.ac.id/files/2015/02/Ross_8th_ed_English.pdf\">A First Course in Probability</a>（8th edition）</p>\n<p><a href=\"http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf\">Probability Theory: The Logic of Science</a></p>\n<p><a href=\"https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf\">Statistical Inference</a></p>\n<p><a href=\"https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\">Convex Optimization</a></p>\n<p><a href=\"http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf\">Elements of Information Theory</a></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/74/80/748baa3d40829627167d74a877102a80.jpg\" alt=\"\" /></p>\n<p></p>\n<!-- [[[read_end]]] -->\n","neighbors":{"left":{"article_title":"06 数学基础 | 明日黄花迹难寻：形式逻辑","id":1639},"right":{"article_title":"07 机器学习 | 数山有路，学海无涯：机器学习概论","id":1669}}},{"article_id":1669,"article_title":"07 机器学习 | 数山有路，学海无涯：机器学习概论","article_content":"<p>不知道你在生活中是否留意过这样的现象：我们可以根据相貌轻易区分出日本人、韩国人和泰国人，却对英国人、俄罗斯人和德国人脸盲。造成这种现象的原因一方面在于日韩泰都是我国的邻国，观察这些国家普通人的机会较多；另一方面，抛开衣妆的因素不论，相同的人种也使得面貌特征更加容易进行比较和辨别。</p>\n<p>因此，根据大量的观察就能总结出不同国别的相貌特点：中国人下颌适中，日本人长脸长鼻，韩国人眼小颧高，泰国人肤色暗深。在做出路人甲来自日本或是路人乙来自韩国的判断时，正是以这些特征作为依据的。</p>\n<p>上面的例子就是简化版的人类学习机制：<strong>从大量现象中提取反复出现的规律与模式</strong>。这一过程在人工智能中的实现就是<strong>机器学习</strong>。</p>\n<p>从形式化角度定义，如果算法利用某些经验使自身在特定任务类上的性能得到改善，就可以说该算法实现了机器学习。而从方法论的角度看，<strong>机器学习是计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科</strong>。</p>\n<p>机器学习可说是从数据中来，到数据中去。假设已有数据具有一定的统计特性，则不同的数据可以视为满足独立同分布的样本。机器学习要做的就是根据已有的训练数据推导出描述所有数据的模型，并根据得出的模型实现对未知的测试数据的最优预测。</p>\n<!-- [[[read_end]]] -->\n<p>在机器学习中，数据并非通常意义上的数量值，而是对于对象某些性质的描述。被描述的性质叫作<strong>属性</strong>，属性的取值称为<strong>属性值</strong>，不同的属性值有序排列得到的向量就是<strong>数据</strong>，也叫<strong>实例</strong>。</p>\n<p>在文首的例子中，黄种人相貌特征的典型属性便包括肤色、眼睛大小、鼻子长短、颧骨高度。标准的中国人实例甲就是属性值{浅、大、短、低 }的组合，标准的韩国人实例乙则是属性值{浅、小、长、高}的组合。</p>\n<p>根据线性代数的知识，数据的不同属性之间可以视为相互独立，因而每个属性都代表了一个不同的维度，这些维度共同张成了<strong>特征空间</strong>。</p>\n<p>每一组属性值的集合都是这个空间中的一个点，因而每个实例都可以视为特征空间中的一个向量，即<strong>特征向量</strong>。</p>\n<p>需要注意的是这里的特征向量不是和特征值对应的那个概念，而是指特征空间中的向量。根据特征向量对输入数据进行分类就能够得到输出。</p>\n<p>在前面的例子中，输入数据是一个人的相貌特征，输出数据就是中国人/日本人/韩国人/泰国人四中选一。而在实际的机器学习任务中，输出的形式可能更加复杂。根据输入输出类型的不同，预测问题可以分为以下三类。</p>\n<ul>\n<li><strong>分类问题</strong>：输出变量为有限个离散变量，当个数为2时即为最简单的二分类问题；</li>\n<li><strong>回归问题</strong>：输入变量和输出变量均为连续变量；</li>\n<li><strong>标注问题</strong>：输入变量和输出变量均为变量序列。</li>\n</ul>\n<p>但在实际生活中，每个国家的人都不是同一个模子刻出来的，其长相自然也会千差万别，因而一个浓眉大眼的韩国人可能被误认为中国人，一个肤色较深的日本人也可能被误认为泰国人。</p>\n<p>同样的问题在机器学习中也会存在。一个算法既不可能和所有训练数据符合得分毫不差，也不可能对所有测试数据预测得精确无误。因而<strong>误差性能就成为机器学习的重要指标之一</strong>。</p>\n<p><strong>在机器学习中，误差被定义为学习器的实际预测输出与样本真实输出之间的差异</strong>。在分类问题中，常用的误差函数是<strong>错误率</strong>，即分类错误的样本占全部样本的比例。</p>\n<p><strong>误差可以进一步分为训练误差和测试误差两类</strong>。训练误差指的是学习器在训练数据集上的误差，也称经验误差；测试误差指的是学习器在新样本上的误差，也称泛化误差。</p>\n<p>训练误差描述的是输入属性与输出分类之间的相关性，能够判定给定的问题是不是一个容易学习的问题。<strong>测试误差则反映了学习器对未知的测试数据集的预测能力，是机器学习中的重要概念</strong>。实用的学习器都是测试误差较低，即在新样本上表现较好的学习器。</p>\n<p>学习器依赖已知数据对真实情况进行拟合，即由学习器得到的模型要尽可能逼近真实模型，因此要在训练数据集中尽可能提取出适用于所有未知数据的普适规律。</p>\n<p>然而，一旦过于看重训练误差，一味追求预测规律与训练数据的符合程度，就会把训练样本自身的一些非普适特性误认为所有数据的普遍性质，从而导致学习器泛化能力的下降。</p>\n<p>在前面的例子中，如果接触的外国人较少，从没见过双眼皮的韩国人，思维中就难免出现“单眼皮都是韩国人”的错误定式，这就是典型的<strong>过拟合现象</strong>，把训练数据的特征错当做整体的特征。</p>\n<p><strong>过拟合出现的原因通常是学习时模型包含的参数过多，从而导致训练误差较低但测试误差较高</strong>。</p>\n<p>与过拟合对应的是<strong>欠拟合</strong>。如果说造成过拟合的原因是学习能力太强，造成欠拟合的原因就是学习能力太弱，以致于训练数据的基本性质都没能学到。如果学习器的能力不足，甚至会把黑猩猩的图像误认为人，这就是欠拟合的后果。</p>\n<p>在实际的机器学习中，欠拟合可以通过改进学习器的算法克服，但过拟合却无法避免，只能尽量降低其影响。由于训练样本的数量有限，因而具有有限个参数的模型就足以将所有训练样本纳入其中。</p>\n<p>可模型的参数越多，能与这个模型精确相符的数据也就越少，将这样的模型运用到无穷的未知数据当中，过拟合的出现便不可避免。更何况训练样本本身还可能包含一些噪声，这些随机的噪声又会给模型的精确性带来额外的误差。</p>\n<p>整体来说，<strong>测试误差与模型复杂度之间呈现的是抛物线的关系</strong>。当模型复杂度较低时，测试误差较高；随着模型复杂度的增加，测试误差将逐渐下降并达到最小值；之后当模型复杂度继续上升时，测试误差会随之增加，对应着过拟合的发生。</p>\n<p>在模型选择中，为了对测试误差做出更加精确的估计，一种广泛使用的方法是<strong>交叉验证</strong>。交叉验证思想在于重复利用有限的训练样本，通过将数据切分成若干子集，让不同的子集分别组成训练集与测试集，并在此基础上反复进行训练、测试和模型选择，达到最优效果。</p>\n<p>如果将训练数据集分成10个子集$D_{1-10}$进行交叉验证，则需要对每个模型进行10轮训练，其中第1轮使用的训练集为$D_2$~$D_{10}$这9个子集，训练出的学习器在子集$D_1$上进行测试；第2轮使用的训练集为$D_1$和$D_3$~$D_{10}$这9个子集，训练出的学习器在子集$D_2$上进行测试。依此类推，当模型在10个子集全部完成测试后，其性能就是10次测试结果的均值。<strong>不同模型中平均测试误差最小的模型也就是最优模型</strong>。</p>\n<p>除了算法本身，参数的取值也是影响模型性能的重要因素，同样的学习算法在不同的参数配置下，得到的模型性能会出现显著的差异。因此，<strong>调参，也就是对算法参数进行设定，是机器学习中重要的工程问题，这一点在今天的神经网络与深度学习中体现得尤为明显</strong>。</p>\n<p>假设一个神经网络中包含1000个参数，每个参数又有10种可能的取值，对于每一组训练/测试集就有$1000 ^ {10}$个模型需要考察，因而<strong>在调参过程中，一个主要的问题就是性能和效率之间的折中</strong>。</p>\n<p>在人类的学习中，有的人可能有高人指点，有的人则是无师自通。在机器学习中也有类似的分类。根据训练数据是否具有标签信息，可以将机器学习的任务分成以下三类。</p>\n<ul>\n<li><strong>监督学习</strong>：基于已知类别的训练数据进行学习；</li>\n<li><strong>无监督学习</strong>：基于未知类别的训练数据进行学习；</li>\n<li><strong>半监督学习</strong>：同时使用已知类别和未知类别的训练数据进行学习。</li>\n</ul>\n<p><strong>受学习方式的影响，效果较好的学习算法执行的都是监督学习的任务</strong>。即使号称自学成才、完全脱离了对棋谱依赖的AlphaGo Zero，其训练过程也要受围棋胜负规则的限制，因而也脱不开监督学习的范畴。</p>\n<p>监督学习假定训练数据满足独立同分布的条件，并根据训练数据学习出一个由输入到输出的映射模型。反映这一映射关系的模型可能有无数种，所有模型共同构成了假设空间。<strong>监督学习的任务就是在假设空间中根据特定的误差准则找到最优的模型</strong>。</p>\n<p><strong>根据学习方法的不同，监督学习可以分为生成方法与判别方法两类</strong>。</p>\n<p>生成方法是根据输入数据和输出数据之间的联合概率分布确定条件概率分布$P(Y|X)$，这种方法表示了输入X与输出Y之间的生成关系；判别方法则直接学习条件概率分布$P(Y|X)$或决策函数$f(X)$，这种方法表示了根据输入X得出输出Y的预测方法。</p>\n<p><strong>两相对比，生成方法具有更快的收敛速度和更广的应用范围，判别方法则具有更高的准确率和更简单的使用方式</strong>。</p>\n<p>今天我和你分享了机器学习的基本原理与基础概念，其要点如下：</p>\n<ul>\n<li>机器学习是计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科；</li>\n<li>根据输入输出类型的不同，机器学习可分为分类问题、回归问题、标注问题三类；</li>\n<li>过拟合是机器学习中不可避免的问题，可通过选择合适的模型降低其影响；</li>\n<li>监督学习是目前机器学习的主流任务，包括生成方法和判别方法两类。</li>\n</ul>\n<p>在图像识别领域中，高识别率的背后是大量被精细标记的图像样本，而对数以百万计的数字图像进行标记无疑需要耗费大量人力。借鉴近期AlphaGo Zero的成功经验，如何降低机器学习中的人工干预程度，同时提升算法效率呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/48/4c/4877cc0a3c4a5690364a24a35862324c.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"（课外辅导）数学基础 | 拓展阅读参考书","id":1807},"right":{"article_title":"08 机器学习 | 简约而不简单：线性回归","id":1865}}},{"article_id":1865,"article_title":"08 机器学习 | 简约而不简单：线性回归","article_content":"<p>数学中的线性模型可谓“简约而不简单”：它既能体现出重要的基本思想，又能构造出功能更加强大的非线性模型。在机器学习领域，线性回归就是这样一类基本的任务，它应用了一系列影响深远的数学工具。</p>\n<p>在数理统计中，回归分析是确定多种变量间相互依赖的定量关系的方法。<strong>线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数</strong>。在众多回归分析的方法里，线性回归模型最易于拟合，其估计结果的统计特性也更容易确定，因而得到广泛应用。而在机器学习中，回归问题隐含了输入变量和输出变量均可连续取值的前提，因而利用线性回归模型可以对任意输入给出对输出的估计。</p>\n<p>1875年，从事遗传问题研究的英国统计学家弗朗西斯·高尔顿正在寻找父代与子代身高之间的关系。在分析了1078对父子的身高数据后，他发现这些数据的散点图大致呈直线状态，即父亲的身高和儿子的身高呈正相关关系。而在正相关关系背后还隐藏着另外一个现象：矮个子父亲的儿子更可能比父亲高；而高个子父亲的儿子更可能比父亲矮。</p>\n<p>受表哥查尔斯·达尔文的影响，高尔顿将这种现象称为“<strong>回归效应</strong>”，即大自然将人类身高的分布约束在相对稳定而不产生两极分化的整体水平，并给出了历史上第一个线性回归的表达式：y = 0.516x + 33.73，式中的y和x分别代表以英寸为单位的子代和父代的身高。</p>\n<p>高尔顿的思想在今天的机器学习中依然保持着旺盛的生命力。假定一个实例可以用列向量${\\bf x} = (x_1; x_2; \\cdots, x_n)$表示，每个$x_i$代表了实例在第i个属性上的取值，线性回归的作用就是习得一组参数$w_i, i = 0, 1, \\cdots, n$，使预测输出可以表示为以这组参数为权重的实例属性的线性组合。如果引入常量$x_0 = 1$，线性回归试图学习的模型就是</p>\n<p>$$f({\\bf x}) = {\\bf w} ^ T {\\bf x} = \\sum\\limits_{i = 0}^{n} w_i \\cdot x_i $$</p>\n<p>当实例只有一个属性时，输入和输出之间的关系就是二维平面上的一条直线；当实例的属性数目较多时，线性回归得到的就是n+1维空间上的一个超平面，对应一个维度等于n的线性子空间。</p>\n<p>在训练集上确定系数$w_i$时，预测输出$f({\\bf x})$和真实输出$y$之间的误差是关注的核心指标。在线性回归中，这一误差是以<strong>均方误差</strong>来定义的。当线性回归的模型为二维平面上的直线时，均方误差就是预测输出和真实输出之间的<strong>欧几里得距离</strong>，也就是两点间向量的$L ^ 2$范数。而以使均方误差取得最小值为目标的模型求解方法就是<strong>最小二乘法</strong>，其表达式可以写成</p>\n<p>$${\\mathbf{w}}^* = \\mathop {\\arg \\min }\\limits_{\\mathbf{w}} \\sum\\limits_{k = 1} {{{({{\\mathbf{w}}^T}{{\\mathbf{x}}_k} - {y_k})}^2}} $$</p>\n<p>$$= \\mathop {\\arg \\min }\\limits_{\\mathbf{w}} \\sum\\limits_{k = 1} || y_k - \\mathbf{w}^T \\mathbf{x}_k ||^2 $$</p>\n<p>式中每个${\\bf x}_k$代表训练集中的一个样本。<strong>在单变量线性回归任务中，最小二乘法的作用就是找到一条直线，使所有样本到直线的欧式距离之和最小</strong>。</p>\n<p>说到这里，问题就来了：凭什么使均方误差最小化的参数就是和训练样本匹配的最优模型呢？</p>\n<!-- [[[read_end]]] -->\n<p>这个问题可以从概率论的角度阐释。线性回归得到的是统计意义上的拟合结果，在单变量的情形下，可能每一个样本点都没有落在求得的直线上。</p>\n<p>对这个现象的一种解释是回归结果可以完美匹配理想样本点的分布，但训练中使用的真实样本点是理想样本点和噪声叠加的结果，因而与回归模型之间产生了偏差，而每个样本点上噪声的取值就等于$y_k - f(\\mathbf{x}_k)$。</p>\n<p>假定影响样本点的噪声满足参数为$(0, \\sigma ^ 2)$的正态分布（还记得正态分布的概率密度公式吗？），这意味着噪声等于0的概率密度最大，幅度（无论正负）越大的噪声出现的概率越小。在这种情形下，对参数$\\mathbf{w}$的推导就可以用<strong>最大似然的方式</strong>进行，即在已知样本数据及其分布的条件下，找到使样本数据以最大概率出现的假设。</p>\n<p>单个样本$\\mathbf{x}_k$出现的概率实际上就是噪声等于$y_k - f(\\mathbf{x}_k)$的概率，而相互独立的所有样本同时出现的概率则是每个样本出现概率的乘积，其表达式可以写成</p>\n<p>$$p({{\\mathbf{x}}_1},{{\\mathbf{x}}_2}, \\cdots {{\\mathbf{x}}_k}, \\cdots |{\\mathbf{w}}) =$$</p>\n<p>$$ \\prod\\limits_k {\\frac{1}{{\\sqrt {2\\pi } \\sigma }}} \\exp [ - \\frac{1}{{2{\\sigma ^2}}}{({y_k} - {{\\mathbf{w}}^T}{{\\mathbf{x}}_k})^2}]$$</p>\n<p>而最大似然估计的任务就是让以上表达式的取值最大化。出于计算简便的考虑，上面的乘积式可以通过取对数的方式转化成求和式，且取对数的操作并不会影响其单调性。经过一番运算后，上式的最大化就可以等效为$\\sum\\limits_{k} (y_k - \\mathbf{w} ^ T \\mathbf{x}_k) ^ 2$的最小化。这不就是最小二乘法的结果么？</p>\n<p>因此，<strong>对于单变量线性回归而言，在误差函数服从正态分布的情况下，从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的</strong>。</p>\n<p>确定了最小二乘法的最优性，接下来的问题就是如何求解均方误差的最小值。在单变量线性回归中，其回归方程可以写成$y = w_1x + w_0$。根据最优化理论，将这一表达式代入均方误差的表达式中，并分别对$w_1$和$w_0$求偏导数，令两个偏导数均等于0的取值就是线性回归的最优解，其解析式可以写成</p>\n<p>$$ w_1 = \\dfrac{\\sum\\limits_{k = 1}^m y_k(x_k - \\frac{1}{m} \\sum\\limits_{k = 1}^m x_k)}{\\sum\\limits_{k = 1}^m x_k^2 - \\frac{1}{m} (\\sum\\limits_{k = 1}^m x_k) ^ 2}$$</p>\n<p>$$w_0 = \\dfrac{1}{m} \\sum\\limits_{k = 1}^m (y_k - w_1x_k)$$</p>\n<p><strong>单变量线性回归只是一种最简单的特例</strong>。子代的身高并非仅仅由父母的遗传基因决定，营养条件、生活环境等因素都会产生影响。当样本的描述涉及多个属性时，这类问题就被称为<strong>多元线性回归</strong>。</p>\n<p>多元线性回归中的参数$\\mathbf{w}$也可以用最小二乘法进行估计，其最优解同样用偏导数确定，但参与运算的元素从向量变成了矩阵。在理想的情况下，多元线性回归的最优参数为</p>\n<p>$$ {\\mathbf{w}}^* = (\\mathbf{X} ^ T  \\mathbf{X}) ^ {-1}  \\mathbf{X} ^ T  \\mathbf{y} $$</p>\n<p>式中的$\\mathbf{X}$是由所有样本${\\bf x} = (x_0; x_1; x_2; \\cdots, x_n)$的转置共同构成的矩阵。但这一表达式只在矩阵$(\\mathbf{X} ^ T  \\mathbf{X})$的逆矩阵存在时成立。在大量复杂的实际任务中，每个样本中属性的数目甚至会超过训练集中的样本总数，此时求出的最优解${\\mathbf{w}}^*$就不是唯一的，解的选择将依赖于学习算法的归纳偏好。</p>\n<p>但不论采用怎样的选取标准，存在多个最优解都是无法改变的事实，这也意味着过拟合的产生。更重要的是，在过拟合的情形下，微小扰动给训练数据带来的毫厘之差可能会导致训练出的模型谬以千里，模型的稳定性也就无法保证。</p>\n<p>要解决过拟合问题，常见的做法是正则化，即添加额外的惩罚项。<strong>在线性回归中，正则化的方式根据其使用惩罚项的不同可以分为两种，分别是“岭回归”和“LASSO回归”</strong>。</p>\n<p><strong>在机器学习中，岭回归方法又被称为“参数衰减”</strong>，于20世纪40年代由前苏联学者安德烈·季霍诺夫提出。当然，彼时机器学习尚未诞生，季霍诺夫提出这一方法的主要目的是解决矩阵求逆的稳定性问题，其思想后来被应用到正则化中，形成了今天的岭回归。</p>\n<p>岭回归实现正则化的方式是在原始均方误差项的基础上添加一个待求解参数的二范数项，即最小化的对象变为$|| y_k - \\mathbf{w}^T \\mathbf{x}_k || ^ 2 + || \\Gamma \\mathbf{w}|| ^ 2$，其中的$\\Gamma$被称为<strong>季霍诺夫矩阵</strong>，通常可以简化为一个常数。</p>\n<p>从最优化的角度看，二范数惩罚项的作用在于优先选择范数较小的$\\mathbf{w}$，这相当于在最小均方误差之外额外添加了一重关于最优解特性的约束条件，将最优解限制在高维空间内的一个球里。岭回归的作用相当于在原始最小二乘的结果上做了缩放，虽然最优解中每个参数的贡献被削弱了，但参数的数目并没有变少。</p>\n<p>LASSO回归的全称是“<strong>最小绝对缩减和选择算子</strong>”（Least Absolute Shrinkage and Selection Operator），由加拿大学者罗伯特·提布什拉尼于1996年提出。与岭回归不同的是，LASSO回归选择了待求解参数的一范数项作为惩罚项，即最小化的对象变为$|| y_k - \\mathbf{w}^T \\mathbf{x}_k || ^ 2 + \\lambda ||\\mathbf{w}||_1$，其中的$\\lambda$是一个常数。</p>\n<p><strong>与岭回归相比，LASSO回归的特点在于稀疏性的引入</strong>。它降低了最优解$\\mathbf{w}$的维度，也就是将一部分参数的贡献削弱为0，这就使得$\\mathbf{w}$中元素的数目大大小于原始特征的数目。</p>\n<p>这或多或少可以看作奥卡姆剃刀原理的一种实现：当主要矛盾和次要矛盾同时存在时，优先考虑的必然是主要矛盾。虽然饮食、环境、运动等因素都会影响身高的变化，但决定性因素显然只存在在染色体上。值得一提的是，<strong>引入稀疏性是简化复杂问题的一种常用方法，在数据压缩、信号处理等其他领域中亦有广泛应用</strong>。</p>\n<p>从概率的角度来看，最小二乘法的解析解可以利用正态分布以及最大似然估计求得，这在前文已有说明。岭回归和LASSO回归也可以从概率的视角进行阐释：岭回归是在$w_i$满足正态先验分布的条件下，用最大后验概率进行估计得到的结果；LASSO回归是在$w_i$满足拉普拉斯先验分布的条件下，用最大后验概率进行估计得到的结果。</p>\n<p><strong>但无论岭回归还是LASSO回归，其作用都是通过惩罚项的引入抑制过拟合现象，以训练误差的上升为代价，换取测试误差的下降</strong>。将以上两种方法的思想结合可以得到新的优化方法，在此就不做赘述了。</p>\n<p>今天我和你分享了机器学习基本算法之一的线性回归的基本原理，其要点如下：</p>\n<ul>\n<li>线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数；</li>\n<li>最小二乘法可用于解决单变量线性回归问题，当误差函数服从正态分布时，它与最大似然估计等价；</li>\n<li>多元线性回归问题也可以用最小二乘法求解，但极易出现过拟合现象；</li>\n<li>岭回归和LASSO回归分别通过引入二范数惩罚项和一范数惩罚项抑制过拟合。</li>\n</ul>\n<p>在深度学习大行其道的今天，巨量的参数已经成为常态。在参数越来越多，模型越来越复杂的趋势下，线性回归还能发挥什么样的作用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/c2/3d/c213a86d22def0da9a92fe3092605f3d.jpg\" alt=\"\" /></p>\n","neighbors":{"left":{"article_title":"07 机器学习 | 数山有路，学海无涯：机器学习概论","id":1669},"right":{"article_title":"09 机器学习 | 大道至简：朴素贝叶斯方法","id":1866}}},{"article_id":1866,"article_title":"09 机器学习 | 大道至简：朴素贝叶斯方法","article_content":"<p>周二我和你分享了机器学习中的线性回归算法，这一算法解决的是从连续取值的输入映射为连续取值的输出的回归问题。今天我分享的算法则用于解决分类问题，即将连续取值的输入映射为离散取值的输出，算法的名字叫作“<strong>朴素贝叶斯方法</strong>”。</p>\n<p>解决分类问题的依据是数据的属性。朴素贝叶斯分类器假定样本的不同属性满足条件独立性假设，并在此基础上应用贝叶斯定理执行分类任务。<strong>其基本思想在于分析待分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出</strong>。</p>\n<p>假设训练数据的属性由n维随机向量$\\bf x$表示，其分类结果用随机变量y表示，那么x和y的统计规律就可以用联合概率分布$P(X, Y)$描述，每一个具体的样本$(x_i, y_i)$都可以通过$P(X, Y)$独立同分布地产生。</p>\n<p>朴素贝叶斯分类器的出发点就是这个联合概率分布，根据条件概率的性质可以得到</p>\n<p> $$ P(X, Y) = P(Y) \\cdot P(X|Y)$$ </p>\n<p>$$= P(X) \\cdot P(Y|X) $$ </p>\n<p>在上式中，P(Y)代表着每个类别出现的概率，也就是<strong>类先验概率</strong>；P(X|Y)代表着在给定的类别下不同属性出现的概率，也就是<strong>类似然概率</strong>。</p>\n<p>先验概率容易根据训练数据计算出来，只需要统计不同类别样本的数目即可。而似然概率受属性取值数目的影响，其估计较为困难。</p>\n<p>如果每个样本包含100个属性，每个属性的取值都可能有100种，那么对分类的每个结果，要计算的条件概率数目就是$100 ^ 2 = 10000$。在这么多参数的情况下，对似然概率的精确估计就需要庞大的数据量。</p>\n<p>要解决似然概率难以估计的问题，就需要“条件独立性假设”登台亮相。<strong>条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用</strong>。<strong>这样类条件概率就变成了属性条件概率的乘积</strong>，在数学公式上可以体现为</p>\n<p> $$ P(X = {\\bf x}|Y = c) = $$</p>\n<p>$$P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\cdots, X^{(n)} = x^{(n)}|Y = c)$$ </p>\n<p>$$= \\Pi_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c) $$</p>\n<p><strong>这正是朴素贝叶斯方法的“朴素”之处，通过必要的假设来简化计算，并回归问题的本质</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>条件独立性假设对似然概率的估计无疑是个天大的好消息。没有这一假设时，每个样本的分类结果y只能刻画其所有属性$x_1, x_2, \\cdots, x_n$形成的整体，只有具有相同$x_1, x_2, \\cdots, x_n$的样本才能放在一起进行评价。当属性数目较多且数据量较少时，要让n个属性同时取到相同的特征就需要些运气了。</p>\n<p>有了条件独立性假设后，分类结果y就相当于实现了n重复用。每一个样本既可以用于刻画$x_1$，又可以用于刻画$x_n$，这无形中将训练样本的数量扩大为原来的n倍，分析属性的每个取值对分类结果的影响时，也有更多数据作为支撑。</p>\n<p>但需要说明的是，<strong>属性的条件独立性假设是个相当强的假设</strong>。</p>\n<p>一个例子是银行在发放房贷时，需要对贷款申请人的情况进行调研，以确定是否发放贷款。本质上这就是个分类问题，分类的结果是“是”与“否”。分类时则需要考虑申请人的年龄、工作岗位、婚姻状况、收入水平、负债情况等因素。这些因素显然不是相互独立的。中年人的收入通常会高于青年人的收入，已婚者的负债水平通常也会高于未婚者的负债水平。</p>\n<p>因而在实际应用中，属性条件独立性假设会导致数据的过度简化，因而会给分类性能带来些许影响。但它带来的数学上的便利却能极大简化分类问题的计算复杂度，性能上的部分折中也就并非不可接受。</p>\n<p>有了训练数据集，先验概率P(Y)和似然概率P(X|Y)就可以视为已知条件，用来求解后验概率P(Y|X)。对于给定的输入$\\bf x$，朴素贝叶斯分类器利用贝叶斯定理求解后验概率，并将后验概率最大的类作为输出。</p>\n<p>由于在所有后验概率的求解中，边界概率P(X)都是相同的，因而其影响可以忽略。将属性条件独立性假设应用于后验概率求解中，就可以得到朴素贝叶斯分类器的数学表达式</p>\n<p>$$ y = \\arg \\mathop {\\max}\\limits_{c_k} P(y = c_k) \\cdot $$</p>\n<p>$$\\Pi_j P(X^{(j)} = x^{(j)}|Y = c_k) $$</p>\n<p>应用朴素贝叶斯分类器处理连续型属性数据时，通常假定属性数据满足正态分布，再根据每个类别下的训练数据计算出正态分布的均值和方差。</p>\n<p><strong>从模型最优化的角度观察，朴素贝叶斯分类器是平均意义上预测能力最优的模型，也就是使期望风险最小化</strong>。期望风险是风险函数的数学期望，度量的是平均意义下模型预测的误差特性，可以视为单次预测误差在联合概率分布P(X, Y)上的数学期望。</p>\n<p>朴素贝叶斯分类器通过将实例分配到后验概率最大的类中，也就同时让1 - P(Y|X)取得最小值。在以分类错误的实例数作为误差时，期望风险就等于1 - P(Y|X)。这样一来，后验概率最大化就等效于期望风险最小化。</p>\n<p>受训练数据集规模的限制，某些属性的取值在训练集中可能从未与某个类同时出现，这就可能导致属性条件概率为0，此时直接使用朴素贝叶斯分类就会导致错误的结论。</p>\n<p>还是以贷款申请为例，如果在训练集中没有样本同时具有“年龄大于60”的属性和“发放贷款”的标签，那么当一个退休人员申请贷款时，即使他是坐拥百亿身家的李嘉诚，朴素贝叶斯分类器也会因为后验概率等于零而将他无情拒绝。</p>\n<p>因为训练集样本的不充分导致分类错误，显然不是理想的结果。为了避免属性携带的信息被训练集中未曾出现过的属性值所干扰，在计算属性条件概率时需要添加一个称为“<strong>拉普拉斯平滑</strong>”的步骤。</p>\n<p>所谓拉普拉斯平滑就是在计算类先验概率和属性条件概率时，在分子上添加一个较小的修正量，在分母上则添加这个修正量与分类数目的乘积。这就可以保证在满足概率基本性质的条件下，避免了零概率对分类结果的影响。当训练集的数据量较大时，修正量对先验概率的影响也就可以忽略不计了。</p>\n<p>事实上，朴素贝叶斯是一种非常高效的方法。当以分类的正确与否作为误差指标时，只要朴素贝叶斯分类器能够把最大的后验概率找到，就意味着它能实现正确的分类。至于找到的最大后验概率的估计值是否精确，反而没那么重要了。</p>\n<p>如果一个实例在两个类别上的后验概率分别是0.9和0.1，朴素贝叶斯分类器估计出的后验概率就可能是0.6和0.4。虽然数值的精度相差较大，但大小的相对关系并未改变。依据这个粗糙估计的后验概率进行分类，得到的依然是正确的结果。</p>\n<p>上面的说法固然言之成理，却不能解释另外一个疑问。虽然属性条件独立性看起来像是空中楼阁，却给朴素贝叶斯分类器带来了实实在在的优良性能，这其中的奥秘何在？为什么在基础假设几乎永远不成立的情况下，朴素贝叶斯依然能够在绝大部分分类任务中体现出优良性能呢？</p>\n<p>一种可能的解释是：在给定的训练数据集上，两个属性之间可能具有相关性，但这种相关性在每个类别上都以同样的程度体现。 这种情况显然违背了条件独立性假设，却不会破坏朴素贝叶斯分类器的最优性。</p>\n<p>即使相关性在不同类别上的分布不是均匀的也没关系，只看两个单独的属性，它们之间可能存在强烈的依赖关系，会影响分类的结果。但当所有属性之间的依赖关系一起发挥作用时，它们就可能相互抵消，不再影响分类。</p>\n<p>简而言之，决定性的因素是所有属性之间的依赖关系的组合。<strong>影响朴素贝叶斯的分类的是所有属性之间的依赖关系在不同类别上的分布，而不仅仅是依赖关系本身</strong>。可即便如此，属性条件独立性假设依然会影响分类性能。为了放宽这一假设，研究人员又提出了“<strong>半朴素贝叶斯分类器</strong>”的学习方法。</p>\n<p>半朴素贝叶斯分类器考虑了部分属性之间的依赖关系，既保留了属性之间较强的相关性，又不需要完全计算复杂的联合概率分布。<strong>常用的方法是建立独依赖关系：假设每个属性除了类别之外，最多只依赖一个其他属性</strong>。由此，根据属性间依赖关系确定方式的不同，便衍生出了多种独依赖分类器。</p>\n<p>朴素贝叶斯分类器的应用场景非常广泛。它可以根据关键词执行对一封邮件是否是垃圾邮件的二元分类，也可以用来判断社交网络上的账号到底是活跃用户还是僵尸粉。在信息检索领域，这种分类方法尤为实用。总结起来，<strong>以朴素贝叶斯分类器为代表的贝叶斯分类方法的策略是：根据训练数据计算后验概率，基于后验概率选择最佳决策</strong>。</p>\n<p>今天我和你分享了机器学习基本算法之一的朴素贝叶斯方法的基本原理，其要点如下：</p>\n<ul>\n<li>朴素贝叶斯方法利用后验概率选择最佳分类，后验概率可以通过贝叶斯定理求解；</li>\n<li>朴素贝叶斯方法假定所有属性相互独立，基于这一假设将类条件概率转化为属性条件概率的乘积；</li>\n<li>朴素贝叶斯方法可以使期望风险最小化；</li>\n<li>影响朴素贝叶斯分类的是所有属性之间的依赖关系在不同类别上的分布。</li>\n</ul>\n<p>在使用高维数据集时，每个样本都会包含大量的属性，这时属性条件概率连乘的结果会非常接近于零，导致下溢的发生。如何防止因概率过小造成的下溢呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/1d/ab/1d23a0935e1e853e21a0d6a0dab9e4ab.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"08 机器学习 | 简约而不简单：线性回归","id":1865},"right":{"article_title":"10 机器学习 | 衍化至繁：逻辑回归","id":1867}}},{"article_id":1867,"article_title":"10 机器学习 | 衍化至繁：逻辑回归","article_content":"<p>周四我和你分享了机器学习中的朴素贝叶斯分类算法，这一算法解决的是将连续取值的输入映射为离散取值的输出的分类问题。朴素贝叶斯分类器是一类<strong>生成模型</strong>，通过构造联合概率分布$P(X, Y)$实现分类。如果换一种思路，转而用<strong>判别模型</strong>解决分类问题的话，得到的算法就是“<strong>逻辑回归</strong>”。</p>\n<p><strong>虽然顶着“回归”的名号，但逻辑回归解决的却是实打实的分类问题</strong>。之所以取了这个名字，原因在于它来源于对线性回归算法的改进。通过引入单调可微函数$g(\\cdot)$，线性回归模型就可以推广为$y = g ^ {-1} (\\mathbf{w} ^ T \\mathbf{x})$，进而将线性回归模型的连续预测值与分类任务的离散标记联系起来。当$g(\\cdot)$取成对数函数的形式时，线性回归就演变为了逻辑回归。</p>\n<!-- [[[read_end]]] -->\n<p>在最简单的二分类问题中，分类的标记可以抽象为0和1，因而线性回归中的实值输出需要映射为二进制的结果。逻辑回归中，实现这一映射是对数几率函数，也叫Sigmoid函数</p>\n<p> $$ y = \\dfrac{1}{1 + e ^ {-z}} = \\dfrac{1}{1 + e ^ {- (\\mathbf{w} ^ T \\mathbf{x})}} $$ </p>\n<p><strong>之所以选择对数几率函数，是因为它具备良好的特性</strong>。</p>\n<p>首先，对数几率函数能够将线性回归从负无穷到正无穷的输出范围压缩到(0, 1)之间，无疑更加符合对二分类任务的直观感觉。</p>\n<p>其次，当线性回归的结果$z = 0$时，逻辑回归的结果$y = 0.5$，这可以视为一个分界点：当$z &gt; 0$时，$y &gt; 0.5$，此时逻辑回归的结果就可以判为正例；当$z &lt; 0$时，$y &lt; 0.5$，逻辑回归的结果就可以判为反例。</p>\n<p>显然，对数几率函数能够在线性回归和逻辑回归之间提供更好的可解释性。这种可解释性可以从数学的角度进一步诠释。</p>\n<p>如果将对数几率函数的结果$y$视为样本$\\mathbf{x}$作为正例的可能性，则$1 - y$就是其作为反例的可能性，两者的比值$0 &lt; \\dfrac{y}{1 -y} &lt; +\\infty$称为几率，体现的是样本作为正例的相对可能性。如果对几率函数取对数，并将前文中的公式代入，可以得到 </p>\n<p>$$ \\ln \\dfrac{y}{1 -y} = \\mathbf{w} ^ T \\mathbf{x} + b $$</p>\n<p>由此可见，当利用逻辑回归模型解决分类任务时，线性回归的结果正是以对数几率的形式出现的。</p>\n<p>归根结底，逻辑回归模型由条件概率分布表示 </p>\n<p>$$ p(y = 1 | \\mathbf{x} ) = \\dfrac{e ^ {\\mathbf{w} ^ T \\mathbf{x} + b}}{1 + e ^ {\\mathbf{w} ^ T \\mathbf{x} + b}}$$</p>\n<p>$$p(y = 0 | \\mathbf{x} ) = \\dfrac{1}{1 + e ^ {\\mathbf{w} ^ T \\mathbf{x} + b}} $$</p>\n<p><strong>对于给定的实例，逻辑回归模型比较两个条件概率值的大小，并将实例划分到概率较大的分类之中</strong>。</p>\n<p><strong>学习时，逻辑回归模型在给定的训练数据集上应用最大似然估计法确定模型的参数</strong>。对给定的数据集${ (\\mathbf{x}_i, y_i)}$，逻辑回归使每个样本属于其真实标记的概率最大化，以此为依据确定$\\mathbf{w}$的最优值。由于每个样本的输出$y_i$都满足两点分布，且不同的样本之间相互独立，因而似然函数可以表示为</p>\n<p>$$ L(\\mathbf{w} | \\mathbf{x}) = \\Pi_{i = 1}^N [p(y = 1 | \\mathbf{x}_i, \\mathbf{w})] ^ {y_i} $$</p>\n<p>$$\\cdot [1 - p(y = 1 | \\mathbf{x}_i, \\mathbf{w})] ^ {1 - y_i} $$ </p>\n<p>利用对数操作将乘积转化为求和，就可以得到对数似然函数</p>\n<p> $$ \\log L(\\mathbf{w} | \\mathbf{x}) = \\sum\\limits_{i = 1}^N y_i \\log [p(y = 1 | \\mathbf{x}_i, \\mathbf{w})] $$</p>\n<p>$$+ (1 - y_i) \\log [1 - p(y = 1 | \\mathbf{x}_i, \\mathbf{w})]$$ </p>\n<p>由于单个样本的标记$y_i$只能取得0或1，因而上式中的两项中只有一个有非零的取值。将每个条件概率的对数几率函数形式代入上式，经过化简可以得到</p>\n<p>$$ \\log L(\\mathbf{w} | \\mathbf{x}) = \\sum\\limits_{i = 1}^N y_i \\cdot (\\mathbf{w} ^ T \\mathbf{x}_i) $$</p>\n<p>$$- \\log (1 + e ^ {\\mathbf{w} ^ T \\mathbf{x}_i}) $$ </p>\n<p><strong>寻找以上函数的最大值就是以对数似然函数为目标函数的最优化问题，通常通过“梯度下降法”或拟“牛顿法”求解</strong>。</p>\n<p>当训练数据集是从所有数据中均匀抽取且数量较大时，以上结果还有一种信息论角度的阐释方式：<strong>对数似然函数的最大化可以等效为待求模型与最大熵模型之间KL散度的最小化</strong>。这意味着最优的估计对参数做出的额外假设是最少的，这无疑与最大熵原理不谋而合。</p>\n<p>从数学角度看，线性回归和逻辑回归之间的渊源来源于非线性的对数似然函数；而从特征空间的角度看，两者的区别则在于数据判定边界的变化。判定边界可以类比为棋盘上的楚河汉界，边界两侧分别对应不同类型的数据。</p>\n<p>以最简单的二维平面直角坐标系为例。受模型形式的限制，利用线性回归只能得到直线形式的判定边界；逻辑回归则在线性回归的基础上，通过对数似然函数的引入使判定边界的形状不再受限于直线，而是推广为更加复杂的曲线形式，更加精细的分类也就不在话下。</p>\n<p>逻辑回归与线性回归的关系称得上系出同门，与朴素贝叶斯分类的关系则是殊途同归。两者虽然都可以利用条件概率$P(Y|X)$完成分类任务，实现的路径却截然不同。</p>\n<p>朴素贝叶斯分类器是生成模型的代表，其思想是先由训练数据集估计出输入和输出的联合概率分布，再根据联合概率分布来生成符合条件的输出，$P(Y|X)$以后验概率的形式出现。</p>\n<p>逻辑回归模型则是判别模型的代表，其思想是先由训练数据集估计出输入和输出的条件概率分布，再根据条件概率分布来判定对于给定的输入应该选择哪种输出，$P(Y|X)$以似然概率的形式出现。</p>\n<p><strong>即便原理不同，逻辑回归与朴素贝叶斯分类器在特定的条件下依然可以等效</strong>。用朴素贝叶斯分类器处理二分类任务时，假设对每个$\\mathbf{x}_i$，属性条件概率$p(\\mathbf{x}_i | Y = y_k)$都满足正态分布，且正态分布的标准差与输出标记$Y$无关，那么根据贝叶斯定理，后验概率就可以写成 </p>\n<p>$$ p(Y = 0 | X) =$$</p>\n<p>$$ \\dfrac{p(Y = 0) \\cdot p(X | Y = 0)}{p(Y = 1) \\cdot p(X | Y = 1) + p(Y = 0) \\cdot p(X | Y = 0)} $$ </p>\n<p> $$ = \\dfrac{1}{1 + \\exp (\\ln \\frac{p(Y = 1) \\cdot p(X | Y = 1)}{p(Y = 0) \\cdot p(X | Y = 0)})} $$ </p>\n<p>根据朴素贝叶斯方法的假设，类条件概率可以表示为属性条件概率的乘积，因而令$p(Y = 0) = p_0$并将满足正态分布的属性条件概率$p(\\mathbf{x}_i | Y = y_k)$代入以上表达式中，经过一番计算就可以得到</p>\n<p> $$ p(Y = 0 | X) = $$</p>\n<p>$$\\dfrac{1}{1 + \\exp(\\ln \\frac{1 - p_0}{p_0} + \\sum\\limits_i(\\frac{\\mu_{i1} - \\mu_{i0}}{\\sigma_i^2} X_i + \\frac{\\mu_{i0} ^ 2 - \\mu_{i1} ^ 2}{2\\sigma_i^2}))} $$ </p>\n<p>不难看出，上式的形式和逻辑回归中条件概率$p(y = 0 | \\mathbf{x} )$的形式是完全一致的，这表明朴素贝叶斯方法和逻辑回归模型学习到的是同一个模型。实际上，在$p(x | Y)$的分布属于指数分布族这个更一般的假设下，类似的结论都是成立的。</p>\n<p>说完了联系，再来看看区别。<strong>两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果</strong>。当训练样本数接近无穷大时，逻辑回归的渐近分类准确率要优于朴素贝叶斯方法。而且逻辑回归并不完全依赖于属性之间相互独立的假设，即使给定违反这一假设的数据，逻辑回归的条件似然最大化算法也会调整其参数以实现最大化的数据拟合。相比之下，逻辑回归的偏差更小，但方差更大。</p>\n<p>除此之外，<strong>两者的区别还在于收敛速度的不同</strong>。逻辑回归中参数估计的收敛速度要慢于朴素贝叶斯方法。当训练数据集的容量较大时，逻辑回归的性能优于朴素贝叶斯方法；但在训练数据稀缺时，两者的表现就会发生反转。</p>\n<p>二分类任务只是特例，更通用的情形是多分类的问题，例如手写数字的识别。要让逻辑回归处理多分类问题，就要做出一些改进。</p>\n<p><strong>一种改进方式是通过多次二分类实现多个类别的标记</strong>。这等效为直接将逻辑回归应用在每个类别之上，对每个类别都建立一个二分类器。如果输出的类别标记数目为$m$，就可以得到$m$个针对不同标记的二分类逻辑回归模型，而对一个实例的分类结果就是这$m$个分类函数中输出值最大的那个。在这种方式中，对一个实例执行分类需要多次使用逻辑回归算法，其效率显然比较低下。</p>\n<p><strong>另一种多分类的方式通过直接修改逻辑回归输出的似然概率，使之适应多分类问题，得到的模型就是Softmax回归</strong>。Softmax回归给出的是实例在每一种分类结果下出现的概率</p>\n<p> $$ p(Y = k | x) = \\dfrac{e ^ {\\mathbf{w_k} ^ T \\mathbf{x}}}{\\sum\\limits_{k = 1}^K e ^ {\\mathbf{w_k} ^ T \\mathbf{x}}} $$ </p>\n<p>式中的$\\mathbf{w_k}$代表和类别$k$相关的权重参数。Softmax回归模型的训练与逻辑回归模型类似，都可以转化为通过梯度下降法或者拟牛顿法解决最优化问题。</p>\n<p>虽然都能实现多分类的任务，但两种方式的适用范围略有区别。当分类问题的所有类别之间明显互斥，即输出结果只能属于一个类别时，Softmax分类器是更加高效的选择；当所有类别之间不存在互斥关系，可能有交叉的情况出现时，多个二分类逻辑回归模型就能够得到多个类别的标记。</p>\n<p>今天我和你分享了机器学习基本算法之一的逻辑回归方法的基本原理，其要点如下：</p>\n<ul>\n<li>逻辑回归模型是对线性回归的改进，用于解决分类问题；</li>\n<li>逻辑回归输出的是实例属于每个类别的似然概率，似然概率最大的类别就是分类结果；</li>\n<li>在一定条件下，逻辑回归模型与朴素贝叶斯分类器是等价的；</li>\n<li>多分类问题时可以通过多次使用二分类逻辑回归或者使用Softmax回归解决。</li>\n</ul>\n<p>前文对逻辑回归的分析都是在概率理论的基础上完成的。但在二分类任务中，逻辑回归的作用可以视为是在平面直角坐标系上划定一条数据分类的判定边界。那么逻辑回归的作用能不能从几何角度理解，并推广到高维空间呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d8/aa/d81794d22373b75dd79da8655adacdaa.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"09 机器学习 | 大道至简：朴素贝叶斯方法","id":1866},"right":{"article_title":"11 机器学习 | 步步为营，有章可循：决策树","id":2008}}},{"article_id":2008,"article_title":"11 机器学习 | 步步为营，有章可循：决策树","article_content":"<p>决策树算法是解决分类问题的另一种方法。与基于概率推断的朴素贝叶斯分类器和逻辑回归模型不同，<strong>决策树算法采用树形结构，使用层层推理来实现最终的分类</strong>。与贝叶斯分类器相比，决策树的优势在于构造过程无需使用任何先验条件，因而适用于探索式的知识发现。</p>\n<p>决策树的分类方法更接近人类的判断机制，这可以通过买房的实例说明。</p>\n<p>面对眼花缭乱的房源，普通人优先考虑的都是每平方米的价格因素，价格不贵就买，价格贵了就不买。在价格合适的前提下，面积就是下一个待确定的问题，面积不小就买，面积小了就不买。如果面积合适，位置也是不容忽视的因素，单身业主会考虑房源离工作地点的远近，离单位近就买，离单位远就不买；为人父母的则要斟酌是不是学区房，是学区房就买，不是学区房就不买。如果位置同样称心，就可以再根据交通是否便捷、物业是否良好、价格是否有优惠等条件进一步筛选，确定最后的购买对象。</p>\n<p>前面的例子模拟了一套购房策略。在这套策略中，业主对每个可选房源都要做出“买”与“不买”的决策结果，而“每平米价格”、“房屋面积”、“学区房”等因素共同构成了决策的判断条件，在每个判断条件下的选择表示的是不同情况下的决策路径，而每个“买”或是“不买”的决定背后都包含一系列完整的决策过程。决策树就是将以上过程形式化、并引入量化指标后形成的分类算法。</p>\n<p><strong>决策树是一个包含根节点、内部节点和叶节点的树结构，其根节点包含样本全集，内部节点对应特征属性测试，叶节点则代表决策结果</strong>。从根节点到每个叶节点的每条路径都对应着一个从数据到决策的判定流程。使用决策树进行决策的过程就是从根节点开始，测试待分类项的特征属性，并按照其值选择输出的内部节点。当选择过程持续到到达某个叶节点时，就将该叶节点存放的类别作为决策结果。</p>\n<p>由于决策树是基于特征对实例进行分类的，因而其学习的本质是从训练数据集中归纳出一组用于分类的“如果......那么......”规则。在学习的过程中，这组规则集合既要在训练数据上有较高的符合度，也要具备良好的泛化能力。<strong>决策树模型的学习过程包括三个步骤：特征选择、决策树生成和决策树剪枝</strong>。</p>\n<!-- [[[read_end]]] -->\n<p><strong>特征选择决定了使用哪些特征来划分特征空间</strong>。在训练数据集中，每个样本的属性可能有很多个，在分类结果中起到的作用也有大有小。因而特征选择的作用在于筛选出与分类结果相关性较高，也就是分类能力较强的特征。理想的特征选择是在每次划分之后，分支节点所包含的样本都尽可能属于同一个类别。</p>\n<p><strong>在特征选择中通常使用的准则是信息增益</strong>。<strong>机器学习中的信息增益就是通信理论中的互信息，是信息论的核心概念之一</strong>。信息增益描述的是在已知特征后对数据分类不确定性的减少程度，因而特征的信息增益越大，得到的分类结果的不确定度越低，特征也就具有越强的分类能力。根据信息增益准则选择特征的过程，就是自顶向下进行划分，在每次划分时计算每个特征的信息增益并选取最大值的过程。信息增益的计算涉及信源熵和条件熵的公式，这在前面的专栏内容中有所涉及，在此就不重复了。</p>\n<p>信息增益的作用可以用下面的实例来定性说明。</p>\n<p>在银行发放贷款时，会根据申请人的特征决定是否发放。假设在贷款申请的训练数据中，每个样本都包含年龄、是否有工作、是否有房产、信贷情况等特征，并根据这些特征确定是否同意贷款。一种极端的情形是申请人是否有房产的属性取值和是否同意贷款的分类结果完全吻合，即在训练数据中，每个有房的申请人都对应同意贷款，而每个没房的申请人都对应不同意贷款。这种情况下，“是否有房产”这个特征就具有最大的信息增益，它完全消除了分类结果的不确定性。在处理测试实例时，只要根据这个特征就可以确定分类结果，甚至无需考虑其他特征的取值。</p>\n<p>相比之下，另一种极端的情形是申请人的年龄和是否同意贷款的分类结果可能完全无关，即在训练数据中，青年/中年/老年每个年龄段内，同意贷款与不同意贷款的样本数目都大致相等。这相当于分类结果在年龄特征每个取值上都是随机分布的，两者之间没有任何规律可言。这种特征的信息增益很小，也不具备分类能力。一般来说，抛弃这样的特征对决策树学习精度的影响不大。</p>\n<p>在最早提出的决策树算法——<strong>ID3算法</strong>中，决策树的生成就利用信息增益准则选择特征。ID3算法构建决策树的具体方法是从根节点出发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点都递归调用以上算法生成新的子节点，直到信息增益都很小或没有特征可以选择为止。</p>\n<p>ID3算法使用的是信息增益的绝对取值，而信息增益的运算特性决定了当属性的可取值数目较多时，其信息增益的绝对值将大于取值较少的属性。这样一来，如果在决策树的初始阶段就进行过于精细的分类，其泛化能力就会受到影响，无法对真实的实例做出有效预测。</p>\n<p>为了避免信息增益准则对多值属性的偏好，ID3算法的提出者在其基础上提出了改进版，也就是<strong> C4.5算法</strong>。C4.5算法不是直接使用信息增益，而是引入“信息增益比”指标作为最优划分属性选择的依据。信息增益比等于使用属性的特征熵归一化后的信息增益，而每个属性的特征熵等于按属性取值计算出的信息熵。在特征选择时，C4.5算法先从候选特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的作为节点特征，这就保证了对多值属性和少值属性一视同仁。在决策树的生成上，C4.5算法与ID3算法类似。</p>\n<p><strong>无论是ID3算法还是C4.5算法，都是基于信息论中熵模型的指标实现特征选择，因而涉及大量的对数计算。另一种主要的决策树算法CART算法则用基尼系数取代了熵模型</strong>。</p>\n<p>CART算法的全称是分类与回归树（Classification and Regression Tree），既可以用于分类也可以用于回归。假设数据中共有$K$个类别，第$k$个类别的概率为$p_k$，则基尼系数等于$1 - \\sum\\limits_{i = 0}^K p_k^2$。基尼系数在与熵模型高度近似的前提下，避免了对数运算的使用，使得CART分类树具有较高的执行效率。</p>\n<p>为了进一步简化分类模型，CART分类树算法每次只对某个特征的值进行二分而非多分，最终生成的就是<strong>二叉树模型</strong>。因而在计算基尼系数时，需要对每个特征找到使基尼系数最小的最优切分点，在树生成时根据最优特征和最优切分点生成两个子节点，将训练数据集按照特征分配到子节点中去。如果在某个特征$A$上有$A_1$、$A_2$和$A_3$三种类别，CART分类树在进行二分时就会考虑{$A_1$}/{$A_2, A_3$}、{$A_2$}/{$ A_1, A_3$}、{$A_3$}/{$ A_1, A_2$}这三种分类方法，从中找到基尼系数最小的组合。</p>\n<p><strong>同其他机器学习算法一样，决策树也难以克服过拟合的问题，“剪枝”是决策树对抗过拟合的主要手段</strong>。</p>\n<p>园丁给树苗剪枝是为了让树形完好，<strong>决策树剪枝则是通过主动去掉分支以降低过拟合的风险，提升模型的泛化性能</strong>。</p>\n<p>那么如何判定泛化性能的提升呢？其方法是<strong>定义决策树整体的损失函数并使之极小化，这等价于使用正则化的最大似然估计进行模型选择</strong>。另一种更简单的方法是在训练数据集中取出一部分用于模型验证，根据验证集分类精度的变化决定是否进行剪枝。</p>\n<p><strong>决策树的剪枝策略可以分为预剪枝和后剪枝</strong>。</p>\n<p>预剪枝是指在决策树的生成过程中，在划分前就对每个节点进行估计，如果当前节点的划分不能带来泛化性能的提升，就直接将当前节点标记为叶节点。</p>\n<p>预剪枝的好处在于禁止欠佳节点的展开，在降低过拟合风险的同时显著减少了决策树的时间开销。但它也会导致“误伤”的后果，某些分支虽然当前看起来没用，在其基础上的后续划分却可能让泛化性能显著提升，预剪枝策略将这些深藏不露的节点移除，无疑会矫枉过正，带来欠拟合的风险。</p>\n<p>相比之下，后剪枝策略是先从训练集生成一棵完整的决策树，计算其在验证集上的分类精度，再在完整决策树的基础上剪枝，通过比较剪枝前和剪枝后的分类精度决定分支是否保留。和预剪枝相比，后剪枝策略通常可以保留更多的分支，其欠拟合的风险较小。但由于需要逐一考察所有内部节点，因而其训练开销较大。</p>\n<p>以上的决策树算法虽然结构上简单直观，逻辑上容易解释，但一个重要缺点是“一言堂”，即只依据一个最优特征执行分类决策。</p>\n<p>在实际问题中，分类结果通常会受到多个因素的影响，因而需要对不同特征综合考虑。<strong>依赖多个特征进行分类决策的就是多变量决策树</strong>。在特征空间上，单变量决策树得到的分类边界是与坐标轴平行的分段，多变量决策树的分类边界则是斜线的形式。受篇幅所限，关于多变量决策树的内容在此不做赘述。</p>\n<p>今天我和你分享了机器学习基本算法之一的决策树的基本原理，其要点如下：</p>\n<ul>\n<li>决策树是包含根节点、内部节点和叶节点的树结构，通过判定不同属性的特征来解决分类问题；</li>\n<li>决策树的学习过程包括特征选择、决策树生成、决策树剪枝三个步骤；</li>\n<li>决策树生成的基础是特征选择，特征选择的指标包括信息增益、信息增益比和基尼系数；</li>\n<li>决策树的剪枝策略包括预剪枝和后剪枝。</li>\n</ul>\n<p>在对决策树算法的分析中，自始至终都没有涉及似然概率和后验概率这些之前频繁出现的概念，但这并不意味着决策树算法与概率论完全无关。如何从概率角度看待决策树呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/1e/d9/1e291ba6ca4b799c186f5faf0d084dd9.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"10 机器学习 | 衍化至繁：逻辑回归","id":1867},"right":{"article_title":"12 机器学习 | 穷则变，变则通：支持向量机","id":2028}}},{"article_id":2028,"article_title":"12 机器学习 | 穷则变，变则通：支持向量机","article_content":"<p>1963年，在前苏联莫斯科控制科学学院攻读统计学博士学位的弗拉基米尔·瓦普尼克和他的同事阿列克谢·切尔沃宁基斯共同提出了支持向量机算法，随后几年两人又在此基础上进一步完善了统计学习理论。可受当时国际环境的影响，这些以俄文发表的成果并没有得到西方学术界的重视。直到1990年，瓦普尼克随着移民潮到达美国，统计学习理论才得到了它应有的重视，并在二十世纪末大放异彩。瓦普尼克本人也于2014年加入Facebook的人工智能实验室，并获得了包括罗森布拉特奖和冯诺伊曼奖章等诸多个人荣誉。</p>\n<p>具体说来，<strong>支持向量机是一种二分类算法，通过在高维空间中构造超平面实现对样本的分类</strong>。最简单的情形是训练数据线性可分的情况，此时的支持向量机就被弱化为线性可分支持向量机，这可以视为广义支持向量机的一种特例。</p>\n<p>线性可分的数据集可以简化为二维平面上的点集。在平面直角坐标系中，如果有若干个点全部位于$x$轴上方，另外若干个点全部位于$x$轴下方，这两个点集就共同构成了一个线性可分的训练数据集，而$x$轴就是将它们区分开来的一维超平面，也就是直线。</p>\n<p>如果在上面的例子上做进一步的假设，假定$x$轴上方的点全部位于直线$y = 1$上及其上方，$x$轴下方的点全部位于直线$y = -2$上及其下方。如此一来，任何平行于$x$轴且在(-2, 1)之间的直线都可以将这个训练集分开。那么问题来了：在这么多划分超平面中，哪一个是最好的呢？</p>\n<!-- [[[read_end]]] -->\n<p>直观看来，最好的分界线应该是直线$y = -0.5$，因为这条分界线正好位于两个边界的中间，与两个类别的间隔可以同时达到最大。当训练集中的数据因噪声干扰而移动时，这个最优划分超平面的划分精确度所受的影响最小，因而具有最强的泛化能力。</p>\n<p>在高维的特征空间上，划分超平面可以用简单的线性方程描述</p>\n<p>$$\\mathbf{w}^T \\cdot \\mathbf{x} + b = 0$$</p>\n<p>式中的$n$维向量$\\mathbf{w}$为<strong>法向量</strong>，决定了超平面的方向；$b$为<strong>截距</strong>，决定了超平面和高维空间中原点的距离。划分超平面将特征空间分为两个部分。位于法向量所指向一侧的数据被划分为正类，其分类标记$y = +1$；位于另一侧的数据被划分为负类，其分类标记$y = -1$。<strong>线性可分支持向量机就是在给定训练数据集的条件下，根据间隔最大化学习最优的划分超平面的过程</strong>。</p>\n<p>给定超平面后，特征空间中的样本点$\\mathbf{x}_i$到超平面的距离可以表示为</p>\n<p>$$ r = \\dfrac{\\mathbf{w}^T \\cdot \\mathbf{x} + b}{|| \\mathbf{w} ||} $$</p>\n<p>显然，这个距离是个归一化的距离，因而被称为<strong>几何间隔</strong>。结合前文的描述，通过合理设置参数$\\mathbf{w}$和$b$，可以使每个样本点到最优划分超平面的距离都不小于-1，即满足以下关系 </p>\n<p>$$\\mathbf{w}^T \\cdot \\mathbf{x}_i + b \\ge 1, y_i = +1$$\n$$\\mathbf{w}^T \\cdot \\mathbf{x}_i + b \\le -1, y_i = -1$$</p>\n<p>需要注意的是，式中的距离是非归一化的距离，被称为<strong>函数间隔</strong>。函数间隔和几何间隔的区别就在于未归一化和归一化的区别。</p>\n<p>在特征空间中，距离划分超平面最近的样本点能让上式取得等号，这些样本被称为“<strong>支持向量</strong>”，两个异类支持向量到超平面的距离之和为$2 / || \\mathbf{w} ||$。因而对于线性可分支持向量机来说，其任务就是在满足上面不等式的条件下，寻找$2 / || \\mathbf{w} ||$的最大值。由于最大化$|| \\mathbf{w} || ^ {-1}$等效于最小化$|| \\mathbf{w} || ^ 2$，因而上述问题可以改写为求解$\\dfrac{1}{2} || \\mathbf{w} || ^ 2$的最小值。</p>\n<p><strong>线性可分支持向量机是使硬间隔最大化的算法</strong>。在实际问题中，训练数据集中通常会出现噪声或异常点，导致其线性不可分。要解决这个问题就需要将学习算法一般化，得到的就是<strong>线性支持向量机</strong>(去掉了“可分”条件)。</p>\n<p><strong>线性支持向量机的通用性体现在将原始的硬间隔最大化策略转变为软间隔最大化</strong>。在线性不可分的训练集中，导致不可分的只是少量异常点，只要把这些异常点去掉，余下的大部分样本点依然满足线性可分的条件。</p>\n<p>从数学上看，线性不可分意味着某些样本点距离划分超平面的函数间隔不满足不小于1的约束条件，因而需要对每个样本点引入大于零的松弛变量$\\xi \\ge 0$，使得函数间隔和松弛变量的和不小于1。这样一来，软间隔最大化下的约束条件就变成 </p>\n<p>$$\\mathbf{w}^T \\mathbf{x}_i + b \\ge 1 - \\xi_i, y_i = +1$$\n $$\\mathbf{w}^T \\mathbf{x}_i + b \\le 1 - \\xi_i, y_i = -1$$</p>\n<p>相应地，最优化问题中的目标函数就演变为$\\dfrac{1}{2} || \\mathbf{w} || ^ 2 + C\\sum\\limits_{i = 1}^N \\xi_i$，其中$C &gt; 0$被称为<strong>惩罚参数</strong>，表示对误分类的惩罚力度。这个最小化目标函数既使函数间隔尽量大，也兼顾了误分类点的个数。与要求所有样本都划分正确的硬间隔相比，软间隔允许某些样本不满足硬间隔的约束，但会限制这类特例的数目。</p>\n<p>前文中涉及的分类问题都假定两类数据点可以用原始特征空间上的超平面区分开来，这类问题就是线性问题；如果原始空间中不存在能够正确划分的超平面，问题就演变成了非线性问题。在二维平面直角坐标系中，如果按照与原点之间的距离对数据点进行分类的话，分类的模型就不再是一条直线，而是一个圆，也就是<strong>超曲面</strong>。这个问题就是个非线性问题，与距离的平方形式相呼应。</p>\n<p><strong>不论是线性可分支持向量机还是线性支持向量机，都只能处理线性问题，对于非线性问题则无能为力</strong>。可如果能将样本从原始空间映射到更高维度的特征空间之上，在新的特征空间上样本就可能是线性可分的。如果样本的属性数有限，那么一定存在一个高维特征空间使样本可分。<strong>将原始低维空间上的非线性问题转化为新的高维空间上的线性问题，这就是核技巧的基本思想</strong>。</p>\n<p>核技巧的例子可以用中国象棋来解释。开战之前，红黑两军各自陈兵，不越雷池一步，只需楚河汉界便可让不同的棋子泾渭分明。可随着车辚辚马萧萧激战渐酣，红黑棋子捉对厮杀，难分敌我，想要再造一条楚河汉界将混在一起的两军区分开已无可能。</p>\n<p>好在我们还有高维空间。不妨把两军之争想象成一场围城大战，红帅率兵于高墙坚守，黑将携卒在低谷强攻。如此一来，不管棋盘上的棋子如何犬牙交错，只要能够在脑海中构造出一个立体城池，便可以根据位置的高低将红黑区分开来。这，就是棋盘上的核技巧。</p>\n<p>当核技巧应用到支持向量机中时，原始空间与新空间之间的转化是通过非线性变换实现的。假设原始空间是<strong>低维欧几里得空间</strong>$\\mathcal{X}$，新空间为<strong>高维希尔伯特空间</strong>$\\mathcal{H}$，则从$\\mathcal{X}$到$\\mathcal{H}$的映射可以用函数$\\phi (x) : \\mathcal{X} \\rightarrow \\mathcal{H}$表示。核函数可以表示成映射函数内积的形式，即 </p>\n<p>$$ K(x, z) = \\phi (x) \\cdot \\phi (z) $$</p>\n<p><strong>核函数有两个特点</strong>。第一，其计算过程是在低维空间上完成的，因而避免了高维空间（可能是无穷维空间）中复杂的计算；第二，对于给定的核函数，高维空间$\\mathcal{H}$和映射函数$\\phi$的取法并不唯一。一方面，高维空间的取法可以不同；另一方面，即使在同一个空间上，映射函数也可以有所区别。</p>\n<p>核函数的使用涉及一些复杂的数学问题，其结论是<strong>一般的核函数都是正定核函数</strong>。正定核函数的充要条件是由函数中任意数据的集合形成的核矩阵都是半正定的，这意味着任何一个核函数都隐式定义了一个成为“再生核希尔伯特空间”的特征空间，其中的数学推导在此不做赘述。<strong>在支持向量机的应用中，核函数的选择是一个核心问题</strong>。不好的核函数会将样本映射到不合适的特征空间，从而导致分类性能不佳。常用的核函数包括以下几种：</p>\n<ul>\n<li><strong>线性核</strong>：$K(\\mathbf{X}, \\mathbf{Y}) = \\mathbf{X} ^ T \\mathbf{Y}$</li>\n<li><strong>多项式核</strong>：$K(\\mathbf{X}, \\mathbf{Y}) = (\\mathbf{X} ^ T \\mathbf{Y} + c) ^ d$，$c$为常数，$d \\ge 1$为多项式次数</li>\n<li><strong>高斯核</strong>：$K(\\mathbf{X}, \\mathbf{Y}) = \\exp (-\\dfrac{|| \\mathbf{X} - \\mathbf{Y} || ^ 2}{2\\sigma ^ 2})$，$\\sigma &gt; 0$为高斯核的带宽</li>\n<li><strong>拉普拉斯核</strong>：$K(\\mathbf{X}, \\mathbf{Y}) = \\exp (-\\dfrac{|| \\mathbf{X} - \\mathbf{Y} ||}{\\sigma})$，$\\sigma &gt; 0$</li>\n<li><strong>Sigmoid核</strong>：$K(\\mathbf{X}, \\mathbf{Y}) = \\tanh (\\beta \\mathbf{X} ^ T \\mathbf{Y} + \\theta)$，$\\beta &gt; 0, \\theta &lt; 0$</li>\n</ul>\n<p><strong>核函数可以将线性支持向量机扩展为非线性支持向量机</strong>。非线性支持向量机的约束条件比较复杂，受篇幅所限，在此没有给出，你可以从相关书籍中查阅。非线性支持向量机是最普适的情形，前文中的两种情况都可以视为非线性支持向量机的特例，因而通常的支持向量机算法并不区分是否线性可分。</p>\n<p>至此，我按照从简单到复杂的顺序，向你介绍了支持向量机三种模型的原理。在实际的应用中，对最优化目标函数的求解需要应用到最优化的理论，在这里对其思路加以简单说明。如果将支持向量机的最优化作为原始问题，应用最优化理论中的拉格朗日对偶性，可以通过求解其对偶问题得到原始问题的最优解。<strong>三种模型的参数学习都可以转化为对偶问题的求解</strong>。</p>\n<p>在算法实现的过程中，支持向量机会遇到在大量训练样本下，全局最优解难以求得的尴尬。目前，高效实现支持向量机的主要算法是<strong>SMO算法</strong>（Sequential Minimal Optimization，序列最小最优化）。支持向量机的学习问题可以形式化为凸二次规划问题的求解，SMO算法的特点正是不断将原始的二次规划问题分解为只有两个变量的二次规划子问题，并求解子问题的解析解，直到所有变量满足条件为止。</p>\n<p>支持向量机的一个重要性质是当训练完成后，最终模型只与支持向量有关，这也是“支持向量机”这个名称的来源。正如发明者瓦普尼克所言：<strong>支持向量机这个名字强调了这类算法的关键是如何根据支持向量构建出解，算法的复杂度也主要取决于支持向量的数目</strong>。</p>\n<p>今天我和你分享了机器学习基本算法之一的支持向量机的基本原理，其要点如下：</p>\n<ul>\n<li>线性可分支持向量机通过硬间隔最大化求出划分超平面，解决线性分类问题；</li>\n<li>线性支持向量机通过软间隔最大化求出划分超平面，解决线性分类问题；</li>\n<li>非线性支持向量机利用核函数实现从低维原始空间到高维特征空间的转换，在高维空间上解决非线性分类问题；</li>\n<li>支持向量机的学习是个凸二次规划问题，可以用SMO算法快速求解。</li>\n</ul>\n<p>支持向量机主要用于解决分类任务，那么它能否推而广之，用于解决回归任务呢？在回归任务中，支持向量又应该如何表示呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e0/52/e0dccef2f1529f49b9c981ec2a3d4352.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"11 机器学习 | 步步为营，有章可循：决策树","id":2008},"right":{"article_title":"13 机器学习 | 三个臭皮匠，赛过诸葛亮：集成学习","id":2030}}},{"article_id":2030,"article_title":"13 机器学习 | 三个臭皮匠，赛过诸葛亮：集成学习","article_content":"<p>在无线通信中，有一种广受欢迎的“MIMO”传输技术。MIMO的全称是多输入多输出（Multiple Input Multiple Output），其早期配置是在发送端和接收端同时布置多个发射机和多个接收机，每个发射机发送相同的信号副本，而每个接收机则接收到来自多个发射机的不同信号，这些信号经历的衰减是相互独立的。这样一来，在接收端多个信号同时被严重衰减的概率就会以指数形式减小，通过获得分集增益带来误码率的下降与信道容量的提升。</p>\n<p>无线通信中的分集思想在机器学习中的对应就是集成学习。<strong>集成学习正是使用多个个体学习器来获得比每个单独学习器更好的预测性能</strong>。</p>\n<p>监督学习的任务是通过假设空间搜索来找到能够对特定问题给出良好预测的假设。但问题是即使这样的假设存在，能否找到也在两可之间。因而集成学习的作用就是将多个可以得到假设整合为单个更好的假设，其一般结构是先产生一组个体学习器，再使用某种策略将它们加以整合。每个组中的个体学习器如果属于同一类型（比如都是线性回归或者都是决策树），形成的就是<strong>同质集成</strong>；相应地，由不同类型学习器得到的集成则称为<strong>异质集成</strong>。</p>\n<p>直观来看，性能优劣不一的个体学习器放在一块儿可能产生的是更加中庸的效果，即比最差的要好，也比最好的要差。那么集成学习如何实现“1 + 1 &gt; 2”呢？这其实是对个体学习器提出了一些要求。</p>\n<!-- [[[read_end]]] -->\n<p><strong>一方面，个体学习器的性能要有一定的保证</strong>。如果每个个体学习器的分类精度都不高，在集成时错误的分类结果就可能占据多数，导致集成学习的效果甚至会劣于原始的个体学习器，正如俗语所言“和臭棋手下棋，越下越臭”。</p>\n<p><strong>另一方面，个体学习器的性能要有一定的差异，和而不同才能取得进步</strong>。多样性（diversity）是不同的个体学习器性能互补的前提，这恰与MIMO中分集（diversity）的说法不谋而合。</p>\n<p>在MIMO中，一个重要的前提条件是不同信号副本传输时经历的衰减要相互独立。同样的原则在机器学习中体现为<strong>个体学习器的误差相互独立</strong>。但由于个体学习器是为了解决相同问题训练出来的，要让它们的性能完全独立着实是勉为其难。尤其是当个体学习器的准确性较高时，要获得多样性就不得不以牺牲准确性作为代价。由此，<strong>集成学习的核心问题在于在多样性和准确性间做出折中，进而产生并结合各具优势的个体学习器</strong>。</p>\n<p><strong>个体学习器的生成方式很大程度上取决于数据的使用策略</strong>。根据训练数据使用方法的不同，<strong>集成学习方法可以分为两类：个体学习器间存在强依赖关系因而必须串行生成的序列化方法，和个体学习器之间不存在强依赖关系因而可以同时生成的并行化方法</strong>。</p>\n<p>序列化方法中的数据使用机制被称为<strong>提升（Boosting）</strong>，其基本思路是对所有训练数据进行多次重复应用，每次应用前需要对样本的概率分布做出调整，以达到不同的训练效果。</p>\n<p>与Boosting相比，并行化方法中的数据使用机制是将原始的训练数据集拆分成若干互不交叠的子集，再根据每个子集独立地训练出不同的个体学习器。这种方法被称为<strong>自助聚合</strong>（Bootstrap AGgregation），简称<strong>打包（Bagging）</strong>。在Bagging机制中，不同个体学习器之间的多样性容易得到保证；但由于每个个体学习器只能使用一小部分数据进行学习，其效果就容易出现断崖式下跌。</p>\n<p>在基于训练数据集生成样本的子集时，Bagging采用的是放回抽样的策略，即某些样本可能出现在不同的子集之中，而另外某些样本可能没有出现在任何子集之内。计算未被抽取概率的极限可以得到，放回抽样会导致36.8%的训练数据没有出现在采样数据集中。这些未使用的数据没有参与个体学习器的训练，但可以作为验证数据集，用于对学习器的泛化性能做出包外估计，包外估计得到的泛化误差已被证明是真实值的无偏估计。</p>\n<p><strong>典型的序列化学习算法是自适应提升方法（Adaptive Boosting），人送绰号AdaBoost</strong>。在解决分类问题时，提升方法遵循的是循序渐进的原则。<strong>先通过改变训练数据的权重分布，训练出一系列具有粗糙规则的弱个体分类器，再基于这些弱分类器进行反复学习和组合，构造出具有精细规则的强分类器</strong>。从以上的思想中不难看出，<strong>AdaBoost要解决两个主要问题：训练数据权重调整的策略和弱分类器结果的组合策略</strong>。</p>\n<p><strong>在训练数据的权重调整上，AdaBoost采用专项整治的方式</strong>。在每一轮训练结束后，提高分类错误的样本权重，降低分类正确的样本权重。因此在下一轮次的训练中，弱分类器就会更加重视错误样本的处理，从而得到性能的提升。这就像一个学生在每次考试后专门再把错题重做一遍，有针对性地弥补不足。虽然训练数据集本身没有变化，但不同的权重使数据在每一轮训练中发挥着不同的作用。</p>\n<p><strong>在AdaBoost的弱分类器组合中，每一轮得到的学习器结果都会按照一定比例叠加到前一轮的判决结果，并参与到下一轮次权重调整之后的学习器训练中</strong>。当学习的轮数达到预先设定的数目$T$时，最终分类器的输出就是$T$个个体学习器输出的线性组合。每个个体学习器在最终输出的权重与其分类错误率相关，个体学习器中的分类错误率越低，其在最终分类器中起到的作用就越大。但需要注意的是，所有个体学习器权重之和并不必须等于1。</p>\n<p>根据以上的主要策略，可以归纳出算法的特点。<strong>随着训练过程的深入，弱学习器的训练重心逐渐被自行调整到的分类器错误分类的样本上，因而每一轮次的模型都会根据之前轮次模型的表现结果进行调整，这也是AdaBoost的名字中“自适应”的来源</strong>。</p>\n<p>前面介绍的是AdaBoost的执行策略。换个视角来看，<strong>AdaBoost可以视为使用加法模型，以指数函数作为损失函数，使用前向分步算法的二分类学习方法</strong>。加法模型反映出AdaBoost以个体学习器的线性组合作为最终分类器的特性。在这个模型下求解指数型损失函数的最小值是个复杂的问题，但可以通过每次只学习线性组合其中的一项来简化其处理，这种方法就是前向分步算法。前向分步算法注意学习基函数的过程和AdaBoost注意学习个体学习器的过程是一致的。</p>\n<p>典型的并行化学习方法是<strong>随机森林方法</strong>。正所谓“独木不成林”，随机森林就是对多个决策树模型的集成。“随机”的含义体现在两方面：一是每个数据子集中的样本是在原始的训练数据集中随机抽取的，这在前文中已有论述；二是在决策树生成的过程中引入了随机的属性选择。<strong>在随机森林中，每棵决策树在选择划分属性时，首先从结点的属性集合中随机抽取出包含$k$个属性的一个子集，再在这个子集中选择最优的划分属性生成决策树</strong>。</p>\n<p>为什么要执行随机的属性选择呢？其目的在于保证不同基决策树之间的多样性。如果某一个或几个属性对输出的分类结果有非常强的影响，那么很可能所有不同的个体决策树都选择了这些属性，这将导致不同子集上训练出个体决策树呈现出众口一辞的同质性，对原始训练样本的有放回随机抽取也就失去了意义。在这个意义上，随机特征选取是对集成学习算法中多样性的一重保护措施。</p>\n<p>在合成策略上，随机森林通常采用少数服从多数的策略，选择在个体决策树中出现最多的类别标记作为最终的输出结果。当数据较多时，也可以采用更加强大的学习法，即通过另一个单独的学习器实现复杂合成策略的学习。随机森林是罕有的具有强通用性的机器学习方法，能够以较小的计算开销在多种现实任务中展现出强大的性能。</p>\n<p><strong>数据使用机制的不同在泛化误差的构成上也有体现</strong>。</p>\n<p>以Boosting方法为代表的序列化方法使用了全体训练数据，并根据每次训练的效果不断迭代以使损失函数最小化，因而可以降低平均意义上的偏差，能够基于泛化能力较弱的学习器构建出较强的集成。</p>\n<p>以Bagging方法为代表的并行化方法则利用原始训练数据生成若干子集，因而受异常点的影响较小，对在每个子集上训练出的不完全相关的模型取平均也有助于平衡不同模型之间的性能，因而可以一定程度上降低方差。</p>\n<p>今天我和你分享了机器学习基本算法之一的集成学习的基本原理，其要点如下：</p>\n<ul>\n<li>集成学习使用多个个体学习器来获得比每个单独学习器更好的预测性能，包括序列化方法和并行化方法两类；</li>\n<li>多样性要求集成学习中的不同个体学习器之间具有足够的差异性；</li>\n<li>序列化方法采用Boosting机制，通过重复使用概率分布不同的训练数据实现集成，可以降低泛化误差中的偏差；</li>\n<li>并行化方法采用Bagging机制，通过在训练数据中多次自助抽取不同的采样子集实现集成，可以降低泛化误差中的方差。</li>\n</ul>\n<p>Boosting机制和Bagging机制各具特色，在集成学习中发挥着不同的作用。那么能否将两者结合起来，达到互补的效果呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/f9/06/f9cb9acf82ea46e7c07df95c18602a06.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"12 机器学习 | 穷则变，变则通：支持向量机","id":2028},"right":{"article_title":"14 机器学习 | 物以类聚，人以群分：聚类分析","id":2196}}},{"article_id":2196,"article_title":"14 机器学习 | 物以类聚，人以群分：聚类分析","article_content":"<p>20世纪40年代，美国心理学家罗伯特·泰昂和雷蒙德·卡泰尔借鉴人类学中的研究方法，提出“聚类分析”的概念，通过从相关矩阵中提取互相关的成分进行性格因素的研究。随着时间的推移，聚类分析的应用范围越来越广泛，逐渐演化成一种主要的机器学习方法。</p>\n<p><strong>聚类分析是一种无监督学习方法，其目标是学习没有分类标记的训练样本，以揭示数据的内在性质和规律</strong>。具体来说，聚类分析要将数据集划分为若干个互不相交的子集，每个子集中的元素在某种度量之下都与本子集内的元素具有更高的相似度。</p>\n<p>用这种方法划分出的子集就是“<strong>聚类</strong>”（或称为“簇”），每个聚类都代表了一个潜在的类别。分类和聚类的区别也正在于此：<strong>分类是先确定类别再划分数据；聚类则是先划分数据再确定类别</strong>。</p>\n<p>聚类分析本身并不是具体的算法，而是要解决的一般任务，从名称就可以看出这项任务的两个核心问题：<strong>一是如何判定哪些样本属于同一“类”，二是怎么让同一类的样本“聚”在一起</strong>。</p>\n<p><strong>解决哪些样本属于同一“类”的问题需要对相似性进行度量</strong>。无论采用何种划定标准，聚类分析的原则都是让类内样本之间的差别尽可能小，而类间样本之间的差别尽可能大。<strong>度量相似性最简单的方法就是引入距离测度</strong>，聚类分析正是通过计算样本之间的距离来判定它们是否属于同一个“类”。根据线性代数的知识，如果每个样本都具有$N$个特征，那就可以将它们视为$N$维空间中的点，进而计算不同点之间的距离。</p>\n<!-- [[[read_end]]] -->\n<p>作为数学概念的距离需要满足<strong>非负性</strong>（不小于0）、<strong>同一性</strong>（任意点与其自身之间的距离为0）、<strong>对称性</strong>（交换点的顺序不改变距离）、<strong>直递性</strong>（满足三角不等式）等性质。在聚类分析中常用的距离是“<strong>闵可夫斯基距离</strong>”，其定义为 </p>\n<p>$$ \\text{dist}_{\\text{mk}} (\\mathbf{x}_i, \\mathbf{x}_j) = (\\sum\\limits_{n = 1}^N | x_{in} - x_{jn} | ^ p) ^ {\\frac{1}{p}} $$ </p>\n<p>式中的$p$是个常数。当$p = 2$时，闵可夫斯基距离就变成了<strong>欧式距离</strong>，也就是通常意义上的长度。</p>\n<p><strong>确定了“类”的标准之后，接下来就要考虑如何让同一类的样本“聚”起来，也就是聚类算法的设计</strong>。目前，公开发表的聚类算法已经超过了100种，但无监督学习的特性决定了不可能存在通用的聚类算法。根据应用场景的差别，不同算法在组成聚类的概念以及有效找到聚类的方法上也会有所区别。针对某种模型设计的算法在其他模型的数据集上表现的一塌糊涂也不是什么新鲜事。接下来我介绍的就是几类最主要的聚类算法及其实例。</p>\n<p><strong>层次聚类又被称为基于连接的聚类，其核心思想源于样本应当与附近而非远离的样本具有更强的相关性</strong>。由于聚类生成的依据是样本之间的距离，因而聚类的特性可以用聚类内部样本之间的距离尺度来刻画。聚类的划分是在不同的距离水平上完成的，划分过程就可以用<strong>树状图</strong>来描述，这也解释了&quot;层次聚类&quot;这个名称的来源。</p>\n<p><strong>层次聚类对数据集的划分既可以采用自顶向下的拆分策略，也可以采用自底向上的会聚策略，后者是更加常用的方法</strong>。在采用会聚策略的层次聚类算法中，数据集中的每个样本都被视为一个初始聚类。算法每执行一步，就将距离最近的两个初始聚类合并，这个过程将一直重复到合并后的数目达到预设数为止。</p>\n<p>根据距离计算方式的不同，会聚算法可以分为<strong>单链接算法、全链接算法和均链接算法</strong>等。单链接算法利用的是最小距离，它由两个不同聚类中相距最近的样本决定；全链接算法利用的是最大距离，它由两个不同聚类中相距最远的样本决定；均链接算法利用的是平均距离，它由两个不同聚类中的所有样本共同决定。虽然存在特定的快速算法，但会聚策略的计算复杂度依然较高。相比之下，拆分策略更加复杂，在此就不做介绍了。</p>\n<p><strong>原型聚类又被称为基于质心的聚类，其核心思想是每个聚类都可以用一个质心表示</strong>。原型聚类将给定的数据集初始分裂为若干聚类，每个聚类都用一个中心向量来刻画，然后通过反复迭代来调整聚类中心和聚类成员，直到每个聚类不再变化为止。</p>\n<p><strong>k均值算法是典型的原型聚类算法，它将聚类问题转化为最优化问题</strong>。具体做法是先找到$k$个聚类中心，并将所有样本分配给最近的聚类中心，分配的原则是让所有样本到其聚类中心的距离平方和最小。显然，距离平方和越小意味着每个聚类内样本的相似度越高。但是这个优化问题没有办法精确求解，因而只能搜索近似解。$k$均值算法就是<strong>利用贪心策略</strong>，通过迭代优化来近似求解最小平方和的算法。</p>\n<p>k均值算法的计算过程非常直观。首先从数据集中随机选取$k$个样本作为$k$个聚类各自的中心，接下来对其余样本分别计算它们到这$k$个中心的距离，并将样本划分到离它最近的中心所对应的聚类中。当所有样本的聚类归属都确定后，再计算每个聚类中所有样本的算术平均数，作为聚类新的中心，并将所有样本按照$k$个新的中心重新聚类。这样，<strong>“取平均-重新计算中心-重新聚类”的过程将不断迭代，直到聚类结果不再变化为止</strong>。</p>\n<p>大多数k均值类型的算法需要预先指定聚类的数目$k$，这是算法为人诟病的一个主要因素。此外，由于算法优化的对象是每个聚类的中心，因而k均值算法倾向于划分出相似大小的聚类，这会降低聚类边界的精确性。</p>\n<p><strong>分布聚类又被称为基于概率模型的聚类，其核心思想是假定隐藏的类别是数据空间上的一个分布</strong>。在分布聚类中，每个聚类都是最可能属于同一分布的对象的集合。这种聚类方式类似于数理统计中获得样本的方式，也就是每个聚类都由在总体中随机抽取独立同分布的样本组成。其缺点则在于无法确定隐含的概率模型是否真的存在，因而常常导致过拟合的发生。</p>\n<p>聚类分析的任务原本是将数据划分到不同的聚类中，可如果我们将样本看作观察值，将潜在类别看作隐藏变量，那么就可以逆向认为数据集是由不同的聚类产生的。在这个前提下，基于概率模型的聚类分析的任务是推导出最可能产生出已有数据集的$k$个聚类，并度量这$k$个聚类产生已有数据集的似然概率。</p>\n<p><strong>基于概率模型的聚类实质上就是进行参数估计，估计出聚类的参数集合以使似然函数最大化</strong>。<strong>期望极大算法（Expectation Maximization algorithm）是典型的基于概率模型的聚类方法</strong>。EM算法是一种迭代算法，用于含有不可观察的隐变量的概率模型中，在这种模型下对参数做出极大似然估计，因而可以应用在聚类分析之中。</p>\n<p><strong>EM算法执行的过程包括“期望”和“最大化”两个步骤</strong>。将待估计分布的参数值随机初始化后，期望步骤利用初始参数来估计样本所属的类别；最大化步骤利用估计结果求取当似然函数取得极大值时，对应参数值的取值。两个步骤不断迭代，直到收敛为止。</p>\n<p><strong>密度聚类又被称为基于密度的聚类，其核心思想是样本分布的密度能够决定聚类结构</strong>。每个样本集中分布的区域都可以看作一个聚类，聚类之间由分散的噪声点区分。密度聚类算法根据样本密度考察样本间的可连接性，再基于可连接样本不断扩展聚类以获得最终结果。</p>\n<p>最流行的基于密度的聚类方法是<strong>利用噪声的基于密度的空间聚类</strong>（Density-Based Spatial Clustering of Applications with Noise），简称 <strong>DBSCAN</strong>。DBSCAN通过将距离在某一个阈值之内的点连接起来而形成聚类，但连接的对象只限于满足密度标准的点。$\\epsilon$ -邻域这一概念给出的对邻域的限制，密度的可连接性则通过密度直达关系、密度可达关系、密度相连关系等一系列标准定义，根据这些概念形成的聚类就是由密度可达关系导出的最大的密度相连样本集合。</p>\n<p>根据思想的不同，聚类方法主要包括以上的四种类型，每种类型上也有其典型算法。受篇幅的限制，关于每种典型算法我并没有展开详细的阐述，你可以查阅相关文献以了解这些算法的细节。除了介绍的这些传统方法之外，现代聚类方法还包括<strong>谱聚类</strong>和<strong>模糊聚类</strong>等等，在此就不做介绍了。</p>\n<p>正所谓“物以类聚，人以群分”，聚类分析的一个重要应用就是用户画像。在商业应用中判别新用户的类型时，往往要先对用户进行聚类分析，再根据聚类分析的结果训练分类模型，用于用户类型的判别。聚类使用的特征通常包括人口学变量、使用场景、行为数据等。</p>\n<p>今天我和你分享了机器学习基本算法之一的聚类分析的基本原理，其要点如下：</p>\n<ul>\n<li>聚类分析是一种无监督学习方法，通过学习没有分类标记的训练样本发现数据的内在性质和规律；</li>\n<li>数据之间的相似性通常用距离度量，类内差异应尽可能小，类间差异应尽可能大；</li>\n<li>根据形成聚类方式的不同，聚类算法可以分为层次聚类、原型聚类、分布聚类、密度聚类等几类；</li>\n<li>聚类分析的一个重要应用是对用户进行分组与归类。</li>\n</ul>\n<p>由于其无监督学习的特性，在评估聚类方法时也就不存在绝对客观的标准。尤其是在数据多样化、高维化、非均衡化的趋势下，评价聚类方法的难度甚至超出了设计聚类方法的难度。那么在评价聚类方法时应该考虑哪些因素呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/be/6f/be9208083ca3c520e1c530efd3b4dd6f.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"13 机器学习 | 三个臭皮匠，赛过诸葛亮：集成学习","id":2030},"right":{"article_title":"15 机器学习 | 好钢用在刀刃上：降维学习","id":2197}}},{"article_id":2197,"article_title":"15 机器学习 | 好钢用在刀刃上：降维学习","article_content":"<p>毛主席在《矛盾论》中提出了主要矛盾和次要矛盾的概念：“研究任何过程，如果是存在着两个以上矛盾的复杂过程的话，就要用全力找出它的主要矛盾。”这种哲学观点也可以用来指导机器学习。</p>\n<p>一个学习任务通常会涉及样本的多个属性，但并非每个属性在问题的解决中都具有同等重要的地位，有些属性可能举足轻重，另一些则可能无关紧要。<strong>根据凡事抓主要矛盾的原则，对举足轻重的属性要给予足够的重视，无关紧要的属性则可以忽略不计，这在机器学习中就体现为降维的操作</strong>。</p>\n<p><strong>主成分分析是一种主要的降维方法，它利用正交变换将一组可能存在相关性的变量转换成一组线性无关的变量，这些线性无关的变量就是主成分</strong>。多属性的大样本无疑能够提供更加丰富的信息，但也不可避免地增加了数据处理的工作量。更重要的是，多数情况下不同属性之间会存在相互依赖的关系，如果能够充分挖掘属性之间的相关性，属性空间的维度就可以降低。</p>\n<p>在现实生活中少不了统计个人信息的场合，而在个人信息的表格里通常会包括“学历”和“学位”两个表项。因为学位和学历代表着两个独立的过程，因此单独列出是没有问题的。但在我国现行的惯例下，这两者通常会一并取得。两者之间的相关性足以让我们根据一个属性的取值去推测另一个属性的取值，因此只要保留其中一个就够了。</p>\n<p>但这样的推测是不是永远准确呢？也不是。如果毕业论文的答辩没有通过，就会出现只有学历而没有学位的情形；对于在职研究生来说，只有学位没有学历的情形也不稀奇。这说明如果将学历和学位完全等同，就会在这些特例上出现错误，也就意味着信息的损失。这是降维操作不可避免的代价。</p>\n<p>以上的例子只是简单的定性描述，说明了降维的出发点和可行性。<strong>在实际的数据操作中，主成分分析解决的就是确定以何种标准确定属性的保留还是丢弃，以及度量降维之后的信息损失</strong>。</p>\n<p>从几何意义来看，主成分分析是要将原始数据拟合成新的$n$维椭球体，这个椭球体的每个轴代表着一个主成分。如果椭球体的某个轴线较短，那么该轴线所代表的主成分的方差也很小。在数据集的表示中省略掉该轴线以及其相应的主成分，只会丢失相当小的信息量。具体说来，主成分分析遵循如下的步骤：</p>\n<ul>\n<li><strong>数据规范化</strong>：对$m$个样本的相同属性值求出算术平均数，再用原始数据减去平均数，得到规范化后的数据；</li>\n<li><strong>协方差矩阵计算</strong>：对规范化后的新样本计算不同属性之间的协方差矩阵，如果每个样本有$n$个属性，得到的协方差矩阵就是$n$维方阵；</li>\n<li><strong>特征值分解</strong>：求解协方差矩阵的特征值和特征向量，并将特征向量归一化为单位向量；</li>\n<li><strong>降维处理</strong>：将特征值按照降序排序，保留其中最大的$k$个，再将其对应的$k$个特征向量分别作为列向量组成特征向量矩阵；</li>\n<li><strong>数据投影</strong>：将减去均值后的$m \\times n$维数据矩阵和由$k$个特征向量组成的$n \\times k$维特征向量矩阵相乘，得到的$m \\times k$维矩阵就是原始数据的投影。</li>\n</ul>\n<p>经过这几步简单的数学运算后，原始的$n$维特征就被映射到新的$k$维特征之上。这些相互正交的新特征就是主成分。需要注意的是，<strong>主成分分析中降维的实现并不是简单地在原始特征中选择一些保留，而是利用原始特征之间的相关性重新构造出新的特征</strong>。</p>\n<p>为什么简单的数学运算能够带来良好的效果呢？</p>\n<!-- [[[read_end]]] -->\n<p>从线性空间的角度理解，主成分分析可以看成将正交空间中的样本点以最小误差映射到一个超平面上。如果这样的超平面存在，那它应该具备以下的性质：一方面，不同样本点在这个超平面上的投影要尽可能地分散；另一方面，所有样本点到这个超平面的距离都应该尽可能小。</p>\n<p>样本点在超平面上的投影尽可能分散体现出的是<strong>最大方差原理</strong>。在信号处理理论中，当信号的均值为零时，方差反映的就是信号的能量，能量越大的信号对抗噪声和干扰的能力也就越强。而让投影后样本点的方差最大化，就是要让超平面上的投影点尽可能地分散。如果原始信号的投影都集中在超平面的同一个区域，不同的信号之间就会难以区分。</p>\n<p>在数学上，投影后所有样本点的方差可以记作$\\sum\\limits_i \\mathbf{W}^T \\mathbf{x}_i \\mathbf{x}_i^T \\mathbf{W}$，式中每个$n$维向量$\\mathbf{x}_i$都代表具有$n$个属性的样本点，$\\mathbf{W}$则是经过投影变换后得到的新坐标系。</p>\n<p>最大方差要求的正是求解最优的$\\mathbf{W}$，以使前面的方差表达式，也就是对应矩阵所有对角线元素的和最大化。经过数学处理后可以得到，使方差最大化的$\\mathbf{W}$就是由所有最大特征值的特征向量组合在一起形成的，也就是主成分分析的解。</p>\n<p>在线性回归中，我向你介绍了最小均方误差的概念，主成分分析的最优性也可以从这个角度来审视。所有样本点到这个超平面的距离都应该尽可能小，意味着这些点到平面距离之和同样最小。原始样本点在低维超平面上的投影的表达式是$\\mathbf{z}_i = (z_{i1}; z_{i2}; \\cdots; z_{ik})$，其中每个$z_{ij} = \\mathbf{w}_j^T \\mathbf{x}_i$是原始样本点$\\mathbf{x}_i$在低维超平面上第$j$维上的坐标。</p>\n<p>因而，原始样本点和在投影超平面上重构出的样本点之间的距离可以表示为$|| \\sum\\limits_{j = 1}^k z_{ij}\\mathbf{w}_j - \\mathbf{x}_i ||_2^2$，在整个训练集上对距离求和并最小化，求出的解就是最小均方误差意义下的最优超平面。经过数学处理后可以得到，使均方误差最小化的$\\mathbf{W}$就是由所有最大特征值的特征向量组合在一起形成的，同样是主成分分析的解。</p>\n<p>在主成分分析中，保留的主成分的数目是由用户来确定的。<strong>一个经验方法是保留所有大于1的特征值，以其对应的特征向量来做坐标变换</strong>。此外，也可以根据不同特征值在整体中的贡献，以一定比例进行保留。具体方法是计算新数据和原始数据之间的误差，令误差和原始数据能量的比值小于某个预先设定的阈值。</p>\n<p>主成分分析能够对数据进行降维处理，保留正交主成分中最重要的部分，在压缩数据的同时最大程度地保持了原有信息。主成分分析的优点在于完全不受参数的限制，即不需要先验的参数或模型对计算过程的人为干预，分析的结果只与数据有关。但有得必有失，这个特点的另一面是即使用户具有对训练数据集的先验知识，也没有办法通过参数化等方法加以利用。</p>\n<p>除此之外，由于主成分分析中利用的是协方差矩阵，因而只能去除线性相关关系，对更加复杂的非线性相关性就无能为力了。<strong>解决以上问题的办法是将支持向量机中介绍过的核技巧引入主成分分析，将先验知识以非线性变换的形式体现，因而扩展了主成分分析的应用范围</strong>。</p>\n<p>主成分分析实现降维的前提是通过正交变换对不同属性进行去相关。另一种更加直观的降维方式则是直接对样本的属性做出筛选，这种降维方法就是“<strong>特征选择</strong>”。<strong>特征选择的出发点在于去除不相关的特征往往能够降低学习任务的难度，它和主成分分析共同构成了处理高维数据的两大主流技术</strong>。</p>\n<p>特征选择与特征提取不同。特征提取是根据原始特征的功能来创建新特征，特征选择则是选取原始特征中的一个子集用于学习任务。特征选择通常用于特征较多而样本较少的问题中，使用特征选择技术的核心前提是数据包含许多冗余和不相关特征，它们可以在不引起信息损失的情况下被移除。特征选择的主要应用场景包括书面文本分析和DNA微阵列数据的分析，这些场景下样本的数目通常数以百计，每个样本却可能包含成千上万的特征。</p>\n<p><strong>特征选择算法是搜索新的特征子集和对搜索结果进行评估两个步骤的组合</strong>。最简单的选择方法是测试每个可能的特征子集，从中找出错误率最小的特征子集。这是一个详尽的空间搜索，但子集的数目会随着特征数目的增加以指数方式增加，特征稍多时就无法进行。相比之下，一种可行的做法是产生一个特征子集并评价其效果，根据评价结果再产生新的特征子集并继续评价，直到无法找到更好的候选子集为止。</p>\n<p><strong>根据评价方式的不同，特征选择算法主要可以分为包裹法、过滤法和嵌入法三类</strong>。包裹法使用预测模型来评价特征子集。每个新的特征子集都用来训练一个模型，并在验证数据集上进行测试，模型在验证数据集上的错误率也就是该子集的分数。由于包裹法为每个子集都要训练一个新的模型，因而其计算量较大，但它的优点是能够找到适用特定模型的最佳特征子集。</p>\n<p>与包裹法不同，过滤法先对数据集进行特征选择，也就是对初始特征进行过滤，再用过滤后的特征来训练模型。过滤法的计算负荷要小于包裹法，但由于特征子集和预测模型之间并没有建立对应关系，因而其性能也会劣于包裹法。但另一方面，过滤法得到的特征子集具有更强的通用性，也有助于揭示不同特征之间的关系。许多过滤器提供的不是一个明确的最佳功能子集，而是不同特征的排名，因而过滤法可以作为包裹法的预处理步骤来使用。</p>\n<p>嵌入法结合了包裹法和过滤法的思路，将特征选择和模型训练两个过程融为一体，使学习器在训练过程中自动完成特征选择。其典型的方法是引入$L_1$范数作为正则项的<strong>LASSO方法</strong>。LASSO方法的原理在线性回归中已经做过介绍，在此不做重复。一般来说，嵌入法的计算复杂度在包裹法和过滤法之间。</p>\n<p>今天我和你分享了机器学习基本算法之一的主成分分析的基本原理，也介绍了另一种降维技术特征选择，其要点如下：</p>\n<ul>\n<li>主成分分析利用正交变换将可能存在相关性的原始属性转换成一组线性无关的新属性，并通过选择重要的新属性实现降维；</li>\n<li>主成分分析的解满足最大方差和最小均方误差两类约束条件，因而具有最大可分性和最近重构性；</li>\n<li>特征选择则是选取原始特征中的一个子集用于学习任务，是另一种主要的降维技术；</li>\n<li>特征选择的关键问题是对特征子集的评价，主要的特征选择算法包括包裹法、过滤法和嵌入法。</li>\n</ul>\n<p>降维操作实质上体现出的是主要矛盾和次要矛盾的取舍问题。在资源有限的条件下，一定要优先解决主要矛盾。那么你能想到这种思想在生活中或是科学技术上的哪些其他应用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/66/23/669d6a62837e1d733668767e254f3923.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"14 机器学习 | 物以类聚，人以群分：聚类分析","id":2196},"right":{"article_title":"（课外辅导）机器学习 | 拓展阅读参考书","id":2113}}},{"article_id":2113,"article_title":"（课外辅导）机器学习 | 拓展阅读参考书","article_content":"<p>在机器学习上，首先要推荐的是两部国内作者的著作：<strong>李航</strong>博士所著的**《统计学习方法》<strong>和</strong>周志华<strong>教授的</strong>《机器学习》**。</p>\n<p><strong>《统计学习方法》</strong>采用“总-分-总”的结构，在梳理了统计学习的基本概念后，系统而全面地介绍了统计学习中的10种主要方法，最后对这些算法做了总结与比较。这本书以数学公式为主，介绍每种方法时都给出了详尽的数学推导，几乎不含任何废话，因而对读者的数学背景也提出了较高的要求。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/5a/02/5a31acce48f7670dbe07d4dc1a44ae02.jpg\" alt=\"\" /></p>\n<p>相比之下，<strong>《机器学习》</strong>覆盖的范围更广，具有更强的导论性质，有助于了解机器学习的全景。书中涵盖了机器学习中几乎所有算法类别的基本思想、适用范围、优缺点与主要实现方式，并穿插了大量通俗易懂的实例。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/1d/a7/1d2978a3a0be5484a4a2b7cd44315aa7.jpg\" alt=\"\" /></p>\n<p>如果说《统计学习方法》胜在深度，那么《机器学习》就胜在广度。在具备广度的前提下，可以根据《机器学习》中提供的丰富参考文献继续深挖。</p>\n<p>读完以上两本书，就可以阅读一些经典著作了。经典著作首推 <strong>Tom Mitchell</strong> 所著的 <strong>Machine Learning</strong>，中译本名为<strong>《机器学习》</strong>。本书成书于1997年，虽然难以覆盖机器学习中的最新进展，但对于基本理论和核心算法的论述依然鞭辟入里，毕竟经典理论经得起时间的考验。这本书的侧重点也在于广度，并不涉及大量复杂的数学推导，是比较理想的入门书籍。作者曾在自己的主页上说本书要出新版，并补充了一些章节的内容，也许近两年可以期待新版本的出现。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/8d/24/8db4ee07712129f8f812a6941faf3d24.png\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/ac/b7/ace0ea1cb079c34ccb23853f69b756b7.jpg\" alt=\"\" /></p>\n<p>另一本经典著作是 <strong>Trevor Hastie</strong> 等人所著的<br />\n<strong>Elements of Statistical Learning</strong>，于2016年出版了第二版。这本书没有中译，只有影印本。高手的书都不会用大量复杂的数学公式来吓唬人（专于算法推导的书除外），这一本也不例外。它强调的是各种学习方法的内涵和外延，相比于具体的推演，通过方法的来龙去脉来理解其应用场景和发展方向恐怕更加重要。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/a5/78/a51fe56010cbcb12faeee21961d50578.png\" alt=\"\" /></p>\n<p>压轴登场的非 <strong>Christopher Bishop</strong> 所著的 <strong>Pattern Recognition and Machine Learning</strong> 莫属了。本书出版于2007年，没有中译本，也许原因在于将这样一本煌煌巨著翻译出来不知要花费多少挑灯夜战的夜晚。这本书的特点在于将机器学习看成一个整体，不管于基于频率的方法还是贝叶斯方法，不管是回归模型还是分类模型，都只是一个问题的不同侧面。作者能够开启上帝视角，将机器学习的林林总总都纳入一张巨网之中，遗憾的是，大多数读者跟不上他高屋建瓴的思路（也包括我自己）。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/04/95/04ffae83d470b348faf25ca226fc4d95.png\" alt=\"\" /></p>\n<p>最后推荐的是 <strong>David J C MacKay</strong> 所著的 <strong>Information Theory, Inference and Learning Algorithms</strong>，成书于2003年，中译本名为<strong>《信息论，推理与学习算法》</strong>。本书作者是一位全才型的科学家，这本书也并非机器学习的专著，而是将多个相关学科熔于一炉，内容涉猎相当广泛。相比于前面板着脸的教科书，阅读本书的感觉就像在和作者聊天，他会在谈笑间抛出各种各样的问题让你思考。广泛的主题使本书的阅读体验并不轻松，但可以作为扩展视野的一个调节。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/3c/7c/3c18bf34b0c7b8a626895693a3e3967c.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/ee/77/eeda44dbac3454a958dcc7d783b02277.png\" alt=\"\" /></p>\n<p><strong>部分书目链接</strong></p>\n<ul>\n<li><a href=\"http://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf\">Machine Learning</a></li>\n</ul>\n<p>-<a href=\"https://web.stanford.edu/~hastie/Papers/ESLII.pdf\"> Elements of Statistical Learning</a></p>\n<ul>\n<li>\n<p><a href=\"http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\">Pattern Recognition and Machine Learning</a></p>\n</li>\n<li>\n<p><a href=\"http://www.inference.org.uk/itprnn/book.pdf\">Information Theory, Inference and Learning Algorithms</a></p>\n</li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/3b/a4/3b20b7273943ac7dd29602f4d02b18a4.jpg\" alt=\"\" /></p>\n<p></p>\n<!-- [[[read_end]]] -->\n","neighbors":{"left":{"article_title":"15 机器学习 | 好钢用在刀刃上：降维学习","id":2197},"right":{"article_title":"16 人工神经网络 | 道法自然，久藏玄冥：神经网络的生理学背景","id":2208}}},{"article_id":2208,"article_title":"16 人工神经网络 | 道法自然，久藏玄冥：神经网络的生理学背景","article_content":"<p>当下，人工智能主流的研究方法是连接主义。连接主义学派并不认为人工智能源于数理逻辑，也不认为智能的关键在于思维方式。这一学派把智能建立在神经生理学和认知科学的基础上，强调智能活动是将大量简单的单元通过复杂方式相互连接后并行运行的结果。</p>\n<p>基于以上的思路，连接主义学派通过人工构建神经网络的方式来模拟人类智能。<strong>它以工程技术手段模拟人脑神经系统的结构和功能，通过大量的非线性并行处理器模拟人脑中众多的神经元，用处理器复杂的连接关系模拟人脑中众多神经元之间的突触行为</strong>。相较符号主义学派，连接主义学派显然更看重是智能赖以实现的“硬件”。</p>\n<p>人类智能的本质是什么？这是认知科学的基本问题。根据自底向上的分析方法，人类智能的本质很大程度上取决于“什么是认知的基本单元”。目前的理论和实验结果表明，要分析认知基本单元，合理的方法既不是物理推理也不是数学分析，而是设计科学实验加以验证。大量的实验结果显示，从被认知的客体角度来看，认知基本单元是知觉组织形成的“知觉物体”。</p>\n<p>知觉物体概念的形成具备其特殊的物理基础。<strong>脑神经科学研究表明，人脑由大约千亿个神经细胞及亿亿个神经突触组成，这些神经细胞及其突触共同构成了一个庞大的生物神经网络</strong>。每个神经细胞通过突触与其他神经细胞进行连接与通信。当通过突触所接收到的信号强度超过某个阈值时，神经细胞便会进入激活状态，并通过突触向上层神经细胞发送激活信号。人类所有与意识及智能相关的活动，都是通过特定区域神经细胞间的相互激活与协同工作而实现的。</p>\n<p>作为一个复杂的多级系统，大脑思维来源于功能的逐级整合。神经元的功能被整合为神经网络的功能，神经网络的功能被整合为神经回路的功能，神经回路的功能最终被整合为大脑的思维功能。但巧妙的是，在逐级整合的过程中，每一个层次上实现的都是”1 + 1 &gt; 2”的效果，在较高层次上产生了较低层次的每个子系统都不具备的“突生功能”。这就意味着思维问题不能用还原论的方法来解决，即不能靠发现单个细胞的结构和物质分子来解决。<strong>揭示出能把大量神经元组装成一个功能系统的设计原理，这才是问题的实质所在</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>研究表明，感觉神经元仅对其敏感的事物属性作出反应。外部事物属性一般以光波、声波、电波等方式作为输入刺激人类的生物传感器，而感觉神经元输出的感觉编码是一种可符号化的心理信息。因此，感觉属性检测是一类将数值信息转化为符号信息的定性操作。</p>\n<p>感觉将事物属性转化为感觉编码，不仅能让大脑检测到相应属性，还在事物属性集与人脑感觉记忆集之间建立起对应关系，所以感觉属性检测又叫<strong>感觉定性映射</strong>。神经网络对来自神经元的各简单属性的感觉映象加以组合，得到的就是关于整合属性的感觉映象。比如大脑整合苹果的颜色属性（如红色）和形状属性（如圆形）的感觉映象的结果，得到的就是苹果又红又圆这个整合属性的感觉映象。</p>\n<p>在感觉映射下，事物属性结构与其感觉映象结构之间应保持不变，也就是说，感觉映射应该是事物属性集与其感觉记忆集之间的一个同态映射。通常所说的人脑认知是外部世界的反映，就是感觉同态的一种通俗说法。</p>\n<p>对以上枯燥难懂的文字加以梳理，就是这样一幅图景。人类自从他能被叫做人的那一天起就具备识别物体的能力了：这是剑齿虎，那是长毛象，手里的是棍子。其实剑齿虎也好，长毛象也罢，不过是不同波长不同数量的光子的组合，是我们的视网膜和大脑的视觉皮层把这些光子进一步加工为不同的属性，这就是信息抽象的过程。这种认知可以归结为一个<strong>高度抽象化的加工模型</strong>。在这个模型中，信息的加工具有从简单到复杂的层次化特征，在每个层次上都有相应的表征，无论是特征提取还是认知加工，都是由不同表征的组合完成的。</p>\n<p><strong>表征处理的物质基础是神经元，大量神经元群体的同步活动是实现表征和加工的生理学机制</strong>。单个神经元只能表征极为简单的信息，但当它们通过神经电活动有节律的同步震荡整合在一起时，复杂的功能就诞生了。<strong>从信息科学的角度看，整个加工过程可以理解为多次特征提取，提取出的特征从简单到复杂，甚至“概念”这种十分抽象的特征也可以被提取出来</strong>。</p>\n<p>但如果人类的认知过程只是提取当前信息的特征并进行分类这么简单的话，它也不值得如此大费笔墨。认知还和注意、情绪等系统有着极强的交互作用，这些功能也和认知密切相关。人的情绪对认知的影响绝非中晚期才启动的高级过程，它的作用远比我们想象的基础得多。焦虑症、抑郁症等情感疾病的患者与不受这些情感疾病困扰的人相比，对负性情绪信息有注意偏向，对带有负面色彩的情绪刺激更容易关注，这种注意偏向发生在视觉感知的早期阶段，其具体机理至今还笼罩在迷雾之中。</p>\n<p><strong>从物质基础的角度看，人类智能是建立在有机物基础上的碳基智能，而人工智能是建立在无机物基础上的硅基智能</strong>。尽管人工神经网络模拟的是人类神经系统的工作方式，但它归根结底是一套软件，而不是像神经元一样的物质实体，依然要运行在通用的计算机上，所以人工神经网络也属于硅基智能的范畴。</p>\n<p><strong>碳基智能与硅基智能的本质区别在于架构，正是架构决定了数据的传输与处理是否能够同时进行</strong>。今日计算机的基础是半个多世纪之前诞生的冯·诺伊曼结构体系，而冯·诺伊曼结构体系的一个核心特征是运算单元和存储单元的分离，两者由数据总线连接。运算单元需要从数据总线接收来自存储单元的数据，运算完成后再将运算结果通过数据总线传回给存储单元。这样一来，数据的传输与处理就无法同步进行，运算单元和存储单元之间的性能差距也限制了计算机的整体表现。</p>\n<p>数据并非为了存储而存储，而是为了在需要时能够快速提取而存储，归根到底存储的作用是提升数据处理的有效性。遗憾的是，这显然不是计算机的强项。虽然处理器的处理速度和硬盘的容量增势迅猛，但数据总线的传输速度依然是电脑性能的瓶颈：数据不能被即时地送到它该去的地方。由此看来，今日计算机对存储的追求甚至有些舍本逐末。</p>\n<p>相比之下，在人类和老鼠等其他哺乳动物的大脑中，数据的传输和处理都由突触和神经元之间的交互完成。重要的是，数据的传输和处理是同步进行的，并不存在先传输后处理的顺序。在同样的时间和空间上，哺乳动物的大脑就能够在分布式的神经系统上交换和处理信息，这绝对是计算机难以望其项背的。</p>\n<p>此外，人的记忆过程也不仅仅是数据存储的过程，还伴随着去粗取精的提炼与整合。记忆的过程在某种意义上更是忘记的过程，是保留精华去除糟粕的过程。一个聪明人也许会忘记知识中的大量细节，但一定会记住细节背后的规律。碳基大脑的容量恐怕永远也无法和硅基硬盘相比，但是其对数据的使用效率同样是硅基硬盘难以企及的。</p>\n<p>今天我和你分享了人工神经网络的生理学背景，也对人类认知的物理基础与工作机制做了简单的介绍，其要点如下：</p>\n<ul>\n<li>思维过程是神经元的连接活动过程，由大量突触相互动态联系着的众多神经元协同作用来实现；</li>\n<li>大脑的思维源于从神经元到神经网络再到神经回路的功能逐级整合；</li>\n<li>大脑对信息的加工可以理解为复杂的多次特征提取过程；</li>\n<li>在大脑中，数据的传输和处理是同步进行的。</li>\n</ul>\n<p>随着技术的发展，神经科学和认知科学正在加深对人工智能的影响，包括李飞飞和德米斯·哈萨比斯在内的一些领军研究者都具备神经科学的学术背景。那么神经科学在人工智能的研究中到底应该扮演什么样的角色呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/eb/30/eb0908de1ebf5914ced2fa63fcf34c30.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"（课外辅导）机器学习 | 拓展阅读参考书","id":2113},"right":{"article_title":"17 人工神经网络 | 一个青年才俊的意外死亡：神经元与感知器","id":2444}}},{"article_id":2444,"article_title":"17 人工神经网络 | 一个青年才俊的意外死亡：神经元与感知器","article_content":"<p>1943年，美国芝加哥大学的神经科学家沃伦·麦卡洛克和他的助手沃尔特·皮茨发表了论文《神经活动中思想内在性的逻辑演算》（A Logical Calculus of Ideas Immanent in Nervous Activity），系统阐释了他们的想法：<strong>一个极度简化的机械大脑</strong>。麦卡洛克和皮茨首先将神经元的状态二值化，再通过复杂的方式衔接不同的神经元，从而实现对抽象信息的逻辑运算。正是这篇论文宣告了人工神经网络的呱呱坠地，它传奇的故事自此徐徐展开。</p>\n<p>与生理学上的神经网络类似，麦卡洛克和皮茨的人工神经网络也由类似神经元的基本单元构成，这一基本单元以两位发明者的名字命名为“<strong>MP神经元</strong>（MP neuron）”。大脑中的神经元接受神经树突的兴奋性突触后电位和抑制性突触后电位，产生出沿其轴突传递的神经元的动作电位；MP神经元则接受一个或多个输入，并对输入的线性加权进行非线性处理以产生输出。假定MP神经元的输入信号是个$N + 1$维向量$(x_0, x_1, \\cdots, x_N)$，第i个分量的权重为$w_i$，则其输出可以写成</p>\n<p> $$ y = \\phi (\\sum\\limits_{i = 0}^N w_i x_i)$$</p>\n<p>上式中的$x_0$通常被赋值为+1，也就使$w_0$变成固定的偏置输入$b$。</p>\n<p>MP神经元中的函数$\\phi (\\cdot)$被称为<strong>传递函数</strong>，用于将加权后的输入转换为输出。传递函数通常被设计成连续且有界的非线性增函数，但<strong>在MP神经元中，麦卡洛克和皮茨将输入和输出都限定为二进制信号，使用的传递函数则是不连续的符号函数</strong>。符号函数以预先设定的阈值作为参数：当输入大于阈值时，符号函数输出1，反之则输出0。这样一来，MP神经元的工作形式就类似于数字电路中的逻辑门，能够实现类似“逻辑与”或者“逻辑或”的功能，因而又被称为“阈值逻辑单元”。</p>\n<!-- [[[read_end]]] -->\n<p>MP神经元虽然简单实用，但它缺乏一个在人工智能中举足轻重的特性，也就是<strong>学习机制</strong>。1949年，加拿大心理学家唐纳德·赫布提出了以其名字命名的“<strong>赫布理论</strong>”，其核心观点是学习的过程主要是通过神经元之间突触的形成和变化来实现的。通俗地说，两个神经细胞之间通过神经元进行的交流越多，它们之间的联系就会越来越强化，学习的效果也在联系不断强化的过程中逐渐产生。</p>\n<p>从人工神经网络的角度来看，赫布理论的意义在于<strong>给出了改变模型神经元之间权重的准则</strong>。如果两个神经元同时被激活，它们的权重就应该增加；而如果它们分别被激活，两者之间的权重就应该降低。如果两个结点倾向于同时输出相同的结果，两者就应具有较强的正值权重；反过来，倾向于输出相反结果的结点之间则应具有较强的负值权重。</p>\n<p>遗憾的是，赫布的学习机制并不适用于MP神经元，因为MP神经元中的权重$w_i$都是固定不变的，不能做出动态的调整。幸运的事，会学习的神经元模型并没有让人等待太久。1957年，任职于美国康奈尔大学航天实验室的心理学家弗兰克·罗森布拉特受到赫布理论的启发，提出了著名的“<strong>感知器（perceptron）”模型</strong>。</p>\n<p>感知器并不是真实的器件，而是一种二分类的监督学习算法，能够决定由向量表示的输入是否属于某个特定类别。作为第一个用算法精确定义的神经网络，感知器由输入层和输出层组成。输入层负责接收外界信号，输出层是MP神经元，也就是阈值逻辑单元。每个输入信号（也就是特征）都以一定的权重被送入MP神经元中，MP神经元则利用符号将特征的线性组合映射为分类输出。</p>\n<p>给定一个包含若干输入输出对应关系实例的训练集时，感知器引入了学习机制，能够通过权重的调整提升分类的效果，其具体的学习步骤为：</p>\n<p> 1.初始化权重$\\mathbf{w}(0)$和阈值，其中权重可以初始化为0或较小的随机数；</p>\n<p> 2.对训练集中的第$j$个样本，将其输入向量$\\mathbf{x}_j$送入已初始化的感知器，得到输出$y_j(t)$；</p>\n<p> 3.根据$y_j(t)$和样本$j$的给定输出结果$d_j$，按以下规则更新权重向量；</p>\n<p>$$w_i(t + 1) = w_i(t) + \\eta [d_j - y_j(t)] \\cdot x_{j, i}$$</p>\n<p>$ 0 \\le i \\le n$</p>\n<p> 4.重复以上两个步骤，直到训练次数达到预设值。</p>\n<p>式中的正常数$0 &lt; \\eta \\le 1$被称为<strong>学习率参数</strong>，是修正误差的一个比例系数。</p>\n<p>显然，<strong>第三步要对感知器的权重进行更新，是学习算法的核心步骤</strong>。这一步的作用在于评估不同输入对分类精确度的影响。如果分类结果和真实结果相同，则保持权重不变；如果不同，就需要具体情况具体分析：如果输出值应该为0但实际为1，就要减少$\\mathbf{x}_j$中输入值为1的分量的权重；如果输出值应该为1但实际为0，则要增加$\\mathbf{x}_j$中输入值为1的分量的权重。</p>\n<p>感知器能够学习的前提是它具有收敛性。罗森布拉特证明了当输入数据线性可分时，感知器学习算法能够在有限次的迭代后收敛，并且得到的决策面是位于两类之间的超平面。本质上讲，<strong>在执行二分类问题时，感知器以所有误分类点到超平面的总距离作为损失函数，用随机梯度下降法不断使损失函数下降，直到得到正确的分类结果</strong>。</p>\n<p>除了优良的收敛性能外，感知器还有很多优点。首先是<strong>非参数化特性</strong>，即没有做出任何关于固有分布形式的假设，只是通过不同分布重叠区域产生的误差来运行，这意味着即使输入数据是非高斯分布时，算法依然能够正常工作。其次是<strong>自适应性</strong>，只要给定训练数据集，算法就可以基于误差修正自适应地调整参数而无需人工介入，这无疑在MP神经元的基础上前进了一大步。</p>\n<p>综上所述，作为神经网络的基础，感知器通过传递函数确定输出，神经元之间通过权重传递信息，权重的变化则根据误差来进行调节。由此，神经网络实现了最基础的学习过程，标志着人工神经网络开始蹒跚学步。</p>\n<p><strong>虽然感知器的形式简洁优雅，但它的应用范围也相当有限：只能解决线性分类问题</strong>。所谓线性分类意指所有的正例和负例可以通过高维空间中的一个超平面完全分开而不产生错误。如果一个圆形被分成一黑一白两个半圆，这就是个线性可分的问题；可如果是个太极图的话，单单一条直线就没法把黑色和白色完全区分开来了，这对应着线性不可分问题。</p>\n<p>感知器对于线性不可分问题无能为力。如果训练数据集不是线性可分的，也就是正例不能通过超平面与负例分离，那么感知器就永远不可能将所有输入向量正确分类。在这种情况下，标准学习算法下不会产生“近似”的解决方案，而是出现振荡，导致算法完全失败。</p>\n<p>批判感知器最有名的大字报就是所谓的<strong>“异或”问题</strong>。异或操作是一种两输入的逻辑操作：当两个输入不同时，输出为真；而当两个输入相同时，输出为假。异或操作可以放在包含四个象限的平面直角坐标系下观察：在第一象限和第三象限中，横坐标和纵坐标的符号相同；而在第二象限和第四象限中，横坐标和纵坐标的符号相反。这样一来，一三象限上的两个点(1, 1)和(-1, -1)就可以归为一类，二四象限上的两个点(-1, 1)和(1, -1)则可以归为另一类。划分这四个点就是一个二分类问题。</p>\n<p>这个问题不是一个线性分类问题，因为找不到任何一条直线能将将正方形中两组对角线上的顶点分为一类。1969年，新科图灵奖得主马文·明斯基和他的麻省理工学院同侪塞默尔·帕波特合著了《感知器：计算几何简介》（Perceptrons: An Introduction to Computational Geometry）一书，系统论证了感知器模型的两个关键问题。第一，单层感知器无法解决以异或为代表的线性不可分问题；第二，受硬件水平的限制，当时的计算机无法完成训练感知器所需要的超大的计算量。</p>\n<p>明斯基的著作对感知器无疑是致命的打击。异或问题是最简单的逻辑问题之一，如果连异或的分类都无法解决，这样的模型存在的意义就颇为有限了。于是，政界对神经网络的热情被这一盆冷水浇了个透心凉，这直接导致了科研经费的断崖式下跌；而学界则视人工神经网络研究为洪水猛兽，避之不及。连接主义学派的第一波凛冬就此到来。</p>\n<p>只是在学术争鸣背后，明斯基的所作所为更藏着些文人相轻的意味。罗森布拉特其人性格张扬，于区区34岁的年纪提出感知器模型后更加受人追捧，俨然一副学术网红的模样。更重要的是，大量来自政府的经费都涌向罗森布拉特的项目，也许这才是让明斯基不爽的真正原因。此外，明斯基和罗森布拉特求学路上的交集也暗示着两人之间的纠葛冰冻三尺，绝非一日之寒。</p>\n<p>感知器的问题给了明斯基釜底抽薪的契机。在1969年版《感知器》中，明斯基不仅论证了感知器在科学上的局限，更夹带了不少对罗森布拉特进行人身攻击的私货。罗森布拉特对感知器在符号处理方面的局限心知肚明，但他万万没想到这些缺陷会被明斯基以如此敌意而致命的方式呈现出来。1971年，罗森布拉特在43岁生日当天划船时意外淹死，至于这究竟是意外事故还是以死明志，恐怕只有他自己知道了。</p>\n<p>今天我和你分享了神经网络的鼻祖感知器的基本原理，其要点如下：</p>\n<ul>\n<li>人工神经网络的神经元用传递函数对输入的线性加权进行非线性处理以产生输出；</li>\n<li>感知器是一种二分类的监督学习算法，通过自适应调整权重解决线性分类问题；</li>\n<li>感知器的神经元之间通过权重传递信息，权重的变化根据误差来进行调节；</li>\n<li>感知器不能解决以异或为代表的线性不可分问题。</li>\n</ul>\n<p>虽然存在缺陷，但感知器依然是人工神经网络研究史上里程碑式的模型，那么如何才能让它跨过异或问题这座大山呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/61/15/61cedd706a18b0b7b07697c6443f4715.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"16 人工神经网络 | 道法自然，久藏玄冥：神经网络的生理学背景","id":2208},"right":{"article_title":"18 人工神经网络 | 左手信号，右手误差：多层感知器","id":2446}}},{"article_id":2446,"article_title":"18 人工神经网络 | 左手信号，右手误差：多层感知器","article_content":"<p>虽然异或问题成为感知器和早期神经网络的阿喀琉斯之踵，但它并非无解的问题。<strong>恰恰相反，解决它的思路相当简单，就是将单层感知器变成多层感知器</strong>。下图就是一个多层感知器的实例，这个包含单个隐藏层的神经网络能够完美地解决异或问题。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/ca/43/ca2dac3a4fd898ca87ff59b2be01aa43.png\" alt=\"\">\n（图片来自<em>Machine Learning: An Algorithmic Perspective</em>, 2nd Edition, Figure 4.2）</p>\n<p>假定两个输入节点A和B的二进制输入分别为1和0，则根据图中的权重系数可以计算出神经元C的输入为0.5，而神经元D的输入为0。在由C和D构成的隐藏层中，由于C的输入大于0，因而符号函数使其输出为1；由于D的输入等于0，符号函数则使其输出为0。在输出节点的神经元E上，各路输入线性组合的结果为0.5，因而E的输出，也是神经网络整体的输出，为1，与两个输入的异或相等。在此基础上可以进一步证明，这个神经网络的运算规则就是异或操作的运算规则。</p>\n<p><strong>多层感知器（multilayer perceptron）包含一个或多个在输入节点和输出节点之间的隐藏层（hidden layer），除了输入节点外，每个节点都是使用非线性激活函数的神经元</strong>。而在不同层之间，多层感知器具有全连接性，即任意层中的每个神经元都与它前一层中的所有神经元或者节点相连接，连接的强度由网络中的权重系数决定。<strong>多层感知器是一类前馈人工神经网络</strong>（feedforward neural network）。网络中每一层神经元的输出都指向输出方向，也就是向前馈送到下一层，直到获得整个网络的输出为止。</p>\n<p>多层感知器的训练包括以下步骤：首先确定给定输入和当前权重下的输出，再将输出和真实值相减得到误差函数，最后根据误差函数更新权重。在训练过程中，虽然信号的流向是输出方向，但计算出的误差函数和信号传播的方向相反，也就是向输入方向传播的，正因如此，这种学习方式得名<strong>反向传播</strong>（backpropagation）。<strong>反向传播算法通过求解误差函数关于每个权重系数的偏导数，以此使误差最小化来训练整个网络</strong>。</p>\n<p><strong>在反向传播算法中，首先要明确误差函数的形式</strong>。当多层感知器具有多个输出时，每个分类结果$y_j$与真实结果$d_j$之间都会存在误差。在单层感知器中，误差直接被定义为两者之间的差值。但在多个输出的情形下，如果第一个输出神经元的误差大于零，第二个输出神经元的误差小于零，这两部分误差就可能部分甚至完全抵消，造成分类结果准确无误的假象。</p>\n<p>如何避免这个问题呢？</p>\n<!-- [[[read_end]]] -->\n<p>为了避免这个问题，<strong>在反向传播算法中，每个输出神经元的误差都被写成平方项的形式，整个神经网络的误差则是所有输出神经元的误差之和</strong>。如此一来，误差函数就以<strong>二次型</strong>的形式体现，也就避免了符号的影响。</p>\n<p><strong>明确定义了误差函数后，就要想方设法让它取得最小值。影响误差函数的因素无外乎三个：输入信号、传递函数和权重系数</strong>。输入信号是完全不依赖于神经网络的外部信号，无法更改；传递函数在网络设计过程中已经确定，同样无法更改；所以在算法执行的过程中，能够更新的就只有权重系数了。</p>\n<p>既然提到了传递函数，就有必要对多层感知器中的传递函数加以说明。单层感知器中使用的符号函数有一个缺点：它是不连续的函数，因而不能在不连续点上求解微分。为了解决这个弊端，<strong>多层感知器采用对数几率函数作为传递函数</strong>。还记得对数几率函数在哪里出现过吗？没错，在机器学习中的逻辑回归算法当中。你可以回忆一下对数几率函数的表达式。</p>\n<p>根据最优化理论，<strong>求解误差函数的最小值就要找到误差函数的梯度，再根据梯度调整权重系数，使误差函数最小化</strong>。对误差函数的求解从输出节点开始，通过神经网络逆向传播，直到回溯到输入节点。这背后的原理在于权重系数的变化对输出的影响方式并非直接修改，而是通过隐藏层逐渐扩散。这样环环相扣的作用方式体现在数学上就是<strong>求导的链式法则</strong>。</p>\n<p><strong>链式法则是个非常有用的数学工具，它的思想是求解从权重系数到误差函数这个链条上每一环的作用，再将每一环的作用相乘，得到的就是链条整体的效果</strong>。利用链式法则求出梯度后，再以目标的负梯度方向对权重系数进行调整，以逐渐逼近误差函数的最小值。</p>\n<p>将平方误差函数、对数几率函数、求导链式法则三大法宝放在一起，就可以召唤出反向传播算法的流程：</p>\n<ol>\n<li>初始化网络中所有权重系数和阈值；</li>\n<li>在前向计算中，将训练样本送入输入节点，在输出节点得到训练结果，再以平方误差形式计算训练输出和真实输出之间的误差函数$E = \\dfrac{1}{2} \\sum\\limits_j (d_j - y_j ) ^ 2$；</li>\n<li>在反向计算中，计算神经网络的局域梯度$\\dfrac{\\partial {E}}{\\partial {w_{hi}}}$，并根据局域梯度和学习率$\\eta$从输出层到隐藏层对权重系数进行逐层更新$\\Delta {w_{hi}} = - \\eta \\dfrac{\\partial {E}}{\\partial {w_{hi}}}$；</li>\n<li>利用新样本训练多层感知器，迭代进行前向计算和反向计算，直到满足停止准则。</li>\n</ol>\n<p>抛开冗杂的数学符号和运算不论，反向传播算法的原理其实并没有那么复杂。如果你能在头脑中勾勒出包含单个隐藏层的多层感知器结构，那不妨直观想象一下当某个输入上出现细微的扰动会导致什么结果？</p>\n<p>由于层与层之间的连接方式是全连接，因而单个输入的微小变化会传递到所有的隐藏神经元，每个隐藏神经元都会感受到来自输入的波动。由于隐藏神经元的输入是输入信号的线性组合，因而输入端的扰动体现在隐藏神经元上就经过了一重权重系数的放大。经过权重系数放大后的扰动又被送入非线性的传递函数里，传递函数输入端的扰动导致的输出端的改变就要再乘以一个传递函数的导数作为放大因子。</p>\n<p>上面的这个过程发生在单个的隐藏神经元上，而在隐藏层中每个神经元上发生的都是同样的故事，所以整个网络的输出变化就等于所有隐藏神经元上由“权重系数 + 传递函数”计算出的输出变化的总和。不难看出，误差函数对单个输入的偏导数就是“权重系数 + 传递函数”的联合作用。</p>\n<p>看到这儿你可能会问，说的这些都是输入信号的扰动对输出结果的影响方式，这和反向传播有什么关系呢？当然有啦！当输入信号不变时，两个导数之间的关系就完全由权重系数的变化决定。将上面过程中输入信号和权重系数的角色做个调换，就可以得出误差函数对权重系数的偏导数就是“输入信号 + 传递函数”的联合作用。而这，恰恰就是链式法则的原理。</p>\n<p><strong>多层感知器的核心结构就是隐藏层</strong>，之所以被称为隐藏层是因为这些神经元并不属于网络的输入或输出。<strong>在多层神经网络中，隐藏神经元的作用在于特征检测</strong>。随着学习过程的不断进行，隐藏神经元将训练数据变换到新的特征空间之上，并逐渐识别出训练数据的突出特征。</p>\n<p>在解决实际问题时，多层感知器的设计要考虑一些工程因素。假设单隐层的多层感知器有$L$个输入节点、$M$个隐藏节点和$N$个输出节点，那这个网络的权重系数总数就是$(L + 1) \\times M + (M + 1) \\times N$。这些权重的取值都需要由反向传播算法确定，而反向传播算法又由训练数据的错误来驱动。因而用于训练的数据越多，多层感知器的学习效果也就越好。<strong>一个经验法则是训练样本数目应该是权重系数数目的10倍，这显然对计算能力提出了较高的要求</strong>。从这个角度看，明斯基对感知器的批评是站得住脚的。</p>\n<p>抛开训练数据量不论，<strong>隐藏层和隐藏神经元的数目也是网络设计中需要考虑的问题</strong>。在数学上可以证明，单个隐藏层就能够实现凸区域，双隐藏层更是可以实现任意形状的凸区域，也就能够解决任何复杂的分类问题。</p>\n<p>在隐藏层数目不变的前提下，区域的复杂程度还能够随着隐藏神经元数目的增加而提升。在数学上同样可以证明，只要隐藏神经元的数目足够多，一个隐藏层就能使多层感知器以任意精度逼近任意复杂度的连续函数。通常情况下，多层感知器不会选择两个以上的隐藏层，因为层数越多，要追踪哪些权重正在被更新就越困难，又不会带来性能的提升。</p>\n<p>多层感知器的训练要需要多次遍历整个数据集，因而<strong>迭代次数就成为另一个重要的问题</strong>。预先设定迭代次数无法保证训练效果，预先设定误差阈值则可能导致算法无法终止。因而常用的办法是：<strong>一旦误差函数停止减小，就终止学习算法</strong>。</p>\n<p>同其他机器学习方法一样，多层感知器也面临<strong>过拟合</strong>的风险。模型的泛化能力可以通过验证集来监督，也就能够在一定程度上避免过拟合的发生。<strong>当训练集的误差下降但验证集的误差上升时让训练立即停止，这就是所谓“早停”的过拟合抑制策略</strong>。当然，<strong>正则化方法</strong>也可以应用在多层感知器的训练中。</p>\n<p>今天我和你分享了多层感知器和反向传播的基本原理，关于反向传播具体的数学细节你可以参考相关文献，其要点如下：</p>\n<ul>\n<li>在感知器的输入层和输出层之间添加隐藏层，就可以得到多层感知器；</li>\n<li>多层感知器是一类前馈神经网络，采用的是反向传播的学习方式；</li>\n<li>反向传播算法要根据误差函数的梯度来调整权重系数，需要应用求导的链式法则；</li>\n<li>单个隐藏层就能使多层感知器以任意精度逼近任意复杂度的连续函数。</li>\n</ul>\n<p>反向传播算法是在1986年由乔弗雷·辛顿提出的，可今天，作为反向传播之父的辛顿却要大义灭亲。辛顿的观点是没有目标函数就无法进行反向传播，而如果数据没有标签自然就没有目标函数了。因此，要实现无监督学习就必须告别反向传播算法。那么应该如何看待辛顿的观点呢？</p>\n<p>欢迎发表你的观点。欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/b8/6e/b885f71e6bef6d5c6d8ae96e73fa2a6e.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"17 人工神经网络 | 一个青年才俊的意外死亡：神经元与感知器","id":2444},"right":{"article_title":"19 人工神经网络 | 各人自扫门前雪：径向基函数神经网络","id":2447}}},{"article_id":2447,"article_title":"19 人工神经网络 | 各人自扫门前雪：径向基函数神经网络","article_content":"<p>多层感知器是一类全局逼近的神经网络，网络的每个权重对任何一个输出都会产生同等程度的影响。因而对于每次训练，网络都要调整全部权值，这就造成全局逼近网络的收敛速度较慢。显然，这是一种牵一发而动全身的<strong>全局作用</strong>方式。</p>\n<p>与全局作用对应的是<strong>局部作用</strong>。在局部作用中，每个局部神经元只对特定区域的输入产生响应。如果输入在空间上是相近的，对这些输入的反应应该是相似的，那么被这些输入激活的神经元也应该是同一批神经元。进一步推广又可以得到，如果一个输入$A$在另外两个输入$B_1$和$B_2$的空间位置之间，那么响应输入$B_1$和$B_2$的神经元也应该在一定程度上被$A$激活。</p>\n<p>神经元的局部作用原理有它的生理学依据。当你仰望夜空中的点点繁星时，茫茫暗夜中的星光激活的是视觉神经的特定部分。随着地球的自转，星光也会移动，虽然亮度没有变化，但不同位置的星光激活的就是视觉神经中的不同部分，因而产生响应的神经元也会发生变化。有些原本被激活的神经元会因为目标对象的移出而被抑制，有些原本被抑制的神经元则因为目标对象的移入而被激活。</p>\n<p>在神经科学中，这个概念被称为“<strong>感受野</strong>（receptive field）”。一个感觉神经元的感受野指的是位于这一区域内的适当刺激能够引起该神经元反应的区域。人类神经的感受野的变化方式可以在人工神经网络中以权重系数的形式体现出来，而<strong>按照感受野的变化规律设置权重系数，得到的就是“径向基函数神经网络”</strong>（Radial Basis Function Network , RBFN）。</p>\n<!-- [[[read_end]]] -->\n<p><strong>径向基网络通常包含三层：一个输入层、一个隐藏层和一个输出层</strong>。其中<strong>隐藏层是径向基网络的核心结构</strong>。每个隐藏神经元都选择径向基函数作为传递函数，对输入分量的组合加以处理。需要注意的是，输入节点和隐藏节点之间是直接相连的，权重系数为1。</p>\n<p>径向基函数是只取决于与中心矢量的距离的函数，也就是不管不同的点是在东西还是南北，只要它们和中心点之间的距离相同，其函数值就是相同的。径向基函数的图形会根据确定的中心点呈现圆周对称的性质。如果将欧氏距离作为距离的度量，函数的形式就可以定义为平缓变化的高斯函数，其表达式就是</p>\n<p>$$ \\rho (\\mathbf{x}, \\mathbf{w}_i, \\sigma) = \\exp (\\dfrac{- || \\mathbf{x} - \\mathbf{w}_i || ^ 2}{2\\sigma ^ 2}) $$ </p>\n<p>式中的每个中心向量$\\mathbf{w}_i$都是径向基网络中的权重系数。一般情况下，所有高斯函数会共享同一个带宽$\\sigma$，因而将不同隐藏单元区分开来的就是<strong>中心向量</strong>$\\mathbf{w}_i$。每个径向基函数的输出按照参数进行线性组合后，再被送到输出神经元加以处理。输出神经元通常是普通的MP神经元。</p>\n<p>前面介绍的是径向基网络的生理学依据，下面让我们看看这套方法在数学上的意义。隐藏层的作用是实现从输入空间到非显式特征空间的非线性变换，由此，低维空间上的非线性可分数据就被映射到高维空间之中。在一定的条件下，转换后的数据变为线性可分的可能性很高，这一点已经在数学上得到证明。</p>\n<p>看到这里，你是否有似曾相识的感觉呢？没错，<strong>这和支持向量机的思路是一样的</strong>！支持向量机中的核技巧能够把低维空间中的非线性问题映射成高维空间中的线性问题，将低维空间中曲面形式的决策边界转化为高维空间中的超平面，从而降低分类问题的难度。在支持向量机中常用的核函数就包括高斯核函数，回忆一下，它的形式是不是和径向基网络中的高斯函数形式一模一样呢？</p>\n<p><strong>将低维问题投射到高维空间是解决线性不可分问题的通用方法。但是在某些情况下，一个非线性映射就足以在低维空间中解决问题，而不需要升高隐藏空间的维数，提升算法的复杂度</strong>。高斯函数就是这类“杀鸡焉用宰牛刀”的实例。</p>\n<p>还是以异或问题为例。在异或问题中，四个数据点(0, 0), (0, 1), (1, 0), (1, 1)是正方形的四个顶点，处于对角线方向上的两个点属于同一个模式。如果引入高斯函数$\\Phi_i (x) = \\exp(- || \\mathbf{x} - \\mathbf{u}_i || ^ 2), i = 1, 2$，其中$\\mathbf{u}_1 = [0, 0], \\mathbf{u}_2 = [1, 1]$，则原始的四个数据点就被变换为(0.1353, 1), (0.3678, 0.3678), (0.3678, 0.3678), (1, 0.1353)。经过高斯函数的变换后，(0, 1)和(1, 0)两个具有相同异或结果的点重合在一起，异或问题也变成了线性可分问题。但在之前的处理中，空间的维数并未增加，问题的转化只用到了非线性的高斯函数。 </p>\n<p><strong>另一个理解径向基网络的角度是多变量插值</strong>。整体上看，径向基网络的作用是学习一个高维空间上的超曲面，根据训练数据进行训练的过程就是对超曲面进行拟合的过程。但由于数据中存在噪声，因而训练得到的结果还需要泛化处理，泛化的任务就是在数据点之间进行插值，使插值后的曲面仍然要经过所有数据点。在插值过程中，使用的插值函数就是不同类型的径向基函数。</p>\n<p>训练数据中的每个样本都是新的高维空间中的一个点，插值要做的是把所有离散的点连成一片，形成一个曲面。那么插值操作具体是如何进行的呢？高斯形式的径向基函数将每个训练样本映射为一个连续的函数，函数的中心就是样本点的取值。在整个空间内对所有的高斯函数求和，得到的就是拟合出的曲面。</p>\n<p>当新样本出现时，其在曲面上的映射值就等于所有高斯径向基函数在这个数据点上的函数值之和。感受野理论告诉我们，每个训练数据对曲面的影响都只限于其数据周边的一个小范围内，因而在新样本的插值结果中，贡献较大的是离它比较近的训练数据。如果某些训练数据距离新样本较远，就不会对新样本产生影响。</p>\n<p><strong>在实际应用中，对径向基网络的训练包括两个步骤</strong>。第一步的任务是初始化中心向量$\\mathbf{w}_i$的位置，中心向量的位置既可以随机分配，也可以通过<strong>K均值聚类</strong>这一无监督学习的方法完成。这个步骤对应的是隐藏层的训练。第二步的任务是用线性模型拟合初始化的隐藏层中的各个中心向量，拟合的损失函数设定为最小均方误差函数，使用<strong>递归最小二乘法</strong>（Recursive Least Square）使损失函数最小化。这个步骤对应的是对输出层的训练。</p>\n<p>使用K均值算法训练隐藏层时，聚类的数目K决定了隐藏神经元的数目，通过这个参数设计者可以控制径向基网络的性能和计算复杂度。算法的参数确定后，就能够对训练数据进行无监督的分类，计算出的每个聚类的中心就是高斯函数的中心。出于简化设计的考虑，每个高斯函数的带宽都是相同的，并被统一设置为$\\sigma = d_{\\max} / \\sqrt{2K}$，其中$d_{\\max}$是不同中心之间的最大距离。这种带宽的配置符合K均值算法中的中心散布，保证了各个高斯函数既不会太宽也不会太窄。</p>\n<p>隐藏层的训练完成后，就可以开始输出层的训练了。输出层的输入信号是每个隐藏神经元输出信号的线性组合，因而递归最小二乘法是训练权重向量的合适选择。权重向量和隐藏神经元输出之间的关系可以表示为 </p>\n<p>$$ \\mathbf{R}(n) \\mathbf{\\hat w}(n) = \\mathbf{r}(n) $$ </p>\n<p>式中的$\\mathbf{R}$表示K个隐藏神经元输出之间的相关矩阵，$\\mathbf{\\hat w}$表示待训练的未知权重向量，$\\mathbf{r}$表示期望响应和隐藏单元输出之间的$K \\times 1$维互相关向量，自变量n则代表了训练的轮次。递归最小二乘法的作用在于简化逆矩阵$\\mathbf{R} ^ {-1}(n)$的求解，其详细的推导过程在此就不介绍了。</p>\n<p>在训练完成后还可以添加额外的一个步骤，<strong>利用反向传播算法对径向基网络的所有参数进行一次微调，以达到更好的训练效果</strong>。这是由于无论是K均值聚类还是递归最小二乘法，都是针对特定层次的优化，反向传播优化的对象则是作为整体的径向基网络，它可以在统计意义上保证了整个系统的最优性。</p>\n<p>与感知器类型的神经网络相比，径向基网络代表的则是局部逼近的工作方式。神经元的输入离径向基函数中心越近，神经元的激活程度就越高。但两者都能够实现<strong>通用逼近</strong>（universal approximation），也就是对任意非线性函数的逼近。</p>\n<p>今天我和你分享了径向基函数神经网络的基本原理，其要点如下：</p>\n<ul>\n<li>径向基网络采用局部逼近方式，每个神经元只对特定的输入信号产生作用；</li>\n<li>径向基网络的隐藏神经元使用径向基函数作为传递函数，常用的径向基函数是高斯函数；</li>\n<li>径向基函数可以将低维空间上的线性不可分问题转化为高维空间上的线性可分问题；</li>\n<li>使用高斯函数的径向基网络可以用K均值聚类算法结合递归最小二乘法进行训练。</li>\n</ul>\n<p>径向基神经网络是核技巧在神经网络中的应用，那么你还能想到核技巧及其思想的哪些其他应用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/12/1e/12b28b058ec981788aabe18881c5781e.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"18 人工神经网络 | 左手信号，右手误差：多层感知器","id":2446},"right":{"article_title":"20 人工神经网络 | 看不见的手：自组织特征映射","id":2643}}},{"article_id":2643,"article_title":"20 人工神经网络 | 看不见的手：自组织特征映射","article_content":"<p>无论是全局逼近的多层感知器，还是局部逼近的径向基网络，在训练中用到的都是监督学习的方法。<strong>如果将无监督学习引入神经网络中，对应的结构就是自组织特征映射</strong>（Self-Organizing Map），这是芬兰赫尔辛基大学的泰乌沃·柯霍宁于1981年提出的一类神经网络。</p>\n<p>相比于前面介绍的神经网络，自组织映射有两个明显的不同。</p>\n<p><strong>第一，它能够将高维的输入数据映射到低维空间之上（通常是二维空间），因而起到降维的作用</strong>。在降维的同时，自组织映射妙就妙在还能维持数据在高维空间上的原始拓扑，将高维空间中相似的样本点映射到网络输出层的邻近神经元上，从而保留输入数据的结构化特征。</p>\n<p><strong>第二，自组织映射采用的是竞争性学习而非传统的纠错学习</strong>。在竞争性学习中，对输入样本产生响应的权利并不取决于预设的权重系数，而是由各个神经元相互竞争得到的。不断竞争的过程就是网络中不同神经元的作用不断专门化的过程。</p>\n<p>竞争性学习的理念来自于神经科学的研究。在生物的神经系统中存在着一种名叫“<strong>侧向抑制</strong>”的效应，它描述的是兴奋的神经元会降低相邻神经元活性的现象。侧向抑制能够阻止从侧向刺激兴奋神经元到邻近神经元的动作电位的传播。什么意思呢？当某个神经元受到刺激而产生兴奋时，再刺激相近的神经元，则后者的兴奋对前者就会产生抑制作用。这种抑制作用会使神经元之间出现竞争，在竞争中胜出的神经元就可以“胜者通吃”，将竞争失败的神经元全部抑制掉。</p>\n<p>自组织映射中的竞争性学习模拟的就是上述的侧向抑制机制。自组织映射的拓扑结构并非如多层感知器般的层次结构，而是一张一维或者二维的网格，网格中的每个节点都代表一个神经元，神经元的权重系数则是和输入数据的维度相同的向量。在拓扑结构中，每个神经元的位置都不是随意选取的，而是和功能有着直接的关系。距离较近的神经元能够处理模式相似的数据，距离较远的神经元处理对象的差异也会很大。</p>\n<p>由于神经元在网格中的位置至关重要，因而训练过程就是在空间上对神经元进行有序排列的过程。自组织映射为神经元建立起一个坐标系，由于每个网格神经元对应一类特定的输入模式，输入模式的内在统计特征就是通过神经元的坐标来表示的。</p>\n<p>因此，<strong>自组织映射的主要任务就是将任意维度的输入模式转换为一维或二维的离散映射，并以拓扑有序的方式自适应地实现这个映射</strong>。在训练过程中，自组织映射中每个神经元的权重系数首先要初始化，初始化的方式通常是将其赋值为较小的随机数，这可以保证不引入无关的先验信息。当初始化完成后，网络的训练就包括以下三个主要过程。</p>\n<!-- [[[read_end]]] -->\n<ol>\n<li><p><strong>竞争过程</strong>：对每个输入模式，网络中的神经元都计算其判别函数的取值，判别函数值最大的神经元成为竞争过程的优胜者；</p>\n</li>\n<li><p><strong>合作过程</strong>：获胜神经元决定兴奋神经元的拓扑邻域的空间位置；</p>\n</li>\n<li><p><strong>自适应过程</strong>：兴奋神经元适当调节其权重系数，以增加它们对于当前输入模式的判别函数值，强化未来对类似输入模式的响应。</p>\n</li>\n</ol>\n<p><strong>竞争过程的实质是找到输入模式和神经元之间的最佳匹配</strong>。由于输入模式$\\mathbf{x}$和权重系数$\\mathbf{w}$是具有相同维度的向量， 因而可以计算两者的内积作为判别函数。在线性代数中我曾提到，内积表示的是向量之间的关系，两个归一化向量的相关度越高，其内积也就越大。因而通过选择具有最大内积$\\mathbf{w}_j^T \\mathbf{x}$的神经元，就可以决定兴奋神经元拓扑邻域的中心位置。</p>\n<p>两个向量的内积越大，它们之间的欧氏距离就越小，因而<strong>内积最大化的匹配准则等效于欧氏距离最小化。从这个角度看，获胜神经元就是对输入模式的最佳匹配</strong>。</p>\n<p><strong>竞争过程确定了合作神经元的拓扑邻域的中心，合作过程就要界定中心之外的拓扑邻域</strong>。借鉴侧向抑制效应，自组织映射中的拓扑邻域被定义成以最佳匹配神经元和兴奋神经元之间的侧向距离为自变量的单峰函数。由于神经元在传导时倾向于激活距离较近的神经元，因而拓扑邻域的幅度在距离为0的中心点取到最大值，并随着侧向距离的增加而单调衰减。同时，拓扑邻域还应该满足平移不变性，也就是邻域幅度不依赖于中心点的位置。</p>\n<p>除了单峰值和平移不变性之外，自组织映射中的拓扑邻域还有另一个特点，就是其大小会随着时间的推移而收缩，也就是幅度值随着时间的增加而下降。这个时变性质的意义在于逐渐减少需要更新的神经元的数量。</p>\n<p>网络的自组织特性要求神经元的权重系数具备自动调节的功能。权重系数的整体变化趋势是向数据靠拢，因而其分布也将趋于输入模式的分布。权重系数的更新方程如下所示</p>\n<p>$$ \\mathbf{w}_j(n + 1) = $$</p>\n<p>$$\\mathbf{w}_j(n) + \\eta (n) h_{j, i(\\mathbf{x})} (n) [\\mathbf{x}(n) - \\mathbf{w}_j(n)] $$ </p>\n<p>式中的$\\eta (n)$是随时间增加而下降的学习率参数，$h_{j, i(\\mathbf{x})} (n)$则是拓扑邻域函数。</p>\n<p>因而，<strong>自适应过程可以分为两个阶段</strong>。<strong>第一阶段是排序阶段</strong>，权重系数的拓扑排序在这个阶段形成；<strong>第二阶段是收敛阶段</strong>，通过微调特征映射实现对输入模式的精确描述。只要算法的参数没有问题，自组织映射就能将完全无序的初始状态按照输入模式以有组织的方式重构，这也是“自组织”的含义。</p>\n<p>由于自组织映射采用的是迭代的训练方法，初始值的好坏便至关重要。传统算法采用的是随机方式的初始化，后来又提出了基于主成分的初始化方法。事实上，初始化方法的性能取决于输入数据集的几何形状。如果数据集在主成分上的投影是线性的，就应该选择主成分初始化。可对于非线性数据集而言，随机初始化反而是最优选择。</p>\n<p>综合起来，自组织映射的训练算法可以归纳为以下几个步骤：</p>\n<ol>\n<li>使用主成分法或随机法初始化神经元的权重系数；</li>\n<li>选取训练集中的样本用于激活整个网络；</li>\n<li>根据最小距离准则寻找最佳匹配神经元；</li>\n<li>通过更新方程调整所有神经元的权重系数；</li>\n<li>重复以上步骤直到在从输入模式到神经元的映射关系中观察不到明显变化。</li>\n</ol>\n<p>从输入模式到神经元的映射关系被称为<strong>特征映射</strong>，它具有很多良好的性质。由于权重系数的维度会远远小于输入数据的可能数目，因而特征映射可以对输入空间提供一个良好的近似，起到数据压缩的作用。</p>\n<p>从这个角度理解，<strong>自组织映射可以看成是一个编码器-解码器模型</strong>：寻找最佳匹配神经元就是对输入模式进行编码，确定权重系数则是对编码结果进行解码，邻域函数则可以表示对编解码过程造成干扰的噪声的概率密度。自组织映射的这个性质与信息论中用于数据压缩的向量量化方法不谋而合。</p>\n<p>当算法收敛后，特征映射就能够表示出输入模式的统计特性。如果输入空间上的某些数据出现的概率较高，它们就会被映射到输出空间中更大的区域上，也就会涵盖更多的神经元。但这个趋势只有定性的意义，自组织映射不能给出输入数据固有概率分布的精确表示，因而不具备定量描述的能力。虽然不能定量描述固有分布，但自组织映射依然能够选择出一组最优的特征，使这组特征在逼近固有分布时具有最佳的效果。</p>\n<p>自组织映射通过以无监督方式训练大量数据实现特征映射，以实现不同类型数据的区分。其简便易行的特性和强大的可视化能力使它在不少需要大规模数据训练的应用中得到使用。自组织映射一个典型的应用是在图像处理中检测和描述语义目标和目标类之间的存在关系，以及自然语言中单词的语义规则。</p>\n<p>今天我和你分享了自组织特征映射的基本原理，其要点如下：</p>\n<ol>\n<li>自组织映射是一类无监督学习的神经网络，模拟了生物神经系统的竞争性学习机制；</li>\n<li>自组织映射能将任意维度的输入模式转换为一维或二维的离散映射，得到的特征映射是拓扑有序的；</li>\n<li>在拓扑映射中，输出神经元的空间位置对应了输入数据的模式或特征；</li>\n<li>自组织映射网络的训练包括竞争过程、合作过程和自适应过程等几个主要步骤。</li>\n</ol>\n<p>自组织映射具有通过时间上的演化在无序中产生有序的内在能力，这与目前同样火热的非线性科学与复杂系统的研究有几分神似。那么关于复杂性的研究能否应用于人工智能之中呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/98/2b/98778ce46a430c847021f437eefdb62b.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"19 人工神经网络 | 各人自扫门前雪：径向基函数神经网络","id":2447},"right":{"article_title":"21 人工神经网络 | 水无至清，人莫至察：模糊神经网络","id":2646}}},{"article_id":2646,"article_title":"21 人工神经网络 | 水无至清，人莫至察：模糊神经网络","article_content":"<p>模糊神经网络是一类特殊的神经网络，它是神经网络和模糊逻辑结合形成的混合智能系统，通过将模糊系统的类人推理方式与神经网络的学习和连接结构相融合来协同这两种技术。简单来说，<strong>模糊神经网络（fuzzy neural network）就是将常规的神经网络赋予模糊输入信号和模糊权值，其作用在于利用神经网络结构来实现模糊逻辑推理</strong>。</p>\n<p>在生活中，我们在臧否人物时很少给出非黑即白的二元评价。这是因为每个人在生活中都扮演着复杂的多重角色，不是好人就是坏人的评判方式显然有失客观。这就是模糊理论在生活中最直接的体现。正如美国加州大学伯克利分校的洛特菲·扎戴所说：“当系统的复杂性增加时，我们使它精确化的能力将减小。直到达到一个阈值，一旦超越这个阈值，复杂性和精确性将相互排斥。”</p>\n<p>1965年，正是这位洛特菲·扎戴提出了与模糊数学相关的一系列理论，并由此衍生出模糊系统的概念。1988年，供职于日本松下电气的高木秀幸和小林功提出了将神经网络与模糊逻辑结合的想法，这标志着神经模糊系统（Neuro-fuzzy system）的诞生。神经模糊系统的基础是<strong>模糊集合</strong>和一组“如果......那么......”形式的<strong>模糊规则</strong>，利用神经网络的非线性和学习机制获得类人的推理能力。1993年，意大利帕多瓦大学的乔万尼·波尔托兰提出了将多层前馈神经网络模糊化的思路，这就是这里所讨论的模糊神经网络。</p>\n<p>需要说明的是，模糊神经网络和神经模糊系统是不同的。神经模糊系统的输入和输出都是确定对象。因此在神经模糊系统中，必备的结构是模糊化层和去模糊化层。模糊化层用于将输入的确定对象模糊化，去模糊化层则用于将输出的模糊对象转化为确定对象。相比之下，模糊神经网络的输入和输出都是模糊对象，完成的也是模糊推理的功能。</p>\n<p>在介绍模糊神经网络之前，有必要对一些基本概念加以解释。模糊理论中最基本的概念是<strong>模糊集合</strong>。在不模糊的集合里，每个元素和集合之间的隶属关系是明确的，也就是要么属于集合，要么不属于集合，两者之间泾渭分明。可<strong>在模糊集合中，元素和集合之间的关系不是非此即彼的明确定性关系，而是用一个叫做隶属度的函数定量表示</strong>。在现实中评判某个人物的时候，通常会说他“七分功三分过”或是“三分功七分过”，这里的三七开就可以看成是隶属函数。</p>\n<!-- [[[read_end]]] -->\n<p><strong>模糊集合是对“对象和集合之间关系”的描述，模糊数描述的则是对象本身</strong>。“人到中年”是很多人愿意用来自嘲的一句话，可中年到底是个什么范围呢？利用排除法可以轻松确定25岁算不上中年，55岁也算不上中年，可要是对中年给出一个明确的正向定义就困难了。因而如果把“中年”看作一个数的话，它就是个模糊数。模糊数在数学上的严格定义是根据模糊集合推导出来的，是个归一化的模糊集合，但通俗地说，<strong>模糊数就是只有取值范围而没有精确数值的数</strong>。</p>\n<p>模糊数可以用来构造模糊数据，对模糊数据进行运算时，依赖的规则叫做<strong>扩展原理</strong>。对两个确定的数做运算，得到的结果肯定还是一个确定的数。可一旦模糊数参与到运算中来，得到的结果也将变成一个模糊数。扩展原理及其引申得到的模糊算术，定义的就是运算给模糊数的模糊程度带来的变化，这当然也是一个通俗的说法。在模糊算术中，传统的加减乘和内积等运算都被改造成对模糊集合的运算。</p>\n<p>有了这些概念，就可以进一步解释模糊神经网络。模糊神经网络的拓扑与架构和传统的多层前馈神经网络相同，但其输入信号和权重系数都是模糊集合，因而其输出信号同样是模糊集合。而在网络内部，处理输入信号和权重系数的则是模糊数学，隐藏神经元表示的就是隶属函数和模糊规则。模糊化的处理必然会影响神经网络的特性，因而学习算法的设计和通用逼近特性的保持就成为模糊神经网络要解决的核心问题。</p>\n<p><strong>构成模糊神经网络的基本单元是模糊化的神经元</strong>。模糊神经元的输入信号和权重系数都是模糊数，传递函数也需要对模糊集合上的加权结果进行处理。模糊神经元的组合形成模糊神经网络。模糊神经网络的训练方式同传统的神经网络类似，即定义一个误差函数并使其最小化。但由于模糊数不能进行微积分的计算，因此不能直接对模糊的权重系数使用反向传播，需要在处理误差时需要针对模糊数的特性提出新的方法。两种常用的方法是<strong>基于水平集的方法</strong>和<strong>基于遗传算法的方法</strong>。</p>\n<p>基于水平集的方法采用的是直接推导的方式，通过确定模糊集合的水平集对模糊数中的元素加以筛选。假如一个模糊数中包含三个元素x、y和z，其参数分别是0.3、0.6和0.7，那么当截断点等于0.5时，这个模糊数的0.5水平集就会把参数为0.3的元素x过滤掉，只保留参数大于0.5的y和z。在神经网络的训练中，训练算法的作用就是通过计算偏导数和应用反向传播算法，优化每个水平集的截断点，从而确定模糊权值。</p>\n<p>水平集方法的致命缺陷在于缺乏理论依据，这严重限制了它的应用。一个不加任何限制的模糊数肯定不能用有限个参数来描述，因而要基于水平集方法来设计通用的模糊神经网络学习算法是不可能的。当模糊数被限制为三角形或者梯形时，算法可以得到一定的简化。</p>\n<p>在模糊神经网络的训练中，如果保持学习率参数不变，误差函数就难以快速收敛。即使收敛也可能陷入局部最小值上，在不同的学习率参数下得到不同的局部最小值。为了处理这个问题，模糊神经网络引入了一种叫做“<strong>共轭梯度（conjugate gradient）”的机制</strong>，使训练过程能够找到全局最优解。而要确定最优的学习率，就要借助<strong>遗传算法</strong>（genetic algorithm）。</p>\n<p>共轭梯度方法比较复杂，在这里不做展开。相比基于水平集的方法，基于遗传算法的方法是摸着石头过河，通过迭代逐步找到误差函数的最小值，从而确定权重系数。但它并没有解决水平集方法的本质问题，适用范围仍然被局限在三角形和梯形这类特殊的模糊数上，纯属换汤不换药的做法。</p>\n<p>除了学习算法之外，<strong>逼近性能也是模糊神经网络设计中的核心问题</strong>。传统神经网络可以通过引入少量隐藏层实现对任意问题的逼近，但模糊神经网络能不能达到这个要求尚需证明。受学习算法的限制，对逼近性能的考察也只能针对特定类型的模糊神经网络进行。研究结果表明，当输入和权值都被限定为实数时，四层的前馈网络就足以逼近任意形式的模糊取值函数。这意味着模糊神经网络和传统神经网络的性能是相同的，因为抛去输入层和输出层之外，四层网络就只剩两个隐藏层了。</p>\n<p><strong>模糊神经网络是一种混合智能系统，能够同时处理语言信息和数据信息，因而是研究非线性复杂系统的有效工具，也是软计算的核心研究内容之一，在模式识别、系统分析和信号处理等领域都有成功应用的实例</strong>。但相对于打了翻身仗的传统神经网络，摆在模糊神经网络面前的依然是雄关如铁的境地。</p>\n<p>今天我和你分享了模糊神经网络的基本概念，其要点如下：</p>\n<ul>\n<li>模糊神经网络是神经网络和模糊逻辑结合形成的混合智能系统；</li>\n<li>模糊神经网络的输入信号、权重系数和输出信号全都是模糊集合；</li>\n<li>模糊神经网络的主要学习算法包括基于水平集的方法和基于遗传算法的方法；</li>\n<li>模糊神经网络具有和传统神经网络类似的通用逼近特性。</li>\n</ul>\n<p>模糊理论代表了一种思维方式，它更接近于人类的思考习惯。那么融合了定性和定量的模糊理论会给用于规则推演的人工智能带来什么样的启发呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/5d/e1/5d8c880ee0e5dd330df08e9db32558e1.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"20 人工神经网络 | 看不见的手：自组织特征映射","id":2643},"right":{"article_title":"（课外辅导）人工神经网络 | 拓展阅读参考书","id":2655}}},{"article_id":2655,"article_title":"（课外辅导）人工神经网络 | 拓展阅读参考书","article_content":"<p>人工神经网络方向的参考书首推<strong>Simon Haykin</strong>所著的<strong>Neural Networks and Learning Machines</strong>，英文版于2008年出到第三版，中译本名为<strong>《神经网络与机器学习》</strong>，也有影印本。本书是“大而全”的参考手册类型，全书以数学推导为主，对每种主流的神经网络算法都做了详细说明。Simon Haykin是通信系统与信号处理的专家，在这本神经网络的专著中也穿插了大量信号处理和信息论中常用的工具，散发着浓厚的数学气息，因而适合在具有一定基础的条件下阅读。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/8e/8a/8e0cf48760b8944395babcd09cdc4c8a.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/46/15/46b787c830381c687d1f2a45149a5615.jpg\" alt=\"\" /></p>\n<p>另一本神经网络的专著是<strong>Martin Hagan</strong>等人合著的<strong>Neural Network Design</strong>，英文版于2014年出到第二版，中译本名为<strong>《神经网络设计》</strong>，对应原书第一版。本书的几位作者是Matlab中神经网络工具箱的开发者，因而其专业性和权威性毋庸置疑。和Simon Haykin的学究著作相比，本书轻推导而重演示，主要通过实例手把手地解释神经网络的工作原理，对线性代数和最优化等基本工具也有涉及。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/6f/f6/6f17d24d62dcb25f0f95df10f3166bf6.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/3f/28/3f5680bdfe7050e45ac28c645a6c2b28.jpg\" alt=\"\" /></p>\n<p><strong>Sandhya Samarasinghe</strong>所著的<strong>Neural Networks for Applied Sciences and Engineering</strong>同样是非常好的参考书，英文版出版于2007年，暂无中译本。正如书名所示，本书是一本面向应用场景的书籍，侧重于神经网络在工程中，尤其是在基于数据进行模式识别中的应用。书中同样包含丰富的实例，其中不乏取材于真实的数据分析案例，和现实结合得相当紧密的实例。本书是难得的理论与实践并重的参考书，有利于扩展神经网络研究的视野，对初学者也非常友好。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/23/80/2350dda254d12072413381b7173f5580.jpg\" alt=\"\" /></p>\n<p>最后一本是<strong>Stephen Marsland</strong>所著的<strong>Machine Learning: An Algorithmic Perspective</strong>，英文版于2015年出到第二版，暂无中文版。本书的主题是机器学习，但对包括感知器、多层感知器、径向基网络和自组织映射等主流的神经网络算法都用专门的章节加以介绍。在介绍中，作者侧重于对算法生理学背景的描述，以及对于算法原理的直观解释，这对神经网络研究的入门者无疑颇有裨益。遗憾的是，本书的行文略显啰嗦。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/80/15/8054e4a9b17436f796e68f06c66e6715.jpg\" alt=\"\" /></p>\n<p><strong>部分书目链接</strong></p>\n<ul>\n<li>\n<p><a href=\"https://cours.etsmtl.ca/sys843/REFS/Books/ebook_Haykin09.pdf\">Neural Networks and Learning Machines</a></p>\n</li>\n<li>\n<p><a href=\"http://hagan.okstate.edu/NNDesign.pdf\">Neural Network Design</a></p>\n</li>\n<li>\n<p><a href=\"https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20An%20Algorithmic%20Perspective%20%282nd%20ed.%29%20%5BMarsland%202014-10-08%5D.pdf\">Machine Learning: An Algorithmic Perspective</a></p>\n</li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/ac/1a/ac486b5ea731cdb8be87823e15c5931a.jpg\" alt=\"\" /></p>\n<p></p>\n<!-- [[[read_end]]] -->\n","neighbors":{"left":{"article_title":"21 人工神经网络 | 水无至清，人莫至察：模糊神经网络","id":2646},"right":{"article_title":"22 深度学习 | 空山鸣响，静水流深：深度学习概述","id":2656}}},{"article_id":2656,"article_title":"22 深度学习 | 空山鸣响，静水流深：深度学习概述","article_content":"<p>多年以后，面对李世石与AlphaGo的厮杀，加里·卡斯帕罗夫将会回想起，与深蓝对弈的那个遥远的下午。彼时的对手是一个庞大的超级计算机，记下变化多端的开局，以固定的逻辑决策应对中局，穷极所有可能性筛选残局。人工智能蹒跚学步之时，许多概念尚且是空中楼阁，计算机能够战胜优秀的人类围棋选手，这样的想法不啻于天方夜谭。</p>\n<p>棋类游戏的核心在于根据棋局判断下一手的最优下法，深蓝通过穷举的方法在国际象棋的棋局中解决了这个问题。在64格的国际象棋棋盘上，深蓝的运算能力决定了它能算出12手棋之后的局面下的最优解，而身为人类棋手执牛耳者的卡斯帕罗夫最多只能算出10手棋，多出来的两手棋就会成为左右战局的关键因素。</p>\n<p><strong>深蓝的核心在于“算”：利用强大的计算资源来优化目标函数</strong>。深蓝本身就是一套专用于国际象棋的硬件，大部分逻辑规则是以特定的象棋芯片电路实现，辅以较少量负责调度与实现高阶功能的软件代码。其算法的核心是暴力穷举：生成所有可能的下法，然后执行尽可能深的搜索，并不断对局面进行评估，尝试找出最佳的一手。</p>\n<p>可在围棋棋盘上，可以落子的点数达到了361个。别说12手棋，就是6手棋的运算量都已经接近于天文数字！这使得计算机相对于人脑的运算优势变得微不足道，走出优于人类棋手的妙手的概率也微乎其微，这也是为什么计算机会在围棋领域被看衰的原因。</p>\n<p>围棋的棋盘状态远比国际象棋复杂，以穷举法进行最优落子策略的推演无异于痴人说梦。事实上，顶级的围棋棋手更多地依赖模糊的直觉来评判特定的棋盘状态的好坏。但理性推演与感性判断之间似乎存在着不可逾越的巨大鸿沟，尤其是对于计算机程序而言，依赖直觉是不可能的事情。因此并没有显而易见的方式来将国际象棋领域的成功复制到围棋上，直到AlphaGo的横空出世。</p>\n<p><strong>与深蓝相比，AlphaGo的核心则在于“想”</strong>。与专用硬件深蓝不同，AlphaGo是一套能够运行在通用硬件之上的纯软件程序。它汲取了人类棋手海量的棋谱数据，并依赖人工神经网络和深度学习技术从这些数据中学会了预测人类棋手在任意的棋盘状态下走子的概率，模拟了以人类棋手的思维方式对棋局进行思考的过程。</p>\n<!-- [[[read_end]]] -->\n<p>与深蓝的区别正是AlphaGo的突破之处。早期的计算机就已经被用来搜索优化已有函数的方式，深蓝的特点仅仅在于搜索的目标是优化尽管复杂但是形式大多数由已有的国际象棋知识表达的函数，其思想与人工智能早期的多数程序并无二致。而在整个算法中，除了“获胜”这个概念，AlphaGo对于围棋规则一无所知，更遑论定式等围棋的高级专门概念了。</p>\n<p>这也是AlphaGo和深蓝的本质区别。同是战胜了棋类世界冠军，<strong>深蓝仍然是专注于国际象棋的、以暴力穷举为基础的专用人工智能；AlphaGo是几乎没有特定领域知识的、基于机器学习的、更加通用的人工智能</strong>。</p>\n<p>但AlphaGo的故事还远没有结束，2017年10月，其升级版AlphaGo Zero诞生。自学成才AlphaGo Zero完全抛弃了来自棋谱数据的人类经验，而是通过左右互搏（自己和自己对弈）迅速提升棋力，在AlphaGo的基础上青出于蓝。放在人类世界里，这就如一个从没上过学的学霸靠自己刷题复习，刷了一个月就拿到高考状元般不可思议。</p>\n<p>可仅仅一个月后，DeepMind又推出了AlphaGo Zero的升级版：AlphaZero。AlphaZero一不需要人工特征，二不需要棋谱知识，三不需要特定优化，可就是这个“三无”算法实现了对围棋、国际象棋和日本将棋的通吃，以摧枯拉朽之势横扫这些棋类游戏的顶级算法，再一次向世人展示了它的无穷潜力。</p>\n<p>AlphaGo的胜利也是深度学习的胜利。<strong>深度学习是利用包含多个隐藏层的人工神经网络实现的学习</strong>。在介绍多层感知器时，我提到在原理上，两个隐藏层足以解决任何类型的非线性分类问题，因而浅层神经网络最多只会包含两个隐藏层。相比于浅层神经网络，正是这“多个”隐藏层给深度学习带来了无与伦比的优势。</p>\n<p>在深度学习中，每个层可以对数据进行不同水平的抽象，层间的交互能够使较高层在较低层得到的特征基础上实现更加复杂的特征提取。不同层上特征的组合既能解决更加复杂的非线性问题，也能识别更加复杂的非线性模式。</p>\n<p>与人工神经网络一样，深度学习的思想同样来源于生理学上的研究进展。1981年，两位神经生物学家大卫·胡贝尔和托尔斯滕·魏泽尔连同另一位科学家分享了诺贝尔医学奖，他们二位的主要贡献在于“发现了视觉系统的信息处理方式，即<strong>可视皮层是分级的</strong>”。</p>\n<p>1958年，胡贝尔和魏泽尔在美国的约翰霍普金斯大学开展关于瞳孔区域与大脑皮层神经元对应关系的研究。他们给小猫展现形状和亮度各不相同的物体，并改变每个物体放置的位置与角度。在这一过程中，小猫的瞳孔感受不同类型和不同强度的刺激，小猫的后脑上则被插入电极，用来测量神经元的活跃程度。</p>\n<p>这一实验的目的是验证一个假设：位于后脑皮层的不同视觉神经元与瞳孔感受到的刺激信号之间，存在某种相关性。一旦瞳孔受到某种特定的刺激，后脑皮层的某些特定神经元就会活跃。经过长期的实验，胡贝尔和魏泽尔发现了“<strong>方向选择性细胞</strong>”：当瞳孔发现了眼前物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。这一发现不仅在生理学上具有里程碑式的意义，更是无心插柳地促成了人工智能在四十年后的突破性发展。</p>\n<p>方向选择性细胞的发现表明，神经-中枢-大脑的工作过程，或许正是一个不断迭代、不断抽象的过程。人眼处理来自外界的视觉信息时，首先提取出目标物的边缘特性，再从边缘特性中提取出目标物的特征，最后将不同特征组合成相应的整体，进而准确地区分不同的物体。在这个过程中，高层特征是低层特征的组合，从低层到高层，特征变得越来越抽象，语义的表现就越来越清晰，对目标物的识别也就越来越精确。</p>\n<p>深度学习正是受启于视觉系统的感受方式。在深度学习中，这个过程可以利用多个隐藏层进行模拟。第一个隐藏层学习到“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。当然，这样的识别思想不只适用于视觉信息的处理，对其他类型的信息同样适用。</p>\n<p>虽然关于认知的理论早已成熟，但真正推动深度学习走到聚光灯下的还是工程上的两大进步，也就是<strong>数据的井喷和计算力的飙升</strong>。在目前的算法水平上，要提取出复杂的深层特征，必须要以大量训练样本为基础。简单的数据集只会让深度学习“消化不良”，因而<strong>大数据和深度学习的搭配称得上天作之合</strong>。</p>\n<p>另一方面，多个隐藏层的使用引入了大量隐藏神经元，这使得深度神经网络的参数将以指数形式增加，给网络的训练带来沉重的计算负荷。而基于向量和矩阵计算的图形处理单元（graphical processing unit, GPU）完美解决了这一问题，其庞大的处理吞吐量能够使深度神经网络的训练显著提速，因而也成了深度学习的标配硬件。<strong>如果说深度学习是一台探矿机，大数据就是那座有待挖掘的金矿，计算能力的进展则为这台探矿机提供了源源不断的动力</strong>。</p>\n<p>尽管取得了很多成果，但关于深度学习技术的争论正显出愈演愈烈的趋势，“深度学习是炼金术”的争议更是让不少大咖针锋相对，嘴仗打个不停。研究者批判深度学习的一个焦点在于其缺乏坚实的理论基础。在机器学习和浅层神经网络中，绝大部分最优解的最优性都是可以证明的。但在深度学习中，很多结果都是经验性而非理论性的，隐隐透出一丝看天吃饭的无力感。</p>\n<p>抛开互怼的言论不论，深度学习在理论上依然有诸多要解决的问题，这也催生了“信息瓶颈”等新理论与新方法。目前看来，深度学习的背后深入的数学原理仍处在隐藏层中，我们唯一能够确定的是关于深度学习的研究依然任重道远。</p>\n<p>今天我和你分享了深度学习的一些简介，其要点如下：</p>\n<ul>\n<li>深度学习实际上是基于具有多个隐藏层的神经网络的学习；</li>\n<li>深度学习的思想来源于人类处理视觉信息的方式；</li>\n<li>深度学习的发展得益于数据的井喷和计算力的飙升；</li>\n<li>深度学习的理论基础依然有待深入。</li>\n</ul>\n<p>深度学习的理论问题已经成为人工智能讨论中的一个焦点。那我们到底是不管三七二十一，只需要享受深度学习方法所带来的红利；还是打破砂锅问到底，把背后的道理分析得水落石出呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/47/fd/47ee8f04b5cad11d9b3c03ec2b2ccbfd.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"（课外辅导）人工神经网络 | 拓展阅读参考书","id":2655},"right":{"article_title":"23 深度学习 | 前方有路，未来可期：深度前馈网络","id":2874}}},{"article_id":2874,"article_title":"23 深度学习 | 前方有路，未来可期：深度前馈网络","article_content":"<p>深度前馈网络（Deep Feedforward Network）是具有深度结构的前馈神经网络，可以看成是进化版的多层感知器。与只有一个或两个隐藏层的浅层网络相比，深度前馈网络具有更多的隐藏层数目，从而具备了更强的特征提取能力。</p>\n<p>深度前馈网络不考虑输入数据可能具备的任何特定结构，也就是不使用关于数据的先验信息。但特征提取能力增强的代价是运算复杂度的提升。因而，<strong>网络架构的建立、损失函数的选择、输出单元和隐藏单元的设计、训练误差的处理等问题就成为深度前馈网络设计中的一系列核心问题</strong>。</p>\n<p><strong>在深度前馈网络的设计中，确定架构是首要考虑的关键问题</strong>。架构决定着网络中包含多少基本单元，以及这些基本单元之间如何相互连接。几乎所有前馈网络采用的都是链式架构，即前一层的输出是后一层的输入。在这样的链式架构中，层的数目和每一层中神经元的数目就是网络的主要变量。</p>\n<p>介绍多层感知器时我曾提到了通用逼近的性质，这个性质的严格形式是通用逼近定理。<strong>通用逼近定理的内容是如果一个前馈网络具有单个隐藏层，这个隐藏层又有足够但是有限数目的神经元，这个神经网络就可以以任意精度逼近任意连续函数</strong>。虽然在这个定理的初始证明中，隐藏神经元的传递函数是具有“挤压”性质的非线性函数，但定理的成立性实际上并不取决于传递函数的性质，而是由网络的前馈架构所决定的。</p>\n<p>通用逼近定理是一个存在性定理，它说明需要的神经网络是肯定存在的，却并没有指明具体的构造方法。所以在给定一个目标函数时，我们可以确定单隐藏层的感知器一定能够将它表示出来，却对隐藏层需要多少神经元毫无把握。这个数目很可能是个天文数字，这会让网络结构在计算机上根本无法实现。即使能够设计出这么复杂的算法，要对它进行训练和泛化也近乎天方夜谭。</p>\n<!-- [[[read_end]]] -->\n<p>深度前馈网络的出现克服的正是单隐藏层带来的复杂性问题：<strong>使用深度架构的模型既能减少表示目标函数时所需要的单元数量，也能有效降低泛化误差，在一定程度上抑制过拟合的发生</strong>。深度架构对复杂函数具有更强的表示能力，这一点已经在数学上得到证明。除此之外，深度架构也暗含了这样的假设：<strong>待学习的复杂函数可以视为若干简单函数的层次化结合</strong>。由于深度学习的思想本就借鉴自神经科学，这个假设也就合情合理了。</p>\n<p>在层与层之间，深度架构采用的最常见的方式是<strong>全连接</strong>，意味着相邻层次中的任意神经元都两两相连。全连接是最简单也最复杂的架构，说它简单是因为连接方式只有一种选择，不需要考虑保留哪些连接或是放弃哪些连接；说它复杂是因为这种方式需要确定的参数最大，给网络训练带来了很大的麻烦。目前也有大量神经网络使用了稀疏的连接方式，这需要结合应用场景具体分析。</p>\n<p><strong>任何机器学习算法都可以看成是对某个预设函数的最优化方法，深度前馈网络也不例外</strong>。与其他神经网络一样，深度前馈网络也利用<strong>梯度信息</strong>进行学习，在处理误差时采用的是<strong>反向传播方法</strong>，利用反向传播求出梯度后再使用<strong>随机梯度下降法</strong>寻找损失函数的最小值。但深度网络的非线性特性常常会影响到学习算法的收敛性能，这也是使用非线性函数的神经网络的一个固有特性。</p>\n<p>在学习中，损失函数的选择是深度神经网络设计中另一个重要环节。深度前馈网络选择损失函数的准则与其他机器学习算法并无二致：<strong>回归问题的损失函数通常是最小均方误差，而分类问题的损失函数通常是交叉熵</strong>（Cross-Entropy）。</p>\n<p>交叉熵描述的是真实的数据分布和神经网络输出分布之间的差异性。当我们使用神经网络的输出去预测符合真实分布的测试数据时，网络输出并不能完全消除关于测试数据的不确定性。这部分剩余的不确定性可以由泛化误差体现，在信息论中则由交叉熵来定义。</p>\n<p>如果$p(x)$和$q(x)$是两个离散分布，则两者的交叉熵可以表示为</p>\n<p>$$ H(p, q) = - \\sum_x p(x) \\log q(x) $$</p>\n<p>在深度前馈网络中，损失函数被定义为分类结果$\\mathbf{y}$关于训练数据$\\mathbf{x}$的条件概率的负对数之和</p>\n<p>$$ J(\\mathbf{\\theta}) = - \\sum_k \\ln p(\\mathbf{y}_k | \\mathbf{x}_k)$$</p>\n<p>确定损失函数后，网络的训练过程就是找到使以上表达式最小的参数$\\mathbf{\\theta}$的过程。<strong>其实无论是最小均方误差还是交叉熵，体现的都是概率论中最大似然估计的原理</strong>。最小均方误差与最大似然估计的关系在线性回归算法中已有说明，你可以回忆一下。而分类问题中，只要条件概率具有高斯分布的形式，训练数据与模型分布之间的交叉熵的最大化与最小均方误差同样殊途同归。</p>\n<p>使用对数似然作为损失函数的一个优点是它的通用性，概率论上的直观意义使它适用于几乎所有的模型。另一个好处则体现在它可以避免因损失函数饱和造成的梯度消失，这是由于对数操作可以抵消掉大部分传递函数中的指数效果，从而更清晰地展示出变化趋势。</p>\n<p>损失函数的表示与输出单元的选择密切相关，输出单元的传递函数决定了交叉熵的具体表达式。输出层的作用是对隐藏层提取出的特征施加额外的变换以得到输出，变换的形式则有多种选择。最简单的变换形式就是线性变换，它将隐藏特征的线性组合作为输出，简单而实用。</p>\n<p>除了线性变换，<strong>对数几率函数</strong>也是一种常用的变换函数，其形式在机器学习模块的逻辑回归中已经做过介绍。对数几率函数得到的是一类软输出，因为它能将隐藏特征的线性组合映射到概率之上。将对数几率函数加以推广就可以得到<strong>softmax函数</strong>，也叫<strong>柔性最大值函数</strong>。对数几率函数只能解决二分类问题，softmax函数则可以解决多分类问题，也就是每个样本可以同时属于不同的类别。softmax函数的形式同样在逻辑回归中做了介绍，你可以回忆一下。</p>\n<p>前面介绍的三种传递函数是输出单元的常见选择，对大部分机器学习模型是通用的。<strong>而在深层前馈网络的设计中，一个独有的问题就是隐藏单元的设计，也就是隐藏神经元的传递函数如何选择</strong>。但这个问题目前尚不存在放之四海而皆准的通用准则，关于这个环节的有限知识全都集中在一类常用的激活函数上，也就是<strong>整流线性单元</strong>（Rectified Linear Unit, ReLU）。</p>\n<p>整流线性单元是隐藏单元理想设计的万金油，当你不知道选择何种激活函数时，那么用它准保没错。整流线性单元的数学表达式是</p>\n<p>$$ {\\text{ReLU}}(x) = \\max { 0, x } $$</p>\n<p>整流线性单元在一半定义域上取值为0，在另一半定义域上则处于激活状态。虽然在0点这个间断点上不满足可微的条件，但在激活的区域上，整流线性单元的一阶导数是常数1，二阶导数则恒等于0。相比于引入二阶效应的其他激活函数，整流线性单元的梯度方向在学习中具有更大的用处。</p>\n<p>整流线性单元的一个缺点是它可能会“杀死”特定的隐藏神经元，因为一旦取值为0，这个函数就不会重新激活。解决这个问题的方法是对原始函数做一些微小的修正，得到的变体包括<strong>渗漏整流单元</strong>（Leaky ReLU）和<strong>指数整流单元</strong>（Exponential ReLU），它们的共同点是将原来的0输出替换为一个较小的数值，以避免神经元的失效。</p>\n<p>除了整流线性单元之外，<strong>对数几率函数</strong>和<strong>双曲正切函数</strong>等其他函数也可以用于隐藏神经元中，但并不常见。相比于这些函数，整流线性单元的优势在于它的线性特性，这一性质使得模型更加容易优化，对黑箱中的隐藏神经元而言尤其如此。</p>\n<p>可如果要针对执行特定任务的深度前馈网络选择最优化的隐藏单元，就是个依赖经验的过程。想要预测哪种隐藏单元的性能最佳完全依赖于直觉，至于直觉靠不靠谱，就得通过验证数据集来检验训练效果。从这个角度看，说深度学习是撞大运的炼金术不是没有道理的。</p>\n<p>今天我和你分享了作为学习模型的深度前馈网络存在的一些共性问题，其要点如下：</p>\n<ul>\n<li>深度前馈网络利用深度架构实现工程上可实现的对任意函数的通用逼近；</li>\n<li>深度前馈网络使用梯度下降的方法进行学习；</li>\n<li>深度前馈网络的损失函数通常是交叉熵或最小均方误差；</li>\n<li>深度前馈网络的隐藏神经元通常使用整流线性单元作为传递函数。</li>\n</ul>\n<p>在深度前馈网络中，深度结构是在空间维度上展开的，那么能否在时间维度上引入类似的结构，实现不同时刻之间的信息流动呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/06/c1/069de9897e302f8f9dc372b7459a0ec1.jpg\" alt=\"\" /></p>\n","neighbors":{"left":{"article_title":"22 深度学习 | 空山鸣响，静水流深：深度学习概述","id":2656},"right":{"article_title":"24 深度学习 | 小树不修不直溜：深度学习中的正则化","id":2875}}},{"article_id":2875,"article_title":"24 深度学习 | 小树不修不直溜：深度学习中的正则化","article_content":"<p><strong>正则化（Regularization）作为抑制过拟合的手段，是机器学习和深度学习之中必不可少的环节，具有举足轻重的地位</strong>。好的机器学习算法不仅要在训练集上表现出色，当推广到未知的测试数据时，其优良的性能依然能够得以保持。<strong>正则化就是一类通过显式设计降低泛化误差，以提升算法通用性的策略的统称</strong>。由于深度学习中涉及的参数众多，正则化就变得尤为重要。</p>\n<p><strong>正则化被定义为对学习算法的修改，这些修改的目的在于减少泛化误差</strong>。通常说来，泛化误差的下降是以训练误差的上升为代价的，但有些算法也能兼顾泛化误差和训练误差的良好性能。</p>\n<p><strong>正则化处理可以看成是奥卡姆剃刀原则（Occam&#39;s razor）在学习算法上的应用</strong>。奥卡姆剃刀原则的表述是：“当两个假说具有完全相同的解释力和预测力时，以那个较为简单的假说作为讨论依据。”在机器学习中，正则化处理得到的正是更加简单的模型。</p>\n<p><strong>从概率论角度看，许多正则化技术对应的是在模型参数上施加一定的先验分布，其作用是改变泛化误差的结构</strong>。正则化是对欠拟合和过拟合的折中，在不过度增加偏差的情况下显著减少方差。正则化能够改变数据分布，让通过模型得到的数据分布尽可能和真实的数据生成过程相匹配。</p>\n<p>虽然目前在深度学习中应用的正则化方式称得上“八仙过海，各显神通”，却并不存在能够系统描述这些方法、并进一步指导设计的通用主线。因此，要从通观全局的角度看待正则化处理，还是要“不忘初心”，从根本目的着眼。</p>\n<p>机器学习的任务是拟合出一个从输入$x$到输出$y$的分布，拟合的过程是使期望风险函数最小化的过程。正则化处理使待最小化的函数中既包含结构化的误差函数，也包含人为引入的正则化项。由于未知分布的期望风险不能直接求解，因而需要引入训练数据集，以在训练数据集上计算出的经验风险来近似期望风险，并通过经验风险最小化实现期望风险最小化。</p>\n<p>以上就是学习算法的整体流程，也是正则化大展拳脚的主战场，正则化的处理就是针对学习算法中的不同变量来展开的。这样看来，正则化策略就可以分为以下几类：</p>\n<!-- [[[read_end]]] -->\n<ul>\n<li>基于训练数据（data）的正则化</li>\n<li>基于网络架构（network architecture）的正则化</li>\n<li>基于误差函数（error function）的正则化</li>\n<li>基于正则化项（the regularization term）的正则化</li>\n<li>基于最优化过程（optimization）的正则化</li>\n</ul>\n<p>训练模型的质量很大程度上取决于训练数据。 除了选择噪声较小的训练数据外，还可以通过正规化来提升训练数据的质量。正则化处理数据的一个目的是执行预处理和特征提取，从而将特征空间或数据分布修改为其他形式； 另一个目的是通过生成新样本来创建具有更大容量、甚至是无限容量的增强数据集。这两个目的之间相互独立，因而可以结合起来使用。</p>\n<p><strong>对训练数据正则化的做法是在训练数据集上施加变换，从而产生新的训练数据集</strong>。变换的形式是以满足某种概率分布的随机变量为自变量的函数，最简单的实例就是向数据添加随机的高斯噪声。由于提升机器学习模型泛化性能最直接的办法就是使用更多的数据进行训练，因而使用随机参数的变换可以用于生成“假”数据，这种方法被称为<strong>数据集增强</strong>（data augmentation）。</p>\n<p>数据集增强的对象通常只包括输入层和隐藏层，而不包括输出层。这样做的效果是将训练数据集映射到一个新的概率分布上，并用这个新分布来计算经验风险函数。变换中参数的随机性使我们可以在不同的参数之下生成多组新数据，从而通过数据量的增加来减小期望风险和经验风险之间的差别。</p>\n<p>除了数据集增强外，另一种针对训练数据的正则化方法是 <strong>Dropout</strong>。Dropout是一种集成方法，通过结合多个模型来降低泛化误差。之所以说Dropout属于基于训练数据的正则化，是因为它构造不同的数据集来训练不同的模型，每个数据集则通过对原始数据集进行“有放回采样”得到。</p>\n<p>Dropout的关键想法是在训练期间从神经网络中随机丢弃神经元及其连接，得到简化的网络。而在测试时，一个简单的小权重网络就可以逼近所有这些简化网络的预测的平均效果。其优点在于计算简单方便，同时还具有对不同的模型和训练过程的普适性。但Dropout对训练集容量的要求很高，少量训练样本并不能发挥其优势。</p>\n<p>从输入到输出的映射必须具有某些特质才能很好地适应数据，而对输入-输出映射进行假设的方法正对应着网络结构的选择，这激发了基于网络架构的正则化方法。对映射的假设既可以关注深度网络中不同层次的具体操作，也可以关注层与层之间的连接方式。<strong>基于网络架构的正则化通常会简化关于映射的假设，再让网络架构逐步逼近简化后的映射</strong>。这限制了模型的搜索空间，为找到更好的解提供了可能性。</p>\n<p><strong>参数共享（weight sharing）是一类重用参数的正则化方法</strong>。通过强迫某些参数相等，可以让不同模型共享唯一的参数，从而让它们对相似的输入产生相似的输出。如果放宽参数共享的条件，使它们不必完全相等而是相互接近，对应的就是对参数范数添加正则化项。常用的参数共享方法是将一个监督学习模型的参数正则化，令其接近另一个无监督学习的模型，那么这个无监督学习模型就可以匹配监督模型的参数。</p>\n<p><strong>另一种针对网络架构的处理是对传递函数的正则化（activation regularization）</strong>。一些传递函数是专门为正则化设计的，比如在Dropout中使用的maxout单元，它能在测试时更精确地近似模型集合预测结果的几何平均值。而通过添加噪声，原始的确定传递函数就可以被泛化为随机模型，其分布特性也就可以被利用起来。</p>\n<p>基于误差函数的正则化和基于正则化项的正则化可以放在一起讨论。理想情况下，误差函数应当适当地反映算法的性能，并体现出数据分布的一些特点（比如均方误差或交叉熵）。对误差函数进行正则化就相当于添加额外的学习任务，从而导致其目标发生变化，这部分变化就会体现在误差函数中额外的正则化项上。因而在大部分情况下，对基于正则化项的正则化的讨论就包含了基于误差函数的正则化。</p>\n<p><strong>正则化项也叫做惩罚项</strong>。与误差函数不同，正则化项与目标无关，而是用于表示所需模型的其他属性。误差函数表示的是算法输出与目标输出之间的一致性，正则化项表示的则是关于映射关系的额外的假设。 这一特点决定了正则化项的值可以通过未标记的测试样本来计算，利用测试数据改进学习模型。</p>\n<p><strong>常用的正则化项是权重衰减项（weight decay）</strong>。深度学习中的参数包括每个神经元中的权重系数和偏置。由于每个权重会指定两个变量之间相互作用的方式，因而拟合权重所需要的数据量要比拟合偏置多得多。相比之下，每个偏置只控制一个变量，即使不对它做正则化也不会产生太大方差，正则化的方式不对反而还会增加算法的偏差。这是正则化的对象只包括权重而不包括偏置的原因。</p>\n<p>在权重衰减中，正则化项是以范数的形式表示的，常用的范数包括$L ^ 2$范数和$L ^ 1$范数。回忆一下，当这两种范数作为正则化项被应用在线性回归中时，分别对应着岭回归和LASSO回归。</p>\n<p>$L ^ 2$范数作为正则化项时，其作用是使权重系数更加接近原点。引入权重衰减后，在每一步的梯度更新之前，权重向量都会被收缩。整体来看，这使得在显著减小目标函数方向上的权重保留完好，无益于目标函数减小的方向所对应的分量则会因正则化而被逐渐地衰减掉。从泛化误差的角度来说，$L ^ 2$范数能够感知具有较高方差的输入，与这些输入特征相关的权重则被收缩。</p>\n<p>相比之下，$L ^ 1$范数和$L ^ 2$范数有本质上的区别。$L ^ 1$正则化得到的是稀疏的解，它将一部分较小的权重直接砍掉。这样做可以从可用的特征子集中选择出有意义的特征，从而简化学习问题。</p>\n<p><strong>最后一类正则化方法是基于最优化过程的正则化</strong>。根据其作用阶段的不同，这类正则化方法可以分为三种：<strong>对初始化（initialization）的正则化，对参数更新（weight update）的正则化，对终止条件（termination）的正则化</strong>。对初始化的正则化影响的是权重系数的最初选择，这既可以通过在特定的概率分布中选择初始参数完成，也可以经由预训练实现。对参数更新的正则化则包括对更新规则的改进和对权重参数的过滤。</p>\n<p><strong>早停（early dropping）</strong>是一类重要的正则化方法，它是针对终止条件的正则化。当训练的表示能力过强时，泛化误差会呈现出U形：随着训练时间的增加先降低再升高。这意味着只有返回使泛化误差最低的参数集设置，才能得到更低的泛化误差。当测试集上的泛化误差在预先指定的循环次数内没有改善时，训练就会终止，这种策略就是早停。</p>\n<p><strong>早停的简单性和有效性使它成为深度学习中应用最广泛的正则化方法</strong>。它不需要改变训练过程，也就不会影响到学习动态，唯一要注意的问题就是要避免陷入局部最优解。早停的正则化效果在数学上也有解释，在简单的模型下，早停和$L ^ 2$正则化是等价的。</p>\n<p>需要注意的是，以上的分类方式并不是互斥的，一种正则化方法通常针对的是学习算法中的多个变量。做出分类的目的是帮助你更好地理解正则化的设计思路。此外，前文对具体正则化方法的介绍也是挂一漏万。正则化是深度学习研究中最活跃的领域之一，如果要深入了解每种方法，最好的办法莫过于阅读原始文献。</p>\n<p>今天我和你分享了深度学习中实现正则化的思路，其要点如下：</p>\n<ul>\n<li><p>基于训练数据的正则化方法包括数据集增强和Dropout；</p>\n</li>\n<li><p>基于网络架构的正则化方法包括参数共享和传递函数正则化；</p>\n</li>\n<li><p>基于误差函数和正则化项的正则化方法包括使用$L ^ 2$范数和$L ^ 1$范数；</p>\n</li>\n<li><p>基于最优化过程的正则化方法包括早停。</p>\n</li>\n</ul>\n<p>既然正则化方法可以基于学习算法中的多个变量来实现，那么你觉得那种方式能够达到最好的抑制过拟合效果呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/9f/2d/9fa7cbe21ce4e114a2712ed2d9508b2d.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"23 深度学习 | 前方有路，未来可期：深度前馈网络","id":2874},"right":{"article_title":"25 深度学习 | 玉不琢不成器：深度学习中的优化","id":2876}}},{"article_id":2876,"article_title":"25 深度学习 | 玉不琢不成器：深度学习中的优化","article_content":"<p>除了正则化之外，优化也是深度学习需要解决的一个核心问题。由于深度神经网络中的隐藏层数目较多，因而将整个网络作为一个整体进行优化是非常困难的事情，需要花费大量的时间和计算力。出于效率和精确性的考虑，在深度学习的优化上需要使用专门的技术。</p>\n<p>出于可解性的考虑，传统机器学习算法往往会小心翼翼地选择代价函数和优化条件，将待优化问题转化为容易求解的<strong>凸优化问题</strong>。但在神经网络，尤其是在深度神经网络中，更一般的非凸情况是不可避免的，这就给深度学习中的优化带来很多额外的挑战。</p>\n<p>当待优化的代价函数的输入是$n$维向量时，其二阶导数就包含针对不同变量分别求偏导所得到的很多分量，将这些分量按顺序排列可以得到<strong>Hessian矩阵</strong>。而在神经网络的训练中，Hessian矩阵的病态问题非常常见，甚至无论优化问题是否具有凸优化的形式，病态的Hessian矩阵都会出现。</p>\n<p>在线性方程$\\mathbf{A} \\mathbf{x} = \\mathbf{b}$中，当系数矩阵$\\mathbf{A}$的微小扰动会给解集$\\mathbf{x}$带来较大幅度的波动时，这样的矩阵就被称为<strong>病态矩阵</strong>（ill-conditioned matrix）。</p>\n<p>病态矩阵是科学计算不愿打交道的对象，因为数值精度导致的不可避免的舍入误差可能会给输出带来巨大的偏离，正所谓“差之毫厘，谬以千里”。在神经网络的训练中，病态矩阵的影响体现在梯度下降的不稳定性上。当应用随机梯度下降解决优化问题时，病态矩阵对输入的敏感性会导致很小的更新步长也会增加代价函数，使学习的速度变得异常缓慢。</p>\n<p><strong>深度神经网络面临的另一个挑战是局部极小值的问题</strong>。凸优化问题的数学特性保证了局部极小值和全局最小值之间的等价关系。因而在优化一个凸问题时，任何形式的临界点都可以看成是可行解。而在神经网络，尤其是深度模型中，代价函数甚至会具有不可列无限多个局部极小值，这显然会妨碍对全局最小值的寻找，导致搜索陷入局部最优的陷阱中。</p>\n<!-- [[[read_end]]] -->\n<p>神经网络之所以会具有这么多局部极小值，原因在于隐藏变量的不可辨认性。如果将神经网络中的几个隐藏神经元及其所有系数调换的话，得到的新模型和原始模型之间是等价的。假定深度模型中包含$m$个隐藏层，每个层中又都有$n$个神经元，那么隐藏单元的排列方式就会多达$(n!) ^ m$种。这么多神经网络的变体是没法相互区分的，因而它们都有相同的局部极小值。</p>\n<p>除了局部极小值外，另一类在优化中不受欢迎的点叫做<strong>鞍点</strong>（saddle point）。鞍点是梯度为0的临界点，但它既不是极大值也不是极小值。从函数图像上看，多变量函数的鞍点在一个方向上向上弯曲，在另一个方向上则向下弯曲，从而形成了类似马鞍的形状。由于牛顿法的目标是寻找梯度为零的临界点，因而会受鞍点的影响较大，高维空间中鞍点数目的激增就会严重限制牛顿法的性能。</p>\n<p>虽然存在着这样那样的潜在问题，但深度学习采用的依然是传统优化方法及其改进，这是由于隐藏层的未知特性使设计有针对性的优化方法变得非常困难。<strong>随机梯度下降法（stochastic gradient descent）就是在传统机器学习和深度神经网络中都能发挥作用的经典算法</strong>。</p>\n<p>机器学习中的最优化是令给定的期望风险函数最小化，而期望风险又可以用训练集上的经验风险$Q(w)$代替。训练集上的经验风险等于所有样本风险函数$Q_i(w)$的均值。当训练集的数据量较大，并且经验风险又难以用简单的解析式来表示时，计算所有梯度的求和就会变成一个复杂的计算任务，消耗大量的计算资源。</p>\n<p>随机梯度下降法是原始梯度下降法的一种改良。如果把求解最优化问题想象成爬山，那么随机梯度下降法就是每走一步就换个方向。为了节省每次迭代的计算成本，随机梯度下降在每一次迭代中都使用训练数据集的一个较小子集来求解梯度的均值，这在大规模机器学习问题中，特别是深度学习中非常有效。在给定学习率$\\eta$和前一轮的参数估计值$w$时，随机梯度下降法的每一轮迭代执行以下操作</p>\n<ul>\n<li><p>从训练集中随机抽取$n$个样本，在这个样本子集上计算梯度估计值$\\dfrac{1}{n} \\sum\\limits_{i = 1}^n \\nabla_w Q_i(w)$</p>\n</li>\n<li><p>根据计算出的结果更新参数$w \\leftarrow w - \\eta \\cdot \\dfrac{1}{n} \\sum\\limits_{i = 1}^n \\nabla_w Q_i(w)$</p>\n</li>\n</ul>\n<p>以上迭代过程一直重复到$Q(w)$取得最小值或迭代次数达到最大值为止。学习率$\\eta$是随机梯度下降法中的重要参数，它通常被设置为随着训练次数的增加而逐渐减小。这一选择体现的是训练时间和优化性能之间的折中。</p>\n<p>相对于让所有训练样本的风险函数最小化的原始梯度下降法，随机梯度下降是让每几个样本的风险函数最小化，虽然不是每次迭代得到的结果都指向全局最优方向，但大方向终归是没有错的，其最终的结果往往也在全局最优解附近。<strong>在随机梯度下降法的基础上进行改进可以得到其他的优化方式，改进的手段主要有两种：一种是随机降低噪声，另一种是使用二阶导数近似</strong>。</p>\n<p>在应用中，随机梯度下降会受到噪声的影响。当学习率固定时，噪声会阻止算法的收敛；而当学习率逐渐衰减时，噪声也会将收敛速度压低到次线性水平。降噪方法正是为了抑制噪声的影响应运而生，降噪的方式既包括<strong>提升单次梯度估计的精度</strong>，也包括<strong>提升迭代过程的精度</strong>，常用的算法包括<strong>动态采样、梯度聚合和迭代平均</strong>三类。</p>\n<p><strong>动态采样和梯度聚合两类方法是通过使用固定的步长来获得线性的收敛速度，进而实现降噪</strong>。动态采样方法通过逐渐增加梯度计算中使用的子集容量来实现降噪，随着优化过程的进行，基于更多样本得到的梯度估计也就越​​来越精确。梯度聚合方法则将先前迭代中得到的梯度估计存储下来，并在每次迭代中对这些估计中的一个或多个进行更新，再通过将搜索方向定义为之前轮次中梯度估计的加权平均，来改善搜索方向的准确性。</p>\n<p>和前面两种方法相比，<strong>迭代平均方法不是通过对梯度估计求平均，而是对每次迭代得到的参数结果求平均来实现降噪</strong>。迭代平均方法可以在保证算法收敛性的前提下提升学习率，从而缩短收敛时间。这种方法在思想上更接近原始的随机梯度下降法，虽然它的收敛速度依然保持在次线性水平，却可以降低结果的方差，从而有效地对抗过拟合问题。</p>\n<p>前面三类方法的作用都是降低噪声的影响。<strong>要提升随机梯度下降法的性能，还可以通过使用二阶导数近似的信息来抑制高度非线性和病态目标函数的不利影响</strong>。在梯度下降法中应用的一阶导数不具有线性不变性，二阶导数的引入可以解决这一问题。另一方面，使用二阶导数意味着在使用泰勒展开近似目标函数时，二阶项的引入可以加快对最值点的逼近。总而言之，二阶导数近似方法主要的优势在于提升收敛速度。</p>\n<p>二阶导数近似方法都可以看成对传统的牛顿法的改进，具体方法包括拟牛顿法、高斯牛顿法和无Hessian牛顿法等，受篇幅限制，在此就不做展开介绍了。</p>\n<p>除了随机降噪和二阶导数近似这两大类主要方法之外，还有一些自立门户的算法也能够对随机梯度下降做出改进，典型的例子包括<strong>动量方法</strong>（momentum）、<strong>加速下降方法</strong>（accelerated gradient descent）和<strong>坐标下降方法</strong>（coordinate descent）。</p>\n<p>动量方法引入了超参数$v$作为速度，它代表参数在参数空间上移动的方向和速率，移动参数的力则是负梯度。速度$v$的引入就会让之前的移动产生累加效应，累加的效果则体现在学习率上。如果参数在一个下降较快的方向上移动，那么就迈开大步向前冲，对应着学习率的增加；如果参数在忽左忽右地兜圈子，那就找一条中间路线向前走。这样一来，算法就不太会陷入到局部最优中而无法抽身了。</p>\n<p>加速下降法是由俄罗斯数学家尤里·涅斯捷罗夫发明的，因而常以他的名字来命名。加速下降法的思想与标准动量方法类似，唯一的区别在于加速下降法对动量的计算是在施加当前速度之后才进行的。计算出基于之前的更新得到的动量$v$后，加速下降法先将参数$w$临时更新为$w + v$，再在$w + v$上计算梯度。与标准动量方法相比，加速下降法可以避免过快更新，因而稳定性更好。</p>\n<p>在很多情况下，“分而治之”是解决优化问题的高效方法，坐标下降法体现的就是这种思想。当优化问题中存在多个自变量时，坐标下降法先保持其他变量不变，针对单一变量$x_1$最小化$f(\\mathbf{x})$，再针对单一变量$x_2$最小化，依此类推，直到所有变量循环完成。为了保证对单一变量的优化能够达到全局优化的效果，待优化的目标函数最好是个凸函数，还可以添加$L ^ 1$范数的正则化项。此外，每轮优化中随机选取单一变量也可以取得较好的效果。</p>\n<p>深度学习中的优化问题是个复杂的问题，新的优化方法也在不断出现。但我希望能在这些乱花渐欲迷人眼的方法背后，帮你理解它们背后共性的设计准则。</p>\n<p>今天我和你分享了深度学习中实现优化的思路，其要点如下：</p>\n<ul>\n<li><p>深度学习中的优化需要解决病态矩阵、局部极小值和鞍点等问题；</p>\n</li>\n<li><p>深度学习优化中的降噪方法包括动态采样、梯度聚合和迭代平均；</p>\n</li>\n<li><p>深度学习优化中的二阶导数近似方法是对原始牛顿法的各种改进；</p>\n</li>\n<li><p>其他优化方法包括动量方法、加速下降方法和坐标下降方法。</p>\n</li>\n</ul>\n<p>既然优化方法包含不同的切入角度，那么你觉得哪一种优化方法可能具有更好的效果呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/08/5d/0839aae2ab1f3c4bfb630369d843c65d.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"24 深度学习 | 小树不修不直溜：深度学习中的正则化","id":2875},"right":{"article_title":"26 深度学习 | 空竹里的秘密：自编码器","id":3287}}},{"article_id":3287,"article_title":"26 深度学习 | 空竹里的秘密：自编码器","article_content":"<p><span class=\"orange\">自编码器</span>（auto-encoder）是一类执行无监督学习任务的神经网络结构，它的目的是学习一组数据的重新表达，也就是编码。</p>\n<p>在结构上，自编码器是包含若干隐藏层的深度前馈神经网络，其独特之处是输入层和输出层的单元数目相等；在功能上，自编码器的目的不是根据输入来预测输出，而是重建网络的输入，正是这样的功能将自编码器和其他神经网络区分开来。由于自编码器的图形表示像极了杂技中使用的道具空竹，因而也得了个“空竹网络”的雅号。</p>\n<p><strong>自编码器结构由编码映射和解码映射两部分组成</strong>。如果将编码映射记作$\\phi$，解码映射记作$\\psi$，自编码器的作用就是将输入$\\mathbf{X}$改写为$(\\psi \\circ \\phi) (\\mathbf{X})$，这相当于将输入从一个表象下转换到另一个表象下来表示，就像量子力学中粒子不同表象之间的变化一样。<strong>如果以均方误差作为网络训练中的损失函数，自编码器的目的就是找到使均方误差最小的编解码映射的组合</strong>，即</p>\n<p>$$ \\phi, \\psi = \\arg \\min_{\\phi, \\psi} || \\mathbf{X} - (\\phi \\circ \\psi) (\\mathbf{X}) || ^ 2$$</p>\n<p>在最简单的情形，也就是只有一个隐藏层的情形下，自编码器隐藏层的输出就是编码映射。当隐藏层的维度小于输入数据的维度时，这就是个<span class=\"orange\">欠完备（undercomplete）的自编码器</span>。欠完备自编码器的作用相当于对输入信号做了主成分分析，隐藏层的$k$个线性神经元在均方误差准则下保留贡献最大的$k$个主成分，原始信号就被投影到由这$k$个主成分所展成的新空间上。在自编码器的另一端，输出层将隐藏层的输出转换为自编码器的整体输出，从而实现了解码映射的功能。</p>\n<p>如果隐藏神经元的传递函数是非线性的，编码映射就能够捕捉到输入分布中更加复杂的特征，均方误差准则也可以写成对数似然函数$-\\log p(\\mathbf{X} | \\phi (\\mathbf{X}))$的形式。当误差$p(\\mathbf{X} | \\phi (\\mathbf{X}))$满足高斯分布时，均方误差和最大似然是等价的。</p>\n<p><strong>从信息论的角度看，编码映射可以看成是对输入信源$\\mathbf{X}$的有损压缩</strong>。有损压缩的特点决定了它不可能对所有输入都具有较小的信息量损失，因而学习的作用就是习得在训练数据集上更加精确的映射，并希望这样的映射在测试数据上同样表现良好，也就是使自编码器具有较好的泛化性能。</p>\n<p>当自编码器的隐藏单元数目大于输入信号的维度，也就是编码映射的分量数目大于输入信号的分量数目时，这就是个<span class=\"orange\">过度完备（overcomplete）的自编码器</span>。过度完备的自编码器面临的一个严重问题是如果没有额外约束的话，那么它可能只能够习得识别功能，得到的编码映射和解码映射都是恒等映射，这显然是白费功夫。</p>\n<!-- [[[read_end]]] -->\n<p>可出人意料的是，自编码器的这个缺点恰恰能被过度完备性克服。实际结果表明，当使用随机梯度下降训练时，过度完备的非线性自编码器能够生成有用的表示。这个结论是由将自编码器的输出作为其他分类算法的输入，并统计分类错误得到的。</p>\n<p>对这种现象的解释是当结合早停使用时，随机梯度下降的效果类似于对参数的$L ^ 2$正则化。为了实现连续输入的完美重构，具有非线性隐藏单元的单隐藏层自编码器需要在隐藏层中使用较小的权重，而在输出层使用较大的权重。尤其是在处理二进制输入时，大的权重系数还有利于最小化重构误差。</p>\n<p>但由于种种隐式或显式正则化规则的限制，大权重系数是难以实现的，因而自编码器只能对数据进行重新的编码。这意味着在新的表象下，数据的表示利用的是训练集中存在的统计规律，而不是使用恒等映射简单地复制粘贴。</p>\n<p>虽然自编码器通常只包含单个隐藏层，但这并不是强制的选择。和其他前馈网络一样，自编码器也会受益于深度结构。通用近似定理保证了具有单隐藏层的自编码器能够以任意精度逼近任意的连续函数，但难以对这样的自编码器任意添加额外的约束条件。相比之下，深度自编码器既能降低某些函数的计算成本，也能降低训练数据的数量要求，还能得到更高的压缩效率。</p>\n<p><strong>在实际中，训练深度自编码器的普遍策略是先训练一些浅层自编码器，再利用这些浅层自编码器贪心地预训练深度结构，因而浅层自编码器可以看作深度自编码器的中间件</strong>。换句话说，当需要使用3个隐藏层的深度自编码器时，我们的做法是将3个单隐藏层的自编码器逐层堆叠，而不会去直接训练一个5层的深度架构。<strong>用浅层自编码器搭建成的深度自编码器被称为栈式自编码器</strong>（stacked autoencoder）。</p>\n<p><strong>栈式自编码器的训练策略可以归结为两个步骤：无监督预训练+有监督微调</strong>。无监督预训练就是每个浅层自编码器的工作方式。原始数据输入$\\mathbf{X}$被用于训练网络的第一层，得到参数后，网络第一层就可以将原始输入转化成为由隐藏单元激活值组成的向量$\\mathbf{X} ^ 1$；接下来把$\\mathbf{X} ^ 1$作为第二层的输入，继续训练得到第二层的参数后又可以计算出第二层的输出$\\mathbf{X} ^ 2$。对后面的各层也采取策略，即将前层的输出作为后层输入的方式依次训练。</p>\n<p>在以上的训练方式中，训练每一层的参数时都会保持其他各层的参数固定不变。如果要得到全局意义上更优的结果，需要在上述预训练过程完成之后，利用反向传播算法同时调整所有层的参数以改善全局性能，这个过程就是<span class=\"orange\">微调</span>。在参数训练到接近收敛时，使用微调会得到良好的效果。如果设计的深度自编码器只是用于分类的话，惯用的做法是丢掉栈式自编码器的解码映射，直接把最后一个隐藏层的输出送入softmax分类器进行分类。如此一来，softmax分类错误的梯度值就可以直接反向传播给编码映射了。</p>\n<p>同其他深度结构一样，逐层训练给栈式自编码器带来的是强大的表达能力，这体现在它通常能够获取到输入的从部分到整体的层次化结构。如果栈式自编码器的输入是几何形状，那么它的第一层就会学习如何识别边缘，第二层会在第一层学到的边缘概念的基础上，学习如何通过边缘的组合得到轮廓，更高层还会学到如何通过轮廓的组合得到不同的形状这类更高阶的特征。</p>\n<p><strong>自编码器面对的一个问题是，对输入信号什么样的表达才能称为好的表达呢</strong>？同深度学习领域中的大多数问题一样，这个问题也不存在标准答案。从不同的角度回答它，得到的就是对原始自编码器的不同改进。前文中提到的过度完备的自编码器就是改进之一。由于在高维的隐藏层中，大部分神经元是被抑制的，只有少数能够输出特征表达，因而这类结构又被称为<span class=\"orange\">稀疏自编码器</span>（sparse autoencoder）。</p>\n<p>稀疏自编码器以稀疏性参数表示神经元的平均活跃程度，稀疏性参数为0.01意味着每次映射中只有1%的神经元被激活，但每次被激活的不会总是相同的1%。稀疏自编码器在损失函数中引入KL散度作为正则化项，来控制激活神经元的数目，进而控制特征表达的有效性。</p>\n<p>如果一个自编码器要从噪声中恢复出信号，对被干扰的非理想数据$\\tilde{\\mathbf{X}}$进行编解码，就不能对输入仅仅施加恒等复制，而是要移除噪声的影响，这样的自编码器就是<span class=\"orange\">去噪自编码器</span>（denoising autoencoder）。相比于原始的自编码器，去噪自编码器要求对非理想数据的处理结果尽可能逼近原始数据，最小均方误差的形式也变为$|| \\mathbf{X} - (\\phi \\circ \\psi) (\\tilde{\\mathbf{X}}) || ^ 2$。</p>\n<p>在去噪自编码器的训练中，需要引入条件分布$p(\\tilde{\\mathbf{X}} | \\mathbf{X})$，这个分布表示了从数据样本$\\mathbf{X}$产生损坏样本$\\tilde{\\mathbf{X}}$的概率。自编码器则将$(\\mathbf{X}, \\tilde{\\mathbf{X}})$作为训练样本来学习重构分布$p(\\mathbf{X} | \\tilde{\\mathbf{X}})$，也就是在从损坏样本到原始数据样本的逆概率问题。在引入编码映射和解码映射之后，待学习的重构分布就变成了$p(\\mathbf{X} | (\\phi \\circ \\psi) (\\tilde{\\mathbf{X}}))$。</p>\n<p><strong>上面的逆概率最大化问题可以转化为负对数似然的最小化，并利用基于梯度的方法求解。另一种训练方法是得分匹配</strong>，它让模型在每个样本点上获得与数据分布相同的得分，因而可以得到概率分布的一致估计，得分的形式则是一个特定的梯度场。</p>\n<p>去噪自编码器的作用是对抗信号中的噪声，<span class=\"orange\">收缩自编码器</span>（contractive autoencoder）的作用则是对抗信号中的微小扰动，这可以通过在损失函数中添加显式的正则化项来实现。正则化项首先要计算隐藏层输出值关于权重的<span class=\"orange\">雅各比矩阵</span>（Jacobi matrix），再来计算得到的雅各比矩阵的 <span class=\"orange\">Frobenius范数</span>，也就是矩阵所有元素平方和的平方根。通过雅各比矩阵和F范数的计算，收缩自编码器就能抑制训练样本在低维度流形曲面上的扰动。</p>\n<p>如果要使用自编码器来建立生成模型，用到的就是<span class=\"orange\">变分自编码器</span>（variational autoencoder）。变分自编码器的主要特点是在编码映射中添加了额外的限制，使隐藏层的参数大致遵循正态分布。这样一来，自编码器的所有参数都被转换为随机变量，计算随机变量分布参数的过程就是“推理”的过程。引入变分逼近法可以将推理问题转化为优化问题，并使用随机梯度下降法解决。</p>\n<p>由于均方误差只适用于描述不同数值之间的差异，而不能描述不同概率分布的区别，因而变分自编码器同样使用KL散度作为误差度量。在有限的训练数据集上，变分自编码器能够学习到样本的概率分布，并利用这个概率分布进一步生成新的样本，因而可以作为生成模型使用。</p>\n<p>今天我和你分享了深度学习中自编码器的原理与特点，其要点如下：</p>\n<ul>\n<li>自编码器是一种无监督学习方式，目的在于学习数据的重新表达；</li>\n<li>多个浅层自编码器级联可以得到深度的栈式自编码器，并使用无监督预训练结合有监督微调的方式加以训练；</li>\n<li>稀疏自编码器利用稀疏的高维表达提取出训练集中隐含的统计规律；</li>\n<li>变分自编码器对隐藏层做参数化处理，可以用于学习数据的生成模型。</li>\n</ul>\n<p>深度神经网络既可以用于分类，也可以用于特征提取，而自编码器恰恰具有提取特征的作用。那么如何看待基于深度学习的特征提取和基于人工经验的特征提取之间的区别呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/5a/0d/5ae922d7af41c821bb06cb95d10ee50d.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"25 深度学习 | 玉不琢不成器：深度学习中的优化","id":2876},"right":{"article_title":"27 深度学习 | 困知勉行者勇：深度强化学习","id":3429}}},{"article_id":3429,"article_title":"27 深度学习 | 困知勉行者勇：深度强化学习","article_content":"<p>在2017年新鲜出炉的《麻省理工科技评论》十大突破性技术中，“强化学习”榜上有名。如果把时钟调回到一年多之前的围棋人机大战，彼时的深度强化学习在AlphaGo对李世乭的横扫中就已经初露峥嵘。而在进化版AlphaGo Zero中，深度强化学习更是大放异彩，AlphaGo Zero之所以能够摆脱对人类棋谱的依赖，其原因就在于使用纯粹的深度强化学习进行端到端的自我对弈，从而超越了人类的围棋水平。</p>\n<p>要介绍深度强化学习就不得不先说一说强化学习的故事。相比于纯人造的监督学习和无监督学习，强化学习的思想根源来自于认知科学。20世纪初，美国心理学家爱德华·桑代克在对教育过程的研究中提出了强化学习的原始理论，而作为人工智能方法的强化学习则力图使计算机在没有明确指导的情况下实现自主学习，完成从数据到决策的转变。</p>\n<p><strong><span class=\"orange\">强化学习</span>（reinforcement learning）实质上是智能系统从环境到行为的学习过程，智能体通过与环境的互动来改善自身的行为，改善准则是使某个累积奖励函数最大化</strong>。具体来说，强化学习是基于环境反馈实现决策制定的通用框架，根据不断试错得到来自环境的奖励或者惩罚，从而实现对趋利决策信念的不断增强。它强调在与环境的交互过程中实现学习，产生能获得最大利益的习惯性行为。</p>\n<p>强化学习的特点在于由环境提供的强化信号只是对智能体所产生动作的好坏作一种评价，和监督学习中清晰明确的判定结果相比，环境的反馈只能提供很少的信息。所以强化学习需要在探索未知领域和遵从已有经验之间找到平衡。一方面，智能体要在陌生的环境中不断摸着石头过河，来探索新行为带来的奖励；另一方面，智能体也要避免在探索中玩儿脱，不能放弃根据已有经验来踏踏实实地获得最大收益的策略。</p>\n<p><strong>描述强化学习最常用的模式是<span class=\"orange\">马尔可夫决策过程</span></strong>（Markov decision process）。马尔可夫决策过程是由离散时间随机控制的过程，可以用以下的四元组来定义</p>\n<ul>\n<li>$S$：由智能体和环境所处的所有<span class=\"orange\">可能状态</span>构成的有限集合</li>\n<li>$A$：由智能体的所有<span class=\"orange\">可能动作</span>构成的有限集合</li>\n<li>$P_a(s, s&#39;) = \\text{Pr}(s_{t + 1} = s&#39; | s_t = s, a_t = a)$：智能体在$t$时刻做出的动作$a$使马尔可夫过程的状态从$t$时刻的$s$转移为$t + 1$时刻的$s&#39;$的概率</li>\n<li>$R_a(s, s&#39;)$：智能体通过动作$a$使状态从$s$转移到$s&#39;$得到的实时奖励</li>\n</ul>\n<!-- [[[read_end]]] -->\n<p>除了这个四元组之外，<em>强化学习还包括一个要素，就是描述主体如何获取奖励的规则</em>。强化学习主体和环境之间的交互是以离散时间步的方式实现的。在某个时间点上，智能体对环境进行观察，得到这一时刻的奖励，接下来它就会在动作集中选择一个动作发送给环境。来自智能体的动作既能改变环境的状态，也会改变来自环境的奖励。而在智能体与环境不断互动的过程中，它的终极目标就是让自己得到的奖励最大化。</p>\n<p><strong><span class=\"orange\">深度强化学习</span>（deep reinforcement learning）是深度学习和强化学习的结合，它将深度学习的感知能力和强化学习的决策能力熔于一炉，用深度学习的运行机制达到强化学习的优化目标，从而向通用人工智能迈进</strong>。</p>\n<p>根据实施方式的不同，深度强化学习方法可以分成三类，分别是<em>基于价值、基于策略和基于模型的深度强化学习</em>。</p>\n<p><strong>基于价值（value-based）的深度强化学习的基本思路是建立一个价值函数的表示</strong>。价值函数（value function）通常被称为$Q$函数，以状态空间$S$和动作空间$A$为自变量。但对价值函数的最优化可以说是醉翁之意不在酒，其真正目的是确定智能体的行动策略——没错，就是前文中“基于策略”的那个策略。</p>\n<p>策略是从状态空间到动作空间的映射，表示的是智能体在状态$s_t$下选择动作$a$，执行这一动作并以概率$P_a(s_t, s_{t + 1})$转移到下一状态$s_{t + 1}$，同时接受来自环境的奖赏$R_a(s_t, s_{t + 1})$。价值函数和策略的关系在于它可以表示智能体一直执行某个固定策略所能获得的累积回报。如果某个策略在所有状态-动作组合上的期望回报优于所有其他策略，这就是个最优策略。<em>基于价值的深度强化学习就是要通过价值函数来找到最优策略</em>，最优策略的数目可能不止一个，但总能找到其中之一。</p>\n<p><strong>在没有“深度”的强化学习中，使用价值函数的算法叫做 <span class=\"orange\">Q学习算法</span></strong>（Q-learning）。Q算法其实非常简单，就是在每个状态下执行不同的动作，来观察得到的奖励，并迭代执行这个操作。本质上说，Q学习算法是有限集上的搜索方法，如果出现一个不在原始集合中的新状态，Q算法就无能为力了，所以这是一种不具备泛化能力的算法，也就不能对未知的情况做出预测。</p>\n<p>为了实现具有预测功能的Q算法，深度强化学习采用的方式是将Q算法的参数也作为未知的变量，用神经网络来训练Q算法的参数，这样做得到的就是<span class=\"orange\">深度Q网络</span>。深度Q网络中有两种值得一提的机制，分别是<span class=\"orange\">经验回放</span>和<span class=\"orange\">目标Q网络</span>。</p>\n<p>经验回放的作用就是避免“熊瞎子掰苞米，掰新的扔旧的”这种窘境。通过将以往的状态转移数据存储下来并作为训练数据使用，经验回放能够克服数据之间的相关性，避免网络收敛到局部极小值。</p>\n<p>目标Q网络则对当前Q值和目标Q值做了区分，单独使用一个新网络来产生目标Q值。这相当于对当前Q值和目标Q值进行去相关，从而克服了非平稳目标函数的影响，避免算法得到震荡的结果。</p>\n<p>2016年以来，研究者又对深度Q网络提出了其他方面的改进，感兴趣的话，你可以搜索相关的论文。</p>\n<p>既然对价值函数的学习也是以最优策略为终极目标，那为什么不绕开价值函数，直接来学习策略呢？</p>\n<p><strong>基于策略（strategy-based）的深度强化学习的基本思路就是直接搜索能够使未来奖励最大化的最优策略</strong>。具体的做法是利用深度神经网络对策略进行参数化的表示，再利用策略梯度方法进行优化，通过不断计算总奖励的期望关于策略参数的梯度来更新策略参数，最终收敛到最优策略上。</p>\n<p><em>策略梯度方法的思想是直接使用逼近函数来近似表示和优化策略，通过增加总奖励较高情况的出现概率来逼近最优策略</em>。其运算方式和深度学习中的随机梯度下降法类似，都是在负梯度的方向上寻找最值，以优化深度网络的参数。</p>\n<p>这种方法的问题是在每一轮的策略梯度更新中都需要大量智能体与环境的互动轨迹作为训练数据，但在强化学习中，大量的在线训练数据是难以获取的，这无疑给策略梯度方法带来了很大的限制。</p>\n<p><strong>一种实用的策略梯度方法是<span class=\"orange\">无监督强化辅助学习</span></strong>（UNsupervised REinforcement and Auxiliary Learning），简称<span class=\"orange\">UNREAL算法</span>。<em>UNREAL算法的核心是行动者-评论家（actor-critic）机制，两者分别代表两个不同的网络</em>。</p>\n<p>行动者是策略网络，用于对策略进行更新；评论家则是价值函数网络，通过逼近状态-动作对的价值函数来判定哪些是有价值的策略。这种机制就和人类的行为方式非常接近了，也就是用价值观来指导行为，而行为经验又会对价值观产生反作用。</p>\n<p>在行动者-评论家机制的基础上，UNREAL做出了一些改进。首先是<em>采用异步训练的思想</em>，即让多个训练环境同时采集数据并执行训练，这不仅提升了数据的采样速度，也提升了算法的训练速度。在不同训练环境采集样本还能避免样本之间的强相关性，有利于神经网络的性能提升。</p>\n<p>UNREAL的另一个改进是<em>引入了多重的辅助任务</em>。多个辅助任务同时训练单个网络既能加快学习速度，又能进一步提升性能，代价则是计算量的增加。常用的辅助任务包括控制任务和回馈预测任务。需要注意的是，虽然并行执行的任务种类不同，但它们使用的都是同样的训练数据，因而可以看成是对已有数据价值更加充分的挖掘与利用。</p>\n<p>无论是基于价值还是基于策略的深度强化学习方法，都没有对环境模型做出任何先验假设。<strong>基于模型（model-based）的深度强化学习的基本思路是构造关于环境的模型，再用这个模型来指导决策</strong>。关于环境的模型可以使用<span class=\"orange\">转移概率</span>$p(r, s&#39; | s, a)$来表示，它描述了从当前的状态和动作到下一步的状态和奖励的可能性。将转移概率在状态空间和动作空间上遍历，就可以得到完整的转移概率张量。不同的转移概率可以通过深度网络训练得到。</p>\n<p>和前两种方法相比，基于模型的方法存在很多问题，相关的研究和应用也比较少。在转移概率的估计中，每个概率值上都会存在误差，而这些误差在较长的状态转移序列上累积起来，可能会达到相当惊人的水平。这将导致计算出的奖励值和真实奖励值之间的南辕北辙。</p>\n<p>可即便如此，基于模型的方法仍有较强的现实意义。它能减少与真实环境进行互动的次数，而这种互动在实践中往往是受限的。此外，如果能学到一个足够准确的环境模型，对智能体的控制难度也会大大降低。</p>\n<p>今天我和你分享了深度强化学习的简单原理与方法分类，其要点如下：</p>\n<ul>\n<li>深度强化学习是深度学习和强化学习的结合，有望成为实现通用人工智能的关键技术；</li>\n<li>基于价值的深度强化学习的基本思路是建立价值函数的表示，通过优化价值函数得到最优策略；</li>\n<li>基于策略的深度强化学习的基本思路是直接搜索能够使未来奖励最大化的最优策略；</li>\n<li>基于模型的深度强化学习的基本思路是构造关于环境的转移概率模型，再用这个模型指导策略。</li>\n</ul>\n<p>深度强化学习的三种实现方式各具特色，各有千秋，那么能不能将它们优势互补，从而发挥更大的作用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/7c/98/7cfffe5b9642991df0847f3650492d98.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"26 深度学习 | 空竹里的秘密：自编码器","id":3287},"right":{"article_title":"（课外辅导）深度学习 | 拓展阅读参考书","id":3223}}},{"article_id":3223,"article_title":"（课外辅导）深度学习 | 拓展阅读参考书","article_content":"<p>由于深度学习是近五年才流行起来的概念，参考资料因而屈指可数。要推荐深度学习的参考书，就不得不提炙手可热的 <strong>Deep Learning</strong>，中译本名为<strong>《深度学习》</strong>。这本由 <strong>Ian Goodfellow</strong>、<strong>Yoshua Bengio</strong> 和 <strong>Aaron Courville</strong> 合著的大部头是迄今为止唯一一部关于深度学习的专著，号称“深度学习圣经”。几位作者都是人工智能领域响当当的人物，这让本书的质量得以保证。内容上，本书既覆盖了深度学习中的共性问题，也介绍了常见的技术和未来的研究方向，广度是足够的。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d6/9a/d66fc063cd34cf2aa4eda8495dd66c9a.jpg\" alt=\"\" /></p>\n<p>这本书的问题，我认为在于过于琐碎。但这并非是作者水平问题，而是因为深度学习本身还没有形成完整的理论框架，于是阅读本书时难免会有“天上一脚，地下一脚”的感觉，章节之间难觅较强的关联性。这个角度看，这本书更像是把所有菜一股脑扔到锅里炖出来的杂菜汤，反而缺少了调理的过程。另外，前后章节的深度跨度也比较大，阅读体验就像从马里亚纳海沟一下子跳上珠穆朗玛峰。</p>\n<p>近期有一本新出的关于深度学习的中文书，名叫<strong>《人工智能中的深度结构学习》</strong>。它实际上就是 <strong>Yoshua Bengio</strong> 与2009年发表的综述性论文 <strong><a href=\"https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf\">Learning Deep Architectures for AI</a></strong> 的翻译版。和前面那本大书相比，Bengio这篇论文的架构更加明晰，至少能把深度学习这个事儿的来龙去脉讲清楚。此外，由于成文时间较早，Bengio的论文也没有太广的覆盖面，而是集中火力介绍了一些早期的基本模型。对于入门者来说，这篇论文是不错的阅读选择。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/48/9a/480ebf045e2a552dcd84b57a7df1199a.jpg\" alt=\"\" /></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/09/c6/098b4345092ce64b10bc2a0c498ee3c6.jpg\" alt=\"\" /></p>\n<p>另一篇值得推荐的综述文章是微软研究院的 <strong>Li Deng</strong> 和 <strong>Dong Yu</strong> 合撰的 <strong>Deep Learning: Methods and Applications</strong>，成文于2014年。正所谓文如其名，本文前几个章节介绍深度学习的常用方法，后几个章节介绍深度学习在语音处理、信息检索这些领域中的应用，条分缕析，娓娓道来。由于像微软这样的企业主要从事的是将理论落地的工作，因而本文关于应用的论述是颇有价值的。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/6f/e7/6f0b8f750de8b0fbab71edabcb52dee7.jpg\" alt=\"\" /></p>\n<p>除了以上的著作外，著名的计算机图书出版商O’Reilly也出版了一系列名字里面包含Deep Learning的图书。但这些书的关注点都在于实际应用，也就是深度网络的实现，因而内容也侧重于开发平台、开源库以及工具集的使用这样的内容。可能是出于完整性的考虑，这些书中也着实花了笔墨来介绍相关原理，这就让它们看起来不伦不类了。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/c3/36/c3ced74533172a1093c23274d3123b36.jpg\" alt=\"\" /></p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"27 深度学习 | 困知勉行者勇：深度强化学习","id":3429},"right":{"article_title":"28 深度学习框架下的神经网络 | 枯木逢春：深度信念网络","id":3431}}},{"article_id":3431,"article_title":"28 深度学习框架下的神经网络 | 枯木逢春：深度信念网络","article_content":"<p>2006年，深度学习的祖师爷乔弗里·辛顿提出了<span class=\"orange\">深度信念网络模型</span>，它吹响了连接主义学派复兴的号角，也打开了通向人工智能新世界的大门。</p>\n<p><strong>深度信念网络是一种概率生成模型，能够建立输入数据和输出类别的联合概率分布</strong>。网络中包含多个隐藏层，隐藏层中的隐藏变量通常是二进制数，用来对输入信号进行特征提取。输入信号从深度信念网络的最底层输入，并自下而上有向地传递给隐藏层。而在网络最上面的两层中，神经元之间的连接是没有方向并且对称的，这两个层次共同构成了联想记忆。</p>\n<p>从功能上看，深度信念网络的每一个隐藏层都代表着对输入数据的一种中间表示，而隐藏层中的每个神经元都代表着输入数据不同层次上的特征，不同层神经元之间的连接则代表着不同层次特征之间的联系，所有特征和特征之间的所有关系共同形成了对输入数据的抽象描述。</p>\n<p><strong>从结构上看，复杂的深度信念网络可以看成由若干简单的学习单元构成的整体，而构成它的基本单元就是<span class=\"orange\">受限玻尔兹曼机</span></strong>（restricted boltzmann machine）。受限玻尔兹曼机早在1986年便已诞生，可直到20年后才因辛顿的成果而得到重视。</p>\n<p><strong>受限玻尔兹曼机的模型非常简单，就是一个两层的神经网络，包括一个可见层和一个隐藏层</strong>。可见层用来接收数据，隐藏层则用来处理数据。可见层和隐藏层以全连接的方式相连，也就是任意两个不同层次中的神经元都会两两相连。但同一层中的神经元则不会互相连接，因而每个层内也就没有信息流动，这正是其名称中“受限”的来源。</p>\n<p>回忆一下神经网络中介绍过的神经元的工作机制：每个隐藏神经元的输入都是数据向量中所有元素的线性组合，这个线性组合和偏置信号相加后，共同作为神经元传递函数的输入，而传递函数的输出就是隐藏神经元的输出。但受限玻尔兹曼机所做的远非得到个输出这么简单的事情，它还要以无监督的方式对数据进行重构。即使没有更深层的网络结构，数据也会在输入层和隐藏层中进行多次前向和反向的传递。</p>\n<p>在隐藏神经元得到输出后，受限玻尔兹曼机需要将输出结果反馈给可见层。具体的做法是保持所有连接的权重系数不变，但是将方向反转，这样一来，每个隐藏单元的输出就会按照已经确定的系数反馈给可见层，可见层的每个神经元接收到的反馈信息是不同隐藏单元输出的线性组合。反馈信息和一组新的偏置分量求和就得到了对原始输入的估计，估计值和原始输入的差值则表示了重构误差。通过让重构误差在可见层和隐藏层之间循环往复地传播，就可以求出使重构误差最小化的一组权重系数。</p>\n<p>以上的学习算法就是由辛顿提出的<strong><span class=\"orange\">对比散度</span>（contrastive divergence）方法</strong>，它既能让隐藏层准确地提取可见层的特征，也能根据隐藏层的特征较好地还原出可见层。当隐藏层和可见层的神经元都使用S型函数作为传递函数时，神经元的输出就可以视为单个节点的激活概率。在这种情况下，对比散度方法具体的训练过程包括以下几个步骤：</p>\n<!-- [[[read_end]]] -->\n<ol>\n<li>输入训练样本列向量$\\mathbf{v}$，计算隐层节点的概率，在此基础上从这一概率分布中抽取一个隐层节点激活向量的样本列向量$\\mathbf{h}$；</li>\n<li>计算$\\mathbf{v}$和$\\mathbf{h}$的外积$\\mathbf{v} \\mathbf{h} ^ T$，结果记为“正梯度”；</li>\n<li>利用$\\mathbf{h}$重构可见层节点的激活向量样本$\\mathbf{v&#39;}$，输入$\\mathbf{v&#39;}$再次获得一个隐层节点的激活向量样本$\\mathbf{h&#39;}$ ；</li>\n<li>计算$\\mathbf{v&#39;}$和$\\mathbf{h&#39;}$的外积$\\mathbf{v&#39;} \\mathbf{h&#39;} ^ T$，结果记为“负梯度”；</li>\n<li>使用正梯度和负梯度的差值，以学习率$\\epsilon$更新权重系数，即$\\mathbf{W} = \\mathbf{W} + \\epsilon (\\mathbf{v} \\mathbf{h} ^ T - \\mathbf{v&#39;} \\mathbf{h&#39;} ^ T)$；</li>\n<li>以学习率$\\epsilon$更新可见层的偏置系数$\\mathbf{a}$和隐藏层的偏置系数$\\mathbf{b}$，即$\\mathbf{a} = \\mathbf{a} + \\epsilon (\\mathbf{v} - \\mathbf{v&#39;}), \\mathbf{b} = \\mathbf{b} + \\epsilon (\\mathbf{h} - \\mathbf{h&#39;})$}。</li>\n</ol>\n<p><strong>对比散度的训练过程本质上是求出一个最符合训练数据集统计特性的概率分布，也就是使训练数据集出现的概率最大的分布参数</strong>。在数据的前向传输中，受限玻尔兹曼机的目标是在给定权重系数$w$的条件下利用输入$x$预测输出$a$，也就是求解条件概率$p(a | x; w)$；而在使用输出$a$重构输入$x$的反向传输中，受限玻尔兹曼机的目标变成了求解条件概率$p(x | a; w)$。将两个条件概率结合，就可以得到输入输出的联合概率分布$p(x, a)$。</p>\n<p><strong>将几个受限玻尔兹曼机堆叠在一起，就可以得到<span class=\"orange\">深度信念网络</span></strong>（deep belief network）。除了最顶层和最底层外，深度信念网络的每个隐藏层都扮演着双重角色：它既作为之前神经元的隐藏层，也作为之后神经元的可见层。在之前自编码器的介绍中我曾提到，栈式自编码器的训练遵循的是无监督预训练结合有监督微调的策略，深度信念网络采用的同样是这套训练方式，两者的区别只是基本单元的不同。</p>\n<p><strong>深度信念网络的无监督预训练也是逐层实现的</strong>。对于构成深度信念网络的第一个受限玻尔兹曼机来说，它的可见层就是深度网络的输入层。利用输入样本$x = h ^ {(0)}$训练这个玻尔兹曼机，得到的结果就是条件概率$p(h ^ {(1)} | h ^ {(0)})$，其中$h ^ {(1)}$是第一个玻尔兹曼机的隐藏层，也就是深度信念网络的第一个隐藏层的输出。第一个玻尔兹曼机的隐藏层又是第二个玻尔兹曼机的可见层，因此$h ^ {(1)}$就可以作为输入样本来训练得到深度网络第二个隐藏层的输出$h ^ {(2)}$。不断重复以上步骤，就可以完成对所有玻尔兹曼机，或者说所有隐藏层的逐层预训练。</p>\n<p><strong>栈式自编码器使用softmax分类器实现有监督微调，深度信念网络采用的方法则是在最顶层的受限玻尔兹曼机上又添加了额外的反向传播层</strong>。反向传播层以受限玻尔兹曼机的输出作为它的输入，执行有监督的训练，再将训练误差自顶向下地传播到每一个受限玻尔兹曼机当中，以实现对整个网络的微调。这也是为什么深度信念网络要在最顶上的两层进行无方向连接的原因。在实际的使用中，用来做微调的网络无需被局限在反向传播上，大部分用于分类的判别模型都能够胜任这个任务。</p>\n<p><strong>其实相比于深度信念网络这个具体的模型，辛顿的贡献更大程度上在于对深度模型训练方法的改进。不夸张地说，正是这套训练策略引领了近十年深度学习的复兴。这种复兴不仅体现在训练效率的提升上，更体现在研究者对训练机制的关注上。</strong></p>\n<p><strong>传统的反向传播方法应用于深度结构在原则上是可行的，可实际操作中却无法解决<span class=\"orange\">梯度弥散</span>（gradient vanishing）的问题</strong>。所谓梯度弥散指的是当误差反向传播时，传播的距离越远，梯度值就变得越小，参数更新的也就越慢。这会导致在输出层附近，隐藏层的参数已经收敛；而在输入层附近，隐藏层的参数几乎没有变化，还是随机选择的初始值。在这种情况下，对网络整体的优化也就无从谈起了。</p>\n<p>相比之下，基于预训练的训练方法就不会受梯度弥散的困扰。在梯度下降之前先执行无监督预训练能够在所有隐藏层上一视同仁实现良好的优化效果，并给深度结构带来强大的表达能力。这使得无监督预训练一度成为训练深度结构的不二法门。</p>\n<p>但随着研究的不断深入，事实表明无监督预训练并没有人们想象地那么神奇。良好的初始化策略完全可以比逐层预训练更加高效，而梯度弥散的根源并不是反向传播算法的问题，而是在于非线性传递函数非理想的性质。<strong>虽然目前深度信念网络的应用远不如卷积神经网络等其他模型广泛，但它却是一次吃螃蟹的成功尝试。如果没有这次尝试，也许我们依然在单个隐藏层的神经网络中兜兜转转，坐井观天</strong>。</p>\n<p>今天我和你分享了深度信念网络的基本概念和基本原理，其要点如下：</p>\n<ul>\n<li>深度信念网络是一种生成模型，能够建立输入和输出的联合概率分布；</li>\n<li>受限玻尔兹曼机是构成深度信念网络的基本单元，是由可见层和隐藏层构成的神经网络；</li>\n<li>受限玻尔兹曼机的训练方法是对比散度法，通过可见层和隐藏层的多轮交互实现；</li>\n<li>深度神经网络的通用训练方式是无监督逐层预训练和有监督微调的结合。</li>\n</ul>\n<p>在前面介绍的自编码器中，稀疏性既可以降低运算量，也能提升训练效果，发挥着重要的作用。那么同样的原理能否应用在受限玻尔兹曼机和深度信念网络之中呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/6e/45/6ee015991274b820f056695c8b5f9e45.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"（课外辅导）深度学习 | 拓展阅读参考书","id":3223},"right":{"article_title":"29 深度学习框架下的神经网络 | 见微知著：卷积神经网络","id":3638}}},{"article_id":3638,"article_title":"29 深度学习框架下的神经网络 | 见微知著：卷积神经网络","article_content":"<p>2017年9月13日，苹果公司推出了新一代智能手机iPhone X。相比于它的前辈们，iPhone X的一项重要卖点就是引入了Face ID人脸识别技术，用户直接刷脸就可以解锁手机。虽然目前看来，Face ID的识别率远没有苹果声称的那么“高精度”，但更加简单便捷的人脸识别无疑是未来的发展方向。而人脸识别乃至图像识别中的一项关键技术，就是<strong><span class=\"orange\">卷积神经网络</span></strong>。</p>\n<p>诞生于1989年的卷积神经网络已近而立之年，但它的首秀直到9岁才姗姗来迟。1998年，今日的深度学习扛鼎者之一燕乐存提出了第一个卷积神经网络模型LeNet-5，用来识别手写文本。遗憾的是，这个小朋友因为胃口太大（消耗计算资源多），并不招人喜欢。直到2006年，辛顿提出的逐层初始化训练算法才让韬光养晦的卷积神经网络一鸣惊人，这个少年也渐渐成长为神经网络和深度学习队伍中的中坚力量。</p>\n<p>顾名思义，<strong>卷积神经网络（convolutional neural network）指的是至少在某一层中用卷积运算（convolution）来代替矩阵乘法的神经网络</strong>。卷积运算的特性决定了神经网络适用于处理具有网格状结构的数据。最典型的网格型数据就是数字图像，不管是灰度图像还是彩色图像，都是定义在二维像素网格上的一组标量或向量。因而卷积神经网络自诞生以来，便广泛地应用于图像与文本识别之中，并逐渐扩展到自然语音处理等其他领域。</p>\n<p>要介绍卷积神经网络，首先要从卷积运算说起。<strong><span class=\"orange\">卷积</span></strong>是对两个函数进行的一种数学运算，在不同的学科中有不同的解释方式。在卷积网络中，两个参与运算的函数分别叫做输入和核函数（kernel function）。<strong>本质上讲，卷积就是以核函数作为权重系数，对输入进行加权求和的过程</strong>。为了突出这个本质，卷积神经网络中针对二维函数的卷积运算在原始的数学定义上做了一些调整，可以写成以下形式</p>\n<p>$$ Y(i, j) = (X * H)(i, j) = $$</p>\n<p>$$\\sum\\limits_m \\sum\\limits_n X(i + m, j + n)H(m, n) $$</p>\n<p>用生活中的实例类比，卷积就可以看成是做菜，输入函数是原料，核函数则是菜谱。对于同一个输入函数鲤鱼来说，如果核函数中酱油的权重较大，输出的就是红烧鱼；如果核函数中糖和醋的权重较大，输出的就是杭帮菜的西湖醋鱼；如果核函数中辣椒的权重较大，输出的就是朝鲜族风味的辣鱼。不同的菜谱对应不同的口味，不同的核函数也对应不同的输出。</p>\n<p>之所以将卷积运算应用于图像识别当中，是因为它具有一些优良的性质。<strong>卷积神经网络的稀疏感知性、参数共享性和平移不变性都有助于将它应用在图像处理之中</strong>。</p>\n<!-- [[[read_end]]] -->\n<p><strong><span class=\"orange\">稀疏感知性</span>（sparse interaction）指的是卷积层核函数的大小通常远远小于图像的大小</strong>。输入图像可能在两个维度上都有几千个像素，但核函数最大也不会超过几十个像素。选择较小的核函数一方面有助于发现图像中细微的局部特征，另一方面也可以提升算法的存储效率和运行效率。核函数选取背后的原理在于对图像的全局感知可以通过将多个局部感知综合得到，这其实也符合人类的认知方式。</p>\n<p><strong><span class=\"orange\">参数共享性</span>（parameter sharing）指的则是在一个模型中使用相同的参数</strong>，说白了就是在每一轮训练中用单个的核函数去和图像的所有分块来做卷积，这无疑能够显著降低核函数参数的数目。在卷积神经网络中，参数共享的基础是对图像特征的提取与图像的位置无关。如果在图像的一个区域上，某些像素的组合构成一条直线，那么在图像的其他区域，具有相同灰度的像素组合仍然是直线，而不会变成一个圆。这说明图像的统计特性并不取决于空间位置，因而对于整个图像都可以使用同样的学习特征。</p>\n<p><strong><span class=\"orange\">平移不变性</span>（translational equivalence）指的是当卷积的输入产生平移时，其输出等于原始输出做出相同数量的平移，这说明平移操作和核函数的作用是可以交换的</strong>。从卷积的线性特性出发很容易推导出平移不变性。平移不变性其实可以看成是离散时间域上的线性移不变系统在二维空间上的扩展，它在只关心某些特征是否出现，而不考虑出现的位置时具有重要的作用。</p>\n<p>卷积神经网络的结构并非卷积运算的简单组合，而是包含几个功能不同的层次。<strong>当输入图像被送入卷积神经网络后，先后要循环通过卷积层、激活层和池化层，最后从全连接层输出分类结果</strong>。每个层次各司其职，各负其责，都发挥着不可替代的作用。</p>\n<p>卷积层无疑是卷积神经网络的核心部分，其参数是一个或者多个随机初始化的核函数。核函数就像探照灯一样，逐行逐列地扫描输入图像，对像素矩阵进行从左到右，从上到下的滑动覆盖。每一个被核函数的光圈覆盖的区域都是和核函数维度相同的像素组合，并且作为输入和核函数进行卷积。当核函数将输入图像全部扫描完毕后，计算出的所有卷积结果又可以构成一个矩阵，这个新矩阵就是<strong>特征映射</strong>（feature map）。卷积层得到的特征映射一般会送到激活层处理，给系统添加非线性元素。激活层首选的传递函数是整流线性单元，它可以激活特征映射中的负值。</p>\n<p>为什么简单的卷积运算能完成图像的分类任务呢？解释这个问题还要回归到卷积的运算上。细心的你一定发现了，<strong>虽然卷积的表达式具有二维的形式，可如果把二维的输入和核函数拉成一维向量的话，卷积计算的实际上就是两者的内积</strong>！内积的作用是描述两个向量的关系，因而卷积的结果反映的正是输入像素和核函数之间的近似程度。卷积的输出越大表明两者之间的相似性越高，输出越小就意味着两者没什么共性。</p>\n<p>正因如此，<strong>通过合理设置核函数的性质，卷积层就能够提取出图像的特征</strong>。如果选取的核函数表示一个直角，原始图像中的直角就会体现为特征映射中一个较大的数值，根据这个数的坐标就可以确定曲线在输入图像中的位置。所以在卷积神经网络的实际应用中，通常会同时训练多个不同的核函数，以提取输入图像中不同类型的特征。</p>\n<p>卷积神经网络的卷积层之间通常周期性地会插入<strong><span class=\"orange\">池化层</span></strong>（pooling layer）。池化层更恰当的名字是下采样层（downsampling layer），它的作用是对得到的特征映射矩阵进行筛选。卷积层给出了核函数和原始图像每个局部之间的近似关系，但这里面真正对图像分析有帮助的只是取值较大，也就是和核函数相似程度较高的部分。因而<strong>常见的最大池化（max pooling）的做法就是将特征映射划分为若干个矩形区域，挑选每个区域中的最大值，也就是最明显的特征作为下采样的结果</strong>。这样做在显著降低数据量的同时也能减少过拟合的风险。</p>\n<p>直观来看，池化机制之所以能够发挥作用，其原因在于特征在图像中的绝对位置远不及它和其他特征的相对位置的关系来的重要。例如在判定一张图像中是否包含人脸时，我们需要在图像中找到左右对应地两只眼睛，但不需要确定这两只眼睛的精确位置。</p>\n<p><strong>池化机制的应用也可以看成是参数共享的体现：在一个图像区域有用的特征极有可能在另一个区域同样适用</strong>。因而对不同位置的特征进行聚合统计就是提取图像主要特征的有效方法。此外，池化操作还给图像带来了旋转不变性，因为无论图像如何旋转，每个区域的最大值都不会改变，因而池化并不会给图像结构造成影响。</p>\n<p>卷积层和池化层的循环使用能够实现对图像特征的逐层提取，而根据提取出的特征得到图像的分类与标记则要交给全连接层完成。由于全连接层中的神经元与前一层中的所有激活神经元都有连接，因此它们的激活与否可以通过矩阵乘法计算，这和常规的神经网络别无二致。全连接层可以使用softmax分类器得到原始图像属于不同类别的概率，对应的损失函数通常选择交叉熵。</p>\n<p>将前面介绍的卷积神经网络结构加以总结，就可以得到它的工作流程：输入层将待处理的图像转化为一个或者多个像素矩阵，卷积层利用一个或多个卷积核从像素矩阵中提取特征，得到的特征映射经过非线性函数处理后被送入池化层，由池化层执行降维操作。卷积层和池化层的交替使用可以使卷积神经网络提取出不同层次上的图像特征。最后得到的特征作为全连接层的输入，由全连接层的分类器输出分类结果。</p>\n<p>在卷积神经网络的训练里，待训练的参数是卷积核，也就是卷积层中的权重系数矩阵。训练采用的也是反向传播的方法，参数的不断更新能够提升图像特征提取的精度。</p>\n<p>最近两年，关于卷积神经网络的一项重要进展是<strong><span class=\"orange\">残差网络</span></strong>的提出。将深度结构应用于卷积神经网络当中可以增强表达能力，在图像分类和目标检测等问题上表现出优异的性能。可是当网络的层数超过特定的阈值时，训练误差也会随着层数的增加而增加，网络的性能不仅不能提升，反而会出现显著的退化。残差网络正是通过残差结构单元解决了深度网络性能下降的问题，使网络层数可以达到千层以上。</p>\n<p>今天我和你分享了卷积神经网络的原理与机制，受篇幅所限，诸如数据维度的变化和图像边界的策略设计等具体的技术细节并未涉及。其要点如下：</p>\n<ul>\n<li>卷积神经网络是应用了卷积运算的神经网络，适用于处理网格化数据；</li>\n<li>卷积神经网络具有稀疏感知性、参数共享性和平移不变性；</li>\n<li>卷积神经网络的结构包括交替出现的卷积层、激活层和池化层，以及作为输出的全连接层；</li>\n<li>卷积神经网络的作用是逐层提取输入对象的特征。</li>\n</ul>\n<p>在卷积神经网络中，很多参数都会对性能产生影响。那么在设计卷积层和池化层时，需要考虑哪些具体的因素呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/4d/e7/4dce2701152a658ff621948a3ed26ce7.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"28 深度学习框架下的神经网络 | 枯木逢春：深度信念网络","id":3431},"right":{"article_title":"30 深度学习框架下的神经网络 | 昨日重现：循环神经网络","id":3639}}},{"article_id":3639,"article_title":"30 深度学习框架下的神经网络 | 昨日重现：循环神经网络","article_content":"<p>今天是除夕，明天就是春节啦，在这里，给你拜个早年，祝你狗年大吉，新春快乐，在新的一年里，福旺运旺！</p>\n<p>今天我们继续讨论深度学习框架下的神经网络，聊一聊<strong><span class=\"orange\">循环神经网络</span></strong>。</p>\n<p>2017年，一本名叫《阳光失了玻璃窗》的诗集出版了。这本来是再普通不过的事情，可诗集的作者是赫赫有名的网红机器人微软小冰，或者更确切地说，小冰背后的算法，就让事情变得没那么简单了。</p>\n<p>我在网上拜读了一些小冰的诗，实话实说，它们让我想起了几年前那些将简单的旋律和节奏随机排列组合而批量生产出来，至今仍在广场舞音乐中大行其道的网络歌曲。但小冰的诗显然技高一筹，循环神经网络和递归神经网络这些高大上的技术让它的排列组合更加难以捉摸。</p>\n<p>在深度学习中，<span class=\"orange\">RNN</span>这个缩写有两层含义，它既可以表示<span class=\"orange\">循环神经网络</span>（Recurrent Neural Network），也可以表示<span class=\"orange\">递归神经网络</span>（Recursive Neural Network）。巧的是，这两个RNN之间的关系还很密切：循环神经网络可以看成是递归神经网络的特例，递归神经网络则可以视为循环神经网络的推广。</p>\n<p><strong>循环神经网络和我们前面介绍的所有神经网络都不一样，它的独特之处在于引入了“时间”的维度，因而适用于处理时间序列类型的数据</strong>。回忆一下上次分享的卷积神经网络，它具有空间上的参数共享的特性，也就是同样的核函数可以应用在图像的不同区域之上。如果把参数共享调整到时间的维度上，让神经网络使用相同的权重系数来处理具有先后顺序的数据，得到的就是循环神经网络。</p>\n<p>从结构上看，使用神经网络处理可变长度的输入时，在时间上共享参数是非常有必要的。定义在空间上的数据不会无穷无尽地延伸，即使大如《清明上河图》也有确定的边界存在。在很多图像识别的任务中，输入图像的像素数目甚至是有特定要求的。但对于一个以时间为自变量的变长数据来说，很难说清楚数据的终点在哪里，抑或这个终点根本就不存在。</p>\n<p>这种情况之下，如果对每一个时间点上的数据都计算一次神经网络的权重系数，无疑会带来极大的计算负荷。循环神经网络就是将长度不定的输入分割为等长度的小块，再使用相同的权重系数进行处理，从而实现对变长输入的计算与处理。</p>\n<!-- [[[read_end]]] -->\n<p><strong>从功能上看，时间维度上的参数共享可以充分利用数据之间的时域关联性</strong>。在生活中你一定有这样的经验：前后文的语境能够对信息进行有效的补充。在大大小小的英语考试中都少不了一类叫做“完形填空”的题目，这类题目的要求就是根据上下文的语义选择用于填空的合适的词语。比方说妈妈在厨房里突然喊我：“菜炒好了，赶紧来......”，即使后面的话没有听清楚，也能猜到十有八九是让我赶紧吃饭，而不是洗衣服或者其他什么事情。这利用的就是数据的<strong>时域关联</strong>。</p>\n<p>循环神经网络对时域的关联性的利用体现在时刻$t$的输出既取决于当前时刻的输入，也取决于网络在前一时刻$t - 1$甚至于更早的输出。从这个意义上讲，<strong>循环神经网络引入了反馈机制，因而具有记忆的功能</strong>。正是记忆功能使循环神经网络能够提取来自序列自身的信息，这是传统的前馈神经网络所无法做到的。</p>\n<p>关于循环神经网络的记忆特性，可以做出进一步的解释：<strong>其实前馈网络在某种程度上同样具有记忆，只要神经网络的参数经过最优化，优化的参数中就会包含着过往数据的踪迹</strong>。但最优化的记忆只局限于训练数据集上。当训练出的模型应用到新的测试数据集上时，其参数并不会根据测试数据的表现做出进一步的调整，因而前馈神经网络的记忆其实是“冻结”的记忆。</p>\n<p>相比之下，在循环神经网络中，记忆的作用更加宽泛。它的作用不是给每种类型的特征分配固定的权重，而是描述一系列时序事件之间的关系，即使这些事件之间可能没有明显而紧密的时间关联，但它们之间的相关性依然可能如草蛇灰线般伏延千里，而这正是循环网络的记忆要挖掘的对象。</p>\n<p>两种网络代表了两种不同的知识类型。</p>\n<p>前馈网络适用于表示客观性的知识。当我们学会分辨颜色之后，这项技能就会伴随我们一生，不会随时间或环境的变化而变化，就像一块红布不会因为放了几十年或者是拿到另一个城市而变成蓝色。</p>\n<p>循环网络则适用于表示主观性的知识。每种语言中都有同音不同义的词汇，那么在听到一个&quot;jing&quot;的音节时，就要根据前后的其他音节来判断它到底是干净的“净”还是安静的“静”。很多主观性知识正隐藏在数据的顺序之中，而这种顺序恰恰可以由循环神经网络刻画。</p>\n<p>具体来说，输入序列的内部信息存储在循环神经网络的隐藏层中，并随着时间的推移在隐藏层中流转。循环神经网络的记忆特性可以用以下公式表示</p>\n<p>$$ \\mathbf{h}_t = f(\\mathbf{W} \\mathbf{x}_t + \\mathbf{U} \\mathbf{h}_{t - 1} ) $$ </p>\n<p>这个式子的含义在于将时刻$t$的输入$\\mathbf{x}_t$的加权结果和时刻$t - 1$的隐藏层状态$\\mathbf{h}_{t - 1}$的加权结果共同作为传递函数的输入，得到的就是隐藏层在时刻$t$的输出$\\mathbf{h}_t$。$\\mathbf{W}$表示了从输入到状态的权重矩阵，$\\mathbf{U}$则表示了从状态到状态的转移矩阵。直观地看起来，$\\mathbf{h}_t$只取决于$\\mathbf{h}_{t - 1}$，但由于$\\mathbf{h}_{t - 1}$又取决于$\\mathbf{h}_{t - 2}$，$\\mathbf{h}_{t - 2}$又取决于$\\mathbf{h}_{t - 3}$，因而$\\mathbf{h}_t$和之前所有时刻的隐藏层状态都是有关系的。</p>\n<p>对循环神经网络的训练就是根据输出结果和真实结果之间的误差不断调整参数$\\mathbf{W}$和$\\mathbf{U}$，直到达到预设要求的过程，其目的是实现对输入序列的精确划分。<strong>其训练方法也是基于梯度的反向传播算法，但和其他前馈网络不同的是，这里的反向传播是通过时间进行的</strong>（backpropagation through time）。</p>\n<p>由于循环神经网络的每个状态都与之前的所有状态相关，因而在基于时间的反向传播中，对当前时刻的参数求偏导一定会涉及前一时刻的参数。这其实和原始的反向传播算法毫无区别，只不过在链式法则中添加了一组关于时间的中间变量。</p>\n<p>在普通的循环神经网络中，记忆只会涉及到过去的状态。如果想让循环神经网络利用来自未来的信息，就要让当前的状态和以后时刻的状态同样建立起联系，得到的就是<strong><span class=\"orange\">双向循环神经网络</span></strong>（bidirectional recurrent neural network）。</p>\n<p>双向循环网络包括正向计算和反向计算两个环节，在正向计算中，时刻$t$的隐藏层状态$\\mathbf{h}_t$与过去的$\\mathbf{h}_{t - 1}$相关；而在反向计算中，时刻$t$的隐藏层状态$\\mathbf{h}_t$与未来的$\\mathbf{h}_{t + 1}$相关。由于正向计算和反向计算的权重系数是不共享的，因而双向循环网络需要分别计算正向和反向的结果，并将两者的组合作为隐藏层的最终参数。</p>\n<p>将深度结构引入双向循环神经网络就可以得到深度循环网络，它和其他深度结构一样，具有多个隐藏层。每个隐藏层的状态$\\mathbf{h}_t^i$既取决于同一时刻前一隐藏层的状态$\\mathbf{h}_t^{i - 1}$，也取决于同一隐藏层在前一时刻的状态$\\mathbf{h}_{t - 1}^i$。</p>\n<p>深度结构的作用在于建立更清晰的表示，这也可以用完形填空的例子来做类比，有些填空只需要根据它所在的句子便可以推断出来，这对应着单个隐藏层在时间维度上的依赖性；有些填空则可能要通读整段甚至全文才能确定它合适的意义，这就对应着深度结构在时间维度和空间维度上共有的依赖性。</p>\n<p>循环神经网络的特点是在时间维度上共享参数，从而展开处理序列。如果换一种展开方式，将序列数据展开成树状结构，用到的就是<strong><span class=\"orange\">递归神经网络</span></strong>。递归神经网络在处理数据使用的也是相同的权重系数，但这里的相同并非体现在时间上，而是体现在结构上。递归网络首先将输入数据转化为某种拓扑结构，再在相同的结构上递归使用相同的权重系数，通过遍历方式得到结构化的预测。</p>\n<p><strong>在自然语言处理中，递归神经网络可以解决时间序列无法解决的问题</strong>。比方说，“两个大学的老师”是个有歧义的句子，如果单纯地将它拆分为词序列是无法消解歧义性的，只有用语法解析树来表示才能表示出定语“两个”到底描述的是“大学”还是“老师”。</p>\n<p>将数据用树状结构表示后，递归神经网络的作用是将它们进一步表示成向量，映射到表示语义的向量空间之中。在语义空间上既可以度量单个向量的尺度，比如判定句子的感情色彩到底是褒义还是贬义；也可以度量不同向量之间的关系，比如确定两个句子意义上的相似程度。</p>\n<p>递归神经网络通过树状结构将一个完整的句子打散为若干分量的组合，生成的向量就是树结构的根节点。在训练中，递归网络使用的也是误差反向传播的方法，误差从树结构的根节点反向传播到每个叶子节点。但在实际应用中，递归神经网络远不如循环神经网络受欢迎，这是因为它的训练需要高度的人工干预，将训练集的每个句子标注为语法解析树的形式不仅需要人工完成，而且费时费力。从性价比的角度看，递归网络远不如循环网络来得实在。</p>\n<p>今天我和你分享了循环神经网络和递归神经网络的基本原理与简要的工作机制。其要点如下：</p>\n<ul>\n<li>循环神经网络是具有记忆的神经网络，适用于处理序列化数据；</li>\n<li>循环神经网络引入反馈结构，能够在时间上共享参数，从而具有记忆；</li>\n<li>循环神经网络的扩展包括双向循环网络和深度循环网络；</li>\n<li>递归神经网络能够处理具有层次化结构的数据，可以看成循环神经网络的推广。</li>\n</ul>\n<p>循环神经网络的记忆机制与人类的记忆机制颇为相似。那么人类记忆还有哪些特点可以借鉴到神经网络的设计当中呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/3b/05/3b35d656105e4d355b968f7f292d9a05.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"29 深度学习框架下的神经网络 | 见微知著：卷积神经网络","id":3638},"right":{"article_title":"31 深度学习框架下的神经网络 | 左右互搏：生成式对抗网络","id":3643}}},{"article_id":3643,"article_title":"31 深度学习框架下的神经网络 | 左右互搏：生成式对抗网络","article_content":"<p>2016年10月，谷歌大脑的研究者们做了一个有趣的实验：使用三个并不复杂的神经网络执行保密通信的任务，两个合法通信方共享的只有保密的密钥，而没有商定的加解密算法，第三个神经网络则作为窃听者出现。</p>\n<p>这种配置显然颠覆了密码学的常识：无论是公钥体制还是私钥体制，加解密的算法都是已知的。如果合法通信双方不能统一加解密的方法，实现保密通信就是“巧妇难为无米之炊”。可谷歌偏偏不信这个邪，他们就是要让神经网络实现双盲的加密与解密。</p>\n<p>实验的结果同样令人惊讶：经过不断的试错与调整，接收方可以精确恢复出发送方的明文，而窃听者的错误率稳定在50%左右，意味着她的破译只是随机的猜测。<strong>这个实验的意义在于展示出神经网络的潜能</strong>：它们不仅能够在欠缺先验规则的条件下，通过对大量数据的无监督学习完成目标，甚至还能够在学习过程中根据实际条件的变化对完成目标的方式进行动态调整。</p>\n<p>这个实验的环境，就是在最近两年名声大噪的<strong><span class=\"orange\">生成式对抗网络</span></strong>。生成式对抗网络（generative adversarial network）由《深度学习》的第一作者伊安·古德菲洛提出，<strong>这是一类在无监督学习中使用的人工智能算法，由两个在零和游戏框架下相互竞争的神经网络实现</strong>。“零和游戏”（zero-sum game）这个术语来自于博弈论，意思是博弈双方的利益之和为零。由于一方的收益必然意味着另一方的损失，因而双方不可能实现合作，属于非合作博弈。</p>\n<p><strong>生成式对抗网络里的两个玩家一个叫<span class=\"orange\">生成器</span>（generator），一个叫<span class=\"orange\">判别器</span>（discriminator），均可以采用深度神经网络实现，这两者之间的对抗就是网络训练的主旋律</strong>。生成器像是白骨精，想方设法从随机噪声中模拟真实数据样本的潜在分布，以生成以假乱真的数据样本；判别器则是孙悟空，凭一双火眼金睛来判断输入到底是人畜无害的真实数据还是生成器假扮的伪装者。零和博弈中的竞争促使双方不断进化，直到“假作真时真亦假”，真真假假不可区分为止。</p>\n<p>两个玩家费这么大劲对抗的目的是什么呢？就是建立数据的生成模型，使生成器尽可能精确估测出数据样本的分布。从学习方式上看，对抗性学习固然属于无监督学习，但对抗的引入使学习可以利用来自判别器的反馈信息，因而又蕴含着某些监督学习的影子。</p>\n<!-- [[[read_end]]] -->\n<p>由于生成器和判别器处于零和博弈之中，因而对网络的训练就可以等效成对以下目标函数的<strong>极大-极小问题</strong></p>\n<p>$$ \\arg \\min_g \\max_D { -\\dfrac{1}{2} \\int_x [p_{data}(x) \\log (D(x)) + p_g(x) \\log (1 - D(x))] {\\rm d}x } $$</p>\n<p>其中“极大”是让判别器区分真实数据和伪造数据的准确率最大化，“极小”则是让生成器生成的数据被判别器发现的概率最小化。对整体极大-极小问题的优化可以通过<strong>交替迭代训练</strong>的方式实现。</p>\n<p>交替迭代训练通常从判别器开始，也就是在给定生成器的条件下来求解最优的判别器。由于生成式对抗网络使用的是基于对数几率函数的二分类判别器，因而使用交叉熵作为损失函数是合理的选择。</p>\n<p>由于判别器要将来自真实分布的真样本标注为1，因而对数几率函数的输出需要越大越好；反过来，对来自生成器的假样本要标注为0，此时的输出就越小越好，也就是输出的相反数越大越好。这样一来，对判别器的优化就转化为求解以下目标函数的最小值</p>\n<p>$$ f(x) = -\\dfrac{1}{2} \\int_x [p_{data}(x) \\log (D(x)) + p_g(x) \\log (1 - D(x))] {\\rm d}x $$</p>\n<p>式中的$p_{data}(x)$表示数据的真实分布，$p_g(x)$表示生成器的数据分布，$D(x)$则表示判别器对数据$x$的概率输出。在给定生成器的条件下可以求出，使以上函数取得最小值的最优解是</p>\n<p>$$D^*_G(x) = \\dfrac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$</p>\n<p>这表明生成式对抗网络估计的实际是两个概率分布密度的比值。</p>\n<p>优化完判别器，就该轮到生成器了。对生成器的优化意味着希望判别器对假样本的输出越大越好，因而需要优化$p_g(x)$，以使前文目标函数中的第二项最小。当且仅当$p_{data}(x) = p_g(x)$时，整个网络的目标函数可以取得全局最优解。</p>\n<p>这表明在算法收敛时，生成器学到的分布和数据的真实分布完全一致，而判别器对每个样本的输出都等于0.5。在生成式对抗网络的实际训练时，一般采用先更新多次判别器的参数，再对生成器的参数执行一次更新的方法。</p>\n<p>既然都是学习数据的分布，那生成式对抗网络和其他生成模型又有什么区别呢？</p>\n<p>首先，传统的生成模型是定义了模型的分布，进而去求解参数。比如说在已知数据满足正态分布的前提下，生成模型会通过最大似然估计等方法根据样本来求解正态的均值和方差。可要是生成人脸呢？没人知道人脸满足什么样的先验分布，只能通过不断尝试来逐渐逼近，这时传统的生成模型就无能为力了。<strong>生成式对抗网络好就好在摆脱了对模型分布的依赖，也不限制生成的维度，因而大大拓宽了生成数据样本的范围</strong>。</p>\n<p>其次，<strong>生成式对抗网络能够整合不同的损失函数，增加了设计的自由度</strong>。生成式对抗网络是没有显式的损失函数的，之所以这么说是因为它训练的目标是生成器，判别器只是训练过程中的副产品。对于生成器来说，因为判别器被用来度量生成分布和真实分布之间的偏差，所以判别器其实就是它的损失函数。而作为损失函数的判别器又会随着真实分布的变化而变化。从这个角度看，生成式对抗网络可以自动学习潜在的损失函数，这是传统的生成模型没法做到的。</p>\n<p><strong>除了优点之外，生成式对抗网络也有它的问题。最主要的一个问题就是缺乏理论基础</strong>。</p>\n<p>回到文首那个密码学的例子：我们只是知道了合法通信方能够达成关于密码算法的共识，但这个共识的达成过程还是个黑箱。关于生成器为什么能够从随机样本出发学习到真实的数据分布也缺乏清晰的理论解释。凡此种种都让生成式对抗网络看起来更像是沙上之塔。没有坚实的理论基础，对算法的推广自然存在困难。除了在图像生成等少数领域表现突出，生成式对抗网络在大多数任务上还是乏善可陈。在算法的原理尚不清楚时，想要实现优化自然是空中楼阁。</p>\n<p><strong>生成式对抗网络面临的另一个主要问题就是训练的难度</strong>。对抗网络的训练目标是在连续分布的高维度参数下达到纳什均衡，也就是让生成器和判别器的损失函数同时取得最小值。但由于待优化的问题可能是个非凸的问题，直接追求纳什均衡可能会让算法难以收敛，从而引发模型的欠拟合，导致表示能力不足。生成式对抗网络的提出者古德菲洛针对训练难的问题也提出了一系列改进措施，并应用在了半监督学习问题上，取得了不错的效果。</p>\n<p><strong>虽然优缺点都很明显，但生成式对抗网络的提出依然可以看成是深度学习的一次突破</strong>。给定一只猫的图片，过往的神经网络算法只能区分出它到底是不是猫，还不一定分得准确。可生成式神经网络却能模仿现有的图片画出一只类似的猫。不管这是简单的数据拟合，还是更加高级的抽象特征重组，它都是由机器自己完成的再创作，这种行为方式无疑更加接近于真实的人类。</p>\n<p>关于生成式对抗网络还有一个有趣的事实。自2014年诞生以来，各种各样的对抗网络变体层出不穷，其中有名有姓的就超过了200种，给这些变体命名让拉丁字母都不够用了。可这些改进到底有多少效果呢？谷歌公司近期的一项研究表明：没有证据表明哪种变体能够带来实质上的改进。换句话说，改来改去的结果是王小二过年，一年不如一年。</p>\n<p>出现这种问题的原因就在于理论基础的缺失。没有理论基础就没有明确的改进方向，因而只能像没头苍蝇一样，从应用问题出发盲目地摸索优化技巧。运气好的话，通过优化架构或是损失函数可以在特定任务上获得性能的提升，但提升表现的适用范围往往狭窄，换一个场合就不好用了。这其实不只是生成式对抗网络，更是整个深度学习所深陷的“炼金术”尴尬处境的体现。</p>\n<p>今天我和你分享了生成式对抗网络的原理与机制。其要点如下：</p>\n<ul>\n<li>生成式对抗网络是一类运行在零和博弈框架下的无监督学习算法，由生成器和判别器构成；</li>\n<li>生成器的目的是精确模拟真实数据的分布，判别器的目的是精确区分真实数据和生成数据；</li>\n<li>生成式对抗网络的主要优点是超越了传统神经网络分类和特征提取的功能，能够按照真实数据的特点生成新的数据；</li>\n<li>生成式对抗网络的主要问题是理论基础的缺失。</li>\n</ul>\n<p>生成式对抗网络的一个重要的潜在应用就是让人工智能在没有明确指导的情况下学习，使算法的学习方式向人类的学习方式转变。那么如何看待生成式对抗网络在通用人工智能研究中的前景呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/3f/7f/3f505cb5fd0b5f18eece1522718a707f.jpg\" alt=\"\" /></p>\n","neighbors":{"left":{"article_title":"30 深度学习框架下的神经网络 | 昨日重现：循环神经网络","id":3639},"right":{"article_title":"32 深度学习框架下的神经网络 | 三重门：长短期记忆网络","id":3644}}},{"article_id":3644,"article_title":"32 深度学习框架下的神经网络 | 三重门：长短期记忆网络","article_content":"<p>在之前的专栏中，我和你分享了循环神经网络的原理，而今天要介绍的<strong><span class=\"orange\">长短期记忆网络</span></strong>就是一类特殊的循环神经网络。这个词的断句方式是“长-短期记忆网络”，表达的含义是<strong>一类可以持续很长时间的短期记忆模型</strong>。对时隙长度的不敏感性是这种模型的优势，因而它适用于序列中信息之间的时滞不确定的情况。</p>\n<p>循环神经网络通过在时间上共享参数引入了记忆特性，从而将先前的信息应用在当前的任务上，可这种记忆通常只有有限的深度。有追剧经历的都会知道，国外的电视剧通常是每周更新一集，可即使经历了一周的空窗期，我们依然能将前一集的内容和新一集的情节无缝衔接起来。但循环神经网络的记忆就没有这么强的延续性，别说是一个星期的断片儿，插播一段五分钟广告就足以让它的记忆脱节，造成理解上的混乱。</p>\n<p>真实世界中的信息不是静止的，而是不断经历着流转与跃变，如果神经网络不能保存长期记忆的话，它处理信息的能力就会大打折扣。长短期记忆网络（long short-term memory）的作用就是实现长期记忆，更准确地说，是实现任意长度的记忆。精巧的设计使记住长期的信息成为了长短期记忆网络的默认行为，而不是需要付出很大代价才能获得的能力。</p>\n<p><strong>从机制上讲，要实现长期记忆，神经网络既要学会记忆，也要学会遗忘</strong>。长期记忆的基础是足够的存储，但宝贵的存储不能被滥用，它不是收集桶，有用的没用的都一股脑儿往里面扔。<strong>长期记忆要求模型具备对信息价值的判断能力，结合自身的状态确定哪些信息应该保留，而哪些信息应该舍弃</strong>。比方说电视剧里的一段支线情节结束了，模型就应当重置相关的信息，只需保留对应的结果。同理，当收到新的输入信息时，模型也要判断这些信息是否有用，以及是否需要保存。</p>\n<p>除了添加遗忘机制之外，<strong>长短期记忆单元还要能够将长期记忆聚焦成工作记忆，也就是哪一部分记忆需要立刻使用</strong>。有用的信息也不会每时每刻都有用，因而记忆单元并不会始终使用所有的长期记忆，而是根据当前的相关性做出取舍，这就类似于人类注意力的工作方式。遗忘和选择使长短期记忆网络能够对记忆做出更细粒度的处理，它不同于循环神经网络一视同仁的方式，因而可以实现对信息进行长期而精确的跟踪。</p>\n<p>长短期记忆网络是由相应的基本单元构成的。长短期记忆的基本单元的作用在需要时取出并聚焦记忆，通常包括<strong>四个功能不同的隐藏层：<span class=\"orange\">记忆模块</span>（memory cell）、<span class=\"orange\">输入门</span>（input gate）、<span class=\"orange\">输出门</span>（output gate）和<span class=\"orange\">遗忘门</span>（forget gate）</strong>，这比只有一个激活函数的一般循环神经网络要复杂得多。</p>\n<!-- [[[read_end]]] -->\n<p>记忆模块的作用时存储数值或是状态，存储的时限既可以是长期也可以是短期。另外的“三重门”则用于控制信息的有选择通过，三者都使用对数几率函数作为传递函数。</p>\n<p>在这“三重门”中，输入门决定哪些新信息被存放在记忆模块中，遗忘门决定哪些信息被从记忆模块中丢弃，输出门则决定记忆模块中的哪些信息被用于计算整个长短期记忆单元的输出。值得一提的是，长短期记忆网络的最初版本只有输入门和输出门，遗忘门是作为一项改进添加的。</p>\n<p>下面来看看长短期记忆单元的工作流程：根据遗忘机制，记忆模块要根据时刻$t$的输入来更新现有的记忆，这个过程首先由遗忘门来完成。如果网络处理的对象是这样一句话：“李雷XXX，韩梅梅XXX”，那么当“韩梅梅”出现时，遗忘门就能够察觉到主语的变化，从而降低“李雷”在记忆单元中的权重。在很多种语言中，主语性别的改变也意味着动词词形的变化。</p>\n<p>当然，记忆单元的更新不一定意味着完全的替换，对新输入的部分信息和原始存储中的部分信息加以整合也是可以的。遗忘门的输入包括这个长短期记忆单元在时刻$t - 1$的输出$y(t - 1)$和时刻$t$的输入$x(t)$，两者的加权组合再送进对数几率函数计算输出，其表达式可以写成</p>\n<p>$$ \\mathbf{f}(t) = \\sigma (\\mathbf{W}_f \\mathbf{x}(t) + \\mathbf{R}_f \\mathbf{y}(t - 1) + \\mathbf{b}_f) $$ </p>\n<p>其中$\\sigma{\\cdot}$表示对数几率函数。如果遗忘门的输出为0，意味着记忆单元的当前存储要被全部舍弃，输出为1则意味着全部保留。</p>\n<p>在决定哪些来自输入的信息进入到记忆模块中时，就轮到输入门发挥作用了。<strong>遗忘门的作用是弃旧，输入门的作用则是图新</strong>，将新来的“韩梅梅”添加到记忆模块之中。输入门的工作机制与遗忘门类似，但是更加复杂，它首先用对数几率函数对即时输入和上一时刻的输出的组合进行过滤，过滤的作用一方面在于确定哪些信息被保留，另一方面则在于确定这些信息以何种比例被添加到记忆单元之中。将待保留的结果与权重系数相乘，就得到了输入门的输出。过滤结果和权重的表达式分别为 </p>\n<p>$$ \\mathbf{i}(t) = \\sigma (\\mathbf{W}_i \\mathbf{x}(t) + \\mathbf{R}_i \\mathbf{y}(t - 1) + \\mathbf{b}_i) $$</p>\n<p>$$ \\tilde{\\mathbf C}(t) = \\tanh (\\mathbf{W}_z \\mathbf{x}(t) + \\mathbf{R}_z \\mathbf{y}(t - 1) + \\mathbf{b}_z)$$</p>\n<p>遗忘门和输入门的工作完成后，记忆模块的状态就是“万事俱备，只欠更新”。更新操作是舍弃旧信息和添加新信息的组合，其表达式可以写成</p>\n<p>$$ \\mathbf{C}(t) =\\mathbf{f}(t) \\odot \\mathbf{C}(t - 1) + \\mathbf{i}(t) \\odot \\tilde{\\mathbf C}(t)$$ </p>\n<p>式中的$\\odot$代表外积计算。</p>\n<p>更新了记忆模块的状态后，就要从当前的单元状态中选择有用的信息输出，这部分工作由输出门完成。由于主语已经由李雷变成了韩梅梅，那么谓语出现“化妆”地可能性就远大于出现“打球”的可能性。输出门同样利用对数几率函数对即时输入和上一时刻的输出的组合进行过滤，过滤的目的生成一组权重系数，其整体的表达式可以写成</p>\n<p>$$ \\mathbf{o}(t) = \\sigma (\\mathbf{W}_o \\mathbf{x}(t) + \\mathbf{R}_o \\mathbf{y}(t - 1) + \\mathbf{b}_o)$$</p>\n<p><strong>输出门输出权重系数的作用是对记忆模块的状态进行加权。但加权对象不是记忆状态本身，而是记忆状态的双曲正切函数结果</strong>。因而长短期记忆单元在时刻$t$的输出就可以表示为</p>\n<p>$$ y(t) = \\tanh(\\mathbf C(t)) \\odot \\mathbf{o}(t) $$ </p>\n<p>这一输出又将作为记忆单元在$t + 1$时刻的输入出现。</p>\n<p>前文介绍的是长短期记忆网络的基本结构，一种改进的方法是加入所谓的“<strong>门镜连接</strong>（peephole connection）”。设计门镜连接的出发点是语义信息的载体不仅包括具体的文字，也包括文字之间的时序。即使在通信高度发达的今天，某些民族依然保持着用具有明显节奏和模式的鼓声来传递消息的古老传统。这种思想有它的现实意义，即事件之间的时间差，也就是通常所说的“节奏”，也可以作为模式而被识别。</p>\n<p>与其临渊羡鱼，不如退而结网，门镜连接体现的就是这一古老的哲理。门镜连接的作用是让长短期记忆单元中的三重门都能接受来自记忆模块的输入，这就意味着每个门都能观察到模块当前的状态，并将状态信息应用到更新之中。这一改进的作用在于提升长短期记忆网络对时间的识别精度。</p>\n<p>和原始的循环神经网络相比，长短期记忆网络解决了梯度弥散的问题，梯度弥散这种现象可以用复利计算做类比，即使一个赌徒每轮只损失1%的赌本，一座金山也会很快输个精光。根据求导的链式法则，循环神经网络的层次和时间之间是通过连续的乘法运算关联起来的，正是这大量的乘法运算使梯度以指数方式下降，以至于小到无法用于网络学习。而长短期记忆网络通过门隐藏层的使用强制性地将误差转化为加法运算，从而避免了梯度快速消失的问题。</p>\n<p><strong>目前，长短期记忆网络最著名的应用恐怕非谷歌翻译莫属</strong>。谷歌公司于2016年发表的论文中提到，谷歌的神经机器翻译系统（Google Neural Machine Translation）就是由带有 8 个编码器和 8 个解码器的深度长短期记忆网络组成，还使用了额外的注意力机制和残差连接。相比于原来使用的基于短语的系统，新系统的翻译误差平均降低了60%，这是非常明显的提升。</p>\n<p>今天我和你分享了长短期记忆网络的基本原理与简单工作机制。其要点如下：</p>\n<ul>\n<li>长短期记忆网络可以实现任意长度的记忆，对信息进行长期而精确的跟踪；</li>\n<li>长短期记忆单元的组成包括记忆模块、输入门、遗忘门和输出门；</li>\n<li>长短期记忆网络根据当前的输入、当前的记忆和前一时刻的输出确定当前的输出；</li>\n<li>长短期记忆网络能够解决梯度弥散的问题。</li>\n</ul>\n<p>长短期记忆网络的作用不仅在于做些阅读理解，它可以让人工智能理解事物之间的长序联系。那么长短期记忆网络会不会在训练机器的推理能力上带来突破呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/2e/14/2e463cd67177ecafb547c36d65524a14.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"31 深度学习框架下的神经网络 | 左右互搏：生成式对抗网络","id":3643},"right":{"article_title":"33 深度学习之外的人工智能 | 一图胜千言：概率图模型","id":3646}}},{"article_id":3646,"article_title":"33 深度学习之外的人工智能 | 一图胜千言：概率图模型","article_content":"<p>在前面两个模块中，我和你分享了神经网络和深度学习的知识。神经网络是理解深度表征的模型，深度学习是训练深度神经网络的算法，两者是一脉相承的关系。<strong>本质上讲，神经网络和深度学习都是由数据驱动的，大量有标记的训练样本是复杂模型取得良好性能的前提，这也解释了为什么直到近年来深度学习才得以蓬勃发展</strong>。</p>\n<p>但深度学习远非实现人工智能的唯一途径，在接下来的四篇文章中，就让我和你聊一聊深度学习之外的人工智能。</p>\n<p>早年间的人工智能赖以实现的基础是<strong>逻辑学</strong>，但逻辑学适用的是理想化的，不存在任何不确定性的世界。可遗憾的是，真实世界是由随机性和误差主宰的，在这光怪陆离的环境中，纯粹的数理逻辑就如同古板的老夫子一般与周遭格格不入。</p>\n<p>可即使抛开噪声与干扰不论，很多问题也没有固定的解。在医学上，即使子女的基因和母亲的基因已经确定，父亲的基因也可以有多种可能。要解决这类不确定性推理的问题，就必须借助<strong>概率论</strong>的方法。而将概率方法与图论结合起来，得到的就是今天的主题：<strong><span class=\"orange\">概率图模型</span></strong>。</p>\n<p><strong>概率图模型（probabilistic graphical model）也叫结构化概率模型，是用图论表现随机变量之间的条件依赖关系的建模方法。典型的概率图模型包括<span class=\"orange\">贝叶斯网络</span>和<span class=\"orange\">马尔可夫随机场</span>，分别对应着有向图模型和无向图模型。</strong></p>\n<!-- [[[read_end]]] -->\n<p>贝叶斯网络（Bayesian network）的拓扑结构是<strong>有向无环图</strong>，“有向”指的是连接不同顶点的边是有方向的，起点和终点不能调换；“无环”指的是从任意顶点出发都无法经过若干条边回到该点，在图中找不到任何环路。</p>\n<p>贝叶斯网络中的顶点表示随机变量，一个顶点既可以表示可观察的显式变量，也可以表示未知参数和隐变量。如果不同的变量之间存在因果关系，那么相应的顶点就会由带箭头的边连接起来，箭头的方向由表示原因的变量指向表示结果的变量，边的权重就是对应的条件概率值。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/9c/6d/9c48b643e9484274470622da7231ab6d.png\" alt=\"\">\n<span class=\"reference\">图片引自《深度学习》图3.7</span></p>\n<p><strong>贝叶斯网络的作用是表示出随机变量之间的条件依赖关系，将多个随机变量的联合概率分布分解为条件概率因子乘积的形式</strong>。</p>\n<p>根据条件概率的链式法则，四个随机变量$A, B, C, D$的联合概率可以写成</p>\n<p>$$ p(A, B, C, D, E) = p(A) \\cdot p(B | A) $$</p>\n<p>$$\\cdot p(C | B, A) \\cdot p(D | C, B, A) \\cdot p(E | D, C, B, A) $$</p>\n<p>那么根据上面的概率图，联合概率就可以简化为</p>\n<p>$$ p(A, B, C, D, E) = p(A) \\cdot p(B | A)$$</p>\n<p> $$\\cdot p(C | B, A) \\cdot p(D | B) \\cdot p(E | C) $$</p>\n<p>不知道这个例子是否让你想起另一种机器学习的方法：<strong>朴素贝叶斯分类</strong>。朴素贝叶斯分类的基础假设是不同的属性之间条件独立，因此类条件概率可以表示成属性条件概率的乘积。但在绝大多数情形下，这个假设是不成立的。将属性之间的依赖关系纳入后，得到的就是通用的<strong>贝叶斯网络</strong>。</p>\n<p>贝叶斯网络引出的一个重要概念是<strong>条件独立性</strong>（conditionally independence）。如果事件$a$和$b$在另一事件$c$发生时条件独立，就意味着在$c$发生时，$a$发生与否与$b$发生与否相互无关，两者联合的条件概率等于单独条件概率的乘积，其数学表达式为</p>\n<p>$$p(a, b | c) = p(a | c) \\cdot p(b | c)$$</p>\n<p>条件独立性可以这样理解，假设天降大雪，那么一对夫妻各自从工作单位回家的话，他们到家的时间都会因下雪而延长，但两个人各自耽误的时间在下雪的条件下是独立的；可如果老公要去接老婆一起回家的话，条件独立性就不再满足了。</p>\n<p>条件独立性是概率论视角下的概念，如果从图论的角度看，变量之间的依赖与独立对应的是顶点之间的连通与分隔。直观理解，两个直接相连的变量肯定不会条件独立。在不直接相连的两个变量之间，信息必须通过其他变量传递，如果信息传递的通道全部被堵塞，那么两个变量之间就能够满足条件独立性。假设两个变量$X$和$Y$之间通过第三个变量$Z$连接，那么连接的方式可以分为以下三种（注意箭头方向的区别）：</p>\n<ul>\n<li><strong>顺连</strong>：$X \\rightarrow Z \\rightarrow Y$或$X \\leftarrow Z \\leftarrow Y$</li>\n<li><strong>分连</strong>：$X \\leftarrow Z \\rightarrow Y$</li>\n<li><strong>汇连</strong>：$X \\rightarrow Z \\leftarrow Y$</li>\n</ul>\n<p>在顺连的情况下，当变量$Z$已知时，$X$和$Y$条件独立。这是因为确定的中间变量不能通过变化传递信息，$Z$的固定取值不会随$X$的变化而变化，所以$X$的变化自然也不会通过$Z$影响到$Y$。这里的变量$Z$就像一扇固定的水闸，隔开了$X$和$Y$之间的流动，也就堵塞了两者之间的信息通道。</p>\n<p>分连的情况与顺连类似，$Z$扮演的同样是固定水闸的角色。分连代表了一因多果的情况，当这个原因确定后，所有的结果之间都不会相互产生影响，因而$Z$已知时，$X$和$Y$条件独立。</p>\n<p>反过来，在多因一果的汇连情形中，当$Z$未知时，来自$X$和$Y$的信息都会从这个窟窿里漏掉，此时$X$和$Y$条件独立。反过来，$Z$已知意味着将这个窟窿堵上，$X$和$Y$之间就构造出信息流动的通路，也就不满足条件独立的条件。需要注意的是，这里的$X, Y, Z$既可以是单个的顶点，也可以是顶点的集合。</p>\n<p>条件独立性又可以引出贝叶斯网络另一个良好的性质，通俗的说就是<strong>贝叶斯网络的每个顶点只取决于有箭头指向它的其他顶点，而与没有箭头指向它的其他顶点条件独立</strong>。这个性质简化了贝叶斯网络中联合概率分布的计算，也可以节省大量的运算和存储。</p>\n<p>当研究者要对真实世界建模时，最理想的方案是建立起包罗万象的联合分布，这就要求计算出分布中所有随机变量的所有可能取值的概率，并将它们存储起来。假定一个模型中包含8个二进制的随机变量，需要存储的概率值数目就会达到$2 ^ {8} = 256$个。</p>\n<p>可如果在这8个变量中，每个变量最多只取决于其他两个变量，那么贝叶斯网络的概率表需要存储的只是相关顶点之间所有可能的状态组合，最多只要计算$8 \\times 2 ^ 2 = 32$个值，这不仅降低了运算复杂度，也节省了存储空间。尤其是在实际问题中，涉及的变量数目可能成千上万时，只要每个顶点都只有少量的父节点，贝叶斯网络在运算上的优势就体现得尤为明显。</p>\n<p>贝叶斯网络的另一个优点是<strong>能够根据各变量之间的条件依赖性，利用局部分布来求得所有随机变量的联合分布</strong>。这个特点让贝叶斯网络在解决复杂问题时不需要关于某个实例的完整知识，而是从小范围的特定知识出发，逐步向复杂问题推广。</p>\n<p>贝叶斯网络是基于不确定性的因果推断模型，其作用体现在在不完备的信息下，根据已观察的随机变量推断未观察的随机变量。在给定变量之间层次关系的前提下，如果按照自底向上的方式执行推断，就是由果溯因的诊断，这就像医生根据症状诊治病因（诊断过程其实就可以看成基于经验的贝叶斯推理过程）；如果按照自顶向下的方式执行推断，就是由因得果的推理，这使得贝叶斯网络可以用作数据的生成模型。</p>\n<p><strong>贝叶斯网络的构造包括两个步骤</strong>：首先要根据变量之间的依赖关系建立网络的拓扑结构，其次要根据拓扑结构计算每条边上的权重，也就是条件概率。网络结构的构建通常需要特定的领域知识，条件概率表的构造也就是贝叶斯网络的训练，在网络的规模较大时可以使用期望最大化等方法实现。</p>\n<p><strong>将贝叶斯网络的有向边替换为无向边，得到的就是马尔可夫随机场</strong>。<strong>马尔可夫随机场（Markov random field）属于无向图模型，它的每个顶点表示一个随机变量，每条边则表示随机变量之间的依赖关系</strong>。使用无向边使马尔可夫随机场不能表示出推理关系，但这也解除了对环路结构的限制。环路的存在使它可以表示循环依赖，而这又是贝叶斯网络无法做到的。和贝叶斯网络相比，<strong>马尔可夫随机场侧重于表示随机变量之间的相互作用</strong>。</p>\n<p>贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。而在马尔可夫随机场中，由于变量之间没有明确的因果关系，联合概率分布通常会表达为一系列势函数的乘积。这些乘积的求和并不一定等于1，因而必须进行归一化才能得到有效的概率分布。马尔可夫随机场中的联合概率分布可以写成</p>\n<p>$$p(\\mathbf{x}) = \\dfrac{1}{Z} \\mathop \\Pi \\limits_k f_k(x_{{ k }})$$</p>\n<p>式中的每个$x_{{ k }}$代表一个团，团是所有顶点的一个子集，并且团中所有的顶点都是全连接的；$f_k(\\cdot)$表示每个团对应的势函数，通常取对数线性模型；$Z$表示的则是用于归一化的配分函数。</p>\n<p>在马尔可夫随机场中，随机变量的独立性和顶点的连通性之间也有对应关系，但其形式却比贝叶斯网络更加简单。要判断两个变量是否独立，只需要观察对应的顶点之间有没有将它们连接在一起的边就可以了。只有在两个顶点之间不存在任意一条信息通路时，对应的随机变量才是条件独立的。条件独立通常意味着两者之间所有的顶点都有固定的观察值。</p>\n<p>无论是贝叶斯网络还是马尔可夫随机场，都属于结构化的概率模型。它们提供了将概率模型的结构可视化的简单方式，而对图形的观察可以加深对模型性质的认识，其中最主要的性质就是变量之间的条件独立性。此外，概率图模型还可以表示学习和推断过程中的复杂计算，隐式地承载了图形背后的数学表达，这在以下解决问题的三步框架得以体现。</p>\n<ul>\n<li><strong>表示</strong>：如何对不确定性和随机变量之间的关系进行建模</li>\n<li><strong>推断</strong>：如何从建立的模型中推演出要求解的概率</li>\n<li><strong>学习</strong>：对于给定的数据来说，何种模型是正确的</li>\n</ul>\n<p>受篇幅所限，关于三步框架的内容就不做进一步的展开了。</p>\n<p>今天我和你分享了概率图模型的原理，包括有向的贝叶斯网络和无向的马尔可夫随机场。其要点如下：</p>\n<ul>\n<li>概率图模型是概率论与图论的结合，是用图论表现随机变量之间的条件依赖关系的建模方法；</li>\n<li>贝叶斯网络是有向无环图，侧重于表示随机变量之间的依赖关系；</li>\n<li>马尔可夫随机场是无向图，侧重于表示随机变量之间的相互作用；</li>\n<li>概率图模型体现了“表示-推断-学习”的问题解决框架。</li>\n</ul>\n<p>作为两种不同的模型，贝叶斯网络和马尔可夫随机场各有优势。那么它们能不能结合起来，各自避短扬长，共同发挥作用呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/57/48/57eda304a18b35999beaadbfc1c32348.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"32 深度学习框架下的神经网络 | 三重门：长短期记忆网络","id":3644},"right":{"article_title":"34 深度学习之外的人工智能 | 乌合之众的逆袭：集群智能","id":3649}}},{"article_id":3649,"article_title":"34 深度学习之外的人工智能 | 乌合之众的逆袭：集群智能","article_content":"<p>梅拉妮·米切尔在《复杂》中举过一个例子：在巴西的亚马逊雨林中，几十万只行军蚁（已知的行为最简单的生物）正在行进。用现在时髦的话说，这是一支去中心化、自组织的大军。在这个蚁蚁平等的团体中，单个蚂蚁几乎没有视力，也不具备什么智能。可聚集成团体的他们组成了扇形的团状，一路风卷残云地吃掉所有能吃掉的，带走所有能带走的。高效的它们只需一天就能消灭雨林里一个足球场面积内的所有食物。到了夜间，蚂蚁会自发形成一个球体，保护起蚁后和幼蚁，天亮后又各就各位，继续行军。</p>\n<p>快速飞行的蝙蝠群在狭窄的洞穴中互不碰撞，大雁群在飞行时自发地排列成人字形、海洋鱼群通过几何构型充分利用水流的能量......这些自然界中的集群行为早早就吸引了人类的注意。在由大量数目的生物个体构成的群体中，不同个体之间的局部行为并非互不相关，而是互相作用和影响，进而作为整体化的协调有序的行为产生对外界环境的响应。生物群体正是通过个体行为之间的互动达到“整体大于部分之和”的有利效果，就像一百只行军蚁只会横冲直撞，一百万只行军蚁却能整齐划一。</p>\n<p>实现集群智能的智能体必须能够在环境中表现出自主性、反应性、学习性和自适应性等一系列智能特性，但这并不意味着群体中的个体都很复杂。<strong>集群智能的核心是由众多简单个体组成的群体能够通过相互之间的简单合作来实现某些功能，完成某些任务</strong>。其中，“简单个体”是指单个个体只具有简单的能力或智能，“简单合作”是指个体与其邻近的个体只能进行某种简单的直接通信或通过改变环境间接与其它个体通信，从而可以相互影响、协同动作。</p>\n<p>与高大上的深度学习不同，集群智能既不需要汪洋浩瀚的物理数据，也不需要艰深晦涩的数学算法，难道蚂蚁和大雁会计算微积分吗？集群智能的基础只是作用于个体的运行准则和作用于整体的通用目标，这些目标通常还都很简单。可正是数量足够庞大的简单规则才孕育出了整体意义上的高级智能，这也验证了量变引发质变的哲学观点。</p>\n<p>从抽象的角度来说，群体行为是大量自驱动个体的集体运动，每个自驱动个体都遵守一定的行为准则，当它们按照这些准则相互作用时就会表现出上述的复杂行为。群体本身不具备中心化的结构，而是通过个体之间的识别与协同达成<strong>稳定的分布式结构</strong>。这个分布式结构会随着环境的变化，以自身为参考系不断趋于新的稳定。<strong><span class=\"orange\">集群智能</span></strong>（swarm intelligence）正是群居性生物通过协作表现出的自组织与分布式的宏观智能行为，它具有如下的特点：</p>\n<!-- [[[read_end]]] -->\n<p><strong>第一个特点是可扩展性</strong>。广义的可扩展性意味着系统可以在增加体量的同时保持功能的稳定，而不需要重新定义其部件交互的方式。集群智能系统中的作用方式通常是个体间的间接通信，因而互动的个体数量往往不会随着群体中个体的总数量的增加而增长，每个个体的行为只受群体维度的松散影响。在人造的集群智能算法中，可扩展性保证了通过简单扩充来增强算法性能的可行性，而不需要重新编程。</p>\n<p><strong>第二个特点是并行性</strong>。集群智能系统天然具有并行性，这是因为组成集群的个体完全可以同时在不同的地方执行不同的行为。在人造的集群智能算法中，并行性能够增强系统的灵活性，能够在同时处理复杂任务不同方面的集群中自行组织。同时，并行性也决定了集群智能具有较强的环境适应能力和较强的鲁棒性（Robust），不会由于若干个体出现故障而影响群体对整个问题的求解。</p>\n<p><strong>第三个特点是容错性</strong>。去中心化和自组织的特点让容错性成为集群智能系统的固有属性。由于集群系统由许多可以互换的个体组成，他们中没有一个总司令负责控制整个系统的行为，所以一个失败的个体很容易被另一个完全正常运作的个体所取代。除此之外，去中心化的特性也大大降低了系统全面崩溃的可能性。</p>\n<p><strong>用集群智能方法实现人工智能，代表的是研究方式的转变</strong>。不管是人工神经网络还是深度学习，其本质都是对人类思维的<strong>功能模拟</strong>，采用的方法论是机械论的哲学，而机械论中先验规则设定的前提决定了功能模拟的方式很难产生人类水平的智能。既然机械论这条路走不通，研究者们便开始转向另一条可能的道路：<strong>从结构模拟出发，通过人为创造类似人类脑神经系统的结构模型，实现智能的大规模涌现</strong>。</p>\n<p>功能模拟的任务是重新制造出类人脑的复杂系统，结构模拟的任务则是复制出人脑这个复杂系统。一般来说，复制的难度要低于制造的难度。二进制的计算机可以通过算法代替人类执行特定的任务，但算法的核心问题也恰恰在于只能执行特定的任务，而不具备通用性，想要生成人类思维的特质，比如创造力和想象力，更是绝无可能。</p>\n<p>结构模拟的理论基础在于对复杂系统的认识。当构成一个系统的基本单元数量极为庞大时，将这些个体看作一个整体，就会有一些全新的属性、规律或模式自发地冒出来，这种现象就称为“<strong><span class=\"orange\">涌现</span></strong>”（emergence）。</p>\n<p>对于涌现现象来说，大量微观层次上的交互导致对其功能性的描述过于困难，结构化的模拟反倒成为一种直观有效的方式。对涌现现象的代表性应用，就是以蚁群算法为代表的各类<strong><span class=\"orange\">粒子群算法</span></strong>。</p>\n<p>在蚁群系统中，单只蚂蚁便是构成这一复杂系统的基本单元。在单只蚂蚁活动范围受限的情况下，整个蚁群中不同蚂蚁之间的协作与联动，却能够很快找到一条将食物搬回巢穴的最优路径。更重要的是，这一行为完全是自组织的行为，并不存在一个上帝视角的蚁后如元帅般指挥蚂蚁们的行动轨迹，这条最优线路完全是自发找到的。</p>\n<p>蚁群的行为给善于向自然学习的人类以新的启发：只对单只蚂蚁的简单行为进行建模，系统中不同蚂蚁间复杂的大规模互动过程交给蚂蚁自己完成，这样能否产生“涌现”的效果呢？</p>\n<p>基于这样的想法，意大利米兰理工学院的马可·多里戈博士在他的博士论文中提出了<strong><span class=\"orange\">蚁群优化算法</span></strong>（ant colony optimization）：通过对蚁群在寻找食物过程中发现最优路径行为的模拟来寻找问题的最优解。</p>\n<p>蚁群算法的初始化是让第一批蚂蚁随机出发，独立且并行地搜索问题的解，对解的搜索就是蚂蚁对路径的选择。在行进的过程中，蚂蚁会在自己的路径上释放信息素，信息素的强度是与解的最优程度成正比的，新的蚂蚁则会根据已有信息素的强度选择自己的行进路径。</p>\n<p>随着搜索的不断进行，信息素也会发生动态变化：旧蚂蚁留下的信息素会不断蒸发，新蚂蚁经过后则会产生新的信息素。蚁群的集群化搜索会产生聚集作用，最优路径会被越来越多的蚂蚁发现，信息素的强度也会逐渐增大，从而将更多蚂蚁吸引过来，被下一只蚂蚁选择的概率也就进一步增加。<strong>蚁群算法就是利用这种正反馈机制逐步遍历解空间，使搜索向最优解推进</strong>。</p>\n<p>在不断的实践中，蚁群算法不仅被证明是个相当靠谱的优化算法，还激发出了鸟群、蜂群等其他类似原理的最优化算法。当然，蚁群算法也有它的限制条件：只有当基本单元的数量足够巨大时，“涌现”的结果才会出现。当蚂蚁数目太少的时候是达不到“涌现”效果的。</p>\n<p><strong>蚁群的行为与蚁群算法的诞生对人工智能的研究也是一种启示：相对于宏观的功能模拟，结构模拟重于微观的解构</strong>。</p>\n<p>功能模拟是个自顶向下的过程：先确定要完成的任务，再来设计任务实现的方式。结构模拟则是个自底向上的过程：只对最底层的基本单元及其运作方式进行定义，并让这些基本单元在微观层面上进行交互，从而自发地生成各种宏观行为。</p>\n<p>在这个过程中，结构模拟回避了复杂系统中复杂性形成的过程，以放任自流的方式让微观单元自己完成。这未尝不是个好办法：当现有的技术水平远远不足以透彻地研究复杂系统时，与其通过功能模拟人为地对其运作方式进行武断地判断，不如采用无为而治的方式，发挥基本单元的自然天性，看看它能造出个什么世界。</p>\n<p>今天我和你分享了集群智能方法的现实依据，以及它所代表的研究方向。其要点如下：</p>\n<ul>\n<li>集群智能是由众多简单个体通过自组织和去中心化的简单合作实现的智能；</li>\n<li>集群智能具有可扩展性、并行性和容错性等特点；</li>\n<li>集群智能体现出微观个体之间的相互作用能够实现整体大于部分之和的效果，其实例是蚁群算法；</li>\n<li>集群智能在人工智能中的应用代表的是从宏观模仿到微观解构的方向转变。</li>\n</ul>\n<p>虽说神经网络模拟的是思维功能，但从结构模拟的角度看，深度神经网络其实也可以归入集群智能的范畴。那么关于集群智能的研究又能给神经网络带来哪些启发呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e7/51/e7151984e06f3ee537179af1cb7a1d51.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"33 深度学习之外的人工智能 | 一图胜千言：概率图模型","id":3646},"right":{"article_title":"35 深度学习之外的人工智能 | 授人以鱼不如授人以渔：迁移学习","id":4009}}},{"article_id":4009,"article_title":"35 深度学习之外的人工智能 | 授人以鱼不如授人以渔：迁移学习","article_content":"<p>无论是小学还是大学，在教学中都会强调的一个问题就是“举一反三”，将学到的规律灵活地运用到其他相似的场景下。而要想让人工智能学会举一反三，用到的就是<strong>迁移学习技术</strong>。</p>\n<p><strong><span class=\"orange\">迁移学习</span>（transfer learning）是运用已学习的知识来求解不同但相关领域问题的新的机器学习方法，目的是让机器“学会学习”</strong>。当训练数据和测试数据来自不同的特征空间或遵循不同的概率分布时，如果能够将从训练数据上习得的知识迁移到测试数据上，就可以回避掉复杂的数据标签工作，进而提升学习性能。迁移学习就是解决这个问题的学习框架，它能够对仅有少量甚至没有标签样本进行学习，从而解决目标问题。</p>\n<p>许多机器学习和数据挖掘算法都建立在两个主要假设之上：第一，训练样本和测试数据必须处于相同的特征空间并具有相同的分布；第二，有足够的高质量训练样本作为学习资源。</p>\n<p>遗憾的是，这两个假设在大多数实际应用中难以成立。一方面，训练数据和测试数据在时间上的差异可能导致分布规律的变化；另一方面，对大量数据进行标注不仅费时费力，还容易受到知识产权问题的影响。一旦没有数据，再好的深度学习方法都是无源之水，无本之木，难以发挥作用。</p>\n<p>迁移学习的出现给解决这些问题带来了一丝曙光。其实说到底，<strong>迁移学习可以看作是提升学习算法泛化性能的一种思路</strong>。现实世界并非标准化和结构化的数据集，而是杂乱无章的万花筒，包含着大量从未在训练集中出现过的全新场景，这足以让许多在训练集上无往不胜的人工智能变成真实世界中的“人工智障”。<strong>迁移学习有助于算法处理全新场景下的问题，利用一般化的规律去应对富于变化的环境</strong>。</p>\n<p>在迁移学习中，已有的知识（包括样本数据集及其分布）叫做<strong>源域</strong>，要学习的新知识叫做<strong>目标域</strong>。同样，待解决的任务也可以分为<strong>源任务</strong>和<strong>目标任务</strong>。根据源域/目标域和源任务/目标任务的关系，迁移学习问题可以划分为以下三类：</p>\n<!-- [[[read_end]]] -->\n<ul>\n<li><strong><span class=\"orange\">归纳迁移学习</span></strong>（inductive transfer learning）：源域与目标域可能相同也可能不同，目标任务与源任务不同；</li>\n<li><strong><span class=\"orange\">直推式迁移学习</span></strong>（transductive transfer learning）：源域与目标域不同，目标任务与源任务相同；</li>\n<li><strong><span class=\"orange\">无监督迁移学习</span></strong>（unsupervised transfer learning）：目标任务与源任务不同，且源域数据和目标域数据都没有标签。</li>\n</ul>\n<p><strong>在从源域到目标域的迁移过程中，迁移学习要解决的三个问题是：迁移什么、能不能迁移、如何迁移</strong>。</p>\n<p>“迁移什么”明确的是迁移学习的作用对象。显然，迁移的对象应该是普适性的知识，而不是特定领域的知识。用来背古诗词的技巧不一定适用于背化学方程式，这时需要迁移的就是整体意义上的记忆方法。</p>\n<p>在确定了迁移什么之后，接下来就要解决能不能迁移的问题，只有当源域和目标域具有较高的相关性时，迁移才会有助于学习。如果不管三七二十一，把知识生搬硬套到风马牛不相及的问题上，反而会起到负作用。这就像骑自行车的技巧可以用在骑摩托车上，因为它们都有两个轮子，可如果用骑自行车的方法骑三轮车的话，只怕是要翻车了。</p>\n<p>解决了上面两个问题之后，面对的就是迁移学习中的核心问题——<strong>如何迁移</strong>。迁移学习的具体方法包括以下四种：</p>\n<ul>\n<li><strong>基于样本的迁移学习</strong>（instance transfer）；</li>\n<li><strong>基于特征的迁移学习</strong>（feature representation transfer）；</li>\n<li><strong>基于模型的迁移学习</strong>（parameter transfer）；</li>\n<li><strong>基于关系的迁移学习</strong>（relational knowledge transfer）。</li>\n</ul>\n<p><strong>基于样本的迁移学习是对已有样本的重用过程，它通过调整源域中原始样本的权重系数，使之和目标域匹配，进而应用在目标域中</strong>。这种方法通过度量训练样本和测试样本之间的相似度来重新分配源域数据的采样权重，相似度越大的样本对目标任务的训练越有利，其权重也会得到强化，相似度小的样本权重则被削弱。</p>\n<p><strong>在归纳迁移学习中，TrAdaBoost是典型的基于样本的方法</strong>。它假定两个域中的数据具有不同的分布，但使用完全相同的特征和标签。由于源域和目标域之间的分布差异，不同的源域数据对目标任务的学习既可能有用也可能有害。TrAdaBoost通过多次迭代的方式调整源域数据的权重，以鼓励“好”数据和抑制“坏”数据。</p>\n<p>基于样本的方法也可以用于直推式迁移学习之中。这种情况下，学习的出发点是使经验风险最小化，使用的方法则是基于重要性采样的方法。重要性采样的作用在于将关于未知的目标域数据的期望转化为关于目标域数据和已知的源域数据之间关系的期望，对两组数据关系的估计则可以通过核均值匹配或者使KL散度最小化等方法完成。</p>\n<p><strong>基于特征的迁移学习是特征表示的重建过程，它通过特征变换使得源域数据与目标域数据处在同一个特征空间之上，再在这个公共空间上进行学习</strong>。这种方法适用于所有的迁移学习任务。特征迁移的基本思想是学习在相关任务中共享的一组低维特征表示，对特征的降维采用的也是特征映射和特征选择两种方式（降维学习的两种方式）。</p>\n<p>基于特征选择的迁移学习是识别出源领域与目标领域中共有的特征表示，再基于这些共有特征实现知识迁移。由于与样本类别高度相关的特征应该在模型中具备更高的权重，因此在找出源域和目标域的共同特征后，需要从目标域数据中选择特有的特征来对共同特征训练出的通用分类器进行精化，从而得到适合于目标域数据的分类器。</p>\n<p>基于特征映射的迁移学习是把每个领域的数据从原始高维特征空间映射到新的低维特征空间，使源域数据和目标域数据在新的低维空间上具有相同的分布，这样就可以利用低维空间表示的有标签的源域数据来训练目标域上的分类器。特征映射和特征选择的区别在于映射得到的特征不属于原始的特征的子集，而是全新的特征。</p>\n<p><strong>基于模型的迁移学习是已有模型的转移过程，它假设源任务和目标任务共享一些参数或者一些先验分布，将在训练数据上训练好的成熟模型应用到目标域上解决问题</strong>。应用在归纳迁移学习中的大多数模型方法都来自于多任务学习，但多任务学习试图同时提升源任务和目标任务的学习效果，而迁移学习只是利用源域数据来提高目标域的性能。在迁移学习中，通过给目标域的损失函数分配更大的权重，可以确保在目标域中获得更好的性能。</p>\n<p><strong>基于关系的迁移学习是问题结构的复制过程，如果源域和目标域之间共享了某种相似关系，那就可以将源域上的逻辑关系网络应用到目标域上</strong>。与其他三种方法不同，关系学习方法处理的是关系域中的迁移学习问题，其中的数据在概率上不需要满足独立同分布的条件，但是一定要能够用类似的关系来表示，最典型的实例就是物理网络的数据和社交网络的数据。在这种方法中，数理逻辑是常用的工具。</p>\n<p>除了以上的具体方法之外，迁移学习还需要解决一系列理论问题。既然迁移学习的目的是让机器学会学习，向人类学习的机制取经就是最简单的途径。人类学习的一个重要特点就是能够从纷繁复杂的现象中抽象出共性的问题，实现实例和规律的剥离。如果机器可以学会这种思维，把问题的内容和结构区分开来，其举一反三的能力就会产生质的飞跃。</p>\n<p>另一方面，从迁移学习很容易进一步得到“<strong>元学习</strong>”（meta-learning）的概念。元学习的目标是通过对元数据的自动化学习来提升学习算法的灵活性，是对学习方法的学习。每个特定的学习算法都建立在一系列有关数据的假设基础上，这叫作<strong>归纳偏差</strong>。只有当归纳偏差和学习问题相匹配时，算法才能获得良好的效果。这不仅给机器学习和数据挖掘算法添加了应用的约束，给它们的实用性造成了相当大的限制。</p>\n<p>正因如此，<strong>学习的灵活性对通用人工智能至关重要</strong>。通过使用学习问题的属性、算法属性、派生模式等不同类型的元数据，元学习可以对不同的学习算法进行学习，选择，更改或组合，以有效地解决给定的学习问题。其实遗传进化就是对基因编码的学习过程，这个过程在大脑中执行，这意味着我们每个人其实都是元学习的践行者。但是元学习到底能不能推广到机器学习之中，还是个未知数。</p>\n<p>今天我和你分享了迁移学习的基本原理和常用方法。其要点如下：</p>\n<ul>\n<li>迁移学习是运用已学习的知识来求解不同但相关领域问题的新的机器学习方法，目的是让机器“学会学习”；</li>\n<li>迁移学习适用于跨领域和小数据的学习任务；</li>\n<li>迁移学习的任务类型可以分为归纳迁移学习，直推式迁移学习和无监督迁移学习；</li>\n<li>迁移学习的学习方法包括基于样本、基于特征、基于模型和基于关系。</li>\n</ul>\n<p>2016年，人工智能的大咖吴恩达表示“继深度学习之后，迁移学习将引领下一波机器学习技术”。那么你如何看待迁移学习的前景呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/a3/bb/a331dead77d7e3e1d9f1939ed38534bb.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"34 深度学习之外的人工智能 | 乌合之众的逆袭：集群智能","id":3649},"right":{"article_title":"36 深度学习之外的人工智能 | 滴水藏海：知识图谱","id":4010}}},{"article_id":4010,"article_title":"36 深度学习之外的人工智能 | 滴水藏海：知识图谱","article_content":"<p>近期，关于“深度学习是不是炼金术”的争议愈演愈烈。这些争议产生的原因在于<strong>深度学习的黑箱特性</strong>：虽然深度学习算法能够将图片中的猫咪辨识出来，却无法详细地解释为什么会做出这样的判断，其判定方法是否具备普适性也无从知晓。</p>\n<p>因此，<strong>人工智能的一个重要研究方向就是开发具有更好的可解释性，更容易被人理解的人工智能模型</strong>。这样的模型将能够克服现有人工智能在概念认知和语言认知上的巨大障碍，它不但会辨认图片里有一只猫，还能指出它是依据猫特有的眼睛、耳朵和胡子这些特征做出的判断。</p>\n<p>要构造可解释的人工智能，靠大数据去训练复杂模型肯定是不靠谱的，还是要回归到逻辑推演的路径上，而<strong>知识图谱很可能成为可解释人工智能中的一项关键技术</strong>。</p>\n<p><strong><span class=\"orange\">知识图谱</span>（knowledge graph）是由大量的概念实体以及它们之间的关系共同构成的语义网络</strong>。某种程度上，它类似于今天备受推崇的思维导图，但是具有更加规范的层次结构和更加强大的知识表示能力。</p>\n<p>《福尔摩斯探案集》的作者亚瑟·柯南道尔有句名言：“一个逻辑学家，不需要亲眼见过或听过大西洋和尼亚加拉大瀑布，他从一滴水中就能推测出它们。”</p>\n<p>知识图谱也是如此，它采集星罗棋布的碎片化信息和数据，然后按标准化的思考方式加以整理，再将各个看似不相关但背后有着共同联系的信息关联起来，挖掘出背后的规律。</p>\n<p>为了构造知识的基本框架，知识图谱除了包含实体之外，一般还包括概念、属性、关系等一系列信息。人类的智能源于对知识内部表示的推理过程，这也是早期人工智能的大咖们热衷于符号主义的原因。因为人类的认知过程正是不断用概念、属性和关系去理解世界和解释世界的过程，而这些理解共同构成了人脑中高度组织化和结构化的知识图谱。</p>\n<!-- [[[read_end]]] -->\n<p>知识图谱中的概念、属性和关系可以用下面的一个例子来理解：当我们提到莱昂内尔·梅西的时候，即使不熟悉足球的人也很可能知道他是个足球运动员，这里的“足球运动员”就是概念；经常看球的人则会知道梅西来自阿根廷，效力在巴塞罗那俱乐部，这些都属于实体梅西的属性；更狂热的球迷还会知道内马尔是梅西的前队友，克里斯蒂亚诺·罗纳尔多是梅西的主要竞争对手，这就是实体梅西和其他实体之间的关系。</p>\n<p>同数理逻辑一样，知识图谱也可以用于知识的推理。知识推理最广泛的应用就是知识库问答，也就是理解自然语言中的问题，并从知识库中寻找答案。但正所谓“生也有涯，知也无涯”，即使最庞大的百科全书也不可能将所有知识尽收囊中，这时就需要知识图谱大显身手了。<strong>知识图谱可以根据已有实体的概念、属性和关系为新的概念自动生成属性；也可以明确不同新实体之间的关系</strong>。</p>\n<p><strong>具体说来，知识推理可以分为归纳和演绎两类，分别表示了“从特殊到一般”和“从一般到特殊”的过程</strong>。</p>\n<p>所谓归纳（induction）是指从某类事物的大量实例出发，推理出关于这类事物的一般性结论。如果在我认识的程序员朋友中，小张很聪明，老李很聪明，大刘也很聪明，那我就有理由相信，所有的程序员都很聪明。这就是归纳的过程。可以看出，<strong>归纳推理能够从旧知识中获取新知识，是知识的增殖过程</strong>。</p>\n<p>将归纳的过程调转方向，得到的就是演绎。演绎（deduction）指的是从一般性的前提出发，推导出个别结论或者具体陈述的过程。既然我已经归纳出“所有的程序员都很聪明”的结论，那么当遇到一个陌生的程序员小赵时，即使对他一无所知，我也可以通过演绎得出“小赵很聪明”这个符合一般性原则的具体陈述。</p>\n<p>如果你对数理逻辑的内容还有印象，那就不难发现，经典的三段论实际上就是一类演绎推理。虽然演绎推理可以用来解决复杂的问题，但它只是将已有事实揭示出来，而不能够产生新知识。</p>\n<p><strong>数理逻辑能够实现的推理建立在硬性约束的基础上，只能实现非黑即白的推理过程，相比之下，知识图谱则可以实现软性推理</strong>。</p>\n<p>在归纳推理中，软性推理的一个应用是<strong><span class=\"orange\">路径排序算法</span></strong>（path ranking algorithm）。</p>\n<p>在知识图谱中，实体是由二元关系相连接的，因而现实世界中的规则在知识图谱中就体现为不同实体之间的关系路径。路径排序算法正是以实体之间的路径为依据，在不完全的知识库中学习目标关系的分类器。</p>\n<p><strong>路径排序算法的实现包括特征抽取、特征计算和分类器训练三个步骤</strong>。特征抽取的任务是生成路径特征的集合并加以筛选，筛选出的特征被应用在训练数据集上，对每个训练样本都计算出其对应的特征取值，最后根据训练样本为每个目标关系训练出一个分类器。将分类器应用在知识图谱上，就可以自动挖掘并筛选出可靠的规则。</p>\n<p>软性推理也可以应用在演绎过程中，得到的就是<strong>马尔可夫逻辑网</strong>和<strong>概率软逻辑</strong>。</p>\n<p><span class=\"orange\">马尔可夫逻辑网</span>（Markov logic network）是将无向概率图模型和一阶谓词逻辑相结合得到的统计关系学习模型。这个网络是一阶逻辑公式的集合，其中的每个逻辑规则都被分配一个实数作为权重系数，以此实现规则的软化。规则的权重越大，意味着它的约束力越强，当权重为正无穷时，软性规则就退化为硬性规则。</p>\n<p>你可能知道小品里的包袱：“1加1在什么情况下等于3？在算错的情况下等于3！”这就是典型的以对错来区分的硬性规则。但在由软性规则构造出的马尔可夫逻辑网中，1加1等于3也是合法的，但这个合法规则只存在于另外的一个平行世界之中，这个世界和真实世界的差别很大，其存在的可能性很小，因而与它相关的规则成立的概率也会很低。</p>\n<p>利用马尔可夫逻辑网对知识图谱建模后，就可以利用已有的规则和权重系数来推断未知事实成立的概率。如果规则和权重系数部分未知或者全部未知时，则可以自动学习规则和权重，这也就是<strong>马尔可夫随机场的结构学习</strong>。</p>\n<p>如果对马尔可夫逻辑网加以扩展，给网络中每个顶点所代表的原子事实赋予一个连续分布的真值，得到的就是<span class=\"orange\">概率软逻辑</span>（probabilistic soft logic）。概率软逻辑能够对不确定的事实和规则同时建模，因而具有更强的不确定性处理能力。连续真值的引入也有助于问题的优化，从而大大提升了推理效率。</p>\n<p>归纳推理也好，演绎推理也罢，实现的都是符号推理，也就是在知识图谱中的实体和关系符号上直接进行推理。与符号推理对应的是<strong>数值推理</strong>。数值推理使用数值计算的方法来模拟推理过程，其典型的实现方案就是<strong>基于分布式表示的推理</strong>。</p>\n<p><strong><span class=\"orange\">分布式知识表示</span>（knowledge graph embedding）是将包含实体和关系的知识图谱组件嵌入到连续的向量空间中，以便在保持知识图谱内在结构的同时简化操作</strong>。</p>\n<p>在分布式的表示中，首先要定义出实体和关系在向量空间中的表示形式，其次要定义打分函数来衡量每个实体-关系组成立的可能性，最后通过构造优化问题来学习实体和关系的低维向量表示。</p>\n<p>在分布式知识表示的基础上，数值推理可以完成多类任务：利用打分函数计算所有备选答案的得分，可以实现知识图谱上的链接预测；根据某个实体元组的得分是否超过预先设定的阈值，可以对元组进行分类；计算不同实体的表示向量及其相似程度，则可以对实体进行解析......受篇幅所限，对具体推理任务的详细原理就不做介绍了。</p>\n<p>今天我和你分享了知识图谱的基本原理与简单应用。其要点如下：</p>\n<ul>\n<li>知识图谱是由大量的概念实体以及它们之间的关系构成的语义网络；</li>\n<li>用知识图谱实现从特殊到一般的归纳推理，典型的方法是路径排序算法；</li>\n<li>用知识图谱实现从一般到特殊的演绎推理，典型的方法是马尔可夫逻辑网和概率软逻辑；</li>\n<li>用知识图谱实现数值推理，典型的方法是基于分布式知识表示的方法。</li>\n</ul>\n<p>在数理逻辑中我曾提到，人工智能进行推理的一个关键问题是常识的缺失。那么知识图谱的出现是否能够给计算机注入常识呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/94/7a/9492443eef81027a5d1c7edb04fb6c7a.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"35 深度学习之外的人工智能 | 授人以鱼不如授人以渔：迁移学习","id":4009},"right":{"article_title":"37 应用场景 | 你是我的眼：计算机视觉","id":4020}}},{"article_id":4020,"article_title":"37 应用场景 | 你是我的眼：计算机视觉","article_content":"<p>2015年，微软上线了一个颜龄识别的机器人网站 how-old.net。这个网站可以根据用户上传的照片从面相上分析人物的年龄，一经推出便火爆全球，判断的正确率也很不赖。</p>\n<p>而在背后支撑这个娱乐性网站的，正是微软<strong>基于机器学习和深度学习的人脸特征提取技术</strong>。微软的颜龄识别算法首先执行人脸检测，再利用常见的分类和回归算法实现性别判定和年龄判定，在机器学习的框架下完成所有的任务。</p>\n<p>计算机视觉称得上是个古老的学科，它的任务是用计算机实现视觉感知功能，代替人眼执行对目标的识别、跟踪、测量和处理等任务，并从数字图像中获取信息。<strong>传统的计算机视觉方法通常包括图像预处理、特征提取、特征筛选、图像识别等几个步骤</strong>。</p>\n<p>对于给定的数字图像，计算机在处理时要先执行二次采样、平滑去噪、对比度提升和尺度调整等预处理操作，再对图像中的线条、边缘等全局特征和边角、斑点等局部特征，乃至更加复杂的运动和纹理特征进行检测，检测到的特征会被进一步用来对目标进行分类，或者估测特定的参数。</p>\n<p>虽然取得了不俗的进展，但计算机视觉的传统方法依然存在很大的局限，问题就出在待提取的特征要由人工手动设计，而不能让计算机自主学习。检测图像中的足球需要人为地设计出黑白块的特征，如果检测的对象变成篮球，那就要重新设计曲线纹路的特征。这样的计算机视觉其实是人类视觉的延伸，它的识别本质上讲还是由人类来完成的。</p>\n<p>如此一来，良好特征的设计就成为了视觉处理的关键和瓶颈。手工设计特征既需要大量的专门领域知识，也需要不断测试和调整，努力和运气缺一不可。而另一方面，现有的图像分类器都是像支持向量机这样的通用分类器，并没有针对数字图像的特征做出专门的优化。想要对特征设计和分类器训练这两个独立过程做出整体上的联合优化，其难度可想而知。</p>\n<p>好在，深度学习的横空出世改变了一切。在2012年的大规模视觉识别挑战赛（Large Scale Visual Recognition Challenge）上，辛顿带着他的深度神经网络AlexNet横扫了所有基于浅层特征的算法，以16.42%的错误率摘得桂冠。相形之下，东京大学26.17%的错误率和牛津大学26.79%的错误率显得黯然失色。</p>\n<p><strong>在图像识别中，应用最广的深度模型非卷积神经网络莫属</strong>。2012年大放异彩的AlexNet采用了包含7个隐藏层的卷积神经网络，总共有65万个神经元，待训练的参数数目更是达到了惊人的6千万。如此复杂的模型在训练上也会颇费功夫：用于训练的图像达到百万级别，这将花费2个GPU一周的时间。</p>\n<p>但这样的付出是值得的。和传统的数字图像处理技术相比，卷积神经网络不仅能够实现层次化的特征提取，还具备良好的迁移特性，在包含不同对象的图像中都能取得良好的效果。关于卷积神经网络的原理，你可以回顾一下之前的介绍。</p>\n<p>在计算机视觉领域，微软可以说是厚积薄发的巨头。以微软亚洲研究院为主的研究机构深耕于深度学习在计算机视觉中的应用，取得了一系列令人瞩目的成果。2015年，微软亚洲研究院的何恺明研究员提出了<strong><span class=\"orange\">深度残差网络</span></strong>（Deep Residual Network），又打开了计算机视觉一扇崭新的大门。</p>\n<!-- [[[read_end]]] -->\n<p>顾名思义，残差（residual）是残差网络的核心元素，但这个概念却并不复杂。没有引入残差的普通网络将输入$x$映射为$H(x)$，训练的意义就在于使用大量训练数据来拟合出映射关系$H(x)$。可残差网络独辟蹊径，它所拟合的对象并不是完整的映射$H(x)$，而是映射结果与输入之间的残差函数$F(x) = H(x) - x$。换句话说，<strong>整个网络只需要学习输入和输出之间差异的部分，这就是残差网络的核心思想</strong>。</p>\n<p>很简单吧？可这个小改动却蕴藏着大能量。和8层的AlexNet相比，何恺明论文中的残差网络达到了152层，真可以说是相当深了。网络深度的增加也带来了性能的提升，在2015年的大规模视觉识别挑战赛，深度残差网络以3.57%的错误率技压群雄，比以往的最好成绩提升了1%以上。可别小瞧这1个百分点，从95分进步到96分的难度可远远大于从85分进步到95分的难度。</p>\n<p>为什么引入残差能够带来优良的效果呢？这是因为残差网络在一定程度上解决了深度结构训练难的问题，降低了优化的难度。在深层网络中，层与层之间的乘积关系导致了<strong>梯度弥散</strong>（gradient vanishing）和<strong>梯度爆炸</strong>（gradient explosion）这些常见的问题，参数初始化不当很容易造成网络的不收敛。但残差网络有效地解决了这个问题，即使是一百层甚至是一千层的网络也可以达到收敛。</p>\n<p>为什么残差网络具有这样良好的性能？一种解释是将残差网络看作许多不同长度训练路径的集合。虽然网络的层数很多，但训练过程并不会遍历所有的层次，110层残差网络中的大部分梯度最多也只会涉及34个层的深度。如果说传统的梯度下降走的是人满为患的经济舱通道，那残差网络中的梯度走的就是畅通无阻的头等舱通道。这意味着较长的路径不会对训练产生任何的梯度贡献，残差网络也正是通过引入能够在整个深度网络中传递梯度的短路径绕开了梯度弥散的问题。</p>\n<p>从表示能力上看，深层模型应该是优于浅层模型的，因为将多出来的层设置为恒等映射，深层模型就会退化为浅层模型，因而深层模型的解集应该包含浅层模型的解集。但深层模型并不是将浅层模型简单地堆叠起来。当实际网络的层数增加时，受收敛性能的影响，无论是训练误差还是测试误差都会不降反升。残差的操作相当于用恒等映射对待学习的未知映射做了一重预处理，因而学习的对象就从原始的未知映射$H(x)$变成了对恒等映射的扰动$F(x)$，这就使深度结构的优势得以发挥。</p>\n<p>除了残差网络之外，另一个新结构是由美国康奈尔大学和Facebook人工智能研究院合作提出的<strong><span class=\"orange\">密集连接卷积网络</span></strong>（Densely Connected Convolutional Network）。网络中的“密集连接”指的是网络中的任意两层都有直接的连接，每个层的输入都是之前所有层输出的集合。这样一来，每个层次都要处理所有提取出来的低层与高层特征。</p>\n<p>密集网络的研究者提到，他们的想法借鉴了残差网络的思想，但密集网络的独特之处在于所有层都可以直接获取前面所有层中的特征，而残差网络中的层只能获取到和它相邻的那个层次。如果能够在空间上想象一下密集连接网络，你就会发现它和之前所有卷积网络模型的区别在于对层次化的递进结构的改进，这个模型更像是个全连接的扁平化网络。全连接的特性提升了结点，也就是不同层之间的交互性，让提取出的特征在不同的层次上得到重复的利用，起到整合信息流的作用。</p>\n<p>全连接的另外一个优势是训练难度的下降。在每一层中，损失函数和原始输入信号都是直接连接的，因此也能够避免连续相乘导致的梯度弥散。</p>\n<p>由于密集网络采用全连接的方式，参数数目和层数目之间就是平方的关系，因而当层数较多时，密集网络会出现参数爆炸的问题。为了克服连接重复利用导致的特征冗余，密集网络的每一层都只学习非常少的特征，而不像其他网络一样，在每个层上都要输出成百上千个特征。</p>\n<p>此外，密集网络还设计了瓶颈层（bottleneck layer）加变换层（translation layer）的结构，借此降低参数的数量。和残差网络相比，密集网络的参数数目不但不会增加，还有大概一半的下降。直接通过改变网络结构达到正则化的效果，密集网络绝对称得上匠心独运。</p>\n<p>虽然在近两年取得了突破性的进展，但对于深度学习的质疑依然存在。超大的参数数量不由得让人怀疑深度学习得到的其实就是某种程度的过拟合，而训练中的参数选择看起来也没有跳出经验科学的阶段。这也是对深度学习的一点警示：<strong>工程上的进展固然让人欣喜，但理论问题的解决依然任重道远</strong>。</p>\n<p>今天我和你分享了深度学习在计算机视觉，主要是物体识别中的应用，要点如下：</p>\n<ul>\n<li>在传统的计算机视觉方法中，特征设计和分类器训练是割裂的；</li>\n<li>以卷积神经网络为代表的深度结构可以实现通用的物体识别算法；</li>\n<li>深度残差网络将输出和输入之间的残差作为拟合对象，解决了深度神经网络训练难的问题；</li>\n<li>密集连接网络采用全连接方式，实现了特征的高度重用，降低了参数数量和训练难度。</li>\n</ul>\n<p>无论是残差网络还是密集网络，其实都不是惊天动地的理论突破，而是用较为简单的改进换来了良好的效果。那么这会给你带来什么样的启示呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><strong><span class=\"reference\">拓展学习</span></strong></p>\n<p><span class=\"reference\">关于计算机视觉，可以关注国际计算机视觉大会 <strong>ICCV</strong>（International Conference on Computer Vision），每两年举办一次。ICCV从 1987 年开始，已有 30 年的历史，是计算机视觉顶级会议。</span></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d9/aa/d99c200046dc728cb8977c02bdec07aa.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"36 深度学习之外的人工智能 | 滴水藏海：知识图谱","id":4010},"right":{"article_title":"38 应用场景 | 嘿, Siri：语音处理","id":4021}}},{"article_id":4021,"article_title":"38 应用场景 | 嘿, Siri：语音处理","article_content":"<p>Siri是由苹果公司开发的智能语音助手。2011年10月，Siri以系统内置应用的方式随iPhone 4s一起发布，并被逐步集成到苹果的全线产品之中。Siri支持自然语言的输入与输出，可以通过与用户的语言交互实现朗读短信、介绍餐厅、询问天气、设置闹钟等简单功能，它还能不断学习新的声音和语调，并提供对话式的应答。今天，我就结合苹果公司关于Siri的介绍简单谈谈人工智能中的语音处理。</p>\n<p>Siri的语音处理包括<strong>语音识别</strong>和<strong>语音合成</strong>两部分。<span class=\"orange\">语音识别</span>（speech recognition）的作用是听懂用户的话，<span class=\"orange\">语音合成</span>（speech synthesis）的作用则是生成Siri自己的回答。目前在苹果公司公开的技术博客Apple Machine Learning Journal上，主要给出的是语音合成的技术方案，但这些方案对语音识别也有启发。 </p>\n<p>在很多游戏和软件中，语音提示都是由声优提前录制而成，但以Siri为代表的实时语音助手们必须采用语音合成技术。<strong>业界主流的语音合成方法有两种：单元选择和参数合成</strong>。</p>\n<p>当具备足够数量的高品质录音时，单元选择方法能够合成出自然的高质量语音。相比之下，参数合成方法得到的结果虽然更加流利且容易识别，其整体质量却有所不及，因而适用于语料库较小的情景。</p>\n<p>将两者结合起来就得到了<strong>混合单元选择模式</strong>：其基本思路仍然是单元选择的思路，在预测需要选择的单元时则采用参数方法，Siri正是采用了这种模式。</p>\n<p>要实现高质量的语音合成，足够的录音语料是必备的基础。但这些语料不可能覆盖所有的表达，因而需要将其划分为音素和半音素等更微小的基本单元，再根据由输入语音转换成的文本将基本单元重组，合成全新的语音。</p>\n<p>当然，这样的重组绝非易事：在自然语言中，每个音素的选择既依赖于相邻音素，也取决于整体语句的音韵。单元选择方法完成的正是基本单元重组的任务：既要与输入的文本对应，又要生成符合语句内容的音调与音韵，同时还不能出现明显的打喯儿与中断。</p>\n<p><strong>Siri的语音合成系统包括文本分析、音韵生成、单元选择、波形串联四个模块，前两个环节对应前端的文本处理，后两个环节则对应后端的信号处理</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>文本分析模块既要对输入文本进行标音，也要提取其音韵特征，包括将非规范的数字和缩略语进行转换，从而生成单词标音，解析语法、重音和分句等信息。这些信息将被提供给音韵生成模块，用于确定输出语音的音调和音长等声学特征。基于这些特征，单元选择模块执行搜索，选择出最优的单元序列。这些序列最后由波形串联模块拼接为连续无间断的语音，作为输出呈现给手机用户。</p>\n<p>在音韵生成过程中，机器学习通常被用于确定文本与语音之间的关系，并根据文本背后的语义特征来预测输出语音的特征。比如说如果文本是个疑问句，那么输出的语音就应该以升调结尾，文本中的感情色彩也会影响语音特征的选择。</p>\n<p>理想的音韵模型可以由机器学习训练得到：输入是数字化的语言特征，其中包括音素的一致性、音素的前后关系、音节/单词/短语级别的位置特点等内容；输出则是经过数字化的声音特征，包括频谱特性、基波频率、音素持续时间等内容。经过训练，机器学习的算法就能完成从语言特征到声音特征的映射，生成的声音特征被用于指导单元选择过程，至关重要的指导因素就是合适的音调和音长。</p>\n<p>在后端，录制的语音流数据首先要根据语音识别声学模型进行分段，在这个过程中，输入的语音序列需要与从语音信号中提取出的声学特征进行对准。分割后的语音段则被用来生成语音单元数据库，这个数据库还可以根据每个单元的语境和声学特征进行增强。将从前端获得的声音特征代入到语音单元数据库中，就可以使用<strong>维特比算法</strong>（Viterbi algorithm）搜索用于语音合成的最佳路径。</p>\n<p>对于每个目标半音素，维特比算法都可以搜索出一个最优单元序列来合成它，评价最优性的指标包括两条：<strong>目标成本</strong>和<strong>拼接成本</strong>。</p>\n<p>目标成本用于评价目标声音特征和每个单元的声音特征的区别；拼接成本则用于评价相邻单元之间的声学差异。维特比搜索的目的就是找到一条使目标成本和拼接成本的加权和最小的单元路径。当最优的单元序列确定后，每个单元的波形便由波形串联模块拼接在一起，形成输出语音。</p>\n<p><strong>Siri的独特之处在于将深度学习应用在了混合单元选择模式中：用基于深度学习的一体化模型代替传统的隐马尔可夫模型指导最优单元序列的搜索，以自动并准确地预测数据库中单元的目标损失和拼接损失。</strong></p>\n<p>Siri使用的技术是<strong><span class=\"orange\">深度混合密度网络</span></strong>（Mixture Density Network），这是传统的深度神经网络和高斯混合模型（Gaussian Mixture Model）的组合。</p>\n<p>深度神经网络用于对输入特征和输出特征之间复杂而非线性的数量关系进行建模，通常使用反向传播方式来调整网络的权重参数，实现神经网络的训练；高斯混合模型用于在给定一组高斯分布的输入条件下，对输出的概率分布进行建模，通常使用期望最大化算法进行训练。</p>\n<p>深度混合密度网络结合了两者的优点。</p>\n<p>其输入是二进制序列，虽然是离散变量，它们却包含一些连续变化的特征，这些特征包括很多信息。其中既有当前音素与其前后各2个音素（共5个）之间的多元音素信息，也有从音节到单词到短语再到句子级别的信息，以及额外的重读和强调特征。</p>\n<p>其输出则是包含基波频率、梅尔倒谱系数、单元持续时间等一系列声音特征。除此之外，输出还包括每个特征的方差值，这些方差表示了上下文的相关权重。</p>\n<p>将语音合成的过程倒转过来，就是语音识别的近似过程。<strong>语音识别能够将语音信号转换成对应的文本信息，其系统通常包含预处理、特征提取、声学模型，语言模型和字典解码等几个模块</strong>。</p>\n<p>预处理模块通常用来滤除语音中的低频噪声，并对语音加以分段。特征提取模块将语音信号从时域变换到频域，在频域上提取语音的特征参数。接下来，声学模型将语音特征映射为不同的音素，语言模型将音素映射为可能的词组，字典解码则根据语言习惯，选择最可能的词组序列作为输出。</p>\n<p>在传统的语音识别中，特征选择过程通常使用<strong>梅尔倒谱系数</strong>（Mel Frequency Cepstral Coefficient）作为特征。梅尔倒谱系数能够对信号进行降维，将声音的基波信息提取到倒谱域的高频部分，声道信息提取到倒谱域的低频部分。</p>\n<p>声学模型采用的是<strong>高斯混合模型</strong>和<strong>隐马尔可夫模型</strong>（Hidden Markov Model）的组合，隐马尔可夫模型可以通过隐藏节点的引入解决特征序列到多个语音基本单元之间的对应关系，也使用<strong>期望最大化算法</strong>加以训练。语言模型可以根据语言的统计特性建立，统计语言模型假定当前词的概率只与之前若干个词相关，因而可以用于估计特定词语出现在给定上下文中的概率。</p>\n<p>随着神经网络和深度学习的发展，相关的技术也被应用在声学建模之中。包括卷积神经网络、循环神经网络和长短期记忆网络在内的主流神经网络模型都已经得到使用，并取得了不错的效果。<strong>与隐马尔可夫模型相比，神经网络的优点在于不依赖对特征统计特性的任何假设，但其缺点则是对时间上的依赖关系的建模能力较差，因而缺乏处理连续识别任务的能力</strong>。</p>\n<p>值得一提的是，<strong>Siri在声学模型的训练中用到了迁移学习技术</strong>，通过跨带宽和跨语言的初始化来提升神经网络的声学模型。</p>\n<p>目前，Siri的应用范围覆盖了所有的主流语言，而苹果的研究表明：<strong>不同语言、不同带宽语音数据的神经网络训练可以在同样的框架下进行，其基础是神经网络中特征变换的泛化特性，这使得特征变换的方法不依赖于具体的语言</strong>。</p>\n<p>因此，经过宽带语音信号预训练的神经网络可以直接用于蓝牙语音信号的训练；经过英语预训练的神经网络也可以直接用于汉语的训练。这种迁移学习技术大大提升了Siri在小数据集下的表现。</p>\n<p>今天我结合苹果公司公开的一些资料，以Siri为例和你分享了语音处理的一些技术进展。其要点如下：</p>\n<ul>\n<li>语音处理可以分为语音识别和语音合成两类任务；</li>\n<li>语音合成过程包括文本分析、音韵生成、单元选择、波形串联等步骤；</li>\n<li>语音识别过程包括预处理、特征提取、声学模型，语言模型和字典解码等步骤；</li>\n<li>深度学习和迁移学习等技术都已经被应用在语音处理之中。</li>\n</ul>\n<p>语音处理的最终目的不是简单地分析或者合成声音，而是为了更好地和人交互，从而以更简捷的方式解决问题。从交互的角度来看，你认为目前的语音助手还存在着哪些不足呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/2c/13/2cafffd82d32c23a0c3ccbee4aee0913.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"37 应用场景 | 你是我的眼：计算机视觉","id":4020},"right":{"article_title":"39 应用场景 | 心有灵犀一点通：对话系统","id":4022}}},{"article_id":4022,"article_title":"39 应用场景 | 心有灵犀一点通：对话系统","article_content":"<p>去年，人工智能领域的后起之秀Facebook着实火了一把。根据2017年6月17日美国《大西洋月刊》的报道，Facebook人工智能实验室设计的两个聊天机器人在谈判的训练中，发展出了一种全新的、只有它们自己能够理解的语言。</p>\n<p>这一爆炸性的消息一时间引得各路媒体蜂拥而至，煞有介事地讨论着人工智能如何颠覆人类对语言的理解，进而联想到人工智能会不会进化为热映新片《异形：契约》中戴维的角色，人类的命运仿佛又被推上了风口浪尖。</p>\n<p>可事实真的像媒体描述的那样耸人听闻吗？如果在网上搜索这些内容，你会发现那就像两个孩子之间的咿呀学语，根本没有任何语义可言——也确实是人类无法理解的语言。于是，本来是一次模型训练的失误，愣是被唯恐天下不乱的媒体解读为世界末日的启示录，让人哭笑不得。</p>\n<p>言归正传，<strong>人工智能的一个基本挑战就是赋予机器使用自然语言与人交流的能力</strong>。虽然这一目标在科幻电影中早已实现，但在现实生活中依然任重而道远。</p>\n<p>所有对话系统的祖师爷是诞生于1966年的Eliza，它由麻省理工学院的人工智能专家约瑟夫·魏岑鲍姆设计。有些出人意料的是，她扮演的角色是心理咨询师，并在这个角色上获得了相当程度的认可。Eliza根据人工设计的脚本与人类交流，她并不理解对话内容，只是通过模式匹配搜索合适的回复。</p>\n<p>设计者魏岑鲍姆将Eliza的基本技术问题总结为以下五个：<strong>重要词语的识别，最小语境范围的判定，恰当的转化选择，适当回复的生成和结束对话的能力</strong>。这个提纲为前赴后继的对话系统研究者们指明了研究方向。</p>\n<p>如果说Eliza代表了对话系统的1.0版对话机器人，那么以Siri、Cortana和Google Now为代表的语音助手就代表了对话系统的2.0版，也就是<strong>智能个人助理</strong>。它们的作用是提供各种被动性和主动性的帮助，以辅助用户在多个垂直领域完成任务。</p>\n<p>还是以Siri为例。Siri的系统运行环境被称为“活跃本体”。在这里，执行系统调用所有系统数据、词典、模型和程序，对用户输入进行解析，从而理解用户意图并调用外部服务。解析的过程由执行系统完成，它包含语言解释器、会话流控制器和任务控制器三个部分。语言解释器对文本形式的用户输入进行解析，会话流控制器根据语言解释器的解析结果生成会话，协同任务控制器确定Siri的输出。</p>\n<p>随着社交网络的发展，作为对话系统3.0版的<strong>社交聊天机器人</strong>正在走向成熟，它们可以满足用户对于沟通、情感及社会归属感的感性需求。这一领域的一个大玩家是互联网的新晋巨头Facebook。自2015年起，Facebook开展了大量针对对话系统的研究。而在2017年的<strong>国际学习表征会议</strong>（International Conference on Learning Representations）上，Facebook也系统地展示了在对话系统上的研究成果。</p>\n<p>Facebook的一个研究方向是通过端到端方式自行训练对话系统的可能性。诸如语音助手之类的传统对话系统都是目标导向的，即对话的目的是在有限的轮次内解决某些问题。为达到这一目的，语音助手采用的是空位填充的方式，每个空位代表着对话涉及内容的一个特征量。</p>\n<p>对于餐厅预订的对话系统来说，其空位就会包括餐厅位置、价格区间、菜式类型等内容。但这样的对话系统不仅需要大量的人为训练，而且只适用于特定问题，难以推广到其他应用场景之下。不同任务定义的特征量是不同的，餐厅预订的对话系统显然不能实现推荐电影的功能。</p>\n<p>相比之下，<strong>基于神经网络的端到端对话系统不需要人为介入，而是从对话本身中进行学习</strong>。所有元素都是从过往的对话过程中训练产生的，这就无需对对话的内容做出任何先验假设。这种方式由于无需人为干预，训练出来的对话系统也具备更强的通用性。</p>\n<!-- [[[read_end]]] -->\n<p><strong>端到端对话系统采用的模型是记忆网络</strong>。相对于普通神经网络，记忆网络的优势在于能够实现长期记忆，对话中的每一句话都被存储到记忆模块中，保证了信息不在压缩的过程中被丢失。正因为如此，记忆网络能够实现基于过往对话内容的学习，通过迭代读取已有对话的内容并结合短期的上下文语义来决定输出。</p>\n<p>Facebook对对话系统的期望远非实现预定餐厅这么简单，他们的愿景是实现具备通用人工智能水平的语音助手，既可以自由聊天又能够根据目标实现知识库调用。其研究一方面通过记忆网络的引入避免了大量的人工干预，解决了通用对话的方法论问题；另一方面提出了将一个对话任务分解为若干子任务的方法，简化了技术实现和定量评价的难度，给通用对话研究开辟了一个崭新的方向。</p>\n<p>在Facebook看来，对话系统中人为干预的作用是纠正机器所犯的错误，并让机器不会被同一块石头绊倒两次。在大多数研究依然聚焦在对固定数据集中的标签数据进行学习时，<strong>Facebook独辟蹊径，提出了通过与人类对话者的线上互动实现学习的想法</strong>。其研究的对话系统既能根据负反馈对模型进行调整，也能根据正反馈对模型进行加强，这对在通用环境中使用有监督模型训练出的聊天机器人尤其重要。</p>\n<p>在与对话伙伴的实时互动中，对机器人对话内容的反馈被纳入深度学习的整体框架中。对话是在问答任务的背景下进行的，机器人必须以简短的故事或一组事实来回答对话伙伴的一系列问题。</p>\n<p>在这个过程中，反馈的形式包括两种：一种是在常规的深度学习中使用的明确的数字类型回复，另一种是在人类对话中更加常见的文本式反馈。在处理反馈的过程中，如何以最小的反馈量实现对机器人的高效训练，机器人如何利用不同类型的反馈信号，在实时学习中如何避免收敛性差与不稳定性等都是需要解决的问题。</p>\n<p>在论文中，聊天机器人使用的仍然是基于记忆网络的端到端模型，并采用深度学习进行训练。训练的结果表明，在考虑学习算法中不稳定性问题的前提下，线上的实时训练和迭代训练都能够得到较好的效果。在机器人已有初始固定数据集训练基础的前提下，与人类实时或半实时的互动学习能够有效提升机器人的表现，这样的学习过程甚至可以无限地持续下去，这是不是意味着学无止境的道理对人工智能同样适用呢？</p>\n<p><strong>Facebook的研究重新审视了监督在对话系统训练中的作用</strong>。监督的目的不是纠正某个单独语句的错误，而是从策略上动态改善对话表现。与人类对话伙伴实时互动，在某种程度上讲可以说是对机器思维方式的训练。互动的引入突破了暴力搜索的传统方法，让聊天机器人基于推理而非寻找实现对话内容的选取，这也给通用人工智能带来了一丝微弱的曙光。</p>\n<p>当人类遇到难以回答的问题时可以选择提问，聊天机器人却只能要么选择猜测，要么调用外部的搜索引擎，这显然达不到良好的学习效果。在互动中，聊天机器人不仅要回答问题，更要提出问题，并从提问中进行学习。智能的聊天机器人应该能够在线上或线下的强化学习中通过提问得到提升。</p>\n<p>机器人的提问可以细化为三个场景：当机器人不能理解对话伙伴的表述时，需要进行问题澄清；当机器人在对已有知识库的推理中遇到麻烦时，需要进行知识运用；当机器人的已有知识库不完整时，则需要进行知识获取。</p>\n<p>不同场景下机器人提问的方式也不相同：在问题澄清中，机器人会要求同伴确认问题或重新表述问题；在知识运用中，机器人会请求相关的知识或询问问题是否与某个特定的知识有关；在知识获取中，机器人则会直接要求对话同伴给出答案。</p>\n<p>相对于特定的提问任务，在提问互动中更重要的是机器人学习在什么时机提问以及提问什么内容。在现实中，机器人的提问消耗的通常是人类的耐心，一个十万个为什么一样的聊天机器人肯定会被迅速弃用。</p>\n<p>因此，机器人对何时提问和提问什么要有一定的判断力，而这种判断力可以通过<strong>强化学习</strong>来建模和训练。实验的结果表明，理想的提问策略是机器人首先学习对话任务，再根据基于提问成本的问答策略和自身回答问题的能力来学习改善自己的性能。</p>\n<p>今天我结合Facebook公司公开的资料，和你分享了对话系统的发展历程与一些最新进展。其要点如下：</p>\n<ul>\n<li>早期的对话系统通过模式匹配和智能短语搜索对人类的合适回复；</li>\n<li>智能个人助理可以帮助用户在多个垂直领域完成任务；</li>\n<li>社交聊天机器人的作用是满足用户的情感需求；</li>\n<li>神经网络能够帮助社交聊天机器人实现通用化的学习。</li>\n</ul>\n<p>社交聊天机器人的发展也带来了关于道德规范的问题，一些机器人从社交网络上学到的想法需要引起注意与警惕。结合人工智能在无人武器中的规模化应用，对人工智能进行法律约束似乎已经迫在眉睫。那么应该如何看待与应对人工智能带来的伦理问题呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/9e/62/9e2357643a257b218f8a6c55694f3462.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"38 应用场景 | 嘿, Siri：语音处理","id":4021},"right":{"article_title":"40 应用场景 | 数字巴别塔：机器翻译","id":4023}}},{"article_id":4023,"article_title":"40 应用场景 | 数字巴别塔：机器翻译","article_content":"<p>根据圣经旧约《创世纪》中的记载，大洪水劫后，诺亚的子孙们在巴比伦附近的示拿地定居。说着同样语言的人类联合起来兴建巴别塔，这让上帝深为他们的虚荣和傲慢而震怒。于是他悄悄地离开天国来到人间，变乱了人类的语言，无法交流的人们做鸟兽散，巴别塔的伟念也就轰然倒塌。</p>\n<p>圣经中对语言诞生的描述充满了天谴的色彩，虽然事实根本就不是这么回事，但语言的差异的的确确给人类的沟通与交流平添了诸多隔阂。难道伟大的巴别塔注定只是存在于幻想之中的空中楼阁吗？</p>\n<p>令人沮丧的是，眼下这个问题的答案还是“是”。但在世界首台计算机ENIAC于1946年诞生后，科学家们就提出了利用计算机实现不同语言之间的自动翻译的想法。而在经历了超过一个甲子的岁月后，机器翻译已经取得了长足的进展，今天的执牛耳者则无疑是互联网巨头谷歌。</p>\n<p>机器翻译源于对自然语言的处理。<strong>1949年，洛克菲勒基金会的科学家沃伦·韦弗提出了利用计算机实现不同语言的自动翻译的想法，并且得到了学术界和产业界的广泛支持</strong>。韦弗的观点也代表了当时学术界的主流意见，就是以逐字对应的方法实现机器翻译。</p>\n<p>语言作为信息的载体，其本质可以被视为一套编码与解码系统，只不过这套系统的作用对象是客观世界与人类社会。将字/词看成构成语言的基本元素的话，每一种语言就都可以解构为所有字/词组成的集合。而引入中介语言可以把所有语言的编码统一成为用于机器翻译的中间层，进而实现翻译。</p>\n<p>比方说，同样是“自己”这个概念，在汉字中用“我”来表示，在英语中则用“I”来表示，机器翻译的作用就是在“我”和“I”这两个不同语言中的基本元素之间架起一座桥梁，实现准确的对应。</p>\n<p>然而乐观和热情不能左右现实存在的客观阻力。今天看来，这样的一一对应未免过于简单。同一个词可能存在多种意义，在不同的语言环境下也具有不同的表达效果，逐字对应的翻译在意义单一的专业术语上能有较好的表现，但在日常生活的复杂语言中就会演化为一场灾难。</p>\n<p>但天无绝人之路，进入二十世纪七十年代后，全球化浪潮的出现催生了客观需求，计算机性能的发展则突破了技术瓶颈，这两点让机器翻译重新回到人们的视野之中。</p>\n<p>这一时期的机器翻译有了全新的理论基础：语言学巨擘诺姆·乔姆斯基在其经典著作《句法结构》（Syntactic Structures）中对语言的内涵做了深入的阐述，他的核心观点是语言的基本元素并非字词，而是句子，一种语言中无限的句子可以由有限的规则推导出来。</p>\n<p>语言学的进化也对机器翻译的方法论产生了根本性的影响：韦弗推崇的基于字/词的字典匹配方法被推翻，基于规则的句法分析方法粉墨登场。这里的“规则”指的是句法结构与语序特点。</p>\n<p>显然，基于规则的机器翻译更贴近于人类的思考方式，也就是把一个句子视为整体，即使进行拆分也并不简单地依赖字词，而是根据逻辑关系进行处理。这使得人类翻译非常灵活，即使是不服从语法规则，甚至存在语病的句子都可以翻译得准确无误。</p>\n<p>正因如此，基于规则的机器翻译甫一诞生便受到众多推崇，似乎成为一劳永逸的不二法门。可理想虽然丰满，现实却依然骨感。基于句法规则的机器翻译也很快遇到了新问题：在面对多样句法的句子中，并没有比它的字词前任优秀多少，任何一款翻译软件都没法把“我勒个去”翻译成“Oh my God”。</p>\n<p>基于规则的窘境迫使研究者们重新思考机器翻译的原则。语言的形成过程是自底向上的过程，语法规则并不是在语言诞生之前预先设计出来的，而是在语言的进化过程中不断形成的。这促使机器翻译从基于规则的方法走向基于实例的方法：既然人类可以从已有语言中提取规则，机器为什么不能呢？<strong>眼下，基于深度学习和海量数据的统计机器翻译已是业界主流，谷歌正是这个领域的领头羊与先行者</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>在<strong>基于神经机器翻译</strong>（Google Neural Machine Translation）的算法横空出世之前，谷歌翻译也经历了超过十年的蛰伏。自2001年诞生后，谷歌翻译就一直在不温不火的状态中挣扎。直到痛定思痛的高层变换思路，将技术团队中的主要力量从原本的语言学家替换为计算机科学家。<strong>这意味着机器翻译的实现理念从句法结构与语序特点的规则化解构转换为对大量平行语料的统计分析构建模型，曙光才出现在地平线上</strong>。</p>\n<p>神经机器翻译最主要的特点是整体处理，也就是将整个句子视作翻译单元，对句子中的每一部分进行带有逻辑的关联翻译，翻译每个字词时都包含着整句话的逻辑。用一个不甚恰当的类比来描述：如果说基于短语的翻译结果是庖丁解牛得到的全牛宴，神经机器翻译的结果就是最大程度保持原貌的烤全牛。</p>\n<p>在结构上，谷歌的神经机器翻译建立了由长短期记忆层构成了分别用于编码和译码的递归神经网络，并引入了注意力机制和残差连接，让翻译的速度和准确度都能达到用户的要求。编码器和译码器都由8个长短期记忆层构成，两个网络中不同的长短期记忆层以残差连接。编码器网络的最底层和译码器网络的最顶层则通过注意力模块进行连接，其作用在于使译码器网络在译码过程中分别关注输入语句的不同部分。</p>\n<p>在输出端，译码器网络选择一个使特定目标函数最大化的序列作为翻译的输出。谷歌采用波束搜索技术来选取最优的输出序列，并使用覆盖率惩罚和长度正则化来优化搜索过程。覆盖率惩罚的作用是保证输入语句中每个部分都有对应的输出结果，避免漏译情况的发生；长度正则化则可以控制输出语句的长度：如果没有这一处理，译码器网络将优先选择较短的序列作为输出。</p>\n<p>出于效率的考虑，神经机器翻译同时使用了数据并行计算和模型并行计算。数据并行计算的作用在于并行训练模型的多个副本，模型并行计算的作用则在于提升每个副本中梯度计算的速度。此外，谷歌还在精确性和速度之间做出了一些折中，利用量化推断技术降低算术计算的精确性，以换取运行速度的大幅度提升。</p>\n<p>在提出神经机器翻译仅仅两个月后，谷歌又提出了“<strong>零知识翻译</strong>”的概念。这一系统在前文系统的基础上更进一步，只用一套模型便可以实现103种不同语言的互译，这无疑意味着系统通用性的极大提升：一个神经网络以任何语言作为输入并转换成任何输出语言，而不需要任意输入-输出语言之间的两两配对。换言之，<strong>谷歌实现了一把解锁不同语言的万能钥匙，这一通用的解决方案对机器翻译而言无疑具有里程碑式的意义</strong>。</p>\n<p>这一多语种互译系统是对原始系统改进的结果，它并未修改基础系统的模型架构，而是在输入语句之前人为地添加标志以确定翻译的目标语言。通过共享同一个词胞数据集，这一单个模型就能够在不添加额外参数的前提下实现多语种的高质量互译。</p>\n<p>虽然在模型训练的过程中不可能将每种语言都纳入数据库，但互译系统可以通过特定的“桥接”操作实现对在训练过程中没有明确遇见过的语言对之间的互相翻译，这也就是“零知识翻译”的含义。</p>\n<p>零知识翻译的实现要归功于神经网络的迁移学习特性。“桥接”操作即是在两种陌生的语言中引入一种其他语言作为中介，用英文“桥接”起西班牙文和葡萄牙文的互译意味着现将输入的西班牙文翻译为英文，再将英文的中间结果转译为葡萄牙文。这种方式无疑会使翻译的时间翻倍，而额外的转译过程也可能造成原意的偏离。</p>\n<p>但它的优势同样明显，那就是无需对两种陌生语言进行训练。要知道，如果对任意两种语言构成的输入-输出对进行训练，要训练的模型数目将达到$100 ^ 2 = 10000$的级别；而对所有语言的翻译使用英语作为中介的话，要训练的模型数目将只有$100 \\times 2 = 200$，后者在工程上实现的难度显然远远小于前者。</p>\n<p>除了在实际翻译中的应用，零知识翻译还提供了关于语言本身更深层次上的洞见，系统是否自行生成了一种语义表征体系？语言和文字本身就可以视为对客观世界进行编码的系统。而在转译的过程中，神经网络并非简单地记忆不同短语之间的映射关系，而是直接对句子的语义进行重新编码。这套由神经网络自主研发的语义系统被谷歌称为“<strong>国际通用语</strong>”（interlingua）。<strong>某种意义上讲，这才是真正的“世界语”，而这一现象对未来语言学发展的影响同样值得关注</strong>。</p>\n<p>虽然谷歌在机器翻译领域取得了令人侧目的成就，但实话实说，机器翻译的准确率依然处在较低的水平上，甚至经常出现一些令人啼笑皆非的结果。但它代表的是人类相互尊重、相互理解的热切渴求。有了海量的数据和先进的算法，这些曾经遥远的愿景与渴求正在变得触手可及。</p>\n<p>今天我结合谷歌公开发表的论文，和你分享了机器翻译的发展历程与一些最新进展。其要点如下：</p>\n<ul>\n<li>早期的机器翻译采用的是逐字对应的方法；</li>\n<li>语言学的进展使机器翻译转而依赖句法规则；</li>\n<li>谷歌将神经网络引入机器翻译之中，利用大量数据提升翻译精确性；</li>\n<li>神经网络可以通过迁移学习“桥接”不同的语言，实现零知识翻译。</li>\n</ul>\n<p>在另一个角度上，机器翻译的进展对语言本身的发展也存在着反作用。语言并不是自然出现的，而是人类社会的产物，其发展也势必会受到人类文化的影响。那么<strong>人工智能的发展到底会对人类自身产生何种反作用呢？</strong></p>\n<p><strong><span class=\"orange\">这是人工智能的终极问题。</span></strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e3/5b/e337c988eefbf0cfc8d6a30df3e3755b.jpg\" alt=\"\"></p>\n<p>下周二，3月13日 20:30，我会在极客时间做一场直播，主题是“人工智能必备的数学基础”，带你梳理专栏的第一个模块，希望能够解答一些你在这一模块的疑惑。</p>\n<p>关于数学基础你还有哪些问题，欢迎你给我留言，我们直播见！</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/30/8f/30a44a639b41a0ca8ccb9a61eead288f.jpg\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"39 应用场景 | 心有灵犀一点通：对话系统","id":4022},"right":{"article_title":"课外谈 | “人工智能基础课”之二三闲话","id":2880}}},{"article_id":2880,"article_title":"课外谈 | “人工智能基础课”之二三闲话","article_content":"<p>不知不觉，专栏上新已一月有余。各位朋友在阅读之余的互动带给我不少的思考，借此机会呢，和你聊聊这个专栏的杂七杂八。</p>\n<h2 id=\"-\">为什么要做基础课这个专栏？</h2>\n<p>正如开篇词中所说，人工智能很可能成为下一个生存必备的技能。不管以后是不是吃这碗饭，对人工智能原理和方法的理解都不可或缺，这是技术进步倒逼出来的结果。深一步讲，先贤孟子“劳心者治人，劳力者治于人”的名句一针见血、言犹在耳。两千多年后的今天，虽然劳心和劳力的方式都发生了翻天覆地的变化，可治与受治的关系依然不动如山。更可怕的是，不进则退的劳力者甚至没有资格受治于人，而不得不受治于算法。《未来简史》中对这个问题的论述，有些是难得的理性思考，你可以读一读。</p>\n<p>在这个日新月异的时代，懂点儿人工智能的知识绝对有必要，极客邦的技术人们能够敏锐地捕捉到这个趋势，其前瞻性值得佩服。可是，当编辑和我商谈开设一个关于人工智能基础知识的专栏时，我的第一感觉是费力不讨好。为什么呢？因为理工类学科的基础知识普及和品三国或者解读红楼梦完全是两码事。人文学科的核心价值在于观点和见解，这是对主观意识进行加工的产物；而理工学科的核心价值在于事实和规律，这是客观世界的存在方式。这个核心区别决定了讲文学可以天马行空，可讲科学必须脚踏实地。要将脚踏实地的东西天马行空地呈现出来，绝对是个棘手的问题。</p>\n<p>正因如此，我们的专栏定位在技术的设计思想与实现原理。根据我自己学习和教学的经验，<strong>要在有限的时间里完成新知识的入门，首要问题是理解它为什么有用，其次才是掌握它如何起作用</strong>。在每篇专栏的文章中，我都力图通过通俗的语言解释清楚方法背后的意义：<strong>每种方法都有其诞生背景和适用场景，理解这些才有助于认识方法的核心价值</strong>。如果不管青红皂白摆出长篇大论的数学公式，文章的半数字符都是拉丁字母和希腊字母，这不仅对阅读者是种折磨，也和“基础课”的宗旨背道而驰。</p>\n<p>但想必你也有这样的经验：解释复杂问题时，高度抽象的公式的表达能力远远胜过啰哩啰嗦的语言。所以如何能像宣传语所说的通俗易懂地把算法解释清楚，其伤脑筋的程度大大超出了我的预期。对困难的估计不足使写作时每每因为找不到合适的实例抓耳挠腮已经成为常态，可一旦找到一个形神兼备的类比，那大呼过瘾的兴奋又溢于言表。对我自己而言，这又何尝不是学习与进步的过程呢？</p>\n<h2 id=\"-\">在专栏里我们会讨论哪些内容？</h2>\n<p>在具体内容的设置上，我认为专栏作为一个相对长期而稳定的输出过程，结构化和系统性是应该满足的基本要求，而人工智能基础课这个主题恰恰具有内容编排上先天的优势。出于这样的考虑，专栏的内容以当下主流的连接主义学派为主，覆盖了数学基础、机器学习、人工神经网络、深度学习、应用场景等内容，力求给出对人工智能发展现状完整的概括。</p>\n<p>这个专栏能给予你对人工智能概貌性的理解，但它只是块敲门砖，而不是打开人工智能神秘宫殿的万能钥匙。想要深入理解人工智能，你还需要大量时间、精力、甚至金钱的付出。毕竟，劳心者也不是那么好当的。</p>\n<p>此外，专栏的目标场景是茶余饭后的碎片化时间，因而采用的是音频为主、文字为辅的形式。在有限的容量中，我的优先选择一定是将每个话题的核心内容呈现给你。而有舍才有得意味着在突出主题的指导思想下，辅助性和支撑性内容就必然有所缩减，被舍弃的辅助支撑内容就是数学推导以及代码实现。当然，这部分内容的淡化也出于操作性上的考虑，因为公式和代码既不适合以音频的方式表达，也不适合在移动终端上处理。</p>\n<p>但对数学推导和代码实现的淡化并不意味着这两部分内容无足轻重。如果有志于从事人工智能理论方面的研究，那么理论问题就是必须征服的大山。<strong>我建议你在通过专栏理解了每种方法背后的思想与思路后，再结合推荐的参考书目以及二级参考书目去深入数学细节</strong>。当然，<strong>阅读最新的论文也是跟进最新研究进展的有效方法</strong>。大部分顶级会议论文都可以在会议网站或预印本网站arxiv.org上免费获取，可以根据自己的需要有选择地阅读。</p>\n<h2 id=\"-\">纸上得来终觉浅，绝知此事要躬行</h2>\n<p><strong>如果要在人工智能的工程岗位上大显身手，就要让知识转化成能力。其实要实现这个转化，最好的办法就是参与到实战项目中</strong>。无论是科研项目还是工程项目都能让你在实践中成长，这个“不想学也得学”的过程虽然痛苦，却有涅槃的奇效。如果没有参加项目的条件，一个退而求其次的选择是做一做网络公开课的大作业。包括辛顿和吴恩达这些大咖都在各种平台上开设有人工智能的公开课程，完成其中的项目也是很好的实践方式。</p>\n<p>虽然网络上的资源非常丰富，大量优质的代码唾手可得，但我还是建议你<strong>亲自动手编一编写一写</strong>。毕竟终日而思不如须臾之所学，即使是Hello World在实际编写时都可能出现这样那样的问题，远非想象中那么简单。在这里也给你推荐一本Peter Harrington所著的Machine Learning in Action，中译本名为<strong>《机器学习实战》</strong>。本书是基于Python语言，面向应用的算法书，包含大量现实案例，可以作为代码学习的参考。</p>\n<h2 id=\"-\">我们该如何对待人工智能？</h2>\n<p>最后要说的是，对待人工智能，我们既不能轻视它，也不该神化它。只要掌握人工智能的原理与方法，就永远轮不到它来主宰你的命运。所以我们务必以平常心来看待人工智能，不要被媒体的聒噪所忽悠。<strong>对人工智能的态度应该是对知识本身的尊重，而不是对虚无缥缈的超智能的莫名惶恐</strong>。回头想想，你在学习高等数学或者C语言的时候会有它们会毁灭世界的担忧吗？人工智能和高数、C语言又有什么本质区别呢？</p>\n<p>在此，我要衷心感谢你的厚爱与反馈，它们既为我指明了改进的方向，也是督促我奉献更好内容的不竭动力。当然，受个人水平所限，专栏中肯定存在着各种各样的不足，恳请你继续提出宝贵建议，让你我一起进步、成长。</p>\n<p>与君共勉！</p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"40 应用场景 | 数字巴别塔：机器翻译","id":4023},"right":{"article_title":"推荐阅读 | 我与人工智能的故事","id":4167}}},{"article_id":4167,"article_title":"推荐阅读 | 我与人工智能的故事","article_content":"<p>不知不觉中，专栏已经更新到了最后一个模块。在这个过程中，收到了很多同学的留言。我看到了每一条留言背后的那份努力，今天就和你分享一位同学的故事。</p>\n<p> <img src=\"https://static001.geekbang.org/resource/image/d4/90/d492f621e67c2119d6a3267abdbc3890.jpg\" alt=\"\">\n<span class=\"reference\">留言来自《深度学习 | 玉不琢不成器：深度学习中的优化》</span></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/20/1d/207010e576da83f1c1ad5efd46ede91d.jpg\" alt=\"\">\n<span class=\"reference\">留言来自《深度学习之外的人工智能 | 一图胜千言：概率图模型》</span></p>\n<p>下面就是徐凌的故事，我们一起学习。</p>\n<hr>\n<h2 id=\"-\">一、立志学人工智能的缘由</h2>\n<p>我本人平时很喜欢看科普知识，自从2016年得知Alpha Go战胜李世石之后，对人工智能很感兴趣。通过网络搜索，我报了吴恩达在Cousera上面的公开课。虽然在这门公开课里，吴恩达力求深入简出，但还是有很多内容我不太懂。</p>\n<p>我是2005年大学毕业的，专业是市场营销，所以在数学和编程基础上我比较薄弱，再加上我毕业也有十几年了，甚至有很多中学数学也都遗忘了。刚好这段时间我听了喜马拉雅上的《卓老板聊科技》里讲到数学家伽罗瓦的故事，引起我对数学的兴趣。所以我决心学好数学。</p>\n<p>2016年我首先把中学数学快速地再学了一遍，其实中学数学很多内容比如三角函数，当时学的时候觉得无用，但是其实都是很重要的基础。从2016年下半年开始，我从单变量微积分开始学进阶一些的数学。</p>\n<h2 id=\"-\">二、我的学习计划</h2>\n<p>2017年，我自学了单变量微积分、多变量微积分、线性代数、微分方程、离散数学和统计学。编程方面，去年我学了Python，Matlab和SQL。 Python是跟人工智能关系很大的语言，而Matlab在机器学习上面也很重要。</p>\n<p>今年我计划继续加强微积分、线性代数、微分方程和离散数学方面知识。同时会在概率学、数论、数学分析和优化论上面花费一些功夫。今年下半年，我还打算开始学习随机过程（网上看到随机过程比较难）。</p>\n<p>编程方面，我原来打算学Java和C++语言。但是之前在知识星球上和群友交流，别人意见是：编程语言不需要学太多。考虑到数学可能更重要，今年我在编程方面的计划是，主要是继续巩固Matlab和Python的技能。</p>\n<p>明年我的计划大体是开始学习集合论、图论和泛函分析。开始尝试阅读一些人工智能方面的相关论文。</p>\n<p>除此之外，我每天大概会花20分钟左右学习英语，这个时间并不固定，一般都是拿坐电梯、排队买咖啡的时间学英语。我觉得学英语还是很重要的，只有英语好才能听懂视频课程，看懂原版教材，此外以后要读英文论文也需要一定的英语能力。</p>\n<h2 id=\"-\">三、学习方法</h2>\n<p>我觉得学习方法是很重要的，很多东西学不好大多不是智商问题，更可能是没有学习热情，或者学习方法不对。通过这几年的学习，我总结出不少我认为很有效的学习方法，同时也学习了很多别人先进的学习方法。在这方面我从万维钢老师的专栏中学到不少。</p>\n<ul>\n<li><p>近年来一万小时刻意练习理论很火。万维钢老师说过数学是特别适合刻意练习的，我很想试验一下1万小时下来，我能不能真的从无基础开始成为人工智能方面的专家。这里刻意练习很重要，虽然现在社会上终身学习的风气很旺，但是大多数人的学习是娱乐性的，这不能叫做刻意练习。比如我一开始通过看可汗学院视频学数学，虽然可汗的数学视频非常好，但是我觉得看可汗学院并不是很好的学数学的办法。因为一来可汗学院教的数学内容偏简单，像微积分，线性代数等章节，可汗只讲了很基础的内容。二来可汗学院习题不够。我一开始通过可汗学院学数学，当时看视频觉得都懂了，结果一个月后回顾时发现所有知识点全都忘光了。这主要原因就是缺乏练习。<strong>学数学一定要多做习题，把一种算法练到成为条件反射才算是掌握了这个算法，我觉得这才是刻意练习的精髓</strong>。</p>\n</li>\n<li><p>我觉得上麻省理工学院的 <strong>MIT OPEN COUSERWARE</strong> 网站学数学很有效，这个网站上面有麻省理工的很多课程的教学视频，老师讲课质量都非常高，比国内的高数教学视频要好。除此之外，对于不少课程，这个网站上面还有教科书推荐，课后习题和答案，甚至还有期末考试试卷和答案。我觉得除了看他们的视频之外，按照他上面的推荐买一些英文原版的教科书来学。每个章节结束后自觉做习题很重要。最重要的是学完一门课自己应该做几份这门课的期末试题，我对自己的标准是自己做卷子能达到80%以上的才能算过关，否则应该重新学一遍。</p>\n</li>\n</ul>\n<ul>\n<li><strong>我把全年的学习分成50个小计划，这样每完成一个计划有一定的成就感。同时每100小时作为一个学习区块，不同的知识内容按照不同的难度分配不同的学习时间区块</strong>。这样做一来可以逼迫自己在一些知识上花费足够的时间，二来可以避免有些知识上花费过度的时间精力。因为每个人时间都是有限的，假如你一年自学有1000小时可供利用，你不希望在某个细节上花费掉三四百小时，导致其他知识没时间学。要设定一个难度合理的计划其实也不容易，去年我的计划就远远没完成，今年过了6周时间，我也只完成了3周的计划，希望能尽快补上。</li>\n</ul>\n<ul>\n<li>时间管理方面，其实很多闲暇时间都可以被利用上。<strong>我现在大体能做到把一天之内闲暇时间都利用上了</strong>。走路，上厕所，洗澡时可以听音频节目，通勤时可以阅读一些学习内容。我觉得自己开车是很浪费时间的事情，坐公共交通的话就可以把通勤时间利用上了。其实光是坐电梯时都能挤出不少时间。我算过，平均每天坐电梯，等电梯的时间都要十几分钟左右。这段时间其实拿来背单词很不错，15分钟可以背50个单词左右。其实每天要真挤时间，可以挤出很多时间来学习，一般人也就每天睡7个半小时，工作8小时，每天剩下8.5小时除去一些杂事和适当娱乐，<strong>理论上也能拿出3到4小时出来</strong>。周末在家可以花更多时间学习。当然有时候压力太大也会偷懒，比如我去年下半年玩某款游戏上瘾，试过连续两三晚一直在玩游戏的。自己尽量克服就好。<strong>我这两年一共学习了2039小时，我计划是每年学习1500小时，这样7年就可以完成1万小时学习</strong>。目前实际上还没有完成计划。</li>\n</ul>\n<ul>\n<li>我个人经验是学习时最好设定1小时闹钟，学习一个小时，休息3分钟比较好。学习期间尽量手机调飞行模式，不要听音乐避免干扰。这样能最大化学习效率。我的心态是：只有1万个小时，浪费1小时就少一小时，这样能珍惜时间。</li>\n</ul>\n<ul>\n<li>根据艾宾浩斯记忆法，我每半年会花100小时左右复习一下之前学习的东西。</li>\n</ul>\n<ul>\n<li>我在极客时间订阅了王天一教授的《人工智能基础课》和朱赟的《技术管理课》，同时在得到和喜马拉雅上也订阅了一些节目。其实这些节目作为平时调剂，休闲娱乐的内容还是挺不错的。</li>\n</ul>\n<ul>\n<li><strong>通过分享学习</strong>。我自己有一个公众号，主要做科普知识分享，公众号叫“凌哥杂谈”。通过分享来巩固记忆和大家一起交流是愉快的，也欢迎大家订阅。</li>\n</ul>\n<h2 id=\"-\">四、我觉得学人工智能的未来优势</h2>\n<p>我觉得如果能真正掌握人工智能技术，未来一定是有不错的前景的。我感觉这项技术不像之前几年很火但是几年后就销声匿迹的其他互联网概念，这项技术未来应该会有越来越大的市场前景。而且和纯粹学计算机相比，人工智能的门槛较高，尤其是数学门槛。这阻碍了一部分老程序员转行从事人工智能的流动性。</p>\n<h2 id=\"-\">五、学习的心态</h2>\n<p>有些人觉得终身学习是一件很苦的事，其实把这个当成一个兴趣的话，学习也可以当成一种娱乐。有很多人每天花费很多时间做健身，我以前也练过健身。其实训练头脑跟练健身是一样的，只不过我们训练的是自己的大脑而已。而且练健身的话，只要一停，半年后身体就复原了。而对大脑充电，有很多知识可以保持很长时间。我以前也业余练过长跑，参加过马拉松，<strong>我觉得学习一项技能和练健身和练长跑所需的自律和练习方法都有相通之处</strong>。只不过很多人没有意识到学习也是一种锻炼。</p>\n<p>以上是我的学习心得，和大家分享一下，希望大家能够共同进步。</p>\n<!-- [[[read_end]]] -->\n<hr>\n<p>上面是徐凌同学的分享，我自己也受益颇多。我想每个人都有自己的学习经历和故事，你和人工智能有哪些故事呢？也欢迎你写一写，就像徐凌所说，通过“分享来学习”。</p>\n<p>独行者速，众行者远，在分享交流中拓展视野、提升格局，让我们共同进步。</p>\n<p><span class=\"orange\">分享“我与人工智能的故事”，等你留言！</span></p>\n<p><span class=\"orange\">欢迎你踊跃参与，极客时间会送上30元奖励红包！</span></p>\n<p></p>\n","neighbors":{"left":{"article_title":"课外谈 | “人工智能基础课”之二三闲话","id":2880},"right":{"article_title":"直播回顾 | 机器学习必备的数学基础","id":4955}}},{"article_id":4955,"article_title":"直播回顾 | 机器学习必备的数学基础","article_content":"<p>自从12月份开始更新到现在，我们这个专栏已经不知不觉走过了4个月的时间。在这4个月的时间当中，我和大家一块分享了最近火热的人工智能的技术，包括它的一些数学基础、机器学习的算法以及神经网络，还有深度学习这样一些热点的话题。</p>\n<p>俗话说得好，编筐编篓，全在收口。能在最后一次更新的时候和大家做一次这样的视频直播的分享，我觉得也是非常荣幸，能够有机会和大家进行一个更加深入的交流。</p>\n<p>我们今天分享的内容主要是<strong>关于机器学习中的基础数学</strong>。</p>\n<p>从这个主题当中，我也可以感受到，大家在学习人工智能的时候，一个务实的态度。一方面，在接触人工智能的时候，一些前沿的算法或者是一些前沿的模型，我们是要接触的，但是归根到底，还是要回归到基础数学当中来。</p>\n<p>如果不把基础打好，你是没有办法真正理解这些复杂的模型的，所以说<strong>不管算法它有多么复杂，最终都是简单数学的一个若干的组合</strong>。只不过在这个组合的过程当中，它的难度会越来越大，它实现的功能也会越来越强。</p>\n<p>关于人工智能或者说机器学习当中的这些基本数学，我们今天分享的内容，包括这样四个部分：</p>\n<p>第一，我们关注的是这个机器学习当中它会用到哪些基础数学；</p>\n<p>第二，这些数学在机器学习，或者说在人工智能这个大环境下都能起到一些什么样的作用；</p>\n<p>第三，有了这个学科，有了数学，分析了它的作用之后，我们需要把它掌握到一个什么样的程度；</p>\n<p>最后，简单介绍一下，如何在短时间内，高效率地掌握。</p>\n<h2 id=\"-\">机器学习会用到哪些数学基础</h2>\n<p>第一部分，我们先来看一看机器学习需要哪些数学的基础。我们可以先引用一个专家的定义。这个专家是来自美国华盛顿大学的佩德罗·多明戈斯（Pedro Domingos），这也是人工智能领域的一个老兵。他对于机器学习给出了这样一个定义，<strong><span class=\"orange\">机器学习是由三个部分组成，分别是表示、评价，还有优化</span></strong>。这样的三个步骤，实际上也就对应着在机器学习当中所需要的数学。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/cc/d9/cc3765efbc2c06d1e058d001eb90fed9.png\" alt=\"\"></p>\n<p><strong><span class=\"orange\">机器学习三部曲之表示</span></strong></p>\n<!-- [[[read_end]]] -->\n<p><strong>在表示这一步当中，我们需要建立起数据，还有实际问题的抽象模型</strong>。所以，这里面就包括了两个方面。一方面我们要对要解决的实际问题进行抽象化处理。比方说我们要设计一个算法，判断一个邮件它到底是不是一封垃圾邮件，那么得到的结果无外乎两种，要么是，要么不是。</p>\n<p>这样一个问题如果对它做抽象，实际上就是个二分类。是呢，我们可以把它定义成0，不是呢，可以把它定义成1。所以，你这个问题最终要解决的是什么呢？输出一个0或者1的结果。当然你把0和1的意义调过来也可以，用1代表是垃圾邮件，0代表不是，也是可以的。</p>\n<p>所以，在表示的过程当中，我们要解决的问题就是把我们面临的真实世界当中的一些物理问题给它抽象化，抽象成一个数学问题。抽象出来这个数学问题之后，我们要进一步去解决它，还要对这个数据进行表示。</p>\n<p><strong>对于问题抽象完了以后，我们还要对数据进行抽象</strong>。在判定这个邮件到底是不是垃圾邮件的时候，我们要怎么判断呢？要根据它的特征进行判断，看一看这个邮件里的关健字是否有关于推销的，或者关于产品的。这些关键字，我们就要把它表示成一个特征，表示成一个向量，或者表示成其他的形式。表示成向量也好，表示成其他形式也好，都是对这个数据做出了抽象。</p>\n<p>在表示阶段，我们需要建立的是数据，还有问题的抽象模型。把这个模型建立出来，然后去寻找合理的算法。</p>\n<ul>\n<li><strong>K-近邻算法</strong></li>\n</ul>\n<p>在机器学习当中，我们常见的有K-近邻算法。K-近邻算法在我们的专栏中没有提到，因为它太简单了。它实际上就是，找到一个样本点和这个样本点最近的几个邻居，最近的这K个邻居。按照少数服从多数的原则，对它进行分类，这就是K-近邻算法。</p>\n<ul>\n<li><strong>回归模型</strong></li>\n</ul>\n<p>除此之外，还有线性回归这样的统计学习方法。我们建立一个线性回归模型，当然，对二分类我们可以建立逻辑回归模型。</p>\n<ul>\n<li><strong>决策树</strong></li>\n</ul>\n<p>还有像决策树这样的方法。决策树它不依赖于数据，完全是自顶向下的一个设计。线性回归也好，逻辑回归也好，它是从数据反过来去推导模型，而决策树直接去用模型判定数据，两个方向不太一样。</p>\n<ul>\n<li><strong>SVM支持向量机</strong></li>\n</ul>\n<p>最后，还有SVM支持向量机这样的纯数学方法。所以说表示的部分，我们需要把问题和数据进行抽象，这个时候我们就要用到抽象的工具。</p>\n<p><strong><span class=\"orange\">机器学习三部曲之评价</span></strong></p>\n<p>给定了模型之后，我们如何评价这个模型的好坏呢？这个时候就需要<strong>设定一个目标函数，来评价这个模型的性质</strong>。</p>\n<ul>\n<li><strong>设定目标函数</strong></li>\n</ul>\n<p>目标函数的选取也可以有多种形式。像对于我们说到的垃圾邮件这种问题，我们可以定义一个错误率。比方说一个邮件它原本不是垃圾邮件，但是我这个算法误判成了垃圾邮件，这就是一个错例。所以呢，<strong>错误率在分类问题当中是个常用的指标，或者说常用的目标函数</strong>。</p>\n<ul>\n<li><strong>最小均方误差和最大后验概率</strong></li>\n</ul>\n<p>那么<strong>在回归当中呢，我们会使用最小均方误差这样一个常用目标函数，尤其是在线性回归里</strong>。除此之外呢，还有最大后验概率，一些其他的指标。</p>\n<p><strong><span class=\"orange\">机器学习三部曲之优化</span></strong></p>\n<p>有了目标函数以后，我们要求解这个目标函数在模型之下的一个最优解，这个模型能够获取到的最小错误率，或者最小均方误差是多少呢？我们要求出一个特定的值。没有这个值的话，你如何评价不同的模型它到底是好是坏呢？所以说<strong>优化这个步骤的作用是求解目标函数在模型之下的一个最优解，看看这个模型在解决这个问题的时候，最好能达到什么样的程度。</strong></p>\n<p>所以说，多明戈斯教授总结到的机器学习的三个步骤，包括了表示、评价、优化这样三个步骤，在这三个步骤当中我们会用到不同的数学公式来分别解决这三个问题。</p>\n<p><strong><span class=\"orange\">今天的机器学习解决什么问题</span></strong></p>\n<p>我们今天所说的这个机器学习应该和它原始的含义是有些区别的。我们都知道，人工智能是上个世纪50年代到60年代这个区间上建立起来的。最早的时候，机器学习不是像今天这样子，它执行的是什么任务呢？执行的是纯粹的逻辑推理，用来干吗呢？用来证明这个数学定理。</p>\n<p>所以说在早期的机器学习当中，在这个算法当中，我们看到的可能都是一些逻辑上的符号，看不到一个数字，可能也看不到一个加减乘除。整篇的证明你连个数字都找不着。</p>\n<p>但是今天呢，随着数据体量的庞大，我们获得数据越来越容易，所以机器学习的任务也相应的发生一些改变。我们关注的点不再是纯逻辑的一个处理。因为纯逻辑处理的应用范围有限，毕竟我们真实世界当中没有那么多理想的情况去给你处理。</p>\n<p>所以今天的机器学习，更多的解决的问题是什么呢？<strong>是在给定一些数据的情况之下来做出一定的判断，或者做出一定的推断</strong>。</p>\n<p>比方说，我根据这个星期或者这个月的这个股价，来判断一下明天我买的这个股票到底是涨还是跌，或者说我根据前一个月或者前两个月、前三个月这个彩票的出奖号码来确定下一期摇奖出的是什么号。当然这些问题其实并没有什么特别明显的规律在里面，我们可能更关注的是，能够解决的问题是有内在的规律的，不是完全的随机分布，不会完全是一个随机事件，而是要通过数据挖掘出数据内部的规律，再通过这些规律来指导我们下一步的判断，或者说指导下一步的决策。这是我们机器学习的一个主要任务。这个任务分解开来就包括了表示、评价，还有优化这样三个步骤。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/59/6b/59fc35f712f0ed62c03e3a76ef2f506b.png\" alt=\"\"></p>\n<p><strong><span class=\"orange\">三种数学工具之线性代数</span></strong></p>\n<p>在这三个步骤中，应用了三种不同的工具。在表示这个步骤当中，我们主要使用的工具是什么呢？就是线性代数。线性代数呢，我们在这个专栏里面也提到，它起到的一个最主要的作用就是把具体的事物转化成抽象的数学模型。<strong>不管你的世界当中有多么纷繁复杂，我们都可以把它转化成一个向量，或者一个矩阵的形式。这就是线性代数最主要的作用。</strong></p>\n<p>所以，在线性代数解决表示这个问题的过程中，我们主要包括这样两个部分：一方面是<strong>线性空间理论</strong>，也就是我们说的向量、矩阵、变换这样一些问题；第二个是<strong>矩阵分析</strong>。给定一个矩阵，我们可以对它做所谓的SVD分解，做奇异值分解，或者是做其他的一些分析。</p>\n<p>这样两个部分共同构成了我们机器学习当中所需要的线性代数，当然了，这两者也是各有侧重。线性空间的话，我们主要应用在一些解决理论问题当中，矩阵分析在理论当中有使用，在实践当中也有一些使用。</p>\n<p><strong><span class=\"orange\">三种数学工具之概率统计</span></strong></p>\n<p>我们说到，线性代数起作用是在表示的过程当中。在评价过程中，我们需要使用到概率统计。<strong>概率统计包括了两个方面，一方面是数理统计，另外一方面是概率论</strong>。</p>\n<p>数理统计好理解，我们机器学习当中应用的很多模型都是来源于数理统计。比方说最简单的线性回归，还有逻辑回归，它实际上都是来源于统计学。在具体地给定了目标函数之后，我们在实际地去评价这个目标函数的时候，我们会用到一些概率论。比方说给定了一个分布，我要求解这个目标函数的期望值。在平均意义上，这个目标函数能达到什么程度呢？这个时候就需要使用到概率论。所以说在评价这个过程中，我们会主要应用到概率统计的一些知识。</p>\n<p>实际上对于数理统计来说，我们在评价模型的时候，不只关注的是一个目标函数，我们可能还关注一些它的统计特性。比如说它的置信度，或者是其他的一些指标。你这个模型建立起来，它的可信性程度到底有多大，这些在早期的机器学习算法当中也是需要考虑的。当然随着神经网络，随着深度学习的兴起，这部分内容实际上渐渐地衰落，或者渐渐地被忽略。你在神经网络当中可能只需要达到一个这个好的目标函数，好的指标就行，至于说它的置信度，这些我们不去考虑。</p>\n<p>所以说，这也是深度学习不太受学数学，或者说学统计学的人待见的一个原因。因为统计学强调什么呢？强调可解释性，你这个模型能够达到什么样的指标，我们能把它清清楚楚地讲明白，为什么能够达到这样的指标，它的原理在哪？它背后的根据在哪？</p>\n<p>我给定一个分布，假如说高斯分布，那么再给定一个模型，我就可以通过严谨而简洁的这个数学推导，把这个结果以公式的形式给它呈现出来，这个看起来就很高大上，或者说很清楚。</p>\n<p>但神经网络和深度学习，现在还达不到这样可解释的程度。所以说现在也有人批评，说深度学习是炼金术，主要的原因在这里。我只能够通过调参数调出一个比较好的结果，但是到底这个结果为什么会出现？哪些因素会影响到它？可能还不是那么清晰。所以呢，关于概率统计，我们主要应用在评价这个过程中。</p>\n<p><strong><span class=\"orange\">三种数学工具之最优化理论</span></strong></p>\n<p>关于优化，就不用说了，我们肯定用到的是最优化理论。在最优化理论当中，主要的研究方向是<strong>凸优化</strong>。</p>\n<p>凸优化当然它有些限制，但它的好处是什么呢？能够简化这个问题的解。因为在优化当中我们都知道，我们要求的是一个最大值，或者是最小值，但实际当中我们可能会遇到一些局部的极大值，局部的极小值，还有鞍点这样的点。凸优化可以避免这个问题。在凸优化当中，极大值就是最大值，极小值也就是最小值。</p>\n<p>但在实际当中，尤其是引入了神经网络还有深度学习之后，凸优化的应用范围越来越窄，很多情况下它不再适用，所以这里面我们主要用到的是<strong>无约束优化</strong>。我在整个范围之内，我对参数，对输入并没有限定。在整个的输入范围内去求解，不设置额外的约束条件。</p>\n<p>同时，在神经网络当中应用最广的一个算法，一个优化方法，就是<strong>反向传播</strong>。应用反向传播，我们可以实现深度网络每一层的这个预训练，完成预训练之后，我们再对整体进行微调，能够达到最优的效果。所以说在优化这一块，我们应用到的最主要的就是最优化理论。</p>\n<p><strong><span class=\"orange\">三种数学工具和三个步骤并非一一对应</span></strong></p>\n<p>我们今天谈论这个机器学习当中，用到的基础数学都包括哪些呢？包括这三种，线性代数，概率统计，还有最优化理论。这是我们在机器学习当中用到的最基础的一些数学工具。如果大概做一个分类，分别对应到我们机器学习当中，表示、评价，还有优化这样三个步骤。</p>\n<p>当然，这种应用它也并不是说一一对应的关系。在表示当中我只用到线性代数，概率统计一点儿都不涉及，同样地，我在评价的时候，线性代数也不涉及，不是这样，都会有一个交叉的过程，但是在每个步骤当中应用到的主要工具还是有所区别。 </p>\n<p><strong><span class=\"orange\">高等数学是数学工具的基础</span></strong></p>\n<p>当然，在数学工具当中，我们并没有涉及到高等数学，高等数学我们就把它当作一个基础，一个基础中的基础。不光是人工智能，或者说机器学习，只要有数学参与的地方，我们都需要有高等数学这个基础。那么<strong>具体到机器学习当中，我们在高等数学这一块儿用到的比较多的，可能包括求导，微分，这样的一些内容。当然还有这个积分，我们在求解这个目标函数的期望值的时候可能也会遇到。</strong></p>\n<p>所以到这呢，我们就说，我们介绍了机器学习当中用到了哪些数学。主要就是这三块，线性代数，概率统计，还有最优化，那么任何复杂的算法实际上都是由这三者的结合叠加所构造出来的，那么这三者在机器学习当中他们起到的作用分别是什么呢？我们可以具体地来看一看。</p>\n<h2 id=\"-\">三种数学工具在机器学习中的作用</h2>\n<p><strong><span class=\"orange\">线性代数在机器学习中的作用</span></strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/54/6f/54f6a6a623eae083206e15ad9615b66f.png\" alt=\"\"></p>\n<p><strong>1.将具体事物抽象为数学对象</strong></p>\n<p>对于线性代数来说，我们可以对它做一个简单的定义。所谓线性代数是什么？就是数量和结构的一个组合，也就是说，线性代数等于数量加上结构。本身数量呢，它是一个单独的数。对于单个的数我们没有结构可言，对于单个的对象没有结构可言。但是当我们把一组数，或者一堆数排列到一块儿的时候，这个排列不是随机的排列，而是有一定的顺序进行排列的时候，这个时候，数目之间的顺序或者数量之间的顺序就形成了一种结构，这个结构就可以蕴含一定的信息，能够供我们去使用。</p>\n<p>除了顺序之外，结构还有另外一层含义。我可以对数量定义一些运算。在线性空间里面我们提到，基本的运算包括什么呢？包括加法，包括数乘，这样一些运算。有了运算之后，我们就可以对不同的对象，单个的数目放在一块儿，按照一定的顺序排列在一起，我们可以把它组成一个向量，组成这样一个对象。那么有了加法，数乘这样一些运算之后，你就可以对这个对象再来进行一些操作。这样的话，就实现了把具体事物给它抽象成数学对象，这样的一个过程。这就是线性代数最主要的一个作用。当然不光是在机器学习里面，在其他应用到线性代数的场合也是一样：把具体的事物抽象成为数学对象。</p>\n<p><strong>2.提升大规模运算的效率</strong></p>\n<p>当然除此之外呢，它还有另外一个优势，线性代数还有另外一个作用，就是能够提升大规模运算的效率。因为在现代的机器学习当中，我们要处理的数据都是海量的数据，数据的数量是呈指数形式的增长。我们要处理的数据越来越多，如果只是简单地说，用最传统的方法，用一个一个的for循环去处理高维的矩阵，它的效率肯定是相当低下。有了线性代数之后，我们可以把矩阵的运算引入到机器学习的算法当中，通过一些额外的库，或者一些额外的软件包，提升大规模运算的效率。这里面最直观的一个例子就是<strong>MATLAB软件</strong>。MATLAB软件本身名字叫矩阵实验室。它的特点，或者说它的卖点就在于，对矩阵，或者说对向量它操作的高效率。如果你在MATLAB软件当中编程的话，你用矩阵操作和用这个for循环实现，两者做一个对比，你会明显地感受到它的差异。</p>\n<p>总结一下，<strong>线性代数，它就等于数量和结构的组合。它的作用，一方面可以把具体的事物抽象成数学对象，另外一方面，可以提升大规模运算的效率。</strong></p>\n<p><strong>举例：人脸检测中的特征脸方法</strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/f6/b3/f66df104720f4fddd57b28d3fa932db3.png\" alt=\"\"></p>\n<p>这里面我也举一个例子，是人脸检测当中一个所谓的<strong>特征脸方法</strong>。特征脸的方法在人脸检测当中算是比较老的方法，在八九十年代就已经提出并且使用。而且相对于深度学习，或者基于神经网络的人脸检测的方法来说，它的效率，或者说准确率可能会稍微低一些，但是它比较能够说明线性代数的作用。</p>\n<p>这个特征脸的方法，本质是什么样的呢？当我们对人脸做一个判断的时候，我们可以把这个人脸做一个分解。根据人的面貌特征，可以把它的眼睛的特征拿出来，把鼻子的特征拿出来，把嘴的特征拿出来。这样的话，一张人脸就可以分布到眼睛、鼻子、嘴这样不同的器官维度之上。<strong>特征脸的方法采用的也是这样一个思路，只不过呢，它并不是按照眼睛、鼻子、嘴这种方法来处理，而是把一个人脸拆成了几个正交的、不同的图像，这里面每个图像实际上代表的都是一个正交基</strong>。</p>\n<p>我们这里给出一个例子。一个人脸，我们可以把它拆成三份，或者把它投影到一个三维空间之上。那么，这三个维度分别代表什么呢？每一个维度其实也是一个方方正正的图像，但这个图像你说它看起来是什么呢？有一个人脸的轮廓，但它并不是真正的人脸，而是对原始的一部分数据（当然不只是一张这个人脸的图像），可能对很多张图像做了一个<strong>主成分分析</strong>。做这个分析之后，我们得到的几个结果。</p>\n<p>这三张看似人脸，实际上并不是人脸的<strong>正交基</strong>，就构成了我们对人脸进行分解的一个基始。一张图片进来之后，这里面有个人脸，我们就可以把它投影到这三个基始之上，得到相应的系数。这样的话，就构成了我们说的特征脸的方法。</p>\n<p>所以说，特征脸的方法，它能够非常清晰地体现出来我们线性代数的思维，也就是把具体的事物抽象到一个线性空间之上，来进行求解。当然这个线性空间可能是我们平时所接触到的物理空间、二维空间，一横一竖两条线，三维空间再加上一个高度，也可能是更加抽象的空间。在抽象的空间之上，我们得到的就是这个看似人脸，实际上它有更清晰的物理意义的一个基始，这样的概念。</p>\n<p>当然关于这个特征脸方法具体的实现方式，如何做这个主成分分析，受时间所限，我们就不介绍具体的细节，感兴趣的话，大家可以自己了解一下相关的算法。这是线性代数，它主要的作用就是实现抽象化，当然还有一部分的效率因素。</p>\n<p><strong><span class=\"orange\">概率统计在机器学习中的作用</span></strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/3c/d7/3c15a335f973369fda283d0a9b1106d7.png\" alt=\"\"></p>\n<p><strong>1.利用数据学习模型</strong></p>\n<p>如果我们说，线性代数可以看成是数量还有结构的组合的话，那么概率统计就可以看成是模型还有数据的组合。那么模型和数据组合在一块，实际上是双向的处理。</p>\n<p>我们机器学习有学习的阶段，我们要利用这个数据去训练这个模型。这个阶段，我们是用数据去学习这个模型。在模型里面，我们就可以去选择。有那么多的模型，像我们刚才说到的，有K-近邻的模型，有回归模型，有决策树，还有支持向量机，这样不同的模型。</p>\n<p>我训练的任务就是用数据来学习这些模型，来确定这个模型的参数，最终得到一个确定的模型。这就可以看成什么呢？看成是在给定数据的情况下，我来求解这个参数，它的条件概率。给定的数据，如果有一部分参数的条件概率是最大的，那么就选择这部分参数，作为我这个模型的参数。实际上，训练过程解决的就是这样一个问题。</p>\n<p>当然具体来说，包括生成模型，包括判别模型，那么生成模型我们求解的是输入输出的一个联合概率分布，那么判别模型是一个条件概率分布。但不管怎么样，很多情况下，我们关注的目标都是分布，那么利用数据进行训练的过程也就是学习这个分布的过程。</p>\n<p><strong>2.利用模型推断数据</strong></p>\n<p>接下来呢，在训练结束之后，我们要这个模型要来干什么呢？要进行预测，也就是说，利用这个模型来进行数据的推断。</p>\n<p>给定这个模型，我给到一个输入，我输入可能是一个特征，一些特征的组合，形成一个向量。我把这个输入的向量代入到模型当中，就可以求出一个结果，当然也可能是多个结果。我取这个概率最大的结果作为一个输出，这个过程就是反过来利用模型去推断数据的一个过程。</p>\n<p>所以我们说，<strong>概率统计等于模型和数据的一个组合，这个组合是双向的。在学习阶段，我们利用数据来训练模型，在预测阶段，我们利用模型反过来去推断这个数据</strong>。  </p>\n<p>所以，在概率统计这一块，我们关注的是模型的使用，还有概率的求解。当然两者不是完全区别开的，是相互融合的。在建立模型的时候，我们会利用到一些先验概率分布。在求解目标函数的时候，我们也会涉及求解数学期望这样一些操作。</p>\n<p>这里面我们也给出了一个实例，就是回归分析还有机器学习方法的比较。</p>\n<p><strong>举例：回归分析与机器学习方法</strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/44/05/4452fceb5ff68244c6b056541294cc05.png\" alt=\"\"></p>\n<p>在介绍线性回归的时候，我们提到了最小均方误差的概念。最小均方误差等于什么呢？它等于y<sub>i</sub>-f(x<sub>i</sub>)，两者的差值代表了我的预测值和真实值之间的距离。当然这个差值可正可负，如果一个正的误差加上一个负的误差，它们两者的结果可能接近于0，但实际上这两个误差都存在的，并不是说它不存在。所以在这里我们再加一个平方，这样，就能够定量的度量它的误差幅度，而不会受到符号的影响，这就得到了一个标准的均方误差的表达式，也就是y-f(x)，这整个做一个平方。</p>\n<p>在此基础上，我们可以对它做一些扩展。比方说对于这个[y-f(x)]^2，我在前面给它乘一个常数K。我们知道，乘一个常数K并不会影响到这个函数的最大值或者最小值。不管乘与不乘，在原来取到最大值的地方依然能取到最大值，原来能取到最小值的地方依然取到最小值，不会影响它极值的分布。那么引入这样一个常数以后，我们还可以引入另外一个常数，就是在后面再加上一个常数C。这个常数和前面那个常数K一样，都不会影响到最小值和最大值的分布。你该在哪取到最小值还是在哪取到最小值。对它做一个改造，我们就可以得到一个新的均方误差表示式。</p>\n<p>有了这样一个扩展的模型之后，我们就可以对它做一个改变，或者说对它做一个修正。首先呢，对于这个常数K我们在设定的时候，认为它是一个常数。那么如果我们把它改成一个变量呢？不再把它取成一个常数，而是一个和X相关的变量，这样的改造得到的就是所谓的<strong>核方法</strong>。</p>\n<p>在原始的[y-f(x)]^2的基础上，我给它乘以一个系数，这样的话，相当于给原始的每一项，每一个X，每个不同的X，它的均方误差加上了一个权重，这个就是核方法。但在支持向量机里面我们也提到，核方法的作用是，把低维空间构造成高维空间，把原始的数据从低维空间映射到高维空间之上。这样的话，低维的非线性问题可能就变成了一个高维的线性问题。但是它最基本的操作，或者说最基本的原理是在这里。</p>\n<p>我们对原始的均方误差给它做一个修饰，那么最常见的这个高斯和实际上就定义了，它定义的是什么呢？这个参数和我关注的点到这个数据点的距离相关，假如说我关注的点离这个数据点越近，可能它的权重就越大，那么离数据点越远呢，它权重就越小。相当于根据距离施加一个不同的权重，这是核方法它的一个思路。</p>\n<p>在核方法当中，我们改变的是这个常数K。同样地，我们也可以对另外一个常数C做一个处理，加入一个和这个模型相关的项，那么这个项不知道会不会有一种似曾相识的感觉。没错，实际上，它就可以看成是这个<strong>正则化项</strong>。</p>\n<p>那么我们添加正则化项，或者是惩罚项的这个方法，也就对应了所谓正则化的方法。在线性回归当中，如果你加到一个二阶的正则化项，二范数的正则化项，我对这个模型的二范数做一个限定，那么得到的就是所谓的<strong>岭回归</strong>，如果对一范数做一个限定，得到就是所谓的<strong>Lasso回归</strong>。</p>\n<p>当然除了这个模型的范数之外，我们也可以对模型的求导进行一个约束。如果对模型的二阶导做一个约束的话，实际上我们得到的就是一个线性的模型，一阶导得到的就是一个常数的模型。但不管怎么样，它代表的都是正则化的一个思路，我通过添加正则化项去对这个模型做出一个控制。</p>\n<p>在这个扩展的模型当中，我们改变了常数K，得到了核方法，改变了常数C得到了正则化项，当然了也可以对原始的f(x)，这个模型做一个处理，把它写成一组函数的组合形式，那么这里得到的就是<strong>基函数</strong>的形式。</p>\n<p>比方说我可以把这个函数X，函数X^2，\n它不同的指数项分别作为一个基始，那么把这个基始每一组基，应该说每个基始赋予一个系数，那么得到的就是ax+bx^2，那么做出的这样一个拓展之后，得到的就是一个基函数的这样一个更加复杂的模型。</p>\n<p>所以说在这个回归分析，在这个机器学习，我们可以看到方法有很多种，但是归根到底都可以看成是最基本的线性回归方法的一个扩展，我们对线性回归方法做各种各样的改进，可以得到不同的方法。  </p>\n<p>其实深度学习也是一样，我们也可以从统计的角度来去观察，去解释它。所以我们说，这就是概率统计在机器学习当中的一个作用，因为统计的一部分核心内容就是回归分析。那么在机器学习当中建立模型的时候，我们很大程度会应用到这个模型，那么在具体计算的时候就会涉及到各种各样的概率，比如说条件概率、联合概率，还有信息熵，或者是KL散度、交叉熵，这样一些由概率导出来的信息论里的一些性能指标，这都是我们会用的。</p>\n<p><strong><span class=\"orange\">最优化理论在机器学习中的作用</span></strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/73/19/73d5325998e74936fdd4938d4a79e119.png\" alt=\"\"></p>\n<p>概率统计呢，我们可以把它解释成这个模型和数据的一个组合，那么最优化的话，就可以看成是目标和约束的一个组合。</p>\n<p>在这里面，我们最优化的目标是什么呢？是求解，让这个期望函数，或者让目标函数取到最值的解。手段是什么呢？就是通过调整模型的参数来实现。</p>\n<p>为什么要调整这个模型的参数？因为很多时候，我们想求解到这个解析解是求不出来的。在很多复杂的问题当中呢，这个解析解是没有办法求出来的。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/f3/3c/f3c49841be9370e375c2526821a2763c.png\" alt=\"\"></p>\n<p>对于线性回归来说，我们可以求解出Beta的一个表达式，那样一个矩阵相乘，求逆，再进行相乘的一个表达式。很多时候，这个解析解我们求不到，求不到怎么办？就只能一点一点去试，一步一步去找，我要的最小值或者最大值，它到底在哪？这个时候就会用到我们最优化的方法，包括<strong>梯度下降</strong>，包括其他的一些方法。</p>\n<p>在使用这些方法的时候，我们要注意调整一些参数。<strong>一方面是模型的参数，另外一方面还有所谓的超参数</strong>。调整模型参数，一方面，它的作用让我们找到真正的最小值，或者找到真正的最大值。另外一方面，避免在寻找的过程中把最小值，或者最大值，本来你是能找到的，但是这个超参数没有设计好，比如说我的步长、速率没有设计好，把这个点错过，要避免这样一些问题。所以说对于最优化而言，我们可以把它看成是目标，还有参数的一个组合，通过这两者来找到我们想要的合适的点。</p>\n<h2 id=\"-\">这些数学基础需要掌握到什么程度？</h2>\n<p><img src=\"https://static001.geekbang.org/resource/image/36/b0/36404f72d98e817a10920679aa48b5b0.png\" alt=\"\"></p>\n<p>刚才呢，我们结合这些实例，解释了线性代数，概率论，概率学统计，还有最优化，在机器学习当中的一些作用。接下来我们来看一看，需要掌握到什么程度。需要掌握到什么程度呢？实际上，应该说是一个见仁见智的问题。当然理想的情况肯定是掌握得越多越好，最好你能把所有的数学都掌握到，不光是我们提到的这些，甚至更加高级的你都会，这是最好的效果。当然在实际当中，我们不可能，没有那么多精力去专门地钻研到这个数学当中，所以说这种理想的情况也是不存在的。那么具体来说，掌握到什么程度呢？</p>\n<p><strong><span class=\"orange\">三重境界之能使用</span></strong></p>\n<p>我在这里列出来了三个阶段。第一个阶段呢，我管它叫做能使用。也就是说，<strong>给定一个模型，我能够用它来根据给定的输入来求解输出，也就是利用已知的方法来解决问题</strong>。那么这个已知的方法，我可以把它看成一个黑箱子，我不关注这个过程，不关注这个方法是如何解决问题，只要能够解决问题就行。可能已经有了一个算法，那么我只需要对数据做一些处理，把这个数据送入到算法当中，得到一个输出，我能看明白这个输出是怎么回事，这就可以。这是能使用的阶段，我只是做一个算法的使用者，我能把它用清楚就够了。</p>\n<p><strong><span class=\"orange\">三重境界之能看懂</span></strong></p>\n<p>如果在能使用的基础上再进一步，那么就是能看懂，<strong>我不光用这个已知的方法来解决问题，同时我还能够理解这个方法的工作原理。知其然，还能知其所以然</strong>。能使用就是知其然，能看懂就是知其所以然。那么这个方法可能背后有一些数学推导，会涉及到一些概率，最优化，还有线性代数的一些使用。那么这个能看懂，就要求你具备相关的知识，能够把这个推导的过程给它顺下来，知道这个方法具体是怎么来工作。</p>\n<p><strong><span class=\"orange\">三重境界之能设计</span></strong></p>\n<p>如果在这个能看懂的基础上，再进一步的话，我们可以把它叫做能设计。<strong>我把已知方法理解之后，我还可以根据我的问题，根据我自己的实际问题的特点，来开发一些新的方法</strong>。要么呢，<strong>可以对已知的方法我来做一些改进</strong>，使它更符合我自己的一个待解决问题的方法，或者说我开发一个完全新的方法，就是重新推导，推倒重来，直接设计一个新的方法。那么很显然，这个呢，对于数学功底就有更深层次的一个要求。</p>\n<p>所以我们说对于数学的掌握程度包括这样的三个层次，能使用，能看懂，还能设计。那么具体在实际当中，你需要做到哪个程度，那么就要根据自己的实际情况来做出判断。 </p>\n<p><strong><span class=\"orange\">如何判断应该掌握到哪种深度？</span></strong></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d3/8e/d353200e16bb6a07671e3a65c22e208e.png\" alt=\"\"></p>\n<p>当然在这个判断当中呢，我们要给大家说一个问题，就是深度。如何来进行选择，如果来进行选择呢？我觉得应该有一条准则，这条准则是什么呢？就是<strong>数学是手段</strong>。</p>\n<p>在机器学习当中，我们的目的并不是说研究多深的数学，而是用这个数学去解决我实际的问题。同样的道理，我们在从事相应的工作的时候，<strong>学数学也是个手段</strong>。</p>\n<p><strong>我们并不是说为了学习数学而去学数学，而是为了真正的去解决实际的问题来去学习这些数学。</strong>因为我相信可能咱们专栏的读者朋友，大多数都是已经入职，或者说已经工作岗位上，这样的话，我们就不可能像学生这样有大把的自由时间，有大把的精力去放在这个数学学习当中，因为我们要工作，我们能够自由支配的时间是有限的，所以我们要把时间放到合理的地方去，拿这个时间去解决关键的问题。</p>\n<p>第二点，我们学习数学也并不是说成为一个数学家，或者成为一个科学家，我们学它的目的也是为了解决我们实际当中的问题，所以说呢，在这种情况之下，你对学习的深度，就要有一个清晰的把控。学到什么程度可以呢？这个我自己的观点是，<strong>只要足够解决问题就够了</strong>。至于说再深入，如果你有余力可以继续去深入，如果没有余力也可以等到以后我再需要深入的时候，再来进行相应的学习，这是我个人的观点。这个见仁见智，大家可以根据自己的情况来进行参考。</p>\n<h2 id=\"-\">如何尽快、高效率掌握数学知识？</h2>\n<p><img src=\"https://static001.geekbang.org/resource/image/ab/90/aba8d67dc741a1a24b0b6807e8338290.png\" alt=\"\"></p>\n<p><strong><span class=\"orange\">掌握核心概念</span></strong></p>\n<p>在这方面，我给出的建议是，一方面是，我们要握核心概念，在线性代数当中核心概念是什么？就是线性空间，向量矩阵以及对于向量矩阵的度量，包括范数、包括内积这些，这些就是它的核心概念。那么在概率统计当中，频率学派，还有贝叶斯学派，他们两者之间的区别是一个核心概念，同时呢，像期望方差这些指标，还有条件概率，这样的一些概念，条件概率联合概率这样一些概念也是核心概念。那么在最优化当中，这些算法，这个梯度下降法，或者牛顿法，这就是核心概念。</p>\n<p><strong><span class=\"orange\">以点带面</span></strong></p>\n<p>在时间有限的情况下，我们一定要把有限的精力集中在重要的知识上。先把这些核心概念搞清楚，再通过这些核心的概念，来以点代面，从这些关键的问题去铺开，慢慢地去接触其他的问题。</p>\n<p><strong><span class=\"orange\">问题导向</span></strong></p>\n<p>最后一点呢，我觉得，在学习的时候，我们可以以问题为导向，就是结合着我们实际的需求，结合我们实际的问题，来决定我们去学什么。这个呢，和我们前面所说到的这个掌握到什么程度也是一样，掌握到什么程度也是相通的。因为毕竟我们学习，机器学习，学习机器学习当中的数学都是为了解决问题。如果不能解决问题的话，你学到的这个东西的价值就没有能够解决问题的这个知识的价值大。当然我们也不能说一点价值都没有。在学习的时候，大家可以尝试着以问题为导向。带着问题去探索这些知识，带着问题去学习知识，可能你会发现，这样会得到更高的效率。</p>\n<h2 id=\"-\">推荐书目</h2>\n<p>推荐书目在我们专栏里面也有相应的推送。但是在这里，我想要跟大家推荐的两本书，都是关于机器学习，或者说都是关于统计学习，一本叫<em>An Introduction Statistical to Learning</em>，另一本叫<em>The Elements of Statistical Learning</em>。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/2b/1a/2be9d9453deabc2b26af0214c9016e1a.png\" alt=\"\"></p>\n<p>两本书的作者是同一拨人，有两个共同的作者。它们讲述的都是统计学习，或者机器学习的方法。其中前一本可以看成是后一本的简化版。它更通俗、更简单一些，后面这个稍微有些难。为什么推荐这两本呢？因为<strong>这两本，它会更深入地挖掘方法背后的一些数学含义</strong>。我们能够看到的教科书，它主要的作用是把这个方法讲清楚。来了个方法，比如说线性回归，怎么推导，怎么区算，一大堆公式写出来，那么给出一个推导的结果，或者说决策树，或者其他的算法都一样，可能到这里就结束了，我的任务就完成了。</p>\n<p>这两本书，它不光是介绍方法推导，它可能更看中统计学上的意义。我们为什么要这么做，我们设计这个方法在统计学上有什么意义。因为作者都是统计学家，有几十年的积淀，有几十年的研究积淀，所以，相比于其他市面上教科书，我认为他们两个insight可能更多一些，大家看起来，也会有相应的难度。可能并不是说，像我们平时的教科书那么简单。这些书，我们一遍两遍是读不完的，如果真正要从事人工智能、机器学习这方面的工作的话，可能要多读几遍，每读一遍也会有不同的收获。</p>\n<h2 id=\"q-a\">Q&amp;A</h2>\n<p><strong>Q：机器学习算法与传统算法学习之间有什么样的关联？</strong>\nA：我觉得机器学习，或者说统计学习，它可能是一个偏数学的算法。我们偏推导出一个论证，如何用我刚达到最小的误差，或者说达到最小的目标函数，或者最大的目标函数这些。那么传统的算法，计算机的算法，我理解计算机的算法，可能更关注一些复杂度、时间复杂度，或者是运算复杂度，我能在什么样的时间之内去把这个问题解决，这两者是主要的区别。一个偏重于计算机，一个偏重于数学，计算机的算法是机器学习算法的基础，机器学习算法是在纸面上的，如果真的把它实现的话，可能要依赖于计算机的算法。</p>\n<p><strong>Q：人工智能可能是处于一个技术的上升期，大家都比较关注，上升期有什么走向，能不能谈一下您的观点？</strong></p>\n<p>A：我觉得它的走向就是在各行各业落地。人工智能本身是个方法，或者说它是个手段，它的目的是真正解决实际问题。我如何在实际当中，我在各行各业应用的时候，如何用它来提升我的效率，降低它的成本，这是一个方向。这个未来发展的方向，如果能落地，这个技术，或者这个学科就是一个有生命力的技术，一个真正有生命力的学科。如果在实际的场景下找不到它的业务，那么就会被淘汰。这也是刚才我所说的，大家在学习人工智能时候以问题为导向，看它能真正解决我们行业当中，或者工作当中哪些问题，从这个角度去出发，去学习，这样能带来更高的效率，同时也符合这个技术，或者这个学科的发展方向。</p>\n<p><strong>Q：老师有没有开第二期专栏的计划？</strong></p>\n<p>A：感谢这位朋友的关注。第二期专栏的计划我们是有的，而且第二期的内容是更加侧重于实例。在第一期的时候，我们的名称叫《人工智能基础课》。大家也看到了，会以理论为主，以介绍算法的原理为主。那么在第二期，我们可能把更多的焦点放在实际应用上。就是当你理解了一个算法之后，我如何去把它用到实际当中，去解决这个实际的问题。这是我们第二期的一个重点内容。 </p>\n<p></p>\n","neighbors":{"left":{"article_title":"推荐阅读 | 我与人工智能的故事","id":4167},"right":{"article_title":"第2季回归 | 这次我们来聊聊机器学习","id":8642}}},{"article_id":8642,"article_title":"第2季回归 | 这次我们来聊聊机器学习","article_content":"<p>你好，我是王天一，好久不见。</p>\n<p>我想告诉你个好消息，我的新专栏“<span class=\"orange\">机器学习40讲</span>”终于可以和你见面了！</p>\n<p>首先，要谢谢你的一路陪伴，咱们“人工智能基础课”这个专栏，从去年12月5日上线以来，累计订阅5000+，可以说，是你的支持让我有了写新专栏的动力。</p>\n<p>基础课是学习人工智能的入门第一课，相当于给了你一张人工智能的地图，我希望你可以按图索骥，一点点摸清楚人工智能的大概轮廓，找到学习的方向。</p>\n<p>但是人工智能领域的内容浩如烟海，40期的基础课也基本上是浅尝辄止，只能算是带着你品尝了一下味道。要想继续在人工智能领域深耕，就要沿着人工智能的学习路径，继续打好基础，而人工智能里最重要的基础一定是机器学习。</p>\n<p>近年来深度学习很火热，特别是CNN、RNN等深度学习模型都取得了很好的效果，获得了极大的关注。但是深度学习的很多模型、算法其实都是根植于机器学习的。对机器学习没有深入学习和深刻理解的话，也很难真正掌握深度学习的精髓。毫无疑问，<strong>机器学习依然占据着人工智能的核心地位，也是人工智能中发展最快的分支之一</strong>。</p>\n<p>在这个新专栏中，我会从机器学习中的共性问题讲起，从<strong>统计机器学习</strong>和<strong>概率图模型</strong>两个角度，详细解读一系列最流行的机器学习模型。除了理论之外，在每个模型的介绍中还会穿插一些基于Python语言的简单实例，帮你加强对于模型的理解。</p>\n<p>“机器学习40讲”专栏共40期，分为3大模块。</p>\n<p><strong>机器学习概观</strong>。这一模块将从频率学派与贝叶斯学派这两个视角来看机器学习，并讨论超脱于模型和方法之外的一些共性问题，包括模型的分类方式、设计准则、评估指标等。</p>\n<p><strong>统计机器学习模型</strong>。这一模块将以线性模型为主线，讨论模型的多种扩展和修正，如正则化、线性降维、核方法、基函数变化、随机森林等，探究从简单线性回归到复杂深度网络的发展历程。</p>\n<p><strong>概率图模型</strong>。这一模块将以高斯分布为起点，将高斯分布应用到从简单到复杂的图模型中，由此认识不同的模型特性与不同的计算技巧，如朴素贝叶斯、高斯混合模型、马尔科夫随机场等。</p>\n<p>专栏详细目录如下：</p>\n<p>  <img src=\"https://static001.geekbang.org/resource/image/5d/78/5d51a2322e9cf8bce991fdfbb952da78.jpg\" alt=\"\"></p>\n<p>希望可以和你一起，在人工智能的道路上继续探索。</p>\n<p>对了，专栏的运营同学还给你发了<strong>10元专属优惠券</strong>，优惠券可以和限时特价同享，也就是只用35元就能买到原价68元的“机器学习40讲”，但是优惠券有效期仅<strong>4天</strong>，所以提醒你抓紧使用。</p>\n<p>我们新专栏见！</p>\n<!-- [[[read_end]]] -->\n<p><a href=\"https://time.geekbang.org/column/intro/97?utm_source=app&amp;utm_medium=62&amp;utm_campaign=97-presell&amp;utm_content=new-article\"><img src=\"https://static001.geekbang.org/resource/image/35/18/355b6c6ffaec827191b09dfdcecf8618.jpg\" alt=\"\"></a></p>\n","neighbors":{"left":{"article_title":"直播回顾 | 机器学习必备的数学基础","id":4955},"right":{"article_title":"新书 | 《裂变：秒懂人工智能的基础课》","id":9118}}},{"article_id":9118,"article_title":"新书 | 《裂变：秒懂人工智能的基础课》","article_content":"<p>你好，我是王天一，今天发一个彩蛋：我的新书终于跟你见面了！</p>\n<p>这本书脱胎于我们的专栏，书名是《裂变：秒懂人工智能的基础课》。</p>\n<p>我们的专栏共40期，在三个多月的时间里，我和你一起学习了人工智能领域最关键的基础知识，包括数学基础、机器学习、神经网络、深度学习以及深度学习之外的人工智能，还讨论了人工智能在视觉、语音、对话系统和机器翻译领域中的应用。</p>\n<p>这三个多月里，有五千多位同学参与到我们的基础课当中，给予了很多高质量的反馈，因此这本书其实是我们共同创作的成果。</p>\n<p>书的框架和专栏的模块保持一致，在此基础上补充了参考文献，对排版也做了调整，以期给你最佳的阅读体验。希望这本书能够完成这样一个使命：<strong>复习专栏内容，打造人工智能的知识架构</strong>。</p>\n<p>我们的课程是基础课，通过最低程度使用繁杂的公式来让内容通俗易懂。但这可不代表内容简单，恰恰相反，专栏的每一个模块都有其深度，需要反复复习，才能达到通透掌握的程度。</p>\n<p>这里我引用老师木给本书写的一段推荐语：“读者可以把这本书当成一个索引去了解人工智能的基本原理，若仍无法满足好奇心的话，再去原始文献中找到细节，甚至找到源代码和数据集实践，这是一个由薄到厚的过程；当对磅礴繁杂的内容融会贯通之后，发现机器学习的要义已被这一本薄书涵盖了，真是奇妙无比。”</p>\n<p>这本书就是一份简历、一份提纲，勾勒出人工智能的基本框架，是一个“薄”的开始，引领你走过一个“厚”的过程，再回来后，就一定会有更高远的视野和更深刻的洞见。那个时候，你一定会对老师木所说的“奇妙无比”心领神会！</p>\n<p>所以，你完全可以把这本书当作这门课程的课本，复习的时候，配合音频，根据自己的掌握情况，有针对性地练习，这也是现在大家都熟悉的“刻意练习”的关键。要知道，刻意练习不是简单的重复，而是针对自己似懂非懂、还没完全掌握的内容，刻意去训练提升自己。</p>\n<p>最后，作为对这个专栏最好的沉淀，欢迎你阅读这本书，我们共同努力的结晶！</p>\n<p><span class=\"orange\">6月13~15日首发，三天限时优惠。《裂变：秒懂人工智能的基础课》新书，原价￥59，现价￥49！你可以从发现页进入极客商城后进行购买。</span></p>\n<p><span class=\"orange\">体贴的运营同学还给你发了短信，可以领取5元礼券（与优惠价同享），有效期到6月30日，提醒你抓紧使用。</span></p>\n<!-- [[[read_end]]] -->\n<p>  <img src=\"https://static001.geekbang.org/resource/image/4a/a1/4acbebba2566be583d0254659ef68fa1.jpg\" alt=\"\"></p>\n","neighbors":{"left":{"article_title":"第2季回归 | 这次我们来聊聊机器学习","id":8642},"right":{"article_title":"一键到达 | 数学基础复习课","id":4647}}},{"article_id":4647,"article_title":"一键到达 | 数学基础复习课","article_content":"<p>任何学习都是一个过程，需要不断复习，以强化理解。</p>\n<p>从今天开始，我会带你复习专栏的内容。按照专栏的七个模块，我为你把每个模块里发布过的文章要点卡都梳理出来，你可以用来回顾每一篇的内容。点击要点卡，一键到达你最需要温故的那一篇。</p>\n<p>今天来复习专栏的第一部分内容，<span class=\"orange\">数学基础</span>。在这个模块里，我们一起学习了线性代数、概率论、数理统计、最优化方法、信息论和形式逻辑这几个内容。</p>\n<p>复习的过程中，还有什么困惑，获得了什么收获，都欢迎你继续给我留言。</p>\n<p><a href=\"https://time.geekbang.org/column/article/1340\"><img src=\"https://static001.geekbang.org/resource/image/e4/a2/e4111df16317c6c9a400ed9494c2f8a2.jpg\" alt=\"\" /></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1341\"><img src=\"https://static001.geekbang.org/resource/image/60/87/60dfa8c61a5847be616f08d18b36a587.jpg\" alt=\"\" /></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1498\"><img src=\"https://static001.geekbang.org/resource/image/55/6d/553fd5c4498ba56b75a15fc99770dc6d.jpg\" alt=\"\" /></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1507\"><img src=\"https://static001.geekbang.org/resource/image/13/8e/13a4991f9bc5b7c3717f47ea28b4d18e.jpg\" alt=\"\" /></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1573\"><img src=\"https://static001.geekbang.org/resource/image/e2/e5/e248d05acca0ac225b043a775bb221e5.jpg\" alt=\"\" /></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1639\"><img src=\"https://static001.geekbang.org/resource/image/d4/59/d4e00273015088b3d813ddb59fc4f659.jpg\" alt=\"\" /></a></p>\n<!-- [[[read_end]]] -->\n<p><a href=\"https://time.geekbang.org/column/article/1807\"><img src=\"https://static001.geekbang.org/resource/image/78/81/7828bdf7ac66aff3898fd038ae790381.jpg\" alt=\"\" /></a></p>\n","neighbors":{"left":{"article_title":"新书 | 《裂变：秒懂人工智能的基础课》","id":9118},"right":{"article_title":"一键到达 | 机器学习复习课","id":4672}}},{"article_id":4672,"article_title":"一键到达 | 机器学习复习课","article_content":"<p>今天我们来复习专栏的第二部分内容，<span class=\"orange\">机器学习</span>。在这个模块里，我们一起学习了线性回归、逻辑回归、决策树、朴素贝叶斯以及支持向量机等内容。</p>\n<p><span class=\"reference\">点击要点卡，直达你最需要复习的那一篇。</span></p>\n<p><a href=\"https://time.geekbang.org/column/article/1669\"><img src=\"https://static001.geekbang.org/resource/image/48/4c/4877cc0a3c4a5690364a24a35862324c.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1865\"><img src=\"https://static001.geekbang.org/resource/image/c2/3d/c213a86d22def0da9a92fe3092605f3d.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1866\"><img src=\"https://static001.geekbang.org/resource/image/1d/ab/1d23a0935e1e853e21a0d6a0dab9e4ab.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/1867\"><img src=\"https://static001.geekbang.org/resource/image/d8/aa/d81794d22373b75dd79da8655adacdaa.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2008\"><img src=\"https://static001.geekbang.org/resource/image/1e/d9/1e291ba6ca4b799c186f5faf0d084dd9.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2028\"><img src=\"https://static001.geekbang.org/resource/image/e0/52/e0dccef2f1529f49b9c981ec2a3d4352.jpg\" alt=\"\"></a></p>\n<!-- [[[read_end]]] -->\n<p><a href=\"https://time.geekbang.org/column/article/2030\"><img src=\"https://static001.geekbang.org/resource/image/f9/06/f9cb9acf82ea46e7c07df95c18602a06.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2196\"><img src=\"https://static001.geekbang.org/resource/image/be/6f/be9208083ca3c520e1c530efd3b4dd6f.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2197\"><img src=\"https://static001.geekbang.org/resource/image/66/23/669d6a62837e1d733668767e254f3923.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2113\"><img src=\"https://static001.geekbang.org/resource/image/3b/a4/3b20b7273943ac7dd29602f4d02b18a4.jpg\" alt=\"\"></a></p>\n<p></p>\n","neighbors":{"left":{"article_title":"一键到达 | 数学基础复习课","id":4647},"right":{"article_title":"一键到达 | 人工神经网络复习课","id":4719}}},{"article_id":4719,"article_title":"一键到达 | 人工神经网络复习课","article_content":"<p>今天我们来复习专栏的第三部分内容，<span class=\"orange\">人工神经网络</span>。在这个模块里，我们一起学习了神经元、感知器、多层感知器、自组织特征映射、模糊神经网络等内容。</p>\n<p><span class=\"reference\">点击要点卡，直达你最需要复习的那一篇。</span></p>\n<p><a href=\"https://time.geekbang.org/column/article/2208\"><img src=\"https://static001.geekbang.org/resource/image/eb/30/eb0908de1ebf5914ced2fa63fcf34c30.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2444\"><img src=\"https://static001.geekbang.org/resource/image/61/15/61cedd706a18b0b7b07697c6443f4715.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2446\"><img src=\"https://static001.geekbang.org/resource/image/b8/6e/b885f71e6bef6d5c6d8ae96e73fa2a6e.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2447\"><img src=\"https://static001.geekbang.org/resource/image/12/1e/12b28b058ec981788aabe18881c5781e.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2643\"><img src=\"https://static001.geekbang.org/resource/image/98/2b/98778ce46a430c847021f437eefdb62b.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2646\"><img src=\"https://static001.geekbang.org/resource/image/5d/e1/5d8c880ee0e5dd330df08e9db32558e1.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2655\"><img src=\"https://static001.geekbang.org/resource/image/ac/1a/ac486b5ea731cdb8be87823e15c5931a.jpg\" alt=\"\"></a></p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"一键到达 | 机器学习复习课","id":4672},"right":{"article_title":"一键到达 | 深度学习复习课","id":4721}}},{"article_id":4721,"article_title":"一键到达 | 深度学习复习课","article_content":"<p>今天我们来复习专栏的第四部分内容，<span class=\"orange\">深度学习</span>。在这个模块里，我们一起学习了深度前馈网络、正则化、自编码器、深度强化学习等内容。</p>\n<p><span class=\"reference\">点击要点卡，直达你最需要复习的那一篇。</span></p>\n<p><a href=\"https://time.geekbang.org/column/article/2656\"><img src=\"https://static001.geekbang.org/resource/image/47/fd/47ee8f04b5cad11d9b3c03ec2b2ccbfd.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2874\"><img src=\"https://static001.geekbang.org/resource/image/06/c1/069de9897e302f8f9dc372b7459a0ec1.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2875\"><img src=\"https://static001.geekbang.org/resource/image/9f/2d/9fa7cbe21ce4e114a2712ed2d9508b2d.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/2876\"><img src=\"https://static001.geekbang.org/resource/image/08/5d/0839aae2ab1f3c4bfb630369d843c65d.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3287\"><img src=\"https://static001.geekbang.org/resource/image/5a/0d/5ae922d7af41c821bb06cb95d10ee50d.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3429\"><img src=\"https://static001.geekbang.org/resource/image/7c/98/7cfffe5b9642991df0847f3650492d98.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3223\"><img src=\"https://static001.geekbang.org/resource/image/c3/36/c3ced74533172a1093c23274d3123b36.jpg\" alt=\"\"></a></p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"一键到达 | 人工神经网络复习课","id":4719},"right":{"article_title":"一键到达 | 深度学习框架下的神经网络复习课","id":4724}}},{"article_id":4724,"article_title":"一键到达 | 深度学习框架下的神经网络复习课","article_content":"<p>今天我们来复习专栏的第五部分内容，<span class=\"orange\">深度学习框架下的神经网络</span>。在这个模块里，我们一起学习了深度信念网络、卷积神经网络、循环神经网络、长短期记忆网络等内容。</p>\n<p><span class=\"reference\">点击要点卡，直达你最需要复习的那篇。</span></p>\n<p><a href=\"https://time.geekbang.org/column/article/3431\"><img src=\"https://static001.geekbang.org/resource/image/6e/45/6ee015991274b820f056695c8b5f9e45.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3638\"><img src=\"https://static001.geekbang.org/resource/image/4d/e7/4dce2701152a658ff621948a3ed26ce7.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3639\"><img src=\"https://static001.geekbang.org/resource/image/3b/05/3b35d656105e4d355b968f7f292d9a05.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3643\"><img src=\"https://static001.geekbang.org/resource/image/3f/7f/3f505cb5fd0b5f18eece1522718a707f.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3644\"><img src=\"https://static001.geekbang.org/resource/image/2e/14/2e463cd67177ecafb547c36d65524a14.jpg\" alt=\"\"></a></p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"一键到达 | 深度学习复习课","id":4721},"right":{"article_title":"一键到达 | 深度学习之外的人工智能复习课","id":4725}}},{"article_id":4725,"article_title":"一键到达 | 深度学习之外的人工智能复习课","article_content":"<p>今天我们来复习专栏的第六部分内容，<span class=\"orange\">深度学习之外的人工智能</span>。在这个模块里，我们一起学习了概率图、知识图谱、迁移学习等内容。</p>\n<p><span class=\"reference\">点击要点卡，直达你最需要复习的那篇。</span></p>\n<p><a href=\"https://time.geekbang.org/column/article/3646\"><img src=\"https://static001.geekbang.org/resource/image/57/48/57eda304a18b35999beaadbfc1c32348.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/3649\"><img src=\"https://static001.geekbang.org/resource/image/e7/51/e7151984e06f3ee537179af1cb7a1d51.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/4009\"><img src=\"https://static001.geekbang.org/resource/image/a3/bb/a331dead77d7e3e1d9f1939ed38534bb.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/4010\"><img src=\"https://static001.geekbang.org/resource/image/94/7a/9492443eef81027a5d1c7edb04fb6c7a.jpg\" alt=\"\"></a></p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"一键到达 | 深度学习框架下的神经网络复习课","id":4724},"right":{"article_title":"一键到达 | 应用场景复习课","id":4726}}},{"article_id":4726,"article_title":"一键到达 | 应用场景复习课","article_content":"<p>今天我们来复习专栏的最后一部分内容，<span class=\"orange\">应用场景</span>。在这个模块里，我们一起学习了人工智能技术在计算机视觉、语音处理、对话系统和机器翻译这四个经典场景的应用。</p>\n<p><span class=\"reference\">点击要点卡，直达你最需要复习的那篇。</span></p>\n<p><a href=\"https://time.geekbang.org/column/article/4020\"><img src=\"https://static001.geekbang.org/resource/image/d9/aa/d99c200046dc728cb8977c02bdec07aa.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/4021\"><img src=\"https://static001.geekbang.org/resource/image/2c/13/2cafffd82d32c23a0c3ccbee4aee0913.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/4022\"><img src=\"https://static001.geekbang.org/resource/image/9e/62/9e2357643a257b218f8a6c55694f3462.jpg\" alt=\"\"></a></p>\n<p><a href=\"https://time.geekbang.org/column/article/4023\"><img src=\"https://static001.geekbang.org/resource/image/e3/5b/e337c988eefbf0cfc8d6a30df3e3755b.jpg\" alt=\"\"></a></p>\n<!-- [[[read_end]]] -->\n<p></p>\n","neighbors":{"left":{"article_title":"一键到达 | 深度学习之外的人工智能复习课","id":4725},"right":{"article_title":"结课 | 溯洄从之，道阻且长","id":4607}}},{"article_id":4607,"article_title":"结课 | 溯洄从之，道阻且长","article_content":"<p><img src=\"https://static001.geekbang.org/resource/image/49/e1/4958e57baae6925a8d0006bca09cf7e1.jpg\" alt=\"\" /></p>\n<p>三个月的时间弹指即逝，40期的《人工智能基础课》已走到尾声。在专栏里，我和你一起走马观花地浏览了人工智能的技术概观，以<strong>建立人工智能初步的知识体系</strong>。</p>\n<p>算起来，经历了三起三落之后，人工智能已步入花甲之年。虽然在创造者眼中仍然是个孩子，但对你我这些后学之辈来说，这位老人身上还是有无穷的奥秘有待探索。在我看来，<strong>《人工智能基础课》应该是一份简历、一份提纲，勾勒出这个领域的基本框架</strong>。当然，简历无法浓缩一个人丰富的经历，40篇文章的体量也不足以覆盖人工智能的全部内容。想要基于这份提纲深入研究，还需要大量的时间和精力的付出。</p>\n<p>在互动中，不少朋友都表达了从事人工智能工作的意愿。实话实说，把这门技艺当成茶余饭后的谈资和养活自己的饭碗是截然不同的两码事情，所以需要深思熟虑之后再做选择。</p>\n<p>如果你还没有走出校园，那就应该踏踏实实学好相关的基础课程和专业课程，仰望星空不如脚踏实地，一味好高骛远只会把理想变成毫无根基的空中楼阁。</p>\n<p>如果你已经走入了工作岗位，那一定会知道不能盲目追逐热点的道理。所以在决定转行之前，务必要想清楚想做什么和能做什么，别让自己成为热点的牺牲品。</p>\n<p>相比于转行，我认为更好的做法是<strong>将人工智能融入到现有的工作之中。人工智能的价值在于落地，它的优势则是几乎在所有领域都有用武之地</strong>。</p>\n<p><strong><span class=\"orange\">与其星辰大海，不如近水楼台，将自身专门的领域知识和人工智能的方法结合，以处理和解决实际问题，并在实践中不断学习和进化，才是搭上人工智能这列快车的正确方法。</span></strong></p>\n<!-- [[[read_end]]] -->\n<p>今天的互联网行业中可说是言必称AI，不扯上点儿人工智能出门都不好意思和别人打招呼。在我看来，这或许并不是什么可喜可贺的现象。资本的热炒在客观上助长了人工智能的泡沫化趋势，也让各行各业的非技术人员产生不切实际的认知甚至幻想。如果有志于从事这个行业，一定不要被旁观者的狂欢所传染和迷惑，务必要保持当局者的冷静。</p>\n<p>客观地说，人类智能和人工智能各有所长，互为补充。人类的优势在于想象力和创造力，机器则更擅长完成具有固定规则的重复性劳动。人类的科学精英们重建自身的宏愿，在大自然看来甚至不值一哂。一个连自己的出身都没搞清楚的孩子，就做起了造物主的春秋大梦。即使有朝一日真的具备了造物主的能力，我也想象不出一个具备思考能力的新物种的出现，对人类自身会有什么样的积极意义。</p>\n<p>因此，在我个人看来，通用人工智能是非常有价值的科学问题，但现在就把大量资源投入其中未免不得要领。一口吃不成个胖子的道理对科学研究同样适用，<strong>逐步在应用中积累经验，再从经验中提取出理论，才是合理的路径</strong>。在从事人工智能的工作时，胡适先生的名言“多研究些问题，少谈些主义”可以说是再合适不过了。</p>\n<p>把目光放长远些就不难发现，人工智能已经不是个单纯的科学问题，其背后蕴藏的是强烈的哲学内涵和社会意义。在如火如荼的发展势头下，人工智能完全可能左右文明的走向甚至人类自身的命运，但绝不会是以乐观的方式。</p>\n<p>1931年，阿尔多斯·赫胥黎发表了小说《美丽新世界》，讲述了由人类创造的技术如何反过来主宰人类自身的故事。而在近80年后的今天，小说中的美丽新世界从未离我们如此之近。</p>\n<p>从昨天的大数据，到今天的人工智能，再到明天的区块链，技术变革的大潮正以不可阻挡之势汹涌来袭。<strong>在这奔腾不息的潮流中，没人能够超然世外，可即使做不成风口浪尖的弄潮儿，也要小心别被裹挟到潜伏的暗流之中无法自拔</strong>。</p>\n<p>与君共勉。</p>\n<p><a href=\"http://cn.mikecrm.com/cMBzGWA\"><img src=\"https://static001.geekbang.org/resource/image/51/3c/510e9f4184aa3b513f23a895ec749e3c.jpg\" alt=\"\" /></a></p>\n","neighbors":{"left":{"article_title":"一键到达 | 应用场景复习课","id":4726},"right":{"article_title":"结课测试 | 这些人工智能的知识你都掌握了吗？","id":226194}}},{"article_id":226194,"article_title":"结课测试 | 这些人工智能的知识你都掌握了吗？","article_content":"<p>你好，我是王天一。</p><p>到这里，《人工智能基础课》这门课程已经全部结束了。我给你准备了一个结课小测试，来帮助你检验自己的学习效果。</p><p>这套测试题共有 20 道题目，包括11道单选题和9道多选题，满分 100 分，系统自动评分。</p><p>还等什么，点击下面按钮开始测试吧！</p><p><a href=\"http://time.geekbang.org/quiz/intro?act_id=104&exam_id=220\"><img src=\"https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201\" alt=\"\"></a></p><!-- [[[read_end]]] -->","neighbors":{"left":{"article_title":"结课 | 溯洄从之，道阻且长","id":4607},"right":[]}}]