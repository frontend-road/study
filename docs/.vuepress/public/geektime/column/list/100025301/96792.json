{"id":96792,"title":"16 | Spark Streaming：Spark的实时流计算API","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的内容是“Spark Streaming”。</p><p>通过上一讲的内容，我们深入了解了Spark SQL API。通过它，我们可以像查询关系型数据库一样查询Spark的数据，并且对原生数据做相应的转换和动作。</p><p>但是，无论是DataFrame API还是DataSet API，都是基于批处理模式对静态数据进行处理的。比如，在每天某个特定的时间对一天的日志进行处理分析。</p><p>在第二章中你已经知道了，批处理和流处理是大数据处理最常见的两个场景。那么作为当下最流行的大数据处理平台之一，Spark是否支持流处理呢？</p><p>答案是肯定的。</p><p>早在2013年，Spark的流处理组件Spark Streaming就发布了。之后经过好几年的迭代与改进，现在的Spark Streaming已经非常成熟，在业界应用十分广泛。</p><p>今天就让我们一起揭开Spark Streaming的神秘面纱，让它成为我们手中的利器。</p><h2>Spark Streaming的原理</h2><p>Spark Streaming的原理与微积分的思想很类似。</p><p>在大学的微积分课上，你的老师一定说过，微分就是无限细分，积分就是对无限细分的每一段进行求和。它本质上把一个连续的问题转换成了无限个离散的问题。</p><!-- [[[read_end]]] --><p>比如，用微积分思想求下图中阴影部分S的面积。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/eb/1cef18cc51ef652c90d05c170c04e7eb.png?wh=1142*640\" alt=\"\"></p><p>我们可以把S无限细分成无数个小矩形，因为矩形的宽足够短，所以它顶端的边近似是一个直线。这样，把容易计算的矩形面积相加，就得到不容易直接计算的不规则图形面积。</p><p>你知道，流处理的数据是一系列连续不断变化，且无边界的。我们永远无法预测下一秒的数据是什么样。Spark Streaming用时间片拆分了无限的数据流，然后对每一个数据片用类似于批处理的方法进行处理，输出的数据也是一块一块的。如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/a2/2e5d3fdbe0bb09a7f2cf219df1d41ca2.png?wh=4758*1348\" alt=\"\"></p><p>Spark Streaming提供一个对于流数据的抽象DStream。DStream可以由来自Apache Kafka、Flume或者HDFS的流数据生成，也可以由别的DStream经过各种转换操作得来。讲到这里，你是不是觉得内容似曾相识？</p><p>没错，底层DStream也是由很多个序列化的RDD构成，按时间片（比如一秒）切分成的每个数据单位都是一个RDD。然后，Spark核心引擎将对DStream的Transformation操作变为针对Spark中对 RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。</p><p>之前的DataFrame和DataSet也是同样基于RDD，所以说RDD是Spark最基本的数据抽象。就像Java里的基本数据类型（Primitive Type）一样，所有的数据都可以用基本数据类型描述。</p><p>也正是因为这样，无论是DataFrame，还是DStream，都具有RDD的不可变性、分区性和容错性等特质。</p><p>所以，Spark是一个高度统一的平台，所有的高级API都有相同的性质，它们之间可以很容易地相互转化。Spark的野心就是用这一套工具统一所有数据处理的场景。</p><p>由于Spark Streaming将底层的细节封装起来了，所以对于开发者来说，只需要操作DStream就行。接下来，让我们一起学习DStream的结构以及它支持的转换操作。</p><h2>DStream</h2><p>下图就是DStream的内部形式，即一个连续的RDD序列，每一个RDD代表一个时间窗口的输入数据流。</p><p><img src=\"https://static001.geekbang.org/resource/image/66/ac/66b4562bcbd4772160f0f5766b59b5ac.png?wh=4758*1217\" alt=\"\"></p><p>对DStream的转换操作，意味着对它包含的每一个RDD进行同样的转换操作。比如下边的例子。</p><pre><code>sc = SparkContext(master, appName)\nssc = StreamingContext(sc, 1)\nlines = sc.socketTextStream(&quot;localhost&quot;, 9999)\nwords = lines.flatMap(lambda line: line.split(&quot; &quot;))\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/72/b4/72d05c02bf547f5c993fb0b3349343b4.png?wh=4758*2084\" alt=\"\"></p><p>首先，我们创建了一个lines的DStream，去监听来自本机9999端口的数据流，每一个数据代表一行文本。然后，对lines进行flatMap的转换操作，把每一个文本行拆分成词语。</p><p>本质上，对一个DStream进行flatMap操作，就是对它里边的每一个RDD进行flatMap操作，生成了一系列新的RDD，构成了一个新的代表词语的DStream。</p><p>正因为DStream和RDD的关系，RDD支持的所有转换操作，DStream都支持，比如map、flatMap、filter、union等。这些操作我们在前边学习RDD时都详细介绍过，在此不做赘述。</p><p>此外，DStream还有一些特有操作，如滑动窗口操作，我们可以一起探讨。</p><h3>滑动窗口操作</h3><p>任何Spark Streaming的程序都要首先创建一个<strong>StreamingContext</strong>的对象，它是所有Streaming操作的入口。</p><p>比如，我们可以通过StreamingContext来创建DStream。前边提到的例子中，lines这个DStream就是由名为sc的StreamingContext创建的。</p><p>StreamingContext中最重要的参数是批处理的<strong>时间间隔</strong>，即把流数据细分成数据块的粒度。</p><p>这个时间间隔决定了流处理的延迟性，所以，需要我们根据需求和资源来权衡间隔的长度。上边的例子中，我们把输入的数据流以秒为单位划分，每一秒的数据会生成一个RDD进行运算。</p><p>有些场景中，我们需要每隔一段时间，统计过去某个时间段内的数据。比如，对热点搜索词语进行统计，每隔10秒钟输出过去60秒内排名前十位的热点词。这是流处理的一个基本应用场景，很多流处理框架如Apache Flink都有原生的支持。所以，Spark也同样支持滑动窗口操作。</p><p>从统计热点词这个例子，你可以看出滑动窗口操作有两个基本参数：</p><ul>\n<li>窗口长度（window length）：每次统计的数据的时间跨度，在例子中是60秒；</li>\n<li>滑动间隔（sliding interval）：每次统计的时间间隔，在例子中是10秒。</li>\n</ul><p>显然，由于Spark Streaming流处理的最小时间单位就是StreamingContext的时间间隔，所以这两个参数一定是它的整数倍。</p><p><img src=\"https://static001.geekbang.org/resource/image/93/e3/933bd108299c65a3eb00329f345119e3.png?wh=4758*2066\" alt=\"\"></p><p>最基本的滑动窗口操作是window，它可以返回一个新的DStream，这个DStream中每个RDD代表一段时间窗口内的数据，如下例所示。</p><pre><code>windowed_words = words.window(60, 10)\n</code></pre><p>windowed_words代表的就是热词统计例子中我们所需的DStream，即它里边每一个数据块都包含过去60秒内的词语，而且这样的块每10秒钟就会生成一个。</p><p>此外，Spark Streaming还支持一些“进阶”窗口操作。如countByWindow、reduceByWindow、reduceByKeyAndWindow和countByValueAndWindow，在此不做深入讨论。</p><h2>Spark Streaming的优缺点</h2><p>讲了这么多Spark Streaming，不管内部实现也好，支持的API也好，我们还并不明白它的优势是什么，相比起其他流处理框架的缺点是什么。只有明白了这些，才能帮助我们在实际工作中决定是否使用Spark Streaming。</p><p>首先，Spark Streaming的优点很明显，由于它的底层是基于RDD实现的，所以RDD的优良特性在它这里都有体现。</p><p>比如，数据容错性，如果RDD 的某些分区丢失了，可以通过依赖信息重新计算恢复。</p><p>再比如运行速度，DStream同样也能通过persist()方法将数据流存放在内存中。这样做的好处是遇到需要多次迭代计算的程序时，速度优势十分明显。</p><p>而且，Spark Streaming是Spark生态的一部分。所以，它可以和Spark的核心引擎、Spark SQL、MLlib等无缝衔接。换句话说，对实时处理出来的中间数据，我们可以立即在程序中无缝进行批处理、交互式查询等操作。这个特点大大增强了Spark Streaming的优势和功能，使得基于Spark Streaming的应用程序很容易扩展。</p><p>而Spark Streaming的主要缺点是实时计算延迟较高，一般在秒的级别。这是由于Spark Streaming不支持太小的批处理的时间间隔。</p><p>在第二章中，我们讲过准实时和实时系统，无疑Spark Streaming是一个准实时系统。别的流处理框架，如Storm的延迟性就好很多，可以做到毫秒级。</p><h2>小结</h2><p>Spark Streaming，作为Spark中的流处理组件，把连续的流数据按时间间隔划分为一个个数据块，然后对每个数据块分别进行批处理。</p><p>在内部，每个数据块就是一个RDD，所以Spark Streaming有RDD的所有优点，处理速度快，数据容错性好，支持高度并行计算。</p><p>但是，它的实时延迟相比起别的流处理框架比较高。在实际工作中，我们还是要具体情况具体分析，选择正确的处理框架。</p><h2>思考题</h2><p>如果想要优化一个Spark Streaming程序，你会从哪些角度入手？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":97315,"user_name":"Hobbin","can_delete":false,"product_type":"c1","uid":1513048,"ip_address":"","ucode":"3341E0241C5E3E","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqTwSGXttkspRF37CTYQvVsibWicKJDtseiaE3DibfsSAHiaFM2Iwb04hg3O0Bq9JfG358A7Tlhia6vAhDw/132","comment_is_top":false,"comment_ctime":1558654977,"is_pvip":false,"replies":[{"id":"48472","content":"你说的很对，Structured streaming是Spark流处理的未来，所以我在第17讲以及之后的实战演练才重点介绍了它。此处介绍spark streaming一来是因为它的原理很基本也很重要，二来它承接了之前介绍的RDD API。<br><br>所以我觉得Structured Streaming会替代Spark Streaming，但是很难替代Flink。Flink在流处理上的天然优势很难被Spark超越，让我们拭目以待Strucutred Streaming未来会如何发展。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567373530,"ip_address":"","comment_id":97315,"utype":1}],"discussion_count":6,"race_medal":0,"score":"134702641153","product_id":100025301,"comment_content":"老师，Spark团队对Spark streaming更新越来越少，Spark streaming存在使用Processing time 而非 Event time，批流代码不统一等问题，而Structured streaming对这些都有一定改进。所以Structure streaming 会替代Spark streaming或者Flink，成为主流的流计算引擎吗？","like_count":32,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451222,"discussion_content":"你说的很对，Structured streaming是Spark流处理的未来，所以我在第17讲以及之后的实战演练才重点介绍了它。此处介绍spark streaming一来是因为它的原理很基本也很重要，二来它承接了之前介绍的RDD API。\n\n所以我觉得Structured Streaming会替代Spark Streaming，但是很难替代Flink。Flink在流处理上的天然优势很难被Spark超越，让我们拭目以待Strucutred Streaming未来会如何发展。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567373530,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":10962,"discussion_content":"这篇没有课代表。\n对于实时计算慢的问题，可否通过缓存的形式达到改善呢，比如把前几分钟的结果计算出来，然后和最近1秒的结果做去重之类的","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1568344442,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1111899,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","nickname":"时隐时现","note":"","ucode":"DA4D622FF84920","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":32229,"discussion_content":"这个思路好，但是还是不能解决时间精度问题，即如何获取毫秒级的实时数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571018187,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":10962,"ip_address":""},"score":32229,"extra":""},{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":202561,"discussion_content":"这就是micro-lambda架构嘛，不过应该有不少aggregation并不支持这种整合吧？而且个人认为spark并不是实时计算慢，而是RDD这套针对批处理的抽象在spark streaming中转译成流窗口的workflow时天生所固有的时间损失，矛头只能指向spark的批处理设计思想了","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1583933397,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":10962,"ip_address":""},"score":202561,"extra":""},{"author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":209572,"discussion_content":"有启发👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584637354,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":202561,"ip_address":""},"score":209572,"extra":""}]},{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382981,"discussion_content":"没太懂，把前几分钟的结果计算出来，和最近1秒的结果做去重是什么意思的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625815504,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":97509,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1558686669,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"70278163405","product_id":100025301,"comment_content":"批处理可以选择spark；流处理：spark stream，storm，Flink；还有现在大统一的beam<br>请问：这些技术都要学一遍吗？精力放在哪个技术上？<br>如果我是初学者，我能直接学beam其它都不学吗？","like_count":17,"discussions":[{"author":{"id":1503796,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/6XZUTxfoWu6qsSbVYsP9HBbIFpO0PSPRHVVtrqTd8e7gOOJzicLt4EeaJtxXBpLpPrBd1Xm8zl9b6iaLSPLwNOsQ/132","nickname":"ArchieGu","note":"","ucode":"F66CA1E09CA222","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":882,"discussion_content":"同问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562136175,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1247844,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/4J6FicYsBYPrEibPaYX0f5e9ArNr21vE0UyhLQZxiaNdlOoia61b2sH5Aemux520IeL0FzmIic8G4a7z02zRBMwE6iaA/132","nickname":"叨叨","note":"","ucode":"2930DFEFB3CB8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1503796,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/6XZUTxfoWu6qsSbVYsP9HBbIFpO0PSPRHVVtrqTd8e7gOOJzicLt4EeaJtxXBpLpPrBd1Xm8zl9b6iaLSPLwNOsQ/132","nickname":"ArchieGu","note":"","ucode":"F66CA1E09CA222","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6958,"discussion_content":"hadoop，zookeeper,hbase,kafka等等也是需要了解的。了解这些有助于你更好的发现并解决问题。国内现状一些bat公司分的很细具体到某一个技术比如你说的只学一个技术，学到可以为了适应公司业务而调整源码。别的公司可能会要求你了解的越多越好。","likes_number":7,"is_delete":false,"is_hidden":false,"ctime":1567220344,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":882,"ip_address":""},"score":6958,"extra":""}]}]},{"had_liked":false,"id":97970,"user_name":"邱从贤※klion26","can_delete":false,"product_type":"c1","uid":1027239,"ip_address":"","ucode":"36DF21F2B9E94C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/a7/4d41966a.jpg","comment_is_top":false,"comment_ctime":1558856825,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"57393431673","product_id":100025301,"comment_content":"上一条留言没有说完。<br>spark streaming 需要设置 batch time 是多少，这决定时效性，以及调度的 overhead，另外要看自己需要的吞吐多大，并发是不是有特殊需求。<br>spark streaming 有几个点不太喜欢，修改业务逻辑后，需要删除 checkpoint 才行，这会导致从头计算；慢节点没法解决，当一个 batch 里面有一个节点很难的时候，整个 batch 都无法完成。<br>一个反常识的点：实时 etl 同样吞吐下，flink 比 spark streaming 更节省资源。<br>另外官方已经放弃 spark streaming，转向structured streaming，但是从邮件列表看又没有 commiter 在管，导致 pr 没人 review，或许这和 spark 整体的重心或者方向有关吧","like_count":14},{"had_liked":false,"id":97362,"user_name":"lwenbin","can_delete":false,"product_type":"c1","uid":1202109,"ip_address":"","ucode":"05C4CC6BE0B56C","user_header":"https://static001.geekbang.org/account/avatar/00/12/57/bd/acf40fa0.jpg","comment_is_top":false,"comment_ctime":1558660361,"is_pvip":false,"replies":[{"id":"48578","content":"如果transform链很长，在流处理中确实会影响处理的实时性，你的想法是对的。如果只有一条很长的链，在Spark的框架中，也很难去优化。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567411789,"ip_address":"","comment_id":97362,"utype":1}],"discussion_count":2,"race_medal":0,"score":"35918398729","product_id":100025301,"comment_content":"没用过spark streaming, 用storm比较多。<br>觉得流处理关键在于要在窗口内尽快地把到来的数据处理完，不要造成数据堆积，内存溢出。<br>其中牵涉到了如何高效地接受数据，如何并行尽快地处理数据。<br>觉得优化可以从：接受输入，处理算法，处理单元数量，GC调优等方面入手吧。<br>有个问题，对于RDD如果transform链很长，感觉是否会对性能造成一定影响，特别是流式或者图形计算？老师能否解答一下。<br>谢谢！","like_count":9,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451249,"discussion_content":"如果transform链很长，在流处理中确实会影响处理的实时性，你的想法是对的。如果只有一条很长的链，在Spark的框架中，也很难去优化。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567411789,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1111899,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","nickname":"时隐时现","note":"","ucode":"DA4D622FF84920","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":32238,"discussion_content":"前面某讲中不是有个例子是读取文件，过滤关键字，返回满足条件的第1行吗，这个不是transform链吗，如何这条链很长，不正好可以从整体逻辑进行优化吗，避免了全文件扫描？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571018421,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":186828,"user_name":"Fiery","can_delete":false,"product_type":"c1","uid":1897610,"ip_address":"","ucode":"CDB000687A6B14","user_header":"","comment_is_top":false,"comment_ctime":1583934917,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27353738693","product_id":100025301,"comment_content":"既然DStream底层还是RDD，那我认为针对RDD的一些优化策略对DStream也有效。比如平衡RDD分区减少数据倾斜，在tranformation链中优先使用filter&#47;select&#47;first减少数据量，尽量串接窄依赖函数方便实现节点间并行计算和单节点链式计算优化，join时优化分区或使用broadcast减少stage间shuffle。<br><br>另外专门针对流数据的处理，个人经验上主要是要根据业务需求微调window length和sliding interval以达到吞吐量和延时之间的一个最优平衡，时间窗口越大，一个RDD可以一次批处理的数据就越多，Spark的优势就可以发挥出来，吞吐量就上去了。而滑动间隔越大，windowed DStream在固定时间内的RDD就越少，系统的任务队列里同时需要处理的计算当然就越少，不过这两个调整都会加大数据更新延迟和牺牲数据实时性，所以说要根据业务真实需求谨慎调整。<br><br>不过个人理解RDD里面用来避免重复计算的cache和persist无法用来减少窗口滑动产生的重复计算，因为窗口每滑动一次，都产生一个新的RDD，而persist只针对其中某个RDD进行缓存，在RDD这种low level api里面，应该是无法知道下个窗口中的RDD和现在的RDD到底有多少数据是重叠的。对于这点理解是否正确望老师解答！","like_count":7},{"had_liked":false,"id":97596,"user_name":"Ming","can_delete":false,"product_type":"c1","uid":1516011,"ip_address":"","ucode":"69BB73B8AB7E3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/21/eb/bb2e7a3b.jpg","comment_is_top":false,"comment_ctime":1558705049,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14443606937","product_id":100025301,"comment_content":"优化的定义很广，不知道在这个领域大家提到这个词主要指的是什么？望解答<br><br>不过，对具体实现细节不了解的情况下有几个猜测：<br>我会改变时间粒度，来减少RDD本身带来的开销，上文的例子里时间粒度如果设置成10秒应该逻辑上也是可行的。<br>另外，我大概会考虑多使用persist来减少因为窗口滑动产生的重复计算。","like_count":3,"discussions":[{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":202570,"discussion_content":"个人感觉persist无法用来减少窗口滑动产生的重复计算，因为窗口每滑动一次，都产生一个新的RDD，而persist只针对其中某个RDD进行缓存，在RDD这种low level api里面，应该是无法知道下个窗口中的RDD和现在的RDD到底有多少数据是重叠的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583934027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":200108,"user_name":"不记年","can_delete":false,"product_type":"c1","uid":1045945,"ip_address":"","ucode":"287E40C68356DC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f5/b9/888fe350.jpg","comment_is_top":false,"comment_ctime":1585544586,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5880511882","product_id":100025301,"comment_content":"在满足需求的情况下尽可能的使用更宽的窗口长度，减少rdd的处理链","like_count":1},{"had_liked":false,"id":113293,"user_name":"王翔宇🍼","can_delete":false,"product_type":"c1","uid":1177925,"ip_address":"","ucode":"D7AA5EDEC8D612","user_header":"https://static001.geekbang.org/account/avatar/00/11/f9/45/eaa8beb0.jpg","comment_is_top":false,"comment_ctime":1562937064,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1562937064","product_id":100025301,"comment_content":"sc和ssc的区别是什么？我理解ssc才是那个streamingContext吧，如果是这样，那么又出现错误了。","like_count":0},{"had_liked":false,"id":98453,"user_name":"渡码","can_delete":false,"product_type":"c1","uid":1348536,"ip_address":"","ucode":"8FD8B863D1DA0C","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/b8/6510592e.jpg","comment_is_top":false,"comment_ctime":1559005838,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559005838","product_id":100025301,"comment_content":"稳定性：对于7*24小时的流式任务至关重要<br>低延迟高吞吐量","like_count":1},{"had_liked":false,"id":97969,"user_name":"邱从贤※klion26","can_delete":false,"product_type":"c1","uid":1027239,"ip_address":"","ucode":"36DF21F2B9E94C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/a7/4d41966a.jpg","comment_is_top":false,"comment_ctime":1558856415,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558856415","product_id":100025301,"comment_content":"spark streaming 批次的大小设置多少合适，从官方宣传来看已经被 structure steeaming 替代。","like_count":0}]}