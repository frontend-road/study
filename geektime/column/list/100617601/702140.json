{"id":702140,"title":"06｜调用模型：使用OpenAI API还是微调开源Llama2/ChatGLM？","content":"<p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>之前，我们花了两节课的内容讲透了提示工程的原理以及LangChain中的具体使用方式。今天，我们来着重讨论Model I/O中的第二个子模块，LLM。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/81/cd7e1506af5b6a8e382c2c9eab4d7481.jpg?wh=4000x1536\" alt=\"\"></p><p>让我们带着下面的问题来开始这一节课的学习。大语言模型，不止ChatGPT一种。调用OpenAI的API，当然方便且高效，不过，如果我就是想用其他的模型（比如说开源的Llama2或者ChatGLM），该怎么做？再进一步，如果我就是想在本机上从头训练出来一个新模型，然后在LangChain中使用自己的模型，又该怎么做？</p><p>关于大模型的微调（或称精调）、预训练、重新训练、乃至从头训练，这是一个相当大的话题，不仅仅需要足够的知识和经验，还需要大量的语料数据、GPU硬件和强大的工程能力。别说一节课了，我想两三个专栏也不一定能讲全讲透。不过，我可以提纲挈领地把大模型的训练流程和使用方法给你缕一缕。这样你就能体验到，在LangChain中使用自己微调的模型是完全没问题的。</p><h2>大语言模型发展史</h2><p>说到语言模型，我们不妨先从其发展史中去了解一些关键信息。</p><p>Google 2018 年的论文名篇Attention is all you need，提出了Transformer架构，也给这一次AI的腾飞点了火。Transformer是几乎所有预训练模型的核心底层架构。基于Transformer预训练所得的大规模语言模型也被叫做“基础模型”（Foundation Model 或Base Model）。</p><!-- [[[read_end]]] --><p>在这个过程中，模型学习了词汇、语法、句子结构以及上下文信息等丰富的语言知识。这种在大量数据上学到的知识，为后续的下游任务（如情感分析、文本分类、命名实体识别、问答系统等）提供了一个通用的、丰富的语言表示基础，为解决许多复杂的NLP问题提供了可能。</p><p>在预训练模型出现的早期，BERT毫无疑问是最具代表性的，也是影响力最大的模型。BERT通过同时学习文本的前向和后向上下文信息，实现对句子结构的深入理解。BERT之后，各种大型预训练模型如雨后春笋般地涌现，自然语言处理（NLP）领域进入了一个新时代。这些模型推动了NLP技术的快速发展，解决了许多以前难以应对的问题，比如翻译、文本总结、聊天对话等等，提供了强大的工具。</p><p><img src=\"https://static001.geekbang.org/resource/image/7f/a6/7f1108deceaa4b5281ed431598f1b0a6.jpg?wh=10666x4300\" alt=\"\" title=\"各种预训练语言模型\"></p><p>当然，现今的预训练模型的趋势是参数越来越多，模型也越来越大，训练一次的费用可达几百万美元。这样大的开销和资源的耗费，只有世界顶级大厂才能够负担得起，普通的学术组织和高等院校很难在这个领域继续引领科技突破，这种现象开始被普通研究人员所诟病。</p><p><img src=\"https://static001.geekbang.org/resource/image/95/ef/95828d4e2234e7130bb2d500455092ef.jpg?wh=10666x6000\" alt=\"\" title=\"模型越来越大，参数越来越多\"></p><h2>预训练+微调的模式</h2><p>不过，话虽如此，大型预训练模型的确是工程师的福音。因为，经过预训练的大模型中所习得的语义信息和所蕴含的语言知识，能够非常容易地向下游任务迁移。NLP应用人员可以对模型的头部或者部分参数根据自己的需要进行适应性的调整，这通常涉及在相对较小的有标注数据集上进行有监督学习，让模型适应特定任务的需求。</p><p>这就是对预训练模型的微调（Fine-tuning）。微调过程相比于从头训练一个模型要快得多，且需要的数据量也要少得多，这使得作为工程师的我们能够更高效地开发和部署各种NLP解决方案。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/af/75edd66d67ec8a20326b69514c9d9daf.jpg?wh=10666x5061\" alt=\"\" title=\"预训练+微调的应用模式\"></p><p>图中的“具体任务”，其实也可以更换为“具体领域”。那么总结来说：</p><ul>\n<li><strong>预训练</strong>：在大规模无标注文本数据上进行模型的训练，目标是让模型学习自然语言的基础表达、上下文信息和语义知识，为后续任务提供一个通用的、丰富的语言表示基础。</li>\n<li><strong>微调</strong>：在预训练模型的基础上，可以根据特定的下游任务对模型进行微调。现在你经常会听到各行各业的人说：<em>我们的优势就是领域知识嘛！我们比不过国内外大模型，我们可以拿开源模型做垂直领域嘛！做垂类模型！</em>—— 啥叫垂类？指的其实就是根据领域数据微调开源模型这件事儿。</li>\n</ul><p>这种预训练+微调的大模型应用模式优势明显。首先，预训练模型能够将大量的通用语言知识迁移到各种下游任务上，作为应用人员，我们不需要自己寻找语料库，从头开始训练大模型，这减少了训练时间和数据需求；其次，微调过程可以快速地根据特定任务进行优化，简化了模型部署的难度；最后，预训练+微调的架构具有很强的可扩展性，可以方便地应用于各种自然语言处理任务，大大提高了NLP技术在实际应用中的可用性和普及程度，给我们带来了巨大的便利。</p><p>好，下面咱们开始一步步地使用开源模型。今天我要带你玩的模型主要是Meta（Facebook）推出的Llama2。当然你可以去Llama的官网下载模型，然后通过Llama官方 <a href=\"https://github.com/facebookresearch/llama\">GitHub</a> 中提供的方法来调用它。但是，我还是会推荐你从HuggingFace下载并导入模型。因为啊，前天百川，昨天千问，今天流行Llama，明天不就流行别的了嘛。模型总在变，但是HuggingFace一直在那里，支持着各种开源模型。我们学东西，尽量选择学一次能够复用的知识。</p><h2>用 HuggingFace 跑开源模型</h2><h3>注册并安装 HuggingFace</h3><p>第一步，还是要登录 <a href=\"https://huggingface.co/\">HuggingFace</a> 网站，并拿到专属于你的Token。（如果你做了前面几节课的实战案例，那么你应该已经有这个API Token了）</p><p>第二步，用 <code>pip install transformers</code> 安装HuggingFace Library。详见<a href=\"https://huggingface.co/docs/transformers/installation\">这里</a>。</p><p>第三步，在命令行中运行 <code>huggingface-cli login</code>，设置你的API Token。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/e6/5fa0c088652c8776f5ec50a059b1b1e6.png?wh=1475x668\" alt=\"\"></p><p>当然，也可以在程序中设置你的API Token，但是这不如在命令行中设置来得安全。</p><pre><code class=\"language-plain\"># 导入HuggingFace API Token\nimport os\nos.environ['HUGGINGFACEHUB_API_TOKEN'] = '你的HuggingFace API Token'\n</code></pre><h3>申请使用 Meta 的 Llama2 模型</h3><p>在HuggingFace的Model中，找到 <a href=\"https://huggingface.co/meta-llama/Llama-2-7b\">meta-llama/Llama-2-7b</a>。注意，各种各样版本的Llama2模型多如牛毛，我们这里用的是最小的7B版。此外，还有13b\\70b\\chat版以及各种各样的非Meta官方版。</p><p><img src=\"https://static001.geekbang.org/resource/image/88/17/88a4b8d60cc93e77c3573663aa096217.png?wh=3028x710\" alt=\"\"></p><p>选择meta-llama/Llama-2-7b这个模型后，你能够看到这个模型的基本信息。如果你是第一次用Llama，你需要申请Access，因为我已经申请过了，所以屏幕中间有句话：“You have been granted access to this model”。从申请到批准，大概是几分钟的事儿。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/ce/46c59c59545720ccff6d7c560792d4ce.png?wh=3029x1662\" alt=\"\"></p><h3>通过 HuggingFace 调用 Llama</h3><p>好，万事俱备，现在我们可以使用HuggingFace的Transformers库来调用Llama啦！</p><pre><code class=\"language-plain\"># 导入必要的库\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# 加载预训练模型的分词器\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n\n# 加载预训练的模型\n# 使用 device_map 参数将模型自动加载到可用的硬件设备上，例如GPU\nmodel = AutoModelForCausalLM.from_pretrained(\n          \"meta-llama/Llama-2-7b-chat-hf\", \n          device_map = 'auto')  \n\n# 定义一个提示，希望模型基于此提示生成故事\nprompt = \"请给我讲个玫瑰的爱情故事?\"\n\n# 使用分词器将提示转化为模型可以理解的格式，并将其移动到GPU上\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# 使用模型生成文本，设置最大生成令牌数为2000\noutputs = model.generate(inputs[\"input_ids\"], max_new_tokens=2000)\n\n# 将生成的令牌解码成文本，并跳过任何特殊的令牌，例如[CLS], [SEP]等\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 打印生成的响应\nprint(response)\n</code></pre><p>这段程序是一个很典型的HuggingFace的Transformers库的用例，该库提供了大量预训练的模型和相关的工具。</p><ul>\n<li>导入AutoTokenizer：这是一个用于自动加载预训练模型的相关分词器的工具。分词器负责将文本转化为模型可以理解的数字格式。</li>\n<li>导入AutoModelForCausalLM：这是用于加载因果语言模型（用于文本生成）的工具。</li>\n<li>使用from_pretrained方法来加载预训练的分词器和模型。其中，<code>device_map = 'auto'</code> 是为了自动地将模型加载到可用的设备上，例如GPU。</li>\n<li>然后，给定一个提示（prompt）：<code>\"请给我讲个玫瑰的爱情故事?\"</code>，并使用分词器将该提示转换为模型可以接受的格式，<code>return_tensors=\"pt\"</code> 表示返回PyTorch张量。语句中的 <code>.to(\"cuda\")</code> 是GPU设备格式转换，因为我在GPU上跑程序，不用这个的话会报错，如果你使用CPU，可以试一下删掉它。</li>\n<li>最后使用模型的 <code>.generate()</code> 方法生成响应。<code>max_new_tokens=2000</code> 限制生成的文本的长度。使用分词器的 <code>.decode() </code>方法将输出的数字转化回文本，并且跳过任何特殊的标记。</li>\n</ul><p>因为是在本地进行推理，耗时时间比较久。在我的机器上，大概需要30s～2min产生结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/93/98/933b7b11512bd06a977027cbbfd8d198.png?wh=1653x651\" alt=\"\"></p><p>这样的回答肯定不能直接用做商业文案，而且，我的意思是玫瑰花相关的故事，它明显把玫瑰理解成一个女孩的名字了。所以，开源模型，尤其是7B的小模型和Open AI的ChatGPT还是有一定差距的。</p><h2>LangChain 和 HuggingFace 的接口</h2><p>讲了半天，LangChain未出场。下面让我们看一看，如何把HuggingFace里面的模型接入LangChain。</p><h3>通过 HuggingFace Hub</h3><p>第一种集成方式，是通过HuggingFace Hub。HuggingFace Hub 是一个开源模型中心化存储库，主要用于分享、协作和存储预训练模型、数据集以及相关组件。</p><p>我们给出一个HuggingFace Hub 和LangChain集成的代码示例。</p><pre><code class=\"language-plain\"># 导入HuggingFace API Token\nimport os\nos.environ['HUGGINGFACEHUB_API_TOKEN'] = '你的HuggingFace API Token'\n\n# 导入必要的库\nfrom langchain import PromptTemplate, HuggingFaceHub, LLMChain\n\n# 初始化HF LLM\nllm = HuggingFaceHub(\n&nbsp; &nbsp; repo_id=\"google/flan-t5-small\",\n&nbsp; &nbsp; #repo_id=\"meta-llama/Llama-2-7b-chat-hf\",\n)\n\n# 创建简单的question-answering提示模板\ntemplate = \"\"\"Question: {question}\n              Answer: \"\"\"\n\n# 创建Prompt          \nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n\n# 调用LLM Chain --- 我们以后会详细讲LLM Chain\nllm_chain = LLMChain(\n&nbsp; &nbsp; prompt=prompt,\n&nbsp; &nbsp; llm=llm\n)\n\n# 准备问题\nquestion = \"Rose is which type of flower?\"\n\n# 调用模型并返回结果\nprint(llm_chain.run(question))\n</code></pre><p>可以看出，这个集成过程非常简单，只需要在HuggingFaceHub类的repo_id中指定模型名称，就可以直接下载并使用模型，模型会自动下载到HuggingFace的Cache目录，并不需要手工下载。</p><p>初始化LLM，创建提示模板，生成提示的过程，你已经很熟悉了。这段代码中有一个新内容是我通过llm_chain来调用了LLM。这段代码也不难理解，有关Chain的概念我们以后还会详述。</p><p>不过，我尝试使用meta-llama/Llama-2-7b-chat-hf这个模型时，出现了错误，因此我只好用比较旧的模型做测试。我随便选择了google/flan-t5-small，问了它一个很简单的问题，想看看它是否知道玫瑰是哪一种花。</p><p><img src=\"https://static001.geekbang.org/resource/image/5b/71/5bfc31eacb422fcd1d148bb1a2b3bf71.png?wh=547x149\" alt=\"\"></p><p>模型告诉我，玫瑰是花。对，答案只有一个字，flower。这…不得不说，2023年之前的模型，和2023年之后的模型，水平没得比。以前的模型能说话就不错了。</p><p><img src=\"https://static001.geekbang.org/resource/image/yy/3e/yyc2177bc3c06f1d738f26985b9fbd3e.png?wh=578x92\" alt=\"\" title=\"ChatGPT 本 T 的回答\"></p><h3>通过 HuggingFace Pipeline</h3><p>既然HuggingFace Hub还不能完成Llama-2的测试，让我们来尝试另外一种方法，HuggingFace Pipeline。HuggingFace 的 Pipeline 是一种高级工具，它简化了多种常见自然语言处理（NLP）任务的使用流程，使得用户不需要深入了解模型细节，也能够很容易地利用预训练模型来做任务。</p><p>让我来看看下面的示例：</p><pre><code class=\"language-plain\"># 指定预训练模型的名称\nmodel = \"meta-llama/Llama-2-7b-chat-hf\"\n\n# 从预训练模型中加载词汇器\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model)\n\n# 创建一个文本生成的管道\nimport transformers\nimport torch\npipeline = transformers.pipeline(\n&nbsp; &nbsp; \"text-generation\",\n&nbsp; &nbsp; model=model,\n&nbsp; &nbsp; torch_dtype=torch.float16,\n&nbsp; &nbsp; device_map=\"auto\",\n&nbsp; &nbsp; max_length = 1000\n)\n\n# 创建HuggingFacePipeline实例\nfrom langchain import HuggingFacePipeline\nllm = HuggingFacePipeline(pipeline = pipeline, \n                          model_kwargs = {'temperature':0})\n\n# 定义输入模板，该模板用于生成花束的描述\ntemplate = \"\"\"\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 为以下的花束生成一个详细且吸引人的描述：\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 花束的详细信息：\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ```{flower_details}```\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"\"\"\n\n# 使用模板创建提示\nfrom langchain import PromptTemplate, &nbsp;LLMChain\nprompt = PromptTemplate(template=template, \n                     input_variables=[\"flower_details\"])\n\n# 创建LLMChain实例\nfrom langchain import PromptTemplate\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\n# 需要生成描述的花束的详细信息\nflower_details = \"12支红玫瑰，搭配白色满天星和绿叶，包装在浪漫的红色纸中。\"\n\n# 打印生成的花束描述\nprint(llm_chain.run(flower_details))\n</code></pre><p>这里简单介绍一下代码中使用到的transformers pipeline的配置参数。</p><p><img src=\"https://static001.geekbang.org/resource/image/41/7e/41yyb05408bd6a16e349f89279548f7e.jpg?wh=2490x1034\" alt=\"\"></p><p>生成的结果之一如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/1b/3c/1bb303ec8bd8150d23bebc79035af13c.jpg?wh=6396x3569\" alt=\"\"></p><p>此结果不敢恭维。但是，后续的测试告诉我，这很有可能是7B这个模型太小，尽管有形成中文的相应能力，但是能力不够强大，也就导致了这样的结果。</p><p>至此，通过HuggingFace接口调用各种开源模型的尝试成功结束。下面，我们进行最后一个测试，看看LangChain到底能否直接调用本地模型。</p><h2>用 LangChain 调用自定义语言模型</h2><p>最后，我们来尝试回答这节课开头提出的问题，假设你就是想训练属于自己的模型。而且出于商业秘密的原因，不想开源它，不想上传到HuggingFace，就是要在本机运行模型。此时应该如何利用LangChain的功能？</p><p>我们可以创建一个LLM的衍生类，自己定义模型。而LLM这个基类，则位于langchain.llms.base中，通过from langchain.llms.base import LLM语句导入。</p><p>这个自定义的LLM类只需要实现一个方法：</p><ul>\n<li>_call方法：用于接收输入字符串并返回响应字符串。</li>\n</ul><p>以及一个可选方法：</p><ul>\n<li>_identifying_params方法：用于帮助打印此类的属性。</li>\n</ul><p>下面，让我们先从HuggingFace的<a href=\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main\">这里</a>，下载一个llama-2-7b-chat.ggmlv3.q4_K_S.bin模型，并保存在本地。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/1c/54c0ec3cbe3c3cyy6988de10f619b51c.png?wh=3879x1976\" alt=\"\"></p><p>你可能会质疑我，不是说自己训练，自己微调，不再用HuggingFace了吗？</p><p>不好意思，容许我解释一下。自己训练一个能用的模型没那么容易。这个模型，它并不是原始的Llama模型，而是TheBloke这位老兄用他的手段为我们量化过的新模型，你也可以理解成，他已经为我们压缩或者说微调了Llama模型。</p><blockquote>\n<p><span class=\"reference\">量化是AI模型大小和性能优化的常用技术，它将模型的权重简化到较少的位数，以减少模型的大小和计算需求，让大模型甚至能够在CPU上面运行。当你看到模型的后缀有GGML或者GPTQ，就说明模型已经被量化过，其中GPTQ 是一种仅适用于 GPU 的特定格式。GGML 专为 CPU 和 Apple M 系列设计，但也可以加速 GPU 上的某些层。llama-cpp-python这个包就是为了实现GGML而制作的。</span></p>\n</blockquote><p>所以，这里你就假设，咱们下载下来的llama-2-7b-chat.ggmlv3.q4_K_S.bin这个模型，就是你自己微调过的。将来你真的微调了Llama2、ChatGLM、百川或者千问的开源版，甚至是自己从头训练了一个mini-ChatGPT，你也可以保存为you_own_model.bin的格式，就按照下面的方式加载到LangChain之中。</p><p>然后，为了使用llama-2-7b-chat.ggmlv3.q4_K_S.bin这个模型，你需要安装 pip install llama-cpp-python 这个包。</p><pre><code class=\"language-plain\"># 导入需要的库\nfrom llama_cpp import Llama\nfrom typing import Optional, List, Mapping, Any\nfrom langchain.llms.base import LLM\n\n# 模型的名称和路径常量\nMODEL_NAME = 'llama-2-7b-chat.ggmlv3.q4_K_S.bin'\nMODEL_PATH = '/home/huangj/03_Llama/'\n\n# 自定义的LLM类，继承自基础LLM类\nclass CustomLLM(LLM):\n&nbsp; &nbsp; model_name = MODEL_NAME\n\n&nbsp; &nbsp; # 该方法使用Llama库调用模型生成回复\n&nbsp; &nbsp; def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n&nbsp; &nbsp; &nbsp; &nbsp; prompt_length = len(prompt) + 5\n&nbsp; &nbsp; &nbsp; &nbsp; # 初始化Llama模型，指定模型路径和线程数\n&nbsp; &nbsp; &nbsp; &nbsp; llm = Llama(model_path=MODEL_PATH+MODEL_NAME, n_threads=4)\n&nbsp; &nbsp; &nbsp; &nbsp; # 使用Llama模型生成回复\n&nbsp; &nbsp; &nbsp; &nbsp; response = llm(f\"Q: {prompt} A: \", max_tokens=256)\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; # 从返回的回复中提取文本部分\n&nbsp; &nbsp; &nbsp; &nbsp; output = response['choices'][0]['text'].replace('A: ', '').strip()\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 返回生成的回复，同时剔除了问题部分和额外字符\n&nbsp; &nbsp; &nbsp; &nbsp; return output[prompt_length:]\n\n&nbsp; &nbsp; # 返回模型的标识参数，这里只是返回模型的名称\n&nbsp; &nbsp; @property\n&nbsp; &nbsp; def _identifying_params(self) -&gt; Mapping[str, Any]:\n&nbsp; &nbsp; &nbsp; &nbsp; return {\"name_of_model\": self.model_name}\n\n&nbsp; &nbsp; # 返回模型的类型，这里是\"custom\"\n&nbsp; &nbsp; @property\n&nbsp; &nbsp; def _llm_type(self) -&gt; str:\n&nbsp; &nbsp; &nbsp; &nbsp; return \"custom\"\n&nbsp; &nbsp;&nbsp;\n\n# 初始化自定义LLM类\nllm = CustomLLM()\n\n# 使用自定义LLM生成一个回复\nresult = llm(\"昨天有一个客户抱怨他买了花给女朋友之后，两天花就枯了，你说作为客服我应该怎么解释？\")\n\n# 打印生成的回复\nprint(result)\n</code></pre><p>代码中需要解释的内容不多，基本上就是CustomLLM类的构建和使用，类内部通过Llama类来实现大模型的推理功能，然后直接返回模型的回答。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/59/0275183b3863e602c59afb94707aca59.jpg?wh=6369x1322\" alt=\"\"></p><p>似乎Llama经过量化之后，虽然仍读得懂中文，但是不会讲中文了。</p><p>翻译成中文，他的回答是这样的。</p><p><em>当客户抱怨他们为女朋友买的花在两天内就枯萎了，我会以礼貌和专业的方式这样解释：</em></p><p><em>“感谢您把这个问题告诉我们。对于给您带来的任何不便，我深感抱歉。有可能这些花没有被正确地存储或照料，这可能影响了它们的生命期。我们始终以提供高质量的产品为荣，但有时可能会出现意外的问题。请您知道，我们非常重视您的满意度并随时为您提供帮助。您希望我为您提供替换或退款吗？”</em></p><p>看上去，除了中文能力不大灵光之外，Llama2的英文表现真的非常完美，和GPT3.5差距不是很大，要知道：</p><ol>\n<li>这可是开源模型，而且是允许商业的免费模型。</li>\n<li>这是在本机 CPU 的环境下运行的，模型的推理速度还是可以接受的。</li>\n<li>这仅仅是Llama的最小版本，也就是7B的量化版，就达到了这么好的效果。</li>\n</ol><p>基于上述三点原因，我给Llama2打98.5分。</p><h2>总结时刻</h2><p>今天的课程到此就结束了，相信你学到了很多新东西吧。的确，进入大模型开发这个领域，就好像打开了通往新世界的一扇门，有太多的新知识，等待着你去探索。</p><p>现在，你已经知道大模型训练涉及在大量数据上使用深度学习算法，通常需要大量计算资源和时间。训练后，模型可能不完全适合特定任务，因此需要微调，即在特定数据集上继续训练，以使模型更适应该任务。为了减小部署模型的大小和加快推理速度，模型还会经过量化，即将模型参数从高精度格式减少到较低精度。</p><p>如果你想继续深入学习大模型，那么有几个工具你不得不接着研究。</p><ul>\n<li>PyTorch是一个流行的深度学习框架，常用于模型的训练和微调。</li>\n<li>HuggingFace是一个开源社区，提供了大量预训练模型和微调工具，尤其是NLP任务。</li>\n<li>LangChain则擅长于利用大语言模型的推理功能，开发新的工具或应用，完成特定的任务。</li>\n</ul><p>这些工具和库在AI模型的全生命周期中起到关键作用，使研究者和开发者更容易开发和部署高效的AI系统。</p><h2>思考题</h2><ol>\n<li>现在请你再回答一下，什么时候应该使用OpenAI的API？什么时候应该使用开源模型？或者自己开发/微调的模型？<br>\n<span class=\"orange\">提示：的确，文中没有给出这个问题的答案。因为这个问题并没有标准答案。</span><br>\n&nbsp;</li>\n<li>请你使用HuggingFace的Transformers库，下载新的模型进行推理，比较它们的性能。<br>\n&nbsp;</li>\n<li>请你在LangChain中，使用HuggingFaceHub和HuggingFace Pipeline这两种接口，调用当前最流行的大语言模型。<br>\n<span class=\"orange\">提示：HuggingFace Model 页面，有模型下载量的当月排序，当月下载最多的模型就是最流行的模型。</span></li>\n</ol><p>期待在留言区看到你的分享，我们一起交流探讨，共创一个良好的学习氛围。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2>延伸阅读</h2><ol>\n<li>Llama2，开源的可商用类ChatGPT模型，<a href=\"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\">Facebook链接</a>、<a href=\"https://github.com/facebookresearch/llama\">GitHub链接</a></li>\n<li>HuggingFace <a href=\"https://huggingface.co/docs/transformers/index\">Transformer</a> 文档</li>\n<li>PyTorch 官方<a href=\"https://pytorch.org/tutorials/\">教程</a>、<a href=\"https://pytorch.org/docs/stable/index.html\">文档</a></li>\n<li><a href=\"https://github.com/PanQiWei/AutoGPTQ\">AutoGPTQ</a> 基于GPTQ算法的大模型量化工具包</li>\n<li><a href=\"https://github.com/ggerganov/llama.cpp\">Llama CPP</a> 支持 <a href=\"https://github.com/ggerganov/ggml\">GGML</a>，目标是在MacBook（或类似的非GPU的普通家用硬件环境）上使用4位整数量化运行Llama模型</li>\n</ol>","comments":[{"had_liked":false,"id":381480,"user_name":"nick","can_delete":false,"product_type":"c1","uid":1061843,"ip_address":"福建","ucode":"91263882076AC0","user_header":"https://static001.geekbang.org/account/avatar/00/10/33/d3/30a89b21.jpg","comment_is_top":false,"comment_ctime":1695267477,"is_pvip":false,"replies":[{"id":138973,"content":"好问题啊同学。当我们谈论大模型，特别是像GPT系列这样的大语言模型时，这确实改变了多轮对话的构建方式。但这并不意味着传统的意图分类和槽填充等技术就完全没有用处了。下面是一些考量哈（下面的部分回答来自于ChatGPT，但经过了作者的审定和补充🤩）：\n1. 减少手工标注需求：大模型如GPT能理解并生成自然语言，因此，对于很多常见的对话场景，你不再需要像传统方法那样进行大量的意图分类和槽填充标注。模型具有一定的通用知识，可以帮你做意图的分类哈。\n2. 微调和特定任务训练：即使有了大模型，你仍可能需要进行微调，特别是针对特定的垂直领域或特定的业务场景。这通常需要一个相对较小但高质量的标注数据集。\n3. 结合传统和新方法：大模型和传统的NLU（自然语言理解）方法可以结合使用。例如，你可以首先使用意图分类器判断用户的主要意图，然后将这个意图和用户的原始查询一起传递给大模型，让其生成更具体的回复。\n4. 模型可解释性：传统的方法，如意图分类和槽填充，通常更具解释性。你知道模型为什么做出某个决策，因为它基于明确的意图和实体。而大模型的决策过程可能更加“黑箱”。\n5. 成本效率和资源需求：大模型通常需要更多的计算资源。而且最好的大模型入GPT4是要付费的呀。在某些实时或资源受限的环境中，使用轻量级的传统方法可能更为合适。\n6. 错误和偏见：大模型可能会犯一些出乎意料的错误，或者可能继承了预训练数据中的偏见。使用传统方法，你对数据和模型的控制更为明确。\n\n结合大模型的能力和传统的意图分类和槽填充方法可能是一个更好的策略。根据具体的业务需求和场景来决定使用哪种方法，要结合两者的优点。","user_name":"作者回复","user_name_real":"作者","uid":1809833,"ctime":1695287527,"ip_address":"新加坡","comment_id":381480,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，有个疑问，原来使用像百度AI、腾讯AI做垂直领域的多轮对话，往往需要维护语料意图啥的，那有了大模型后，这些工作还需要做么，如果要那怎么做？跟传统维护语料意图有什么区别。谢谢","like_count":4,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628433,"discussion_content":"好问题啊同学。当我们谈论大模型，特别是像GPT系列这样的大语言模型时，这确实改变了多轮对话的构建方式。但这并不意味着传统的意图分类和槽填充等技术就完全没有用处了。下面是一些考量哈（下面的部分回答来自于ChatGPT，但经过了作者的审定和补充🤩）：\n1. 减少手工标注需求：大模型如GPT能理解并生成自然语言，因此，对于很多常见的对话场景，你不再需要像传统方法那样进行大量的意图分类和槽填充标注。模型具有一定的通用知识，可以帮你做意图的分类哈。\n2. 微调和特定任务训练：即使有了大模型，你仍可能需要进行微调，特别是针对特定的垂直领域或特定的业务场景。这通常需要一个相对较小但高质量的标注数据集。\n3. 结合传统和新方法：大模型和传统的NLU（自然语言理解）方法可以结合使用。例如，你可以首先使用意图分类器判断用户的主要意图，然后将这个意图和用户的原始查询一起传递给大模型，让其生成更具体的回复。\n4. 模型可解释性：传统的方法，如意图分类和槽填充，通常更具解释性。你知道模型为什么做出某个决策，因为它基于明确的意图和实体。而大模型的决策过程可能更加“黑箱”。\n5. 成本效率和资源需求：大模型通常需要更多的计算资源。而且最好的大模型入GPT4是要付费的呀。在某些实时或资源受限的环境中，使用轻量级的传统方法可能更为合适。\n6. 错误和偏见：大模型可能会犯一些出乎意料的错误，或者可能继承了预训练数据中的偏见。使用传统方法，你对数据和模型的控制更为明确。\n\n结合大模型的能力和传统的意图分类和槽填充方法可能是一个更好的策略。根据具体的业务需求和场景来决定使用哪种方法，要结合两者的优点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695287528,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":389026,"user_name":"javanb","can_delete":false,"product_type":"c1","uid":2238481,"ip_address":"美国","ucode":"5B2D296D290736","user_header":"https://static001.geekbang.org/account/avatar/00/22/28/11/2cbd941d.jpg","comment_is_top":false,"comment_ctime":1711417622,"is_pvip":false,"replies":[{"id":141716,"content":"大模型微调还是一个新兴方向,没有现成的固定范式,还需要企业根据自身情况多实践和摸索。有论文指出，大模型能力较强时，做提示工程，也就是RAG，给模型以知识，比微调效果差不了多少。\n(1)数据比例: 实际上并没有一个固定的最佳比例,需要根据任务的需求和行业数据的质量来权衡。过多的通用数据可能会 dilute 行业数据的效果,过少又无法保证通用能力。我建议可以先尝试2:1到1:2的比例,然后根据效果做进一步调整。\n(2)训练方式:这也是需要实践的问题。Lora可以更高效地适应新领域,但如果行业数据很丰富,freeze+微调的方式理论上效果会更好一些。\n(3)数据类型:除了公司内部的真实业务数据(最重要),还可以考虑引入一些高质量的行业语料,如行业百科、教材、论文等,这有助于补充背景知识。另外在训 NL 的时候,也可以适当引入一些reflection和chain of thought的数据,促进模型形成更完善的推理能力。\n(4)prompt engineering:精心设计任务的prompt和instruction,引导模型按照期望的方式输出,会很大程度上影响效果。多实践几种prompt范式。\n","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1712809912,"ip_address":"新加坡","comment_id":389026,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，最近我就在微调垂直行业大模型，但是效果不理想，我这边采用了Lora的训练方式，行业数据和通用数据大约是1:1。行业数据量大约10G。但是出来的结果，通用能力降低了，并且行业能力也并没有那么理想。 所以有几个疑问不知老师能否传授一下SFT的经验、1：我们通常行业数据配比是多少？ 2：哪种训练方式更合适？lora还是freeze  3：哪种类型的数据更合适？目前是大量的公司内部的数据，还需要哪些数据？","like_count":3,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641606,"discussion_content":"大模型微调还是一个新兴方向,没有现成的固定范式,还需要企业根据自身情况多实践和摸索。有论文指出，大模型能力较强时，做提示工程，也就是RAG，给模型以知识，比微调效果差不了多少。\n(1)数据比例: 实际上并没有一个固定的最佳比例,需要根据任务的需求和行业数据的质量来权衡。过多的通用数据可能会 dilute 行业数据的效果,过少又无法保证通用能力。我建议可以先尝试2:1到1:2的比例,然后根据效果做进一步调整。\n(2)训练方式:这也是需要实践的问题。Lora可以更高效地适应新领域,但如果行业数据很丰富,freeze+微调的方式理论上效果会更好一些。\n(3)数据类型:除了公司内部的真实业务数据(最重要),还可以考虑引入一些高质量的行业语料,如行业百科、教材、论文等,这有助于补充背景知识。另外在训 NL 的时候,也可以适当引入一些reflection和chain of thought的数据,促进模型形成更完善的推理能力。\n(4)prompt engineering:精心设计任务的prompt和instruction,引导模型按照期望的方式输出,会很大程度上影响效果。多实践几种prompt范式。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712809912,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383021,"user_name":"远游","can_delete":false,"product_type":"c1","uid":2103322,"ip_address":"北京","ucode":"73792ED128D57C","user_header":"https://static001.geekbang.org/account/avatar/00/20/18/1a/5f36bb6e.jpg","comment_is_top":false,"comment_ctime":1698308991,"is_pvip":false,"replies":[{"id":139528,"content":"使用 virtualenv 或 venv 创建一个新的虚拟环境吧，然后在这个环境中尝试安装 llama-cpp-python。这或许可以避免因为系统级的 Python 包的冲突而导致的问题。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698381479,"ip_address":"瑞士","comment_id":383021,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，macos安装pip install llama-cpp-python报错：\n                                      ^\n      &#47;private&#47;var&#47;folders&#47;x_&#47;l6wthj_d7mb_gzcx3wvkmcvc0000gn&#47;T&#47;pip-install-ms76ov67&#47;llama-cpp-python_9ce52d9b45d04a73950f2448a45590a0&#47;vendor&#47;llama.cpp&#47;ggml.c:2243:11: note: expanded from macro &#39;GGML_F32x4_REDUCE&#39;\n          res = _mm_cvtss_f32(_mm_hadd_ps(t0, t0));                     \\\n              ~ ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      12 warnings generated.\n      [10&#47;13] &#47;Library&#47;Developer&#47;CommandLineTools&#47;usr&#47;bin&#47;c++ -DACCELERATE_LAPACK_ILP64 -DACCELERATE_NEW_LAPACK -DGGML_USE_ACCELERATE -DGGML_USE_K_QUANTS -DGGML_USE_METAL -DLLAMA_BUILD -DLLAMA_SHARED -D_DARWIN_C_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I&#47;private&#47;var&#47;folders&#47;x_&#47;l6wthj_d7mb_gzcx3wvkmcvc0000gn&#47;T&#47;pip-install-ms76ov67&#47;llama-cpp-python_9ce52d9b45d04a73950f2448a45590a0&#47;vendor&#47;llama.cpp&#47;. -O3 -DNDEBUG -std=gnu++11 -isysroot &#47;Library&#47;Developer&#47;CommandLineTools&#47;SDKs&#47;MacOSX10.15.sdk -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-declarations -Wmissing-noreturn -Wmissing-prototypes -Wextra-semi -MD -MT vendor&#47;llama.cpp&#47;CMakeFiles&#47;llama.dir&#47;llama.cpp.o -MF vendor&#47;llama.cpp&#47;CMakeFiles&#47;llama.dir&#47;llama.cpp.o.d -o vendor&#47;llama.cpp&#47;CMakeFiles&#47;llama.dir&#47;llama.cpp.o -c &#47;private&#47;var&#47;folders&#47;x_&#47;l6wthj_d7mb_gzcx3wvkmcvc0000gn&#47;T&#47;pip-install-ms76ov67&#47;llama-cpp-python_9ce52d9b45d04a73950f2448a45590a0&#47;vendor&#47;llama.cpp&#47;llama.cpp\n      ninja: build stopped: subcommand failed.\n\n      *** CMake build failed\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for llama-cpp-python\nFailed to build llama-cpp-python\nERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630390,"discussion_content":"使用 virtualenv 或 venv 创建一个新的虚拟环境吧，然后在这个环境中尝试安装 llama-cpp-python。这或许可以避免因为系统级的 Python 包的冲突而导致的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698381479,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381841,"user_name":"陈东","can_delete":false,"product_type":"c1","uid":2213995,"ip_address":"中国台湾","ucode":"FCDE6D237CC621","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Ge7uhlEVxicQT73YuomDPrVKI8UmhqxKWrhtO5GMNlFjrHWfd3HAjgaSribR4Pzorw8yalYGYqJI4VPvUyPzicSKg/132","comment_is_top":false,"comment_ctime":1695960476,"is_pvip":true,"replies":[{"id":139089,"content":"嗯，这个项目看起来是把ChatGPT整合到微信中。让我先研究一下这个项目先。有了解的同学也可以说一说。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1696060424,"ip_address":"新加坡","comment_id":381841,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"GITHUB上的类似WeChat chat类项目https:&#47;&#47;github.com&#47;zhayujie&#47;chatgpt-on-wechat，如何结合LangChain和huggingFace结合修改？找不到方向？老师可以加餐讲一讲吗？谢谢。","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628869,"discussion_content":"嗯，这个项目看起来是把ChatGPT整合到微信中。让我先研究一下这个项目先。有了解的同学也可以说一说。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696060424,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384549,"user_name":"Geek_cb5e16","can_delete":false,"product_type":"c1","uid":3787038,"ip_address":"上海","ucode":"C3AD2EE0103B1E","user_header":"","comment_is_top":false,"comment_ctime":1701154446,"is_pvip":false,"replies":[{"id":140925,"content":"差不多。这是搜到的内容，基本上解释的清晰。Pipeline更简易操作一些。\nHuggingFace和使用Pipeline HuggingFaceHUB存在一些差异。这些差异主要体现在使用方式和应用场景上。\nHuggingFace Pipeline: HuggingFace的Pipeline是一个简化的接口，用于从HuggingFace Hub加载和使用各种模型。它提供了一个通用的、易于使用的方法，用于各种任务，如文本分类、语音识别、图像分割等。即使您对特定的模型或任务不熟悉，也可以使用Pipeline轻松实现推理任务。Pipeline抽象了与特定任务相关的所有其他可用Pipeline，可以处理多种类型的输入，并自动加载默认模型和预处理类​​​​​​。\nHuggingFace Hub: HuggingFace Hub是一个平台，提供超过350,000个模型、75,000个数据集和150,000个演示应用程序（Spaces）。在Hub上，您可以找到、下载和使用各种任务的预训练模型。它是一个为机器学习社区提供协作和构建ML应用程序的开源平台。您可以使用Hub上的模型，但需要自己处理模型加载和推理逻辑，这与Pipeline提供的简化方法略有不同。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705071219,"ip_address":"瑞士","comment_id":384549,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师  要使用HuggingFace 和 Pipeline HuggingFaceHUB 有什么区别 还是不太明白","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635731,"discussion_content":"差不多。这是搜到的内容，基本上解释的清晰。Pipeline更简易操作一些。\nHuggingFace和使用Pipeline HuggingFaceHUB存在一些差异。这些差异主要体现在使用方式和应用场景上。\nHuggingFace Pipeline: HuggingFace的Pipeline是一个简化的接口，用于从HuggingFace Hub加载和使用各种模型。它提供了一个通用的、易于使用的方法，用于各种任务，如文本分类、语音识别、图像分割等。即使您对特定的模型或任务不熟悉，也可以使用Pipeline轻松实现推理任务。Pipeline抽象了与特定任务相关的所有其他可用Pipeline，可以处理多种类型的输入，并自动加载默认模型和预处理类​​​​​​。\nHuggingFace Hub: HuggingFace Hub是一个平台，提供超过350,000个模型、75,000个数据集和150,000个演示应用程序（Spaces）。在Hub上，您可以找到、下载和使用各种任务的预训练模型。它是一个为机器学习社区提供协作和构建ML应用程序的开源平台。您可以使用Hub上的模型，但需要自己处理模型加载和推理逻辑，这与Pipeline提供的简化方法略有不同。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705071219,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383209,"user_name":"Monin","can_delete":false,"product_type":"c1","uid":3144503,"ip_address":"上海","ucode":"AA6C4C4F19FA14","user_header":"https://static001.geekbang.org/account/avatar/00/2f/fb/37/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1698677984,"is_pvip":false,"replies":[{"id":139610,"content":"同学，微调模型的这个课题太大，几句话说不明白。你说的Loar应该是Lora？那只是多种微调策略中的一种。我也在学习中，我看到极客时间也有关于微调模型的公开课和训练营。等我学明白了，我也总结成一个专栏。我新出一个AI视频课里面，有一节涉及到一些微调的各种方法的基础知识，但较为皮毛。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698772095,"ip_address":"瑞士","comment_id":383209,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师 垂类模型   当有了相应领域数据后  一般用什么进行微调开源模型？ Loar？ finetune吗","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630683,"discussion_content":"同学，微调模型的这个课题太大，几句话说不明白。你说的Loar应该是Lora？那只是多种微调策略中的一种。我也在学习中，我看到极客时间也有关于微调模型的公开课和训练营。等我学明白了，我也总结成一个专栏。我新出一个AI视频课里面，有一节涉及到一些微调的各种方法的基础知识，但较为皮毛。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698772095,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382300,"user_name":"Monin","can_delete":false,"product_type":"c1","uid":3144503,"ip_address":"上海","ucode":"AA6C4C4F19FA14","user_header":"https://static001.geekbang.org/account/avatar/00/2f/fb/37/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1697017685,"is_pvip":false,"replies":[{"id":139272,"content":"同学可以看这个Link：https:&#47;&#47;python.langchain.com&#47;docs&#47;integrations&#47;llms&#47;\n给出了所有LangChain支持的模型的初始化示例代码。比如说https:&#47;&#47;python.langchain.com&#47;docs&#47;integrations&#47;llms&#47;chatglm\nfrom langchain.llms import ChatGLM\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# import os\n\ntemplate = &quot;&quot;&quot;{question}&quot;&quot;&quot;\nprompt = PromptTemplate(template=template, input_variables=[&quot;question&quot;])\n\n# default endpoint_url for a local deployed ChatGLM api server\nendpoint_url = &quot;http:&#47;&#47;127.0.0.1:8000&quot;\n\n# direct access endpoint in a proxied environment\n# os.environ[&#39;NO_PROXY&#39;] = &#39;127.0.0.1&#39;\n\nllm = ChatGLM(\n    endpoint_url=endpoint_url,\n    max_token=80000,\n    history=[[&quot;我将从美国到中国来旅游，出行前希望了解中国的城市&quot;, &quot;欢迎问我任何问题。&quot;]],\n    top_p=0.9,\n    model_kwargs={&quot;sample_model_args&quot;: False},\n)\n\n如果找不到你要的模型，那么，你只能通过https:&#47;&#47;python.langchain.com&#47;docs&#47;integrations&#47;llms&#47;huggingface_hub或者https:&#47;&#47;python.langchain.com&#47;docs&#47;integrations&#47;llms&#47;huggingface_pipelines 来导入所需要的模型。\n","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697160367,"ip_address":"瑞士","comment_id":382300,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师  用LangChain调用自定义语言模型时，在初始化具体模型时如何确定相对应的具体类名。比如目前性能最好的XwinLM模型，在HuggingFace上下载合适的模型后  如何找到对应的初始化类？","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629509,"discussion_content":"同学可以看这个Link：https://python.langchain.com/docs/integrations/llms/\n给出了所有LangChain支持的模型的初始化示例代码。比如说https://python.langchain.com/docs/integrations/llms/chatglm\nfrom langchain.llms import ChatGLM\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# import os\n\ntemplate = &#34;&#34;&#34;{question}&#34;&#34;&#34;\nprompt = PromptTemplate(template=template, input_variables=[&#34;question&#34;])\n\n# default endpoint_url for a local deployed ChatGLM api server\nendpoint_url = &#34;http://127.0.0.1:8000&#34;\n\n# direct access endpoint in a proxied environment\n# os.environ[&#39;NO_PROXY&#39;] = &#39;127.0.0.1&#39;\n\nllm = ChatGLM(\n    endpoint_url=endpoint_url,\n    max_token=80000,\n    history=[[&#34;我将从美国到中国来旅游，出行前希望了解中国的城市&#34;, &#34;欢迎问我任何问题。&#34;]],\n    top_p=0.9,\n    model_kwargs={&#34;sample_model_args&#34;: False},\n)\n\n如果找不到你要的模型，那么，你只能通过https://python.langchain.com/docs/integrations/llms/huggingface_hub或者https://python.langchain.com/docs/integrations/llms/huggingface_pipelines 来导入所需要的模型。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697160367,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3144503,"avatar":"https://static001.geekbang.org/account/avatar/00/2f/fb/37/791d0f5e.jpg","nickname":"Monin","note":"","ucode":"AA6C4C4F19FA14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629408,"discussion_content":"老师  我用llama类实验了下是可以的  应该是因为XwinLM基于Llama 2为基础进行微调？  那今后对于新的LLM 如何找到对应的初始化类？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697019827,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382209,"user_name":"balance","can_delete":false,"product_type":"c1","uid":1007182,"ip_address":"日本","ucode":"324D909BBE69DE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/4e/85502e98.jpg","comment_is_top":false,"comment_ctime":1696907665,"is_pvip":false,"replies":[{"id":139257,"content":"模型应该是很难在CPU上运行。可以查找一下量化以后的版本。包括llama-cpp这个库https:&#47;&#47;github.com&#47;ggerganov&#47;llama.cpp","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697018713,"ip_address":"新加坡","comment_id":382209,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师好! 本地电脑运行 Llama-2-7b-chat-hf 模型案例代码，需要什么硬件配置？我运行情况是，输出Loading checkpoint shards: 100%|██████████| 2&#47;2 [00:04&lt;00:00,  2.40s&#47;it] 后一直不动","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629406,"discussion_content":"模型应该是很难在CPU上运行。可以查找一下量化以后的版本。包括llama-cpp这个库https://github.com/ggerganov/llama.cpp","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697018713,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381388,"user_name":"无限可能","can_delete":false,"product_type":"c1","uid":1007654,"ip_address":"美国","ucode":"D3CE995904BF58","user_header":"https://static001.geekbang.org/account/avatar/00/0f/60/26/35ef9bef.jpg","comment_is_top":false,"comment_ctime":1695131940,"is_pvip":false,"replies":[{"id":138930,"content":"这个错误通常是Torch和你的CUDA Version不兼容。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695138460,"ip_address":"新加坡","comment_id":381388,"utype":1}],"discussion_count":5,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"MabBook M2上面的DEMO都运行不了，提示`AssertionError: Torch not compiled with CUDA enabled`，不知道是不是使用的方式不正确。","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628282,"discussion_content":"这个错误通常是Torch和你的CUDA Version不兼容。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695138461,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1650788,"avatar":"https://static001.geekbang.org/account/avatar/00/19/30/64/eb6d3a8a.jpg","nickname":"jackchow","note":"","ucode":"050F3FDFE8F808","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646947,"discussion_content":".to(&#34;cuda&#34;)改成.to(&#34;cpu&#34;)，mac 电脑好像不兼容 cuda 的方式，但是如果用 cpu 的方式跑速度很慢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719113266,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广西","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2409553,"avatar":"https://static001.geekbang.org/account/avatar/00/24/c4/51/5bca1604.jpg","nickname":"aLong","note":"","ucode":"11DB1B9C579811","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632843,"discussion_content":"我是intel 2019那个版本的本，我也遇到同样的情况。我看官网就是让 执行 `pip3 install torch torchvision torchaudio  ` 但是还这样。 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701422151,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1192961,"avatar":"https://static001.geekbang.org/account/avatar/00/12/34/01/30ca98e6.jpg","nickname":"arronK","note":"","ucode":"58DC6FBF2CF0C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628723,"discussion_content":"直接用 llama.cpp，Metal build 一下，发挥 M2 的潜力。速度跑的不错的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695742200,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1007654,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/60/26/35ef9bef.jpg","nickname":"无限可能","note":"","ucode":"D3CE995904BF58","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628285,"discussion_content":"Mac的M2系统是不是就不支持CUDA","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695139819,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386177,"user_name":"Alan","can_delete":false,"product_type":"c1","uid":1226806,"ip_address":"广东","ucode":"7185C21632D4A7","user_header":"https://static001.geekbang.org/account/avatar/00/12/b8/36/2e922a82.jpg","comment_is_top":false,"comment_ctime":1704269610,"is_pvip":false,"replies":[{"id":140824,"content":"嗯嗯，是啊，LangChain发展非常快。我目前在Github上面贴了一个requirements.txt文件。但是，估计跟着这个文件走，也会有Warning或者Error。\n下次新的课程，我会弄一个镜像环境，Docker之类的，方便同学复现。\n这次课程只好麻烦同学自己研究，更新。这也是学习的一部分。而且，LangChain的版本变化实在太快，看看新的版本也有好处。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1704774612,"ip_address":"瑞士","comment_id":386177,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"黄佳老师好，有没有可能弄一个可运行的镜像环境。每次运行实例程序都有各种环境问题，包括python库的版本兼容等。","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635373,"discussion_content":"嗯嗯，是啊，LangChain发展非常快。我目前在Github上面贴了一个requirements.txt文件。但是，估计跟着这个文件走，也会有Warning或者Error。\n下次新的课程，我会弄一个镜像环境，Docker之类的，方便同学复现。\n这次课程只好麻烦同学自己研究，更新。这也是学习的一部分。而且，LangChain的版本变化实在太快，看看新的版本也有好处。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704774612,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":3737316,"avatar":"https://static001.geekbang.org/account/avatar/00/39/06/e4/77d5b985.jpg","nickname":"Geek_b42160","note":"","ucode":"6838E5069BA742","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":641259,"discussion_content":"老师 docker镜像这会有出吗？我看requments.txt这里没有lama_cpp的版本，我这边报错：ror loading model: unknown (magic, version) combination: 67676a74, 00000003; is this really a GGML file? 似乎是lama_cpp版本不匹配 。看模型里面的提示：自2023年8月21日起，llama.cpp不再支持GGML型号。完全不知道哪个版本可用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712417332,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":635373,"ip_address":"浙江","group_id":0},"score":641259,"extra":""}]}]},{"had_liked":false,"id":385335,"user_name":"Charlie","can_delete":false,"product_type":"c1","uid":3717083,"ip_address":"新加坡","ucode":"B0E8E54F5DC893","user_header":"https://static001.geekbang.org/account/avatar/00/38/b7/db/dd23d4c3.jpg","comment_is_top":false,"comment_ctime":1702458403,"is_pvip":false,"replies":[{"id":140843,"content":"&quot;gguf_init_from_file: invalid magic characters &#39;tjgg&#39;&quot; 通常表明您尝试加载的模型文件格式或内容不正确，版本不兼容之类的。换了模型之后通了吗？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1704816566,"ip_address":"瑞士","comment_id":385335,"utype":1}],"discussion_count":3,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"llama-cpp-python v0.2.22 加载 llama-2-7b-chat.ggmlv3.q4_K_S.bin 时报错，错误信息如下：\ngguf_init_from_file: invalid magic characters &#39;tjgg&#39;\nerror loading model: llama_model_loader: failed to load model from &#47;Users&#47;xxx&#47;PycharmProjects&#47;hello_langchain&#47;OfflineModel&#47;llama-2-7b-chat.ggmlv3.q4_K_S.bin\n\n准备尝试用gguf model （llama-2-7b-chat.Q4_K_S.gguf）去加载。\nhttps:&#47;&#47;huggingface.co&#47;TheBloke&#47;Llama-2-7B-Chat-GGUF&#47;tree&#47;main\n","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635426,"discussion_content":"&#34;gguf_init_from_file: invalid magic characters &#39;tjgg&#39;&#34; 通常表明您尝试加载的模型文件格式或内容不正确，版本不兼容之类的。换了模型之后通了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704816566,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1052191,"avatar":"https://static001.geekbang.org/account/avatar/00/10/0e/1f/d0472177.jpg","nickname":"厉害了我的国","note":"","ucode":"CD0A54A1B998AA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":638682,"discussion_content":"也遇到了这个问题，llama-cpp-python的版本是0.2.55","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1709798465,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3717083,"avatar":"https://static001.geekbang.org/account/avatar/00/38/b7/db/dd23d4c3.jpg","nickname":"Charlie","note":"","ucode":"B0E8E54F5DC893","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":637318,"discussion_content":"通了，谢谢。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1708337308,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384609,"user_name":"高浩宇","can_delete":false,"product_type":"c1","uid":2267923,"ip_address":"江苏","ucode":"2B158EE9541F23","user_header":"https://static001.geekbang.org/account/avatar/00/22/9b/13/47feb437.jpg","comment_is_top":false,"comment_ctime":1701236860,"is_pvip":false,"replies":[{"id":140417,"content":"英文会是Llama2，中文可以选择智谱的模型。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1702312184,"ip_address":"瑞士","comment_id":384609,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"Llama 2和智谱AI的ChatGLM3-6b哪个适合用来做基础模型","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633571,"discussion_content":"英文会是Llama2，中文可以选择智谱的模型。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702312184,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1943234,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJL1f6hdtibKfZL0brOBrNNw1iaiafibg9byrsMkO5rOoeXkTFSzket0bbVDNdNV0nnpkhMVa7MGBgCOw/132","nickname":"黄维","note":"","ucode":"7B8B75DC9E2E63","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":640514,"discussion_content":"老师您好，我最近也做中文的某领域的垂直模型，基于 LLM 去 Fine Tuning，我想找到适合我场景的 LLM，目前初定 ChatGLM3，但是暂时没有找到权威的数据来说明 ChatGLM3 比 Llama2 要更好，我是新人，请问我应该如何去得到证据和结果呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1711551879,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384547,"user_name":"Geek_cb5e16","can_delete":false,"product_type":"c1","uid":3787038,"ip_address":"上海","ucode":"C3AD2EE0103B1E","user_header":"","comment_is_top":false,"comment_ctime":1701154339,"is_pvip":false,"replies":[{"id":140922,"content":"对啊。但是开源模型需要部署再本地GPU上面也是一些成本。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705070582,"ip_address":"瑞士","comment_id":384547,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"大规模使用语言模型的时候 适合使用开源模型 \n量大的时候时候私有化模型会减少一部分成本\n","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635728,"discussion_content":"对啊。但是开源模型需要部署再本地GPU上面也是一些成本。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705070582,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383733,"user_name":"Liberalism","can_delete":false,"product_type":"c1","uid":1233947,"ip_address":"北京","ucode":"BD0A293B928668","user_header":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","comment_is_top":false,"comment_ctime":1699524047,"is_pvip":false,"replies":[{"id":140197,"content":"现在拿到了吗？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1700583801,"ip_address":"瑞士","comment_id":383733,"utype":1}],"discussion_count":9,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"我的 Llama 2 已经申请两天了，还在待审核中","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632246,"discussion_content":"现在拿到了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700583801,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1233947,"avatar":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","nickname":"Liberalism","note":"","ucode":"BD0A293B928668","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":632784,"discussion_content":"黄佳老师，目前没有拿到，因为有其他代替，也没着急","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701337802,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":632246,"ip_address":"北京","group_id":0},"score":632784,"extra":""}]},{"author":{"id":2409553,"avatar":"https://static001.geekbang.org/account/avatar/00/24/c4/51/5bca1604.jpg","nickname":"aLong","note":"","ucode":"11DB1B9C579811","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632445,"discussion_content":"我申请很快，在huggingFace申请时，上面那个有一个链接，里面留的邮箱要和这边一样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700788271,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3781054,"avatar":"https://static001.geekbang.org/account/avatar/00/39/b1/be/99930181.jpg","nickname":"派大星的菠萝包🍍","note":"","ucode":"C1EBE63783BD69","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632081,"discussion_content":"请问您现在通过了吗？我已经3天了，还没通过","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700459808,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":5,"child_discussions":[{"author":{"id":1233947,"avatar":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","nickname":"Liberalism","note":"","ucode":"BD0A293B928668","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":3781054,"avatar":"https://static001.geekbang.org/account/avatar/00/39/b1/be/99930181.jpg","nickname":"派大星的菠萝包🍍","note":"","ucode":"C1EBE63783BD69","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632161,"discussion_content":"到今天都没有通过","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700536396,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":632081,"ip_address":"天津","group_id":0},"score":632161,"extra":""},{"author":{"id":3781054,"avatar":"https://static001.geekbang.org/account/avatar/00/39/b1/be/99930181.jpg","nickname":"派大星的菠萝包🍍","note":"","ucode":"C1EBE63783BD69","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1233947,"avatar":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","nickname":"Liberalism","note":"","ucode":"BD0A293B928668","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632592,"discussion_content":"您拿到了吗？我拿到了，发现是有个地方的信息没填，才一直没通过","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701049367,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":632161,"ip_address":"上海","group_id":0},"score":632592,"extra":""},{"author":{"id":1233947,"avatar":"https://static001.geekbang.org/account/avatar/00/12/d4/1b/6444e933.jpg","nickname":"Liberalism","note":"","ucode":"BD0A293B928668","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":3781054,"avatar":"https://static001.geekbang.org/account/avatar/00/39/b1/be/99930181.jpg","nickname":"派大星的菠萝包🍍","note":"","ucode":"C1EBE63783BD69","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632783,"discussion_content":"您快说下是哪里没有填写？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701337780,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":632592,"ip_address":"北京","group_id":0},"score":632783,"extra":""}]}]},{"had_liked":false,"id":382642,"user_name":"Geek_995b81","can_delete":false,"product_type":"c1","uid":3733604,"ip_address":"广东","ucode":"D99E0838B97C0B","user_header":"","comment_is_top":false,"comment_ctime":1697712018,"is_pvip":false,"replies":[{"id":139451,"content":"哦，那么你需要用这个rest API实现你自己的custom LLM类接口，把他们的回复功能视为你自己的LLM。详情参考：https:&#47;&#47;python.langchain.com&#47;docs&#47;modules&#47;model_io&#47;models&#47;llms&#47;custom_llm","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698141890,"ip_address":"瑞士","comment_id":382642,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"老师请问一个问题，我是用了其他同学私有化部署的chatGLM模型，他们提供了一个rest api的接口给我去掉，相当于一个chat功能的接口，如果我自己用langchain去接，是应该找他们要chatGLM部署的服务端口吗？我有点弄不清楚","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630157,"discussion_content":"哦，那么你需要用这个rest API实现你自己的custom LLM类接口，把他们的回复功能视为你自己的LLM。详情参考：https://python.langchain.com/docs/modules/model_io/models/llms/custom_llm","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698141890,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1329887,"avatar":"https://static001.geekbang.org/account/avatar/00/14/4a/df/5775a407.jpg","nickname":"frank","note":"","ucode":"11DA7E7BCC2874","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629912,"discussion_content":"同文: 如何需要访问自己封装的OpenAi接口(rest api), 如何使用LangChain呢? 请老师帮忙解答下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697790558,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382516,"user_name":"抽象派","can_delete":false,"product_type":"c1","uid":2599971,"ip_address":"广东","ucode":"6879F90CB702FC","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/YflLdCdbUAkfr9LPzF50EibDrMxBibPicQ5NNAETaPP0ytTmuR3h6QNichDMhDbR2XelSIXpOrPwbiaHgBkMJYOeULA/132","comment_is_top":false,"comment_ctime":1697512374,"is_pvip":false,"replies":[{"id":139449,"content":"应该是模型本身说话比较简单吧。可以在HuggingFace的Playground里面用网页版的API试试这个模型的效果么？可以考虑在提示语里面加入：请给我超过200字的回答，细致一点。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698141391,"ip_address":"瑞士","comment_id":382516,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"老师，试用了 ‘bigcode&#47;starcoder’，但是生成的内容没有openai的多。设置了max_length也没有用，可以帮我看看是不是我的使用方式不对吗？\ndef start_coder() -&gt; BaseLanguageModel:\n    llm = HuggingFaceHub(\n        repo_id=&quot;bigcode&#47;starcoder&quot;, model_kwargs={&quot;max_length&quot;: 16384, &quot;temperature&quot;: 0.5}\n    )\n    return llm","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630155,"discussion_content":"应该是模型本身说话比较简单吧。可以在HuggingFace的Playground里面用网页版的API试试这个模型的效果么？可以考虑在提示语里面加入：请给我超过200字的回答，细致一点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698141391,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382355,"user_name":"庄楚斌","can_delete":false,"product_type":"c1","uid":1355263,"ip_address":"广东","ucode":"DFD2AF81A6BA31","user_header":"https://static001.geekbang.org/account/avatar/00/14/ad/ff/e7ed1a08.jpg","comment_is_top":false,"comment_ctime":1697102928,"is_pvip":false,"replies":[{"id":139296,"content":"目前我版本升级之后langchain                 0.0.305","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697451990,"ip_address":"瑞士","comment_id":382355,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"site-packages&#47;langchain&#47;llms&#47;base.py&quot;, line 607, in generate\n    self.callbacks,\n    ^^^^^^^^^^^^^^ object has no attribute &#39;callbacks&#39;  langchain 版本是多少呢？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629633,"discussion_content":"目前我版本升级之后langchain                 0.0.305","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697451990,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382354,"user_name":"庄楚斌","can_delete":false,"product_type":"c1","uid":1355263,"ip_address":"广东","ucode":"DFD2AF81A6BA31","user_header":"https://static001.geekbang.org/account/avatar/00/14/ad/ff/e7ed1a08.jpg","comment_is_top":false,"comment_ctime":1697102891,"is_pvip":false,"replies":[{"id":139297,"content":"同学留言重复了哈 目前我版本升级之后langchain                 0.0.305","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697452008,"ip_address":"瑞士","comment_id":382354,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"site-packages&#47;langchain&#47;llms&#47;base.py&quot;, line 607, in generate\n    self.callbacks,\n    ^^^^^^^^^^^^^^","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629634,"discussion_content":"同学留言重复了哈 目前我版本升级之后langchain                 0.0.305","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697452008,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382172,"user_name":"Geek_2a4c8d","can_delete":false,"product_type":"c1","uid":3721202,"ip_address":"广东","ucode":"4D18496AF38971","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqFqGgBQU7libL1KIov7X5SqNUgIztTOcCX584JcVD8wejgqtbaoBKkFGRBXXkLA1Q57afep1ibBNqw/132","comment_is_top":false,"comment_ctime":1696833298,"is_pvip":false,"replies":[{"id":139246,"content":"这个模型对于你的MacBook来说太大了，你可以试一试llama-cpp-python这个库来对模型进行量化，然后再CPU上运行。再Github上搜一下llama-cpp即可。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1696988555,"ip_address":"瑞士","comment_id":382172,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"老师请教一个问题，我在MacBook上运行案例 macos 13.6。报：RuntimeError: MPS backend out of memory (MPS allocated: 6.69 GB, other allocations: 8.35 MB, max allowed: 6.77 GB). Tried to allocate 172.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n。 按照提示将 PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 配置到环境变量了 ，也参考了一些网上办法，都没解决。","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629364,"discussion_content":"这个模型对于你的MacBook来说太大了，你可以试一试llama-cpp-python这个库来对模型进行量化，然后再CPU上运行。再Github上搜一下llama-cpp即可。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696988555,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382045,"user_name":"棟","can_delete":false,"product_type":"c1","uid":2050798,"ip_address":"广东","ucode":"D85800A3BA3CF8","user_header":"https://static001.geekbang.org/account/avatar/00/1f/4a/ee/fe035424.jpg","comment_is_top":false,"comment_ctime":1696593681,"is_pvip":false,"replies":[{"id":139181,"content":"其它同学看看有没有什么好的建议呢？\ntransformers库会将模型下载到~&#47;.cache&#47;huggingface&#47;transformers&#47;目录中。如果知道模型的URL，您可以手动下载模型，然后放在模型目录中，然后加载本地模型。\n","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1696674605,"ip_address":"瑞士","comment_id":382045,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"老师，请教一个疑问，通过代码下载模型到对应设备的时候总是网络中断，导致下载失败，有没有网络中断仍然可以恢复下载的方法：\npipeline = transformers.pipeline( &quot;text-generation&quot;,    model=model,    torch_dtype=torch.float16, device_map=&quot;auto&quot;,    max_length = 1000)\nllm = HuggingFacePipeline(pipeline = pipeline,   model_kwargs = {&#39;temperature&#39;:0})\n有了解的朋友也请指点一下，感谢！\nDownloading shards:   0%|                                                                                                           | 0&#47;2 [15:49&lt;?, ?it&#47;s]\nDownloading (…)of-00002.safetensors:   7%|████▉                                                                      | 650M&#47;9.98G [15:48&lt;3:46:43, 686kB&#47;s]","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629112,"discussion_content":"其它同学看看有没有什么好的建议呢？\ntransformers库会将模型下载到~/.cache/huggingface/transformers/目录中。如果知道模型的URL，您可以手动下载模型，然后放在模型目录中，然后加载本地模型。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696674605,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2050798,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/4a/ee/fe035424.jpg","nickname":"棟","note":"","ucode":"D85800A3BA3CF8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629117,"discussion_content":"感谢老师解疑，观察window系统在用户.cache/huggingface/hub/有对应的模型名称目录。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696677813,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381852,"user_name":"zhhuyi","can_delete":false,"product_type":"c1","uid":1444450,"ip_address":"上海","ucode":"3D24A407AE0ABF","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erXRSYibXVPR8OMP7ZQxFPg6nn9vlcqUH1QeFibdk0HJRRSoYiaZiblHZicgicJ8OEicttqyI70jZl0y8iaPA/132","comment_is_top":false,"comment_ctime":1695992329,"is_pvip":false,"replies":[{"id":139085,"content":"中秋快乐！🏮","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1696057012,"ip_address":"新加坡","comment_id":381852,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"怎么微调模型呢？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628864,"discussion_content":"中秋快乐！🏮","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696057013,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381616,"user_name":"Monin","can_delete":false,"product_type":"c1","uid":3144503,"ip_address":"上海","ucode":"AA6C4C4F19FA14","user_header":"https://static001.geekbang.org/account/avatar/00/2f/fb/37/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1695544399,"is_pvip":false,"replies":[{"id":139015,"content":"这个我也没有什么好办法，相信与科学上网的方式有关。可以去HuggingFace中文社区讨论讨论。HuggingFace也有微信公众号，可以留言问问看。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695702596,"ip_address":"新加坡","comment_id":381616,"utype":1}],"discussion_count":2,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"老师 https:&#47;&#47;huggingface.co&#47; 官网经常访问不通，老师有什么稳定访问的办法吗？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628660,"discussion_content":"这个我也没有什么好办法，相信与科学上网的方式有关。可以去HuggingFace中文社区讨论讨论。HuggingFace也有微信公众号，可以留言问问看。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695702596,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1059926,"avatar":"https://static001.geekbang.org/account/avatar/00/10/2c/56/ff7a9730.jpg","nickname":"许灵","note":"","ucode":"0296EC9929B570","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":640879,"discussion_content":"可以用镜像：https://hf-mirror.com/","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1711961287,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381342,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1695084891,"is_pvip":false,"replies":[{"id":138913,"content":"import torch\nif torch.cuda.is_available():\n    print(&quot;GPU is available!&quot;)\n    print(&quot;GPU Name:&quot;, torch.cuda.get_device_name(0))\nelse:\n    print(&quot;No GPU detected!&quot;)\n一般的笔记本是没有GPU的，除了特别的配置，比如HP ZBook.\n在设备管理器中，显示适配器。如果除了集成的Intel显卡外，还有其他显卡（如NVIDIA或AMD），则表示有独立的GPU。\n我们的课程不需要GPU，因为我们不训练模型。\n","user_name":"作者回复","user_name_real":"作者","uid":1809833,"ctime":1695089789,"ip_address":"新加坡","comment_id":381342,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"怎么知道电脑上是否有GPU？\n我的笔记本是16年年底买的，惠普笔记本，CPU是i7 8核，设备管理器里面好像没有看到GPU。","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628213,"discussion_content":"import torch\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"No GPU detected!\")\n一般的笔记本是没有GPU的，除了特别的配置，比如HP ZBook.\n在设备管理器中，显示适配器。如果除了集成的Intel显卡外，还有其他显卡（如NVIDIA或AMD），则表示有独立的GPU。\n我们的课程不需要GPU，因为我们不训练模型。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695089789,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381332,"user_name":"阿斯蒂芬","can_delete":false,"product_type":"c1","uid":1024164,"ip_address":"广东","ucode":"61D5E3BDA4EBC5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg","comment_is_top":false,"comment_ctime":1695049115,"is_pvip":false,"replies":[{"id":138911,"content":"慢慢来，一项一项跑通，同学你会学到很多！！","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695086398,"ip_address":"新加坡","comment_id":381332,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"预训练、微调、量化，这几个一知半解的概念，在这篇文章中捋清了不少，🙏\n这几课都还没来得及跟上实操上机，惭愧惭愧，先mark","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628205,"discussion_content":"慢慢来，一项一项跑通，同学你会学到很多！！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695086399,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381298,"user_name":"AI编程派","can_delete":false,"product_type":"c1","uid":1009446,"ip_address":"江苏","ucode":"117FBEB03C80F2","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/26/6f7b9fd8.jpg","comment_is_top":false,"comment_ctime":1695004347,"is_pvip":false,"replies":[{"id":138891,"content":"同学用同样的步骤，试一下咱们中文的模型ChatGLM或者百川？看看能用不？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695009114,"ip_address":"新加坡","comment_id":381298,"utype":1}],"discussion_count":5,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"Llama 申请下载的时候遇到两个问题：\n1：Country 选中China, 就提示 Sorry, the download is not available in your region.\n2：Country 选项中有Taiwan, zz问题 :)","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628145,"discussion_content":"同学用同样的步骤，试一下咱们中文的模型ChatGLM或者百川？看看能用不？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695009114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1899757,"avatar":"","nickname":"yanyu-xin","note":"","ucode":"3AA389F9E4C236","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648391,"discussion_content":"Country，我选HongKong ， 科学上网也是通过香港，联系用了国内真实号码，申请很快通过。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1721525327,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1192961,"avatar":"https://static001.geekbang.org/account/avatar/00/12/34/01/30ca98e6.jpg","nickname":"arronK","note":"","ucode":"58DC6FBF2CF0C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628724,"discussion_content":"挂载美国节点的梯子申请。或者找别人 fork 过去的下载","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695742300,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1177248,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f6/a0/8ea0bfba.jpg","nickname":"Yimmy","note":"","ucode":"E66EAF7050C74A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628461,"discussion_content":"同问，也遇到了一样的问题，Country 选中China, 就提示 Sorry, the download is not available in your region.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695312405,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2409553,"avatar":"https://static001.geekbang.org/account/avatar/00/24/c4/51/5bca1604.jpg","nickname":"aLong","note":"","ucode":"11DB1B9C579811","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1177248,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f6/a0/8ea0bfba.jpg","nickname":"Yimmy","note":"","ucode":"E66EAF7050C74A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632446,"discussion_content":"那就选其他国家。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700788333,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":628461,"ip_address":"北京","group_id":0},"score":632446,"extra":""}]}]},{"had_liked":false,"id":381289,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"广东","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1694999543,"is_pvip":true,"replies":[{"id":138889,"content":"Llama_Index，在RAG这个领域，无论从Meta Data的创建，索引的创建，嵌入节点的结构创建，检索的方式。其思路和具体实现的粒度都是比LangChain更细的。这个我在开篇词的时候针对同学的问题回答过一遍了。如果任务需要更精细的RAG，需要考虑Llama_Index。但是LangChain也会在这个领域增加投入吧。二者是有重合的。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695008298,"ip_address":"新加坡","comment_id":381289,"utype":1}],"discussion_count":3,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"llama_index 怎么样？ 很多功能 langchain 已经有了， llama_index 这个的价值体现在哪里？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628143,"discussion_content":"Llama_Index，在RAG这个领域，无论从Meta Data的创建，索引的创建，嵌入节点的结构创建，检索的方式。其思路和具体实现的粒度都是比LangChain更细的。这个我在开篇词的时候针对同学的问题回答过一遍了。如果任务需要更精细的RAG，需要考虑Llama_Index。但是LangChain也会在这个领域增加投入吧。二者是有重合的。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1695008299,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2657690,"avatar":"https://static001.geekbang.org/account/avatar/00/28/8d/9a/6c9a252e.jpg","nickname":"Geek_7ca963","note":"","ucode":"610B7FD6D30207","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628276,"discussion_content":"赞 最近也一直困惑这个问题 现在看起来和搜索的结合还是得 llamaindex 而且langchain可以配合着llama index一起用？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695133337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2657690,"avatar":"https://static001.geekbang.org/account/avatar/00/28/8d/9a/6c9a252e.jpg","nickname":"Geek_7ca963","note":"","ucode":"610B7FD6D30207","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628344,"discussion_content":"可以结合起来一起用 没问题。😻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695216149,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":628276,"ip_address":"新加坡","group_id":0},"score":628344,"extra":""}]}]},{"had_liked":false,"id":392699,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1721568960,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"逐一运行老师的课程代码。\n1、&quot;申请使用 Meta 的 Llama2 模型&quot;，填写了香港地址，国内移动号码，通过香港科学上网，很快批准下载模型。\n\n2、“通过 HuggingFace 调用 Llama”的”01_HugggingFace_Llama.py“\n（1）因为是手提电脑，没有GPU, 将模型调到CPU上预训练：\n旧代码：inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)\n新代码： inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cpu&quot;)\n（2）运行时间很久，加上自动下载模型，有几个小时。运行结果与课程基本一致。也是将“玫瑰”作为女孩名字来编故事的。\n\n3、“通过 HuggingFace Hub”，运行代码结果也是一个词：“flower&quot;\n\n4、“通过 HuggingFace Pipeline”，运行代码结果杂乱。\n\n5、 在”用 LangChain 调用自定义语言模型“的”04_LangChain_CustomizeModel.py“，问题：\n（1）安装 llama-cpp-python 时遇到错误，要在VS 中安装编译工具和配置环境；\n（2）出现 ValueError: Failed to load model from file 错误。这是课程中采用llama-2-7b-chat.ggmlv3.q4_K_S.bin 模型，但是GGML 格式已经被 GGUF 格式取代，且 llama.cpp 从 2023 年 8 月 21 日起不再支持 GGML 模型文件。 下载改用”TheBloke&#47;Llama-2-7B-Chat-GGUF“模型，修改：\n旧代码： MODEL_NAME = &#39;llama-2-7b-chat.ggmlv3.q4_K_S.bin&#39;\n新代码： MODEL_NAME = &quot;llama-2-7b-chat.Q4_K_M.gguf&quot;\n（3）运行结果与课程基本一致，也是英文，翻译后，发现回答结果很不错：\n“感谢您就您最近的购买与我们联系。很抱歉，您收到的鲜花在交货两天后状况不佳。我们理解这对你来说一定是多么令人失望，特别是因为它们是作为送给你女朋友的礼物。\n我们想向您保证，我们认真对待这些问题，并正在立即调查此事。我们的团队将调查该问题，并与我们的供应商合作，以确保我们的鲜花具有最高的质量和新鲜度。我们还将根据您的喜好为您提供鲜花的全额退款或更换。\n请知道，我们重视您的满意，并感谢您对此事的耐心和理解。如果还有什么我们可以为您提供帮助的，请随时与我们联系。”","like_count":3},{"had_liked":false,"id":389622,"user_name":"在路上1619","can_delete":false,"product_type":"c1","uid":2909770,"ip_address":"上海","ucode":"A2F828C18FBCD4","user_header":"https://static001.geekbang.org/account/avatar/00/2c/66/4a/a5ecf67a.jpg","comment_is_top":false,"comment_ctime":1713098440,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"黄老师好，我运行第一个例子前后花了将近8分钟，用的是4090的GPU，体验下来这个速度还是太慢了。如果生产上部署70b的模型，一般选什么样的GPU？","like_count":2},{"had_liked":false,"id":392387,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1720767101,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":2,"score":4,"product_id":100617601,"comment_content":"第6讲打卡~\n确实就像老师说的，进入大模型领域，仿佛打开了新世界的大门，需要学习和理解的东西非常多，感觉每天都充满了对知识的渴望😂","like_count":1},{"had_liked":false,"id":388237,"user_name":"StopLiu","can_delete":false,"product_type":"c1","uid":1007344,"ip_address":"北京","ucode":"1553FA4E3A285D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/f0/fab69114.jpg","comment_is_top":false,"comment_ctime":1709693193,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100617601,"comment_content":"老师好，私有化部署，如何通过 langchain 调用本地模型的 embedding？","like_count":1},{"had_liked":false,"id":394490,"user_name":"杰","can_delete":false,"product_type":"c1","uid":1181014,"ip_address":"广东","ucode":"126701C4A6D5A7","user_header":"https://static001.geekbang.org/account/avatar/00/12/05/56/ed492eba.jpg","comment_is_top":false,"comment_ctime":1726932282,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100617601,"comment_content":"老师，为什么用transformers运行就要怎么慢，我还是用gpu运行的。但是ollama运行相同模型，在cpu上运行，都是非常快的","like_count":0},{"had_liked":false,"id":394448,"user_name":"刘双荣","can_delete":false,"product_type":"c1","uid":3870283,"ip_address":"北京","ucode":"FB42C207F24A18","user_header":"https://static001.geekbang.org/account/avatar/00/3b/0e/4b/ff4a21de.jpg","comment_is_top":false,"comment_ctime":1726822304,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100617601,"comment_content":"时间 24-09-20  ，复制代码提示 很多方法过期，不再支持，建义更新个最新版的，或是指定对应的 版本 ，","like_count":0},{"had_liked":false,"id":392937,"user_name":"Miyazaki Hayao","can_delete":false,"product_type":"c1","uid":3926086,"ip_address":"越南","ucode":"A7CFB188EDFE7D","user_header":"https://static001.geekbang.org/account/avatar/00/3b/e8/46/12f29c2b.jpg","comment_is_top":false,"comment_ctime":1722232295,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100617601,"comment_content":"老师，最后执行的时候，出现这样的错误，应该怎么解决呀：\nValueError: Failed to load model from file: E:&#47;Jztop&#47;models&#47;llama&#47;llama-2-7b-chat.ggmlv3.q4_K_S.bin\nException ignored in: &lt;function Llama.__del__ at 0x000002A42047C7C0&gt;\nTraceback (most recent call last):\n  File &quot;d:\\Jztop\\anaconda3\\envs\\langchain\\Lib\\site-packages\\llama_cpp\\llama.py&quot;, line 2089, in __del__\n    if self._lora_adapter is not None:\n       ^^^^^^^^^^^^^^^^^^\nAttributeError: &#39;Llama&#39; object has no attribute &#39;_lora_adapter&#39;","like_count":0},{"had_liked":false,"id":387084,"user_name":"海是蓝天的倒影","can_delete":false,"product_type":"c1","uid":2300783,"ip_address":"上海","ucode":"18355BB7DC364D","user_header":"https://static001.geekbang.org/account/avatar/00/23/1b/6f/ee41e363.jpg","comment_is_top":false,"comment_ctime":1706408951,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":1,"score":5,"product_id":100617601,"comment_content":"```\n# 自定义的LLM类，继承自基础LLM类\nclass CustomLLM(LLM):\n    model_name = MODEL_NAME\n    \n    # 该方法用于使用Llama库调用模型生成回复\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        prompt_length = len(prompt) + 5\n        print(&#39;prompt_length&#39;, prompt_length, Llama(model_path=MODEL_PATH+MODEL_NAME, n_threads=4))\n        # 初始化Llama模型，指定模型线路和线程数\n        llm = Llama(model_path=MODEL_PATH+MODEL_NAME, n_threads=4)\n        # 使用Llama模型生成回复\n        response = llm(f&quot;Q: {prompt} A: &quot;, max_tokens=256)\n        \n        print(&#39;response&#39;, response)\n        # 从返回的回复中提取文本部分\n        output = response[&#39;choices&#39;][0][&#39;text&#39;].replace(&#39;A: &#39;, &#39;&#39;).strip()\n        \n        # 返回生成的回复，同时剔除了问题部分和额外字符\n        return output[prompt_length:]\n    \n    # 返回模型的标识参数，这里只是返回模型的名称\n    @property\n    def _identifying_params(self) -&gt; Mapping[str, Any]:\n        return {&quot;name_of_model&quot;: self.model_name}\n    \n     #返回模型的类型 这里是”custom“\n    @property\n    def _llm_type(self) -&gt; str:\n        return &quot;custom&quot;\n```\n```response = llm(f&quot;Q: {prompt} A: &quot;, max_tokens=256)```\n老师，文章自定义模型微调中这行代码不太理解，麻烦请教下","like_count":0}]}