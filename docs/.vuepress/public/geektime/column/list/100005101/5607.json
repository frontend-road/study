{"id":5607,"title":"17 | 结合上下文信息的Bandit算法","content":"<p>上一篇文章我说到，Bandit算法用的是一种走一步看一步的思路，这一点看上去非常佛系，似乎一点都不如机器学习深度学习那样厚德载物，但是且慢下结论，先看看我在前面介绍的那几个Bandit算法。</p>\n<h2>UCB回顾</h2>\n<p>这些Bandit算法，都有一个特点：完全没有使用候选臂的特征信息。特征可是机器学习的核心要素，也是机器学习泛化推广的依赖要素。</p>\n<p>没有使用特征信息的Bandit算法，问题就在于只能对当前已有的这些候选臂进行选择，对于新加入的候选只能从0开始积累数据，而不能借助已有的候选泛化作用。</p>\n<p>举个例子，假如有一个用户是鹿晗的粉丝，通过Bandit算法有两个鹿晗的广告得到展示，并得到了较好的收益。</p>\n<p>那么对于一个新的广告，如果具有鹿晗这个特征，直觉上前两个鹿晗广告的收益信息可以泛化到当前新广告上，新广告就不是完全从0开始积累数据，而是有了一定的基础，这样的收敛会更快。</p>\n<p>UCB和汤普森采样这两个Bandit算法在实际中表现很好。于是，前辈们就决定送UCB去深造一下，让它能够从候选臂的特征信息中学到一些知识。</p>\n<p>UCB就是置信上边界的简称，所以UCB这个名字就反映了它的全部思想。置信区间可以简单直观地理解为不确定性的程度，区间越宽，越不确定，反之就很确定。</p>\n<ol>\n<li>每个候选的回报均值都有个置信区间，随着试验次数增加，置信区间会变窄，相当于逐渐确定了到底是回报丰厚还是亏了。</li>\n<li>每次选择前，都根据已经试验的结果重新估计每个候选的均值及置信区间。</li>\n<li>选择置信区间上界最大的那个候选。</li>\n</ol>\n<p>“选择置信区间上界最大的那个候选”，这句话反映了几个意思：</p>\n<ol>\n<li>如果候选的收益置信区间很宽，相当于被选次数很少，还不确定，那么它会倾向于被多次选择，这个是算法冒风险的部分；</li>\n<li>如果候选的置信区间很窄，相当于被选次数很多，比较确定其好坏了，那么均值大的倾向于被多次选择，这个是算法保守稳妥的部分；</li>\n<li>UCB是一种乐观冒险的算法，它每次选择前根据置信区间上界排序，反之如果是悲观保守的做法，可以选择置信区间下界排序。</li>\n</ol>\n<h2>LinUCB</h2>\n<p>“Yahoo!”的科学家们在2010年基于UCB提出了LinUCB算法，它和传统的UCB算法相比，最大的改进就是加入了特征信息，每次估算每个候选的置信区间，不再仅仅是根据实验，而是根据特征信息来估算，这一点就非常的“机器学习”了。</p>\n<p>在广告推荐领域，每一个选择的样本，由用户和物品一起构成，用户特征，物品特征，其他上下文特征共同表示出这个选择，把这些特征用来估计这个选择的预期收益和预期收益的置信区间，就是LinUCB要做的事情。</p>\n<p>LinUCB算法做了一个假设：一个物品被选择后推送给一个用户，其收益和特征之间呈线性关系。在具体原理上，LinUCB有一个简单版本以及一个高级版本。简单版本其实就是让每一个候选臂之间完全互相无关，参数不共享。高级版本就是候选臂之间共享一部分参数。</p>\n<p>先从简单版本讲起。</p>\n<p>还是举个例子，假设现在两个用户，用户有一个特征就是性别，性别特征有两个维度，男，女。现在有四个商品要推荐给这两个用户，示意如下。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/ea/4b/eae9254fef3cdfc681247f58fa740b4b.png?wh=480*256\" alt=\"\" /></p>\n<p>两个用户就是Bandit算法要面对的上下文，表示成特征就是下面的样子。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d4/ae/d47f7f976d7a0fd651f8f4be67f8aeae.png?wh=580*408\" alt=\"\" /></p>\n<p>每一次推荐时，用特征和每一个候选臂的参数去预估它的预期收益和置信区间。</p>\n<p>$x_{i}\\times\\theta_{j}$，这就是给男性用户推荐剃须刀，给女性用户推荐口红，即使是新用户，也可以作出比随机猜测好的推荐，再观察用户是否会点击，用点击信息去更新那个被推荐了的候选臂的参数。</p>\n<p>这里的例子还简化了一个地方，就是没有计算置信区间，这是UCB的精髓。下面来补上。</p>\n<p>假如D是候选臂在m次被选择中积累的特征，相当于就是m条样本，特征维度是d，所以D是一个矩阵，维度是m x d。</p>\n<p>这m次被选择，每次得到用户的点击或者没点击，把这个反馈信息记录为一个m x 1的向量，叫做C。所以这个候选臂对应的参数就是d x 1的向量，d就是特征维度数，记录为一个戴帽子的西塔，$\\hat{\\theta}$。</p>\n<p>按照LinUCB认为，参数和特征之间线性相乘就应该得到收益：</p>\n<p>$$D_{m \\times d} \\times \\hat{\\theta_{d \\times 1}} = C_{m \\times 1}$$<br />\n你看D也知道，C也知道，要求 $\\theta$ ，这就很简单了。</p>\n<p>$$ \\hat{\\theta}_{d \\times 1} = (D_{m \\times d}^{T})^{-1} C_{m \\times 1}$$</p>\n<p>但是由于数据稀疏，实际上求参数西塔时不会这样简单粗暴，而是采用岭回归的方法，给原始特征矩阵加上一个单位对角矩阵后再参与计算：</p>\n<p>$$ \\hat{\\theta}_{d \\times 1} = (D_{m \\times d}^{T}D_{m \\times d} + I_{d \\times d})^{-1}D_{m \\times d}^{T}C_{m \\times 1}$$</p>\n<p>每一个候选臂都像这样去更新它的参数，同时，得到参数后，在真正做选择时，用面对上下文的特征和候选臂的参数一起。</p>\n<p>除了估算期望收益，还要计算置信区间的上边界，如果x是上下文特征，则期望收益和置信上边界的计算方法分别是下面的样子。</p>\n<p>期望收益：</p>\n<p>$$\\hat{r} = x^{T}_{d \\times 1}\\hat{\\theta}_ {d \\times1 }$$</p>\n<p>置信区间上边界：</p>\n<p>$$\\hat{b} = \\alpha \\sqrt{x^{T}_{d \\times 1}(D_{m \\times d}^{T}D_{m \\times d} + I_{d \\times d})^{-1}x_{d \\times 1}}$$</p>\n<p>这两个计算结果都是标量数值。置信区间计算公式虽然看起来复杂，实际上反应的思想也很直观，随着被选择次数的增加，也就是m增加，这个置信上边界是越来越小的。</p>\n<p>每一次选择时给每一个候选臂都计算这两个值，相加之后选择最大那个候选臂输出，就是LinUCB了。</p>\n<p>刚才说到了岭回归（ridge regression），这里多说一句，岭回归主要用于当样本数小于特征数时，对回归参数进行修正。对于加了特征的Bandit问题，正好符合这个特点：试验次数（样本）少于特征数。</p>\n<p>信息量有点大，我在这里再一次列出LinUCB的重点。</p>\n<ol>\n<li>LinUCB不再是上下文无关地，像盲人摸象一样从候选臂中去选择了，而是要考虑上下文因素，比如是用户特征、物品特征和场景特征一起考虑。</li>\n<li>每一个候选臂针对这些特征各自维护一个参数向量，各自更新，互不干扰。</li>\n<li>每次选择时用各自的参数去计算期望收益和置信区间，然后按照置信区间上边界最大的输出结果。</li>\n<li>观察用户的反馈，简单说就是“是否点击”，将观察的结果返回，结合对应的特征，按照刚才给出的公式，去重新计算这个候选臂的参数。</li>\n</ol>\n<p>当LinUCB的特征向量始终取1，每个候选臂的参数是收益均值的时候，LinUCB就是UCB。</p>\n<p>说完简单版的LinUCB，再看看高级版的LinUCB。与简单版的相比，就是认为有一部分特征对应的参数是在所有候选臂之间共享的，所谓共享，也就是无论是哪个候选臂被选中，都会去更新这部分参数。</p>\n<!-- [[[read_end]]] -->\n<h2>构建特征</h2>\n<p>LinUCB算法有一个很重要的步骤，就是给用户和物品构建特征，也就是刻画上下文。</p>\n<p>在“Yahoo！”的应用中，物品是文章。它对特征做了一些工程化的处理，这里稍微讲一下，可供实际应用时参考借鉴。</p>\n<p>首先，原始用户特征有下面几个。</p>\n<ol>\n<li>人口统计学：性别特征（2类），年龄特征（离散成10个区间）。</li>\n<li>地域信息：遍布全球的大都市，美国各个州。</li>\n<li>行为类别：代表用户历史行为的1000个类别取值。</li>\n</ol>\n<p>其次，原始文章特征有：</p>\n<ol>\n<li>URL类别：根据文章来源分成了几十个类别。</li>\n<li>编辑打标签：编辑人工给内容从几十个话题标签中挑选出来的。</li>\n</ol>\n<p>原始特征向量先经过归一化，变成单位向量。</p>\n<p>再对原始用户特征做第一次降维，降维的方法就是利用用户特征和物品特征以及用户的点击行为去拟合一个矩阵W。</p>\n<p>$$\\phi_{u}^{T}W\\phi_{a}^{T}$$</p>\n<p>就用逻辑回归拟合用户对文章的点击历史，得到的W直觉上理解就是：能够把用户特征映射到物品特征上，相当于对用户特征降维了，映射方法是下面这样。</p>\n<p>$$\\psi_{u}=\\phi_{u}^{T}W$$</p>\n<p>这一步可以将原始的1000多维用户特征投射到文章的80多维的特征空间。</p>\n<p>然后，用投射后的80多维特征对用户聚类，得到5个类，文章页同样聚类成5个类，再加上常数1，用户和文章各自被表示成6维向量。</p>\n<p>接下来就应用前面的LinUCB算法就是了，特征工程依然还是很有效的。</p>\n<h2>总结</h2>\n<p>今天我和你分享了一种上下文有关的Bandit算法，叫做LinUCB，它有这么几个优点：</p>\n<ol>\n<li>由于加入了特征，所以收敛比UCB更快，也就是比UCB更快见效；</li>\n<li>各个候选臂之间参数是独立的，可以互相不影响地更新参数；</li>\n<li>由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章；</li>\n</ol>\n<p>当然，LinUCB以及所有的Bandit算法都有个缺点：同时处理的候选臂数量不能太多，不超过几百个最佳。因为每一次要计算每一个候选臂的期望收益和置信区间，一旦候选太多，计算代价将不可接受。</p>\n<p>LinUCB只是一个推荐框架，可以将这个框架应用在很多地方，比如投放广告，为用户选择兴趣标签，你还可以发挥聪明才智，看看它还能用来解决什么问题，欢迎留言一起交流。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/87/b0/873b086966136189db14874181823fb0.jpg?wh=1110*549\" alt=\"\" /></p>\n","comments":[{"had_liked":false,"id":15303,"user_name":"cyrilliang","can_delete":false,"product_type":"c1","uid":1138322,"ip_address":"","ucode":"7E9D0FEFFAA32D","user_header":"https://static001.geekbang.org/account/avatar/00/11/5e/92/cd3ad588.jpg","comment_is_top":false,"comment_ctime":1531099237,"is_pvip":false,"replies":[{"id":"5882","content":"你真相了。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1532350301,"ip_address":"","comment_id":15303,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23005935717","product_id":100005101,"comment_content":"真是越能看到后面的人越少。老师讲得不错，赞一个","like_count":5,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":420328,"discussion_content":"你真相了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1532350301,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1127175,"avatar":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","nickname":"JustDoDT","note":"","ucode":"6AF0B80F00EAEF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":258315,"discussion_content":"能看懂的人越来越少，第一次接触很正常，老师讲的如果我是第二次接触，大部分会能看懂。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588677302,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":11665,"user_name":"范深","can_delete":false,"product_type":"c1","uid":1009754,"ip_address":"","ucode":"25F3EC78D00AC4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/68/5a/4e7754d3.jpg","comment_is_top":false,"comment_ctime":1528277672,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23003114152","product_id":100005101,"comment_content":"Bandit 的精髓之一就是在没有特征，没有状态的情况下进行选择。如果LinUCB引进来了，说明U-I的特征也有了，这时候能用的算法就很多了，包括DQN等。我理解的对吗？","like_count":5},{"had_liked":false,"id":5516,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1523449098,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14408350986","product_id":100005101,"comment_content":"简单搜索了下，除了典型的商品和内容推荐，也有人在非工业领域用于搜索更好的游戏策略(蒙特卡罗模拟)，更好的短信交互策略提高用户互动目标的完成度。如果更幻想一些，理论上有足够多的数据也可以协助更好地诊断疾病标签和推荐可能更有效的治疗方案(如果医疗机构有意愿积极协作)。","like_count":4,"discussions":[{"author":{"id":1586881,"avatar":"https://static001.geekbang.org/account/avatar/00/18/36/c1/bdc84029.jpg","nickname":"流沙岁月","note":"","ucode":"33EBB654450BCE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526,"discussion_content":"在诊断领域模型的泛化能力比较容易受质疑，因为治疗方案与医生个人偏好强关联，所以在收集了足够的A医院A医生的诊断历史数据情况下训练的模型，很难适用B医院B医生","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1561641448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":162551,"user_name":"Austin","can_delete":false,"product_type":"c1","uid":1207496,"ip_address":"","ucode":"A99DBCFE8D2558","user_header":"https://static001.geekbang.org/account/avatar/00/12/6c/c8/1908ea47.jpg","comment_is_top":false,"comment_ctime":1576552940,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5871520236","product_id":100005101,"comment_content":"感觉就是矩阵分解呀","like_count":1},{"had_liked":false,"id":18684,"user_name":"星","can_delete":false,"product_type":"c1","uid":1021369,"ip_address":"","ucode":"50490F47C4A6D2","user_header":"","comment_is_top":false,"comment_ctime":1533548938,"is_pvip":false,"discussion_count":4,"race_medal":0,"score":"5828516234","product_id":100005101,"comment_content":"老师好。看完您的视频，就能找份推荐的工作了吗？谢谢","like_count":1,"discussions":[{"author":{"id":1308552,"avatar":"https://static001.geekbang.org/account/avatar/00/13/f7/88/da243c77.jpg","nickname":"大魔王","note":"","ucode":"5EB049D18E122F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":291041,"discussion_content":"还差的远","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594688668,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1127175,"avatar":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","nickname":"JustDoDT","note":"","ucode":"6AF0B80F00EAEF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":258328,"discussion_content":"严格的告诉你不能","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588678540,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1002093,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4a/6d/d9fde443.jpg","nickname":"chenjielin","note":"","ucode":"10C88684BC1BB8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":251367,"discussion_content":"一夜暴富不是上进心","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588082256,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1972342,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/18/76/58c32789.jpg","nickname":"自由如风","note":"","ucode":"822040CF0D5235","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":248614,"discussion_content":"还有视频吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587881123,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":299680,"user_name":"dtttt","can_delete":false,"product_type":"c1","uid":2660367,"ip_address":"","ucode":"AD63A57A8FF60E","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ajNVdqHZLLBkrZdibnbjt4E1Sntg9ZyrUIFAUSbvs9ic2lTGA4a7IiblLEMZ8rwz2uIK72rE2iaVpJh5Mqme25omJg/132","comment_is_top":false,"comment_ctime":1624791703,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1624791703","product_id":100005101,"comment_content":"老师好，如何从 LinUCB 的置信区间上边界的公式得出 m 越大，其置信区间越小的呢？","like_count":0},{"had_liked":false,"id":285433,"user_name":"全","can_delete":false,"product_type":"c1","uid":1132314,"ip_address":"","ucode":"C478EAFACC4E3B","user_header":"https://static001.geekbang.org/account/avatar/00/11/47/1a/645ab65b.jpg","comment_is_top":false,"comment_ctime":1616812704,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1616812704","product_id":100005101,"comment_content":"利用context 特征，并且将用户和臂的特征进行相似性计算(这里可以有很多方式，线性，双线性，cos，模型）作为最后的收益。","like_count":0},{"had_liked":false,"id":284890,"user_name":"全","can_delete":false,"product_type":"c1","uid":1132314,"ip_address":"","ucode":"C478EAFACC4E3B","user_header":"https://static001.geekbang.org/account/avatar/00/11/47/1a/645ab65b.jpg","comment_is_top":false,"comment_ctime":1616513568,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1616513568","product_id":100005101,"comment_content":"感觉linucb 也可以做用户粗粒度画像","like_count":0},{"had_liked":false,"id":256589,"user_name":"在西伯利亚森林养猫","can_delete":false,"product_type":"c1","uid":1952827,"ip_address":"","ucode":"325717E50D469F","user_header":"https://static001.geekbang.org/account/avatar/00/1d/cc/3b/8ce7c7b1.jpg","comment_is_top":false,"comment_ctime":1603684732,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1603684732","product_id":100005101,"comment_content":"ψu​=ϕuT​ W <br>这个公式可以详细讲讲吗，构建特征工程哪儿没有明白","like_count":1},{"had_liked":false,"id":162550,"user_name":"Austin","can_delete":false,"product_type":"c1","uid":1207496,"ip_address":"","ucode":"A99DBCFE8D2558","user_header":"https://static001.geekbang.org/account/avatar/00/12/6c/c8/1908ea47.jpg","comment_is_top":false,"comment_ctime":1576552925,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1576552925","product_id":100005101,"comment_content":"没讲清楚，都有这么多数据积累了，干嘛不直接用LR","like_count":0,"discussions":[{"author":{"id":1900273,"avatar":"","nickname":"李大胆","note":"","ucode":"92349E76262D3D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":224338,"discussion_content":"特征是有了，但是监督学习需要样本呀，也就是在样本不足的时候用linucb，样本充足了就会使用各种复杂模型了","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1586278861,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1064167,"avatar":"https://static001.geekbang.org/account/avatar/00/10/3c/e7/eb5aea59.jpg","nickname":"山药","note":"","ucode":"A1EA25B081617C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":301669,"discussion_content":"另外一个，我的理解用LinUCB还关注解决的是EE问题，LR会可劲的往得分最高的方向选择。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598603212,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":102891,"user_name":"qingyunhe","can_delete":false,"product_type":"c1","uid":1062292,"ip_address":"","ucode":"6179894CF64F07","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKotwgsX9quRzOt8WqWHCxJ1DzA5pETgeialJs5NmP1icZsWJwNgv8I2icgz504wKHg4AuxMzpFibnmvg/132","comment_is_top":false,"comment_ctime":1560322450,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560322450","product_id":100005101,"comment_content":"哈哈，放心，我会学完的~~ 只不过学到这儿，是我第一个需要课下再消化一下的小节，光那篇论文需要细细研读一遍~~~","like_count":0},{"had_liked":false,"id":91440,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1557039501,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557039501","product_id":100005101,"comment_content":"讲的很好，就是后面的linUCB感觉是个大工程，需要不断优化不断累积修正特征","like_count":0}]}