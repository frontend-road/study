{"id":698985,"title":"14｜变形金刚：Transformer是如何让模型变大的？","content":"<p>你好，我是 Tyler。</p><p>在上一节课中，你已经学习了自然语言处理（NLP）的预训练模型技术。经过持续不断的探索，NLP领域迎来了许多重大的突破。其中，Transformer模型及其衍生模型BERT和GPT系列就是最具代表性的例子。这些研究成果为NLP预训练模型的发展带来了曙光。</p><p>不过，这只是大语言模型波澜壮阔发展历史的开端。随后，自然语言处理（NLP）的预训练模型技术在短时间内取得了飞跃式进展，迅速迈入了技术爆炸的阶段。其中一个关键因素是Transformer的问世，其出色的性能和训练效率提升为大型模型技术的发展创造了必要的条件。</p><p>因此这节课，我就会带你啃下这个 Transformer 这个硬骨头，不过请放心，它的原理其实并不复杂。但想要理解它，请务必确保你已经认真学习了<a href=\"https://time.geekbang.org/column/article/696734\">第12节课</a>的内容，<strong>深入理解上节课提到的 Seq2Seq 架构以及编码器和解码器的作用</strong>。</p><p>因为没有这些基础的话，你学习<strong>Transformer 的过程就会有点像在听天书。</strong>当然了，我也会延续我们课程的风格，尽可能可以通过白话让你理解它。</p><h2>注意力机制</h2><p><a href=\"https://time.geekbang.org/column/article/698540\">上节课</a>我们提到了注意力机制。你可能已经发现，早期的注意力机制是需要附着在其他网络架构上才能发挥作用。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/92/33/92ecc911789d6aa174e20fff1b068e33.jpg?wh=1867x707\" alt=\"\" title=\"机器翻译：SANVis: Visual Analytics for Understanding Self-Attention Networks\"></p><p><img src=\"https://static001.geekbang.org/resource/image/83/aa/834720936f6d00b71684912bb12dabaa.jpg?wh=1867x1050\" alt=\"\" title=\"计算机视觉：Focal Modulation Networks\"></p><p>如图所示，我们已经在上节课学习了注意力机制的经典应用，也就是 LSTM w/ Attention。不过，这个机制不仅适用于此，还可以应用于视觉模型如CNN，甚至一些推荐系统算法中。这是因为，学会使用注意力是这些场景的共同需求。</p><p><img src=\"https://static001.geekbang.org/resource/image/f1/2f/f12d7a87d6ce94030742b0d3b0b5a42f.jpg?wh=1867x851\" alt=\"\" title=\"推荐系统：Deep Interest Network for Click-Through Rate Prediction\"></p><p>Google的研究人员认为，既然注意力机制这么有效，不如把它发挥到极致，也正是这个想法直接造就了 Transformer。没错，Transformer模型的主要灵感正式来源于注意力机制。接下来，我们来深入地聊一聊，Transformer 的工作流程。</p><p>首先，我们先来回顾一下Seq2Seq（Sequence-to-Sequence）架构所做的事情。</p><p>以机器翻译为例，这个过程里，编码器首先会将输入序列从源空间（例如中文的现实世界表达）投影到一个语义空间的向量表示中。接着，解码器将这个空间向量从语义空间映射回目标空间（例如英文的现实世界表达），生成一个新的序列作为翻译输出。</p><p>那么， Transformer 是怎么解决这几个空间之间的映射问题的呢？这就要涉及到 Transformer 的具体细节了。</p><p>在此之前，我想先教给你两个理解和掌握复杂事物的方法。首先，你需要自顶向下地去学习，而不要被裹挟到细节的漩涡之中。第二，我们要去拆解事物的每个组件，剖析各个组件的必要性，这样会帮助你理解，为什么它被设计成了现在这个样子。</p><p>所以，我们接下来先自顶向下地理解 Transformer，然后挨个拆解它的组件。如下图所示，Transformer 总体上是一个编码器-解码器的架构。</p><p><img src=\"https://static001.geekbang.org/resource/image/d1/fb/d138a33aefd0yy3c90b549f6496f8efb.jpg?wh=1867x1050\" alt=\"\"></p><p>左侧是 Transformer 的编码器，它由很多个 Encoder 堆叠组成。右侧是它的解码器，由许多的Decoder堆叠组成。现在，我们先拿出放大镜看看编码器部分的内部构造吧，因为解码器其实和编码器是同构的，没有太大的区别。</p><h2>进入高维空间</h2><p>为了降低你的学习压力，我将通过一个小故事来串联 Transformer 各个部分的作用。先来听听故事的内容吧：故事发生在几千年后，由于那时地球的环境日益恶化，人类最终决定进行星际移民，离开这颗母星。</p><p>为了实现这个目标，我们需要让我们的星际舰队，探索宇宙中适合居住的新家园。为此，我们提出了一个名为Transformer 的搜寻计划，计划的目标是让人类尽快地找到目标星球。下面我们的计划开始了。</p><p>第一步，我们的舰队要先进入太空，才能开始星际之旅。这就好像Transformer模型，需要把我们的文本输入转化成高维向量一样，毕竟机器世界无法直接理解文本数据。</p><p>所以，Encoder 的第一层嵌入层会将输入的词汇转换为向量表示，也就是把输入映射到一个向量空间中，以便模型更好地理解输入内容。</p><p>这听起来很熟悉，对吧？没错，这就是我们<a href=\"https://time.geekbang.org/column/article/689434\">第6节课</a>里学习到的空间投影。所以，这里你可以使用独热编码对输入进行编码，让Transformer自学调整参数，或者使用像是Word2Vec预训练好的投影模型进行编码，让Transformer少走弯路。</p><h3>组成编队：位置编码</h3><p>很好，在完成第一步之后，你的舰队已经成功升空。接下来，为了确保舰船在无边无际的宇宙中，能够准确报告自己的位置并进行有效通讯，我们需要为每个成员分配独一无二的编号。这就是 Transformer 中 Endcoder 的第二层，也就是位置编码层所做的事情。</p><p>位置编码层会为嵌入层输出的这些向量添加位置信息，把位置信息融合到嵌入向量中，这能帮助模型区分不同词汇在输入中所处的位置。你可能会问，那么为什么RNN不需要位置编码呢？</p><p>这是一个很好的问题。要知道，RNN 是串行处理输入数据的，所以每个 RNN 单元其实都包含了所在位置之前的全部输入信息，所以对当前位置输入的处理是有状态的。</p><p>但是，Transformer会并行地处理所有输入的内容，所以各个并行单元会无状态地处理每个输入。因此，我们需要在最开始就给每个输入的嵌入向量一个位置编号，这样模型才能通过输入判断它在整体中的位置。</p><h2>寻找目标星球</h2><p>好了，在完成了这两步之后，舰船已经完成了升空和编队，下一步就要开始漫长的星际航行了。</p><h3>宇宙航行：自注意力机制</h3><p>随着太空舰队司令的一声令下，舰船纷纷启航进入广袤无垠的宇宙中，收集来自各个方向的信号。这个寻找目标的过程，在 Transformer中则是由自注意力机制来完成的。</p><p>自注意力机制会针对每个带有位置编码的输入向量，去计算和其他位置的关联程度，从而捕捉输入内部的上下文关联信息，形成一个注意力权重的分布作为后续层的输入，指导模型的学习过程。你可以参考后面的伪代码来加深理解。</p><pre><code class=\"language-python\">def self_attention(input_embedding):\n&nbsp; &nbsp; # 输入的嵌入向量矩阵，假设为input_embedding，维度为[序列长度, 嵌入维度]\n&nbsp; &nbsp; sequence_length, embedding_dim = input_embedding.shape\n&nbsp; &nbsp; # 通过线性变换得到Q、K、V\n&nbsp; &nbsp; Q = linear_transform(input_embedding)\n&nbsp; &nbsp; K = linear_transform(input_embedding)\n&nbsp; &nbsp; V = linear_transform(input_embedding)\n&nbsp; &nbsp; # 计算注意力分数\n&nbsp; &nbsp; attention_scores = Q @ K.T / sqrt(embedding_dim)\n&nbsp; &nbsp; # 进行归一化\n&nbsp; &nbsp; attention_weights = softmax(attention_scores, axis=1)\n&nbsp; &nbsp; # 将权重与V相乘得到上下文向量\n&nbsp; &nbsp; context_vector = attention_weights @ V\n&nbsp; &nbsp; return context_vector\n</code></pre><p>在这里，有一个常被问到的问题，那就是：“注意力机制和自注意力机制的区别是什么？”</p><p>我来回答一下这个问题，帮助你区分、理解。注意力机制关注的是输入与输出之间的关联。比如模型对中译英训练数据 “我爱火锅” 译为 “I love hotpot” 的学习过程中，我们希望基于注意力机制的 LSTM 在翻译“我”的这个 Decoder 的位置上，注意力更多地集中在 “I” 上，而不是 “love” 上。</p><p>而自注意力机制的主要目标，则是确定输入词与词之间的内部关联性。比方说，在 “道可道非常道” 这句话中。我们希望 Transformer 可以通过自注意力机制，学到句中“非常道”指的是原句中的第一个“道”字而不是第三个。这种机制可以让 Transformer 学习理解这句话中更多隐含的信息和深意。</p><h3>平行宇宙：多头注意力</h3><p>在拥有了自注意力机制之后，你的舰队已经纷纷出发去搜寻目标星球了。这时，你的太空军司令在军帐中开始琢磨，是否有方法可以加快寻找目标的过程。恰逢这时候地球上发生了科技爆炸，人类获得了观测平行宇宙中事件的能力。所以，你的太空军司令决定在平行宇宙中收集舰队信息，以加速寻找目标星球的过程。</p><p>这个观测多个平行宇宙中舰队的航行情况的能力，正是 Transformer 中的多头注意力（Multi-Head Attention）模块所具备的。Transformer 模型会使用多组同构的自注意力（Self-Attention）模块，并行学习出多组不同的权重，每组权重表示了它根据输入信息所学习的不同自注意力权重。</p><p>最终通过将多组自注意力计算的结果拼接在一起，通过线性变换得到多头自注意力模块的联合输出。</p><p>总之啊，多头注意力机制类似于赛马机制，它有助于减少模型初始化的随机性对模型效果的影响。所以即使只留下一个注意力头也能使用，但这会导致模型的稳定性和多样性无法得到保障，进而造成模型的性能下降。</p><h2>集体智慧决策</h2><p>现在，太空军司令通过观察不同平行宇宙中的事件，就能不断汇总各个宇宙中舰队的信息。然而，随着平行宇宙的不断扩展，太空军司令发现自己疲于奔命，很难做出准确的决策，于是他开始想办法提高工作效率。</p><h3>跨级沟通：残差归一化</h3><p>首先，他决定授予各个舰队一定的权限，允许它们跨级沟通，以提高信息传递的效率。实际上，这就是之前我们学过的残差结构。</p><p>在Transformer的每个子层中，都使用了<strong>残差连接</strong>和<strong>层归一化</strong>来稳定训练过程。如果你记不起这部分原理的话，可以回顾一下<a href=\"https://time.geekbang.org/column/article/692796\">第11节课</a>中的ResNet。</p><h3>掌控全局：前馈神经网络</h3><p>在解决了信息传输的问题之后，为了提高信息汇总的效率，他又决定采用集体智慧决策的方式，通过设立总参谋部，分层汇总各个平行宇宙的舰队情况。这就是 Transformer 中多层感知机，也就是 MLP 的作用了。</p><p>从下图中你可以看出，在 Transformer 中，每个“头”都会输出到 MLP 中进一步的汇总信息，用来增加 Transformer 模型的拟合能力，提升决策的效率和效果。</p><p>至此，你已经掌握了 Transformer 中所有的核心组件以及它们的作用了。</p><p><img src=\"https://static001.geekbang.org/resource/image/a7/bb/a700089a17a8833059c3905d3c377cbb.jpg?wh=1867x1050\" alt=\"p5\"></p><h2>小结</h2><p>今天的内容告一段落，我们做个总结吧。这节课我们讲述了一个关于星际舰队寻找新家园的故事。</p><p>在这个故事中，我们首先见证了舰队的升空启航，进入宇宙空间。这就好像 Transformer 模型中把文本输入数据转化成了高维空间向量的过程。</p><p>随后，为了确保每艘舰船在航行中，能够准确地报告自己的位置，并且进行有效的通讯，我们为每位成员分配了独一无二的编号，这也是 Transformer 中位置编码的作用。</p><p>随着太空军司令的一声令下，舰船们各自启航，寻找目标。这实际上就是自注意力机制所承担的任务，它负责在茫茫的宇宙中寻找目标。</p><p>同时，为了提高寻找过程的效率，太空军司令还拥有了一种“超能力”，那就是窥探平行宇宙中，各种可能的寻找路径。这就是多头注意力机制的巧妙之处，它可以极大地加速寻找的过程。</p><p>最后，司令汇总了所有舰船的探索结果，通过加权汇总的方式确定了目标星球的位置，就像 MLP 层在模型中的作用一样。</p><p>你可以对照后面的图来加深理解。是不是现在已经对 Transformer 各个部分的作用一目了然了?</p><p><img src=\"https://static001.geekbang.org/resource/image/59/28/5935b6590b7733d8751c8fa40a6f8528.jpg?wh=1867x1050\" alt=\"\"></p><p>最初，Transformer只是机器翻译领域的一次尝试，但其惊人的效果和训练效率让它备受瞩目。在下一堂课中，我们将继续深入探讨Transformer最经典的变种，也就是GPT系列模型的原理和发展历程，敬请期待！</p><h2>思考题</h2><p>请你想一想，我们的模型里是否可以去掉位置编码？</p><p>恭喜完成我们第 14 次打卡学习，期待你在留言区和我交流互动。如果你觉得有收获，也欢迎你分享给你身边的朋友，邀 TA 一起讨论。</p>","comments":[{"had_liked":false,"id":380888,"user_name":"水木","can_delete":false,"product_type":"c1","uid":1288226,"ip_address":"广西","ucode":"30E935000F36C6","user_header":"https://static001.geekbang.org/account/avatar/00/13/a8/22/09c4211e.jpg","comment_is_top":false,"comment_ctime":1694412806,"is_pvip":false,"replies":[{"id":138829,"content":"你好，水木！感谢你的支持。后面的课程中我也会保持这个风格，尽量用白话的方式，让大家快速理解各种大模型技术的本质。","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1694706347,"ip_address":"北京","comment_id":380888,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"这堂课讲的很有意思啊，用变形金刚 星际舰队寻找目标星球的剧情把抽象的问题方法具体形象化。","like_count":5,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627963,"discussion_content":"你好，水木！感谢你的支持。后面的课程中我也会保持这个风格，尽量用白话的方式，让大家快速理解各种大模型技术的本质。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694706347,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381591,"user_name":"一只豆","can_delete":false,"product_type":"c1","uid":1003886,"ip_address":"广东","ucode":"73953B25ADC953","user_header":"https://static001.geekbang.org/account/avatar/00/0f/51/6e/efb76357.jpg","comment_is_top":false,"comment_ctime":1695484410,"is_pvip":false,"replies":[{"id":139687,"content":"你好，一只豆。感谢你一以贯之的支持！如你所说，在联系前后课程内容，找到之间的联系，并拼凑出整个拼图后，会得到拥有上帝视角的快乐，很替你高兴！","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1699058585,"ip_address":"北京","comment_id":381591,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"二刷课程的我，在每一节课中反复感受到 老师自顶向下的教学思想：不仅在每一节课中（本节讲解是我看到所有Transformer讲解中最棒的），也反映在 对整个AI技术发展的探索历史的上帝视角。 纵观这门课程，很多句子都能让学习者恍然大悟，达到“一眼万年”的境界。。。","like_count":3,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630911,"discussion_content":"你好，一只豆。感谢你一以贯之的支持！如你所说，在联系前后课程内容，找到之间的联系，并拼凑出整个拼图后，会得到拥有上帝视角的快乐，很替你高兴！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699058586,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380895,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"美国","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1694421342,"is_pvip":false,"replies":[{"id":139545,"content":"你好，Paul Shan！首先我们为每个位置的输入都加入了位置编码信息，为了让他们可以有效的融合，保持相同维度是必要的。其实最关键的问题是如何用这个同维度的向量，来表示位置的信息。\n\n这里说一下核心原理，在 Transformer 的内部计算中，位置编码的设计，能让两个不同位置的编码在进行矢量运算后，能提取出他们的相对位置距离（j-i），以此来完成位置的表示。\n\n这是一个很好的问题，希望你在后面的课程中还能提出更多高质量的问题。","user_name":"作者回复","user_name_real":"作者","uid":1002568,"ctime":1698415711,"ip_address":"北京","comment_id":380895,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"RNN是一个单词一个单词处理的，自带顺序，Transformer批量处理，位置信息丢失了，必须加入位置信息才能让模型学到单词之间的位置关系。请问一个问题，为什么位置信息是直接加入到embedding的输出，感觉位置信息和embedding的输出是不同维度的东西，用不同维度表示可能更合理一点，但是我也能理解本来维度已经很高了，直接加不会增加维度，除了不增加维度，直接加还有什么其他理由吗，多谢","like_count":3,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630434,"discussion_content":"你好，Paul Shan！首先我们为每个位置的输入都加入了位置编码信息，为了让他们可以有效的融合，保持相同维度是必要的。其实最关键的问题是如何用这个同维度的向量，来表示位置的信息。\n\n这里说一下核心原理，在 Transformer 的内部计算中，位置编码的设计，能让两个不同位置的编码在进行矢量运算后，能提取出他们的相对位置距离（j-i），以此来完成位置的表示。\n\n这是一个很好的问题，希望你在后面的课程中还能提出更多高质量的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698415711,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1050002,"avatar":"https://static001.geekbang.org/account/avatar/00/10/05/92/b609f7e3.jpg","nickname":"骨汤鸡蛋面","note":"","ucode":"2AC141A523E710","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627756,"discussion_content":"同问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694502267,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381933,"user_name":"周晓英","can_delete":false,"product_type":"c1","uid":1361053,"ip_address":"美国","ucode":"157378B0272243","user_header":"https://static001.geekbang.org/account/avatar/00/14/c4/9d/0f4ea119.jpg","comment_is_top":false,"comment_ctime":1696235109,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"位置编码在 Transformer 模型中起到了非常重要的作用。\n\n保持顺序信息:\n\n在文本处理任务中，词语之间的顺序关系是非常重要的。但是，由于 Transformer 的自注意力机制是对所有位置的词语同时进行处理，它本身无法区分词语的顺序。位置编码的加入能够提供顺序信息，使得模型能够区分不同位置的词语。\n使模型具备顺序感:\n\n当位置编码被加入到输入向量中时，每个位置的向量现在都包含了关于其位置的信息。这使得模型能够根据词语的相对位置来学习和作出预测。\n泛化能力:\n\n通过位置编码，模型可以更好地泛化到不同长度的序列，因为它学会了词语之间的相对位置关系。\n如果去掉位置编码，Transformer 模型就失去了词语顺序的信息，这会严重影响模型的性能，特别是在需要理解语言顺序的任务中，如机器翻译、文本摘要等。在一些不需要顺序信息的任务中，可能位置编码不是严格必要的，但在大多数自然语言处理任务中，位置编码是非常重要的。","like_count":4},{"had_liked":false,"id":395898,"user_name":"Seachal","can_delete":false,"product_type":"c1","uid":1301397,"ip_address":"北京","ucode":"A6F69EA7879EEF","user_header":"https://static001.geekbang.org/account/avatar/00/13/db/95/daad899f.jpg","comment_is_top":false,"comment_ctime":1732364704,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":1,"score":2,"product_id":100613101,"comment_content":"位置编码在Transformer模型中扮演着相当重要的角色。它帮助模型区分输入文本中不同词汇的位置，这是理解文本上下文和语义关系的基础。如果没有位置编码，模型可能会难以判断词汇之间的顺序和相对位置，从而影响对文本的整体理解。因此，我认为在Transformer模型中，位置编码是不可或缺的。  \n Transformer模型先把文本转成高维向量，再通过位置编码给每个词标上位置，这样模型就知道每个词在哪儿了。然后自注意力机制上场，找词与词之间的联系，形成注意力权重，引导模型学习。这比传统注意力机制厉害多了，能学到更多隐藏信息。\n\nTransformer模型在自然语言处理上表现超赞，给大型模型技术发展开了个好头。里面还包括多头注意力、残差归一化、前馈神经网络这些核心部件，都挺关键的。","like_count":0},{"had_liked":false,"id":395591,"user_name":"St.Peter","can_delete":false,"product_type":"c1","uid":1042358,"ip_address":"上海","ucode":"49E9974D26AB59","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e7/b6/c9b56731.jpg","comment_is_top":false,"comment_ctime":1731321299,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100613101,"comment_content":"不能去掉位置编码。位置编码保证了一个句子的顺序，去掉位置编码会乱序","like_count":0}]}