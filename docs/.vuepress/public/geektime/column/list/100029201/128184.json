{"id":128184,"title":"38 | 调优Kafka，你做到了吗？","content":"<p>你好，我是胡夕。今天我要和你分享的主题是：如何调优Kafka。</p><h2>调优目标</h2><p>在做调优之前，我们必须明确优化Kafka的目标是什么。通常来说，调优是为了满足系统常见的非功能性需求。在众多的非功能性需求中，性能绝对是我们最关心的那一个。不同的系统对性能有不同的诉求，比如对于数据库用户而言，性能意味着请求的响应时间，用户总是希望查询或更新请求能够被更快地处理完并返回。</p><p>对Kafka而言，性能一般是指吞吐量和延时。</p><p>吞吐量，也就是TPS，是指Broker端进程或Client端应用程序每秒能处理的字节数或消息数，这个值自然是越大越好。</p><p>延时和我们刚才说的响应时间类似，它表示从Producer端发送消息到Broker端持久化完成之间的时间间隔。这个指标也可以代表端到端的延时（End-to-End，E2E），也就是从Producer发送消息到Consumer成功消费该消息的总时长。和TPS相反，我们通常希望延时越短越好。</p><p>总之，高吞吐量、低延时是我们调优Kafka集群的主要目标，一会儿我们会详细讨论如何达成这些目标。在此之前，我想先谈一谈优化漏斗的问题。</p><h2>优化漏斗</h2><p>优化漏斗是一个调优过程中的分层漏斗，我们可以在每一层上执行相应的优化调整。总体来说，层级越靠上，其调优的效果越明显，整体优化效果是自上而下衰减的，如下图所示：</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/94/59/94486dc0eb55b68855478ef7e5709359.png?wh=1950*1248\" alt=\"\"></p><p><strong>第1层：应用程序层</strong>。它是指优化Kafka客户端应用程序代码。比如，使用合理的数据结构、缓存计算开销大的运算结果，抑或是复用构造成本高的对象实例等。这一层的优化效果最为明显，通常也是比较简单的。</p><p><strong>第2层：框架层</strong>。它指的是合理设置Kafka集群的各种参数。毕竟，直接修改Kafka源码进行调优并不容易，但根据实际场景恰当地配置关键参数的值，还是很容易实现的。</p><p><strong>第3层：JVM层</strong>。Kafka Broker进程是普通的JVM进程，各种对JVM的优化在这里也是适用的。优化这一层的效果虽然比不上前两层，但有时也能带来巨大的改善效果。</p><p><strong>第4层：操作系统层</strong>。对操作系统层的优化很重要，但效果往往不如想象得那么好。与应用程序层的优化效果相比，它是有很大差距的。</p><h2>基础性调优</h2><p>接下来，我就来分别介绍一下优化漏斗的4个分层的调优。</p><h3>操作系统调优</h3><p>我先来说说操作系统层的调优。在操作系统层面，你最好在挂载（Mount）文件系统时禁掉atime更新。atime的全称是access time，记录的是文件最后被访问的时间。记录atime需要操作系统访问inode资源，而禁掉atime可以避免inode访问时间的写入操作，减少文件系统的写操作数。你可以执行<strong>mount -o noatime命令</strong>进行设置。</p><p>至于文件系统，我建议你至少选择ext4或XFS。尤其是XFS文件系统，它具有高性能、高伸缩性等特点，特别适用于生产服务器。值得一提的是，在去年10月份的Kafka旧金山峰会上，有人分享了ZFS搭配Kafka的案例，我们在专栏<a href=\"https://time.geekbang.org/column/article/101763\">第8讲</a>提到过与之相关的<a href=\"https://www.confluent.io/kafka-summit-sf18/kafka-on-zfs\">数据报告</a>。该报告宣称ZFS多级缓存的机制能够帮助Kafka改善I/O性能，据说取得了不错的效果。如果你的环境中安装了ZFS文件系统，你可以尝试将Kafka搭建在ZFS文件系统上。</p><p>另外就是swap空间的设置。我个人建议将swappiness设置成一个很小的值，比如1～10之间，以防止Linux的OOM Killer开启随意杀掉进程。你可以执行sudo sysctl vm.swappiness=N来临时设置该值，如果要永久生效，可以修改/etc/sysctl.conf文件，增加vm.swappiness=N，然后重启机器即可。</p><p>操作系统层面还有两个参数也很重要，它们分别是<strong>ulimit -n和vm.max_map_count</strong>。前者如果设置得太小，你会碰到Too Many File Open这类的错误，而后者的值如果太小，在一个主题数超多的Broker机器上，你会碰到<strong>OutOfMemoryError：Map failed</strong>的严重错误，因此，我建议在生产环境中适当调大此值，比如将其设置为655360。具体设置方法是修改/etc/sysctl.conf文件，增加vm.max_map_count=655360，保存之后，执行sysctl -p命令使它生效。</p><p>最后，不得不提的就是操作系统页缓存大小了，这对Kafka而言至关重要。在某种程度上，我们可以这样说：给Kafka预留的页缓存越大越好，最小值至少要容纳一个日志段的大小，也就是Broker端参数log.segment.bytes的值。该参数的默认值是1GB。预留出一个日志段大小，至少能保证Kafka可以将整个日志段全部放入页缓存，这样，消费者程序在消费时能直接命中页缓存，从而避免昂贵的物理磁盘I/O操作。</p><h3>JVM层调优</h3><p>说完了操作系统层面的调优，我们来讨论下JVM层的调优，其实，JVM层的调优，我们还是要重点关注堆设置以及GC方面的性能。</p><p>1.设置堆大小。</p><p>如何为Broker设置堆大小，这是很多人都感到困惑的问题。我来给出一个朴素的答案：<strong>将你的JVM堆大小设置成6～8GB</strong>。</p><p>在很多公司的实际环境中，这个大小已经被证明是非常合适的，你可以安心使用。如果你想精确调整的话，我建议你可以查看GC log，特别是关注Full GC之后堆上存活对象的总大小，然后把堆大小设置为该值的1.5～2倍。如果你发现Full GC没有被执行过，手动运行jmap -histo:live &lt; pid &gt;就能人为触发Full GC。</p><p>2.GC收集器的选择。</p><p><strong>我强烈建议你使用G1收集器，主要原因是方便省事，至少比CMS收集器的优化难度小得多</strong>。另外，你一定要尽力避免Full GC的出现。其实，不论使用哪种收集器，都要竭力避免Full GC。在G1中，Full GC是单线程运行的，它真的非常慢。如果你的Kafka环境中经常出现Full GC，你可以配置JVM参数-XX:+PrintAdaptiveSizePolicy，来探查一下到底是谁导致的Full GC。</p><p>使用G1还很容易碰到的一个问题，就是<strong>大对象（Large Object）</strong>，反映在GC上的错误，就是“too many humongous allocations”。所谓的大对象，一般是指至少占用半个区域（Region）大小的对象。举个例子，如果你的区域尺寸是2MB，那么超过1MB大小的对象就被视为是大对象。要解决这个问题，除了增加堆大小之外，你还可以适当地增加区域大小，设置方法是增加JVM启动参数-XX:+G1HeapRegionSize=N。默认情况下，如果一个对象超过了N/2，就会被视为大对象，从而直接被分配在大对象区。如果你的Kafka环境中的消息体都特别大，就很容易出现这种大对象分配的问题。</p><h3>Broker端调优</h3><p>我们继续沿着漏斗往上走，来看看Broker端的调优。</p><p>Broker端调优很重要的一个方面，就是合理地设置Broker端参数值，以匹配你的生产环境。不过，后面我们在讨论具体的调优目标时再详细说这部分内容。这里我想先讨论另一个优化手段，<strong>即尽力保持客户端版本和Broker端版本一致</strong>。不要小看版本间的不一致问题，它会令Kafka丧失很多性能收益，比如Zero Copy。下面我用一张图来说明一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/53/6e/5310d7d29235b080c872e0a9eb396e6e.png?wh=816*388\" alt=\"\"></p><p>图中蓝色的Producer、Consumer和Broker的版本是相同的，它们之间的通信可以享受Zero Copy的快速通道；相反，一个低版本的Consumer程序想要与Producer、Broker交互的话，就只能依靠JVM堆中转一下，丢掉了快捷通道，就只能走慢速通道了。因此，在优化Broker这一层时，你只要保持服务器端和客户端版本的一致，就能获得很多性能收益了。</p><h3>应用层调优</h3><p>现在，我们终于来到了漏斗的最顶层。其实，这一层的优化方法各异，毕竟每个应用程序都是不一样的。不过，有一些公共的法则依然是值得我们遵守的。</p><ul>\n<li><strong>不要频繁地创建Producer和Consumer对象实例</strong>。构造这些对象的开销很大，尽量复用它们。</li>\n<li><strong>用完及时关闭</strong>。这些对象底层会创建很多物理资源，如Socket连接、ByteBuffer缓冲区等。不及时关闭的话，势必造成资源泄露。</li>\n<li><strong>合理利用多线程来改善性能</strong>。Kafka的Java Producer是线程安全的，你可以放心地在多个线程中共享同一个实例；而Java Consumer虽不是线程安全的，但我们在专栏<a href=\"https://time.geekbang.org/column/article/108512\">第20讲</a>讨论过多线程的方案，你可以回去复习一下。</li>\n</ul><h2>性能指标调优</h2><p>接下来，我会给出调优各个目标的参数配置以及具体的配置原因，希望它们能够帮助你更有针对性地调整你的Kafka集群。</p><h3>调优吞吐量</h3><p>首先是调优吞吐量。很多人对吞吐量和延时之间的关系似乎有些误解。比如有这样一种提法还挺流行的：假设Kafka每发送一条消息需要花费2ms，那么延时就是2ms。显然，吞吐量就应该是500条/秒，因为1秒可以发送1 / 0.002 = 500条消息。因此，吞吐量和延时的关系可以用公式来表示：TPS = 1000 / Latency(ms)。但实际上，吞吐量和延时的关系远不是这么简单。</p><p>我们以Kafka Producer为例。假设它以2ms的延时来发送消息，如果每次只是发送一条消息，那么TPS自然就是500条/秒。但如果Producer不是每次发送一条消息，而是在发送前等待一段时间，然后统一发送<strong>一批</strong>消息，比如Producer每次发送前先等待8ms，8ms之后，Producer共缓存了1000条消息，此时总延时就累加到10ms（即 2ms + 8ms）了，而TPS等于1000 / 0.01 = 100,000条/秒。由此可见，虽然延时增加了4倍，但TPS却增加了将近200倍。这其实也是批次化（batching）或微批次化（micro-batching）目前会很流行的原因。</p><p>在实际环境中，用户似乎总是愿意用较小的延时增加的代价，去换取TPS的显著提升。毕竟，从2ms到10ms的延时增加通常是可以忍受的。事实上，Kafka Producer就是采取了这样的设计思想。</p><p>当然，你可能会问：发送一条消息需要2ms，那么等待8ms就能累积1000条消息吗？答案是可以的！Producer累积消息时，一般仅仅是将消息发送到内存中的缓冲区，而发送消息却需要涉及网络I/O传输。内存操作和I/O操作的时间量级是不同的，前者通常是几百纳秒级别，而后者则是从毫秒到秒级别不等，因此，Producer等待8ms积攒出的消息数，可能远远多于同等时间内Producer能够发送的消息数。</p><p>好了，说了这么多，我们该怎么调优TPS呢？我来跟你分享一个参数列表。</p><p><img src=\"https://static001.geekbang.org/resource/image/7a/cb/7aec00207dc149bd804d20df6e3b9ccb.jpg?wh=1713*1983\" alt=\"\"></p><p>我稍微解释一下表格中的内容。</p><p>Broker端参数num.replica.fetchers表示的是Follower副本用多少个线程来拉取消息，默认使用1个线程。如果你的Broker端CPU资源很充足，不妨适当调大该参数值，加快Follower副本的同步速度。因为在实际生产环境中，<strong>配置了acks=all的Producer程序吞吐量被拖累的首要因素，就是副本同步性能</strong>。增加这个值后，你通常可以看到Producer端程序的吞吐量增加。</p><p>另外需要注意的，就是避免经常性的Full GC。目前不论是CMS收集器还是G1收集器，其Full GC采用的是Stop The World的单线程收集策略，非常慢，因此一定要避免。</p><p><strong>在Producer端，如果要改善吞吐量，通常的标配是增加消息批次的大小以及批次缓存时间，即batch.size和linger.ms</strong>。目前它们的默认值都偏小，特别是默认的16KB的消息批次大小一般都不适用于生产环境。假设你的消息体大小是1KB，默认一个消息批次也就大约16条消息，显然太小了。我们还是希望Producer能一次性发送更多的消息。</p><p>除了这两个，你最好把压缩算法也配置上，以减少网络I/O传输量，从而间接提升吞吐量。当前，和Kafka适配最好的两个压缩算法是<strong>LZ4和zstd</strong>，不妨一试。</p><p>同时，由于我们的优化目标是吞吐量，最好不要设置acks=all以及开启重试。前者引入的副本同步时间通常都是吞吐量的瓶颈，而后者在执行过程中也会拉低Producer应用的吞吐量。</p><p>最后，如果你在多个线程中共享一个Producer实例，就可能会碰到缓冲区不够用的情形。倘若频繁地遭遇TimeoutException：Failed to allocate memory within the configured max blocking time这样的异常，那么你就必须显式地增加<strong>buffer.memory</strong>参数值，确保缓冲区总是有空间可以申请的。</p><p>说完了Producer端，我们来说说Consumer端。Consumer端提升吞吐量的手段是有限的，你可以利用多线程方案增加整体吞吐量，也可以增加fetch.min.bytes参数值。默认是1字节，表示只要Kafka Broker端积攒了1字节的数据，就可以返回给Consumer端，这实在是太小了。我们还是让Broker端一次性多返回点数据吧。</p><h3>调优延时</h3><p>讲完了调优吞吐量，我们来说说如何优化延时，下面是调优延时的参数列表。</p><p><img src=\"https://static001.geekbang.org/resource/image/26/3a/2688329a0614601fed497f3858c98e3a.jpg?wh=1803*1083\" alt=\"\"></p><p>在Broker端，我们依然要增加num.replica.fetchers值以加快Follower副本的拉取速度，减少整个消息处理的延时。</p><p>在Producer端，我们希望消息尽快地被发送出去，因此不要有过多停留，所以必须设置linger.ms=0，同时不要启用压缩。因为压缩操作本身要消耗CPU时间，会增加消息发送的延时。另外，最好不要设置acks=all。我们刚刚在前面说过，Follower副本同步往往是降低Producer端吞吐量和增加延时的首要原因。</p><p>在Consumer端，我们保持fetch.min.bytes=1即可，也就是说，只要Broker端有能返回的数据，立即令其返回给Consumer，缩短Consumer消费延时。</p><h2>小结</h2><p>好了，我们来小结一下。今天，我跟你分享了Kafka调优方面的内容。我们先从调优目标开始说起，然后我给出了调优层次漏斗，接着我分享了一些基础性调优，包括操作系统层调优、JVM层调优以及应用程序调优等。最后，针对Kafka关心的两个性能指标吞吐量和延时，我分别从Broker、Producer和Consumer三个维度给出了一些参数值设置的最佳实践。</p><p>最后，我来分享一个性能调优的真实小案例。</p><p>曾经，我碰到过一个线上环境的问题：该集群上Consumer程序一直表现良好，但是某一天，它的性能突然下降，表现为吞吐量显著降低。我在查看磁盘读I/O使用率时，发现其明显上升，但之前该Consumer Lag很低，消息读取应该都能直接命中页缓存。此时磁盘读突然飙升，我就怀疑有其他程序写入了页缓存。后来经过排查，我发现果然有一个测试Console Consumer程序启动，“污染”了部分页缓存，导致主业务Consumer读取消息不得不走物理磁盘，因此吞吐量下降。找到了真实原因，解决起来就简单多了。</p><p>其实，我给出这个案例的真实目的是想说，对于性能调优，我们最好按照今天给出的步骤一步一步地窄化和定位问题。一旦定位了原因，后面的优化就水到渠成了。</p><p><img src=\"https://static001.geekbang.org/resource/image/d4/38/d40b07ca7bcc0fc43c5266b0b2c81c38.jpg?wh=2069*2560\" alt=\"\"></p><h2>开放讨论</h2><p>请分享一个你调优Kafka的真实案例，详细说说你是怎么碰到性能问题的，又是怎么解决的。</p><p>欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"37 | 主流的Kafka监控框架","id":127192},"right":{"article_title":"39 | 从0搭建基于Kafka的企业级实时日志流处理平台","id":130907}},"comments":[{"had_liked":false,"id":129109,"user_name":"诗泽","can_delete":false,"product_type":"c1","uid":1031865,"ip_address":"","ucode":"F28BE01C3FD12F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/be/b9/f2481c2c.jpg","comment_is_top":false,"comment_ctime":1567043769,"is_pvip":false,"replies":[{"id":"48170","content":"是的","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567126797,"ip_address":"","comment_id":129109,"utype":1}],"discussion_count":5,"race_medal":0,"score":"100351291577","product_id":100029201,"comment_content":"请问最后这个例子中的测试 Console Consume是怎样污染缓存页的？是因为它读取了比较老的数据，使得新数据被写入磁盘导致的吗？","like_count":23,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465281,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567126797,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2649276,"avatar":"https://static001.geekbang.org/account/avatar/00/28/6c/bc/f751786b.jpg","nickname":"Leo","note":"","ucode":"CEBAD9CDCFC2A3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":548294,"discussion_content":"就像数据库 大批量读取老的无效数据就会污染BufferPool","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1643117736,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1034204,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c7/dc/9408c8c2.jpg","nickname":"ban","note":"","ucode":"E523CE97E48266","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":11942,"discussion_content":"请问，Consume读取老的数据，为什么新数据会写入磁盘。这个不太懂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568451102,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1130590,"avatar":"https://static001.geekbang.org/account/avatar/00/11/40/5e/b8fada94.jpg","nickname":"Ryoma","note":"","ucode":"7F692369239692","race_medal":2,"user_type":1,"is_pvip":true},"reply_author":{"id":1034204,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c7/dc/9408c8c2.jpg","nickname":"ban","note":"","ucode":"E523CE97E48266","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":179791,"discussion_content":"本来新的数据会在页缓存中，这样消费速度很快。但是由于有新的消费者，读取了旧数据，这部分数据从磁盘中读取，也会放在页缓存中，故而页缓存中的新数据越来越少，这样消费新的数据都需要从磁盘中读取。表现就是性能突然下降","likes_number":10,"is_delete":false,"is_hidden":false,"ctime":1582253600,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":11942,"ip_address":""},"score":179791,"extra":""},{"author":{"id":2707289,"avatar":"","nickname":"Geek_cdbba2","note":"","ucode":"8C439240FBC2B6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1130590,"avatar":"https://static001.geekbang.org/account/avatar/00/11/40/5e/b8fada94.jpg","nickname":"Ryoma","note":"","ucode":"7F692369239692","race_medal":2,"user_type":1,"is_pvip":true},"discussion":{"id":572283,"discussion_content":"可是经常会有新消费者组进来呀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652691235,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":179791,"ip_address":""},"score":572283,"extra":""}]}]},{"had_liked":false,"id":129291,"user_name":"诗泽","can_delete":false,"product_type":"c1","uid":1031865,"ip_address":"","ucode":"F28BE01C3FD12F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/be/b9/f2481c2c.jpg","comment_is_top":false,"comment_ctime":1567080776,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"57401655624","product_id":100029201,"comment_content":"如果将kafka 部署到k8s 中，因为k8s 的节点都是禁用swap 的，所以文中提到的swappiness 设置也就失效了","like_count":13},{"had_liked":false,"id":170934,"user_name":"diyun","can_delete":false,"product_type":"c1","uid":1615373,"ip_address":"","ucode":"B4A4290F35A3E0","user_header":"","comment_is_top":false,"comment_ctime":1578787555,"is_pvip":false,"replies":[{"id":"66336","content":"还是consumer和broker版本必须一致才能使用zero copy这样的特性 --- 是的，否则就要在broker端做消息转换，这样消息对象会在jvm堆上重建，丧失了zero copy特性。<br><br>","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1578876982,"ip_address":"","comment_id":170934,"utype":1}],"discussion_count":1,"race_medal":0,"score":"44528460515","product_id":100029201,"comment_content":"老师你好，我们生产遇到一个问题，一个kafka broker集群升级到1.0.0后，consumer 客户端版本还是老的0.10.0.1，这样没法使用zero copy特性了导致有大量数据写入后kafka broker OOM了。请问我如果把consumer 客户端版本升级到1.0.0后，他连接的另一个老的broker集群（还是0.10.0.1版本）是否还能使用zero copy？kafka版本是向下兼容的吗，还是consumer和broker版本必须一致才能使用zero copy这样的特性。","like_count":10,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":481171,"discussion_content":"还是consumer和broker版本必须一致才能使用zero copy这样的特性 --- 是的，否则就要在broker端做消息转换，这样消息对象会在jvm堆上重建，丧失了zero copy特性。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1578876982,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":148731,"user_name":"wgcris","can_delete":false,"product_type":"c1","uid":1527666,"ip_address":"","ucode":"842B76EB6B8320","user_header":"","comment_is_top":false,"comment_ctime":1573056803,"is_pvip":false,"replies":[{"id":"57284","content":"关闭自动leader均衡，手动调整leader迁移是目前比较好的做法","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1573089195,"ip_address":"","comment_id":148731,"utype":1}],"discussion_count":2,"race_medal":0,"score":"40227762467","product_id":100029201,"comment_content":"你好，请教您一个问题，关于leader均衡，如果集群规模比较大，一次leader均衡会有上千个partition要进行leader切换，这会导致客户端很长时间不可用，目前针对这个场景有没有一些比较成熟的解决方案？","like_count":9,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":473585,"discussion_content":"关闭自动leader均衡，手动调整leader迁移是目前比较好的做法","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1573089195,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1527666,"avatar":"","nickname":"wgcris","note":"","ucode":"842B76EB6B8320","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":45835,"discussion_content":"追代码发现，在服务端处理切换leader的操作是单线程加锁的，能否把这块做成多线程的，来减少大量leader切换耗时长的问题呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573089555,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129894,"user_name":"外星人","can_delete":false,"product_type":"c1","uid":1132861,"ip_address":"","ucode":"D8469B13F2AB37","user_header":"https://static001.geekbang.org/account/avatar/00/11/49/3d/4ac37cc2.jpg","comment_is_top":false,"comment_ctime":1567318778,"is_pvip":false,"replies":[{"id":"48508","content":"没有一定之规。不过据官网文章，单broker最多能承受2000个分区，这个和性能还是有很大关系的。毕竟分区数越多，物理IO性能就可能越差","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567385984,"ip_address":"","comment_id":129894,"utype":1}],"discussion_count":3,"race_medal":0,"score":"31632089850","product_id":100029201,"comment_content":"你好，一个broker建议最多存放多少个topic partition啊？这个个数和broker 的性能有啥关系吗？","like_count":7,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465702,"discussion_content":"没有一定之规。不过据官网文章，单broker最多能承受2000个分区，这个和性能还是有很大关系的。毕竟分区数越多，物理IO性能就可能越差","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567385984,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1913360,"avatar":"","nickname":"刘京","note":"","ucode":"AF42F5795CE077","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":260683,"discussion_content":"2000个分区是指2000个副本（leader+follower）还是2000个leader？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588895175,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1132861,"avatar":"https://static001.geekbang.org/account/avatar/00/11/49/3d/4ac37cc2.jpg","nickname":"外星人","note":"","ucode":"D8469B13F2AB37","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7277,"discussion_content":"主要性能开销在磁盘吗？那如果增多盘呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567470497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129120,"user_name":"lmtoo","can_delete":false,"product_type":"c1","uid":1133918,"ip_address":"","ucode":"FCD5B9C941D448","user_header":"https://static001.geekbang.org/account/avatar/00/11/4d/5e/c5c62933.jpg","comment_is_top":false,"comment_ctime":1567044588,"is_pvip":false,"replies":[{"id":"48168","content":"mount -l，默认是开启的，如果发现noatime则是关闭的。Linux 2.6.30引入了relatime。有了relatime，atime的更新时机被缩小了，如果atime=mtime就不会被更新了。","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567126762,"ip_address":"","comment_id":129120,"utype":1}],"discussion_count":2,"race_medal":0,"score":"31631815660","product_id":100029201,"comment_content":"怎么查询linux是否开启了atime","like_count":7,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465290,"discussion_content":"mount -l，默认是开启的，如果发现noatime则是关闭的。Linux 2.6.30引入了relatime。有了relatime，atime的更新时机被缩小了，如果atime=mtime就不会被更新了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567126762,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564943,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJUJKviaecwxpAZCAnHWap86kXUichv5JwUoAtrUNy4ugC0kMMmssFDdyayKFgAoA9Z62sqMZaibbvUg/132","nickname":"Geek_edc612","note":"","ucode":"3E01DE3CE4BF3E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6679,"discussion_content":"cat /etc/fstab","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567047846,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129222,"user_name":"小鱼","can_delete":false,"product_type":"c1","uid":1564537,"ip_address":"","ucode":"8C4157FE127B5E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/79/2ef99993.jpg","comment_is_top":false,"comment_ctime":1567064987,"is_pvip":false,"replies":[{"id":"48164","content":"&quot;producer版本与broker不一致时，也会降低性能&quot; --- 不会的","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567126492,"ip_address":"","comment_id":129222,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27336868763","product_id":100029201,"comment_content":"关于zero copy那块，是不是仅针对consumer端而言呢？还是说，producer版本与broker不一致时，也会降低性能？","like_count":6,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465341,"discussion_content":"&amp;quot;producer版本与broker不一致时，也会降低性能&amp;quot; --- 不会的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567126492,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":133224,"user_name":"ban","can_delete":false,"product_type":"c1","uid":1034204,"ip_address":"","ucode":"E523CE97E48266","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c7/dc/9408c8c2.jpg","comment_is_top":false,"comment_ctime":1568450887,"is_pvip":false,"replies":[{"id":"51248","content":"操作系统会被最近读取的page缓存起来，所以会“污染”页缓存","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1568593953,"ip_address":"","comment_id":133224,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23043287367","product_id":100029201,"comment_content":"老师，为什么测试 Console Consume读取比较老的数据，新的数据为什么会写入磁盘？这里不懂，Consume只是读取怎么会影响到写入","like_count":5,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467303,"discussion_content":"操作系统会被最近读取的page缓存起来，所以会“污染”页缓存","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568593953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1000347,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/43/9b/50927dfc.jpg","nickname":"小飞","note":"","ucode":"4C126548FBA94C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":20069,"discussion_content":"老数据（lag 很大的数据）在磁盘，新数据经常被使用在 cache中。\ncache 会比磁盘快几百倍吧。\n当需要考数据时候，就必须走磁盘了。 涉及磁盘io 的操作，效率都会明显下降。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569253804,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":135839,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1569287706,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18749156890","product_id":100029201,"comment_content":"没有kafka性能调优的经验，不过性能调优的思路是一致的。优化漏斗很形象，大部分调优主要在应用层，再深一点会到框架层，此时就需要对框架有很好的掌握啦！再深一点就到JVM了，这里主要是看内存空间分配是否合理，垃圾收集器是否正确选择。系统层调优，貌似没做过，这一层就必须对操作系统非常了解了。<br>万变不离其宗，提高性能的思路就那么几种：<br>1：使用更快的硬件，比如：内存<br>2：使用合适的数据结构<br>3：异步化<br>4：并行化<br>5：异步化和并行化，其实是在出现速度差的情况下，充分利用更快的组件的思路。","like_count":4},{"had_liked":false,"id":129559,"user_name":"ProgramGeek","can_delete":false,"product_type":"c1","uid":1008217,"ip_address":"","ucode":"3F0E3963C4FB57","user_header":"https://static001.geekbang.org/account/avatar/00/0f/62/59/a01a5ddd.jpg","comment_is_top":false,"comment_ctime":1567154260,"is_pvip":false,"replies":[{"id":"48517","content":"如果你并没有共享KafkaProducer实例，那么每个生产者最好设置成不同的transactional.id。2.0版本开始支持ACL前缀，可以用kafka-acls.sh --resource-pattern-type prefixed 一试","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567386642,"ip_address":"","comment_id":129559,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18747023444","product_id":100029201,"comment_content":"老师，最近遇到一个问题，在kafka加入SASL  ACL中，生产的时候出现需要给事务ID赋权，那有个问题在有多生产者的情况下，同一主题下的事务ID能一样吗？如果ID不能一样，那我在加入kafka的时候每次都需要赋权怎么办","like_count":4,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465539,"discussion_content":"如果你并没有共享KafkaProducer实例，那么每个生产者最好设置成不同的transactional.id。2.0版本开始支持ACL前缀，可以用kafka-acls.sh --resource-pattern-type prefixed 一试","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567386642,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206441,"user_name":"张振宇","can_delete":false,"product_type":"c1","uid":1130691,"ip_address":"","ucode":"7A6FD7294E65FF","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/c3/e545ba80.jpg","comment_is_top":false,"comment_ctime":1586861859,"is_pvip":false,"replies":[{"id":"77340","content":"hmmm... 直接删除log文件的做法非常不优雅。。。<br>如果你要删除至少也要关闭Kafka集群之后再这么做。<br>最优雅的方式还是通过删除topic的命令来执行","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587004068,"ip_address":"","comment_id":206441,"utype":1}],"discussion_count":2,"race_medal":0,"score":"14471763747","product_id":100029201,"comment_content":"老师，发现kafka的log文件比较大，执行rm删除log后df- h还未释放空间。<br>然后lsof -n &#47;data |grep deleted  发现查出来的进程是kafka的进程号，也不敢直接kill<br>想咨询下老师这种情况如何处理，有没更优雅的方式来优化。","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491789,"discussion_content":"hmmm... 直接删除log文件的做法非常不优雅。。。\n如果你要删除至少也要关闭Kafka集群之后再这么做。\n最优雅的方式还是通过删除topic的命令来执行","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587004068,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1272591,"avatar":"https://static001.geekbang.org/account/avatar/00/13/6b/0f/293b999c.jpg","nickname":"旋风","note":"","ucode":"B9FF02969F4307","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":307218,"discussion_content":"哥们是运维吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600535823,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":184836,"user_name":"SAM(陈嘉奇)","can_delete":false,"product_type":"c1","uid":1430905,"ip_address":"","ucode":"7A7B2767DE22E0","user_header":"https://static001.geekbang.org/account/avatar/00/15/d5/79/3d711fed.jpg","comment_is_top":false,"comment_ctime":1583416702,"is_pvip":false,"replies":[{"id":"71458","content":"嗯嗯，我看到那篇文章了。似乎swappiness作用的真实性有待确认。","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1583456711,"ip_address":"","comment_id":184836,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14468318590","product_id":100029201,"comment_content":"老师你好我查到的swappiness的解释是这样的:<br>This control is used to define how aggressive (sic) the kernel will swap memory pages. Higher values will increase aggressiveness, lower values decrease the amount of swap. A value of 0 instructs the kernel not to initiate swap until the amount of free and file-backed pages is less than the high water mark in a zone. <br>按这个意思swap = 0的意思应该不是不交换。","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":486140,"discussion_content":"嗯嗯，我看到那篇文章了。似乎swappiness作用的真实性有待确认。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583456711,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206849,"user_name":"李先生","can_delete":false,"product_type":"c1","uid":1237614,"ip_address":"","ucode":"D9039715F7D290","user_header":"https://static001.geekbang.org/account/avatar/00/12/e2/6e/0a300829.jpg","comment_is_top":false,"comment_ctime":1586944665,"is_pvip":false,"replies":[{"id":"77319","content":"针对外层消息。你下面的子消息自行维护","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587003186,"ip_address":"","comment_id":206849,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10176879257","product_id":100029201,"comment_content":"胡哥：<br>消费者避免重复消费消息，比如kafka中的一条消息包含10个子消息，是针对kafka中的一条消息做幂等呢，还是针对kafka的每个子消息做幂等呢？","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491923,"discussion_content":"针对外层消息。你下面的子消息自行维护","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003186,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":303431,"user_name":"vcjmhg","can_delete":false,"product_type":"c1","uid":1526461,"ip_address":"","ucode":"B508D1E9B3F974","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132","comment_is_top":false,"comment_ctime":1626784431,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5921751727","product_id":100029201,"comment_content":"这个优化思路感觉超级好！！","like_count":1},{"had_liked":false,"id":278088,"user_name":"John","can_delete":false,"product_type":"c1","uid":1018282,"ip_address":"","ucode":"F5ADD6603903D5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/89/aa/eb4c37db.jpg","comment_is_top":false,"comment_ctime":1612755480,"is_pvip":true,"replies":[{"id":"101013","content":"oom也没事啊，关键是要弄明白原因。其实我们并不是怕oom","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1612833132,"ip_address":"","comment_id":278088,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5907722776","product_id":100029201,"comment_content":"请问老师，swappiness越小越不积极swap内存，不是更容易oom吗？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515270,"discussion_content":"oom也没事啊，关键是要弄明白原因。其实我们并不是怕oom","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612833132,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":237651,"user_name":"Minze","can_delete":false,"product_type":"c1","uid":1534718,"ip_address":"","ucode":"2367DA6C09BE7C","user_header":"https://static001.geekbang.org/account/avatar/00/17/6a/fe/446c0282.jpg","comment_is_top":false,"comment_ctime":1595913682,"is_pvip":false,"replies":[{"id":"87822","content":"哪个版本的Kafka呢？如果是java consumer，查一下有没有提交位移失败的log？","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1595937736,"ip_address":"","comment_id":237651,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5890880978","product_id":100029201,"comment_content":"老师你好，我最近发现有时候consumergroup的current offset经常会几秒钟不动，而这是log end offset是不断增加的。我该从哪些方向着手检查呢？已经检查过了consumer端，消费的方法并没有阻塞的情况，执行时间都是毫秒级的，手动commit offset","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":502493,"discussion_content":"哪个版本的Kafka呢？如果是java consumer，查一下有没有提交位移失败的log？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595937736,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":191398,"user_name":"大坏狐狸","can_delete":false,"product_type":"c1","uid":1325678,"ip_address":"","ucode":"5044F89A505C5B","user_header":"https://static001.geekbang.org/account/avatar/00/14/3a/6e/e39e90ca.jpg","comment_is_top":false,"comment_ctime":1584770270,"is_pvip":false,"replies":[{"id":"73521","content":"不一定在任何场景下都是相反的","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1584846942,"ip_address":"","comment_id":191398,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5879737566","product_id":100029201,"comment_content":"优化tps和优化延迟 参数都是相反的啊，这个要根据需求来优化么。<br>那么tps如果增加了 延迟不是高了么，总的时间消耗 跟优化延迟哪个更可取呢? ","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":488164,"discussion_content":"不一定在任何场景下都是相反的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584846942,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":357194,"user_name":"小铁จุ๊บMia","can_delete":false,"product_type":"c1","uid":1389980,"ip_address":"广东","ucode":"1DBE9EDA70E61C","user_header":"https://static001.geekbang.org/account/avatar/00/15/35/9c/4ed5ae0a.jpg","comment_is_top":false,"comment_ctime":1663057181,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1663057181","product_id":100029201,"comment_content":"既然讲到调优，怎么对kafka进行性能压测呢？","like_count":0},{"had_liked":false,"id":335260,"user_name":"海水","can_delete":false,"product_type":"c1","uid":1191244,"ip_address":"","ucode":"68D7E454CC0819","user_header":"https://static001.geekbang.org/account/avatar/00/12/2d/4c/983ce1b9.jpg","comment_is_top":false,"comment_ctime":1645430897,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1645430897","product_id":100029201,"comment_content":"单一broker cpu 和带宽过高 感觉像是__consumer_offsets 相关的原因 老师 需要怎么排查问题呢","like_count":0},{"had_liked":false,"id":314754,"user_name":"曾泽浩","can_delete":false,"product_type":"c1","uid":1104601,"ip_address":"","ucode":"A7E5CF9E1571A2","user_header":"https://static001.geekbang.org/account/avatar/00/10/da/d9/f051962f.jpg","comment_is_top":false,"comment_ctime":1633403092,"is_pvip":false,"replies":[{"id":"114434","content":"这个和zero copy关系不大","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1634004337,"ip_address":"","comment_id":314754,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1633403092","product_id":100029201,"comment_content":"老师你好，Producer与broker之间有用到zero copy吗？看之前的课程有说到broker端会做一层的数据校验","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527784,"discussion_content":"这个和zero copy关系不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634004337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286569,"user_name":"毛怪","can_delete":false,"product_type":"c1","uid":1879932,"ip_address":"","ucode":"1E54CE9755AEF3","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/7c/6d90b40a.jpg","comment_is_top":false,"comment_ctime":1617378351,"is_pvip":false,"replies":[{"id":"104155","content":"能否描述得再详细一点，比如什么叫拉取失败，是说有异常抛出吗？","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1617670667,"ip_address":"","comment_id":286569,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617378351","product_id":100029201,"comment_content":"最近线上碰到一个问题spark指定分区消费Kafka，出现一个分区间歇性拉取消息失败，其他分区都正常。这个分区所在的brake有其他分区的leader。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518032,"discussion_content":"能否描述得再详细一点，比如什么叫拉取失败，是说有异常抛出吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617670667,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":262524,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1605756685,"is_pvip":false,"replies":[{"id":"95340","content":"删除有几种维度，比如按照时间和按照大小。按照时间的话就拿当前时间与日志段中最大时间戳进行比较——通常情况下，拥有最大时间戳的消息就是日志段的最后一条消息","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1605838287,"ip_address":"","comment_id":262524,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1605756685","product_id":100029201,"comment_content":"请教老师，Kafka删除数据的逻辑怎样的？<br>之前了解到删除数据最小单元是segment，是要segment中的所有数据都过期的再进行清理么<br><br>","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":509869,"discussion_content":"删除有几种维度，比如按照时间和按照大小。按照时间的话就拿当前时间与日志段中最大时间戳进行比较——通常情况下，拥有最大时间戳的消息就是日志段的最后一条消息","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605838287,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":250946,"user_name":"eric-xin","can_delete":false,"product_type":"c1","uid":1005634,"ip_address":"","ucode":"B10BB7796CDF6B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/58/42/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1601287303,"is_pvip":false,"replies":[{"id":"91823","content":"“每隔3周左右就会fullgc一次” --- 这个频率也还好吧，没有那么不堪吧。。。<br><br>先看下用的什么垃圾回收器，每种gc collector触发full gc的机制和原理也不太相同。另外还是要分析具体的heap dump。<br>","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1601344170,"ip_address":"","comment_id":250946,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1601287303","product_id":100029201,"comment_content":"老师 你好，我们生产环境碰到一个问题，broker有5个节点，单节点8核16G内存，有170多个topic，qps不高，堆内存设置6Gb，每隔3周左右机会fullgc一次，目前只能通过重启临时解决，课程中提到的gc分析方法都尝试过了，一直定位不到原因，您这边有没有排查思路？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506342,"discussion_content":"“每隔3周左右就会fullgc一次” --- 这个频率也还好吧，没有那么不堪吧。。。\n\n先看下用的什么垃圾回收器，每种gc collector触发full gc的机制和原理也不太相同。另外还是要分析具体的heap dump。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1601344170,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1005634,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/58/42/abb7bfe3.jpg","nickname":"eric-xin","note":"","ucode":"B10BB7796CDF6B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":309562,"discussion_content":"垃圾回收使用g1，一般3周左右，g1 old generation 占比会上涨到90%以上，一旦fullgc（13s左右），集群就开始报错，要是不重启集群会越来越严重，常见错误有：\norg.apache.kafka.common.errors.NotLeaderForPartitionException、WARN Unable to reconnect to ZooKeeper service，对线上broker直接heap dump，集群也会不稳定，kafka有没有轻量级的heap分析方案？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1601347684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":232259,"user_name":"James","can_delete":false,"product_type":"c1","uid":1134861,"ip_address":"","ucode":"48B0F2A334D1C1","user_header":"https://static001.geekbang.org/account/avatar/00/11/51/0d/fc1652fe.jpg","comment_is_top":false,"comment_ctime":1593936817,"is_pvip":false,"replies":[{"id":"85758","content":"会的。因为需要进行消息格式转换","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593997359,"ip_address":"","comment_id":232259,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593936817","product_id":100029201,"comment_content":"请问下,零拷贝这块,要是客户端生产者版本与Broker版本不一致,会失去这个特性吗&#47;","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500586,"discussion_content":"会的。因为需要进行消息格式转换","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593997359,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":232257,"user_name":"James","can_delete":false,"product_type":"c1","uid":1134861,"ip_address":"","ucode":"48B0F2A334D1C1","user_header":"https://static001.geekbang.org/account/avatar/00/11/51/0d/fc1652fe.jpg","comment_is_top":false,"comment_ctime":1593936303,"is_pvip":false,"replies":[{"id":"85759","content":"如果是Java客户端，那么下载Broker之后就默认自带客户端了","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593997403,"ip_address":"","comment_id":232257,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593936303","product_id":100029201,"comment_content":"请问老师,客户端使用的版本是不是从官网上找到服务端版本对应的客户端版本吧","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500585,"discussion_content":"如果是Java客户端，那么下载Broker之后就默认自带客户端了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593997403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":225794,"user_name":"高志强","can_delete":false,"product_type":"c1","uid":1276563,"ip_address":"","ucode":"68737002043752","user_header":"https://static001.geekbang.org/account/avatar/00/13/7a/93/c9302518.jpg","comment_is_top":false,"comment_ctime":1591859677,"is_pvip":false,"replies":[{"id":"83188","content":"通常只是会提升consumer端的TPS","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1591871238,"ip_address":"","comment_id":225794,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1591859677","product_id":100029201,"comment_content":"fetch.min.bytes=1  ，设置这个参数后，就会加快consumer消费速度么","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":497994,"discussion_content":"通常只是会提升consumer端的TPS","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591871238,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":182393,"user_name":"婷","can_delete":false,"product_type":"c1","uid":1612372,"ip_address":"","ucode":"1264B8F777B7CD","user_header":"https://static001.geekbang.org/account/avatar/00/18/9a/54/5d51fda4.jpg","comment_is_top":false,"comment_ctime":1582780334,"is_pvip":false,"replies":[{"id":"70665","content":"先确定消费很慢的含义吧。比如是从Kafka获取数据慢还是拿到数据处理速度慢。我碰到的很多情况是数据处理速度慢，这就和Kafka没关系了","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1582851488,"ip_address":"","comment_id":182393,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1582780334","product_id":100029201,"comment_content":"老师，您好。有个问题请教一下:集群中某个主题的消费很慢，于是对该主题加了分区，结果还是很慢。老师有没有好的解决办法？或者查找问题的思路。谢谢","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485326,"discussion_content":"先确定消费很慢的含义吧。比如是从Kafka获取数据慢还是拿到数据处理速度慢。我碰到的很多情况是数据处理速度慢，这就和Kafka没关系了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1582851488,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1612372,"avatar":"https://static001.geekbang.org/account/avatar/00/18/9a/54/5d51fda4.jpg","nickname":"婷","note":"","ucode":"1264B8F777B7CD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":195357,"discussion_content":"是消费者消费数据很慢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583262926,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129405,"user_name":"外星人","can_delete":false,"product_type":"c1","uid":1132861,"ip_address":"","ucode":"D8469B13F2AB37","user_header":"https://static001.geekbang.org/account/avatar/00/11/49/3d/4ac37cc2.jpg","comment_is_top":false,"comment_ctime":1567126228,"is_pvip":false,"replies":[{"id":"48223","content":"读物理磁盘，磁盘读通常是很好的","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567133369,"ip_address":"","comment_id":129405,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1567126228","product_id":100029201,"comment_content":"怎么知道是读物理盘？还是读页缓存呢？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465449,"discussion_content":"读物理磁盘，磁盘读通常是很好的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567133369,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1132861,"avatar":"https://static001.geekbang.org/account/avatar/00/11/49/3d/4ac37cc2.jpg","nickname":"外星人","note":"","ucode":"D8469B13F2AB37","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7042,"discussion_content":"嗯？没太理解？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567318690,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129200,"user_name":"许童童","can_delete":false,"product_type":"c1","uid":1003005,"ip_address":"","ucode":"4B799C0C6BC678","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4d/fd/0aa0e39f.jpg","comment_is_top":false,"comment_ctime":1567062120,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1567062120","product_id":100029201,"comment_content":"没有调优过Kafka ，但学习了老师今天的优化漏斗，感觉收获挺大，优化漏斗是否可以反过来变成优化金字塔。有些提升吞吐量的参数和降低延迟的参数是冲突的，这就要看业务更注重哪一块的性能了，一般用Kafka都是追求大吞吐量。","like_count":0},{"had_liked":false,"id":129121,"user_name":"Dovelol","can_delete":false,"product_type":"c1","uid":1253384,"ip_address":"","ucode":"9B5DDF7720F307","user_header":"https://static001.geekbang.org/account/avatar/00/13/20/08/bc06bc69.jpg","comment_is_top":false,"comment_ctime":1567044609,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1567044609","product_id":100029201,"comment_content":"老师好，页缓存指的是操作系统的PageCache吗？想请教下为什么给kafka预留的页缓存最好是一个日志段的大小，我的理解是一个日志段对应一个topic的partition，往往kafka都是及时读写的，所以是不是页缓存的大小和吞吐量有关系，也就是每秒的消费数据大小，最好保证消费的时候刚刚发送的数据还在页缓存中。","like_count":0,"discussions":[{"author":{"id":1325678,"avatar":"https://static001.geekbang.org/account/avatar/00/14/3a/6e/e39e90ca.jpg","nickname":"大坏狐狸","note":"","ucode":"5044F89A505C5B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210626,"discussion_content":"这里是页缓存大小改成一个G么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584756736,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}