{"id":80712,"title":"23丨SVM（下）：如何进行乳腺癌检测？","content":"<p>讲完了SVM的原理之后，今天我来带你进行SVM的实战。</p><p>在此之前我们先来回顾一下SVM的相关知识点。SVM是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。</p><p>上一节中讲到了硬间隔、软间隔、非线性SVM，以及分类间隔的公式，你可能会觉得比较抽象。这节课，我们会在实际使用中，讲解对工具的使用，以及相关参数的含义。</p><h2>如何在sklearn中使用SVM</h2><p>在Python的sklearn工具包中有SVM算法，首先需要引用工具包：</p><pre><code>from sklearn import svm\n</code></pre><p>SVM既可以做回归，也可以做分类器。</p><p>当用SVM做回归的时候，我们可以使用SVR或LinearSVR。SVR的英文是Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。</p><p>当做分类器的时候，我们使用的是SVC或者LinearSVC。SVC的英文是Support Vector Classification。</p><p>我简单说一下这两者之前的差别。</p><p>从名字上你能看出LinearSVC是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到SVM是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。</p><!-- [[[read_end]]] --><p>如果是针对非线性的数据，需要用到SVC。在SVC中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。</p><p>如何创建一个SVM分类器呢？</p><p>我们首先使用SVC的构造函数：model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)，这里有三个重要的参数kernel、C和gamma。</p><p>kernel代表核函数的选择，它有四种选择，只不过默认是rbf，即高斯核函数。</p><ol>\n<li>\n<p>linear：线性核函数</p>\n</li>\n<li>\n<p>poly：多项式核函数</p>\n</li>\n<li>\n<p>rbf：高斯核函数（默认）</p>\n</li>\n<li>\n<p>sigmoid：sigmoid核函数</p>\n</li>\n</ol><p>这四种函数代表不同的映射方式，你可能会问，在实际工作中，如何选择这4种核函数呢？我来给你解释一下：</p><p>线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。</p><p>多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。</p><p>高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。</p><p>了解深度学习的同学应该知道sigmoid经常用在神经网络的映射中。因此当选用sigmoid核函数时，SVM实现的是多层神经网络。</p><p>上面介绍的4种核函数，除了第一种线性核函数外，其余3种都可以处理线性不可分的数据。</p><p>参数C代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为1.0。当C越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C越小，泛化能力越强，但是准确性会降低。</p><p>参数gamma代表核函数的系数，默认为样本特征数的倒数，即gamma = 1 / n_features。</p><p>在创建SVM分类器之后，就可以输入训练集对它进行训练。我们使用model.fit(train_X,train_y)，传入训练集中的特征值矩阵train_X和分类标识train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用prediction=model.predict(test_X)来对结果进行预测，传入测试集中的样本特征矩阵test_X，可以得到测试集的预测分类结果prediction。</p><p>同样我们也可以创建线性SVM分类器，使用model=svm.LinearSVC()。在LinearSVC中没有kernel这个参数，限制我们只能使用线性核函数。由于LinearSVC对线性分类做了优化，对于数据量大的线性可分问题，使用LinearSVC的效率要高于SVC。</p><p>如果你不知道数据集是否为线性，可以直接使用SVC类创建SVM分类器。</p><p>在训练和预测中，LinearSVC和SVC一样，都是使用model.fit(train_X,train_y)和model.predict(test_X)。</p><h2>如何用SVM进行乳腺癌检测</h2><p>在了解了如何创建和使用SVM分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集，<a href=\"https://github.com/cystanford/breast_cancer_data/\">点击这里进行下载</a>。</p><p>医疗人员采集了患者乳腺肿块经过细针穿刺(FNA)后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/97/6a/97a33c5bfc182d571e9707db653eff6a.png?wh=864*326\" alt=\"\"><br>\n数据表一共包括了32个字段，代表的含义如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/13/1e6af6fa8bebdfba10457c111b5e9c13.jpg?wh=622*1031\" alt=\"\"></p><p>上面的表格中，mean代表平均值，se代表标准差，worst代表最大值（3个最大值的平均值）。每张图像都计算了相应的特征，得出了这30个特征值（不包括ID字段和分类标识结果字段diagnosis），实际上是10个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry和fractal_dimension_mean）的3个维度，平均、标准差和最大值。这些特征值都保留了4位数字。字段中没有缺失的值。在569个患者中，一共有357个是良性，212个是恶性。</p><p>好了，我们的目标是生成一个乳腺癌诊断的SVM分类器，并计算这个分类器的准确率。首先设定项目的执行流程：</p><p><img src=\"https://static001.geekbang.org/resource/image/97/f9/9768905bf3cf6d8946a64caa8575e1f9.png?wh=1216*558\" alt=\"\"></p><ol>\n<li>\n<p>首先我们需要加载数据源；</p>\n</li>\n<li>\n<p>在准备阶段，需要对加载的数据源进行探索，查看样本特征和特征值，这个过程你也可以使用数据可视化，它可以方便我们对数据及数据之间的关系进一步加深了解。然后按照“完全合一”的准则来评估数据的质量，如果数据质量不高就需要做数据清洗。数据清洗之后，你可以做特征选择，方便后续的模型训练；</p>\n</li>\n<li>\n<p>在分类阶段，选择核函数进行训练，如果不知道数据是否为线性，可以考虑使用SVC(kernel=‘rbf’) ，也就是高斯核函数的SVM分类器。然后对训练好的模型用测试集进行评估。</p>\n</li>\n</ol><p>按照上面的流程，我们来编写下代码，加载数据并对数据做部分的探索：</p><pre><code># 加载数据集，你需要把数据放到目录中\ndata = pd.read_csv(&quot;./data.csv&quot;)\n# 数据探索\n# 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来\npd.set_option('display.max_columns', None)\nprint(data.columns)\nprint(data.head(5))\nprint(data.describe())\n</code></pre><p>这是部分的运行结果，完整结果你可以自己跑一下。</p><pre><code>Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst'],\n      dtype='object')\n         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0 \n</code></pre><p>接下来，我们就要对数据进行清洗了。</p><p>运行结果中，你能看到32个字段里，id是没有实际含义的，可以去掉。diagnosis字段的取值为B或者M，我们可以用0和1来替代。另外其余的30个字段，其实可以分成三组字段，下划线后面的mean、se和worst代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。</p><pre><code># 将特征字段分成3组\nfeatures_mean= list(data.columns[2:12])\nfeatures_se= list(data.columns[12:22])\nfeatures_worst=list(data.columns[22:32])\n# 数据清洗\n# ID列没有用，删除该列\ndata.drop(&quot;id&quot;,axis=1,inplace=True)\n# 将B良性替换为0，M恶性替换为1\ndata['diagnosis']=data['diagnosis'].map({'M':1,'B':0})\n</code></pre><p>然后我们要做特征字段的筛选，首先需要观察下features_mean各变量之间的关系，这里我们可以用DataFrame的corr()函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。</p><pre><code># 将肿瘤诊断结果可视化\nsns.countplot(data['diagnosis'],label=&quot;Count&quot;)\nplt.show()\n# 用热力图呈现features_mean字段之间的相关性\ncorr = data[features_mean].corr()\nplt.figure(figsize=(14,14))\n# annot=True显示每个方格的数据\nsns.heatmap(corr, annot=True)\nplt.show()\n</code></pre><p>这是运行的结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/4d/a65435de48cee8091bd5f83d286ddb4d.png?wh=864*658\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/07/6e/0780e76fd3807759ab4881c2c39cb76e.png?wh=862*673\" alt=\"\"><br>\n热力图中对角线上的为单变量自身的相关系数是1。颜色越浅代表相关性越大。所以你能看出来radius_mean、perimeter_mean和area_mean相关性非常大，compactness_mean、concavity_mean、concave_points_mean这三个字段也是相关的，因此我们可以取其中的一个作为代表。</p><p>那么如何进行特征选择呢？</p><p>特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。</p><p>我们能看到mean、se和worst这三组特征是对同一组内容的不同度量方式，我们可以保留mean这组特征，在特征选择中忽略掉se和worst。同时我们能看到mean这组特征中，radius_mean、perimeter_mean、area_mean这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean这三个属性相关性大。我们分别从这2类中选择1个属性作为代表，比如radius_mean和compactness_mean。</p><p>这样我们就可以把原来的10个属性缩减为6个属性，代码如下：</p><pre><code># 特征选择\nfeatures_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean','symmetry_mean', 'fractal_dimension_mean'] \n</code></pre><p>对特征进行选择之后，我们就可以准备训练集和测试集：</p><pre><code># 抽取30%的数据作为测试集，其余作为训练集\ntrain, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n# 抽取特征选择的数值作为训练和测试数据\ntrain_X = train[features_remain]\ntrain_y=train['diagnosis']\ntest_X= test[features_remain]\ntest_y =test['diagnosis']\n</code></pre><p>在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：</p><pre><code># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1\nss = StandardScaler()\ntrain_X = ss.fit_transform(train_X)\ntest_X = ss.transform(test_X)\n</code></pre><p>最后我们可以让SVM做训练和预测了：</p><pre><code># 创建SVM分类器\nmodel = svm.SVC()\n# 用训练集做训练\nmodel.fit(train_X,train_y)\n# 用测试集做预测\nprediction=model.predict(test_X)\nprint('准确率: ', metrics.accuracy_score(test_y,prediction))\n</code></pre><p>运行结果：</p><pre><code>准确率:  0.9181286549707602\n</code></pre><p>准确率大于90%，说明训练结果还不错。完整的代码你可以从<a href=\"https://github.com/cystanford/breast_cancer_data\">GitHub</a>上下载。</p><h2>总结</h2><p>今天我带你一起做了乳腺癌诊断分类的SVM实战，从这个过程中你应该能体会出来整个执行的流程，包括数据加载、数据探索、数据清洗、特征选择、SVM训练和结果评估等环节。</p><p>sklearn已经为我们提供了很好的工具，对上节课中讲到的SVM的创建和训练都进行了封装，让我们无需关心中间的运算细节。但正因为这样，我们更需要对每个流程熟练掌握，通过实战项目训练数据化思维和对数据的敏感度。</p><p><img src=\"https://static001.geekbang.org/resource/image/79/82/797fe646ae4668139600fca2c50c5282.png?wh=864*309\" alt=\"\"><br>\n最后给你留两道思考题吧。还是这个乳腺癌诊断的数据，请你用LinearSVC，选取全部的特征（除了ID以外）作为训练数据，看下你的分类器能得到多少的准确度呢？另外你对sklearn中SVM使用又有什么样的体会呢？</p><p>欢迎在评论区与我分享你的答案，也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事，一起来交流，一起来进步。</p>","neighbors":{"left":{"article_title":"22丨SVM（上）：如何用一根棍子将蓝红两色球分开？","id":79975},"right":{"article_title":"24丨KNN（上）：如何根据打斗和接吻次数来划分电影类型？","id":80983}},"comments":[{"had_liked":false,"id":70989,"user_name":"Geek_dancer","can_delete":false,"product_type":"c1","uid":1440561,"ip_address":"","ucode":"F66D454E58E35E","user_header":"https://static001.geekbang.org/account/avatar/00/15/fb/31/f0a884a3.jpg","comment_is_top":false,"comment_ctime":1551239132,"is_pvip":false,"replies":[{"id":"40007","content":"赞下，不光做了练习，还做了分析总结。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562222448,"ip_address":"","comment_id":70989,"utype":2}],"discussion_count":2,"race_medal":0,"score":"186234832860","product_id":100021701,"comment_content":"默认SVC训练模型，6个特征变量，训练集准确率：96.0%，测试集准确率：92.4%<br>默认SVC训练模型，10个特征变量，训练集准确率：98.7% ，测试集准确率：98.2%<br>LinearSVC训练模型， 6个特征变量， 训练集准确率：93.9%，测试集准确率：92.3%<br>LinearSVC训练模型， 10个特征变量， 训练集准确率：99.4%，测试集准确率：96.0%<br><br>结论：<br>1. 增加特征变量可以提高准确率，可能是因为模型维度变高，模型变得更加复杂。可以看出特征变量的选取很重要。<br>2. 训练集拟合都比较好，但是测试集准确率出现不同程度的下降。<br>3. 模型训练的准确率与人类水平之间偏差可以通过增加特征变量或采用新的训练模型来降低；模型训练的准确率与测试集测试的准确率之间的方差可以通过正则化，提高泛化性能等方式来降低。","like_count":44,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440935,"discussion_content":"赞下，不光做了练习，还做了分析总结。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562222448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2360909,"avatar":"https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg","nickname":".","note":"","ucode":"B37F4749DE7E23","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":341225,"discussion_content":"大神  真特么专业","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610355569,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73287,"user_name":"reverse","can_delete":false,"product_type":"c1","uid":1240138,"ip_address":"","ucode":"0B546E540DF096","user_header":"https://static001.geekbang.org/account/avatar/00/12/ec/4a/40a2ba79.jpg","comment_is_top":false,"comment_ctime":1551851596,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"134695837772","product_id":100021701,"comment_content":"极客时间数据分析实战45讲的详细笔记(包含markdown、图片、思维导图 代码) github地址： https:&#47;&#47;github.com&#47;xiaomiwujiecao&#47;DataAnalysisInAction","like_count":31,"discussions":[{"author":{"id":2031616,"avatar":"","nickname":"Geek_255b45","note":"","ucode":"058D1EB016E8E9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289532,"discussion_content":"大赞！非常感谢您的无私分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594131403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1240138,"avatar":"https://static001.geekbang.org/account/avatar/00/12/ec/4a/40a2ba79.jpg","nickname":"reverse","note":"","ucode":"0B546E540DF096","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2031616,"avatar":"","nickname":"Geek_255b45","note":"","ucode":"058D1EB016E8E9","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289594,"discussion_content":"很久没更新了 如果您有要更新的细节 可以补充哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594140385,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":289532,"ip_address":""},"score":289594,"extra":""}]}]},{"had_liked":false,"id":87344,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555572568,"is_pvip":false,"replies":[{"id":"40009","content":"对的 特征选择，以及数据的处理都很重要。选择不同的特征，有时候比选择不同的分类算法更重要。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562222587,"ip_address":"","comment_id":87344,"utype":2}],"discussion_count":1,"race_medal":0,"score":"57390147416","product_id":100021701,"comment_content":"利用SVM做分类，特征选择影响度大，要想SVM分类准确，人工处理数据这一步很重要","like_count":13,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447406,"discussion_content":"对的 特征选择，以及数据的处理都很重要。选择不同的特征，有时候比选择不同的分类算法更重要。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562222587,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":76864,"user_name":"hlz-123","can_delete":false,"product_type":"c1","uid":1433586,"ip_address":"","ucode":"B7E5EF0C260BD2","user_header":"","comment_is_top":false,"comment_ctime":1552732238,"is_pvip":false,"replies":[{"id":"40008","content":"1、首先需要理解数据规范化的意义，当我们在做数据挖掘之前，不同的指标可能具有不同的量纲，如果我们不消除这些指标之间的取值范围差别，以及不同量纲所造成的影响的话，就会影响到后续数据分析（分类、聚类等）的结果。因此我们需要进行规范化处理，也就是让数据按照一定的标准落入到一个特定的区间内，便于我们后续分析使用。<br>2、在做处理的时候，训练集和测试集都需要进行数据规范化，其实在代码上并不麻烦，而且这么做属于数据挖掘前的一项基础工作。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562222514,"ip_address":"","comment_id":76864,"utype":2}],"discussion_count":1,"race_medal":0,"score":"44502405198","product_id":100021701,"comment_content":"首先要说，老师的课讲得非常好，深奥的算法和理论通过生动有趣的例子让人通俗易懂，兴趣盎然。<br>老师的本课案例中，对特征数据都做了Z-Score规范化处理（正态分布），准确率在90%以上，如果数据不做规范化处理，准确率在88%左右，我的问题：<br>1、数据规范化处理，是不是人为地提供了准确率？实际情况，数据不一定是正态分布。<br>2、模型建好后，在实际应用中去评估某个案例时，该案例数据是不是也要规范化，这样做是不是很麻烦并且数据对比不是很直观呢？","like_count":10,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":443477,"discussion_content":"1、首先需要理解数据规范化的意义，当我们在做数据挖掘之前，不同的指标可能具有不同的量纲，如果我们不消除这些指标之间的取值范围差别，以及不同量纲所造成的影响的话，就会影响到后续数据分析（分类、聚类等）的结果。因此我们需要进行规范化处理，也就是让数据按照一定的标准落入到一个特定的区间内，便于我们后续分析使用。\n2、在做处理的时候，训练集和测试集都需要进行数据规范化，其实在代码上并不麻烦，而且这么做属于数据挖掘前的一项基础工作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562222514,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70598,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551145237,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"31615916309","product_id":100021701,"comment_content":"# encoding=utf-8<br>from sklearn import svm<br>from sklearn import metrics<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>import pandas as pd<br>import seaborn  as sns<br>import matplotlib.pyplot as plt<br><br># 加载数据集，你需要把数据放到目录中<br>data = pd.read_csv(&quot;.&#47;data.csv&quot;)<br># 数据探索<br># 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来<br>pd.set_option(&#39;display.max_columns&#39;, None)<br>#print(data.columns)<br>#print(data.head(5))<br>#print(data.describe())<br><br># 将特征字段分成3组<br>features_mean= list(data.columns[2:12])<br>features_se= list(data.columns[12:22])<br>features_worst=list(data.columns[22:32])<br># 数据清洗<br># ID列没有用，删除该列<br>data.drop(&quot;id&quot;,axis=1,inplace=True)<br># 将B良性替换为0，M恶性替换为1<br>data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;: 1, &#39;B&#39;: 0})<br><br># 特征选择<br>features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,<br>       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,<br>       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,<br>       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,<br>       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,<br>       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,<br>       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,<br>       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,<br>       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;]<br><br># 抽取30%的数据作为测试集，其余作为训练集<br>train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test<br># 抽取特征选择的数值作为训练和测试数据<br>train_X = train[features_remain]<br>train_y=train[&#39;diagnosis&#39;]<br>test_X= test[features_remain]<br>test_y =test[&#39;diagnosis&#39;]<br><br># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1<br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.transform(test_X)<br><br># 创建SVM分类器<br>model = svm.LinearSVC()<br># 用训练集做训练<br>model.fit(train_X,train_y)<br># 用测试集做预测<br>prediction=model.predict(test_X)<br>print(&#39;准确率: &#39;, metrics.accuracy_score(prediction,test_y))<br><br>准确率: 0.9707602339181286","like_count":7,"discussions":[{"author":{"id":1068361,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","nickname":"明翼","note":"","ucode":"E77F86BEB3D5C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":46704,"discussion_content":"特征字段分成三组干嘛用的，没看出来……","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573197301,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65410,"user_name":"Rickie","can_delete":false,"product_type":"c1","uid":1352052,"ip_address":"","ucode":"F859B7837DFD18","user_header":"https://static001.geekbang.org/account/avatar/00/14/a1/74/3dfa4436.jpg","comment_is_top":false,"comment_ctime":1549338522,"is_pvip":false,"replies":[{"id":"40005","content":"如果选择全部的特征，一是运算量大，二是可能会过于拟合，造成实验中的准确率高，但是在实际工作中可能会存在偏差。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562222214,"ip_address":"","comment_id":65410,"utype":2}],"discussion_count":1,"race_medal":0,"score":"31614109594","product_id":100021701,"comment_content":"思考题：<br>使用全部数据进行训练得到的准确率为0.9766，高于示例中的准确率。是否是由于多重共线性，使得测试结果偏高？","like_count":7,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438441,"discussion_content":"如果选择全部的特征，一是运算量大，二是可能会过于拟合，造成实验中的准确率高，但是在实际工作中可能会存在偏差。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562222214,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":149702,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1573349214,"is_pvip":false,"replies":[{"id":"62771","content":"不错的分享，从EDA中找到特征的关系，大家都可以看下","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577090840,"ip_address":"","comment_id":149702,"utype":1}],"discussion_count":3,"race_medal":0,"score":"18753218398","product_id":100021701,"comment_content":"老师我利用了结果和特征的相关性，选择特征，发现结果更好：<br># 特征选择 按照结果和数据相关性选择特征准确率0.9707602339181286<br>features_remain = [&#39;radius_mean&#39;,&#39;perimeter_mean&#39;,&#39;area_mean&#39;,&#39;concave points_mean&#39;,&#39;radius_worst&#39;,&#39;perimeter_worst&#39;,&#39;area_worst&#39;,&#39;concave points_worst&#39;]","like_count":4,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":473945,"discussion_content":"不错的分享，从EDA中找到特征的关系，大家都可以看下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577090840,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1102507,"avatar":"https://static001.geekbang.org/account/avatar/00/10/d2/ab/55015ae6.jpg","nickname":"陈奇","note":"","ucode":"E4866AFD9D53B1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":49283,"discussion_content":"请问你是怎么去发现结果和特征的相关性的？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573569787,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1068361,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","nickname":"明翼","note":"","ucode":"E77F86BEB3D5C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1102507,"avatar":"https://static001.geekbang.org/account/avatar/00/10/d2/ab/55015ae6.jpg","nickname":"陈奇","note":"","ucode":"E4866AFD9D53B1","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":49314,"discussion_content":"不是有个变量直接关系的热图吗，看的出来","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573571184,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":49283,"ip_address":""},"score":49314,"extra":""}]}]},{"had_liked":false,"id":206640,"user_name":"Ricky","can_delete":false,"product_type":"c1","uid":1940971,"ip_address":"","ucode":"0BC8F8DB52F01D","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9d/eb/2c7f3d3b.jpg","comment_is_top":false,"comment_ctime":1586912666,"is_pvip":false,"replies":[{"id":"104216","content":"问题1：<br>确实没有定论。在日常业务场景中，按对任务目标的理解，把相关的特征的数据拉齐后建模，再筛减特征提高准确率。在比赛中，由于对该业务不一定十分了解，那么久需要做各种特征工程的尝试，如：特征提取、特征转换、特征组合等等，同学可以多看看kaggle比赛的高分开源notebook。<br>问题2：老师这里进行了简化，实际你可以对data.corr()相关系数进行热力图展示，和目标值diagnosis低相关性的特征常会被舍弃，比如se的特征；另外当几个特征之间相关性非常强趋近于1时，常称之为多重共线性，对于存在多重共线性的特征，常只保留其中一个特征。当然做特征选择时，还有其他工具，比如：PCA、低方差分析等，老师这里只展示了相关系数可视化后进行特征选择的方法。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617730634,"ip_address":"","comment_id":206640,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10176847258","product_id":100021701,"comment_content":"谢谢，提2个问题，<br>1）在实际应用中如何平衡特征变量和准确率的关系？有没有方法论？<br>增加特征变量意味着增加运算时间，提高准确率，但是这个得失怎么把握？同时如何评估会增加多少运算时间，一个一个尝试似乎比较费劲吧<br>2）此文的案例是选用平均值，丢弃了最大值和标准差，这个是多少案例的通用做法么？<br><br>谢谢","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491848,"discussion_content":"问题1：\n确实没有定论。在日常业务场景中，按对任务目标的理解，把相关的特征的数据拉齐后建模，再筛减特征提高准确率。在比赛中，由于对该业务不一定十分了解，那么久需要做各种特征工程的尝试，如：特征提取、特征转换、特征组合等等，同学可以多看看kaggle比赛的高分开源notebook。\n问题2：老师这里进行了简化，实际你可以对data.corr()相关系数进行热力图展示，和目标值diagnosis低相关性的特征常会被舍弃，比如se的特征；另外当几个特征之间相关性非常强趋近于1时，常称之为多重共线性，对于存在多重共线性的特征，常只保留其中一个特征。当然做特征选择时，还有其他工具，比如：PCA、低方差分析等，老师这里只展示了相关系数可视化后进行特征选择的方法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617730634,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":200762,"user_name":"恬恬","can_delete":false,"product_type":"c1","uid":1237977,"ip_address":"","ucode":"12335280B53040","user_header":"https://static001.geekbang.org/account/avatar/00/12/e3/d9/bda5e991.jpg","comment_is_top":false,"comment_ctime":1585647345,"is_pvip":false,"replies":[{"id":"104329","content":"SVC(kernel=&#39;linear&#39;）和LinearSVC()使用的都是线性核。<br>默认情况下，LinearSVC最小化squared hinge loss，而SVC最小化hinge loss。LinearSVC基于liblinear实现，事实上会惩罚截距(penalize the intercept), 然而，SVC是基于libsvm实现的，并不会惩罚截距。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617902862,"ip_address":"","comment_id":200762,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10175581937","product_id":100021701,"comment_content":"对比几组feature后，发现用feature_worst进行训练，效果更好。<br>1）SVC(kernel=&#39;linear&#39;)的测试集准确率为：99.42%；<br>2)  LinearSVC()的测试集准确率为：97.07%<br>2）SVC()的测试集准确率为：96.49%<br>觉得建模过程中，特征选择很重要，不同的数据集划分，正负样本是否平衡也会对结果有一定的影响，所以最好是可以采用交叉验证来训练模型。这个地方多次测试SVC(kernel=&#39;linear&#39;）和LinearSVC()，感觉还是会存在2个百分点左右的差异，这两个都算是线性分类，是因为采用了不同的线性核函数吗？还是其他参数或是方法差异的原因呢？","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490129,"discussion_content":"SVC(kernel=&amp;#39;linear&amp;#39;）和LinearSVC()使用的都是线性核。\n默认情况下，LinearSVC最小化squared hinge loss，而SVC最小化hinge loss。LinearSVC基于liblinear实现，事实上会惩罚截距(penalize the intercept), 然而，SVC是基于libsvm实现的，并不会惩罚截距。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617902862,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1919541,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg","nickname":"完美坚持","note":"","ucode":"AE0261D8DDEF64","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365951,"discussion_content":"涉及到源代码和底层算法了……","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617930831,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":151402,"user_name":"Ronnyz","can_delete":false,"product_type":"c1","uid":1488280,"ip_address":"","ucode":"9F34527B1D343D","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg","comment_is_top":false,"comment_ctime":1573713688,"is_pvip":false,"replies":[{"id":"62667","content":"对 一般是这个规律，具体还是要看数据集的分布情况，所以不同的模型之间没有绝对好坏之分，和数据集也有关系","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577082234,"ip_address":"","comment_id":151402,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5868680984","product_id":100021701,"comment_content":"选取全部特征：<br>SVM分类器准确率： 0.9824561403508771<br>cross_val_score的准确率为：0.9727<br>linearSVM分类器的准确率： 0.9766081871345029<br>cross_val_score的准确率为：0.9652<br><br>选取mean相关特征：<br>SVM分类器准确率： 0.9239766081871345<br>cross_val_score的准确率为：0.9321<br>linearSVM分类器的准确率： 0.9298245614035088<br>cross_val_score的准确率为：0.9247<br><br>数据结果上看：<br>SVM的结果要好于linearSVM;<br>选取多特征的结果要好于选取少特征的结果","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474461,"discussion_content":"对 一般是这个规律，具体还是要看数据集的分布情况，所以不同的模型之间没有绝对好坏之分，和数据集也有关系","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577082234,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87394,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555582134,"is_pvip":false,"replies":[{"id":"64245","content":"Good Job 滢","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577615182,"ip_address":"","comment_id":87394,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850549430","product_id":100021701,"comment_content":"语言Python3.6  没有z-score规范化数据以及规范化后两种情况前提预测准确率，使用LinearSVC，选取所有mean属性<br><br>import  pandas as  pd<br>import  matplotlib.pyplot as  plt<br>import  seaborn as  sns<br>from sklearn.model_selection import  train_test_split<br>from sklearn import  svm<br>from sklearn import  metrics<br>from sklearn.preprocessing import  StandardScaler<br><br>#导入数据<br>path = &#39;&#47;Users&#47;apple&#47;Desktop&#47;GitHubProject&#47;Read mark&#47;数据分析&#47;geekTime&#47;data&#47;&#39;<br>data = pd.read_csv(path + &#39;breast_cancer&#47;data.csv&#39;)<br><br>#数据探索<br>pd.set_option(&#39;display.max_columns&#39;, None)<br>print(data.columns)<br>print(data.head(5))<br>print(data.describe())<br><br>#将特征字段进行分组<br>features_mean = list(data.columns[2:12])<br>features_se = list(data.columns[12:22])<br>features_worst = list(data.columns[22:32])<br><br>#数据清洗<br>#删除ID列<br>data.drop(&#39;id&#39;,axis=1,inplace=True)<br>#将良性B替换为0，将恶性替换为1<br>data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;B&#39;:0,&#39;M&#39;:1})<br><br>#将肿瘤诊断结果可视化<br>sns.countplot(data[&#39;diagnosis&#39;],label=&#39;count&#39;)<br>plt.show()<br>#计算相关系数<br>corr = data[features_mean].corr()<br>plt.figure(figsize=(14,14))<br><br>#用热力图呈现相关性，显示每个方格的数据<br>sns.heatmap(corr,annot=True)<br>plt.show()<br><br>#特征选择，选择所有的mean数据<br>feature_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,<br>       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,<br>       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]<br><br>#抽取30%特征选择作为测试数据，其余作为训练集<br>train,test = train_test_split(data,test_size=0.3)<br>#抽取特征选择作为训练和测试数据<br>train_data = train[feature_remain]<br>train_result = train[&#39;diagnosis&#39;]<br>test_data = test[feature_remain]<br>test_result = test[&#39;diagnosis&#39;]<br><br>#创建SVM分类器<br>model = svm.LinearSVC()<br>#用训练集做训练<br>model.fit(train_data,train_result)<br>#用测试集做预测<br>prediction = model.predict(test_data)<br>#准确率<br>print(&#39;准确率:&#39;, metrics.accuracy_score(prediction,test_result))<br><br>#规范化数据，再预估准确率<br>z_score = StandardScaler()<br>train_data = z_score.fit_transform(train_data)<br>test_data = z_score.transform(test_data)<br>#用新数据做训练<br>new_model = svm.LinearSVC()<br>new_model.fit(train_data,train_result)<br>#重新预测<br>new_prediction = new_model.predict(test_data)<br>#准确率<br>print(&#39;准确率:&#39;,metrics.accuracy_score(new_prediction,test_result))","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447427,"discussion_content":"Good Job 滢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615182,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":74365,"user_name":"三硝基甲苯","can_delete":false,"product_type":"c1","uid":1141929,"ip_address":"","ucode":"C492B058C2A5C0","user_header":"","comment_is_top":false,"comment_ctime":1552203025,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5847170321","product_id":100021701,"comment_content":"用K折交叉验证，LinearSVC的准确率是92.64% SVC是92.98% <br>至于SVC的使用，我一开始直接按照自己的想法写完以后，会有聚合警告，然后看了一下，是数据没有进行StandardScaler，我觉得这个步骤容易忘记。","like_count":1},{"had_liked":false,"id":68291,"user_name":"third","can_delete":false,"product_type":"c1","uid":1025114,"ip_address":"","ucode":"9A37408A834F0B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg","comment_is_top":false,"comment_ctime":1550479226,"is_pvip":false,"replies":[{"id":"40006","content":"挺好的，慢慢来，做的多了就好了！","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562222359,"ip_address":"","comment_id":68291,"utype":2}],"discussion_count":1,"race_medal":0,"score":"5845446522","product_id":100021701,"comment_content":"第二个，准确率 0.935672514619883。<br><br>感觉还蛮好用的，只是不是很熟练的使用各个算法做分类和回归","like_count":1,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439612,"discussion_content":"挺好的，慢慢来，做的多了就好了！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562222359,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65311,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":false,"comment_ctime":1549240594,"is_pvip":false,"replies":[{"id":"64610","content":"可以用PCA做特征选择，PCA相当于是降维工具，手动的可解释性会比较强","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577622398,"ip_address":"","comment_id":65311,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5844207890","product_id":100021701,"comment_content":"老师可以用PCA进行特征选择吗？如果可以，那和你这种手动的方法比有什么差别","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438399,"discussion_content":"可以用PCA做特征选择，PCA相当于是降维工具，手动的可解释性会比较强","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622398,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288993,"user_name":"Soul of the Dragon","can_delete":false,"product_type":"c1","uid":2438011,"ip_address":"","ucode":"21603099E51B8A","user_header":"https://static001.geekbang.org/account/avatar/00/25/33/7b/9e012181.jpg","comment_is_top":false,"comment_ctime":1618813117,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1618813117","product_id":100021701,"comment_content":"本人在思考题的基础上，进行了进一步的探索。使用features_se和features_worst的数据，分别采用SVC和LinearSVC建模进行训练。将不同样本训练的准确率进行比较后，得出以下结论：<br>1. 从模型选择上看，使用SVC模型训练的准确率总体要高于LinearSVC;<br>2. 从样本数量上来看，除features_worst数据集外，选择全样本训练的准确率要高于选择筛选特征后样本的准确率；<br>3. 从数据集的比较来看，选择features_worst数据集进行训练的准确率在三个数据集中总体是最高的。","like_count":0},{"had_liked":false,"id":275151,"user_name":"晨曦","can_delete":false,"product_type":"c1","uid":2396062,"ip_address":"","ucode":"A64E0343B53C25","user_header":"https://static001.geekbang.org/account/avatar/00/24/8f/9e/a0a36b1c.jpg","comment_is_top":false,"comment_ctime":1611335168,"is_pvip":false,"replies":[{"id":"100863","content":"SVM也是支持多分类的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1612705069,"ip_address":"","comment_id":275151,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1611335168","product_id":100021701,"comment_content":"老师，如果这个病症改为不得病，轻症，重症三个分类，不是二分类问题，对应改成分类序号，0,1,2。那么这套算法是不是也不算错","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514233,"discussion_content":"SVM也是支持多分类的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612705069,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268617,"user_name":"McKee Chen","can_delete":false,"product_type":"c1","uid":2037505,"ip_address":"","ucode":"F74B76542FAB65","user_header":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","comment_is_top":false,"comment_ctime":1608274422,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1608274422","product_id":100021701,"comment_content":"学习过程中遇到以下两个问题：<br>1.随机划分不同的训练集训练出来的模型最后得出来的准确率是不一样的，有95%的准确率，也有97%的准确率，等等，这种情况下是要不断训练模型，直到观察到最高的准确率，然后选用该模型吗？还是随意选择一个模型，只要准确率够高就行？<br>2.准确率高等价于预测效果精度高吗？<br><br>#用 LinearSVC，选取全部的特征（除了 ID 以外）作为训练数据，查看分类器的准确度<br>import pandas as pd<br>import seaborn as sns<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>#数据加载<br>data = pd.read_csv(r&#39;C:\\Users\\chentaoyu\\Desktop\\breast_cancer_data&amp;codes\\data.csv&#39;)<br>#数据探索<br>pd.set_option(&#39;display.max_columns&#39;, None)#显示全部列，不隐藏中间列<br># print(data.info())#说明各特征没有缺失值<br># print(data.head())#列出前五行<br># print(data.columns)#看出所有特征名<br># print(data.tail())#列出后五行<br>#数据清洗<br>data.drop(&#39;id&#39;,axis=1,inplace=True)#舍去ID列<br>data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;B&#39;:0, &#39;M&#39;:1})#将诊断特征中的结果数据化<br>#选取全部的特征值进行训练，选30%为测试数据，其余为训练数据<br>features = data[list(data.columns[1:31])]<br>test = data[&#39;diagnosis&#39;]               <br>train_X, test_X, train_y, test_y = train_test_split(features, test ,test_size=0.3)<br>#对数据进行规范化，避免因数据量级不同造成误差<br>train_X = StandardScaler().fit_transform(train_X)<br>test_X = StandardScaler().fit_transform(test_X)<br>#创建SVM分类器<br>from sklearn import svm<br>linear_svc = svm.LinearSVC()<br>linear_svc.fit(train_X, train_y)<br>#输入测试值，通过该模型预测测试结果<br>prediction = linear_svc.predict(test_X)<br>print(&#39;准确率: &#39;, accuracy_score(test_y, prediction))<br><br>#输出结果<br>准确率:  0.9766081871345029","like_count":0},{"had_liked":false,"id":263135,"user_name":"非同凡想","can_delete":false,"product_type":"c1","uid":1934969,"ip_address":"","ucode":"713FD449A49D5A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg","comment_is_top":false,"comment_ctime":1606017504,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1606017504","product_id":100021701,"comment_content":"import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn import svm<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.metrics import accuracy_score<br>import pandas as pd<br><br>df = pd.read_csv(&#39;~&#47;Documents&#47;breast_cancer_data&#47;data.csv&#39;)<br>print(df.info())<br><br>features = df.columns[2:]<br>#features = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]<br><br># 将B良性替换为0，M恶性替换为1<br>df[&#39;diagnosis&#39;] = df[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})<br><br>train , test = train_test_split(df, test_size=0.3)<br><br>train_x = train[features]<br>train_y = train[&#39;diagnosis&#39;]<br>test_x = test[features]<br>test_y = test[&#39;diagnosis&#39;]<br><br>##z_score标准化数据<br>ss = StandardScaler()<br>train_x = ss.fit_transform(train_x)<br>test_x = ss.transform(test_x)<br><br>model = svm.LinearSVC()<br><br>model.fit(train_x, train_y)<br>prediction = model.predict(test_x)<br>print(&#39;accuracy : %f&#39;,accuracy_score(prediction, test_y))<br><br>## SVC       30个特征值  准确度为：98 %<br>## SVC        6个特征值  准确度为：92 %<br>## LinearSVC 30个特征值  准确度为：97 %<br>## LinearSVC  6个特征值  准确度为：92 %<br><br>","like_count":0},{"had_liked":false,"id":244223,"user_name":"邹洲","can_delete":false,"product_type":"c1","uid":2140519,"ip_address":"","ucode":"C7D0AF30B85D56","user_header":"https://static001.geekbang.org/account/avatar/00/20/a9/67/b53898d7.jpg","comment_is_top":false,"comment_ctime":1598433822,"is_pvip":false,"replies":[{"id":"103501","content":"之所以每次运行不一样，是因为拆分数据集的方法train_test_split()每次会随机拆分数据集；可以通过它的参数random_state指定一个整数作为随机数种子，可以保证每次运行拆分得到的都是同样的数据集。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1616689490,"ip_address":"","comment_id":244223,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1598433822","product_id":100021701,"comment_content":"# coding=utf-8<br>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>#导入sklearn中的SVM库<br>from sklearn import svm, metrics<br><br>#构造线性分类模型<br>model = svm.SVC(kernel=&quot;rbf&quot;,C=1.0,gamma=&quot;auto&quot;)<br>model2 = svm.LinearSVC()<br>&quot;&quot;&quot;<br>kernel :<br>    linear:线性模型，当模型为linearsvm时，就表明没有kernel参数<br>    poly：多项式模型<br>    rbf:高斯函数<br>    sigmoid:sigmoid核函数<br>C：目标函数的惩罚系数<br>gamma :核函数系数<br>&quot;&quot;&quot;<br><br>#实战开始--加载数据<br>data = pd.read_csv(&#39;data.csv&#39;,engine=&#39;python&#39;)<br># pd.set_option(&#39;display.max_columns&#39;,None)<br>#查看一下数据信息<br># print(data.columns)<br># print(data.info())<br># print(data.head(5))<br># print(data[&#39;diagnosis&#39;].value_counts())<br><br>#数据处理<br>#id字段对于分类无用，删除即可<br>data.drop(&#39;id&#39;,axis=1,inplace=True)<br>#diagnosis字段为字符型，组需转换为数值型0 1<br>data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})<br><br>feature_mean = data.columns[1:11]<br>feature_se = data.columns[11:21]<br>feature_max = data.columns[21:31]<br><br><br>#统计一下肿瘤人数情况<br>sns.countplot(data[&#39;diagnosis&#39;],label=&#39;Count&#39;)<br># plt.show()<br><br>#查看各个特征的相关度<br>corr = data[feature_mean].corr()<br>plt.figure(figsize=(14,14))<br>sns.heatmap(corr,annot=True)<br># plt.show()<br><br>feature_sel = [&#39;radius_mean&#39;,&#39;texture_mean&#39;,&#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;,&#39;fractal_dimension_mean&#39;]<br><br><br>train_data,test_data = train_test_split(data,test_size=0.3)<br><br>train_feature = train_data[feature_sel]<br>train_label = train_data[&#39;diagnosis&#39;]<br>test_feature = test_data[feature_sel]<br>test_label = test_data[&#39;diagnosis&#39;]<br><br>#开始训练数据和测试数据<br>#采用Z-Score规范化处理，保证每一个特征维度的数据均值为0，方差为1<br>ss = StandardScaler()<br>train_feature = ss.fit_transform(train_feature)<br>test_feature = ss.transform(test_feature)<br><br>#开始预测<br>model.fit(train_feature,train_label)<br>model2.fit(train_feature,train_label)<br>predict_label = model.predict(test_feature)<br>predict_label2 = model2.predict(test_feature)<br>#准确度 -- 每次运行不一样<br>print(&quot;高斯准确率：&quot;,metrics.accuracy_score(test_label,predict_label))<br>print(&quot;线性准确度：&quot;,metrics.accuracy_score(test_label,predict_label2))<br><br><br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504525,"discussion_content":"之所以每次运行不一样，是因为拆分数据集的方法train_test_split()每次会随机拆分数据集；可以通过它的参数random_state指定一个整数作为随机数种子，可以保证每次运行拆分得到的都是同样的数据集。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616689490,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":243348,"user_name":"朱一江","can_delete":false,"product_type":"c1","uid":2112502,"ip_address":"","ucode":"497E4A3FD34F9E","user_header":"","comment_is_top":false,"comment_ctime":1598067439,"is_pvip":false,"replies":[{"id":"103066","content":"是的。sklearn中的准确率API accuracy_score()是真实值和预测值之间进行比较得出。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1616085380,"ip_address":"","comment_id":243348,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1598067439","product_id":100021701,"comment_content":"我们所测的准确率是与train_y进行比较的吗<br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504263,"discussion_content":"是的。sklearn中的准确率API accuracy_score()是真实值和预测值之间进行比较得出。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616085380,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":233984,"user_name":"冷忆丶思乡月","can_delete":false,"product_type":"c1","uid":2043556,"ip_address":"","ucode":"4D7493B97A872E","user_header":"https://static001.geekbang.org/account/avatar/00/1f/2e/a4/51d72aaf.jpg","comment_is_top":false,"comment_ctime":1594545210,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1594545210","product_id":100021701,"comment_content":"测试了一下三种类型的特征值的预测准确率：<br>worst 最高，<br>mean 次之，<br>se 最差<br>造成这种情况，有没有哪些概念性的知识解释呢？<br>分类的问题可不可以这样理解，算法将个体差异的权重放大，将相似点的权重减小，最终计算一个重视差异的值来区分分类。最大值最小值恰好就是分类个体的差异数据，所以对分类效果会更好。但是mean均值，会缩减这种差异性，所以分类性会差一些？","like_count":0},{"had_liked":false,"id":195271,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1585152282,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585152282","product_id":100021701,"comment_content":"提个问题：corr相关性矩阵，里面的接近1和接近-1该怎么选择特征，选择一对接近1和-1的特征会把效果抵消吗？","like_count":0},{"had_liked":false,"id":184364,"user_name":"鱼非子","can_delete":false,"product_type":"c1","uid":1818595,"ip_address":"","ucode":"BB76AE2CB4D680","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg","comment_is_top":false,"comment_ctime":1583298753,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583298753","product_id":100021701,"comment_content":"import numpy as np<br>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn import svm<br>from sklearn import metrics<br><br><br>data = pd.read_csv(&quot;.&#47;data.csv&quot;)<br>pd.set_option(&#39;display.max_columns&#39;, None)<br># print(data.head(5))<br># print(data.columns)<br># print(data.describe())<br><br>features_mean = list(data.columns[2:12])<br>features_se = list(data.columns[12:22])<br>features_worst = list(data.columns[22:32])<br><br>data.drop(&quot;id&quot;,axis=1,inplace=True)<br>data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})<br>y = data[&#39;diagnosis&#39;]<br># sns.countplot(data[&#39;diagnosis&#39;],label=&quot;Count&quot;)<br># plt.show()<br>data.drop(&quot;diagnosis&quot;,axis=1,inplace=True)<br># print(data.head(5))<br>#<br># corr = data[features_mean].corr()<br># plt.figure(figsize=(14,14))<br># sns.heatmap(corr,annot=True)<br># plt.show()<br><br>features_remain = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]<br># X_train, X_test, y_train, y_test = train_test_split(data[features_remain], data[&#39;diagnosis&#39;], test_size=0.3,random_state=0)<br>X_train, X_test, y_train, y_test = train_test_split(data,y,test_size=0.3)<br># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1<br>ss = StandardScaler()<br>X_train = ss.fit_transform(X_train)<br>X_test = ss.transform(X_test)<br><br># model = svm.SVC()<br>model = svm.LinearSVC()<br>model.fit(X_train,y_train)<br><br>prediction = model.predict(X_test)<br>print(&quot;准确率：&quot;,metrics.accuracy_score(prediction,y_test))<br><br>准确率： 0.9649122807017544","like_count":0},{"had_liked":false,"id":184361,"user_name":"鱼非子","can_delete":false,"product_type":"c1","uid":1818595,"ip_address":"","ucode":"BB76AE2CB4D680","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg","comment_is_top":false,"comment_ctime":1583298109,"is_pvip":false,"replies":[{"id":"104370","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617905375,"ip_address":"","comment_id":184361,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1583298109","product_id":100021701,"comment_content":"import numpy as np<br>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn import svm<br>from sklearn import metrics<br><br><br>data = pd.read_csv(&quot;.&#47;data.csv&quot;)<br>pd.set_option(&#39;display.max_columns&#39;, None)<br># print(data.head(5))<br># print(data.columns)<br># print(data.describe())<br><br>features_mean = list(data.columns[2:12])<br>features_se = list(data.columns[12:22])<br>features_worst = list(data.columns[22:32])<br><br>data.drop(&quot;id&quot;,axis=1,inplace=True)<br>data[&#39;diagnosis&#39;] = data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})<br># print(data.head(5))<br><br>sns.countplot(data[&#39;diagnosis&#39;],label=&quot;Count&quot;)<br>plt.show()<br><br>corr = data[features_mean].corr()<br>plt.figure(figsize=(14,14))<br>sns.heatmap(corr,annot=True)<br>plt.show()<br><br>features_remain = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]<br>X_train, X_test, y_train, y_test = train_test_split(data[features_remain], data[&#39;diagnosis&#39;], test_size=0.3,random_state=0)<br><br># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1<br>ss = StandardScaler()<br>X_train = ss.fit_transform(X_train)<br>X_test = ss.transform(X_test)<br><br>model = svm.SVC()<br>model.fit(X_train,y_train)<br><br>prediction = model.predict(X_test)<br>print(&quot;准确率：&quot;,metrics.accuracy_score(prediction,y_test))<br><br>准确率： 0.9239766081871345<br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485987,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617905375,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":183866,"user_name":"学技术攒钱开宠物店","can_delete":false,"product_type":"c1","uid":1849036,"ip_address":"","ucode":"3EA297FD4C9635","user_header":"https://static001.geekbang.org/account/avatar/00/1c/36/cc/499625d3.jpg","comment_is_top":false,"comment_ctime":1583150481,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1583150481","product_id":100021701,"comment_content":"我用全部特征linearSVC准确率直接到1了0.0","like_count":0,"discussions":[{"author":{"id":2041368,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/26/18/733c3dfa.jpg","nickname":"少有人走的路","note":"","ucode":"944613DA3E0E81","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585333,"discussion_content":"我猜你把diagnosis当成了特征进行训练","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661486917,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":140565,"user_name":"追梦","can_delete":false,"product_type":"c1","uid":1183831,"ip_address":"","ucode":"54C6E76E8FE033","user_header":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","comment_is_top":false,"comment_ctime":1571005633,"is_pvip":false,"replies":[{"id":"63488","content":"有两种方式，第一方式直接画图，可视化的方式呈现，是否具有线性的关系，另一种方式就是用线性可分的分类器来做分类，看下结果能否正确划分","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577514288,"ip_address":"","comment_id":140565,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1571005633","product_id":100021701,"comment_content":"老师，请问用什么方法可以判定数据集是否为线性可分的呢","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":470445,"discussion_content":"有两种方式，第一方式直接画图，可视化的方式呈现，是否具有线性的关系，另一种方式就是用线性可分的分类器来做分类，看下结果能否正确划分","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577514288,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":136274,"user_name":"chenzhenwei","can_delete":false,"product_type":"c1","uid":1054446,"ip_address":"","ucode":"298B4A4CE50A3C","user_header":"https://static001.geekbang.org/account/avatar/00/10/16/ee/73c85368.jpg","comment_is_top":false,"comment_ctime":1569401828,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1569401828","product_id":100021701,"comment_content":"为什么LinearSVC使用所有的属性效果比使用六个不相关的属性好很多？","like_count":0},{"had_liked":false,"id":117546,"user_name":"Geek_6a6ff8","can_delete":false,"product_type":"c1","uid":1590704,"ip_address":"","ucode":"AC09EFE68BD0F5","user_header":"","comment_is_top":false,"comment_ctime":1564071124,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1564071124","product_id":100021701,"comment_content":"没有软间隔的代码吗？","like_count":0,"discussions":[{"author":{"id":1600014,"avatar":"","nickname":"dkzzw","note":"","ucode":"09B48DBB8603A1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":3785,"discussion_content":"软间隔其实是设置c参数。c越小，越允许犯错。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1564808848,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":112898,"user_name":"夜路破晓","can_delete":false,"product_type":"c1","uid":1353577,"ip_address":"","ucode":"9B875F94B759B9","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/3hZfficKPGCq2kjFBu9SgaMjibJTEl7iaW1ta6pZNyiaWP8XEsNpunlnsiaOtBpWTXfT5BvRP3qNByml6p9rtBvqewg/132","comment_is_top":false,"comment_ctime":1562838901,"is_pvip":false,"replies":[{"id":"63708","content":"data = pd.read_csv(&quot;.&#47;data.csv&quot;)<br>features_mean=list(data.columns[2:12]) <br>我刚才试了下，是可以的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577529871,"ip_address":"","comment_id":112898,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1562838901","product_id":100021701,"comment_content":"features_mean=list(data.columns[2:12]) <br>这行报错:<br>TypeError: &#39;DataFrame&#39; object is not callable<br>如何改善?","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":457949,"discussion_content":"data = pd.read_csv(&amp;quot;./data.csv&amp;quot;)\nfeatures_mean=list(data.columns[2:12]) \n我刚才试了下，是可以的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577529871,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":101415,"user_name":"姜泮昌","can_delete":false,"product_type":"c1","uid":1107213,"ip_address":"","ucode":"89B63270BAE099","user_header":"https://static001.geekbang.org/account/avatar/00/10/e5/0d/b4258141.jpg","comment_is_top":false,"comment_ctime":1559803664,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559803664","product_id":100021701,"comment_content":"准备训练集和测试集的代码是否有问题？<br>test_X= test[features_remain]<br>这里应该是features_remain吧？","like_count":0},{"had_liked":false,"id":95967,"user_name":"张晓辉","can_delete":false,"product_type":"c1","uid":1085046,"ip_address":"","ucode":"1CD9717DE399C5","user_header":"https://static001.geekbang.org/account/avatar/00/10/8e/76/6d55e26f.jpg","comment_is_top":false,"comment_ctime":1558277358,"is_pvip":false,"replies":[{"id":"64167","content":"对 线性的关系更准确","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577610881,"ip_address":"","comment_id":95967,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1558277358","product_id":100021701,"comment_content":"采用linearSVC, 预测准确率更高。<br>import pandas as pd <br>from sklearn.model_selection import train_test_split<br>from sklearn import svm<br>from sklearn.metrics import accuracy_score<br>from sklearn.preprocessing import StandardScaler<br><br>data = pd.read_csv(&#39;data.csv&#39;)<br>data.drop(&#39;id&#39;, axis=1, inplace=True)<br><br>feature_names = list(data.columns)<br>feature_names.remove(&#39;diagnosis&#39;)<br><br>traindata, testdata = train_test_split(data, test_size=0.3)<br>train_x = traindata[feature_names]<br>train_y = traindata[&#39;diagnosis&#39;]<br>test_x = testdata[feature_names]<br>test_y = testdata[&#39;diagnosis&#39;]<br><br>ss = StandardScaler()<br>train_x = ss.fit_transform(train_x)<br>test_x = ss.transform(test_x)<br>                    <br>model = svm.LinearSVC()<br>model.fit(train_x, train_y)<br>prediction = model.predict(test_x)<br>print(&quot;The accuracy is %f&quot; % accuracy_score(prediction, test_y))","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450662,"discussion_content":"对 线性的关系更准确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577610881,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":82333,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1554186631,"is_pvip":false,"replies":[{"id":"64318","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577616700,"ip_address":"","comment_id":82333,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1554186631","product_id":100021701,"comment_content":"import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn import svm<br>from sklearn import metrics<br><br># 加载数据集，你需要把数据放到目录中<br>data = pd.read_csv(&quot;.&#47;breast_cancer_data-master&#47;data.csv&quot;)<br><br># 数据探索<br># 因为数据集中列比较多，我们需要把 dataframe 中的列全部显示出来<br>pd.set_option(&#39;display.max_columns&#39;, None)<br>print(data.columns)<br>print(data.head(5))<br>print(data.describe())<br><br># 将特征字段分成 3 组<br>features_mean= list(data.columns[2:12])<br>features_se= list(data.columns[12:22])<br>features_worst=list(data.columns[22:32])<br><br># 数据清洗<br># ID 列没有用，删除该列<br>data.drop(&quot;id&quot;,axis=1,inplace=True)<br># 将 B 良性替换为 0，M 恶性替换为 1<br>data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})<br><br># 将肿瘤诊断结果可视化<br>sns.countplot(data[&#39;diagnosis&#39;],label=&quot;Count&quot;)<br>plt.show()<br># 用热力图呈现 features_mean 字段之间的相关性<br>corr = data[features_mean].corr()<br>plt.figure(figsize=(14,14))<br># annot=True 显示每个方格的数据<br>sns.heatmap(corr, annot=True)<br>plt.show()<br><br># 特征选择<br>#features_remain = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;] <br>features_remain = data.columns[1:31]<br>print(features_remain)<br>print(&#39;-&#39;*100)<br><br># 抽取 30% 的数据作为测试集，其余作为训练集<br>train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test<br># 抽取特征选择的数值作为训练和测试数据<br>train_X = train[features_remain]<br>train_y=train[&#39;diagnosis&#39;]<br>test_X= test[features_remain]<br>test_y =test[&#39;diagnosis&#39;]<br><br># 采用 Z-Score 规范化数据，保证每个特征维度的数据均值为 0，方差为 1<br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.transform(test_X)<br><br><br># 创建 SVM 分类器<br>model = svm.LinearSVC()<br># 用训练集做训练<br>model.fit(train_X,train_y)<br># 用测试集做预测<br>prediction=model.predict(test_X)<br>print(&#39;准确率: &#39;, metrics.accuracy_score(prediction,test_y))<br><br>------<br>准确率:  0.9649122807017544","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":445557,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577616700,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":77191,"user_name":"圆圆的大食客","can_delete":false,"product_type":"c1","uid":1361827,"ip_address":"","ucode":"B5B87E08869507","user_header":"https://static001.geekbang.org/account/avatar/00/14/c7/a3/1e2f9f5a.jpg","comment_is_top":false,"comment_ctime":1552880897,"is_pvip":false,"replies":[{"id":"64408","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577618442,"ip_address":"","comment_id":77191,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1552880897","product_id":100021701,"comment_content":"# -*- coding: utf-8 -*-<br>&quot;&quot;&quot;<br>Created on Sun Mar 17 23:18:31 2019<br><br>@author: xcma1<br>&quot;&quot;&quot;<br> <br>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn import metrics<br><br># 加载数据集，你需要把数据放到目录中<br>data = pd.read_csv(&quot;.&#47;data.csv&quot;)<br># 数据探索<br># 因为数据集中列比较多，我们需要把 dataframe 中的列全部显示出来<br>pd.set_option(&#39;display.max_columns&#39;, None)<br># 将特征字段分成 3 组<br>features_mean= list(data.columns[2:12])<br>features_se= list(data.columns[12:22])<br>features_worst=list(data.columns[22:32])<br># 数据清洗<br><br># ID 列没有用，删除该列<br>data.drop(&quot;id&quot;,axis=1,inplace=True)<br><br># 将 B 良性替换为 0，M 恶性替换为 1<br>data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})<br><br># 特征选择<br>features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,<br>       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,<br>       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,<br>       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,<br>       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,<br>       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,<br>       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,<br>       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,<br>       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;] <br><br># 抽取 30% 的数据作为测试集，其余作为训练集<br>train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test<br><br># 抽取特征选择的数值作为训练和测试数据<br>train_X = train[features_remain]<br>train_y=train[&#39;diagnosis&#39;]<br>test_X= test[features_remain]<br>test_y =test[&#39;diagnosis&#39;]<br><br># 采用 Z-Score 规范化数据，保证每个特征维度的数据均值为 0，方差为 1<br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.transform(test_X)<br><br># 创建 SVM 分类器<br>model = svm.SVC()<br># 用训练集做训练<br>model.fit(train_X,train_y)<br># 用测试集做预测<br>prediction=model.predict(test_X)<br>print(&#39;准确率: &#39;, metrics.accuracy_score(prediction,test_y))<br><br><br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":443628,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577618442,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":76807,"user_name":"叮当猫","can_delete":false,"product_type":"c1","uid":1360159,"ip_address":"","ucode":"175BB66517E21B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg","comment_is_top":false,"comment_ctime":1552715743,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1552715743","product_id":100021701,"comment_content":"通过多次运行，发现每次结果也不太一样，但是大体上，svc的效果好于linearsvc，多特征好于少特征。<br><br>附代码如下：<br>#-coding=utf-8<br>from sklearn import svm<br>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn import metrics<br><br>def train_func(func, features_remain, desc):<br>    #分类阶段：模型训练<br>    #抽取30%的数据作为测试集，其余作为训练集<br>    train, test = train_test_split(data, test_size=0.3)<br>    train_x = train[features_remain]<br>    train_y = train[&#39;diagnosis&#39;]<br>    test_x = test[features_remain]<br>    test_y = test[&#39;diagnosis&#39;]<br><br>    ss = StandardScaler()<br>    train_x = ss.fit_transform(train_x)<br>    test_x = ss.transform(test_x)<br><br>    #创建SVM分类器<br>    if(func == &quot;linear&quot;):<br>        model = svm.LinearSVC()<br>    else:<br>        model = svm.SVC()<br>    #用训练集做训练<br>    model.fit(train_x, train_y)<br>    #用测试集做预测<br><br>    #分类阶段：模型评估<br>    predict_y = model.predict(test_x)<br>    infos1 = &quot;测试集准确率：&quot; + str(metrics.accuracy_score(predict_y, test_y))<br><br>    predict_yy = model.predict(train_x)<br>    infos2 = &quot;训练集准确率：&quot; + str(metrics.accuracy_score(predict_yy, train_y))<br>    print desc + &quot;:&quot; + infos1 + &quot; &quot; + infos2<br><br><br>#准备阶段：数据探索<br>data = pd.read_csv(&#39;.&#47;breast_cancer_data-master&#47;data.csv&#39;)<br>#把所有的列都显示出来（在打印的时候）<br>pd.set_option(&#39;display.max_columns&#39;, None)<br><br>features_mean = list(data.columns[2:12])<br>features_se = list(data.columns[12:22])<br>features_worst = list(data.columns[22:32])<br><br>#准备阶段：数据清洗，id列没有用，删除该列<br>data.drop(&quot;id&quot;, axis=1, inplace=True)<br><br>#准备阶段：数据可视化<br>sns.countplot(data[&#39;diagnosis&#39;], label=&#39;Count&#39;)<br>plt.show()<br>corr = data[features_mean].corr()<br>plt.figure(figsize=(14,14))<br>sns.heatmap(corr, annot=True)<br>plt.show()<br><br>#分类阶段：特征选择<br>features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]<br><br>#分类阶段：模型训练+模型评估<br>train_func(&#39;svc&#39;, features_remain, &#39;svc_six&#39;)<br>train_func(&#39;svc&#39;, features_mean, &#39;svc_all&#39;)<br>train_func(&#39;linear&#39;, features_remain, &#39;linearsvc_six&#39;)<br>train_func(&#39;linear&#39;, features_mean, &#39;linearsvc_all&#39;)","like_count":0},{"had_liked":false,"id":71891,"user_name":"fancy","can_delete":false,"product_type":"c1","uid":1243166,"ip_address":"","ucode":"0C51F80B9C35B1","user_header":"https://static001.geekbang.org/account/avatar/00/12/f8/1e/0d5f8336.jpg","comment_is_top":false,"comment_ctime":1551469717,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1551469717","product_id":100021701,"comment_content":"使用LinearSVC和全部特征作为训练集时，分类器的准确率达到了99.4152%，在其他条件不变的情况下，其准确率高于SVC。","like_count":0},{"had_liked":false,"id":71350,"user_name":"ldw","can_delete":false,"product_type":"c1","uid":1339627,"ip_address":"","ucode":"51B0DE3F6FE278","user_header":"https://static001.geekbang.org/account/avatar/00/14/70/eb/e7a2dba9.jpg","comment_is_top":false,"comment_ctime":1551325442,"is_pvip":false,"replies":[{"id":"64491","content":"1-2天？ 根据自己的情况来判断，如果是早期入门的话，时间略长，如果有一定的基础，1天内肯定是OK的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577620049,"ip_address":"","comment_id":71350,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551325442","product_id":100021701,"comment_content":"陈老师，这堂课留的课后任务，包括可能使用的数据清洗，您会期望您团队的人用多长时间完成？超过多长时间以上，就是不合格的？谢谢🙏","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441118,"discussion_content":"1-2天？ 根据自己的情况来判断，如果是早期入门的话，时间略长，如果有一定的基础，1天内肯定是OK的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577620049,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70576,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551143057,"is_pvip":false,"replies":[{"id":"25190","content":"代表的含义是：radius_mean，perimeter_mean，area_mean这三个指标正相关，因此选择其中一个代表即可（我在正文中也写到了）<br>你说的标注第一列第3-4行也是对的，因为这几个指标都是正相关。完整的看第一行的第3-4列也可以标注上，实际上这三个指标可以重新组成一个小矩形。<br>我的标注（第一行第一列+第34行第34列，代表的是这三个指标相关）起到提示的作用，最主要的还是说明：radius_mean，perimeter_mean，area_mean这三个指标正相关。这个是最终的结果。<br>Anyway 你把第一列第3-4行标注出来，或者第一行第3-4列标注出来都是对的","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1551146767,"ip_address":"","comment_id":70576,"utype":2}],"discussion_count":1,"race_medal":0,"score":"1551143057","product_id":100021701,"comment_content":"勘误：热力学图中的第一个蓝色框框应该是标记在第1列第3-4行上，而不是第1列第1行。","like_count":0,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440704,"discussion_content":"代表的含义是：radius_mean，perimeter_mean，area_mean这三个指标正相关，因此选择其中一个代表即可（我在正文中也写到了）\n你说的标注第一列第3-4行也是对的，因为这几个指标都是正相关。完整的看第一行的第3-4列也可以标注上，实际上这三个指标可以重新组成一个小矩形。\n我的标注（第一行第一列+第34行第34列，代表的是这三个指标相关）起到提示的作用，最主要的还是说明：radius_mean，perimeter_mean，area_mean这三个指标正相关。这个是最终的结果。\nAnyway 你把第一列第3-4行标注出来，或者第一行第3-4列标注出来都是对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551146767,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":69057,"user_name":"TKbook","can_delete":false,"product_type":"c1","uid":1073829,"ip_address":"","ucode":"F6E0E99CC79059","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg","comment_is_top":false,"comment_ctime":1550653188,"is_pvip":false,"replies":[{"id":"64521","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621213,"ip_address":"","comment_id":69057,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1550653188","product_id":100021701,"comment_content":"# 特征选择<br>features_all = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,<br>       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,<br>       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,<br>       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,<br>       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,<br>       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,<br>       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,<br>       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,<br>       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;]<br>from sklearn.model_selection import train_test_split<br># 抽取30%的数据作为测试集，其余作为训练集<br>train, test = train_test_split(data, test_size=0.3)<br># 抽取特征选择的的数据作为训练和测试数据<br>train_X = train[features_all]<br>train_y = train[&#39;diagnosis&#39;]<br>test_X = test[features_all]<br>test_y = test[&#39;diagnosis&#39;]<br>from sklearn.preprocessing import StandardScaler<br># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1<br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.transform(test_X)<br>from sklearn import svm<br>from sklearn.metrics import accuracy_score<br># 创建SVM分类器<br>model = svm.LinearSVC()<br># 用训练集做训练<br>model.fit(train_X, train_y)<br># 用测试集做预测<br>prediction = model.predict(test_X)<br>print(&#39;准确率：&#39;, accuracy_score(prediction, test_y))<br>准确率： 0.9824561403508771","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439936,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621213,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67627,"user_name":"深白浅黑","can_delete":false,"product_type":"c1","uid":1123923,"ip_address":"","ucode":"DCCAA31DE8B127","user_header":"https://static001.geekbang.org/account/avatar/00/11/26/53/60fe31fb.jpg","comment_is_top":false,"comment_ctime":1550212642,"is_pvip":false,"replies":[{"id":"64559","content":"Good Sharing","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621751,"ip_address":"","comment_id":67627,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1550212642","product_id":100021701,"comment_content":"使用全部特征：（相同训练集和测试集）<br>LinearSVC准确率:  0.9298245614035088<br>SVC高斯核准确率: 0.9415204678362573<br>SVM首先是有监督的学习模型，需要数据有较好的分类属性。其次依据硬间隔、软间隔和核函数的应用，可以解决线性分类和非线性分类的问题。最后在使用过程中，需要对数据的特征进行有针对性的降维，利用数据的相关性，对相关性较大的类别属性选择其中一个作为特征，在特征选取后，要进行标准化处理。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439287,"discussion_content":"Good Sharing","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621751,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67617,"user_name":"JingZ","can_delete":false,"product_type":"c1","uid":1023464,"ip_address":"","ucode":"6F97895B2CC375","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132","comment_is_top":false,"comment_ctime":1550211538,"is_pvip":false,"replies":[{"id":"64560","content":"SVM是不错的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621760,"ip_address":"","comment_id":67617,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1550211538","product_id":100021701,"comment_content":"#svm 使用还是蛮方便的，完全特征，准确率达到97%以上<br><br>import pandas as pd<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn import svm<br>from sklearn import metrics<br><br>#加载数据<br>data = pd.read_csv(‘.&#47;data.csv&#39;)<br><br>#数据探索<br>pd.set_option(&#39;display.max_columns&#39;, None)<br>print(data.columns)<br>print(data.head(5))<br>print(data.describe())<br><br>#数据清洗<br>data.drop(&#39;id&#39;, axis=1, inplace=True)<br>data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1, &#39;B&#39;:0})<br><br>#特征选择<br>features_remain = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,<br>       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,<br>       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,<br>       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,<br>       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,<br>       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,<br>       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,<br>       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,<br>       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;]<br><br>#30% 测试集、训练集<br>train, test = train_test_split(data, test_size = 0.3)<br><br>#抽取特征选择的数值作为训练和测试数据<br>train_X = train[features_remain]<br>train_y = train[&#39;diagnosis&#39;]<br>test_X = test[features_remain]<br>test_y = test[&#39;diagnosis&#39;]<br><br>#采用 Z-score 规范化数据<br>ss = StandardScaler()<br>train_X = ss.fit_transform(train_X)<br>test_X = ss.transform(test_X)<br><br>#创建 LinearSVC 分类器<br>model = svm.LinearSVC()<br><br>#用训练集做训练<br>model.fit(train_X, train_y)<br><br>#用测试集做预测<br>prediction = model.predict(test_X)<br>print(&#39;准确率：&#39;, metrics.accuracy_score(prediction, test_y))","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439281,"discussion_content":"SVM是不错的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}