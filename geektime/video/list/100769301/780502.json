{"id":780502,"title":"09｜数据探索：用和大模型交互的方式做探索性数据分析","content":"<p><span class=\"orange\">点击“展开”查看“精华文字稿”</span></p><h2>探索数据的方法有哪些</h2><p>这节课我们来学习怎么探索数据。探索数据的方法可以帮助我们在正式深入分析前了解数据的基本情况。这些方法不仅为我们提供数据的直观理解，还能指导我们更有效地处理数据分析的后续步骤。</p><p>常见的数据探索方法有哪些呢？咱们掌握下面这5个就可以了。</p><ol>\n<li>描述性统计：计算关键的统计指标，如平均值、中位数、标准差等，这些指标揭示了数据的中心趋势和离散程度。</li>\n<li>数据可视化：通过图表如直方图、箱线图、散点图等来展示数据的分布、集中趋势和异常值。这种视觉表示有助于直观地识别数据的模式和潜在问题。</li>\n<li>相关性分析：评估不同变量之间的关系，使用如皮尔逊相关系数来衡量变量间的线性关系强度。</li>\n<li>分组与聚合：对数据进行分类汇总，分析不同类别或组的数据行为，如通过分组平均值来比较不同类别的表现。</li>\n<li>初步建模：建立简单的统计或机器学习模型，初步探索数据中的依赖关系，为后续复杂模型的开发提供基线。</li>\n</ol><p>那么探索数据的工具，就是咱们学过的ChatGPT、Python。另外，这节课里我们既要复习前面学过的数据清洗技能，还得能将他们结合在一起使用。</p><p>掌握数据探索的各种方法，其实是为进入更系统的探索性数据分析（EDA）铺平道路。 EDA是一个更全面的数据分析过程，不仅会用到这些数据探索方法，还会加入更多的统计测试和复杂的可视化方法，目的都是未来深入挖掘数据中的信息，获得我们需要的数据洞察和结论，最终能帮助我们解决真实问题。</p>","comments":[{"had_liked":false,"id":391915,"user_name":"balance","can_delete":false,"product_type":"c3","uid":1007182,"ip_address":"山西","ucode":"324D909BBE69DE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/4e/85502e98.jpg","comment_is_top":false,"comment_ctime":1719386703,"is_pvip":false,"replies":[{"id":142785,"content":"GPT-4o 执行 Python 脚本的工作原理\n环境支持：\nGPT-4o 并不是直接执行代码的，而是依赖于后端的环境支持。OpenAI 为 GPT-4o 配备了一些计算和编程环境，使得它能够通过调用外部插件或工具来执行 Python 脚本。\n\n插件与工具集成：\nGPT-4o 通过集成特定的插件或工具（如 Python 解释器）来支持代码执行。这些插件允许模型在特定的沙箱环境中运行代码，获取计算结果，并将结果返回给用户。\n\n图表生成：\n在 Python 中生成图表通常使用库如 Matplotlib 或 Plotly。GPT-4o 可以通过生成相应的 Python 脚本来创建图表，并在执行后将图表作为结果的一部分返回。","user_name":"作者回复","user_name_real":"编辑","uid":1056235,"ctime":1723008508,"ip_address":"广东","comment_id":391915,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100769301,"comment_content":"大语言模型本身不具备方法调用的能力，chatGPT 4o是怎么做到执行python脚本，并输出图表结果的呢？难道老师使用的这个其实是个智能体？","like_count":1,"discussions":[{"author":{"id":1056235,"avatar":"https://static001.geekbang.org/account/avatar/00/10/1d/eb/b2123759.jpg","nickname":"尹会生","note":"","ucode":"D1093DBD093617","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649217,"discussion_content":"GPT-4o 执行 Python 脚本的工作原理\n环境支持：\nGPT-4o 并不是直接执行代码的，而是依赖于后端的环境支持。OpenAI 为 GPT-4o 配备了一些计算和编程环境，使得它能够通过调用外部插件或工具来执行 Python 脚本。\n\n插件与工具集成：\nGPT-4o 通过集成特定的插件或工具（如 Python 解释器）来支持代码执行。这些插件允许模型在特定的沙箱环境中运行代码，获取计算结果，并将结果返回给用户。\n\n图表生成：\n在 Python 中生成图表通常使用库如 Matplotlib 或 Plotly。GPT-4o 可以通过生成相应的 Python 脚本来创建图表，并在执行后将图表作为结果的一部分返回。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723008508,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391914,"user_name":"balance","can_delete":false,"product_type":"c3","uid":1007182,"ip_address":"山西","ucode":"324D909BBE69DE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/4e/85502e98.jpg","comment_is_top":false,"comment_ctime":1719386463,"is_pvip":false,"replies":[{"id":142784,"content":"如果数据量很大，可能会超出单次对话的token限制，如何处理这种情况？\n数据采样:\n\n抽样法: 从数据集中抽取有代表性的小样本进行分析，比如使用随机采样、分层抽样等方法。这样可以大大减少需要处理的数据量，同时保留数据的代表性。\n\n分批处理: 将数据分成多个批次上传到ChatGPT，每批只处理一部分数据。虽然增加了处理的次数，但可以避免超出限制。 也可以利用程序实现自动分批处理，参考文档实现文本分割（文档转换器）“https:&#47;&#47;python.langchain.com.cn&#47;docs&#47;modules&#47;data_connection&#47;document_transformers&#47;”\n\n另一种方法是 聚合和总结:\n\n数据聚合: 在数据库中先进行数据聚合和汇总，生成较小的数据集。这可以是通过SQL中的GROUP BY等聚合函数来实现。\n降维与压缩: 使用数据降维技术（如PCA）或数据压缩方法，将数据规模缩小，同时保持数据特性。","user_name":"作者回复","user_name_real":"编辑","uid":1056235,"ctime":1723008344,"ip_address":"广东","comment_id":391914,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100769301,"comment_content":"在实际的工作中，数据量都不是几条，可能是上千、万、亿条数据，不能像课程里面演示的直接对话框贴数据，应该怎么处理呢？","like_count":0,"discussions":[{"author":{"id":1056235,"avatar":"https://static001.geekbang.org/account/avatar/00/10/1d/eb/b2123759.jpg","nickname":"尹会生","note":"","ucode":"D1093DBD093617","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649216,"discussion_content":"如果数据量很大，可能会超出单次对话的token限制，如何处理这种情况？\n数据采样:\n\n抽样法: 从数据集中抽取有代表性的小样本进行分析，比如使用随机采样、分层抽样等方法。这样可以大大减少需要处理的数据量，同时保留数据的代表性。\n\n分批处理: 将数据分成多个批次上传到ChatGPT，每批只处理一部分数据。虽然增加了处理的次数，但可以避免超出限制。 也可以利用程序实现自动分批处理，参考文档实现文本分割（文档转换器）“https://python.langchain.com.cn/docs/modules/data_connection/document_transformers/”\n\n另一种方法是 聚合和总结:\n\n数据聚合: 在数据库中先进行数据聚合和汇总，生成较小的数据集。这可以是通过SQL中的GROUP BY等聚合函数来实现。\n降维与压缩: 使用数据降维技术（如PCA）或数据压缩方法，将数据规模缩小，同时保持数据特性。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723008344,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]}]}