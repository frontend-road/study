{"id":209425,"title":"18 | 神经网络基础：训练神经网络","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://github.com/geektime-geekbang/NLP\">https://github.com/geektime-geekbang/NLP</a></p>","comments":[{"had_liked":false,"id":222846,"user_name":"皮特尔","can_delete":false,"product_type":"c3","uid":1017161,"ip_address":"","ucode":"313862C91DD325","user_header":"https://static001.geekbang.org/account/avatar/00/0f/85/49/585c69c4.jpg","comment_is_top":false,"comment_ctime":1590929215,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"梯度消失和梯度爆炸的讲解很形象","like_count":7},{"had_liked":false,"id":186853,"user_name":"人工智能混饭人","can_delete":false,"product_type":"c3","uid":1104354,"ip_address":"","ucode":"71CF953311BD81","user_header":"https://static001.geekbang.org/account/avatar/00/10/d9/e2/d75009ef.jpg","comment_is_top":false,"comment_ctime":1583938892,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"梯度爆炸一般是clip；而梯度消失一般是换激活函数或者使用LSTM,GRU，感觉BN也会有一点效果","like_count":3},{"had_liked":false,"id":313238,"user_name":"sky","can_delete":false,"product_type":"c3","uid":2325199,"ip_address":"","ucode":"C7BA135845E1FF","user_header":"https://static001.geekbang.org/account/avatar/00/23/7a/cf/c42dd74e.jpg","comment_is_top":false,"comment_ctime":1632321678,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"从训练反向传播看，梯度更新时候，会存在大量的梯度连乘情况。梯度消失是由于多个导数&lt;1不断相乘，最终梯度更新会接近于0，也就是梯度消失；同理，参数和梯度的共同作用也可能产生&gt;1的不断相乘，结果就是梯度爆炸","like_count":2},{"had_liked":false,"id":315097,"user_name":"PrimeYue","can_delete":false,"product_type":"c3","uid":2089823,"ip_address":"","ucode":"13DE7DB16D5C53","user_header":"https://static001.geekbang.org/account/avatar/00/1f/e3/5f/2c8a4980.jpg","comment_is_top":false,"comment_ctime":1633687041,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"梯度爆炸：\n更好的初始化策略；更好的优化方法，更好的激活函数，更好的Normalization，Skip Connect","like_count":1},{"had_liked":false,"id":301757,"user_name":"王黎杰","can_delete":false,"product_type":"c3","uid":1049245,"ip_address":"","ucode":"CA5FB3921FC961","user_header":"https://static001.geekbang.org/account/avatar/00/10/02/9d/f7bd9922.jpg","comment_is_top":false,"comment_ctime":1625840528,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"一阶导数控制步长，二阶控制学习率的大小","like_count":1},{"had_liked":false,"id":192589,"user_name":"JaneIDK","can_delete":false,"product_type":"c3","uid":1883313,"ip_address":"","ucode":"8F6E293767C62B","user_header":"https://static001.geekbang.org/account/avatar/00/1c/bc/b1/3ba14f09.jpg","comment_is_top":false,"comment_ctime":1584862285,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"笔记：\n现实生活中我们比较难以发现梯度消失或梯度爆炸，或发现了后做的也比较有限。\n一般来讲发生梯度爆炸我们可以做的是：把梯度设置一个上限；\n发生梯度消失我们要做的是：（除了改网络架构之外），试试不同的初始值， 或者改变步长。","like_count":1},{"had_liked":false,"id":186291,"user_name":"写点啥呢","can_delete":false,"product_type":"c3","uid":1065272,"ip_address":"","ucode":"C19032CF1C41BA","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/38/4f89095b.jpg","comment_is_top":false,"comment_ctime":1583820585,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"请教老师，优化方法是为了找到极值，理想情况随着随机探索极值位置，梯度渐近为零，请问“梯度消失”和这种预期接近极值时候梯度渐近变小的区别是什么？同样对“梯度爆炸”有类似的疑问，如果出现梯度爆炸，是不是意味着搜索极值的方向出现了问题，这时候是不是应该更改初始值和步长参数重新一轮计算？谢谢。","like_count":1,"discussions":[{"author":{"id":1098079,"avatar":"https://static001.geekbang.org/account/avatar/00/10/c1/5f/7631a4b5.jpg","nickname":"凌空","note":"","ucode":"B50835B9A80E68","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":202138,"discussion_content":"拿调多房调麦克风这个例子来说，所谓的极值是最末的输出是你耳朵最舒服的音量，梯度消失是最末没有输出了，梯度爆炸是最末啸叫了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1583879674,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2325199,"avatar":"https://static001.geekbang.org/account/avatar/00/23/7a/cf/c42dd74e.jpg","nickname":"sky","note":"","ucode":"C7BA135845E1FF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":395661,"discussion_content":"好问题。我试着说下自己的看法\n1，‘请问“梯度消失”和这种预期接近极值时候梯度渐近变小的区别是什么’，我理解梯度消失和爆炸是一种训练的病态情况，发生梯度消失，也就意味着训练可能会失败，反映在损失函数上就是，损失函数值居高不下，无论epoch如何增加。而正常的梯度逐渐变小，假设一切正常，则一切会让人满意\n2.梯度爆炸，跟搜索极值方向应该是无关的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632322061,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}