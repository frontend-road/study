{"id":355028,"title":"05 | 调度系统：“数据不动代码动”到底是什么意思？","content":"<p>你好，我是吴磊。</p><p>在日常的开发与调优工作中，为了充分利用硬件资源，我们往往需要手工调节任务并行度来提升CPU利用率，控制任务并行度的参数是Spark的配置项：spark.default.parallelism。增加并行度确实能够充分利用闲置的CPU线程，但是，parallelism数值也不宜过大，过大反而会引入过多的调度开销，得不偿失。</p><p>这个调优技巧可以说是老生常谈了，网上到处都可以搜得到。那你知道为什么parallelism数值过大调度开销会呈指数级增长吗？调度开销具体又是指什么呢？以及，如果不想一个数值一个数值的尝试，parallelism数值究竟该怎么设置，才能以最少的时间获得最好的效果？如果你还没有答案，或者说还没有把握答对，接下来你就要好好听我讲。</p><p>这一讲，我会通过一个机器学习案例，来和你一起聊聊调度系统是什么，它是怎么工作的，从而帮助你摆脱调优总是停留在知其然、不知其所以然的尴尬境地。</p><h2>案例：对用户兴趣特征做Label Encoding</h2><p>在机器学习应用中，特征工程几乎占据了算法同学80%的时间和精力，毕竟，一份质量优良的训练样本限定了模型效果的上限和天花板，我们要讲的案例就来自特征工程中一个典型的处理场景：Label Encoding（标签编码）。</p><!-- [[[read_end]]] --><p>什么是Label encoding呢？模型特征按照是否连续可以分为两类：连续性数值特征和离散型特征，离散型特征往往以字符串的形式存在，比如用户兴趣特征就包括体育、政治、军事和娱乐等。对于很多机器学习算法来说，字符串类型的数据是不能直接消费的，需要转换为数值才行，例如把体育、政治、军事、娱乐映射为0、1、2、3，这个过程在机器学习领域有个术语就叫Label encoding。</p><p>我们这一讲的案例，就是要对用户兴趣特征做Label encoding，简单来说就是以固定的模板把字符串转换为数值，然后将千亿条样本中的用户兴趣转换为对应的索引值。固定模板是离线模型训练与线上模型服务之间的文件接口，内容仅包含用户兴趣这一列，字符串已按事先约定好的规则进行排序。我们需要注意的是，用户兴趣包含4个层级，因此这个模板文件较大，记录数达到万级别。</p><pre><code>//模板文件\n//用户兴趣\n体育-篮球-NBA-湖人\n军事-武器-步枪-AK47\n</code></pre><p>那具体怎么转换呢？例如，我们可以将用户兴趣“体育-篮球-NBA-湖人”映射为0，将兴趣“军事-武器-步枪-AK47”映射为1，以此类推。应该说，需求还是相当明确的，我身边的同学们拿到需求之后，奔儿都没打，以迅雷不及掩耳之势就实现了如下的处理函数。</p><pre><code>/**\n实现方式1\n输入参数：模板文件路径，用户兴趣字符串\n返回值：用户兴趣字符串对应的索引值\n*/\n \n//函数定义\ndef findIndex(templatePath: String, interest: String): Int = {\nval source = Source.fromFile(filePath, &quot;UTF-8&quot;)\nval lines = source.getLines().toArray\nsource.close()\nval searchMap = lines.zip(0 until lines.size).toMap\nsearchMap.getOrElse(interest, -1)\n}\n \n//Dataset中的函数调用\nfindIndex(filePath, &quot;体育-篮球-NBA-湖人&quot;)\n</code></pre><p>我们可以看到这个函数有两个形参，一个是模板文件路径，另一个是训练样本中的用户兴趣。处理函数首先读取模板文件，然后根据文件中排序的字符串构建一个从兴趣到索引的Map映射，最后在这个Map中查找第二个形参传入的用户兴趣，如果能找到则返回对应的索引，找不到的话则返回-1。</p><p>这段代码看上去似乎没什么问题，同学们基于上面的函数对千亿样本做Label encoding，在20台机型为C5.4xlarge AWS EC2的分布式集群中花费了5个小时。坦白说，这样的执行性能，我是不能接受的。你可能会说：“需求就是这个样子，还能有什么别的办法呢？”我们不妨来看另外一种实现方式。</p><pre><code>/**\n实现方式2\n输入参数：模板文件路径，用户兴趣字符串\n返回值：用户兴趣字符串对应的索引值\n*/\n \n//函数定义\nval findIndex: (String) =&gt; (String) =&gt; Int = {\n(filePath) =&gt;\nval source = Source.fromFile(filePath, &quot;UTF-8&quot;)\nval lines = source.getLines().toArray\nsource.close()\nval searchMap = lines.zip(0 until lines.size).toMap\n(interest) =&gt; searchMap.getOrElse(interest, -1)\n}\nval partFunc = findIndex(filePath)\n \n//Dataset中的函数调用\npartFunc(&quot;体育-篮球-NBA-湖人&quot;)\n</code></pre><p>同学们基于第二种方式对相同的数据集做Label encoding之后，在10台同样机型的分布式集群中花了不到20分钟就把任务跑完了。可以说，执行性能的提升是显而易见的。那么，两份代码有什么区别呢？</p><p>我们可以看到，相比于第一份代码，第二份代码的函数体内没有任何变化，还是先读取模板文件、构建Map映射、查找用户兴趣，最后返回索引。最大的区别就是第二份代码对高阶函数的使用，具体来说有2点：</p><ol>\n<li>处理函数定义为高阶函数，形参是模板文件路径，返回结果是从用户兴趣到索引的函数；</li>\n<li>封装千亿样本的Dataset所调用的函数，不是第一份代码中的findIndex，而是用模板文件调用findIndex得到的partFunc，partFunc是形参为兴趣、结果为索引的普通标量函数。</li>\n</ol><p>那么，高阶函数真有这么神奇吗？其实，性能的提升并不是高阶函数的功劳，而是调度系统在起作用。</p><h2>Spark的调度系统是如何工作的？</h2><p>Spark调度系统的核心职责是，<strong>先将用户构建的DAG转化为分布式任务，结合分布式集群资源的可用性，基于调度规则依序把分布式任务分发到执行器</strong>。这个过程听上去就够复杂的了，为了方便你理解，我们还是先来讲一个小故事。</p><h3>土豆工坊流水线升级</h3><p>在学完了内存计算的第二层含义之后，土豆工坊的老板决定对土豆加工流水线做升级，来提高工坊的生产效率和灵活性。</p><p>这里，我们先对内存计算的第二层含义做个简单地回顾，它指的是<strong>同一Stage中的所有操作会被捏合为一个函数，这个函数一次性会被地应用到输入数据上，并且一次性地产生计算结果</strong>。</p><p>升级之前的土豆加工流程DAG被切分为3个执行阶段Stage，它们分别是Stage 0、Stage 1、Stage 2。其中，Stage 0产出即食薯片，Stage 1分发调味品，Stage 2则产出不同尺寸、不同风味的薯片。我们重点关注Stage 0，Stage 0有3个加工环节，分别是清洗、切片和烘焙。这3个环节需要3种不同的设备，即清洗机、切片机和烤箱。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/5f/3fcb3e400db91198a7499c016ccfb45f.jpg?wh=4674*1786\" alt=\"\" title=\"土豆工坊加工流程的3个执行阶段\"></p><p>工坊有3条流水线，每种设备都需要3套，在成本方面要花不少钱呢，因此工坊老板一直绞尽脑汁想把设备方面的成本降下来。</p><p>此时，工头儿建议：“老板，我听说市场上有一种可编程的土豆加工设备，它是个黑盒子并且只有输入口和输出口，从外面看不见里面的操作流程。不过黑盒子受程序控制，给定输入口的食材，我们可以编写程序控制黑盒子的输出。有了这个可编程设备，咱们不但省了钱，将来还可以灵活地扩充产品线。比方想生产各种风味的薯条或是土豆泥，只需要更换一份程序加载到黑盒子里就行啦！”</p><p>老板听后大喜，决定花钱购入可编程土豆加工设备，替换并淘汰现有的清洗机、切片机和烤箱。</p><p>于是，工坊的加工流水线就变成了如下的样子。工人们的工作也从按照DAG流程图的关键步骤，在流水线上安装相应的设备，变成了把关键步骤编写相应的程序加载到黑盒内。这样一来，这家工坊的生产力也从作坊式的生产方式，升级到了现代化流水线的作业模式。</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/46/dc4f5f39a166ca93080c5a7c0ea0d446.jpg?wh=10527*4467\" alt=\"\" title=\"演进的土豆加工流水线\"></p><p>那么，这个故事跟我们今天要讲的调度系统有什么关系呢？事实上，Spark调度系统的工作流程包含如下5个步骤：</p><p><strong>1. 将DAG拆分为不同的运行阶段Stages；</strong><br>\n<strong>2. 创建分布式任务Tasks和任务组TaskSet；</strong><br>\n<strong>3. 获取集群内可用<strong><strong>的</strong></strong>硬件资源情况；</strong><br>\n<strong>4. 按照调度规则决定优先调度哪些任务/组；</strong><br>\n<strong>5. 依序将分布式任务分发到执行器Executor。</strong></p><p>除了第4步以外，其他几步和土豆工坊流水线上的关键步骤都是一一对应的，它们的对应关系如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/63/83/635462108ee2fc09991708f0856bcb83.jpg?wh=4299*1293\" alt=\"\"></p><p>现在，你可能会觉得用故事来记这几个步骤好像多此一举，但当我们学完了所有的原理之后，再回过头来把故事的主线串联起来，你就会惊喜地发现，所有的原理你都能轻松地记住和理解，这可比死记硬背的效率要高得多。</p><h3>调度系统中的核心组件有哪些？</h3><p>接下来，我们深入到流程中的每一步去探究Spark调度系统是如何工作的。不过在此之前，我们得先弄清楚调度系统都包含哪些关键组件，不同组件之间如何交互，它们分别担任了什么角色，才能更好地理解流程中的每一步。</p><p>Spark调度系统包含3个核心组件，分别是DAGScheduler、TaskScheduler和SchedulerBackend。这3个组件都运行在Driver进程中，它们通力合作将用户构建的DAG转化为分布式任务，再把这些任务分发给集群中的Executors去执行。不过，它们的名字都包含Scheduler，光看名字还真是丈二和尚摸不着头脑，所以我把它们和调度系统流程中5个步骤的对应关系总结在了下表中，你可以看一看。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/52/46bb66fed5d52b09407d66881cf0df52.jpeg?wh=1661*908\" alt=\"\"></p><h4>1. DAGScheduler</h4><p>DAGScheduler的主要职责有二：一是把用户DAG拆分为Stages，如果你不记得这个过程可以回顾一下<a href=\"https://time.geekbang.org/column/article/353808\">上一讲</a>的内容；二是在Stage内创建计算任务Tasks，这些任务囊括了用户通过组合不同算子实现的数据转换逻辑。然后，执行器Executors接收到Tasks，会将其中封装的计算函数应用于分布式数据分片，去执行分布式的计算过程。</p><p>不过，如果我们给集群中处于繁忙或者是饱和状态的Executors分发了任务，执行效果会大打折扣。因此，<strong>在分发任务之前，调度系统得先判断哪些节点的计算资源空闲，然后再把任务分发过去</strong>。那么，调度系统是怎么判断节点是否空闲的呢？</p><h4>2. SchedulerBackend</h4><p>SchedulerBackend就是用来干这个事的，它是对于资源调度器的封装与抽象，为了支持多样的资源调度模式如Standalone、YARN和Mesos，SchedulerBackend提供了对应的实现类。在运行时，Spark根据用户提供的MasterURL，来决定实例化哪种实现类的对象。MasterURL就是你通过各种方式指定的资源管理器，如--master spark://ip:host（Standalone 模式）、--master yarn（YARN 模式）。</p><p>对于集群中可用的计算资源，SchedulerBackend会用一个叫做ExecutorDataMap的数据结构，来记录每一个计算节点中Executors的资源状态。ExecutorDataMap是一种HashMap，它的Key是标记Executor的字符串，Value是一种叫做ExecutorData的数据结构，ExecutorData用于封装Executor的资源状态，如RPC地址、主机地址、可用CPU核数和满配CPU核数等等，它相当于是对Executor做的“资源画像”。</p><p><img src=\"https://static001.geekbang.org/resource/image/a7/a9/a7f8d49bbf1f8b0a125ffca87f079aa9.jpg?wh=1920*943\" alt=\"\" title=\"ExecutorDataMap映射表\"></p><p>总的来说，对内，SchedulerBackend用ExecutorData对Executor进行资源画像；对外，SchedulerBackend以WorkerOffer为粒度提供计算资源，WorkerOffer封装了Executor ID、主机地址和CPU核数，用来表示一份可用于调度任务的空闲资源。显然，基于Executor资源画像，SchedulerBackend可以同时提供多个WorkerOffer用于分布式任务调度。WorkerOffer这个名字起得蛮有意思，Offer的字面意思是公司给你提供的工作机会，结合Spark调度系统的上下文，就变成了使用硬件资源的机会。</p><p>好了，到此为止，要调度的计算任务有了，就是DAGScheduler通过Stages创建的Tasks；可用于调度任务的计算资源也有了，即SchedulerBackend提供的一个又一个WorkerOffer。如果从供需的角度看待任务调度，DAGScheduler就是需求端，SchedulerBackend就是供给端。</p><h4>3. TaskScheduler</h4><p>左边有需求，右边有供给，如果把Spark调度系统看作是一个交易市场的话，那么中间还需要有个中介来帮它们对接意愿、撮合交易，从而最大限度地提升资源配置的效率。在Spark调度系统中，这个中介就是TaskScheduler。<strong>TaskScheduler的职责是，基于既定的规则与策略达成供需双方的匹配与撮合</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/82/yy/82e86e1b3af101100015bcfd81f0f7yy.jpg?wh=3033*1072\" alt=\"\" title=\"Spark分布式任务调度流程\"></p><p>显然，TaskScheduler的核心是任务调度的规则和策略，<strong>TaskScheduler的调度策略分为两个层次，一个是不同Stages之间的调度优先级，一个是Stages内不同任务之间的调度优先级</strong>。</p><p>首先，对于两个或多个Stages，如果它们彼此之间不存在依赖关系、互相独立，在面对同一份可用计算资源的时候，它们之间就会存在竞争关系。这个时候，先调度谁、或者说谁优先享受这份计算资源，大家就得基于既定的规则和协议照章办事了。</p><p><strong>对于这种Stages之间的任务调度，TaskScheduler提供了2种调度模式，分别是FIFO（先到先得）和FAIR（公平调度）。</strong> FIFO非常好理解，在这种模式下，Stages按照被创建的时间顺序来依次消费可用计算资源。这就好比在二手房交易市场中，两个人同时看中一套房子，不管两个人各自愿意出多少钱，谁最先交定金，中介就优先给谁和卖家撮合交易。</p><p>你可能会说：“这不合常理啊！如果第二个人愿意出更多的钱，卖家自然更乐意和他成交。”没错，考虑到开发者的意愿度，TaskScheduler提供了FAIR公平调度模式。在这种模式下，哪个Stages优先被调度，取决于用户在配置文件fairscheduler.xml中的定义。</p><p>在配置文件中，Spark允许用户定义不同的调度池，每个调度池可以指定不同的调度优先级，用户在开发过程中可以关联不同作业与调度池的对应关系，这样不同Stages的调度就直接和开发者的意愿挂钩，也就能享受不同的优先级待遇。对应到二手房交易的例子中，如果第二个人乐意付30%的高溢价，中介自然乐意优先撮合他与卖家的交易。</p><p>说完了不同Stages之间的调度优先级，我们再来说说同一个Stages内部不同任务之间的调度优先级，Stages内部的任务调度相对来说简单得多。<strong>当TaskScheduler接收到来自SchedulerBackend的WorkerOffer后，TaskScheduler会优先挑选那些满足本地性级别要求的任务进行分发</strong>。众所周知，本地性级别有4种：Process local &lt; Node local &lt; Rack local &lt; Any。从左到右分别是进程本地性、节点本地性、机架本地性和跨机架本地性。从左到右，计算任务访问所需数据的效率越来越差。</p><p>进程本地性表示计算任务所需的输入数据就在某一个Executor进程内，因此把这样的计算任务调度到目标进程内最划算。同理，如果数据源还未加载到Executor进程，而是存储在某一计算节点的磁盘中，那么把任务调度到目标节点上去，也是一个不错的选择。再次，如果我们无法确定输入源在哪台机器，但可以肯定它一定在某个机架上，本地性级别就会退化到Rack local。</p><p>DAGScheduler划分Stages、创建分布式任务的过程中，会为每一个任务指定本地性级别，本地性级别中会记录该任务有意向的计算节点地址，甚至是Executor进程ID。换句话说，<strong>任务自带调度意愿，它通过本地性级别告诉TaskScheduler自己更乐意被调度到哪里去</strong>。</p><p>既然计算任务的个人意愿这么强烈，TaskScheduler作为中间商，肯定要优先满足人家的意愿。这就像一名码农想要租西二旗的房子，但是房产中介App推送的结果都是东三环国贸的房子，那么这个中介的匹配算法肯定有问题。</p><p>由此可见，<strong>Spark调度系统的原则是尽可能地让数据呆在原地、保持不动，同时尽可能地把承载计算任务的代码分发到离数据最近的地方，从而最大限度地降低分布式系统中的网络开销</strong>。毕竟，分发代码的开销要比分发数据的代价低太多，这也正是“数据不动代码动”这个说法的由来。</p><p>总的来说，TaskScheduler根据本地性级别遴选出待计算任务之后，先对这些任务进行序列化。然后，交给SchedulerBackend，SchedulerBackend根据ExecutorData中记录的RPC地址和主机地址，再将序列化的任务通过网络分发到目的主机的Executor中去。最后，Executor接收到任务之后，把任务交由内置的线程池，线程池中的多线程则并发地在不同数据分片之上执行任务中封装的数据处理函数，从而实现分布式计算。</p><h2>性能调优案例回顾</h2><p>知道了调度系统是如何工作的，我们就可以回过头来说说开头Label encoding的开发案例中，2种实现方式的差别到底在哪儿了。我们先来回顾案例中处理函数的主要计算步骤：</p><ol>\n<li>读取并遍历模板文件内容，建立从字符串到数值的字典；</li>\n<li>根据样本中的用户兴趣，查找字典并返回兴趣字符串对应的数值索引。</li>\n</ol><p><strong>2种实现方式的本质区别在于，函数中2个计算步骤的分布式计算过程不同。在第1种实现方式中，函数是一个接收两个形参的普通标量函数，Dataset调用这个函数在千亿级样本上做Label encoding。</strong></p><p>在Spark任务调度流程中，该函数在Driver端交由DAGScheduler打包为Tasks，经过TaskScheduler调度给SchedulerBackend，最后由SchedulerBackend分发到集群中的Executors中去执行。这意味着集群中的每一个Executors都需要执行函数中封装的两个计算步骤，要知道，第一个步骤中遍历文件内容并建立字典的计算开销还是相当大的。</p><p>反观第2种实现方式，2个计算步骤被封装到一个高阶函数中。用户代码先在Driver端用模板文件调用这个高阶函数，完成第一步计算建立字典的过程，同时输出一个只带一个形参的标量函数，这个标量函数内携带了刚刚建好的映射字典。最后，Dataset将这个标量函数作用于千亿样本之上做Label encoding。</p><p>发现区别了吗？在第2种实现中，函数的第一步计算只在Driver端计算一次，分发给集群中所有Executors的任务中封装的是携带了字典的标量函数。然后在Executors端，Executors在各自的数据分片上调用该函数，省去了扫描模板文件、建立字典的开销。最后，我们只需要把样本中的用户兴趣传递进去，函数就能以O(1)的查询效率返回数值结果。</p><p>对于一个有着成百上千Executors的分布式集群来说，这2种不同的实现方式带来的性能差异还是相当可观的。因此，如果你能把Spark调度系统的工作原理牢记于心，我相信在代码开发或是review的过程中，你都能够意识到第一个计算步骤会带来的性能问题。<strong>这种开发过程中的反思，其实就是在潜移默化地建立以性能为导向的开发习惯</strong>。</p><h2>小结</h2><p>今天这一讲，我们先通过一个机器学的案例对比了2种实现方式的性能差异，知道了对于调度系统一知半解，很有可能在开发过程中引入潜在的性能隐患。为此，我梳理了调度系统工作流程的5个主要步骤：</p><ol>\n<li>将DAG拆分为不同的运行阶段Stages；</li>\n<li>创建分布式任务Tasks和任务组TaskSet；</li>\n<li>获取集群内可用硬件资源情况；</li>\n<li>按照调度规则决定优先调度哪些任务/组；</li>\n<li>依序将分布式任务分发到执行器Executor；</li>\n</ol><p>结合这5个步骤，我们深入分析了Spark调度系统的工作原理，我们可以从核心职责和核心原则这两方面来归纳：</p><ol>\n<li>Spark调度系统的核心职责是，先将用户构建的DAG转化为分布式任务，结合分布式集群资源的可用性，基于调度规则依序把分布式任务分发到执行器Executors；</li>\n<li>Spark调度系统的核心原则是，尽可能地让数据呆在原地、保持不动，同时尽可能地把承载计算任务的代码分发到离数据最近的地方（Executors或计算节点），从而最大限度地降低分布式系统中的网络开销。</li>\n</ol><h2>每日一练</h2><ol>\n<li>DAGScheduler在创建Tasks的过程中，是如何设置每一个任务的本地性级别？</li>\n<li>在计算与存储分离的云计算环境中，Node local本地性级别成立吗？你认为哪些情况下成立？哪些情况下不成立？</li>\n</ol><p>期待在留言区看到你的思考和答案，如果你的朋友也正急需搞清楚调度系统的工作原理，也欢迎你把这一讲转发给他，我们下一讲见！</p>","neighbors":{"left":{"article_title":"04 | DAG与流水线：到底啥叫“内存计算”？","id":353808},"right":{"article_title":"06 | 存储系统：空间换时间，还是时间换空间？","id":355081}},"comments":[{"had_liked":false,"id":298360,"user_name":"慢慢卢","can_delete":false,"product_type":"c1","uid":1329566,"ip_address":"","ucode":"853D399100D83B","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","comment_is_top":false,"comment_ctime":1624056247,"is_pvip":false,"replies":[{"id":"108348","content":"好问题~ 我们分资源调度和任务调度两种情况来说。<br><br>Spark在做任务调度之前，SchedulerBackend封装的调度器，比如Yarn、Mesos、Standalone，实际上已经完成了资源调度，换句话说，整个集群有多少个containers&#47;executors，已经是一件确定的事情了。而且，每个Executors的CPU和内存，也都是确定的了（因为你启动Spark集群的时候，使用配置项指定了每个Executors的CPU和内存分别是多少）。资源调度器在做资源调度的时候，确实是同时需要CPU和内存信息的。<br><br>资源调度完成后，Spark开始任务调度，你的问题，其实是任务调度范畴的问题。也就是TaskScheduler在准备调度任务的时候，要事先知道都有哪些Executors可用，注意，是可用。也就是TaskScheduler的核心目的，在于获取“可用”的Executors。<br><br>现在来回答你的问题，也就是：为什么ExecutorData不存储于内存相关的信息。答案是：不需要。一来，TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。二来，每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。<br><br>再来说Free Memory，首先，我们说过，Spark对于内存的预估不准，再者，每个Executors的可用内存都会随着GC的执行而动态变化，因此，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。一者是不准，二来确实没用，因为TaskScheduler拿不到数据分片大小这样的信息，TaskScheduler在Driver端，而数据分片是在目标Executors，所以TaskScheduler拿到Free Memory也没啥用，因为它也不能判断说：task要处理的数据分片，是不是超过了目标Executors的可用内存。<br><br>综上，ExecutorData的数据结构中，只保存了CPU信息，而没有记录内存消耗等信息。不知道这些能不能解答你的问题？有问题再聊哈~","user_name":"作者回复","comment_id":298360,"uid":"1043100","ip_address":"","utype":1,"ctime":1624271317,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"190602617271","product_id":100073401,"comment_content":"任务调度的时候不考虑可用内存大小吗","like_count":45,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522112,"discussion_content":"好问题~ 我们分资源调度和任务调度两种情况来说。\n\nSpark在做任务调度之前，SchedulerBackend封装的调度器，比如Yarn、Mesos、Standalone，实际上已经完成了资源调度，换句话说，整个集群有多少个containers/executors，已经是一件确定的事情了。而且，每个Executors的CPU和内存，也都是确定的了（因为你启动Spark集群的时候，使用配置项指定了每个Executors的CPU和内存分别是多少）。资源调度器在做资源调度的时候，确实是同时需要CPU和内存信息的。\n\n资源调度完成后，Spark开始任务调度，你的问题，其实是任务调度范畴的问题。也就是TaskScheduler在准备调度任务的时候，要事先知道都有哪些Executors可用，注意，是可用。也就是TaskScheduler的核心目的，在于获取“可用”的Executors。\n\n现在来回答你的问题，也就是：为什么ExecutorData不存储于内存相关的信息。答案是：不需要。一来，TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。二来，每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。\n\n再来说Free Memory，首先，我们说过，Spark对于内存的预估不准，再者，每个Executors的可用内存都会随着GC的执行而动态变化，因此，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。一者是不准，二来确实没用，因为TaskScheduler拿不到数据分片大小这样的信息，TaskScheduler在Driver端，而数据分片是在目标Executors，所以TaskScheduler拿到Free Memory也没啥用，因为它也不能判断说：task要处理的数据分片，是不是超过了目标Executors的可用内存。\n\n综上，ExecutorData的数据结构中，只保存了CPU信息，而没有记录内存消耗等信息。不知道这些能不能解答你的问题？有问题再聊哈~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1624271317,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":386827,"discussion_content":"老师的解答很准确，跟我的想法一致，一来是太难预估，二是还受gc的实时影响，spark没必要这么干","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627822288,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285226,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1616677221,"is_pvip":false,"replies":[{"id":"103538","content":"好问题，是这样的，资源调度和任务调度是分开的。<br><br>资源调度主要看哪些节点可以启动executors，是否能满足executors所需的cpu数量要求，这个时候，不会考虑任务、数据本地性这些因素。<br><br>资源调度完成之后，在任务调度阶段，spark负责计算每个任务的本地性，效果就是task明确知道自己应该调度到哪个节点，甚至是哪个executors。最后scheduler Backend会把task代码，分发到目标节点的目标executors，完成任务调度，实现数据不动代码动。<br><br>所以，二者是独立的，不能混为一谈哈～","user_name":"作者回复","comment_id":285226,"uid":"1043100","ip_address":"","utype":1,"ctime":1616722005,"user_name_real":"吴磊"}],"discussion_count":6,"race_medal":0,"score":"83221055845","product_id":100073401,"comment_content":"老师，数据尽量不动，比如有部分数据在节点A，那么移动计算难道不是要在节点A上启动Excutor才可以进行计算吗？但是Excutor不是在申请资源的时候就确定了在哪几个节点上启动Excutor吗？老师请指教一下","like_count":19,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517620,"discussion_content":"好问题，是这样的，资源调度和任务调度是分开的。\n\n资源调度主要看哪些节点可以启动executors，是否能满足executors所需的cpu数量要求，这个时候，不会考虑任务、数据本地性这些因素。\n\n资源调度完成之后，在任务调度阶段，spark负责计算每个任务的本地性，效果就是task明确知道自己应该调度到哪个节点，甚至是哪个executors。最后scheduler Backend会把task代码，分发到目标节点的目标executors，完成任务调度，实现数据不动代码动。\n\n所以，二者是独立的，不能混为一谈哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616722005,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2956361,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ4VFiaGZicIG5Fx9pMd8vibntD6E91IdzKgER10wJUSas2G8zib1pl5yzMFvkIA5zLBDB8Wa21xkynIw/132","nickname":"sweet smile","note":"","ucode":"B0D0B3502AA7A1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":559933,"discussion_content":"我理解所以数据分片一般是3个副本，这样就能保证同一个机架和不同机架都有数据，资源调度的时候是不是尽量也会基于机架的原则来调度啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649066986,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2829627,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/2d/3b/6d8e79ee.jpg","nickname":"木杉","note":"","ucode":"C691F8D72A9FB1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555292,"discussion_content":"Executor的启动时候无法直接考虑到数据本地性的问题， 但是默认追寻 尽量分散的规则. ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646837258,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1329566,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","nickname":"慢慢卢","note":"","ucode":"853D399100D83B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":378986,"discussion_content":"我觉得是如果资源调度时没有在A节点部署executors，后面的任务调度也不会调度到A节点，这时必须要移动数据。任务调度基于资源调度之上，不是完全没有关系吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623575124,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1329566,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","nickname":"慢慢卢","note":"","ucode":"853D399100D83B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":386825,"discussion_content":"你说的这个跟老师说的不矛盾，先调度资源，如果都没有在a节点创建executor，那么更谈不上后面的任务调度。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627821359,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":378986,"ip_address":""},"score":386825,"extra":""},{"author":{"id":2455712,"avatar":"https://static001.geekbang.org/account/avatar/00/25/78/a0/7a248ddc.jpg","nickname":"福","note":"","ucode":"F2FC7AF5D433C6","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1329566,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","nickname":"慢慢卢","note":"","ucode":"853D399100D83B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":410718,"discussion_content":"假设A,B,C,  A,B 在一个机架，AC 在不同的机架，b,c都有executors,  A节点没有executors,那么我在b节点申请资源呀，那么他们就是Rack local，比在c节点申请资源更合理（因为没有跨机架）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635765751,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":378986,"ip_address":""},"score":410718,"extra":""}]}]},{"had_liked":false,"id":285004,"user_name":"L3nvy","can_delete":false,"product_type":"c1","uid":1271157,"ip_address":"","ucode":"0B74B27C121D56","user_header":"https://static001.geekbang.org/account/avatar/00/13/65/75/f9d7e8b7.jpg","comment_is_top":false,"comment_ctime":1616574877,"is_pvip":false,"replies":[{"id":"103371","content":"第一题给满分💯，看答案就知道认真去读源码了，赞一个～ 第二题也答对了。第二题，再想想，还有其他cases吗？","user_name":"作者回复","comment_id":285004,"uid":"1043100","ip_address":"","utype":1,"ctime":1616583566,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"57451149725","product_id":100073401,"comment_content":"1. <br>位置信息通过特定的字符串前缀格式标识 <br>executor_[hostname]_[executorid]<br>[hostname]<br>hdfs_cache_[hostname]<br><br>DAGScheduler会尝试获取RDD的每个Partition的偏好位置信息，a.如果RDD被缓存，通过缓存的位置信息获取每个分区的位置信息；b.如果RDD有preferredLocations属性，通过preferredLocations获取每个分区的位置信息；c. \b遍历RDD的所有是NarrowDependency的父RDD，找到第一个满足a,b条件的位置信息<br><br>DAGScheduler将生成好的TaskSet提交给TaskSetManager进行任务的本地性级别计算<br><br>2.<br>感觉像是Spark on Kubernetes这种场景<br>应该和相关存储配置有关；不太了解，猜想的话。如果是配置的Spark中间过程使用的存储是分布式存储，Node Local应该不成立；如果就是单个容器的内部空间，或者挂载到主机上的空间，应该可以成立<br>","like_count":14,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517550,"discussion_content":"第一题给满分💯，看答案就知道认真去读源码了，赞一个～ 第二题也答对了。第二题，再想想，还有其他cases吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616583566,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1271157,"avatar":"https://static001.geekbang.org/account/avatar/00/13/65/75/f9d7e8b7.jpg","nickname":"L3nvy","note":"","ucode":"0B74B27C121D56","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361058,"discussion_content":"跟着专栏的节奏多学一下，老师留的每日一练有点难啊，😳，希望后面能答疑一下","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616589831,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1271157,"avatar":"https://static001.geekbang.org/account/avatar/00/13/65/75/f9d7e8b7.jpg","nickname":"L3nvy","note":"","ucode":"0B74B27C121D56","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362550,"discussion_content":"已经答得很好了呀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616981031,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":361058,"ip_address":""},"score":362550,"extra":""}]}]},{"had_liked":false,"id":288872,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1618742295,"is_pvip":false,"replies":[{"id":"104854","content":"非常好，思考的很深入~ 你说的没错，由于map没有用广播，所以每个task都会携带这个map，有额外的网络和内存存储开销。<br><br>但是，你要看跟谁比，跟广播比，第二种写法确实不如广播，但如果你跟第一种实现比，性能提升会非常明显。在第一种实现下，task不会携带map，而是在Executor临时去读文件、临时创建那个map，这个重复的计算开销，远大于task分发携带map带来的网络和内存开销。<br><br>简言之， 你说的非常对，不过咱们这一讲要强调的关键是调度系统，因此后来并没有用广播进一步优化，讲道理来说，一定是广播的实现方式是最优的。","user_name":"作者回复","comment_id":288872,"uid":"1043100","ip_address":"","utype":1,"ctime":1618759259,"user_name_real":"吴磊"}],"discussion_count":5,"race_medal":0,"score":"48863382551","product_id":100073401,"comment_content":"老师，我这是二刷您的课程了，但我想说课程的例子没看懂。第二种用部分函数的例子里，是节约了哪步操作呢？读文件应该只要Driver读一次就够了。但是zipWithIndex生成的map呢，由于没有把它广播出去，那应该还是每个task都会被拷贝一份全量的map吧。我这样的理解对吗？如果是对的，那感觉性能提升也不应该那么明显吧…","like_count":12,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518757,"discussion_content":"非常好，思考的很深入~ 你说的没错，由于map没有用广播，所以每个task都会携带这个map，有额外的网络和内存存储开销。\n\n但是，你要看跟谁比，跟广播比，第二种写法确实不如广播，但如果你跟第一种实现比，性能提升会非常明显。在第一种实现下，task不会携带map，而是在Executor临时去读文件、临时创建那个map，这个重复的计算开销，远大于task分发携带map带来的网络和内存开销。\n\n简言之， 你说的非常对，不过咱们这一讲要强调的关键是调度系统，因此后来并没有用广播进一步优化，讲道理来说，一定是广播的实现方式是最优的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618759259,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2843392,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/EEbkgyA07UnmOOV8fwicZX4X2Jq4YKdH6k3thefXXvFUhJib4wicVltbTKEVvuyTy0ObovROTPtAFxmok6IicTicF2w/132","nickname":"May","note":"","ucode":"9CE47CFB030C33","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537286,"discussion_content":"仔细读了评论，好像理解了：\n1）Source. fromFile (LocalPath)方式加载，可加载本地文件，这里本地文件指的是非集群方式\n2）实现方法一中：一个函数findIndex外和代码【findIndex(filePath, &#34;体育-篮球-NBA-湖人&#34;)】，被打包发给了executor,所以 每个executor都需要执行，如果有RDD需要用到这个函数，那么RDD中的每一条数据都需要执行一次\n2）实现方法二中：重点在【val partFunc = findIndex(filePath)】，老师在评论中说【\n没有被apply到分布式数据集的计算，都是在driver执行。比如常见的数组、list、字符串，等等。rdd. dataframe. 这些分布式数据集上apply的计算逻辑，才会分发到executors去执行。】所以partFunc 是在driver端计算出来的，也就是【输出一个只带一个形参的标量函数，这个标量函数内携带了刚刚建好的映射字典】，然后把partFunc发给每个executor，这个executor不用再做读文件和生成map的操作了；","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1639019854,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535073,"discussion_content":"问下说第二种方法不如广播是因为方法二是以task为粒度分发map，而广播是以executor为粒度分发map吗","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1638345329,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1883035,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/bb/9b/d649f756.jpg","nickname":"月夜枫","note":"","ucode":"A153D482576782","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":380566,"discussion_content":"这个问题是很多人都想问的","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1624584742,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2843392,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/EEbkgyA07UnmOOV8fwicZX4X2Jq4YKdH6k3thefXXvFUhJib4wicVltbTKEVvuyTy0ObovROTPtAFxmok6IicTicF2w/132","nickname":"May","note":"","ucode":"9CE47CFB030C33","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537276,"discussion_content":"我还是有点不明白：1）为什么在第二种实现方式中：用户代码先在 Driver 端用模板文件调用这个高阶函数，生成了映射字典 2）怎么去判断操作是在driver端实现还是executor上实现呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639017734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285366,"user_name":"小学生敬亭山","can_delete":false,"product_type":"c1","uid":1239584,"ip_address":"","ucode":"4879D8BBF74913","user_header":"https://static001.geekbang.org/account/avatar/00/12/ea/20/78ab5f92.jpg","comment_is_top":false,"comment_ctime":1616754909,"is_pvip":false,"replies":[{"id":"103582","content":"好思路，完全没问题～","user_name":"作者回复","comment_id":285366,"uid":"1043100","ip_address":"","utype":1,"ctime":1616808712,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"40271460573","product_id":100073401,"comment_content":"老师正例这个，先建map，再broadcast map 是不是一样的逻辑","like_count":9,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517658,"discussion_content":"好思路，完全没问题～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616808712,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":303394,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1626767669,"is_pvip":false,"replies":[{"id":"110175","content":"非常好的问题~ 你说的是对的，这里是我没有说清楚。不管是实现方式1、还是实现方式2，都是RDD里的每一条数据都去运行一次函数。原文中“这意味着集群中的每一个 Executors 都需要执行函数中封装的两个计算步骤”这个表述，是不准确的。准确的说法，应该是像你说的，每条数据，都需要执行这两个计算步骤。感谢老弟的提醒和纠正~ 你说的是对的，就按你自己的思路来理解就好~<br><br>另外，要想让每个Executor只处理一次，那么咱们就只能依赖广播变量了，只有广播变量才能做到这一点。","user_name":"作者回复","comment_id":303394,"uid":"1043100","ip_address":"","utype":1,"ctime":1627481975,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"27396571445","product_id":100073401,"comment_content":"老师，请问对于第一种函数的写法和调用，为何是每个executor只处理一次，而不是对RDD里的每一条数据都去运行一遍函数，然后都加载一次map呢？请问这个函数在spark内核里如何解析和运作的呢，他如何知道里面有个map只去初始化一次，而不是每条数据都运行一次呢？","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523659,"discussion_content":"非常好的问题~ 你说的是对的，这里是我没有说清楚。不管是实现方式1、还是实现方式2，都是RDD里的每一条数据都去运行一次函数。原文中“这意味着集群中的每一个 Executors 都需要执行函数中封装的两个计算步骤”这个表述，是不准确的。准确的说法，应该是像你说的，每条数据，都需要执行这两个计算步骤。感谢老弟的提醒和纠正~ 你说的是对的，就按你自己的思路来理解就好~\n\n另外，要想让每个Executor只处理一次，那么咱们就只能依赖广播变量了，只有广播变量才能做到这一点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627481975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311627,"user_name":"张笑笑","can_delete":false,"product_type":"c1","uid":2362595,"ip_address":"","ucode":"CB907FFB760DD9","user_header":"https://static001.geekbang.org/account/avatar/00/24/0c/e3/b415984e.jpg","comment_is_top":false,"comment_ctime":1631352183,"is_pvip":false,"replies":[{"id":"112953","content":"其实不用特别在意高阶函数，它的核心作用，其实是创建出一个包含了查询字典的函数对象。真正分发到Executors的代码，是这个包含了字典的函数对象。<br><br>而创建字典的过程，只在Driver端做一次，因为传递给高阶函数第一个参数、生成“带查询字典的函数对象”的动作，只做了一次，所以创建字典的开销，只有在Driver的那一次。","user_name":"作者回复","comment_id":311627,"uid":"1043100","ip_address":"","utype":1,"ctime":1631461124,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"23106188663","product_id":100073401,"comment_content":"吴老师，您给的这个案例中，第二个实现方式上使用了高阶函数，看了几次，确实还是没明白，为什么使用这种写法，它只在driver端做一次计算?为什么就省去了读取文件，创建字典的开小了，迷惑中...","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526684,"discussion_content":"其实不用特别在意高阶函数，它的核心作用，其实是创建出一个包含了查询字典的函数对象。真正分发到Executors的代码，是这个包含了字典的函数对象。\n\n而创建字典的过程，只在Driver端做一次，因为传递给高阶函数第一个参数、生成“带查询字典的函数对象”的动作，只做了一次，所以创建字典的开销，只有在Driver的那一次。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631461124,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1030156,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7BDPIqkNsYm2ibVUkhIAuniaBOcRIiaroNsKT7YasCRGkVxbba5gsZccBRkGs1rlXRJScvDMebqcSGw/132","nickname":"不算帅","note":"","ucode":"1907C992E4DF30","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558274,"discussion_content":"知道这种高阶函数用法之前，我通常的做法是将Map做成Broadcast。然后在DatasetAPI中引用Broadcast资源，也能实现字典的一次生成，多处使用。\n跟这里的高阶函数的区别，我个人理解:\nBroadcast是一次性将数据发到executor, 做成全局变量，后面跑在这个executor上的所有Task都引用这一个变量。\n高阶函数则是将函数内带的字典数据序列化，每个Task启动时，再将数据反序列化成字典。\n所以原则上，Broadcast效率更高一些","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648186511,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1943439,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/a7/8f/0d8e6d34.jpg","nickname":"陈子","note":"","ucode":"CDC23530B6235A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":397330,"discussion_content":"也就是说案例二中，driver 端发送给 Executor 的是序列化后的函数，也就是“函数逻辑+变量数据”。仔细看可以发现，driver 端是将 rdd 转为了 array，通过 array 的 zip 生成映射后的 map 数据，并对 map 数据执行 getOrElse，这些都不是 RDD，所以 driver 直接将序列化后的函数发送给了 Executor。如果是 RDD 操作的话，应该就是分布式任务分发了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632585129,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287792,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1618151014,"is_pvip":true,"replies":[{"id":"104548","content":"666，不容易，赞坚持不懈~ 👍","user_name":"作者回复","comment_id":287792,"uid":"1043100","ip_address":"","utype":1,"ctime":1618220531,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"23092987494","product_id":100073401,"comment_content":"&#47;**<br>   * Create a TaskLocation from a string returned by getPreferredLocations.<br>   * These strings have the form executor_[hostname]_[executorid], [hostname], or<br>   * hdfs_cache_[hostname], depending on whether the location is cached.<br>   *&#47; 终于找到了榜一所说的location信息","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518424,"discussion_content":"666，不容易，赞坚持不懈~ 👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618220531,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285078,"user_name":"来世愿做友人 A","can_delete":false,"product_type":"c1","uid":1181606,"ip_address":"","ucode":"EF20966B0F27E1","user_header":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","comment_is_top":false,"comment_ctime":1616600506,"is_pvip":false,"replies":[{"id":"103477","content":"如果你说的是shuffle read阶段的locality，咱们换个角度思考这个问题。shuffle map阶段，每个map task把中间文件写到本地盘。shuffle read阶段，每个reduce task需要从集群的所有节点拉数据，走网络。这个过程是由shuffle的实现机制决定的。<br><br>因此，从reduce视角看过去，所有中间文件都是它的“数据源”，这些数据源，散落在集群的每一个节点，因此，每个reduce task的locality，在最好的情况下，能做到rack local，最差的时候，那就是any。<br><br>正是因为这样，所以我们会强调，竭尽全力避免shuffle。因为它的实现机制，决定了reduce task的计算一定会让数据走网络。<br><br>不知道这么说能不能回答你的问题哈～ 要是我理解错了，就再at我，咱们继续讨论哈～","user_name":"作者回复","comment_id":285078,"uid":"1043100","ip_address":"","utype":1,"ctime":1616673230,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"23091436986","product_id":100073401,"comment_content":"第一题：因为是为每个 partition 建立一个task，所以在建立task之前，都会获取每个partition的位置偏好信息。首先判断 rdd 是否被缓存过，通过 rddId + splitIndex 组合成 blockId 判断。如果没有，判断preferredLocations，看起来是判断是否 checkpoint 过。如果还没有，向上获取父rdd，如果是窄依赖，循环上面的判断逻辑。这里想问个问题，代码里直到task分发，似乎没有看到关于shuffle的位置偏好。比如中间有个shuffle过程，shuffle结果写在磁盘小文件，是不是下个 stage 的 task 应该发到父 stage 的所在 executor 更合适？目前没看到这个逻辑，想问问老师","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517580,"discussion_content":"如果你说的是shuffle read阶段的locality，咱们换个角度思考这个问题。shuffle map阶段，每个map task把中间文件写到本地盘。shuffle read阶段，每个reduce task需要从集群的所有节点拉数据，走网络。这个过程是由shuffle的实现机制决定的。\n\n因此，从reduce视角看过去，所有中间文件都是它的“数据源”，这些数据源，散落在集群的每一个节点，因此，每个reduce task的locality，在最好的情况下，能做到rack local，最差的时候，那就是any。\n\n正是因为这样，所以我们会强调，竭尽全力避免shuffle。因为它的实现机制，决定了reduce task的计算一定会让数据走网络。\n\n不知道这么说能不能回答你的问题哈～ 要是我理解错了，就再at我，咱们继续讨论哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616673230,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2485585,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","nickname":"Geek_d794f8","note":"","ucode":"1E20DA4FF8B800","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":588527,"discussion_content":"如果是这样的话，那只有第一个stage是从数据源（比如说hdfs）读取数据，需要考虑数据本地性的问题；后面的stage都是以shuffle划分的，因此都是从它的前一个stage的落到本地磁盘的文件中读取数据，因此后面这些stage也不存在本地性的问题，就如老师说的：在最好的情况下，能做到rack local，最差的时候，那就是any。\n老师，这样理解对吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1663815066,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":517580,"ip_address":"北京"},"score":588527,"extra":""}]},{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361671,"discussion_content":"了解了，谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616726021,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286690,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1617501894,"is_pvip":false,"replies":[{"id":"104120","content":"没问题，就是locality wait，就是有些task是有调度倾向的，preferredLocations。但是，它想要去的executors，可能正在忙，没有空闲cpu。这个时候两个选择，要么，等executors忙完；要么放弃，调度到其他节点或是executors，退而求其次。locality wait默认3s，但是可以调。不过一般3s就行。除非有些io密集型，必须要node local，这个时候，可以适当调大，多等等。<br><br>其实就是平衡，等待时间和执行时间的平衡，看你具体场景。","user_name":"作者回复","comment_id":286690,"uid":"1043100","ip_address":"","utype":1,"ctime":1617516509,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"18797371078","product_id":100073401,"comment_content":"关于任务调度:<br>默认情况下，会先调度process local那批tasks;然后依次是node,rack,any。<br><br>在调度了最契合locality的tasks后还有空闲executor。下一批task本来是有资源可用的，但最适合执行task的executor已被占用，此时会评估下一批tasks等待时间和在空闲executor执行数据传输时间，如果等待时间大于数据传输则直接调度到空闲executor，否则继续等待。<br><br>把wait参数设置为0，则可以不进行等待，有资源时直接调度执行<br><br>这块逻辑一直有点乱。磊哥看下哪有问题嘛？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518063,"discussion_content":"没问题，就是locality wait，就是有些task是有调度倾向的，preferredLocations。但是，它想要去的executors，可能正在忙，没有空闲cpu。这个时候两个选择，要么，等executors忙完；要么放弃，调度到其他节点或是executors，退而求其次。locality wait默认3s，但是可以调。不过一般3s就行。除非有些io密集型，必须要node local，这个时候，可以适当调大，多等等。\n\n其实就是平衡，等待时间和执行时间的平衡，看你具体场景。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617516509,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284917,"user_name":"白音","can_delete":false,"product_type":"c1","uid":2526221,"ip_address":"","ucode":"B83B0269B8E9A0","user_header":"https://static001.geekbang.org/account/avatar/00/26/8c/0d/42e16041.jpg","comment_is_top":false,"comment_ctime":1616545958,"is_pvip":false,"replies":[{"id":"103359","content":"对，反例的情况，需要把模板文件分发到每个executors，每个executors都需要：1 读文件，建字典；2 在千亿样本上查找字典。<br><br>正例里面，driver读文件、建字典；executors上面，只做第二步。","user_name":"作者回复","comment_id":284917,"uid":"1043100","ip_address":"","utype":1,"ctime":1616550201,"user_name_real":"吴磊"}],"discussion_count":5,"race_medal":0,"score":"18796415142","product_id":100073401,"comment_content":"示例中关于读文件没太理解想请教下老师。<br>Source.fromFile 用于读本地文件，所以用spark读文件不是应该用 sc.textFile 来从hdfs目录读取? 或者示例的意思是在跑这段代码之前已经将模板文件分发到了集群每个executor本地吗？ <br>","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517509,"discussion_content":"对，反例的情况，需要把模板文件分发到每个executors，每个executors都需要：1 读文件，建字典；2 在千亿样本上查找字典。\n\n正例里面，driver读文件、建字典；executors上面，只做第二步。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616550201,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2455062,"avatar":"https://static001.geekbang.org/account/avatar/00/25/76/16/916b907f.jpg","nickname":"🍟🍟🍟","note":"","ucode":"DBD5DCEBD78B4C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":585702,"discussion_content":"老师你好，想问下第一步driver读文件的时候，是建立了一个函数（输入interest返回index），那么这个函数我理解底层还是一个大的map，包括千亿样本的一个map，这样不会把driver的内存撑爆吗？\n而且方法1是executor执行了加载了所有的字典（假设10个executor），那么由于并行查找，执行时间应该是方法2（driver查找所有数据并建立字段map）的十分之一时间，怎么方法2会更快呢?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661765179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":517509,"ip_address":"广东"},"score":585702,"extra":""}]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363351,"discussion_content":"我明白了，就是把加载进来的模板文件加载到函数里面，发送到各个Executor.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617176606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":363653,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617254011,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":363351,"ip_address":""},"score":363653,"extra":""}]},{"author":{"id":2526221,"avatar":"https://static001.geekbang.org/account/avatar/00/26/8c/0d/42e16041.jpg","nickname":"白音","note":"","ucode":"B83B0269B8E9A0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360863,"discussion_content":"了解了，thx~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616550518,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285061,"user_name":"October","can_delete":false,"product_type":"c1","uid":1137879,"ip_address":"","ucode":"CEDA78F4A5F8B1","user_header":"https://static001.geekbang.org/account/avatar/00/11/5c/d7/e4673fde.jpg","comment_is_top":false,"comment_ctime":1616594631,"is_pvip":false,"replies":[{"id":"103478","content":"非常好的问题！非常好！通过问题就能看出来，这一篇读的非常认真，赞一个先～ 是这样，你查的没错，首先用户的说法确实容易造成困惑，其实就是应用中多个不同的job，换句话说，就是你应用中的多个actions。开发的时候，你可以把job assign到某个调度池，从而区分不同job在资源争抢上的优先级。<br><br>回答你最后的问题，换一种问法，其实就是什么场景下，一个应用会有多个job。其实这种cases很多，我最先想到的，就是etl，因为etl往往有多个目的，读取多个数据源，输出多种不同阶段的数据，这是常有的事，因此etl至少是一个。<br><br>另外一个，机器学习中的样本过程，通过一个应用生产多个schema的训练样本，这也会涉及到多个job。<br><br>你可以沿着这个思路，再想想还有没有其他cases哈～","user_name":"作者回复","comment_id":285061,"uid":"1043100","ip_address":"","utype":1,"ctime":1616673955,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"14501496519","product_id":100073401,"comment_content":"老师，您提到如果taskScheduler采用Fair调度策略对不同stages进行调度，可以为不同的用户配置不同的调度池，刚开始这个地方有些不理解，同一个应用程序中，怎么会有不同的用户？ 于是查了一下官网，官网貌似说的是这里的用户不是程序提交的那个用户，这里的用户对应提交job的一个线程，不知道自己理解的是否正确。 另外，如果我的理解正确的话，在同一个应用程序中使用不同的线程提交job，这个使用方式，我目前还没有见过，请问老师，大概什么场景下，会用不同线程提交job？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517571,"discussion_content":"非常好的问题！非常好！通过问题就能看出来，这一篇读的非常认真，赞一个先～ 是这样，你查的没错，首先用户的说法确实容易造成困惑，其实就是应用中多个不同的job，换句话说，就是你应用中的多个actions。开发的时候，你可以把job assign到某个调度池，从而区分不同job在资源争抢上的优先级。\n\n回答你最后的问题，换一种问法，其实就是什么场景下，一个应用会有多个job。其实这种cases很多，我最先想到的，就是etl，因为etl往往有多个目的，读取多个数据源，输出多种不同阶段的数据，这是常有的事，因此etl至少是一个。\n\n另外一个，机器学习中的样本过程，通过一个应用生产多个schema的训练样本，这也会涉及到多个job。\n\n你可以沿着这个思路，再想想还有没有其他cases哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616673955,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387317,"discussion_content":"我感觉你和老师在说两件事情，另外，没听说过在线程里提交job的说法～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628122727,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":308961,"user_name":"站在桥上看风景","can_delete":false,"product_type":"c1","uid":2066559,"ip_address":"","ucode":"09960C9E1C1C38","user_header":"https://static001.geekbang.org/account/avatar/00/1f/88/7f/97459eff.jpg","comment_is_top":false,"comment_ctime":1629873281,"is_pvip":false,"replies":[{"id":"111971","content":"是的，没错~ <br><br>FIFO、FAIR是Standalone的两种调度模式；<br>YARN的话，选择更多，就是你说的这几个~<br><br>我这里没有交代清楚，谢谢提醒哈~","user_name":"作者回复","comment_id":308961,"uid":"1043100","ip_address":"","utype":1,"ctime":1630043080,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"10219807873","product_id":100073401,"comment_content":"吴老师，FIFO与FAIR应该是在使用standalone时的情况是吧，如果使用yarn的话资源调度就是FIFO、Capacity Scheduler、Fair Scheduler这三个的选择了是吧","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525694,"discussion_content":"是的，没错~ \n\nFIFO、FAIR是Standalone的两种调度模式；\nYARN的话，选择更多，就是你说的这几个~\n\n我这里没有交代清楚，谢谢提醒哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630043080,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285927,"user_name":"🚤","can_delete":false,"product_type":"c1","uid":1460463,"ip_address":"","ucode":"95D7F773789B74","user_header":"https://static001.geekbang.org/account/avatar/00/16/48/ef/4750cb14.jpg","comment_is_top":false,"comment_ctime":1617075881,"is_pvip":false,"replies":[{"id":"103841","content":"没错，和广播殊途同归～ 这里咱们为了强调调度系统，所以采用了这种实现方式哈～","user_name":"作者回复","comment_id":285927,"uid":"1043100","ip_address":"","utype":1,"ctime":1617102853,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"10207010473","product_id":100073401,"comment_content":"如果是我来写Label Encoding的话，在模板数据量不大的情况下，我会第一时间把模板数据转成Map之后广播出去。<br>看了老师的正例，感觉其实和广播的意思是差不多的吧","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517825,"discussion_content":"没错，和广播殊途同归～ 这里咱们为了强调调度系统，所以采用了这种实现方式哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617102853,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285362,"user_name":"小学生敬亭山","can_delete":false,"product_type":"c1","uid":1239584,"ip_address":"","ucode":"4879D8BBF74913","user_header":"https://static001.geekbang.org/account/avatar/00/12/ea/20/78ab5f92.jpg","comment_is_top":false,"comment_ctime":1616753486,"is_pvip":false,"replies":[{"id":"103584","content":"这一步比较重，从数据处理的最终目的来说，这一步算是开销，放在driver端算一次是最佳策略，避免这个开销在executors断反复计算。<br><br>没有被apply到分布式数据集的计算，都是在driver执行。比如常见的数组、list、字符串，等等。rdd. dataframe. 这些分布式数据集上apply的计算逻辑，才会分发到executors去执行。","user_name":"作者回复","comment_id":285362,"uid":"1043100","ip_address":"","utype":1,"ctime":1616809564,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"10206688078","product_id":100073401,"comment_content":"老师好，为什么第二个示例（也就是正例），建hashmap这个代码就一定是在driver端执行，然后代码再进入调度系统我不是太明白？<br>什么时候代码会在driver端执行？<br>","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517657,"discussion_content":"这一步比较重，从数据处理的最终目的来说，这一步算是开销，放在driver端算一次是最佳策略，避免这个开销在executors断反复计算。\n\n没有被apply到分布式数据集的计算，都是在driver执行。比如常见的数组、list、字符串，等等。rdd. dataframe. 这些分布式数据集上apply的计算逻辑，才会分发到executors去执行。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616809564,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310727,"user_name":"薛峰","can_delete":false,"product_type":"c1","uid":2546023,"ip_address":"","ucode":"DA8CAF1473D2B9","user_header":"https://static001.geekbang.org/account/avatar/00/26/d9/67/9bca6a6e.jpg","comment_is_top":false,"comment_ctime":1630877694,"is_pvip":false,"replies":[{"id":"112643","content":"对的，python用闭包也可以做到这一点~ 除了用闭包函数之外，还可以考虑广播变量~","user_name":"作者回复","comment_id":310727,"uid":"1043100","ip_address":"","utype":1,"ctime":1630938822,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5925844990","product_id":100073401,"comment_content":"很有启发，谢谢磊哥，我也想问一下如果用python的话也需要同样的操作么？<br>比如<br>dic_file=&#47;path&#47;to&#47;dic_file<br>def func_lower(dic_file, keyword):<br>  load dic_file,<br>  find keyword<br>  return index<br><br><br>def func_higher(keyword):<br>  return func_lower(dic_file,keyword)","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526325,"discussion_content":"对的，python用闭包也可以做到这一点~ 除了用闭包函数之外，还可以考虑广播变量~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630938822,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293097,"user_name":"Geek_fb1b68","can_delete":false,"product_type":"c1","uid":2130762,"ip_address":"","ucode":"95A78CCA43AFCE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erGLUqIRJ2gJXIDNhfXSp4vxeb7pibQcNt1Lpicbfsvzf0ILdNZrLDfLcKZXkTEhy8U0ewWDeZ0b5Pg/132","comment_is_top":false,"comment_ctime":1621215248,"is_pvip":false,"replies":[{"id":"106293","content":"对，是这么回事~","user_name":"作者回复","comment_id":293097,"uid":"1043100","ip_address":"","utype":1,"ctime":1621414601,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5916182544","product_id":100073401,"comment_content":"看了这篇 总结是spark drive端coding尽量产生的是计算关系 让计算关系在excutor端lazy触发","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520073,"discussion_content":"对，是这么回事~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621414601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286852,"user_name":"hel","can_delete":false,"product_type":"c1","uid":1359507,"ip_address":"","ucode":"E9F93F32BCD7D7","user_header":"https://static001.geekbang.org/account/avatar/00/14/be/93/247fb2c8.jpg","comment_is_top":false,"comment_ctime":1617632355,"is_pvip":false,"replies":[{"id":"104157","content":"对，是这样的流程。yarn先做资源调度，把符合条件的container交给spark driver（yarn app master），然后driver完成任务调度，保证任务的本地性级别。","user_name":"作者回复","comment_id":286852,"uid":"1043100","ip_address":"","utype":1,"ctime":1617670812,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5912599651","product_id":100073401,"comment_content":"在spark on  yarn上spark向yarn申请资源，yarn是会返回所有空闲可用资源然后spark自己根据本地性原则来挑选的吗","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518108,"discussion_content":"对，是这样的流程。yarn先做资源调度，把符合条件的container交给spark driver（yarn app master），然后driver完成任务调度，保证任务的本地性级别。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617670812,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285242,"user_name":"牛红灯","can_delete":false,"product_type":"c1","uid":1784932,"ip_address":"","ucode":"1C050185FC0CDE","user_header":"https://static001.geekbang.org/account/avatar/00/1b/3c/64/09510f36.jpg","comment_is_top":false,"comment_ctime":1616682062,"is_pvip":false,"replies":[{"id":"103543","content":"是这样，模板文件是本地文件，因此可以从driver读进内存、构建字典，最后从driver分发到executors。咱们这里为了讲调度系统、强调任务分发，所以并没有给出完整的代码。所以你看到dataset那里apply函数就结束了，后面也没有写是把数据落盘、还是collect这些actions，目的还是让大家把注意力集中到与调度开销息息相关的那几行代码～","user_name":"作者回复","comment_id":285242,"uid":"1043100","ip_address":"","utype":1,"ctime":1616723407,"user_name_real":"吴磊"}],"discussion_count":4,"race_medal":0,"score":"5911649358","product_id":100073401,"comment_content":"老师,只有触发action算子的才在driver上运行吧,第二种方法中也没有提到是action呀,为什么读文件是在driver上呢","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517624,"discussion_content":"是这样，模板文件是本地文件，因此可以从driver读进内存、构建字典，最后从driver分发到executors。咱们这里为了讲调度系统、强调任务分发，所以并没有给出完整的代码。所以你看到dataset那里apply函数就结束了，后面也没有写是把数据落盘、还是collect这些actions，目的还是让大家把注意力集中到与调度开销息息相关的那几行代码～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616723407,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":386828,"discussion_content":"其实本质并不是高阶函数的原因导致变快的，根本原因是读文件的代码只在driver执行了一次，而不是在每个executor上都执行一遍，其实也可以吧高阶函数分成两个函数写，在实际开发上遇到这个场景，会采用先在driver上执行读写文件，然后广播变量分发的方式，避免没必要的重算。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1627823500,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1784932,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/3c/64/09510f36.jpg","nickname":"牛红灯","note":"","ucode":"1C050185FC0CDE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361999,"discussion_content":"那为什么只是换了个高接函数，就由从excutor变成了driver呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616821831,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1784932,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/3c/64/09510f36.jpg","nickname":"牛红灯","note":"","ucode":"1C050185FC0CDE","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362553,"discussion_content":"高阶的第一步，是在driver算得，第二步需要分发到driver","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616981121,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":361999,"ip_address":""},"score":362553,"extra":""}]}]},{"had_liked":false,"id":285228,"user_name":"尔雅","can_delete":false,"product_type":"c1","uid":1366229,"ip_address":"","ucode":"848C3B0D8B5441","user_header":"https://static001.geekbang.org/account/avatar/00/14/d8/d5/00f31ac9.jpg","comment_is_top":false,"comment_ctime":1616677903,"is_pvip":false,"replies":[{"id":"103542","content":"不不，和偏函数没什么关系哈。我只是用Partial functions来举例，目的是为了讲调度系统。Partial functions不是关键哈，任务调度过程中，分发了哪些东西到executors，这个才重要哈，这些分发到东西，决定了executors的负载。<br><br>Partial functions只是方便举例，我顺手就拿过来用了，它只是一种实现形式哈～<br><br>dag拆分stages的关键因素是shuffle哈，shuffle是边界，是关键。","user_name":"作者回复","comment_id":285228,"uid":"1043100","ip_address":"","utype":1,"ctime":1616722468,"user_name_real":"吴磊"}],"discussion_count":5,"race_medal":0,"score":"5911645199","product_id":100073401,"comment_content":"吴老师,<br>偏函数是将多个操作转换为一个操作了吗,<br>如果不使用偏函数,就会造成dag 将操作拆分成多个 stages<br>造成调度的负载升高,可以这样理解吗","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517622,"discussion_content":"不不，和偏函数没什么关系哈。我只是用Partial functions来举例，目的是为了讲调度系统。Partial functions不是关键哈，任务调度过程中，分发了哪些东西到executors，这个才重要哈，这些分发到东西，决定了executors的负载。\n\nPartial functions只是方便举例，我顺手就拿过来用了，它只是一种实现形式哈～\n\ndag拆分stages的关键因素是shuffle哈，shuffle是边界，是关键。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616722468,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1366229,"avatar":"https://static001.geekbang.org/account/avatar/00/14/d8/d5/00f31ac9.jpg","nickname":"尔雅","note":"","ucode":"848C3B0D8B5441","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362607,"discussion_content":"好的，谢谢吴老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616991135,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362567,"discussion_content":"偏函数，又笔误了😭","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616983139,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361778,"discussion_content":"难道是我对偏函数理解有误 0_0 这和偏函数没什么关系吧 A partial function is a function that does not provide an answer for every possible input value it can be given. It provides an answer only for a subset of possible data, and defines the data it can handle","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616750948,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362566,"discussion_content":"对，之前回复的太急，笔误了，本讲的是高阶函数，不是便函数哈，笔误了。后面讲到spark sql优化规则，这些rules是很好的便函数的例子～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616983099,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":361778,"ip_address":""},"score":362566,"extra":""}]}]},{"had_liked":false,"id":355818,"user_name":"🍟🍟🍟","can_delete":false,"product_type":"c1","uid":2455062,"ip_address":"广东","ucode":"DBD5DCEBD78B4C","user_header":"https://static001.geekbang.org/account/avatar/00/25/76/16/916b907f.jpg","comment_is_top":false,"comment_ctime":1661765192,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1661765192","product_id":100073401,"comment_content":"老师你好，想问下第一步driver读文件的时候，是建立了一个函数（输入interest返回index），那么这个函数我理解底层还是一个大的map，包括千亿样本的一个map，这样不会把driver的内存撑爆吗？<br>而且方法1是executor执行了加载了所有的字典（假设10个executor），那么由于并行查找，执行时间应该是方法2（driver查找所有数据并建立字段map）的十分之一时间，怎么方法2会更快呢?","like_count":0},{"had_liked":false,"id":351612,"user_name":"组织灵魂 王子健","can_delete":false,"product_type":"c1","uid":2957218,"ip_address":"","ucode":"94DB462CEAFAD3","user_header":"https://static001.geekbang.org/account/avatar/00/2d/1f/a2/0ac2dc38.jpg","comment_is_top":false,"comment_ctime":1658003132,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1658003132","product_id":100073401,"comment_content":"老师Label Encoding这个例子证明一点，就是写代码时候深刻意识到自己写的某段代码是在driver端执行的还是在Executors端执行的自己心里要非常清楚。","like_count":0},{"had_liked":false,"id":342587,"user_name":"里咯破","can_delete":false,"product_type":"c1","uid":1224546,"ip_address":"","ucode":"2DA41A6D44B3C4","user_header":"https://static001.geekbang.org/account/avatar/00/12/af/62/5eeb9041.jpg","comment_is_top":false,"comment_ctime":1650356284,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1650356284","product_id":100073401,"comment_content":"老师，我还有几个没搞懂的地方。1.reduce task的locality问题，在DAGScheduler生成reduce端taskset的时候，由于数据此时还在map端，reduce task选择最优位置的规则是尽量匹配执行过map task的executor吗？2.基于前一个问题，reduce task会复用执行map task的executor吗？还是重新选择executor呢3.reduce task是怎么确定自己要拉取哪一部分数据的呢。请老师指教一下，谢谢","like_count":0},{"had_liked":false,"id":336648,"user_name":"欧阳硕","can_delete":false,"product_type":"c1","uid":1815279,"ip_address":"","ucode":"2D027E63EC6C15","user_header":"https://static001.geekbang.org/account/avatar/00/1b/b2/ef/a0b79b16.jpg","comment_is_top":false,"comment_ctime":1646273922,"is_pvip":true,"replies":[{"id":"123251","content":"没错，老弟一针见血，总结得非常到位~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1646725240,"ip_address":"","comment_id":336648,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1646273922","product_id":100073401,"comment_content":"例子中的两种代码其实从返回形式就可见一斑了：方法一根据driver端路径地址返回一个int，意思就是每次调用都需要全扫描该路径才能得到返回结果；而第二种方法返回的是一个从string到int的映射关系，也就是说调用只依赖传入的string，也就避免了全盘扫描.换句话说，等同于将这个集合在不同节点上的广播加调用的封装.","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555038,"discussion_content":"没错，老弟一针见血，总结得非常到位~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646725240,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":330005,"user_name":"Geek_eba94c","can_delete":false,"product_type":"c1","uid":2878566,"ip_address":"","ucode":"562037A79EFC4A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIX30nQ8op79Cv4IlYYEcIP7vB7Js9Ers80kbCnqoY5B4vqDhnRuiaeiaSfYsVsPRVBsnicVTKSRQwHg/132","comment_is_top":false,"comment_ctime":1641725757,"is_pvip":false,"replies":[{"id":"120569","content":"好问题，Scala的语法，确实比较迷~<br>val findIndex: (String) =&gt; (String) =&gt; Int =<br>在这段代码里面，“(String) =&gt; (String) =&gt; Int”是findIndex这个变量的类型，它的含义是：<br>1）首先这个类型，指的是一个函数<br>2）函数的输入参数，是一个String的变量，也就是第一个(String)<br>3）而函数的输出，是另外一个函数，也就是“(String) =&gt; Int”，这个就好理解一些，这个函数输入参数是String的变量，输出是Int的变量<br><br>所以说，findIndex是一个高阶函数，他的输入是String，而输出是一个函数。<br>","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1642069134,"ip_address":"","comment_id":330005,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641725757","product_id":100073401,"comment_content":"请问文中第二段代码：“&#47;&#47;函数定义<br>val findIndex: (String) =&gt; (String) =&gt; Int = ”中，为什么要写两次“(String) =&gt; ”？公司刚上产品线，临时学的Scala，求大佬解答，谢谢！","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545874,"discussion_content":"好问题，Scala的语法，确实比较迷~\nval findIndex: (String) =&gt; (String) =&gt; Int =\n在这段代码里面，“(String) =&gt; (String) =&gt; Int”是findIndex这个变量的类型，它的含义是：\n1）首先这个类型，指的是一个函数\n2）函数的输入参数，是一个String的变量，也就是第一个(String)\n3）而函数的输出，是另外一个函数，也就是“(String) =&gt; Int”，这个就好理解一些，这个函数输入参数是String的变量，输出是Int的变量\n\n所以说，findIndex是一个高阶函数，他的输入是String，而输出是一个函数。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642069135,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325847,"user_name":"Geek_eb29a4","can_delete":false,"product_type":"c1","uid":2855459,"ip_address":"","ucode":"CF88D4ED5F4A25","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","comment_is_top":false,"comment_ctime":1639191395,"is_pvip":false,"replies":[{"id":"118715","content":"咱们先来说函数，在Scala里面，函数本身其实就是对象。<br>至于高阶函数，它指的是这样的函数：<br>1）参数本身是函数<br>2）返回结果的类型，是函数<br>在咱们的例子里面，findIndex(filePath)的返回结果，是一个函数，所以说，findIndex本身是高阶函数。<br>而val partFunc = findIndex(filePath)，那么partFunc这个函数对象，它本身包含了已经构建好的字典，在调用partFunc(&quot;体育-篮球-NBA-湖人&quot;)的话，这个函数partFunc就可以根据String，来返回字符串了~ 不需要每个Executors去再次扫描文件、构建字典","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639668912,"ip_address":"","comment_id":325847,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639191395","product_id":100073401,"comment_content":"&#39;’ 省去了扫描模板文件、建立字典的开销 &#39;&#39;<br>高阶函数为什么有这个效果？ 这个要参考什么学习资料？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539342,"discussion_content":"咱们先来说函数，在Scala里面，函数本身其实就是对象。\n至于高阶函数，它指的是这样的函数：\n1）参数本身是函数\n2）返回结果的类型，是函数\n在咱们的例子里面，findIndex(filePath)的返回结果，是一个函数，所以说，findIndex本身是高阶函数。\n而val partFunc = findIndex(filePath)，那么partFunc这个函数对象，它本身包含了已经构建好的字典，在调用partFunc(&#34;体育-篮球-NBA-湖人&#34;)的话，这个函数partFunc就可以根据String，来返回字符串了~ 不需要每个Executors去再次扫描文件、构建字典","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639668913,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325843,"user_name":"Geek_eb29a4","can_delete":false,"product_type":"c1","uid":2855459,"ip_address":"","ucode":"CF88D4ED5F4A25","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","comment_is_top":false,"comment_ctime":1639190289,"is_pvip":false,"replies":[{"id":"118716","content":"同上哈~ 在上一个问题回答过啦~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639668956,"ip_address":"","comment_id":325843,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639190289","product_id":100073401,"comment_content":"吴老师， 第二个是因为下面代码变化 吗？ 那解析下这两个分别带来什么提高吗？<br><br><br>val searchMap = lines.zip(0 until lines.size).toMap<br>(interest) =&gt; searchMap.getOrElse(interest, -1)<br><br>val partFunc = findIndex(filePath)","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539343,"discussion_content":"同上哈~ 在上一个问题回答过啦~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639668956,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325517,"user_name":"Pytrick","can_delete":false,"product_type":"c1","uid":1690447,"ip_address":"","ucode":"2759D2BB51D981","user_header":"https://static001.geekbang.org/account/avatar/00/19/cb/4f/7466a488.jpg","comment_is_top":false,"comment_ctime":1639014185,"is_pvip":false,"replies":[{"id":"118125","content":"后面会整理的哈~ ","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639045648,"ip_address":"","comment_id":325517,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639014185","product_id":100073401,"comment_content":"跪求pyspark代码， 完全看不懂scala","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537378,"discussion_content":"后面会整理的哈~ ","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1639045648,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":324301,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1638364597,"is_pvip":false,"replies":[{"id":"117761","content":"节点指的是服务器，就是一台计算机，Executors是JVM进程哈，一个节点，可以有多个Executors","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1638458659,"ip_address":"","comment_id":324301,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1638364597","product_id":100073401,"comment_content":"老师问下 节点 和 executor 是一个概念吗？都是物理上的一台计算机？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535541,"discussion_content":"节点指的是服务器，就是一台计算机，Executors是JVM进程哈，一个节点，可以有多个Executors","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638458659,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535713,"discussion_content":"谢谢老师 那一个 task 是占用一个cpu吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638522984,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":324056,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1638272816,"is_pvip":false,"replies":[{"id":"117608","content":"是的，没错~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1638285390,"ip_address":"","comment_id":324056,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1638272816","product_id":100073401,"comment_content":"老师问下这里 “Spark 调度系统的核心职责是，先将用户构建的 DAG 转化为分布式任务” 这里是不是包含了两个过程？首先根据 action 算子把用户写的 application 转化为一系列 jobs，然后根据 shuffle 把每个 job 转化为一系列 stages(分布式任务)?","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":534855,"discussion_content":"是的，没错~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638285390,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320833,"user_name":"xujunwei","can_delete":false,"product_type":"c1","uid":1269727,"ip_address":"","ucode":"E129638FD1481F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq6dppX1EPFeNOiaYOEClwPrO71zlLoprY0YffxSIk7773sjo4CE5juo0ppIzgfJ7Y5eUbwYl1SkNg/132","comment_is_top":false,"comment_ctime":1636526349,"is_pvip":true,"replies":[{"id":"116378","content":"文件还是在Driver读取，字典也是在Driver端创建。partFunc这个函数对象，会包含有字典，这样一来，当partFunc被分发到Executors的时候，想要根据字符串来查索引，只需用字符串调用partFunc就好了，不需要再临时读文件、创建字典，省去了很多开销","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1636555699,"ip_address":"","comment_id":320833,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636526349","product_id":100073401,"comment_content":"吴老师你好？ 我有一个问题可以帮忙解答一下吗？ <br>第二种方式讲到以下调用，这样文件是在driver上读取还是在executor上读取？<br>&#47;&#47;Dataset中的函数调用<br>partFunc(&quot;体育-篮球-NBA-湖人&quot;)","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530135,"discussion_content":"文件还是在Driver读取，字典也是在Driver端创建。partFunc这个函数对象，会包含有字典，这样一来，当partFunc被分发到Executors的时候，想要根据字符串来查索引，只需用字符串调用partFunc就好了，不需要再临时读文件、创建字典，省去了很多开销","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636555699,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":318678,"user_name":"阳台","can_delete":false,"product_type":"c1","uid":2340596,"ip_address":"","ucode":"CD76C0917139AF","user_header":"","comment_is_top":false,"comment_ctime":1635389093,"is_pvip":false,"replies":[{"id":"115573","content":"有的，那就得启用FAIR调度机制，FAIR机制的细节，咱们课程里没有展开，老弟可以参考Spark官网FAIR机制给出的例子就行。<br><br>大概步骤是：<br>1）定义资源队列，给不同队列设置权重和优先级<br>2）在应用的不同Job中，指定哪些Job属于哪些队列<br><br>然后就可以了，Spark在运行时，会选择不同的资源队列，并行地执行可以并行执行（不存在依赖关系）的Job。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635425380,"ip_address":"","comment_id":318678,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1635389093","product_id":100073401,"comment_content":"老师，spark可以并行提交job吗？比如，现在一个应用有4个job,后三个依赖第一个job的结果，能不能让第一个job结束后，后三个job并行提交到集群里面执行。而不是顺序执行每一个job?<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529342,"discussion_content":"有的，那就得启用FAIR调度机制，FAIR机制的细节，咱们课程里没有展开，老弟可以参考Spark官网FAIR机制给出的例子就行。\n\n大概步骤是：\n1）定义资源队列，给不同队列设置权重和优先级\n2）在应用的不同Job中，指定哪些Job属于哪些队列\n\n然后就可以了，Spark在运行时，会选择不同的资源队列，并行地执行可以并行执行（不存在依赖关系）的Job。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635425380,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2328830,"avatar":"https://static001.geekbang.org/account/avatar/00/23/88/fe/157a2d11.jpg","nickname":"李悦城","note":"","ucode":"F72974986FA015","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585517,"discussion_content":"是并行执行多个job，因为DAGScheduler是调度剩下的没有父节点的Stage，形象点说，就是每次剥离叶子","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661648612,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313664,"user_name":"陈子","can_delete":false,"product_type":"c1","uid":1943439,"ip_address":"","ucode":"CDC23530B6235A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/a7/8f/0d8e6d34.jpg","comment_is_top":false,"comment_ctime":1632585574,"is_pvip":false,"replies":[{"id":"113616","content":"好问题，如果是hdfs，spark通过namenode获取元数据。其他的分布式存储，基本都做不到“代码动”。因为存储和计算是分离的，比如s3","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632656508,"ip_address":"","comment_id":313664,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1632585574","product_id":100073401,"comment_content":"老师，我是 Spark 新手，有个问题请教。对于在 HDFS 上的数据，是由多个 block 组成的，这些 block 及其副本散落在多个节点上，那么 SchedulerBackend 又是如何知道将哪个 Task 调度到哪个节点上来实现 Process Local的呢？对于其他非 HDFS 分布式存储上面的数据，Spark 也可以做到移动计算吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527431,"discussion_content":"好问题，如果是hdfs，spark通过namenode获取元数据。其他的分布式存储，基本都做不到“代码动”。因为存储和计算是分离的，比如s3","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632656508,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":309309,"user_name":"黄宪坤","can_delete":false,"product_type":"c1","uid":2541053,"ip_address":"","ucode":"8D0453A30A3771","user_header":"https://static001.geekbang.org/account/avatar/00/26/c5/fd/bb7fd00b.jpg","comment_is_top":false,"comment_ctime":1630040753,"is_pvip":false,"replies":[{"id":"112506","content":"这个yarn的调度模式之一，Spark并没有Capacity Scheduler这种调度器哈~<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630765340,"ip_address":"","comment_id":309309,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1630040753","product_id":100073401,"comment_content":"Capacity Scheduler会用到吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525821,"discussion_content":"这个yarn的调度模式之一，Spark并没有Capacity Scheduler这种调度器哈~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630765340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":306199,"user_name":"果子","can_delete":false,"product_type":"c1","uid":1051904,"ip_address":"","ucode":"0A669B6DE26F21","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","comment_is_top":false,"comment_ctime":1628431320,"is_pvip":false,"replies":[{"id":"111183","content":"你的理解没有任何问题，在大多数的Spark应用中，在计算流程上，我们的业务逻辑往往是串行的DAG，这个时候，大部分情况下，Stages之间都是前后串行的依赖关系。这个时候，是FIFO还是FAIR，没有任何区别。<br><br>FIFO、FAIR的主要作用，是给那些有多个Job的应用准备的。对于一些较为复杂、或者说逻辑上比较绕的应用来说，同一个应用，会有多个不同的、地位平等的Job。换句话说，一个DAG上，有多个并列、平行的计算分支。在这种情况下，平行分支上的Stages之间，是不存在依赖关系的，所谓的FIFO、FAIR，是这个时候在起作用。也就是说，是先来的分支先serve，还是每个分支都有公平的机会获取到计算资源，主要是这么个区别。<br><br>那么问题来，平时开发的时候，有哪些场景会需要这种多个Job的应用呢。我能想到的，机器学习的样本工程是一个场景。<br><br>在做特征工程的时候，我们往往需要尝试不同的方法来生成训练样本，比如，组合特征，或是生成特征，等等。对于不同的特征工程，我们想要看他们各自的效果，就得生成多样的训练样本。这个时候，同一份数据，就可能有多个处理分支，每个分支最终生成一份训练样本，用于后续的模型训练。这是一个场景。<br><br>再者，在数仓场景下，同一份源数据，多个不同用户反复查询，就会触发多个Actions&#47;Jobs，这个时候，也涉及到先serve谁、后serve谁的问题，是先到先得，还是FAIR。<br><br>等等，诸如此类，你不妨想想，日常的开发中还有没有其他类似的场景。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1628836564,"ip_address":"","comment_id":306199,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1628431320","product_id":100073401,"comment_content":"吴老师，有个不太理解的地方，stage之间的调度是根据Yarn中的FIFO或FAIRE调度器来调度的，但是stage之间的调度本身就是有顺序的啊，比如ResultStage要依赖于ShuffMapStage，只有ShuffMapStage执行完了才能执行ResultStage，他们两者之间本身就存在依赖关系，串行执行的，为什么还需要调度器呢指定调度规则呢，谁先调度不是本身就规定好的吗？这块我是哪里理解错了吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524663,"discussion_content":"你的理解没有任何问题，在大多数的Spark应用中，在计算流程上，我们的业务逻辑往往是串行的DAG，这个时候，大部分情况下，Stages之间都是前后串行的依赖关系。这个时候，是FIFO还是FAIR，没有任何区别。\n\nFIFO、FAIR的主要作用，是给那些有多个Job的应用准备的。对于一些较为复杂、或者说逻辑上比较绕的应用来说，同一个应用，会有多个不同的、地位平等的Job。换句话说，一个DAG上，有多个并列、平行的计算分支。在这种情况下，平行分支上的Stages之间，是不存在依赖关系的，所谓的FIFO、FAIR，是这个时候在起作用。也就是说，是先来的分支先serve，还是每个分支都有公平的机会获取到计算资源，主要是这么个区别。\n\n那么问题来，平时开发的时候，有哪些场景会需要这种多个Job的应用呢。我能想到的，机器学习的样本工程是一个场景。\n\n在做特征工程的时候，我们往往需要尝试不同的方法来生成训练样本，比如，组合特征，或是生成特征，等等。对于不同的特征工程，我们想要看他们各自的效果，就得生成多样的训练样本。这个时候，同一份数据，就可能有多个处理分支，每个分支最终生成一份训练样本，用于后续的模型训练。这是一个场景。\n\n再者，在数仓场景下，同一份源数据，多个不同用户反复查询，就会触发多个Actions/Jobs，这个时候，也涉及到先serve谁、后serve谁的问题，是先到先得，还是FAIR。\n\n等等，诸如此类，你不妨想想，日常的开发中还有没有其他类似的场景。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1628836564,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390543,"discussion_content":"纠正一个问题，spark job的调度不是由yarn来决定了，yarn只是用来调度提交给它的不同应用，而spark job的调度由它自己的调度器支配，也就是yarn先分配资源给应用，然后应用里的任务如何调度取决于它自己的决定，这就是所谓的双层调度模型","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1629882257,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":302649,"user_name":"sparkjoy","can_delete":false,"product_type":"c1","uid":1000179,"ip_address":"","ucode":"827B3116EED3B8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/f3/e945e4ac.jpg","comment_is_top":false,"comment_ctime":1626316269,"is_pvip":false,"replies":[{"id":"109763","content":"这个问题可以参考给“慢慢卢”的回复哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1626691614,"ip_address":"","comment_id":302649,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1626316269","product_id":100073401,"comment_content":"老师，executordata里为什么不维护内存总量和可用内存呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523372,"discussion_content":"这个问题可以参考给“慢慢卢”的回复哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626691614,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301634,"user_name":"hadoop_admin","can_delete":false,"product_type":"c1","uid":1300466,"ip_address":"","ucode":"7EAFA775505C7E","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJGMphabeneYXs9otkAtr67RvxJClDa7jPe7w8yExg4YaS2FGJruDKMj5yN1E90o6MFibnicH8gM0ibg/132","comment_is_top":false,"comment_ctime":1625794573,"is_pvip":false,"replies":[{"id":"109188","content":"是的，没错，理解得非常到位~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1625802449,"ip_address":"","comment_id":301634,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1625794573","product_id":100073401,"comment_content":"这个spark应用的资源申请是在任务提交的时候，通过其他的资源管理系统分配的，相当于每个spark应用都有自己虚拟集群。用完之后就回收了。而spark的任务调度都是在自己的虚拟集群之上做的。其实在目前的计算和存储分离的情况下，很多数据的本地性性能优势都不存在了。。。。不知道理解的是否正确","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523049,"discussion_content":"是的，没错，理解得非常到位~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625802449,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":298868,"user_name":"魔法海","can_delete":false,"product_type":"c1","uid":2015537,"ip_address":"","ucode":"ACC78501A9980F","user_header":"https://static001.geekbang.org/account/avatar/00/1e/c1/31/e991c364.jpg","comment_is_top":false,"comment_ctime":1624350970,"is_pvip":false,"replies":[{"id":"108450","content":"把读取文件的代码放在外面也可以哈，不过这是另外一种调优思路了，就是单独读取文件内容，然后配合广播变量，把从文件创建好的字典，以广播变量的形式分发到Executors，Executors直接读取广播变量的内容。<br><br>相反，如果不配合广播，仅仅是把读取文件的代码拎出来，是不行的哈~ 不妨想想这是为什么？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624434036,"ip_address":"","comment_id":298868,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1624350970","product_id":100073401,"comment_content":"直观的看了下代码，感觉方式一直接把读取的代码放到函数外面就好了，和spark本身的优化好像没什么关系？是不是没看明白我","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522264,"discussion_content":"把读取文件的代码放在外面也可以哈，不过这是另外一种调优思路了，就是单独读取文件内容，然后配合广播变量，把从文件创建好的字典，以广播变量的形式分发到Executors，Executors直接读取广播变量的内容。\n\n相反，如果不配合广播，仅仅是把读取文件的代码拎出来，是不行的哈~ 不妨想想这是为什么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624434036,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291530,"user_name":"xuchuan","can_delete":false,"product_type":"c1","uid":1954690,"ip_address":"","ucode":"3967199AA078C7","user_header":"https://static001.geekbang.org/account/avatar/00/1d/d3/82/c3e57eb3.jpg","comment_is_top":false,"comment_ctime":1620351243,"is_pvip":false,"replies":[{"id":"105618","content":"没错，正解~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620397044,"ip_address":"","comment_id":291530,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620351243","product_id":100073401,"comment_content":"如果在对象存储，如s3那么就是计算存储分离的典型场景nodelocal不成立，要加载，或者当然一定程度利用对象存储的过滤能力。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519497,"discussion_content":"没错，正解~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620397044,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291092,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620018895,"is_pvip":false,"replies":[{"id":"105487","content":"两道题都答得很好~<br><br>1. 补充一下，“计算过程的中间结果数据”，这个有两种情况，一个是分布式数据缓存（内存&amp;磁盘），一个是Shuffle中间文件（磁盘）。对于这些中间结果，也是按照你前面说的Partition所在位置来决定的。比如内存中分片，计算他们的Tasks，就可以享受到PROCESS_LOCAL，或是NODE_LOCAL，这取决于缓存是在内存还是磁盘。对于Shuffle中间文件，那就爱莫能助了，至少是RACK_LOCAL。<br><br>2. 没错，就是看计算和存储是否分离~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620118171,"ip_address":"","comment_id":291092,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620018895","product_id":100073401,"comment_content":"1. Task的本地性是由Partition分区的本地性来决定，因为task和partition是一对一的关系。根据源码，翻译一下自己觉得比较通俗的理解（如有错误还望老师指正）就是：如果是读取数据源（如HDFS），那么会尽可能在哪些已启动Executor且存在要读取数据的节点上来启动Task；如果是计算过程的中间结果数据（有可能在内存，也有可能在磁盘），会尽可能地在有空闲资源并满足条件的Executor&#47;节点上启动Task<br>2. 第二个问题中的场景没怎么接触过，要看存储环境和计算环境是不是相同的宿主机吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519369,"discussion_content":"两道题都答得很好~\n\n1. 补充一下，“计算过程的中间结果数据”，这个有两种情况，一个是分布式数据缓存（内存&amp;amp;磁盘），一个是Shuffle中间文件（磁盘）。对于这些中间结果，也是按照你前面说的Partition所在位置来决定的。比如内存中分片，计算他们的Tasks，就可以享受到PROCESS_LOCAL，或是NODE_LOCAL，这取决于缓存是在内存还是磁盘。对于Shuffle中间文件，那就爱莫能助了，至少是RACK_LOCAL。\n\n2. 没错，就是看计算和存储是否分离~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620118171,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287421,"user_name":"pyg","can_delete":false,"product_type":"c1","uid":1699503,"ip_address":"","ucode":"2E95F019818B73","user_header":"https://static001.geekbang.org/account/avatar/00/19/ee/af/c667876f.jpg","comment_is_top":false,"comment_ctime":1617939456,"is_pvip":false,"replies":[{"id":"104394","content":"对，就是task要分发到离数据最近的地方～ 每个task都会有preferred Location，而且级别不同，依次是process，node，rack和any，优先选择最优的process，满足不了就退而求其次～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617953480,"ip_address":"","comment_id":287421,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617939456","product_id":100073401,"comment_content":"吴老师，不知我理解得对不对，回顾一下RDD的prefered location，它是指RDD的数据尽可能的靠近数据源，比如初始数据源、父RDD位置；本章讲的调度系统，与RDD中的prefered location和compute有关，创建RDD时离数据源近，compute代码执行的位置尽可能的靠近数据源RDD","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518306,"discussion_content":"对，就是task要分发到离数据最近的地方～ 每个task都会有preferred Location，而且级别不同，依次是process，node，rack和any，优先选择最优的process，满足不了就退而求其次～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617953480,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287054,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1617761038,"is_pvip":false,"replies":[{"id":"104244","content":"是的，没错～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617777491,"ip_address":"","comment_id":287054,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617761038","product_id":100073401,"comment_content":"所以”数据不动代码动“仅仅说的是同个Stage的前提。如果不是一个Stage，必定有shffle，数据一定会动的。<br>这样子理解？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518175,"discussion_content":"是的，没错～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617777491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286373,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1617271755,"is_pvip":true,"replies":[{"id":"103930","content":"本地性是task粒度的哈，同一stage内部，不同task的本地性可能是不同的。再想想～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617276310,"ip_address":"","comment_id":286373,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1617271755","product_id":100073401,"comment_content":"Task的本地属性是由所属RDD的stage决定的。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517961,"discussion_content":"本地性是task粒度的哈，同一stage内部，不同task的本地性可能是不同的。再想想～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617276310,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1329566,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","nickname":"慢慢卢","note":"","ucode":"853D399100D83B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":378989,"discussion_content":"stage是一个逻辑概率，task是最终被调度到executor执行的单位，不同的task是理解为不同executor上执行的task吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623577258,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1329566,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132","nickname":"慢慢卢","note":"","ucode":"853D399100D83B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387315,"discussion_content":"说的对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628122438,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":378989,"ip_address":""},"score":387315,"extra":""}]}]},{"had_liked":false,"id":286159,"user_name":"fsc2016","can_delete":false,"product_type":"c1","uid":1255585,"ip_address":"","ucode":"5480F05703A974","user_header":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","comment_is_top":false,"comment_ctime":1617172570,"is_pvip":false,"replies":[{"id":"103900","content":"挺有意思的场景，其实这里没调度系统什么事，主要是数据倾斜的问题。这里的关键是构造by用户的物品转移序列样本。有了样本其实就可以拿来训练了。但是这里有data skew的风险，因为不同用户购买行为不一样，有的序列短，有的序列就会特别长，因此会skew。<br><br>解决思路分类讨论吧，分加盐还是不加盐。<br><br>不加盐，用window over user by 时间排序就好了，为了应对倾斜，要么把时间窗口缩短；要么把倾斜的user但拿出来处理，分而治之。<br><br>加盐，这个比较复杂，就是在user上加随机数，先shuffle把数据打散，然后各自构建转移序列，但最后还要merge回来、去盐化。merge的时候可以用join，然后把各自排序的时间序，用sort merge重新merge成一个序列（by user）。当然这里你可能需要定义udf来专门干merge这个事，除了物品还要带上时间，因为你要按照时间顺序去merge。虽然复杂，但不失为一种办法。<br><br>哪种方法最好，还有看你的数据分布，欢迎继续讨论，另外有了结果，不妨share出来哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617240734,"ip_address":"","comment_id":286159,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1617172570","product_id":100073401,"comment_content":"老师，请教一个问题，在Graph Embedding中，基于用户行为的构造的物品概率转移矩阵，进行随机游走构造用户行为序列中，这个物品概率转移矩阵，如果物品维度很大，那么在driver和分发到executer端很容易oom，spark怎么解决类似这样问题的，想听听老师的想法！","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517888,"discussion_content":"挺有意思的场景，其实这里没调度系统什么事，主要是数据倾斜的问题。这里的关键是构造by用户的物品转移序列样本。有了样本其实就可以拿来训练了。但是这里有data skew的风险，因为不同用户购买行为不一样，有的序列短，有的序列就会特别长，因此会skew。\n\n解决思路分类讨论吧，分加盐还是不加盐。\n\n不加盐，用window over user by 时间排序就好了，为了应对倾斜，要么把时间窗口缩短；要么把倾斜的user但拿出来处理，分而治之。\n\n加盐，这个比较复杂，就是在user上加随机数，先shuffle把数据打散，然后各自构建转移序列，但最后还要merge回来、去盐化。merge的时候可以用join，然后把各自排序的时间序，用sort merge重新merge成一个序列（by user）。当然这里你可能需要定义udf来专门干merge这个事，除了物品还要带上时间，因为你要按照时间顺序去merge。虽然复杂，但不失为一种办法。\n\n哪种方法最好，还有看你的数据分布，欢迎继续讨论，另外有了结果，不妨share出来哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617240734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363655,"discussion_content":"别客气，确实很好的场景，有了发现分享出来哈~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617254046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1255585,"avatar":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","nickname":"fsc2016","note":"","ucode":"5480F05703A974","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363613,"discussion_content":"谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617245315,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285322,"user_name":"坤2021牛","can_delete":false,"product_type":"c1","uid":1676097,"ip_address":"","ucode":"A26B321F300785","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/cKeYVTKJCJhrfTCBkEGla1WA7W0S9FPZrTR3ovYJFhcKo7kl72gR9VibCufhHsjOar2Z6mZlFKb8VEkkDv9lqVA/132","comment_is_top":false,"comment_ctime":1616740150,"is_pvip":false,"replies":[{"id":"104439","content":"有收获就好哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618058025,"ip_address":"","comment_id":285322,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616740150","product_id":100073401,"comment_content":"收获很大，以前这些地方理解的不细，现在一下都清晰了。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517644,"discussion_content":"有收获就好哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618058025,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284995,"user_name":"October","can_delete":false,"product_type":"c1","uid":1137879,"ip_address":"","ucode":"CEDA78F4A5F8B1","user_header":"https://static001.geekbang.org/account/avatar/00/11/5c/d7/e4673fde.jpg","comment_is_top":false,"comment_ctime":1616573606,"is_pvip":false,"replies":[{"id":"103372","content":"好问题，有一点需要特别注意，spark的资源调度和任务调度是分开的哈。也就是说，不论你用standalone、yarn还是mesos，spark在申请硬件资源的时候，以cpu、memory量化申请executors的过程，是先于任务调度的。用你的话说，executors已经提前申请好了，申请executors的时候，只看cpu和memory是否满足要求，不会考虑locality这些与任务有关的细节。咱们这一讲，focus在任务调度的讲解，其实在任务调度之前，资源调度就已经完成了。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616585338,"ip_address":"","comment_id":284995,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616573606","product_id":100073401,"comment_content":"老师，taskscheduler的调度是把taskset根据一定的调度算法分配给不同的executor，那么我有一个问题，executor资源的申请是在任务的三级调度之前就已经申请好了吗？如果是这样的话，executor的申请时，有没有考虑优先在数据所在的节点开启executor?","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517544,"discussion_content":"好问题，有一点需要特别注意，spark的资源调度和任务调度是分开的哈。也就是说，不论你用standalone、yarn还是mesos，spark在申请硬件资源的时候，以cpu、memory量化申请executors的过程，是先于任务调度的。用你的话说，executors已经提前申请好了，申请executors的时候，只看cpu和memory是否满足要求，不会考虑locality这些与任务有关的细节。咱们这一讲，focus在任务调度的讲解，其实在任务调度之前，资源调度就已经完成了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616585338,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284990,"user_name":"October","can_delete":false,"product_type":"c1","uid":1137879,"ip_address":"","ucode":"CEDA78F4A5F8B1","user_header":"https://static001.geekbang.org/account/avatar/00/11/5c/d7/e4673fde.jpg","comment_is_top":false,"comment_ctime":1616572849,"is_pvip":false,"replies":[{"id":"103374","content":"语言不重要哈，只要能get到调度系统的工作原理、调度开销，能理解不同实现方式在运行时的计算过程、识别分布式执行效率最高的实现方式，就达到目的了哈～ 至于说是oo、还是fp，不重要。黑猫白猫，能抓耗子就是好猫～ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616585887,"ip_address":"","comment_id":284990,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616572849","product_id":100073401,"comment_content":"两种实现方式都能理解，但是现阶段很难写出像老师这样的非常scala风格的代码。  如果是我的话，实现方式可能会定义一个类，这个类包含这个字典属性，在driver端都文件，完成字典属性的初始化，在exexutor查字典。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517540,"discussion_content":"语言不重要哈，只要能get到调度系统的工作原理、调度开销，能理解不同实现方式在运行时的计算过程、识别分布式执行效率最高的实现方式，就达到目的了哈～ 至于说是oo、还是fp，不重要。黑猫白猫，能抓耗子就是好猫～ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616585887,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284954,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1616559356,"is_pvip":false,"replies":[{"id":"103376","content":"对，物理分离的话，都不成立～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616586094,"ip_address":"","comment_id":284954,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616559356","product_id":100073401,"comment_content":"老师，对于第二点，如果计算和存储分离的情况，我理解的是：对于shuffleMapStage来说，node local不成立；对于resultStage来说，从外部数据源读或者从shuffle数据读，node local也都不成立","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517522,"discussion_content":"对，物理分离的话，都不成立～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616586094,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284924,"user_name":"毓秀靚泽","can_delete":false,"product_type":"c1","uid":2104781,"ip_address":"","ucode":"898FB1E2CBE73C","user_header":"https://static001.geekbang.org/account/avatar/00/20/1d/cd/83264741.jpg","comment_is_top":false,"comment_ctime":1616548769,"is_pvip":false,"replies":[{"id":"104440","content":"有收获就好哈～ 多参与讨论～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618058047,"ip_address":"","comment_id":284924,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616548769","product_id":100073401,"comment_content":"收获颇大，以前没往这块儿想","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517512,"discussion_content":"有收获就好哈～ 多参与讨论～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618058047,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}