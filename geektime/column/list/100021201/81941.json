{"id":81941,"title":"27 | 决策树：信息增益、增益比率和基尼指数的运用","content":"<p>你好，我是黄申。</p><p>上一节，我通过问卷调查的案例，给你解释了信息熵和信息增益的概念。被测者们每次回答一道问题，就会被细分到不同的集合，每个细分的集合纯净度就会提高，而熵就会下降。在测试结束的时候，如果所有被测者都被分配到了相应的武侠人物名下，那么每个人物分组都是最纯净的，熵值都为0。于是，测试问卷的过程就转化为“如何将熵从3.32下降到0”的过程。</p><p>由于每道问题的区分能力不同，而我们对问题的选择会影响熵下降的幅度。这个幅度就是信息增益。如果问卷题的顺序选择得好，我们可以更快速地完成对用户性格的判定。这一节我们就继续这个话题，看看如何获得一个更简短的问卷设计，把这个核心思想推广到更为普遍的决策树分类算法中。</p><h2>如何通过信息熵挑选合适的问题？</h2><p>为了实现一个更简短的问卷，你也许很自然地就想到，每次选择问题的时候，我们可以选择信息增益最高的问题，这样熵值下降得就最快。这的确是个很好的方法。我们来试一试。</p><p>我们现在开始选择第一个问题。首先，依次计算“性别”“智商”“情商”“侠义”和“个性”对人物进行划分后的信息增益。我们得到如下结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/11/9f135e031841f15012ed997a1dd30a11.png?wh=1276*156\" alt=\"\"></p><p>显然，第一步我们会选择“侠义”，之后用户就会被细分为3组。</p><p><img src=\"https://static001.geekbang.org/resource/image/7c/42/7cd2a03c7b5fd5f6b94e69c89b70c142.png?wh=1278*446\" alt=\"\"><img src=\"https://static001.geekbang.org/resource/image/a1/b8/a12483f003569c79899c143d28c332b8.png?wh=1272*400\" alt=\"\"><img src=\"https://static001.geekbang.org/resource/image/09/10/0957e7645a48a21e9409886963270b10.png?wh=1274*388\" alt=\"\"></p><p>针对第一组，我们继续选择在当前这组中，区分力最强、也是信息增益最高的问题。根据计算的结果我们应该选择有关“性别”的问题，然后进一步地细分。后续的步骤依次类推，直到所有人物都被分开，对于第二组和第三组我们也进行同样地操作。整个过程稍微有点复杂，为了帮你理解，我把它画成了一个图。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/9d/1c/9db1ccd4fd4aa2cd03d74f02811abb1c.png?wh=1666*850\" alt=\"\"></p><p>从这个图可以看出来，对于每种人物的判断，我们至多需要问3个问题，没有必要问全5个问题。比如，对于人物J和C，我们只需要问2个问题。假设读者属于10种武侠人物的概率是均等的，那么我们就可以利用之前介绍的知识，来计算读者需要回答的问题数量之期望值。每种人物出现的概率是0.1，8种人物需要问3个问题，2种人物需要问2个问题，那么回答问题数的期望值是0.8 * 3 + 0.2 * 2 = 2.8（题）。</p><p>如果我们每次不选熵值最高的问题，而选择熵值最低的问题呢？</p><p>我计算了一下，最差的情况下，我们要问完全部5个问题，才能确定被测者所对应的武侠人物。而且问4个问题的情况也不少，回答问题数的期望值会在4到5之间，明显要多于基于最高熵来选择题目的方法。当然，如果测试的目标和问题很多，基于熵的问题选择其运算量就会比较大，我们就可以通过编程来自动化整个过程，最终达到优化问卷设计的目的。</p><p>好了，现在我们总结一下，如何才能进行高效的问卷调查。最核心的思想是，根据当前的概率分布，挑选在当前阶段区分能力更强的那些问题。具体的步骤有三个。</p><p>第一步，根据分组中的人物类型，为每个集合计算信息熵，并通过全部集合的熵值加权平均，获得整个数据集的熵。注意，一开始集合只有一个，并且包含了所有的武侠人物。</p><p>第二步，根据信息增益，计算每个问卷题的区分能力。挑选区分能力最强的题目，并对每个集合进行更细的划分。</p><p>第三步，有了新的划分之后，回到第一步，重复第一和第二步，直到没有更多的问卷题，或者所有的人物类型都已经被区分开来。这一步也体现了递归的思想。</p><p>其实，上述这个过程就体现了训练<strong>决策树</strong>（Decision Tree）的基本思想。决策树学习属于归纳推理算法之一，适用于分类问题。在前面介绍朴素贝叶斯的时候，我说过，分类算法主要包括了建立模型和分类新数据两个阶段。决定问卷题出现顺序的这个过程，其实就是建立决策树模型的过程。</p><p>你可以看到，整个构建出来的图就是一个树状结构，这也是“决策树”这个名字的由来。而根据用户对每个问题的答案，从决策树的根节点走到叶子节点，最后来判断其属于何种人物类型，这个过程就是分类新数据的过程。</p><p>让我们把问卷案例泛化一下，将武侠人物的类型变为机器学习中的训练样本，将问卷中的题目变为机器学习中的特征，那么问卷调查的步骤就可以泛化为决策树构建树的步骤。</p><p>第一步，根据集合中的样本分类，为每个集合计算信息熵，并通过全部集合的熵值加权平均，获得整个数据集的熵。注意，一开始集合只有一个，并且包含了所有的样本。</p><p>第二步，根据信息增益，计算每个特征的区分能力。挑选区分能力最强的特征，并对每个集合进行更细的划分。</p><p>第三步，有了新的划分之后，回到第一步，重复第一步和第二步，直到没有更多的特征，或者所有的样本都已经被分好类。</p><p>有一点需要注意的是，问卷案例中的每类武侠人物都只有一个样本，而在泛化的机器学习问题中，每个类型对应了多个样本。也就是说，我们可以有很多个郭靖，而且每个人的属性并不完全一致，但是它们的分类都是“郭靖”。正是因为这个原因，决策树通常都只能把整体的熵降低到一个比较低的值，而无法完全降到0。这也意味着，训练得到的决策树模型，常常无法完全准确地划分训练样本，只能求到一个近似的解。</p><h2>几种决策树算法的异同</h2><p>随着机器学习的快速发展，人们也提出了不少优化版的决策树。采用信息增益来构建决策树的算法被称为<a href=\"https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95\"><strong>ID3</strong></a>（Iterative Dichotomiser 3，迭代二叉树3代）。但是这个算法有一个缺点，它一般会优先考虑具有较多取值的特征，因为取值多的特征会有相对较大的信息增益。这是为什么呢？</p><p>你仔细观察一下信息熵的定义，就能发现背后的原因。更多的取值会把数据样本划分为更多更小的分组，这样熵就会大幅降低，信息增益就会大幅上升。但是这样构建出来的树，很容易导致机器学习中的过拟合现象，不利于决策树对新数据的预测。为了克服这个问题，人们又提出了一个改进版，<a href=\"https://zh.wikipedia.org/wiki/C4.5%E7%AE%97%E6%B3%95\">C4.5算法</a>。</p><p>这个算法使用<strong>信息增益率</strong>（Information Gain Ratio）来替代信息增益，作为选择特征的标准，并降低决策树过拟合的程度。信息增益率通过引入一个被称作<strong>分裂信息</strong>（Split Information）的项来惩罚取值较多的特征，我把相应的公式给你列出来了。</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/a2/1d2d93ed55bfe09f256a9b72ca6c88a2.png?wh=884*178\" alt=\"\"></p><p>其中，训练数据集$P$通过属性$T$的属性值，划分为$n$个子数据集，$|Pi|$表示第$i$个子数据集中样本的数量，$|P|$表示划分之前数据集中样本总数量。 这个公式看上去和熵很类似，其实并不相同。</p><p>熵计算的时候考虑的是，集合内数据是否属于同一个类，因此即使集合数量很多，但是集合内的数据如果都是来自相同的分类（或分组），那么熵还是会很低。而这里的分裂信息是不同的，它只考虑子集的数量。如果某个特征取值很多，那么相对应的子集数量就越多，最终分裂信息的值就会越大。正是因为如此，人们可以使用分裂信息来惩罚取值很多的特征。具体的计算公式如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/01/c4/01f28b759dae3e5fa7139535984eb6c4.png?wh=798*174\" alt=\"\"></p><p>其中$Gain(P,T)$是数据集$P$使用特征$T$之后的信息增益，$GainRatio(P,T)$是数据集$P$使用特征$T$之后的信息增益率。</p><p>另一种常见的决策树<strong>是CART算法</strong>（Classification and Regression Trees，分类与回归树）。这种算法和ID3、C4.5相比，主要有两处不同：</p><ul>\n<li>\n<p>在分类时，CART不再采用信息增益或信息增益率，而是采用基尼指数（Gini）来选择最好的特征并进行数据的划分；</p>\n</li>\n<li>\n<p>在ID3和C4.5决策树中，算法根据特征的属性值划分数据，可能会划分出多个组。而CART算法采用了二叉树，每次把数据切成两份，分别进入左子树、右子树。</p>\n</li>\n</ul><p>当然，CART算法和ID3、C4.5也有类似的地方。首先，CART中每一次迭代都会降低基尼指数，这类似于ID3、C4.5降低信息熵的过程。另外，基尼指数描述的也是纯度，与信息熵的含义相似。我们可以用下面这个公式来计算每个集合的纯度。</p><p><img src=\"https://static001.geekbang.org/resource/image/99/69/99ac2d02888b882c6a411316b037d369.png?wh=406*152\" alt=\"\"></p><p>其中，$n$为集合$P$中所包含的不同分组（或分类）数量。如果集合$P$中所包含的不同分组越多，那么这个集合的基尼指数越高，纯度越低。</p><p>然后，我们需要计算整个数据集的基尼指数。</p><p><img src=\"https://static001.geekbang.org/resource/image/de/3f/de267e07a31038e848d4396a45ccf23f.png?wh=556*162\" alt=\"\"></p><p>其中，$m$为全集使用特征$T$划分后，所形成的子集数量。$P_{j}$为第$j$个集合。</p><p>无论是何种决策树算法，来自信息论的几个重要概念：信息熵、信息增益、信息增益率、基尼指数都起到了重要的作用。如果你能很好的学习并运用这些概念，那么决策树这种类型的算法就不难理解了。</p><h2>总结</h2><p>通过这两节的介绍，我想你对信息熵、信息增益、基尼指数等信息论的概念，以及基于这些概念的决策树分类算法应该有了一定了解。决策树算法的优势在于，容易理解和实现。此外，对于通过样本训练所得的树结构，其每个结点都是基于某个数据特征的判定，对于我们的阅读和解释来说都是很方便的。</p><p>当然，决策树也有不足。之前我已经提到，这类算法受训练样本的影响很大，比较容易过拟合。在预测阶段，如果新的数据和原来的训练样本差异较大，那么分类效果就会比较差。为此人们也提出了一些优化方案，比如剪枝和随机森林。如果感兴趣，你可以自己去研究一下。</p><h2>思考题</h2><p>刚刚我提到了，如果每次都选择使得信息增益最小的问题，那么构建出来的答题路径就相对冗长。你可以自己动手计算一下用户要回答问题数的期望。</p><p><span class=\"orange\">欢迎留言和我分享，也欢迎你在留言区写下今天的学习笔记。如果你有朋友对决策树感兴趣，你可以点击“请朋友读”，把今天的内容分享给他，说不定就帮他解决一个问题。</span></p>","comments":[{"had_liked":false,"id":72914,"user_name":"Peng","can_delete":false,"product_type":"c1","uid":1108103,"ip_address":"","ucode":"D15A78DD1AC18C","user_header":"https://static001.geekbang.org/account/avatar/00/10/e8/87/89561ed0.jpg","comment_is_top":false,"comment_ctime":1551754278,"is_pvip":false,"replies":[{"id":"26571","content":"可以逐个理解，每次理解一点都是进步👍","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1551811392,"ip_address":"","comment_id":72914,"utype":1}],"discussion_count":2,"race_medal":0,"score":"44501427238","product_id":100021201,"comment_content":"开始看不懂了，我再多看几遍试试。","like_count":10,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441825,"discussion_content":"可以逐个理解，每次理解一点都是进步👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551811392,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1411096,"avatar":"https://static001.geekbang.org/account/avatar/00/15/88/18/9744d5ec.jpg","nickname":"小超人","note":"","ucode":"0D8A433F3E3737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394439,"discussion_content":"我也是 😭","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631883017,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":132241,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1568067064,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31632838136","product_id":100021201,"comment_content":"信息增益，新分类的引入导致熵的减少。<br>信息增益率，计算熵的时候还考虑了多个子项的数目。在计算分组后集合的熵，采用加权平均之后，还要除以，集合分组不同数目引入的熵。<br>Gini指数是一种新的计算混乱度的方法，熵是基于对数加权的，Gini指数是基于平方的相反数求和再加一。Gini指数求导以后是线性的，随着概率减少变化度比熵更敏感（熵的导数是对数），也就更惩罚小概率事件。<br><br>","like_count":7},{"had_liked":false,"id":131818,"user_name":"张九州","can_delete":false,"product_type":"c1","uid":1455679,"ip_address":"","ucode":"414C0CFC741B4F","user_header":"https://static001.geekbang.org/account/avatar/00/16/36/3f/95a9a40a.jpg","comment_is_top":false,"comment_ctime":1567917428,"is_pvip":false,"replies":[{"id":"50546","content":"pj表示第j组元素出现的概率，这里用在某个划分的组之中，第j组元素的数量除以这个划分的元素数量来计算","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1568054818,"ip_address":"","comment_id":131818,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14452819316","product_id":100021201,"comment_content":"计算整个数据集基尼指数，pj是什么 如何计算？","like_count":3,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466632,"discussion_content":"pj表示第j组元素出现的概率，这里用在某个划分的组之中，第j组元素的数量除以这个划分的元素数量来计算","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568054818,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":121567,"user_name":"lifes a Struggle","can_delete":false,"product_type":"c1","uid":1588303,"ip_address":"","ucode":"B19093752719BC","user_header":"https://static001.geekbang.org/account/avatar/00/18/3c/4f/979bdccc.jpg","comment_is_top":false,"comment_ctime":1565160261,"is_pvip":false,"replies":[{"id":"44722","content":"是的👍","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1565195815,"ip_address":"","comment_id":121567,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14450062149","product_id":100021201,"comment_content":"知道了，老师的案例中个体都是一个单独的分类，所有在原始集合中可以采用-n*(Pi*log(2,Pi))的形式进行信息熵的计算。如果存在分类的数据不均匀，通过各个分类的信息熵求和即可。","like_count":3,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":461855,"discussion_content":"是的👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565195815,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208152,"user_name":"罗耀龙@坐忘","can_delete":false,"product_type":"c1","uid":1917663,"ip_address":"","ucode":"3CEA258DE7F3C7","user_header":"https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg","comment_is_top":false,"comment_ctime":1587284900,"is_pvip":true,"discussion_count":0,"race_medal":5,"score":"10177219492","product_id":100021201,"comment_content":"茶艺师学编程<br><br>1、思考题：按照最少信息增益来划分，用户回答题目的期望是多少？<br><br>0.2*2+0.1*3+0.4*4+0.1*5+0.2*6=4<br><br>（换成人话，就是10个人，有两人要回答两题，有一人要回答三题，有四人要回答四题，有一人要回答五题，有两人要回答六题，他们的期望就是四题）<br><br>2、关于ID3和C4.5算法：<br><br>两者都是罗斯•昆兰（Ross Quinlan）的作品，ID3决策树诞生于1986年，在7年后（1993年）罗斯就推出了它的改进版C4.5算法，后者被称为数据挖掘十大算法之一。<br>","like_count":2},{"had_liked":false,"id":165408,"user_name":"总统老唐","can_delete":false,"product_type":"c1","uid":1490070,"ip_address":"","ucode":"F2CC66E5BB4871","user_header":"https://static001.geekbang.org/account/avatar/00/16/bc/96/c679bb3d.jpg","comment_is_top":false,"comment_ctime":1577234789,"is_pvip":false,"replies":[{"id":"65016","content":"这里主要是通用性，对于CART算法确实可以将m简写为2，对于其他使用基尼指数的算法仍然保持m","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1577840489,"ip_address":"","comment_id":165408,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10167169381","product_id":100021201,"comment_content":"既然每次都是分解成左右两个子树，为什么CART算法公式中的m不直接写成2","like_count":2,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":479150,"discussion_content":"这里主要是通用性，对于CART算法确实可以将m简写为2，对于其他使用基尼指数的算法仍然保持m","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577840489,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":74403,"user_name":"qinggeouye","can_delete":false,"product_type":"c1","uid":1251536,"ip_address":"","ucode":"5B53EEDD7BEC9C","user_header":"https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg","comment_is_top":false,"comment_ctime":1552217081,"is_pvip":false,"replies":[{"id":"27204","content":"因为决策树是一种分类算法，我们有训练样本告诉我们每个数据样本属于何种分类，所以这里的分类、分组都是根据训练样本中的分类标签。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1552235044,"ip_address":"","comment_id":74403,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10142151673","product_id":100021201,"comment_content":"某个特征 T 取值越多，数据集 P 划分时分组越多，划分后的「信息熵」越小，「信息增益」越大。「分裂信息」是为了解决某个特征 T 取值过多，造成机器学习过拟合，而引入的一种惩罚项，惩罚取值多的特征。<br><br>老师，「基尼指数」没怎么看明白，第一个式子中「n 为集合 P 中所包含的不同分组或分类的数量」该怎么理解？求和符号后面的 pi 是什么含义？谢谢～","like_count":2,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442562,"discussion_content":"因为决策树是一种分类算法，我们有训练样本告诉我们每个数据样本属于何种分类，所以这里的分类、分组都是根据训练样本中的分类标签。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1552235044,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":265897,"discussion_content":"pi是指分组i在集合P中的概率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589453117,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67595,"user_name":"Thinking","can_delete":false,"product_type":"c1","uid":1155754,"ip_address":"","ucode":"1AAD6AE9F6B678","user_header":"https://static001.geekbang.org/account/avatar/00/11/a2/aa/bf65e8be.jpg","comment_is_top":false,"comment_ctime":1550205653,"is_pvip":false,"replies":[{"id":"23976","content":"好的，后面我会考虑多从公式的角度出发","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1550250816,"ip_address":"","comment_id":67595,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10140140245","product_id":100021201,"comment_content":"建议老师每堂课后能配多几个具有代表性的，针对性的练习题辅助理解概念和公式。","like_count":2,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439269,"discussion_content":"好的，后面我会考虑多从公式的角度出发","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550250816,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":197368,"user_name":"灰太狼","can_delete":false,"product_type":"c1","uid":1420624,"ip_address":"","ucode":"4FB0501E6AFADB","user_header":"https://static001.geekbang.org/account/avatar/00/15/ad/50/3cb818e8.jpg","comment_is_top":false,"comment_ctime":1585372012,"is_pvip":false,"replies":[{"id":"75052","content":"是的👍","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1585630064,"ip_address":"","comment_id":197368,"utype":1}],"discussion_count":4,"race_medal":0,"score":"5880339308","product_id":100021201,"comment_content":"老师，基尼系数公式里的pi是指分组i在集合P中的概率吗？","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":489508,"discussion_content":"是的👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585630064,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":265895,"discussion_content":"那n个概率的平方之和，n越大平方之和越大，那么用1减的话，应该越小呀，老师说的越大，不太明白","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589452841,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1420624,"avatar":"https://static001.geekbang.org/account/avatar/00/15/ad/50/3cb818e8.jpg","nickname":"灰太狼","note":"","ucode":"4FB0501E6AFADB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":266274,"discussion_content":"n越大，对应的各个小数越小，平方和越小","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1589499424,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":265895,"ip_address":""},"score":266274,"extra":""},{"author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1420624,"avatar":"https://static001.geekbang.org/account/avatar/00/15/ad/50/3cb818e8.jpg","nickname":"灰太狼","note":"","ucode":"4FB0501E6AFADB","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":266608,"discussion_content":"哦哦，有道理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589540413,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":266274,"ip_address":""},"score":266608,"extra":""}]}]},{"had_liked":false,"id":150121,"user_name":"💢 星星💢","can_delete":false,"product_type":"c1","uid":1254392,"ip_address":"","ucode":"A402B765222C35","user_header":"https://static001.geekbang.org/account/avatar/00/13/23/f8/24fcccea.jpg","comment_is_top":false,"comment_ctime":1573464898,"is_pvip":false,"replies":[{"id":"57949","content":"你好，感谢支持，我暂时还没有时间在其他地方发表博文。如果有好的想法写作，当然首选极客时间专栏啦😆。正在和编辑筹划下一个专栏，期待与大家再次交流","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1573584779,"ip_address":"","comment_id":150121,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5868432194","product_id":100021201,"comment_content":"老师随机森林有没有好的文章推荐，另外老师有没有公众号，或者其他方式可以白嫖看您的文章呢，当然也期待老师出新的专栏，虽然这个专栏对于我来说已经是新的挑战。但是非常喜欢读老师的文章。","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474070,"discussion_content":"你好，感谢支持，我暂时还没有时间在其他地方发表博文。如果有好的想法写作，当然首选极客时间专栏啦😆。正在和编辑筹划下一个专栏，期待与大家再次交流","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573584779,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":69554,"user_name":"Joe","can_delete":false,"product_type":"c1","uid":1337998,"ip_address":"","ucode":"EC76699640B7BF","user_header":"https://static001.geekbang.org/account/avatar/00/14/6a/8e/7b6ea886.jpg","comment_is_top":false,"comment_ctime":1550764282,"is_pvip":false,"replies":[{"id":"24686","content":"你是指计算信息熵、信息增益和基尼指数？可以使用现成的机器学习包计算，如果希望自己计算也不难，遵循专栏中的公式就可以了。后面我有时间整理一下代码。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1550770045,"ip_address":"","comment_id":69554,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5845731578","product_id":100021201,"comment_content":"老师，请问有没有相关代码实现的方式，能否给出参考链接。","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440166,"discussion_content":"你是指计算信息熵、信息增益和基尼指数？可以使用现成的机器学习包计算，如果希望自己计算也不难，遵循专栏中的公式就可以了。后面我有时间整理一下代码。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550770045,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1474214,"avatar":"https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg","nickname":"骑行的掌柜J","note":"","ucode":"3163102651C653","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":281129,"discussion_content":"可以直接去scikit-learn官网查看相关决策树的代码 有示例","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591676336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":356464,"user_name":"013923","can_delete":false,"product_type":"c1","uid":3035193,"ip_address":"上海","ucode":"1214DAADBCA848","user_header":"","comment_is_top":false,"comment_ctime":1662346035,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1662346035","product_id":100021201,"comment_content":"学完一章，谢谢老师！","like_count":0},{"had_liked":false,"id":228453,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1592699361,"is_pvip":false,"replies":[{"id":"85061","content":"很好的实现","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1593403913,"ip_address":"","comment_id":228453,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1592699361","product_id":100021201,"comment_content":"根据信息增益划分的原理，简单写了一个python程序计算了一下问题期望数，程序输出结果如下：<br>选择信息增益小的问题对人物分类，问题数的期望值=3.7<br>选择信息增益大的问题对人物分类，问题数的期望值=2.8<br><br>程序代码如下：<br>import pandas as pd<br># 初始化10个武侠人物的属性<br>p = { &#39;性别&#39;:[&#39;男&#39;,&#39;男&#39;,&#39;男&#39;,&#39;男&#39;,&#39;男&#39;,&#39;女&#39;,&#39;女&#39;,&#39;女&#39;,&#39;女&#39;,&#39;女&#39;]<br>             ,&#39;智商&#39;:[&#39;高&#39;,&#39;高&#39;,&#39;高&#39;,&#39;高&#39;,&#39;中&#39;,&#39;高&#39;,&#39;高&#39;,&#39;高&#39;,&#39;高&#39;,&#39;中&#39;]<br>             ,&#39;情商&#39;:[&#39;高&#39;,&#39;高&#39;,&#39;中&#39;,&#39;中&#39;,&#39;高&#39;,&#39;高&#39;,&#39;中&#39;,&#39;中&#39;,&#39;中&#39;,&#39;中&#39;]<br>             ,&#39;侠义&#39;:[&#39;高&#39;,&#39;中&#39;,&#39;低&#39;,&#39;中&#39;,&#39;高&#39;,&#39;低&#39;,&#39;高&#39;,&#39;高&#39;,&#39;低&#39;,&#39;中&#39;]<br>             ,&#39;个性&#39;:[&#39;开朗&#39;,&#39;拘谨&#39;,&#39;开朗&#39;,&#39;拘谨&#39;,&#39;开朗&#39;,&#39;开朗&#39;,&#39;开朗&#39;,&#39;拘谨&#39;,&#39;开朗&#39;,&#39;开朗&#39;]}<br>name = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;I&#39;,&#39;J&#39;]<br>knight = pd.DataFrame(data=p,index=name)<br><br># 定义问题类型及增益大小<br>problem_type = {&#39;性别&#39;:1,&#39;智商&#39;:0.72,&#39;情商&#39;:0.97,&#39;侠义&#39;:1.58,&#39;个性&#39;:0.88}<br><br># 计算测试问题的期望值<br>def get_exp_problems(knight, problem_type):<br>#BEGIN<br><br>    result = {}<br>    sub_knight = [(knight,0)]<br>    temp_sub_knight = []<br><br>    for pt in problem_type:<br><br>        #用某一个问题类型对每一组侠客进行划分<br>        for sub in sub_knight:<br>            temp_sub = sub[0].groupby(by = [pt])<br><br>            #该问题没有将该组侠客进行划分，放入中间结果，继续下一组划分<br>            if len(temp_sub) &lt;= 1:<br>                temp_sub_knight.append((sub[0],sub[1]))<br>                continue<br><br>            # 对划分后的结果进行处理<br>            for label, member in temp_sub:<br>                list_member = list(member.index)<br>                if len(list_member) == 1:<br>                    result[list_member[0]] = sub[1] + 1<br>                else:<br>                    temp_sub_knight.append((member, sub[1]+1))<br><br>        sub_knight.clear()<br>        sub_knight.extend(temp_sub_knight)<br>        temp_sub_knight.clear()<br><br>    # 计算问题数的期望值<br>    exp = 0<br>    for k in result:<br>        exp += result[k]&#47;len(name)<br><br>    return exp<br>#END<br><br>def main():<br>    problem = dict(sorted(problem_type.items(), key=lambda x: x[1], reverse=False))<br>    print(&#39;选择信息增益小的问题对人物分类，问题数的期望值={}&#39;.format(round(get_exp_problems(knight, problem),2)))<br><br>    problem = dict(sorted(problem_type.items(), key=lambda x: x[1], reverse=True))<br>    print(&#39;选择信息增益大的问题对人物分类，问题数的期望值={}&#39;.format(round(get_exp_problems(knight, problem),2)))<br><br>if __name__ == &#39;__main__&#39;:<br>    main()","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499045,"discussion_content":"很好的实现","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593403913,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":225186,"user_name":"骑行的掌柜J","can_delete":false,"product_type":"c1","uid":1474214,"ip_address":"","ucode":"3163102651C653","user_header":"https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg","comment_is_top":false,"comment_ctime":1591678107,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591678107","product_id":100021201,"comment_content":"思考题：“如果每次都选择使得信息增益最小的问题”<br>我算出来结果是 期望值是3.9。。。跟楼上两个哥们的不一样。。。（因为A、C、D、F、H都要问四轮；B、E、J都要三轮；G、I 要问五轮）不过都是在4的附近","like_count":0},{"had_liked":false,"id":218360,"user_name":"哈达syn$","can_delete":false,"product_type":"c1","uid":1986368,"ip_address":"","ucode":"F576169E5516F5","user_header":"https://static001.geekbang.org/account/avatar/00/1e/4f/40/6cfa75cb.jpg","comment_is_top":false,"comment_ctime":1589789100,"is_pvip":false,"replies":[{"id":"81370","content":"哈哈，正好写这篇的时候金庸老爷子过世，也算纪念一下","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1590280246,"ip_address":"","comment_id":218360,"utype":1}],"discussion_count":0,"race_medal":0,"score":"1589789100","product_id":100021201,"comment_content":"王天一喜欢用足球举例子，黄申喜欢武侠小说呀","like_count":0},{"had_liked":false,"id":199330,"user_name":"拉普达","can_delete":false,"product_type":"c1","uid":1930686,"ip_address":"","ucode":"0E524C0D99B2A0","user_header":"https://static001.geekbang.org/account/avatar/00/1d/75/be/6f3ab95e.jpg","comment_is_top":false,"comment_ctime":1585486928,"is_pvip":false,"replies":[{"id":"75041","content":"列出所有可能的路径，大致在这个范围","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1585629156,"ip_address":"","comment_id":199330,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1585486928","product_id":100021201,"comment_content":"期望是3.8次 ","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":489820,"discussion_content":"列出所有可能的路径，大致在这个范围","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585629156,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":182420,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1582784681,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1582784681","product_id":100021201,"comment_content":"要点总结：<br>CART算法和 ID3、C4.5 相比，主要有两处不同：<br>在分类时，CART 不再采用信息增益或信息增益率，而是采用基尼指数（Gini）来选择最好的特征并进行数据的划分；<br>在 ID3 和 C4.5 决策树中，算法根据特征的属性值划分数据，可能会划分出多个组。<br>而 CART 算法采用了二叉树，每次把数据切成两份，分别进入左子树、右子树。","like_count":0},{"had_liked":false,"id":121558,"user_name":"lifes a Struggle","can_delete":false,"product_type":"c1","uid":1588303,"ip_address":"","ucode":"B19093752719BC","user_header":"https://static001.geekbang.org/account/avatar/00/18/3c/4f/979bdccc.jpg","comment_is_top":false,"comment_ctime":1565158838,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565158838","product_id":100021201,"comment_content":"老师，请问一下，当原始集合中的数据，本身是分布不均匀的，这个时候该如何计算它的信息熵呢？如：集合｛A,A,A,B,B,C｝","like_count":0},{"had_liked":false,"id":121453,"user_name":"abson","can_delete":false,"product_type":"c1","uid":1517591,"ip_address":"","ucode":"ED6E65CB4221AE","user_header":"https://static001.geekbang.org/account/avatar/00/17/28/17/2ee45db9.jpg","comment_is_top":false,"comment_ctime":1565141423,"is_pvip":false,"replies":[{"id":"44723","content":"你是指在实际项目中如何获得数据吗？这个要看具体的应用场景和需求，通常的规则是尽量覆盖不同的情况。比如不同时间段、不同用户分组、不同地域等等","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1565195903,"ip_address":"","comment_id":121453,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1565141423","product_id":100021201,"comment_content":"老师，有个疑问，像前几篇文章讲隐马尔科夫、信息熵和本节讲的决策树，数据是怎么来的，用什么方法去统计才能拿到相对偏差较少的数据","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":461797,"discussion_content":"你是指在实际项目中如何获得数据吗？这个要看具体的应用场景和需求，通常的规则是尽量覆盖不同的情况。比如不同时间段、不同用户分组、不同地域等等","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565195903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95484,"user_name":"动摇的小指南针","can_delete":false,"product_type":"c1","uid":1132592,"ip_address":"","ucode":"D1795D52F078BA","user_header":"https://static001.geekbang.org/account/avatar/00/11/48/30/77c6c4ec.jpg","comment_is_top":false,"comment_ctime":1558062484,"is_pvip":false,"replies":[{"id":"34201","content":"由于是标注数据，所以这个n是根据原有分类的标签来看的","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1558108591,"ip_address":"","comment_id":95484,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1558062484","product_id":100021201,"comment_content":"基尼系数中，基于特征T划分出来的子集m中，m的每个子集又有n个不同的分组。请问这个n是根据什么来进行划分的呢","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450475,"discussion_content":"由于是标注数据，所以这个n是根据原有分类的标签来看的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1558108591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":77260,"user_name":"Bora.Don","can_delete":false,"product_type":"c1","uid":1076329,"ip_address":"","ucode":"F6358D3A3228FB","user_header":"https://static001.geekbang.org/account/avatar/00/10/6c/69/d5a28079.jpg","comment_is_top":false,"comment_ctime":1552899556,"is_pvip":false,"replies":[{"id":"28269","content":"这里n和m表示分别表示使用特征划分后形成的分组，以及分类标签形成的分组，这和决策树的分叉是不同的","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1552927588,"ip_address":"","comment_id":77260,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1552899556","product_id":100021201,"comment_content":"老师，你好，既然CART算法是二叉树，那么在计算基尼指数的时候，n和m是不是就是定值：2？<br>CART算法又是如何保证是二叉树的呢？CART算法没看懂","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":443662,"discussion_content":"这里n和m表示分别表示使用特征划分后形成的分组，以及分类标签形成的分组，这和决策树的分叉是不同的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1552927588,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":75344,"user_name":"冰冷的梦","can_delete":false,"product_type":"c1","uid":1262658,"ip_address":"","ucode":"6F3C7EFCBA5FFD","user_header":"https://static001.geekbang.org/account/avatar/00/13/44/42/af72265f.jpg","comment_is_top":false,"comment_ctime":1552381598,"is_pvip":false,"replies":[{"id":"27552","content":"我在第32篇有详细讲解，你可以参考","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1552408594,"ip_address":"","comment_id":75344,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1552381598","product_id":100021201,"comment_content":"老师，什么是过拟合啊？","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442869,"discussion_content":"我在第32篇有详细讲解，你可以参考","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1552408594,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}