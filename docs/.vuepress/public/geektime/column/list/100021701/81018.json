{"id":81018,"title":"25丨KNN（下）：如何对手写数字进行识别？","content":"<p>今天我来带你进行KNN的实战。上节课，我讲了KNN实际上是计算待分类物体与其他物体之间的距离，然后通过统计最近的K个邻居的分类情况，来决定这个物体的分类情况。</p><p>这节课，我们先看下如何在sklearn中使用KNN算法，然后通过sklearn中自带的手写数字数据集来进行实战。</p><p>之前我还讲过SVM、朴素贝叶斯和决策树分类，我们还可以用这个数据集来做下训练，对比下这四个分类器的训练结果。</p><h2>如何在sklearn中使用KNN</h2><p>在Python的sklearn工具包中有KNN算法。KNN既可以做分类器，也可以做回归。如果是做分类，你需要引用：</p><pre><code>from sklearn.neighbors import KNeighborsClassifier\n</code></pre><p>如果是做回归，你需要引用：</p><pre><code>from sklearn.neighbors import KNeighborsRegressor\n\n</code></pre><p>从名字上你也能看出来Classifier对应的是分类，Regressor对应的是回归。一般来说如果一个算法有Classifier类，都能找到相应的Regressor类。比如在决策树分类中，你可以使用DecisionTreeClassifier，也可以使用决策树来做回归DecisionTreeRegressor。</p><p>好了，我们看下如何在sklearn中创建KNN分类器。</p><p>这里，我们使用构造函数KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数，我分别来讲解下：</p><!-- [[[read_end]]] --><p>1.n_neighbors：即KNN中的K值，代表的是邻居的数量。K值如果比较小，会造成过拟合。如果K值比较大，无法将未知物体分类出来。一般我们使用默认值5。</p><p>2.weights：是用来确定邻居的权重，有三种方式：</p><ul>\n<li>\n<p>weights=uniform，代表所有邻居的权重相同；</p>\n</li>\n<li>\n<p>weights=distance，代表权重是距离的倒数，即与距离成反比；</p>\n</li>\n<li>\n<p>自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。</p>\n</li>\n</ul><p>3.algorithm：用来规定计算邻居的方法，它有四种方式：</p><ul>\n<li>\n<p>algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择auto；</p>\n</li>\n<li>\n<p>algorithm=kd_tree，也叫作KD树，是多维空间的数据结构，方便对关键数据进行检索，不过KD树适用于维度少的情况，一般维数不超过20，如果维数大于20之后，效率反而会下降；</p>\n</li>\n<li>\n<p>algorithm=ball_tree，也叫作球树，它和KD树一样都是多维空间的数据结果，不同于KD树，球树更适用于维度大的情况；</p>\n</li>\n<li>\n<p>algorithm=brute，也叫作暴力搜索，它和KD树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。</p>\n</li>\n</ul><p>4.leaf_size：代表构造KD树或球树时的叶子数，默认是30，调整leaf_size会影响到树的构造和搜索速度。</p><p>创建完KNN分类器之后，我们就可以输入训练集对它进行训练，这里我们使用fit()函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的KNN分类器。然后可以使用predict()函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。</p><h2>如何用KNN对手写数字进行识别分类</h2><p>手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果0-9一一对应起来。完整的手写数字数据集MNIST里面包括了60000个训练样本，以及10000个测试样本。如果你学习深度学习的话，MNIST基本上是你接触的第一个数据集。</p><p>今天我们用sklearn自带的手写数字数据集做KNN分类，你可以把这个数据集理解成一个简版的MNIST数据集，它只包括了1797幅数字图像，每幅图像大小是8*8像素。</p><p>好了，我们先来规划下整个KNN分类的流程：</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/78/8af94562f6bd3ac42036ec47f5ad2578.jpg?wh=2373*1087\" alt=\"\"><br>\n整个训练过程基本上都会包括三个阶段：</p><ol>\n<li>\n<p>数据加载：我们可以直接从sklearn中加载自带的手写数字数据集；</p>\n</li>\n<li>\n<p>准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个8*8的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；</p>\n</li>\n<li>\n<p>分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。</p>\n</li>\n</ol><p>好了，按照上面的步骤，我们一起来实现下这个项目。</p><p>首先是加载数据和对数据的探索：</p><pre><code># 加载数据\ndigits = load_digits()\ndata = digits.data\n# 数据探索\nprint(data.shape)\n# 查看第一幅图像\nprint(digits.images[0])\n# 第一幅图像代表的数字含义\nprint(digits.target[0])\n# 将第一幅图像显示出来\nplt.gray()\nplt.imshow(digits.images[0])\nplt.show()\n</code></pre><p>运行结果：</p><pre><code>(1797, 64)\n[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n0\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/62/3c/625b7e95a22c025efa545d7144ec5f3c.png?wh=616*632\" alt=\"\"><br>\n我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个8*8的像素矩阵，上面这幅图像是一个“0”，从训练集的分类标注中我们也可以看到分类标注为“0”。</p><p>sklearn自带的手写数字数据集一共包括了1797个样本，每幅图像都是8*8像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为KNN算法和距离定义相关，我们需要对数据进行规范化处理，采用Z-Score规范化，代码如下：</p><pre><code># 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）\ntrain_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)\n# 采用Z-Score规范化\nss = preprocessing.StandardScaler()\ntrain_ss_x = ss.fit_transform(train_x)\ntest_ss_x = ss.transform(test_x)\n</code></pre><p>然后我们构造一个KNN分类器knn，把训练集的数据传入构造好的knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到KNN分类器准确率，代码如下：</p><pre><code># 创建KNN分类器\nknn = KNeighborsClassifier() \nknn.fit(train_ss_x, train_y) \npredict_y = knn.predict(test_ss_x) \nprint(&quot;KNN准确率: %.4lf&quot; % accuracy_score(test_y, predict_y))\n</code></pre><p>运行结果：</p><pre><code>KNN准确率: 0.9756\n</code></pre><p>好了，这样我们就构造好了一个KNN分类器。之前我们还讲过SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：</p><pre><code># 创建SVM分类器\nsvm = SVC()\nsvm.fit(train_ss_x, train_y)\npredict_y=svm.predict(test_ss_x)\nprint('SVM准确率: %0.4lf' % accuracy_score(test_y, predict_y))\n# 采用Min-Max规范化\nmm = preprocessing.MinMaxScaler()\ntrain_mm_x = mm.fit_transform(train_x)\ntest_mm_x = mm.transform(test_x)\n# 创建Naive Bayes分类器\nmnb = MultinomialNB()\nmnb.fit(train_mm_x, train_y) \npredict_y = mnb.predict(test_mm_x) \nprint(&quot;多项式朴素贝叶斯准确率: %.4lf&quot; % accuracy_score(test_y, predict_y))\n# 创建CART决策树分类器\ndtc = DecisionTreeClassifier()\ndtc.fit(train_mm_x, train_y) \npredict_y = dtc.predict(test_mm_x) \nprint(&quot;CART决策树准确率: %.4lf&quot; % accuracy_score(test_y, predict_y))\n</code></pre><p>运行结果如下：</p><pre><code>SVM准确率: 0.9867\n多项式朴素贝叶斯准确率: 0.8844\nCART决策树准确率: 0.8556\n</code></pre><p>这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为Z-Score会将数值规范化为一个标准的正态分布，即均值为0，方差为1，数值会包含负数。因此我们需要采用Min-Max规范化，将数据规范化到[0,1]范围内。</p><p>好了，我们整理下这4个分类器的结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/0f/e8/0f498e0197935bfe15d9b1209bad8fe8.png?wh=974*334\" alt=\"\"><br>\n你能看出来KNN的准确率还是不错的，和SVM不相上下。</p><p>你可以自己跑一遍整个代码，在运行前还需要import相关的工具包（下面的这些工具包你都会用到，所以都需要引用）：</p><pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_digits\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\n</code></pre><p>代码中，我使用了train_test_split做数据集的拆分，使用matplotlib.pyplot工具包显示图像，使用accuracy_score进行分类器准确率的计算，使用preprocessing中的StandardScaler和MinMaxScaler做数据的规范化。</p><p>完整的代码你可以从<a href=\"https://github.com/cystanford/knn\">GitHub</a>上下载。</p><h2>总结</h2><p>今天我带你一起做了手写数字分类识别的实战，分别用KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在这个过程中你应该对数据探索、数据可视化、数据规范化、模型训练和结果评估的使用过程有了一定的体会。在数据量不大的情况下，使用sklearn还是方便的。</p><p>如果数据量很大，比如MNIST数据集中的6万个训练数据和1万个测试数据，那么采用深度学习+GPU运算的方式会更适合。因为深度学习的特点就是需要大量并行的重复计算，GPU最擅长的就是做大量的并行计算。</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/e1/d08f489c3bffaacb6910f32a0fa600e1.png?wh=788*400\" alt=\"\"><br>\n最后留两道思考题吧，请你说说项目中KNN分类器的常用构造参数，功能函数都有哪些，以及你对KNN使用的理解？如果把KNN中的K值设置为200，数据集还是sklearn中的手写数字数据集，再跑一遍程序，看看分类器的准确率是多少？</p><p>欢迎在评论区与我分享你的答案，也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事。</p><p></p>","neighbors":{"left":{"article_title":"24丨KNN（上）：如何根据打斗和接吻次数来划分电影类型？","id":80983},"right":{"article_title":"26丨K-Means（上）：如何给20支亚洲球队做聚类？","id":81390}},"comments":[{"had_liked":false,"id":65735,"user_name":"不做键盘侠","can_delete":false,"product_type":"c1","uid":1154451,"ip_address":"","ucode":"30C123DCB9696C","user_header":"https://static001.geekbang.org/account/avatar/00/11/9d/93/945393c1.jpg","comment_is_top":false,"comment_ctime":1549612022,"is_pvip":false,"replies":[{"id":"25527","content":"一个很好的问题。我在train的时候用到了：train_ss_x = ss.fit_transform(train_x)<br>实际上：fit_transform是fit和transform两个函数都执行一次。所以ss是进行了fit拟合的。只有在fit拟合之后，才能进行transform<br>在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。<br>另外，如果我们没有fit，直接进行transform会报错，因为需要先fit拟合，才可以进行transform。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1551321435,"ip_address":"","comment_id":65735,"utype":2}],"discussion_count":6,"race_medal":0,"score":"117513729014","product_id":100021701,"comment_content":"为什么test只需要使用transform就可以了？test_ss_x = ss.transform(test_x）<br>","like_count":28,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438570,"discussion_content":"一个很好的问题。我在train的时候用到了：train_ss_x = ss.fit_transform(train_x)\n实际上：fit_transform是fit和transform两个函数都执行一次。所以ss是进行了fit拟合的。只有在fit拟合之后，才能进行transform\n在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。\n另外，如果我们没有fit，直接进行transform会报错，因为需要先fit拟合，才可以进行transform。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551321435,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1350086,"avatar":"","nickname":"Geek_dd384f","note":"","ucode":"4C009F32AF6839","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":697,"discussion_content":"preprocessing.StandardScaler 和 preprocessing.scale的区别，preprocessing.StandardScaler 保存了训练集的参数，可以直接使用他转化测试集","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1561968901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1115232,"avatar":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","nickname":"Fan","note":"","ucode":"3BF28670FD9407","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385017,"discussion_content":"在进行test的时候，我们已经在train的时候fit过了，所以直接transform即可。 啥时候fit过了呢？train 跟 test 的数据集都不一样。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1626851883,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2917819,"avatar":"","nickname":"Geek_35a6a8","note":"","ucode":"E8EC58E33079C0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1115232,"avatar":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","nickname":"Fan","note":"","ucode":"3BF28670FD9407","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":572552,"discussion_content":"是ss拟合过了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652850494,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":385017,"ip_address":""},"score":572552,"extra":""}]},{"author":{"id":1016737,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/83/a1/733caaac.jpg","nickname":"Ksnshejn","note":"","ucode":"0EF16B721AA25A","race_medal":1,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":335072,"discussion_content":"为什么要做fit呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608084940,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":223752,"discussion_content":"fit只能用在训练集上，因为需要得到训练集的分布特征（平均值、方差等）。用训练集上得到的特征，去transform测试集。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586251807,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":84624,"user_name":"Ricardo","can_delete":false,"product_type":"c1","uid":1203334,"ip_address":"","ucode":"FB125A0A28AA73","user_header":"https://static001.geekbang.org/account/avatar/00/12/5c/86/97bb7338.jpg","comment_is_top":false,"comment_ctime":1554887129,"is_pvip":false,"replies":[{"id":"40548","content":"看的很认真，我刚查了下官方文档确实是先写y_true，然后是y_pred，也就是：accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)<br>关于score的计算，是判断y_true和y_pred是否相等，也就是 score = y_true == y_pred，然后再根据样本的权重做归一化处理，调用_weighted_sum(score, sample_weight, normalize)<br>所以我刚用代码测试了下accuracy_score(y_true, y_pred)和accuracy_score(y_pred, y_true)的结果是一样的。Anyway，规范的话应该按照官方文档的顺序来调用参数。多谢反馈","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562553682,"ip_address":"","comment_id":84624,"utype":2}],"discussion_count":1,"race_medal":0,"score":"113224036825","product_id":100021701,"comment_content":"accuracy_score的参数顺序都错了，由于是计算真实标签和预测标签重合个数与总个数的比值，总能得到正确的答案，但是官方文档中写明的正确顺序应该是(y_true,y_pred)","like_count":27,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":446408,"discussion_content":"看的很认真，我刚查了下官方文档确实是先写y_true，然后是y_pred，也就是：accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n关于score的计算，是判断y_true和y_pred是否相等，也就是 score = y_true == y_pred，然后再根据样本的权重做归一化处理，调用_weighted_sum(score, sample_weight, normalize)\n所以我刚用代码测试了下accuracy_score(y_true, y_pred)和accuracy_score(y_pred, y_true)的结果是一样的。Anyway，规范的话应该按照官方文档的顺序来调用参数。多谢反馈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562553682,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67062,"user_name":"牛奶布丁","can_delete":false,"product_type":"c1","uid":1368494,"ip_address":"","ucode":"38994AEF4A6E78","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/KQpJHrQFQnezpyMlffXh9m9Dh6o8Z2yZXw8lEN73TyltgMGgDjhAz2cTbMpe2jgwWzkPr5Ribf2LgIDOE77kLdA/132","comment_is_top":false,"comment_ctime":1550065744,"is_pvip":false,"replies":[{"id":"25526","content":"多项式朴素贝叶斯实际上是符合多项式分布，不会存在负数。而高斯朴素贝叶斯呈现的是高斯分布，也就是正态分布，比如均值为0，方差为1的标准正态分布，可以存在负数。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1551321400,"ip_address":"","comment_id":67062,"utype":2}],"discussion_count":1,"race_medal":0,"score":"57384640592","product_id":100021701,"comment_content":"老师，为什么做多项式朴素贝叶斯分类的时候，传入的数据不能有负数呢，之前老师讲文本分类的时候好像没有提到这一点？","like_count":13,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439036,"discussion_content":"多项式朴素贝叶斯实际上是符合多项式分布，不会存在负数。而高斯朴素贝叶斯呈现的是高斯分布，也就是正态分布，比如均值为0，方差为1的标准正态分布，可以存在负数。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551321400,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":137708,"user_name":"Yiuway","can_delete":false,"product_type":"c1","uid":1583887,"ip_address":"","ucode":"9A8EE7282F9BB1","user_header":"https://static001.geekbang.org/account/avatar/00/18/2b/0f/a1f8dd5d.jpg","comment_is_top":false,"comment_ctime":1569827165,"is_pvip":false,"replies":[{"id":"53352","content":"数据比较零散的话可以使用Min-Max规范化，如果数据符合高斯分布，可以使用Z-Score规范化。<br>有些分类方法对归一化比较敏感，比如GaussianNB，效果就不一定好。不过大部分情况下，还是需要先对数据做规范化处理","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1570424408,"ip_address":"","comment_id":137708,"utype":1}],"discussion_count":2,"race_medal":0,"score":"53109434717","product_id":100021701,"comment_content":"在做项目的时候，应该什么时候用Min-Max,什么时候用Z-Score呢？当我不做规范化的时候，反而准确率更高，这是为什么呢？在数据规范化该什么时候做不太理解，希望得到回复！","like_count":12,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":469222,"discussion_content":"数据比较零散的话可以使用Min-Max规范化，如果数据符合高斯分布，可以使用Z-Score规范化。\n有些分类方法对归一化比较敏感，比如GaussianNB，效果就不一定好。不过大部分情况下，还是需要先对数据做规范化处理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570424408,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":223756,"discussion_content":"什么叫数据比较零散？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586252009,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87434,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555593883,"is_pvip":false,"replies":[{"id":"64240","content":"对的 Good Job 滢离数据总监越来越近了","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577615111,"ip_address":"","comment_id":87434,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27325397659","product_id":100021701,"comment_content":"用代码计算来以下准确率：<br>knn默认k值为5 准确率:0.9756<br>knn的k值为200的准确率:0.8489<br>SVM分类准确率:0.9867<br>高斯朴素贝叶斯准确率:0.8111<br>多项式朴素贝叶斯分类器准确率:0.8844<br>CART决策树准确率:0.8400<br><br>K值的选取如果过大，正确率降低。 <br>算法效率排行 SVM &gt; KNN(k值在合适范围内) &gt;多项式朴素贝叶斯 &gt; CART &gt; 高斯朴素贝叶斯<br>","like_count":6,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447447,"discussion_content":"对的 Good Job 滢离数据总监越来越近了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615111,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68403,"user_name":"third","can_delete":false,"product_type":"c1","uid":1025114,"ip_address":"","ucode":"9A37408A834F0B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg","comment_is_top":false,"comment_ctime":1550495899,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23025332379","product_id":100021701,"comment_content":"KNN常用的构造参数<br>KNeighborsClassifier(n_neighbors=5,weights=&#39;uniform&#39;,algorithm=&#39;auto&#39;,leaf_size=30)<br>n_neighbors是邻居的数目<br><br>weights是权重<br>uniform是权重相同，求平均值<br>distance是根据距离的倒数<br>自定义<br><br>algorithm规定邻居的方式<br>auto根据数据自动选择<br>kd_tree，多维空间的数据结构，一般不超过20维，对关键数据检索很方便<br>ball_tree，适用于维度大的<br>brute包里搜索，线性扫描<br><br>leaf_size是叶子数<br><br>","like_count":5},{"had_liked":false,"id":109134,"user_name":"Geek_dd384f","can_delete":false,"product_type":"c1","uid":1350086,"ip_address":"","ucode":"4C009F32AF6839","user_header":"","comment_is_top":false,"comment_ctime":1561968829,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14446870717","product_id":100021701,"comment_content":"#preprocessing.StandardScaler 和preprocessing.scale的区别<br>#使用sklearn.preprocessing.StandardScaler类，使用该类的好处在于可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据。<br>ss = preprocessing.StandardScaler()<br>train_ss_x = ss.fit_transform(train_x)  #这里的fit_transform相当于先fit 再 transform<br>test_ss_x = ss.transform(test_x)     #这里没有使用fit_transform 就是因为使用了StandardScaler()<br>#使用sklearn.preprocessing.scale()函数，可以直接将给定数据进行标准化。<br>#train_ss_x = preprocessing.scale(train_x)<br>#test_ss_x = preprocessing.scale(test_x)<br>","like_count":3,"discussions":[{"author":{"id":1068361,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","nickname":"明翼","note":"","ucode":"E77F86BEB3D5C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":41775,"discussion_content":"fit和transform分别是做什么的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1572502555,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67466,"user_name":"Lee","can_delete":false,"product_type":"c1","uid":1313515,"ip_address":"","ucode":"ED3D7200A948C3","user_header":"https://static001.geekbang.org/account/avatar/00/14/0a/eb/6d6a94d2.jpg","comment_is_top":false,"comment_ctime":1550153324,"is_pvip":false,"replies":[{"id":"40547","content":"对的，K值过大，无法将未知物体分类出来，会降低准确率。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562553608,"ip_address":"","comment_id":67466,"utype":2}],"discussion_count":1,"race_medal":0,"score":"14435055212","product_id":100021701,"comment_content":"KNN 中的 K 值设置为 200，KNN 准确率: 0.8489，k值过大，导致部分未知物体没有分类出来，所以准确率下降了","like_count":3,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439211,"discussion_content":"对的，K值过大，无法将未知物体分类出来，会降低准确率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562553608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67795,"user_name":"FORWARD―MOUNT","can_delete":false,"product_type":"c1","uid":1357857,"ip_address":"","ucode":"CD8E9ECF882980","user_header":"https://static001.geekbang.org/account/avatar/00/14/b8/21/c03839f1.jpg","comment_is_top":false,"comment_ctime":1550280099,"is_pvip":false,"replies":[{"id":"25524","content":"对 训练集的特征矩阵和分类结果。对应test_x和test_y是测试集的特征矩阵和分类结果。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1551321205,"ip_address":"","comment_id":67795,"utype":2}],"discussion_count":2,"race_medal":0,"score":"10140214691","product_id":100021701,"comment_content":"train_x与train_y都是训练集？<br>","like_count":2,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439378,"discussion_content":"对 训练集的特征矩阵和分类结果。对应test_x和test_y是测试集的特征矩阵和分类结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551321205,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":223757,"discussion_content":"通常，x表示特征，y表示类别（标签）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586252064,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67659,"user_name":"JingZ","can_delete":false,"product_type":"c1","uid":1023464,"ip_address":"","ucode":"6F97895B2CC375","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132","comment_is_top":false,"comment_ctime":1550219141,"is_pvip":false,"replies":[{"id":"25525","content":"对的K值大未必好","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1551321364,"ip_address":"","comment_id":67659,"utype":2}],"discussion_count":1,"race_medal":0,"score":"10140153733","product_id":100021701,"comment_content":"#knn 将K值调为200，准确率变为0.8489了，相比较默认K=5的准确率 0.9756，下降13%<br><br>from sklearn.datasets import load_digits<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn import preprocessing<br>from sklearn.neighbors import KNeighborsClassifier<br>from sklearn.metrics import accuracy_score<br><br>#加载数据<br>digits = load_digits()<br>data = digits.data<br><br>#数据探索<br>print(data.shape)<br><br>#查看第一幅图像<br>print(digits.images[0])<br>print(digits.target[0])<br><br>#数据可视化<br>plt.gray()<br>plt.imshow(digits.images[0])<br>plt.show()<br><br>#训练集 测试集<br>train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)<br><br>#采用 Z-Score 规范化<br>ss = preprocessing.StandardScaler()<br>train_ss_x = ss.fit_transform(train_x)<br>test_ss_x = ss.transform(test_x)<br><br>#创建 KNN 分类器<br>knn = KNeighborsClassifier(n_neighbors=200)<br><br>#用训练集训练<br>knn.fit(train_ss_x, train_y)<br><br>#用测试集预测<br>predict_y = knn.predict(test_ss_x)<br><br>#模型评估<br>print(&#39;KNN 准确率：%.4lf&#39; % accuracy_score(predict_y, test_y))","like_count":2,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439310,"discussion_content":"对的K值大未必好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551321364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":151527,"user_name":"Ronnyz","can_delete":false,"product_type":"c1","uid":1488280,"ip_address":"","ucode":"9F34527B1D343D","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg","comment_is_top":false,"comment_ctime":1573736590,"is_pvip":false,"replies":[{"id":"59805","content":"random_state 就是随机数种子，没有必要调整个参数，每次运算结果不同 还是正常的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1574756111,"ip_address":"","comment_id":151527,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5868703886","product_id":100021701,"comment_content":"老师能解释下数据分割时random_state的取值有什么规范吗？<br>我自己测试的random_state=666与老师=33得出的准确度还是有一些差距的：<br>KNN准确率：0.9778<br>SVM准确率：0.9733<br>多项式朴素贝叶斯准确率：0.9067<br>CART决策树准确率：0.8489","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474516,"discussion_content":"random_state 就是随机数种子，没有必要调整个参数，每次运算结果不同 还是正常的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574756111,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66604,"user_name":"从未在此","can_delete":false,"product_type":"c1","uid":1354589,"ip_address":"","ucode":"5A4AA275D8EE9A","user_header":"https://static001.geekbang.org/account/avatar/00/14/ab/5d/430ed3b6.jpg","comment_is_top":false,"comment_ctime":1549965356,"is_pvip":false,"replies":[{"id":"64583","content":"可以","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577622007,"ip_address":"","comment_id":66604,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5844932652","product_id":100021701,"comment_content":"那个标准化函数已经在训练集上拟合并产生了平均值和标准差。所以测试集用同样的标准直接拿来用就行了","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438845,"discussion_content":"可以","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622007,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":346128,"user_name":"Geek_35a6a8","can_delete":false,"product_type":"c1","uid":2917819,"ip_address":"","ucode":"E8EC58E33079C0","user_header":"","comment_is_top":false,"comment_ctime":1652850724,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1652850724","product_id":100021701,"comment_content":"运行出的图像是什么意思，应该怎么看呢<br>","like_count":0},{"had_liked":false,"id":303550,"user_name":"Fan","can_delete":false,"product_type":"c1","uid":1115232,"ip_address":"","ucode":"3BF28670FD9407","user_header":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","comment_is_top":false,"comment_ctime":1626852589,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626852589","product_id":100021701,"comment_content":"train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25, random_state=33) 这句代码中random_state=33 是什么意思呢？","like_count":0},{"had_liked":false,"id":303549,"user_name":"Fan","can_delete":false,"product_type":"c1","uid":1115232,"ip_address":"","ucode":"3BF28670FD9407","user_header":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","comment_is_top":false,"comment_ctime":1626852315,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626852315","product_id":100021701,"comment_content":"贴下代码<br><br>from operator import imod<br>from scipy.sparse.construct import random<br>from sklearn.model_selection import train_test_split<br>from sklearn import preprocessing<br>from sklearn.metrics import accuracy_score<br>from sklearn.datasets import load_digits<br>from sklearn.neighbors import KNeighborsClassifier<br>from sklearn.svm import SVC<br>from sklearn.naive_bayes import MultinomialNB<br>from sklearn.tree import DecisionTreeClassifier<br>import matplotlib.pyplot as plt<br><br>#加载数据<br>digits = load_digits()<br>data = digits.data<br><br># 数据探索<br>print(data.shape)<br># print(data[0])<br><br># 查看第一幅图像<br>print(digits.images[0])<br>print(digits.target[0])<br><br>plt.gray()<br>plt.imshow(digits.images[0])<br>plt.show()<br><br>train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)<br># 采用Z-Score规范化<br>ss = preprocessing.StandardScaler()<br>train_ss_x = ss.fit_transform(train_x)<br>test_ss_x=ss.transform(test_x)<br><br># 创建KNN分类器<br>knn=KNeighborsClassifier()<br>knn.fit(train_ss_x, train_y)<br>predict_y=knn.predict(test_ss_x)<br>print(&quot;KNN准确率: %.4lf&quot; % accuracy_score(test_y, predict_y))<br><br># 创建SVM分类器<br>svm = SVC()<br>svm.fit(train_ss_x, train_y)<br>predict_y=svm.predict(test_ss_x)<br>print(&quot;SVM准确率: %0.4lf&quot; % accuracy_score(test_y, predict_y))<br><br># 采用Min-Max规范化<br>mm=preprocessing.MinMaxScaler()<br>train_mm_x=mm.fit_transform(train_x)<br>test_mm_x=mm.transform(test_x)<br># 创建Naive Bayes分类器<br>mnb=MultinomialNB()<br>mnb.fit(train_mm_x, train_y)<br>predict_y=mnb.predict(test_mm_x)<br>print(&quot;多项式朴素贝叶斯准确率: %.4lf&quot; % accuracy_score(test_y, predict_y))<br><br># 创建CART决策树分类器<br>dtc=DecisionTreeClassifier()<br>dtc.fit(train_mm_x, train_y)<br>predict_y=dtc.predict(test_mm_x)<br>print(&quot;CART决策树准确率: %.4lf&quot; % accuracy_score(test_y, predict_y))<br><br>===============<br>KNN准确率: 0.9756<br>SVM准确率: 0.9867<br>多项式朴素贝叶斯准确率: 0.8844<br>CART决策树准确率: 0.8578","like_count":0},{"had_liked":false,"id":297928,"user_name":"彭涛","can_delete":false,"product_type":"c1","uid":1890860,"ip_address":"","ucode":"86EACC03B46A37","user_header":"https://static001.geekbang.org/account/avatar/00/1c/da/2c/1ac255a2.jpg","comment_is_top":false,"comment_ctime":1623846353,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1623846353","product_id":100021701,"comment_content":"老师您好，请问这里的 KNN 分类器没看见 K 值是如何设置的，请问是使用了默认值吗？","like_count":0,"discussions":[{"author":{"id":1115232,"avatar":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","nickname":"Fan","note":"","ucode":"3BF28670FD9407","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385021,"discussion_content":"是的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626852545,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287704,"user_name":"完美坚持","can_delete":false,"product_type":"c1","uid":1919541,"ip_address":"","ucode":"AE0261D8DDEF64","user_header":"https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg","comment_is_top":false,"comment_ctime":1618117135,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1618117135","product_id":100021701,"comment_content":"1. 如何实现用CV来选择合适的k<br>2. 深度学习+GPU运算怎么实现","like_count":0},{"had_liked":false,"id":269609,"user_name":"McKee Chen","can_delete":false,"product_type":"c1","uid":2037505,"ip_address":"","ucode":"F74B76542FAB65","user_header":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","comment_is_top":false,"comment_ctime":1608714734,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1608714734","product_id":100021701,"comment_content":"KNN常用构造函数为KNeighborsClassifier(n_neighbors=5, weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, leaf_size=30)<br>其中:<br>n_neighbors代表K值，K值不宜太大，会欠拟合；也不宜太小，会过拟合；<br>weights代表邻居的权重；<br>algorithm代表计算邻居的方法<br>leaf_size达标构造KD树或球树的叶子数<br><br>计算逻辑: 对样本数据集按一定比例进行划分，训练集通过fit函数对模型进行训练，然后将测试集的数据输入训练好的模型中，再通过predict函数预测测试集的分类结果，最后通过accuracy_score计算模型的准确率<br><br>KNN是一种监督学习，可以用于图像识别、字符识别、文本分类，K值通过交叉验证得出，且K值不能太大<br><br>#当K值=200时，计算KNN分类器的准确率<br>#导入包<br>import matplotlib.pyplot as plt<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.preprocessing import MinMaxScaler<br>from sklearn.metrics import accuracy_score<br>from sklearn.svm import SVC<br>from sklearn.neighbors import KNeighborsClassifier<br>from sklearn.datasets import load_digits<br>from sklearn.naive_bayes import MultinomialNB<br>from sklearn.tree import DecisionTreeClassifier<br>#数据加载<br>digits = load_digits()<br>#获得图像和分类结果<br>data = digits.data<br>target = digits.target<br>#将样本集进行划分，选取25%作为测试集，其余为训练集<br>train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25, random_state=33)<br>#由于数据量级不统一，对数据进行标准化处理<br>ss = StandardScaler()<br>train_ss_x = ss.fit_transform(train_x)<br>test_ss_x = ss.transform(test_x)<br>#创建KNN分类器<br>knn = KNeighborsClassifier(n_neighbors=200)#取K值=200<br>#训练分类器<br>knn.fit(train_ss_x, train_y)<br>#预测测试集结果<br>predict_y = knn.predict(test_ss_x)<br>#计算准确率<br>print(&#39;KNN分类器准确率为:&#39;, accuracy_score(test_y, predict_y))<br><br>#输出结果<br>KNN分类器准确率为: 0.8488888888888889<br><br><br><br>        <br>","like_count":0},{"had_liked":false,"id":257898,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1604198896,"is_pvip":false,"replies":[{"id":"103004","content":"当模型中存在对不同特征进行线性加权求和这种操作的时候通常都需要对特征进行规范化操作，例如：StandardScaler(标准化)或MinMaxScaler(归一化)等）。<br>进行规范化后，有利于加速模型收敛，且可以避免某列特征由于值较大而被模型判断为具有较大权重。<br>建议你利用线性回归模型，对未规范化和规范化后的特征，分别建模，然后打印出拟合后的模型权重，看看每个特征权重的差别。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1615996252,"ip_address":"","comment_id":257898,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1604198896","product_id":100021701,"comment_content":"老师，这个z-score规范化，把数据变成标准正态分布，在这个例子里的作用是什么？也就是说数据变化前是什么样的，变化后又是什么样的……如果不这么变化会带来什么结果？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508494,"discussion_content":"当模型中存在对不同特征进行线性加权求和这种操作的时候通常都需要对特征进行规范化操作，例如：StandardScaler(标准化)或MinMaxScaler(归一化)等）。\n进行规范化后，有利于加速模型收敛，且可以避免某列特征由于值较大而被模型判断为具有较大权重。\n建议你利用线性回归模型，对未规范化和规范化后的特征，分别建模，然后打印出拟合后的模型权重，看看每个特征权重的差别。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615996252,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":230054,"user_name":"§mc²ompleXWr","can_delete":false,"product_type":"c1","uid":1932586,"ip_address":"","ucode":"8D2527DE0F760B","user_header":"https://static001.geekbang.org/account/avatar/00/1d/7d/2a/4c7e2e2f.jpg","comment_is_top":false,"comment_ctime":1593248317,"is_pvip":false,"replies":[{"id":"103712","content":"CART决策树如果不使用random_state参数指定随机数种子，则每次准确率将会不同。<br> 即使分割器设置为“best”，每个分割中的特征也始终是随机排列的。 <br>当max_features &lt;n_features时，算法将在每个分割处随机选择max_features，然后再在其中找到最佳分割。 <br>但即使max_features = n_features，找到的最佳分割也可能因不同的运行而有所不同。 <br>就是这种情况，如果对于几个分割而言标准的改进是相同的，并且必须随机选择一个分割。 ","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617035206,"ip_address":"","comment_id":230054,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593248317","product_id":100021701,"comment_content":"为什么每次计算KNN和SVM分类器的准确率都是一样的？而朴素贝叶斯和决策树分类器每次计算的准确率都不一样呢？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499748,"discussion_content":"CART决策树如果不使用random_state参数指定随机数种子，则每次准确率将会不同。\n 即使分割器设置为“best”，每个分割中的特征也始终是随机排列的。 \n当max_features &amp;lt;n_features时，算法将在每个分割处随机选择max_features，然后再在其中找到最佳分割。 \n但即使max_features = n_features，找到的最佳分割也可能因不同的运行而有所不同。 \n就是这种情况，如果对于几个分割而言标准的改进是相同的，并且必须随机选择一个分割。 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617035206,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214443,"user_name":"LiLi","can_delete":false,"product_type":"c1","uid":1984523,"ip_address":"","ucode":"FCD30AF4360BDB","user_header":"https://static001.geekbang.org/account/avatar/00/1e/48/0b/9400afbb.jpg","comment_is_top":false,"comment_ctime":1588750587,"is_pvip":false,"replies":[{"id":"103761","content":"对，Z-Score规范化后，特征会符合均值0，标准差1的高斯分布。之所以这样做，是因为Z-Score规范化后，不容易受异常值影响。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617041350,"ip_address":"","comment_id":214443,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588750587","product_id":100021701,"comment_content":"“因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化” <br>--这里不是很明白，为何跟距离相关就选择Z-Score规范化？距离符合高斯分布？希望老师和同学们指点一下，谢谢！","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494056,"discussion_content":"对，Z-Score规范化后，特征会符合均值0，标准差1的高斯分布。之所以这样做，是因为Z-Score规范化后，不容易受异常值影响。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617041350,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206697,"user_name":"Ricky","can_delete":false,"product_type":"c1","uid":1940971,"ip_address":"","ucode":"0BC8F8DB52F01D","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9d/eb/2c7f3d3b.jpg","comment_is_top":false,"comment_ctime":1586918086,"is_pvip":false,"replies":[{"id":"104215","content":"imgs = digits.images<br>print(imgs.shape)<br>你会看到打印结果是(1797, 8, 8)，由左到右依次代表1797个样本，图像高度、图像宽度；在图像构成的数组中，如果该图像的通道数channel为1则会省略不写，通道数为1的图像你可以理解为灰度图像。手机拍摄的图像读取进来通道数为3，所以如果想把自己手写的图像转为灰度图需要使用opencv包对图像进行灰度化。然后拉平为一维数组后再送入模型进行判断。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617727665,"ip_address":"","comment_id":206697,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586918086","product_id":100021701,"comment_content":"问个问题：digits数据集中描述图像的格式什么？如果有一张外部的图片需要用这个模型来判断，应该怎么转化？<br>谢谢！","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491870,"discussion_content":"imgs = digits.images\nprint(imgs.shape)\n你会看到打印结果是(1797, 8, 8)，由左到右依次代表1797个样本，图像高度、图像宽度；在图像构成的数组中，如果该图像的通道数channel为1则会省略不写，通道数为1的图像你可以理解为灰度图像。手机拍摄的图像读取进来通道数为3，所以如果想把自己手写的图像转为灰度图需要使用opencv包对图像进行灰度化。然后拉平为一维数组后再送入模型进行判断。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617727665,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":200453,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1585587430,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585587430","product_id":100021701,"comment_content":"交作业：https:&#47;&#47;github.com&#47;LearningChanging&#47;Data-analysis-in-action&#47;tree&#47;master&#47;25-KNN%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AF%B9%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BF%9B%E8%A1%8C%E8%AF%86%E5%88%AB%EF%BC%9F","like_count":0},{"had_liked":false,"id":194338,"user_name":"陈金鑫","can_delete":false,"product_type":"c1","uid":1077127,"ip_address":"","ucode":"609A24832CA80C","user_header":"https://static001.geekbang.org/account/avatar/00/10/6f/87/669263b4.jpg","comment_is_top":false,"comment_ctime":1585052153,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585052153","product_id":100021701,"comment_content":"报FutureWarning<br>C:\\develop\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from &#39;auto&#39; to &#39;scale&#39; in version 0.22 to account better for unscaled features. Set gamma explicitly to &#39;auto&#39; or &#39;scale&#39; to avoid this warning.<br>  &quot;avoid this warning.&quot;, FutureWarning)<br><br>解决方案：<br>svm = SVC(gamma=&#39;auto&#39;)<br>或<br>svm = SVC(gamma=&#39;scale&#39;)","like_count":0},{"had_liked":false,"id":184382,"user_name":"鱼非子","can_delete":false,"product_type":"c1","uid":1818595,"ip_address":"","ucode":"BB76AE2CB4D680","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg","comment_is_top":false,"comment_ctime":1583303857,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583303857","product_id":100021701,"comment_content":"import numpy as np<br>import pandas as pd<br>from sklearn.datasets import load_digits<br>from sklearn import svm<br>from sklearn.neighbors import KNeighborsClassifier<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn import preprocessing<br>from sklearn import metrics<br>from sklearn.model_selection import train_test_split<br>from sklearn.naive_bayes import MultinomialNB<br>from sklearn.tree import DecisionTreeClassifier<br><br><br>digits = load_digits()<br>data = digits.data<br># print(data.shape)<br># print(digits.images[0])<br># plt.gray()<br># plt.imshow(digits.images[0])<br># plt.show()<br><br><br># 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）<br>train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25)<br># 采用Z-Score规范化<br>ss = preprocessing.StandardScaler()<br>train_ss_x = ss.fit_transform(train_x)<br>test_ss_x = ss.transform(test_x)<br><br>model1 = KNeighborsClassifier()<br>model1.fit(train_x,train_y)<br>predict = model1.predict(test_x)<br>print(&quot;knn准确率：&quot;,metrics.accuracy_score(predict,test_y))<br><br>model2 = svm.SVC()<br>model2.fit(train_x,train_y)<br>predict = model2.predict(test_x)<br>print(&quot;svm准确率：&quot;,metrics.accuracy_score(predict,test_y))<br><br>model3 = MultinomialNB()<br>model3.fit(train_x,train_y)<br>predict = model3.predict(test_x)<br>print(&quot;贝叶斯准确率：&quot;,metrics.accuracy_score(predict,test_y))<br><br>model4 = DecisionTreeClassifier()<br>model4.fit(train_x,train_y)<br>predict = model4.predict(test_x)<br>print(&quot;决策树准确率：&quot;,metrics.accuracy_score(predict,test_y))<br><br>knn准确率： 0.9866666666666667<br>svm准确率： 0.9888888888888889<br>贝叶斯准确率： 0.8844444444444445<br>决策树准确率： 0.8511111111111112","like_count":0},{"had_liked":false,"id":184376,"user_name":"鱼非子","can_delete":false,"product_type":"c1","uid":1818595,"ip_address":"","ucode":"BB76AE2CB4D680","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg","comment_is_top":false,"comment_ctime":1583303131,"is_pvip":false,"replies":[{"id":"104369","content":"很棒！！Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617905359,"ip_address":"","comment_id":184376,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1583303131","product_id":100021701,"comment_content":"import numpy as np<br>import pandas as pd<br>from sklearn.datasets import load_digits<br>from sklearn import svm<br>from sklearn.neighbors import KNeighborsClassifier<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from sklearn import preprocessing<br>from sklearn import metrics<br>from sklearn.model_selection import train_test_split<br><br><br>digits = load_digits()<br>data = digits.data<br>print(data.shape)<br>print(digits.images[0])<br>plt.gray()<br>plt.imshow(digits.images[0])<br>plt.show()<br><br><br># 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）<br>train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)<br># 采用Z-Score规范化<br>ss = preprocessing.StandardScaler()<br>train_ss_x = ss.fit_transform(train_x)<br>test_ss_x = ss.transform(test_x)<br><br>model = KNeighborsClassifier()<br>model.fit(train_x,train_y)<br>predict = model.predict(test_x)<br>print(&quot;knn准确率：&quot;,metrics.accuracy_score(predict,test_y))<br><br>knn准确率： 0.9844444444444445","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485991,"discussion_content":"很棒！！Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617905359,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":180153,"user_name":"will","can_delete":false,"product_type":"c1","uid":1750093,"ip_address":"","ucode":"A36CADCB62BFB5","user_header":"https://static001.geekbang.org/account/avatar/00/1a/b4/4d/f072c7f9.jpg","comment_is_top":false,"comment_ctime":1582200773,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1582200773","product_id":100021701,"comment_content":"请问能梳理一下啥时候用fit 啥时候transform，是不是fit+transform=fit_transform","like_count":0},{"had_liked":false,"id":167079,"user_name":"Untitled","can_delete":false,"product_type":"c1","uid":1039464,"ip_address":"","ucode":"8DD6ABA3E81A2E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dc/68/006ba72c.jpg","comment_is_top":false,"comment_ctime":1577686844,"is_pvip":false,"discussion_count":0,"race_medal":2,"score":"1577686844","product_id":100021701,"comment_content":"KNeighborsClassifier(n_neighbors=5, weights=&#39;uiform&#39;, algorithm=&#39;auto&#39;, left_size=30)<br># n_neighbors:k值，默认5<br># weights:邻居的权重，uniform是邻居的权重相同，distance是权重是距离的倒数，自定义函数-自定义不同距离对应的权重。<br># algorithm规定邻居的方法：<br>#- auto是根据数据情况自动选择合适的算法，<br>#- kd_tree是多维空间数据结构，方便对关键数据进行检索，<br>#- ball_tree是球树，也是用于多维空间数据结构，但KD树适合维度少于20，ball_tree适合维度大的情况<br>#- brute是暴力搜索，采用线性扫描，训练集大时效率低<br># leaf_size：ball_tree或KD树的叶子数，默认30<br><br>KNN就是根据设置的n_neighbors数，找最邻近的n_neighbors的分类结果来进行预测<br><br>n_neighbors设置为200时，准确性为0.8533，用时也比默认值多了。","like_count":0},{"had_liked":false,"id":119699,"user_name":"ken","can_delete":false,"product_type":"c1","uid":1137330,"ip_address":"","ucode":"606DA1C78E67F2","user_header":"","comment_is_top":false,"comment_ctime":1564658182,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564658182","product_id":100021701,"comment_content":"实际上 数字图片只需要0 和1 区分开来 就可以了吧  期待问答","like_count":0},{"had_liked":false,"id":116715,"user_name":"FeiFei","can_delete":false,"product_type":"c1","uid":1045586,"ip_address":"","ucode":"01CD655DD4E56C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg","comment_is_top":false,"comment_ctime":1563894083,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1563894083","product_id":100021701,"comment_content":"#n_neighbors：K值<br>#weights：节点权重。全一样，或者距离的倒数，或者自定义函数<br>#algorithm：auto;kd_tree;ball_tree;brute<br>#leaf_size:叶子节点数量<br><br>0.8489","like_count":0},{"had_liked":false,"id":96560,"user_name":"张晓辉","can_delete":false,"product_type":"c1","uid":1085046,"ip_address":"","ucode":"1CD9717DE399C5","user_header":"https://static001.geekbang.org/account/avatar/00/10/8e/76/6d55e26f.jpg","comment_is_top":false,"comment_ctime":1558449303,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558449303","product_id":100021701,"comment_content":"如果把K设置为200，KNN的准确率会下降到0.84。比之前默认值要差很多。<br><br>from sklearn.neighbors import KNeighborsClassifier<br>from sklearn.datasets import load_digits<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.preprocessing import MinMaxScaler<br>import matplotlib.pyplot as plt <br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.svm import SVC<br>from sklearn.naive_bayes import MultinomialNB<br>from sklearn.tree import DecisionTreeClassifier<br><br>digits = load_digits()<br>data = digits.data<br>train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)<br>ss = StandardScaler()<br><br>train_ss_x = ss.fit_transform(train_x)<br>test_ss_x = ss.transform(test_x)<br><br>knn = KNeighborsClassifier()<br>knn.fit(train_ss_x, train_y)<br>predict_y = knn.predict(test_ss_x)<br>print(&quot;KNN precision %.4lf&quot; % accuracy_score(predict_y, test_y))<br><br>svm = SVC(gamma=&#39;auto&#39;)<br>svm.fit(train_ss_x, train_y)<br>predict_y = svm.predict(test_ss_x)<br>print(&quot;SVM precision %.4lf&quot; % accuracy_score(predict_y, test_y))<br><br>mm = MinMaxScaler()<br>train_mm_x = mm.fit_transform(train_x)<br>test_mm_x = mm.transform(test_x)<br>mnb = MultinomialNB()<br>mnb.fit(train_mm_x, train_y)<br>predict_y = mnb.predict(test_mm_x)<br>print(&quot;MultinomialNB precision %.4lf&quot; % accuracy_score(predict_y, test_y))<br><br>dtc = DecisionTreeClassifier()<br>dtc.fit(train_mm_x, train_y)<br>predict_y = dtc.predict(test_mm_x)<br>print(&quot;DecisionTree precision %.4lf&quot; % accuracy_score(predict_y, test_y))","like_count":0},{"had_liked":false,"id":87439,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555594432,"is_pvip":false,"replies":[{"id":"64239","content":"可以这样，用sklearn来寻找最优的超参数","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577615089,"ip_address":"","comment_id":87439,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555594432","product_id":100021701,"comment_content":"老师，想问个问题，KNeighborsClassifier的默认k值为5，我们可以给其设置默认k值。在上一节中讲到K值的选取用交叉验证，如果用sklearn实现的话，我们需要给KNeighborsClassifier设定不同的k值来寻找最优K值吗？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447451,"discussion_content":"可以这样，用sklearn来寻找最优的超参数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615089,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86182,"user_name":"XP@培新","can_delete":false,"product_type":"c1","uid":1082478,"ip_address":"","ucode":"45CC483F62FE53","user_header":"https://static001.geekbang.org/account/avatar/00/10/84/6e/0bf62245.jpg","comment_is_top":false,"comment_ctime":1555320474,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555320474","product_id":100021701,"comment_content":"请教老师一个可能很“外行”的问题，KNN分类器训练输出到底是什么？ 感觉直接算不就可以了吗？ 谢谢","like_count":0},{"had_liked":false,"id":85707,"user_name":"周飞","can_delete":false,"product_type":"c1","uid":1073374,"ip_address":"","ucode":"F85FA236EB0C0D","user_header":"https://static001.geekbang.org/account/avatar/00/10/60/de/5c67895a.jpg","comment_is_top":false,"comment_ctime":1555208523,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555208523","product_id":100021701,"comment_content":"如果把 KNN 中的 K 值设置为 200 ,那么准确率是 0.8489","like_count":0},{"had_liked":false,"id":85616,"user_name":"周飞","can_delete":false,"product_type":"c1","uid":1073374,"ip_address":"","ucode":"F85FA236EB0C0D","user_header":"https://static001.geekbang.org/account/avatar/00/10/60/de/5c67895a.jpg","comment_is_top":false,"comment_ctime":1555144103,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555144103","product_id":100021701,"comment_content":"knn分类器的常用构造参数有：<br>1.n_neighbors  代表邻居的数量。<br>2.weights： 代表所有邻居的权重，其中 uniform 代表所有邻居权重相同， distance  代表权重是距离的倒数。还可以自定义。<br>3.algorithm: 计算邻居的方法，auto代表 根据数据的情况自动选择，kd_tree 是kd树，适用于维数不超过20的情况。ball_tree是球树，可以用于维度更大的情况。brute 是暴力搜索。<br>4.leaf_size:是kd树或者球树的叶子数量，默认是20.","like_count":0},{"had_liked":false,"id":81522,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1553935326,"is_pvip":false,"replies":[{"id":"64330","content":"对的 总结的不错","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577616965,"ip_address":"","comment_id":81522,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1553935326","product_id":100021701,"comment_content":"knn算法就是说，邻居大多数是什么你就是什么。<br>n_neighbors是邻居的数目<br><br>weights是权重<br>uniform是权重相同，求平均值<br>distance是根据距离的倒数<br>自定义<br><br>algorithm规定邻居选择的方式<br>auto根据数据自动选择<br>kd_tree，类似平衡二叉树，提高查找效率，多维空间的数据结构，一般不超过20维，对关键数据检索很方便<br>ball_tree，适用于维度大的<br>brute 暴力搜索，线性扫描<br><br>leaf_size是叶子数<br><br>k为200的时候准确率降低。<br>多项式分布没有负数，高斯分布可以有负数。<br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":445240,"discussion_content":"对的 总结的不错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577616965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":76967,"user_name":"叮当猫","can_delete":false,"product_type":"c1","uid":1360159,"ip_address":"","ucode":"175BB66517E21B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg","comment_is_top":false,"comment_ctime":1552788531,"is_pvip":false,"replies":[{"id":"64414","content":"对 K值大不一定效果好","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577618829,"ip_address":"","comment_id":76967,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1552788531","product_id":100021701,"comment_content":"n_neighbors =5时，准确率是0.9756，当n_neighbors =200时，准确率是0.8489，K值大不一定效果好，可能会欠拟合。<br><br>KNeighborsClassifier(n_neighbors=5, weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, leaf_size=30)<br>#参数n_neighbors，即knn中的k值，k值选取过小容易过拟合，k值选取过大鲁棒性强但欠拟合<br>#参数weights，用来确定邻居的权重，uniform表示所有邻居权重相同，distance代表权重是距离的倒数，与距离成反比，自定义函数表示可以自定义不同距离对应的权重。<br>#参数algorithm，用来规定计算邻居的方法，<br>#参数algorithm=auto根据数据情况自动选择合适的算法<br>#参数algorithm=kd_tree，kd树，多维空间的数据结构，方便对关键数据进行检索，适用维度少的情况，不超过20，大于20后效率下降<br>#参数algorithm=ball_tree，球树，和kd树医院都是多维空间数据结果，适用于维度大的情况<br>#参数algorithm=brute，暴力所搜，采用线性扫描，而不是通过构造树结果进行快速检索，训练集大的时候效率低<br>#参数leaf_size，代表构造kd树或球树时的叶子树，默认是30，调整它会影响到树的构造和搜索速度","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":443519,"discussion_content":"对 K值大不一定效果好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577618829,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72268,"user_name":"fancy","can_delete":false,"product_type":"c1","uid":1243166,"ip_address":"","ucode":"0C51F80B9C35B1","user_header":"https://static001.geekbang.org/account/avatar/00/12/f8/1e/0d5f8336.jpg","comment_is_top":false,"comment_ctime":1551554992,"is_pvip":false,"replies":[{"id":"64476","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619803,"ip_address":"","comment_id":72268,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551554992","product_id":100021701,"comment_content":"1. KNN分类器的常用构造参数：<br>n_neighbors := k = 5(默认下)<br>weights = &#39;uniform&#39;&#47;&#39;distance&#39;&#47;自定义函数<br>algorithm=&#39;auto&#39;&#47;&#39;ball_tree&#39;&#47;&#39;kd_tree&#39;&#47;&#39;brute&#39;<br>leaf_size<br>2. 功能函数<br>fit(train_x,train_y)--训练分类器<br>predict(test_x)--用分类器预测测试集<br>3.改变K值<br>当k=200时，预测结果的准确性从97%左右下降到了84.89%","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441516,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619803,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":71036,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551251325,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1551251325","product_id":100021701,"comment_content":"A1：<br>参数<br>\t1. n_neighbors<br>\t\tKNN中的K值，代表的是邻居的数量<br>\t\tK值如果比较小，会造成过拟合<br>\t\tK值比较大，无法将未知物体分类出来<br>\t\t默认值5<br>\t2. weights<br>\t\t确定邻居的权重<br>\t\t三种方式<br>\t\t\tweights=uniform<br>\t\t\t\t代表所有邻居的权重相同<br>\t\t\tweights=distance<br>\t\t\t\t代表权重是距离的倒数，即与距离成反比<br>\t\t\t自定义函数<br>\t\t\t\t自定义不同距离所对应的权重<br>\t3. algorithm<br>\t\t规定计算邻居的方法<br>\t\t四种方式<br>\t\t\talgorithm=auto<br>\t\t\t\t根据数据的情况自动选择适合的算法<br>\t\t\t\t默认情况选择auto<br>\t\t\talgorithm=kd_tree<br>\t\t\t\tKD树<br>\t\t\t\t多维空间的数据结构<br>\t\t\t\t方便对关键数据进行检索<br>\t\t\t\t适用于维度少的情况<br>\t\t\t\t\t不超过20维<br>\t\t\talgorithm=ball_tree<br>\t\t\t\t球树<br>\t\t\t\t多维空间<br>\t\t\t\t适用于维度大的情况<br>\t\t\talgorithm=brute<br>\t\t\t\t暴力搜索<br>\t\t\t\t线性扫描<br>\t\t\t\t训练集大，效率很低<br>\t4. leaf_size<br>\t\t构造KD树或球树时的叶子数<br>\t\t默认是30<br>\t\t影响到树的构造和搜索速度<br><br>A2：<br>knn = KNeighborsClassifier(n_neighbors=200)  <br><br>#KNN准确率: 0.8489","like_count":0},{"had_liked":false,"id":69716,"user_name":"TKbook","can_delete":false,"product_type":"c1","uid":1073829,"ip_address":"","ucode":"F6E0E99CC79059","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg","comment_is_top":false,"comment_ctime":1550816867,"is_pvip":false,"replies":[{"id":"64513","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621115,"ip_address":"","comment_id":69716,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1550816867","product_id":100021701,"comment_content":"from sklearn.metrics import accuracy_score<br># 创建KNN分类器<br>knn = KNeighborsClassifier(n_neighbors=200)<br>knn.fit(train_ss_x, train_y)<br>predict_y = knn.predict(test_ss_x)<br>print(&#39;KNN准确率：%.4lf&#39; % accuracy_score(predict_y, test_y))<br><br>KNN准确率：0.8489","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440241,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621115,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":69693,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1550811073,"is_pvip":false,"replies":[{"id":"64514","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621123,"ip_address":"","comment_id":69693,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1550811073","product_id":100021701,"comment_content":"1、项目中 KNN 分类器的常用构造参数，功能函数都有哪些<br>1）n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。<br>2）weights：是用来确定邻居的权重，有三种方式：<br>weights=uniform，代表所有邻居的权重相同；<br>weights=distance，代表权重是距离的倒数，即与距离成反比；<br>自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。<br>3）algorithm：用来规定计算邻居的方法，它有四种方式：<br>algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；<br>algorithm=kd_tree，也叫作 KD 树，适用于维度少的情况<br>algorithm=ball_tree，也叫作球树，球树更适用于维度大的情况；<br>algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。<br>4）leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。<br><br>2、KNN使用过程分为3步<br>1. 数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；<br>2. 准备阶段：需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。<br>3. 分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。<br><br>3、如果把 KNN 中的 K 值设置为 200，数据集还是 sklearn 中的手写数字数据集，再跑一遍程序，看看分类器的准确率是多少？<br>在创建KNN分类器的代码：<br>knn=KNeighborsClassifier(n_neighbors=200)<br>KNN 准确率：0.8489","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440229,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621123,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}