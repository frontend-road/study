{"id":79762,"title":"21丨朴素贝叶斯分类（下）：如何对文档进行分类？","content":"<p>我们上一节讲了朴素贝叶斯的工作原理，今天我们来讲下这些原理是如何指导实际业务的。</p><p>朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理NLP的工具。</p><p>今天我带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是sklearn这个机器学习神器。</p><h2>sklearn机器学习包</h2><p>sklearn的全称叫Scikit-learn，它给我们提供了3个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。</p><p>这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：</p><p><strong>高斯朴素贝叶斯</strong>：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。</p><p><strong>多项式朴素贝叶斯</strong>：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的TF-IDF值等。</p><p><strong>伯努利朴素贝叶斯</strong>：特征变量是布尔变量，符合0/1分布，在文档分类中特征是单词是否出现。</p><!-- [[[read_end]]] --><p>伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为1，否则为0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。</p><h2>什么是TF-IDF值呢？</h2><p>我在多项式朴素贝叶斯中提到了“词的TF-IDF值”，如何理解这个概念呢？</p><p>TF-IDF是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。</p><p>TF-IDF实际上是两个词组Term Frequency和Inverse Document Frequency的总称，两者缩写为TF和IDF，分别代表了词频和逆向文档频率。</p><p><strong>词频TF</strong>计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。</p><p><strong>逆向文档频率IDF</strong>，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF越大就代表该单词的区分度越大。</p><p><strong>所以TF-IDF实际上是词频TF和逆向文档频率IDF的乘积</strong>。这样我们倾向于找到TF和IDF取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。</p><h2>TF-IDF如何计算</h2><p>首先我们看下词频TF和逆向文档概率IDF的公式。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/4d/bc31ff1f31f9cd26144404221f705d4d.png?wh=276*87\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/b7/65/b7ad53560f61407e6964e7436da14365.png?wh=469*80\" alt=\"\"></p><p>为什么IDF的分母中，单词出现的文档数要加1呢？因为有些单词可能不会存在文档中，为了避免分母为0，统一给单词出现的文档数都加1。</p><p><strong>TF-IDF=TF*IDF。</strong></p><p>你可以看到，TF-IDF值就是TF与IDF的乘积,这样可以更准确地对文档进行分类。比如“我”这样的高频单词，虽然TF词频高，但是IDF值很低，整体的TF-IDF也不高。</p><p>我在这里举个例子。假设一个文件夹里一共有10篇文档，其中一篇文档有1000个单词，“this”这个单词出现20次，“bayes”出现了5次。“this”在所有文档中均出现过，而“bayes”只在2篇文档中出现过。我们来计算一下这两个词语的TF-IDF值。</p><p>针对“this”，计算TF-IDF值：</p><p><img src=\"https://static001.geekbang.org/resource/image/63/12/63abe3ce8aa0ea4a78ba537b5504df12.png?wh=226*77\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/b5/7e/b5ac88c4e2a71cc2d4ceef4c01e0ba7e.png?wh=403*76\" alt=\"\"></p><p>所以TF-IDF=0.02*(-0.0414)=-8.28e-4。</p><p>针对“bayes”，计算TF-IDF值：</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/8d/3bbe56a7b76513604bfe6b39b890dd8d.png?wh=241*53\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/1e/2e/1e8b7465b9949fe071e95aede172a52e.png?wh=375*50\" alt=\"\"></p><p>TF-IDF=0.005*0.5229=2.61e-3。</p><p>很明显“bayes”的TF-IDF值要大于“this”的TF-IDF值。这就说明用“bayes”这个单词做区分比单词“this”要好。</p><p><strong>如何求TF-IDF</strong></p><p>在sklearn中我们直接使用TfidfVectorizer类，它可以帮我们计算单词TF-IDF向量的值。在这个类中，取sklearn计算的对数log时，底数是e，不是10。</p><p>下面我来讲下如何创建TfidfVectorizer类。</p><h2>TfidfVectorizer类的创建：</h2><p>创建TfidfVectorizer的方法是：</p><pre><code>TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n</code></pre><p>我们在创建的时候，有两个构造参数，可以自定义停用词stop_words和规律规则token_pattern。需要注意的是传递的数据结构，停用词stop_words是一个列表List类型，而过滤规则token_pattern是正则表达式。</p><p>什么是停用词？停用词就是在分类中没有用的词，这些词一般词频TF高，但是IDF很低，起不到分类的作用。为了节省空间和计算时间，我们把这些词作为停用词stop words，告诉机器这些词不需要帮我计算。</p><p><img src=\"https://static001.geekbang.org/resource/image/04/e9/040723cc99b36e8ad7e45aa31e0690e9.png?wh=592*97\" alt=\"\"><br>\n当我们创建好TF-IDF向量类型时，可以用fit_transform帮我们计算，返回给我们文本矩阵，该矩阵表示了每个单词在每个文档中的TF-IDF值。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/43/0d2263fbc97beb520680382f08656b43.png?wh=468*71\" alt=\"\"><br>\n在我们进行fit_transform拟合模型后，我们可以得到更多的TF-IDF向量属性，比如，我们可以得到词汇的对应关系（字典类型）和向量的IDF值，当然也可以获取设置的停用词stop_words。</p><p><img src=\"https://static001.geekbang.org/resource/image/a4/6b/a42780a5bca0531e75a294b4e2fe356b.png?wh=468*128\" alt=\"\"><br>\n举个例子，假设我们有4个文档：</p><p>文档1：this is the bayes document；</p><p>文档2：this is the second second document；</p><p>文档3：and the third one；</p><p>文档4：is this the document。</p><p>现在想要计算文档里都有哪些单词，这些单词在不同文档中的TF-IDF值是多少呢？</p><p>首先我们创建TfidfVectorizer类：</p><pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vec = TfidfVectorizer()\n</code></pre><p>然后我们创建4个文档的列表documents，并让创建好的tfidf_vec对documents进行拟合，得到TF-IDF矩阵：</p><pre><code>documents = [\n    'this is the bayes document',\n    'this is the second second document',\n    'and the third one',\n    'is this the document'\n]\ntfidf_matrix = tfidf_vec.fit_transform(documents)\n</code></pre><p>输出文档中所有不重复的词：</p><pre><code>print('不重复的词:', tfidf_vec.get_feature_names())\n</code></pre><p>运行结果</p><pre><code>不重复的词: ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']\n</code></pre><p>输出每个单词对应的id值：</p><pre><code>print('每个单词的ID:', tfidf_vec.vocabulary_)\n</code></pre><p>运行结果</p><pre><code>每个单词的ID: {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n</code></pre><p>输出每个单词在每个文档中的TF-IDF值，向量里的顺序是按照词语的id顺序来的：</p><pre><code>print('每个单词的tfidf值:', tfidf_matrix.toarray())\n</code></pre><p>运行结果：</p><pre><code>每个单词的tfidf值: [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n  0.33040189 0.         0.40412895]\n [0.         0.         0.27230147 0.27230147 0.         0.85322574\n  0.22262429 0.         0.27230147]\n [0.55280532 0.         0.         0.         0.55280532 0.\n  0.28847675 0.55280532 0.        ]\n [0.         0.         0.52210862 0.52210862 0.         0.\n  0.42685801 0.         0.52210862]]\n</code></pre><h2>如何对文档进行分类</h2><p>如果我们要对文档进行分类，有两个重要的阶段：</p><p><img src=\"https://static001.geekbang.org/resource/image/25/c3/257e01f173e8bc78b37b71b2358ff7c3.jpg?wh=2460*1167\" alt=\"\"></p><ol>\n<li>\n<p><strong>基于分词的数据准备</strong>，包括分词、单词权重计算、去掉停用词；</p>\n</li>\n<li>\n<p><strong>应用朴素贝叶斯分类进行分类</strong>，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。</p>\n</li>\n</ol><p>下面，我分别对这些模块进行介绍。</p><p><strong>模块1：对文档进行分词</strong></p><p>在准备阶段里，最重要的就是分词。那么如果给文档进行分词呢？英文文档和中文文档所使用的分词工具不同。</p><p>在英文文档中，最常用的是NTLK包。NTLK包中包含了英文的停用词stop words、分词和标注方法。</p><pre><code>import nltk\nword_list = nltk.word_tokenize(text) #分词\nnltk.pos_tag(word_list) #标注单词的词性\n</code></pre><p>在中文文档中，最常用的是jieba包。jieba包中包含了中文的停用词stop words和分词方法。</p><pre><code>import jieba\nword_list = jieba.cut (text) #中文分词\n</code></pre><p><strong>模块2：加载停用词表</strong></p><p>我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在stop_words.txt，然后利用Python的文件读取函数读取文件，保存在stop_words数组中。</p><pre><code>stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]\n</code></pre><p><strong>模块3：计算单词的权重</strong></p><p>这里我们用到sklearn里的TfidfVectorizer类，上面我们介绍过它使用的方法。</p><p>直接创建TfidfVectorizer类，然后使用fit_transform方法进行拟合，得到TF-IDF特征空间features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间features。</p><pre><code>tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\nfeatures = tf.fit_transform(train_contents)\n</code></pre><p>这里max_df参数用来描述单词在文档中的最高出现率。假设max_df=0.5，代表一个单词在50%的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。</p><p>一般很少设置min_df，因为min_df通常都会很小。</p><p><strong>模块4：生成朴素贝叶斯分类器</strong></p><p>我们将特征训练集的特征空间train_features，以及训练集对应的分类train_labels传递给贝叶斯分类器clf，它会自动生成一个符合特征空间和对应分类的分类器。</p><p>这里我们采用的是多项式贝叶斯分类器，其中alpha为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为0。为了解决这个问题，我们需要做平滑处理。</p><p>当alpha=1时，使用的是Laplace平滑。Laplace平滑就是采用加1的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加1得到的概率变化可以忽略不计，也同时避免了零概率的问题。</p><p>当0&lt;alpha&lt;1时，使用的是Lidstone平滑。对于Lidstone平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置alpha为0.001。</p><pre><code># 多项式贝叶斯分类器\nfrom sklearn.naive_bayes import MultinomialNB  \nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n</code></pre><p><strong>模块5：使用生成的分类器做预测</strong></p><p>首先我们需要得到测试集的特征矩阵。</p><p>方法是用训练集的分词创建一个TfidfVectorizer类，使用同样的stop_words和max_df，然后用这个TfidfVectorizer类对测试集的内容进行fit_transform拟合，得到测试集的特征矩阵test_features。</p><pre><code>test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\ntest_features=test_tf.fit_transform(test_contents)\n</code></pre><p>然后我们用训练好的分类器对新数据做预测。</p><p>方法是使用predict函数，传入测试集的特征矩阵test_features，得到分类结果predicted_labels。predict函数做的工作就是求解所有后验概率并找出最大的那个。</p><pre><code>predicted_labels=clf.predict(test_features)\n</code></pre><p><strong>模块6：计算准确率</strong></p><p>计算准确率实际上是对分类模型的评估。我们可以调用sklearn中的metrics包，在metrics中提供了accuracy_score函数，方便我们对实际结果和预测的结果做对比，给出模型的准确率。</p><p>使用方法如下：</p><pre><code>from sklearn import metrics\nprint metrics.accuracy_score(test_labels, predicted_labels)\n</code></pre><h2>数据挖掘神器sklearn</h2><p>从数据挖掘的流程来看，一般包括了获取数据、数据清洗、模型训练、模型评估和模型部署这几个过程。</p><p>sklearn中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中，sklearn提供了metrics包，帮我们对预测结果与实际结果进行评估。</p><p>在文档分类的项目中，我们针对文档的特点，给出了基于分词的准备流程。一般来说NTLK包适用于英文文档，而jieba适用于中文文档。我们可以根据文档选择不同的包，对文档提取分词。这些分词就是贝叶斯分类中最重要的特征属性。基于这些分词，我们得到分词的权重，即特征矩阵。</p><p>通过特征矩阵与分类结果，我们就可以创建出朴素贝叶斯分类器，然后用分类器进行预测，最后预测结果与实际结果做对比即可以得到分类器在测试集上的准确率。</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/6e/2e2962ddb7e85a71e0cecb9c6d13306e.png?wh=874*286\" alt=\"\"></p><h2>练习题</h2><p>我已经讲了中文文档分类中的6个关键的模块，最后，我给你留一道对中文文档分类的练习题吧。</p><p>我将中文文档数据集上传到了GitHub上，<a href=\"https://github.com/cystanford/text_classification\">点击这里下载</a>。</p><p>数据说明：</p><ol>\n<li>文档共有4种类型：女性、体育、文学、校园；</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/67/28/67abc1783f7c4e7cd69194fafc514328.png?wh=585*120\" alt=\"\"></p><ol>\n<li>训练集放到train文件夹里，测试集放到test文件夹里，停用词放到stop文件夹里。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/0c/0f/0c374e3501cc28a24687bc030733050f.png?wh=580*97\" alt=\"\"><br>\n请使用朴素贝叶斯分类对训练集进行训练，并对测试集进行验证，并给出测试集的准确率。</p><p>最后你不妨思考一下，假设我们要判断一个人的性别，是通过身高、体重、鞋码、外貌等属性进行判断的，如果我们用朴素贝叶斯做分类，适合使用哪种朴素贝叶斯分类器？停用词的作用又是什么？</p><p>欢迎你在评论区进行留言，与我分享你的答案。也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事。</p><p></p>","neighbors":{"left":{"article_title":"20丨朴素贝叶斯分类（上）：如何让机器判断男女？","id":79294},"right":{"article_title":"22丨SVM（上）：如何用一根棍子将蓝红两色球分开？","id":79975}},"comments":[{"had_liked":false,"id":67318,"user_name":"北方","can_delete":false,"product_type":"c1","uid":1114754,"ip_address":"","ucode":"D04589B652AC29","user_header":"https://static001.geekbang.org/account/avatar/00/11/02/82/abed70a0.jpg","comment_is_top":true,"comment_ctime":1550129182,"is_pvip":false,"replies":[{"id":39233,"content":"正确，大家可以看下这份代码\n通过load_data加载数据，得到documents, labels\n通过train_fun进行训练和预测，得到测试集的准确率。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1561713387,"ip_address":"","comment_id":67318,"utype":2}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"#!&#47;usr&#47;bin&#47;env python\n# -*- coding:utf8 -*-\n# __author__ = &#39;北方姆Q&#39;\n# __datetime__ = 2019&#47;2&#47;14 14:04\n\nimport os\nimport jieba\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nLABEL_MAP = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}\n# 加载停用词\nwith open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;rb&#39;) as f:\n    STOP_WORDS = [line.strip() for line in f.readlines()]\n\n\ndef load_data(base_path):\n    &quot;&quot;&quot;\n    :param base_path: 基础路径\n    :return: 分词列表，标签列表\n    &quot;&quot;&quot;\n    documents = []\n    labels = []\n\n    for root, dirs, files in os.walk(base_path):    # 循环所有文件并进行分词打标\n        for file in files:\n            label = root.split(&#39;\\\\&#39;)[-1]        # 因为windows上路径符号自动转成\\了，所以要转义下\n            labels.append(label)\n            filename = os.path.join(root, file)\n            with open(filename, &#39;rb&#39;) as f:     # 因为字符集问题因此直接用二进制方式读取\n                content = f.read()\n                word_list = list(jieba.cut(content))\n                words = [wl for wl in word_list]\n                documents.append(&#39; &#39;.join(words))\n    return documents, labels\n\n\ndef train_fun(td, tl, testd, testl):\n    &quot;&quot;&quot;\n    构造模型并计算测试集准确率，字数限制变量名简写\n    :param td: 训练集数据\n    :param tl: 训练集标签\n    :param testd: 测试集数据\n    :param testl: 测试集标签\n    :return: 测试集准确率\n    &quot;&quot;&quot;\n    # 计算矩阵\n    tt = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)\n    tf = tt.fit_transform(td)\n    # 训练模型\n    clf = MultinomialNB(alpha=0.001).fit(tf, tl)\n    # 模型预测\n    test_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tt.vocabulary_)\n    test_features = test_tf.fit_transform(testd)\n    predicted_labels = clf.predict(test_features)\n    # 获取结果\n    x = metrics.accuracy_score(testl, predicted_labels)\n    return x\n\n\n# text classification与代码同目录下\ntrain_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#39;)\ntest_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#39;)\nx = train_fun(train_documents, train_labels, test_documents, test_labels)\nprint(x)","like_count":34},{"had_liked":false,"id":64694,"user_name":"szm","can_delete":false,"product_type":"c1","uid":1352697,"ip_address":"","ucode":"53E753D3D04431","user_header":"https://static001.geekbang.org/account/avatar/00/14/a3/f9/9180d6d1.jpg","comment_is_top":true,"comment_ctime":1548860544,"is_pvip":false,"replies":[{"id":39231,"content":"完整代码已上传到 https:&#47;&#47;github.com&#47;cystanford&#47;text_classification\n你也可以参考下专栏评论中其他人写的代码","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1561713312,"ip_address":"","comment_id":64694,"utype":2}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"需要完整代码，不然看不明白！","like_count":37,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438138,"discussion_content":"完整代码已上传到 https://github.com/cystanford/text_classification\n你也可以参考下专栏评论中其他人写的代码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561713312,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1707557,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/0e/25/d1eedaf5.jpg","nickname":"有育哥无奔波","note":"","ucode":"45461C13950FAD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":50393,"discussion_content":"github打不开","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573715737,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2360909,"avatar":"https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg","nickname":".","note":"","ucode":"B37F4749DE7E23","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1707557,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/0e/25/d1eedaf5.jpg","nickname":"有育哥无奔波","note":"","ucode":"45461C13950FAD","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340423,"discussion_content":"打不开去翻墙","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609999762,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":50393,"ip_address":"","group_id":0},"score":340423,"extra":""}]}]},{"had_liked":false,"id":64531,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":true,"comment_ctime":1548816011,"is_pvip":false,"replies":[{"id":39232,"content":"已上传到 https:&#47;&#47;github.com&#47;cystanford&#47;text_classification\n也有同学自己写出来了，你可以都运行一下。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1561713332,"ip_address":"","comment_id":64531,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"老师，能不能在答疑的时候给这道题的完整代码看看","like_count":18,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438138,"discussion_content":"完整代码已上传到 https://github.com/cystanford/text_classification\n你也可以参考下专栏评论中其他人写的代码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561713312,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1707557,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/0e/25/d1eedaf5.jpg","nickname":"有育哥无奔波","note":"","ucode":"45461C13950FAD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":50393,"discussion_content":"github打不开","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573715737,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2360909,"avatar":"https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg","nickname":".","note":"","ucode":"B37F4749DE7E23","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1707557,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/0e/25/d1eedaf5.jpg","nickname":"有育哥无奔波","note":"","ucode":"45461C13950FAD","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340423,"discussion_content":"打不开去翻墙","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609999762,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":50393,"ip_address":"","group_id":0},"score":340423,"extra":""}]}]},{"had_liked":false,"id":64628,"user_name":"姜戈","can_delete":false,"product_type":"c1","uid":1058972,"ip_address":"","ucode":"45C4BE93C8E4CC","user_header":"https://static001.geekbang.org/account/avatar/00/10/28/9c/73e76b19.jpg","comment_is_top":false,"comment_ctime":1548837560,"is_pvip":false,"replies":[{"id":64626,"content":"多谢姜戈，一起加油！","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622622,"ip_address":"","comment_id":64628,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"看过很多朴素贝叶斯原理和分类的讲解文章，很少能像前辈这样既有理论，又有实战的讲解，让大家既了解了理论知识，又有相应实际的操作经验可学，真的好棒，这个专栏，必须多多点赞，为老师加油！！！","like_count":16,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438108,"discussion_content":"多谢姜戈，一起加油！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622622,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66665,"user_name":"Geek_z0wqck","can_delete":false,"product_type":"c1","uid":1339763,"ip_address":"","ucode":"B531A2A2E89969","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIlShPdVFbIaUu0wtcuSrlkG9r5zBedPuuN4Pyichof0QnMvr4J0G4kykyA4cgrlYibZ6wZ6NJNevFQ/132","comment_is_top":false,"comment_ctime":1549978125,"is_pvip":false,"replies":[{"id":64582,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622000,"ip_address":"","comment_id":66665,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"https:&#47;&#47;github.com&#47;yourSprite&#47;AnalysisExcercise&#47;tree&#47;master&#47;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB","like_count":5,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438860,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622000,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131858,"user_name":"几何","can_delete":false,"product_type":"c1","uid":1109688,"ip_address":"","ucode":"2482EB23BE4584","user_header":"https://static001.geekbang.org/account/avatar/00/10/ee/b8/da245945.jpg","comment_is_top":false,"comment_ctime":1567939115,"is_pvip":false,"replies":[{"id":53377,"content":"我们做的大部分工作都是数据预处理，使用模型到是不复杂，这些分类模型使用的方式都差不多。\n你可以多做练习，先针对一个数据集，采用不同的分类算法看下准确率。这样就会理解，当数据确定的情况下，如何使用不同的分类算法。\n然后再针对不同的数据集，进行数据预处理：数据加载，数据探索，数据规范化等。数据预处理的技巧比较多，需要遇到不同的数据集，多做练习","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1570430615,"ip_address":"","comment_id":131858,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"老师，弱弱的说一句，代码感觉能看明白，但是不明白的是模型是如何使用的， 比如上一节和本节，都是只知道了准确率，但是对于有新的要处理的数据，如何做，怎么做好总是感觉差一点点东西。","like_count":4,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438860,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622000,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67225,"user_name":"Jack","can_delete":false,"product_type":"c1","uid":1298503,"ip_address":"","ucode":"F28E084BD9F1C6","user_header":"","comment_is_top":false,"comment_ctime":1550113191,"is_pvip":false,"replies":[{"id":64578,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621953,"ip_address":"","comment_id":67225,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"#!&#47;usr&#47;bin&#47;env python\n# coding: utf-8\n\nimport os\nimport jieba\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n# 1. 加载数据\n# 加载停用词表\nl_stopWords = set()\nwith open(&#39;.&#47;text_classification&#47;text_classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;) as l_f:\n    for l_line in l_f:\n        l_stopWords.add(l_line.strip())\n\nl_labelMap = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}\n# 加载训练数据和测试数据\ndef LoadData(filepath):\n    l_documents = []\n    l_labels = []\n    for root, dirs, files in os.walk(filepath):\n        for l_file in files:\n            l_label = root.split(&#39;&#47;&#39;)[-1]\n            l_filename = os.path.join(root, l_file)\n            \n            with open(l_filename, &#39;r&#39;) as l_f:\n                l_content = l_f.read()\n                l_wordlist = list(jieba.cut(l_content))\n                l_words = [item for item in l_wordlist if item not in l_stopWords]\n                l_documents.append(&#39; &#39;.join(l_words))\n                l_labels.append(l_labelMap[l_label])\n                \n    return l_documents, l_labels\n\nl_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;train&#39;)\nl_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;test&#39;)\n\n# # 2. 计算权重矩阵\nl_tfidfVec = TfidfVectorizer(max_df=0.5)\nl_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)\n\n# for item in l_tfidfVec.get_feature_names():\n#     print item\n# print l_tfidfVec.get_feature_names()\n# print l_tfidfVec.vocabulary_\nprint l_tfidfMatrix.toarray().shape\n\n# # 3. 朴素贝叶斯模型\n# ## 3.1 模型训练\nl_clf = MultinomialNB(alpha=0.001)\nl_clf.fit(l_tfidfMatrix, l_trainLabels)\n\n# ## 3.2 模型预测\nl_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)\nl_testFeature = l_testTfidf.fit_transform(l_testDocuments)\nl_hats = l_clf.predict(l_testFeature)\n\n# ## 3.3 模型评估\nfrom sklearn.metrics import accuracy_score\nprint accuracy_score(l_hats, l_testLabels)","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439104,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":261744,"user_name":"Jasmine","can_delete":false,"product_type":"c1","uid":2208886,"ip_address":"","ucode":"405CE5FEF52AB5","user_header":"https://static001.geekbang.org/account/avatar/00/21/b4/76/edb4988e.jpg","comment_is_top":false,"comment_ctime":1605510523,"is_pvip":false,"replies":[{"id":103002,"content":"fit_transform()是先调用fit()方法拟合模型，然后再调用transform()方法进行特征转换。如果在测试集上继续使用fit_transform()，将会再次调用fit重新拟合模型，这样可能会导致转换后的训练集和测试集出现差异。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1615995601,"ip_address":"","comment_id":261744,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"老师，我想请教一下，计算单词权重时，为什么train_features用的fit_transform方法，而test_feature用的是transform","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":509591,"discussion_content":"fit_transform()是先调用fit()方法拟合模型，然后再调用transform()方法进行特征转换。如果在测试集上继续使用fit_transform()，将会再次调用fit重新拟合模型，这样可能会导致转换后的训练集和测试集出现差异。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615995601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2344228,"avatar":"https://static001.geekbang.org/account/avatar/00/23/c5/24/182c710f.jpg","nickname":"lin","note":"","ucode":"356B5A4152B02D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":341112,"discussion_content":"1.如果test_feature要用train_features训练过的TfidfVectorizer进行transform，因为该TfidfVectorizer已经fit了，不能再fit，如果再fit_transform，后面用clf进行predict的时候会出错；（因为用test_feature重新fit后的TfidfVectorizer的vocabulary是不同的）\n2.如果要用fit_transform，可以重新创建个TfidfVectorizer类，并且设置该类的vocabulary参数为train_features训练过的TfidfVectorizer的vocabulary_属性，如tfidf2=TfidfVectorizer(vocabulary=last_tdifdvector.vocabulary_)\ntfidf2.fit_transform(test_feature)","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1610293146,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87074,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555507966,"is_pvip":false,"replies":[{"id":64248,"content":"take it easy","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577615300,"ip_address":"","comment_id":87074,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"最后面的代码太乱，很多都不知道从哪里来的，无法顺着看下去~~~","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447302,"discussion_content":"take it easy","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615300,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1387369,"avatar":"https://static001.geekbang.org/account/avatar/00/15/2b/69/7aace61c.jpg","nickname":"Answer Liu","note":"","ucode":"7F1E3D96EAD35C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":54950,"discussion_content":"哪个地方","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574328509,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":83106,"user_name":"Geek_hve78z","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1554426376,"is_pvip":false,"replies":[{"id":64300,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577616383,"ip_address":"","comment_id":83106,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"# -*- coding:utf8 -*-\n# 系统：mac \n\n# 1. 加载数据\n# 加载停用词表\n\nl_stopWords = [line.strip() for line in open(&#39;.&#47;text_classification-master&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).readlines()]  \n   \nl_labelMap = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}\n# 加载训练数据和测试数据\ndef LoadData(filepath):\n    l_documents = []\n    l_labels = []\n    \n    for root, dirs, files in os.walk(filepath):\n        for l_file in files:\n            if l_file==&#39;.DS_Store&#39;:\n                continue\n            l_label = root.split(&#39;&#47;&#39;)[-1]\n            l_filename = os.path.join(root, l_file)\n            \n            with open(l_filename, &#39;r&#39;,encoding=&#39;gbk&#39;) as l_f:\n                try:\n                    l_content = l_f.read()\n                except Exception as err:\n                    print(err)\n                    print(l_filename)\n                    continue\n                generator = jieba.cut(l_content)\n                words = &#39; &#39;.join(generator)\n                l_wordlist=words.split(&#39; &#39;)\n                l_words = [item for item in l_wordlist if item not in l_stopWords]\n                l_documents.append(&#39; &#39;.join(l_words))\n                l_labels.append(l_labelMap[l_label])\n                \n    return l_documents, l_labels\n\nl_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;train&#39;)\nl_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;test&#39;)\n\n# # 2. 计算权重矩阵\nl_tfidfVec = TfidfVectorizer(max_df=0.5)\nl_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)\n\nprint (l_tfidfMatrix.toarray().shape)\n\n# # 3. 朴素贝叶斯模型\n# ## 3.1 模型训练\nl_clf = MultinomialNB(alpha=0.001)\nl_clf.fit(l_tfidfMatrix, l_trainLabels)\n\n# ## 3.2 模型预测\nl_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)\nl_testFeature = l_testTfidf.fit_transform(l_testDocuments)\nl_hats = l_clf.predict(l_testFeature)\n\n# ## 3.3 模型评估\nfrom sklearn.metrics import accuracy_score\nprint (accuracy_score(l_hats, l_testLabels))","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447302,"discussion_content":"take it easy","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615300,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1387369,"avatar":"https://static001.geekbang.org/account/avatar/00/15/2b/69/7aace61c.jpg","nickname":"Answer Liu","note":"","ucode":"7F1E3D96EAD35C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":54950,"discussion_content":"哪个地方","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574328509,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67318,"user_name":"北方","can_delete":false,"product_type":"c1","uid":1114754,"ip_address":"","ucode":"D04589B652AC29","user_header":"https://static001.geekbang.org/account/avatar/00/11/02/82/abed70a0.jpg","comment_is_top":true,"comment_ctime":1550129182,"is_pvip":false,"replies":[{"id":39233,"content":"正确，大家可以看下这份代码\n通过load_data加载数据，得到documents, labels\n通过train_fun进行训练和预测，得到测试集的准确率。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1561713387,"ip_address":"","comment_id":67318,"utype":2}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"#!&#47;usr&#47;bin&#47;env python\n# -*- coding:utf8 -*-\n# __author__ = &#39;北方姆Q&#39;\n# __datetime__ = 2019&#47;2&#47;14 14:04\n\nimport os\nimport jieba\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nLABEL_MAP = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}\n# 加载停用词\nwith open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;rb&#39;) as f:\n    STOP_WORDS = [line.strip() for line in f.readlines()]\n\n\ndef load_data(base_path):\n    &quot;&quot;&quot;\n    :param base_path: 基础路径\n    :return: 分词列表，标签列表\n    &quot;&quot;&quot;\n    documents = []\n    labels = []\n\n    for root, dirs, files in os.walk(base_path):    # 循环所有文件并进行分词打标\n        for file in files:\n            label = root.split(&#39;\\\\&#39;)[-1]        # 因为windows上路径符号自动转成\\了，所以要转义下\n            labels.append(label)\n            filename = os.path.join(root, file)\n            with open(filename, &#39;rb&#39;) as f:     # 因为字符集问题因此直接用二进制方式读取\n                content = f.read()\n                word_list = list(jieba.cut(content))\n                words = [wl for wl in word_list]\n                documents.append(&#39; &#39;.join(words))\n    return documents, labels\n\n\ndef train_fun(td, tl, testd, testl):\n    &quot;&quot;&quot;\n    构造模型并计算测试集准确率，字数限制变量名简写\n    :param td: 训练集数据\n    :param tl: 训练集标签\n    :param testd: 测试集数据\n    :param testl: 测试集标签\n    :return: 测试集准确率\n    &quot;&quot;&quot;\n    # 计算矩阵\n    tt = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)\n    tf = tt.fit_transform(td)\n    # 训练模型\n    clf = MultinomialNB(alpha=0.001).fit(tf, tl)\n    # 模型预测\n    test_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tt.vocabulary_)\n    test_features = test_tf.fit_transform(testd)\n    predicted_labels = clf.predict(test_features)\n    # 获取结果\n    x = metrics.accuracy_score(testl, predicted_labels)\n    return x\n\n\n# text classification与代码同目录下\ntrain_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#39;)\ntest_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#39;)\nx = train_fun(train_documents, train_labels, test_documents, test_labels)\nprint(x)","like_count":34,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439141,"discussion_content":"正确，大家可以看下这份代码\n通过load_data加载数据，得到documents, labels\n通过train_fun进行训练和预测，得到测试集的准确率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561713387,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1797397,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/uF1xskrrH1VeESeK2Ah1UYOdjbllqTS7IkaVic57gPUX8T03j9xIwtBGFH15nOwFfZdkhrS7eTdYS80NVCExv4Q/132","nickname":"Geek_15bb76","note":"","ucode":"95B0BB2B9AC158","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":161399,"discussion_content":"在模型预测里面，还需要test_tf的步骤吗？不是直接test_features = tt.transform(testd)就可以了吗？多做一部test_tf的意义在哪里？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580896849,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64694,"user_name":"szm","can_delete":false,"product_type":"c1","uid":1352697,"ip_address":"","ucode":"53E753D3D04431","user_header":"https://static001.geekbang.org/account/avatar/00/14/a3/f9/9180d6d1.jpg","comment_is_top":true,"comment_ctime":1548860544,"is_pvip":false,"replies":[{"id":39231,"content":"完整代码已上传到 https:&#47;&#47;github.com&#47;cystanford&#47;text_classification\n你也可以参考下专栏评论中其他人写的代码","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1561713312,"ip_address":"","comment_id":64694,"utype":2}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"需要完整代码，不然看不明白！","like_count":37,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439141,"discussion_content":"正确，大家可以看下这份代码\n通过load_data加载数据，得到documents, labels\n通过train_fun进行训练和预测，得到测试集的准确率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561713387,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1797397,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/uF1xskrrH1VeESeK2Ah1UYOdjbllqTS7IkaVic57gPUX8T03j9xIwtBGFH15nOwFfZdkhrS7eTdYS80NVCExv4Q/132","nickname":"Geek_15bb76","note":"","ucode":"95B0BB2B9AC158","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":161399,"discussion_content":"在模型预测里面，还需要test_tf的步骤吗？不是直接test_features = tt.transform(testd)就可以了吗？多做一部test_tf的意义在哪里？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580896849,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64531,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":true,"comment_ctime":1548816011,"is_pvip":false,"replies":[{"id":39232,"content":"已上传到 https:&#47;&#47;github.com&#47;cystanford&#47;text_classification\n也有同学自己写出来了，你可以都运行一下。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1561713332,"ip_address":"","comment_id":64531,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"老师，能不能在答疑的时候给这道题的完整代码看看","like_count":18,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438052,"discussion_content":"已上传到 https://github.com/cystanford/text_classification\n也有同学自己写出来了，你可以都运行一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561713332,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64628,"user_name":"姜戈","can_delete":false,"product_type":"c1","uid":1058972,"ip_address":"","ucode":"45C4BE93C8E4CC","user_header":"https://static001.geekbang.org/account/avatar/00/10/28/9c/73e76b19.jpg","comment_is_top":false,"comment_ctime":1548837560,"is_pvip":false,"replies":[{"id":64626,"content":"多谢姜戈，一起加油！","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622622,"ip_address":"","comment_id":64628,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"看过很多朴素贝叶斯原理和分类的讲解文章，很少能像前辈这样既有理论，又有实战的讲解，让大家既了解了理论知识，又有相应实际的操作经验可学，真的好棒，这个专栏，必须多多点赞，为老师加油！！！","like_count":16,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438052,"discussion_content":"已上传到 https://github.com/cystanford/text_classification\n也有同学自己写出来了，你可以都运行一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561713332,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66665,"user_name":"Geek_z0wqck","can_delete":false,"product_type":"c1","uid":1339763,"ip_address":"","ucode":"B531A2A2E89969","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIlShPdVFbIaUu0wtcuSrlkG9r5zBedPuuN4Pyichof0QnMvr4J0G4kykyA4cgrlYibZ6wZ6NJNevFQ/132","comment_is_top":false,"comment_ctime":1549978125,"is_pvip":false,"replies":[{"id":64582,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622000,"ip_address":"","comment_id":66665,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"https:&#47;&#47;github.com&#47;yourSprite&#47;AnalysisExcercise&#47;tree&#47;master&#47;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB","like_count":5,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438108,"discussion_content":"多谢姜戈，一起加油！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622622,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131858,"user_name":"几何","can_delete":false,"product_type":"c1","uid":1109688,"ip_address":"","ucode":"2482EB23BE4584","user_header":"https://static001.geekbang.org/account/avatar/00/10/ee/b8/da245945.jpg","comment_is_top":false,"comment_ctime":1567939115,"is_pvip":false,"replies":[{"id":53377,"content":"我们做的大部分工作都是数据预处理，使用模型到是不复杂，这些分类模型使用的方式都差不多。\n你可以多做练习，先针对一个数据集，采用不同的分类算法看下准确率。这样就会理解，当数据确定的情况下，如何使用不同的分类算法。\n然后再针对不同的数据集，进行数据预处理：数据加载，数据探索，数据规范化等。数据预处理的技巧比较多，需要遇到不同的数据集，多做练习","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1570430615,"ip_address":"","comment_id":131858,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"老师，弱弱的说一句，代码感觉能看明白，但是不明白的是模型是如何使用的， 比如上一节和本节，都是只知道了准确率，但是对于有新的要处理的数据，如何做，怎么做好总是感觉差一点点东西。","like_count":4,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466648,"discussion_content":"我们做的大部分工作都是数据预处理，使用模型到是不复杂，这些分类模型使用的方式都差不多。\n你可以多做练习，先针对一个数据集，采用不同的分类算法看下准确率。这样就会理解，当数据确定的情况下，如何使用不同的分类算法。\n然后再针对不同的数据集，进行数据预处理：数据加载，数据探索，数据规范化等。数据预处理的技巧比较多，需要遇到不同的数据集，多做练习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570430615,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1252206,"avatar":"https://static001.geekbang.org/account/avatar/00/13/1b/6e/cd8fea9f.jpg","nickname":"RecordLiu","note":"","ucode":"41178CC21337FB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616325,"discussion_content":"同问，训练好模型后怎么调用，怎么对新数据进行预测呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682695373,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"福建","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67225,"user_name":"Jack","can_delete":false,"product_type":"c1","uid":1298503,"ip_address":"","ucode":"F28E084BD9F1C6","user_header":"","comment_is_top":false,"comment_ctime":1550113191,"is_pvip":false,"replies":[{"id":64578,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621953,"ip_address":"","comment_id":67225,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"#!&#47;usr&#47;bin&#47;env python\n# coding: utf-8\n\nimport os\nimport jieba\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n# 1. 加载数据\n# 加载停用词表\nl_stopWords = set()\nwith open(&#39;.&#47;text_classification&#47;text_classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;) as l_f:\n    for l_line in l_f:\n        l_stopWords.add(l_line.strip())\n\nl_labelMap = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}\n# 加载训练数据和测试数据\ndef LoadData(filepath):\n    l_documents = []\n    l_labels = []\n    for root, dirs, files in os.walk(filepath):\n        for l_file in files:\n            l_label = root.split(&#39;&#47;&#39;)[-1]\n            l_filename = os.path.join(root, l_file)\n            \n            with open(l_filename, &#39;r&#39;) as l_f:\n                l_content = l_f.read()\n                l_wordlist = list(jieba.cut(l_content))\n                l_words = [item for item in l_wordlist if item not in l_stopWords]\n                l_documents.append(&#39; &#39;.join(l_words))\n                l_labels.append(l_labelMap[l_label])\n                \n    return l_documents, l_labels\n\nl_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;train&#39;)\nl_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;test&#39;)\n\n# # 2. 计算权重矩阵\nl_tfidfVec = TfidfVectorizer(max_df=0.5)\nl_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)\n\n# for item in l_tfidfVec.get_feature_names():\n#     print item\n# print l_tfidfVec.get_feature_names()\n# print l_tfidfVec.vocabulary_\nprint l_tfidfMatrix.toarray().shape\n\n# # 3. 朴素贝叶斯模型\n# ## 3.1 模型训练\nl_clf = MultinomialNB(alpha=0.001)\nl_clf.fit(l_tfidfMatrix, l_trainLabels)\n\n# ## 3.2 模型预测\nl_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)\nl_testFeature = l_testTfidf.fit_transform(l_testDocuments)\nl_hats = l_clf.predict(l_testFeature)\n\n# ## 3.3 模型评估\nfrom sklearn.metrics import accuracy_score\nprint accuracy_score(l_hats, l_testLabels)","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466648,"discussion_content":"我们做的大部分工作都是数据预处理，使用模型到是不复杂，这些分类模型使用的方式都差不多。\n你可以多做练习，先针对一个数据集，采用不同的分类算法看下准确率。这样就会理解，当数据确定的情况下，如何使用不同的分类算法。\n然后再针对不同的数据集，进行数据预处理：数据加载，数据探索，数据规范化等。数据预处理的技巧比较多，需要遇到不同的数据集，多做练习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570430615,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1252206,"avatar":"https://static001.geekbang.org/account/avatar/00/13/1b/6e/cd8fea9f.jpg","nickname":"RecordLiu","note":"","ucode":"41178CC21337FB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616325,"discussion_content":"同问，训练好模型后怎么调用，怎么对新数据进行预测呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682695373,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"福建","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":261744,"user_name":"Jasmine","can_delete":false,"product_type":"c1","uid":2208886,"ip_address":"","ucode":"405CE5FEF52AB5","user_header":"https://static001.geekbang.org/account/avatar/00/21/b4/76/edb4988e.jpg","comment_is_top":false,"comment_ctime":1605510523,"is_pvip":false,"replies":[{"id":103002,"content":"fit_transform()是先调用fit()方法拟合模型，然后再调用transform()方法进行特征转换。如果在测试集上继续使用fit_transform()，将会再次调用fit重新拟合模型，这样可能会导致转换后的训练集和测试集出现差异。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1615995601,"ip_address":"","comment_id":261744,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"老师，我想请教一下，计算单词权重时，为什么train_features用的fit_transform方法，而test_feature用的是transform","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439104,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87074,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555507966,"is_pvip":false,"replies":[{"id":64248,"content":"take it easy","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577615300,"ip_address":"","comment_id":87074,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"最后面的代码太乱，很多都不知道从哪里来的，无法顺着看下去~~~","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":509591,"discussion_content":"fit_transform()是先调用fit()方法拟合模型，然后再调用transform()方法进行特征转换。如果在测试集上继续使用fit_transform()，将会再次调用fit重新拟合模型，这样可能会导致转换后的训练集和测试集出现差异。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615995601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2344228,"avatar":"https://static001.geekbang.org/account/avatar/00/23/c5/24/182c710f.jpg","nickname":"lin","note":"","ucode":"356B5A4152B02D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":341112,"discussion_content":"1.如果test_feature要用train_features训练过的TfidfVectorizer进行transform，因为该TfidfVectorizer已经fit了，不能再fit，如果再fit_transform，后面用clf进行predict的时候会出错；（因为用test_feature重新fit后的TfidfVectorizer的vocabulary是不同的）\n2.如果要用fit_transform，可以重新创建个TfidfVectorizer类，并且设置该类的vocabulary参数为train_features训练过的TfidfVectorizer的vocabulary_属性，如tfidf2=TfidfVectorizer(vocabulary=last_tdifdvector.vocabulary_)\ntfidf2.fit_transform(test_feature)","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1610293146,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":83106,"user_name":"Geek_hve78z","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1554426376,"is_pvip":false,"replies":[{"id":64300,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577616383,"ip_address":"","comment_id":83106,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"# -*- coding:utf8 -*-\n# 系统：mac \n\n# 1. 加载数据\n# 加载停用词表\n\nl_stopWords = [line.strip() for line in open(&#39;.&#47;text_classification-master&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).readlines()]  \n   \nl_labelMap = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}\n# 加载训练数据和测试数据\ndef LoadData(filepath):\n    l_documents = []\n    l_labels = []\n    \n    for root, dirs, files in os.walk(filepath):\n        for l_file in files:\n            if l_file==&#39;.DS_Store&#39;:\n                continue\n            l_label = root.split(&#39;&#47;&#39;)[-1]\n            l_filename = os.path.join(root, l_file)\n            \n            with open(l_filename, &#39;r&#39;,encoding=&#39;gbk&#39;) as l_f:\n                try:\n                    l_content = l_f.read()\n                except Exception as err:\n                    print(err)\n                    print(l_filename)\n                    continue\n                generator = jieba.cut(l_content)\n                words = &#39; &#39;.join(generator)\n                l_wordlist=words.split(&#39; &#39;)\n                l_words = [item for item in l_wordlist if item not in l_stopWords]\n                l_documents.append(&#39; &#39;.join(l_words))\n                l_labels.append(l_labelMap[l_label])\n                \n    return l_documents, l_labels\n\nl_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;train&#39;)\nl_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;test&#39;)\n\n# # 2. 计算权重矩阵\nl_tfidfVec = TfidfVectorizer(max_df=0.5)\nl_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)\n\nprint (l_tfidfMatrix.toarray().shape)\n\n# # 3. 朴素贝叶斯模型\n# ## 3.1 模型训练\nl_clf = MultinomialNB(alpha=0.001)\nl_clf.fit(l_tfidfMatrix, l_trainLabels)\n\n# ## 3.2 模型预测\nl_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)\nl_testFeature = l_testTfidf.fit_transform(l_testDocuments)\nl_hats = l_clf.predict(l_testFeature)\n\n# ## 3.3 模型评估\nfrom sklearn.metrics import accuracy_score\nprint (accuracy_score(l_hats, l_testLabels))","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":445863,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577616383,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65478,"user_name":"wzhan366","can_delete":false,"product_type":"c1","uid":1393863,"ip_address":"","ucode":"AF59C23A5C8E9D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqMiaIuXLFmXvVlnP9Do2icudO3JV6l5ueicWYYFhZb2ftT9XSKHFHJWa33XLnUKlCSs0JhvI7omF8Mg/132","comment_is_top":false,"comment_ctime":1549394497,"is_pvip":false,"replies":[{"id":64606,"content":"嗯 英文不会有编码问题","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622312,"ip_address":"","comment_id":65478,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"建议 大家先做英文版本，因为中文的unicode encode和decode不是很好弄，不利于中间步骤的可视化。如果对代码有疑惑，可以试试这个pipeline， sklearn 的。 不过，这个没有用NTLK。","like_count":2},{"had_liked":false,"id":250244,"user_name":"lemonlxn","can_delete":false,"product_type":"c1","uid":2184921,"ip_address":"","ucode":"520B4842201018","user_header":"","comment_is_top":false,"comment_ctime":1601001919,"is_pvip":false,"replies":[{"id":103014,"content":"总结的不错！很棒","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1616001122,"ip_address":"","comment_id":250244,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"此处没有对 sklearn 的tfidf进行讲解，为完善结果，总结如下：\n\n传统 TF_IDF = TF * IDF\n\n\tTF  = 该文档某单词出现次数 &#47; 该文档的总单词数\n\n\tIDF = loge(文档总数 &#47; 该单词出现的文档数 + 1)\n\n\nsklearn TF_IDF = TF * IDF\n\n\tTF  = 该文档某单词出现次数\n        IDF = loge((1 + n) &#47; (1 + dn)) + 1\n       \n        其中 n  为训练集文档数\n               dn 为测试集出现该单词的文档数\n        \n        如果norm=None 则结果为 \n          X.toarray()\n        \n        如果 norm=&#39;l2&#39; 则结果为：\n          X.toarray() &#47; np.sum(X.toarray() ** 2) ** 0.5\n        \n        如果 norm=&#39;l1&#39;则结果为\n          X.toarray() &#47; np.sum(X.toarray())","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438463,"discussion_content":"嗯 英文不会有编码问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622312,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":247598,"user_name":"黄争辉","can_delete":false,"product_type":"c1","uid":1070692,"ip_address":"","ucode":"970DD0C5D876FD","user_header":"https://static001.geekbang.org/account/avatar/00/10/56/64/7d80093c.jpg","comment_is_top":false,"comment_ctime":1599752761,"is_pvip":false,"replies":[{"id":103061,"content":"tfidf_vec.get_feature_names() 返回的是所有单词的单词列表，每个单词在该列表中的索引和tfidf_vec.vocabulary_返回的字典中每个单词的value一致。\n希望可以帮到你","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1616084130,"ip_address":"","comment_id":247598,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"这里百思不得解，\n训练集中，（4个文件的词混在一起）每个词的 TF-IDF值和分类都混在一起了，是如何实现分类器的呢？\n","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505446,"discussion_content":"tfidf_vec.get_feature_names() 返回的是所有单词的单词列表，每个单词在该列表中的索引和tfidf_vec.vocabulary_返回的字典中每个单词的value一致。\n希望可以帮到你","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616084130,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64829,"user_name":"Viola","can_delete":false,"product_type":"c1","uid":1245457,"ip_address":"","ucode":"7E2E1A7654D877","user_header":"","comment_is_top":false,"comment_ctime":1548920004,"is_pvip":false,"replies":[{"id":64621,"content":"https:&#47;&#47;github.com&#47;cystanford&#47;text_classification","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622556,"ip_address":"","comment_id":64829,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"有大量代码实现的 ，能否将github地址贴一下","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438196,"discussion_content":"https://github.com/cystanford/text_classification","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622556,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286435,"user_name":"许愿字节上岸冲冲冲","can_delete":false,"product_type":"c1","uid":2161241,"ip_address":"","ucode":"B5D82B71761BA9","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/SAicxHNJGBiavTgLlYXydetlV4S1Lr1icEbVVCY7LCFK0WVnP8udTWCwkibevnclMWOnfREugguzLM11aBunicIicyCg/132","comment_is_top":false,"comment_ctime":1617300012,"is_pvip":false,"replies":[{"id":104208,"content":"fit()方法是用于从一个训练集中学习模型参数；\ntransform()方法则是利用fit训练好的模型对数据进行转换，在TfidfVectorizer中，你可以把它理解为利用训练好的模型计算TF-IDF并返回；\nfit_transform()是高效的将模型的训练和转换合并到了一起，它会先调用fit训练模型参数，之后再调用transform对特征进行转换。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1617724735,"ip_address":"","comment_id":286435,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"老师，是不是可以理解为fit函数计算了idf的值和特征单词，transform函数计算了tf的值并与idf相乘算出了真正的tf-idf。所以train_features调用了fit_transform计算出了idf，test_features调用了transform则通过test_features计算出的TF值乘以了之前通过train_features算出的idf值得出来最终的TF-IDF矩阵？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517987,"discussion_content":"fit()方法是用于从一个训练集中学习模型参数；\ntransform()方法则是利用fit训练好的模型对数据进行转换，在TfidfVectorizer中，你可以把它理解为利用训练好的模型计算TF-IDF并返回；\nfit_transform()是高效的将模型的训练和转换合并到了一起，它会先调用fit训练模型参数，之后再调用transform对特征进行转换。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617724735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272235,"user_name":".","can_delete":false,"product_type":"c1","uid":2360909,"ip_address":"","ucode":"B37F4749DE7E23","user_header":"https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg","comment_is_top":false,"comment_ctime":1610000376,"is_pvip":false,"replies":[{"id":101924,"content":"可以将训练好的模型保存到本地。常用保存方法是调用pickle这个包来保存机器学习模型。当需要应用模型进行预测时，对特征使用和训练集相同的特征工程后，直接加载本地训练好的模型，使用predict方法进行预测。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1614364460,"ip_address":"","comment_id":272235,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"最后只是得出来了 分类的准确率？那么 再有一批新的文档数据 怎么把分类算法应用上去把新的文档数据进行分类开呢？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517987,"discussion_content":"fit()方法是用于从一个训练集中学习模型参数；\ntransform()方法则是利用fit训练好的模型对数据进行转换，在TfidfVectorizer中，你可以把它理解为利用训练好的模型计算TF-IDF并返回；\nfit_transform()是高效的将模型的训练和转换合并到了一起，它会先调用fit训练模型参数，之后再调用transform对特征进行转换。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617724735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263017,"user_name":"非同凡想","can_delete":false,"product_type":"c1","uid":1934969,"ip_address":"","ucode":"713FD449A49D5A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg","comment_is_top":false,"comment_ctime":1605948553,"is_pvip":false,"replies":[{"id":100891,"content":"继续坚持","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1612706469,"ip_address":"","comment_id":263017,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"交作业：\nimport jieba\nfrom sklearn.naive_bayes import MultinomialNB\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\n\nBASE_DIR=&#39;&#47;home&#47;zjtprince&#47;Documents&#47;text_classification&#47;text classification&#47;&#39;\n\ndef cut_text(filepath):\n    text = open(filepath,&#39;r&#39;,encoding=&#39;gb18030&#39;).read()\n    words = jieba.cut(text)\n    return &#39; &#39;.join(words) ;\n\ndef load_features_and_labels(dir , label):\n    features = []\n    labels = []\n    files = os.listdir(dir)\n    for file in files:\n        features.append(cut_text(dir + os.sep + file))\n        labels.append(label)\n    return features , labels\n\ndef build_word_list_and_label_list(type_name):\n    train_features1, labels1 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;女性&#39;, &#39;女性&#39;)\n    train_features2, labels2 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;文学&#39;, &#39;文学&#39;)\n    train_features3, labels3 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;校园&#39;, &#39;校园&#39;)\n    train_features4, labels4 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;体育&#39;, &#39;体育&#39;)\n    train_list = train_features1 + train_features2 + train_features3 + train_features4\n    label_list = labels1 + labels2 + labels3 + labels4\n    return train_list, label_list\n\ndef load_stop_words():\n    stop_words = open(BASE_DIR+&quot;stop&#47;stopword.txt&quot;, &#39;r&#39;,encoding=&#39;utf-8&#39;).read()\n    stop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;)\n    return stop_words.split(&#39;\\n&#39;)\n\nif __name__ == &#39;__main__&#39;:\n    stop_words = load_stop_words()\n    train_list, label_list = build_word_list_and_label_list(&#39;train&#39;)\n    test_list, test_labels = build_word_list_and_label_list(&#39;test&#39;)\n\n    vec = TfidfVectorizer(stop_words=stop_words)\n    vec.fit(train_list)\n    train_data = vec.transform(train_list)\n    test_data = vec.transform(test_list)\n\n    bayes = MultinomialNB(alpha=0.001)\n    ctf = bayes.fit(train_data, label_list)\n\n    predict = ctf.predict(test_data)\naccur = accuracy_score(predict,test_labels)\nprint(&quot;准确率为：%f&quot; , accur)","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513210,"discussion_content":"可以将训练好的模型保存到本地。常用保存方法是调用pickle这个包来保存机器学习模型。当需要应用模型进行预测时，对特征使用和训练集相同的特征工程后，直接加载本地训练好的模型，使用predict方法进行预测。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614364460,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2344228,"avatar":"https://static001.geekbang.org/account/avatar/00/23/c5/24/182c710f.jpg","nickname":"lin","note":"","ucode":"356B5A4152B02D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340766,"discussion_content":"课程代码的predicted_labels就是应用分类模型，对测试数据集进行预测后的结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610125364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":257392,"user_name":"与君共勉","can_delete":false,"product_type":"c1","uid":1763046,"ip_address":"","ucode":"1FB5EBFCF510A1","user_header":"","comment_is_top":false,"comment_ctime":1603954882,"is_pvip":false,"replies":[{"id":103007,"content":"tfidf_matrix = tfidf_vec.fit_transform(documents)\nprint(tfidf_matrix) \n得到的tfidf_matrix是TF-IDF构成的稀疏矩阵。如：稀疏矩阵的第一行结果：(0, 2)\t 0.4041289471327509，其中(0, 2)的0代表第1个句子中，2代表tfidf_vec.get_feature_names()返回的单词列表中第2个单词，0.4041289471327509代表TF-IDF值；","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1615998601,"ip_address":"","comment_id":257392,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"那个矩阵每个元素代表什么？每个单词的TF-IDF值吗？怎么感觉对不上。不同的行的又代表什么呢？详细解释一下这个矩阵","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510072,"discussion_content":"继续坚持","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612706469,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":250050,"user_name":"lemonlxn","can_delete":false,"product_type":"c1","uid":2184921,"ip_address":"","ucode":"520B4842201018","user_header":"","comment_is_top":false,"comment_ctime":1600915338,"is_pvip":false,"replies":[{"id":103015,"content":"不错！！！很棒","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1616001160,"ip_address":"","comment_id":250050,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"IDF = log2(total_document &#47; (appear_document + 1 )) \n\n由于 total_document 是固定的，如果希望 IDF 越大，则 appear_document 越小越好。\n\nappear_document 越小，那么IDF则会越大，该词区分度也会越大，该文档可以通过这个词，更容易区分其他文档","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506118,"discussion_content":"不错！！！很棒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616001160,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":202203,"user_name":"timeng27","can_delete":false,"product_type":"c1","uid":1914163,"ip_address":"","ucode":"C92D7484F7FF35","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAl7V1ibk8gX62W5I4SER2zbQAj3gy5icJlavGhnAmxENCia7QFm8lE3YBc5HOHvlyNVFz7rQKFQ7dA/132","comment_is_top":false,"comment_ctime":1585923442,"is_pvip":false,"replies":[{"id":103991,"content":"正确！加油哦！","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1617292586,"ip_address":"","comment_id":202203,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"我准备先大致过一遍，里面的练习还是不错的。\n不过这样听着音频学习，还是不习惯。\n\n三个朴素贝叶斯:\n1,高斯——数据连续变化；\n2,多项——常用文本分类，TF-IDF\n3,伯努利——0&#47;1","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490531,"discussion_content":"正确！加油哦！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617292586,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65478,"user_name":"wzhan366","can_delete":false,"product_type":"c1","uid":1393863,"ip_address":"","ucode":"AF59C23A5C8E9D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqMiaIuXLFmXvVlnP9Do2icudO3JV6l5ueicWYYFhZb2ftT9XSKHFHJWa33XLnUKlCSs0JhvI7omF8Mg/132","comment_is_top":false,"comment_ctime":1549394497,"is_pvip":false,"replies":[{"id":64606,"content":"嗯 英文不会有编码问题","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622312,"ip_address":"","comment_id":65478,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"建议 大家先做英文版本，因为中文的unicode encode和decode不是很好弄，不利于中间步骤的可视化。如果对代码有疑惑，可以试试这个pipeline， sklearn 的。 不过，这个没有用NTLK。","like_count":2,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438463,"discussion_content":"嗯 英文不会有编码问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622312,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":250244,"user_name":"lemonlxn","can_delete":false,"product_type":"c1","uid":2184921,"ip_address":"","ucode":"520B4842201018","user_header":"","comment_is_top":false,"comment_ctime":1601001919,"is_pvip":false,"replies":[{"id":103014,"content":"总结的不错！很棒","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1616001122,"ip_address":"","comment_id":250244,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"此处没有对 sklearn 的tfidf进行讲解，为完善结果，总结如下：\n\n传统 TF_IDF = TF * IDF\n\n\tTF  = 该文档某单词出现次数 &#47; 该文档的总单词数\n\n\tIDF = loge(文档总数 &#47; 该单词出现的文档数 + 1)\n\n\nsklearn TF_IDF = TF * IDF\n\n\tTF  = 该文档某单词出现次数\n        IDF = loge((1 + n) &#47; (1 + dn)) + 1\n       \n        其中 n  为训练集文档数\n               dn 为测试集出现该单词的文档数\n        \n        如果norm=None 则结果为 \n          X.toarray()\n        \n        如果 norm=&#39;l2&#39; 则结果为：\n          X.toarray() &#47; np.sum(X.toarray() ** 2) ** 0.5\n        \n        如果 norm=&#39;l1&#39;则结果为\n          X.toarray() &#47; np.sum(X.toarray())","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506161,"discussion_content":"总结的不错！很棒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616001122,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":247598,"user_name":"黄争辉","can_delete":false,"product_type":"c1","uid":1070692,"ip_address":"","ucode":"970DD0C5D876FD","user_header":"https://static001.geekbang.org/account/avatar/00/10/56/64/7d80093c.jpg","comment_is_top":false,"comment_ctime":1599752761,"is_pvip":false,"replies":[{"id":103061,"content":"tfidf_vec.get_feature_names() 返回的是所有单词的单词列表，每个单词在该列表中的索引和tfidf_vec.vocabulary_返回的字典中每个单词的value一致。\n希望可以帮到你","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1616084130,"ip_address":"","comment_id":247598,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"这里百思不得解，\n训练集中，（4个文件的词混在一起）每个词的 TF-IDF值和分类都混在一起了，是如何实现分类器的呢？\n","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506161,"discussion_content":"总结的不错！很棒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616001122,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64829,"user_name":"Viola","can_delete":false,"product_type":"c1","uid":1245457,"ip_address":"","ucode":"7E2E1A7654D877","user_header":"","comment_is_top":false,"comment_ctime":1548920004,"is_pvip":false,"replies":[{"id":64621,"content":"https:&#47;&#47;github.com&#47;cystanford&#47;text_classification","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622556,"ip_address":"","comment_id":64829,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"有大量代码实现的 ，能否将github地址贴一下","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505446,"discussion_content":"tfidf_vec.get_feature_names() 返回的是所有单词的单词列表，每个单词在该列表中的索引和tfidf_vec.vocabulary_返回的字典中每个单词的value一致。\n希望可以帮到你","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616084130,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286435,"user_name":"许愿字节上岸冲冲冲","can_delete":false,"product_type":"c1","uid":2161241,"ip_address":"","ucode":"B5D82B71761BA9","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/SAicxHNJGBiavTgLlYXydetlV4S1Lr1icEbVVCY7LCFK0WVnP8udTWCwkibevnclMWOnfREugguzLM11aBunicIicyCg/132","comment_is_top":false,"comment_ctime":1617300012,"is_pvip":false,"replies":[{"id":104208,"content":"fit()方法是用于从一个训练集中学习模型参数；\ntransform()方法则是利用fit训练好的模型对数据进行转换，在TfidfVectorizer中，你可以把它理解为利用训练好的模型计算TF-IDF并返回；\nfit_transform()是高效的将模型的训练和转换合并到了一起，它会先调用fit训练模型参数，之后再调用transform对特征进行转换。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1617724735,"ip_address":"","comment_id":286435,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"老师，是不是可以理解为fit函数计算了idf的值和特征单词，transform函数计算了tf的值并与idf相乘算出了真正的tf-idf。所以train_features调用了fit_transform计算出了idf，test_features调用了transform则通过test_features计算出的TF值乘以了之前通过train_features算出的idf值得出来最终的TF-IDF矩阵？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438196,"discussion_content":"https://github.com/cystanford/text_classification","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622556,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272235,"user_name":".","can_delete":false,"product_type":"c1","uid":2360909,"ip_address":"","ucode":"B37F4749DE7E23","user_header":"https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg","comment_is_top":false,"comment_ctime":1610000376,"is_pvip":false,"replies":[{"id":101924,"content":"可以将训练好的模型保存到本地。常用保存方法是调用pickle这个包来保存机器学习模型。当需要应用模型进行预测时，对特征使用和训练集相同的特征工程后，直接加载本地训练好的模型，使用predict方法进行预测。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1614364460,"ip_address":"","comment_id":272235,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"最后只是得出来了 分类的准确率？那么 再有一批新的文档数据 怎么把分类算法应用上去把新的文档数据进行分类开呢？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513210,"discussion_content":"可以将训练好的模型保存到本地。常用保存方法是调用pickle这个包来保存机器学习模型。当需要应用模型进行预测时，对特征使用和训练集相同的特征工程后，直接加载本地训练好的模型，使用predict方法进行预测。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614364460,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2344228,"avatar":"https://static001.geekbang.org/account/avatar/00/23/c5/24/182c710f.jpg","nickname":"lin","note":"","ucode":"356B5A4152B02D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340766,"discussion_content":"课程代码的predicted_labels就是应用分类模型，对测试数据集进行预测后的结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610125364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263017,"user_name":"非同凡想","can_delete":false,"product_type":"c1","uid":1934969,"ip_address":"","ucode":"713FD449A49D5A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg","comment_is_top":false,"comment_ctime":1605948553,"is_pvip":false,"replies":[{"id":100891,"content":"继续坚持","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1612706469,"ip_address":"","comment_id":263017,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"交作业：\nimport jieba\nfrom sklearn.naive_bayes import MultinomialNB\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\n\nBASE_DIR=&#39;&#47;home&#47;zjtprince&#47;Documents&#47;text_classification&#47;text classification&#47;&#39;\n\ndef cut_text(filepath):\n    text = open(filepath,&#39;r&#39;,encoding=&#39;gb18030&#39;).read()\n    words = jieba.cut(text)\n    return &#39; &#39;.join(words) ;\n\ndef load_features_and_labels(dir , label):\n    features = []\n    labels = []\n    files = os.listdir(dir)\n    for file in files:\n        features.append(cut_text(dir + os.sep + file))\n        labels.append(label)\n    return features , labels\n\ndef build_word_list_and_label_list(type_name):\n    train_features1, labels1 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;女性&#39;, &#39;女性&#39;)\n    train_features2, labels2 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;文学&#39;, &#39;文学&#39;)\n    train_features3, labels3 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;校园&#39;, &#39;校园&#39;)\n    train_features4, labels4 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;体育&#39;, &#39;体育&#39;)\n    train_list = train_features1 + train_features2 + train_features3 + train_features4\n    label_list = labels1 + labels2 + labels3 + labels4\n    return train_list, label_list\n\ndef load_stop_words():\n    stop_words = open(BASE_DIR+&quot;stop&#47;stopword.txt&quot;, &#39;r&#39;,encoding=&#39;utf-8&#39;).read()\n    stop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;)\n    return stop_words.split(&#39;\\n&#39;)\n\nif __name__ == &#39;__main__&#39;:\n    stop_words = load_stop_words()\n    train_list, label_list = build_word_list_and_label_list(&#39;train&#39;)\n    test_list, test_labels = build_word_list_and_label_list(&#39;test&#39;)\n\n    vec = TfidfVectorizer(stop_words=stop_words)\n    vec.fit(train_list)\n    train_data = vec.transform(train_list)\n    test_data = vec.transform(test_list)\n\n    bayes = MultinomialNB(alpha=0.001)\n    ctf = bayes.fit(train_data, label_list)\n\n    predict = ctf.predict(test_data)\naccur = accuracy_score(predict,test_labels)\nprint(&quot;准确率为：%f&quot; , accur)","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510072,"discussion_content":"继续坚持","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612706469,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":257392,"user_name":"与君共勉","can_delete":false,"product_type":"c1","uid":1763046,"ip_address":"","ucode":"1FB5EBFCF510A1","user_header":"","comment_is_top":false,"comment_ctime":1603954882,"is_pvip":false,"replies":[{"id":103007,"content":"tfidf_matrix = tfidf_vec.fit_transform(documents)\nprint(tfidf_matrix) \n得到的tfidf_matrix是TF-IDF构成的稀疏矩阵。如：稀疏矩阵的第一行结果：(0, 2)\t 0.4041289471327509，其中(0, 2)的0代表第1个句子中，2代表tfidf_vec.get_feature_names()返回的单词列表中第2个单词，0.4041289471327509代表TF-IDF值；","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1615998601,"ip_address":"","comment_id":257392,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"那个矩阵每个元素代表什么？每个单词的TF-IDF值吗？怎么感觉对不上。不同的行的又代表什么呢？详细解释一下这个矩阵","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508319,"discussion_content":"tfidf_matrix = tfidf_vec.fit_transform(documents)\nprint(tfidf_matrix) \n得到的tfidf_matrix是TF-IDF构成的稀疏矩阵。如：稀疏矩阵的第一行结果：(0, 2)\t 0.4041289471327509，其中(0, 2)的0代表第1个句子中，2代表tfidf_vec.get_feature_names()返回的单词列表中第2个单词，0.4041289471327509代表TF-IDF值；","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615998601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":250050,"user_name":"lemonlxn","can_delete":false,"product_type":"c1","uid":2184921,"ip_address":"","ucode":"520B4842201018","user_header":"","comment_is_top":false,"comment_ctime":1600915338,"is_pvip":false,"replies":[{"id":103015,"content":"不错！！！很棒","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1616001160,"ip_address":"","comment_id":250050,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"IDF = log2(total_document &#47; (appear_document + 1 )) \n\n由于 total_document 是固定的，如果希望 IDF 越大，则 appear_document 越小越好。\n\nappear_document 越小，那么IDF则会越大，该词区分度也会越大，该文档可以通过这个词，更容易区分其他文档","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508319,"discussion_content":"tfidf_matrix = tfidf_vec.fit_transform(documents)\nprint(tfidf_matrix) \n得到的tfidf_matrix是TF-IDF构成的稀疏矩阵。如：稀疏矩阵的第一行结果：(0, 2)\t 0.4041289471327509，其中(0, 2)的0代表第1个句子中，2代表tfidf_vec.get_feature_names()返回的单词列表中第2个单词，0.4041289471327509代表TF-IDF值；","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615998601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":202203,"user_name":"timeng27","can_delete":false,"product_type":"c1","uid":1914163,"ip_address":"","ucode":"C92D7484F7FF35","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAl7V1ibk8gX62W5I4SER2zbQAj3gy5icJlavGhnAmxENCia7QFm8lE3YBc5HOHvlyNVFz7rQKFQ7dA/132","comment_is_top":false,"comment_ctime":1585923442,"is_pvip":false,"replies":[{"id":103991,"content":"正确！加油哦！","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1617292586,"ip_address":"","comment_id":202203,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"我准备先大致过一遍，里面的练习还是不错的。\n不过这样听着音频学习，还是不习惯。\n\n三个朴素贝叶斯:\n1,高斯——数据连续变化；\n2,多项——常用文本分类，TF-IDF\n3,伯努利——0&#47;1","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506118,"discussion_content":"不错！！！很棒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616001160,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":150600,"user_name":"Ronnyz","can_delete":false,"product_type":"c1","uid":1488280,"ip_address":"","ucode":"9F34527B1D343D","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg","comment_is_top":false,"comment_ctime":1573566105,"is_pvip":false,"replies":[{"id":62670,"content":"慢慢调整，看下怎么优化","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577082688,"ip_address":"","comment_id":150600,"utype":1}],"discussion_count":3,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"练习题：\nacc_score :  0.755 比老师的代码运行的结果差了许多，不知道是什么原因呢\n思考题：\n身高、体重 ：这是连续型变量，适合高斯贝叶斯\n鞋码、外貌 ：这个一般用离散取值，适合多项式贝叶斯\n","like_count":0},{"had_liked":false,"id":112540,"user_name":"FeiFei","can_delete":false,"product_type":"c1","uid":1045586,"ip_address":"","ucode":"01CD655DD4E56C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg","comment_is_top":false,"comment_ctime":1562754523,"is_pvip":false,"replies":[{"id":41025,"content":"没有分类好的数据，你指的应该是测试集。训练数据都是做好分类的，也就是有标签，要不怎么做训练呢。测试集是未知的，可以用训练好的模型进行分类，然后和测试集的结果进行比对，得到准确率。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1562849578,"ip_address":"","comment_id":112540,"utype":1}],"discussion_count":2,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"我有个疑问，贝叶斯是基于训练数据来进行分类的，训练数据已经有标签了。\n但在没有分类好的数据前，怎么去把一些数据打上这个标签？\n感觉我陷入了一个死循环。\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474213,"discussion_content":"慢慢调整，看下怎么优化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577082688,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1747799,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/ab/57/a57266b0.jpg","nickname":"Handsome","note":"","ucode":"61EFF4A1BAA5C6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":138863,"discussion_content":"同学能分享一下练习的数据吗？我在GitHub上下载的不行，解压后有乱码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1579268351,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2285481,"avatar":"https://static001.geekbang.org/account/avatar/00/22/df/a9/bf395f99.jpg","nickname":"赵宁","note":"","ucode":"7E5C648BA67DAA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1747799,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/ab/57/a57266b0.jpg","nickname":"Handsome","note":"","ucode":"61EFF4A1BAA5C6","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":334033,"discussion_content":"链接：https://pan.baidu.com/s/1oL5f3AR4uGrQqtMQzz1gKg \n提取码：efoc (7天有效)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607698649,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":138863,"ip_address":"","group_id":0},"score":334033,"extra":""}]}]},{"had_liked":false,"id":78612,"user_name":"听妈妈的话","can_delete":false,"product_type":"c1","uid":1462417,"ip_address":"","ucode":"089D797A39C791","user_header":"https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg","comment_is_top":false,"comment_ctime":1553183079,"is_pvip":false,"replies":[{"id":64382,"content":"不错的分享","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577618065,"ip_address":"","comment_id":78612,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"我的代码位于：https:&#47;&#47;pastebin.com&#47;kqjXgy0c ，最终结果0.925\n注意: 中文分词，TfidfVectorizer增加一个参数：tokenizer=jieba.cut,（来自github jieba issue: https:&#47;&#47;github.com&#47;fxsjy&#47;jieba&#47;issues&#47;332）\n\n\ntrain_contents=[]\ntrain_labels=[]\ntest_contents=[]\ntest_labels=[]\n#  导入文件\nimport os\nimport io\nstart=os.listdir(r&#39;text classification&#47;train&#39;)\nfor item in start:\n    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;\n    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;\n    for file in os.listdir(test_path):\n        with open(test_path+file,encoding=&quot;GBK&quot;) as f:\n            test_contents.append(f.readline())\n            #print(test_contents)\n            test_labels.append(item)\n    for file in os.listdir(train_path):\n        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:\n            train_contents.append(f.readline())\n            train_labels.append(item)\nprint(len(train_contents),len(test_contents))\n \n# 导入stop word\nimport jieba\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB  \nstop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]\n \n# 分词方式使用jieba,计算单词的权重\ntf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_contents)\nprint(train_features.shape)\n \n模块 4：生成朴素贝叶斯分类器\n# 多项式贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n \n模块 5：使用生成的分类器做预测\ntest_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features=test_tf.fit_transform(test_contents)\n \nprint(test_features.shape)\npredicted_labels=clf.predict(test_features)\nprint(metrics.accuracy_score(test_labels, predicted_labels))\n \n# 最终结果0.925\n\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":457788,"discussion_content":"没有分类好的数据，你指的应该是测试集。训练数据都是做好分类的，也就是有标签，要不怎么做训练呢。测试集是未知的，可以用训练好的模型进行分类，然后和测试集的结果进行比对，得到准确率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562849578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2285481,"avatar":"https://static001.geekbang.org/account/avatar/00/22/df/a9/bf395f99.jpg","nickname":"赵宁","note":"","ucode":"7E5C648BA67DAA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333861,"discussion_content":"感觉老师没有回答想问的，训练集的标签应该是人工根据经验打上的？我也想知道……","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607655472,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":74421,"user_name":"小莫","can_delete":false,"product_type":"c1","uid":1367575,"ip_address":"","ucode":"031C73F866E43B","user_header":"https://static001.geekbang.org/account/avatar/00/14/de/17/a3b8f785.jpg","comment_is_top":false,"comment_ctime":1552222139,"is_pvip":false,"replies":[{"id":64442,"content":"我的GitHub上应该有 ","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619327,"ip_address":"","comment_id":74421,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"老师，完整代码能贴出来吗？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442570,"discussion_content":"我的GitHub上应该有 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619327,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":74263,"user_name":"三硝基甲苯","can_delete":false,"product_type":"c1","uid":1141929,"ip_address":"","ucode":"C492B058C2A5C0","user_header":"","comment_is_top":false,"comment_ctime":1552148831,"is_pvip":false,"replies":[{"id":64443,"content":"Good Sharing","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619348,"ip_address":"","comment_id":74263,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"import jieba\nimport glob\nimport io\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nclassification = [&quot;campus&quot;, &quot;female&quot;, &quot;sports&quot;, &quot;literature&quot;]\ntrain_files_list = []\ntest_files_list = []\ntrainpathprefix = &quot;.&#47;text_classification&#47;train&#47;&quot;\ntestpathprefix = &quot;.&#47;text_classification&#47;test&#47;&quot;\npathsuffix = &quot;&#47;*.txt&quot;\ntrain_label = []\ntest_label = []\ntrain_docments = []\ntest_docments = []\nstopword_path = &#39;.&#47;text_classification&#47;stop&#47;stopword.txt&#39;\n\nfor i in classification:\n    trainpathstr = trainpathprefix + i + pathsuffix\n    testpathstr = testpathprefix + i + pathsuffix\n    trainpathlist = glob.glob(trainpathstr)\n    lentrainlist = len(trainpathlist)\n    train_label += [i for j in range(lentrainlist)]\n    testpathlist = glob.glob(testpathstr)\n    lentestlist = len(testpathlist)\n    test_label += [i for j in range(lentestlist)]\n    train_files_list += trainpathlist\n    test_files_list += testpathlist\n\nfor i in train_files_list:\n    f = open(i, &#39;r&#39;)\n    content = f.readlines()[0]\n    contentlist = list(jieba.cut(content))\n    contentwithspace = &quot; &quot;.join(contentlist)\n    train_docments.append(contentwithspace)\n\nfor i in test_files_list:\n    f = open(i, &#39;r&#39;)\n    content = f.readlines()[0]\n    contentlist = list(jieba.cut(content))\n    contentwithspace = &#39; &#39;.join(contentlist)\n    test_docments.append(contentwithspace)\n\nstopwords = [l.strip(&#39;\\n&#39;) for l in io.open(stopword_path, encoding=&#39;utf-8&#39;).readlines()]\ntrain_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5)\ntrain_features = train_tf.fit_transform(train_docments)\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_label)\ntest_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5, vocabulary=train_tf.vocabulary_)\ntest_features = test_tf.fit_transform(test_docments)\npredicted_labels = clf.predict(test_features)\nprint(metrics.accuracy_score(test_label, predicted_labels))\n运动的300.txt文件因为字符问题手动修改了一下。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442500,"discussion_content":"Good Sharing","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619348,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72011,"user_name":"飞Lisa","can_delete":false,"product_type":"c1","uid":1397876,"ip_address":"","ucode":"D0FF5D6EDA9C38","user_header":"https://static001.geekbang.org/account/avatar/00/15/54/74/2959ff0b.jpg","comment_is_top":false,"comment_ctime":1551498398,"is_pvip":false,"replies":[{"id":64480,"content":"多谢Lisa，你也很不错\n比较推荐用Kaggle做练习，另外你可以平时多看看知乎，或者加入到兴趣小组一起来成长","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619888,"ip_address":"","comment_id":72011,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"我觉得老师你讲的，太好了！以前看机器学习总是看不进去，看了你的讲解真的让我提起了兴趣，一下子看了十几篇。然后请问一下老师，要继续锻炼机器学习的实战能力的话你推荐什么书或者课程或者练习项目？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441425,"discussion_content":"多谢Lisa，你也很不错\n比较推荐用Kaggle做练习，另外你可以平时多看看知乎，或者加入到兴趣小组一起来成长","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619888,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68336,"user_name":"一语中的","can_delete":false,"product_type":"c1","uid":1320112,"ip_address":"","ucode":"E1A0EFCEAD83B4","user_header":"https://static001.geekbang.org/account/avatar/00/14/24/b0/a6e0b03a.jpg","comment_is_top":false,"comment_ctime":1550485281,"is_pvip":false,"replies":[{"id":64538,"content":"加油！不错","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621452,"ip_address":"","comment_id":68336,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"前后看了3遍，我终于理解了，也可以自己敲出结果了！","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441425,"discussion_content":"多谢Lisa，你也很不错\n比较推荐用Kaggle做练习，另外你可以平时多看看知乎，或者加入到兴趣小组一起来成长","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619888,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66918,"user_name":"乔巴","can_delete":false,"product_type":"c1","uid":1150334,"ip_address":"","ucode":"B32ED596B7A6C7","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg","comment_is_top":false,"comment_ctime":1550041344,"is_pvip":false,"replies":[{"id":64580,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621966,"ip_address":"","comment_id":66918,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"#code:utf-8\nimport pandas as pd\nimport numpy as np\nimport io\nimport os\nimport jieba\n\ndef preprocess(path_name):\n    text_with_spaces=&quot;&quot;\n    textfile=open(path_name,&quot;r&quot;).read() \n    textcut=jieba.cut(textfile)\n    for word in textcut:\n        text_with_spaces+=word+&quot; &quot;\n    return text_with_spaces\n\ndef loadtrainset(path,classtag):\n    allfiles=os.listdir(path)\n    processed_textset=[]\n    allclasstags=[]\n    for thisfile in allfiles:\n        path_name=path+&quot;&#47;&quot;+thisfile\n        processed_textset.append(preprocess(path_name))\n        allclasstags.append(classtag)\n    return processed_textset,allclasstags\n\nstop_words = open(&#39;D:&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read()\nstop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;) \nstop_words = stop_words.split(&#39;\\n&#39;) \n\nprocessed_textdata1,class1=loadtrainset(&quot;D:&#47;train&#47;女性&quot;, &quot;女性&quot;)\nprocessed_textdata2,class2=loadtrainset(&quot;D:&#47;train&#47;体育&quot;, &quot;体育&quot;)\nprocessed_textdata3,class3=loadtrainset(&quot;D:&#47;train&#47;文学&quot;, &quot;文学&quot;)\nprocessed_textdata4,class4=loadtrainset(&quot;D:&#47;train&#47;校园&quot;, &quot;校园&quot;)\nintegrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4\nclasstags_list=class1+class2+class3+class4\nprint(integrated_train_data[0])\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(integrated_train_data)\n\ntrain_labels=[0]\n\nclf = MultinomialNB(alpha=0.01).fit(train_features, classtags_list)\n\ntest_textdata1,testClass1=loadtrainset(&quot;D:&#47;test&#47;女性&quot;, &quot;女性&quot;)\ntest_textdata2,testClass2=loadtrainset(&quot;D:&#47;test&#47;体育&quot;, &quot;体育&quot;)\ntest_textdata3,testClass3=loadtrainset(&quot;D:&#47;test&#47;文学&quot;, &quot;文学&quot;)\ntest_textdata4,testClass4=loadtrainset(&quot;D:&#47;test&#47;校园&quot;, &quot;校园&quot;)\nintegrated_test_data=test_textdata1+test_textdata2+test_textdata3+test_textdata4\nclasstags_list=testClass1+testClass2+testClass3+testClass4\n\ntest_tf = TfidfVectorizer( max_df=0.5)\ntest_features=tf.transform(integrated_test_data)\npredicted_labels=clf.predict(test_features)\n\nprint(metrics.accuracy_score(classtags_list, predicted_labels))\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439633,"discussion_content":"加油！不错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621452,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65266,"user_name":"雨先生的晴天","can_delete":false,"product_type":"c1","uid":1246015,"ip_address":"","ucode":"71850548322A1C","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/3f/09308258.jpg","comment_is_top":false,"comment_ctime":1549174144,"is_pvip":false,"replies":[{"id":64612,"content":"https:&#47;&#47;github.com&#47;cystanford","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622418,"ip_address":"","comment_id":65266,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"还是希望老师可以在github分享一下代码，练习题还是没有办法解","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438976,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621966,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":78610,"user_name":"听妈妈的话","can_delete":false,"product_type":"c1","uid":1462417,"ip_address":"","ucode":"089D797A39C791","user_header":"https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg","comment_is_top":false,"comment_ctime":1553182835,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"# 由于评论不支持markdown，代码放在https:&#47;&#47;pastebin.com&#47;kqjXgy0c\n\ntrain_contents=[]\ntrain_labels=[]\ntest_contents=[]\ntest_labels=[]\n#  导入文件\nimport os\nimport io\nstart=os.listdir(r&#39;text classification&#47;train&#39;)\nfor item in start:\n    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;\n    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;\n    for file in os.listdir(test_path):\n        with open(test_path+file,encoding=&quot;GBK&quot;) as f:\n            test_contents.append(f.readline())\n            #print(test_contents)\n            test_labels.append(item)\n    for file in os.listdir(train_path):\n        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:\n            train_contents.append(f.readline())\n            train_labels.append(item)\nprint(len(train_contents),len(test_contents))\n\n# 导入stop word\nimport jieba\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB  \nstop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]\n\n# 分词方式使用jieba,计算单词的权重\ntf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_contents)\nprint(train_features.shape)\n\n模块 4：生成朴素贝叶斯分类器\n# 多项式贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\n模块 5：使用生成的分类器做预测\ntest_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features=test_tf.fit_transform(test_contents)\n\nprint(test_features.shape)\npredicted_labels=clf.predict(test_features)\nprint(metrics.accuracy_score(test_labels, predicted_labels))\n\n# 最终结果0.925","like_count":5,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438380,"discussion_content":"https://github.com/cystanford","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622418,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":150600,"user_name":"Ronnyz","can_delete":false,"product_type":"c1","uid":1488280,"ip_address":"","ucode":"9F34527B1D343D","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg","comment_is_top":false,"comment_ctime":1573566105,"is_pvip":false,"replies":[{"id":62670,"content":"慢慢调整，看下怎么优化","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577082688,"ip_address":"","comment_id":150600,"utype":1}],"discussion_count":3,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"练习题：\nacc_score :  0.755 比老师的代码运行的结果差了许多，不知道是什么原因呢\n思考题：\n身高、体重 ：这是连续型变量，适合高斯贝叶斯\n鞋码、外貌 ：这个一般用离散取值，适合多项式贝叶斯\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474213,"discussion_content":"慢慢调整，看下怎么优化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577082688,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1747799,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/ab/57/a57266b0.jpg","nickname":"Handsome","note":"","ucode":"61EFF4A1BAA5C6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":138863,"discussion_content":"同学能分享一下练习的数据吗？我在GitHub上下载的不行，解压后有乱码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1579268351,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2285481,"avatar":"https://static001.geekbang.org/account/avatar/00/22/df/a9/bf395f99.jpg","nickname":"赵宁","note":"","ucode":"7E5C648BA67DAA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1747799,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/ab/57/a57266b0.jpg","nickname":"Handsome","note":"","ucode":"61EFF4A1BAA5C6","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":334033,"discussion_content":"链接：https://pan.baidu.com/s/1oL5f3AR4uGrQqtMQzz1gKg \n提取码：efoc (7天有效)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607698649,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":138863,"ip_address":"","group_id":0},"score":334033,"extra":""}]}]},{"had_liked":false,"id":112540,"user_name":"FeiFei","can_delete":false,"product_type":"c1","uid":1045586,"ip_address":"","ucode":"01CD655DD4E56C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg","comment_is_top":false,"comment_ctime":1562754523,"is_pvip":false,"replies":[{"id":41025,"content":"没有分类好的数据，你指的应该是测试集。训练数据都是做好分类的，也就是有标签，要不怎么做训练呢。测试集是未知的，可以用训练好的模型进行分类，然后和测试集的结果进行比对，得到准确率。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1562849578,"ip_address":"","comment_id":112540,"utype":1}],"discussion_count":2,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"我有个疑问，贝叶斯是基于训练数据来进行分类的，训练数据已经有标签了。\n但在没有分类好的数据前，怎么去把一些数据打上这个标签？\n感觉我陷入了一个死循环。\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":457788,"discussion_content":"没有分类好的数据，你指的应该是测试集。训练数据都是做好分类的，也就是有标签，要不怎么做训练呢。测试集是未知的，可以用训练好的模型进行分类，然后和测试集的结果进行比对，得到准确率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562849578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2285481,"avatar":"https://static001.geekbang.org/account/avatar/00/22/df/a9/bf395f99.jpg","nickname":"赵宁","note":"","ucode":"7E5C648BA67DAA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333861,"discussion_content":"感觉老师没有回答想问的，训练集的标签应该是人工根据经验打上的？我也想知道……","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607655472,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":78612,"user_name":"听妈妈的话","can_delete":false,"product_type":"c1","uid":1462417,"ip_address":"","ucode":"089D797A39C791","user_header":"https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg","comment_is_top":false,"comment_ctime":1553183079,"is_pvip":false,"replies":[{"id":64382,"content":"不错的分享","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577618065,"ip_address":"","comment_id":78612,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"我的代码位于：https:&#47;&#47;pastebin.com&#47;kqjXgy0c ，最终结果0.925\n注意: 中文分词，TfidfVectorizer增加一个参数：tokenizer=jieba.cut,（来自github jieba issue: https:&#47;&#47;github.com&#47;fxsjy&#47;jieba&#47;issues&#47;332）\n\n\ntrain_contents=[]\ntrain_labels=[]\ntest_contents=[]\ntest_labels=[]\n#  导入文件\nimport os\nimport io\nstart=os.listdir(r&#39;text classification&#47;train&#39;)\nfor item in start:\n    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;\n    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;\n    for file in os.listdir(test_path):\n        with open(test_path+file,encoding=&quot;GBK&quot;) as f:\n            test_contents.append(f.readline())\n            #print(test_contents)\n            test_labels.append(item)\n    for file in os.listdir(train_path):\n        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:\n            train_contents.append(f.readline())\n            train_labels.append(item)\nprint(len(train_contents),len(test_contents))\n \n# 导入stop word\nimport jieba\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB  \nstop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]\n \n# 分词方式使用jieba,计算单词的权重\ntf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_contents)\nprint(train_features.shape)\n \n模块 4：生成朴素贝叶斯分类器\n# 多项式贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n \n模块 5：使用生成的分类器做预测\ntest_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features=test_tf.fit_transform(test_contents)\n \nprint(test_features.shape)\npredicted_labels=clf.predict(test_features)\nprint(metrics.accuracy_score(test_labels, predicted_labels))\n \n# 最终结果0.925\n\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444165,"discussion_content":"不错的分享","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577618065,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":74421,"user_name":"小莫","can_delete":false,"product_type":"c1","uid":1367575,"ip_address":"","ucode":"031C73F866E43B","user_header":"https://static001.geekbang.org/account/avatar/00/14/de/17/a3b8f785.jpg","comment_is_top":false,"comment_ctime":1552222139,"is_pvip":false,"replies":[{"id":64442,"content":"我的GitHub上应该有 ","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619327,"ip_address":"","comment_id":74421,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"老师，完整代码能贴出来吗？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444165,"discussion_content":"不错的分享","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577618065,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":74263,"user_name":"三硝基甲苯","can_delete":false,"product_type":"c1","uid":1141929,"ip_address":"","ucode":"C492B058C2A5C0","user_header":"","comment_is_top":false,"comment_ctime":1552148831,"is_pvip":false,"replies":[{"id":64443,"content":"Good Sharing","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619348,"ip_address":"","comment_id":74263,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"import jieba\nimport glob\nimport io\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nclassification = [&quot;campus&quot;, &quot;female&quot;, &quot;sports&quot;, &quot;literature&quot;]\ntrain_files_list = []\ntest_files_list = []\ntrainpathprefix = &quot;.&#47;text_classification&#47;train&#47;&quot;\ntestpathprefix = &quot;.&#47;text_classification&#47;test&#47;&quot;\npathsuffix = &quot;&#47;*.txt&quot;\ntrain_label = []\ntest_label = []\ntrain_docments = []\ntest_docments = []\nstopword_path = &#39;.&#47;text_classification&#47;stop&#47;stopword.txt&#39;\n\nfor i in classification:\n    trainpathstr = trainpathprefix + i + pathsuffix\n    testpathstr = testpathprefix + i + pathsuffix\n    trainpathlist = glob.glob(trainpathstr)\n    lentrainlist = len(trainpathlist)\n    train_label += [i for j in range(lentrainlist)]\n    testpathlist = glob.glob(testpathstr)\n    lentestlist = len(testpathlist)\n    test_label += [i for j in range(lentestlist)]\n    train_files_list += trainpathlist\n    test_files_list += testpathlist\n\nfor i in train_files_list:\n    f = open(i, &#39;r&#39;)\n    content = f.readlines()[0]\n    contentlist = list(jieba.cut(content))\n    contentwithspace = &quot; &quot;.join(contentlist)\n    train_docments.append(contentwithspace)\n\nfor i in test_files_list:\n    f = open(i, &#39;r&#39;)\n    content = f.readlines()[0]\n    contentlist = list(jieba.cut(content))\n    contentwithspace = &#39; &#39;.join(contentlist)\n    test_docments.append(contentwithspace)\n\nstopwords = [l.strip(&#39;\\n&#39;) for l in io.open(stopword_path, encoding=&#39;utf-8&#39;).readlines()]\ntrain_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5)\ntrain_features = train_tf.fit_transform(train_docments)\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_label)\ntest_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5, vocabulary=train_tf.vocabulary_)\ntest_features = test_tf.fit_transform(test_docments)\npredicted_labels = clf.predict(test_features)\nprint(metrics.accuracy_score(test_label, predicted_labels))\n运动的300.txt文件因为字符问题手动修改了一下。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442570,"discussion_content":"我的GitHub上应该有 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619327,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72011,"user_name":"飞Lisa","can_delete":false,"product_type":"c1","uid":1397876,"ip_address":"","ucode":"D0FF5D6EDA9C38","user_header":"https://static001.geekbang.org/account/avatar/00/15/54/74/2959ff0b.jpg","comment_is_top":false,"comment_ctime":1551498398,"is_pvip":false,"replies":[{"id":64480,"content":"多谢Lisa，你也很不错\n比较推荐用Kaggle做练习，另外你可以平时多看看知乎，或者加入到兴趣小组一起来成长","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619888,"ip_address":"","comment_id":72011,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"我觉得老师你讲的，太好了！以前看机器学习总是看不进去，看了你的讲解真的让我提起了兴趣，一下子看了十几篇。然后请问一下老师，要继续锻炼机器学习的实战能力的话你推荐什么书或者课程或者练习项目？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442500,"discussion_content":"Good Sharing","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619348,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68336,"user_name":"一语中的","can_delete":false,"product_type":"c1","uid":1320112,"ip_address":"","ucode":"E1A0EFCEAD83B4","user_header":"https://static001.geekbang.org/account/avatar/00/14/24/b0/a6e0b03a.jpg","comment_is_top":false,"comment_ctime":1550485281,"is_pvip":false,"replies":[{"id":64538,"content":"加油！不错","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621452,"ip_address":"","comment_id":68336,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"前后看了3遍，我终于理解了，也可以自己敲出结果了！","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439633,"discussion_content":"加油！不错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621452,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66918,"user_name":"乔巴","can_delete":false,"product_type":"c1","uid":1150334,"ip_address":"","ucode":"B32ED596B7A6C7","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg","comment_is_top":false,"comment_ctime":1550041344,"is_pvip":false,"replies":[{"id":64580,"content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621966,"ip_address":"","comment_id":66918,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"#code:utf-8\nimport pandas as pd\nimport numpy as np\nimport io\nimport os\nimport jieba\n\ndef preprocess(path_name):\n    text_with_spaces=&quot;&quot;\n    textfile=open(path_name,&quot;r&quot;).read() \n    textcut=jieba.cut(textfile)\n    for word in textcut:\n        text_with_spaces+=word+&quot; &quot;\n    return text_with_spaces\n\ndef loadtrainset(path,classtag):\n    allfiles=os.listdir(path)\n    processed_textset=[]\n    allclasstags=[]\n    for thisfile in allfiles:\n        path_name=path+&quot;&#47;&quot;+thisfile\n        processed_textset.append(preprocess(path_name))\n        allclasstags.append(classtag)\n    return processed_textset,allclasstags\n\nstop_words = open(&#39;D:&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read()\nstop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;) \nstop_words = stop_words.split(&#39;\\n&#39;) \n\nprocessed_textdata1,class1=loadtrainset(&quot;D:&#47;train&#47;女性&quot;, &quot;女性&quot;)\nprocessed_textdata2,class2=loadtrainset(&quot;D:&#47;train&#47;体育&quot;, &quot;体育&quot;)\nprocessed_textdata3,class3=loadtrainset(&quot;D:&#47;train&#47;文学&quot;, &quot;文学&quot;)\nprocessed_textdata4,class4=loadtrainset(&quot;D:&#47;train&#47;校园&quot;, &quot;校园&quot;)\nintegrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4\nclasstags_list=class1+class2+class3+class4\nprint(integrated_train_data[0])\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(integrated_train_data)\n\ntrain_labels=[0]\n\nclf = MultinomialNB(alpha=0.01).fit(train_features, classtags_list)\n\ntest_textdata1,testClass1=loadtrainset(&quot;D:&#47;test&#47;女性&quot;, &quot;女性&quot;)\ntest_textdata2,testClass2=loadtrainset(&quot;D:&#47;test&#47;体育&quot;, &quot;体育&quot;)\ntest_textdata3,testClass3=loadtrainset(&quot;D:&#47;test&#47;文学&quot;, &quot;文学&quot;)\ntest_textdata4,testClass4=loadtrainset(&quot;D:&#47;test&#47;校园&quot;, &quot;校园&quot;)\nintegrated_test_data=test_textdata1+test_textdata2+test_textdata3+test_textdata4\nclasstags_list=testClass1+testClass2+testClass3+testClass4\n\ntest_tf = TfidfVectorizer( max_df=0.5)\ntest_features=tf.transform(integrated_test_data)\npredicted_labels=clf.predict(test_features)\n\nprint(metrics.accuracy_score(classtags_list, predicted_labels))\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438976,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621966,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65266,"user_name":"雨先生的晴天","can_delete":false,"product_type":"c1","uid":1246015,"ip_address":"","ucode":"71850548322A1C","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/3f/09308258.jpg","comment_is_top":false,"comment_ctime":1549174144,"is_pvip":false,"replies":[{"id":64612,"content":"https:&#47;&#47;github.com&#47;cystanford","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577622418,"ip_address":"","comment_id":65266,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"还是希望老师可以在github分享一下代码，练习题还是没有办法解","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438380,"discussion_content":"https://github.com/cystanford","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577622418,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":78610,"user_name":"听妈妈的话","can_delete":false,"product_type":"c1","uid":1462417,"ip_address":"","ucode":"089D797A39C791","user_header":"https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg","comment_is_top":false,"comment_ctime":1553182835,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"# 由于评论不支持markdown，代码放在https:&#47;&#47;pastebin.com&#47;kqjXgy0c\n\ntrain_contents=[]\ntrain_labels=[]\ntest_contents=[]\ntest_labels=[]\n#  导入文件\nimport os\nimport io\nstart=os.listdir(r&#39;text classification&#47;train&#39;)\nfor item in start:\n    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;\n    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;\n    for file in os.listdir(test_path):\n        with open(test_path+file,encoding=&quot;GBK&quot;) as f:\n            test_contents.append(f.readline())\n            #print(test_contents)\n            test_labels.append(item)\n    for file in os.listdir(train_path):\n        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:\n            train_contents.append(f.readline())\n            train_labels.append(item)\nprint(len(train_contents),len(test_contents))\n\n# 导入stop word\nimport jieba\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB  \nstop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]\n\n# 分词方式使用jieba,计算单词的权重\ntf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_contents)\nprint(train_features.shape)\n\n模块 4：生成朴素贝叶斯分类器\n# 多项式贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\n模块 5：使用生成的分类器做预测\ntest_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features=test_tf.fit_transform(test_contents)\n\nprint(test_features.shape)\npredicted_labels=clf.predict(test_features)\nprint(metrics.accuracy_score(test_labels, predicted_labels))\n\n# 最终结果0.925","like_count":5},{"had_liked":false,"id":72239,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1551538686,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"适合用高斯朴素贝叶斯。因为身高、体重、鞋码是连续变量。\n停用词的作用是去除无关词，减少重复计算。\n","like_count":5},{"had_liked":false,"id":64801,"user_name":"上官","can_delete":false,"product_type":"c1","uid":1353145,"ip_address":"","ucode":"037968A8E0C566","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAeZ2VCia2y3bW9N7EMfgBqX8WClXUydwaXDPcK7Bm3XaMnMKx7q5ffA0UuTeJmEusxtQAibf8djCA/132","comment_is_top":false,"comment_ctime":1548913946,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"print(&#39;不重复的词:&#39;, tfidf_vec.get_feature_names())\n运行结果：  不重复的词: [&#39;and&#39;, &#39;bayes&#39;, &#39;document&#39;, &#39;is&#39;, &#39;one&#39;, &#39;second&#39;, &#39;the&#39;, &#39;third&#39;, &#39;this&#39;]\n这明明就是打印所有词啊，有重复的啊\n","like_count":4},{"had_liked":false,"id":65183,"user_name":"王小王","can_delete":false,"product_type":"c1","uid":1038933,"ip_address":"","ucode":"212709B65D2D74","user_header":"https://static001.geekbang.org/account/avatar/00/0f/da/55/df35904f.jpg","comment_is_top":false,"comment_ctime":1549092748,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"能不能讲解下本堂课的练习题？","like_count":2},{"had_liked":false,"id":389300,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1712064224,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"语句输出文档中所有不重复的词的代码： print(&#39;不重复的词:&#39;, tfidf_vec.get_feature_names()) ，发生异常。\n在尝试使用`TfidfVectorizer`对象的`get_feature_names`方法时，遇到了`AttributeError: &#39;TfidfVectorizer&#39; object has no attribute &#39;get_feature_names&#39;`的错误。这个错误通常是因为在较新版本的`scikit-learn`中，`get_feature_names`方法已经被`get_feature_names_out`方法替代。\n-----\n将原代码是修改为： print(&#39;不重复的词:&#39;, tfidf_vec.get_feature_names_out()) 。","like_count":1},{"had_liked":false,"id":195276,"user_name":"Jonathan","can_delete":false,"product_type":"c1","uid":1614395,"ip_address":"","ucode":"3BFE37F7A76B62","user_header":"https://static001.geekbang.org/account/avatar/00/18/a2/3b/4e927009.jpg","comment_is_top":false,"comment_ctime":1585152860,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"老师，想请教一下文中计算TF-IDF的例子，从公式上看tfIdf_Matrix=tf*idf。我能准确计算出idf值，然后再依据tf公式计算出来的结果，怎么算也算不出例子中所给出的矩阵结果。所以，我的问题是ft是怎么算出来的。谢谢。","like_count":1,"discussions":[{"author":{"id":1387369,"avatar":"https://static001.geekbang.org/account/avatar/00/15/2b/69/7aace61c.jpg","nickname":"Answer Liu","note":"","ucode":"7F1E3D96EAD35C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":54896,"discussion_content":"这里类似于去重处理，重复的词也只保留一个","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574323419,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1488453,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132","nickname":"羊小看","note":"","ucode":"90F58F80A75520","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":39077,"discussion_content":"你说的重复是指什么？是说这里应该输出仅出现一次的单词？\n这里的不重复是说，所有单词放在一起，做一个去重处理，类似于集合","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571889012,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":76787,"user_name":"叮当猫","can_delete":false,"product_type":"c1","uid":1360159,"ip_address":"","ucode":"175BB66517E21B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg","comment_is_top":false,"comment_ctime":1552708359,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"#-coding=utf-8\nimport os\nimport pandas as pd\nimport jieba\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef load_data(path):\n    l_labels = []\n    l_documents = []\n    #os.walk返回三元组(root, dirs, files)\n    #root指的是当前正在遍历的这个文件夹本身的地址\n    #dirs是一个list，内容是该文件夹中所有的目录的名字\n    #files是一个list，内容是该文件夹中所有的文件，不包含子目录\n    for root, dirs, files in os.walk(path):\n        print root, dirs, files\n        for l_file in files:\n            l_label = root.split(&#39;&#47;&#39;)[-1]\n            l_filepath =  os.path.join(root, l_file)\n            with open(l_filepath, &#39;r&#39;) as l_f:\n                l_content = l_f.read()\n                l_words = &#39; &#39;.join(list(jieba.cut(l_content)) )\n                l_labels.append(l_label)\n                l_documents.append(l_words)\n    return l_documents, l_labels\n\n#第一步：对文档进行分词\ntrain_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#47;&#39;)\ntest_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#47;&#39;)\n\n#第二步：加载停用词\nSTOP_WORDS = [line.strip() for line in open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39; ,&#39;r&#39;).readlines()]\n\n#第三步：计算单词的权重\ntf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)\ntrain_features = tf.fit_transform(train_documents)\n\n#第四步：生成朴素贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\n#第五步：使用生成的分类器做预测\ntest_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features = test_tf.fit_transform(test_documents)\n\npredict_labels = clf.predict(test_features)\n\n#第六步：计算准确率\nprint metrics.accuracy_score(test_labels, predict_labels)","like_count":1},{"had_liked":false,"id":64830,"user_name":"堂吉诃德","can_delete":false,"product_type":"c1","uid":1357227,"ip_address":"","ucode":"6A6D3D4866729B","user_header":"https://static001.geekbang.org/account/avatar/00/14/b5/ab/30e1531e.jpg","comment_is_top":false,"comment_ctime":1548920100,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\ntest_features=test_tf.fit_transform(test_contents)\n\n老师你好！\n我之前按照文章的方法把训练集和测试集分开计算特征矩阵，之后用多项式NB模型训练后预测时报错：维度不匹配（dimension mismatch). \n\n目前我是先把训练集和测试集都分词完毕后把两边的数据都放在一起（共3306+200=3506条）使用一个TfidfVectorizer拟合后得到features再用切片分成两部分，之后能正常使用贝叶斯模型预测。当然这样测试集也损失了自身的特征。\n\n请问要使用什么方法能让模型正常预测呢？如果现在我想使用模型预测一个文本必须要把这个文本分词后转为和trian_features相同的长度。\n\n我开始复制的代码里TfidfVectorizer有一个vocabulary=train_vocabulary参数我没懂，请问下是使用这个参数保持长度相同吗？我试了传tf.vacabulary_进去没有作用。\n\n请老师指教。","like_count":1},{"had_liked":false,"id":64827,"user_name":"Viola","can_delete":false,"product_type":"c1","uid":1245457,"ip_address":"","ucode":"7E2E1A7654D877","user_header":"","comment_is_top":false,"comment_ctime":1548919932,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"贝叶斯在真实数据分析中，应用场景主要有哪些？老师能举例说下吗","like_count":1},{"had_liked":false,"id":64568,"user_name":"Rickie","can_delete":false,"product_type":"c1","uid":1352052,"ip_address":"","ucode":"F859B7837DFD18","user_header":"https://static001.geekbang.org/account/avatar/00/14/a1/74/3dfa4436.jpg","comment_is_top":false,"comment_ctime":1548824254,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"老师，token_pattern里的正则中（?u)是什么意思呀？","like_count":1},{"had_liked":false,"id":391978,"user_name":"初","can_delete":false,"product_type":"c1","uid":1187792,"ip_address":"上海","ucode":"E124D8A566DA43","user_header":"https://static001.geekbang.org/account/avatar/00/12/1f/d0/660502a4.jpg","comment_is_top":false,"comment_ctime":1719565174,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"import os\nimport jieba\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n\ndef cut_words(file_path):\n    cut_words_res = &#39;&#39;\n    with open(file_path, &#39;r&#39;, encoding=&#39;gb18030&#39;) as f:\n        text = f.read()\n    for word in jieba.cut(text):\n        cut_words_res += word + &#39; &#39;\n    return cut_words_res\n\n\ndef load_file(file_path):\n    word_lists = []\n    label_lists = []\n    for root, dirs, files in os.walk(file_path):\n        for file in files:\n            label = root.split(&#39;\\\\&#39;)[-1]\n            word_lists.append(cut_words(os.path.join(root, file)))\n            label_lists.append(label)\n    return word_lists,label_lists\n\n\ndir_path_test = &#39;D:\\\\下载\\\\学习素材\\\\机器学习\\\\text_classification-master\\\\text classification\\\\test&#39;\ndir_path_train = &#39;D:\\\\下载\\\\学习素材\\\\机器学习\\\\text_classification-master\\\\text classification\\\\train&#39;\ndir_path_stop = &#39;D:\\\\下载\\\\学习素材\\\\机器学习\\\\text_classification-master\\\\text classification\\\\stop\\\\stopword.txt&#39;\n\n\n# 列表构建-训练集、测试集\ntrain_words_list, train_labels = load_file(dir_path_train)\ntest_words_list, test_labels = load_file(dir_path_test)\n# 停用词列表\nstop_words = open(dir_path_stop, &#39;r&#39;, encoding=&#39;utf-8-sig&#39;).read()\nstop_words = stop_words.split(&#39;\\n&#39;)\n# 计算权重\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_words_list)\ntest_features = tf.transform(test_words_list)\n# 生成朴素贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\npredicted_labels = clf.predict(test_features)\n\nprint(metrics.accuracy_score(test_labels, predicted_labels))\n\n那个gb18030还真不知道从哪里挖出来，尝试了下让chardet检测，检测出来是None....","like_count":0},{"had_liked":false,"id":72239,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1551538686,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"适合用高斯朴素贝叶斯。因为身高、体重、鞋码是连续变量。\n停用词的作用是去除无关词，减少重复计算。\n","like_count":5},{"had_liked":false,"id":64801,"user_name":"上官","can_delete":false,"product_type":"c1","uid":1353145,"ip_address":"","ucode":"037968A8E0C566","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAeZ2VCia2y3bW9N7EMfgBqX8WClXUydwaXDPcK7Bm3XaMnMKx7q5ffA0UuTeJmEusxtQAibf8djCA/132","comment_is_top":false,"comment_ctime":1548913946,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"print(&#39;不重复的词:&#39;, tfidf_vec.get_feature_names())\n运行结果：  不重复的词: [&#39;and&#39;, &#39;bayes&#39;, &#39;document&#39;, &#39;is&#39;, &#39;one&#39;, &#39;second&#39;, &#39;the&#39;, &#39;third&#39;, &#39;this&#39;]\n这明明就是打印所有词啊，有重复的啊\n","like_count":4,"discussions":[{"author":{"id":1387369,"avatar":"https://static001.geekbang.org/account/avatar/00/15/2b/69/7aace61c.jpg","nickname":"Answer Liu","note":"","ucode":"7F1E3D96EAD35C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":54896,"discussion_content":"这里类似于去重处理，重复的词也只保留一个","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574323419,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1488453,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132","nickname":"羊小看","note":"","ucode":"90F58F80A75520","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":39077,"discussion_content":"你说的重复是指什么？是说这里应该输出仅出现一次的单词？\n这里的不重复是说，所有单词放在一起，做一个去重处理，类似于集合","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571889012,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65183,"user_name":"王小王","can_delete":false,"product_type":"c1","uid":1038933,"ip_address":"","ucode":"212709B65D2D74","user_header":"https://static001.geekbang.org/account/avatar/00/0f/da/55/df35904f.jpg","comment_is_top":false,"comment_ctime":1549092748,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"能不能讲解下本堂课的练习题？","like_count":2},{"had_liked":false,"id":389300,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1712064224,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"语句输出文档中所有不重复的词的代码： print(&#39;不重复的词:&#39;, tfidf_vec.get_feature_names()) ，发生异常。\n在尝试使用`TfidfVectorizer`对象的`get_feature_names`方法时，遇到了`AttributeError: &#39;TfidfVectorizer&#39; object has no attribute &#39;get_feature_names&#39;`的错误。这个错误通常是因为在较新版本的`scikit-learn`中，`get_feature_names`方法已经被`get_feature_names_out`方法替代。\n-----\n将原代码是修改为： print(&#39;不重复的词:&#39;, tfidf_vec.get_feature_names_out()) 。","like_count":1},{"had_liked":false,"id":195276,"user_name":"Jonathan","can_delete":false,"product_type":"c1","uid":1614395,"ip_address":"","ucode":"3BFE37F7A76B62","user_header":"https://static001.geekbang.org/account/avatar/00/18/a2/3b/4e927009.jpg","comment_is_top":false,"comment_ctime":1585152860,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"老师，想请教一下文中计算TF-IDF的例子，从公式上看tfIdf_Matrix=tf*idf。我能准确计算出idf值，然后再依据tf公式计算出来的结果，怎么算也算不出例子中所给出的矩阵结果。所以，我的问题是ft是怎么算出来的。谢谢。","like_count":1,"discussions":[{"author":{"id":1135420,"avatar":"https://static001.geekbang.org/account/avatar/00/11/53/3c/c86e3052.jpg","nickname":"猛仔","note":"","ucode":"7EFC244D8726D7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":224709,"discussion_content":"我也算不出来例子中所给出的矩阵结果，感觉可能是sklearn包内又对数据做了某种规范化处理？求解","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586324638,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":76787,"user_name":"叮当猫","can_delete":false,"product_type":"c1","uid":1360159,"ip_address":"","ucode":"175BB66517E21B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg","comment_is_top":false,"comment_ctime":1552708359,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"#-coding=utf-8\nimport os\nimport pandas as pd\nimport jieba\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef load_data(path):\n    l_labels = []\n    l_documents = []\n    #os.walk返回三元组(root, dirs, files)\n    #root指的是当前正在遍历的这个文件夹本身的地址\n    #dirs是一个list，内容是该文件夹中所有的目录的名字\n    #files是一个list，内容是该文件夹中所有的文件，不包含子目录\n    for root, dirs, files in os.walk(path):\n        print root, dirs, files\n        for l_file in files:\n            l_label = root.split(&#39;&#47;&#39;)[-1]\n            l_filepath =  os.path.join(root, l_file)\n            with open(l_filepath, &#39;r&#39;) as l_f:\n                l_content = l_f.read()\n                l_words = &#39; &#39;.join(list(jieba.cut(l_content)) )\n                l_labels.append(l_label)\n                l_documents.append(l_words)\n    return l_documents, l_labels\n\n#第一步：对文档进行分词\ntrain_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#47;&#39;)\ntest_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#47;&#39;)\n\n#第二步：加载停用词\nSTOP_WORDS = [line.strip() for line in open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39; ,&#39;r&#39;).readlines()]\n\n#第三步：计算单词的权重\ntf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)\ntrain_features = tf.fit_transform(train_documents)\n\n#第四步：生成朴素贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\n#第五步：使用生成的分类器做预测\ntest_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features = test_tf.fit_transform(test_documents)\n\npredict_labels = clf.predict(test_features)\n\n#第六步：计算准确率\nprint metrics.accuracy_score(test_labels, predict_labels)","like_count":1},{"had_liked":false,"id":64830,"user_name":"堂吉诃德","can_delete":false,"product_type":"c1","uid":1357227,"ip_address":"","ucode":"6A6D3D4866729B","user_header":"https://static001.geekbang.org/account/avatar/00/14/b5/ab/30e1531e.jpg","comment_is_top":false,"comment_ctime":1548920100,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\ntest_features=test_tf.fit_transform(test_contents)\n\n老师你好！\n我之前按照文章的方法把训练集和测试集分开计算特征矩阵，之后用多项式NB模型训练后预测时报错：维度不匹配（dimension mismatch). \n\n目前我是先把训练集和测试集都分词完毕后把两边的数据都放在一起（共3306+200=3506条）使用一个TfidfVectorizer拟合后得到features再用切片分成两部分，之后能正常使用贝叶斯模型预测。当然这样测试集也损失了自身的特征。\n\n请问要使用什么方法能让模型正常预测呢？如果现在我想使用模型预测一个文本必须要把这个文本分词后转为和trian_features相同的长度。\n\n我开始复制的代码里TfidfVectorizer有一个vocabulary=train_vocabulary参数我没懂，请问下是使用这个参数保持长度相同吗？我试了传tf.vacabulary_进去没有作用。\n\n请老师指教。","like_count":1},{"had_liked":false,"id":64827,"user_name":"Viola","can_delete":false,"product_type":"c1","uid":1245457,"ip_address":"","ucode":"7E2E1A7654D877","user_header":"","comment_is_top":false,"comment_ctime":1548919932,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"贝叶斯在真实数据分析中，应用场景主要有哪些？老师能举例说下吗","like_count":1},{"had_liked":false,"id":64568,"user_name":"Rickie","can_delete":false,"product_type":"c1","uid":1352052,"ip_address":"","ucode":"F859B7837DFD18","user_header":"https://static001.geekbang.org/account/avatar/00/14/a1/74/3dfa4436.jpg","comment_is_top":false,"comment_ctime":1548824254,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"老师，token_pattern里的正则中（?u)是什么意思呀？","like_count":1},{"had_liked":false,"id":391978,"user_name":"初","can_delete":false,"product_type":"c1","uid":1187792,"ip_address":"上海","ucode":"E124D8A566DA43","user_header":"https://static001.geekbang.org/account/avatar/00/12/1f/d0/660502a4.jpg","comment_is_top":false,"comment_ctime":1719565174,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100021701,"comment_content":"import os\nimport jieba\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n\ndef cut_words(file_path):\n    cut_words_res = &#39;&#39;\n    with open(file_path, &#39;r&#39;, encoding=&#39;gb18030&#39;) as f:\n        text = f.read()\n    for word in jieba.cut(text):\n        cut_words_res += word + &#39; &#39;\n    return cut_words_res\n\n\ndef load_file(file_path):\n    word_lists = []\n    label_lists = []\n    for root, dirs, files in os.walk(file_path):\n        for file in files:\n            label = root.split(&#39;\\\\&#39;)[-1]\n            word_lists.append(cut_words(os.path.join(root, file)))\n            label_lists.append(label)\n    return word_lists,label_lists\n\n\ndir_path_test = &#39;D:\\\\下载\\\\学习素材\\\\机器学习\\\\text_classification-master\\\\text classification\\\\test&#39;\ndir_path_train = &#39;D:\\\\下载\\\\学习素材\\\\机器学习\\\\text_classification-master\\\\text classification\\\\train&#39;\ndir_path_stop = &#39;D:\\\\下载\\\\学习素材\\\\机器学习\\\\text_classification-master\\\\text classification\\\\stop\\\\stopword.txt&#39;\n\n\n# 列表构建-训练集、测试集\ntrain_words_list, train_labels = load_file(dir_path_train)\ntest_words_list, test_labels = load_file(dir_path_test)\n# 停用词列表\nstop_words = open(dir_path_stop, &#39;r&#39;, encoding=&#39;utf-8-sig&#39;).read()\nstop_words = stop_words.split(&#39;\\n&#39;)\n# 计算权重\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_words_list)\ntest_features = tf.transform(test_words_list)\n# 生成朴素贝叶斯分类器\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\npredicted_labels = clf.predict(test_features)\n\nprint(metrics.accuracy_score(test_labels, predicted_labels))\n\n那个gb18030还真不知道从哪里挖出来，尝试了下让chardet检测，检测出来是None....","like_count":0,"discussions":[{"author":{"id":1135420,"avatar":"https://static001.geekbang.org/account/avatar/00/11/53/3c/c86e3052.jpg","nickname":"猛仔","note":"","ucode":"7EFC244D8726D7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":224709,"discussion_content":"我也算不出来例子中所给出的矩阵结果，感觉可能是sklearn包内又对数据做了某种规范化处理？求解","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586324638,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":333390,"user_name":"木木夕","can_delete":false,"product_type":"c1","uid":1042385,"ip_address":"","ucode":"75BDB2ED7B9A4E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e7/d1/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1644322991,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"TF-IDF是个计算出来的数值，是连续的数值，为什么文本分类用多项式贝叶斯，而不是高斯贝叶斯呢？高斯贝叶斯是适合连续变量，多项式贝叶斯是适合离散变量的。如果特征变量是词频，那用多项式贝叶斯是合理的。","like_count":0},{"had_liked":false,"id":332929,"user_name":"通天白鼠","can_delete":false,"product_type":"c1","uid":2118179,"ip_address":"","ucode":"5033B4E1240002","user_header":"https://static001.geekbang.org/account/avatar/00/20/52/23/b3551be1.jpg","comment_is_top":false,"comment_ctime":1643838725,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"没有理解呀。\ntrain_words_list = train_words_list1 + train_words_list2 + train_words_list3 + train_words_list4\ntrain_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4\n这里所有的单词全部拉出来形成一个长长的列表传递进去训练，又没有分段，如何知道哪些单词对应的是哪个label呢。","like_count":0},{"had_liked":false,"id":296729,"user_name":"徐进","can_delete":false,"product_type":"c1","uid":2536761,"ip_address":"","ucode":"9B12B9C25C779C","user_header":"https://static001.geekbang.org/account/avatar/00/26/b5/39/c7e3b074.jpg","comment_is_top":false,"comment_ctime":1623133980,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"先按照jieba组件！！！  Windows 下使用命令安装：在联网状态下，在命令行下输入 pip install jieba 进行安装，安装完成后会提示安装成功","like_count":0},{"had_liked":false,"id":295659,"user_name":"冉修远","can_delete":false,"product_type":"c1","uid":2104138,"ip_address":"","ucode":"61899FB4DB0CC6","user_header":"","comment_is_top":false,"comment_ctime":1622536145,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"基于TF-IDF的文本分类为什么使用多项式朴素贝叶斯分类器？\n文本的TF-IDF向量的值是一个浮点数，这个属性应该是连续的，为什么不用高斯朴素贝叶斯分类器呢？","like_count":0},{"had_liked":false,"id":272809,"user_name":"lin","can_delete":false,"product_type":"c1","uid":2344228,"ip_address":"","ucode":"356B5A4152B02D","user_header":"https://static001.geekbang.org/account/avatar/00/23/c5/24/182c710f.jpg","comment_is_top":false,"comment_ctime":1610292495,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"有个奇怪的问题，将jieba.cut通过TfidfVectorizer的参数tokenizer调用来训练模型比先使用jieba.cut处理文档后再进行训练的预测得分更高…\n\nfrom sklearn.metrics import accuracy_score\nimport jieba\nimport os\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import defaultdict\n\nroot = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())),\n                    &quot;Lesson21-朴素贝叶斯分类(下)&quot;, &quot;text_classification&quot;, &quot;text classification&quot;)\nstop_word_path = os.path.join(root, &quot;stop&quot;, &quot;stopword.txt&quot;)\ntest_path = os.path.join(root, &quot;test&quot;)\ntrain_path = os.path.join(root, &quot;train&quot;)\n\n\ndef load_train_files(train_path):\n    path_dict = defaultdict(list)\n    for path, folders, files in os.walk(train_path):\n        rootName = path.split(&quot;\\\\&quot;)[-1]\n        for file in files:\n            if &quot;.txt&quot; in file:\n                path_dict[rootName].append(os.path.join(path, file))\n    train_documents = []\n    train_labels = []\n    for label, files in path_dict.items():\n        for file in files:\n            try:\n                document = &quot; &quot;.join([word.strip() for word in jieba.cut(\n                    open(file, &quot;r&quot;, encoding=&quot;gb18030&quot;).read())])\n                train_documents.append(document)\n                train_labels.append(label)\n            except Exception as e:\n                print(e, file)\n    return train_documents, train_labels\n\ntrain_documents, train_labels = load_train_files(train_path)\ntest_documents, test_labels = load_train_files(test_path)\nstop_words = [word.strip() for word in open(\n    stop_word_path, &quot;r&quot;, encoding=&quot;utf-8-sig&quot;).readlines()]\n\ntrain_tfidf_vect = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = train_tfidf_vect.fit_transform(train_documents)\ntest_features = train_tfidf_vect.transform(test_documents)\n\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\npredicted_labels = clf.predict(test_features)\n\nprint(predicted_labels)\nscore = accuracy_score(test_labels, predicted_labels)\nprint(score)\n","like_count":0},{"had_liked":false,"id":211093,"user_name":"南辞","can_delete":false,"product_type":"c1","uid":1915530,"ip_address":"","ucode":"098E01EB97E901","user_header":"https://static001.geekbang.org/account/avatar/00/1d/3a/8a/76b03c2f.jpg","comment_is_top":false,"comment_ctime":1587894540,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"完成作业，不能发布，提示系统错误。\n后面作业的操作，感觉主要是关于数据处理的比较多点，感觉实际工作中不会这么简单，希望后面更能够接近真是的工作情况。加油","like_count":0},{"had_liked":false,"id":203579,"user_name":"Simon","can_delete":false,"product_type":"c1","uid":1914504,"ip_address":"","ucode":"A8A2E3E57BD029","user_header":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","comment_is_top":false,"comment_ctime":1586235977,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"应该先讲下N-gram，这里用到的是unigram。","like_count":0},{"had_liked":false,"id":194569,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1585068373,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"代码里有个需要注意的地方：\ndef cut_words(file_path):\n    &quot;&quot;&quot;\n    对文本进行切词\n    :param file_path: txt文本路径\n    :return: 用空格分词的字符串\n    &quot;&quot;&quot;\n    text_with_spaces = &#39;&#39;\n    text = open(file_path, &#39;r&#39;, encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;).read()\n    textcut = jieba.cut(text)\n    for word in textcut:\n        text_with_spaces += word + &#39; &#39;\n    return text_with_spaces\n\ntext = open(file_path, &#39;r&#39;, encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;).read()\n注意添加：errors=&#39;ignore&#39;","like_count":0},{"had_liked":false,"id":188764,"user_name":"极客中心","can_delete":false,"product_type":"c1","uid":1542363,"ip_address":"","ucode":"F1CE36E3E47A3C","user_header":"https://static001.geekbang.org/account/avatar/00/17/88/db/adad4661.jpg","comment_is_top":false,"comment_ctime":1584413484,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"print(&#39;每个单词的tfidf值:&#39;, tfidf_matrix.toarray())\n\n打印结果:\n每个单词的tfidf值:\n [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n  0.33040189 0.         0.40412895]\n [0.         0.         0.40412895 0.40412895 0.         0.63314609\n  0.33040189 0.         0.40412895]\n [0.55280532 0.         0.         0.         0.55280532 0.\n  0.28847675 0.55280532 0.        ]\n [0.         0.         0.52210862 0.52210862 0.         0.\n  0.42685801 0.         0.52210862]]       打印出来的是什么意思？ 乱七八糟的。","like_count":0},{"had_liked":false,"id":153906,"user_name":"Answer Liu","can_delete":false,"product_type":"c1","uid":1387369,"ip_address":"","ucode":"7F1E3D96EAD35C","user_header":"https://static001.geekbang.org/account/avatar/00/15/2b/69/7aace61c.jpg","comment_is_top":false,"comment_ctime":1574328420,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"思考题，应该是用高斯朴素贝叶斯，停用词对任何分类都没有参考价值，所有应该去除。","like_count":0},{"had_liked":false,"id":333390,"user_name":"木木夕","can_delete":false,"product_type":"c1","uid":1042385,"ip_address":"","ucode":"75BDB2ED7B9A4E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e7/d1/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1644322991,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"TF-IDF是个计算出来的数值，是连续的数值，为什么文本分类用多项式贝叶斯，而不是高斯贝叶斯呢？高斯贝叶斯是适合连续变量，多项式贝叶斯是适合离散变量的。如果特征变量是词频，那用多项式贝叶斯是合理的。","like_count":0},{"had_liked":false,"id":332929,"user_name":"通天白鼠","can_delete":false,"product_type":"c1","uid":2118179,"ip_address":"","ucode":"5033B4E1240002","user_header":"https://static001.geekbang.org/account/avatar/00/20/52/23/b3551be1.jpg","comment_is_top":false,"comment_ctime":1643838725,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"没有理解呀。\ntrain_words_list = train_words_list1 + train_words_list2 + train_words_list3 + train_words_list4\ntrain_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4\n这里所有的单词全部拉出来形成一个长长的列表传递进去训练，又没有分段，如何知道哪些单词对应的是哪个label呢。","like_count":0},{"had_liked":false,"id":296729,"user_name":"徐进","can_delete":false,"product_type":"c1","uid":2536761,"ip_address":"","ucode":"9B12B9C25C779C","user_header":"https://static001.geekbang.org/account/avatar/00/26/b5/39/c7e3b074.jpg","comment_is_top":false,"comment_ctime":1623133980,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"先按照jieba组件！！！  Windows 下使用命令安装：在联网状态下，在命令行下输入 pip install jieba 进行安装，安装完成后会提示安装成功","like_count":0},{"had_liked":false,"id":295659,"user_name":"冉修远","can_delete":false,"product_type":"c1","uid":2104138,"ip_address":"","ucode":"61899FB4DB0CC6","user_header":"","comment_is_top":false,"comment_ctime":1622536145,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"基于TF-IDF的文本分类为什么使用多项式朴素贝叶斯分类器？\n文本的TF-IDF向量的值是一个浮点数，这个属性应该是连续的，为什么不用高斯朴素贝叶斯分类器呢？","like_count":0},{"had_liked":false,"id":272809,"user_name":"lin","can_delete":false,"product_type":"c1","uid":2344228,"ip_address":"","ucode":"356B5A4152B02D","user_header":"https://static001.geekbang.org/account/avatar/00/23/c5/24/182c710f.jpg","comment_is_top":false,"comment_ctime":1610292495,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"有个奇怪的问题，将jieba.cut通过TfidfVectorizer的参数tokenizer调用来训练模型比先使用jieba.cut处理文档后再进行训练的预测得分更高…\n\nfrom sklearn.metrics import accuracy_score\nimport jieba\nimport os\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import defaultdict\n\nroot = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())),\n                    &quot;Lesson21-朴素贝叶斯分类(下)&quot;, &quot;text_classification&quot;, &quot;text classification&quot;)\nstop_word_path = os.path.join(root, &quot;stop&quot;, &quot;stopword.txt&quot;)\ntest_path = os.path.join(root, &quot;test&quot;)\ntrain_path = os.path.join(root, &quot;train&quot;)\n\n\ndef load_train_files(train_path):\n    path_dict = defaultdict(list)\n    for path, folders, files in os.walk(train_path):\n        rootName = path.split(&quot;\\\\&quot;)[-1]\n        for file in files:\n            if &quot;.txt&quot; in file:\n                path_dict[rootName].append(os.path.join(path, file))\n    train_documents = []\n    train_labels = []\n    for label, files in path_dict.items():\n        for file in files:\n            try:\n                document = &quot; &quot;.join([word.strip() for word in jieba.cut(\n                    open(file, &quot;r&quot;, encoding=&quot;gb18030&quot;).read())])\n                train_documents.append(document)\n                train_labels.append(label)\n            except Exception as e:\n                print(e, file)\n    return train_documents, train_labels\n\ntrain_documents, train_labels = load_train_files(train_path)\ntest_documents, test_labels = load_train_files(test_path)\nstop_words = [word.strip() for word in open(\n    stop_word_path, &quot;r&quot;, encoding=&quot;utf-8-sig&quot;).readlines()]\n\ntrain_tfidf_vect = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = train_tfidf_vect.fit_transform(train_documents)\ntest_features = train_tfidf_vect.transform(test_documents)\n\nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\npredicted_labels = clf.predict(test_features)\n\nprint(predicted_labels)\nscore = accuracy_score(test_labels, predicted_labels)\nprint(score)\n","like_count":0},{"had_liked":false,"id":211093,"user_name":"南辞","can_delete":false,"product_type":"c1","uid":1915530,"ip_address":"","ucode":"098E01EB97E901","user_header":"https://static001.geekbang.org/account/avatar/00/1d/3a/8a/76b03c2f.jpg","comment_is_top":false,"comment_ctime":1587894540,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"完成作业，不能发布，提示系统错误。\n后面作业的操作，感觉主要是关于数据处理的比较多点，感觉实际工作中不会这么简单，希望后面更能够接近真是的工作情况。加油","like_count":0},{"had_liked":false,"id":203579,"user_name":"Simon","can_delete":false,"product_type":"c1","uid":1914504,"ip_address":"","ucode":"A8A2E3E57BD029","user_header":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","comment_is_top":false,"comment_ctime":1586235977,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"应该先讲下N-gram，这里用到的是unigram。","like_count":0},{"had_liked":false,"id":194569,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1585068373,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"代码里有个需要注意的地方：\ndef cut_words(file_path):\n    &quot;&quot;&quot;\n    对文本进行切词\n    :param file_path: txt文本路径\n    :return: 用空格分词的字符串\n    &quot;&quot;&quot;\n    text_with_spaces = &#39;&#39;\n    text = open(file_path, &#39;r&#39;, encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;).read()\n    textcut = jieba.cut(text)\n    for word in textcut:\n        text_with_spaces += word + &#39; &#39;\n    return text_with_spaces\n\ntext = open(file_path, &#39;r&#39;, encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;).read()\n注意添加：errors=&#39;ignore&#39;","like_count":0},{"had_liked":false,"id":188764,"user_name":"极客中心","can_delete":false,"product_type":"c1","uid":1542363,"ip_address":"","ucode":"F1CE36E3E47A3C","user_header":"https://static001.geekbang.org/account/avatar/00/17/88/db/adad4661.jpg","comment_is_top":false,"comment_ctime":1584413484,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"print(&#39;每个单词的tfidf值:&#39;, tfidf_matrix.toarray())\n\n打印结果:\n每个单词的tfidf值:\n [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n  0.33040189 0.         0.40412895]\n [0.         0.         0.40412895 0.40412895 0.         0.63314609\n  0.33040189 0.         0.40412895]\n [0.55280532 0.         0.         0.         0.55280532 0.\n  0.28847675 0.55280532 0.        ]\n [0.         0.         0.52210862 0.52210862 0.         0.\n  0.42685801 0.         0.52210862]]       打印出来的是什么意思？ 乱七八糟的。","like_count":0},{"had_liked":false,"id":153906,"user_name":"Answer Liu","can_delete":false,"product_type":"c1","uid":1387369,"ip_address":"","ucode":"7F1E3D96EAD35C","user_header":"https://static001.geekbang.org/account/avatar/00/15/2b/69/7aace61c.jpg","comment_is_top":false,"comment_ctime":1574328420,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100021701,"comment_content":"思考题，应该是用高斯朴素贝叶斯，停用词对任何分类都没有参考价值，所有应该去除。","like_count":0},{"had_liked":false,"id":145078,"user_name":"HHH","can_delete":false,"product_type":"c1","uid":1009357,"ip_address":"","ucode":"5BCF8B0CC26A29","user_header":"https://static001.geekbang.org/account/avatar/00/0f/66/cd/3e169ff3.jpg","comment_is_top":false,"comment_ctime":1572181963,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"贝叶斯分类从举的例子来看应该是监督学习的，有在非监督学习下应用的场景么？","like_count":0},{"had_liked":false,"id":144293,"user_name":"羊小看","can_delete":false,"product_type":"c1","uid":1488453,"ip_address":"","ucode":"90F58F80A75520","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132","comment_is_top":false,"comment_ctime":1571888636,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"用朴素贝叶斯做文本分类时，是把各文档中各单词的Tfidf值作为特征值的，然后测试样本以同样的单词顺序来生成特征值，再计算概率。","like_count":0},{"had_liked":false,"id":138194,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1570089438,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"想问一下老师：系统是如何通过模型来对文本进行分类的，例如一个单词，在测试集的某个文本中出现，但其在训练集中出现在两个不同分类的文本中，系统是根据什么来确定测试集中的文本是属于哪个分类的？","like_count":0},{"had_liked":false,"id":77400,"user_name":"余璋","can_delete":false,"product_type":"c1","uid":1125790,"ip_address":"","ucode":"0E34B99FD1D42E","user_header":"https://static001.geekbang.org/account/avatar/00/11/2d/9e/f41bee94.jpg","comment_is_top":false,"comment_ctime":1552919103,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"老师，请问一下代码中的train_labels需要去重吗？word_list 和 train_contents的关系是什么？word_list需要去重吗？features 和 train_features 的关系是什么？","like_count":0},{"had_liked":false,"id":74393,"user_name":"周飞","can_delete":false,"product_type":"c1","uid":1073374,"ip_address":"","ucode":"F85FA236EB0C0D","user_header":"https://static001.geekbang.org/account/avatar/00/10/60/de/5c67895a.jpg","comment_is_top":false,"comment_ctime":1552211802,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"#!&#47;usr&#47;bin&#47;env python\n# -*- coding:utf8 -*-\nimport os\nimport jieba\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n\ndef load_data(base_path):    \n    documents = []\n    labels = []\n    for root, dirs, files in os.walk(base_path): # 循环所有文件并进行分词打标        \n        for file in files:\n            label = root.split(&#39;\\\\&#39;)[-1] # 因为windows上路径符号自动转成\\了，所以要转义下\n            labels.append(label)\n            filename = os.path.join(root, file)\n            with open(filename, &#39;rb&#39;) as f: # 因为字符集问题因此直接用二进制方式读取\n                content = f.read()\n                word_list = list(jieba.cut(content))                \n                words = [wl for wl in word_list]\n                documents.append(&#39; &#39;.join(words))\n    return documents, labels\n\ntrain_contents, train_labels = load_data(&#39;.&#47;text_classification&#47;train&#39;)\ntest_contents, test_labels = load_data(&#39;.&#47;text_classification&#47;test&#39;)\nstop_words = []\n\nimport io\nstop_words = [line.strip().encode(&#39;utf-8&#39;) for line in io.open(&#39;.&#47;text_classification&#47;stop&#47;stopword.txt&#39;).readlines()]\n\n\n\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_contents)\n# 多项式贝叶斯分类器\n \nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\ntest_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features=test_tf.fit_transform(test_contents)\npredicted_labels=clf.predict(test_features)\nprint (metrics.accuracy_score(test_labels, predicted_labels))\n\n不知道为什么 结果是0。\n\n\n\n\n\n\n\n\n","like_count":0},{"had_liked":false,"id":73985,"user_name":"吃饭睡觉打窦窦","can_delete":false,"product_type":"c1","uid":1357253,"ip_address":"","ucode":"CF5D4397180D8F","user_header":"","comment_is_top":false,"comment_ctime":1552053202,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"ValueError: dimension mismatch  提示，还有编码问题，无法解决","like_count":0},{"had_liked":false,"id":69341,"user_name":"Geek_hve78z","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1550720004,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"一、最后你不妨思考一下，假设我们要判断一个人的性别，是通过身高、体重、鞋码、外貌等属性进行判断的，如果我们用朴素贝叶斯做分类，适合使用哪种朴素贝叶斯分类器？停用词的作用又是什么？\n\n适合用高斯朴素贝叶斯。因为身高、体重、鞋码是连续变量。\n停用词的作用是去除无关词，保留数字型变量。\n如果使用外貌进行判断，则使用多项式朴素贝叶斯，因为外貌都适用单词进行描述。\n\n二、文章内容困惑点\n在模块4:生成朴素贝叶斯分类器，特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 是如何获取的。老师并没有讲清楚。","like_count":0},{"had_liked":false,"id":66930,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":false,"comment_ctime":1550043947,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"身高体重等等这些适合用高斯分类器，停用词的作用是减少重复计算","like_count":0,"discussions":[{"author":{"id":1183831,"avatar":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","nickname":"追梦","note":"","ucode":"54C6E76E8FE033","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":29797,"discussion_content":"那这个分析模型停用词写什么呢？有点蒙","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570784777,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66913,"user_name":"乔巴","can_delete":false,"product_type":"c1","uid":1150334,"ip_address":"","ucode":"B32ED596B7A6C7","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg","comment_is_top":false,"comment_ctime":1550040269,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"但准确度不高，不知怎么优化\n# -*- coding: utf-8 -*-\n#coding:utf-8\nimport os   \nimport jieba \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\ndef preprocess(path_name):\n    text_with_spaces=&quot;&quot;\n    textfile=open(path_name,&quot;r&quot;).read() \n    textcut=jieba.cut(textfile)\n    for word in textcut:\n        text_with_spaces+=word+&quot; &quot;\n    return text_with_spaces\n\ndef loadtrainset(path,classtag):\n    allfiles=os.listdir(path)\n    processed_textset=[]\n    allclasstags=[]\n    for thisfile in allfiles:\n        path_name=path+&quot;&#47;&quot;+thisfile\n        processed_textset.append(preprocess(path_name))\n        allclasstags.append(classtag)\n    return processed_textset,allclasstags\n\nprocessed_textdata1,class1=loadtrainset(&quot;D:&#47;train&#47;女性&quot;, &quot;女性&quot;)\nprocessed_textdata2,class2=loadtrainset(&quot;D:&#47;train&#47;体育&quot;, &quot;体育&quot;)\nprocessed_textdata3,class3=loadtrainset(&quot;D:&#47;train&#47;文学&quot;, &quot;文学&quot;)\nprocessed_textdata4,class4=loadtrainset(&quot;D:&#47;train&#47;校园&quot;, &quot;校园&quot;)\nintegrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4\nclasstags_list=class1+class2+class3+class4\ncount_vector = CountVectorizer()\n\nvector_matrix = count_vector.fit_transform(integrated_train_data)\ntrain_tfidf = TfidfTransformer(use_idf=False).fit_transform(vector_matrix)\nclf = MultinomialNB().fit(train_tfidf,classtags_list)#\n\ntest_textdata1,testClass1=loadtrainset(&quot;D:&#47;test&#47;女性&quot;, &quot;女性&quot;)\ntest_textdata2,testClass2=loadtrainset(&quot;D:&#47;test&#47;体育&quot;, &quot;体育&quot;)\ntest_textdata3,testClass3=loadtrainset(&quot;D:&#47;test&#47;文学&quot;, &quot;文学&quot;)\ntest_textdata4,testClass4=loadtrainset(&quot;D:&#47;test&#47;校园&quot;, &quot;校园&quot;)\nintegrated_test_data=test_textdata1+test_textdata2+test_textdata3+test_textdata4\nclasstags_list=testClass1+testClass2+testClass3+testClass4\nnew_count_vector = count_vector.transform(integrated_test_data)\nnew_tfidf= TfidfTransformer(use_idf=False).fit_transform(new_count_vector)\npredict_result = clf.predict(new_tfidf) \n\nprint(metrics.accuracy_score(classtags_list, predict_result))\n\n","like_count":0},{"had_liked":false,"id":66896,"user_name":"乔巴","can_delete":false,"product_type":"c1","uid":1150334,"ip_address":"","ucode":"B32ED596B7A6C7","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg","comment_is_top":false,"comment_ctime":1550038972,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"TfidfVectorizer获取特征值运行异常，但CountVectorizer可以，\nimport pandas as pd\nimport numpy as np\nimport io\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn import metrics\nimport os\nimport jieba \n\ndef preprocess(path_name):\n    text_with_spaces=&quot;&quot;\n    textfile=open(path_name,&quot;r&quot;).read() #\n    textcut=jieba.cut(textfile)\n    for word in textcut:\n        text_with_spaces+=word+&quot; &quot;\n    return text_with_spaces\n\n#loadtrainset\ndef loadtrainset(path,classtag):\n    allfiles=os.listdir(path)\n    processed_textset=[]\n    allclasstags=[]\n    for thisfile in allfiles:\n        path_name=path+&quot;&#47;&quot;+thisfile\n        processed_textset.append(preprocess(path_name))\n        allclasstags.append(classtag)\n    return processed_textset,allclasstags\n\nstop_words = [line.strip() for line in io.open(&#39;.&#47;textClassification&#47;stop&#47;stopword.txt&#39;, encoding=&#39;utf-8&#39;).readlines()]\n\n#计算单词的权重 得到特征空间 features\n\nprocessed_textdata1,class1=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;女性&quot;, &quot;女性&quot;)\nprocessed_textdata2,class2=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;体育&quot;, &quot;体育&quot;)\nprocessed_textdata3,class3=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;文学&quot;, &quot;文学&quot;)\nprocessed_textdata4,class4=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;校园&quot;, &quot;校园&quot;)\nintegrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4\nclasstags_list=class1+class2+class3+class4\n\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(integrated_train_data)\n\n# 多项式贝叶斯分类器\nclf = MultinomialNB(alpha=1).fit(train_features, classtags_list)\n\ntest_textdata1,testClass1=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;test&#47;女性&quot;, &quot;女性&quot;)\nintegrated_test_data=test_textdata1\nclasstags_list=testClass1\ntest_features=tf.fit_transform(integrated_test_data)\npredicted_labels=clf.predict(test_features)\n\n#准确率\nprint(metrics.accuracy_score(classtags_list, predicted_labels))","like_count":0,"discussions":[{"author":{"id":1183831,"avatar":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","nickname":"追梦","note":"","ucode":"54C6E76E8FE033","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":29797,"discussion_content":"那这个分析模型停用词写什么呢？有点蒙","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570784777,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":145078,"user_name":"HHH","can_delete":false,"product_type":"c1","uid":1009357,"ip_address":"","ucode":"5BCF8B0CC26A29","user_header":"https://static001.geekbang.org/account/avatar/00/0f/66/cd/3e169ff3.jpg","comment_is_top":false,"comment_ctime":1572181963,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"贝叶斯分类从举的例子来看应该是监督学习的，有在非监督学习下应用的场景么？","like_count":0},{"had_liked":false,"id":144293,"user_name":"羊小看","can_delete":false,"product_type":"c1","uid":1488453,"ip_address":"","ucode":"90F58F80A75520","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132","comment_is_top":false,"comment_ctime":1571888636,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"用朴素贝叶斯做文本分类时，是把各文档中各单词的Tfidf值作为特征值的，然后测试样本以同样的单词顺序来生成特征值，再计算概率。","like_count":0},{"had_liked":false,"id":138194,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1570089438,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"想问一下老师：系统是如何通过模型来对文本进行分类的，例如一个单词，在测试集的某个文本中出现，但其在训练集中出现在两个不同分类的文本中，系统是根据什么来确定测试集中的文本是属于哪个分类的？","like_count":0},{"had_liked":false,"id":77400,"user_name":"余璋","can_delete":false,"product_type":"c1","uid":1125790,"ip_address":"","ucode":"0E34B99FD1D42E","user_header":"https://static001.geekbang.org/account/avatar/00/11/2d/9e/f41bee94.jpg","comment_is_top":false,"comment_ctime":1552919103,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"老师，请问一下代码中的train_labels需要去重吗？word_list 和 train_contents的关系是什么？word_list需要去重吗？features 和 train_features 的关系是什么？","like_count":0},{"had_liked":false,"id":74393,"user_name":"周飞","can_delete":false,"product_type":"c1","uid":1073374,"ip_address":"","ucode":"F85FA236EB0C0D","user_header":"https://static001.geekbang.org/account/avatar/00/10/60/de/5c67895a.jpg","comment_is_top":false,"comment_ctime":1552211802,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"#!&#47;usr&#47;bin&#47;env python\n# -*- coding:utf8 -*-\nimport os\nimport jieba\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n\ndef load_data(base_path):    \n    documents = []\n    labels = []\n    for root, dirs, files in os.walk(base_path): # 循环所有文件并进行分词打标        \n        for file in files:\n            label = root.split(&#39;\\\\&#39;)[-1] # 因为windows上路径符号自动转成\\了，所以要转义下\n            labels.append(label)\n            filename = os.path.join(root, file)\n            with open(filename, &#39;rb&#39;) as f: # 因为字符集问题因此直接用二进制方式读取\n                content = f.read()\n                word_list = list(jieba.cut(content))                \n                words = [wl for wl in word_list]\n                documents.append(&#39; &#39;.join(words))\n    return documents, labels\n\ntrain_contents, train_labels = load_data(&#39;.&#47;text_classification&#47;train&#39;)\ntest_contents, test_labels = load_data(&#39;.&#47;text_classification&#47;test&#39;)\nstop_words = []\n\nimport io\nstop_words = [line.strip().encode(&#39;utf-8&#39;) for line in io.open(&#39;.&#47;text_classification&#47;stop&#47;stopword.txt&#39;).readlines()]\n\n\n\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(train_contents)\n# 多项式贝叶斯分类器\n \nclf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n\ntest_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)\ntest_features=test_tf.fit_transform(test_contents)\npredicted_labels=clf.predict(test_features)\nprint (metrics.accuracy_score(test_labels, predicted_labels))\n\n不知道为什么 结果是0。\n\n\n\n\n\n\n\n\n","like_count":0,"discussions":[{"author":{"id":1209322,"avatar":"https://static001.geekbang.org/account/avatar/00/12/73/ea/9c9acc43.jpg","nickname":"Jackom","note":"","ucode":"166111CDB92469","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":163938,"discussion_content":"替换成这个就行了：\nfor root, dirs, files in os.walk(base_path): # 循环所有文件并进行分词打标 \n        for file in files:\n            if file == &#39;.DS_Store&#39;: # 过滤掉mac系统的隐藏文件\n                continue\n            label = root.split(os.sep)[-1]","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1581129717,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73985,"user_name":"吃饭睡觉打窦窦","can_delete":false,"product_type":"c1","uid":1357253,"ip_address":"","ucode":"CF5D4397180D8F","user_header":"","comment_is_top":false,"comment_ctime":1552053202,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"ValueError: dimension mismatch  提示，还有编码问题，无法解决","like_count":0},{"had_liked":false,"id":69341,"user_name":"Geek_hve78z","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1550720004,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"一、最后你不妨思考一下，假设我们要判断一个人的性别，是通过身高、体重、鞋码、外貌等属性进行判断的，如果我们用朴素贝叶斯做分类，适合使用哪种朴素贝叶斯分类器？停用词的作用又是什么？\n\n适合用高斯朴素贝叶斯。因为身高、体重、鞋码是连续变量。\n停用词的作用是去除无关词，保留数字型变量。\n如果使用外貌进行判断，则使用多项式朴素贝叶斯，因为外貌都适用单词进行描述。\n\n二、文章内容困惑点\n在模块4:生成朴素贝叶斯分类器，特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 是如何获取的。老师并没有讲清楚。","like_count":0},{"had_liked":false,"id":66930,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":false,"comment_ctime":1550043947,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"身高体重等等这些适合用高斯分类器，停用词的作用是减少重复计算","like_count":0,"discussions":[{"author":{"id":1209322,"avatar":"https://static001.geekbang.org/account/avatar/00/12/73/ea/9c9acc43.jpg","nickname":"Jackom","note":"","ucode":"166111CDB92469","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":163938,"discussion_content":"替换成这个就行了：\nfor root, dirs, files in os.walk(base_path): # 循环所有文件并进行分词打标 \n        for file in files:\n            if file == &#39;.DS_Store&#39;: # 过滤掉mac系统的隐藏文件\n                continue\n            label = root.split(os.sep)[-1]","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1581129717,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":66913,"user_name":"乔巴","can_delete":false,"product_type":"c1","uid":1150334,"ip_address":"","ucode":"B32ED596B7A6C7","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg","comment_is_top":false,"comment_ctime":1550040269,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"但准确度不高，不知怎么优化\n# -*- coding: utf-8 -*-\n#coding:utf-8\nimport os   \nimport jieba \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\ndef preprocess(path_name):\n    text_with_spaces=&quot;&quot;\n    textfile=open(path_name,&quot;r&quot;).read() \n    textcut=jieba.cut(textfile)\n    for word in textcut:\n        text_with_spaces+=word+&quot; &quot;\n    return text_with_spaces\n\ndef loadtrainset(path,classtag):\n    allfiles=os.listdir(path)\n    processed_textset=[]\n    allclasstags=[]\n    for thisfile in allfiles:\n        path_name=path+&quot;&#47;&quot;+thisfile\n        processed_textset.append(preprocess(path_name))\n        allclasstags.append(classtag)\n    return processed_textset,allclasstags\n\nprocessed_textdata1,class1=loadtrainset(&quot;D:&#47;train&#47;女性&quot;, &quot;女性&quot;)\nprocessed_textdata2,class2=loadtrainset(&quot;D:&#47;train&#47;体育&quot;, &quot;体育&quot;)\nprocessed_textdata3,class3=loadtrainset(&quot;D:&#47;train&#47;文学&quot;, &quot;文学&quot;)\nprocessed_textdata4,class4=loadtrainset(&quot;D:&#47;train&#47;校园&quot;, &quot;校园&quot;)\nintegrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4\nclasstags_list=class1+class2+class3+class4\ncount_vector = CountVectorizer()\n\nvector_matrix = count_vector.fit_transform(integrated_train_data)\ntrain_tfidf = TfidfTransformer(use_idf=False).fit_transform(vector_matrix)\nclf = MultinomialNB().fit(train_tfidf,classtags_list)#\n\ntest_textdata1,testClass1=loadtrainset(&quot;D:&#47;test&#47;女性&quot;, &quot;女性&quot;)\ntest_textdata2,testClass2=loadtrainset(&quot;D:&#47;test&#47;体育&quot;, &quot;体育&quot;)\ntest_textdata3,testClass3=loadtrainset(&quot;D:&#47;test&#47;文学&quot;, &quot;文学&quot;)\ntest_textdata4,testClass4=loadtrainset(&quot;D:&#47;test&#47;校园&quot;, &quot;校园&quot;)\nintegrated_test_data=test_textdata1+test_textdata2+test_textdata3+test_textdata4\nclasstags_list=testClass1+testClass2+testClass3+testClass4\nnew_count_vector = count_vector.transform(integrated_test_data)\nnew_tfidf= TfidfTransformer(use_idf=False).fit_transform(new_count_vector)\npredict_result = clf.predict(new_tfidf) \n\nprint(metrics.accuracy_score(classtags_list, predict_result))\n\n","like_count":0},{"had_liked":false,"id":66896,"user_name":"乔巴","can_delete":false,"product_type":"c1","uid":1150334,"ip_address":"","ucode":"B32ED596B7A6C7","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg","comment_is_top":false,"comment_ctime":1550038972,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100021701,"comment_content":"TfidfVectorizer获取特征值运行异常，但CountVectorizer可以，\nimport pandas as pd\nimport numpy as np\nimport io\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn import metrics\nimport os\nimport jieba \n\ndef preprocess(path_name):\n    text_with_spaces=&quot;&quot;\n    textfile=open(path_name,&quot;r&quot;).read() #\n    textcut=jieba.cut(textfile)\n    for word in textcut:\n        text_with_spaces+=word+&quot; &quot;\n    return text_with_spaces\n\n#loadtrainset\ndef loadtrainset(path,classtag):\n    allfiles=os.listdir(path)\n    processed_textset=[]\n    allclasstags=[]\n    for thisfile in allfiles:\n        path_name=path+&quot;&#47;&quot;+thisfile\n        processed_textset.append(preprocess(path_name))\n        allclasstags.append(classtag)\n    return processed_textset,allclasstags\n\nstop_words = [line.strip() for line in io.open(&#39;.&#47;textClassification&#47;stop&#47;stopword.txt&#39;, encoding=&#39;utf-8&#39;).readlines()]\n\n#计算单词的权重 得到特征空间 features\n\nprocessed_textdata1,class1=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;女性&quot;, &quot;女性&quot;)\nprocessed_textdata2,class2=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;体育&quot;, &quot;体育&quot;)\nprocessed_textdata3,class3=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;文学&quot;, &quot;文学&quot;)\nprocessed_textdata4,class4=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;train&#47;校园&quot;, &quot;校园&quot;)\nintegrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4\nclasstags_list=class1+class2+class3+class4\n\ntf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\ntrain_features = tf.fit_transform(integrated_train_data)\n\n# 多项式贝叶斯分类器\nclf = MultinomialNB(alpha=1).fit(train_features, classtags_list)\n\ntest_textdata1,testClass1=loadtrainset(&quot;.&#47;text_classification&#47;textClassification&#47;test&#47;女性&quot;, &quot;女性&quot;)\nintegrated_test_data=test_textdata1\nclasstags_list=testClass1\ntest_features=tf.fit_transform(integrated_test_data)\npredicted_labels=clf.predict(test_features)\n\n#准确率\nprint(metrics.accuracy_score(classtags_list, predicted_labels))","like_count":0},{"had_liked":false,"id":65596,"user_name":"无才不肖生","can_delete":false,"product_type":"c1","uid":1065687,"ip_address":"","ucode":"A2F83FC7405792","user_header":"https://static001.geekbang.org/account/avatar/00/10/42/d7/1f1634af.jpg","comment_is_top":false,"comment_ctime":1549476272,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100021701,"comment_content":"首先，训练集和测试集中部分文件无法读取，我是直接跳过没加载，感觉少了点不大影响。\n其次，我最后的预测结果都是女性，正确率只有百分之25\n最后，我想问下，特征矩阵是4*20705这个对吗","like_count":0},{"had_liked":false,"id":64889,"user_name":"JingZ","can_delete":false,"product_type":"c1","uid":1023464,"ip_address":"","ucode":"6F97895B2CC375","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132","comment_is_top":false,"comment_ctime":1548941937,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100021701,"comment_content":"感觉对算法接触比较少的转型者，这个分类还是有点没思路~希望老师推荐本类似《利用python进行数据分析》书看看~Numpy和Pandas还算熟了，但Sklearn具体应用较少~","like_count":0},{"had_liked":false,"id":65596,"user_name":"无才不肖生","can_delete":false,"product_type":"c1","uid":1065687,"ip_address":"","ucode":"A2F83FC7405792","user_header":"https://static001.geekbang.org/account/avatar/00/10/42/d7/1f1634af.jpg","comment_is_top":false,"comment_ctime":1549476272,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100021701,"comment_content":"首先，训练集和测试集中部分文件无法读取，我是直接跳过没加载，感觉少了点不大影响。\n其次，我最后的预测结果都是女性，正确率只有百分之25\n最后，我想问下，特征矩阵是4*20705这个对吗","like_count":0},{"had_liked":false,"id":64889,"user_name":"JingZ","can_delete":false,"product_type":"c1","uid":1023464,"ip_address":"","ucode":"6F97895B2CC375","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132","comment_is_top":false,"comment_ctime":1548941937,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100021701,"comment_content":"感觉对算法接触比较少的转型者，这个分类还是有点没思路~希望老师推荐本类似《利用python进行数据分析》书看看~Numpy和Pandas还算熟了，但Sklearn具体应用较少~","like_count":0}]}