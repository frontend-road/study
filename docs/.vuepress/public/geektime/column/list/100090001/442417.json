{"id":442417,"title":"27 | 模型训练（中）：回归、分类和聚类算法详解","content":"<p>你好，我是吴磊。</p><p>在上一讲，我们学习了决策树系列算法，包括决策树、GBDT和随机森林。今天这一讲，我们来看看在Spark MLlib框架下，如何将这些算法应用到实际的场景中。</p><p>你还记得我们给出的Spark MLlib模型算法“全景图”么？对于这张“全景图”，我们会时常回顾它。一方面，它能为我们提供“全局视角”，再者，有了它，我们就能够轻松地把学习过的内容对号入座，从而对于学习的进展，做到心中有数。</p><p><img src=\"https://static001.geekbang.org/resource/image/f1/54/f1d0ce11953030d6a9eb4475c7827d54.jpg?wh=1920x2035\" alt=\"图片\" title=\"Spark MLlib支持的模型算法\"></p><p>今天这一讲，我们会结合房屋预测场景，一起学习回归、分类与聚类中的典型算法在Spark MLlib框架下的具体用法。掌握这些用法之后，针对同一类机器学习问题（回归、分类或是聚类），你就可以在其算法集合中，灵活、高效地做算法选型。</p><h2>房屋预测场景</h2><p>在这个场景中，我们有3个实例，分别是房价预测、房屋分类和房屋聚类。房价预测我们并不陌生，在前面的学习中，我们一直在尝试把房价预测得更准。</p><p>房屋分类，它指的是，给定离散标签（Label），如“OverallQual”（房屋质量），结合房屋属性特征，将所有房屋分类到相应的标签取值，如房屋质量的“好、中、差”三类。</p><p>而房屋聚类，它指的是，在不存在标签的情况下，根据房屋特征向量，结合“物以类聚”的思想，将相似的房屋聚集到一起，形成聚类。</p><!-- [[[read_end]]] --><h3>房价预测</h3><p>在特征工程的两讲中，我们一直尝试使用线性模型来拟合房价，但线性模型的拟合能力相当有限。决策树系列模型属于非线性模型，在拟合能力上，更胜一筹。经过之前的讲解，想必你对Spark MLlib框架下模型训练的“套路”，已经了然于胸，模型训练基本上可以分为3个环节：</p><ul>\n<li>准备训练样本</li>\n<li>定义模型，并拟合训练数据</li>\n<li>验证模型效果</li>\n</ul><p>除了模型定义，第一个与第三个环节实际上是通用的。不论我们采用哪种模型，训练样本其实都大同小异，度量指标（不论是用于回归的RMSE，还是用于分类的AUC）本身也与模型无关。因此，今天这一讲，我们把重心放在<strong>第二个环节</strong>，对于代码实现，我们在文稿中也只粘贴这一环节的代码，其他环节的代码，你可以参考特征工程的两讲的内容。</p><p><a href=\"https://time.geekbang.org/column/article/440409\">上一讲</a>我们学过了决策树系列模型及其衍生算法，也就是随机森林与GBDT算法。这两种算法既可以解决分类问题，也可以用来解决回归问题。既然GBDT擅长拟合残差，那么我们不妨用它来解决房价预测的（回归）问题，而把随机森林留给后面的房屋分类。</p><p>要用GBDT来拟合房价，我们首先还是先来准备训练样本。</p><pre><code class=\"language-scala\">// numericFields代表数值字段，indexFields为采用StringIndexer处理后的非数值字段\nval assembler = new VectorAssembler()\n.setInputCols(numericFields ++ indexFields)\n.setOutputCol(\"features\")\n&nbsp;\n// 创建特征向量“features”\nengineeringDF = assembler.transform(engineeringDF)\n&nbsp;\nimport org.apache.spark.ml.feature.VectorIndexer\n&nbsp;\n// 区分离散特征与连续特征\nval vectorIndexer = new VectorIndexer()\n.setInputCol(\"features\")\n.setOutputCol(\"indexedFeatures\")\n// 设定区分阈值\n.setMaxCategories(30)\n&nbsp;\n// 完成数据转换\nengineeringDF = vectorIndexer.fit(engineeringDF).transform(engineeringDF)\n</code></pre><p>我们之前已经学过了VectorAssembler的用法，它用来把多个字段拼接为特征向量。你可能已经发现，在VectorAssembler之后，我们使用了一个新的特征处理函数对engineeringDF进一步做了转换，这个函数叫作VectorIndexer。它是用来干什么的呢？</p><p>简单地说，它用来帮助决策树系列算法（如GBDT、随机森林）区分离散特征与连续特征。连续特征也即数值型特征，数值之间本身是存在大小关系的。而离散特征（如街道类型）在经过StringIndexer转换为数字之后，<strong>数字与数字之间会引入原本并不存在的大小关系</strong>（具体你可以回看<a href=\"https://time.geekbang.org/column/article/438660\">第25讲</a>）。</p><p>这个问题要怎么解决呢？首先，对于经过StringIndexer处理过的离散特征，VectorIndexer会进一步对它们编码，抹去数字之间的比较关系，从而明确告知GBDT等算法，该特征为离散特征，数字与数字之间相互独立，不存在任何关系。</p><p>VectorIndexer对象的setMaxCategories方法，用于设定阈值，该阈值用于区分离散特征与连续特征，我们这里设定的阈值为30。这个阈值有什么用呢？凡是多样性（Cardinality）大于30的特征，后续的GBDT模型会把它们看作是连续特征，而多样性小于30的特征，GBDT会把它们当作是离散特征来进行处理。</p><p>说到这里，你可能会问：“对于一个特征，区分它是连续的、还是离散的，有这么重要吗？至于这么麻烦吗？”</p><p>还记得在决策树基本原理中，特征的“提纯”能力这个概念吗？对于同样一份数据样本，同样一个特征，连续值与离散值的“提纯”能力可能有着天壤之别。还原特征原本的“提纯”能力，将为决策树的合理构建，打下良好的基础。</p><p>好啦，样本准备好之后，接下来，我们就要定义并拟合GBDT模型了。</p><pre><code class=\"language-scala\">import org.apache.spark.ml.regression.GBTRegressor\n&nbsp;\n// 定义GBDT模型\nval gbt = new GBTRegressor()\n.setLabelCol(\"SalePriceInt\")\n.setFeaturesCol(\"indexedFeatures\")\n// 限定每棵树的最大深度\n.setMaxDepth(5)\n// 限定决策树的最大棵树\n.setMaxIter(30)\n&nbsp;\n// 区分训练集、验证集\nval Array(trainingData, testData) = engineeringDF.randomSplit(Array(0.7, 0.3))\n&nbsp;\n// 拟合训练数据\nval gbtModel = gbt.fit(trainingData)\n</code></pre><p>可以看到，我们通过定义GBTRegressor来定义GBDT模型，其中setLabelCol、setFeaturesCol都是老生常谈的方法了，不再赘述。值得注意的是<strong>setMaxDepth和setMaxIter，这两个方法用于避免GBDT模型出现过拟合的情况，前者限定每棵树的深度，而后者直接限制了GBDT模型中决策树的总体数目。后面的训练过程，依然是调用模型的fit方法</strong>。</p><p>到此为止，我们介绍了如何通过定义GBDT模型，来拟合房价。后面的效果评估环节，鼓励你结合<a href=\"https://time.geekbang.org/column/article/435583\">第23讲</a>的模型验证部分，去自行尝试，加油！</p><h3>房屋分类</h3><p>接下来，我们再来说说房屋分类。我们知道，在“<a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview\">House Prices - Advanced Regression Techniques</a>”竞赛项目中，数据集总共有79个字段。在之前，我们一直把售价SalePrice当作是预测标的，也就是Label，而用其他字段构建特征向量。</p><p>现在，我们来换个视角，把房屋质量OverallQual看作是Label，让售价SalePrice作为普通字段去参与构建特征向量。在房价预测的数据集中，房屋质量是离散特征，它的取值总共有10个，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/5e/656632edab2fefe869f1a01ba4f1b75e.jpg?wh=1920x373\" alt=\"图片\" title=\"OverallQual取值范围\"></p><p>如此一来，我们就把先前的回归问题（预测连续值），转换成了分类问题（预测离散值）。不过，不管是什么机器学习问题，模型训练都离不开那3个环节：</p><ul>\n<li>准备训练样本</li>\n<li>定义模型，并拟合训练数据</li>\n<li>验证模型效果</li>\n</ul><p>在训练样本的准备上，除了把预测标的从SalePrice替换为OverallQual，我们完全可以复用刚刚使用GBDT来预测房价的代码实现。</p><pre><code class=\"language-scala\">// Label字段：\"OverallQual\"\nval labelField: String = \"OverallQual\"\n&nbsp;\nimport org.apache.spark.sql.types.IntegerType\nengineeringDF = engineeringDF\n.withColumn(\"indexedOverallQual\", col(labelField).cast(IntegerType))\n.drop(labelField)\n</code></pre><p>接下来，我们就可以定义随机森林模型、并拟合训练数据。实际上，除了类名不同，RandomForestClassifier在用法上与GBDT的GBTRegressor几乎一模一样，如下面的代码片段所示。</p><pre><code class=\"language-scala\">import org.apache.spark.ml.regression.RandomForestClassifier\n&nbsp;\n// 定义随机森林模型\nval rf= new RandomForestClassifier ()\n// Label不再是房价，而是房屋质量\n.setLabelCol(\"indexedOverallQual\")\n.setFeaturesCol(\"indexedFeatures\")\n// 限定每棵树的最大深度\n.setMaxDepth(5)\n// 限定决策树的最大棵树\n.setMaxIter(30)\n&nbsp;\n// 区分训练集、验证集\nval Array(trainingData, testData) = engineeringDF.randomSplit(Array(0.7, 0.3))\n&nbsp;\n// 拟合训练数据\nval rfModel = rf.fit(trainingData)\n</code></pre><p>模型训练好之后，在第三个环节，我们来初步验证模型效果。</p><p>需要注意的是，衡量模型效果时，回归与分类问题，各自有一套不同的度量指标。毕竟，回归问题预测的是连续值，我们往往用不同形式的误差（如RMSE、MAE、MAPE，等等）来评价回归模型的好坏。而分类问题预测的是离散值，因此，我们通常采用那些能够评估分类“纯度”的指标，比如说准确度、精准率、召回率，等等。</p><p><img src=\"https://static001.geekbang.org/resource/image/ed/ed/edbcb3d0b5c080c0a03ce702f4d5eded.jpg?wh=1920x846\" alt=\"图片\" title=\"不同机器学习问题的度量指标\"></p><p>这里，我们以Accuracy（准确度）为例，来评估随机森林模型的拟合效果，代码如下所示。</p><pre><code class=\"language-scala\">import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n&nbsp;\n// 在训练集上做推理\nval trainPredictions = rfModel.transform(trainingData)\n&nbsp;\n// 定义分类问题的评估对象\nval evaluator = new MulticlassClassificationEvaluator()\n.setLabelCol(\"indexedOverallQual\")\n.setPredictionCol(\"prediction\")\n.setMetricName(\"accuracy\")\n&nbsp;\n// 在训练集的推理结果上，计算Accuracy度量值\nval accuracy = evaluator.evaluate(trainPredictions)\n</code></pre><p>好啦，到此为止，我们以房价预测和房屋分类为例，分别介绍了如何在Spark MLlib框架下去应对回归问题与分类问题。分类与回归，是监督学习中最典型的两类模型算法，是我们必须要熟悉并掌握的。接下来，让我们以房屋聚类为例，说一说非监督学习。</p><h3>房屋聚类</h3><p>与监督学习相对，非监督学习，泛指那些数据样本中没有Label的机器学习问题。</p><p>以房屋数据为例，整个数据集包含79个字段。如果我们把“SalePrice”和“OverallQual”这两个字段抹掉，那么原始数据集就变成了不带Label的数据样本。你可能会好奇：“对于这些没有Label的样本，我们能拿他们做些什么呢？”</p><p>其实能做的事情还真不少，基于房屋数据，我们可以结合“物以类聚”的思想，使用K-means算法把他们进行分门别类的处理。再者，在下一讲电影推荐的例子中，我们还可以基于频繁项集算法，挖掘出不同电影之间共现的频次与关联规则，从而实现推荐。</p><p>今天我们先来讲K-mean，结合数据样本的特征向量，根据向量之间的相对距离，K-means算法可以把所有样本划分为K个类别，这也是算法命名中“K”的由来。举例来说，图中的每个点，都代表一个向量，给定不同的K值，K-means划分的结果会随着K的变化而变化。</p><p><img src=\"https://static001.geekbang.org/resource/image/55/95/559728179ed5212b50a586608f870295.jpg?wh=1920x565\" alt=\"图片\" title=\"K-means算法示意图\"></p><p>在Spark MLlib的开发框架下，我们可以轻而易举地对任意向量做聚类。</p><p>首先，在模型训练的第一个环节，我们先把训练样本准备好。注意，这一次，我们去掉了“SalePrice”和“OverallQual”这两个字段。</p><pre><code class=\"language-scala\">import org.apache.spark.ml.feature.VectorAssembler\n&nbsp;\nval assembler = new VectorAssembler()\n// numericFields包含连续特征，oheFields为离散特征的One hot编码\n.setInputCols(numericFields ++ oheFields)\n.setOutputCol(\"features\")\n</code></pre><p>接下来，在第二个环节，我们来定义K-means模型，并使用刚刚准备好的样本，去做模型训练。可以看到，模型定义非常简单，只需实例化KMeans对象，并通过setK指定K值即可。</p><pre><code class=\"language-scala\">import org.apache.spark.ml.clustering.KMeans\n&nbsp;\nval kmeans = new KMeans().setK(20)\n&nbsp;\nval Array(trainingSet, testSet) = engineeringDF\n.select(\"features\")\n.randomSplit(Array(0.7, 0.3))\n&nbsp;\nval model = kmeans.fit(trainingSet)\n</code></pre><p>这里，我们准备把不同的房屋划分为20个不同的类别。完成训练之后，我们同样需要对模型效果进行评估。由于数据样本没有Label，因此，先前回归与分类的评估指标，不适合像K-means这样的非监督学习算法。</p><p>K-means的设计思想是“物以类聚”，既然如此，那么同一个类别中的向量应该足够地接近，而不同类别中向量之间的距离，应该越远越好。因此，我们可以用距离类的度量指标（如欧氏距离）来量化K-means的模型效果。</p><pre><code class=\"language-scala\">import org.apache.spark.ml.evaluation.ClusteringEvaluator\n&nbsp;\nval predictions = model.transform(trainingSet)\n&nbsp;\n// 定义聚类评估器\nval evaluator = new ClusteringEvaluator()\n&nbsp;\n// 计算所有向量到分类中心点的欧氏距离\nval euclidean = evaluator.evaluate(predictions)\n</code></pre><p>好啦，到此为止，我们使用非监督学习算法K-means，根据房屋向量，对房屋类型进行了划分。不过你要注意，使用这种方法划分出的类型，是没有真实含义的，比如它不能代表房屋质量，也不能代表房屋评级。既然如此，我们用K-means忙活了半天，图啥呢？</p><p>尽管K-means的结果没有真实含义，但是它以量化的形式，刻画了房屋之间的相似性与差异性。你可以这样来理解，我们用K-means为房屋生成了新的特征，相比现有的房屋属性，这个生成的新特征（Generated Features）往往与预测标的（如房价、房屋类型）有着更强的关联性，所以让这个新特性参与到监督学习的训练，就有希望优化/提升监督学习的模型效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/47/e5/474ec41fcf52dc20dd01fb79da8183e5.jpg?wh=1920x721\" alt=\"图片\" title=\"打卡回归、分类与聚类算法\"></p><p>好啦，到此为止，结合房价预测、房屋分类和房屋聚类三个实例，我们成功打卡了回归、分类和聚类这三类模型算法。恭喜你！离Spark MLlib模型算法通关，咱们还有一步之遥。在下一讲，我们会结合电影推荐的场景，继续学习两个有趣的模型算法：协同过滤与频繁项集。</p><h2>重点回顾</h2><p>今天这一讲，你首先需要掌握K-means算法的基本原理。聚类的设计思想，是“物以类聚、人以群分”，给定任意向量集合，K-means都可以把它划分为K个子集合，从而完成聚类。</p><p>K-means的计算主要依赖向量之间的相对距离，它的计算结果，一方面可以直接用于划分“人群”、“种群”，另一方面可以拿来当做生成特征，去参与到监督学习的训练中去。</p><p>此外，你需要掌握GBTRegressor和RandomForestClassifier的一般用法。其中，setLabelCol与setFeaturesCol分别用于指定模型的预测标的与特征向量。而setMaxDepth与setMaxIter分别用于设置模型的超参数，也即最大树深与最大迭代次数（决策树的数量），从而避免模型出现过拟合的情况。</p><h2>每课一练</h2><p>对于房价预测与房屋分类这两个场景，你觉得在它们之间，有代码（尤其是特征工程部分的代码）复用的必要和可能性吗？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这一讲的内容分享给更多的同事、朋友。</p>","comments":[{"had_liked":false,"id":324808,"user_name":"小新","can_delete":false,"product_type":"c1","uid":1690884,"ip_address":"","ucode":"DCAD04665E2CF8","user_header":"https://static001.geekbang.org/account/avatar/00/19/cd/04/e27b7803.jpg","comment_is_top":false,"comment_ctime":1638634236,"is_pvip":false,"replies":[{"id":"117828","content":"最开始，其实是想出一门Spark MLlib + 机器学习的课程，但是后来跟极客时间商量之后，大家觉得Spark + 机器学习，受众面更小，所以后来改成了基础入门课，也就是咱们现在这门课程《零基础入门Spark》，后面找机会写一些专门针对机器学习的东西吧~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1638674729,"ip_address":"","comment_id":324808,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1638634236","product_id":100090001,"comment_content":"老师后面可以出一个详细的ML的课程吗？也是这样的讲解方式。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536073,"discussion_content":"最开始，其实是想出一门Spark MLlib + 机器学习的课程，但是后来跟极客时间商量之后，大家觉得Spark + 机器学习，受众面更小，所以后来改成了基础入门课，也就是咱们现在这门课程《零基础入门Spark》，后面找机会写一些专门针对机器学习的东西吧~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638674729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322693,"user_name":"pythonbug","can_delete":false,"product_type":"c1","uid":1487274,"ip_address":"","ucode":"1A70CA92FFF8EB","user_header":"https://wx.qlogo.cn/mmopen/vi_32/wgMMrp1hvSB3E30KqZvMsj3KQdAI3T1uQM77LT7hZ65nVSjPGRg3AbUOyiahnssA6AIT5PAkyHFmlTBzUH9gdyQ/132","comment_is_top":false,"comment_ctime":1637552490,"is_pvip":true,"replies":[{"id":"117408","content":"老弟不必沮丧哈，机器学习就是这样，高级玩法试了一圈，发现效果并没有显著提升，有的时候，甚至更差，这个不必郁闷，大家一开始都是这样的~<br><br>根本原因在于，我们对于数据、特征、模型（学习器）之间的本质区别，并没有从根儿上理解到位。咱们这门入门课，机器学习部分，受课程要求所限，篇章太少了，实际上机器学习涉及的方方面面非常多。机器学习，并不是跑个模型，调调参数那么简单，否则的话，算法工程师的工资不会那么高~<br><br>模型效果，80%依赖特征，20%依赖模型选择、调优、调参，等等，我们从线性模型介绍到树模型，实际上，更多的是课程上的需要，为了给大家介绍更多种类型的模型，让大家知道，当需要类似模型和算法的时候，Spark都支持哪些，不支持哪些，从哪里选，怎么用。<br><br>但严格来说，并不是复杂模型，就一定比简单模型效果更好，在同一个假设空间，能够完美拟合训练数据的假设有多个，并不是复杂的模型，就是更好的。对于房价预测的例子，其实重中之重还是特征，老弟不妨去文中Kaggle对应的项目，聊一下排名靠前的那些项目，他们大部分都是使用各种“骚操作”来处理、生成特征，反而模型并不复杂。<br><br>由于咱们课程定位是入门，所以特征工程的部分，其实更得的，也是介绍方法，Spark MLlib支持哪些，不支持哪些，怎么用，真要用到的时候，怎么选。其实，咱们并没有针对这个项目，去仔细地做特征选择、特征生成，等等，这些非常必要的工作。<br><br>所以说，老弟不必气馁，咱们这里效果不好，主要是很多事情其实没多到位，并不是机器学习本身不给力，或是过于缥缈，一起加油哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637986512,"ip_address":"","comment_id":322693,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637552490","product_id":100090001,"comment_content":"老师好，房价预测那个例子，我用GBDT试了一下，结果发现比咱们一开始入门那个例子计算出来的rmse还要大。。没有明显变小，树的深度和最大棵数以及阈值都是和您一样的。<br>其中，预测对比我用的是：<br>val predictions: DataFrame = gbtModel.transform(testData).select(&quot;SalePriceInt&quot;, &quot;prediction&quot;)<br><br>    import org.apache.spark.ml.evaluation.RegressionEvaluator<br><br>    val evaluator = new RegressionEvaluator().setLabelCol(&quot;SalePriceInt&quot;).setPredictionCol(&quot;prediction&quot;).setMetricName(&quot;rmse&quot;)<br>    val rmse = evaluator.evaluate(predictions)<br>    println(&quot;Root Mean Squared Error (RMSE) on test data = &quot; + rmse)<br>然后代码中我还设置了一个maxBins，不设置会报错。修改后的如下：<br>val gbt = new GBTRegressor()<br>      .setLabelCol(&quot;SalePriceInt&quot;)<br>      .setFeaturesCol(&quot;indexedFeatures&quot;)<br>      &#47;&#47; 限定每棵树的最大深度<br>      .setMaxDepth(5)<br>      &#47;&#47; 限定决策树的最大棵树<br>      .setMaxIter(10)<br>      .setMaxBins(113)<br><br>就是很头疼，怎么用了好的模型，表现还变的糟糕了呢。<br>非常有兴趣的一直看到这里，突然觉得没有兴趣了。。。麻烦老师看看能不能帮忙指点一下。<br>全部代码比较长，贴不过来","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533819,"discussion_content":"老弟不必沮丧哈，机器学习就是这样，高级玩法试了一圈，发现效果并没有显著提升，有的时候，甚至更差，这个不必郁闷，大家一开始都是这样的~\n\n根本原因在于，我们对于数据、特征、模型（学习器）之间的本质区别，并没有从根儿上理解到位。咱们这门入门课，机器学习部分，受课程要求所限，篇章太少了，实际上机器学习涉及的方方面面非常多。机器学习，并不是跑个模型，调调参数那么简单，否则的话，算法工程师的工资不会那么高~\n\n模型效果，80%依赖特征，20%依赖模型选择、调优、调参，等等，我们从线性模型介绍到树模型，实际上，更多的是课程上的需要，为了给大家介绍更多种类型的模型，让大家知道，当需要类似模型和算法的时候，Spark都支持哪些，不支持哪些，从哪里选，怎么用。\n\n但严格来说，并不是复杂模型，就一定比简单模型效果更好，在同一个假设空间，能够完美拟合训练数据的假设有多个，并不是复杂的模型，就是更好的。对于房价预测的例子，其实重中之重还是特征，老弟不妨去文中Kaggle对应的项目，聊一下排名靠前的那些项目，他们大部分都是使用各种“骚操作”来处理、生成特征，反而模型并不复杂。\n\n由于咱们课程定位是入门，所以特征工程的部分，其实更得的，也是介绍方法，Spark MLlib支持哪些，不支持哪些，怎么用，真要用到的时候，怎么选。其实，咱们并没有针对这个项目，去仔细地做特征选择、特征生成，等等，这些非常必要的工作。\n\n所以说，老弟不必气馁，咱们这里效果不好，主要是很多事情其实没多到位，并不是机器学习本身不给力，或是过于缥缈，一起加油哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637986512,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321235,"user_name":"六月的余晖","can_delete":false,"product_type":"c1","uid":1182098,"ip_address":"","ucode":"758F107BAD0FEE","user_header":"https://static001.geekbang.org/account/avatar/00/12/09/92/201da8a7.jpg","comment_is_top":false,"comment_ctime":1636721320,"is_pvip":false,"replies":[{"id":"116632","content":"暂时还没有哈，杂务缠身，时间精力确实有限，等忙过这段时间再说吧，对不住老弟","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1636757387,"ip_address":"","comment_id":321235,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636721320","product_id":100090001,"comment_content":"老师后续有计划出Flink的课程吗","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530247,"discussion_content":"暂时还没有哈，杂务缠身，时间精力确实有限，等忙过这段时间再说吧，对不住老弟","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636757387,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}