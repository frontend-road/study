{"id":116111,"title":"31 | 常见工具脚本大汇总","content":"<p>你好，我是胡夕。今天我要跟你分享的主题是：Kafka常见的脚本汇总。</p><h2>命令行脚本概览</h2><p>Kafka默认提供了很多个命令行脚本，用于实现各种各样的功能和运维管理。今天我以2.2版本为例，详细地盘点下这些命令行工具。下图展示了2.2版本提供的所有命令行脚本。</p><p><img src=\"https://static001.geekbang.org/resource/image/f7/ee/f74fe9a3aef75f8db6533e1011571eee.png?wh=938*1611\" alt=\"\"></p><p>从图中我们可以知道，2.2版本总共提供了30个SHELL脚本。图中的windows实际上是个子目录，里面保存了Windows平台下的BAT批处理文件。其他的.sh文件则是Linux平台下的标准SHELL脚本。</p><p>默认情况下，不加任何参数或携带 --help运行SHELL文件，会得到该脚本的使用方法说明。下面这张图片展示了kafka-log-dirs脚本的调用方法。</p><p><img src=\"https://static001.geekbang.org/resource/image/04/cc/04953999a40ea439fce05a57b29714cc.png?wh=1950*940\" alt=\"\"></p><p>有了这些基础的了解，我来逐一地说明这些脚本的用途，然后再给你详细地介绍一些常见的脚本。</p><p>我们先来说说connect-standalone和connect-distributed两个脚本。这两个脚本是Kafka Connect组件的启动脚本。在专栏<a href=\"https://time.geekbang.org/column/article/100285\">第4讲</a>谈到Kafka生态时，我曾说过社区提供了Kafka Connect组件，用于实现Kafka与外部世界系统之间的数据传输。Kafka Connect支持单节点的Standalone模式，也支持多节点的Distributed模式。这两个脚本分别是这两种模式下的启动脚本。鉴于Kafka Connect不在我们的讨论范围之内，我就不展开讲了。</p><!-- [[[read_end]]] --><p>接下来是kafka-acls脚本。它是用于设置Kafka权限的，比如设置哪些用户可以访问Kafka的哪些主题之类的权限。在专栏后面，我会专门来讲Kafka安全设置的内容，到时候我们再细聊这个脚本。</p><p>下面是kafka-broker-api-versions脚本。<strong>这个脚本的主要目的是验证不同Kafka版本之间服务器和客户端的适配性</strong>。我来举个例子，下面这两张图分别展示了2.2版本Server端与2.2版本Client端和1.1.1版本Client端的适配性。</p><p><img src=\"https://static001.geekbang.org/resource/image/59/39/594776ae03ca045b3203f873b75c3139.png?wh=1854*275\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/a3/84/a3604bb34011c372d7f63912ffdf0584.png?wh=1854*249\" alt=\"\"></p><p>我截取了部分输出内容，现在我稍微解释一下这些输出的含义。我们以第一行为例：</p><blockquote>\n<p><em>Produce(0): 0 to 7 [usable: 7]</em></p>\n</blockquote><p>“Produce”表示Produce请求，生产者生产消息本质上就是向Broker端发送Produce请求。该请求是Kafka所有请求类型中的第一号请求，因此序号是0。后面的“0 to 7”表示Produce请求在Kafka 2.2中总共有8个版本，序号分别是0到7。“usable：7”表示当前连入这个Broker的客户端API能够使用的版本号是7，即最新的版本。</p><p>请注意这两张图中红线部分的差异。在第一张图中，我们使用2.2版本的脚本连接2.2版本的Broker，usable自然是7，表示能使用最新版本。在第二张图中，我们使用1.1版本的脚本连接2.2版本的Broker，usable是5，这表示1.1版本的客户端API只能发送版本号是5的Produce请求。</p><p>如果你想了解你的客户端版本与服务器端版本的兼容性，那么最好使用这个命令来检验一下。值得注意的是，<strong>在0.10.2.0之前，Kafka是单向兼容的，即高版本的Broker能够处理低版本Client发送的请求，反过来则不行。自0.10.2.0版本开始，Kafka正式支持双向兼容，也就是说，低版本的Broker也能处理高版本Client的请求了</strong>。</p><p>接下来是kafka-configs脚本。对于这个脚本，我想你应该已经很熟悉了，我们在讨论参数配置和动态Broker参数时都提到过它的用法，这里我就不再赘述了。</p><p>下面的两个脚本是重量级的工具行脚本：kafka-console-consumer和kafka-console-producer。在某种程度上，说它们是最常用的脚本也不为过。这里我们暂时先跳过，后面我会重点介绍它们。</p><p>关于producer和consumer，成组出现的还有另外一组脚本：kafka-producer-perf-test和kafka-consumer-perf-test。它们分别是生产者和消费者的性能测试工具，非常实用，稍后我会重点介绍。</p><p>接下来的kafka-consumer-groups命令，我在介绍重设消费者组位移时稍有涉及，后面我们来聊聊该脚本的其他用法。</p><p>kafka-delegation-tokens脚本可能不太为人所知，它是管理Delegation Token的。基于Delegation Token的认证是一种轻量级的认证机制，补充了现有的SASL认证机制。</p><p>kafka-delete-records脚本用于删除Kafka的分区消息。鉴于Kafka本身有自己的自动消息删除策略，这个脚本的实际出场率并不高。</p><p>kafka-dump-log脚本可谓是非常实用的脚本。它能查看Kafka消息文件的内容，包括消息的各种元数据信息，甚至是消息体本身。</p><p>kafka-log-dirs脚本是比较新的脚本，可以帮助查询各个Broker上的各个日志路径的磁盘占用情况。</p><p>kafka-mirror-maker脚本是帮助你实现Kafka集群间的消息同步的。在专栏后面，我会单独用一讲的内容来讨论它的用法。</p><p>kafka-preferred-replica-election脚本是执行Preferred Leader选举的。它可以为指定的主题执行“换Leader”的操作。</p><p>kafka-reassign-partitions脚本用于执行分区副本迁移以及副本文件路径迁移。</p><p>kafka-topics脚本你应该很熟悉了，所有的主题管理操作，都是由该脚本来实现的。</p><p>kafka-run-class脚本则颇为神秘，你可以用这个脚本执行任何带main方法的Kafka类。在Kafka早期的发展阶段，很多工具类都没有自己专属的SHELL脚本，比如刚才提到的kafka-dump-log，你只能通过运行kafka-run-class kafka.tools.DumpLogSegments的方式来间接实现。如果你用文本编辑器打开kafka-dump-log.sh，你会发现它实际上调用的就是这条命令。后来社区逐渐为这些重要的工具类都添加了专属的命令行脚本，现在kafka-run-class脚本的出场率大大降低了。在实际工作中，你几乎遇不上要直接使用这个脚本的场景了。</p><p>对于kafka-server-start和kafka-server-stop脚本，你应该不会感到陌生，它们是用于启动和停止Kafka Broker进程的。</p><p>kafka-streams-application-reset脚本用来给Kafka Streams应用程序重设位移，以便重新消费数据。如果你没有用到Kafka Streams组件，这个脚本对你来说是没有用的。</p><p>kafka-verifiable-producer和kafka-verifiable-consumer脚本是用来测试生产者和消费者功能的。它们是很“古老”的脚本了，你几乎用不到它们。另外，前面提到的Console Producer和Console Consumer完全可以替代它们。</p><p>剩下的zookeeper开头的脚本是用来管理和运维ZooKeeper的，这里我就不做过多介绍了。</p><p>最后说一下trogdor脚本。这是个很神秘的家伙，官网上也不曾出现它的名字。据社区内部资料显示，它是Kafka的测试框架，用于执行各种基准测试和负载测试。一般的Kafka用户应该用不到这个脚本。</p><p>好了，Kafka自带的所有脚本我全部梳理了一遍。虽然这些描述看起来有点流水账，但是，有了这些基础的认知，我们才能更好地利用这些脚本。下面我就来详细介绍一下重点的脚本操作。</p><h2>重点脚本操作</h2><h3>生产消息</h3><p>生产消息使用kafka-console-producer脚本即可，一个典型的命令如下所示：</p><pre><code>$ bin/kafka-console-producer.sh --broker-list kafka-host:port --topic test-topic --request-required-acks -1 --producer-property compression.type=lz4\n&gt;\n</code></pre><p>在这段命令中，我们指定生产者参数acks为-1，同时启用了LZ4的压缩算法。这个脚本可以很方便地让我们使用控制台来向Kafka的指定主题发送消息。</p><h3>消费消息</h3><p>下面再来说说数据消费。如果要快速地消费主题中的数据来验证消息是否存在，运行kafka-console-consumer脚本应该算是最便捷的方法了。常用的命令用法如下：</p><pre><code>$ bin/kafka-console-consumer.sh --bootstrap-server kafka-host:port --topic test-topic --group test-group --from-beginning --consumer-property enable.auto.commit=false \n</code></pre><p>注意，在这段命令中，我们指定了group信息。如果没有指定的话，每次运行Console Consumer，它都会自动生成一个新的消费者组来消费。久而久之，你会发现你的集群中有大量的以console-consumer开头的消费者组。通常情况下，你最好还是加上group。</p><p>另外，from-beginning等同于将Consumer端参数auto.offset.reset设置成earliest，表明我想从头开始消费主题。如果不指定的话，它会默认从最新位移读取消息。如果此时没有任何新消息，那么该命令的输出为空，你什么都看不到。</p><p>最后，我在命令中禁掉了自动提交位移。通常情况下，让Console Consumer提交位移是没有意义的，毕竟我们只是用它做一些简单的测试。</p><h3>测试生产者性能</h3><p>如果你想要对Kafka做一些简单的性能测试。那么不妨试试下面这一组工具。它们分别用于测试生产者和消费者的性能。</p><p>我们先说测试生产者的脚本：<strong>kafka-producer-perf-test</strong>。它的参数有不少，但典型的命令调用方式是这样的。</p><pre><code>$ bin/kafka-producer-perf-test.sh --topic test-topic --num-records 10000000 --throughput -1 --record-size 1024 --producer-props bootstrap.servers=kafka-host:port acks=-1 linger.ms=2000 compression.type=lz4\n\n2175479 records sent, 435095.8 records/sec (424.90 MB/sec), 131.1 ms avg latency, 681.0 ms max latency.\n4190124 records sent, 838024.8 records/sec (818.38 MB/sec), 4.4 ms avg latency, 73.0 ms max latency.\n10000000 records sent, 737463.126844 records/sec (720.18 MB/sec), 31.81 ms avg latency, 681.00 ms max latency, 4 ms 50th, 126 ms 95th, 604 ms 99th, 672 ms 99.9th.\n</code></pre><p>上述命令向指定主题发送了1千万条消息，每条消息大小是1KB。该命令允许你在producer-props后面指定要设置的生产者参数，比如本例中的压缩算法、延时时间等。</p><p>该命令的输出值得好好说一下。它会打印出测试生产者的吞吐量(MB/s)、消息发送延时以及各种分位数下的延时。一般情况下，消息延时不是一个简单的数字，而是一组分布。或者说，<strong>我们应该关心延时的概率分布情况，仅仅知道一个平均值是没有意义的</strong>。这就是这里计算分位数的原因。通常我们关注到<strong>99th分位</strong>就可以了。比如在上面的输出中，99th值是604ms，这表明测试生产者生产的消息中，有99%消息的延时都在604ms以内。你完全可以把这个数据当作这个生产者对外承诺的SLA。</p><h3>测试消费者性能</h3><p>测试消费者也是类似的原理，只不过我们使用的是<strong>kafka-consumer-perf-test</strong>脚本，命令如下：</p><pre><code>$ bin/kafka-consumer-perf-test.sh --broker-list kafka-host:port --messages 10000000 --topic test-topic\nstart.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec\n2019-06-26 15:24:18:138, 2019-06-26 15:24:23:805, 9765.6202, 1723.2434, 10000000, 1764602.0822, 16, 5651, 1728.1225, 1769598.3012\n</code></pre><p>虽然输出格式有所差别，但该脚本也会打印出消费者的吞吐量数据。比如本例中的1723MB/s。有点令人遗憾的是，它没有计算不同分位数下的分布情况。因此，在实际使用过程中，这个脚本的使用率要比生产者性能测试脚本的使用率低。</p><h3>查看主题消息总数</h3><p>很多时候，我们都想查看某个主题当前的消息总数。令人惊讶的是，Kafka自带的命令竟然没有提供这样的功能，我们只能“绕道”获取了。所谓的绕道，是指我们必须要调用一个未被记录在官网上的命令。命令如下：</p><pre><code>$ bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka-host:port --time -2 --topic test-topic\n\ntest-topic:0:0\ntest-topic:1:0\n\n$ bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list kafka-host:port --time -1 --topic test-topic\n\ntest-topic:0:5500000\ntest-topic:1:5500000\n</code></pre><p>我们要使用Kafka提供的工具类<strong>GetOffsetShell</strong>来计算给定主题特定分区当前的最早位移和最新位移，将两者的差值累加起来，就能得到该主题当前总的消息数。对于本例来说，test-topic总的消息数为5500000 + 5500000，等于1100万条。</p><h3>查看消息文件数据</h3><p>作为Kafka使用者，你是不是对Kafka底层文件里面保存的内容很感兴趣? 如果是的话，你可以使用kafka-dump-log脚本来查看具体的内容。</p><pre><code>$ bin/kafka-dump-log.sh --files ../data_dir/kafka_1/test-topic-1/00000000000000000000.log \nDumping ../data_dir/kafka_1/test-topic-1/00000000000000000000.log\nStarting offset: 0\nbaseOffset: 0 lastOffset: 14 count: 15 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1561597044933 size: 1237 magic: 2 compresscodec: LZ4 crc: 646766737 isvalid: true\nbaseOffset: 15 lastOffset: 29 count: 15 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 1237 CreateTime: 1561597044934 size: 1237 magic: 2 compresscodec: LZ4 crc: 3751986433 isvalid: true\n......\n</code></pre><p>如果只是指定 --files，那么该命令显示的是消息批次（RecordBatch）或消息集合（MessageSet）的元数据信息，比如创建时间、使用的压缩算法、CRC校验值等。</p><p><strong>如果我们想深入看一下每条具体的消息，那么就需要显式指定 --deep-iteration参数</strong>，如下所示：</p><pre><code>$ bin/kafka-dump-log.sh --files ../data_dir/kafka_1/test-topic-1/00000000000000000000.log --deep-iteration\nDumping ../data_dir/kafka_1/test-topic-1/00000000000000000000.log\nStarting offset: 0\nbaseOffset: 0 lastOffset: 14 count: 15 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1561597044933 size: 1237 magic: 2 compresscodec: LZ4 crc: 646766737 isvalid: true\n| offset: 0 CreateTime: 1561597044911 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 1 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 2 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 3 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 4 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 5 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 6 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 7 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 8 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 9 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 10 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 11 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 12 CreateTime: 1561597044932 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 13 CreateTime: 1561597044933 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\n| offset: 14 CreateTime: 1561597044933 keysize: -1 valuesize: 1024 sequence: -1 headerKeys: []\nbaseOffset: 15 lastOffset: 29 count: 15 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 1237 CreateTime: 1561597044934 size: 1237 magic: 2 compresscodec: LZ4 crc: 3751986433 isvalid: true\n......\n</code></pre><p>在上面的输出中，以竖线开头的就是消息批次下的消息信息。如果你还想看消息里面的实际数据，那么还需要指定 <strong> --print-data-log参数</strong>，如下所示：</p><pre><code>$ bin/kafka-dump-log.sh --files ../data_dir/kafka_1/test-topic-1/00000000000000000000.log --deep-iteration --print-data-log\n</code></pre><h3>查询消费者组位移</h3><p>接下来，我们来看如何使用kafka-consumer-groups脚本查看消费者组位移。在上一讲讨论重设消费者组位移的时候，我们使用的也是这个命令。当时我们用的是 <strong> --reset-offsets参数</strong>，今天我们使用的是 <strong> --describe参数</strong>。假设我们要查询Group  ID是test-group的消费者的位移，那么命令如图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/ee/f4b7d92cdebff84998506afece1f61ee.png?wh=2100*146\" alt=\"\"></p><p>图中的CURRENT-OFFSET表示该消费者当前消费的最新位移，LOG-END-OFFSET表示对应分区最新生产消息的位移，LAG列是两者的差值。CONSUMER-ID是Kafka消费者程序自动生成的一个ID。截止到2.2版本，你都无法干预这个ID的生成过程。如果运行该命令时，这个消费者程序已经终止了，那么此列的值为空。</p><h2>小结</h2><p>好了，我们小结一下。今天我们一起梳理了Kafka 2.2版本自带的所有脚本，我给出了常见的运维操作的工具行命令。希望这些命令对你操作和管理Kafka集群有所帮助。另外，我想强调的是，由于Kafka依然在不断演进，我们今天提到的命令的用法很可能会随着版本的变迁而发生变化。在具体使用这些命令时，你最好详细地阅读一下它们的Usage说明。</p><p><img src=\"https://static001.geekbang.org/resource/image/5b/32/5b0daf262cde0c853a5cf9fbc9dfa332.jpg?wh=2069*2580\" alt=\"\"></p><h2>开放讨论</h2><p>你在使用Kafka命令的过程中，曾经踩过哪些“坑”，或者说有哪些惨痛的经历呢？</p><p>欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"30 | 怎么重设消费者组位移？","id":116070},"right":{"article_title":"32 | KafkaAdminClient：Kafka的运维利器","id":118319}},"comments":[{"had_liked":false,"id":123254,"user_name":"玉剑冰锋","can_delete":false,"product_type":"c1","uid":1214202,"ip_address":"","ucode":"8EA56A71BA5B22","user_header":"https://static001.geekbang.org/account/avatar/00/12/86/fa/4bcd7365.jpg","comment_is_top":false,"comment_ctime":1565654149,"is_pvip":false,"replies":[{"id":"45265","content":"1. 第一个办法挺好的；2. 删除&#47;controller有点狠，不如删除&#47;admin&#47;reassign_partitions","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565656813,"ip_address":"","comment_id":123254,"utype":1}],"discussion_count":1,"race_medal":0,"score":"160479444101","product_id":100029201,"comment_content":"要说坑遇到两个，这两个坑都是留言提问老师帮忙解答，非常感谢！第一个就是数据量过大导致重启单台kafka时间近一个小时才能恢复，通过调大num.recovery.threads.per.data.dir解决;第二个就是分区迁移，出现in progress或者failed状态，通过zk删除&#47;controller来解决，测试环境测试没问题，生产出现同样问题，但是担心大量分区重新选举leader，所以一直没有试，不知道老师还有没有其他好办法","like_count":37,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462598,"discussion_content":"1. 第一个办法挺好的；2. 删除/controller有点狠，不如删除/admin/reassign_partitions","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565656813,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":150324,"user_name":"注定非凡","can_delete":false,"product_type":"c1","uid":1113597,"ip_address":"","ucode":"80673056E131B7","user_header":"https://static001.geekbang.org/account/avatar/00/10/fd/fd/326be9bb.jpg","comment_is_top":false,"comment_ctime":1573519373,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"48818159629","product_id":100029201,"comment_content":"2.2版本提供了30多个Shell脚本<br>\t（1）：connect-standalone：支持kafka Connect组件支持单节点Standalone模式<br>\t（2）：connect-distributed：支持多节点的Distributed模式。<br><br>\t（3）：kafka-acls：用于设置Kafka权限，如设置哪些用户可以访问哪些主题之类的权限。<br>\t（4）：kafka-broker-api-versions：主要目的是验证不同Kafka版本之间服务器和客户端的适配性。<br>\t（5）：kafka-configs：用于配置管理<br><br>\t（6）：kafka-console-consumer:<br>\t（7）：kafka-console-producer:<br><br>\t（8）：kafka-producer-perf-test和kafka-consumer-perf-test :用于生产者和消费者的性能测试<br><br>\t（9）：kafka-consumer-groups：消费者位移时多有涉及<br>\t（10）：kafka-delegation-tokens：管理Delegation Token的，基于Delegation Token的认证是一种轻量级的认证机制，补充了现有的SASL认证机制。<br><br>\t（11）：kafka-delete-records：用于删除Kafka的分区消息。<br>\t（12）：kafka-dump-log：能够查看kafka消息文件的内容，包括消息的各种元数据信息<br>\t（13）：kafka-log-dirs:可以帮助查询各个Broker上的各个日志路径的磁盘占用情况<br>\t（14）：kafka-mirror-maker：可以帮助实现kafka集群间消息同步<br>\t（15）：kafka-preferred-replica-election：执行Preferred Leader选举。他可以为指定的主题执行“换Leader”的操作。<br>\t（16）：kafka-reassign-partitions：用于执行分区副本迁移以及副本文件路径迁移<br>\t（17）：kafka-topics：所有主题管理操作，都是有该脚本来实现。<br>\t（18）：kafka-run-class：可以用这个脚本执行任何带main方法的Kafka类。<br><br>\t（19）：kafka-server-start和kafka-server-stop：启动和停止Kafka Broker进程<br>\t（20）：kafka-streams-application-reset：用来给kafka-Streams应用程序重试位移，以便重新消费数据。<br><br>\t（21）：kafka-verifiabel-producer和kafka-verifiable-consumer是用来测试生产者和消费者功能的。<br>\t<br>\t（22）：zookeeper开头的脚本是用来管理和运维Zookeeper的。<br>","like_count":11},{"had_liked":false,"id":129695,"user_name":"What for","can_delete":false,"product_type":"c1","uid":1254381,"ip_address":"","ucode":"1308573CE047B0","user_header":"https://static001.geekbang.org/account/avatar/00/13/23/ed/a4a774a8.jpg","comment_is_top":false,"comment_ctime":1567221961,"is_pvip":false,"replies":[{"id":"48513","content":"没有其他取值了。-1表示latest，-2表示earliest","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1567386099,"ip_address":"","comment_id":129695,"utype":1}],"discussion_count":2,"race_medal":0,"score":"44516894921","product_id":100029201,"comment_content":"请问老师查看主题分区消息总数 run-class 脚本中的 --time 参数，-1 和 -2 分别代表什么意思？还有其他的取值么？谢谢！","like_count":10,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465607,"discussion_content":"没有其他取值了。-1表示latest，-2表示earliest","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567386099,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1336475,"avatar":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","nickname":"J.Smile","note":"","ucode":"C4D98DFDBF7584","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":284767,"discussion_content":"good","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592631511,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":190137,"user_name":"大坏狐狸","can_delete":false,"product_type":"c1","uid":1325678,"ip_address":"","ucode":"5044F89A505C5B","user_header":"https://static001.geekbang.org/account/avatar/00/14/3a/6e/e39e90ca.jpg","comment_is_top":false,"comment_ctime":1584606269,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"35944344637","product_id":100029201,"comment_content":"阅读到后面发现人变少了，应该是很多人没有坚持下来吧。我希望自己能看完","like_count":8,"discussions":[{"author":{"id":1134861,"avatar":"https://static001.geekbang.org/account/avatar/00/11/51/0d/fc1652fe.jpg","nickname":"James","note":"","ucode":"48B0F2A334D1C1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":288678,"discussion_content":"哈哈,一开始看仅仅是看工作使用到的,我现在看到这里是多余时间看下来的.哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593839849,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":159791,"user_name":"丁丁历险记","can_delete":false,"product_type":"c1","uid":1661704,"ip_address":"","ucode":"A43829E454C067","user_header":"https://static001.geekbang.org/account/avatar/00/19/5b/08/b0b0db05.jpg","comment_is_top":false,"comment_ctime":1575790050,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31640561122","product_id":100029201,"comment_content":"where there is a shell  there is a way","like_count":7},{"had_liked":false,"id":123310,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1565658456,"is_pvip":false,"replies":[{"id":"45287","content":"client id 主要用于区分JMX指标和日志中的不同consumer； consumer id 指的是member id，目前是Kafka自动生成的，对用户意义不大；group id是表示consumer group组id的，非常重要，必须要指定。","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565674547,"ip_address":"","comment_id":123310,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31630429528","product_id":100029201,"comment_content":"client id, consumer id, consumer group id这几个id作用和区别是什么？","like_count":7,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462618,"discussion_content":"client id 主要用于区分JMX指标和日志中的不同consumer； consumer id 指的是member id，目前是Kafka自动生成的，对用户意义不大；group id是表示consumer group组id的，非常重要，必须要指定。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565674547,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123316,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1565658560,"is_pvip":false,"replies":[{"id":"45286","content":"嗯嗯，确实是。因为是不同的人写的。你可以提一个KIP，然后把它们统一一下：）","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565674494,"ip_address":"","comment_id":123316,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27335462336","product_id":100029201,"comment_content":"broker-list和bootstrap-servers参数感觉用得很混乱啊，有什么规律吗？能否统一一下？","like_count":7,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462620,"discussion_content":"嗯嗯，确实是。因为是不同的人写的。你可以提一个KIP，然后把它们统一一下：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565674494,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":220222,"user_name":"Geek_bb","can_delete":false,"product_type":"c1","uid":1812606,"ip_address":"","ucode":"AF771478D5F3A0","user_header":"","comment_is_top":false,"comment_ctime":1590208523,"is_pvip":false,"replies":[{"id":"81432","content":"副本是一个commit log，里面保存了多条消息。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1590329601,"ip_address":"","comment_id":220222,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10180143115","product_id":100029201,"comment_content":"副本是一个消息还是多个消息打包？<br>这个问题困扰了很久，没有看到有提到，据我所知gemfire就是副本就是多个record，这样能够减少传输大小。<br>希望得到老师的解答。谢谢","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496093,"discussion_content":"副本是一个commit log，里面保存了多条消息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590329601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123267,"user_name":"蒙开强","can_delete":false,"product_type":"c1","uid":1317706,"ip_address":"","ucode":"61B3183781B9F7","user_header":"https://static001.geekbang.org/account/avatar/00/14/1b/4a/f9df2d06.jpg","comment_is_top":false,"comment_ctime":1565656142,"is_pvip":false,"replies":[{"id":"45263","content":"主要是邮件组。","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565656729,"ip_address":"","comment_id":123267,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10155590734","product_id":100029201,"comment_content":"老师，你好，这个kafka的社区从哪里可以看呢，有时候看官网没有详细说明，比如我在官网看kafka幂等性，上面只有参数设定，并没有详细说明","like_count":3,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462602,"discussion_content":"主要是邮件组。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565656729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1034204,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c7/dc/9408c8c2.jpg","nickname":"ban","note":"","ucode":"E523CE97E48266","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6326,"discussion_content":"什么邮件组","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566832901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1333607,"avatar":"https://static001.geekbang.org/account/avatar/00/14/59/67/f4ba1da4.jpg","nickname":"Hello world","note":"","ucode":"4D2EF3034571B7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5444,"discussion_content":"什么邮件组","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566272073,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123778,"user_name":"Geek_edc612","can_delete":false,"product_type":"c1","uid":1564943,"ip_address":"","ucode":"3E01DE3CE4BF3E","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJUJKviaecwxpAZCAnHWap86kXUichv5JwUoAtrUNy4ugC0kMMmssFDdyayKFgAoA9Z62sqMZaibbvUg/132","comment_is_top":false,"comment_ctime":1565755180,"is_pvip":false,"replies":[{"id":"45611","content":"确认下broker是否启动成功了吧","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565856162,"ip_address":"","comment_id":123778,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5860722476","product_id":100029201,"comment_content":"老师，我最近线上集群遇到了一个奇怪的情况，部分topic设置的是3副本，但是所有分区都是只有一个isr，不知道这种情况是什么原因导致的？<br>","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462811,"discussion_content":"确认下broker是否启动成功了吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565856162,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":353852,"user_name":"ppd0705","can_delete":false,"product_type":"c1","uid":1155646,"ip_address":"湖南","ucode":"EB63D4E3FD1E9A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKotsBr2icbYNYlRSlicGUD1H7lulSTQUAiclsEz9gnG5kCW9qeDwdYtlRMXic3V6sj9UrfKLPJnQojag/132","comment_is_top":false,"comment_ctime":1659863163,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1659863163","product_id":100029201,"comment_content":"可以用 kafka-get-offsets.sh 查看消费位置了","like_count":0},{"had_liked":false,"id":340082,"user_name":"ABC","can_delete":false,"product_type":"c1","uid":1054958,"ip_address":"","ucode":"7501AD9C0C4A70","user_header":"https://static001.geekbang.org/account/avatar/00/10/18/ee/a1ed60d1.jpg","comment_is_top":false,"comment_ctime":1648598908,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1648598908","product_id":100029201,"comment_content":"https:&#47;&#47;github.com&#47;apache&#47;kafka&#47;tree&#47;trunk&#47;bin 最新版的kafka，大概提供了接近40个脚本。","like_count":0},{"had_liked":false,"id":326222,"user_name":"18923814485","can_delete":false,"product_type":"c1","uid":1367070,"ip_address":"","ucode":"12C6A79B360EB5","user_header":"","comment_is_top":false,"comment_ctime":1639444458,"is_pvip":false,"replies":[{"id":"118457","content":"可以看下具体jar包的版本","user_name":"作者回复","user_name_real":"编辑","uid":"1288090","ctime":1639451006,"ip_address":"","comment_id":326222,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639444458","product_id":100029201,"comment_content":"kafka-broker-api-versions怎么知道现在的各个请求是哪个版本，比如图中给出的是2.1和1.1版本的客户端支持的produce请求的版本，但是实际使用过程中，怎么判断当前client和server是处于不兼容的状态？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":538589,"discussion_content":"可以看下具体jar包的版本","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639451006,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292085,"user_name":"Geek_388a76","can_delete":false,"product_type":"c1","uid":1443709,"ip_address":"","ucode":"7366E03EAF6DA6","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/4uX05lCn7jZlQ7S7w6CJ8lanbSSTdx66X0bUZsdzdWx6zqwiazRhicaEjTF0DI6srTMrtKcLibe3N1xiatCfvzdTFw/132","comment_is_top":false,"comment_ctime":1620666116,"is_pvip":false,"replies":[{"id":"105883","content":"你先看看是不是用了老版本的consumer，那么需要使用--zookeeper来查询位移","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1620818977,"ip_address":"","comment_id":292085,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620666116","product_id":100029201,"comment_content":"老师，最近项目用的kafka是0.11.0.3版本的，遇到一个问题：<br>在生产集群使用&#47;kafka&#47;bin&#47;kafka-consumer-groups.sh  --bootstrap-server master1:9092，master2:9092  --list这个命令没办法获取到消费者组<br>测试环境单机版又可以获取到，<br>请问下这是因为集群的原因吗？<br>项目是storm结合kafka的，kafka作为storm的spout,还是困惑这个命令使用是限制storm的api还是说环集群环境没效的原因。<br>非常感谢！<br>","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519675,"discussion_content":"你先看看是不是用了老版本的consumer，那么需要使用--zookeeper来查询位移","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620818977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":261316,"user_name":"雄鹰","can_delete":false,"product_type":"c1","uid":1786819,"ip_address":"","ucode":"67E0C4BDE7F6F2","user_header":"https://static001.geekbang.org/account/avatar/00/1b/43/c3/2c53acd7.jpg","comment_is_top":false,"comment_ctime":1605267797,"is_pvip":true,"replies":[{"id":"94900","content":"都是异步的，不区分同步异步","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1605422917,"ip_address":"","comment_id":261316,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1605267797","product_id":100029201,"comment_content":"老师您好，我用的kafka是2.3.0版本，用 kafka-producer-perf-test.sh测试生产者的性能基准，请问这个版本还区分同步和异步的方式吗？在网上查了一下，有资料说新版本的都是异步的了，不再区分同步和异步？请老师帮忙确认一下，谢谢！","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":509447,"discussion_content":"都是异步的，不区分同步异步","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605422917,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":232039,"user_name":"James","can_delete":false,"product_type":"c1","uid":1134861,"ip_address":"","ucode":"48B0F2A334D1C1","user_header":"https://static001.geekbang.org/account/avatar/00/11/51/0d/fc1652fe.jpg","comment_is_top":false,"comment_ctime":1593839782,"is_pvip":false,"replies":[{"id":"85634","content":"是指users和dev两个邮件组","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593868395,"ip_address":"","comment_id":232039,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593839782","product_id":100029201,"comment_content":"请问老师,邮件组是啥.<br>在官网上查使用说明,有时候仅仅只是参数说明;","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500486,"discussion_content":"是指users和dev两个邮件组","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593868395,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":231104,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1593583835,"is_pvip":false,"replies":[{"id":"85363","content":"Broker启动时会打印","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593607027,"ip_address":"","comment_id":231104,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593583835","product_id":100029201,"comment_content":"怎么打印出broker端所有的默认参数？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500159,"discussion_content":"Broker启动时会打印","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593607027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":223036,"user_name":"Action","can_delete":false,"product_type":"c1","uid":1239234,"ip_address":"","ucode":"FFFD1537C6BB3C","user_header":"https://static001.geekbang.org/account/avatar/00/12/e8/c2/77a413a7.jpg","comment_is_top":false,"comment_ctime":1590984877,"is_pvip":false,"replies":[{"id":"82336","content":"这是首次执行吗？如果不是可能该topic的数据已经消费完了。另外看下second主题下真有数据可以消费吗？最后就是确认下Kafka集群的连通性","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1591094199,"ip_address":"","comment_id":223036,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1590984877","product_id":100029201,"comment_content":".&#47;kafka-console-consumer.sh --bootstrap-server 192.168.1.67:9092 --topic second --group second --from-beginning --consumer-property enable.auto.commit=false<br>老师您好，问个简单得问题，我使用上面的命令消费数据，就是整体hang住了，并没有消费，请问是什么原因呀？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496995,"discussion_content":"这是首次执行吗？如果不是可能该topic的数据已经消费完了。另外看下second主题下真有数据可以消费吗？最后就是确认下Kafka集群的连通性","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591094199,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219236,"user_name":"地下城勇士","can_delete":false,"product_type":"c1","uid":1739992,"ip_address":"","ucode":"91ACD73D56A9F1","user_header":"","comment_is_top":false,"comment_ctime":1589970867,"is_pvip":false,"replies":[{"id":"80976","content":"这方面的资料不多。网上的多是一些零星资料。不过你还是可以看看这个：https:&#47;&#47;docs.confluent.io&#47;current&#47;connect&#47;index.html","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1589974278,"ip_address":"","comment_id":219236,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1589970867","product_id":100029201,"comment_content":"请问一下老师，kafka-connect有什么相关资料吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495730,"discussion_content":"这方面的资料不多。网上的多是一些零星资料。不过你还是可以看看这个：https://docs.confluent.io/current/connect/index.html","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589974278,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":136346,"user_name":"XThundering","can_delete":false,"product_type":"c1","uid":1343394,"ip_address":"","ucode":"B301F36E7EEB5E","user_header":"https://static001.geekbang.org/account/avatar/00/14/7f/a2/8ccf5c85.jpg","comment_is_top":false,"comment_ctime":1569413006,"is_pvip":false,"replies":[{"id":"52307","content":"端口已被占用，查一下吧","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1569457006,"ip_address":"","comment_id":136346,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1569413006","product_id":100029201,"comment_content":"Error: Exception thrown by the agent:java.rmi.server.ExportException:Port alread in use,出现这个Exception,kafka 版本号2.2.1","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":468584,"discussion_content":"端口已被占用，查一下吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569457006,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":134954,"user_name":"hpfish","can_delete":false,"product_type":"c1","uid":1217377,"ip_address":"","ucode":"86DB066E5B6D89","user_header":"https://static001.geekbang.org/account/avatar/00/12/93/61/3e4607c7.jpg","comment_is_top":false,"comment_ctime":1568962162,"is_pvip":false,"replies":[{"id":"51956","content":"你是否指定了group.id呢？ ","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1569201323,"ip_address":"","comment_id":134954,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1568962162","product_id":100029201,"comment_content":"老师,我在使用 kafka-console-consumer 消费一个主题时会出现没有任何输出的情况,但是如果通过--partition 指定消费该主题的某个分区,就能够成功消费到消息,这种情况您遇到过吗","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467974,"discussion_content":"你是否指定了group.id呢？ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569201323,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1217377,"avatar":"https://static001.geekbang.org/account/avatar/00/12/93/61/3e4607c7.jpg","nickname":"hpfish","note":"","ucode":"86DB066E5B6D89","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":23441,"discussion_content":"指定了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569818237,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":125341,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1566170694,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1566170694","product_id":100029201,"comment_content":"打卡<br>1：默认情况下，不加任何参数或携带 --help 运行 SHELL 文件，会得到该脚本的使用方法说明。<br>这个比较有意思，各种命令脚本都可以先使用这钟方式，查看一下具体的使用说明。<br><br>2：重点脚本功能<br>2-1：kafka-console-producer——生产消息<br>2-2：kafka-console-consumer——消费消息<br>2-3：kafka-producer-perf-test——测试生产者性能<br>2-4：kafka-consumer-perf-test——测试消费者性能<br>2-5：kafka-dump-log——查看消息日志的具体内容<br>2-6：kafka-consumer-groups——查看消费者组位移<br>","like_count":0},{"had_liked":false,"id":125106,"user_name":"wgcris","can_delete":false,"product_type":"c1","uid":1527666,"ip_address":"","ucode":"842B76EB6B8320","user_header":"","comment_is_top":false,"comment_ctime":1566087504,"is_pvip":false,"replies":[{"id":"46041","content":"嗯嗯，其实也不需要太精细化的优化，给pagecache越大越好就行了","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1566181567,"ip_address":"","comment_id":125106,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1566087504","product_id":100029201,"comment_content":"老师，你好，我想请教一下，关于Kafka pagecache的优化，目前社区有好的解决方案吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":463436,"discussion_content":"嗯嗯，其实也不需要太精细化的优化，给pagecache越大越好就行了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566181567,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1527666,"avatar":"","nickname":"wgcris","note":"","ucode":"842B76EB6B8320","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5621,"discussion_content":"不好意思我可能没有表述清楚，我说的是因为pagecache不可控，可能存在pagecache污染的情况，这种场景下会对客户端读写有很大的影响，针对这种场景是否有一些解决方法？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566387036,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123848,"user_name":"xiaoniu","can_delete":false,"product_type":"c1","uid":1166713,"ip_address":"","ucode":"837893E7D9691E","user_header":"https://static001.geekbang.org/account/avatar/00/11/cd/79/00dc7885.jpg","comment_is_top":false,"comment_ctime":1565771256,"is_pvip":true,"replies":[{"id":"45639","content":"目前只能依赖应用自行实现","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565876416,"ip_address":"","comment_id":123848,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1565771256","product_id":100029201,"comment_content":"老师，你好，kafka中有没有比较好用的延时消费操作，目前工作中，很多kafka数据不能直接立刻消费，而是要等几十秒（依赖第三方数据到位后），才能消费。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462843,"discussion_content":"目前只能依赖应用自行实现","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565876416,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2165300,"avatar":"https://static001.geekbang.org/account/avatar/00/21/0a/34/3e29eb5a.jpg","nickname":"尚小树","note":"","ucode":"9BCB0DF0F3C8E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":308507,"discussion_content":"老师我也遇到了同样的问题\n消费消息时要等待某些资源，资源到位的时间范围是0~30秒之间。\n我的办法是，如果发现资源还没到就开个线程先睡一会（10秒），之后再把当前的消息重新写回kafka的这个topic中，这样重试N次（3次）还没有等到就认为消息不可达，记录错误日志。请问老师我这样设计是否合理呢？老师有没有更好的办法？\n另外，一路听下来，超级喜欢老师的声音，听着音频读着文稿是下班后最享受的时光了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600964071,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123616,"user_name":"我来也","can_delete":false,"product_type":"c1","uid":1205253,"ip_address":"","ucode":"773D6104F56767","user_header":"https://static001.geekbang.org/account/avatar/00/12/64/05/6989dce6.jpg","comment_is_top":false,"comment_ctime":1565713368,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565713368","product_id":100029201,"comment_content":"晚上才有时间看专栏。<br>今天白天就花在这些脚本上一个多小时吧。<br>kafka-consumer-groups.sh —help 居然没有提示。<br>好像是 kafka 2.11的（不太确定了）<br>由于配置了kafka账号验证的信息，导致涉及到kafka的网上的命令都自信不成功，zookeeper的可以成功。<br>返回的失败就是timeout，自己也猜测可能跟权限相关。<br>想看下参数帮助也没有，最后花了近个把小时才在网上找到参数，填一个配置文件。<br>哎，自己解决问题起来太慢了。","like_count":0},{"had_liked":false,"id":123463,"user_name":"大力水手","can_delete":false,"product_type":"c1","uid":1367822,"ip_address":"","ucode":"D44F1ABFE4C5C8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erkCRjibSLNBsK2umhGU2dFy4KUKbZjf7WdibiaL2a0icqvLwC4QskLCCehSnQYLuJsrvT5uVjYSETtMQ/132","comment_is_top":false,"comment_ctime":1565680981,"is_pvip":false,"replies":[{"id":"45325","content":"你是说设置这个参数吗？可以使用kafka-configs脚本","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1565742307,"ip_address":"","comment_id":123463,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1565680981","product_id":100029201,"comment_content":"retention.ms 保留时长有脚本吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462684,"discussion_content":"你是说设置这个参数吗？可以使用kafka-configs脚本","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565742307,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1367822,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erkCRjibSLNBsK2umhGU2dFy4KUKbZjf7WdibiaL2a0icqvLwC4QskLCCehSnQYLuJsrvT5uVjYSETtMQ/132","nickname":"大力水手","note":"","ucode":"D44F1ABFE4C5C8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":4788,"discussion_content":"# 设置日志保留时长为72小时\nkafka-configs.sh --zookeeper localhost:2181  --entity-type topics --entity-name test --alter --add-config retention.ms=259200000\n# 设置后需要刷新一下配置空间才会收缩\nkafka-topics.sh --zookeeper localhost:2181 --describe --topics-with-overrides","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565742729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1367822,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erkCRjibSLNBsK2umhGU2dFy4KUKbZjf7WdibiaL2a0icqvLwC4QskLCCehSnQYLuJsrvT5uVjYSETtMQ/132","nickname":"大力水手","note":"","ucode":"D44F1ABFE4C5C8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":4787,"discussion_content":"恩，谢谢、已经解决了。执行一段时间后才会释放空间，不知道是执行了overwrite覆盖参数空间才会释放还是内部执行时间到了空间释放的？执行完第一句空间好像并没有释放。等号这里是kv对，有空格也不会生效。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565742720,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}