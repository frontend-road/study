{"id":248863,"title":"05 | 计数服务设计（上）","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/geektime-distributed\">https://gitee.com/geektime-geekbang/geektime-distributed</a></p>","comments":[{"had_liked":false,"id":229903,"user_name":"zhou","can_delete":false,"product_type":"c3","uid":1087243,"ip_address":"","ucode":"E1CE8575B3F106","user_header":"https://static001.geekbang.org/account/avatar/00/10/97/0b/a943bcb3.jpg","comment_is_top":false,"comment_ctime":1593182483,"is_pvip":false,"replies":[{"id":84921,"content":"对，课程演示的是一种方便教学的理想模型。实践中，一般通过Nginx或者网关的访问日志(access log)，采集用户的视频点击观看日志，再发送到计数服务。这样就不耦合。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1593270221,"ip_address":"","comment_id":229903,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师，用户在观看视频时放入消息队列，这个是否应该有专门一个服务去接收用户观看触发的计数请求，然后放入消息队列。也就是在技术服务前，还有一个接收技术请求的服务","like_count":5,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499689,"discussion_content":"对，课程演示的是一种方便教学的理想模型。实践中，一般通过Nginx或者网关的访问日志(access log)，采集用户的视频点击观看日志，再发送到计数服务。这样就不耦合。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593270221,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227915,"user_name":"0xABC","can_delete":false,"product_type":"c3","uid":1019555,"ip_address":"","ucode":"E64CD8BED96D8D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8e/a3/d7e5fe8a.jpg","comment_is_top":false,"comment_ctime":1592526639,"is_pvip":false,"replies":[{"id":84198,"content":"对，对于至少一次交付(At Least Once)的消息队列，都需要考虑去重问题，如果涉及业务的话，还要考虑幂等，第三章也会讲到。一般在消息进入消息队列之前，可以给它加一个唯一的id，这样消费端既可以用缓存窗口(cache window)等技术做去重处理，也方便后续业务做幂等处理。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592667144,"ip_address":"","comment_id":227915,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"波波老师好，还有个问题再请教一下\n还是 聚合-&gt; 内部队列 -&gt; db写入 这个pipeline，整个消费过程中，当 聚合-&gt; 内部队列 成功后就提交 ack 给 MQ，但是这个提交的 ack 可能由于某种原因是否也会提交失败；这样就可能造成重试，所以说也有可能会出现重复消息，这样 聚合 出来的就会有重复的聚合结果，然后放入 内部队列 中，是否也要考虑 在 db 写入环境保证去重呢（是否也可以理解为 写入操作 最好也提供幂等的处理）？？","like_count":4,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498833,"discussion_content":"对，对于至少一次交付(At Least Once)的消息队列，都需要考虑去重问题，如果涉及业务的话，还要考虑幂等，第三章也会讲到。一般在消息进入消息队列之前，可以给它加一个唯一的id，这样消费端既可以用缓存窗口(cache window)等技术做去重处理，也方便后续业务做幂等处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592667144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1068571,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","nickname":"飞翔","note":"","ucode":"65AF6AF292DAD6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":284941,"discussion_content":"但是老师 消息都已经聚合 并且写入inner queue，A=3， 这个时候已经分不出单个消息的id了呀， 这样怎么去重？","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1592675795,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227806,"user_name":"Geek_zbvt62","can_delete":false,"product_type":"c3","uid":1046714,"ip_address":"","ucode":"81EA27ADD9EC1A","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f8/ba/d28174a9.jpg","comment_is_top":false,"comment_ctime":1592485007,"is_pvip":false,"replies":[{"id":83980,"content":"如果你的aggregator同时做聚合和写入DB的事情，不是不可以，但是会有一些问题，比方说：\n\n1. 一个是你的aggreator同时干了两个事情，违反单一职责原则，后续如果要调整写入数据库逻辑，aggregator就得跟着一起改。后面发布也要一起发布，不能单独发布DB writer。如果有中间queue的话，DB writer暂时下线都没关系，queue会缓冲计算结果。\n2. aggregator因为同时干两件事情，当写入数据库慢，可能阻Aggregator线程，甚至影响到聚合运算性能。\n\n内部queue是持久化的，在企业开发中，一般认为数据能够落地(落磁盘文件，或者DB)，那么数据就不会再丢，宕机了重启都能找回来。当然这个不是绝对，只是相对内存数据来说的，内存数据宕机就没有了。\n\n","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592499533,"ip_address":"","comment_id":227806,"utype":1}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"我也不太能理解internal queue的必要性。aggregator在db或网络问题时重试提交，感觉效果一样。internal queue如果本地持久化，也有丢数据的风险，如果依旧借用Kafka做冗余，那也面临写入时网络的抖动问题。\n","like_count":3,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498789,"discussion_content":"如果你的aggregator同时做聚合和写入DB的事情，不是不可以，但是会有一些问题，比方说：\n\n1. 一个是你的aggreator同时干了两个事情，违反单一职责原则，后续如果要调整写入数据库逻辑，aggregator就得跟着一起改。后面发布也要一起发布，不能单独发布DB writer。如果有中间queue的话，DB writer暂时下线都没关系，queue会缓冲计算结果。\n2. aggregator因为同时干两件事情，当写入数据库慢，可能阻Aggregator线程，甚至影响到聚合运算性能。\n\n内部queue是持久化的，在企业开发中，一般认为数据能够落地(落磁盘文件，或者DB)，那么数据就不会再丢，宕机了重启都能找回来。当然这个不是绝对，只是相对内存数据来说的，内存数据宕机就没有了。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592499533,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1729060,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","nickname":"托尼斯威特","note":"","ucode":"98A1035527292E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":290809,"discussion_content":"Aggregator写 ConcurrentHashMap, DBWriter 读ConcurrentHashMap, 可以是不同的线程, 没有违反单一责任原则. ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594610385,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1046714,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f8/ba/d28174a9.jpg","nickname":"Geek_zbvt62","note":"","ucode":"81EA27ADD9EC1A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":284613,"discussion_content":"谢谢波波的解答","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592569275,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1251111,"avatar":"https://static001.geekbang.org/account/avatar/00/13/17/27/ec30d30a.jpg","nickname":"Jxin","note":"","ucode":"4C03928388C413","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":284344,"discussion_content":"就计数这个功能，我和你一样的看法。\n\n谈谈我的理解：\n1.内部队列的价值不是防止丢消息，而是减少消息重试时，计数的计算资源开销。\n2.死信队列可以“跳过”异常数据，且不丢消息，方便排查。然后能保证正常数据到db的存储。\n\n不管是内部队列还是死信队列，在代码量，系统复杂度，磁盘资源都是有开销的。而如果只是计数功能，重试的计算成本会高于这些吗？如果低于这些，那这一系列的调优就挺尴尬。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592496872,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227225,"user_name":"intomymind","can_delete":false,"product_type":"c3","uid":1105126,"ip_address":"","ucode":"B954C9BFEC8667","user_header":"https://static001.geekbang.org/account/avatar/00/10/dc/e6/cfb409ab.jpg","comment_is_top":false,"comment_ctime":1592317848,"is_pvip":false,"replies":[{"id":83708,"content":"有必要，尽管是pull模式，但是DB慢或者挂，或者之间的网络出现抖动&#47;瞬断，这些问题在实际生产环境中都无法避免。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592324228,"ip_address":"","comment_id":227225,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"在消费者端也就是Consumer MQ Client采用pull模式的话就已经减轻的后端数据的压力，Internal Queue 还有存在的必要吗，而且引入一个新的队列也增加了一个丢数据的风险点","like_count":3,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498541,"discussion_content":"有必要，尽管是pull模式，但是DB慢或者挂，或者之间的网络出现抖动/瞬断，这些问题在实际生产环境中都无法避免。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592324228,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194545,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3a/31/6a127efe.jpg","nickname":"漂泊的松鼠","note":"","ucode":"281CF2FAA6F904","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":293164,"discussion_content":"还有1个坑，dbwriter要做成同步写，否则加了internalQ也不好使，之前为了追求性能，dbwriter用的是hbaseclient的异步模式，然后流量稍微高一点，hbase的regionserver 进程就开始蹦蹦跳跳了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595467297,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194545,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3a/31/6a127efe.jpg","nickname":"漂泊的松鼠","note":"","ucode":"281CF2FAA6F904","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":293162,"discussion_content":"没有internal queue 削峰，deadletter queue经常被接消息，相当于dead在做削峰，这个场景不符合ta主流用途。前面的mqclient拉取的数据是很大的，因为mqclient会有很多个且是异步，扔给聚合服务就提交偏移量，而不是等写入db后才拉取下一批，所以某分钟的聚合维度空间会很大，这时db就吃不消了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595467090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1729060,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","nickname":"托尼斯威特","note":"","ucode":"98A1035527292E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":290847,"discussion_content":"懂了, 每次写DB不是写一行, 因为每个consumer要同时聚合多个videoId的count, 一次性写多行会比较慢, 可以用一个本地队列, 本地消费者DBWriter慢慢写DB, 平滑写入速率. ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594622392,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1729060,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","nickname":"托尼斯威特","note":"","ucode":"98A1035527292E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":290804,"discussion_content":"波波老师, 我也有同样的疑问. \n假设后端的DB已经是按照videoId 做过sharding的了, 每次写DB都是插入一行 (videoId, minuteTimestamp, count, eventType), 所以也不用加锁, 性能应该可以保证, (如果DB还慢, 就得增加shard). 考虑偶尔DB短时间不可用, 还有deadletter queue 可以接住数据, 以后再重试. \n\n所以这个internal queue 为什么是必要的呢? ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594608996,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2113464,"avatar":"","nickname":"Geek_47e6bf","note":"","ucode":"B101445163A7D5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1729060,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","nickname":"托尼斯威特","note":"","ucode":"98A1035527292E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":303869,"discussion_content":"这里发表下个人的观点\n1.这里我假设他是使用的处理完聚合再向kafka提交偏移量(因为读时提交偏移量会存在数据丢失问题),在这种情况下,如果直接由聚合模块向mysql存数据,而mysql恰好挂了,要么直接被接到了deadqueue(在这里作者应该是把deadline当作兜底机制,而不是正常流程的一部分,个人看来其实deadqueue其实可以抛弃,把internal queue偏移量提交做在插入数据库成功之后就可以了),要么就放弃这次结果重新跑任务,造成无意义的多次聚合数据,浪费资源\n2.链路问题,借用queue,起到数据暂存的作用,就像游戏中的复活点,不至于一旦出现问题,需要重跑整个流程\n3.sharding是可以保证,但是前提是你要预估准确,如果出现偶发大流量打过来,例如热搜的视频,你是无法即使扩容的(或者说videoID一致,他们落在了同一个库/表上面,当然这里可以做一致性哈希再聚合数据的方式解决),基于这层考虑,用到了internalqueue","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1599398382,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":290804,"ip_address":"","group_id":0},"score":303869,"extra":""}]}]},{"had_liked":false,"id":241301,"user_name":"Vincent_","can_delete":false,"product_type":"c3","uid":1313831,"ip_address":"","ucode":"0904F7CE04E788","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/ibbEWWUTH7964UOnwpBPC8Lhb1TS4s7XMEXTPKHPUBlj58GVkdRQRqa6EydIRL2I1uJDzeichLj86gJfTpzcgcCA/132","comment_is_top":false,"comment_ctime":1597241519,"is_pvip":false,"replies":[{"id":89251,"content":"redis的最常见使用场景是作为缓存(Cache)来用，比如把redis作为DB的缓存。nosql大致可以分为四种，KV缓存(包括redis)是一种，文档存储(如MongoDB)是一种，列式数据库(Hbase&#47;Cassandra)是一种，还有图数据库(如Neo4j)也是一种。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1597418336,"ip_address":"","comment_id":241301,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师好，结合上一篇缓存设计，好像redis这个nosql在nosql领域大多数情况都只当做一个不持久化数据也就是有过期时间的缓存，而不是当一个高可用的数据层。像上一章说的nosql可以用redis做吗？因为redis是内存级的操作，所以我们才不在需要持久化的情形里考虑使用他来代替其他磁盘级的db吗？","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":503707,"discussion_content":"redis的最常见使用场景是作为缓存(Cache)来用，比如把redis作为DB的缓存。nosql大致可以分为四种，KV缓存(包括redis)是一种，文档存储(如MongoDB)是一种，列式数据库(Hbase/Cassandra)是一种，还有图数据库(如Neo4j)也是一种。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597418336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":234619,"user_name":"明月朔风","can_delete":false,"product_type":"c3","uid":1638570,"ip_address":"","ucode":"C9CAF7B14C38FA","user_header":"https://static001.geekbang.org/account/avatar/00/19/00/aa/1f5fceda.jpg","comment_is_top":false,"comment_ctime":1594735393,"is_pvip":false,"replies":[{"id":86547,"content":"k8s容器环境是麻烦一点，一种办法是考虑Persistent Volume持久卷，持久卷可以挂载磁盘或者远程存储。\n\n另外一种办法，索性把计算结果再发送到kafka队列，再让DB Writer去kafka消费数据再写入DB。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1594743794,"ip_address":"","comment_id":234619,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"波波老师，Internal Queue，有什么好方法能保证不丢失吗？想过写本地文件，但是k8，一旦重启，不一定漂移到哪里去了","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501441,"discussion_content":"k8s容器环境是麻烦一点，一种办法是考虑Persistent Volume持久卷，持久卷可以挂载磁盘或者远程存储。\n\n另外一种办法，索性把计算结果再发送到kafka队列，再让DB Writer去kafka消费数据再写入DB。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594743794,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228238,"user_name":"古德","can_delete":false,"product_type":"c3","uid":1037755,"ip_address":"","ucode":"E3F646BB73F60E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d5/bb/98b93862.jpg","comment_is_top":false,"comment_ctime":1592622097,"is_pvip":false,"replies":[{"id":84215,"content":"1. 消费端去重的一种简单做法，是每个消息进入队列时，header里头给一个唯一id，然后消费端利用缓存窗口技术进行去重，如果单位时间内某个id已经有缓存过，就去重丢弃。\n2. 我之前就开源过做一个叫bigqueue(https:&#47;&#47;github.com&#47;bulldog2011&#47;bigqueue)的磁盘持久化queue，基于内存映射技术的，性能很高。你如果有研发能力，自己开发一个简化的磁盘queue，工作量其实也不是很大。嵌入式db可以考虑sqlite或者berkelyDB，都可以实现简单queue的功能。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592668641,"ip_address":"","comment_id":228238,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"波波老师好,有几个疑问交流一下\n1.在这个架构设计中，Kafka消息是有可能重复发送，在接收端做幂等性判断时，具体怎么做比较好（该场景的业务数据感觉没法做）\n2.如果DBWriter不可用，Internal Queue缓冲到磁盘或者本地数据库时，能不能对比较主流的实现方式介绍一下（具体用什么磁盘缓冲库，或者嵌入数据库等）","like_count":2,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498968,"discussion_content":"1. 消费端去重的一种简单做法，是每个消息进入队列时，header里头给一个唯一id，然后消费端利用缓存窗口技术进行去重，如果单位时间内某个id已经有缓存过，就去重丢弃。\n2. 我之前就开源过做一个叫bigqueue(https://github.com/bulldog2011/bigqueue)的磁盘持久化queue，基于内存映射技术的，性能很高。你如果有研发能力，自己开发一个简化的磁盘queue，工作量其实也不是很大。嵌入式db可以考虑sqlite或者berkelyDB，都可以实现简单queue的功能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592668641,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1729060,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","nickname":"托尼斯威特","note":"","ucode":"98A1035527292E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":290807,"discussion_content":"老师你太强了! bigqueue 可以开一门课了!","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1594609826,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":237430,"user_name":"发条橙子 。","can_delete":false,"product_type":"c3","uid":1259218,"ip_address":"","ucode":"ED076F4534FFED","user_header":"https://static001.geekbang.org/account/avatar/00/13/36/d2/c7357723.jpg","comment_is_top":false,"comment_ctime":1595828621,"is_pvip":false,"replies":[{"id":89082,"content":"1. 一般queue的两头都需要一个线程，比方说这里的计数消费者里头有两个queue，所以第一个queue的前面有一个consumer thread，第一和第二个queue之间有一个aggregator聚合线程，第二个queue后面有一个DB writer线程。\n\n2. 同步到DB就是一个批量插入(batch insert)，其中每一条记录信息主要包括时间 + 视频id + 一分钟观看计数这些字段。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1597159875,"ip_address":"","comment_id":237430,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"波波老师 小白问个问题 ：\n技术消费者的详细设计 ， 从mq消费端开始到dbwriter是一个应用服务的在一个线程内处理好的么 ？\n1. 按照时序图来说， 是在一个领域服务内（service层）完成聚合的逻辑然后写到自己本地的队列中（又是一个service)最后调用dbwriter service将结果更新到 db这个样子么 \n\n2. 如果消费端是集群服务 ，多个消费端同时做本地内存计算， 最后同步到db是采用增加的方式么\n","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":502414,"discussion_content":"1. 一般queue的两头都需要一个线程，比方说这里的计数消费者里头有两个queue，所以第一个queue的前面有一个consumer thread，第一和第二个queue之间有一个aggregator聚合线程，第二个queue后面有一个DB writer线程。\n\n2. 同步到DB就是一个批量插入(batch insert)，其中每一条记录信息主要包括时间 + 视频id + 一分钟观看计数这些字段。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597159875,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228442,"user_name":"飞翔","can_delete":false,"product_type":"c3","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1592678553,"is_pvip":false,"replies":[{"id":84553,"content":"没有规定失败几次写入死信队列，一般可以设置成3次，最好做成动态可配置，可以按需动态调整。\n\n立即重试可以，高级一点的可以按指数级退避再重试(退1秒重试，不行再退2秒，不行再退4秒。。。)","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592929957,"ip_address":"","comment_id":228442,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师 DBwrite 一般失败几次 就把消息 扔到死信队列中呀。 还有DBwrite 写入database 失败之后 是立刻重试呢 还是需要过一段时间？","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499038,"discussion_content":"没有规定失败几次写入死信队列，一般可以设置成3次，最好做成动态可配置，可以按需动态调整。\n\n立即重试可以，高级一点的可以按指数级退避再重试(退1秒重试，不行再退2秒，不行再退4秒。。。)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592929957,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228441,"user_name":"飞翔","can_delete":false,"product_type":"c3","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1592677764,"is_pvip":false,"replies":[{"id":84551,"content":"1. deadletter queue可以是本地的嵌入式DB，例如sqlite，或者berkeley DB，也可以是本地持久化queue，例如我开源的github.com&#47;bulldog2011&#47;bigqueue\n\n2. 只要一个后台线程，定期监控deadletter queue，有消息的话定期重试写入DB即可。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592929684,"ip_address":"","comment_id":228441,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100053601,"comment_content":"老师 deadletter queue 除了用kafka partition 外 还能用什么呀， 还有当数据写入到了deadletter queue 之后 这里边的数据怎么处理 啥时候 写入db呀","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499037,"discussion_content":"1. deadletter queue可以是本地的嵌入式DB，例如sqlite，或者berkeley DB，也可以是本地持久化queue，例如我开源的github.com/bulldog2011/bigqueue\n\n2. 只要一个后台线程，定期监控deadletter queue，有消息的话定期重试写入DB即可。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592929684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194545,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3a/31/6a127efe.jpg","nickname":"漂泊的松鼠","note":"","ucode":"281CF2FAA6F904","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":293182,"discussion_content":"死信队列中的消息怎么查看、管理、监控呢？我觉得发到kafka独立的topic比本地队列较省事儿一些","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1595470534,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228440,"user_name":"飞翔","can_delete":false,"product_type":"c3","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1592676295,"is_pvip":false,"replies":[{"id":84549,"content":"可以考虑用一个集中式redis缓存进行去重，设定过期时间，比方说1分钟(只要大于检查点提交周期时间就可以)。\n\n每次aggregator从分区消费者取出消息，就去看redis中有没有对应id，没有就将id缓存起来，有的话说明是重复，丢弃。\n","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592929523,"ip_address":"","comment_id":228440,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"老师 您说 kafka 检查点提交 为了提升性能 是异步提交 会产生重复消费，能详细说说这个怎么处理嘛 如果给每个消息header 头里唯一的id， 但是在消费端 消息都已经聚合 并且写入inner queue，A=3， 这个时候已经分不出单个消息的id了呀， 这样怎么去重？","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499036,"discussion_content":"可以考虑用一个集中式redis缓存进行去重，设定过期时间，比方说1分钟(只要大于检查点提交周期时间就可以)。\n\n每次aggregator从分区消费者取出消息，就去看redis中有没有对应id，没有就将id缓存起来，有的话说明是重复，丢弃。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592929523,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":228438,"user_name":"飞翔","can_delete":false,"product_type":"c3","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1592676116,"is_pvip":false,"replies":[{"id":84547,"content":"可以new一个新的ConcurrentHashMap，去替换掉那个老的，这样就不需要清理了。老的统计完就可以丢弃了。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592929103,"ip_address":"","comment_id":228438,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"老师 比如aggregator 是一个concurrenthashmap， 当一个线程把他写入innerqueue的时候， 写完了 要把这个ocncurernthashmap清空 对吧，但是同时计数请求 还是不断的在写入concurrenthashmap， 这不导致混乱了嘛","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499034,"discussion_content":"可以new一个新的ConcurrentHashMap，去替换掉那个老的，这样就不需要清理了。老的统计完就可以丢弃了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592929103,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227009,"user_name":"0xABC","can_delete":false,"product_type":"c3","uid":1019555,"ip_address":"","ucode":"E64CD8BED96D8D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8e/a3/d7e5fe8a.jpg","comment_is_top":false,"comment_ctime":1592267458,"is_pvip":false,"replies":[{"id":83706,"content":"不需要那样。\n\n对于一分钟聚合，如果用kafka的话，只要偏移提交超过1分钟即可(auto.commit.interval.ms, 也可以在写入内部队列后手动提交)，这样，如果消费者挂了重启，1分钟内的数据还可以从Kafka读出来，重新计算就好了。\n\n另外内部队列是持久化的，只要计算结果写入了内部队列，加上死信队列，最终结果一定会写入DB。\n","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1592324033,"ip_address":"","comment_id":227009,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"波波老师好，有个疑问是这种 pipeline 的设计，是 聚合-&gt; 内部队列 -&gt; db写入，整个过程都成功之后再给 MQ 发送确认 ACK 吗？不然的话，这个pipeline有可能在处理到任何环节出现宕机等情况，消息就有可能丢失","like_count":1,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498457,"discussion_content":"不需要那样。\n\n对于一分钟聚合，如果用kafka的话，只要偏移提交超过1分钟即可(auto.commit.interval.ms, 也可以在写入内部队列后手动提交)，这样，如果消费者挂了重启，1分钟内的数据还可以从Kafka读出来，重新计算就好了。\n\n另外内部队列是持久化的，只要计算结果写入了内部队列，加上死信队列，最终结果一定会写入DB。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592324033,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":298122,"user_name":"独孤九剑","can_delete":false,"product_type":"c3","uid":2230909,"ip_address":"","ucode":"6C1253E2B8C1D4","user_header":"https://static001.geekbang.org/account/avatar/00/22/0a/7d/ac715471.jpg","comment_is_top":false,"comment_ctime":1623918183,"is_pvip":false,"replies":[{"id":108678,"content":"学好数据结构是用好中间件的基础，很多中间件的底层都基于某种数据结构，MQ本质可以抽象为数组或链表。","user_name":"作者回复","user_name_real":"杨波","uid":1030344,"ctime":1624810305,"ip_address":"","comment_id":298122,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"MQ本质上也是个“队列”，可基于“数组”或“链表”实现，这样就与数据结构打通了，幸亏前几天学习了波波老师的数据结构课程。","like_count":0,"discussions":[{"author":{"id":1030344,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b8/c8/c94d38a7.jpg","nickname":"杨波","note":"","ucode":"FA3418BB703BCA","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522022,"discussion_content":"学好数据结构是用好中间件的基础，很多中间件的底层都基于某种数据结构，MQ本质可以抽象为数组或链表。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624810305,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":351097,"user_name":"青阳","can_delete":false,"product_type":"c3","uid":2139807,"ip_address":"","ucode":"3591D366BAB0B9","user_header":"https://static001.geekbang.org/account/avatar/00/20/a6/9f/3c60fffd.jpg","comment_is_top":false,"comment_ctime":1657526292,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"计数服务实例有多个的话，会存在丢失更新的问题吗，比如计数服务A读取当前计数为1，计数服务B读取当前计数也为1，然后都加1，再存入数据库，最后数据库变为2，而不是3。","like_count":0},{"had_liked":false,"id":332695,"user_name":"Geek_ea6e2f","can_delete":false,"product_type":"c3","uid":1905501,"ip_address":"","ucode":"D7670E71216791","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eopYFxjU2gAiasicnPt1o0O5Ro2sPAsR8M2szjULDAE0NY6ribWB0kEyp7aKlnhevfhib1icKv6wpw2o5A/132","comment_is_top":false,"comment_ctime":1643497114,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"波波老师您好，想问一下这里kafka的partition是根据video id的话，那么是否意味着每个video都有一个自己的queue和consumer呢？这样的话会不会partition太多了？还是说partition之后是一个区间，类似于A-D有一个queue，E-F有一个queue？","like_count":0},{"had_liked":false,"id":330902,"user_name":"张旭","can_delete":false,"product_type":"c3","uid":1256155,"ip_address":"","ucode":"8029AFFC0AF9D4","user_header":"https://static001.geekbang.org/account/avatar/00/13/2a/db/86437192.jpg","comment_is_top":false,"comment_ctime":1642261352,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100053601,"comment_content":"波波老师，如果Aggregator计算结果放到的internal queue用的是kafka，我DB writer在写入数据库后再ack，那么最后的死信队列应该也不需要了吧？","like_count":0}]}