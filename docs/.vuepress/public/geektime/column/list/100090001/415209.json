{"id":415209,"title":"01｜Spark：从“大数据的Hello World”开始","content":"<p>你好，我是吴磊。</p><p>从这节课开始，我们先来学习Spark的“基础知识”模块，对Spark的概念和核心原理先做一个整体的了解。我并不会从RDD、DAG这些基本概念给你讲起。坦白地说，这些抽象的概念枯燥而又乏味，对于刚开始接触Spark的你来说，很难学进去。因此，我们不妨反其道而行之，先从实战入手，用一个小例子来直观地认识Spark，看看Spark都能做些什么。</p><p>这就好比我们学习一门新的编程语言，往往都是从“Hello World”开始。我还记得，刚刚学编程那会，屏幕上打印出的“Hello World”，足足让我兴奋了一整天，让我莫名地有一种“I can change the world”的冲动。</p><p>今天这一讲，我们就从“大数据的Hello World”开始，去学习怎么在Spark之上做应用开发。不过，“大数据的Hello World”并不是把字符串打印到屏幕上这么简单，而是要先对文件中的单词做统计计数，然后再打印出频次最高的5个单词，江湖人称“Word Count”。</p><p>之所以会选择Word Count，作为我们迈入Spark门槛的第一个项目，主要有两个原因，一是Word Count场景比较简单、容易理解；二是Word Count麻雀虽小，但五脏俱全，一个小小的Word Count，就能够牵引出Spark许多的核心原理，帮助我们快速入门。</p><!-- [[[read_end]]] --><p>好啦，话不多说，下面我们正式开启Word Count之旅。</p><h2><strong>准备工作</strong></h2><p>巧妇难为无米之炊，要做Word Count，我们得先把源文件准备好。</p><p>咱们做Word Count的初衷是学习Spark，因此源文件的内容无足轻重。这里我提取了<a href=\"https://en.wikipedia.org/wiki/Apache_Spark\">Wikipedia中对Spark的介绍</a>来做我们的源文件。我把它保存到了与课程配套的GitHub项目中，并把它命名为“wikiOfSpark.txt”。你可以从<a href=\"https://github.com/wulei-bj-cn/learn-spark/blob/main/chapter01/wikiOfSpark.txt\">这里</a>下载它。</p><p>为了跑通Word Count实例，我们还需要在本地（Local）部署Spark运行环境。这里的“本地”，指的是你手头能够获取到的任何计算资源，比如服务器、台式机，或是笔记本电脑。</p><p>在本地部署Spark运行环境非常简单，即便你从来没有和Spark打过交道，也不必担心。<strong>只需要下面这3个步骤，我们就可以完成Spark的本地部署了</strong>。</p><ol>\n<li><strong>下载安装包：</strong>从<a href=\"http://spark.apache.org/downloads.html\">Spark官网</a>下载安装包，选择最新的预编译版本即可；</li>\n<li><strong>解压：</strong>解压Spark安装包到任意本地目录；</li>\n<li><strong>配置：</strong>将“${解压目录}/bin”配置到PATH环境变量。</li>\n</ol><p>我这里给你准备了一个本地部署的小视频，你可以直观地感受一下。</p><p><video poster=\"https://media001.geekbang.org/d5c5a960d1d84fcd8d02c3b38515d530/snapshots/f09e3d96d7ef47e98c2e7155198480f2-00004.jpg\" preload=\"none\" controls=\"\"><source src=\"https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/341986fd-17bef16db30-0000-0000-01d-dbacd.mp4\" type=\"video/mp4\"><source src=\" https://media001.geekbang.org/8eaaf98af7614dc1b5297ebca1e2c746/56a5b58801ea45419cf2760b23b4dc49-b111872a4e8294fade9a938a484f8b67-sd.m3u8\" type=\"application/x-mpegURL\"></video></p><p>接下来，我们确认一下Spark是否部署成功。打开命令行终端，敲入“spark-shell --version”命令，如果该命令能成功地打印出Spark版本号，就表示我们大功告成了，就像这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/51/34/51664d2f8479aac9099a1b2736ebee34.jpg?wh=1112x726\" alt=\"\" title=\"验证Spark本地部署成功\"></p><p>在后续的实战中，我们会用spark-shell来演示Word Count的执行过程。spark-shell是提交Spark作业众多方式中的一种，我们在后续的课程中还会展开介绍，这里你不妨暂时把它当做是Spark中的Linux shell。spark-shell提供交互式的运行环境（REPL，Read-Evaluate-Print-Loop），以“所见即所得”的方式，让开发者在提交源代码之后，就可以迅速地获取执行结果。</p><p>不过，需要注意的是，spark-shell在运行的时候，依赖于Java和Scala语言环境。因此，为了保证spark-shell的成功启动，你需要在本地预装Java与Scala。好消息是，关于Java与Scala的安装，网上的资料非常丰富，你可以参考那些资料来进行安装，咱们在本讲就不再赘述Java与Scala的安装步骤啦。</p><h2><strong>梳理</strong>Word Count的计算步骤</h2><p>做了一番准备之后，接下来，我们就可以开始写代码了。不过，在“下手”之前，咱们不妨一起梳理下Word Count的计算步骤，先做到心中有数，然后再垒代码也不迟。</p><p>之前我们提到，Word Count的初衷是对文件中的单词做统计计数，打印出频次最高的5个词汇。那么Word Count的第一步就很明显了，当然是得读取文件的内容，不然咱们统计什么呢？</p><p>我们准备好的文件是wikiOfSpark.txt，它以纯文本的方式记录了关于Spark的简单介绍，我摘取了其中的部分内容给你看一下：</p><p><img src=\"https://static001.geekbang.org/resource/image/8f/d3/8fa70d857b684ef85cd5fa92611651d3.png?wh=1292x394\" alt=\"\" title=\"wikiOfSpark.txt内容摘要\"></p><p>我们知道，文件的读取往往是以行（Line）为单位的。不难发现，wikiOfSpark.txt的每一行都包含多个单词。</p><p>我们要是以“单词”作为粒度做计数，就需要对每一行的文本做分词。分词过后，文件中的每一句话，都被打散成了一个个单词。这样一来，我们就可以按照单词做分组计数了。这就是Word Count的计算过程，主要包含如下3个步骤：</p><ol>\n<li><strong>读取内容</strong>：调用Spark文件读取API，加载wikiOfSpark.txt文件内容；</li>\n<li><strong>分词</strong>：以行为单位，把句子打散为单词；</li>\n<li><strong>分组计数</strong>：按照单词做分组计数。</li>\n</ol><p>明确了计算步骤后，接下来我们就可以调用Spark开发API，对这些步骤进行代码实现，从而完成Word Count的应用开发。</p><p>众所周知，Spark支持种类丰富的开发语言，如Scala、Java、Python，等等。你可以结合个人偏好和开发习惯，任意选择其中的一种进行开发。尽管不同语言的开发API在语法上有着细微的差异，但不论是功能方面、还是性能方面，Spark对于每一种语言的支持都是一致的。换句话说，同样是Word Count，你用Scala实现也行，用Python实现也可以，两份代码的执行结果是一致的。不仅如此，在同样的计算资源下，两份代码的执行效率也是一样的。</p><p>因此，就Word Count这个示例来说，开发语言不是重点，我们不妨选择Scala。你可能会说：“我本来对Spark就不熟，更没有接触过Scala，一上来就用Scala演示Spark应用代码，理解起来会不会很困难？”</p><p>其实大可不必担心，Scala语法比较简洁，Word Count的Scala实现不超过10行代码。再者，对于Word Count中的每一行Scala代码，我会带着你手把手、逐行地进行讲解和分析。我相信，跟着我过完一遍代码之后，你能很快地把它“翻译”成你熟悉的语言，比如Java或Python。另外，绝大多数的Spark 源码都是由 Scala 实现的，接触并了解一些Scala的基本语法，有利于你后续阅读、学习Spark源代码。</p><h2>Word Count代码实现</h2><p>选定了语言，接下来，我们就按照读取内容、分词、分组计数这三步来看看Word Count具体怎么实现。</p><h3>第一步，读取内容</h3><p>首先，我们调用SparkContext的textFile方法，读取源文件，也就是wikiOfSpark.txt，代码如下表所示：</p><pre><code class=\"language-scala\">import org.apache.spark.rdd.RDD\n \n// 这里的下划线\"_\"是占位符，代表数据文件的根目录\nval rootPath: String = _\nval file: String = s\"${rootPath}/wikiOfSpark.txt\"\n \n// 读取文件内容\nval lineRDD: RDD[String] = spark.sparkContext.textFile(file) \n</code></pre><p>在这段代码中，你可能会发现3个新概念，分别是spark、sparkContext和RDD。</p><p>其中，spark和sparkContext分别是两种不同的开发入口实例：</p><ul>\n<li>spark是开发入口SparkSession实例（Instance），SparkSession在spark-shell中会由系统自动创建；</li>\n<li>sparkContext是开发入口SparkContext实例。</li>\n</ul><p>在Spark版本演进的过程中，从2.0版本开始，SparkSession取代了SparkContext，成为统一的开发入口。换句话说，要开发Spark应用，你必须先创建SparkSession。关于SparkSession和SparkContext，我会在后续的课程做更详细的介绍，这里你只要记住它们是必需的开发入口就可以了。</p><p>我们再来看看RDD，RDD的全称是Resilient Distributed Dataset，意思是“弹性分布式数据集”。RDD是Spark对于分布式数据的统一抽象，它定义了一系列分布式数据的基本属性与处理方法。关于RDD的定义、内涵与作用，我们留到<a href=\"https://time.geekbang.org/column/article/417164\">下一讲</a>再去展开。</p><p>在这里，你不妨先简单地把RDD理解成“数组”，比如代码中的lineRDD变量，它的类型是RDD[String]，你可以暂时把它当成元素类型是String的数组，数组的每个元素都是文件中的一行字符串。</p><p>获取到文件内容之后，下一步我们就要做分词了。</p><h3>第二步，分词</h3><p>“分词”就是把“数组”的行元素打散为单词。要实现这一点，我们可以调用RDD的flatMap方法来完成。flatMap操作在逻辑上可以分成两个步骤：<strong>映射</strong>和<strong>展平</strong>。</p><p>这两个步骤是什么意思呢？我们还是结合Word Count的例子来看：</p><pre><code class=\"language-scala\">// 以行为单位做分词\nval wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(\" \"))&nbsp;\n</code></pre><p>要把lineRDD的行元素转换为单词，我们得先用分隔符对每个行元素进行分割（Split），咱们这里的分隔符是空格。</p><p>分割之后，每个行元素就都变成了单词数组，元素类型也从String变成了Array[String]，像这样以元素为单位进行转换的操作，统一称作“<strong>映射</strong>”。</p><p>映射过后，RDD类型由原来的RDD[String]变为RDD[Array[String]]。如果把RDD[String]看成是“数组”的话，那么RDD[Array[String]]就是一个“二维数组”，它的每一个元素都是单词。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/2a/1e364df2yy57857efafc1023c102942a.jpg?wh=2284x758\" alt=\"\" title=\"以行为单位做分词\"></p><p>为了后续对单词做分组，我们还需要对这个“二维数组”做<strong>展平</strong>，也就是去掉内层的嵌套结构，把“二维数组”还原成“一维数组”，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/b5/34/b51c79dfeeb393456f2392011688c934.jpg?wh=2284x643\" alt=\"\" title=\"分词后做展平\"></p><p>就这样，在flatMap算子的作用下，原来以行为元素的lineRDD，转换成了以单词为元素的wordRDD。</p><p>不过，值得注意的是，我们用“空格”去分割句子，有可能会产生空字符串。所以，在完成“映射”和“展平”之后，对于这样的“单词”，我们要把其中的空字符串都过滤掉，这里我们调用RDD的filter方法来过滤：</p><pre><code class=\"language-scala\">// 过滤掉空字符串\nval cleanWordRDD: RDD[String] = wordRDD.filter(word =&gt; !word.equals(\"\"))\n</code></pre><p>这样一来，我们在分词阶段就得到了过滤掉空字符串之后的单词“数组”，类型是RDD[String]。接下来，我们就可以准备做分组计数了。</p><h3>第三步，分组计数</h3><p>在RDD的开发框架下，聚合类操作，如计数、求和、求均值，需要依赖<strong>键值对</strong>（Key Value Pair）类型的数据元素，也就是（Key，Value）形式的“数组”元素。</p><p>因此，在调用聚合算子做分组计数之前，我们要先把RDD元素转换为（Key，Value）的形式，也就是把RDD[String]映射成RDD[(String, Int)]。</p><p>其中，我们统一把所有的Value置为1。这样一来，对于同一个的单词，在后续的计数运算中，我们只要对Value做累加即可，就像这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/9c/ac/9c4c96fe9a9f48f0cb7e2e1f3374f5ac.jpg?wh=2284x866\" alt=\"\" title=\"把元素转换为（Key，Value）形式\"></p><p>下面是对应的代码：</p><pre><code class=\"language-scala\">// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))&nbsp;\n</code></pre><p>这样一来，RDD就由原来存储String元素的cleanWordRDD，转换为了存储（String，Int）的kvRDD。</p><p>完成了形式的转换之后，我们就该正式做分组计数了。分组计数其实是两个步骤，也就是先“分组”，再“计数”。下面，我们使用聚合算子reduceByKey来同时完成分组和计数这两个操作。</p><p>对于kvRDD这个键值对“数组”，reduceByKey先是按照Key（也就是单词）来做分组，分组之后，每个单词都有一个与之对应的Value列表。然后根据用户提供的聚合函数，对同一个Key的所有Value做reduce运算。</p><p>这里的reduce，你可以理解成是一种计算步骤或是一种计算方法。当我们给定聚合函数后，它会用折叠的方式，把包含多个元素的列表转换为单个元素值，从而统计出不同元素的数量。</p><p>在Word Count的示例中，我们调用reduceByKey实现分组计算的代码如下：</p><pre><code class=\"language-scala\">// 按照单词做分组计数\nval wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)&nbsp;\n</code></pre><p>可以看到，我们传递给reduceByKey算子的聚合函数是(x, y) =&gt; x + y，也就是累加函数。因此，在每个单词分组之后，reduce会使用累加函数，依次折叠计算Value列表中的所有元素，最终把元素列表转换为单词的频次。对于任意一个单词来说，reduce的计算过程都是一样的，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/c4/c1b5022a6c8d62226ba53af3dd90b2c4.jpg?wh=2284x747\" alt=\"\" title=\"reduce操作示意图\"></p><p>reduceByKey完成计算之后，我们得到的依然是类型为RDD[(String, Int)]的RDD。不过，与kvRDD不同，wordCounts元素的Value值，记录的是每个单词的统计词频。到此为止，我们就完成了Word Count主逻辑的开发与实现。</p><p><img src=\"https://static001.geekbang.org/resource/image/79/33/7948a3d9c923791e139397988fcc6433.jpg?wh=2284x791\" alt=\"\" title=\"reduceByKey转换示意图\"></p><p>在程序的最后，我们还要把wordCounts按照词频做排序，并把词频最高的5个单词打印到屏幕上，代码如下所示。</p><pre><code class=\"language-scala\">// 打印词频最高的5个词汇\nwordCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(false).take(5)\n</code></pre><h3><strong>代码执行</strong></h3><p>应用开发完成之后，我们就可以把代码丢进已经准备好的本地Spark部署环境里啦。首先，我们打开命令行终端（Terminal），敲入“spark-shell”，打开交互式运行环境，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/65/4d/65257daf7030ca80c3da9c615edb654d.jpg?wh=1790x978\" alt=\"\" title=\"spark-shell启动界面\"></p><p>然后，把我们开发好的代码，依次敲入spark-shell。为了方便你操作，我把完整的代码实现整理到下面了：</p><pre><code class=\"language-scala\">import org.apache.spark.rdd.RDD\n \n// 这里的下划线\"_\"是占位符，代表数据文件的根目录\nval rootPath: String = _\nval file: String = s\"${rootPath}/wikiOfSpark.txt\"\n \n// 读取文件内容\nval lineRDD: RDD[String] = spark.sparkContext.textFile(file)\n \n// 以行为单位做分词\nval wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(\" \"))\nval cleanWordRDD: RDD[String] = wordRDD.filter(word =&gt; !word.equals(\"\"))\n \n// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))\n// 按照单词做分组计数\nval wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)\n \n// 打印词频最高的5个词汇\nwordCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(false).take(5)\n</code></pre><p>我们把上面的代码依次敲入到spark-shell之后，spark-shell最终会把词频最高的5个单词打印到屏幕上：</p><p><img src=\"https://static001.geekbang.org/resource/image/6a/f4/6acf199e126b205c92c2ec03ffba33f4.png?wh=1524x84\" alt=\"\" title=\"打印词频最高的5个词汇\"></p><p>在Wikipedia的Spark介绍文本中，词频最高的单词分别是the、Spark、a、and和of，除了“Spark”之外，其他4个单词都是常用的停用词（Stop Word），因此它们几个高居榜首也就不足为怪了。</p><p>好啦，到此为止，我们在Spark之上，完成了“大数据领域Hello World”的开发与实现，恭喜你跨入大数据开发的大门！</p><h2>重点回顾</h2><p>今天这一讲，我们围绕着Word Count，初步探索并体验了Spark应用开发。你首先需要掌握的是Spark的本地部署，从而可以通过spark-shell来迅速熟悉Spark，获得对Spark的“第一印象”。要在本地部署Spark，你需要遵循3个步骤：</p><ul>\n<li>从<a href=\"http://spark.apache.org/downloads.html\">Spark官网</a>下载安装包，选择最新的预编译版本即可；</li>\n<li>解压Spark安装包到任意本地目录；</li>\n<li>将“${解压目录}/bin”配置到PATH环境变量。</li>\n</ul><p>然后，我们一起分析并实现了入门Spark的第一个应用程序：Word Count。在我们的例子中，Word Count要完成的计算任务，是先对文件中的单词做统计计数，然后再打印出频次最高的5个单词。它的实现过程分为3个步骤：</p><ul>\n<li>读取内容：调用Spark文件读取API，加载wikiOfSpark.txt文件内容；</li>\n<li>分词：以行为单位，把句子打散为单词；</li>\n<li>分组计数：按照单词做分组计数。</li>\n</ul><p>也许你对RDD API还不熟悉，甚至从未接触过Scala，不过没关系，完成了这次“大数据的Hello World”开发之旅，你就已经踏上了新的征程。在接下来的课程里，让我们携手并肩，像探索新大陆一样，一层一层地剥开Spark的神秘面纱，加油！</p><h2>每课一练</h2><p>在Word Count的代码实现中，我们用到了多种多样的RDD算子，如map、filter、flatMap和reduceByKey，除了这些算子以外，你知道还有哪些常用的RDD算子吗？（提示，可以结合<a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\">官网</a>去查找）。</p><p>另外，你能说说，以上这些算子都有哪些共性或是共同点吗？</p><p>欢迎你把答案分享到评论区，我在评论区等你。</p><p>如果这一讲对你有帮助，也欢迎你分享给自己的朋友，我们下一讲再见！</p>","neighbors":{"left":{"article_title":"开篇词 | 入门Spark，你需要学会“三步走”","id":415208},"right":{"article_title":"02 | RDD与编程模型：延迟计算是怎么回事？","id":417164}},"comments":[{"had_liked":false,"id":312944,"user_name":"Alvin-L","can_delete":false,"product_type":"c1","uid":1603052,"ip_address":"","ucode":"5AC96AAB75B720","user_header":"https://static001.geekbang.org/account/avatar/00/18/75/ec/c60b29f5.jpg","comment_is_top":true,"comment_ctime":1632133863,"is_pvip":false,"replies":[{"id":"113453","content":"赞👍，Perfect！<br><br>可以作为Python标杆代码，供后续同学参考~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632327030,"ip_address":"","comment_id":312944,"utype":1}],"discussion_count":5,"race_medal":0,"score":"9.2233720986165002e+18","product_id":100090001,"comment_content":"在Python中运行:<br>``` <br>from pyspark import SparkContext<br><br>textFile = SparkContext().textFile(&quot;.&#47;wikiOfSpark.txt&quot;)<br>wordCount = (<br>    textFile.flatMap(lambda line: line.split(&quot; &quot;))<br>    .filter(lambda word: word != &quot;&quot;)<br>    .map(lambda word: (word, 1))<br>    .reduceByKey(lambda x, y: x + y)<br>    .sortBy(lambda x: x[1], False)<br>    .take(5)<br>)<br>print(wordCount)<br>#显示: [(&#39;the&#39;, 67), (&#39;Spark&#39;, 63), (&#39;a&#39;, 54), (&#39;and&#39;, 51), (&#39;of&#39;, 50)]<br>``` <br>","like_count":14,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527174,"discussion_content":"赞👍，Perfect！\n\n可以作为Python标杆代码，供后续同学参考~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632327030,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1408343,"avatar":"https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg","nickname":"王璀璨","note":"","ucode":"9873E12D503CB4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":398632,"discussion_content":"使用这个代码报错:py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.  请问一下有谁知道是什么原因吗，我找了好多种办法都没解决","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632823461,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1408343,"avatar":"https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg","nickname":"王璀璨","note":"","ucode":"9873E12D503CB4","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399135,"discussion_content":"可以试试这里的方法：\n\nhttps://stackoverflow.com/questions/50064646/py4j-protocol-py4jjavaerror-occurred-while-calling-zorg-apache-spark-api-python","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632910351,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":398632,"ip_address":""},"score":399135,"extra":""},{"author":{"id":1408343,"avatar":"https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg","nickname":"王璀璨","note":"","ucode":"9873E12D503CB4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":399404,"discussion_content":"这里面的方式都试了，还是没有用。会是因为是windows环境报错的原因吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632964058,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":399135,"ip_address":""},"score":399404,"extra":""},{"author":{"id":1408343,"avatar":"https://static001.geekbang.org/account/avatar/00/15/7d/57/c94b6a93.jpg","nickname":"王璀璨","note":"","ucode":"9873E12D503CB4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":399414,"discussion_content":"最终发现的问题竟然是安装的spark和pyspark版本没一致。。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632965168,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":399135,"ip_address":""},"score":399414,"extra":""}]}]},{"had_liked":false,"id":310937,"user_name":"liugddx","can_delete":false,"product_type":"c1","uid":1016163,"ip_address":"","ucode":"FDB66E03A74422","user_header":"https://static001.geekbang.org/account/avatar/00/0f/81/63/2ceecb43.jpg","comment_is_top":false,"comment_ctime":1630992793,"is_pvip":false,"replies":[{"id":"112695","content":"好问题，Hadoop的范畴可大可小。<br><br>往小了说，Hadoop特指HDFS、YARN、MapReduce这三个组件，他们分别是Hadoop分布式文件系统、分布式任务调度框架、分布式计算引擎。<br><br>往大了说，Hadoop生态包含所有由这3个组件衍生出的大数据产品，如Hive、Hbase、Pig、Sqoop，等等。<br><br>Spark和Hadoop的关系，是共生共赢的关系。Spark的定位是分布式计算引擎，因此，它的直接“竞争对手”，是MapReduce，也就是Hadoop的分布式计算引擎。Spark是内存计算引擎，而MapReduce在计算的过程中，需要频繁落盘，因此，一般来说，相比MapReduce，Spark在执行性能上，更胜一筹。<br><br>对于HDFS、YARN，Spark可与之完美结合，实际上，在很多的使用场景中，Spark的数据源往往存储于HDFS，而YARN是Spark重要的资源调度框架之一。<br><br>大体上是这些，当然，还可以说的更细，老弟可以继续在后台留言，我们继续讨论~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631024087,"ip_address":"","comment_id":310937,"utype":1}],"discussion_count":4,"race_medal":0,"score":"147659880857","product_id":100090001,"comment_content":"我是一个大数据小白，我想咨询下spark和hadoop在大数据体系下的关系？","like_count":34,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526408,"discussion_content":"好问题，Hadoop的范畴可大可小。\n\n往小了说，Hadoop特指HDFS、YARN、MapReduce这三个组件，他们分别是Hadoop分布式文件系统、分布式任务调度框架、分布式计算引擎。\n\n往大了说，Hadoop生态包含所有由这3个组件衍生出的大数据产品，如Hive、Hbase、Pig、Sqoop，等等。\n\nSpark和Hadoop的关系，是共生共赢的关系。Spark的定位是分布式计算引擎，因此，它的直接“竞争对手”，是MapReduce，也就是Hadoop的分布式计算引擎。Spark是内存计算引擎，而MapReduce在计算的过程中，需要频繁落盘，因此，一般来说，相比MapReduce，Spark在执行性能上，更胜一筹。\n\n对于HDFS、YARN，Spark可与之完美结合，实际上，在很多的使用场景中，Spark的数据源往往存储于HDFS，而YARN是Spark重要的资源调度框架之一。\n\n大体上是这些，当然，还可以说的更细，老弟可以继续在后台留言，我们继续讨论~","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1631024087,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2823352,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/14/b8/2d030d91.jpg","nickname":"禅","note":"","ucode":"FA0E4FD1672854","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":561357,"discussion_content":"可以接着往下讲吗？期待","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649605961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":526408,"ip_address":""},"score":561357,"extra":""}]},{"author":{"id":1016163,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/81/63/2ceecb43.jpg","nickname":"liugddx","note":"","ucode":"FDB66E03A74422","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392824,"discussion_content":"吴老师，我还想问关于版本的问题，大数据很多组件都有版本不兼容的问题，我学习的话该用什么版本呢？会不会和其他比如hadoop有不兼容等等问题？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631149643,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1016163,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/81/63/2ceecb43.jpg","nickname":"liugddx","note":"","ucode":"FDB66E03A74422","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399136,"discussion_content":"这个就case by case了，不过都用最新的稳定版，一般都没啥问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632910388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392824,"ip_address":""},"score":399136,"extra":""}]}]},{"had_liked":false,"id":312691,"user_name":"Neo-dqy","can_delete":false,"product_type":"c1","uid":1924786,"ip_address":"","ucode":"9BF300EB1DDD00","user_header":"https://static001.geekbang.org/account/avatar/00/1d/5e/b2/aceb3e41.jpg","comment_is_top":false,"comment_ctime":1631944433,"is_pvip":false,"replies":[{"id":"113342","content":"Scala语法确实比较灵活，一般来说，简单表达式用()，复杂表达式用{}。<br><br>比如，简单函数体：(x =&gt; x + 1)、(_ + 1)，等等；<br>复杂函数体：{case x: String =&gt; “一大堆关于x的转换” }<br><br>关于最后的问题，也就是Scala也可以实现Word Count，和Spark有什么区别。这个问题比较重要，老弟需要用心听一下~<br><br>其实，任何一种语言，都可以实现任何计算逻辑，毕竟是高级编程语言，基本上都是图灵完备的。所以说像word count这种非常简单的逻辑，不只是scala，其他语言都能搞定。但是，Scala也好、Java也罢，再或者是Python，他们实现的word count，只能在单机跑，而Spark实现的Word Count，是在分布式环境跑。这，是本质的区别。<br><br>Spark，最核心的能力，就是分布式计算，利用大规模集群的能力。这是最本质的区别。<br><br>比如，现在有2万亿行的文本，需要你计算word count，你用scala也能在单机实现，但是，这个word count只能在单机跑，对于一般的机器，大概率是跑不动的。<br><br>可是，在分布式集群中，这样的量级，就轻松得多了。Spark，更多的，是提供一种分布式计算的能力，它提供给开发者简单的开发API，让开发者可以像开发单机应用那样，轻而易举地开发分布式应用，让分布式计算对于开发者来说，变得透明。开发者只需要关注业务逻辑，或者说计算逻辑，而不必关心分布式系统底层的调度、分发、数据交换、等等和分布式计算本身有关的东西。<br><br>所以说，Scala和Spark，两者没有可比性。当然，除了Spark，现在还有非常多的分布式计算框架，比如MapReduce、Flink、Presto、TensorFlow，等等，大家都是玩分布式计算的，只不过“术业有专攻”，各司其职~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632051865,"ip_address":"","comment_id":312691,"utype":1}],"discussion_count":5,"race_medal":0,"score":"66056453873","product_id":100090001,"comment_content":"老师好！wordCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(false).take(5)这行代码我还存在疑问，为什么这里的map函数使用了花括号{ }而不是上面一些算子的( )，同时这个case又是什么意思？这一行代码非常像我曾经在Python中使用字典数据结构，然后根据字典值的升序排序。最后，貌似Scala语言本身就可以实现wordcount案例，那么它本身的实现和spark实现相比，spark有什么优势呢？","like_count":15,"discussions":[{"author":{"id":1598796,"avatar":"https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg","nickname":"welldo","note":"","ucode":"D38E75364CD2E3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":405379,"discussion_content":"第一个问题,吴老师回答得比较简单,我来深入挖掘一下.\n这种语法,在scala里叫偏函数,直接说这个名词,可能有点儿懵,\n我们来看一下推演过程.\n\n题目:\n这里有个集合,val list = List(1, 2, 3, 4, &#34;abc&#34;)\n要求将集合list中的所有数字元素+1，忽略掉 非数字 的元素，并返回一个新的集合\n很显然,答案为(2, 3, 4, 5)\n\n方式1(古老方式)\n使用scala自带的filter算子(筛掉非int),和 map算子(剩下的int +1)\n\n方式2(稍微先进的方式,使用&#34;模式匹配&#34;，相当于java中的switch)\n    def add1(item :Any)={\n        item match {\n            case x:Int => x +1\n            case _ =>\n        }\n    }\n\n\n方式3(先进的方式,偏函数)\n使用场景：不是针对所有元素都进行逻辑操作，而只针对符合条件的元素进行操作\nval pf = new PartialFunction[Any, Int] {\n      override def isDefinedAt(x: Any): Boolean = { }\n      override def apply(v1: Any): Int = { }\n    }\n\n解释：isDefinedAt（）函数,会遍历集合的所有元素，如果返回值为true ,则继续对此元素执行 apply（）函数, 构建返回值。\n对应到这个题,也就是isDefinedAt()函数 筛选int型, 然后在 apply()函数中+1\n\n方式4(先进的方式,偏函数的简写版本)\n因为上面那一坨实在太臃肿且标准化,所以用语法糖简写如下\nlist.collect {\n      case i: Int => i + 1\n    }\n\n总结:把大括号内的一组case语句封装为函数，我们称之为偏函数.\n\n在老师的示例代码中,map{case (k, v) => (v, k)}\n就是把二元组kv, 变成vk","likes_number":6,"is_delete":false,"is_hidden":false,"ctime":1634562666,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527081,"discussion_content":"Scala语法确实比较灵活，一般来说，简单表达式用()，复杂表达式用{}。\n\n比如，简单函数体：(x =&amp;gt; x + 1)、(_ + 1)，等等；\n复杂函数体：{case x: String =&amp;gt; “一大堆关于x的转换” }\n\n关于最后的问题，也就是Scala也可以实现Word Count，和Spark有什么区别。这个问题比较重要，老弟需要用心听一下~\n\n其实，任何一种语言，都可以实现任何计算逻辑，毕竟是高级编程语言，基本上都是图灵完备的。所以说像word count这种非常简单的逻辑，不只是scala，其他语言都能搞定。但是，Scala也好、Java也罢，再或者是Python，他们实现的word count，只能在单机跑，而Spark实现的Word Count，是在分布式环境跑。这，是本质的区别。\n\nSpark，最核心的能力，就是分布式计算，利用大规模集群的能力。这是最本质的区别。\n\n比如，现在有2万亿行的文本，需要你计算word count，你用scala也能在单机实现，但是，这个word count只能在单机跑，对于一般的机器，大概率是跑不动的。\n\n可是，在分布式集群中，这样的量级，就轻松得多了。Spark，更多的，是提供一种分布式计算的能力，它提供给开发者简单的开发API，让开发者可以像开发单机应用那样，轻而易举地开发分布式应用，让分布式计算对于开发者来说，变得透明。开发者只需要关注业务逻辑，或者说计算逻辑，而不必关心分布式系统底层的调度、分发、数据交换、等等和分布式计算本身有关的东西。\n\n所以说，Scala和Spark，两者没有可比性。当然，除了Spark，现在还有非常多的分布式计算框架，比如MapReduce、Flink、Presto、TensorFlow，等等，大家都是玩分布式计算的，只不过“术业有专攻”，各司其职~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632051865,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2759990,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/1d/36/a0a283f8.jpg","nickname":"千江有水千江月","note":"","ucode":"23F5A65024CB67","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":406992,"discussion_content":"case 模式匹配  花括号表示代码块，简单的用（），复杂的用{}，内部可以用；隔开每个简单代码。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634885005,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2642450,"avatar":"https://static001.geekbang.org/account/avatar/00/28/52/12/36300520.jpg","nickname":"大大怪下士。","note":"","ucode":"5DFC38A7D54D9B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":401218,"discussion_content":"Spark做的是一种分布式计算，它的底层代码我目前还没有读过，但是你可以这样理解，它将运算的逻辑发给集群中的一些节点，每个节点分别去做计算。而如果你仅仅使用Java或者其他语言编写wordcount，你应该最多只能在一台机器上做运算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633605924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2768136,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/3d/08/fcf92621.jpg","nickname":"小玲铛🍯","note":"","ucode":"C9608A91BA66A0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":395539,"discussion_content":"回答的太好了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632313297,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310926,"user_name":"Vic","can_delete":false,"product_type":"c1","uid":1898294,"ip_address":"","ucode":"234BBDADBEF8C0","user_header":"https://static001.geekbang.org/account/avatar/00/1c/f7/36/ccf3b5d1.jpg","comment_is_top":false,"comment_ctime":1630989581,"is_pvip":true,"replies":[{"id":"112697","content":"这个是我的锅，哈哈~ <br><br>这里是我偷懒了，我应该在这条code上面加个注释，这里“_”（下划线）的意思，是你的文件根目录。Scala里面，用“_”表示一些不重要、不关心的东西，所以我偷用了Scala的“_”。但这里确实会引起困惑，不用管这个，我的锅，用你的文件根目录替换掉这里的“_”就好~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631024301,"ip_address":"","comment_id":310926,"utype":1}],"discussion_count":9,"race_medal":0,"score":"31695760653","product_id":100090001,"comment_content":"遇到这个问题<br>scala&gt; val rootPath: String = _<br>&lt;console&gt;:24: error: unbound placeholder parameter<br>       val rootPath: String = _<br>网上搜一下，说这是汇编错误。<br>要把val 改成var , 但会遇到&quot;_&quot;这default值是null。<br>org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:&#47;Users&#47;vic&#47;src&#47;data&#47;null&#47;wikiOfSpark.txt<br>这一段就先跳过root_path，直接给file一个路径，是可以成功运行&quot;word count&quot;,得到和老师一样的结果:<br>[Stage 0:&gt;                                                          (0 + 2) &#47;                                                                               res0: Array[(Int, String)] = Array((67,the), (63,Spark), (54,a), (51,and), (50,of))","like_count":7,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526403,"discussion_content":"这个是我的锅，哈哈~ \n\n这里是我偷懒了，我应该在这条code上面加个注释，这里“_”（下划线）的意思，是你的文件根目录。Scala里面，用“_”表示一些不重要、不关心的东西，所以我偷用了Scala的“_”。但这里确实会引起困惑，不用管这个，我的锅，用你的文件根目录替换掉这里的“_”就好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631024301,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1647570,"avatar":"https://static001.geekbang.org/account/avatar/00/19/23/d2/6ccb3d6c.jpg","nickname":"hope","note":"","ucode":"8CABACA63DF653","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392542,"discussion_content":"加一个注释说明应该就可以吧～","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1631032097,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1647570,"avatar":"https://static001.geekbang.org/account/avatar/00/19/23/d2/6ccb3d6c.jpg","nickname":"hope","note":"","ucode":"8CABACA63DF653","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392657,"discussion_content":"对的，后面会让编辑帮忙在后台加一下~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631090773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392542,"ip_address":""},"score":392657,"extra":""},{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":1647570,"avatar":"https://static001.geekbang.org/account/avatar/00/19/23/d2/6ccb3d6c.jpg","nickname":"hope","note":"","ucode":"8CABACA63DF653","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392727,"discussion_content":"已经在正文增加了，谢谢大伙儿反馈","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1631103443,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392542,"ip_address":""},"score":392727,"extra":""}]},{"author":{"id":1898294,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/f7/36/ccf3b5d1.jpg","nickname":"Vic","note":"","ucode":"234BBDADBEF8C0","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392604,"discussion_content":"哈哈，谢谢老师的说明。我欣赏老师这种坦率和幽默的性格。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1631068639,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1031075,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/bb/a3/af469d27.jpg","nickname":"Qilin Lou","note":"","ucode":"29AD43329E7404","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392451,"discussion_content":"这里确实稍微有点迷惑，在 Scala 里面直接这样写会有一个语法错。@JavaXu 和你的搞法都挺好，其实我更建议讲师在这里改成???","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1631009555,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1031075,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/bb/a3/af469d27.jpg","nickname":"Qilin Lou","note":"","ucode":"29AD43329E7404","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392658,"discussion_content":"后面会在code上面补充个注释哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631090813,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392451,"ip_address":""},"score":392658,"extra":""}]},{"author":{"id":2758135,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/15/f7/aba61f1b.jpg","nickname":"JavaXu","note":"","ucode":"B2EBC7BEEF4D1C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392413,"discussion_content":"val rootPath: String = _   意思是让你自己设置一下你本地文件的目录\n比如 val rootPath: String = s&#34;/Users/MyName/Documents/spark-learning&#34;","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1630999285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2758135,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/15/f7/aba61f1b.jpg","nickname":"JavaXu","note":"","ucode":"B2EBC7BEEF4D1C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392659,"discussion_content":"是这个意思~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631090878,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392413,"ip_address":""},"score":392659,"extra":""}]}]},{"had_liked":false,"id":310935,"user_name":"崔小豪","can_delete":false,"product_type":"c1","uid":2604437,"ip_address":"","ucode":"ABB59CE5209E67","user_header":"https://static001.geekbang.org/account/avatar/00/27/bd/95/e923a332.jpg","comment_is_top":false,"comment_ctime":1630992132,"is_pvip":false,"replies":[{"id":"112696","content":"欢迎~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631024120,"ip_address":"","comment_id":310935,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23105828612","product_id":100090001,"comment_content":"前排占座！三年前接触过 Spark 今天从头再学！","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526406,"discussion_content":"欢迎~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631024120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312725,"user_name":"Neo-dqy","can_delete":false,"product_type":"c1","uid":1924786,"ip_address":"","ucode":"9BF300EB1DDD00","user_header":"https://static001.geekbang.org/account/avatar/00/1d/5e/b2/aceb3e41.jpg","comment_is_top":false,"comment_ctime":1631954321,"is_pvip":false,"replies":[{"id":"113344","content":"是的，没错~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632052047,"ip_address":"","comment_id":312725,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18811823505","product_id":100090001,"comment_content":"老师我可以再问一下，如果我是用IDEA创建Spark项目，是不是只要配置好Scala的SDK，然后在pom文件中加入对应版本号的spark依赖，就会自动下载spark包了？这个时候不需要再去官网下载spark了吗，同时也不再需要使用spark-shell了吗？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527089,"discussion_content":"是的，没错~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632052047,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":348495,"user_name":"杨帅","can_delete":false,"product_type":"c1","uid":1684811,"ip_address":"","ucode":"0A558B1BA62E44","user_header":"","comment_is_top":false,"comment_ctime":1655135528,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"14540037416","product_id":100090001,"comment_content":"老师，spark-3.2.1 默认从hdfs读取文件，所以rootPath需要加上协议 file:&#47;&#47;.","like_count":3},{"had_liked":false,"id":327787,"user_name":"浮生若梦","can_delete":false,"product_type":"c1","uid":1454469,"ip_address":"","ucode":"5048F62293A484","user_header":"https://static001.geekbang.org/account/avatar/00/16/31/85/5fd92ebe.jpg","comment_is_top":false,"comment_ctime":1640307584,"is_pvip":false,"replies":[{"id":"119390","content":"满分💯，赞👍~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1640355674,"ip_address":"","comment_id":327787,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14525209472","product_id":100090001,"comment_content":"Java实现：<br><br>SparkConf sparkConf = new SparkConf().setAppName(&quot;Test&quot;).setMaster(&quot;local[*]&quot;);<br>        JavaSparkContext JSC = new JavaSparkContext(sparkConf);<br><br>        &#47;&#47; 读取文件内容<br>        JavaRDD&lt;String&gt; lineRDD = JSC.textFile(&quot;wikiOfSpark.txt&quot;);<br>        &#47;&#47; 以行为单位做分词<br>        JavaRDD&lt;String&gt; wordRDD = lineRDD.flatMap(new FlatMapFunction&lt;String, String&gt;() {<br>            @Override<br>            public Iterator&lt;String&gt; call(String s) throws Exception {<br>                return  Arrays.asList(s.split(&quot; &quot;)).iterator();<br>            }<br>        });<br>        JavaRDD&lt;String&gt; cleanWordRDD = wordRDD.filter(new Function&lt;String, Boolean&gt;() {<br>            @Override<br>            public Boolean call(String s) throws Exception {<br>                return !s.equals(&quot;&quot;);<br>            }<br>        });<br><br>        &#47;&#47; 把RDD元素转换为（Key，Value）的形式<br>        JavaPairRDD&lt;String, Integer&gt; kvRDD = cleanWordRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {<br>            @Override<br>            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {<br>                return new Tuple2&lt;String, Integer&gt;(s,1);<br>            }<br>        });<br>        &#47;&#47; 按照单词做分组计数<br>        JavaPairRDD&lt;String, Integer&gt; wordCounts = kvRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {<br>            @Override<br>            public Integer call(Integer integer, Integer integer2) throws Exception {<br>                return integer+integer2;<br>            }<br>        });<br>        &#47;&#47; 打印词频最高的5个词汇(先将元组的key value交换一下顺序，然后在调用sortByKey())<br>        wordCounts.mapToPair((row)-&gt;  new Tuple2&lt;&gt;(row._2,row._1)).sortByKey(false).foreach(new VoidFunction&lt;Tuple2&lt;Integer, String&gt;&gt;() {<br>            @Override<br>            public void call(Tuple2&lt;Integer, String&gt; stringIntegerTuple2) throws Exception {<br>                System.out.println(stringIntegerTuple2._1 + &quot;:&quot; + stringIntegerTuple2._2);<br>            }<br>        });<br><br>        &#47;&#47;关闭context<br>        JSC.close();","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":541399,"discussion_content":"满分💯，赞👍~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640355674,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312601,"user_name":"火炎焱燚","can_delete":false,"product_type":"c1","uid":2767491,"ip_address":"","ucode":"DB11784DD94059","user_header":"https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg","comment_is_top":false,"comment_ctime":1631886464,"is_pvip":false,"replies":[{"id":"113340","content":"Cool~<br><br>后面等专栏更完了，打算把所有Scala代码整理出一份Python版本的，老弟有没有兴趣一起呀~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632051299,"ip_address":"","comment_id":312601,"utype":1}],"discussion_count":2,"race_medal":0,"score":"14516788352","product_id":100090001,"comment_content":"Python版代码为：<br><br>file=&#39;~~~&#47;wikiOfSpark.txt&#39;<br>lineRDD=sc.textFile(file)<br>lineRDD.first() # 会打印出lineRDD的第一行： u&#39;Apache Spark&#39;，如果出错则不打印<br>wordRDD=lineRDD.flatMap(lambda line: line.split(&quot; &quot;))<br>cleanWordRDD=wordRDD.filter(lambda word: word!=&#39;&#39;)<br>kvRDD=cleanWordRDD.map(lambda word:(word,1))<br>wordCounts=kvRDD.reduceByKey(lambda x,y:x+y)<br>wordCounts.map(lambda (k,v):(v,k)).sortByKey(False).take(5)","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527041,"discussion_content":"Cool~\n\n后面等专栏更完了，打算把所有Scala代码整理出一份Python版本的，老弟有没有兴趣一起呀~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632051299,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2767491,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg","nickname":"火炎焱燚","note":"","ucode":"DB11784DD94059","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":401913,"discussion_content":"好的啊，我这儿主要学习python，所以打算把课程中的代码都用python实现一遍。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1633762936,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":343030,"user_name":"陈金鑫","can_delete":false,"product_type":"c1","uid":1077127,"ip_address":"","ucode":"609A24832CA80C","user_header":"https://static001.geekbang.org/account/avatar/00/10/6f/87/669263b4.jpg","comment_is_top":false,"comment_ctime":1650602160,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10240536752","product_id":100090001,"comment_content":"又学了一遍，wordcount一句话实现：<br>spark.sparkContext.textFile(file).flatMap(_.split(&quot; &quot;)).filter(!_.equals(&quot;&quot;)).map((_, 1)).reduceByKey(_ + _).map(t =&gt; (t._2, t._1)).sortByKey(false).take(5)","like_count":2},{"had_liked":false,"id":333067,"user_name":"国度","can_delete":false,"product_type":"c1","uid":1697428,"ip_address":"","ucode":"525DA45D56E8C9","user_header":"https://static001.geekbang.org/account/avatar/00/19/e6/94/98f30daf.jpg","comment_is_top":false,"comment_ctime":1644044352,"is_pvip":false,"replies":[{"id":"121998","content":"赞学习打卡~ 一起加油~ ","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1644674396,"ip_address":"","comment_id":333067,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10233978944","product_id":100090001,"comment_content":"2022年2月5号学习打卡记录<br>机器环境：ROG14<br>操作系统：win11 + wsl Ubuntu20.04<br>环境变量：<br>----------------------------<br><br>export SPARK_HOME=&#47;mnt&#47;c&#47;spark&#47;spark-2.4.8-bin-hadoop2.7<br>export JAVA_HOME=&#47;mnt&#47;c&#47;linux_environment&#47;jdk&#47;jdk1.8.0_321<br>export M2_HOME=&#47;mnt&#47;c&#47;linux_environment&#47;apache-maven-3.8.4<br>export SCALA_HOME=&#47;mnt&#47;c&#47;linux_environment&#47;scala3-3.1.1<br><br>export PATH=$SPARK_HOME&#47;bin:$SCALA_HOME&#47;bin:$M2_HOME&#47;bin:$JAVA_HOME&#47;bin:$PATH<br><br>---------------------------<br>希望帮助和我一样从零开始一起学习的同学躲避一些坑：<br><br>坑1：jdk版本不兼容：<br>一开始使用jdk17版本，在启动过程中一直报错，降为1.8后启动成功；<br><br>坑2：hadoop版本问题：<br>hadoop3.2.1 逐步使用Dataset，报错类型转换异常；<br>由于scala经验不足，暂时无法大规模改写老师的代码，降低版本为spark2.4.8<br>下载地址：https:&#47;&#47;dlcdn.apache.org&#47;spark&#47; 可以选择适合的版本下载<br><br>原理性的还没有搞懂，目前在第一阶段，读懂，简单改写为主；<br><br>感谢吴磊老师的课","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":550695,"discussion_content":"赞学习打卡~ 一起加油~ ","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1644674396,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322124,"user_name":"猫太太","can_delete":false,"product_type":"c1","uid":1537617,"ip_address":"","ucode":"7A1D1CAA3AF260","user_header":"https://static001.geekbang.org/account/avatar/00/17/76/51/96291466.jpg","comment_is_top":false,"comment_ctime":1637198209,"is_pvip":false,"replies":[{"id":"117221","content":"不需要的~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637653868,"ip_address":"","comment_id":322124,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10227132801","product_id":100090001,"comment_content":"请问在本地部署spark环境不需要先安装hadoop么","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532612,"discussion_content":"不需要的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637653868,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312596,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1631884796,"is_pvip":false,"replies":[{"id":"113339","content":"spark-shell中，spark是默认的SparkSession实例，你是在spark-shell中执行这段代码吗？如果是在IDE，需要自己明确定义SparkSession实例的，比如：<br><br>import org.apache.spark.sql.SparkSession<br><br>val spark = SparkSession.builder<br>  .master(&quot;local[2]&quot;)<br>  .appName(&quot;SparkSession Example&quot;)<br>  .getOrCreate()","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632051238,"ip_address":"","comment_id":312596,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10221819388","product_id":100090001,"comment_content":"问下执行 val lineRDD: RDD[String] = spark.sparkContext.textFile(file) 报错error: not found: value spark是怎么回事？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527038,"discussion_content":"spark-shell中，spark是默认的SparkSession实例，你是在spark-shell中执行这段代码吗？如果是在IDE，需要自己明确定义SparkSession实例的，比如：\n\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder\n  .master(&amp;quot;local[2]&amp;quot;)\n  .appName(&amp;quot;SparkSession Example&amp;quot;)\n  .getOrCreate()","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632051238,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":350438,"user_name":"🌈你是人间四月天💫","can_delete":false,"product_type":"c1","uid":2141510,"ip_address":"","ucode":"3DF00177C07A06","user_header":"https://static001.geekbang.org/account/avatar/00/20/ad/46/f3c56862.jpg","comment_is_top":false,"comment_ctime":1656915495,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5951882791","product_id":100090001,"comment_content":"像我这种零基础的，看到很懵，第一个demo代码是spark，这个变量怎么来的？这个真的适合零基础？","like_count":1,"discussions":[{"author":{"id":2941084,"avatar":"","nickname":"Geek_ce7292","note":"","ucode":"7C89258611315E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":583649,"discussion_content":"同小白前来报道，一起加油呀~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660269953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"中国香港"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":342661,"user_name":"Gavin_From_Mars","can_delete":false,"product_type":"c1","uid":1568046,"ip_address":"","ucode":"B66CC36E3F8129","user_header":"https://static001.geekbang.org/account/avatar/00/17/ed/2e/7a171500.jpg","comment_is_top":false,"comment_ctime":1650389909,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5945357205","product_id":100090001,"comment_content":"按照文中安装后一直有问题。。。启动spark看到默认带scala2.12.15.但是启动spark shell有错误。“error: not found: value spark”，然后在shell中执行文中的wordcount也是同样的错误","like_count":1,"discussions":[{"author":{"id":1169841,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d9/b1/e0db45de.jpg","nickname":"圈圈gor","note":"","ucode":"30B3FDAE5D92B6","race_medal":2,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":566991,"discussion_content":"你这是在window下执行的吗？我遇到同样问题，切换到linux里就不会了，会自动创建这个对象。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650810531,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":341492,"user_name":"我爱夜来香","can_delete":false,"product_type":"c1","uid":2609930,"ip_address":"","ucode":"10761E677EF05F","user_header":"https://static001.geekbang.org/account/avatar/00/27/d3/0a/92640aae.jpg","comment_is_top":false,"comment_ctime":1649659919,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5944627215","product_id":100090001,"comment_content":"老师,DAG计算模型的优势可不可以这么理解:spark在没有遇到宽依赖之前,所有的操作包括wordRDD到lineRDD到wordRDD到kvRDD等都是一次性在内存中完成的,最终输出kvRDD到磁盘中进行shuffle,只会有一次落盘操作;<br>而mapReduce中比如把整个文本切割成一行一行,在内存中完成后需要落盘,再由一行一行转换成键值对又要落盘.中间涉及了太多写盘操作。换句话说,spark基于内存计算就是充分利用内存?麻烦老师解答一下","like_count":1},{"had_liked":false,"id":311235,"user_name":"Z","can_delete":false,"product_type":"c1","uid":2758639,"ip_address":"","ucode":"A404BDC6C4435C","user_header":"https://static001.geekbang.org/account/avatar/00/2a/17/ef/d0a1a069.jpg","comment_is_top":false,"comment_ctime":1631113822,"is_pvip":false,"replies":[{"id":"112843","content":"看看分隔符是不是用了空字符串？分隔符应该是空格“ ”","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631236752,"ip_address":"","comment_id":311235,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5926081118","product_id":100090001,"comment_content":"为啥我的结果是单个字母呢？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526533,"discussion_content":"看看分隔符是不是用了空字符串？分隔符应该是空格“ ”","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631236752,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311025,"user_name":"钱鹏 Allen","can_delete":false,"product_type":"c1","uid":2518863,"ip_address":"","ucode":"7E95E82C0717DA","user_header":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","comment_is_top":false,"comment_ctime":1631022850,"is_pvip":true,"replies":[{"id":"112755","content":"刚刚开始，需要注意的细节确实比较多，慢慢的就好了~ 一起加油~ 💪","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631090606,"ip_address":"","comment_id":311025,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5925990146","product_id":100090001,"comment_content":"注意空格“ ”，和空字符串“”，前者有空格，后者没有<br><br>书写的时候，根据自己的文件所在目录来，比如我的是 &#47;input&#47;wikiOfSpark.txt<br>不要遗漏后缀名。<br><br>学习的过程，需要给自己一些耐心和鼓励，一起加油把！","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526438,"discussion_content":"刚刚开始，需要注意的细节确实比较多，慢慢的就好了~ 一起加油~ 💪","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631090606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310970,"user_name":"GAC·DU","can_delete":false,"product_type":"c1","uid":1385403,"ip_address":"","ucode":"7847FBE1C13740","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","comment_is_top":false,"comment_ctime":1631003916,"is_pvip":true,"replies":[{"id":"112691","content":"没错~ Exactly！","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631023241,"ip_address":"","comment_id":310970,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5925971212","product_id":100090001,"comment_content":"Spark RDD算子分为Transformation算子和Action算子，Transformation算子基本上都是延迟计算，需要通过调用Action算子进行触发。","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526414,"discussion_content":"没错~ Exactly！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631023241,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":360792,"user_name":"Geek_f09cec","can_delete":false,"product_type":"c1","uid":3197217,"ip_address":"广东","ucode":"789CED750CD905","user_header":"","comment_is_top":false,"comment_ctime":1666856957,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666856957","product_id":100090001,"comment_content":"将“${解压目录}&#47;bin”配置到 PATH 环境变量。这一步不懂，在哪儿配置，怎么配置呢？","like_count":0},{"had_liked":false,"id":353734,"user_name":"VenFox","can_delete":false,"product_type":"c1","uid":1002112,"ip_address":"广东","ucode":"11F08AD5685505","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4a/80/b1862ca3.jpg","comment_is_top":false,"comment_ctime":1659707991,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1659707991","product_id":100090001,"comment_content":"老师，请问什么叫算子？可以理解成函数吗？","like_count":0,"discussions":[{"author":{"id":2941084,"avatar":"","nickname":"Geek_ce7292","note":"","ucode":"7C89258611315E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":583650,"discussion_content":"浅浅的帮助老师回答一下，你的理解应该是没有问题的哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660269988,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"中国香港"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":348726,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1655350132,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1655350132","product_id":100090001,"comment_content":"import org.apache.spark.SparkConf<br>import org.apache.spark.rdd.RDD<br>import org.apache.spark.sql.SparkSession<br>object Main extends App {<br>  println(&quot;Hello, World!&quot;)<br><br>  &#47;&#47; 这里的下划线&quot;_&quot;是占位符，代表数据文件的根目录<br>  val rootPath: String = &quot;.&quot;<br>  val file: String = s&quot;$rootPath&#47;wikiOfSpark.txt&quot;<br><br>  val conf = new SparkConf()<br>    .setMaster(&quot;local[2]&quot;)<br>    .setAppName(&quot;Spark SQL basic example&quot;)<br><br>  val spark = SparkSession<br>    .builder()<br>    .config(conf)<br>    .getOrCreate()<br><br>  &#47;&#47; 读取文件内容<br>  val lineRDD: RDD[String] = spark.sparkContext.textFile(file)<br><br><br>  lineRDD.take(10).foreach(println)<br><br>  &#47;&#47; 以行为单位做分词<br>  val wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(&quot; &quot;))<br>  wordRDD.take(200).foreach(println)<br><br>  &#47;&#47; 过滤掉空字符串<br>  val cleanWordRDD: RDD[String] = wordRDD.filter(word =&gt; !word.equals(&quot;&quot;))<br>  cleanWordRDD.take(200).foreach(println)<br><br>  &#47;&#47; 把RDD元素转换为（Key，Value）的形式<br>  val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))<br>  kvRDD.take(10).foreach(println)<br><br>  &#47;&#47; 按照单词做分组计数<br>  val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)<br>  wordCounts.take(10).foreach(println)<br><br>  &#47;&#47; 打印词频最高的5个词汇<br>  val top5: Array[(Int, String)] = wordCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(ascending = false).take(5)<br><br>  println(top5.mkString(&quot;Array(&quot;, &quot;, &quot;, &quot;)&quot;))<br>}","like_count":0},{"had_liked":false,"id":346893,"user_name":"easy-cloud","can_delete":false,"product_type":"c1","uid":1107744,"ip_address":"","ucode":"3067EE44DAEF40","user_header":"https://static001.geekbang.org/account/avatar/00/10/e7/20/4f78c4e4.jpg","comment_is_top":false,"comment_ctime":1653525727,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1653525727","product_id":100090001,"comment_content":"如果这个文件有10个t，这个时候如果读取呀","like_count":0},{"had_liked":false,"id":346460,"user_name":"wedde","can_delete":false,"product_type":"c1","uid":1128756,"ip_address":"","ucode":"A49B1732CBE5DD","user_header":"https://static001.geekbang.org/account/avatar/00/11/39/34/10fe2b93.jpg","comment_is_top":false,"comment_ctime":1653149772,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1653149772","product_id":100090001,"comment_content":"作者你好~我在centos6.5上安装spark-3.2.1-bin-hadoop3.2。环境变量都配置正确了。运行spark-shell --version出现下面报错应该怎么解决?<br>Caused by: java.net.UnknownHostException: spark: spark: No address associated with hostname<br>\tat java.net.InetAddress.getLocalHost(InetAddress.java:1506)<br>\tat org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:1005)<br>\tat org.apache.spark.util.Utils$.localIpAddress$lzycompute(Utils.scala:998)<br>\tat org.apache.spark.util.Utils$.localIpAddress(Utils.scala:998)<br>\tat org.apache.spark.util.Utils$.$anonfun$localCanonicalHostName$1(Utils.scala:1055)<br>\tat scala.Option.getOrElse(Option.scala:189)<br>\tat org.apache.spark.util.Utils$.localCanonicalHostName(Utils.scala:1055)<br>\tat org.apache.spark.internal.config.package$.&lt;init&gt;(package.scala:990)<br>\tat org.apache.spark.internal.config.package$.&lt;clinit&gt;(package.scala)<br>\t... 10 more","like_count":0,"discussions":[{"author":{"id":1184678,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJDgV2qia6eAL7Fb4egX3odViclRRwOlkfCBrjhU9lLeib90KGkIDjdddSibNVs47N90L36Brgnr6ppiag/132","nickname":"ddww","note":"","ucode":"2871112FC9B3F7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":573242,"discussion_content":"你看，报错很明显了。spark: spark: No address associated with hostname。我不太清楚你运行时传的参数错误，还是你hostname设置的错误","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653290932,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":344778,"user_name":"xiong","can_delete":false,"product_type":"c1","uid":1099375,"ip_address":"","ucode":"C1BFD9EF96372F","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/6f/ac3003fa.jpg","comment_is_top":false,"comment_ctime":1651762922,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1651762922","product_id":100090001,"comment_content":"虽然现在的机器跑不动spark，但仍然需要学习啊，哈哈","like_count":0,"discussions":[{"author":{"id":1184678,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJDgV2qia6eAL7Fb4egX3odViclRRwOlkfCBrjhU9lLeib90KGkIDjdddSibNVs47N90L36Brgnr6ppiag/132","nickname":"ddww","note":"","ucode":"2871112FC9B3F7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":573243,"discussion_content":"在本地跑呀，其实没有什么好神秘的，把它当做普通的java程序就好了。并且，我不信你的电脑无法运行简单的spark程序","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653291062,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":339683,"user_name":"Spoon","can_delete":false,"product_type":"c1","uid":1959822,"ip_address":"","ucode":"2FF9193AD482C2","user_header":"https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg","comment_is_top":false,"comment_ctime":1648288885,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1648288885","product_id":100090001,"comment_content":"Java实现<br>https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;core&#47;WordCountJob.java<br>&#47;&#47;top5 频率单词<br>List&lt;Tuple2&lt;Integer, String&gt;&gt; top5CntWordList = rawRdd.flatMap(line -&gt; Splitter.on(&quot; &quot;).splitToList(line)<br>            .iterator())<br>            .filter(word -&gt; !Strings.isNullOrEmpty(word))<br>            .mapToPair(word -&gt; Tuple2.apply(word, 1))<br>            .reduceByKey(Integer::sum)<br>            .mapToPair(Tuple2::swap)<br>            .sortByKey(false)<br>            .take(5);","like_count":0},{"had_liked":false,"id":316564,"user_name":"Luke Skywalker","can_delete":false,"product_type":"c1","uid":1799433,"ip_address":"","ucode":"5345D0407363A0","user_header":"https://static001.geekbang.org/account/avatar/00/1b/75/09/464ade1e.jpg","comment_is_top":false,"comment_ctime":1634393928,"is_pvip":false,"replies":[{"id":"114669","content":"问下细节信息：<br><br>1）什么操作系统（Linux、Windows、Mac）？<br>2）是PySpark吗？<br>3）这个WARN是否影响spark-shell正常运行？<br><br>看上去是环境变量设置问题，老弟再多发些细节，我们一起再看看~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634533792,"ip_address":"","comment_id":316564,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634393928","product_id":100090001,"comment_content":"老师，请问我运行spark-shell总是报这个警告是什么问题呢？搜了好多解决办法也没去掉WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528414,"discussion_content":"问下细节信息：\n\n1）什么操作系统（Linux、Windows、Mac）？\n2）是PySpark吗？\n3）这个WARN是否影响spark-shell正常运行？\n\n看上去是环境变量设置问题，老弟再多发些细节，我们一起再看看~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634533792,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313200,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1632308323,"is_pvip":false,"replies":[{"id":"113468","content":"那不应该，Spark版本号多少？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632357979,"ip_address":"","comment_id":313200,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1632308323","product_id":100090001,"comment_content":"关于前几天问的 执行 val lineRDD: RDD[String] = spark.sparkContext.textFile(file) 报错error: not found: value spark 的问题，我确实是在spark-shell执行的，应该不需要显式创建spark-session吧","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527273,"discussion_content":"那不应该，Spark版本号多少？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632357979,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":396433,"discussion_content":"2.2.0","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632442577,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399137,"discussion_content":"那着实不应该，spark-shell里面，spark是默认的SparkSession instance，神奇了~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632910454,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":396433,"ip_address":""},"score":399137,"extra":""},{"author":{"id":1568046,"avatar":"https://static001.geekbang.org/account/avatar/00/17/ed/2e/7a171500.jpg","nickname":"Gavin_From_Mars","note":"","ucode":"B66CC36E3F8129","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":565116,"discussion_content":"我也是相同的错误。。。spark3.2.1一模一样的错误","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650389763,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":399137,"ip_address":""},"score":565116,"extra":""}]}]},{"had_liked":false,"id":311912,"user_name":"zhihui","can_delete":false,"product_type":"c1","uid":1649037,"ip_address":"","ucode":"780ED9E443525F","user_header":"https://static001.geekbang.org/account/avatar/00/19/29/8d/2869a10b.jpg","comment_is_top":false,"comment_ctime":1631530171,"is_pvip":false,"replies":[{"id":"113019","content":"1. IDE中需要导入Spark的开发包<br>2. 需要过滤掉空字符串：“”（就是79后面的“”）","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631544033,"ip_address":"","comment_id":311912,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1631530171","product_id":100090001,"comment_content":"1.<br>val lineRDD: RDD[String] = spark.sparkContext.textFile(file)<br>idea中，sparkContext报红，提示 “value sparkContext is not a member of spark”。<br>但是复制到spark-shell却可以执行。<br>我是写java的，总觉的这是导包问题，但又找不到这个包。一脸懵。<br>2. 我按照老师代码写的，一摸一样。执行结果是Array[(Int, String)] = Array((79,&quot;&quot;), (67,the), (63,Spark), (54,a), (51,and))","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526792,"discussion_content":"1. IDE中需要导入Spark的开发包\n2. 需要过滤掉空字符串：“”（就是79后面的“”）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631544033,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311839,"user_name":"undefined","can_delete":false,"product_type":"c1","uid":1001917,"ip_address":"","ucode":"72E03BBEF25E42","user_header":"https://static001.geekbang.org/account/avatar/00/0f/49/bd/02b20ca1.jpg","comment_is_top":false,"comment_ctime":1631501474,"is_pvip":false,"replies":[{"id":"113021","content":"Perfect~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631544165,"ip_address":"","comment_id":311839,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1631501474","product_id":100090001,"comment_content":"WordCount中有涉及两类算子，首先是map、filter、flatMap、 reduceByKey、sortByKey、sortBy等转换算子，属于延迟执行，需要另一种行动算子进行触发，行动算子包括：take、count、foreach、collect、first; ","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526767,"discussion_content":"Perfect~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631544165,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311748,"user_name":"巴普洛夫的","can_delete":false,"product_type":"c1","uid":1787466,"ip_address":"","ucode":"0FE795ECA04B1D","user_header":"https://static001.geekbang.org/account/avatar/00/1b/46/4a/b0cd391e.jpg","comment_is_top":false,"comment_ctime":1631440861,"is_pvip":false,"replies":[{"id":"112952","content":"一步步说哈~<br><br>map{case (k, v) =&gt; (v, k)}，这一步是把（Key，Value）对调，目的是按照“计数”来排序。比如，原来是（Spark，63），这一步之后，这条记录就变成了（63，Spark），不妨把这一步拆解开来，用first探索一下~<br><br>sortByKey(false) 是降序排序，注意，这时候的Key，不再是单词了，而是调换顺序之后的Value（比如63），也就是单词计数。<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631460925,"ip_address":"","comment_id":311748,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1631440861","product_id":100090001,"comment_content":"wordCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(false) <br>这一步是做了什么呢，没有见过的语法","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526729,"discussion_content":"一步步说哈~\n\nmap{case (k, v) =&amp;gt; (v, k)}，这一步是把（Key，Value）对调，目的是按照“计数”来排序。比如，原来是（Spark，63），这一步之后，这条记录就变成了（63，Spark），不妨把这一步拆解开来，用first探索一下~\n\nsortByKey(false) 是降序排序，注意，这时候的Key，不再是单词了，而是调换顺序之后的Value（比如63），也就是单词计数。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631460925,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1948015,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/b9/6f/35bd9674.jpg","nickname":"TheEve","note":"","ucode":"70551999133804","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":407311,"discussion_content":"// 打印词频最高的5个词汇\n// (v, k)对调的写法\nwordCounts.map { case (k, v) => (v, k) }.sortByKey(false).take(5).foreach(println)\n// (v, k)不对调的写法\nwordCounts.sortBy(_._2, false).take(5).foreach(println)\n\n// 注：这里用的是Scala~~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634977041,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311170,"user_name":"keke","can_delete":false,"product_type":"c1","uid":1077379,"ip_address":"","ucode":"BE28B8110DF861","user_header":"https://static001.geekbang.org/account/avatar/00/10/70/83/36ab65ec.jpg","comment_is_top":false,"comment_ctime":1631093356,"is_pvip":true,"replies":[{"id":"112908","content":"有更多的报错信息吗？可以把完整的报错信息贴出来，一起看看~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631344300,"ip_address":"","comment_id":311170,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1631093356","product_id":100090001,"comment_content":"报错“此时不应有 \\spark-3.1.2-bin-hadoop3\\bin\\..&#39;。”是啥问题呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526504,"discussion_content":"有更多的报错信息吗？可以把完整的报错信息贴出来，一起看看~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631344300,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1077379,"avatar":"https://static001.geekbang.org/account/avatar/00/10/70/83/36ab65ec.jpg","nickname":"keke","note":"","ucode":"BE28B8110DF861","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":393347,"discussion_content":"已经找到原因了，路径中不能有空格，之后还遇到了java版本太高模块化的错误，都解决了，spark已经安装完了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631368735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1077379,"avatar":"https://static001.geekbang.org/account/avatar/00/10/70/83/36ab65ec.jpg","nickname":"keke","note":"","ucode":"BE28B8110DF861","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":394263,"discussion_content":"Cool","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631804951,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":393347,"ip_address":""},"score":394263,"extra":""}]}]},{"had_liked":false,"id":311097,"user_name":"大志","can_delete":false,"product_type":"c1","uid":1039047,"ip_address":"","ucode":"648335930C2E55","user_header":"https://static001.geekbang.org/account/avatar/00/0f/da/c7/66f5fcea.jpg","comment_is_top":false,"comment_ctime":1631071163,"is_pvip":false,"replies":[{"id":"112756","content":"赞~ 👍 功夫不负有心人！","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631090682,"ip_address":"","comment_id":311097,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1631071163","product_id":100090001,"comment_content":"Win10系统又装了Hadoop，并下载了winutils.exe这个文件放到Hadoop bin目录下，换成了spark-2.4.6-bin-hadoop2.7运行spark-shell终于不报错了。Windows下文件路径写法val file: String = s&quot;file:&#47;&#47;&#47;D:&#47;wikiOfSpark.txt&quot;","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526473,"discussion_content":"赞~ 👍 功夫不负有心人！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631090682,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310954,"user_name":"JavaXu","can_delete":false,"product_type":"c1","uid":2758135,"ip_address":"","ucode":"B2EBC7BEEF4D1C","user_header":"https://static001.geekbang.org/account/avatar/00/2a/15/f7/aba61f1b.jpg","comment_is_top":false,"comment_ctime":1630999446,"is_pvip":false,"replies":[{"id":"112692","content":"讲道理Scala是需要单独安装的，神奇~ 不过没关系，spark-shell好使就行，重点是应用开发，我们不妨暂且把注意力先放到代码上~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631023550,"ip_address":"","comment_id":310954,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1630999446","product_id":100090001,"comment_content":"在macOS上，似乎默认安装了scala。<br>总之我安装了spark之后，spark-shell，然后键入代码，可以执行并得到结果。<br><br>或者是我安装的spark包，里面自带了scala？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526412,"discussion_content":"讲道理Scala是需要单独安装的，神奇~ 不过没关系，spark-shell好使就行，重点是应用开发，我们不妨暂且把注意力先放到代码上~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631023550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310941,"user_name":"Qilin Lou","can_delete":false,"product_type":"c1","uid":1031075,"ip_address":"","ucode":"29AD43329E7404","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bb/a3/af469d27.jpg","comment_is_top":false,"comment_ctime":1630995668,"is_pvip":false,"replies":[{"id":"112694","content":"好问题，我也是这么想，其实支持个filterNot不是什么难事，不知道为什么Spark社区没有提供这个算子。我猜可能是执着于“极客精神”，觉得filter能实现的功能，没必要再多添个算子~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631023717,"ip_address":"","comment_id":310941,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1630995668","product_id":100090001,"comment_content":"我其实比较喜欢用filterNot，这样里面的函数就不用取反，不过Spark里居然没有提供","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526410,"discussion_content":"好问题，我也是这么想，其实支持个filterNot不是什么难事，不知道为什么Spark社区没有提供这个算子。我猜可能是执着于“极客精神”，觉得filter能实现的功能，没必要再多添个算子~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631023717,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310871,"user_name":"Qilin Lou","can_delete":false,"product_type":"c1","uid":1031075,"ip_address":"","ucode":"29AD43329E7404","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bb/a3/af469d27.jpg","comment_is_top":false,"comment_ctime":1630940604,"is_pvip":false,"replies":[{"id":"112674","content":"哈哈，Java、Scala安装比较简单～ spark-shell需要用到","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630986991,"ip_address":"","comment_id":310871,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1630940604","product_id":100090001,"comment_content":"大意了，没想到用的是一台新电脑，还得先装Java Runtime","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526383,"discussion_content":"哈哈，Java、Scala安装比较简单～ spark-shell需要用到","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630986991,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310835,"user_name":"Stéphane 胡","can_delete":false,"product_type":"c1","uid":2757945,"ip_address":"","ucode":"BACE52B2E1C3E1","user_header":"https://static001.geekbang.org/account/avatar/00/2a/15/39/095dc1c2.jpg","comment_is_top":false,"comment_ctime":1630928952,"is_pvip":false,"replies":[{"id":"112635","content":"讲道理是周一、周三、周五各更一篇，不过今天上线我理解是预售，所以这周三、周五暂不更新。从下周开始，每周一、周三、周五各更一篇哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630937288,"ip_address":"","comment_id":310835,"utype":1}],"discussion_count":5,"race_medal":0,"score":"1630928952","product_id":100090001,"comment_content":"您好老师，请问一下多久会更新一讲呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526373,"discussion_content":"讲道理是周一、周三、周五各更一篇，不过今天上线我理解是预售，所以这周三、周五暂不更新。从下周开始，每周一、周三、周五各更一篇哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630937288,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1058183,"avatar":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","nickname":"peter","note":"","ucode":"261C3FC001DE2D","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392562,"discussion_content":"吴老师：请教另外一个问题：本课程有微信学习群吗？我在PC段登录极客时间，在“课程介绍”里面没有发现关于微信学习群的信息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631061346,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1058183,"avatar":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","nickname":"peter","note":"","ucode":"261C3FC001DE2D","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":392670,"discussion_content":"之后运营同学会帮忙拉个学习群~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631093711,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392562,"ip_address":""},"score":392670,"extra":""}]},{"author":{"id":1058183,"avatar":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","nickname":"peter","note":"","ucode":"261C3FC001DE2D","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":392557,"discussion_content":"吴老师：您的课很好，点赞！！！请教一个关于环境的问题：以前学hadoop的时候，需要很多台机器。我是在自己的笔记本电脑上搭建虚拟机（linux），印象中环境搭建比较麻烦。 请问：本课程后续内容，是否会要求搭建多台虚拟机？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631060131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1058183,"avatar":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","nickname":"peter","note":"","ucode":"261C3FC001DE2D","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":392672,"discussion_content":"不需要的~ 考虑到大家有可能不方便搭建分布式环境，所以咱们的项目在选型的时候，就只选择那些在单机环境就可以跑的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631093819,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":392557,"ip_address":""},"score":392672,"extra":""}]}]}]}