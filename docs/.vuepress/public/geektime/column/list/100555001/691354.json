{"id":691354,"title":"19｜ 实战项目（三）：动手做一个自己的LoRA模型","content":"<p>你好，我是南柯。</p><p>上一讲我们已经学习了LoRA的算法原理，搞懂了引入LoRA技术减少可学习参数的技巧。</p><p>如今LoRA几乎家喻户晓，我们在Civitai或者Hugging Face上，也能找到各种各样的LoRA模型。这些LoRA模型既可以代表人物形象、动物形象或者某个特定物体，也可以代表水彩风、油画风这种特定的风格。</p><p>这一讲我们不妨自己动手，从零开始训练自己的LoRA模型。我们会以宝可梦生成和彩铅风格生成为例，完成两个模型的训练，借此探索LoRA模型表达内容和表达风格的能力如何实现。</p><h2>如何训练一个LoRA</h2><p>在我们动手训练LoRA前，我先为你预告一下整个流程。</p><p>对于LoRA的训练，我们首先需要考虑两个问题：数据集获取和基础模型选择。幸运的是，我们已经熟悉了 <a href=\"https://huggingface.co/models\">Hugging Face</a> 和 <a href=\"https://civitai.com/\">Civitai</a> 这两个强大的开源社区，可以免费获取到海量数据集和基础模型。</p><h3>数据准备</h3><p>我们可以使用 <a href=\"https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions\">Hugging Face</a> 上现有的数据集，完成宝可梦的生成任务。这个数据集中包含800多张训练图片。从后面的数据集说明中你可以看到，每一张图，我们都可以获取到它对应的prompt。</p><p><img src=\"https://static001.geekbang.org/resource/image/85/f3/857851382cf9e59969c4dd85a8ab55f3.png?wh=1568x774\" alt=\"\"></p><p>首先，我们可以通过后面这两行代码下载并加载数据集。</p><pre><code class=\"language-python\">from datasets import load_dataset\n\ndataset = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train\")\n</code></pre><!-- [[[read_end]]] --><p>接着，我们便可以通过后面这几行代码，可视化数据集中的图片和对应prompt。</p><pre><code class=\"language-python\">from PIL import Image \n\nwidth, height = 360, 360\nnew_image = Image.new('RGB', (2*width, 2*height))\n\nnew_image.paste(dataset[0][\"image\"].resize((width, height)), (0, 0))\nnew_image.paste(dataset[1][\"image\"].resize((width, height)), (width, 0))\nnew_image.paste(dataset[2][\"image\"].resize((width, height)), (0, height))\nnew_image.paste(dataset[3][\"image\"].resize((width, height)), (width, height))\n\nfor idx in range(4):\n  print(dataset[idx][\"text\"])\n\ndisplay(new_image)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/a5/39/a55e4011d73560e40fe87d9c743b3c39.png?wh=1102x1012\" alt=\"\"></p><p>当然，你也可以使用自己手中的图片做一个原创LoRA。如果你是一个插画师，那么你可以用自己曾经的作品，来训练一个贴近你自己风格的专属LoRA模型，帮助你进行创作。</p><p>SD模型的微调需要同时使用图片和prompt。<strong>如果我们手中的图片没有prompt，那么还需要使用一些方法为图片生成prompt</strong>，这里我们选择使用名为 <a href=\"https://huggingface.co/docs/transformers/model_doc/blip\">BLIP</a> 的模型完成这个任务。</p><p>提到BLIP这个名字，你难免会联想到我们已经学过的CLIP。虽然名字差不多，但它们还是不一样的。你可以这样来区分记忆，CLIP模型提取图像和文本表征，用于跨模态理解任务。而BLIP从图像生成prompt，用于跨模态生成任务。</p><p>我以Hugging Face上的<a href=\"https://huggingface.co/datasets/litmonster0521/pencildrawing\">彩铅风格数据</a>为例，说明一下怎么用BLIP为每一张彩铅图片生成prompt。后面的图展示的就是这批彩铅图像的样例。</p><p><img src=\"https://static001.geekbang.org/resource/image/84/c1/847yy9251b9597355bb1fb5a33eb97c1.jpg?wh=4409x2480\" alt=\"\"></p><p>我们要在彩铅模型训练的Colab中，运行make_captions.py这个脚本，并指定原始图片的路径。这样脚本就会自动下载好BLIP模型，并针对提供的每张图依次进行模型推理生成prompt。你可以点开图像查看prompt的生成效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/61/57/618abbae5b55cca44b99d45f0074f857.png?wh=1587x549\" alt=\"\"></p><p>到此为止，我们已经完成了本次实战课两个数据集的准备工作。你可以根据自己想要完成的LoRA训练任务，参考上面的过程准备训练数据。</p><h3>基础模型选择</h3><p>搞定了训练数据，我们再来看怎么选基础模型。想要训练出理想的LoRA效果，<strong>选择一个与训练目标风格接近的基础模型，会大大降低训练难度</strong>。</p><p>比如说我们要训练某个二次元形象的LoRA模型，选择擅长动漫生成的Anything系列模型，相比于选择擅长写实人像风格生成的Chilloutmix模型而言，就是更好的选择。</p><p>这一讲我们的目标是宝可梦和彩铅风格这两个任务，我们可以选择 <a href=\"https://civitai.com/models/9409\">Anything V5模型</a>作为基础模型。你可以在 <a href=\"https://civitai.com/models/9409\">Civitai</a> 中找到这个模型的权重，按照下面图中展示的方法把模型下载到本地，或者右键复制链接地址。</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/f8/a6829343f7fc27bcbd19bbd0e297e6f8.jpeg?wh=2245x1075\" alt=\"\"></p><p>我们运行后面这条指令便可以完成基础模型的下载。</p><pre><code class=\"language-bash\"># -O 用于制定文件的存储路径\n!wget -c https://civitai.com/api/download/models/90854 -O anything_v5.safetensors\n</code></pre><p>你也可以在Civitai或者Hugging Face中找到其他模型的下载路径，替换上面脚本中的下载链接即可完成基础模型下载的任务。比如，如果你想下载 <a href=\"https://huggingface.co/Linaqruf/stolen/tree/main/pruned-models\">ChilloutMix模型</a>，就可以使用后面这行指令。</p><pre><code class=\"language-bash\"># -O 用于制定文件的存储路径\n!wget -c https://huggingface.co/Linaqruf/stolen/resolve/main/pruned-models/chillout_mix-pruned.safetensors -O chillout_mix-pruned.safetensors\n</code></pre><h3>LoRA训练过程</h3><p>准备好数据和基础模型之后，我们再来看一下训练LoRA模型的核心代码逻辑。我们在<a href=\"https://time.geekbang.org/column/article/685751\">第12讲</a>已经学过如何微调一个Stable Diffusion模型，这里我们来回顾一下SD训练的核心代码。</p><pre><code class=\"language-python\"> for epoch in range(num_train_epochs):\n     for step, batch in enumerate(train_dataloader):\n     \n         # VAE模块将图像编码到潜在空间\n         latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n         \n         # 随机噪声 &amp; 加噪到第t步\n         noise = torch.randn_like(latents)\n         timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps)\n         noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n         \n         # 使用CLIP将文本描述作为输入\n         encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n         target = noise\n         \n         # 预测噪声并计算loss\n         model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n         loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n         optimizer.step()\n</code></pre><p>其实这也正是训练一个LoRA模型的核心代码。这里该怎么理解呢？</p><p>我带你回顾一下SD模型的几个关键模块：VAE、CLIP文本编码器、UNet，通常只有UNet的权重是需要更新的。在使用LoRA微调SD的过程中，LoRA模型影响的是UNet中注意力模块的投影层权重，也就是后面示例代码中的W_Q、W_K和W_V。</p><pre><code class=\"language-python\"># 从同一个输入序列产生Q、K和V向量。\nQ = X * W_Q\nK = X * W_K\nV = X * W_V\n\n# 计算Q和K向量之间的点积，得到注意力分数。\nScaled_Dot_Product = (Q * K^T) / sqrt(d_k)\n\n# 应用Softmax函数对注意力分数进行归一化处理，获得注意力权重。\nAttention_Weights = Softmax(Scaled_Dot_Product)\n\n# 将注意力权重与V向量相乘，得到输出向量。\nOutput = Attention_Weights * V\n</code></pre><p>我们以UNet模型中某一层的某一个交叉注意力模块投影矩阵为例来看看。我们已经知道，prompt的文本表征通过交叉注意力模块完成信息注入，用于计算得到对应的K、V向量，而Q向量源自带噪声的图像潜在表示。</p><p>下面图片展示的就是这个过程，红框中的部分就是我们要训练的LoRA模型权重。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/b2/46194ef5e13f9533e320c7fd88306fb2.jpg?wh=4409x2480\" alt=\"\"></p><p>W原始投影矩阵的权重，维度是dxd。根据我们预先设置矩阵的秩r，我们可以得到随机初始化的权重矩阵A和权重矩阵B，把它们作为要训练的LoRA模型，维度分别是dxr和rxd。</p><p>在训练过程中，W保持固定，要优化的部分是矩阵A和矩阵B。如果X作为输入，Y作为模型输出，使用LoRA的情况下，计算过程大致是后面这个公式。</p><p>$$Y = （W + A\\cdot B）\\cdot X$$</p><p>在UNet模型中，有几十处这样的注意力模块投影矩阵，我们需要逐一优化对应数量的权重矩阵A和权重矩阵B。当LoRA模型训练完成后，我们只需要保存这里的几十处LoRA权重即可，这些权重参数一般只占用几十M的存储空间。</p><p>如果前面的公式推理你暂时没法理解也不要紧，你只需要记住：<strong>训练LoRA的过程仍旧是更新UNet模块，只不过代码中注意力模块的投影层权重会保持不变，更新的是对应的LoRA模型权重</strong>。</p><h3>LoRA权重作用</h3><p>了解了LoRA的训练，我们再来看看LoRA模型使用时候的技巧。你也许还记得，在WebUI中，我们会给LoRA模型设置一个权重值，比如0.7。这个权重值会直接决定LoRA模型发挥作用的强弱，你可以参考后面截图，红框里就是这个参数的位置。</p><p><img src=\"https://static001.geekbang.org/resource/image/6e/9a/6ec86c8d7e8d68f38d4d534946yye09a.png?wh=1134x449\" alt=\"\"></p><p>那么，这个参数是如何起作用的呢？我们来看下面的公式。公式中的weight就是LoRA与基础模型组合时的权重（比如前面WebUI截图红框中的1），A和B代表的是LoRA模型的权重参数。</p><p>$$Y = （W + weight * A\\cdot B）\\cdot X$$</p><p>如果我们同时使用多个模型，本质上就是下面这种计算方式。</p><p>$$Y = （W + weight_{1} * A_{1}\\cdot B_{1} + weight_{2} * A_{2}\\cdot B_{2} + weight_{n} * A_{n}\\cdot B_{n}）\\cdot X$$</p><p>这样一来，你是否就了解了多个LoRA组合的算法原理了呢？比方说，三个LoRA同时用，就相当于模型要听“三个上司”的话，每个上司都会影响输出结果Y，很可能这些影响会相互干扰。</p><p>没错，我们实际操作时，如果发现多个LoRA混用生成的图像效果并不好，其实是因为各个LoRA模型的权重值都被加到了基础模型上，导致最终AI绘画模型参数有点“四不像”。</p><h2>代码实战</h2><p>搞懂了上面的知识，我们这就来来实战演练一下，开始LoRA训练任务。GitHub上有不少LoRA训练的代码仓，比如 <a href=\"https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py\">diffusers 的 LoRA 训练</a>代码量较少，阅读起来压力小，适合初学者作为参考。</p><h3>一次完整的训练</h3><p>使用这个代码仓的训练也非常简单，但需要你拥有独立的GPU环境。没有独立GPU的同学也不用担心，我后面会讲解怎么用Colab完成训练。</p><p>首先，你需要先在你的命令行环境下，登录Hugging Face账号，保证你的代码能够访问到Hugging Face服务器上的数据和基础模型。</p><pre><code class=\"language-bash\">huggingface-cli login \n# 密码在你的Hugging Face账号Setting页面获取 \n</code></pre><p>然后你只需要将上面的<a href=\"https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py\">训练代码</a>拷贝到你的机器上，然后创建一个run.sh文件，写下后面的启动指令。</p><pre><code class=\"language-bash\">export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\" \n\naccelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n  --resolution=512 --random_flip \\\n  --train_batch_size=1 \\\n  --num_train_epochs=10 --checkpointing_steps=5000 \\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n  --seed=42 \\\n  --output_dir=\"sd-pokemon-model-lora\" \\\n  --validation_prompt=\"cute dragon creature\"\n</code></pre><p>然后我们运行这个启动脚本，便可以完成宝可梦的LoRA模型训练。</p><pre><code class=\"language-bash\">sh run.sh \n</code></pre><p>需要注意，上面启动脚本中的基础模型是SD1.4，你可以在Hugging Face中获取其他基础模型的model_id进行替换。比如可以通过一行代码，把基础模型替换为Anything V5模型。</p><pre><code class=\"language-bash\">export MODEL_NAME= \"stablediffusionapi/anything-v5\"\n</code></pre><p>耐心等待20分钟（这里使用的是A100资源+预先下载的模型，不同GPU会有差异），我们就完成了LoRA模型的训练。</p><p>接着，我们不妨使用我们训练好LoRA生成图片，看看效果如何。你可以参考后面的代码完成这一步。第七行代码的prompt你可以按自己的想法灵活更换。</p><pre><code class=\"language-python\">from diffusers import StableDiffusionPipeline\nimport torch\nmodel_path = \"你的LoRA路径/sd-model-finetuned-lora-t4\"\npipe = StableDiffusionPipeline.from_pretrained(\"stablediffusionapi/anything-v5\", torch_dtype=torch.float16)\npipe.unet.load_attn_procs(model_path)\npipe.to(\"cuda\")\nprompt = \"A pokemon with green eyes and red legs.\"\n# prompt = \"Girl with a pearl earring\"\nimage = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\nimage.save(\"pokemon.png\")\n</code></pre><p>现在，你可以点开图片查看我们LoRA模型的生成效果，可以看到，我们的LoRA模型学到了宝可梦风格的“精髓之处”，图片的配色和线条都和宝可梦风格相似。</p><p><img src=\"https://static001.geekbang.org/resource/image/37/b5/374d8835203aee394ecb3b1fab3a6eb5.jpg?wh=4409x2480\" alt=\"\"></p><h3>推荐一个Colab</h3><p>除了diffusers官方的LoRA实现，GitHub上有一些效果更好的LoRA实现。这里我推荐一个可调参数更多的 <a href=\"https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb\">Colab链接</a>。根据我们这一讲要完成的LoRA任务，我对原始的Colab进行了一些定制化的改造。</p><p>宝可梦风格的LoRA训练任务，你可以点开这个 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson19/LoRA_train_pokemon.ipynb\">Colab链接</a>做练习。</p><p>而彩铅风格的LoRA训练任务，你可以点开这个<a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson19/LoRA_train_pencil_drawing.ipynb\">Colab链接</a>来练习。在彩铅风格的Colab代码中，我们会使用BLIP模型给每一张图片生成prompt。</p><p>为了方便你体验效果，在配置好Colab的GPU环境后，你可以直接点击全部运行，这样就能“一键”完成LoRA的训练。</p><p><img src=\"https://static001.geekbang.org/resource/image/1d/02/1d3b6f340f6e8283412f70d03e571202.png?wh=1885x816\" alt=\"\"></p><p>耐心等待LoRA训练完成后，我们便可以看到LoRA模型的生成效果。以宝可梦的效果为例，能直观的感受到，在相同的测试prompt下，我们Colab的生成效果要优于使用diffusers代码仓的训练效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/d4/3f0ebdbbd66a924421705c8fbd9d6fd4.jpg?wh=4409x2480\" alt=\"\"></p><p>这里我们用到的prompt信息如下。</p><pre><code class=\"language-plain\">Prompt：A pokemon with green eyes and red legs\nPrompt：Girl with a pearl earring\nNegative Prompt：lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n采样器：Eular a\n随机种子：1025\n采样步数：20\n分辨率：512x512\nCFG Scale: 7\nLoRA：pokemon.safetensors [使用Colab训练]\nLoRA weight：1.0\n</code></pre><p>我们可以再感受下彩铅LoRA的效果。从后面两张的图片可以看出，我们的LoRA模型学到了一种 “2D感” 的铅笔画风格。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/60/f3131b2dc6a6b1c3c7b3134f3991e260.jpg?wh=4409x2480\" alt=\"\"></p><p>这里这里我们用到的prompt信息如下。</p><pre><code class=\"language-plain\">Prompt：A drawing of a beautiful girl\nPrompt：Girl with a pearl earring\nNegative Prompt：lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n采样器：Eular a\n随机种子：1025\n采样步数：20\n分辨率：512x512\nCFG Scale: 7\nLoRA：pencil.safetensors [使用Colab训练]\nLoRA weight：1.0\n</code></pre><p>以彩铅风格为例，我在Colab代码中为你增加了一些注释，来辅助你理解每个模块都负责做哪些事情，建议你课后阅读一下。如果你需要更换自己的训练数据，只需要替换掉代码中的训练图像拷贝部分即可。</p><pre><code class=\"language-python\"># 将我们的训练数据拷贝到训练路径下\n# 如果你需要使用自己准备的图片，需要将你的数据拷贝到\n# /content/LoRA/train_data/custom_data路径下\nos.system(\"cp -r /content/caiqian_style/* /content/LoRA/train_data/custom_data\")\n</code></pre><h2>配合WebUI使用</h2><p>搞定了LoRA模型的训练，咱们再把它加入到WebUI上试试效果。</p><p>我们可以将训练得到的LoRA模型下载到本地，放在WebUI的LoRA文件夹中，然后就可以在WebUI直接使用我们刚刚训练的LoRA模型了。</p><pre><code class=\"language-bash\"># LoRA模型放置路径为：\n/你的WebUI安装路径/extensions/sd-webui-additional-networks/models/lora\n</code></pre><p>把模型放到相应位置后，别忘了刷新WebUI的LoRA模型库，加载我们刚刚放置的LoRA模型。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/c3/c9c9c9b8d82f633ac54c2fabdd9097c3.png?wh=1143x569\" alt=\"\"></p><p>一切准备完毕，我们这就来测试下WebUI的使用效果，可以使用下面这组prompt来测试宝可梦模型。</p><pre><code class=\"language-plain\">Prompt：A pokemon with red eyes, big ears\nNegative Prompt：lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n采样器：Eular a\n随机种子：1025\n采样步数：20\n分辨率：512x512\nCFG Scale: 7\nLoRA：pokemon.safetensors [使用Colab训练]\nLoRA weight：1.0\n</code></pre><p>后面的图片就是对应的生成结果，一只有点模糊的“宝可梦”呈现在我们眼前。</p><p><img src=\"https://static001.geekbang.org/resource/image/f7/9f/f7c79f2cb07e53a251c04fda7444979f.png?wh=512x512\" alt=\"\"></p><p>效果还可以，但清晰度不够。这时我们可以利用WebUI的超分模块，得到更高清的图片。你可以点开图片查看超分后的宝可梦精灵效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/bf/7c/bfa48fa38c01dd11b05b508d74185d7c.png?wh=1142x716\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/8b/d6/8b3d5ee4f8735e49b94d1e6c5ce090d6.png?wh=1024x1024\" alt=\"\" title=\"重绘强度 0.3，超分后图像效果\"></p><p>到此为止，我们已经走通了准备训练数据、图像prompt生成、基础模型选择、LoRA训练、LoRA本地使用、超分功能修复生成效果的完整流程。</p><p>关于LoRA模型，我们还可以做两个有意思的效果测试。第一个测试是将我们得到的两个LoRA模型组合使用。你可以在WebUI中按照图中的方式进行操作。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/a6/f861677398de0ae32ea4082a045684a6.png?wh=1125x557\" alt=\"\"></p><p>我们使用后面的prompt信息进行测试。</p><pre><code class=\"language-plain\">prompt: A pokemon with red eyes, big ears\nprompt: a drawing of a beautiful girl\nNegative Prompt：lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n采样器：Eular a\n随机种子：1025\n采样步数：20\n分辨率：512x512\nCFG Scale: 7\n</code></pre><p>得到的效果是下面这个样子。可以看到，左面的宝可梦同时带上了彩铅的风格，右面的女孩子带上了宝可梦的“画风”。通过不同的LoRA权重配比，我们就能“调制出”我们心仪的风格。</p><p><img src=\"https://static001.geekbang.org/resource/image/ee/fe/eebcf4019d52b49491fec5e8e19a2afe.jpg?wh=4409x2480\" alt=\"\"></p><p>第二个测试是将LoRA与其他基础模型搭配使用。比如我们训练宝可梦LoRA和彩铅风格LoRA的基础模型是Anything V5，我们可以试试将这两个LoRA与Realistic V3.0模型搭配使用。你可以点开图片查看结果，可以看到，生成效果并不符合我们预期的风格。这个测试说明，<strong>LoRA模型与其他基础模型搭配使用时需要谨慎处理</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/0a/87/0a8f12a8805821165327030c94894887.jpg?wh=4409x2480\" alt=\"\"></p><p>其实我们常用的各种基础模型，通常都是基于各种SD模型微调来的，有的模型甚至是基于微调后的模型再次微调。因此，我们可以将这些基础模型想象成一个大家族，不同模型之间存在一定的亲缘关系。也正是因为这个原因，我们训练得到的LoRA模型，和亲缘关系近的基础模型组合往往更容易实现预期的效果。</p><h2>总结时刻</h2><p>今天我们通过实战的形式加深了对LoRA模型的认识。我们从零到一，完成了宝可梦LoRA和彩铅LoRA两个模型的训练。</p><p>训练LoRA有两个基本前提：图文数据和基础模型。对于图文数据，我们可以从Hugging Face上直接获取，也可以用BLIP模型对我们自己的数据进行prompt打标。而基础模型，我们可以从Hugging Face或者Civitai上按需选择。</p><p>之后我们分析了在SD模型中LoRA权重的作用位置，也就是UNet模型的注意力模块，并进一步了解了LoRA权重的作用机理。然后，我们分别使用diffusers官方LoRA代码仓和一个改造后的Colab，完成了我们的LoRA训练任务。这部分的重点是理解Colab中的LoRA代码实现。</p><p>最后，我们将训练得到的LoRA模型导入WebUI，完成了图像生成、图像超分、多LoRA组合、更换基础模型测试等任务。建议你课后自己多练习，也可以将自己训练的LoRA模型发布到开源社区，供其他朋友体验和分享，这样学习效果会更好。</p><p>这一讲的重点，你可以点开下面的导图复习回顾。</p><p><img src=\"https://static001.geekbang.org/resource/image/56/37/566185db89f38848d1a04945d26e8637.jpg?wh=3900x1720\" alt=\"\"></p><h2>思考题</h2><p>选择你一个你喜欢的物体或者一种你喜欢的风格，使用diffusers代码仓或者我们的Colab链接，完成你自己的LoRA模型训练。</p><p>期待你在留言区和我交流讨论，也推荐你把今天的内容分享给身边更多朋友，和他一起尝试训练LoRA模型。</p>","comments":[{"had_liked":false,"id":380383,"user_name":"Geek_7401d2","can_delete":false,"product_type":"c1","uid":1464868,"ip_address":"北京","ucode":"05703276C5046B","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKuaZauv0hcyH9e51azzYTt2rFQPia1ryfupuAVYYeDaicp1ictV7dciarbAXUb2bz2x0qu9x6tL4VVhA/132","comment_is_top":false,"comment_ctime":1693466158,"is_pvip":false,"replies":[{"id":138563,"content":"你好。针对问题1，以人物LoRA为例，我的经验是以半身照为主，可以混合一些全身照和面部特写，不用混合很多。图片的数量越多越好，推荐不少于10张。针对问题2，训练步数可以根据效果来定，一般而言，300-500步效果会比较好，训练步数过多模型会出现过拟合，多样性能力会变差。针对问题3，咱们代码中有可视化看每一轮次的训练效果，可以挑选一个效果好的模型。一般来说，选择300-500步训练后的模型，不要选择步数太少的模型。针对问题4，diffusers的代码中可调整的参数非常少，数据增广较少、text_encoder学习率等指标设置也不够灵活等。对于开发者而言，可以以diffusers代码仓为基础，逐渐把这些特性加进去，模型的效果会有提升。希望能帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693549108,"ip_address":"北京","comment_id":380383,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师你好，看完后还是不知道如何入手训练，有几个问题\n1、选择素材图片时要用多少张，什么样的图片合适，比方说训练某个人物的Lora时，选择该人的图片时要选择什么样的，全身照、半身照、面部特写等各占多少合适\n2、Lora 模型训练多少轮（num_epochs）合适\n3、训练完会有多个Lora模型，选择哪一个呢，选最后一轮训练的吗\n4、我理解训练lora模型的原理是一样的，为什么同样的素材、用同样的基础模型，用不同的代码会出现不同的训练效果，文中用到的这两个代码库差异在哪呢","like_count":5,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626972,"discussion_content":"你好。针对问题1，以人物LoRA为例，我的经验是以半身照为主，可以混合一些全身照和面部特写，不用混合很多。图片的数量越多越好，推荐不少于10张。针对问题2，训练步数可以根据效果来定，一般而言，300-500步效果会比较好，训练步数过多模型会出现过拟合，多样性能力会变差。针对问题3，咱们代码中有可视化看每一轮次的训练效果，可以挑选一个效果好的模型。一般来说，选择300-500步训练后的模型，不要选择步数太少的模型。针对问题4，diffusers的代码中可调整的参数非常少，数据增广较少、text_encoder学习率等指标设置也不够灵活等。对于开发者而言，可以以diffusers代码仓为基础，逐渐把这些特性加进去，模型的效果会有提升。希望能帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693549108,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381211,"user_name":"cmsgoogle","can_delete":false,"product_type":"c1","uid":1746343,"ip_address":"美国","ucode":"CD69D796709195","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLhs7ykGEy46a8ljg3LPvXTRxpgMLEhvZoAYIQL6I46OEqbNV4U1mXryhJt1bE3mhf7ey6jfl3IyQ/132","comment_is_top":false,"comment_ctime":1694823013,"is_pvip":false,"replies":[{"id":138993,"content":"你好，感谢你的反馈。训练用时因计算资源而异，如果需要下载模型还会受网速影响，应该加一些说明。我们会根据反馈进行调整。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1695566301,"ip_address":"北京","comment_id":381211,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"使用diffusers库训练Lora，文中提到：耐心等待 20 分钟，我们就完成了 LoRA 模型的训练。\n需要说明下是什么环境，如果再colab上使用T4服务器，大约要1个小时10多分钟。","like_count":1,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628592,"discussion_content":"你好，感谢你的反馈。训练用时因计算资源而异，如果需要下载模型还会受网速影响，应该加一些说明。我们会根据反馈进行调整。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695566302,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381008,"user_name":"王大叶","can_delete":false,"product_type":"c1","uid":1217905,"ip_address":"北京","ucode":"AA4BA0EF1D8AC0","user_header":"https://static001.geekbang.org/account/avatar/00/12/95/71/20b96bc8.jpg","comment_is_top":false,"comment_ctime":1694581388,"is_pvip":false,"replies":[{"id":139005,"content":"你好。针对第一个问题，我的看法是精细化打标不是很有必要，只需要保证发型、眼镜、服饰颜色这些最基本的信息被涵盖即可，很多时候训练人像LoRA的Prompt甚至是“A photo of a &lt;sks&gt; man”这种粗糙的描述。针对第二个问题，把男性图片标注成 1girl是由于DeepDanbooru的分类精度不够导致的，Midjourney有一个描述功能，写出的Prompt比较准确、信息也比较丰富，可以试试。也可以试试Qwen-VL这种多模态大模型的Caption效果，应该也不错。希望能帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1695568814,"ip_address":"北京","comment_id":381008,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师好，请教两个问题：\n1. 对于人像 LoRA 的训练，精细化的打标是否有必要，对 LoRA 质量的影响会很大吗？\n2. 实验发现用 deepbooru 给写实人像打标不是很准确，比如经常会把男性图片标注成 1girl，用 BLIP 打标信息又比较少，无法完全涵盖画面的内容。请教人像 LoRA 训练有什么推荐的打标方法吗？","like_count":1,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628605,"discussion_content":"你好。针对第一个问题，我的看法是精细化打标不是很有必要，只需要保证发型、眼镜、服饰颜色这些最基本的信息被涵盖即可，很多时候训练人像LoRA的Prompt甚至是“A photo of a &lt;sks&gt; man”这种粗糙的描述。针对第二个问题，把男性图片标注成 1girl是由于DeepDanbooru的分类精度不够导致的，Midjourney有一个描述功能，写出的Prompt比较准确、信息也比较丰富，可以试试。也可以试试Qwen-VL这种多模态大模型的Caption效果，应该也不错。希望能帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695568814,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380373,"user_name":"易企秀-郭彦超","can_delete":false,"product_type":"c1","uid":2343516,"ip_address":"北京","ucode":"2E25574FAB1B3B","user_header":"https://static001.geekbang.org/account/avatar/00/23/c2/5c/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1693451713,"is_pvip":false,"replies":[{"id":138564,"content":"你好。这是个很有意思的想法，更直观的，我们可以训练两个LoRA，每个LoRA影响不同的UNet模型Attention模块，这样融合的时候便不会在权重上产生叠加效应。但有一个问题，使用这种方式，即使我们能够将不同的LoRA权重解耦开，最终得到的效果大概率还是两个LoRA的中间态，因为这些LoRA都对图像生成的过程都产生了影响，这些影响是叠加还是冲突无法预测。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693549406,"ip_address":"北京","comment_id":380373,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"Y=（W+weight1​∗A1​⋅B1​+weight2​∗A2​⋅B2​）⋅X\n老师你好，按照上面公式权重融合的过程是加权融合，W的值在相同维度上会同时受到A和B的影响，最终导致结果既不像A也不像B, 有没有一种累计方式 避免A和B的互相影响？ 比如多lora融合前先merge， 根据CNN的思想， 取d&#47;2的A模型参数量 与d&#47;2的B模型参数量 合并成新的d*d lora模型，新的模型保留了原始的A 和 B的部分参数 并没有累加A和B\n","like_count":1,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626973,"discussion_content":"你好。这是个很有意思的想法，更直观的，我们可以训练两个LoRA，每个LoRA影响不同的UNet模型Attention模块，这样融合的时候便不会在权重上产生叠加效应。但有一个问题，使用这种方式，即使我们能够将不同的LoRA权重解耦开，最终得到的效果大概率还是两个LoRA的中间态，因为这些LoRA都对图像生成的过程都产生了影响，这些影响是叠加还是冲突无法预测。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693549406,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2343516,"avatar":"https://static001.geekbang.org/account/avatar/00/23/c2/5c/791d0f5e.jpg","nickname":"易企秀-郭彦超","note":"","ucode":"2E25574FAB1B3B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626983,"discussion_content":"两个lora模型各取一半 横向拼接 或者 交叉拼接呢，","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693556391,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381098,"user_name":"Geek_ca0b19","can_delete":false,"product_type":"c1","uid":3712957,"ip_address":"广东","ucode":"022E56BDCD5DA5","user_header":"","comment_is_top":false,"comment_ctime":1694673860,"is_pvip":false,"replies":[{"id":139000,"content":"你好！使用5-6张图是可以训练出一个风格化LoRA模型的，需要保证这些图片的风格一致。我们可以在MidLibrary上获取很多风格图片：https:&#47;&#47;midlibrary.io&#47;styles。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1695567998,"ip_address":"北京","comment_id":381098,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师好我有一个问题\n如果采用五六张类似风格画风的图，可以通过这几张图训练出一个代表类似风格的lora吗？","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628599,"discussion_content":"你好！使用5-6张图是可以训练出一个风格化LoRA模型的，需要保证这些图片的风格一致。我们可以在MidLibrary上获取很多风格图片：https://midlibrary.io/styles。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1695567998,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381086,"user_name":"石沉溪洞","can_delete":false,"product_type":"c1","uid":2444830,"ip_address":"广东","ucode":"CCB5E014C2C92C","user_header":"https://static001.geekbang.org/account/avatar/00/25/4e/1e/137ad3cf.jpg","comment_is_top":false,"comment_ctime":1694661558,"is_pvip":false,"replies":[{"id":139001,"content":"你好，将基础模型更换为SDXL需要在代码中做出一定改动，可以参考这个Colab：https:&#47;&#47;github.com&#47;Linaqruf&#47;kohya-trainer&#47;blob&#47;main&#47;kohya-LoRA-trainer-XL.ipynb","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1695568126,"ip_address":"北京","comment_id":381086,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师您好，请问这个能支持将基本模型改为SDXL吗？谢谢您","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628600,"discussion_content":"你好，将基础模型更换为SDXL需要在代码中做出一定改动，可以参考这个Colab：https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695568126,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380640,"user_name":"@二十一大叔","can_delete":false,"product_type":"c1","uid":1075954,"ip_address":"上海","ucode":"394A7E80C5034C","user_header":"https://static001.geekbang.org/account/avatar/00/10/6a/f2/8829a0b8.jpg","comment_is_top":false,"comment_ctime":1693921009,"is_pvip":false,"replies":[{"id":138661,"content":"你好。Colab的代码在加餐篇中我们计划继续深扒。当前如果想理解lora的训练细节，直接看这个脚本就可以：https:&#47;&#47;github.com&#47;huggingface&#47;diffusers&#47;blob&#47;main&#47;examples&#47;text_to_image&#47;train_text_to_image_lora.py。我们用的Colab在这份代码的基础上，增加了BLIP、数据增广的功能，底层逻辑是一样的。希望能帮助到你，欢迎多交流讨论。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694267243,"ip_address":"北京","comment_id":380640,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师，lora训练可以写一个本地运行的python版本吗，colab上看的不是很明白","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627551,"discussion_content":"你好。Colab的代码在加餐篇中我们计划继续深扒。当前如果想理解lora的训练细节，直接看这个脚本就可以：https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py。我们用的Colab在这份代码的基础上，增加了BLIP、数据增广的功能，底层逻辑是一样的。希望能帮助到你，欢迎多交流讨论。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694267244,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380627,"user_name":"陈问渔","can_delete":false,"product_type":"c1","uid":1194166,"ip_address":"上海","ucode":"6838D015BB9CBB","user_header":"https://static001.geekbang.org/account/avatar/00/12/38/b6/234a17a7.jpg","comment_is_top":false,"comment_ctime":1693906446,"is_pvip":false,"replies":[{"id":138662,"content":"你好，make_captions这个脚本可以看这个链接：https:&#47;&#47;github.com&#47;Linaqruf&#47;kohya-trainer&#47;blob&#47;main&#47;finetune&#47;make_captions.py。这个脚本完成两件事情：下载BLIP模型，为每张图生成prompt。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694267316,"ip_address":"北京","comment_id":380627,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"请问 make_captions.py 的代码在哪看呀？","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627552,"discussion_content":"你好，make_captions这个脚本可以看这个链接：https://github.com/Linaqruf/kohya-trainer/blob/main/finetune/make_captions.py。这个脚本完成两件事情：下载BLIP模型，为每张图生成prompt。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1694267316,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1541555,"avatar":"https://static001.geekbang.org/account/avatar/00/17/85/b3/5f40f485.jpg","nickname":"恨％心~","note":"","ucode":"EAB4B7E3F311C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627334,"discussion_content":"这里https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson19/LoRA_train_pencil_drawing.ipynb","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693998888,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380516,"user_name":"Alex","can_delete":false,"product_type":"c1","uid":1352753,"ip_address":"江苏","ucode":"1770CA7050647A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI8qibAw4lRCic1pbnA6yzQU3UqtQm3NqV1bUJ5EiaUnJ24V1yf4rtY7n2Wx7ZVvTemqq5a61ERWrrHA/132","comment_is_top":false,"comment_ctime":1693746551,"is_pvip":false,"replies":[{"id":138674,"content":"你好，additional network这个插件需要单独安装，可以按照这个说明来安装：https:&#47;&#47;github.com&#47;kohya-ss&#47;sd-webui-additional-networks。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694269658,"ip_address":"北京","comment_id":380516,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"最新的sd 的bweui 的additional networks 是已经集成了么？我不太确定 我安装了一下 没有显示这个选项  请老师指教下 ","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627567,"discussion_content":"你好，additional network这个插件需要单独安装，可以按照这个说明来安装：https://github.com/kohya-ss/sd-webui-additional-networks。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694269658,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380380,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1693463358,"is_pvip":false,"replies":[{"id":138560,"content":"你好，应用商店里有不少AI绘画APP，可以检索一下。当前模型推理需要较多算力，所以主要还是通过调用各种云服务来完成绘画过程。希望能帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693541272,"ip_address":"北京","comment_id":380380,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"是否有安卓手机上可以使用绘画AI?","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626968,"discussion_content":"你好，应用商店里有不少AI绘画APP，可以检索一下。当前模型推理需要较多算力，所以主要还是通过调用各种云服务来完成绘画过程。希望能帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693541272,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386501,"user_name":"zec123","can_delete":false,"product_type":"c1","uid":3735275,"ip_address":"广东","ucode":"B3645BEDD0C6D2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI64XUGp3cKv99kb0dB3TQgod7yeCDEKe94QVP5rSv4yzR24CcE6UqD6wxjMo2z9CRn06S7fvrqMQ/132","comment_is_top":false,"comment_ctime":1704967378,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"老师，24年1月11日运行colab彩铅代码，在def train代码块会发成报错导致运行不了报错如下：RuntimeError: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more \ninformation:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need \nto add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m \nbitsandbytes\n        and open an issue at: https:&#47;&#47;github.com&#47;TimDettmers&#47;bitsandbytes&#47;issues\nCalledProcessError: Command &#39;[&#39;&#47;usr&#47;bin&#47;python3&#39;, &#39;train_network.py&#39;, \n&#39;--sample_prompts=&#47;content&#47;LoRA&#47;config&#47;sample_prompt.txt&#39;, \n&#39;--dataset_config=&#47;content&#47;LoRA&#47;config&#47;dataset_config.toml&#39;, \n&#39;--config_file=&#47;content&#47;LoRA&#47;config&#47;config_file.toml&#39;]&#39; returned non-zero exit status 1.","like_count":0,"discussions":[{"author":{"id":1231787,"avatar":"https://static001.geekbang.org/account/avatar/00/12/cb/ab/1aac53bf.jpg","nickname":"董航","note":"","ucode":"9CA208FD26F849","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":642719,"discussion_content":"解决了吗\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713775631,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"中国香港","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1165347,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c8/23/c43b926e.jpg","nickname":"Baymax","note":"","ucode":"3B05766EED7DE0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1231787,"avatar":"https://static001.geekbang.org/account/avatar/00/12/cb/ab/1aac53bf.jpg","nickname":"董航","note":"","ucode":"9CA208FD26F849","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652299,"discussion_content":"解决了吗？我也卡这里了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1728637067,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":642719,"ip_address":"北京","group_id":0},"score":652299,"extra":""}]}]},{"had_liked":false,"id":383245,"user_name":"ALAN","can_delete":false,"product_type":"c1","uid":1240164,"ip_address":"广东","ucode":"70E3B1C730E63F","user_header":"https://static001.geekbang.org/account/avatar/00/12/ec/64/7403c694.jpg","comment_is_top":false,"comment_ctime":1698742313,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"还有这块代码在哪个文件里，好像也没找到。\n for epoch in range(num_train_epochs):\n     for step, batch in enumerate(train_dataloader):\n     \n         # VAE模块将图像编码到潜在空间\n         latents = vae.encode(batch[&quot;pixel_values&quot;].to(weight_dtype)).latent_dist.sample()\n         \n         # 随机噪声 &amp; 加噪到第t步\n         noise = torch.randn_like(latents)\n         timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps)\n         noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n         \n         # 使用CLIP将文本描述作为输入\n         encoder_hidden_states = text_encoder(batch[&quot;input_ids&quot;])[0]\n         target = noise\n         \n         # 预测噪声并计算loss\n         model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n         loss = F.mse_loss(model_pred.float(), target.float(), reduction=&quot;mean&quot;)\n         optimizer.step()","like_count":0},{"had_liked":false,"id":382797,"user_name":"糖糖丸","can_delete":false,"product_type":"c1","uid":1347921,"ip_address":"浙江","ucode":"79762E88808283","user_header":"https://static001.geekbang.org/account/avatar/00/14/91/51/3da9420d.jpg","comment_is_top":false,"comment_ctime":1698060555,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"文章里的make_captions.py文件，在哪里可以看？","like_count":0},{"had_liked":false,"id":380352,"user_name":"Toni","can_delete":false,"product_type":"c1","uid":3206957,"ip_address":"瑞士","ucode":"E6B2FACCC1E000","user_header":"https://static001.geekbang.org/account/avatar/00/30/ef/2d/757bb0d3.jpg","comment_is_top":false,"comment_ctime":1693407481,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"在LoRA 模型训练时，采用变化的学习率(learning rate)，从较大的学习率开始，逐渐将其减小到较小值，帮助优化器(optimizer)能够较快较好地达到全局或局部最优，以期训练出的模型有更高的质量。\n\n为了比较，采用同样的基础模型，训练集，优化器，但将学习率调度器(Learning Rate Scheduler)改成了Cosine，lr_scheduler = &quot;cosine_with_restarts&quot;，出图质量有明显改进。\n\n还有其它优化模型的方法，大家可以分享。","like_count":0,"discussions":[{"author":{"id":3206957,"avatar":"https://static001.geekbang.org/account/avatar/00/30/ef/2d/757bb0d3.jpg","nickname":"Toni","note":"","ucode":"E6B2FACCC1E000","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626880,"discussion_content":"学习率的初始值为 unet_lr = 1e-4 和 text_encoder_lr = 5e-5，然后使用Consine 函数变化形式使学习率下降，具体参数如下:\n\noptimizer_type = &#34;AdamW8bit&#34;\noptimizer_args = &#34;&#34;\ntrain_unet = True\nunet_lr = 1e-4\ntrain_text_encoder = True\ntext_encoder_lr = 5e-5\nlr_scheduler = &#34;cosine_with_restarts&#34;\nlr_warmup_steps = 0\nlr_scheduler_num_cycles = 10\nlr_scheduler_power = 0","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693464754,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2343516,"avatar":"https://static001.geekbang.org/account/avatar/00/23/c2/5c/791d0f5e.jpg","nickname":"易企秀-郭彦超","note":"","ucode":"2E25574FAB1B3B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626863,"discussion_content":"你好，动态学习率的配置可以发一下吗，初始多少，递减速率多少？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693450953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}