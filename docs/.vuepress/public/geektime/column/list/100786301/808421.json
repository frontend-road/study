{"id":808421,"title":"17｜评估：如何对电商客服模型专有训练进行能力测评？","content":"<p>你好，我是金伟。</p><p>总体而言，大模型测评才是整个项目的核心。基于电商客服对问答准确性的要求，只有发布之前测试充分，才能保证发布后不出现bug。问题是，怎么才能通过测评保证电商客服模型的准确性呢？</p><p>很多人可能会想，直接用程序自动测试？或者用其他能力更强的大模型来测评？<strong>我的<strong><strong>实际经验表明</strong></strong>，<strong><strong>最重要的是人工测评</strong></strong>。</strong></p><p>在真实电商客服项目中，我们采用人工测评+自动化测评结合的方案。</p><p>我们先从测评和训练之间的关系开始说。</p><h2>测评过程</h2><p>首先需要注意的是，为了保证测评效果，测评数据集和训练数据集要完全隔离。真实项目里，测评分为3类测评，分别是人工测评，通用测评和私有task测评。</p><p>测评和训练的关系不是先训练再测评，而是<strong>在模型训练过程中分别评分，随时根据结果调整模型训练过程。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/90/21/9092743f0a6b83dd37a41c69c7aa8c21.png?wh=1920x1117\" alt=\"图片\"></p><p>如果这3个测评非要做一个重要性排序的话，那么人工测评其实是最重要的。人工测评的方法是<strong>将智能客服项目最关心的能力设计为一系列问题，把问题和期望的回答写到Excel表格中，由人工挨个根据微调后的大模型进行评分。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/70/95/70b0abb8e5bf9f6757df43dcb1073595.png?wh=1920x804\" alt=\"图片\"></p><p>需要注意，在智能客服上线之后遇到的新问题也要补充到表格里。</p><p>如果说人工测评是测试大模型微调后有多少新能力，那么<strong>通用测试集测评则是保证大模型基础和通用能力不下降</strong><strong>。</strong>通用测试集一般是公开的，我们只要用测试集对微调后的大模型测评，结果和原有大模型的评分对比即可。</p><!-- [[[read_end]]] --><p>例如下面是Llama系列模型在各个测试集下的通用测评结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/8y/5a/8yy5f27b395yyd211c3349c9b0be1a5a.png?wh=1920x953\" alt=\"图片\"></p><p><strong>最后</strong>，<strong>私有task测评是人工测评的补充。</strong>怎么理解呢？比如我们之前提到的客服项目中的地址信息提取能力，是一个要通过微调大模型提升的能力，这个可以用人工测评也可以写一个自动测评程序测评，写一个自动程序就是私有task，最终结合人工评分和私有task测评评分来说明这项能力。</p><p>那这些得到的不同评分结果如何判断呢？下面我用一个过程描述的例子来说一说。</p><p>比如，在模型训练中，模型训练跑了1万步左右，做一次人类评分，同时做私有task评分，此时人类评分7，私有task评分5分，继续跑到2万步在做一次测评。如果私有task下降到2分，大概率出问题了。如果跑到2万步，私有task增加到6分，则训练正常。</p><p>我把一些经验值整理成流程图，可能更有参考价值。注意，人工测评和私有task结合，通用测试集测评则是独立的。总体标准是，人工评分要增加，通用测试集评分不能下降得太高。</p><p><img src=\"https://static001.geekbang.org/resource/image/35/2c/359e7fb3129c5246ccce6dbf8b36992c.png?wh=1920x1086\" alt=\"图片\"></p><p>真实的人工测评也不是傻傻的靠人工挨个比对结果，有很多提高效率的方法。</p><h2>核心：人工测评</h2><p>人工测评是大模型的测评的核心环节，核心要点是问题集的设计，只列出重点关注的问题集，问题数量不用太大，随时补充生产遇到的重要问题。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/bc/32193390414a97736e6c35932108f4bc.png?wh=1920x1080\" alt=\"图片\"></p><h2>测评流程</h2><p>在工程实践上，可以用Python做一个小工具，把上述问题集写在Excel中，在用Python调用模型接口自动向专有模型进行提问，将回答回填到关注的问题后面一列。</p><pre><code class=\"language-plain\">import csv\n\n\n# 假设你有一个模型接口函数，名称为`get_model_answer`，它接受一个问题并返回回答\ndef get_model_answer(question):\n&nbsp; &nbsp; # 这里调用你的模型接口，比如通过API请求获取回答\n&nbsp; &nbsp; # 这里假设返回一个示例回答\n&nbsp; &nbsp; return \"这是模型生成的示例回答。\"\n\n\n# 读取CSV文件，处理数据\ndef process_csv(input_file, output_file):\n&nbsp; &nbsp; with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n&nbsp; &nbsp; &nbsp; &nbsp; reader = csv.DictReader(infile)\n&nbsp; &nbsp; &nbsp; &nbsp; fieldnames = reader.fieldnames\n&nbsp; &nbsp; &nbsp; &nbsp; writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n\n\n&nbsp; &nbsp; &nbsp; &nbsp; writer.writeheader()\n&nbsp; &nbsp; &nbsp; &nbsp; for row in reader:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 获取问题示例\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; question = row['问题示例']\n\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 调用模型接口获取回答\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; answer = get_model_answer(question)\n\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 将回答填充到对应的列中\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; row['回答'] = answer\n\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 初始评分列为空\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; row['评分'] = ''\n\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 写入新行\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; writer.writerow(row)\n\n\n# 输入和输出CSV文件的路径\ninput_csv = 'input.csv'\noutput_csv = 'output.csv'\n\n\n# 处理CSV文件\nprocess_csv(input_csv, output_csv)\n</code></pre><p>经过程序运行后，得到的表格类似如下格式。<br>\n<img src=\"https://static001.geekbang.org/resource/image/29/9f/29a4e1c32166889e0e52ef6dbb89b09f.png?wh=1920x1137\" alt=\"图片\"><br>\n最后人工检查Excel里的内容，给出评分即可。</p><h3>实测与回调</h3><p>如果测评结果出现异常，比如评分降低，怎么进一步分析呢？</p><p><strong>其实只有两个可能，代码不对或数据不对，导致参数变化不正常。</strong>而大多情况都是数据不对，最关键的前置工作，数字孪生生成的数据要经过人工检查，没错，还是人工检查，这一步不能通过程序。</p><p>例如下面的地址信息提取的数据模版。数字孪生生成的每条数据输入和输出需要检查是否完全匹配，如果错误数据过多，自然会导致微调后的大模型评分下降。</p><p><img src=\"https://static001.geekbang.org/resource/image/2f/96/2f126fe2e6a424fc7002949dd99fea96.png?wh=1920x830\" alt=\"图片\"></p><p>关于训练数据量经验也至关重要。一般微调小的能力，千级数据量就足够了，复杂的能力，比如例子中的地址抽取能力则需要万级别数据量。</p><h2>通用能力评分</h2><p>人工测评有一定的人工成本，通用能力测评主要是算力成本。</p><p>用通用测试集测评可以了解当前训练有没有导致模型严重劣化，如果有问题，需要找出原因从前一次导出的参数重新进行训练。经验表明，如果新开始训练没有导入原版的优化器参数，分数下跌很多是正常的。针对这种情况，你可以参入正常语料进行训练，一段时间之后，就会恢复比较正常的评分。</p><h3>MMLU数据集</h3><p>对模型的通用能力测评，核心就是要设计通用能力的测评问题和答案，然后根据大模型对这些问题的回答进行评分。</p><p>不管具体用哪个数据集进行测评，最基本的问题-答案数据结构都是一样的，类似下面的例子。</p><pre><code class=\"language-plain\">{\n  \"question\": \"What is the capital of France?\",\n  \"options\": {\n    \"A\": \"Berlin\",\n    \"B\": \"Madrid\",\n    \"C\": \"Paris\",\n    \"D\": \"Rome\"\n  },\n  \"correct_answer\": \"C\"\n}\n</code></pre><p>我们在实战中采用MMLU数据集，这是一个通用能力测试数据集，其实不同的测评方案就是数据集不一样，每一个数据集有自己的测评方法，不需要单独编写程序。</p><p>MMLU（Massive Multitask Language Understanding）数据集包含了来自57个不同领域的大量多选题，主要特点就是<strong>多学科覆盖</strong>，包括STEM（科学、技术、工程、数学）、人文科学、社科、法律、医学等，评估模型在不同领域的知识掌握情况。而且他还具备<strong>多任务学习能力</strong>。可以说，MMLU的设计目的就是评估模型的多任务学习能力，也就是让模型在不同任务之间切换并保持高水平表现的能力。</p><p>下面是这个数据集57个数据的截图展示。</p><p><img src=\"https://static001.geekbang.org/resource/image/18/4c/186099f2409fa6a15890f78598deec4c.png?wh=1502x1236\" alt=\"图片\"></p><p>那我们做通用能力测评需要57个任务全部跑一遍吗？其实只要跑子任务就行，核心思想是对客服场景的相关能力评估，运行流程是自动化的。</p><p><img src=\"https://static001.geekbang.org/resource/image/70/e8/706374e7622dac51ef0c53bd1071c2e8.png?wh=1920x860\" alt=\"图片\"></p><h3>自动化流程</h3><p>具体的测评工具可以用 <strong>LM-Evaluation-Harness</strong> 框架，<strong>LM-Evaluation-Harness</strong> 是一个用于评估和比较语言模型（Language Models, LMs）性能的工具包。</p><p>首先，克隆并安装 <code>lm-evaluation-harness</code> 库。</p><pre><code class=\"language-plain\">git clone https://github.com/EleutherAI/lm-evaluation-harness.git\ncd lm-evaluation-harness\npip install -e .\n</code></pre><p>在运行之前，需要明确几点，<code>lm-evaluation-harness</code> 框架可以直接测评各种开源的大模型，也可以使用各种公开数据集来测评。在我们客服项目中，被测评的模型选择私有的大模型，而数据集选择 <code>mmlu</code>。</p><p>使用以下命令进行评估。</p><pre><code class=\"language-plain\">lm_eval \\\n    --model hf \\\n    --model_args pretrained=\"your-hf-model-name-or-path\" \\ #私有模型路径\n    --tasks mmlu \\ #数据集\n    --device cuda \\\n    --output_path ./results.json\n</code></pre><p>参数说明：</p><ul>\n<li>\n<p><code>your-hf-model-name-or-path</code> 替换为你的微调模型的名称或路径。</p>\n</li>\n<li>\n<p><code>--tasks</code> 指定要评估的任务，这里是 <code>mmlu</code>。</p>\n</li>\n<li>\n<p><code>--device</code> 指定设备（如 <code>cuda</code> 或 <code>cpu</code>）。</p>\n</li>\n</ul><p>运行完毕可以在<code>results.json</code>中查看结果，<code>lm-evaluation-harness</code> 也会在终端显示模型的评估结果，包括准确率等指标。</p><p>下面是我的模型评估命令和结果。</p><pre><code class=\"language-plain\">lm_eval \\\n    --model hf \\\n    --model_args pretrained=\"/root/autodl-tmp/outs/jinwei01\" \\\n    --tasks mmlu \\\n    --device cuda \\\n    --output_path ./results.json\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/d4/b8/d4f4063820a3f939b1073f3d819086b8.png?wh=1502x412\" alt=\"图片\"></p><p>这套流程可以帮助你快速评估微调后的模型在 MMLU 数据集上的表现，而不需要进行复杂的代码修改。</p><p>这里有一个重要的问题，测评的results是一个准确率数值，并不是评分。我们怎么做呢？工程实战中可以用这个准确率作为基准和微调前对比，微调中对比。不用纠结于模型评分。</p><p><img src=\"https://static001.geekbang.org/resource/image/3e/b6/3e2cdbcc95a475c252a913100a64beb6.png?wh=1920x672\" alt=\"图片\"></p><p>另外，关于<strong>数据集大小，</strong>MMLU 数据集包含 57 个任务，每个任务中有数百到上千个问题。如果评估所有任务，时间会比较长，不用担心，很正常。</p><p>还有一个重要的问题，你可能会想，MMLU是英文测试集，我们的智能客服场景是中文，这种测评方法能评估中文通用能力表现吗？实际情况是英文数据集就可以评估中文能力，也就是说如果微调导致英文通用能力下降，中文能力也随之下降。不过你要把MMLU翻译为中文或干脆用中文测评集也没问题。</p><h2>私有task评分</h2><p>通用数据集是全自动测评，私有task评分也是全自动测评，基本思想是针对我们的智能客服项目，设计关键的问题和标准答案，用自动化测评的方法来评估这些问题，并得到评分。</p><p>私有task评估的目标和人工评估其实是一样的，都是为了测试模型微调后的新能力有没有提升，那为什么不用私有task评估完全替代人工呢？这个问题你跟着我做一遍项目的私有task测评就清楚了。</p><p>还是使用 <strong>LM-Evaluation-Harness</strong> 工具包。这里有必要再理解一下这个工具的核心思想是什么。</p><p><strong>LM-Evaluation-Harness</strong> 实际上提供一套测评框架，可以自定义测评任务，在电商客服中，可以理解为：将一些我们要通过微调完成的任务，写出测评任务，这些任务就是私有task，用框架重点测评这些task，评估我们模型的微调的效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/96/9d/96da60a2c44d222ed19c49c81404ed9d.png?wh=1920x660\" alt=\"图片\"></p><h3>私有task设计</h3><p>要做好私有task测评，一定要先搞清楚私有task的关键概念，我把这些概念整理到一张图中了，供你参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/3a/ca/3ab6e259397079076eb19e3c7e0134ca.png?wh=1920x1139\" alt=\"图片\"></p><p>你跟着我把私有task全过程运行一次，就能理解这些关键概念了。下面的配置、运行、评估过程包含了几个关键的工程经验，是我们实战过程中的总结。</p><p>运行私有task评估和通用能力测评命令是一样的，唯一的不同就是 <code>tasks</code> 参数要配置为我们自己的task配置文件。</p><pre><code class=\"language-plain\">lm_eval \\\n    --model hf \\\n    --model_args pretrained=\"/root/autodl-tmp/outs/jinwei01\" \\\n    --tasks /path/my_custom_task.yaml \\ #task配置文件\n    --device cuda \\\n    --output_path ./results.json\n</code></pre><p>这条命令看起来非常简单，但要实际运行成功，需要注意的细节特别多，第一个注意点，私有task的task配置要在lm_eval的tasks目录下。我们就曾经因为把这个配置文件放在别的目录，导致命令运行一直失败。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/db/e2b44aced7510d86cfd74ecd968d58db.png?wh=1096x902\" alt=\"图片\"><br>\n如上图所示，我这里的私有task名字叫 <code>my_custom_task</code>，建立的配置文件<code>my_custom_task.yaml</code>，这里yaml配置的第一项 task 也必须写为 <code>my_custom_task</code>，千万不要填别的名字，我们也曾经掉到这个坑里。</p><pre><code class=\"language-plain\">task: my_custom_task\n... ...\n</code></pre><p>第二项重要工作就是数据工程了。既然是私有task测评，那数据集也需要自己创建，也就是平常说的数据工程。至于数据格式，因为我们只用来做测评，只要完善 <code>test</code> 这一段的数据即可，采用问答格式，具体可以看下面数据集中的 <code>test</code> 部分。</p><pre><code class=\"language-plain\">{\n  \"train\": [\n    ... ...\n  ],\n  \"validation\": [\n    ... ...\n  ],\n  \"test\": [\n    {\n      \"question\": \"1+1=\",\n      \"answer\": \"2\"\n    }\n  ]\n}\n</code></pre><p>针对智能客服场景的 <code>test</code> 数据格式，要看具体评估功能来确定，比如评估地址提取能力的就应该是如下的格式。</p><pre><code class=\"language-plain\">\"test\": [\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; \"question\": \"从以下信息中提取地址：XX省XX市XX区XX街道XX号\",\n&nbsp; &nbsp; &nbsp; \"answer\": \"{\\\"province\\\": \\\"XX省\\\", \\\"city\\\": \\\"XX市\\\", \\\"district\\\": \\\"XX区\\\", \\\"street\\\": \\\"XX街道\\\", \\\"number\\\": \\\"XX号\\\"}\"\n&nbsp; &nbsp; }\n    ... ...\n&nbsp; ]\n</code></pre><p>数据集文件的创建过程分为两步，第一步是建立一个 <code>dataset.json</code> 数据集，具体格式就是上面例子里的格式。这里涉及到第二个重要的注意点，<code>dataset.json</code> 数据集必须包含 <code>train</code> 和 <code>validation</code> 模块，即使它们在评估中用不到。</p><p>我们刚才建立的数据集是json格式的，在私有task测评中，我们的经验是要讲json格式的数据集转化为 <code>.arrow</code> 格式，这就是数据集创建的第二步。</p><p>单独写一个Python数据处理的脚步，完成这个转化。</p><pre><code class=\"language-plain\">import json\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\ndef json_to_arrow(input_json, output_arrow_file, data_split):\n    # 读取 JSON 数据\n    with open(input_json, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n\n    ... ...\n\n\n# 示例用法\njson_to_arrow('dataset.json', 'train.arrow', 'train')\njson_to_arrow('dataset.json', 'validation.arrow', 'validation')\njson_to_arrow('dataset.json', 'test.arrow', 'test')\n</code></pre><p>最后才是完善 <code>my_custom_task.yaml</code> 配置文件，其核心是数据集的配置。也就是 <code>training_split</code>、<code>validation_split</code>、<code>test_split</code> 这几项配置。</p><pre><code class=\"language-plain\">task: my_custom_task\ndataset_path: arrow\ndataset_name: null\ntraining_split: train\nvalidation_split: validation  # 如果你有验证集，指定其名称\ntest_split: test  # 如果你有测试集，指定其名称\ndataset_kwargs:\n    data_files:\n        train: ./train.arrow\n        test: ./test.arrow\n        validation: ./validation.arrow  # 指定本地数据集文件的路径\n\n\ndoc_to_text: \"{{question}}\"  # 输入列\ndoc_to_target: \"{{answer}}\"  # 目标列\n\n\nmetric_list: #评估算法\n  - metric: bleu\n    aggregation: mean\n    higher_is_better: true\n</code></pre><p>还有一点需要注意，配置文件中年的 <code>输入列</code> 和 <code>目标列</code> 需要和数据集字段对应。</p><p>现在就可以运行评估程序了吗？还不行，你注意配置文件的最后一项 <code>metric_list</code>，表示评估算法，什么意思呢？我们先看一个实际运行的问答数据例子。</p><pre><code class=\"language-plain\">问：这个内存条的频率是多少，能支持超频吗？\n答：这款内存条的默认频率为3200MHz。它支持超频\nanswer：这款内存条的默认频率为3200MHz。是的支持超频\n</code></pre><p>现在最核心的问题是，怎么评估这个回答是不是准确呢？在智能客服项目中，我们要自定义个评估算法，你接着看我分析。</p><h3>评分算法</h3><p>最简单的算法就是完全匹配算法，如果回答和answer完全一样记100分，否则0分。当然针对大模型这个场景，使用更灵活的评估方法，而不是简单的字符串匹配。常见的方法包括使用 相似度评估（例如 BLEU、ROUGE、BERTScore 等）或 模糊匹配。</p><p>我们选用的是 <code>BLEU</code> 算法，下面的 <code>bleu</code> 和 <code>agg_bleu</code> 分别是单个数据的bleu计算和整体的bleu评分。</p><pre><code class=\"language-plain\">import evaluate  # 导入 evaluate 库，用于加载 BLEU 评估函数\n\n\ndef bleu(predictions, references):\n    # 定义一个简单的 BLEU 函数，只返回第一个预测和参考\n    return (predictions[0], references[0])\n\n\nimport sacrebleu  # 导入 sacrebleu 库，用于计算 BLEU 分数\n\n\ndef agg_bleu(items):\n    # 聚合多个预测和参考，计算整体的 BLEU 分数\n    predictions, references = zip(*items)  # 将预测和参考分离成两个列表\n    bleu_score = sacrebleu.corpus_bleu(predictions, [references])  # 使用 sacrebleu 计算整体 BLEU 分数\n    return bleu_score.score  # 返回 BLEU 分数\n\n\ndef agg_bleu1(items):\n    # 另一个计算整体 BLEU 分数的函数，使用 evaluate 库的 BLEU 评估函数\n    bleu_fn = evaluate.load(\"bleu\")  # 加载 BLEU 评估函数\n    predictions, references = zip(*items)  # 将预测和参考分离成两个列表\n    return bleu_fn.compute(predictions=predictions, references=references)[\"bleu\"]  # 计算并返回 BLEU 分数\n</code></pre><p>为什么我们要自己实现bleu算法呢？这就是私有task项目中的第四个注意点，也是工程经验，如果你在配置文件中写默认的bleu评估配置，评估程序会一直报错，我们甚至去修改了框架的源代码都无法解决。</p><pre><code class=\"language-plain\">metric_list:\n  - metric: bleu\n     aggregation: mean\n     higher_is_better: true\n</code></pre><p>最简单的解决方案就是自己写一个bleu的算法模块，修改task的配置文件，把评估算法配置到自己的评估算法上。</p><pre><code class=\"language-plain\">metric_list:\n  # - metric: bleu\n  #   aggregation: mean\n  #   higher_is_better: true\n  - metric: !function metrics.bleu\n    aggregation: !function metrics.agg_bleu\n    higher_is_better: true\n</code></pre><p>下面是这个私有task的最终运行效果。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/d9/605a5bb1e84997727346c5a6d322c4d9.png?wh=1710x232\" alt=\"图片\"></p><p>我们可以回过头分析一下，这种基于相似度的评估算法实际上还是不够准确的，因此私有task评估也只适合有标准答案的测评，主要是答案要完全精准的。</p><h2>小结</h2><p>真实的大模型微调项目中，测评过程和数据工程一样重要，一个大模型不可能一次微调成功，往往要经过数据-&gt;训练-&gt;评估-&gt;修改数据，这样循环往复的过程才能微调完成。</p><p>之前课程也提到，只有当大模型遇到问题的时候才需要微调，因此评估的重点是看微调后的大模型是否具备新的特性，其方法是人工测评+私有task测评，其中核心参考人工测评的结果，私有task可以完全实现自动化，其评分作为辅助参考。</p><p>另外一方面，大模型微调之后，原有基础能力不能下降，如何评估呢？可以通过通用测评数据集的测评来完成，这一步和私有task一样，都可以通过 <strong>LM-Evaluation-Harness</strong> 测评框架自动化完成。</p><p>最后怎么根据这些评分来调整你的训练和数据呢？我整理了一张图供你参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/c8/c9/c8548cb9b597765929cd05365eba64c9.png?wh=1920x1086\" alt=\"图片\"></p><h2>思考题</h2><p>在私有task评估中，我们提到评估算法的好坏决定了私有task的评估水平，那么可以用大模型来作为评估算法吗？具体怎么做？</p><p>欢迎你在留言区和我交流。如果觉得有所收获，也可以把课程分享给更多的朋友一起学习。我们下节课见！</p><p><a href=\"https://jsj.top/f/hm26hN\">&gt;&gt;戳此加入课程交流群</a></p>","neighbors":{"left":{"article_title":"16｜训练：真实的电商客服模型训练过程藏有哪些魔鬼细节？","id":807931},"right":{"article_title":"18｜实战升级：如何提升电商客服模型理解JSON等数据协议的能力","id":808898}},"comments":[{"had_liked":false,"id":394323,"user_name":"Geek_a7be42","can_delete":false,"product_type":"c1","uid":2851466,"ip_address":"广东","ucode":"51DC303600185F","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/I38GzuHkWp5LEEEdkBgbfM1ctuX23oiayhsJ0xbcHUotNUkSmpppk7mRVzhWxG8m67T71YA0kDzVeKTYmibp926w/132","comment_is_top":false,"comment_ctime":1726486576,"is_pvip":false,"replies":[{"id":143437,"content":"真的有","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1729077607,"ip_address":"北京","comment_id":394323,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"真的会有电商用AI客服吗，感觉也太复杂了，市场变化如果太快的话且不是白搞了","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652516,"discussion_content":"真的有","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729077607,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1024195,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg","nickname":"石云升","note":"","ucode":"78F1DD33EFD000","race_medal":1,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651565,"discussion_content":"做售前应该问题不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1727074482,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}