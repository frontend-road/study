{"id":464855,"title":"44｜数据处理：应用程序和数据如何打交道？","content":"<p>你好，我是陈天。</p><p>我们开发者无论是从事服务端的开发，还是客户端的开发，和数据打交道是必不可少的。</p><p>对于客户端来说，从服务端读取到的数据，往往需要做缓存（内存缓存或者 SQLite 缓存），甚至需要本地存储（文件或者 SQLite）。</p><p>对于服务器来说，跟数据打交道的场景就更加丰富了。除了数据库和缓存外，还有大量文本数据的索引（比如搜索引擎）、实时的消息队列对数据做流式处理，或者非实时的批处理对数据仓库（data warehouse）中的海量数据进行 ETL（Extract、Transform and Load）。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/eb/1c42e693f0848b4a389870f848ffaeeb.png?wh=1482x807\" alt=\"图片\"></p><p>今天我们就来讲讲如何用 Rust 做数据处理，主要讲两部分，如何用 Rust 访问关系数据库，以及如何用 Rust 对半结构化数据进行分析和处理。希望通过学习这一讲的内容，尤其是后半部分的内容，能帮你打开眼界，对数据处理有更加深刻的认识。</p><h2>访问关系数据库</h2><p>作为互联网应用的最主要的数据存储和访问工具，关系数据库，是几乎每门编程语言都有良好支持的数据库类型。</p><p>在 Rust 下，有几乎所有主流关系数据库的驱动，比如 <a href=\"https://github.com/sfackler/rust-postgres\">rust-postgres、rust-mysql-simple</a> 等，不过一般我们不太会直接使用数据库的驱动来访问数据库，因为那样会让应用过于耦合于某个数据库，所以我们会使用 ORM。</p><!-- [[[read_end]]] --><p>Rust 下有 <a href=\"https://diesel.rs/\">diesel</a> 这个非常成熟的 ORM，还有 <a href=\"https://github.com/SeaQL/sea-orm\">sea-orm</a> 这样的后起之秀。diesel 不支持异步，而 sea-orm 支持异步，所以，有理由相信，随着 sea-orm 的不断成熟，会有越来越多的应用在 sea-orm 上构建。</p><p>如果你觉得 ORM 太过笨重，繁文缛节太多，但又不想直接使用某个数据库的驱动来访问数据库，那么你还可以用 <a href=\"https://github.com/launchbadge/sqlx\">sqlx</a>。sqlx 提供了对多种数据库（Postgres、MySQL、SQLite、MSSQL）的异步访问支持，并且不使用 DSL 就可以对 SQL query 做编译时检查，非常轻便；它可以从数据库中直接查询出来一行数据，也可以通过派生宏自动把行数据转换成对应的结构。</p><p>今天，我们就尝试使用 sqlx 处理用户注册和登录这两个非常常见的功能。</p><h3>sqlx</h3><p>构建下面的表结构来处理用户登录信息：</p><pre><code class=\"language-sql\">CREATE TABLE IF NOT EXISTS users\n(\n    id              INTEGER PRIMARY KEY NOT NULL,\n    email           VARCHAR UNIQUE      NOT NULL,\n    hashed_password VARCHAR             NOT NULL\n);\n</code></pre><p>特别说明一下，在数据库中存储用户信息需要非常谨慎，尤其是涉及敏感的数据，比如密码，需要使用特定的哈希算法存储。OWASP 对密码的存储有如下<a href=\"https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html\">安全建议</a>：</p><ol>\n<li>如果 Argon2id 可用，那么使用 Argon2id（需要目标机器至少有 15MB 内存）。</li>\n<li>如果 Argon2id 不可用，那么使用 bcrypt（算法至少迭代 10 次）。</li>\n<li>之后再考虑 scrypt / PBKDF2。</li>\n</ol><p>Argon2id 是 Argon2d 和 Argon2i 的组合，Argon2d 提供了强大的抗 GPU 破解能力，但在特定情况下会容易遭受<a href=\"https://zh.wikipedia.org/wiki/%E6%97%81%E8%B7%AF%E6%94%BB%E5%87%BB\">旁路攻击</a>（side-channel attacks），而 Argon2i 则可以防止旁路攻击，但抗 GPU 破解稍弱。所以只要是编程语言支持 Argo2id，那么它就是首选的密码哈希工具。</p><p>Rust 下有完善的 <a href=\"https://github.com/RustCrypto/password-hashes\">password-hashes</a> 工具，我们可以使用其中的 <a href=\"https://github.com/RustCrypto/password-hashes/tree/master/argon2\">argon2</a> crate，用它生成的一个完整的，包含所有参数的密码哈希长这个样子：</p><pre><code class=\"language-plain\">$argon2id$v=19$m=4096,t=3,p=1$l7IEIWV7puJYJAZHyyut8A$OPxL09ODxp/xDQEnlG1NWdOsTr7RzuleBtiYQsnCyXY\n</code></pre><p>这个字符串里包含了 argon2id 的版本（19）、使用的内存大小（4096k）、迭代次数（3 次）、并行程度（1 个线程），以及 base64 编码的 salt 和 hash。</p><p>所以，当新用户注册时，我们使用 argon2 把传入的密码哈希一下，存储到数据库中；当用户使用 email/password 登录时，我们通过 email 找到用户，然后再通过 argon2 验证密码。数据库的访问使用 sqlx，为了简单起见，避免安装额外的数据库，就使用 SQLite来存储数据（如果你本地有 MySQL 或者 PostgreSQL，可以自行替换相应的语句）。</p><p>有了这个思路，我们创建一个新的项目，添加相关的依赖：</p><pre><code class=\"language-rust\">[dev-dependencies]\nanyhow = \"1\"\nargon2 = \"0.3\"\nlazy_static = \"1\"\nrand_core = { version = \"0.6\", features = [\"std\"] }\nsqlx = { version = \"0.5\", features = [\"runtime-tokio-rustls\", \"sqlite\"] }\ntokio = { version = \"1\", features = [\"full\" ] }\n</code></pre><p>然后创建 examples/user.rs，添入代码，你可以对照详细的注释来理解：</p><pre><code class=\"language-rust\">use anyhow::{anyhow, Result};\nuse argon2::{\n    password_hash::{rand_core::OsRng, PasswordHash, PasswordHasher, SaltString},\n    Argon2, PasswordVerifier,\n};\nuse lazy_static::lazy_static;\nuse sqlx::{sqlite::SqlitePoolOptions, SqlitePool};\nuse std::env;\n\n/// Argon2 hash 使用的密码\nconst ARGON_SECRET: &amp;[u8] = b\"deadbeef\";\nlazy_static! {\n    /// Argon2\n    static ref ARGON2: Argon2&lt;'static&gt; = Argon2::new_with_secret(\n        ARGON_SECRET,\n        argon2::Algorithm::default(),\n        argon2::Version::default(),\n        argon2::Params::default()\n    )\n    .unwrap();\n}\n\n/// user 表对应的数据结构，处理 login/register\npub struct UserDb {\n    pool: SqlitePool,\n}\n\n/// 使用 FromRow 派生宏把从数据库中读取出来的数据转换成 User 结构\n#[allow(dead_code)]\n#[derive(Debug, sqlx::FromRow)]\npub struct User {\n    id: i64,\n    email: String,\n    hashed_password: String,\n}\n\nimpl UserDb {\n    pub fn new(pool: SqlitePool) -&gt; Self {\n        Self { pool }\n    }\n\n    /// 用户注册：在 users 表中存储 argon2 哈希过的密码\n    pub async fn register(&amp;self, email: &amp;str, password: &amp;str) -&gt; Result&lt;i64&gt; {\n        let hashed_password = generate_password_hash(password)?;\n        let id = sqlx::query(\"INSERT INTO users(email, hashed_password) VALUES (?, ?)\")\n            .bind(email)\n            .bind(hashed_password)\n            .execute(&amp;self.pool)\n            .await?\n            .last_insert_rowid();\n\n        Ok(id)\n    }\n\n    /// 用户登录：从 users 表中获取用户信息，并用验证用户密码\n    pub async fn login(&amp;self, email: &amp;str, password: &amp;str) -&gt; Result&lt;String&gt; {\n        let user: User = sqlx::query_as(\"SELECT * from users WHERE email = ?\")\n            .bind(email)\n            .fetch_one(&amp;self.pool)\n            .await?;\n        println!(\"find user: {:?}\", user);\n        if let Err(_) = verify_password(password, &amp;user.hashed_password) {\n            return Err(anyhow!(\"failed to login\"));\n        }\n\n        // 生成 JWT token（此处省略 JWT token 生成的细节）\n        Ok(\"awesome token\".into())\n    }\n}\n\n/// 重新创建 users 表\nasync fn recreate_table(pool: &amp;SqlitePool) -&gt; Result&lt;()&gt; {\n    sqlx::query(\"DROP TABLE IF EXISTS users\").execute(pool).await?;\n    sqlx::query(\n        r#\"CREATE TABLE IF NOT EXISTS users(\n                id              INTEGER PRIMARY KEY NOT NULL,\n                email           VARCHAR UNIQUE      NOT NULL,\n                hashed_password VARCHAR             NOT NULL)\"#,\n    )\n    .execute(pool)\n    .await?;\n    Ok(())\n}\n\n/// 创建安全的密码哈希\nfn generate_password_hash(password: &amp;str) -&gt; Result&lt;String&gt; {\n    let salt = SaltString::generate(&amp;mut OsRng);\n    Ok(ARGON2\n        .hash_password(password.as_bytes(), &amp;salt)\n        .map_err(|_| anyhow!(\"failed to hash password\"))?\n        .to_string())\n}\n\n/// 使用 argon2 验证用户密码和密码哈希\nfn verify_password(password: &amp;str, password_hash: &amp;str) -&gt; Result&lt;()&gt; {\n    let parsed_hash =\n        PasswordHash::new(password_hash).map_err(|_| anyhow!(\"failed to parse hashed password\"))?;\n    ARGON2\n        .verify_password(password.as_bytes(), &amp;parsed_hash)\n        .map_err(|_| anyhow!(\"failed to verify password\"))?;\n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let url = env::var(\"DATABASE_URL\").unwrap_or(\"sqlite://./data/example.db\".into());\n    // 创建连接池\n    let pool = SqlitePoolOptions::new()\n        .max_connections(5)\n        .connect(&amp;url)\n        .await?;\n\n    // 每次运行都重新创建 users 表\n    recreate_table(&amp;pool).await?;\n\n    let user_db = UserDb::new(pool.clone());\n    let email = \"tyr@awesome.com\";\n    let password = \"hunter42\";\n\n    // 新用户注册\n    let id = user_db.register(email, password).await?;\n    println!(\"registered id: {}\", id);\n\n    // 用户成功登录\n    let token = user_db.login(email, password).await?;\n    println!(\"Login succeeded: {}\", token);\n\n    // 登录失败\n    let result = user_db.login(email, \"badpass\").await;\n    println!(\"Login should fail with bad password: {:?}\", result);\n\n    Ok(())\n}\n</code></pre><p>在这段代码里，我们把 argon2 的能力稍微包装了一下，提供了 <code>generate_password_hash</code> 和 <code>verify_password</code> 两个方法给注册和登录使用。对于数据库的访问，我们提供了一个连接池 SqlitePool，便于无锁访问。</p><p>你可能注意到了这句写法：</p><pre><code class=\"language-rust\">let user: User = sqlx::query_as(\"SELECT * from users WHERE email = ?\")\n    .bind(email)\n    .fetch_one(&amp;self.pool)\n    .await?;\n</code></pre><p>是不是很惊讶，一般来说，这是 ORM 才有的功能啊。没错，它再次体现了 Rust trait 的强大：我们并不需要 ORM 就可以把数据库中的数据跟某个 Model 结合起来，只需要在查询时，提供想要转换成的数据结构 T: FromRow 即可。</p><p>看 query_as 函数和 FromRow trait 的定义（<a href=\"https://docs.rs/sqlx-core/0.5.9/src/sqlx_core/query_as.rs.html#157-160\">代码</a>）：</p><pre><code class=\"language-rust\">pub fn query_as&lt;'q, DB, O&gt;(sql: &amp;'q str) -&gt; QueryAs&lt;'q, DB, O, &lt;DB as HasArguments&lt;'q&gt;&gt;::Arguments&gt;\nwhere\n    DB: Database,\n    O: for&lt;'r&gt; FromRow&lt;'r, DB::Row&gt;,\n{\n    QueryAs {\n        inner: query(sql),\n        output: PhantomData,\n    }\n}\n\npub trait FromRow&lt;'r, R: Row&gt;: Sized {\n    fn from_row(row: &amp;'r R) -&gt; Result&lt;Self, Error&gt;;\n}\n</code></pre><p>要让一个数据结构支持 FromRow，很简单，使用 sqlx::FromRow 派生宏即可：</p><pre><code class=\"language-rust\">#[derive(Debug, sqlx::FromRow)]\npub struct User {\n    id: i64,\n    email: String,\n    hashed_password: String,\n}\n</code></pre><p>希望这个例子可以让你体会到 Rust 处理数据库的强大和简约。我们用 Rust 写出了 Node.js / Python 都不曾拥有的直观感受。另外，sqlx 是一个非常漂亮的 crate，有空的话建议你也看看它的源代码，开头介绍的 sea-orm，底层也是使用了 sqlx。</p><p><strong>特别说明</strong>，以上例子如果运行失败，可以去<a href=\"https://github.com/tyrchen/geektime-rust/tree/master/44_data_processing/data\">GitHub</a>上把 example.db 拷贝到本地 data 目录下，然后运行。</p><h2>用 Rust 对半结构化数据进行分析</h2><p>在生产环境中，我们会累积大量的半结构化数据，比如各种各样的日志、监控数据和分析数据。</p><p>以日志为例，虽然通常会将其灌入日志分析工具，通过可视化界面进行分析和问题追踪，但偶尔我们也需要自己写点小工具进行处理，一般，会用 Python 来处理这样的任务，因为 Python 有 pandas 这样用起来非常舒服的工具。然而，pandas 太吃内存，运算效率也不算高。有没有更好的选择呢？</p><p>在第 6 讲我们介绍过 <a href=\"https://github.com/pola-rs/polars\">polars</a>，也用 polars 和 <a href=\"https://github.com/sqlparser-rs/sqlparser-rs\">sqlparser</a> 写了一个处理 csv 的工具，其实 polars 底层使用了 <a href=\"https://arrow.apache.org\">Apache arrow</a>。如果你经常进行大数据处理，那么你对列式存储（<a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\">columnar datastore</a>）和 <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe\">Data Frame</a> 应该比较熟悉，arrow 就是一个在内存中进行存储和运算的列式存储，它是构建下一代数据分析平台的基础软件。</p><p>由于 Rust 在业界的地位越来越重要，Apache arrow 也构建了完全用 <a href=\"https://github.com/apache/arrow-rs\">Rust 实现的版本</a>，并在此基础上构建了高效的 in-memory 查询引擎 <a href=\"https://github.com/apache/arrow-datafusion\">datafusion</a> ，以及在某些场景下可以取代 Spark 的分布式查询引擎 <a href=\"https://github.com/apache/arrow-datafusion/blob/master/ballista/README.md\">ballista</a>。</p><p>Apache arrow 和 datafusion 目前已经有很多重磅级的应用，其中最令人兴奋的是 <a href=\"https://github.com/influxdata/influxdb_iox\">InfluxDB IOx</a>，它是<a href=\"https://www.influxdata.com/blog/announcing-influxdb-iox/\">下一代的 InfluxDB 的核心引擎</a>。</p><p>来一起感受一下 datafusion 如何使用：</p><pre><code class=\"language-rust\">use datafusion::prelude::*;\nuse datafusion::arrow::util::pretty::print_batches;\nuse datafusion::arrow::record_batch::RecordBatch;\n\n#[tokio::main]\nasync fn main() -&gt; datafusion::error::Result&lt;()&gt; {\n  // register the table\n  let mut ctx = ExecutionContext::new();\n  ctx.register_csv(\"example\", \"tests/example.csv\", CsvReadOptions::new()).await?;\n\n  // create a plan to run a SQL query\n  let df = ctx.sql(\"SELECT a, MIN(b) FROM example GROUP BY a LIMIT 100\").await?;\n\n  // execute and print results\n  df.show().await?;\n  Ok(())\n}\n</code></pre><p>在这段代码中，我们通过 CsvReadOptions 推断 CSV 的 schema，然后将其注册为一个逻辑上的 example 表，之后就可以通过 SQL 进行查询了，是不是非常强大？</p><p>下面我们就使用 datafusion，来构建一个 Nginx 日志的命令行分析工具。</p><h3>datafusion</h3><p>在这门课程的 <a href=\"https://github.com/tyrchen/geektime-rust/tree/master/44_data_processing/fixtures\">GitHub repo</a> 里，我放了个从网上找到的样本日志，改名为 nginx_logs.csv（注意后缀需要是 csv），其格式如下：</p><pre><code class=\"language-bash\">93.180.71.3 - - \"17/May/2015:08:05:32 +0000\" GET \"/downloads/product_1\" \"HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\"\n93.180.71.3 - - \"17/May/2015:08:05:23 +0000\" GET \"/downloads/product_1\" \"HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\"\n80.91.33.133 - - \"17/May/2015:08:05:24 +0000\" GET \"/downloads/product_1\" \"HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.17)\"\n</code></pre><p>这个日志共有十个域，除了几个 “-”，无法猜测到是什么内容外，其它的域都很好猜测。</p><p>由于 nginx_logs 的格式是在 Nginx 配置中构建的，所以，日志文件，并不像 CSV 文件那样有一行 header，没有 header，就无法让 datafusion 直接帮我们推断出 schema，也就是说<strong>我们需要显式地告诉 datafusion 日志文件的 schema 长什么样</strong>。</p><p>不过对于 datafusuion 来说，创建一个 schema 很简单，比如：</p><pre><code class=\"language-rust\">let schema = Arc::new(Schema::new(vec![\n    Field::new(\"ip\", DataType::Utf8, false),\n    Field::new(\"code\", DataType::Int32, false),\n]));\n</code></pre><p>为了最大的灵活性，我们可以对应地构建一个简单的 schema 定义文件，里面每个字段按顺序对应 nginx 日志的字段：</p><pre><code class=\"language-yaml\">---\n- name: ip\n  type: string\n- name: unused1\n  type: string\n- name: unused2\n  type: string\n- name: date\n  type: string\n- name: method\n  type: string\n- name: url\n  type: string\n- name: version\n  type: string\n- name: code\n  type: integer\n- name: len\n  type: integer\n- name: unused3\n  type: string\n- name: ua\n  type: string\n</code></pre><p>这样，未来如果遇到不一样的日志文件，我们可以修改 schema 的定义，而无需修改程序本身。</p><p>对于这个 schema 定义文件，使用 <a href=\"https://github.com/serde-rs/serde\">serde</a> 和 <a href=\"https://github.com/dtolnay/serde-yaml\">serde-yaml</a> 来读取，然后再实现 From trait 把 SchemaField 对应到 datafusion 的 Field 结构：</p><pre><code class=\"language-rust\">#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord, Hash)]\n#[serde(rename_all = \"snake_case\")]\npub enum SchemaDataType {\n    /// Int64\n    Integer,\n    /// Utf8\n    String,\n    /// Date64,\n    Date,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]\nstruct SchemaField {\n    name: String,\n    #[serde(rename = \"type\")]\n    pub(crate) data_type: SchemaDataType,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Ord)]\nstruct SchemaFields(Vec&lt;SchemaField&gt;);\n\nimpl From&lt;SchemaDataType&gt; for DataType {\n    fn from(dt: SchemaDataType) -&gt; Self {\n        match dt {\n            SchemaDataType::Integer =&gt; Self::Int64,\n            SchemaDataType::Date =&gt; Self::Date64,\n            SchemaDataType::String =&gt; Self::Utf8,\n        }\n    }\n}\n\nimpl From&lt;SchemaField&gt; for Field {\n    fn from(f: SchemaField) -&gt; Self {\n        Self::new(&amp;f.name, f.data_type.into(), false)\n    }\n}\n\nimpl From&lt;SchemaFields&gt; for SchemaRef {\n    fn from(fields: SchemaFields) -&gt; Self {\n        let fields: Vec&lt;Field&gt; = fields.0.into_iter().map(|f| f.into()).collect();\n        Arc::new(Schema::new(fields))\n    }\n}\n</code></pre><p>有了这个基本的 schema 转换的功能，就可以构建我们的 nginx 日志处理结构及其功能了：</p><pre><code class=\"language-rust\">/// nginx 日志处理的数据结构\npub struct NginxLog {\n    ctx: ExecutionContext,\n}\n\nimpl NginxLog {\n    /// 根据 schema 定义，数据文件以及分隔符构建 NginxLog 结构\n    pub async fn try_new(schema_file: &amp;str, data_file: &amp;str, delim: u8) -&gt; Result&lt;Self&gt; {\n        let content = tokio::fs::read_to_string(schema_file).await?;\n        let fields: SchemaFields = serde_yaml::from_str(&amp;content)?;\n        let schema = SchemaRef::from(fields);\n\n        let mut ctx = ExecutionContext::new();\n        let options = CsvReadOptions::new()\n            .has_header(false)\n            .delimiter(delim)\n            .schema(&amp;schema);\n        ctx.register_csv(\"nginx\", data_file, options).await?;\n\n        Ok(Self { ctx })\n    }\n\n    /// 进行 sql 查询\n    pub async fn query(&amp;mut self, query: &amp;str) -&gt; Result&lt;Arc&lt;dyn DataFrame&gt;&gt; {\n        let df = self.ctx.sql(query).await?;\n        Ok(df)\n    }\n}\n</code></pre><p>仅仅写了 80 行代码，就完成了 nginx 日志文件的读取、解析和查询功能，其中 50 行代码还是为了处理 schema 配置文件。是不是有点不敢相信自己的眼睛？</p><p>datafusion/arrow 也太强大了吧？这个简洁的背后，是 10w 行 arrow 代码和 1w 行 datafusion 代码的功劳。</p><p>再来写段代码调用它：</p><pre><code class=\"language-rust\">#[tokio::main]\nasync fn main() -&gt; Result&lt;()&gt; {\n    let mut nginx_log =\n        NginxLog::try_new(\"fixtures/log_schema.yml\", \"fixtures/nginx_logs.csv\", b' ').await?;\n    // 从 stdin 中按行读取内容，当做 sql 查询，进行处理\n    let stdin = io::stdin();\n    let mut lines = stdin.lock().lines();\n\n    while let Some(Ok(line)) = lines.next() {\n        if !line.starts_with(\"--\") {\n            println!(\"{}\", line);\n            // 读到一行 sql，查询，获取 dataframe\n            let df = nginx_log.query(&amp;line).await?;\n            // 简单显示 dataframe\n            df.show().await?;\n        }\n    }\n\n    Ok(())\n}\n</code></pre><p>在这段代码里，我们从 stdin 中获取内容，把每一行输入都作为一个 SQL 语句传给 nginx_log.query，然后显示查询结果。</p><p>来测试一下：</p><pre><code class=\"language-sql\">❯ echo \"SELECT ip, count(*) as total, cast(avg(len) as int) as avg_len FROM nginx GROUP BY ip ORDER BY total DESC LIMIT 10\" | cargo run --example log --quiet\nSELECT ip, count(*) as total, cast(avg(len) as int) as avg_len FROM nginx GROUP BY ip ORDER BY total DESC LIMIT 10\n+-----------------+-------+---------+\n| ip              | total | avg_len |\n+-----------------+-------+---------+\n| 216.46.173.126  | 2350  | 220     |\n| 180.179.174.219 | 1720  | 292     |\n| 204.77.168.241  | 1439  | 340     |\n| 65.39.197.164   | 1365  | 241     |\n| 80.91.33.133    | 1202  | 243     |\n| 84.208.15.12    | 1120  | 197     |\n| 74.125.60.158   | 1084  | 300     |\n| 119.252.76.162  | 1064  | 281     |\n| 79.136.114.202  | 628   | 280     |\n| 54.207.57.55    | 532   | 289     |\n+-----------------+-------+---------+\n</code></pre><p>是不是挺厉害？我们可以充分利用 SQL 的强大表现力，做各种复杂的查询。不光如此，还可以从一个包含了多个 sql 语句的文件中，一次性做多个查询。比如我创建了这样一个文件 analyze.sql：</p><pre><code class=\"language-sql\">-- 查询 ip 前 10 名\nSELECT ip, count(*) as total, cast(avg(len) as int) as avg_len FROM nginx GROUP BY ip ORDER BY total DESC LIMIT 10\n-- 查询 UA 前 10 名\nselect ua, count(*) as total from nginx group by ua order by total desc limit 10\n-- 查询访问最多的 url 前 10 名\nselect url, count(*) as total from nginx group by url order by total desc limit 10\n-- 查询访问返回 body 长度前 10 名\nselect len, count(*) as total from nginx group by len order by total desc limit 10\n-- 查询 HEAD 请求\nselect ip, date, url, code, ua from nginx where method = 'HEAD' limit 10\n-- 查询状态码是 403 的请求\nselect ip, date, url, ua from nginx where code = 403 limit 10\n-- 查询 UA 为空的请求\nselect ip, date, url, code from nginx where ua = '-' limit 10\n-- 复杂查询，找返回 body 长度的 percentile 在 0.5-0.7 之间的数据\nselect * from (select ip, date, url, ua, len, PERCENT_RANK() OVER (ORDER BY len) as len_percentile from nginx where code = 200 order by len desc) as t where t.len_percentile &gt; 0.5 and t.len_percentile &lt; 0.7 order by t.len_percentile desc limit 10\n</code></pre><p>那么，我可以这样获取结果：</p><pre><code class=\"language-sql\">❯ cat fixtures/analyze.sql | cargo run --example log --quiet\nSELECT ip, count(*) as total, cast(avg(len) as int) as avg_len FROM nginx GROUP BY ip ORDER BY total DESC LIMIT 10\n+-----------------+-------+---------+\n| ip              | total | avg_len |\n+-----------------+-------+---------+\n| 216.46.173.126  | 2350  | 220     |\n| 180.179.174.219 | 1720  | 292     |\n| 204.77.168.241  | 1439  | 340     |\n| 65.39.197.164   | 1365  | 241     |\n| 80.91.33.133    | 1202  | 243     |\n| 84.208.15.12    | 1120  | 197     |\n| 74.125.60.158   | 1084  | 300     |\n| 119.252.76.162  | 1064  | 281     |\n| 79.136.114.202  | 628   | 280     |\n| 54.207.57.55    | 532   | 289     |\n+-----------------+-------+---------+\nselect ua, count(*) as total from nginx group by ua order by total desc limit 10\n+-----------------------------------------------+-------+\n| ua                                            | total |\n+-----------------------------------------------+-------+\n| Debian APT-HTTP/1.3 (1.0.1ubuntu2)            | 11830 |\n| Debian APT-HTTP/1.3 (0.9.7.9)                 | 11365 |\n| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21) | 6719  |\n| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.16) | 5740  |\n| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.22) | 3855  |\n| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.17) | 1827  |\n| Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.7)  | 1255  |\n| urlgrabber/3.9.1 yum/3.2.29                   | 792   |\n| Debian APT-HTTP/1.3 (0.9.7.8)                 | 750   |\n| urlgrabber/3.9.1 yum/3.4.3                    | 708   |\n+-----------------------------------------------+-------+\nselect url, count(*) as total from nginx group by url order by total desc limit 10\n+----------------------+-------+\n| url                  | total |\n+----------------------+-------+\n| /downloads/product_1 | 30285 |\n| /downloads/product_2 | 21104 |\n| /downloads/product_3 | 73    |\n+----------------------+-------+\nselect len, count(*) as total from nginx group by len order by total desc limit 10\n+-----+-------+\n| len | total |\n+-----+-------+\n| 0   | 13413 |\n| 336 | 6652  |\n| 333 | 3771  |\n| 338 | 3393  |\n| 337 | 3268  |\n| 339 | 2999  |\n| 331 | 2867  |\n| 340 | 1629  |\n| 334 | 1393  |\n| 332 | 1240  |\n+-----+-------+\nselect ip, date, url, code, ua from nginx where method = 'HEAD' limit 10\n+----------------+----------------------------+----------------------+------+-------------------------+\n| ip             | date                       | url                  | code | ua                      |\n+----------------+----------------------------+----------------------+------+-------------------------+\n| 184.173.149.15 | 23/May/2015:15:05:53 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:30 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:33 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:34 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:52 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:43 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:42 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:17:05:46 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |\n| 5.153.24.140   | 23/May/2015:18:05:10 +0000 | /downloads/product_2 | 200  | Wget/1.13.4 (linux-gnu) |\n| 184.173.149.16 | 24/May/2015:18:05:37 +0000 | /downloads/product_2 | 403  | Wget/1.13.4 (linux-gnu) |\n+----------------+----------------------------+----------------------+------+-------------------------+\nselect ip, date, url, ua from nginx where code = 403 limit 10\n+----------------+----------------------------+----------------------+-----------------------------------------------------------------------------------------------------+\n| ip             | date                       | url                  | ua                                                                                                  |\n+----------------+----------------------------+----------------------+-----------------------------------------------------------------------------------------------------+\n| 184.173.149.15 | 23/May/2015:15:05:53 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 5.153.24.140   | 23/May/2015:17:05:33 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 5.153.24.140   | 23/May/2015:17:05:34 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 184.173.149.16 | 24/May/2015:18:05:37 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 195.88.195.153 | 24/May/2015:23:05:05 +0000 | /downloads/product_2 | curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3 |\n| 184.173.149.15 | 25/May/2015:04:05:14 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 87.85.173.82   | 17/May/2015:14:05:07 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 87.85.173.82   | 17/May/2015:14:05:11 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 194.76.107.17  | 17/May/2015:16:05:50 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n| 194.76.107.17  | 17/May/2015:17:05:40 +0000 | /downloads/product_2 | Wget/1.13.4 (linux-gnu)                                                                             |\n+----------------+----------------------------+----------------------+-----------------------------------------------------------------------------------------------------+\nselect ip, date, url, code from nginx where ua = '-' limit 10\n+----------------+----------------------------+----------------------+------+\n| ip             | date                       | url                  | code |\n+----------------+----------------------------+----------------------+------+\n| 217.168.17.150 | 01/Jun/2015:14:06:45 +0000 | /downloads/product_2 | 200  |\n| 217.168.17.180 | 01/Jun/2015:14:06:15 +0000 | /downloads/product_2 | 200  |\n| 217.168.17.150 | 01/Jun/2015:14:06:18 +0000 | /downloads/product_1 | 200  |\n| 204.197.211.70 | 24/May/2015:06:05:02 +0000 | /downloads/product_2 | 200  |\n| 91.74.184.74   | 29/May/2015:14:05:17 +0000 | /downloads/product_2 | 403  |\n| 91.74.184.74   | 29/May/2015:15:05:43 +0000 | /downloads/product_2 | 403  |\n| 91.74.184.74   | 29/May/2015:22:05:53 +0000 | /downloads/product_2 | 403  |\n| 217.168.17.5   | 31/May/2015:02:05:16 +0000 | /downloads/product_2 | 200  |\n| 217.168.17.180 | 20/May/2015:23:05:22 +0000 | /downloads/product_2 | 200  |\n| 204.197.211.70 | 21/May/2015:02:05:34 +0000 | /downloads/product_2 | 200  |\n+----------------+----------------------------+----------------------+------+\nselect * from (select ip, date, url, ua, len, PERCENT_RANK() OVER (ORDER BY len) as len_percentile from nginx where code = 200 order by len desc) as t where t.len_percentile &gt; 0.5 and t.len_percentile &lt; 0.7 order by t.len_percentile desc limit 10\n+----------------+----------------------------+----------------------+-----------------------------+------+--------------------+\n| ip             | date                       | url                  | ua                          | len  | len_percentile     |\n+----------------+----------------------------+----------------------+-----------------------------+------+--------------------+\n| 54.229.83.18   | 26/May/2015:00:05:34 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |\n| 54.244.37.198  | 18/May/2015:10:05:39 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |\n| 67.132.206.254 | 29/May/2015:07:05:52 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |\n| 128.199.60.184 | 24/May/2015:00:05:09 +0000 | /downloads/product_1 | urlgrabber/3.10 yum/3.4.3   | 2592 | 0.6342190216041719 |\n| 54.173.6.142   | 27/May/2015:14:05:21 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |\n| 104.156.250.12 | 03/Jun/2015:11:06:51 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |\n| 115.198.47.126 | 25/May/2015:11:05:13 +0000 | /downloads/product_1 | urlgrabber/3.10 yum/3.4.3   | 2592 | 0.6342190216041719 |\n| 198.105.198.4  | 29/May/2015:07:05:34 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |\n| 107.23.164.80  | 31/May/2015:09:05:34 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.4.3  | 2592 | 0.6342190216041719 |\n| 108.61.251.29  | 31/May/2015:10:05:16 +0000 | /downloads/product_1 | urlgrabber/3.9.1 yum/3.2.29 | 2592 | 0.6342190216041719 |\n+----------------+----------------------------+----------------------+-----------------------------+------+--------------------+\n</code></pre><h2>小结</h2><p>今天我们介绍了如何使用 Rust 处理存放在关系数据库中的结构化数据，以及存放在文件系统中的半结构化数据。</p><p>虽然在工作中，我们不太会使用 arrow/datafusion 去创建某个“下一代”的数据处理平台，但拥有了处理半结构化数据的能力，可以解决很多非常实际的问题。</p><p>比如每隔 10 分钟扫描 Nginx / CDN，以及应用服务器过去 10 分钟的日志，找到某些非正常的访问，然后把该用户/设备的访问切断一阵子。这样的特殊需求，一般的数据平台很难处理，需要我们自己撰写代码来实现。此时，arrow/datafusion 这样的工具就很方便。</p><h3>思考题</h3><ol>\n<li>请你自己阅读 diesel 或者 sea-orm 的文档，然后尝试把我们直接用 sqlx 构建的用户注册/登录的功能使用 diesel 或者 sea-orm 实现。</li>\n<li>datafusion 不但支持 csv，还支持 ndJSON / parquet / avro 等数据类型。如果你公司的生产环境下有这些类型的半结构化数据，可以尝试着阅读相关文档，使用 datafusion 来读取和查询它们。</li>\n</ol><p>感谢你的收听。恭喜你完成了第44次Rust学习，打卡之旅马上就要结束啦，我们下节课见。</p>","comments":[{"had_liked":false,"id":325684,"user_name":"pedro","can_delete":false,"product_type":"c1","uid":1200704,"ip_address":"","ucode":"F40C839DDFD599","user_header":"https://static001.geekbang.org/account/avatar/00/12/52/40/e57a736e.jpg","comment_is_top":false,"comment_ctime":1639098470,"is_pvip":false,"replies":[{"id":"118821","content":"我也是特别忙，好几次都差一点断更。留言更是拖到现在才开始一点点回复，惭愧啊。","user_name":"作者回复","user_name_real":"编辑","uid":"1079375","ctime":1639805334,"ip_address":"","comment_id":325684,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23113934950","product_id":100085301,"comment_content":"最近工作很忙，经常加班，有时间抽空来看看，仍然收获颇丰。<br><br>看完今天这一章，有一种强烈的感受：这不仅是Rust编程第一课，很有可能也是唯一的一课，内容太丰富了。","like_count":6,"discussions":[{"author":{"id":1079375,"avatar":"https://static001.geekbang.org/account/avatar/00/10/78/4f/e74f870c.jpg","nickname":"Tyr","note":"","ucode":"EAAFC8063202E0","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539694,"discussion_content":"我也是特别忙，好几次都差一点断更。留言更是拖到现在才开始一点点回复，惭愧啊。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639805334,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325787,"user_name":"乌龙猹","can_delete":false,"product_type":"c1","uid":2739949,"ip_address":"","ucode":"43F94A0DEC54BE","user_header":"https://static001.geekbang.org/account/avatar/00/29/ce/ed/3dbe915b.jpg","comment_is_top":false,"comment_ctime":1639130185,"is_pvip":false,"replies":[{"id":"118817","content":"加油！除了学，还要大量地练习，尝试用 Rust 解决身边的小问题。","user_name":"作者回复","user_name_real":"编辑","uid":"1079375","ctime":1639804785,"ip_address":"","comment_id":325787,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5934097481","product_id":100085301,"comment_content":"课程进入到了尾声，感觉啥也没学到，感觉又学到了很多。整个课程内容夯实，体系结构清晰。值得反复品味。这是 Rust 编程的第一课，而真正的 Rust 之旅才刚刚开始。","like_count":2,"discussions":[{"author":{"id":1079375,"avatar":"https://static001.geekbang.org/account/avatar/00/10/78/4f/e74f870c.jpg","nickname":"Tyr","note":"","ucode":"EAAFC8063202E0","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539690,"discussion_content":"加油！除了学，还要大量地练习，尝试用 Rust 解决身边的小问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639804785,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325734,"user_name":"罗杰","can_delete":false,"product_type":"c1","uid":1320487,"ip_address":"","ucode":"96BAFAA147341F","user_header":"https://static001.geekbang.org/account/avatar/00/14/26/27/eba94899.jpg","comment_is_top":false,"comment_ctime":1639109271,"is_pvip":false,"replies":[{"id":"118818","content":"嗯，同感","user_name":"作者回复","user_name_real":"编辑","uid":"1079375","ctime":1639804796,"ip_address":"","comment_id":325734,"utype":1}],"discussion_count":1,"race_medal":2,"score":"5934076567","product_id":100085301,"comment_content":"哈，我也觉得 ORM 太过笨重，还是 sqlx 直观。我的项目全部都是 sqlx，C++ 和 Go 都是。","like_count":2,"discussions":[{"author":{"id":1079375,"avatar":"https://static001.geekbang.org/account/avatar/00/10/78/4f/e74f870c.jpg","nickname":"Tyr","note":"","ucode":"EAAFC8063202E0","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539691,"discussion_content":"嗯，同感","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639804796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":347124,"user_name":"Geek_b251b3","can_delete":false,"product_type":"c1","uid":1784735,"ip_address":"","ucode":"20F2D9F03D6CF5","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/icicEO4n9yiacB9SjFfPsLfOUME0iccwXcYOq2xXiaIQu5cQtDFeEC0kwYQkXyVkW4oick67CjpjZkIUvVGZhMkFmCSA/132","comment_is_top":false,"comment_ctime":1653728319,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1653728319","product_id":100085301,"comment_content":"陈老师能点一下为什么使用pool的时候需要的是pool.clone()么","like_count":0},{"had_liked":false,"id":337897,"user_name":"Jagger","can_delete":false,"product_type":"c1","uid":1914208,"ip_address":"","ucode":"0BD13223AEE076","user_header":"https://static001.geekbang.org/account/avatar/00/1d/35/60/d3e723a7.jpg","comment_is_top":false,"comment_ctime":1647143342,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1647143342","product_id":100085301,"comment_content":"sqlx 连接MySQL8 存在8小时时差的问题，陈老师有没有什么好的办法解决？","like_count":0,"discussions":[{"author":{"id":2139074,"avatar":"https://static001.geekbang.org/account/avatar/00/20/a3/c2/7b3c67aa.jpg","nickname":"silence","note":"","ucode":"9F6EC234A94B93","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585245,"discussion_content":"这是时区相关问题，可能是数据当中是标准时间，但是本地是北京时间","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661417693,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":337474,"user_name":"Geek_994f3b","can_delete":false,"product_type":"c1","uid":2494458,"ip_address":"","ucode":"3E1FBCFD14DD58","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DoNwd7Fm9Ynsk0ZoHbfY4fXbwctf7SR6Jdyh7HIKwI2Dsgh56rT80ndpd4xeriareZ0MibdgiavicccFqjOztpMfqQ/132","comment_is_top":false,"comment_ctime":1646838694,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1646838694","product_id":100085301,"comment_content":"用户注册登录有什么最佳实践可以参考么","like_count":0},{"had_liked":false,"id":327749,"user_name":"Colt","can_delete":false,"product_type":"c1","uid":1117983,"ip_address":"","ucode":"7BDB5F469E325D","user_header":"https://static001.geekbang.org/account/avatar/00/11/0f/1f/e894ae27.jpg","comment_is_top":false,"comment_ctime":1640268099,"is_pvip":false,"replies":[{"id":"120808","content":"多谢！1) 我让编辑加一句，2) 已更新 github repo，也让编辑在文章中帮忙修改了。","user_name":"作者回复","user_name_real":"编辑","uid":"1079375","ctime":1642310783,"ip_address":"","comment_id":327749,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1640268099","product_id":100085301,"comment_content":"补充下:<br>运行example&#47;user 的例子需要注意下面2点:<br>1. 需要在example 同级目录下创建 data&#47;example.db 文件否则报 `unable to open database file`<br>2. `recreate_table` 方法下第一条sql 需要改为 `drop table if exists users` 否则首次运行会报 `no such table: users`","like_count":0,"discussions":[{"author":{"id":1079375,"avatar":"https://static001.geekbang.org/account/avatar/00/10/78/4f/e74f870c.jpg","nickname":"Tyr","note":"","ucode":"EAAFC8063202E0","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546451,"discussion_content":"多谢！1) 我让编辑加一句，2) 已更新 github repo，也让编辑在文章中帮忙修改了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642310784,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}