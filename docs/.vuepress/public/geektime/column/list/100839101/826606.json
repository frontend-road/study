{"id":826606,"title":"09｜从零实现一个角色扮演的聊天机器人","content":"<p>你好，我是郑晔！</p><p>前面我们介绍了 LangChain 最核心的抽象，相信你现在已经能够用 LangChain 完成一些一次性的简单任务了。从这一讲开始，我们会尝试开发一些大模型应用。通过这些应用，你会逐渐了解到常见的应用类型有哪些，以及如何使用 LangChain 开发这些应用。当然，我们还会遇到很多之前没有讲到过的 LangChain 抽象，我会结合开发的内容给你做一些介绍。</p><p>这一讲，我们就从最简单的聊天机器人开始讲起。</p><h2>简单的聊天机器人</h2><p>前面说过，ChatGPT 之所以火爆，很大程度上是拜聊天模式所赐，人们对于聊天模式的熟悉，降低了 ChatGPT 的理解门槛。开发一个好的聊天机器人并不容易，但开发一个聊天机器人，尤其是有了 LangChain 之后，还是很容易的。我们这一讲的目标就是开发一个简单的聊天机器人，它也会成为我们后面几讲的基础，你会看到一个聊天机器人是怎样逐渐变得强大起来。</p><p>出于简化的目的，我们的目标是打造一个命令行的聊天机器人。下面就是这个最简版聊天机器人的实现：</p><pre><code class=\"language-python\">from langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nchat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n\nwhile True:\n    user_input = input(\"You:&gt; \")\n    if user_input.lower() == 'exit':\n        break\n    stream = chat_model.stream([HumanMessage(content=user_input)])\n    for chunk in stream:\n        print(chunk.content, end='', flush=True)\n    print()\n</code></pre><!-- [[[read_end]]] --><p>在这段代码里，我们循环地等待用户输入，如果遇到了 exit 就退出。其中采用大模型的处理部分我们之前都讲过，理解起来应该不困难。</p><p>你可以把这段代码运行起来，和它聊聊。不过，聊上几句你就会发现一个问题：它完全记不住你们聊天的上下文。</p><pre><code class=\"language-bash\">You:&gt; 我是 dreamhead\n你好，Dreamhead！有什么我可以帮你的吗？\nYou:&gt; 我是谁？\n你是提问者，但我无法知道你的具体身份。如果你愿意，可以告诉我更多关于你的信息！\n</code></pre><p>为什么会这样？回忆一下 OpenAI API，它是一个 HTTP 请求。我们知道，HTTP 请求最大的特点是无状态，也就是说，服务端不会保留任何会话信息。对于每次都完成一个独立的任务，无状态是没有任何问题的。但对聊天机器人来说，就会出现对之前会话一无所知的情况。这显然无法满足我们对一个聊天机器人的基本预期，我们需要解决这个问题。</p><h2>能记事的聊天机器人</h2><p>既然 API 本身无法保留会话信息，一种常见的解决方案就是把聊天历史告诉大模型，帮助大模型了解之前的会话内容，也就是给大模型提供之前聊天的上下文。这样从用户的观感上看，这个聊天机器人就能按照我们之前聊天的内容持续对话。</p><p>我们当然可以自己维护聊天历史，在需要的时候，传递给大模型，但这么通用的需求，LangChain 已经为我们实现好了，下面就是一个例子：</p><pre><code class=\"language-python\">from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\nfrom langchain_core.runnables import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nchat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n\nstore = {}\n\ndef get_session_history(session_id: str) -&gt; BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    return store[session_id]\n\nwith_message_history = RunnableWithMessageHistory(chat_model, get_session_history)\n\nconfig = {\"configurable\": {\"session_id\": \"dreamhead\"}}\n\nwhile True:\n    user_input = input(\"You:&gt; \")\n    if user_input.lower() == 'exit':\n        break\n    stream = with_message_history.stream(\n        [HumanMessage(content=user_input)],\n        config=config\n    )\n    for chunk in stream:\n        print(chunk.content, end='', flush=True)\n    print()\n</code></pre><p>为了支持聊天历史，LangChain 引入了一个抽象叫 ChatMessageHistory。为了简单，我们这里使用了 InMemoryChatMessageHistory，也就是在内存存放的聊天历史。有了聊天历史，就需要把聊天历史和模型结合起来，这就是 RunnableWithMessageHistory 所起的作用，它就是一个把聊天历史和链封装到一起的一个类。</p><p>这里的 Runnable 是一个接口，它表示一个工作单元。我们前面说过，组成链是由一个一个的组件组成的。严格地说，这些组件都实现了 Runnable 接口，甚至链本身也实现了 Runnable 接口，我们之前讨论的 invoke、stream 等接口都是定义在 Runnable 里，可以说，Runnable 是真正的基础类型，<strong>LCEL 之所以能够以声明式的方式起作用，Runnable 接口是关键。</strong></p><p>不过，在真实的编码过程中，我们很少会直接面对 Runnable，大多数时候我们看见的都是各种具体类型。只是你会在很多具体类的名字中见到 Runnable，这里的 RunnableWithMessageHistory 就是其中一个。</p><p>你会发现我们传给 RunnableWithMessageHistory 的参数是一个函数，也就是这里的  <code>get_session_history</code>，正如它的名字所示，它会获取会话的历史。如果我们在开发的是一个多人聊天的应用，每个人就是一个不同的会话，在聊天时，我们需要获取到自己的聊天历史。在我们这个实现里，获取聊天历史的依据是 <code>session_id</code>。<code>get_session_history</code> 的实现很简单，如果不存在这个 session_id 对应的聊天历史，就创建一个新的，如果存在，就返回已有的。</p><p>其它的代码大多数与之前一样，只是用封装了聊天历史的对象（ <code>with_message_history</code>）代替了直接的模型，另外，在调用 <code>stream</code> 时，我还传入了一个配置，这个配置里存放了一个 <code>session_id</code>，它就会在调用 <code>get_session_history</code> 时作为参数传过去。</p><p>有了聊天历史，我们再和这个聊天机器人聊天，它看上去就正常多了：</p><pre><code class=\"language-bash\">You:&gt; 我是 dreamhead\n你好，dreamhead！有什么我可以帮助你的吗？\nYou:&gt; 我是谁?\n你是“dreamhead”，这是你刚刚告诉我的名字。如果你想分享更多关于自己或者你的兴趣爱好，我很乐意听！\n</code></pre><h2>角色扮演的聊天机器人</h2><p>现在我们拥有了一个能和我们正常沟通的聊天机器人，但这个聊天机器人本身并不具备任何能力。你应该看过让 AI 扮演某个人物角色，在对话的全过程中，它都会以这个角色的口吻来进行回复，而无需我们做任何额外的设置。接下来，我们就来实现这样一个聊天机器人，在这个例子里，我们实现的是一个 AI 孔子：</p><pre><code class=\"language-python\">from langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nchat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n\nstore = {}\n\ndef get_session_history(session_id: str) -&gt; BaseChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    return store[session_id]\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"你现在扮演孔子的角色，尽量按照孔子的风格回复，不要出现‘子曰’\",\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nwith_message_history = RunnableWithMessageHistory(\n    prompt | chat_model,\n    get_session_history\n)\n\nconfig = {\"configurable\": {\"session_id\": \"dreamhead\"}}\n\n\nwhile True:\n    user_input = input(\"You:&gt; \")\n    if user_input.lower() == 'exit':\n        break\n    stream = with_message_history.stream(\n        {\"messages\": [HumanMessage(content=user_input)]},\n        config=config\n    )\n    for chunk in stream:\n        print(chunk.content, end='', flush=True)\n    print()\n</code></pre><p>如果让聊天机器人可以扮演一个角色，本质上就是我们把设定好的提示词每次都发送给大模型，根据我们上一讲的内容，把固定的提示词和用户每次的输入分开，这就是提示词模板（PromptTemplate）发挥的作用。在这个例子里面，我们就设定了一个提示词，让聊天机器人能够扮演孔子的角色：</p><pre><code class=\"language-python\">prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"你现在扮演孔子的角色，尽量按照孔子的风格回复，不要出现‘子曰’\",\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n</code></pre><p>我们在前面已经介绍过提示词模板的用法，这里新增的就是 MessagesPlaceholder，它是一个占位符，变量名是  <code>messages</code>，可以替换成一个消息列表。相比于单个变量，消息列表给了用户更多灵活处理的空间。所以，我们给模型传递参数的时候，也使用了消息列表：</p><pre><code class=\"language-python\"> {\"messages\": [HumanMessage(content=user_input)]}\n</code></pre><p>另外，组装链的过程是我们先把提示词模板和模型组装成一个链，然后，再用 RunnableWithMessageHistory 去封装它。正如我前面所说，链也是一个 Runnable。</p><pre><code class=\"language-python\">with_message_history = RunnableWithMessageHistory(\n    prompt | chat_model,\n    get_session_history\n)\n</code></pre><p>下面是我和这个聊天机器人的一段对话，可以看到，它已经能按照我们的设定回答相应的问题了：</p><pre><code class=\"language-bash\">You:&gt; 我是 dreamhead，我现在有些迷茫\n\n\n迷茫者，心中若有疑惑，宜静心思之。人生如行路，前方虽有曲折，然若能明确所求，必能找到方向。可与友人交流，或于书中寻智慧，皆可助于明理。切记，迷茫乃常态，然勤思与行之，方可渐入佳境。\n\n\nYou:&gt; 你能就我的情况，给我写几句话么，称呼就写我的名字\n\n\nDreamhead，人生之路，有时迷雾重重，然心中若有明灯，便无惧前行。可静心思考，问己所求，亦可倾听他人之见，方能拨云见日。愿你在探索中，渐悟自我，明亮未来。\n</code></pre><p>在实现聊天机器人的过程中，还有一个很现实的问题，我们需要处理一下。如果不加任何限制，所有的聊天历史都会附加到新的会话中，随着聊天的进行，聊天历史很快就会超过大模型的上下文窗口大小。一种典型处理办法是，对聊天历史进行限制。</p><p>LangChain 提供了一个 <code>trim_messages</code> 用来控制消息的规模，它提供了很多控制消息规模的参数：</p><ul>\n<li>max_tokens，限制最大的 Token 数量。</li>\n<li>strategy，限制的策略，从前面保留（first），还是从后面保留（last）。</li>\n<li>allow_partial，是否允许把拆分消息。</li>\n<li>include_system，是否要保留最开始的系统提示词。</li>\n</ul><p>这其中最关键的是就是<strong>max_tokens</strong>，这也是我们限制消息规模的主要原因。不过，这里是按照 Token 进行计算，我们该怎么计算 Token 呢？对 OpenAI API 来说，一种常见的解决方案是采用 tiktoken，这是一个专门用来处理的 Token 的程序库。下面是采用了 tiktoken 的计算 Token 的实现：</p><pre><code class=\"language-python\">from typing import List\nimport tiktoken\nfrom langchain_core.messages import SystemMessage, trim_messages, BaseMessage, HumanMessage, AIMessage, ToolMessage\n\ndef str_token_counter(text: str) -&gt; int:\n    enc = tiktoken.get_encoding(\"o200k_base\")\n    return len(enc.encode(text))\n\ndef tiktoken_counter(messages: List[BaseMessage]) -&gt; int:\n    num_tokens = 3\n    tokens_per_message = 3\n    tokens_per_name = 1\n    for msg in messages:\n        if isinstance(msg, HumanMessage):\n            role = \"user\"\n        elif isinstance(msg, AIMessage):\n            role = \"assistant\"\n        elif isinstance(msg, ToolMessage):\n            role = \"tool\"\n        elif isinstance(msg, SystemMessage):\n            role = \"system\"\n        else:\n            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n        num_tokens += (\n                tokens_per_message\n                + str_token_counter(role)\n                + str_token_counter(msg.content)\n        )\n        if msg.name:\n            num_tokens += tokens_per_name + str_token_counter(msg.name)\n    return num_tokens\n</code></pre><p>有了这些基础之后，我们就可以改造一下角色扮演机器人：</p><pre><code class=\"language-python\">from langchain_core.messages import trim_messages\n\ntrimmer = trim_messages(\n    max_tokens=4096,\n    strategy=\"last\",\n    token_counter=tiktoken_counter,\n    include_system=True,\n)\n\nwith_message_history = RunnableWithMessageHistory(\n    trimmer | prompt | chat_model,\n    get_session_history\n)\n</code></pre><p>在这里，<code>trim_messages</code> 设置了最大 Token 数（max_tokens=4096），从后面保留消息（strategy=“last”），Token 计数方式设置成我们前面编写的 <code>tiktoken_counter</code>。另外，因为我们是角色扮演的聊天机器人，第一句的系统消息就是角色设置，我们选择保留它（include_system=True）。</p><p><code>trim_messages</code> 的参数如果有消息，它会直接按照规则处理消息，如果没有消息，它就会生成一个用来处理消息的组件。在这个例子里，我们只设置了处理规则，所以，它就生成了一个处理消息的组件，我们把它串到我们的链里：</p><pre><code class=\"language-python\">trimmer | prompt | chat_model\n</code></pre><p>到这里，我们已经拥有了一个可以正常运行的聊天机器人了，你可以和它好好聊聊了！</p><h2>总结时刻</h2><p>这一讲，我们构建了一个命令行版的聊天机器人。我们可以通过 ChatMessageHistory 管理聊天历史，然后用 RunnableWithMessageHistory 把它和我们编写的链结合起来。</p><p>我们还实现了一个角色扮演类的聊天机器人，关键点就是将提示词模板（PromptTemplate）结合进来。在实际的使用中，我们需要限制传给大模型的消息规模，通过 <code>trim_messages</code> 就可以完成限制消息规模的任务。</p><p>如果今天的内容你只能记住一件事，那请记住，<strong>实现一个聊天机器人的关键点就是管理好聊天历史</strong>。</p><h2>练习题</h2><p>这一讲我们的聊天机器人用了 InMemoryChatMessageHistory，把聊天历史存放到内存，也就是每次重启就相当于全新的聊天机器人。我希望你能把它改造成一个可以持久化的聊天历史，让它变得更加实用，欢迎你把改造的心得分享在留言区。</p>","neighbors":{"left":{"article_title":"08｜LangChain：核心抽象","id":825655},"right":{"article_title":"10｜RAG：让大模型知道更多东西","id":827289}},"comments":[{"had_liked":false,"id":395827,"user_name":"晴天了","can_delete":false,"product_type":"c1","uid":1359669,"ip_address":"北京","ucode":"5310E02F8DF1F0","user_header":"https://static001.geekbang.org/account/avatar/00/14/bf/35/0e3a92a7.jpg","comment_is_top":false,"comment_ctime":1732185015,"is_pvip":false,"replies":[{"id":143781,"content":"这个调用关系写得很好！","user_name":"作者回复","user_name_real":"编辑","uid":1258861,"ctime":1732692877,"ip_address":"陕西","comment_id":395827,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"个人理解的结构\n\n大模型返回的消息 =  历史运行器(大模型,  历史存储逻辑).调用( [人类消息], 人类会话标识).","like_count":4,"discussions":[{"author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":654432,"discussion_content":"这个调用关系写得很好！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1732692877,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395973,"user_name":"Geek_682837","can_delete":false,"product_type":"c1","uid":1715823,"ip_address":"广东","ucode":"55C29E9EDD9265","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIaxhA2xdIRKQ0rXZhrjcWPYp4hR1mjST6lcBeuYTg0Omf0hyREfr8VH8JWXEyEVq82ht7wdzWsyA/132","comment_is_top":false,"comment_ctime":1732551458,"is_pvip":false,"replies":[{"id":143783,"content":"我用到版本是 0.3.0","user_name":"作者回复","user_name_real":"编辑","uid":1258861,"ctime":1732692941,"ip_address":"陕西","comment_id":395973,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"这里langchain的版本是多少？用kimi的Moonshot遇到这个错了，Error in RootListenersTracer.on_llm_end callback: KeyError(&#39;message&#39;)，怎么解决？","like_count":0,"discussions":[{"author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":654434,"discussion_content":"我用到版本是 0.3.0","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1732692941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395800,"user_name":"晴天了","can_delete":false,"product_type":"c1","uid":1359669,"ip_address":"北京","ucode":"5310E02F8DF1F0","user_header":"https://static001.geekbang.org/account/avatar/00/14/bf/35/0e3a92a7.jpg","comment_is_top":false,"comment_ctime":1732102500,"is_pvip":false,"replies":[{"id":143751,"content":"Go 版本我还真没了解过，不是官方维护的，估计会有不少差异","user_name":"作者回复","user_name_real":"编辑","uid":1258861,"ctime":1732315606,"ip_address":"陕西","comment_id":395800,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"langchain 用golang版本也没关系吧 和教程不冲突吧\n\nhttps:&#47;&#47;github.com&#47;tmc&#47;langchaingo","like_count":0,"discussions":[{"author":{"id":1258861,"avatar":"https://static001.geekbang.org/account/avatar/00/13/35/6d/07a42f81.jpg","nickname":"郑晔","note":"","ucode":"1EBD5AA5D4FC89","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":654280,"discussion_content":"Go 版本我还真没了解过，不是官方维护的，估计会有不少差异","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1732315606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395811,"user_name":"willmyc","can_delete":false,"product_type":"c1","uid":1016703,"ip_address":"广东","ucode":"486B28F89DFBEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/83/7f/7b1f3f68.jpg","comment_is_top":false,"comment_ctime":1732155582,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100839101,"comment_content":"老师，你好！第三段代码中好像还需要导入如下的包才能正常运行：from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder","like_count":1}]}