{"id":103750,"title":"29 | 如何测试Beam Pipeline？","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“如何测试Beam Pipeline”。</p><p>在上一讲中，我们结合了第7讲的内容，一起学习了在Beam的世界中我们该怎么设计好对应的设计模式。而在今天这一讲中，我想要讲讲在日常开发中经常会被忽略的，但是又非常重要的一个开发环节——测试。</p><p>你知道，我们设计好的Beam数据流水线通常都会被放在分布式环境下执行，具体每一步的Transform都会被分配到任意的机器上面执行。如果我们在运行数据流水线时发现结果出错了，那么想要定位到具体的机器，再到上面去做调试是不现实的。</p><p>当然还有另一种方法，读取一些样本数据集，再运行整个数据流水线去验证哪一步逻辑出错了。但这是一项非常耗时耗力的工作。即便我们可以把样本数据集定义得非常小，从而缩短运行数据流水线运行所需的时间。但是万一我们所写的是多步骤数据流水线的话，就不知道到底在哪一步出错了，我们必须把每一步的中间结果输出出来进行调试。</p><p>基于以上种种的原因，在我们正式将数据流水线放在分布式环境上面运行之前，先完整地测试好整个数据流水线逻辑，就变得尤为重要了。</p><p>为了解决这些问题，Beam提供了一套完整的测试SDK。让我们可以在开发数据流水线的同时，能够实现对一个Transform逻辑的单元测试，也可以对整个数据流水线端到端（End-to-End）地测试。</p><!-- [[[read_end]]] --><p>在Beam所支持的各种Runners当中，有一个Runner叫作DirectRunner。DirectRunner其实就是我们的本地机器。也就是说，如果我们指定Beam的Runner为DirectRunner的话，整个Beam数据流水线都会放在本地机器上面运行。我们在运行测试程序的时候可以利用这个DirectRunner来跑测试逻辑。</p><p>在正式讲解之前，有一点是我需要提醒你的。如果你喜欢自行阅读Beam的相关技术文章或者是示例代码的话，可能你会看见一些测试代码使用了在Beam SDK中的一个测试类，叫作DoFnTester来进行单元测试。这个DoFnTester类可以让我们传入一个用户自定义的函数（User Defined Function/UDF）来进行测试。</p><p>通过<a href=\"https://time.geekbang.org/column/article/101735\">第25讲</a>的内容我们已经知道，一个最简单的Transform可以用一个ParDo来表示，在使用它的时候，我们需要继承DoFn这个抽象类。这个DoFnTester接收的对象就是我们继承实现的DoFn。在这里，我们把一个DoFn看作是一个单元来进行测试了。但这并不是Beam所提倡的。</p><p>因为在Beam中，数据转换的逻辑都是被抽象成Transform，而不是Transform里面的ParDo这些具体的实现。<strong>每个Runner具体怎么运行这些ParDo</strong>，<strong>对于用户来说应该都是透明的。</strong>所以，在Beam的2.4.0版本之后，Beam SDK将这个类标记成了Deprecated，转而推荐使用Beam SDK中的TestPipeline。</p><p>所以，我在这里也建议你，在写测试代码的时候，不要使用任何和DoFnTester有关的SDK。</p><h2>Beam的Transform单元测试</h2><p>说完了注意事项，那事不宜迟，我们就先从一个Transform的单元测试开始，看看在Beam是如何做测试的（以下所有的测试示例代码都是以Java为编程语言来讲解）。</p><p>一般来说，Transform的单元测试可以通过以下五步来完成：</p><ol>\n<li>创建一个Beam测试SDK中所提供的TestPipeline实例。</li>\n<li>创建一个静态（Static）的、用于测试的输入数据集。</li>\n<li>使用Create Transform来创建一个PCollection作为输入数据集。</li>\n<li>在测试数据集上调用我们需要测试的Transform上并将结果保存在一个PCollection上。</li>\n<li>使用PAssert类的相关函数来验证输出的PCollection是否是我所期望的结果。</li>\n</ol><p>假设我们要处理的数据集是一个整数集合，处理逻辑是过滤掉数据集中的奇数，将输入数据集中的偶数输出。为此，我们通过继承DoFn类来实现一个产生偶数的Transform，它的输入和输出数据类型都是Integer。</p><p>Java</p><pre><code>static class EvenNumberFn extends DoFn&lt;Integer, Integer&gt; {\n @ProcessElement\n public void processElement(@Element Integer in, OutputReceiver&lt;Integer&gt; out) {\n   if (in % 2 == 0) {\n     out.output(in);\n   }\n }\n}\n</code></pre><p>那我们接下来就根据上面所讲的测试流程，测试这个EvenNumerFn Transform，来一步步创建我们的单元测试。</p><h3>创建TestPipeline</h3><p>第一步，创建TestPipeline。创建一个TestPipeline实例的代码非常简单，示例如下：</p><p>Java</p><pre><code>...\nPipeline p = TestPipeline.create();\n...\n</code></pre><p>如果你还记得在<a href=\"https://time.geekbang.org/column/article/102182\">第26讲</a>中如何创建数据流水线的话，可以发现，TestPipeline实例的创建其实不用给这个TestPipeline定义选项（Options）。因为TestPipeline中create函数已经在内部帮我们创建好一个测试用的Options了。</p><h3>创建静态输入数据集</h3><p>Java</p><pre><code>...\nstatic final List&lt;Integer&gt; INPUTS = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);\n...\n</code></pre><p>第二步，创建静态的输入数据集。创建静态的输入数据集的操作就和我们平时所写的普通Java代码一样，在示例中，我调用了Arrays类的asList接口来创建一个拥有10个整数的数据集。</p><h3>使用Create Transform创建PCollection</h3><p>在创建完静态数据集后，我们进入第三步，创建一个PCollection作为输入数据集。在Beam原生支持的Transform里面，有一种叫作Create Transform，我们可以利用这个Create Transform将Java Collection的数据转换成为Beam的数据抽象PCollection，具体的做法如下：</p><p>Java</p><pre><code>...\nPCollection&lt;Integer&gt; input = p.apply(Create.of(INPUTS)).setCoder(VarIntCoder.of());\n...\n</code></pre><h3>调用Transform处理逻辑</h3><p>第四步，调用Transform处理逻辑。有了数据抽象PCollection，我们就需要在测试数据集上调用我们需要测试的Transform处理逻辑，并将结果保存在一个PCollection上。</p><p>Java</p><pre><code>...\nPCollection&lt;String&gt; output = input.apply(ParDo.of(new EvenNumberFn()));\n...\n</code></pre><p>根据<a href=\"https://time.geekbang.org/column/article/101735\">第25讲</a>的内容，我们只需要在这个输入数据集上调用apply抽象函数，生成一个需要测试的Transform，并且传入apply函数中就可以了。</p><h3>验证输出结果</h3><p>第五步，验证输出结果。在验证结果的阶段，我们需要调用PAssert类中的函数来验证输出结果是否和我们期望的一致，示例如下。</p><p>Java</p><pre><code>...\nPAssert.that(output).containsInAnyOrder(2, 4, 6, 8, 10);\n...\n</code></pre><p>完成了所有的步骤，我们就差运行这个测试的数据流水线了。很简单，就是调用TestPipeline的run函数，整个Transform的单元测试示例如下：</p><p>Java</p><pre><code>final class TestClass {\n    static final List&lt;Integer&gt; INPUTS = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);\n\n    public void testFn() {\n      Pipeline p = TestPipeline.create();\n      PCollection&lt;Integer&gt; input = p.apply(Create.of(INPUTS)).setCoder(VarIntCoder.of());\n      PCollection&lt;String&gt; output = input.apply(ParDo.of(new EvenNumberFn()));\n      PAssert.that(output).containsInAnyOrder(2, 4, 6, 8, 10);\n      p.run();\n    }\n}\n</code></pre><p>有一点需要注意的是，TestPipeline的run函数是在单元测试的结尾处调用的，PAssert的调用必须在TestPipeliner调用run函数之前调用。</p><h2>Beam的端到端测试</h2><p>在一般的现实应用中，我们设计的都是多步骤数据流水线，就拿我在<a href=\"https://time.geekbang.org/column/article/90081\">第一讲</a>中举到的处理美团外卖电动车的图片为例子，其中就涉及到了多个输入数据集，而结果也有可能会根据实际情况有多个输出。</p><p>所以，我们在做测试的时候，往往希望能有一个端到端的测试。在Beam中，端到端的测试和Transform的单元测试非常相似。唯一的不同点在于，我们要为所有的输入数据集创建测试数据集，而不是只针对某一个Transform来创建。对于在数据流水线的每一个应用到Write Transfrom的地方，我们都需要用到PAssert类来验证输出数据集。</p><p>所以，端到端测试的步骤也分五步，具体内容如下：</p><ol>\n<li>创建一个Beam测试SDK中所提供的TestPipeline实例。</li>\n<li>对于多步骤数据流水线中的每个输入数据源，创建相对应的静态（Static）测试数据集。</li>\n<li>使用Create Transform，将所有的这些静态测试数据集转换成PCollection作为输入数据集。</li>\n<li>按照真实数据流水线逻辑，调用所有的Transforms操作。</li>\n<li>在数据流水线中所有应用到Write Transform的地方，都使用PAssert来替换这个Write Transform，并且验证输出的结果是否我们期望的结果相匹配。</li>\n</ol><p>为了方便说明，我们就在之前的例子中多加一步Transform和一个输出操作来解释如何写端到端测试。假设，我们要处理数据集是一个整数集合，处理逻辑是过滤掉奇数，将输入数据集中的偶数转换成字符串输出。同时，我们也希望对这些偶数求和并将结果输出，示例如下：</p><p>Java</p><pre><code>final class Foo {\n  static class EvenNumberFn extends DoFn&lt;Integer, Integer&gt; {\n    @ProcessElement\n    public void processElement(@Element Integer in, OutputReceiver&lt;Integer&gt; out) {\n      if (in % 2 == 0) {\n        out.output(in);\n      }\n    }\n  }\n\n  static class ParseIntFn extends DoFn&lt;String, Integer&gt; {\n    @ProcessElement\n    public void processElement(@Element String in, OutputReceiver&lt;Integer&gt; out) {\n      out.output(Integer.parseInt(in));\n    }\n  }\n\n  public static void main(String[] args) {\n    PipelineOptions options = PipelineOptionsFactory.create();\n    Pipeline p = Pipeline.create(options);\n    PCollection&lt;Integer&gt; input = p.apply(TextIO.read().from(&quot;filepath/input&quot;)).apply(ParDo.of(new ParseIntFn()));\n    PCollection&lt;Integer&gt; output1 = input.apply(ParDo.of(new EvenNumberFn()));\n    output1.apply(ToString.elements()).apply(TextIO.write().to(&quot;filepath/evenNumbers&quot;));\n    PCollection&lt;Integer&gt; sum = output1.apply(Combine.globally(new SumInts()));\n    sum.apply(ToString.elements()).apply(TextIO.write().to(&quot;filepath/sum&quot;));\n    p.run();\n  }\n}\n</code></pre><p>从上面的示例代码中你可以看到，我们从一个外部源读取了一系列输入数据进来，将它转换成了整数集合。同时，将我们自己编写的EvenNumberFn Transform应用在了这个输入数据集上。得到了所有偶数集合之后，我们先将这个中间结果输出，然后再针对这个偶数集合求和，最后将这个结果输出。</p><p>整个数据流水线总共有一次对外部数据源的读取和两次的输出，我们按照端到端测试的步骤，为所有的输入数据集创建静态数据，然后将所有有输出的地方都使用PAssert类来进行验证。整个测试程序如下所示：</p><p>Java</p><pre><code>final class TestClass {\n\n  static final List&lt;String&gt; INPUTS =\n      Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;);\n\n  static class EvenNumberFn extends DoFn&lt;Integer, Integer&gt; {\n    @ProcessElement\n    public void processElement(@Element Integer in, OutputReceiver&lt;Integer&gt; out) {\n      if (in % 2 == 0) {\n        out.output(in);\n      }\n    }\n  }\n\n  static class ParseIntFn extends DoFn&lt;String, Integer&gt; {\n    @ProcessElement\n    public void processElement(@Element String in, OutputReceiver&lt;Integer&gt; out) {\n      out.output(Integer.parseInt(in));\n    }\n  }\n\n  public void testFn() {\n    Pipeline p = TestPipeline.create();\n    PCollection&lt;String&gt; input = p.apply(Create.of(INPUTS)).setCoder(StringUtf8Coder.of());\n    PCollection&lt;Integer&gt; output1 = input.apply(ParDo.of(new ParseIntFn())).apply(ParDo.of(new EvenNumberFn()));\n    PAssert.that(output1).containsInAnyOrder(2, 4, 6, 8, 10);\n    PCollection&lt;Integer&gt; sum = output1.apply(Combine.globally(new SumInts()));\n    PAssert.that(sum).is(30);\n    p.run();\n  }\n}\n</code></pre><p>在上面的示例代码中，我们用TestPipeline替换了原来的Pipeline，创建了一个静态输入数据集并用Create Transform转换成了PCollection，最后将所有用到Write Transform的地方都用PAssert替换掉，来验证输出结果是否是我们期望的结果。</p><h2>小结</h2><p>今天我们一起学习了在Beam中写编写测试逻辑的两种方式，分别是针对一个Transform的单元测试和针对整个数据流水线的端到端测试。Beam提供的SDK能够让我们不需要在分布式环境下运行程序而是本地机器上运行。测试在整个开发环节中是非常的一环，我强烈建议你在正式上线自己的业务逻辑之前先对此有一个完整的测试。</p><h2>思考题</h2><p>如果让你来利用Beam SDK来测试你日常处理的数据逻辑，你会如何编写测试呢？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":109614,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1562066583,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27331870359","product_id":100025301,"comment_content":"我期望是基于spark或flink讲解的重实践思想，轻知识，这个可以自己下去学","like_count":7},{"had_liked":false,"id":132362,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1568097764,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5863065060","product_id":100025301,"comment_content":"看来我得先去学学Java了，不会java，看不太懂。","like_count":2},{"had_liked":false,"id":108839,"user_name":"Ming","can_delete":false,"product_type":"c1","uid":1516011,"ip_address":"","ucode":"69BB73B8AB7E3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/21/eb/bb2e7a3b.jpg","comment_is_top":false,"comment_ctime":1561898565,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5856865861","product_id":100025301,"comment_content":"我觉得测试的话，相对麻烦的地方还是在工程脚手架的设计上。<br>显然代码本身要抽象封装好，确保测试能覆盖生产代码，而不是生产代码的“拷贝”。<br>但有些在代码之外的问题让我挺好奇的：<br>如何保证测试数据的格式和生产数据的格式同步？<br>流处理的测试怎么模拟时间？<br>团队是如何在流程上确保pipeline必须经过测试才能运行的，是通过CI&#47;CD来自动执行pipeline？还是往往通过人力把关？<br>","like_count":2},{"had_liked":false,"id":243421,"user_name":"之渊","can_delete":false,"product_type":"c1","uid":1876212,"ip_address":"","ucode":"02B9299DBB4881","user_header":"https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg","comment_is_top":false,"comment_ctime":1598101932,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1598101932","product_id":100025301,"comment_content":"我觉得挺好的，对于我们新手来说很友好。<br>谁都是从新手过来的，感谢大佬对新手的支持。<br>代码示例：<br>https:&#47;&#47;gitee.com&#47;oumin12345&#47;daimademojihe&#47;tree&#47;master&#47;cloudx&#47;bigdata&#47;src&#47;main&#47;java&#47;test&#47;beam","like_count":0},{"had_liked":false,"id":151384,"user_name":"杰洛特","can_delete":false,"product_type":"c1","uid":1098146,"ip_address":"","ucode":"46D0574654F8AC","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqEacia8yO1dR5Tal9B7w8PzTRrViajlAvDph96OqcuBGe29icbXOibhibGmaBcO7BfpVia0Y8ksZwsuAYQ/132","comment_is_top":false,"comment_ctime":1573711631,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1573711631","product_id":100025301,"comment_content":"TestClass 里的这个 PCollection&lt;String&gt; output = input.apply(ParDo.of(new EvenNumberFn())); 里面的泛型是不是写错了？偶数是 Integer 吧？","like_count":0,"discussions":[{"author":{"id":1730052,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/66/04/22190a73.jpg","nickname":"weiming","note":"","ucode":"3EC2968FB08501","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545807,"discussion_content":"同问 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642055774,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":111639,"user_name":"李孟","can_delete":false,"product_type":"c1","uid":1006768,"ip_address":"","ucode":"AD2349CB12F130","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5c/b0/77e5f8c8.jpg","comment_is_top":false,"comment_ctime":1562577201,"is_pvip":true,"replies":[{"id":"42064","content":"谢谢你的提问！有Beam SQL，不过现在只支持Java。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1563507509,"ip_address":"","comment_id":111639,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1562577201","product_id":100025301,"comment_content":"Beam 有类似sparksql的api吗？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":457398,"discussion_content":"谢谢你的提问！有Beam SQL，不过现在只支持Java。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563507509,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]}]}