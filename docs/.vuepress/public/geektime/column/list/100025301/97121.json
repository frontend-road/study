{"id":97121,"title":"17 | Structured Streaming：如何用DataFrame API进行实时数据分析?","content":"<p>你好，我是蔡元楠。</p><p>上一讲中，我们介绍了Spark中的流处理库Spark Streaming。它将无边界的流数据抽象成DStream，按特定的时间间隔，把数据流分割成一个个RDD进行批处理。所以，DStream API与RDD API高度相似，也拥有RDD的各种性质。</p><p>在第15讲中，我们比较过RDD和DataSet/DataFrame。你还记得DataSet/DataFrame的优点吗？你有没有想过，既然已经有了RDD API，我们为什么还要引入DataSet/DataFrame呢？</p><p>让我们来回顾一下DataSet/DataFrame的优点（为了方便描述，下文中我们统一用DataFrame来代指DataSet和DataFrame）：</p><ul>\n<li>DataFrame 是<strong>高级API</strong>，提供类似于<strong>SQL</strong>的query接口，方便熟悉关系型数据库的开发人员使用；</li>\n<li><strong>Spark SQL执行引擎会自动优化DataFrame程序</strong>，而用RDD API开发的程序本质上需要工程师自己构造RDD的DAG执行图，所以依赖于工程师自己去优化。</li>\n</ul><p>那么我们自然会想到，如果可以拥有一个基于DataFrame API的流处理模块，作为工程师的我们就不需要去用相对low level的DStream API去处理无边界数据，这样会大大提升我们的开发效率。</p><!-- [[[read_end]]] --><p>基于这个思想，2016年，Spark在其2.0版本中推出了结构化流数据处理的模块Structured Streaming。</p><p>Structured Streaming是基于Spark SQL引擎实现的，依靠Structured Streaming，在开发者眼里，流数据和静态数据没有区别。我们完全可以像批处理静态数据那样去处理流数据。随着流数据的持续输入，Spark SQL引擎会帮助我们持续地处理新数据，并且更新计算结果。</p><p>今天，就让我们来一起学习Structured Streaming的原理以及应用。</p><h2>Structured Streaming模型</h2><p>流数据处理最基本的问题就是如何对不断更新的无边界数据建模。</p><p>之前讲的Spark Streaming就是把流数据按一定的时间间隔分割成许多个小的数据块进行批处理。在Structured Streaming的模型中，我们要把数据看成一个无边界的关系型的数据表。每一个数据都是表中的一行，不断会有新的数据行被添加到表里来。我们可以对这个表做任何类似批处理的查询，Spark会帮我们不断对新加入的数据进行处理，并更新计算结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/37/bb1845be9f34ef7d232a509f90ae0337.jpg?wh=2152*1262\" alt=\"\"></p><p>与Spark Streaming类似，Structured Streaming也是将输入的数据流按照时间间隔（以一秒为例）划分成数据段。每一秒都会把新输入的数据添加到表中，Spark也会每秒更新输出结果。输出结果也是表的形式，输出表可以写入硬盘或者HDFS。</p><p>这里我要介绍一下Structured Streaming的三种输出模式。</p><ol>\n<li>完全模式（Complete Mode）：整个更新过的输出表都被写入外部存储；</li>\n<li>附加模式（Append Mode）：上一次触发之后新增加的行才会被写入外部存储。如果老数据有改动则不适合这个模式；</li>\n<li>更新模式（Update Mode）：上一次触发之后被更新的行才会被写入外部存储。</li>\n</ol><p>需要注意的是，Structured Streaming并不会完全存储输入数据。每个时间间隔它都会读取最新的输入，进行处理，更新输出表，然后把这次的输入删除。Structured Streaming只会存储更新输出表所需要的信息。</p><p>Structured Streaming的模型在根据事件时间（Event Time）处理数据时十分方便。</p><p>我们在第六讲中曾经讲过事件时间和处理时间（Processing Time）的区别。这里我再简单说一下。事件时间指的是事件发生的时间，是数据本身的属性；而处理时间指的是Spark接收到数据的时间。</p><p>很多情况下，我们需要基于事件时间来处理数据。比如说，统计每个小时接到的订单数量，一个订单很有可能在12:59被创建，但是到了13:01才被处理。</p><p>在Structured Streaming的模型中，由于每个数据都是输入数据表中的一行，那么事件时间就是行中的一列。依靠DataFrame API提供的类似于SQL的接口，我们可以很方便地执行基于时间窗口的查询。</p><h2>Streaming DataFrame API</h2><p>在Structured Streaming发布以后，DataFrame既可以代表静态的有边界数据，也可以代表无边界数据。之前对静态DataFrame的各种操作同样也适用于流式DataFrame。接下来，让我们看几个例子。</p><h3>创建DataFrame</h3><p>SparkSession.readStream()返回的DataStreamReader可以用于创建流DataFrame。它支持多种类型的数据流作为输入，比如文件、Kafka、socket等。</p><pre><code>socketDataFrame = spark\n   .readStream\n   .format(&quot;socket&quot;）\n   .option(&quot;host&quot;, &quot;localhost&quot;)\n   .option(&quot;port&quot;, 9999)\n   .load()\n</code></pre><p>上边的代码例子创建了一个DataFrame，用来监听来自localhost:9999的数据流。</p><h3>基本的查询操作</h3><p>流DataFrame同静态DataFrame一样，不仅支持类似SQL的查询操作（如select和where等），还支持RDD的转换操作（如map和filter）。 让我们一起来看下面的例子。</p><p>假设我们已经有一个DataFrame代表一个学生的数据流，即每个数据是一个学生，每个学生有名字（name）、年龄（age）、身高（height）和年级（grade）四个属性，我们可以用DataFrame API去做类似于SQL的Query。</p><pre><code>df = … // 这个DataFrame代表学校学生的数据流，schema是{name: string, age: number, height: number, grade: string}\ndf.select(&quot;name&quot;).where(&quot;age &gt; 10&quot;) // 返回年龄大于10岁的学生名字列表\ndf.groupBy(&quot;grade&quot;).count() // 返回每个年级学生的人数\ndf.sort_values([‘age’], ascending=False).head(100) //返回100个年龄最大的学生 \n</code></pre><p>在这个例子中，通过第二行我们可以得到所有年龄在10岁以上的学生名字，第三行可以得到每个年级学生的人数，第四行得到100个年龄最大的学生信息。此外，DataFrame还支持很多基本的查询操作，在此不做赘述。</p><p>我们还可以通过isStreaming函数来判断一个DataFrame是否代表流数据。</p><pre><code>df.isStreaming()\n</code></pre><h3></h3><p>基于事件时间的时间窗口操作</p><p>在学习Spark Streaming的时间窗口操作时，我们举过一个例子，是每隔10秒钟输出过去60秒的前十热点词。这个例子是基于处理时间而非事件时间的。</p><p>现在让我们设想一下，如果数据流中的每个词语都有一个时间戳代表词语产生的时间，那么要怎样实现，每隔10秒钟输出过去60秒内产生的前十热点词呢？你可以看看下边的代码。</p><pre><code>words = ...  #这个DataFrame代表词语的数据流，schema是 { timestamp: Timestamp, word: String}\n\n\nwindowedCounts = words.groupBy(\n   window(words.timestamp, &quot;1 minute&quot;, &quot;10 seconds&quot;),\n   words.word\n).count()\n.sort(desc(&quot;count&quot;))\n.limit(10)\n</code></pre><p>基于词语的生成时间，我们创建了一个窗口长度为1分钟，滑动间隔为10秒的window。然后，把输入的词语表根据window和词语本身聚合起来，并统计每个window内每个词语的数量。之后，再根据词语的数量进行排序，只返回前10的词语。</p><p>在Structured Streaming基于时间窗口的聚合操作中，groupBy是非常常用的。</p><h3>输出结果流</h3><p>当经过各种SQL查询操作之后，我们创建好了代表最终结果的DataFrame。下一步就是开始对输入数据流的处理，并且持续输出结果。</p><p>我们可以用Dataset.writeStream()返回的DataStreamWriter对象去输出结果。它支持多种写入位置，如硬盘文件、Kafka、console和内存等。</p><pre><code>query = wordCounts\n   .writeStream\n   .outputMode(&quot;complete&quot;)\n   .format(&quot;csv&quot;)\n   .option(&quot;path&quot;, &quot;path/to/destination/dir&quot;)\n   .start()\n\n\nquery.awaitTermination()\n</code></pre><p>在上面这个代码例子中，我们选择了完全模式，把输出结果流写入了CSV文件。</p><h2>Structured Streaming与Spark Streaming对比</h2><p>接下来，让我们对比一下Structured Streaming和上一讲学过的Spark Streaming。看看同为流处理的组件的它们各有什么优缺点。</p><h3>简易度和性能</h3><p>Spark Streaming提供的DStream API与RDD API很类似，相对比较低level。</p><p>当我们编写 Spark Streaming 程序的时候，本质上就是要去构造RDD的DAG执行图，然后通过 Spark Engine 运行。这样开发者身上的担子就很重，很多时候要自己想办法去提高程序的处理效率。这不是Spark作为一个数据处理框架想看到的。对于好的框架来说，开发者只需要专注在业务逻辑上，而不用操心别的配置、优化等繁杂事项。</p><p>Structured Streaming提供的DataFrame API就是这么一个相对高level的API，大部分开发者都很熟悉关系型数据库和SQL。<strong>这样的数据抽象可以让他们用一套统一的方案去处理批处理和流处理</strong>，不用去关心具体的执行细节。</p><p>而且，DataFrame API是在Spark SQL的引擎上执行的，Spark SQL有非常多的优化功能，比如执行计划优化和内存管理等，所以Structured Streaming的应用程序性能很好。</p><h3>实时性</h3><p>在上一讲中我们了解到，Spark Streaming是准实时的，它能做到的最小延迟在一秒左右。</p><p>虽然Structured Streaming用的也是类似的微批处理思想，每过一个时间间隔就去拿来最新的数据加入到输入数据表中并更新结果，但是相比起Spark Streaming来说，它更像是实时处理，能做到用更小的时间间隔，最小延迟在100毫秒左右。</p><p>而且在Spark 2.3版本中，Structured Streaming引入了连续处理的模式，可以做到真正的毫秒级延迟，这无疑大大拓展了Structured Streaming的应用广度。不过现在连续处理模式还有很多限制，让我们期待它的未来吧。</p><h3>对事件时间的支持</h3><p>就像我们在前边讲过的，Structured Streaming对基于事件时间的处理有很好的支持。</p><p>由于Spark Streaming是把数据按接收到的时间切分成一个个RDD来进行批处理，所以它很难基于数据本身的产生时间来进行处理。如果某个数据的处理时间和事件时间不一致的话，就容易出问题。比如，统计每秒的词语数量，有的数据先产生，但是在下一个时间间隔才被处理，这样几乎不可能输出正确的结果。</p><p>Structured Streaming还有很多其他优点。比如，它有更好的容错性，保证了端到端exactly once的语义等等。所以综合来说，Structured Streaming是比Spark Streaming更好的流处理工具。</p><h2>思考题</h2><p>在基于事件时间的窗口操作中，Structured Streaming是怎样处理晚到达的数据，并且返回正确结果的呢？</p><p>比如，在每十分钟统计词频的例子中，一个词语在1:09被生成，在1:11被处理，程序在1:10和1:20都输出了对应的结果，在1:20输出时为什么可以把这个词语统计在内？这样的机制有没有限制？</p><p>欢迎你把自己的答案写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"16 | Spark Streaming：Spark的实时流计算API","id":96792},"right":{"article_title":"18 | Word Count：从零开始运行你的第一个Spark应用","id":97658}},"comments":[{"had_liked":false,"id":98804,"user_name":"青石","can_delete":false,"product_type":"c1","uid":1215531,"ip_address":"","ucode":"B0056AD6453322","user_header":"https://static001.geekbang.org/account/avatar/00/12/8c/2b/3ab96998.jpg","comment_is_top":false,"comment_ctime":1559097601,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"57393672449","product_id":100025301,"comment_content":"watermark，process time - event time &gt; watermark则直接丢失，process time - event time <br>&lt; watermark则接收数据处理，更新结果表。","like_count":14},{"had_liked":false,"id":101946,"user_name":"se7en","can_delete":false,"product_type":"c1","uid":1045247,"ip_address":"","ucode":"E3727BF5186267","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f2/ff/2a27214e.jpg","comment_is_top":false,"comment_ctime":1560066103,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"48804706359","product_id":100025301,"comment_content":"同样都是微批处理，为什么spark streaming 就不能处理微秒，而structure streaming就可以","like_count":12,"discussions":[{"author":{"id":1106802,"avatar":"https://static001.geekbang.org/account/avatar/00/10/e3/72/afd1eef0.jpg","nickname":"柳年思水","note":"","ucode":"65589C121B904A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":1933,"discussion_content":"https://databricks.com/blog/2018/03/20/low-latency-continuous-processing-mode-in-structured-streaming-in-apache-spark-2-3-0.html，这篇文章可以看下","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1563105866,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1106802,"avatar":"https://static001.geekbang.org/account/avatar/00/10/e3/72/afd1eef0.jpg","nickname":"柳年思水","note":"","ucode":"65589C121B904A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":202824,"discussion_content":"这篇说的是新的continuous processing引擎，我觉得原问题问的是为什么基于micro-batch的structured streaming能做到100毫秒，但是spark streaming就只能是秒级，不过我猜是因为后来只针对structured streaming做了一些关于窗口的优化，但是spark streaming已经不更新了没包含这些优化而已","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1583952142,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":1933,"ip_address":""},"score":202824,"extra":""}]}]},{"had_liked":false,"id":171552,"user_name":"彭琳","can_delete":false,"product_type":"c1","uid":1804585,"ip_address":"","ucode":"E4E5CEE6FF6366","user_header":"https://static001.geekbang.org/account/avatar/00/1b/89/29/1fba918b.jpg","comment_is_top":false,"comment_ctime":1578966053,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"35938704421","product_id":100025301,"comment_content":"关于思考题……据说是有水印机制，跟踪数据的事件时间，阈值内的延迟数据将会被聚合，比阈值更延迟的数据将被删除，在内存中留有一个中间状态。若有不对，请指正；不过推荐看一下这篇文章《Spark 2.3.0 Structured Streaming详解》，相当于官网翻译：https:&#47;&#47;blog.csdn.net&#47;l_15156024189&#47;article&#47;details&#47;81612860","like_count":9},{"had_liked":false,"id":132388,"user_name":"大大丸子🍡","can_delete":false,"product_type":"c1","uid":1215601,"ip_address":"","ucode":"BE3D49F2D9EDCA","user_header":"https://static001.geekbang.org/account/avatar/00/12/8c/71/a52ac5c1.jpg","comment_is_top":false,"comment_ctime":1568105429,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31632876501","product_id":100025301,"comment_content":"1、Structured Streaming是基于事件事件处理，而不是处理事件，所以，延迟接收的数据，是能被统计到对应的事件时间窗口的<br>2、设定数据延迟的窗口时间阈值，通过判断阈值来决定延迟数据是否需要纳入统计；这个阈值的设定可以避免大量数据的延迟导致的性能问题","like_count":7},{"had_liked":false,"id":100852,"user_name":"向黎明敬礼","can_delete":false,"product_type":"c1","uid":1241248,"ip_address":"","ucode":"4D7E18FB29C531","user_header":"https://static001.geekbang.org/account/avatar/00/12/f0/a0/f83416e9.jpg","comment_is_top":false,"comment_ctime":1559655013,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14444556901","product_id":100025301,"comment_content":"withWatermark函数第一个参数是 数据表中的时间戳字段的字段名，第二个参数是延迟的时间阈值","like_count":4},{"had_liked":false,"id":98304,"user_name":"Ming","can_delete":false,"product_type":"c1","uid":1516011,"ip_address":"","ucode":"69BB73B8AB7E3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/21/eb/bb2e7a3b.jpg","comment_is_top":false,"comment_ctime":1558952323,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14443854211","product_id":100025301,"comment_content":"我不确定有没有完全理解问题..<br><br>我想大概是因为，输出时间所对应的窗口可以故意设置的比输出时间稍微早一点，这样可以对数据延迟有一定的抗性。不然例子中的1:09分的数据就没机会被使用了。<br><br>不过相应的，这样的机制似乎终究是个妥协，妥协的越大，实时性就越差。","like_count":4},{"had_liked":false,"id":105562,"user_name":"Geek_86e573","can_delete":false,"product_type":"c1","uid":1506862,"ip_address":"","ucode":"3480C15374B999","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/mHibFJhnKJC5wRazXPevbWoRbMkCaJibzSekf3DoJAGygHgXKVIO6zK37e1LVLzpUI7iaER8W93dqyTmQmmuIC4rg/132","comment_is_top":false,"comment_ctime":1561031852,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10150966444","product_id":100025301,"comment_content":"用过才知道，这个东西目前坑还挺多","like_count":3,"discussions":[{"author":{"id":1168504,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","nickname":"斯盖丸","note":"","ucode":"B881D14B028F14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":213013,"discussion_content":"2.4的话有哪些坑啊……？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585043000,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":104672,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1560817130,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10150751722","product_id":100025301,"comment_content":"各个类的继承关系最好画一个图，不然在这几个章节打转搞得有点晕","like_count":2},{"had_liked":false,"id":98152,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1558921364,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10148855956","product_id":100025301,"comment_content":"spark structure streaming有没有类似flink的sideOutput机制？支持超过watermark的事件被处理到","like_count":2},{"had_liked":false,"id":104989,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1560903484,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5855870780","product_id":100025301,"comment_content":"最后的思考题只知道flink有一个watermark机制可以保证","like_count":1},{"had_liked":false,"id":98341,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1558963133,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5853930429","product_id":100025301,"comment_content":"一般是处理滞后一定时间的数据，超过了这个时间范围，就会舍弃","like_count":1},{"had_liked":false,"id":98141,"user_name":"方伟","can_delete":false,"product_type":"c1","uid":1177897,"ip_address":"","ucode":"71194E674DC6D7","user_header":"https://static001.geekbang.org/account/avatar/00/11/f9/29/d03c7ad5.jpg","comment_is_top":false,"comment_ctime":1558920024,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5853887320","product_id":100025301,"comment_content":"我知道在flink中可以通过watermark来处理这样的场景，在Structured Streaming中应该也是这样的方式来处理吧。","like_count":1},{"had_liked":false,"id":98087,"user_name":"Rainbow","can_delete":false,"product_type":"c1","uid":1259525,"ip_address":"","ucode":"248A7E2C05E4DE","user_header":"https://static001.geekbang.org/account/avatar/00/13/38/05/67aae6c8.jpg","comment_is_top":false,"comment_ctime":1558912111,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5853879407","product_id":100025301,"comment_content":"10分钟统计一次，按照处理时间分1:00-1:10，1:10-1:20；所以单词的处理时间位于第二个区间会被第二次统计到；如果按照事件时间，sql里time&gt;1:00 and time&lt;1:10就可以把单词归类到第一个区间，这么理解对吗，老师？","like_count":1},{"had_liked":false,"id":200111,"user_name":"爬行的蜗牛","can_delete":false,"product_type":"c1","uid":1033956,"ip_address":"","ucode":"6623B62DE63CE9","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/QD6bf8hkS5dHrabdW7M7Oo9An1Oo3QSxqoySJMDh7GTraxFRX77VZ2HZ13x3R4EVYddIGXicRRDAc7V9z5cLDlA/132","comment_is_top":false,"comment_ctime":1585545310,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585545310","product_id":100025301,"comment_content":"选择更新模式是不是就可以解决解决这个问题呢， ","like_count":0},{"had_liked":false,"id":155883,"user_name":".","can_delete":false,"product_type":"c1","uid":1361729,"ip_address":"","ucode":"D2E44AAF2AA31C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJtr0Mee6woib9x0EHPEXh7Or4ZSUicVJgBfskSZZ3zrxAeCqcAselFIZk8uAmSNVDiadBSWyhMiaqL6Q/132","comment_is_top":false,"comment_ctime":1574773625,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1574773625","product_id":100025301,"comment_content":"各位大佬好，流式处理应该消息应该只被消费一次吧，waterMark机制可以确保在1:20输出，什么情况下在1:10输出了对应的结果呢？求解。","like_count":0},{"had_liked":false,"id":117884,"user_name":"windcaller","can_delete":false,"product_type":"c1","uid":1514157,"ip_address":"","ucode":"1CA3E849805770","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/ad/faf1bf19.jpg","comment_is_top":false,"comment_ctime":1564164471,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564164471","product_id":100025301,"comment_content":"我用那个withWaterMark限制时间窗口进行思考题中的数据过滤时候，就感觉怪怪的，有时候放弃掉，有时候就怎么都不放弃，一直不太理解这块内容<br>","like_count":0},{"had_liked":false,"id":107274,"user_name":"淹死的大虾","can_delete":false,"product_type":"c1","uid":1488430,"ip_address":"","ucode":"79DB76C60F8404","user_header":"https://static001.geekbang.org/account/avatar/00/16/b6/2e/096790e1.jpg","comment_is_top":false,"comment_ctime":1561510689,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1561510689","product_id":100025301,"comment_content":"structure streaming相当于一直在更新输出一个表，这个表有事件时间信息，所以可以按事件时间处理；spark streaming只能按处理时间来的rdd处理，缺少一个汇总","like_count":0},{"had_liked":false,"id":99035,"user_name":"张凯江","can_delete":false,"product_type":"c1","uid":1057462,"ip_address":"","ucode":"301905828F489D","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/b6/3d8fcc2c.jpg","comment_is_top":false,"comment_ctime":1559133508,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559133508","product_id":100025301,"comment_content":"输出模式支持呀。<br>完全模式和更新模式哈。","like_count":0},{"had_liked":false,"id":98773,"user_name":"锦","can_delete":false,"product_type":"c1","uid":1468298,"ip_address":"","ucode":"CB0EB4B68C468B","user_header":"https://static001.geekbang.org/account/avatar/00/16/67/8a/babd74dc.jpg","comment_is_top":false,"comment_ctime":1559093190,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559093190","product_id":100025301,"comment_content":"我觉得可能是通过冗余计算上一个时间窗口中的数据来实现的。<br>局限性就是不支持迟到太久的数据","like_count":0},{"had_liked":false,"id":98731,"user_name":"周凯","can_delete":false,"product_type":"c1","uid":1251177,"ip_address":"","ucode":"C7097F9E86E7B6","user_header":"https://static001.geekbang.org/account/avatar/00/13/17/69/3dea1b7d.jpg","comment_is_top":false,"comment_ctime":1559087854,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559087854","product_id":100025301,"comment_content":"程序在1:10处理的是1:09之前生成的数据，往后推10分钟，那1:20处理的是1:19之前生成的数据","like_count":0},{"had_liked":false,"id":98715,"user_name":"胡鹏","can_delete":false,"product_type":"c1","uid":1326455,"ip_address":"","ucode":"52644EC57FA4DB","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/77/45e5e06d.jpg","comment_is_top":false,"comment_ctime":1559068490,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559068490","product_id":100025301,"comment_content":"老师, 我最近遇到个问题还望帮忙提点一下:<br>1. 需求: 统计实时订单量(类似)<br>2. 通过maxwell读取binlog数据同步到kafka<br>3. spark-streaming处理kafka里面的数据<br>4. spark-sql定义不同的实时报表<br><br>这样做的时候, 对于不同sql定义的报表我就懵了, <br>   假如昨天需求方写了10个SQL放到数据库, 然后我们启动流计算, 提交job到spark, 那么10个实时的报表就开始变动起来了<br>   但是今天需求方说, 这里还有两个指标需要统计一下, 就给我了2条SQL,<br><br>(先说明下前提, maxwell把mysql的数据提取出来提交到了一个kafka的topic里面)<br>疑问点出来了: <br>    1. 如果从新提交一个2条sql的job, 就得独立消费kafka数据, 否则数据有遗漏,  (相当于一条河流, 做了多个截断),     与其对比的是: 在之前提交10个SQL的job中, 先写好SQL来源是动态从某个数据库某张表取出来的, 然后数据流来了直接共享server进行计算,    (相当于一条河流一次截断, 多个筛选, 复用了job的提交和kafka消费这一步),  不知道后者是否可行, 或是有什么坑? <br>    2. 假如选择了问题1 的第一种情况, 且假如重复消费很消耗新能,  然后我想到了替代方案,不同的数据库binlog放到不同的kafka的topic中,  计算出结果之后再聚合, (这样做缺点是不是就是开发程序非常麻烦呢)?<br><br>目前存在如上两个疑问, 我目前觉得第一个问题的第二种情况比较靠谱, 希望可以求证, 或者我原本思考方向就是错的,     还望老师帮忙指点一下","like_count":0},{"had_liked":false,"id":98421,"user_name":"RocWay","can_delete":false,"product_type":"c1","uid":1088024,"ip_address":"","ucode":"377CD114BABBF7","user_header":"https://static001.geekbang.org/account/avatar/00/10/9a/18/3596069c.jpg","comment_is_top":false,"comment_ctime":1559003138,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559003138","product_id":100025301,"comment_content":"我的理解是：虽然设立了窗口，但是有些事件可能由于网络或其他原因迟到了，这些迟到的事件也要被计算在内。否则这段窗口内的数据计算就会“不准”。当然也不能无限允许迟到，所以Spark也设立了watermark。如果窗口的结束时间减去watermark，比某个事件的时间还“晚”，那这个事件就不能算在这个窗口里。","like_count":0},{"had_liked":false,"id":98355,"user_name":"安","can_delete":false,"product_type":"c1","uid":1502447,"ip_address":"","ucode":"C9A5B99149DB3A","user_header":"https://static001.geekbang.org/account/avatar/00/16/ec/ef/99f8dc7b.jpg","comment_is_top":false,"comment_ctime":1558966729,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558966729","product_id":100025301,"comment_content":"因为1:20是处理时间，1:09是事件时间，1点20输出数据的事件时间还没到1点20。是这样吗？不太确定","like_count":0},{"had_liked":false,"id":98154,"user_name":"Cyber_Derek","can_delete":false,"product_type":"c1","uid":1239646,"ip_address":"","ucode":"D9B3596BDC35A4","user_header":"https://static001.geekbang.org/account/avatar/00/12/ea/5e/79dd13b5.jpg","comment_is_top":false,"comment_ctime":1558921805,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558921805","product_id":100025301,"comment_content":"不知道老师能否也讲解一些关于flink方面的东西呢？","like_count":0}]}