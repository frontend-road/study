{"id":422468,"title":"04 | The Google File System （二）： 如何应对网络瓶颈？","content":"<p>你好，我是徐文浩。今天这一讲，我们接着来学习GFS论文中第二个重要的设计决策，也就是根据实际的硬件情况来进行系统设计。</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/a3/8aaaa025cffe70a59925a4887ffae8a3.jpg?wh=1920x884\" alt=\"图片\"></p><p>大数据系统本就是为“性能”而生的，因为单台服务器已经满足不了我们的性能需要。所以我们需要通过搭建成百上千台服务器，组成一个大数据集群。然而，上千台服务器的集群一样有来自各种硬件性能的限制。</p><p>在单台服务器下，我们的硬件瓶颈常常是硬盘。而到了一个分布式集群里，我们又有了一个新的瓶颈，那就是<strong>网络</strong>。</p><p>那么在这一讲里，我们就来看看网络层面的硬件瓶颈，是如何影响了GFS的设计的。在学完这一讲之后，希望你能够理解，<strong>任何一个系统设计，都需要考虑硬件性能</strong>。并且学会在对自己的设计进行评估的时候，能够寻找到系统的硬件瓶颈在哪里。</p><h2>GFS的硬件配置</h2><p>不知道你有没有想过，2003年的GFS是跑在什么样的硬件服务器上的呢？论文的第6部分还真的透露了一些信息给我们。Google拿来做微基准测试（Micro-Benchmark）的服务器集群的配置是这样的：</p><ul>\n<li>19台服务器、1台master、2台master的只读副本、16台chunkserver，以及另外16台GFS的客户端；</li>\n<li>所有服务器的硬件配置完全相同，都是双核1.45 GHz的奔腾3处理器 + 2GB内存 + 两块80GB的5400rpm的机械硬盘 + 100 Mbps的全双工网卡。</li>\n<li>然后把所有的19台GFS集群的机器放在一台交换机上，所有的16台GFS的客户端放在另外一台交换机上，两台交换机之间通过带宽是1Gbps的网线连接起来。</li>\n</ul><!-- [[[read_end]]] --><p>而Google跑在内部实际使用的真实集群，虽然论文里没有给出具体的硬件配置，但我们也可以反向推算一下。论文第6部分有一个Table 2，里面有A和B两个集群。根据表格里面的数据可以计算得出，里面的A集群平均每台chunkserver大约有200GB的硬盘，每台chunkserver需要的Metadata（元数据）大约是38MB。而里面的B集群则是800GB的硬盘，以及93MB的Metadata。这样看起来，除了可以多插几块硬盘增加一些存储空间之外，前面测试集群的硬件配置完全够用了。</p><p><img src=\"https://static001.geekbang.org/resource/image/ce/9a/ce97bd552029ab8e6a506a120049749a.jpg?wh=1920x955\" alt=\"图片\" title=\"以上数据可以从论文6.2部分的Table 2中计算得出\"></p><p>在这个硬件配置中，5400转（rpm）的硬盘，读写数据的吞吐量通常是在60MB/s~90MB/s左右。而且我们通常会插入多块硬盘，比如集群B，就需要10块80GB的硬盘，这样就意味着整体硬盘的吞吐量可以达到500MB/s以上。但是，100Mbps的网卡的极限吞吐率只有12.5MB/s，这个也就意味着，当我们从GFS读写数据的时候，<strong>瓶颈就在网络上</strong>。</p><p>那么下面，我们就来看一看针对这样的硬件瓶颈，GFS都做了哪些针对性的设计。</p><h2>GFS的数据写入</h2><p>我们先来看看一个客户端是怎么向GFS集群里面写数据的。在上一讲里，我带你了解了一个GFS客户端怎么从集群里读取数据。相信你学完之后会觉得特别简单，感觉也就是个几千行代码的事儿。不过，写文件可就没有那么简单了。</p><p>实际上，读数据简单，是因为不管我们有多少个客户端并发去读一个文件，读到的内容都不会有区别，即使它们读同一个chunk是分布在不同chunkserver。我们不是靠在读取中做什么特殊的动作，来保障客户端读到的数据都一样。<strong>“保障读到的数据一样”这件事情，其实是在数据写入的过程中来保障的。</strong></p><p>写入和读取不同的是，读取只需要读一个chunkserver，最坏的情况无非是读不到重试。而写入，则是同时要写三份副本，如果一个写失败，两个写成功了，数据就已经不一致了。</p><p>那么，GFS是怎么解决这样的问题的呢？下面我就带你来看一下，GFS写入数据的具体步骤。</p><ul>\n<li>第一步，客户端会去问master要写入的数据，应该在哪些chunkserver上。</li>\n<li>第二步，和读数据一样，master会告诉客户端所有的次副本（secondary replica）所在的chunkserver。这还不够，master还会告诉客户端哪个replica是“老大”，也就是主副本（primary replica），数据此时以它为准。</li>\n<li>第三步，拿到数据应该写到哪些chunkserver里之后，客户端会把要写的数据发给所有的replica。不过此时，chunkserver拿到发过来的数据后还不会真的写下来，只会把数据<strong>放在一个LRU的缓冲区里</strong>。</li>\n<li>第四步，等到所有次副本都接收完数据后，客户端就会发送一个写请求给到主副本。我在上节课一开始就说过，GFS面对的是几百个并发的客户端，所以主副本可能会收到很多个客户端的写入请求。主副本自己会给这些请求排一个顺序，确保所有的数据写入是有一个固定顺序的。接下来，主副本就开始按照这个顺序，把刚才LRU的缓冲区里的数据写到实际的chunk里去。</li>\n<li>第五步，主副本会把对应的写请求转发给所有的次副本，所有次副本会和主副本以同样的数据写入顺序，把数据写入到硬盘上。</li>\n<li>第六步，次副本的数据写入完成之后，会回复主副本，我也把数据和你一样写完了。</li>\n<li>第七步，主副本再去告诉客户端，这个数据写入成功了。而如果在任何一个副本写入数据的过程中出错了，这个出错都会告诉客户端，也就意味着这次写入其实失败了。</li>\n</ul><p>所以在GFS的数据写入过程中，可能会出现主副本写入成功，但是次副本写入出错的情况。在这种情况下，客户端会认为写入失败了。但是这个时候，同一个chunk在不同chunkserver上的数据可能会出现不一致的情况，这个问题我们会放到下一讲来深入讨论。</p><p><img src=\"https://static001.geekbang.org/resource/image/cd/e6/cd111d95dde55f57eb7cecf23da4e7e6.jpg?wh=1920x1080\" alt=\"图片\" title=\"GFS的数据写入采用了控制流和数据流分离的方式\"></p><p>我在这里也放了一张图，来帮助你理解这个数据写入的流程。从这张图上你会发现，GFS的数据写入使用了两个很有意思的模式，来解决这节课一开始我提到的网络带宽的瓶颈问题。</p><h3>分离控制流和数据流</h3><p>第一个模式是控制流和数据流的分离。</p><p>和之前从GFS上读数据一样，GFS客户端只从master拿到了chunk data在哪个chunkserver的元数据，实际的数据读写都不再需要通过master。另外，不仅具体的数据传输不经过master，后续的数据在多个chunkserver上同时写入的协调工作，也不需要经过master。</p><p>这也就是说，控制流和数据流的分离，不仅仅是数据写入不需要通过master，更重要的是实际的数据传输过程，和提供写入指令的动作是完全分离的。</p><p>其次，是采用了<strong>流水线（pipeline）</strong>式的网络传输。数据不一定是先给到主副本，而是看网络上离哪个chunkserver近，就给哪个chunkserver，数据会先在chunkserver的缓冲区里存起来，就是前面提到的第3步。但是写入操作的指令，也就是上面的第4~7步，则都是由客户端发送给主副本，再由主副本统一协调写入顺序、拿到操作结果，再给到客户端的。</p><h3>流水线式的网络数据传输</h3><p>之所以要这么做，还是因为GFS最大的瓶颈就在网络。如果用一个最直观的想法来进行数据传输，我们可以把所有数据直接都从客户端发给三个chunkserver。</p><p>但是这种方法的问题在于，<strong>客户端的出口网络会立刻成为瓶颈</strong>。</p><p>比如，我们要发送1GB的数据给GFS，客户端的出口网络带宽有100MB/秒，那么我们只需要10秒就能把数据发送完。但是因为三个chunkserver的数据都要从客户端发出，所以要30s才能把所有的数据都发送完，而且这个时候，三个chunkserver的网络带宽都没有用满，各自只用了1/3，网络并没有被有效地利用起来。</p><p>而在流水线式的传输方式下，客户端可以先把所有数据，传输给到网络里离自己最近的次副本A，然后次副本A一边接收数据，一边把对应的数据传输给到离自己最近的另一个副本，也就是主副本。</p><p>同样的，主副本可以如法炮制，把数据也同时传输给次副本B。在这样的流水线式的数据传输方式下，只要网络上没有拥堵的情况，只需要10秒多一点点，就可以把所有的数据从客户端，传输到三个副本所在的chunkserver上。</p><p><img src=\"https://static001.geekbang.org/resource/image/41/d0/410ac2b91de26ecedfc4b2382decced0.jpg?wh=1920x1080\" alt=\"图片\" title=\"流式传输的方式可以有效利用满客户端和chunkserver的网络带宽\"></p><p>不过到这里你可能要问了：<strong>为什么客户端传输数据，是先给离自己最近的次副本A，而不是先给主副本呢？</strong></p><p>这个问题，也和数据中心的实际网络结构有关，你可以先看看下面这张数据中心的网络拓扑图。</p><p><img src=\"https://static001.geekbang.org/resource/image/34/9f/34417d1ee235d2876e2ab3987e57cb9f.jpg?wh=1920x1080\" alt=\"图片\" title=\"传统的三层交换的数据中心拓扑图\"></p><p>要知道，我们几百台服务器所在的数据中心，一般都是通过三层交换机连通起来的：</p><ul>\n<li>同一个机架（Rack）上的服务器，都会接入到一台<strong>接入层交换机</strong>（Access Switch）上；</li>\n<li>各个机架上的接入层交换机，都会连接到某一台<strong>汇聚层交换机</strong>（Aggregation Switch）上；</li>\n<li>而汇聚层交换机，再会连接到多台<strong>核心交换机</strong>（Core Switch）上。</li>\n</ul><p>那么根据这个网络拓扑图，你会发现，两台服务器如果在同一个机架上，它们之间的网络传输只需要通过接入层的交换机即可。在这种情况下，除了两台服务器本身的网络带宽之外，它们只会占用所在的接入层交换机的带宽。</p><p>但是，如果两台服务器不在一个机架，乃至不在一个VLAN的情况下，数据传输就要通过汇聚层交换机，甚至是核心交换机了。而如果大量的数据传输，都是在多个不同的VLAN之间进行的，那么汇聚层交换机乃至核心交换机的带宽，就会成为瓶颈。</p><p>所以我们再回到之前的链式传输的场景，GFS最大利用网络带宽，同时又减少网络瓶颈的选择就是这样的：</p><ul>\n<li>首先，客户端把数据传输给离自己“最近”的，也就是在同一个机架上的次副本A服务器；</li>\n<li>然后，次副本A服务器再把数据传输给离自己“最近”的，在不同机架，但是处于同一个汇聚层交换机下的主副本服务器上；</li>\n<li>最后，主副本服务器，再把数据传输给在另一个汇聚层交换机下的次副本B服务器。</li>\n</ul><p>这样的传输顺序，就最大化地利用了每台服务器的带宽，并且减少了交换机的带宽瓶颈。而如果我们非要先把数据从客户端传输给主副本，再从主副本传输到次副本A，那么同样的数据就需要多通过汇聚层交换机一次，从而就占用了更多的汇聚层交换机的资源。</p><h2>独特的Snapshot操作</h2><p>那么，在做了分离控制流和数据流，以及使用流水线式的数据传输方式之后，对于GFS的网络传输上，还有什么其他的优化空间吗？</p><p>你别说还真的有，那就是<strong>为常见的文件复制操作单独设计一个指令</strong>。</p><p>复制文件，相信这个是你用自己的电脑的时候，会常常做的事儿。在GFS上，如果我们用笨一点的办法，自然是通过客户端把文件从chunkserver读回来，再通过客户端把数据写回去。这样的话，读数据也经过一次网络传输，写回三个副本服务器，即使是流水线式的传输，也要三次传输，一共需要把数据在网络上搬运四次。</p><p>所以，GFS就专门为文件复制设计了一个Snapshot指令，当客户端通过这个指令进行文件复制的时候，这个指令会通过控制流，下达到主副本服务器，主副本服务器再把这个指令下达到次副本服务器。不过接下来，客户端并不需要去读取或者写入数据，而是各个chunkserver会直接在本地把对应的chunk复制一份。</p><p>这样，数据流就完全不需要通过网络传输了。相信这个聪明的方法你也一定想到了。</p><h2>小结</h2><p>好了，通过这节课的学习，现在你对GFS是如何写入数据和复制文件应该就已经非常清楚了。那么，这里我们一起来回顾一下吧。</p><p>这节课，我先带你看了一下GFS的分布式集群的硬件配置，你会发现2003年GFS的最大的硬件瓶颈就是在网络。于是，在GFS设计数据写入机制的时候，就是经过仔细分析后针对这个问题来设计的。你可以重点关注以下这三个要点。</p><p>第一，和读数据一样，GFS采用了控制流和数据流分离的方式。在写入数据的时候，客户端只是从master拿到chunk所在位置的元数据，而在实际的数据传输过程中，并不需要master参与，从而就避免了master成为瓶颈。</p><p>第二，在客户端向多个chunkserver写入数据的时候，采用了“就近”的流水线式传输的方案。这种方式，就尽可能有效地利用了客户端、chunkserver乃至于交换机的带宽。</p><p>第三，对于常见的文件复制这个操作，GFS专门设计了一个Snapshot的指令，针对文件复制，会直接在chunkserver本地进行，完全避免了网络传输。</p><p>其实，这三个动作，都不是什么理论上的创新，而是完全针对当时数据中心的网络架构、服务器硬件规格所进行的设计。也就是说，基于硬件设计，实际上不只是GFS的一个非常重要的核心设计思想，它更是贯穿整个大数据系统领域的一个重要的主题。</p><h2>推荐阅读</h2><p>学完了这一讲之后，如果你对数据中心的网络架构有了一些设计思路，可以再去读一读Facebook工程团队在2014年写的这篇<a href=\"http://Introducing%20data%20center%20fabric,%20the%20next-generation%20Facebook%20data%20center%20network\">文章</a>：Introducing data center fabric, the next-generation Facebook data center network。</p><p>而如果你觉得自己对于计算机的各类硬件性能不熟悉，我推荐你回头看一看我之前的《<a href=\"https://time.geekbang.org/column/intro/170\">深入浅出计算机组成原理</a>》的专栏，特别是其中关于存储器的部分。我们在后面的论文解读中，还会不断根据各种硬件特性和性能来思考我们的设计。</p><p>回头看起来，在“大数据”爆发之前，数据中心的数据流量通常是“南北大，东西小”，也就是大部分数据都是从某一台服务器，经过几层交换机，进入互联网，返回给终端用户。而在“大数据”爆发之后，数据中心的大量数据传输变成了数据中心的服务器横向之间的传输，而这个也让工程师们开始重新基于需求，重新设计数据中心需要的硬件和网络拓扑。那么，在学完今天这节课之后，我想你一定会对基于硬件性能设计系统，有更多的思考。</p><h2>思考题</h2><p>最后我想说，学习论文并不是背诵，重要是得总结和思考。所以我给你留了一道思考题，欢迎你和其他同学一起讨论一下。</p><p>在你接触过的系统和代码中，有没有什么设计，也是深度考虑了实际的硬件性能和瓶颈的呢？你可以留言说说，我们共同交流、互相进步。</p>","neighbors":{"left":{"article_title":"03 | The Google File System （一）： Master的三个身份","id":421579},"right":{"article_title":"05 | The Google File System （三）： 多写几次也没关系","id":422636}},"comments":[{"had_liked":false,"id":313816,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1632697768,"is_pvip":true,"replies":[{"id":"113918","content":"👍","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1633164461,"ip_address":"","comment_id":313816,"utype":1}],"discussion_count":1,"race_medal":0,"score":"83237076392","product_id":100091101,"comment_content":"mysql利用b+出度打，层级底的特性，尽可能减少一次查询中随机io开销。<br>kafka利用磁盘顺序写入较随机写入快的特性，批量顺序写文件。<br>redis ignite 等内存数据库都基于内存性能远胜于磁盘等持久化外部存储，从而基于内存做存储系统。","like_count":20,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527480,"discussion_content":"👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633164461,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313958,"user_name":"webmin","can_delete":false,"product_type":"c1","uid":1047014,"ip_address":"","ucode":"98B0CA882454E8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f9/e6/47742988.jpg","comment_is_top":false,"comment_ctime":1632755872,"is_pvip":true,"replies":[{"id":"113895","content":"<br>论文大部分都是很精简的，所以如果每段内容彻底弄清楚其实展开都有很多问题可以讲，而且GFS的核心瓶颈往往就在网络，所以我特地深入展开一下。<br><br>关于网络优化，更多是因为过去的经历和经验中遇到很多这方面的困扰吧。因为十年前是没有云计算平台可以用的，需要自己来建数据中心。跨地域的数据中心的网络传输也要想办法自己解决，所以的确会遇到各种应用开发遇不到，但是教科书和论文里遇到的问题。","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1633159372,"ip_address":"","comment_id":313958,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35992494240","product_id":100091101,"comment_content":"今天课程中关于网络优化的内容，基本是出自GFS论文中的3.2节Data Flow，我很好奇是因为老师有关于广告系统的开发经验，所以能从一个300个单词左右的小节中看出这么丰富的信息，还是老师有其它的分析框架或辨识方法？还望老师抽时间传授。<br><br>另加一个注解流水线（pipeline）式的网络传输是有效利用了网络是全双工的原理，即左手进右手出，左右各100Mb。","like_count":8,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527528,"discussion_content":"\n论文大部分都是很精简的，所以如果每段内容彻底弄清楚其实展开都有很多问题可以讲，而且GFS的核心瓶颈往往就在网络，所以我特地深入展开一下。\n\n关于网络优化，更多是因为过去的经历和经验中遇到很多这方面的困扰吧。因为十年前是没有云计算平台可以用的，需要自己来建数据中心。跨地域的数据中心的网络传输也要想办法自己解决，所以的确会遇到各种应用开发遇不到，但是教科书和论文里遇到的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633159372,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321781,"user_name":"核桃","can_delete":false,"product_type":"c1","uid":1385204,"ip_address":"","ucode":"7AB05270CBCCCB","user_header":"https://static001.geekbang.org/account/avatar/00/15/22/f4/9fd6f8f0.jpg","comment_is_top":false,"comment_ctime":1637044632,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27406848408","product_id":100091101,"comment_content":"这里其实是隐藏了一个功能，就是GFS能识别到机架上的服务器拓扑结构的，不然分配的时候是无法感知到到底哪个节点是离客户端比较近的。另外一般分配数据节点的时候，有时候客户端并不一定在集群内发起的，而是在外部的。那么这时候分配的原则也是两个节点可能会近点，但是第三个会远离，甚至在不同机房中。","like_count":7},{"had_liked":false,"id":313972,"user_name":"陈迪","can_delete":false,"product_type":"c1","uid":1019744,"ip_address":"","ucode":"1A64122CC47337","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8f/60/be0a8805.jpg","comment_is_top":false,"comment_ctime":1632787379,"is_pvip":false,"replies":[{"id":"113899","content":"好问题，值得探讨。<br><br>从大数据系统来看，随着SSD的廉价，原先觉得SSD不适合作为Hadoop的存储层这一点在逐渐失效。SSD，机械硬盘，乃至磁带冷备随着数据越来越多，价格越来越便宜，成为各有应用场景的硬件了。<br><br>SSD的出现使得随机读的性能上了几个数量级，不过这个针对的更多是数据系统的Serving层。<br><br>不过网络瓶颈似乎变化不大。","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1633160149,"ip_address":"","comment_id":313972,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27402591155","product_id":100091101,"comment_content":"思考一个问题，20年过去了，硬件环境已经哪些发生了根本性的变化？现代的分布式文件系统应该什么样的？","like_count":6,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527534,"discussion_content":"好问题，值得探讨。\n\n从大数据系统来看，随着SSD的廉价，原先觉得SSD不适合作为Hadoop的存储层这一点在逐渐失效。SSD，机械硬盘，乃至磁带冷备随着数据越来越多，价格越来越便宜，成为各有应用场景的硬件了。\n\nSSD的出现使得随机读的性能上了几个数量级，不过这个针对的更多是数据系统的Serving层。\n\n不过网络瓶颈似乎变化不大。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633160149,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313863,"user_name":"Ping","can_delete":false,"product_type":"c1","uid":1812754,"ip_address":"","ucode":"FC4743D8FEF61C","user_header":"https://static001.geekbang.org/account/avatar/00/1b/a9/12/e041e7b2.jpg","comment_is_top":false,"comment_ctime":1632721652,"is_pvip":false,"replies":[{"id":"113819","content":"Ping同学，<br><br>你好，你看我在里面放的数据中心的网络架构图。按照上北下南，左西右东来理解方位。<br><br>在有大数据需求之前，一般我们的网络流量，都是互联网请求访问服务器，然后通过三层交换机到达某一台具体服务器，服务器给出结果，再通过三层交换机返回给外部。这里面的网络流量都是南北向的。<br><br>但是在大数据处理过程中，大部分数据传输都是最下面一层服务器之间互相传输，也就是东西向的。","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1632917271,"ip_address":"","comment_id":313863,"utype":1}],"discussion_count":2,"race_medal":0,"score":"27402525428","product_id":100091101,"comment_content":"能再解释下“南北大，东西小”是什么意思吗？","like_count":7,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527496,"discussion_content":"Ping同学，\n\n你好，你看我在里面放的数据中心的网络架构图。按照上北下南，左西右东来理解方位。\n\n在有大数据需求之前，一般我们的网络流量，都是互联网请求访问服务器，然后通过三层交换机到达某一台具体服务器，服务器给出结果，再通过三层交换机返回给外部。这里面的网络流量都是南北向的。\n\n但是在大数据处理过程中，大部分数据传输都是最下面一层服务器之间互相传输，也就是东西向的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632917271,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1219553,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9b/e1/aa0af424.jpg","nickname":"可加","note":"","ucode":"C0DB61661144DA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536192,"discussion_content":"我理解南北向就是coordinator 或者 master节点向 子节点的流量，东西向是node server 之间的 peer to peer 流量","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638712906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314828,"user_name":"音速起子代购","can_delete":false,"product_type":"c1","uid":2744922,"ip_address":"","ucode":"98C5A7A7D25475","user_header":"https://static001.geekbang.org/account/avatar/00/29/e2/5a/6696d429.jpg","comment_is_top":false,"comment_ctime":1633488162,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10223422754","product_id":100091101,"comment_content":"请问一个问题，，chunk的副本位置关系不是由master掌握吗？那在复制过程中，由主副本如何转发通知次副本消息呢？主副本如何知道次副本的位置？","like_count":2,"discussions":[{"author":{"id":1883431,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/bd/27/e653a220.jpg","nickname":"Xiaosong","note":"","ucode":"28A03027343F9D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":531043,"discussion_content":"master算好chunk储存的位置告诉客户端后，客户端在向第一台机器发数据的时候就带了其他几个chunk的信息","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637211687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313914,"user_name":"在路上","can_delete":false,"product_type":"c1","uid":1402511,"ip_address":"","ucode":"6E31908EFE1107","user_header":"https://static001.geekbang.org/account/avatar/00/15/66/8f/02be926d.jpg","comment_is_top":false,"comment_ctime":1632736346,"is_pvip":false,"replies":[{"id":"113920","content":"我们先要弄清楚什么是网速？我们关注的是延时还是吞吐量？<br><br>在pipeline传输里，A节点传输数据到B，B转手给到C，C再给D，B和C都不需要等数据传输完再往下发。而是收多少立刻再发多少。<br>所以时间就是 B&#47;T + R个节点之间的延时（A-&gt;B，B-&gt;C，C-&gt;D 一共三个延时）T。","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1633164871,"ip_address":"","comment_id":313914,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10222670938","product_id":100091101,"comment_content":"徐老师好，GFS论文3.2 Data Flow中提到“Without networkcongestion, the ideal elapsed time for transferring B bytes to R replicas is B&#47;T + RL where T is the network throughput and L is latency to transfer bytes between two machines. Our network links are typically 100 Mbps (T), and L is far below 1 ms. Therefore, 1 MB can ideally be distributed in about 80 ms.”<br><br>我不明白的是RL部分的计算，为什么L&lt;1ms，L的大小和要传输的数据大小无关吗？为什么不是B&#47;T1 + R*B&#47;T2，T1表示客户端到GFS的网速，T2表示GFS集群内的网速？希望能得到老师的指点。","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527518,"discussion_content":"我们先要弄清楚什么是网速？我们关注的是延时还是吞吐量？\n\n在pipeline传输里，A节点传输数据到B，B转手给到C，C再给D，B和C都不需要等数据传输完再往下发。而是收多少立刻再发多少。\n所以时间就是 B/T + R个节点之间的延时（A-&amp;gt;B，B-&amp;gt;C，C-&amp;gt;D 一共三个延时）T。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633164871,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":329219,"user_name":"香樟树","can_delete":false,"product_type":"c1","uid":1182172,"ip_address":"","ucode":"6D840ECACFF503","user_header":"https://static001.geekbang.org/account/avatar/00/12/09/dc/37f0ab16.jpg","comment_is_top":false,"comment_ctime":1641216804,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"5936184100","product_id":100091101,"comment_content":"抛个砖，分享一个MySQL关于磁盘IO的优化策略。大并发写入数据时磁盘IO是性能瓶颈，MySQL通过批量写（比如两个维度：攒够一定量的数据或者达到一定的时间间隔）来减少IO；同样的，大量并发读取数据时，磁盘IO也是性能瓶颈，MySQL通过连续读（将所需要的数据页以及与其相邻的数据页一起读入内存）来减少IO","like_count":1,"discussions":[{"author":{"id":2956164,"avatar":"","nickname":"Geek_718f9d","note":"","ucode":"4CB58AF9D99842","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558562,"discussion_content":"gfs里也有这样的思想，延时换吞吐","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648377284,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317790,"user_name":"Helios","can_delete":false,"product_type":"c1","uid":1380758,"ip_address":"","ucode":"BE6B98EE8F0D09","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKJrOl63enWXCRxN0SoucliclBme0qrRb19ATrWIOIvibKIz8UAuVgicBMibIVUznerHnjotI4dm6ibODA/132","comment_is_top":false,"comment_ctime":1634967072,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5929934368","product_id":100091101,"comment_content":"请教一下老师，现在网络带宽还会成为设计的瓶颈么？","like_count":1,"discussions":[{"author":{"id":1219553,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9b/e1/aa0af424.jpg","nickname":"可加","note":"","ucode":"C0DB61661144DA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536193,"discussion_content":"即便网络带宽不是主要瓶颈，data locality 也是要考虑到的吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638712989,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317787,"user_name":"Helios","can_delete":false,"product_type":"c1","uid":1380758,"ip_address":"","ucode":"BE6B98EE8F0D09","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKJrOl63enWXCRxN0SoucliclBme0qrRb19ATrWIOIvibKIz8UAuVgicBMibIVUznerHnjotI4dm6ibODA/132","comment_is_top":false,"comment_ctime":1634966264,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5929933560","product_id":100091101,"comment_content":"“主副本自己会给这些请求排一个顺序，确保所有的数据写入是有一个固定顺序的”<br>并发保证顺序是很难得，尤其是在分布式的情况下，各个机器的clock可能不一致，同步NTP也有早晚，请教老师GFS是如何保证顺序呢","like_count":1},{"had_liked":false,"id":315174,"user_name":"全有","can_delete":false,"product_type":"c1","uid":1488139,"ip_address":"","ucode":"D16B42B1F71E4E","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/0b/df89c357.jpg","comment_is_top":false,"comment_ctime":1633741241,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"5928708537","product_id":100091101,"comment_content":"Pipeline式写入，其实会存在副本写入成功，主副本写入失败的情况，与我们常见的Es,Kafka 等里面的主副本概念流程是不一样的（优先写主)","like_count":1,"discussions":[{"author":{"id":2767262,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/39/9e/ce0c30db.jpg","nickname":"董相宏","note":"","ucode":"86045E74246B24","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":412197,"discussion_content":"不会存在从副本写入成功的而主副本失败的，可以看他首先是做到数据传输，并没有写入磁盘，只有在主副本完成传输后才会统一协调所有副本统一写入","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1636100462,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537012,"discussion_content":"请教一下数据写入副本LRU时如果缓存满了被淘汰了怎么办？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638934413,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314283,"user_name":"leslie","can_delete":false,"product_type":"c1","uid":1324255,"ip_address":"","ucode":"798E7C1CC98CC2","user_header":"https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg","comment_is_top":false,"comment_ctime":1632941172,"is_pvip":false,"replies":[{"id":"113894","content":"Windows现在有了WSL2了，体验还是不错的。","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1633157789,"ip_address":"","comment_id":314283,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5927908468","product_id":100091101,"comment_content":"UDP替代TCP去实现网络的传输，传完了就好了；监控而已-丢失了再传即可；不过当时忽略了一个关键问题-windows并不适合去用很多适合在linux下的软件，导致了备份和恢复的灾难性隐患。","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527628,"discussion_content":"Windows现在有了WSL2了，体验还是不错的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633157789,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":356827,"user_name":"Spoon","can_delete":false,"product_type":"c1","uid":1959822,"ip_address":"浙江","ucode":"2FF9193AD482C2","user_header":"https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg","comment_is_top":false,"comment_ctime":1662620967,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1662620967","product_id":100091101,"comment_content":"零拷贝，利用DMA避免了两次内核态和用户态的切换。","like_count":0},{"had_liked":false,"id":352080,"user_name":"日就月将","can_delete":false,"product_type":"c1","uid":2651148,"ip_address":"","ucode":"0F9BA55A2898FF","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/cabLXAUXiavXnEckAgo971o4l1CxP4L9wOV2eUGTyKBUicTib6gJyKV9iatM4GG1scz5Ym17GOzXWQEGzhE31tXUtQ/132","comment_is_top":false,"comment_ctime":1658375462,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1658375462","product_id":100091101,"comment_content":"请教一下 控制流和数据流 具体是什么意思呢","like_count":0},{"had_liked":false,"id":340729,"user_name":"攀峰2022","can_delete":false,"product_type":"c1","uid":2925376,"ip_address":"","ucode":"0DD786F5F2FFD9","user_header":"https://static001.geekbang.org/account/avatar/00/2c/a3/40/73e71a52.jpg","comment_is_top":false,"comment_ctime":1649083657,"is_pvip":false,"discussion_count":0,"race_medal":1,"score":"1649083657","product_id":100091101,"comment_content":"chunkserver里面是不是数据元data element，我看您的课里使用元数据metadata描述的","like_count":0},{"had_liked":false,"id":339562,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1648188112,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1648188112","product_id":100091101,"comment_content":"Kafka的数据零拷贝，数据不进入到用户态，直接从网卡转发出去。","like_count":0},{"had_liked":false,"id":334428,"user_name":"xxx","can_delete":false,"product_type":"c1","uid":1096652,"ip_address":"","ucode":"E79CEA70430449","user_header":"https://static001.geekbang.org/account/avatar/00/10/bb/cc/fac12364.jpg","comment_is_top":false,"comment_ctime":1644930687,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1644930687","product_id":100091101,"comment_content":"关于数据中心网络演进的知识，可以参考刘超的《趣谈网络协议》。另外这里的 Pipeline 我觉得用词不好，用“接力式传输”我觉得更传神一点😂","like_count":0,"discussions":[{"author":{"id":1282641,"avatar":"https://static001.geekbang.org/account/avatar/00/13/92/51/0505254d.jpg","nickname":"Fighter","note":"","ucode":"B1925EE400372D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555213,"discussion_content":"流水线不是更好理解 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646812227,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":331138,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1642443951,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1642443951","product_id":100091101,"comment_content":"老师，为什么写数据是先缓存再通过主副本控制写入磁盘？","like_count":0},{"had_liked":false,"id":328445,"user_name":"zixuan","can_delete":false,"product_type":"c1","uid":1131300,"ip_address":"","ucode":"C72920DD05B074","user_header":"https://static001.geekbang.org/account/avatar/00/11/43/24/3f9f7c70.jpg","comment_is_top":false,"comment_ctime":1640742127,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1640742127","product_id":100091101,"comment_content":"snapshot不是直接复制各个副本，而是用了chunk引用数来做copy on write。","like_count":0},{"had_liked":false,"id":328108,"user_name":"Jeffery","can_delete":false,"product_type":"c1","uid":2682406,"ip_address":"","ucode":"54A5720484AE27","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJt6TyUk2YZXDQ5PMATLAL77b99ACVEhtLrs49koS8UHuiaaEUGKzLw7bThTMqCbSDXdwlW3uue2mw/132","comment_is_top":false,"comment_ctime":1640533267,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1640533267","product_id":100091101,"comment_content":"手机的刘海屏算不算基于硬件性能和瓶颈的设计呢？（屏下摄像头技术还不成熟）","like_count":0},{"had_liked":false,"id":320353,"user_name":"强子","can_delete":false,"product_type":"c1","uid":1251068,"ip_address":"","ucode":"4F528E9A9FE516","user_header":"https://static001.geekbang.org/account/avatar/00/13/16/fc/3a98fe8d.jpg","comment_is_top":false,"comment_ctime":1636274158,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1636274158","product_id":100091101,"comment_content":"老师，看文章的时候，有个细节没太明白：写入数据的时候，为什么要写入LRU的缓存区？是为了一个时效性问题么？还有一个问题就是缓存区写入完成之后，主节点会发送指令给两个副本节点进行固定顺序的写入，这个为什么要按照固定顺序呢？","like_count":0,"discussions":[{"author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537013,"discussion_content":"我可以回答第二个问题是为了确保并发写入时副本之间的数据一致性，但是我不能回答第一个问题，同时我不懂如果LRU写满了产生缓存淘汰要怎么办","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1638934525,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":318209,"user_name":"sljoai","can_delete":false,"product_type":"c1","uid":1018071,"ip_address":"","ucode":"FF88C4BA265DE0","user_header":"https://static001.geekbang.org/account/avatar/00/0f/88/d7/07f8bc6c.jpg","comment_is_top":false,"comment_ctime":1635208271,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1635208271","product_id":100091101,"comment_content":"请问一下：一般在就近选择chunckserver的时候，是依赖哪些条件来判断物理距离的远近的呢？ip,机架信息嘛？","like_count":0,"discussions":[{"author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537018,"discussion_content":"我可以就着HDFS的论文回答你：\n1. A distance between two nodes can be calculated by summing up their distances to their closest common ancestor. 就是加一下两个节点分别到common节点的距离\n2. 另外再附赠你一个，HDFS allows an administrator to configure a script that returns a node’s rack identification given a node’s address. The NameNode is the central place that resolves the rack location of each DataNode. When a DataNode registers with the NameNode, the NameNode runs a configured script to decide which rack the node belongs to. If no such a script is configured, the NameNode assumes that all the nodes belong to a default single rack. 管理员可以配置一个脚本用来通过node的地址计算node的rack id，这些信息都是通过NameNode计算并resolve的，如果没有配置脚本，默认所有节点都在一个机架下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638934816,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317788,"user_name":"Helios","can_delete":false,"product_type":"c1","uid":1380758,"ip_address":"","ucode":"BE6B98EE8F0D09","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKJrOl63enWXCRxN0SoucliclBme0qrRb19ATrWIOIvibKIz8UAuVgicBMibIVUznerHnjotI4dm6ibODA/132","comment_is_top":false,"comment_ctime":1634966823,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1634966823","product_id":100091101,"comment_content":"“然后，次副本 A 服务器再把数据传输给离自己“最近”的，在不同机架，但是处于同一个汇聚层交换机下的主副本服务器上；”<br>老师，为什么主副本服务器一定和次副本 A 服务器处于同一个汇聚层交换机呢？那要是次副本 B 服务器请求主副本服务器不就不在同一个汇聚层交换机了么","like_count":0},{"had_liked":false,"id":314954,"user_name":"Geek_88604f","can_delete":false,"product_type":"c1","uid":1501234,"ip_address":"","ucode":"33DD1318E53814","user_header":"","comment_is_top":false,"comment_ctime":1633611418,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1633611418","product_id":100091101,"comment_content":"老师在前面的章节中有提到过，大数据的一个特点是用廉价的PC来代替大型机或小型机。那么所有相关的设计就会围绕这一点做文章，包括大数据的计算和存储。从计算层面来将，如何根据不同服务器的资源情况合理调度计算负载；存储层面来讲，通过先写缓存来提高写入速率，通过Append方式来提高吞吐，通过后台定期合并小文件来避免高的写入时延（copy on write和merge on read的权衡）等","like_count":0},{"had_liked":false,"id":314810,"user_name":"火娃儿","can_delete":false,"product_type":"c1","uid":1252132,"ip_address":"","ucode":"2704841763FD70","user_header":"https://static001.geekbang.org/account/avatar/00/13/1b/24/1006c208.jpg","comment_is_top":false,"comment_ctime":1633445064,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1633445064","product_id":100091101,"comment_content":"HBASE 就是 利用LSM ,用一定的 归并 门限来 逐步 合并 每次 写操作，达到门限再进行 写磁盘操作。","like_count":0},{"had_liked":false,"id":314535,"user_name":"Lebron","can_delete":false,"product_type":"c1","uid":1327057,"ip_address":"","ucode":"042295490BEBE1","user_header":"https://static001.geekbang.org/account/avatar/00/14/3f/d1/910a7dc9.jpg","comment_is_top":false,"comment_ctime":1633180229,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1633180229","product_id":100091101,"comment_content":"老师，你贴的那篇文章我无法访问呢，有没有国内的链接，像你之前课程的那种PDF的链接？","like_count":0},{"had_liked":false,"id":314052,"user_name":"Geek_2e6a7e","can_delete":false,"product_type":"c1","uid":2027323,"ip_address":"","ucode":"BCDD3367AC16FD","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIyPPFIyvytj0LJrpHicVrTqibuLWLWcR5VqzArSHZicwJYC6gKrIF6GTxx4MakS6xiaxZBCw8icCPB8wQ/132","comment_is_top":false,"comment_ctime":1632819215,"is_pvip":true,"replies":[{"id":"113823","content":"抱歉，我没有使用或者研究过MaxCompute。不过我相信市场上主流的大数据方案其实底层逻辑都是相同的。<br><br>我简单看了一下MaxCompute的介绍，我觉得MaxCompute更像一个“产品”而不是一种专门的“技术”，是对多种数据需求，包装成了一个整体的产品供云计算的用户使用。","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1632918044,"ip_address":"","comment_id":314052,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1632819215","product_id":100091101,"comment_content":"阿里自研MaxCompute大规模集群计算有什么特别创新的地方么，这块有相关资料或者论文参考下么？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527558,"discussion_content":"抱歉，我没有使用或者研究过MaxCompute。不过我相信市场上主流的大数据方案其实底层逻辑都是相同的。\n\n我简单看了一下MaxCompute的介绍，我觉得MaxCompute更像一个“产品”而不是一种专门的“技术”，是对多种数据需求，包装成了一个整体的产品供云计算的用户使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632918044,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2027323,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIyPPFIyvytj0LJrpHicVrTqibuLWLWcR5VqzArSHZicwJYC6gKrIF6GTxx4MakS6xiaxZBCw8icCPB8wQ/132","nickname":"Geek_2e6a7e","note":"","ucode":"BCDD3367AC16FD","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399475,"discussion_content":"很好奇内部是不是有啥黑科技或者创新的地方。比如这篇文章《英特尔助阿里云MaxCompute以100TB数据规模创世界纪录》","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632972833,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314034,"user_name":"zhanyd","can_delete":false,"product_type":"c1","uid":1073845,"ip_address":"","ucode":"4C994EE512A3C4","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/b5/4159fa05.jpg","comment_is_top":false,"comment_ctime":1632814480,"is_pvip":false,"replies":[{"id":"113917","content":"👍","user_name":"作者回复","user_name_real":"徐文浩","uid":"1053568","ctime":1633164419,"ip_address":"","comment_id":314034,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1632814480","product_id":100091101,"comment_content":"适合自己的就是最好的，不一定要去追求什么高大上的技术，能够低成本满足需求的就是好方案。创新不一定是要创造出什么新东西， 把一些东西按适当的方式组合在一起也是创新。","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527551,"discussion_content":"👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633164419,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313871,"user_name":"在路上","can_delete":false,"product_type":"c1","uid":1402511,"ip_address":"","ucode":"6E31908EFE1107","user_header":"https://static001.geekbang.org/account/avatar/00/15/66/8f/02be926d.jpg","comment_is_top":false,"comment_ctime":1632724913,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1632724913","product_id":100091101,"comment_content":"徐老师好，分布式系统的提出就是因为单机硬件资源受限，需要将计算和存储任务交给多台计算机协同完成，传统的网络拓扑结构让网络资源不能像cpu、memory和disk一样分片，所以网络往往成为分布式系统的瓶颈。在多级分流系统中，常常分层处理网络流量，减少流量向深层节点传播。比如设计秒杀系统时，可以在第一层随机让一部分请求失败，防止流量把下游的系统打垮。在人脸识别系统中，可以利用边缘节点检查基本特征，做到快速失败。在大数据系统中，采用计算向数据靠拢的原则，减少流量在集群中传输。硬盘在写入数据时为了提高性能，会先把数据写入页缓存，再批量持久化。如果为了保证数据不丢失，让数据立即持久化，会降低写入性能，更好的做法是提供多个写入节点，用更多的服务器来避免故障。","like_count":0}]}