{"id":816673,"title":"02｜如何善用LLaMA 3长文本处理能力？","content":"<p>你好，我是Tyler。</p><p>在之前的课程中，我们深入探讨了如何利用 LLaMA 模型进行对话，并明确了LLaMA 在多轮对话中的核心机制。这一机制通过存储历史对话内容，将无状态的大模型推理服务转变为有状态的多轮对话服务。</p><p>这一机制显著提升了对话系统在处理复杂对话情境时的能力，但也带来了新的挑战：随着历史对话轮次的增加，模型的输入长度也随之增长。为应对这一挑战，大模型必须具备处理长文本输入的能力，这不仅是对现有技术的扩展，也是长文本处理能力发展的关键驱动力。</p><p>本节课我们将深入探讨处理长文本输入时的关键技术工作，包括当前的解决方案、优势及其局限性。我们将系统性地分析如何优化长文本输入的处理，以满足大规模对话系统对长文本的需求，并评估这些技术方案在实际应用中的表现和潜在改进方向。</p><h2>长文本处理的发展与现状</h2><p>LLaMA 模型的长文本处理能力有了显著提升。从 LLaMA 2 的 2048 个 token 扩展到 4096 个 token，再到 LLaMA 3 支持的 8000 个 token，甚至通过进一步微调可以处理更长的文本。这意味着这些模型现在可以处理更长的文本，理解能力也得到了增强，生成的回应更加连贯，相关性也更强。图表展示了不同模型在长文本处理方面的进展。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/b5/b5/b558303bce1d3801c0fc0bd42575d5b5.png?wh=400x591\" alt=\"图片\"></p><p>长文本处理的发展不仅是技术的进步，也是对实际应用需求的响应。长文本处理的能力直接影响到大规模对话系统在复杂场景下的表现。</p><h2>LLM 长文本的必要性</h2><p>在开篇的时候，我们提到了长文本对多轮对话的重要性，包括智能体自我或彼此之间的复杂提示词应用场景。长文本支持的多轮对话场景中，提示词的长度限制会影响智能体的思考深度。</p><p>下图展示了基于 ReAct 提示词的智能体多轮思考能力。可以观察到，智能体在每一步根据最新的外部数据判断是否能够完成任务。提示词的长度在一定程度上限制了智能体思考的深度。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/ce/e940161046168f3f8f0f4a9608831dce.png?wh=818x699\" alt=\"图片\"></p><p>以下是一个代码示例，展示如何使用 LLaMA 进行对话任务：</p><pre><code class=\"language-python\">from langchain_ollama.llms import OllamaLLM\nfrom langchain.agents import load_tools, initialize_agent\nfrom langchain.tools import Tool\nimport numexpr as ne\n\n# 定义数学计算工具\ndef calculate_expression(expression):\n&nbsp; &nbsp; return ne.evaluate(expression).item()&nbsp; # 使用 numexpr 计算表达式并返回结果\n\nmath_tools = [\n&nbsp; &nbsp; Tool(\n&nbsp; &nbsp; &nbsp; &nbsp; name=\"Numexpr Math\",\n&nbsp; &nbsp; &nbsp; &nbsp; func=calculate_expression,\n&nbsp; &nbsp; &nbsp; &nbsp; description=\"一个能够进行高效数学计算的工具，参数是输入的数学表达式\"\n&nbsp; &nbsp; )\n]\n\n# 初始化 LLM\nllm = OllamaLLM(model=\"llama3\")\n\n# 定义 agent\nagent = initialize_agent(\n&nbsp; &nbsp; tools=math_tools,&nbsp; # 使用自定义的数学计算工具\n&nbsp; &nbsp; llm=llm,&nbsp;\n&nbsp; &nbsp; agent_type=\"zero-shot-react-description\",&nbsp;\n&nbsp; &nbsp; verbose=True,\n&nbsp; &nbsp; agent_kwargs={\"handle_parsing_errors\": True}&nbsp; # 处理解析错误\n)\n\n# 用户问题\nuser_question = \"What is 37593 * 67?\"\nprint(f\"用户问题：{user_question}\")\n\n# 执行并打印结果\nresponse = agent.run(user_question)\nprint(response)\n\n</code></pre><p>假设上述代码处理一个关于计算问题的初始提示，并经过多轮对话，可能的输出如下：</p><pre><code class=\"language-plain\">Question: What is 37593 * 67?\nThought: Let's get started!\n\nThought: I need to perform a mathematical operation using Numexpr Math.\n\nAction: Numexpr Math\n\nAction Input: 37593 * 67\nObservation: 2518731\nThought:I'm excited to help! Here are the answers:\n\nFinal Answer: 2518731\n\n&gt; Finished chain.\n</code></pre><p>但是这都谈不上长文本输入的刚需，因为存在一些提示词压缩的替代方案。真正长文本的“刚需”场景，我们只能把一个完整的文件放进去，需要通过上下文内容联合分析，才能生成出目标内容的场景。</p><p>长文本处理的“刚需”场景包括处理完整的文档，如文章、代码、脚本、报告等。以下是一个将 Python 代码翻译为 JavaScript 的例子，展示了全文输入的必要性：</p><pre><code class=\"language-python\">import ollama\n\ndef translate_code_to_js(python_code):\n&nbsp; &nbsp; # 优化翻译提示\n&nbsp; &nbsp; prompt = f\"\"\"\nYou are a code translation assistant. Your task is to translate Python code into JavaScript while ensuring the following:\n\n1. Maintain the original logic and functionality.\n2. Adapt Python-specific constructs to their JavaScript equivalents.\n3. Use clear and idiomatic JavaScript syntax.\n\nHere is the Python code you need to translate:\n\nPython Code:\n{python_code}\n\nPlease provide the corresponding JavaScript code below:\nJavaScript Code:\n\"\"\"\n\n&nbsp; &nbsp; # 使用 Ollama 模型 'llama3' 进行对话\n&nbsp; &nbsp; response = ollama.chat(model='llama3', messages=[\n&nbsp; &nbsp; &nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'role': 'user',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'content': prompt,\n&nbsp; &nbsp; &nbsp; &nbsp; },\n&nbsp; &nbsp; ])\n\n&nbsp; &nbsp; # 获取生成的 JavaScript 代码\n&nbsp; &nbsp; js_code = response['message']['content']\n&nbsp; &nbsp; return js_code\n\n# 示例 Python 代码\npython_code = \"\"\"\ndef fibonacci(n):\n&nbsp; &nbsp; a, b = 0, 1\n&nbsp; &nbsp; while n &gt; 0:\n&nbsp; &nbsp; &nbsp; &nbsp; yield a\n&nbsp; &nbsp; &nbsp; &nbsp; a, b = b, a + b\n&nbsp; &nbsp; &nbsp; &nbsp; n -= 1\n\nfor number in fibonacci(5):\n&nbsp; &nbsp; print(number)\n\"\"\"\n\n# 调用函数并打印 JavaScript 代码\nprint(translate_code_to_js(python_code))\n\n</code></pre><p>输出的代码：</p><pre><code class=\"language-javascript\">function* fibonacci(n) {\n&nbsp; let a = 0, b = 1;\n&nbsp; for (let i = 0; i &lt; n; i++) {\n&nbsp; &nbsp; yield a;\n&nbsp; &nbsp; [a, b] = [b, a + b];\n&nbsp; }\n}\n\nfor (const number of fibonacci(5)) {\n&nbsp; console.log(number);\n}\n</code></pre><p>这个例子表明，通过多次输入不同的代码片段，很难让模型生成全局一致的变量声明和逻辑表达。因此，长文本输入在一些场景下是不可或缺的。</p><p>长文本处理能力的快速发展，不由让大家提出了一个新的问题：提示语工程方法，比如 RAG 和思维链（CoT），是否仍然必要？</p><h2>长文本处理的局限性</h2><h3>长文本处理性能问题</h3><p>上节课我们讨论了LLaMA生成内容的方法。我们了解到，LLaMA在处理长文本时，会面临很大的算力开销。接下来，我们将更详细地了解LLaMA的工作原理，特别是它的自回归生成机制如何影响推理时的算力需求，以及为什么长文本会消耗更多资源。</p><p>这里快速回顾一下，LLaMA模型的文本生成依赖于自回归方法。自回归生成方法逐步生成每一个词：</p><ul>\n<li>\n<p>初始化上下文：模型从一个起始文本或种子信息开始。</p>\n</li>\n<li>\n<p>预测下一个词：每生成一个新词，模型会根据当前上下文（已经生成的词）来预测下一个最可能的词。它会计算当前上下文与词汇表中所有可能词汇的相似度，找到最合适的下一个词。</p>\n</li>\n<li>\n<p>更新上下文：生成的新词会被添加到上下文中，模型会更新状态，准备生成下一个词。这一过程会一直进行，直到生成完整的文本。</p>\n</li>\n</ul><p>自回归方法的推理算力开销和输入文本的长度有很大关系。</p><ol>\n<li>\n<p>自注意力机制的计算复杂度：由于自注意力机制的计算复杂度为O(n^2)，其中n是序列长度，处理长文本时计算复杂度会显著增加。每新增一个位置，模型需要处理和记忆整个上下文，从而导致计算时间和资源需求以平方级增长。</p>\n</li>\n<li>\n<p>显存占用：处理长文本不仅增加了计算复杂度，还增加了显存的需求。由于模型需要在生成每一个词时保留和计算之前所有位置的状态，显存占用也随序列长度增加而增加。</p>\n</li>\n</ol><p>因此，尽管LLaMA模型在处理文本生成时展现了强大的能力，但长文本的输入仍然会导致显著的资源消耗。这包括计算时间和显存的需求。因此，在实际应用中，需要对长文本进行合理的裁剪和预处理，以平衡资源的消耗并保持模型的高效运行。</p><p>生成长文本通常需要大量的计算和通信资源，并且耗时较长。在这种情况下，RAG 的关键作用是提升生成质量的同时保持效率。例如，在处理长文档时，我们往往只需要其中的一个关键片段来回答问题。下面基于向量数据库和 LLaMA 3 RAG 的例子可以很好地说明这一点。</p><pre><code class=\"language-javascript\">import chromadb\nfrom chromadb.config import Settings\nfrom FlagEmbedding import BGEM3FlagModel\nimport ollama&nbsp; # 导入Ollama库\n\n# 初始化Chroma数据库\nchroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n\n# 创建一些测试文档\ndocuments = [\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"合同是两方或多方之间的法律协议，通常包括各方的权利和义务。合同必须具备合法性和可执行性。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc1\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"在合同中，主要义务包括：1) 付款义务，2) 商品交付义务，3) 相关服务的提供。合同中的这些义务必须在约定的时间内履行。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc2\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"合同的解除通常需要双方的同意，或者由于法律规定的特殊情况，如违约或不可抗力事件。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc3\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"违约责任是指一方未能履行合同义务时，应承担的法律后果，通常包括赔偿损失和继续履行合同的责任。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc4\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"在合同生效之前，所有相关方必须理解合同条款，并同意其内容。签字是合同生效的重要标志。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc5\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"合约的履行必须符合诚信原则，即各方应诚实守信地履行自己的义务，并尊重对方的合法权益。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc6\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"在合同争议中，双方可通过调解、仲裁或诉讼的方式解决争端。选择合适的方式取决于争议的性质及金额。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc7\"}\n&nbsp; &nbsp; },\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; \"page_content\": \"关于合同的法律法规各国有所不同，了解适用的法律条款是签订合同前的重要步骤。\",\n&nbsp; &nbsp; &nbsp; &nbsp; \"metadata\": {\"id\": \"doc8\"}\n&nbsp; &nbsp; }\n]\n\n# 初始化BGE M3模型\nmodel = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n\n# 将文档添加到向量存储中\ndocumentation_collection = chroma_client.get_or_create_collection(name=\"legal_docs\")\n\n# 生成文档嵌入并添加到集合中\nfor doc in documents:\n&nbsp; &nbsp; embedding = model.encode([doc['page_content']], batch_size=1)['dense_vecs'][0]\n&nbsp; &nbsp; documentation_collection.add(\n&nbsp; &nbsp; &nbsp; &nbsp; ids=[doc['metadata']['id']],&nbsp; # 假设文档有唯一的id\n&nbsp; &nbsp; &nbsp; &nbsp; embeddings=[embedding],\n&nbsp; &nbsp; &nbsp; &nbsp; documents=[doc['page_content']]\n&nbsp; &nbsp; )\n\n# 查询示例\nquery = \"合同中的主要义务是什么？\"\nquery_embedding = model.encode([query], batch_size=1)['dense_vecs'][0]\n\n# 执行向量查询\nresults = documentation_collection.query(\n&nbsp; &nbsp; query_embeddings=[query_embedding],\n&nbsp; &nbsp; n_results=1&nbsp; # 获取最相似的一个结果\n)\n\n# 提取检索到的文档内容\ndata = results['documents'][0]&nbsp; # 假设只检索到一个结果\ndocument_content = data&nbsp; # 这里取出文档内容\n\n# 将上下文与查询一起传递给 Ollama LLM\nprompt = f\"根据以下信息，请回答：{query}\"\n\n# 使用Ollama生成响应\noutput = ollama.chat(model='llama3', messages=[\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; 'role': 'user',\n&nbsp; &nbsp; &nbsp; &nbsp; 'content': f\"使用以下数据：{document_content}. 响应这个提示：{prompt}\"\n&nbsp; &nbsp; },\n])\n\n# 输出生成的结果\nprint(\"生成的结果：\", output['message']['content'])\n</code></pre><p>在这段代码中，RAG 模型通过检索与问题相关的文本片段来生成回应。这种方法可以减少长文本处理的计算负担，提高响应速度。</p><p>另一方面，思维链方法可以提升生成内容的质量，但长文本的信息量过大时，LLM（大语言模型）的指令理解能力会显著下降。这与思维链的目标，即通过分解和聚焦指令，正好相反。通过思维链多步推理的 Python 例子，可以更清楚地看到这一点。</p><pre><code class=\"language-javascript\">from langchain.chains import LLMChain, SequentialChain\nfrom langchain_ollama.llms import OllamaLLM\nfrom langchain.prompts import PromptTemplate\n\n# 创建 LLM 实例\nllm = OllamaLLM(model=\"llama3\")&nbsp; # 使用 Ollama 模型\n\ndef create_chain(template: str, input_vars: list, output_key: str) -&gt; LLMChain:\n&nbsp; &nbsp; \"\"\"创建 LLMChain 实例\"\"\"\n&nbsp; &nbsp; prompt = PromptTemplate(\n&nbsp; &nbsp; &nbsp; &nbsp; input_variables=input_vars,\n&nbsp; &nbsp; &nbsp; &nbsp; template=template\n&nbsp; &nbsp; )\n&nbsp; &nbsp; return LLMChain(llm=llm, prompt=prompt, output_key=output_key)\n\n# 第一步：头脑风暴解决方案\ntemplate_step1 = \"\"\"\n步骤 1:\n我面临一个关于{input}的问题。请提供三个不同的解决方案，考虑到以下因素：{perfect_factors}。\nA:\n\"\"\"\nchain1 = create_chain(template_step1, [\"input\", \"perfect_factors\"], \"solutions\")\n\n# 第二步：评估解决方案\ntemplate_step2 = \"\"\"\n步骤 2:\n请评估以下解决方案的优缺点、实施难度和预期结果，并为每个方案分配成功概率和信心水平。\n{solutions}\nA:\n\"\"\"\nchain2 = create_chain(template_step2, [\"solutions\"], \"review\")\n\n# 第三步：深化思考过程\ntemplate_step3 = \"\"\"\n步骤 3:\n请深入分析每个解决方案，提供实施策略、所需资源和潜在障碍，同时考虑意外结果及应对措施。\n{review}\nA:\n\"\"\"\nchain3 = create_chain(template_step3, [\"review\"], \"deepen_thought_process\")\n\n# 第四步：排序解决方案\ntemplate_step4 = \"\"\"\n步骤 4:\n根据评估和分析结果，对解决方案进行排序，说明理由，并给出最终考虑。\n{deepen_thought_process}\nA:\n\"\"\"\nchain4 = create_chain(template_step4, [\"deepen_thought_process\"], \"ranked_solutions\")\n\n# 将各个链条连接起来\noverall_chain = SequentialChain(\n&nbsp; &nbsp; chains=[chain1, chain2, chain3, chain4],\n&nbsp; &nbsp; input_variables=[\"input\", \"perfect_factors\"],\n&nbsp; &nbsp; output_variables=[\"ranked_solutions\"],\n&nbsp; &nbsp; verbose=True\n)\n\n# 示例输入\nresult = overall_chain({\n&nbsp; &nbsp; \"input\": \"人类对火星的殖民\",\n&nbsp; &nbsp; \"perfect_factors\": \"地球与火星之间的距离非常遥远，使得定期补给变得困难\"\n})\n\nprint(result)\n\n</code></pre><p><strong>因此，CoT 和 RAG 依然是提升长文本生成质量的重要方法，仍然具有不可或缺的价值</strong>。</p><h2>面向超长文本的定制与优化</h2><p>随着大模型技术的发展，处理超长文本的能力也在不断提升，除了我们前面提到的标准版本 LLaMA之外，我们还有专门针对超长文本处理的定制版本。例如，针对法律文书的处理，我们可以利用 1M（百万级别）上下文长度的模型来处理复杂的法律文本。这些超长文本处理能力可以有效地解决全局用词一致性问题。以下是一些面向长文本的优化版本：</p><pre><code class=\"language-javascript\">import ollama\n\n\nwith open('data/legal_document.txt', 'r', encoding='utf-8') as f:\n&nbsp; &nbsp; legal_text = f.read()\n\n# 系统提示词，指定模型的背景信息和回答风格\nSYSTEM_PROMPT = f'法律文书内容: {legal_text}\\n\\n' + \"\"\"\n上下文: 你是一个法律助手，基于提供的法律文书内容提供答案。\n你只能讨论文书中的内容。用户将询问有关文书的具体问题，你需要基于文书内容提供详细的回答。\n如果信息不足以回答问题，请要求用户提供更具体的文书部分。\n\"\"\"\nQUESTION = '文中关于合同中的主要义务的描述是否一致？'\n\n# 构造聊天消息\nmessages = [\n&nbsp; &nbsp; {'role': 'system', 'content': SYSTEM_PROMPT},\n&nbsp; &nbsp; {'role': 'user', 'content': QUESTION},\n]\n\n# 调用Ollama的LLM生成结果\nresponse = ollama.chat(model='llama3-gradient', messages=messages)\n\n# 输出结果\nprint('\\n\\n')\nprint(f'系统提示: ...{SYSTEM_PROMPT[-500:]}')&nbsp; # 打印最后500个字符的系统提示\nprint(f'用户提问:', QUESTION)\nprint('回答:')\nprint(response['message']['content'])&nbsp; # 输出生成的回答\n</code></pre><h2>小结</h2><p>好的，我们来总结一下这节课的内容吧。这节课我们讨论了长文本处理的几个关键点。</p><p>首先，我们聊到了长文本处理的现状以及它在多轮对话、智能体和上下文分析中的重要性。长文本处理能力现在成了各大公司争相提升的焦点，因为它在处理复杂信息时的价值非常大。</p><p>但要注意，长文本处理并不是万能的解决方案。由于LLM的Next Token Prediction的特性，处理长文本会消耗很多算力，所以我们得在必要时才用它。也就是说，虽然长文本处理很重要，但它的高计算开销意味着我们要谨慎使用。</p><p>另外，虽然像CoT这样的多步推理技术在优化性能方面表现不错，但它们还不能完全替代长文本处理的能力。这说明我们需要学会在实际应用中，判断什么时候该用长文本处理，什么时候可以用其他优化方法。</p><p>最后，我们举了一个例子，展示了针对长文本优化的LLaMA 3模型。这个模型能处理百万级别的数据，并在一些特定场景中展现了它的独特价值。这个例子说明了长文本处理在实际应用中的潜力和必要性。</p><h2>思考题</h2><p>具备长文本处理能力后，是否意味着我们可以将所有内容一并交给大模型处理呢？欢迎你把思考后的结果分享到留言区，和我一起讨论，如果你觉得这节课的内容对你有帮助的话，欢迎你分享给其他朋友，我们下节课再见！</p>","comments":[{"had_liked":false,"id":395933,"user_name":"小虎子🐯","can_delete":false,"product_type":"c1","uid":2843479,"ip_address":"北京","ucode":"4C9530B3FB407B","user_header":"https://static001.geekbang.org/account/avatar/00/2b/63/57/cba4c68b.jpg","comment_is_top":true,"comment_ctime":1732500616,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"课程代码地址：https:&#47;&#47;github.com&#47;tylerelyt&#47;LLaMa-in-Action","like_count":0},{"had_liked":false,"id":396436,"user_name":"午夜修铃","can_delete":false,"product_type":"c1","uid":2413066,"ip_address":"北京","ucode":"EF8DB6CD219CC1","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/iaK3hr7TO6WRFgmFkia0Q75ThkpyIOQtnXSttpI1ib8ianXxbb6LlPgPLZNOj5niaZnBt6Htps2Jia11v9l2VeL7EPibw/132","comment_is_top":false,"comment_ctime":1734447520,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":1,"score":2,"product_id":100828301,"comment_content":"Windows本地模型，使用ollama时如何调用本地显卡？","like_count":0},{"had_liked":false,"id":396435,"user_name":"午夜修铃","can_delete":false,"product_type":"c1","uid":2413066,"ip_address":"北京","ucode":"EF8DB6CD219CC1","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/iaK3hr7TO6WRFgmFkia0Q75ThkpyIOQtnXSttpI1ib8ianXxbb6LlPgPLZNOj5niaZnBt6Htps2Jia11v9l2VeL7EPibw/132","comment_is_top":false,"comment_ctime":1734447201,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":1,"score":2,"product_id":100828301,"comment_content":"生成的结果： 根据给出的数据，合同中的主要义务包括：\n\n1. 付款义务\n2. 商品交付义务\n3. 相关服务的提供\nException ignored in: &lt;function AbsEmbedder.__del__ at 0x00000174DDF64540&gt;\nTraceback (most recent call last):\n  File &quot;C:\\Users\\xxx\\miniconda3\\Lib\\site-packages\\FlagEmbedding\\abc\\inference\\AbsEmbedder.py&quot;, line 270, in __del__\n  File &quot;C:\\Users\\xxx\\miniconda3\\Lib\\site-packages\\FlagEmbedding\\abc\\inference\\AbsEmbedder.py&quot;, line 89, in stop_self_pool\nTypeError: &#39;NoneType&#39; object is not callable\n\n这里报错了。\nflagembedding             1.3.3                   ","like_count":0},{"had_liked":false,"id":396079,"user_name":"江慧明","can_delete":false,"product_type":"c1","uid":1008096,"ip_address":"广东","ucode":"A6CC6836A7A085","user_header":"https://static001.geekbang.org/account/avatar/00/0f/61/e0/c20f2457.jpg","comment_is_top":false,"comment_ctime":1732976340,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":1,"score":2,"product_id":100828301,"comment_content":"具备长文本处理能力，也意味着需要更多资源，显存资源，CPU资源，也意味着回答用户问题需要更长时间，所以具备长文本处理能力模型，在具体实践中也不能毫无节制使用，而是充分有理由使用，为取得速度、资源、上下文的平衡，我们需要充分设计，利用langchain工具，可以把问题拆分多个聚焦点，然后用LLM快速获得答案，然后再讲问题组合","like_count":0},{"had_liked":false,"id":395301,"user_name":"旅梦开发团","can_delete":false,"product_type":"c1","uid":1622373,"ip_address":"江西","ucode":"B8A284C955B8CD","user_header":"https://static001.geekbang.org/account/avatar/00/18/c1/65/2fb5c4ce.jpg","comment_is_top":false,"comment_ctime":1730167336,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"在合同RAG的案例中 我安装FlagEmbedding报错了，可能是python版本的问题。 我使用nomic-embed-text:latest 嵌入模型代替， 大家也可以试试。 我的完整代码如下\n```python\n# 向量数据库\nimport chromadb\n# Settings 配置\nfrom chromadb.config import Settings\n# 生成器嵌入 图像处 自然语言处理\n# from FlagEmbedding import BGEM3FlagModel\nimport ollama\n\nchroma_client = chromadb.PersistentClient(path=&quot;.&#47;chromadb&quot;)\n# document, metadata embedding\ndocuments = [\n...\n]\n\ndocumentation_collection = chroma_client.get_or_create_collection(name=&quot;legal_docs&quot;)\n\nfor doc in documents:\n  embedding = ollama.embeddings(model=&#39;nomic-embed-text:latest&#39;,prompt=doc[&#39;page_content&#39;])\n  # print(embedding.get(&#39;embedding&#39;))\n  # id, embedding, 原内容\n  documentation_collection.add(\n    ids=[doc[&#39;metadata&#39;][&#39;id&#39;]],\n    embeddings=embedding.get(&#39;embeddings&#39;),\n    documents=[doc[&#39;page_content&#39;]]\n  )\n\nquery = &quot;合同是什么？&quot;\nquery_embedding = ollama.embeddings(model=&#39;nomic-embed-text:latest&#39;, prompt=query)\nquery_embedding = query_embedding.get(&#39;embedding&#39;)[:384] \n# print(query_embedding.get())\n# # # 根据embedding 查询\nresults = documentation_collection.query(\n  query_embeddings = query_embedding,\n  n_results=1\n)\n# # # # 取出原内容\ndata = results[&#39;documents&#39;][0]\ndocument_content = data\n\nprompt = f&quot;根据一下信息， 请回答：{query}&quot;\n\noutput = ollama.chat(model=&#39;qwen2.5:latest&#39;, messages=[\n  {\n    &#39;role&#39;: &#39;user&#39;,\n    &#39;content&#39;: f&quot;使用一下数据：{document_content},响应这个提示：{prompt}&quot;\n  }\n])\n\nprint(&quot;生成的结果：&quot;, output[&#39;message&#39;][&#39;content&#39;])\n```","like_count":0},{"had_liked":false,"id":395134,"user_name":"willmyc","can_delete":false,"product_type":"c1","uid":1016703,"ip_address":"广东","ucode":"486B28F89DFBEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/83/7f/7b1f3f68.jpg","comment_is_top":false,"comment_ctime":1729568404,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"超长文本是否应该考虑跟llama3多轮对话的情况，llama3如何处理，可否介绍一下，谢谢！","like_count":0}]}