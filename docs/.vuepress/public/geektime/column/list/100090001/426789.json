{"id":426789,"title":"16 | 数据转换：如何在DataFrame之上做数据处理？","content":"<p>你好，我是吴磊。</p><p>在上一讲，我们学习了创建DataFrame的各种途径与方法，那么，有了DataFrame之后，我们该如何在DataFrame之上做数据探索、数据分析，以及各式各样的数据转换呢？在数据处理完毕之后，我们又该如何做数据展示与数据持久化呢？今天这一讲，我们就来解答这些疑问。</p><p>为了给开发者提供足够的灵活性，对于DataFrame之上的数据处理，Spark SQL支持两类开发入口：一个是大家所熟知的结构化查询语言：SQL，另一类是DataFrame开发算子。就开发效率与执行效率来说，二者并无优劣之分，选择哪种开发入口，完全取决于开发者的个人偏好与开发习惯。</p><p>与RDD类似，DataFrame支持种类繁多的开发算子，但相比SQL语言，DataFrame算子的学习成本相对要高一些。因此，本着先易后难的思路，咱们先来说说DataFrame中SQL语句的用法，然后再去理解DataFrame开发算子。</p><h2>SQL语句</h2><p>对于任意的DataFrame，我们都可以使用createTempView或是createGlobalTempView在Spark SQL中创建临时数据表。</p><p>两者的区别在于，createTempView创建的临时表，其生命周期仅限于SparkSession内部，而createGlobalTempView创建的临时表，可以在同一个应用程序中跨SparkSession提供访问。有了临时表之后，我们就可以使用SQL语句灵活地倒腾表数据。</p><!-- [[[read_end]]] --><p>通过后面这段代码，我为你演示了如何使用createTempView创建临时表。我们首先用toDF创建了一个包含名字和年龄的DataFrame，然后调用createTempView方法创建了临时表。</p><pre><code class=\"language-scala\">import org.apache.spark.sql.DataFrame\nimport spark.implicits._\n&nbsp;\nval seq = Seq((\"Alice\", 18), (\"Bob\", 14))\nval df = seq.toDF(\"name\", \"age\")\n&nbsp;\ndf.createTempView(\"t1\")\nval query: String = \"select * from t1\"\n// spark为SparkSession实例对象\nval result: DataFrame = spark.sql(query)\n&nbsp;\nresult.show\n&nbsp;\n/** 结果打印\n+-----+---+\n| n ame| age|\n+-----+---+\n| Alice| 18|\n| Bob| 14|\n+-----+---+\n*/\n</code></pre><p>以上表为例，我们先是使用spark.implicits._隐式方法通过toDF来创建DataFrame，然后在其上调用createTempView来创建临时表“t1”。接下来，给定SQL查询语句“query”，我们可以通过调用SparkSession提供的sql API来提请执行查询语句，得到的查询结果被封装为新的DataFrame。</p><p>值得一提的是，与RDD的开发模式一样，<strong>DataFrame之间的转换也属于延迟计算，当且仅当出现Action类算子时，如上表中的show，所有之前的转换过程才会交付执行</strong>。</p><p>Spark SQL采用<a href=\"https://www.antlr.org/\">ANTLR</a>语法解析器，来解析并处理SQL语句。我们知道，ANTLR是一款强大的、跨语言的语法解析器，因为它全面支持SQL语法，所以广泛应用于Oracle、Presto、Hive、ElasticSearch等分布式数据仓库和计算引擎。因此，像Hive或是Presto中的SQL查询语句，都可以平滑地迁移到Spark SQL。</p><p>不仅如此，Spark SQL还提供大量Built-in Functions（内置函数），用于辅助数据处理，如array_distinct、collect_list，等等。你可以浏览官网的<a href=\"https://spark.apache.org/docs/3.0.1/api/sql/index.html\">Built-in Functions页面</a>查找完整的函数列表。结合SQL语句以及这些灵活的内置函数，你就能游刃有余地应对数据探索、数据分析这些典型的数据应用场景。</p><p>SQL语句相对比较简单，学习路径短、成本低，你只要搞清楚如何把DataFrame转化为数据表，剩下的事就水到渠成了。接下来，我们把主要精力放在DataFrame支持的各类算子上，这些算子提供的功能，往往能大幅提升开发效率，让我们事半功倍。</p><h2>DataFrame算子</h2><p>不得不说，DataFrame支持的算子丰富而又全面，这主要源于DataFrame特有的“双面”属性。一方面，DataFrame来自RDD，与RDD具有同源性，因此RDD支持的大部分算子，DataFrame都支持。另一方面，DataFrame携带Schema，是结构化数据，因此它必定要提供一套与结构化查询同源的计算算子。</p><p>正是由于这样“双面”的特性，我们从下图可以看到，DataFrame所支持的算子，用“琳琅满目”来形容都不为过。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/97/50bd70a2dcf01b631eff86c286d9eb97.jpg?wh=1920x442\" alt=\"图片\" title=\"DataFrame算子大全\"></p><p>人类的大脑偏好结构化的知识，为了方便你记忆与理解，我把DataFrame上述两个方面的算子，进一步划分为6大类，<strong>它们分别是RDD同源类算子、探索类算子、清洗类算子、转换类算子、分析类算子和持久化算子</strong>。</p><p>你可能会困扰：“天呐！这么多算子要学，这不是逼我从入门到放弃吗？”别着急，上面这张图，你可以把它当作是“DataFrame算子脑图”，或是一本字典。在日常的开发中，思路枯竭的时候，你不妨把它翻出来，看看哪些算子能够帮你实现业务逻辑。</p><p>今天这一讲，我们也会根据这张“脑图”，重点讲解其中最常用、最关键的部分。</p><h3>同源类算子</h3><p>我们从DataFrame中的RDD同源类算子说起，这些算子在RDD算子那三讲做过详细的介绍，如果你对有哪个算子的作用或含义记不清了，不妨回看之前的三讲。我按照之前的分类，把这些算子整理成了一张表格。</p><p><img src=\"https://static001.geekbang.org/resource/image/5c/53/5c377yy9b592e2ae1a31cb68da2d5553.jpg?wh=1592x917\" alt=\"图片\" title=\"RDD同源类算子\"></p><h3>探索类算子</h3><p>接下来就是DataFrame的探索类算子。所谓探索，指的是数据探索，这类算子的作用，在于帮助开发者初步了解并认识数据，比如数据的模式（Schema）、数据的分布、数据的“模样”，等等，为后续的应用开发奠定基础。</p><p>对于常用的探索类算子，我把它们整理到了下面的表格中，你不妨先看一看，建立“第一印象”。</p><p><img src=\"https://static001.geekbang.org/resource/image/7c/15/7c95d03be0a733b1498ba4b99f2e1d15.jpg?wh=1588x789\" alt=\"图片\" title=\"探索类算子\"></p><p>我们来依次“避轻就重”地说一说这些算子。首先，columns/schema/printSchema这3个算子类似，都可以帮我们获取DataFrame的数据列和Schema。尤其是printSchema，它以纯文本的方式将Data Schema打印到屏幕上，如下所示。</p><pre><code class=\"language-scala\">import org.apache.spark.sql.DataFrame\nimport spark.implicits._\n&nbsp;\nval employees = Seq((1, \"John\", 26, \"Male\"), (2, \"Lily\", 28, \"Female\"), (3, \"Raymond\", 30, \"Male\"))\nval employeesDF: DataFrame = employees.toDF(\"id\", \"name\", \"age\", \"gender\")\n&nbsp;\nemployeesDF.printSchema\n&nbsp;\n/** 结果打印\nroot\n|-- id: integer (nullable = false)\n|-- name: string (nullable = true)\n|-- age: integer (nullable = false)\n|-- gender: string (nullable = true)\n*/\n</code></pre><p>了解数据模式之后，我们往往想知道数据具体长什么样子，对于这个诉求，show算子可以帮忙达成。在默认情况下，show会随机打印出DataFrame的20条数据记录。</p><pre><code class=\"language-scala\">employeesDF.show\n&nbsp;\n/** 结果打印\n+---+-------+---+------+\n| id| name|age|gender|\n+---+-------+---+------+\n| 1| John| 26| Male|\n| 2| Lily| 28|Female|\n| 3|Raymond| 30| Male|\n+---+-------+---+------+\n*/\n</code></pre><p>看清了数据的“本来面目”之后，你还可以进一步利用describe去查看数值列的统计分布。比如，通过调用employeesDF.describe(“age”)，你可以查看age列的极值、平均值、方差等统计数值。</p><p>初步掌握了数据的基本情况之后，如果你对当前DataFrame的执行计划感兴趣，可以通过调用explain算子来获得Spark SQL给出的执行计划。explain对于执行效率的调优来说，有着至关重要的作用，后续课程中我们还会结合具体的实例，来深入讲解explain的用法和释义，在这里，你仅需知道explain是用来查看执行计划的就好。</p><h3>清洗类算子</h3><p>完成数据探索以后，我们正式进入数据应用的开发阶段。在数据处理前期，我们往往需要对数据进行适当地“清洗”，“洗掉”那些不符合业务逻辑的“脏数据”。DataFrame提供了如下算子，来帮我们完成这些脏活儿、累活儿。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/1b/c1c55259a2f14870606ab4c182d8921b.jpg?wh=1588x789\" alt=\"图片\" title=\"清洗类算子\"></p><p>首先，drop算子允许开发者直接把指定列从DataFrame中予以清除。举个例子，对于上述的employeesDF，假设我们想把性别列清除，那么直接调用 employeesDF.drop(“gender”) 即可。如果要同时清除多列，只需要在drop算子中用逗号把多个列名隔开即可。</p><p>第二个是distinct，它用来为DataFrame中的数据做去重。还是以employeesDF为例，当有多条数据记录的所有字段值都相同时，使用distinct可以仅保留其中的一条数据记录。</p><p>接下来是dropDuplicates，它的作用也是去重。不过，与distinct不同的是，dropDuplicates可以指定数据列，因此在灵活性上更胜一筹。还是拿employeesDF来举例，这个DataFrame原本有3条数据记录，如果我们按照性别列去重，最后只会留下两条记录。其中，一条记录的gender列是“Male”，另一条的gender列为“Female”，如下所示。</p><pre><code class=\"language-scala\">employeesDF.show\n&nbsp;\n/** 结果打印\n+---+-------+---+------+\n| id| name|age|gender|\n+---+-------+---+------+\n| 1| John| 26| Male|\n| 2| Lily| 28|Female|\n| 3|Raymond| 30| Male|\n+---+-------+---+------+\n*/\n&nbsp;\nemployeesDF.dropDuplicates(\"gender\").show\n&nbsp;\n/** 结果打印\n+---+----+---+------+\n| id|name|age|gender|\n+---+----+---+------+\n| 2|Lily| 28|Female|\n| 1|John| 26| Male|\n+---+----+---+------+\n*/\n</code></pre><p>表格中的最后一个算子是na，它的作用是选取DataFrame中的null数据，na往往要结合drop或是fill来使用。例如，employeesDF.na.drop用于删除DataFrame中带null值的数据记录，而employeesDF.na.fill(0) 则将DataFrame中所有的null值都自动填充为整数零。这两种用例在数据清洗的场景中都非常常见，因此，你需要牢牢掌握na.drop与na.fill的用法。</p><p>数据清洗过后，我们就得到了一份“整洁而又干净”的数据，接下来，可以放心大胆地去做各式各样的数据转换，从而实现业务逻辑需求。</p><h3>转换类算子</h3><p>转换类算子的主要用于数据的生成、提取与转换。转换类的算子的数量并不多，但使用方式非常灵活，开发者可以变着花样地变换数据。</p><p><img src=\"https://static001.geekbang.org/resource/image/4a/77/4a3f705c5bde5a597yy5bf8c78b15e77.jpg?wh=1640x1047\" alt=\"图片\" title=\"转换类算子\"></p><p>首先，select算子让我们可以按照列名对DataFrame做投影，比如说，如果我们只关心年龄与性别这两个字段的话，就可以使用下面的语句来实现。</p><pre><code class=\"language-scala\">employeesDF.select(\"name\", \"gender\").show\n&nbsp;\n/** 结果打印\n+-------+------+\n| name|gender|\n+-------+------+\n| John| Male|\n| Lily|Female|\n|Raymond| Male|\n+-------+------+\n*/\n</code></pre><p>不过，虽然用起来比较简单，但select算子在功能方面不够灵活。在灵活性这方面，selectExpr做得更好。比如说，基于id和姓名，我们想把它们拼接起来生成一列新的数据。像这种需求，正是selectExpr算子的用武之地。</p><pre><code class=\"language-scala\">employeesDF.selectExpr(\"id\", \"name\", \"concat(id, '_', name) as id_name\").show\n&nbsp;\n/** 结果打印\n+---+-------+---------+\n| id| name| id_name|\n+---+-------+---------+\n| 1| John| 1_John|\n| 2| Lily| 2_Lily|\n| 3|Raymond|3_Raymond|\n+---+-------+---------+\n*/\n</code></pre><p>这里，我们使用concat这个函数，把id列和name列拼接在一起，生成新的id_name数据列。</p><p>接下来的where和withColumnRenamed这两个算子比较简单，where使用SQL语句对DataFrame做数据过滤，而withColumnRenamed的作用是字段重命名。</p><p>比如，想要过滤出所有性别为男的员工，我们就可以用employeesDF.where(“gender = ‘Male’”)来实现。如果打算把employeesDF当中的“gender”重命名为“sex”，就可以用withColumnRenamed来帮忙：employeesDF.withColumnRenamed(“gender”, “sex”)。</p><p>紧接着的是withColumn，虽然名字看上去和withColumnRenamed很像，但二者在功能上有着天壤之别。</p><p>withColumnRenamed是重命名现有的数据列，而withColumn则用于生成新的数据列，这一点上，withColumn倒是和selectExpr有着异曲同工之妙。withColumn也可以充分利用Spark SQL提供的Built-in Functions来灵活地生成数据。</p><p>比如，基于年龄列，我们想生成一列脱敏数据，隐去真实年龄，你就可以这样操作。</p><pre><code class=\"language-scala\">employeesDF.withColumn(\"crypto\", hash($\"age\")).show\n&nbsp;\n/** 结果打印\n+---+-------+---+------+-----------+\n| id| name|age|gender| crypto|\n+---+-------+---+------+-----------+\n| 1| John| 26| Male|-1223696181|\n| 2| Lily| 28|Female|-1721654386|\n| 3|Raymond| 30| Male| 1796998381|\n+---+-------+---+------+-----------+\n*/\n</code></pre><p>可以看到，我们使用内置函数hash，生成一列名为“crypto”的新数据，数据值是对应年龄的哈希值。有了新的数据列之后，我们就可以调用刚刚讲的drop，把原始的age字段丢弃掉。</p><p>表格中的最后一个算子是explode，这个算子很有意思，它的作用是展开数组类型的数据列，数组当中的每一个元素，都会生成一行新的数据记录。为了更好地演示explode的用法与效果，我们把employeesDF数据集做个简单的调整，给它加上一个interests兴趣字段。</p><pre><code class=\"language-scala\">val seq = Seq( (1, \"John\", 26, \"Male\", Seq(\"Sports\", \"News\")),\n(2, \"Lily\", 28, \"Female\", Seq(\"Shopping\", \"Reading\")),\n(3, \"Raymond\", 30, \"Male\", Seq(\"Sports\", \"Reading\"))\n)\n&nbsp;\nval employeesDF: DataFrame = seq.toDF(\"id\", \"name\", \"age\", \"gender\", \"interests\")\nemployeesDF.show\n&nbsp;\n/** 结果打印\n+---+-------+---+------+-------------------+\n| id| name|age|gender| interests|\n+---+-------+---+------+-------------------+\n| 1| John| 26| Male| [Sports, News]|\n| 2| Lily| 28|Female|[Shopping, Reading]|\n| 3|Raymond| 30| Male| [Sports, Reading]|\n+---+-------+---+------+-------------------+\n*/\n&nbsp;\nemployeesDF.withColumn(\"interest\", explode($\"interests\")).show\n&nbsp;\n/** 结果打印\n+---+-------+---+------+-------------------+--------+\n| id| name|age|gender| interests|interest|\n+---+-------+---+------+-------------------+--------+\n| 1| John| 26| Male| [Sports, News]| Sports|\n| 1| John| 26| Male| [Sports, News]| News|\n| 2| Lily| 28|Female|[Shopping, Reading]|Shopping|\n| 2| Lily| 28|Female|[Shopping, Reading]| Reading|\n| 3|Raymond| 30| Male| [Sports, Reading]| Sports|\n| 3|Raymond| 30| Male| [Sports, Reading]| Reading|\n+---+-------+---+------+-------------------+--------+\n*/\n</code></pre><p>可以看到，我们多加了一个兴趣列，列数据的类型是数组，每个员工都有零到多个兴趣。</p><p>如果我们想把数组元素展开，让每个兴趣都可以独占一条数据记录。这个时候就可以使用explode，再结合withColumn，生成一列新的interest数据。这列数据的类型是单个元素的String，而不再是数组。有了新的interest数据列之后，我们可以再次利用drop算子，把原本的interests列抛弃掉。</p><p>数据转换完毕之后，我们就可以通过数据的关联、分组、聚合、排序，去做数据分析，从不同的视角出发去洞察数据。这个时候，我们还要依赖Spark SQL提供的多个分析类算子。</p><h3>分析类算子</h3><p>毫不夸张地说，前面的探索、清洗、转换，都是在为数据分析做准备。<strong>在大多数的数据应用中，数据分析往往是最为关键的那环，甚至是应用本身的核心目的</strong>。因此，熟练掌握分析类算子，有利于我们提升开发效率。</p><p>Spark SQL的分析类算子看上去并不多，但灵活组合使用，就会有“千变万化”的效果，让我们一起看看。</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/1c/6f1b2yy4ceb3e7c5bbyyec25e04e791c.jpg?wh=1564x797\" alt=\"图片\" title=\"分析类算子\"></p><p>为了演示上述算子的用法，我们先来准备两张数据表：employees和salaries，也即员工信息表和薪水表。我们的想法是，通过对两张表做数据关联，来分析员工薪水的分布情况。</p><pre><code class=\"language-scala\">import spark.implicits._\nimport org.apache.spark.sql.DataFrame\n&nbsp;\n// 创建员工信息表\nval seq = Seq((1, \"Mike\", 28, \"Male\"), (2, \"Lily\", 30, \"Female\"), (3, \"Raymond\", 26, \"Male\"))\nval employees: DataFrame = seq.toDF(\"id\", \"name\", \"age\", \"gender\")\n&nbsp;\n// 创建薪水表\nval seq2 = Seq((1, 26000), (2, 30000), (4, 25000), (3, 20000))\nval salaries:DataFrame = seq2.toDF(\"id\", \"salary\")\n&nbsp;\nemployees.show\n&nbsp;\n/** 结果打印\n+---+-------+---+------+\n| id| name|age|gender|\n+---+-------+---+------+\n| 1| Mike| 28| Male|\n| 2| Lily| 30|Female|\n| 3|Raymond| 26| Male|\n+---+-------+---+------+\n*/\n&nbsp;\nsalaries.show\n&nbsp;\n/** 结果打印\n+---+------+\n| id|salary|\n+---+------+\n| 1| 26000|\n| 2| 30000|\n| 4| 25000|\n| 3| 20000|\n+---+------+\n*/\n</code></pre><p>那么首先，我们先用join算子把两张表关联起来，关联键（Join Keys）我们使用两张表共有的id列，而关联形式（Join Type）自然是内关联（Inner Join）。</p><pre><code class=\"language-scala\">val jointDF: DataFrame = salaries.join(employees, Seq(\"id\"), \"inner\")\n&nbsp;\njointDF.show\n&nbsp;\n/** 结果打印\n+---+------+-------+---+------+\n| id|salary| name|age|gender|\n+---+------+-------+---+------+\n| 1| 26000| Mike| 28| Male|\n| 2| 30000| Lily| 30|Female|\n| 3| 20000|Raymond| 26| Male|\n+---+------+-------+---+------+\n*/\n</code></pre><p>可以看到，我们在salaries之上调用join算子，join算子的参数有3类。第一类是待关联的数据表，在我们的例子中就是员工表employees。第二类是关联键，也就是两张表之间依据哪些字段做关联，我们这里是id列。第三类是关联形式，我们知道，关联形式有inner、left、right、anti、semi等等，这些关联形式我们下一讲再展开，这里你只需要知道Spark SQL支持这些种类丰富的关联形式即可。</p><p>数据完成关联之后，我们实际得到的仅仅是最细粒度的事实数据，也就是每个员工每个月领多少薪水。这样的事实数据本身并没有多少价值，我们往往需要从不同的维度出发，对数据做分组、聚合，才能获得更深入、更有价值的数据洞察。</p><p>比方说，我们想以性别为维度，统计不同性别下的总薪水和平均薪水，借此分析薪水与性别之间可能存在的关联关系。</p><pre><code class=\"language-scala\">val aggResult = fullInfo.groupBy(\"gender\").agg(sum(\"salary\").as(\"sum_salary\"), avg(\"salary\").as(\"avg_salary\"))\n&nbsp;\naggResult.show\n&nbsp;\n/** 数据打印\n+------+----------+----------+\n|gender|sum_salary|avg_salary|\n+------+----------+----------+\n|Female| 30000| 30000.0|\n| Male| 46000| 23000.0|\n+------+----------+----------+\n*/\n</code></pre><p>这里，我们先是使用groupBy算子按照“gender”列做分组，然后使用agg算子做聚合运算。在agg算子中，我们分别使用sum和avg聚合函数来计算薪水的总数和平均值。Spark SQL对于聚合函数的支持，我们同样可以通过<a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\">Built-in Functions页面</a>来进行检索。结合Built-in Functions提供的聚合函数，我们就可以灵活地对数据做统计分析。</p><p>得到统计结果之后，为了方便查看，我们还可以使用sort或是orderBy算子对结果集进行排序，二者在用法与效果上是完全一致的，如下表所示。</p><pre><code class=\"language-scala\">aggResult.sort(desc(\"sum_salary\"), asc(\"gender\")).show\n&nbsp;\n/** 结果打印\n+------+----------+----------+\n|gender|sum_salary|avg_salary|\n+------+----------+----------+\n| Male| 46000| 23000.0|\n|Female| 30000| 30000.0|\n+------+----------+----------+\n*/\n&nbsp;\naggResult.orderBy(desc(\"sum_salary\"), asc(\"gender\")).show\n&nbsp;\n/** 结果打印\n+------+----------+----------+\n|gender|sum_salary|avg_salary|\n+------+----------+----------+\n| Male| 46000| 23000.0|\n|Female| 30000| 30000.0|\n+------+----------+----------+\n*/\n</code></pre><p>可以看到，sort / orderBy支持按照多列进行排序，且可以通过desc和asc来指定排序方向。其中desc表示降序排序，相应地，asc表示升序排序。</p><p>好啦，到此为止，我们沿着数据的生命周期，分别梳理了生命周期不同阶段的Spark SQL算子，它们分别是探索类算子、清洗类算子、转换类算子和分析类算子。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/97/50bd70a2dcf01b631eff86c286d9eb97.jpg?wh=1920x442\" alt=\"图片\" title=\"数据生命周期与DataFrame算子\"></p><p>所谓行百里者半九十，纵观整个生命周期，我们还剩下数据持久化这一个环节。对于最后的这个持久化环节，Spark SQL提供了write API，与上一讲介绍的read API相对应，write API允许开发者把数据灵活地物化为不同的文件格式。</p><h3>持久化类算子</h3><p>没有对比就没有鉴别，在学习write API之前，我们不妨先来回顾一下上一讲介绍的read API。</p><p><img src=\"https://static001.geekbang.org/resource/image/52/43/525441865dede68fa5a9138cb930de43.jpg?wh=1920x654\" alt=\"图片\" title=\"read API一般用法\"></p><p>如上图所示，read API有3个关键点，一是由format指定的文件格式，二是由零到多个option组成的加载选项，最后一个是由load标记的源文件路径。</p><p>与之相对，write API也有3个关键环节，分别是同样由format定义的文件格式，零到多个由option构成的“写入选项”，以及由save指定的存储路径，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/73/cd/7348bdf46f3d051b891620a0c3f22dcd.jpg?wh=1920x680\" alt=\"图片\" title=\"write API一般用法\"></p><p>这里的format和save，与read API中的format和load是一一对应的，分别用于指定文件格式与存储路径。实际上，option选项也是类似的，除了mode以外，write API中的选项键与read API中的选项键也是相一致的，如seq用于指定CSV文件分隔符、dbtable用于指定数据表名、等等，你可以通过回顾<a href=\"https://time.geekbang.org/column/article/426101\">上一讲</a>来获取更多的option选项。</p><p>在read API中，mode选项键用于指定读取模式，如permissive, dropMalformed, failFast。但在write API中，mode用于指定“写入模式”，分别有Append、Overwrite、ErrorIfExists、Ignore这4种模式，它们的含义与描述如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/f9/48/f9367e4bb06702f396325183014ef448.jpg?wh=1594x789\" alt=\"图片\" title=\"不同写入模式的含义\"></p><p>有了write API，我们就可以灵活地把DataFrame持久化到不同的存储系统中，为数据的生命周期画上一个圆满的句号。</p><h2>重点回顾</h2><p>今天这一讲，我们主要围绕数据的生命周期，学习了Spark SQL在不同数据阶段支持的处理算子，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/97/50bd70a2dcf01b631eff86c286d9eb97.jpg?wh=1920x442\" alt=\"图片\" title=\"Spark SQL算子一览\"></p><p>图中涉及的算子很多，尽管大部分我们都举例讲过了，但要在短时间之内一下子掌握这么多内容，确实强人所难。不过，你不用担心，今天这一讲，最主要的目的，还是想让你对Spark SQL支持的算子有一个整体的把握。</p><p>至于每个算子具体是用来做什么的，在日后的开发工作中，你可以反复地翻看这一讲，结合实践慢慢地加深印象，这样学习更高效。我也强烈建议你空闲时把官网的<a href=\"https://spark.apache.org/docs/3.0.1/api/sql/index.html\">Built-in Functions列表</a>过一遍，对这些内置函数的功能做到心中有数，实现业务逻辑时才会手到擒来。</p><p>除了DataFrame本身支持的算子之外，在功能上，SQL完全可以实现同样的数据分析。给定DataFrame，你只需通过createTempView或是createGlobalTempView来创建临时表，然后就可以通过写SQL语句去进行数据的探索、倾斜、转换与分析。</p><p>最后，需要指出的是，DataFrame算子与SQL查询语句之间，并没有优劣之分，他们可以实现同样的数据应用，而且在执行性能方面也是一致的。因此，你可以结合你的开发习惯与偏好，自由地在两者之间进行取舍。</p><h2>每课一练</h2><p>在转换类算子中，我们举例介绍了explode这个算子，它的作用是按照以数组为元素的数据列，把一条数据展开（爆炸）成多条数据。结合这个算子的作用，你能否分析一下，explode操作是否会引入Shuffle计算呢？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这一讲分享给有需要的朋友。</p>","neighbors":{"left":{"article_title":"15 | 数据源与数据格式：DataFrame从何而来？","id":426101},"right":{"article_title":"17 | 数据关联：不同的关联形式与实现机制该怎么选？","id":427470}},"comments":[{"had_liked":false,"id":318745,"user_name":"kingcall","can_delete":false,"product_type":"c1","uid":1056982,"ip_address":"","ucode":"508884DC684B5B","user_header":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","comment_is_top":false,"comment_ctime":1635409504,"is_pvip":false,"replies":[{"id":"115575","content":"正解，满分💯","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635425513,"ip_address":"","comment_id":318745,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23110245984","product_id":100090001,"comment_content":"explode 不会引入shuffle,因为shuffle是在众多计算节点进行数据传输，explode虽然会导致数据条数变多但是都是在一台节点上的","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529368,"discussion_content":"正解，满分💯","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635425513,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2446756,"avatar":"https://static001.geekbang.org/account/avatar/00/25/55/a4/a06abd2d.jpg","nickname":"钟强","note":"","ucode":"C4227385241E70","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555540,"discussion_content":"请问这里的计算节点是指什么？ worker? executor? task ?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646968838,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":341374,"user_name":"Spoon","can_delete":false,"product_type":"c1","uid":1959822,"ip_address":"","ucode":"2FF9193AD482C2","user_header":"https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg","comment_is_top":false,"comment_ctime":1649570621,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5944537917","product_id":100090001,"comment_content":"Java代码实现<br>https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;sql&#47;DataFrameOperatorJob.java","like_count":2},{"had_liked":false,"id":347074,"user_name":"Geek_b2839b","can_delete":false,"product_type":"c1","uid":2876299,"ip_address":"","ucode":"6D8ABA989AB724","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIw0Nnvrrt9fV1wHVfBlPzrZmxNCRTbWPfNEbCEMtuoj6gw0LlMbbS3gtRLgLMfCoAV3TXsk5giavw/132","comment_is_top":false,"comment_ctime":1653696247,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1653696247","product_id":100090001,"comment_content":"老师请问一下，使用spark同步hive数据到Oracle的时候，由于executor失败重试，导致偶尔出现同步时hive数据和Oracle数据不一致的情况，这个该怎么解决呢","like_count":0},{"had_liked":false,"id":330775,"user_name":"小李","can_delete":false,"product_type":"c1","uid":1903790,"ip_address":"","ucode":"30BD251EE1B1E2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/iaQgtbE98VGIVIyribdo6dgLOnaNoe7ZdUuPr60ibsduibscrzQCTzdW2AfL9nxwe8YlSK75gOnK3YbAJKTaFPxibdg/132","comment_is_top":false,"comment_ctime":1642152517,"is_pvip":false,"replies":[{"id":"120673","content":"这两条语句的效果是一样的，在spark sql里面的优化过程，在效果方面，完全一样。像你说的，也会做map端聚合，这些优化机制，都是有的~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1642257248,"ip_address":"","comment_id":330775,"utype":1}],"discussion_count":1,"race_medal":1,"score":"1642152517","product_id":100090001,"comment_content":"请问一下：df.select().groupBy().count()与df.select().groupBy().agg(count(lit(1)))内部处理逻辑会不一样吗，还是会都会经过spark sql优化引擎优化成map阶段预聚合？比如会不会像rdd的aggregateByKey或者reduceByKey一样在shuffle write阶段做partition内的预聚合。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546257,"discussion_content":"这两条语句的效果是一样的，在spark sql里面的优化过程，在效果方面，完全一样。像你说的，也会做map端聚合，这些优化机制，都是有的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642257248,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":323319,"user_name":"Geek_935079","can_delete":false,"product_type":"c1","uid":2851472,"ip_address":"","ucode":"85BFB004D53AA5","user_header":"","comment_is_top":false,"comment_ctime":1637831977,"is_pvip":false,"replies":[{"id":"117417","content":"是的~ 这里typo了，感谢老弟提醒~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637987434,"ip_address":"","comment_id":323319,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1637831977","product_id":100090001,"comment_content":"val aggResult = fullInfo.groupBy这里是不是要改为val aggResult = jointDF.groupBy","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533830,"discussion_content":"是的~ 这里typo了，感谢老弟提醒~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637987434,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3078266,"avatar":"https://static001.geekbang.org/account/avatar/00/2e/f8/7a/b3b5e29e.jpg","nickname":"Li,Yanjie","note":"","ucode":"708A4982F272C2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":588699,"discussion_content":"嗯。我还在纳闷，这个fullInfo从哪里来的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1664010319,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317800,"user_name":"火炎焱燚","can_delete":false,"product_type":"c1","uid":2767491,"ip_address":"","ucode":"DB11784DD94059","user_header":"https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg","comment_is_top":false,"comment_ctime":1634973579,"is_pvip":false,"replies":[{"id":"115458","content":"👍👍👍","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635258628,"ip_address":"","comment_id":317800,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634973579","product_id":100090001,"comment_content":"<br># 5-分析类算子<br># 创建员工df<br>seq=[(1,&#39;Mike&#39;,28,&#39;Male&#39;),<br>     (2, &quot;Lily&quot;, 30, &quot;Female&quot;),<br>     (3, &quot;Raymond&quot;, 26, &quot;Male&quot;)]<br>employees=spark.createDataFrame(seq,[&#39;id&#39;,&#39;name&#39;,&#39;age&#39;,&#39;gender&#39;])<br>employees.show()<br><br># 创建薪水df<br>seq2=[(1, 26000), (2, 30000), (4, 25000), (3, 20000)]<br>salaries=spark.createDataFrame(seq2,[&#39;id&#39;,&#39;salary&#39;])<br>salaries.show()<br><br># 将员工df和薪水df进行合并，inner方式：<br>joinDF=salaries.join(employees,&#39;id&#39;,&#39;inner&#39;)<br>joinDF.show()<br><br># 按照性别统计出薪水之和，平均值<br>from pyspark.sql import functions as f<br>aggResult=joinDF.groupBy(&#39;gender&#39;).agg(f.sum(&#39;salary&#39;).alias(&#39;sum_salary&#39;),f.avg(&#39;salary&#39;).alias(&#39;avg_salary&#39;))<br>aggResult.show()<br><br># 排序，sort方法和orderBy方法一样<br>aggResult.sort(f.desc(&#39;sum_salary&#39;),f.asc(&#39;gender&#39;)).show()<br>aggResult.orderBy(f.desc(&#39;sum_salary&#39;),f.asc(&#39;gender&#39;)).show()","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528980,"discussion_content":"👍👍👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635258628,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317799,"user_name":"火炎焱燚","can_delete":false,"product_type":"c1","uid":2767491,"ip_address":"","ucode":"DB11784DD94059","user_header":"https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg","comment_is_top":false,"comment_ctime":1634973566,"is_pvip":false,"replies":[{"id":"115457","content":"辛苦老弟~ 太棒了！","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635258610,"ip_address":"","comment_id":317799,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634973566","product_id":100090001,"comment_content":"python 代码为：<br><br># 1-SQL语句：<br>from pyspark import SparkContext, SparkConf<br>from pyspark.sql.session import SparkSession<br>sc = SparkContext()<br>spark = SparkSession(sc)<br><br>seq=[(&#39;Alice&#39;,18), (&#39;Bob&#39;,14)]<br>column = [&#39;name&#39;,&#39;age&#39;]<br>df=spark.createDataFrame(seq,column)<br>df.createTempView(&#39;t1&#39;)<br>query=&quot;select * from t1&quot;<br>result=spark.sql(query)<br>result.show()<br><br># 2-探索类算子：<br>employees=[(1, &quot;John&quot;, 26, &quot;Male&quot;), (2, &quot;Lily&quot;, 28, &quot;Female&quot;), (3, &quot;Raymond&quot;, 30, &quot;Male&quot;)]<br>employeesDF=spark.createDataFrame(employees,[&#39;id&#39;,&#39;name&#39;,&#39;age&#39;,&#39;gender&#39;])<br>employeesDF.printSchema()<br>employeesDF.show() <br>age_df=employeesDF.describe(&#39;age&#39;)<br>age_df.show()<br><br># 3-清洗类算子：<br># 删除某一列数据<br>employeesDF.drop(&#39;gender&#39;).show()<br># distinct对所有数据去重，注意无法设置列名<br>employeesDF.distinct().show()<br># dropDuplicates可以对某几列去重，灵活性更高<br>employeesDF.dropDuplicates([&#39;gender&#39;]).show()<br><br># 4-转换类算子<br># 选择某几列组成新的df<br>employeesDF.select([&#39;name&#39;,&#39;gender&#39;]).show()<br>employeesDF.select(&#39;name&#39;).show()<br># selectExpr用选择表达式来组成新的df<br>employeesDF.selectExpr(&quot;id&quot;, &quot;name&quot;, &quot;concat(id, &#39;_&#39;, name) as id_name&quot;).show()<br># where选择满足条件的内容<br>employeesDF.where(&quot;gender=&#39;Male&#39;&quot;).show()<br># 对列名重命名：将gender重命名为sex<br>employeesDF.withColumnRenamed(&#39;gender&#39;,&#39;sex&#39;).show() <br># 在原列进行修改后组成新的一列，将age都+10岁<br>employeesDF.withColumn(&quot;crypto&quot;, employeesDF[&#39;age&#39;]+10).show()<br># drop删除某一列<br>employeesDF.withColumn(&quot;crypto&quot;, employeesDF[&#39;age&#39;]+10).drop(&#39;age&#39;).show()<br><br># explode拆分list<br>seq2 =[(1, &quot;John&quot;, 26, &quot;Male&quot;,[&quot;Sports&quot;, &quot;News&quot;]),<br>(2, &quot;Lily&quot;, 28, &quot;Female&quot;, [&quot;Shopping&quot;, &quot;Reading&quot;]),<br>(3, &quot;Raymond&quot;, 30, &quot;Male&quot;, [&quot;Sports&quot;, &quot;Reading&quot;])]<br>employeesDF2=spark.createDataFrame(seq2,[&#39;id&#39;,&#39;name&#39;,&#39;age&#39;,&#39;gender&#39;,&#39;interests&#39;])<br>from pyspark.sql.functions import explode<br>employeesDF2.withColumn(&#39;interest&#39;,explode(employeesDF2[&#39;interests&#39;])).show()","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528979,"discussion_content":"辛苦老弟~ 太棒了！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635258610,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316747,"user_name":"小玲铛🍯","can_delete":false,"product_type":"c1","uid":2768136,"ip_address":"","ucode":"C9608A91BA66A0","user_header":"https://static001.geekbang.org/account/avatar/00/2a/3d/08/fcf92621.jpg","comment_is_top":false,"comment_ctime":1634539174,"is_pvip":false,"replies":[{"id":"114725","content":"没错，createOrReplaceTempView更实用，避免重复建表带来的报错~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634571759,"ip_address":"","comment_id":316747,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634539174","product_id":100090001,"comment_content":"自己开发的时候createTempView会在内存中创建临时表,重新运行的话会报table is exist 错误,建议使用 createOrReplaceTempView","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528486,"discussion_content":"没错，createOrReplaceTempView更实用，避免重复建表带来的报错~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634571759,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316544,"user_name":"GAC·DU","can_delete":false,"product_type":"c1","uid":1385403,"ip_address":"","ucode":"7847FBE1C13740","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","comment_is_top":false,"comment_ctime":1634379265,"is_pvip":true,"replies":[{"id":"114724","content":"说到Shuffle，我们往往需要分类讨论。<br><br>重分区算子repartition、ByKey算子确实会引入Shuffle，这个是确定性的。<br><br>不确定的是join，一般来说，join都会引入Shuffle。不过有一种特殊的join，学名叫Collocated Join，这种join是不会引入Shuffle的。<br><br>名字听上去挺唬人，但本质上，就是参与join的两张表，提前按照join keys，做好了分区。因而在join的时候，自然就不会有Shuffle了。不过实际应用中，Collocated Join场景并不多，因此暂时可以忽略掉~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634571675,"ip_address":"","comment_id":316544,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634379265","product_id":100090001,"comment_content":"Spark中Shuffle算子的分类：重分区算子、ByKey算子、Join算子","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528408,"discussion_content":"说到Shuffle，我们往往需要分类讨论。\n\n重分区算子repartition、ByKey算子确实会引入Shuffle，这个是确定性的。\n\n不确定的是join，一般来说，join都会引入Shuffle。不过有一种特殊的join，学名叫Collocated Join，这种join是不会引入Shuffle的。\n\n名字听上去挺唬人，但本质上，就是参与join的两张表，提前按照join keys，做好了分区。因而在join的时候，自然就不会有Shuffle了。不过实际应用中，Collocated Join场景并不多，因此暂时可以忽略掉~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634571675,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}