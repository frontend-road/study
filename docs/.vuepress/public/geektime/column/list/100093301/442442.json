{"id":442442,"title":"14 | 构建网络：一站式实现模型搭建与训练","content":"<p>你好，我是方远。</p><p>前面我们花了不少时间，既学习了数据部分的知识，还研究了模型的优化方法、损失函数以及卷积计算。你可能感觉这些知识还有些零零散散，但其实我们不知不觉中，已经拿下了模型训练的必学内容。</p><p>今天这节课，也是一个中期小练习，是我们检验自己学习效果的好时机。我会带你使用PyTorch构建和训练一个自己的模型。</p><p>具体我是这么安排的，首先讲解搭建网络必备的基础模块——nn.Module模块，也就是如何自己构建一个网络，并且训练它，换句话说，就是搞清楚VGG、Inception那些网络是怎么训练出来的。然后我们再看看如何借助Torchvision的模型作为预训练模型，来训练我们自己的模型。</p><h2>构建自己的模型</h2><p>让我们直接切入主题，使用PyTorch，自己构建并训练一个线性回归模型，来拟合出训练集中的走势分布。</p><p>我们先随机生成训练集X与对应的标签Y，具体代码如下：</p><pre><code class=\"language-python\">import numpy as np\nimport random\nfrom matplotlib import pyplot as plt\n\nw = 2\nb = 3\nxlim = [-10, 10]\nx_train = np.random.randint(low=xlim[0], high=xlim[1], size=30)\n\ny_train = [w * x + b + random.randint(0,2) for x in x_train]\n\nplt.plot(x_train, y_train, 'bo')\n</code></pre><!-- [[[read_end]]] --><p>上述代码中生成的数据，整理成散点图以后，如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/f2/11/f2d24e9e7ea5737a78032b686282ca11.jpg?wh=900x621\" alt=\"图片\"></p><p>熟悉回归的同学应该知道，我们的回归模型为：$y = wx+b$。这里的x与y，其实就对应上述代码中的x_train与y_train，而w与b正是我们要学习的参数。</p><p>好，那么我们看看如何构建这个模型。我们还是先看代码，再具体讲解。</p><pre><code class=\"language-python\">import torch\nfrom torch import nn\n\nclass LinearModel(nn.Module):\n&nbsp; def __init__(self):\n&nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; self.weight = nn.Parameter(torch.randn(1))\n&nbsp; &nbsp; self.bias = nn.Parameter(torch.randn(1))\n\n&nbsp; def forward(self, input):\n&nbsp; &nbsp; return (input * self.weight) + self.bias\n</code></pre><p>通过上面这个线性回归模型的例子，我们可以引出构建网络时的重要几个知识点。</p><p>1.<strong>必须继承nn.Module类</strong>。</p><p>2.<strong>重写__init__()方法</strong>。通常来说要把有需要学习的参数的层放到构造函数中，例如，例子中的weight与bias，还有我们之前学习的卷积层。我们在上述的__init__()中使用了nn.Parameter()，它主要的作用就是作为nn.Module中可训练的参数使用。</p><p>3.<strong>forward()是必须重写的方法</strong>。看函数名也可以知道，它是用来定义这个模型是如何计算输出的，也就是前向传播。对应到我们的例子，就是获得最终输出y=weight * x+bias的计算结果。对于一些不需要学习参数的层，一般来说可以放在这里。例如，BN层，激活函数还有Dropout。</p><h3>nn.Module模块</h3><p>nn.Module是所有神经网络模块的基类。当我们自己要设计一个网络结构的时候，就要继承该类。也就说，其实Torchvison中的那些模型，也都是通过继承nn.Module模块来构建网络模型的。</p><p>需要注意的是，<strong>模块本身是callable的，当调用它的时候，就是执行forward函数，也就是前向传播</strong>。</p><p>我们还是结合代码例子直观感受一下。请看下面的代码，先创建一个LinearModel的实例model，然后model(x)就相当于调用LinearModel中的forward方法。</p><pre><code class=\"language-python\">model = LinearModel()\nx = torch.tensor(3)\ny = model(x)\n</code></pre><p>在我们之前的课程里已经讲过，模型是通过前向传播与反向传播来计算梯度，然后更新参数的。我想学到这里，应该没有几个人会愿意去写反向传播和梯度更新之类的代码吧。</p><p>这个时候PyTorch的优点就体现出来了，当你训练时，PyTorch的求导机制会帮你自动完成这些令人头大的计算。</p><p>除了刚才讲过的内容，关于初始化方法__init__，你还需要关注的是，<strong>必须调用父类的构造方法才可以</strong>，也就是这行代码：</p><pre><code class=\"language-python\">super().__init__()\n</code></pre><p>因为在nn.Module的__init__()中，会初始化一些有序的字典与集合。这些集合用来存储一些模型训练过程的中间变量，如果不初始化nn.Module中的这些参数的话，模型就会报下面的错误。</p><pre><code class=\"language-python\">AttributeError: cannot assign parameters before Module.__init__() call\n</code></pre><h3>模型的训练</h3><p>我们的模型定义好之后，还没有被训练。要想训练我们的模型，就需要用到损失函数与优化方法，这一部分前面课里（如果你感觉陌生的话，可以回顾11～13节课）已经学过了，所以现在我们直接看代码就可以了。</p><p>这里选择的是MSE损失与SGD优化方法。</p><pre><code class=\"language-python\">model = LinearModel()\n# 定义优化器\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\ny_train = torch.tensor(y_train, dtype=torch.float32)\nfor _ in range(1000):\n    input = torch.from_numpy(x_train)\n    output = model(input)\n    loss = nn.MSELoss()(output, y_train)\n    model.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre><p>经过1000个Epoch的训练以后，我们可以打印出模型的weight与bias，看看是多少。</p><p>对于一个模型的<strong>可训练</strong>的参数，我们可以通过named_parameters()来查看，请看下面代码。</p><pre><code class=\"language-python\">for parameter in model.named_parameters():\n&nbsp; print(parameter)\n# 输出：\n('weight', Parameter containing:\ntensor([2.0071], requires_grad=True))\n('bias', Parameter containing:\ntensor([3.1690], requires_grad=True))\n</code></pre><p>可以看到，weight是2.0071，bias是3.1690，你再回头对一下我们创建训练数据的w与b，它们是不是一样呢？</p><p>我们刚才说过，继承一个nn.Module之后，可以定义自己的网络模型。Module同样可以作为另外一个Module的一部分，被包含在网络中。比如，我们要设计下面这样的一个网络：</p><p><img src=\"https://static001.geekbang.org/resource/image/25/c6/2574b93463fd3dbd0e97661d6a06ffc6.jpg?wh=1372x689\" alt=\"\"></p><p>观察图片很容易就会发现，在这个网络中有大量重复的结构。上图中的3x3与2x2的卷积组合，按照我们开篇的讲解的话，我们需要把每一层卷积都定义到__init__()，然后再在forward中定义好执行方法就可以了，例如下面的伪代码：</p><pre><code class=\"language-python\">class CustomModel(nn.Module):\n&nbsp; def __init__(self):\n&nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')\n&nbsp; &nbsp; self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; self.conv_m_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')\n&nbsp; &nbsp; self.conv_m_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; self.conv_n_1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding='same')\n&nbsp; &nbsp; self.conv_n_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, padding='same')\n\n&nbsp; def forward(self, input):\n&nbsp; &nbsp; x = self.conv1_1(input)\n&nbsp; &nbsp; x = self.conv1_2(x)\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; x = self.conv_m_1(x)\n&nbsp; &nbsp; x = self.conv_m_2(x)\n&nbsp; &nbsp; ...&nbsp; &nbsp;&nbsp;\n&nbsp; &nbsp; x = self.conv_n_1(x)\n&nbsp; &nbsp; x = self.conv_n_2(x)\n&nbsp; &nbsp; ...\n&nbsp; &nbsp; return x\n</code></pre><p>其实这部分重复的结构完全可以放在一个单独的module中，然后，在我们模型中直接调用这部分即可，具体实现你可以参考下面的代码：</p><pre><code class=\"language-python\">class CustomLayer(nn.Module):\n  def __init__(self, input_channels, output_channels):\n    super().__init__()\n    self.conv1_1 = nn.Conv2d(in_channels=input_channels, out_channels=3, kernel_size=3, padding='same')\n    self.conv1_2 = nn.Conv2d(in_channels=3, out_channels=output_channels, kernel_size=2, padding='same')\n    \n  def forward(self, input):\n    x = self.conv1_1(input)\n    x = self.conv1_2(x)\n    return x\n    \n</code></pre><p>然后呢，CustomModel就变成下面这样了：</p><pre><code class=\"language-python\">class CustomModel(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.layer1 = CustomLayer(1，1)\n    ...\n    self.layerm = CustomLayer(1，1)\n    ...\n    self.layern = CustomLayer(1，1)\n  \n  def forward(self, input):\n    x = self.layer1(input)\n    ...\n    x = self.layerm(x)\n    ...    \n    x = self.layern(x)\n    ...\n    return x\n</code></pre><p>熟悉深度学习的同学，一定听过残差块、Inception块这样的多层的一个组合。你没听过也没关系，在图像分类中我还会讲到。这里你只需要知道，这种多层组合的结构是类似的，对于这种组合，我们就可以用上面的代码的方式实现。</p><h3>模型保存与加载</h3><p>我们训练好的模型最终的目的，就是要为其他应用提供服务的，这就涉及到了模型的保存与加载。</p><p>模型保存与加载的话有两种方式。PyTorch模型的后缀名一般是pt或pth，这都没有关系，只是一个后缀名而已。我们接着上面的回归模型继续讲模型的保存与加载。</p><h4>方式一：只保存训练好的参数</h4><p>第一种方式就是只保存训练好的参数。然后加载模型的时候，你需要通过代码加载网络结构，然后再将参数赋予网络。</p><p>只保存参数的代码如下所示：</p><pre><code class=\"language-python\">torch.save(model.state_dict(), './linear_model.pth')\n</code></pre><p>第一个参数是模型的state_dict，而第二个参数要保存的位置。</p><p>代码中的state_dict是一个字典，在模型被定义之后会自动生成，存储的是模型<strong>可训练</strong>的参数。我们可以打印出线性回归模型的state_dict，如下所示：</p><pre><code class=\"language-python\">model.state_dict()\n输出：OrderedDict([('weight', tensor([[2.0071]])), ('bias', tensor([3.1690]))])\n</code></pre><p>加载模型的方式如下所示：</p><pre><code class=\"language-python\"># 先定义网络结构\nlinear_model = LinearModel()\n# 加载保存的参数\nlinear_model.load_state_dict(torch.load('./linear_model.pth'))\nlinear_model.eval()\nfor parameter in linear_model.named_parameters():\n&nbsp; print(parameter)\n输出：\n('weight', Parameter containing:\ntensor([[2.0071]], requires_grad=True))\n('bias', Parameter containing:\ntensor([3.1690], requires_grad=True))\n</code></pre><p>这里有个model.eval()需要你注意一下，因为有些层（例如，Dropout与BN）在训练时与评估时的状态是不一样的，当进入评估时要执行model.eval()，模型才能进入评估状态。这里说的评估不光光指代评估模型，也包括模型上线时候时的状态。</p><h4>方式二：保存网络结构与参数</h4><p>相比第一种方式，这种方式在加载模型的时候，不需要加载网络结构了。具体代码如下所示：</p><pre><code class=\"language-python\"># 保存整个模型\ntorch.save(model, './linear_model_with_arc.pth')\n# 加载模型，不需要创建网络了\nlinear_model_2 = torch.load('./linear_model_with_arc.pth')\nlinear_model_2.eval()\nfor parameter in linear_model_2.named_parameters():\n&nbsp; print(parameter)\n# 输出：\n('weight', Parameter containing:\ntensor([[2.0071]], requires_grad=True))\n('bias', Parameter containing:\ntensor([3.1690], requires_grad=True))\n</code></pre><p>这样操作以后，如果你成功输出了相应数值，而且跟之前保存的模型的参数一致，就说明加载对了。</p><h2>使用Torchvison中的模型进行训练</h2><p>我们前面说过，Torchvision提供了一些封装好的网络结构，我们可以直接拿过来使用。但是并没有细说如何使用它们在我们的数据集上进行训练。今天，我们就来看看如何使用这些网络结构，在我们自己的数据上训练我们自己的模型。</p><h3>再说微调</h3><p>其实，Torchvision提供的模型最大的作用就是当作我们训练时的预训练模型，用来加速我们模型收敛的速度，这就是所谓的微调。</p><p>对于微调，最关键的一步就是之前讲的<strong>调整最后全连接层输出的数目</strong>。Torchvision中只是对各大网络结构的复现，而不是对它们进行了统一的封装，所以在修改全连接层时，不同的网络有不同的修改方法。</p><p>不过你也别担心，这个修改并不复杂，你只需要打印出网络结构，就可以知道如何修改了。我们接下来以AlexNet为例带你尝试一下如何微调。</p><p>前面讲Torchvision的时候其实提到过一次微调，那个时候说的是固定整个网络的参数，只训练最后的全连接层。今天我再给你介绍另外一种微调的方式，那就是修改全连接层之后，整个网络都重新开始训练。只不过，这时候要使用预训练模型的参数作为初始化的参数，这种方式更为常用。</p><p>接下来，我们就看看如何使用Torchvision中模型进行微调。</p><p>首先，导入模型。代码如下：</p><pre><code class=\"language-python\">import torchvision.models as models\nalexnet = models.alexnet(pretrained=True)\n</code></pre><p>这一步如果你不能“科学上网”的话，可能会比较慢。你可以先根据命令中提示的url手动下载，然后使用今天讲的模型加载的方式加载预训练模型，代码如下所示：</p><pre><code class=\"language-python\">import torchvision.models as models\nalexnet = models.alexnet()\nalexnet.load_state_dict(torch.load('./model/alexnet-owt-4df8aa71.pth'))\n</code></pre><p>为了验证加载是否成功，我们让它对下图进行预测：<br>\n<img src=\"https://static001.geekbang.org/resource/image/66/20/666181b44c23e9075debe0daf6126b20.jpg?wh=1920x1867\" alt=\"图片\"></p><p>代码如下：</p><pre><code class=\"language-python\">from PIL import Image\nimport torchvision\nimport torchvision.transforms as transforms\n\nim = Image.open('dog.jpg')\n\ntransform = transforms.Compose([\n&nbsp; &nbsp; transforms.RandomResizedCrop((224,224)),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n&nbsp; &nbsp; ])\n\ninput_tensor = transform(im).unsqueeze(0)\nalexnet(input_tensor).argmax()\n输出：263\n</code></pre><p>运行了前面的代码之后，对应到ImageNet的类别标签中可以找到，263对应的是Pembroke（柯基狗），这就证明模型已经加载成功了。<br>\n这个过程中有两个重点你要留意。</p><p>首先，因为Torchvision中所有图像分类的预训练模型，它们都是在ImageNet上训练的。所以，输入数据需要是3通道的数据，也就是shape为(B, 3, H, W)的Tensor，B为batchsize。我们需要使用均值为[0.485, 0.456, 0.406]，标准差为[0.229, 0.224, 0.225]对数据进行正规化。</p><p>另外，从理论上说，大部分的经典卷积神经最后采用全连接层（也就是机器学习中的感知机）进行分类，这也导致了网络的输入尺寸是固定的。但是，在Torchvision的模型可以接受任意尺寸的输入的。</p><p>这是因为Torchvision对模型做了优化，有的网络是在最后的卷积层采用了全局平均，或者采用的是全卷积网络。这两种方式都可以让网络接受在最小输入尺寸基础之上，任意尺度的输入。这一点，你现在可能认识得还不够清楚，不过别担心，以后我们学习完图像分类理论之后，你会理解得更加透彻。</p><p>我们回到微调这个主题。正如刚才所说，训练一个AlexNet需要的数据必须是三通道数据。所以，在这里我使用了CIFAR-10公开数据集举例。</p><p>CIFAR-10数据集一共有60000张图片构成，共10个类别，每一类包含6000图片。每张图片为32x32的RGB图片。其中50000张图片作为训练集，10000张图片作为测试集。</p><p>可以说CIFAR-10是非常接近真实项目数据的数据集了，因为真实项目中的数据通常是RGB三通道数据，而CIFAR-10同样是三通道数据。</p><p>我们用之前讲的make_grid方法，将CIFAR-10的数据打印出来，代码如下：</p><pre><code class=\"language-python\">cifar10_dataset = torchvision.datasets.CIFAR10(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transforms.ToTensor(),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\n# 取32张图片的tensor\ntensor_dataloader = DataLoader(dataset=cifar10_dataset,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=32)\ndata_iter = iter(tensor_dataloader)\nimg_tensor, label_tensor = data_iter.next()\nprint(img_tensor.shape)\ngrid_tensor = torchvision.utils.make_grid(img_tensor, nrow=16, padding=2)\ngrid_img = transforms.ToPILImage()(grid_tensor)\ndisplay(grid_img)\n</code></pre><p>请注意，上述代码中的transform，我为了打印图片只使用了transform.ToTensor()输出图片，结果如下所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/f0/f2/f0b5f4ed8c24fbb81c7b7c71a907a6f2.jpg?wh=546x546\" alt=\"\"></p><p>这里我特别说明一下，因为这个训练集的数据都是32x32的，所以你现在看到的就是原图效果，图片大小并不影响咱们的学习。</p><p>下面我们要做的是修改全连接层，直接print就可以打印出网络结构，代码如下：</p><pre><code class=\"language-python\">print(alexnet)\n输出：\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n</code></pre><p>可以看到，最后全连接层输入是4096个单元，输出是1000个单元，我们要把它修改为输出是10个单元的全连接层（CIFR10有10类）。代码如下：</p><pre><code class=\"language-python\"># 提取分类层的输入参数\nfc_in_features = alexnet.classifier[6].in_features\n\n# 修改预训练模型的输出分类数\nalexnet.classifier[6] = torch.nn.Linear(fc_in_features, 10)\nprint(alexnet)\n输出：\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n</code></pre><p>这时，你可以发现输出就变为10个单元了。</p><p>接下来就是在CIFAR-10上，使用AlexNet作为预训练模型训练我们自己的模型了。首先是数据读入，代码如下：</p><pre><code class=\"language-python\">transform = transforms.Compose([\n&nbsp; &nbsp; transforms.RandomResizedCrop((224,224)),\n&nbsp; &nbsp; transforms.ToTensor(),\n&nbsp; &nbsp; transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n&nbsp; &nbsp; ])\ncifar10_dataset = torchvision.datasets.CIFAR10(root='./data',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;train=False,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;transform=transform,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;target_transform=None,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;download=True)\ndataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_size=32,&nbsp; &nbsp; &nbsp; &nbsp;# 输出的batch大小\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;shuffle=True,&nbsp; &nbsp; &nbsp; &nbsp;# 数据是否打乱\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;num_workers=2)&nbsp; &nbsp; &nbsp; # 进程数, 0表示只有主进程\n</code></pre><p>这里需要注意的是，我更改了transform，并且将图片resize到224x224大小。这个尺寸是Torchvision中推荐的一个最小训练尺寸。模型就是我们修改后的AlexNet，之后的训练跟我们之前讲的是一样的。<br>\n先定义优化器，代码如下：</p><pre><code class=\"language-python\">optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n</code></pre><p>然后开始模型训练，是不是感觉后面的代码很眼熟，没错，它跟我们之前讲的一样：</p><pre><code class=\"language-python\"># 训练3个Epoch\nfor epoch in range(3):\n&nbsp; &nbsp; for item in dataloader:&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; output = alexnet(item[0])\n&nbsp; &nbsp; &nbsp; &nbsp; target = item[1]\n&nbsp; &nbsp; &nbsp; &nbsp; # 使用交叉熵损失函数\n&nbsp; &nbsp; &nbsp; &nbsp; loss = nn.CrossEntropyLoss()(output, target)\n&nbsp; &nbsp; &nbsp; &nbsp; print('Epoch {}, Loss {}'.format(epoch + 1 , loss))\n&nbsp; &nbsp; &nbsp; &nbsp; #以下代码的含义，我们在之前的文章中已经介绍过了\n&nbsp; &nbsp; &nbsp; &nbsp; alexnet.zero_grad()\n&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()\n&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()\n</code></pre><p>这里用到的微调方式，就是所有参数都需要进行重新训练。</p><p>而第一种方式（固定整个网络的参数，只训练最后的全连接层），只需要在读取完预训练模型之后，将全连接层之前的参数全部锁死即可，也就是让他们无法训练，我们模型训练时，只训练全连接层就行了，其余一切都不变。代码如下所示：</p><pre><code class=\"language-python\">alexnet = models.alexnet()\nalexnet.load_state_dict(torch.load('./model/alexnet-owt-4df8aa71.pth'))\nfor param in alexnet.parameters():\n    param.requires_grad = False\n</code></pre><p>说到这里，我们的模型微调就讲完了，你可以自己动手试试看。</p><h2>总结</h2><p>今天的内容，主要是围绕如何自己搭建一个网络模型，我们介绍了nn.Module模块以及围绕它的一些方法。</p><p>根据这讲我分享给你的思路，之后如果你有什么想法时，就可以快速搭建一个模型进行训练和验证。</p><p>其实，实际的开发中，我们很少会自己去构建一个网络，绝大多数都是直接使用前人已经构建好的一些经典网络，例如，Torchvision中那些模型。当你去看一些还没有被封装到PyTorch的模型的时候，今天所学的内容就能够帮你直接借鉴前人的工作结果，训练属于自己的模型。</p><p>最后，我再结合自己的学习研究经验，给有兴趣了解更多深度学习知识的同学提供一些学习线索。目前我们只讲了卷积层，对于一个网络还有很多其余层，比如Dropout、Pooling层、BN层、激活函数等。Dropout函数、Pooling层、激活函数相对比较好理解，BN层可能稍微复杂一些。</p><p>另外，细心的小伙伴应该发现了，我们在打印AlexNet网络结构中的时候，它的一部分是使用nn.Sequential构建的。<strong>nn.Sequential是一种快速构建网络的方式</strong>，有了这节课的知识作储备，弄懂这个方式你会觉得非常简单，也推荐你去看看。</p><h2>每课一练</h2><p>请你自己构建一个卷积神经网络，基于CIFAR-10，训练一个图像分类模型。因为还没有学习图像分类原理，所以我先帮你写好了网络的结构，需要你补全数据读取、损失函数(交叉熵损失)与优化方法（SGD）等部分。</p><pre><code class=\"language-python\">class MyCNN(nn.Module):\n&nbsp; &nbsp; def __init__(self):\n&nbsp; &nbsp; &nbsp; &nbsp; super().__init__()\n&nbsp; &nbsp; &nbsp; &nbsp; self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n&nbsp; &nbsp; &nbsp; &nbsp; # conv1输出的特征图为222x222大小\n&nbsp; &nbsp; &nbsp; &nbsp; self.fc = nn.Linear(16 * 222 * 222, 10)\n\n&nbsp; &nbsp; def forward(self, input):\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.conv1(input)\n&nbsp; &nbsp; &nbsp; &nbsp; # 进去全连接层之前，先将特征图铺平\n&nbsp; &nbsp; &nbsp; &nbsp; x = x.view(x.shape[0], -1)\n&nbsp; &nbsp; &nbsp; &nbsp; x = self.fc(x)\n&nbsp; &nbsp; &nbsp; &nbsp; return x\n</code></pre><p>欢迎你在留言区和我交流讨论。如果这节课对你有帮助，也推荐你顺手分享给更多的同事、朋友，跟他一起学习进步。</p>","neighbors":{"left":{"article_title":"加餐 | 机器学习其实就那么几件事","id":440393},"right":{"article_title":"15 | 可视化工具：如何实现训练的可视化监控？","id":444252}},"comments":[{"had_liked":false,"id":321794,"user_name":"vcjmhg","can_delete":false,"product_type":"c1","uid":1526461,"ip_address":"","ucode":"B508D1E9B3F974","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132","comment_is_top":false,"comment_ctime":1637048728,"is_pvip":false,"replies":[{"id":"117273","content":"👍🏻👍🏻 ^^","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1637734497,"ip_address":"","comment_id":321794,"utype":1}],"discussion_count":2,"race_medal":0,"score":"48881688984","product_id":100093301,"comment_content":"class MyCNN(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)<br>        # conv1输出的特征图为222x222大小<br>        self.fc = nn.Linear(16 * 222 * 222, 10)<br><br>    def forward(self, input):<br>        x = self.conv1(input)<br>        # 进去全连接层之前，先将特征图铺平<br>        x = x.view(x.shape[0], -1)<br>        x = self.fc(x)<br>        return x<br># 尽量使用gpu进行训练,如果没有cpu则使用gpu来训练<br>device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<br>cnn = MyCNN().to(device)<br>transform = transforms.Compose([<br>    # 修改裁剪图片的尺寸<br>    transforms.RandomResizedCrop((224, 224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>])<br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,<br>                                               train=False,<br>                                               transform=transform,<br>                                               target_transform=None,<br>                                               download=True)<br>dataloader = DataLoader(dataset=cifar10_dataset,  # 传入的数据集, 必须参数<br>                        batch_size=32,  # 输出的batch大小<br>                        shuffle=True,  # 数据是否打乱<br>                        num_workers=2)  # 进程数, 0表示只有主进程<br># 定义优化器<br>optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br>steps = 0<br>for epoch in range(16):<br>    for item in dataloader:<br>        steps += 1<br>        output = cnn(item[0].to(device))<br>        target = item[1].to(device)<br>        # 使用交叉熵损失函数<br>        loss = nn.CrossEntropyLoss()(output, target)<br>        # 每100步打印一次loss<br>        if steps % 100 == 0:<br>            print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1, loss))<br>        cnn.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br># 测试分类结果<br>im = Image.open(&#39;data&#47;img.png&#39;)<br>input_tensor = transform(im).unsqueeze(0)<br>result = cnn(input_tensor.to(device)).argmax()<br>print(result)<br># tensor(3, device=&#39;cuda:0&#39;)","like_count":11,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532931,"discussion_content":"👍🏻👍🏻 ^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637734497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532930,"discussion_content":"^^👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637734472,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":338210,"user_name":"Mr_李冲","can_delete":false,"product_type":"c1","uid":1631606,"ip_address":"","ucode":"61FAF80BA5FB0B","user_header":"https://static001.geekbang.org/account/avatar/00/18/e5/76/5d0b66aa.jpg","comment_is_top":false,"comment_ctime":1647354303,"is_pvip":true,"replies":[{"id":"123662","content":"你好，感谢你的留言。<br>如果保证输入一样，正常不会这样的。<br>你检查检查你的预处理的resize是不是会随机切割。<br>还是不行的话，可以把你的代码发给我看看。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1647408794,"ip_address":"","comment_id":338210,"utype":1}],"discussion_count":7,"race_medal":0,"score":"10237288895","product_id":100093301,"comment_content":"我在使用预训练好的alexnet-owt-7be5be79.pth模型，反复执行下面这段代码的时候<br>alexnet(input_tensor).argmax()<br>得到的结果并不总是263，而有时候会得到151和264或者其他的数值，请问我最终应该相信哪一个预测结果呢，是进行多次预测取预测次数最多的那个吗？还是有别的科学的方法呢？","like_count":2,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556526,"discussion_content":"你好，感谢你的留言。\n如果保证输入一样，正常不会这样的。\n你检查检查你的预处理的resize是不是会随机切割。\n还是不行的话，可以把你的代码发给我看看。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647408794,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1237846,"avatar":"https://static001.geekbang.org/account/avatar/00/12/e3/56/5fc6c92c.jpg","nickname":"小伟","note":"","ucode":"43235F32B7F9D8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":575619,"discussion_content":"RandomResizedCrop剪裁是按照像素1:1的去剪裁，下载的原图是1920*1867按照224*224大小剪裁会剩余很小一部分，识别会影响比较大，建议直接将原图使用Resize到224*224大小，识别率会高很多，但是还是偶尔会出现返回结果是264或者151的情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654958773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1237846,"avatar":"https://static001.geekbang.org/account/avatar/00/12/e3/56/5fc6c92c.jpg","nickname":"小伟","note":"","ucode":"43235F32B7F9D8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":575617,"discussion_content":"试了下RandomResizedCrop，大概80%情况是263，换成CenterCrop,手动修改原图大小为224*224，效果也不理想，是不是这个标准模型识别本身正确的概率就不高呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654956608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556900,"discussion_content":"因为使用的是transforms.RandomResizedCrop，会随机切割再resize。\nhttps://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n换一个不随机的resize方法就好了\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647568690,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1631606,"avatar":"https://static001.geekbang.org/account/avatar/00/18/e5/76/5d0b66aa.jpg","nickname":"Mr_李冲","note":"","ucode":"61FAF80BA5FB0B","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556805,"discussion_content":"我的代码是这样的，每次执行有时候会不太一样","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647520630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1631606,"avatar":"https://static001.geekbang.org/account/avatar/00/18/e5/76/5d0b66aa.jpg","nickname":"Mr_李冲","note":"","ucode":"61FAF80BA5FB0B","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556804,"discussion_content":"import torch\nimport torchvision.models as models\nalexnet = models.alexnet(pretrained=True)\n\nfrom PIL import Image\nimport torchvision\nimport torchvision.transforms as transforms\nfrom urllib.request import urlopen\nfrom PIL import Image\nurl = &#39;https://static001.geekbang.org/resource/image/66/20/666181b44c23e9075debe0daf6126b20.jpg?wh=1920x1867&#39;\nim = Image.open(urlopen(url))\n\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\ninput_tensor = transform(im).unsqueeze(0)\nalexnet(input_tensor).argmax()","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647520608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1241994,"avatar":"https://static001.geekbang.org/account/avatar/00/12/f3/8a/776a5bef.jpg","nickname":"有點壞","note":"","ucode":"274E9FAFE5DF73","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556612,"discussion_content":"你好，这个问题怎么解决的呢？ 我也遇到了同样的问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647425642,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321763,"user_name":"clee","can_delete":false,"product_type":"c1","uid":1467446,"ip_address":"","ucode":"122875102B23A8","user_header":"","comment_is_top":false,"comment_ctime":1637033684,"is_pvip":false,"replies":[{"id":"116838","content":"hi。<br>CIFAR10的图片比较小，猫和狗、鹿和马比较像，一部分图片预测错误是可以接受的。可以在下面尝试1之后，用验证集评估一下模型精确与召回，看看是否为一个可接受的结果，如果是的话，那部分图片分错是没问题的。<br><br>可以做如下尝试：<br>1. 把train=Fasle改为True。我为了讲解方便直接使用验证集训练的。<br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,<br>                                               train=False,<br>                                               transform=transform,<br>                                           target_transform=None,<br>                                               download=True)<br>2. 把学习率调小一点试试。^^","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1637050341,"ip_address":"","comment_id":321763,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10226968276","product_id":100093301,"comment_content":"调整之后，可以正常训练了，但是测试数据的时候我发现有些图片分类会有问题，比如狗和猫，鹿和马就容易分类错误，这是因为欠拟合吗？应该如何优化？","like_count":2,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530346,"discussion_content":"hi。\nCIFAR10的图片比较小，猫和狗、鹿和马比较像，一部分图片预测错误是可以接受的。可以在下面尝试1之后，用验证集评估一下模型精确与召回，看看是否为一个可接受的结果，如果是的话，那部分图片分错是没问题的。\n\n可以做如下尝试：\n1. 把train=Fasle改为True。我为了讲解方便直接使用验证集训练的。\ncifar10_dataset = torchvision.datasets.CIFAR10(root=&amp;#39;./data&amp;#39;,\n                                               train=False,\n                                               transform=transform,\n                                           target_transform=None,\n                                               download=True)\n2. 把学习率调小一点试试。^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637050341,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":346346,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1653037310,"is_pvip":false,"replies":[{"id":"126418","content":"这个只能通过项目总结了。<br>每一种损失函数会解决不同的问题，可以看看图像分割那节。<br>^^","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1653287964,"ip_address":"","comment_id":346346,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5948004606","product_id":100093301,"comment_content":"照抄抄出来了，但是对损失函数选取，优化器参数设置，还是一脸懵","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":573238,"discussion_content":"这个只能通过项目总结了。\n每一种损失函数会解决不同的问题，可以看看图像分割那节。\n^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653287964,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":345534,"user_name":"zhangting","can_delete":false,"product_type":"c1","uid":2950861,"ip_address":"","ucode":"565BB6CEE694E0","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ajNVdqHZLLDm5eHbw1fuicJiaXercgBI48O0Idt2mHUElmZyBM4o119NkndU1SNpsv8rZzKFibj8z1FibFAdNEO3zw/132","comment_is_top":false,"comment_ctime":1652363685,"is_pvip":false,"replies":[{"id":"126122","content":"你好，感谢你的留言。<br>输出5不是正确的吗？<br>cifar10里dog就是5啊","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1652411232,"ip_address":"","comment_id":345534,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5947330981","product_id":100093301,"comment_content":"老师请教个问题，为什么改全连接后，训练出来的模型，总是输出的是5。而未做改动前，预测出来的是263.源码如下：<br><br>alexnet = models.alexnet()<br>alexnet.load_state_dict(torch.load(&#39;.&#47;model&#47;alexnet-owt-7be5be79.pth&#39;)) <br><br>fc_in_features = alexnet.classifier[6].in_features<br><br>alexnet.classifier[6] = torch.nn.Linear(fc_in_features, 10)<br>print(alexnet)<br><br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>    ])<br><br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,                                       <br>                                               train=False,                                       <br>                                               transform=transform,                                       <br>                                               target_transform=None,                                       <br>                                               download=False)<br>dataloader = DataLoader(dataset=cifar10_dataset, batch_size=32, shuffle=True, num_workers=4)<br><br>optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>for epoch in range(3):<br>    for item in dataloader: <br>        output = alexnet(item[0])<br>        target = item[1]<br>        loss = nn.CrossEntropyLoss()(output, target)<br>        print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss))<br>        alexnet.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>       <br>im = Image.open(&#39;dog.jpg&#39;)<br>transform = transforms.Compose([<br>    transforms.Resize((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>    ])<br>input_tensor = transform(im).unsqueeze(0)<br><br>alexnet.eval()<br><br>print(alexnet(input_tensor).argmax())","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":571798,"discussion_content":"你好，感谢你的留言。\n输出5不是正确的吗？\ncifar10里dog就是5啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652411232,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":2950861,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/ajNVdqHZLLDm5eHbw1fuicJiaXercgBI48O0Idt2mHUElmZyBM4o119NkndU1SNpsv8rZzKFibj8z1FibFAdNEO3zw/132","nickname":"zhangting","note":"","ucode":"565BB6CEE694E0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":571801,"discussion_content":"老师，那为啥未做微调前是263？初学者不是很明白，麻烦老师了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652412136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":571798,"ip_address":""},"score":571801,"extra":""},{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2950861,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/ajNVdqHZLLDm5eHbw1fuicJiaXercgBI48O0Idt2mHUElmZyBM4o119NkndU1SNpsv8rZzKFibj8z1FibFAdNEO3zw/132","nickname":"zhangting","note":"","ucode":"565BB6CEE694E0","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":572228,"discussion_content":"之前的模型是在ImageNet上训练的，ImageNet狗的类别是263。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652665935,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":571801,"ip_address":""},"score":572228,"extra":""}]}]},{"had_liked":false,"id":344443,"user_name":"刘利","can_delete":false,"product_type":"c1","uid":1503779,"ip_address":"","ucode":"A46571E6B1C58E","user_header":"https://static001.geekbang.org/account/avatar/00/16/f2/23/bb13b3ed.jpg","comment_is_top":false,"comment_ctime":1651560077,"is_pvip":false,"replies":[{"id":"125779","content":"你好，刘利，感谢留言。<br>就只训练全连接层了。<br>finetune的时候会根据你的分类任务，新加一个全连接层(而不是使用原模型的)","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1651716473,"ip_address":"","comment_id":344443,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5946527373","product_id":100093301,"comment_content":"hi，老师，微调的时候，如果这样写，参数都不更新了，那还有哪部分参数会被训练呢？是需要再接一层全连接层么？<br>alexnet = models.alexnet()<br>alexnet.load_state_dict(torch.load(&#39;.&#47;model&#47;alexnet-owt-4df8aa71.pth&#39;))<br>for param in alexnet.parameters(): <br>    param.requires_grad = False","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":570248,"discussion_content":"你好，刘利，感谢留言。\n就只训练全连接层了。\nfinetune的时候会根据你的分类任务，新加一个全连接层(而不是使用原模型的)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1651716473,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1212044,"avatar":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","nickname":"hallo128","note":"","ucode":"3921D6E11CFCB1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":580704,"discussion_content":"只训练全连接层这里我也是有疑问，那是说不用增加修改预训练模型的输出分类数了吗（alexnet.classifier[6] = torch.nn.Linear(fc_in_features, 10)）？还是说再固定参数后，自己再手动加一层吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1658324577,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":570248,"ip_address":""},"score":580704,"extra":""}]}]},{"had_liked":false,"id":321262,"user_name":"马克图布","can_delete":false,"product_type":"c1","uid":1019274,"ip_address":"","ucode":"9E3879D6A55244","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg","comment_is_top":false,"comment_ctime":1636728821,"is_pvip":false,"replies":[{"id":"116681","content":"你好，马克图布，感谢你的留言。<br>对，内置了Softmax。<br><br>均值[0.485, 0.456, 0.406]，标准差[0.229, 0.224, 0.225]<br>是ImageNet的均值与标准差。torchvision中的模型都是在ImageNet上训练的。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636806587,"ip_address":"","comment_id":321262,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5931696117","product_id":100093301,"comment_content":"思考：<br><br>使用 `nn.CrossEntropyLoss` 作为 loss function 时，会自动在网络最后添加 `nn.LogSoftmax` 和 `nn.nLLLos`，因此不用再在 fc 层后面手动添加 Softmax 层；<br><br>问题：<br><br>transform 中，标准化的 mean 和 std 是如何确定的（我们需要使用均值为[0.485, 0.456, 0.406]，标准差为[0.229, 0.224, 0.225]对数据进行正规化）？","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530253,"discussion_content":"你好，马克图布，感谢你的留言。\n对，内置了Softmax。\n\n均值[0.485, 0.456, 0.406]，标准差[0.229, 0.224, 0.225]\n是ImageNet的均值与标准差。torchvision中的模型都是在ImageNet上训练的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636806587,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":358837,"user_name":"杨杰","can_delete":false,"product_type":"c1","uid":1131823,"ip_address":"北京","ucode":"74817EA9499843","user_header":"https://static001.geekbang.org/account/avatar/00/11/45/2f/b0b0dd74.jpg","comment_is_top":false,"comment_ctime":1664876097,"is_pvip":false,"replies":[{"id":"130615","content":"你好，因为使用了随机剪裁。<br>transforms.RandomResizedCrop((224,224))。<br>并且没有加alexnet.eval()","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1665306307,"ip_address":"北京","comment_id":358837,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1664876097","product_id":100093301,"comment_content":"import torchvision.models as models<br>from PIL import Image<br>import torchvision<br>import torchvision.transforms as transforms<br><br>alexnet = models.alexnet(pretrained=True)<br><br>im = Image.open(&#39;.&#47;data&#47;dog.jpg&#39;)<br><br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>    ])<br><br>input_tensor = transform(im).unsqueeze(0)<br>print(alexnet(input_tensor).argmax())<br><br>以上代码输出的不一定是264，这个是啥情况？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":589769,"discussion_content":"你好，因为使用了随机剪裁。\ntransforms.RandomResizedCrop((224,224))。\n并且没有加alexnet.eval()","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1665306307,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":354011,"user_name":"Geek_827444","can_delete":false,"product_type":"c1","uid":3091330,"ip_address":"北京","ucode":"B205EFB4F6DC01","user_header":"","comment_is_top":false,"comment_ctime":1660014706,"is_pvip":false,"replies":[{"id":"129292","content":"抱歉，回复迟了。<br>我执行了你的代码，并没有报错。你提示什么错误呢？","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1661320358,"ip_address":"北京","comment_id":354011,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1660014706","product_id":100093301,"comment_content":"老师您好：我一步一步按照咱们那个步骤来的，为什么代码运行不了那？谢谢您！<br><br>import torch<br>import torch.nn as nn<br>import torch.optim as optim<br>import torchvision.models as models<br><br>import torchvision.models as models<br>alexnet = models.alexnet(pretrained=True)<br><br><br>import torchvision.transforms as transforms<br><br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])<br>])<br><br>import torchvision<br><br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,#注：这里是存在\t\t\t\t\t\t\t\t\t\t\t\t\tpycharm文件当中的data文件夹里<br>                                              train=False,<br>                                              transform=transform,<br>                                              target_transform=None,<br>                                              download=True)<br><br>from torch.utils.data import DataLoader<br><br>dataloader = DataLoader(dataset=cifar10_dataset,<br>                       batch_size=32,<br>                       shuffle=True,<br>                       num_workers=2)<br><br><br><br>fc_in_features = alexnet.classifier[6].in_features<br>alexnet.classifier[6] = torch.nn.Linear(fc_in_features,10)<br><br>optimizer = torch.optim.SGD(alexnet.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>for epoch in range(3):<br>    # ↓定义比对的元素<br>    for item in dataloader:<br>        output = alexnet(item[0])<br>        target = item[1]<br><br>        # ↓使用损失函数<br>        loss = nn.CrossEntropyLoss()(output, target)<br>        print(&#39;Epoch{},Loss{}&#39;.format(epoch + 1, loss))<br><br>        # ↓更新损失函数和优化函数<br>        alexnet.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br><br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585070,"discussion_content":"抱歉，回复迟了。\n我执行了你的代码，并没有报错。你提示什么错误呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661320358,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":353830,"user_name":"John(易筋)","can_delete":false,"product_type":"c1","uid":1180202,"ip_address":"北京","ucode":"BB4E58DD4B8F15","user_header":"https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg","comment_is_top":false,"comment_ctime":1659844770,"is_pvip":true,"replies":[{"id":"128766","content":"👍🏻","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1660014421,"ip_address":"北京","comment_id":353830,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1659844770","product_id":100093301,"comment_content":"巨人肩膀@马克图库<br>import torch<br>import torch.nn as nn<br>import torchvision<br>import torchvision.transforms as transforms<br>from PIL import Image<br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224, 224)), <br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>])<br><br>cifar10_train_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,train=True,transform=transform,target_transform=None)<br>train_loader = torch.utils.data.DataLoader(dataset=cifar10_train_dataset,batch_size=32,shuffle=True,num_workers=2) <br>classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)<br>n_total_steps = len(train_loader)<br><br>class MyCNN(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)<br>        self.fc = nn.Linear(16 * 222 * 222, 10)<br>        <br>    def forward(self, input):<br>        x = self.conv1(input)<br>        x = x.view(x.shape[0], -1)<br>        x = self.fc(x)<br>        return x<br>    <br>device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)<br>model = MyCNN().to(device)<br><br>criterion = nn.CrossEntropyLoss() <br>optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>print(&#39;Start training...&#39;)<br>for epoch in range(4):<br>    for i, (images, labels) in enumerate(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        labels_pred = model(images)<br>        loss = criterion(labels_pred, labels)<br>        <br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step<br>        <br>        if (i+1) % 100 == 0:<br>            print(f&#39;Epoch [{epoch+1}&#47;{4}], Step [{i+1:5d}&#47;{n_total_steps}], Loss={loss.item():.4f}&#39;)<br><br>print(&#39;Training complete!&#39;)<br><br>with torch.no_grad():<br>    im = Image.open(&#39;.&#47;images&#47;dog.jpg&#39;)<br>    input_tensor = transform(im).unsqueeze(0).to(device)<br>    label_pred = model(input_tensor).argmax()<br>    print(f&#39;Your label predicted: {classes[label_pred]}&#39;)","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":583291,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660014422,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":353824,"user_name":"John(易筋)","can_delete":false,"product_type":"c1","uid":1180202,"ip_address":"广东","ucode":"BB4E58DD4B8F15","user_header":"https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg","comment_is_top":false,"comment_ctime":1659840989,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1659840989","product_id":100093301,"comment_content":"&#47;&#47; 这里需要 import DataLoader<br>from torch.utils.data import Dataset, DataLoader<br><br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#47;&#39;,<br>                                              train=False,<br>                                              transform=transforms.ToTensor(),<br>                                              target_transform=None,<br>                                              download=True)<br>tensor_dataloader = DataLoader(dataset=cifar10_dataset,                               <br>                               batch_size=32)<br>data_iter = iter(tensor_dataloader)<br>img_tensor, label_tensor = data_iter.next()<br>print(img_tensor.shape)<br>grid_tensor = torchvision.utils.make_grid(img_tensor, nrow=16, padding=2)<br>grid_img = transforms.ToPILImage()(grid_tensor)<br>display(grid_img)<br>","like_count":0,"discussions":[{"author":{"id":1074927,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcZh2daicchEpEiaQZcSesibP6DcklcSE0aejLqA97iaqJjQF3ZbO0NmIUwAIIXwqv4iaz5EYICTPfGJg/132","nickname":"rock","note":"","ucode":"D104A18BC07906","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":588260,"discussion_content":"打一出来的就两行，老师发的图怎么很多行？ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1663638213,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":352528,"user_name":"Zeurd","can_delete":false,"product_type":"c1","uid":2919792,"ip_address":"陕西","ucode":"7CE1E0F76C3313","user_header":"https://static001.geekbang.org/account/avatar/00/2c/8d/70/b0047299.jpg","comment_is_top":false,"comment_ctime":1658747345,"is_pvip":true,"replies":[{"id":"128356","content":"你好，Zeurd，感谢你的留言。V3的话比较特殊，有两个输出，使用第一个就行。<br>你可以这样修改你的代码。<br>output, _ = your_v3_model(data)<br><br>具体你可以参考这里，https:&#47;&#47;pytorch.org&#47;tutorials&#47;beginner&#47;finetuning_torchvision_models_tutorial.html#inception-v3<br><br>Finally, Inception v3 was first described in Rethinking the Inception Architecture for Computer Vision. This network is unique because it has two output layers when training. The second output is known as an auxiliary output and is contained in the AuxLogits part of the network. The primary output is a linear layer at the end of the network. Note, when testing we only consider the primary output. ","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1659079496,"ip_address":"陕西","comment_id":352528,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1658747345","product_id":100093301,"comment_content":"老师，想请问一下，我在换了一个模型InceptionV3，做损失函数的时候，会提示我argument &#39;input&#39; (position 1) must be Tensor, not InceptionOutputs这是什么原因呢？我用output预测完输出的是一个InceptionOutPuts的量，而不是Tensor形式的，当我想用torch.tensor又会提示我不是一维的，不能转化","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":581942,"discussion_content":"你好，Zeurd，感谢你的留言。V3的话比较特殊，有两个输出，使用第一个就行。\n你可以这样修改你的代码。\noutput, _ = your_v3_model(data)\n\n具体你可以参考这里，https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#inception-v3\n\nFinally, Inception v3 was first described in Rethinking the Inception Architecture for Computer Vision. This network is unique because it has two output layers when training. The second output is known as an auxiliary output and is contained in the AuxLogits part of the network. The primary output is a linear layer at the end of the network. Note, when testing we only consider the primary output. ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1659079496,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"陕西"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":349434,"user_name":"万化8af10b","can_delete":false,"product_type":"c1","uid":1916746,"ip_address":"","ucode":"A5A212C8AE843E","user_header":"https://static001.geekbang.org/account/avatar/00/1d/3f/4a/b1c9e5e3.jpg","comment_is_top":false,"comment_ctime":1655969256,"is_pvip":true,"replies":[{"id":"127284","content":"应为例子里我担心训练集数量多，所以用的测试集进行训练。<br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;, train=False, transform=transform, target_transform=None, download=True)<br><br>train=False了，只有10000张图片。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1656309260,"ip_address":"","comment_id":349434,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1655969256","product_id":100093301,"comment_content":"老师，请教一下，上面的例子<br><br># 训练3个Epoch<br>for epoch in range(3): <br>for item in dataloader: <br>output = alexnet(item[0]) target = item[1] <br># 使用交叉熵损失函数 loss = nn.CrossEntropyLoss()(output, target) <br>print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss)) #以下代码的含义，我们在之前的文章中已经介绍过了 alexnet.zero_grad() <br>loss.backward()<br> optimizer.step()<br><br><br>每epoch里面作的次数我做了计数是313次backwards，为什么会是这个数字，我计算了以下50000张训练集，50000&#47;32&#47;2=781.25次才对阿。请老师点明我算错在哪里？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":577717,"discussion_content":"应为例子里我担心训练集数量多，所以用的测试集进行训练。\ncifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, transform=transform, target_transform=None, download=True)\n\ntrain=False了，只有10000张图片。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656309260,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":344442,"user_name":"刘利","can_delete":false,"product_type":"c1","uid":1503779,"ip_address":"","ucode":"A46571E6B1C58E","user_header":"https://static001.geekbang.org/account/avatar/00/16/f2/23/bb13b3ed.jpg","comment_is_top":false,"comment_ctime":1651559981,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1651559981","product_id":100093301,"comment_content":"hi，老师，微调的时候，","like_count":0},{"had_liked":false,"id":341423,"user_name":"Geek_fc975d","can_delete":false,"product_type":"c1","uid":2459939,"ip_address":"","ucode":"05B6507FE4349B","user_header":"https://static001.geekbang.org/account/avatar/00/25/89/23/e71f180b.jpg","comment_is_top":false,"comment_ctime":1649594287,"is_pvip":false,"replies":[{"id":"124818","content":"你好，感谢留言。<br>1. item是迭代一次dataloader返回的数据，他包含batch_size条数据或batch_size张图片<br>2. Epoch的解释在“加餐 | 机器学习其实就那么几件事”<br>3. 训练速度取决于硬件。可以在更好的CPU或GPU上尝试训练，速度会变快的。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1649744312,"ip_address":"","comment_id":341423,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1649594287","product_id":100093301,"comment_content":"or epoch in range(3): for item in dataloader: output = alexnet(item[0]) target = item[1] # 使用交叉熵损失函数 loss = nn.CrossEntropyLoss()(output, target) print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss)) #以下代码的含义，我们在之前的文章中已经介绍过了 alexnet.zero_grad() loss.backward() optimizer.step()<br><br>想请教老师和各位同学，这个item是多少， 是cifar10的总数据数&#47;batch_size吗？原先我以为这个值等于batch-size，看起来这个结果不对呢。<br><br>还想请教下epoch这个参数是指什么， 是说按照这个batch_size将整个数据集训练几次吗？<br><br>我在Jupyter中训练，训练的速度超级慢，有什么提升的方法吗？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":561910,"discussion_content":"你好，感谢留言。\n1. item是迭代一次dataloader返回的数据，他包含batch_size条数据或batch_size张图片\n2. Epoch的解释在“加餐 | 机器学习其实就那么几件事”\n3. 训练速度取决于硬件。可以在更好的CPU或GPU上尝试训练，速度会变快的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649744312,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":327315,"user_name":"璐璐棒","can_delete":false,"product_type":"c1","uid":1240847,"ip_address":"","ucode":"73A0702A703F23","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIW9p3Q9OuX2kGgroDacib99YvjkichWdl54YBVicno8wNPW9gKibUNb0QPtYvTljSWFdjgXdrGPr7Q5g/132","comment_is_top":false,"comment_ctime":1640057431,"is_pvip":false,"replies":[{"id":"119120","content":"你好，璐璐棒。感谢你的留言。<br>把PNG转成RGB就可以使用预训练模型进行训练了。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1640066199,"ip_address":"","comment_id":327315,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1640057431","product_id":100093301,"comment_content":"你好，老师，请教下，想用预训练的effientnet b0来训练自己的数据，自己的数据是png的图片，这个行的通吗，是需要修改下输入通道数就可以吗，还是说不能使用预训练的模型，可以重新训练一个新的模型？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":540478,"discussion_content":"你好，璐璐棒。感谢你的留言。\n把PNG转成RGB就可以使用预训练模型进行训练了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640066199,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":324287,"user_name":"Hit黎明分明🎩","can_delete":false,"product_type":"c1","uid":2806487,"ip_address":"","ucode":"6476C9EFB2117E","user_header":"https://static001.geekbang.org/account/avatar/00/2a/d2/d7/7f00bea1.jpg","comment_is_top":false,"comment_ctime":1638356867,"is_pvip":false,"replies":[{"id":"117706","content":"hello.，你好。<br>Input type (torch.FloatTensor) 是说输入的数据在cpu上。<br>weight type (torch.cuda.FloatTensor)是说模型的数据在gpu上。<br>所以报了这个错误。<br>看你的代码应该是损失函数不在GPU上。<br>可以这样改一下试试。<br>loss = nn.CrossEntropyLoss().to(device)(output,target)<br>数据、模型、损失函数，有GPU的话都要放到GPU上。<br>    <br>","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1638408649,"ip_address":"","comment_id":324287,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1638356867","product_id":100093301,"comment_content":"<br>#数据加载<br><br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>    ])<br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,<br>                                       train=False,<br>                                       transform=transform,<br>                                       target_transform=None,<br>                                       download=True)<br>dataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数<br>                               batch_size=32,       # 输出的batch大小<br>                               shuffle=True,       # 数据是否打乱<br>                               num_workers=0)      # 进程数, 0表示只有主进程<br><br><br>#定义模型<br>class myCNN(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3,16,kernel_size=3)<br>        self.fc = nn.Linear(16 * 222 *222 ,10)<br>        <br>    def forward(self,input):<br>        x = self.conv1(input)<br>        # 进去全连接层之前，先将特征图铺平<br>        x = x.view(x.shape[0],-1)<br>        x = self.fc(x)<br>        return x<br><br># 尽量使用gpu进行训练,如果没有cpu则使用gpu来训练<br>device = torch.device(&quot;cuda:0&quot;if torch.cuda.is_available()else &quot;cpu&quot;)<br><br>cnn = myCNN().to(device)<br><br><br>optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br>steps = 0<br><br>for epoch in range(30):<br>    for item in dataloader:<br>        steps += 1<br>        output = cnn(item[0])<br>        target = item[1].to(device)<br>        loss = nn.CrossEntropyLoss()(output,target)<br>        print(&#39;Epoch{},Loss{}&#39;.format(epoch + 1 ,loss))<br>        cnn.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>im = Image.open(&#39;data&#47;img.png&#39;)<br>input_tensor = transform(im).unsqueeze(0)<br>result = cnn(input_tensor.to(device)).argmax()<br>print(result)<br>老师您好，我运行以后出现报错：<br>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535305,"discussion_content":"hello.，你好。\nInput type (torch.FloatTensor) 是说输入的数据在cpu上。\nweight type (torch.cuda.FloatTensor)是说模型的数据在gpu上。\n所以报了这个错误。\n看你的代码应该是损失函数不在GPU上。\n可以这样改一下试试。\nloss = nn.CrossEntropyLoss().to(device)(output,target)\n数据、模型、损失函数，有GPU的话都要放到GPU上。\n    \n","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1638408649,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2808726,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/db/96/885df919.jpg","nickname":"Challenge And Change","note":"","ucode":"466A0FFCEF7C00","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536299,"discussion_content":"output = cnn(item[0].to(device))  是这样吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638755527,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2806487,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/d2/d7/7f00bea1.jpg","nickname":"Hit黎明分明🎩","note":"","ucode":"6476C9EFB2117E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535413,"discussion_content":"您好，我修改了loss函数好像还是显示一样的错误，可能是我的输入数据没放到GPU上吗？我不知道怎么将所有数据都调到GPU上\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638433626,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":322953,"user_name":"autiplex","can_delete":false,"product_type":"c1","uid":2817246,"ip_address":"","ucode":"6D8DB528CC3A3E","user_header":"https://static001.geekbang.org/account/avatar/00/2a/fc/de/6e2cb960.jpg","comment_is_top":false,"comment_ctime":1637668457,"is_pvip":false,"replies":[{"id":"117271","content":"你好。感谢你的留言。我用你发的代码，并没有出现你说的错误。<br>我查了一下似乎是多进程的问题。你可以这样试试：<br>1. num_worker 设为-1试试。<br>2. num_worker 设为1试试。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1637734158,"ip_address":"","comment_id":322953,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637668457","product_id":100093301,"comment_content":"from PIL import Image<br>from torch.utils.data import DataLoader<br>import torch<br>from torch import nn<br>import torchvision.transforms as transforms<br>import torchvision<br><br><br>#  读取数据<br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>    ])<br><br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,<br>                                       train=False,<br>                                       transform=transform,<br>                                       target_transform=None,<br>                                       download=True)<br>dataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数<br>                               batch_size=32,       # 输出的batch大小<br>                               shuffle=True,       # 数据是否打乱<br>                               num_workers=2)      # 进程数, 0表示只有主进程<br><br><br>#  网络结构<br>class MyCNN(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)<br>        # conv1输出的特征图为222x222大小<br>        self.fc = nn.Linear(16 * 222 * 222, 10)<br><br>    def forward(self, input):<br>        x = self.conv1(input)<br>        # 进去全连接层之前，先将特征图铺平<br>        x = x.view(x.shape[0], -1)<br>        x = self.fc(x)<br>        return x<br><br>cnn = MyCNN()<br>#  损失函数<br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>#  训练10个Epoch<br>for epoch in range(10):<br>    for item in dataloader: <br>        output = cnn(item[0])<br>        target = item[1]<br>        loss = criterion(output, target)<br>        print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss))<br>        #以下代码的含义，我们在之前的文章中已经介绍过了<br>        cnn.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br><br>#  测试结果<br>pass<br><br>老师上面的代码运行会发生下面的报错，我查了一下说是要放在__name__==&#39;__main__&#39;里运行，我没太明白<br><br>The &quot;freeze_support()&quot; line can be omitted if the program<br>        is not going to be frozen to produce an executable.<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532925,"discussion_content":"你好。感谢你的留言。我用你发的代码，并没有出现你说的错误。\n我查了一下似乎是多进程的问题。你可以这样试试：\n1. num_worker 设为-1试试。\n2. num_worker 设为1试试。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637734158,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321561,"user_name":"zhaobk","can_delete":false,"product_type":"c1","uid":1291596,"ip_address":"","ucode":"30BEE12D2CF020","user_header":"https://static001.geekbang.org/account/avatar/00/13/b5/4c/6b9528f8.jpg","comment_is_top":false,"comment_ctime":1636947451,"is_pvip":false,"replies":[{"id":"116841","content":"hi，zhaobk，你好。谢谢你的留言。<br>看代码似乎没有问题。<br>CIFAR10的图片比较小，猫和狗比较像，一部分图片预测错误是可以接受的。可以把cifar10_dataset = torchvision.datasets.CIFAR10中的train改为True试一试，我为了方便直接用测试集作为训练集了。<br>","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1637052933,"ip_address":"","comment_id":321561,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636947451","product_id":100093301,"comment_content":"#图像分类<br>class MyCNN(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)<br>        # conv1输出的特征图为222x222大小<br>        self.fc = nn.Linear(16 * 222 * 222, 10)<br><br>    def forward(self, input):<br>        x = self.conv1(input)<br>        # 进去全连接层之前，先将特征图铺平<br>        x = x.view(x.shape[0], -1)<br>        x = self.fc(x)<br>        return x<br><br><br>#定义读取数据格式 mean和std是ImageNet的均值与标准差。torchvision中的模型都是在ImageNet上训练的。<br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406],<br>                         std=[0.229, 0.224, 0.225])<br>])<br>#获取数据<br><br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,<br>                                       train=False,<br>                                       transform=transform,<br>                                       target_transform=None,<br>                                       download=True)<br><br><br>#加载数据<br>dataloader = DataLoader(<br>    dataset=cifar10_dataset, # 传入的数据集, 必须参数<br>    batch_size=32,       # 输出的batch大小<br>    shuffle=True,       # 数据是否打乱<br>    num_workers=0)      # 进程数, 0表示只有主进程<br><br>cnn = MyCNN()<br>#定义优化器<br>optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>for epoch in range(10):<br>    for item in dataloader:<br>        cnt+=1<br>        output = cnn(item[0])<br>        target = item[1]<br>        # 使用交叉熵损失函数<br>        loss = nn.CrossEntropyLoss()(output, target)<br>        print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss))<br>        #首先要通过zero_grad()函数把梯度清零<br><br>        cnn.zero_grad()<br>        # 算完loss之后进行反向传播，这个过程之后梯度会记录在变量中<br>        loss.backward()<br>        # 用计算的梯度去做优化<br>        optimizer.step()<br><br>im = Image.open(&#39;dog.jpg&#39;)<br>input_tensor = transform(im).unsqueeze(0)<br>print(cnn(input_tensor).argmax())<br><br>我训练完loss总是在1.8~~2.0之间震荡，最后用dog.jpg测试出来的分类是3，cat。请问老师，我的代码有问题么？还是训练的少？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530318,"discussion_content":"hi，zhaobk，你好。谢谢你的留言。\n看代码似乎没有问题。\nCIFAR10的图片比较小，猫和狗比较像，一部分图片预测错误是可以接受的。可以把cifar10_dataset = torchvision.datasets.CIFAR10中的train改为True试一试，我为了方便直接用测试集作为训练集了。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637052933,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321467,"user_name":"cab","can_delete":false,"product_type":"c1","uid":1983360,"ip_address":"","ucode":"E096A122C35ADE","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ0F94uoYZQicSOIfEfSr9gH7CTKibNBsS6d9PRDd8cy7bdTCF9jibXYtf0esGqsQAItHnElejIFovxg/132","comment_is_top":false,"comment_ctime":1636889377,"is_pvip":false,"replies":[{"id":"116770","content":"hi，cab。感谢你的留言。因为改过之后可以接受任意尺度（在可以正常前向传播的情况下）的输入。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636941538,"ip_address":"","comment_id":321467,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636889377","product_id":100093301,"comment_content":"请问一下为什么PyTorch中的AlexNet网络结构与论文中的不一样呢？<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530298,"discussion_content":"hi，cab。感谢你的留言。因为改过之后可以接受任意尺度（在可以正常前向传播的情况下）的输入。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636941538,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321264,"user_name":"马克图布","can_delete":false,"product_type":"c1","uid":1019274,"ip_address":"","ucode":"9E3879D6A55244","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg","comment_is_top":false,"comment_ctime":1636729087,"is_pvip":false,"replies":[{"id":"116687","content":"hi，马克图布。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻^^，厉害。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636808224,"ip_address":"","comment_id":321264,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636729087","product_id":100093301,"comment_content":"代码：<br><br>import torch<br>import torch.nn as nn<br>import torchvision<br>import torchvision.transforms as transforms<br>from PIL import Image<br><br><br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224, 224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>])<br><br>cifar10_train_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;, train=True, transform=transform, target_transform=None, download=True)<br>train_loader = torch.utils.data.DataLoader(dataset=cifar10_train_dataset, batch_size=32, shuffle=True, num_workers=2)<br>classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)<br>n_total_steps = len(train_loader)<br><br><br>class MyCNN(nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)<br>        self.fc = nn.Linear(16 * 222 * 222, 10)<br><br>    def forward(self, input):<br>        x = self.conv1(input)<br>        x = x.view(x.shape[0], -1)<br>        x = self.fc(x)<br>        return x<br><br><br>device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)<br>model = MyCNN().to(device)<br><br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>print(&#39;Start training...&#39;)<br>for epoch in range(4):<br>    for i, (images, labels) in enumerate(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        labels_pred = model(images)<br>        loss = criterion(labels_pred, labels)<br><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br><br>        if (i+1) % 100 == 0:<br>            print(f&#39;Epoch [{epoch+1}&#47;{4}], Step [{i+1:5d}&#47;{n_total_steps}], Loss = {loss.item():.4f}&#39;)<br>print(&#39;Training complete!&#39;)<br><br>print(&#39;Start testing...&#39;)<br>with torch.no_grad():<br>    im = Image.open(&#39;dog.jpg&#39;)<br>    input_tensor = my_transform(im).unsqueeze(0).to(device)<br>    label_pred = model(input_tensor).argmax()<br>    print(f&#39;Your label predicted：{classes[label_pred]}&#39;)<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530255,"discussion_content":"hi，马克图布。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻^^，厉害。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636808224,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321238,"user_name":"clee","can_delete":false,"product_type":"c1","uid":1467446,"ip_address":"","ucode":"122875102B23A8","user_header":"","comment_is_top":false,"comment_ctime":1636722136,"is_pvip":false,"replies":[{"id":"116686","content":"hi, clee。<br>原因是学习率设置有点大了，我用0.0001训练了是ok的。<br><br>使用optim.SGD(net.parameters())（1个模型对应1个优化器）初始化的时候。<br>self.optimizer.zero_grad()与self.model.zero_grad()是一样的。<br><br>如果1个优化器对应多个模型的时候，就要具体情况具体分析了。如果直接optimizer.zero_grad()的话，就会把所有模型的梯度清零。<br><br>loss.backward()与optimizer.step() 是不同的作用。<br>loss.backward()是算完loss之后进行反向梯度传播，这个过程之后梯度会记录在变量中。<br>optimizer.step() 是用计算的梯度去做优化。<br>","user_name":"作者回复","user_name_real":"作者","uid":"2802608","ctime":1636808193,"ip_address":"","comment_id":321238,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1636722136","product_id":100093301,"comment_content":"import torch<br>import torch.nn as nn<br>from torch.utils.data import DataLoader<br>from torchvision import transforms<br>import torchvision<br><br>transform = transforms.Compose([transforms.RandomResizedCrop((224,224)), <br>                                transforms.ToTensor(), <br>                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])<br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;, <br>                                               train=False, <br>                                               transform=transform, <br>                                               target_transform=None, <br>                                               download=True)<br>dataloader = DataLoader(dataset=cifar10_dataset, batch_size=32)<br>class MyCNN(nn.Module): <br>  def __init__(self): <br>    super().__init__()<br>    self.conv1 = nn.Conv2d(3, 16, kernel_size=3)<br>    self.fc = nn.Linear(16 * 222 * 222, 10)<br>  def forward(self, input):<br>    x = self.conv1(input)<br>    # 进去全连接层之前，先将特征图铺平<br>    x = x.view(x.shape[0], -1)<br>    x = self.fc(x)<br>    return x<br><br>class MyCNN_Model():<br>  def __init__(self):<br>    self.learning_rate = 0.001<br>    self.epoches = 100<br>    self.model = MyCNN()<br>    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)<br>    self.loss_function = nn.CrossEntropyLoss()<br><br>  def train(self):<br>    for epoch in range(self.epoches):<br>      for item in dataloader:<br>        prediction = self.model(item[0])<br>        loss = self.loss_function(prediction, item[1])<br>        self.optimizer.zero_grad()<br>        loss.backward()<br>        self.optimizer.step()<br>      print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss))<br>    torch.save(self.model.state_dict(), &#39;mycnn.pth&#39;)<br><br>疑问一：Loss输出特别大，是哪个参数没有配置正确吗?<br>Epoch 1, Loss 44.85285568237305<br>Epoch 2, Loss 6869.75048828125，<br>疑问二：<br>loss.backward()，optimizer.step() 两行代码都是执行了方法而已，没有传递任何参数，那么损失函数和优化器内部和模型是怎么关联起来的。<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530248,"discussion_content":"hi, clee。\n原因是学习率设置有点大了，我用0.0001训练了是ok的。\n\n使用optim.SGD(net.parameters())（1个模型对应1个优化器）初始化的时候。\nself.optimizer.zero_grad()与self.model.zero_grad()是一样的。\n\n如果1个优化器对应多个模型的时候，就要具体情况具体分析了。如果直接optimizer.zero_grad()的话，就会把所有模型的梯度清零。\n\nloss.backward()与optimizer.step() 是不同的作用。\nloss.backward()是算完loss之后进行反向梯度传播，这个过程之后梯度会记录在变量中。\noptimizer.step() 是用计算的梯度去做优化。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636808193,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":531014,"discussion_content":"hi，clee。不好意思，上次的回复有点问题，我修改了一下。麻烦再看一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637205131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321195,"user_name":"Geek_8cfc4c","can_delete":false,"product_type":"c1","uid":2156103,"ip_address":"","ucode":"83EF0B576530E2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7uYZvwhrLHsJICstGXaOvUNZnyq0aO7gF0OLicMyZAZFnRiaDyvM1lGxnEDP2VUMV3m6UjiazMmSNGQ/132","comment_is_top":false,"comment_ctime":1636703529,"is_pvip":false,"replies":[{"id":"116683","content":"不应该啊，一模一样的代码和数据吗？我似乎也没什么线索。。。。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636806672,"ip_address":"","comment_id":321195,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636703529","product_id":100093301,"comment_content":"有个神奇问题，我在mac上跑模型训练loss感觉没有收敛的迹象<br>用linux机器跑就收敛了……<br>版本号都是一样的……<br>这可能是什么问题呢？","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530243,"discussion_content":"不应该啊，一模一样的代码和数据吗？我似乎也没什么线索。。。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636806672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321178,"user_name":"王珎","can_delete":false,"product_type":"c1","uid":2822974,"ip_address":"","ucode":"5074BA9DD3DE88","user_header":"https://static001.geekbang.org/account/avatar/00/2b/13/3e/d028cddd.jpg","comment_is_top":false,"comment_ctime":1636699546,"is_pvip":false,"replies":[{"id":"116685","content":"你好，不是运气好^^。CIFAR10比较简单，10个Epoch应该差不多。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636807554,"ip_address":"","comment_id":321178,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1636699546","product_id":100093301,"comment_content":"from PIL import Image<br>import torchvision<br>import torchvision.transforms as transforms<br>import torch<br>from torch.utils.data import DataLoader<br><br># 数据读取<br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop((224,224)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>    ])<br>cifar10_dataset = torchvision.datasets.CIFAR10(root=&#39;.&#47;data&#39;,<br>                                       train=False, #下载测试集<br>                                       transform=transform,<br>                                       target_transform=None,<br>                                       download=True)<br>dataloader = DataLoader(dataset=cifar10_dataset, # 传入的数据集, 必须参数<br>                               batch_size=32,       # 输出的batch大小<br>                               shuffle=True,       # 数据是否打乱<br>                               num_workers=2)   # 进程数, 0表示只有主进程<br><br># 定义计算设备<br>device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) <br><br># 实例化模型<br>cnn = MyCNN().to(device)<br># 定义优化器<br>optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)<br><br>steps = 0<br>for epoch in range(10):<br>    for item in dataloader: <br>        steps += 1<br>        output = cnn(item[0].to(device))<br>        target = item[1].to(device)<br>        # 使用交叉熵损失函数<br>        loss = nn.CrossEntropyLoss()(output, target)<br>        <br>        if steps % 50 == 0:<br>            print(&#39;Epoch {}, Loss {}&#39;.format(epoch + 1 , loss))<br>        cnn.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br><br><br># 用训练好的模型预测<br>im = Image.open(&#39;dog.jpg&#39;)<br>input_tensor = transform(im).unsqueeze(0)<br>cnn(input_tensor.to(device)).argmax()<br><br>输出：tensor(5, device=&#39;cuda:0&#39;)<br><br>预测类别是“狗”。<br><br>是运气好吗？只训练了10 epochs，最后显示的loss 1.8左右<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530237,"discussion_content":"你好，不是运气好^^。CIFAR10比较简单，10个Epoch应该差不多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636807554,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1019274,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg","nickname":"马克图布","note":"","ucode":"9E3879D6A55244","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414375,"discussion_content":"同学好，你在构建数据集的时候，似乎是用了 CIFAR10 的测试集作为训练集，torchvision.datasets.CIFAR10() 的 train 参数应该是 True 才为使用其训练集，否则使用的是其测试集 ^^ 但是拿它来训练自己的模型好像也没关系 hhh\n\n另外，我训练集的 loss 也是 1.8-2.2 之间， num_epochs 为 4，batch_size 为 32。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636729504,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321148,"user_name":"Geek_8cfc4c","can_delete":false,"product_type":"c1","uid":2156103,"ip_address":"","ucode":"83EF0B576530E2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7uYZvwhrLHsJICstGXaOvUNZnyq0aO7gF0OLicMyZAZFnRiaDyvM1lGxnEDP2VUMV3m6UjiazMmSNGQ/132","comment_is_top":false,"comment_ctime":1636688742,"is_pvip":false,"replies":[{"id":"116684","content":"你好，谢谢你的留言。<br>关于unsqueeze()。你说的对。这是有点多余，等我给去掉。<br><br>关于模型版本。恩，是的，torchvision版本不同，模型会重新训练。<br><br>模型地址在https:&#47;&#47;github.com&#47;pytorch&#47;vision&#47;blob&#47;release&#47;0.10&#47;torchvision&#47;models&#47;alexnet.py#L11<br><br>你可以切换不同的版本看看。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1636807430,"ip_address":"","comment_id":321148,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1636688742","product_id":100093301,"comment_content":"对于本节课有几个问题不太明白还望赐教 （pytorch版本&#39;1.10.0&#39;）<br><br>1. 模型训练那 部分<br>`y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(0)`<br>unsqueeze的意义是啥呢？ (y_train本来size是Size([30])正好和x_train匹配)<br>这样做反而得到了一个warning<br>`<br>UserWarning: Using a target size (torch.Size([1, 30])) that is different to the input size (torch.Size([30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.<br>`<br><br>2. 再说微调 部分<br>一开始直接下载的AlexNet模型(alexnet-owt-7be5be79.pth), 预测结果是151<br>后来使用于文中一样的版本才变成263<br><br>因为torchvision的版本不同，预训练的模型结果就有了差异么？<br>如果是的话我看到的标签列表中151是吉娃娃（Chihuahua）……<br><br>谢谢<br>","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530233,"discussion_content":"你好，谢谢你的留言。\n关于unsqueeze()。你说的对。这是有点多余，等我给去掉。\n\n关于模型版本。恩，是的，torchvision版本不同，模型会重新训练。\n\n模型地址在https://github.com/pytorch/vision/blob/release/0.10/torchvision/models/alexnet.py#L11\n\n你可以切换不同的版本看看。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636807430,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1019274,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg","nickname":"马克图布","note":"","ucode":"9E3879D6A55244","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414380,"discussion_content":"第一个我也有同样的疑问；\n\n第二个的话，可能是由于 RandomResizedCrop() 切分的位置不同，导致 AlexNet 预测结果有些差异，多跑几次就能出现 263 了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636730100,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1702638,"avatar":"","nickname":"GEEKBANG_9421399","note":"","ucode":"1E3E1D6D6F2901","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1019274,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg","nickname":"马克图布","note":"","ucode":"9E3879D6A55244","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":550522,"discussion_content":"第二个我也觉得是RandomResizedCrop()的原因，我下载下来的狗狗图片size是(1920, 1867)，感觉如果Crop掉太多部分的话，会预测不准的。我把RandomResizedCrop()改成了Resize()，预测结果就是263了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1644575410,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":414380,"ip_address":""},"score":550522,"extra":""}]}]}]}