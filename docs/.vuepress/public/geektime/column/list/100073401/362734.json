{"id":362734,"title":"15 | 内存视角（一）：如何最大化内存的使用效率？","content":"<p>你好，我是吴磊。</p><p>上一讲我们说，想要提升CPU利用率，最重要的就是合理分配执行内存，但是，执行内存只是Spark内存分区的一部分。因此，想要合理分配执行内存，我们必须先从整体上合理划分好Spark所有的内存区域。</p><p>可在实际开发应用的时候，身边有不少同学向我抱怨：“Spark划分不同内存区域的原理我都知道，但我还是不知道不同内存区域的大小该怎么设置，纠结来、纠结去。最后，所有跟内存有关的配置项，我还是保留了默认值。”</p><p>这种不能把原理和实践结合起来的情况很常见，所以今天这一讲，我就从熟悉的Label Encoding实例出发，<strong>一步步带你去分析不同情况下，不同内存区域的调整办法，</strong>帮你归纳出最大化内存利用率的常规步骤。这样，你在调整内存的时候，就能结合应用的需要，做到有章可循、有的放矢。</p><h2>从一个实例开始</h2><p>我们先来回顾一下<a href=\"https://time.geekbang.org/column/article/355028\">第5讲</a>中讲过的Label Encoding。在Label Encoding的业务场景中，我们需要对用户兴趣特征做Encoding。依据模板中兴趣字符串及其索引位置，我们的任务是把千亿条样本中的用户兴趣转换为对应的索引值。模板文件的内容示例如下所示。</p><pre><code>//模板文件\n//用户兴趣\n体育-篮球-NBA-湖人\n军事-武器-步枪-AK47\n</code></pre><p>实现的代码如下所示，注意啦，这里的代码是第5讲中优化后的版本。</p><!-- [[[read_end]]] --><pre><code>/**\n输入参数：模板文件路径，用户兴趣字符串\n返回值：用户兴趣字符串对应的索引值\n*/\n//函数定义\nval findIndex: (String) =&gt; (String) =&gt; Int = {\n(filePath) =&gt;\nval source = Source.fromFile(filePath, &quot;UTF-8&quot;)\nval lines = source.getLines().toArray\nsource.close()\nval searchMap = lines.zip(0 until lines.size).toMap\n(interest) =&gt; searchMap.getOrElse(interest, -1)\n}\nval partFunc = findIndex(filePath)\n\n//Dataset中的函数调用\npartFunc(&quot;体育-篮球-NBA-湖人&quot;)\n</code></pre><p>下面，咱们先一起回顾一下代码实现思路，再来分析它目前存在的性能隐患，最后去探讨优化它的方法。</p><p>首先，findIndex函数的主体逻辑比较简单，就是读取模板文件和构建Map映射，以及查找用户兴趣并返回索引。不过，findIndex函数被定义成了高阶函数。这样一来，当以模板文件为实参调用这个高阶函数的时候，我们会得到一个内置了Map查找字典的标量函数partFunc，最后在千亿样本上调用partFunc完成数据转换。<strong>利用高阶函数，我们就避免了让Executor中的每一个Task去读取模板文件，以及从头构建Map字典这种执行低效的做法。</strong></p><p>在运行时，这个函数在Driver端会被封装到一个又一个的Task中去，随后Driver把这些Task分发到Executor，Executor接收到任务之后，交由线程池去执行（调度系统的内容可以回顾<a href=\"https://time.geekbang.org/column/article/355028\">第5讲）</a>。这个时候，每个Task就像是一架架小飞机，携带着代码“乘客”和数据“行李”，从Driver飞往Executor。Task小飞机在Executor机场着陆之后，代码“乘客”乘坐出租车或是机场大巴，去往JVM stack；数据“行李”则由专人堆放在JVM Heap，也就是我们常说的堆内内存。</p><p>回顾Label encoding中的findIndex函数不难发现，其中大部分都是代码“乘客”，唯一的数据“行李”是名为searchMap的Map字典。像这样用户自定义的数据结构，消耗的内存区域就是堆内内存的User Memory（Spark对内存区域的划分内容可以回顾一下<a href=\"https://time.geekbang.org/column/article/355662\">第7讲</a>）。</p><h3>User Memory性能隐患</h3><p>回顾到这里，你觉得findIndex函数有没有性能隐患呢？你可以先自己思考一下，有了答案之后再来看我下面的分析。</p><p>答案当然是“有”。首先，每架小飞机都携带这么一份数据“大件行李”，自然需要消耗更多的“燃油”，这里的“燃油”指的<strong>是Task分发过程中带来的网络开销</strong>。其次，因为每架小飞机着陆之后，都会在Executor的“旅客行李专区”User Memory寄存上这份同样的数据“行李”，所以，<strong>User Memory需要确保有足够的空间可以寄存所有旅客的行李，也就是大量的重复数据</strong>。</p><p>那么，User Memory到底需要准备出多大的内存空间才行呢？我们不妨来算一算。这样的计算并不难，只需要用飞机架次乘以行李大小就可以了。</p><p>用户自定义的数据结构往往是用于辅助函数完成计算任务的，所以函数执行完毕之后，它携带的数据结构的生命周期也就告一段落。<strong>因此，在Task的数量统计上，我们不必在意一个Executor总共需要处理多少个Task，只需要关注它在同一时间可以并行处理的Task数量，也就是Executor的线程池大小即可</strong>。</p><p>我们说过，Executor线程池大小由spark.executor.cores和spark.task.cpus这两个参数的商（spark.executor.cores/spark.task.cpus）决定，我们暂且把这个商记作#threads。</p><p>接下来是估算数据“行李”大小，由于searchMap并不是分布式数据集，因此我们不必采用先Cache，再提取Spark执行计划统计信息的方式。对于这样的Java数据结构，我们完全可以在REPL中，通过Java的常规方法估算数据存储大小，估算得到的searchMap大小记为#size。</p><p>好啦！现在，我们可以算出，User Memory至少需要提供#threads * #size这么大的内存空间，才能支持分布式任务完成计算。但是，对于User Memory内存区域来说，使用#threads * #size的空间去重复存储同样的数据，本身就是降低了内存的利用率。那么，我们该怎么省掉#threads * #size的内存消耗呢？</p><h2>性能调优</h2><p>学习过广播变量之后，想必你头脑中已经有了思路。没错，咱们可以尝试使用广播变量，来对示例中的代码进行优化。</p><p>仔细观察findIndex函数，我们不难发现，函数的核心计算逻辑有两点。一是读取模板文件、创建Map映射字典；二是以给定字符串对字典进行查找，并返回查找结果。显然，千亿样本转换的核心需求是其中的第二个环节。既然如此，我们完全可以把创建好的Map字典封装成广播变量，然后分发到各个Executors中去。</p><p>有了广播变量的帮忙，凡是发往同一个Executor的Task小飞机，都无需亲自携带数据“行李”，这些大件行李会由“联邦广播快递公司”派货机专门发往各个Executors，Driver和每个Executors之间，都有一班这样的货运专线。思路说完了，优化后的代码如下所示。</p><pre><code>/**\n广播变量实现方式\n*/\n//定义广播变量\nval source = Source.fromFile(filePath, &quot;UTF-8&quot;)\nval lines = source.getLines().toArray\nsource.close()\nval searchMap = lines.zip(0 until lines.size).toMap\nval bcSearchMap = sparkSession.sparkContext.broadcast(searchMap)\n \n//在Dataset中访问广播变量\nbcSearchMap.value.getOrElse(&quot;体育-篮球-NBA-湖人&quot;, -1)\n\n</code></pre><p>上面代码的实现思路很简单：第一步还是读取模板文件、创建Map字典；第二步，把Map字典封装为广播变量。这样一来，在对千亿样本进行转换时，我们直接通过bcSearchMap.value读取广播变量内容，然后，通过调用Map字典的getOrElse方法来获取用户兴趣对应的索引值。</p><p>相比最开始的第一种实现方式，第二种实现方式的代码改动还是比较小的，那这一版代码对内存的消耗情况有什么改进呢？</p><p>我们发现，Task小飞机的代码“乘客”换人了！<strong>小飞机之前需要携带函数findIndex，现在则换成了一位“匿名的乘客”</strong>：一个读取广播变量并调用其getOrElse方法的匿名函数。由于这位“匿名的乘客”将大件行李托运给了“联邦广播快递公司”的专用货机，因此，Task小飞机着陆后，没有任何“行李”需要寄存到User Memory。换句话说，优化后的版本不会对User Memory内存区域进行占用，所以第一种实现方式中#threads * #size的内存消耗就可以省掉了。</p><h3>Storage Memory规划</h3><p>这样一来，原来的内存消耗转嫁到了广播变量身上。但是，广播变量也会消耗内存，这会不会带来新的性能隐患呢？那我们就来看看，广播变量消耗的具体是哪块内存区域。</p><p><a href=\"https://time.geekbang.org/column/article/355081\">回顾存储系统那一讲</a>，我们说过，Spark存储系统主要有3个服务对象，分别是Shuffle中间文件、RDD缓存和广播变量。它们都由Executor上的BlockManager进行管理，对于数据在内存和磁盘中的存储，BlockManager利用MemoryStore和DiskStore进行抽象和封装。</p><p>那么，广播变量所携带的数据内容会物化到MemoryStore中去，以Executor为粒度为所有Task提供唯一的一份数据拷贝。MemoryStore产生的内存占用会被记入到Storage Memory的账上。<strong>因此，广播变量消耗的就是Storage Memory内存区域</strong>。</p><p>接下来，我们再来盘算一下，第二种实现方式究竟需要耗费多少内存空间。由于广播变量的分发和存储以Executor为粒度，因此每个Executor消耗的内存空间，就是searchMap一份数据拷贝的大小。searchMap的大小我们刚刚计算过就是#size。</p><p>明确了Storage Memory内存区域的具体消耗之后，我们自然可以根据公式：（spark.executor.memory – 300MB）* spark.memory.fraction * spark.memory.storageFraction去有针对性地调节相关的内存配置项。</p><h2>内存规划两步走</h2><p>现在，咱们在两份不同的代码实现下，分别定量分析了不同内存区域的消耗与占用。对于这些消耗做到心中有数，我们自然就能够相应地去调整相关的配置项参数。基于这样的思路，想要最大化内存利用率，我们需要遵循两个步骤：</p><ul>\n<li><strong>预估内存占用</strong></li>\n<li><strong>调整内存配置项</strong></li>\n</ul><p>我们以堆内内存为例，来讲一讲内存规划的两步走具体该如何操作。我们都知道，堆内内存划分为Reserved Memory、User Memory、Storage Memory和Execution Memory这4个区域。预留内存固定为300MB，不用理会，其他3个区域需要你去规划。</p><h3>预估内存占用</h3><p>首先，我们来说内存占用的预估，主要分为三步。</p><p>第一步，计算User Memory的内存消耗。我们先汇总应用中包含的自定义数据结构，并估算这些对象的总大小#size，然后<strong>用#size乘以Executor的线程池大小，即可得到User Memory区域的内存消耗#User</strong>。</p><p>第二步，计算Storage Memory的内存消耗。我们先汇总应用中涉及的广播变量和分布式数据集缓存，分别估算这两类对象的总大小，分别记为#bc、#cache。另外，我们把集群中的Executors总数记作#E。这样，<strong>每个Executor中Storage Memory区域的内存消耗的公式就是：#Storage = #bc + #cache / #E</strong>。</p><p>第三步，计算执行内存的消耗。学习上一讲，我们知道执行内存的消耗与多个因素有关。第一个因素是Executor线程池大小#threads，第二个因素是数据分片大小，而数据分片大小取决于数据集尺寸#dataset和并行度#N。因此，<strong>每个Executor中执行内存的消耗的计算公式为：#Execution = #threads * #dataset / #N</strong>。</p><h3>调整内存配置项</h3><p>得到这3个内存区域的预估大小#User、#Storage、#Execution之后，调整相关的内存配置项就是一件水到渠成的事情（由公式（spark.executor.memory – 300MB）* spark.memory.fraction * spark.memory.storageFraction）可知），这里我们也可以分为3步。</p><p>首先，根据定义，<strong>spark.memory.fraction可以由公式（#Storage + #Execution）/（#User + #Storage + #Execution）计算得到</strong>。</p><p>同理，<strong>spark.memory.storageFraction的数值应该参考（#Storage）/（#Storage + #Execution）</strong>。</p><p>最后，对于Executor堆内内存总大小spark.executor.memory的设置，我们自然要参考4个内存区域的总消耗，也就是<strong>300MB + #User + #Storage + #Execution。不过，我们要注意，利用这个公式计算的前提是，不同内存区域的占比与不同类型的数据消耗一致</strong>。</p><p>总的来说，在内存规划的两步走中，第一步预估不同区域的内存占比尤为关键，因为第二步中参数的调整完全取决于第一步的预估结果。如果你按照这两个步骤去设置相关的内存配置项，相信你的应用在运行时就能够充分利用不同的内存区域，避免出现因参数设置不当而导致的内存浪费现象，从而在整体上提升内存利用率。</p><h2>小结</h2><p>合理划分Spark所有的内存区域，是同时提升CPU与内存利用率的基础。因此，掌握内存规划很重要，在今天这一讲，我们把内存规划归纳为两步走。</p><p>第一步是预估内存占用。</p><ul>\n<li>求出User Memory区域的内存消耗，公式为：#User=#size乘以Executor线程池的大小。</li>\n<li>求出每个Executor中Storage Memory区域的内存消耗，公式为：#Storage = #bc + #cache / #E。</li>\n<li>求出执行内存区域的内存消耗，公式为：#Execution = #threads * #dataset / #N。</li>\n</ul><p>第二步是调整内存配置项：根据公式得到的3个内存区域的预估大小#User、#Storage、#Execution，去调整（spark.executor.memory – 300MB）* spark.memory.fraction * spark.memory.storageFraction公式中涉及的所有配置项。</p><ul>\n<li>spark.memory.fraction可以由公式（#Storage + #Execution）/（#User + #Storage + #Execution）计算得到。</li>\n<li>spark.memory.storageFraction的数值应该参考（#Storage）/（#Storage + #Execution）。</li>\n<li>spark.executor.memory的设置，可以通过公式300MB + #User + #Storage + #Execution得到。</li>\n</ul><p>这里，我还想多说几句，<strong>内存规划两步走终归只是手段，它最终要达到的效果和目的，是确保不同内存区域的占比与不同类型的数据消耗保持一致，从而实现内存利用率的最大化</strong>。</p><h2>每日一练</h2><ol>\n<li>你知道估算Java对象存储大小的方法有哪些吗？不同的方法又有哪些优、劣势呢？</li>\n<li>对于内存规划的第一步来说，要精确地预估运行时每一个区域的内存消耗，很费时、费力，调优的成本很高。如果我们想省略掉第一步的精确计算，你知道有哪些方法能够粗略、快速地预估不同内存区域的消耗占比吗？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","neighbors":{"left":{"article_title":"14 | CPU视角：如何高效地利用CPU？","id":362710},"right":{"article_title":"16 | 内存视角（二）：如何有效避免Cache滥用？","id":363445}},"comments":[{"had_liked":false,"id":288618,"user_name":"金角大王","can_delete":false,"product_type":"c1","uid":1259375,"ip_address":"","ucode":"DFBA85FB2FD0B0","user_header":"https://static001.geekbang.org/account/avatar/00/13/37/6f/b3337e6d.jpg","comment_is_top":false,"comment_ctime":1618563216,"is_pvip":false,"replies":[{"id":"104779","content":"好问题，我们换个角度讨论这个问题。你的问题其实是：为什么广播变量可以复用，但是User Memory中的数据却不行。<br><br>想要复用数据，前提是有“一个人”或是“一个地方”，明确记录了需要复用的这份数据，存储在什么地方，只有这样，后面的任务才能复用这份数据，这样的信息，又叫元数据、元信息。<br><br>这就好比，你去宜家买家具，你是需要根据家具的Id，去仓库中自提货物的。家具Id上明确标记了，你需要的货物，存在哪个货架、哪个层、哪个位置。如果没有这样的元信息，偌大的宜家家居超市，你不可能找到你需要的东西。<br><br>广播变量这个“货物”的具体地址，BlockManager会帮忙记录，所以“后来的人”（Task），如果需要访问广播变量，它能从BlockManager那里迅速地知道广播变量存储在哪里（哪个executor的什么位置），可以迅速地访问数据。<br><br>但是，User Memory也好，Storage、Execution memory也罢，他们本质上就是JVM heap。JVM heap虽然也会记录对象引用对应的存储地址，但是，它没有办法区分，两份数据，在内容上，是不是一样的。实际上，JVM也没有这样的义务。<br><br>因此，同一个Task当中两份完全一样的数据，到了JVM那里，它并不知道：“OK，这两份数据一样，后来的人只要访问已有的数据就OK了”。对他来说，这是两份不同的数据，即便内容都一样，他照样会单独花费存储空间来存储。根本原因在于，他并没有Spark的运行时上下文， 不能像BlockManager那样维护全局的元数据。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618638383,"ip_address":"","comment_id":288618,"utype":1}],"discussion_count":1,"race_medal":0,"score":"121877647504","product_id":100073401,"comment_content":"老师，为何UserMemory中自定义数据结构不能像bc那样在StorageMemory中只存一份？","like_count":29,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518685,"discussion_content":"好问题，我们换个角度讨论这个问题。你的问题其实是：为什么广播变量可以复用，但是User Memory中的数据却不行。\n\n想要复用数据，前提是有“一个人”或是“一个地方”，明确记录了需要复用的这份数据，存储在什么地方，只有这样，后面的任务才能复用这份数据，这样的信息，又叫元数据、元信息。\n\n这就好比，你去宜家买家具，你是需要根据家具的Id，去仓库中自提货物的。家具Id上明确标记了，你需要的货物，存在哪个货架、哪个层、哪个位置。如果没有这样的元信息，偌大的宜家家居超市，你不可能找到你需要的东西。\n\n广播变量这个“货物”的具体地址，BlockManager会帮忙记录，所以“后来的人”（Task），如果需要访问广播变量，它能从BlockManager那里迅速地知道广播变量存储在哪里（哪个executor的什么位置），可以迅速地访问数据。\n\n但是，User Memory也好，Storage、Execution memory也罢，他们本质上就是JVM heap。JVM heap虽然也会记录对象引用对应的存储地址，但是，它没有办法区分，两份数据，在内容上，是不是一样的。实际上，JVM也没有这样的义务。\n\n因此，同一个Task当中两份完全一样的数据，到了JVM那里，它并不知道：“OK，这两份数据一样，后来的人只要访问已有的数据就OK了”。对他来说，这是两份不同的数据，即便内容都一样，他照样会单独花费存储空间来存储。根本原因在于，他并没有Spark的运行时上下文， 不能像BlockManager那样维护全局的元数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618638383,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288657,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1618582318,"is_pvip":false,"replies":[{"id":"104777","content":"对，没错，文中说的数据集大小是内存中的数据集。<br><br>这块咱们有过介绍哈~ 最精确的办法：<br><br>val df: DataFrame = _<br>df.cache.count <br><br>val plan = df.queryExecution.logical<br>val estimated: BigInt = spark<br>.sessionState<br>.executePlan(plan)<br>.optimizedPlan<br>.stats<br>.sizeInBytes<br><br>就是先Cache，再查看物理执行计划，可以获得数据集在内存中存储大小。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618637438,"ip_address":"","comment_id":288657,"utype":1}],"discussion_count":1,"race_medal":0,"score":"87517928238","product_id":100073401,"comment_content":"还有个疑问，想请教下磊哥<br>文中说的数据集大小是内存中的数据集吧<br>文件落盘后数据集大小可以很方便查看，那内存中数据集大小怎么看呢","like_count":21,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518697,"discussion_content":"对，没错，文中说的数据集大小是内存中的数据集。\n\n这块咱们有过介绍哈~ 最精确的办法：\n\nval df: DataFrame = _\ndf.cache.count \n\nval plan = df.queryExecution.logical\nval estimated: BigInt = spark\n.sessionState\n.executePlan(plan)\n.optimizedPlan\n.stats\n.sizeInBytes\n\n就是先Cache，再查看物理执行计划，可以获得数据集在内存中存储大小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618637438,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288840,"user_name":"zxk","can_delete":false,"product_type":"c1","uid":1221195,"ip_address":"","ucode":"4BB2BD9D2BCD04","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","comment_is_top":false,"comment_ctime":1618725443,"is_pvip":false,"replies":[{"id":"104878","content":"都是非常好的问题。<br>问题一：你说的没错，不管是哪片内存区域，实际上都是JVM Heap的一部分。回答你的问题：如果User Memory空间不足，但是Spark Memory（Storage + Execution）空闲，会不会OOM？对于这种情况，即使你自定的数据结构其实超过了User Memory，但实际上，并不会立即报OOM，因为Spark对于堆内内存的预估，没有那么精确。这里有些Tricky，对于Spark划定的各种“线”，也就是通过配置项设置的不同区域的百分比，它类似于一种“软限制”，也就是建议你遵守它划出的一道道“线”，但如果你没有遵守，强行“跨线”，只要其他区域还有空间，你真的抢占了，Spark也不会立即阻止你。这就好比“中国式”过马路，红绿灯是一种“软限制”，即使是红灯，只要没有机动车通行，咱们照样可以凑一堆人过马路，但如果突然窜出来一辆车，把咱撞了，那就是咱们理亏，因为我们是“闯红灯”的那一方。Spark的“线”也类似，是一种“软限制”。但是，“出来混迟早是要还的”，你的自定义数据结构，占了本该属于Spark Memory的地盘，那么Spark在执行任务或是缓存的时候，很有可能就跟着倒霉，最后（加重加粗）“看上去”是执行任务或是缓存任务OOM，但本质上，是因为你的自定义数据结构，提前占了人家的地方。这种时候，你会很难debug。这也就是为什么Spark要求开发者要遵循各个配置项的设置，不要“越线”。<br><br>问题二：还是User Memory，那个map就是自定义数据结构，它会消耗User Memory。你的那种用法，本质上就是闭包。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618815188,"ip_address":"","comment_id":288840,"utype":1}],"discussion_count":1,"race_medal":0,"score":"74633169475","product_id":100073401,"comment_content":"老师，这边想请教两个问题。<br>问题一：User Memory、Execution Memory、Storage Memory 是属于 Spark 自身对内存区域的划分，但 Spark 的 executor 实际上又是一个 JVM，假如我把 User Memory 设置的非常小，又自定义了一个很大的数据结构，此时 User Memory 不够用了，而 Execution Memory、Storage Memory 还有很大的空闲，那么这时候会不会 OOM？如果是 GC 又不太符合 JVM 的 GC 条件。<br>问题二：在使用 mapPartition 算子的时候，如果我在进入迭代前外部定义了一个 map，然后迭代中往这个 map 添加数据，那么这个 map 又是占用哪部分内存的？","like_count":18,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518747,"discussion_content":"都是非常好的问题。\n问题一：你说的没错，不管是哪片内存区域，实际上都是JVM Heap的一部分。回答你的问题：如果User Memory空间不足，但是Spark Memory（Storage + Execution）空闲，会不会OOM？对于这种情况，即使你自定的数据结构其实超过了User Memory，但实际上，并不会立即报OOM，因为Spark对于堆内内存的预估，没有那么精确。这里有些Tricky，对于Spark划定的各种“线”，也就是通过配置项设置的不同区域的百分比，它类似于一种“软限制”，也就是建议你遵守它划出的一道道“线”，但如果你没有遵守，强行“跨线”，只要其他区域还有空间，你真的抢占了，Spark也不会立即阻止你。这就好比“中国式”过马路，红绿灯是一种“软限制”，即使是红灯，只要没有机动车通行，咱们照样可以凑一堆人过马路，但如果突然窜出来一辆车，把咱撞了，那就是咱们理亏，因为我们是“闯红灯”的那一方。Spark的“线”也类似，是一种“软限制”。但是，“出来混迟早是要还的”，你的自定义数据结构，占了本该属于Spark Memory的地盘，那么Spark在执行任务或是缓存的时候，很有可能就跟着倒霉，最后（加重加粗）“看上去”是执行任务或是缓存任务OOM，但本质上，是因为你的自定义数据结构，提前占了人家的地方。这种时候，你会很难debug。这也就是为什么Spark要求开发者要遵循各个配置项的设置，不要“越线”。\n\n问题二：还是User Memory，那个map就是自定义数据结构，它会消耗User Memory。你的那种用法，本质上就是闭包。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618815188,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292476,"user_name":"静心","can_delete":false,"product_type":"c1","uid":1789481,"ip_address":"","ucode":"B80DE4B5C923D3","user_header":"https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg","comment_is_top":false,"comment_ctime":1620834260,"is_pvip":false,"replies":[{"id":"105935","content":"好问题，是这样的。<br><br>其实这里的关键因素，是谁先谁后的问题，一旦确定了哪个为先，我们就可以参考14讲“三足鼎立”的思路，去调整相关配置项。我们可以分类讨论。<br><br>比如在大厂，很多时候，硬件资源是受限的，因此你能拿到多少计算资源，其实是有quota的，尤其是在大厂多个团队共享一个超大集群的情况下。这个时候，你就得让数据去迁就计算资源，那这就意味着，Executors相关的参数，得先定，然后再去倒推并行度相关的参数。<br><br>相反，如果你是初创，或者团队独享分布式集群，说白了就是“家里有矿”。这个时候，你可以让计算资源去迁就数据，也就是，先定数据相关的，比如并行度相关参数，然后，再去定Executors相关的并发度、内存大小，等等。<br><br>总之，case by case，核心就是谁先谁后，不同的场景，结合实际情况，由你来决定谁优先调整，谁是被动地跟着调整。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620899475,"ip_address":"","comment_id":292476,"utype":1}],"discussion_count":1,"race_medal":0,"score":"57455409108","product_id":100073401,"comment_content":"老师，我发现14讲中，计算合理的并行度，依赖之一是在执行内存大小给定的前提下。而15讲中，计算执行内存大小，依赖之一是在并行度给定的前提下。所以到底如何破局呢？","like_count":14,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519844,"discussion_content":"好问题，是这样的。\n\n其实这里的关键因素，是谁先谁后的问题，一旦确定了哪个为先，我们就可以参考14讲“三足鼎立”的思路，去调整相关配置项。我们可以分类讨论。\n\n比如在大厂，很多时候，硬件资源是受限的，因此你能拿到多少计算资源，其实是有quota的，尤其是在大厂多个团队共享一个超大集群的情况下。这个时候，你就得让数据去迁就计算资源，那这就意味着，Executors相关的参数，得先定，然后再去倒推并行度相关的参数。\n\n相反，如果你是初创，或者团队独享分布式集群，说白了就是“家里有矿”。这个时候，你可以让计算资源去迁就数据，也就是，先定数据相关的，比如并行度相关参数，然后，再去定Executors相关的并发度、内存大小，等等。\n\n总之，case by case，核心就是谁先谁后，不同的场景，结合实际情况，由你来决定谁优先调整，谁是被动地跟着调整。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620899475,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288995,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1618814230,"is_pvip":false,"replies":[{"id":"104941","content":"有不少类库可以“拿来即用”，比如Java类库：Instrumentation，或者第三方库，比如RamUsageEstimator，可以多搜一搜，这样的工具还是蛮多的~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618917150,"ip_address":"","comment_id":288995,"utype":1}],"discussion_count":1,"race_medal":0,"score":"53158421782","product_id":100073401,"comment_content":"REPL 中，通过 Java 的常规方法估算数据存储大小<br>老师，这个过程具体是怎么做呢。","like_count":13,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518788,"discussion_content":"有不少类库可以“拿来即用”，比如Java类库：Instrumentation，或者第三方库，比如RamUsageEstimator，可以多搜一搜，这样的工具还是蛮多的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618917150,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291243,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620138283,"is_pvip":false,"replies":[{"id":"105748","content":"标准答案了，💯。你说的这个方法很有参考价值，虽然是粗略估算，但对于大多数情况来说，其实八九不离十了~ <br><br>非常赞，看上去“粗略”，但很实用~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620658922,"ip_address":"","comment_id":291243,"utype":1}],"discussion_count":1,"race_medal":0,"score":"48864778539","product_id":100073401,"comment_content":"老师讲的预估内存占用非常细，但就像老师给出的第二题中说的那样，如果Spark应用程序中的计算逻辑很多，这样预估自然是很精确，但是会花费大量时间，成本巨大！分享一下自己平时粗略估算内存占用的方法（如有不对，还望老师纠正）：<br>1、Storage Memory估算：将要缓存到内存的RDD&#47;Dataset&#47;Dataframe或广播变量进行cache，然后在Spark WEBUI的Storage标签页下直接查看所有的内存占用，大致就对应Storage Memory。<br><br>2、Execution Memory估算：有了Storage Memory，因为默认情况下Execution Memory和Storage Memory占用Spark Memory的比例是相同的，这里可以将Execution Memory和Storage Memory设置为相同。<br><br>3、User Memory：如果应用中没有使用太多自定义数据类型，保持默认值即可；如果使用了很多自定义数据类型，按老师说的方式进行估算即可。<br><br>上面只是一个粗略的估算，可能需要根据任务的执行情况进行一些调整。","like_count":12,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519411,"discussion_content":"标准答案了，💯。你说的这个方法很有参考价值，虽然是粗略估算，但对于大多数情况来说，其实八九不离十了~ \n\n非常赞，看上去“粗略”，但很实用~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620658922,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292040,"user_name":"licl1008","can_delete":false,"product_type":"c1","uid":1265135,"ip_address":"","ucode":"6467D6DB97A00A","user_header":"","comment_is_top":false,"comment_ctime":1620654008,"is_pvip":false,"replies":[{"id":"105878","content":"咱们先说“内存膨胀系数”，我们可以根据Spark UI，来估算磁盘上的数据，load到内存之后，会膨胀多少倍。具体怎么做呢？<br><br>打开Spark UI，找到某一个Stage，任何一个都可以，然后进入Description页面，里面会有一些详细的metrics，其中有两个非常的重要，就是我们拿来粗略计算“内存膨胀系数”用的。也就是Shuffle spill (memory) 和Shuffle spill (disk)，我们说的“内存膨胀系数”，就是二者的商，也就是：<br><br>Memory Expansion Ratio = Shuffle spill (memory) &#47; Shuffle spill (disk)<br><br>原理其实特别简单，这两个metrics，针对的是同一份数据进行统计的，只不过一个是在内存中的大小，一个是在磁盘上的大小，因此两者的商，就是内存膨胀系数。<br><br>有了这个“内存膨胀系数”，结合你磁盘上的分布式数据集，以及哪些需要执行、哪些需要缓存，你就可以很方便的计算出，他们在内存中的大小，从而相应地去设置Storage Memory和Execution Memory。<br><br>当然，这是一种粗略的办法，不过虽然粗略，但是很高效。拿到这个系数之后，对于数据在内存中的占用，估算起来就很方便了。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620813468,"ip_address":"","comment_id":292040,"utype":1}],"discussion_count":4,"race_medal":0,"score":"44570326968","product_id":100073401,"comment_content":"老师 课后思考中提到内存规划第一步很麻烦 您在留言回复中提到可以根据sparkUI估算&#39;数据放大倍数&#39;，然后粗略估算内存。请问具体是如何操作？ 根据您这个方法，可以同时得出 user execution storage三个空间的内存大小？ 谢谢指导","like_count":11,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519648,"discussion_content":"咱们先说“内存膨胀系数”，我们可以根据Spark UI，来估算磁盘上的数据，load到内存之后，会膨胀多少倍。具体怎么做呢？\n\n打开Spark UI，找到某一个Stage，任何一个都可以，然后进入Description页面，里面会有一些详细的metrics，其中有两个非常的重要，就是我们拿来粗略计算“内存膨胀系数”用的。也就是Shuffle spill (memory) 和Shuffle spill (disk)，我们说的“内存膨胀系数”，就是二者的商，也就是：\n\nMemory Expansion Ratio = Shuffle spill (memory) / Shuffle spill (disk)\n\n原理其实特别简单，这两个metrics，针对的是同一份数据进行统计的，只不过一个是在内存中的大小，一个是在磁盘上的大小，因此两者的商，就是内存膨胀系数。\n\n有了这个“内存膨胀系数”，结合你磁盘上的分布式数据集，以及哪些需要执行、哪些需要缓存，你就可以很方便的计算出，他们在内存中的大小，从而相应地去设置Storage Memory和Execution Memory。\n\n当然，这是一种粗略的办法，不过虽然粗略，但是很高效。拿到这个系数之后，对于数据在内存中的占用，估算起来就很方便了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620813468,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1265135,"avatar":"","nickname":"licl1008","note":"","ucode":"6467D6DB97A00A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":374972,"discussion_content":"我发现我ui也没有这两项 原来不止我一个😂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621424981,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2348970,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKu91INK055zppBLDjVaNyNaCJZGqKYaZzsy24HD3gic7iaW6Xj7FWHAZ947jAiaYibfJuMU7zEDKeJibg/132","nickname":"Geek_85ee81","note":"","ucode":"04251E1FE43384","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1265135,"avatar":"","nickname":"licl1008","note":"","ucode":"6467D6DB97A00A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":543602,"discussion_content":"我也是","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641221131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":374972,"ip_address":""},"score":543602,"extra":""}]},{"author":{"id":1357603,"avatar":"https://static001.geekbang.org/account/avatar/00/14/b7/23/ea83d6eb.jpg","nickname":"-.-","note":"","ucode":"3CDC606BDFDD80","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":374544,"discussion_content":"老师，想请教一下，我之前理解shuffle的时候只有memory不够的时候才会溢写到磁盘，所以Shuffle spill (memory) 和Shuffle spill (disk)应该不是同一份数据吧？而且我这边线上任务都有shuffle过程但是spark ui 里面Shuffle spill (memory) 和Shuffle spill (disk)都没有。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621239671,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288886,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1618751316,"is_pvip":false,"replies":[{"id":"104879","content":"好问题，确实，并发度、并行度、执行内存，三者“三足鼎立”。并发度和执行内存，都是一开始就设置好了，在整个作业的执行过程中，都是一样的，不会改变。但是，并行度不是，并行度可以随着作业的进展，随时调整。<br><br>所以，在不同的Stages内，为了维持3者的平衡，我们可以通过调整并行度，来维持“三足鼎立”的平衡。那么问题来，在不同的Stages，咋调整并行度呢？<br><br>有两种办法：<br>1. spark.sql.shuffle.partitions<br>通过spark.conf.set(spark.sql.shuffle.partitions, XXX)来控制每个Stages的并行度大小。这么做有个缺点，就是当你Stages比较多得时候，这个配置项频繁地改来改去，应用的维护成本会很高。<br><br>2. AQE的自动分区合并，也就是让Spark自动帮你算合适的并行度。但是这个需要相关的配置项，比如你期望的每个分片大小，等等。配置项的具体细节可以参考“配置项”的第二讲哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618816742,"ip_address":"","comment_id":288886,"utype":1}],"discussion_count":2,"race_medal":0,"score":"18798620500","product_id":100073401,"comment_content":"老师，我有一个疑惑，对于有多个stage的任务，每个stage的内存预估的情况可能不一样。那样就无法给一个比较适合所有stage得内存配置？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518762,"discussion_content":"好问题，确实，并发度、并行度、执行内存，三者“三足鼎立”。并发度和执行内存，都是一开始就设置好了，在整个作业的执行过程中，都是一样的，不会改变。但是，并行度不是，并行度可以随着作业的进展，随时调整。\n\n所以，在不同的Stages内，为了维持3者的平衡，我们可以通过调整并行度，来维持“三足鼎立”的平衡。那么问题来，在不同的Stages，咋调整并行度呢？\n\n有两种办法：\n1. spark.sql.shuffle.partitions\n通过spark.conf.set(spark.sql.shuffle.partitions, XXX)来控制每个Stages的并行度大小。这么做有个缺点，就是当你Stages比较多得时候，这个配置项频繁地改来改去，应用的维护成本会很高。\n\n2. AQE的自动分区合并，也就是让Spark自动帮你算合适的并行度。但是这个需要相关的配置项，比如你期望的每个分片大小，等等。配置项的具体细节可以参考“配置项”的第二讲哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618816742,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388822,"discussion_content":"第一种做法实际操作起来太麻烦，如果是一个复杂的有很多混洗算子的大sql，你们怎么去分别设置并行度？第二种做法是可以利用spark3.0的优秀机智，但是现在很多公司还是用2的版本，毕竟3.0刚刚发布一年不到，老师的课程看下来，总体感觉质量高，对原理摸得比较透，但是实际调优措施有些理想化，是不是真正在线上生产环境用过？","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1629000258,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293613,"user_name":"licl1008","can_delete":false,"product_type":"c1","uid":1265135,"ip_address":"","ucode":"6467D6DB97A00A","user_header":"","comment_is_top":false,"comment_ctime":1621465482,"is_pvip":false,"replies":[{"id":"106723","content":"对，你说的没错，执行内存所消耗的，就是PartitionedPairBuffer、PartitionedAppendOnlyMap这些数据结构，以及排序等操作所必需的内存空间。对于Execution Memory的计算，其实咱们说的一直都是估算，因为我们没有办法做精确地计算，因为你比如像刚刚说的那些数据结构，实际上因为有Spill的机制来做保护，因此并不是数据分片大于这些数据结构就会立即OOM。<br><br>咱们本讲推荐的计算方法，实际上是一种保守的计算不同内存区域的估算方法，目的是让不同区域分配相对均衡，避免潜在的OOM隐患。给出的估算公式：#Execution = #threads * #dataset &#47; #N；更多地是从平衡和稳定性的角度出发，因为在内存区域与数据集大小配比均衡的情况下，性能往往不会太差~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621780999,"ip_address":"","comment_id":293613,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10211400074","product_id":100073401,"comment_content":"老师 我是接着昨天的提问再问一下😂  execution mem内存估算中，#dataset不用算#bc过的数据集。但我的疑惑是：#bc只是把文件系统的数据搬到了内存，同样只是一个Relation而已，和执行要的内存似乎没什么联系。execution时，比如一些aggregate操作，不是同样也要把这个#bc relation的数据读出来，然后驻留在内存中处理吗。那岂不是也是要算入执行内存？或者我没懂算执行内存的理论依据到底是什么？我理解的执行内存是做aggregate sort等操作需要的内存空间，也是就pairedbuffer这之类的结构的大小   谢谢老师不吝赐教😄","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520290,"discussion_content":"对，你说的没错，执行内存所消耗的，就是PartitionedPairBuffer、PartitionedAppendOnlyMap这些数据结构，以及排序等操作所必需的内存空间。对于Execution Memory的计算，其实咱们说的一直都是估算，因为我们没有办法做精确地计算，因为你比如像刚刚说的那些数据结构，实际上因为有Spill的机制来做保护，因此并不是数据分片大于这些数据结构就会立即OOM。\n\n咱们本讲推荐的计算方法，实际上是一种保守的计算不同内存区域的估算方法，目的是让不同区域分配相对均衡，避免潜在的OOM隐患。给出的估算公式：#Execution = #threads * #dataset / #N；更多地是从平衡和稳定性的角度出发，因为在内存区域与数据集大小配比均衡的情况下，性能往往不会太差~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621780999,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288557,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1618537628,"is_pvip":false,"replies":[{"id":"104751","content":"对，如果没有自定义的数据结构，可以把spark.memory.fraction调高，多给Spark的执行任务和缓存任务分配些内存空间。<br><br>毕竟，没有自定义结构，User memory留着也是浪费。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618566672,"ip_address":"","comment_id":288557,"utype":1}],"discussion_count":8,"race_medal":0,"score":"10208472220","product_id":100073401,"comment_content":"老师，结合这一节内容和之前的，自定义数据结构其实和hive中的表关系不大，不涉及自定义，那么这时候是不是就可以把存储自定义这部分的内存匀出来给到统一内存身上","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518660,"discussion_content":"对，如果没有自定义的数据结构，可以把spark.memory.fraction调高，多给Spark的执行任务和缓存任务分配些内存空间。\n\n毕竟，没有自定义结构，User memory留着也是浪费。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618566672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368718,"discussion_content":"我的第一反应也是这样，在hive场景下是不是就没有User Memory的使用场景","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618816571,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369106,"discussion_content":"对，纯Hive SQL，都是直接处理分布式数据集，不需要自定义数据结构来辅助，因此基本不需要User Memory","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1618929841,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":368718,"ip_address":""},"score":369106,"extra":""},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":390964,"discussion_content":"同理,纯spark sql也不需要user memory","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630202856,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":369106,"ip_address":""},"score":390964,"extra":""}]},{"author":{"id":1168504,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","nickname":"斯盖丸","note":"","ucode":"B881D14B028F14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368257,"discussion_content":"spark.memory.fraction最多可以多少呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618634098,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1168504,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","nickname":"斯盖丸","note":"","ucode":"B881D14B028F14","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369107,"discussion_content":"100%其实也是OK的，不过通常不推荐这么干，除非你完全确定没有任何自定义的数据结构，否则的话，最好还是留些富余，满招损嘛~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1618929917,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":368257,"ip_address":""},"score":369107,"extra":""},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":388823,"discussion_content":"有个疑问，既然spark是采用软限制来针对各个区域，那么实际运行时，执行内存需要更多的内存，但是用户内存并没有用到，那么执行内存是可以自动抢占的吧，因为软限制根本限制不住，这样就等于无意中使用了百分百或者百分之九十，而并不需要特意去调整这个百分比配置项","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629000729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":369107,"ip_address":""},"score":388823,"extra":""},{"author":{"id":1789481,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg","nickname":"静心","note":"","ucode":"B80DE4B5C923D3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":407655,"discussion_content":"+1","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635081129,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":388823,"ip_address":""},"score":407655,"extra":""}]}]},{"had_liked":false,"id":326238,"user_name":"Geek_eb29a4","can_delete":false,"product_type":"c1","uid":2855459,"ip_address":"","ucode":"CF88D4ED5F4A25","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","comment_is_top":false,"comment_ctime":1639447881,"is_pvip":false,"replies":[{"id":"118718","content":"回复在这里哈：<br>咱们先来说函数，在Scala里面，函数本身其实就是对象。<br>至于高阶函数，它指的是这样的函数：<br>1）参数本身是函数<br>2）返回结果的类型，是函数<br>在咱们的例子里面，findIndex(filePath)的返回结果，是一个函数，所以说，findIndex本身是高阶函数。<br>而val partFunc = findIndex(filePath)，那么partFunc这个函数对象，它本身包含了已经构建好的字典，在调用partFunc(&quot;体育-篮球-NBA-湖人&quot;)的话，这个函数partFunc就可以根据String，来返回字符串了~ 不需要每个Executors去再次扫描文件、构建字典","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639669091,"ip_address":"","comment_id":326238,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5934415177","product_id":100073401,"comment_content":"利用高阶函数，我们就避免了让 Executor 中的每一个 Task 去读取模板文件，以及从头构建 Map 字典这种执行低效的做法。<br><br>这个原理 有什么参考资料可以看一下？<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539346,"discussion_content":"回复在这里哈：\n咱们先来说函数，在Scala里面，函数本身其实就是对象。\n至于高阶函数，它指的是这样的函数：\n1）参数本身是函数\n2）返回结果的类型，是函数\n在咱们的例子里面，findIndex(filePath)的返回结果，是一个函数，所以说，findIndex本身是高阶函数。\n而val partFunc = findIndex(filePath)，那么partFunc这个函数对象，它本身包含了已经构建好的字典，在调用partFunc(&#34;体育-篮球-NBA-湖人&#34;)的话，这个函数partFunc就可以根据String，来返回字符串了~ 不需要每个Executors去再次扫描文件、构建字典","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639669092,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":305969,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1628251233,"is_pvip":false,"replies":[{"id":"111174","content":"不不，数据的处理是以Task&#47;Partition为粒度的，所以这个数据结构的计算，应该乘以Task个数，而不是数据记录的个数哈~ 以Task粒度去计算哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1628831911,"ip_address":"","comment_id":305969,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5923218529","product_id":100073401,"comment_content":"老师，比如一个filter算子里的自定义函数我用了一个自定义数据结构，但是filter算子是RDD里N条都要去执行的，那么这个自定义数据结构将会存在N份，直到GC回收为止。这里这个内存占用大小就很难估算了，算他是1份的占用明显不合理，但是我又不知道有多少条数据会调用，这时如何估算比较好呢？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524577,"discussion_content":"不不，数据的处理是以Task/Partition为粒度的，所以这个数据结构的计算，应该乘以Task个数，而不是数据记录的个数哈~ 以Task粒度去计算哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628831911,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":332400,"user_name":"Geek_1564ee","can_delete":false,"product_type":"c1","uid":2728791,"ip_address":"","ucode":"E5388E16BBDC8A","user_header":"","comment_is_top":false,"comment_ctime":1643205061,"is_pvip":false,"replies":[{"id":"121495","content":"好问题~<br><br>读写缓冲区消耗的是Execution memory；另外，spark.shuffle.file.buffer和spark.reducer.maxSizeInFlight这两个参数，可以说是task粒度的，它指的是，每个文件、每个reducer task，可以拿来缓冲数据的内存大小","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1643355478,"ip_address":"","comment_id":332400,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1643205061","product_id":100073401,"comment_content":"老师，你好，想问下shuffle map阶段和reduce阶段写入和读取缓冲区是消耗的storage memory内存区域么？另外spark.shuffle.file.buffer和spark.reducer.maxSizeInFlight是以task为粒度的么？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":548746,"discussion_content":"好问题~\n\n读写缓冲区消耗的是Execution memory；另外，spark.shuffle.file.buffer和spark.reducer.maxSizeInFlight这两个参数，可以说是task粒度的，它指的是，每个文件、每个reducer task，可以拿来缓冲数据的内存大小","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1643355478,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":326245,"user_name":"Geek_eb29a4","can_delete":false,"product_type":"c1","uid":2855459,"ip_address":"","ucode":"CF88D4ED5F4A25","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","comment_is_top":false,"comment_ctime":1639449402,"is_pvip":false,"replies":[{"id":"118719","content":"在上一个问题回复了哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639669212,"ip_address":"","comment_id":326245,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639449402","product_id":100073401,"comment_content":"<br>利用高阶函数，我们就避免了让 Executor 中的每一个 Task 去读取模板文件，以及从头构建 Map 字典这种执行低效的做法。<br><br>麻烦吴老师结合这个网址，讲解下 高阶匿名函数的优化点吗？<br>我找了很多资料，大概这个有点对得上 https:&#47;&#47;www.jianshu.com&#47;p&#47;0a3150afb7ed<br><br><br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539347,"discussion_content":"在上一个问题回复了哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639669212,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":302745,"user_name":"Geek_0419b7","can_delete":false,"product_type":"c1","uid":2627327,"ip_address":"","ucode":"FC816BBBC81A74","user_header":"","comment_is_top":false,"comment_ctime":1626351667,"is_pvip":false,"replies":[{"id":"109764","content":"这部分属于User Memory~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1626691681,"ip_address":"","comment_id":302745,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1626351667","product_id":100073401,"comment_content":"吴老师您好！在Spark算子中创建的对象属于哪一部分的内存呢？就比如说在map函数里创建的对象。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523407,"discussion_content":"这部分属于User Memory~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626691681,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":299681,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1624791844,"is_pvip":false,"replies":[{"id":"108790","content":"这部分咱们在07讲：内存管理有过介绍哈，可以回顾下黄小乙和张麻子的故事~ 你说的没错，Storage和Execution之间确实会相互抢占，不过配置项还是要稍微设置一下的，因为两者抢占的优先级是不一样的~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624954920,"ip_address":"","comment_id":299681,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1624791844","product_id":100073401,"comment_content":"新一点版本的spark 对应Storage和Execution的内存是不是可以相互抢占的。这样的话，还需要特殊配置两个内存的配置占比么","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522505,"discussion_content":"这部分咱们在07讲：内存管理有过介绍哈，可以回顾下黄小乙和张麻子的故事~ 你说的没错，Storage和Execution之间确实会相互抢占，不过配置项还是要稍微设置一下的，因为两者抢占的优先级是不一样的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624954920,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293540,"user_name":"licl1008","can_delete":false,"product_type":"c1","uid":1265135,"ip_address":"","ucode":"6467D6DB97A00A","user_header":"","comment_is_top":false,"comment_ctime":1621424629,"is_pvip":false,"replies":[{"id":"106322","content":"好问题，不算#bc，#bc是要记入到Storage memory的，这部分内存区域的归属和划分，在后面内存的三讲会详细展开哈~ 可以重点关注下~<br><br>你说得对，只需要关注“没有被广播的数据集”就好~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621434138,"ip_address":"","comment_id":293540,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1621424629","product_id":100073401,"comment_content":"老师  算execution mem时，比如两个数据集join的场景, 有一个小数据集broadcast了, 那#dataset要包括#bc吗？ 还是只用算那个没有被广播的数据集的大小？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520260,"discussion_content":"好问题，不算#bc，#bc是要记入到Storage memory的，这部分内存区域的归属和划分，在后面内存的三讲会详细展开哈~ 可以重点关注下~\n\n你说得对，只需要关注“没有被广播的数据集”就好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621434138,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291661,"user_name":"Geek_81beba","can_delete":false,"product_type":"c1","uid":2540707,"ip_address":"","ucode":"FCED45C338D00F","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJt9RvLXn5KxqNiccCyxRGy0IDHdqOOiazoH7aqku4GlELB4guibOGibEqPF740iaNwKoe6BjicgmgjR6Vw/132","comment_is_top":false,"comment_ctime":1620402275,"is_pvip":false,"replies":[{"id":"105666","content":"好问题，Hive的好处是，你把作业丢给他，不用关心每个Executors的资源设置，Hive根据工作负载，自动帮你搞定。<br><br>但是Spark的开发者，就需要考虑Executors数量、每个Executors的cpu、memory设置，从而平衡计算资源。<br><br>回答你的问题：我觉得有两个方向吧。<br><br>一个云原生的Spark，比如Databricks提供的“Spark As A Service”，你只需要选择机型、以及集群规模，根本就不用关心Executors数量、CPU、Memory资源配置这些细节问题。再比如Spark Over Kubernetes，也能够做到以Pod为粒度自动缩放，虽然也需要一些设置，但是集群资源的灵活性是非常高的。<br><br>另一个方向，仅仅是探索，就是用Machine Learning来预判作业所需的硬件资源，然后进行相应的设置。这个方向其实还是很有前途的，不过难点在于有效样本的积累，只有样本量足够大，Machine Learning学出来的Model才有实用价值。<br><br>有的没的扯了很多，不知道能否解答你的疑问~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620470788,"ip_address":"","comment_id":291661,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620402275","product_id":100073401,"comment_content":"如何配置spark的参数选项，让他像hive一样自动划分内存和cpu然后稳定的运行呢","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519537,"discussion_content":"好问题，Hive的好处是，你把作业丢给他，不用关心每个Executors的资源设置，Hive根据工作负载，自动帮你搞定。\n\n但是Spark的开发者，就需要考虑Executors数量、每个Executors的cpu、memory设置，从而平衡计算资源。\n\n回答你的问题：我觉得有两个方向吧。\n\n一个云原生的Spark，比如Databricks提供的“Spark As A Service”，你只需要选择机型、以及集群规模，根本就不用关心Executors数量、CPU、Memory资源配置这些细节问题。再比如Spark Over Kubernetes，也能够做到以Pod为粒度自动缩放，虽然也需要一些设置，但是集群资源的灵活性是非常高的。\n\n另一个方向，仅仅是探索，就是用Machine Learning来预判作业所需的硬件资源，然后进行相应的设置。这个方向其实还是很有前途的，不过难点在于有效样本的积累，只有样本量足够大，Machine Learning学出来的Model才有实用价值。\n\n有的没的扯了很多，不知道能否解答你的疑问~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620470788,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288655,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1618582121,"is_pvip":false,"replies":[{"id":"104780","content":"哈哈，Scala玩得那么溜，修改源码无障碍，咋会不懂Java、JVM，你咋做到的？教教我，哈哈~<br><br>第二个不太对哈，首先和API没什么关系，不论是SQL、DF还是DS，都需要按照比例划分不同的内存区域。<br><br>再者，不能按照代码行数去估算内存消耗，这个不科学，主要是两者之间没有必然联系，用统计的话说，两者不是正相关的。<br><br>不过，3个区域各自占1&#47;3，但是不失为一个不错的初始值，不过这也仅仅是个开始，only a start point。还是要结合作业对于内存的消耗占比，相应地调整不同区域的空间大小。<br><br>提示：可以通过Spark UI的一些指标，来粗略地估计“数据内存放大倍数”，然后再去预估不同空间的消耗占比。（好吧，我又开始挖坑了，Spark UI后面一定找机会直播一把，不过不妨先熟悉、了解下Spark UI，先有个整体的认知）<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618639113,"ip_address":"","comment_id":288655,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1618582121","product_id":100073401,"comment_content":"1.C&#47;C++党，阅读修改spark源码无障碍，虽然有些scala高级语法叫不上名，不影响理解源码。就是不懂java。。😅<br>2.我觉得应该根据使用场景直接预估。比如如果仅仅使用sql，那么fraction和storageFraction尽量调小就行;如果是使用DataSet API较多，省略保留内存情况下，#user,#storage,#execution各占1&#47;3，然后根据代码中自定义数据结构和cache方法在代码总行数中出现的频率，对原本1&#47;3做微调。<br>不知道对不对，磊哥看下。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518695,"discussion_content":"哈哈，Scala玩得那么溜，修改源码无障碍，咋会不懂Java、JVM，你咋做到的？教教我，哈哈~\n\n第二个不太对哈，首先和API没什么关系，不论是SQL、DF还是DS，都需要按照比例划分不同的内存区域。\n\n再者，不能按照代码行数去估算内存消耗，这个不科学，主要是两者之间没有必然联系，用统计的话说，两者不是正相关的。\n\n不过，3个区域各自占1/3，但是不失为一个不错的初始值，不过这也仅仅是个开始，only a start point。还是要结合作业对于内存的消耗占比，相应地调整不同区域的空间大小。\n\n提示：可以通过Spark UI的一些指标，来粗略地估计“数据内存放大倍数”，然后再去预估不同空间的消耗占比。（好吧，我又开始挖坑了，Spark UI后面一定找机会直播一把，不过不妨先熟悉、了解下Spark UI，先有个整体的认知）\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618639113,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388827,"discussion_content":"我觉得层主的意思是用sql时，不会有cache的操作，那么存储百分比可以调小，这个没什么问题，但是spark1.6后是采用抢占的方式，调不调小，问题不大吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629001706,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}