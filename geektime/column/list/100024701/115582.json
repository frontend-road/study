{"id":115582,"title":"58 | cgroup技术：内部创业公司应该独立核算成本","content":"<p>我们前面说了，容器实现封闭的环境主要靠两种技术，一种是“看起来是隔离”的技术Namespace，另一种是用起来是隔离的技术cgroup。</p><p>上一节我们讲了“看起来隔离“的技术Namespace，这一节我们就来看一下“用起来隔离“的技术cgroup。</p><p>cgroup全称是control group，顾名思义，它是用来做“控制”的。控制什么东西呢？当然是资源的使用了。那它都能控制哪些资源的使用呢？我们一起来看一看。</p><p>首先，cgroup定义了下面的一系列子系统，每个子系统用于控制某一类资源。</p><ul>\n<li>CPU子系统，主要限制进程的CPU使用率。</li>\n<li>cpuacct 子系统，可以统计 cgroup 中的进程的 CPU 使用报告。</li>\n<li>cpuset 子系统，可以为 cgroup 中的进程分配单独的 CPU 节点或者内存节点。</li>\n<li>memory 子系统，可以限制进程的 Memory 使用量。</li>\n<li>blkio 子系统，可以限制进程的块设备 IO。</li>\n<li>devices 子系统，可以控制进程能够访问某些设备。</li>\n<li>net_cls 子系统，可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块（traffic control）对数据包进行控制。</li>\n<li>freezer 子系统，可以挂起或者恢复 cgroup 中的进程。</li>\n</ul><!-- [[[read_end]]] --><p>这么多子系统，你可能要说了，那我们不用都掌握吧？没错，这里面最常用的是对于CPU和内存的控制，所以下面我们详细来说它。</p><p>在容器这一章的第一节，我们讲了，Docker有一些参数能够限制CPU和内存的使用，如果把它落地到cgroup里面会如何限制呢？</p><p>为了验证Docker的参数与cgroup的映射关系，我们运行一个命令特殊的docker run命令，这个命令比较长，里面的参数都会映射为cgroup的某项配置，然后我们运行docker ps，可以看到，这个容器的id为3dc0601189dd。</p><pre><code>docker run -d --cpu-shares 513 --cpus 2 --cpuset-cpus 1,3 --memory 1024M --memory-swap 1234M --memory-swappiness 7 -p 8081:80 testnginx:1\n\n# docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                  NAMES\n3dc0601189dd        testnginx:1         &quot;/bin/sh -c 'nginx -…&quot;   About a minute ago   Up About a minute   0.0.0.0:8081-&gt;80/tcp   boring_cohen\n</code></pre><p>在Linux上，为了操作cgroup，有一个专门的cgroup文件系统，我们运行mount命令可以查看。</p><pre><code># mount -t cgroup\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)\ncgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)\ncgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\ncgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)\n</code></pre><p>cgroup文件系统多挂载到/sys/fs/cgroup下，通过上面的命令行，我们可以看到我们可以用cgroup控制哪些资源。</p><p>对于CPU的控制，我在这一章的第一节讲过，Docker可以控制cpu-shares、cpus和cpuset。</p><p>我们在/sys/fs/cgroup/下面能看到下面的目录结构。</p><pre><code>drwxr-xr-x 5 root root  0 May 30 17:00 blkio\nlrwxrwxrwx 1 root root 11 May 30 17:00 cpu -&gt; cpu,cpuacct\nlrwxrwxrwx 1 root root 11 May 30 17:00 cpuacct -&gt; cpu,cpuacct\ndrwxr-xr-x 5 root root  0 May 30 17:00 cpu,cpuacct\ndrwxr-xr-x 3 root root  0 May 30 17:00 cpuset\ndrwxr-xr-x 5 root root  0 May 30 17:00 devices\ndrwxr-xr-x 3 root root  0 May 30 17:00 freezer\ndrwxr-xr-x 3 root root  0 May 30 17:00 hugetlb\ndrwxr-xr-x 5 root root  0 May 30 17:00 memory\nlrwxrwxrwx 1 root root 16 May 30 17:00 net_cls -&gt; net_cls,net_prio\ndrwxr-xr-x 3 root root  0 May 30 17:00 net_cls,net_prio\nlrwxrwxrwx 1 root root 16 May 30 17:00 net_prio -&gt; net_cls,net_prio\ndrwxr-xr-x 3 root root  0 May 30 17:00 perf_event\ndrwxr-xr-x 5 root root  0 May 30 17:00 pids\ndrwxr-xr-x 5 root root  0 May 30 17:00 systemd\n</code></pre><p>我们可以想象，CPU的资源控制的配置文件，应该在cpu,cpuacct这个文件夹下面。</p><pre><code># ls\ncgroup.clone_children  cpu.cfs_period_us  notify_on_release\ncgroup.event_control   cpu.cfs_quota_us   release_agent\ncgroup.procs           cpu.rt_period_us   system.slice\ncgroup.sane_behavior   cpu.rt_runtime_us  tasks\ncpuacct.stat           cpu.shares         user.slice\ncpuacct.usage          cpu.stat\ncpuacct.usage_percpu   docker\n</code></pre><p>果真，这下面是对CPU的相关控制，里面还有一个路径叫docker。我们进入这个路径。</p><pre><code>]# ls\ncgroup.clone_children\ncgroup.event_control\ncgroup.procs\ncpuacct.stat\ncpuacct.usage\ncpuacct.usage_percpu\ncpu.cfs_period_us\ncpu.cfs_quota_us\ncpu.rt_period_us\ncpu.rt_runtime_us\ncpu.shares\ncpu.stat\n3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd\nnotify_on_release\ntasks\n</code></pre><p>这里面有个很长的id，是我们创建的docker的id。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# ls\ncgroup.clone_children  cpuacct.usage_percpu  cpu.shares\ncgroup.event_control   cpu.cfs_period_us     cpu.stat\ncgroup.procs           cpu.cfs_quota_us      notify_on_release\ncpuacct.stat           cpu.rt_period_us      tasks\ncpuacct.usage          cpu.rt_runtime_us\n</code></pre><p>在这里，我们能看到cpu.shares，还有一个重要的文件tasks。这里面是这个容器里所有进程的进程号，也即所有这些进程都被这些CPU策略控制。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat tasks \n39487\n39520\n39526\n39527\n39528\n39529\n</code></pre><p>如果我们查看cpu.shares，里面就是我们设置的513。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat cpu.shares\n513\n</code></pre><p>另外，我们还配置了cpus，这个值其实是由cpu.cfs_period_us和cpu.cfs_quota_us共同决定的。cpu.cfs_period_us是运行周期，cpu.cfs_quota_us是在周期内这些进程占用多少时间。我们设置了cpus为2，代表的意思是，在周期100000微秒的运行周期内，这些进程要占用200000微秒的时间，也即需要两个CPU同时运行一个整的周期。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat cpu.cfs_period_us\n100000\n[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat cpu.cfs_quota_us\n200000\n</code></pre><p>对于cpuset，也即CPU绑核的参数，在另外一个文件夹里面/sys/fs/cgroup/cpuset，这里面同样有一个docker文件夹，下面同样有docker id 也即3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd文件夹，这里面的cpuset.cpus就是配置的绑定到1、3两个核。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat cpuset.cpus\n1,3\n</code></pre><p>这一章的第一节我们还讲了Docker可以限制内存的使用量，例如memory、memory-swap、memory-swappiness。这些在哪里控制呢？</p><p>/sys/fs/cgroup/下面还有一个memory路径，控制策略就是在这里面定义的。</p><pre><code>[root@deployer memory]# ls\ncgroup.clone_children               memory.memsw.failcnt\ncgroup.event_control                memory.memsw.limit_in_bytes\ncgroup.procs                        memory.memsw.max_usage_in_bytes\ncgroup.sane_behavior                memory.memsw.usage_in_bytes\ndocker                              memory.move_charge_at_immigrate\nmemory.failcnt                      memory.numa_stat\nmemory.force_empty                  memory.oom_control\nmemory.kmem.failcnt                 memory.pressure_level\nmemory.kmem.limit_in_bytes          memory.soft_limit_in_bytes\nmemory.kmem.max_usage_in_bytes      memory.stat\nmemory.kmem.slabinfo                memory.swappiness\nmemory.kmem.tcp.failcnt             memory.usage_in_bytes\nmemory.kmem.tcp.limit_in_bytes      memory.use_hierarchy\nmemory.kmem.tcp.max_usage_in_bytes  notify_on_release\nmemory.kmem.tcp.usage_in_bytes      release_agent\nmemory.kmem.usage_in_bytes          system.slice\nmemory.limit_in_bytes               tasks\nmemory.max_usage_in_bytes           user.slice\n</code></pre><p>这里面全是对于memory的控制参数，在这里面我们可看到了docker，里面还有容器的id作为文件夹。</p><pre><code>[docker]# ls\n3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd\ncgroup.clone_children\ncgroup.event_control\ncgroup.procs\nmemory.failcnt\nmemory.force_empty\nmemory.kmem.failcnt\nmemory.kmem.limit_in_bytes\nmemory.kmem.max_usage_in_bytes\nmemory.kmem.slabinfo\nmemory.kmem.tcp.failcnt\nmemory.kmem.tcp.limit_in_bytes\nmemory.kmem.tcp.max_usage_in_bytes\nmemory.kmem.tcp.usage_in_bytes\nmemory.kmem.usage_in_bytes\nmemory.limit_in_bytes\nmemory.max_usage_in_bytes\nmemory.memsw.failcnt\nmemory.memsw.limit_in_bytes\nmemory.memsw.max_usage_in_bytes\nmemory.memsw.usage_in_bytes\nmemory.move_charge_at_immigrate\nmemory.numa_stat\nmemory.oom_control\nmemory.pressure_level\nmemory.soft_limit_in_bytes\nmemory.stat\nmemory.swappiness\nmemory.usage_in_bytes\nmemory.use_hierarchy\nnotify_on_release\ntasks\n\n[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# ls\ncgroup.clone_children               memory.memsw.failcnt\ncgroup.event_control                memory.memsw.limit_in_bytes\ncgroup.procs                        memory.memsw.max_usage_in_bytes\nmemory.failcnt                      memory.memsw.usage_in_bytes\nmemory.force_empty                  memory.move_charge_at_immigrate\nmemory.kmem.failcnt                 memory.numa_stat\nmemory.kmem.limit_in_bytes          memory.oom_control\nmemory.kmem.max_usage_in_bytes      memory.pressure_level\nmemory.kmem.slabinfo                memory.soft_limit_in_bytes\nmemory.kmem.tcp.failcnt             memory.stat\nmemory.kmem.tcp.limit_in_bytes      memory.swappiness\nmemory.kmem.tcp.max_usage_in_bytes  memory.usage_in_bytes\nmemory.kmem.tcp.usage_in_bytes      memory.use_hierarchy\nmemory.kmem.usage_in_bytes          notify_on_release\nmemory.limit_in_bytes               tasks\nmemory.max_usage_in_bytes\n</code></pre><p>在docker id的文件夹下面，有一个memory.limit_in_bytes，里面配置的就是memory。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat memory.limit_in_bytes\n1073741824\n</code></pre><p>还有memory.swappiness，里面配置的就是memory-swappiness。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat memory.swappiness\n7\n</code></pre><p>还有就是memory.memsw.limit_in_bytes，里面配置的是memory-swap。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat memory.memsw.limit_in_bytes\n1293942784\n</code></pre><p>我们还可以看一下tasks文件的内容，tasks里面是容器里面所有进程的进程号。</p><pre><code>[3dc0601189dd218898f31f9526a6cfae83913763a4da59f95ec789c6e030ecfd]# cat tasks \n39487\n39520\n39526\n39527\n39528\n39529\n</code></pre><p>至此，我们看到了cgroup对于Docker资源的控制，在用户态是如何表现的。我画了一张图总结一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/0f/1c762a6283429ff3587a7fc370fc090f.png?wh=3106*1366\" alt=\"\"></p><p>在内核中，cgroup是如何实现的呢？</p><p>首先，在系统初始化的时候，cgroup也会进行初始化，在start_kernel中，cgroup_init_early和cgroup_init都会进行初始化。</p><pre><code>asmlinkage __visible void __init start_kernel(void)\n{\n......\n  cgroup_init_early();\n......\n  cgroup_init();\n......\n}\n</code></pre><p>在cgroup_init_early和cgroup_init中，会有下面的循环。</p><pre><code>for_each_subsys(ss, i) {\n\tss-&gt;id = i;\n\tss-&gt;name = cgroup_subsys_name[i];\n......\n\tcgroup_init_subsys(ss, true);\n}\n\n#define for_each_subsys(ss, ssid)\t\t\t\t\t\\\n\tfor ((ssid) = 0; (ssid) &lt; CGROUP_SUBSYS_COUNT &amp;&amp;\t\t\\\n\t     (((ss) = cgroup_subsys[ssid]) || true); (ssid)++)\n</code></pre><p>for_each_subsys会在cgroup_subsys数组中进行循环。这个cgroup_subsys数组是如何形成的呢？</p><pre><code>#define SUBSYS(_x) [_x ## _cgrp_id] = &amp;_x ## _cgrp_subsys,\nstruct cgroup_subsys *cgroup_subsys[] = {\n#include &lt;linux/cgroup_subsys.h&gt;\n};\n#undef SUBSYS\n</code></pre><p>SUBSYS这个宏定义了这个cgroup_subsys数组，数组中的项定义在cgroup_subsys.h头文件中。例如，对于CPU和内存有下面的定义。</p><pre><code>//cgroup_subsys.h\n\n#if IS_ENABLED(CONFIG_CPUSETS)\nSUBSYS(cpuset)\n#endif\n\n#if IS_ENABLED(CONFIG_CGROUP_SCHED)\nSUBSYS(cpu)\n#endif\n\n#if IS_ENABLED(CONFIG_CGROUP_CPUACCT)\nSUBSYS(cpuacct)\n#endif\n\n#if IS_ENABLED(CONFIG_MEMCG)\nSUBSYS(memory)\n#endif\n</code></pre><p>根据SUBSYS的定义，SUBSYS(cpu)其实是[cpu_cgrp_id] = &amp;cpu_cgrp_subsys，而SUBSYS(memory)其实是[memory_cgrp_id] = &amp;memory_cgrp_subsys。</p><p>我们能够找到cpu_cgrp_subsys和memory_cgrp_subsys的定义。</p><pre><code>cpuset_cgrp_subsys\nstruct cgroup_subsys cpuset_cgrp_subsys = {\n\t.css_alloc\t= cpuset_css_alloc,\n\t.css_online\t= cpuset_css_online,\n\t.css_offline\t= cpuset_css_offline,\n\t.css_free\t= cpuset_css_free,\n\t.can_attach\t= cpuset_can_attach,\n\t.cancel_attach\t= cpuset_cancel_attach,\n\t.attach\t\t= cpuset_attach,\n\t.post_attach\t= cpuset_post_attach,\n\t.bind\t\t= cpuset_bind,\n\t.fork\t\t= cpuset_fork,\n\t.legacy_cftypes\t= files,\n\t.early_init\t= true,\n};\n\ncpu_cgrp_subsys\nstruct cgroup_subsys cpu_cgrp_subsys = {\n\t.css_alloc\t= cpu_cgroup_css_alloc,\n\t.css_online\t= cpu_cgroup_css_online,\n\t.css_released\t= cpu_cgroup_css_released,\n\t.css_free\t= cpu_cgroup_css_free,\n\t.fork\t\t= cpu_cgroup_fork,\n\t.can_attach\t= cpu_cgroup_can_attach,\n\t.attach\t\t= cpu_cgroup_attach,\n\t.legacy_cftypes\t= cpu_files,\n\t.early_init\t= true,\n};\n\nmemory_cgrp_subsys\nstruct cgroup_subsys memory_cgrp_subsys = {\n\t.css_alloc = mem_cgroup_css_alloc,\n\t.css_online = mem_cgroup_css_online,\n\t.css_offline = mem_cgroup_css_offline,\n\t.css_released = mem_cgroup_css_released,\n\t.css_free = mem_cgroup_css_free,\n\t.css_reset = mem_cgroup_css_reset,\n\t.can_attach = mem_cgroup_can_attach,\n\t.cancel_attach = mem_cgroup_cancel_attach,\n\t.post_attach = mem_cgroup_move_task,\n\t.bind = mem_cgroup_bind,\n\t.dfl_cftypes = memory_files,\n\t.legacy_cftypes = mem_cgroup_legacy_files,\n\t.early_init = 0,\n};\n</code></pre><p>在for_each_subsys的循环里面，cgroup_subsys[]数组中的每一个cgroup_subsys，都会调用cgroup_init_subsys，对于cgroup_subsys对于初始化。</p><pre><code>static void __init cgroup_init_subsys(struct cgroup_subsys *ss, bool early)\n{\n\tstruct cgroup_subsys_state *css;\n......\n\tidr_init(&amp;ss-&gt;css_idr);\n\tINIT_LIST_HEAD(&amp;ss-&gt;cfts);\n\n\t/* Create the root cgroup state for this subsystem */\n\tss-&gt;root = &amp;cgrp_dfl_root;\n\tcss = ss-&gt;css_alloc(cgroup_css(&amp;cgrp_dfl_root.cgrp, ss));\n......\n\tinit_and_link_css(css, ss, &amp;cgrp_dfl_root.cgrp);\n......\n\tcss-&gt;id = cgroup_idr_alloc(&amp;ss-&gt;css_idr, css, 1, 2, GFP_KERNEL);\n\tinit_css_set.subsys[ss-&gt;id] = css;\n......\n\tBUG_ON(online_css(css));\n......\n}\n</code></pre><p>cgroup_init_subsys里面会做两件事情，一个是调用cgroup_subsys的css_alloc函数创建一个cgroup_subsys_state；另外就是调用online_css，也即调用cgroup_subsys的css_online函数，激活这个cgroup。</p><p>对于CPU来讲，css_alloc函数就是cpu_cgroup_css_alloc。这里面会调用 sched_create_group创建一个struct task_group。在这个结构中，第一项就是cgroup_subsys_state，也就是说，task_group是cgroup_subsys_state的一个扩展，最终返回的是指向cgroup_subsys_state结构的指针，可以通过强制类型转换变为task_group。</p><pre><code>struct task_group {\n\tstruct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\t/* schedulable entities of this group on each cpu */\n\tstruct sched_entity **se;\n\t/* runqueue &quot;owned&quot; by this group on each cpu */\n\tstruct cfs_rq **cfs_rq;\n\tunsigned long shares;\n\n#ifdef\tCONFIG_SMP\n\tatomic_long_t load_avg ____cacheline_aligned;\n#endif\n#endif\n\n\tstruct rcu_head rcu;\n\tstruct list_head list;\n\n\tstruct task_group *parent;\n\tstruct list_head siblings;\n\tstruct list_head children;\n\n\tstruct cfs_bandwidth cfs_bandwidth;\n};\n</code></pre><p>在task_group结构中，有一个成员是sched_entity，前面我们讲进程调度的时候，遇到过它。它是调度的实体，也即这一个task_group也是一个调度实体。</p><p>接下来，online_css会被调用。对于CPU来讲，online_css调用的是cpu_cgroup_css_online。它会调用sched_online_group-&gt;online_fair_sched_group。</p><pre><code>void online_fair_sched_group(struct task_group *tg)\n{\n\tstruct sched_entity *se;\n\tstruct rq *rq;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\trq = cpu_rq(i);\n\t\tse = tg-&gt;se[i];\n\t\tupdate_rq_clock(rq);\n\t\tattach_entity_cfs_rq(se);\n\t\tsync_throttle(tg, i);\n\t}\n}\n</code></pre><p>在这里面，对于每一个CPU，取出每个CPU的运行队列rq，也取出task_group的sched_entity，然后通过attach_entity_cfs_rq将sched_entity添加到运行队列中。</p><p>对于内存来讲，css_alloc函数就是mem_cgroup_css_alloc。这里面会调用 mem_cgroup_alloc，创建一个struct mem_cgroup。在这个结构中，第一项就是cgroup_subsys_state，也就是说，mem_cgroup是cgroup_subsys_state的一个扩展，最终返回的是指向cgroup_subsys_state结构的指针，我们可以通过强制类型转换变为mem_cgroup。</p><pre><code>struct mem_cgroup {\n\tstruct cgroup_subsys_state css;\n\n\t/* Private memcg ID. Used to ID objects that outlive the cgroup */\n\tstruct mem_cgroup_id id;\n\n\t/* Accounted resources */\n\tstruct page_counter memory;\n\tstruct page_counter swap;\n\n\t/* Legacy consumer-oriented counters */\n\tstruct page_counter memsw;\n\tstruct page_counter kmem;\n\tstruct page_counter tcpmem;\n\n\t/* Normal memory consumption range */\n\tunsigned long low;\n\tunsigned long high;\n\n\t/* Range enforcement for interrupt charges */\n\tstruct work_struct high_work;\n\n\tunsigned long soft_limit;\n\n......\n\tint\tswappiness;\n......\n\t/*\n\t * percpu counter.\n\t */\n\tstruct mem_cgroup_stat_cpu __percpu *stat;\n\n\tint last_scanned_node;\n\n\t/* List of events which userspace want to receive */\n\tstruct list_head event_list;\n\tspinlock_t event_list_lock;\n\n\tstruct mem_cgroup_per_node *nodeinfo[0];\n\t/* WARNING: nodeinfo must be the last member here */\n};\n</code></pre><p>在cgroup_init函数中，cgroup的初始化还做了一件很重要的事情，它会调用cgroup_init_cftypes(NULL, cgroup1_base_files)，来初始化对于cgroup文件类型cftype的操作函数，也就是将struct kernfs_ops *kf_ops设置为cgroup_kf_ops。</p><pre><code>struct cftype cgroup1_base_files[] = {\n......\n    {   \n        .name = &quot;tasks&quot;,\n        .seq_start = cgroup_pidlist_start,\n        .seq_next = cgroup_pidlist_next,\n        .seq_stop = cgroup_pidlist_stop,\n        .seq_show = cgroup_pidlist_show,\n        .private = CGROUP_FILE_TASKS,\n        .write = cgroup_tasks_write,\n    },  \n}\n\nstatic struct kernfs_ops cgroup_kf_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.open\t\t\t= cgroup_file_open,\n\t.release\t\t= cgroup_file_release,\n\t.write\t\t\t= cgroup_file_write,\n\t.seq_start\t\t= cgroup_seqfile_start,\n\t.seq_next\t\t= cgroup_seqfile_next,\n\t.seq_stop\t\t= cgroup_seqfile_stop,\n\t.seq_show\t\t= cgroup_seqfile_show,\n};\n</code></pre><p>在cgroup初始化完毕之后，接下来就是创建一个cgroup的文件系统，用于配置和操作cgroup。</p><p>cgroup是一种特殊的文件系统。它的定义如下：</p><pre><code>struct file_system_type cgroup_fs_type = {\n\t.name = &quot;cgroup&quot;,\n\t.mount = cgroup_mount,\n\t.kill_sb = cgroup_kill_sb,\n\t.fs_flags = FS_USERNS_MOUNT,\n};\n</code></pre><p>当我们mount这个cgroup文件系统的时候，会调用cgroup_mount-&gt;cgroup1_mount。</p><pre><code>struct dentry *cgroup1_mount(struct file_system_type *fs_type, int flags,\n\t\t\t     void *data, unsigned long magic,\n\t\t\t     struct cgroup_namespace *ns)\n{\n\tstruct super_block *pinned_sb = NULL;\n\tstruct cgroup_sb_opts opts;\n\tstruct cgroup_root *root;\n\tstruct cgroup_subsys *ss;\n\tstruct dentry *dentry;\n\tint i, ret;\n\tbool new_root = false;\n......\n\troot = kzalloc(sizeof(*root), GFP_KERNEL);\n\tnew_root = true;\n\n\tinit_cgroup_root(root, &amp;opts);\n\n\tret = cgroup_setup_root(root, opts.subsys_mask, PERCPU_REF_INIT_DEAD);\n......\n\tdentry = cgroup_do_mount(&amp;cgroup_fs_type, flags, root,\n\t\t\t\t CGROUP_SUPER_MAGIC, ns);\n......\n\treturn dentry;\n}\n</code></pre><p>cgroup被组织成为树形结构，因而有cgroup_root。init_cgroup_root会初始化这个cgroup_root。cgroup_root是cgroup的根，它有一个成员kf_root，是cgroup文件系统的根struct kernfs_root。kernfs_create_root就是用来创建这个kernfs_root结构的。</p><pre><code>int cgroup_setup_root(struct cgroup_root *root, u16 ss_mask, int ref_flags)\n{\n\tLIST_HEAD(tmp_links);\n\tstruct cgroup *root_cgrp = &amp;root-&gt;cgrp;\n\tstruct kernfs_syscall_ops *kf_sops;\n\tstruct css_set *cset;\n\tint i, ret;\n\n\troot-&gt;kf_root = kernfs_create_root(kf_sops,\n\t\t\t\t\t   KERNFS_ROOT_CREATE_DEACTIVATED,\n\t\t\t\t\t   root_cgrp);\n\troot_cgrp-&gt;kn = root-&gt;kf_root-&gt;kn;\n\n\tret = css_populate_dir(&amp;root_cgrp-&gt;self);\n\tret = rebind_subsystems(root, ss_mask);\n......\n\tlist_add(&amp;root-&gt;root_list, &amp;cgroup_roots);\n\tcgroup_root_count++;\n......\n\tkernfs_activate(root_cgrp-&gt;kn);\n......\n}\n</code></pre><p>就像在普通文件系统上，每一个文件都对应一个inode，在cgroup文件系统上，每个文件都对应一个struct kernfs_node结构，当然kernfs_root作为文件系的根也对应一个kernfs_node结构。</p><p>接下来，css_populate_dir会调用cgroup_addrm_files-&gt;cgroup_add_file-&gt;cgroup_add_file，来创建整棵文件树，并且为树中的每个文件创建对应的kernfs_node结构，并将这个文件的操作函数设置为kf_ops，也即指向cgroup_kf_ops 。</p><pre><code>static int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,\n\t\t\t   struct cftype *cft)\n{\n\tchar name[CGROUP_FILE_NAME_MAX];\n\tstruct kernfs_node *kn;\n......\n\tkn = __kernfs_create_file(cgrp-&gt;kn, cgroup_file_name(cgrp, cft, name),\n\t\t\t\t  cgroup_file_mode(cft), 0, cft-&gt;kf_ops, cft,\n\t\t\t\t  NULL, key);\n......\n}\n\nstruct kernfs_node *__kernfs_create_file(struct kernfs_node *parent,\n\t\t\t\t\t const char *name,\n\t\t\t\t\t umode_t mode, loff_t size,\n\t\t\t\t\t const struct kernfs_ops *ops,\n\t\t\t\t\t void *priv, const void *ns,\n\t\t\t\t\t struct lock_class_key *key)\n{\n\tstruct kernfs_node *kn;\n\tunsigned flags;\n\tint rc;\n\n\tflags = KERNFS_FILE;\n\n\tkn = kernfs_new_node(parent, name, (mode &amp; S_IALLUGO) | S_IFREG, flags);\n\n\tkn-&gt;attr.ops = ops;\n\tkn-&gt;attr.size = size;\n\tkn-&gt;ns = ns;\n\tkn-&gt;priv = priv;\n......\n\trc = kernfs_add_one(kn);\n......\n\treturn kn;\n}\n</code></pre><p>从cgroup_setup_root返回后，接下来，在cgroup1_mount中，要做的一件事情是cgroup_do_mount，调用kernfs_mount真的去mount这个文件系统，返回一个普通的文件系统都认识的dentry。这种特殊的文件系统对应的文件操作函数为kernfs_file_fops。</p><pre><code>const struct file_operations kernfs_file_fops = {\n\t.read\t\t= kernfs_fop_read,\n\t.write\t\t= kernfs_fop_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.mmap\t\t= kernfs_fop_mmap,\n\t.open\t\t= kernfs_fop_open,\n\t.release\t= kernfs_fop_release,\n\t.poll\t\t= kernfs_fop_poll,\n\t.fsync\t\t= noop_fsync,\n};\n</code></pre><p>当我们要写入一个CGroup文件来设置参数的时候，根据文件系统的操作，kernfs_fop_write会被调用，在这里面会调用kernfs_ops的write函数，根据上面的定义为cgroup_file_write，在这里会调用cftype的write函数。对于CPU和内存的write函数，有以下不同的定义。</p><pre><code>static struct cftype cpu_files[] = {\n#ifdef CONFIG_FAIR_GROUP_SCHED\n    {   \n        .name = &quot;shares&quot;,\n        .read_u64 = cpu_shares_read_u64,\n        .write_u64 = cpu_shares_write_u64,\n    },  \n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n    {   \n        .name = &quot;cfs_quota_us&quot;,\n        .read_s64 = cpu_cfs_quota_read_s64,\n        .write_s64 = cpu_cfs_quota_write_s64,\n    },  \n    {   \n        .name = &quot;cfs_period_us&quot;,\n        .read_u64 = cpu_cfs_period_read_u64,\n        .write_u64 = cpu_cfs_period_write_u64,\n    },  \n}\n\n\nstatic struct cftype mem_cgroup_legacy_files[] = {\n    {   \n        .name = &quot;usage_in_bytes&quot;,\n        .private = MEMFILE_PRIVATE(_MEM, RES_USAGE),\n        .read_u64 = mem_cgroup_read_u64,\n    },  \n    {   \n        .name = &quot;max_usage_in_bytes&quot;,\n        .private = MEMFILE_PRIVATE(_MEM, RES_MAX_USAGE),\n        .write = mem_cgroup_reset,\n        .read_u64 = mem_cgroup_read_u64,\n    },  \n    {   \n        .name = &quot;limit_in_bytes&quot;,\n        .private = MEMFILE_PRIVATE(_MEM, RES_LIMIT),\n        .write = mem_cgroup_write,\n        .read_u64 = mem_cgroup_read_u64,\n    },  \n    {   \n        .name = &quot;soft_limit_in_bytes&quot;,\n        .private = MEMFILE_PRIVATE(_MEM, RES_SOFT_LIMIT),\n        .write = mem_cgroup_write,\n        .read_u64 = mem_cgroup_read_u64,\n    },  \n}\n</code></pre><p>如果设置的是cpu.shares，则调用cpu_shares_write_u64。在这里面，task_group的shares变量更新了，并且更新了CPU队列上的调度实体。</p><pre><code>int sched_group_set_shares(struct task_group *tg, unsigned long shares)\n{\n\tint i;\n\n\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES));\n\n\ttg-&gt;shares = shares;\n\tfor_each_possible_cpu(i) {\n\t\tstruct rq *rq = cpu_rq(i);\n\t\tstruct sched_entity *se = tg-&gt;se[i];\n\t\tstruct rq_flags rf;\n\n\t\tupdate_rq_clock(rq);\n\t\tfor_each_sched_entity(se) {\n\t\t\tupdate_load_avg(se, UPDATE_TG);\n\t\t\tupdate_cfs_shares(se);\n\t\t}\n\t}\n......\n}\n</code></pre><p>但是这个时候别忘了，我们还没有将CPU的文件夹下面的tasks文件写入进程号呢。写入一个进程号到tasks文件里面，按照cgroup1_base_files里面的定义，我们应该调用cgroup_tasks_write。</p><p>接下来的调用链为：cgroup_tasks_write-&gt;__cgroup_procs_write-&gt;cgroup_attach_task-&gt; cgroup_migrate-&gt;cgroup_migrate_execute。将这个进程和一个cgroup关联起来，也即将这个进程迁移到这个cgroup下面。</p><pre><code>static int cgroup_migrate_execute(struct cgroup_mgctx *mgctx)\n{\n\tstruct cgroup_taskset *tset = &amp;mgctx-&gt;tset;\n\tstruct cgroup_subsys *ss;\n\tstruct task_struct *task, *tmp_task;\n\tstruct css_set *cset, *tmp_cset;\n......\n\tif (tset-&gt;nr_tasks) {\n\t\tdo_each_subsys_mask(ss, ssid, mgctx-&gt;ss_mask) {\n\t\t\tif (ss-&gt;attach) {\n\t\t\t\ttset-&gt;ssid = ssid;\n\t\t\t\tss-&gt;attach(tset);\n\t\t\t}\n\t\t} while_each_subsys_mask();\n\t}\n......\n}\n</code></pre><p>每一个cgroup子系统会调用相应的attach函数。而CPU调用的是cpu_cgroup_attach-&gt; sched_move_task-&gt; sched_change_group。</p><pre><code>static void sched_change_group(struct task_struct *tsk, int type)\n{\n\tstruct task_group *tg;\n\n\ttg = container_of(task_css_check(tsk, cpu_cgrp_id, true),\n\t\t\t  struct task_group, css);\n\ttg = autogroup_task_group(tsk, tg);\n\ttsk-&gt;sched_task_group = tg;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tif (tsk-&gt;sched_class-&gt;task_change_group)\n\t\ttsk-&gt;sched_class-&gt;task_change_group(tsk, type);\n\telse\n#endif\n\t\tset_task_rq(tsk, task_cpu(tsk));\n}\n</code></pre><p>在sched_change_group中设置这个进程以这个task_group的方式参与调度，从而使得上面的cpu.shares起作用。</p><p>对于内存来讲，写入内存的限制使用函数mem_cgroup_write-&gt;mem_cgroup_resize_limit来设置struct mem_cgroup的memory.limit成员。</p><p>在进程执行过程中，申请内存的时候，我们会调用handle_pte_fault-&gt;do_anonymous_page()-&gt;mem_cgroup_try_charge()。</p><pre><code>int mem_cgroup_try_charge(struct page *page, struct mm_struct *mm,\n\t\t\t  gfp_t gfp_mask, struct mem_cgroup **memcgp,\n\t\t\t  bool compound)\n{\n\tstruct mem_cgroup *memcg = NULL;\n......\n\tif (!memcg)\n\t\tmemcg = get_mem_cgroup_from_mm(mm);\n\n\tret = try_charge(memcg, gfp_mask, nr_pages);\n......\n}\n</code></pre><p>在mem_cgroup_try_charge中，先是调用get_mem_cgroup_from_mm获得这个进程对应的mem_cgroup结构，然后在try_charge中，根据mem_cgroup的限制，看是否可以申请分配内存。</p><p>至此，cgroup对于内存的限制才真正起作用。</p><h2>总结时刻</h2><p>内核中cgroup的工作机制，我们在这里总结一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/c4/c9cc56d20e6a4bac0f9657e6380a96c4.png?wh=5836*5203\" alt=\"\"></p><p>第一步，系统初始化的时候，初始化cgroup的各个子系统的操作函数，分配各个子系统的数据结构。</p><p>第二步，mount cgroup文件系统，创建文件系统的树形结构，以及操作函数。</p><p>第三步，写入cgroup文件，设置cpu或者memory的相关参数，这个时候文件系统的操作函数会调用到cgroup子系统的操作函数，从而将参数设置到cgroup子系统的数据结构中。</p><p>第四步，写入tasks文件，将进程交给某个cgroup进行管理，因为tasks文件也是一个cgroup文件，统一会调用文件系统的操作函数进而调用cgroup子系统的操作函数，将cgroup子系统的数据结构和进程关联起来。</p><p>第五步，对于CPU来讲，会修改scheduled entity，放入相应的队列里面去，从而下次调度的时候就起作用了。对于内存的cgroup设定，只有在申请内存的时候才起作用。</p><h2>课堂练习</h2><p>这里我们用cgroup限制了CPU和内存，如何限制网络呢？给你一个提示tc，请你研究一下。</p><p>欢迎留言和我分享你的疑惑和见解，也欢迎收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。</p>","comments":[{"had_liked":false,"id":122556,"user_name":"行者","can_delete":false,"product_type":"c1","uid":1063734,"ip_address":"","ucode":"EA31201A7C5AE1","user_header":"https://static001.geekbang.org/account/avatar/00/10/3b/36/2d61e080.jpg","comment_is_top":false,"comment_ctime":1565423273,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"35925161641","product_id":100024701,"comment_content":"老师，麻烦讲下华为鸿蒙系统，它和linux区别与联系是什么？","like_count":8,"discussions":[{"author":{"id":1208637,"avatar":"https://static001.geekbang.org/account/avatar/00/12/71/3d/da8dc880.jpg","nickname":"游弋云端","note":"","ucode":"A960E8F5AA25B9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":23053,"discussion_content":"宏内核与微内核的区别~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1569748170,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":122113,"user_name":"安排","can_delete":false,"product_type":"c1","uid":1260026,"ip_address":"","ucode":"F78CFA9624CAEF","user_header":"https://static001.geekbang.org/account/avatar/00/13/39/fa/a7edbc72.jpg","comment_is_top":false,"comment_ctime":1565306696,"is_pvip":false,"replies":[{"id":"46333","content":"掉电后，进程都挂了，pid都变了，cgroup还有啥用","user_name":"作者回复","user_name_real":"刘超@网易云","uid":"1001590","ctime":1566295963,"ip_address":"","comment_id":122113,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23040143176","product_id":100024701,"comment_content":"Cgroup文件系统是只存在内存中吗？每一次设置之后在掉电后是不是就消失了？","like_count":5,"discussions":[{"author":{"id":1001590,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/48/76/0c7d4d23.jpg","nickname":"刘超","note":"","ucode":"196BF3F499E8FE","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462104,"discussion_content":"掉电后，进程都挂了，pid都变了，cgroup还有啥用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566295963,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":122271,"user_name":"刘桢","can_delete":false,"product_type":"c1","uid":1482815,"ip_address":"","ucode":"3BFAB1C9772EB4","user_header":"https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg","comment_is_top":false,"comment_ctime":1565327779,"is_pvip":false,"replies":[{"id":"46328","content":"加油","user_name":"作者回复","user_name_real":"刘超@网易云","uid":"1001590","ctime":1566289734,"ip_address":"","comment_id":122271,"utype":1}],"discussion_count":6,"race_medal":0,"score":"18745196963","product_id":100024701,"comment_content":"二十天闭关冲北邮","like_count":4,"discussions":[{"author":{"id":1001590,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/48/76/0c7d4d23.jpg","nickname":"刘超","note":"","ucode":"196BF3F499E8FE","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462181,"discussion_content":"加油","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566289734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2027659,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f0/8b/a806789b.jpg","nickname":"int8","note":"","ucode":"EDE812592B8C06","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414339,"discussion_content":"现在哪个厂？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636724662,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1000473,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/44/19/17fadc62.jpg","nickname":"郭蕾","note":"","ucode":"34F4C07D1C5FE8","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":192903,"discussion_content":"考上了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583109961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1482815,"avatar":"https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg","nickname":"刘桢","note":"","ucode":"3BFAB1C9772EB4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1000473,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/44/19/17fadc62.jpg","nickname":"郭蕾","note":"","ucode":"34F4C07D1C5FE8","race_medal":0,"user_type":8,"is_pvip":false},"discussion":{"id":195363,"discussion_content":"么得，数学太低了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583266480,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":192903,"ip_address":""},"score":195363,"extra":""},{"author":{"id":1000473,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/44/19/17fadc62.jpg","nickname":"郭蕾","note":"","ucode":"34F4C07D1C5FE8","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":1482815,"avatar":"https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg","nickname":"刘桢","note":"","ucode":"3BFAB1C9772EB4","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":195437,"discussion_content":"工作吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583281924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":195363,"ip_address":""},"score":195437,"extra":""},{"author":{"id":1482815,"avatar":"https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg","nickname":"刘桢","note":"","ucode":"3BFAB1C9772EB4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1000473,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/44/19/17fadc62.jpg","nickname":"郭蕾","note":"","ucode":"34F4C07D1C5FE8","race_medal":0,"user_type":8,"is_pvip":false},"discussion":{"id":198757,"discussion_content":"最近正在找","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583510165,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":195437,"ip_address":""},"score":198757,"extra":""}]}]},{"had_liked":false,"id":154646,"user_name":"fhchina","can_delete":false,"product_type":"c1","uid":1057157,"ip_address":"","ucode":"CF9329FD16F6CE","user_header":"https://static001.geekbang.org/account/avatar/00/10/21/85/609a2e51.jpg","comment_is_top":false,"comment_ctime":1574499073,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14459400961","product_id":100024701,"comment_content":"cpu.cfs_period_us的单位是us, 微秒不是毫秒","like_count":3},{"had_liked":false,"id":222284,"user_name":"羊仔爸比","can_delete":false,"product_type":"c1","uid":1465584,"ip_address":"","ucode":"746A1099298811","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLTFKH3aA1FVyz7VvAIlISibAPbmiaAyQ5fAK3ElyEcXuRmsmicAefXxkhbC11icjBgfbXPXkHHt5O0xw/132","comment_is_top":false,"comment_ctime":1590736825,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"5885704121","product_id":100024701,"comment_content":"老师：<br>      请教一下docker 里面我设置了 内存的limit是2g，cgroup 文件中memory.usage_in_bytes这个文件是是包含memory.stat中的total_rss 和total_cache 相加的大小，oom kill的时候会根据memory.usage_in_bytes的值kill吗，如果不是根据这个文件的值kill是根据哪个值进行kill的？","like_count":1,"discussions":[{"author":{"id":1628712,"avatar":"https://static001.geekbang.org/account/avatar/00/18/da/28/b09d3747.jpg","nickname":"Frame","note":"","ucode":"57865AC3A1BAD5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":554545,"discussion_content":"内存分配不出来，就可能 oom；page cache 是可能主动/直接回收的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646459453,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":348987,"user_name":"Sudouble","can_delete":false,"product_type":"c1","uid":1365574,"ip_address":"","ucode":"B369B09DAF8D20","user_header":"https://static001.geekbang.org/account/avatar/00/14/d6/46/5eb5261b.jpg","comment_is_top":false,"comment_ctime":1655624035,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1655624035","product_id":100024701,"comment_content":"这么多篇深度文章，很好的诠释了十几年前的一个疑问，为什么不让电脑突然断电！内存、缓存里存的大量数据，操作系统还没有触发写，这时突然断电，这部分数据全都丢了。","like_count":0},{"had_liked":false,"id":336538,"user_name":"EST4What","can_delete":false,"product_type":"c1","uid":2762597,"ip_address":"","ucode":"912EDD458E4394","user_header":"https://static001.geekbang.org/account/avatar/00/2a/27/65/0790efd2.jpg","comment_is_top":false,"comment_ctime":1646205976,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1646205976","product_id":100024701,"comment_content":"为什么我mount -t cgroup会显示，而切换到&#47;sys&#47;fs&#47;cgroup时，文件却不见了呢<br><br>[root@jenkins cgroup]# mount -t cgroup <br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=&#47;usr&#47;lib&#47;systemd&#47;systemd-cgroups-agent,name=systemd)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuacct,cpu)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_prio,net_cls)<br>cgroup on &#47;sys&#47;fs&#47;cgroup&#47;pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids)<br>[root@jenkins cgroup]# ls<br>[root@jenkins cgroup]# ll<br>total 0<br><br>","like_count":0},{"had_liked":false,"id":300852,"user_name":"songyy","can_delete":false,"product_type":"c1","uid":1007201,"ip_address":"","ucode":"34D6443B91676D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/61/985f3eb7.jpg","comment_is_top":false,"comment_ctime":1625404302,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1625404302","product_id":100024701,"comment_content":"testnginx 不在docker的repo里面吧。示例里面<br><br> docker run -d --cpu-shares 513 --cpus 2 --cpuset-cpus 1,3 --memory 1024M --memory-swap 1234M --memory-swappiness 7 -p 8081:80 testnginx:1<br><br>这个代码就跑不起来了","like_count":0},{"had_liked":false,"id":290160,"user_name":"吴钩","can_delete":false,"product_type":"c1","uid":2062402,"ip_address":"","ucode":"0EB50E8144BCDE","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKOnpl8fRB9r2vED2s8j7Arwbn2K6M6HUBWNjgoqV4uqe94fTGK4WGpOJLeRxXcBXk3dp23eQR0AQ/132","comment_is_top":false,"comment_ctime":1619408229,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1619408229","product_id":100024701,"comment_content":"对namespace和cgroup了解增加了很多，赞！","like_count":0},{"had_liked":false,"id":122309,"user_name":"许童童","can_delete":false,"product_type":"c1","uid":1003005,"ip_address":"","ucode":"4B799C0C6BC678","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4d/fd/0aa0e39f.jpg","comment_is_top":false,"comment_ctime":1565336103,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565336103","product_id":100024701,"comment_content":"跟着老师一起精进。","like_count":0}]}