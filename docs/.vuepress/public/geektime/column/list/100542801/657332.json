{"id":657332,"title":"13｜基于关键词的召回：如何使用关键词吸引用户？","content":"<p>你好，我是黄鸿波。</p><p>在讲解了基于时间的召回和基于热度的召回后，今天我们进入到基于规则召回的最后一种——基于关键词的召回，我将本节课分为了下面两个大的部分。</p><ol>\n<li>基于关键词的召回。</li>\n<li>提取关键词的几种获取方式。<br>\n那么话不多说，我们直接开始本节课的内容。</li>\n</ol><p></p><h2>基于关键词的召回</h2><p>对于任何一篇文章、段落、标题等，只要是以文字形式展现的内容，我们都能够通过一系列的词语组合起来代表一篇文章的主旨，而这一系列的词语就是文章的<strong>关键词</strong>。</p><p>文章的标题是一篇文章最精辟的概括，摘要是一篇文章的主旨，正文是文章的详细信息。对于一篇文章来说，我们最先看到的是关键词，其次是摘要，然后才是文章正文的本身。因此，在做关键词提取和文章的特征时，我们一般需要分开来做关键词的提取和保存。</p><p>基于关键词的召回往往与用户画像或搜索关键词息息相关。在<a href=\"https://time.geekbang.org/column/article/655495\">用户画像</a>中实际上也会有用户常看的内容关键词的集合，甚至还可以对这些关键词的出现频率进行排序，从而来确定这些关键词的重要性。而在基于关键词召回的算法中，我们可以将文章的关键词和用户画像中用户标签的关键词进行结合，从而进行基于关键词的召回。</p><p>除了用户画像外，用户的搜索内容往往是基于关键词召回的关键所在。一般来讲，一旦用户搜索了一个关键词，说明用户对这个关键词表示的内容是非常感兴趣的。因此，我们在做推荐的时候就需要尽可能把关键词相关的内容给展示出来，从而增加用户的黏性和点击率。</p><!-- [[[read_end]]] --><p>在与用户搜索内容结合的时候，一般来讲，我们会采取多种不同的方式来结合。最简单直接的方式是<strong>关键词与热度结合</strong>。</p><p>当用户在进行关键词搜索时，我们在后台就可以知道用户目前搜索的关键词是什么，然后将关键词放到内容画像中进行搜索，这个时候我们可能会找到一个、多个或者根本没有相关关键词的内容。当有多个和该关键词相关的内容之后，我们就可以将这些内容按照热度降序进行排列，然后展现给用户。</p><p>当与之匹配的关键词内容只有一个或者没有时，我们就得用另外一种形式对内容进行补充：<strong>结合协同过滤或基于关键词相似度的embedding召回</strong>。</p><p>这两块的召回部分会在后面的课程中详细讲解，在这里我先来说一下大致的原理。我们在操作的时候如果发现召回的内容非常少，这个时候有两种策略：<strong>不给用户推荐</strong>和<strong>给用户推荐相似的内容</strong>。</p><p>如果我们要推荐相似内容，我们实际上最简单的方法就是去找关键词的相似关键词，或者直接拿这个关键词与我们关键词库的每一个词做距离计算，找到距离最相机的几个关键词，然后再将这些关键词作为目标关键词进行关键词召回，这样就能够召回更多的内容进行推荐。</p><p>你能看到，关键词召回需要联合其他形式才能起到最好的效果。</p><p></p><p></p><h2>提取关键词的几种常见算法</h2><p>说完关键词召回怎么做之后，我们进入到提取关键词的算法部分。</p><h3>TF-IDF算法</h3><p>要了解TF-IDF算法，需要先澄清三个概念：词频、去停用词和逆文档频率。以文档$D$为例，用$\\omega$来表示一个关键词，可用以下公式统计出$\\omega$在文档$D$中出现的频率，即“词频（Term Frequency, TF）”。</p><p>$TF_{\\omega,D_{i}}=\\frac{count(\\omega)}{D_{i}}$</p><p>下面这个公式就是上面公式的文字化表示。</p><p><img src=\"https://static001.geekbang.org/resource/image/6e/67/6ebf415ac25e7d8a1a5eec72fcc2d267.png?wh=936x165\" alt=\"图片\"></p><p>我们经常会发现，用词频算法提取的词往往是“的”“是”“在”这类词，这类词即为“停用词”。在进行关键词提取的任务中，去停用词是一个非常必要的前提条件，只有做完去停用词之后，统计出来的结果才是相对准确的。</p><p>假定文档题目是《中国人均收入调查》，那么提取的关键词很有可能是“中国”“收入”“GDP”等词，而这类词虽然是这篇文章中的关键词，但并不能代表整篇文章的特性，因为这类词在其他的文章中也是高频词。那么怎么才能找到一个词，它既是关键词又能代表这篇文章的特性呢？这个时候我们需要一个新的名词，即“逆文档频率”（Inverse Document Frequency，IDF），文档总数$n$与词$\\omega$所出现文件数$doc(\\omega,D)$比值的对数，可以表示为下面这个公式。</p><p><img src=\"https://static001.geekbang.org/resource/image/90/4d/909025e5fac8a050ee136eb4eb5d434d.png?wh=667x209\" alt=\"图片\"></p><p>下面这个公式就是上面公式的文字化表示。</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/d5/c5b11c6e0a3a71a23f26e9bb9a8a36d5.png?wh=1119x208\" alt=\"图片\"></p><p>从上面公式可以看出，如果一个词越常见（如“中国”“收入”“GDP”），那么其逆文档频率就会越小。也就是说，逆文档频率越大，关键词在其他的文档中出现概率越小，意味着越是这篇文章的关键词。为了避免逆文档频率大到所有的文档都不包含该关键词，上述公式的分母加上1。</p><p>那么，一篇文档的TF-IDF值就与一个词在文档中出现的次数成正比，与该词在其他文档中出现的次数成反比。</p><p><img src=\"https://static001.geekbang.org/resource/image/0e/86/0e311099e58a78e59e3d3b7e1d546886.png?wh=925x114\" alt=\"图片\"></p><p>因此，TF-IDF算法即对关键词的TF-IDF值做降序排序，最后取前N个关键词作为最终的结果。</p><h3>TextRank算法</h3><p>与TF-IDF不同，TextRank在进行关键词提取时能够考虑到相邻词语之间的语义关系，而TF-IDF只是针对每个词的词频进行统计，并没有充分考虑词之间的语义信息。这就衍生出TextRank的另外一个优势：<strong>脱离语料库</strong>。</p><p>脱离语料库是指TextRank并不需要像TF-IDF一样使用语料库进行训练，而是直接针对单篇文章就可以进行关键词的提取，因此TextRank算法对于小批量数据来说是一个非常理想的选择。</p><p>从原理上来讲，TextRank算法是基于PageRank算法衍生出来的。其核心思想是通过词之间的相邻关系构建网络，然后用PageRank迭代计算每个节点的rank值，排序rank值即可得到关键词。</p><p>TextRank算法是一个无向图的算法。所谓的无向是指两个词之间的边是无向的，这也是TextRank算法与PageRank算法最大的区别。</p><p>我们先简要地介绍一下PageRank算法。PageRank算法是Google用来计算网页权重值的一种算法，在PageRank算法中，如果一个链接的权重值越大，说明这个链接就越重要。假设现在有四个网页分别为网页A、网页B、网页C和网页D，在这四个网页中，所有除了网页A的网页都只链接到了网页A，那么A的PR（PageRank）值就是网页B、网页C和网页D的PageRank值的总和，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/8a/2af745ae865a27a1092d47993a723a8a.png?wh=534x336\" alt=\"图片\"></p><p>可以用下面这个公式来表示。</p><p><img src=\"https://static001.geekbang.org/resource/image/d5/f5/d53320aca0ec5959590aec5d244a8bf5.png?wh=1047x134\" alt=\"图片\"></p><p>如果我们重新定义一下这四个网页的指向关系，网页B指向网页A和网页C、网页C指向网页A，网页D指向其他的所有网页，那么就有下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/50/e0da8f58634158423a2ec5a0b046c550.png?wh=1120x761\" alt=\"图片\"></p><p>网页B有两个向外的链接，其中一个链接到了网页A，因此，网页A对于网页B的PR值为$\\frac{PR(B)}{2}$；网页C只有1个向外的链接（即网页A），因此，网页C对于网页A的PR值为$\\frac{PR©}{1}$；网页D有3个向外的链接，其中一个链接到了网页A，因此，网页D对于网页A的PR值为$\\frac{PR(D)}{3}$。由此得到网页A的总的PR值为下面这个公式。</p><p><img src=\"https://static001.geekbang.org/resource/image/4b/f6/4b408ea43842242942bd8328b434daf6.png?wh=1280x280\" alt=\"图片\"></p><p>在实际PR值的计算中我们还要考虑到，当用户到达某个页面之后，是否还会继续通过这个页面向前。因此我们需要在PR值的计算中增加一项阻尼系数$d$，因此我们可以得到下面这个PR算法的推广公式。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/38/2ceee05fe31cf0138dbe32aaccea4a38.png?wh=1371x209\" alt=\"图片\"></p><p>其中<em>d</em>为阻尼系数，表示在任意时刻，用户到达某页面后并继续向后浏览的概率；$PR(T_{i})$代表页面$T_{i}$的PR值；$C(T_{i})$代表页面$T_{i}$指向其他页面的边的个数。</p><p>当然，这只是PR值计算的最基本的形式，在实际Google的PR算法中还有很多其他的变形以及其他系数，并且每隔一段时间这个系数和公式还会有微调。然后Google利用PR算法，当用户在Google上进行搜索时，就会计算与搜索关键词相关联的网页并计算出这些网页的PR值，然后将计算出来的PR值倒序排序，根据PR值从大到小的结果展示给用户。</p><p>TextRank算法实际上是PageRank算法的一个变体，其中最大的变化在于PageRank算法是基于网页，而TextRank算法是基于词；另外PageRank算法中网页之间是一个有向关系，也就是说网页之间是有很明确的方向性，而TextRank算法是一个无向图，也就是说我们在表示两个词之间的权重时，对于两个词来讲是相同的。</p><p>因此，我们可以类比PageRank算法来说明一下TextRank提取关键词的具体思路。</p><ol>\n<li>\n<p>对整个文本进行句子切分。</p>\n</li>\n<li>\n<p>将待提取关键词的文本语句进行数据清洗，包括去停用词、无用词和标点符号、然后对其进行分词处理，形成候选关键词。</p>\n</li>\n<li>\n<p>构建候选关键词的图，这个图可以表示为<em>G</em>（<em>V</em>,<em>E</em>），其中<em>V</em>是由第2步生成的结果集，采用共现结果的方式；<em>E</em>是由两个节点之间的边组成，可以把<em>E</em>看出<em>V</em>×<em>V</em>的集合；图中任意两个点之间的权重$V_{i}$和$V_{j}$之间边的权重可以表示为$W_{ij}$。对于给定的点$V_{i}$来说，&nbsp;$In(V_{i})$为 指向该点的点集合，&nbsp;$Out(V_{i})$为点$V_{i}$指向的点集合。由此，我们可以通过下面这个公式算出点$V_{i}$的权重值。<br>\n<img src=\"https://static001.geekbang.org/resource/image/d8/d2/d84yy205b61331b45326f6dcb72406d2.png?wh=1044x200\" alt=\"图片\"></p>\n</li>\n<li>\n<p>根据第三步中给出的公式，初始化各个节点的权重，然后迭代计算各个节点的权重，直至收敛。在这里的收敛是指经过N轮迭代，使得得到一个相对稳定的误差，而这个误差小于某个规定的阈值之内（一般我们将这个阈值设置为0.001）。</p>\n</li>\n<li>\n<p>对上述得到的各个节点的权重进行倒序排序，从而得到这个文本中最重要的N个词，作为候选关键词。</p>\n</li>\n<li>\n<p>将第五步得到的候选关键词在原始的文本中进行标记，如果形成了相邻的词组，则组成一个多词关键词。</p>\n</li>\n</ol><p>到这里，使用TextRank算法得到关键词的整套流程也就结束了。</p><h3>两种算法联合提取关键词</h3><p>前面的实现全部基于一种算法，但是在真正的企业工程化中，我们往往使用多种算法进行联合关键词提取。</p><p>在进行关键词提取之前，首先要对自己准备提取关键词的文本有比较全面的了解。虽然每个算法都能进行关键词提取，但是针对不同的文本内容及文本量来说，每种关键词提取算法的表现是不同的。</p><p>比如，当文本量大到可组成一个数据集时，我们可以考虑使用TF-IDF算法作为关键词提取的主要算法；但是如果文本量相对比较少（或者针对某一篇文章来进行关键词提取），那么常见做法就是使用TextRank算法。</p><p>单独使用一种算法的话，关键词提取的准确率会大打折扣，那么最好的做法就是联合使用几种算法，常见的做法是TF-IDF+TextRank。</p><p>不同算法关注的重点不同，就会导致结果差异比较大。例如针对下面同样一段文本，我们使用TF-IDF和TextRank来做一个对比。</p><blockquote>\n<p>北京时间5月28日消息，据《北京青年报》官微透露，北京中赫国安归化球员李可将入选新一期国家队的大名单。而他将成为国足历史首位归化国脚，目前相关手续应该已经得到落实，意味着李可具备代表中国队参赛的资格。北京时间5月28日消息，据《北京青年报》官微透露，北京中赫国安归化球员李可将入选新一期国家队的大名单。而他将成为国足历史首位归化国脚，目前相关手续应该已经得到落实，意味着李可具备代表中国队参赛的资格。</p>\n</blockquote><p>对这段文本进行数据清洗后，得到如下文本。</p><blockquote>\n<p>北京时间5月28日消息，据《北京青年报》官微透露，北京中赫国安归化球员李可将入选新一期国家队的大名单。而他将成为国足历史首位归化国脚，目前相关手续应该已经得到落实，意味着李可具备代表中国队参赛的资格。北京时间5月28日消息，据《北京青年报》官微透露，北京中赫国安归化球员李可将入选新一期国家队的大名单。而他将成为国足历史首位归化国脚，目前相关手续应该已经得到落实，意味着李可具备代表中国队参赛的资格。</p>\n</blockquote><p>然后使用开源jieba库编写的整体算法进行处理，代码如下。</p><pre><code class=\"language-plain\">import jieba\nimport jieba.analyse\nfrom segment import Segment\n&nbsp;\n&nbsp;\nclass Model(object):\n&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.seg = Segment(\"../common/data/stopword.txt\", \"../common/data/user_dict.txt\")\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;def process_text(self, text):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;words_list = self.seg.cut(text)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;words_list = ' '.join(words_list)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return words_list\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;def get_keyword(self, words_list, param, use_pos=True):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if use_pos:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 选定部分词性\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allow_pos = ('n', 'nr', 'nr1', 'nr2', 'ns', 'nsf', 'nt', 'nz', 'nl', 'ng', 'nr', 'vn')\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allow_pos = ()\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if param == 'tfidf':\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tfidf_keywords = jieba.analyse.extract_tags(words_list, topK=10, withWeight=False, allowPOS=allow_pos)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return tfidf_keywords\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif param == 'textrank':\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;textrank_keywords = jieba.analyse.textrank(words_list, topK=10, withWeight=False, allowPOS=allow_pos)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return textrank_keywords\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;def keyword_interact(self, tfidf_keyword, textrank_keyword):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return list(set(tfidf_keyword).intersection(set(textrank_keyword)))\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;def keyword_topk(self, tfidf_keyword, textrank_keyword, k):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combine = list(tfidf_keyword)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for word in textrank_keyword:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;combine.append(word)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return list(set(combine))\n&nbsp;\n&nbsp;\nif __name__ == \"__main__\":\n&nbsp;&nbsp;&nbsp;&nbsp;model = Model()\n&nbsp;&nbsp;&nbsp;&nbsp;text = '北京时间5月28日消息，据《北京青年报》官微透露，北京中赫国安归化球员李可将入选新一期国家队的大名单。而他将成为国足历史首位归化国脚，目前相关手续应该已经得到落实，意味着李可具备代表中国队参赛的资格。北京时间5月28日消息，据《北京青年报》官微透露，北京中赫国安归化球员李可将入选新一期国家队的大名单。而他将成为国足历史首位归化国脚，目前相关手续应该已经得到落实，意味着李可具备代表中国队参赛的资格。'\n&nbsp;&nbsp;&nbsp;&nbsp;words_list = model.process_text(text)\n&nbsp;&nbsp;&nbsp;&nbsp;tfidf_keyword = model.get_keyword(words_list, param='tfidf')\n&nbsp;&nbsp;&nbsp;&nbsp;print('tfidf_keyword', tfidf_keyword)\n&nbsp;&nbsp;&nbsp;&nbsp;textrank_keyword = model.get_keyword(words_list, param='textrank')\n&nbsp;&nbsp;&nbsp;&nbsp;print(\"textrank_keyword\", textrank_keyword)\n&nbsp;&nbsp;&nbsp;&nbsp;keyword_interact = model.keyword_interact(tfidf_keyword, textrank_keyword)\n&nbsp;&nbsp;&nbsp;&nbsp;print('keyword_interact', keyword_interact)\n&nbsp;&nbsp;&nbsp;&nbsp;keyword_topk = model.keyword_topk(tfidf_keyword, textrank_keyword, 3)\n&nbsp;&nbsp;&nbsp;&nbsp;print('keyword_topk', keyword_topk)\n&nbsp;\n</code></pre><p>使用TFIDF算法提取的关键词如下。</p><pre><code class=\"language-plain\">tfidf_keyword ['归化', '李可', '官微', '中赫', '国脚', '北京青年报', '中国队', '北京', '参赛', '国家队']\n</code></pre><p>使用TextRank算法提取的关键词如下。</p><pre><code class=\"language-plain\">textrank_keyword ['归化', '北京', '国安', '消息', '历史', '参赛', '时间', '李可', '代表', '球员']\n</code></pre><p>在企业中有两种方法联合算法，一种是取两种算法的交集（即这两种算法都认为是关键词的部分），那么我们认为这个关键词一定就能代表文章；另外一种是用两种算法的交集与每种算法的Top3关键词求并集，这样做默认了每种算法的前三个关键词都是真正的关键词。</p><p>方法一：两种算法的交集，我们可以得出的结果关键词集合如下。</p><pre><code class=\"language-plain\">keyword_interact ['李可', '北京', '参赛', '归化']\n</code></pre><p>方法二：两种算法的交集与每种算法的Top3关键词求并集，我们可以得出的结果关键词集合如下。</p><pre><code class=\"language-plain\">keyword_topk ['归化', '李可', '国安', '北京', '官微']\n</code></pre><p>从结果可以看出，方法一（取算法交集）的效果会更好一些。但是实际上在企业中，效果的好坏要根据实际的文本类型进行实验才能得出，而不是通过其他人的实验结果简单粗暴地做出结论。</p><h2>总结</h2><p>到这里，这节课就已经接近尾声了，我们来对今天的课程做一个简单的总结。学完今天的课程，你应该知道下面这三个要点。</p><ol>\n<li>什么是基于关键词的召回算法。</li>\n<li>基于关键词的召回算法一般是如何实现的。</li>\n<li>提取关键词中两种常见的算法：TF-IDF和TextRank算法，以及这两种算法的实现原理。</li>\n</ol><h2>课后题</h2><p>最后你布置两个小作业。</p><ol>\n<li>使用关键词提取技术提取我们爬取的文章内容。</li>\n<li>想一想，我们还可以用关键词召回算法配合哪些算法进行使用。</li>\n</ol>","neighbors":{"left":{"article_title":"12｜基于热度的召回：如何使用热门内容来吸引用户？","id":656948},"right":{"article_title":"14｜基于Flask的推荐服务：搭建基础的Flask服务","id":659243}},"comments":[{"had_liked":false,"id":374630,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1684242945,"is_pvip":false,"replies":[{"id":136886,"content":"可以的，其实有很多基于深度学习和机器学习的关键词提取的库，你可以去百度下这些库的用法，jieba就是其中一个。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1684858558,"ip_address":"广东","comment_id":374630,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"关键词的获取，是否有现成的可用的工具？就是说拿来就能用、基本不用开发。比如我要搭建一个推荐系统，也用到了关键词获取，导入一个库然后调用其API就可以直接获取；或者运行一个工具软件，可以直接获取；或者某个平台提供该服务。等等。","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619203,"discussion_content":"可以的，其实有很多基于深度学习和机器学习的关键词提取的库，你可以去百度下这些库的用法，jieba就是其中一个。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684858559,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380646,"user_name":"Weitzenböck","can_delete":false,"product_type":"c1","uid":2669122,"ip_address":"江苏","ucode":"78C92583084ABA","user_header":"https://static001.geekbang.org/account/avatar/00/28/ba/42/5ca553bd.jpg","comment_is_top":false,"comment_ctime":1693926692,"is_pvip":false,"replies":[{"id":140013,"content":"https:&#47;&#47;github.com&#47;ipeaking&#47;recommendation\n\nhttps:&#47;&#47;github.com&#47;ipeaking&#47;scrapy_sina","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860313,"ip_address":"广东","comment_id":380646,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师，能提供一下stopword.txt和user_dict.txt，而且我在运行代码的时候出现了定义Segment","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631647,"discussion_content":"https://github.com/ipeaking/recommendation\n\nhttps://github.com/ipeaking/scrapy_sina","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860313,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2189310,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","nickname":"悟尘","note":"","ucode":"4E7E854340D3A4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633611,"discussion_content":"在这里面没找到user_dict.txt这个文件呀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702379367,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380643,"user_name":"Weitzenböck","can_delete":false,"product_type":"c1","uid":2669122,"ip_address":"江苏","ucode":"78C92583084ABA","user_header":"https://static001.geekbang.org/account/avatar/00/28/ba/42/5ca553bd.jpg","comment_is_top":false,"comment_ctime":1693924043,"is_pvip":false,"replies":[{"id":140015,"content":"https:&#47;&#47;github.com&#47;ipeaking&#47;recommendation\n\nhttps:&#47;&#47;github.com&#47;ipeaking&#47;scrapy_sina","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860347,"ip_address":"广东","comment_id":380643,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"这个课程真的是一点代码和资料的github都没有吗？","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631649,"discussion_content":"https://github.com/ipeaking/recommendation\n\nhttps://github.com/ipeaking/scrapy_sina","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860347,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374582,"user_name":"Geek_ccc0fd","can_delete":false,"product_type":"c1","uid":1461544,"ip_address":"广东","ucode":"DB53D576AEC020","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","comment_is_top":false,"comment_ctime":1684203144,"is_pvip":false,"replies":[{"id":136907,"content":"不错，可以推广给同学们","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685029782,"ip_address":"广东","comment_id":374582,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"我装的jieba==0.42.1可以直接对句子提取关键词，分词的部分已经封装在jieba代码里面了，修改了一下关键词提取代码：\nimport jieba\nfrom jieba.analyse import extract_tags\nfrom jieba.analyse import textrank\n\nclass KeywordModel(object):\n    def __init__(self):\n        jieba.load_userdict(&#39;..&#47;data&#47;user_dict.csv&#39;)\n        jieba.analyse.set_stop_words(&#39;..&#47;data&#47;stopWord.txt&#39;)\n\n    def get_keywords(self, sentence, type, topK=10, pos=(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;)):\n        &quot;&quot;&quot;\n        获取关键词\n        :param sentence: 文本\n        :param type: 使用哪种关键词算法，可选：tfidf,textrank\n        :param topK: 获取topK关键词\n        :param pos: 分词保留的词性类型，eg:(&#39;ns&#39;, &#39;n&#39;, &#39;vn&#39;, &#39;v&#39;)\n        :return:\n        &quot;&quot;&quot;\n        if type == &#39;tfidf&#39;:\n            tfidf_keywords = extract_tags(sentence, topK=topK, allowPOS=pos)\n            return tfidf_keywords\n        elif type == &#39;textrank&#39;:\n            textrank_keywords = textrank(sentence, topK=topK, allowPOS=pos)\n            return textrank_keywords\n\n    def keyword_interact(self, tfidf_keyword, textrank_keyword):\n        &quot;&quot;&quot;\n        关键词交集\n        :param tfidf_keyword:\n        :param textrank_keyword:\n        :return:\n        &quot;&quot;&quot;\n        return list(set(tfidf_keyword).intersection(set(textrank_keyword)))\n\n    def keyword_combine(self, tfidf_keyword, textrank_keyword):\n        &quot;&quot;&quot;\n        关键词并集\n        :param tfidf_keyword:\n        :param textrank_keyword:\n        :param k:\n        :return:\n        &quot;&quot;&quot;\n        combine = list(tfidf_keyword)\n        for word in textrank_keyword:\n            combine.append(word)\n        return list(set(combine))\n\n    def keyword_combine_topk(self, tfidf_keyword, textrank_keyword, k):\n        &quot;&quot;&quot;\n        关键词topk并集\n        :param tfidf_keyword:\n        :param textrank_keyword:\n        :param k:\n        :return:\n        &quot;&quot;&quot;\n        combine = list(tfidf_keyword[:k])\n        for word in textrank_keyword[:k]:\n            combine.append(word)\n        return list(set(combine))","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619365,"discussion_content":"不错，可以推广给同学们","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685029782,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1542987,"avatar":"https://static001.geekbang.org/account/avatar/00/17/8b/4b/15ab499a.jpg","nickname":"风轻扬","note":"","ucode":"DB972F2DF059C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650553,"discussion_content":"老哥,user_dict.csv，能分享一下吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725243961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374403,"user_name":"翡翠虎","can_delete":false,"product_type":"c1","uid":1448015,"ip_address":"广西","ucode":"2572E93C4C57A5","user_header":"https://static001.geekbang.org/account/avatar/00/16/18/4f/9e4d5591.jpg","comment_is_top":false,"comment_ctime":1684111601,"is_pvip":false,"replies":[{"id":136906,"content":"一般常用的停用词表可以在网上找到，或者找敏感词表，有些github上面也会公布，可以搜关键词stopwords。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685029755,"ip_address":"广东","comment_id":374403,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"有常用的停用词表吗","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619364,"discussion_content":"一般常用的停用词表可以在网上找到，或者找敏感词表，有些github上面也会公布，可以搜关键词stopwords。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685029755,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1448015,"avatar":"https://static001.geekbang.org/account/avatar/00/16/18/4f/9e4d5591.jpg","nickname":"翡翠虎","note":"","ucode":"2572E93C4C57A5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":619774,"discussion_content":"好的，谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685493054,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":619364,"ip_address":"广西","group_id":0},"score":619774,"extra":""}]}]},{"had_liked":false,"id":388004,"user_name":"Juha","can_delete":false,"product_type":"c1","uid":1347609,"ip_address":"北京","ucode":"5B9301CC960D84","user_header":"https://static001.geekbang.org/account/avatar/00/14/90/19/b3403815.jpg","comment_is_top":false,"comment_ctime":1709125459,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师，这里的from segment import Segment，segment怎么安装呀，通过pip install segment的这个包没有Segment","like_count":2},{"had_liked":false,"id":395746,"user_name":"Geek_33ecd9","can_delete":false,"product_type":"c1","uid":3969857,"ip_address":"中国香港","ucode":"1C148E0B953895","user_header":"","comment_is_top":false,"comment_ctime":1731916355,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"https:&#47;&#47;github.com&#47;ipeaking&#47;scrapy_sina\n这个github没有MongoDB相关的代码呀？","like_count":0}]}