{"id":97658,"title":"18 | Word Count：从零开始运行你的第一个Spark应用","content":"<p>你好，我是蔡元楠。</p><p>今天我们来从零开始运行你的第一个Spark应用。</p><p>我们先来回顾一下模块三的学习路径。</p><p>首先，我们由浅入深地学习了Spark的基本数据结构RDD，了解了它这样设计的原因，以及它所支持的API。</p><p>之后，我们又学习了Spark SQL的DataSet/DataFrame API，了解到它不仅提供类似于SQL query的接口，大大提高了开发者的工作效率，还集成了Catalyst优化器，可以提升程序的性能。</p><p>这些API应对的都是批处理的场景。</p><p>再之后，我们学习了Spark的流处理模块：Spark Streaming和Structured Streaming。两者都是基于微批处理（Micro batch processing）的思想，将流数据按时间间隔分割成小的数据块进行批处理，实时更新计算结果。</p><p>其中Structured Streaming也是使用DataSet/DataFrame API，这套API在某种程度上统一了批处理和流处理，是当前Spark最流行的工具，我们必需要好好掌握。</p><p>虽然学习了这么多API以及它们的应用，但是大部分同学还没有从零开始写一个完整的Spark程序，可能更没有运行Spark程序的经历。纸上谈兵并不能帮助我们在工作生活中用Spark解决实际问题。所以，今天我就和你一起做个小练习，从在本地安装Spark、配置环境开始，为你示范怎样一步步解决之前提到数次的统计词频（Word Count）的问题。</p><!-- [[[read_end]]] --><p>通过今天的学习，你可以收获：</p><ul>\n<li>怎样安装Spark以及其他相关的模块；</li>\n<li>知道什么是SparkContext、SparkSession；</li>\n<li>一个完整的Spark程序应该包含哪些东西；</li>\n<li>用RDD、DataFrame、Spark Streaming如何实现统计词频。</li>\n</ul><p>这一讲中，我们使用的编程语言是Python，操作系统是Mac OS X。</p><p>在这一讲以及之前文章的例子中，我们都是用Python作为开发语言。虽然原生的Spark是用Scala实现，但是在大数据处理领域中，我个人最喜欢的语言是Python。因为它非常简单易用，应用非常广泛，有很多的库可以方便我们开发。</p><p>当然Scala也很棒，作为一个函数式编程语言，它很容易用链式表达对数据集进行各种处理，而且它的运行速度是最快的，感兴趣的同学可以去学习一下。</p><p>虽然Spark还支持Java和R，但是我个人不推荐你使用。用Java写程序实在有些冗长，而且速度上没有优势。</p><p>操作系统选Mac OS X是因为我个人喜欢使用Macbook，当然Linux/Ubuntu也很棒。</p><h2>安装Spark</h2><p>首先，我们来简单介绍一下如何在本地安装Spark，以及用Python实现的Spark库——PySpark。</p><p>在前面的文章中，我们了解过，Spark的job都是JVM（Java Virtual Machine）的进程，所以在安装运行Spark之前，我们需要确保已经安装Java Developer Kit（JDK）。在命令行终端中输入：</p><pre><code>java -version\n</code></pre><p>如果命令行输出了某个Java的版本，那么说明你已经有JDK或者JRE在本地。如果显示无法识别这个命令，那么说明你还没有安装JDK。这时，你可以去<a href=\"https://www.oracle.com/technetwork/java/javase/downloads/index.html\">Oracle的官网</a>去下载安装JDK，然后配置好环境变量。</p><p>同样，我们需要确保Python也已经被安装在本地了。在命令行输入“Python”或者“Python3”，如果可以成功进入交互式的Python Shell，就说明已经安装了Python。否则，需要去<a href=\"https://www.python.org/downloads/\">Python官网</a>下载安装Python。这里，我推荐你使用Python3而不是Python2。</p><p>我们同样可以在本地预装好Hadoop。Spark可以脱离Hadoop运行，不过有时我们也需要依赖于HDFS和YARN。所以，这一步并不是必须的，你可以自行选择。</p><p>接下来我们就可以安装Spark。首先去<a href=\"https://spark.apache.org/downloads.html\">Spark官网</a>的下载界面。在第一个下拉菜单里选择最新的发布，第二个菜单最好选择与Hadoop 2.7兼容的版本。因为有时我们的Spark程序会依赖于HDFS和YARN，所以选择最新的Hadoop版本比较好。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/28/e934ae8f6f3f2394e1d14153953f4328.png?wh=1600*895\" alt=\"\"></p><p>下载好之后，解压缩Spark安装包，并且把它移动到/usr/local目录下，在终端中输入下面的代码。</p><pre><code>$ tar -xzf ~/Dowmloads/spark-2.4.3-bin-hadoop2.7.tg\n$ mv spark-2.4.3-bin-hadoop2.7.tgz /usr/local/spark\n</code></pre><p>经过上述步骤，从官网下载并安装Spark的文件，这样我们便完成了Spark的安装。但是，Spark也是要进行相应的环境变量配置的。你需要打开环境变量配置文件。</p><pre><code>vim ~/.bash_profile\n</code></pre><p>并在最后添加一段代码。</p><pre><code>export SPARK_HOME=/usr/local/spark\nexport PATH=$PATH:$SPARK_HOME/bin\n</code></pre><p>这样，所需的步骤都做完之后，我们在命令行控制台输入PySpark，查看安装情况。如果出现下面的欢迎标志，就说明安装完毕了。</p><pre><code>Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n      /_/\n\nUsing Python version 2.7.10 (default, Oct  6 2017 22:29:07)\nSparkSession available as 'spark'.\n&gt;&gt;&gt;\n</code></pre><h2>基于RDD API的Word Count程序</h2><p>配置好所需的开发环境之后，下一步就是写一个Python程序去统计词语频率。我们都知道这个程序的逻辑应该是如下图所示的。</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/2e/b0b16243323bb871959e9a86b803992e.jpg?wh=1768*506\" alt=\"\"></p><p>对于中间的先map再reduce的处理，我相信通过前面的学习，所有同学都可以用RDD或者DataFrame实现。</p><p>但是，我们对于Spark程序的入口是什么、如何用它读取和写入文件，可能并没有了解太多。所以，接下来让我们先接触一下Spark程序的入口。</p><p>在Spark 2.0之前，<strong>SparkContext</strong>是所有Spark任务的入口，它包含了Spark程序的基本设置，比如程序的名字、内存大小、并行处理的粒度等，Spark的驱动程序需要利用它来连接到集群。</p><p>无论Spark集群有多少个节点做并行处理，每个程序只可以有唯一的SparkContext，它可以被SparkConf对象初始化。</p><pre><code>conf = SparkConf().setAppName(appName).setMaster(master)\nsc = SparkContext(conf=conf)\n</code></pre><p>这个appName参数是一个在集群UI上展示应用程序的名称，master参数是一个Spark、Mesos 或YARN的集群URL，对于本地运行，它可以被指定为“local”。</p><p>在统计词频的例子中，我们需要通过SparkContext对象来读取输入文件，创建一个RDD，如下面的代码所示。</p><pre><code>text_file = sc.textFile(&quot;file://…...&quot;) //替换成实际的本地文件路径。\n</code></pre><p>这里的text_file是一个RDD，它里面的每一个数据代表原文本文件中的一行。</p><p>在这些版本中，如果要使用Spark提供的其他库，比如SQL或Streaming，我们就需要为它们分别创建相应的context对象，才能调用相应的API，比如的DataFrame和DStream。</p><pre><code>hc = HiveContext(sc)\nssc = StreamingContext(sc)\n</code></pre><p>在Spark 2.0之后，随着新的DataFrame/DataSet API的普及化，Spark引入了新的<strong>SparkSession</strong>对象作为所有Spark任务的入口。</p><p>SparkSession不仅有SparkContext的所有功能，它还集成了所有Spark提供的API，比如DataFrame、Spark Streaming和Structured Streaming，我们再也不用为不同的功能分别定义Context。</p><p>在统计词频的例子中，我们可以这样初始化SparkSession以及创建初始RDD。</p><pre><code>spark = SparkSession\n       .builder\n       .appName(appName)\n       .getOrCreate()\ntext_file = spark.read.text(&quot;file://….&quot;).rdd.map(lambda r: r[0])\n</code></pre><p>由于SparkSession的普适性，我推荐你尽量使用它作为你们Spark程序的入口。随后的学习中，我们会逐渐了解怎样通过它调用DataFrame和Streaming API。</p><p>让我们回到统计词频的例子。在创建好代表每一行文本的RDD之后，接下来我们便需要两个步骤。</p><ol>\n<li>把每行的文本拆分成一个个词语；</li>\n<li>统计每个词语的频率。</li>\n</ol><p>对于第一步，我们可以用flatMap去把行转换成词语。对于第二步，我们可以先把每个词语转换成（word, 1）的形式，然后用reduceByKey去把相同词语的次数相加起来。这样，就很容易写出下面的代码了。</p><pre><code>counts = lines.flatMap(lambda x: x.split(' '))\n                  .map(lambda x: (x, 1))\n                  .reduceByKey(add)\n</code></pre><p>这里counts就是一个包含每个词语的（word，count）pair的RDD。</p><p>相信你还记得，只有当碰到action操作后，这些转换动作才会被执行。所以，接下来我们可以用collect操作把结果按数组的形式返回并输出。</p><pre><code>output = counts.collect()\nfor (word, count) in output:\n    print(&quot;%s: %i&quot; % (word, count))\nspark.stop() // 停止SparkSession\n</code></pre><h2>基于DataFrame API的Word Count程序</h2><p>讲完基于RDD API的Word Count程序，接下来让我们学习下怎样用DataFrame API来实现相同的效果。</p><p>在DataFrame的世界中，我们可以把所有的词语放入一张表，表中的每一行代表一个词语，当然这个表只有一列。我们可以对这个表用一个groupBy()操作把所有相同的词语聚合起来，然后用count()来统计出每个group的数量。</p><p>但是问题来了，虽然Scala和Java支持对DataFrame进行flatMap操作，但是Python并不支持。那么要怎样把包含多个词语的句子进行分割和拆分呢？这就要用到两个新的操作——explode和split。split是pyspark.sql.functions库提供的一个函数，它作用于DataFrame的某一列，可以把列中的字符串按某个分隔符分割成一个字符串数组。</p><p>explode同样是pyspark.sql.functions库提供的一个函数，通俗点的翻译是“爆炸”，它也作用于DataFrame的某一列，可以为列中的数组或者map中每一个元素创建一个新的Row。</p><p>由于之前代码中创建的df_lines这个DataFrame中，每一行只有一列，每一列都是一个包含很多词语的句子，我们可以先对这一列做split，生成一个新的列，列中每个元素是一个词语的数组；再对这个列做explode，可以把数组中的每个元素都生成一个新的Row。这样，就实现了类似的flatMap功能。这个过程可以用下面的三个表格说明。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/55/c9ebf1f324a73539a2a57ce8151e4455.png?wh=3410*3180\" alt=\"\"></p><p>接下来我们只需要对Word这一列做groupBy，就可以统计出每个词语出现的频率，代码如下。</p><pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nif __name__ == &quot;__main__&quot;:\n   spark = SparkSession\n       .builder\n       .appName(‘WordCount’)\n       .getOrCreate()\n   lines = spark.read.text(&quot;sample.txt&quot;)\n   wordCounts = lines\n       .select(explode(split(lines.value, &quot; &quot;))\n       .alias(&quot;word&quot;))\n       .groupBy(&quot;word&quot;)\n       .count()\n   wordCounts.show()\n   \n   spark.stop()\n</code></pre><p>从这个例子，你可以很容易看出使用DataSet/DataFrame API的便利性——我们不需要创建（word, count）的pair来作为中间值，可以直接对数据做类似SQL的查询。</p><h2>小结</h2><p>通过今天的学习，我们掌握了如何从零开始创建一个简单的Spark的应用程序，包括如何安装Spark、如何配置环境、Spark程序的基本结构等等。</p><h2>实践题</h2><p>希望你可以自己动手操作一下，这整个过程只需要跑通一次，以后就可以脱离纸上谈兵，真正去解决实际问题。</p><p>欢迎你在留言中反馈自己动手操作的效果。</p><p>如果你跑通了，可以在留言中打个卡。如果遇到了问题，也请你在文章中留言，与我和其他同学一起讨论。</p><p></p>","neighbors":{"left":{"article_title":"17 | Structured Streaming：如何用DataFrame API进行实时数据分析?","id":97121},"right":{"article_title":"19 | 综合案例实战：处理加州房屋信息，构建线性回归模型","id":98374}},"comments":[{"had_liked":false,"id":98741,"user_name":"朱同学","can_delete":false,"product_type":"c1","uid":1514233,"ip_address":"","ucode":"2EF7D5A051712C","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/f9/180f347a.jpg","comment_is_top":false,"comment_ctime":1559089212,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"83163467836","product_id":100025301,"comment_content":"java万金油，什么都可以干，人好招，特别是我们这种偏远地区，scala，虽然开发效率高，但是人少，难招，所以我们大数据团队选择了java。至于运行效率，py是最慢的，java和scala应该半斤八俩吧","like_count":20},{"had_liked":false,"id":105523,"user_name":"科学Jia","can_delete":false,"product_type":"c1","uid":1080409,"ip_address":"","ucode":"95430413F82A69","user_header":"https://static001.geekbang.org/account/avatar/00/10/7c/59/26b1e65a.jpg","comment_is_top":false,"comment_ctime":1561020806,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"40215726470","product_id":100025301,"comment_content":"女同学看完2015年出的spark快速大数据分析这本书以后，再来看老师写的这些文字，觉得言简意赅，印象深刻，至于用什么语言倒无所谓了，主要是思路。后期希望老师能多说一些案例和处理中需要注意的技巧。","like_count":9},{"had_liked":false,"id":98864,"user_name":"一","can_delete":false,"product_type":"c1","uid":1220750,"ip_address":"","ucode":"28E0605EA1AE88","user_header":"https://static001.geekbang.org/account/avatar/00/12/a0/8e/6e4c7509.jpg","comment_is_top":false,"comment_ctime":1559109597,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31623880669","product_id":100025301,"comment_content":"看了这一讲意识到之前对Python欠缺了重视，现在明白Python在大数据处理领域是很有竞争力的，因为Spark和众多的库的原因，甚至超越Java，所以现在要重新重视起来Python的学习了","like_count":7},{"had_liked":false,"id":101814,"user_name":"hallo128","can_delete":false,"product_type":"c1","uid":1212044,"ip_address":"","ucode":"3921D6E11CFCB1","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","comment_is_top":false,"comment_ctime":1559981251,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27329785027","product_id":100025301,"comment_content":"【以下代码可以运行，但对df格式的操作是借助二楼的网址去找的，具体含义也不太清楚，只是可以运行出来】<br><br>#python前运行调用包<br>from pyspark.sql import SparkSession<br>from pyspark.sql.functions import explode<br>from pyspark.sql.functions import split<br><br>#初始化SparkSession程序入口<br>spark = SparkSession.builder.appName(&quot;WordCount&quot;).getOrCreate()<br>#读入文档<br>ds_lines = spark.read.text(&quot;&#47;Users&#47;apple&#47;code_tool&#47;spark&#47;WordCount&#47;demo.md&quot;)<br>#针对df特定的计算格式<br>words = ds_lines.select(<br>   explode(<br>       split(ds_lines.value, &quot; &quot;)<br>   ).alias(&quot;word&quot;)<br>)<br>#返回的RDD进行计数<br>wordCounts = words.groupBy(&quot;word&quot;).count()<br>#展示<br>wordCounts.show()<br>#关闭spark<br>spark.stop()","like_count":6},{"had_liked":false,"id":98815,"user_name":"9527","can_delete":false,"product_type":"c1","uid":1032735,"ip_address":"","ucode":"04B51C09E3C7B8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c2/1f/343f2dec.jpg","comment_is_top":false,"comment_ctime":1559099645,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"27328903421","product_id":100025301,"comment_content":"spark_session = SparkSession.builder.appName(&quot;PySparkShell&quot;).getOrCreate()<br>ds_lines = spark_session.read.textFile(&quot;README.md&quot;)<br>ds = ds_lines.flatMap(lambda x: x.split(&#39; &#39;)).groupBy(&quot;Value&quot;).count()<br>ds.show()<br><br>我执行这段的时候报错了<br>AttributeError: &#39;DataFrameReader&#39; object has no attribute &#39;textFile&#39;<br>如果把textFile()改成text()就对了<br>再执行flatMap那段，也报错了<br>AttributeError: &#39;DataFrame&#39; object has no attribute &#39;flatMap&#39;<br>是不是API变动了，我用的是2.4.3版本单机执行的","like_count":6,"discussions":[{"author":{"id":1231677,"avatar":"https://static001.geekbang.org/account/avatar/00/12/cb/3d/1c582da8.jpg","nickname":"心有双木","note":"","ucode":"7A625AB9F03C9F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":9849,"discussion_content":"作者都说了，Python不支持flatMap","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568218855,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":101786,"user_name":"hallo128","can_delete":false,"product_type":"c1","uid":1212044,"ip_address":"","ucode":"3921D6E11CFCB1","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","comment_is_top":false,"comment_ctime":1559971315,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"18739840499","product_id":100025301,"comment_content":"“虽然 Spark 还支持 Java 和 R，但是我个人不推荐你使用。用 Java 写程序实在有些冗长，而且速度上没有优势。”<br>推荐使用，还是应该详细说明对比下，不能只因为自己偏好某种工具给出建议。对于spark原生来说，速度和库同步更新更快的是Scala，如果你想随时用到spark最新功能库的话，就应该选择Scala，同时速度也是最快的。<br>至于Python，R，Java，一方面和你的熟悉程度有关，另一方面也与你到底准备用spark来做什么的目的有关。是集群控制，还是数据分析，还是建模，来选择合适的编程语言与spark进行连接编写。","like_count":4,"discussions":[{"author":{"id":1110662,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f2/86/d689f77e.jpg","nickname":"Hank_Yan","note":"","ucode":"86899B561C502B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289633,"discussion_content":"有道理，像我主要做数据分析的工作，scala相关的库比较少。还是倾向于python，因为有一些在mr时代的积累。不过scala肯定是更新最及时的，应该也得掌握。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594169467,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":103342,"user_name":"Quincy","can_delete":false,"product_type":"c1","uid":1194412,"ip_address":"","ucode":"FFFBB9FFCB98A4","user_header":"https://static001.geekbang.org/account/avatar/00/12/39/ac/76beadf1.jpg","comment_is_top":false,"comment_ctime":1560417539,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14445319427","product_id":100025301,"comment_content":"Spark 不应该是首选Scala 么","like_count":3},{"had_liked":false,"id":99628,"user_name":"青石","can_delete":false,"product_type":"c1","uid":1215531,"ip_address":"","ucode":"B0056AD6453322","user_header":"https://static001.geekbang.org/account/avatar/00/12/8c/2b/3ab96998.jpg","comment_is_top":false,"comment_ctime":1559269522,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14444171410","product_id":100025301,"comment_content":"#!&#47;usr&#47;bin&#47;python3<br><br>import os<br>from pyspark import SparkContext, SparkConf<br><br><br>os.environ[&#39;SPARK_HOME&#39;] = &#39;&#47;usr&#47;local&#47;spark&#39;<br>os.environ[&#39;HADOOP_HOME&#39;] = &#39;&#47;usr&#47;local&#47;hadoop-2.7.7&#39;<br><br>conf = SparkConf().setAppName(&#39;WordCount&#39;).setMaster(&#39;local&#39;)<br>sc = SparkContext(&#39;local&#39;, &#39;pyspark&#39;, conf=conf)<br><br>text_file = sc.textFile(&#39;file:&#47;&#47;&#47;Users&#47;albert.ming.xu&#47;Downloads&#47;text.txt&#39;)<br><br>counts = text_file.filter(lambda x: len(x.strip()) &gt; 0).flatMap(lambda x: x.split(&#39; &#39;)).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)<br><br>print(&#39;|{0: ^20}|{1: ^20}|&#39;.format(&#39;Word&#39;, &#39;Count&#39;))<br>for (word, num) in counts.take(10):<br>    print(&#39;|{0: ^20}|{1: ^20}|&#39;.format(word, num))<br><br>","like_count":3},{"had_liked":false,"id":99105,"user_name":"J Zhang","can_delete":false,"product_type":"c1","uid":1049115,"ip_address":"","ucode":"A64B2E61A8B6A8","user_header":"https://static001.geekbang.org/account/avatar/00/10/02/1b/e1e89267.jpg","comment_is_top":false,"comment_ctime":1559143233,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14444045121","product_id":100025301,"comment_content":"用java写 有点冗长  我不敢苟同，因为java8 已经是函数编程了！而且spark开发我觉得大部分还是spark  sql多点！这样基本没啥区别  ","like_count":3},{"had_liked":false,"id":98909,"user_name":"这个名字居然都有","can_delete":false,"product_type":"c1","uid":1476984,"ip_address":"","ucode":"8972F5BF888261","user_header":"https://static001.geekbang.org/account/avatar/00/16/89/78/311dbb8b.jpg","comment_is_top":false,"comment_ctime":1559115214,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14444017102","product_id":100025301,"comment_content":"老师，你给一个完整的案例吧，","like_count":3,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2913,"discussion_content":"https://github.com/xiumuzjq/spark-exercise/tree/master/18/python3","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564037119,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":99075,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1559139605,"is_pvip":false,"replies":[{"id":"42764","content":"SparkSession.read.text()读取文件后生成的DataFrame只有一列，它的默认名字就是“value”。我们用lines.value去读取这一列，是同样的道理。之后我们给新的列重命名为”word”，所以groupBy的参数变成了”word”。","user_name":"作者回复","user_name_real":"廿七","uid":"1386753","ctime":1563948298,"ip_address":"","comment_id":99075,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10149074197","product_id":100025301,"comment_content":".groupBy(&quot;Value&quot;)这个value是什么意思？","like_count":2,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451963,"discussion_content":"SparkSession.read.text()读取文件后生成的DataFrame只有一列，它的默认名字就是“value”。我们用lines.value去读取这一列，是同样的道理。之后我们给新的列重命名为”word”，所以groupBy的参数变成了”word”。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563948298,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":98831,"user_name":"大志","can_delete":false,"product_type":"c1","uid":1039047,"ip_address":"","ucode":"648335930C2E55","user_header":"https://static001.geekbang.org/account/avatar/00/0f/da/c7/66f5fcea.jpg","comment_is_top":false,"comment_ctime":1559101795,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10149036387","product_id":100025301,"comment_content":"老师，本地已经安装了Spark，有Demo吗，只看代码片段的话还是无从下手啊","like_count":2,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2914,"discussion_content":"https://github.com/xiumuzjq/spark-exercise/tree/master/18/python3","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1564037131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129966,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1567341418,"is_pvip":false,"replies":[{"id":"48471","content":"嗯，这位同学说的很好，用pip install安装pyspark确实方便。我介绍的方法比较普遍试用。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567372157,"ip_address":"","comment_id":129966,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5862308714","product_id":100025301,"comment_content":"python 直接安装<br>pip install pyspark<br>pip帮你搞定一切安装配置问题。<br>参考资料：<br>https:&#47;&#47;pypi.org&#47;project&#47;pyspark&#47;","like_count":1,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465740,"discussion_content":"嗯，这位同学说的很好，用pip install安装pyspark确实方便。我介绍的方法比较普遍试用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567372157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1127175,"avatar":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","nickname":"JustDoDT","note":"","ucode":"6AF0B80F00EAEF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7135,"discussion_content":"pip install pyspark\nLooking in indexes: http://mirrors.aliyun.com/pypi/simple/\nCollecting pyspark\n  Downloading http://mirrors.aliyun.com/pypi/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n     |████████████████████████████████| 215.7MB 959kB/s \nCollecting py4j==0.10.7 (from pyspark)\n  Downloading http://mirrors.aliyun.com/pypi/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n     |████████████████████████████████| 204kB 1.1MB/s \nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... done\n  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130426 sha256=858f78c54ac8dfed77b01d6670cf5f70e091c173dd39b5684bded524986f6cf9\n  Stored in directory: /Users/cool/Library/Caches/pip/wheels/ed/4f/a4/4d0a0b5d12fcd76fee6b8f572ce005b55ee0f489e2cb898855\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.4\n包不小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567397895,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":98916,"user_name":"Bing","can_delete":false,"product_type":"c1","uid":1059579,"ip_address":"","ucode":"1A9745BBF0CDDB","user_header":"https://static001.geekbang.org/account/avatar/00/10/2a/fb/e2b29825.jpg","comment_is_top":false,"comment_ctime":1559116002,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5854083298","product_id":100025301,"comment_content":"flatMap是rdd的算子，df不能直接用，可以explode行转列","like_count":1},{"had_liked":false,"id":328328,"user_name":"xxx","can_delete":false,"product_type":"c1","uid":1096652,"ip_address":"","ucode":"E79CEA70430449","user_header":"https://static001.geekbang.org/account/avatar/00/10/bb/cc/fac12364.jpg","comment_is_top":false,"comment_ctime":1640671185,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1640671185","product_id":100025301,"comment_content":"文中的示例是可以运行的，稍微改改：<br><br>from pyspark.sql import SparkSession<br>from pyspark.sql.functions import *<br><br>if __name__ == &quot;__main__&quot;:<br>    spark = SparkSession.builder.appName(&#39;WordCount&#39;).getOrCreate()<br>    lines = spark.read.text(&quot;wikiOfSpark.txt&quot;)<br>    wordCounts = lines.select(explode(split(lines.value, &quot; &quot;)).alias(&quot;word&quot;)).groupBy(&quot;word&quot;).count().sort(desc(&quot;count&quot;))<br>    wordCounts.show()<br><br>    spark.stop()","like_count":1},{"had_liked":false,"id":285562,"user_name":"stars","can_delete":false,"product_type":"c1","uid":1055100,"ip_address":"","ucode":"8A75D9E1909729","user_header":"https://static001.geekbang.org/account/avatar/00/10/19/7c/25abe455.jpg","comment_is_top":false,"comment_ctime":1616899202,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1616899202","product_id":100025301,"comment_content":"前面还好，到这里看不懂了，环境搭建完成，代码怎么执行，完全走不下去，是不是简单说一下","like_count":0},{"had_liked":false,"id":278294,"user_name":"黑黑白","can_delete":false,"product_type":"c1","uid":1177260,"ip_address":"","ucode":"42DC6725B03F53","user_header":"https://static001.geekbang.org/account/avatar/00/11/f6/ac/a7da8788.jpg","comment_is_top":false,"comment_ctime":1612859728,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1612859728","product_id":100025301,"comment_content":"from operator import add<br><br>from pyspark.sql import SparkSession<br><br>if __name__ == &quot;__main__&quot;:<br>    spark = SparkSession.builder.appName(&quot;rdd&quot;)\\<br>        .config(&quot;spark.driver.bindAddress&quot;, &quot;127.0.0.1&quot;)\\<br>        .getOrCreate()<br>    lines = spark.read.text(&quot;file:&#47;&#47;&#47;mnt&#47;d&#47;playground&#47;bigdata&#47;spark001&#47;sample.txt&quot;)\\<br>        .rdd.map(lambda r: r[0])<br>    counts = lines.flatMap(lambda x: x.split(&#39; &#39;)).map(lambda x: (x, 1)).reduceByKey(add)<br>    output = counts.collect()<br><br>    for (word, count) in output:<br>        print(&quot;%s: %i&quot; % (word, count))<br>    spark.stop()","like_count":0},{"had_liked":false,"id":265112,"user_name":"whatever","can_delete":false,"product_type":"c1","uid":2271248,"ip_address":"","ucode":"D6A8BB06AB6731","user_header":"https://static001.geekbang.org/account/avatar/00/22/a8/10/ee8b8c0b.jpg","comment_is_top":false,"comment_ctime":1606785127,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1606785127","product_id":100025301,"comment_content":"给所有小白到我这个程度踩了初级坑的人：<br><br>如果运行pyspark报错 ERROR SparkContext: Error initializing SparkContext，说明需要修改主机名<br>vim ~&#47;.bash_profile<br>添加<br>export SPARK_LOCAL_HOSTNAME=localhost<br>编辑完后重新加载，执行<br>source ~&#47;.bash_profile<br>再运行pyspark试试","like_count":0},{"had_liked":false,"id":193680,"user_name":"娄江国","can_delete":false,"product_type":"c1","uid":1018479,"ip_address":"","ucode":"6C2AAE4E409286","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8a/6f/3d4f7e31.jpg","comment_is_top":false,"comment_ctime":1584947933,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584947933","product_id":100025301,"comment_content":"在pyspark中执行的程序，为什么在spark的管控台上，看不到对应的Application呢？","like_count":0},{"had_liked":false,"id":187339,"user_name":"刘润森","can_delete":false,"product_type":"c1","uid":1236556,"ip_address":"","ucode":"84101C670A6747","user_header":"https://static001.geekbang.org/account/avatar/00/12/de/4c/a51ece16.jpg","comment_is_top":false,"comment_ctime":1584087613,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584087613","product_id":100025301,"comment_content":"我在centos搭建Spark集群，怎么用Pyspark在window连接spark集群","like_count":0},{"had_liked":false,"id":130185,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1567398773,"is_pvip":false,"replies":[{"id":"48577","content":"👍🏻","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567411628,"ip_address":"","comment_id":130185,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1567398773","product_id":100025301,"comment_content":"实践成功<br>demo.txt:<br>I hava a dog<br>He has a Dog<br>RDD写法：<br>[(&#39;I&#39;, 1), (&#39;hava&#39;, 1), (&#39;a&#39;, 2), (&#39;dog&#39;, 1), (&#39;He&#39;, 1), (&#39;has&#39;, 1), (&#39;Dog&#39;, 1)]<br>[(&#39;a&#39;, 2), (&#39;I&#39;, 1), (&#39;hava&#39;, 1), (&#39;dog&#39;, 1), (&#39;He&#39;, 1), (&#39;has&#39;, 1), (&#39;Dog&#39;, 1)]<br>DF写法：<br>[Row(word=&#39;dog&#39;, count=1), Row(word=&#39;He&#39;, count=1), Row(word=&#39;Dog&#39;, count=1), Row(word=&#39;I&#39;, count=1), Row(word=&#39;a&#39;, count=2), Row(word=&#39;hava&#39;, count=1), Row(word=&#39;has&#39;, count=1)]<br>[Row(word=&#39;a&#39;, count=2), Row(word=&#39;I&#39;, count=1), Row(word=&#39;Dog&#39;, count=1), Row(word=&#39;hava&#39;, count=1), Row(word=&#39;dog&#39;, count=1), Row(word=&#39;has&#39;, count=1), Row(word=&#39;He&#39;, count=1)]<br><br>从启动到出结果，DF写法速度要比rdd慢。","like_count":0,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465847,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567411628,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":119533,"user_name":"西北偏北","can_delete":false,"product_type":"c1","uid":1043160,"ip_address":"","ucode":"64BD69C84EE6A1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132","comment_is_top":false,"comment_ctime":1564623070,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564623070","product_id":100025301,"comment_content":"对于词频统计的场景，除了用map reduce的方式，对应到sql就是group by，基于每个单词分组然后统计每个分组的大小","like_count":0},{"had_liked":false,"id":108504,"user_name":"Qi Liu 刘祺","can_delete":false,"product_type":"c1","uid":1235615,"ip_address":"","ucode":"F93B7A8CAB2DCE","user_header":"https://static001.geekbang.org/account/avatar/00/12/da/9f/460284b9.jpg","comment_is_top":false,"comment_ctime":1561788048,"is_pvip":false,"replies":[{"id":"40733","content":"加油⛽️","user_name":"作者回复","user_name_real":"唱唱","uid":"1386753","ctime":1562670180,"ip_address":"","comment_id":108504,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1561788048","product_id":100025301,"comment_content":"继续学习~","like_count":0,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":456012,"discussion_content":"加油⛽️","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562670180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":99127,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1559147454,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559147454","product_id":100025301,"comment_content":"老师我想问一下，如果大数据学习用python、java、还是Scala？<br>python虽然代码少，但不是说性能上，运行速度上不及java和go吗？","like_count":0},{"had_liked":false,"id":98917,"user_name":"fresh","can_delete":false,"product_type":"c1","uid":1075101,"ip_address":"","ucode":"9FA6DCFC44DBE1","user_header":"https://static001.geekbang.org/account/avatar/00/10/67/9d/25650474.jpg","comment_is_top":false,"comment_ctime":1559116057,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559116057","product_id":100025301,"comment_content":"能用java 写代码吗？","like_count":0},{"had_liked":false,"id":98900,"user_name":"许童童","can_delete":false,"product_type":"c1","uid":1003005,"ip_address":"","ucode":"4B799C0C6BC678","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4d/fd/0aa0e39f.jpg","comment_is_top":false,"comment_ctime":1559114508,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1559114508","product_id":100025301,"comment_content":"环境搭好了，下一步不知道怎么操作了。","like_count":0,"discussions":[{"author":{"id":1524861,"avatar":"https://static001.geekbang.org/account/avatar/00/17/44/7d/0ad53639.jpg","nickname":"stone","note":"","ucode":"B8A268EB1CD5CA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":445,"discussion_content":"me 2, 不过作为一个搞前端的，本来也是来熟悉下概念的 ：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561560758,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}