{"id":224344,"title":"22 | 面对海量数据，如何才能查得更快?","content":"<p>你好，我是李玥。</p><p>我们接着上节课的话题，来继续说海量数据。上节课我们讲了，如何来保存原始数据，那我们知道，原始数据的数据量太大了，能存下来就很不容易了，这个数据是没法直接来给业务系统查询和分析的。有两个原因，一是数据量太大了，二是也没有很好的数据结构和查询能力，来支持业务系统查询。</p><p>所以一般的做法是，用流计算或者是批计算，把原始数据再进行一次或者多次的过滤、汇聚和计算，把计算结果落到另外一个存储系统中去，由这个存储再给业务系统提供查询支持。这里的“流计算”，指的是Flink、Storm这类的实时计算，批计算是Map-Reduce或者Spark这类的非实时计算。</p><p>上节课我们说过，像点击流、监控和日志这些原始数据是“海量数据中的海量数据”，这些原始数据经过过滤汇总和计算之后，大多数情况下数据量会有量级的下降，比如说从TB级别的数据量，减少到GB级别。</p><p>有的业务，计算后的数据非常少，比如说一些按天粒度的汇总数据，或者排行榜类的数据，用什么存储都能满足要求。那有一些业务，没法通过事先计算的方式解决全部的问题。原始数据经过计算后产生的计算结果，数据量相比原始数据会减少一些，但仍然是海量数据。并且，我们还要在这个海量数据上，提供性能可以接受的查询服务。</p><!-- [[[read_end]]] --><p>今天这节课我们就来聊一聊，面对这样的海量数据，如何才能让查询更快一些。</p><h2>常用的分析类系统应该如何选择存储？</h2><p>查询海量数据的系统，大多都是离线分析类系统，你可以简单地理解为类似于做报表的系统，也就是那些主要功能是对数据做统计分析的系统。这类系统是重度依赖于存储的。选择什么样的存储系统、使用什么样的数据结构来存储数据，直接决定了数据查询、聚合和分析的性能。</p><p>分析类系统对存储的需求一般是这样的：</p><ol>\n<li>一般用于分析的数据量都会比在线业务大出几个数量级，这需要存储系统能保存海量数据；</li>\n<li>能在海量的数据上做快速的聚合、分析和查询。注意这里面所说的“快速”，前提是处理GB、TB甚至PB级别的海量数据，在这么大的数据量上做分析，几十秒甚至几分钟都算很快了，和在线业务要求的毫秒级速度是不一样的；</li>\n<li>由于数据大多数情况下都是异步写入，对于写入性能和响应时延，一般要求不高；</li>\n<li>分析类系统不直接支撑前端业务，所以也不要求高并发。</li>\n</ol><p>然后我们看有哪些可供选择的存储产品。如果你的系统的数据量在GB量级以下，MySQL仍然是可以考虑的，因为它的查询能力足以应付大部分分析系统的业务需求。并且可以和在线业务系统合用一个数据库，不用做ETL（数据抽取），省事儿并且实时性好。这里还是要提醒你，最好给分析系统配置单独的MySQL实例，避免影响线上业务。</p><p>如果数据量级已经超过MySQL极限，可以选择一些列式数据库，比如：HBase、Cassandra、ClickHouse，这些产品对海量数据，都有非常好的查询性能，在正确使用的前提下，10GB量级的数据查询基本上可以做到秒级返回。高性能的代价是功能上的缩水，这些数据库对数据的组织方式都有一些限制，查询方式上也没有MySQL那么灵活。大多都需要你非常了解这些产品的脾气秉性，按照预定的姿势使用，才能达到预期的性能。</p><p>另外一个值得考虑的选择是Elasticsearch（ES），ES本来是一个为了搜索而生的存储产品，但是也支持结构化数据的存储和查询。由于它的数据都存储在内存中，并且也支持类似于Map-Reduce方式的分布式并行查询，所以对海量结构化数据的查询性能也非常好。</p><p>最重要的是，ES对数据组织方式和查询方式的限制，没有其他列式数据库那么死板。也就是说，ES的查询能力和灵活性是要强于上述这些列式数据库的。在这个级别的几个选手中，我个人强烈建议你优先考虑ES。但是ES有一个缺点，就是你需要给它准备大内存的服务器，硬件成本有点儿高。</p><p>数据量级超过TB级的时候，对这么大量级的数据做统计分析，无论使用什么存储系统，都快不到哪儿去。这个时候的性能瓶颈已经是磁盘IO和网络带宽了。这种情况下，实时的查询和分析肯定做不了。解决的办法都是，定期把数据聚合和计算好，然后把结果保存起来，在需要时对结果再进行二次查询。这么大量级的数据，一般都选择保存在HDFS中，配合Map-Reduce、Spark、Hive等等这些大数据生态圈产品做数据聚合和计算。</p><h2>转变你的思想：根据查询来选择存储系统</h2><p>面对海量数据，仅仅是根据数据量级来选择存储系统，是远远不够的。</p><p>经常有朋友会问：“我的系统，每天都产生几个GB的数据量，现在基本已经慢得查不出来了，你说我换个什么数据库能解决问题呢？”那我的回答都是，对不起，换什么数据库也解决不了你的问题。为什么这么说呢？</p><p>因为在过去的几十年里面，存储技术和分布式技术，在基础理论方面并没有什么本质上突破。技术发展更多的是体现在应用层面上，比如说，集群管理简单，查询更加自动化，像Map-Reduce这些。不同的存储系统之间，并没有本质的差异。它们的区别只是，存储引擎的数据结构、存储集群的构建方式，以及提供的查询能力，这些方面的差异。这些差异，使得每一种存储，在它擅长的一些领域或者场景下，会有很好的性能表现。</p><p>比如说，最近很火的RocksDB、LevelDB，它们的存储结构LSM-Tree，其实就是日志和跳表的组合，单从数据结构的时间复杂度上来说，和“老家伙”MySQL采用的B+树，有本质的提升吗？没有吧，时间复杂度都是O(log n)。但是，LSM-Tree在某些情况下，它利用日志有更好的写性能表现。没有哪种存储能在所有情况下，都具有明显的性能优势，所以说，<strong>存储系统没有银弹，<strong><strong>不要指望简单</strong></strong>地<strong><strong>更换一种数据库</strong></strong>，就可以解决数据量大，查询慢的问题。</strong></p><p>但是，在特定的场景下，通过一些优化方法，把查询性能提升几十倍甚至几百倍，这个都是有可能的。这里面有个很重要的思想就是，<strong>根据查询来选择存储系统和数据结构</strong>。我们前面的课程《<a href=\"https://time.geekbang.org/column/article/208675\">06 | 如何用Elasticsearch构建商品搜索系统</a>》，就是把这个思想实践得很好的一个例子。ES采用的倒排索引的数据结构，并没有比MySQL的B+树更快或者说是更先进，但是面对“全文搜索”这个查询需求，选择使用ES的倒排索引，就比使用其他的存储系统和数据结构，性能上要高出几十倍。</p><p>再举个例子，大家都知道，京东的物流速度是非常快的。经常是，一件挺贵的衣服，下单之后，还没来得及后悔，已经送到了。京东的物流之所以能做到这么快，有一个很重要的原因是，它有一套智能的补货系统，根据历史的物流数据，对未来的趋势做出预测，来给全国每个仓库补货。这样京东就可以做到，你下单买的商品，很大概率在离你家几公里那个京东仓库里就有货，这样自然很快就送到了。这个系统的背后，它需要分析每天几亿条物流数据，每条物流数据又细分为几段到几十段，那每天的物流数据就是几十亿的量级。</p><p>这份物流数据，它的用途也非常多，比如说，智能补货系统要用；调度运力的系统也要用；评价每个站点儿、每个快递小哥的时效达成情况，还要用这个数据；物流规划人员同样要用这个数据进行分析，对物流网络做持续优化。</p><p>那用什么样的存储系统保存这些物流数据，才能满足这些查询需求呢？显然，任何一种存储系统，都满足不了这么多种查询需求。我们需要根据每一种需求，去专门选择合适的存储系统，定义适合的数据结构，各自解决各自的问题。而不是用一种数据结构，一个数据库去解决所有的问题。</p><p>对于智能补货和运力调度这两个系统，它的区域性很强，那我们可以把数据按照区域（省或者地市）做分片，再汇总一份全国的跨区物流数据，这样绝大部分查询都可以落在一个分片上，查询性能就会很好。</p><p>对于站点儿和人的时效达成情况，这种业务的查询方式以点查询为主，那可以考虑事先在计算的时候，按照站点儿和人把数据汇总好，存放到一些分布式KV存储中，基本上可以做到毫秒级查询性能。而对于物流规划的查询需求，查询方式是多变的，可以把数据放到Hive表中，按照时间进行分片。</p><p>我们之前也讲到过，按照时间分片是对查询最友好的分片方式。物流规划人员可以在上面执行一些分析类的查询任务，一个查询任务即使是花上几个小时，用来验证一个新的规划算法，也是可以接受的。</p><h2>小结</h2><p>海量数据的主要用途就是支撑离线分析类业务的查询，根据数据量规模不同，由小到大可以选择：关系型数据库，列式数据库和一些大数据存储系统。对于TB量级以下的数据，如果可以接受相对比较贵的硬件成本，ES是一个不错的选择。</p><p>对于海量数据来说，选择存储系统没有银弹，重要的是转变思想，根据业务对数据的查询方式，反推数据应该使用什么存储系统、如何分片，以及如何组织。即使是同样一份数据，也要根据不同的查询需求，组织成不同的数据结构，存放在适合的存储系统中，才能在每一种业务中都达到理想的查询性能。</p><h2>思考题</h2><p>今天的课后思考题是这样的，我们要做一个日志系统，收集全公司所有系统的全量程序日志，给开发和运维人员提供日志的查询和分析服务，你会选择用什么存储系统来存储这些日志？原因是什么？欢迎你在留言区与我讨论。</p><p>感谢你的阅读，如果你觉得今天的内容对你有帮助，也欢迎把它分享给你的朋友。</p>","neighbors":{"left":{"article_title":"21 | 类似“点击流”这样的海量数据应该如何存储？","id":224162},"right":{"article_title":"23 | MySQL经常遇到的高可用、分片问题，NewSQL是如何解决的？","id":225398}},"comments":[{"had_liked":false,"id":207106,"user_name":"李玥","can_delete":false,"product_type":"c1","uid":1501046,"ip_address":"","ucode":"B19E91EE248591","user_header":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","comment_is_top":true,"comment_ctime":1587007060,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"9.2233721587009004e+18","product_id":100046801,"comment_content":"Hi，我是李玥。<br><br>这里回顾一下上节课的思考题：<br><br>课后请你想一下，为什么 Kafka 能做到几倍于 HDFS 的吞吐能力，技术上的根本原因是什么？<br><br>答案：<br><br>这个问题的最根本原因是，对于磁盘来说，顺序读写的性能要远远高于随机读写，这个性能差距视不同的磁盘，大约在几十倍左右。Kafka是为顺序读写设计的，儿HDFS是为随机读写的设计的，所以在顺序写入的时候，Kafka的性能会更好。","like_count":29},{"had_liked":false,"id":207114,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1587008233,"is_pvip":true,"discussion_count":2,"race_medal":0,"score":"139025961705","product_id":100046801,"comment_content":"对于思考题，会选择 ES 作为存储系统，这里是因为是<br>1: 日志一般是根据时间线来保存的，而且不用保存历史的数据，只需要保存最近 15天或者 7天的数据就可以满足要求，数量量不是很大<br>2: 查看日志的时候一般都会使用全文搜索， ES 可以高效的支持全文搜索","like_count":33,"discussions":[{"author":{"id":1333572,"avatar":"https://static001.geekbang.org/account/avatar/00/14/59/44/0fe24fc3.jpg","nickname":"清月","note":"","ucode":"D0CC2A7BF2056A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":335152,"discussion_content":"选择具体的存储我觉得还要考虑具体的业务场景。\n比如这里提到的ES，不考虑资源方面的问题，首先得弄清ES的数据模式，他是一种高内聚自包含的数据模式，所以要搞清楚数据结构。同时在这种模式下ES在做多对多关联查询的时候，支持的就不是很友好。\n这是个人的粗见，有不对之处还请指正。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608106928,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1435733,"avatar":"https://static001.geekbang.org/account/avatar/00/15/e8/55/92f82281.jpg","nickname":"MClink","note":"","ucode":"F479190923355C","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":287557,"discussion_content":"我也这样认为，其实不少公司都是这么做的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593481600,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":242429,"user_name":"Monday","can_delete":false,"product_type":"c1","uid":1250907,"ip_address":"","ucode":"77B9BACC783598","user_header":"https://static001.geekbang.org/account/avatar/00/13/16/5b/83a35681.jpg","comment_is_top":false,"comment_ctime":1597720385,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"83202099009","product_id":100046801,"comment_content":"本篇细细品尝了N遍，最后我们系统在达到mysql性能瓶颈的数量级（千万）时，我们引入了es。将部分查询接口由mysql转到es，数据实时同步使用canal，历史数据同步使用logstash。","like_count":20},{"had_liked":false,"id":207238,"user_name":"1","can_delete":false,"product_type":"c1","uid":1895904,"ip_address":"","ucode":"44133D009755C7","user_header":"https://static001.geekbang.org/account/avatar/00/1c/ed/e0/c63d6a80.jpg","comment_is_top":false,"comment_ctime":1587031400,"is_pvip":false,"replies":[{"id":"77852","content":"启动之后他会把放到硬盘的数据放到内存里","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1587353097,"ip_address":"","comment_id":207238,"utype":1}],"discussion_count":3,"race_medal":0,"score":"44536704360","product_id":100046801,"comment_content":"我对内存数据库有个疑问，是启动之后他会把放到硬盘的数据放到内存里？还是查询过一次之后把结果放到内存里","like_count":10,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492053,"discussion_content":"启动之后他会把放到硬盘的数据放到内存里","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587353097,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1014349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/7a/4d/b0228a1a.jpg","nickname":"平风造雨","note":"","ucode":"F9EE4704F31E22","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558758,"discussion_content":"内存数据库一般指的是主存数据库，表示对数据的维护，比如增删改查都在内存中处理。可以通过必要的机制，同步或者异步的把内存数据写入磁盘做持久化。典型的内存数据库明星产品就是redis。非主存形式的数据库，对数据的维护操作，是以磁盘等介质为主的，内存的使用一般称为memory pool/buffer，是用来实现性能优化和加速处理的主要方式。\n两个产品一个重要的区别就是，一旦内存不够，主存数据库可能就无法“正常”工作（出现各种问题），但是非主存数据尽管读写延迟会增加，吞吐量下降，但可能还是可以相对“正常”的去运行。所以一般意义上，主存数据库需要使用方仔细评估业务数据的容量，提前对内存容量做规划，目前先进的主存数据库会通过原生的分布式设计，实现比较好的扩展和分区能力，来满足更多的高频/海量快速数据的场景。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648454314,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366576,"discussion_content":"例如redis会自动从rdb或者aof文件加载数据（取决于配置）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618122921,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":213741,"user_name":"me不是一个人战斗","can_delete":false,"product_type":"c1","uid":1422247,"ip_address":"","ucode":"F9763CF4B628FF","user_header":"https://static001.geekbang.org/account/avatar/00/15/b3/a7/d3ffc8ac.jpg","comment_is_top":false,"comment_ctime":1588544901,"is_pvip":false,"replies":[{"id":"79926","content":"简单的说：ES是“可靠的”存储，而Redis是“不可靠的”存储。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1589168124,"ip_address":"","comment_id":213741,"utype":1}],"discussion_count":3,"race_medal":0,"score":"27358348677","product_id":100046801,"comment_content":"之前介绍es的时候，也说过es作为分布式内存数据库，这个如何理解？es并没有像redis一样，把所以数据都存储在内存里，求解释，谢谢～","like_count":6,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493858,"discussion_content":"简单的说：ES是“可靠的”存储，而Redis是“不可靠的”存储。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589168124,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1082132,"avatar":"https://static001.geekbang.org/account/avatar/00/10/83/14/099742ae.jpg","nickname":"xzy","note":"","ucode":"483350A630625E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":322296,"discussion_content":"ES 并不能称为内存数据库，只是他会把 FST 加载导内存，且底层的 lucene 使用的是 MMAP，尽可能的使用系统内存。而内存毕竟是有限的，在数据量较大时，查询未命中内存时，依旧会利用系统的缺页中断从磁盘加载数据。其实现在很多存储系统也是这样搞的，因此称 ES 为内存数据库并不严谨，只有像 redis、memcached 这类数据才叫做内存 kv 数据库。 ","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1604719910,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1361746,"avatar":"https://static001.geekbang.org/account/avatar/00/14/c7/52/c5adf218.jpg","nickname":"喜欢地球的阿培同学","note":"","ucode":"5F97037585F857","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261700,"discussion_content":"es分布式内存数据库应该是指：es非常吃内存，会将倒排索引等等信息保存在内存中，而且es底层lucece也会把很多信息放到内存中，所以才叫分布式内存数据库吧。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588994031,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209724,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1587605267,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"23062441747","product_id":100046801,"comment_content":"思考题：存储全量程序日志，提供查询和分析服务，我会首先考虑使用时序数据库，比如InfluxDB、OpenTSDB。原因有两点：1）日志具有强时间轴性，且需要有非常好的写性能；2）日志需要提供查询分析，时序数据库能提供很好的读性能，也能提供很方便的查询和聚合数据的能力。","like_count":5,"discussions":[{"author":{"id":2019534,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/d0/ce/a81126ea.jpg","nickname":"代先生。","note":"","ucode":"1CBFB13D7FD3B1","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":584762,"discussion_content":"CK不考虑吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661095032,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"河南"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207096,"user_name":"hello","can_delete":false,"product_type":"c1","uid":1464199,"ip_address":"","ucode":"854500026E2187","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKhuGLVRYZibOTfMumk53Wn8Q0Rkg0o6DzTicbibCq42lWQoZ8lFeQvicaXuZa7dYsr9URMrtpXMVDDww/132","comment_is_top":false,"comment_ctime":1587006127,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"23061842607","product_id":100046801,"comment_content":"时序数据库，如Influxdb","like_count":5,"discussions":[{"author":{"id":2372470,"avatar":"https://static001.geekbang.org/account/avatar/00/24/33/76/9582bf05.jpg","nickname":"浩","note":"","ucode":"B1A08AE574DA67","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":587083,"discussion_content":"我们的接口访问量也是用这个写的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1662777791,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249508,"user_name":"djfhchdh","can_delete":false,"product_type":"c1","uid":1484184,"ip_address":"","ucode":"E71D75328CE398","user_header":"https://static001.geekbang.org/account/avatar/00/16/a5/98/a65ff31a.jpg","comment_is_top":false,"comment_ctime":1600677769,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14485579657","product_id":100046801,"comment_content":"日志分为系统日志和业务服务日志。系统日志的格式较为一致，而业务服务的日志格式都不太一样。系统日志关乎服务器的运行状态，对实时性要求较高，日志量也很大，对存储系统的读写吞吐量要求比较大，可以选择Kafka存储，而查询和分析可以考虑es，按照时间段来查询。而业务日志可以采用HDFS来存储，用hive来查询分析。","like_count":3},{"had_liked":false,"id":212091,"user_name":"seg-上海","can_delete":false,"product_type":"c1","uid":1760320,"ip_address":"","ucode":"46BA8DCB06ED5E","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/ajNVdqHZLLBllicLBj61g1ibmCeWzLYpQYEteTOtAAAypoIg6CD19ibXQBbM09VsME9Ta1G8ubwk0ibjiacItavibaeg/132","comment_is_top":false,"comment_ctime":1588060121,"is_pvip":false,"replies":[{"id":"79188","content":"这个时候的性能瓶颈已经网络和磁盘的IO了。而且MR的“慢”也是相对的，当查询数据量足够大的时候，MR的性能还是非常不错的。","user_name":"作者回复","user_name_real":"李玥","uid":"1501046","ctime":1588513191,"ip_address":"","comment_id":212091,"utype":1}],"discussion_count":4,"race_medal":0,"score":"14472962009","product_id":100046801,"comment_content":"数据量没到PB的时候直接用ES,再大的话，估计得用MR了，但MR会不会太慢了","like_count":4,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493430,"discussion_content":"这个时候的性能瓶颈已经网络和磁盘的IO了。而且MR的“慢”也是相对的，当查询数据量足够大的时候，MR的性能还是非常不错的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588513191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1433535,"avatar":"https://static001.geekbang.org/account/avatar/00/15/df/bf/96b50d1e.jpg","nickname":"😚 46","note":"","ucode":"EED0EBBBF80A43","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":272916,"discussion_content":"弱弱的问一下MR是啥？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1590375531,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1806768,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJLQZzmZq5MInIt27FZVGGq8MmayX9cGGnzxI7d5Ealibmxh0je1ZYyg5r3dByX6WtG2huXsQoJHXg/132","nickname":"Alan.Liu","note":"","ucode":"78269B636DABE8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1433535,"avatar":"https://static001.geekbang.org/account/avatar/00/15/df/bf/96b50d1e.jpg","nickname":"😚 46","note":"","ucode":"EED0EBBBF80A43","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":304251,"discussion_content":"Map Reduce ?","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1599529920,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":272916,"ip_address":""},"score":304251,"extra":""},{"author":{"id":1433535,"avatar":"https://static001.geekbang.org/account/avatar/00/15/df/bf/96b50d1e.jpg","nickname":"😚 46","note":"","ucode":"EED0EBBBF80A43","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1806768,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJLQZzmZq5MInIt27FZVGGq8MmayX9cGGnzxI7d5Ealibmxh0je1ZYyg5r3dByX6WtG2huXsQoJHXg/132","nickname":"Alan.Liu","note":"","ucode":"78269B636DABE8","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":304275,"discussion_content":"感觉应该是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599535297,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":304251,"ip_address":""},"score":304275,"extra":""}]}]},{"had_liked":false,"id":328194,"user_name":"凯文小猪","can_delete":false,"product_type":"c1","uid":1980201,"ip_address":"","ucode":"36D8AD0229547F","user_header":"https://static001.geekbang.org/account/avatar/00/1e/37/29/b3af57a7.jpg","comment_is_top":false,"comment_ctime":1640599146,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10230533738","product_id":100046801,"comment_content":"二刷打卡：<br>老师实际上讲了一个思路 就是对同一份数据 是可以路由多份到不同类型的存储结构:<br>1.对于线上普通查询 有两种方式：<br>  1） 传统mysql 这种一般是订单类查询 可以使用分库分表 或是mysql聚合表、redis聚合表抗一抗<br>  2）ES查询  ES的问题有两点：一是存在热点key问题 因为es是基于内存 所以一定会有命中率要求。二是ES天生不支持改动表结构 这个和日志文件只能递增是一样的。分页查询可以利用瀑布流 也就是snapshot来模拟。<br>2.实时计算<br>这种一般是要HDFS来支持 传统的java后端无能为力 ，这里要注意的是通常数据很脏 要洗数据才能开始计算<br>3.离线查询 <br>以点击流、或是物流来说 每日数据量在10TB以上 那么通常使用HSFS HBASE 一类来存储 ，但是他们的查询方式是有要求的 不可能如同mysql那么随意<br><br>以上是一个小结 这部分最大的问题除了存储介质选型要求设计人员很高的素质外 还有同一份数据在不同存储介质一致性问题，而这本身也是冗余带来的一致性问题。<br>我的思想是 考虑到不同存储介质之间写差异 我个人推荐使用财务冲账方式来处理 ，即对于正向流程可以正常处理 而出现逆向流程则通过反向订单方式 再生成一条记录将其中和掉。这样末端查询也可以保持多存储介质间数据一致性。<br><br>==================================<br>回答下思考题：<br>对于开发来说 通常查的是线上实时及近3天，7天内数据 所以我推荐是用EFK 同时每日需要将3天外或7天外数据将其清除 所以压力不大<br>对于运维来说 通常查的是实时数据 但是要求数据有关联 所以除了传统方式外 还需要用时序数据库来关联 得到整个站点的地图","like_count":3},{"had_liked":false,"id":239836,"user_name":"云封","can_delete":false,"product_type":"c1","uid":1542521,"ip_address":"","ucode":"51C2F84EEA1944","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/aobibE2ABHn3njdaHBY23hcZcIs71aRahryuUDcLghQqTjmwghEIgKYelBERlNK881MP0oRpWGnrQdscD85dZ9g/132","comment_is_top":false,"comment_ctime":1596674999,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10186609591","product_id":100046801,"comment_content":"老师你好，我想问下一个20多台greenplum的集群，一个表大概100亿的数据，通过手机号查找相关的联系人经常把表给整挂了。后来按照时间分片，查询性能也比较慢，老师有没有比较好的建议","like_count":2},{"had_liked":false,"id":207085,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1587004039,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10176938631","product_id":100046801,"comment_content":"我们采取的方式是，最近的三天日志存在es里，旧的数据存在S3，查询的时候使用spectrum","like_count":3},{"had_liked":false,"id":274371,"user_name":"Monday","can_delete":false,"product_type":"c1","uid":1250907,"ip_address":"","ucode":"77B9BACC783598","user_header":"https://static001.geekbang.org/account/avatar/00/13/16/5b/83a35681.jpg","comment_is_top":false,"comment_ctime":1610981334,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"5905948630","product_id":100046801,"comment_content":"第15章，得到MySQL适合TB级别以下<br>从这节课开始，我们课程将进入最后一部分“海量数据篇”，这节课也是我们最后一节主要讲 MySQL 的课程。解决海量数据的问题，必须要用到分布式的存储集群，因为 MySQL 本质上是一个单机数据库，所以很多场景下不是太适合存 TB 级别以上的数据。<br><br>本章，获得MySQL适合GB级别。。。<br>然后我们看有哪些可供选择的存储产品。如果你的系统的数据量在 GB 量级以下，MySQL 仍然是可以考虑的<br><br>是编辑写错了，还是我理解 错了？<br>","like_count":1,"discussions":[{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366577,"discussion_content":"实时业务需要使用到MySQL的事务，保证数据的一致性\n而日志并不需要，所以能用MySQL就用MySQL，减少技术成本\n看描述就是GB量级，MySQL本质是单机，100G也能做，但数据量增大需要自己分库分表，不如直接用原生分布式数据库，更加适合场景","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1618123323,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2019534,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/d0/ce/a81126ea.jpg","nickname":"代先生。","note":"","ucode":"1CBFB13D7FD3B1","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":573207,"discussion_content":"TiDb可考虑吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653270153,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207059,"user_name":"Wind","can_delete":false,"product_type":"c1","uid":1586425,"ip_address":"","ucode":"3679663B706C86","user_header":"https://static001.geekbang.org/account/avatar/00/18/34/f9/c46c0fff.jpg","comment_is_top":false,"comment_ctime":1587000835,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"5881968131","product_id":100046801,"comment_content":"如果数据量不是太大，我会选ELK","like_count":1,"discussions":[{"author":{"id":1262519,"avatar":"https://static001.geekbang.org/account/avatar/00/13/43/b7/74a91222.jpg","nickname":"寒浅","note":"","ucode":"6846680A3B04AD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382193,"discussion_content":"ES吧..","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625469300,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1809686,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/16/a2fb8136.jpg","nickname":"季某人","note":"","ucode":"1EEF2A58C3BD79","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1262519,"avatar":"https://static001.geekbang.org/account/avatar/00/13/43/b7/74a91222.jpg","nickname":"寒浅","note":"","ucode":"6846680A3B04AD","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385207,"discussion_content":"ELK是日志三件套，Elasticsearch、Logstash 和 Kibana","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626942834,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":382193,"ip_address":""},"score":385207,"extra":""}]}]},{"had_liked":false,"id":207044,"user_name":"leslie","can_delete":false,"product_type":"c1","uid":1324255,"ip_address":"","ucode":"798E7C1CC98CC2","user_header":"https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg","comment_is_top":false,"comment_ctime":1586999356,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5881966652","product_id":100046801,"comment_content":"DB和OPS从业多年越来越觉得很难单一用某套系统去解决问题，记得老师在消息队列的课程提及过；消息队列的作用是削峰填谷，最近在思考&quot;中台&quot;真实的作用是什么？是不是真的偶然？是不是就是由于现在单一无力解决而造就了中台的诞生。<br>mysql早期的分引擎处理其实比现在更合理，5.7开始读写都一套引擎反而限制了；虽然业界普遍认为8更好，可是个人觉得早期的做法避免了跨库；读写全部依赖一种东西是不可能真的做到平衡的。就像老师文中提及“需求决定数据库的选择”，日志系统其实合适的很多关键还是要看怎么操作；记得曾经听说过有一套数据库是基本不做DML的，只做查询性能极其好。<br>&quot;需求决定选择&quot;我觉得这才是现在对于DB这块最合理的选择。<br>谢谢老师的分享，期待后续的课程。","like_count":2,"discussions":[{"author":{"id":1304302,"avatar":"https://static001.geekbang.org/account/avatar/00/13/e6/ee/e3c4c9b3.jpg","nickname":"Cranliu","note":"","ucode":"DC2DE84B142FDA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":240481,"discussion_content":"oracle的12c之后对analyse做了较多的增强，个人觉得oracle的定位大约就是 “融合”，至少在一定量的级别上是这样的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587368480,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207023,"user_name":"何妨","can_delete":false,"product_type":"c1","uid":1385377,"ip_address":"","ucode":"EC3983BFF7992A","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/a1/b08f3ee7.jpg","comment_is_top":false,"comment_ctime":1586997978,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5881965274","product_id":100046801,"comment_content":"还是看量级,课程中已有说到，少－mysql,中－ES,多－HDFS","like_count":1},{"had_liked":false,"id":350680,"user_name":"Gatsby","can_delete":false,"product_type":"c1","uid":1474954,"ip_address":"","ucode":"5DBFDA12556BDB","user_header":"https://static001.geekbang.org/account/avatar/00/16/81/8a/15a96a64.jpg","comment_is_top":false,"comment_ctime":1657092036,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1657092036","product_id":100046801,"comment_content":"es + kibana做实时查询<br>hive做离线数据分析查询","like_count":0},{"had_liked":false,"id":323487,"user_name":"SochiLee","can_delete":false,"product_type":"c1","uid":1537865,"ip_address":"","ucode":"47596594EDF4D7","user_header":"https://static001.geekbang.org/account/avatar/00/17/77/49/445eea2d.jpg","comment_is_top":false,"comment_ctime":1637927567,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637927567","product_id":100046801,"comment_content":"es作为数仓，合理吗？","like_count":0},{"had_liked":false,"id":307974,"user_name":"idiot","can_delete":false,"product_type":"c1","uid":2526391,"ip_address":"","ucode":"D7A6E980B530B4","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/fcftgBsticCicEEkuzB0GTkHIocX62YVTSvnhR1c94sccj42lVaYXrmcZyhzUI3l9NcvuN1rXLhXt2eBrZZ0Tw7A/132","comment_is_top":false,"comment_ctime":1629345541,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1629345541","product_id":100046801,"comment_content":"lsm的关键不是算法复杂度，而是磁盘io，把多个随机写合并成批量写","like_count":0,"discussions":[{"author":{"id":2989416,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLia17ibYsLic20bEFNkvObLpXicfUpYd9OeWvKxml0rNic3NDyRQ6KHl7wtEp0x993tJsTDsLHX2UHRYw/132","nickname":"Geek_761876","note":"","ucode":"D87D0B6D815DE1","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":583358,"discussion_content":"所以LSM对于写大于读的场景比较有优势","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660052584,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":306586,"user_name":"Geek_f71330","can_delete":false,"product_type":"c1","uid":1617615,"ip_address":"","ucode":"40F8CD661E8F59","user_header":"https://static001.geekbang.org/account/avatar/00/18/ae/cf/6186d936.jpg","comment_is_top":false,"comment_ctime":1628615132,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1628615132","product_id":100046801,"comment_content":"存储采取influxdb等列式数据库，压缩率高。且个时间强相关。<br><br>读时热点数据使用es，比如只保留30天。","like_count":0},{"had_liked":false,"id":306156,"user_name":"OM","can_delete":false,"product_type":"c1","uid":1302746,"ip_address":"","ucode":"E33C66A70802A5","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoWfXendN7czHpsyaWKLPK6Na9P5czquJ7Wdre4TibZQ5SQib88edyuib3LpCVFkp0gII2wyvvR8tEIA/132","comment_is_top":false,"comment_ctime":1628409698,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1628409698","product_id":100046801,"comment_content":"某些业务用oracle一体机或者国产一体机，无需怎么麻烦，10TB数据通过一体机的机制过滤成1TB，再10GB，再到用户是1GB的量，智能扫描，开源很多库还无法做到。","like_count":0},{"had_liked":false,"id":230560,"user_name":"Grocker","can_delete":false,"product_type":"c1","uid":2003618,"ip_address":"","ucode":"39A324CDDCE918","user_header":"","comment_is_top":false,"comment_ctime":1593417687,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1593417687","product_id":100046801,"comment_content":"我所在的项目就是做报表系统的，目前用到了greenplum 和 elasticsearch","like_count":0},{"had_liked":false,"id":207643,"user_name":"aoe","can_delete":false,"product_type":"c1","uid":1121758,"ip_address":"","ucode":"1C6201EDB4E954","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","comment_is_top":false,"comment_ctime":1587126882,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1587126882","product_id":100046801,"comment_content":"这么大的数据，已经买不起硬盘存储了","like_count":0,"discussions":[{"author":{"id":1886331,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/c8/7b/153181d7.jpg","nickname":"夜辉","note":"","ucode":"9421385F51FF9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366579,"discussion_content":"https://cloud.baidu.com/campaign/bccdiscount/index.html?track=cp:googlesem|pf:pc|pp:G-SEM-huodong-21BCCzhuanchang-BOS|pu:bos-tongyongci|ci:21bcc|kw:10106520&amp;gclid=Cj0KCQjwmcWDBhCOARIsALgJ2QcJlXdNEX9_1Uwxowzq-4jGM6rw7iEpXyTIbOa1FVQt8ijsiXPKtwoaAqISEALw_wcB#showView=bos\n\n看了下腾讯云 200T ，12个月，数据频繁读取，用于数据分析，118800元","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618123788,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}