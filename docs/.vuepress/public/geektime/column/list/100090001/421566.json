{"id":421566,"title":"07 | RDD常用算子（二）：Spark如何实现数据聚合？","content":"<p>你好，我是吴磊。</p><p>积累了一定的理论基础之后，今天我们继续来学习RDD常用算子。在<a href=\"https://time.geekbang.org/column/article/418079\">RDD常用算子（一）</a>那一讲，我们讲了四个算子map、mapPartitions、flatMap和filter，同时留了这样一道思考题：“这些算子之间，有哪些共同点？”</p><p>今天我们就来揭晓答案。首先，在功能方面，这4个算子都用于RDD内部的数据转换，而学习过Shuffle的工作原理之后，我们不难发现，这4个算子当中，没有任何一个算子，会引入Shuffle计算。</p><p>而今天我们要学习的几个算子则恰恰相反，它们都会引入繁重的Shuffle计算。这些算子分别是groupByKey、reduceByKey、aggregateByKey和sortByKey，也就是表格中加粗的部分。</p><p>我们知道，在数据分析场景中，典型的计算类型分别是分组、聚合和排序。而groupByKey、reduceByKey、aggregateByKey和sortByKey这些算子的功能，恰恰就是用来实现分组、聚合和排序的计算逻辑。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/fc/c97a717512897749d5db659fb583c8fc.jpg?wh=1597x1977\" alt=\"图片\" title=\"RDD算子分类表\"></p><p>尽管这些算子看上去相比其他算子的适用范围更窄，也就是它们只能作用（Apply）在Paired RDD之上，所谓Paired RDD，它指的是元素类型为（Key，Value）键值对的RDD。</p><!-- [[[read_end]]] --><p>但是在功能方面，可以说，它们承担了数据分析场景中的大部分职责。因此，掌握这些算子的用法，是我们能够游刃有余地开发数据分析应用的重要基础。那么接下来，我们就通过一些实例，来熟悉并学习这些算子的用法。</p><p>我们先来说说groupByKey，坦白地说，相比后面的3个算子，groupByKey在我们日常开发中的“出镜率”并不高。之所以要先介绍它，主要是为后续的reduceByKey和aggregateByKey这两个重要算子做铺垫。</p><h2>groupByKey：分组收集</h2><p>groupByKey的字面意思是“按照Key做分组”，但实际上，groupByKey算子包含两步，即<strong>分组</strong>和<strong>收集</strong>。</p><p>具体来说，对于元素类型为（Key，Value）键值对的Paired RDD，groupByKey的功能就是对Key值相同的元素做分组，然后把相应的Value值，以集合的形式收集到一起。换句话说，groupByKey会把RDD的类型，由RDD[(Key, Value)]转换为RDD[(Key, Value集合)]。</p><p>这么说比较抽象，我们还是用一个小例子来说明groupByKey的用法。还是我们熟知的Word Count，对于分词后的一个个单词，假设我们不再统计其计数，而仅仅是把相同的单词收集到一起，那么我们该怎么做呢？按照老规矩，咱们还是先来给出代码实现：</p><pre><code class=\"language-scala\">import org.apache.spark.rdd.RDD\n&nbsp;\n// 以行为单位做分词\nval cleanWordRDD: RDD[String] = _ // 完整代码请参考第一讲的Word Count\n// 把普通RDD映射为Paired RDD\nval kvRDD: RDD[(String, String)] = cleanWordRDD.map(word =&gt; (word, word))\n&nbsp;\n// 按照单词做分组收集\nval words: RDD[(String, Iterable[String])] = kvRDD.groupByKey()\n</code></pre><p>结合前面的代码可以看到，相比之前的Word Count，我们仅需做两个微小的改动，即可实现新的计算逻辑。第一个改动，是把map算子的映射函数f，由原来的word =&gt; （word，1）变更为word =&gt; （word，word），这么做的效果，是把kvRDD元素的Key和Value都变成了单词。</p><p>紧接着，第二个改动，我们用groupByKey替换了原先的reduceByKey。相比reduceByKey，groupByKey的用法要简明得多。groupByKey是无参函数，要实现对Paired RDD的分组、收集，我们仅需在RDD之上调用groupByKey()即可。</p><p>尽管groupByKey的用法非常简单，但它的计算过程值得我们特别关注，下面我用一张示意图来讲解上述代码的计算过程，从而让你更加直观地感受groupByKey可能存在的性能隐患。</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/77/6f1c7fb6bebd3yy43b1404835fe86d77.jpg?wh=1920x1071\" alt=\"图片\" title=\"groupByKey计算过程\"></p><p>从图上可以看出，为了完成分组收集，对于Key值相同、但分散在不同数据分区的原始数据记录，Spark需要通过Shuffle操作，跨节点、跨进程地把它们分发到相同的数据分区。我们之前在<a href=\"https://time.geekbang.org/column/article/420399\">第6讲</a>中说了，<strong>Shuffle是资源密集型计算</strong>，对于动辄上百万、甚至上亿条数据记录的RDD来说，这样的Shuffle计算会产生大量的磁盘I/O与网络I/O开销，从而严重影响作业的执行性能。</p><p>虽然groupByKey的执行效率较差，不过好在它在应用开发中的“出镜率”并不高。原因很简单，在数据分析领域中，分组收集的使用场景很少，而分组聚合才是统计分析的刚需。</p><p>为了满足分组聚合多样化的计算需要，Spark提供了3种RDD算子，允许开发者灵活地实现计算逻辑，它们分别是reduceByKey、aggregateByKey和combineByKey。</p><p>reduceByKey我们并不陌生，第1讲的Word Count实现就用到了这个算子，aggregateByKey是reduceByKey的“升级版”，相比reduceByKey，aggregateByKey用法更加灵活，支持的功能也更加完备。</p><p>接下来，我们先来回顾reduceByKey，然后再对aggregateByKey进行展开。相比aggregateByKey，combineByKey仅在初始化方式上有所不同，因此，我把它留给你作为课后作业去探索。</p><h2>reduceByKey：分组聚合</h2><p>reduceByKey的字面含义是“按照Key值做聚合”，它的计算逻辑，就是根据聚合函数f给出的算法，把Key值相同的多个元素，聚合成一个元素。</p><p>在<a href=\"https://time.geekbang.org/column/article/415209\">第1讲</a>Word Count的实现中，我们使用了reduceByKey来实现分组计数：</p><pre><code class=\"language-scala\">// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))\n&nbsp;\n// 按照单词做分组计数\nval wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x: Int, y: Int) =&gt; x + y)\n</code></pre><p>重温上面的这段代码，你有没有觉得reduceByKey与之前讲过的map、filter这些算子有一些相似的地方？没错，给定处理函数f，它们的用法都是“算子(f)”。只不过<strong>对于map来说，我们把f称作是映射函数，对filter来说，我们把f称作判定函数，而对于reduceByKey，我们把f叫作聚合函数。</strong></p><p>在上面的代码示例中，reduceByKey的聚合函数是匿名函数：(x, y) =&gt; x + y。与map、filter等算子的用法一样，你也可以明确地定义带名函数f，然后再用reduceByKey(f)的方式实现同样的计算逻辑。</p><p>需要强调的是，给定RDD[(Key类型，Value类型)]，聚合函数f的类型，必须是（Value类型，Value类型） =&gt; （Value类型）。换句话说，函数f的形参，必须是两个数值，且数值的类型必须与Value的类型相同，而f的返回值，也必须是Value类型的数值。</p><p>咱们不妨再举一个小例子，让你加深对于reduceByKey算子的理解。</p><p>接下来，我们把Word Count的计算逻辑，改为随机赋值、提取同一个Key的最大值。也就是在kvRDD的生成过程中，我们不再使用映射函数word =&gt; (word, 1)，而是改为word =&gt; (word, 随机数)，然后再使用reduceByKey算子来计算同一个word当中最大的那个随机数。</p><p>你可以先停下来，花点时间想一想这个逻辑该怎么实现，然后再来参考下面的代码：</p><pre><code class=\"language-scala\">import scala.util.Random._\n&nbsp;\n// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, nextInt(100)))\n&nbsp;\n// 显示定义提取最大值的聚合函数f\ndef f(x: Int, y: Int): Int = {\nreturn math.max(x, y)\n}\n&nbsp;\n// 按照单词提取最大值\nval wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey(f)\n</code></pre><p>观察上面的代码片段，不难发现，reduceByKey算子的用法还是比较简单的，只需要先定义好聚合函数f，然后把它传给reduceByKey算子就行了。那么在运行时，上述代码的计算又是怎样的一个过程呢？</p><p>我把reduceByKey的计算过程抽象成了下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/ac/85/aca17fe4d0fa5a52f4b9e73056aa1185.jpg?wh=1920x951\" alt=\"图片\" title=\"reduceByKey计算过程\"></p><p>从图中你可以看出来，尽管reduceByKey也会引入Shuffle，但相比groupByKey以全量原始数据记录的方式消耗磁盘与网络，reduceByKey在落盘与分发之前，会先在Shuffle的Map阶段做初步的聚合计算。</p><p>比如，在数据分区0的处理中，在Map阶段，reduceByKey把Key同为Streaming的两条数据记录聚合为一条，聚合逻辑就是由函数f定义的、取两者之间Value较大的数据记录，这个过程我们称之为“<strong>Map端聚合</strong>”。相应地，数据经由网络分发之后，在Reduce阶段完成的计算，我们称之为“<strong>Reduce端聚合</strong>”。</p><p>你可能会说：“做了Map聚合又能怎样呢？相比groupByKey，reduceByKey带来的性能收益并不算明显呀！”确实，就上面的示意图来说，我们很难感受到reduceByKey带来的性能收益。不过，量变引起质变，在工业级的海量数据下，相比groupByKey，reduceByKey通过在Map端大幅削减需要落盘与分发的数据量，往往能将执行效率提升至少一倍。</p><p>应该说，对于大多数分组&amp;聚合的计算需求来说，只要设计合适的聚合函数f，你都可以使用reduceByKey来实现计算逻辑。不过，术业有专攻，reduceByKey算子的局限性，<strong>在于其Map阶段与Reduce阶段的计算逻辑必须保持一致，这个计算逻辑统一由聚合函数f定义</strong>。当一种计算场景需要在两个阶段执行不同计算逻辑的时候，reduceByKey就爱莫能助了。</p><p>比方说，还是第1讲的Word Count，我们想对单词计数的计算逻辑做如下调整：</p><ul>\n<li>在Map阶段，以数据分区为单位，计算单词的加和；</li>\n<li>而在Reduce阶段，对于同样的单词，取加和最大的那个数值。</li>\n</ul><p>显然，Map阶段的计算逻辑是sum，而Reduce阶段的计算逻辑是max。对于这样的业务需求，reduceByKey已无用武之地，这个时候，就轮到aggregateByKey这个算子闪亮登场了。</p><h2>aggregateByKey：更加灵活的聚合算子</h2><p>老规矩，算子的介绍还是从用法开始。相比其他算子，aggregateByKey算子的参数比较多。要在Paired RDD之上调用aggregateByKey，你需要提供一个初始值，一个Map端聚合函数f1，以及一个Reduce端聚合函数f2，aggregateByKey的调用形式如下所示：</p><pre><code class=\"language-scala\">val rdd: RDD[(Key类型，Value类型)] = _\nrdd.aggregateByKey(初始值)(f1, f2)\n</code></pre><p>初始值可以是任意数值或是字符串，而聚合函数我们也不陌生，它们都是带有两个形参和一个输出结果的普通函数。就这3个参数来说，比较伤脑筋的，是它们之间的类型需要保持一致，具体来说：</p><ul>\n<li>初始值类型，必须与f2的结果类型保持一致；</li>\n<li>f1的形参类型，必须与Paired RDD的Value类型保持一致；</li>\n<li>f2的形参类型，必须与f1的结果类型保持一致。</li>\n</ul><p>不同类型之间的一致性描述起来比较拗口，咱们不妨结合示意图来加深理解：</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/f7/b0a1c86590f4213fa0fc62f5dd4ca3f7.jpg?wh=1920x568\" alt=\"图片\" title=\"aggregateByKey参数之间的类型一致性\"></p><p>熟悉了aggregateByKey的用法之后，接下来，我们用aggregateByKey这个算子来实现刚刚提到的“先加和，再取最大值”的计算逻辑，代码实现如下所示：</p><pre><code class=\"language-scala\">// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))\n&nbsp;\n// 显示定义Map阶段聚合函数f1\ndef f1(x: Int, y: Int): Int = {\nreturn x + y\n}\n&nbsp;\n// 显示定义Reduce阶段聚合函数f2\ndef f2(x: Int, y: Int): Int = {\nreturn math.max(x, y)\n}\n&nbsp;\n// 调用aggregateByKey，实现先加和、再求最大值\nval wordCounts: RDD[(String, Int)] = kvRDD.aggregateByKey(0) (f1, f2)\n</code></pre><p>怎么样？是不是很简单？结合计算逻辑的需要，我们只需要提前定义好两个聚合函数，同时保证参数之间的类型一致性，然后把初始值、聚合函数传入aggregateByKey算子即可。按照惯例，我们还是通过aggregateByKey在运行时的计算过程，来帮你深入理解算子的工作原理：</p><p><img src=\"https://static001.geekbang.org/resource/image/62/f3/62d25ab5df4fa53da4283263bb2128f3.jpg?wh=1920x1040\" alt=\"图片\" title=\"reduceByKey计算过程\"></p><p>不难发现，在运行时，与reduceByKey相比，aggregateByKey的执行过程并没有什么两样，最主要的区别，还是Map端聚合与Reduce端聚合的计算逻辑是否一致。值得一提的是，与reduceByKey一样，aggregateByKey也可以通过Map端的初步聚合来大幅削减数据量，在降低磁盘与网络开销的同时，提升Shuffle环节的执行性能。</p><h2>sortByKey：排序</h2><p>在这一讲的最后，我们再来说说sortByKey这个算子，顾名思义，它的功能是“按照Key进行排序”。给定包含（Key，Value）键值对的Paired RDD，sortByKey会以Key为准对RDD做排序。算子的用法比较简单，只需在RDD之上调用sortByKey()即可：</p><pre><code class=\"language-scala\">val rdd: RDD[(Key类型，Value类型)] = _\nrdd.sortByKey()\n</code></pre><p>在默认的情况下，sortByKey按照Key值的升序（Ascending）对RDD进行排序，如果想按照降序（Descending）来排序的话，你需要给sortByKey传入false。总结下来，关于排序的规则，你只需要记住如下两条即可：</p><ul>\n<li>升序排序：调用sortByKey()、或者sortByKey(true)；</li>\n<li>降序排序：调用sortByKey(false)。</li>\n</ul><h2>重点回顾</h2><p>今天这一讲，我们介绍了数据分析场景中常用的4个算子，它们分别是groupByKey、reduceByKey、aggregateByKey和sortByKey，掌握这些算子的用法与原理，将为你游刃有余地开发数据分析应用打下坚实基础。</p><p>关于这些算子，你首先需要了解它们之间的共性。<strong>一来，这4个算子的作用范围，都是Paired RDD；二来，在计算的过程中，它们都会引入Shuffle</strong>。而Shuffle往往是Spark作业执行效率的瓶颈，因此，在使用这4个算子的时候，对于它们可能会带来的性能隐患，我们要做到心中有数。</p><p>再者，你需要掌握每一个算子的具体用法与工作原理。groupByKey是无参算子，你只需在RDD之上调用groupByKey()即可完成对数据集的分组和收集。但需要特别注意的是，<strong>以全量原始数据记录在集群范围内进行落盘与网络分发，会带来巨大的性能开销。</strong>因此，除非必需，你应当尽量避免使用groupByKey算子。</p><p>利用聚合函数f，reduceByKey可以在Map端进行初步聚合，大幅削减需要落盘与分发的数据量，从而在一定程度上能够显著提升Shuffle计算的执行效率。对于绝大多数分组&amp;聚合的计算需求，只要聚合函数f设计得当，reduceByKey都能实现业务逻辑。reduceByKey也有其自身的局限性，那就是其Map阶段与Reduce阶段的计算逻辑必须保持一致。</p><p>对于Map端聚合与Reduce端聚合计算逻辑不一致的情况，aggregateByKey可以很好地满足这样的计算场景。aggregateByKey的用法是aggregateByKey(初始值)(Map端聚合函数，Reduce端聚合函数)，对于aggregateByKey的3个参数，你需要保证它们之间类型的一致性。一旦类型一致性得到满足，你可以通过灵活地定义两个聚合函数，来翻着花样地进行各式各样的数据分析。</p><p>最后，对于排序类的计算需求，你可以通过调用sortByKey来进行实现。sortByKey支持两种排序方式，在默认情况下，sortByKey()按Key值的升序进行排序，sortByKey()与sortByKey(true)的效果是一样的。如果想按照降序做排序，你只需要调用sortByKey(false)即可。</p><p>到此为止，我们一起学习了RDD常用算子的前两大类，也就是数据转换和数据聚合。在日常的开发工作中，应该说绝大多数的业务需求，都可以通过这些算子来实现。</p><p>因此恭喜你，毫不夸张地说，学习到这里，你的一只脚已经跨入了Spark分布式应用开发的大门。不过，我们还不能骄傲，“学会”和“学好”之间还有一定的距离，在接下来的时间里，期待你和我一起继续加油，真正做到吃透Spark、玩转Spark！</p><h2>每课一练</h2><p>这一讲到这里就要结束了，今天的练习题是这样的：</p><p>学习过reduceByKey和aggregateByKey之后，你能说说它们二者之间的联系吗？你能用aggregateByKey来实现reduceByKey的功能吗？</p><p>欢迎你分享你的答案。如果这一讲对你有帮助，也欢迎你把这一讲分享给自己的朋友，和他一起来讨论一下本讲的练习题，我们下一讲再见。</p>","comments":[{"had_liked":false,"id":313781,"user_name":"Geek_2dfa9a","can_delete":false,"product_type":"c1","uid":1435535,"ip_address":"","ucode":"B5FE7971F5E773","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epGTSTvn7r4ibk1PuaUrSvvLdviaLcne50jbvvfiaxKkM5SLibeP6jibA2bCCQBqETibvIvcsOhAZlwS8kQ/132","comment_is_top":true,"comment_ctime":1632663677,"is_pvip":false,"replies":[{"id":"113647","content":"完美，满分💯！置顶🔝<br><br>数组的hashcode，其实就是它的内存地址，就像你说的，跟内容无关。因此，它没有办法利用hashPartitioner完成分发，所以说，key是数组的rdd，应用范围非常有限。<br><br>除了hashPartitioner，Spark还支持rangePartitioner，这种分区器，不需要hashcode，只要实现了comparator（能比较）就行。所以即便非要用hashcode来排序，也ok。这个时候，内容是否一样，已经不重要了。<br><br>reduce阶段需要聚合的场景，一定是需要hashPartitioner的，由于数组不支持hashPartitioner，它也就走不到reduce那一步，没有这样的code path～<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632745166,"ip_address":"","comment_id":313781,"utype":1}],"discussion_count":2,"race_medal":0,"score":"9.2233721072068997e+18","product_id":100090001,"comment_content":"最近公司在搞黑客马拉松，我忙于做一个数仓血缘图计算的项目来晚啦。<br>reduceByKey和aggregateByKey底层使用了同一个方法实现：combineByKeyWithClassTag，该方法是将KV型的RDD[(K, V)]转换为RDD[(K, C)]，<br>类似于分组聚合，既然要找到reduceByKey和aggregateByKey的联系那肯定要从下至上由共性开始分析，combineByKeyWithClassTag方法声明如下：<br>  def combineByKeyWithClassTag[C](<br>      createCombiner: V =&gt; C,<br>      mergeValue: (C, V) =&gt; C,<br>      mergeCombiners: (C, C) =&gt; C,<br>      partitioner: Partitioner,<br>      mapSideCombine: Boolean = true,<br>      serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope {<br>  }<br>字数限制了，我把方法实现放在下面评论。<br>首先讲三个高阶函数入参：createCombiner，mergeValue，mergeCombiners数，在方法里组成了Aggregator对象，Aggregator<br>其实就是spark对分组聚合（Shuffle）操作的抽象，如果不清楚spark分组聚合的过程这三个高阶函数不好理解，简单点讲，RDD按照Key分组后因为<br>不同Partition里会有相同Key，因此对于Key=k1这个大组会有多个小组（k11,k12...k1n）,首先createCombiner会给k11,k12...k1n）<br>每个小组赋一个初始值C0，然后mergeValue把小组内的每个记录叠加给初始值得到一个小组值C00（其实就是map端聚合），最后再把所有小组的<br>小组值合并成一个KV型RDD（注意这里V已经变成了C类型）。<br>再讲参数partitioner，了解RDD的话应该清楚，就是这个RDD的分区规则，这里的入参就是聚合后RDD的分区规则，如果相同的话，那Shuffle就完全不需要了，<br>直接task本地聚合一下就好了，源码里也就直接mapPartitions就结束了，如果聚合前后分区规则不相同的话，那么就会返回一个ShuffledRDD。<br>最后讲下参数mapSideCombine和serializer，mapSideCombine就是是否在map端聚合，方法开头的校验可以看到keyClass是数组时不支持<br>map端聚合和哈希分区，这里是为什么呢？不太熟悉scala的我查了下stackoverflow，原来scala里的数组和Java的一样，hashcode只是数组对象本身的<br>hashcode，和内容没关系，那自然没办法map端聚合了，<br>这里请问老师，那数组作为KVRDD的时候，reduce端的聚合是怎么完成判等的呢？<br>serializer是指出数据如何序列化的，序列化就先不说了，不然又要讲好多。<br><br>最后总结下，reduceByKey和aggregateByKey底层实现完全相同，都是combineByKeyWithClassTag，只不过reduceByKey调用<br>combineByKeyWithClassTag的入参mergeValue和mergeCombiners是相等的，aggregateByKey是用户指定可以不等的，也就是说<br>reduceByKey是一种特殊的aggregateByKey。","like_count":17,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527467,"discussion_content":"完美，满分💯！置顶🔝\n\n数组的hashcode，其实就是它的内存地址，就像你说的，跟内容无关。因此，它没有办法利用hashPartitioner完成分发，所以说，key是数组的rdd，应用范围非常有限。\n\n除了hashPartitioner，Spark还支持rangePartitioner，这种分区器，不需要hashcode，只要实现了comparator（能比较）就行。所以即便非要用hashcode来排序，也ok。这个时候，内容是否一样，已经不重要了。\n\nreduce阶段需要聚合的场景，一定是需要hashPartitioner的，由于数组不支持hashPartitioner，它也就走不到reduce那一步，没有这样的code path～\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632745166,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1435535,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epGTSTvn7r4ibk1PuaUrSvvLdviaLcne50jbvvfiaxKkM5SLibeP6jibA2bCCQBqETibvIvcsOhAZlwS8kQ/132","nickname":"Geek_2dfa9a","note":"","ucode":"B5FE7971F5E773","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":397748,"discussion_content":"combineByKeyWithClassTag方法完整实现\ndef combineByKeyWithClassTag[C](\n      createCombiner: V => C,\n      mergeValue: (C, V) => C,\n      mergeCombiners: (C, C) => C,\n      partitioner: Partitioner,\n      mapSideCombine: Boolean = true,\n      serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope {\n    require(mergeCombiners != null, &#34;mergeCombiners must be defined&#34;) // required as of Spark 0.9.0\n    if (keyClass.isArray) {\n      if (mapSideCombine) {\n        throw new SparkException(&#34;Cannot use map-side combining with array keys.&#34;)\n      }\n      if (partitioner.isInstanceOf[HashPartitioner]) {\n        throw new SparkException(&#34;HashPartitioner cannot partition array keys.&#34;)\n      }\n    }\n    val aggregator = new Aggregator[K, V, C](\n      self.context.clean(createCombiner),\n      self.context.clean(mergeValue),\n      self.context.clean(mergeCombiners))\n    if (self.partitioner == Some(partitioner)) {\n      self.mapPartitions(iter => {\n        val context = TaskContext.get()\n        new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))\n      }, preservesPartitioning = true)\n    } else {\n      new ShuffledRDD[K, V, C](self, partitioner)\n        .setSerializer(serializer)\n        .setAggregator(aggregator)\n        .setMapSideCombine(mapSideCombine)\n    }\n  }","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632666382,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":339766,"user_name":"Spoon","can_delete":false,"product_type":"c1","uid":1959822,"ip_address":"","ucode":"2FF9193AD482C2","user_header":"https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg","comment_is_top":false,"comment_ctime":1648370201,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5943337497","product_id":100090001,"comment_content":"Java实现代码<br>https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;core&#47;AggOpJob.java","like_count":0},{"had_liked":false,"id":353857,"user_name":"J","can_delete":false,"product_type":"c1","uid":1002675,"ip_address":"湖北","ucode":"EC6B45BD3E128D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4c/b3/931dcd9e.jpg","comment_is_top":false,"comment_ctime":1659868015,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1659868015","product_id":100090001,"comment_content":"讲解aggregateByKey计算过程时，图例错写成了“reduceByKey计算过程”","like_count":0},{"had_liked":false,"id":343724,"user_name":"睿晞","can_delete":false,"product_type":"c1","uid":2970268,"ip_address":"","ucode":"25AA3449AFB630","user_header":"https://static001.geekbang.org/account/avatar/00/2d/52/9c/8a60675c.jpg","comment_is_top":false,"comment_ctime":1651023484,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1651023484","product_id":100090001,"comment_content":"aggregateByKey算子中第一个默认参数的使用方法是什么啊？是直接参与到第二个聚合函数（reduce端）里面运算吗？比如，默认参数是0，然后如果第二个聚合函数是max求最大值，初始默认参数是参与比较的，用0和每个字段中的值比较，是这个意思吗？","like_count":0},{"had_liked":false,"id":330226,"user_name":"Geek_2a0deb","can_delete":false,"product_type":"c1","uid":1315176,"ip_address":"","ucode":"DBF960FDFB77C0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKIkpsQkTyLtfxgib35o0ho9nWmCHwJL8BYibJPPT22fkT1aTwHhwQc0krINWjTVRjibF1bMTgia5mflg/132","comment_is_top":false,"comment_ctime":1641872072,"is_pvip":false,"replies":[{"id":"120669","content":"没错，满分~ 💯","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1642244228,"ip_address":"","comment_id":330226,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641872072","product_id":100090001,"comment_content":"reduceByKey 和 aggregateByKey的区别在于reduceByKey在map端和reduce时的聚合函数一致，而aggregateByKey在map端和reduce端聚合函数可以不一致，联系就是reduceByKey可以认为是一种特殊的aggregateByKey（map和reduce是同一个函数）如果用算子来描述:reduceByKey(f)=aggregateByKey(初始值) (f,f)","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546233,"discussion_content":"没错，满分~ 💯","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642244228,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":327507,"user_name":"Botanic","can_delete":false,"product_type":"c1","uid":1543891,"ip_address":"","ucode":"AA8C028A6C08DC","user_header":"https://static001.geekbang.org/account/avatar/00/17/8e/d3/1a3bb2cc.jpg","comment_is_top":false,"comment_ctime":1640156383,"is_pvip":false,"replies":[{"id":"119389","content":"满分，💯，赞👍~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1640355589,"ip_address":"","comment_id":327507,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1640156383","product_id":100090001,"comment_content":"```<br># 使用 aggregateByKey 来实现 reduceByKey<br>def f1(x, y):<br>    # 显示定义Map阶段聚合函数f1，求加和<br>    return x+y<br><br>import random<br># 实验3：aggregateByKey 使用说明<br>textFile = SparkContext().textFile(&quot;..&#47;wikiOfSpark.txt&quot;)<br>wordCount = (<br>            textFile.flatMap(lambda line: line.split(&quot; &quot;))<br>                .filter(lambda word: word != &quot;&quot;)<br>                    .map(lambda word: (word, 1))<br>                        .aggregateByKey(0, f1, f1)<br>                            .sortBy(lambda x: x[1], False)<br>                                .take(5))<br>print(wordCount)<br>```","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":541398,"discussion_content":"满分，💯，赞👍~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640355589,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321428,"user_name":"pythonbug","can_delete":false,"product_type":"c1","uid":1487274,"ip_address":"","ucode":"1A70CA92FFF8EB","user_header":"https://wx.qlogo.cn/mmopen/vi_32/wgMMrp1hvSB3E30KqZvMsj3KQdAI3T1uQM77LT7hZ65nVSjPGRg3AbUOyiahnssA6AIT5PAkyHFmlTBzUH9gdyQ/132","comment_is_top":false,"comment_ctime":1636873094,"is_pvip":true,"replies":[{"id":"116848","content":"没什么关系哈，aggregateByKey可以理解成是reduceByKey的“升级版”，功能更灵活，能够应对的计算场景更多~<br><br>实际上，reduceByKey完全可以用aggregateByKey来实现~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1637059251,"ip_address":"","comment_id":321428,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636873094","product_id":100090001,"comment_content":"我感觉aggregateByKey直接作用在刚刚读取数据的RDD上的情况很少，因为刚刚从数据源读取出来的数据分区大多数时候是没啥业务含义的，所以Map阶段的聚合也没有太大意义。所以猜测，aggregateByKey可能大多数情况是跟在reduceByKey之后，那个时候已经对数据按照业务分区好了。那个时候Map阶段的聚合才有一些意义，不知道猜的对不对","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530362,"discussion_content":"没什么关系哈，aggregateByKey可以理解成是reduceByKey的“升级版”，功能更灵活，能够应对的计算场景更多~\n\n实际上，reduceByKey完全可以用aggregateByKey来实现~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637059251,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314941,"user_name":"十年","can_delete":false,"product_type":"c1","uid":1115805,"ip_address":"","ucode":"A246C22CD6E5F2","user_header":"https://static001.geekbang.org/account/avatar/00/11/06/9d/3c121a1c.jpg","comment_is_top":false,"comment_ctime":1633600200,"is_pvip":false,"replies":[{"id":"114045","content":"数值的话，可以指定非零初始值，字符串的话，可以指定任意前缀字符串~ 主要看业务需要了","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633601547,"ip_address":"","comment_id":314941,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1633600200","product_id":100090001,"comment_content":"请问老师，aggregateByKey的初始值有什么作用？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527836,"discussion_content":"数值的话，可以指定非零初始值，字符串的话，可以指定任意前缀字符串~ 主要看业务需要了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633601547,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2086573,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/d6/ad/850992a5.jpg","nickname":"William","note":"","ucode":"EC29D9CD616183","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414496,"discussion_content":"似乎是为了申明输出类型","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636790797,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313997,"user_name":"钱鹏 Allen","can_delete":false,"product_type":"c1","uid":2518863,"ip_address":"","ucode":"7E95E82C0717DA","user_header":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","comment_is_top":false,"comment_ctime":1632794962,"is_pvip":true,"replies":[{"id":"113782","content":"对的～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632878479,"ip_address":"","comment_id":313997,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1632794962","product_id":100090001,"comment_content":"reduceByKey 和 aggregateByKey的联系：将相同的key值进行聚合。不同点：reduceByKey()采用的是相同的func，在map阶段使用sum操作，reduce阶段采用max操作就不满足。<br>aggregateByKey可以看做是更一般的reduceByKey，<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527541,"discussion_content":"对的～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632878479,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}