{"id":837407,"title":"09｜网络编程：如何进行网络IO编程降消耗，提吞吐？","content":"<p>你好，我是徐逸。</p><p>前面，我们花了不少篇幅一同深入学习了Go服务高性能编码技巧，来全力保障线上服务的性能。不过呢，除了我们写的业务逻辑代码，服务框架本身对于性能也有着举足轻重的影响。而影响框架性能的一个很重要的因素，就是框架所使用的网络IO模型。</p><p>今天我们就来聊聊网络IO模型、epoll技术和Golang底层网络IO的原理。掌握网络IO模型、epoll技术和Golang底层网络IO原理，不仅有助于你更好地做框架选型，而且还能提升你使用Go开发更底层网络程序的能力。</p><h2>网络IO模型</h2><p>在介绍具体的网络IO模型之前，先让我们来想一想，一次网络IO的过程大概是什么样的呢？</p><p>就像下面的图一样，以读IO为例，网络数据要被咱们的应用程序接收到，可以划分为下面两个阶段。</p><ol>\n<li>\n<p>数据准备阶段，驱动程序和操作系统内核从网卡读取数据到socket的接收缓冲区。</p>\n</li>\n<li>\n<p>数据复制阶段，由应用程序将内核空间socket缓冲区的数据复制到用户空间。</p>\n</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/43/45/434d1c7ac5bf2ea7e8c94af9df05c445.jpg?wh=3534x2557\" alt=\"\" title=\"图1 网络数据传输\"></p><p>应用程序对这两个阶段的不同处理方式，就形成了不同的网络IO模型。那么应用程序对这两个阶段有哪几种处理方式呢？</p><h3>阻塞IO</h3><p>我们先来看看数据准备阶段的处理方式。就像下面的图一样，<strong>当我们的应用程序进行网络IO调用时，如果socket缓冲区还没有准备好，我们可以让应用线程阻塞在IO调用方法里，而不直接返回，这就是阻塞IO模型</strong>。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/4c/82/4cc6a6yy6e58c9eb3a7f6fe5e3658182.jpg?wh=3279x2167\" alt=\"\" title=\"图2 阻塞 IO 模型\"></p><p>那么使用阻塞IO的方式，会有什么问题呢？</p><p>当我们使用阻塞IO模型时，为了能及时处理多个连接的读写请求，就像下面的伪代码一样，<strong>每个连接我们都需要创建一个专门的线程来处理</strong>。在高性能服务器场景，当和客户端的连接比较多时，<strong>阻塞IO会导致创建比较多的线程，增加内存占用和上下文切换成本，降低服务器处理请求的吞吐</strong>。</p><pre><code class=\"language-go\">for {\n   // 获取本轮待处理的 fd\n   fd = accept() // 获取连接\n   new Thread(){ // 创建线程处理连接\n       // 从 fd 中读数据\n       data = read(fd)  \n       // 处理数据 \n       handle(data)  \n   }\n}\n</code></pre><h3>非阻塞IO</h3><p>为了解决高性能场景阻塞IO会创建较多线程的问题。操作系统给我们提供了非阻塞IO的方式，就像下面的图一样，<strong>当应用线程调用操作系统提供的读写方法时，如果socket缓冲区还没准备好，网络IO系统调用立即返回，不再阻塞应用线程。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/37/25/37e8ee15c4690220e8554yyfaf047425.jpg?wh=4221x2167\" alt=\"\" title=\"图3 非阻塞 IO 模型\"></p><p>使用非阻塞IO编程模型，我们可以实现线程复用，当一个连接的socket缓冲区未就绪时，线程可以处理另一个连接的请求，而不再陷入阻塞，从而解决阻塞IO模式线程数过多的问题。就像下面的伪代码一样。</p><pre><code class=\"language-go\">// 多个待服务的 fd \nfds = [fd1,fd2,fd3,...]\ni = 0\nfor {\n   fd = fds[i]        \n   // 尝试从连接中获取数据，socket未就绪直接返回，不再阻塞\n   data,err = tryRead(fd)  \n   // 读取数据成功，处理数据\n   if err == nil{\n      handle(data) \n   } \n   // 10ms后再推进流程，否则不断轮询会消耗过多CPU资源\n   sleep(10ms)\n   i++\n   if i == len(fds){\n     i = 0\n   }\n}\n</code></pre><p>那么非阻塞IO模型有什么问题呢？</p><p>非阻塞IO模型需要利用轮询不断做系统调用，浪费大量CPU资源。而且，当内核接收到数据时，数据需要等到应用线程下一次轮询才能复制到用户空间，得不到立刻处理，这可能会导致请求响应的延时比较高。</p><h3>IO多路复用</h3><p>为了能高效、及时地处理大量连接的 I/O 事件，操作系统还提供了<strong>IO多路复用</strong>的方式，让我们的应用线程能及时感知到socket缓冲区就绪的事件。</p><p>就像下面的图一样，我们<strong>可以在一个线程里阻塞监听多个连接的网络IO事件</strong>，当有连接的socket缓冲区准备好，IO多路复用的方法就会返回，让应用线程能及时处理连接的网络请求。</p><p><img src=\"https://static001.geekbang.org/resource/image/ce/59/cee08ec7a5ff6a952b53da8afea65c59.jpg?wh=3893x2167\" alt=\"\" title=\"图4 IO 多路复用\"></p><p>使用IO多路复用的方式，就像下面的伪代码一样，当没有连接的网络IO就绪时，多路复用的epoll_wait方法会<strong>阻塞，避免线程不断轮询消耗CPU资源</strong>。同时，网络IO就绪时，epoll_wait方法会立即返回，确保应用线程能<strong>及时感知网络IO就绪事件，避免处理请求不及时</strong>。</p><pre><code class=\"language-go\">// 多个待服务的 fd \nfds = [fd1,fd2,fd3,...]\nfor {\n   // 多路复用，阻塞同时监听多个连接\n   readyFds=epoll_wait(fds)\n   for i=0;i&lt;len(readyFds);i++{\n      // 开线程处理已就绪连接\n      new Thread(){\n          fd = readyFds[i]\n          data,err = read(fd)  \n           // 读取数据成功，处理数据\n           if err == nil{\n              handle(data) \n           } \n      }\n   }\n}\n</code></pre><h3>异步IO</h3><p>前面3种IO模型，由于在数据处理阶段或者是数据复制阶段，需要阻塞应用线程，因此属于同步IO模型。</p><p>实际上，还有一种完全不需要阻塞应用线程的网络IO模型——异步IO模型。就像下面的图一样，使用异步IO模型，应用线程从网络中读数据时，直接调用操作系统的方法并立即返回，由内核负责将socket缓冲区数据复制到用户空间，然后通知线程完成，整个过程完全没阻塞。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/2c/e98028b230e90ae719e1419764e1a12c.jpg?wh=4083x2166\" alt=\"\" title=\"图5 异步 IO\"></p><p>当然，因为各个平台对异步I/O模型的支持程度不一，且这种方式使用起来复杂度较高，因此使用并不是很广泛。目前主流网络服务器采用的多是I/O多路复用模型。</p><p>那么操作系统提供了哪些系统调用，来支持应用程序实现IO多路复用模型呢？</p><h2>epoll技术解析</h2><p>就拿主流的Linux内核来说，它主要提供了select、poll 和 epoll 三种 I/O 多路复用技术。</p><p>其中epoll是对select和poll机制的改进，能够提供更好的性能和扩展性，特别适用于高并发的网络服务器程序，我们接下来就重点学习一下epoll的功能。</p><p>如果我们想使用epoll技术来实现多路复用，可以使用Linux提供的下面三个系统调用。</p><ol>\n<li>\n<p>epoll_create函数，它的功能是在Linux内核创建一个内核需要监听的网络连接池子。</p>\n</li>\n<li>\n<p>epoll_ctl函数，它的功能是增删改池子里需要监听的连接和事件。</p>\n</li>\n<li>\n<p>epoll_wait函数，它的功能是阻塞等待池子里连接的网络IO事件。</p>\n</li>\n</ol><pre><code class=\"language-cpp\">#include &lt;sys/epoll.h&gt;\nint epoll_create(int size);\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);\nint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);\n</code></pre><p>当然，在使用epoll技术时，需要特别注意的一点是epoll触发模式的选择。也就是说，当我们每次调用epoll_wait方法时，操作系统是否需要反复通知应用线程某个连接的就绪事件<strong>。</strong></p><p>Linux提供了两种触发模式供我们选择。</p><p>一种是水平触发模式(Level - Triggered，LT）。<strong>在水平触发模式下，只要连接的socket缓冲区满足可读或可写的条件，epoll_wait函数就会返回这个连接的IO就绪事件</strong>。</p><p>以数据读取为例，假如某个连接的socket接收缓冲区中有 80 字节的数据，当我们调用epoll_wait返回后，应用线程读取了 20 字节。如果应用水平触发模式，那么当我们再次调用epoll_wait方法时，还会返回这个连接的网络IO就绪事件，因为还有 60 字节的数据在缓冲区中，直到这 80 字节的数据也被读完为止。</p><p>水平触发模式的优点是编程实现简单，缺点是当应用线程在读写缓冲区数据的过程中，由于没有读写完，epoll_wait会频繁返回这些连接的IO事件，导致应用程序需要不断地处理这些事件，这可能会增加系统的开销，降低性能。</p><p>Linux提供的另一种触发模式是边缘触发(Edge - Triggered，ET)。<strong>在边缘触发模式下，epoll_wait只会在连接对应的网络IO事件状态发生变化时才会返回这个事件</strong>，比如从不可读变为可读，或者从不可写变为可写。</p><p>仍以数据读取为例，当连接的socket接收缓冲区一开始没有数据时，如果有新的数据到达，epoll_wait会返回这个连接的可读事件，此时应用线程需要尽可能将缓冲区中的所有数据读取完。如果没有全部读取，下一次epoll_wait调用将不会返回这个连接的可读IO事件，直到又有新的数据到达。</p><p>在高并发场景下，边缘触发模式可以减少epoll_wait的返回次数，减少系统调用次数，提高系统的性能。但是边缘触发模式也存在缺点，假如应用线程在处理网络IO事件的过程中出错，或者没有及时处理网络IO事件，由于不会再收到IO事件就绪通知，处理不当很容易导致数据丢失。</p><h2>Golang网络IO模型</h2><p>掌握网络IO模型和epoll技术之后，我们已经搭建起了坚实的理论基础。现在，让我们来看看Go语言是如何巧妙运用这些理念和技术，来实现高性能网络通信的。</p><p>下面是我用Golang的net库实现的一个简单的TCP服务器，它的核心是下面几个方法的调用。</p><ul>\n<li>\n<p>Listen方法，用于创建一个 tcp 端口监听器 listener。</p>\n</li>\n<li>\n<p>Accept方法，用于阻塞获取到达的 tcp 连接。</p>\n</li>\n<li>\n<p>Read方法和Write方法，协程阻塞进行读写网络IO。</p>\n</li>\n</ul><pre><code class=\"language-go\">package main\n\nimport (\n    \"fmt\"\n    \"net\"\n)\n\nfunc handleConnection(conn net.Conn) {\n    defer conn.Close()\n    // 用于读取客户端发送数据的缓冲区\n    buffer := make([]byte, 1024)\n    for {\n        n, err := conn.Read(buffer)\n        if err!= nil {\n            fmt.Println(\"读取客户端数据出错:\", err)\n            break\n        }\n        // 输出客户端发送的数据\n        fmt.Printf(\"从客户端接收到: %s\\n\", buffer[:n])\n        // 向客户端发送响应信息\n        _, err = conn.Write([]byte(\"已收到你的消息\\n\"))\n        if err!= nil {\n            fmt.Println(\"向客户端发送响应出错:\", err)\n            break\n        }\n    }\n}\n\nfunc main() {\n    // 监听的地址和端口\n    listenAddr := \":8888\"\n    // 创建监听的TCP套接字\n    listener, err := net.Listen(\"tcp\", listenAddr)\n    if err!= nil {\n        fmt.Println(\"监听出错:\", err)\n        return\n    }\n    defer listener.Close()\n\n    fmt.Printf(\"服务器正在监听 %s\\n\", listenAddr)\n    for {\n        // 接受客户端连接\n        conn, err := listener.Accept()\n        if err!= nil {\n            fmt.Println(\"接受客户端连接出错:\", err)\n            continue\n        }\n        // 启动一个协程来处理客户端连接\n        go handleConnection(conn)\n    }\n}\n</code></pre><p>Golang I/O 多路复用和epoll调用的细节，就隐藏在这些方法内部和Golang运行时里。</p><p>首先，我们来看看Listen方法，实际上，它最终会调用操作系统的epoll_create方法，创建一个epoll池。</p><pre><code class=\"language-go\">//runtime/netpoll_epoll.go\nfunc netpollinit() {\n    // 调通epoll_create，创建epoll池\n    epfd = epollcreate1(_EPOLL_CLOEXEC)\n}\n</code></pre><p>创建完epoll池，它会将需要监听网络连接的文件描述符(fd)，通过调用操作系统的epoll_ctl方法，加入到epoll池子里。</p><pre><code class=\"language-go\">//runtime/netpoll_epoll.go\nfunc netpollopen(fd uintptr, pd *pollDesc) int32 {\n    var ev epollevent\n    ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET\n    *(**pollDesc)(unsafe.Pointer(&amp;ev.data)) = pd\n    // 调用epoll_ctl，将需要监听的连接加到epoll池里\n    return -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &amp;ev)\n}\n</code></pre><p>接着，我们来看看Accept方法。Accept方法会尝试非阻塞获取TCP连接，如果能够获取到，则会调用epoll_ctl方法将新连接加入epoll池。</p><pre><code class=\"language-go\">//runtime/netpoll_epoll.go\nfunc netpollopen(fd uintptr, pd *pollDesc) int32 {\n    var ev epollevent\n    ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET\n    *(**pollDesc)(unsafe.Pointer(&amp;ev.data)) = pd\n    return -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &amp;ev)\n}\n</code></pre><p>如果获取不到连接，协程会陷入阻塞，并触发协程调度。</p><pre><code class=\"language-go\">//runtime/netpoll.go\n\n// returns true if IO is ready, or false if timedout or closed\n// waitio - wait only for completed IO, ignore errors\nfunc netpollblock(pd *pollDesc, mode int32, waitio bool) bool {\n    // IO未就绪，协程阻塞\n    if waitio || netpollcheckerr(pd, mode) == 0 {\n        gopark(netpollblockcommit, unsafe.Pointer(gpp), waitReasonIOWait, traceEvGoBlockNet, 5)\n    }\n}\n</code></pre><p>然后，我们来看看Read和Write方法。Read和Write方法会尝试非阻塞读写数据，如果socket缓冲区就绪，就会进入前面网络IO模型讲到的数据复制阶段；如果未就绪，调用方法的协程会阻塞。</p><pre><code class=\"language-go\">//runtime/netpoll.go\n\n// returns true if IO is ready, or false if timedout or closed\n// waitio - wait only for completed IO, ignore errors\nfunc netpollblock(pd *pollDesc, mode int32, waitio bool) bool {\n    // IO未就绪，协程阻塞\n    if waitio || netpollcheckerr(pd, mode) == 0 {\n        gopark(netpollblockcommit, unsafe.Pointer(gpp), waitReasonIOWait, traceEvGoBlockNet, 5)\n    }\n}\n</code></pre><p>最后，让我们来看看，Golang运行时是如何感知网络IO就绪事件，唤醒因网络IO事件未就绪而陷入阻塞的协程的。Golang运行时里，下面几个地方会调用操作系统的epoll_wait方法完成这个目标。</p><p>第一个地方是在全局监控任务 sysmon里。在程序启动时，Golang底层会单独启动一个线程，用于执行 sysmon 监控任务。</p><pre><code class=\"language-go\">// runtime/proc.go\nfunc main() {\n    // 启动一个线程\n    systemstack(func() {\n        newm(sysmon, nil, -1)\n    })\n    \n}\n</code></pre><p>在监控任务里，每隔 10ms会轮询调用<strong>netpoll 函数</strong>，这个函数会尝试取出网络IO事件就绪的协程列表，进行唤醒操作。</p><pre><code class=\"language-go\">func sysmon() {\n    for {\n        // 每隔10s周期处理\n        // poll network if not polled for more than 10ms\n        lastpoll := sched.lastpoll.Load()\n        if netpollinited() &amp;&amp; lastpoll != 0 &amp;&amp; lastpoll+10*1000*1000 &lt; now {\n            sched.lastpoll.CompareAndSwap(lastpoll, now)\n            // 获取网络IO就绪的协程列表\n            list, delta := netpoll(0) // non-blocking - returns list of goroutines\n            if !list.empty() {\n                // 唤醒就绪协程\n                injectglist(&amp;list)\n            }\n        }\n    }\n}\n</code></pre><p>而 netpoll 方法的底层，就像下面的代码一样，会基于非阻塞模式调用操作系统的epoll_wait 方法，获取到就绪事件队列 events。然后遍历事件队列，将对应的协程添加到协程列表中返回给上层用于执行唤醒操作。</p><pre><code class=\"language-go\">//  runtime/netpoll_epoll.go\n// netpoll checks for ready network connections.\n// Returns list of goroutines that become runnable.\nfunc netpoll(delay int64) (gList, int32) {\n    var events [128]syscall.EpollEvent\n    // 调用epoll_wait非阻塞获取就绪的网络IO事件\n    n, errno := syscall.EpollWait(epfd, events[:], int32(len(events)), waitms)\n    //从events中获取事件对应的协程\n    \n    return toRun, delta\n}\n</code></pre><p>第二个地方是在协程调度流程中。在进行协程调度时，<strong>findRunnable函数</strong>会为当前处理器寻找下一个可执行的协程。如果此时没有可调度协程，findRunnable函数就会尝试获取网络IO就绪的协程用于调度执行。</p><pre><code class=\"language-go\">// runtime/proc.go\n// Finds a runnable goroutine to execute.\nfunc findRunnable() (gp *g, inheritTime, tryWakeP bool) {\n    // Poll network.\n    \n    // P 本地队列和全局队列都没有待执行的协程\n    if netpollinited() &amp;&amp; netpollAnyWaiters() &amp;&amp; sched.lastpoll.Load() != 0 {\n        // 调用epoll_wait获取IO事件就绪协程列表\n        if list, delta := netpoll(0); !list.empty() { // non-blocking\n            gp := list.pop()\n            // 唤醒相关协程\n            injectglist(&amp;list)\n        }\n    }\n}\n</code></pre><p>第三个地方是在GC流程中。在 GC 过程中，每次调用完STW（stop the world）后，都会调用 start the world，此时也会对网络IO就绪的协程进行唤醒操作，以便网络IO事件能得到及时处理。</p><pre><code class=\"language-go\">//runtime/proc.go\n\n// stattTheWorldWithSema returns now.\nfunc startTheWorldWithSema(now int64, w worldStop) int64 {\n    if netpollinited() {\n        list, delta := netpoll(0) // non-blocking\n        injectglist(&amp;list) // 唤醒IO就绪的协程列表\n    }\n}\n</code></pre><h2>小结</h2><p>今天这节课，我们一起学习了网络编程相关的核心知识，包括网络IO模型、epoll技术和Golang底层网络模型的原理。现在让我们来回顾一下这节课学到的网络编程知识。</p><p>网络IO模型有阻塞IO、非阻塞IO、IO多路复用和异步IO多种类型，实践中比较常用的是IO多路复用模型。</p><p>之后我们重点了解了Linux底层的多路复用技术——epoll，操作系统提供了epoll_create、epoll_ctl和epoll_wait三个方法给我们使用。在使用时，我们需要注意触发模式的选择。</p><p>最后，我以一段TCP服务器代码为例，深入分析了Go语言底层是如何巧妙运用网络IO模型和epoll技术，来实现高性能网络通信的。希望你能够用心体会今天讲到的网络编程知识，提升你使用Go开发更底层网络程序的能力。</p><h2>思考题</h2><p>虽然Golang网络库的性能已经很高了，但还是有不少高性能网络库在Golang 官方库的基础上进行了改进，请找一个高性能网络库并分析它的改进点。</p><p>欢迎你把你的答案分享在评论区，也欢迎你把这节课的内容分享给需要的朋友，我们下节课再见！</p>","comments":[{"had_liked":false,"id":396659,"user_name":"CodeFish-Xiao","can_delete":false,"product_type":"c1","uid":2217807,"ip_address":"广东","ucode":"E0834E4560B0BE","user_header":"https://static001.geekbang.org/account/avatar/00/21/d7/4f/5059c43a.jpg","comment_is_top":false,"comment_ctime":1735264135,"is_pvip":false,"replies":[{"id":143982,"content":"高性能网络编程核心就是网络IO模型的使用，分析Go net库的实现也是为了让大家有个感知，底层库是如何使用网络IO模型实现高性能网络IO的。\n对应用层研发来说，这部分内容确实是比较偏底层，看起来并不需要我们自己写代码，但是了解底层实现也是有好处的，一个是做网络库和框架的选型，包括我问的问题，也是为了引导大家去分析其它网络库，对比差异点，如果不了解底层知识是很难理解不同库的差异去做选型的，另一个面试需要。\n对底层网络库的研发来说，这部分内容本身，就是可以借鉴的地方。","user_name":"作者回复","user_name_real":"编辑","uid":1313417,"ctime":1735482305,"ip_address":"广东","comment_id":396659,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100843701,"comment_content":"感觉这篇的质量跟之前比下降了，Golang运行时的网络实现和Linux本身的IO复用优化，但是实际上我们用Golang进行网络编程该进行哪些优化没有讲到\n","like_count":1,"discussions":[{"author":{"id":1313417,"avatar":"https://static001.geekbang.org/account/avatar/00/14/0a/89/eb8c28a4.jpg","nickname":"徐逸","note":"","ucode":"DCFDEE08FD263A","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":655646,"discussion_content":"高性能网络编程核心就是网络IO模型的使用，分析Go net库的实现也是为了让大家有个感知，底层库是如何使用网络IO模型实现高性能网络IO的。\n对应用层研发来说，这部分内容确实是比较偏底层，看起来并不需要我们自己写代码，但是了解底层实现也是有好处的，一个是做网络库和框架的选型，包括我问的问题，也是为了引导大家去分析其它网络库，对比差异点，如果不了解底层知识是很难理解不同库的差异去做选型的，另一个面试需要。\n对底层网络库的研发来说，这部分内容本身，就是可以借鉴的地方。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1735482305,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":396663,"user_name":"lJ","can_delete":false,"product_type":"c1","uid":2562558,"ip_address":"江苏","ucode":"CC29D06A16FF93","user_header":"https://static001.geekbang.org/account/avatar/00/27/19/fe/d31344db.jpg","comment_is_top":false,"comment_ctime":1735270780,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":2,"product_id":100843701,"comment_content":"1. 老师能讲一讲io_uring吗，有哪些知名的应用，大厂的态度，Golang的支持情况等\n2. epoll ET模式存在数据丢失的风险，如果接收缓冲区足够大的情况下，还存在丢失吗，后续新的数据到达重新触发通知，应用程序应该可以读取到之前未读完的数据吧\n3. golang net是如何解决数据丢失的风险的，在使用epoll ET编程时有哪些开发规范或最佳实践应对这个问题\n4. 思考题\ngolang net设计了 BIO模式的 API，为每个连接都分配一个 goroutine。 这在高并发下，会产生大量的 goroutine，需要频繁的上下文切换，增大Goroutine 调度器的开销。  \na. evio，使用事件驱动模型，采用单线程或多线程事件循环，比协程并发模型更轻量。  \nb. netpoll，使用gopool池、高效的内存复用、支持检查连接是否存活，可以及时清理池中失效的连接，降低资源占用。   \nc. gnet，也是使用ants池，高效、可重用而且自动伸缩的内存 buffer。   \n以上三个库没有看过源码，简单看了官方文档。个人觉得，evio与netpoll，gnet的不同之处是抛开了协程并发模型，完全基于epoll事件循环模型。而后两者还是采用协程模型，主要使用了协程池，内存池优化达到资源复用，减轻了调度器和GC开销。","like_count":1,"discussions":[{"author":{"id":1567014,"avatar":"https://static001.geekbang.org/account/avatar/00/17/e9/26/472e16e4.jpg","nickname":"Amosヾ","note":"","ucode":"833F6FCB4042AD","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":655679,"discussion_content":"golang net 是使用 LT 的吧？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1735574152,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2562558,"avatar":"https://static001.geekbang.org/account/avatar/00/27/19/fe/d31344db.jpg","nickname":"lJ","note":"","ucode":"CC29D06A16FF93","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1567014,"avatar":"https://static001.geekbang.org/account/avatar/00/17/e9/26/472e16e4.jpg","nickname":"Amosヾ","note":"","ucode":"833F6FCB4042AD","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":655689,"discussion_content":"//runtime/netpoll_epoll.go\nfunc netpollopen(fd uintptr, pd *pollDesc) int32 {\n    var ev epollevent\n    ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET\n    *(**pollDesc)(unsafe.Pointer(&amp;ev.data)) = pd\n    return -epollctl(epfd, _EPOLL_CTL_ADD, int32(fd), &amp;ev)\n}\nev.events设置了_EPOLLET","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1735612148,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":655679,"ip_address":"江苏","group_id":0},"score":655689,"extra":""}]}]}]}