{"id":93965,"title":"11 | Kappa架构：利用Kafka锻造的屠龙刀","content":"<p>你好，我是蔡元楠。</p><p>今天我要分享的主题是Kappa架构。</p><p>同样身为大规模数据处理架构，Kappa架构这把利用Kafka锻造的“屠龙刀”，它与Lambda架构的不同之处在哪里呢？</p><p>上一讲中，我讲述了在处理大规模数据时所用到经典架构，Lambda架构。我先来带你简要回顾一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/8f/23/8fe667211309978b2dd6cb6948939923.jpg?wh=634*227\" alt=\"\"></p><p>Lambda架构结合了批处理和流处理的架构思想，将进入系统的大规模数据同时送入这两套架构层中，分别是批处理层（Batch Layer）和速度层（Speed Layer），同时产生两套数据结果并存入服务层。</p><p>批处理层有着很好的容错性，同时也因为保存着所有的历史记录，使产生的数据集具有很好的准确性。速度层可以及时地处理流入的数据，因此具有低延迟性。最终服务层将这两套数据结合，并生成一个完整的数据视图提供给用户。</p><p>Lambda架构也具有很好的灵活性，你可以将现有开源生态圈中不同的平台套入这个架构，具体请参照上一讲内容。</p><h2>Lambda架构的不足</h2><p>虽然Lambda架构使用起来十分灵活，并且可以适用于很多的应用场景，但在实际应用的时候，Lambda架构也存在着一些不足，主要表现在它的维护很复杂。</p><p>使用Lambda架构时，架构师需要维护两个复杂的分布式系统，并且保证他们逻辑上产生相同的结果输出到服务层中。</p><!-- [[[read_end]]] --><p>举个例子吧，我们在部署Lambda架构的时候，可以部署Apache Hadoop到批处理层上，同时部署Apache Flink到速度层上。</p><p>我们都知道，在分布式框架中进行编程其实是十分复杂的，尤其是我们还会针对不同的框架进行专门的优化。所以几乎每一个架构师都认同，Lambda架构在实战中维护起来具有一定的复杂性。</p><p>那要怎么解决这个问题呢？我们先来思考一下，造成这个架构维护起来如此复杂的根本原因是什么呢？</p><p>维护Lambda架构的复杂性在于我们要同时维护两套系统架构：批处理层和速度层。我们已经说过了，在架构中加入批处理层是因为从批处理层得到的结果具有高准确性，而加入速度层是因为它在处理大规模数据时具有低延时性。</p><p>那我们能不能改进其中某一层的架构，让它具有另外一层架构的特性呢？</p><p>例如，改进批处理层的系统让它具有更低的延时性，又或者是改进速度层的系统，让它产生的数据视图更具准确性和更加接近历史数据呢？</p><p>另外一种在大规模数据处理中常用的架构——Kappa架构（Kappa Architecture），便是在这样的思考下诞生的。</p><h2>Kappa架构</h2><p>Kappa架构是由LinkedIn的前首席工程师杰伊·克雷普斯（Jay Kreps）提出的一种架构思想。克雷普斯是几个著名开源项目（包括Apache Kafka和Apache Samza这样的流处理系统）的作者之一，也是现在Confluent大数据公司的CEO。</p><p>克雷普斯提出了一个改进Lambda架构的观点：</p><blockquote>\n<p>我们能不能改进Lambda架构中速度层的系统性能，使得它也可以处理好数据的完整性和准确性问题呢？我们能不能改进Lambda架构中的速度层，使它既能够进行实时数据处理，同时也有能力在业务逻辑更新的情况下重新处理以前处理过的历史数据呢？</p>\n</blockquote><p>他根据自身多年的架构经验发现，我们是可以做到这样的改进的。</p><p>在前面Publish–Subscribe模式那一讲中，我讲到过像Apache Kafka这样的流处理平台是具有永久保存数据日志的功能的。通过平台的这一特性，我们可以重新处理部署于速度层架构中的历史数据。</p><p>下面我就以Apache Kafka为例来讲述整个全新架构的过程。</p><p>第一步，部署Apache Kafka，并设置数据日志的保留期（Retention Period）。这里的保留期指的是你希望能够重新处理的历史数据的时间区间。</p><p>例如，如果你希望重新处理最多一年的历史数据，那就可以把Apache Kafka中的保留期设置为365天。如果你希望能够处理所有的历史数据，那就可以把Apache Kafka中的保留期设置为“永久（Forever）”。</p><p>第二步，如果我们需要改进现有的逻辑算法，那就表示我们需要对历史数据进行重新处理。</p><p>我们需要做的就是重新启动一个Apache Kafka作业实例（Instance）。这个作业实例将重头开始，重新计算保留好的历史数据，并将结果输出到一个新的数据视图中。我们知道Apache Kafka的底层是使用Log Offset来判断现在已经处理到哪个数据块了，所以只需要将Log Offset设置为0，新的作业实例就会重头开始处理历史数据。</p><p>第三步，当这个新的数据视图处理过的数据进度赶上了旧的数据视图时，我们的应用便可以切换到从新的数据视图中读取。</p><p>第四步，停止旧版本的作业实例，并删除旧的数据视图。</p><p>这个架构就如同下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/ff/f45975d67c2bf9640e6361c8c23727ff.png?wh=684*286\" alt=\"\"></p><p>与Lambda架构不同的是，Kappa架构去掉了批处理层这一体系结构，而只保留了速度层。你只需要在业务逻辑改变又或者是代码更改的时候进行数据的重新处理。</p><p>当然了，你也可以在我上面讲到的步骤中做一些优化。</p><p>例如不执行第4步，也就是不删除旧的数据视图。这样的好处是当你发现代码逻辑出错时可以及时回滚（Roll Back）到上一个版本的数据视图中去。又或者是你想在服务层提供A/B测试，保留多个数据视图版本将有助于你进行A/B测试。</p><p>在介绍完Kappa架构的概念后，我想通过一个实战例子，来和你进一步学习Kappa架构是如何应用在现实场景中的。</p><h2>《纽约时报》内容管理系统架构实例</h2><p>《纽约时报》是一个在美国纽约出版，在整个美国乃至全世界都具有相当影响力的日报。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/2b/a93e2423d7da056ae325ea639b4daf2b.jpg?wh=2792*1322\" alt=\"\"></p><p>《纽约时报》的内容管理系统收集、保存着各种各样来源的文档。这些文档有从第三方收集来的资料，也有自己报社编辑部所撰写的故事。当你访问《纽约时报》网站主页时，甚至能够查到162年前的新闻报道。</p><p>可想而知，要处理这么大规模的内容，并将这些内容提供于在线搜索、订阅的个性化推荐以及前端应用程序等等的服务，是一个非常棘手的任务。</p><p>我们先来看看他们曾经使用过的一个老式系统架构。</p><p><img src=\"https://static001.geekbang.org/resource/image/ff/f1/ff0050f97b7b2b1be3d66353900a66f1.png?wh=525*407\" alt=\"\"></p><p>我们可以看到，这种系统架构是一种相当典型的基于API的架构，无论是在系统调度上还是使用场景上都存在着自身的不足。我来给你举一些例子。</p><ul>\n<li>\n<p>不同的内容API可能由不同的团队开发，从而造成API有不同的语义，也有可能需要不同的参数。</p>\n</li>\n<li>\n<p>调用不同API所得到的内容结果可能有不同的格式，在应用端需要重新进行规范化（Standardization）。</p>\n</li>\n<li>\n<p>如果客户端上会实时推送一些新的热点新闻或者突发新闻（Breaking News），那么在上述基于API的架构中，想要实时获知新闻的话，就需要让客户端不停地做轮询操作（Polling）。轮询操作在这里指的是客户端定期地重复调用系统API来查看是否有新的新闻内容，这无疑增加了系统的复杂性。</p>\n</li>\n<li>\n<p>客户端很难访问以前发布过的内容。即便我们知道这些已发布过的新闻列表需要从哪里获取，进行API调用去检索每个单独的新闻列表还是需要花很长的时间。而过多的API调用又会给服务器产生很大的负荷。</p>\n</li>\n</ul><p>那现在你再来看看当《纽约时报》采取了Kappa架构之后，新的系统架构是什么样的。</p><p><img src=\"https://static001.geekbang.org/resource/image/73/61/73877382d7b1bfc01bbcd9951ec7ca61.png?wh=654*401\" alt=\"\"></p><p>首先，Kappa架构在系统调度这个层面上统一了开发接口。</p><p>你可以看到，中间的Kappa架构系统规范好了输入数据和输出数据的格式之后，任何需要传送到应用端的数据都必须按照这个接口输入给Kappa架构系统。而所有的应用端客户都只需要按照Kappa架构系统定义好的输出格式接收传输过来的数据。这样就解决了API规范化的问题。</p><p>我们再来看看增加了中间一层Kappa架构之后数据传输速度上的变化。</p><p>因为Apache Kafka是可以实时推送消息数据的，这样一来，任何传输进中间Kappa架构的数据都会被实时推送到接收消息的客户端中。这样就避免了在应用层面上做定期轮询，从而减少了延时。而对于重新访问或者处理发布过的新闻内容这一问题，还记得我之前和你讲述过的Kafka特性吗？只需要设置Log Offset为0就可以重新读取所有内容了。</p><p>在讲述完Kappa架构和它的应用实例之后，我想强调一下，Kappa架构也是有着它自身的不足的。</p><p>因为Kappa架构只保留了速度层而缺少批处理层，在速度层上处理大规模数据可能会有数据更新出错的情况发生，这就需要我们花费更多的时间在处理这些错误异常上面。</p><p>还有一点，Kappa架构的批处理和流处理都放在了速度层上，这导致了这种架构是使用同一套代码来处理算法逻辑的。所以Kappa架构并不适用于批处理和流处理代码逻辑不一致的场景。</p><h2>小结</h2><p>在最近两讲中，我们学习到了Lambda架构和Kappa架构这两种大规模数据处理架构，它们都各自有着自身的优缺点。我们需要按照实际情况来权衡利弊，看看我们在业务中到底需要使用到哪种架构。</p><p>如果你所面对的业务逻辑是设计一种稳健的机器学习模型来预测即将发生的事情，那么你应该优先考虑使用Lambda架构，因为它拥有批处理层和速度层来确保更少的错误。</p><p>如果你所面对的业务逻辑是希望实时性比较高，而且客户端又是根据运行时发生的实时事件来做出回应的，那么你就应该优先考虑使用Kappa架构。</p><h2>思考题</h2><p>在学习完Lambda架构和Kappa架构之后，你能说出Kappa架构相对Lambda架构的优势吗？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":93407,"user_name":"程序设计的艺术","can_delete":false,"product_type":"c1","uid":1107847,"ip_address":"","ucode":"CA3817C11CD9E6","user_header":"https://static001.geekbang.org/account/avatar/00/10/e7/87/9ad59b98.jpg","comment_is_top":false,"comment_ctime":1557464869,"is_pvip":false,"replies":[{"id":"33554","content":"谢谢你的提问！<br>1. 批处理和实时运算中的处理肯定都具有准确性的，只是这里所说的高准确性是批处理结果相对于流处理结果而言的，因为毕竟批处理所处理的数据是历史数据。举个例子，假设我们拥有一个人近几年的上班目的地的历史数据，大部分时间这个人上班都在A地，只有少部分时间在B地。那批处理层所处理完这些历史数据之后可能会判断出这个人的常规上班地点是A，出差地点是B。如果是实时处理的话，可能刚好拿到的数据就是出差的那几天地点B，那实时处理的判断可能会判断出这个人的常规上班地点是B了。所以相对而言，批处理具有高准确性。<br>2. 无论是数据出错或者还是逻辑出错，批处理和实时处理肯定都会发生的。流处理的处理方式并不是按照任务为单位来计算的，通常都是把每一份数据当作是一个消息，流入的消息处理过了就不会再回头了。批处理的处理方式就是不断的有定时任务去重新处理所有历史数据。在Kappa架构下，除非你的logging做得非常好，能够知道在从哪一个数据时间点上出错了，那就把offset调回那个点重新从那个点开始重新计算。如果是逻辑有所改变了，那肯定是要全部数据从头完全重新计算了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557692311,"ip_address":"","comment_id":93407,"utype":1}],"discussion_count":2,"race_medal":0,"score":"117521581861","product_id":100025301,"comment_content":"有几个地方没太明白：<br>1.批处理数据具有很高的准确性，实时运算处理就没有？<br>2.关于数据错误，两者应该都有吧？数据错误后两者的处理不一样吗？比如10个任务中的3个有错误，批处理和实时处理都应该可以找到3个任务重新计算吧？<br>为什么实时运算需要完全从头计算所有任务？<br>谢谢","like_count":27,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449618,"discussion_content":"谢谢你的提问！\n1. 批处理和实时运算中的处理肯定都具有准确性的，只是这里所说的高准确性是批处理结果相对于流处理结果而言的，因为毕竟批处理所处理的数据是历史数据。举个例子，假设我们拥有一个人近几年的上班目的地的历史数据，大部分时间这个人上班都在A地，只有少部分时间在B地。那批处理层所处理完这些历史数据之后可能会判断出这个人的常规上班地点是A，出差地点是B。如果是实时处理的话，可能刚好拿到的数据就是出差的那几天地点B，那实时处理的判断可能会判断出这个人的常规上班地点是B了。所以相对而言，批处理具有高准确性。\n2. 无论是数据出错或者还是逻辑出错，批处理和实时处理肯定都会发生的。流处理的处理方式并不是按照任务为单位来计算的，通常都是把每一份数据当作是一个消息，流入的消息处理过了就不会再回头了。批处理的处理方式就是不断的有定时任务去重新处理所有历史数据。在Kappa架构下，除非你的logging做得非常好，能够知道在从哪一个数据时间点上出错了，那就把offset调回那个点重新从那个点开始重新计算。如果是逻辑有所改变了，那肯定是要全部数据从头完全重新计算了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557692311,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1506831,"avatar":"https://static001.geekbang.org/account/avatar/00/16/fe/0f/5c778928.jpg","nickname":"一雪","note":"","ucode":"224DE0CDEA4067","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":12630,"discussion_content":"用“稳定性”这个词或许更好？稳定意味着更可靠的估计","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1568556647,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93660,"user_name":"朱同学","can_delete":false,"product_type":"c1","uid":1514233,"ip_address":"","ucode":"2EF7D5A051712C","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/f9/180f347a.jpg","comment_is_top":false,"comment_ctime":1557550700,"is_pvip":false,"replies":[{"id":"33468","content":"谢谢你的留言！哈哈，可以这么理解，而且这个队列还必须具有重新处理所有历史数据的能力。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557562188,"ip_address":"","comment_id":93660,"utype":1}],"discussion_count":3,"race_medal":0,"score":"96046831212","product_id":100025301,"comment_content":"我感觉这是用队列代替了hdfs","like_count":22,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449717,"discussion_content":"谢谢你的留言！哈哈，可以这么理解，而且这个队列还必须具有重新处理所有历史数据的能力。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1557562188,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1070154,"avatar":"https://static001.geekbang.org/account/avatar/00/10/54/4a/d8c7cacb.jpg","nickname":"徐飞","note":"","ucode":"C4A0B6D45338E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331852,"discussion_content":"kafka替代不了现有数仓：1、SQL的使用；2、历史数据是物料、数据分析逻辑是无止境的；3、现有技术体系的沉淀","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1606992489,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1692756,"avatar":"https://static001.geekbang.org/account/avatar/00/19/d4/54/7263deb2.jpg","nickname":"吃饭","note":"","ucode":"AF0D7165D5F049","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":319554,"discussion_content":"全量处理历史数据怎么快的起来呢？\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604049582,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93211,"user_name":":)","can_delete":false,"product_type":"c1","uid":1239198,"ip_address":"","ucode":"23D505949442B6","user_header":"https://static001.geekbang.org/account/avatar/00/12/e8/9e/6550a051.jpg","comment_is_top":false,"comment_ctime":1557426146,"is_pvip":false,"replies":[{"id":"33252","content":"谢谢你的留言！不错的总结！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557448120,"ip_address":"","comment_id":93211,"utype":1}],"discussion_count":1,"race_medal":0,"score":"83161804770","product_id":100025301,"comment_content":"1.kappa架构使用更少的技术栈，实时和历史部分都是同一套技术栈。lambda架构为了解决历史部分和实时部分可能会使用不同的技术栈。<br><br>2.kappa架构使用了统一的处理逻辑。而lambda架构分别为历史和实时部分使用了两套逻辑。一旦需求变更，两套逻辑都要同时变更。<br><br>3.kappa架构具有流式处理的特点和优点。比如可以具有多个订阅者，比如具有更高的吞吐量。<br><br>","like_count":19,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449528,"discussion_content":"谢谢你的留言！不错的总结！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557448120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93222,"user_name":"Rainbow","can_delete":false,"product_type":"c1","uid":1259525,"ip_address":"","ucode":"248A7E2C05E4DE","user_header":"https://static001.geekbang.org/account/avatar/00/13/38/05/67aae6c8.jpg","comment_is_top":false,"comment_ctime":1557442790,"is_pvip":false,"replies":[{"id":"33278","content":"谢谢你的提问！Spark不算Kappa架构，Spark是有能力可以处理批处理和流处理，我们可以利用Spark搭建Kappa架构出来，但是它本身不具备这种架构的思想在里面。这就好比我们可以借助编程语言实现一些算法，但是我们不会说这种编程语言就属于这种算法一样。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557454491,"ip_address":"","comment_id":93222,"utype":1}],"discussion_count":1,"race_medal":0,"score":"74571886822","product_id":100025301,"comment_content":"spark算kappa架构吗？批处理 流处理一套","like_count":17,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449532,"discussion_content":"谢谢你的提问！Spark不算Kappa架构，Spark是有能力可以处理批处理和流处理，我们可以利用Spark搭建Kappa架构出来，但是它本身不具备这种架构的思想在里面。这就好比我们可以借助编程语言实现一些算法，但是我们不会说这种编程语言就属于这种算法一样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557454491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95517,"user_name":"罗钛龙","can_delete":false,"product_type":"c1","uid":1259075,"ip_address":"","ucode":"10B546AEAA8D08","user_header":"https://static001.geekbang.org/account/avatar/00/13/36/43/b89372d1.jpg","comment_is_top":false,"comment_ctime":1558072747,"is_pvip":false,"replies":[{"id":"34205","content":"谢谢你的提问！Kappa架构是具有从头计算的能力，但是这个架构并不太适用于需要经常重新计算历史数据的应用场景。当我们发现有重大逻辑错误出现或者修改的时候才会从头计算所有的数据。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1558128256,"ip_address":"","comment_id":95517,"utype":1}],"discussion_count":2,"race_medal":0,"score":"44507745707","product_id":100025301,"comment_content":"有个问题不明白，实时数据量很大的化，每次都从头计算，是否削弱了速度层实时性的特点，这样不和初心违背了么？","like_count":10,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450492,"discussion_content":"谢谢你的提问！Kappa架构是具有从头计算的能力，但是这个架构并不太适用于需要经常重新计算历史数据的应用场景。当我们发现有重大逻辑错误出现或者修改的时候才会从头计算所有的数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1558128256,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1070154,"avatar":"https://static001.geekbang.org/account/avatar/00/10/54/4a/d8c7cacb.jpg","nickname":"徐飞","note":"","ucode":"C4A0B6D45338E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331854,"discussion_content":"所以应用场景非常局限","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606992563,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96023,"user_name":"我只是想改个名字","can_delete":false,"product_type":"c1","uid":1513064,"ip_address":"","ucode":"59C36D985D8819","user_header":"https://static001.geekbang.org/account/avatar/00/17/16/68/c4d612b3.jpg","comment_is_top":false,"comment_ctime":1558313492,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"40213019156","product_id":100025301,"comment_content":"在我看来，Kappa架构和lambda架构没有绝对的谁好，就拿我们现在的业务场景，MySQL同步至tidb平台，原数据进kafka就有可能有问题，而我们又是金融公司，就只能选择lambda架构随时对数据进行校验，对最终数据进行纠正。","like_count":9},{"had_liked":false,"id":93373,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1557458914,"is_pvip":false,"replies":[{"id":"33341","content":"谢谢你的留言！是的呢，现在只有用Kafka来实现Kappa架构。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557465903,"ip_address":"","comment_id":93373,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35917197282","product_id":100025301,"comment_content":"这样批和流的压力就全压到kafka 上了，对kafka并发的要求也非常高，应该只有kafka 能做到这件事了吧<br>","like_count":8,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449603,"discussion_content":"谢谢你的留言！是的呢，现在只有用Kafka来实现Kappa架构。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557465903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93521,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1557484860,"is_pvip":false,"replies":[{"id":"33463","content":"谢谢你的留言！你说得也没有错，现在硅谷这边实践得比较多的可能就Linkedin或者Confluent了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557561081,"ip_address":"","comment_id":93521,"utype":1}],"discussion_count":2,"race_medal":0,"score":"31622255932","product_id":100025301,"comment_content":"不可能啊，如果要保存长久的数据，那么kafka的集群容量得有多大，按每天就一个t来说，加上默认副本3，那一年要存的数据量就得很多了。 这还可能只是其中一个业务。国内有小公司敢这样做吗","like_count":7,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449665,"discussion_content":"谢谢你的留言！你说得也没有错，现在硅谷这边实践得比较多的可能就Linkedin或者Confluent了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557561081,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1067981,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4b/cd/185e5378.jpg","nickname":"泊浮目","note":"","ucode":"182A7CC2F6BDAB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":286373,"discussion_content":"HDFS也有副本啊。况且，无论是HDFS还是Kafka，相关的副本数都是可以设置的。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1593151413,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93323,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1557452631,"is_pvip":false,"replies":[{"id":"33288","content":"谢谢你的留言总结！是的，现在Kappa架构用得比较多的是Confluent的数据平台，不过他们也没有放出Benchmark，具体和Lambda相比性能差距多少还不确定。不过我赞同你说kappa不适合批处理多的架构，毕竟如果常常要重做批处理的话，性能肯定会受影响的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557456919,"ip_address":"","comment_id":93323,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27327256407","product_id":100025301,"comment_content":"如果批处理比较多的话，每次都从kafka的earlist offset消费的话，第一会耗费很长很长时间，而且消费者如果资源不够多，会导致任务堆积的吧。所以kappa不适合批处理多的架构。<br><br>Kappa架构因为整合了批处理层和速度层，优势就是:<br>1. 实时性比较高，适合对实时性要求高的场景<br>2. 业务逻辑可以使用统一的API来编写，那么对于之后的业务需求变更和代码维护都比较友好","like_count":6,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449583,"discussion_content":"谢谢你的留言总结！是的，现在Kappa架构用得比较多的是Confluent的数据平台，不过他们也没有放出Benchmark，具体和Lambda相比性能差距多少还不确定。不过我赞同你说kappa不适合批处理多的架构，毕竟如果常常要重做批处理的话，性能肯定会受影响的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557456919,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105231,"user_name":"夷，这也可以","can_delete":false,"product_type":"c1","uid":1487004,"ip_address":"","ucode":"09D87564352730","user_header":"https://static001.geekbang.org/account/avatar/00/16/b0/9c/0afabca6.jpg","comment_is_top":false,"comment_ctime":1560942537,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23035779017","product_id":100025301,"comment_content":"蔡老师好！看了之后有几点疑问：Lamabda基本上很好理解，Kappa还是不理解。目前的理解是这样的<br>1、Lamabda是离线每天计算T+1数据 联合 当天实时处理的数据；T+1的数据会更新覆盖掉昨天实时数据。<br>2、Kappa架构是从设定的时间数据开始，对每一条数据进行处理并和以前实时计算的结果数据聚会出结果，不断实时更新数据，直至实时处理的数据是目前最新的。<br>a、Kappa从开发来说只有实时逻辑，不涉及T+1的批量逻辑了。<br>b、数据的实时性来说，Kappa和Lambda都能拿到当前最新的全量&quot;结果&quot;数据。<br>c、如果有需求变更了，2种架构其实都要开发。<br>d、如果数据发声错误时，Lambda排查的时候相对Kappa会容易些，但是Kappa也可以通过业务处理逻辑对一个周期（比如天、周、月）的结果进行保存来使得查错和Lambada一样。对于纠错更正来说，如果业务逻辑比较简单只需在最后的迭代中修正即OK，那么2者也没有什么区别。但是如果业务逻辑比较复杂，不能简单的修改最后迭代结果，而需要从新迭代的情况，那么Lambda相对Kappa要容易，毕竟批处理的迭代次数相对较少。实际情况不能简单的修改最后迭代结果的情况应该毕竟少，而且一般发生错误都是就近的也就是当前时间不长，矫正比较容易。而且发现久远错误更正的情况也应该毕竟小。<br>这样来看Kappa相对Lambda还有些优势的，不过从稳定性来看Lambda要强点。<br>见识较少。不知道对不对","like_count":5},{"had_liked":false,"id":95023,"user_name":"又双叒叕是一年啊","can_delete":false,"product_type":"c1","uid":1000015,"ip_address":"","ucode":"E067320E537DEE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/4f/ff1ac464.jpg","comment_is_top":false,"comment_ctime":1557941467,"is_pvip":false,"replies":[{"id":"33976","content":"谢谢你的提问！按照你的说法你的应用需求是需要定时处理离线数据的，Kappa还是不太适合这种应用场景。这种应用场景可以用crontab加batch job完成。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557966933,"ip_address":"","comment_id":95023,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18737810651","product_id":100025301,"comment_content":"处理这种大数据量有窗口期的比如近30天的任务聚合计算可以用这种模式吗？是需要用kafka stream？每天都需要计算一次近30天的任务计算全量数据很大都是离线日志产生的数据","like_count":4,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450303,"discussion_content":"谢谢你的提问！按照你的说法你的应用需求是需要定时处理离线数据的，Kappa还是不太适合这种应用场景。这种应用场景可以用crontab加batch job完成。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557966933,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":98822,"user_name":"Zoe","can_delete":false,"product_type":"c1","uid":1528912,"ip_address":"","ucode":"A5D671919EE7B1","user_header":"https://static001.geekbang.org/account/avatar/00/17/54/50/8a76a8cc.jpg","comment_is_top":false,"comment_ctime":1559100297,"is_pvip":false,"replies":[{"id":"37133","content":"谢谢你的提问！你的理解没有错，一般来说是只处理delta部分就可以了。需要从头开始处理的场景一般都是在发现之前的逻辑有重大的错误或者说新加了一些字段需要backfill以前的数据。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1560295155,"ip_address":"","comment_id":98822,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14444002185","product_id":100025301,"comment_content":"老师，请问一下，纽约时报这个例子，是不是每次只处理delta部分而不是把log offset设成0更好一些？<br>我粗浅的理解是可以把batch layer想成一个cache，感觉这种基于time series的每次只需要处理new data的部分再把结果和之前的cache聚合在一起就可以了？<br>还是说业界普遍的做法都是从头开始处理，因为相较于找delta考虑overlap的困难就不那么意额外的处理时间和机器运行的成本？","like_count":3,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451852,"discussion_content":"谢谢你的提问！你的理解没有错，一般来说是只处理delta部分就可以了。需要从头开始处理的场景一般都是在发现之前的逻辑有重大的错误或者说新加了一些字段需要backfill以前的数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1560295155,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":203621,"user_name":"王鹏","can_delete":false,"product_type":"c1","uid":1934785,"ip_address":"","ucode":"B79003C125BE00","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Pu41KPIhAp72l0N7kOFL738owSAIT5EyH0oUUMacQRWzeFV77QDjSDNTSFNvjNZib1myibvxAfQAsAY5KzsIia73w/132","comment_is_top":false,"comment_ctime":1586242210,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10176176802","product_id":100025301,"comment_content":"个人感觉像最近出现的有些OLAP的数据，比如TIDB和Clickhouse，可能提供了另一种思路，这个存储引擎即支持批数据有支持流数据，并能提供高效的查询，我们最近的项目就是使用kafka+flink+Clickhouse这种，感觉一个OLAP工具是不是可以解决一些问题。","like_count":2},{"had_liked":false,"id":154858,"user_name":"YYY","can_delete":false,"product_type":"c1","uid":1734646,"ip_address":"","ucode":"0171B049750AA3","user_header":"","comment_is_top":false,"comment_ctime":1574581949,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10164516541","product_id":100025301,"comment_content":"https:&#47;&#47;www.cnblogs.com&#47;xiaodf&#47;p&#47;11642555.html 我在这个博文里面看到了老师讲课一样的内容 是19年10月份发布的 还有微信公众号","like_count":2},{"had_liked":false,"id":93751,"user_name":"珅剑","can_delete":false,"product_type":"c1","uid":1504220,"ip_address":"","ucode":"4290D5C140F80F","user_header":"https://static001.geekbang.org/account/avatar/00/16/f3/dc/80b0cd23.jpg","comment_is_top":false,"comment_ctime":1557579934,"is_pvip":false,"replies":[{"id":"33522","content":"谢谢你的提问！这个问题问得挺好的，我觉得还是要看应用场景吧。某些场景例如IoT这种，永远有大量数据流入的，而且实时处理IoT数据的场景可能并不需要处理到所有的历史记录，可能kappa架构就不一定合适用了。这种情况下即便逻辑需要修改，可能会采取部分历史数据dual-write，新数据直接switch到新逻辑上。说白了能够用上kappa架构的场景，肯定是历史数据可以赶得上新数据视图的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557649027,"ip_address":"","comment_id":93751,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10147514526","product_id":100025301,"comment_content":"新数据视图处理过得数据进度有可能赶上旧的数据视图吗？原先的数据视图应该也是实时不断更新的","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449761,"discussion_content":"谢谢你的提问！这个问题问得挺好的，我觉得还是要看应用场景吧。某些场景例如IoT这种，永远有大量数据流入的，而且实时处理IoT数据的场景可能并不需要处理到所有的历史记录，可能kappa架构就不一定合适用了。这种情况下即便逻辑需要修改，可能会采取部分历史数据dual-write，新数据直接switch到新逻辑上。说白了能够用上kappa架构的场景，肯定是历史数据可以赶得上新数据视图的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557649027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93714,"user_name":"jasine","can_delete":false,"product_type":"c1","uid":1063862,"ip_address":"","ucode":"1DB4E410C19BBB","user_header":"https://static001.geekbang.org/account/avatar/00/10/3b/b6/c3e2371a.jpg","comment_is_top":false,"comment_ctime":1557567050,"is_pvip":false,"replies":[{"id":"33523","content":"谢谢你的提问！这个问题可以这么看，因为重跑Kafka数据的是一个新的实例，它不影响现在正在运行的Kafka实例，所以说这段时间还是可以继续提供服务的。而不同的Kafka实例所产生的数据视图是不同版本的，只有当新的实例数据视图赶上旧的实例时，我们才会进行数据视图的切换。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557649316,"ip_address":"","comment_id":93714,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10147501642","product_id":100025301,"comment_content":"老师您好，麻烦请教下 如果实时与历史批量流程合在一起 那么重跑的时候kafka offset置成0 到latest这段时间是不是实时就无法提供服务了 怎么解决这个问题呢 感谢","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449740,"discussion_content":"谢谢你的提问！这个问题可以这么看，因为重跑Kafka数据的是一个新的实例，它不影响现在正在运行的Kafka实例，所以说这段时间还是可以继续提供服务的。而不同的Kafka实例所产生的数据视图是不同版本的，只有当新的实例数据视图赶上旧的实例时，我们才会进行数据视图的切换。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557649316,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":124483,"user_name":"tonyzhang","can_delete":false,"product_type":"c1","uid":1384914,"ip_address":"","ucode":"933CDB37106F17","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL58j1S6Ax1MMagApMzRnIkxy3rYxtmsmQibAst80NRzamx161ibwu6T1jxoicB8yg8TNLI4NZRBzGYg/132","comment_is_top":false,"comment_ctime":1565911180,"is_pvip":false,"replies":[{"id":"47607","content":"这就是kappa的理念","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1566882903,"ip_address":"","comment_id":124483,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5860878476","product_id":100025301,"comment_content":"老师，我想问一下，像lambda架构分成批处理和实时处理的两层，最终都是输出到对应的结果表供应用层分析；那为什么不能先进行实时计算，再把实时计算的结果写入到批处理的表中，这样批处理的数据就会不断累加，也能达到分析历史整体数据的目的，同时计算任务只需要实时计算的任务，批处理的数据是通过同步的形式进行，维护成本也能降低，不知道我的理解对不对？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":463199,"discussion_content":"这就是kappa的理念","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566882903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":108676,"user_name":"滩涂曳尾","can_delete":false,"product_type":"c1","uid":1187478,"ip_address":"","ucode":"40F650F2A419D4","user_header":"https://static001.geekbang.org/account/avatar/00/12/1e/96/c735ad6b.jpg","comment_is_top":false,"comment_ctime":1561847590,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5856814886","product_id":100025301,"comment_content":"一句话心得:<br>kappa架构主要就是利用kafka做流批一体化，得益于kafka可以方便地把[任意历史时刻, 当前时刻]的数据“框进去”——<br>1. kafka保留时间: 滑动窗口size<br>2. kafka LogOffset: 滑动窗口offset","like_count":1},{"had_liked":false,"id":107704,"user_name":"西北偏北","can_delete":false,"product_type":"c1","uid":1043160,"ip_address":"","ucode":"64BD69C84EE6A1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132","comment_is_top":false,"comment_ctime":1561598133,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5856565429","product_id":100025301,"comment_content":"基于kafka可以永久缓存数据的特性，将kafka当作一个存储引擎<br><br>基于kafka可以通过offset回溯回溯的特点来基于推送的获取和处理历史数据","like_count":1},{"had_liked":false,"id":93581,"user_name":"科学Jia","can_delete":false,"product_type":"c1","uid":1080409,"ip_address":"","ucode":"95430413F82A69","user_header":"https://static001.geekbang.org/account/avatar/00/10/7c/59/26b1e65a.jpg","comment_is_top":false,"comment_ctime":1557504971,"is_pvip":false,"replies":[{"id":"33526","content":"谢谢你的提问！男同学坦然回答：其实Kappa架构和Lambda架构很大不同的一点是Kappa架构没有了批处理层这一概念的。所以在Kappa架构中，并没有批处理结果和实时处理结果相结合这么一说。一般来说Kappa架构不太适用于需要经常处理历史数据的场景，但Kappa架构具备的能力是，如果你发现逻辑出错了，它有能力重新处理之前处理过的数据。而你所问到的重新处理历史数据得到的新视图，它是属于另外一个数据版本了，和老版本的数据视图是没有任何关系的，所以不会对实时数据产生影响。你可以把它们想象成两个不同的平行宇宙，虽然拥有了同样的数据，但处理起来互不干涉对方的世界。可能这个比喻不太恰当，如果还有不懂的话也欢迎你继续留言提问！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557650144,"ip_address":"","comment_id":93581,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5852472267","product_id":100025301,"comment_content":"老师，女同学举手提问：我不太理解例子中提到重新计算历史数据得到新的视图，这会对实时数据产生什么影响么？或者说实时数据需要依赖批处理的视图么？我还没法把批处理的结果和实时处理结果相结合，能否再具体一点呢？","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449686,"discussion_content":"谢谢你的提问！男同学坦然回答：其实Kappa架构和Lambda架构很大不同的一点是Kappa架构没有了批处理层这一概念的。所以在Kappa架构中，并没有批处理结果和实时处理结果相结合这么一说。一般来说Kappa架构不太适用于需要经常处理历史数据的场景，但Kappa架构具备的能力是，如果你发现逻辑出错了，它有能力重新处理之前处理过的数据。而你所问到的重新处理历史数据得到的新视图，它是属于另外一个数据版本了，和老版本的数据视图是没有任何关系的，所以不会对实时数据产生影响。你可以把它们想象成两个不同的平行宇宙，虽然拥有了同样的数据，但处理起来互不干涉对方的世界。可能这个比喻不太恰当，如果还有不懂的话也欢迎你继续留言提问！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557650144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":361023,"user_name":"紫日","can_delete":false,"product_type":"c1","uid":1207410,"ip_address":"北京","ucode":"73A8DEE323AC19","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqXSb2jAzlMM0JdTjWrNiaq2uR9eeloBYp906POddb9evmuj5f4CUoO6ge8TibibwtZicnl1sRHic9rW7g/132","comment_is_top":false,"comment_ctime":1667121653,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1667121653","product_id":100025301,"comment_content":"原来有批处理和流处理，可用通过lambda架构提高可用性，省去人工选择&#47;合并结果，这个也时候部分业务可以这样，相当一部分业务都是用不了lambda架构；技术提供可能路径，业务解决定最终路线。<br>kappa一套逻辑处理历史数据和实时数据，历史数据不用搬用吗？大数据的根本问题数据量过大，搬用成本太高或者做不到！","like_count":0},{"had_liked":false,"id":350587,"user_name":"🐺 🐶","can_delete":false,"product_type":"c1","uid":2704444,"ip_address":"","ucode":"84712B0436EC6F","user_header":"https://static001.geekbang.org/account/avatar/00/29/44/3c/8bb9e8b4.jpg","comment_is_top":false,"comment_ctime":1657018032,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1657018032","product_id":100025301,"comment_content":"想问一下老师，lambda批处理和流处理代码逻辑不一致是指什么？相同的kappa架构的逻辑一致是指？","like_count":0},{"had_liked":false,"id":337731,"user_name":"哇哈哈","can_delete":false,"product_type":"c1","uid":1175537,"ip_address":"","ucode":"47453D1C96A1DD","user_header":"https://static001.geekbang.org/account/avatar/00/11/ef/f1/8b06801a.jpg","comment_is_top":false,"comment_ctime":1646997312,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1646997312","product_id":100025301,"comment_content":"我觉得Kappa架构是伪命题，Lambda架构存在的原因是因为没法快速地完成全量的批处理，所以新增一个速度层弥补批量处理全量数据的延时问题，这是Lambda架构出现的问题背景，kappa直接把这个问题背景给否定了，觉得存在一个方法能快速完成全量批处理，然后可以怎么怎么处理，如果问题背景都不存在了，还有什么讨论方案的意义。","like_count":0},{"had_liked":false,"id":267216,"user_name":"Phantom01","can_delete":false,"product_type":"c1","uid":1140424,"ip_address":"","ucode":"9FA50D3B0A06DB","user_header":"https://static001.geekbang.org/account/avatar/00/11/66/c8/f598a816.jpg","comment_is_top":false,"comment_ctime":1607652667,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1607652667","product_id":100025301,"comment_content":"听上去有点像用 double buffer 来计算时间窗口的结果。在新数据 ready之前用历史数据，准备好以后再切过来。不过缺点可能是计算窗口数据量太大，新计算的可能追不上","like_count":0},{"had_liked":false,"id":254985,"user_name":"完美伪装","can_delete":false,"product_type":"c1","uid":2169761,"ip_address":"","ucode":"A30C8B24E54C0B","user_header":"https://static001.geekbang.org/account/avatar/00/21/1b/a1/61ae5854.jpg","comment_is_top":false,"comment_ctime":1603245004,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1603245004","product_id":100025301,"comment_content":"老师，您写的这篇文章kappa架构没有就计算引擎，kafka不能直接帮我们计算结果吧，是在服务层吗？","like_count":0},{"had_liked":false,"id":252268,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1602218927,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1602218927","product_id":100025301,"comment_content":"kappa可以处理的数据规模也不如lamabda吧？","like_count":0},{"had_liked":false,"id":137870,"user_name":"飞天大白菜","can_delete":false,"product_type":"c1","uid":1676519,"ip_address":"","ucode":"77344998A30D6E","user_header":"https://static001.geekbang.org/account/avatar/00/19/94/e7/bfef4648.jpg","comment_is_top":false,"comment_ctime":1569902894,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1569902894","product_id":100025301,"comment_content":"请问一下，用kappa architecture来做时间窗口统计是它的一个合理应用吗？<br><br>比如说我想每隔一段时间统计过去一小时的topK（暂时忽略处理时间过长问题），那么我是否可以每次将kafka consumer group的位移回调一个小时，然后直接运行一次topK算法？","like_count":0,"discussions":[{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":201758,"discussion_content":"按照flink的模型，大概是按照你设置的时间间隔，在operator中生成新的cache开始收集数据，直到收集了开始收集时间之后的一个小时数据后，做聚合再传给下一级operator（收集过程中可能也会传一些中间数据给下游，减少最后的计算压力）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583834829,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":103996,"user_name":"李恒达","can_delete":false,"product_type":"c1","uid":1120147,"ip_address":"","ucode":"E9F1AC9E74CA16","user_header":"https://static001.geekbang.org/account/avatar/00/11/17/93/981dc959.jpg","comment_is_top":false,"comment_ctime":1560585382,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1560585382","product_id":100025301,"comment_content":"我是新手，没有接触过这个东西，想问一下老师，这个架构的过程是不是可以这样理解：<br>旧的数据视图运行的同时，在另一个通道中，重新计算新的数据视图，训练好了以后就把旧的替换掉，使用新的数据视图，如此循环往复。<br>","like_count":0},{"had_liked":false,"id":102070,"user_name":"天下行走","can_delete":false,"product_type":"c1","uid":1462449,"ip_address":"","ucode":"73A4CD8BB91235","user_header":"https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLfrbMvhKQYhxP6ziaHaj4KUNRzst8u7BZsWUsazK8oTLXcNH6sDGITl6icy3IiaGFe9Iiae12LuTrF1g/132","comment_is_top":false,"comment_ctime":1560128456,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560128456","product_id":100025301,"comment_content":"老师是否可以讲下实时计算里得视图，因为两种架构都用到了，而且感觉是非常关键的一部分","like_count":0},{"had_liked":false,"id":94670,"user_name":"渡码","can_delete":false,"product_type":"c1","uid":1348536,"ip_address":"","ucode":"8FD8B863D1DA0C","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/b8/6510592e.jpg","comment_is_top":false,"comment_ctime":1557881040,"is_pvip":false,"replies":[{"id":"33875","content":"谢谢你的总结！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557884546,"ip_address":"","comment_id":94670,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557881040","product_id":100025301,"comment_content":"优点：数据统一，算法统一，接口统一<br>缺点：与现有大数据架构协同处理的兼容性不如lambda","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450165,"discussion_content":"谢谢你的总结！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557884546,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94530,"user_name":"ECHO","can_delete":false,"product_type":"c1","uid":1101496,"ip_address":"","ucode":"E78B3B557D469C","user_header":"https://static001.geekbang.org/account/avatar/00/10/ce/b8/92178ccd.jpg","comment_is_top":false,"comment_ctime":1557834494,"is_pvip":false,"replies":[{"id":"34207","content":"谢谢指出！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1558128853,"ip_address":"","comment_id":94530,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557834494","product_id":100025301,"comment_content":"回答貌似不对。这个offset是针对一个消费组的，不是全局。","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450098,"discussion_content":"谢谢指出！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1558128853,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93899,"user_name":"廖师虎","can_delete":false,"product_type":"c1","uid":1485562,"ip_address":"","ucode":"1297068AE141DA","user_header":"https://static001.geekbang.org/account/avatar/00/16/aa/fa/3ad0a689.jpg","comment_is_top":false,"comment_ctime":1557656380,"is_pvip":false,"replies":[{"id":"33553","content":"谢谢你的经验之谈！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557690287,"ip_address":"","comment_id":93899,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557656380","product_id":100025301,"comment_content":"个人觉得Kafka并不适合长期保存和处理海量数据，所以Kappa并不适合海量数据场景。使用Pulsar+Presto倒是可以满足","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449820,"discussion_content":"谢谢你的经验之谈！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557690287,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93569,"user_name":"挖矿的小戈","can_delete":false,"product_type":"c1","uid":1503917,"ip_address":"","ucode":"2078A85139BD5D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/9chAb6SjxFiapSeicsAsGqzziaNlhX9d5aEt8Z0gUNsZJ9dICaDHqAypGvjv4Bx3PryHnj7OFnOXFOp7Ik21CVXEA/132","comment_is_top":false,"comment_ctime":1557500702,"is_pvip":false,"replies":[{"id":"33466","content":"谢谢你的留言，总结得很到位！最后我想说的是，无论是Beam还是Flink，现在底层思想都是根据Dataflow Model来实现的，所以在Google Cloud Dataflow刚放出来的时候，Flink是作为Beam的唯一可替代runner。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557562058,"ip_address":"","comment_id":93569,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557500702","product_id":100025301,"comment_content":"1. kappa架构就是为了解决，lambda架构的复杂性以及维护成本高等痛点；<br>2. 所以才有了后面的Flink、Beam、Spark这些正在努力做流批统一的框架<br>3. 不过个人而言，更倾向于Flink这种流处理为核心，批处理只是流处理的一个子集；而Flink对于状态、容错、一致性等等都做得挺好的，很看好<br>4. 另外，对于Beam正在做的统一大数据编程规范的，也非常看好，哈哈，想想，掌握一套API就可以，底层想跑什么计算引擎，根据场景需求自由选择，美滋滋，不过Flink、Spark国内用的多，Beam貌似用的比较少，不知道国外是什么情况","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449683,"discussion_content":"谢谢你的留言，总结得很到位！最后我想说的是，无论是Beam还是Flink，现在底层思想都是根据Dataflow Model来实现的，所以在Google Cloud Dataflow刚放出来的时候，Flink是作为Beam的唯一可替代runner。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557562058,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93442,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1557471415,"is_pvip":false,"replies":[{"id":"33464","content":"谢谢你的提问！<br>1. 一般来说Apache项目都可以在官方网站上看到这些信息。以Apache Spark为例，官网上会说到“Apache Spark™ is a unified analytics engine for large-scale data processing”。那看到data processing这些关键字大概可以知道它是大数据处理的框架。<br>2. 在硅谷这边的话一般可以去不同大公司的engineering website看看最新的技术。当然你也可以看看极客邦科技底下的InfoQ架构师峰会。<br>3. 我不太了解运维岗位在国内具体的职责，不过在硅谷这边是有专门的SRE去维护一个产品的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557561660,"ip_address":"","comment_id":93442,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557471415","product_id":100025301,"comment_content":"老师，我问几个很low的问题，你别介意哈😄<br>1. apache怎么看哪些项目是属于大数据，否是项级项目，项目创建日期？<br>2. 如果想了解大数据，IT技术（如编程、架构、运维）的新方向有哪些网站可以看呀？<br>3. 听说国外是没运维的职位，只有SRE？运维很多开发都代替了运维，还有自动化，运维职位那不是慢慢消失了？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449635,"discussion_content":"谢谢你的提问！\n1. 一般来说Apache项目都可以在官方网站上看到这些信息。以Apache Spark为例，官网上会说到“Apache Spark™ is a unified analytics engine for large-scale data processing”。那看到data processing这些关键字大概可以知道它是大数据处理的框架。\n2. 在硅谷这边的话一般可以去不同大公司的engineering website看看最新的技术。当然你也可以看看极客邦科技底下的InfoQ架构师峰会。\n3. 我不太了解运维岗位在国内具体的职责，不过在硅谷这边是有专门的SRE去维护一个产品的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557561660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93349,"user_name":"奥卡姆剃刀","can_delete":false,"product_type":"c1","uid":1143983,"ip_address":"","ucode":"BF71372CA50EDC","user_header":"https://static001.geekbang.org/account/avatar/00/11/74/af/6f39dcf8.jpg","comment_is_top":false,"comment_ctime":1557455049,"is_pvip":false,"replies":[{"id":"33340","content":"谢谢你的留言！说得没错！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557465797,"ip_address":"","comment_id":93349,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557455049","product_id":100025301,"comment_content":"对比lambda和kappa 两种架构的优缺点，总结：没有完美的架构，最好的架构一定是根据业务需求演变过来的。","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449593,"discussion_content":"谢谢你的留言！说得没错！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557465797,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93335,"user_name":"邱从贤※klion26","can_delete":false,"product_type":"c1","uid":1027239,"ip_address":"","ucode":"36DF21F2B9E94C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/a7/4d41966a.jpg","comment_is_top":false,"comment_ctime":1557453449,"is_pvip":false,"replies":[{"id":"33279","content":"谢谢你的经验分享！是的，而且很多时候批处理层和速度层所产生的结果只是逻辑上一致，格式上可能还不一定能够放在一起核对。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557455418,"ip_address":"","comment_id":93335,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557453449","product_id":100025301,"comment_content":"kappa 最好的地方在于代码写一份就好了，维护成本大大降低，曾经核对过 mr 和 storm 产生的结果，真的很麻烦（还只对过一次），如果逻辑变更比较频繁的话，改一次需要对一次","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449589,"discussion_content":"谢谢你的经验分享！是的，而且很多时候批处理层和速度层所产生的结果只是逻辑上一致，格式上可能还不一定能够放在一起核对。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557455418,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93317,"user_name":"锦","can_delete":false,"product_type":"c1","uid":1468298,"ip_address":"","ucode":"CB0EB4B68C468B","user_header":"https://static001.geekbang.org/account/avatar/00/16/67/8a/babd74dc.jpg","comment_is_top":false,"comment_ctime":1557451957,"is_pvip":false,"replies":[{"id":"33289","content":"谢谢你的留言！无论是Kappa还是Lambda架构其实都支持发布订阅模式，不过Kappa和Lambda相比，只需要维护一套系统。后面你所说的缺点部分我是赞同的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557457041,"ip_address":"","comment_id":93317,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557451957","product_id":100025301,"comment_content":"Kappa是解决lombda架构维护两套处理逻辑的复杂性痛点而诞生的，它使用kafka配合处理流处理业务，具有简单，处理数据快，支持发布订阅模式的优点？。但同时也带来了数据错误和异常处理的成本。<br>","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449580,"discussion_content":"谢谢你的留言！无论是Kappa还是Lambda架构其实都支持发布订阅模式，不过Kappa和Lambda相比，只需要维护一套系统。后面你所说的缺点部分我是赞同的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557457041,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93311,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1557451704,"is_pvip":false,"replies":[{"id":"33286","content":"谢谢你的留言！我也赞同你的观点，其实如果数据处理框架能够统一批流处理API的话，我们也就不用这样维护一个Lambda架构了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557456247,"ip_address":"","comment_id":93311,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557451704","product_id":100025301,"comment_content":"kappa架构和lambda架构都有各自的优点和缺点和各自的使用场景。关键是要不要维护两套不同的技术栈，现在像spark和flink都在往批流统一的方向努力，目的就是统一技术栈，减少开发和运维的成本。","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449577,"discussion_content":"谢谢你的留言！我也赞同你的观点，其实如果数据处理框架能够统一批流处理API的话，我们也就不用这样维护一个Lambda架构了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557456247,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93302,"user_name":"miwucc","can_delete":false,"product_type":"c1","uid":1326429,"ip_address":"","ucode":"7935BD907119AE","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/5d/ac666969.jpg","comment_is_top":false,"comment_ctime":1557451100,"is_pvip":false,"replies":[{"id":"33323","content":"谢谢你的提问！<br>1. 如果我没有理解错你的问题的话，没错，在Kappa架构里，所有的数据清洗或者标准化都需要在Speed Layer完成。至于Lambda架构的话，批处理层可能是直接读取的清洗好结构化好的数据，也可能是原始数据直接传进去进行数据清洗，这要看具体怎么设计了。<br>2. 如果要写两套逻辑的话基本上代码也就不一样了，这个时候从high level看其实也就变成了Lambda了。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557459792,"ip_address":"","comment_id":93302,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557451100","product_id":100025301,"comment_content":"文中说的kappa的两个缺点，不是很理解。<br><br>第一点，说实时数据需要处理数据异常，按理说说lambda结构的速度层也会面对这个问题啊。还是是说kappa的历史数据计算层在计算的时候同时需要清洗数据，而lambda的批处理层是直接读取的清洗好结构化好的数据？<br><br>弟二点：说如果历史计算和实时计算需要不同逻辑时候不适用。就在该框架下写两套逻辑不就行了么？为啥非要用lambda架构再写两套逻辑，还面对技术栈切换<br><br>","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449572,"discussion_content":"谢谢你的提问！\n1. 如果我没有理解错你的问题的话，没错，在Kappa架构里，所有的数据清洗或者标准化都需要在Speed Layer完成。至于Lambda架构的话，批处理层可能是直接读取的清洗好结构化好的数据，也可能是原始数据直接传进去进行数据清洗，这要看具体怎么设计了。\n2. 如果要写两套逻辑的话基本上代码也就不一样了，这个时候从high level看其实也就变成了Lambda了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557459792,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93301,"user_name":"君哥聊技术","can_delete":false,"product_type":"c1","uid":1325816,"ip_address":"","ucode":"2C9A22BCE4C79E","user_header":"https://static001.geekbang.org/account/avatar/00/14/3a/f8/c1a939e7.jpg","comment_is_top":false,"comment_ctime":1557451086,"is_pvip":false,"replies":[{"id":"33303","content":"谢谢你的留言与提问！<br><br>如果历史数据非常大的话，kafka offset置为0来计算历史数据肯定会挺耗时的。至于为什么要重新计算数据，归根结底都是因为代码逻辑有了改变，那既然代码逻辑改变了以前的结果肯定也不能用了，所以用批结果加流结果作为下次计算的批结果是不太可能的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557459041,"ip_address":"","comment_id":93301,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557451086","product_id":100025301,"comment_content":"1.如果批处理和流处理的逻辑一致，那可以选择用kappa架构，否则lambda架构更好<br><br>如果选择kappa架构，kafka offset置为0来计算批数据，会不会非常耗时呢？可以用批结果加流结果作为下次计算的批结果吗","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449571,"discussion_content":"谢谢你的留言与提问！\n\n如果历史数据非常大的话，kafka offset置为0来计算历史数据肯定会挺耗时的。至于为什么要重新计算数据，归根结底都是因为代码逻辑有了改变，那既然代码逻辑改变了以前的结果肯定也不能用了，所以用批结果加流结果作为下次计算的批结果是不太可能的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557459041,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93290,"user_name":"小度","can_delete":false,"product_type":"c1","uid":1206404,"ip_address":"","ucode":"A986223E0CB516","user_header":"https://static001.geekbang.org/account/avatar/00/12/68/84/9fa4dc6e.jpg","comment_is_top":false,"comment_ctime":1557450341,"is_pvip":false,"replies":[{"id":"33274","content":"谢谢你的提问！现在还没有什么架构是完美的，具体使用哪一个架构肯定还是要根据自身需求和应用场景来做取舍。Kappa架构的提出就是想改善Lambda架构里需要维护两套系统而提出的，所以并不能融合使用。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557453470,"ip_address":"","comment_id":93290,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557450341","product_id":100025301,"comment_content":"有没有完美的架构？Kappa 架构Lambda 架构都有优缺点能否融合使用？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449566,"discussion_content":"谢谢你的提问！现在还没有什么架构是完美的，具体使用哪一个架构肯定还是要根据自身需求和应用场景来做取舍。Kappa架构的提出就是想改善Lambda架构里需要维护两套系统而提出的，所以并不能融合使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557453470,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93262,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1557448688,"is_pvip":false,"replies":[{"id":"33262","content":"谢谢你的总结！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557450392,"ip_address":"","comment_id":93262,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1557448688","product_id":100025301,"comment_content":"kappa架构优点：<br>1.流处理和批处理同一套算法逻辑，更简单。<br>2.数据格式统一，给后端处理带来方便。<br>3.批量处理的速度更快。<br>","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449551,"discussion_content":"谢谢你的总结！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557450392,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":201775,"discussion_content":"关于第3点，为什么kappa的批量处理的速度更快呢？使用流处理框架来做批处理框架擅长的事，再快也不可能比批处理框架快吧？有没有什么具体的原因这么总结呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583836629,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93259,"user_name":"jon","can_delete":false,"product_type":"c1","uid":1253287,"ip_address":"","ucode":"5768A34E292CAA","user_header":"https://static001.geekbang.org/account/avatar/00/13/1f/a7/d379ca4f.jpg","comment_is_top":false,"comment_ctime":1557448440,"is_pvip":false,"replies":[{"id":"33263","content":"谢谢你的留言！很不错的总结，把优缺点都罗列出来了！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557451240,"ip_address":"","comment_id":93259,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557448440","product_id":100025301,"comment_content":"kappa优点：不用分别编写批、流两套处理逻辑，采用发布-订阅模式，接受多个订阅者，可重复消费历史数据。<br>kappa缺点：在速度层进行大规模实时流处理容易处理数据更新错误，可靠性不如批处理层。<br>","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449549,"discussion_content":"谢谢你的留言！很不错的总结，把优缺点都罗列出来了！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557451240,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93230,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1557444515,"is_pvip":false,"replies":[{"id":"33273","content":"谢谢你的提问！Kafka的优点之一就是拥有高吞吐量的，而Kafka系统的设计里肯定是有集群存在的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557453341,"ip_address":"","comment_id":93230,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557444515","product_id":100025301,"comment_content":"如果kafka处理全部数据，数据量非常大会不用卡死？要使用群集吗？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449536,"discussion_content":"谢谢你的提问！Kafka的优点之一就是拥有高吞吐量的，而Kafka系统的设计里肯定是有集群存在的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557453341,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]}]}