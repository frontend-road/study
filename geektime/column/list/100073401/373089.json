{"id":373089,"title":"27 | 大表Join小表：广播变量容不下小表怎么办？","content":"<p>你好，我是吴磊。</p><p>在数据分析领域，大表Join小表的场景非常普遍。不过，大小是个相对的概念，通常来说，大表与小表尺寸相差3倍以上，我们就将其归类为“大表Join小表”的计算场景。因此，大表Join小表，仅仅意味着参与关联的两张表尺寸悬殊。</p><p>对于大表Join小表这种场景，我们应该优先考虑BHJ，它是Spark支持的5种Join策略中执行效率最高的。<strong>BHJ处理大表Join小表时的前提条件是，广播变量能够容纳小表的全量数据。</strong>但是，如果小表的数据量超过广播阈值，我们又该怎么办呢？</p><p>今天这一讲，我们就结合3个真实的业务案例，来聊一聊这种情况的解决办法。虽然这3个案例不可能覆盖“大表Join小表”场景中的所有情况，但是，分析并汇总案例的应对策略和解决办法，有利于我们在调优的过程中开阔思路、发散思维，从而避免陷入“面对问题无所适从”的窘境。</p><h2>案例1：Join Key远大于Payload</h2><p>在第一个案例中，大表100GB、小表10GB，它们全都远超广播变量阈值（默认10MB）。因为小表的尺寸已经超过8GB，在大于8GB的数据集上创建广播变量，Spark会直接抛出异常，中断任务执行，所以Spark是没有办法应用BHJ机制的。那我们该怎么办呢？先别急，我们来看看这个案例的业务需求。</p><!-- [[[read_end]]] --><p>这个案例来源于计算广告业务中的流量预测，流量指的是系统中一段时间内不同类型用户的访问量。这里有三个关键词，第一个是“系统”，第二个是“一段时间”，第三个是“用户类型”。时间粒度好理解，就是以小时为单位去统计流量。用户类型指的是采用不同的维度来刻画用户，比如性别、年龄、教育程度、职业、地理位置。系统指的是流量来源，比如平台、站点、频道、媒体域名。</p><p>在系统和用户的维度组合之下，流量被细分为数以百万计的不同“种类”。比如，来自XX平台XX站点的在校大学生的访问量，或是来自XX媒体XX频道25-45岁女性的访问量等等。</p><p>我们知道，流量预测本身是个时序问题，它和股价预测类似，都是基于历史、去预测未来。在我们的案例中，为了预测上百万种不同的流量，咱们得先为每种流量生成时序序列，然后再把这些时序序列喂给机器学习算法进行模型训练。</p><p>统计流量的数据源是线上的访问日志，它记录了哪类用户在什么时间访问了哪些站点。要知道，我们要构建的，是以小时为单位的时序序列，但由于流量的切割粒度非常细致，因此有些种类的流量不是每个小时都有访问量的，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/7e/9f/7e8fc62ca38cb1a0a557b6b7eb1af99f.jpg?wh=2934*924\" alt=\"\" title=\"某种流量在过去24小时的记录情况\"></p><p>我们可以看到，在过去的24小时中，某种流量仅在20-24点这5个时段有数据记录，其他时段无记录，也就是流量为零。在这种情况下，我们就需要用“零”去补齐缺失时段的序列值。那么我们该怎么补呢？</p><p><strong>因为业务需求是填补缺失值，所以在实现层面，我们不妨先构建出完整的全零序列，然后以系统、用户和时间这些维度组合为粒度，用统计流量去替换全零序列中相应位置的“零流量”。</strong>这个思路描述起来比较复杂，用图来理解会更直观、更轻松一些。</p><p><img src=\"https://static001.geekbang.org/resource/image/90/fe/90d03196d4c71c133344f913e4f69dfe.jpg?wh=3999*1554\" alt=\"\"></p><p>首先，我们生成一张全零流量表，如图中左侧的“负样本表”所示。这张表的主键是划分流量种类的各种维度，如性别、年龄、平台、站点、小时时段等等。表的Payload只有一列，也即访问量，在生成“负样本表”的时候，这一列全部置零。</p><p>然后，我们以同样的维度组合统计日志中的访问量，就可以得到图中右侧的“正样本表”。不难发现，两张表的Schema完全一致，要想获得完整的时序序列，我们只需要把外表以“左连接（Left Outer Join）”的形式和内表做关联就好了。具体的查询语句如下：</p><pre><code>//左连接查询语句\nselect t1.gender, t1.age, t1.city, t1.platform, t1.site, t1.hour, coalesce(t2.access, t1.access) as access\nfrom t1 left join t2 on\nt1.gender = t2.gender and\nt1.age = t2.age and\nt1.city = t2.city and\nt1.platform = t2.platform and\nt1.site = t2.site and\nt1.hour = t2.hour\n</code></pre><p>使用左连接的方式，我们刚好可以用内表中的访问量替换外表中的零流量，两表关联的结果正是我们想要的时序序列。“正样本表”来自访问日志，只包含那些存在流量的时段，而“负样本表”是生成表，它包含了所有的时段。因此，在数据体量上，负样本表远大于正样本表，这是一个典型的“大表Join小表”场景。尽管小表（10GB）与大表（100GB）相比，在尺寸上相差一个数量级，但两者的体量都不满足BHJ的先决条件。因此，Spark只好退而求其次，选择SMJ（Shuffle Sort Merge Join）的实现方式。</p><p>我们知道，SMJ机制会引入Shuffle，将上百GB的数据在全网分发可不是一个明智的选择。那么，根据“能省则省”的开发原则，我们有没有可能“省去”这里的Shuffle呢？要想省去Shuffle，我们只有一个办法，就是把SMJ转化成BHJ。你可能会说：“都说了好几遍了，小表的尺寸10GB远超广播阈值，我们还能怎么转化呢？”</p><p>办法总比困难多，我们先来反思，关联这两张表的目的是什么？目的是以维度组合（Join Keys）为基准，用内表的访问量替换掉外表的零值。那么，这两张表有哪些特点呢？<strong>首先，两张表的Schema完全一致。其次，无论是在数量、还是尺寸上，两张表的Join Keys都远大于Payload。</strong>那么问题来了，要达到我们的目的，一定要用那么多、那么长的Join Keys做关联吗？</p><p>答案是否定的。在上一讲，我们介绍过Hash Join的实现原理，在Build阶段，Hash Join使用哈希算法生成哈希表。在Probe阶段，哈希表一方面可以提供O(1)的查找效率，另一方面，在查找过程中，Hash Keys之间的对比远比Join Keys之间的对比要高效得多。受此启发，我们为什么不能计算Join Keys的哈希值，然后把生成的哈希值当作是新的Join Key呢？</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/69/1c93475b8d2d5d18d6b49fb7a258db69.jpg?wh=4719*1869\" alt=\"\"></p><p>我们完全可以基于现有的Join Keys去生成一个全新的数据列，它可以叫“Hash Key”。生成的方法分两步：</p><ul>\n<li>把所有Join Keys拼接在一起，把性别、年龄、一直到小时拼接成一个字符串，如图中步骤1、3所示</li>\n<li>使用哈希算法（如MD5或SHA256）对拼接后的字符串做哈希运算，得到哈希值即为“Hash Key”，如上图步骤2、4所示</li>\n</ul><p>在两张表上，我们都进行这样的操作。如此一来，在做左连接的时候，为了把主键一致的记录关联在一起，我们不必再使用数量众多、冗长的原始Join Keys，用单一的生成列Hash Key就可以了。相应地，SQL查询语句也变成了如下的样子。</p><pre><code>//调整后的左连接查询语句\nselect t1.gender, t1.age, t1.city, t1.platform, t1.site, t1.hour, coalesce(t2.access, t1.access) as access\nfrom t1 left join t2 on\nt1.hash_key = t2. hash_key\n</code></pre><p>添加了这一列之后，我们就可以把内表，也就是“正样本表”中所有的Join Keys清除掉，大幅缩减内表的存储空间，上图中的步骤5演示了这个过程。当内表缩减到足以放进广播变量的时候，我们就可以把SMJ转化为BHJ，从而把SMJ中的Shuffle环节彻底省掉。</p><p>这样一来，清除掉Join Keys的内表的存储大小就变成了1.5GB。对于这样的存储量级，我们完全可以使用配置项或是强制广播的方式，来完成Shuffle Join到Broadcast Join的转化，具体的转化方法你可以参考广播变量那一讲（<a href=\"https://time.geekbang.org/column/article/360837\">第13讲</a>）。</p><p>案例1说到这里，其实已经基本解决了，不过这里还有一个小细节需要我们特别注意。<strong>案例1优化的关键在于，先用Hash Key取代Join Keys，再清除内表冗余数据。Hash Key实际上是Join Keys拼接之后的哈希值。既然存在哈希运算，我们就必须要考虑哈希冲突的问题。</strong></p><p>哈希冲突我们都很熟悉，它指的就是不同的数据源经过哈希运算之后，得到的哈希值相同。在案例1当中，如果我们为了优化引入的哈希计算出现了哈希冲突，就会破坏原有的关联关系。比如，本来两个不相等的Join Keys，因为哈希值恰巧相同而被关联到了一起。显然，这不是我们想要的结果。</p><p>消除哈希冲突隐患的方法其实很多，比如“二次哈希”，也就是我们用两种哈希算法来生成Hash Key数据列。两条不同的数据记录在两种不同哈希算法运算下的结果完全相同，这种概率几乎为零。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/bf/bb79544467e98f9d2b6f13a437bb2dbf.jpg?wh=1500*1584\" alt=\"\"></p><h2>案例2：过滤条件的Selectivity较高</h2><p>除了Join Keys远大于Payload的情况会导致我们无法选择BHJ，还有一种情况是过滤条件的Selectivity较高。这个案例来源于电子商务场景，在星型（Start Schema）数仓中，我们有两张表，一张是订单表orders，另一张是用户表users。订单表是事实表（Fact），而用户表是维度表（Dimension）。</p><p>这个案例的业务需求很简单，是统计所有头部用户贡献的营业额，并按照营业额倒序排序。订单表和用户表的Schema如下表所示。</p><pre><code>// 订单表orders关键字段\nuserId, Int\nitemId, Int\nprice, Float\nquantity, Int\n \n// 用户表users关键字段\nid, Int\nname, String\ntype, String //枚举值，分为头部用户和长尾用户\n\n</code></pre><p>给定上述数据表，我们只需把两张表做内关联，然后分组、聚合、排序，就可以实现业务逻辑，具体的查询语句如下所示。</p><pre><code>//查询语句\nselect (orders.price * order.quantity) as revenue, users.name\nfrom orders inner join users on orders.userId = users.id\nwhere users.type = ‘Head User’\ngroup by users.name\norder by revenue desc\n</code></pre><p>在这个案例中，事实表的存储容量在TB量级，维度表是20GB左右，也都超过了广播阈值。其实，这样的关联场景在电子商务、计算广告以及推荐搜索领域都很常见。</p><p><strong>对于两张表都远超广播阈值的关联场景来说，如果我们不做任何调优的，Spark就会选择SMJ策略计算。</strong>在10台C5.4xlarge AWS EC2的分布式环境中，SMJ要花费将近5个小时才完成两张表的关联计算。这样的执行效率，我们肯定无法接受，但我们又能做哪些优化呢？你不妨先花上两分钟去想一想，然后再来一起跟我去分析。</p><p>仔细观察上面的查询语句，我们发现这是一个典型的星型查询，也就是事实表与维度表关联，且维表带过滤条件。维表上的过滤条件是users.type = ‘Head User’，即只选取头部用户。而通常来说，相比普通用户，头部用户的占比很低。换句话说，这个过滤条件的选择性（Selectivity）很高，它可以帮助你过滤掉大部分的维表数据。在我们的案例中，由于头部用户占比不超过千分之一，因此过滤后的维表尺寸很小，放进广播变量绰绰有余。</p><p>这个时候我们就要用到AQE了，我们知道AQE允许Spark SQL在运行时动态地调整Join策略。我们刚好可以利用这个特性，把最初制定的SMJ策略转化为BHJ策略（千万别忘了，AQE默认是关闭的，要想利用它提供的特性，我们得先把spark.sql.adaptive.enabled配置项打开）。</p><p>不过，即便过滤条件的选择性很高，在千分之一左右，过滤之后的维表还是有20MB大小，这个尺寸还是超过了默认值广播阈值10MB。因此，我们还需要把广播阈值spark.sql.autoBroadcastJoinThreshold调高一些，比如1GB，AQE才会把SMJ降级为BHJ。做了这些调优之后，在同样的集群规模下，作业的端到端执行时间从之前的5个小时缩减为30分钟。</p><p>让作业的执行性能提升了一个数量级之后，我们的调优就结束了吗？在调优的本质那一讲，我们一再强调，随着木桶短板的此消彼长，调优是一个不断持续的过程。在这个过程中，我们需要因循瓶颈的变化，动态地切换调优方法，去追求一种所有木板齐平、没有瓶颈的状态。</p><p>那么，当我们用动态Join策略，把SMJ策略中Shuffle引入的海量数据分发这块短板补齐之后，还有没有“新晋”的短板需要修理呢？</p><p><strong>对于案例中的这种星型关联，我们还可以利用DPP机制来减少事实表的扫描量，进一步减少I/O开销、提升性能。</strong>和AQE不同，DPP并不需要开发者特别设置些什么，只要满足条件，DPP机制会自动触发。</p><p>但是想要使用DPP做优化，还有3个先决条件需要满足：</p><ul>\n<li>DPP仅支持等值Joins，不支持大于或者小于这种不等值关联关系</li>\n<li>维表过滤之后的数据集，必须要小于广播阈值，因此开发者要注意调整配置项spark.sql.autoBroadcastJoinThreshold</li>\n<li>事实表必须是分区表，且分区字段（可以是多个）必须包含Join Key</li>\n</ul><p>我们可以直接判断出查询满足前两个条件，满足第一个条件很好理解。满足第二个条件是因为，经过第一步AQE的优化之后，广播阈值足够大，足以容纳过滤之后的维表。那么，要想利用DPP机制，我们必须要让orders成为分区表，也就是做两件事情：</p><ul>\n<li>创建一张新的订单表orders_new，并指定userId为分区键</li>\n<li>把原订单表orders的全部数据，灌进这张新的订单表orders_new</li>\n</ul><pre><code>//查询语句\nselect (orders_new.price * orders_new.quantity) as revenue, users.name\nfrom orders_new inner join users on orders_new.userId = users.id\nwhere users.type = ‘Head User’\ngroup by users.name\norder by revenue desc\n</code></pre><p>用orders_new表替换orders表之后，在同样的分布式环境下，查询时间就从30分钟进一步缩短到了15分钟。</p><p>你可能会说：“为了利用DPP，重新建表、灌表，这样也需要花费不少时间啊！这不是相当于把运行时间从查询转嫁到建表、灌数了吗？”你说的没错，确实是这么回事。如果为了查询效果，临时再去修改表结构、迁移数据确实划不来，属于“临时抱佛脚”。因此，为了最大限度地利用DPP，在做数仓规划的时候，开发者就应该结合常用查询与典型场景，提前做好表结构的设计，这至少包括Schema、分区键、存储格式等等。</p><h2>案例3：小表数据分布均匀</h2><p>在上面的两个案例中，我们都是遵循“能省则省”的开发原则，想方设法地把Shuffle Joins切换为Broadcast Joins，从而消除Shuffle。但是，总有那么一些“顽固”的场景，无论我们怎么优化，也没办法做到这一点。那么对于这些“顽固分子”，我们该怎么办呢？</p><p>我们知道，如果关联场景不满足BHJ条件，Spark SQL会优先选择SMJ策略完成关联计算。但是在上一讲我们说到，<strong>当参与Join的两张表尺寸相差悬殊且小表数据分布均匀的时候，SHJ往往比SMJ的执行效率更高</strong>。原因很简单，小表构建哈希表的开销要小于两张表排序的开销。</p><p>我们还是以上一个案例的查询为例，不过呢，这次我们把维表的过滤条件去掉，去统计所有用户贡献的营业额。在10台C5.4xlarge AWS EC2的分布式环境中，去掉过滤条件的SMJ花费了将近7个小时才完成两张表的关联计算。</p><pre><code>//查询语句\nselect (orders.price * order.quantity) as revenue, users.name\nfrom orders inner join users on orders.userId = users.id\ngroup by users.name\norder by revenue desc\n</code></pre><p>由于维表的查询条件不复存在，因此案例2中的两个优化方法，也就是AQE Join策略调整和DPP机制，也都失去了生效的前提。<strong>这种情况下，我们不妨使用Join Hints来强制Spark SQL去选择SHJ策略进行关联计算</strong>，调整后的查询语句如下表所示。</p><pre><code>//添加Join hints之后的查询语句\nselect /*+ shuffle_hash(orders) */ (orders.price * order.quantity) as revenue, users.name\nfrom orders inner join users on orders.userId = users.id\ngroup by users.name\norder by revenue desc\n</code></pre><p>将Join策略调整为SHJ之后，在同样的集群规模下，作业的端到端执行时间从之前的7个小时缩减到5个小时，相比调优前，我们获得了将近30%的性能提升。</p><p>需要注意的是，<strong>SHJ要想成功地完成计算、不抛OOM异常，需要保证小表的每个数据分片都能放进内存</strong>。这也是为什么，我们要求小表的数据分布必须是均匀的。如果小表存在数据倾斜的问题，那么倾斜分区的OOM将会是一个大概率事件，SHJ的计算也会因此而中断。</p><h2>小结</h2><p>今天这一讲，我们从3个案例出发，探讨并解锁了不同场景下“大表Join小表”的优化思路和应对方法。</p><p>首先，我们定义了什么是“大表Join小表”。一般来说，参与Join的两张表在尺寸上相差3倍以上，就可以看作是“大表Join小表”的计算场景。</p><p>其次，我们讲了3个大表Join小表场景下，无法选择BHJ的案例。</p><p>第一个案例是Join Keys远大于Payload的数据关联，我们可以使用映射方法（如哈希运算），用较短的字符串来替换超长的Join Keys，从而大幅缩减小表的存储空间。如果缩减后的小表，足以放进广播变量，我们就可以将SMJ转换为BHJ，=来消除繁重的Shuffle计算。需要注意的是，映射方法要能够有效地避免“映射冲突”的问题，避免出现不同的Join Keys被映射成同一个数值。</p><p>第二个案例是，如果小表携带过滤条件，且过滤条件的选择性很高，我们可以通过开启AQE的Join策略调整特性，在运行时把SMJ转换为BHJ，从而大幅提升执行性能。这里有两点需要我们特别注意：一是，为了能够成功完成转换，我们需要确保过滤之后的维表尺寸小于广播阈值；二是，如果大表本身是按照Join Keys进行分区的话，那么我们还可以充分利用DPP机制，来进一步缩减大表扫描的I/O开销，从而提升性能。</p><p>第三个案例是，如果小表不带过滤条件，且尺寸远超广播阈值。如果小表本身的数据分布比较均匀，我们可以考虑使用Join hints强行要求Spark SQL在运行时选择SHJ关联策略。一般来说，在“大表Join小表”的场景中，相比SMJ，SHJ的执行效率会更好一些。背后的原因在于，小表构建哈希表的开销，要小于两张表排序的开销。</p><h2>每日一练</h2><ol>\n<li>对于案例1，我们的核心思路是用哈希值来替代超长的Join Keys，除了用哈希值以外，你觉得还有其他的思路或是办法，去用较短的字符串来取代超长的Join Keys吗？</li>\n<li>对于案例2，利用AQE Join策略调整和DDP机制的关键，是确保过滤后的维表小于广播阈值。你能说说，都有哪些方法可以用来计算过滤后的维表大小吗？</li>\n<li>对于案例3，假设20GB的小表存在数据倾斜，强行把SMJ转化为SHJ会抛OOM异常。这个时候，你认为还有可能继续优化吗？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","comments":[{"had_liked":false,"id":292863,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1621043344,"is_pvip":false,"replies":[{"id":"106271","content":"第一题超赞~ 这个方法非常好，实际上我觉得比较hash的方法还棒！因为它天然能避开冲突的问题，而且存储效率很高，机智如你~ 真的特别好的方法，其实本质上这可以看成是一种编码方式了，老弟你也搞机器学习吗？编码的思路在Machine Learning用的非常普遍~ 当然这里有一个字典维护的成本，不过如果Join Keys的cardinality不是很夸张的话，其实还好~<br><br>2、3的思路都没问题~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621408869,"ip_address":"","comment_id":292863,"utype":1}],"discussion_count":4,"race_medal":0,"score":"18800912528","product_id":100073401,"comment_content":"问题回答<br>1.给join keys每个字段维护一个字典，每个字段值在字典内对应一个唯一的整数。拿到每个字段指定的种整数，然后组装起来，作为新的join key。<br> 2.把维度表查询sql单独拿出来，缓存其df，查看其屋里计划，可以知道它结果集大小。<br>3.参考AQE的自动倾斜处理特性，把数据倾斜的分区拆分开，然后再进行SHJ","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519989,"discussion_content":"第一题超赞~ 这个方法非常好，实际上我觉得比较hash的方法还棒！因为它天然能避开冲突的问题，而且存储效率很高，机智如你~ 真的特别好的方法，其实本质上这可以看成是一种编码方式了，老弟你也搞机器学习吗？编码的思路在Machine Learning用的非常普遍~ 当然这里有一个字典维护的成本，不过如果Join Keys的cardinality不是很夸张的话，其实还好~\n\n2、3的思路都没问题~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621408869,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1187106,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","nickname":"Fendora范东_","note":"","ucode":"63EE9DBEE08D70","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375068,"discussion_content":"磊哥，我是搞olap数据库内核方向的😀","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1621473628,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1187106,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","nickname":"Fendora范东_","note":"","ucode":"63EE9DBEE08D70","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375927,"discussion_content":"666~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621870918,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":375068,"ip_address":""},"score":375927,"extra":""}]},{"author":{"id":1581708,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eozzWrelqaW7GWmbGGMejOV3DIDsMXTpib5ib9W5yHM0s1oDpvOb70G2SdjRs7evxv8T3VVjZTeEeyQ/132","nickname":"Geek_c12c37","note":"","ucode":"7EFDD2E596F18E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382406,"discussion_content":"有点类似bitmap","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625558877,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":298764,"user_name":"超级丹","can_delete":false,"product_type":"c1","uid":1300070,"ip_address":"","ucode":"6735EF97F10649","user_header":"https://static001.geekbang.org/account/avatar/00/13/d6/66/62206d01.jpg","comment_is_top":false,"comment_ctime":1624318779,"is_pvip":false,"replies":[{"id":"108449","content":"确实，本质上就是Join Key和分区键的矛盾。<br><br>分区键要求Cardinality不能太高，否则就会出现文件分布过细、过散，分布式文件系统负担过重。但是另一方面，Join Key往往是userId、orderId、txId这类Cardinality很大的字段，这个是由业务逻辑决定的，很难改变。<br><br>因此，分区键和Join Key就会存在博弈，换句话说，是牺牲文件系统效率来强行触发DPP，还是保证文件系统效率而放弃DPP，这里面就会有一个取舍的问题。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624433671,"ip_address":"","comment_id":298764,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10214253371","product_id":100073401,"comment_content":"老师，我有个疑问，dpp，不会导致inode过高吗？ 特别是在用户上做分区这种。。。","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522235,"discussion_content":"确实，本质上就是Join Key和分区键的矛盾。\n\n分区键要求Cardinality不能太高，否则就会出现文件分布过细、过散，分布式文件系统负担过重。但是另一方面，Join Key往往是userId、orderId、txId这类Cardinality很大的字段，这个是由业务逻辑决定的，很难改变。\n\n因此，分区键和Join Key就会存在博弈，换句话说，是牺牲文件系统效率来强行触发DPP，还是保证文件系统效率而放弃DPP，这里面就会有一个取舍的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624433671,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382501,"discussion_content":"我觉得这可能不是博弈的问题，而是数仓的设计者在设计之初很大程度上不会按照这些id字段建分区，即使他知道有些id后面会被频繁使用，但是保不准后面有什么其他需求，单纯按照id做分区太冒险了，在实际场景几乎没有","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1625620110,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":302843,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1626404994,"is_pvip":false,"replies":[{"id":"109815","content":"前两题满分💯~ <br><br>第三题的话，AQE是需要Shuffle才能生效的，因此，要想利用AQE消除数据倾斜，得先对小表做repartition才行。<br><br>另外一种可行的办法，就是可以参考后面29讲负隅顽抗里面的“两阶段加盐Shuffle”，通过加盐把数据打散，让原本不均衡的数据，变得均衡，然后再用SHJ去做关联。当然，这种方法的开发成本会高很多，不过算是一种比较通用的方法。<br><br>这些方法，这些“招儿”，其实不好说孰优孰劣，还要结合具体数据量、场景，结合实际效果去看。不过，艺多不压身，调优比较重要的一点，就是要思考广、“招儿”多，然后通过对比不同优化方法的优劣势和效果差异，来更好地理解Spark，这样后面再有类似情况，就能轻车熟路啦~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1626775258,"ip_address":"","comment_id":302843,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5921372290","product_id":100073401,"comment_content":"问题回答。<br><br>第一题：<br>对每个字段的值生成一个独热编码，以字典来维护，然后再组合各个字段的独热编码，形成一个新的join key。当然这里不一定用独热编码，也能用二进制编码，如果想更加节省空间更加折腾，可以使用 哈夫曼编码，哈夫曼编码 从本质上讲，是将最宝贵的资源（最短的编码）给出现概率最大的信息。<br><br>第二题：<br>先 Cache，再查看执行计划，有示例代码，在第17篇内存视角（三）：OMM都是谁的锅？怎么破？<br><br>第三题：<br>AQE的自动倾斜处理，在 Reduce 阶段，当 Reduce Task 所需处理的分区尺寸大于一定阈值时，利用 OptimizeSkewedJoin 策略，AQE 会把大分区拆成多个小分区。（阀值可自行设置的）","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523453,"discussion_content":"前两题满分💯~ \n\n第三题的话，AQE是需要Shuffle才能生效的，因此，要想利用AQE消除数据倾斜，得先对小表做repartition才行。\n\n另外一种可行的办法，就是可以参考后面29讲负隅顽抗里面的“两阶段加盐Shuffle”，通过加盐把数据打散，让原本不均衡的数据，变得均衡，然后再用SHJ去做关联。当然，这种方法的开发成本会高很多，不过算是一种比较通用的方法。\n\n这些方法，这些“招儿”，其实不好说孰优孰劣，还要结合具体数据量、场景，结合实际效果去看。不过，艺多不压身，调优比较重要的一点，就是要思考广、“招儿”多，然后通过对比不同优化方法的优劣势和效果差异，来更好地理解Spark，这样后面再有类似情况，就能轻车熟路啦~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626775258,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384877,"discussion_content":"非常感谢老师耐心的回答，受益良多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626779642,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293963,"user_name":"CycleGAN","can_delete":false,"product_type":"c1","uid":1679661,"ip_address":"","ucode":"9FD04813911A02","user_header":"https://static001.geekbang.org/account/avatar/00/19/a1/2d/599e9051.jpg","comment_is_top":false,"comment_ctime":1621652093,"is_pvip":false,"replies":[{"id":"106637","content":"对，做小维表的目的，实际上还是把Shuffle joins转化成Broadcast joins，这个转化带来的收益实在是太大了！所以千方百计地尝试做Broadcast joins，确实是非常值得的一件事~<br><br>预估大小UI没问题，另外还可以用executePlan查看执行计划的方法做预估。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621695668,"ip_address":"","comment_id":293963,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5916619389","product_id":100073401,"comment_content":"我们生产端用的2.4也没有AQE机制，但是我觉得减小维度表的大小也能模拟出来，要在维度表上做设计，最有效的还是分区做好，比如我们按时间分区，每个分区的数据局小很多，然后摘出来需要的列，列上的长str做编码，再反映射，列上的长join key做编码等。感觉把维表做小还是值得的，性价比很高。<br>然后分析大小的话，一是看cache后看大小，主要还是ui页面里的sql计划里看是不是Broadcast，倒是也不需要非常精准。<br>","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520449,"discussion_content":"对，做小维表的目的，实际上还是把Shuffle joins转化成Broadcast joins，这个转化带来的收益实在是太大了！所以千方百计地尝试做Broadcast joins，确实是非常值得的一件事~\n\n预估大小UI没问题，另外还可以用executePlan查看执行计划的方法做预估。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621695668,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293033,"user_name":"zxk","can_delete":false,"product_type":"c1","uid":1221195,"ip_address":"","ucode":"4BB2BD9D2BCD04","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","comment_is_top":false,"comment_ctime":1621163182,"is_pvip":false,"replies":[{"id":"106286","content":"不可以哈，Spark SQL目前还没有那么智能，你这么写的效果，和单独把users.type = ‘Head User’当作过滤条件的效果是一样的，都是通过Spark SQL AQE来动态触发Join策略调整，还做不到在静态优化阶段就提前决定在运行时把Shuffle Joins转化为Broadcast Joins。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621413176,"ip_address":"","comment_id":293033,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5916130478","product_id":100073401,"comment_content":"老师我想请问下，在第二个案例中，如果我将 join 条件写成 on orders.userId = users.id and users.type = ‘Head User’  ，是否能实现维度表提前过滤并达到 Broadcast 的要求？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520047,"discussion_content":"不可以哈，Spark SQL目前还没有那么智能，你这么写的效果，和单独把users.type = ‘Head User’当作过滤条件的效果是一样的，都是通过Spark SQL AQE来动态触发Join策略调整，还做不到在静态优化阶段就提前决定在运行时把Shuffle Joins转化为Broadcast Joins。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621413176,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382495,"discussion_content":"为什么不可以，如果先对小表结合过滤条件放入临时表，然后大表关联这张临时小表，有人会说，建立临时表有开销，我记得前几讲中这张小表的分区字段就是type，而且从实际的效果来看，即使这张小表分区字段不是type也关系不大，实际做过滤也是很快的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625617200,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":356199,"user_name":"林红波 :)","can_delete":false,"product_type":"c1","uid":3163171,"ip_address":"福建","ucode":"C4EEA6BFDEEF11","user_header":"https://static001.geekbang.org/account/avatar/00/30/44/23/8f5d80ad.jpg","comment_is_top":false,"comment_ctime":1662036061,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1662036061","product_id":100073401,"comment_content":"老师，第二个案例不是通过谓词下推，维表的数据量已经降下来了吗？为什么还要开启AQE？AQE不是通过判断shuffle map输出的中间文件来执行的吗？","like_count":0},{"had_liked":false,"id":337867,"user_name":"陌生的心酸","can_delete":false,"product_type":"c1","uid":2829738,"ip_address":"","ucode":"2F221D93403D20","user_header":"https://static001.geekbang.org/account/avatar/00/2b/2d/aa/e33e9edd.jpg","comment_is_top":false,"comment_ctime":1647097425,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1647097425","product_id":100073401,"comment_content":"有个疑问，对于方案二，如果维表【用户表】本身userID超大，还用来建分区表，这个是不是会导致hdfs namenode负载过重，甚至导致超过hive限制的分区数量，创建失败？","like_count":0},{"had_liked":false,"id":331416,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1642580491,"is_pvip":false,"replies":[{"id":"121204","content":"确实，可以利用AQE的自动倾斜处理来handle 20G的小表，不过其实我们在下一章节会介绍，AQE的自动倾斜处理，只能handle task粒度的倾斜，没有办法处理Executors粒度的倾斜，这个时候，还是需要依赖两阶段的Shuffle来完成，具体的加盐操作可以参考后面一章哈","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1642780285,"ip_address":"","comment_id":331416,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1642580491","product_id":100073401,"comment_content":"老师您好 您在评论 @陈威洋 对问题3的回答时说 &quot;AQE是需要Shuffle才能生效的，因此，要想利用AQE消除数据倾斜，得先对小表做repartition才行&quot;，但是问题3的背景已经是小表尺寸大于广播阈值 因此必须走shuffle了呀，那AQE在这个shuffle的map文件输出之后进行分区倾斜处理不就可以了吗？为什么要额外对小表进行一次 repartition 呢？谢谢老师~","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":547649,"discussion_content":"确实，可以利用AQE的自动倾斜处理来handle 20G的小表，不过其实我们在下一章节会介绍，AQE的自动倾斜处理，只能handle task粒度的倾斜，没有办法处理Executors粒度的倾斜，这个时候，还是需要依赖两阶段的Shuffle来完成，具体的加盐操作可以参考后面一章哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642780285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":323467,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1637916729,"is_pvip":false,"replies":[{"id":"117406","content":"SHJ和SMJ，最主要的区别，是SMJ可以利用外排，或者说可以依赖磁盘，对于内存的依赖，没那个重。但是SHJ就不同，HJ的build phase，必须完全依赖内存，这也是为什么会对小表的数量体量、数据分布有要求。一旦小表某一个分区，超过Task内存上限，就直接报OOM，整个Job也就fail了<br>","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637985971,"ip_address":"","comment_id":323467,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637916729","product_id":100073401,"comment_content":"老师，SHJ 要求小表数据均匀，防止某一分区OOM，是因为小表hash 后，会把相同的key, reduce到一个分区吗？那么这样SMJ 同样也会在shuffle 后有oom 的风险呀？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533817,"discussion_content":"SHJ和SMJ，最主要的区别，是SMJ可以利用外排，或者说可以依赖磁盘，对于内存的依赖，没那个重。但是SHJ就不同，HJ的build phase，必须完全依赖内存，这也是为什么会对小表的数量体量、数据分布有要求。一旦小表某一个分区，超过Task内存上限，就直接报OOM，整个Job也就fail了\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637985971,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":307277,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1629000339,"is_pvip":false,"replies":[{"id":"111313","content":"编码的思路很赞~ 不过用自增列是不行的，同样的Join Key，应该有同样的编码，这样才不影响原有的关联逻辑，自增列会导致同样的Join Key，有不同的编码。<br><br>比如，Join Key是order_id，那么在transactions表中，会有多个同样的order_id出现，也就是多笔交易同属于一个订单，这个时候，自增列编码是不行的。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629105601,"ip_address":"","comment_id":307277,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1629000339","product_id":100073401,"comment_content":"第一题首先打开aqe，对基表df.dropDupilcates(***).withColumn(&quot;code&quot;, monotonically_increasing_id())，这样就完成全局编码了。然后驱动表和基表都join上这个字典表，将组合join keys转换为字典值","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525102,"discussion_content":"编码的思路很赞~ 不过用自增列是不行的，同样的Join Key，应该有同样的编码，这样才不影响原有的关联逻辑，自增列会导致同样的Join Key，有不同的编码。\n\n比如，Join Key是order_id，那么在transactions表中，会有多个同样的order_id出现，也就是多笔交易同属于一个订单，这个时候，自增列编码是不行的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629105601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":307168,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1628919198,"is_pvip":false,"replies":[{"id":"111317","content":"是的， 这个use case比较特殊，就是Join Keys特别多，而且都是String类型，因此整体数据体量很大。确实，像你说的，使用哈希的目的，就是缩减Join Keys大小，那么咱们自然要确认缩减之后的Join Keys，一定要远远小于原始的Join Keys，否则做这件事就没有意义啦~<br><br>Tungsten那个，整型占用4个字节，这个和数据类型有关，其实和Tungsten没什么关系的。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629107961,"ip_address":"","comment_id":307168,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1628919198","product_id":100073401,"comment_content":"老师，对于哈希处理join keys或许要考虑基表的列数量和每列数据类型，因为毕竟两个哈希函数计算出来的结果再拼接，长度不下于32甚至大于64。假如原始列内容大部分都是英文和数字，而且大小或长度都不厉害，哈希处理后存储开销会不会更加大了呢？最后有一个问题需要确认下，在tungsten的数据结构里，一个整形数据，是否用到了32bit的存储空间呢？一个utf-8编码的中文是否占用2-4个字节？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525067,"discussion_content":"是的， 这个use case比较特殊，就是Join Keys特别多，而且都是String类型，因此整体数据体量很大。确实，像你说的，使用哈希的目的，就是缩减Join Keys大小，那么咱们自然要确认缩减之后的Join Keys，一定要远远小于原始的Join Keys，否则做这件事就没有意义啦~\n\nTungsten那个，整型占用4个字节，这个和数据类型有关，其实和Tungsten没什么关系的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629107961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301896,"user_name":"siys","can_delete":false,"product_type":"c1","uid":1950249,"ip_address":"","ucode":"3F6E368A53CF9B","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/z2z2OI0z9rbCngwkzrnP8mmUOXqDjS5I2u4ibfbb2eyvLicFwiclymCYFduU7gtksO6aAvSkbkfEhqCM4fXXQwCsg/132","comment_is_top":false,"comment_ctime":1625965579,"is_pvip":true,"replies":[{"id":"109765","content":"采用userId做分区键确实太细，实际应用中，我们几乎不会用这么细的ID做分区键。<br><br>先来说分桶吧，首先分桶一来能避免shuffle，二来能节省I&#47;O（Bucketing pruning），不过，单纯用分桶，是不能触发DPP机制的。<br><br>再来说说分区+分桶，你是指采用不同的Keys吗？比如分区用比较粗粒度的字段、分桶用userId，是这个意思吗？假设分区键不是userId，那么即便分桶用了userId，Spark SQL也不会触发DPP。<br><br>DPP的条件确实比较苛刻，要求分区键包含Join Keys，因此分桶并不能帮上什么忙。<br><br>不过，尽管如此，分桶在避免Shuffle上倒确实是把好手~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1626692392,"ip_address":"","comment_id":301896,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1625965579","product_id":100073401,"comment_content":"案例2中orders_new如果采用分区加分桶的方式能够触发DPP吗，采用userid分的太细了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523120,"discussion_content":"采用userId做分区键确实太细，实际应用中，我们几乎不会用这么细的ID做分区键。\n\n先来说分桶吧，首先分桶一来能避免shuffle，二来能节省I/O（Bucketing pruning），不过，单纯用分桶，是不能触发DPP机制的。\n\n再来说说分区+分桶，你是指采用不同的Keys吗？比如分区用比较粗粒度的字段、分桶用userId，是这个意思吗？假设分区键不是userId，那么即便分桶用了userId，Spark SQL也不会触发DPP。\n\nDPP的条件确实比较苛刻，要求分区键包含Join Keys，因此分桶并不能帮上什么忙。\n\n不过，尽管如此，分桶在避免Shuffle上倒确实是把好手~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626692392,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1511712,"avatar":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","nickname":"wow_xiaodi","note":"","ucode":"B3FB301556A7EA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388694,"discussion_content":"分桶几乎没啥用，假设是按userid分桶，你不能确保用户表筛选出的userid分布刚刚好在某个桶内，到最后，你还是得全量扫描数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628916362,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294702,"user_name":"姚先生","can_delete":false,"product_type":"c1","uid":1181069,"ip_address":"","ucode":"B2FAE540D562E1","user_header":"https://static001.geekbang.org/account/avatar/00/12/05/8d/27db1e0f.jpg","comment_is_top":false,"comment_ctime":1622048362,"is_pvip":false,"replies":[{"id":"107058","content":"一点都没想错，完全同意你说的。这个案例确实有些牵强，对不住大家，时间比较急，没有找到更好的案例。这里确实存在你说的隐患，本质上就是Join Key和分区键的矛盾。<br><br>分区键要求Cardinality不能太高，否则就会出现文件分布过细、过散，分布式文件系统负担过重。但是另一方面，Join Key往往是userId、orderId、txId这类Cardinality很大的字段，这个是由业务逻辑决定的，很难改变。<br><br>因此，分区键和Join Key就会存在博弈，换句话说，是牺牲文件系统效率来强行触发DPP，还是保证文件系统效率而放弃DPP，这里面就会有一个取舍的问题。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1622125623,"ip_address":"","comment_id":294702,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1622048362","product_id":100073401,"comment_content":"个人感觉，案例2的解决方案略显牵强。不知道是不是想错了，分享一下。orders表的分区键改为userId，hdfs文件会有count(distinct userId)个文件夹，可能的问题：1.小文件过多造成扫描文件的task数目过多进而增大调度开销;2.hdfs namenode负载问题","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520774,"discussion_content":"一点都没想错，完全同意你说的。这个案例确实有些牵强，对不住大家，时间比较急，没有找到更好的案例。这里确实存在你说的隐患，本质上就是Join Key和分区键的矛盾。\n\n分区键要求Cardinality不能太高，否则就会出现文件分布过细、过散，分布式文件系统负担过重。但是另一方面，Join Key往往是userId、orderId、txId这类Cardinality很大的字段，这个是由业务逻辑决定的，很难改变。\n\n因此，分区键和Join Key就会存在博弈，换句话说，是牺牲文件系统效率来强行触发DPP，还是保证文件系统效率而放弃DPP，这里面就会有一个取舍的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622125623,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293402,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1621384411,"is_pvip":false,"replies":[{"id":"106262","content":"咱们举了3个案例，其中第二个案例需要用到3.0的AQE，具体来说是Join策略调整；第三个案例是用join hints把Shuffle SMJ转化为Shuffle HJ。这两个case利用的新特性，确实是3.0才支持。<br><br>不过第一个，也就是Join Keys比Payload大很多，这个技巧并不需要3.0哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621399953,"ip_address":"","comment_id":293402,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1621384411","product_id":100073401,"comment_content":"老师，这些都是基于spark3.0的新特性，那对于3.0之前的版本又该怎么解决呢，毕竟3.0版本是新出的，对于大多数公司不一定使用了该版本","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520197,"discussion_content":"咱们举了3个案例，其中第二个案例需要用到3.0的AQE，具体来说是Join策略调整；第三个案例是用join hints把Shuffle SMJ转化为Shuffle HJ。这两个case利用的新特性，确实是3.0才支持。\n\n不过第一个，也就是Join Keys比Payload大很多，这个技巧并不需要3.0哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621399953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}