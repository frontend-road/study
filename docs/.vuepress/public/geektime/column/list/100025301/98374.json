{"id":98374,"title":"19 | 综合案例实战：处理加州房屋信息，构建线性回归模型","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“综合案例实战：处理加州房屋信息，构建线性回归模型”。</p><p>通过之前的学习，我们对Spark各种API的基本用法有了一定的了解，还通过统计词频的实例掌握了如何从零开始写一个Spark程序。那么现在，让我们从一个真实的数据集出发，看看如何用Spark解决实际问题。</p><h2>数据集介绍</h2><p>为了完成今天的综合案例实战，我使用的是美国加州1990年房屋普查的数据集。</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/5c/a9c1d749f2d1c43261a043aa77056f5c.png?wh=1142*593\" alt=\"\"></p><p>数据集中的每一个数据都代表着一块区域内房屋和人口的基本信息，总共包括9项：</p><ol>\n<li>该地区中心的纬度（latitude）</li>\n<li>该地区中心的经度（longitude）</li>\n<li>区域内所有房屋屋龄的中位数（housingMedianAge）</li>\n<li>区域内总房间数（totalRooms）</li>\n<li>区域内总卧室数（totalBedrooms）</li>\n<li>区域内总人口数（population）</li>\n<li>区域内总家庭数（households）</li>\n<li>区域内人均收入中位数（medianIncome）</li>\n<li>该区域房价的中位数（medianHouseValue）</li>\n</ol><p>也就是说，我们可以把每一个数据看作一个地区，它含有9项我们关心的信息，也就是上面提到的9个指标。比如下面这个数据：</p><pre><code>-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000'\n</code></pre><p>这个数据代表该地区的经纬度是（-122.230000,37.880000），这个地区房屋历史的中位数是41年，所有房屋总共有880个房间，其中有129个卧室。这个地区内共有126个家庭和322位居民，人均收入中位数是8.3252万，房价中位数是45.26万。</p><!-- [[[read_end]]] --><p>这里的地域单位是美国做人口普查的最小地域单位，平均一个地域单位中有1400多人。在这个数据集中共有两万多个这样的数据。显然，这样小的数据量我们并“不需要”用Spark来处理，但是，它可以起到一个很好的示例作用。这个数据集可以从<a href=\"http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\">网上</a>下载到。这个数据集是在1997年的一篇学术论文中创建的，感兴趣的同学可以去亲自下载，并加以实践。</p><p>那么我们今天的目标是什么呢？就是用已有的数据，构建一个<strong>线性回归模型</strong>，来预测房价。</p><p>我们可以看到，前8个属性都可能对房价有影响。这里，我们假设这种影响是线性的，我们就可以找到一个类似<strong>A=b<em>B+c</em>C+d<em>D+…+i</em>I</strong>的公式，A代表房价，B到I分别代表另外八个属性。这样，对于不在数据集中的房子，我们可以套用这个公式来计算出一个近似的房价。由于专栏的定位是大规模数据处理专栏，所以我们不会细讲统计学的知识。如果你对统计学知识感兴趣，或者还不理解什么是线性回归的话，可以去自行学习一下。</p><h2>进一步了解数据集</h2><p>每当我们需要对某个数据集进行处理时，不要急着写代码。你一定要先观察数据集，了解它的特性，并尝试对它做一些简单的预处理，让数据的可读性更好。这些工作我们最好在Spark的交互式Shell上完成，而不是创建python的源文件并执行。因为，在Shell上我们可以非常直观而简便地看到每一步的输出。</p><p>首先，让我们把数据集读入Spark。</p><pre><code>from pyspark.sql import SparkSession\n\n# 初始化SparkSession和SparkContext\nspark = SparkSession.builder\n  .master(&quot;local&quot;)\n  .appName(&quot;California Housing &quot;)\n  .config(&quot;spark.executor.memory&quot;, &quot;1gb&quot;)\n  .getOrCreate()\nsc = spark.sparkContext\n\n# 读取数据并创建RDD\nrdd = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.data')\n\n# 读取数据每个属性的定义并创建RDD\nheader = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.domain')\n</code></pre><p>这样，我们就把房屋信息数据和每个属性的定义读入了Spark，并创建了两个相应的RDD。你还记得吧？RDD是有一个惰性求值的特性的，所以，我们可以用collect()函数来把数据输出在Shell上。</p><pre><code>header.collect()\n\n[u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']\n</code></pre><p>这样，我们就得到了每个数据所包含的信息，这和我们前面提到的9个属性的顺序是一致的，而且它们都是连续的值，而不是离散的。你需要注意的是，collect()函数会把所有数据都加载到内存中，如果数据很大的话，有可能会造成内存泄漏，所以要小心使用。平时比较常见的方法是用take()函数去只读取RDD中的某几个元素。</p><p>由于RDD中的数据可能会比较大，所以接下来让我们读取它的前两个数据。</p><pre><code>rdd.take(2)\n\n[u'-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000', u'-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']\n</code></pre><p>由于我们是用SparkContext的textFile函数去创建RDD，所以每个数据其实是一个大的字符串，各个属性之间用逗号分隔开来。这不利于我们之后的处理，因为我们可能会需要分别读取每个对象的各个属性。所以，让我们用map函数把大字符串分隔成数组，这会方便我们的后续操作。</p><pre><code>rdd = rdd.map(lambda line: line.split(&quot;,&quot;))\nrdd.take(2)\n\n[[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000'], [u'-122.220000', u'37.860000', u'21.000000', u'7099.000000', u'1106.000000', u'2401.000000', u'1138.000000', u'8.301400', u'358500.000000']]\n</code></pre><p>我们在前面学过，Spark SQL的DataFrame API在查询结构化数据时更方便使用，而且性能更好。在这个例子中你可以看到，数据的schema是定义好的，我们需要去查询各个列，所以DataFrame API显然更加适用。所以，我们需要先把RDD转换为DataFrame。</p><p>具体来说，就是需要把之前用数组代表的对象，转换成为Row对象，再用toDF()函数转换成DataFrame。</p><pre><code>from pyspark.sql import Row\n\ndf = rdd.map(lambda line: Row(longitude=line[0],\n                             latitude=line[1],\n                             housingMedianAge=line[2],\n                             totalRooms=line[3],\n                             totalBedRooms=line[4],\n                             population=line[5],\n                             households=line[6],\n                             medianIncome=line[7],\n                             medianHouseValue=line[8])).toDF()\n</code></pre><p>现在我们可以用show()函数打印出这个DataFrame所含的数据表。</p><pre><code>df.show()\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/de/24/de91764e7e7cc3d143a8217400ec0524.png?wh=1142*488\" alt=\"\"></p><p>这里每一列的数据格式都是string，但是，它们其实都是数字，所以我们可以通过cast()函数把每一列的类型转换成float。</p><pre><code>def convertColumn(df, names, newType)\n for name in names:\n    df = df.withColumn(name, df[name].cast(newType))\n return df\n\ncolumns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n\ndf = convertColumn(df, columns, FloatType())\n</code></pre><p>转换成数字有很多优势。比如，我们可以按某一列，对所有对象进行排序，也可以计算平均值等。比如，下面这段代码就可以统计出所有建造年限各有多少个房子。</p><pre><code>df.groupBy(&quot;housingMedianAge&quot;).count().sort(&quot;housingMedianAge&quot;,ascending=False).show()\n</code></pre><h2>预处理</h2><p>通过上面的数据分析，你可能会发现这些数据还是不够直观。具体的问题有：</p><ol>\n<li>房价的值普遍都很大，我们可以把它调整成相对较小的数字；</li>\n<li>有的属性没什么意义，比如所有房子的总房间数和总卧室数，我们更加关心的是平均房间数；</li>\n<li>在我们想要构建的线性模型中，房价是结果，其他属性是输入参数。所以我们需要把它们分离处理；</li>\n<li>有的属性最小值和最大值范围很大，我们可以把它们标准化处理。</li>\n</ol><p>对于第一点，我们观察到大多数房价都是十万起的，所以可以用withColumn()函数把所有房价都除以100000。</p><pre><code>df = df.withColumn(&quot;medianHouseValue&quot;, col(&quot;medianHouseValue&quot;)/100000)\n</code></pre><p>对于第二点，我们可以添加如下三个新的列：</p><ul>\n<li>每个家庭的平均房间数：roomsPerHousehold</li>\n<li>每个家庭的平均人数：populationPerHousehold</li>\n<li>卧室在总房间的占比：bedroomsPerRoom</li>\n</ul><p>当然，你们可以自由添加你们觉得有意义的列，这里的三个是我觉得比较典型的。同样，用withColumn()函数可以容易地新建列。</p><pre><code>df = df.withColumn(&quot;roomsPerHousehold&quot;, col(&quot;totalRooms&quot;)/col(&quot;households&quot;))\n  .withColumn(&quot;populationPerHousehold&quot;, col(&quot;population&quot;)/col(&quot;households&quot;))\n  .withColumn(&quot;bedroomsPerRoom&quot;, col(&quot;totalBedRooms&quot;)/col(&quot;totalRooms&quot;))\n</code></pre><p>同样，有的列是我们并不关心的，比如经纬度，这个数值很难有线性的意义。所以我们可以只留下重要的信息列。</p><pre><code>df = df.select(&quot;medianHouseValue&quot;,\n             &quot;totalBedRooms&quot;,\n             &quot;population&quot;,\n             &quot;households&quot;,\n             &quot;medianIncome&quot;,\n             &quot;roomsPerHousehold&quot;,\n             &quot;populationPerHousehold&quot;,\n             &quot;bedroomsPerRoom&quot;)\n</code></pre><p>对于第三点，最简单的办法就是把DataFrame转换成RDD，然后用map()函数把每个对象分成两部分：房价和一个包含其余属性的列表，然后在转换回DataFrame。</p><pre><code>from pyspark.ml.linalg import DenseVector\n\ninput_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\ndf = spark.createDataFrame(input_data, [&quot;label&quot;, &quot;features&quot;])\n</code></pre><p>我们重新把两部分重新标记为“label”和“features”，label代表的是房价，features代表包括其余参数的列表。</p><p>对于第四点，数据的标准化我们可以借助Spark的机器学习库Spark ML来完成。Spark ML也是基于DataFrame，它提供了大量机器学习的算法实现、数据流水线（pipeline）相关工具和很多常用功能。由于本专栏的重点是大数据处理，所以我们并没有介绍Spark ML，但是我强烈推荐同学们有空去了解一下它。</p><p>在这个AI和机器学习的时代，我们不能落伍。</p><pre><code>from pyspark.ml.feature import StandardScaler\n\nstandardScaler = StandardScaler(inputCol=&quot;features&quot;, outputCol=&quot;features_scaled&quot;)\nscaler = standardScaler.fit(df)\nscaled_df = scaler.transform(df)\n</code></pre><p>在第二行，我们创建了一个StandardScaler，它的输入是features列，输出被我们命名为features_scaled。第三、第四行，我们把这个scaler对已有的DataFrame进行处理，让我们看下代码块里显示的输出结果。</p><pre><code>scaled_df.take(1)\n\n[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264]))]\n</code></pre><p>我们可以清楚地看到，这一行新增了一个features_scaled的列，它里面每个数据都是标准化过的，我们应该用它，而非features来训练模型。</p><h2>创建模型</h2><p>上面的预处理都做完后，我们终于可以开始构建线性回归模型了。</p><p>首先，我们需要把数据集分为训练集和测试集，训练集用来训练模型，测试集用来评估模型的正确性。DataFrame的randomSplit()函数可以很容易的随机分割数据，这里我们将80%的数据用于训练，剩下20%作为测试集。</p><pre><code>train_data, test_data = scaled_df.randomSplit([.8,.2],seed=123)\n</code></pre><p>用Spark ML提供的LinearRegression功能，我们可以很容易得构建一个线性回归模型，如下所示。</p><pre><code>from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol='features_scaled', labelCol=&quot;label&quot;, maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlinearModel = lr.fit(train_data)\n</code></pre><p>LinearRegression可以调节的参数还有很多，你可以去<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression\">官方API文档</a>查阅，这里我们只是示范一下。</p><h2>模型评估</h2><p>现在有了模型，我们终于可以用linearModel的transform()函数来预测测试集中的房价，并与真实情况进行对比。代码如下所示。</p><pre><code>predicted = linearModel.transform(test_data)\npredictions = predicted.select(&quot;prediction&quot;).rdd.map(lambda x: x[0])\nlabels = predicted.select(&quot;label&quot;).rdd.map(lambda x: x[0])\npredictionAndLabel = predictions.zip(labels).collect()\n</code></pre><p>我们用RDD的zip()函数把预测值和真实值放在一起，这样可以方便地进行比较。比如让我们看一下前两个对比结果。</p><pre><code>predictionAndLabel.take(2)\n\n[(1.4491508524918457, 1.14999), (1.5831547768979277, 0.964)]\n</code></pre><p>这里可以看出，我们的模型预测的结果有些偏小，这可能有多个因素造成。最直接的原因就是房价与我们挑选的列并没有强线性关系，而且我们使用的参数也可能不够准确。</p><p>这一讲我只是想带着你一起体验下处理真实数据集和解决实际问题的感觉，想要告诉你的是这种通用的思想，并帮助你继续熟悉Spark各种库的用法，并不是说房价一定就是由这些参数线性决定了。感兴趣的同学可以去继续优化，或者尝试别的模型。</p><h2>小结</h2><p>这一讲我们通过一个真实的数据集，通过以下步骤解决了一个实际的数据处理问题：</p><ol>\n<li>观察并了解数据集</li>\n<li>数据清洗</li>\n<li>数据的预处理</li>\n<li>训练模型</li>\n<li>评估模型</li>\n</ol><p>其实这里还可以有与“优化与改进”相关的内容，这里没有去阐述是因为我们的首要目的依然是熟悉与使用Spark各类API。相信通过今天的学习，你初步了解了数据处理问题的一般思路，并强化了对RDD、DataFrame和机器学习API的使用。</p><h2>实践与思考题</h2><p>今天请你下载这个数据集，按文章的介绍去动手实践一次。如果有时间的话，还可以对这个过程的优化和改进提出问题并加以解决。</p><p>欢迎你在留言板贴出自己的idea。如果你觉得有所收获，也欢迎你把文章分享给朋友。</p><p></p>","neighbors":{"left":{"article_title":"18 | Word Count：从零开始运行你的第一个Spark应用","id":97658},"right":{"article_title":"20 | 流处理案例实战：分析纽约市出租车载客信息","id":98537}},"comments":[{"had_liked":false,"id":99674,"user_name":"coder","can_delete":false,"product_type":"c1","uid":1399673,"ip_address":"","ucode":"929E3FFD14EFC8","user_header":"https://static001.geekbang.org/account/avatar/00/15/5b/79/d55044ac.jpg","comment_is_top":false,"comment_ctime":1559277071,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"70278753807","product_id":100025301,"comment_content":"老师的代码可以po到Github上，这样大家都可以学习了🌝🌝🌝","like_count":17},{"had_liked":false,"id":130262,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1567414829,"is_pvip":false,"replies":[{"id":"49311","content":"给你点赞👍加油","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1567618495,"ip_address":"","comment_id":130262,"utype":1}],"discussion_count":3,"race_medal":0,"score":"48812055085","product_id":100025301,"comment_content":"终于跑通了，不容易啊，刚开始数据集没下载正确。有空值，老师给的数据集较干净。<br>别的数据集，要记得去除空值。<br>下面是实践代码jupyter<br>https:&#47;&#47;github.com&#47;LearningChanging&#47;spark-exercise&#47;blob&#47;master&#47;19&#47;CaliforniaHousing.ipynb","like_count":12,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465878,"discussion_content":"给你点赞👍加油","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567618495,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1096652,"avatar":"https://static001.geekbang.org/account/avatar/00/10/bb/cc/fac12364.jpg","nickname":"xxx","note":"","ucode":"E79CEA70430449","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":542836,"discussion_content":"谢谢大哥，跑通了。不过这个线性回归一点也不准啊😂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640858108,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028947,"avatar":"","nickname":"Geek5350","note":"","ucode":"6EB4553707C357","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375351,"discussion_content":"请问数据集在哪里下载？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621586963,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":99862,"user_name":"kylin","can_delete":false,"product_type":"c1","uid":1351248,"ip_address":"","ucode":"3588B62A4F9534","user_header":"https://static001.geekbang.org/account/avatar/00/14/9e/50/21e0beca.jpg","comment_is_top":false,"comment_ctime":1559345251,"is_pvip":true,"replies":[{"id":"42766","content":"PySpark的SQL库只有DataFrame，并没有DataSet。不过在Scala和Java中，DataSet已经成为了统一的SQL入口。","user_name":"作者回复","user_name_real":"廿七","uid":"1386753","ctime":1563948592,"ip_address":"","comment_id":99862,"utype":1}],"discussion_count":2,"race_medal":0,"score":"48803985507","product_id":100025301,"comment_content":"请问为什么不用dateset进行数据处理而是用dateFrame? ","like_count":12,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452300,"discussion_content":"PySpark的SQL库只有DataFrame，并没有DataSet。不过在Scala和Java中，DataSet已经成为了统一的SQL入口。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563948592,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2701,"discussion_content":"python 不支持 dataset","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563869173,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":104094,"user_name":"hufox","can_delete":false,"product_type":"c1","uid":1022874,"ip_address":"","ucode":"94675994D8EE16","user_header":"https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg","comment_is_top":false,"comment_ctime":1560650203,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"14445552091","product_id":100025301,"comment_content":"最后一句的代码改成 predictionAndLabel[:2] ，可以了！","like_count":3,"discussions":[{"author":{"id":1220750,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a0/8e/6e4c7509.jpg","nickname":"一","note":"","ucode":"28E0605EA1AE88","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":14597,"discussion_content":"感谢分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568772654,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1149798,"avatar":"https://static001.geekbang.org/account/avatar/00/11/8b/66/2b55a7ac.jpg","nickname":"周","note":"","ucode":"E4D57FF40B355A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5786,"discussion_content":"请问你的数据有权限下载么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566465022,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":177713,"user_name":"咸鱼与果汁","can_delete":false,"product_type":"c1","uid":1283861,"ip_address":"","ucode":"F79383599ACB02","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoRyUPicEMqGsbsMicHPuvwM8nibfgK8Yt0AibAGUmnic7rLF4zUZ4dBj4ialYz54fOD6sURKwuJIWBNjhg/132","comment_is_top":false,"comment_ctime":1581478761,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5876446057","product_id":100025301,"comment_content":"spark df的数据处理还是略显复杂，感觉大部分的算法人员还是使用pandas进行数据预处理，请问使用pandas是不是就无法发挥spark RDD的威力了？这种情况下spark就相当于是一个异步任务处理框架？","like_count":2,"discussions":[{"author":{"id":1110662,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f2/86/d689f77e.jpg","nickname":"Hank_Yan","note":"","ucode":"86899B561C502B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289637,"discussion_content":"数据量太大的时候，pandas乏力。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594169715,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":134100,"user_name":"henry","can_delete":false,"product_type":"c1","uid":1122491,"ip_address":"","ucode":"7FD012EE0D3035","user_header":"https://static001.geekbang.org/account/avatar/00/11/20/bb/43d63c5f.jpg","comment_is_top":false,"comment_ctime":1568743082,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5863710378","product_id":100025301,"comment_content":"最后一步，“模型预测的结果有些偏小”，这一点，从结果上看，不是应该预测的结果要大一些吗？","like_count":1},{"had_liked":false,"id":123526,"user_name":"黄智寿","can_delete":false,"product_type":"c1","uid":1249951,"ip_address":"","ucode":"43685DE62F720B","user_header":"https://static001.geekbang.org/account/avatar/00/13/12/9f/b6eb3471.jpg","comment_is_top":false,"comment_ctime":1565693416,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5860660712","product_id":100025301,"comment_content":"老师，你好，数据集的下载地址能发一下吗？","like_count":1},{"had_liked":false,"id":106284,"user_name":"毛毛","can_delete":false,"product_type":"c1","uid":1257705,"ip_address":"","ucode":"D6F93A13E9046A","user_header":"https://static001.geekbang.org/account/avatar/00/13/30/e9/13fb8d51.jpg","comment_is_top":false,"comment_ctime":1561255194,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5856222490","product_id":100025301,"comment_content":"老师，建议在案例讲解时对用到的算法大概解释下，比如srandscaler，这不是太清楚什么用处？谢谢","like_count":1,"discussions":[{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":202828,"discussion_content":"scaler就是线性代数中对多维数据的转换函数，你如果了解数据归一化就知道这是做什么的了，大概来讲就是要把数据集中的数据无损的映射到[0,1]这个区间之中，方便训练的时算法快速收敛","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583958768,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":104093,"user_name":"hufox","can_delete":false,"product_type":"c1","uid":1022874,"ip_address":"","ucode":"94675994D8EE16","user_header":"https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg","comment_is_top":false,"comment_ctime":1560649780,"is_pvip":false,"discussion_count":3,"race_medal":0,"score":"5855617076","product_id":100025301,"comment_content":"在执行最后一句代码predictionAndLabel.take(2)时报错：<br>AttributeError                            Traceback (most recent call last)<br>&lt;ipython-input-35-0700ca2381fb&gt; in &lt;module&gt;<br>----&gt; 1 predictionAndLabel.take(2)<br><br>AttributeError: &#39;list&#39; object has no attribute &#39;take&#39;","like_count":2,"discussions":[{"author":{"id":1022874,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg","nickname":"hufox","note":"","ucode":"94675994D8EE16","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6146,"discussion_content":"你可以从这里下载 https://ndownloader.figshare.com/files/5976036","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1566725774,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1170811,"avatar":"https://static001.geekbang.org/account/avatar/00/11/dd/7b/c1efb1d9.jpg","nickname":"胡永","note":"","ucode":"931DA91D191FEE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":350751,"discussion_content":"可以改为 predictionAndLabel[:2]","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1613998353,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1149798,"avatar":"https://static001.geekbang.org/account/avatar/00/11/8b/66/2b55a7ac.jpg","nickname":"周","note":"","ucode":"E4D57FF40B355A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5810,"discussion_content":"请问这个样本数据集 你有么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566467718,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":100904,"user_name":"gotojeff","can_delete":false,"product_type":"c1","uid":1212606,"ip_address":"","ucode":"1E49871F44F9E5","user_header":"https://static001.geekbang.org/account/avatar/00/12/80/be/8350f94d.jpg","comment_is_top":false,"comment_ctime":1559683553,"is_pvip":false,"replies":[{"id":"42767","content":"感谢提问。PySpark现在不支持DataSet，只有Scala和Java支持。这是由语言特性决定的，Python是动态类型的语言，而DataSet是强类型的，要求在编译时检测类型安全。所以，在所有用Python的代码例子中，我用的都是DataFrame。<br><br>大部分人都同意在Spark中，Scala和Python是优于Java和R的。至于在Spark生态中，Scala和Python孰优孰劣，这是个很主观的问题，我们不能只因为不支持DataSet这一点就说Python比Scala差。<br><br>Scala确实很优秀，Spark原生的实现就是用Scala写的，所以任何新发布的功能肯定支持Scala，官方文档也都是用Scala来举例子。而且Scala的性能要优于Python。但是Python也有很多优点，比如容易学习、应用场景广。这两种语言在Spark的世界中都可以满足我们绝大多数的需求，选择任何一个都不是错误的。","user_name":"作者回复","user_name_real":"廿七","uid":"1386753","ctime":1563948702,"ip_address":"","comment_id":100904,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5854650849","product_id":100025301,"comment_content":"dataset不支持python, 所以在python里只有DF，这算不算python的一大劣势？scala是更好的选择？","like_count":2,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452729,"discussion_content":"感谢提问。PySpark现在不支持DataSet，只有Scala和Java支持。这是由语言特性决定的，Python是动态类型的语言，而DataSet是强类型的，要求在编译时检测类型安全。所以，在所有用Python的代码例子中，我用的都是DataFrame。\n\n大部分人都同意在Spark中，Scala和Python是优于Java和R的。至于在Spark生态中，Scala和Python孰优孰劣，这是个很主观的问题，我们不能只因为不支持DataSet这一点就说Python比Scala差。\n\nScala确实很优秀，Spark原生的实现就是用Scala写的，所以任何新发布的功能肯定支持Scala，官方文档也都是用Scala来举例子。而且Scala的性能要优于Python。但是Python也有很多优点，比如容易学习、应用场景广。这两种语言在Spark的世界中都可以满足我们绝大多数的需求，选择任何一个都不是错误的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563948702,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1149798,"avatar":"https://static001.geekbang.org/account/avatar/00/11/8b/66/2b55a7ac.jpg","nickname":"周","note":"","ucode":"E4D57FF40B355A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5809,"discussion_content":"请问这个样本数据集 你有么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566467697,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":100381,"user_name":"abc-web","can_delete":false,"product_type":"c1","uid":1371804,"ip_address":"","ucode":"DE3B873863EFF9","user_header":"https://static001.geekbang.org/account/avatar/00/14/ee/9c/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1559542989,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5854510285","product_id":100025301,"comment_content":"老师代码可以上github吗，这样同学们可以参考下","like_count":1},{"had_liked":false,"id":99648,"user_name":"Zoe","can_delete":false,"product_type":"c1","uid":1528912,"ip_address":"","ucode":"A5D671919EE7B1","user_header":"https://static001.geekbang.org/account/avatar/00/17/54/50/8a76a8cc.jpg","comment_is_top":false,"comment_ctime":1559273293,"is_pvip":false,"replies":[{"id":"36674","content":"👍","user_name":"作者回复","user_name_real":"Geek_Test","uid":"1568170","ctime":1559990482,"ip_address":"","comment_id":99648,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5854240589","product_id":100025301,"comment_content":"看前两篇文章时还在想，没什么练手的机会啊，今天就推送了实战练习，有一种终于跟上大神思维的幻觉，开心！","like_count":1,"discussions":[{"author":{"id":1568170,"avatar":"https://static001.geekbang.org/account/avatar/00/17/ed/aa/c2b9e399.jpg","nickname":"Geek_Test","note":"","ucode":"2A70B596E8F49D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452204,"discussion_content":"👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1559990482,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293885,"user_name":"Geek5350","can_delete":false,"product_type":"c1","uid":2028947,"ip_address":"","ucode":"6EB4553707C357","user_header":"","comment_is_top":false,"comment_ctime":1621587032,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1621587032","product_id":100025301,"comment_content":"请问数据集在哪里下载？","like_count":0},{"had_liked":false,"id":272985,"user_name":"寻水的小鱼","can_delete":false,"product_type":"c1","uid":1502696,"ip_address":"","ucode":"A3935497FEAD1C","user_header":"https://static001.geekbang.org/account/avatar/00/16/ed/e8/8985d6e0.jpg","comment_is_top":false,"comment_ctime":1610377690,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1610377690","product_id":100025301,"comment_content":"&gt;&gt;&gt; <br>&gt;&gt;&gt; def convertColumn(df, names, newType)<br>  File &quot;&lt;stdin&gt;&quot;, line 1<br>    def convertColumn(df, names, newType)<br>                                        ^<br>SyntaxError: invalid syntax<br>","like_count":0},{"had_liked":false,"id":243192,"user_name":"之渊","can_delete":false,"product_type":"c1","uid":1876212,"ip_address":"","ucode":"02B9299DBB4881","user_header":"https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg","comment_is_top":false,"comment_ctime":1597989592,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1597989592","product_id":100025301,"comment_content":"java 版的代码demo : https:&#47;&#47;gitee.com&#47;oumin12345&#47;daimademojihe&#47;tree&#47;master&#47;cloudx&#47;bigdata&#47;src&#47;main&#47;java&#47;test&#47;spark<br>建议初学者没写过的可以自己敲一下。<br>机器学习的入门可以看看 ：<br>https:&#47;&#47;my.oschina.net&#47;ouminzy&#47;blog&#47;4437101","like_count":0},{"had_liked":false,"id":216146,"user_name":"xianhai","can_delete":false,"product_type":"c1","uid":1073505,"ip_address":"","ucode":"906578663CEB3E","user_header":"https://static001.geekbang.org/account/avatar/00/10/61/61/677e8f92.jpg","comment_is_top":false,"comment_ctime":1589196462,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1589196462","product_id":100025301,"comment_content":"最好还是给完整的代码，节省初学者的时间。","like_count":0},{"had_liked":false,"id":178490,"user_name":"Chloe","can_delete":false,"product_type":"c1","uid":1004953,"ip_address":"","ucode":"C4848ED5B35752","user_header":"https://static001.geekbang.org/account/avatar/00/0f/55/99/4bdadfd3.jpg","comment_is_top":false,"comment_ctime":1581704431,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1581704431","product_id":100025301,"comment_content":"报错：<br>&quot;<br>Traceback (most recent call last):<br>  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;<br>NameError: name &#39;FloatType&#39; is not defined<br>&quot;<br>Google了一下: https:&#47;&#47;stackoverflow.com&#47;questions&#47;40701122&#47;unexpected-type-class-pyspark-sql-types-datatypesingleton-when-casting-to-i<br><br>加了这句就好了：<br>from pyspark.sql.types import FloatType<br><br>大家还有人也遇到这个错误吗？","like_count":0},{"had_liked":false,"id":130255,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1567413618,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1567413618","product_id":100025301,"comment_content":"StandardScaler 归一化之后，两列变成 NaN 了搞不明白<br>+-----+--------------------+--------------------+<br>|label|            features|     features_scaled|<br>+-----+--------------------+--------------------+<br>|4.526|[129.0,322.0,126....|[NaN,0.2843362208...|<br>|3.585|[1106.0,2401.0,11...|[NaN,2.1201592122...|<br>+-----+--------------------+--------------------+<br><br>scaled_df.take(2)<br>[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([nan, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, nan])),<br> Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([nan, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, nan]))]","like_count":0,"discussions":[{"author":{"id":1127175,"avatar":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","nickname":"JustDoDT","note":"","ucode":"6AF0B80F00EAEF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7174,"discussion_content":"原因是我自己找的原始数据集里面含有空值。用之前要对空值进行处理，要不然就是这个结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567414975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":120666,"user_name":"西北偏北","can_delete":false,"product_type":"c1","uid":1043160,"ip_address":"","ucode":"64BD69C84EE6A1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132","comment_is_top":false,"comment_ctime":1564969067,"is_pvip":false,"replies":[{"id":"47611","content":"在实践中看到需要可以回头看一下基础知识","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1566883126,"ip_address":"","comment_id":120666,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1564969067","product_id":100025301,"comment_content":"一些实际的大数据处理，确实需要数学啊……怎么才能把数学学好？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":461424,"discussion_content":"在实践中看到需要可以回头看一下基础知识","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566883126,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}