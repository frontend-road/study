{"id":90081,"title":"01 | 为什么MapReduce会被硅谷一线公司淘汰？","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“为什么MapReduce会被硅谷一线公司淘汰”。</p><p>我有幸几次与来Google参观的同行进行交流，当谈起数据处理技术时，他们总是试图打探MapReduce方面的经验。</p><p>这一点让我颇感惊讶，因为在硅谷，早已没有人去谈论MapReduce了。</p><p>今天这一讲，我们就来聊聊为什么MapReduce会被硅谷一线公司淘汰。</p><p>我们先来沿着时间线看一下超大规模数据处理的重要技术以及它们产生的年代。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/ca/54a0178e675d0054cda83b5dc89b1dca.png?wh=5000*3092\" alt=\"\"></p><p>我认为可以把超大规模数据处理的技术发展分为三个阶段：石器时代，青铜时代，蒸汽机时代。</p><h3>石器时代</h3><p>我用“石器时代”来比喻MapReduce诞生之前的时期。</p><p>数据的大规模处理问题早已存在。早在2003年的时候，Google就已经面对大于600亿的搜索量。</p><p>但是数据的大规模处理技术还处在彷徨阶段。当时每个公司或者个人可能都有自己的一套工具处理数据。却没有提炼抽象出一个系统的方法。</p><h3>青铜时代</h3><p>2003年，MapReduce的诞生标志了超大规模数据处理的第一次革命，而开创这段青铜时代的就是下面这篇论文《MapReduce: Simplified Data Processing on Large Clusters》。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/ae/61/ae9083e7b1f5cdd97deda1c8a1344861.png?wh=2817*1556\" alt=\"\"></p><p>杰夫（Jeff Dean）和桑杰（Sanjay Ghemawat）从纷繁复杂的业务逻辑中，为我们抽象出了Map和Reduce这样足够通用的编程模型。后面的Hadoop仅仅是对于GFS、BigTable、MapReduce 的依葫芦画瓢，我这里不再赘述。</p><h3>蒸汽机时代</h3><p>到了2014年左右，Google内部已经几乎没人写新的MapReduce了。</p><p>2016年开始，Google在新员工的培训中把MapReduce替换成了内部称为FlumeJava（不要和Apache Flume混淆，是两个技术）的数据处理技术。</p><p>这标志着青铜时代的终结，同时也标志着蒸汽机时代的开始。</p><p><span class=\"orange\">我跳过“铁器时代”之类的描述，是因为只有工业革命的概念才能解释从MapReduce进化到FlumeJava的划时代意义。</span></p><p>Google内部的FlumeJava和它后来的开源版本Apache Beam所引进的统一的编程模式，将在后面的章节中为你深入解析。</p><p>现在你可能有一个疑问 ：为什么MapReduce会被取代？今天我将重点为你解答。</p><h2>高昂的维护成本</h2><p>使用MapReduce，你需要严格地遵循分步的Map和Reduce步骤。当你构造更为复杂的处理架构时，往往需要协调多个Map和多个Reduce任务。</p><p>然而，每一步的MapReduce都有可能出错。</p><p>为了这些异常处理，很多人开始设计自己的协调系统（orchestration）。例如，做一个状态机（state machine）协调多个MapReduce，这大大增加了整个系统的复杂度。</p><p>如果你搜 “MapReduce orchestration” 这样的关键词，就会发现有很多书，整整一本都在写怎样协调MapReduce。</p><p>你可能会惊讶于MapReduce的复杂度。我也经常会看到一些把MapReduce说得过度简单的误导性文章。</p><p>例如，“把海量的××数据通过MapReduce导入大数据系统学习，就能产生××人工智能”。似乎写文的“专家”动动嘴就能点石成金。</p><p>而现实的MapReduce系统的复杂度是超过了“伪专家”的认知范围的。下面我来举个例子，告诉你MapReduce有多复杂。</p><p>想象一下这个情景，你的公司要<strong>预测美团的股价</strong>，其中一个重要特征是活跃在街头的美团外卖电动车数量，而你负责<strong>处理所有美团外卖电动车的图片</strong>。</p><p>在真实的商用环境下，为了解决这个问题，你可能至少需要10个MapReduce任务：</p><p><img src=\"https://static001.geekbang.org/resource/image/44/c7/449ebd6c5950f5b7691d34d13a781ac7.jpg?wh=4075*2658\" alt=\"\"></p><p><span class=\"orange\">首先，我们需要搜集每日的外卖电动车图片。</span></p><p>数据的搜集往往不全部是公司独自完成，许多公司会选择部分外包或者众包。所以在<strong>数据搜集</strong>（Data collection）部分，你至少需要4个MapReduce任务：</p><ol>\n<li>\n<p>数据导入（data ingestion）：用来把散落的照片（比如众包公司上传到网盘的照片）下载到你的存储系统。</p>\n</li>\n<li>\n<p>数据统一化（data normalization）：用来把不同外包公司提供过来的各式各样的照片进行格式统一。</p>\n</li>\n<li>\n<p>数据压缩（compression）：你需要在质量可接受的范围内保持最小的存储资源消耗 。</p>\n</li>\n<li>\n<p>数据备份（backup）：大规模的数据处理系统我们都需要一定的数据冗余来降低风险。</p>\n</li>\n</ol><p>仅仅是做完数据搜集这一步，离真正的业务应用还差得远。</p><p>真实的世界是如此不完美，我们<span class=\"orange\">需要一部分数据质量控制（quality control）流程</span>，比如：</p><ol>\n<li>\n<p>数据时间有效性验证 （date validation）：检测上传的图片是否是你想要的日期的。</p>\n</li>\n<li>\n<p>照片对焦检测（focus detection）：你需要筛选掉那些因对焦不准而无法使用的照片。</p>\n</li>\n</ol><p>最后才到你负责的重头戏——<span class=\"orange\">找到这些图片里的外卖电动车</span>。而这一步因为人工的介入是最难控制时间的。你需要做4步：</p><ol>\n<li>\n<p>数据标注问题上传（question uploading）：上传你的标注工具，让你的标注者开始工作。</p>\n</li>\n<li>\n<p>标注结果下载（answer downloading）：抓取标注完的数据。</p>\n</li>\n<li>\n<p>标注异议整合（adjudication）：标注异议经常发生，比如一个标注者认为是美团外卖电动车，另一个标注者认为是京东快递电动车。</p>\n</li>\n<li>\n<p>标注结果结构化（structuralization）: 要让标注结果可用，你需要把可能非结构化的标注结果转化成你的存储系统接受的结构。</p>\n</li>\n</ol><p>这里我不再深入每个MapReduce任务的技术细节，因为本章的重点仅仅是理解MapReduce的复杂度。</p><p>通过这个案例，我想要阐述的观点是，因为<span class=\"orange\">真实的商业MapReduce场景极端复杂，像上面这样10个子任务的MapReduce系统在硅谷一线公司司空见惯</span>。</p><p>在应用过程中，每一个MapReduce任务都有可能出错，都需要重试和异常处理的机制。所以，协调这些子MapReduce的任务往往需要和业务逻辑紧密耦合的状态机。</p><p>这样过于复杂的维护让系统开发者苦不堪言。</p><h2>时间性能“达不到”用户的期待</h2><p>除了高昂的维护成本，MapReduce的时间性能也是个棘手的问题。</p><p>MapReduce是一套如此精巧复杂的系统，如果使用得当，它是青龙偃月刀，如果使用不当，它就是一堆废铁。不幸的是并不是每个人都是关羽。</p><p>在实际的工作中，不是每个人都对MapReduce细微的配置细节了如指掌。</p><p>在现实中，业务往往需求一个刚毕业的新手在3个月内上线一套数据处理系统，而他很可能从来没有用过MapReduce。这种情况下开发的系统是很难发挥好MapReduce的性能的。</p><p>你一定想问，MapReduce的性能优化配置究竟复杂在哪里呢？</p><p>我想Google500多页的MapReduce性能优化手册足够说明它的复杂度了。这里我举例讲讲MapReduce的分片（sharding）难题，希望能窥斑见豹，引发大家的思考。</p><p>Google曾经在2007年到2012年间做过一个对于1PB数据的大规模排序实验，来测试MapReduce的性能。</p><p>从2007年的排序时间12小时，到2012年的排序时间缩短至0.5小时。即使是Google，也花了5年的时间才不断优化了一个MapReduce流程的效率。</p><p>2011年，他们在Google Research的博客上公布了初步的成果。</p><p><img src=\"https://static001.geekbang.org/resource/image/db/6b/db4bb58536ffe3b6addd88803a77396b.jpg?wh=1331*973\" alt=\"\"></p><p>其中有一个重要的发现，就是他们在MapReduce的性能配置上花了非常多的时间。包括了缓冲大小(buffer size），分片多少（number of shards），预抓取策略（prefetch），缓存大小（cache size）等等。</p><p>所谓的分片，是指把大规模的的数据分配给不同的机器/工人，流程如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/38/b08b95244530aeb0171e3e35c9bfb638.png?wh=2917*2038\" alt=\"\"></p><p>选择一个好的分片函数（sharding function）为何格外重要？让我们来看一个例子。</p><p>假如你在处理Facebook的所有用户数据，你选择了按照用户的年龄作为分片函数（sharding function）。我们来看看这时候会发生什么。</p><p>因为用户的年龄分布不均衡（假如在20~30这个年龄段的Facebook用户最多），导致我们在下图中worker C上分配到的任务远大于别的机器上的任务量。</p><p><img src=\"https://static001.geekbang.org/resource/image/5c/91/5c719600021f738e8c7edf82197eac91.png?wh=3658*2804\" alt=\"\"></p><p>这时候就会发生掉队者问题（stragglers）。别的机器都完成了Reduce阶段，只有worker C还在工作。</p><p>当然它也有改进方法。掉队者问题可以通过MapReduce的性能剖析（profiling）发现。 如下图所示，箭头处就是掉队的机器。</p><p><img src=\"https://static001.geekbang.org/resource/image/63/ca/6399416524eb0dec1e292ea01b2294ca.png?wh=6750*1946\" alt=\"\"></p><p><span class=\"reference\">图片引用：Chen, Qi, Cheng Liu, and Zhen Xiao. “Improving MapReduce performance using smart speculative execution strategy.” IEEE Transactions on Computers 63.4 (2014): 954-967.</span></p><p>回到刚刚的Google大规模排序实验。</p><p>因为MapReduce的分片配置异常复杂，在2008年以后，Google改进了MapReduce的分片功能，引进了动态分片技术 (dynamic sharding），大大简化了使用者对于分片的手工调整。</p><p>在这之后，包括动态分片技术在内的各种崭新思想被逐渐引进，奠定了下一代大规模数据处理技术的雏型。</p><h2>小结</h2><p>这一讲中，我们分析了两个MapReduce之所以被硅谷一线公司淘汰的“致命伤”：高昂的维护成本和达不到用户期待的时间性能。</p><p>文中也提到了下一代数据处理技术雏型。这就是2008年左右在Google西雅图研发中心诞生的FlumeJava，它一举解决了上面MapReduce的短板。</p><p>另外，它还带来了一些别的优点：更好的可测试性；更好的可监控性；从1条数据到1亿条数据无缝扩展，不需要修改一行代码，等等。</p><p>在后面的章节中，我们将具体展开这几点，通过深入解析Apache Beam（FlumeJava的开源版本），揭开MapReduce继任者的神秘面纱。</p><h2>思考题</h2><p>如果你在Facebook负责处理例子中的用户数据，你会选择什么分片函数，来保证均匀分布的数据分片?</p><p>欢迎你把答案写在留言区，与我和其他同学一起探讨。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":86770,"user_name":"mgxian","can_delete":false,"product_type":"c1","uid":1014806,"ip_address":"","ucode":"7B7E77E6A83B87","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7c/16/4d1e5cc1.jpg","comment_is_top":true,"comment_ctime":1555459507,"is_pvip":false,"replies":[{"id":"31216","content":"谢谢你的答案！这个答案很新颖啊，我觉得光从年龄这个问题上来讲，你的思路是可以把20多岁变成12、22、32、42等等。希望你能在以后遇到问题时也能保持这样创新思维，也希望你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555465191,"ip_address":"","comment_id":86770,"utype":1}],"discussion_count":2,"race_medal":0,"score":"9.2233730734973993e+18","product_id":100025301,"comment_content":"把年龄倒过来比如 28 岁 变成 82 来分片","like_count":242,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447164,"discussion_content":"谢谢你的答案！这个答案很新颖啊，我觉得光从年龄这个问题上来讲，你的思路是可以把20多岁变成12、22、32、42等等。希望你能在以后遇到问题时也能保持这样创新思维，也希望你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555465191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1204919,"avatar":"https://static001.geekbang.org/account/avatar/00/12/62/b7/1fed6f02.jpg","nickname":"醇雾","note":"","ucode":"4A4C27B530FA30","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":22185,"discussion_content":"应该是属于跳跃性切片","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569587242,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86792,"user_name":"Codelife","can_delete":false,"product_type":"c1","uid":1041421,"ip_address":"","ucode":"10458683978083","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLhMtBwGqqmyhxp5uaDTvvp18iaalQj8qHv6u8rv1FQXGozfl3alPvdPHpEsTWwFPFVOoP6EeKT4bw/132","comment_is_top":true,"comment_ctime":1555461609,"is_pvip":false,"replies":[{"id":"31204","content":"谢谢你的答案！应该是一个很有经验的高级工程师了吧。使用Consistent hashing是可以很好地解决平均分配和当机器增减后重新hashing的问题。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555463803,"ip_address":"","comment_id":86792,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233723218781e+18","product_id":100025301,"comment_content":"我们最早采用的是哈希算法，后来发现增删节点泰麻烦，改为带虚拟节点的一致性哈希环开处理，稍微复杂点，但是性能还好","like_count":67,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447173,"discussion_content":"谢谢你的答案！应该是一个很有经验的高级工程师了吧。使用Consistent hashing是可以很好地解决平均分配和当机器增减后重新hashing的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555463803,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86746,"user_name":"王伟","can_delete":false,"product_type":"c1","uid":1445561,"ip_address":"","ucode":"22D0188425E839","user_header":"https://static001.geekbang.org/account/avatar/00/16/0e/b9/7866f19d.jpg","comment_is_top":true,"comment_ctime":1555455441,"is_pvip":false,"replies":[{"id":"31192","content":"你好王伟！首先感谢你的提问！<br><br>我不确定你所说的实时同步是想表达Eventual Consistency还是Strong Consistency，那我就争对两个都说说自己的愚见吧。<br><br>因为会员信息都会保存在商家库中，所以这里我假设商家库的信息可以作为source of truth。<br><br>如果你指的是Eventual Consistency的话，可以在会员更新商家库的同时将会员信息利用Pub&#47;Sub发送给会员库去更新。考虑到Pub&#47;Sub中间有可能会丢包，我们可以再建立一个定时任务每隔一段时间将全部商家库中的信息扫描一遍再更新到会员库中。当然具体的实现可以再作优化，因为商家库是按商家编号分库的，我们可以记录下哪些商家编号的表最近有更新我们就只扫描那些表，而不用扫描全局的表。<br><br>如果你指的是Strong Consistency的话，我们可以在中间再创建一个State Machine，记录是否两个库都同时更新了。在读取会员信息的时候，我们需要查询这个State Machine，只有当两个库同时都更新的时候才将会员信息返回。根据第九讲的CAP理论，这样的做法其实会牺牲掉Availability，也就是你的服务可用性。<br><br>当然具体的需求你会比我更了解，所以相信你能够从中做出设计上的取舍。也欢迎你继续留言提问，我们可以一起讨论学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555458305,"ip_address":"","comment_id":86746,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233721801441997e+18","product_id":100025301,"comment_content":"你好！我工作中遇到这样的场景：会员在我们平台注册，信息会保存在对应商家的商家库中，现在需要将商家库中的信息实时的同步到另一台服务的会员库中，商家库是按照商家编号分库，而且商家库和会员库没在同一台服务器部署。想请教一下，像这种我们如何做到实时同步？","like_count":34,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447150,"discussion_content":"你好王伟！首先感谢你的提问！\n\n我不确定你所说的实时同步是想表达Eventual Consistency还是Strong Consistency，那我就争对两个都说说自己的愚见吧。\n\n因为会员信息都会保存在商家库中，所以这里我假设商家库的信息可以作为source of truth。\n\n如果你指的是Eventual Consistency的话，可以在会员更新商家库的同时将会员信息利用Pub/Sub发送给会员库去更新。考虑到Pub/Sub中间有可能会丢包，我们可以再建立一个定时任务每隔一段时间将全部商家库中的信息扫描一遍再更新到会员库中。当然具体的实现可以再作优化，因为商家库是按商家编号分库的，我们可以记录下哪些商家编号的表最近有更新我们就只扫描那些表，而不用扫描全局的表。\n\n如果你指的是Strong Consistency的话，我们可以在中间再创建一个State Machine，记录是否两个库都同时更新了。在读取会员信息的时候，我们需要查询这个State Machine，只有当两个库同时都更新的时候才将会员信息返回。根据第九讲的CAP理论，这样的做法其实会牺牲掉Availability，也就是你的服务可用性。\n\n当然具体的需求你会比我更了解，所以相信你能够从中做出设计上的取舍。也欢迎你继续留言提问，我们可以一起讨论学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555458305,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87282,"user_name":"SpanningWings","can_delete":false,"product_type":"c1","uid":1503174,"ip_address":"","ucode":"A5E98A2626F187","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/dV2JvjoAOHOibxVqExibsBv0ib9jJ9zD8icYaDtFbicUgP0GmRbzmgujvz6pOl6drUcgdvfQXTJpOOY9OL45WrkInbA/132","comment_is_top":true,"comment_ctime":1555558364,"is_pvip":false,"replies":[{"id":"31399","content":"再次看到了你的提问，感谢！<br><br>以下纯属个人愚见。<br>无论是MapReduce的partitioning，还是GFS的chunkservers，它们的设计思想都是将文件分割成固定大小的chunks来维护，而每一个chunk都会有一个deterministic的64位唯一标识符。这种设计思想是和consistent hashing不一样的，可以称为是Central Coordinator。<br><br>而历史原因也是存在的，GFS和MapReduce是分别在2003年和2004年公开论文的，而Distributed Hash Table这种思想，也就是Consistent hashing，是在2007年Amazon发表了Dynamo: Amazon&#39;s Highly Available Key-value Store这篇论文后被大家所广泛认同的。<br><br>最后我想说的是，设计一个通用架构给所有开发者使用和根据自身应用场景所设计出来的架构，它们的侧重点会有所不同。如果只是自身业务需要并且不需要太考虑时间复杂度，那当然可以自己去实现consistent hashing，毕竟hashing后取模和consistent hashing时每次都要计算环节点的时间复杂度肯定是不一样的。<br><br>希望这对你有所帮助，如果有所收获的话也欢迎你分享给朋友，谢谢！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555571846,"ip_address":"","comment_id":87282,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233721028348006e+18","product_id":100025301,"comment_content":"还想到一个问题有关consistent hashing的。map reduce下层的GFS也没有采用consistent hashing来控制分片，这又是为什么？老师有空回答下吗？<br>","like_count":16,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447379,"discussion_content":"再次看到了你的提问，感谢！\n\n以下纯属个人愚见。\n无论是MapReduce的partitioning，还是GFS的chunkservers，它们的设计思想都是将文件分割成固定大小的chunks来维护，而每一个chunk都会有一个deterministic的64位唯一标识符。这种设计思想是和consistent hashing不一样的，可以称为是Central Coordinator。\n\n而历史原因也是存在的，GFS和MapReduce是分别在2003年和2004年公开论文的，而Distributed Hash Table这种思想，也就是Consistent hashing，是在2007年Amazon发表了Dynamo: Amazon&amp;#39;s Highly Available Key-value Store这篇论文后被大家所广泛认同的。\n\n最后我想说的是，设计一个通用架构给所有开发者使用和根据自身应用场景所设计出来的架构，它们的侧重点会有所不同。如果只是自身业务需要并且不需要太考虑时间复杂度，那当然可以自己去实现consistent hashing，毕竟hashing后取模和consistent hashing时每次都要计算环节点的时间复杂度肯定是不一样的。\n\n希望这对你有所帮助，如果有所收获的话也欢迎你分享给朋友，谢谢！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555571846,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86784,"user_name":"maye","can_delete":false,"product_type":"c1","uid":1444901,"ip_address":"","ucode":"F48E3814DF0FB4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/dHawibgghoCDFTuT4uIF3yibBxhgSuGaiagGMO8QYSBL0L2IcJ2TvggBjZattqSeeoJY5S1CmcyfuE7gkpb4P6fibg/132","comment_is_top":true,"comment_ctime":1555460740,"is_pvip":false,"replies":[{"id":"31203","content":"你好Maye，谢谢你的留言与提问！<br><br>第一问我也说说我的愚见吧。关于流处理和批处理的关系我更倾向于批处理可以算是流处理的一个子集吧。我们可以这么抽象地看，流计算所处理的都是无限数据集，而我们从中按照时间窗口抽取一小段出来的话，这一小段有边界的数据集其实也就是批处理所处理的数据集了。所以说批处理算是流处理的一个子集吧。但是现在流计算中两大问题：1）Exactly once delivery 2）message order，还没有非常完美的解决方案，但是我相信可以攻克的。所以未来趋势还是趋于统一。现在Google所推出的Apache Beam项目其实也是想解决这样一个问题，统一批处理和流处理的编程接口。更详细的内容我会在后面的章节展开讲解。<br><br>思考题你也看到了问题的本质，就是能找到趋于平均分配的分片处理方式。<br><br>欢迎你继续留言提问，一起交流学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555463493,"ip_address":"","comment_id":86784,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233720985398006e+18","product_id":100025301,"comment_content":"个人愚见：虽然MapReduce引擎存在性能和维护成本上的问题，但是由于Hive的封装使其适用性很广泛，学习成本很低，但是实际使用过程中和Spark等相比性能差太多了。不过对于计算引擎模型的理解方面，MapReduce还是一个很经典的入门模型，对于未来迁移到其他计算引擎也是有很大帮助的。<br>还有一个个人问题：不知道蔡老师对于流计算和批处理的关系是怎么看待的？流计算有可能完全取代批处理么？<br>关于思考题：问题的核心店在于Reducer Key是否倾斜，个人认为可以按照update_time之类的时间字段进行分片处理。","like_count":14,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447170,"discussion_content":"你好Maye，谢谢你的留言与提问！\n\n第一问我也说说我的愚见吧。关于流处理和批处理的关系我更倾向于批处理可以算是流处理的一个子集吧。我们可以这么抽象地看，流计算所处理的都是无限数据集，而我们从中按照时间窗口抽取一小段出来的话，这一小段有边界的数据集其实也就是批处理所处理的数据集了。所以说批处理算是流处理的一个子集吧。但是现在流计算中两大问题：1）Exactly once delivery 2）message order，还没有非常完美的解决方案，但是我相信可以攻克的。所以未来趋势还是趋于统一。现在Google所推出的Apache Beam项目其实也是想解决这样一个问题，统一批处理和流处理的编程接口。更详细的内容我会在后面的章节展开讲解。\n\n思考题你也看到了问题的本质，就是能找到趋于平均分配的分片处理方式。\n\n欢迎你继续留言提问，一起交流学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555463493,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86930,"user_name":"alexgreenbar","can_delete":false,"product_type":"c1","uid":1003655,"ip_address":"","ucode":"87ED7233E2767C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/50/87/dde718fa.jpg","comment_is_top":true,"comment_ctime":1555481219,"is_pvip":true,"replies":[{"id":"31270","content":"😄 很多问题确实很有思考价值，我也学到了很多之前没考虑到的。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555485277,"ip_address":"","comment_id":86930,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233720942448005e+18","product_id":100025301,"comment_content":"赞一个，几乎每问必答，无论是否小白问题，很务实，具备高手风范！","like_count":13,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447250,"discussion_content":"😄 很多问题确实很有思考价值，我也学到了很多之前没考虑到的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555485277,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86827,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":true,"comment_ctime":1555464655,"is_pvip":false,"replies":[{"id":"31249","content":"Round robin确实能保证均匀但是有个很大的问题是没有容错。因为在分布式处理的时候数据处理顺序是“随机”的，可能是shard 1&#47;2&#47;3也可能是 shard 1&#47;3&#47;2，如果发现shard 2所有任务挂了（机器坏了）需要重试，如果有确定的sharding function很容易找出shard 2的任务，round robin的话就无法还原shard 2任务了。当然你可以说我再搞个数据库把round robin结果保存，但那样就更复杂了。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474278,"ip_address":"","comment_id":86827,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233720770649006e+18","product_id":100025301,"comment_content":"如果不需要按某些字段做聚合分析，只是普通数据处理的话，直接用Round Robin分片即可。我想了解什么是“动态分片”技术？即使不用MR，其他大数据处理框架也需要用到“分片”，毕竟大数据的处理是“分而治之”，如何分才能分得好是关键。日常工作中经常遇到数据倾斜问题，也是由于分片不合理导致的。如果对于待处理的数据你了解到好办，知道用哪些字段作分片最合适，但如果遇到不熟悉的数据你又该如何分片？而不是等到出现数据倾斜问题的时候才发现，进行分片修改再重跑呢？谢谢老师指教！","like_count":9,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447198,"discussion_content":"Round robin确实能保证均匀但是有个很大的问题是没有容错。因为在分布式处理的时候数据处理顺序是“随机”的，可能是shard 1/2/3也可能是 shard 1/3/2，如果发现shard 2所有任务挂了（机器坏了）需要重试，如果有确定的sharding function很容易找出shard 2的任务，round robin的话就无法还原shard 2任务了。当然你可以说我再搞个数据库把round robin结果保存，但那样就更复杂了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474278,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86747,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":true,"comment_ctime":1555455575,"is_pvip":false,"replies":[{"id":"31214","content":"谢谢你的答案！这个答案不错。不过取余运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取余运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464842,"ip_address":"","comment_id":86747,"utype":1}],"discussion_count":1,"race_medal":0,"score":"9.2233720641800008e+18","product_id":100025301,"comment_content":"一般用户信息表都存在一个id，有的是递增的数字id，有的是类似uuid随机字符串，对于递增的直接对机器数量取余，如果是字符串通过比较均衡的hash函数操作后再对机器数量取余即可。","like_count":7,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447151,"discussion_content":"谢谢你的答案！这个答案不错。不过取余运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取余运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464842,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87064,"user_name":"木卫六","can_delete":false,"product_type":"c1","uid":1199495,"ip_address":"","ucode":"D113DF578C5BF5","user_header":"https://static001.geekbang.org/account/avatar/00/12/4d/87/57236a2d.jpg","comment_is_top":false,"comment_ctime":1555506468,"is_pvip":true,"replies":[{"id":"31338","content":"谢谢你的答案！你在每个答案里都分别给出这个答案所存在的不足，这一点我是非常赞赏的。在开发设计中没有哪个答案是特别完美的，我们能做的是分析哪一个才是最符合自身应用需求，进而改善。<br><br>1. 是的，倒置年龄的digit可以改善均分的问题，但是也存在hot spot的问题。<br>2. 我在其它的留言也回复过，随机分区的话还有一个缺点是当分区任务失败需要重新分区的时候，分区结果不再是deterministic的。<br>3. 总结得不错。<br><br>欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555526515,"ip_address":"","comment_id":87064,"utype":1}],"discussion_count":1,"race_medal":0,"score":"61685048612","product_id":100025301,"comment_content":"年龄是值域在0-120（假定）之间的数值，难以分片的原因正是因为年龄的十位数权重过大，所以我觉得一切有效降低十位数权重的哈希算法应该都是可行的。<br>1.对于年龄ABC，比如倒置CBA，或(C*大质数1+B*较小质数+C)%numPartitions，这类方法应该可以明显改善分布不均，但是对某些单一热点无解，比如25岁用户特别多；<br>2.随机分区，可做到很好均衡，对combine，io等优化不友好<br>3. 先采样+动态合并和拆分，实现过于复杂，效果可能不稳定<br><br>这是我的想法，请老师指正。","like_count":14,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447298,"discussion_content":"谢谢你的答案！你在每个答案里都分别给出这个答案所存在的不足，这一点我是非常赞赏的。在开发设计中没有哪个答案是特别完美的，我们能做的是分析哪一个才是最符合自身应用需求，进而改善。\n\n1. 是的，倒置年龄的digit可以改善均分的问题，但是也存在hot spot的问题。\n2. 我在其它的留言也回复过，随机分区的话还有一个缺点是当分区任务失败需要重新分区的时候，分区结果不再是deterministic的。\n3. 总结得不错。\n\n欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555526515,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86895,"user_name":"TKbook","can_delete":false,"product_type":"c1","uid":1073829,"ip_address":"","ucode":"F6E0E99CC79059","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg","comment_is_top":false,"comment_ctime":1555471779,"is_pvip":false,"replies":[{"id":"31238","content":"哈哈有收获就好","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555472075,"ip_address":"","comment_id":86895,"utype":1}],"discussion_count":1,"race_medal":0,"score":"44505144739","product_id":100025301,"comment_content":"在评论在看到Consistent hashing，特地去搜索看了下，终于明白了。评论干货很多。。","like_count":10,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447234,"discussion_content":"哈哈有收获就好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555472075,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86829,"user_name":"JensonYao","can_delete":false,"product_type":"c1","uid":1499730,"ip_address":"","ucode":"D4A9765F1112BD","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erQ5LXNgaZ3ReArPrY4YeT5mNVtBpiazFEQzNuUXxzdLOWtMliaGicNCpjaOezRISARHXPibkA4ACgib1g/132","comment_is_top":false,"comment_ctime":1555464755,"is_pvip":false,"replies":[{"id":"31259","content":"线下做了研究了很好啊。这三个看起来都可以吧。一般场景我觉得可以选择复杂度低的第一种，后面的对于普通场景可能都有点overkill。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555475284,"ip_address":"","comment_id":86829,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35915203123","product_id":100025301,"comment_content":"MapReduce是从纷繁复杂的业务逻辑中，为我们抽象出了 Map 和 Reduce这样足够通用的编程模型。<br>缺点：<br>1、复杂度高<br>\t当你构造更为复杂的处理架构时，往往进行任务划分，而且每一步都可能出错。而且往往比认为的复杂的多。<br>2、时间性能达不到用户要求<br>\tGoogle500 多页的 MapReduce 性能优化手册<br>\t1PB的排序从12小时优化到0.5小时花了5年<br><br>思考题：如果你在 Facebook 负责处理例子中的用户数据，你会选择什么分片函数，来保证均匀分布的数据分片?<br>由于没有过相关的经验，从网上查了下资料，常见的数据分片有1、hash 2、consistent hash without virtual node 3、consistent hash with virtual node 4、range based<br>文章中使用的方法就是range based方法，缺点在于区间大小固定，但是数据量不确定，所以会导致不均匀。<br>其他三种方法都可以保证均匀分布的数据分片，但是节点增删导致的数据迁移成本不同。<br>1、hash函数节点增删时，可能需要调整散列函数函数，导致大量的数据迁移<br>　　consistent hash是将数据按照特征值映射到一个首尾相接的hash环上，同时也将节点映射到这个环上。对于数据，从数据在环上的位置开始，顺时针找到的第一个节点即为数据的存储节点<br>2、consistent hash without virtual node 增删的时候只会影响到hash环上响应的节点，不会发生大规模的数据迁移。但是，在增加节点的时候，只能分摊一个已存在节点的压力；同样，在其中一个节点挂掉的时候，该节点的压力也会被全部转移到下一个节点<br>3、consistent hash with virtual node 在实际工程中，一般会引入虚拟节点（virtual node）的概念。即不是将物理节点映射在hash换上，而是将虚拟节点映射到hash环上。虚拟节点的数目远大于物理节点，因此一个物理节点需要负责多个虚拟节点的真实存储。操作数据的时候，先通过hash环找到对应的虚拟节点，再通过虚拟节点与物理节点的映射关系找到对应的物理节点。引入虚拟节点后的一致性hash需要维护的元数据也会增加：第一，虚拟节点在hash环上的问题，且虚拟节点的数目又比较多；第二，虚拟节点与物理节点的映射关系。但带来的好处是明显的，当一个物理节点失效是，hash环上多个虚拟节点失效，对应的压力也就会发散到多个其余的虚拟节点，事实上也就是多个其余的物理节点。在增加物理节点的时候同样如此。<br>引用blog：http:&#47;&#47;www.cnblogs.com&#47;xybaby&#47;p&#47;7076731.html<br><br>所以这样看具体采用何种方式要结合其他的因素（显示场景，成本？），如何抉择我也不是很清楚。","like_count":8,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447199,"discussion_content":"线下做了研究了很好啊。这三个看起来都可以吧。一般场景我觉得可以选择复杂度低的第一种，后面的对于普通场景可能都有点overkill。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555475284,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87084,"user_name":"牛冠群","can_delete":false,"product_type":"c1","uid":1505449,"ip_address":"","ucode":"2FB7D82AD2C9A9","user_header":"https://static001.geekbang.org/account/avatar/00/16/f8/a9/93e418f5.jpg","comment_is_top":false,"comment_ctime":1555509942,"is_pvip":false,"replies":[{"id":"31342","content":"看到“30天速成”字样的资料可以直接扔掉","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555532393,"ip_address":"","comment_id":87084,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27325313718","product_id":100025301,"comment_content":"您好，学习周期有点长，能不能加快些进度。感谢！","like_count":6,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447304,"discussion_content":"看到“30天速成”字样的资料可以直接扔掉","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555532393,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87048,"user_name":"monkeyking","can_delete":false,"product_type":"c1","uid":1196221,"ip_address":"","ucode":"4AF7ECF79C4379","user_header":"https://static001.geekbang.org/account/avatar/00/12/40/bd/acb9d02a.jpg","comment_is_top":false,"comment_ctime":1555503395,"is_pvip":false,"replies":[{"id":"31327","content":"是对的思路！随机前缀这个我在另一个回复上也提到了，“真”随机会影响错误重试，因为没法还原当时的随机数，比如分片2的任务全部失败，找不到哪些是分片2了。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523011,"ip_address":"","comment_id":87048,"utype":1}],"discussion_count":2,"race_medal":0,"score":"27325307171","product_id":100025301,"comment_content":"按照user_id哈希或者给user_id加一个随机数前缀","like_count":6,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447294,"discussion_content":"是对的思路！随机前缀这个我在另一个回复上也提到了，“真”随机会影响错误重试，因为没法还原当时的随机数，比如分片2的任务全部失败，找不到哪些是分片2了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523011,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":198849,"discussion_content":"所以可以用伪随机，比如用数据另一个field的值hash过后取几位作为前缀或后缀，取几位取决于如何平衡partition balance和data colocation这两种优化策略","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583520771,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87033,"user_name":"张德","can_delete":false,"product_type":"c1","uid":1101929,"ip_address":"","ucode":"31FE63E8725EFC","user_header":"https://static001.geekbang.org/account/avatar/00/10/d0/69/5dbdc245.jpg","comment_is_top":false,"comment_ctime":1555500265,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23030336745","product_id":100025301,"comment_content":"给作者一百五十个赞👍","like_count":5},{"had_liked":false,"id":86946,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1555485985,"is_pvip":false,"replies":[{"id":"31331","content":"是的，我觉得你总结的很好！","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523558,"ip_address":"","comment_id":86946,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23030322465","product_id":100025301,"comment_content":"我们公司现在还在使用hadoop streaming 的MapReduce，默认mapper 结果是按key sort 过得，在reducer 中借此实现join和group by的复杂操作，经常为了Join 一个table就要多写四个job","like_count":5,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447257,"discussion_content":"是的，我觉得你总结的很好！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523558,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86923,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1555479721,"is_pvip":false,"replies":[{"id":"31337","content":"谢谢你的留言！我认同你的观点，MR确实在每一步都需要经过磁盘的数据读取和结果的写入，速度上也就比不上Spark了。希望后面能继续看见你留言，一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555525661,"ip_address":"","comment_id":86923,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23030316201","product_id":100025301,"comment_content":"MR的劣势刚好对应了Spark的优势<br><br>1. 通过DAG RDD进行数据链式处理，最终只有一个job，大大降低了大数量MR的维护成本<br>2. 优先基于内存计算的Spark相对于基于磁盘计算的MR也大幅度提高了计算性能，缩短计算时间<br><br>个人觉得，这两点可以作为MR和Spark的主要区别。","like_count":5,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447247,"discussion_content":"谢谢你的留言！我认同你的观点，MR确实在每一步都需要经过磁盘的数据读取和结果的写入，速度上也就比不上Spark了。希望后面能继续看见你留言，一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555525661,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89081,"user_name":"王昊","can_delete":false,"product_type":"c1","uid":1071985,"ip_address":"","ucode":"1F2D3857AA54C4","user_header":"https://static001.geekbang.org/account/avatar/00/10/5b/71/140a10e8.jpg","comment_is_top":false,"comment_ctime":1556073015,"is_pvip":false,"replies":[{"id":"32092","content":"谢谢你的提问！任何“高级”的技术都离不开底层的支持，而万变不离其宗，底层思想其实在各个数据处理平台中都是相通的。我觉得如果是刚开始学习的话是需要把基本的思想先学明白，例如可以读一读MapReduce的论文：&quot;MapReduce: Simplified Data Processing on Large Clusters&quot;。而后自己的动手实践当然也是少不了的。如果还有时间的话，可以听一听工业界上大公司的技术演讲，看看在实际应用场景中它们是如何搭建自己的技术栈的。希望这些对你有所帮助。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556224226,"ip_address":"","comment_id":89081,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735942199","product_id":100025301,"comment_content":"老师您好，我现在正在读研究生(专业计算机技术)，喜欢数据科学，编写过简单的MR，谢谢您高屋建瓴地述说其历史发展，想请教您比如很多学生应该怎么学习，如何成长为优秀的工程师，是应该从头从MR学起还是直接学高级的技术？","like_count":4,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448008,"discussion_content":"谢谢你的提问！任何“高级”的技术都离不开底层的支持，而万变不离其宗，底层思想其实在各个数据处理平台中都是相通的。我觉得如果是刚开始学习的话是需要把基本的思想先学明白，例如可以读一读MapReduce的论文：&amp;quot;MapReduce: Simplified Data Processing on Large Clusters&amp;quot;。而后自己的动手实践当然也是少不了的。如果还有时间的话，可以听一听工业界上大公司的技术演讲，看看在实际应用场景中它们是如何搭建自己的技术栈的。希望这些对你有所帮助。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556224226,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86945,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1555485857,"is_pvip":false,"replies":[{"id":"31330","content":"看来你很有经验！确实是很经常出现的问题","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523499,"ip_address":"","comment_id":86945,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735355041","product_id":100025301,"comment_content":"现在写MapReduce job 开几百个worker经常有1，2个卡着不结束，基本都要在下班前赶着启动耗时长的任务。 我们分片用户是用的 country+username 的 hash，还是比较均匀的","like_count":4,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447256,"discussion_content":"看来你很有经验！确实是很经常出现的问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523499,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86929,"user_name":"mjl","can_delete":false,"product_type":"c1","uid":1044391,"ip_address":"","ucode":"B84138FF626850","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ef/a7/6e8f3636.jpg","comment_is_top":false,"comment_ctime":1555480737,"is_pvip":false,"replies":[{"id":"31333","content":"这个思路看起来是做了很多课后研究了！希望后面也能继续参与讨论！","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523756,"ip_address":"","comment_id":86929,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735349921","product_id":100025301,"comment_content":"个人理解，对于已知数据分布情况的数据，我们大多数情况下能找到合适的一个分区策略对数据进行分片。但实际上这对于数据开发者来说，就需要知道整体数据的一个基本情况。而对于数据倾斜，基本分为分区策略不当导致的倾斜以及单热点key的倾斜，对于后者，无论用户设置什么分区策略，都无法对数据进行分割。<br>对于数据倾斜问题的话，spark 3.0版本计划合入的AE功能给出了一定的方案。对于倾斜的partition，在shuffleWrite阶段就可以统计每个map输出的各个分区信息，然后根据这些信息来调整下一个stage的并发度。进一步的话，对于两表join，一张表有存在热点key的话，可以广播另外一张表的该partition，最终与其他分区的join结果做union。基于这个思路的话，engine其实是能很灵活的处理数据倾斜类问题，而不用用户去花精力研究选择。","like_count":4,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447249,"discussion_content":"这个思路看起来是做了很多课后研究了！希望后面也能继续参与讨论！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523756,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86768,"user_name":"旭东(Frank)","can_delete":false,"product_type":"c1","uid":1024486,"ip_address":"","ucode":"176FA629800062","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a1/e6/50da1b2d.jpg","comment_is_top":false,"comment_ctime":1555459213,"is_pvip":false,"replies":[{"id":"31256","content":"这个点不错，可以先分析一下数据分布","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474964,"ip_address":"","comment_id":86768,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735328397","product_id":100025301,"comment_content":"区分热点数据（占用）大的数据，以及关联性大的数据。应该有个数据抽样分析，再做具体分片策略","like_count":4,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447163,"discussion_content":"这个点不错，可以先分析一下数据分布","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474964,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87350,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1555574466,"is_pvip":false,"replies":[{"id":"31467","content":"再次看到了你的留言，谢谢！<br>就像我之前的回答一样，Beam的诞生更多是想抽象出一个统一的编程模型来处理批处理和流处理，使不同的平台相互兼容，让开发者有能力在不同的平台中转移。无论是Spark还是Flink，它们都可以选择根据Dataflow Model来编写自己的底层实现。<br><br>关于调优的话Beam有根据不同平台来编写专门的API去编写配置。当然了，因为需要统一编程接口，你对底层的控制就没有原生Spark或者Flink那么好。<br><br>关于是否采用Beam的问题，历史因素也占了很大比重。很多时候进行平台的转移可能对开发者来说是一个overkill。就像我之前所说，现在越来越多的平台开始采用Dataflow Model来编写自己的底层实现。所以理论上，在未来开发者或者公司就可以不必过于担心转移数据处理平台时的迁移成本了<br><br>希望对你有帮助，如果有收获也请分享给朋友！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555627228,"ip_address":"","comment_id":87350,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440476354","product_id":100025301,"comment_content":"如果我读Beam的文档没有理解错的话，Beam只是spark 和flink 的各种API的一个封装，本身并没有runner，该有的调优还得在spark 和flink上面做，所以除了google以外的公司用Beam的还是蛮少的","like_count":3,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447410,"discussion_content":"再次看到了你的留言，谢谢！\n就像我之前的回答一样，Beam的诞生更多是想抽象出一个统一的编程模型来处理批处理和流处理，使不同的平台相互兼容，让开发者有能力在不同的平台中转移。无论是Spark还是Flink，它们都可以选择根据Dataflow Model来编写自己的底层实现。\n\n关于调优的话Beam有根据不同平台来编写专门的API去编写配置。当然了，因为需要统一编程接口，你对底层的控制就没有原生Spark或者Flink那么好。\n\n关于是否采用Beam的问题，历史因素也占了很大比重。很多时候进行平台的转移可能对开发者来说是一个overkill。就像我之前所说，现在越来越多的平台开始采用Dataflow Model来编写自己的底层实现。所以理论上，在未来开发者或者公司就可以不必过于担心转移数据处理平台时的迁移成本了\n\n希望对你有帮助，如果有收获也请分享给朋友！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555627228,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87089,"user_name":"渡码","can_delete":false,"product_type":"c1","uid":1348536,"ip_address":"","ucode":"8FD8B863D1DA0C","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/b8/6510592e.jpg","comment_is_top":false,"comment_ctime":1555510056,"is_pvip":false,"replies":[{"id":"31322","content":"第一个问题hive和MR是apple and orange，不太好对比吧。hive更类似于SQL。<br><br>第二个问题恰恰是这个专栏的重点，会教会大家怎么解析框架的设计思路。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555522242,"ip_address":"","comment_id":87089,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440411944","product_id":100025301,"comment_content":"认同您说的MR的局限性，因此建立在MR之上的hive有用武之地。面对不断出现的新框架我们怎么快速掌握它的设计，尤其是即便看文档也会觉得模棱两可，这时候有必要深入到源码中吗，您在这后续课程中有没有相关的经验分享","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447306,"discussion_content":"第一个问题hive和MR是apple and orange，不太好对比吧。hive更类似于SQL。\n\n第二个问题恰恰是这个专栏的重点，会教会大家怎么解析框架的设计思路。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555522242,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86950,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1555486249,"is_pvip":false,"replies":[{"id":"31351","content":"你的问题都很实际啊。job和job的互相抢占并不是spark独有的问题，都需要一些优先级系统，哪些job优先级高。还有异常处理，错误重试，任何数据处理系统都需要解决。这也是为什么这篇文章里提到多任务状态机，很多时候为了异常处理免不了这些系统以至于增加了复杂度。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555545033,"ip_address":"","comment_id":86950,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440388137","product_id":100025301,"comment_content":"我们组还在用Hadoop Streaming而没有使用spark的原因是spark 内存使用不加节制，经常新起的job把周期性job的内存吃光，导致他们有时会挂掉，不知道这个问题是否有很好的解决方法。我们还是在保守地使用hadoop streaming，麻烦得很","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447259,"discussion_content":"你的问题都很实际啊。job和job的互相抢占并不是spark独有的问题，都需要一些优先级系统，哪些job优先级高。还有异常处理，错误重试，任何数据处理系统都需要解决。这也是为什么这篇文章里提到多任务状态机，很多时候为了异常处理免不了这些系统以至于增加了复杂度。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555545033,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86913,"user_name":"楚翔style","can_delete":false,"product_type":"c1","uid":1174846,"ip_address":"","ucode":"E715F82C34A9AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/ed/3e/c1725237.jpg","comment_is_top":false,"comment_ctime":1555476225,"is_pvip":false,"replies":[{"id":"31334","content":"“离线数据“你指的是批处理？怎么定义“稳定”？","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523819,"ip_address":"","comment_id":86913,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440378113","product_id":100025301,"comment_content":"mapreduce更适合处理离线数据吧，而且数据量大了比spark要稳定。楠兄怎么看？","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447242,"discussion_content":"“离线数据“你指的是批处理？怎么定义“稳定”？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523819,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86780,"user_name":"veio007","can_delete":false,"product_type":"c1","uid":1034026,"ip_address":"","ucode":"9B364EDC1C04C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c7/2a/02eea2ba.jpg","comment_is_top":false,"comment_ctime":1555460475,"is_pvip":false,"replies":[{"id":"31257","content":"这个可能和round robin那位同学思路差不多，也是合理的方法。重复一下在那边我的看法，这个需要对一个master节点来同步，而且出错了难以还原比如work 2坏了所有任务重试，找不到当时的任务了。也会增加系统复杂度。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555475093,"ip_address":"","comment_id":86780,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440362363","product_id":100025301,"comment_content":"如果是我，外面弄个自增的计数器，然后每次计数器的当前值对worker个数取模，但是就算每个人分配同等数量的数据，同样会出现有人快有人慢的情况，机器的性能处理能力不一样或者其他干扰项都可能会有影响","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447169,"discussion_content":"这个可能和round robin那位同学思路差不多，也是合理的方法。重复一下在那边我的看法，这个需要对一个master节点来同步，而且出错了难以还原比如work 2坏了所有任务重试，找不到当时的任务了。也会增加系统复杂度。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555475093,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86735,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1555439538,"is_pvip":false,"replies":[{"id":"31262","content":"如果还没开始学，就可以直接开始学我们这里介绍的apache beam吧。如果已经开始学了，肯定也是有收获的，学习永远没有最好的“时机”，因为技术永远在发展更新啊。就像买不到最时髦的衣服一样。还是如第一篇虽说，看一个技术要看到它怎么解决问题，学习他的思路。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555475733,"ip_address":"","comment_id":86735,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440341426","product_id":100025301,"comment_content":"那现在还在用MapReduce的大数据软件怎么搞了？也会被慢慢淘汰？还需要学习吗？","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447144,"discussion_content":"如果还没开始学，就可以直接开始学我们这里介绍的apache beam吧。如果已经开始学了，肯定也是有收获的，学习永远没有最好的“时机”，因为技术永远在发展更新啊。就像买不到最时髦的衣服一样。还是如第一篇虽说，看一个技术要看到它怎么解决问题，学习他的思路。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555475733,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86731,"user_name":"呆小木","can_delete":false,"product_type":"c1","uid":1123343,"ip_address":"","ucode":"B06CAC484C00DB","user_header":"https://static001.geekbang.org/account/avatar/00/11/24/0f/196fa05b.jpg","comment_is_top":false,"comment_ctime":1555438319,"is_pvip":false,"replies":[{"id":"31252","content":"是正确的思路，似乎不需要先哈希，可以直接对用户id取模。随机字符串那步骤似乎有点过度复杂化了","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474702,"ip_address":"","comment_id":86731,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440340207","product_id":100025301,"comment_content":"对用户id取哈希值，然后用哈希值对分区数取模。由于不同的id还是有可能计算得到相同的哈希值，也就是所谓的哈希碰撞，从而产生数据倾斜，可以在Map端给id拼上一个随机字符串，让它计算得到的哈希值分配更均匀，到Reduce端再去掉随机串。请老师指点😜ོ","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447141,"discussion_content":"是正确的思路，似乎不需要先哈希，可以直接对用户id取模。随机字符串那步骤似乎有点过度复杂化了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474702,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87172,"user_name":"Li221","can_delete":false,"product_type":"c1","uid":1503168,"ip_address":"","ucode":"8BAC4A93A4AE3A","user_header":"https://static001.geekbang.org/account/avatar/00/16/ef/c0/8adceaed.jpg","comment_is_top":false,"comment_ctime":1555534105,"is_pvip":false,"replies":[{"id":"31349","content":"谢谢！大家的问题确实都很有意义。希望后面也能看到你的交流讨论","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555544591,"ip_address":"","comment_id":87172,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145468697","product_id":100025301,"comment_content":"给元楠老师100个赞，每问必答，是所有专栏里最敬业的老师了！","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447333,"discussion_content":"谢谢！大家的问题确实都很有意义。希望后面也能看到你的交流讨论","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555544591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87016,"user_name":"买老实","can_delete":false,"product_type":"c1","uid":1199842,"ip_address":"","ucode":"A8FB87A4E3EEDB","user_header":"https://static001.geekbang.org/account/avatar/00/12/4e/e2/03371799.jpg","comment_is_top":false,"comment_ctime":1555497055,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10145431647","product_id":100025301,"comment_content":"都是数学的均衡排列的 好像都是数学方法的抽象","like_count":2},{"had_liked":false,"id":86970,"user_name":"有铭","can_delete":false,"product_type":"c1","uid":1046302,"ip_address":"","ucode":"2C7CB36CA5C04C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/3XbCueYYVWTiclv8T5tFpwiblOxLphvSZxL4ujMdqVMibZnOiaFK2C5nKRGv407iaAsrI0CDICYVQJtiaITzkjfjbvrQ/132","comment_is_top":false,"comment_ctime":1555489234,"is_pvip":false,"replies":[{"id":"31335","content":"谢谢你的认同！也希望后面能继续看到你的留言。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555525180,"ip_address":"","comment_id":86970,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145423826","product_id":100025301,"comment_content":"今天这文章将MapReduce复杂度问题让我豁然开朗，以前我总觉得很多讲MapReduce的文章，讲的轻描淡写的，颇有点“把冰箱门打开，把大象装进去，把冰箱门关上”的味道。我当初就觉得可能MapReduce没那么简单。","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447264,"discussion_content":"谢谢你的认同！也希望后面能继续看到你的留言。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555525180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86921,"user_name":"M","can_delete":false,"product_type":"c1","uid":1117633,"ip_address":"","ucode":"88F6F1433A01A3","user_header":"https://static001.geekbang.org/account/avatar/00/11/0d/c1/d36816df.jpg","comment_is_top":false,"comment_ctime":1555478172,"is_pvip":false,"replies":[{"id":"31350","content":"shuffle时，但是前一个mr的output也会成为下一个mr的input。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555544684,"ip_address":"","comment_id":86921,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145412764","product_id":100025301,"comment_content":"请问文中所说的分片是指InputFormat读取数据时还是指shuffle时的分发？","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447246,"discussion_content":"shuffle时，但是前一个mr的output也会成为下一个mr的input。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555544684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86810,"user_name":"JohnT3e","can_delete":false,"product_type":"c1","uid":1063982,"ip_address":"","ucode":"CF4AAAC933529C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132","comment_is_top":false,"comment_ctime":1555463404,"is_pvip":false,"replies":[{"id":"31210","content":"谢谢你的答案！这个答案不错。相信你看到了问题的本质，希望能尽可能地让分片平均分布。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464631,"ip_address":"","comment_id":86810,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145397996","product_id":100025301,"comment_content":"如果处理逻辑和用户年龄相关，那么可以在原始年龄上加随机值的方式，尽可能散列。如果需要的话，后期再对结果进行合并。如果处理逻辑和年龄无关，那么可以选择散列性好的字段，比如用户唯一标识。在sharding时hash生成方面可以选取一些散列性好的算法，比如murmurhash算法。","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447186,"discussion_content":"谢谢你的答案！这个答案不错。相信你看到了问题的本质，希望能尽可能地让分片平均分布。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464631,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86716,"user_name":"西西弗与卡夫卡","can_delete":false,"product_type":"c1","uid":1001710,"ip_address":"","ucode":"B4C27B8335B76A","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/ee/872ad07e.jpg","comment_is_top":false,"comment_ctime":1555431794,"is_pvip":true,"replies":[{"id":"31251","content":"再加上取模的话的确是mapreduce默认的配置，但是要注意根据shard数量动态调整模的数量。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474563,"ip_address":"","comment_id":86716,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145366386","product_id":100025301,"comment_content":"按用户在数据库中的唯一id","like_count":2,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447135,"discussion_content":"再加上取模的话的确是mapreduce默认的配置，但是要注意根据shard数量动态调整模的数量。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474563,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":97300,"user_name":"Bin滨","can_delete":false,"product_type":"c1","uid":1298294,"ip_address":"","ucode":"7F113EF3AF81E9","user_header":"https://static001.geekbang.org/account/avatar/00/13/cf/76/9e413c61.jpg","comment_is_top":false,"comment_ctime":1558642752,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5853610048","product_id":100025301,"comment_content":"关于王伟提到的问题 “商家库中的信息实时的同步到另一台服务的会员库”可以用CDC把write ahead logging 写到kafka topic 上 来做数据同步。redhat开源的项目debezium你可以看看。","like_count":1},{"had_liked":false,"id":90518,"user_name":"D","can_delete":false,"product_type":"c1","uid":1493885,"ip_address":"","ucode":"02D70D1317B1B6","user_header":"https://static001.geekbang.org/account/avatar/00/16/cb/7d/2a65ded5.jpg","comment_is_top":false,"comment_ctime":1556545231,"is_pvip":false,"replies":[{"id":"32996","content":"谢谢你的留言！你说的是kudu吗？这一块我不是特别了解。不过我想说的是，选择一个技术栈没有绝对的好与不好，你会比我更加了解自身的应用场景、需求和开发成本，根据自身的需要去选择一个搭配就好了。我看网上很多关于Apache impala + kudu的搭配，应该是个不错的选择。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1557218015,"ip_address":"","comment_id":90518,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5851512527","product_id":100025301,"comment_content":"老师，kudo怎么样，适合用来存储数据？我们现在是用impala作为计算引擎，kudo用来存储数据","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448553,"discussion_content":"谢谢你的留言！你说的是kudu吗？这一块我不是特别了解。不过我想说的是，选择一个技术栈没有绝对的好与不好，你会比我更加了解自身的应用场景、需求和开发成本，根据自身的需要去选择一个搭配就好了。我看网上很多关于Apache impala + kudu的搭配，应该是个不错的选择。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557218015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88874,"user_name":"Mr.韩先生","can_delete":false,"product_type":"c1","uid":1503727,"ip_address":"","ucode":"86AECF53AB91AD","user_header":"https://static001.geekbang.org/account/avatar/00/16/f1/ef/dcc3b4c1.jpg","comment_is_top":false,"comment_ctime":1556018292,"is_pvip":false,"replies":[{"id":"32094","content":"谢谢你的留言！哈希完之后取余分片是很常规的做法，可以的。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556224545,"ip_address":"","comment_id":88874,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850985588","product_id":100025301,"comment_content":"个人愚见，也不知道是不是说到点上，只是把自己的想法说出来，还请老师点评。<br>个人理解:发生上诉问题，主要原因是数据倾斜，首先可以预见的，FB根据产品定位，若按年龄分区，一定会造成数据倾斜。是否可以取年龄的哈希值对分片数量取余，余数相同分为一区。以上为个人愚见，还望老师点评。","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447932,"discussion_content":"谢谢你的留言！哈希完之后取余分片是很常规的做法，可以的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556224545,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87279,"user_name":"Geek_daeead","can_delete":false,"product_type":"c1","uid":1502731,"ip_address":"","ucode":"31DEC14EF47F83","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJuMT3ZicYDx7RrWku2oJsiblADvCkw9UxR42yqFBNU7kb961jUAnOgkrljgf1zkFRME80Txo0oKeRA/132","comment_is_top":false,"comment_ctime":1555557805,"is_pvip":false,"replies":[{"id":"31393","content":"似乎并没有提到python为主力语言，我们示例用python是因为简单方便大部分人看懂。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555566918,"ip_address":"","comment_id":87279,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850525101","product_id":100025301,"comment_content":"请问老师，python作为主力语言的话，学习升级的路线是怎样的呢，有没有推荐的路线呢？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447377,"discussion_content":"似乎并没有提到python为主力语言，我们示例用python是因为简单方便大部分人看懂。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555566918,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87270,"user_name":"毕洪宇","can_delete":false,"product_type":"c1","uid":1004607,"ip_address":"","ucode":"C712FE798C715B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/3f/a3942e77.jpg","comment_is_top":false,"comment_ctime":1555555725,"is_pvip":false,"replies":[{"id":"31401","content":"的确是很有价值的问题。会在后面探讨。<br><br>感谢提问，觉得有收获欢迎分享给朋友","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555572317,"ip_address":"","comment_id":87270,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850523021","product_id":100025301,"comment_content":"请问后面会分享liquid shard是怎么做的吗？比如如何解决data consistency，how to compute proper split size ；我觉得对于解决一般的straggler 问题非常具有启发性，多谢","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447374,"discussion_content":"的确是很有价值的问题。会在后面探讨。\n\n感谢提问，觉得有收获欢迎分享给朋友","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555572317,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87244,"user_name":"Hunter Liu","can_delete":false,"product_type":"c1","uid":1474463,"ip_address":"","ucode":"BD1FD203A295DC","user_header":"https://static001.geekbang.org/account/avatar/00/16/7f/9f/ca7b20cd.jpg","comment_is_top":false,"comment_ctime":1555552604,"is_pvip":false,"replies":[{"id":"31400","content":"问题很好。并不能减少业务的步骤。但是我这里的点是想说MR每部分开，多步骤管理比较困难。<br><br>如果觉得有收获，欢迎把文章分享给朋友","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555572180,"ip_address":"","comment_id":87244,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850519900","product_id":100025301,"comment_content":"老师好，零基础小白提问，文中的美团案例，如果不用MR，其他方案能够减少处理流程吗，减少的是哪几步，效率上会提高多少呢？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447364,"discussion_content":"问题很好。并不能减少业务的步骤。但是我这里的点是想说MR每部分开，多步骤管理比较困难。\n\n如果觉得有收获，欢迎把文章分享给朋友","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555572180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87225,"user_name":"桂桂","can_delete":false,"product_type":"c1","uid":1146833,"ip_address":"","ucode":"5291C55AFE1066","user_header":"https://static001.geekbang.org/account/avatar/00/11/7f/d1/d9de3d0a.jpg","comment_is_top":false,"comment_ctime":1555550944,"is_pvip":false,"replies":[{"id":"31394","content":"恕我直言这方面还是市场空白。不同应用场景需要的标注功能挺不一样，比如我做的医疗AI领域对于标注者专业知识要求高，也需要异议整合等功能，还需要多模态同时标注。我觉得你在评估工具好不好时，可以从两个方面看，一个是是否方便拓展，比如今天标注这些问题，明天那些问题模态不一样能否很快设置。另一个就是功能是否满足你们场景需要了。<br><br>感谢提问，有收获欢迎分享。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555567155,"ip_address":"","comment_id":87225,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850518240","product_id":100025301,"comment_content":"请问，常用标注工具有吗，能推荐吗","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447352,"discussion_content":"恕我直言这方面还是市场空白。不同应用场景需要的标注功能挺不一样，比如我做的医疗AI领域对于标注者专业知识要求高，也需要异议整合等功能，还需要多模态同时标注。我觉得你在评估工具好不好时，可以从两个方面看，一个是是否方便拓展，比如今天标注这些问题，明天那些问题模态不一样能否很快设置。另一个就是功能是否满足你们场景需要了。\n\n感谢提问，有收获欢迎分享。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555567155,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87179,"user_name":"大王叫我来巡山","can_delete":false,"product_type":"c1","uid":1099513,"ip_address":"","ucode":"1B8D0C701BC95E","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/f9/caf27bd3.jpg","comment_is_top":false,"comment_ctime":1555545475,"is_pvip":false,"replies":[{"id":"31395","content":"感谢肯定，有收获欢迎分享","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555567187,"ip_address":"","comment_id":87179,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850512771","product_id":100025301,"comment_content":"收益匪浅，写书的一定要为自己负责，看过太多垃圾书，并且是书带你进坑，其实作者的功力都不够，在描述某些问题的处理的时候能看出来作者自己都没有处理过，卖得好的书不一定是好书","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447336,"discussion_content":"感谢肯定，有收获欢迎分享","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555567187,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87085,"user_name":"hallo128","can_delete":false,"product_type":"c1","uid":1212044,"ip_address":"","ucode":"3921D6E11CFCB1","user_header":"https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg","comment_is_top":false,"comment_ctime":1555509951,"is_pvip":false,"replies":[{"id":"31323","content":"仔细分析年龄分布倒也可以吧，但是不太可拓展，以后分布变了或者别的问题就不一定适应了。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555522433,"ip_address":"","comment_id":87085,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850477247","product_id":100025301,"comment_content":"年龄数据一般来说是服从0-100范围内的正态分布，我们可以按正态分布进行模拟确定近似等数据量的分位点。以后再数据量逐渐被收集后，再不断的优化分位点的确定。","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447305,"discussion_content":"仔细分析年龄分布倒也可以吧，但是不太可拓展，以后分布变了或者别的问题就不一定适应了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555522433,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87049,"user_name":"尚科","can_delete":false,"product_type":"c1","uid":1165054,"ip_address":"","ucode":"F23A164954CA5A","user_header":"https://static001.geekbang.org/account/avatar/00/11/c6/fe/cf8b21ab.jpg","comment_is_top":false,"comment_ctime":1555503412,"is_pvip":false,"replies":[{"id":"31326","content":"很新颖，我没想到，确实对这个特定场景有些有效。我能想到的问题是如果机器数量大于12（12个月），不太可拓展。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555522896,"ip_address":"","comment_id":87049,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850470708","product_id":100025301,"comment_content":"思考题，可考虑使用用户注册日期的月、日来分片","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447295,"discussion_content":"很新颖，我没想到，确实对这个特定场景有些有效。我能想到的问题是如果机器数量大于12（12个月），不太可拓展。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555522896,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87020,"user_name":"买老实","can_delete":false,"product_type":"c1","uid":1199842,"ip_address":"","ucode":"A8FB87A4E3EEDB","user_header":"https://static001.geekbang.org/account/avatar/00/12/4e/e2/03371799.jpg","comment_is_top":false,"comment_ctime":1555497317,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5850464613","product_id":100025301,"comment_content":"好像在图片 抗噪点 和噪音抗噪都有类似的算法问题","like_count":1},{"had_liked":false,"id":86993,"user_name":"hd900","can_delete":false,"product_type":"c1","uid":1057579,"ip_address":"","ucode":"8CCC03F3BA080A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/IjFRWDxE5DUQZ4YJHVjN862xsDjhLObf2kKibFha2vcpNoHdzoqoBsvGibdPSNGZCJI8akhVPecf8vS9xgee2Dng/132","comment_is_top":false,"comment_ctime":1555493415,"is_pvip":false,"replies":[{"id":"31329","content":"很有意思的思路！我不知道，不知道注册时长是否是均匀分布","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523235,"ip_address":"","comment_id":86993,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850460711","product_id":100025301,"comment_content":"根据用户的注册时长划分，老用户新用户分开","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447273,"discussion_content":"很有意思的思路！我不知道，不知道注册时长是否是均匀分布","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523235,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86940,"user_name":"Mark Lee","can_delete":false,"product_type":"c1","uid":1502726,"ip_address":"","ucode":"5CD6B3369DA198","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/06/598eddb6.jpg","comment_is_top":false,"comment_ctime":1555485602,"is_pvip":false,"replies":[{"id":"31332","content":"哈哈，身份证太有意思了！我不知道使用用户的身份id是否合法&#47;道德&#47;隐私。纯技术上似乎可行","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555523648,"ip_address":"","comment_id":86940,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850452898","product_id":100025301,"comment_content":"把身份证后面的随机数截取到前面，或者有规律的加3到4位的随机数","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447254,"discussion_content":"哈哈，身份证太有意思了！我不知道使用用户的身份id是否合法/道德/隐私。纯技术上似乎可行","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555523648,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86918,"user_name":"亭亭亭","can_delete":false,"product_type":"c1","uid":1283931,"ip_address":"","ucode":"DFD135FDEA79EA","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJF2gTFBleTUHJtpV4KeyMGdiaycVkrX1JgZJITFfmUa4xWyiapicRF7h0Ur4YI5IdDJ9N43Q6ey8vyA/132","comment_is_top":false,"comment_ctime":1555477869,"is_pvip":false,"replies":[{"id":"31269","content":"谢谢，也期待你的留言，与你一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555484927,"ip_address":"","comment_id":86918,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850445165","product_id":100025301,"comment_content":"期待更新","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447244,"discussion_content":"谢谢，也期待你的留言，与你一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555484927,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86917,"user_name":"Geek_e8d453","can_delete":false,"product_type":"c1","uid":1504929,"ip_address":"","ucode":"AE56A8891F9FEC","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq3oGeTb2L8UfTN3QrypZXD0JofEBuNWjLDQ5b4BEklyQ6xoicgK5z1RMibJMsHwNMOSOhibpZyMtXzg/132","comment_is_top":false,"comment_ctime":1555477533,"is_pvip":false,"replies":[{"id":"31339","content":"谢谢你的答案！问题本质上是想找到一个能很好均分的方法来分片。采取按照出生日期来划分的话还需要考虑到如果某一台机器宕机了，我们要如何重新分配原本属于这台机器上的用户。希望能继续看到你的留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555526692,"ip_address":"","comment_id":86917,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850444829","product_id":100025301,"comment_content":"小白来说说思路，因为不知道要分多少份，开始想的是可以按生日来分，这样大数据的情况下只有2.29可能有会有点问题其他应该是均衡的，但这样只有365份。后来又想到如果有个唯一对应并且自增的用户id，就从末位开始。如果是16位的取3位就能切4000份了。。。。。","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447243,"discussion_content":"谢谢你的答案！问题本质上是想找到一个能很好均分的方法来分片。采取按照出生日期来划分的话还需要考虑到如果某一台机器宕机了，我们要如何重新分配原本属于这台机器上的用户。希望能继续看到你的留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555526692,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86912,"user_name":"多襄丸","can_delete":false,"product_type":"c1","uid":1074310,"ip_address":"","ucode":"1AA1497C5A293C","user_header":"https://static001.geekbang.org/account/avatar/00/10/64/86/f5a9403a.jpg","comment_is_top":false,"comment_ctime":1555476073,"is_pvip":false,"replies":[{"id":"31268","content":"可以，这是最常见的sharding function","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555481856,"ip_address":"","comment_id":86912,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850443369","product_id":100025301,"comment_content":"可以使用 hash(userId)%机器节点数  来做嘛?","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447241,"discussion_content":"可以，这是最常见的sharding function","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555481856,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86907,"user_name":"白头发少年","can_delete":false,"product_type":"c1","uid":1502941,"ip_address":"","ucode":"BFB20E66282BA7","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK7w1ic4GGvrLOWnKjhhVx3HvNuAcqCVpF8fIb1O8ALoUjxN1UKD6sHBsUr774GCPeF2afReHjeuMQ/132","comment_is_top":false,"comment_ctime":1555473974,"is_pvip":false,"replies":[{"id":"31258","content":"好像和专栏这节内容不想关。可以做个dual-write吧，或者一个pub&#47;sub通知另一个。后面的章节会提到类似的概念。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555475197,"ip_address":"","comment_id":86907,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850441270","product_id":100025301,"comment_content":"您好，我们的业务数据存储到mysql中的，可是在大数据这块，我们需要把mysql的数据导入到hbase,可是mysql的数据有可能实时更改，怎么做到mysql的数据更改后，能实时同步到hbase?","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447239,"discussion_content":"好像和专栏这节内容不想关。可以做个dual-write吧，或者一个pub/sub通知另一个。后面的章节会提到类似的概念。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555475197,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86879,"user_name":"sudo","can_delete":false,"product_type":"c1","uid":1504731,"ip_address":"","ucode":"09BD2D87129EF7","user_header":"","comment_is_top":false,"comment_ctime":1555469865,"is_pvip":false,"replies":[{"id":"31237","content":"专栏后面有介绍类似的应用，欢迎继续关注。改成beam或者spark完全可以。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555472025,"ip_address":"","comment_id":86879,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850437161","product_id":100025301,"comment_content":"文中提到MapReduce被淘汰了，但有很多场景利用了MapReduce Streaming跑复杂的python逻辑，如跑个TensorFlow inference，没法直接改成Spark，请问下这种场景目前的最佳实践是什么？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447227,"discussion_content":"专栏后面有介绍类似的应用，欢迎继续关注。改成beam或者spark完全可以。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555472025,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86816,"user_name":"Austin","can_delete":false,"product_type":"c1","uid":1207496,"ip_address":"","ucode":"A99DBCFE8D2558","user_header":"https://static001.geekbang.org/account/avatar/00/12/6c/c8/1908ea47.jpg","comment_is_top":false,"comment_ctime":1555463662,"is_pvip":false,"replies":[{"id":"31218","content":"谢谢你的答案！欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555467648,"ip_address":"","comment_id":86816,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850430958","product_id":100025301,"comment_content":"hash partition","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447190,"discussion_content":"谢谢你的答案！欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555467648,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86815,"user_name":"无形","can_delete":false,"product_type":"c1","uid":1016889,"ip_address":"","ucode":"B740E2A68A17A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","comment_is_top":false,"comment_ctime":1555463566,"is_pvip":true,"replies":[{"id":"31205","content":"谢谢你的答案！在分片的时候也需要考虑实施方案的时间复杂度噢。欢迎你继续留言讨论，一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464164,"ip_address":"","comment_id":86815,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850430862","product_id":100025301,"comment_content":"首选讨论可行的分区策论，比如年龄段、地区、账号ID等。然后选择某一比如年龄分组，然后统计各个年龄段的人数分布情况，求方差，如果数据量分布量差异比较大就缩小年龄分组范围重复以上操作，如果数据量差异还比较大，换一种分区策略，选择地区重复以上策略，如果任意单一策略都没法使得机器处理的数据量接近，就同时使用两种策略，比如年龄和地区相结合，重复以上操作，直到试完了所有的策略的全组合，对每个组合进行比较，选择方差最小的作为分片的方法","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447189,"discussion_content":"谢谢你的答案！在分片的时候也需要考虑实施方案的时间复杂度噢。欢迎你继续留言讨论，一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464164,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86808,"user_name":"Keanu","can_delete":false,"product_type":"c1","uid":1491643,"ip_address":"","ucode":"69681E14953EFA","user_header":"https://static001.geekbang.org/account/avatar/00/16/c2/bb/03429750.jpg","comment_is_top":false,"comment_ctime":1555463294,"is_pvip":false,"replies":[{"id":"31207","content":"谢谢你的答案！如果有些首位字母或数字的使用率很高，而其它字母或数字的使用率低的话，那些使用率高的字母或数字会造成Hot Spot的问题噢。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464312,"ip_address":"","comment_id":86808,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850430590","product_id":100025301,"comment_content":"我会考虑使用用户邮箱名首位的字母或数字进行分片。","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447185,"discussion_content":"谢谢你的答案！如果有些首位字母或数字的使用率很高，而其它字母或数字的使用率低的话，那些使用率高的字母或数字会造成Hot Spot的问题噢。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464312,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86806,"user_name":"Freud","can_delete":false,"product_type":"c1","uid":1503467,"ip_address":"","ucode":"649E6ED55FE7F8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoV6yaH2ib6B0K0DT8FxR9mgttfcF3u3fRS2yCEfg9kgibDPaAGccBKScVeIm34oapTHuVBPDNP9U9g/132","comment_is_top":false,"comment_ctime":1555463282,"is_pvip":false,"replies":[{"id":"31260","content":"听起来不错，但是得保证随机数是均匀的和是可重复的。可重复的意思是，比如你在shard 2上任务都失败了，需要能还原出错的任务重试。如果是“真”随机，就无法还原了。需要是比较好的伪随机。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555475495,"ip_address":"","comment_id":86806,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850430578","product_id":100025301,"comment_content":"元楠老师，关于思考我认为引用随机标记的思想，第一次map输入时，在key上拼接随机数，经过第一个mrjob的处理后，再将标记去掉，这样可以大大减小数据倾斜。可以基本保证第一次数据分片的随机性。这个思想是我在Hive优化中学习到的，期待您的指导意见","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447183,"discussion_content":"听起来不错，但是得保证随机数是均匀的和是可重复的。可重复的意思是，比如你在shard 2上任务都失败了，需要能还原出错的任务重试。如果是“真”随机，就无法还原了。需要是比较好的伪随机。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555475495,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86802,"user_name":"Rainbow","can_delete":false,"product_type":"c1","uid":1259525,"ip_address":"","ucode":"248A7E2C05E4DE","user_header":"https://static001.geekbang.org/account/avatar/00/13/38/05/67aae6c8.jpg","comment_is_top":false,"comment_ctime":1555462472,"is_pvip":false,"replies":[{"id":"31261","content":"如果能应对现在的应用也可以，不需要为了赶时髦而换。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555475550,"ip_address":"","comment_id":86802,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850429768","product_id":100025301,"comment_content":"我们线上离线分析还是走hive，底层还是mapreduce，按老师说早淘汰了，我们该怎么改进","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447180,"discussion_content":"如果能应对现在的应用也可以，不需要为了赶时髦而换。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555475550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86798,"user_name":"笑若海","can_delete":false,"product_type":"c1","uid":1283537,"ip_address":"","ucode":"A10EF247EE4B5B","user_header":"https://static001.geekbang.org/account/avatar/00/13/95/d1/7d3834ef.jpg","comment_is_top":false,"comment_ctime":1555462034,"is_pvip":false,"replies":[{"id":"31208","content":"谢谢你的答案！DHT是一个不错的选择！欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464381,"ip_address":"","comment_id":86798,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850429330","product_id":100025301,"comment_content":"使用分布式hash","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447177,"discussion_content":"谢谢你的答案！DHT是一个不错的选择！欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464381,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86794,"user_name":"Mac Kwan","can_delete":false,"product_type":"c1","uid":1001958,"ip_address":"","ucode":"FC80EBC9CD37A8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/49/e6/4f00fe55.jpg","comment_is_top":false,"comment_ctime":1555461656,"is_pvip":false,"replies":[{"id":"31209","content":"谢谢你的答案！这个答案不错。不过取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464554,"ip_address":"","comment_id":86794,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850428952","product_id":100025301,"comment_content":"我会考虑使用用户ID进行取模运算或者哈希计算来尝试把数据尽可能地均匀地分配给不同的worker","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447174,"discussion_content":"谢谢你的答案！这个答案不错。不过取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464554,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86763,"user_name":"刘楠","can_delete":false,"product_type":"c1","uid":1120773,"ip_address":"","ucode":"9F19D44CBEE039","user_header":"https://static001.geekbang.org/account/avatar/00/11/1a/05/f154d134.jpg","comment_is_top":false,"comment_ctime":1555458808,"is_pvip":false,"replies":[{"id":"31211","content":"谢谢你的答案！取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464663,"ip_address":"","comment_id":86763,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850426104","product_id":100025301,"comment_content":"id hash后，取模？","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447160,"discussion_content":"谢谢你的答案！取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464663,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86756,"user_name":"李强Belo","can_delete":false,"product_type":"c1","uid":1209754,"ip_address":"","ucode":"5D3E9A4769037C","user_header":"https://static001.geekbang.org/account/avatar/00/12/75/9a/e55cabf8.jpg","comment_is_top":false,"comment_ctime":1555457563,"is_pvip":false,"replies":[{"id":"31213","content":"谢谢你的答案！看得出来你是一个思维很缜密的人！欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464817,"ip_address":"","comment_id":86756,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850424859","product_id":100025301,"comment_content":"如果一定要用用户的年龄来做分片，我觉得可以事先调查全球人类年龄结构发布，结合实际跑下来的性能分析，可以得出fb自己的用户年龄分布，然后在用到真实的业务计算场景中。当然，每个业务也还有自己的用户年龄分布，因为业务的特殊性。这确实比较麻烦，要预先做的工作很多。各业务团队需要自己去挖掘用户年龄分布。","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447155,"discussion_content":"谢谢你的答案！看得出来你是一个思维很缜密的人！欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464817,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86748,"user_name":"jacksu","can_delete":false,"product_type":"c1","uid":1502971,"ip_address":"","ucode":"C593C3E71C1B5E","user_header":"","comment_is_top":false,"comment_ctime":1555455973,"is_pvip":false,"replies":[{"id":"31212","content":"谢谢你的答案！用户ID也是有可能造成Hot Spot问题的噢。欢迎你继续留言提问，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464736,"ip_address":"","comment_id":86748,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850423269","product_id":100025301,"comment_content":"用户ID","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447152,"discussion_content":"谢谢你的答案！用户ID也是有可能造成Hot Spot问题的噢。欢迎你继续留言提问，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464736,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86742,"user_name":"江鸟","can_delete":false,"product_type":"c1","uid":1059868,"ip_address":"","ucode":"10F059D951BC1C","user_header":"https://static001.geekbang.org/account/avatar/00/10/2c/1c/1e8a9330.jpg","comment_is_top":false,"comment_ctime":1555453290,"is_pvip":false,"replies":[{"id":"31215","content":"谢谢你的答案！这个答案不错。不过取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555464856,"ip_address":"","comment_id":86742,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850420586","product_id":100025301,"comment_content":"哈希取模的方式","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447148,"discussion_content":"谢谢你的答案！这个答案不错。不过取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555464856,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86740,"user_name":"Lion","can_delete":false,"product_type":"c1","uid":1046693,"ip_address":"","ucode":"294219DA53038C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f8/a5/08b868e8.jpg","comment_is_top":false,"comment_ctime":1555452457,"is_pvip":false,"replies":[{"id":"31255","content":"好像没有完全理解你的意思？你说是按照用户所在地？可能地区和年龄一样分布不均吧","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474925,"ip_address":"","comment_id":86740,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850419753","product_id":100025301,"comment_content":"按地区和数量两个条件分区。","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447146,"discussion_content":"好像没有完全理解你的意思？你说是按照用户所在地？可能地区和年龄一样分布不均吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474925,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86739,"user_name":"青玄","can_delete":false,"product_type":"c1","uid":1356135,"ip_address":"","ucode":"C61BD9BCEFEF66","user_header":"https://static001.geekbang.org/account/avatar/00/14/b1/67/1d3c2f25.jpg","comment_is_top":false,"comment_ctime":1555452371,"is_pvip":true,"replies":[{"id":"31250","content":"可以考虑一下如果机器数量大于12的话？","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474385,"ip_address":"","comment_id":86739,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850419667","product_id":100025301,"comment_content":"能否选择用户的出生月份作为分片函数呢？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447145,"discussion_content":"可以考虑一下如果机器数量大于12的话？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474385,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86732,"user_name":"henry","can_delete":false,"product_type":"c1","uid":1122491,"ip_address":"","ucode":"7FD012EE0D3035","user_header":"https://static001.geekbang.org/account/avatar/00/11/20/bb/43d63c5f.jpg","comment_is_top":false,"comment_ctime":1555438564,"is_pvip":false,"replies":[{"id":"31253","content":"也是可以，但是很多数据库并不会带原生“行数”这个概念，特别是分布式的数据库。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555474760,"ip_address":"","comment_id":86732,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850405860","product_id":100025301,"comment_content":"按每个用户在数据文件中的行数来分配","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447142,"discussion_content":"也是可以，但是很多数据库并不会带原生“行数”这个概念，特别是分布式的数据库。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555474760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86726,"user_name":"Daryl","can_delete":false,"product_type":"c1","uid":1333757,"ip_address":"","ucode":"3757AB87702FBD","user_header":"https://static001.geekbang.org/account/avatar/00/14/59/fd/128cc75b.jpg","comment_is_top":false,"comment_ctime":1555435421,"is_pvip":false,"replies":[{"id":"31217","content":"谢谢你的答案！id拼接时间戳的操作可以让Key更加的随机，你看到了问题的本质。取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555467537,"ip_address":"","comment_id":86726,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850402717","product_id":100025301,"comment_content":"id拼接时间戳，取模来分片","like_count":1,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447140,"discussion_content":"谢谢你的答案！id拼接时间戳的操作可以让Key更加的随机，你看到了问题的本质。取模运算在机器有增减的时候会遇到麻烦，所有的用户必须重新取模运算一遍。Consistent Hashing可以很好地解决这个问题。欢迎你继续留言，我们一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555467537,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":350295,"user_name":"钱鹏 Allen","can_delete":false,"product_type":"c1","uid":2518863,"ip_address":"","ucode":"7E95E82C0717DA","user_header":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","comment_is_top":false,"comment_ctime":1656757608,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1656757608","product_id":100025301,"comment_content":"使用hash表进行处理，将20到29这个年龄段的群体重新进行划分<br><br>","like_count":0},{"had_liked":false,"id":333194,"user_name":"ispark","can_delete":false,"product_type":"c1","uid":1891334,"ip_address":"","ucode":"4676C45E485117","user_header":"","comment_is_top":false,"comment_ctime":1644204058,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1644204058","product_id":100025301,"comment_content":"引起热点的原因是事先进行分片,导致集群出现热点或单节点堆积,那么是不是可以把数据先发送到一个队列中,集群中的各个机器到队列中每次只获取少量数据进行处理,完成后再获取,如此重复,直至队列清空","like_count":0},{"had_liked":false,"id":333127,"user_name":"FooBee","can_delete":false,"product_type":"c1","uid":1368189,"ip_address":"","ucode":"88C0ECA229858D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJyos9KR4uD08N54vPmfOq6E63QGvZ9gVxTe7Vo94x0iclq05YQf1auJEG17zI6hORCn8plwdzxS1Q/132","comment_is_top":false,"comment_ctime":1644116104,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1644116104","product_id":100025301,"comment_content":"apache crunch我记得也是为了复刻flumeJava？crunch和beam啥区别？为啥crunch这么快就退休了？","like_count":0},{"had_liked":false,"id":322088,"user_name":"dog_brother","can_delete":false,"product_type":"c1","uid":1619597,"ip_address":"","ucode":"9F64D3C6D815FB","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83er6OV33jHia3U9LYlZEx2HrpsELeh3KMlqFiaKpSAaaZeBttXRAVvDXUgcufpqJ60bJWGYGNpT7752w/132","comment_is_top":false,"comment_ctime":1637161491,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1637161491","product_id":100025301,"comment_content":"年龄对10取模","like_count":0},{"had_liked":false,"id":319045,"user_name":"Geek_e4f9c9","can_delete":false,"product_type":"c1","uid":2831497,"ip_address":"","ucode":"2548B4DFAB36E0","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/ZztnVws3RicNjdqHtKO3HCdtLs0weFfbUtYNOQTPPzTZjnUxJQ77FyzICutoZNDZSJo2Wu7xvVZAsBlndTH2TZA/132","comment_is_top":false,"comment_ctime":1635523529,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1635523529","product_id":100025301,"comment_content":"蔡老师好，我是一个大数据开发小白，我觉得这题应该按地区分片，我觉得分片越多应该每台机器压力就越小。","like_count":0},{"had_liked":false,"id":310942,"user_name":"高景洋","can_delete":false,"product_type":"c1","uid":2717072,"ip_address":"","ucode":"532188513579E4","user_header":"","comment_is_top":false,"comment_ctime":1630996403,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1630996403","product_id":100025301,"comment_content":"1、预测一下10年内的用户增长总量，算出每一个分片能放的下多少数据<br><br>2、将用户名 做一下md5，取md5串的前3位，按前三位取的话，可以分出4096个分片<br>","like_count":0},{"had_liked":false,"id":295176,"user_name":"      ","can_delete":false,"product_type":"c1","uid":2628433,"ip_address":"","ucode":"F202AB0B5CF302","user_header":"https://static001.geekbang.org/account/avatar/00/28/1b/51/d8eac816.jpg","comment_is_top":false,"comment_ctime":1622282311,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1622282311","product_id":100025301,"comment_content":"典型的数据倾斜问题[旺柴]","like_count":0},{"had_liked":false,"id":269691,"user_name":"叶凡","can_delete":false,"product_type":"c1","uid":2385263,"ip_address":"","ucode":"AD31402DA8ED2F","user_header":"https://static001.geekbang.org/account/avatar/00/24/65/6f/e7a56d6e.jpg","comment_is_top":false,"comment_ctime":1608737710,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1608737710","product_id":100025301,"comment_content":"老师，我想问既然MapReduce已经被哇谷淘汰，那如今在国外还有国家用MapReduce的技术吗？如今的国内外研究现状又是如何呢？","like_count":0},{"had_liked":false,"id":224374,"user_name":"Geek_Coke","can_delete":false,"product_type":"c1","uid":1795779,"ip_address":"","ucode":"CEBA67D4A41079","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/lZziapCQpYNzHHOzSRhcBO3pHm6VddSgediaTZe1KU9kNrGnh13oSes5kVfe6NaQNouP6kG0rU8bibQgBcGwueibmw/132","comment_is_top":false,"comment_ctime":1591356626,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591356626","product_id":100025301,"comment_content":"我的理解是，用户数据通过分片函数分片，这个得看map的key值，hadoop的mr框架默认是hash key取模，日常情况下，如果相同的key过多，出现数据倾斜，可以对key值进行加盐打散，再进行分片。","like_count":0},{"had_liked":false,"id":223900,"user_name":"Why","can_delete":false,"product_type":"c1","uid":1878356,"ip_address":"","ucode":"21C4A52D9ED778","user_header":"https://static001.geekbang.org/account/avatar/00/1c/a9/54/8b0d4b0f.jpg","comment_is_top":false,"comment_ctime":1591228643,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591228643","product_id":100025301,"comment_content":"很遗憾这么久才发现这个专栏！不过还是很庆幸没有掉队太久！现在大部分的分布式架构基本都采用了分片技术，比如ES和Kafka这种可以处理大规模数据的中间件，文中说的分片函数应该就是路由算法吧，这些中间件都支持随机路由，循环路由以及哈希路由！以前做过分表存储的设计，对每条数据的ID进行哈希取余，然后命中对应的table！","like_count":0},{"had_liked":false,"id":189529,"user_name":"牛世淮","can_delete":false,"product_type":"c1","uid":1908688,"ip_address":"","ucode":"6A492898C46834","user_header":"","comment_is_top":false,"comment_ctime":1584525468,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584525468","product_id":100025301,"comment_content":"不好意思，老师，我刚刚的回复没有考虑周全。我刚刚在看其他人的回复看到maye提到Reducer Key是否倾斜这个问题。我那个方案是不完美的。","like_count":0},{"had_liked":false,"id":189518,"user_name":"牛世淮","can_delete":false,"product_type":"c1","uid":1908688,"ip_address":"","ucode":"6A492898C46834","user_header":"","comment_is_top":false,"comment_ctime":1584524439,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584524439","product_id":100025301,"comment_content":"关于sharding_function这个函数，还有提到的动态sharding技术。我觉是不是可以采用一种反馈机制。master在sharding时，根据每个work当时的状态。动态分配任务。在一个集群中每个节点的性能是不一样的。采取能者多劳的策略。并不一定每个节点不管性能高低都平均分配相同的任务。","like_count":0},{"had_liked":false,"id":187425,"user_name":"李宏杰","can_delete":false,"product_type":"c1","uid":1906382,"ip_address":"","ucode":"2D850143C92262","user_header":"","comment_is_top":false,"comment_ctime":1584107367,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584107367","product_id":100025301,"comment_content":"你好！请问我们是否可以通过对数据进行取模分配，或者根据事先配置好的权重进行轮询分配呢？","like_count":0},{"had_liked":false,"id":186242,"user_name":"杰之7","can_delete":false,"product_type":"c1","uid":1297232,"ip_address":"","ucode":"F7DA2E21085332","user_header":"https://static001.geekbang.org/account/avatar/00/13/cb/50/66d0bd7f.jpg","comment_is_top":false,"comment_ctime":1583806355,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583806355","product_id":100025301,"comment_content":"学习这一节后，关于老师提出了MR处理框架会被替代，后面出现的Spark就解决了MR的时间性能的问题，快了大约2个数量级的效率。维护成本问题后面继续跟着老师学习。","like_count":0},{"had_liked":false,"id":184790,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1583404227,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583404227","product_id":100025301,"comment_content":"我这边是自己实现了整套mapreduce系统，mapreduce的出错处理我这边很简单，出错了整个计算重来，在我们的实际应用过程也没有出现很大的问题，毕竟我们面向的是小集群，千亿级数据。我们的mapreduce系统实现很简单。主要复杂度是业务作业流，列存和索引技术。至于分片，我们针对不同业务表自定义分片，实际计算过程中，歪斜的情况也很少","like_count":0},{"had_liked":false,"id":184788,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1583403682,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583403682","product_id":100025301,"comment_content":"用户名hash","like_count":0},{"had_liked":false,"id":139067,"user_name":"Chad Wu","can_delete":false,"product_type":"c1","uid":1563223,"ip_address":"","ucode":"46B74551EB5F6C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIia6JywZv309ugNmusDlRnxibezH5iapAh4v40umnc5TYwcQfgFjvYFXHjIicZ7NpFozFjnWBBW8iaagA/132","comment_is_top":false,"comment_ctime":1570539624,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1570539624","product_id":100025301,"comment_content":"IT小白一个, 我打算对现有的数据先做一个水塘抽样, 把TB, PB级别的用户压缩成GB或者MB; 随后再对年龄进行排序,n等分获得适当的sharding","like_count":0},{"had_liked":false,"id":119441,"user_name":"sun留白","can_delete":false,"product_type":"c1","uid":1531348,"ip_address":"","ucode":"53FEEA5E244C9A","user_header":"https://static001.geekbang.org/account/avatar/00/17/5d/d4/e5ea1c25.jpg","comment_is_top":false,"comment_ctime":1564588628,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564588628","product_id":100025301,"comment_content":"取证件号码进行hash分组","like_count":0},{"had_liked":false,"id":118412,"user_name":"人唯优","can_delete":false,"product_type":"c1","uid":1503129,"ip_address":"","ucode":"7949FE2500759B","user_header":"https://static001.geekbang.org/account/avatar/00/16/ef/99/cc30e2ca.jpg","comment_is_top":false,"comment_ctime":1564361931,"is_pvip":false,"replies":[{"id":"43409","content":"加油。","user_name":"作者回复","user_name_real":"廿七","uid":"1386753","ctime":1564367046,"ip_address":"","comment_id":118412,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1564361931","product_id":100025301,"comment_content":"今天开始走第二遍阅读","like_count":0,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":460398,"discussion_content":"加油。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564367046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":115396,"user_name":"格非","can_delete":false,"product_type":"c1","uid":1004569,"ip_address":"","ucode":"89FABFFC377131","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/19/95ff4cbd.jpg","comment_is_top":false,"comment_ctime":1563560515,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1563560515","product_id":100025301,"comment_content":"请教一个问题，您在文章中提到：shard是将数据分发给worker的过程，但mapreduce不是就近选择本地节点上的数据进行处理吗？hdfs文件通过输入格式分成一系列的input split，每个split由一个map任务处理，什么情况会出现shard呢？另外，能给推荐一些相关资料吗，一些实际优化的案例啥的，谢谢了","like_count":0},{"had_liked":false,"id":114310,"user_name":"夜吾夜","can_delete":false,"product_type":"c1","uid":1293142,"ip_address":"","ucode":"FC8729883EAD62","user_header":"https://static001.geekbang.org/account/avatar/00/13/bb/56/41cbcda2.jpg","comment_is_top":false,"comment_ctime":1563268671,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1563268671","product_id":100025301,"comment_content":"您好，作者，问下假如采用一致性哈希环来作为数据分片的策略，这个环是在每一个物理节点上有一个呢？还是所有节点组成一个环？","like_count":0},{"had_liked":false,"id":105999,"user_name":"风中花","can_delete":false,"product_type":"c1","uid":1085237,"ip_address":"","ucode":"067E0A1E116844","user_header":"https://static001.geekbang.org/account/avatar/00/10/8f/35/f1839bb2.jpg","comment_is_top":false,"comment_ctime":1561122829,"is_pvip":false,"replies":[{"id":"38467","content":"谢谢你的留言，也谢谢你对我的支持，加油！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561336658,"ip_address":"","comment_id":105999,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1561122829","product_id":100025301,"comment_content":"打卡第二次！我不知道老师能否看到我的评论！我深深感到老师的责任与对我们这些读者的尊重！我看是每问必回！我基本都看完大家的提问！都有老师的点评！真心为老师点赞！虽然是小白！但我还是认真看完！虽然没有问题！但我想在知识不断的积累过程！会有问题的浮现！期待更精彩！","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454908,"discussion_content":"谢谢你的留言，也谢谢你对我的支持，加油！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561336658,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":99232,"user_name":"西北偏北","can_delete":false,"product_type":"c1","uid":1043160,"ip_address":"","ucode":"64BD69C84EE6A1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132","comment_is_top":false,"comment_ctime":1559179949,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559179949","product_id":100025301,"comment_content":"Mapreduce由于其使用复杂度和性能问题,已不在作为互联网公司大数据处理的最佳选择，现在使用beam","like_count":0},{"had_liked":false,"id":94444,"user_name":"很吵请安静","can_delete":false,"product_type":"c1","uid":1155651,"ip_address":"","ucode":"194DCC3D5288BA","user_header":"https://static001.geekbang.org/account/avatar/00/11/a2/43/96857244.jpg","comment_is_top":false,"comment_ctime":1557814244,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557814244","product_id":100025301,"comment_content":"今天搜了下MapReduce 被淘汰的原因，其中一点是 在machine learning方面，MapReduce 框架不适合iterative processing ， 蔡老师可以帮忙解释下吗？谢谢","like_count":0},{"had_liked":false,"id":94384,"user_name":"吴晓亮","can_delete":false,"product_type":"c1","uid":1529305,"ip_address":"","ucode":"BA8D7D31137201","user_header":"https://static001.geekbang.org/account/avatar/00/17/55/d9/326dd1fd.jpg","comment_is_top":false,"comment_ctime":1557798487,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557798487","product_id":100025301,"comment_content":"问个hadoop的基本问题可以吧,我是刚刚入门,还在看hadoop的部分,我们部门是做SWE engineer的,就是负责所有产品的nightly regression log的分析,比方我一晚上几万个几百万的log,按hadoop分block的概念,一个log会分很多block,如果我希望做语义分析之类(yacc),我希望一个mapper是从头到尾分析一个log,而不是一个切片(block是按line方式cut的吗?也许可以自定义吧,原谅我还没看完)怎么做呢? 另外我需要结合文件的路径关键字做key,这个应该也没问题吧?谢谢","like_count":0},{"had_liked":false,"id":90933,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1556735896,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556735896","product_id":100025301,"comment_content":"那个一致性哈希在mr的源码能贴出来嘛，跪求","like_count":0},{"had_liked":false,"id":90557,"user_name":"朱月俊","can_delete":false,"product_type":"c1","uid":1017707,"ip_address":"","ucode":"4DA0728B862FBD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/87/6b/0b6cd39a.jpg","comment_is_top":false,"comment_ctime":1556556194,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556556194","product_id":100025301,"comment_content":"如果一个任务需要多个map reduce子任务完成，维护成本就非常高，无法接受。<br>同时，使用map reduce，也需要对应的分布式文件系统。<br>对于facebook问题，先按照年龄拆分，每个年龄段分为一个map reduce任务，继续拆分，从而解决数据倾斜问题。","like_count":0},{"had_liked":false,"id":89930,"user_name":"Geek_1987v5","can_delete":false,"product_type":"c1","uid":1514466,"ip_address":"","ucode":"603A31AF885149","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/lUOVNGBvDTqss5XExibXsOrx1mAM7raMhQbdEHdkAeIEGLoK2wJXjy1QiaDKZlQ9vLjTyZcia39KVmrpzJB8zRhqA/132","comment_is_top":false,"comment_ctime":1556334298,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556334298","product_id":100025301,"comment_content":"不知道数据对焦检测用的什么方法，能不能给讲讲。","like_count":0},{"had_liked":false,"id":89439,"user_name":"Meiyan","can_delete":false,"product_type":"c1","uid":1008580,"ip_address":"","ucode":"BC2FB7ED2CBF9B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/63/c4/b616c251.jpg","comment_is_top":false,"comment_ctime":1556174487,"is_pvip":false,"replies":[{"id":"32088","content":"谢谢你的提问！MapReduce最早是在2003年的时候由Google开发的一个内部数据处理平台。在2004年时候Google公布了MapReduce数据处理模型的论文，之后Doug Cutting和Mike Cafarella根据这篇论文的思想在2005年的时候写出了一套数据处理平台，而这个平台于2011年在Apache项目中孵化取名为Hadoop。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556223524,"ip_address":"","comment_id":89439,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1556174487","product_id":100025301,"comment_content":"描述石器时代和青铜时代的那张图里，为什么mapreduce比hadoop还早，我理解mapreduce应该是hadoop里面一部分，为什么分开了，而且mapreduce还比hadoop早？谢谢老师","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448133,"discussion_content":"谢谢你的提问！MapReduce最早是在2003年的时候由Google开发的一个内部数据处理平台。在2004年时候Google公布了MapReduce数据处理模型的论文，之后Doug Cutting和Mike Cafarella根据这篇论文的思想在2005年的时候写出了一套数据处理平台，而这个平台于2011年在Apache项目中孵化取名为Hadoop。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556223524,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89437,"user_name":"Gavin","can_delete":false,"product_type":"c1","uid":1498375,"ip_address":"","ucode":"6F8AC3EC5FA260","user_header":"https://static001.geekbang.org/account/avatar/00/16/dd/07/14aa90dd.jpg","comment_is_top":false,"comment_ctime":1556174315,"is_pvip":false,"replies":[{"id":"32089","content":"谢谢你的提问！使用出生年份奇偶数做分片的话只能分片到两台机器上，也会对服务器造成很大压力。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556223594,"ip_address":"","comment_id":89437,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1556174315","product_id":100025301,"comment_content":"在Facebook例子中，在20~30 这个年龄段产生数据偏移；如果使用出生年份奇偶数做分片呢？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448132,"discussion_content":"谢谢你的提问！使用出生年份奇偶数做分片的话只能分片到两台机器上，也会对服务器造成很大压力。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556223594,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89409,"user_name":"ljf10000","can_delete":false,"product_type":"c1","uid":1494064,"ip_address":"","ucode":"44957329A61EAE","user_header":"https://wx.qlogo.cn/mmopen/vi_32/DYAIOgq83eop9WylZJicLQ5wib49kcMPqCTRT1aThh6mMAVl6qseLwbVOLhicVLjZCxCoyQd5CrrHHibs2CVPaoK3g/132","comment_is_top":false,"comment_ctime":1556165999,"is_pvip":false,"replies":[{"id":"33009","content":"可能可以参考下10介绍的lambda架构","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1557220458,"ip_address":"","comment_id":89409,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1556165999","product_id":100025301,"comment_content":"我也比较关注流处理。如果流数据产生的速率非常快，能实时保存下来就已经非常有挑战，那么离线检索&#47;分析改如何处理？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448118,"discussion_content":"可能可以参考下10介绍的lambda架构","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557220458,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89258,"user_name":"meijing0114","can_delete":false,"product_type":"c1","uid":1012416,"ip_address":"","ucode":"B349D33E2F3ECC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/72/c0/b09911a0.jpg","comment_is_top":false,"comment_ctime":1556119420,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556119420","product_id":100025301,"comment_content":"专门去读了一遍MapReduce的论文，感觉论文本身非常精彩。对大规模数据处理问题的抽象，以及整个map reduce系统的设计，各种容错、优化等都考虑的很全面。或许是因为业务场景的复杂化，单个业务链路对应的map reduce操作远超15年前，所以有些落伍了吧。","like_count":0},{"had_liked":false,"id":89197,"user_name":"Geek_1c9f7c","can_delete":false,"product_type":"c1","uid":1110419,"ip_address":"","ucode":"9A5611F231C474","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKic4Sia2vW3FdODLrSLzGEXqq2s6wRywMXSWHNdPY9Ge1ecW57pQ29dXRMSSl6aYLpv2uXH2U3Nliaw/132","comment_is_top":false,"comment_ctime":1556102885,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556102885","product_id":100025301,"comment_content":"向量化数据后 kmeans分割。<br>比hash要复杂。","like_count":0},{"had_liked":false,"id":89196,"user_name":"al_培龙","can_delete":false,"product_type":"c1","uid":1039597,"ip_address":"","ucode":"C8E9D775D28D91","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dc/ed/3fe13e55.jpg","comment_is_top":false,"comment_ctime":1556102740,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556102740","product_id":100025301,"comment_content":"安装年龄的个位数值分呢，应该均匀很多","like_count":0},{"had_liked":false,"id":89066,"user_name":"Yafei","can_delete":false,"product_type":"c1","uid":1233728,"ip_address":"","ucode":"BEA5B162DF2364","user_header":"https://static001.geekbang.org/account/avatar/00/12/d3/40/39d41615.jpg","comment_is_top":false,"comment_ctime":1556071082,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556071082","product_id":100025301,"comment_content":"画出整张social network，然后通过KMeans聚类来分片。","like_count":0},{"had_liked":false,"id":89050,"user_name":"Chn.K","can_delete":false,"product_type":"c1","uid":1285191,"ip_address":"","ucode":"F82E8CE20C16FA","user_header":"https://static001.geekbang.org/account/avatar/00/13/9c/47/50cf2cab.jpg","comment_is_top":false,"comment_ctime":1556069053,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556069053","product_id":100025301,"comment_content":"老师，你好，我是一名大数据处理方面的菜鸟，之前没接触过这方面的东西，包括理论与实践，但是对这方面的东西还比较感兴趣，所以也买了老师的课来学习，我想问的是对于我这样一个菜鸟，该怎么去学习大数据呢？有没有好的书籍、论文或者网站推荐，然后在学习过程中有没有可以练手的小项目跟着走？因为我从事的不是大数据方向的工作，但是总觉得如果只学习理论远远不够，理解不到精髓…请老师回复，谢谢！","like_count":0},{"had_liked":false,"id":88596,"user_name":"Tomcat","can_delete":false,"product_type":"c1","uid":1346364,"ip_address":"","ucode":"B270CEED693256","user_header":"https://static001.geekbang.org/account/avatar/00/14/8b/3c/0462eca7.jpg","comment_is_top":false,"comment_ctime":1555949606,"is_pvip":false,"replies":[{"id":"31783","content":"日期也同样容易分布不均","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555988252,"ip_address":"","comment_id":88596,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555949606","product_id":100025301,"comment_content":"我想应该是按照日期来做分片会更加合理一些。因为其他的维度都会产生极大的波动性，容易造成数据倾斜，导致作为掉队。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447843,"discussion_content":"日期也同样容易分布不均","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555988252,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88056,"user_name":"JasonYe","can_delete":false,"product_type":"c1","uid":1131560,"ip_address":"","ucode":"05841AD4B65C99","user_header":"https://static001.geekbang.org/account/avatar/00/11/44/28/948cab86.jpg","comment_is_top":false,"comment_ctime":1555831078,"is_pvip":false,"replies":[{"id":"31701","content":"谢谢你的留言！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555889290,"ip_address":"","comment_id":88056,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555831078","product_id":100025301,"comment_content":"我觉得先统计分布，对于分布过大的key,增加第二key再次打散","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447701,"discussion_content":"谢谢你的留言！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555889290,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88052,"user_name":"slowforce","can_delete":false,"product_type":"c1","uid":1507235,"ip_address":"","ucode":"5A50DB726F00EC","user_header":"","comment_is_top":false,"comment_ctime":1555828340,"is_pvip":false,"replies":[{"id":"31664","content":"按照地域分配也是有可能有hot spot的问题。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555832530,"ip_address":"","comment_id":88052,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555828340","product_id":100025301,"comment_content":"按用户的地域来分配是不是也能达到比较均衡的效果？","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447699,"discussion_content":"按照地域分配也是有可能有hot spot的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555832530,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87994,"user_name":"邱从贤※klion26","can_delete":false,"product_type":"c1","uid":1027239,"ip_address":"","ucode":"36DF21F2B9E94C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/a7/4d41966a.jpg","comment_is_top":false,"comment_ctime":1555802901,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555802901","product_id":100025301,"comment_content":"前面的回答中没有看到定语在分片函数，这样的话，预聚合就不能算了","like_count":0},{"had_liked":false,"id":87993,"user_name":"邱从贤※klion26","can_delete":false,"product_type":"c1","uid":1027239,"ip_address":"","ucode":"36DF21F2B9E94C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/a7/4d41966a.jpg","comment_is_top":false,"comment_ctime":1555802752,"is_pvip":false,"replies":[{"id":"31656","content":"谢谢你的答案！你理解得很对，本质上我们是要避免有hot spot的出现。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555809619,"ip_address":"","comment_id":87993,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555802752","product_id":100025301,"comment_content":"思考题这个我想就是要避免 hotkey，hotkey 会导致满节点，这个可以像留言里面把20映射到12，22，32等，这样每个 key 就被打散了，我想还可以在 map 端做一些预聚合，这样总 key 数量会减少（不过这个有局限）。","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447675,"discussion_content":"谢谢你的答案！你理解得很对，本质上我们是要避免有hot spot的出现。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555809619,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87805,"user_name":"Geek_f3da91","can_delete":false,"product_type":"c1","uid":1345117,"ip_address":"","ucode":"CB98284986DB2C","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKnicAgw2mJL0zJOs6lVQX1QNjLmXaYlSytcJfIDZMicNqlS5FWS5IVugHkzQZhicHia894Diajof5559A/132","comment_is_top":false,"comment_ctime":1555720768,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555720768","product_id":100025301,"comment_content":"看了评论好几遍没看明白一致性Hash(Consistent Hashing)是在哪个环节使用了，一开始理解成在shuffle过程中的map分片存储了，就是在计算的时候增减机器导致怎么选择哪台机器存储数据分片。 现在我理解是说集群在数据分片存储，根据存储算法，一致性hash可以解决在增减机器的时候，不需要全部重新分配数据分片存储位置，但一样还会出现数据分片因为增减机器出现了局部范围的数据分片重新调整的问题，是这么理解吧？ 还有一个问题是，分片重新分配是在balancer的时候重新局部重分配的，不知道理解的对吗？","like_count":0},{"had_liked":false,"id":87718,"user_name":"而立斋","can_delete":false,"product_type":"c1","uid":1087258,"ip_address":"","ucode":"5FED6E9E148195","user_header":"https://static001.geekbang.org/account/avatar/00/10/97/1a/389eab84.jpg","comment_is_top":false,"comment_ctime":1555670184,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555670184","product_id":100025301,"comment_content":"使用一致性哈希算法，来解决数据分片问题<br>https:&#47;&#47;en.wikipedia.org&#47;wiki&#47;Consistent_hashing","like_count":0},{"had_liked":false,"id":87667,"user_name":"silverhawk","can_delete":false,"product_type":"c1","uid":1018649,"ip_address":"","ucode":"BFBC8AF32868DA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8b/19/a15d060d.jpg","comment_is_top":false,"comment_ctime":1555656065,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555656065","product_id":100025301,"comment_content":"这种原始数据不均匀估计得hash了再分片，其实用户年龄分布离线都可以计算，所以不是太难啊","like_count":0},{"had_liked":false,"id":87653,"user_name":"是KK呀","can_delete":false,"product_type":"c1","uid":1064894,"ip_address":"","ucode":"5DCEB53A13D049","user_header":"https://static001.geekbang.org/account/avatar/00/10/3f/be/c7141382.jpg","comment_is_top":false,"comment_ctime":1555650643,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555650643","product_id":100025301,"comment_content":"按照用户的年龄然后后面加一个随机数来分片，reduce阶段结束之后再把随机数去掉。","like_count":0},{"had_liked":false,"id":87646,"user_name":"Helloworld","can_delete":false,"product_type":"c1","uid":1048517,"ip_address":"","ucode":"24C6D00C76995A","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ff/c5/37f59e20.jpg","comment_is_top":false,"comment_ctime":1555648009,"is_pvip":false,"replies":[{"id":"31522","content":"上传的是数据","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555654699,"ip_address":"","comment_id":87646,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555648009","product_id":100025301,"comment_content":"question uploading 上传的应该不是工具，而是上传data进行annotate&#47;tag吧？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447546,"discussion_content":"上传的是数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555654699,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87595,"user_name":"gaoliming","can_delete":false,"product_type":"c1","uid":1480705,"ip_address":"","ucode":"6C5A7D1C4F97AC","user_header":"https://static001.geekbang.org/account/avatar/00/16/98/01/a102bfb9.jpg","comment_is_top":false,"comment_ctime":1555639148,"is_pvip":false,"replies":[{"id":"31502","content":"Java 不是必须。任何语言都不是必须。职业规划角度我觉得选一门语言深入一下，别的语言其实都能触类旁通。<br><br>现在开始的话mapreduce可以跳过。学习一下肯定也没坏处。<br><br>后面有实战的部分的<br><br>感谢留言！觉得有收获欢迎分享给朋友","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555643954,"ip_address":"","comment_id":87595,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555639148","product_id":100025301,"comment_content":"另外请问，如果想从事大数据的工作，JAVA SE编程的经验是不是必须的那？我看很多的人都介绍java编程的重要性，能否从职业规划的角度给出一些建议那？如果学习大数据，还mapreduce还需要学习吗？麻烦百忙中解答！另外我们有一些动手的实验环节的介绍和指导吗？因为我觉得您宝贵的经验的分享和传递，还是需要我们这些小白多动手实验的,    谢谢。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447523,"discussion_content":"Java 不是必须。任何语言都不是必须。职业规划角度我觉得选一门语言深入一下，别的语言其实都能触类旁通。\n\n现在开始的话mapreduce可以跳过。学习一下肯定也没坏处。\n\n后面有实战的部分的\n\n感谢留言！觉得有收获欢迎分享给朋友","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555643954,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87589,"user_name":"gaoliming","can_delete":false,"product_type":"c1","uid":1480705,"ip_address":"","ucode":"6C5A7D1C4F97AC","user_header":"https://static001.geekbang.org/account/avatar/00/16/98/01/a102bfb9.jpg","comment_is_top":false,"comment_ctime":1555638683,"is_pvip":false,"replies":[{"id":"31503","content":"年份还是存在不均匀的问题吧。id取决于生成方式，range不太好设置。可以参考下别的留言，fingerprint 取模是简单的方式。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555644036,"ip_address":"","comment_id":87589,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555638683","product_id":100025301,"comment_content":"蔡总您好，我觉得可以采用年份（year）进行sharding分片，或者按照用户ID range 进行分片，来达到分片均匀的目的。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447521,"discussion_content":"年份还是存在不均匀的问题吧。id取决于生成方式，range不太好设置。可以参考下别的留言，fingerprint 取模是简单的方式。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555644036,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87463,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1555599331,"is_pvip":false,"replies":[{"id":"31463","content":"看数据的分布是在正确的思路上","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555611618,"ip_address":"","comment_id":87463,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555599331","product_id":100025301,"comment_content":"没使用过MR，听文章的意思这个东西既费成本(难使用难维护)又达不到预期的性能，那确实应该被淘汰。<br>大的东西，比如数据，一次处理不了只能分批次处理，分治的思想是深入骨髓的，关键怎么分查起来方便快速是个难题。<br>思考题:我目前能想到的思路，一个就是一致性哈希算法来分，另一个先将数据区别对待，比如20以下和30以上的作为一波，20～30作为另一波，然后针对这两波数据采用不同的分片处理方式这样数据倾斜应该会少一些。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447464,"discussion_content":"看数据的分布是在正确的思路上","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555611618,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87388,"user_name":"Alphacoo","can_delete":false,"product_type":"c1","uid":1502956,"ip_address":"","ucode":"5BE4064D26F3B0","user_header":"","comment_is_top":false,"comment_ctime":1555581004,"is_pvip":false,"replies":[{"id":"31465","content":"1 可以用<br>2 这个专栏可以帮你设置一个很好的起跑线。后面还是通过实践精进吧<br>3 看你的应用场景？我觉得很少人会需要了解hdfs的底层实现，了解一下设计理念肯定有帮助","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555617874,"ip_address":"","comment_id":87388,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555581004","product_id":100025301,"comment_content":"想问下大牛几个问题:<br>1.mapreduce在某些问题中会不会还可以用？<br>2.怎么学习Beam?直接看官方文档？<br>3.Hadoop的另外两个部分hdfs和yarn该学到什么程度？","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447423,"discussion_content":"1 可以用\n2 这个专栏可以帮你设置一个很好的起跑线。后面还是通过实践精进吧\n3 看你的应用场景？我觉得很少人会需要了解hdfs的底层实现，了解一下设计理念肯定有帮助","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555617874,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87303,"user_name":"Vito","can_delete":false,"product_type":"c1","uid":1058567,"ip_address":"","ucode":"9AB7BC1AB3EF7A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersxYcRm1mxLy6RlE5p1yibk5qFYwPS0E5XnI3XrS42rQW8pWJIDeTlshEGjSvl4uOVtEzU6IuOwOA/132","comment_is_top":false,"comment_ctime":1555564048,"is_pvip":true,"replies":[{"id":"31396","content":"确实有一些相似之处。很好的比较。<br><br>有收获欢迎分享给朋友。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555567235,"ip_address":"","comment_id":87303,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555564048","product_id":100025301,"comment_content":"倒排索引，Hbase 设计rowkey也面临同样的问题。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447389,"discussion_content":"确实有一些相似之处。很好的比较。\n\n有收获欢迎分享给朋友。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555567235,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87231,"user_name":"SpanningWings","can_delete":false,"product_type":"c1","uid":1503174,"ip_address":"","ucode":"A5E98A2626F187","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/dV2JvjoAOHOibxVqExibsBv0ib9jJ9zD8icYaDtFbicUgP0GmRbzmgujvz6pOl6drUcgdvfQXTJpOOY9OL45WrkInbA/132","comment_is_top":false,"comment_ctime":1555551415,"is_pvip":false,"replies":[{"id":"31398","content":"谢谢你的答案！这一讲的问题我只是单纯想让大家思考处理大规模数据如何分片的问题，不涉及到后面的查询操作，不够你能考虑到其它use cases也是非常不错的。希望能继续看到你的留言，一起学习进步！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555571267,"ip_address":"","comment_id":87231,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555551415","product_id":100025301,"comment_content":"谢谢老师。大家提到consistent hashing，这样会打的很均匀。  我有个问题就是老师的问题是假设FB的处理哈，我不知道这样的处理是不是还包括未来的查询。如果是查询的话那我们是不是要建立些索引什么的，那这样一致性哈希是不是就不合适了。还是按照一个重要的key来分片，然后动态调整分片的范围比较好？这样查询可以避免full table scan. ","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447357,"discussion_content":"谢谢你的答案！这一讲的问题我只是单纯想让大家思考处理大规模数据如何分片的问题，不涉及到后面的查询操作，不够你能考虑到其它use cases也是非常不错的。希望能继续看到你的留言，一起学习进步！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555571267,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":87229,"user_name":"J Zhang","can_delete":false,"product_type":"c1","uid":1049115,"ip_address":"","ucode":"A64B2E61A8B6A8","user_header":"https://static001.geekbang.org/account/avatar/00/10/02/1b/e1e89267.jpg","comment_is_top":false,"comment_ctime":1555551401,"is_pvip":false,"replies":[{"id":"31397","content":"确实有相似之处，但是我在另几个问题里提到，需要保证加盐是deterministic，不然错误重试无法还原分片任务。<br><br>感谢提问，有收获欢迎把专栏分享给朋友。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555567394,"ip_address":"","comment_id":87229,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555551401","product_id":100025301,"comment_content":"我想对于分片，hbase 的rowkey设计最佳实践给予了很大的参考价值，比如可以通过加盐这种方式可以使得数据很均衡","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447355,"discussion_content":"确实有相似之处，但是我在另几个问题里提到，需要保证加盐是deterministic，不然错误重试无法还原分片任务。\n\n感谢提问，有收获欢迎把专栏分享给朋友。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555567394,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86953,"user_name":"Jaker","can_delete":false,"product_type":"c1","uid":1460817,"ip_address":"","ucode":"61FCEDE3B94F49","user_header":"https://static001.geekbang.org/account/avatar/00/16/4a/51/02678317.jpg","comment_is_top":false,"comment_ctime":1555486725,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555486725","product_id":100025301,"comment_content":"蔡老师你好，我作为一个移动端开发者，需要哪些技术储备能听懂您的这篇专栏。。","like_count":0}]}