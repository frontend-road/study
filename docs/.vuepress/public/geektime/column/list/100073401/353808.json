{"id":353808,"title":"04 | DAG与流水线：到底啥叫“内存计算”？","content":"<p>你好，我是吴磊。</p><p>在日常的开发工作中，我发现有两种现象很普遍。</p><p>第一种是缓存的滥用。无论是RDD，还是DataFrame，凡是能产生数据集的地方，开发同学一律用cache进行缓存，结果就是应用的执行性能奇差无比。开发同学也很委屈：“Spark不是内存计算的吗？为什么把数据缓存到内存里去，性能反而更差了？”</p><p>第二种现象是关于Shuffle的。我们都知道，Shuffle是Spark中的性能杀手，在开发应用时要尽可能地避免Shuffle操作。不过据我观察，很多初学者都没有足够的动力去重构代码来避免Shuffle，这些同学的想法往往是：“能把业务功能实现就不错了，费了半天劲去重写代码就算真的消除了Shuffle，能有多大的性能收益啊。”</p><p>以上这两种现象可能大多数人并不在意，但往往这些细节才决定了应用执行性能的优劣。在我看来，造成这两种现象的根本原因就在于，开发者对Spark内存计算的理解还不够透彻。所以今天，我们就来说说Spark的内存计算都有哪些含义？</p><h2>第一层含义：分布式数据缓存</h2><p>一提起Spark的“内存计算”的含义，你的第一反应很可能是：Spark允许开发者将分布式数据集缓存到计算节点的内存中，从而对其进行高效的数据访问。没错，这就是内存计算的<strong>第一层含义：众所周知的分布式数据缓存。</strong></p><!-- [[[read_end]]] --><p>RDD cache确实是Spark分布式计算引擎的一大亮点，也是对业务应用进行性能调优的诸多利器之一，很多技术博客甚至是Spark官网，都在不厌其烦地强调RDD cache对于应用执行性能的重要性。</p><p>正因为考虑到这些因素，很多开发者才会在代码中不假思索地滥用cache机制，也就是我们刚刚提到的第一个现象。但是，这些同学都忽略了一个重要的细节：只有需要频繁访问的数据集才有必要cache，对于一次性访问的数据集，cache不但不能提升执行效率，反而会产生额外的性能开销，让结果适得其反。</p><p>之所以会忽略这么重要的细节，背后深层次的原因在于，开发者对内存计算的理解仅仅停留在缓存这个层面。因此，当业务应用的执行性能出现问题时，只好死马当活马医，拼命地抓住cache这根救命稻草，结果反而越陷越深。</p><p>接下来，我们就重点说说内存计算的第二层含义：<strong>Stage内部的流水线式计算模式。</strong></p><p><strong>在Spark中，内存计算有两层含义：第一层含义就是众所周知的分布式数据缓存，第二层含义是Stage内的流水线式计算模式</strong>。关于RDD缓存的工作原理，我会在后续的课程中为你详细介绍，今天咱们重点关注内存计算的第二层含义就可以了。</p><h2>第二层含义：Stage内的流水线式计算模式</h2><p>很显然，要弄清楚内存计算的第二层含义，咱们得从DAG的Stages划分说起。在这之前，我们先来说说什么是DAG。</p><h3>什么是DAG？</h3><p>DAG全称Direct Acyclic Graph，中文叫有向无环图。顾名思义，DAG 是一种“图”。我们知道，任何一种图都包含两种基本元素：顶点（Vertex）和边（Edge），顶点通常用于表示实体，而边则代表实体间的关系。<strong>在Spark的DAG中，顶点是一个个RDD，边则是RDD之间通过dependencies属性构成的父子关系。</strong></p><p>从理论切入去讲解DAG，未免枯燥乏味，所以我打算借助上一讲土豆工坊的例子，来帮助你直观地认识DAG。上一讲，土豆工坊成功地实现了同时生产 3 种不同尺寸的桶装“原味”薯片。但是，在将“原味”薯片推向市场一段时间以后，工坊老板发现季度销量直线下滑，不由得火往上撞、心急如焚。此时，工坊的工头儿向他建议：“老板，咱们何不把流水线稍加改造，推出不同风味的薯片，去迎合市场大众的多样化选择？”然后，工头儿把改装后的效果图交给老板，老板看后甚是满意。</p><p><img src=\"https://static001.geekbang.org/resource/image/3a/86/3a7f115eaa6c2c307f80e3616e7e9c86.jpg?wh=11713*5238\" alt=\"\" title=\"土豆工坊流水线效果图\"></p><p>不过，改造流水线可是个大工程，为了让改装工人能够高效协作，工头儿得把上面的改造设想抽象成一张施工流程图。有了这张蓝图，工头儿才能给负责改装的工人们分工，大伙儿才能拧成一股绳、劲儿往一处使。在上一讲中，我们把食材形态类比成RDD，把相邻食材形态的关系看作是RDD间的依赖，那么显然，流水线的施工流程图就是DAG。</p><p><img src=\"https://static001.geekbang.org/resource/image/25/75/25a9c00533032886c00c23a351ac9a75.jpg?wh=4861*2364\" alt=\"\" title=\"DAG：土豆工坊流水线的设计流程图\"></p><p>因为DAG中的每一个顶点都由RDD构成，对应到上图中就是带泥的土豆potatosRDD，清洗过的土豆cleanedPotatosRDD，以及调料粉flavoursRDD等等。DAG的边则标记了不同RDD之间的依赖与转换关系。很明显，上图中DAG的每一条边都有指向性，而且整张图不存在环结构。</p><p>那DAG是怎么生成的呢？</p><p>我们都知道，在Spark的开发模型下，应用开发实际上就是灵活运用算子实现业务逻辑的过程。开发者在分布式数据集如RDD、 DataFrame或Dataset之上调用算子、封装计算逻辑，这个过程会衍生新的子RDD。与此同时，子RDD会把dependencies属性赋值到父RDD，把compute属性赋值到算子封装的计算逻辑。以此类推，在子RDD之上，开发者还会继续调用其他算子，衍生出新的RDD，如此往复便有了DAG。</p><p>因此，<strong>从开发者的视角出发，DAG的构建是通过在分布式数据集上不停地调用算子来完成的</strong>。</p><h3>Stages的划分</h3><p>现在，我们知道了什么是DAG，以及DAG是如何构建的。不过，DAG毕竟只是一张流程图，Spark需要把这张流程图转化成分布式任务，才能充分利用分布式集群并行计算的优势。这就好比土豆工坊的施工流程图毕竟还只是蓝图，是工头儿给老板画的一张“饼”，工头儿得想方设法把它转化成实实在在的土豆加工流水线，让流水线能够源源不断地生产不同风味的薯片，才能解决老板的燃眉之急。</p><p>简单地说，从开发者构建DAG，到DAG转化的分布式任务在分布式环境中执行，其间会经历如下4个阶段：</p><ul>\n<li>回溯DAG并划分Stages</li>\n<li>在Stages中创建分布式任务</li>\n<li>分布式任务的分发</li>\n<li>分布式任务的执行</li>\n</ul><p>刚才我们说了，内存计算的第二层含义在stages内部，因此这一讲我们只要搞清楚DAG是怎么划分Stages就够了。至于后面的3个阶段更偏向调度系统的范畴，所以我会在下一讲给你讲清楚其中的来龙去脉。</p><p>如果用一句话来概括从DAG到Stages的转化过程，那应该是：<strong>以Actions算子为起点，从后向前回溯DAG，以Shuffle操作为边界去划分Stages</strong>。</p><p>接下来，我们还是以土豆工坊为例来详细说说这个过程。既然DAG是以Shuffle为边界去划分Stages，我们不妨先从上帝视角出发，看看在土豆工坊设计流程图的DAG中，都有哪些地方需要执行数据分发的操作。当然，在土豆工坊，数据就是各种形态的土豆和土豆片儿。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/5f/3fcb3e400db91198a7499c016ccfb45f.jpg?wh=4674*1786\" alt=\"\" title=\"DAG以Shuffle为边界划分出3个Stages\"></p><p>仔细观察上面的设计流程图，我们不难发现，有两个地方需要分发数据。第一个地方是薯片经过烘焙烤熟之后，把即食薯片按尺寸大小分发到下游的流水线上，这些流水线会专门处理固定型号的薯片，也就是图中从bakedChipsRDD到flavouredBakedChipsRDD的那条线。同理，不同的调料粉也需要按照风味的不同分发到下游的流水线上，用于和固定型号的即食薯片混合，也就是图中从flavoursRDD到flavouredBakedChipsRDD那条分支。</p><p>同时，我们也能发现，土豆工坊的DAG应该划分3个Stages出来，如图中所示。其中，Stage 0包含四个RDD，从带泥土豆potatosRDD到即食薯片bakedChipsRDD。Stage 1比较简单，它只有一个RDD，就是封装调味粉的flavoursRDD。Stage 2包含两个RDD，一个是加了不同风味的即食薯片flavouredBakedChipsRDD，另一个表示组装成桶已经准备售卖的桶装薯片bucketChipsRDD。</p><p>你可能会问：“费了半天劲，把DAG变成Stages有啥用呢？”还真有用！内存计算的第二层含义，就隐匿于从DAG划分出的一个又一个Stages之中。不过，要弄清楚Stage内的流水线式计算模式，我们还是得从Hadoop MapReduce的计算模型说起。</p><h3>Stage中的内存计算</h3><p>基于内存的计算模型并不是凭空产生的，而是根据前人的教训和后人的反思精心设计出来的。这个前人就是Hadoop MapReduce，后人自然就是Spark。</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/d7/fbb396536260f43c8764a8e6452a4fd7.jpg?wh=4454*1635\" alt=\"\" title=\"Hadoop MapReduce的计算模型\"></p><p>MapReduce提供两类计算抽象，分别是Map和Reduce：Map抽象允许开发者通过实现map 接口来定义数据处理逻辑；Reduce抽象则用于封装数据聚合逻辑。MapReduce计算模型最大的问题在于，所有操作之间的数据交换都以磁盘为媒介。例如，两个Map操作之间的计算，以及Map与Reduce操作之间的计算都是利用本地磁盘来交换数据的。不难想象，这种频繁的磁盘I/O必定会拖累用户应用端到端的执行性能。</p><p>那么，这和Stage内的流水线式计算模式有啥关系呢？我们再回到土豆工坊的例子中，把目光集中在即食薯片分发之前，也就是刚刚划分出来的Stage 0。这一阶段包含3个处理操作，即清洗、切片和烘焙。按常理来说，流水线式的作业方式非常高效，带泥土豆被清洗过后，会沿着流水线被传送到切片机，切完的生薯片会继续沿着流水线再传送到烘焙烤箱，整个过程一气呵成。如果把流水线看作是计算节点内存的话，那么清洗、切片和烘焙这3个操作都是在内存中完成计算的。</p><p><img src=\"https://static001.geekbang.org/resource/image/6e/3b/6e9863b69aca6072b81e6d8e6826903b.jpg?wh=2496*1041\" alt=\"\" title=\"Stage 0包含清洗、切片、烘焙3个操作\"></p><p>你可能会说：“内存计算也不过如此，跟MapReduce相比，不就是把数据和计算都挪到内存里去了吗？”事情可能并没有你想象的那么简单。</p><p>在土豆工坊的例子里，Stage 0中的每个加工环节都会生产出中间食材，如清洗过的土豆、土豆片、即食薯片。我们刚刚把流水线比作内存，这意味着每一个算子计算得到的中间结果都会在内存中缓存一份，以备下一个算子运算，这个过程与开发者在应用代码中滥用RDD cache简直如出一辙。如果你曾经也是逢RDD便cache，应该不难想象，采用这种计算模式，Spark的执行性能不见得比MapReduce强多少，尤其是在Stages中的算子数量较多的时候。</p><p>既然不是简单地把数据和计算挪到内存，那Stage内的流水线式计算模式到底长啥样呢？在Spark中，<strong>流水线计算模式指的是：在同一Stage内部，所有算子融合为一个函数，Stage的输出结果由这个函数一次性作用在输入数据集而产生</strong>。这也正是内存计算的第二层含义。下面，我们用一张图来直观地解释这一计算模式。</p><p><img src=\"https://static001.geekbang.org/resource/image/03/03/03052d8fc98dcf1740ec4a7c29234403.jpg?wh=4816*1786\" alt=\"\" title=\"内存计算的第二层含义\"></p><p>如图所示，在上面的计算流程中，如果你把流水线看作是内存，每一步操作过后都会生成临时数据，如图中的clean和slice，这些临时数据都会缓存在内存里。但在下面的内存计算中，所有操作步骤如clean、slice、bake，都会被捏合在一起构成一个函数。这个函数一次性地作用在“带泥土豆”上，直接生成“即食薯片”，在内存中不产生任何中间数据形态。</p><p><strong>因此你看，所谓内存计算，不仅仅是指数据可以缓存在内存中，更重要的是让我们明白了，通过计算的融合来大幅提升数据在内存中的转换效率，进而从整体上提升应用的执行性能。</strong></p><p>这个时候，我们就可以回答开头提出的第二个问题了：费劲去重写代码、消除Shuffle，能有多大的性能收益？</p><p>由于计算的融合只发生在Stages内部，而Shuffle是切割Stages的边界，因此一旦发生Shuffle，内存计算的代码融合就会中断。但是，当我们对内存计算有了多方位理解以后，就不会一股脑地只想到用cache去提升应用的执行性能，而是会更主动地想办法尽量避免Shuffle，让应用代码中尽可能多的部分融合为一个函数，从而提升计算效率。</p><h2>小结</h2><p>这一讲，我们以两个常见的现象为例，探讨了Spark内存计算的含义。</p><p>在Spark中，内存计算有两层含义：第一层含义就是众所周知的分布式数据缓存，第二层含义是Stage内的流水线式计算模式。</p><p>对于第二层含义，我们需要先搞清楚DAG和Stages划分，从开发者的视角出发，DAG的构建是通过在分布式数据集上不停地调用算子来完成的，DAG以Actions算子为起点，从后向前回溯，以Shuffle操作为边界，划分出不同的Stages。</p><p>最后，我们归纳出内存计算更完整的第二层含义：同一Stage内所有算子融合为一个函数，Stage的输出结果由这个函数一次性作用在输入数据集而产生。</p><h2>每日一练</h2><p>今天的内容重在理解，我希望你能结合下面两道思考题来巩固一下。</p><ol>\n<li>\n<p>我们今天说了，DAG以Shuffle为边界划分Stages，那你知道Spark是根据什么来判断一个操作是否会引入Shuffle的呢？</p>\n</li>\n<li>\n<p>在Spark中，同一Stage内的所有算子会融合为一个函数。你知道这一步是怎么做到的吗？</p>\n</li>\n</ol><p>期待在留言区看到你的思考和答案，如果对内存计算还有很多困惑，也欢迎你写在留言区，我们下一讲见！</p>","neighbors":{"left":{"article_title":"03 | RDD：为什么你必须要理解弹性分布式数据集？","id":353297},"right":{"article_title":"05 | 调度系统：“数据不动代码动”到底是什么意思？","id":355028}},"comments":[{"had_liked":false,"id":284631,"user_name":"来世愿做友人 A","can_delete":false,"product_type":"c1","uid":1181606,"ip_address":"","ucode":"EF20966B0F27E1","user_header":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","comment_is_top":false,"comment_ctime":1616395557,"is_pvip":false,"replies":[{"id":"103218","content":"Perfect！答得已经很完美了，不过咱们再进一步，第二题，假设不是rdd API，而是dataframe、dataset，spark对于同一个stage内的算子，会有哪些优化呢？","user_name":"作者回复","comment_id":284631,"uid":"1043100","ip_address":"","utype":1,"ctime":1616420375,"user_name_real":"吴磊"}],"discussion_count":7,"race_medal":0,"score":"164825152805","product_id":100073401,"comment_content":"问题1: rdd 会有 dep 属性，用来区分是否是 shuffle 生成的 rdd. 而 dep 属性的确定主要是根据子 rdd 是否依赖父 rdd 的某一部分数据，这个就得看他两的分区器(如果 tranf&#47;action 有的话)。如果分区器一致，就不会产生 shuffle。<br>问题2: 在 task 启动后，会调用 rdd iterator 进行算子链的递归生成，调用 stage 图中最后一个 rdd 的 compute 方法，一般如果是 spark 提供的 rdd，compute 函数大都会继续调用父 rdd 的 iterator 方法，直到到 stage 的根 rdd，一般都是 sourceRdd，比如 hadoopRdd，KakaRdd，就会返回 source iterator。开始返回，如果子rdd 是 map 转换的，就会组成 itr.map(f)。如果再下一个是 filter 转换，就会组成 itr.map(f1).filter(f2)，以此类推。不知道这边理解对不对，有点绕","like_count":38,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517413,"discussion_content":"Perfect！答得已经很完美了，不过咱们再进一步，第二题，假设不是rdd API，而是dataframe、dataset，spark对于同一个stage内的算子，会有哪些优化呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616420375,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2415368,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DLcTlSKOQlrhRq1hzBNvnWfENsyFrxNnhJ5UPibPMLazy9c2nBlSd1sxHqzHaOTTaZIYkEDAby3HpdianMxt6Dsw/132","nickname":"Joey","note":"","ucode":"6856FA3A28B32C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":547145,"discussion_content":"这个问题：字节面试的时候问到过，我回答的是，Catalyst会对抽象语法树生成的逻辑计划进行优化，比如说谓词下推，会尽量的将fliter下推到数据源，减少计算的数据量，而rdd却很难做到；当然这这是一个点，还请老师补充","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642558608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":517413,"ip_address":""},"score":547145,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366423,"discussion_content":"好问题哈，单就rdd算子来说，在同一个stage内部，spark不会真的去创建、合成一个函数链，而是通过不同rdd算子Iterator的嵌套，在逻辑上，形成一个函数链。这里我们说“捏合”，坦白说不够严谨，不过重点在于表达“内存计算”的第二层含义。在后面要讲的tungsten，它是真的真的会把一个stage内部的code，在运行时on the fly地重构出一份新的代码出来。这两者有本质的不同。","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1618057599,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361124,"discussion_content":"这个的确没考虑过。这个应该涉及到 Catalyst 的相关优化，ds 和 df 带有相关的 schema，猜测应该和序列化存储开销有点关系。后面带着这个问题继续跟进，感谢老师","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616596719,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362547,"discussion_content":"是的，后面的课程会有讲哈～","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616980807,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":361124,"ip_address":""},"score":362547,"extra":""},{"author":{"id":1705386,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/usX58SweWDFqjCmtkvPOWIfjqN2GqydQYqW53bIcFI4DGBmp6O2LZxZL1UYsVPRuEP03dEJcK3d9jHdYZVn8ug/132","nickname":"maben996","note":"","ucode":"95390116090D23","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":366005,"discussion_content":"磊哥，“来世愿做友人 A” 的答案还是有点疑惑，意思是在同一个stage的函数最后会合成一个函数链吗？itr.map(f1).filter(f2) 类似于这种？那不同stage最后会有多个函数链? itr1.map(f1).filter(f2)  和 itr2.map(f1).filter(f2) 这样吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617939304,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362547,"ip_address":""},"score":366005,"extra":""},{"author":{"id":1418226,"avatar":"https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg","nickname":"Sampson","note":"","ucode":"BA78CA29A6D898","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1705386,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/usX58SweWDFqjCmtkvPOWIfjqN2GqydQYqW53bIcFI4DGBmp6O2LZxZL1UYsVPRuEP03dEJcK3d9jHdYZVn8ug/132","nickname":"maben996","note":"","ucode":"95390116090D23","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":542861,"discussion_content":"我也想知道这个问题，请教下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640866043,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":366005,"ip_address":""},"score":542861,"extra":""}]}]},{"had_liked":false,"id":284887,"user_name":"Shockang","can_delete":false,"product_type":"c1","uid":1263546,"ip_address":"","ucode":"B871D731F6E6B1","user_header":"https://static001.geekbang.org/account/avatar/00/13/47/ba/d36340c1.jpg","comment_is_top":false,"comment_ctime":1616512826,"is_pvip":false,"replies":[{"id":"103358","content":"说得太好了！以史为鉴知兴替，就是这个道理。在纵向上把视角拉高，其实就更容易理解很多新概念、新办法、新框架、新引擎。说的真好，后续多讨论哈～","user_name":"作者回复","comment_id":284887,"uid":"1043100","ip_address":"","utype":1,"ctime":1616549956,"user_name_real":"吴磊"}],"discussion_count":6,"race_medal":0,"score":"121875597114","product_id":100073401,"comment_content":"正如老师在文章里面提到的一样，Hadoop MapReduce使用硬盘来存储中间结果，而 Spark自从诞生以来就一直标榜自己是内存计算，可能有些同学会比较奇怪，为什么内存明显比硬盘快，MR 不去选择内存计算，实际上 MR 也有在使用内存的，比如环形缓冲区的存在就可以说明，之所以这样做，一个很重要的原因是 MR 诞生的年代（04 年）内存比较贵，后来随着科技发展，内存价格在不断下降，大家如果仔细研究就会发现比如 Spark 比如 Redis 等充分利用内存来计算的框架都是 10 年左右出现的，就是在这个时候内存价格开始大幅度下降的。我之所以说这么多，其实就想说明，事物的发展都是有规律的，大数据的背后也潜藏着各种规律，把握好这些规律，个人认为对于理解记忆各种不同的大数据技术都是很有帮助的。","like_count":28,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517502,"discussion_content":"说得太好了！以史为鉴知兴替，就是这个道理。在纵向上把视角拉高，其实就更容易理解很多新概念、新办法、新框架、新引擎。说的真好，后续多讨论哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616549956,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2066559,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/88/7f/97459eff.jpg","nickname":"站在桥上看风景","note":"","ucode":"09960C9E1C1C38","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390176,"discussion_content":"对内存昂贵的观点赞同，当年Google设计MR时使用的集群也在跑比MR更重要的应用，常常占用MR资源，为了解决MR因此导致的任务失败数据丢失，所以频繁的写磁盘。现在Google也推出了类似spark内存计算的MR","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629703408,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1051904,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","nickname":"果子","note":"","ucode":"0A669B6DE26F21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367535,"discussion_content":"还有mr落盘一个重要因数考虑的是稳定性，讲究一步一个脚印，mr世界观是认为数据量非常大的，当你数据达到一定程度时，可能得换mr来跑。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618388877,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1051904,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","nickname":"果子","note":"","ucode":"0A669B6DE26F21","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":403614,"discussion_content":"认同","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634117397,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":367535,"ip_address":""},"score":403614,"extra":""}]},{"author":{"id":1271157,"avatar":"https://static001.geekbang.org/account/avatar/00/13/65/75/f9d7e8b7.jpg","nickname":"L3nvy","note":"","ucode":"0B74B27C121D56","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363039,"discussion_content":"学习了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617102440,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362011,"discussion_content":"是的哦，刚才我就在想MapReduce中间的过程也会有内存的使用，为啥就不是内存计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616825093,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284588,"user_name":"Bennan","can_delete":false,"product_type":"c1","uid":2427818,"ip_address":"","ucode":"8B6EE3DCBB1356","user_header":"https://static001.geekbang.org/account/avatar/00/25/0b/aa/09c1215f.jpg","comment_is_top":false,"comment_ctime":1616376556,"is_pvip":false,"replies":[{"id":"103223","content":"好问题，我认为算的，多个操作在内存中完成统一的数据转换，我认为这就是内存计算。mr不同的map任务之间也是需要落盘的哟～ 更何况，同一stage内部，spark还有wscg这种优化，因此即便是同一个map stage之间的比拼，效率上spark也会比mr更好。","user_name":"作者回复","comment_id":284588,"uid":"1043100","ip_address":"","utype":1,"ctime":1616421321,"user_name_real":"吴磊"}],"discussion_count":7,"race_medal":0,"score":"35976114924","product_id":100073401,"comment_content":"内存计算的第二层含义真的算内存计算吗，mr不是也可以把spark的多个map操作放到一个map任务吗，我认为只是在api层面spark更简单","like_count":8,"discussions":[{"author":{"id":1051904,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","nickname":"果子","note":"","ucode":"0A669B6DE26F21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367548,"discussion_content":"spark优于mr的原因感觉更像是其map阶段的迭代式计算模型，mr多个map之间是要落盘的，spark这种迭代计算是直接通过内存来计算的，有点像IO流的计算模型，类似于装饰模式。spark判断宽窄依赖实际上是通过shuffle算子来判断的比如reducebykey等等这些聚合算子。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1618389691,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1051904,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PNmB3mOQ4jTSoDMGPwp5j8a2NL1Mibu4iaebBNnuDQltb2yZ3sygJpxTHuLrG9ewCDLEialorSK3pzXBCQ3JFCZPA/132","nickname":"果子","note":"","ucode":"0A669B6DE26F21","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377039,"discussion_content":"说得好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622471046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":367548,"ip_address":""},"score":377039,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517396,"discussion_content":"好问题，我认为算的，多个操作在内存中完成统一的数据转换，我认为这就是内存计算。mr不同的map任务之间也是需要落盘的哟～ 更何况，同一stage内部，spark还有wscg这种优化，因此即便是同一个map stage之间的比拼，效率上spark也会比mr更好。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616421321,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":3046392,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/ugib9sF9icd9dhibQoAA025hibbD5zgZTiaddLoeEH457hrkBBhtQK6qknTWt270rHCtBZqeqsbibtHghgjdkPx3DyIw/132","nickname":"唐方刚","note":"","ucode":"93DA58C3DCCF1B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":584879,"discussion_content":"一个MR任务的多个map任务之间不是互相独立的吗？map和map之间哪里有落盘","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661177956,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":517396,"ip_address":"广东"},"score":584879,"extra":""}]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362012,"discussion_content":"老板，什么是wscg","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616825179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":362548,"discussion_content":"whole stage code gen","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616980843,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362012,"ip_address":""},"score":362548,"extra":""}]},{"author":{"id":2010843,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/cojb2AA3eM620kb7hj7YoG8k56TKsdCmVletmYKYwibickH5Ced8UyxicpY9icZEM2ZTcqyUaEk2PRmH1FVLtGTggw/132","nickname":"orangelin","note":"","ucode":"730CC9997C16F2","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360174,"discussion_content":"mr不是只能 map-reduce吗，多个有依赖的任务需要多个job进程来执行把","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616381987,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284593,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1616378536,"is_pvip":false,"replies":[{"id":"103220","content":"答得挺好～ 追问一句哈，第一题，spark怎么判断一个dependency是不是shuffle Dependency呢？","user_name":"作者回复","comment_id":284593,"uid":"1043100","ip_address":"","utype":1,"ctime":1616420683,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"31681149608","product_id":100073401,"comment_content":"问题一：每个rdd会有个dependencies的属性，deps记录的是该rdd与父rdd之间的依赖关系，deps类型是Seq[dependency], 如果dependency类型是shuffleDenpendency，那么spark就会视其操作为shuffle操作，然后进行stage的切割。<br><br>问题二：stage执行时，spark会调用该stage末尾rdd的iterator方法，然后iterator方法实现逻辑是：将该rdd的compute方法作用下父rdd的compute计算结果之上，从而得到该rdd的分区","like_count":7,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517399,"discussion_content":"答得挺好～ 追问一句哈，第一题，spark怎么判断一个dependency是不是shuffle Dependency呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616420683,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362599,"discussion_content":"可以参考“来世愿做友人 A”同学的答案哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616989745,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1137879,"avatar":"https://static001.geekbang.org/account/avatar/00/11/5c/d7/e4673fde.jpg","nickname":"October","note":"","ucode":"CEDA78F4A5F8B1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360416,"discussion_content":"某些算子会产生shufflerdd？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616428246,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291087,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620014474,"is_pvip":false,"replies":[{"id":"105482","content":"两道题答得都不错~ <br><br>1. 宽窄依赖和算子类型，确实是判定Shuffle的主要依据，不过，还需要结合数据本身的分布来看。比如，你可以搜搜Collocated Join，这种Join情况是不会引入Shuffle的哈~<br>2.WSCG这个提得很好，不过，这个是只有Spark SQL才能享受到的特性，也就是当你使用DataFrame、Dataset或是SQL进行开发的时候，才能享受到这个特性。对于纯粹的RDD API来说，所谓的“捏合”，其实是一种伪“捏合”，它是通过同一个Stage内部多个RDD算子compute函数嵌套的方式，来完成“捏合”。","user_name":"作者回复","comment_id":291087,"uid":"1043100","ip_address":"","utype":1,"ctime":1620116530,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"23094850954","product_id":100073401,"comment_content":"1.  DAG以Shuffle划分Stages，Shuffle的产生主要通过宽依赖和窄依赖，而宽窄依赖主要通过不同的算子来产生，比如产生窄依赖的算子：map，flatMap，filter，mapPartitions，union；产生宽依赖的算子：cogroup，join，groupyByKey，reduceByKey，combineByKey，distinct，repartition<br>2. 官网上看到过：WholeStageCodegen 全阶段代码生成将多个operators编译成一个Java函数来提升性能。","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519366,"discussion_content":"两道题答得都不错~ \n\n1. 宽窄依赖和算子类型，确实是判定Shuffle的主要依据，不过，还需要结合数据本身的分布来看。比如，你可以搜搜Collocated Join，这种Join情况是不会引入Shuffle的哈~\n2.WSCG这个提得很好，不过，这个是只有Spark SQL才能享受到的特性，也就是当你使用DataFrame、Dataset或是SQL进行开发的时候，才能享受到这个特性。对于纯粹的RDD API来说，所谓的“捏合”，其实是一种伪“捏合”，它是通过同一个Stage内部多个RDD算子compute函数嵌套的方式，来完成“捏合”。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620116530,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":302248,"user_name":"sparkjoy","can_delete":false,"product_type":"c1","uid":1000179,"ip_address":"","ucode":"827B3116EED3B8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/f3/e945e4ac.jpg","comment_is_top":false,"comment_ctime":1626146095,"is_pvip":false,"replies":[{"id":"109464","content":"是的~ 父子RDD的partitioner一致，就意味着他们会划分到同一个Stage~","user_name":"作者回复","comment_id":302248,"uid":"1043100","ip_address":"","utype":1,"ctime":1626250389,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"18806015279","product_id":100073401,"comment_content":"第一题，主要看父rdd的分区器是否一致，如果一致则生成子rdd的过程中不会产生shuffle","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523240,"discussion_content":"是的~ 父子RDD的partitioner一致，就意味着他们会划分到同一个Stage~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626250389,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287994,"user_name":"Wiggle Wiggle","can_delete":false,"product_type":"c1","uid":1036052,"ip_address":"","ucode":"EFE746551BA998","user_header":"https://static001.geekbang.org/account/avatar/00/0f/cf/14/384258ba.jpg","comment_is_top":false,"comment_ctime":1618245097,"is_pvip":false,"replies":[{"id":"104592","content":"非常好的问题，这个edge case非常有意思，我们来细说说~ 取决于你如何调用coalesce(1, shuffle = false&#47;true)，分两种情况。<br><br>1. shuffle = false，就像你说的，所有操作，从一开始，并行度都是1，都在一个executor计算，显然，这个时候，整个作业非常慢，奇慢无比<br><br>2. shuffle = true，这个时候，coalesce就会引入shuffle，切割stage。coalesce之前，用源数据DataFrame的并行度，这个时候是多个Executors真正的并行计算；coalesce之后，也就是shuffle之后，并行度下降为1，所有父RDD的分区，全部shuffle到一个executor，交给一个task去计算。显然，相比前一种，这种实现在执行效率上，更好一些。因此，如果业务应用必须要这么做，推荐这一种实现方法。","user_name":"作者回复","comment_id":287994,"uid":"1043100","ip_address":"","utype":1,"ctime":1618306925,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"14503146985","product_id":100073401,"comment_content":"说个最极端的情况，如果对一个dataframe Read以后做了一堆不会触发shuffle 的操作，最后又调用了一下coalesce(1)，然后write ，那是不是就意味着从读数据开始的所有操作都会在一个executor上完成？ ","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518481,"discussion_content":"非常好的问题，这个edge case非常有意思，我们来细说说~ 取决于你如何调用coalesce(1, shuffle = false/true)，分两种情况。\n\n1. shuffle = false，就像你说的，所有操作，从一开始，并行度都是1，都在一个executor计算，显然，这个时候，整个作业非常慢，奇慢无比\n\n2. shuffle = true，这个时候，coalesce就会引入shuffle，切割stage。coalesce之前，用源数据DataFrame的并行度，这个时候是多个Executors真正的并行计算；coalesce之后，也就是shuffle之后，并行度下降为1，所有父RDD的分区，全部shuffle到一个executor，交给一个task去计算。显然，相比前一种，这种实现在执行效率上，更好一些。因此，如果业务应用必须要这么做，推荐这一种实现方法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618306925,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1515374,"avatar":"https://static001.geekbang.org/account/avatar/00/17/1f/6e/7a6788f3.jpg","nickname":"爱吃猫的鱼","note":"","ucode":"494EA47546CF02","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":398857,"discussion_content":"假如要写回为一个文件，coalesce(1) shuffle=true 时顺序不保证，之前的排序失效。coalesce(1) shuffle=false，可保证结果，却又有性能问题。除了先 coalesce(1) 再排序再写回一个文件，还有其他的方案么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632845016,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":328474,"user_name":"Geek_18fe90","can_delete":false,"product_type":"c1","uid":2260535,"ip_address":"","ucode":"6BF21B093883BA","user_header":"","comment_is_top":false,"comment_ctime":1640748296,"is_pvip":false,"replies":[{"id":"119854","content":"Map阶段的并行度，会沿用父RDD的并行度，比如沿用HadoopRDD的并行度，这样的话，就是源文件原始的分片数量。Reduce阶段，可以通过repartition来调整，如果没有调整，默认按照spark.sql.shuffle.partitions来走~","user_name":"作者回复","comment_id":328474,"uid":"1043100","ip_address":"","utype":1,"ctime":1641003977,"user_name_real":"编辑"}],"discussion_count":1,"race_medal":0,"score":"10230682888","product_id":100073401,"comment_content":"spark shuffle前后的分区数是如何计算的","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":543194,"discussion_content":"Map阶段的并行度，会沿用父RDD的并行度，比如沿用HadoopRDD的并行度，这样的话，就是源文件原始的分片数量。Reduce阶段，可以通过repartition来调整，如果没有调整，默认按照spark.sql.shuffle.partitions来走~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641003977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285354,"user_name":"小学生敬亭山","can_delete":false,"product_type":"c1","uid":1239584,"ip_address":"","ucode":"4879D8BBF74913","user_header":"https://static001.geekbang.org/account/avatar/00/12/ea/20/78ab5f92.jpg","comment_is_top":false,"comment_ctime":1616750012,"is_pvip":false,"replies":[{"id":"103558","content":"好问题，先来回答你的问题：<br><br>1. 不是数据大于内存就会溢出到磁盘，取决于分片大小和每个task的可用内存。这部分在cpu、内存视角那几讲会详细展开，怎么平衡并行度、线程池、内存消耗。到时候可以关注一下哈～<br><br>2. shuffle的过程确实有落盘的步骤，但也仅限shuffle操作。stage内部是流水线式的内存计算，不会有落盘的动作。","user_name":"作者回复","comment_id":285354,"uid":"1043100","ip_address":"","utype":1,"ctime":1616756970,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"10206684604","product_id":100073401,"comment_content":"老师您好，我请教个问题。既然是大数据，那么假设数据很大，无论怎么分区或者分布式，单个机器的内存都放不下，那这个时候spark是怎么计算的呢？必然会有一部分在磁盘一部分在内存吧，这种情况spark是如何避免落盘，如何提升效率的呢。","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517653,"discussion_content":"好问题，先来回答你的问题：\n\n1. 不是数据大于内存就会溢出到磁盘，取决于分片大小和每个task的可用内存。这部分在cpu、内存视角那几讲会详细展开，怎么平衡并行度、线程池、内存消耗。到时候可以关注一下哈～\n\n2. shuffle的过程确实有落盘的步骤，但也仅限shuffle操作。stage内部是流水线式的内存计算，不会有落盘的动作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616756970,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286550,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1617369900,"is_pvip":false,"replies":[{"id":"104088","content":"两道题都回答得挺完美～ expr代码生成是tungsten比较早期的，我理解是一种铺垫和过度，目的是辅助最终的wscg。单单是expr code gen，提升不了多少性能。","user_name":"作者回复","comment_id":286550,"uid":"1043100","ip_address":"","utype":1,"ctime":1617430682,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5912337196","product_id":100073401,"comment_content":"1.DAG以shuffle划分stage;  判断shuffle的依据是  rdd的deps属性是narrowDeps还是shuffleDeps;  deps类型怎么得来的，肯定是构造rdd时生成的;构造rdd时依据什么来生成不同类型的deps呢，这块还没深究，猜测是根据算子类型，比如window func或者aggregator。<br>2.所有算子融合到一起是通过全阶段代码生成。如果不能进行全阶段代码生成就进行基本表达式代码生成，但基本表达式代码生成每个算子处理逻辑还是分开的，所以磊哥能解释下仅仅进行基本表达式代码生成好处在哪嘛","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518022,"discussion_content":"两道题都回答得挺完美～ expr代码生成是tungsten比较早期的，我理解是一种铺垫和过度，目的是辅助最终的wscg。单单是expr code gen，提升不了多少性能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617430682,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285352,"user_name":"小学生敬亭山","can_delete":false,"product_type":"c1","uid":1239584,"ip_address":"","ucode":"4879D8BBF74913","user_header":"https://static001.geekbang.org/account/avatar/00/12/ea/20/78ab5f92.jpg","comment_is_top":false,"comment_ctime":1616749838,"is_pvip":false,"replies":[{"id":"103559","content":"第一题答得不错，追问：spark怎么判断1对多、多对多呢？<br><br>第二题很多点都说的很好，比如链式调用、catalyst优化、tungsten，不过逻辑有些混乱，可以关注后面的catalyst优化器和tungsten这两讲，把这里的思路理清楚哈～<br>","user_name":"作者回复","comment_id":285352,"uid":"1043100","ip_address":"","utype":1,"ctime":1616757703,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5911717134","product_id":100073401,"comment_content":"问题1:逻辑层面上，如果聚焦在当前节点，看前1个节点和当前节点的关系。存在1对1，1对多，多对1，多对多四种可能。所谓shuffle就是不能链式调用了，需要用到上一步的多个节点。可以理解成上一步的数据要交出来混在一起又重新发出去。因此发生了网络传输或者落盘。多对1和多对多可能会shuffle.代码实现层面就是有 依赖关系可以在stage回溯的时候可以用。<br>问题2.能融合得益于函数式编程的思想，可以链式调用，然后通过生成类似于树结构的语法分析，然后生成逻辑执行计划，物理执行计划。spark有所谓&quot;钨丝计划&quot;。然后更深入的优化内容，那我就说不清楚了。","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517652,"discussion_content":"第一题答得不错，追问：spark怎么判断1对多、多对多呢？\n\n第二题很多点都说的很好，比如链式调用、catalyst优化、tungsten，不过逻辑有些混乱，可以关注后面的catalyst优化器和tungsten这两讲，把这里的思路理清楚哈～\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616757703,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284795,"user_name":"Elon","can_delete":false,"product_type":"c1","uid":1004727,"ip_address":"","ucode":"DB5BE4DA9A1690","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/b7/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1616476350,"is_pvip":false,"replies":[{"id":"103357","content":"哈哈，喜欢就好哈～","user_name":"作者回复","comment_id":284795,"uid":"1043100","ip_address":"","utype":1,"ctime":1616549713,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5911443646","product_id":100073401,"comment_content":"不得不说，这个土豆的例子可是太棒了～","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517471,"discussion_content":"哈哈，喜欢就好哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616549713,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":360257,"user_name":"Geek_853ebe","can_delete":false,"product_type":"c1","uid":2879657,"ip_address":"福建","ucode":"30ED5730F39F69","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6Nic6V6iawbbH0UZKtNv9SOMSED5xIQjeq6wgFuia8D4HUNQYHQdn1BOTMOrDuUCddaiaV5Nmmw8RDg/132","comment_is_top":false,"comment_ctime":1666342907,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666342907","product_id":100073401,"comment_content":"路人转粉，老师怎么加群呀","like_count":0},{"had_liked":false,"id":360256,"user_name":"Geek_853ebe","can_delete":false,"product_type":"c1","uid":2879657,"ip_address":"福建","ucode":"30ED5730F39F69","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6Nic6V6iawbbH0UZKtNv9SOMSED5xIQjeq6wgFuia8D4HUNQYHQdn1BOTMOrDuUCddaiaV5Nmmw8RDg/132","comment_is_top":false,"comment_ctime":1666342843,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666342843","product_id":100073401,"comment_content":"老师，请教下DataSet Api中的union一定会让数据变成一个分区吗？<br>在线上有个卡死的任务，十几个select union在一起，看卡住的stage中，数据倾斜到了一个task中，其他task空跑，看时间线这个job中其他的action都是并发跑的很快。","like_count":0},{"had_liked":false,"id":350930,"user_name":"松花酿酒，春水煎茶","can_delete":false,"product_type":"c1","uid":1947809,"ip_address":"","ucode":"843A6AB55FCF99","user_header":"https://static001.geekbang.org/account/avatar/00/1d/b8/a1/019574ad.jpg","comment_is_top":false,"comment_ctime":1657355144,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1657355144","product_id":100073401,"comment_content":"问题一：分算子，根据宽窄依赖或者父子RDD partitioner来判断；","like_count":0},{"had_liked":false,"id":288835,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1618723247,"is_pvip":false,"replies":[{"id":"104857","content":"df.cache()<br>df.count<br><br>或是<br>val cacheDf = df.cache()<br>cacheDf.count<br><br>都可以，action是必需的，没有action，不会触发缓存的计算和存储，这可不是画蛇添足哈~<br>","user_name":"作者回复","comment_id":288835,"uid":"1043100","ip_address":"","utype":1,"ctime":1618761056,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1618723247","product_id":100073401,"comment_content":"请问下老师，spark里cache的正确姿势是什么？<br>是直接df.cache()还是val cacheDf = df.cache()呢？另外不管cache还是persist都是lazy的，所以有必要紧接着一句df.count()让它马上执行吗？因为这样会平白无故多一个job，不知道是不是画蛇添足了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518746,"discussion_content":"df.cache()\ndf.count\n\n或是\nval cacheDf = df.cache()\ncacheDf.count\n\n都可以，action是必需的，没有action，不会触发缓存的计算和存储，这可不是画蛇添足哈~\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618761056,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285741,"user_name":"Gti","can_delete":false,"product_type":"c1","uid":1504510,"ip_address":"","ucode":"21995DFB1AAFB3","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/t1aR8h117KxusZQHQ9urp6hr3jMA9icnWR3tLlYZ5M1wbgXIqRTKfLHJ9iciaTgliaPhfV5s5fYrARMZySKHltMlUg/132","comment_is_top":false,"comment_ctime":1616998598,"is_pvip":false,"replies":[{"id":"103696","content":"map是写到本地盘啊，没说写到hdfs啊","user_name":"作者回复","comment_id":285741,"uid":"1043100","ip_address":"","utype":1,"ctime":1617014381,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"1616998598","product_id":100073401,"comment_content":"map的结果不是都写到本地磁盘吗？reducer从hdfs去mapper的结果？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517770,"discussion_content":"map是写到本地盘啊，没说写到hdfs啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617014381,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2707626,"avatar":"https://static001.geekbang.org/account/avatar/00/29/50/aa/dca4eb39.jpg","nickname":"慎始敬终","note":"","ucode":"4697B3FF562542","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":581697,"discussion_content":"老师好！这个map指的是mapReduce的map还是Spark的shuffleMap阶段的map，这两种map都是写hdfs吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1658922788,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363656,"discussion_content":"确实是写的HDFS，我的锅，这里笔误写错了，稍后修正哈~ 感谢提醒~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617254175,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285533,"user_name":"Geek2014","can_delete":false,"product_type":"c1","uid":2028957,"ip_address":"","ucode":"9EB356D8DF287E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","comment_is_top":false,"comment_ctime":1616859616,"is_pvip":false,"replies":[{"id":"103653","content":"确实，mr的map阶段也能用多个算子，一来开发成本高，二来spark在同一个stage内部，还有whole stage code gen哈<br><br>追问一下：spark怎么判断宽窄依赖呢？","user_name":"作者回复","comment_id":285533,"uid":"1043100","ip_address":"","utype":1,"ctime":1616978919,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1616859616","product_id":100073401,"comment_content":"MR可以开发者自己手动在一个map方法里整合多个算子的功能啊，只是spark做了简化。<br><br>问题1主要就是宽窄依赖的问题<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517703,"discussion_content":"确实，mr的map阶段也能用多个算子，一来开发成本高，二来spark在同一个stage内部，还有whole stage code gen哈\n\n追问一下：spark怎么判断宽窄依赖呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616978919,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285457,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1616825001,"is_pvip":true,"replies":[{"id":"103599","content":"没问题，追问一下：spark怎么判断宽窄依赖呢？","user_name":"作者回复","comment_id":285457,"uid":"1043100","ip_address":"","utype":1,"ctime":1616839424,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"1616825001","product_id":100073401,"comment_content":"是否需要进行shuffle。当RDD和父RDD的依赖关系是宽依赖是，就会进行数据的shuffle.","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517680,"discussion_content":"没问题，追问一下：spark怎么判断宽窄依赖呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616839424,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363317,"discussion_content":"宽依赖就是子RDD的分区是由多个父rdd的分区计算而来。根据上面的回复，推断出来是RDD的dep关联判断。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617167602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284668,"user_name":"尔雅","can_delete":false,"product_type":"c1","uid":1366229,"ip_address":"","ucode":"848C3B0D8B5441","user_header":"https://static001.geekbang.org/account/avatar/00/14/d8/d5/00f31ac9.jpg","comment_is_top":false,"comment_ctime":1616410082,"is_pvip":false,"replies":[{"id":"103217","content":"你的意思是说：按某列分区，然后算lead吗？<br><br>类似于这种：<br>window = Window.orderBy(&quot;&quot;).partitionBy(&quot;&quot;)<br>df = df.withColumn(&#39;&#39;,lead(col(&#39;&#39;)).over(window))<br><br>如果是的话，那么一定会shuffle。因为你想，你是按照某列为单位计算aggregation，那么就需要先把同样key的数据分发到同一个进程，然后才能计算集合逻辑。因此一定会shuffle。<br><br>","user_name":"作者回复","comment_id":284668,"uid":"1043100","ip_address":"","utype":1,"ctime":1616420015,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"1616410082","product_id":100073401,"comment_content":"吴老师，请问lead 添加partition 算shuffle操作吗","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517426,"discussion_content":"你的意思是说：按某列分区，然后算lead吗？\n\n类似于这种：\nwindow = Window.orderBy(&amp;quot;&amp;quot;).partitionBy(&amp;quot;&amp;quot;)\ndf = df.withColumn(&amp;#39;&amp;#39;,lead(col(&amp;#39;&amp;#39;)).over(window))\n\n如果是的话，那么一定会shuffle。因为你想，你是按照某列为单位计算aggregation，那么就需要先把同样key的数据分发到同一个进程，然后才能计算集合逻辑。因此一定会shuffle。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616420015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1366229,"avatar":"https://static001.geekbang.org/account/avatar/00/14/d8/d5/00f31ac9.jpg","nickname":"尔雅","note":"","ucode":"848C3B0D8B5441","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360352,"discussion_content":"吴老师，非常感谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616422236,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284598,"user_name":"orangelin","can_delete":false,"product_type":"c1","uid":2010843,"ip_address":"","ucode":"730CC9997C16F2","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/cojb2AA3eM620kb7hj7YoG8k56TKsdCmVletmYKYwibickH5Ced8UyxicpY9icZEM2ZTcqyUaEk2PRmH1FVLtGTggw/132","comment_is_top":false,"comment_ctime":1616381776,"is_pvip":true,"replies":[{"id":"103222","content":"第一题逻辑没问题，不过，spark怎么判断打散还是不打散呢？<br><br>第二题可以参考freedom同学的答案哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616420954,"ip_address":"","comment_id":284598,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616381776","product_id":100073401,"comment_content":"问题1：是根据算子得依赖关系来判断的，如果是宽依赖则会划分阶段，如果是窄依赖则不会，如何判断宽窄依赖呢？本质是看父RDD分区是否被打散到子RDD中，如果打散则为宽依赖<br>问题2：不是很清楚具体怎么实现，请老师解答，我的理解是首先同一个阶段的函数必然都是窄依赖关系","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517402,"discussion_content":"第一题逻辑没问题，不过，spark怎么判断打散还是不打散呢？\n\n第二题可以参考freedom同学的答案哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616420954,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284591,"user_name":"天渡","can_delete":false,"product_type":"c1","uid":1322829,"ip_address":"","ucode":"B4D031E05E266E","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJrZb9pm07aicqH4rErIanMN5owmsBO6rWa6VSkpFMFjVRKOKkNM6JEf2gvQ2g23xUiammg7PUykJFA/132","comment_is_top":false,"comment_ctime":1616377993,"is_pvip":false,"replies":[{"id":"103219","content":"追问一下哈，spark怎么判断宽依赖还是窄依赖呢？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616420557,"ip_address":"","comment_id":284591,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616377993","product_id":100073401,"comment_content":"当RDD之间的依赖关系为宽依赖时，就会发生shuffle","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517398,"discussion_content":"追问一下哈，spark怎么判断宽依赖还是窄依赖呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616420557,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284590,"user_name":"老A","can_delete":false,"product_type":"c1","uid":1806678,"ip_address":"","ucode":"6B3BB0851583BD","user_header":"https://static001.geekbang.org/account/avatar/00/1b/91/56/f714ad14.jpg","comment_is_top":false,"comment_ctime":1616377990,"is_pvip":false,"replies":[{"id":"103221","content":"第一题逻辑是对的，不过，spark根据什么来判断这一点呢？😉<br><br>第二题，可以参考freedom同学的答案哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616420885,"ip_address":"","comment_id":284590,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1616377990","product_id":100073401,"comment_content":"1.根据一个分区的数据是否分发到下游RDD里的多个分区来判断是否有shuffle 。<br>2.应该是根据作用在RDD上的计算compute，然后根据函数式编程的调用链来实现的吧，请老师解答吧😀<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517397,"discussion_content":"第一题逻辑是对的，不过，spark根据什么来判断这一点呢？😉\n\n第二题，可以参考freedom同学的答案哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616420885,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1011838,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJaibVSzB9hD3OR2er8HnEzO7ib1soibw9DAKrwM5lIeaEC0rTlBjP6UsYDCPGS1ARnWMqccAsasAzicw/132","nickname":"呵呵鱼","note":"","ucode":"D7D752E22C9B6F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385047,"discussion_content":"一个分区的数据分发到下游多个分区，但是如果下游的一个分区仅来自于上游的一个分区，也就是一对多的情况是窄依赖吧？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626861809,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362600,"discussion_content":"可以参考“来世愿做友人 A”同学的答案哈，怀疑我之前看眼花了，但我明明记得有位freedom同学答的也不错\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616989826,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1581245,"avatar":"https://static001.geekbang.org/account/avatar/00/18/20/bd/5656b5d7.jpg","nickname":"走刀口 💰","note":"","ucode":"C7E2B812A4A02C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362512,"discussion_content":"freedom是谁，我在评论里没找到","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616973520,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}