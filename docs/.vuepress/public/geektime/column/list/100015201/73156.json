{"id":73156,"title":"50 | 让日志无处可逃：容器日志收集与管理","content":"<p>你好，我是张磊。今天我和你分享的主题是：让日志无处可逃之容器日志收集与管理。</p><p>在前面的文章中，我为你详细讲解了 Kubernetes 的核心监控体系和自定义监控体系的设计与实现思路。而在本篇文章里，我就来为你详细介绍一下Kubernetes 里关于容器日志的处理方式。</p><p>首先需要明确的是，Kubernetes 里面对容器日志的处理方式，都叫作cluster-level-logging，即：这个日志处理系统，与容器、Pod以及Node的生命周期都是完全无关的。这种设计当然是为了保证，无论是容器挂了、Pod 被删除，甚至节点宕机的时候，应用的日志依然可以被正常获取到。</p><p>而对于一个容器来说，当应用把日志输出到 stdout 和 stderr 之后，容器项目在默认情况下就会把这些日志输出到宿主机上的一个 JSON 文件里。这样，你通过 kubectl logs 命令就可以看到这些容器的日志了。</p><p>上述机制，就是我们今天要讲解的容器日志收集的基础假设。而如果你的应用是把文件输出到其他地方，比如直接输出到了容器里的某个文件里，或者输出到了远程存储里，那就属于特殊情况了。当然，我在文章里也会对这些特殊情况的处理方法进行讲述。</p><p>而 Kubernetes 本身，实际上是不会为你做容器日志收集工作的，所以为了实现上述cluster-level-logging，你需要在部署集群的时候，提前对具体的日志方案进行规划。而 Kubernetes 项目本身，主要为你推荐了三种日志方案。</p><!-- [[[read_end]]] --><p><strong>第一种，在 Node 上部署 logging agent，将日志文件转发到后端存储里保存起来。</strong>这个方案的架构图如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/b5/43/b5515aed076aa6af63ace55b62d36243.jpg?wh=2284*1610\" alt=\"\"></p><p>不难看到，这里的核心就在于 logging agent ，它一般都会以 DaemonSet 的方式运行在节点上，然后将宿主机上的容器日志目录挂载进去，最后由 logging-agent 把日志转发出去。</p><p>举个例子，我们可以通过 Fluentd 项目作为宿主机上的 logging-agent，然后把日志转发到远端的 ElasticSearch 里保存起来供将来进行检索。具体的操作过程，你可以通过阅读<a href=\"https://kubernetes.io/docs/user-guide/logging/elasticsearch\">这篇文档</a>来了解。另外，在很多 Kubernetes 的部署里，会自动为你启用 <a href=\"https://linux.die.net/man/8/logrotate\">logrotate</a>，在日志文件超过10MB 的时候自动对日志文件进行 rotate 操作。</p><p>可以看到，在 Node 上部署 logging agent最大的优点，在于一个节点只需要部署一个 agent，并且不会对应用和 Pod 有任何侵入性。所以，这个方案，在社区里是最常用的一种。</p><p>但是也不难看到，这种方案的不足之处就在于，它要求应用输出的日志，都必须是直接输出到容器的 stdout 和 stderr 里。</p><p>所以，<strong>Kubernetes 容器日志方案的第二种，就是对这种特殊情况的一个处理，即：当容器的日志只能输出到某些文件里的时候，我们可以通过一个 sidecar 容器把这些日志文件重新输出到 sidecar 的 stdout 和 stderr 上，这样就能够继续使用第一种方案了。</strong>这个方案的具体工作原理，如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/48/20/4863e3d7d1ef02a5a44e431369ac4120.jpg?wh=2284*1610\" alt=\"\"></p><p>比如，现在我的应用 Pod 只有一个容器，它会把日志输出到容器里的/var/log/1.log 和2.log 这两个文件里。这个 Pod 的 YAML 文件如下所示：</p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - &gt;\n      i=0;\n      while true;\n      do\n        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;\n        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;\n        i=$((i+1));\n        sleep 1;\n      done\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  volumes:\n  - name: varlog\n    emptyDir: {}\n</code></pre><p>在这种情况下，你用 kubectl logs 命令是看不到应用的任何日志的。而且我们前面讲解的、最常用的方案一，也是没办法使用的。</p><p>那么这个时候，我们就可以为这个 Pod 添加两个 sidecar容器，分别将上述两个日志文件里的内容重新以 stdout 和 stderr 的方式输出出来，这个 YAML 文件的写法如下所示：</p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - &gt;\n      i=0;\n      while true;\n      do\n        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;\n        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;\n        i=$((i+1));\n        sleep 1;\n      done\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-log-1\n    image: busybox\n    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-log-2\n    image: busybox\n    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  volumes:\n  - name: varlog\n    emptyDir: {}\n</code></pre><p>这时候，你就可以通过 kubectl logs 命令查看这两个 sidecar 容器的日志，间接看到应用的日志内容了，如下所示：</p><pre><code>$ kubectl logs counter count-log-1\n0: Mon Jan 1 00:00:00 UTC 2001\n1: Mon Jan 1 00:00:01 UTC 2001\n2: Mon Jan 1 00:00:02 UTC 2001\n...\n$ kubectl logs counter count-log-2\nMon Jan 1 00:00:00 UTC 2001 INFO 0\nMon Jan 1 00:00:01 UTC 2001 INFO 1\nMon Jan 1 00:00:02 UTC 2001 INFO 2\n...\n</code></pre><p>由于 sidecar 跟主容器之间是共享 Volume 的，所以这里的 sidecar 方案的额外性能损耗并不高，也就是多占用一点 CPU 和内存罢了。</p><p>但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是 sidecar 的 stdout 和 stderr 对应的 JSON 文件。这对磁盘是很大的浪费。所以说，除非万不得已或者应用容器完全不可能被修改，否则我还是建议你直接使用方案一，或者直接使用下面的第三种方案。</p><p><strong>第三种方案，就是通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去。</strong>也就是相当于把方案一里的 logging agent，放在了应用 Pod 里。这种方案的架构如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/d4/c7/d464401baec60c11f96dfeea3ae3a9c7.jpg?wh=2284*998\" alt=\"\"></p><p>在这种方案里，你的应用还可以直接把日志输出到固定的文件里而不是 stdout，你的 logging-agent 还可以使用 fluentd，后端存储还可以是 ElasticSearch。只不过， fluentd 的输入源，变成了应用的日志文件。一般来说，我们会把 fluentd 的输入源配置保存在一个 ConfigMap 里，如下所示：</p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\ndata:\n  fluentd.conf: |\n    &lt;source&gt;\n      type tail\n      format none\n      path /var/log/1.log\n      pos_file /var/log/1.log.pos\n      tag count.format1\n    &lt;/source&gt;\n    \n    &lt;source&gt;\n      type tail\n      format none\n      path /var/log/2.log\n      pos_file /var/log/2.log.pos\n      tag count.format2\n    &lt;/source&gt;\n    \n    &lt;match **&gt;\n      type google_cloud\n    &lt;/match&gt;\n</code></pre><p>然后，我们在应用 Pod 的定义里，就可以声明一个Fluentd容器作为 sidecar，专门负责将应用生成的1.log 和2.log转发到 ElasticSearch 当中。这个配置，如下所示：</p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: counter\nspec:\n  containers:\n  - name: count\n    image: busybox\n    args:\n    - /bin/sh\n    - -c\n    - &gt;\n      i=0;\n      while true;\n      do\n        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;\n        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;\n        i=$((i+1));\n        sleep 1;\n      done\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: count-agent\n    image: k8s.gcr.io/fluentd-gcp:1.30\n    env:\n    - name: FLUENTD_ARGS\n      value: -c /etc/fluentd-config/fluentd.conf\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n    - name: config-volume\n      mountPath: /etc/fluentd-config\n  volumes:\n  - name: varlog\n    emptyDir: {}\n  - name: config-volume\n    configMap:\n      name: fluentd-config\n</code></pre><p>可以看到，这个 Fluentd 容器使用的输入源，就是通过引用我们前面编写的 ConfigMap来指定的。这里我用到了 Projected Volume 来把 ConfigMap 挂载到 Pod 里。如果你对这个用法不熟悉的话，可以再回顾下第15篇文章<a href=\"https://time.geekbang.org/column/article/40466\">《 深入解析Pod对象（二）：使用进阶》</a>中的相关内容。</p><p>需要注意的是，这种方案虽然部署简单，并且对宿主机非常友好，但是这个 sidecar 容器很可能会消耗较多的资源，甚至拖垮应用容器。并且，由于日志还是没有输出到 stdout上，所以你通过 kubectl logs 是看不到任何日志输出的。</p><p>以上，就是 Kubernetes 项目对容器应用日志进行管理最常用的三种手段了。</p><h1>总结</h1><p>在本篇文章中，我为你详细讲解了Kubernetes 项目对容器应用日志的收集方式。综合对比以上三种方案，我比较建议你将应用日志输出到 stdout 和 stderr，然后通过在宿主机上部署 logging-agent 的方式来集中处理日志。</p><p>这种方案不仅管理简单，kubectl logs 也可以用，而且可靠性高，并且宿主机本身，很可能就自带了 rsyslogd 等非常成熟的日志收集组件来供你使用。</p><p>除此之外，还有一种方式就是在编写应用的时候，就直接指定好日志的存储后端，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/13/99/13e8439d9945fea58c9672fc4ca30799.jpg?wh=2284*705\" alt=\"\"></p><p>在这种方案下，Kubernetes 就完全不必操心容器日志的收集了，这对于本身已经有完善的日志处理系统的公司来说，是一个非常好的选择。</p><p>最后需要指出的是，无论是哪种方案，你都必须要及时将这些日志文件从宿主机上清理掉，或者给日志目录专门挂载一些容量巨大的远程盘。否则，一旦主磁盘分区被打满，整个系统就可能会陷入崩溃状态，这是非常麻烦的。</p><h1>思考题</h1><ol>\n<li>\n<p>请问，当日志量很大的时候，直接将日志输出到容器 stdout 和 stderr上，有没有什么隐患呢？有没有解决办法呢？</p>\n</li>\n<li>\n<p>你还有哪些容器收集的方案，是否可以分享一下？</p>\n</li>\n</ol><p>感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。</p><p><img src=\"https://static001.geekbang.org/resource/image/96/25/96ef8576a26f5e6266c422c0d6519725.jpg?wh=1110*659\" alt=\"\"></p>","neighbors":{"left":{"article_title":"49 | Custom Metrics: 让Auto Scaling不再“食之无味”","id":72693},"right":{"article_title":"51 | 谈谈Kubernetes开源社区和未来走向","id":73477}},"comments":[{"had_liked":false,"id":50524,"user_name":"lovelife","can_delete":false,"product_type":"c1","uid":1031745,"ip_address":"","ucode":"AC39E599A53448","user_header":"https://static001.geekbang.org/account/avatar/00/0f/be/41/e3ece193.jpg","comment_is_top":false,"comment_ctime":1545006907,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"186228600635","product_id":100015201,"comment_content":"如果每秒日志量很大时，直接输出到容器的stdout和stderr,很容易就把系统日志配额用满，因为对系统默认日志工具是针对单服务(例如docker)而不是进程进行限额的，最终导致的结果就是日志被吞掉。解决办法一个是增加配额，一个是给容器挂上存储，将日志输出到存储上。当然日志量大也要考虑写日志时过多的磁盘读写导致整个节点的整体性能下降。","like_count":44,"discussions":[{"author":{"id":1180705,"avatar":"https://static001.geekbang.org/account/avatar/00/12/04/21/77f32b11.jpg","nickname":"黄廉温","note":"","ucode":"39574051341D6D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5667,"discussion_content":"文章说kubernates会接管stdout stderr 把日志写到宿主机的json文件，之后的操作都是基于json文件。所以系统日记配额限制不需要担心。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1566430882,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":56097,"user_name":"Hammer_T","can_delete":false,"product_type":"c1","uid":1155259,"ip_address":"","ucode":"122C663CFFE2B8","user_header":"https://static001.geekbang.org/account/avatar/00/11/a0/bb/2e204da1.jpg","comment_is_top":false,"comment_ctime":1546392767,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"61675934911","product_id":100015201,"comment_content":"想问一下老师，如果一个容器里的日志有很多种，都输出到 stdout，收集的时候还能分得清是哪个吗？","like_count":14,"discussions":[{"author":{"id":1971269,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/oltLEqTrmHm2aJP99BK6tHu5h7hp4aj08wR5Wt6H31iadFduDAVvjYKmhQ2nvGbLV3lkVdiat2GRasgWXoJeTibUg/132","nickname":"杨","note":"","ucode":"7EFEFE285975C6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":347435,"discussion_content":"我们现在是有一个容器名  然后每调用一个接口都有唯一的id  最后通过logstash发送到es 通过kibana查询","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1612231875,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1310798,"avatar":"https://static001.geekbang.org/account/avatar/00/14/00/4e/be2b206b.jpg","nickname":"吴小智","note":"","ucode":"C7C9F58B5C9F7B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":297068,"discussion_content":"打日志的时候加个标志呗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1596765525,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":270826,"user_name":"ch_ort","can_delete":false,"product_type":"c1","uid":1580926,"ip_address":"","ucode":"B79746E687F29E","user_header":"","comment_is_top":false,"comment_ctime":1609293101,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"35969031469","product_id":100015201,"comment_content":" Kubernetes项目的监控体系现在已经被Prometheus&quot;一统&quot;，而Prometheus与Kuberentes类似，也是来自Google内部系统的设计理念。<br><br>Prometheus项目工作的核心：通过pull方式拉取监控对象的metric数据，存储到时序数据库中供后续检索。<br>时序数据库的特点：支持大批量写入、高性能搜索、聚合。<br>基于这样的核心，Prometheus剩下的组件就是用来配合这套机制运行，比如<br>Pushgateway: 允许被监控对象以Push的方式向Prometheus推送数据<br>Alertmanager：根据Metrics信息灵活地设置报警<br>Grafana：活动配置监控数据可视化页面<br><br>Kubernetes借助Promethus监控体系，可以提供Custom Metrics的能力，即自定义指标。Custom Metrics借助Aggregator APIServer扩展机制来实现，即对APIServer的请求，会先经过Aggreator来转发，它会根据URL来决定把请求转发给自定义的Custom Metrics APIServer，还是Kubernetes的APIServer。有了这样的体系，就可以方便的实现自定义指标的感知了<br>比如，现在启动了一个Custom Metrics APIServer，它对应的url是custom.metrics.k8s.io，当我需要获取某一个Pod内的自定义指标（例：http_requests）：<br><br>    https:&#47;&#47;&lt;apiserver_ip&gt;&#47;apis&#47;custom-metrics.metrics.k8s.io&#47;v1beta1&#47;namespaces&#47;default&#47;pods&#47;sample-metrics-app&#47;http_requests <br><br> 这个请求就会被Custom Metrics APIServer接收，然后它就会去Promethus里查询名叫sample-metrics-app这个pod的http_requests指标。而Promethus可以通过定期访问Pod的一个API来采集这个指标。<br><br>Kubernetes对容器日志的处理方式都叫做cluster-level-logging。容器默认情况下会把日志输出到宿主机上的一个JSON文件，这样，通过kubectl logs命令就可以看到这些容器的日志了。<br><br>Kuberentes提供了三种日志收集方案：<br>（1）logging agent:  pod会默认日志通过stdout&#47;stderr输出到宿主机的一个目录，宿主机上以DaemonSet启动一个logging-agent，这个logging-agent定期将日志转存到后端。<br>优势： 1)对Pod无侵入 2)一个node只需要一个agent 3）可以通过kubectl logs查看日志<br>劣势： 必须将日志输出到stdout&#47;stderr<br>（2) sidecar模式： pod将日志输出到一个容器目录，同pod内启动一个sidecar读取这些日志输出到stdout&#47;stderr，后续就跟方案1）一样了。<br>优势：1）sidecar跟主容器是共享Volume的，所以cpu和内存损耗不大。2）可以通过kubectl logs查看日志<br>劣势：机器上实际存了两份日志，浪费磁盘空间，因此不建议使用<br>（3）sidercar模式2：pod将日志输出到一个容器文件，同pod内启动一个sidecar读取这个文件并直接转存到后端存储。<br>优势：部署简单，宿主机友好<br>劣势：1） 这个sidecar容器很可能会消耗比较多的资源，甚至拖垮应用容器。2）通过kubectl logs是看不到任何日志输出的。<br>","like_count":9,"discussions":[{"author":{"id":1559076,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJTqcBC6lVDsQE7f1Tr5elKibRfJv7v3RRok4Q6DyBxjFvlPNngWgWyVqZHLD60ibzicMtTxLZjZy2tw/132","nickname":"Geek_ce0dd6","note":"","ucode":"CBA409D71045ED","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585705,"discussion_content":"我居然看评论比看文章还认真~~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661766448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":148975,"user_name":"fraηz","can_delete":false,"product_type":"c1","uid":1223067,"ip_address":"","ucode":"8B4D02DCD760E6","user_header":"https://static001.geekbang.org/account/avatar/00/12/a9/9b/61b5fc4c.jpg","comment_is_top":false,"comment_ctime":1573120416,"is_pvip":false,"discussion_count":6,"race_medal":0,"score":"31637891488","product_id":100015201,"comment_content":"filebeat作为sidecar容器采集主应用容器日志，然后发送到ELK存储日志，这样可行吗？","like_count":8,"discussions":[{"author":{"id":1285511,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoG6W3ugJo1XHR7kaibrIb7ff6oLcvLJM1fbQ9YdH9d6qkzMouS16SnWic4YOrOHFV5uOanTkYTQc9A/132","nickname":"TeacherLee24","note":"","ucode":"90D832A489764B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535215,"discussion_content":"会和业务进程抢占资源吧","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1638369189,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1586425,"avatar":"https://static001.geekbang.org/account/avatar/00/18/34/f9/c46c0fff.jpg","nickname":"Wind","note":"","ucode":"3679663B706C86","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":305593,"discussion_content":"我也就是这么打算的，正在实施中","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600010570,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":4,"child_discussions":[{"author":{"id":1272591,"avatar":"https://static001.geekbang.org/account/avatar/00/13/6b/0f/293b999c.jpg","nickname":"旋风","note":"","ucode":"B9FF02969F4307","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1586425,"avatar":"https://static001.geekbang.org/account/avatar/00/18/34/f9/c46c0fff.jpg","nickname":"Wind","note":"","ucode":"3679663B706C86","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":376015,"discussion_content":"实施的怎么样了，效果好吗，现在有什么不错的方案了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621928364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":305593,"ip_address":""},"score":376015,"extra":""},{"author":{"id":2651519,"avatar":"https://static001.geekbang.org/account/avatar/00/28/75/7f/ff5dd565.jpg","nickname":"Cantevenl","note":"","ucode":"F5A62BA4355F6A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1272591,"avatar":"https://static001.geekbang.org/account/avatar/00/13/6b/0f/293b999c.jpg","nickname":"旋风","note":"","ucode":"B9FF02969F4307","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384262,"discussion_content":"行的 使用filebeat作为sidecar更好","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1626446975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":376015,"ip_address":""},"score":384262,"extra":""},{"author":{"id":1033096,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c3/88/d16816a8.jpg","nickname":"如来神掌","note":"","ucode":"45E20FF935BD2F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2651519,"avatar":"https://static001.geekbang.org/account/avatar/00/28/75/7f/ff5dd565.jpg","nickname":"Cantevenl","note":"","ucode":"F5A62BA4355F6A","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":394445,"discussion_content":"sidecar也轻量，不知道分配了多大资源?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631884736,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":384262,"ip_address":""},"score":394445,"extra":""}]}]},{"had_liked":false,"id":53035,"user_name":"姜戈","can_delete":false,"product_type":"c1","uid":1119654,"ip_address":"","ucode":"8CF8A5AD9A3E10","user_header":"https://static001.geekbang.org/account/avatar/00/11/15/a6/723854ee.jpg","comment_is_top":false,"comment_ctime":1545573674,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"27315377450","product_id":100015201,"comment_content":"阿里的log-pilot是个不错的选择","like_count":6,"discussions":[{"author":{"id":1199969,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4f/61/00083e41.jpg","nickname":"小白","note":"","ucode":"7ACE14C0C4AE61","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":4956,"discussion_content":"就是没维护了，/狗头","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1565857660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1207457,"avatar":"https://static001.geekbang.org/account/avatar/00/12/6c/a1/80d83f0a.jpg","nickname":"Ellison","note":"","ucode":"A2FB94D4F6A332","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":352941,"discussion_content":"已用了， 感觉效果不错啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614913602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":244562,"user_name":"昀溪","can_delete":false,"product_type":"c1","uid":1493286,"ip_address":"","ucode":"BFAF31542BC24A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoCl6Nxf9oW9sDOoibA7p8lKf0jqjPeDszqI4e7iavicQHtbtyibHIhLibyXYAaT02l7GRQvM9BJUxh6yQ/132","comment_is_top":false,"comment_ctime":1598581867,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"23073418347","product_id":100015201,"comment_content":"老师，我们遇到一个问题也和日志有关，我们的POD都设置了limits，但是由于K8S统计POD的内存是包含缓存的，我们的POD日志输出是输出到一个文件，该文件是通过挂载emptydir的方式来做，然后使用阿里云的日志服务来收集。这里就面临一个问题，日志虽然采集走了，但是日志文件还留在目录里，它也会被算在POD的内存使用量里面，这就很容易造成OOM，请问这个问题应该怎么来处理，有没有什么思路提供一下。当然我们也会清理日志目录，比如一天前的，但是当天的日志如果很大依然会被算在POD的内存里。","like_count":5,"discussions":[{"author":{"id":1108695,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ea/d7/9506fe35.jpg","nickname":"pllsxyc","note":"","ucode":"402B95BCA36779","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":302384,"discussion_content":"使用hostpath类型，然后宿主机上做个logrotate，可以接很多服务都打在统一的一个日志目录里面，按照服务类型开子目录就好了，最后统一做一个logrotate，阿里云默认使用emptyDir是比较坑，我今天也发现这个问题，吓尿","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1598897698,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1445486,"avatar":"https://static001.geekbang.org/account/avatar/00/16/0e/6e/3d3661c3.jpg","nickname":"李昂纳多.奥斯特洛夫斯基","note":"","ucode":"3E2B76117ABBAB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1108695,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ea/d7/9506fe35.jpg","nickname":"pllsxyc","note":"","ucode":"402B95BCA36779","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":326266,"discussion_content":"emptydir也是在宿主机创建一个临时挂载目录，数据会落到宿主机，你可以看下/var/lib/kubelet下，写调度定时清理这个路径下的日志文件就可以了","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1605573643,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":302384,"ip_address":""},"score":326266,"extra":""}]}]},{"had_liked":false,"id":223431,"user_name":"曹顺详","can_delete":false,"product_type":"c1","uid":2012814,"ip_address":"","ucode":"FAF35A88EB207D","user_header":"https://static001.geekbang.org/account/avatar/00/1e/b6/8e/bc83bcae.jpg","comment_is_top":false,"comment_ctime":1591087668,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"23065924148","product_id":100015201,"comment_content":"老师好，请教一下，在应用代码层就指定日志存储后端这种解决方案有没有示例说一下。优缺点是什么？比如说是不是会增加应用的资源消耗？","like_count":5,"discussions":[{"author":{"id":1022156,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/98/cc/d7793058.jpg","nickname":"lgjchina","note":"","ucode":"4B5482C935BB3C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":316521,"discussion_content":"就像应用编程的时候把重要的操作日志写到关系数据库一样，应用可以直接写到指定的ES里面，缺点是你的应用要有异步容错能力，就是ES挂了或者缓慢，应用不会因为写日志而挂掉。传统的写文件日志，出发磁盘故障一般不会导致应用挂掉。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1603420830,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":158144,"user_name":"hhhh","can_delete":false,"product_type":"c1","uid":1256101,"ip_address":"","ucode":"9E87017424B382","user_header":"https://static001.geekbang.org/account/avatar/00/13/2a/a5/625c0a2e.jpg","comment_is_top":false,"comment_ctime":1575338207,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23050174687","product_id":100015201,"comment_content":"日志文件大小会很快增加，当时我到了nginx访问日志半小时就好几G，由于没给日志文件挂载存储，docker的文件目录会很快就增大了，引发了磁盘使用报警。","like_count":5},{"had_liked":false,"id":95464,"user_name":"王景迁","can_delete":false,"product_type":"c1","uid":1360656,"ip_address":"","ucode":"4CD9A1179AE084","user_header":"https://static001.geekbang.org/account/avatar/00/14/c3/10/3f18e402.jpg","comment_is_top":false,"comment_ctime":1558059965,"is_pvip":false,"discussion_count":5,"race_medal":0,"score":"18737929149","product_id":100015201,"comment_content":"为什么第3种方案sidecar容器会消耗很多资源？","like_count":4,"discussions":[{"author":{"id":2046510,"avatar":"http://thirdwx.qlogo.cn/mmopen/Q3auHgzwzM5HeRd9XvF6YM5V1BnnqNcGPVGvlqmnsBhy0jRtP9X9Ym8knu0MH9AB56s78dltbFRaj1icygLOO2A/132","nickname":"Geek_72070e","note":"","ucode":"F8C35131C24949","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":293037,"discussion_content":"很多日志收集工具本身要做日志的解析，很多是正则匹配，很耗cpu和内存，曾经logstash所占的资源比我应用本身还要多","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1595419797,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1033096,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c3/88/d16816a8.jpg","nickname":"如来神掌","note":"","ucode":"45E20FF935BD2F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2046510,"avatar":"http://thirdwx.qlogo.cn/mmopen/Q3auHgzwzM5HeRd9XvF6YM5V1BnnqNcGPVGvlqmnsBhy0jRtP9X9Ym8knu0MH9AB56s78dltbFRaj1icygLOO2A/132","nickname":"Geek_72070e","note":"","ucode":"F8C35131C24949","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394446,"discussion_content":"所以上filebeat， 比logstash省不少资源","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1631884946,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":293037,"ip_address":""},"score":394446,"extra":""}]},{"author":{"id":1176852,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f5/14/92b373dd.jpg","nickname":"网管","note":"","ucode":"608175DF616365","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":320477,"discussion_content":"因为第三种方案在Pod里装了一个运行Fluntd的容器啊， 第二种只是把应用日志tail -f 输出到stdout stderr","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604377756,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1008348,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/62/dc/8876c73b.jpg","nickname":"moooofly","note":"","ucode":"4A20795C281B6F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":265636,"discussion_content":"是哦，为什么3方案比2方案更耗资源呢，同是sidecar","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589422684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1218404,"avatar":"https://static001.geekbang.org/account/avatar/00/12/97/64/c4d9ce92.jpg","nickname":"漩涡鸣人","note":"","ucode":"FFF6596A9F90C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1008348,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/62/dc/8876c73b.jpg","nickname":"moooofly","note":"","ucode":"4A20795C281B6F","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":566341,"discussion_content":"方案二就一个tail命令，方案三更复杂些吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650652256,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":265636,"ip_address":""},"score":566341,"extra":""}]}]},{"had_liked":false,"id":89422,"user_name":"谈修竹","can_delete":false,"product_type":"c1","uid":1007179,"ip_address":"","ucode":"EFD8E8C2EF0474","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/4b/c95a8500.jpg","comment_is_top":false,"comment_ctime":1556171743,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14441073631","product_id":100015201,"comment_content":"最近在Istio，里面的Mixer好像可以支持多种Observability能力，包括Logging","like_count":3},{"had_liked":false,"id":342375,"user_name":"includestdio.h","can_delete":false,"product_type":"c1","uid":2314854,"ip_address":"","ucode":"5027BACE9319CD","user_header":"https://static001.geekbang.org/account/avatar/00/23/52/66/3e4d4846.jpg","comment_is_top":false,"comment_ctime":1650214070,"is_pvip":true,"discussion_count":0,"race_medal":4,"score":"10240148662","product_id":100015201,"comment_content":"公司目前采用的方案三，promtail 以sidecar方式运行在应用Pod中，日志通过promtail发到Loki ，最后再用 Grafana 展示","like_count":3},{"had_liked":false,"id":320593,"user_name":"陈斯佳","can_delete":false,"product_type":"c1","uid":1259323,"ip_address":"","ucode":"C236F874FC767A","user_header":"https://static001.geekbang.org/account/avatar/00/13/37/3b/495e2ce6.jpg","comment_is_top":false,"comment_ctime":1636415101,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10226349693","product_id":100015201,"comment_content":"第五十课:让日志无处可逃:容器日志收集与管理<br><br>Kubernetes 里面对容器日志的处理方式，都叫作 cluster-level-logging，即：这个日志处理系统，与容器、Pod 以及 Node 的生命周期都是完全无关的。<br><br>而对于一个容器来说，当应用把日志输出到 stdout 和 stderr 之后，容器项目在默认情况下就会把这些日志输出到宿主机上的一个 JSON 文件里。<br><br>三种日志方案:<br>第一种，在 Node 上部署 logging agent，将日志文件转发到后端存储里保存起来。这种方案的不足之处就在于，它要求应用输出的日志，都必须是直接输出到容器的 stdout 和 stderr 里。<br>第二种，就是对这种特殊情况的一个处理，即：当容器的日志只能输出到某些文件里的时候，我们可以通过一个 sidecar 容器把这些日志文件重新输出到 sidecar 的 stdout 和 stderr 上，这样就能够继续使用第一种方案了。<br>第三种方案，就是通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去。","like_count":1},{"had_liked":false,"id":277457,"user_name":"微末凡尘","can_delete":false,"product_type":"c1","uid":1099525,"ip_address":"","ucode":"25EDFD0914D0F4","user_header":"https://static001.geekbang.org/account/avatar/00/10/c7/05/19c5c255.jpg","comment_is_top":false,"comment_ctime":1612421742,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10202356334","product_id":100015201,"comment_content":"我们公司目前的日志收集方案：在同一个pod中再起一个logstash容器，读取应用容器的日志，存储到redis中，然后宿主机启动一个logstash服务，从redis中读取数据存入文件中，为什么不把直接日志文件直接挂载到远程目录文件中呢？因为相同的pod有好几个，如果日志文件同时挂载到同一个文件可能会造成死锁。","like_count":2,"discussions":[{"author":{"id":1033096,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c3/88/d16816a8.jpg","nickname":"如来神掌","note":"","ucode":"45E20FF935BD2F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394448,"discussion_content":"所以可以加个容器ID来区分","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631885077,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":127062,"user_name":"zik_kinbun","can_delete":false,"product_type":"c1","uid":1033107,"ip_address":"","ucode":"DEA91FB24E8AE7","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c3/93/ed8127d6.jpg","comment_is_top":false,"comment_ctime":1566547262,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10156481854","product_id":100015201,"comment_content":"我们采用kafka进行存储，特定程序再判断转发到ES","like_count":3},{"had_liked":false,"id":271547,"user_name":"ch_ort","can_delete":false,"product_type":"c1","uid":1580926,"ip_address":"","ucode":"B79746E687F29E","user_header":"","comment_is_top":false,"comment_ctime":1609687784,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5904655080","product_id":100015201,"comment_content":" Kubernetes项目的监控体系现在已经被Prometheus&quot;一统&quot;，而Prometheus与Kuberentes类似，也是来自Google内部系统的设计理念。<br><br>Prometheus项目工作的核心：通过pull方式拉取监控对象的metric数据，存储到时序数据库中供后续检索。<br>时序数据库的特点：支持大批量写入、高性能搜索、聚合。<br>基于这样的核心，Prometheus剩下的组件就是用来配合这套机制运行，比如<br>Pushgateway: 允许被监控对象以Push的方式向Prometheus推送数据<br>Alertmanager：根据Metrics信息灵活地设置报警<br>Grafana：活动配置监控数据可视化页面<br><br>Kubernetes借助Promethus监控体系，可以提供Custom Metrics的能力，即自定义指标。Custom Metrics借助Aggregator APIServer扩展机制来实现，即对APIServer的请求，会先经过Aggreator来转发，它会根据URL来决定把请求转发给自定义的Custom Metrics APIServer，还是Kubernetes的APIServer。有了这样的体系，就可以方便的实现自定义指标的感知了<br>比如，现在启动了一个Custom Metrics APIServer，它对应的url是custom.metrics.k8s.io，当我需要获取某一个Pod内的自定义指标（例：http_requests）：<br><br>    https:&#47;&#47;&lt;apiserver_ip&gt;&#47;apis&#47;custom-metrics.metrics.k8s.io&#47;v1beta1&#47;namespaces&#47;default&#47;pods&#47;sample-metrics-app&#47;http_requests <br><br> 这个请求就会被Custom Metrics APIServer接收，然后它就会去Promethus里查询名叫sample-metrics-app这个pod的http_requests指标。而Promethus可以通过定期访问Pod的一个API来采集这个指标。<br><br>Kubernetes对容器日志的处理方式都叫做cluster-level-logging。容器默认情况下会把日志输出到宿主机上的一个JSON文件，这样，通过kubectl logs命令就可以看到这些容器的日志了。<br><br>Kuberentes提供了三种日志收集方案：<br>（1）logging agent:  pod默认会将日志通过stdout&#47;stderr输出到宿主机的一个目录，宿主机上以DaemonSet启动一个logging-agent，这个logging-agent定期将日志转存到后端。<br>优势： 1)对Pod无侵入 2)一个node只需要一个agent 3）可以通过kubectl logs查看日志<br>劣势： 必须将日志输出到stdout&#47;stderr<br>（2) sidecar模式： pod将日志输出到一个容器目录，同pod内启动一个sidecar读取这些日志输出到stdout&#47;stderr，后续就跟方案1）一样了。<br>优势：1）sidecar跟主容器是共享Volume的，所以cpu和内存损耗不大。2）可以通过kubectl logs查看日志<br>劣势：机器上实际存了两份日志，浪费磁盘空间，因此不建议使用<br>（3）sidercar模式2：pod将日志输出到一个容器文件，同pod内启动一个sidecar读取这个文件并直接转存到后端存储。<br>优势：部署简单，宿主机友好<br>","like_count":1},{"had_liked":false,"id":50504,"user_name":"DJH","can_delete":false,"product_type":"c1","uid":1256740,"ip_address":"","ucode":"2BDEF123B3DB6A","user_header":"https://static001.geekbang.org/account/avatar/00/13/2d/24/28acca15.jpg","comment_is_top":false,"comment_ctime":1545003459,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5839970755","product_id":100015201,"comment_content":"张老师，请教一个问题：对于容器宿主机（或者说K8S的Node），一般文件系统分配的方案是怎样的？对于生产环境的Node，Container运行和存放镜像的文件系统一般需要多大？","like_count":1},{"had_liked":false,"id":321500,"user_name":"神毓逍遥","can_delete":false,"product_type":"c1","uid":2147220,"ip_address":"","ucode":"83351CB18B190E","user_header":"https://static001.geekbang.org/account/avatar/00/20/c3/94/e89ebc50.jpg","comment_is_top":false,"comment_ctime":1636907100,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1636907100","product_id":100015201,"comment_content":"这篇实用性不错","like_count":0},{"had_liked":false,"id":297168,"user_name":"SIMON LEE_啟明","can_delete":false,"product_type":"c1","uid":2656692,"ip_address":"","ucode":"2304172752F8E7","user_header":"https://static001.geekbang.org/account/avatar/00/28/89/b4/3a07938c.jpg","comment_is_top":false,"comment_ctime":1623338364,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1623338364","product_id":100015201,"comment_content":"老師你好，请问有需要管理K8s本身的日志吗？比方说持久化存储kubelet或apiServer的日志，用作日后运维。","like_count":0},{"had_liked":false,"id":279131,"user_name":"卡特","can_delete":false,"product_type":"c1","uid":1060491,"ip_address":"","ucode":"FF63677089E701","user_header":"https://static001.geekbang.org/account/avatar/00/10/2e/8b/32a8c5a0.jpg","comment_is_top":false,"comment_ctime":1613633365,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1613633365","product_id":100015201,"comment_content":"张老师：<br>可否同时结合方案1和方案3；<br>假设我有一个pod,里面有应用容器，日志收集sidecar容器；<br>1.应用容器的日志输出到stdout,stderr,会落到宿主机上的某个目录的某个文件中；<br>2.我的日志收集sidercar容器直接把第1步的日志文件作为输入源，转发到远程的loggingbackend服务中；<br><br>以上假设是否可行？如果可行，是不是解决了方案1和方案3的缺点？<br><br>","like_count":0,"discussions":[{"author":{"id":1351752,"avatar":"https://static001.geekbang.org/account/avatar/00/14/a0/48/a10fa801.jpg","nickname":"gxxc","note":"","ucode":"519A2C05146DE9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":378750,"discussion_content":"你理解的错误点还挺多的。。建议你再看一遍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623389249,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":240005,"user_name":"InfoQ_1165acf8e7fe","can_delete":false,"product_type":"c1","uid":2110485,"ip_address":"","ucode":"F609F3636D73B0","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKzbriccqBJoZ7q6lLsqZGrz6PTbHbVlxNI97bYPxnJDjBgDWYhpuquibuicqW0pwBlIQPQwmjb0eHyQ/132","comment_is_top":false,"comment_ctime":1596711039,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1596711039","product_id":100015201,"comment_content":"请问，当日志量很大的时候，直接将日志输出到容器 stdout 和 stderr 上，有没有什么隐患呢？有没有解决办法呢？<br><br>老师回答一下呗","like_count":0},{"had_liked":false,"id":136242,"user_name":"随意门","can_delete":false,"product_type":"c1","uid":1316502,"ip_address":"","ucode":"6AADD9C133AFCB","user_header":"https://static001.geekbang.org/account/avatar/00/14/16/96/1538f36c.jpg","comment_is_top":false,"comment_ctime":1569394393,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1569394393","product_id":100015201,"comment_content":"请问，如何在部署Kubernetes时候，开启日志的rotate功能？","like_count":0,"discussions":[{"author":{"id":1108695,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ea/d7/9506fe35.jpg","nickname":"pllsxyc","note":"","ucode":"402B95BCA36779","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":302905,"discussion_content":"可以使用hostPath落到磁盘上，然后使用系统的logrotate来轮转","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599060280,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":51192,"user_name":"微leng","can_delete":false,"product_type":"c1","uid":1026375,"ip_address":"","ucode":"DFF074789DDD7F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a9/47/3718cf90.jpg","comment_is_top":false,"comment_ctime":1545130020,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1545130020","product_id":100015201,"comment_content":"问老师一个问题，我在看pod的CPU监控信息时，prometheus显示的是占用CPU的时长，怎么判断当前pod的CPU的负载呢？","like_count":0}]}