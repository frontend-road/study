{"id":111522,"title":"54 | 存储虚拟化（下）：如何建立自己保管的单独档案库？","content":"<p>上一节，我们讲了qemu启动过程中的存储虚拟化。好了，现在qemu启动了，硬盘设备文件已经打开了。那如果我们要往虚拟机的一个进程写入一个文件，该怎么做呢？最终这个文件又是如何落到宿主机上的硬盘文件的呢？这一节，我们一起来看一看。</p><h2>前端设备驱动virtio_blk</h2><p>虚拟机里面的进程写入一个文件，当然要通过文件系统。整个过程和咱们在<a href=\"https://time.geekbang.org/column/article/97876\">文件系统</a>那一节讲的过程没有区别。只是到了设备驱动层，我们看到的就不是普通的硬盘驱动了，而是virtio的驱动。</p><p>virtio的驱动程序代码在Linux操作系统的源代码里面，文件名叫drivers/block/virtio_blk.c。</p><pre><code>static int __init init(void)\n{\n\tint error;\n\tvirtblk_wq = alloc_workqueue(&quot;virtio-blk&quot;, 0, 0);\n\tmajor = register_blkdev(0, &quot;virtblk&quot;);\n\terror = register_virtio_driver(&amp;virtio_blk);\n......\n}\n\nmodule_init(init);\nmodule_exit(fini);\n\nMODULE_DEVICE_TABLE(virtio, id_table);\nMODULE_DESCRIPTION(&quot;Virtio block driver&quot;);\nMODULE_LICENSE(&quot;GPL&quot;);\n\nstatic struct virtio_driver virtio_blk = {\n......\n\t.driver.name\t\t\t= KBUILD_MODNAME,\n\t.driver.owner\t\t\t= THIS_MODULE,\n\t.id_table\t\t\t= id_table,\n\t.probe\t\t\t\t= virtblk_probe,\n\t.remove\t\t\t\t= virtblk_remove,\n......\n};\n</code></pre><p>前面我们介绍过设备驱动程序，从这里的代码中，我们能看到非常熟悉的结构。它会创建一个workqueue，注册一个块设备，并获得一个主设备号，然后注册一个驱动函数virtio_blk。</p><p>当一个设备驱动作为一个内核模块被初始化的时候，probe函数会被调用，因而我们来看一下virtblk_probe。</p><pre><code>static int virtblk_probe(struct virtio_device *vdev)\n{\n\tstruct virtio_blk *vblk;\n\tstruct request_queue *q;\n......\n\tvdev-&gt;priv = vblk = kmalloc(sizeof(*vblk), GFP_KERNEL);\n\tvblk-&gt;vdev = vdev;\n\tvblk-&gt;sg_elems = sg_elems;\n\tINIT_WORK(&amp;vblk-&gt;config_work, virtblk_config_changed_work);\n......\n\terr = init_vq(vblk);\n......\n\tvblk-&gt;disk = alloc_disk(1 &lt;&lt; PART_BITS);\n\tmemset(&amp;vblk-&gt;tag_set, 0, sizeof(vblk-&gt;tag_set));\n\tvblk-&gt;tag_set.ops = &amp;virtio_mq_ops;\n\tvblk-&gt;tag_set.queue_depth = virtblk_queue_depth;\n\tvblk-&gt;tag_set.numa_node = NUMA_NO_NODE;\n\tvblk-&gt;tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\n\tvblk-&gt;tag_set.cmd_size =\n\t\tsizeof(struct virtblk_req) +\n\t\tsizeof(struct scatterlist) * sg_elems;\n\tvblk-&gt;tag_set.driver_data = vblk;\n\tvblk-&gt;tag_set.nr_hw_queues = vblk-&gt;num_vqs;\n\terr = blk_mq_alloc_tag_set(&amp;vblk-&gt;tag_set);\n......\n\tq = blk_mq_init_queue(&amp;vblk-&gt;tag_set);\n\tvblk-&gt;disk-&gt;queue = q;\n\tq-&gt;queuedata = vblk;\n\tvirtblk_name_format(&quot;vd&quot;, index, vblk-&gt;disk-&gt;disk_name, DISK_NAME_LEN);\n\tvblk-&gt;disk-&gt;major = major;\n\tvblk-&gt;disk-&gt;first_minor = index_to_minor(index);\n\tvblk-&gt;disk-&gt;private_data = vblk;\n\tvblk-&gt;disk-&gt;fops = &amp;virtblk_fops;\n\tvblk-&gt;disk-&gt;flags |= GENHD_FL_EXT_DEVT;\n\tvblk-&gt;index = index;\n......\n\tdevice_add_disk(&amp;vdev-&gt;dev, vblk-&gt;disk);\n\terr = device_create_file(disk_to_dev(vblk-&gt;disk), &amp;dev_attr_serial);\n......\n}\n</code></pre><p>在virtblk_probe中，我们首先看到的是struct request_queue，这是每一个块设备都有的一个队列。还记得吗？它有两个函数，一个是make_request_fn函数，用于生成request；另一个是request_fn函数，用于处理request。</p><!-- [[[read_end]]] --><p>这个request_queue的初始化过程在blk_mq_init_queue中。它会调用blk_mq_init_allocated_queue-&gt;blk_queue_make_request。在这里面，我们可以将make_request_fn函数设置为blk_mq_make_request，也就是说，一旦上层有写入请求，我们就通过blk_mq_make_request这个函数，将请求放入request_queue队列中。</p><p>另外，在virtblk_probe中，我们会初始化一个gendisk。前面我们也讲了，每一个块设备都有这样一个结构。</p><p>在virtblk_probe中，还有一件重要的事情就是，init_vq会来初始化virtqueue。</p><pre><code>static int init_vq(struct virtio_blk *vblk)\n{\n\tint err;\n\tint i;\n\tvq_callback_t **callbacks;\n\tconst char **names;\n\tstruct virtqueue **vqs;\n\tunsigned short num_vqs;\n\tstruct virtio_device *vdev = vblk-&gt;vdev;\n......\n\tvblk-&gt;vqs = kmalloc_array(num_vqs, sizeof(*vblk-&gt;vqs), GFP_KERNEL);\n\tnames = kmalloc_array(num_vqs, sizeof(*names), GFP_KERNEL);\n\tcallbacks = kmalloc_array(num_vqs, sizeof(*callbacks), GFP_KERNEL);\n\tvqs = kmalloc_array(num_vqs, sizeof(*vqs), GFP_KERNEL);\n......\n\tfor (i = 0; i &lt; num_vqs; i++) {\n\t\tcallbacks[i] = virtblk_done;\n\t\tnames[i] = vblk-&gt;vqs[i].name;\n\t}\n\n\t/* Discover virtqueues and write information to configuration.  */\n\terr = virtio_find_vqs(vdev, num_vqs, vqs, callbacks, names, &amp;desc);\n\n\tfor (i = 0; i &lt; num_vqs; i++) {\n\t\tvblk-&gt;vqs[i].vq = vqs[i];\n\t}\n\tvblk-&gt;num_vqs = num_vqs;\n......\n}\n</code></pre><p>按照上面的原理来说，virtqueue是一个介于客户机前端和qemu后端的一个结构，用于在这两端之间传递数据。这里建立的struct virtqueue是客户机前端对于队列的管理的数据结构，在客户机的linux内核中通过kmalloc_array进行分配。</p><p>而队列的实体需要通过函数virtio_find_vqs查找或者生成，所以这里我们还把callback函数指定为virtblk_done。当buffer使用发生变化的时候，我们需要调用这个callback函数进行通知。</p><pre><code>static inline\nint virtio_find_vqs(struct virtio_device *vdev, unsigned nvqs,\n\t\t\tstruct virtqueue *vqs[], vq_callback_t *callbacks[],\n\t\t\tconst char * const names[],\n\t\t\tstruct irq_affinity *desc)\n{\n\treturn vdev-&gt;config-&gt;find_vqs(vdev, nvqs, vqs, callbacks, names, NULL, desc);\n}\n\nstatic const struct virtio_config_ops virtio_pci_config_ops = {\n\t.get\t\t= vp_get,\n\t.set\t\t= vp_set,\n\t.generation\t= vp_generation,\n\t.get_status\t= vp_get_status,\n\t.set_status\t= vp_set_status,\n\t.reset\t\t= vp_reset,\n\t.find_vqs\t= vp_modern_find_vqs,\n\t.del_vqs\t= vp_del_vqs,\n\t.get_features\t= vp_get_features,\n\t.finalize_features = vp_finalize_features,\n\t.bus_name\t= vp_bus_name,\n\t.set_vq_affinity = vp_set_vq_affinity,\n\t.get_vq_affinity = vp_get_vq_affinity,\n};\n</code></pre><p>根据virtio_config_ops的定义，virtio_find_vqs会调用vp_modern_find_vqs。</p><pre><code>static int vp_modern_find_vqs(struct virtio_device *vdev, unsigned nvqs,\n\t\t\t      struct virtqueue *vqs[],\n\t\t\t      vq_callback_t *callbacks[],\n\t\t\t      const char * const names[], const bool *ctx,\n\t\t\t      struct irq_affinity *desc)\n{\n\tstruct virtio_pci_device *vp_dev = to_vp_device(vdev);\n\tstruct virtqueue *vq;\n\tint rc = vp_find_vqs(vdev, nvqs, vqs, callbacks, names, ctx, desc);\n\t/* Select and activate all queues. Has to be done last: once we do\n\t * this, there's no way to go back except reset.\n\t */\n\tlist_for_each_entry(vq, &amp;vdev-&gt;vqs, list) {\n\t\tvp_iowrite16(vq-&gt;index, &amp;vp_dev-&gt;common-&gt;queue_select);\n\t\tvp_iowrite16(1, &amp;vp_dev-&gt;common-&gt;queue_enable);\n\t}\n\n\treturn 0;\n}\n</code></pre><p>在vp_modern_find_vqs中，vp_find_vqs会调用vp_find_vqs_intx。</p><pre><code>static int vp_find_vqs_intx(struct virtio_device *vdev, unsigned nvqs,\n\t\tstruct virtqueue *vqs[], vq_callback_t *callbacks[],\n\t\tconst char * const names[], const bool *ctx)\n{\n\tstruct virtio_pci_device *vp_dev = to_vp_device(vdev);\n\tint i, err;\n\n\tvp_dev-&gt;vqs = kcalloc(nvqs, sizeof(*vp_dev-&gt;vqs), GFP_KERNEL);\n\terr = request_irq(vp_dev-&gt;pci_dev-&gt;irq, vp_interrupt, IRQF_SHARED,\n\t\t\tdev_name(&amp;vdev-&gt;dev), vp_dev);\n\tvp_dev-&gt;intx_enabled = 1;\n\tvp_dev-&gt;per_vq_vectors = false;\n\tfor (i = 0; i &lt; nvqs; ++i) {\n\t\tvqs[i] = vp_setup_vq(vdev, i, callbacks[i], names[i],\n\t\t\t\t     ctx ? ctx[i] : false,\n\t\t\t\t     VIRTIO_MSI_NO_VECTOR);\n......\n\t}\n}\n</code></pre><p>在vp_find_vqs_intx中，我们通过request_irq注册一个中断处理函数vp_interrupt，当设备的配置信息发生改变，会产生一个中断，当设备向队列中写入信息时，也会产生一个中断，我们称为vq中断，中断处理函数需要调用相应的队列的回调函数。</p><p>然后，我们根据队列的数目，依次调用vp_setup_vq，完成virtqueue、vring的分配和初始化。</p><pre><code>static struct virtqueue *vp_setup_vq(struct virtio_device *vdev, unsigned index,\n\t\t\t\t     void (*callback)(struct virtqueue *vq),\n\t\t\t\t     const char *name,\n\t\t\t\t     bool ctx,\n\t\t\t\t     u16 msix_vec)\n{\n\tstruct virtio_pci_device *vp_dev = to_vp_device(vdev);\n\tstruct virtio_pci_vq_info *info = kmalloc(sizeof *info, GFP_KERNEL);\n\tstruct virtqueue *vq;\n\tunsigned long flags;\n......\n\tvq = vp_dev-&gt;setup_vq(vp_dev, info, index, callback, name, ctx,\n\t\t\t      msix_vec);\n\tinfo-&gt;vq = vq;\n\tif (callback) {\n\t\tspin_lock_irqsave(&amp;vp_dev-&gt;lock, flags);\n\t\tlist_add(&amp;info-&gt;node, &amp;vp_dev-&gt;virtqueues);\n\t\tspin_unlock_irqrestore(&amp;vp_dev-&gt;lock, flags);\n\t} else {\n\t\tINIT_LIST_HEAD(&amp;info-&gt;node);\n\t}\n\tvp_dev-&gt;vqs[index] = info;\n\treturn vq;\n}\n\nstatic struct virtqueue *setup_vq(struct virtio_pci_device *vp_dev,\n\t\t\t\t  struct virtio_pci_vq_info *info,\n\t\t\t\t  unsigned index,\n\t\t\t\t  void (*callback)(struct virtqueue *vq),\n\t\t\t\t  const char *name,\n\t\t\t\t  bool ctx,\n\t\t\t\t  u16 msix_vec)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = vp_dev-&gt;common;\n\tstruct virtqueue *vq;\n\tu16 num, off;\n\tint err;\n\n\t/* Select the queue we're interested in */\n\tvp_iowrite16(index, &amp;cfg-&gt;queue_select);\n\n\t/* Check if queue is either not available or already active. */\n\tnum = vp_ioread16(&amp;cfg-&gt;queue_size);\n\n\t/* get offset of notification word for this vq */\n\toff = vp_ioread16(&amp;cfg-&gt;queue_notify_off);\n\n\tinfo-&gt;msix_vector = msix_vec;\n\n\t/* create the vring */\n\tvq = vring_create_virtqueue(index, num,\n\t\t\t\t    SMP_CACHE_BYTES, &amp;vp_dev-&gt;vdev,\n\t\t\t\t    true, true, ctx,\n\t\t\t\t    vp_notify, callback, name);\n\t/* activate the queue */\n\tvp_iowrite16(virtqueue_get_vring_size(vq), &amp;cfg-&gt;queue_size);\n\tvp_iowrite64_twopart(virtqueue_get_desc_addr(vq),\n\t\t\t     &amp;cfg-&gt;queue_desc_lo, &amp;cfg-&gt;queue_desc_hi);\n\tvp_iowrite64_twopart(virtqueue_get_avail_addr(vq),\n\t\t\t     &amp;cfg-&gt;queue_avail_lo, &amp;cfg-&gt;queue_avail_hi);\n\tvp_iowrite64_twopart(virtqueue_get_used_addr(vq),\n\t\t\t     &amp;cfg-&gt;queue_used_lo, &amp;cfg-&gt;queue_used_hi);\n......\n\treturn vq;\n}\n\nstruct virtqueue *vring_create_virtqueue(\n\tunsigned int index,\n\tunsigned int num,\n\tunsigned int vring_align,\n\tstruct virtio_device *vdev,\n\tbool weak_barriers,\n\tbool may_reduce_num,\n\tbool context,\n\tbool (*notify)(struct virtqueue *),\n\tvoid (*callback)(struct virtqueue *),\n\tconst char *name)\n{\n\tstruct virtqueue *vq;\n\tvoid *queue = NULL;\n\tdma_addr_t dma_addr;\n\tsize_t queue_size_in_bytes;\n\tstruct vring vring;\n\n\t/* TODO: allocate each queue chunk individually */\n\tfor (; num &amp;&amp; vring_size(num, vring_align) &gt; PAGE_SIZE; num /= 2) {\n\t\tqueue = vring_alloc_queue(vdev, vring_size(num, vring_align),\n\t\t\t\t\t  &amp;dma_addr,\n\t\t\t\t\t  GFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);\n\t\tif (queue)\n\t\t\tbreak;\n\t}\n\n\tif (!queue) {\n\t\t/* Try to get a single page. You are my only hope! */\n\t\tqueue = vring_alloc_queue(vdev, vring_size(num, vring_align),\n\t\t\t\t\t  &amp;dma_addr, GFP_KERNEL|__GFP_ZERO);\n\t}\n\n\tqueue_size_in_bytes = vring_size(num, vring_align);\n\tvring_init(&amp;vring, num, queue, vring_align);\n\n\tvq = __vring_new_virtqueue(index, vring, vdev, weak_barriers, context, notify, callback, name);\n\n\tto_vvq(vq)-&gt;queue_dma_addr = dma_addr;\n\tto_vvq(vq)-&gt;queue_size_in_bytes = queue_size_in_bytes;\n\tto_vvq(vq)-&gt;we_own_ring = true;\n\n\treturn vq;\n}\n</code></pre><p>在vring_create_virtqueue中，我们会调用vring_alloc_queue，来创建队列所需要的内存空间，然后调用vring_init初始化结构struct vring，来管理队列的内存空间，调用__vring_new_virtqueue，来创建struct vring_virtqueue。</p><p>这个结构的一开始，是struct virtqueue，它也是struct virtqueue的一个扩展，紧接着后面就是struct vring。</p><pre><code>struct vring_virtqueue {\n\tstruct virtqueue vq;\n\n\t/* Actual memory layout for this queue */\n\tstruct vring vring;\n......\n}\n</code></pre><p>至此我们发现，虚拟机里面的virtio的前端是这样的结构：struct virtio_device里面有一个struct vring_virtqueue，在struct vring_virtqueue里面有一个struct vring。</p><h2>中间virtio队列的管理</h2><p>还记不记得我们上面讲qemu初始化的时候，virtio的后端有数据结构VirtIODevice，VirtQueue和vring一模一样，前端和后端对应起来，都应该指向刚才创建的那一段内存。</p><p>现在的问题是，我们刚才分配的内存在客户机的内核里面，如何告知qemu来访问这段内存呢？</p><p>别忘了，qemu模拟出来的virtio block device只是一个PCI设备。对于客户机来讲，这是一个外部设备，我们可以通过给外部设备发送指令的方式告知外部设备，这就是代码中vp_iowrite16的作用。它会调用专门给外部设备发送指令的函数iowrite，告诉外部的PCI设备。</p><p>告知的有三个地址virtqueue_get_desc_addr、virtqueue_get_avail_addr，virtqueue_get_used_addr。从客户机角度来看，这里面的地址都是物理地址，也即GPA（Guest Physical Address）。因为只有物理地址才是客户机和qemu程序都认可的地址，本来客户机的物理内存也是qemu模拟出来的。</p><p>在qemu中，对PCI总线添加一个设备的时候，我们会调用virtio_pci_device_plugged。</p><pre><code>static void virtio_pci_device_plugged(DeviceState *d, Error **errp)\n{\n    VirtIOPCIProxy *proxy = VIRTIO_PCI(d);\n......\n    memory_region_init_io(&amp;proxy-&gt;bar, OBJECT(proxy),\n                              &amp;virtio_pci_config_ops,\n                              proxy, &quot;virtio-pci&quot;, size);\n......\n}\n\nstatic const MemoryRegionOps virtio_pci_config_ops = {\n    .read = virtio_pci_config_read,\n    .write = virtio_pci_config_write,\n    .impl = {\n        .min_access_size = 1,\n        .max_access_size = 4,\n    },\n    .endianness = DEVICE_LITTLE_ENDIAN,\n};\n</code></pre><p>在这里面，对于这个加载的设备进行I/O操作，会映射到读写某一块内存空间，对应的操作为virtio_pci_config_ops，也即写入这块内存空间，这就相当于对于这个PCI设备进行某种配置。</p><p>对PCI设备进行配置的时候，会有这样的调用链：virtio_pci_config_write-&gt;virtio_ioport_write-&gt;virtio_queue_set_addr。设置virtio的queue的地址是一项很重要的操作。</p><pre><code>void virtio_queue_set_addr(VirtIODevice *vdev, int n, hwaddr addr)\n{\n    vdev-&gt;vq[n].vring.desc = addr;\n    virtio_queue_update_rings(vdev, n);\n}\n</code></pre><p>从这里我们可以看出，qemu后端的VirtIODevice的VirtQueue的vring的地址，被设置成了刚才给队列分配的内存的GPA。</p><p><img src=\"https://static001.geekbang.org/resource/image/25/d0/2572f8b1e75b9eaab6560866fcb31fd0.jpg\" alt=\"\"></p><p>接着，我们来看一下这个队列的格式。</p><p><img src=\"https://static001.geekbang.org/resource/image/49/db/49414d5acc81933b66410bbba102b0db.jpg\" alt=\"\"></p><pre><code>/* Virtio ring descriptors: 16 bytes.  These can chain together via &quot;next&quot;. */\nstruct vring_desc {\n\t/* Address (guest-physical). */\n\t__virtio64 addr;\n\t/* Length. */\n\t__virtio32 len;\n\t/* The flags as indicated above. */\n\t__virtio16 flags;\n\t/* We chain unused descriptors via this, too */\n\t__virtio16 next;\n};\n\nstruct vring_avail {\n\t__virtio16 flags;\n\t__virtio16 idx;\n\t__virtio16 ring[];\n};\n\n/* u32 is used here for ids for padding reasons. */\nstruct vring_used_elem {\n\t/* Index of start of used descriptor chain. */\n\t__virtio32 id;\n\t/* Total length of the descriptor chain which was used (written to) */\n\t__virtio32 len;\n};\n\nstruct vring_used {\n\t__virtio16 flags;\n\t__virtio16 idx;\n\tstruct vring_used_elem ring[];\n};\n\nstruct vring {\n\tunsigned int num;\n\n\tstruct vring_desc *desc;\n\n\tstruct vring_avail *avail;\n\n\tstruct vring_used *used;\n};\n</code></pre><p>vring包含三个成员：</p><ul>\n<li>vring_desc指向分配的内存块，用于存放客户机和qemu之间传输的数据。</li>\n<li>avail-&gt;ring[]是发送端维护的环形队列，指向需要接收端处理的vring_desc。</li>\n<li>used-&gt;ring[]是接收端维护的环形队列，指向自己已经处理过了的vring_desc。</li>\n</ul><h2>数据写入的流程</h2><p>接下来，我们来看，真的写入一个数据的时候，会发生什么。</p><p>按照上面virtio驱动初始化的时候的逻辑，blk_mq_make_request会被调用。这个函数比较复杂，会分成多个分支，但是最终都会调用到request_queue的virtio_mq_ops的queue_rq函数。</p><pre><code>struct request_queue *q = rq-&gt;q;\nq-&gt;mq_ops-&gt;queue_rq(hctx, &amp;bd);\n\nstatic const struct blk_mq_ops virtio_mq_ops = {\n\t.queue_rq\t= virtio_queue_rq,\n\t.complete\t= virtblk_request_done,\n\t.init_request\t= virtblk_init_request,\n\t.map_queues\t= virtblk_map_queues,\n};\n</code></pre><p>根据virtio_mq_ops的定义，我们现在要调用virtio_queue_rq。</p><pre><code>static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t   const struct blk_mq_queue_data *bd)\n{\n\tstruct virtio_blk *vblk = hctx-&gt;queue-&gt;queuedata;\n\tstruct request *req = bd-&gt;rq;\n\tstruct virtblk_req *vbr = blk_mq_rq_to_pdu(req);\n......\n\terr = virtblk_add_req(vblk-&gt;vqs[qid].vq, vbr, vbr-&gt;sg, num);\n......\n\tif (notify)\n\t\tvirtqueue_notify(vblk-&gt;vqs[qid].vq);\n\treturn BLK_STS_OK;\n}\n</code></pre><p>在virtio_queue_rq中，我们会将请求写入的数据，通过virtblk_add_req放入struct virtqueue。</p><p>因此，接下来的调用链为：virtblk_add_req-&gt;virtqueue_add_sgs-&gt;virtqueue_add。</p><pre><code>static inline int virtqueue_add(struct virtqueue *_vq,\n\t\t\t\tstruct scatterlist *sgs[],\n\t\t\t\tunsigned int total_sg,\n\t\t\t\tunsigned int out_sgs,\n\t\t\t\tunsigned int in_sgs,\n\t\t\t\tvoid *data,\n\t\t\t\tvoid *ctx,\n\t\t\t\tgfp_t gfp)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct scatterlist *sg;\n\tstruct vring_desc *desc;\n\tunsigned int i, n, avail, descs_used, uninitialized_var(prev), err_idx;\n\tint head;\n\tbool indirect;\n......\n\thead = vq-&gt;free_head;\n\n\tindirect = false;\n\tdesc = vq-&gt;vring.desc;\n\ti = head;\n\tdescs_used = total_sg;\n\n\tfor (n = 0; n &lt; out_sgs; n++) {\n\t\tfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\n\t\t\tdma_addr_t addr = vring_map_one_sg(vq, sg, DMA_TO_DEVICE);\n......\n\t\t\tdesc[i].flags = cpu_to_virtio16(_vq-&gt;vdev, VRING_DESC_F_NEXT);\n\t\t\tdesc[i].addr = cpu_to_virtio64(_vq-&gt;vdev, addr);\n\t\t\tdesc[i].len = cpu_to_virtio32(_vq-&gt;vdev, sg-&gt;length);\n\t\t\tprev = i;\n\t\t\ti = virtio16_to_cpu(_vq-&gt;vdev, desc[i].next);\n\t\t}\n\t}\n\n\t/* Last one doesn't continue. */\n\tdesc[prev].flags &amp;= cpu_to_virtio16(_vq-&gt;vdev, ~VRING_DESC_F_NEXT);\n\n\t/* We're using some buffers from the free list. */\n\tvq-&gt;vq.num_free -= descs_used;\n\n\t/* Update free pointer */\n\tvq-&gt;free_head = i;\n\n\t/* Store token and indirect buffer state. */\n\tvq-&gt;desc_state[head].data = data;\n\n\t/* Put entry in available array (but don't update avail-&gt;idx until they do sync). */\n\tavail = vq-&gt;avail_idx_shadow &amp; (vq-&gt;vring.num - 1);\n\tvq-&gt;vring.avail-&gt;ring[avail] = cpu_to_virtio16(_vq-&gt;vdev, head);\n\n\t/* Descriptors and available array need to be set before we expose the new available array entries. */\n\tvirtio_wmb(vq-&gt;weak_barriers);\n\tvq-&gt;avail_idx_shadow++;\n\tvq-&gt;vring.avail-&gt;idx = cpu_to_virtio16(_vq-&gt;vdev, vq-&gt;avail_idx_shadow);\n\tvq-&gt;num_added++;\n......\n\treturn 0;\n}\n</code></pre><p>在virtqueue_add函数中，我们能看到，free_head指向的整个内存块空闲链表的起始位置，用head变量记住这个起始位置。</p><p>接下来，i也指向这个起始位置，然后是一个for循环，将数据放到内存块里面，放的过程中，next不断指向下一个空闲位置，这样空闲的内存块被不断的占用。等所有的写入都结束了，i就会指向这次存放的内存块的下一个空闲位置，然后free_head就指向i，因为前面的都填满了。</p><p>至此，从head到i之间的内存块，就是这次写入的全部数据。</p><p>于是，在vring的avail变量中，在ring[]数组中分配新的一项，在avail的位置，avail的计算是avail_idx_shadow &amp; (vq-&gt;vring.num - 1)，其中，avail_idx_shadow是上一次的avail的位置。这里如果超过了ring[]数组的下标，则重新跳到起始位置，就说明是一个环。这次分配的新的avail的位置就存放新写入的从head到i之间的内存块。然后是avail_idx_shadow++，这说明这一块内存可以被接收方读取了。</p><p>接下来，我们回到virtio_queue_rq，调用virtqueue_notify通知接收方。而virtqueue_notify会调用vp_notify。</p><pre><code>bool vp_notify(struct virtqueue *vq)\n{\n\t/* we write the queue's selector into the notification register to\n\t * signal the other end */\n\tiowrite16(vq-&gt;index, (void __iomem *)vq-&gt;priv);\n\treturn true;\n}\n</code></pre><p>然后，我们写入一个I/O会触发VM exit。我们在解析CPU的时候看到过这个逻辑。</p><pre><code>int kvm_cpu_exec(CPUState *cpu)\n{\n    struct kvm_run *run = cpu-&gt;kvm_run;\n    int ret, run_ret;\n......\n    run_ret = kvm_vcpu_ioctl(cpu, KVM_RUN, 0);\n......\n    switch (run-&gt;exit_reason) {\n        case KVM_EXIT_IO:\n            DPRINTF(&quot;handle_io\\n&quot;);\n            /* Called outside BQL */\n            kvm_handle_io(run-&gt;io.port, attrs,\n                          (uint8_t *)run + run-&gt;io.data_offset,\n                          run-&gt;io.direction,\n                          run-&gt;io.size,\n                          run-&gt;io.count);\n            ret = 0;\n            break;\n    }\n......\n}\n</code></pre><p>这次写入的也是一个I/O的内存空间，同样会触发virtio_ioport_write，这次会调用virtio_queue_notify。</p><pre><code>void virtio_queue_notify(VirtIODevice *vdev, int n)\n{\n    VirtQueue *vq = &amp;vdev-&gt;vq[n];\n......\n    if (vq-&gt;handle_aio_output) {\n        event_notifier_set(&amp;vq-&gt;host_notifier);\n    } else if (vq-&gt;handle_output) {\n        vq-&gt;handle_output(vdev, vq);\n    }\n}\n</code></pre><p>virtio_queue_notify会调用VirtQueue的handle_output函数，前面我们已经设置过这个函数了，是virtio_blk_handle_output。</p><p>接下来的调用链为：virtio_blk_handle_output-&gt;virtio_blk_handle_output_do-&gt;virtio_blk_handle_vq。</p><pre><code>bool virtio_blk_handle_vq(VirtIOBlock *s, VirtQueue *vq)\n{\n    VirtIOBlockReq *req;\n    MultiReqBuffer mrb = {};\n    bool progress = false;\n......\n    do {\n        virtio_queue_set_notification(vq, 0);\n\n        while ((req = virtio_blk_get_request(s, vq))) {\n            progress = true;\n            if (virtio_blk_handle_request(req, &amp;mrb)) {\n                virtqueue_detach_element(req-&gt;vq, &amp;req-&gt;elem, 0);\n                virtio_blk_free_request(req);\n                break;\n            }\n        }\n\n        virtio_queue_set_notification(vq, 1);\n    } while (!virtio_queue_empty(vq));\n\n    if (mrb.num_reqs) {\n        virtio_blk_submit_multireq(s-&gt;blk, &amp;mrb);\n    }\n......\n    return progress;\n}\n</code></pre><p>在virtio_blk_handle_vq中，有一个while循环，在循环中调用函数virtio_blk_get_request从vq中取出请求，然后调用virtio_blk_handle_request处理从vq中取出的请求。</p><p>我们先来看virtio_blk_get_request。</p><pre><code>static VirtIOBlockReq *virtio_blk_get_request(VirtIOBlock *s, VirtQueue *vq)\n{\n    VirtIOBlockReq *req = virtqueue_pop(vq, sizeof(VirtIOBlockReq));\n\n    if (req) {\n        virtio_blk_init_request(s, vq, req);\n    }\n    return req;\n}\n\nvoid *virtqueue_pop(VirtQueue *vq, size_t sz)\n{\n    unsigned int i, head, max;\n    VRingMemoryRegionCaches *caches;\n    MemoryRegionCache *desc_cache;\n    int64_t len;\n    VirtIODevice *vdev = vq-&gt;vdev;\n    VirtQueueElement *elem = NULL;\n    unsigned out_num, in_num, elem_entries;\n    hwaddr addr[VIRTQUEUE_MAX_SIZE];\n    struct iovec iov[VIRTQUEUE_MAX_SIZE];\n    VRingDesc desc;\n    int rc;\n......\n    /* When we start there are none of either input nor output. */\n    out_num = in_num = elem_entries = 0;\n\n    max = vq-&gt;vring.num;\n\n    i = head;\n\n    caches = vring_get_region_caches(vq);\n    desc_cache = &amp;caches-&gt;desc;\n    vring_desc_read(vdev, &amp;desc, desc_cache, i);\n......\n    /* Collect all the descriptors */\n    do {\n        bool map_ok;\n\n        if (desc.flags &amp; VRING_DESC_F_WRITE) {\n            map_ok = virtqueue_map_desc(vdev, &amp;in_num, addr + out_num,\n                                        iov + out_num,\n                                        VIRTQUEUE_MAX_SIZE - out_num, true,\n                                        desc.addr, desc.len);\n        } else {\n            map_ok = virtqueue_map_desc(vdev, &amp;out_num, addr, iov,\n                                        VIRTQUEUE_MAX_SIZE, false,\n                                        desc.addr, desc.len);\n        }\n......\n        rc = virtqueue_read_next_desc(vdev, &amp;desc, desc_cache, max, &amp;i);\n    } while (rc == VIRTQUEUE_READ_DESC_MORE);\n......\n    /* Now copy what we have collected and mapped */\n    elem = virtqueue_alloc_element(sz, out_num, in_num);\n    elem-&gt;index = head;\n    for (i = 0; i &lt; out_num; i++) {\n        elem-&gt;out_addr[i] = addr[i];\n        elem-&gt;out_sg[i] = iov[i];\n    }\n    for (i = 0; i &lt; in_num; i++) {\n        elem-&gt;in_addr[i] = addr[out_num + i];\n        elem-&gt;in_sg[i] = iov[out_num + i];\n    }\n\n    vq-&gt;inuse++;\n......\n    return elem;\n}\n</code></pre><p>我们可以看到，virtio_blk_get_request会调用virtqueue_pop。在这里面，我们能看到对于vring的操作，也即从这里面将客户机里面写入的数据读取出来，放到VirtIOBlockReq结构中。</p><p>接下来，我们就要调用virtio_blk_handle_request处理这些数据。所以接下来的调用链为：virtio_blk_handle_request-&gt;virtio_blk_submit_multireq-&gt;submit_requests。</p><pre><code>static inline void submit_requests(BlockBackend *blk, MultiReqBuffer *mrb,int start, int num_reqs, int niov)\n{\n    QEMUIOVector *qiov = &amp;mrb-&gt;reqs[start]-&gt;qiov;\n    int64_t sector_num = mrb-&gt;reqs[start]-&gt;sector_num;\n    bool is_write = mrb-&gt;is_write;\n\n    if (num_reqs &gt; 1) {\n        int i;\n        struct iovec *tmp_iov = qiov-&gt;iov;\n        int tmp_niov = qiov-&gt;niov;\n        qemu_iovec_init(qiov, niov);\n\n        for (i = 0; i &lt; tmp_niov; i++) {\n            qemu_iovec_add(qiov, tmp_iov[i].iov_base, tmp_iov[i].iov_len);\n        }\n\n        for (i = start + 1; i &lt; start + num_reqs; i++) {\n            qemu_iovec_concat(qiov, &amp;mrb-&gt;reqs[i]-&gt;qiov, 0,\n                              mrb-&gt;reqs[i]-&gt;qiov.size);\n            mrb-&gt;reqs[i - 1]-&gt;mr_next = mrb-&gt;reqs[i];\n        }\n\n        block_acct_merge_done(blk_get_stats(blk),\n                              is_write ? BLOCK_ACCT_WRITE : BLOCK_ACCT_READ,\n                              num_reqs - 1);\n    }\n\n    if (is_write) {\n        blk_aio_pwritev(blk, sector_num &lt;&lt; BDRV_SECTOR_BITS, qiov, 0,\n                        virtio_blk_rw_complete, mrb-&gt;reqs[start]);\n    } else {\n        blk_aio_preadv(blk, sector_num &lt;&lt; BDRV_SECTOR_BITS, qiov, 0,\n                       virtio_blk_rw_complete, mrb-&gt;reqs[start]);\n    }\n}\n</code></pre><p>在submit_requests中，我们看到了BlockBackend。这是在qemu启动的时候，打开qcow2文件的时候生成的，现在我们可以用它来写入文件了，调用的是blk_aio_pwritev。</p><pre><code>BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,\n                            QEMUIOVector *qiov, BdrvRequestFlags flags,\n                            BlockCompletionFunc *cb, void *opaque)\n{\n    return blk_aio_prwv(blk, offset, qiov-&gt;size, qiov,\n                        blk_aio_write_entry, flags, cb, opaque);\n}\n\nstatic BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset, int bytes,\n                                void *iobuf, CoroutineEntry co_entry,\n                                BdrvRequestFlags flags,\n                                BlockCompletionFunc *cb, void *opaque)\n{\n    BlkAioEmAIOCB *acb;\n    Coroutine *co;\n    acb = blk_aio_get(&amp;blk_aio_em_aiocb_info, blk, cb, opaque);\n    acb-&gt;rwco = (BlkRwCo) {\n        .blk    = blk,\n        .offset = offset,\n        .iobuf  = iobuf,\n        .flags  = flags,\n        .ret    = NOT_DONE,\n    };\n    acb-&gt;bytes = bytes;\n    acb-&gt;has_returned = false;\n\n    co = qemu_coroutine_create(co_entry, acb);\n    bdrv_coroutine_enter(blk_bs(blk), co);\n\n    acb-&gt;has_returned = true;\n    return &amp;acb-&gt;common;\n}\n</code></pre><p>在blk_aio_pwritev中，我们看到，又是创建了一个协程来进行写入。写入完毕之后调用virtio_blk_rw_complete-&gt;virtio_blk_req_complete。</p><pre><code>static void virtio_blk_req_complete(VirtIOBlockReq *req, unsigned char status)\n{\n    VirtIOBlock *s = req-&gt;dev;\n    VirtIODevice *vdev = VIRTIO_DEVICE(s);\n\n    trace_virtio_blk_req_complete(vdev, req, status);\n\n    stb_p(&amp;req-&gt;in-&gt;status, status);\n    virtqueue_push(req-&gt;vq, &amp;req-&gt;elem, req-&gt;in_len);\n    virtio_notify(vdev, req-&gt;vq);\n}\n</code></pre><p>在virtio_blk_req_complete中，我们先是调用virtqueue_push，更新vring中used变量，表示这部分已经写入完毕，空间可以回收利用了。但是，这部分的改变仅仅改变了qemu后端的vring，我们还需要通知客户机中virtio前端的vring的值，因而要调用virtio_notify。virtio_notify会调用virtio_irq发送一个中断。</p><p>还记得咱们前面注册过一个中断处理函数vp_interrupt吗？它就是干这个事情的。</p><pre><code>static irqreturn_t vp_interrupt(int irq, void *opaque)\n{\n\tstruct virtio_pci_device *vp_dev = opaque;\n\tu8 isr;\n\n\t/* reading the ISR has the effect of also clearing it so it's very\n\t * important to save off the value. */\n\tisr = ioread8(vp_dev-&gt;isr);\n\n\t/* Configuration change?  Tell driver if it wants to know. */\n\tif (isr &amp; VIRTIO_PCI_ISR_CONFIG)\n\t\tvp_config_changed(irq, opaque);\n\n\treturn vp_vring_interrupt(irq, opaque);\n}\n</code></pre><p>就像前面说的一样vp_interrupt这个中断处理函数，一是处理配置变化，二是处理I/O结束。第二种的调用链为：vp_interrupt-&gt;vp_vring_interrupt-&gt;vring_interrupt。</p><pre><code>irqreturn_t vring_interrupt(int irq, void *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n......\n\tif (vq-&gt;vq.callback)\n\t\tvq-&gt;vq.callback(&amp;vq-&gt;vq);\n\n\treturn IRQ_HANDLED;\n}\n</code></pre><p>在vring_interrupt中，我们会调用callback函数，这个也是在前面注册过的，是virtblk_done。</p><p>接下来的调用链为：virtblk_done-&gt;virtqueue_get_buf-&gt;virtqueue_get_buf_ctx。</p><pre><code>void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,\n\t\t\t    void **ctx)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tvoid *ret;\n\tunsigned int i;\n\tu16 last_used;\n......\n\tlast_used = (vq-&gt;last_used_idx &amp; (vq-&gt;vring.num - 1));\n\ti = virtio32_to_cpu(_vq-&gt;vdev, vq-&gt;vring.used-&gt;ring[last_used].id);\n\t*len = virtio32_to_cpu(_vq-&gt;vdev, vq-&gt;vring.used-&gt;ring[last_used].len);\n......\n\t/* detach_buf clears data, so grab it now. */\n\tret = vq-&gt;desc_state[i].data;\n\tdetach_buf(vq, i, ctx);\n\tvq-&gt;last_used_idx++;\n......\n\treturn ret;\n}\n</code></pre><p>在virtqueue_get_buf_ctx中，我们可以看到，virtio前端的vring中的last_used_idx加一，说明这块数据qemu后端已经消费完毕。我们可以通过detach_buf将其放入空闲队列中，留给以后的写入请求使用。</p><p>至此，整个存储虚拟化的写入流程才全部完成。</p><h2>总结时刻</h2><p>下面我们来总结一下存储虚拟化的场景下，整个写入的过程。</p><ul>\n<li>在虚拟机里面，应用层调用write系统调用写入文件。</li>\n<li>write系统调用进入虚拟机里面的内核，经过VFS，通用块设备层，I/O调度层，到达块设备驱动。</li>\n<li>虚拟机里面的块设备驱动是virtio_blk，它和通用的块设备驱动一样，有一个request  queue，另外有一个函数make_request_fn会被设置为blk_mq_make_request，这个函数用于将请求放入队列。</li>\n<li>虚拟机里面的块设备驱动是virtio_blk会注册一个中断处理函数vp_interrupt。当qemu写入完成之后，它会通知虚拟机里面的块设备驱动。</li>\n<li>blk_mq_make_request最终调用virtqueue_add，将请求添加到传输队列virtqueue中，然后调用virtqueue_notify通知qemu。</li>\n<li>在qemu中，本来虚拟机正处于KVM_RUN的状态，也即处于客户机状态。</li>\n<li>qemu收到通知后，通过VM exit指令退出客户机状态，进入宿主机状态，根据退出原因，得知有I/O需要处理。</li>\n<li>qemu调用virtio_blk_handle_output，最终调用virtio_blk_handle_vq。</li>\n<li>virtio_blk_handle_vq里面有一个循环，在循环中，virtio_blk_get_request函数从传输队列中拿出请求，然后调用virtio_blk_handle_request处理请求。</li>\n<li>virtio_blk_handle_request会调用blk_aio_pwritev，通过BlockBackend驱动写入qcow2文件。</li>\n<li>写入完毕之后，virtio_blk_req_complete会调用virtio_notify通知虚拟机里面的驱动。数据写入完成，刚才注册的中断处理函数vp_interrupt会收到这个通知。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/79/0c/79ad143a3149ea36bc80219940d7d00c.jpg\" alt=\"\"></p><h2>课堂练习</h2><p>请你沿着代码，仔细分析并牢记virtqueue的结构以及写入和读取方式。这个结构在下面的网络传输过程中，还要起大作用。</p><p>欢迎留言和我分享你的疑惑和见解，也欢迎收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。</p><p><img src=\"https://static001.geekbang.org/resource/image/8c/37/8c0a95fa07a8b9a1abfd394479bdd637.jpg\" alt=\"\"></p>","neighbors":{"left":{"article_title":"53 | 存储虚拟化（上）：如何建立自己保管的单独档案库？","id":110697},"right":{"article_title":"55 | 网络虚拟化：如何成立独立的合作部？","id":111686}},"comments":[]}