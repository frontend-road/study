{"id":355081,"title":"06 | 存储系统：空间换时间，还是时间换空间？","content":"<p>你好，我是吴磊。</p><p>今天，我们来学习Spark的存储系统，它和我们上一讲学过的调度系统一样，都是Spark分布式计算引擎的基础设施之一。</p><p>你可能会问：“在日常的开发工作中，除了业务逻辑实现，我真的需要去关心这么底层的东西吗？”确实，存储系统离开发者比较远。不过，如果把目光落在存储系统所服务的对象上，你很可能会改变这种看法。</p><p>接下来，咱们就先来看看Spark 存储系统都为谁服务，再去探讨它有哪些重要组件，以及它是如何工作的，带你一次性摸透存储系统。</p><h2>Spark存储系统是为谁服务的？</h2><p>Spark 存储系统用于存储 3个方面的数据，<strong>分别是RDD 缓存、Shuffle 中间文件、广播变量。我们一个一个来说。</strong></p><p>RDD缓存指的<strong>是将RDD以缓存的形式物化到内存或磁盘的过程</strong>。对于一些计算成本和访问频率都比较高的RDD来说，缓存有两个好处：一是通过截断DAG，可以降低失败重试的计算开销；二是通过对缓存内容的访问，可以有效减少从头计算的次数，从整体上提升作业端到端的执行性能。</p><p>而要说起Shuffle中间文件，我们就不得不提Shuffle这个话题。在很多场景中，Shuffle都扮演着性能瓶颈的角色，解决掉Shuffle引入的问题之后，执行性能往往能有立竿见影的提升。因此，凡是与Shuffle有关的环节，你都需要格外地重视。</p><!-- [[[read_end]]] --><p>关于Shuffle的工作原理，我们后面会详细来讲。这里，咱们先简单理解一下Shuffle的计算过程就可以了。它的计算过程可以分为2个阶段：</p><ul>\n<li><strong>Map阶段</strong>：Shuffle writer按照Reducer的分区规则将中间数据写入本地磁盘；</li>\n<li><strong>Reduce 阶段</strong>：Shuffle reader从各个节点下载数据分片，并根据需要进行聚合计算。</li>\n</ul><p>Shuffle中间文件实际上就是Shuffle Map阶段的输出结果，这些结果会以文件的形式暂存于本地磁盘。在Shuffle Reduce阶段，Reducer通过网络拉取这些中间文件用于聚合计算，如求和、计数等。在集群范围内，Reducer想要拉取属于自己的那部分中间数据，就必须要知道这些数据都存储在哪些节点，以及什么位置。而这些关键的元信息，正是由Spark存储系统保存并维护的。因此你看，<strong>没有存储系统，Shuffle是玩不转的。</strong></p><p>最后，我们再来说说广播变量。在日常开发中，广播变量往往用于在集群范围内分发访问频率较高的小数据。<strong>利用存储系统，广播变量可以在Executors进程范畴内保存全量数据。</strong>这样一来，对于同一Executors内的所有计算任务，应用就能够以Process local的本地性级别，来共享广播变量中携带的全量数据了。</p><p>总的来说，<strong>这3个服务对象是Spark应用性能调优的有力“抓手”，而它们又和存储系统有着密切的联系，因此想要有效运用这3个方面的调优技巧，我们就必须要对存储系统有足够的理解。</strong></p><h2>存储系统的基本组件有哪些？</h2><p>与调度系统类似，Spark存储系统是一个囊括了众多组件的复合系统，如BlockManager、BlockManagerMaster、MemoryStore、DiskStore和DiskBlockManager等等。</p><p>不过，家有千口、主事一人，<strong>BlockManager是其中最为重要的组件，它在Executors端负责统一管理和协调数据的本地存取与跨节点传输</strong>。这怎么理解呢？我们可以从2方面来看。</p><ul>\n<li>\n<p>对外，BlockManager与Driver端的BlockManagerMaster通信，不仅定期向BlockManagerMaster汇报本地数据元信息，还会不定时按需拉取全局数据存储状态。另外，不同Executors的BlockManager之间也会以Server/Client模式跨节点推送和拉取数据块。</p>\n</li>\n<li>\n<p>对内，BlockManager通过组合存储系统内部组件的功能来实现数据的存与取、收与发。</p>\n</li>\n</ul><p>那么，对于RDD缓存、Shuffle中间文件和广播变量这3个服务对象来说，BlockManager又是如何存储的呢？<strong>Spark存储系统提供了两种存储抽象：MemoryStore和DiskStore。BlockManager正是利用它们来分别管理数据在内存和磁盘中的存取。</strong></p><p>其中，广播变量的全量数据存储在Executors进程中，因此它由MemoryStore管理。Shuffle中间文件往往会落盘到本地节点，所以这些文件的落盘和访问就要经由DiskStore。相比之下，RDD缓存会稍微复杂一些，由于RDD缓存支持内存缓存和磁盘缓存两种模式，因此我们要视情况而定，缓存在内存中的数据会封装到MemoryStore，缓存在磁盘上的数据则交由DiskStore管理。</p><p>有了MemoryStore和DiskStore，我们暂时解决了数据“存在哪儿”的问题。但是，这些数据该以“什么形式”存储到MemoryStore和DiskStore呢？<strong>对于数据的存储形式，Spark存储系统支持两种类型：对象值（Object Values）和字节数组（Byte Array）</strong>。它们之间可以相互转换，其中，对象值压缩为字节数组的过程叫做序列化，而字节数组还原成原始对象值的过程就叫做反序列化。</p><p>形象点来说，序列化的字节数组就像是从宜家家具超市购买的待组装板材，对象值则是将板材根据说明书组装而成的各种桌椅板凳。显而易见，对象值这种存储形式的优点是拿来即用、所见即所得，缺点是所需的存储空间较大、占地儿。相比之下，序列化字节数组的空间利用率要高得多。不过要是你着急访问里面的数据对象，还得进行反序列化，有点麻烦。</p><p><strong>由此可见，对象值和字节数组二者之间存在着一种博弈关系，</strong>也就是所谓的“以空间换时间”和“以时间换空间”，两者之间该如何取舍，我们还是要看具体的应用场景。<strong>核心原则就是：如果想省地儿，你可以优先考虑字节数组；如果想以最快的速度访问对象，还是对象值更直接一些。</strong> 不过，这种选择的烦恼只存在于 MemoryStore 之中，而DiskStore只能存储序列化后的字节数组，毕竟，凡是落盘的东西，都需要先进行序列化。</p><h2>透过RDD缓存看MemoryStore</h2><p>知道了存储系统有哪些核心的组件，下面，我们接着来说说MemoryStore和DiskStore这两个组件是怎么管理内存和磁盘数据的。</p><p>刚刚我们提到，<strong>MemoryStore同时支持存储对象值和字节数组这两种不同的数据形式，并且统一采用MemoryEntry数据抽象对它们进行封装</strong>。</p><p>MemoryEntry有两个实现类：DeserializedMemoryEntry和SerializedMemoryEntry，分别用于封装原始对象值和序列化之后的字节数组。DeserializedMemoryEntry用 Array[T]来存储对象值序列，其中T是对象类型，而SerializedMemoryEntry使用ByteBuffer来存储序列化后的字节序列。</p><p>得益于MemoryEntry对于对象值和字节数组的统一封装，MemoryStore能够借助一种高效的数据结构来统一存储与访问数据块：LinkedHashMap[BlockId, MemoryEntry]，即 Key 为BlockId，Value 是MemoryEntry的链式哈希字典。在这个字典中，一个Block对应一个MemoryEntry。显然，这里的MemoryEntry既可以是DeserializedMemoryEntry，也可以是 SerializedMemoryEntry。有了这个字典，我们通过BlockId即可方便地查找和定位MemoryEntry，实现数据块的快速存取。</p><p>概念这么多，命名也这么相似，是不是看起来就让人“头大”？别着急，接下来，咱们以RDD缓存为例，来看看存储系统是如何利用这些数据结构，把RDD封装的数据实体缓存到内存里去。</p><p>在RDD的语境下，我们往往用数据分片（Partitions/Splits）来表示一份分布式数据，但在存储系统的语境下，我们经常会用数据块（Blocks）来表示数据存储的基本单元。<strong>在逻辑关系上，RDD的数据分片与存储系统的Block一一对应，也就是说一个RDD数据分片会被物化成一个内存或磁盘上的Block。</strong></p><p>因此，如果用一句话来概括缓存RDD的过程，就是将RDD计算数据的迭代器（Iterator）进行物化的过程，流程如下所示。具体来说，可以分成三步走。</p><p><img src=\"https://static001.geekbang.org/resource/image/1y/0b/1yy5fd9f111f4cab0edc7cf582bd2b0b.jpg?wh=4326*1359\" alt=\"\" title=\"利用MemoryStore在内存中缓存RDD数据内容\"></p><p>既然要把数据内容缓存下来，自然得先把RDD的迭代器展开成实实在在的数据值才行。因此，<strong>第一步就是通过调用putIteratorAsValues或是putIteratorAsBytes方法，把RDD迭代器展开为数据值，然后把这些数据值暂存到一个叫做ValuesHolder的数据结构里。</strong>这一步，我们通常把它叫做“Unroll”。</p><p><strong>第二步，为了节省内存开销，我们可以在存储数据值的ValuesHolder上直接调用toArray或是toByteBuffer操作，把ValuesHolder转换为MemoryEntry数据结构。</strong>注意啦，这一步的转换不涉及内存拷贝，也不产生额外的内存开销，因此Spark官方把这一步叫做“从Unroll memory到Storage memory的Transfer（转移）”。</p><p><strong>第三步，这些包含RDD数据值的MemoryEntry和与之对应的BlockId，会被一起存入Key 为BlockId、Value 是MemoryEntry引用的链式哈希字典中。</strong>因此，LinkedHashMap[BlockId, MemoryEntry]缓存的是关于数据存储的元数据，MemoryEntry才是真正保存RDD数据实体的存储单元。换句话说，大面积占用内存的不是哈希字典，而是一个又一个的MemoryEntry。</p><p>总的来说，RDD数据分片、Block和MemoryEntry三者之间是一一对应的，当所有的RDD数据分片都物化为MemoryEntry，并且所有的（Block ID, MemoryEntry）对都记录到LinkedHashMap字典之后，RDD就完成了数据缓存到内存的过程。</p><p>这里，你可能会问：“如果内存空间不足以容纳整个RDD怎么办？”很简单，强行把大RDD塞进有限的内存空间肯定不是明智之举，所以Spark会按照LRU策略逐一清除字典中最近、最久未使用的Block，以及其对应的MemoryEntry。相比频繁的展开、物化、换页所带来的性能开销，缓存下来的部分数据对于RDD高效访问的贡献可以说微乎其微。</p><h2>透过Shuffle看DiskStore</h2><p>相比MemoryStore，DiskStore就相对简单很多，因为它并不需要那么多的中间数据结构才能完成数据的存取。<strong>DiskStore中数据的存取本质上就是字节序列与磁盘文件之间的转换</strong>，它通过putBytes方法把字节序列存入磁盘文件，再通过getBytes方法将文件内容转换为数据块。</p><p>不过，要想完成两者之间的转换，像数据块与文件的对应关系、文件路径等等这些元数据是必不可少的。MemoryStore采用链式哈希字典来维护类似的元数据，DiskStore这个狡猾的家伙并没有亲自维护这些元数据，而是请了DiskBlockManager这个给力的帮手。</p><p><strong>DiskBlockManager的主要职责就是，记录逻辑数据块Block与磁盘文件系统中物理文件的对应关系，每个Block都对应一个磁盘文件。</strong>同理，每个磁盘文件都有一个与之对应的Block ID，这就好比货架上的每一件货物都有唯一的 ID 标识。</p><p>DiskBlockManager在初始化的时候，首先根据配置项spark.local.dir在磁盘的相应位置创建文件目录。然后，在spark.local.dir指定的所有目录下分别创建子目录，子目录的个数由配置项spark.diskStore.subDirectories控制，它默认是64。所有这些目录均用于存储通过DiskStore进行物化的数据文件，如RDD缓存文件、Shuffle中间结果文件等。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/4f/1eccayy6d9b7348ceea3cf3b12913a4f.jpg?wh=2802*1711\" alt=\"\" title=\"DiskStore 中数据的存与取\"></p><p>接下来，我们再以Shuffle中间文件为例，来说说DiskStore与DiskBlockManager的交互过程。</p><p><strong>Spark默认采用SortShuffleManager来管理Stages间的数据分发，在Shuffle write过程中，有3类结果文件：temp_shuffle_XXX、shuffle_XXX.data和shuffle_XXX.index。</strong>Data文件存储分区数据，它是由temp文件合并而来的，而index文件记录data文件内不同分区的偏移地址。Shuffle中间文件具体指的就是data文件和index文件，temp文件作为暂存盘文件最终会被删除。</p><p>在Shuffle write的不同阶段，Shuffle manager通过BlockManager调用DiskStore的putBytes方法将数据块写入文件。文件由DiskBlockManager创建，文件名就是putBytes方法中的Block ID，这些文件会以“temp_shuffle”或“shuffle”开头，保存在spark.local.dir目录下的子目录里。</p><p>在Shuffle read阶段，Shuffle manager再次通过BlockManager调用DiskStore的getBytes方法，读取data文件和index文件，将文件内容转化为数据块，最终这些数据块会通过网络分发到Reducer端进行聚合计算。</p><h2>小结</h2><p>掌握存储系统是我们进行Spark性能调优的关键一步，我们可以分为三步来掌握。</p><p>第一步，我们要明确存储系统的服务对象，分别是RDD缓存、Shuffle和广播变量。</p><ul>\n<li>RDD缓存：一些计算成本和访问频率较高的RDD，可以以缓存的形式物化到内存或磁盘中。这样一来，既可以避免DAG频繁回溯的计算开销，也能有效提升端到端的执行性能</li>\n<li>Shuffle：Shuffle中间文件的位置信息，都是由Spark存储系统保存并维护的，没有存储系统，Shuffle是玩不转的</li>\n<li>广播变量：利用存储系统，广播变量可以在Executors进程范畴内保存全量数据，让任务以Process local的本地性级别，来共享广播变量中携带的全量数据。</li>\n</ul><p>第二步，我们要搞清楚存储系统的两个重要组件：MemoryStore和DiskStore。其中，MemoryStore用来管理数据在内存中的存取，DiskStore用来管理数据在磁盘中的存取。</p><p>对于存储系统的3个服务对象来说，广播变量由MemoryStore管理，Shuffle中间文件的落盘和访问要经由DiskStore，而RDD缓存因为会同时支持内存缓存和磁盘缓存两种模式，所以两种组件都有可能用到。</p><p>最后，我们要理解MemoryStore和DiskStore的工作原理。</p><p>MemoryStore支持对象值和字节数组，统一采用MemoryEntry数据抽象对它们进行封装。对象值和字节数组二者之间存在着一种博弈关系，所谓的“以空间换时间”和“以时间换空间”，两者的取舍还要看具体的应用场景。</p><p>DiskStore则利用DiskBlockManager维护的数据块与磁盘文件的对应关系，来完成字节序列与磁盘文件之间的转换。</p><h2>每日一练</h2><ol>\n<li>结合RDD数据存储到MemoryStore的过程，你能推演出通过MemoryStore通过getValues/getBytes方法去访问RDD缓存内容的过程吗？</li>\n<li>参考RDD缓存存储的过程，你能推演出广播变量存入MemoryStore的流程吗？</li>\n</ol><p>期待在留言区看到你的思考和讨论，我们下一讲见！</p>","comments":[{"had_liked":false,"id":285438,"user_name":"Shockang","can_delete":false,"product_type":"c1","uid":1263546,"ip_address":"","ucode":"B871D731F6E6B1","user_header":"https://static001.geekbang.org/account/avatar/00/13/47/ba/d36340c1.jpg","comment_is_top":false,"comment_ctime":1616814029,"is_pvip":false,"replies":[{"id":"103598","content":"Shock老弟，不得不说，简直太棒了！Perfect x 2，标准答案，天衣无缝，满分💯！<br><br>从答案能看出来，这部分源码你已经吃的相当透了，大赞～ 咱们有读者群，不知道你在不在里边，务必加入啊！详情页有入口，实在找不到，加我微信好友，搜rJunior，或者“方块K”，我把你拉进去～","user_name":"作者回复","comment_id":285438,"uid":"1043100","ip_address":"","utype":1,"ctime":1616837255,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"169120538573","product_id":100073401,"comment_content":"1.getBytes&#47;getValues 的实现都比较简单，都是先对LinkedHashMap加锁，通过blockId取出对应的MemoryEntry，然后通过模式匹配，getBytes负责处理序列化的SerializedMemoryEntry，并返回Option[ChunkedByteBuffer]，ChunkedByteBuffer是一个只读字节缓冲区，物理上存储为多个块而不是单个连续数组；getValues负责处理对象值序列DeserializedMemoryEntry，返回一个Iterator<br>2.我先描述下调用链路：TorrentBroadcast#writeBlocks -&gt; BlockManager#putBytes -&gt; BlockManager#save 到这一步会判断存储级别，如果useMemory&amp;&amp;deserialized，会走这条链路：BlockManager#saveDeserializedValuesToMemoryStore -&gt; MemoryStore#putIteratorAsValues -&gt; MemoryStore#putIterator，这一步尝试将给定的块作为值或字节放入内存存储。 但是迭代器可能太大，无法具体化并存储在内存中。为了避免OOM异常，这里会逐渐展开迭代器，同时定期检查是否有足够的可用内存。如果块被成功物化，那么物化过程中使用的临时展开内存就被“转移”到存储内存中；再回到上面存储级别的判断，如果使用内存并且序列化，则走下面的调用链路：BlockManager#saveSerializedValuesToMemoryStore -&gt; MemoryStore#putBytes，这里会测试MemoryStore中是否有足够的空间。如果空间足够，则创建ByteBuffer并将其放入MemoryStore。否则就不会创建ByteBuffer。最终会用SerializedMemoryEntry将 ByteBuffer 封装起来，放到老师在文中提到的LinkedHashMap。可惜极客时间评论没办法发图片，不然调用链路看起来会更直观。","like_count":40,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517675,"discussion_content":"Shock老弟，不得不说，简直太棒了！Perfect x 2，标准答案，天衣无缝，满分💯！\n\n从答案能看出来，这部分源码你已经吃的相当透了，大赞～ 咱们有读者群，不知道你在不在里边，务必加入啊！详情页有入口，实在找不到，加我微信好友，搜rJunior，或者“方块K”，我把你拉进去～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616837255,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1589973,"avatar":"https://static001.geekbang.org/account/avatar/00/18/42/d5/1f020437.jpg","nickname":"Blex先生","note":"","ucode":"5B3987B724D5BF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":370952,"discussion_content":"跟着大佬们的思路看了MemoryStore.scala的部分代码，获益匪浅！","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1619590363,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285392,"user_name":"超级达达","can_delete":false,"product_type":"c1","uid":1159758,"ip_address":"","ucode":"153FCE3113AD11","user_header":"https://static001.geekbang.org/account/avatar/00/11/b2/4e/13a993a5.jpg","comment_is_top":false,"comment_ctime":1616771142,"is_pvip":false,"replies":[{"id":"103581","content":"非常好的问题！LinkedHashMap也是一种HashMap的，但在内部用一个双向链表，来维护键值对的顺序，每个键值对同时存储在哈希表、和双向链表中。<br><br>我们来看LinkedHashMap特性：<br>1 插入有序，这个好理解，就是在链表后追加元素<br>2 访问有序，这个就厉害了。访问有序说的是，对一个kv操作，不管put、get，元素都会被挪到链表的末尾，味道出来了～<br><br>在spark rdd cache场景下，第一个特性不重要，重要的是第二个特性。当storage memory不足，spark需要删除rdd cache的时候，遵循的是lru，那么问题来了，它咋实现的lru，答案就是它充分利用LinkedHashMap第二个特性，啥也不用做，就轻松地做到了这一点，机不机智？ 6不6？","user_name":"作者回复","comment_id":285392,"uid":"1043100","ip_address":"","utype":1,"ctime":1616808420,"user_name_real":"吴磊"}],"discussion_count":7,"race_medal":0,"score":"87516117062","product_id":100073401,"comment_content":"这里为什么用到了LinkedHashMap而不是普通的HashMap？什么场景下需要保证访问Block的有序性？","like_count":21,"discussions":[{"author":{"id":1544692,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83errHypG6kuO0V0bRwp74rm8srjoQ4zXUBNNLMcY19uNdz8Ea3rOFuBJibXMHWePMwBYpGsyyxiav0ibw/132","nickname":"闯闯","note":"","ucode":"D32958F8EF26BD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":376070,"discussion_content":"本文中提到Map太大会使用LRU算法进行清除。那么使用LinkedHashMap是最好的选择。因为LinkedHashMap能直接实现LRU算法。这部分属于算法的范畴了：下面给一个Demo\n\npublic class LinkedLRUCache<K, V> extends LinkedHashMap<K, V> {\n\n    private int cacheSize;\n\n    public LinkedLRUCache(int cacheSize) {\n        // initialCapacity loadFactor 都不重要, 重要的是 accessOrder\n        super((int) (Math.ceil((cacheSize / 0.75)) + 1), 0.75f, true);\n        this.cacheSize = cacheSize;\n    }\n\n    @Override\n    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {\n        return size() > cacheSize;\n    }\n}","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1621948283,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1544692,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83errHypG6kuO0V0bRwp74rm8srjoQ4zXUBNNLMcY19uNdz8Ea3rOFuBJibXMHWePMwBYpGsyyxiav0ibw/132","nickname":"闯闯","note":"","ucode":"D32958F8EF26BD","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377041,"discussion_content":"牛！赞~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1622471285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":376070,"ip_address":""},"score":377041,"extra":""}]},{"author":{"id":1159758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/b2/4e/13a993a5.jpg","nickname":"超级达达","note":"","ucode":"153FCE3113AD11","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361958,"discussion_content":"一语点破，谢谢老师！","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1616812777,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1159758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/b2/4e/13a993a5.jpg","nickname":"超级达达","note":"","ucode":"153FCE3113AD11","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404066,"discussion_content":"问的好啊，和同学沾光了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634219375,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":361958,"ip_address":""},"score":404066,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517666,"discussion_content":"非常好的问题！LinkedHashMap也是一种HashMap的，但在内部用一个双向链表，来维护键值对的顺序，每个键值对同时存储在哈希表、和双向链表中。\n\n我们来看LinkedHashMap特性：\n1 插入有序，这个好理解，就是在链表后追加元素\n2 访问有序，这个就厉害了。访问有序说的是，对一个kv操作，不管put、get，元素都会被挪到链表的末尾，味道出来了～\n\n在spark rdd cache场景下，第一个特性不重要，重要的是第二个特性。当storage memory不足，spark需要删除rdd cache的时候，遵循的是lru，那么问题来了，它咋实现的lru，答案就是它充分利用LinkedHashMap第二个特性，啥也不用做，就轻松地做到了这一点，机不机智？ 6不6？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616808420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2406292,"avatar":"https://static001.geekbang.org/account/avatar/00/24/b7/94/22121c60.jpg","nickname":"Kendrick","note":"","ucode":"5DF1269295D24E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":376009,"discussion_content":"赞","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1621926481,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390017,"discussion_content":"想问一下，使用spark-sql rdd cache 的正确姿势是怎样的呢，结合lru清楚策略，是不是不用担心cache导致oom的问题（只是个人理想主义 》.-.《","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629603894,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285404,"user_name":"来世愿做友人 A","can_delete":false,"product_type":"c1","uid":1181606,"ip_address":"","ucode":"EF20966B0F27E1","user_header":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","comment_is_top":false,"comment_ctime":1616777305,"is_pvip":false,"replies":[{"id":"103583","content":"Perfect + Awesome！欢迎来群里参与更多讨论哈～ 课程的详情页有读者群的二维码，期待你的加入～<br><br>ps：有个小细节，从executors拉取分片、分担driver压力，据我了解，这部分功能，code已经ready，不过应该还没有merge到master。至少之前是这样哈～ 不知道最近有没有什么变化","user_name":"作者回复","comment_id":285404,"uid":"1043100","ip_address":"","utype":1,"ctime":1616809285,"user_name_real":"吴磊"}],"discussion_count":7,"race_medal":0,"score":"40271482969","product_id":100073401,"comment_content":"第一题：无论 getBytes 还是 getValues，都是使用 blockId 从文中的 linkedHashMap 获取 memoryEntry，并且都转换成 Iterator 返回<br>第二题：同理，因为都有那几个 BlockManager 等组件管理，所以，广播变量首先也需要 blockId，查看子类实现有 BroadcastBlockId，格式是broadcast_&quot; + broadcastId + xxx。使用 blockCast 的时候，首先在 driver 端进行存储，广播变量driver端默认是 MEMORY_AND_DISK，并且优先写入 memory，只有存不下才写入 disk，并且似乎无法修改这存储等级，并且会存两份数据，一份是供 driver 端使用，和默认4m压缩的序列化数据（MEMORY_AND_DISK_SER)，供 executor 远程拉取，blockId 分别是 broadcast_&quot; + broadcastId 和 broadcast_&quot; + broadcastId + &quot;piece&quot; + range(0, numBlocks) 命名。然后在 executor 端，调用 Broadcast.value，首先会查找本地缓存是否有，没有就会拉取 driver 或者已经拉取的其它 executor 的块，并且是通过 broadcast_&quot; + broadcastId + &quot;piece&quot; + range(0, numBlocks) 随机获取存储块，分散 driver 的压力，然后以 MEMORY_AND_DISK_SER 的级别存储在本地。后续就和 rdd 缓存数据一样类似，用 putBytes 存储到本地缓存或 disk。","like_count":9,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517669,"discussion_content":"Perfect + Awesome！欢迎来群里参与更多讨论哈～ 课程的详情页有读者群的二维码，期待你的加入～\n\nps：有个小细节，从executors拉取分片、分担driver压力，据我了解，这部分功能，code已经ready，不过应该还没有merge到master。至少之前是这样哈～ 不知道最近有没有什么变化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616809285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362646,"discussion_content":"这个ticket：【Executor side broadcast for broadcast joins】https://issues.apache.org/jira/browse/SPARK-17556，看上去还是进行中的状态。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616997696,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":364612,"discussion_content":"哦哦，这个可能我没有描述清楚，我的场景限定在 driver 端产生的 广播变量，这个 broadcase joins 是在 executor 产生的需要广播的变量，场景没考虑全，sorry sorry。这块代码没有研究，后面看看。目前看这种场景应该是先收集到 driver，但是猜测也不会立马广播，而是和场景在 driver 端差不多，触发再拉取，这时候可以从各个已有的节点拉取，减轻负载","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617538455,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362646,"ip_address":""},"score":364612,"extra":""},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366421,"discussion_content":"多讨论讨论挺好的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618057053,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":364612,"ip_address":""},"score":366421,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362644,"discussion_content":"我又看了下master branch，发现广播变量的创建过程还是这样的：\n\nBroadcastExchangeExec中的relationFuture用于获取广播变量内容\n在relationFuture内部：\n  1. 先是调用executeCollectIterator生成内容relation；\n     其中，executeCollectIterator调用collect把结果集收集到driver端\n  2. 然后用sparkContext.broadcast(relation)，把生成好的内容广播到各个Executors\n并没有看到哪里从Executors拉取数据分片、来减轻driver负载。\n\n老弟share一下你看到的代码路径？\n\n并且，这里还有提示driver内存不够的exception：\nnew OutOfMemoryError(&#34;Not enough memory to build and broadcast the table to all &#34; + &#34;worker nodes. As a workaround, you can either disable broadcast by setting &#34; + s&#34;${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 or increase the spark &#34; + s&#34;driver memory by setting ${SparkLauncher.DRIVER_MEMORY} to a higher value.&#34;).initCause(oe.getCause))","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616997335,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":364611,"discussion_content":"TorrentBroadcast 中 private lazy val _value: T = readBroadcastBlock() 是 lazy 修饰的，然后创建实例的时候，调用 TorrentBroadcast.writeBlocks，只是写到了 BlockManager 中，并没有发生广播，只有 executor 的确用到广播变量，才会触发拉取 TorrentBroadcast.readBlocks。判断 executor 的本地 BlockManager 是否有存储，没有会拉取 driver 或者 executor 端的","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617538215,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362644,"ip_address":""},"score":364611,"extra":""}]},{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361992,"discussion_content":"应该是，我看的是 3.0 的分支","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616819497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":300863,"user_name":"特种流氓","can_delete":false,"product_type":"c1","uid":1248825,"ip_address":"","ucode":"D9985CBA8B4AAD","user_header":"https://static001.geekbang.org/account/avatar/00/13/0e/39/174741d1.jpg","comment_is_top":false,"comment_ctime":1625409763,"is_pvip":false,"replies":[{"id":"109049","content":"好问题，还真可以~<br><br>Shuffle write的中间文件，都会落盘到spark.local.dir这个配置项指定的文件目录，如果内存足够大的话，可以用Ramdisk开辟一块内存，用来当作是文件系统。<br><br>然后，把spark.local.dir指向到这里，那么中间文件实际上就是暂存到了内存中，这样就能实现完全的“内存计算”，也就是计算的整个过程，从头到尾，都是在内存里面完成。","user_name":"作者回复","comment_id":300863,"uid":"1043100","ip_address":"","utype":1,"ctime":1625526322,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"27395213539","product_id":100073401,"comment_content":"spark 做shuffle的时候，shuffle write 要写入磁盘，是否可以直接通过内存传输","like_count":7,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522838,"discussion_content":"好问题，还真可以~\n\nShuffle write的中间文件，都会落盘到spark.local.dir这个配置项指定的文件目录，如果内存足够大的话，可以用Ramdisk开辟一块内存，用来当作是文件系统。\n\n然后，把spark.local.dir指向到这里，那么中间文件实际上就是暂存到了内存中，这样就能实现完全的“内存计算”，也就是计算的整个过程，从头到尾，都是在内存里面完成。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625526322,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1296063,"avatar":"https://static001.geekbang.org/account/avatar/00/13/c6/bf/52b3f71d.jpg","nickname":"dawn","note":"","ucode":"1757B28F1EF5C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585374,"discussion_content":"我看到的时候，也在想，一定要落盘嘛，直接丢内存不是更快，反正只是中间数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661501924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江苏"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291143,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620047429,"is_pvip":false,"replies":[{"id":"105489","content":"完美~ Perfect x 2！！！","user_name":"作者回复","comment_id":291143,"uid":"1043100","ip_address":"","utype":1,"ctime":1620120549,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"18799916613","product_id":100073401,"comment_content":"第一题：<br>getValues&#47;getBytes两个方法都是通过blockId来获取缓存数据的，通过blockId获取对应的MemoryEntry的时候会对LinkedHashMap加同步锁，然后通过模式来匹配获取的MemoryEntry是DeserializedMemoryEntry还是SerializedMemoryEntry，getBytes方法用来获取SerializedMemoryEntry并返回ChunkedByteBuffer，getValues方法用来获取DeserializedMemoryEntry并返回迭代器Iterator。<br><br>第二题：<br><br>我们在对一个RDD进行广播之后，返回的是一个Broadcast[T]，而Broadcast是一个抽象类，它目前（Spark 2.4.5的源码）只有一个实现类TorrentBroadcast。TorrentBroadcast的机制：driver将序列化的对象划分为多个小的chunks（chunks为ByteBuffer类型的数组，即Array[ByteBuffer]），然后将这些chunks存储到driver的BlockManager中。<br><br>整个广播变量存入MemoryStore的流程如下：<br><br>1. TorrentBroadcast#writeBlocks()方法：将要广播的对象划分为多个blocks，并将这些blocks保存到BlockManager，主要通过blockManager#putBytes()方法来实现的；<br>2. blockManager#putBytes()方法通过调用 doPutBytes() 方法将序列化的bytes（ChunkedByteBuffer）通过指定的StorageLevel保存到 memoryStore 中；<br>3. 接下来，重点就在doPutBytes() 方法的实现，首先它会根据传入此方法中的 StorageLevel 来判断缓存是写入内存还是磁盘，也就是用 MemoryStore还是DiskStore来进行缓存；<br>4. 如果缓存级别中使用了内存，则会进一步通过缓存级别中有没有指定序列化来判断是存对象值序列还是字节序列。（1）如果是deserialized（非序列化）的，就将bytes进行反序列化，然后调用 memoryStore#putIteratorAsValues()方法——&gt;memoryStore#putIterator() 将block保存到MemoryStore，putIterator()方法中，可能会因为迭代器太大，无法物化存储在内存中。为了避免OOM异常，此方法将逐步展开迭代器，同时定期检查是否有足够的空闲内存。如果block被成功物化，那么物化期间使用的临时展开内存（unroll memory）将被“转移”到存储内存（StorageMemory），因此我们不会获得比存储块实际需要的更多的内存；（2）如果是serialized（序列化）的，还会进一步判断内存模型（MemoryMode）是堆内存（ON_HEAP）还是非堆内存（ON_HEAP），是哪种内存模型，就申请对应的StorageMemory，并将bytes实例化为一个SerializedMemoryEntry放入entries（LinkedHashMap）。<br>5. 如果缓存级别中使用了磁盘，则会调用 diskStore#putBytes()进行数据的缓存。","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519383,"discussion_content":"完美~ Perfect x 2！！！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620120549,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287246,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1617853435,"is_pvip":false,"replies":[{"id":"104302","content":"对，你说的没错，是shuffle过程中，归并排序，这部分细节shuffle那一讲有介绍。具体哪个类，如果我没记错的话，应该是External Sorter，应该是这个，我回去翻翻源码，再确认一下哈～","user_name":"作者回复","comment_id":287246,"uid":"1043100","ip_address":"","utype":1,"ctime":1617881346,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"18797722619","product_id":100073401,"comment_content":"老师你好，文中提到“Data 文件存储分区数据，它是由 temp 文件合并而来的”<br>shuffle过程中的文件合并这个逻辑应该是归并排序来实现的吧，这段逻辑是哪个类负责的？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518246,"discussion_content":"对，你说的没错，是shuffle过程中，归并排序，这部分细节shuffle那一讲有介绍。具体哪个类，如果我没记错的话，应该是External Sorter，应该是这个，我回去翻翻源码，再确认一下哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617881346,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":299243,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1624525845,"is_pvip":false,"replies":[{"id":"108575","content":"你说的算法工程师，我理解是机器学习算法工程师，如果是这样的话，那其实我不推荐去读Spark的源码，因为没有必要。对于机器学习算法工程师来说，熟悉Spark MLlib里面特征处理部分的内容性价比会更高，比如说各种预处理、归一化、离散化；Embedding的各种处理技巧（包括矩阵分解）；特征选择，等等。这些东西对于一个ML算法工程师来说，会非常的有用。<br><br>不过，如果你说的算法工程师，就是数据结构算法工程师，那我觉得读一读Spark的源码实现还是很有帮助的。通过阅读源码，可以学习人家的各种实现方法，比如抽象、封装、算法实现、巧用数据结构（如LinkedHashMap），诸如此类，凡此种种~","user_name":"作者回复","comment_id":299243,"uid":"1043100","ip_address":"","utype":1,"ctime":1624633225,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"10214460437","product_id":100073401,"comment_content":"吴老师，您好，我是Spark底层小白，我的问题是：<br><br>学习Spark源码对于一个算法工程师有什么作用呢？在日常的开发工作中，学习它能够发挥什么作用呢？<br><br>希望得到老师的解惑～ ","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522373,"discussion_content":"你说的算法工程师，我理解是机器学习算法工程师，如果是这样的话，那其实我不推荐去读Spark的源码，因为没有必要。对于机器学习算法工程师来说，熟悉Spark MLlib里面特征处理部分的内容性价比会更高，比如说各种预处理、归一化、离散化；Embedding的各种处理技巧（包括矩阵分解）；特征选择，等等。这些东西对于一个ML算法工程师来说，会非常的有用。\n\n不过，如果你说的算法工程师，就是数据结构算法工程师，那我觉得读一读Spark的源码实现还是很有帮助的。通过阅读源码，可以学习人家的各种实现方法，比如抽象、封装、算法实现、巧用数据结构（如LinkedHashMap），诸如此类，凡此种种~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624633225,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":380711,"discussion_content":"非常感谢老悉心的回复哈！（我说的是推荐算法工程师哈，应该和机器学习工程师差不多）","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1624652732,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":304512,"user_name":"Kung","can_delete":false,"product_type":"c1","uid":1086261,"ip_address":"","ucode":"862B39B7263447","user_header":"https://static001.geekbang.org/account/avatar/00/10/93/35/0cfb5732.jpg","comment_is_top":false,"comment_ctime":1627461744,"is_pvip":false,"replies":[{"id":"110177","content":"多积累，多思考，多问“为什么”~","user_name":"作者回复","comment_id":304512,"uid":"1043100","ip_address":"","utype":1,"ctime":1627482316,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"5922429040","product_id":100073401,"comment_content":"回答的同学都太牛了，我是Spark小白，怎么才能达到这样的深度？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524067,"discussion_content":"多积累，多思考，多问“为什么”~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627482316,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404072,"discussion_content":"加油！！！有志者事竟成","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634220235,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290060,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1619342685,"is_pvip":true,"replies":[{"id":"105204","content":"没错~","user_name":"作者回复","comment_id":290060,"uid":"1043100","ip_address":"","utype":1,"ctime":1619434842,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"5914309981","product_id":100073401,"comment_content":"getBytes&#47;getValue的实现方法是通过BlockID加锁访问entry。Byte获得是序列化之后的字节流，具有ChunkedByteBuffer属性，存储类型属性，对象类型属性。Values获得是非序列化的对象，返回对象数组，对象大小，对象类型，MemoryEntry的Mode是堆上存储。","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519099,"discussion_content":"没错~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619434842,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":354263,"user_name":"Geek_5b7d28","can_delete":false,"product_type":"c1","uid":2057476,"ip_address":"北京","ucode":"BAA1CDD539C369","user_header":"","comment_is_top":false,"comment_ctime":1660225640,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1660225640","product_id":100073401,"comment_content":"相比频繁的展开、物化、换页所带来的性能开销，缓存下来的部分数据对于 RDD 高效访问的贡献可以说微乎其微。<br>老师，这句话怎么理解呢","like_count":0},{"had_liked":false,"id":324631,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1638521704,"is_pvip":false,"replies":[{"id":"117820","content":"老弟可以在回顾下这里哈：<br>“<br>既然要把数据内容缓存下来，自然得先把 RDD 的迭代器展开成实实在在的数据值才行。因此，第一步就是通过调用 putIteratorAsValues 或是 putIteratorAsBytes 方法，把 RDD 迭代器展开为数据值，然后把这些数据值暂存到一个叫做 ValuesHolder 的数据结构里。这一步，我们通常把它叫做“Unroll”。第二步，为了节省内存开销，我们可以在存储数据值的 ValuesHolder 上直接调用 toArray 或是 toByteBuffer 操作，把 ValuesHolder 转换为 MemoryEntry 数据结构。注意啦，这一步的转换不涉及内存拷贝，也不产生额外的内存开销，因此 Spark 官方把这一步叫做“从 Unroll memory 到 Storage memory 的 Transfer（转移）”。第三步，这些包含 RDD 数据值的 MemoryEntry 和与之对应的 BlockId，会被一起存入 Key 为 BlockId、Value 是 MemoryEntry 引用的链式哈希字典中。因此，LinkedHashMap[BlockId, MemoryEntry]缓存的是关于数据存储的元数据，MemoryEntry 才是真正保存 RDD 数据实体的存储单元。换句话说，大面积占用内存的不是哈希字典，而是一个又一个的 MemoryEntry。<br>”<br><br>可以这么简单来理解，LinkedHashMap仅仅是存储关于Cache的元数据，它本身占用的内存空间并不大，真正占用内存的，是一个又一个MemoryEntry，这里面存储的，是实实在在的分区数据。<br><br>关于Iterator那里，老弟需要在温习下DAG的概念哈，DAG中的所有算子的计算，实际上都是以迭代器的方式，从源头访问并读取数据，到了Cache这一步，需要把Iterator展开成实实在在的数据，然后在走Cache后面的2、3两步~","user_name":"作者回复","comment_id":324631,"uid":"1043100","ip_address":"","utype":1,"ctime":1638611380,"user_name_real":"编辑"}],"discussion_count":1,"race_medal":0,"score":"1638521704","product_id":100073401,"comment_content":"老师我想问几个问题：<br>1、rdd缓存的终点 LinkedHashMap 是携带了全量数据被存储到内存中了吗？<br>2、rdd缓存的起点 iterator 会有磁盘io吗？如果没有的话为什么要调用一系列函数把rdd换一种数据结构（LinkedHashMap）存到内存中呢（是因为这种数据结构支持随机读取吗）？如果有的话那读取rdd缓存的时候使用 blockId 从 linkedHashMap 获取 memoryEntry，然后转换成 Iterator 返回，岂不是从“纯内存读取”变成了“需要磁盘io”？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":535931,"discussion_content":"老弟可以在回顾下这里哈：\n“\n既然要把数据内容缓存下来，自然得先把 RDD 的迭代器展开成实实在在的数据值才行。因此，第一步就是通过调用 putIteratorAsValues 或是 putIteratorAsBytes 方法，把 RDD 迭代器展开为数据值，然后把这些数据值暂存到一个叫做 ValuesHolder 的数据结构里。这一步，我们通常把它叫做“Unroll”。第二步，为了节省内存开销，我们可以在存储数据值的 ValuesHolder 上直接调用 toArray 或是 toByteBuffer 操作，把 ValuesHolder 转换为 MemoryEntry 数据结构。注意啦，这一步的转换不涉及内存拷贝，也不产生额外的内存开销，因此 Spark 官方把这一步叫做“从 Unroll memory 到 Storage memory 的 Transfer（转移）”。第三步，这些包含 RDD 数据值的 MemoryEntry 和与之对应的 BlockId，会被一起存入 Key 为 BlockId、Value 是 MemoryEntry 引用的链式哈希字典中。因此，LinkedHashMap[BlockId, MemoryEntry]缓存的是关于数据存储的元数据，MemoryEntry 才是真正保存 RDD 数据实体的存储单元。换句话说，大面积占用内存的不是哈希字典，而是一个又一个的 MemoryEntry。\n”\n\n可以这么简单来理解，LinkedHashMap仅仅是存储关于Cache的元数据，它本身占用的内存空间并不大，真正占用内存的，是一个又一个MemoryEntry，这里面存储的，是实实在在的分区数据。\n\n关于Iterator那里，老弟需要在温习下DAG的概念哈，DAG中的所有算子的计算，实际上都是以迭代器的方式，从源头访问并读取数据，到了Cache这一步，需要把Iterator展开成实实在在的数据，然后在走Cache后面的2、3两步~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638611380,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320361,"user_name":"〆、维生素ゝ","can_delete":false,"product_type":"c1","uid":1230071,"ip_address":"","ucode":"7A2AB5A26F0F49","user_header":"https://static001.geekbang.org/account/avatar/00/12/c4/f7/3cff78f1.jpg","comment_is_top":false,"comment_ctime":1636277358,"is_pvip":false,"replies":[{"id":"116286","content":"没错，从本地的BlockManager获取，BlockManager会定期和BlockManagerMaster同步，获取全局的Block信息，因此，每个BlockManager，手里都有一份完整的元信息拷贝，这些信息都是通过心跳机制来同步的~","user_name":"作者回复","comment_id":320361,"uid":"1043100","ip_address":"","utype":1,"ctime":1636435337,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1636277358","product_id":100073401,"comment_content":"请教一个问题，在reducesult阶段拉取数据的时候，程序是如何做到精确找到数据位置啊 ？从BlockManagerMaster获取信息 BlockManager跨节点同步","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529967,"discussion_content":"没错，从本地的BlockManager获取，BlockManager会定期和BlockManagerMaster同步，获取全局的Block信息，因此，每个BlockManager，手里都有一份完整的元信息拷贝，这些信息都是通过心跳机制来同步的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636435337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320056,"user_name":"福","can_delete":false,"product_type":"c1","uid":2455712,"ip_address":"","ucode":"F2FC7AF5D433C6","user_header":"https://static001.geekbang.org/account/avatar/00/25/78/a0/7a248ddc.jpg","comment_is_top":false,"comment_ctime":1636070402,"is_pvip":true,"replies":[{"id":"116043","content":"linkdhashmap这个数据结构，每个Executors都会存一份，隶属于每个Executors中BlockManager的一部分。Executors的BlockManager，与Driver的BlockManagerMaster，可以理解成是子与父的关系。Driver的BlockManagerMaster，对于linkdhashmap有全局视角，其他Executors从Driver的BlockManagerMaster同步信息，然后逐步完善自己的linkdhashmap内容。<br><br>我理解这个问题的重点，其实本质上是BlockManager与BlockManagerMaster的关系，以及Executors与Driver的关系，不止是linkdhashmap，其他的存储信息，也都是类似的关系。就是，BlockManagerMaster维持全局信息，BlockManager从BlockManagerMaster获取并同步全局信息。当然，BlockManagerMaster的全局信息，也是来自一个个的BlockManager~","user_name":"作者回复","comment_id":320056,"uid":"1043100","ip_address":"","utype":1,"ctime":1636095632,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"1636070402","product_id":100073401,"comment_content":"老师，您好，文中说  你可能会问：“如果内存空间不足以容纳整个 RDD 怎么办？” 您这边主要是说的lru算法来删除最近的，，，，我这个地方不是很理解，linkedHashMap存储了blockid和memoryEntry，它不是存储的完整的一个rdd吧，比如 map得到的rdd.cache A，filter得到的rdd.catch  B，那么rdd A和rdd B应该缓存在多个executor的linkdhashmap中， 而不是一个linkdhashmap来存储整个rdd blockid和memoryEntry","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529857,"discussion_content":"linkdhashmap这个数据结构，每个Executors都会存一份，隶属于每个Executors中BlockManager的一部分。Executors的BlockManager，与Driver的BlockManagerMaster，可以理解成是子与父的关系。Driver的BlockManagerMaster，对于linkdhashmap有全局视角，其他Executors从Driver的BlockManagerMaster同步信息，然后逐步完善自己的linkdhashmap内容。\n\n我理解这个问题的重点，其实本质上是BlockManager与BlockManagerMaster的关系，以及Executors与Driver的关系，不止是linkdhashmap，其他的存储信息，也都是类似的关系。就是，BlockManagerMaster维持全局信息，BlockManager从BlockManagerMaster获取并同步全局信息。当然，BlockManagerMaster的全局信息，也是来自一个个的BlockManager~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636095632,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1221146,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a2/1a/f2c8988b.jpg","nickname":"萝莉巴索小布丁","note":"","ucode":"00C285326B8C77","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":534702,"discussion_content":"老师，就这个问题，我还想问，极端情况下，即使清除所有的blocks，内存空间也无法容纳整个RDD时，就会变成磁盘存储吗？还是说会OOM？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638259638,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":529857,"ip_address":""},"score":534702,"extra":""}]}]},{"had_liked":false,"id":290058,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1619341732,"is_pvip":true,"replies":[{"id":"105205","content":"对，用BlockId去LinkedHashMap里面检索就行~","user_name":"作者回复","comment_id":290058,"uid":"1043100","ip_address":"","utype":1,"ctime":1619434954,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1619341732","product_id":100073401,"comment_content":"MemoryStore通过partition对应的BlockID获得对应的partition MemoryEntry。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519097,"discussion_content":"对，用BlockId去LinkedHashMap里面检索就行~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619434954,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288466,"user_name":"William","can_delete":false,"product_type":"c1","uid":2086573,"ip_address":"","ucode":"EC29D9CD616183","user_header":"https://static001.geekbang.org/account/avatar/00/1f/d6/ad/850992a5.jpg","comment_is_top":false,"comment_ctime":1618480178,"is_pvip":false,"replies":[{"id":"104694","content":"没事，别着急兄弟，说说细节，因为你这么说，我比较难判断。比如说：<br><br>1. 什么模型？LR？SVM？树模型（RF还是GBDT）？K-Means？还是DNN？<br>2. Data schema，数据模式是什么？也就是你的训练样本，都包含哪些字段？都是什么类型？数据是否归一化？<br>3. 模型超参数，比如，epoch、learning rate、树深、多少棵树、多少层layers、激活函数、优化算法（SGD、Adam）？等等。超参本身会影响模型训练的执行性能，比如learning rate太低，就会影响模型收敛效率。再比如优化算法，执行效率上面差的更多。<br>4. 缓存的比例，是不是100%？还是只缓存了一部分？<br><br>期待你的留言，我们一起把问题搞定~","user_name":"作者回复","comment_id":288466,"uid":"1043100","ip_address":"","utype":1,"ctime":1618485073,"user_name_real":"吴磊"}],"discussion_count":4,"race_medal":0,"score":"1618480178","product_id":100073401,"comment_content":"老师，您好。我刚刚接触spark，看你的专栏实在收获良多，非常感谢！在这里心急的向你请教一个问题：在使用spark训练模型发现：仅20W的数据，数据是缓存到内存的，但模型训练时间长达4-5h，完全不知改从何处入手调优，麻烦老师给予指点。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518632,"discussion_content":"没事，别着急兄弟，说说细节，因为你这么说，我比较难判断。比如说：\n\n1. 什么模型？LR？SVM？树模型（RF还是GBDT）？K-Means？还是DNN？\n2. Data schema，数据模式是什么？也就是你的训练样本，都包含哪些字段？都是什么类型？数据是否归一化？\n3. 模型超参数，比如，epoch、learning rate、树深、多少棵树、多少层layers、激活函数、优化算法（SGD、Adam）？等等。超参本身会影响模型训练的执行性能，比如learning rate太低，就会影响模型收敛效率。再比如优化算法，执行效率上面差的更多。\n4. 缓存的比例，是不是100%？还是只缓存了一部分？\n\n期待你的留言，我们一起把问题搞定~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618485073,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390018,"discussion_content":"Perfect x 2","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629604434,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2086573,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/d6/ad/850992a5.jpg","nickname":"William","note":"","ucode":"EC29D9CD616183","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367946,"discussion_content":"1.模型是GBDT，做二分类预测。\n2.样本共30个特征字段，包括数值和离散标签，因为是树模型，未做归一化。\n3.超参数基本未做太多调整，maxIter=200，stepSize=0.1, maxDepth=7，minInstancePerNode=10。\n4.缓存比例说是spark.storage.memoryFraction 参数吗，以默认0.6，未修改。\n另外:executorMemory = 8G，executorCores=2, numExecutor=10, defaultParallelism=30","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618499587,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2086573,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/d6/ad/850992a5.jpg","nickname":"William","note":"","ucode":"EC29D9CD616183","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":370495,"discussion_content":"从两个方面来说：\n\n1. 模型超参：\n1）迭代次数太多了，100就行，maxIter=200，容易过拟合，当然，还要结合你的具体情况，我理解20w的数据，体量非常小了，100都多，可能50就行。我们这边上亿的数据，也才需要100次迭代；\n2）maxDepth，树太深了，5就行，7容易过拟合，4-5是不错的选择，甚至3都行；\n3）把cache用上，把你的training samples，cache到内存里面去，默认的cache mode就行，20W样本，缓存完全能放；\n\n2. 系统集成：\nGBDT能搞定的事情，XGB都能搞定，可以用XGB over Spark（XGBoost4J-Spark）的集成方式来搞定树模型。XGB本身在工程上有并行优化，算法上也有优化，相比GBDT，执行速度快、收敛速度也更快","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1619434787,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":367946,"ip_address":""},"score":370495,"extra":""}]}]},{"had_liked":false,"id":285340,"user_name":"L3nvy","can_delete":false,"product_type":"c1","uid":1271157,"ip_address":"","ucode":"0B74B27C121D56","user_header":"https://static001.geekbang.org/account/avatar/00/13/65/75/f9d7e8b7.jpg","comment_is_top":false,"comment_ctime":1616747169,"is_pvip":false,"replies":[{"id":"103560","content":"Perfect！第二题呢？","user_name":"作者回复","comment_id":285340,"uid":"1043100","ip_address":"","utype":1,"ctime":1616757877,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1616747169","product_id":100073401,"comment_content":"1. 通过blockId在LinkedHashMap[BlockId, MemoryEntry]中获取MemoryEntry，getBytes返回SerializedMemoryEntry中的ByteBuffer，然后还需要反序列化成迭代器才能被使用；getValues返回DeserializedMemoryEntry中的Array[T]并转换成迭代器使用；","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517648,"discussion_content":"Perfect！第二题呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616757877,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285324,"user_name":"Stony.修行僧","can_delete":false,"product_type":"c1","uid":1061277,"ip_address":"","ucode":"0F2368F7D93E4A","user_header":"https://static001.geekbang.org/account/avatar/00/10/31/9d/daad92d2.jpg","comment_is_top":false,"comment_ctime":1616741100,"is_pvip":false,"replies":[{"id":"103561","content":"第一题能再展开说说吗？ 比如具体怎么操作？<br><br>第二题再想想哈，参考rdd cache的过程～","user_name":"作者回复","comment_id":285324,"uid":"1043100","ip_address":"","utype":1,"ctime":1616757987,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1616741100","product_id":100073401,"comment_content":"问题1: 应该是在操作 linkedhashmap<br>问题2: 广播变量值相对比较小，应该是存在在array里面","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517646,"discussion_content":"第一题能再展开说说吗？ 比如具体怎么操作？\n\n第二题再想想哈，参考rdd cache的过程～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616757987,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}