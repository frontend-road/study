{"id":1573,"title":"05 数学基础 | 万物皆数，信息亦然：信息论","content":"<p>近年来的科学研究不断证实，不确定性才是客观世界的本质属性。换句话说，上帝还真就掷骰子。不确定性的世界只能使用概率模型来描述，正是对概率的刻画促成了信息论的诞生。</p>\n<p>1948年，供职于美国贝尔实验室的物理学家克劳德·香农发表了著名论文《通信的数学理论》（A Mathematical Theory of Communication），给出了对信息这一定性概念的定量分析方法，标志着信息论作为一门学科的正式诞生。</p>\n<p>香农在《通信的数学理论》中开宗明义：“通信的基本问题是在一点精确地或近似地复现在另一点所选取的消息。消息通常有意义，即根据某种体系，消息本身指向或关联着物理上或概念上的特定实体。但消息的语义含义与工程问题无关，重要的问题是一条消息来自于一个所有可能的消息的集合。”</p>\n<p>这样一来，所有类型的信息都被抽象为逻辑符号，这拓展了通信任务的范畴与信息论的适用性，也将信息的传播和处理完全剥离。</p>\n<p><strong>信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>在生活中，信息的载体是消息，而不同的消息带来的信息即使在直观感觉上也是不尽相同的。比如，“中国男子足球队获得世界杯冠军”的信息显然要比“中国男子乒乓球队获得世界杯冠军”的信息要大得多。</p>\n<p>究其原因，国足勇夺世界杯是如假包换的小概率事件（如果不是不可能事件的话），发生的可能性微乎其微；而男乒夺冠已经让国人习以为常，丢掉冠军的可能性才是意外。因此，以不确定性来度量信息是一种合理的方式。不确定性越大的消息可能性越小，其提供的信息量就越大。</p>\n<p>香农对信息的量化正是基于以上的思路，他定义了“熵”这一信息论中最基本最重要的概念。“熵”这个词来源于另一位百科全书式的科学家约翰·冯诺伊曼，他的理由是没人知道熵到底是什么。虽然这一概念已经在热力学中得到了广泛使用，但直到引申到信息论后，<strong>熵的本质才被解释清楚，即一个系统内在的混乱程度</strong>。</p>\n<p>在信息论中，如果事件$A$发生的概率为$p(A)$，则这个事件的自信息量的定义为 </p>\n<p>$$ h(A) = - \\log_2 p(A) $$</p>\n<p>如果国足闯进世界杯决赛圈，1:1000的夺冠赔率是个很乐观的估计，用这个赔率计算出的信息量约为10比特；而国乒夺冠的赔率不妨设为1:2，即使在这样高的赔率下，事件的信息量也只有1比特。两者之间的差距正是其可能性相差悬殊的体现。</p>\n<p>根据单个事件的自信息量可以计算包含多个符号的信源的信息熵。信源的信息熵是信源可能发出的各个符号的自信息量在信源构成的概率空间上的统计平均值。如果一个离散信源$X$包含$n$个符号，每个符号$a_i$的取值为$p(a_i)$，则$X$的信源熵为</p>\n<p> $$H(X) = -\\sum\\limits_{i = 1}^n p(a_i) \\log_2 p(a_i)$$</p>\n<p>信源熵描述了信源每发送一个符号所提供的平均信息量，是信源总体信息测度的均值。当信源中的每个符号的取值概率相等时，信源熵取到最大值$\\log _2 n$，意味着信源的随机程度最高。</p>\n<p>在概率论中有<strong>条件概率</strong>的概念，将条件概率扩展到信息论中，就可以得到<strong>条件熵</strong>。如果两个信源之间具有相关性，那么在已知其中一个信源$X$的条件下，另一个信源$Y$的信源熵就会减小。条件熵$H(Y|X)$表示的是在已知随机变量$X$的条件下另一个随机变量$Y$的不确定性，也就是在给定$X$时，根据$Y$的条件概率计算出的熵再对$X$求解数学期望：</p>\n<p>$$H(Y|X) = \\sum_{i = 1}^ n p(x_i) H(Y|X = x_i) $$</p>\n<p>$$= -\\sum_{i = 1}^ n p(x_i) \\sum_{j = 1}^ m p(y_j|x_i) \\log_2 p(y_j|x_i) $$</p>\n<p>$$ = - \\sum_{i = 1}^ n \\sum_{j = 1}^ m p(x_i, y_j) \\log_2 p(y_j|x_i) $$ </p>\n<p><strong>条件熵的意义在于先按照变量$X$的取值对变量$Y$进行了一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照$X$的分布计算其数学期望</strong>。</p>\n<p>以上课为例，学生在教室中可以任意选择座位，那么可能出现的座位分布会很多，其信源熵也就较大。如果对座位的选择添加一个限制条件，比如男生坐左边而女生坐右边，虽然左边的座位分布和右边的座位分布依然是随机的，但相对于未加限制时的情形就会简单很多。这就是分类带来的不确定性的下降。</p>\n<p>定义了条件信息熵后，就可以进一步得到<strong>互信息</strong>的概念 </p>\n<p>$$I(X; Y) = H(Y) - H(Y|X)$$</p>\n<p>互信息等于$Y$的信源熵减去已知$X$时$Y$的条件熵，即由$X$提供的关于$Y$的不确定性的消除，也可以看成是$X$给$Y$带来的<strong>信息增益</strong>。互信息这个名称在通信领域经常使用，信息增益则在机器学习领域中经常使用，两者的本质是一样的。</p>\n<p><strong>在机器学习中，信息增益常常被用于分类特征的选择</strong>。对于给定的训练数据集$Y$，$H(Y)$表示在未给定任何特征时，对训练集进行分类的不确定性；$H(Y|X)$则表示了使用特征$X$对训练集$Y$进行分类的不确定性。信息增益表示的就是特征$X$带来的对训练集$Y$分类不确定性的减少程度，也就是特征$X$对训练集$Y$的区分度。</p>\n<p>显然，信息增益更大的特征具有更强的分类能力。但信息增益的值很大程度上依赖于数据集的信息熵$H(Y)$，因而并不具有绝对意义。为解决这一问题，研究者又提出了<strong>信息增益比</strong>的概念，并将其定义为$g(X, Y) = I(X; Y) / H(Y)$。</p>\n<p>另一个在机器学习中经常使用的信息论概念叫作“<strong>Kullback-Leibler散度</strong>”，简称<strong>KL散度</strong>。KL散度是描述两个概率分布$P$和$Q$之间的差异的一种方法，其定义为</p>\n<p>$$D_{KL}(P||Q) = \\sum_{i = 1}^n p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)}$$</p>\n<p><strong>KL散度是对额外信息量的衡量</strong>。给定一个信源，其符号的概率分布为$P(X)$，就可以设计一种针对$P(X)$的最优编码，使得表示该信源所需的平均比特数最少（等于该信源的信源熵）。</p>\n<p>可是当信源的符号集合不变，而符合的概率分布变为$Q(X)$时，再用概率分布$P(X)$的最优编码对符合分布$Q(X)$的符号编码，此时编码结果的字符数就会比最优值多一些比特。</p>\n<p>KL散度就是用来衡量这种情况下平均每个字符多用的比特数，也可以表示两个分布之间的距离。</p>\n<p><strong>KL散度的两个重要性质是非负性和非对称性</strong>。</p>\n<p>非负性是指KL散度是大于或等于0的，等号只在两个分布完全相同时取到。</p>\n<p>非对称性则是指$D_{KL}(P||Q) \\ne D_{KL}(Q||P)$，即用$P(X)$去近似$Q(X)$和用$Q(X)$去近似$P(X)$得到的偏差是不同的，因此KL散度并不满足数学意义上对距离的定义，这一点需要注意。</p>\n<p>事实上，$D_{KL}(P||Q)$ 和$D_{KL}(Q||P)$代表了两种不同的近似方式。要让$D_{KL}(P||Q)$最小，需要让$Q(X)$在$P(X)$不等于0的位置同样不等于0；要让$D_{KL}(Q||P)$最小，则需要让$Q(X)$在$P(X)$等于0的位置同样等于0。</p>\n<p>除了以上定义的指标之外，信息论中还有一个重要定理，叫作“<strong>最大熵原理</strong>”。<strong>最大熵原理是确定随机变量统计特性时力图最符合客观情况的一种准则。对于一个未知的概率分布，最坏的情况就是它以等可能性取到每个可能的取值</strong>。这个时候的概率分布最均匀，也就是随机变量的随机程度最高，对它进行预测也就最困难。</p>\n<p>从这个角度看，最大熵原理的本质在于在推断未知分布时不引入任何多余的约束和假设，因而可以得到最不确定的结果，预测的风险也就最小。投资理财中的名言“不要把所有鸡蛋放在同一个篮子里”，就可以视为最大熵原理的一个实际应用。</p>\n<p>将最大熵原理应用到分类问题上就可以得到<strong>最大熵模型</strong>。在分类问题中，首先要确定若干特征函数作为分类的依据。为了保证特征函数的有效性，其在模型真实分布$P(X)$上的数学期望和在由训练数据集推导出的经验分布$\\tilde P(X)$上的数学期望应该相等，即对给定特征函数数学期望的估计应该是个无偏估计量。</p>\n<p>这样一来，每一个特征函数就对应了一个约束条件。分类的任务就是在这些约束条件下，确定一个最好的分类模型。由于除了这些约束条件之外，没有任何关于分类的先验知识，因而需要利用最大熵原理，求解出不确定性最大的条件分布，即让以下函数的取值最大化</p>\n<p>$$H(p) = -\\sum\\limits_{x, y} \\tilde p(x) p(y|x) \\log_2 p(y|x) $$</p>\n<p>式中的$p(y|x)$就是分类问题要确定的目标条件分布。计算上式的最大值实质上就是一个约束优化问题，由特征函数确定的约束条件可以通过<strong>拉格朗日乘子</strong>的引入去除其影响，转化为无约束优化问题。从数学上可以证明，这个模型的解是存在且唯一的。</p>\n<p>今天我和你分享了人工智能必备的信息论基础，着重于抽象概念的解释而非数学公式的推导，其要点如下：</p>\n<ul>\n<li>信息论处理的是客观世界中的不确定性；</li>\n<li>条件熵和信息增益是分类问题中的重要参数；</li>\n<li>KL散度用于描述两个不同概率分布之间的差异；</li>\n<li>最大熵原理是分类问题中的常用准则。</li>\n</ul>\n<p>信息论建立在概率的基础上，但其形式并不唯一，除了香农熵外也有其他关于熵的定义。那么概率与信息之间的关系对人工智能有什么启示呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e2/e5/e248d05acca0ac225b043a775bb221e5.jpg?wh=1110*855\" alt=\"\"></p>\n<p></p>\n","comments":[{"had_liked":false,"id":62467,"user_name":"姑射仙人","can_delete":false,"product_type":"c1","uid":1008517,"ip_address":"","ucode":"3EFC1F3E592165","user_header":"https://static001.geekbang.org/account/avatar/00/0f/63/85/1dc41622.jpg","comment_is_top":false,"comment_ctime":1548063572,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"48792703828","product_id":100003101,"comment_content":"可以参考数学之美第二版，第六章 - 信息的度量和作用","like_count":10,"discussions":[{"author":{"id":1556506,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLww7bo5wYQMqqNdiby23DiaDcIR0eNbaUffW7ZCP7eYcGwYtgFCGtIhfFmSqFQpp76I3JTTtibctTUw/132","nickname":"506第一影魔","note":"","ucode":"42E2848F57410A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382171,"discussion_content":"大家都好优秀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625458396,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1747506,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/aa/32/fbf775d8.jpg","nickname":"闲逸","note":"","ucode":"511625E1D29F21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":66872,"discussion_content":"还有得到app信息论四十讲","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1575113411,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":19860,"user_name":"Mr.Button","can_delete":false,"product_type":"c1","uid":1057546,"ip_address":"","ucode":"B6C7B9E3F97073","user_header":"https://static001.geekbang.org/account/avatar/00/10/23/0a/39524860.jpg","comment_is_top":false,"comment_ctime":1534125048,"is_pvip":false,"replies":[{"id":"10494","content":"以2为底计算出的单位就是二进制的比特。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1538282711,"ip_address":"","comment_id":19860,"utype":1}],"discussion_count":2,"race_medal":0,"score":"31598896120","product_id":100003101,"comment_content":"为什么log以2为底的函数这么常见...这里为什么取2","like_count":8,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":422003,"discussion_content":"以2为底计算出的单位就是二进制的比特。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1538282711,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210308,"discussion_content":"信息量有多个单位。以2为底，就是比特。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584716876,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3599,"user_name":"井中月","can_delete":false,"product_type":"c1","uid":1056027,"ip_address":"","ucode":"4ADBD773A50CE2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132","comment_is_top":false,"comment_ctime":1520038118,"is_pvip":false,"replies":[{"id":"762","content":"感谢你指出，这里的符号写的不够清晰，H(Y)其实应该写成H_X(Y)。H(Y)是直接用数据的分类结果计算出来的信息熵，H_X(Y)的下标X表示的是以特征X的取值为变量对数据集计算出的信息熵。所以当关注的特征X不同时，H_X(Y)也是不一样的。<br>信息增益比主要用在决策树当中，作用是消除多个取值的特征导致的偏差，因为多值特征的信息增益很大，但泛化性能却很差。比如，使用姓名作为特征可以得到较大的信息增益，因为它基本可以把每个人区分开来，但这种区分对于分类显然没什么帮助。这时就可以用信息增益比来一定程度上消除对多值属性的偏向性，但也不能完全消除。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1520479673,"ip_address":"","comment_id":3599,"utype":1}],"discussion_count":1,"race_medal":0,"score":"22994874598","product_id":100003101,"comment_content":"王老师，感谢您的回复。但是我还有点疑惑，X表示的是训练集的某个特征，Y相当于是训练集中需要被分类的变量，那么这样的话H(Y)就是一个定值，用它做分母和直接使用信息增益进行特征选择不就是一样的吗？","like_count":6,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415959,"discussion_content":"感谢你指出，这里的符号写的不够清晰，H(Y)其实应该写成H_X(Y)。H(Y)是直接用数据的分类结果计算出来的信息熵，H_X(Y)的下标X表示的是以特征X的取值为变量对数据集计算出的信息熵。所以当关注的特征X不同时，H_X(Y)也是不一样的。\n信息增益比主要用在决策树当中，作用是消除多个取值的特征导致的偏差，因为多值特征的信息增益很大，但泛化性能却很差。比如，使用姓名作为特征可以得到较大的信息增益，因为它基本可以把每个人区分开来，但这种区分对于分类显然没什么帮助。这时就可以用信息增益比来一定程度上消除对多值属性的偏向性，但也不能完全消除。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1520479673,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":1806,"user_name":"秦龙君","can_delete":false,"product_type":"c1","uid":1004181,"ip_address":"","ucode":"2085706DBABD5C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/52/95/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1514516930,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"22989353410","product_id":100003101,"comment_content":"学习了。这篇很难，后半部分暂时还看不懂。","like_count":6},{"had_liked":false,"id":1914,"user_name":"听天由己","can_delete":false,"product_type":"c1","uid":1019531,"ip_address":"","ucode":"59BC32324B88C5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8e/8b/38b93ca0.jpg","comment_is_top":false,"comment_ctime":1515076210,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18694945394","product_id":100003101,"comment_content":"看完之后，我努力应用如下：<br><br>消息是今天我学会了专栏的信息论部分，因为可能性较低，因而信息量较大，信息熵也就越大。<br><br>机器学习中的分类问题，可能还是缺乏较好的类比方法，初入门道就有些迷糊了。<br><br>看了些其他资料，这句话写得很妙，“学习就是一个熵减的过程”，学习的过程也就是使信息的不确定度下降的过程，这似乎就是机器学习的方向，然后再把《信息论、推理与学习算法》下载了，继续学习中。","like_count":5},{"had_liked":false,"id":3643,"user_name":"夜星辰","can_delete":false,"product_type":"c1","uid":1059833,"ip_address":"","ucode":"CD6BCBC73E7D8E","user_header":"https://static001.geekbang.org/account/avatar/00/10/2b/f9/203b0173.jpg","comment_is_top":false,"comment_ctime":1520212910,"is_pvip":false,"replies":[{"id":"783","content":"最大熵表示的是对未知的部分不做任何多余的假设，所以要选择符合已有知识但不确定性最大，也就是熵最大的分布，通俗说就是不要不懂装懂。对交叉熵的最小化意味着数据训练的模型要尽可能地接近真实模型，而真实模型又是建立在最大熵的前提下的。所以在优化时，要不断地调整训练的模型，以期更接近真实情况。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1520524010,"ip_address":"","comment_id":3643,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14405114798","product_id":100003101,"comment_content":"有一点理解上的困惑希望王老师帮忙解答下 <br><br>1. 熵表示的是信息量大小，从公式中知道随着概率增大，熵会变小。而机器学习中常用交叉熵作为目标函数，学习的过程是不断求取最小熵，也就是求取概率最大的参数，等价于极大似然估计法进行参数估计。<br>2. 但是我无法上述理解1和最大熵原理联系起来，请老师佐证下问题","like_count":4,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415984,"discussion_content":"最大熵表示的是对未知的部分不做任何多余的假设，所以要选择符合已有知识但不确定性最大，也就是熵最大的分布，通俗说就是不要不懂装懂。对交叉熵的最小化意味着数据训练的模型要尽可能地接近真实模型，而真实模型又是建立在最大熵的前提下的。所以在优化时，要不断地调整训练的模型，以期更接近真实情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1520524010,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":2154,"user_name":"wolfog","can_delete":false,"product_type":"c1","uid":1005815,"ip_address":"","ucode":"89BFEBE2E00B18","user_header":"https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg","comment_is_top":false,"comment_ctime":1516074695,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14400976583","product_id":100003101,"comment_content":"这个推荐大家可以看看吴军老师的数学之美其中就有关于最大熵和互信息等的介绍，讲的更加详细和通俗一些","like_count":4},{"had_liked":false,"id":15065,"user_name":"水木竹水","can_delete":false,"product_type":"c1","uid":1024566,"ip_address":"","ucode":"648E9CA4DEFAE1","user_header":"","comment_is_top":false,"comment_ctime":1530840273,"is_pvip":false,"replies":[{"id":"5171","content":"数据集确定了，总体的信息熵H(Y)就是常量，所以两个其实是等效的。之所以选信息增益一方面在于它和信息论一脉相承，意义清晰；另一方面，在取值上信息增益是越大越好，如果选一个越小越好的指标，有些反直觉。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1531112029,"ip_address":"","comment_id":15065,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10120774865","product_id":100003101,"comment_content":"首先感谢老师讲的非常好。有个疑惑问下老师，信息增益是H(Y)-H(Y|X)，后者是已知X情况下Y的不确定性，信息增益就是X对Y的确定性消除。H(Y|X)越小，说明X对Y的分类效果越好，为何决策树不直接用H(Y|X)选取主要特征，而用信息增益，H(Y)是变化的吗？","like_count":3,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":420227,"discussion_content":"数据集确定了，总体的信息熵H(Y)就是常量，所以两个其实是等效的。之所以选信息增益一方面在于它和信息论一脉相承，意义清晰；另一方面，在取值上信息增益是越大越好，如果选一个越小越好的指标，有些反直觉。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1531112029,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":9256,"user_name":"星运里的错","can_delete":false,"product_type":"c1","uid":1135940,"ip_address":"","ucode":"DA49E91584F0A1","user_header":"https://static001.geekbang.org/account/avatar/00/11/55/44/024204e2.jpg","comment_is_top":false,"comment_ctime":1526708478,"is_pvip":false,"replies":[{"id":"3851","content":"总体熵 - 特征分类之后每个类别的熵的总和 = 特征的信息增益<br>这里的信息增益表示的是分类之后残留的不确定度。如何一个特征能够将两个类别完全正确地分开，那它的信息增益是最大的，就等于数据集的熵。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1528376817,"ip_address":"","comment_id":9256,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10116643070","product_id":100003101,"comment_content":"信息增益表示的就是特征 X带来的对训练集 Y 分类不确定性的减少程度，也就是特征 X 对训练集 YY的区分度。<br>这句话意思是  总体熵-某个特征下的熵  ＝去除某个特征影响的熵   老师。这个公式对么？<br>我的理解是   熵对应着信息量的多少。熵大，意味着信息量大，信息混杂，也就是不确定性大。<br>当用到决策树中时，要保证分支所考虑的不确定性最小，也就是可用信息量纯净（少），所以我们要用使 某个特征影响的熵 最小的那个特征进行分支，也就是信息增益越大。<br>我感觉。。。我理解的好乱。求老师解惑下","like_count":3,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":418009,"discussion_content":"总体熵 - 特征分类之后每个类别的熵的总和 = 特征的信息增益\n这里的信息增益表示的是分类之后残留的不确定度。如何一个特征能够将两个类别完全正确地分开，那它的信息增益是最大的，就等于数据集的熵。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1528376817,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":191007,"user_name":"Simon","can_delete":false,"product_type":"c1","uid":1914504,"ip_address":"","ucode":"A8A2E3E57BD029","user_header":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","comment_is_top":false,"comment_ctime":1584717226,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5879684522","product_id":100003101,"comment_content":"Kullback-Leibler 散度，也叫库尔贝勒交叉熵","like_count":2},{"had_liked":false,"id":79430,"user_name":"Naraka，","can_delete":false,"product_type":"c1","uid":1236560,"ip_address":"","ucode":"CC3B405A44870E","user_header":"https://static001.geekbang.org/account/avatar/00/12/de/50/317159be.jpg","comment_is_top":false,"comment_ctime":1553483320,"is_pvip":false,"replies":[{"id":"57750","content":"最大熵考虑的是最随机的情况，不做任何多余假设，所以可以认为它最符合实际。虽然得到的结果有最大的不确定性，但这种不确定性和真实情况是匹配的，真实世界就是这么不确定，我们不能人为地去增加信息。就像拿到一个骰子，我们会默认每个面出现的概率都是1&#47;6，这就是最大熵思想。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1573460237,"ip_address":"","comment_id":79430,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5848450616","product_id":100003101,"comment_content":"老师，不知道现在提问还会不会回答，<br>“从这个角度看，最大熵原理的本质在于在推断未知分布时不引入任何多余的约束和假设，因而可以得到最不确定的结果，预测的风险也就最小。”<br>这一段没有看懂，为什么得到最不确定的结果，预测风险会最小？最不确定的，可能性很多，预测的结果不也更吗？","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444497,"discussion_content":"最大熵考虑的是最随机的情况，不做任何多余假设，所以可以认为它最符合实际。虽然得到的结果有最大的不确定性，但这种不确定性和真实情况是匹配的，真实世界就是这么不确定，我们不能人为地去增加信息。就像拿到一个骰子，我们会默认每个面出现的概率都是1/6，这就是最大熵思想。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573460237,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":2153,"user_name":"wolfog","can_delete":false,"product_type":"c1","uid":1005815,"ip_address":"","ucode":"89BFEBE2E00B18","user_header":"https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg","comment_is_top":false,"comment_ctime":1516073657,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5811040953","product_id":100003101,"comment_content":"之前看过吴军老师的《数学之美》，这一张还听得有点眉目，加油。","like_count":1,"discussions":[{"author":{"id":1488020,"avatar":"https://static001.geekbang.org/account/avatar/00/16/b4/94/2796de72.jpg","nickname":"追风筝的人","note":"","ucode":"2993D60F94C396","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363592,"discussion_content":"吴军老师 信息传 也很精彩","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617241422,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":1827,"user_name":"chucklau","can_delete":false,"product_type":"c1","uid":1039793,"ip_address":"","ucode":"1BB1CA463A0C40","user_header":"","comment_is_top":false,"comment_ctime":1514624118,"is_pvip":false,"replies":[{"id":"305","content":"可以参考MacKay的《信息论，推理与学习算法》","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1514819438,"ip_address":"","comment_id":1827,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5809591414","product_id":100003101,"comment_content":"嗯，这篇的内容很难理解，希望有其它更多的相关资料，谢谢老师。","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415543,"discussion_content":"可以参考MacKay的《信息论，推理与学习算法》","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1514819438,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":305044,"user_name":"黑山老妖","can_delete":false,"product_type":"c1","uid":1115958,"ip_address":"","ucode":"A1659F99C5BE1E","user_header":"https://static001.geekbang.org/account/avatar/00/11/07/36/d677e741.jpg","comment_is_top":false,"comment_ctime":1627789223,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1627789223","product_id":100003101,"comment_content":"感觉信息论就是机器学习在信息处理上的应用","like_count":0},{"had_liked":false,"id":286290,"user_name":"追风筝的人","can_delete":false,"product_type":"c1","uid":1488020,"ip_address":"","ucode":"2993D60F94C396","user_header":"https://static001.geekbang.org/account/avatar/00/16/b4/94/2796de72.jpg","comment_is_top":false,"comment_ctime":1617241382,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1617241382","product_id":100003101,"comment_content":"信息是为了消除不确定性，量化信息的单位是比特","like_count":1},{"had_liked":false,"id":190999,"user_name":"Simon","can_delete":false,"product_type":"c1","uid":1914504,"ip_address":"","ucode":"A8A2E3E57BD029","user_header":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","comment_is_top":false,"comment_ctime":1584716924,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1584716924","product_id":100003101,"comment_content":"互信息是一个随机变量包含另一个随机变量信息量的度量。","like_count":1},{"had_liked":false,"id":145932,"user_name":"上善若水","can_delete":false,"product_type":"c1","uid":1653332,"ip_address":"","ucode":"E3F15FB8576626","user_header":"https://static001.geekbang.org/account/avatar/00/19/3a/54/72402617.jpg","comment_is_top":false,"comment_ctime":1572403675,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1572403675","product_id":100003101,"comment_content":"局部信息增益","like_count":1},{"had_liked":false,"id":145929,"user_name":"上善若水","can_delete":false,"product_type":"c1","uid":1653332,"ip_address":"","ucode":"E3F15FB8576626","user_header":"https://static001.geekbang.org/account/avatar/00/19/3a/54/72402617.jpg","comment_is_top":false,"comment_ctime":1572403448,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1572403448","product_id":100003101,"comment_content":"概率只能在条件确定性的环境使用啊","like_count":1},{"had_liked":false,"id":58444,"user_name":"Snail@AI_ML","can_delete":false,"product_type":"c1","uid":1361881,"ip_address":"","ucode":"695C0F0F1C6E5C","user_header":"https://static001.geekbang.org/account/avatar/00/14/c7/d9/9fc367b5.jpg","comment_is_top":false,"comment_ctime":1547084245,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547084245","product_id":100003101,"comment_content":"非常棒，深入浅出，对照了培训课程之后，有一个更清晰的思路，虽然理解程度可能不够深，但觉得目前够用了，安利一波😄","like_count":1},{"had_liked":false,"id":53558,"user_name":"Geek_4b73dd","can_delete":false,"product_type":"c1","uid":1336205,"ip_address":"","ucode":"8FFA534448DFCF","user_header":"https://static001.geekbang.org/account/avatar/00/14/63/8d/bddef9bf.jpg","comment_is_top":false,"comment_ctime":1545674138,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1545674138","product_id":100003101,"comment_content":"老师你好，虽然留言里面提到了，但有一个问题还是不太明白，关于最大熵模型和交叉熵的。在网上看max最大熵模型的解时发现，其实max最大熵模型，就是max模型的最大似然估计，也就是说如果以logistic regression为例的化，max最大熵模型和max logistic regression的最大似然估计是一样的，而max logistic regression的最大似然估计其实就是minimize对应的cross entropy，所以其实最大熵模型和最小化cross entropy是不是其实是一回事？还是我理解的有些不对？谢谢老师啦！","like_count":1},{"had_liked":false,"id":3562,"user_name":"井中月","can_delete":false,"product_type":"c1","uid":1056027,"ip_address":"","ucode":"4ADBD773A50CE2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132","comment_is_top":false,"comment_ctime":1519903139,"is_pvip":false,"replies":[{"id":"698","content":"分母是训练数据集的信息熵，因为这里把训练集定为Y，所以分母就是H(Y)。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1519997963,"ip_address":"","comment_id":3562,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1519903139","product_id":100003101,"comment_content":"王老师，您好，我有个疑问，信息增益比里面的分母是不是应该是H(X)？","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415939,"discussion_content":"分母是训练数据集的信息熵，因为这里把训练集定为Y，所以分母就是H(Y)。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1519997963,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":2494,"user_name":"卡斯瓦德","can_delete":false,"product_type":"c1","uid":1011332,"ip_address":"","ucode":"E944E5BC507D5C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6e/84/45a909a6.jpg","comment_is_top":false,"comment_ctime":1517444946,"is_pvip":false,"replies":[{"id":"540","content":"奇迹其实就是小概率事件的发生","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1517547077,"ip_address":"","comment_id":2494,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1517444946","product_id":100003101,"comment_content":"看完这篇，突然觉得所谓的奇迹，其实就是信息熵不对等的结果，从某个面如何环境，物质看概率为百万分之一，从另一个面如自主意念等，概率可能就是十分之一，那么事件成就的结果其实就是KL后，不同的结果，饿可能总结有点问题，但是有那么个方向的感觉","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415761,"discussion_content":"奇迹其实就是小概率事件的发生","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1517547077,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}