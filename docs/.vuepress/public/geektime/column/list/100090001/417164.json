{"id":417164,"title":"02 | RDD与编程模型：延迟计算是怎么回事？","content":"<p>你好，我是吴磊。</p><p>在上一讲，我们一起开发了一个Word Count小应用，并把它敲入到spark-shell中去执行。Word Count的计算步骤非常简单，首先是读取数据源，然后是做分词，最后做分组计数、并把词频最高的几个词汇打印到屏幕上。</p><p>如果你也动手实践了这个示例，你可能会发现，在spark-shell的REPL里，所有代码都是立即返回、瞬间就执行完毕了，相比之下，只有最后一行代码，花了好长时间才在屏幕上打印出the、Spark、a、and和of这几个单词。</p><p>针对这个现象，你可能会觉得很奇怪：“读取数据源、分组计数应该是最耗时的步骤，为什么它们瞬间就返回了呢？打印单词应该是瞬间的事，为什么这一步反而是最耗时的呢？”要解答这个疑惑，我们还是得从RDD说起。</p><h2>什么是RDD</h2><p>为什么非要从RDD说起呢？首先，RDD是构建Spark分布式内存计算引擎的基石，很多Spark核心概念与核心组件，如DAG和调度系统都衍生自RDD。因此，深入理解RDD有利于你更全面、系统地学习 Spark 的工作原理。</p><p>其次，尽管RDD API使用频率越来越低，绝大多数人也都已经习惯于DataFrame和Dataset API，但是，无论采用哪种API或是哪种开发语言，你的应用在Spark内部最终都会转化为RDD之上的分布式计算。换句话说，如果你想要对Spark作业有更好的把握，前提是你要对RDD足够了解。</p><!-- [[[read_end]]] --><p>既然RDD如此重要，那么它到底是什么呢？用一句话来概括，<strong>RDD是一种抽象，是Spark对于分布式数据集的抽象，它用于囊括所有内存中和磁盘中的分布式数据实体</strong>。</p><p>在<a href=\"https://time.geekbang.org/column/article/415209\">上一讲</a>中，我们把RDD看作是数组，咱们不妨延续这个思路，通过对比RDD与数组之间的差异认识一下RDD。</p><p>我列了一个表，做了一下RDD和数组对比，你可以先扫一眼：</p><p><img src=\"https://static001.geekbang.org/resource/image/71/76/7149ddfb053edfed4397ee27dc09b376.jpg?wh=1369x718\" alt=\"\" title=\"RDD与数组的对比\"></p><p>我在表中从四个方面对数组和RDD进行了对比，现在我来详细解释一下。</p><p>首先，就概念本身来说，数组是实体，它是一种存储同类元素的数据结构，而RDD是一种抽象，它所囊括的是分布式计算环境中的分布式数据集。</p><p>因此，这两者第二方面的不同就是在活动范围，数组的“活动范围”很窄，仅限于单个计算节点的某个进程内，而RDD代表的数据集是跨进程、跨节点的，它的“活动范围”是整个集群。</p><p>至于数组和RDD的第三个不同，则是在数据定位方面。在数组中，承载数据的基本单元是元素，而RDD中承载数据的基本单元是数据分片。在分布式计算环境中，一份完整的数据集，会按照某种规则切割成多份数据分片。这些数据分片被均匀地分发给集群内不同的计算节点和执行进程，从而实现分布式并行计算。</p><p>通过以上对比，不难发现，<strong>数据分片</strong>（Partitions）是RDD抽象的重要属性之一。在初步认识了RDD之后，接下来咱们换个视角，从RDD的重要属性出发，去进一步深入理解RDD。要想吃透RDD，我们必须掌握它的4大属性：</p><ul>\n<li>partitions：数据分片</li>\n<li>partitioner：分片切割规则</li>\n<li>dependencies：RDD依赖</li>\n<li>compute：转换函数</li>\n</ul><p>如果单从理论出发、照本宣科地去讲这4大属性，未免过于枯燥、乏味、没意思！所以，我们从一个制作薯片的故事开始，去更好地理解RDD的4大属性。</p><h2>从薯片的加工流程看RDD的4大属性</h2><p>在很久很久以前，有个生产桶装薯片的工坊，工坊的规模较小，工艺也比较原始。为了充分利用每一颗土豆、降低生产成本，工坊使用 3 条流水线来同时生产 3 种不同尺寸的桶装薯片。3 条流水线可以同时加工 3 颗土豆，每条流水线的作业流程都是一样的，分别是清洗、切片、烘焙、分发和装桶。其中，分发环节用于区分小、中、大号 3 种薯片，3 种不同尺寸的薯片分别被发往第 1、2、3 条流水线。具体流程如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/da/4fc5769e03f68eae79ea92fbb4756bda.jpg?wh=1920x586\" alt=\"图片\" title=\"RDD的生活化类比\"></p><p>好了，故事讲完了。那如果我们把每一条流水线看作是分布式运行环境的计算节点，用薯片生产的流程去类比 Spark 分布式计算，会有哪些有趣的发现呢？</p><p>显然，这里的每一种食材形态，如“带泥土豆”、“干净土豆”、“土豆片”等，都可以看成是一个个RDD。而<strong>薯片的制作过程，实际上就是不同食材形态的转换过程</strong>。</p><p>起初，工人们从麻袋中把“带泥土豆”加载到流水线，这些土豆经过清洗之后，摇身一变，成了“干净土豆”。接下来，流水线上的切片机再把“干净土豆”切成“土豆片”，然后紧接着把这些土豆片放进烤箱。最终，土豆片烤熟之后，就变成了可以放心食用的即食薯片。</p><p>通过分析我们不难发现，不同食材形态之间的转换过程，与Word Count中不同RDD之间的转换过程如出一辙。</p><p>所以接下来，我们就结合薯片的制作流程，去理解RDD的4大属性。</p><p>首先，咱们沿着纵向，也就是从上到下的方向，去观察上图中土豆工坊的制作工艺。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/da/4fc5769e03f68eae79ea92fbb4756bda.jpg?wh=1920x586\" alt=\"图片\" title=\"RDD的生活化类比\"></p><p>我们可以看到对于每一种食材形态来说，流水线上都有多个实物与之对应，比如，“带泥土豆”是一种食材形态，流水线上总共有3颗“脏兮兮”的土豆同属于这一形态。</p><p>如果把“带泥土豆”看成是RDD的话，那么RDD的partitions属性，囊括的正是麻袋里那一颗颗脏兮兮的土豆。同理，流水线上所有洗净的土豆，一同构成了“干净土豆”RDD的partitions属性。</p><p>我们再来看RDD的partitioner属性，这个属性定义了把原始数据集切割成数据分片的切割规则。在土豆工坊的例子中，“带泥土豆”RDD的切割规则是随机拿取，也就是从麻袋中随机拿取一颗脏兮兮的土豆放到流水线上。后面的食材形态，如“干净土豆”、“土豆片”和“即食薯片”，则沿用了“带泥土豆”RDD的切割规则。换句话说，后续的这些RDD，分别继承了前一个RDD的partitioner属性。</p><p>这里面与众不同的是“分发的即食薯片”。显然，“分发的即食薯片”是通过对“即食薯片”按照大、中、小号做分发得到的。也就是说，对于“分发的即食薯片”来说，它的partitioner属性，重新定义了这个RDD数据分片的切割规则，也就是把先前RDD的数据分片打散，按照薯片尺寸重新构建数据分片。</p><p>由这个例子我们可以看出，数据分片的分布，是由RDD的partitioner决定的。因此，RDD的partitions属性，与它的partitioner属性是强相关的。</p><p>横看成岭侧成峰，很多事情换个视角看，相貌可能会完全不同。所以接下来，我们横向地，也就是沿着从左至右的方向，再来观察土豆工坊的制作工艺。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/da/4fc5769e03f68eae79ea92fbb4756bda.jpg?wh=1920x586\" alt=\"图片\" title=\"RDD的生活化类比\"></p><p>不难发现，流水线上的每一种食材形态，都是上一种食材形态在某种操作下进行转换得到的。比如，“土豆片”依赖的食材形态是“干净土豆”，这中间用于转换的操作是“切片”这个动作。回顾Word Count当中RDD之间的转换关系，我们也会发现类似的现象。</p><p><img src=\"https://static001.geekbang.org/resource/image/af/6d/af93e6f10b85df80a7d56a6c1965a36d.jpg?wh=1920x512\" alt=\"图片\" title=\"Word Count中的RDD转换\"></p><p>在数据形态的转换过程中，每个RDD都会通过dependencies属性来记录它所依赖的前一个、或是多个RDD，简称“父RDD”。与此同时，RDD使用compute属性，来记录从父RDD到当前RDD的转换操作。</p><p>拿Word Count当中的wordRDD来举例，它的父RDD是lineRDD，因此，它的dependencies属性记录的是lineRDD。从lineRDD到wordRDD的转换，其所依赖的操作是flatMap，因此，wordRDD的compute属性，记录的是flatMap这个转换函数。</p><p>总结下来，薯片的加工流程，与RDD的概念和4大属性是一一对应的：</p><ul>\n<li>不同的食材形态，如带泥土豆、土豆片、即食薯片等等，对应的就是RDD概念；</li>\n<li>同一种食材形态在不同流水线上的具体实物，就是 RDD 的 partitions 属性；</li>\n<li>食材按照什么规则被分配到哪条流水线，对应的就是 RDD 的 partitioner 属性；</li>\n<li>每一种食材形态都会依赖上一种形态，这种依赖关系对应的是 RDD 中的 dependencies 属性；</li>\n<li>不同环节的加工方法对应 RDD的 compute 属性。</li>\n</ul><p>在你理解了RDD的4大属性之后，还需要进一步了解RDD的编程模型和延迟计算。编程模型指导我们如何进行代码实现，而延迟计算是Spark分布式运行机制的基础。只有搞明白编程模型与延迟计算，你才能流畅地在Spark之上做应用开发，在实现业务逻辑的同时，避免埋下性能隐患。</p><h2>编程模型与延迟计算</h2><p>你还记得我在上一讲的最后，给你留的一道思考题吗：map、filter、flatMap和reduceByKey这些算子，有哪些共同点？现在我们来揭晓答案：</p><p>首先，这4个算子都是作用（Apply）在RDD之上、用来做RDD之间的转换。比如，flatMap作用在lineRDD之上，把lineRDD转换为wordRDD。</p><p>其次，这些算子本身是函数，而且它们的参数也是函数。参数是函数、或者返回值是函数的函数，我们把这类函数统称为“<strong>高阶函数</strong>”（Higher-order Functions）。换句话说，这4个算子，都是高阶函数。</p><p>关于高阶函数的作用与优劣势，我们留到后面再去展开。这里，我们先专注在RDD算子的第一个共性：<strong>RDD转换</strong>。</p><p>RDD是Spark对于分布式数据集的抽象，<strong>每一个RDD都代表着一种分布式数据形态</strong>。比如lineRDD，它表示数据在集群中以行（Line）的形式存在；而wordRDD则意味着数据的形态是单词，分布在计算集群中。</p><p>理解了RDD，那什么是RDD转换呢？别着急，我来以上次Word Count的实现代码为例，来给你讲一下。以下是我们上次用的代码：</p><pre><code class=\"language-scala\">import org.apache.spark.rdd.RDD\nval rootPath: String = _\nval file: String = s\"${rootPath}/wikiOfSpark.txt\"\n// 读取文件内容\nval lineRDD: RDD[String] = spark.sparkContext.textFile(file)\n// 以行为单位做分词\nval wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(\" \"))\nval cleanWordRDD: RDD[String] = wordRDD.filter(word =&gt; !word.equals(\"\"))\n// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))\n// 按照单词做分组计数\nval wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)\n// 打印词频最高的5个词汇\nwordCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(false).take(5)\n</code></pre><p>回顾Word Count示例，我们会发现，Word Count的实现过程，实际上就是不同RDD之间的一个转换过程。仔细观察我们会发现，Word Count示例中一共有4次RDD的转换，我来具体解释一下：</p><p>起初，我们通过调用textFile API生成lineRDD，然后用flatMap算子把lineRDD转换为wordRDD；<br>\n接下来，filter算子对wordRDD做过滤，并把它转换为不带空串的cleanWordRDD；<br>\n然后，为了后续的聚合计算，map算子把cleanWordRDD又转换成元素为（Key，Value）对的kvRDD；<br>\n最终，我们调用reduceByKey做分组聚合，把kvRDD中的Value从1转换为单词计数。</p><p>这4步转换的过程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/af/6d/af93e6f10b85df80a7d56a6c1965a36d.jpg?wh=1920x512\" alt=\"图片\" title=\"Word Count中的RDD转换\"></p><p>我们刚刚说过，RDD代表的是分布式数据形态，因此，<strong>RDD到RDD之间的转换，本质上是数据形态上的转换（Transformations）</strong>。</p><p>在RDD的编程模型中，一共有两种算子，Transformations类算子和Actions类算子。开发者需要使用Transformations类算子，定义并描述数据形态的转换过程，然后调用Actions类算子，将计算结果收集起来、或是物化到磁盘。</p><p>在这样的编程模型下，Spark在运行时的计算被划分为两个环节。</p><ol>\n<li>基于不同数据形态之间的转换，构建<strong>计算流图</strong>（DAG，Directed Acyclic Graph）；</li>\n<li>通过Actions类算子，以<strong>回溯的方式去触发执行</strong>这个计算流图。</li>\n</ol><p>换句话说，开发者调用的各类Transformations算子，并不立即执行计算，当且仅当开发者调用Actions算子时，之前调用的转换算子才会付诸执行。在业内，这样的计算模式有个专门的术语，叫作“<strong>延迟计算</strong>”（Lazy Evaluation）。</p><p>延迟计算很好地解释了本讲开头的问题：为什么Word Count在执行的过程中，只有最后一行代码会花费很长时间，而前面的代码都是瞬间执行完毕的呢？</p><p>这里的答案正是Spark的延迟计算。flatMap、filter、map这些算子，仅用于构建计算流图，因此，当你在spark-shell中敲入这些代码时，spark-shell会立即返回。只有在你敲入最后那行包含take的代码时，Spark才会触发执行从头到尾的计算流程，所以直观地看上去，最后一行代码是最耗时的。</p><p>Spark程序的整个运行流程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/7b/6f82b4a35cdfb526d837d23675yy477b.jpg?wh=1920x472\" alt=\"图片\" title=\"延迟计算\"></p><p>你可能会问：“在RDD的开发框架下，哪些算子属于Transformations算子，哪些算子是Actions算子呢？”</p><p>我们都知道，Spark有很多算子，<a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">Spark官网</a>提供了完整的RDD算子集合，不过对于这些算子，官网更多地是采用一种罗列的方式去呈现的，没有进行分类，看得人眼花缭乱、昏昏欲睡。因此，我把常用的RDD算子进行了归类，并整理到了下面的表格中，供你随时查阅。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/fa/4f277fdda5a4b34b3e2yyb6f570a08fa.jpg?wh=1773x1364\" alt=\"图片\" title=\"图片整理自：https://spark.apache.org/docs/latest/rdd-programming-guide.html\"></p><p>结合每个算子的分类、用途和适用场景，这张表格可以帮你更快、更高效地选择合适的算子来实现业务逻辑。对于表格中不熟悉的算子，比如aggregateByKey，你可以结合官网的介绍与解释，或是进一步查阅网上的相关资料，有的放矢地去深入理解。重要的算子，我们会在之后的课里详细解释。</p><h2>重点回顾</h2><p>今天这一讲，我们重点讲解了RDD的编程模型与延迟计算，并通过土豆工坊的类比介绍了什么是RDD。<strong>RDD是Spark对于分布式数据集的抽象，它用于囊括所有内存中和磁盘中的分布式数据实体</strong>。对于RDD，你要重点掌握它的4大属性，这是我们后续学习的重要基础：</p><ul>\n<li>partitions：数据分片</li>\n<li>partitioner：分片切割规则</li>\n<li>dependencies：RDD依赖</li>\n<li>compute：转换函数</li>\n</ul><p>深入理解RDD之后，你需要熟悉RDD的编程模型。在RDD的编程模型中，开发者需要使用Transformations类算子，定义并描述数据形态的转换过程，然后调用Actions类算子，将计算结果收集起来、或是物化到磁盘。</p><p>而延迟计算指的是，开发者调用的各类Transformations算子，并不会立即执行计算，当且仅当开发者调用Actions算子时，之前调用的转换算子才会付诸执行。</p><h2>每课一练</h2><p>对于Word Count的计算流图与土豆工坊的流水线工艺，尽管看上去毫不相关，风马牛不相及，不过，你不妨花点时间想一想，它们之间有哪些区别和联系？</p><p>欢迎你把答案分享到评论区，我在评论区等你，也欢迎你把这一讲分享给更多的朋友和同事，我们下一讲见！</p>","neighbors":{"left":{"article_title":"01｜Spark：从“大数据的Hello World”开始","id":415209},"right":{"article_title":"03 | RDD常用算子（一）：RDD内部的数据转换","id":418079}},"comments":[{"had_liked":false,"id":318572,"user_name":"cfwseven","can_delete":false,"product_type":"c1","uid":2705659,"ip_address":"","ucode":"D7A23FE58153D0","user_header":"https://static001.geekbang.org/account/avatar/00/29/48/fb/7b27bccb.jpg","comment_is_top":false,"comment_ctime":1635337252,"is_pvip":true,"replies":[{"id":"115569","content":"好问题~<br><br>严谨的说，不是要把action算子设置成延迟计算，而是Spark在实现上，选择了Lazy evaluation这种计算模式。<br><br>TensorFlow同样也采用了类似的计算模式。这种模式有什么好处、或者说收益呢？<br><br>我的理解是：优化空间。<br><br>和Eager evaluation不一样，lazy evaluation先构建计算图，等都构建完了，在付诸执行。这样一来，中间就可以打个时间差，引擎有足够的时间和空间，对用户代码做优化，从而让应用的执行性能在用户无感知的情况下，做到最好。<br><br>换个角度说，引擎选择lazy evaluation，其实是注重“用户体验”（开发者）的一种态度~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635424669,"ip_address":"","comment_id":318572,"utype":1}],"discussion_count":2,"race_medal":0,"score":"100419585060","product_id":100090001,"comment_content":"作者大大能说一下，为什么action算子要设置成延迟计算吗","like_count":24,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529303,"discussion_content":"好问题~\n\n严谨的说，不是要把action算子设置成延迟计算，而是Spark在实现上，选择了Lazy evaluation这种计算模式。\n\nTensorFlow同样也采用了类似的计算模式。这种模式有什么好处、或者说收益呢？\n\n我的理解是：优化空间。\n\n和Eager evaluation不一样，lazy evaluation先构建计算图，等都构建完了，在付诸执行。这样一来，中间就可以打个时间差，引擎有足够的时间和空间，对用户代码做优化，从而让应用的执行性能在用户无感知的情况下，做到最好。\n\n换个角度说，引擎选择lazy evaluation，其实是注重“用户体验”（开发者）的一种态度~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1635424669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2705659,"avatar":"https://static001.geekbang.org/account/avatar/00/29/48/fb/7b27bccb.jpg","nickname":"cfwseven","note":"","ucode":"D7A23FE58153D0","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":410602,"discussion_content":"谢谢老师解答。\n——lazy evaluation先构建计算图，等都构建完了，在付诸执行\n不知道我这样的理解对不对：通过Lazy evaluation，就可以不保存RDD计算链的中间结果，中间结果被下一个RDD算子计算后即抛，可以节约大量内存","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1635737735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311847,"user_name":"GAC·DU","can_delete":false,"product_type":"c1","uid":1385403,"ip_address":"","ucode":"7847FBE1C13740","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","comment_is_top":false,"comment_ctime":1631503112,"is_pvip":true,"replies":[{"id":"113020","content":"Perfect~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631544146,"ip_address":"","comment_id":311847,"utype":1}],"discussion_count":1,"race_medal":0,"score":"61761045256","product_id":100090001,"comment_content":"从两者的整体流程来看，结果都是不可逆的，但是WordCount可以设置Cache和Checkpoint，方便加速访问和故障修复，而土豆加工流程却不可以。土豆加工流程是DAG的概图。","like_count":15,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526771,"discussion_content":"Perfect~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631544146,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":330512,"user_name":"宇","can_delete":false,"product_type":"c1","uid":2890898,"ip_address":"","ucode":"5E507B438D6B0A","user_header":"https://static001.geekbang.org/account/avatar/00/2c/1c/92/dcbad687.jpg","comment_is_top":false,"comment_ctime":1642003512,"is_pvip":true,"replies":[{"id":"120665","content":"好问题，不过老弟这是典型的单机思维，为什么这么说呢？<br>在过程编程里面，清洗、切割、烘烤，是独立的操作，各自耗时不同，确实可以计算出来，不同环节的耗时。<br>但是在Spark里面，我们基本上没有办法区分这些不同操作的独立耗时。<br>1）一方面是，在分布式环境中，某一个操作的耗时，需要看全局节点的统计值，而不是某个机器上的执行时间<br>2）再者，在Spark里面，同一个Stage里面的操作，比如这里的清洗、切割还有烘烤，他们其实会被Whole Stage Code Generation优化为一份代码，在这一份代码里面，不会在区分这些不同的操作了，所以对于他们各自的执行时间，更是无从计算了。老弟可以重点关注后面课程部分的调度系统和Tungsten优化，来熟悉这些细节。<br><br>实际上，在Spark里面，我们往往不会特别关注某种操作的耗时，而主要是关注哪里是Shuffle，哪里有数据集频繁访问、扫描的现象，这些才是Spark主要的性能瓶颈","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1642243110,"ip_address":"","comment_id":330512,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23116839992","product_id":100090001,"comment_content":"怎样知道程序里哪个算子最耗时，现在薯片生产速度很慢，我想知道最耗时生产环节是哪个（清洗，切割，还是烘烤）？","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546229,"discussion_content":"好问题，不过老弟这是典型的单机思维，为什么这么说呢？\n在过程编程里面，清洗、切割、烘烤，是独立的操作，各自耗时不同，确实可以计算出来，不同环节的耗时。\n但是在Spark里面，我们基本上没有办法区分这些不同操作的独立耗时。\n1）一方面是，在分布式环境中，某一个操作的耗时，需要看全局节点的统计值，而不是某个机器上的执行时间\n2）再者，在Spark里面，同一个Stage里面的操作，比如这里的清洗、切割还有烘烤，他们其实会被Whole Stage Code Generation优化为一份代码，在这一份代码里面，不会在区分这些不同的操作了，所以对于他们各自的执行时间，更是无从计算了。老弟可以重点关注后面课程部分的调度系统和Tungsten优化，来熟悉这些细节。\n\n实际上，在Spark里面，我们往往不会特别关注某种操作的耗时，而主要是关注哪里是Shuffle，哪里有数据集频繁访问、扫描的现象，这些才是Spark主要的性能瓶颈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642243111,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312403,"user_name":"燃料喷射器","can_delete":false,"product_type":"c1","uid":1238914,"ip_address":"","ucode":"E6D3C5CA41481A","user_header":"https://static001.geekbang.org/account/avatar/00/12/e7/82/b6e56904.jpg","comment_is_top":false,"comment_ctime":1631787048,"is_pvip":false,"replies":[{"id":"113199","content":"很棒的视角~ 一个是延迟计算（Lazy evaluation），一个是“及早求值”（Early evaluation）","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631804265,"ip_address":"","comment_id":312403,"utype":1}],"discussion_count":2,"race_medal":0,"score":"18811656232","product_id":100090001,"comment_content":"从两者不同点看，workdcount切切实实为延迟计算，而土豆工坊的流程为切实发生的。","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526963,"discussion_content":"很棒的视角~ 一个是延迟计算（Lazy evaluation），一个是“及早求值”（Early evaluation）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631804265,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1056982,"avatar":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","nickname":"kingcall","note":"","ucode":"508884DC684B5B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":408696,"discussion_content":"土豆工坊没有延迟计算","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635304588,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319302,"user_name":"zx","can_delete":false,"product_type":"c1","uid":2831221,"ip_address":"","ucode":"D9661199636B50","user_header":"","comment_is_top":false,"comment_ctime":1635736777,"is_pvip":false,"replies":[{"id":"116037","content":"和partitions属性无关哈~ 不过，在reduceByKey抛异常，倒是有点意思，这说明Spark在这个环节，检查了文件路径并报错。不过，检查路径不代表trigger执行哈，真正的触发执行，还是要等到Action那一步~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1636093865,"ip_address":"","comment_id":319302,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14520638665","product_id":100090001,"comment_content":"计算wordcount的时候文件路径写错了，但是却是在reduceByKey这一步报错，这步并不是action算子，这是和partitions属性有关吗","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529583,"discussion_content":"和partitions属性无关哈~ 不过，在reduceByKey抛异常，倒是有点意思，这说明Spark在这个环节，检查了文件路径并报错。不过，检查路径不代表trigger执行哈，真正的触发执行，还是要等到Action那一步~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636093865,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316199,"user_name":"welldo","can_delete":false,"product_type":"c1","uid":1598796,"ip_address":"","ucode":"D38E75364CD2E3","user_header":"https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg","comment_is_top":false,"comment_ctime":1634196885,"is_pvip":false,"replies":[{"id":"114584","content":"任何一个分布式数据集，RDD也好、DataFrame、Dataset也好，都是有分区、并行度、分区器这些个概念的哈，跟是否Shuffle没有关系<br><br>Shuffle只是改变了原始数据集的分区方式，而已。<br><br>对于HDFS、S3上的分布式数据集，原始的分区数，或者说并行度，就是HDFS、S3中文件原始的分区数，二者一致。<br><br>对于那些在Driver端读的单机文件、或是创建的数据集，可以人为指定分区数（并行度），比如parallelize。<br><br>总之，不管是哪类数据，数据集一旦进入Spark，是一定有并行度、分区数这个概念的，哪怕并行度是1。<br><br>老弟还要好好消化一下这些“土豆”啊，哈哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634292516,"ip_address":"","comment_id":316199,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10224131477","product_id":100090001,"comment_content":"&quot;我们再来看 RDD 的 partitioner 属性，这个属性定义了把原始数据集切割成数据分片的切割规则。在土豆工坊的例子中，“带泥土豆”RDD 的切割规则是随机拿取，也就是从麻袋中随机拿取一颗脏兮兮的土豆放到流水线上。后面的食材形态，如“干净土豆”、“土豆片”和“即食薯片”，则沿用了“带泥土豆”RDD 的切割规则。换句话说，后续的这些 RDD，分别继承了前一个 RDD 的 partitioner 属性。&quot;<br>-----------------------<br>老师, 看完这段话和土豆流水线的图片, 我有几个疑问.<br><br>1. rdd分为&lt;value&gt;类型和&lt;key,value&gt;类型，数据分区方式只作用于&lt;Key，Value&gt;形式的数据。<br>并且刚开始没有任何分区方式，直到遇到包含shuffle的算子时，才会使用“分区方式”，比如hash分区。<br><br>问题1：那么，一坨数据，在成为&lt;key,value&gt;类型的rdd的时候（假如4片），分片方式，是不是平均分成4份呢？<br><br>2. &lt;value&gt;类型的rdd，没有分区器，那么它刚刚生成的时候，也是平均分吗？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528309,"discussion_content":"任何一个分布式数据集，RDD也好、DataFrame、Dataset也好，都是有分区、并行度、分区器这些个概念的哈，跟是否Shuffle没有关系\n\nShuffle只是改变了原始数据集的分区方式，而已。\n\n对于HDFS、S3上的分布式数据集，原始的分区数，或者说并行度，就是HDFS、S3中文件原始的分区数，二者一致。\n\n对于那些在Driver端读的单机文件、或是创建的数据集，可以人为指定分区数（并行度），比如parallelize。\n\n总之，不管是哪类数据，数据集一旦进入Spark，是一定有并行度、分区数这个概念的，哪怕并行度是1。\n\n老弟还要好好消化一下这些“土豆”啊，哈哈~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1634292516,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1056982,"avatar":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","nickname":"kingcall","note":"","ucode":"508884DC684B5B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":408700,"discussion_content":"hive 的分区就是hdfs 的分区，读取kafka 的时候就是kafka 的分区，读取socket 的时候是按照时间分割的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635304745,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1598796,"avatar":"https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg","nickname":"welldo","note":"","ucode":"D38E75364CD2E3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404871,"discussion_content":"老师，我没有表达清楚，我重新提问一下，老师勿见怪。\n\n1. 在内部数据上创建rdd<key,value>，比如parallelize 算子，并且用numSlices手动指定分区数；这个时候我查看分区器rdd.partitioner，结果是NONE\n问题1： 此时数据是如何分区的呢？是平均分吗？\n问题2：重新分区（手动partitionBy 或者遇到shuffle）改变数据的分布时，才会真正使用hash分区器、自定义分区器（按照薯片大小分），对吗？\n\n2. 在内部数据上创建rdd<value>，并且用numSlices手动指定分区数；\n问题3：这类rdd没有分区器，那它又是咋分区的呢？\n\n3. 总结一下老师回答的第三段，\n通过api从外部数据（比如hive）创建rdd时，hive咋分区的，到了spark中就是咋分区的，对吗？\n\n\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634442341,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312635,"user_name":"LJK","can_delete":false,"product_type":"c1","uid":1199213,"ip_address":"","ucode":"12B2441099FF1D","user_header":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","comment_is_top":false,"comment_ctime":1631911988,"is_pvip":false,"replies":[{"id":"113277","content":"是的，感谢提醒~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631932757,"ip_address":"","comment_id":312635,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10221846580","product_id":100090001,"comment_content":"Action中漏了一个reduce，今天才注意到reduceByKey是transformation而reduce是action","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527057,"discussion_content":"是的，感谢提醒~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631932757,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":354730,"user_name":"Geek_a25305","can_delete":false,"product_type":"c1","uid":3144933,"ip_address":"中国台湾","ucode":"3A7A91A5D53F0F","user_header":"","comment_is_top":false,"comment_ctime":1660723065,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1660723065","product_id":100090001,"comment_content":"大神你好 我有个问题:<br><br><br>如果把“带泥土豆”看成是 RDD 的话，那么 RDD 的 partitions 属性，囊括的正是麻袋里那一颗颗脏兮兮的土豆。同理，流水线上所有洗净的土豆，一同构成了“干净土豆”RDD 的 partitions 属性。<br><br>这句话, 是不是要改成:<br><br>囊括的正是麻袋里每一颗脏兮兮的土豆。","like_count":0},{"had_liked":false,"id":348817,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1655433536,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1655433536","product_id":100090001,"comment_content":"RDD这种抽象，平时在IDEA这种开发环境中，小批量数据应该可以debug 吧","like_count":0},{"had_liked":false,"id":346579,"user_name":"Geek_757cbc","can_delete":false,"product_type":"c1","uid":2943897,"ip_address":"","ucode":"ECE10235F88FEA","user_header":"","comment_is_top":false,"comment_ctime":1653250731,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1653250731","product_id":100090001,"comment_content":"既然spark的transformation不会立即计算为什么会立即返回，还是不太理解，难道返回的结果不是计算之后的结果吗","like_count":0,"discussions":[{"author":{"id":2758010,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/w74m73icotZZEiasC6VzRUytfkFkgyYCGAcz16oBWuMXueWOxxVuAnH6IHaZFXkj5OqwlVO1fnocvn9gGYh8gGcw/132","nickname":"Geek_995b78","note":"","ucode":"F9BD1C78366081","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585364,"discussion_content":"我的理解是，transform返回的是计算逻辑，而action返回的是计算结果，是真正经过计算的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661500441,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"河南"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335033,"user_name":"田大侠","can_delete":false,"product_type":"c1","uid":1517333,"ip_address":"","ucode":"9D6026D5B6167C","user_header":"https://static001.geekbang.org/account/avatar/00/17/27/15/abf4f787.jpg","comment_is_top":false,"comment_ctime":1645261826,"is_pvip":true,"replies":[{"id":"122824","content":"理解的非常到位，没有任何问题~ 满分💯","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1645889606,"ip_address":"","comment_id":335033,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1645261826","product_id":100090001,"comment_content":"作者大大有个问题问一下<br>这里的依赖关系有个比较明显的差别<br>1.map计算是一个数据分片依赖于前一个RDD“数据分片”，就是说在同一个分片上可以连续计算直到reduce之前<br>2.reduce计算是依赖关系前面一个RDD的所有的数据集 spark实际计算的时候要等前面所有的map计算完成才能进行reduce操作<br><br>上面的我的两个理解是否有偏差？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553464,"discussion_content":"理解的非常到位，没有任何问题~ 满分💯","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645889606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":323155,"user_name":"Eazow","can_delete":false,"product_type":"c1","uid":1346402,"ip_address":"","ucode":"D81D8FF2B2FF0D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/bmgpp5wc8GLmOdHNQccSgrunK0VdIicB6rpTHXCTF5xEkm2YvPHOX2DwNt2EqTzJ70JD41h0u5qW4R0yXRY1ZCg/132","comment_is_top":false,"comment_ctime":1637745431,"is_pvip":true,"replies":[{"id":"117284","content":"draw.io，有在线版，也有APP~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637806505,"ip_address":"","comment_id":323155,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1637745431","product_id":100090001,"comment_content":"请问土豆工坊图是用什么画滴那？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533163,"discussion_content":"draw.io，有在线版，也有APP~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637806505,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1346402,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/bmgpp5wc8GLmOdHNQccSgrunK0VdIicB6rpTHXCTF5xEkm2YvPHOX2DwNt2EqTzJ70JD41h0u5qW4R0yXRY1ZCg/132","nickname":"Eazow","note":"","ucode":"D81D8FF2B2FF0D","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":533449,"discussion_content":"谢谢磊哥","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637857358,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":533163,"ip_address":""},"score":533449,"extra":"{\"user_type\":1}"}]}]},{"had_liked":false,"id":316614,"user_name":"xuchuan","can_delete":false,"product_type":"c1","uid":1954690,"ip_address":"","ucode":"3967199AA078C7","user_header":"https://static001.geekbang.org/account/avatar/00/1d/d3/82/c3e57eb3.jpg","comment_is_top":false,"comment_ctime":1634453519,"is_pvip":false,"replies":[{"id":"114643","content":"赞👍，正解~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634479617,"ip_address":"","comment_id":316614,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634453519","product_id":100090001,"comment_content":"都是来料加工，目标都是高效率生产。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528434,"discussion_content":"赞👍，正解~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634479617,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}