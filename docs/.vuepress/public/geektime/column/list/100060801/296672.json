{"id":296672,"title":"07 | Embedding进阶：如何利用图结构数据生成Graph Embedding？","content":"<p>你好，我是王喆。</p><p>上一节课，我们一起学习了Embedding技术。我们知道，只要是能够被序列数据表示的物品，都可以通过Item2vec方法训练出Embedding。但是，互联网的数据可不仅仅是序列数据那么简单，越来越多的数据被我们以图的形式展现出来。这个时候，基于序列数据的Embedding方法就显得“不够用”了。但在推荐系统中放弃图结构数据是非常可惜的，因为图数据中包含了大量非常有价值的结构信息。</p><p>那我们怎么样才能够基于图结构数据生成Embedding呢？这节课，我们就重点来讲讲基于图结构的Embedding方法，它也被称为Graph Embedding。</p><h2>互联网中有哪些图结构数据？</h2><p>可能有的同学还不太清楚图结构中到底包含了哪些重要信息，为什么我们希望好好利用它们，并以它们为基础生成Embedding？下面，我就先带你认识一下互联网中那些非常典型的图结构数据（如图1）。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/91/5423f8d0f5c1b2ba583f5a2b2d0aed91.jpeg?wh=1920*654\" alt=\"\" title=\"图1 互联网图结构数据\"></p><p>事实上，图结构数据在互联网中几乎无处不在，最典型的就是我们每天都在使用的<strong>社交网络</strong>（如图1-a）。从社交网络中，我们可以发现意见领袖，可以发现社区，再根据这些“社交”特性进行社交化的推荐，如果我们可以对社交网络中的节点进行Embedding编码，社交化推荐的过程将会非常方便。</p><!-- [[[read_end]]] --><p><strong>知识图谱</strong>也是近来非常火热的研究和应用方向。像图1b中描述的那样，知识图谱中包含了不同类型的知识主体（如人物、地点等），附着在知识主体上的属性（如人物描述，物品特点），以及主体和主体之间、主体和属性之间的关系。如果我们能够对知识图谱中的主体进行Embedding化，就可以发现主体之间的潜在关系，这对于基于内容和知识的推荐系统是非常有帮助的。</p><p>还有一类非常重要的图数据就是<strong>行为关系类图数据</strong>。这类数据几乎存在于所有互联网应用中，它事实上是由用户和物品组成的“二部图”（也称二分图，如图1c）。用户和物品之间的相互行为生成了行为关系图。借助这样的关系图，我们自然能够利用Embedding技术发掘出物品和物品之间、用户和用户之间，以及用户和物品之间的关系，从而应用于推荐系统的进一步推荐。</p><p>毫无疑问，图数据是具备巨大价值的，如果能将图中的节点Embedding化，对于推荐系统来说将是非常有价值的特征。那下面，我们就进入正题，一起来学习基于图数据的Graph Embedding方法。</p><h2>基于随机游走的Graph Embedding方法：Deep Walk</h2><p>我们先来学习一种在业界影响力比较大，应用也很广泛的Graph Embedding方法，Deep Walk，它是2014年由美国石溪大学的研究者提出的。它的主要思想是在由物品组成的图结构上进行随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入Word2vec进行训练，最终得到物品的Embedding。因此，DeepWalk可以被看作连接序列Embedding和Graph Embedding的一种过渡方法。下图2展示了DeepWalk方法的执行过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/1f/ed/1f28172c62e1b5991644cf62453fd0ed.jpeg?wh=1920*691\" alt=\"\" title=\"图2 DeepWalk方法的过程\"></p><p>接下来，我就参照图2中4个示意图，来为你详细讲解一下DeepWalk的算法流程。</p><p>首先，我们基于原始的用户行为序列（图2a），比如用户的购买物品序列、观看视频序列等等，来构建物品关系图（图2b）。从中，我们可以看出，因为用户U<sub>i</sub>先后购买了物品A和物品B，所以产生了一条由A到B的有向边。如果后续产生了多条相同的有向边，则有向边的权重被加强。在将所有用户行为序列都转换成物品相关图中的边之后，全局的物品相关图就建立起来了。</p><p>然后，我们采用随机游走的方式随机选择起始点，重新产生物品序列（图2c）。其中，随机游走采样的次数、长度等都属于超参数，需要我们根据具体应用进行调整。</p><p>最后，我们将这些随机游走生成的物品序列输入图2d的Word2vec模型，生成最终的物品Embedding向量。</p><p>在上述DeepWalk的算法流程中，唯一需要形式化定义的就是随机游走的跳转概率，也就是到达节点v<sub>i</sub>后，下一步遍历v<sub>i</sub> 的邻接点v<sub>j</sub> 的概率。如果物品关系图是有向有权图，那么从节点v<sub>i</sub> 跳转到节点v<sub>j</sub> 的概率定义如下：</p><p>$$P\\left(v_{j} \\mid v_{i}\\right)=\\left\\{\\begin{array}{ll}\\frac{M_{i j}}{\\sum_{j \\in N_{+}\\left(V_{i}\\right)}}, m_{i j} &amp; v_{j} \\in N_{+}\\left(v_{i}\\right) \\\\\\ 0, &amp; \\mathrm{e}_{i j} \\notin \\varepsilon\\end{array}\\right.$$</p><p>其中，N+(v<sub>i</sub>)是节点v<sub>i</sub>所有的出边集合，M<sub>ij</sub>是节点v<sub>i</sub>到节点v<sub>j</sub>边的权重，即DeepWalk的跳转概率就是跳转边的权重占所有相关出边权重之和的比例。如果物品相关图是无向无权重图，那么跳转概率将是上面这个公式的一个特例，即权重M<sub>ij</sub>将为常数1，且N+(v<sub>i</sub>)应是节点v<sub>i</sub>所有“边”的集合，而不是所有“出边”的集合。</p><p>再通过随机游走得到新的物品序列，我们就可以通过经典的Word2vec的方式生成物品Embedding了。当然，关于Word2vec的细节你可以回顾上一节课的内容，这里就不再赘述了。</p><h2>在同质性和结构性间权衡的方法，Node2vec</h2><p>2016年，斯坦福大学的研究人员在DeepWalk的基础上更进一步，他们提出了Node2vec模型。Node2vec通过调整随机游走跳转概率的方法，让Graph Embedding的结果在网络的<strong>同质性</strong>（Homophily）和<strong>结构性</strong>（Structural Equivalence）中进行权衡，可以进一步把不同的Embedding输入推荐模型，让推荐系统学习到不同的网络结构特点。</p><p>我这里所说的网络的<strong>“同质性”指的是距离相近节点的Embedding应该尽量近似</strong>，如图3所示，节点u与其相连的节点s<sub>1</sub>、s<sub>2</sub>、s<sub>3</sub>、s<sub>4</sub>的Embedding表达应该是接近的，这就是网络“同质性”的体现。在电商网站中，同质性的物品很可能是同品类、同属性，或者经常被一同购买的物品。</p><p>而<strong>“结构性”指的是结构上相似的节点的Embedding应该尽量接近</strong>，比如图3中节点u和节点s<sub>6</sub>都是各自局域网络的中心节点，它们在结构上相似，所以它们的Embedding表达也应该近似，这就是“结构性”的体现。在电商网站中，结构性相似的物品一般是各品类的爆款、最佳凑单商品等拥有类似趋势或者结构性属性的物品。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/82/e28b322617c318e1371dca4088ce5a82.jpeg?wh=1920*693\" alt=\"\" title=\"图3 网络的BFS和 DFS示意图\"></p><p>理解了这些基本概念之后，那么问题来了，Graph Embedding的结果究竟是怎么表达结构性和同质性的呢？</p><p>首先，为了使Graph Embedding的结果能够表达网络的“<strong>结构性</strong>”，在随机游走的过程中，我们需要让游走的过程更倾向于<strong>BFS（Breadth First Search，宽度优先搜索）</strong>，因为BFS会更多地在当前节点的邻域中进行游走遍历，相当于对当前节点周边的网络结构进行一次“微观扫描”。当前节点是“局部中心节点”，还是“边缘节点”，亦或是“连接性节点”，其生成的序列包含的节点数量和顺序必然是不同的，从而让最终的Embedding抓取到更多结构性信息。</p><p>而为了表达“<strong>同质性</strong>”，随机游走要更倾向于<strong>DFS（Depth First Search，深度优先搜索）</strong>才行，因为DFS更有可能通过多次跳转，游走到远方的节点上。但无论怎样，DFS的游走更大概率会在一个大的集团内部进行，这就使得一个集团或者社区内部节点的Embedding更为相似，从而更多地表达网络的“同质性”。</p><p>那在Node2vec算法中，究竟是怎样控制BFS和DFS的倾向性的呢？</p><p>其实，它主要是通过节点间的跳转概率来控制跳转的倾向性。图4所示为Node2vec算法从节点t跳转到节点v后，再从节点v跳转到周围各点的跳转概率。这里，你要注意这几个节点的特点。比如，节点t是随机游走上一步访问的节点，节点v是当前访问的节点，节点x<sub>1</sub>、x<sub>2</sub>、x<sub>3</sub>是与v相连的非t节点，但节点x<sub>1</sub>还与节点t相连，这些不同的特点决定了随机游走时下一次跳转的概率。</p><p><img src=\"https://static001.geekbang.org/resource/image/6y/59/6yyec0329b62cde0a645eea8dc3a8059.jpeg?wh=1920*871\" alt=\"\" title=\"图4 Node2vec的跳转概率\"></p><p>这些概率我们还可以用具体的公式来表示，从当前节点v跳转到下一个节点x的概率$\\pi_{v x}=\\alpha_{p q}(t, x) \\cdot \\omega_{v x}$ ，其中wvx是边vx的原始权重，$\\alpha_{p q}(t, x)$是Node2vec定义的一个跳转权重。到底是倾向于DFS还是BFS，主要就与这个跳转权重的定义有关了。这里我们先了解一下它的精确定义，我再作进一步的解释：</p><p>$$\\alpha_{p q(t, x)=}\\left\\{\\begin{array}{ll}\\frac{1}{p} &amp; \\text { 如果 } d_{t x}=0\\\\\\ 1 &amp; \\text { 如果 } d_{t x}=1\\\\\\frac{1}{q} &amp; \\text { 如果 } d_{t x}=2\\end{array}\\right.$$</p><p>$\\alpha_{p q}(t, x)$里的d<sub>tx</sub>是指节点t到节点x的距离，比如节点x<sub>1</sub>其实是与节点t直接相连的，所以这个距离d<sub>tx</sub>就是1，节点t到节点t自己的距离d<sub>tt</sub>就是0，而x<sub>2</sub>、x<sub>3</sub>这些不与t相连的节点，d<sub>tx</sub>就是2。</p><p>此外，$\\alpha_{p q}(t, x)$中的参数p和q共同控制着随机游走的倾向性。参数p被称为返回参数（Return Parameter），p越小，随机游走回节点t的可能性越大，Node2vec就更注重表达网络的结构性。参数q被称为进出参数（In-out Parameter），q越小，随机游走到远方节点的可能性越大，Node2vec更注重表达网络的同质性。反之，当前节点更可能在附近节点游走。你可以自己尝试给p和q设置不同大小的值，算一算从v跳转到t、x<sub>1</sub>、x<sub>2</sub>和x<sub>3</sub>的跳转概率。这样一来，应该就不难理解我刚才所说的随机游走倾向性的问题啦。</p><p>Node2vec这种灵活表达同质性和结构性的特点也得到了实验的证实，我们可以通过调整p和q参数让它产生不同的Embedding结果。图5上就是Node2vec更注重同质性的体现，从中我们可以看到，距离相近的节点颜色更为接近，图5下则是更注重结构性的体现，其中结构特点相近的节点的颜色更为接近。</p><p><img src=\"https://static001.geekbang.org/resource/image/d2/3a/d2d5a6b6f31aeee3219b5f509a88903a.jpeg?wh=1424*1080\" alt=\"\" title=\"图5 Node2vec实验结果\"></p><p>毫无疑问，Node2vec所体现的网络的同质性和结构性，在推荐系统中都是非常重要的特征表达。由于Node2vec的这种灵活性，以及发掘不同图特征的能力，我们甚至可以把不同Node2vec生成的偏向“结构性”的Embedding结果，以及偏向“同质性”的Embedding结果共同输入后续深度学习网络，以保留物品的不同图特征信息。</p><h2>Embedding是如何应用在推荐系统的特征工程中的？</h2><p>到这里，我们已经学习了好几种主流的Embedding方法，包括序列数据的Embedding方法，Word2vec和Item2vec，以及图数据的Embedding方法，Deep Walk和Node2vec。那你有没有想过，我为什么要在特征工程这一模块里介绍Embedding呢？Embedding又是怎么应用到推荐系统中的呢？这里，我就来做一个统一的解答。</p><p>第一个问题不难回答，由于Embedding的产出就是一个数值型特征向量，所以Embedding技术本身就可以视作特征处理方式的一种。只不过与简单的One-hot编码等方式不同，Embedding是一种更高阶的特征处理方法，它具备了把序列结构、网络结构、甚至其他特征融合到一个特征向量中的能力。</p><p>而第二个问题的答案有三个，因为Embedding在推荐系统中的应用方式大致有三种，分别是“直接应用”“预训练应用”和“End2End应用”。</p><p>其中，“<strong>直接应用</strong>”最简单，就是在我们得到Embedding向量之后，直接利用Embedding向量的相似性实现某些推荐系统的功能。典型的功能有，利用物品Embedding间的相似性实现相似物品推荐，利用物品Embedding和用户Embedding的相似性实现“猜你喜欢”等经典推荐功能，还可以利用物品Embedding实现推荐系统中的召回层等。当然，如果你还不熟悉这些应用细节，也完全不用担心，我们在之后的课程中都会讲到。</p><p>“<strong>预训练应用</strong>”指的是在我们预先训练好物品和用户的Embedding之后，不直接应用，而是把这些Embedding向量作为特征向量的一部分，跟其余的特征向量拼接起来，作为推荐模型的输入参与训练。这样做能够更好地把其他特征引入进来，让推荐模型作出更为全面且准确的预测。</p><p>第三种应用叫做“<strong>End2End应用</strong>”。看上去这是个新的名词，它的全称叫做“End to End Training”，也就是端到端训练。不过，它其实并不神秘，就是指我们不预先训练Embedding，而是把Embedding的训练与深度学习推荐模型结合起来，采用统一的、端到端的方式一起训练，直接得到包含Embedding层的推荐模型。这种方式非常流行，比如图6就展示了三个包含Embedding层的经典模型，分别是微软的Deep Crossing，UCL提出的FNN和Google的Wide&amp;Deep。它们的实现细节我们也会在后续课程里面介绍，你这里只需要了解这个概念就可以了。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/78/e9538b0b5fcea14a0f4bbe2001919978.jpg?wh=1920*599\" alt=\"\" title=\"图6 带有Embedding层的深度学习模型\"></p><h2>小结</h2><p>这节课我们一起学习了Graph Embedding的两种主要方法，分别是Deep Walk和Node2vec，并且我们还总结了Embedding技术在深度学习推荐系统中的应用方法。</p><p>学习Deep Walk方法关键在于理解它的算法流程，首先，我们基于原始的用户行为序列来构建物品关系图，然后采用随机游走的方式随机选择起始点，重新产生物品序列，最后将这些随机游走生成的物品序列输入Word2vec模型，生成最终的物品Embedding向量。</p><p>而Node2vec相比于Deep Walk，增加了随机游走过程中跳转概率的倾向性。如果倾向于宽度优先搜索，则Embedding结果更加体现“结构性”。如果倾向于深度优先搜索，则更加体现“同质性”。</p><p>最后，我们介绍了Embedding技术在深度学习推荐系统中的三种应用方法，“直接应用”“预训练”和“End2End训练”。这些方法各有特点，它们都是业界主流的应用方法，随着课程的不断深入，我会带你一步一步揭开它们的面纱。</p><p>老规矩，在课程的最后，我还是用表格的方式总结了这次课的关键知识点，你可以利用它来复习巩固。</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/e6/d03ce492866f9fb85b4fbf5fa39346e6.jpeg?wh=1920*749\" alt=\"\"></p><p>至此，我们就完成了所有Embedding理论部分的学习。下节课，我们再一起进入Embedding和Graph Embedding的实践部分，利用Sparrow Recsys的数据，使用Spark实现Embedding的训练，希望你到时能跟我一起动起手来！</p><h2>课后思考</h2><p>你能尝试对比一下Embedding预训练和Embedding End2End训练这两种应用方法，说出它们之间的优缺点吗？</p><p>欢迎在留言区分享你的思考和答案，如果这节Graph Embedding的课程让你有所收获，那不妨也把这节课分享给你的朋友们，我们下节课见！</p>","neighbors":{"left":{"article_title":"06 | Embedding基础：所有人都在谈的Embedding技术到底是什么？","id":295939},"right":{"article_title":"08 | Embedding实战：如何使用Spark生成Item2vec和Graph Embedding？","id":296932}},"comments":[{"had_liked":false,"id":267174,"user_name":"浣熊当家","can_delete":false,"product_type":"c1","uid":1952248,"ip_address":"","ucode":"939F06050423E4","user_header":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","comment_is_top":false,"comment_ctime":1607619132,"is_pvip":false,"replies":[{"id":"96983","content":"非常棒的实践。<br><br>所以我一直说实践出真知，从来都不是哪个算法比另外一个更好。我觉得你说的没错，deepwark的抽样过程保留了转移矩阵的“主要框架”，但同时当抽样次数不太高的时候，item embedding的覆盖率反而没有item2vec好。<br><br>这次的实践经验推荐其他的同学参考，赞！","user_name":"作者回复","comment_id":267174,"uid":"1662192","ip_address":"","utype":1,"ctime":1607650023,"user_name_real":"王喆"}],"discussion_count":3,"race_medal":0,"score":"319435199036","product_id":100060801,"comment_content":"王喆老师！我刚刚问了那个deepwalk在原本就是序列数据上的应用的问题，我说我能想到的优势就是扩充样本， 但是通过一系列的尝试，我觉得好像是恰恰相反！用deepwalk的时候生成比原序列样本少，才能降低噪音，抓住主要关联。特别想跟老师探讨一下这个结论。<br><br>具体是这样的， 我用我们公司的用户浏览网页的序列数据，来做网页的embedding，原本有 500k条序列，一开始我用deepwalk生成了原数据两倍的样本（1mm）的samples， 结果训练出来的embedding，网页之间的similarity很低 （每个网页跟最近网页的similarity值达到0.5左右， 如果直接用原样本可达0.7）， 接着我试着降低deepwalk生成样本的数量，最后用了跟您同样的20k，通过随机抽查，效果特别的好（可以达到0.9以上，而且结果很make sense）。所以我觉得deepwalk的好处反而是去掉多余噪音信息，关注主要矛盾，所以一般要生成比原样本更少的样本量","like_count":75,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511536,"discussion_content":"非常棒的实践。\n\n所以我一直说实践出真知，从来都不是哪个算法比另外一个更好。我觉得你说的没错，deepwark的抽样过程保留了转移矩阵的“主要框架”，但同时当抽样次数不太高的时候，item embedding的覆盖率反而没有item2vec好。\n\n这次的实践经验推荐其他的同学参考，赞！","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1607650023,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2555962,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/vr3iawgpxbGWtZhFQtZXQxrqzHRQ9VF1bC0opzfNbvzibV5zBzK0lqm835bCnEOnicrviavu1IlSVs7OuQPPKRHZYA/132","nickname":"Geek_3d5fb7","note":"","ucode":"F0E89009F77B04","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":587835,"discussion_content":"&#34;所以我觉得deepwalk的好处反而是去掉多余噪音信息，关注主要矛盾，所以一般要生成比原样本更少的样本量&#34; -------》王老师 请问：最后参与训练的是 500k的样本加上 20k 生成的样本吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1663314666,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":511536,"ip_address":"北京"},"score":587835,"extra":""}]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":355998,"discussion_content":"非常棒的工作经验~ 点赞！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615515850,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253632,"user_name":"微波","can_delete":false,"product_type":"c1","uid":2206497,"ip_address":"","ucode":"0116F316097B55","user_header":"https://static001.geekbang.org/account/avatar/00/21/ab/21/d118e1c4.jpg","comment_is_top":false,"comment_ctime":1602815959,"is_pvip":false,"replies":[{"id":"92634","content":"推荐关注我的推荐系统paper list吧，经典的不能再经典了。https:&#47;&#47;github.com&#47;wzhe06&#47;Reco-papers","user_name":"作者回复","comment_id":253632,"uid":"1662192","ip_address":"","utype":1,"ctime":1602826738,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"186286409687","product_id":100060801,"comment_content":"王老师，对于深度学习这块儿我是个新手，查找网上的东西真是太多了，好像说的都有道理，真是不知道该看些啥，能否推荐一些经典papers作为进一步学习的资料吗？十分感谢！","like_count":43,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507132,"discussion_content":"推荐关注我的推荐系统paper list吧，经典的不能再经典了。https://github.com/wzhe06/Reco-papers","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1602826738,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253796,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1602899789,"is_pvip":false,"replies":[{"id":"92824","content":"赞，标准正确答案。","user_name":"作者回复","comment_id":253796,"uid":"1662192","ip_address":"","utype":1,"ctime":1603094563,"user_name_real":"王喆"}],"discussion_count":4,"race_medal":0,"score":"177696558925","product_id":100060801,"comment_content":"Embedding预训练的优点：1.更快。因为对于End2End的方式，Embedding层的优化还受推荐算法的影响，这会增加计算量。2.难收敛。推荐算法是以Embedding为前提的，在端到端的方式中，在训练初期由于Embedding层的结果没有意义，所以推荐模块的优化也可能不太有意义，可能无法有效收敛。<br>Embedding端到端的优点：可能收敛到更好的结果。端到端因为将Embedding和推荐算法连接起来训练，那么Embedding层可以学习到最有利于推荐目标的Embedding结果。","like_count":42,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507186,"discussion_content":"赞，标准正确答案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603094563,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":314993,"discussion_content":"是的，embedding层往往包含大量参数，e2e训练比较难以收敛。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1603220634,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1349749,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6mh3xlaMoGtWjmVJh2LutdLcQcPbKNjRlVru3bx8ynPhgwuGhhdzTkwEMoXbvBtgkcDSfom1kZg/132","nickname":"夜雨声烦","note":"","ucode":"87D8DB1E32522A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":313960,"discussion_content":"请问“2，难收敛”值得是Embedding End2End的时候比较难收敛吗？而下面说端到端“可能收敛到更好的结果”，这个收敛是指结果，上面“难收敛”指的是Embedding训练是吧。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603108174,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2208459,"avatar":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","nickname":"张弛 Conor","note":"","ucode":"193EBA4A64BAB3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1349749,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6mh3xlaMoGtWjmVJh2LutdLcQcPbKNjRlVru3bx8ynPhgwuGhhdzTkwEMoXbvBtgkcDSfom1kZg/132","nickname":"夜雨声烦","note":"","ucode":"87D8DB1E32522A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":316977,"discussion_content":"对的～前者是训练过程，后者是训练结果～","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1603493305,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":313960,"ip_address":""},"score":316977,"extra":""}]}]},{"had_liked":false,"id":272087,"user_name":"轩","can_delete":false,"product_type":"c1","uid":2405529,"ip_address":"","ucode":"454CC1A0734A4C","user_header":"https://static001.geekbang.org/account/avatar/00/24/b4/99/79a21147.jpg","comment_is_top":false,"comment_ctime":1609929117,"is_pvip":false,"replies":[{"id":"98666","content":"非常非常好的思考，推荐给其他同学。我觉得你说的都没错。<br><br>我想进一步讨论的是第一点，到底模型需不需要重新训练。<br>我觉得对于有些上层模型是不用的，比如这些上层模型只接受embedding的similarity，不直接接收emb本身。<br><br>那么对于接收emb本身直接到网络结构里的，我觉得确实是需要重新训练的。<br><br>当然欢迎大家继续就这个问题讨论，很好的insight","user_name":"作者回复","comment_id":272087,"uid":"1662192","ip_address":"","utype":1,"ctime":1609972096,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"91804242333","product_id":100060801,"comment_content":"老师您好，关于课后思考题有些疑惑，预训练emb和end2end emb：<br>首先 预训练emb实现了模型和emb的解耦，解耦之后，模型只需要关注emb即可，emb就是物品的本征表示，线上服务也就是查redis拿emb完成推断。缺点么，感觉有风险？假如emb是由上游提供，上游重train之后，每一维的隐含意义就变化了，下游模型必须重新train，否则不就出错了？<br><br>end2end的训练的话，对emb可以finetune，理论性能更高，但是总感觉不甚灵活？对于新的物品不停更新发布，岂不是nn.embedding的vocab需要不停的扩充，模型也需要不停的再次训练？<br><br>嗯，感觉在工程落地时，面对非静态的物品集，要么不灵活要么有风险？","like_count":22,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513147,"discussion_content":"非常非常好的思考，推荐给其他同学。我觉得你说的都没错。\n\n我想进一步讨论的是第一点，到底模型需不需要重新训练。\n我觉得对于有些上层模型是不用的，比如这些上层模型只接受embedding的similarity，不直接接收emb本身。\n\n那么对于接收emb本身直接到网络结构里的，我觉得确实是需要重新训练的。\n\n当然欢迎大家继续就这个问题讨论，很好的insight","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609972096,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":257391,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1603954871,"is_pvip":false,"replies":[{"id":"93756","content":"非常好的问题：<br>1、需要归一化。注意我们在原文中一直强调我们要计算的是跳转“权重”，而不是跳转“概率”，既然是权重，我们就需要通过归一化转换成跳转概率。<br><br>2、设置两个参数，是因为与一个点相连的点有三类，dtx=0，1，2。我们固定一类点的权重为1，就需要分别给另外两类点不同的控制参数，就是p和q，这样更灵活。如果只用一个参数，当然也可以，但灵活性就差一点。","user_name":"作者回复","comment_id":257391,"uid":"1662192","ip_address":"","utype":1,"ctime":1604018684,"user_name_real":"王喆"}],"discussion_count":2,"race_medal":0,"score":"40258660535","product_id":100060801,"comment_content":"请问老师，有两个问题有点疑惑，第一个问题是采用Node2Vec算法时，当前节点v到下一个节点x的概率在经过进出参数和返回参数调整后是否需要做概率的归一化操作，使节点v到所有下一节点的概率为1呢？第二个问题是既然我们希望网络要么体现“同质性”要么体现“结构性”的特点，那么为什么一定要设定两个参数p和q，而不是仅用一个参数m(打比方)来实现，当m小，就是同质性强，结构性弱，当m大，就是同质性弱，结构性强？","like_count":9,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508318,"discussion_content":"非常好的问题：\n1、需要归一化。注意我们在原文中一直强调我们要计算的是跳转“权重”，而不是跳转“概率”，既然是权重，我们就需要通过归一化转换成跳转概率。\n\n2、设置两个参数，是因为与一个点相连的点有三类，dtx=0，1，2。我们固定一类点的权重为1，就需要分别给另外两类点不同的控制参数，就是p和q，这样更灵活。如果只用一个参数，当然也可以，但灵活性就差一点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604018684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2735716,"avatar":"https://static001.geekbang.org/account/avatar/00/29/be/64/f01a091c.jpg","nickname":"VICTOR","note":"","ucode":"2EB9D8E02F2ED8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":534440,"discussion_content":"老师您好，我在第8章尝试实现Node2vec时碰到了这个问题，想请教一下在这种情况下，对于（previous_node, current_node, next_node）三元组对应的一个alpha*weight的值，应该对于current_node采用什么样的归一化方法呢？是直接 值除以和，还是使用 min-max 呢，这分别又会有什么影响呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638188848,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":508318,"ip_address":""},"score":534440,"extra":""}]}]},{"had_liked":false,"id":255228,"user_name":"Dikiwi","can_delete":false,"product_type":"c1","uid":2198644,"ip_address":"","ucode":"F6E4C2008C7F7B","user_header":"https://static001.geekbang.org/account/avatar/00/21/8c/74/2bbd132d.jpg","comment_is_top":false,"comment_ctime":1603285759,"is_pvip":false,"replies":[{"id":"93039","content":"这个点非常好。e2e训练的好处之一就是能够找到embedding层在这个模型结构下的最优解。预训练可能损失一些效果。<br><br>但好处你也说过了，可以大幅加快收敛的速度。","user_name":"作者回复","comment_id":255228,"uid":"1662192","ip_address":"","utype":1,"ctime":1603347434,"user_name_real":"王喆"}],"discussion_count":3,"race_medal":0,"score":"31668056831","product_id":100060801,"comment_content":"直观理解，预训练的emb本身因为是有一定意义的，所以喂给mlp之后理论上可以加速收敛，但因为这个emb是通过其他方法训练出来的，本身不是对该模型服务的，所以很可能走到局部最优解？","like_count":8,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507650,"discussion_content":"这个点非常好。e2e训练的好处之一就是能够找到embedding层在这个模型结构下的最优解。预训练可能损失一些效果。\n\n但好处你也说过了，可以大幅加快收敛的速度。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603347434,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2198644,"avatar":"https://static001.geekbang.org/account/avatar/00/21/8c/74/2bbd132d.jpg","nickname":"Dikiwi","note":"","ucode":"F6E4C2008C7F7B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":316729,"discussion_content":"预训练的emb+finetune会不会是最佳选择呢？模型效果应该是可以逼近最优解的，但是怀疑与直接随机初始化emb进行训练的收敛速度差不多，毕竟这个emb理论上对这个mlp来讲基本上也就差不多是一个随机初始化的值。","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1603446325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":317863,"discussion_content":"一般来说业界偏向预训练的方案，因为emb的更新速度没必要特别高，但DNN的部分可以做快速的更新以catch到最新的数据变化。而且确实像你说的，e2e训练在实际的应用中没有发现效果有明显提高。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1603609560,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253678,"user_name":"远方蔚蓝","can_delete":false,"product_type":"c1","uid":1095490,"ip_address":"","ucode":"514A0BA621DF1A","user_header":"https://static001.geekbang.org/account/avatar/00/10/b7/42/1f44ce49.jpg","comment_is_top":false,"comment_ctime":1602829356,"is_pvip":false,"replies":[{"id":"92699","content":"会讲GraphSAGE的原理和细节。","user_name":"作者回复","comment_id":253678,"uid":"1662192","ip_address":"","utype":1,"ctime":1602871518,"user_name_real":"王喆"}],"discussion_count":2,"race_medal":0,"score":"31667600428","product_id":100060801,"comment_content":"老师后面会介绍一下GraphSAGE和GAT在推荐的应用与实践吗，业界现在用的挺多。","like_count":7,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507144,"discussion_content":"会讲GraphSAGE的原理和细节。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602871518,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2665110,"avatar":"https://static001.geekbang.org/account/avatar/00/28/aa/96/09f19dc9.jpg","nickname":"神经蛙","note":"","ucode":"65E5A5747A5797","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":381537,"discussion_content":"LINE这个工作在工业界应用多吗？当年调研network embedding领域的时候感觉主要的几个工作就是deepwalk，LINE和node2vec。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625120511,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253643,"user_name":"Geek_63ee39","can_delete":false,"product_type":"c1","uid":2202096,"ip_address":"","ucode":"A41379ADD76480","user_header":"https://static001.geekbang.org/account/avatar/00/21/99/f0/ba3c0208.jpg","comment_is_top":false,"comment_ctime":1602818435,"is_pvip":false,"replies":[{"id":"92647","content":"这是非常常见的一个问题，也推荐其他有疑问的同学关注。<br><br>推荐参考原文中的解释。<br><br>We observe that BFS and DFS strategies play a key role in producing representations that reflect either of the above equivalences.<br><br>In particular, the neighborhoods sampled by BFS lead to embeddings that correspond closely to structural equivalence. <br><br>The opposite is true for DFS which can explore larger parts of the network as it can move further away from the source node u (with sample size k being fixed). <br><br>In DFS, the sampled nodes more accurately reflect a macro-view of the neighborhood which is essential in inferring communities based on homophily.<br><br>原文地址 https:&#47;&#47;github.com&#47;wzhe06&#47;Reco-papers&#47;blob&#47;master&#47;Embedding&#47;%5BNode2vec%5D%20Node2vec%20-%20Scalable%20Feature%20Learning%20for%20Networks%20%28Stanford%202016%29.pdf<br><br>","user_name":"作者回复","comment_id":253643,"uid":"1662192","ip_address":"","utype":1,"ctime":1602829150,"user_name_real":"王喆"}],"discussion_count":5,"race_medal":0,"score":"31667589507","product_id":100060801,"comment_content":"“首先，为了使 Graph Embedding 的结果能够表达网络的“结构性”，在随机游走的过程中，我们需要让游走的过程更倾向于 BFS（Breadth First Search，宽度优先搜索）”<br>这里应该是DFS吧？并且同质性是使用BFS","like_count":7,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507135,"discussion_content":"这是非常常见的一个问题，也推荐其他有疑问的同学关注。\n\n推荐参考原文中的解释。\n\nWe observe that BFS and DFS strategies play a key role in producing representations that reflect either of the above equivalences.\n\nIn particular, the neighborhoods sampled by BFS lead to embeddings that correspond closely to structural equivalence. \n\nThe opposite is true for DFS which can explore larger parts of the network as it can move further away from the source node u (with sample size k being fixed). \n\nIn DFS, the sampled nodes more accurately reflect a macro-view of the neighborhood which is essential in inferring communities based on homophily.\n\n原文地址 https://github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BNode2vec%5D%20Node2vec%20-%20Scalable%20Feature%20Learning%20for%20Networks%20%28Stanford%202016%29.pdf\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602829150,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2202096,"avatar":"https://static001.geekbang.org/account/avatar/00/21/99/f0/ba3c0208.jpg","nickname":"Geek_63ee39","note":"","ucode":"A41379ADD76480","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":312997,"discussion_content":"文中后面小结中提到：“而 Node2vec 相比于 Deep Walk，增加了随机游走过程中跳转概率的倾向性，如果倾向于宽度优先搜索，则 Embedding 结果更加体现“同质性”；如果倾向于深度优先搜索，则更加体现“结构性”。”\n\n这里和前面正文的说法似乎有些矛盾，麻烦老师再看一看。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1602905088,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2202096,"avatar":"https://static001.geekbang.org/account/avatar/00/21/99/f0/ba3c0208.jpg","nickname":"Geek_63ee39","note":"","ucode":"A41379ADD76480","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":315358,"discussion_content":"这段确实有错误，会修改原文，多谢指正！","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1603266366,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":312997,"ip_address":""},"score":315358,"extra":""},{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1127175,"avatar":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","nickname":"JustDoDT","note":"","ucode":"6AF0B80F00EAEF","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":315362,"discussion_content":"这段没有问题，p越小，更倾向BFS，更倾向结构性。q越小，更倾向DFS，更倾向同质性。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1603266456,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":314953,"ip_address":""},"score":315362,"extra":""},{"author":{"id":1127175,"avatar":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","nickname":"JustDoDT","note":"","ucode":"6AF0B80F00EAEF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":315612,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603293596,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":315362,"ip_address":""},"score":315612,"extra":""}]}]},{"had_liked":false,"id":280235,"user_name":"inkachenko","can_delete":false,"product_type":"c1","uid":2441833,"ip_address":"","ucode":"0AC06DE6387388","user_header":"https://static001.geekbang.org/account/avatar/00/25/42/69/8c56cea0.jpg","comment_is_top":false,"comment_ctime":1614138717,"is_pvip":false,"replies":[{"id":"101873","content":"我在sparrow里的实现是后一种，我觉得更合理，否则会改变数据的分布。","user_name":"作者回复","comment_id":280235,"uid":"1662192","ip_address":"","utype":1,"ctime":1614304619,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"23088975197","product_id":100060801,"comment_content":"老师，我想问一下deep-walk随机选择起始点的时候，是所有节点等概率选取呢？还是像HMM一样，以原始行为序列中节点出现次数为权重建立一个初始状态概率分布，再随机选取呢？感觉后一种更加合理。。","like_count":5,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516031,"discussion_content":"我在sparrow里的实现是后一种，我觉得更合理，否则会改变数据的分布。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614304619,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253642,"user_name":"雪焰🐻🥑","can_delete":false,"product_type":"c1","uid":2202366,"ip_address":"","ucode":"A8F45F16BBEFB9","user_header":"https://static001.geekbang.org/account/avatar/00/21/9a/fe/a984c940.jpg","comment_is_top":false,"comment_ctime":1602818399,"is_pvip":false,"replies":[{"id":"92645","content":"最直接的方式是直接concatenate 再交由后续多层神经网络处理。<br><br>为了实现一些重点embedding之间的交叉，也可以进行多embedding之间的element wise交叉，或者一些乘积操作后输入后续mlp。<br><br>基于emb的操作非常多，这里没有统一的答案，也是各种模型层出不穷的原因。","user_name":"作者回复","comment_id":253642,"uid":"1662192","ip_address":"","utype":1,"ctime":1602827619,"user_name_real":"王喆"}],"discussion_count":2,"race_medal":0,"score":"23077654879","product_id":100060801,"comment_content":"对于文中的:&quot;“预训练应用”指的是在我们预先训练好物品和用户的 Embedding 之后，不直接应用，而是把这些 Embedding 向量作为特征向量的一部分，跟其余的特征向量拼接起来&quot;<br>请问老师，比如对文本的embedding x和图像的embedding y会是得到不同的维度，这种情况下怎么把x和y拼接起来输入DL 模型呢？直接concatenate么？不知道下节课会不会涉及到具体操作，谢谢老师!","like_count":5,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507134,"discussion_content":"最直接的方式是直接concatenate 再交由后续多层神经网络处理。\n\n为了实现一些重点embedding之间的交叉，也可以进行多embedding之间的element wise交叉，或者一些乘积操作后输入后续mlp。\n\n基于emb的操作非常多，这里没有统一的答案，也是各种模型层出不穷的原因。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602827619,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":356010,"discussion_content":"非常棒的问题~点赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615517201,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272252,"user_name":"Wa","can_delete":false,"product_type":"c1","uid":2403705,"ip_address":"","ucode":"96E28E50FCA96C","user_header":"","comment_is_top":false,"comment_ctime":1610006401,"is_pvip":false,"replies":[{"id":"98736","content":"用户依次点击虽然被你的展示顺序bias，但是仍然包含了用户共同的兴趣特点。所以完全可以使用item2vec。当然最终的效果要看你的实践结果，这里都是经验上的猜测。","user_name":"作者回复","comment_id":272252,"uid":"1662192","ip_address":"","utype":1,"ctime":1610046500,"user_name_real":"王喆"}],"discussion_count":2,"race_medal":0,"score":"18789875585","product_id":100060801,"comment_content":"一直没动手尝试item2vec和graph embedding相关算法，因为对于我们的业务，不同商品之间关系比较独立，不存在“先看了A明星新闻，再看与他有绯闻的B明星新闻，再看他们共同作品的新闻...”这种有时序关系的用户行为序列，所以不确定用类似word2vec这种作用于文本（文本天然具有强前后相关性）的模型是否有效。我们的用户依次点击A - B - C - D可能仅仅是因为展示列表时这个顺序，而不存在A离B近而离D远这种信息，不知道老师怎么看。","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513214,"discussion_content":"用户依次点击虽然被你的展示顺序bias，但是仍然包含了用户共同的兴趣特点。所以完全可以使用item2vec。当然最终的效果要看你的实践结果，这里都是经验上的猜测。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610046500,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2527743,"avatar":"https://static001.geekbang.org/account/avatar/00/26/91/ff/aa5e573a.jpg","nickname":"牛牪犇","note":"","ucode":"007A5E511EA243","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":361404,"discussion_content":"我觉得item2vec可能真有效，因为根据cbow或skip-gram框架进行的训练，讲究的是邻域相似(同一个语境)，而与时序的先后无关, 而且数据量有一定的规模了，可能会降低展示列表的影响","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616663295,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265367,"user_name":"浣熊当家","can_delete":false,"product_type":"c1","uid":1952248,"ip_address":"","ucode":"939F06050423E4","user_header":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","comment_is_top":false,"comment_ctime":1606879123,"is_pvip":false,"replies":[{"id":"96525","content":"哈哈，这其实是个好问题。TensorBoard里其实是有embedidng可视化的工具的，可以研究一下。其他还有Embedding Projector之类的，可以自己研究一下。","user_name":"作者回复","comment_id":265367,"uid":"1662192","ip_address":"","utype":1,"ctime":1606940577,"user_name_real":"王喆"}],"discussion_count":3,"race_medal":0,"score":"18786748307","product_id":100060801,"comment_content":"老师知道有什么sample code， 可以把转移概率矩阵（项目中的transitionMatrix 和itemDistribution ）生成图5这种graph可视化图吗？感觉在做presentation的时候，人们就认图，没有图感觉说再多也没有热烈的反馈","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510904,"discussion_content":"哈哈，这其实是个好问题。TensorBoard里其实是有embedidng可视化的工具的，可以研究一下。其他还有Embedding Projector之类的，可以自己研究一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606940577,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1952248,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","nickname":"浣熊当家","note":"","ucode":"939F06050423E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333845,"discussion_content":"老师， 我用python 的networkx库实现了transitionMatrix的可视化（类似图5），可不可以contribute到您的项目里。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607651316,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1952248,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","nickname":"浣熊当家","note":"","ucode":"939F06050423E4","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":334570,"discussion_content":"当然可以，但是python代码的话我建议你先放到TFRecModel那个Python模块里，我到时候会review","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607903502,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":333845,"ip_address":""},"score":334570,"extra":""}]}]},{"had_liked":false,"id":280067,"user_name":"褚江","can_delete":false,"product_type":"c1","uid":2352835,"ip_address":"","ucode":"A01870AC63342A","user_header":"https://static001.geekbang.org/account/avatar/00/23/e6/c3/35099b03.jpg","comment_is_top":false,"comment_ctime":1614068816,"is_pvip":false,"replies":[{"id":"101871","content":"embedding层应该是没法跟树模型结合的。但是训练好的embedding也许可以放到树模型里面做进一步训练。仅作参考。","user_name":"作者回复","comment_id":280067,"uid":"1662192","ip_address":"","utype":1,"ctime":1614304557,"user_name_real":"王喆"}],"discussion_count":3,"race_medal":0,"score":"14498970704","product_id":100060801,"comment_content":"王老师，我想请问， embedding层可以加入到树模型中吗？很多时候在比较小的样本上训练，感觉Lgbm会好很多，但好像没有人这样做是吗？<br>","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515973,"discussion_content":"embedding层应该是没法跟树模型结合的。但是训练好的embedding也许可以放到树模型里面做进一步训练。仅作参考。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614304557,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2512010,"avatar":"","nickname":"Geek_4a8688","note":"","ucode":"A394B5A54A6FB9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":404255,"discussion_content":"可以试着喂给lgb的，我试过node2vec的embedding，序列直接取和或者均值，当做特征列输入给lgb，效果还不错。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1634269987,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2613785,"avatar":"","nickname":"pptbt29","note":"","ucode":"673C3835A1E659","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384034,"discussion_content":"embedding之间做的交叉产生的值类似于cosine similarity或inner product可作为特征放入树模型中","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626340751,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":275335,"user_name":"Dive","can_delete":false,"product_type":"c1","uid":2325781,"ip_address":"","ucode":"9374AADF0EB8A0","user_header":"https://static001.geekbang.org/account/avatar/00/23/7d/15/c099a439.jpg","comment_is_top":false,"comment_ctime":1611477152,"is_pvip":false,"replies":[{"id":"99966","content":"这两种情况都会存在，但业界第一种用的比较多。<br><br>后续课程还会讨论这个问题，可以继续学习。","user_name":"作者回复","comment_id":275335,"uid":"1662192","ip_address":"","utype":1,"ctime":1611551249,"user_name_real":"王喆"}],"discussion_count":2,"race_medal":0,"score":"14496379040","product_id":100060801,"comment_content":"王老师，好多论文里介绍完自己的深度学习推荐模型A都会加一个预训练嵌入的对比。我理解是将预训练得到的嵌入作为深度学习推荐系统模型A的嵌入的初始化；我其实很疑惑，实际中应该是情况①还是情况②呢？感谢老师~<br>①这些嵌入在深度学习模型中就固定住，不会更新<br>②这些嵌入仅仅是初始化作用，在模型中还会更新","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514314,"discussion_content":"这两种情况都会存在，但业界第一种用的比较多。\n\n后续课程还会讨论这个问题，可以继续学习。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611551249,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2325781,"avatar":"https://static001.geekbang.org/account/avatar/00/23/7d/15/c099a439.jpg","nickname":"Dive","note":"","ucode":"9374AADF0EB8A0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":344833,"discussion_content":"谢谢老师的回答~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1611582384,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":306486,"user_name":"香格里拉飞龙","can_delete":false,"product_type":"c1","uid":1339856,"ip_address":"","ucode":"C1263416EE85E3","user_header":"https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKS70PShNZaxpibFc1gWuvibbg3hXR4YKm3MkNgX0n56hWUicN0JfB2GQ6I9UicBfKABH6dkfVDPohA6Q/132","comment_is_top":false,"comment_ctime":1628581493,"is_pvip":false,"replies":[{"id":"110989","content":"我觉得这个理解没问题，赞。","user_name":"作者回复","comment_id":306486,"uid":"1662192","ip_address":"","utype":1,"ctime":1628618804,"user_name_real":"王喆"}],"discussion_count":2,"race_medal":0,"score":"10218516085","product_id":100060801,"comment_content":"老师，关于同质性和结构性及其表达，不知这样理解是否可行呢？<br>1.倾向于广度优先搜索时（p越小），节点更容易在起始点周围跳转，而且经常会返回前一节点，反应微观的、局部的关系。<br>比如起始点为图3中节点u，游走长度设为4。所以游走序列可能是u s1 u s1，u s1 u s2，u s1 s2 s1，u s1 s2 u，……，经过许多次游走后，会发现游走序列大部分都在u及其相邻点s1、s2、s3、s4之间转悠，而且其中会有一部分序列在转悠一圈后又回到u。于是可以稍微推断出u是中心节点，且s6的情况与之类似。<br>而如果起始点设为s9，虽然也有一部分游走序列中多次出现s9，但是若序列从s9到s6，之后又跳转到了s5或s7，就无法再回到s9。在广度优先中，s9为起始点无法返回自身的概率显然比u为起始点无法返回自身的概率大。<br>故广度优先倾向于表现结构性。<br>2.倾向于深度优先搜索时（q越小），节点更容易跳转至更远处节点，反应宏观的节点关系。<br>依然以起始点u举例，游走长度为4。深度优先下更能游走至更远更新的节点，游走序列可能是u s1 s2 s4，u s1 s3 s4，u s1 s2 u，u s1 s2 s5，……，游走序列在更大的概率上游走至s5、s6，并且因为s1和s3并没有外部的节点与之相连，会有一部分序列依然在u和其相邻节点中转悠。所以表现出同质性，与同一节点距离相近的，高度互联且属于相似社区。<br>还有一种理解。如果一个节点a及其四个相邻节点高度互联，与上图中u极其邻点相似，然而这四个邻点又各自与其他点相连，这些更远处点距离a点为2。于是在游走过程中，有一部分游走序列在a点及其邻点间转悠，也有一部分游走至更远点，不再回来。但是因为a与邻点高度互联，在a与邻点间游走的概率更大。此也能表达同质性。<br>","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524787,"discussion_content":"我觉得这个理解没问题，赞。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628618804,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1339856,"avatar":"https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKS70PShNZaxpibFc1gWuvibbg3hXR4YKm3MkNgX0n56hWUicN0JfB2GQ6I9UicBfKABH6dkfVDPohA6Q/132","nickname":"香格里拉飞龙","note":"","ucode":"C1263416EE85E3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388190,"discussion_content":"ok谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628647498,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290015,"user_name":"Geek_fdb832","can_delete":false,"product_type":"c1","uid":2532699,"ip_address":"","ucode":"0BA1479D33DC07","user_header":"","comment_is_top":false,"comment_ctime":1619321827,"is_pvip":false,"replies":[{"id":"105186","content":"一般不把这些属性信息直接放在graph里面游走，而是在主推荐模型中引入，或者是用GNN等方式来学习。","user_name":"作者回复","comment_id":290015,"uid":"1662192","ip_address":"","utype":1,"ctime":1619411835,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"10209256419","product_id":100060801,"comment_content":"想求教一下王老师，graph embedding只是embedding了graph有关的信息 (node之间的关系) 吗？如果一个item有些其他的性质，比如price, name, category, 这方面的信息能也能在随机游走的过程中放在graph embedding里吗？","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519088,"discussion_content":"一般不把这些属性信息直接放在graph里面游走，而是在主推荐模型中引入，或者是用GNN等方式来学习。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619411835,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289508,"user_name":"DAMIAN","can_delete":false,"product_type":"c1","uid":2547259,"ip_address":"","ucode":"37A7C27DBDBCC5","user_header":"https://static001.geekbang.org/account/avatar/00/26/de/3b/c15d2d9f.jpg","comment_is_top":false,"comment_ctime":1619057797,"is_pvip":false,"replies":[{"id":"105067","content":"这两点都答的非常好。<br><br>最后的问题，当然是可行的，比如之前FNN就这么做过，可以有效的降低收敛所需的epoch数量。","user_name":"作者回复","comment_id":289508,"uid":"1662192","ip_address":"","utype":1,"ctime":1619112637,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"10208992389","product_id":100060801,"comment_content":"课后思考：<br>1. pre-training 优点：embedding泛化能力强，即使后接不同的任务，也都可以work，bp的层数很少，收敛比较快<br>缺点：精度一般比较低，不能针对特定任务优化<br>2. end2end: 优点：一般精度比前者高，embedding可以针对特定任务优化<br>缺点：训练慢，embedding几乎不可能用来做其他的任务，泛化能力比较差<br><br>想请问老师，按照迁移学习的思路，将预训练的embedding放到end2end模型中fine-tuning会不会有好的效果呢？","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518937,"discussion_content":"这两点都答的非常好。\n\n最后的问题，当然是可行的，比如之前FNN就这么做过，可以有效的降低收敛所需的epoch数量。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619112637,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":277293,"user_name":"Sanders","can_delete":false,"product_type":"c1","uid":1697075,"ip_address":"","ucode":"3D460FEEDCDF34","user_header":"","comment_is_top":false,"comment_ctime":1612347928,"is_pvip":false,"replies":[{"id":"100812","content":"1.可以这样理解<br>2.也许可以等价，但node2vec跟label的值没有直接关系，还是先做采样，在做word2vec","user_name":"作者回复","comment_id":277293,"uid":"1662192","ip_address":"","utype":1,"ctime":1612596010,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"10202282520","product_id":100060801,"comment_content":"结合Word2Vec这样理解对不对？<br>1. Random Walk和Node2Vec实际上是构建语料-训练样本的过程，Graph Embedding结构和之前Word2Vec是一样的<br>2. Word2Vec中会将相邻词的label值设为1，Node2Vec会用跳转概率作为相邻节点的label值","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514989,"discussion_content":"1.可以这样理解\n2.也许可以等价，但node2vec跟label的值没有直接关系，还是先做采样，在做word2vec","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612596010,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263412,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1606128361,"is_pvip":false,"replies":[{"id":"95615","content":"你可以思考一下可不可以，类似kmeans这类方法是如何聚类的，能不能应用在图结构数据上。","user_name":"作者回复","comment_id":263412,"uid":"1662192","ip_address":"","utype":1,"ctime":1606181580,"user_name_real":"王喆"}],"discussion_count":3,"race_medal":0,"score":"10196062953","product_id":100060801,"comment_content":"Node embedding 的最终结果看起来看起来更像聚类结果。<br>是否可以利用聚类算法来实现Node embedding的过程呢？","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510178,"discussion_content":"你可以思考一下可不可以，类似kmeans这类方法是如何聚类的，能不能应用在图结构数据上。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606181580,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1073125,"avatar":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","nickname":"范闲","note":"","ucode":"F21FD7DF6BA53C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":329516,"discussion_content":"仔细想了一下还是不可以的。\n1. Node Embedding的主要目的是为了拿到隐藏表示层，也就是embedding向量\n2.kmeans的聚类算法主要的输入都是向量表示\n3.从结构来说，kmeans只能通过距离来表示点的关系。Node embedding除了这种点的关系，其实还有层次上的依赖关系。\n不知道说的对不对，请王老师指正下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606396439,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1073125,"avatar":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","nickname":"范闲","note":"","ucode":"F21FD7DF6BA53C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":334571,"discussion_content":"你说的很好，我觉得本质上的重要区别是，kmeans的输出是一个聚类结果，embedding方法的输出是一个vector，这个vector是一个高维的表示，它包含的信息是比一个聚类结果id高很多的。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1607903687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":329516,"ip_address":""},"score":334571,"extra":""}]}]},{"had_liked":false,"id":303814,"user_name":"Roy Liang","can_delete":false,"product_type":"c1","uid":1098898,"ip_address":"","ucode":"1DF5FC831A35DA","user_header":"https://static001.geekbang.org/account/avatar/00/10/c4/92/338b5609.jpg","comment_is_top":false,"comment_ctime":1627011173,"is_pvip":false,"replies":[{"id":"109967","content":"跟uber的人随便聊过一些，主要特点是基于lbs的召回设计。其他没有本质的区别。","user_name":"作者回复","comment_id":303814,"uid":"1662192","ip_address":"","utype":1,"ctime":1627018139,"user_name_real":"王喆"}],"discussion_count":1,"race_medal":0,"score":"5921978469","product_id":100060801,"comment_content":"老师，有基于地图的推荐系统案例介绍吗？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523834,"discussion_content":"跟uber的人随便聊过一些，主要特点是基于lbs的召回设计。其他没有本质的区别。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627018139,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291459,"user_name":"谁将新樽盛旧月","can_delete":false,"product_type":"c1","uid":2583928,"ip_address":"","ucode":"40045EEA19DAAC","user_header":"https://static001.geekbang.org/account/avatar/00/27/6d/78/c4a02882.jpg","comment_is_top":false,"comment_ctime":1620294666,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5915261962","product_id":100060801,"comment_content":"这章真的难","like_count":1},{"had_liked":false,"id":290299,"user_name":"MutouMan","can_delete":false,"product_type":"c1","uid":2454131,"ip_address":"","ucode":"E2E78C6EE25E80","user_header":"https://static001.geekbang.org/account/avatar/00/25/72/73/d707c8be.jpg","comment_is_top":false,"comment_ctime":1619491488,"is_pvip":false,"replies":[{"id":"105351","content":"非常好","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1619729602,"ip_address":"","comment_id":290299,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5914458784","product_id":100060801,"comment_content":"End2end的方法肯定能够得到更精确的表达：1. 数据的时效性； 2. 和其它特征共同训练，还可以学习到交叉后的效果； 3. 根据我们设定的目标（loss function），得到的embedding会更趋向于我们想要的结果，可能会损失些generalization，但是bias会减少。但是end2end明显就是要花很多时间了，不可能做到线上训练和推荐都在ms级。<br>预训练：最大的好处就是模型上线后，可以快速抽取特征进行召回然后排序。当然就是会有时间上的滞后，推荐的效果是会比理论上差。","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519165,"discussion_content":"非常好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619729602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283825,"user_name":"乔伊波伊","can_delete":false,"product_type":"c1","uid":2300036,"ip_address":"","ucode":"1E3EDC2D997FB2","user_header":"https://static001.geekbang.org/account/avatar/00/23/18/84/2f950e5e.jpg","comment_is_top":false,"comment_ctime":1615948231,"is_pvip":false,"replies":[{"id":"103412","content":"这种做法不是特别常见，当然你也可以去尝试。你说的倾向性问题原理上感觉不会存在这个现象，还是需要实践验证。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1616636157,"ip_address":"","comment_id":283825,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5910915527","product_id":100060801,"comment_content":"老师你好，请问像xgboost等树模型适合使用embedding出来的特征作为输入吗，这样在树分裂的时候会不会更倾向于embedding特征而弱化原始特征如年龄、点击率等","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517158,"discussion_content":"这种做法不是特别常见，当然你也可以去尝试。你说的倾向性问题原理上感觉不会存在这个现象，还是需要实践验证。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616636157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283434,"user_name":"温华","can_delete":false,"product_type":"c1","uid":2444759,"ip_address":"","ucode":"5F1E5C752E906C","user_header":"https://static001.geekbang.org/account/avatar/00/25/4d/d7/5f835e27.jpg","comment_is_top":false,"comment_ctime":1615780805,"is_pvip":false,"replies":[{"id":"102837","content":"非常好","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615794080,"ip_address":"","comment_id":283434,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5910748101","product_id":100060801,"comment_content":"Embeddding预训练<br><br>优点：embeddding训练与模型训练解耦，训练后的embedding再喂给推荐模型，可以加速收敛<br><br>缺点：工程实现上更复杂，因为解耦，需要训练两个模型，embedding模型产生的embedding向量需要刷到线上特征库进行存储，然后才能给模型输入时使用<br><br>E2E<br><br>优点：embeddding训练包含在推荐模型训练过程中，体现了端到端，流程上更容易，不用再额外考虑embedding训练<br><br>缺点：embedding层包含大量参数，训练可能难以收敛，甚至可能因为embedding矩阵过大，计算资源不够导致无法训练","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517039,"discussion_content":"非常好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615794080,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":278682,"user_name":"Peter","can_delete":false,"product_type":"c1","uid":1054531,"ip_address":"","ucode":"71ED7FAA6FF754","user_header":"https://static001.geekbang.org/account/avatar/00/10/17/43/1ef14026.jpg","comment_is_top":false,"comment_ctime":1613197388,"is_pvip":false,"replies":[{"id":"101385","content":"非常好的点。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1613529040,"ip_address":"","comment_id":278682,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5908164684","product_id":100060801,"comment_content":"你能尝试对比一下 Embedding 预训练和 Embedding End2End 训练这两种应用方法，说出它们之间的优缺点吗？<br>预训练：<br>优点；更容易实现，实现更快，且不需要标注数据<br>缺点：学习出的向量仅有数据的结构距离特性，无法进一步完成具体任务。<br>e2e：<br>优点：可以完成更多的任务，由于是监督学习，可以获得更高的任务指标。<br>缺点：需要标注数据，成本高","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515464,"discussion_content":"非常好的点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1613529040,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":267170,"user_name":"浣熊当家","can_delete":false,"product_type":"c1","uid":1952248,"ip_address":"","ucode":"939F06050423E4","user_header":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","comment_is_top":false,"comment_ctime":1607617012,"is_pvip":false,"replies":[{"id":"96982","content":"是这样，意义并不大。我希望通过这个例子让大家掌握graph embedding的原理，并不是说deep walk的效果就一定比item2vec好。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1607649818,"ip_address":"","comment_id":267170,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5902584308","product_id":100060801,"comment_content":"老师， 用deep walk解决原本就是graph的结构数据很有道理，但是像我们电影推荐这正，原本是序列的数据，通过建图，然后在生成序列数据，这样用deepwalk，相比于直接用原有序列样本直接导入word2vec的有什么优势呢？我唯一能想到的好处就是生成比原有序列更多的样本，来扩充样本数量防止过拟合。 但是如果原有序列数据已经足够大，是不是deepwalk就没有意义了。","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511534,"discussion_content":"是这样，意义并不大。我希望通过这个例子让大家掌握graph embedding的原理，并不是说deep walk的效果就一定比item2vec好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607649818,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2298734,"avatar":"https://static001.geekbang.org/account/avatar/00/23/13/6e/f1e23980.jpg","nickname":"Macielyoung","note":"","ucode":"DC33AC43B0B5BA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":336285,"discussion_content":"我理解是不是通过deep walk可能会引入原先不存在的序列（通过序列相邻构造出的），这个在一定程度上增加了泛化能力，提高了节点之间的关联程度。","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1608544663,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":262627,"user_name":"Lucifer","can_delete":false,"product_type":"c1","uid":2325595,"ip_address":"","ucode":"FE65094A67640D","user_header":"https://static001.geekbang.org/account/avatar/00/23/7c/5b/7a8d842c.jpg","comment_is_top":false,"comment_ctime":1605785360,"is_pvip":false,"replies":[{"id":"95325","content":"课程中没有说过图5上使用bfs构图，下图是用dfs构图的吧？只是给出了同质性和结构性的例子。<br><br>不过这几个概念确实比较绕，需要多厘清一下。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1605832913,"ip_address":"","comment_id":262627,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5900752656","product_id":100060801,"comment_content":"图5实验中，上图：采用bfs构图，近的emb相似同色，体现同质性；下图：采用dfs构图，远的emb相似同色，体现同构性。跟下文以及答疑中的bfs对应同构，dfs对应同质是不是有些矛盾。","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":509904,"discussion_content":"课程中没有说过图5上使用bfs构图，下图是用dfs构图的吧？只是给出了同质性和结构性的例子。\n\n不过这几个概念确实比较绕，需要多厘清一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605832913,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":345974,"user_name":"星晴","can_delete":false,"product_type":"c1","uid":2195455,"ip_address":"","ucode":"F748EC40E922CF","user_header":"https://static001.geekbang.org/account/avatar/00/21/7f/ff/0b2adf91.jpg","comment_is_top":false,"comment_ctime":1652747591,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1652747591","product_id":100060801,"comment_content":"请问老师，embedding层可以与浅层线性模型，比如fm，ffm结合吗？","like_count":0},{"had_liked":false,"id":340825,"user_name":"Geek_f53f8f","can_delete":false,"product_type":"c1","uid":1214330,"ip_address":"","ucode":"0A82214CDB9091","user_header":"https://static001.geekbang.org/account/avatar/00/12/87/7a/ae1ecc1a.jpg","comment_is_top":false,"comment_ctime":1649165668,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1649165668","product_id":100060801,"comment_content":"王喆老师，您好！我最近在研究召回使用的embedding用于精排中。对于精排而言，一般使用的是曝光和点击作为正负样本，而召回，或者粗排针对全部内容库中的物料数据。这样召回模型训练出来的embedding在精排能直接使用吗？在我理解里，二者的样本分布已经发生了变化，直接使用的效果应该不会太好，但是又没有什么比较好的使用思路（本来想将用户的行为序列的embeeding和候选物料求相似度，做一类相似序列特征。但是这样线上的压力好像会非常大）？","like_count":0},{"had_liked":false,"id":339106,"user_name":"Zack","can_delete":false,"product_type":"c1","uid":1060280,"ip_address":"","ucode":"14D6A0CF867A3B","user_header":"https://static001.geekbang.org/account/avatar/00/10/2d/b8/b5056bb0.jpg","comment_is_top":false,"comment_ctime":1647918656,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1647918656","product_id":100060801,"comment_content":"老师你好，这应该说的是结构性吧！<br>“我这里所说的网络的“同质性”指的是距离相近节点的 Embedding 应该尽量近似，如图 3 所示，节点 u 与其相连的节点 s1、s2、s3、s4的 Embedding 表达应该是接近的，这就是网络“同质性”的体现。”","like_count":0},{"had_liked":false,"id":338198,"user_name":"Right as rain","can_delete":false,"product_type":"c1","uid":1266580,"ip_address":"","ucode":"08CE2EA47154E5","user_header":"https://static001.geekbang.org/account/avatar/00/13/53/94/0ae2a315.jpg","comment_is_top":false,"comment_ctime":1647349866,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1647349866","product_id":100060801,"comment_content":"王老师，有没有embedding可视化的工具，可以用来对比向量间是不是相似的？","like_count":0},{"had_liked":false,"id":304313,"user_name":"Geek_d72f25","can_delete":false,"product_type":"c1","uid":2644298,"ip_address":"","ucode":"AE765F79A1B47D","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/PYPYsHxv1LianDoTxyIajETYSfBUDDicfNMS1hwCicIVWI3UREWbhBm9f1H1rGwBUQ11cIjhbIA5gQ4QQYmicCjGIw/132","comment_is_top":false,"comment_ctime":1627351284,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1627351284","product_id":100060801,"comment_content":"单独训练出来的emb如何在下游使用中保持语义不变，结果有意义[疑问]","like_count":0},{"had_liked":false,"id":295506,"user_name":"马龙流","can_delete":false,"product_type":"c1","uid":1087792,"ip_address":"","ucode":"16F9CE022297FF","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erUKWZy1fBBcJncWRNh9M3TkjThqgsIIpmGOTCyg2IN80IDf3COkeWyTLHliczAppIkfBgCJTsUn1g/132","comment_is_top":false,"comment_ctime":1622454720,"is_pvip":false,"replies":[{"id":"108156","content":"这是graph embedding灵活的地方，可以根据你的需求自己去定义，比如node2vec就是在权重上进行改变，使之倾向于不同的目标。<br><br>一般来说边的权重跟两个node之间产生联系次数的多少成正相关。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1623876276,"ip_address":"","comment_id":295506,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1622454720","product_id":100060801,"comment_content":"在这边，边和边的权重怎么确定呢？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521108,"discussion_content":"这是graph embedding灵活的地方，可以根据你的需求自己去定义，比如node2vec就是在权重上进行改变，使之倾向于不同的目标。\n\n一般来说边的权重跟两个node之间产生联系次数的多少成正相关。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623876276,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293859,"user_name":"β","can_delete":false,"product_type":"c1","uid":2410273,"ip_address":"","ucode":"7709E405121059","user_header":"https://static001.geekbang.org/account/avatar/00/24/c7/21/6a8e267a.jpg","comment_is_top":false,"comment_ctime":1621579678,"is_pvip":false,"replies":[{"id":"106654","content":"直观的理解是，DFS希望在一个大集团内部游走，如果只用BFS附近游走，那么拿不到一个比较大集团内部的相似性。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1621704181,"ip_address":"","comment_id":293859,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1621579678","product_id":100060801,"comment_content":"你好，结构性通过BFS随机游走搜索一个中心节点和它周围的节点建立，结构性关系。但同质性为啥是DFS随机游走？比如你随机游走的结果是u和s6，它们应该是同结构性的，和u同质的应该还是它周围那些节点，就是之前结构性随机游走建立的。？？？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520401,"discussion_content":"直观的理解是，DFS希望在一个大集团内部游走，如果只用BFS附近游走，那么拿不到一个比较大集团内部的相似性。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621704181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283017,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1615519960,"is_pvip":false,"replies":[{"id":"102829","content":"出边指的是有向边中从当前点到其他点的边。<br><br>边就是无向图中的跟当前点相连的边了。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615793229,"ip_address":"","comment_id":283017,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1615519960","product_id":100060801,"comment_content":"你能尝试对比一下 Embedding 预训练和 Embedding End2End 训练这两种应用方法，说出它们之间的优缺点吗？<br>Embedding 预训练<br>优点：<br>1. 更容易实现，实现更快，且不需要标注数据；<br>缺点：<br>1. 难以收敛；<br>2. 较于End2End，预训练可能损失一些效果；<br><br>Embedding End2End 训练<br>优点：<br>1. 能够找到embedding层在这个模型结构下的最优解。<br>2. 收敛速度快；<br>缺点：<br>1. 需要标注数据，成本高；<br>2. End2End训练在实际的应用中没有发现效果有明显提高。<br><br>王老师，您好！~ 请问： <br>有向有权图：N+(v_i) 是节点 v_i所有的出边集合，这里的“出边”是指哪些边呢？<br>无向无权图：N+(v_i) 是节点 v_i所有的边集合，这里的 “边” 是指哪些边呢？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516910,"discussion_content":"出边指的是有向边中从当前点到其他点的边。\n\n边就是无向图中的跟当前点相连的边了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615793229,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357400,"discussion_content":"感谢老师回答^_^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615800955,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":356960,"discussion_content":"找到解释了哈：\n\n对于有向图，vi的邻接表中每个表结点都对应于以vi为始点射出的一条边。因此，将有向图的邻接表称为出边表。\n\n无向图的边集合应该是类似的概念。\n\n谢谢老师～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615714481,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268135,"user_name":"辛湛","can_delete":false,"product_type":"c1","uid":1043741,"ip_address":"","ucode":"493A8CDFD3FE83","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1d/bfd0a242.jpg","comment_is_top":false,"comment_ctime":1608082805,"is_pvip":false,"replies":[{"id":"97384","content":"怎么能确定更新后特征分布还是相似的，这个问题没听明白。<br><br>还有 尤其是特征是打印的， 打印是什么意思？","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608100739,"ip_address":"","comment_id":268135,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608082805","product_id":100060801,"comment_content":"老师问一下一般word2vec这种都是预训练作为特征用到推荐精排或粗排模型的,那么word2vec如何更新呢 怎么能确定更新后特征分布还是相似的。尤其是特征是打印的，是否要进行离线的特征回滚?谢谢","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511855,"discussion_content":"怎么能确定更新后特征分布还是相似的，这个问题没听明白。\n\n还有 尤其是特征是打印的， 打印是什么意思？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608100739,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":254010,"user_name":"叫我小邬就好了","can_delete":false,"product_type":"c1","uid":2198325,"ip_address":"","ucode":"5FA8C6DE790F50","user_header":"https://static001.geekbang.org/account/avatar/00/21/8b/35/a09d4efe.jpg","comment_is_top":false,"comment_ctime":1603014648,"is_pvip":false,"replies":[{"id":"92822","content":"关注下节课","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603094390,"ip_address":"","comment_id":254010,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1603014648","product_id":100060801,"comment_content":"王老师，请问有这些Embedding的代码实例吗，","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507250,"discussion_content":"关注下节课","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603094390,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}