{"id":224795,"title":"02 | 日志（上）：日志究竟是如何加载日志段的？","content":"<p>你好，我是胡夕。今天我来讲讲Kafka源码的日志（Log）对象。</p><p>上节课，我们学习了日志段部分的源码，你可以认为，<strong>日志是日志段的容器，里面定义了很多管理日志段的操作</strong>。坦率地说，如果看Kafka源码却不看Log，就跟你买了这门课却不知道作者是谁一样。在我看来，Log对象是Kafka源码（特别是Broker端）最核心的部分，没有之一。</p><p>它到底有多重要呢？我和你分享一个例子，你先感受下。我最近正在修复一个Kafka的Bug（<a href=\"https://issues.apache.org/jira/browse/KAFKA-9157\">KAFKA-9157</a>）：在某些情况下，Kafka的Compaction操作会产生很多空的日志段文件。如果要避免这些空日志段文件被创建出来，就必须搞懂创建日志段文件的原理，而这些代码恰恰就在Log源码中。</p><p>既然Log源码要管理日志段对象，那么它就必须先把所有日志段对象加载到内存里面。这个过程是怎么实现的呢？今天，我就带你学习下日志加载日志段的过程。</p><p>首先，我们来看下Log对象的源码结构。</p><h2>Log源码结构</h2><p>Log源码位于Kafka core工程的log源码包下，文件名是Log.scala。总体上，该文件定义了10个类和对象，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/81/ce/8126a191f63d9abea860d71992b0aece.jpg?wh=863*606\" alt=\"\"></p><p>那么，这10个类和对象都是做什么的呢？我先给你简单介绍一下，你可以对它们有个大致的了解。</p><!-- [[[read_end]]] --><p>不过，在介绍之前，我先提一句，图中括号里的C表示Class，O表示Object。还记得我在上节课提到过的伴生对象吗？没错，同时定义同名的Class和Object，就属于Scala中的伴生对象用法。</p><p>我们先来看伴生对象，也就是LogAppendInfo、Log和RollParams。</p><p><strong>1.LogAppendInfo</strong></p><ul>\n<li>LogAppendInfo（C）：保存了一组待写入消息的各种元数据信息。比如，这组消息中第一条消息的位移值是多少、最后一条消息的位移值是多少；再比如，这组消息中最大的消息时间戳又是多少。总之，这里面的数据非常丰富（下节课我再具体说说）。</li>\n<li>LogAppendInfo（O）: 可以理解为其对应伴生类的工厂方法类，里面定义了一些工厂方法，用于创建特定的LogAppendInfo实例。</li>\n</ul><p><strong>2.Log</strong></p><ul>\n<li>Log（C）: Log源码中最核心的代码。这里我先卖个关子，一会儿细聊。</li>\n<li>Log（O）：同理，Log伴生类的工厂方法，定义了很多常量以及一些辅助方法。</li>\n</ul><p><strong>3.RollParams</strong></p><ul>\n<li>RollParams（C）：定义用于控制日志段是否切分（Roll）的数据结构。</li>\n<li>RollParams（O）：同理，RollParams伴生类的工厂方法。</li>\n</ul><p>除了这3组伴生对象之外，还有4类源码。</p><ul>\n<li>LogMetricNames：定义了Log对象的监控指标。</li>\n<li>LogOffsetSnapshot：封装分区所有位移元数据的容器类。</li>\n<li>LogReadInfo：封装读取日志返回的数据及其元数据。</li>\n<li>CompletedTxn：记录已完成事务的元数据，主要用于构建事务索引。</li>\n</ul><h2>Log Class &amp; Object</h2><p>下面，我会按照这些类和对象的重要程度，对它们一一进行拆解。首先，咱们先说说Log类及其伴生对象。</p><p>考虑到伴生对象多用于保存静态变量和静态方法（比如静态工厂方法等），因此我们先看伴生对象（即Log Object）的实现。毕竟，柿子先找软的捏！</p><pre><code>object Log {\n  val LogFileSuffix = &quot;.log&quot;\n  val IndexFileSuffix = &quot;.index&quot;\n  val TimeIndexFileSuffix = &quot;.timeindex&quot;\n  val ProducerSnapshotFileSuffix = &quot;.snapshot&quot;\n  val TxnIndexFileSuffix = &quot;.txnindex&quot;\n  val DeletedFileSuffix = &quot;.deleted&quot;\n  val CleanedFileSuffix = &quot;.cleaned&quot;\n  val SwapFileSuffix = &quot;.swap&quot;\n  val CleanShutdownFile = &quot;.kafka_cleanshutdown&quot;\n  val DeleteDirSuffix = &quot;-delete&quot;\n  val FutureDirSuffix = &quot;-future&quot;\n……\n}\n</code></pre><p>这是Log Object定义的所有常量。如果有面试官问你Kafka中定义了多少种文件类型，你可以自豪地把这些说出来。耳熟能详的.log、.index、.timeindex和.txnindex我就不解释了，我们来了解下其他几种文件类型。</p><ul>\n<li>.snapshot是Kafka为幂等型或事务型Producer所做的快照文件。鉴于我们现在还处于阅读源码的初级阶段，事务或幂等部分的源码我就不详细展开讲了。</li>\n<li>.deleted是删除日志段操作创建的文件。目前删除日志段文件是异步操作，Broker端把日志段文件从.log后缀修改为.deleted后缀。如果你看到一大堆.deleted后缀的文件名，别慌，这是Kafka在执行日志段文件删除。</li>\n<li>.cleaned和.swap都是Compaction操作的产物，等我们讲到Cleaner的时候再说。</li>\n<li>-delete则是应用于文件夹的。当你删除一个主题的时候，主题的分区文件夹会被加上这个后缀。</li>\n<li>-future是用于变更主题分区文件夹地址的，属于比较高阶的用法。</li>\n</ul><p>总之，记住这些常量吧。记住它们的主要作用是，以后不要被面试官唬住！开玩笑，其实这些常量最重要的地方就在于，它们能够让你了解Kafka定义的各种文件类型。</p><p>Log Object还定义了超多的工具类方法。由于它们都很简单，这里我只给出一个方法的源码，我们一起读一下。</p><pre><code>def filenamePrefixFromOffset(offset: Long): String = {\n    val nf = NumberFormat.getInstance()\n    nf.setMinimumIntegerDigits(20)\n    nf.setMaximumFractionDigits(0)\n    nf.setGroupingUsed(false)\n    nf.format(offset)\n  }\n</code></pre><p>这个方法的作用是<strong>通过给定的位移值计算出对应的日志段文件名</strong>。Kafka日志文件固定是20位的长度，filenamePrefixFromOffset方法就是用前面补0的方式，把给定位移值扩充成一个固定20位长度的字符串。</p><p>举个例子，我们给定一个位移值是12345，那么Broker端磁盘上对应的日志段文件名就应该是00000000000000012345.log。怎么样，很简单吧？其他的工具类方法也很简单，我就不一一展开说了。</p><p>下面我们来看Log源码部分的重头戏：<strong>Log类</strong>。这是一个2000多行的大类。放眼整个Kafka源码，像Log这么大的类也不多见，足见它的重要程度。我们先来看这个类的定义：</p><pre><code>class Log(@volatile var dir: File,\n          @volatile var config: LogConfig,\n          @volatile var logStartOffset: Long,\n          @volatile var recoveryPoint: Long,\n          scheduler: Scheduler,\n          brokerTopicStats: BrokerTopicStats,\n          val time: Time,\n          val maxProducerIdExpirationMs: Int,\n          val producerIdExpirationCheckIntervalMs: Int,\n          val topicPartition: TopicPartition,\n          val producerStateManager: ProducerStateManager,\n          logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup {\n……\n}\n</code></pre><p>看着好像有很多属性，但其实，你只需要记住两个属性的作用就够了：<strong>dir和logStartOffset</strong>。dir就是这个日志所在的文件夹路径，也就是<strong>主题分区的路径</strong>。而logStartOffset，表示<strong>日志的当前最早位移</strong>。dir和logStartOffset都是volatile var类型，表示它们的值是变动的，而且可能被多个线程更新。</p><p>你可能听过日志的当前末端位移，也就是Log End Offset（LEO），它是表示日志下一条待插入消息的位移值，而这个Log Start Offset是跟它相反的，它表示日志当前对外可见的最早一条消息的位移值。我用一张图来标识它们的区别：</p><p><img src=\"https://static001.geekbang.org/resource/image/38/b4/388672f6dab8571f272ed47c9679c2b4.jpg?wh=4000*2250\" alt=\"\"></p><p>图中绿色的位移值3是日志的Log Start Offset，而位移值15表示LEO。另外，位移值8是高水位值，它是区分已提交消息和未提交消息的分水岭。</p><p>有意思的是，Log End Offset可以简称为LEO，但Log Start Offset却不能简称为LSO。因为在Kafka中，LSO特指Log Stable Offset，属于Kafka事务的概念。这个课程中不会涉及LSO，你只需要知道Log Start Offset不等于LSO即可。</p><p>Log类的其他属性你暂时不用理会，因为它们要么是很明显的工具类属性，比如timer和scheduler，要么是高阶用法才会用到的高级属性，比如producerStateManager和logDirFailureChannel。工具类的代码大多是做辅助用的，跳过它们也不妨碍我们理解Kafka的核心功能；而高阶功能代码设计复杂，学习成本高，性价比不高。</p><p>其实，除了Log类签名定义的这些属性之外，Log类还定义了一些很重要的属性，比如下面这段代码：</p><pre><code>    @volatile private var nextOffsetMetadata: LogOffsetMetadata = _\n    @volatile private var highWatermarkMetadata: LogOffsetMetadata = LogOffsetMetadata(logStartOffset)\n    private val segments: ConcurrentNavigableMap[java.lang.Long, LogSegment] = new ConcurrentSkipListMap[java.lang.Long, LogSegment]\n    @volatile var leaderEpochCache: Option[LeaderEpochFileCache] = None\n</code></pre><p>第一个属性nextOffsetMetadata，它封装了下一条待插入消息的位移值，你基本上可以把这个属性和LEO等同起来。</p><p>第二个属性highWatermarkMetadata，是分区日志高水位值。关于高水位的概念，我们在<a href=\"https://time.geekbang.org/column/intro/100029201\">《Kafka核心技术与实战》</a>这个课程中做过详细解释，你可以看一下<a href=\"https://time.geekbang.org/column/article/112118\">这篇文章</a>（下节课我还会再具体给你介绍下）。</p><p>第三个属性segments，我认为这是Log类中最重要的属性。它保存了分区日志下所有的日志段信息，只不过是用Map的数据结构来保存的。Map的Key值是日志段的起始位移值，Value则是日志段对象本身。Kafka源码使用ConcurrentNavigableMap数据结构来保存日志段对象，就可以很轻松地利用该类提供的线程安全和各种支持排序的方法，来管理所有日志段对象。</p><p>第四个属性是Leader Epoch Cache对象。Leader Epoch是社区于0.11.0.0版本引入源码中的，主要是用来判断出现Failure时是否执行日志截断操作（Truncation）。之前靠高水位来判断的机制，可能会造成副本间数据不一致的情形。这里的Leader Epoch Cache是一个缓存类数据，里面保存了分区Leader的Epoch值与对应位移值的映射关系，我建议你查看下LeaderEpochFileCache类，深入地了解下它的实现原理。</p><p>掌握了这些基本属性之后，我们看下Log类的初始化逻辑：</p><pre><code> locally {\n        val startMs = time.milliseconds\n    \n    \n        // create the log directory if it doesn't exist\n        Files.createDirectories(dir.toPath)\n    \n    \n        initializeLeaderEpochCache()\n    \n    \n        val nextOffset = loadSegments()\n    \n    \n        /* Calculate the offset of the next message */\n        nextOffsetMetadata = LogOffsetMetadata(nextOffset, activeSegment.baseOffset, activeSegment.size)\n    \n    \n        leaderEpochCache.foreach(_.truncateFromEnd(nextOffsetMetadata.messageOffset))\n    \n    \n        logStartOffset = math.max(logStartOffset, segments.firstEntry.getValue.baseOffset)\n    \n    \n        // The earliest leader epoch may not be flushed during a hard failure. Recover it here.\n        leaderEpochCache.foreach(_.truncateFromStart(logStartOffset))\n    \n    \n        // Any segment loading or recovery code must not use producerStateManager, so that we can build the full state here\n        // from scratch.\n        if (!producerStateManager.isEmpty)\n          throw new IllegalStateException(&quot;Producer state must be empty during log initialization&quot;)\n        loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)\n    \n    \n        info(s&quot;Completed load of log with ${segments.size} segments, log start offset $logStartOffset and &quot; +\n          s&quot;log end offset $logEndOffset in ${time.milliseconds() - startMs} \n</code></pre><p>在详细解释这段初始化代码之前，我使用一张图来说明它到底做了什么：</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/a8/a10b81680a449e5b1d8882939061f7a8.jpg?wh=2284*1285\" alt=\"\"></p><p>这里我们重点说说第三步，即加载日志段的实现逻辑，以下是loadSegments的实现代码：</p><pre><code> private def loadSegments(): Long = {\n        // first do a pass through the files in the log directory and remove any temporary files\n        // and find any interrupted swap operations\n        val swapFiles = removeTempFilesAndCollectSwapFiles()\n    \n    \n        // Now do a second pass and load all the log and index files.\n        // We might encounter legacy log segments with offset overflow (KAFKA-6264). We need to split such segments. When\n        // this happens, restart loading segment files from scratch.\n        retryOnOffsetOverflow {\n          // In case we encounter a segment with offset overflow, the retry logic will split it after which we need to retry\n          // loading of segments. In that case, we also need to close all segments that could have been left open in previous\n          // call to loadSegmentFiles().\n          logSegments.foreach(_.close())\n          segments.clear()\n          loadSegmentFiles()\n        }\n    \n    \n        // Finally, complete any interrupted swap operations. To be crash-safe,\n        // log files that are replaced by the swap segment should be renamed to .deleted\n        // before the swap file is restored as the new segment file.\n        completeSwapOperations(swapFiles)\n    \n    \n        if (!dir.getAbsolutePath.endsWith(Log.DeleteDirSuffix)) {\n          val nextOffset = retryOnOffsetOverflow {\n            recoverLog()\n          }\n    \n    \n          // reset the index size of the currently active log segment to allow more entries\n          activeSegment.resizeIndexes(config.maxIndexSize)\n          nextOffset\n        } else {\n           if (logSegments.isEmpty) {\n              addSegment(LogSegment.open(dir = dir,\n                baseOffset = 0,\n                config,\n                time = time,\n                fileAlreadyExists = false,\n                initFileSize = this.initFileSize,\n                preallocate = false))\n           }\n          0\n        }\n\n</code></pre><p>这段代码会对分区日志路径遍历两次。</p><p>首先，它会移除上次Failure遗留下来的各种临时文件（包括.cleaned、.swap、.deleted文件等），removeTempFilesAndCollectSwapFiles方法实现了这个逻辑。</p><p>之后，它会清空所有日志段对象，并且再次遍历分区路径，重建日志段segments Map并删除无对应日志段文件的孤立索引文件。</p><p>待执行完这两次遍历之后，它会完成未完成的swap操作，即调用completeSwapOperations方法。等这些都做完之后，再调用recoverLog方法恢复日志段对象，然后返回恢复之后的分区日志LEO值。</p><p>如果你现在觉得有点蒙，也没关系，我把这段代码再进一步拆解下，以更小的粒度跟你讲下它们做了什么。理解了这段代码之后，你大致就能搞清楚大部分的分区日志操作了。所以，这部分代码绝对值得我们多花一点时间去学习。</p><p>我们首先来看第一步，removeTempFilesAndCollectSwapFiles方法的实现。我用注释的方式详细解释了每行代码的作用：</p><pre><code> private def removeTempFilesAndCollectSwapFiles(): Set[File] = {\n    \n    // 在方法内部定义一个名为deleteIndicesIfExist的方法，用于删除日志文件对应的索引文件\n    \n    def deleteIndicesIfExist(baseFile: File, suffix: String = &quot;&quot;): Unit = {\n    \n    info(s&quot;Deleting index files with suffix $suffix for baseFile $baseFile&quot;)\n    \n    val offset = offsetFromFile(baseFile)\n    \n    Files.deleteIfExists(Log.offsetIndexFile(dir, offset, suffix).toPath)\n    \n    Files.deleteIfExists(Log.timeIndexFile(dir, offset, suffix).toPath)\n    \n    Files.deleteIfExists(Log.transactionIndexFile(dir, offset, suffix).toPath)\n    \n    }\n    \n    var swapFiles = Set[File]()\n    \n    var cleanFiles = Set[File]()\n    \n    var minCleanedFileOffset = Long.MaxValue\n    \n    // 遍历分区日志路径下的所有文件\n    \n    for (file &lt;- dir.listFiles if file.isFile) {\n    \n    if (!file.canRead) // 如果不可读，直接抛出IOException\n    \n    throw new IOException(s&quot;Could not read file $file&quot;)\n    \n    val filename = file.getName\n    \n    if (filename.endsWith(DeletedFileSuffix)) { // 如果以.deleted结尾\n    \n    debug(s&quot;Deleting stray temporary file ${file.getAbsolutePath}&quot;)\n    \n    Files.deleteIfExists(file.toPath) // 说明是上次Failure遗留下来的文件，直接删除\n    \n    } else if (filename.endsWith(CleanedFileSuffix)) { // 如果以.cleaned结尾\n    \n    minCleanedFileOffset = Math.min(offsetFromFileName(filename), minCleanedFileOffset) // 选取文件名中位移值最小的.cleaned文件，获取其位移值，并将该文件加入待删除文件集合中\n    \n    cleanFiles += file\n    \n    } else if (filename.endsWith(SwapFileSuffix)) { // 如果以.swap结尾\n    \n    val baseFile = new File(CoreUtils.replaceSuffix(file.getPath, SwapFileSuffix, &quot;&quot;))\n    \n    info(s&quot;Found file ${file.getAbsolutePath} from interrupted swap operation.&quot;)\n    \n    if (isIndexFile(baseFile)) { // 如果该.swap文件原来是索引文件\n    \n    deleteIndicesIfExist(baseFile) // 删除原来的索引文件\n    \n    } else if (isLogFile(baseFile)) { // 如果该.swap文件原来是日志文件\n    \n    deleteIndicesIfExist(baseFile) // 删除掉原来的索引文件\n    \n    swapFiles += file // 加入待恢复的.swap文件集合中\n    \n    }\n    \n    }\n    \n    }\n    \n    // 从待恢复swap集合中找出那些起始位移值大于minCleanedFileOffset值的文件，直接删掉这些无效的.swap文件\n    \n    val (invalidSwapFiles, validSwapFiles) = swapFiles.partition(file =&gt; offsetFromFile(file) &gt;= minCleanedFileOffset)\n    \n    invalidSwapFiles.foreach { file =&gt;\n    \n    debug(s&quot;Deleting invalid swap file ${file.getAbsoluteFile} minCleanedFileOffset: $minCleanedFileOffset&quot;)\n    \n    val baseFile = new File(CoreUtils.replaceSuffix(file.getPath, SwapFileSuffix, &quot;&quot;))\n    \n    deleteIndicesIfExist(baseFile, SwapFileSuffix)\n    \n    Files.deleteIfExists(file.toPath)\n    \n    }\n    \n    // Now that we have deleted all .swap files that constitute an incomplete split operation, let's delete all .clean files\n    \n    // 清除所有待删除文件集合中的文件\n    \n    cleanFiles.foreach { file =&gt;\n    \n    debug(s&quot;Deleting stray .clean file ${file.getAbsolutePath}&quot;)\n    \n    Files.deleteIfExists(file.toPath)\n    \n    }\n    \n    // 最后返回当前有效的.swap文件集合\n    \n    validSwapFiles\n    \n    }\n</code></pre><p>执行完了removeTempFilesAndCollectSwapFiles逻辑之后，源码开始清空已有日志段集合，并重新加载日志段文件。这就是第二步。这里调用的主要方法是loadSegmentFiles。</p><pre><code>   private def loadSegmentFiles(): Unit = {\n    \n    // 按照日志段文件名中的位移值正序排列，然后遍历每个文件\n    \n    for (file &lt;- dir.listFiles.sortBy(_.getName) if file.isFile) {\n    \n    if (isIndexFile(file)) { // 如果是索引文件\n    \n    val offset = offsetFromFile(file)\n    \n    val logFile = Log.logFile(dir, offset)\n    \n    if (!logFile.exists) { // 确保存在对应的日志文件，否则记录一个警告，并删除该索引文件\n    \n    warn(s&quot;Found an orphaned index file ${file.getAbsolutePath}, with no corresponding log file.&quot;)\n    \n    Files.deleteIfExists(file.toPath)\n    \n    }\n    \n    } else if (isLogFile(file)) { // 如果是日志文件\n    \n    val baseOffset = offsetFromFile(file)\n    \n    val timeIndexFileNewlyCreated = !Log.timeIndexFile(dir, baseOffset).exists()\n    \n    // 创建对应的LogSegment对象实例，并加入segments中\n    \n    val segment = LogSegment.open(dir = dir,\n    \n    baseOffset = baseOffset,\n    \n    config,\n    \n    time = time,\n    \n    fileAlreadyExists = true)\n    \n    try segment.sanityCheck(timeIndexFileNewlyCreated)\n    \n    catch {\n    \n    case _: NoSuchFileException =&gt;\n    \n    error(s&quot;Could not find offset index file corresponding to log file ${segment.log.file.getAbsolutePath}, &quot; +\n    \n    &quot;recovering segment and rebuilding index files...&quot;)\n    \n    recoverSegment(segment)\n    \n    case e: CorruptIndexException =&gt;\n    \n    warn(s&quot;Found a corrupted index file corresponding to log file ${segment.log.file.getAbsolutePath} due &quot; +\n    \n    s&quot;to ${e.getMessage}}, recovering segment and rebuilding index files...&quot;)\n    \n    recoverSegment(segment)\n    \n    }\n    \n    addSegment(segment)\n    \n    }\n    \n    }\n    \n    }\n\n</code></pre><p>第三步是处理第一步返回的有效.swap文件集合。completeSwapOperations方法就是做这件事的：</p><pre><code>  private def completeSwapOperations(swapFiles: Set[File]): Unit = {\n    \n    // 遍历所有有效.swap文件\n    \n    for (swapFile &lt;- swapFiles) {\n    \n    val logFile = new File(CoreUtils.replaceSuffix(swapFile.getPath, SwapFileSuffix, &quot;&quot;)) // 获取对应的日志文件\n    \n    val baseOffset = offsetFromFile(logFile) // 拿到日志文件的起始位移值\n    \n    // 创建对应的LogSegment实例\n    \n    val swapSegment = LogSegment.open(swapFile.getParentFile,\n    \n    baseOffset = baseOffset,\n    \n    config,\n    \n    time = time,\n    \n    fileSuffix = SwapFileSuffix)\n    \n    info(s&quot;Found log file ${swapFile.getPath} from interrupted swap operation, repairing.&quot;)\n    \n    // 执行日志段恢复操作\n    \n    recoverSegment(swapSegment)\n    \n    // We create swap files for two cases:\n    \n    // (1) Log cleaning where multiple segments are merged into one, and\n    \n    // (2) Log splitting where one segment is split into multiple.\n    \n    //\n    \n    // Both of these mean that the resultant swap segments be composed of the original set, i.e. the swap segment\n    \n    // must fall within the range of existing segment(s). If we cannot find such a segment, it means the deletion\n    \n    // of that segment was successful. In such an event, we should simply rename the .swap to .log without having to\n    \n    // do a replace with an existing segment.\n    \n    // 确认之前删除日志段是否成功，是否还存在老的日志段文件\n    \n    val oldSegments = logSegments(swapSegment.baseOffset, swapSegment.readNextOffset).filter { segment =&gt;\n    \n    segment.readNextOffset &gt; swapSegment.baseOffset\n    \n    }\n    \n    // 将生成的.swap文件加入到日志中，删除掉swap之前的日志段\n    \n    replaceSegments(Seq(swapSegment), oldSegments.toSeq, isRecoveredSwapFile = true)\n    \n    }\n    \n    }\n\n</code></pre><p>最后一步是recoverLog操作：</p><pre><code> private def recoverLog(): Long = {\n        // if we have the clean shutdown marker, skip recovery\n        // 如果不存在以.kafka_cleanshutdown结尾的文件。通常都不存在\n        if (!hasCleanShutdownFile) {\n          // 获取到上次恢复点以外的所有unflushed日志段对象\n          val unflushed = logSegments(this.recoveryPoint, Long.MaxValue).toIterator\n          var truncated = false\n    \n    \n          // 遍历这些unflushed日志段\n          while (unflushed.hasNext &amp;&amp; !truncated) {\n            val segment = unflushed.next\n            info(s&quot;Recovering unflushed segment ${segment.baseOffset}&quot;)\n            val truncatedBytes =\n              try {\n                // 执行恢复日志段操作\n                recoverSegment(segment, leaderEpochCache)\n              } catch {\n                case _: InvalidOffsetException =&gt;\n                  val startOffset = segment.baseOffset\n                  warn(&quot;Found invalid offset during recovery. Deleting the corrupt segment and &quot; +\n                    s&quot;creating an empty one with starting offset $startOffset&quot;)\n                  segment.truncateTo(startOffset)\n              }\n            if (truncatedBytes &gt; 0) { // 如果有无效的消息导致被截断的字节数不为0，直接删除剩余的日志段对象\n              warn(s&quot;Corruption found in segment ${segment.baseOffset}, truncating to offset ${segment.readNextOffset}&quot;)\n              removeAndDeleteSegments(unflushed.toList, asyncDelete = true)\n              truncated = true\n            }\n          }\n        }\n    \n    \n        // 这些都做完之后，如果日志段集合不为空\n        if (logSegments.nonEmpty) {\n          val logEndOffset = activeSegment.readNextOffset\n          if (logEndOffset &lt; logStartOffset) { // 验证分区日志的LEO值不能小于Log Start Offset值，否则删除这些日志段对象\n            warn(s&quot;Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). &quot; +\n              &quot;This could happen if segment files were deleted from the file system.&quot;)\n            removeAndDeleteSegments(logSegments, asyncDelete = true)\n          }\n        }\n    \n    \n        // 这些都做完之后，如果日志段集合为空了\n        if (logSegments.isEmpty) {\n        // 至少创建一个新的日志段，以logStartOffset为日志段的起始位移，并加入日志段集合中\n          addSegment(LogSegment.open(dir = dir,\n            baseOffset = logStartOffset,\n            config,\n            time = time,\n            fileAlreadyExists = false,\n            initFileSize = this.initFileSize,\n            preallocate = config.preallocate))\n        }\n    \n    \n        // 更新上次恢复点属性，并返回\n        recoveryPoint = activeSegment.readNextOffset\n        recoveryPoint\n</code></pre><h2>总结</h2><p>今天，我重点向你介绍了Kafka的Log源码，主要包括：</p><ol>\n<li><strong>Log文件的源码结构</strong>：你可以看下下面的导图，它展示了Log类文件的架构组成，你要重点掌握Log类及其相关方法。</li>\n<li><strong>加载日志段机制</strong>：我结合源码重点分析了日志在初始化时是如何加载日志段的。前面说过了，日志是日志段的容器，弄明白如何加载日志段是后续学习日志段管理的前提条件。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/dd/fc/dd2bf4882021d969accb14c0017d9dfc.jpg?wh=2543*3019\" alt=\"\"></p><p>总的来说，虽然洋洋洒洒几千字，但我也只讲了最重要的部分。我建议你多看几遍Log.scala中加载日志段的代码，这对后面我们理解Kafka Broker端日志段管理原理大有裨益。在下节课，我会继续讨论日志部分的源码，带你学习常见的Kafka日志操作。</p><h2>课后讨论</h2><p>Log源码中有个maybeIncrementHighWatermark方法，你能说说它的实现原理吗？</p><p>欢迎你在留言区畅所欲言，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p>","comments":[{"had_liked":false,"id":207936,"user_name":"曾轼麟","can_delete":false,"product_type":"c1","uid":1451391,"ip_address":"","ucode":"D418371AC11270","user_header":"https://static001.geekbang.org/account/avatar/00/16/25/7f/473d5a77.jpg","comment_is_top":true,"comment_ctime":1587219231,"is_pvip":false,"replies":[{"id":"77809","content":"👍👍👍","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587345825,"ip_address":"","comment_id":207936,"utype":1}],"discussion_count":4,"race_medal":0,"score":"9.2233720728016998e+18","product_id":100050101,"comment_content":"先回答老师的问题maybeIncrementHighWatermark的实现：<br>【首先需要注意以下几个内容】：<br>1、这个方法是通过leaderLog这个实例去调用的，当HW更新的时候follower就会更新自身的HW。<br><br>2、leaderLog 是在Partition.scala中的，是分区维度的概念。<br><br>3、maybeIncrementHighWatermark的入参是newHighWatermark，是新的HW标记，但是可能是更新也可能不是。<br><br>【下面来说实现】<br>在maybeIncrementHighWatermark中会先去判断新的newHighWatermark.messageOffset是否大于当前的LEO，如果大于肯定不合理，因为新的HW不可能跑在LEO前面<br><br>然后获取当前高水位的偏移量和元数据。如果偏移元数据不是，已知，将在索引中执行查找并缓存结果，并赋值给oldHighWatermark。<br><br>最后进行判断，如果（oldHighWatermark.messageOffset小于newHighWatermark.messageOffset）<br>则跟新HW的元数据<br><br>或者如果（oldHighWatermark.messageOffset 等于 newHighWatermark.messageOffset）并且当前segmentBaseOffset小于newHighWatermark.segmentBaseOffset<br>也会更新HW的元数据<br><br>最后这个过程是在synchronized的包围下进行的。","like_count":8,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492286,"discussion_content":"👍👍👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587345825,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1060632,"avatar":"https://static001.geekbang.org/account/avatar/00/10/2f/18/0c943f98.jpg","nickname":"秋林","note":"","ucode":"B5882D4EC73648","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":378690,"discussion_content":"哈哈哈没想到这里遇到你，厉害哦","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1623335707,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1451391,"avatar":"https://static001.geekbang.org/account/avatar/00/16/25/7f/473d5a77.jpg","nickname":"曾轼麟","note":"","ucode":"D418371AC11270","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1060632,"avatar":"https://static001.geekbang.org/account/avatar/00/10/2f/18/0c943f98.jpg","nickname":"秋林","note":"","ucode":"B5882D4EC73648","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385954,"discussion_content":"😂😂😂今天才看见，有一段时间没上了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627360609,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":378690,"ip_address":""},"score":385954,"extra":""}]},{"author":{"id":1231549,"avatar":"https://static001.geekbang.org/account/avatar/00/12/ca/bd/a51ae4b2.jpg","nickname":"吃饭饭","note":"","ucode":"95CFA07CDA2957","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":241766,"discussion_content":"硬核啊，Scala 看的头大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587439374,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209755,"user_name":"胡夕","can_delete":false,"product_type":"c1","uid":1288090,"ip_address":"","ucode":"5709A689B6683B","user_header":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","comment_is_top":true,"comment_ctime":1587609124,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"9.2233720384423997e+18","product_id":100050101,"comment_content":"你好，我是胡夕。我来公布上节课的“课后讨论”题答案啦～<br><br>上节课，咱们重点了解了日志段对象，课后我让你思考下如果给定位移值过大truncateTo方法的实现。关于这个问题，我的看法很简单。如果truncateTo的输入offset过大以至于超过了该日志段当前最大的消息位移值，那么这个方法不会执行任何截断操作，因为不会有机会调用log.truncateTo(mapping.position)<br><br>okay，你是怎么考虑的呢？我们可以一起讨论下。","like_count":0,"discussions":[{"author":{"id":1098550,"avatar":"https://static001.geekbang.org/account/avatar/00/10/c3/36/f73653c2.jpg","nickname":"陈阳","note":"","ucode":"7D87DD5F380659","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":344413,"discussion_content":"truncateTo方法的注释有答案，哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611457829,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":213012,"user_name":"花开成海","can_delete":false,"product_type":"c1","uid":1179974,"ip_address":"","ucode":"ED73A4ACFC5F72","user_header":"https://static001.geekbang.org/account/avatar/00/12/01/46/faf75ba6.jpg","comment_is_top":false,"comment_ctime":1588261170,"is_pvip":true,"replies":[{"id":"79131","content":"1. 这些写的确实有点问题。应该是删除孤立的索引文件<br>2. 应该说是JVM GC机制：） FileRocords表示日志段对象的底层物理文件，已加载完成的日志段对象对应的FileRecords是可以被GC的。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588400059,"ip_address":"","comment_id":213012,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10178195762","product_id":100050101,"comment_content":"有两个疑问请老师帮忙看下：<br>1、为什么broker重启要重建所有的索引文件？<br>2、LogSegment的log: FileRecords 表示的是消息内容，加载日志段时候，什么机制使有限内存加载所有segment？","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493668,"discussion_content":"1. 这些写的确实有点问题。应该是删除孤立的索引文件\n2. 应该说是JVM GC机制：） FileRocords表示日志段对象的底层物理文件，已加载完成的日志段对象对应的FileRecords是可以被GC的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588400059,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1348848,"avatar":"https://static001.geekbang.org/account/avatar/00/14/94/f0/54dffdba.jpg","nickname":"ZenHao","note":"","ucode":"324E87C83E7641","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384847,"discussion_content":" 源代码里面确实是重建了索引哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626771075,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207052,"user_name":"北纬8℃","can_delete":false,"product_type":"c1","uid":1433189,"ip_address":"","ucode":"4999F2E037C93C","user_header":"https://static001.geekbang.org/account/avatar/00/15/de/65/51147fb6.jpg","comment_is_top":false,"comment_ctime":1587000264,"is_pvip":false,"replies":[{"id":"77309","content":"哈哈，坚持坚持！","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587002009,"ip_address":"","comment_id":207052,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10176934856","product_id":100050101,"comment_content":"太难了😂","like_count":2,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491993,"discussion_content":"哈哈，坚持坚持！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587002009,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1433189,"avatar":"https://static001.geekbang.org/account/avatar/00/15/de/65/51147fb6.jpg","nickname":"北纬8℃","note":"","ucode":"4999F2E037C93C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":234886,"discussion_content":"想要debug看，尝试失败了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587003858,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312935,"user_name":"Geek_47baf9","can_delete":false,"product_type":"c1","uid":1793038,"ip_address":"","ucode":"DCF187338050DC","user_header":"","comment_is_top":false,"comment_ctime":1632128706,"is_pvip":false,"replies":[{"id":"114445","content":"是啊，感慨Kafka代码依然在保持活力地更新着","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1634004784,"ip_address":"","comment_id":312935,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5927096002","product_id":100050101,"comment_content":"在trunk分支提交记录（commitId=db1f581da7f3440cfd5be93800b4a9a2d7327a35）上 Log.scala已经在2021-08-13 7:10被重命名为UnifiedLog.scala，希望大家看专栏的时候注意点","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527167,"discussion_content":"是啊，感慨Kafka代码依然在保持活力地更新着","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634004784,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1107305,"avatar":"https://static001.geekbang.org/account/avatar/00/10/e5/69/719ec5d0.jpg","nickname":"Jian","note":"","ucode":"17ED4919F22DEC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":564025,"discussion_content":"正常来说应该给个具体的稳定的分支而不是trunk分支，后面来得同学看起来非常痛苦，最新的代码重构了很多内容，跟专栏讲的完全不一样了。其实可以看下redis专栏作者，先主线串起来，后面再讲旁路。一上来就讲某一点的代码让很多同学会看不下去的。说实话，这专栏质量不行。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1650127837,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291710,"user_name":"你瞅啥？没见过这么帅的哈士奇吗","can_delete":false,"product_type":"c1","uid":1127288,"ip_address":"","ucode":"16BA6EF8F8A86C","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/78/cf112171.jpg","comment_is_top":false,"comment_ctime":1620443781,"is_pvip":false,"replies":[{"id":"105692","content":"broker碰到的任何失败","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1620613699,"ip_address":"","comment_id":291710,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5915411077","product_id":100050101,"comment_content":"清除上一次failure留下的文件，这个failure情况是指什么情况下发生的failure情况，是节点挂了重启时的failure还是其他情况，能举个🌰吗","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519546,"discussion_content":"broker碰到的任何失败","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620613699,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":254518,"user_name":"云超","can_delete":false,"product_type":"c1","uid":1581783,"ip_address":"","ucode":"63DFDDAB37A04F","user_header":"https://static001.geekbang.org/account/avatar/00/18/22/d7/ae0ed31f.jpg","comment_is_top":false,"comment_ctime":1603121956,"is_pvip":false,"replies":[{"id":"93056","content":"类似于java中的静态代码块，多用于类或Object初始化执行一段代码之用","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1603354449,"ip_address":"","comment_id":254518,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5898089252","product_id":100050101,"comment_content":"老师，问一个语法上的问题，在上面的代码片段中的locally{xxx}  ，这个locally的作用是什么啊？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507424,"discussion_content":"类似于java中的静态代码块，多用于类或Object初始化执行一段代码之用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603354449,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1581783,"avatar":"https://static001.geekbang.org/account/avatar/00/18/22/d7/ae0ed31f.jpg","nickname":"云超","note":"","ucode":"63DFDDAB37A04F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":316087,"discussion_content":"好的，明白了，谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603357048,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":236857,"user_name":"张子涵","can_delete":false,"product_type":"c1","uid":1743397,"ip_address":"","ucode":"57C509EA32B916","user_header":"https://static001.geekbang.org/account/avatar/00/1a/9a/25/3e5e942b.jpg","comment_is_top":false,"comment_ctime":1595568941,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5890536237","product_id":100050101,"comment_content":" def maybeIncrementHighWatermark(newHighWatermark: LogOffsetMetadata): Option[LogOffsetMetadata] = {<br>    if (newHighWatermark.messageOffset &gt; logEndOffset)  &#47;&#47;对高水位的值进行判断，如果new高水位大于LEO，则报错<br>      throw new IllegalArgumentException(s&quot;High watermark $newHighWatermark update exceeds current &quot; +<br>        s&quot;log end offset $logEndOffsetMetadata&quot;)<br>    lock.synchronized {<br>      val oldHighWatermark = fetchHighWatermarkMetadata<br>      if (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset ||<br>        (oldHighWatermark.messageOffset == newHighWatermark.messageOffset &amp;&amp; oldHighWatermark.onOlderSegment(newHighWatermark))) {<br>        &#47;&#47;若old高水位offset小于new高水位offset<br>        &#47;&#47; 或者（old高水位offset等于new高水位offset，且当前segmentBaseOffset小于new高水位的segmentBaseOffset）<br>        &#47;&#47;则更新高水位值<br>        updateHighWatermarkMetadata(newHighWatermark)<br>        Some(oldHighWatermark)<br>      } else {<br>        None<br>      }<br>    }<br>  }","like_count":1},{"had_liked":false,"id":211082,"user_name":"东风第一枝","can_delete":false,"product_type":"c1","uid":1402697,"ip_address":"","ucode":"0CD0F62E90DAD8","user_header":"https://static001.geekbang.org/account/avatar/00/15/67/49/864dba17.jpg","comment_is_top":false,"comment_ctime":1587893017,"is_pvip":false,"replies":[{"id":"78564","content":"首先我要声明，源码写得也不一定就是对的！我们完全可以针对源码中可能的问题开放讨论哈。<br><br>在LeaderEpochFileCache中，针对epochs的更新和赋值全部都在write lock下，读取epoch用read lock。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587951426,"ip_address":"","comment_id":211082,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882860313","product_id":100050101,"comment_content":"先回答老师的问题：<br>如果HW大于LEO，那么直接抛出IllegalArgumentException异常；<br>否则做如下操作：<br>1. 拿到HW的LogOffsetMetadata<br>2. 当存在以下情况的时候，更新HW，并且返回更新之前的HW<br>\ta. old HW的位移小于new HW<br>\tb. old HW的位移等于new HW, 并且old HW的基准位移小于new HW的基准位移，这种情况说明，new HW对应的segment是一个新的segment<br>3. 如果old HW的位移大于等于new HW，直接返回None<br><br>再请教老师一个问题，LeaderEpochFileCache中有这样一行代码：<br>private var epochs: ArrayBuffer[EpochEntry] = inWriteLock(lock) ......<br>其实就是初始化epochs，我理解这边是读leader-epoch-checkpoint文件，为什么要用inWriteLock写锁，而不是inReadLock。","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493192,"discussion_content":"首先我要声明，源码写得也不一定就是对的！我们完全可以针对源码中可能的问题开放讨论哈。\n\n在LeaderEpochFileCache中，针对epochs的更新和赋值全部都在write lock下，读取epoch用read lock。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587951426,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207943,"user_name":"曾轼麟","can_delete":false,"product_type":"c1","uid":1451391,"ip_address":"","ucode":"D418371AC11270","user_header":"https://static001.geekbang.org/account/avatar/00/16/25/7f/473d5a77.jpg","comment_is_top":false,"comment_ctime":1587220075,"is_pvip":false,"replies":[{"id":"77808","content":"1. 就我个人而言，我觉得也没有什么问题。我觉得作者更多是为了把不同逻辑进行了分组导致遍历多次<br>2. 可能造成Broker的崩溃，无法启动。因为我们公司有小伙伴这么干过：（<br>3. 对的，这样可以快速根据给定offset找到对应的一上一下日志段对象","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587345815,"ip_address":"","comment_id":207943,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5882187371","product_id":100050101,"comment_content":"老师下面我想问一下我的一些问题：<br>1、为什么要遍历两次文件路径呢？我看了一下，如果在删除的时候顺便去加载segment会有什么问题吗？这样是否可以提高加载效率呢？<br>2、我看了一下在removeTempFilesAndCollectSwapFiles方法中minCleanedFileOffset是从文件名filename上面读取的，如果我修改了文件名的offset大小会出现什么意想不到的情况呢？<br>3、我发现segments是使用ConcurrentNavigableMap，而这里的ConcurrentNavigableMap是使用JDK的ConcurrentSkipListMap，使用跳表的目的是为了方便使用offset范围查询segments中的对象吗？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492288,"discussion_content":"1. 就我个人而言，我觉得也没有什么问题。我觉得作者更多是为了把不同逻辑进行了分组导致遍历多次\n2. 可能造成Broker的崩溃，无法启动。因为我们公司有小伙伴这么干过：（\n3. 对的，这样可以快速根据给定offset找到对应的一上一下日志段对象","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587345815,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1085297,"avatar":"https://static001.geekbang.org/account/avatar/00/10/8f/71/29fb7bc2.jpg","nickname":"hgf","note":"","ucode":"A9649ECFDBD9E8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":247530,"discussion_content":"遍历一次解决不了问题，而不是简单的为了逻辑清晰！特别是清理文件过程中，只有遍历完成一遍才能知道minCleanedFileOffset(即哪些.swap文件该清理），如果在第二次遍历前不清理.swap文件，那么加载的有可能是有问题的数据。一次遍历，无法做到这点。而且还有一点，清理.swap所有前，是不能清理.cleaned文件的，万一这次broker又崩了就没法恢复了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587816904,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207569,"user_name":"我是小队长","can_delete":false,"product_type":"c1","uid":1334198,"ip_address":"","ucode":"E0687666EE2031","user_header":"https://static001.geekbang.org/account/avatar/00/14/5b/b6/f404b490.jpg","comment_is_top":false,"comment_ctime":1587110430,"is_pvip":false,"replies":[{"id":"77817","content":"当底层日志文件被删除或损坏的话就可能出现这种情况，因为无法读取文件去获取LEO了。你可以用2.0版本做个试验：<br>1. 发消息到分区日志<br>2. 使用Admin的DeleteRecords命令驱动Log start offset前进<br>3. 关闭Broker<br>4. 删除日志路径<br>5. 重启Broker","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587346641,"ip_address":"","comment_id":207569,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882077726","product_id":100050101,"comment_content":"   &#47;&#47; 这些都做完之后，如果日志段集合不为空<br>   &#47;&#47; 验证分区日志的LEO值不能小于Log Start Offset值，否则删除这些日志段对象<br><br><br>想问下什么时候才会出现这种情况呢？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492172,"discussion_content":"当底层日志文件被删除或损坏的话就可能出现这种情况，因为无法读取文件去获取LEO了。你可以用2.0版本做个试验：\n1. 发消息到分区日志\n2. 使用Admin的DeleteRecords命令驱动Log start offset前进\n3. 关闭Broker\n4. 删除日志路径\n5. 重启Broker","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587346641,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207563,"user_name":"我是小队长","can_delete":false,"product_type":"c1","uid":1334198,"ip_address":"","ucode":"E0687666EE2031","user_header":"https://static001.geekbang.org/account/avatar/00/14/5b/b6/f404b490.jpg","comment_is_top":false,"comment_ctime":1587109019,"is_pvip":false,"replies":[{"id":"77822","content":"首先，不管是否要过滤出符合条件的oldSegments，回复之后都要进行替换，这个你实际上这是因为升级Broker版本而做的防御性编程。在老的版本中，代码写入消息后并不会对位移值进行校验。因此log cleaner老代码可能写入一个非常大的位移值（大于Int.MAX_VALUE）。当broker升级后，这些日志段就不能正常被compact了，因为位移值越界了（新版本加入了位移校验）<br><br>代码需要去搜寻在swap start offset和swap LEO之间的所有日志段对象，但这还不够，还要保证这些日志段的LEO比swap的base offset大才行是同意的吧？否则","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587347824,"ip_address":"","comment_id":207563,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5882076315","product_id":100050101,"comment_content":"老师，找出需要恢复的swap，直接恢复完成不就行了么，为什么还有下面的操作呢？<br><br><br>&#47;&#47; We create swap files for two cases:<br>&#47;&#47; (1) Log cleaning where multiple segments are merged into one, and<br>&#47;&#47; (2) Log splitting where one segment is split into multiple.<br>&#47;&#47;<br>&#47;&#47; Both of these mean that the resultant swap segments be composed of the original set, i.e. the swap segment<br>&#47;&#47; must fall within the range of existing segment(s). If we cannot find such a segment, it means the deletion<br>&#47;&#47; of that segment was successful. In such an event, we should simply rename the .swap to .log without having to<br>&#47;&#47; do a replace with an existing segment.<br>&#47;&#47; 确认之前删除日志段是否成功，是否还存在老的日志段文件<br>val oldSegments = logSegments(swapSegment.baseOffset, swapSegment.readNextOffset).filter { segment =&gt;<br>segment.readNextOffset &gt; swapSegment.baseOffset<br>}<br>&#47;&#47; 如果存在，直接把.swap文件重命名成.log<br>replaceSegments(Seq(swapSegment), oldSegments.toSeq, isRecoveredSwapFile = ","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492169,"discussion_content":"首先，不管是否要过滤出符合条件的oldSegments，回复之后都要进行替换，这个你实际上这是因为升级Broker版本而做的防御性编程。在老的版本中，代码写入消息后并不会对位移值进行校验。因此log cleaner老代码可能写入一个非常大的位移值（大于Int.MAX_VALUE）。当broker升级后，这些日志段就不能正常被compact了，因为位移值越界了（新版本加入了位移校验）\n\n代码需要去搜寻在swap start offset和swap LEO之间的所有日志段对象，但这还不够，还要保证这些日志段的LEO比swap的base offset大才行是同意的吧？否则","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587347824,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":351854,"user_name":"。。。","can_delete":false,"product_type":"c1","uid":2556149,"ip_address":"","ucode":"715BEDC6917E9E","user_header":"https://static001.geekbang.org/account/avatar/00/27/00/f5/596e8e84.jpg","comment_is_top":false,"comment_ctime":1658231133,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1658231133","product_id":100050101,"comment_content":"恢复点是啥啊  文中好像没有提到老师<br>","like_count":0},{"had_liked":false,"id":327414,"user_name":"qgaye","can_delete":false,"product_type":"c1","uid":1702112,"ip_address":"","ucode":"E817609D4ED2D6","user_header":"https://static001.geekbang.org/account/avatar/00/19/f8/e0/d6e3cc8f.jpg","comment_is_top":false,"comment_ctime":1640109581,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1640109581","product_id":100050101,"comment_content":"&#47;&#47; 从待恢复swap集合中找出那些起始位移值大于minCleanedFileOffset值的文件，直接删掉这些无效的.swap文件<br>请问为什么要删除位移值大于minCleanedFileOffset的文件呢","like_count":0},{"had_liked":false,"id":292317,"user_name":"邓斌","can_delete":false,"product_type":"c1","uid":1162877,"ip_address":"","ucode":"548073840A21C9","user_header":"https://static001.geekbang.org/account/avatar/00/11/be/7d/40f6361e.jpg","comment_is_top":false,"comment_ctime":1620782048,"is_pvip":false,"replies":[{"id":"105882","content":"最好还是别放在windows上了，有各种各样的问题","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1620818948,"ip_address":"","comment_id":292317,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620782048","product_id":100050101,"comment_content":"你好，我的Windows10系统在启动源码的时候出现java.nio.file.AccessDeniedException异常，错误信息：Error while writing to checkpoint file C:\\tmp\\kafka\\kafka-logs\\recovery-point-offset-checkpoint (kafka.server.LogDirFailureChannel)和<br>Failed to create or validate data directory C:\\tmp\\kafka-logs (kafka.server.LogDirFailureChannel)。<br>我已经排查了目录和文件的权限都是完全控制，甚至重装系统都没有解决，请教一下，这个错误还有什么办法解决吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519774,"discussion_content":"最好还是别放在windows上了，有各种各样的问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620818948,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":250328,"user_name":"对与错","can_delete":false,"product_type":"c1","uid":1682027,"ip_address":"","ucode":"EF55733E3BD78B","user_header":"https://static001.geekbang.org/account/avatar/00/19/aa/6b/ab9a072a.jpg","comment_is_top":false,"comment_ctime":1601026636,"is_pvip":false,"replies":[{"id":"91719","content":"一对一。log下面分多个segment","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1601170397,"ip_address":"","comment_id":250328,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1601026636","product_id":100050101,"comment_content":"请问分区与Log之间的关系是一对多吗?","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506186,"discussion_content":"一对一。log下面分多个segment","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1601170397,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1030082,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/b7/c2/196932c7.jpg","nickname":"南琛一梦","note":"","ucode":"6338D5428DB2B0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":540311,"discussion_content":"1、一个topic对应多个Partition\n2、一个Partition对应一个逻辑Log对象\n3、一个Log对象(一个Partition)对应一个物理磁盘目录\n4、一个物理磁盘目录包含多个日志相关文件，每一组日志相关文件为一个逻辑Logsegment对象(*.index、*.log、*.timeindex等)\n5、一个Log对象对应多个LogSegment对象","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640014104,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":243969,"user_name":"鲁·本","can_delete":false,"product_type":"c1","uid":1209939,"ip_address":"","ucode":"F1DEB30C21B48E","user_header":"https://static001.geekbang.org/account/avatar/00/12/76/53/21d62a23.jpg","comment_is_top":false,"comment_ctime":1598342676,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1598342676","product_id":100050101,"comment_content":"用注释的方式给出我的理解: <br><br>def maybeIncrementHighWatermark(newHighWatermark: LogOffsetMetadata): Option[LogOffsetMetadata] = {<br>    &#47;&#47;如果期望设置的高水位offset &gt; endOffset 无效值,抛出异常<br>    if (newHighWatermark.messageOffset &gt; logEndOffset)<br>      throw new IllegalArgumentException(s&quot;High watermark $newHighWatermark update exceeds current &quot; +<br>        s&quot;log end offset $logEndOffsetMetadata&quot;)<br><br>    lock.synchronized {<br>      &#47;&#47;获取老的高水位元数据<br>      val oldHighWatermark = fetchHighWatermarkMetadata<br><br>      &#47;&#47; Ensure that the high watermark increases monotonically. We also update the high watermark when the new<br>      &#47;&#47; offset metadata is on a newer segment, which occurs whenever the log is rolled to a new segment.<br>      &#47;&#47;以下场景会执行高水位元数据的更新<br>      &#47;&#47;如果老的高水位offset &lt; 新的高水位offset 或者<br>      &#47;&#47;老的高水位offset == 新的高水位offset 并且 老的高水位元数据的baseoff小于新的高水位元数据的baseoffset(也就是虽然高水位没变,但是日志段发生了滚动)<br>      if (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset ||<br>        (oldHighWatermark.messageOffset == newHighWatermark.messageOffset &amp;&amp; oldHighWatermark.onOlderSegment(newHighWatermark))) {<br>        updateHighWatermarkMetadata(newHighWatermark)<br>        Some(oldHighWatermark)<br>      } else {<br>        None<br>      }<br>    }<br>  }","like_count":0},{"had_liked":false,"id":242532,"user_name":"三颗豆子","can_delete":false,"product_type":"c1","uid":2008638,"ip_address":"","ucode":"632CE3E7563666","user_header":"https://static001.geekbang.org/account/avatar/00/1e/a6/3e/3d18f35a.jpg","comment_is_top":false,"comment_ctime":1597752426,"is_pvip":false,"replies":[{"id":"89547","content":"1、定时任务<br>2、unflushed记录的是未写入到检查点文件的日志段。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1597885591,"ip_address":"","comment_id":242532,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1597752426","product_id":100050101,"comment_content":"请问一下：<br>1、检查点是根据什么依据来建立的呢？是定时任务，还是什么。<br>2、“unflushed日志段”记录的是未提交位移的日志吗？如果是已经提交位移的日志，删掉这些日志段对象，不就映射不到这些日志了吗？如果是未提交位移的日志，那已经提交位移的日志一定在recoveryPoint之内吗，为什么？<br>谢谢。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504030,"discussion_content":"1、定时任务\n2、unflushed记录的是未写入到检查点文件的日志段。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597885591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":230752,"user_name":"懂码哥(GerryWen)","can_delete":false,"product_type":"c1","uid":2049910,"ip_address":"","ucode":"55CD1C9AAD87CD","user_header":"","comment_is_top":false,"comment_ctime":1593483216,"is_pvip":false,"replies":[{"id":"85258","content":"一起加油⛽️","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593506684,"ip_address":"","comment_id":230752,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593483216","product_id":100050101,"comment_content":"读源码真的是件痛并快乐的事，非常幸运能遇到老师您的课程。跟您一起坚持读源码。","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500023,"discussion_content":"一起加油⛽️","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593506684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":229887,"user_name":"yes","can_delete":false,"product_type":"c1","uid":1386201,"ip_address":"","ucode":"612BF6884ED6CC","user_header":"https://static001.geekbang.org/account/avatar/00/15/26/d9/f7e96590.jpg","comment_is_top":false,"comment_ctime":1593177843,"is_pvip":false,"replies":[{"id":"85036","content":"1、为了确保segments中的日志段能够覆盖包含的offset值，因为可能其他线程也要访问segments<br>2、做完了swap，移除swap后缀","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593389507,"ip_address":"","comment_id":229887,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593177843","product_id":100050101,"comment_content":"老师，关于completeSwapOperations里面的replaceSegments(),我有些疑问。<br>replaceSegments方法里面有以下代码：<br>1、sortedNewSegments.reverse.foreach(addSegment(_)) ，这个reverse有什么意义在<br>2、sortedNewSegments.foreach(_.changeFileSuffixes(Log.SwapFileSuffix, &quot;&quot;)) ，为啥newSuffix是“”，不是log","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499683,"discussion_content":"1、为了确保segments中的日志段能够覆盖包含的offset值，因为可能其他线程也要访问segments\n2、做完了swap，移除swap后缀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593389507,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":219079,"user_name":"Roger宇","can_delete":false,"product_type":"c1","uid":1703222,"ip_address":"","ucode":"CBA23C01409349","user_header":"https://static001.geekbang.org/account/avatar/00/19/fd/36/f947c340.jpg","comment_is_top":false,"comment_ctime":1589940951,"is_pvip":false,"replies":[{"id":"80946","content":"log compaction不是log主要功能，而且源码在LogCleaner部分。不影响log的学习：）","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1589942825,"ip_address":"","comment_id":219079,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1589940951","product_id":100050101,"comment_content":"想请问一下老师，Log Compaction的内容计划在什么时候说呢？Log上中的swap文件相关操作硬着头皮没去纠结它的细节，把整体框架看了一遍。但看完日志整个章节也没遇到让人有点心虚啊。本来以为Log Compaction也是日志操作应该会在第一章，但结果没有找到。这部分暂时没理解清楚不会影响之后的源码学习吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495675,"discussion_content":"log compaction不是log主要功能，而且源码在LogCleaner部分。不影响log的学习：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589942825,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215617,"user_name":"尘枫","can_delete":false,"product_type":"c1","uid":1093372,"ip_address":"","ucode":"79698689EDD97A","user_header":"https://static001.geekbang.org/account/avatar/00/10/ae/fc/d647a3e4.jpg","comment_is_top":false,"comment_ctime":1589033357,"is_pvip":false,"replies":[{"id":"79868","content":"你是指哪里的Snapshot呢？<br>","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1589122325,"ip_address":"","comment_id":215617,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1589033357","product_id":100050101,"comment_content":"老师，有时间麻烦讲一下 snapshot相关的源码吧，这个用的也挺多的","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494499,"discussion_content":"你是指哪里的Snapshot呢？\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589122325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214964,"user_name":"贝","can_delete":false,"product_type":"c1","uid":1963914,"ip_address":"","ucode":"19603B3FDECC7B","user_header":"https://static001.geekbang.org/account/avatar/00/1d/f7/8a/09a4c107.jpg","comment_is_top":false,"comment_ctime":1588858648,"is_pvip":false,"replies":[{"id":"79690","content":"副本均衡这个提法很陌生，不太确定是指哪个操作？而且想了解下您是怎么做限流的。很多限流操作是针对全局的，最好详细描述一下场景。","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1588984405,"ip_address":"","comment_id":214964,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588858648","product_id":100050101,"comment_content":"老师，有个问题百思不得其解，在进行topic副本均衡的时候，如果只对topicA进行均衡，要是加了限流不应该只限制该topic么，为什么实践的过程发现如果限流太小会导致其它的一些topic也出现未同步的现象呢，而且无法恢复，只能重启kafka。关于kafka的副本均衡是否考虑讲一下呢？这里的水太深了，限流高了会打垮机器，低了又会出现副本未同步的现象，都不知道该怎么进行副本均衡了","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494260,"discussion_content":"副本均衡这个提法很陌生，不太确定是指哪个操作？而且想了解下您是怎么做限流的。很多限流操作是针对全局的，最好详细描述一下场景。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588984405,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210839,"user_name":"thomas","can_delete":false,"product_type":"c1","uid":1016777,"ip_address":"","ucode":"9AB945308F1B50","user_header":"https://static001.geekbang.org/account/avatar/00/0f/83/c9/5d03981a.jpg","comment_is_top":false,"comment_ctime":1587834049,"is_pvip":true,"replies":[{"id":"78475","content":"第5步要用到上一步更新的LEO值，即nextOffsetMetadata对象","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587868402,"ip_address":"","comment_id":210839,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587834049","product_id":100050101,"comment_content":"老师，请问log类的初始化的5个步骤中，第二（初始化Leader Epoch Cache）和第五步(更新Leader Epoch Cache)为什么要分开，不能把第二步和第五步合并吗？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493123,"discussion_content":"第5步要用到上一步更新的LEO值，即nextOffsetMetadata对象","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587868402,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208794,"user_name":"吃饭饭","can_delete":false,"product_type":"c1","uid":1231549,"ip_address":"","ucode":"95CFA07CDA2957","user_header":"https://static001.geekbang.org/account/avatar/00/12/ca/bd/a51ae4b2.jpg","comment_is_top":false,"comment_ctime":1587439497,"is_pvip":false,"replies":[{"id":"78135","content":"log start offset可以不等于baseOffset。事实上，这两个位移值没有直接的关联。图中仅仅是一个示例","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587518505,"ip_address":"","comment_id":208794,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587439497","product_id":100050101,"comment_content":"文中【图中绿色的位移值 3 是日志的 Log Start Offset】，这里不明白，为什么 logStartOffset 不等于 baseOffset = 0 ？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492624,"discussion_content":"log start offset可以不等于baseOffset。事实上，这两个位移值没有直接的关联。图中仅仅是一个示例","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587518505,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208208,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1587302238,"is_pvip":true,"replies":[{"id":"77742","content":"是的：","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1587307123,"ip_address":"","comment_id":208208,"utype":1}],"discussion_count":1,"race_medal":1,"score":"1587302238","product_id":100050101,"comment_content":"这个 一个Log 对象 就相当与 一个日志分区文件夹吗？ 就是分区文件夹下的所有日志段对象的集合","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492378,"discussion_content":"是的：","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587307123,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}