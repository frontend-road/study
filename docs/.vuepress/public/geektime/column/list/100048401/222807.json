{"id":222807,"title":"09 | 索引更新：刚发布的文章就能被搜到，这是怎么做到的？","content":"<p>你好，我是陈东。</p><p>在前面的课程中，我们讲到，倒排索引是许多检索系统的核心实现方案。比如，搜索引擎对万亿级别网页的索引，就是使用倒排索引实现的。我们还讲到，对于超大规模的网页建立索引会非常耗时，工业界往往会使用分布式技术来并行处理。</p><p>对于发布较久的网页，搜索引擎可以有充足的时间来构建索引。但是一些新的网页和文章，往往发布了几分钟就可以被用户搜索到。这又是怎么做到的呢？今天，我们就来聊一聊这个问题。</p><h2>工业界如何更新内存中的索引？</h2><p>我们先来看这么一个问题：如果现在有一个小规模的倒排索引，它能完全加载在内存中。当有新文章进入内存的时候，倒排索引该如何更新呢？这个问题看似简单，但是实现起来却非常复杂。</p><p>我们能想到最直接的解决思路是，只要解析新文章有哪些关键词，然后将文章ID加入倒排表中关键词对应的文档列表即可。没错，在没有其他用户使用的情况下，这样的方法是可行的。但如果你有过一定的工程经验，你就会知道，在实际应用中，必然会有多个用户同时访问这个索引。</p><p>这个时候，如果我们直接更新倒排索引，就可能造成用户访问错误，甚至会引发程序崩溃。因此，一般来说，我们会对倒排表加上“读写锁”，然后再更新。但是，加上“锁”之后会带来频繁的读写锁切换，整个系统的检索效率会比无锁状态有所下降。</p><!-- [[[read_end]]] --><p>因此，为了使得系统有更好的性能，在工业界的实现中，我们会使用一种叫做“<strong>Double Buffer（双缓冲）机制</strong>”的解决方案，使得我们可以在无锁状态下对索引完成更新。</p><p>所谓“Double Buffer”，就是在内存中同时保存两份一样的索引，一个是索引A，一个是索引B。我们会使用一个指针p指向索引A，表示索引A是当前可访问的索引。那么用户在访问时就会通过指针p去访问索引A。这个时候，如果我们要更新，只更新索引B。这样，索引A和索引B之间就不存在读写竞争的问题了。因此，在这个过程中，索引A和索引B都可以保持无锁的状态。</p><p>那更新完索引B之后，我们该如何告知用户应该来访问索引B呢？这时候，我们可以将指针p通过<a href=\"https://www.infoq.cn/article/atomic-operations-and-contention\">原子操作</a>（即无法被打断的最细粒度操作，在Java和C++11等语言中都有相应实现）从A直接切换到B上。接着，我们就把索引B当作“只读索引”，然后更新索引A。</p><p>通过这样的机制，我们就能同时维护两个倒排索引，保持一个读、一个写，并且来回切换，最终完成高性能的索引更新。不过，为了避免切换太频繁，我们并不是每来一条新数据就更新，而是积累一批新数据以后再批量更新。这就是工业界常用的Double Buffer机制。<br>\n<img src=\"https://static001.geekbang.org/resource/image/ff/f7/ff14e4247a2fc68bfe8f1b13c7d767f7.jpg?wh=1920*1030\" alt=\"\"></p><center><span class=\"reference\">用Double Buffer机制更新索引</span></center><p>用Double Buffer机制更新索引是一个高效的方案，追求检索性能的应用场景常常会使用这种方案。但是对于索引到了一定量级的应用而言，使用Double Buffer会带来翻倍的内存资源开销。比如说，像搜索引擎这样万亿级网页的索引规模，数据大部分存储在磁盘上，更是无法直接使用Double Buffer机制进行更新的。因此，我们还是需要寻找其他的解决方案。</p><h2>如何使用“全量索引结合增量索引”方案？</h2><p>对于大规模的索引更新，工业界常用“全量索引结合增量索引”的方案来完成。下面，我们就一起来探讨一下，这个方案是如何实现索引更新的。</p><p>首先，系统会周期性地处理全部的数据，生成一份完整的索引，也就是<strong>全量索引</strong>。这个索引不可以被实时修改，因此为了提高检索效率，我们可以不加“锁”。那对于实时更新的数据我们应该怎样处理呢？我们会将新接收到的数据单独建立一个可以存在内存中的倒排索引，也就是<strong>增量索引</strong>。当查询发生的时候，我们会同时查询全量索引和增量索引，将合并的结果作为总的结果输出。这就是“<strong>全量索引结合增量索引</strong>”的更新方案。</p><p>其实这个方案还能结合我们上面讲的Double Buffer机制来优化。因为增量索引相对全量索引而言会小很多，内存资源消耗在可承受范围，所以我们可以使用Double Buffer机制对增量索引进行索引更新。这样一来，增量索引就可以做到无锁访问。而全量索引本身就是只读的，也不需要加锁。因此，整个检索过程都可以做到无锁访问，也就提高了系统的检索效率。</p><p>“全量索引结合增量索引”的检索方案，可以很好地处理新增的数据。那对于删除的数据，如果我们不做特殊处理，会有什么问题呢？下面，我们一起来分析一下。</p><p>假设，一个数据存储在全量索引中，但是在最新的实时操作中，它被删除了，那么在增量索引中，这个数据并不存在。当我们检索的时候，增量索引会返回空，但全量索引会返回这个数据。如果我们直接合并这两个检索结果，这个数据就会被留下作为检索结果返回，但是这个数据明明已经被删除了，这就会造成错误。</p><p>要解决这个问题，我们就需要在增量索引中保留删除的信息。最常见的解决方案是增加一个删除列表，将被删除的数据记录在列表中，然后检索的时候，我们将全量倒排表和增量倒排表的检索结果和删除列表作对比。如果结果数据存在于删除列表中，就说明该数据是无效的，我们直接删除它即可。</p><p>因此，完整的“全量索引结合增量索引”检索方案，需要在增量索引中保存一个删除列表。<br>\n<img src=\"https://static001.geekbang.org/resource/image/92/14/927bbd6cb53ceafc61384e0109d6a414.jpg?wh=1920*784\" alt=\"\"></p><center><span class=\"reference\">全量索引结合增量索引的检索方案</span></center><h2>增量索引空间的持续增长如何处理？</h2><p>“全量索引结合增量索引”的方案非常实用，但是内存毕竟有限。如果我们不对内存中的增量索引做任何处理，那随着时间推移，内存就会被写满。因此，我们需要在合适的时机将增量索引合并到全量索引中，释放增量索引的内存空间。</p><p>将增量索引合并到全量索引中的常见方法有3种，分别是：完全重建法、再合并法和滚动合并法。下面，我们一一来看。</p><h3>1. 完全重建法</h3><p>如果增量索引的增长速度不算很快，或者全量索引重建的代价不大，那么我们完全可以在增量索引写满内存空间之前，完全重建一次全量索引，然后将系统查询切换到新的全量索引上。</p><p>这样一来，之前旧的增量索引的空间也可以得到释放。这种方案叫作完全重建法。它对于大部分规模不大的检索系统而言，是十分简单可行的方案。</p><h3>2. 再合并法</h3><p>尽管完全重建法的流程很简单，但是效率并不是最优的。</p><p>在<a href=\"https://time.geekbang.org/column/article/222810\">第8讲</a>中我们讲过，对于较大规模的检索系统而言，在构建索引的时候，我们常常会将大数据集分割成多个小数据集，分别建立小索引，再把它们合并成一个大索引。</p><p>借助这样的思路，我们完全可以把全量索引想象成是一个已经将多个小索引合并好的大索引，再把增量索引想象成是一个新增的小索引。这样一来，我们完全可以直接归并全量索引和增量索引，生成一个新的全量索引，这也就避免了从头处理所有文档的重复开销。这种方法就是效率更高的再合并法。<br>\n<img src=\"https://static001.geekbang.org/resource/image/db/1e/dbdff3486450a78abe1148cd43ba721e.jpg?wh=1920*811\" alt=\"\"></p><center><span class=\"reference\">再合并法</span></center><h3>3. 滚动合并法</h3><p>不过，如果全量索引和增量索引的量级差距过大，那么再合并法的效率依然不高。</p><p>为什么这么说呢？我们以搜索引擎为例来分析一下。在搜索引擎中，增量索引只有上万条记录，但全量索引可能有万亿条记录。这样的两个倒排索引合并的过程中，只有少数词典中的关键词和文档列表会被修改，其他大量的关键词和文档列表都会从旧的全量索引中被原样复制出来，再重写入到新的全量索引中，这会带来非常大的无谓的磁盘读写开销。因此，对于这种量级差距过大的全量索引和增量索引的归并来说，如何避免无谓的数据复制就是一个核心问题。</p><p>最直接的解决思路就是<strong>原地更新法</strong>。所谓“原地更新法”，就是不生成新的全量索引，直接在旧的全量索引上修改。</p><p>但这种方法在工程实现上其实效率并不高，原因有两点。</p><p>首先，它要求倒排文件要拆散成多个小文件，每个关键词对应的文档列表为一个小文件，这样才可以将增量索引中对应的变化直接在对应的小文件上单独修改。但这种超大规模量级的零散小文件的高效读写，许多操作系统是很难支持的。</p><p>其次，由于只有一份全量索引同时支持读和写，那我们就需要“加锁”，这肯定也会影响检索效率。因此，在一些大规模工程中，我们并不会使用原地更新法。</p><p>这就又回到了我们前面要解决的核心问题，也就是如何避免无谓的数据复制，那在工业界中常用的减少无谓数据复制的方法就是<strong>滚动合并法</strong>。所谓滚动合并法，就是先生成多个不同层级的索引，然后逐层合并。</p><p>比如说，一个检索系统在磁盘中保存了全量索引、周级索引和天级索引。所谓<strong>周级索引</strong>，就是根据本周的新数据生成的一份索引，那<strong>天级索引</strong>就是根据每天的新数据生成的一份索引。在滚动合并法中，当内存中的增量索引增长到一定体量时，我们会用再合并法将它合并到磁盘上当天的天级索引文件中。</p><p>由于天级的索引文件条数远远没有全量索引多，因此这不会造成大量的无谓数据复制。等系统中积累了7天的天级索引文件后，我们就可以将这7个天级索引文件合并成一个新的周级索引文件。因此，在每次合并增量索引和全量索引的时候，通过这样逐层滚动合并的方式，就不会进行大量的无谓数据复制的开销。这个过程就叫作滚动合并法。<br>\n<img src=\"https://static001.geekbang.org/resource/image/8e/36/8ef104a67bdeebaf57e16a895cf4d936.jpg?wh=1920*849\" alt=\"\"></p><center><span class=\"reference\">滚动合并法</span></center><h2>重点回顾</h2><p>今天，我们介绍了工业界中，不同规模的倒排索引对应的索引更新方法。</p><p>对于内存资源足够的小规模索引，我们可以直接使用<strong>Double Buffer机制</strong>更新内存中的索引；对于内存资源紧张的大规模索引，我们可以使用“<strong>全量索引结合增量索引</strong>”的方案来更新内存中的索引。</p><p>在“全量索引结合增量索引”的方案中，全量索引根据内存资源的使用情况不同，它既可以存在内存中，也可以存在磁盘上。而增量索引则需要全部存在内存中。</p><p>当增量索引增长到上限时，我们需要合并增量索引和全量索引，根据索引的规模和增长速度，我们可以使用的合并方法有完全重建法、再合并法和滚动合并法。</p><p>除此之外，我们还讲了一个很重要的工业设计思想，就是读写分离。实际上，高效的索引更新方案都应用了读写分离的思想，将主要的数据检索放在一个只读的组件上。这样，检索时就不会有读写同时发生的竞争状态了，也就避免了加锁。事实上，无论是Double Buffer机制，还是全量索引结合增量索引，都是读写分离的典型例子。</p><h2>课堂讨论</h2><p>为什么在增量索引的方案中，对于删除的数据，我们不是像LSM树一样在索引中直接做删除标记，而是额外增加一个删除列表？</p><p>欢迎在留言区畅所欲言，说出你的思考过程。如果有收获，也欢迎把这篇文章分享给你的朋友。</p>","neighbors":{"left":{"article_title":"08 | 索引构建：搜索引擎如何为万亿级别网站生成索引？","id":222810},"right":{"article_title":"10 | 索引拆分：大规模检索系统如何使用分布式技术加速检索？","id":225869}},"comments":[{"had_liked":false,"id":206923,"user_name":"每天晒白牙","can_delete":false,"product_type":"c1","uid":1004698,"ip_address":"","ucode":"A1B102CD933DEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","comment_is_top":false,"comment_ctime":1586957426,"is_pvip":false,"replies":[{"id":"77282","content":"总结得很认真。相信你学完这一课后，再去看es中的segment的处理就会很轻松了，比如segment的生成和合并，还有.del文件存储删除列表等。<br>此外，你还可以思考索引更新这一块，你们当前系统的实现方案是否合理，是否有优化空间等。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586967626,"ip_address":"","comment_id":206923,"utype":1}],"discussion_count":1,"race_medal":0,"score":"53126564978","product_id":100048401,"comment_content":"第一时间看到这个思考题，我没啥思路，看了大家的留言和老师的回复，学到了，把老师的回复总结了起来<br>为什么在增量索引中，对于要删除的数据没有像 LSM 树那样一样在索引中直接做删除标记，而是额外增加一个删除列表？<br>1.倒排所以和 kv 存储还是有不一样的地方，倒排索引的 posting list 元素有很多，每个元素都做删除标记代价较大<br>2.一个文档可能存在多个 key，所以一个文档都要修改删除标记的话，读写很频繁，加班性能下降<br>3.加标记也没什么用处，因为在对 postlist 做合并的过程中，数据都是全部存在的，只有在最后和全量索引合并时才进行真正的删除操作，这样可能还没有把要删除的元素放到一个删除列表中，在最后做交集更高效","like_count":12,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491952,"discussion_content":"总结得很认真。相信你学完这一课后，再去看es中的segment的处理就会很轻松了，比如segment的生成和合并，还有.del文件存储删除列表等。\n此外，你还可以思考索引更新这一块，你们当前系统的实现方案是否合理，是否有优化空间等。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586967626,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206626,"user_name":"Mq","can_delete":false,"product_type":"c1","uid":1178359,"ip_address":"","ucode":"041F572AFAB275","user_header":"https://static001.geekbang.org/account/avatar/00/11/fa/f7/91ac44c5.jpg","comment_is_top":false,"comment_ctime":1586911745,"is_pvip":false,"replies":[{"id":"77193","content":"滚动合并机制的确是最复杂的一种。它的核心思想是“解决小索引和大索引合并的效率问题，避免大索引产生大量无谓的复制操作”。而解决方案则是“在小索引和大索引中间加入中索引进行过渡”。<br>这个设计方案其实会很常见。比如说在lsm树那一课，我说了“假设只有c0树和c1树”，而实际情况是c1树会非常大，合并效率会很低，因此lsm树的设计中就有着多棵不同大小的树。包括leveldb的实现，也会有着多层索引。因此，这是一个值得我们学习和掌握的方法。<br>至于你举的这个例子，结合文中的内容，使用滚动合并的流程是这样的:<br>1.今天增加的网页会先存在内存的增量索引中。<br>2.增量索引满了，要开始合并。<br>3.增量索引和当天的天级索引合并(天级索引不大，所以合并代价小)。<br>4.当天级索引达到了7天时，可以将多个天级索引合并，变成一个新的周级索引。<br>5.当有多个周级索引的时候，全量索引会和多个周级索引合并，生成一份新的全量索引。(不过，一般这一步会用重新生成全量索引来代替，你可以理解为为了保证系统的稳定性，需要定期进行索引重建。就像系统要进行定期重启一样)。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586914688,"ip_address":"","comment_id":206626,"utype":1}],"discussion_count":3,"race_medal":0,"score":"40241617409","product_id":100048401,"comment_content":"看不懂滚动合并机制，老师能结合具体数据分析下，例如我今天增加了几个网页，有倒排索引关键字的value都要加上这个网页，这个滚动合并的流程是咋样的。","like_count":10,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491843,"discussion_content":"滚动合并机制的确是最复杂的一种。它的核心思想是“解决小索引和大索引合并的效率问题，避免大索引产生大量无谓的复制操作”。而解决方案则是“在小索引和大索引中间加入中索引进行过渡”。\n这个设计方案其实会很常见。比如说在lsm树那一课，我说了“假设只有c0树和c1树”，而实际情况是c1树会非常大，合并效率会很低，因此lsm树的设计中就有着多棵不同大小的树。包括leveldb的实现，也会有着多层索引。因此，这是一个值得我们学习和掌握的方法。\n至于你举的这个例子，结合文中的内容，使用滚动合并的流程是这样的:\n1.今天增加的网页会先存在内存的增量索引中。\n2.增量索引满了，要开始合并。\n3.增量索引和当天的天级索引合并(天级索引不大，所以合并代价小)。\n4.当天级索引达到了7天时，可以将多个天级索引合并，变成一个新的周级索引。\n5.当有多个周级索引的时候，全量索引会和多个周级索引合并，生成一份新的全量索引。(不过，一般这一步会用重新生成全量索引来代替，你可以理解为为了保证系统的稳定性，需要定期进行索引重建。就像系统要进行定期重启一样)。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1586914688,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1012416,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/72/c0/b09911a0.jpg","nickname":"meijing0114","note":"","ucode":"B349D33E2F3ECC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337679,"discussion_content":"那么在查询的过程中，也会查询天级索引，周级索引和全量索引，再把结果合并吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609040413,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194626,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","nickname":"牛牛","note":"","ucode":"CFCE68B4F92209","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":260217,"discussion_content":"这个解答很细致、同时也解答了我的疑问、既然周级别与原全量索引的合并成本很高、直接生成全量索引或许是比较好的选择~~~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588855485,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206794,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1586936011,"is_pvip":true,"replies":[{"id":"77245","content":"综合你前一条一起回复，你说到了两个点上:<br>1.倒排索引和kv不一样，posting list元素很多，每个元素都加标记代价太大。<br>2.一个文档可能会影响多个key，因此每个文档都要修改标记的话，读写操作会很频繁，加锁性能下降。<br><br>此外，还有一点是，加上标记也没啥用，在posting list求交并的过程中，依然要全部留下来，等着最后和全量索引合并时才能真正删除。这样的话不如直接用一个delete list存着，最后求交集更高效。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586947889,"ip_address":"","comment_id":206794,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31651707083","product_id":100048401,"comment_content":"对于结尾的问题：我在补偿一下，除了上面说的原因还有就是，一个文档 会有多个 key， 也不可能对文档包含的每个 key 进行文档标记","like_count":7,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491905,"discussion_content":"综合你前一条一起回复，你说到了两个点上:\n1.倒排索引和kv不一样，posting list元素很多，每个元素都加标记代价太大。\n2.一个文档可能会影响多个key，因此每个文档都要修改标记的话，读写操作会很频繁，加锁性能下降。\n\n此外，还有一点是，加上标记也没啥用，在posting list求交并的过程中，依然要全部留下来，等着最后和全量索引合并时才能真正删除。这样的话不如直接用一个delete list存着，最后求交集更高效。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586947889,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":216908,"user_name":"Impressw","can_delete":false,"product_type":"c1","uid":1095291,"ip_address":"","ucode":"536F818BEC8A93","user_header":"https://static001.geekbang.org/account/avatar/00/10/b6/7b/bebb4587.jpg","comment_is_top":false,"comment_ctime":1589366187,"is_pvip":false,"replies":[{"id":"80237","content":"实际的系统的确就是这样实现的。当然，也需要控制一下索引切换(double buffer)或者索引合并(增量+全量)的频率。<br>在搜索引擎中，数据的实时更新可能还不够明显，但是在广告引擎和推荐引擎中，你会直观地感觉到，广告主修改了设置，或者你刚看过一篇文章，相关的反馈马上就发生了","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1589374862,"ip_address":"","comment_id":216908,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23064202667","product_id":100048401,"comment_content":"学到了很多，最近正好遇到了频繁更新索引，为什么能实时被检索到，速度又不会变慢的问题，看了这篇文章茅塞顿开，不过还是想问下，在大规模数据索引，频繁更新，真的能保证实时的情况下，完成更新索引吗","like_count":5,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494985,"discussion_content":"实际的系统的确就是这样实现的。当然，也需要控制一下索引切换(double buffer)或者索引合并(增量+全量)的频率。\n在搜索引擎中，数据的实时更新可能还不够明显，但是在广告引擎和推荐引擎中，你会直观地感觉到，广告主修改了设置，或者你刚看过一篇文章，相关的反馈马上就发生了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589374862,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":212185,"user_name":"出卖灵魂的教练Kerry","can_delete":false,"product_type":"c1","uid":1807943,"ip_address":"","ucode":"8C64517DA556FE","user_header":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","comment_is_top":false,"comment_ctime":1588072588,"is_pvip":true,"replies":[{"id":"78868","content":"都是1，3，5，7。<br>其实你只要理解，双缓存中，每个索引都是完整的全量索引就清楚了。<br>以你的例子来看，步骤如下:<br>1.一开始，索引A中是1，索引B中也是1。<br>2.当3加入时，是加入写索引B,这时B就是(1，3)<br>3.B切换成读索引，这时A变为写索引，这时我们可以将之前变化的数据3也加入到A中，这样A就也是(1，3)了。(当然你也可以完全重建索引更新A)。注意:这一步就是关键。<br>4.当5加入时，加入写索引A，这时A就是(1，3，5)。<br>","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1588075798,"ip_address":"","comment_id":212185,"utype":1}],"discussion_count":4,"race_medal":0,"score":"23062909068","product_id":100048401,"comment_content":"还是有点疑惑，可能太笨了？双缓存情况下，假如更新的索引中id有1，3，5，7...，同时规定更新一个索引，AB读写指针交换一次。假如A开始为写指针，最后更新的结果为A(1,5...)B(3,7...)还是A(1,3,5,7....)B(1,3,5,7.…)","like_count":5,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493450,"discussion_content":"都是1，3，5，7。\n其实你只要理解，双缓存中，每个索引都是完整的全量索引就清楚了。\n以你的例子来看，步骤如下:\n1.一开始，索引A中是1，索引B中也是1。\n2.当3加入时，是加入写索引B,这时B就是(1，3)\n3.B切换成读索引，这时A变为写索引，这时我们可以将之前变化的数据3也加入到A中，这样A就也是(1，3)了。(当然你也可以完全重建索引更新A)。注意:这一步就是关键。\n4.当5加入时，加入写索引A，这时A就是(1，3，5)。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588075798,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028949,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/95/a362f01b.jpg","nickname":"Geek1560","note":"","ucode":"5F27A28B8002E6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":364101,"discussion_content":"请问当写A读B切换到写B读A的时候，如何记录之前写入到A中的数据呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617364481,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2309020,"avatar":"","nickname":"promotion","note":"","ucode":"E87205560B95A9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2028949,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f5/95/a362f01b.jpg","nickname":"Geek1560","note":"","ucode":"5F27A28B8002E6","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":578084,"discussion_content":"c++ 会有原子操作compare_exchange_strong (ces) 机制保障 ，指针从B 到 A， java 会有 CAS 操作保障","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656498256,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":364101,"ip_address":""},"score":578084,"extra":""}]},{"author":{"id":1228500,"avatar":"https://static001.geekbang.org/account/avatar/00/12/be/d4/ff1c1319.jpg","nickname":"金龟","note":"","ucode":"1C7D35C8AE8D9D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":343946,"discussion_content":"解决小索引和大索引合并的效率问题，避免大索引产生大量无谓的复制操作，这个是什么意思呢，老师。小索引和大索引合并，和2个中索引合并应该是一样的才对呀。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611216355,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":217109,"user_name":"PhilZhang","can_delete":false,"product_type":"c1","uid":1088010,"ip_address":"","ucode":"383F1A792C7DF9","user_header":"https://static001.geekbang.org/account/avatar/00/10/9a/0a/922615cf.jpg","comment_is_top":false,"comment_ctime":1589421373,"is_pvip":false,"replies":[{"id":"80293","content":"我分两部分来说。<br>第一，关于要查询的索引数量问题，按你的计算方法，的确是要查13个索引。实际上，在现在集群能力比较强的情况下，我们可以根据自己的情况选择合适的粒度，比如说去掉周级索引，这样只需要全量索引+天级索引+增量索引就好了。因此是1+6+1共7个索引。<br>第二，关于索引副本数的问题。首先分布式系统是每个索引分片都会有多个副本的，增加并发度。副本数看具体情况而定。然后，关于读写分离的两份索引问题，增量索引在内存中使用double buffer，肯定是两份。全量索引和天级索引，由于是在磁盘上，因此在创建的时候会先在磁盘上创建一份，然后将老的删除。因此只会在创建的过程中会有双份，正常时间内只有一份。<br>","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1589431390,"ip_address":"","comment_id":217109,"utype":1}],"discussion_count":2,"race_medal":0,"score":"18769290557","product_id":100048401,"comment_content":"在滚动合并得例子中，如果此时有1个全量索引，5个周级索引，6个天级索引，1个增量索引，此时一次查询就要汇集1+5+6+1一共13个索引得结果是吧，另外为了保证读写分离，每个索引都要保存两份。不知道我的理解是否正确。","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495040,"discussion_content":"我分两部分来说。\n第一，关于要查询的索引数量问题，按你的计算方法，的确是要查13个索引。实际上，在现在集群能力比较强的情况下，我们可以根据自己的情况选择合适的粒度，比如说去掉周级索引，这样只需要全量索引+天级索引+增量索引就好了。因此是1+6+1共7个索引。\n第二，关于索引副本数的问题。首先分布式系统是每个索引分片都会有多个副本的，增加并发度。副本数看具体情况而定。然后，关于读写分离的两份索引问题，增量索引在内存中使用double buffer，肯定是两份。全量索引和天级索引，由于是在磁盘上，因此在创建的时候会先在磁盘上创建一份，然后将老的删除。因此只会在创建的过程中会有双份，正常时间内只有一份。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589431390,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1088010,"avatar":"https://static001.geekbang.org/account/avatar/00/10/9a/0a/922615cf.jpg","nickname":"PhilZhang","note":"","ucode":"383F1A792C7DF9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":267891,"discussion_content":"感谢回复","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589705888,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215083,"user_name":"paulhaoyi","can_delete":false,"product_type":"c1","uid":1105619,"ip_address":"","ucode":"C972F4B459E7D6","user_header":"https://static001.geekbang.org/account/avatar/00/10/de/d3/2aa0177f.jpg","comment_is_top":false,"comment_ctime":1588901521,"is_pvip":false,"replies":[{"id":"79618","content":"不完全一样。<br>比如说滚动合并法中的第一层天级索引和增量索引的合并，其实也可以用再合并法来完成。<br>因此，第一种方法是基础，第二种方法是第一种的优化，第三种方法，是将索引进行组织，多次使用第二种方法(再合并法)。这样理解我觉得是OK的。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1588924655,"ip_address":"","comment_id":215083,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10178836113","product_id":100048401,"comment_content":"合并的三种方法，感觉一二是三的特殊情况，分别是只有一层和两层的三。不知道这么理解对不对？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494307,"discussion_content":"不完全一样。\n比如说滚动合并法中的第一层天级索引和增量索引的合并，其实也可以用再合并法来完成。\n因此，第一种方法是基础，第二种方法是第一种的优化，第三种方法，是将索引进行组织，多次使用第二种方法(再合并法)。这样理解我觉得是OK的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588924655,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208709,"user_name":"出卖灵魂的教练Kerry","can_delete":false,"product_type":"c1","uid":1807943,"ip_address":"","ucode":"8C64517DA556FE","user_header":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","comment_is_top":false,"comment_ctime":1587426023,"is_pvip":true,"replies":[{"id":"77975","content":"是这样，双缓存机制的设计理念是读写分离，一个索引只负责写，另一个索引只负责读。因此，不会存在你说的两个索引同时被写的情况。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1587430026,"ip_address":"","comment_id":208709,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10177360615","product_id":100048401,"comment_content":"双缓存机制有个疑问，假如A更新了一个数据1，B也需要更新数据1，这个如何保证呢？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492586,"discussion_content":"是这样，双缓存机制的设计理念是读写分离，一个索引只负责写，另一个索引只负责读。因此，不会存在你说的两个索引同时被写的情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587430026,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207169,"user_name":"兰柯一梦","can_delete":false,"product_type":"c1","uid":1064990,"ip_address":"","ucode":"C51330CAA4EAB7","user_header":"https://static001.geekbang.org/account/avatar/00/10/40/1e/910aef6a.jpg","comment_is_top":false,"comment_ctime":1587019993,"is_pvip":false,"replies":[{"id":"77405","content":"你的想法很好，其实是有可能的。本质上，你是复用了正排表，让它承载了删除列表的功能。在最后posting list合并的时候，通过查正排表完成过滤(其实就是加餐一中说的哈希表法:将删除列表变成了哈希表)。<br>在系统比较简单的时候，这样使用是OK的。不过当系统足够复杂的时候，我们需要将不同功能和数据进行合理的划分，倒排检索和正排查询有可能是两个不同的环节和模块(包括中间可能还有其他环节，比如抽取特征，打分计算等)。因此从这个角度出发，复杂系统才会抽象出删除列表这个对象，这样就可以不依赖于正排表，从而完成了系统架构的解耦设计。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1587036067,"ip_address":"","comment_id":207169,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10176954585","product_id":100048401,"comment_content":"如果在doc的正排字段中做标记删除是不是也可以呢？ 这样等各个索引进行合并的时候，看doc对应的正排的删除标记，如果是删除状态那边直接丢掉","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492031,"discussion_content":"你的想法很好，其实是有可能的。本质上，你是复用了正排表，让它承载了删除列表的功能。在最后posting list合并的时候，通过查正排表完成过滤(其实就是加餐一中说的哈希表法:将删除列表变成了哈希表)。\n在系统比较简单的时候，这样使用是OK的。不过当系统足够复杂的时候，我们需要将不同功能和数据进行合理的划分，倒排检索和正排查询有可能是两个不同的环节和模块(包括中间可能还有其他环节，比如抽取特征，打分计算等)。因此从这个角度出发，复杂系统才会抽象出删除列表这个对象，这样就可以不依赖于正排表，从而完成了系统架构的解耦设计。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587036067,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":221544,"user_name":"CBGSIMON","can_delete":false,"product_type":"c1","uid":2008077,"ip_address":"","ucode":"CDD021828F6241","user_header":"","comment_is_top":false,"comment_ctime":1590538135,"is_pvip":false,"replies":[{"id":"81694","content":"对于第三种方案(滚动合并法)，你的理解没错，通过分层的设计，使得我们可以减少全量合并的频率。而且，在大多数系统的实现中，其实最后一层的全量合并并不会执行，而是使用完全重建的方式重新生成全量索引。<br><br>对于第二种方法(再合并法)和第一种方法(完全重建法)的区别，其实就在于是否要从头开始对所有的文档进行处理。<br>第一种方法(完全重建法)，需要将所有文档分片，然后分别建立多个小的索引文件，再将这多个小索引文件合并成一个全量索引文件。<br>而第二种方法(再合并法)，它利用已有的全量索引文件，直接和增量索引进行合并就可以了。和第一种方法相比，这样就省去了多个小索引文件生成和合并的过程。因此会更优一些。至于加锁问题，其实并不会加锁。因为我们是生成一个新的全量索引文件，然后将旧的删除。而不是直接在旧的全量索引文件上更新。这其实也是很多系统的设计思路:不进行原地更新，而是生成新的，旧的删除。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1590540988,"ip_address":"","comment_id":221544,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5885505431","product_id":100048401,"comment_content":"针对第三种方案的理解：最终合并到全量的索引中还是要全量遍历，但设计了这样的层级后，就减少了这样的频率。<br>小的和小的合并，等小的达到一定程度再和大的合并<br><br>没看出第2个相对第1个优化在哪里？<br>第2个不也是要重建全量索引么？如果不是重建，是在原来的全量索引上更新，那更新的时候又要加写锁吧？","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496488,"discussion_content":"对于第三种方案(滚动合并法)，你的理解没错，通过分层的设计，使得我们可以减少全量合并的频率。而且，在大多数系统的实现中，其实最后一层的全量合并并不会执行，而是使用完全重建的方式重新生成全量索引。\n\n对于第二种方法(再合并法)和第一种方法(完全重建法)的区别，其实就在于是否要从头开始对所有的文档进行处理。\n第一种方法(完全重建法)，需要将所有文档分片，然后分别建立多个小的索引文件，再将这多个小索引文件合并成一个全量索引文件。\n而第二种方法(再合并法)，它利用已有的全量索引文件，直接和增量索引进行合并就可以了。和第一种方法相比，这样就省去了多个小索引文件生成和合并的过程。因此会更优一些。至于加锁问题，其实并不会加锁。因为我们是生成一个新的全量索引文件，然后将旧的删除。而不是直接在旧的全量索引文件上更新。这其实也是很多系统的设计思路:不进行原地更新，而是生成新的，旧的删除。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590540988,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206724,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1586921539,"is_pvip":false,"replies":[{"id":"77232","content":"这两点都很好。<br>的确posting list里每个元素都加标记，这个代价会远大于lsm这种只存一个元素kv的场景。<br>此外，一个文档被删除，它可能会影响很多key和posting list，这个读写加锁代价不小。<br>还有，即使使用double buffer实现增量索引，但是这个标记也没什么用。我们在增量索引中求交集和并集时，依然要保留所有的元素，这样和全量索引的结果合并时才不会出错。因此提前打上标记并不能加快检索效率。不如最后记录一个delete list，然后快速求交集处理掉。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586935034,"ip_address":"","comment_id":206724,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5881888835","product_id":100048401,"comment_content":"1.如果增加一个删除标记，相当于增量索引的每个内容都有这样一个标记，随着增量的数量变大，内存占用会更高。<br>2.利用删除列表就不会有这样的问题，同样可以避免加锁。","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491880,"discussion_content":"这两点都很好。\n的确posting list里每个元素都加标记，这个代价会远大于lsm这种只存一个元素kv的场景。\n此外，一个文档被删除，它可能会影响很多key和posting list，这个读写加锁代价不小。\n还有，即使使用double buffer实现增量索引，但是这个标记也没什么用。我们在增量索引中求交集和并集时，依然要保留所有的元素，这样和全量索引的结果合并时才不会出错。因此提前打上标记并不能加快检索效率。不如最后记录一个delete list，然后快速求交集处理掉。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586935034,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206666,"user_name":"西西弗与卡夫卡","can_delete":false,"product_type":"c1","uid":1001710,"ip_address":"","ucode":"B4C27B8335B76A","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/ee/872ad07e.jpg","comment_is_top":false,"comment_ctime":1586915070,"is_pvip":true,"replies":[{"id":"77227","content":"避免加锁操作的确是一个考虑因素。新删除一个文档，这个文档里可能有很多key，如果要打删除标记，就意味着这些key和posting list都要执行加锁操作，这个代价的确会比较大。<br><br>而且，即使我们使用double buffer，对于增量索引不加锁，那么你可以想想处理过程，如果对于增量索引的posting list中的文档打上删除标记，在进行交并操作的时候，所有的文档都依然要被留下！因为增量索引的结果需要和全量索引结果合并，如果增量索引的结果没有保留删除标记，那么合并时会出错。<br>既然删除标记要在增量索引处理过程中一直保留，那不如单独记录来得方便。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586930398,"ip_address":"","comment_id":206666,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5881882366","product_id":100048401,"comment_content":"用删除列表而不是打删除标记，可以避免对增量索引加锁","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491858,"discussion_content":"避免加锁操作的确是一个考虑因素。新删除一个文档，这个文档里可能有很多key，如果要打删除标记，就意味着这些key和posting list都要执行加锁操作，这个代价的确会比较大。\n\n而且，即使我们使用double buffer，对于增量索引不加锁，那么你可以想想处理过程，如果对于增量索引的posting list中的文档打上删除标记，在进行交并操作的时候，所有的文档都依然要被留下！因为增量索引的结果需要和全量索引结果合并，如果增量索引的结果没有保留删除标记，那么合并时会出错。\n既然删除标记要在增量索引处理过程中一直保留，那不如单独记录来得方便。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586930398,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":348269,"user_name":"Han","can_delete":false,"product_type":"c1","uid":1122349,"ip_address":"","ucode":"280808D4F641AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/20/2d/dfa5bec8.jpg","comment_is_top":false,"comment_ctime":1654917351,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1654917351","product_id":100048401,"comment_content":"老师，没看到关于全量索引里的数据如果被update了，那应该怎么处理？例如旧的文档被改了内容，怎么把更新同步到全量索引呢？","like_count":0,"discussions":[{"author":{"id":2309020,"avatar":"","nickname":"promotion","note":"","ucode":"E87205560B95A9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":578095,"discussion_content":"也是 delete + insert 的操作","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656507306,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317307,"user_name":"设置","can_delete":false,"product_type":"c1","uid":1044361,"ip_address":"","ucode":"669B726C2944A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ef/89/e1f9ed28.jpg","comment_is_top":false,"comment_ctime":1634737312,"is_pvip":false,"replies":[{"id":"116698","content":"其实是每次从（全量索引+增量索引）中找出来的item都必须去delete list中检查一次，不存在于delete list中的才能被视为有效","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1636814659,"ip_address":"","comment_id":317307,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634737312","product_id":100048401,"comment_content":"hi，大佬，请教一个问题：删除的id维持一个删除的列表，item的新增和修改操作都有可能导致一个id变为有效，那么每次消费到item新增或者修改的消息都要去check下delete list。我感觉全量+增量索引的delete list这块问不是很懂。","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528758,"discussion_content":"其实是每次从（全量索引+增量索引）中找出来的item都必须去delete list中检查一次，不存在于delete list中的才能被视为有效","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636814659,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316145,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1634179403,"is_pvip":true,"replies":[{"id":"116697","content":"系统设计往往是要解耦分别考虑。<br>比如说文件副本的问题，其实可以依赖hdfs等基础服务来存储，通过hdfs自己的副本机制来保障高可用性。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1636814377,"ip_address":"","comment_id":316145,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634179403","product_id":100048401,"comment_content":"话说老师 为了保证高可用 比如天级索引 是不是要复制三份 在三个机器上呀 这个过程是怎么实现的？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528282,"discussion_content":"系统设计往往是要解耦分别考虑。\n比如说文件副本的问题，其实可以依赖hdfs等基础服务来存储，通过hdfs自己的副本机制来保障高可用性。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636814377,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316142,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1634178346,"is_pvip":true,"replies":[{"id":"116696","content":"在工业实现中，增量索引往往都会采用冗余备份的方式，有多台服务器同时提供服务。<br>而且即便都down了，也可以重新读取消息rebuild一次全量索引","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1636814268,"ip_address":"","comment_id":316142,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634178346","product_id":100048401,"comment_content":"如果 增量索引机器down了 增量索引 不久消失了？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528281,"discussion_content":"在工业实现中，增量索引往往都会采用冗余备份的方式，有多台服务器同时提供服务。\n而且即便都down了，也可以重新读取消息rebuild一次全量索引","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636814268,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":295027,"user_name":"cleverxiao","can_delete":false,"product_type":"c1","uid":1589955,"ip_address":"","ucode":"29101C5034C4FE","user_header":"https://static001.geekbang.org/account/avatar/00/18/42/c3/921a9e0e.jpg","comment_is_top":false,"comment_ctime":1622188797,"is_pvip":false,"replies":[{"id":"107521","content":"一般在这种实现中，a和b都是索引，它们是基于数据源c生成的。因此，a和b索引间不需要同步，而是可以从数据源c中重建和更新即可。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1622901015,"ip_address":"","comment_id":295027,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1622188797","product_id":100048401,"comment_content":"双缓冲机制中，A负责读B负责写，一段时间后转换角色，B负责读A负责写，那么实在切换角色的时候进行AB间数据同步的么？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520919,"discussion_content":"一般在这种实现中，a和b都是索引，它们是基于数据源c生成的。因此，a和b索引间不需要同步，而是可以从数据源c中重建和更新即可。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622901015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282824,"user_name":"Christmas","can_delete":false,"product_type":"c1","uid":1219172,"ip_address":"","ucode":"F48F6BE4A7595B","user_header":"https://static001.geekbang.org/account/avatar/00/12/9a/64/ad837224.jpg","comment_is_top":false,"comment_ctime":1615429567,"is_pvip":false,"replies":[{"id":"102778","content":"是的，滚动合并是一种常见的设计思想，在许多系统中都可以看到。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1615713839,"ip_address":"","comment_id":282824,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1615429567","product_id":100048401,"comment_content":"想起来hbase tired compaction 还有各种监控系统的最近一天。最近3天。最近一周，最近一个月，最近一年的数据优化压缩机制","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516848,"discussion_content":"是的，滚动合并是一种常见的设计思想，在许多系统中都可以看到。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615713839,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":275524,"user_name":"Geek_7347cf","can_delete":false,"product_type":"c1","uid":2343516,"ip_address":"","ucode":"2E25574FAB1B3B","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/mmTEibMibic5ibsVpNZkR3HBlpPpZYt0gHGdIqOduLGxRHZpTWRG3q56CT1eejoLgNsdaW5aQGWXfyibN4vm9CicYb3w/132","comment_is_top":false,"comment_ctime":1611570916,"is_pvip":false,"replies":[{"id":"102769","content":"滚动合并按不同时间周期合并，解决的问题是避免大量无谓的数据复制操作。<br>至于去重和合并删除，在每次合并时都可以进行。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1615712854,"ip_address":"","comment_id":275524,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1611570916","product_id":100048401,"comment_content":"滚动合并 按不同时间周期合并的方式 没有解决合并去重和合并删除的问题","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514382,"discussion_content":"滚动合并按不同时间周期合并，解决的问题是避免大量无谓的数据复制操作。\n至于去重和合并删除，在每次合并时都可以进行。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615712854,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":243411,"user_name":"造梦师","can_delete":false,"product_type":"c1","uid":1983429,"ip_address":"","ucode":"705FCED828DC71","user_header":"https://static001.geekbang.org/account/avatar/00/1e/43/c5/288c59ab.jpg","comment_is_top":false,"comment_ctime":1598094959,"is_pvip":false,"replies":[{"id":"89732","content":"1.不是很清楚你说的“数据状态”指的是什么。在搜索引擎或类似的系统中，对于数据，只支持“新增”和“删除”两种操作，并没有“更新”操作，也没有复杂的状态。不知道你说的“数据状态”是不是指删除后从“有效”变为“无效”?如果是数据删除的话，就是文中介绍的删除列表方案。<br>2.如果索引都存在了磁盘上，那么每次读取肯定代价很大。因此在后面两讲，我还介绍了索引拆分和索引分层的机制，使得我们可以竟可能将索引放在内存中，提高检索效率。包括加餐3也介绍了一些分层处理的设计理念。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1598178805,"ip_address":"","comment_id":243411,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1598094959","product_id":100048401,"comment_content":"1 如果只是进行数据状态的更新 应该不用加锁吧<br>2 如果索引都存到了磁盘上 怎么保证相应时间呢","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":504279,"discussion_content":"1.不是很清楚你说的“数据状态”指的是什么。在搜索引擎或类似的系统中，对于数据，只支持“新增”和“删除”两种操作，并没有“更新”操作，也没有复杂的状态。不知道你说的“数据状态”是不是指删除后从“有效”变为“无效”?如果是数据删除的话，就是文中介绍的删除列表方案。\n2.如果索引都存在了磁盘上，那么每次读取肯定代价很大。因此在后面两讲，我还介绍了索引拆分和索引分层的机制，使得我们可以竟可能将索引放在内存中，提高检索效率。包括加餐3也介绍了一些分层处理的设计理念。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598178805,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1983429,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/43/c5/288c59ab.jpg","nickname":"造梦师","note":"","ucode":"705FCED828DC71","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300585,"discussion_content":"状态就比如一个广告从上线状态变为下线状态","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598182563,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":1983429,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/43/c5/288c59ab.jpg","nickname":"造梦师","note":"","ucode":"705FCED828DC71","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300593,"discussion_content":"那其实就是从索引中删除这个广告。对于索引的修改希望无锁的话，可以使用double buffer的机制实现。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598186131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":300585,"ip_address":""},"score":300593,"extra":""}]}]},{"had_liked":false,"id":234863,"user_name":"奇奇","can_delete":false,"product_type":"c1","uid":1399097,"ip_address":"","ucode":"BC86B0CB55E35A","user_header":"","comment_is_top":false,"comment_ctime":1594814961,"is_pvip":true,"replies":[{"id":"86821","content":"要注意一点，索引双缓冲机制并不需要同步索引A和索引B。这两个索引中的数据是不需要一样的。<br>我举一个具体的例子，一般来说，我们会有一个数据源(比如说MySQL中的数据)，然后我们会为当前数据建立一个索引A，然后检索引擎使用索引A进行服务。<br>随着数据源的变化，我们可以间隔一段时间后(比如五分钟)，重新为当前的数据生成一个索引B，然后检索引擎切换到最新的索引B上，继续提供检索服务。这时候，索引B显然和索引A是不一样的。而且，索引A和索引B是两个独立的对象，它们之间并不需要同步数据。对于旧的索引A，我们直接从源数据处重新生成索引即可。<br>","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1594912279,"ip_address":"","comment_id":234863,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1594814961","product_id":100048401,"comment_content":"双缓冲区难道就不需要加锁了吗？假如更新B，这个时候指向A的指针切向B，请问如何保持A和B的一致性？不然这个时候有人更新A，但是B的更新还没有同步到A，这不就不一致了吗？还是只能加锁啊","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501530,"discussion_content":"要注意一点，索引双缓冲机制并不需要同步索引A和索引B。这两个索引中的数据是不需要一样的。\n我举一个具体的例子，一般来说，我们会有一个数据源(比如说MySQL中的数据)，然后我们会为当前数据建立一个索引A，然后检索引擎使用索引A进行服务。\n随着数据源的变化，我们可以间隔一段时间后(比如五分钟)，重新为当前的数据生成一个索引B，然后检索引擎切换到最新的索引B上，继续提供检索服务。这时候，索引B显然和索引A是不一样的。而且，索引A和索引B是两个独立的对象，它们之间并不需要同步数据。对于旧的索引A，我们直接从源数据处重新生成索引即可。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594912279,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1399097,"avatar":"","nickname":"奇奇","note":"","ucode":"BC86B0CB55E35A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":291706,"discussion_content":"那这样会出现幻读哦 索引A能读到的数据 索引B如果没有更新就没有 但是指针已经切过去了 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594912395,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":1399097,"avatar":"","nickname":"奇奇","note":"","ucode":"BC86B0CB55E35A","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":291831,"discussion_content":"首先，索引a要切换到索引b，前提肯定是“索引b已经更新成为最新的索引”，这样是不会造成“索引a能读到，但是索引b读不到”的问题。\n其次，我理解你说的幻读，其实指的应该是“一个事务中要查询两次，结果第一次查询了索引a，第二次查询了索引b，第二次查询的结果和第一次不一致”的问题。对于这样的问题，我们只需要保证一次事务只在一个索引上查询就可以了。这可以使用智能指针实现:当有查询事务进入系统时，系统根据当前指针，将该事务导向索引a;当索引切换时，新的事务会被导向索引b，但老的事务依然在索引a上进行查询。当老事务都结束以后，索引a的引用计数为0，就可以被回收了。\n总的来说，双缓冲机制适用于不要求强一致性的场景，事务1和事务2可以有不同的查询结果。这在搜索引擎和推荐引擎的场景中是适用的。\nPS:极客时间的系统对于我回复下的留言没有提醒。因此我可能无法看到。有问题可以新开一个提问。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594967058,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":291706,"ip_address":""},"score":291831,"extra":""}]}]},{"had_liked":false,"id":230486,"user_name":"流浪在寂寞古城","can_delete":false,"product_type":"c1","uid":1105678,"ip_address":"","ucode":"FE90DCD5DC3A20","user_header":"https://static001.geekbang.org/account/avatar/00/10/df/0e/4e2b06d5.jpg","comment_is_top":false,"comment_ctime":1593398905,"is_pvip":false,"replies":[{"id":"85174","content":"1.如果使用滚动合并法，那么查询时需要同时查询多个不同的索引，并将索引结果合并。<br>2.在进行索引合并的时候，有可能有数据被更新，然后该数据在天级索引和全量索引中都有。这种情况下，应该是天级索引优先级大于全量索引(即最新数据覆盖旧数据)。当然，如果你的系统对于update的操作的处理是删除旧数据，然后为新数据生成一个新文档的话，那么只需要支持简单的索引结果合并就好。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1593479168,"ip_address":"","comment_id":230486,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593398905","product_id":100048401,"comment_content":"目前还没接触过那么大的检索场景。索引的操作无非是增、删、改、查。看到滚动合并法，查询的时候是每一个索引都要去召回吗？如果是update，在天级索引加上这个doc，召回、合并索引的时候取时间最新的吗？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499910,"discussion_content":"1.如果使用滚动合并法，那么查询时需要同时查询多个不同的索引，并将索引结果合并。\n2.在进行索引合并的时候，有可能有数据被更新，然后该数据在天级索引和全量索引中都有。这种情况下，应该是天级索引优先级大于全量索引(即最新数据覆盖旧数据)。当然，如果你的系统对于update的操作的处理是删除旧数据，然后为新数据生成一个新文档的话，那么只需要支持简单的索引结果合并就好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593479168,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":224540,"user_name":"Joe Black","can_delete":false,"product_type":"c1","uid":1052528,"ip_address":"","ucode":"21FE222A286445","user_header":"https://static001.geekbang.org/account/avatar/00/10/0f/70/c8680841.jpg","comment_is_top":false,"comment_ctime":1591434811,"is_pvip":false,"replies":[{"id":"82669","content":"这是一个好问题。为了能便捷地判断是否索引a上的所有读操作都结束了，我们可以使用智能指针。当指向索引A的智能指针的引用计数为0，那么就说明索引A的访问都结束了。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1591448136,"ip_address":"","comment_id":224540,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1591434811","product_id":100048401,"comment_content":"请问下对于Double Buffer机制，当索引A处于只读，索引B可更新时，两者访问都可以不加锁；假设每次对索引A的读访问会耗费一定时间，当B更新完毕后，通过原子操作把当前索引设为B，但是我们必须等待所有对A的读操作都结束了才能同步更新A。那么这个等待该如何处理呢？不使用锁怎么样才能知道对A的读取都完成了呢？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":497515,"discussion_content":"这是一个好问题。为了能便捷地判断是否索引a上的所有读操作都结束了，我们可以使用智能指针。当指向索引A的智能指针的引用计数为0，那么就说明索引A的访问都结束了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591448136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214917,"user_name":"牛牛","can_delete":false,"product_type":"c1","uid":1194626,"ip_address":"","ucode":"CFCE68B4F92209","user_header":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","comment_is_top":false,"comment_ctime":1588847963,"is_pvip":false,"replies":[{"id":"79577","content":"是的。你这是一个很好的例子。你会发现类似的设计其实很常见。掌握这种设计思想，相信会对你设计和实现高并发系统有所帮助。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1588854714,"ip_address":"","comment_id":214917,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588847963","product_id":100048401,"comment_content":"看到`双缓冲`的实现、感觉跟redis的rehash思想有点儿类似 ?<br>redis 进行rehash的是启用 ht[1], 然后一步步在添加的过程中搬移原ht[0]的数据、同时标记搬移的位置, 等到数据搬移完成、就将 ht[1] 切为 ht[0], 同时释放ht[0], 申请新的ht[1] 为下次rehash做准备<br><br>双缓冲是, 更新B索引、保证A索引只读, 更新完成后A&#47;B互换, 损耗就是双倍内存消耗, 不适合大量数据场景(不过也是一种无锁设计)","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494243,"discussion_content":"是的。你这是一个很好的例子。你会发现类似的设计其实很常见。掌握这种设计思想，相信会对你设计和实现高并发系统有所帮助。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588854714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214806,"user_name":"时隐时现","can_delete":false,"product_type":"c1","uid":1111899,"ip_address":"","ucode":"DA4D622FF84920","user_header":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","comment_is_top":false,"comment_ctime":1588822314,"is_pvip":false,"replies":[{"id":"79568","content":"1.在许多系统的设计中，是没有update操作的。删除文档中的关键字，其实等价于删除旧文档，生成新文档。<br>因此，如果是这样设计的话，删除列表只需要存文档ID就好了。<br>2.滚动合并也会遵循读写分离的原则，读一个旧的天级索引，生成一个新的天级索引。这样处理会比较简单高效。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1588847044,"ip_address":"","comment_id":214806,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588822314","product_id":100048401,"comment_content":"老师好，有2个问题：<br>1、删除列表的结构是什么样的？我理解的删除分2种，删除关键字和删除文档。前者只需要将对应的kv提取即可，代价很小。后者还是需要根据文档关键字遍历(二分查找)每个posting list，这一操作并不比打deleted标签小，只是在查询时可以优先读取delete list，这样省去了增量索引和全量索引的归并代价。<br>2、滚动合并<br>需要和磁盘上的天索引合并，此时天索引要不要加锁，如果有查询怎么办？莫非还要对天索引复制一份执行读写分离？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494206,"discussion_content":"1.在许多系统的设计中，是没有update操作的。删除文档中的关键字，其实等价于删除旧文档，生成新文档。\n因此，如果是这样设计的话，删除列表只需要存文档ID就好了。\n2.滚动合并也会遵循读写分离的原则，读一个旧的天级索引，生成一个新的天级索引。这样处理会比较简单高效。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588847044,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206793,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1586935741,"is_pvip":true,"replies":[{"id":"77239","content":"这些是可以并行查找的。而不是串行。<br>而且一般来说，以现在的机器处理能力，周级索引其实也可以不用的，这样也能减少系统复杂度。<br>因此一般系统实现就是:<br>增量索引+ 天级索引+全量索引 三个索引并行检索，再合并结果。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586944773,"ip_address":"","comment_id":206793,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586935741","product_id":100048401,"comment_content":"在滚动合并方案中，查询也要一级一级的进行查询， 先查增量索引---&gt; 天级索引----&gt; 周级索引---&gt; 最后是权量索引。 这个的话查询的链路增加了好多，查询的效率会降低多少？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491904,"discussion_content":"这些是可以并行查找的。而不是串行。\n而且一般来说，以现在的机器处理能力，周级索引其实也可以不用的，这样也能减少系统复杂度。\n因此一般系统实现就是:\n增量索引+ 天级索引+全量索引 三个索引并行检索，再合并结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586944773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206787,"user_name":"一步","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1586934764,"is_pvip":true,"replies":[{"id":"77246","content":"在另一条下面统一回复你了。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586947909,"ip_address":"","comment_id":206787,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586934764","product_id":100048401,"comment_content":"为什么在增量索引的方案中，对于删除的数据，我们不是像 LSM 树一样在索引中直接做删除标记，而是额外增加一个删除列表？<br>这个我认为 ，删除的数据相对全量的数据是非常少的，如果用删除标记，那么全部的数据都要进行标记，这样大量的没有删除的数据都会有个未删除的标志，极大的浪费空间资源","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491903,"discussion_content":"在另一条下面统一回复你了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586947909,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206741,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1586924952,"is_pvip":true,"replies":[{"id":"77228","content":"哈哈，的确说得没那么透，只说了“怎么做”，但没说“为什么”。因此才在课后讨论题让大家想想为什么。<br>你说到了很重要的一点，kv只有一个值，但倒排表是一整个posting list，所以修改代价会大。<br>另一方面，即便是使用double buffer技术对增量索引做无锁更新，但增量索引检索过程中，依然要把所有被删除的文档保留到最后，再和全量索引做合并。<br>那既然所有的标记都要保留到最后一步，不如直接在最后一步用一个delete list来求交集更快，逻辑也清晰。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586933420,"ip_address":"","comment_id":206741,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586924952","product_id":100048401,"comment_content":"对这个问题，老师故意在文章里说的很含糊，只说了个记录删除列表的思路😜。<br>lsm之所以可以和删除标记一起存，核心在于类似kv存，删除标记和对应的v是共享k的，所以要拿是会一起拿出来，就可以判断数据这个时候存在不存在，相当于拿到了值的变迁历史。<br>而这个场景，删除文档，对文档集合而言，也可以添加个删除标记，但对于索引而言，它涉及到很多关键字的poslist里对它的指向，这要一个个都加上吗，如果删除的不多，显然还不如最后返回的时候做一个全局的deletelist判断。","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491887,"discussion_content":"哈哈，的确说得没那么透，只说了“怎么做”，但没说“为什么”。因此才在课后讨论题让大家想想为什么。\n你说到了很重要的一点，kv只有一个值，但倒排表是一整个posting list，所以修改代价会大。\n另一方面，即便是使用double buffer技术对增量索引做无锁更新，但增量索引检索过程中，依然要把所有被删除的文档保留到最后，再和全量索引做合并。\n那既然所有的标记都要保留到最后一步，不如直接在最后一步用一个delete list来求交集更快，逻辑也清晰。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586933420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206709,"user_name":"pedro","can_delete":false,"product_type":"c1","uid":1200704,"ip_address":"","ucode":"F40C839DDFD599","user_header":"https://static001.geekbang.org/account/avatar/00/12/52/40/e57a736e.jpg","comment_is_top":false,"comment_ctime":1586919385,"is_pvip":false,"replies":[{"id":"77231","content":"你很好地吸收了读写分离的思想。全量索引上肯定是不能加删除项的。不过可读写的增量索引上面能否加上删除标记呢？你可以想一想。<br>提示:<br>1.加的话是否有性能损失。<br>2.不管有没有性能损失，加上后，求检索结果的过程是怎么样的？加上这个标记和单独记录一个删除列表相比有帮助么？<br>你在思考以后，可以再看看我的回复。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586934679,"ip_address":"","comment_id":206709,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586919385","product_id":100048401,"comment_content":"按照索引的高性能选择，全量索性是只读的，而增量索引和删除项是可读可写的，所以不会选择在索引上添加删除项，会拉低系统效率。","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491874,"discussion_content":"你很好地吸收了读写分离的思想。全量索引上肯定是不能加删除项的。不过可读写的增量索引上面能否加上删除标记呢？你可以想一想。\n提示:\n1.加的话是否有性能损失。\n2.不管有没有性能损失，加上后，求检索结果的过程是怎么样的？加上这个标记和单独记录一个删除列表相比有帮助么？\n你在思考以后，可以再看看我的回复。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586934679,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206700,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1586918272,"is_pvip":false,"replies":[{"id":"77209","content":"这一步我没有画，不过你可以沿着前面天级索引和周级索引合并的思路思考一下，包括总结时我强调的“读写分离”去想想，我相信，你应该可以得到“不要加锁”这个结论的。<br><br>这里我也补充一点，就是周级到全量索引的合并，其实由于隔的时间已经很久了，因此，很多时候我们会直接完全重建全量索引，一方面，重建时间是足够的，另一方面，也等于定期给系统“重启”，保证系统的稳定性和正确性。","user_name":"作者回复","user_name_real":"陈东","uid":"1165703","ctime":1586922087,"ip_address":"","comment_id":206700,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1586918272","product_id":100048401,"comment_content":"在滚动更新中，周索引往全量索引更新的时候，需要加锁操作么？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491872,"discussion_content":"这一步我没有画，不过你可以沿着前面天级索引和周级索引合并的思路思考一下，包括总结时我强调的“读写分离”去想想，我相信，你应该可以得到“不要加锁”这个结论的。\n\n这里我也补充一点，就是周级到全量索引的合并，其实由于隔的时间已经很久了，因此，很多时候我们会直接完全重建全量索引，一方面，重建时间是足够的，另一方面，也等于定期给系统“重启”，保证系统的稳定性和正确性。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586922087,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}