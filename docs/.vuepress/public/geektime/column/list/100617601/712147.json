{"id":712147,"title":"15｜检索增强生成：通过RAG助力鲜花运营","content":"<p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>在<a href=\"https://time.geekbang.org/column/article/699436\">第2课</a>中，我曾经带着你完成了一个基于本地文档的问答系统。用当下时髦的话说，你实现了一个RAG 应用。</p><p>什么是RAG？其全称为Retrieval-Augmented Generation，即检索增强生成，它结合了检索和生成的能力，为文本序列生成任务引入外部知识。RAG将传统的语言生成模型与大规模的外部知识库相结合，使模型在生成响应或文本时可以动态地从这些知识库中检索相关信息。这种结合方法旨在增强模型的生成能力，使其能够产生更为丰富、准确和有根据的内容，特别是在需要具体细节或外部事实支持的场合。</p><p>RAG 的工作原理可以概括为几个步骤。</p><ol>\n<li><strong>检索</strong><strong>：</strong>对于给定的输入（问题），模型首先使用检索系统从大型文档集合中查找相关的文档或段落。这个检索系统通常基于密集向量搜索，例如ChromaDB、Faiss这样的向量数据库。</li>\n<li><strong>上下文编码</strong><strong>：</strong>找到相关的文档或段落后，模型将它们与原始输入（问题）一起编码。</li>\n<li><strong>生成</strong><strong>：</strong>使用编码的上下文信息，模型生成输出（答案）。这通常当然是通过大模型完成的。</li>\n</ol><p>RAG 的一个关键特点是，它不仅仅依赖于训练数据中的信息，还可以从大型外部知识库中检索信息。这使得RAG模型特别适合处理在训练数据中未出现的问题。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/f3/0d/f326343298bc0bc540978604203a3e0d.jpg?wh=4256x1472\" alt=\"\" title=\"RAG 的 Pipeline\"></p><p>RAG类的任务，目前企业实际应用场景中的需求量相当大，也是LangChain所关注的一个重点内容。在这节课中，我会对LangChain中所有与之相关的工具进行一个梳理，便于你把握LangChain在这个领域中都能够做到些什么。</p><h2>文档加载</h2><p>RAG的第一步是文档加载。LangChain 提供了多种类型的文档加载器，以加载各种类型的文档（HTML、PDF、代码），并与该领域的其他主要提供商如 Airbyte 和 Unstructured.IO 进行了集成。</p><p>下面给出常用的文档加载器列表。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/67/2af251fa78768b54a7d6a4a96423a867.jpg?wh=1584x656\" alt=\"\"></p><h2>文本转换</h2><p>加载文档后，下一个步骤是对文本进行转换，而最常见的文本转换就是把长文档分割成更小的块（或者是片，或者是节点），以适合模型的上下文窗口。LangChain 有许多内置的文档转换器，可以轻松地拆分、组合、过滤和以其他方式操作文档。</p><h3>文本分割器</h3><p>把长文本分割成块听起来很简单，其实也存在一些细节。文本分割的质量会影响检索的结果质量。理想情况下，我们希望将语义相关的文本片段保留在一起。</p><p>LangChain中，文本分割器的工作原理如下：</p><ol>\n<li>将文本分成小的、具有语义意义的块（通常是句子）。</li>\n<li>开始将这些小块组合成一个更大的块，直到达到一定的大小。</li>\n<li>一旦达到该大小，一个块就形成了，可以开始创建新文本块。这个新文本块和刚刚生成的块要有一些重叠，以保持块之间的上下文。</li>\n</ol><p>因此，LangChain提供的各种文本拆分器可以帮助你从下面几个角度设定你的分割策略和参数：</p><ol>\n<li>文本如何分割</li>\n<li>块的大小</li>\n<li>块之间重叠文本的长度</li>\n</ol><p>这些文本分割器的说明和示例如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/51/83/517c22ba8c7d78a755d5b29ec16d3e83.jpg?wh=2020x887\" alt=\"\"></p><p>你可能会关心，文本分割在实践，有哪些具体的考量因素，我总结了下面几点。</p><p><strong>首先，就是LLM 的具体限制。</strong>GPT-3.5-turbo支持的上下文窗口为4096个令牌，这意味着输入令牌和生成的输出令牌的总和不能超过4096，否则会出错。为了保证不超过这个限制，我们可以预留约2000个令牌作为输入提示，留下约2000个令牌作为返回的消息。这样，如果你提取出了五个相关信息块，那么每个片的大小不应超过400个令牌。</p><p><strong>此外，文本分割策略的选择和任务类型相关。</strong></p><ul>\n<li>需要细致查看文本的任务，最好使用较小的分块。例如，拼写检查、语法检查和文本分析可能需要识别文本中的单个单词或字符。垃圾邮件识别、查找剽窃和情感分析类任务，以及搜索引擎优化、主题建模中常用的关键字提取任务也属于这类细致任务。</li>\n<li>需要全面了解文本的任务，则使用较大的分块。例如，机器翻译、文本摘要和问答任务需要理解文本的整体含义。而自然语言推理、问答和机器翻译需要识别文本中不同部分之间的关系。还有创意写作，都属于这种粗放型的任务。</li>\n</ul><p><strong>最后，你也要考虑所分割的文本的性质。</strong>例如，如果文本结构很强，如代码或HTML，你可能想使用较大的块，如果文本结构较弱，如小说或新闻文章，你可能想使用较小的块。</p><p>你可以反复试验不同大小的块和块与块之间重叠窗口的大小，找到最适合你特定问题的解决方案。</p><h3>其他形式的文本转换</h3><p>除拆分文本之外，LangChain中还集成了各种工具对文档执行的其他类型的转换。下面让我们对其进行逐点分析。</p><ol>\n<li>过滤冗余的文档：使用 EmbeddingsRedundantFilter 工具可以识别相似的文档并过滤掉冗余信息。这意味着如果你有多份高度相似或几乎相同的文档，这个功能可以帮助识别并删除这些多余的副本，从而节省存储空间并提高检索效率。</li>\n<li>翻译文档：通过与工具 doctran 进行集成，可以将文档从一种语言翻译成另一种语言。</li>\n<li>提取元数据：通过与工具 doctran 进行集成，可以从文档内容中提取关键信息（如日期、作者、关键字等），并将其存储为元数据。元数据是描述文档属性或内容的数据，这有助于更有效地管理、分类和检索文档。</li>\n<li>转换对话格式：通过与工具 doctran 进行集成，可以将对话式的文档内容转化为问答（Q/A）格式，从而更容易地提取和查询特定的信息或回答。这在处理如访谈、对话或其他交互式内容时非常有用。</li>\n</ol><p>所以说，文档转换不仅限于简单的文本拆分，还可以包含附加的操作，这些操作的目的都是更好地准备和优化文档，以供后续生成更好的索引和检索功能。</p><h2>文本嵌入</h2><p>文本块形成之后，我们就通过LLM来做嵌入（Embeddings），将文本转换为数值表示，使得计算机可以更容易地处理和比较文本。OpenAI、Cohere、Hugging Face 中都有能做文本嵌入的模型。</p><p>Embeddings 会创建一段文本的向量表示，让我们可以在向量空间中思考文本，并执行语义搜索之类的操作，在向量空间中查找最相似的文本片段。</p><p><img src=\"https://static001.geekbang.org/resource/image/b5/ba/b54fc88694120820cd1afea29946d9ba.png?wh=1505x527\" alt=\"\" title=\"图片来源网络\"></p><p>LangChain中的Embeddings 类是设计用于与文本嵌入模型交互的类。这个类为所有这些提供者提供标准接口。</p><pre><code class=\"language-plain\"># 初始化Embedding类\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings()\n</code></pre><p>它提供两种方法：</p><ol>\n<li>第一种是 embed_documents 方法，为文档创建嵌入。这个方法接收多个文本作为输入，意味着你可以一次性将多个文档转换为它们的向量表示。</li>\n<li>第二种是 embed_query 方法，为查询创建嵌入。这个方法只接收一个文本作为输入，通常是用户的搜索查询。</li>\n</ol><p><strong>为</strong><strong>什么需要两种方法？</strong>虽然看起来这两种方法都是为了文本嵌入，但是LangChain将它们分开了。原因是一些嵌入提供者对于文档和查询使用的是不同的嵌入方法。文档是要被搜索的内容，而查询是实际的搜索请求。这两者可能因为其性质和目的，而需要不同的处理或优化。</p><p>embed_documents 方法的示例代码如下：</p><pre><code class=\"language-plain\">embeddings = embeddings_model.embed_documents(\n    [\n        \"您好，有什么需要帮忙的吗？\",\n        \"哦，你好！昨天我订的花几天送达\",\n        \"请您提供一些订单号？\",\n        \"12345678\",\n    ]\n)\nlen(embeddings), len(embeddings[0])\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">(4, 1536)\n</code></pre><p>embed_query 方法的示例代码如下：</p><pre><code class=\"language-plain\">embedded_query = embeddings_model.embed_query(\"刚才对话中的订单号是多少?\")\nembedded_query[:3]\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">[-0.0029746221837547455, -0.007710168602107487, 0.00923260021751183]\n</code></pre><h2>存储嵌入</h2><p>计算嵌入可能是一个时间消耗大的过程。为了加速这一过程，我们可以将计算出的嵌入存储或临时缓存，这样在下次需要它们时，就可以直接读取，无需重新计算。</p><h3>缓存存储</h3><p>CacheBackedEmbeddings是一个支持缓存的嵌入式包装器，它可以将嵌入缓存在键值存储中。具体操作是：对文本进行哈希处理，并将此哈希值用作缓存的键。</p><p>要初始化一个CacheBackedEmbeddings，主要的方式是使用from_bytes_store。其需要以下参数：</p><ul>\n<li>underlying_embedder：实际计算嵌入的嵌入器。</li>\n<li>document_embedding_cache：用于存储文档嵌入的缓存。</li>\n<li>namespace（可选）：用于文档缓存的命名空间，避免与其他缓存发生冲突。</li>\n</ul><p><strong>不同的缓存策略如下：</strong></p><ol>\n<li>InMemoryStore：在内存中缓存嵌入。主要用于单元测试或原型设计。如果需要长期存储嵌入，请勿使用此缓存。</li>\n<li>LocalFileStore：在本地文件系统中存储嵌入。适用于那些不想依赖外部数据库或存储解决方案的情况。</li>\n<li>RedisStore：在Redis数据库中缓存嵌入。当需要一个高速且可扩展的缓存解决方案时，这是一个很好的选择。</li>\n</ol><p>在内存中缓存嵌入的示例代码如下：</p><pre><code class=\"language-plain\"># 导入内存存储库，该库允许我们在RAM中临时存储数据\nfrom langchain.storage import InMemoryStore\n\n# 创建一个InMemoryStore的实例\nstore = InMemoryStore()\n\n# 导入与嵌入相关的库。OpenAIEmbeddings是用于生成嵌入的工具，而CacheBackedEmbeddings允许我们缓存这些嵌入\nfrom langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n\n# 创建一个OpenAIEmbeddings的实例，这将用于实际计算文档的嵌入\nunderlying_embeddings = OpenAIEmbeddings()\n\n# 创建一个CacheBackedEmbeddings的实例。\n# 这将为underlying_embeddings提供缓存功能，嵌入会被存储在上面创建的InMemoryStore中。\n# 我们还为缓存指定了一个命名空间，以确保不同的嵌入模型之间不会出现冲突。\nembedder = CacheBackedEmbeddings.from_bytes_store(\n&nbsp; &nbsp; underlying_embeddings,&nbsp; # 实际生成嵌入的工具\n&nbsp; &nbsp; store,&nbsp; # 嵌入的缓存位置\n&nbsp; &nbsp; namespace=underlying_embeddings.model&nbsp; # 嵌入缓存的命名空间\n)\n\n# 使用embedder为两段文本生成嵌入。\n# 结果，即嵌入向量，将被存储在上面定义的内存存储中。\nembeddings = embedder.embed_documents([\"你好\", \"智能鲜花客服\"])\n</code></pre><p>解释下这段代码。首先我们在内存中设置了一个存储空间，然后初始化了一个嵌入工具，该工具将实际生成嵌入。之后，这个嵌入工具被包装在一个缓存工具中，用于为两段文本生成嵌入。</p><p>至于其他两种缓存器，嵌入的使用方式也不复杂，你可以参考LangChain文档自行学习。</p><h3>向量数据库（向量存储）</h3><p>更常见的存储向量的方式是通过向量数据库（Vector Store）来保存它们。LangChain支持非常多种向量数据库，其中有很多是开源的，也有很多是商用的。比如Elasticsearch、Faiss、Chroma和Qdrant等等。</p><p>因为选择实在是太多了，我也给你列出来了一个表。</p><p><img src=\"https://static001.geekbang.org/resource/image/2e/77/2eb52480f790fd3281ae905ee1c58077.jpg?wh=2911x7734\" alt=\"\"></p><p>那么问题来了，面对这么多种类的向量数据库，应该如何选择呢？</p><p>这就涉及到许多技术和业务层面的考量，你应该<strong>根据具体需求进行选型</strong>。</p><ol>\n<li>数据规模和速度需求：考虑你的数据量大小以及查询速度的要求。一些向量数据库在处理大规模数据时更加出色，而另一些在低延迟查询中表现更好。</li>\n<li>持久性和可靠性：根据你的应用场景，确定你是否需要数据的高可用性、备份和故障转移功能。</li>\n<li>易用性和社区支持：考虑向量数据库的学习曲线、文档的完整性以及社区的活跃度。</li>\n<li>成本：考虑总体拥有成本，包括许可、硬件、运营和维护成本。</li>\n<li>特性：考虑你是否需要特定的功能，例如多模态搜索等。</li>\n<li>安全性：确保向量数据库符合你的安全和合规要求。</li>\n</ol><p>在进行向量数据库的评测时，进行<strong>性能基准测试</strong>是了解向量数据库实际表现的关键。这可以帮助你评估查询速度、写入速度、并发性能等。</p><p>没有“最好”的向量数据库，只有“最适合”的向量数据库。在你的需求上做些研究和测试，确保你选择的向量数据库满足你的业务和技术要求就好。</p><h2>数据检索</h2><p>在LangChain中，Retriever，也就是检索器，是数据检索模块的核心入口，它通过非结构化查询返回相关的文档。</p><h3>向量存储检索器</h3><p>向量存储检索器是最常见的，它主要支持向量检索。当然LangChain也有支持其他类型存储格式的检索器。</p><p>下面实现一个端到端的数据检索功能，我们通过VectorstoreIndexCreator来创建索引，并在索引的query方法中，通过vectorstore类的as_retriever方法，把向量数据库（Vector Store）直接作为检索器，来完成检索任务。</p><pre><code class=\"language-plain\"># 设置OpenAI的API密钥\nimport os\nos.environ[\"OPENAI_API_KEY\"] = 'Your OpenAI Key'\n\n# 导入文档加载器模块，并使用TextLoader来加载文本文件\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader('LangChainSamples/OneFlower/易速鲜花花语大全.txt', encoding='utf8')\n\n# 使用VectorstoreIndexCreator来从加载器创建索引\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator().from_loaders([loader])\n\n# 定义查询字符串, 使用创建的索引执行查询\nquery = \"玫瑰花的花语是什么？\"\nresult = index.query(query)\nprint(result) # 打印查询结果\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">玫瑰花的花语是爱情、热情、美丽。\n</code></pre><p>你可能会觉得，这个数据检索过程太简单了。这就要归功于LangChain的强大封装能力。如果我们审视一下位于vectorstore.py中的VectorstoreIndexCreator类的代码，你就会发现，它其中封装了vectorstore、embedding以及text_splitter，甚至document loader（如果你使用from_documents方法的话）。</p><pre><code class=\"language-plain\">class VectorstoreIndexCreator(BaseModel):\n&nbsp; &nbsp; \"\"\"Logic for creating indexes.\"\"\"\n\n&nbsp; &nbsp; vectorstore_cls: Type[VectorStore] = Chroma\n&nbsp; &nbsp; embedding: Embeddings = Field(default_factory=OpenAIEmbeddings)\n&nbsp; &nbsp; text_splitter: TextSplitter = Field(default_factory=_get_default_text_splitter)\n&nbsp; &nbsp; vectorstore_kwargs: dict = Field(default_factory=dict)\n\n&nbsp; &nbsp; class Config:\n&nbsp; &nbsp; &nbsp; &nbsp; \"\"\"Configuration for this pydantic object.\"\"\"\n\n&nbsp; &nbsp; &nbsp; &nbsp; extra = Extra.forbid\n&nbsp; &nbsp; &nbsp; &nbsp; arbitrary_types_allowed = True\n\n&nbsp; &nbsp; def from_loaders(self, loaders: List[BaseLoader]) -&gt; VectorStoreIndexWrapper:\n&nbsp; &nbsp; &nbsp; &nbsp; \"\"\"Create a vectorstore index from loaders.\"\"\"\n&nbsp; &nbsp; &nbsp; &nbsp; docs = []\n&nbsp; &nbsp; &nbsp; &nbsp; for loader in loaders:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; docs.extend(loader.load())\n&nbsp; &nbsp; &nbsp; &nbsp; return self.from_documents(docs)\n\n&nbsp; &nbsp; def from_documents(self, documents: List[Document]) -&gt; VectorStoreIndexWrapper:\n&nbsp; &nbsp; &nbsp; &nbsp; \"\"\"Create a vectorstore index from documents.\"\"\"\n&nbsp; &nbsp; &nbsp; &nbsp; sub_docs = self.text_splitter.split_documents(documents)\n&nbsp; &nbsp; &nbsp; &nbsp; vectorstore = self.vectorstore_cls.from_documents(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sub_docs, self.embedding, **self.vectorstore_kwargs\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; return VectorStoreIndexWrapper(vectorstore=vectorstore)\n</code></pre><p>因此，上面的检索功能就相当于我们第2课中讲过的一系列工具的整合。而我们也可以用下面的代码，来显式地指定索引创建器的vectorstore、embedding以及text_splitter，并把它们替换成你所需要的工具，比如另外一种向量数据库或者别的Embedding模型。</p><pre><code class=\"language-plain\">from langchain.text_splitter import CharacterTextSplitter\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\nindex_creator = VectorstoreIndexCreator(\n&nbsp; &nbsp; vectorstore_cls=Chroma,\n&nbsp; &nbsp; embedding=OpenAIEmbeddings(),\n&nbsp; &nbsp; text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n)\n</code></pre><p>那么，下一个问题是 index.query(query)，又是如何完成具体的检索及文本生成任务的呢？我们此处既没有看到大模型，又没有看到LangChain的文档检索工具（比如我们在第2课中见过的QARetrival链）。</p><p>秘密仍然存在于源码中，在VectorStoreIndexWrapper类的query方法中，可以看到，在调用方法的同时，RetrievalQA链被启动，以完成检索功能。</p><pre><code class=\"language-plain\">class VectorStoreIndexWrapper(BaseModel):\n&nbsp; &nbsp; \"\"\"Wrapper around a vectorstore for easy access.\"\"\"\n\n&nbsp; &nbsp; vectorstore: VectorStore\n\n&nbsp; &nbsp; class Config:\n&nbsp; &nbsp; &nbsp; &nbsp; \"\"\"Configuration for this pydantic object.\"\"\"\n\n&nbsp; &nbsp; &nbsp; &nbsp; extra = Extra.forbid\n&nbsp; &nbsp; &nbsp; &nbsp; arbitrary_types_allowed = True\n\n&nbsp; &nbsp; def query(\n&nbsp; &nbsp; &nbsp; &nbsp; self,\n&nbsp; &nbsp; &nbsp; &nbsp; question: str,\n&nbsp; &nbsp; &nbsp; &nbsp; llm: Optional[BaseLanguageModel] = None,\n&nbsp; &nbsp; &nbsp; &nbsp; retriever_kwargs: Optional[Dict[str, Any]] = None,\n&nbsp; &nbsp; &nbsp; &nbsp; **kwargs: Any\n&nbsp; &nbsp; ) -&gt; str:\n&nbsp; &nbsp; &nbsp; &nbsp; \"\"\"Query the vectorstore.\"\"\"\n&nbsp; &nbsp; &nbsp; &nbsp; llm = llm or OpenAI(temperature=0)\n&nbsp; &nbsp; &nbsp; &nbsp; retriever_kwargs = retriever_kwargs or {}\n&nbsp; &nbsp; &nbsp; &nbsp; chain = RetrievalQA.from_chain_type(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs\n&nbsp; &nbsp; &nbsp; &nbsp; )\n&nbsp; &nbsp; &nbsp; &nbsp; return chain.run(question)\n</code></pre><p>上面我们用到的向量存储检索器，是向量存储类的轻量级包装器，使其符合检索器接口。它使用向量存储中的搜索方法（例如相似性搜索和 MMR）来查询向量存储中的文本。</p><h3>各种类型的检索器</h3><p>除向量存储检索器之外，LangChain中还提供很多种其他的检索工具。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/a8/f87c2d22bb1e71419ee129c9871724a8.jpg?wh=1761x1210\" alt=\"\"></p><p>这些检索工具，各有其功能特点，你可以查找它们的文档说明，并尝试使用。</p><h2>索引</h2><p>在本节课的最后，我们来看看LangChain中的索引（Index）。简单的说，索引是一种高效地管理和定位文档信息的方法，确保每个文档具有唯一标识并便于检索。</p><p>尽管在<a href=\"https://time.geekbang.org/column/article/699436\">第2课</a>的示例中，我们并没有显式的使用到索引就完成了一个RAG任务，但在复杂的信息检索任务中，有效地管理和索引文档是关键的一步。LangChain 提供的索引 API 为开发者带来了一个高效且直观的解决方案。具体来说，它的优势包括：</p><ul>\n<li>避免重复内容：确保你的向量存储中不会有冗余数据。</li>\n<li>只更新更改的内容：能检测哪些内容已更新，避免不必要的重写。</li>\n<li>省时省钱：不对未更改的内容重新计算嵌入，从而减少了计算资源的消耗。</li>\n<li>优化搜索结果：减少重复和不相关的数据，从而提高搜索的准确性。</li>\n</ul><p>LangChain 利用了记录管理器（RecordManager）来跟踪哪些文档已经被写入向量存储。</p><p>在进行索引时，API 会对每个文档进行哈希处理，确保每个文档都有一个唯一的标识。这个哈希值不仅仅基于文档的内容，还考虑了文档的元数据。</p><p>一旦哈希完成，以下信息会被保存在记录管理器中：</p><ul>\n<li>文档哈希：基于文档内容和元数据计算出的唯一标识。</li>\n<li>写入时间：记录文档何时被添加到向量存储中。</li>\n<li>源 ID：这是一个元数据字段，表示文档的原始来源。</li>\n</ul><p>这种方法确保了即使文档经历了多次转换或处理，也能够精确地跟踪它的状态和来源，确保文档数据被正确管理和索引。</p><h2>总结时刻</h2><p>这节课的内容非常多，而且我给出了很多表格供你查询之用，信息量很大。同时，你可以复习<a href=\"https://time.geekbang.org/column/article/699436\">第2课</a>的内容，我希望你对RAG的流程有个更深的理解。</p><p>通过检索增强生成来存储和搜索非结构化数据的最常见方法是，给这些非结构化的数据做嵌入并存储生成的嵌入向量，然后在查询时给要查询的文本也做嵌入，并检索与嵌入查询“最相似”的嵌入向量。向量数据库则负责存储嵌入数据，并为你执行向量的搜索。</p><p><img src=\"https://static001.geekbang.org/resource/image/39/84/39ab4b67b2689e6daf9a83bc5895b684.jpg?wh=4096x1701\" alt=\"\"></p><p>你看，RAG实际上是为非结构化数据创建了一个“地图”。当用户有查询请求时，该查询同样被嵌入，然后你的应用程序会在这个“地图”中寻找与之最匹配的位置，从而快速准确地检索信息。</p><p>在我们的鲜花运营场景中，RAG当然可以在很多方面发挥巨大的作用。你的鲜花有各种各样的品种、颜色和花语，这些数据往往是自然的、松散的，也就是非结构化的。使用RAG，你可以通过嵌入向量，把库存的鲜花与相关的非结构化信息（如花语、颜色、产地等）关联起来。当客户或者员工想要查询某种鲜花的信息时，系统可以快速地提供准确的答案。</p><p>此外，RAG还可以应用于订单管理。每个订单，无论是客户的姓名、地址、购买的鲜花种类，还是订单状态，都可以被视为非结构化数据。通过RAG，我们可以轻松地嵌入并检索这些订单，为客户提供实时的订单更新、跟踪和查询服务。</p><p>当然，对于订单这样的信息，更常见的情况仍是把它们组织成结构化的数据，存储在数据库中（至少也是CSV或者Excel表中），以便高效、精准地查询。那么，LLM能否帮助我们查询数据库表中的条目呢？在下一课中，我将为你揭晓答案。</p><h2>思考题</h2><ol>\n<li>请你尝试使用一种文本分割器来给你的文档分块。</li>\n<li>请你尝试使用一种新的向量数据库来存储你的文本嵌入。</li>\n<li>请你尝试使用一种新的检索器来提取信息。</li>\n</ol><p>期待在留言区看到你的实践成果，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2>延伸阅读</h2><ol>\n<li>Github：<a href=\"https://github.com/psychic-api/doctran/tree/main\">doctran</a>，辅助LangChain进行文本转换</li>\n<li>文档：LangChain中 <a href=\"https://python.langchain.com/docs/modules/data_connection/indexing\">Indexing</a> 的说明</li>\n</ol>","neighbors":{"left":{"article_title":"14｜工具和工具箱：LangChain中的Tool和Toolkits一览","id":709523},"right":{"article_title":"16｜连接数据库：通过链和代理查询鲜花信息","id":713462}},"comments":[{"had_liked":false,"id":382506,"user_name":"Geek_f55576","can_delete":false,"product_type":"c1","uid":1833846,"ip_address":"浙江","ucode":"E110971B78CF1E","user_header":"","comment_is_top":false,"comment_ctime":1697501155,"is_pvip":false,"replies":[{"id":139339,"content":"有很多具体的策略：\n1. 一般来说，都可以在切片时设置上下文的重叠区域，也就是重叠窗口。可以设大一点。\n2. 有一些包支持把上一个片，和下一个片，都作为Metadata来保存在索引中，这样，检索出一个片，也就同时检索了上下文。\n3. 如果你的文档都是一问，一答的形式。你可以自己修改切片逻辑，刻意的把问答捆绑在一起。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697616814,"ip_address":"瑞士","comment_id":382506,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师如果对文件切片时候，文件中的的问题和回答被切成2个不同的chunck那么经常会无法检索到答案，这种场景有什么优化方法吗？","like_count":3,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629770,"discussion_content":"有很多具体的策略：\n1. 一般来说，都可以在切片时设置上下文的重叠区域，也就是重叠窗口。可以设大一点。\n2. 有一些包支持把上一个片，和下一个片，都作为Metadata来保存在索引中，这样，检索出一个片，也就同时检索了上下文。\n3. 如果你的文档都是一问，一答的形式。你可以自己修改切片逻辑，刻意的把问答捆绑在一起。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697616814,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":389859,"user_name":"万万没想到","can_delete":false,"product_type":"c1","uid":2789057,"ip_address":"山东","ucode":"2F9BA81315D3C7","user_header":"https://static001.geekbang.org/account/avatar/00/2a/8e/c1/ab1110f8.jpg","comment_is_top":false,"comment_ctime":1713693276,"is_pvip":false,"replies":[{"id":142311,"content":"要大模型整合，问题，知识，检索结果，——》最终生成人类能懂的“客服”语言。比如说，谢谢，这就是我检索到的，有其他问题继续问我哟！！——这些。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1717952890,"ip_address":"新加坡","comment_id":389859,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"还是不能明白gpt这样的语言模型在RAG中的作用、不是直接在向量数据库中就查出来了么、最后一步为什么要丢给gpt输出、而不是直接输出","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646429,"discussion_content":"要大模型整合，问题，知识，检索结果，——》最终生成人类能懂的“客服”语言。比如说，谢谢，这就是我检索到的，有其他问题继续问我哟！！——这些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1717952890,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1023093,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9c/75/525e53b1.jpg","nickname":"Hobby","note":"","ucode":"8954830380D44F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":643799,"discussion_content":"这是传统搜索引擎与加持了GPT的Bing的区别，是只给作业答案与提供老师讲解的作业解答的区别。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1714636844,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3881055,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEJB0SU5GGD0QGFAQiaLcqf6G1zkibWtNlK7YwJ7Am9KE6xlwVwRR6sLicYrRWNpLM9ZpicgXJ1IuUYYIibxZuLYkfQOdBicw5ibSh4GibiaCnW6ccIJ3vA/132","nickname":"Geek_69d6c4","note":"","ucode":"498C6F89D5E961","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":643065,"discussion_content":"直接输出那就不是生成式了，而是搜索？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1714014064,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"拉脱维亚","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382323,"user_name":"Lominnave","can_delete":false,"product_type":"c1","uid":1066748,"ip_address":"浙江","ucode":"0674B630718AD9","user_header":"https://static001.geekbang.org/account/avatar/00/10/46/fc/147e38d9.jpg","comment_is_top":false,"comment_ctime":1697073786,"is_pvip":false,"replies":[{"id":139274,"content":"当然可以了。稀疏向量是指一个大多数元素都是零（或未定义、null）的向量。与之相对的是密集向量，其中大部分元素都是非零的。稀疏向量在很多场合都非常有用，特别是在高维数据中，其中很多维度对于某些观察值来说可能是不相关或缺失的。例如，在自然语言处理中，词袋模型（Bag-of-Words, BoW）产生的是稀疏向量，因为大量的词在特定文档中可能没有出现。稀疏向量的优点：存储效率：由于向量中的大部分值都是零，我们只需要存储非零的元素及其索引，这大大节省了存储空间。计算效率：某些算法可以优化处理稀疏数据，从而更快地运行。\n稀疏向量是指一个大多数元素都是零（或未定义、null）的向量。与之相对的是密集向量，其中大部分元素都是非零的。稀疏向量在很多场合都非常有用，特别是在高维数据中，其中很多维度对于某些观察值来说可能是不相关或缺失的。\n\n例如，在自然语言处理中，词袋模型（Bag-of-Words, BoW）产生的是稀疏向量，因为大量的词在特定文档中可能没有出现。\n\n稀疏向量的优点：\n存储效率：由于向量中的大部分值都是零，我们只需要存储非零的元素及其索引，这大大节省了存储空间。\n计算效率：某些算法可以优化处理稀疏数据，从而更快地运行。\n如果同学想往深了学Embedding的原理，可以看一个我的视频课：“生成式预训练语言模型：理论与实战 黄佳”。搜一下就出来了。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697165813,"ip_address":"瑞士","comment_id":382323,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师可以介绍下稀疏向量么，另外稀疏-密集向量结合如何提高召回效果？","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629516,"discussion_content":"当然可以了。稀疏向量是指一个大多数元素都是零（或未定义、null）的向量。与之相对的是密集向量，其中大部分元素都是非零的。稀疏向量在很多场合都非常有用，特别是在高维数据中，其中很多维度对于某些观察值来说可能是不相关或缺失的。例如，在自然语言处理中，词袋模型（Bag-of-Words, BoW）产生的是稀疏向量，因为大量的词在特定文档中可能没有出现。稀疏向量的优点：存储效率：由于向量中的大部分值都是零，我们只需要存储非零的元素及其索引，这大大节省了存储空间。计算效率：某些算法可以优化处理稀疏数据，从而更快地运行。\n稀疏向量是指一个大多数元素都是零（或未定义、null）的向量。与之相对的是密集向量，其中大部分元素都是非零的。稀疏向量在很多场合都非常有用，特别是在高维数据中，其中很多维度对于某些观察值来说可能是不相关或缺失的。\n\n例如，在自然语言处理中，词袋模型（Bag-of-Words, BoW）产生的是稀疏向量，因为大量的词在特定文档中可能没有出现。\n\n稀疏向量的优点：\n存储效率：由于向量中的大部分值都是零，我们只需要存储非零的元素及其索引，这大大节省了存储空间。\n计算效率：某些算法可以优化处理稀疏数据，从而更快地运行。\n如果同学想往深了学Embedding的原理，可以看一个我的视频课：“生成式预训练语言模型：理论与实战 黄佳”。搜一下就出来了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697165813,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382902,"user_name":"zjl","can_delete":false,"product_type":"c1","uid":3020164,"ip_address":"北京","ucode":"A0678190770C44","user_header":"https://static001.geekbang.org/account/avatar/00/2e/15/84/2734c72c.jpg","comment_is_top":false,"comment_ctime":1698168026,"is_pvip":false,"replies":[{"id":139517,"content":"可以啊，把Retrival模块和ReAct代理结合起来。你可以构建自己的Custom Tool。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698375110,"ip_address":"瑞士","comment_id":382902,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"langchain有没有调用本地检索的tool呢？或许可以尝试用本地检索去做reAct","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630376,"discussion_content":"可以啊，把Retrival模块和ReAct代理结合起来。你可以构建自己的Custom Tool。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698375110,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2851308,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epEexjZIhNpYNiaAibdLD0Jsl797U6hianjrDs2QT4Q4HOicIEeILxjOcEF7gXGyQeJRJHaeenibb3N9QQ/132","nickname":"enbool","note":"","ucode":"BB460DCCA87099","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630259,"discussion_content":"应该在tools 里面，或者自己实现","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698221943,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382304,"user_name":"阿斯蒂芬","can_delete":false,"product_type":"c1","uid":1024164,"ip_address":"广东","ucode":"61D5E3BDA4EBC5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg","comment_is_top":false,"comment_ctime":1697018432,"is_pvip":false,"replies":[{"id":139273,"content":"同学的问题特别好。这边LangChain并没有做特别的处理，而目前的LLM还无法处理超长的文本。那么，首先：你的问题要非常细。解读心理活动，解读那部分？哪年哪月？哪个环境？你泛泛，回答肯定不准。你问题细，就有可能检索出来相关的嵌入块。另外，工程上需要设计Metadata给每个块，需要做Summary，需要考虑递归式的检索策略，分层的检索策略。这是另一个大课题了。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697165468,"ip_address":"瑞士","comment_id":382304,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"【第二种是 embed_query 方法，为查询创建嵌入】一直有个疑问，如果query是需要“复杂理解”的，那么是怎么通过“相似度”去match到文档内容的呢。比如文档是一片小说，而query是：请解读文中描写主人翁心理活动的部分？这里面 LangChain 是否做了特殊处理？","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629514,"discussion_content":"同学的问题特别好。这边LangChain并没有做特别的处理，而目前的LLM还无法处理超长的文本。那么，首先：你的问题要非常细。解读心理活动，解读那部分？哪年哪月？哪个环境？你泛泛，回答肯定不准。你问题细，就有可能检索出来相关的嵌入块。另外，工程上需要设计Metadata给每个块，需要做Summary，需要考虑递归式的检索策略，分层的检索策略。这是另一个大课题了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1697165468,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1024164,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg","nickname":"阿斯蒂芬","note":"","ucode":"61D5E3BDA4EBC5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":629756,"discussion_content":"感谢老师指点，又打开了新的思考方向。\n1. “问题要细”，这是工程层面可以解决的吗？还是说要可能要借助一个专门的小模型来扩展 prompt 之类？还是说增加一些 few-shot prompt 也可以提升效果（想来这个我应该亲自实践比较好）？\n2. 从模型层面看，针对性的对这个文学QA场景做微调，是否也能起到提升效果？\n3. metadata、summary、递归式检索、分层检索，这些本课程会有提及吗，如果没有，未来老师有课吗 😄","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697599367,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":629514,"ip_address":"广东","group_id":0},"score":629756,"extra":""}]},{"author":{"id":1070253,"avatar":"https://static001.geekbang.org/account/avatar/00/10/54/ad/6ee2b7cb.jpg","nickname":"Jacob.C","note":"","ucode":"034998E7A7CCD1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633216,"discussion_content":"好问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701913658,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382284,"user_name":"感性即自然的理性","can_delete":false,"product_type":"c1","uid":3716675,"ip_address":"北京","ucode":"61C42C41493132","user_header":"https://static001.geekbang.org/account/avatar/00/38/b6/43/8ee89342.jpg","comment_is_top":false,"comment_ctime":1697004263,"is_pvip":false,"replies":[{"id":139270,"content":"好的，同学的意思是利用Streamlit这种WebUI框架开发交互式的机器学习应用是么？如果是的，我们最后的项目会有一些这部分的内容。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1697159179,"ip_address":"瑞士","comment_id":382284,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"希望老师可以最后的内容里面涉及到相关交互页面的内容，就是图形页面的内容","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629507,"discussion_content":"好的，同学的意思是利用Streamlit这种WebUI框架开发交互式的机器学习应用是么？如果是的，我们最后的项目会有一些这部分的内容。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697159179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384473,"user_name":"日暮途远","can_delete":false,"product_type":"c1","uid":1551030,"ip_address":"上海","ucode":"2A1FBE7894BF35","user_header":"https://static001.geekbang.org/account/avatar/00/17/aa/b6/9d021ab4.jpg","comment_is_top":false,"comment_ctime":1701003251,"is_pvip":false,"replies":[{"id":140933,"content":"好的，多谢同学，马上修改文字！","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1705283782,"ip_address":"瑞士","comment_id":384473,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"embed_documents和embed_query这一块的文档是不是写错了？出现了2次「embed_documents 方法的示例代码如下：」","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635830,"discussion_content":"好的，多谢同学，马上修改文字！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705283783,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383171,"user_name":"qkyong","can_delete":false,"product_type":"c1","uid":1118978,"ip_address":"广东","ucode":"641B0E9D9BC170","user_header":"https://static001.geekbang.org/account/avatar/00/11/13/02/cb050516.jpg","comment_is_top":false,"comment_ctime":1698626725,"is_pvip":false,"replies":[{"id":139609,"content":"同学具体指得那段代码。一般来说text-splitter是生成矢量存储片之前用到的工具。检索时候，就直接计算向量距离。","user_name":"作者回复","user_name_real":"作者","uid":1809833,"ctime":1698771662,"ip_address":"瑞士","comment_id":383171,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"检索时为啥要指定text-splitter？理论上直接计算向量距离呀","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630681,"discussion_content":"同学具体指得那段代码。一般来说text-splitter是生成矢量存储片之前用到的工具。检索时候，就直接计算向量距离。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698771662,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393460,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1723892182,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"用百川智能嵌入模型改写文本嵌入、存储嵌入\n\n## 旧代码1：\nfrom langchain.embeddings import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings()\n## 新代码1：\nfrom langchain_community.embeddings import BaichuanTextEmbeddings  # 导入百川模型\nembeddings_model = BaichuanTextEmbeddings(baichuan_api_key=&quot;key&quot;) # 初始化百川模型。KEY 用你的key代替\n\n## 旧代码2：\nunderlying_embeddings = OpenAIEmbeddings()\nnamespace=underlying_embeddings.model \n## 新代码2：\nunderlying_embeddings=embeddings_model\nnamespace=&quot;baichuan_embeddings&quot;   # 用自定义命名嵌入缓存的命名空间\n\n### 新代码3：采用其他文本分割器、向量数据库、检索器的的代码\n\n# 导入文档\nfrom langchain_community.document_loaders import Docx2txtLoader\nloader = Docx2txtLoader(&#39;.&#47;建筑垃圾处理技术标准.docx&#39;)\n\n# 导入初始化百川模型\nfrom langchain_community.embeddings import BaichuanTextEmbeddings\nembeddings_model = BaichuanTextEmbeddings(baichuan_api_key=&quot;KEY&quot;) #此处用你的key代替\n\n# 从加载器创建索引\nfrom langchain.indexes import VectorstoreIndexCreator\nindex = VectorstoreIndexCreator(embedding=embeddings_model).from_loaders([loader])\n\n# llm用阿里千问大模型替换\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(\n    api_key=&quot;key&quot;, # 用您的KEY替换\n    base_url=&quot;https:&#47;&#47;dashscope.aliyuncs.com&#47;compatible-mode&#47;v1&quot;, \n    model=&#39;qwen-long&#39; \n    )\n\n# 替换文本分割器 \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n\n# 替换向量存储 \nfrom langchain_community.vectorstores import LanceDB\nembeddings = embeddings_model\n\n# 从加载器创建索引 \nindex_creator = VectorstoreIndexCreator(\n    vectorstore_cls= LanceDB, # 使用LanceDB向量存储替换\n    embedding=embeddings, # 使用百川智能嵌入模型替换\n    text_splitter=text_splitter # 使用RecursiveCharacterTextSplitter文本分割器\n)\n\nquery = &quot;各类建筑垃圾如何处理？&quot;\nindex = index_creator.from_loaders([loader])\n# 在query需要增加大语言模型llm\nresult = index.query(llm = llm, question = query)\nprint(result)","like_count":2},{"had_liked":false,"id":394996,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"北京","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1729054629,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"# 文档加载器列表的图片表格中，示例代码的显示出现一些小错误。应该分隔的代码没有用空格分隔，单词连接在一起。如加载文本文档的代码：\nfrom langchain.document_loaders import TextLoaderloader =\nTextloader(&quot;example_data&#47;index.md&quot;)data = loader.load()\n\n代码显示错误，应该在其中增加空格的，和增加换行：\nfrom langchain.document_loaders import TextLoader\nloader = Textloader(&quot;example_data&#47;index.md&quot;) \ndata= loader.load()\n\n# 文档加载和文本分割的示例代码都存在类似分隔、换行显示问题，建议修订。","like_count":1},{"had_liked":false,"id":387984,"user_name":"ManerFan","can_delete":false,"product_type":"c1","uid":1055586,"ip_address":"北京","ucode":"9C7164E19E0284","user_header":"https://static001.geekbang.org/account/avatar/00/10/1b/62/81a5a17d.jpg","comment_is_top":false,"comment_ctime":1709099650,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"相比 langchain的Retrival和llama index，应该如何选择呢？","like_count":1},{"had_liked":false,"id":392601,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1721274669,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":2,"score":3,"product_id":100617601,"comment_content":"第15讲打卡~\n一个典型的RAG系统的核心处理流程：Loading -&gt; Transform(Splitting) -&gt; Embedding -&gt; Store -&gt; Retrieving","like_count":0},{"had_liked":false,"id":392351,"user_name":"Geek_2d85f8","can_delete":false,"product_type":"c1","uid":3917682,"ip_address":"江苏","ucode":"4C155E04676A75","user_header":"","comment_is_top":false,"comment_ctime":1720666967,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"由于langchain的更新，课程里的VectorstoreIndexCreator使用时需要增加embedding，如：\nindex = VectorstoreIndexCreator(embedding=OpenAIEmbeddings()).from_loaders([loader])\n同时在index.query中，也需要增加大语言模型llm（需提前创建），如：\nindex.query(llm = llm, question = query)","like_count":0}]}