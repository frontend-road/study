{"id":741045,"title":"第 20 章 Linux集群","content":"\n<p>一台服务器的硬件配置总是有限的，当服务器上运行的资源超过服务器的承载能力时，必将导致该服务器崩溃。在生产环境中，多数企业会使用多台服务器搭建成一个集群来运行应用程序，这样不仅可以避免单点故障，还能提升服务器的承载能力。</p>\n<p>腾讯的微信软件在国内使用频繁，据腾讯官方提供的数据报告显示，2019年月活跃用户数超过11.5亿。然而，我们很少发现它出现故障。这么大体量的应用，不可能在一台或者几台服务器上运行起来，事实上有数以万计的服务器在微信的后端支撑着。据不完全统计，仅微信这项业务，几乎每天都会有若干台服务器出现故障，但这并没有影响到微信的使用，背后的技术其实就是集群。</p>\n<p>集群从功能实现上分为两种：高可用集群和负载均衡集群。高可用，顾名思义，当一台服务器死机不能提供服务了，还有另外的服务器顶替。就像阿铭刚刚提到的，微信所使用的服务器虽然每天都有死机的，但对于用户来讲是无感知的，并没有影响到使用。负载均衡集群，简单讲就是把用户的请求分摊到多台服务器上，微信那么多用户使用，它就是把众多用户分摊在了不同的服务器上，假如说一台服务器上可以承载一万用户，那么一万台服务器上就可以承载1亿用户。</p>\n<h2 id=\"nav_point_374\">20.1 搭建高可用集群</h2>\n<p>高可用集群，即“HA集群”，也常称作“双机热备”，用于关键业务。常见实现高可用的开源软件有heartbeat和keepalived，其中keepalived还有负载均衡的功能。这两个软件类似，核心原理都是通过心跳线连接两台服务器，正常情况下由一台服务器提供服务，当这台服务器死机时，由备用服务器顶替。</p><!-- [[[read_end]]] -->\n<p>heartbeat软件在2010年就停止了更新，所以在本节试验中阿铭将采用keepalived实现高可用集群。</p>\n<h3 id=\"nav_point_375\">20.1.1 keepalived的工作原理</h3>\n<p>在讲述keepalived的工作原理之前，阿铭先介绍一个协议VRRP（Virtual Router Redundancy Protocol，虚拟路由冗余协议）。它是实现路由高可用的一种通信协议，在这个协议里会将多台功能相同的路由器组成一个小组，这个小组里会有1个master（主）角色和<em>N</em>（<em>N</em>≥1）个backup（备用）角色。工作时，master会通过组播的形式向各个backup发送VRRP协议的数据包，当backup收不到master发来的VRRP数据包时，就会认为master死机了。此时就需要根据各个backup的优先级来决定谁成为新的master。</p>\n<p>而keepalived就是采用这种VRRP协议实现的高可用。keepalived主要有三个模块，分别是core、check和vrrp。其中core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析；check模块负责健康检查；vrrp模块用来实现VRRP协议。</p>\n<h3 id=\"nav_point_376\">20.1.2 安装keepalived</h3>\n<p>刚才提到VRRP协议有1个master角色和至少1个backup角色，所以做本试验需要准备至少两台Linux机器。阿铭拿两台Linux虚拟机192.168.72.128（以下简称128）和192.168.72.129（以下简称129）来完成以下操作，其中128作为master，129作为backup。</p>\n<p>在两台机器上执行如下操作：</p>\n<pre class=\"code-rows\"><code># yum install –y keepalived</code></pre>\n<p>CentOS默认的<code>yum</code>源里就有keepalived包，版本虽然有点老，但不影响使用，当前官方最新版为2.1.3，阿铭<code>yum</code>安装的版本为2.0.10。安装keepalived虽然很简单，但重点在于配置，阿铭将会拿一个实际案例来阐述keepalived的高可用功能。</p>\n<h3 id=\"nav_point_377\">20.1.3 keepalived＋Nginx实现Web高可用</h3>\n<p>在生产环境中，诸多企业把Nginx作为负载均衡器来用，它的重要性很高，一旦死机会导致整个站点不能访问，所以有必要再准备一台备用Nginx，keepalived用在这种场景下非常合适。阿铭管理的业务也是这样，用两台Nginx做负载均衡，每一台上面都安装了keepalived，也就是说keepalived和Nginx安装在一起。在配置之前，阿铭先把两台机器的IP、角色罗列一下，这样你理解起来应该会容易一些。</p>\n<p>master：192.168.72.128安装keepalived＋Nginx</p>\n<p>backup：192.168.72.129安装keepalived＋Nginx</p>\n<p>VIP：192.168.72.100</p>\n<p>VIP对你来说是一个新概念，它的英文名字是“Virtual IP”，即“虚拟IP”，也有人把它叫作“浮动IP”。因为这个IP是由keepalived给服务器配置的，服务器靠这个VIP对外提供服务，当master机器死机，VIP就被分配到backup上，这样用户看来是无感知的。</p>\n<p>编辑master（128）的keepalived配置文件：</p>\n<pre class=\"code-rows\"><code># vim /etc/keepalived/keepalived.conf // 更改成如下内容\nglobal_defs {\n notification_email {\n aming@aminglinux.com // 定义接收告警的人\n }\n notification_email_from root@aminglinux.com // 定义发邮件地址（实际上没用）\n smtp_server 127.0.0.1 // 定义发邮件地址，若为127.0.0.1则使用本机自带邮件服务器发送\n smtp_connect_timeout 30\n router_id LVS_DEVEL\n}\n\nvrrp_script chk_nginx { // chk_nginx为自定义名字，后面还会用到它\n script \"/usr/local/sbin/check_ng.sh\" // 自定义脚本，该脚本为监控Nginx服务的脚本\n interval 3 // 每隔3s执行一次该脚本\n}\n\nvrrp_instance VI_1 {\n state MASTER // 角色为master\n interface ens33 // 针对哪个网卡监听VIP\n virtual_router_id 51\n priority 100 // 权重为100，master的权重要比backup大\n advert_int 1\n authentication {\n auth_type PASS\n auth_pass aminglinux&gt;com // 定义密码，这个密码自定义\n }\n virtual_ipaddress {\n 192.168.72.100 // 定义VIP\n }\n\n track_script {\n chk_nginx // 定义监控脚本，这里和上面vrr_script后面的字符串保持一致\n }\n\n}</code></pre>\n<p>keepalived要实现高可用，监控Nginx服务自然是必不可少的，它本身没有这个功能，需要借助自定义脚本实现，所以我们还需要定义一个监控Nginx服务的脚本，如下：</p>\n<pre class=\"code-rows\"><code># vim /usr/local/sbin/check_ng.sh // 内容如下\n# 时间变量，用于记录日志\nd=`date --date today +%Y%m%d_%H:%M:%S`\n# 计算Nginx进程数量\nn=`ps -C nginx --no-heading|wc -l`\n# 如果进程数量为0，则启动Nginx，并且再次检测Nginx进程数量，\n# 如果还为0，说明Nginx无法启动，此时需要关闭keepalived\nif [ $n -eq \"0\" ]; then\n systemctl start nginx\n n2=`ps -C nginx --no-heading|wc -l`\n if [ $n2 -eq \"0\" ]; then\n echo \"$d nginx down,keepalived will stop\" &gt;&gt; /var/log/check_ng.log\n systemctl stop keepalived\n fi\nfi\n#####以上为脚本内容#####\n# chmod a+x /usr/local/sbin/check_ng.sh // 需要给它x权限，否则无法被keepalived调用</code></pre>\n<p>做完上面所有操作后，就可以启动master上的keepalived了，如果没有启动Nginx服务，它会帮我们自动拉起来，并监听VIP：</p>\n<pre class=\"code-rows\"><code># systemctl start keepalived\n# ip add\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n inet 127.0.0.1/8 scope host lo\n valid_lft forever preferred_lft forever\n inet6 ::1/128 scope host\n valid_lft forever preferred_lft forever\n2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n link/ether 00:0c:29:15:7f:b9 brd ff:ff:ff:ff:ff:ff\n inet 192.168.72.128/24 brd 192.168.72.255 scope global noprefixroute ens33\n valid_lft forever preferred_lft forever\n inet 192.168.72.100/32 scope global ens33\n valid_lft forever preferred_lft forever\n inet6 fe80::454b:a2e5:64e5:67a3/64 scope link noprefixroute\n valid_lft forever preferred_lft forever</code></pre>\n<p>可以看到master上已经自动配置了192.168.72.100这个IP。再来看看Nginx服务是否已经启动：</p>\n<pre class=\"code-rows\"><code># ps aux |grep nginx\nroot 829 0.0 0.0 43056 996 ? Ss 11:29 0:00 nginx: master process /usr/local/nginx/sbin/nginx\nnobody 831 0.0 0.3 76016 6668 ? S 11:29 0:00 nginx: worker process\nnobody 832 0.0 0.3 76016 6668 ? S 11:29 0:00 nginx: worker process</code></pre>\n<p>master上已经大功告成，下面继续配置backup，首先编辑配置文件：</p>\n<pre class=\"code-rows\"><code># vim /etc/keepalived/keepalived.conf // 内容和master大部分一致，state和priority有变化\nglobal_defs {\n notification_email {\n aming@aminglinux.com\n }\n notification_email_from root@aminglinux.com\n smtp_server 127.0.0.1\n smtp_connect_timeout 30\n router_id LVS_DEVEL\n}\n\nvrrp_script chk_nginx {\n script \"/usr/local/sbin/check_ng.sh\"\n interval 3\n}\n\nvrrp_instance VI_1 {\n state BACKUP\n interface ens33\n virtual_router_id 51\n priority 90\n advert_int 1\n authentication {\n auth_type PASS\n auth_pass aminglinux&gt;com\n }\n virtual_ipaddress {\n 192.168.72.100\n }\n\n track_script {\n chk_nginx\n }\n\n}</code></pre>\n<p>编辑监控脚本，如下：</p>\n<pre class=\"code-rows\"><code># vim /usr/local/sbin/check_ng.sh // 内容如下\n# 时间变量，用于记录日志\nd=`date --date today +%Y%m%d_%H:%M:%S`\n# 计算Nginx进程数量\nn=`ps -C nginx --no-heading|wc -l`\n# 如果进程数量为0，则启动Nginx，并且再次检测Nginx进程数量，\n# 如果还为0，说明Nginx无法启动，此时需要关闭keepalived\nif [ $n -eq \"0\" ]; then\n systemctl start nginx\n n2=`ps -C nginx --no-heading|wc -l`\n if [ $n2 -eq \"0\" ]; then\n echo \"$d nginx down,keepalived will stop\" &gt;&gt; /var/log/check_ng.log\n systemctl stop keepalived\n fi\nfi\n#####以上为脚本内容#####\n# chmod a+x /usr/local/sbin/check_ng.sh</code></pre>\n<p>如果你的backup（129）上还未安装Nginx，暂时还不能启动keepalived服务，还需要做如下操作：</p>\n<pre class=\"code-rows\"><code># yum install -y epel-release\n# yum install –y nginx</code></pre>\n<p>Nginx除了可以通过编译安装，也可以使用yum安装。然后，把keepalived服务启动：</p>\n<pre class=\"code-rows\"><code># systemctl start keepalived</code></pre>\n<p>折腾了这么久，阿铭还未检验劳动成果，下面来跟着阿铭一起验证keepalived的高可用吧。</p>\n<p>为了区分开master和backup两个角色，阿铭先把两台机器的Nginx做一个区分，为两个Nginx分别设置一个自定义header，这样在执行<code>curl</code>的时候就可以看出差异来：</p>\n<pre class=\"code-rows\"><code># 分别编辑128和129的nginx.conf\n# vim /usr/local/nginx/conf/nginx.conf // 如果是yum安装的Nginx，配置文件为 /etc/nginx/nginx.conf\n# 在http {} 配置段里，增加如下内容\nadd_header myheader \"128\"; // 这是128的配置\nadd_header myheader \"129\"; // 这是129的配置</code></pre>\n<p>重载配置后，进行测试：</p>\n<pre class=\"code-rows\"><code># systemctl reload nginx\n# curl -I 192.168.72.128 // 可以看到myheader：128\nHTTP/1.1 200 OK\nServer: nginx/1.18.0\nDate: Sun, 05 Jul 2020 03:58:37 GMT\nContent-Type: text/html\nContent-Length: 15\nLast-Modified: Sat, 27 Jun 2020 01:59:46 GMT\nConnection: keep-alive\nETag: \"5ef6a812-f\"\nmyheader: 128\nAccept-Ranges: bytes\n\n# curl -I 192.168.72.129 // 可以看到myheader：129\nHTTP/1.1 200 OK\nServer: nginx/1.18.0\nDate: Sun, 05 Jul 2020 03:58:40 GMT\nContent-Type: text/html\nContent-Length: 15\nLast-Modified: Sat, 27 Jun 2020 01:59:46 GMT\nConnection: keep-alive\nETag: \"5ef6a812-f\"\nmyheader: 129\nAccept-Ranges: bytes</code></pre>\n<p>有了标记之后，后续的操作就有意思了，先把master上的Nginx故意关掉：</p>\n<pre class=\"code-rows\"><code># netstat -lntp |grep nginx\ntcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 829/nginx: master p\ntcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 829/nginx: master p\n# systemctl stop nginx\nStopping nginx (via systemctl): [ 确定 ]\n# netstat -lntp |grep nginx // 关掉之后，确实没有了Nginx服务</code></pre>\n<p>等了片刻，再来检测端口，发现服务又启动了：</p>\n<pre class=\"code-rows\"><code># netstat -lntp |grep nginx\ntcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 829/nginx: master p\ntcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 829/nginx: master p</code></pre>\n<p>你也可以通过/var/log/messages日志看到Nginx被启动的信息：</p>\n<pre class=\"code-rows\"><code>Jul 5 12:02:07 aminglinux-123 systemd[1]: Stopping nginx - high performance web server...\nJul 5 12:02:07 aminglinux-123 systemd[1]: Stopped nginx - high performance web server.\nJul 5 12:02:10 aminglinux-123 systemd[1]: Starting nginx - high performance web server...\nJul 5 12:02:10 aminglinux-123 systemd[1]: Started nginx - high performance web server.</code></pre>\n<p>从时间上可以看出关闭Nginx服务后，大概3秒钟它就又被启动了。下面我们再模拟master死机，只需要在master加上一条<code>iptables</code>规则即可：</p>\n<pre class=\"code-rows\"><code># iptables -I OUTPUT –p vrrp -j DROP</code></pre>\n<p>加完规则后，到backup上看是否被设置了VIP：</p>\n<pre class=\"code-rows\"><code># ip add\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n inet 127.0.0.1/8 scope host lo\n valid_lft forever preferred_lft forever\n inet6 ::1/128 scope host\n valid_lft forever preferred_lft forever\n2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n link/ether 00:0c:29:58:a3:05 brd ff:ff:ff:ff:ff:ff\n inet 192.168.72.129/24 brd 192.168.72.255 scope global noprefixroute ens33\n valid_lft forever preferred_lft forever\n inet 192.168.72.100/32 scope global ens33\n valid_lft forever preferred_lft forever\n inet6 fe80::a7de:e888:acdf:4c8c/64 scope link noprefixroute\n valid_lft forever preferred_lft forever</code></pre>\n<p>确实已经有192.168.72.100这个VIP了，再来看 /var/log/messages：</p>\n<pre class=\"code-rows\"><code>tail /var/log/messages\nJul 5 12:03:56 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:03:56 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:03:56 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:03:56 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:04:01 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:04:01 aminglinux-123 Keepalived_vrrp[2085]: (VI_1) Sending/queueing gratuitous ARPs on ens33 for 192.168.72.100\nJul 5 12:04:01 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:04:01 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:04:01 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100\nJul 5 12:04:01 aminglinux-123 Keepalived_vrrp[2085]: Sending gratuitous ARP on ens33 for 192.168.72.100</code></pre>\n<p>其中也有VIP的相关信息，但是这并不完美，因为在master上依旧有VIP，master上虽然被禁掉了VRRP协议，但它并不认为自己死机了，所以不会释放VIP资源。如果master和backup都绑定了VIP，那么对外提供服务时就会紊乱，这叫作“脑裂”，这种情况是不允许发生的。其实还有一种测试方案，就是直接把master上的keepalived服务关掉，在此之前先将前面增加的iptables规则去掉，运行下面命令：</p>\n<pre class=\"code-rows\"><code># iptables -D OUTPUT -p VRRP –j DROP\n# systemctl stop keepalived</code></pre>\n<p>然后我们直接访问VIP来判断VIP在哪里：</p>\n<pre class=\"code-rows\"><code># curl -I 192.168.72.100\nHTTP/1.1 200 OK\nServer: nginx/1.18.0\nDate: Sun, 05 Jul 2020 04:06:36 GMT\nContent-Type: text/html\nContent-Length: 15\nLast-Modified: Sat, 27 Jun 2020 01:59:46 GMT\nConnection: keep-alive\nETag: \"5ef6a812-f\"\nmyheader: 129\nAccept-Ranges: bytes</code></pre>\n<p>可以判断VIP已经到了129上，再把master上的keepalived服务开启：</p>\n<pre class=\"code-rows\"><code># systemctl start keepalived\n# curl -I 192.168.72.100\ncurl -I 192.168.72.100\nHTTP/1.1 200 OK\nServer: nginx/1.18.0\nDate: Sun, 05 Jul 2020 04:10:17 GMT\nContent-Type: text/html\nContent-Length: 15\nLast-Modified: Sat, 27 Jun 2020 01:59:46 GMT\nConnection: keep-alive\nETag: \"5ef6a812-f\"\nmyheader: 128\nAccept-Ranges: bytes</code></pre>\n<p>此时VIP又回到了master上。试想，如果一台机器死机，keepalived服务必然会停掉，所以这样去验证keepalived的高可用是没有任何问题的。另外,希望你多看一看/var/log/messages日志，在阿铭试验过程中产生了很多keepalived的日志，以后遇到问题时也可以有一个参考。</p>\n<p>如果把Nginx换成其他服务，比如说MySQL，如何做呢？配置思路是一样的，唯一不同的是对MySQL的监控脚本不一样。</p>\n<h2 id=\"nav_point_378\">20.2 搭建负载均衡集群</h2>\n<p>负载均衡集群不难理解，从字面上也能猜到，简单说就是让多台服务器均衡地去承载压力。实现负载均衡集群的开源软件有LVS、keepalived、haproxy、Nginx等，当然也有优秀的商业负载均衡设备，比如F5、NetScaler等。商业的负载均衡解决方案稳定性没话说，但是成本非常昂贵，所以阿铭不多介绍，本节以开源的LVS为主。</p>\n<h3 id=\"nav_point_379\">20.2.1 介绍LVS</h3>\n<p>LVS（Linux Virtual Server）是由国内大牛章文嵩开发的，这款软件的流行度不亚于Apache的httpd，它是一款四层的负载均衡软件，是针对TCP/IP做的转发和路由，所以稳定性和效率相当高。不过LVS最新的版本是基于Linux 2.6内核的，这意味着它已经有多年没有更新了。虽然目前越来越多的企业选择使用Nginx实现负载均衡，但LVS依然被诸多企业应用在核心的架构当中。LVS的架构如图20-1所示，在该架构中有一个核心的角色叫作调度器（Load Balancer），用来分发用户的请求；还有诸多的真实服务器（Real Server），也就是处理用户请求的服务器。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100013/image00432.jpeg\" alt=\"\" width=\"75%\" style=\"width: 75%\" /></p>\n<p><strong>图20-1 LVS架构</strong></p>\n<p>LVS根据实现方式的不同，主要分为三种类型：NAT模式、IP Tunnel（IP隧道）模式、DR模式。</p>\n<p>(1) NAT模式</p>\n<p>前面阿铭介绍iptables时，有介绍过<code>nat</code>表，和这里的NAT其实是一个意思。这种模式的实现原理很简单，调度器会把用户的请求通过预设的iptables规则转发给后端的真实服务器。其中调度器有两个IP，一个是公网IP，一个是内网IP，而真实服务器只有内网IP。用户访问的时候请求的是调度器的公网IP，它会把用户的请求转发到真实服务器的内网IP上。这种模式的好处是节省公网IP，但是调度器会成为一个瓶颈。NAT模式架构如图20-2和图20-3所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100013/image00433.jpeg\" alt=\"\" width=\"70%\" style=\"width: 70%\" /></p>\n<p><strong>图20-2 NAT模式</strong></p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100013/image00434.jpeg\" alt=\"\" width=\"50%\" style=\"width: 50%\" /></p>\n<p><strong>图20-3 NAT模式2</strong></p>\n<p>(2) IP Tunnel模式</p>\n<p>IP隧道是将一个IP报文封装在另一个IP报文中的技术，这可以使目标为一个IP地址的数据报文能被封装和转发到另一个IP地址。像大家熟知的VPN技术其实就是IP隧道。在LVS的IP Tunnel架构中，后端服务器有一组而非一个，所以不可能静态地建立一一对应的隧道，而是动态地选择一台服务器，将请求报文封装和转发给选出的服务器。这样我们可以利用IP隧道的原理，将一组服务器上的网络服务组成在一个IP地址上的虚拟网络服务。IP Tunnel模式架构如图20-4所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100013/image00435.jpeg\" alt=\"\" width=\"75%\" style=\"width: 75%\" /></p>\n<p><strong>图20-4 IP Tunnel模式</strong></p>\n<p>调度器（Load Balancer）将请求报文封装在另一个IP报文中，再将封装后的IP报文转发给真实服务器。真实服务器收到报文后，先将报文解封获得原来目标地址为VIP的报文，服务器发现VIP地址被配置在本地的IP隧道设备上，所以就处理这个请求，然后根据路由表将响应报文直接返回给客户。这种模式下，需要给调度器和所有的真实服务器全部分配公网IP，所以比较浪费公网IP。</p>\n<p>(3) DR模式</p>\n<p>和IP Tunnel模式方法相同，用户的请求被调度器动态地分配到真实服务器上，真实服务器响应请求把结果直接返回给用户。不过，在这种模式下不会封装IP，而是将数据帧的MAC地址改为真实服务器的MAC地址。DR模式架构如图20-5所示。</p>\n<p class=\"pic\"><img img src=\"https://static001.geekbang.org/files/resource/ebook/100013/image00436.jpeg\" alt=\"\" width=\"70%\" style=\"width: 70%\" /></p>\n<p><strong>图20-5 DR模式</strong></p>\n<p>LVS的官方网站给出了三种模式的比较，如表20-1所示。</p>\n<p><strong>表20-1 LVS模式比较</strong></p>\n<table width=\"90%\" border=\"1\">\n<thead>\n<tr>\n<th><p>_</p></th>\n<th><p>VS/NAT</p></th>\n<th><p>VS/TUN</p></th>\n<th><p>VS/DR</p></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><p>server</p></td>\n<td><p>any</p></td>\n<td><p>Tunneling</p></td>\n<td><p>Non-arp device</p></td>\n</tr>\n<tr>\n<td><p>server network</p></td>\n<td><p>private</p></td>\n<td><p>LAN/WAN</p></td>\n<td><p>LAN</p></td>\n</tr>\n<tr>\n<td><p>server number</p></td>\n<td><p>low (10~20)</p></td>\n<td><p>High (100)</p></td>\n<td><p>High (100)</p></td>\n</tr>\n<tr>\n<td><p>server gateway</p></td>\n<td><p>load balancer</p></td>\n<td><p>own router</p></td>\n<td><p>own router</p></td>\n</tr>\n</tbody>\n</table>\n<p>以上三种方法所能支持最大服务器数的估值是假设调度器使用100 MB网卡，调度器的硬件配置与后端服务器的硬件配置相同，而且是针对一般Web服务的。使用更高的硬件配置（如千兆网卡和更快的处理器）作为调度器，调度器所能调度的服务器数量会相应增加。当应用不同时，服务器的数目也会相应地改变。所以，以上数据估计主要是为三种方法的伸缩性进行量化比较。</p>\n<p>根据表20-1的比较可以看出，NAT模式适合小型的集群，机器数量不多，它的优势是节省公网IP。TUN和DR相差不大，都能支撑较大规模的集群，但缺点是浪费公网IP。</p>\n<h3 id=\"nav_point_380\">20.2.2 LVS的调度算法</h3>\n<p>调度器把客户端发来的请求均衡地分发给后端的真实服务器，这是依靠预先设定好的调度算法实现的，在LVS中支持的调度算法主要有以下8种。</p>\n<ol>\n<li><p><strong>轮询调度</strong></p>\n<p>这种算法简称RR（Round-Robin），是非常简单的一种调度算法，它按顺序把请求依次发送给后端的服务器，不管后端服务器的处理速度和响应时间怎样。当后端服务器性能不一致时，用这种调度算法就不合适了。<br />&nbsp;</p>\n</li>\n<li><p><strong>带权重的轮询调度</strong></p>\n<p>这种算法简称WRR（Weighted Round-Robin），比第一种算法多了一个权重的设置，权重越高的服务器被分配到的请求就越多，这样后端服务器性能不一致时，就可以给配置低的服务器较小的权重。<br />&nbsp;</p>\n</li>\n<li><p><strong>最小连接调度</strong></p>\n<p>这种算法简称LC（Least-Connection），会根据各真实服务器上的连接数来决定把新的请求分配给谁，连接数少说明服务器是空闲的，这样把新的请求分配到空闲服务器上才更加合理。<br />&nbsp;</p>\n</li>\n<li><p><strong>带权重最小连接调度</strong></p>\n<p>这种算法简称WLC（Weight Least-Connection），是在最小连接调度的基础上再增加一个权重设置，这样就可以人为地去控制哪些服务器上多分配请求，哪些少分配请求。<br />&nbsp;</p>\n</li>\n<li><p><strong>基于局部性的最少连接调度</strong></p>\n<p>这种算法简称LBLC（Locality-Based Least Connection），是针对请求报文的目标IP地址的负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群中客户请求报文的目标IP地址是变化的。算法的设计目标是在服务器的负载基本平衡的情况下，将具有相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和主存Cache命中率。<br />&nbsp;</p>\n</li>\n<li><p><strong>带复制的基于局部性最少连接调度</strong></p>\n<p>该算法简称LBLCR（Locality-Based Least Connections with Replication），也是针对目标IP地址的负载均衡，它与LBLC算法的不同之处是：它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法是维护从一个目标IP地址到一台服务器的映射。LBLCR算法先根据请求的目标IP地址找出该目标IP地址对应的服务器组，按“最小连接”原则从该服务器组中选出一台服务器，若服务器没有超载，则将请求发送到该服务器；若服务器超载，则按“最小连接”原则从整个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。<br />&nbsp;</p>\n</li>\n<li><p><strong>目标地址散列调度</strong></p>\n<p>该算法（Destination Hashing）也是针对目标IP地址的负载均衡，但它是一种静态映射算法，通过一个散列（hash）函数将一个目标IP地址映射到一台服务器。目标地址散列调度算法先将请求的目标IP地址作为散列键（hash key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，则将请求发送到该服务器，否则返回空。<br />&nbsp;</p>\n</li>\n<li><p><strong>源地址散列调度</strong></p>\n<p>该算法（Source Hashing）正好与目标地址散列调度算法相反，它将请求的源IP地址作为散列键从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，就将请求发送到该服务器，否则返回空。它的算法流程与目标地址散列调度算法的流程基本相似，只不过将请求的目标IP地址换成了请求的源IP地址。</p>\n</li>\n</ol>\n<p>对于以上8种调度算法，阿铭认为前4种用得最多，也最容易理解，它们基本上能满足绝大多数的应用场景。关于LVS的介绍内容还是蛮多的，请不要觉得啰唆，这部分内容在面试的时候经常会被问到，建议你耐心地读一读，理解了自然就掌握了。</p>\n<p>在阿铭10多年的职业生涯里，生产环境中使用LVS的场景几乎没有，所以关于LVS的相关试验，阿铭就省略了，但是keepalived＋LVS还是要给大家验证一下的。</p>\n<h3 id=\"nav_point_381\">20.2.3 使用keepalived＋LVS DR模式实现负载均衡</h3>\n<p>完整的keepalived+LVS架构需要有两台调度器实现高可用，其中提供调度服务的只需要一台，另外一台作为备用。为了节省资源，阿铭只设置一台主keepalived，备用的暂时就省略掉了。以下试验需要准备三台机器。阿铭原来就有两台虚拟机，还需再克隆一台。三台机器的IP分配如下。</p>\n<ul>\n<li><strong>主keepalived（调度器）</strong>：192.168.72.128</li>\n<li><strong>真实服务器rs1</strong>：192.168.72.129</li>\n<li><strong>真实服务器rs2</strong>：192.168.72.130</li>\n<li><strong>VIP</strong>：192.168.72.110</li>\n</ul>\n<p>如果你的机器上还未安装过keepalived，则需要先安装，直接运行<code>yum install keepalived</code>即可。由于阿铭的128机器之前安装过keepalived，所以这一步就省略了。下面编辑keepalived的配置文件：</p>\n<pre class=\"code-rows\"><code>vim /etc/keepalived/keepalived.conf // 更改成如下内容\nvrrp_instance VI_1 {\n # 备用服务器上为 BACKUP\n state MASTER\n # 绑定VIP的网卡为ens33，你的网卡和阿铭的可能不一样，这里需要你改一下\n interface ens33\n virtual_router_id 51\n # 备用服务器上为90\n priority 100\n advert_int 1\n authentication {\n auth_type PASS\n auth_pass aminglinux\n }\n virtual_ipaddress {\n 192.168.72.110\n }\n}\nvirtual_server 192.168.72.110 80 {\n #(每隔10秒查询一次真实服务器的状态)\n delay_loop 10\n #(lvs 算法，rr为轮询算法，wrr为带权重的轮询算法)\n lb_algo rr\n #(DR模式)\n lb_kind DR\n #(同一IP的连接60秒内被分配到同一台真实服务器，为了方便试验，这里将该参数注释了，生产环境建议开启)\n #persistence_timeout 60\n #(用TCP协议检查真实服务器状态)\n protocol TCP\n\n real_server 192.168.72.129 80 {\n #(权重)\n weight 100\n TCP_CHECK {\n #(10秒无响应即超时)\n connect_timeout 10\n nb_get_retry 3\n delay_before_retry 3\n connect_port 80\n }\n }\n real_server 192.168.72.130 80 {\n weight 100\n TCP_CHECK {\n connect_timeout 10\n nb_get_retry 3\n delay_before_retry 3\n connect_port 80\n }\n }\n}\n# systemctl restart keepalived</code></pre>\n<p>两台rs（129和130）上需要编写脚本（内容一样）：</p>\n<pre class=\"code-rows\"><code># vim /usr/local/sbin/lvs_dr_rs.sh // 内容如下\n#/bin/bash\nvip=192.168.72.110\n# 把vip绑定在lo上，是为了rs能直接把结果返回给客户端\nifconfig lo:0 $vip broadcast $vip netmask 255.255.255.255 up\nroute add -host $vip lo:0\n# 以下操作为更改ARP内核参数，目的是让rs顺利发送MAC地址给客户端\necho \"1\" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore\necho \"2\" &gt;/proc/sys/net/ipv4/conf/lo/arp_announce\necho \"1\" &gt;/proc/sys/net/ipv4/conf/all/arp_ignore\necho \"2\" &gt;/proc/sys/net/ipv4/conf/all/arp_announce</code></pre>\n<p>分别在两台机器上执行各自的脚本：</p>\n<pre class=\"code-rows\"><code># bash /usr/local/sbin/lvs_dr_rs.sh // 129和130上执行</code></pre>\n<p>为了便于区分开129和130，阿铭将其虚拟主机默认网页修改了一下：</p>\n<pre class=\"code-rows\"><code># echo \"default_server 129\" &gt; /data/nginx/default/index.html // 修改129的默认网页\n# echo \"default_server 130\" &gt; /data/nginx/default/index.html // 修改130的默认网页\n# curl 192.168.72.129\ndefault_server 129\n# curl 192.168.72.130\ndefault_server 130</code></pre>\n<p>下面看测试效果，但这次就不能用<code>curl</code>命令测试了，因为VIP在三台机器上都有设置，直接<code>curl</code>去访问VIP的话不成功，如果要curl测试，还需要再搞一台同网段的虚拟机才可以。所以只能用浏览器来测试效果，直接在浏览器里访问http://192.168.72.110即可。测试过程阿铭不再详细介绍，最终达到的效果是：第一次访问结果为<code>default_server 129</code>，然后刷新一下就变为<code>default_server 130</code>，再刷新又变为<code>default_server 129</code>，依此类推。</p>\n<h3 id=\"nav_point_382\">20.2.4 使用Nginx实现负载均衡</h3>\n<p>Nginx作为一款优秀的Web服务器，不仅能实现七层负载均衡，还能实现四层负载均衡。这里的七层、四层指的是网络OSI七层模型中的应用层和传输层，对应到Nginx的负载均衡，可以这样理解：七层即可以实现HTTP/HTTPS的负载均衡，四层即可以实现TCP的负载均衡，比如用Nginx对MySQL进行负载均衡。</p>\n<p>先来看七层的负载均衡示例，机器规划如下。</p>\n<ul>\n<li><strong>负载均衡器</strong>：192.168.72.128</li>\n<li><strong>服务器1</strong>：192.168.72.129</li>\n<li><strong>服务器2</strong>：192.168.72.130</li>\n</ul>\n<p>在128上，编辑配置文件：</p>\n<pre class=\"code-rows\"><code>vim /usr/local/nginx/conf/vhost/ld7.conf // 更改成如下内容\nupstream ld_7\n{\n server 192.168.72.129:80;\n server 192.168.72.130:80;\n}\n\nserver\n{\n listen 80;\n server_name www.ld7.com;\n\n location /\n {\n proxy_pass http://ld_7;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n }\n}</code></pre>\n<p>重载配置文件后，使用<code>curl</code>命令测试效果：</p>\n<pre class=\"code-rows\"><code># /usr/local/nginx/sbin/nginx -s reload\n# curl -x192.168.72.128:80 www.ld7.com\ndefault_server 129\n# curl -x192.168.72.128:80 www.ld7.com\ndefault_server 130\n# curl -x192.168.72.128:80 www.ld7.com\ndefault_server 129\n# curl -x192.168.72.128:80 www.ld7.com\ndefault_server 130</code></pre>\n<p>可以看到，每次请求得到的结果不一样。</p>\n<p>下面再来看看四层的负载均衡，首先要修改配置文件nginx.conf，如下：</p>\n<pre class=\"code-rows\"><code># vim /usr/local/nginx/conf/nginx.conf // 在文件最后面，也就是}的下一行增加如下内容\nstream\n{\n include vhost/tcp/*.conf;\n}\n// 说明：这里的stream {}是和http {}配置段同级别的，stream {}里面的配置为tcp相关配置</code></pre>\n<p>然后创建四层负载均衡的配置文件，如下：</p>\n<pre class=\"code-rows\"><code># mkdir /usr/local/nginx/conf/vhost/tcp // 创建tcp目录\nupstream lb_4\n{\n server 192.168.72.129:80;\n server 192.168.72.130:80;\n}\n\nserver\n{\n listen 81;\n proxy_timeout 3s;\n proxy_pass lb_4;\n}\n// 说明：这里监听81端口，区别于七层的80端口</code></pre>\n<p>配置完，检查配置文件是否有问题，结果发现错误：</p>\n<pre class=\"code-rows\"><code># /usr/local/nginx/sbin/nginx -t\nnginx: [emerg] unknown directive \"stream\" in /usr/local/nginx/conf/nginx.conf:68\nnginx: configuration file /usr/local/nginx/conf/nginx.conf test failed</code></pre>\n<p>这是因为我们在编译Nginx时，并没有指定让Nginx支持四层的代理，这就需要重新编译Nginx，增加<code>--with-stream</code>参数：</p>\n<pre class=\"code-rows\"><code># cd /usr/local/src/nginx-1.18.0\n# ./configure --prefix=/usr/local/nginx --with-http_ssl_module --with-stream\n# make\n# make install</code></pre>\n<p>再一次检查配置文件，就可以了：</p>\n<pre class=\"code-rows\"><code># /usr/local/nginx/sbin/nginx -t\nnginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok\nnginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful\n# systemctl restart nginx // 必须要重启一下nginx，81端口才会开启哦</code></pre>\n<p>下面来看看81端口的效果：</p>\n<pre class=\"code-rows\"><code># curl -x192.168.72.128:81 www.abc.com\ndefault_server 129\n# curl -x192.168.72.128:81 www.abc.com\ndefault_server 130\n# curl -x192.168.72.128:81 www.abc.com\ndefault_server 129\n# curl -x192.168.72.128:81 www.abc.com\ndefault_server 130</code></pre>\n<p>这里使用了一个临时域名www.abc.com，其实你无论使用什么域名都可以，这是因为它访问到129和130时都会去访问默认虚拟主机。</p>\n<h2 id=\"nav_point_383\">20.3 课后习题</h2>\n<p>(1) 常见的集群架构有哪些？</p>\n<p>(2) 请列举出可以实现负载均衡集群的开源软件？</p>\n<p>(3) LVS有哪几种工作模式？</p>\n<p>(4) LVS可以支持哪些调度算法？</p>\n<p>(5) 要实现负载均衡集群，至少需要几台机器？</p>\n<p>(6) 对比Nginx和LVS负载均衡，它们两者有什么区别？</p>\n<p>(7) 自己动手用keepalived实现MySQL的高可用集群（要考虑到两台MySQL数据的一致性）。</p>\n\n<br style=\"page-break-after:always\" />","comments":[]}