{"id":816242,"title":"01｜揭示LLaMA 3对话能力的奥秘","content":"<p>你好，我是Tyler。今天我们正式开始学习LLaMA 3的能力模型。</p><p>过去的一年中，大模型技术得到了广泛认可，全行业对大模型的投入也在不断增加。开源社区涌现了许多优秀的模型和框架，推动了大模型技术的普及和应用。在这一年的时间里，LLaMA 系列模型也经历了快速的发展，从 LLaMA 2 到 LLaMA 3，我们看到了性能和应用上的显著提升。</p><p>本季专栏中，我将采用“Learn by doing”的方法，通过简洁的示例，深入剖析大模型技术的本质。我们将探讨LLaMA 3的能力模型，详细解析大模型技术的各个方面，并深入到你在使用LLaMA 3过程中会遇到的各种细节。</p><p>在第一讲中，我将详细介绍LLaMA 3模型的核心能力——对话生成，并展示它在文本生成方面的强大潜力。</p><h2>基本操作：生成内容</h2><p>首先，让我们来了解一下LLaMA 3的核心能力。LLaMA 3主要依赖于Next Token Prediction（下一个词预测）机制，通过预测下一个词来生成连贯的对话。这种机制基于海量文本数据的训练，使模型能够捕捉语言的模式和规律，生成符合上下文逻辑的文本内容。</p><h3>Next Token Prediction</h3><p>Next Token Prediction是大语言模型生成文本的基础。模型处理输入文本的步骤如下：</p><!-- [[[read_end]]] --><ol>\n<li><strong>标记化：</strong>首先，大模型将输入文本分解成一系列的 token（词或子词）。例如，句子“请解释一下黑洞的形成”可能被分解为以下 token：</li>\n</ol><pre><code class=\"language-plain\">[\"请\", \"解释\", \"一\", \"下\", \"黑\", \"洞\", \"的\", \"形\", \"成\"]\n</code></pre><ol start=\"2\">\n<li>\n<p><strong>文字表征：</strong>接下来，将这些 token 转换为模型能够理解的数值形式（通常是嵌入向量）。</p>\n</li>\n<li>\n<p><strong>概率预测：</strong>大模型会根据当前的输入序列计算下一个词的概率分布。这些概率分布表示下一个词的可能性，例如：</p>\n</li>\n</ol><pre><code class=\"language-plain\">{\"黑洞\": 0.1, \"形成\": 0.05, \"是\": 0.2, \"由于\": 0.15, ...}\n</code></pre><ol start=\"4\">\n<li><strong>生成文本：</strong>根据概率分布选择具体的一个词作为下一个词。选择方式可以是贪婪搜索（选择概率最大的词）、随机采样（根据概率分布随机选择）或其他搜索策略（比如Beam Search）。例如，使用贪婪搜索选择概率最大的词“是”，并将其添加到已生成的文本序列中，重复上述步骤。</li>\n</ol><pre><code class=\"language-plain\">[“请”, “解释”, “一”, “下”, “黑”, “洞”, “的”, “形”, “成”, “是”]。\n</code></pre><p>因此，LLaMA 3模型的推理过程是一个循环：通过预测下一个词，将其加入到序列中，再预测下一个词来生成连贯的文本。这种循环造成了大量的模型推理算力开销，这也是为什么OpenAI等公司在API使用中根据Token计费。</p><p>接下来，我们通过一个具体的代码示例来演示LLaMA 3的文本生成过程。假设我们有一个文章开头：</p><blockquote>\n<p>在一个阳光明媚的早晨，Alice决定去森林里探险。她走着走着，突然发现了一条小路。</p>\n</blockquote><p>我们使用LLaMA 3来续写这段话：</p><pre><code class=\"language-python\">import torch\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 下载模型\ncache_dir = './llama_cache'\nmodel_id = snapshot_download(\"LLM-Research/Meta-Llama-3-8B\", cache_dir=cache_dir)\n</code></pre><pre><code class=\"language-python\">import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# 加载分词器和模型\ncache_dir = './llama_cache'\nmodel_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; model_path,\n&nbsp; &nbsp; torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None\n)\n\n# 编码输入并将其移至模型设备\ninput_text = \"在一个阳光明媚的早晨，Alice决定去森林里探险。她走着走着，突然发现了一条小路。\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n\n# 生成并解码文本\nwith torch.no_grad():\n&nbsp; &nbsp; outputs = model.generate(**inputs)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n</code></pre><p>输出结果：</p><blockquote>\n<p>这条小路被两旁茂密的树木掩映着，似乎通向森林深处。Alice决定沿着小路前进，想看看尽头有什么惊喜等着她。</p>\n</blockquote><p>在这个例子中，LLaMA 3 成功地续写了文章内容，我们可以看到 LLaMA 3 流畅地生成了具有创意性的内容，这个生成的过程就是刚刚提到的 Next Token Prediction&nbsp;循环所形成的。</p><h3>停止条件</h3><p>你可能会问，Next Token Prediction 的方式岂不是会一直输出内容不停止？很好的问题！现在我们来聊聊LLaMA 3 <strong>停止输出的条件</strong>，LLaMA 3 的输出由以下几个因素控制：</p><ul>\n<li><strong>最大长度（max_length）：</strong>这是最常用的控制方式之一。在初始化模型时，你可以指定一个最大长度值。当模型生成的文本长度达到该值时，它就会停止输出。</li>\n</ul><p>例如，如果将最大长度设置为100，那么模型生成的文本最长不会超过100个词。</p><pre><code class=\"language-python\">from transformers import AutoTokenizer, AutoModelForCausalLM\n\ncache_dir = './llama_cache'\nmodel_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; model_path,\n&nbsp; &nbsp; torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None\n)\n\nmax_length = 50\ninput_text = \"写一首关于爱情的诗\"\nencoded_input = tokenizer(input_text, return_tensors=\"pt\")\noutput = model.generate(encoded_input.input_ids, max_length=max_length)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n</code></pre><ul>\n<li><strong>其他停止条件：</strong>一些模型还支持其他停止条件。例如检测到重复的文本、低质量的文本等。</li>\n</ul><p>LLaMA 3 可以检测到重复的文本并将其跳过。</p><pre><code class=\"language-python\">from transformers import AutoTokenizer, AutoModelForCausalLM\n\ncache_dir = './llama_cache'\nmodel_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; model_path,\n&nbsp; &nbsp; torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None\n)\n\nrepetition_penalty = 1.2\ninput_text = \"写一首关于爱情的诗：\"\nencoded_input = tokenizer(input_text, return_tensors=\"pt\")\noutput = model.generate(encoded_input.input_ids, repetition_penalty=repetition_penalty)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n</code></pre><p>这些停止条件确保模型生成的文本不会无限延长，可以通过设置不同的参数来控制生成过程。</p><h2>无状态到有状态：对话能力</h2><p>对话生成不仅需要理解上下文，还得保持连贯性。<strong>因此，我们首先要解决的问题，就是大模型服务“无状态”的问题。</strong>为了让“无状态”的LLaMA 3模型具备对话能力，我们可以将先前的“历史会话”作为当前输入的一部分。这样可以保持上下文的连贯性，使模型成为一个“有状态”的服务，从而准确地生成响应。</p><p>以下是实现这一功能的代码示例：</p><pre><code class=\"language-python\">import torch\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 下载模型\ncache_dir = './llama_cache'\nmodel_id = snapshot_download(\"LLM-Research/Meta-Llama-3-8B-Instruct\", cache_dir=cache_dir)\n</code></pre><pre><code class=\"language-python\">from transformers import AutoTokenizer, AutoModelForCausalLM\n\ncache_dir = './llama_cache'\nmodel_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; model_path,\n&nbsp; &nbsp; torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None\n)\n\n# 初始化对话历史\ndialogue_history = [\n&nbsp; &nbsp; \"Customer: Hi, I have an issue with my order.\",\n&nbsp; &nbsp; \"Support: Sure, could you please provide your order number?\",\n&nbsp; &nbsp; \"Customer: Sure, it's #12345.\",\n&nbsp; &nbsp; \"Support: Thank you. Let me check the status for you.\",\n]\n\n# 合并对话历史为一个字符串\ndialogue_history_text = \"\\n\".join(dialogue_history)\n\n# 添加用户输入，模拟当前对话\nuser_input = \"Customer: Can you please expedite the delivery?\"\ninput_text = dialogue_history_text + \"\\n\" + user_input\n\n# 生成文本\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model.generate(input_ids, max_length=100)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Generated Response:\", generated_text)\n\n</code></pre><p>这段代码演示了如何使用 LLaMA 3 模型生成对话响应。</p><p>首先，我们定义了模型和 tokenizer，并初始化了一个简单的对话历史列表。然后，将对话历史转换为单个字符串，并添加用户的当前输入。接下来，我们使用 tokenizer 对输入进行编码，然后通过模型生成响应文本。最后，解码生成的文本并打印出来。</p><p><strong>这里为你带来这节课的第一个重点</strong>，你可能会发现，上面的示例使用了基础版模型，而下面的示例使用了 Instruct 版本的模型。这是为什么呢？这是因为这两个模型的目标不一样，为了服务不同的角色，每个版本 LLaMA 3 模型在训练数据和训练方法上有所不同，针对特定对象进行了微调，比如：</p><ul>\n<li>\n<p>人类用户：通过指令微调，使模型更好地理解和响应人类指令。这种微调使模型能够处理更自然的语言输入，提供更准确和相关的回答。</p>\n</li>\n<li>\n<p>检索系统：结合检索系统的微调方法，提升模型在特定领域的信息检索能力。通过实时检索最新的外部数据，模型可以提供更加准确和时效性强的回答。</p>\n</li>\n<li>\n<p>智能体（多步推理）：模型之间的协作与交互，通过互相微调提升整体智能水平。不同模型可以相互补充，共同完成复杂的任务，从而提高整体性能。</p>\n</li>\n</ul><p>此外，选择不同版本的 LLaMA 3 模型还需要考虑多个维度。</p><ul>\n<li>\n<p>场景数据：根据不同应用场景选择合适的指令微调数据，如语言、行业、文化等。不同领域的数据特点和需求不同，需要针对性地进行模型微调。</p>\n</li>\n<li>\n<p>输入长度：不同版本的模型在输入长度上有所差异，我们需要根据具体应用需求选择适合的模型版本。一些应用场景可能需要处理较长的输入文本，因此选择支持较长输入的模型版本是必要的。</p>\n</li>\n<li>\n<p>参数效率：通过微调降低模型的参数要求，例如使用更小的模型架构或优化算法。在资源有限的环境中，可以优化模型性能以满足需求。通过合理的参数配置，可以在性能和资源消耗之间找到平衡点。</p>\n</li>\n<li>\n<p>量化程度：不同量化程度带来不同的性能提升和效果下降，需要在性能和效果之间找到平衡点。量化可以减少模型的计算和存储需求，但也可能影响生成效果，因此需要根据具体应用情况进行权衡。</p>\n</li>\n</ul><p>这些考虑因素在选择和使用 LLaMA 3 模型时至关重要。随着后面课程的深入，我们将在示例中不断展开这些内容。</p><p>在解决了大模型的“无状态“问题之后，我们再来看另一个大模型的局限性，那就是<strong>训练数据时效性的问题。</strong></p><h2>封闭到开放：检索增强</h2><p>在某些情况下，我们需要 LLaMA 3 训练后产生的最新事实。为了解决这个问题，我们可以结合检索系统，在生成过程中获取最新或特定领域的事实信息。</p><p><strong>为了解决 LLaMA 3 无法提供最新的事实信息的问题</strong>，我们需要用到检索增强生成（RAG）方法。简单来说，RAG 就是结合外部知识库或 API 进行实时检索，并将检索到的内容通过提示语补充到模型的输入中。</p><p>这种方法可以显著提高生成内容的准确性和时效性。例如，在对话过程中调用外部知识库，增强回答的准确性。以下是一个常见的示例实现步骤：</p><ol>\n<li>定义检索机制：首先，我们需要选择一个适当的外部知识库或 API，用于实时检索最新的事实信息。常见的选择包括搜索引擎 API、新闻 API、专门的行业知识库等。</li>\n<li>集成检索系统：将外部检索系统集成到 LLaMA 3 的生成流程中。当模型生成初步响应后，调用检索系统获取最新的相关信息，并将检索到的信息整合到最终的响应中。</li>\n<li>更新生成内容：利用检索到的最新信息，更新和完善模型的初步生成内容，确保回答的准确性和时效性。</li>\n</ol><p>相应的，以下是一个简单的代码示例，演示如何在生成过程中集成外部知识库进行实时检索和信息更新。</p><ol>\n<li>定义检索机制：在 retrieve_information 函数中，我们使用搜索引擎API根据给定的查询关键词检索最新的信息。</li>\n<li>集成检索系统：在 generate_response 函数中，首先使用 LLaMA 3 生成初步响应，然后提取生成文本中的关键词，通过 retrieve_information 函数获取相关的最新信息。</li>\n<li>更新生成内容：将检索到的信息整合到最终的响应中，生成更为准确和时效性的回答。</li>\n</ol><pre><code class=\"language-python\">import requests\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom googleapiclient.discovery import build\n\ncache_dir = './llama_cache'\nmodel_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; model_path,\n&nbsp; &nbsp; torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None\n)\n\n# Google Custom Search API配置\nAPI_KEY = 'YOUR_GOOGLE_API_KEY'\nSEARCH_ENGINE_ID = 'YOUR_SEARCH_ENGINE_ID'\n\n# 检索相关文档的函数\ndef retrieve_documents(query):\n&nbsp; &nbsp; try:\n&nbsp; &nbsp; &nbsp; &nbsp; service = build(\"customsearch\", \"v1\", developerKey=API_KEY)\n&nbsp; &nbsp; &nbsp; &nbsp; res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID).execute()\n&nbsp; &nbsp; &nbsp; &nbsp; results = res.get('items', [])\n&nbsp; &nbsp; &nbsp; &nbsp; documents = [item[\"snippet\"] for item in results]\n&nbsp; &nbsp; &nbsp; &nbsp; return documents\n&nbsp; &nbsp; except Exception as e:\n&nbsp; &nbsp; &nbsp; &nbsp; print(f\"Error retrieving documents: {e}\")\n&nbsp; &nbsp; &nbsp; &nbsp; return []\n\n# 生成答案的函数\ndef generate_answer(query, documents):\n&nbsp; &nbsp; # 限制检索到的文档数量\n&nbsp; &nbsp; documents = documents[:3]\n&nbsp; &nbsp; context = \"\\n\\n\".join(documents) + \"\\n\\nQuestion: \" + query + \"\\nAnswer:\"\n&nbsp; &nbsp; # 编码输入\n&nbsp; &nbsp; inputs = tokenizer(context, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n&nbsp; &nbsp; # 生成答案\n&nbsp; &nbsp; outputs = model.generate(**inputs, max_length=512)\n&nbsp; &nbsp; # 解码答案\n&nbsp; &nbsp; answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n&nbsp; &nbsp; return answer\n\n# 主函数\ndef main():\n&nbsp; &nbsp; query = \"What is the capital of France?\"\n&nbsp; &nbsp; documents = retrieve_documents(query)\n&nbsp; &nbsp; if documents:\n&nbsp; &nbsp; &nbsp; &nbsp; answer = generate_answer(query, documents)\n&nbsp; &nbsp; &nbsp; &nbsp; print(\"Question:\", query)\n&nbsp; &nbsp; &nbsp; &nbsp; print(\"Answer:\", answer)\n&nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; print(\"No documents retrieved.\")\n\nif __name__ == \"__main__\":\n&nbsp; &nbsp; main()\n\n</code></pre><blockquote>\n<p>Question: What is the capital of France?<br>\nAnswer: The capital of France is Paris. It is known for its art, fashion, and culture, and is home to famous landmarks such as the Eiffel Tower and the Louvre Museum.</p>\n</blockquote><p>通过结合外部知识库或 API 进行实时检索和信息更新，我们可以有效解决 LLaMA 3 在生成过程中无法获取最新事实的问题。这种方法不仅提高了回答的准确性，还确保了内容的时效性。在后续课程中，我们将进一步探讨如何使用向量数据和混合检索及重排技术，来构建一个定制化的 RAG 引擎。</p><p>在对话生成的基础上，我们可以进一步探索多智能体架构。多智能体系统可以分担复杂任务，通过不同的智能体协同工作，提供更专业、更高效的服务。</p><h2>单体服务到微服务：多智能体</h2><p>在单步推理方面，LLaMA 3 存在一定的局限性，例如长对话和职责混乱可能会影响模型的推理性能。针对这些问题，多智能体架构提供了一些优化策略。多智能体系统（MAS）通过将多个智能体组织起来协同工作，实现复杂任务的解决。</p><p>每个智能体都有特定的角色和功能，通过相互之间的通信和协作来达成共同目标。在LLaMA 3的应用中，我们可以利用多智能体系统的以下优点：</p><ul>\n<li>\n<p><strong>拆分对话</strong>：将单一对话拆分成多个独立的对话段，以减少对话长度对模型推理性能的影响。例如，在处理一个长篇对话时，可以将对话分成若干段，每段集中讨论一个具体问题。</p>\n</li>\n<li>\n<p><strong>明确职责</strong>：当单个模型需要处理过多问题时，可以通过拆分角色来明确每个模型的职责。例如，在一个复杂的对话系统中，可以设置不同的子模型分别处理用户意图识别、对话管理和应答生成等任务。</p>\n</li>\n<li>\n<p><strong>角色拆分</strong>：通过角色拆分，确保每个模型处理的内容单一而连贯，从而提升整体对话系统的性能和准确性。例如，可以设置一个专门处理技术问题的模型和一个专门处理日常对话的模型，分别负责不同类型的对话。</p>\n</li>\n</ul><p>为了模拟一个计算机科学家（Agent A）和一个法律专家（Agent B）协作解决人工智能合规标准制定的问题，我们设计了如下复杂的任务和分工：</p><ol>\n<li><strong>Agent A（计算机科学家）：</strong>负责提出技术实施的建议和技术难题的解决，熟悉人工智能技术和数据隐私。</li>\n</ol><pre><code class=\"language-python\"># agent_a.py\nfrom common import create_app\n\napp = create_app(\"system: 你是一个熟悉人工智能技术的计算机科学家。\")\n\nif __name__ == '__main__':\n&nbsp; &nbsp; app.run(port=5000)\n</code></pre><ol start=\"2\">\n<li><strong>Agent B（法律专家）：</strong>负责评估合规性、提出法律建议和法规框架，精通法律法规和数据隐私保护。</li>\n</ol><pre><code class=\"language-python\"># agent_b.py\nfrom common import create_app\n\napp = create_app(\"system: 你是一个熟悉法律法规的法律专家。\")\n\nif __name__ == '__main__':\n&nbsp; &nbsp; app.run(port=5001)\n</code></pre><ol start=\"3\">\n<li><strong>Agent C（标准化专家）：</strong>负责协调和整合建议，生成完整的技术合规方案。确保最终的合规方案完整和可实施。</li>\n</ol><pre><code class=\"language-python\"># agent_c.py\nfrom flask import Flask, request, jsonify\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nfrom langchain.agents import initialize_agent, Tool, AgentType\nfrom langchain.llms import HuggingFacePipeline\nimport requests\nimport torch\n\napp = Flask(__name__)\n\ncache_dir = './llama_cache'\nmodel_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; model_path,\n&nbsp; &nbsp; torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None,\n&nbsp; &nbsp; pad_token_id=tokenizer.eos_token_id\n)\n\ndef call_expert(url, task_requirement):\n&nbsp; &nbsp; response = requests.post(url, json={\"intent\": task_requirement}, timeout=5)\n&nbsp; &nbsp; response.raise_for_status()\n&nbsp; &nbsp; return response.json().get(\"response\", \"Error: No response from expert\")\n\nai_expert = lambda task: call_expert(\"http://localhost:5000/chat\", task)\nlaw_expert = lambda task: call_expert(\"http://localhost:5001/chat\", task)\n\nai = Tool.from_function(func=ai_expert, name=\"ai_expert\", description=\"当你需要人工智能专家知识时使用这个工具，输入为具体问题，返回为问题答案\")\nlaw = Tool.from_function(func=law_expert, name=\"law_expert\", description=\"当你需要法律合规专家知识时使用这个工具，输入为具体问题，返回为问题答案\")\n\ntools = [ai, law]\n\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\nllm = HuggingFacePipeline(pipeline=pipe)\n\nagent = initialize_agent(tools,\n&nbsp; &nbsp; &nbsp; &nbsp; llm,\n&nbsp; &nbsp; &nbsp; &nbsp; agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n&nbsp; &nbsp; &nbsp; &nbsp; verbose=True,\n&nbsp; &nbsp; &nbsp; &nbsp; max_iterations = 5,\n&nbsp; &nbsp; &nbsp; &nbsp; handle_parsing_errors = True)\n\n@app.route('/integrate', methods=['POST'])\ndef integrate():\n&nbsp; &nbsp; data = request.get_json()\n&nbsp; &nbsp; task = data.get('task', '')\n&nbsp; &nbsp; res = agent.run(task)\n&nbsp; &nbsp; return jsonify({'response': res})\n\nif __name__ == '__main__':\n&nbsp; &nbsp; app.run(port=5002)\n</code></pre><p>可以看出，在多智能体合作的时候，<strong>各个智能体在彼此的眼中都是工具</strong>。下面，通过 curl 命令向 <a href=\"http://localhost:5002/integrate\">http://localhost:5002/integrate</a> 发送 POST 请求，任务描述为制定人工智能合规标准，避免人工智能伤害人类。</p><p>返回结果为标准化专家（Agent C）通过协调计算机科学家（Agent A）和法律专家（Agent B）的响应生成的综合答案。</p><pre><code class=\"language-bash\">$ curl -X POST http://localhost:5002/integrate \\\n&nbsp; &nbsp; &nbsp; &nbsp;-H \"Content-Type: application/json\" \\\n&nbsp; &nbsp; &nbsp; &nbsp;-d '{\"task\": \"制定人工智能合规标准，避免人工智能伤害人类\"}'\n</code></pre><p>在这个设计示例中，计算机科学家和法律专家在制定人工智能合规标准时进行了分工和协作。Agent A 提出技术实施的建议和解决技术难题，而 Agent B 负责评估合规性、提出法律建议和法规框架。这种分工模式能有效结合技术实施和法律法规，以确保人工智能技术的合法合规性。</p><p>最后，我们展开介绍 common.py，这个函数接收模型目录和种子记忆（seed memory）作为参数，创建并返回一个 python 服务应用。</p><p>这里有一个重点，那就是我们要给每个 Agent 一个种子记忆，也就是它的全局人设，这是智能体应用中最重要的部分，因为<strong>如果没有一个坚固的人设，智能体在长期工作过程中一定会出现偏离</strong>，在后面课程中我将进一步解释这句话的含义。</p><pre><code class=\"language-python\"># common.py\nfrom flask import Flask, request, jsonify\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndef create_app(seed_memory):\n&nbsp; &nbsp; app = Flask(__name__)\n\n&nbsp; &nbsp; cache_dir = './llama_cache'\n&nbsp; &nbsp; model_path = cache_dir + '/LLM-Research/Meta-Llama-3-8B-Instruct'\n&nbsp; &nbsp; tokenizer = AutoTokenizer.from_pretrained(model_path)\n&nbsp; &nbsp; model = AutoModelForCausalLM.from_pretrained(\n&nbsp; &nbsp; &nbsp; &nbsp; model_path,\n&nbsp; &nbsp; &nbsp; &nbsp; torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n&nbsp; &nbsp; &nbsp; &nbsp; device_map=\"auto\" if torch.cuda.is_available() else None,\n&nbsp; &nbsp; &nbsp; &nbsp; pad_token_id=tokenizer.eos_token_id\n&nbsp; &nbsp; )\n\n&nbsp; &nbsp; # 定义聊天接口\n&nbsp; &nbsp; @app.route('/chat', methods=['POST'])\n&nbsp; &nbsp; def chat():\n&nbsp; &nbsp; &nbsp; &nbsp; data = request.get_json()\n&nbsp; &nbsp; &nbsp; &nbsp; intent = data.get('intent', '')\n\n&nbsp; &nbsp; &nbsp; &nbsp; # 构造提示词\n&nbsp; &nbsp; &nbsp; &nbsp; prompt = f\"{seed_memory}\\n请回答以下问题:{intent}\"\n&nbsp; &nbsp; &nbsp; &nbsp; input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n&nbsp; &nbsp; &nbsp; &nbsp; outputs = model.generate(input_ids, max_length=150)\n&nbsp; &nbsp; &nbsp; &nbsp; response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n&nbsp; &nbsp; &nbsp; &nbsp; return jsonify({'response': response})\n\n&nbsp; &nbsp; return app\n</code></pre><p>当然本节课的例子都是为了让你可以快速产生感性认识，这种实现方式适用于简单的示例和小规模的应用，但在生产环境中则需要进一步地学习更复杂的方案。</p><h2>小结</h2><p>好了，到这里我们来做个总结吧。在这节课中，我们介绍了 LLaMA 3 能力的几个层面。</p><p>首先是 NTP（Next Token Prediction）的基础能力。我们详细解析了其原理和实现方式，NTP通过预测下一个词来生成连贯的对话，是大语言模型生成文本的核心机制。我们展示了如何将输入文本分解为token，将其转换为嵌入形式，并通过大语言模型预测下一个词的概率分布来生成文本。我们还介绍了 LLaMA 3 和 LLaMA 3-Instruct 两个版本，并提及了LLaMA 3 的其他变体。</p><p>接着是从无状态到有状态的对话服务转变。我们讲解了如何保留对话历史信息，将无状态的LLM转化为有状态的对话服务，以确保生成的对话保持上下文的连贯性。</p><p>然后是从封闭到开放的实时信息服务，也就是我们常说的检索增强生成（RAG）技术的目标。我们介绍了如何将封闭的 LLM 转变为能够获取实时信息的开放服务。通过整合外部检索系统（例如我们使用的搜索引擎 API），模型可以在生成过程中获取最新的事实信息，以保证生成内容的准确性和时效性。</p><p>最后，我们讨论了多智能体的协作。通过多智能体的配合，我们可以解决长对话带来的推理性能和效果问题，避免职责混乱。明确分工后，各智能体可以处理不同类型的任务，从而提升整体对话系统的效率和准确性。</p><p>通过这些讲解，希望你能更深入地理解 LLaMA 3 的各项能力和应用场景，快速地掌握大模型技术的实际操作方法。</p><p>课程代码地址：<a href=\"https://github.com/tylerelyt/LLaMa-in-Action\">https://github.com/tylerelyt/LLaMa-in-Action</a></p><h2>课后思考</h2><ol>\n<li>使用 LLaMA 3 的不同版本对比实验，在评论区给出你的实验对比效果。</li>\n<li>使用 Ollama 和 HuggingFace LLaMA 3 进行内容生成，对比性能表现。</li>\n</ol><p>期待你在留言区和我交流互动。如果今天的课程对你有帮助，欢迎你把它转发出去。我们下节课见！</p><h2>实验环境</h2><p>我们将使用Ollama进行实验环境的搭建。以下是具体步骤：</p><p>你需要在以下环境中进行操作：</p><ul>\n<li>\n<p>macOS bash</p>\n</li>\n<li>\n<p>Linux bash</p>\n</li>\n<li>\n<p>Windows WSL bash</p>\n</li>\n</ul><p>首先运行以下命令来启动LLaMA 3模型：</p><pre><code class=\"language-bash\">$ ollama run llama3:text\n</code></pre><p>你会看到类似以下输出：</p><pre><code class=\"language-python\">pulling manifest\npulling cebceffdc781... 100% ▕█████████████████████████████████████████████████████████████▏ 4.7 GB\npulling 4fa551d4f938... 100% ▕█████████████████████████████████████████████████████████████▏&nbsp; 12 KB\npulling 0dbc577651fb... 100% ▕█████████████████████████████████████████████████████████████▏&nbsp; 337 B\nverifying sha256 digest\nwriting manifest\nremoving any unused layers\nsuccess\n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre><p>可以通过以下命令测试Ollama是否正常工作：</p><pre><code class=\"language-python\">$ curl -X POST http://localhost:11434/api/generate -d '{\n&nbsp; \"model\": \"llama3\",\n&nbsp; \"prompt\":\"Why is the sky blue?\"\n&nbsp;}'\n</code></pre><p>接下来启动open-webui：</p><pre><code class=\"language-python\">$ docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n</code></pre><p>你会看到类似以下输出：</p><pre><code class=\"language-python\">Unable to find image 'ghcr.io/open-webui/open-webui:main' locally\nmain: Pulling from open-webui/open-webui\n2cc3ae149d28: Pull complete\ndc57dfa1396c: Pull complete\nb275de30f399: Pull complete\n0ea58f563222: Pull complete\n251072225b40: Pull complete\n130662f3df11: Pull complete\n4f4fb700ef54: Pull complete\nde53a1836181: Pull complete\nd28e6308a168: Pull complete\ne2c345686679: Pull complete\n4a9cac9db244: Pull complete\n8f76aa437192: Pull complete\n5e8d46269631: Pull complete\n83e1a8b855bf: Pull complete\n179448cc6367: Pull complete\na3c72f49a0d3: Pull complete\nDigest: sha256:cecf06773cc0621dbe83c25fdeaf9c9bae33799cd7df14790a9b8ccf61b91764\nStatus: Downloaded newer image for ghcr.io/open-webui/open-webui:main\ncbff08075458b6342eef83c36343ec04500fe899281e0f74260aa4ed64bbe374\n</code></pre><p>之后访问 <a href=\"http://localhost:3000\">http://localhost:3000</a>，注册一个账号，就可以看到以下界面使用啦。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/ce/3f345bdbb8c5d0eddef102c700ccd6ce.png?wh=1400x730\" alt=\"图片\"></p>","neighbors":{"left":{"article_title":"开篇词｜从未来走向新的未来：探索大模型技术的无限可能","id":815720},"right":{"article_title":"02｜如何善用LLaMA 3长文本处理能力？","id":816673}},"comments":[{"had_liked":false,"id":395068,"user_name":"兵戈","can_delete":false,"product_type":"c1","uid":1017595,"ip_address":"北京","ucode":"0F1723EBCE1BC0","user_header":"https://static001.geekbang.org/account/avatar/00/0f/86/fb/4add1a52.jpg","comment_is_top":false,"comment_ctime":1729239990,"is_pvip":false,"replies":[{"id":143464,"content":"你好，兵戈！感谢反馈，很好的建议，requirements.txt 已补充。","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1729434772,"ip_address":"北京","comment_id":395068,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"Typler老师的课程代码地址：https:&#47;&#47;github.com&#47;tylerelyt&#47;llama。不过当前工程目录下缺少requirements.txt，可能会让许多跑代码的同学遇到包依赖不一致的问题，我提了一个issue：\nhttps:&#47;&#47;github.com&#47;tylerelyt&#47;llama&#47;issues&#47;1\n希望Typer老师有空补充下 :）","like_count":4,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652679,"discussion_content":"你好，兵戈！感谢反馈，很好的建议，requirements.txt 已补充。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729434773,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1038962,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/da/72/2feef236.jpg","nickname":"keep move","note":"","ucode":"C2C3FD7A78C3FD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653141,"discussion_content":"是不是先要把LLama3模型部署起来，然后在运行示例代码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1730294835,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江苏","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1017595,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/86/fb/4add1a52.jpg","nickname":"兵戈","note":"","ucode":"0F1723EBCE1BC0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652685,"discussion_content":"issue已fix，Tyler老师响应很及时！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729472917,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395035,"user_name":"兵戈","can_delete":false,"product_type":"c1","uid":1017595,"ip_address":"北京","ucode":"0F1723EBCE1BC0","user_header":"https://static001.geekbang.org/account/avatar/00/0f/86/fb/4add1a52.jpg","comment_is_top":false,"comment_ctime":1729150289,"is_pvip":false,"replies":[{"id":143448,"content":"你好兵戈，这里没有加 max_length 主要是为了和后面加上 max_length 的内容形成对比，让大家对 max_length 的功能产生一个直观的理解。\n\n建议已收到，我们会在文章中和大家补充这个设计的背景，感谢反馈！","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1729203250,"ip_address":"北京","comment_id":395035,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"Tyler老师提供示例很不错，其中第一个示例，最好加上max_length参数，否则可能会一直运行停不下来，如下：\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=100)","like_count":3,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652581,"discussion_content":"你好兵戈，这里没有加 max_length 主要是为了和后面加上 max_length 的内容形成对比，让大家对 max_length 的功能产生一个直观的理解。\n\n建议已收到，我们会在文章中和大家补充这个设计的背景，感谢反馈！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729203250,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1017595,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/86/fb/4add1a52.jpg","nickname":"兵戈","note":"","ucode":"0F1723EBCE1BC0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652602,"discussion_content":"感谢Tyler老师的答复","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729239814,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394984,"user_name":"edward","can_delete":false,"product_type":"c1","uid":1604798,"ip_address":"湖南","ucode":"09F7A5B8D2E7BD","user_header":"","comment_is_top":false,"comment_ctime":1729037725,"is_pvip":false,"replies":[{"id":143441,"content":"你好，Edward！一般来说，显存 12GB 以上的 NVIDIA 显卡和苹果 M 系列芯片都能很好地运行 8B LLaMA 模型。如果本机配置有限，还可以考虑使用云资源进行实验。比如魔搭社区提供了免费的 GPU 闲时资源，适合学习和测试。这样可以在资源不足的情况下，灵活地完成实验。","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1729085761,"ip_address":"北京","comment_id":394984,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"请问老师 实验环境需要什么样的机器配置？","like_count":3,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652522,"discussion_content":"你好，Edward！一般来说，显存 12GB 以上的 NVIDIA 显卡和苹果 M 系列芯片都能很好地运行 8B LLaMA 模型。如果本机配置有限，还可以考虑使用云资源进行实验。比如魔搭社区提供了免费的 GPU 闲时资源，适合学习和测试。这样可以在资源不足的情况下，灵活地完成实验。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1729085762,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":1069559,"avatar":"https://static001.geekbang.org/account/avatar/00/10/51/f7/b6c62ff3.jpg","nickname":"052D-131","note":"","ucode":"F6D5B8F6B5A278","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":652645,"discussion_content":"老师好，请问用苹果跑70B的llama模型，需要什么样的配置？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729350653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":652522,"ip_address":"广东","group_id":0},"score":652645,"extra":""}]},{"author":{"id":1162136,"avatar":"https://static001.geekbang.org/account/avatar/00/11/bb/98/db72a54a.jpg","nickname":"琥珀·","note":"","ucode":"7DB119EB3D4F8F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652935,"discussion_content":"AMD的CPU可以运行吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729854491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395341,"user_name":"keep move","can_delete":false,"product_type":"c1","uid":1038962,"ip_address":"江苏","ucode":"C2C3FD7A78C3FD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/da/72/2feef236.jpg","comment_is_top":false,"comment_ctime":1730295686,"is_pvip":false,"replies":[{"id":143533,"content":"同学你好，在课程的介绍页有我们的 Github 地址：https:&#47;&#47;github.com&#47;tylerelyt&#47;llama","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1730469749,"ip_address":"北京","comment_id":395341,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"示例代码如何能运行起来，能否写个简要的步骤呢","like_count":1,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653242,"discussion_content":"同学你好，在课程的介绍页有我们的 Github 地址：https://github.com/tylerelyt/llama","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1730469749,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395063,"user_name":"J Sun","can_delete":false,"product_type":"c1","uid":1003877,"ip_address":"河北","ucode":"18873D0F4EFD8F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/51/65/35f711a1.jpg","comment_is_top":false,"comment_ctime":1729230684,"is_pvip":false,"replies":[{"id":143537,"content":"同学你好，可以跟着项目代码进行练习：https:&#47;&#47;github.com&#47;tylerelyt&#47;llama.git","user_name":"作者回复","user_name_real":"编辑","uid":1002568,"ctime":1730470092,"ip_address":"北京","comment_id":395063,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"模型怎么下载\n","like_count":0,"discussions":[{"author":{"id":1002568,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/4c/48/99de2a2b.jpg","nickname":"Tyler","note":"","ucode":"A657DD2FBAF31D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653246,"discussion_content":"同学你好，可以跟着项目代码进行练习：https://github.com/tylerelyt/llama.git","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1730470092,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395572,"user_name":"S！","can_delete":false,"product_type":"c1","uid":1502788,"ip_address":"广东","ucode":"951BF2B27F0172","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","comment_is_top":false,"comment_ctime":1731257989,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"m2系列的mac pro运行demo一直没有输出结果，console输出如下：\nimport sys; print(&#39;Python %s on %s&#39; % (sys.version, sys.platform))\n&#47;usr&#47;local&#47;bin&#47;python3.10 -X pycache_prefix=&#47;Users&#47;salesonlee&#47;Library&#47;Caches&#47;JetBrains&#47;PyCharm2024.1&#47;cpython-cache &#47;Applications&#47;PyCharm.app&#47;Contents&#47;plugins&#47;python&#47;helpers&#47;pydev&#47;pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 57101 --file &#47;Users&#47;salesonlee&#47;IT&#47;dev_codes&#47;PythonProjects&#47;tylerelyt-llama&#47;chapter1&#47;lesson1&#47;example1.py \nConnected to pydev debugger (build 241.17890.14)\nLoading checkpoint shards: 100%|██████████| 4&#47;4 [01:00&lt;00:00, 15.10s&#47;it]\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n\n\n请问是因为没有使用gpu么，是硬件配置的原因？还是其它原因？ ","like_count":0},{"had_liked":false,"id":395571,"user_name":"S！","can_delete":false,"product_type":"c1","uid":1502788,"ip_address":"广东","ucode":"951BF2B27F0172","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","comment_is_top":false,"comment_ctime":1731257913,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"import sys; print(&#39;Python %s on %s&#39; % (sys.version, sys.platform))\n&#47;usr&#47;local&#47;bin&#47;python3.10 -X pycache_prefix=&#47;Users&#47;salesonlee&#47;Library&#47;Caches&#47;JetBrains&#47;PyCharm2024.1&#47;cpython-cache &#47;Applications&#47;PyCharm.app&#47;Contents&#47;plugins&#47;python&#47;helpers&#47;pydev&#47;pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 57101 --file &#47;Users&#47;salesonlee&#47;IT&#47;dev_codes&#47;PythonProjects&#47;tylerelyt-llama&#47;chapter1&#47;lesson1&#47;example1.py \nConnected to pydev debugger (build 241.17890.14)\nLoading checkpoint shards: 100%|██████████| 4&#47;4 [01:00&lt;00:00, 15.10s&#47;it]\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","like_count":0},{"had_liked":false,"id":395570,"user_name":"S！","can_delete":false,"product_type":"c1","uid":1502788,"ip_address":"广东","ucode":"951BF2B27F0172","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","comment_is_top":false,"comment_ctime":1731257905,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"m2系列mac pro 运行demo一直没有输出结果，console输出如下：","like_count":0},{"had_liked":false,"id":395566,"user_name":"S！","can_delete":false,"product_type":"c1","uid":1502788,"ip_address":"广东","ucode":"951BF2B27F0172","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","comment_is_top":false,"comment_ctime":1731248412,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"运行课程demo需要什么样的碍硬件资源？","like_count":0},{"had_liked":false,"id":395565,"user_name":"S！","can_delete":false,"product_type":"c1","uid":1502788,"ip_address":"广东","ucode":"951BF2B27F0172","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","comment_is_top":false,"comment_ctime":1731245277,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":2,"product_id":100828301,"comment_content":"Mac pro可以运行llama3么","like_count":0,"discussions":[{"author":{"id":1502788,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","nickname":"S！","note":"","ucode":"951BF2B27F0172","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653780,"discussion_content":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# 加载分词器和模型\ncache_dir = &#39;./llama_cache&#39;\nmodel_path = cache_dir + &#39;/LLM-Research/Meta-Llama-3-8B&#39;\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# 检查MPS是否可用\nprint(torch.backends.mps.is_available())\nprint(torch.backends.mps.is_built())\n# 设置设备为MPS，如果可用\ndevice = torch.device(&#34;mps&#34; if torch.backends.mps.is_available() else &#34;cpu&#34;)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n    device_map=&#34;auto&#34; if torch.backends.mps.is_available() else None\n)\n\n# 编码输入并将其移至模型设备\ninput_text = &#34;在一个阳光明媚的早晨，Alice决定去森林里探险。她走着走着，突然发现了一条小路。&#34;\ninputs = tokenizer(input_text, return_tensors=&#34;pt&#34;).to(device)\n\n# 生成并解码文本\nwith torch.no_grad():\n    outputs = model.generate(**inputs)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(generated_text)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1731456939,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1502788,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ee/44/324e3cd6.jpg","nickname":"S！","note":"","ucode":"951BF2B27F0172","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653779,"discussion_content":"m系列的mac 也可以使用gpu， 前提mac os 版本需要在13.0之后，torch版本可以到官网上查看安装，链接：https://pytorch.org/get-started/locally/， 安装脚本：pip3 install torch torchvision torchaudio\npython 代码需要调整，采用mps gpu：\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1731456905,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395196,"user_name":"keep move","can_delete":false,"product_type":"c1","uid":1038962,"ip_address":"美国","ucode":"C2C3FD7A78C3FD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/da/72/2feef236.jpg","comment_is_top":false,"comment_ctime":1729683807,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100828301,"comment_content":"python学习那个方向 pytorch吗","like_count":0},{"had_liked":false,"id":395114,"user_name":"Orson","can_delete":false,"product_type":"c1","uid":3578571,"ip_address":"上海","ucode":"F18FC4C6CD08A3","user_header":"https://static001.geekbang.org/account/avatar/00/36/9a/cb/53af3dc6.jpg","comment_is_top":false,"comment_ctime":1729485504,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100828301,"comment_content":"# agent_a.py\nfrom common import create_app\n\napp = create_app(&quot;system: 你是一个熟悉人工智能技术的计算机科学家。&quot;)\n\nif __name__ == &#39;__main__&#39;:\n    app.run(port=5000)\n请问，在用AutoDL这样的云服务的时候，app.run(port=5000）需要怎么修改。或者，agent_a、b、c和common文件需要做哪些修改？","like_count":0},{"had_liked":false,"id":395085,"user_name":"Allen","can_delete":false,"product_type":"c1","uid":1168265,"ip_address":"上海","ucode":"10D365C791D28F","user_header":"https://static001.geekbang.org/account/avatar/00/11/d3/89/fcf95d32.jpg","comment_is_top":false,"comment_ctime":1729355736,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100828301,"comment_content":"你好，请问在调用模型时with torch.no_grad():为啥后面几段里没有加？会影响性能吗？谢谢。","like_count":0},{"had_liked":false,"id":395084,"user_name":"Dowen Liu","can_delete":false,"product_type":"c1","uid":1440423,"ip_address":"北京","ucode":"DD072D44AD353D","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLCrJQ4AZe8VrDkR6IO03V4Tda9WexVT4zZiahBjLSYOnZb1Y49JvD2f70uQwYSMibUMQvib9NmGxEiag/132","comment_is_top":false,"comment_ctime":1729350880,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100828301,"comment_content":"不用python 能不能做同样的调用呢？比如直接调用接口、spring ai ","like_count":0},{"had_liked":false,"id":395064,"user_name":"J Sun","can_delete":false,"product_type":"c1","uid":1003877,"ip_address":"河北","ucode":"18873D0F4EFD8F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/51/65/35f711a1.jpg","comment_is_top":false,"comment_ctime":1729232093,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100828301,"comment_content":"模型在哪里下载","like_count":0,"discussions":[{"author":{"id":1017595,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/86/fb/4add1a52.jpg","nickname":"兵戈","note":"","ucode":"0F1723EBCE1BC0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652686,"discussion_content":"通过modelscope下载，参见Tyler老师代码工程中的prepare_model文件，核心代码如下：\n```\nfrom modelscope import snapshot_download\n\n# 下载模型\ncache_dir = &#39;./llama_cache&#39;\nmodel_id = snapshot_download(&#34;LLM-Research/Meta-Llama-3-8B&#34;, cache_dir=cache_dir)\n```","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1729473083,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}