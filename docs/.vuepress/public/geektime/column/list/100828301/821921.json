{"id":821921,"title":"11｜如何借助LLaMA 3赋能索引构建？","content":"<p>你好，我是Tyler！</p><p>上节课我们讨论了检索增强生成（RAG，Retrieval-Augmented Generation）方法中 LLaMA 3 所能带来的提升。今天，我们将深入学习LLaMA 3在实际应用中的作用。计算机科学领域有一句常用的话：“垃圾进，垃圾出。” 这句话强调了输入数据的质量对系统输出结果的决定性作用。</p><p>在基于RAG的模型中，输入数据的质量至关重要，它直接决定了生成内容的优劣。RAG 模型主要由两个环节组成：检索（Retrieval）和生成（Generation）。简而言之，R 负责从知识库中检索相关信息，G 基于检索到的内容生成自然语言回答。由此可见，检索阶段的输入质量直接影响生成阶段的表现。如果想提高生成结果的准确性与上下文相关性，首先需要专注于优化检索阶段的表现，例如提高知识库的覆盖面、检索算法的准确度，以及过滤无关信息的能力。</p><h2>基于文章片段向量检索的问题</h2><p>当前许多 RAG 实现采用基于文章片段的向量检索技术，这种方法虽然在某些应用场景下提升了效率，但也存在一些明显的局限性。</p><ol>\n<li>\n<p><strong>语义信息碎片化</strong>：为了适应向量检索的机制，长篇文章通常会被切割成多个独立的小片段。然而，这种片段化处理会割裂文章的语义联系，无法完整保留原文中的上下文关联。在生成阶段，系统由于缺乏跨片段的语义信息，可能输出不连贯或前后矛盾的回答。</p>\n</li>\n<li>\n<p><strong>歧义性问题</strong>：向量检索在处理多义词时容易受限于局部片段的信息，无法理解词语的上下文语境。例如，“cloud”既可能指“云计算”，也可能指“气象云”，甚至日常生活中的“云”。如果模型无法正确消解歧义，检索内容可能偏离用户预期。</p>\n</li>\n<li>\n<p><strong>缺乏全局知识</strong>：向量检索基于片段化的文档信息，通常缺乏对整体语境或跨领域知识的综合理解。这使得系统在应对需要全局视角或跨领域信息的任务时表现较差，生成的回答难以深入挖掘信息之间的复杂关系。</p>\n</li>\n</ol><!-- [[[read_end]]] --><h2>优化方案：通过 LLaMA 3 构建 GraphRAG</h2><p>为了提升 RAG 模型的性能，需要对原始数据进行更加高级的处理，结合更丰富的知识。这里提出的通过 <strong>LLaMA 3</strong> 模型来构建 <strong>GraphRAG</strong> 的方法，将结构化的知识图谱与向量检索结合，从而优化信息的检索与生成。</p><p>首先，我简要介绍一下知识图谱的概念。知识图谱以<strong>三元组形式（主语-谓语-宾语）</strong>来表示信息，捕捉不同知识点之间的关系，并通过这些关系构建出一个相互关联的图结构。知识图谱提供了更深层次的上下文理解，能够帮助系统更好地处理复杂关系，增强生成结果的连贯性与准确性。</p><p>在构建知识图谱的过程中，第一步是从原始文本中提取三元组。我们可以利用 LLaMA 3 模型作为 NER 工具，识别出文本中的实体和它们之间的关系。与传统的 NER 方法相比，LLaMA 3 凭借其强大的语义理解能力，能够识别出更复杂和细粒度的实体关系。</p><p><img src=\"https://static001.geekbang.org/resource/image/42/a1/42f72935477abab5bc57bb3cbb7760a1.png?wh=1868x680\" alt=\"\"></p><p>以下是一个具体的三元组示例：</p><p>通过 LLaMA 3 模型提取三元组时，系统可以识别文本中的复杂关系。例如，输入的句子可能是：“Albert Einstein 于 1905 年提出了相对论。”</p><p>在这个例子中，“Albert Einstein” 是主语，表示一个具体的实体；“提出” 是谓语，描述了主语与宾语之间的关系；“相对论” 是宾语，表示另一个具体的实体。这种结构化的信息表述可以帮助知识图谱捕捉到不同知识点之间的关联。</p><p><strong>三元组示例：</strong></p><ul>\n<li>\n<p>主语（Entity 1）：<strong>Albert Einstein</strong></p>\n</li>\n<li>\n<p>谓语（Relation）：<strong>提出</strong></p>\n</li>\n<li>\n<p>宾语（Entity 2）：<strong>相对论</strong></p>\n</li>\n</ul><p>下面我来介绍一下如何用 LLaMA 3 来提取三元组。</p><h3>1. 提示目标 (Goal)</h3><p>首先，我们需要定义任务目标。提示词的核心要求是：“给定一篇可能与任务相关的文档和实体类型列表，识别文中所有指定类型的实体及其之间的关系。” 这为整个提取任务设定了方向——我们不仅要识别实体，还要深入分析它们之间的关联，以确保最终生成的内容具备准确性和相关性。</p><h3>2. 实体识别 (Step 1)</h3><p>接下来，进入实体识别阶段。我们需要从文中提取所有符合要求的实体，包括每个实体的名称、类型和详细描述。提示词明确指出：“识别所有实体，并提取实体名称、实体类型及其描述。” 这是信息提取的基础，确保了后续关系分析的准确性。LLaMA 3 模型在这一过程中能够高效识别出复杂的命名实体，并为它们添加合适的语义标签。</p><h3>3. 关系识别 (Step 2)</h3><p>完成实体识别后，进入关系识别阶段。在此步骤中，我们需要从已识别的实体中找出那些有明确关系的实体对，并为每对实体提供关系描述及强度评分。提示词要求：“识别出所有明确相关的实体对，并描述其关系及强度。” 通过这种分析，能够有效构建知识图谱中的关系节点，为后续的生成阶段提供更加连贯的上下文支持。</p><h3>4. 输出格式 (Step 3 &amp; 4)</h3><p>最后一步是将识别出的实体和关系以结构化格式输出，以便后续处理和检索。提示词指出：“将步骤1和2中识别出的实体和关系按统一格式输出。” 格式化的输出不仅提升了数据的组织性，还使得后续模型可以高效解析和应用这些三元组信息，特别是在大规模检索任务中。</p><p>微软的 GraphRAG 实现提供了一个实际案例，我们来看一下。</p><pre><code class=\"language-plain\">-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [{entity_types}]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"{{tuple_delimiter}}&lt;entity_name&gt;{{tuple_delimiter}}&lt;entity_type&gt;{{tuple_delimiter}}&lt;entity_description&gt;)\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: an integer score between 1 to 10, indicating strength of the relationship between the source entity and target entity\nFormat each relationship as (\"relationship\"{{tuple_delimiter}}&lt;source_entity&gt;{{tuple_delimiter}}&lt;target_entity&gt;{{tuple_delimiter}}&lt;relationship_description&gt;{{tuple_delimiter}}&lt;relationship_strength&gt;)\n\n3. Return output in {language} as a single list of all the entities and relationships identified in steps 1 and 2. Use **{{record_delimiter}}** as the list delimiter.\n\n4. If you have to translate into {language}, just translate the descriptions, nothing else!\n\n5. When finished, output {{completion_delimiter}}.\n\n-Examples-\n######################\n{examples}\n</code></pre><p>微软的官方 GraphRAG 就是通过这样的提示词，使用大模型生成了构建图片所必要的三元组信息，为 RAG 提供了结构化的索引。</p><p><img src=\"https://static001.geekbang.org/resource/image/c0/91/c01c8302ed531e4a97724342506e3c91.png?wh=1400x721\" alt=\"图片\"></p><p>到这里，我们已经成功构建了一个基础的知识图谱，它为后续的信息检索和推理提供了结构化的数据支持。接下来，我们讨论两个关键的检索策略——本地搜索和全局搜索，帮助我们更好地理解如何在知识图谱中高效检索信息。</p><h2>本地搜索与全局搜索</h2><p>本地搜索专注于在知识图谱中围绕特定实体或少量相关实体进行的局部检索。这种方法非常适合处理与特定上下文相关的查询任务。例如，当用户的问题与某个具体主题或概念密切相关时，本地搜索能够迅速锁定相关的三元组和实体，避免在整个知识图谱中进行无关的冗余搜索。其优势在于速度快、计算资源消耗低，并能保持上下文的连贯性，尤其适用于具体性较强的问题。</p><p>然而，为了使大模型不仅能处理局部问题，还能有效回答全局性的问题，GraphRAG 引入了全局搜索策略。其关键在于社区总结和推理。</p><p>具体而言，在索引构建的第二步，GraphRAG 通过<strong>层次聚类技术</strong>（如常用的 Leiden 算法）对构建好的知识图谱进行社区划分。此过程的目标是将知识图谱按主题划分成多个社区，每个社区包含一组紧密相关的实体和关系。随后，系统为每个社区生成一个摘要，这个摘要可以视为该社区的“主题总结”，帮助我们宏观把握这一部分知识的主要内容。</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/b0/5afe7d7118e7af55dc50ed4ff36e19b0.png?wh=1600x604\" alt=\"图片\"></p><p>因此，当用户提出宏观问题时，系统能够跳过逐字逐句的精确检索，直接从这些社区摘要中快速找到最相关的总结内容。接下来，大模型可以基于这些预先总结的信息，进一步加工、推理并生成最终答案。这一功能显著提升了系统处理复杂全局问题的能力，尤其在需要跨领域综合知识的场景中表现出色。文末提供了基于 LLaMA 3 实现的 GraphRAG 示例，展示了其具体应用效果。</p><h2>GraphRAG 的局限性和注意要点</h2><p>尽管 GraphRAG 展示了强大的搜索和推理能力，但在实际应用中仍然存在一些局限性和需要关注的要点。</p><h3>1. 性能瓶颈</h3><p>在构建索引过程中，特别是在进行社区划分和全局搜索时，系统需要进行大量的推理计算。这种高强度的计算需求可能导致性能瓶颈，尤其是在处理大规模数据集时。因此，建议监控系统资源的使用情况，并根据需要进行优化，以确保在满足性能需求的同时，高效完成信息检索和生成任务。</p><h3>2. 实体识别的模型选择</h3><p>命名实体识别（NER）是构建知识图谱的关键步骤，负责从文本中提取相关实体。尽管 NER 重要，但其本质相对简单，并不总是需要复杂的大型模型。考虑到资源消耗，可以使用体量较小、效率更高的模型来执行 NER 任务，以提高整体系统的响应速度和处理效率。这一调整不仅能减少资源消耗，还能确保系统在处理高频率请求时的稳定性。</p><h3>3. 动态更新机制</h3><p>随着信息的不断更新，静态的知识图谱可能会迅速过时。因此，保持知识图谱的动态性和时效性至关重要。未来的工作可以考虑实现自动化的更新机制，确保知识库中的信息始终是最新的。这样一来，系统在生成内容时将更加准确和相关，能够更好地满足用户的需求。</p><h2>小结</h2><p>学到这里，我们做个总结吧。这节课我们讨论了如何利用 LLaMA 3 来优化 RAG 模型的索引构建工作，关键在于通过构建知识图谱来提高信息提取和生成的效果。首先，LLaMA 3 能够高效进行命名实体识别（NER），从原始文本中提取三元组（主语-谓语-宾语），为知识图谱提供结构化的数据基础。通过准确识别实体及其属性，LLaMA 3 为后续的检索和生成阶段奠定了坚实的基础。</p><p>在实体识别过程中，LLaMA 3 还能够分析并提取实体之间的关系，增强生成内容的上下文相关性。此外，GraphRAG 通过应用层次聚类技术对知识图谱进行社区划分，并为每个社区生成主题摘要。这一策略使系统在处理复杂查询时能够快速找到最相关的信息，而无需逐字检索。</p><p>LLaMA 3 将这些优化整合到索引生成中，显著提升了 RAG 模型在面对复杂问题时的表现，从而确保系统在检索和生成内容时的准确性和效率。</p><h2>思考题</h2><ol>\n<li>\n<p>在哪些特定场景下，全局搜索能够提供更好的结果？</p>\n</li>\n<li>\n<p>如果没有全局搜索的需求，知识图谱的方案是否仍然比文档切片检索更有效？</p>\n</li>\n<li>\n<p>根据课后给出的步骤，将你日常使用的技术文档构建成GraphRAG，看看是否能够达到意想不到的效果。</p>\n</li>\n</ol><p>欢迎你把你的经验分享到留言区，也欢迎你把这节课的内容分享给其他朋友，我们下节课再见！</p><hr><h3>1. 创建环境</h3><p>我们先为 GraphRAG 创建一个新的虚拟环境，使用 Conda 进行方便的管理。执行以下命令来创建并激活环境：</p><pre><code class=\"language-bash\">conda create --name graphenv python=3.11 -y &amp;&amp; conda activate graphenv\n</code></pre><p><strong>注意</strong>：使用 Python 3.11 是因为它与 GraphRAG 和 Ollama 兼容。</p><h3>2. 安装 GraphRAG 和 Ollama</h3><p>在环境创建好之后，执行以下命令来安装 GraphRAG 和 Ollama：</p><pre><code class=\"language-bash\">pip install graphrag==0.1.1 ollama\n</code></pre><h3>3. 准备工作区</h3><p>创建一个目录来存放 RAG 项目文件：</p><pre><code class=\"language-bash\">mkdir -p ./demo/input\n</code></pre><p>将你的输入文本文件添加到 <code>input</code> 目录中。请保持文本简短，以减少索引生成所需的时间和计算资源。</p><h3>4. 初始化 GraphRAG 工作区</h3><p>执行以下命令来初始化 GraphRAG 工作区：</p><pre><code class=\"language-bash\">python -m graphrag.index --init --root ./demo\n</code></pre><p>这个命令将在 <code>./demo</code> 目录下生成 <code>.env</code> 和 <code>settings.yaml</code> 文件。</p><ul>\n<li>\n<p><code>.env</code><strong>文件</strong>包含了运行 GraphRAG 所需的环境变量，默认情况下会生成一个 <code>GRAPHRAG_API_KEY=&lt;API_KEY&gt;</code>。</p>\n</li>\n<li>\n<p><code>settings.yaml</code><strong>文件</strong>是 GraphRAG 的设置文件，可以根据需求修改。</p>\n</li>\n</ul><h3>5. 配置 GraphRAG 使用 Ollama</h3><h4>修改 <code>.env</code></h4><p>将 <code>.env</code> 文件中的 <code>GRAPHRAG_API_KEY</code> 设置为空值：</p><pre><code class=\"language-bash\">GRAPHRAG_API_KEY=EMPTY\n</code></pre><h4>修改 <code>settings.yaml</code></h4><p>在 <code>settings.yaml</code> 文件中进行以下修改，配置 Ollama 进行嵌入计算：</p><pre><code class=\"language-yaml\">llm: \n  api_base: http://localhost:11434/v1 \n  model: meta-llama/Meta-Llama-3-8B-Instruct \nembedding:\n  llm:\n    model: nomic-embed-text \n    api_base: http://localhost:11434/api\n</code></pre><h3>6. 设置 Ollama 进行嵌入计算</h3><ol>\n<li>\n<p>从 <a href=\"https://ollama.com\">Ollama 官方页面</a> 安装 Ollama。</p>\n</li>\n<li>\n<p>安装完成后，执行以下命令来拉取嵌入模型：</p>\n</li>\n</ol><pre><code class=\"language-bash\">ollama pull nomic-embed-text\n</code></pre><p>这将会下载 <code>nomic-embed-text</code> 嵌入模型，用于 GraphRAG 的嵌入计算。</p><h3>7. 修改 GraphRAG 代码以支持 Ollama</h3><p>为了让 Ollama 嵌入模型与 GraphRAG 兼容，我们需要修改 GraphRAG 的部分代码。</p><ol>\n<li>查找 GraphRAG 安装目录：</li>\n</ol><pre><code class=\"language-bash\">pip show graphrag\n</code></pre><p>查看输出的 <code>Location</code> 字段，比如 <code>/Users/username/miniconda3/envs/graphenv/lib/python3.11/site-packages</code>。</p><ol>\n<li>然后导航到该目录，并找到 <code>graphrag/llm/openai/openai_embeddings_llm.py</code> 文件，替换其内容为以下代码：</li>\n</ol><pre><code class=\"language-python\">from typing_extensions import Unpack\nfrom graphrag.llm.base import BaseLLM\nfrom graphrag.llm.types import (\n    EmbeddingInput,\n    EmbeddingOutput,\n    LLMInput,\n)\nimport ollama\n\n\nclass OpenAIEmbeddingsLLM(BaseLLM[EmbeddingInput, EmbeddingOutput]):\n\n\n    async def _execute_llm(\n        self, input: EmbeddingInput, **kwargs: Unpack[LLMInput]\n    ) -&gt; EmbeddingOutput | None:\n        embedding_list = []\n        for inp in input:\n            embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=inp)\n            embedding_list.append(embedding[\"embedding\"])\n        return embedding_list\n</code></pre><p>保存修改。</p><h3>8. 运行 GraphRAG</h3><p>配置完成后，导航到 <code>demo</code> 目录，并执行以下命令来生成索引：</p><pre><code class=\"language-bash\">python -m graphrag.index --root ./demo\n</code></pre><p>等待索引生成完成。</p><h3>9. 查询 GraphRAG</h3><p>一旦索引生成完成，你就可以开始查询了。以下是全局查询的示例：</p><pre><code class=\"language-bash\">python -m graphrag.query --root ./demo --method global \"Your query from the context\"\n</code></pre><p>根据需求，你可以进行更多种类的查询。有关详细查询方式，请参考 GraphRAG 文档。</p>","comments":[]}