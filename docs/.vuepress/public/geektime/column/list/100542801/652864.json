{"id":652864,"title":"08｜数据获取：如何使用Scrapy框架爬取新闻数据？","content":"<p>你好，我是黄鸿波。</p><p>上一节课，我们对Scrapy框架有了一个整体的了解，也实际地安装了Scrapy库并搭建了一个基础的Scrapy工程。这节课，我们就继续在这个工程的基础上爬取新浪新闻中的数据，并对爬取到的数据进行解析。</p><h2>使用Scrapy框架抓取数据</h2><p>我们首先打开sina\\sina\\spider下面的sina_spider.py文件，在这个文件中，Scrapy框架给我们写了一个基础的框架代码。</p><pre><code>import scrapy\n \nclass SinaSpiderSpider(scrapy.Spider):\n    name = 'sina_spider'\n    allowed_domains = ['sina.com.cn']\n    start_urls = ['http://sina.com.cn/']\n \n    def parse(self, response):\n        pass\n</code></pre><p>这段代码主要是对整个爬虫程序做了一个简单的定义，定义了爬虫的名字为“sina_spider”，爬取的域名为“sina.com.cn”，爬取的URL是“<a href=\"http://sina.com.cn/\">http://sina.com.cn/</a>”。最后它还贴心地帮我们定义了一个解析函数，这个解析函数的入参就是服务器返回的response值。现在，我们要开始分析我们要爬取的内容，并对这个函数进行改写。</p><h3>页面分析</h3><p>我们先以网易的国内新闻为例来分析一下。我们先看下面这个界面。</p><p><img src=\"https://static001.geekbang.org/resource/image/10/c5/105f1037b22ba0d30f68346c39c065c5.png?wh=2412x1391\" alt=\"\"></p><p>我们要分析的是界面里最新新闻这个部分。可以看到这个新闻列表中一共包含了下面这几部分：标题、摘要、时间、关键词。我们还可以看到，时间在1小时之内的会显示为“XX分钟前”，在1小时以上的会显示今天具体的某个时间点。</p><!-- [[[read_end]]] --><p>接着我们把页面拉到最下面。</p><p><img src=\"https://static001.geekbang.org/resource/image/43/9e/43ed7a9c7a42f8cf44244f795372ba9e.png?wh=1659x1830\" alt=\"\"></p><p>可以看到，今天之前的新闻会显示出具体的日期，并且最下面有一个导航条用来翻页。</p><p>我们随便点击一条新闻进入详情页看一下。可以看到里面包含了图片和文字，其中文字部分最上面有标题，下面有日期和时间，再下面是正文。当然，还充斥着广告和我们不需要的信息，这些我们暂时不用管。</p><p><img src=\"https://static001.geekbang.org/resource/image/b9/2c/b9eb79c35ae40f4e2d3c4e607d7fa12c.png?wh=1475x1298\" alt=\"\"></p><p>接下来我们分析一下前面的列表以及这个详情页的内容，抓取我们想要的信息。</p><p>我们知道，所有的页面从根本上来说都是由HTML页面构成的，爬虫想要爬取的内容就藏在这些HTML页面中。在Chrome浏览器中，我们按下键盘上的F12键就能够打开开发者工具模式。我们可以利用这个模式查看网页的HTML源文件、请求的信息文件以及网络返回等。我们选择上面的Element选项卡，就能看到网页的HTML源文件，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/0c/cf/0cca5213c59b5eb24c2813850fef0dcf.png?wh=2734x1465\" alt=\"\"></p><p>因为我们要找的是我们需要的列表，所以我们可以点击开发者工具左上角的小箭头，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/0c/8f/0c257c8072f7b1fb8649eb7e1f53cf8f.png?wh=2036x290\" alt=\"\"></p><p>然后用鼠标点击我们想要的列表，右侧的HTML代码就会跟着跳转到相应的部分。</p><p><img src=\"https://static001.geekbang.org/resource/image/dd/8b/dd94c05aa27ebc26148f56cb9c90258b.png?wh=2330x1065\" alt=\"\"></p><p>那么这个时候，我们需要找到这里面的其中一条，然后查看右面的HTML源文件。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/85/4fff3041b2f175a092604a3d29b70285.png?wh=2335x994\" alt=\"\"></p><p>我们可以发现，实际上，在这个列表中，每一条内容都会被包含在class为“feed-card-item”的标签中，我们把这个标签展开来详细地分析一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/d8/5a27522b9d887103a5b270d4930ca6d8.png?wh=1893x719\" alt=\"\"></p><p>可以看到，标题被包含在h2标签里的a标签中，时间被包含在h2标签里class为feed-card-a feed-card-clearfix下面的feed-card-time中，然后这条内容的链接就是h2标签里的a标签的链接。</p><p>好了，知道了我们要的标题、时间以及对应的链接，接下来，我们就可以通过爬虫把它们拿下来了。</p><h3>爬取列表</h3><p>使用Scrapy拿标签，比较方便的一种方法是使用Selenium库。</p><p>Selenium是一个用于测试Web应用程序的工具，Selenium测试可以直接运行在浏览器中，就像真正的用户在操作一样。也就是说，我们可以使用Selenium库来模拟点击、上滑和下滑等操作。</p><p>要想使用这个库，首先要在开发环境中安装它。安装方法也比较简单，直接在我们的Anaconda环境中使用pip安装就好。具体做法是切换到scrapy_recommendation环境中，执行下面的命令。</p><pre><code>pip install selenium\n</code></pre><p>安装完成后如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/78/45128e532b0a6bcd316f8ca61500c078.png?wh=1905x983\" alt=\"\"></p><p>接下来我们就可以使用Selenium来进行浏览器页面访问工作了，我们把上面的代码改写如下。</p><pre><code>import scrapy\nfrom scrapy.http import Request\nfrom selenium import webdriver\n \n \nclass SinaSpiderSpider(scrapy.Spider):\n    name = 'sina_spider'\n \n \n    def __init__(self):\n        self.start_urls = ['https://news.sina.com.cn/china/']\n        self.option = webdriver.ChromeOptions()\n        self.option.add_argument('no=sandbox')\n        self.option.add_argument('--blink-setting=imagesEnable=false')\n \n    def start_requests(self):\n        for url in self.start_urls:\n            yield Request(url=url, callback=self.parse)\n \n    def parse(self, response):\n        driver = webdriver.Chrome(chrome_options=self.option)\n        driver.set_page_load_timeout(30)\n        driver.get(response.url)\n \n        title = driver.find_elements_by_xpath(&quot;//h2[@class='undefined']/a[@target='_blank']&quot;)\n        time = driver.find_elements_by_xpath(&quot;//h2[@class='undefined']/../div[@class='feed-card-a &quot;\n                                             &quot;feed-card-clearfix']/div[@class='feed-card-time']&quot;)\n \n        for i in range(len(title)):\n            print(title[i].text)\n            print(time[i].text)\n</code></pre><p>可以看到，这段代码相比上面那段代码有了非常大的改变。首先，在最上面，我们多导入了两个包。</p><pre><code>from scrapy.http import Request\nfrom selenium import webdriver\n</code></pre><p>这段代码第一行的意思是从Scrapy的http模块导入Request这个包，第二行的意思是从Selenium库导入Webdriver这个包。</p><p>Request包顾名思义，就是用来做HTTP请求。也就是说，我们在向网站服务器请求数据时，就是Request包在起作用。而Selenium下面的webdriver包是一组开源的API，用于自动化测试Web应用程序。在我们的程序中，主要是利用它来打开浏览器，以及设置打开时的一些信息。</p><p>接着，我们把这个class进行了重构，加入了__init__这个构造函数。我们可以先粗略地理解为，当我们运行这个Python文件时，就会先去执行__init__函数里面的内容。</p><p>我们将之前的start_urls加入到了__init__中，并在前面加上self。接着，我们定义了webdriver中关于Chrome浏览器的一些参数。</p><pre><code>self.option = webdriver.ChromeOptions()\nself.option.add_argument('no=sandbox')\nself.option.add_argument('--blink-setting=imagesEnable=false')\n</code></pre><p>我在这里加了两个参数，一个是“no=sandbox”，它表示取消沙盒模式，也就是说让它在root权限下执行。另一个参数是“–blink-setting=imagesEnable=false”，它表示不加载图片，因为我们只想要文字部分，加上这一句可以提升爬取速度和效率。</p><p>接下来，我们增加了一个start_requests()函数，这实际上也是Scrapy自带的函数，它的主要作用是定义Scrapy框架的起始请求，如果在这个起始请求中有重复的URL，它会自动进行去重操作。</p><pre><code>def start_requests(self):\n        for url in self.start_urls:\n            yield Request(url=url, callback=self.parse)\n</code></pre><p>然后在解析函数中，我们定义了一个driver，它调用了webdriver的chrome函数，代表这是使用Chrome浏览器来爬取的。我们还把加载页面的超时时间设置为了30秒，也就是说如果30秒还加载不出来，就去请求下一个页面，而这下一个页面就是从start_requests()函数中获得的。然后我们调用driver.get来获取response的URL，就可以拿到response信息了。</p><pre><code>def parse(self, response):\n    driver = webdriver.Chrome(chrome_options=self.option)\n    driver.set_page_load_timeout(30)\n    driver.get(response.url)\n\n\n    title = driver.find_elements_by_xpath(&quot;//h2[@class='undefined']/a[@target='_blank']&quot;)\n    time = driver.find_elements_by_xpath(&quot;//h2[@class='undefined']/../div[@class='feed-card-a &quot;\n                                         &quot;feed-card-clearfix']/div[@class='feed-card-time']&quot;)\n\n\n    for i in range(len(title)):\n        print(title[i].text)\n        print(time[i].text)\n</code></pre><p>在后面的代码中，我们主要又做了两件事。一个是获取title信息，一个是获取time信息。</p><p>我们使用driver.find_elements_by_xpath()函数获取HTML标签中的内容，根据我们在最前面的分析，title被存在“//h2[@class=‘undefined’]/a[@target=‘_blank’]”中，而time被存在“//h2[@class=‘undefined’]/…/div[@class=‘feed-card-a feed-card-clearfix’]/div[@class=‘feed-card-time’]”中，因此，我们可以通过driver.find_elements_by_xpath()获取到里面的内容。</p><p><strong>要注意的是，我们获取到的内容一般是以一个list的形式存放，所以我们还需要使用for循环拿到里面的信息。</strong></p><p>正常来讲，完成上面这段代码之后，运行main.py文件，就会得到如下图所示的结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/3d/87/3d2c52211yy4acc2b2b45350ac59df87.png?wh=2895x1299\" alt=\"\"></p><p>可以看到，我们想要的时间和标题都已经输出出来了。</p><p>不过，虽然我们现在已经得到了时间，但是输出的格式却不统一：有的显示的是今天的某个时间，有的显示的是日期加时间。所以我们要对时间做进一步处理，可以在刚刚的for循环代码下面加上处理代码。</p><pre><code>\ttoday = datetime.datetime.now()\n            eachtime = time[i].text\n            eachtime = eachtime.replace('今天', str(today.month) + '月' + str(today.day) + '日')\n \n            if '分钟前' in eachtime:\n                minute = int(eachtime.split('分钟前')[0])\n                t = datetime.datetime.now() - datetime.timedelta(minutes=minute)\n                t2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)\n            else:\n                if '年' not in eachtime:\n                    eachtime = str(today.year) + '年' + eachtime\n                t1 = re.split('[年月日:]', eachtime)\n                t2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),\n                                       minute=int(t1[4]))\n \n            print(t2)\n</code></pre><p>我们再运行一下程序，得到如下结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/ed/46/edb1674167760b7b852d23e8cc29e046.png?wh=2552x870\" alt=\"\"></p><p>可以看到，现在时间已经变成了我们想要的样子。<strong>到这里我们的列表爬取工作就完成了，接下来开始爬取详情页的信息。</strong></p><h3>爬取详情页</h3><p>我们知道，如果是人为操作，需要点击相应标题进入详情页。对于爬虫程序来说也是一样的，我们需要从HTML文件中提取标题对应的链接，然后再传给爬虫程序进行数据的爬取，最后处理对应的response。</p><p>我们先来看看怎么获取我们所需要的链接。</p><p><img src=\"https://static001.geekbang.org/resource/image/43/2b/43b7d4b9ed070a150056a4c3d224d42b.png?wh=2007x1249\" alt=\"\"></p><p>通过分析HTML源文件我们可以看到，每一个标题都在a标签中，而a标签里面的href就是它所对应的链接。所以，我们只需要取出href，就可以取出详情页的链接了。</p><p>要实现这一点，我们只需要在解析title后面加上下面这条代码。</p><pre><code>href = title[i].get_attribute('href')\n</code></pre><p>接下来，为了获取正文信息，在Scrapy中，我们需要把request请求给yield出去，在里面放上链接。</p><p>首先，我们把需要的内容yield到下一页，传递它最好的方式就是使用ItemPipeline。可以看到我们创建Scrapy工程之后，工程目录下会自动创建一个items.py。我们打开这个文件，会发现它自带了一些代码。</p><pre><code># Define here the models for your scraped items\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/items.html\n \nimport scrapy\n \n \nclass SinaItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass\n</code></pre><p>这段代码里导入了scrapy包，并自动帮我们创建了一个SinaItem类，这里还通过注释的方法告诉了我们这个类里面应该怎么写。我们就来照葫芦画瓢，把我们想要的字段加入进来。</p><p>代码会变成如下形式。</p><pre><code>class SinaItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = scrapy.Field()\n    desc = scrapy.Field()\n    times = scrapy.Field()\n\ttype = scrapy.Field()\n</code></pre><p>我们在里面定义了四个字段，分别是title、desc、times和type，分别用来表示标题、内容、时间和类型。这里的标题和时间通过列表来获取，内容是详情页里的。而类型我们默认为国内，如果后面我们需要爬取其他的类别，比如综艺、体育等，我们就不需要新建class了，只要重新在这个type字段中赋值即可。</p><p>好了，进行到这里，我们就可以回到我们的爬虫代码去引入这部分内容了。首先我们要在第一行引入我们的item文件，加入下面的代码。</p><pre><code>from sina.items import SinaItem\n</code></pre><p>然后，我们在爬虫文件的parse函数中引入SinaItem类，并在函数的末尾对其赋值我们想要的内容。</p><pre><code>item = SinaItem()\nitem['type'] = 'news'\nitem['title'] = title[i].text\nitem['times'] = t2\n</code></pre><p>接着，我们可以在最后把item给yield出去，在代码的最后加入如下内容。</p><pre><code>yield Request(url=response.urljoin(href), meta={'name': item}, callback=self.parse_namedetail)\n</code></pre><p>这样我们就可以顺利地把item信息和URL给yield出去了。</p><p>我们来看这个yield的写法。我们yield出去的内容有两个，分别是URL信息和item。item用Key-Value的形式传输，我们把它赋值到了Key为“name”的键值对中。</p><p>然后我们在这里还用到了一个callback函数，它实际上是回调了一个名为parse_namedetail的函数。所以，我们需要在下面建立这个函数解析我们的详情页信息。我们在parse函数的下面新建一个函数parse_namedeatal并实现它，代码如下。</p><pre><code>def parse_namedetail(self, response):\n\tselector = Selector(response)\n\tdesc = selector.xpath(&quot;//div[@class='article']/p/text()&quot;).extract()\n\titem = response.meta['name']\n\tdesc = list(map(str.strip, desc))\n\titem['desc'] = ''.join(desc)\n\tprint(item)\n\tyield item\n</code></pre><p>在这个函数中，我们首先建立了一个Selector，它主要是Response用来提取数据的。当Spider的Request得到Response之后，Spider可以使用Selector提取Response中的有用的数据。因此，这里我们传入的是上面的Response信息，也就是详情页的Response。</p><p>然后，我们使用XPath语法解析response中的HTML代码。我们先来看下有哪些XPath表达式，以便后续我们更好地使用它。</p><p><img src=\"https://static001.geekbang.org/resource/image/e2/8b/e276155304958748328c3b3b8855708b.jpg?wh=2076x2000\" alt=\"\"></p><p>然后我们再来看一下详情页的正文部分在Chrome的开发者工具中的源代码。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/c4/f857999da6671773df7a051d8d1174c4.png?wh=2701x1236\" alt=\"\"></p><p>放大代码部分得到如下结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/6d/70/6dfc9455479797619e9dbf7ba2532470.png?wh=2154x1224\" alt=\"\"></p><p>可以看到，所有的正文部分都在class为article和id为article的<code>&lt;div&gt;</code>标签中，并且每个段落都用<code>&lt;p&gt;</code>标签包裹着。也就是说，我们只需要拿到class和id的标签，并且拿到所有的p标签，就可以拿出所有的内容。因此我们可以使用下面这行代码来获取所有的正文内容。</p><pre><code>desc = selector.xpath(&quot;//div[@class='article']/p/text()&quot;).extract()\n</code></pre><p>简单解释一下，我们使用  <code>//</code>  标志跨节点获取到了class为article的段落下面所有p标签中的内容，并把它们提取了出来。接着，我们使用下面的代码拿到了前面传入的item信息，并把desc加入到了item中。</p><pre><code>item = response.meta['name']\ndesc = list(map(str.strip, desc))\nitem['desc'] = ''.join(desc)\n</code></pre><p>最后，我们再把这个item给yield出去就可以了。现在我们再运行一下代码会得到如下输出。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/d3/4f8d9f3b6c23e9c610fe20b6d54cf7d3.png?wh=2730x396\" alt=\"\"></p><p>到这里，我们的数据爬取工作看起来就已经完成了。</p><p>但是等一等，你有没有发现一个问题，现在虽然已经能够爬取到数据，但是只能爬取一页的内容。<strong>这是远远不够的，接下来我们就用程序来实现翻页按钮的点击功能。</strong></p><p>实际上，我们只需要在parse函数中加入如下代码即可。</p><pre><code>try:\n\tdriver.find_element_by_xpath(&quot;//div[@class='feed-card-page']/span[@class='pagebox_next']/a&quot;).click()\nexcept:\n\tBreak\n</code></pre><p>这段代码也不难理解，就是找到翻页导航条的HTML标签，然后找到它的<code>&lt;a&gt;</code>链接，执行点击。</p><p>最后，我们要在最上面加上一个翻页操作，假设我们只需要翻5页，此时parse函数的代码如下。</p><pre><code>def parse(self, response):\n\tdriver = webdriver.Chrome(chrome_options=self.option)\n\tdriver.set_page_load_timeout(30)\n\tdriver.get(response.url)\n \n\tfor i in range(5):\n\t\twhile not driver.find_element_by_xpath(&quot;//div[@class='feed-card-page']&quot;).text:\n\t\t\tdriver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;)\n\t\ttitle = driver.find_elements_by_xpath(&quot;//h2[@class='undefined']/a[@target='_blank']&quot;)\n\t\ttime = driver.find_elements_by_xpath(\n\t\t\t&quot;//h2[@class='undefined']/../div[@class='feed-card-a feed-card-clearfix']/div[@class='feed-card-time']&quot;)\n\t\tfor i in range(len(title)):\n\t\t\tprint(title[i].text)\n\t\t\tprint(time[i].text)\n \n\t\t\ttoday = datetime.datetime.now()\n\t\t\teachtime = time[i].text\n\t\t\teachtime = eachtime.replace('今天', str(today.month) + '月' + str(today.day) + '日')\n \n\t\t\thref = title[i].get_attribute('href')\n \n\t\t\tif '分钟前' in eachtime:\n\t\t\t\tminute = int(eachtime.split('分钟前')[0])\n\t\t\t\tt = datetime.datetime.now() - datetime.timedelta(minutes=minute)\n\t\t\t\tt2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)\n\t\t\telse:\n\t\t\t\tif '年' not in eachtime:\n\t\t\t\t\teachtime = str(today.year) + '年' + eachtime\n\t\t\t\tt1 = re.split('[年月日:]', eachtime)\n\t\t\t\tt2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),\n\t\t\t\t\t\t\t\t\t   minute=int(t1[4]))\n \n\t\t\tprint(t2)\n \n\t\t\titem = SinaItem()\n\t\t\titem['type'] = 'news'\n\t\t\titem['title'] = title[i].text\n\t\t\titem['times'] = t2\n \n\t\t\tyield Request(url=response.urljoin(href), meta={'name': item}, callback=self.parse_namedetail)\n \n\t\ttry:\n\t\t\tdriver.find_element_by_xpath(&quot;//div[@class='feed-card-page']/span[@class='pagebox_next']/a&quot;).click()\n\t\texcept:\n\t\t\tBreak\n</code></pre><p>我们再运行程序，这时我们就可以爬取5页的内容了，在此之后，爬虫会自动停止。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/d8/c4a396f83ba2b9a060a267cf116656d8.png?wh=3637x623\" alt=\"\"></p><p>这样，我们已经能够针对一个栏目来爬取数据了。在后面的课程中，我们会延续这个思路，然后把数据存储起来并做相应的处理。</p><h2>总结</h2><p>这节课到这里也就接近尾声了，我来给你梳理一下这节课的主要内容。</p><ol>\n<li>在Chrome浏览器中，我们可以使用 Chrome 开发者工具来查看页面上的元素、标记和属性，以及查看网络请求、响应和其他诊断信息。</li>\n<li>Scrapy 提供了许多内置的解析器，包括 XPath 和 CSS 选择器等，这些解析器可以帮助我们轻松地从 HTML 页面中提取所需数据。</li>\n<li>在 Scrapy 中，callback 函数可以返回多个请求，并在结果返回时使用 yield 关键字传递给 Scrapy 引擎。</li>\n<li>item.py 文件是 Scrapy 中用于解析和存储数据的 Python 类。在 item.py 文件中，你可以定义数据模型，确定要提取的字段，并定义数据的类型和格式。</li>\n<li>在爬取包含多个页面的网站时，可以使用 next_page() 方法来模拟用户操作，调用click(）函数进行点击。</li>\n</ol><h2>课后题</h2><p>学完这节课，请你试着完成下面两项任务。</p><ol>\n<li>跟着我的讲解，自己实现一遍这节课所讲的内容。</li>\n<li>爬取新浪网站电影板块的内容。</li>\n</ol><p>欢迎你在留言区与我交流讨论，你也可以把代码链接附在评论区，我会选取有代表性的代码进行点评，我们下节课见！</p>","neighbors":{"left":{"article_title":"07｜数据获取：什么是Scrapy框架？","id":651624},"right":{"article_title":"09｜数据存储：如何将爬取到的数据存入数据库中？","id":653611}},"comments":[{"had_liked":false,"id":373939,"user_name":"Geek_ccc0fd","can_delete":false,"product_type":"c1","uid":1461544,"ip_address":"广东","ucode":"DB53D576AEC020","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","comment_is_top":false,"comment_ctime":1683365285,"is_pvip":false,"replies":[{"id":136924,"content":"感谢分享您的经验。确实，Selenium 的 API 有时候会进行更新，需要根据新版本来进行调整。在具体实现中，我们需要结合页面的 HTML 结构来进行 xpath 路径的选择，以确保能够定位到正确的元素。对于一些可能存在多个元素的情况，使用索引可以确保点击到正确的元素，避免影响程序的正常执行。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685035325,"ip_address":"广东","comment_id":373939,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"新安装selemium的API变了，而且xpath获取的路径有点问题，我这里获取不到一页的全部内容，我修改了一下：\ntitle = driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-item&#39;]&#47;h2&#47;a[@target=&#39;_blank&#39;]&quot;)\ntime = driver.find_elements(By.XPATH,&quot;&#47;&#47;div[@class=&#39;feed-card-item&#39;]&#47;h2&#47;..&#47;div[@class=&#39;feed-card-a feed-card-clearfix&#39;]&#47;div[@class=&#39;feed-card-time&#39;]&quot;)\n然后就是翻页点击那里我这边跑下来也有问题，根据xpath会获取两个a标签,所以需要增加索引：\ndriver.find_elements(By.XPATH,&quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)[0].click()","like_count":3,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619391,"discussion_content":"感谢分享您的经验。确实，Selenium 的 API 有时候会进行更新，需要根据新版本来进行调整。在具体实现中，我们需要结合页面的 HTML 结构来进行 xpath 路径的选择，以确保能够定位到正确的元素。对于一些可能存在多个元素的情况，使用索引可以确保点击到正确的元素，避免影响程序的正常执行。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685035325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373757,"user_name":"未来已来","can_delete":false,"product_type":"c1","uid":1187130,"ip_address":"广东","ucode":"3A21ACFD53CB9C","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","comment_is_top":false,"comment_ctime":1683126600,"is_pvip":false,"replies":[{"id":136923,"content":"感谢提醒，后续我会统一修改一下。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685035217,"ip_address":"广东","comment_id":373757,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"截止到 5月3日，新安装的 selemium 只有 find_elements 方法，老师的代码需改为：\n`title = driver.find_elements(By.XPATH, &quot;&#47;&#47;h2[@class=&#39;undefined&#39;]&#47;a[@target=&#39;_blank&#39;]&quot;)`\n`time = driver.find_elements(By.XPATH, &quot;&#47;&#47;h2[@class=&#39;undefined&#39;]&#47;..&#47;div[@class=&#39;feed-card-a feed-card-clearfix&#39;]&#47;div[@class=&#39;feed-card-time&#39;]&quot;)`\n以此类推","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619390,"discussion_content":"感谢提醒，后续我会统一修改一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685035217,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1258480,"avatar":"https://static001.geekbang.org/account/avatar/00/13/33/f0/50c773cd.jpg","nickname":"全国花式伸脖子蹬腿锦标赛冠军🏆","note":"","ucode":"77181EEE13E761","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":635115,"discussion_content":"2024年了，老师还没修改吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704424605,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":619390,"ip_address":"重庆","group_id":0},"score":635115,"extra":""}]}]},{"had_liked":false,"id":382852,"user_name":"Abigail","can_delete":false,"product_type":"c1","uid":2604437,"ip_address":"澳大利亚","ucode":"ABB59CE5209E67","user_header":"https://static001.geekbang.org/account/avatar/00/27/bd/95/882bd4e0.jpg","comment_is_top":false,"comment_ctime":1698122538,"is_pvip":false,"replies":[{"id":140000,"content":"实际上，这个版本相对来讲是通用的，可以用3.9版本来开发","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860010,"ip_address":"广东","comment_id":382852,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"应该设计一个简单点的例子, python 起码也要用 3.9 啊","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631634,"discussion_content":"实际上，这个版本相对来讲是通用的，可以用3.9版本来开发","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860010,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":375536,"user_name":"alexliu","can_delete":false,"product_type":"c1","uid":1112019,"ip_address":"上海","ucode":"DD65983BBC9CD4","user_header":"https://static001.geekbang.org/account/avatar/00/10/f7/d3/2bbc62b2.jpg","comment_is_top":false,"comment_ctime":1685604963,"is_pvip":false,"replies":[{"id":140034,"content":"这个很有可能是你的服务接口部署的问题。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860905,"ip_address":"广东","comment_id":375536,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"在运行下一页click()的时候，有可能出现ElementNotInteractableException错误，解决方案：\n1、在driver.get(response.url)和click()后添加延时time.sleep(2)\n2、保持chrome的窗口大小一致 self.option.add_argument(&quot;--window-size=1960,1080&quot;)\ntry... except...部分代码如下：\n            try:\n                _next = driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)\n                _next[0].click()\n                _time.sleep(2)\n            except StaleElementReferenceException as e:\n                print(&quot; page failed.&quot;, e)\n                _next = driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)\n                _next[0].click()\n                _time.sleep(2)\n            except ElementNotInteractableException as e:\n                print(&quot; not found page.&quot;, e)\n                break\n            except Exception as e:\n                print(&quot;unkwon error: &quot;, e)","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631669,"discussion_content":"这个很有可能是你的服务接口部署的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373934,"user_name":"Geek_ccc0fd","can_delete":false,"product_type":"c1","uid":1461544,"ip_address":"广东","ucode":"DB53D576AEC020","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/EaBxhibOicZe9L7z2icbU4W462l543drFWYqibqczTicj4Msyb2g9pDSGmFTiafW9jibwib7jG6hpAdPMcCowdCiaxHaOdA/132","comment_is_top":false,"comment_ctime":1683361742,"is_pvip":false,"replies":[{"id":136920,"content":"虽然在 Scrapy 中可以通过 `response.xpath` 直接获取网页元素，但是有时候网页内容是通过 JavaScript 动态加载的，此时 Scrapy 可能无法获取这些需要 JavaScript 执行后才能得到的内容。\n而使用 Selenium 就可以完全模拟浏览器行为，包括 JavaScript 的执行，可以获取到完整的网页内容。此外，某些网站会通过一些反爬虫技术来检测访问者是否是真正的浏览器，如果检测到是爬虫，则会拒绝访问。使用 Selenium 可以完美地解决这个问题。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1685035038,"ip_address":"广东","comment_id":373934,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"我们在parse里面可以直接使用response.xpath获取元素，和使用 driver.find_elements是同样的效果，为什么还要用selenium来做浏览器的操作呢？","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619387,"discussion_content":"虽然在 Scrapy 中可以通过 `response.xpath` 直接获取网页元素，但是有时候网页内容是通过 JavaScript 动态加载的，此时 Scrapy 可能无法获取这些需要 JavaScript 执行后才能得到的内容。\n而使用 Selenium 就可以完全模拟浏览器行为，包括 JavaScript 的执行，可以获取到完整的网页内容。此外，某些网站会通过一些反爬虫技术来检测访问者是否是真正的浏览器，如果检测到是爬虫，则会拒绝访问。使用 Selenium 可以完美地解决这个问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685035038,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373692,"user_name":"安菲尔德","can_delete":false,"product_type":"c1","uid":1660533,"ip_address":"天津","ucode":"A7B310463C15F1","user_header":"https://static001.geekbang.org/account/avatar/00/19/56/75/28a29e7c.jpg","comment_is_top":false,"comment_ctime":1683021683,"is_pvip":false,"replies":[{"id":136677,"content":"同学，你好。\nmain.py文件是需要自己创建的。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683878462,"ip_address":"广东","comment_id":373692,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"请问哪有main.py文件呢，没有看到","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617796,"discussion_content":"同学，你好。\nmain.py文件是需要自己创建的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683878462,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373448,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1682514360,"is_pvip":false,"replies":[{"id":136406,"content":"同学你好，节后我会把源码放在github上，然后给你们链接。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1682607054,"ip_address":"广东","comment_id":373448,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"Q3：源码放在什么地方啊？能否把源码集中放到一个公共地方? 比如github等。","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616157,"discussion_content":"同学你好，节后我会把源码放在github上，然后给你们链接。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682607054,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1784001,"avatar":"","nickname":"imageine","note":"","ucode":"05A7EA8F28818C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":643090,"discussion_content":"1年过去了，请问github链接有了么？\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1714036744,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":616157,"ip_address":"山东","group_id":0},"score":643090,"extra":""}]}]},{"had_liked":false,"id":373447,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1682514294,"is_pvip":false,"replies":[{"id":136418,"content":"关于第一个问题，看看能不能通过截图或者其他方式告诉我，关于微信群，我可以和官方商量一下，看看怎么搞。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1682612825,"ip_address":"广东","comment_id":373447,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"Q1：第七课，创建环境的最后几步，不停出错，最后一个错误是：执行“scrapy genspider sina_spider sina.com.cn”，报告：lib\\string.py&quot;, line 132, in substitute\n    return self.pattern.sub(convert, self.template)\nTypeError: cannot use a string pattern on a bytes-like object\n网上搜了，大意说是python2和python3不匹配导致的。 我是完全按照老师的步骤来安装的，安装的是pythno3，怎么会有python2呢？当然，这个文件还没有解决，进行不下去了，郁闷啊。 \nQ2：能否建一个微信群？遇到问题可以协商。 另外，老师能否更及时地回复留言？","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616196,"discussion_content":"关于第一个问题，看看能不能通过截图或者其他方式告诉我，关于微信群，我可以和官方商量一下，看看怎么搞。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682612825,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2189310,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","nickname":"悟尘","note":"","ucode":"4E7E854340D3A4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633541,"discussion_content":"微信群有了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702290039,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1385403,"avatar":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","nickname":"GAC·DU","note":"","ucode":"7847FBE1C13740","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616029,"discussion_content":"项目运行环境创建了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682559530,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373433,"user_name":"GAC·DU","can_delete":false,"product_type":"c1","uid":1385403,"ip_address":"北京","ucode":"7847FBE1C13740","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","comment_is_top":false,"comment_ctime":1682506430,"is_pvip":false,"replies":[{"id":136417,"content":"同学你好，我来回答你的两个问题：\nA1：因为在parse_namedetail中已经获取到了http响应的内容，所以可以直接用，而不是再次请求网络，请求网络会有更多的耗时；\nA2：response对象包含来自Web服务器的HTML响应，并可用于提取响应数据，而selector对象是使用response对象创建的，它提供了一种方便的方法来从响应中选择和提取数据。因此，使用selector而不是response可以更方便地从HTML响应中提取和处理数据。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1682612425,"ip_address":"广东","comment_id":373433,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师，关于代码有些疑惑，第一：为什么parse_namedetail方法不再使用driver发起http请求和获取html标签内容？\n第二：desc = response.xpath(&quot;&#47;&#47;div[@class=&#39;article&#39;]&#47;p&#47;text()&quot;).extract()\n        desc = selector.xpath(&quot;&#47;&#47;div[@class=&#39;article&#39;]&#47;p&#47;text()&quot;).extract() \n我测试了两个代码都可以使用，那为什么不直接使用response，反而要生成一个selector？","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616195,"discussion_content":"同学你好，我来回答你的两个问题：\nA1：因为在parse_namedetail中已经获取到了http响应的内容，所以可以直接用，而不是再次请求网络，请求网络会有更多的耗时；\nA2：response对象包含来自Web服务器的HTML响应，并可用于提取响应数据，而selector对象是使用response对象创建的，它提供了一种方便的方法来从响应中选择和提取数据。因此，使用selector而不是response可以更方便地从HTML响应中提取和处理数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682612425,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":376388,"user_name":"Weitzenböck","can_delete":false,"product_type":"c1","uid":2669122,"ip_address":"江苏","ucode":"78C92583084ABA","user_header":"https://static001.geekbang.org/account/avatar/00/28/ba/42/5ca553bd.jpg","comment_is_top":false,"comment_ctime":1686762157,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"如果出现这个错误：__init__() got an unexpected keyword argument &#39;chrome_options&#39; 代码里改为 driver = webdriver.Chrome(options=self.option)具体源码是：\nclass WebDriver(ChromiumDriver):\n    &quot;&quot;&quot;Controls the ChromeDriver and allows you to drive the browser.&quot;&quot;&quot;\n\n    def __init__(\n        self,\n        options: Options = None,\n        service: Service = None,\n        keep_alive: bool = True,\n    ) -&gt; None:\n        &quot;&quot;&quot;Creates a new instance of the chrome driver. Starts the service and\n        then creates new instance of chrome driver.\n\n        :Args:\n         - options - this takes an instance of ChromeOptions\n         - service - Service object for handling the browser driver if you need to pass extra details\n         - keep_alive - Whether to configure ChromeRemoteConnection to use HTTP keep-alive.\n        &quot;&quot;&quot;\n        self.service = service if service else Service()\n        self.options = options if options else Options()\n        self.keep_alive = keep_alive\n\n        self.service.path = DriverFinder.get_path(self.service, self.options)\n\n        super().__init__(\n            DesiredCapabilities.CHROME[&quot;browserName&quot;],\n            &quot;goog&quot;,\n            self.options,\n            self.service,\n            self.keep_alive,\n        )","like_count":1,"discussions":[{"author":{"id":1542987,"avatar":"https://static001.geekbang.org/account/avatar/00/17/8b/4b/15ab499a.jpg","nickname":"风轻扬","note":"","ucode":"DB972F2DF059C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650031,"discussion_content":"老哥，我遇到你这个问题了。你的完整代码能给贴下吗，改了之后报错：TypeError: ArgOptions.add_argument() missing 1 required positional argument: &#39;argument&#39;","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1724282231,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393608,"user_name":"风轻扬","can_delete":false,"product_type":"c1","uid":1542987,"ip_address":"北京","ucode":"DB972F2DF059C4","user_header":"https://static001.geekbang.org/account/avatar/00/17/8b/4b/15ab499a.jpg","comment_is_top":false,"comment_ctime":1724313128,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100542801,"comment_content":"mac系统，爬取过程中，可能会报错：无法打开chromedriver，因为无法验证开发者。\n如果是brew安装的chromedriver，可以执行：xattr -d com.apple.quarantine &#47;opt&#47;homebrew&#47;bin&#47;chromedriver进行可信授权。\n如果不是brew安装的，需要自己找到chromedriver的安装路径，然后执行xattr -d com.apple.quarantine 你的chromedriver的路径","like_count":0},{"had_liked":false,"id":385267,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1702368613,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100542801,"comment_content":" # 尝试点击下一页\n            try:\n                # next_page_link = WebDriverWait(driver, 30).until(\n                #     EC.element_to_be_clickable(\n                #         (By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;))\n                # )\n                # next_page_link.click()\n                driver.find_elements(By.XPATH, &quot;&#47;&#47;div[@class=&#39;feed-card-page&#39;]&#47;span[@class=&#39;pagebox_next&#39;]&#47;a&quot;)[0].click()\n            except Exception as e:\n                print(f&quot;Error clicking next page link: {e}&quot;)\n                break\n\n打出的异常是：Error clicking next page link: Message: stale element reference: stale element not found\n\n这是因为什么？","like_count":0,"discussions":[{"author":{"id":2189310,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","nickname":"悟尘","note":"","ucode":"4E7E854340D3A4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633608,"discussion_content":"我加了一个sleep，具体代码如下：不知道原因是否像我写的这样？\n  # 执行滚动脚本\n            driver.execute_script(&#34;window.scrollTo(0, document.body.scrollHeight);&#34;)\n            # 暂停程序执行 1 秒钟，需要让页面加载完成，原因可能是：在页面加载期间，某些元素可能会变得不可交互。确保等待足够的时间以确保页面加载完成。\n            time.sleep(1)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702373684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":385257,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1702361899,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100542801,"comment_content":"为什么我的翻页不起作用呢？\n","like_count":0}]}