{"id":331599,"title":"13 | 决策树与随机森林：如何预测用户会不会违约？","content":"<p>你好，我是海丰。</p><p>今天，我们要讲决策树与随机森林。决策树是一种基础的分类和回归算法，随机森林是由多棵决策树集成在一起的集成学习算法，它们都非常常用。</p><p>这节课，我就通过决策树预测用户会不会违约的例子，来给你讲讲决策树和随机森林的原理和应用。</p><h2>如何理解决策树？</h2><p>很多人都有过租房子的经历，那你是怎么决定要不要租一个房子的呢？你可以先想一想，我先把我的做法说一下，我会先判断房子的位置，再看价格，最后看装修。</p><p>更具体点来说，我只会选择离公司近的房子，比如说 5 公里以内的或者通勤时间在 40 分钟以内的。其次，如果价格便宜，不管装修得好不好我都租，如果价格贵那我就要看装修情况，装修好就租，装修不好就不租。</p><p>这就是一棵典型的决策树：对于租房子这个问题，我根据距离、价格、装修这几个条件 ，对一个房子进行了判断，最后得到一个解决结果，就是这个房子我是租或者不租。下图就是这棵决策树的示意图。</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/1c/e358d29d64fa282b6841733b01e5421c.jpeg?wh=1920*1080\" alt=\"\"></p><p>我们可以看到，决策树（Decision Tree）就是一种树形结构的算法，上面的节点代表算法的某一个<strong>特征（如距离、价格），节点上可能存在多个分支，每一个分支代表的是这个特征的不同种类（如距离远、距离近），最后的叶子节点代表最终的决策结果（如租、不租）</strong>。</p><!-- [[[read_end]]] --><h3>决策树的生成</h3><p>知道了决策树的形式和原理，我们再来看看决策树的生成过程，它是决策树的核心。不过，对于产品经理来说，更重要的还是掌握决策树的原理、形式、优缺点。那我把它的详细过程写在下面，就是让你在工作中遇到类似问题的时候，能直接回来补充必要的知识，所以今天我们先对整体过程有个大致了解就可以了。</p><p><strong>决策树生成的过程包括三个部分，分别是特征选择、决策树生成、决策树剪枝。</strong>下面，我们还是拿上面租房子的例子，来说一说这个决策树生成的过程。假设现在有如下条件的一个房子，根据我上面定下的规则，你觉得这个房子我会不会租呢？</p><p><img src=\"https://static001.geekbang.org/resource/image/68/ce/684deee55d00b1942423418db7c361ce.jpeg?wh=1920*458\" alt=\"\"></p><p>我们先看距离，因为这个房子距离公司远，所以根据上面的决策树，我们就能直接得出结论：不租。但是，假设我们的决策树不是用距离作为根节点，而是用价格作为根节点的话，结果会不会不一样呢？</p><p>这个时候，决策棵树可能会变成下面的样子：</p><p><img src=\"https://static001.geekbang.org/resource/image/61/ab/619af82a58ce588849ae5f5068043eab.jpeg?wh=1920*995\" alt=\"\"></p><p>你会发现，我们的决策树一下子变“大”了，判断这个房子的过程就变成了，先看价格，再看装修，最后看距离。我们发现，即使决策树的结构发生了变化，可我们还是会得到之前的结论：不租，所以，决策树的构造只会影响到算法的复杂度和计算的时间，而不会影响决策的结果。</p><p>因此，在实际工作中，我们就需要优化决策树的结构，让它的效率更高，但这具体该怎么做呢？</p><h3>信息熵</h3><p>我们一般会在特征选择和决策树的生成阶段，通过<strong>信息熵</strong>来决定哪些特征重要以及它们应该放到哪个节点上，因为信息熵是用来衡量<strong>一个节点内信息的不确定性的</strong>。一个系统中信息熵越大，这个系统的不确定性就越大，样本就越多样，你也可以理解成是样本的<strong>纯度越低</strong>，信息熵越小，系统的不确定性就越小，样本越趋于一致，那样本的<strong>纯度就越高</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/7d/e1c16a9cd77a60dfc08366yyb82eef7d.jpeg?wh=1920*845\" alt=\"\"></p><p>我们肯定是希望决策树在每次划分的时候，每个条件分支都能够最大化地去划分这些样本，让每个节点的信息熵更低，样本一致性更高。所以，决策树会计算每一个特征划分后样本的“<strong>纯度</strong>”，纯度越高的特征越接近根节点。这样一来，决策树的复杂度和计算时间肯定就会越少，也就不会出现我们刚才说的那种“很大”的决策树。这就是实际工作中我们构造决策树的思路了。</p><p>实际上，决策树的算法有很多，最典型的三种分别是 ID3（Iterative Dichotomiser 3，迭代二叉树3代）、C4.5 和 CART（Classification and Regression Trees，分类与回归树）。ID3 是最初代的决策树算法，它使用的计算指标是信息增益；C4.5 是在 ID3 基础上改进后的算法，它使用的计算指标是信息增益率；CART 分类与回归树，做分类问题时使用的是 <a href=\"https://zh.wikipedia.org/wiki/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0\">Gini 系数（Gini Coefficient，基尼系数）</a>，做回归问题的时候使用的是偏差值。</p><p>作为产品经理，我们简单了解这三种算法的特点就可以了，我在下面对它们进行了总结，你可以参考一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/84/c7/84a031cyyd5e41232b2789b4ab41f6c7.jpeg?wh=1920*793\" alt=\"\"></p><h3>剪枝操作</h3><p>最后，因为决策树很容易出现过拟合情况，所以我们还会引入剪枝操作这个环节。剪枝就是我们对一棵树进行简化，减少它的复杂程度，提高模型的泛化能力。剪枝的原理很好理解，主要就是判断把某个节点去掉之后，模型的准确度会不会降低了，如果没有降低，就可以减掉这个节点。</p><p>剪枝的操作还分为预剪枝和后剪枝，它们的区别是剪枝发生的阶段不同。预剪枝在决策树生成时候同步进行。而后剪枝是决策树生成之后，再对决策树的叶子节点开始一步一步地向根方向剪枝。</p><h2>决策树的应用案例：预测用户违约</h2><p>决策树的生成讲完了，我们重点来看看决策树的应用。在金融风控场景下，我们经常需要判断用户的违约风险。</p><p>最早的风控模型都是使用逻辑回归来做的，因为它相对简单而且可解释性强。但逻辑回归属于线性模型，不能很好处理非线性特征，所以决策树算法也慢慢用于违约风险的预测。接下来，我们就来看看决策树是怎么预测违约风险的。</p><p>决策树预测用户违约的核心思想是：<strong>先获取部分用户的历史数据，历史数据中包括过去的信贷数据和还款结果；然后将贷款客户不断进行分类，直到某个节点的样本用户都是同一个类型为止；最后，再对决策树进行剪枝，简化树的复杂度。</strong></p><p>假设我们得到的用户历史数据如下所示。对于这个表格，我再补充解释一下，过去的信贷数据应该包括申请数据、金融产品相关数据等等。年龄，是否有房这些都属于申请数据，是包括在信贷数据里的。</p><p>还款结果指的是什么时候还款，还了多少，但是做模型设计，定义模型目标变量的时候，我们不可能直接用还款数据，所以我们定义是否逾期作为目标值，也就是 Y 值。1 代表逾期，0 代表不逾期。</p><p><img src=\"https://static001.geekbang.org/resource/image/a3/49/a3d903270195ca1984452862a7e20849.jpeg?wh=1920*865\" alt=\"\"></p><p>因为目前决策树算法中，使用比较多的是 CART 算法，所以我们也选择它进行模型构建，而特征选择阶段会使用 Gini 系数。 CART 算法选择 Gini 系数，是因为信息熵模型使用了大量的对数计算导致效率很低，而 Gini 系统可以避免这个问题，从而提升计算效率。</p><p>从上面的历史数据中，我们可以提取出三个特征，分别是性别、年龄和是否有房。接下来，我们就分别计算一下这三个特征的 Gini 系数。</p><p>首先，性别特征的 Gini 系数直接根据公式计算就可以了，我们假设它就是 0.412。</p><p><img src=\"https://static001.geekbang.org/resource/image/3e/cd/3e00a6868442cb4aa76ca685eb0d52cd.jpeg?wh=1750*453\" alt=\"\"></p><p>我们重点来看第二个特征：年龄。年龄是一个连续的值，我们的历史数据中一共有 4 种数值，每两个相邻值之间都可以是一个特征分类的点（比如说，年龄 22 和年龄 24 的分类点就是23），所以对于年龄这个特征，我们一共有3种不同的分类方式。因为分类方式比较多，相对应的，Gini 的计算方式就会比较复杂，我们需要分别计算3种不同分类方式时的 Gini 系数，选出 Gini 最小的分类方式，把它作为年龄的分类。</p><p>假设年龄在 37 的时候 Gini 系数最小，等于 0.511，那么年龄这个特征的条件分支就是小于 37 和大于 37。</p><p>相同的，我们可以再计算是否有房的 Gini 系数。假设这个特征的 Gini 系数为 0.113，最后，根据 Gini 排序我们就能得到如下的决策树结构。</p><p><img src=\"https://static001.geekbang.org/resource/image/67/74/67169bb3yy9e13dc29f86490b67af374.jpeg?wh=1920*864\" alt=\"\"></p><p>但是，这个决策树还不是最终的结构，因为有些节点我们是可以去掉的。比如说，我们发现有房产这个条件下面的所有节点，去掉和不去掉的时候模型准确性没有变化，那我们就可以把有房产下面的所有节点裁剪掉，从而得到新的决策树。</p><p>这就是剪枝操作，在我们实际工作中通常采取后剪枝的操作，从叶子节点逐步向上判断哪些节点是可以去掉的，剪枝后的决策树如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/03/67/03b29d852885211dd2d5bc567934c867.jpeg?wh=1920*889\" alt=\"\"></p><p>以上就是决策树创建的过程，因为没有进行实际计算，实际结果可能有偏差，你只要理解这个过程就可以了。</p><h2>决策树的优缺点</h2><p>通过上面的学习我们可以发现，决策树的优点和缺点都很明显。由于具有树形结构所以决策树的可解释性强，直观好理解，而且我们还可以从结果向上去追溯原因。采用决策树，我们可以很方便地和领导、业务方、甲方去解释我们的模型是什么，以及有哪些因素影响了模型的结果。</p><p>不过，决策树的缺点也非常明显。当数据量大，数据维度（样本具有的特征或者属性，如价格、位置）很多的时候，决策树会变得非常复杂，训练时间会很久。</p><p>另外，决策树还有一个很明显的缺点就是，需要算法同学手工设置决策树的深度（决策树要分多少层），如果设置了不合适的参数，就很可能造成欠拟合或者过拟合的情况。比如说，深度太浅就意味着你的叶子节点分得不干净，很容易造成欠拟合的情况，深度太深也会导致决策树训练时间太久，复杂度太高，很容易造成过拟合的情况。</p><h2>随机森林：集体的力量</h2><p>在实际工作中，我们既可以只使用一棵决策树来解决问题，也可以使用多棵决策树来共同解决问题，也就是随机森林。</p><p>随机森林（Random Forest）指的是由多棵决策树组成，随机指的是每一个决策树的样本是随机从数据集中采样得到的。假设， 模型由三个决策树A、B、C组成，我们给每棵决策树都随机抽取样本进行训练，由于这三棵树的训练样本不一样，因此它们最后得到的决策结果有可能不同。最后，我们再把这三棵树得到的结果做一个综合，就能得到最终的决策结果了。</p><p>随机森林的原理很好理解，那我们再来说说它的优缺点。因为这个算法是随机从数据集中进行采样的，所以模型的随机性很强，不容易产生过拟合的情况，但正因为样本是随机的，所以模型对于样本数据的异常值也不太敏感。</p><p>其次，因为算法采样的时候，是从整个数据集中抽取其中一部分进行采样，而且随机森林是由多棵树组合而成的，所以模型中的每一棵决策树都可以并行训练和计算，这样一来，在面向大数据量的情况下，随机森林的运行效率更高。</p><p>也正是因为这样，随机森林在训练时候需要的计算成本会更高，而且，就算它们整合之后会比之前单一模型表现好，但在面对复杂样本的时候，它们仍然没有办法很好区分，所以模型上限很低。</p><p>随机森林属于集成学习中的一种。集成学习（Ensemble Learning）可以理解为，不是通过某一个单独的机器学习算法解决问题，而是通过多个机器学习算法结合使用来完成最终的算法，最终达到 1+1&gt;2 的效果。核心原理你可以记成是我们常说的“三个臭皮匠赛过一个诸葛亮”。</p><p>集成学习的内部由很多<a href=\"https://www.cnblogs.com/yumoye/p/11025504.html\">弱监督模型</a>组成， 某一个弱监督模型只在某一个方向上表现比较好，当我们把这些算法合而为一的时候，就会得到一个各方面都会表现较好的模型。集成学习的算法有很多，随机森林是其中比较有代表性的一种。我在下面整理了一个集成学习的思维导图，你可以了解一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/bd/50d6fe5acc9a759798cba086926981bd.jpg?wh=7500*6041\" alt=\"\"></p><h2>总结</h2><p>今天，我们讲了决策树、随机森林的原理、应用和优缺点。理解决策树是理解随机森林和集成学习的基础，不过，作为产品经理，我们的重点不在于理解决策树的生成过程，只是借着它的生成加深对决策树原理和应用的理解。</p><p>总的来说，关于决策树和随机森林，我希望你重点记住这 5 点：</p><ol>\n<li>\n<p>决策树就是一种树形结构的算法，它很直观，可视化很强，但也容易过拟合；</p>\n</li>\n<li>\n<p>决策树特征选择是生成决策树的基础，不同的算法对应了不同的特征选择方式；</p>\n</li>\n<li>\n<p>集成学习是多个机器学习算法的结合；</p>\n</li>\n<li>\n<p>随机森林是集成学习中的一种，由多棵决策树组成；</p>\n</li>\n<li>\n<p>随机森林的原理你可以记成：三个臭皮匠赛过一个诸葛亮，它的特点你可以记成：模型起点高、天花板低。</p>\n</li>\n</ol><p>除此之外，关于决策树和随机森林的应用场景我还想再强调一下。决策树和随机森林模型的可解释度都很高，这就意味着我们可以轻松地把模型的计算逻辑介绍清楚。</p><p>实际上，这一点对于咨询、金融、医疗领域的公司来说非常重要，因为你的客户往往不懂你的模型内部在做什么，但如果你的模型结构清晰，你就能在最短的时间内介绍出模型的优势。而且，因为随机森林这样的集成学习算法融合了多个模型的优点，所以对于解决分类问题来说，决策树和随机森林是当今的机器学习算法的首选，就比如你可能听过的 GBDT、XGBoost 就是决策树的升级版。</p><h2>课后讨论</h2><p>因为产品经理不需要实际进行模型的构建，所以我不会让你去构建一棵决策树，我想请你来梳理一下，你所在的团队中有哪些项目是基于决策树、随机森林或者是升级算法解决的呢？</p><p>欢迎在留言区分享你的经验，我们下节课见！</p>","neighbors":{"left":{"article_title":"12 | 朴素贝叶斯：让AI告诉你，航班延误险该不该买？","id":330159},"right":{"article_title":"14 | 支持向量机：怎么预测股票市场的涨与跌？","id":332227}},"comments":[{"had_liked":false,"id":272881,"user_name":"Jove","can_delete":false,"product_type":"c1","uid":1718016,"ip_address":"","ucode":"F3A8EC941473B8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoaknSzhibWMurEqQ5gmv4UBBRgVQFCnPNscicuwXDaCgRic2cWEfQN1bujne2gqWw4rT3ZKdicpU476Q/132","comment_is_top":false,"comment_ctime":1610345765,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23085182245","product_id":100066601,"comment_content":"Bagging处理分类问题一般用投票法，处理回归问题一般用平均法<br>Boosting对错误分类样本加权训练，对弱分类器中误差小的增大权重","like_count":5},{"had_liked":false,"id":273458,"user_name":"橙gě狸","can_delete":false,"product_type":"c1","uid":2383132,"ip_address":"","ucode":"18E16657D15D0E","user_header":"https://static001.geekbang.org/account/avatar/00/24/5d/1c/f2d45010.jpg","comment_is_top":false,"comment_ctime":1610603540,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14495505428","product_id":100066601,"comment_content":"商品或商户排序，利用gbdt输出item的点击概率，并根据概率排序","like_count":3},{"had_liked":false,"id":321585,"user_name":"甄凡","can_delete":false,"product_type":"c1","uid":1166010,"ip_address":"","ucode":"5A2B5AE4BAA3DE","user_header":"https://static001.geekbang.org/account/avatar/00/11/ca/ba/2bf1690f.jpg","comment_is_top":false,"comment_ctime":1636959370,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10226893962","product_id":100066601,"comment_content":"为什么把有房下面的分支都减了呢，，有房产就不违约？没看懂","like_count":3,"discussions":[{"author":{"id":2341459,"avatar":"https://static001.geekbang.org/account/avatar/00/23/ba/53/c2b79fc5.jpg","nickname":"liqin","note":"","ucode":"879A3D13DA372E","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":590571,"discussion_content":"我也很纳闷，我想应该是作者那个图没画对，复制黏贴的时候出错了，应该是有房产最后的叶节点全部都是不违约。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1665890640,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283799,"user_name":"小太白","can_delete":false,"product_type":"c1","uid":1041975,"ip_address":"","ucode":"96C1A35BCDAA0F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e6/37/74ec8fbb.jpg","comment_is_top":false,"comment_ctime":1615942051,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10205876643","product_id":100066601,"comment_content":"案例：依据特征标签的用户分类，用决策树冷启动，后续用随机森林提高性能和效果。","like_count":3},{"had_liked":false,"id":290208,"user_name":"Coscamy DD","can_delete":false,"product_type":"c1","uid":2574518,"ip_address":"","ucode":"4C0CC2D9661D03","user_header":"https://static001.geekbang.org/account/avatar/00/27/48/b6/820ecda0.jpg","comment_is_top":false,"comment_ctime":1619429873,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5914397169","product_id":100066601,"comment_content":"如通过智能外呼判断用户意向：通过多个问题树形判断关系，判断用户的意向。","like_count":2},{"had_liked":false,"id":283115,"user_name":"Rosa rugosa","can_delete":false,"product_type":"c1","uid":1480782,"ip_address":"","ucode":"A26068A69D7E9B","user_header":"https://static001.geekbang.org/account/avatar/00/16/98/4e/f42d27e8.jpg","comment_is_top":false,"comment_ctime":1615553945,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5910521241","product_id":100066601,"comment_content":"如信贷风控，疾病识别，购房意向项目可能会用到决策树，随机森林或升级算法","like_count":1},{"had_liked":false,"id":329742,"user_name":"张章鱼","can_delete":false,"product_type":"c1","uid":2883328,"ip_address":"","ucode":"E02406BF0F2F8B","user_header":"https://static001.geekbang.org/account/avatar/00/2b/ff/00/053d4ca2.jpg","comment_is_top":false,"comment_ctime":1641522313,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1641522313","product_id":100066601,"comment_content":"怎么最终得出决策结果是否违约的呢？","like_count":0},{"had_liked":false,"id":315199,"user_name":"赖赖","can_delete":false,"product_type":"c1","uid":2789082,"ip_address":"","ucode":"62B95668DFE656","user_header":"https://static001.geekbang.org/account/avatar/00/2a/8e/da/d3225cab.jpg","comment_is_top":false,"comment_ctime":1633747978,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1633747978","product_id":100066601,"comment_content":"基恩系数的讲解链接无法打开","like_count":1},{"had_liked":false,"id":299634,"user_name":"Fay Chen","can_delete":false,"product_type":"c1","uid":2674884,"ip_address":"","ucode":"2E04D8803E4AE6","user_header":"https://static001.geekbang.org/account/avatar/00/28/d0/c4/9dd9ceee.jpg","comment_is_top":false,"comment_ctime":1624770819,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1624770819","product_id":100066601,"comment_content":"为什么房产案例中，决策树“无房”下“性别”要分“好”“不好”？","like_count":1,"discussions":[{"author":{"id":1283016,"avatar":"https://static001.geekbang.org/account/avatar/00/13/93/c8/69512653.jpg","nickname":"种菜的渔民","note":"","ucode":"E864ECDD60DBED","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":386159,"discussion_content":"应该是男 女吧  ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627453784,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283116,"user_name":"Rosa rugosa","can_delete":false,"product_type":"c1","uid":1480782,"ip_address":"","ucode":"A26068A69D7E9B","user_header":"https://static001.geekbang.org/account/avatar/00/16/98/4e/f42d27e8.jpg","comment_is_top":false,"comment_ctime":1615554497,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1615554497","product_id":100066601,"comment_content":"老师好，案例中有些没看明白的地方如下<br>1，案例中性别特征的 Gini 系数怎么计算的？<br>2，案例中年龄的分类方式有三种是哪三种？","like_count":0,"discussions":[{"author":{"id":2653880,"avatar":"https://static001.geekbang.org/account/avatar/00/28/7e/b8/69d840ad.jpg","nickname":"小赖是个憨憨🐛","note":"","ucode":"3852BBFFF91FF7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":379073,"discussion_content":"年龄一共有22、24、36、38这四种情况，然后分割点就是23、（24~36任意一个数）、37这三种","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1623662108,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}