{"id":257053,"title":"29 | GroupMetadataManager：组元数据管理器是个什么东西？","content":"<p>你好，我是胡夕。今天，我们学习GroupMetadataManager类的源码。从名字上来看，它是组元数据管理器，但是，从它提供的功能来看，我更愿意将它称作消费者组管理器，因为它定义的方法，提供的都是添加消费者组、移除组、查询组这样组级别的基础功能。</p><p>不过，这个类的知名度不像KafkaController、GroupCoordinator那么高，你之前可能都没有听说过它。但是，它其实是非常重要的消费者组管理类。</p><p>GroupMetadataManager类是在消费者组Coordinator组件被创建时被实例化的。这就是说，每个Broker在启动过程中，都会创建并维持一个GroupMetadataManager实例，以实现对该Broker负责的消费者组进行管理。更重要的是，生产环境输出日志中的与消费者组相关的大多数信息，都和它息息相关。</p><p>我举一个简单的例子。你应该见过这样的日志输出：</p><pre><code>Removed ××× expired offsets in ××× milliseconds.\n</code></pre><p>这条日志每10分钟打印一次。你有没有想过，它为什么要这么操作呢？其实，这是由GroupMetadataManager类创建的定时任务引发的。如果你不清楚GroupMetadataManager的原理，虽然暂时不会影响你使用，但是，一旦你在实际环境中看到了有关消费者组的错误日志，仅凭日志输出，你是无法定位错误原因的。要解决这个问题，就只有一个办法：<strong>通过阅读源码，彻底搞懂底层实现原理，做到以不变应万变</strong>。</p><!-- [[[read_end]]] --><p>关于这个类，最重要的就是要掌握它是如何管理消费者组的，以及它对内部位移主题的操作方法。这两个都是重磅功能，我们必须要吃透它们的原理，这也是我们这三节课的学习重点。今天，我们先学习它的类定义和管理消费者组的方法。</p><h1>类定义与字段</h1><p>GroupMetadataManager类定义在coordinator.group包下的同名scala文件中。这个类的代码将近1000行，逐行分析的话，显然效率不高，也没有必要。所以，我从类定义和字段、重要方法两个维度给出主要逻辑的代码分析。下面的代码是该类的定义，以及我选取的重要字段信息。</p><pre><code>// brokerId：所在Broker的Id\n// interBrokerProtocolVersion：Broker端参数inter.broker.protocol.version值\n// config: 内部位移主题配置类\n// replicaManager: 副本管理器类\n// zkClient: ZooKeeper客户端\nclass GroupMetadataManager(\n  brokerId: Int, \n  interBrokerProtocolVersion: ApiVersion,\n  config: OffsetConfig, \n  replicaManager: ReplicaManager, \n  zkClient: KafkaZkClient, \n  time: Time,\n  metrics: Metrics) extends Logging with KafkaMetricsGroup {\n  // 压缩器类型。向位移主题写入消息时执行压缩操作\n  private val compressionType: CompressionType = CompressionType.forId(config.offsetsTopicCompressionCodec.codec)\n  // 消费者组元数据容器，保存Broker管理的所有消费者组的数据\n  private val groupMetadataCache = new Pool[String, GroupMetadata]\n  // 位移主题下正在执行加载操作的分区\n  private val loadingPartitions: mutable.Set[Int] = mutable.Set()\n  // 位移主题下完成加载操作的分区\n  private val ownedPartitions: mutable.Set[Int] = mutable.Set()\n  // 位移主题总分区数\n  private val groupMetadataTopicPartitionCount = getGroupMetadataTopicPartitionCount\n  ......\n}\n</code></pre><p>这个类的构造函数需要7个参数，后面的time和metrics只是起辅助作用，因此，我重点解释一下前5个参数的含义。</p><ul>\n<li>brokerId：这个参数我们已经无比熟悉了。它是所在Broker的ID值，也就是broker.id参数值。</li>\n<li>interBrokerProtocolVersion：保存Broker间通讯使用的请求版本。它是Broker端参数inter.broker.protocol.version值。这个参数的主要用途是<strong>确定位移主题消息格式的版本</strong>。</li>\n<li>config：这是一个OffsetConfig类型。该类型定义了与位移管理相关的重要参数，比如位移主题日志段大小设置、位移主题备份因子、位移主题分区数配置等。</li>\n<li>replicaManager：副本管理器类。GroupMetadataManager类使用该字段实现获取分区对象、日志对象以及写入分区消息的目的。</li>\n<li>zkClient：ZooKeeper客户端。该类中的此字段只有一个目的：从ZooKeeper中获取位移主题的分区数。</li>\n</ul><p>除了构造函数所需的字段，该类还定义了其他关键字段，我给你介绍几个非常重要的。</p><p><strong>1.compressionType</strong></p><p><strong>压缩器类型</strong>。Kafka向位移主题写入消息前，可以选择对消息执行压缩操作。是否压缩，取决于Broker端参数offsets.topic.compression.codec值，默认是不进行压缩。如果你的位移主题占用的磁盘空间比较多的话，可以考虑启用压缩，以节省资源。</p><p><strong>2.groupMetadataCache</strong></p><p><strong>该字段是GroupMetadataManager类上最重要的属性，它</strong><strong>保存这个Broker上GroupCoordinator组件管理的所有消费者组元数据。</strong>它的Key是消费者组名称，Value是消费者组元数据，也就是GroupMetadata。源码通过该字段实现对消费者组的添加、删除和遍历操作。</p><p><strong>3.loadingPartitions</strong></p><p><strong>位移主题下正在执行加载操作的分区号集合</strong>。这里需要注意两点：首先，这些分区都是位移主题分区，也就是__consumer_offsets主题下的分区；其次，所谓的加载，是指读取位移主题消息数据，填充GroupMetadataCache字段的操作。</p><p><strong>4.ownedPartitions</strong></p><p><strong>位移主题下完成加载操作的分区号集合</strong>。与loadingPartitions类似的是，该字段保存的分区也是位移主题下的分区。和loadingPartitions不同的是，它保存的分区都是<strong>已经完成加载操作</strong>的分区。</p><p><strong>5.groupMetadataTopicPartitionCount</strong></p><p><strong>位移主题的分区数</strong>。它是Broker端参数offsets.topic.num.partitions的值，默认是50个分区。若要修改分区数，除了变更该参数值之外，你也可以手动创建位移主题，并指定不同的分区数。</p><p>在这些字段中，groupMetadataCache是最重要的，GroupMetadataManager类大量使用该字段实现对消费者组的管理。接下来，我们就重点学习一下该类是如何管理消费者组的。</p><h1>重要方法</h1><p>管理消费者组包含两个方面，对消费者组元数据的管理以及对消费者组位移的管理。组元数据和组位移都是Coordinator端重要的消费者组管理对象。</p><h2>消费者组元数据管理</h2><p>消费者组元数据管理分为查询获取组信息、添加组、移除组和加载组信息。从代码复杂度来讲，查询获取、移除和添加的逻辑相对简单，加载的过程稍微费事些。我们先说说查询获取。</p><h3>查询获取消费者组元数据</h3><p>GroupMetadataManager类中查询及获取组数据的方法有很多。大多逻辑简单，你一看就能明白，比如下面的getGroup方法和getOrMaybeCreateGroup方法：</p><pre><code>// getGroup方法：返回给定消费者组的元数据信息。\n// 若该组信息不存在，返回None\ndef getGroup(groupId: String): Option[GroupMetadata] = {\n  Option(groupMetadataCache.get(groupId))\n}\n// getOrMaybeCreateGroup方法：返回给定消费者组的元数据信息。\n// 若不存在，则视createIfNotExist参数值决定是否需要添加该消费者组\ndef getOrMaybeCreateGroup(groupId: String, createIfNotExist: Boolean): Option[GroupMetadata] = {\n  if (createIfNotExist)\n    // 若不存在且允许添加，则添加一个状态是Empty的消费者组元数据对象\n    Option(groupMetadataCache.getAndMaybePut(groupId, new GroupMetadata(groupId, Empty, time)))\n  else\n    Option(groupMetadataCache.get(groupId))\n}\n</code></pre><p>GroupMetadataManager类的上层组件GroupCoordinator会大量使用这两个方法来获取给定消费者组的数据。这两个方法都会返回给定消费者组的元数据信息，但是它们之间是有区别的。</p><p>对于getGroup方法而言，如果该组信息不存在，就返回None，而这通常表明，消费者组确实不存在，或者是，该组对应的Coordinator组件变更到其他Broker上了。</p><p>而对于getOrMaybeCreateGroup方法而言，若组信息不存在，就根据createIfNotExist参数值决定是否需要添加该消费者组。而且，getOrMaybeCreateGroup方法是在消费者组第一个成员加入组时被调用的，用于把组创建出来。</p><p>在GroupMetadataManager类中，还有一些地方也散落着组查询获取的逻辑。不过它们与这两个方法中的代码大同小异，很容易理解，课下你可以自己阅读下。</p><h3>移除消费者组元数据</h3><p>接下来，我们看下如何移除消费者组信息。当Broker卸任某些消费者组的Coordinator角色时，它需要将这些消费者组从groupMetadataCache中全部移除掉，这就是removeGroupsForPartition方法要做的事情。我们看下它的源码：</p><pre><code>def removeGroupsForPartition(offsetsPartition: Int,\n                             onGroupUnloaded: GroupMetadata =&gt; Unit): Unit = {\n  // 位移主题分区\n  val topicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, offsetsPartition)\n  info(s&quot;Scheduling unloading of offsets and group metadata from $topicPartition&quot;)\n  // 创建异步任务，移除组信息和位移信息\n  scheduler.schedule(topicPartition.toString, () =&gt; removeGroupsAndOffsets)\n  // 内部方法，用于移除组信息和位移信息\n  def removeGroupsAndOffsets(): Unit = {\n    var numOffsetsRemoved = 0\n    var numGroupsRemoved = 0\n    inLock(partitionLock) {\n      // 移除ownedPartitions中特定位移主题分区记录\n      ownedPartitions.remove(offsetsPartition)\n      // 遍历所有消费者组信息\n      for (group &lt;- groupMetadataCache.values) {\n        // 如果该组信息保存在特定位移主题分区中\n        if (partitionFor(group.groupId) == offsetsPartition) {\n          // 执行组卸载逻辑\n          onGroupUnloaded(group)\n          // 关键步骤！将组信息从groupMetadataCache中移除\n          groupMetadataCache.remove(group.groupId, group)\n          // 把消费者组从producer对应的组集合中移除\n          removeGroupFromAllProducers(group.groupId)\n          // 更新已移除组计数器\n          numGroupsRemoved += 1\n          // 更新已移除位移值计数器\n          numOffsetsRemoved += group.numOffsets\n        }\n      }\n    }\n    info(s&quot;Finished unloading $topicPartition. Removed $numOffsetsRemoved cached offsets &quot; +\n      s&quot;and $numGroupsRemoved cached groups.&quot;)\n  }\n}\n</code></pre><p>该方法的主要逻辑是，先定义一个内部方法removeGroupsAndOffsets，然后创建一个异步任务，调用该方法来执行移除消费者组信息和位移信息。</p><p>那么，怎么判断要移除哪些消费者组呢？这里的依据就是<strong>传入的位移主题分区</strong>。每个消费者组及其位移的数据，都只会保存在位移主题的一个分区下。一旦给定了位移主题分区，那么，元数据保存在这个位移主题分区下的消费者组就要被移除掉。removeGroupsForPartition方法传入的offsetsPartition参数，表示Leader发生变更的位移主题分区，因此，这些分区保存的消费者组都要从该Broker上移除掉。</p><p>具体的执行逻辑是什么呢？我来解释一下。</p><p>首先，异步任务从ownedPartitions中移除给定位移主题分区。</p><p>其次，遍历消费者组元数据缓存中的所有消费者组对象，如果消费者组正是在给定位移主题分区下保存的，就依次执行下面的步骤。</p><ul>\n<li>第1步，调用onGroupUnloaded方法执行组卸载逻辑。这个方法的逻辑是上层组件GroupCoordinator传过来的。它主要做两件事情：将消费者组状态变更到Dead状态；封装异常表示Coordinator已发生变更，然后调用回调函数返回。</li>\n<li>第2步，把消费者组信息从groupMetadataCache中移除。这一步非常关键，目的是彻底清除掉该组的“痕迹”。</li>\n<li>第3步，把消费者组从producer对应的组集合中移除。这里的producer，是给Kafka事务用的。</li>\n<li>第4步，增加已移除组计数器。</li>\n<li>第5步，更新已移除位移值计数器。</li>\n</ul><p>到这里，方法结束。</p><h3>添加消费者组元数据</h3><p>下面，我们学习添加消费者组的管理方法，即addGroup。它特别简单，仅仅是调用putIfNotExists将给定组添加进groupMetadataCache中而已。代码如下：</p><pre><code>def addGroup(group: GroupMetadata): GroupMetadata = {\n  val currentGroup = groupMetadataCache.putIfNotExists(group.groupId, group)\n  if (currentGroup != null) {\n    currentGroup\n  } else {\n    group\n  }\n}\n</code></pre><h3>加载消费者组元数据</h3><p>现在轮到相对复杂的加载消费者组了。GroupMetadataManager类中定义了一个loadGroup方法执行对应的加载过程。</p><pre><code>private def loadGroup(\n  group: GroupMetadata, offsets: Map[TopicPartition, CommitRecordMetadataAndOffset],\n  pendingTransactionalOffsets: Map[Long, mutable.Map[TopicPartition, CommitRecordMetadataAndOffset]]): Unit = {\n  trace(s&quot;Initialized offsets $offsets for group ${group.groupId}&quot;)\n  // 初始化消费者组的位移信息\n  group.initializeOffsets(offsets, pendingTransactionalOffsets.toMap)\n  // 调用addGroup方法添加消费者组\n  val currentGroup = addGroup(group)\n  if (group != currentGroup)\n    debug(s&quot;Attempt to load group ${group.groupId} from log with generation ${group.generationId} failed &quot; +\n      s&quot;because there is already a cached group with generation ${currentGroup.generationId}&quot;)\n}\n</code></pre><p>该方法的逻辑有两步。</p><p>第1步，通过initializeOffsets方法，将位移值添加到offsets字段标识的消费者组提交位移元数据中，实现加载消费者组订阅分区提交位移的目的。</p><p>第2步，调用addGroup方法，将该消费者组元数据对象添加进消费者组元数据缓存，实现加载消费者组元数据的目的。</p><h2>消费者组位移管理</h2><p>除了消费者组的管理，GroupMetadataManager类的另一大类功能，是提供消费者组位移的管理，主要包括位移数据的保存和查询。我们总说，位移主题是保存消费者组位移信息的地方。实际上，<strong>当消费者组程序在查询位移时，Kafka总是从内存中的位移缓存数据查询，而不会直接读取底层的位移主题数据。</strong></p><h3>保存消费者组位移</h3><p>storeOffsets方法负责保存消费者组位移。该方法的代码很长，我先画一张图来展示下它的完整流程，帮助你建立起对这个方法的整体认知。接下来，我们再从它的方法签名和具体代码两个维度，来具体了解一下它的执行逻辑。</p><p><img src=\"https://static001.geekbang.org/resource/image/76/e6/76116b323c0c7b024ebe95c3c08e6ae6.jpg?wh=2452*3552\" alt=\"\"></p><p>我先给你解释一下保存消费者组位移的全部流程。</p><p><strong>首先</strong>，storeOffsets方法要过滤出满足特定条件的待保存位移信息。是否满足特定条件，要看validateOffsetMetadataLength方法的返回值。这里的特定条件，是指位移提交记录中的自定义数据大小，要小于Broker端参数offset.metadata.max.bytes的值，默认值是4KB。</p><p>如果没有一个分区满足条件，就构造OFFSET_METADATA_TOO_LARGE异常，并调用回调函数。这里的回调函数执行发送位移提交Response的动作。</p><p>倘若有分区满足了条件，<strong>接下来</strong>，方法会判断当前Broker是不是该消费者组的Coordinator，如果不是的话，就构造NOT_COORDINATOR异常，并提交给回调函数；如果是的话，就构造位移主题消息，并将消息写入进位移主题下。</p><p><strong>然后</strong>，调用一个名为putCacheCallback的内置方法，填充groupMetadataCache中各个消费者组元数据中的位移值，<strong>最后</strong>，调用回调函数返回。</p><p>接下来，我们结合代码来查看下storeOffsets方法的实现逻辑。</p><p>首先我们看下它的方法签名。既然是保存消费者组提交位移的，那么，我们就要知道上层调用方都给这个方法传入了哪些参数。</p><pre><code>// group：消费者组元数据\n// consumerId：消费者组成员ID\n// offsetMetadata：待保存的位移值，按照分区分组\n// responseCallback：处理完成后的回调函数\n// producerId：事务型Producer ID\n// producerEpoch：事务型Producer Epoch值\ndef storeOffsets(\n  group: GroupMetadata,\n  consumerId: String,\n  offsetMetadata: immutable.Map[TopicPartition, OffsetAndMetadata],\n  responseCallback: immutable.Map[TopicPartition, Errors] =&gt; Unit,\n  producerId: Long = RecordBatch.NO_PRODUCER_ID,\n  producerEpoch: Short = RecordBatch.NO_PRODUCER_EPOCH): Unit = {\n  ......\n}\n\n</code></pre><p>这个方法接收6个参数，它们的含义我都用注释的方式标注出来了。producerId和producerEpoch这两个参数是与Kafka事务相关的，你简单了解下就行。我们要重点掌握前面4个参数的含义。</p><ul>\n<li>group：消费者组元数据信息。该字段的类型就是我们之前学到的GroupMetadata类。</li>\n<li>consumerId：消费者组成员ID，仅用于DEBUG调试。</li>\n<li>offsetMetadata：待保存的位移值，按照分区分组。</li>\n<li>responseCallback：位移保存完成后需要执行的回调函数。</li>\n</ul><p>接下来，我们看下storeOffsets的代码。为了便于你理解，我删除了与Kafka事务操作相关的部分。</p><pre><code>// 过滤出满足特定条件的待保存位移数据\nval filteredOffsetMetadata = offsetMetadata.filter { case (_, offsetAndMetadata) =&gt;\n  validateOffsetMetadataLength(offsetAndMetadata.metadata)\n}\n......\nval isTxnOffsetCommit = producerId != RecordBatch.NO_PRODUCER_ID\n// 如果没有任何分区的待保存位移满足特定条件\nif (filteredOffsetMetadata.isEmpty) {\n  // 构造OFFSET_METADATA_TOO_LARGE异常并调用responseCallback返回\n  val commitStatus = offsetMetadata.map { case (k, _) =&gt; k -&gt; Errors.OFFSET_METADATA_TOO_LARGE }\n  responseCallback(commitStatus)\n  None\n} else {\n  // 查看当前Broker是否为给定消费者组的Coordinator\n  getMagic(partitionFor(group.groupId)) match {\n    // 如果是Coordinator\n    case Some(magicValue) =&gt;\n      val timestampType = TimestampType.CREATE_TIME\n      val timestamp = time.milliseconds()\n      // 构造位移主题的位移提交消息\n      val records = filteredOffsetMetadata.map { case (topicPartition, offsetAndMetadata) =&gt;\n        val key = GroupMetadataManager.offsetCommitKey(group.groupId, topicPartition)\n        val value = GroupMetadataManager.offsetCommitValue(offsetAndMetadata, interBrokerProtocolVersion)\n        new SimpleRecord(timestamp, key, value)\n      }\n      val offsetTopicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, partitionFor(group.groupId))\n      // 为写入消息创建内存Buffer\n      val buffer = ByteBuffer.allocate(AbstractRecords.estimateSizeInBytes(magicValue, compressionType, records.asJava))\n      if (isTxnOffsetCommit &amp;&amp; magicValue &lt; RecordBatch.MAGIC_VALUE_V2)\n        throw Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT.exception(&quot;Attempting to make a transaction offset commit with an invalid magic: &quot; + magicValue)\n      val builder = MemoryRecords.builder(buffer, magicValue, compressionType, timestampType, 0L, time.milliseconds(),\n        producerId, producerEpoch, 0, isTxnOffsetCommit, RecordBatch.NO_PARTITION_LEADER_EPOCH)\n      records.foreach(builder.append)\n      val entries = Map(offsetTopicPartition -&gt; builder.build())\n      // putCacheCallback函数定义......\n      if (isTxnOffsetCommit) {\n        ......\n      } else {\n        group.inLock {\n          group.prepareOffsetCommit(offsetMetadata)\n        }\n      }\n      // 写入消息到位移主题，同时调用putCacheCallback方法更新消费者元数据\n      appendForGroup(group, entries, putCacheCallback)\n    // 如果是Coordinator\n    case None =&gt;\n      // 构造NOT_COORDINATOR异常并提交给responseCallback方法\n      val commitStatus = offsetMetadata.map {\n        case (topicPartition, _) =&gt;\n          (topicPartition, Errors.NOT_COORDINATOR)\n      }\n      responseCallback(commitStatus)\n      None\n  }\n}\n</code></pre><p>我为方法的关键步骤都标注了注释，具体流程前面我也介绍过了，应该很容易理解。不过，这里还需要注意两点，也就是appendForGroup和putCacheCallback方法。前者是向位移主题写入消息；后者是填充元数据缓存的。我们结合代码来学习下。</p><p>appendForGroup方法负责写入消息到位移主题，同时传入putCacheCallback方法，更新消费者元数据。以下是它的代码：</p><pre><code>private def appendForGroup(\n  group: GroupMetadata,\n  records: Map[TopicPartition, MemoryRecords],\n  callback: Map[TopicPartition, PartitionResponse] =&gt; Unit): Unit = {\n  replicaManager.appendRecords(\n    timeout = config.offsetCommitTimeoutMs.toLong,\n    requiredAcks = config.offsetCommitRequiredAcks,\n    internalTopicsAllowed = true,\n    origin = AppendOrigin.Coordinator,\n    entriesPerPartition = records,\n    delayedProduceLock = Some(group.lock),\n    responseCallback = callback)\n}\n</code></pre><p>可以看到，该方法就是调用ReplicaManager的appendRecords方法，将消息写入到位移主题中。</p><p>下面，我们再关注一下putCacheCallback方法的实现，也就是将写入的位移值填充到缓存中。我先画一张图来展示下putCacheCallback的逻辑。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/42/bc2fcf199a685a5cc6d32846c53c3042.jpg?wh=2320*3320\" alt=\"\"></p><p>现在，我们结合代码，学习下它的逻辑实现。</p><pre><code>def putCacheCallback(responseStatus: Map[TopicPartition, PartitionResponse]): Unit = {\n  // 确保消息写入到指定位移主题分区，否则抛出异常\n  if (responseStatus.size != 1 || !responseStatus.contains(offsetTopicPartition))\n    throw new IllegalStateException(&quot;Append status %s should only have one partition %s&quot;\n      .format(responseStatus, offsetTopicPartition))\n  // 更新已提交位移数指标\n  offsetCommitsSensor.record(records.size)\n  val status = responseStatus(offsetTopicPartition)\n  val responseError = group.inLock {\n    // 写入结果没有错误\n    if (status.error == Errors.NONE) {\n      // 如果不是Dead状态\n      if (!group.is(Dead)) {\n        filteredOffsetMetadata.foreach { case (topicPartition, offsetAndMetadata) =&gt;\n          if (isTxnOffsetCommit)\n            ......\n          else\n            // 调用GroupMetadata的onOffsetCommitAppend方法填充元数据\n            group.onOffsetCommitAppend(topicPartition, CommitRecordMetadataAndOffset(Some(status.baseOffset), offsetAndMetadata))\n        }\n      }\n      Errors.NONE\n    // 写入结果有错误\n    } else {\n      if (!group.is(Dead)) {\n        ......\n        filteredOffsetMetadata.foreach { case (topicPartition, offsetAndMetadata) =&gt;\n          if (isTxnOffsetCommit)\n            group.failPendingTxnOffsetCommit(producerId, topicPartition)\n          else\n            // 取消未完成的位移消息写入\n            group.failPendingOffsetWrite(topicPartition, offsetAndMetadata)\n        }\n      }\n      ......\n      // 确认异常类型\n      status.error match {\n        case Errors.UNKNOWN_TOPIC_OR_PARTITION\n             | Errors.NOT_ENOUGH_REPLICAS\n             | Errors.NOT_ENOUGH_REPLICAS_AFTER_APPEND =&gt;\n          Errors.COORDINATOR_NOT_AVAILABLE\n\n        case Errors.NOT_LEADER_FOR_PARTITION\n             | Errors.KAFKA_STORAGE_ERROR =&gt;\n          Errors.NOT_COORDINATOR\n\n        case Errors.MESSAGE_TOO_LARGE\n             | Errors.RECORD_LIST_TOO_LARGE\n             | Errors.INVALID_FETCH_SIZE =&gt;\n          Errors.INVALID_COMMIT_OFFSET_SIZE\n        case other =&gt; other\n      }\n    }\n  }\n  // 利用异常类型构建提交返回状态\n  val commitStatus = offsetMetadata.map { case (topicPartition, offsetAndMetadata) =&gt;\n    if (validateOffsetMetadataLength(offsetAndMetadata.metadata))\n      (topicPartition, responseError)\n    else\n      (topicPartition, Errors.OFFSET_METADATA_TOO_LARGE)\n  }\n  // 调用回调函数\n  responseCallback(commitStatus)\n}\n</code></pre><p>putCacheCallback方法的主要目的，是将多个消费者组位移值填充到GroupMetadata的offsets元数据缓存中。</p><p><strong>首先</strong>，该方法要确保位移消息写入到指定位移主题分区，否则就抛出异常。</p><p><strong>之后</strong>，更新已提交位移数指标，然后判断写入结果是否有错误。</p><p>如果没有错误，只要组状态不是Dead状态，就调用GroupMetadata的onOffsetCommitAppend方法填充元数据。onOffsetCommitAppend方法的主体逻辑，是将消费者组订阅分区的位移值写入到offsets字段保存的集合中。当然，如果状态是Dead，则什么都不做。</p><p>如果刚才的写入结果有错误，那么，就通过failPendingOffsetWrite方法取消未完成的位移消息写入。</p><p><strong>接下来</strong>，代码要将日志写入的异常类型转换成表征提交状态错误的异常类型。具体来说，就是将UNKNOWN_TOPIC_OR_PARTITION、NOT_LEADER_FOR_PARTITION和MESSAGE_TOO_LARGE这样的异常，转换到COORDINATOR_NOT_AVAILABLE和NOT_COORDINATOR这样的异常。之后，再将这些转换后的异常封装进commitStatus字段中传给回调函数。</p><p><strong>最后</strong>，调用回调函数返回。至此，方法结束。</p><p>好了，保存消费者组位移信息的storeOffsets方法，我们就学完了，它的关键逻辑，是构造位移主题消息并写入到位移主题，然后将位移值填充到消费者组元数据中。</p><h3>查询消费者组位移</h3><p>现在，我再说说查询消费者组位移，也就是getOffsets方法的代码实现。比起storeOffsets，这个方法要更容易理解。我们看下它的源码：</p><pre><code>def getOffsets(\n  groupId: String, \n  requireStable: Boolean, \n  topicPartitionsOpt: Option[Seq[TopicPartition]]): Map[TopicPartition, PartitionData] = {\n  ......\n  // 从groupMetadataCache字段中获取指定消费者组的元数据\n  val group = groupMetadataCache.get(groupId)\n  // 如果没有组数据，返回空数据\n  if (group == null) {\n    topicPartitionsOpt.getOrElse(Seq.empty[TopicPartition]).map { topicPartition =&gt;\n      val partitionData = new PartitionData(OffsetFetchResponse.INVALID_OFFSET,\n        Optional.empty(), &quot;&quot;, Errors.NONE)\n      topicPartition -&gt; partitionData\n    }.toMap\n  // 如果存在组数据\n  } else {\n    group.inLock {\n      // 如果组处于Dead状态，则返回空数据\n      if (group.is(Dead)) {\n        topicPartitionsOpt.getOrElse(Seq.empty[TopicPartition]).map { topicPartition =&gt;\n          val partitionData = new PartitionData(OffsetFetchResponse.INVALID_OFFSET,\n            Optional.empty(), &quot;&quot;, Errors.NONE)\n          topicPartition -&gt; partitionData\n        }.toMap\n      } else {\n        val topicPartitions = topicPartitionsOpt.getOrElse(group.allOffsets.keySet)\n        topicPartitions.map { topicPartition =&gt;\n          if (requireStable &amp;&amp; group.hasPendingOffsetCommitsForTopicPartition(topicPartition)) {\n            topicPartition -&gt; new PartitionData(OffsetFetchResponse.INVALID_OFFSET,\n              Optional.empty(), &quot;&quot;, Errors.UNSTABLE_OFFSET_COMMIT)\n          } else {\n            val partitionData = group.offset(topicPartition) match {\n              // 如果没有该分区位移数据，返回空数据\n              case None =&gt;\n                new PartitionData(OffsetFetchResponse.INVALID_OFFSET,\n                  Optional.empty(), &quot;&quot;, Errors.NONE)\n              // 从消费者组元数据中返回指定分区的位移数据\n              case Some(offsetAndMetadata) =&gt;\n                new PartitionData(offsetAndMetadata.offset,\n                  offsetAndMetadata.leaderEpoch, offsetAndMetadata.metadata, Errors.NONE)\n            }\n            topicPartition -&gt; partitionData\n          }\n        }.toMap\n      }\n    }\n  }\n}\n</code></pre><p>getOffsets方法首先会读取groupMetadataCache中的组元数据，如果不存在对应的记录，则返回空数据集，如果存在，就接着判断组是否处于Dead状态。</p><p>如果是Dead状态，就说明消费者组已经被销毁了，位移数据也被视为不可用了，依然返回空数据集；若状态不是Dead，就提取出消费者组订阅的分区信息，再依次为它们获取对应的位移数据并返回。至此，方法结束。</p><h1>总结</h1><p>今天，我们学习了GroupMetadataManager类的源码。作为消费者组管理器，它负责管理消费者组的方方面面。其中，非常重要的两个管理功能是消费者组元数据管理和消费者组位移管理，分别包括查询获取、移除、添加和加载消费者组元数据，以及保存和查询消费者组位移，这些方法是上层组件GroupCoordinator倚重的重量级功能载体，你一定要彻底掌握它们。</p><p>我画了一张思维导图，帮助你复习一下今天的重点内容。</p><p><img src=\"https://static001.geekbang.org/resource/image/eb/5a/eb8fe45e1d152e2ac9cb52c81390265a.jpg?wh=2250*1869\" alt=\"\"></p><p>实际上，GroupMetadataManager类的地位举足轻重。虽然它在Coordinator组件中不显山不露水，但却是一些线上问题的根源所在。</p><p>我再跟你分享一个小案例。</p><p>之前，我碰到过一个问题：在消费者组成员超多的情况下，无法完成位移加载，这导致Consumer端总是接收到Marking the coordinator dead的错误。</p><p>当时，我查遍各种资料，都无法定位问题，最终，还是通过阅读源码，发现是这个类的doLoadGroupsAndOffsets方法中创建的buffer过小导致的。后来，通过调大offsets.load.buffer.size参数值，我们顺利地解决了问题。</p><p>试想一下，如果当时没有阅读这部分的源码，仅凭日志，我们肯定无法解决这个问题。因此，我们花三节课的时间，专门阅读GroupMetadataManager类源码，是非常值得的。下节课，我将带你继续研读GroupMetadataManager源码，去探寻有关位移主题的那些代码片段。</p><h1>课后讨论</h1><p>请思考这样一个问题：在什么场景下，需要移除GroupMetadataManager中保存的消费者组记录？</p><p>欢迎在留言区写下你的思考和答案，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p>","neighbors":{"left":{"article_title":"28 | 消费者组元数据（下）：Kafka如何管理这些元数据？","id":256055},"right":{"article_title":"30 | GroupMetadataManager：位移主题保存的只是位移吗？","id":257019}},"comments":[{"had_liked":false,"id":232865,"user_name":"胡夕","can_delete":false,"product_type":"c1","uid":1288090,"ip_address":"","ucode":"5709A689B6683B","user_header":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","comment_is_top":true,"comment_ctime":1594132586,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"9.2233720384489001e+18","product_id":100050101,"comment_content":"你好，我是胡夕。我来公布上节课的“课后讨论”题答案啦～<br><br>上节课，我们重点学习了Kafka如何管理消费者组元数据。课后我请你思考Kafka是怎么确认消费者组使用哪个成员的超时时间作为整个组的超时时间。实际上，Kafka取消费者组下所有成员的最大超时时间作为整个组的超时时间。具体代码可以看下GroupMetadata的rebalanceTimeoutMs方法。<br><br>okay，你同意这个说法吗？或者说你有其他的看法吗？我们可以一起讨论下。","like_count":0},{"had_liked":false,"id":328245,"user_name":"胡小禾","can_delete":false,"product_type":"c1","uid":1132315,"ip_address":"","ucode":"1C23B7492C0C9E","user_header":"https://static001.geekbang.org/account/avatar/00/11/47/1b/64262861.jpg","comment_is_top":false,"comment_ctime":1640618972,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5935586268","product_id":100050101,"comment_content":"看到2.7.1 版本的代码中有这样的,其实在至少这个版本里, 老师最后说的 case 是不会出现了吧?<br><br> if (config.loadBufferSize &lt; bytesNeeded)<br>                  warn(s&quot;Loaded offsets and group metadata from $topicPartition with buffer larger ($bytesNeeded bytes) than &quot; +<br>                    s&quot;configured offsets.load.buffer.size (${config.loadBufferSize} bytes)&quot;)<br><br>                buffer = ByteBuffer.allocate(bytesNeeded)<br><br>","like_count":1},{"had_liked":false,"id":357988,"user_name":"z.l","can_delete":false,"product_type":"c1","uid":1181055,"ip_address":"上海","ucode":"805CC5784D3F76","user_header":"https://static001.geekbang.org/account/avatar/00/12/05/7f/d35ab9a1.jpg","comment_is_top":false,"comment_ctime":1663816197,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1663816197","product_id":100050101,"comment_content":"有个问题，broker刚启动时是怎么从__consumer_offsets中加载offset的？怎么确定某个分区的offset在__consumer_offsets的哪个分区，哪个位置？","like_count":0},{"had_liked":false,"id":232373,"user_name":"伯安知心","can_delete":false,"product_type":"c1","uid":1961835,"ip_address":"","ucode":"6C17706658672C","user_header":"https://static001.geekbang.org/account/avatar/00/1d/ef/6b/5e8f6536.jpg","comment_is_top":false,"comment_ctime":1593986006,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1593986006","product_id":100050101,"comment_content":"从代码看，删除消费组的位移方法是removeGroupsForPartition，只有在处理身份或者位移变动调用handleGroupEmigration，请求这个方法的有2个调用。第一个方法handleLeaderAndIsrRequest判断就是新选择的broker重启太快导致leaderandisr请求变化，收到之前的请求也需要清理组的缓存记录，并且变动的topic是GROUP_METADATA_TOPIC_NAME的就需要删除消费组位移记录，第二个 方法handleStopReplicaRequest判断就是新选择的broker重启太快收到之前的请求也需要清理组的缓存记录。总之就是GROUP_METADATA_TOPIC_NAME变化在handleStopReplicaRequest请求和handleLeaderAndIsrRequest请求中就会有清除消费组位移记录的变化。","like_count":0},{"had_liked":false,"id":232231,"user_name":"二进制傻瓜","can_delete":false,"product_type":"c1","uid":1073261,"ip_address":"","ucode":"FA8E76339CF512","user_header":"https://static001.geekbang.org/account/avatar/00/10/60/6d/3c2a5143.jpg","comment_is_top":false,"comment_ctime":1593931196,"is_pvip":true,"replies":[{"id":"85762","content":"最佳实践是单台broker上分区数最好不超过2000","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1593997703,"ip_address":"","comment_id":232231,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1593931196","product_id":100050101,"comment_content":"胡大您好！单个kafka集群broker数，topic数以及partition数是否有上限？broker数或者topic 数以及partition太多会不会影响kafka的性能？这方面是否有最佳实践？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500575,"discussion_content":"最佳实践是单台broker上分区数最好不超过2000","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593997703,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1101864,"avatar":"https://static001.geekbang.org/account/avatar/00/10/d0/28/af87f7ab.jpg","nickname":"白京京","note":"","ucode":"5FE4B923570534","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300677,"discussion_content":"单台broker上的承载能力不应该是分区数来评估的吧！从机器来讲内存，多少张盘，topic数的话不如用多少个分区来描述的更加准确，还有就是与消息量的大小有关系，broker性能应该收到一张磁盘上不同消息量下能分出多少个分区有关系","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598231513,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}