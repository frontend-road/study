[{"article_id":479350,"article_title":"开篇词 | 想要洞悉系统底层的黑盒？先掌握 eBPF！","article_content":"<p>你好，我是倪朋飞，一名云计算从业者。</p><p>自从十年前参加工作以来，我一直都在云计算领域工作，特别专注于包括虚拟化、软件定义网络、容器等在内的云计算基础设施领域。</p><p>我其实是极客时间的老朋友了，曾在几年前开过<a href=\"https://time.geekbang.org/column/intro/100020901?tab=catalog\">《Linux 性能优化实战》</a>专栏。这门课聚焦于 Linux 性能优化技术，从系统底层原理、性能指标再到实际工作的优化技巧，带你准确分析和优化 Linux 性能问题。</p><p>这门课上线后，同学们的交流热情完全出乎我的意料，也激励着我把专栏的篇幅延长了10多篇。同时，我也注意到，很多同学在学习涉及系统底层知识较多的模块时掉了队，特别是面对系统内核的原理时有些畏惧的心理。</p><p>所以，今天我又给你带来了一门全新的课程。这门课的主要目标就是带你利用 eBPF 去洞悉内核的运行状态，并去解决性能优化、网络观测、安全控制等实际生产环境中的问题。在 eBPF 的助力下，你并不需要成为内核开发者，也可以掌控内核的运行状态。</p><h2>为什么要学习 eBPF？</h2><p>其实，我与 eBPF 的“初次接触”，还要从最流行的网络抓包和分析工具 tcpdump 开始说起。</p><p>我相信你和我一样，在一开始学习 TCP/IP 网络原理时，大量借助了 tcpdump 来了解网络协议的工作原理。而在实际的工作中，大多数网络问题的排查也是借助了 tcpdump 才能了解到网络层面上到底发生了什么事情。</p><!-- [[[read_end]]] --><p>后来，在排查断断续续的网络丢包问题时，我才发现只有 tcpdump 是不够的。tcpdump 只能告诉你网络上传输了哪些包，至于为什么这么传输，它就无能为力了。所以，当时我也搜索了大量的网络资料，偶然发现了 <a href=\"https://github.com/iovisor/bcc\">BCC</a> 这个工具，并借助它顺利解决了很多类似的网络问题。自此以后，网络问题就不再是我的心魔。</p><p><strong>这里我要说的是，tcpdump 和 BCC 之所以这么高效强大，都是得益于 BPF/eBPF 技术。</strong></p><p>eBPF 是什么呢？ 从它的全称“扩展的伯克利数据包过滤器 (Extended Berkeley Packet Filter)” 来看，它是一种数据包过滤技术，是从 BPF (Berkeley Packet Filter) 技术扩展而来的。</p><p>BPF 提供了一种在内核事件和用户程序事件发生时安全注入代码的机制，这就让非内核开发人员也可以对内核进行控制。随着内核的发展，BPF 逐步从最初的数据包过滤扩展到了网络、内核、安全、跟踪等，而且它的功能特性还在快速发展中，这种扩展后的 BPF 被简称为 eBPF（相应的，早期的 BPF 被称为经典 BPF，简称 cBPF）。实际上，现代内核所运行的都是 eBPF，如果没有特殊说明，内核和开源社区中提到的 BPF 等同于 eBPF（在我们的专栏里，它们的含义也完全相同）。</p><p>我想你已经知道，在 eBPF 之前，内核模块是注入内核的最主要机制。由于缺乏对内核模块的安全控制，内核的基本功能很容易被一个有缺陷的内核模块破坏。<strong>而 eBPF 则借助即时编译器（JIT），在内核中运行了一个虚拟机，保证只有被验证安全的 eBPF 指令才会被内核执行。</strong>同时，因为 eBPF 指令依然运行在内核中，无需向用户态复制数据，这就大大提高了事件处理的效率。</p><p>正是由于这些突出的特性，eBPF 现如今已经在故障诊断、网络优化、安全控制、性能监控等领域获得大量应用。比如，Facebook 开源的高性能网络负载均衡器 <a href=\"https://github.com/facebookincubator/katran\">Katran</a>、Isovalent 开源的容器网络方案 <a href=\"https://cilium.io\">Cilium</a> ，以及著名的内核跟踪排错工具 <a href=\"https://github.com/iovisor/bcc\">BCC</a> 和 <a href=\"https://github.com/iovisor/bpftrace\">bpftrace</a> 等，都是基于 eBPF 技术实现的。</p><p>下图（来自 <a href=\"https://ebpf.io\">ebpf.io</a>）是对eBPF 技术及其应用的一个概览：</p><p><img src=\"https://static001.geekbang.org/resource/image/7d/53/7de332b0fd6dc10b757a660305a90153.png?wh=1500x769\" alt=\"图片\" title=\"eBPF 技术概览\"></p><p>可以说，如果你想洞悉内核的运行状态，优化内核网络性能，控制诸如容器等应用程序的安全，那么eBPF 就是一个你必须要掌握的技能。</p><h2>要掌握 eBPF 是不是得先成为内核开发者？</h2><p>到这里，我已经带你初步了解了 eBPF 强大的功能特性，我想你应该对进一步深入学习 eBPF 迫不及待了吧！不过，我猜你也有可能像 5 年前的我一样，在看到 eBPF 需要运行在内核中，并且还涉及到一些内核的编程开发时，心里有点打退堂鼓。那么，我们是不是需要先成为内核开发者，才可以掌握 eBPF 呢？</p><p>实际上，前面我提到的 BCC、bpftrace 等一系列的开源项目已经提供了大量工具，可以帮你解决像故障诊断、性能监控、安全控制等绝大部分场景中的问题。而在你需要开发新的 eBPF 程序时，内核社区提供的 <a href=\"https://github.com/libbpf/libbpf\">libbpf</a> 库不仅帮你避免了直接调用内核函数，而且还提供了跨内核版本的兼容性（即一次编译到处执行，简称 CO-RE）。</p><p>所以，掌握 eBPF 并不需要掌握内核开发。我认为，学习最快的方法就是理解原理的同时配合大量的实践，eBPF 也不例外。下面这三点是学习 eBPF 的重中之重：</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/2e/dcd84984b168a5534d69a445b08c692e.jpg?wh=1920x1443\" alt=\"图片\"></p><p>只要理解了 eBPF 的基本原理，掌握了 eBPF 的运行机制和核心的编程接口，再结合大量的实践技巧，你也可以掌握 eBPF，并把它应用到真实的工作场景中。而这门课会针对不同场景，把这三个方面给你讲清楚。下面我们来具体看看这门课的设计思路。</p><h2>这门课是怎么设计的？</h2><p>为了帮你吃透 eBPF，在理解 eBPF 的同时更好地把它用起来，我会以案例驱动的思路，给你讲解 eBPF 的基本原理、使用方法以及相应的实践案例。</p><p>因为 eBPF 是一个实践性很强的技术，为了更好地掌握它，我希望你在学习这门课之前熟悉 Linux 的基本使用方法，并看得懂简单的 C、Python 等编程语言的基础语法。这样，你就能在后续课程中更好地掌握 eBPF 的实践方法。如果你觉得这些基础知识还没掌握好，也不用太担心，我会在 02 讲中为你详细介绍具体的学习路径、方法技巧，帮你快速查漏补缺，补足基础。</p><p>我会尽量把这门课的内容写得通俗易懂，并帮你划出重点、理出知识脉络，再通过案例分析和套路总结，让你学得更透、用得更熟。总之，我会带你从基础到实践，再结合实际案例，逐层深入 eBPF 相关的系统知识。</p><p><img src=\"https://static001.geekbang.org/resource/image/43/09/4314b5f14ed6cf38199289ab914b8309.jpg?wh=1920x1141\" alt=\"图片\"></p><p>由于 eBPF 还是一个快速发展的技术，也是 Linux 内核社区最活跃和变更最频繁的模块之一，这门课将尝试用全新的方式向你交付。具体来看，<strong>这门课的内容并不会一次性发布完毕，而是按时间分成两大阶段：常规更新阶段 + 动态更新阶段。</strong></p><p>第一个阶段，是像常规的专栏一样，每周更新三篇。这个阶段的内容分成学习准备篇、基础入门篇、实战进阶篇三个模块。</p><p>我会讲解 eBPF 的基本原理、使用方法、案例分析，以及常用工具、学习资料和学习经验总结。这些基本的知识，并不会随着时间的发展过时，它们是你理解 eBPF 机制、把握 eBPF 进化方向的抓手。</p><ul>\n<li><strong>学习准备篇</strong>，介绍 eBPF 的发展历程、工作原理以及主要的应用场景。同时，我也会带你梳理 eBPF 的技术脉络和学习路线，并分享我在学习 eBPF 时总结的技巧。</li>\n<li><strong>基础入门篇</strong>，介绍 eBPF 的基本原理、编程接口，以及进行详细的原理讲解，这包括：\n<ul>\n<li>如何搭建 eBPF 的开发环境；</li>\n<li>如何用好 BCC 并在它的基础上扩展自己的 eBPF 程序；</li>\n<li>如何从零开发一个 eBPF 程序；</li>\n<li>如何根据实际需要选择具体的 eBPF 程序类型。</li>\n</ul>\n</li>\n<li><strong>实战进阶篇</strong>，在了解了 eBPF 的基本使用方法后，我会通过一些案例，带你实践 eBPF 的主要应用场景，这包括：\n<ul>\n<li>如何使用 eBPF 跟踪内核状态；</li>\n<li>如何使用 eBPF 跟踪进程状态；</li>\n<li>如何使用 eBPF 排查网络问题；</li>\n<li>如何使用 eBPF 增强容器安全；</li>\n<li>如何开发一个 eBPF 负载均衡程序。</li>\n</ul>\n</li>\n</ul><p>我相信学完第一个阶段的内容后，你就能掌握 eBPF 的运行原理，并且可以编写自己的 eBPF 程序，观测内核的运行状态，并对 eBPF 常见的应用场景了然于胸。</p><p><strong>第二个阶段则是一个动态的过程，我们准备把它叫作“技术雷达篇”。</strong>在第一阶段结束后的 4 年里，每个季度我都会更新一篇文章，带你持续跟踪内核和开源社区的最新进展和应用案例。</p><p><img src=\"https://static001.geekbang.org/resource/image/a7/5b/a7dfe133bd7c98a98f9427a21b7c705b.jpg?wh=1920x787\" alt=\"图片\"></p><p>也就是说，这门课会全方位地解决你在学习、应用 eBPF 时候的一切重点问题。我会先在第一阶段把基础知识交付给你，然后在第二阶段定期向你交付 eBPF 技术的最新进展、发展趋势。 eBPF 技术时时刻刻在发展变化，但是只要你紧跟这颗“雷达”，就能在第一时间获得我为你梳理的最新信息。这样，你就不用再漫无目的地看资讯、查资料、找重点，可以把更多时间花在用好 eBPF 上。</p><p>由于这是一个全新的动态交付过程，我希望你可以和我，还有学习这门课的其他同学保持交流，分享你的学习心得和实践经验，并为课程未来的内容提供建议。</p><p>我邀请你在接下来的时间里，跟我一起走入 eBPF 的奇妙世界。我希望这门课可以帮你掌握 eBPF 的基础原理，并且学会如何把它们真正落地到你的产品之中，解决真实生产环境中的各种问题。同时，我们还会一起见证未来几年中 eBPF 技术的快速更新，共同探索技术发展的更多可能。接下来，就让我们正式开始这段旅程吧！</p>","neighbors":{"left":[],"right":{"article_title":"01｜技术概览：eBPF 的发展历程及工作原理","id":479384}}},{"article_id":479384,"article_title":"01｜技术概览：eBPF 的发展历程及工作原理","article_content":"<p>你好，我是倪朋飞。</p><p>在正式介绍 eBPF 的使用方法和具体应用之前，我会用两讲的内容，带你了解eBPF的技术脉络和学习路线，为你后面的学习做好准备。</p><p>在开篇词里，我带你一起了解了这门课的设计思路和主要内容，也简单介绍了 eBPF 的主要应用场景，包括故障诊断、网络优化、安全控制、性能监控等。那你可能就要问了：为什么 eBPF 可以应用到这么广泛的领域呢？</p><p>eBPF 广泛的应用场景和强大的功能，跟它的发展历程、基本原理密切相关。那么，eBPF 的发展历程是什么样的？它又是如何在确保安全的前提下，允许非内核开发者去扩展内核的功能的呢？今天，我就带你一起来看看这些问题。</p><h2>eBPF 的发展历程是什么样的?</h2><p>在开篇词中，我曾经提到，eBPF 是从 BPF (Berkeley Packet Filter) 技术扩展而来的。而说起 BPF，它的历史就更悠长了。</p><p>早在 1992 年的 USENIX 会议上，Steven McCanne 和 Van Jacobson 发布的论文“<a href=\"https://www.tcpdump.org/papers/bpf-usenix93.pdf\">The BSD Packet Filter: A New Architecture for User-level Packet Capture</a>” 就为 BSD 操作系统带来了革命性的包过滤机制 BSD Packet Filter（简称为 BPF），这比当时最先进的数据包过滤技术还快 20 倍。为什么性能这么好呢？这主要得益于 BPF 的两大设计：</p><!-- [[[read_end]]] --><ul>\n<li>第一，内核态引入一个新的虚拟机，所有指令都在内核虚拟机中运行。</li>\n<li>第二，用户态使用 BPF 字节码来定义过滤表达式，然后传递给内核，由内核虚拟机解释执行。</li>\n</ul><p>这就使得包过滤可以直接在内核中执行，避免了向用户态复制每个数据包，从而极大提升了包过滤的性能，进而被各大操作系统广泛接受。BPF 最初的名字 BSD Packet Filter ，也被作者的工作单位名所替代，变成了 Berkeley Packet Filter（很巧的是，还是简称 BPF）。</p><p>在 BPF 诞生五年后，Linux 2.1.75 首次引入了 BPF 技术，随后&nbsp;BPF 开始了不温不火的发展历程。其中，Linux 3.0 中增加的 BPF 即时编译器可以算是一个最重大的更新了。它替换掉了原本性能更差的解释器，进一步优化了 BPF 指令运行的效率。但直到此时，BPF 的应用还是仅限于网络包过滤这个传统的领域中。</p><p>时间到了 2014 年。为了研究新的软件定义网络方案，Alexei Starovoitov 为 BPF 带来了第一次革命性的更新，将 BPF 扩展为一个通用的虚拟机，也就是 eBPF。eBPF 不仅扩展了寄存器的数量，引入了全新的 BPF 映射存储，还在 4.x 内核中将原本单一的数据包过滤事件逐步扩展到了内核态函数、用户态函数、跟踪点、性能事件（perf_events）以及安全控制等。</p><p><strong>eBPF 的诞生是 BPF 技术的一个转折点，使得 BPF 不再仅限于网络栈，而是成为内核的一个顶级子系统。</strong></p><p>在内核发展的同时，eBPF 繁荣的生态也进一步促进了 eBPF 的蓬勃发展。这其中，最典型的就是 iovisor 带来的 BCC、bpftrace 等工具，成为 eBPF 在跟踪和排错领域的最佳实践。由于 eBPF 无需修改内核源码和重新编译内核就可以扩展内核的功能，Cilium、Katran、Falco 等一系列基于 eBPF 优化网络和安全的开源项目也逐步诞生。并且，越来越多的开源和商业解决方案开始借助 eBPF，优化其网络、安全以及观测的性能。比如，最流行的网络解决方案之一 Calico，就在最近的版本中引入了 <a href=\"https://www.tigera.io/blog/introducing-the-calico-ebpf-dataplane/\">eBPF 数据面网络</a>，大大提升了网络的性能。</p><p>为了帮你更好地理解 eBPF 的发展历程，我把 eBPF 诞生以来的发展过程整理成了一张图片：</p><p><img src=\"https://static001.geekbang.org/resource/image/b4/ff/b44562381748de369b50403219c0d1ff.jpg?wh=2284x7454\" alt=\"\" title=\"eBPF 发展历程\"></p><p>直到今天，eBPF 依然是内核社区最活跃的子模块之一，还处在一个快速发展的过程中。可以说，<strong>eBPF 开启的创新才刚刚开始</strong>，在未来我们会看到更多的创新案例。正是为了确保每个 eBPF 学习者不掉队，我们把这门课设计成了动态发布的形式，带你随时跟踪这些最新的发展和案例。</p><p>了解了 eBPF 的诞生过程后，还有一点需要你留意：在内核社区的开发讨论中，通常还是使用 BPF 这个缩略词，而在很多应用的文档中可能会倾向使用 eBPF。其实它们的含义是一样的，都是指扩展版的 BPF。</p><h2>eBPF 是怎么工作的?</h2><p>eBPF 程序并不像常规的线程那样，启动后就一直运行在那里，它需要事件触发后才会执行。这些事件包括系统调用、内核跟踪点、内核函数和用户态函数的调用退出、网络事件，等等。借助于强大的内核态插桩（kprobe）和用户态插桩（uprobe），eBPF 程序几乎可以在内核和应用的任意位置进行插桩。</p><p>看到这个令人惊叹的能力，你一定有疑问：这会不会像内核模块一样，一个异常的 eBPF 程序就会损坏整个内核的稳定性呢？其实，<strong>确保安全和稳定一直都是 eBPF 的首要任务</strong>，不安全的 eBPF 程序根本就不会提交到内核虚拟机中执行。</p><p>Linux 内核是如何实现 eBPF 程序的安全和稳定的呢？其实很简单，我带你看个 eBPF 程序的执行过程，你就明白了。</p><p>如下图（图片来自<a href=\"https://www.brendangregg.com/ebpf.html\">brendangregg.com</a>）所示，通常我们借助 <a href=\"https://llvm.org/\">LLVM</a> 把编写的 eBPF 程序转换为 BPF 字节码，然后再通过 bpf 系统调用提交给内核执行。内核在接受 BPF 字节码之前，会首先通过验证器对字节码进行校验，只有校验通过的 BPF 字节码才会提交到即时编译器执行。</p><p><img src=\"https://static001.geekbang.org/resource/image/a7/6a/a7165eea1fd9fc24090a3a1e8987986a.png?wh=1500x550\" alt=\"图片\" title=\"eBPF 程序执行过程\"></p><p>如果 BPF 字节码中包含了不安全的操作，验证器会直接拒绝 BPF 程序的执行。比如，下面就是一些典型的验证过程：</p><ul>\n<li>只有特权进程才可以执行 bpf 系统调用；</li>\n<li>BPF 程序不能包含无限循环；</li>\n<li>BPF 程序不能导致内核崩溃；</li>\n<li>BPF 程序必须在有限时间内完成。</li>\n</ul><p>BPF 程序可以利用 BPF 映射（map）进行存储，而用户程序通常也需要通过 BPF 映射同运行在内核中的 BPF 程序进行交互。如下图（图片来自<a href=\"https://ebpf.io/what-is-ebpf\">ebpf.io</a>）所示，在性能观测中，BPF 程序收集内核运行状态存储在映射中，用户程序再从映射中读出这些状态。</p><p><img src=\"https://static001.geekbang.org/resource/image/53/dd/53af7f7db99c3ca57f981f00303949dd.png?wh=1401x733\" alt=\"图片\" title=\"BPF 映射\"></p><p>可以看到，eBPF 程序的运行需要历经编译、加载、验证和内核态执行等过程，而用户态程序则需要借助 BPF 映射来获取内核态 eBPF 程序的运行状态。</p><h2>eBPF 是万能的吗?</h2><p>看到这里，你是不是因为 eBPF 在扩展内核功能上的强大能力而兴奋不已？我猜你已经迫不及待想要体验一下了。不过，在你体验之前，我还要提醒你一点：eBPF 并不是万能的，它也有很多的局限性。下面是一些最常见的&nbsp;eBPF 限制：</p><ul>\n<li>eBPF 程序必须被验证器校验通过后才能执行，且不能包含无法到达的指令；</li>\n<li>eBPF 程序不能随意调用内核函数，只能调用在 API 中定义的辅助函数；</li>\n<li>eBPF 程序栈空间最多只有 512 字节，想要更大的存储，就必须要借助映射存储；</li>\n<li>在内核 5.2 之前，eBPF 字节码最多只支持 4096 条指令，而 5.2 内核把这个限制提高到了 100 万条；</li>\n<li>由于内核的快速变化，在不同版本内核中运行时，需要访问内核数据结构的 eBPF 程序很可能需要调整源码，并重新编译。</li>\n</ul><p>此外，虽然 Linux 内核很早就已经支持了 eBPF，但很多新特性都是在 4.x 版本中逐步增加的，具体你可以看下<a href=\"https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#main-features\">这个链接</a>。所以，想要稳定运行 eBPF 程序，<strong>内核版本至少需要 4.9 或者更新</strong>。而在开发和学习 eBPF 时，<strong>为了体验最新的 eBPF 特性，我推荐使用更新的 5.x 内核</strong>。在这门课后面的内容中，我还会给你详细讲解开发环境的搭建步骤，以及推荐的 Linux&nbsp;发行版。</p><h2>小结</h2><p>今天，我带你一起探索了 eBPF 技术的发展历程，并梳理了 eBPF 的工作原理。</p><p>eBPF 是从 BPF 技术扩展而来的。BPF 出现后，一直都是网络数据包过滤的核心，但直到 eBPF 诞生前，BPF 都仅用于包过滤这个场景中。eBPF 的诞生是 BPF 技术的一个转折点，使它的应用范围逐步从包过滤扩展到内核函数、用户函数、跟踪点、性能事件、安全控制等全新的领域中。而这也进一步催生了Cilium、Katran、Falco 等一大批基于 eBPF 构建的网络和安全解决方案，形成了繁荣的 eBPF 生态。</p><p>eBPF 程序以内核事件触发的方式运行，并且其运行过程包括编译、加载、验证和内核态执行等。为了保护内核的安全和稳定，如果编译后 BPF 字节码中包含了不安全的操作，验证阶段会直接拒绝 BPF 程序的执行。</p><p>不过，需要提醒你的是：为了确保安全和稳定，eBPF 程序也有很多的限制，这是你在后续的学习过程中需要特别留心的。</p><h2>思考题</h2><p>在这一讲的最后，想和你交流的问题是：在之前的学习和工作中，你有没有使用过 eBPF 呢？如果使用过，你又用 eBPF 解决过哪些实际的问题呢？期待你在留言区分享，并和我交流讨论。</p><p>今天的内容到这里就结束了，欢迎把它分享给你的同事和朋友，我们下一讲见。</p>","neighbors":{"left":{"article_title":"开篇词 | 想要洞悉系统底层的黑盒？先掌握 eBPF！","id":479350},"right":{"article_title":"02 | 先利其器：如何高效学习 eBPF？","id":480094}}},{"article_id":480094,"article_title":"02 | 先利其器：如何高效学习 eBPF？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我们一起了解了 eBPF 的发展历程、基本原理和主要应用场景。eBPF 来源于 Linux 内核子系统 Berkeley Packet Filter (BPF)，最早用于提升网络包过滤的性能。后来，随着 BPF 技术的逐步完善，它的应用范围从内核空间扩展到了用户空间，并逐步在网络、可观测以及安全等方面获得了大量的应用。</p><p>了解过这些的你，很可能遇到了我曾经有过的疑惑：作为 Linux 内核的一部分，eBPF 这么底层的技术，到底该如何学习才能更高效地掌握它？</p><p>这是初学者经常遇到的问题：在学习 eBPF 的知识和原理时，找不到正确的方法，只是照着网络上并不全面的片段文章操作，或者直接去啃内核的源码，这样往往事倍功半。甚至，还可能被海量的信息淹没，失去了持续学习的信心，“从入门到放弃” 。那么今天，我们就一起来看看，怎么才能高效且深入地学习 eBPF。</p><h2>学习这门课需要什么基础？</h2><p>首先，在学习 eBPF 之前你要明白，eBPF 是 Linux 的一部分，它所有的应用都需要在 Linux 系统中完成（虽然 Windows 也已经支持了 eBPF，但暂时不够成熟）。所以，我希望你<strong>至少熟练掌握一种 Linux 系统（比如 Ubuntu、RHEL）的基本使用方法</strong>，包括：</p><!-- [[[read_end]]] --><ul>\n<li>熟悉 Linux 系统常用命令的使用方法；</li>\n<li>熟悉 Linux 系统常用软件包的安装方法；</li>\n<li>了解 Linux 系统常用的网络工具和基本的网络排错方法。</li>\n</ul><p>因为这门课的核心在于 eBPF，所以在后续的内容中，我不会详细介绍这些基本的 Linux 系统使用方法。所以，你需要提前掌握这些基本知识。这样，接下来我在讲解 eBPF 时，你就更容易理解背后的工作原理和详细使用方法，也更容易明白每一步实践操作的具体含义。</p><p>其次，由于 eBPF 是内核的一部分，在实际使用 eBPF 时，大都会涉及一些内核接口的编程。所以，我希望你<strong>有一定的编程基础（比如 C 语言和 Python 语言），并了解如何从源代码编译和运行 C 程序。</strong>以 C 语言和 Python 语言为例，主要包括：</p><ul>\n<li>了解 C 语言和 Python 语言的基本语法格式，并能够看懂简单的 C 语言和 Python 语言程序；</li>\n<li>了解 C 语言程序的编译方法，并了解 make、gcc 等常用编译工具的基本使用方法；</li>\n<li>了解 C 语言和 Python 语言程序的调试方法，并能够借助日志、调试器等，在程序出错时排查问题的原因。</li>\n</ul><p>虽然这些内容看起来可能会很难，并且内容很多，但实际上任何一本 C 语言和 Python 语言的入门书籍都会讲到这些基本知识，这里我推荐适合编程入门者的 《C 程序设计语言》和《Python 编程：从入门到实践》（如果你更喜欢用视频的方式学习，也可以在 B 站中找到很多入门的视频教程）。</p><p>了解了 C 语言和 Python 语言的编程基础，我在后续讲解 eBPF 和 Linux 内核相关的编程接口时，你就能够更好地理解这些编程接口的原理，也更容易通过编程把 eBPF 变成自己的武器。</p><p>最后，虽然 eBPF 是 Linux 内核的一部分，其编程接口也通常涉及一定的内核原理，但<strong>在刚开始学习时，我并不建议你深入去学习内核的源码</strong>。就像我在《Linux 性能优化实战》专栏中提到的那样：学习一门新的技术时，你并不需要了解所有与这门技术相关的细节，只要了解这个技术的基本原理，并建立起整个技术的全局观，你就可以掌握这个技术的核心。</p><h2>学习 eBPF，重点是什么？</h2><p>在开篇词里，我已经提过这一点：学习一门新技术，最快的方法就是理解原理的同时配合大量的实践，eBPF 也不例外。因而，我们学习 eBPF 的重中之重就是：</p><ul>\n<li>理解 eBPF 的基本原理；</li>\n<li>掌握 eBPF 的编程接口；</li>\n<li>通过实践把 eBPF 应用到真正的工作场景中。</li>\n</ul><p>这门课会针对不同场景，把这三个重点方面给你讲清楚，也希望你一定要花时间和心思来消化它们。</p><p>其实，如果你之前学习过我的上一个专栏《Linux 性能优化实战》，你就已经把 eBPF 用到了性能优化的场景中。作为最灵活的动态追踪方法，<a href=\"https://github.com/iovisor/bcc\">BCC</a> 包含的所有工具都是基于 eBPF 开发的。如下图（图片来自 <a href=\"https://www.brendangregg.com/Perf/bcc_tracing_tools.png\">www.brendangregg.com</a>）所示，BCC 提供了贯穿内核各个子系统的动态追踪工具：</p><p><img src=\"https://static001.geekbang.org/resource/image/82/f3/82d8912ebdc2815e29b6dc754a5f03f3.png?wh=1500x1050\" alt=\"图片\" title=\"BCC 工具大全\"></p><p>所有的这些工具都是开源的，它们其实也是学习 eBPF 最好的参考资料。<strong>你可以先把这些工具用起来，然后通过源码了解它们的工作原理，最后再尝试扩展这些工具，增加自己的想法。</strong></p><p>但是切记，BCC 等工具并不是学习的全部。工具只是解决问题的手段，关键还是背后的工作原理。只有掌握了 eBPF 的核心原理，并且结合具体实践融会贯通，你才能真正掌握它们。</p><p>最后，为了让你对 eBPF 有个全面的认识，我画了一张思维导图，里面涵盖了 eBPF 的所有学习重点，这门课中也基本都会讲到。你可以把这张图保存或者打印下来，每学会一部分，就在上面做标记，这样就能更好地记录并把握自己的学习进度。</p><p><img src=\"https://static001.geekbang.org/resource/image/03/9a/030c0c56a9d210690c75770fe6761f9a.jpg?wh=1920x2355\" alt=\"图片\"></p><h2>怎么学习更高效？</h2><p>前面，我给你讲了 eBPF 的学习重点，接下来我再跟你分享几个学习技巧。掌握了这些技巧，你可以学习得更轻松。</p><p><strong>技巧一：虽然对内核源码的理解很重要，但切记，不要陷入内核的实现细节中。</strong></p><p>eBPF 的学习和使用绕不开对内核源码的理解和应用，但并不是说学习 eBPF 就一定要掌握所有的内核源码。实际上，如果你一开始就陷入了内核的具体实现细节中，很可能会因为巨大的困难而产生退意。</p><p>你要记得，学习 eBPF 的目的并不是掌握内核，而是利用 eBPF 这个强大的武器去解决网络、观测、排错以及安全等实际的问题。因而，对初学者来说，掌握 eBPF 的基本原理以及编程接口才是学习的重点。最后再强调一遍，学习时要分清主次，不要陷入内核的实现细节中。</p><p><strong>技巧二：边学习边实践，并大胆借鉴开源项目。</strong></p><p>eBPF 的基本原理当然重要，但只懂原理而不动手实战，并不能真正掌握 eBPF。只有用 eBPF 解决了实际的问题，eBPF 才真正算是你的武器。</p><p>得益于开源软件，无论在 Linux 内核中还是在 GitHub 等开源网站上，你都可以找到大量基于 eBPF 的开源案例。在动手实践 eBPF 时，你可以大胆借鉴它们的思路，学习开源社区中解决同类问题的办法。</p><p><strong>技巧三：多交流、多思考，并参与开源社区讨论。</strong></p><p>作为一个实践性很强，并且在飞速发展的技术，eBPF 的功能特性还在快速迭代中。同时，由于新技术总有一个成熟的过程，在开始接触和应用 eBPF 时，我们也需要留意它在不同内核版本中的限制。</p><p>所以，我希望你在学习 eBPF 的过程中，做到多交流、多思考。你可以随时在评论区给我留言，写下自己的疑问、思考和总结，和我还有其他的学习者一起讨论切磋。看到优秀的开源项目时，你也可以去参与贡献，和开源社区的大拿们共同完善 eBPF 生态。</p><h2>小结</h2><p>今天，我带你一起梳理了高效学习 eBPF 的方法，并分享了一些我在学习 eBPF 时整理的技巧。</p><p>相对于其他技术来说，eBPF 是一个更接近 Linux 底层的技术。虽然学习它之前不需要你掌握内核的开发，但由于 eBPF 的应用总是伴随着大量的开发过程，你最好可以先熟悉 Linux 系统的基本使用方法，以及 C、Python 等语言的基础编程。</p><p>在学习 eBPF 技术时，你也不需要掌握所有相关技术的细节，只要重点把握 eBPF 基本原理和编程接口，再配合大量的工作实践，你就可以完全掌握 eBPF 技术。</p><p>当然了，eBPF 还处在飞速发展的过程中，多了解开源社区的进展和最新应用案例，多参与开源社区讨论，可以帮你更深刻地理解 eBPF 的核心原理，同时更好地把握 eBPF 未来的发展方向。</p><h2>思考题</h2><p>这一讲和上一讲的内容，都是为我们后续的学习做准备的，你可以把它们当作课前的热身环节。从下一讲开始，我们就要正式进入 eBPF 的世界了。所以，我想请你聊一聊：</p><ol>\n<li>你之前有没有了解过基于 eBPF 的开源项目，可以和同学们分享你觉得优秀的项目吗？</li>\n<li>在学习和使用 eBPF 时，你有哪些心得，又遇到过哪些问题？欢迎在评论区分享。</li>\n</ol><p>今天的内容就结束了，欢迎把它分享给你的同事和朋友，我们下一讲见。</p>","neighbors":{"left":{"article_title":"01｜技术概览：eBPF 的发展历程及工作原理","id":479384},"right":{"article_title":"03 | 初窥门径：开发并运行你的第一个 eBPF 程序","id":481090}}},{"article_id":481090,"article_title":"03 | 初窥门径：开发并运行你的第一个 eBPF 程序","article_content":"<p>你好，我是倪朋飞。</p><p>通过前面两讲，我已经带你为正式进入 eBPF 的学习做好了准备，接下来我们进入第二个模块“基础入门篇”的学习。在这个模块里，你会学习到 eBPF 的开发环境搭建方法、运行原理、编程接口，以及各种类型 eBPF 程序的事件触发机制和应用场景。</p><p>上一讲，我带你一起梳理了 eBPF 的学习路径和学习技巧，并特别强调了动手实践在学习 eBPF 过程中的重要性。那么，eBPF 程序到底是什么样子的？如何搭建 eBPF 的开发环境，又如何开发一个 eBPF 应用呢？</p><p>今天，我就带你一起上手开发第一个 eBPF 程序。</p><h2>如何选择 eBPF 开发环境？</h2><p>在前两讲中，我曾提到，虽然 Linux 内核很早就已经支持了 eBPF，但很多新特性都是<a href=\"https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#main-features\">在 4.x 版本中逐步增加的</a>。所以，想要稳定运行 eBPF 程序，内核至少需要 <strong>4.9</strong> 或者更新的版本。而在开发和学习 eBPF 时，为了体验和掌握最新的 eBPF 特性，我推荐使用更新的 <strong>5.x</strong> 内核。</p><p>作为 eBPF 最重大的改进之一，一次编译到处执行（简称 CO-RE）解决了内核数据结构在不同版本差异导致的兼容性问题。不过，在使用 CO-RE 之前，内核需要开启  <code>CONFIG_DEBUG_INFO_BTF=y</code> 和 <code>CONFIG_DEBUG_INFO=y</code> 这两个编译选项。为了避免你在首次学习 eBPF 时就去重新编译内核，我推荐使用已经默认开启这些编译选项的发行版，作为你的开发环境，比如：</p><!-- [[[read_end]]] --><ul>\n<li>Ubuntu 20.10+</li>\n<li>Fedora 31+</li>\n<li>RHEL 8.2+</li>\n<li>Debian 11+</li>\n</ul><p>你可以到公有云平台上创建这些发行版的虚拟机，也可以借助 <a href=\"https://www.vagrantup.com\">Vagrant</a>&nbsp;、<a href=\"https://multipass.run\">Multipass</a> 等工具，创建本地的虚拟机。比如，使用我最喜欢的 <a href=\"https://www.vagrantup.com/downloads\">Vagrant</a>&nbsp;，通过下面几步就可以创建出一个 Ubuntu 21.10 的虚拟机：</p><pre><code class=\"language-bash\">#&nbsp;创建和启动Ubuntu 21.10虚拟机\nvagrant init&nbsp;ubuntu/impish64\nvagrant up\n\n# 登录到虚拟机\nvagrant ssh\n</code></pre><h2>如何搭建 eBPF 开发环境？</h2><p>虚拟机创建好之后，接下来就需要安装 eBPF 开发和运行所需要的开发工具，这包括：</p><ul>\n<li>将 eBPF 程序编译成字节码的 LLVM；</li>\n<li>C 语言程序编译工具 make；</li>\n<li>最流行的 eBPF 工具集 BCC 和它依赖的内核头文件；</li>\n<li>与内核代码仓库实时同步的 libbpf；</li>\n<li>同样是内核代码提供的 eBPF 程序管理工具 bpftool。</li>\n</ul><p>你可以执行下面的命令，来安装这些必要的开发工具：</p><pre><code class=\"language-shell\">#&nbsp;For Ubuntu20.10+\nsudo apt-get install -y  make clang llvm libelf-dev&nbsp;libbpf-dev bpfcc-tools libbpfcc-dev linux-tools-$(uname -r) linux-headers-$(uname -r)\n\n# For RHEL8.2+\nsudo yum install libbpf-devel make clang llvm elfutils-libelf-devel bpftool bcc-tools bcc-devel\n</code></pre><p>如果你已经熟悉了 Linux 内核的自定义编译和安装方法，并选择在旧的发行版中通过自行编译和升级内核搭建开发环境，上述的开发工具流程也需要做适当的调整。这里特别提醒下，libbpf-dev 这个库很可能需要从源码安装，具体的步骤你可以参考 libbpf 的 <a href=\"https://github.com/libbpf/libbpf\">GitHub 仓库</a>。</p><h2>如何开发第一个 eBPF 程序？</h2><p>当前面这些开发工具和依赖库安装完成后，一个完整的 eBPF 开发环境就准备好了。接下来，你肯定迫不及待地想要体验一下 eBPF 的强大功能了。</p><p>不过，在开发 eBPF 程序之前，我们先来看一下 eBPF 的开发和执行过程。如下图（图片来自 <a href=\"https://www.brendangregg.com/ebpf.html\">brendangregg.com</a>）所示，一般来说，这个过程分为以下 5 步：</p><ul>\n<li>第一步，使用 C 语言开发一个 eBPF 程序；</li>\n<li>第二步，借助 LLVM 把 eBPF 程序编译成 BPF 字节码；</li>\n<li>第三步，通过 bpf 系统调用，把 BPF 字节码提交给内核；</li>\n<li>第四步，内核验证并运行 BPF 字节码，并把相应的状态保存到 BPF 映射中；</li>\n<li>第五步，用户程序通过 BPF 映射查询 BPF 字节码的运行状态。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/a7/6a/a7165eea1fd9fc24090a3a1e8987986a.png?wh=1500x550\" alt=\"图片\" title=\"eBPF 程序执行过程\"></p><p>这里的每一步，我们当然可以自己动手去完成。但对初学者来说，我推荐你从 BCC（BPF Compiler Collection）开始学起。</p><p>BCC 是一个 BPF 编译器集合，包含了用于构建 BPF 程序的编程框架和库，并提供了大量可以直接使用的工具。使用 BCC 的好处是，<strong>它把上述的 eBPF 执行过程通过内置框架抽象了起来，并提供了 Python、C++ 等编程语言接口</strong>。这样，你就可以直接通过 Python 语言去跟 eBPF 的各种事件和数据进行交互。</p><p>接下来，我就以跟踪 <a href=\"https://man7.org/linux/man-pages/man2/open.2.html\">openat()</a>（即打开文件）这个系统调用为例，带你来看看如何开发并运行第一个 eBPF 程序。</p><p>使用 BCC 开发 eBPF 程序，可以把前面讲到的五步简化为下面的三步。</p><h3><strong>第一步：使用 C 开发一个 eBPF 程序</strong></h3><p>新建一个  <code>hello.c</code>&nbsp;文件，并输入下面的内容：</p><pre><code class=\"language-c++\">int hello_world(void *ctx)\n{\n&nbsp; &nbsp; bpf_trace_printk(\"Hello, World!\");\n&nbsp; &nbsp; return 0;\n}\n</code></pre><p>就像所有编程语言的“&nbsp;Hello World&nbsp;”示例一样，这段代码的含义就是打印一句 “Hello, World!” 字符串。其中， <code>bpf_trace_printk()</code>&nbsp;是一个最常用的 BPF 辅助函数，它的作用是输出一段字符串。不过，由于 eBPF 运行在内核中，它的输出并不是通常的标准输出（stdout），而是内核调试文件  <code>/sys/kernel/debug/tracing/trace_pipe</code>&nbsp;，你可以直接使用  <code>cat</code>  命令来查看这个文件的内容。</p><h3>第二步：使用 Python 和 BCC 库开发一个用户态程序</h3><p>接下来，创建一个  <code>hello.py</code>  文件，并输入下面的内容：</p><pre><code class=\"language-python\">#!/usr/bin/env python3\n# 1) import bcc library\nfrom bcc import BPF\n\n# 2) load BPF program\nb = BPF(src_file=\"hello.c\")\n# 3) attach kprobe\nb.attach_kprobe(event=\"do_sys_openat2\", fn_name=\"hello_world\")\n# 4) read and print&nbsp;/sys/kernel/debug/tracing/trace_pipe\nb.trace_print()\n</code></pre><p>让我们来看看每一处的具体含义：</p><ul>\n<li>第 1) 处导入了 BCC&nbsp;库的 BPF 模块，以便接下来调用；</li>\n<li>第 2) 处调用 BPF() 加载第一步开发的 BPF 源代码；</li>\n<li>第 3) 处将 BPF 程序挂载到内核探针（简称 kprobe），其中  <code>do_sys_openat2()</code> 是系统调用  <code>openat()</code>  在内核中的实现；</li>\n<li>第 4) 处则是读取内核调试文件  <code>/sys/kernel/debug/tracing/trace_pipe</code>  的内容，并打印到标准输出中。</li>\n</ul><p>在运行的时候，BCC 会调用 LLVM，把 BPF 源代码编译为字节码，再加载到内核中运行。</p><h3>第三步：执行 eBPF 程序</h3><p>用户态程序开发完成之后，最后一步就是执行它了。需要注意的是， eBPF 程序需要以 root 用户来运行，非 root 用户需要加上 sudo 来执行：</p><pre><code class=\"language-python\">sudo python3 hello.py\n</code></pre><p>稍等一会，你就可以看到如下的输出：</p><pre><code class=\"language-python\">b' cat-10656 [006] d... 2348.114455: bpf_trace_printk: Hello, World!'\n</code></pre><p>输出的格式可由  <code>/sys/kernel/debug/tracing/trace_options</code>&nbsp;来修改。比如前面这个默认的输出中，每个字段的含义如下所示：</p><ul>\n<li>cat-10656 表示进程的名字和 PID；</li>\n<li>[006] 表示 CPU 编号；</li>\n<li>d… 表示一系列的选项；</li>\n<li>2348.114455 表示时间戳；</li>\n<li>bpf_trace_printk 表示函数名；</li>\n<li>最后的 “Hello, World!” 就是调用  <code>bpf_trace_printk()</code>  传入的字符串。</li>\n</ul><p>到了这里，恭喜你已经成功开发并运行了第一个 eBPF 程序！不过，短暂的兴奋之后，你会发现这个程序还有不少的缺点，比如：</p><ul>\n<li>既然跟踪的是打开文件的系统调用，除了调用这个接口进程的名字之外，被打开的文件名也应该在输出中；</li>\n<li><code>bpf_trace_printk()</code>&nbsp;的输出格式不够灵活，像是 CPU 编号、bpf_trace_printk 函数名等内容没必要输出；</li>\n<li>……</li>\n</ul><p>实际上，我并不推荐通过内核调试文件系统输出日志的方式。一方面，它会带来很大的性能问题；另一方面，所有的 eBPF 程序都会把内容输出到同一个位置，很难根据 eBPF 程序去区分日志的来源。</p><p>那么，怎么来解决这些问题呢？接下来，我们就试着一起改进这个程序。</p><h2>如何改进第一个 eBPF 程序？</h2><p>在&nbsp;<a href=\"https://time.geekbang.org/column/article/479384\">01 讲</a>&nbsp;中我曾提到，BPF 程序可以利用 BPF 映射（map）进行数据存储，而用户程序也需要通过 BPF 映射，同运行在内核中的 BPF 程序进行交互。所以，为了解决上面提到的第一个问题，即获取被打开文件名的问题，我们就要引入 BPF 映射。</p><p>为了简化 BPF 映射的交互，BCC 定义了一系列的<a href=\"https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md#output\">库函数和辅助宏定义</a>。比如，你可以使用  <code>BPF_PERF_OUTPUT</code>  来定义一个 Perf 事件类型的 BPF 映射，代码如下：</p><pre><code class=\"language-c++\">// 包含头文件\n#include &lt;uapi/linux/openat2.h&gt;\n#include &lt;linux/sched.h&gt;\n\n// 定义数据结构\nstruct data_t {\n&nbsp; u32 pid;\n&nbsp; u64 ts;\n&nbsp; char comm[TASK_COMM_LEN];\n&nbsp; char fname[NAME_MAX];\n};\n\n// 定义性能事件映射\nBPF_PERF_OUTPUT(events);\n</code></pre><p>然后，在 eBPF 程序中，填充这个数据结构，并调用  <code>perf_submit()</code>  把数据提交到刚才定义的 BPF 映射中：</p><pre><code class=\"language-c++\">// 定义kprobe处理函数\nint hello_world(struct pt_regs *ctx, int dfd, const char __user * filename, struct open_how *how)\n{\n&nbsp; struct data_t data = { };\n\n  // 获取PID和时间\n&nbsp; data.pid = bpf_get_current_pid_tgid();\n&nbsp; data.ts = bpf_ktime_get_ns();\n\n  // 获取进程名\n&nbsp; if (bpf_get_current_comm(&amp;data.comm, sizeof(data.comm)) == 0)\n&nbsp; {\n&nbsp; &nbsp; bpf_probe_read(&amp;data.fname, sizeof(data.fname), (void *)filename);\n&nbsp; }\n\n  // 提交性能事件\n&nbsp; events.perf_submit(ctx, &amp;data, sizeof(data));\n&nbsp; return 0;\n}\n</code></pre><p>其中，以 bpf 开头的函数都是 eBPF 提供的辅助函数，比如：</p><ul>\n<li><code>bpf_get_current_pid_tgid</code>  用于获取进程的 TGID 和 PID。因为这儿定义的 data.pid 数据类型为 u32，所以高 32 位舍弃掉后就是进程的 PID；</li>\n<li><code>bpf_ktime_get_ns</code>  用于获取系统自启动以来的时间，单位是纳秒；</li>\n<li><code>bpf_get_current_comm</code>  用于获取进程名，并把进程名复制到预定义的缓冲区中；</li>\n<li><code>bpf_probe_read</code>  用于从指定指针处读取固定大小的数据，这里则用于读取进程打开的文件名。</li>\n</ul><p>有了 BPF 映射之后，前面我们调用的&nbsp;<code>bpf_trace_printk()</code>&nbsp;其实就不再需要了，因为用户态进程可以直接从 BPF 映射中读取内核 eBPF 程序的运行状态。</p><p>这其实也就是上面提到的第二个待解决问题。那么，怎样从用户态读取 BPF 映射内容并输出到标准输出（stdout）呢？</p><p>在 BCC 中，与 eBPF 程序中  <code>BPF_PERF_OUTPUT</code>&nbsp;相对应的用户态辅助函数是  <code>open_perf_buffer()</code> 。它需要传入一个回调函数，用于处理从 Perf 事件类型的 BPF 映射中读取到的数据。具体的使用方法如下所示：</p><pre><code class=\"language-python\">from bcc import BPF\n\n# 1) load BPF program\nb = BPF(src_file=\"trace-open.c\")\nb.attach_kprobe(event=\"do_sys_openat2\", fn_name=\"hello_world\")\n\n# 2) print header\nprint(\"%-18s %-16s %-6s %-16s\" % (\"TIME(s)\", \"COMM\", \"PID\", \"FILE\"))\n\n# 3) define the callback for perf event\nstart = 0\ndef print_event(cpu, data, size):\n&nbsp; &nbsp; global start\n&nbsp; &nbsp; event = b[\"events\"].event(data)\n&nbsp; &nbsp; if start == 0:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; start = event.ts\n&nbsp; &nbsp; time_s = (float(event.ts - start)) / 1000000000\n&nbsp; &nbsp; print(\"%-18.9f %-16s %-6d %-16s\" % (time_s, event.comm, event.pid, event.fname))\n\n# 4) loop with callback to print_event\nb[\"events\"].open_perf_buffer(print_event)\nwhile 1:\n&nbsp; &nbsp; try:\n&nbsp; &nbsp; &nbsp; &nbsp; b.perf_buffer_poll()\n&nbsp; &nbsp; except KeyboardInterrupt:\n&nbsp; &nbsp; &nbsp; &nbsp; exit()\n</code></pre><p>让我们来看看每一处的具体含义：</p><ul>\n<li>第 1) 处跟前面的 Hello World 一样，加载 eBPF 程序并挂载到内核探针上；</li>\n<li>第 2) 处则是输出一行 Header 字符串表示数据的格式；</li>\n<li>第 3) 处的  <code>print_event</code>  定义一个数据处理的回调函数，打印进程的名字、PID 以及它调用 openat 时打开的文件；</li>\n<li>第 4) 处的  <code>open_perf_buffer</code>  定义了名为 “events” 的 Perf 事件映射，而后通过一个循环调用  <code>perf_buffer_poll</code>  读取映射的内容，并执行回调函数输出进程信息。</li>\n</ul><p>将前面的 eBPF 程序保存到  <code>trace-open.c</code> ，然后再把上述的 Python 程序保存到  <code>trace-open.py</code> 之后（你可以在 GitHub <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bcc-apps/python/trace_open.py\">ebpf-apps</a> 上找到完整的代码），就能以 root 用户来运行了：</p><pre><code class=\"language-python\">sudo python3 trace-open.py\n</code></pre><p>稍等一会，你会看到类似下面的输出：</p><pre><code class=\"language-python\">TIME(s)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; COMM&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PID&nbsp; &nbsp; FILE\n2.384485400&nbsp; &nbsp; &nbsp; &nbsp; b'irqbalance'&nbsp; &nbsp; 991&nbsp; &nbsp; b'/proc/interrupts'\n2.384750400&nbsp; &nbsp; &nbsp; &nbsp; b'irqbalance'&nbsp; &nbsp; 991&nbsp; &nbsp; b'/proc/stat'\n2.384838400&nbsp; &nbsp; &nbsp; &nbsp; b'irqbalance'&nbsp; &nbsp; 991&nbsp; &nbsp; b'/proc/irq/0/smp_affinity'\n</code></pre><p>恭喜，你已经开发了第一个完整的 eBPF 程序。相对于前面的 Hello World，它的输出不仅格式更为清晰，还把进程打开的文件名输出出来了，这在排查频繁打开文件相关的性能问题时尤其有用。</p><h2>小结</h2><p>今天，我带你一起搭建了 eBPF 的开发环境，安装了 eBPF 开发时常用的工具和依赖库。并且，我从最简单的 Hello World 开始，带你借助 BCC 从零开发了一个跟踪 <a href=\"https://man7.org/linux/man-pages/man2/open.2.html\">openat()</a>&nbsp;系统调用的 eBPF 程序。</p><p>通常，开发一个 eBPF 程序需要经过开发 C 语言 eBPF 程序、编译为 BPF 字节码、加载 BPF 字节码到内核、内核验证并运行 BPF 字节码，以及用户程序读取 BPF 映射五个步骤。使用 BCC 的好处是，它把这几个步骤通过内置框架抽象了起来，并提供了简单易用的 Python 接口，这可以帮你大大简化 eBPF 程序的开发。</p><p>除此之外，BCC 提供的一系列工具不仅可以直接用在生产环境中，还是你学习和开发新的 eBPF 程序的最佳参考示例。在课程后续的内容中，我还会带你深入 BCC 的详细使用方法。</p><h2>思考题</h2><p>最后，我想请你聊一聊这几个问题：</p><ol>\n<li>你通常都是如何搭建 Linux 和 eBPF 环境的？</li>\n<li>在今天的案例操作中，你遇到了什么问题，又是如何解决的呢？</li>\n<li>虽然今天开发的程序非常短小，你觉得它能否在日常的工作中帮助到你呢？</li>\n</ol><p>欢迎在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"02 | 先利其器：如何高效学习 eBPF？","id":480094},"right":{"article_title":"04 | 运行原理：eBPF 是一个新的虚拟机吗？","id":481889}}},{"article_id":481889,"article_title":"04 | 运行原理：eBPF 是一个新的虚拟机吗？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你一起搭建了 eBPF 的开发环境，并从最简单的 Hello World 开始，带你借助 BCC 库从零开发了一个跟踪 <a href=\"https://man7.org/linux/man-pages/man2/open.2.html\">openat()</a>&nbsp;系统调用的 eBPF 程序。</p><p>不过，虽然第一个 eBPF 程序已经成功运行起来了，你很可能还在想：这个 eBPF 程序到底是如何编译成内核可识别的格式的？又是如何在内核中运行起来的？还有，既然允许普通用户去修改内核的行为，它又是如何确保内核安全的呢？</p><p>今天，我就带你一起深入看看 eBPF 虚拟机的原理，以及&nbsp;eBPF 程序是如何执行的。</p><h2>eBPF 虚拟机是如何工作的？</h2><p>eBPF 是一个运行在内核中的虚拟机，很多人在初次接触它时，会把它跟系统虚拟化（比如kvm）中的虚拟机弄混。其实，虽然都被称为“虚拟机”，系统虚拟化和 eBPF 虚拟机还是有着本质不同的。</p><p>系统虚拟化基于 x86 或 arm64 等通用指令集，这些指令集足以完成完整计算机的所有功能。而为了确保在内核中安全地执行，eBPF 只提供了非常有限的指令集。这些指令集可用于完成一部分内核的功能，但却远不足以模拟完整的计算机。为了更高效地与内核进行交互，eBPF 指令还有意采用了 C 调用约定，其提供的辅助函数可以在 C 语言中直接调用，极大地方便了 eBPF 程序的开发。</p><!-- [[[read_end]]] --><p>如下图（图片来自 <a href=\"https://www.usenix.org/conference/lisa21/presentation/gregg-bpf\">BPF Internals</a>）所示，eBPF 在内核中的运行时主要由&nbsp;5&nbsp;个模块组成：</p><p><img src=\"https://static001.geekbang.org/resource/image/45/d2/453f8d99cea1b35da8f6c57e552yy3d2.png?wh=915x503\" alt=\"图片\" title=\"eBPF 运行时\"></p><ul>\n<li>第一个模块是&nbsp;<strong>eBPF 辅助函数</strong>。它提供了一系列用于 eBPF 程序与内核其他模块进行交互的函数。这些函数并不是任意一个 eBPF 程序都可以调用的，具体可用的函数集由 BPF 程序类型决定。关于 BPF 程序类型，我会在 06 讲 中进行讲解。</li>\n<li>第二个模块是&nbsp;<strong>eBPF 验证器</strong>。它用于确保 eBPF 程序的安全。验证器会将待执行的指令创建为一个有向无环图（DAG），确保程序中不包含不可达指令；接着再模拟指令的执行过程，确保不会执行无效指令。</li>\n<li>第三个模块是由&nbsp;<strong>11 个 64 位寄存器、一个程序计数器和一个 512 字节的栈组成的存储模块</strong>。这个模块用于控制 eBPF 程序的执行。其中，R0 寄存器用于存储函数调用和 eBPF 程序的返回值，这意味着函数调用最多只能有一个返回值；R1-R5 寄存器用于函数调用的参数，因此函数调用的参数最多不能超过 5 个；而 R10 则是一个只读寄存器，用于从栈中读取数据。</li>\n<li>第四个模块是<strong>即时编译器</strong>，它将 eBPF 字节码编译成本地机器指令，以便更高效地在内核中执行。</li>\n<li>第五个模块是&nbsp;<strong>BPF 映射（map）</strong>，它用于提供大块的存储。这些存储可被用户空间程序用来进行访问，进而控制 eBPF 程序的运行状态。</li>\n</ul><p>关于 BPF 辅助函数和 BPF 映射的具体内容，我在后面的课程中还会为你详细介绍。接下来，我们先来看看 BPF 指令的具体格式，以及它是如何加载到内核中，又是何时运行的。</p><h2>BPF 指令是什么样的？</h2><p>只看图中的这些模块，你可能觉得它们并不是太直观。所以接下来，我们还是用上一讲的 Hello World 作为例子，一起看下 BPF 指令到底是什么样子的。</p><p>首先，回顾一下上一讲的  eBPF 程序&nbsp;Hello World&nbsp;的源代码。它的逻辑其实很简单，先调用&nbsp;  <code>bpf_trace_printk</code>  输出一个 “Hello, World!” 字符串，然后就返回成功了：</p><pre><code class=\"language-c++\">int hello_world(void *ctx)\n{\n&nbsp; bpf_trace_printk(\"Hello, World!\");\n&nbsp; return 0;\n}\n</code></pre><p>然后，我们通过 BCC 的 Python 库，加载并运行了这个 eBPF 程序：</p><pre><code class=\"language-python\">#!/usr/bin/env python3\n# This is a Hello World example of BPF.\nfrom bcc import BPF\n\n# load BPF program\nb = BPF(src_file=\"hello.c\")\nb.attach_kprobe(event=\"do_sys_openat2\", fn_name=\"hello_world\")\nb.trace_print()\n</code></pre><p>在终端中运行下面的命令，就可以启动这个 eBPF 程序（注意， BCC 帮你完成了编译和加载的过程）：</p><pre><code class=\"language-python\">sudo python3 hello.py\n</code></pre><p><strong>接下来，我为你介绍一个新的工具 bpftool，<strong><strong>用它可以</strong></strong>查看 eBPF 程序的运行状态。</strong></p><p>首先，打开一个新的终端，执行下面的命令，查询系统中正在运行的 eBPF 程序：</p><pre><code class=\"language-bash\"># sudo bpftool prog list\n89: kprobe  name hello_world  tag 38dd440716c4900f  gpl\n      loaded_at 2021-11-27T13:20:45+0000  uid 0\n      xlated 104B  jited 70B  memlock 4096B\n      btf_id 131\n      pids python3(152027)\n</code></pre><p>输出中，89 是这个 eBPF 程序的编号，kprobe 是程序的类型，而 hello_world 是程序的名字。</p><p>有了 eBPF 程序编号之后，执行下面的命令就可以导出这个 eBPF 程序的指令（注意把 89 替换成你查询到的编号）：</p><pre><code class=\"language-bash\">sudo bpftool prog dump xlated id 89\n</code></pre><p>你会看到如下所示的输出：</p><pre><code class=\"language-bash\">int hello_world(void * ctx):\n; int hello_world(void *ctx)\n&nbsp; &nbsp;0: (b7) r1 = 33&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /* ! */\n; ({ char _fmt[] = \"Hello, World!\"; bpf_trace_printk_(_fmt, sizeof(_fmt)); });\n&nbsp; &nbsp;1: (6b) *(u16 *)(r10 -4) = r1\n&nbsp; &nbsp;2: (b7) r1 = 1684828783&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /*&nbsp;dlro */\n&nbsp; &nbsp;3: (63) *(u32 *)(r10 -8) = r1\n&nbsp; &nbsp;4: (18) r1 = 0x57202c6f6c6c6548&nbsp; /*&nbsp;W ,olleH */\n&nbsp; &nbsp;6: (7b) *(u64 *)(r10 -16) = r1\n&nbsp; &nbsp;7: (bf) r1 = r10\n;\n&nbsp; &nbsp;8: (07) r1 += -16\n; ({ char _fmt[] = \"Hello, World!\"; bpf_trace_printk_(_fmt, sizeof(_fmt)); });\n&nbsp; &nbsp;9: (b7) r2 = 14\n&nbsp; 10: (85) call bpf_trace_printk#-61616\n; return 0;\n&nbsp; 11: (b7) r0 = 0\n&nbsp; 12: (95) exit\n</code></pre><p>其中，分号开头的部分，正是我们前面写的 C 代码，而其他行则是具体的 BPF 指令。具体每一行的 BPF 指令又分为三部分：</p><ul>\n<li>第一部分，冒号前面的数字 0-12 ，代表 BPF 指令行数；</li>\n<li>第二部分，括号中的16进制数值，表示 BPF 指令码。它的具体含义你可以参考 <a href=\"https://github.com/iovisor/bpf-docs/blob/master/eBPF.md\">IOVisor BPF 文档</a>，比如第 0 行的 0xb7 表示为 64 位寄存器赋值。</li>\n<li>第三部分，括号后面的部分，就是 BPF 指令的伪代码。</li>\n</ul><p>结合前面讲述的各个寄存器的作用，不难理解这些 BPF 指令的含义：</p><ul>\n<li>第0-8行，借助 R10 寄存器从栈中把字符串 “Hello, World!” 读出来，并放入 R1 寄存器中；</li>\n<li>第9行，向 R2 寄存器写入字符串的长度 14（即代码注释里面的  <code>sizeof(_fmt)</code> ）；</li>\n<li>第10行，调用 BPF 辅助函数  <code>bpf_trace_printk</code>  输出字符串；</li>\n<li>第11行，向 R0 寄存器写入0，表示程序的返回值是0；</li>\n<li>最后一行，程序执行成功退出。</li>\n</ul><p>总结起来，<strong>这些指令先通过 R1 和 R2 寄存器设置了</strong> <code>bpf_trace_printk</code> <strong>的参数，然后调用</strong> <code>bpf_trace_printk</code> <strong>函数输出字符串，最后再通过 R0 寄存器返回成功。</strong></p><p>实际上，你也可以通过类似的 <a href=\"https://man7.org/linux/man-pages/man2/bpf.2.html#EXAMPLES\">BPF 指令</a>来开发 eBPF 程序（具体指令的定义，请参考 <a href=\"https://elixir.bootlin.com/linux/v5.4/source/include/uapi/linux/bpf_common.h\">include/uapi/linux/bpf_common.h</a> 以及 <a href=\"https://elixir.bootlin.com/linux/v5.4/source/include/uapi/linux/bpf.h\">include/uapi/linux/bpf.h</a>），不过通常并不推荐你这么做。跟一开始的 C 程序相比，你会发现 BPF 指令的可读性和可维护性明显要差得多。所以，我建议你还是使用 C 语言来开发 eBPF 程序，而只把&nbsp;BPF 指令作为排查 eBPF 程序疑难杂症时的参考。</p><p>这里，我来简单讲讲&nbsp;BPF 指令加载后是如何运行的。当这些 BPF 指令加载到内核后， BPF 即时编译器会将其编译成本地机器指令，最后才会执行编译后的机器指令：</p><pre><code class=\"language-bash\"># bpftool prog dump jited id 89\nint hello_world(void * ctx):\nbpf_prog_38dd440716c4900f_hello_world:\n; int hello_world(void *ctx)\n&nbsp; &nbsp;0:\tnopl&nbsp; &nbsp;0x0(%rax,%rax,1)\n&nbsp; &nbsp;5:\txchg&nbsp; &nbsp;%ax,%ax\n&nbsp; &nbsp;7:\tpush&nbsp; &nbsp;%rbp\n&nbsp; &nbsp;8:\tmov&nbsp; &nbsp; %rsp,%rbp\n&nbsp; &nbsp;b:\tsub&nbsp; &nbsp; $0x10,%rsp\n&nbsp; 12:\tmov&nbsp; &nbsp; $0x21,%edi\n; ({ char _fmt[] = \"Hello, World!\"; bpf_trace_printk_(_fmt, sizeof(_fmt)); });\n&nbsp; 17:\tmov&nbsp; &nbsp; %di,-0x4(%rbp)\n&nbsp; 1b:\tmov&nbsp; &nbsp; $0x646c726f,%edi\n&nbsp; 20:\tmov&nbsp; &nbsp; %edi,-0x8(%rbp)\n&nbsp; 23:\tmovabs $0x57202c6f6c6c6548,%rdi\n&nbsp; 2d:\tmov&nbsp; &nbsp; %rdi,-0x10(%rbp)\n&nbsp; 31:\tmov&nbsp; &nbsp; %rbp,%rdi\n;\n&nbsp; 34:\tadd&nbsp; &nbsp; $0xfffffffffffffff0,%rdi\n; ({ char _fmt[] = \"Hello, World!\"; bpf_trace_printk_(_fmt, sizeof(_fmt)); });\n&nbsp; 38:\tmov&nbsp; &nbsp; $0xe,%esi\n&nbsp; 3d:\tcall&nbsp; &nbsp;0xffffffffd8c7e834\n; return 0;\n&nbsp; 42:\txor&nbsp; &nbsp; %eax,%eax\n&nbsp; 44:\tleave\n&nbsp; 45:\tret\n</code></pre><p>这些机器指令的含义跟前面的 BPF 指令是类似的，但具体的指令和寄存器都换成了 x86 的格式。你不需要掌握这些机器指令的具体含义，只要知道查询的具体方法就足够了。这是因为，就像你曾接触过的其他高级语言一样，在实际的 eBPF 使用过程中，并不需要直接使用机器指令，而是 eBPF 虚拟机帮你自动完成了转换。</p><h2>eBPF 程序是什么时候执行的？</h2><p>到这里，我想你已经理解了 BPF 指令的具体格式，以及它与  C 源代码之间的对应关系。不过，这个 eBPF 程序到底是什么时候执行的呢？接下来，我们再一起看看 BPF 指令的加载和执行过程。</p><p>在上一讲中我提到，BCC 负责了 eBPF 程序的编译和加载过程。因而，要了解 BPF 指令的加载过程，就可以从 BCC 执行 eBPF 程序的过程入手。</p><p>那么，怎么才能查看到 BCC 的执行过程呢？我想，你一定想到了，那就是跟踪它的系统调用过程。</p><p>首先，我们打开一个终端，执行下面的命令：</p><pre><code class=\"language-bash\"># -ebpf表示只跟踪bpf系统调用\nsudo strace -v -f -ebpf ./hello.py\n</code></pre><p>稍等一会，你会看到如下的输出：</p><pre><code class=\"language-bash\">bpf(BPF_PROG_LOAD,\n    {\n        prog_type=BPF_PROG_TYPE_KPROBE,\n        insn_cnt=13,\n        insns=[\n            {code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0x21},\n            {code=BPF_STX|BPF_H|BPF_MEM, dst_reg=BPF_REG_10, src_reg=BPF_REG_1, off=-4, imm=0},\n            {code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0x646c726f},\n            {code=BPF_STX|BPF_W|BPF_MEM, dst_reg=BPF_REG_10, src_reg=BPF_REG_1, off=-8, imm=0},\n            {code=BPF_LD|BPF_DW|BPF_IMM, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0x6c6c6548},\n            {code=BPF_LD|BPF_W|BPF_IMM, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0x57202c6f},\n            {code=BPF_STX|BPF_DW|BPF_MEM, dst_reg=BPF_REG_10, src_reg=BPF_REG_1, off=-16, imm=0},\n            {code=BPF_ALU64|BPF_X|BPF_MOV, dst_reg=BPF_REG_1, src_reg=BPF_REG_10, off=0, imm=0},\n            {code=BPF_ALU64|BPF_K|BPF_ADD, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0xfffffff0},\n            {code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_2, src_reg=BPF_REG_0, off=0, imm=0xe},\n            {code=BPF_JMP|BPF_K|BPF_CALL, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0x6},\n            {code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0},\n            {code=BPF_JMP|BPF_K|BPF_EXIT, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0}\n        ],\n        prog_name=\"hello_world\",\n        ...\n    },\n    128) = 4\n</code></pre><p>这些参数看起来很复杂，但实际上，如果你查询  <code>bpf</code> 系统调用的格式（执行  <code>man bpf</code> 命令），就可以发现，它实际上只需要三个参数：</p><pre><code class=\"language-bash\">int bpf(int cmd, union bpf_attr *attr, unsigned int size);\n</code></pre><p>对应前面的 strace 输出结果，这三个参数的具体含义如下。</p><ul>\n<li>第一个参数是  <code>BPF_PROG_LOAD</code> ， 表示加载 BPF 程序。</li>\n<li>第二个参数是  <code>bpf_attr</code>  类型的结构体，表示 BPF 程序的属性。其中，有几个需要你留意的参数，比如：\n<ul>\n<li><code>prog_type</code>  表示 BPF 程序的类型，这儿是  <code>BPF_PROG_TYPE_KPROBE</code> ，跟我们Python 代码中的  <code>attach_kprobe</code>  一致；</li>\n<li><code>insn_cnt</code>  (instructions count) 表示指令条数；</li>\n<li><code>insns</code>  (instructions) 包含了具体的每一条指令，这儿的 13 条指令跟我们前面  <code>bpftool prog dump</code>  的结果是一致的（具体的指令格式，你可以参考内核中 <a href=\"https://elixir.bootlin.com/linux/v5.4/source/include/uapi/linux/bpf.h#L65\">bpf_insn</a> 的定义）；</li>\n<li><code>prog_name</code>  则表示 BPF 程序的名字，即  <code>hello_world</code> 。</li>\n</ul>\n</li>\n<li>第三个参数 128 表示属性的大小。</li>\n</ul><p>到这里，我们已经了解了 bpf 系统调用的基本格式。对于  <code>bpf</code>  系统调用在内核中的实现原理，你并不需要详细了解。我们只要知道它的具体功能，就可以掌握 eBPF 的核心原理了。当然，如果你对它的实现方法有兴趣的话，可以参考内核源码 kernel/bpf/syscall.c 中 <a href=\"https://elixir.bootlin.com/linux/v5.4/source/kernel/bpf/syscall.c#L2837\">SYSCALL_DEFINE3</a> 的实现。</p><p>BPF 程序加载到内核后，并不会立刻执行，那么它什么时候才会执行呢？这里，回想一下我在 <a href=\"https://time.geekbang.org/column/article/479384\">01 讲</a> 中提到的 eBPF 的基本原理：</p><blockquote>\n<p>eBPF 程序并不像常规的线程那样，启动后就一直运行在那里，它需要事件触发后才会执行。这些事件包括系统调用、内核跟踪点、内核函数和用户态函数的调用退出、网络事件，等等。</p>\n</blockquote><p>对于我们的 Hello World 来说，由于调用了  <code>attach_kprobe</code>  函数，很明显，这是一个内核跟踪事件：</p><pre><code class=\"language-bash\">b.attach_kprobe(event=\"do_sys_openat2\", fn_name=\"hello_world\")\n</code></pre><p>所以，除了把 eBPF 程序加载到内核之外，还需要把加载后的程序跟具体的内核函数调用事件进行绑定。在 eBPF 的实现中，诸如内核跟踪（kprobe）、用户跟踪（uprobe）等的事件绑定，都是通过  <code>perf_event_open()</code>  来完成的。</p><p>为什么这么说呢？我们再用  <code>strace</code>  来确认一下。把前面  <code>strace</code>  命令中的  <code>-ebpf</code>  参数去掉，重新执行：</p><pre><code class=\"language-bash\">sudo strace -v -f ./hello.py\n</code></pre><p>忽略无关的输出后，你会发现如下的系统调用：</p><pre><code class=\"language-c++\">...\n/* 1) 加载BPF程序 */\nbpf(BPF_PROG_LOAD,...) = 4\n...\n\n/* 2）查询事件类型 */\nopenat(AT_FDCWD, \"/sys/bus/event_source/devices/kprobe/type\", O_RDONLY) = 5\nread(5, \"6\\n\", 4096)                    = 2\nclose(5)                                = 0\n...\n\n/* 3）创建性能监控事件 */\nperf_event_open(\n    {\n        type=0x6 /* PERF_TYPE_??? */,\n        size=PERF_ATTR_SIZE_VER7,\n        ...\n        wakeup_events=1,\n        config1=0x7f275d195c50,\n        ...\n    },\n    -1,\n    0,\n    -1,\n    PERF_FLAG_FD_CLOEXEC) = 5\n\n/* 4）绑定BPF到kprobe事件 */\nioctl(5, PERF_EVENT_IOC_SET_BPF, 4)     = 0\n...\n</code></pre><p>从输出中，你可以看出 BPF 与性能事件的绑定过程分为以下几步：</p><ul>\n<li>首先，借助 bpf 系统调用，加载 BPF 程序，并记住返回的文件描述符；</li>\n<li>然后，查询 kprobe 类型的事件编号。BCC 实际上是通过  <code>/sys/bus/event_source/devices/kprobe/type</code> 来查询的；</li>\n<li>接着，调用  <code>perf_event_open</code>  创建性能监控事件。比如，事件类型（type 是上一步查询到的 6）、事件的参数（ <code>config1 包含了内核函数 do_sys_openat2</code> ）等；</li>\n<li>最后，再通过  <code>ioctl</code>  的  <code>PERF_EVENT_IOC_SET_BPF</code>  命令，将 BPF 程序绑定到性能监控事件。</li>\n</ul><p>对于绑定性能监控（perf event）的内核实现原理，你也不需要详细了解，只需要知道它的具体功能，就足够我们掌握 eBPF 了。如果你对它的实现方法有兴趣的话，可以参考内核源码 <a href=\"https://elixir.bootlin.com/linux/v5.4/source/kernel/events/core.c#L9039\">perf_event_set_bpf_prog</a> 的实现；而最终性能监控调用 BPF 程序的实现，则可以参考内核源码 <a href=\"https://elixir.bootlin.com/linux/v5.4/source/kernel/trace/trace_kprobe.c#L1351\">kprobe_perf_func</a> 的实现。</p><h2>小结</h2><p>今天，我带你一起梳理了 eBPF 在内核中的实现原理，并以上一讲的 Hello World 程序为例，借助 bpftool、strace 等工具，带你观察了 BPF 指令的具体格式。</p><p>然后，我们从 BCC 执行 eBPF 程序的过程入手，一起看了BPF 指令的加载和执行过程。用高级语言开发的 eBPF 程序，需要首先编译为 BPF 字节码（即 BPF 指令），然后借助  <code>bpf</code>  系统调用加载到内核中，最后再通过性能监控等接口，与具体的内核事件进行绑定。这样，内核的性能监控模块才会在内核事件发生时，自动执行我们开发的 eBPF 程序。</p><h2>思考题</h2><p>最后，我想邀请你来聊一聊这两个问题。</p><ol>\n<li>你通常是如何快速理解一门新技术的运行原理的？</li>\n<li>在今天的内容中，我使用 strace 跟踪 BCC 程序，进而找到了相关的系统调用。那么，有没有可能直接使用 BCC 来跟踪  <code>bpf</code> 系统调用呢？如果你的答案是肯定的，可以试着把它开发出来，并在评论区分享你的实践经验。</li>\n</ol><p>欢迎在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"03 | 初窥门径：开发并运行你的第一个 eBPF 程序","id":481090},"right":{"article_title":"05 | 编程接口：eBPF 程序是怎么跟内核进行交互的？","id":482459}}},{"article_id":482459,"article_title":"05 | 编程接口：eBPF 程序是怎么跟内核进行交互的？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你一起梳理了 eBPF 在内核中的实现原理，以及 BPF 指令的具体格式。</p><p>用高级语言开发的 eBPF 程序，需要首先编译为 BPF 字节码，然后借助 bpf 系统调用加载到内核中，最后再通过性能监控等接口与具体的内核事件进行绑定。这样，内核的性能监控模块才会在内核事件发生时，自动执行我们开发的 eBPF 程序。</p><p>那么，eBPF 程序到底是如何跟内核事件进行绑定的？又该如何跟内核中的其他模块进行交互呢？今天，我就带你一起看看 eBPF 程序的编程接口。</p><h2>BPF 系统调用</h2><p>如下图（图片来自<a href=\"https://www.brendangregg.com/ebpf.html\">brendangregg.com</a>）所示，一个完整的 eBPF 程序通常包含用户态和内核态两部分。其中，用户态负责 eBPF 程序的加载、事件绑定以及 eBPF 程序运行结果的汇总输出；内核态运行在 eBPF 虚拟机中，负责定制和控制系统的运行状态。</p><p><img src=\"https://static001.geekbang.org/resource/image/a7/6a/a7165eea1fd9fc24090a3a1e8987986a.png?wh=1500x550\" alt=\"图片\"></p><p>对于用户态程序来说，我想你已经了解，<strong>它们与内核进行交互时必须要通过系统调用来完成</strong>。而对应到 eBPF 程序中，我们最常用到的就是 <a href=\"https://man7.org/linux/man-pages/man2/bpf.2.html\">bpf 系统调用</a>。</p><p>在命令行中输入 <code>man bpf</code>&nbsp;，就可以查询到 BPF 系统调用的调用格式：</p><pre><code class=\"language-plain\">#include &lt;linux/bpf.h&gt;\n\nint bpf(int cmd, union bpf_attr *attr, unsigned int size);\n</code></pre><!-- [[[read_end]]] --><p>BPF 系统调用接受三个参数：</p><ul>\n<li>第一个，cmd ，代表操作命令，比如上一讲中我们看到的 BPF_PROG_LOAD 就是加载 eBPF 程序；</li>\n<li>第二个，attr，代表 bpf_attr 类型的 eBPF 属性指针，不同类型的操作命令需要传入不同的属性参数；</li>\n<li>第三个，size ，代表属性的大小。</li>\n</ul><p>注意，<strong>不同版本的内核所支持的 BPF 命令是不同的</strong>，具体支持的命令列表可以参考内核头文件 include/uapi/linux/bpf.h 中  <code>bpf_cmd</code> 的定义。比如，v5.13 内核已经支持 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L828\">36 个 BPF 命令</a>：</p><pre><code class=\"language-plain\">enum bpf_cmd {\n&nbsp; BPF_MAP_CREATE,\n&nbsp; BPF_MAP_LOOKUP_ELEM,\n&nbsp; BPF_MAP_UPDATE_ELEM,\n&nbsp; BPF_MAP_DELETE_ELEM,\n&nbsp; BPF_MAP_GET_NEXT_KEY,\n&nbsp; BPF_PROG_LOAD,\n&nbsp; BPF_OBJ_PIN,\n&nbsp; BPF_OBJ_GET,\n&nbsp; BPF_PROG_ATTACH,\n&nbsp; BPF_PROG_DETACH,\n&nbsp; BPF_PROG_TEST_RUN,\n&nbsp; BPF_PROG_GET_NEXT_ID,\n&nbsp; BPF_MAP_GET_NEXT_ID,\n&nbsp; BPF_PROG_GET_FD_BY_ID,\n&nbsp; BPF_MAP_GET_FD_BY_ID,\n&nbsp; BPF_OBJ_GET_INFO_BY_FD,\n&nbsp; BPF_PROG_QUERY,\n&nbsp; BPF_RAW_TRACEPOINT_OPEN,\n&nbsp; BPF_BTF_LOAD,\n&nbsp; BPF_BTF_GET_FD_BY_ID,\n&nbsp; BPF_TASK_FD_QUERY,\n&nbsp; BPF_MAP_LOOKUP_AND_DELETE_ELEM,\n&nbsp; BPF_MAP_FREEZE,\n&nbsp; BPF_BTF_GET_NEXT_ID,\n&nbsp; BPF_MAP_LOOKUP_BATCH,\n&nbsp; BPF_MAP_LOOKUP_AND_DELETE_BATCH,\n&nbsp; BPF_MAP_UPDATE_BATCH,\n&nbsp; BPF_MAP_DELETE_BATCH,\n&nbsp; BPF_LINK_CREATE,\n&nbsp; BPF_LINK_UPDATE,\n&nbsp; BPF_LINK_GET_FD_BY_ID,\n&nbsp; BPF_LINK_GET_NEXT_ID,\n&nbsp; BPF_ENABLE_STATS,\n&nbsp; BPF_ITER_CREATE,\n&nbsp; BPF_LINK_DETACH,\n&nbsp; BPF_PROG_BIND_MAP,\n};\n</code></pre><p>为了方便你掌握，我把用户程序中常用的命令整理成了一个表格，你可以在需要时参考：<br>\n<img src=\"https://static001.geekbang.org/resource/image/7c/88/7cc33d0bdd8a3ba0dda7f533f3375b88.jpg?wh=1920x1080\" alt=\"图片\"></p><h2>BPF 辅助函数</h2><p>说完用户态程序的 bpf 系统调用格式，我们再来看看内核态的 eBPF 程序。</p><p>eBPF 程序并不能随意调用内核函数，因此，内核定义了一系列的辅助函数，用于 eBPF 程序与内核其他模块进行交互。比如，上一讲的 Hello World 示例中使用的 bpf_trace_printk() 就是最常用的一个辅助函数，用于向调试文件系统（/sys/kernel/debug/tracing/trace_pipe）写入调试信息。</p><p>这里补充一个知识点：从内核 5.13 版本开始，部分内核函数（如&nbsp;<code>tcp_slow_start()</code>、<code>tcp_reno_ssthresh()</code>&nbsp;等）也可以被 BPF 程序直接调用了，具体你可以查看<a href=\"https://lwn.net/Articles/856005\">这个链接</a>。 不过，这些函数只能在 TCP 拥塞控制算法的 BPF 程序中调用，所以本课程不会过多展开。</p><p>需要注意的是，并不是所有的辅助函数都可以在 eBPF 程序中随意使用，不同类型的 eBPF 程序所支持的辅助函数是不同的。比如，对于 Hello World 示例这类内核探针（kprobe）类型的 eBPF 程序，你可以在命令行中执行  <code>bpftool feature probe</code>&nbsp;，来查询当前系统支持的辅助函数列表：</p><pre><code class=\"language-plain\">$ bpftool feature probe\n...\neBPF helpers supported for program type kprobe:\n\t- bpf_map_lookup_elem\n\t- bpf_map_update_elem\n\t- bpf_map_delete_elem\n\t- bpf_probe_read\n\t- bpf_ktime_get_ns\n\t- bpf_get_prandom_u32\n\t- bpf_get_smp_processor_id\n\t- bpf_tail_call\n\t- bpf_get_current_pid_tgid\n\t- bpf_get_current_uid_gid\n\t- bpf_get_current_comm\n\t- bpf_perf_event_read\n\t- bpf_perf_event_output\n\t- bpf_get_stackid\n\t- bpf_get_current_task\n\t- bpf_current_task_under_cgroup\n\t- bpf_get_numa_node_id\n\t- bpf_probe_read_str\n\t- bpf_perf_event_read_value\n\t- bpf_override_return\n\t- bpf_get_stack\n\t- bpf_get_current_cgroup_id\n\t- bpf_map_push_elem\n\t- bpf_map_pop_elem\n\t- bpf_map_peek_elem\n\t- bpf_send_signal\n\t- bpf_probe_read_user\n\t- bpf_probe_read_kernel\n\t- bpf_probe_read_user_str\n\t- bpf_probe_read_kernel_str\n...\n</code></pre><p>对于这些辅助函数的详细定义，你可以在命令行中执行  <code>man bpf-helpers</code>&nbsp;，或者参考内核头文件 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L1463\">include/uapi/linux/bpf.h</a>&nbsp;，来查看它们的详细定义和使用说明。为了方便你掌握，我把常用的辅助函数整理成了一个表格，你可以在需要时参考：<br>\n<img src=\"https://static001.geekbang.org/resource/image/0b/cb/0b3edac18276a1236dde7135b961d8cb.jpg?wh=1920x1080\" alt=\"图片\"></p><p>这其中，需要你特别注意的是以<code>bpf_probe_read</code>  开头的一系列函数。我在上一讲中已经提到，eBPF 内部的内存空间只有寄存器和栈。所以，要访问其他的内核空间或用户空间地址，就需要借助&nbsp;<code>bpf_probe_read</code>  这一系列的辅助函数。这些函数会进行安全性检查，并禁止缺页中断的发生。</p><p>而在 eBPF 程序需要大块存储时，就不能像常规的内核代码那样去直接分配内存了，而是必须通过 BPF 映射（BPF Map）来完成。接下来，我带你看看 BPF 映射的具体原理。</p><h2>BPF 映射</h2><p>BPF 映射用于提供大块的键值存储，这些存储可被用户空间程序访问，进而获取 eBPF 程序的运行状态。eBPF 程序最多可以访问 64 个不同的 BPF 映射，并且不同的 eBPF 程序也可以通过相同的 BPF 映射来共享它们的状态。下图（图片来自<a href=\"https://docs.cilium.io/en/stable/bpf\">docs.cilium.io</a>）展示了&nbsp;BPF 映射的基本使用方法。</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/11/d87b409fa85d3a07973a8689b228cf11.png?wh=576x383\" alt=\"图片\" title=\"BPF 映射\"></p><p>在前面的 BPF 系统调用和辅助函数小节中，你也看到，有很多系统调用命令和辅助函数都是用来访问 BPF 映射的。我相信细心的你已经发现了：BPF 辅助函数中并没有 BPF 映射的创建函数，BPF 映射只能通过用户态程序的系统调用来创建。比如，你可以通过下面的示例代码来创建一个 BPF 映射，并返回映射的文件描述符：</p><pre><code class=\"language-c++\">int bpf_create_map(enum bpf_map_type map_type,\n\t\t&nbsp; &nbsp;unsigned int key_size,\n\t\t&nbsp; &nbsp;unsigned int value_size, unsigned int max_entries)\n{\n&nbsp; union bpf_attr attr = {\n&nbsp; &nbsp; .map_type = map_type,\n&nbsp; &nbsp; .key_size = key_size,\n&nbsp; &nbsp; .value_size = value_size,\n&nbsp; &nbsp; .max_entries = max_entries\n&nbsp; };\n&nbsp; return bpf(BPF_MAP_CREATE, &amp;attr, sizeof(attr));\n}\n</code></pre><p>这其中，最关键的是设置映射的类型。内核头文件<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L867\"> include/uapi/linux/bpf.h </a>中的  <code>bpf_map_type</code> 定义了所有支持的映射类型，你可以使用如下的 bpftool 命令，来查询当前系统支持哪些映射类型：</p><pre><code class=\"language-bash\">$ bpftool feature probe | grep map_type\neBPF map_type hash is available\neBPF map_type array is available\neBPF map_type prog_array is available\neBPF map_type perf_event_array is available\neBPF map_type percpu_hash is available\neBPF map_type percpu_array is available\neBPF map_type stack_trace is available\n...\n</code></pre><p>在下面的表格中，我给你整理了几种最常用的映射类型及其功能和使用场景：<br>\n<img src=\"https://static001.geekbang.org/resource/image/f3/d7/f3210199e6689e7659057a935e7fc5d7.jpg?wh=1920x1080\" alt=\"图片\"><br>\n如果你的 eBPF 程序使用了 BCC 库，你还可以使用预定义的宏来简化 BPF 映射的创建过程。比如，对哈希表映射来说，BCC 定义了  <code>BPF_HASH(name, key_type=u64, leaf_type=u64, size=10240)</code>，因此，你就可以通过下面的几种方法来创建一个哈希表映射：</p><pre><code class=\"language-c++\">// 使用默认参数 key_type=u64, leaf_type=u64, size=10240\nBPF_HASH(stats);\n\n// 使用自定义key类型，保持默认 leaf_type=u64, size=10240\nstruct key_t {\n&nbsp; char c[80];\n};\nBPF_HASH(counts, struct key_t);\n\n// 自定义所有参数\nBPF_HASH(cpu_time, uint64_t, uint64_t, 4096);\n</code></pre><p>除了创建之外，映射的删除也需要你特别注意。BPF 系统调用中并没有删除映射的命令，这是因为 <strong>BPF 映射会在用户态程序关闭文件描述符的时候自动删除</strong>（即<code>close(fd)</code> ）。 如果你想在程序退出后还保留映射，就需要调用  <code>BPF_OBJ_PIN</code> 命令，将映射挂载到 /sys/fs/bpf 中。</p><p>在调试 BPF 映射相关的问题时，你还可以通过 bpftool 来查看或操作映射的具体内容。比如，你可以通过下面这些命令创建、更新、输出以及删除映射：</p><pre><code class=\"language-c++\">//创建一个哈希表映射，并挂载到/sys/fs/bpf/stats_map(Key和Value的大小都是2字节)\nbpftool map create /sys/fs/bpf/stats_map type hash key 2 value 2 entries 8 name stats_map\n\n//查询系统中的所有映射\nbpftool map\n//示例输出\n//340: hash&nbsp; name stats_map&nbsp; flags 0x0\n//&nbsp; &nbsp; &nbsp; &nbsp; key 2B&nbsp; value 2B&nbsp; max_entries 8&nbsp; memlock 4096B\n\n//向哈希表映射中插入数据\nbpftool map update name stats_map key 0xc1 0xc2 value 0xa1 0xa2\n\n//查询哈希表映射中的所有数据\n \nbpftool map dump name stats_map\n//示例输出\n//key: c1 c2&nbsp; value: a1 a2\n//Found 1 element\n\n//删除哈希表映射\nrm /sys/fs/bpf/stats_map\n</code></pre><h2><strong>BPF 类型格式 (BTF)</strong></h2><p>了解过 BPF 辅助函数和映射之后，我们再来看一个开发 eBPF 程序时最常碰到的问题：内核数据结构的定义。</p><p>在安装 BCC 工具的时候，你可能就注意到了，内核头文件 <code>linux-headers-$(uname -r)</code> 也是必须要安装的一个依赖项。这是因为 BCC 在编译 eBPF 程序时，需要从内核头文件中找到相应的内核数据结构定义。这样，你在调用 <code>bpf_probe_read</code> 时，才能从内存地址中提取到正确的数据类型。</p><p>但是，编译时依赖内核头文件也会带来很多问题。主要有这三个方面：</p><ul>\n<li>首先，在开发 eBPF 程序时，为了获得内核数据结构的定义，就需要引入一大堆的内核头文件；</li>\n<li>其次，内核头文件的路径和数据结构定义在不同内核版本中很可能不同。因此，你在升级内核版本时，就会遇到找不到头文件和数据结构定义错误的问题；</li>\n<li>最后，在很多生产环境的机器中，出于安全考虑，并不允许安装内核头文件，这时就无法得到内核数据结构的定义。<strong>在程序中重定义数据结构</strong>虽然可以暂时解决这个问题，但也很容易把使用着错误数据结构的 eBPF 程序带入新版本内核中运行。</li>\n</ul><p>那么，这么多的问题该怎么解决呢？不用担心，BPF 类型格式（BPF Type Format, BTF）的诞生正是为了解决这些问题。从内核 5.2 开始，只要开启了 <code>CONFIG_DEBUG_INFO_BTF</code>，在编译内核时，内核数据结构的定义就会自动内嵌在内核二进制文件 vmlinux 中。并且，你还可以借助下面的命令，把这些数据结构的定义导出到一个头文件中（通常命名为 <code>vmlinux.h</code>）:</p><pre><code class=\"language-plain\">bpftool btf dump file /sys/kernel/btf/vmlinux format c &gt; vmlinux.h\n</code></pre><p>如下图（图片来自<a href=\"https://www.grant.pizza/blog/vmlinux-header\">GRANT SELTZER博客</a>）所示，有了内核数据结构的定义，你在开发 eBPF 程序时只需要引入一个 <code>vmlinux.h</code> 即可，不用再引入一大堆的内核头文件了。</p><p><img src=\"https://static001.geekbang.org/resource/image/45/20/45bbf696e8620d322d857ceab3871720.jpg?wh=1920x1204\" alt=\"\" title=\"vmlinux.h的使用示意图\"></p><p>同时，借助 BTF、bpftool 等工具，我们也可以更好地了解 BPF 程序的内部信息，这也会让调试变得更加方便。比如，在查看 BPF 映射的内容时，你可以直接看到结构化的数据，而不只是十六进制数值：</p><pre><code class=\"language-c++\"># bpftool map dump id 386\n[\n&nbsp; {\n&nbsp; &nbsp; &nbsp; \"key\": 0,\n&nbsp; &nbsp; &nbsp; \"value\": {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"eth0\": {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"value\": 0,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"ifindex\": 0,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"mac\": []\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; }\n&nbsp; }\n]\n</code></pre><p>解决了内核数据结构的定义问题，接下来的问题就是，<strong>如何让 eBPF 程序在内核升级之后，不需要重新编译就可以直接运行</strong>。eBPF 的一次编译到处执行（Compile Once Run Everywhere，简称 CO-RE）项目借助了 BTF 提供的调试信息，再通过下面的两个步骤，使得 eBPF 程序可以适配不同版本的内核：</p><ul>\n<li>第一，通过对 BPF 代码中的访问偏移量进行重写，解决了不同内核版本中数据结构偏移量不同的问题；</li>\n<li>第二，在 libbpf 中预定义不同内核版本中的数据结构的修改，解决了不同内核中数据结构不兼容的问题。</li>\n</ul><p>BTF和一次编译到处执行带来了很多的好处，但你也需要注意这一点：它们都要求比较新的内核版本（&gt;=5.2），并且需要非常新的发行版（如 Ubuntu 20.10+、RHEL 8.2+ 等）才会默认打开内核配置 <code>CONFIG_DEBUG_INFO_BTF</code>。对于旧版本的内核，虽然它们不会再去内置 BTF 的支持，但开源社区正在尝试通过 <a href=\"https://github.com/aquasecurity/btfhub\">BTFHub</a> 等方法，为它们提供 BTF 调试信息。</p><h2>小结</h2><p>今天，我带你一起梳理了 eBPF 程序跟内核交互的基本方法。</p><p>一个完整的 eBPF 程序，通常包含用户态和内核态两部分：用户态程序需要通过 BPF 系统调用跟内核进行交互，进而完成 eBPF 程序加载、事件挂载以及映射创建和更新等任务；而在内核态中，eBPF 程序也不能任意调用内核函数，而是需要通过 BPF 辅助函数完成所需的任务。尤其是在访问内存地址的时候，必须要借助  <code>bpf_probe_read</code> 系列函数读取内存数据，以确保内存的安全和高效访问。</p><p>在 eBPF 程序需要大块存储时，我们还需要根据应用场景，引入特定类型的 BPF 映射，并借助它向用户空间的程序提供运行状态的数据。</p><p>这一讲的最后，我还带你一起了解了 BTF 和 CO-RE 项目，它们在提供轻量级调试信息的同时，还解决了跨内核版本的兼容性问题。很多开源社区的 eBPF 项目（如 BCC 等）也都在向 BTF 进行迁移。</p><h2>思考题</h2><p>最后，我想邀请你来聊一聊：</p><ol>\n<li>你是如何理解 BPF 系统调用和 BPF 辅助函数的？</li>\n<li>除了今天讲到的内容，bpftool 还提供了哪些有趣的功能呢？给你一个小提示：可以使用 man bpftool 查询它的使用文档。</li>\n</ol><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"04 | 运行原理：eBPF 是一个新的虚拟机吗？","id":481889},"right":{"article_title":"06 | 事件触发：各类 eBPF 程序的触发机制及其应用场景","id":483364}}},{"article_id":483364,"article_title":"06 | 事件触发：各类 eBPF 程序的触发机制及其应用场景","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你一起梳理了 eBPF 程序跟内核交互的基本方法。一个完整的 eBPF 程序通常包含用户态和内核态两部分：用户态程序通过 BPF 系统调用，完成 eBPF 程序的加载、事件挂载以及映射创建和更新，而内核态中的 eBPF 程序则需要通过 BPF 辅助函数完成所需的任务。</p><p>在上一讲中我也提到，并不是所有的辅助函数都可以在 eBPF 程序中随意使用，不同类型的 eBPF 程序所支持的辅助函数是不同的。那么，eBPF 程序都有哪些类型，而不同类型的 eBPF 程序又有哪些独特的应用场景呢？今天，我就带你一起来看看。</p><h2>eBPF 程序可以分成几类？</h2><p>eBPF 程序类型决定了一个 eBPF 程序可以挂载的事件类型和事件参数，这也就意味着，内核中不同事件会触发不同类型的 eBPF 程序。</p><p>根据内核头文件 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L908\">include/uapi/linux/bpf.h</a> 中  <code>bpf_prog_type</code> 的定义，Linux 内核 v5.13 已经支持 30 种不同类型的 eBPF 程序（注意，&nbsp;<code>BPF_PROG_TYPE_UNSPEC</code>表示未定义）：</p><pre><code class=\"language-c++\">enum bpf_prog_type {\n\tBPF_PROG_TYPE_UNSPEC, /* Reserve 0 as invalid program type */\n\tBPF_PROG_TYPE_SOCKET_FILTER,\n\tBPF_PROG_TYPE_KPROBE,\n\tBPF_PROG_TYPE_SCHED_CLS,\n\tBPF_PROG_TYPE_SCHED_ACT,\n\tBPF_PROG_TYPE_TRACEPOINT,\n\tBPF_PROG_TYPE_XDP,\n\tBPF_PROG_TYPE_PERF_EVENT,\n\tBPF_PROG_TYPE_CGROUP_SKB,\n\tBPF_PROG_TYPE_CGROUP_SOCK,\n\tBPF_PROG_TYPE_LWT_IN,\n\tBPF_PROG_TYPE_LWT_OUT,\n\tBPF_PROG_TYPE_LWT_XMIT,\n\tBPF_PROG_TYPE_SOCK_OPS,\n\tBPF_PROG_TYPE_SK_SKB,\n\tBPF_PROG_TYPE_CGROUP_DEVICE,\n\tBPF_PROG_TYPE_SK_MSG,\n\tBPF_PROG_TYPE_RAW_TRACEPOINT,\n\tBPF_PROG_TYPE_CGROUP_SOCK_ADDR,\n\tBPF_PROG_TYPE_LWT_SEG6LOCAL,\n\tBPF_PROG_TYPE_LIRC_MODE2,\n\tBPF_PROG_TYPE_SK_REUSEPORT,\n\tBPF_PROG_TYPE_FLOW_DISSECTOR,\n\tBPF_PROG_TYPE_CGROUP_SYSCTL,\n\tBPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE,\n\tBPF_PROG_TYPE_CGROUP_SOCKOPT,\n\tBPF_PROG_TYPE_TRACING,\n\tBPF_PROG_TYPE_STRUCT_OPS,\n\tBPF_PROG_TYPE_EXT,\n\tBPF_PROG_TYPE_LSM,\n\tBPF_PROG_TYPE_SK_LOOKUP,\n};\n</code></pre><!-- [[[read_end]]] --><p>对于具体的内核来说，因为不同内核的版本和编译配置选项不同，一个内核并不会支持所有的程序类型。你可以在命令行中执行下面的命令，来查询当前系统支持的程序类型：</p><pre><code class=\"language-plain\">bpftool feature probe | grep program_type\n</code></pre><p>执行后，你会得到如下的输出：</p><pre><code class=\"language-plain\">eBPF program_type socket_filter is available\neBPF program_type kprobe is available\neBPF program_type sched_cls is available\neBPF program_type sched_act is available\neBPF program_type tracepoint is available\neBPF program_type xdp is available\neBPF program_type perf_event is available\n...\neBPF program_type ext is NOT available\neBPF program_type lsm is NOT available\neBPF program_type sk_lookup is available\n</code></pre><p>在这些输出中，你可以看到当前内核支持 kprobe、xdp、perf_event 等程序类型，而不支持 ext、lsm 等程序类型。</p><p>根据具体功能和应用场景的不同，这些程序类型大致可以划分为三类：</p><ul>\n<li>第一类是跟踪，即从内核和程序的运行状态中提取跟踪信息，来了解当前系统正在发生什么。</li>\n<li>第二类是网络，即对网络数据包进行过滤和处理，以便了解和控制网络数据包的收发过程。</li>\n<li>第三类是除跟踪和网络之外的其他类型，包括安全控制、BPF 扩展等等。</li>\n</ul><p>接下来，我就带你一起分别看看，每一类 eBPF 程序都有哪些具体的类型，以及这些不同类型的程序都是由哪些事件触发执行的。</p><h2>跟踪类 eBPF 程序</h2><p>先看第一类，也就是跟踪类 eBPF 程序。</p><p><strong>跟踪类 eBPF 程序主要用于从系统中提取跟踪信息，进而为监控、排错、性能优化等提供数据支撑。</strong>比如，我们前几讲中的 Hello World 示例就是一个 <code>BPF_PROG_TYPE_KPROBE</code> 类型的跟踪程序，它的目的是跟踪内核函数是否被某个进程调用了。</p><p>为了方便你查询，我把常见的跟踪类 BPF 程序的主要功能以及使用限制整理成了一个表格，你可以在需要时参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/04/38/042fe319b51yy6bc153ce0f877f54a38.jpg?wh=1920x1098\" alt=\"图片\"></p><p>这其中，KPROBE、TRACEPOINT 以及 PERF_EVENT 都是最常用的 eBPF 程序类型，大量应用于监控跟踪、性能优化以及调试排错等场景中。我们前几讲中提到的 <a href=\"https://github.com/iovisor/bcc\">BCC</a>工具集，其中包含的绝大部分工具也都属于这个类型。</p><h2>网络类 eBPF 程序</h2><p>看完跟踪类 eBPF 程序，我们再来看看网络类 eBPF 程序。</p><p><strong>网络类 eBPF 程序主要用于对网络数据包进行过滤和处理，进而实现网络的观测、过滤、流量控制以及性能优化等各种丰富的功能。</strong>根据事件触发位置的不同，网络类 eBPF 程序又可以分为 XDP（eXpress Data Path，高速数据路径）程序、TC（Traffic Control，流量控制）程序、套接字程序以及 cgroup 程序，下面我们来分别看看。</p><h3><strong>XDP 程序</strong></h3><p>XDP 程序的类型定义为 <code>BPF_PROG_TYPE_XDP</code>，它在<strong>网络驱动程序刚刚收到数据包时</strong>触发执行。由于无需通过繁杂的内核网络协议栈，XDP 程序可用来实现高性能的网络处理方案，常用于 DDoS 防御、防火墙、4 层负载均衡等场景。</p><p>你需要注意，XDP 程序并不是绕过了内核协议栈，它只是在内核协议栈之前处理数据包，而处理过的数据包还可以正常通过内核协议栈继续处理。你可以通过下面的图片（图片来自 <a href=\"https://www.iovisor.org/technology/xdp\">iovisor.org</a>）加深对&nbsp;XDP 相对内核协议栈位置的理解：</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/31/3b77fea948d6264bfb4b4c266526dd31.png?wh=768x420\" alt=\"图片\" title=\"XDP概览\"></p><p>根据网卡和网卡驱动是否原生支持 XDP 程序，XDP 运行模式可以分为下面这三种：</p><ul>\n<li>通用模式。它不需要网卡和网卡驱动的支持，XDP 程序像常规的网络协议栈一样运行在内核中，性能相对较差，一般用于测试；</li>\n<li>原生模式。它需要网卡驱动程序的支持，XDP 程序在网卡驱动程序的早期路径运行；</li>\n<li>卸载模式。它需要网卡固件支持 XDP 卸载，XDP 程序直接运行在网卡上，而不再需要消耗主机的 CPU 资源，具有最好的性能。</li>\n</ul><p>无论哪种模式，XDP 程序在处理过网络包之后，都需要根据 eBPF 程序执行结果，决定数据包的去处。这些执行结果对应以下 5 种 XDP 程序结果码：</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/a7/a2cayy9f21129590a91ca07604b070a7.jpg?wh=1920x1237\" alt=\"图片\"></p><p>通常来说，XDP 程序通过 <code>ip link</code> 命令加载到具体的网卡上，加载格式为：</p><pre><code class=\"language-bash\"># eth1 为网卡名\n# xdpgeneric 设置运行模式为通用模式\n# xdp-example.o 为编译后的 XDP 字节码\nsudo ip link set dev eth1 xdpgeneric object xdp-example.o\n</code></pre><p>而卸载 XDP 程序也是通过 <code>ip link</code> 命令，具体参数如下：</p><pre><code class=\"language-plain\">sudo ip link set veth1 xdpgeneric off\n</code></pre><p>除了 <code>ip link</code>之外， BCC 也提供了方便的库函数，让我们可以在同一个程序中管理 XDP 程序的生命周期：</p><pre><code class=\"language-python\">from bcc import BPF\n\n# 编译XDP程序\nb = BPF(src_file=\"xdp-example.c\")\nfn = b.load_func(\"xdp-example\", BPF.XDP)\n\n# 加载XDP程序到eth0网卡\ndevice = \"eth0\"\nb.attach_xdp(device, fn, 0)\n\n# 其他处理逻辑\n...\n\n# 卸载XDP程序\nb.remove_xdp(device)\n</code></pre><h3><strong>TC 程序</strong></h3><p>TC 程序的类型定义为 <code>BPF_PROG_TYPE_SCHED_CLS</code> 和 <code>BPF_PROG_TYPE_SCHED_ACT</code>，分别作为 <a href=\"https://tldp.org/HOWTO/Traffic-Control-HOWTO/index.html\">Linux 流量控制</a> 的分类器和执行器。Linux 流量控制通过网卡队列、排队规则、分类器、过滤器以及执行器等，实现了对网络流量的整形调度和带宽控制。</p><p>下图（图片来自 <a href=\"http://linux-ip.net/articles/Traffic-Control-HOWTO/\">linux-ip.net</a>）展示了&nbsp;HTB（Hierarchical Token Bucket，层级令牌桶）流量控制的工作原理：</p><p><img src=\"https://static001.geekbang.org/resource/image/3c/69/3c445830476ed2f32d71e99309b26369.png?wh=1369x1049\" alt=\"图片\" title=\"HTB 流量控制\"></p><p>由于 Linux 流量控制并非本课程的重点，这里我就不过多展开了。如果你对它还不熟悉，可以参考&nbsp;<a href=\"https://tldp.org/HOWTO/Traffic-Control-HOWTO/index.html\">官方文档</a>&nbsp;进行学习。</p><p>得益于内核 v4.4 引入的 <a href=\"https://docs.cilium.io/en/v1.8/bpf/#tc-traffic-control\">direct-action</a> 模式，TC 程序可以直接在一个程序内完成分类和执行的动作，而无需再调用其他的 TC 排队规则和分类器，具体如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/31/d5/31ecf04f2477bd4765be9544a62deed5.jpg?wh=1920x888\" alt=\"图片\" title=\"TC eBPF 程序与网络协议栈的关系\"></p><p>同 XDP 程序相比，TC 程序可以<strong>直接获取内核解析后的网络报文数据结构</strong><code>sk_buff</code>（XDP 则是 <code>xdp_buff</code>），并且可<strong>在网卡的接收和发送两个方向上执行</strong>（XDP 则只能用于接收）。下面我们来具体看看&nbsp;TC 程序的执行位置：</p><ul>\n<li>对于接收的网络包，TC 程序在网卡接收（GRO）之后、协议栈处理（包括 IP 层处理和 iptables 等）之前执行；</li>\n<li>对于发送的网络包，TC 程序在协议栈处理（包括 IP 层处理和 iptables 等）之后、数据包发送到网卡队列（GSO）之前执行。</li>\n</ul><p>除此之外，由于 TC 运行在内核协议栈中，不需要网卡驱动程序做任何改动，因而可以挂载到任意类型的网卡设备（包括容器等使用的虚拟网卡）上。</p><p>同 XDP 程序一样，TC eBPF 程序也可以通过 Linux 命令行工具来加载到网卡上，不过相应的工具要换成 <code>tc</code>。你可以通过下面的命令，分别加载接收和发送方向的 eBPF 程序：</p><pre><code class=\"language-bash\"># 创建 clsact 类型的排队规则\nsudo tc qdisc add dev eth0 clsact\n\n# 加载接收方向的 eBPF 程序\nsudo tc filter add dev eth0 ingress bpf da obj tc-example.o sec ingress\n\n# 加载发送方向的 eBPF 程序\nsudo tc filter add dev eth0 egress bpf da obj tc-example.o sec egress\n</code></pre><h3><strong>套接字程序</strong></h3><p>套接字程序用于过滤、观测或重定向套接字网络包，具体的种类也比较丰富。根据类型的不同，套接字 eBPF 程序可以挂载到套接字（socket）、控制组（cgroup ）以及网络命名空间（netns）等各个位置。你可以根据具体的应用场景，选择一个或组合多个类型的 eBPF 程序，去控制套接字的网络包收发过程。</p><p>这里，我把常见的套接字程序类型，以及它们的应用场景和挂载方法整理成了一个表格，你可以在需要时参考：</p><p><img src=\"https://static001.geekbang.org/resource/image/0e/44/0e57bf041262114198fd29e1e5c04044.jpg?wh=1920x1566\" alt=\"图片\"></p><h3><strong>cgroup 程序</strong></h3><p>cgroup 程序用于<strong>对 cgroup 内所有进程的网络过滤、套接字选项以及转发等进行动态控制</strong>，它最典型的应用场景是对容器中运行的多个进程进行网络控制。</p><p>cgroup 程序的种类比较丰富，我也帮你整理了一个表格，方便你在需要时查询：</p><p><img src=\"https://static001.geekbang.org/resource/image/c3/52/c37b8849096311726e734e8549fd9452.jpg?wh=1920x1216\" alt=\"图片\"></p><p>这些类型的 BPF 程序都可以通过 BPF 系统调用的 <code>BPF_PROG_ATTACH</code> 命令来进行挂载，并设置<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L942\">挂载类型</a>为匹配的 <code>BPF_CGROUP_xxx</code> 类型。比如，在挂载 <code>BPF_PROG_TYPE_CGROUP_DEVICE</code> 类型的 BPF 程序时，需要设置 <code>bpf_attach_type</code> 为 <code>BPF_CGROUP_DEVICE</code>：</p><pre><code class=\"language-c++\">union bpf_attr attr = {};\nattr.target_fd = target_fd;            // cgroup文件描述符\nattr.attach_bpf_fd = prog_fd;          // BPF程序文件描述符\nattr.attach_type = BPF_CGROUP_DEVICE;  // 挂载类型为BPF_CGROUP_DEVICE\n\nif (bpf(BPF_PROG_ATTACH, &amp;attr, sizeof(attr)) &lt; 0) {\n  return -errno;\n}\n\n...\n</code></pre><p>注意，这几类网络 eBPF 程序是在不同的事件触发时执行的，因此，在实际应用中我们通常可以把多个类型的 eBPF 程序结合起来，一起使用，来实现复杂的网络控制功能。比如，最流行的 Kubernetes 网络方案 Cilium 就大量使用了 XDP、TC 和套接字 eBPF 程序，如下图（图片来自 Cilium <a href=\"https://docs.cilium.io/en/v1.11/concepts/ebpf/lifeofapacket/\">官方文档</a>，图中黄色部分即为 Cilium eBPF 程序）所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/45/13/452c809d6e3335fb933a8f991eedf113.png?wh=1454x714\" alt=\"图片\" title=\"Cilium eBPF 数据面\"></p><h2>其他类 eBPF 程序</h2><p>除了上面的跟踪和网络 eBPF 程序之外，Linux 内核还支持很多其他的类型。这些类型的 eBPF 程序虽然不太常用，但在需要的时候也可以帮你解决很多特定的问题。</p><p>我将这些无法划分到网络和跟踪的 eBPF 程序都归为其他类，并帮你整理了一个表格：</p><p><img src=\"https://static001.geekbang.org/resource/image/93/f2/93ae17801e82579e07937e5f1595a0f2.jpg?wh=1920x1332\" alt=\"图片\"></p><p>这个表格列出了一些不太常用的 eBPF 程序类型，你可以先大致浏览下，在需要的时候再去深入了解。</p><h2>小结</h2><p>今天，我带你一起梳理了 eBPF 程序的主要类型，以及不同类型 eBPF 程序的应用场景。</p><p>根据具体功能和应用场景的不同，我们可以把 eBPF 程序分为跟踪、网络和其他三类：</p><ul>\n<li>跟踪类 eBPF 程序主要用于从系统中提取跟踪信息，进而为监控、排错、性能优化等提供数据支撑；</li>\n<li>网络类 eBPF 程序主要用于对网络数据包进行过滤和处理，进而实现网络的观测、过滤、流量控制以及性能优化等；</li>\n<li>其他类则包含了跟踪和网络之外的其他&nbsp;eBPF&nbsp;程序类型，如安全控制、BPF 扩展等。</li>\n</ul><p>虽然每个 eBPF 程序都有特定的类型和触发事件，但这并不意味着它们都是完全独立的。通过 BPF 映射提供的状态共享机制，各种不同类型的 eBPF 程序完全可以相互配合，不仅可以绕过单个 eBPF 程序指令数量的限制，还可以实现更为复杂的控制逻辑。</p><h2>思考题</h2><p>最后，我想邀请你来聊一聊：</p><ol>\n<li>你是怎么理解 eBPF 程序类型的呢？</li>\n<li>如果让你来重新设计类似于 Cilium 的网络方案，你会如何选择 eBPF 程序类型呢？</li>\n</ol><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"05 | 编程接口：eBPF 程序是怎么跟内核进行交互的？","id":482459},"right":{"article_title":"07 | 内核跟踪（上）：如何查询内核中的跟踪点？","id":484207}}},{"article_id":484207,"article_title":"07 | 内核跟踪（上）：如何查询内核中的跟踪点？","article_content":"<p>你好，我是倪朋飞。</p><p>在上一个模块“基础入门篇”中，我带你搭建了 eBPF 的开发环境，并详细介绍了 eBPF 程序的工作原理、编程接口以及事件触发机制。学习完这些内容，我想你已经掌握了 eBPF 必备的基础知识，并通过简单的示例，初步体验了 eBPF 程序的开发和执行过程。</p><p>我在前面的内容中反复强调过，学习一门新技术，最快的方法就是在理解原理的同时配合大量的实践，eBPF 也不例外。所以，从这一讲起我们开始“实战进阶篇”的学习，一起进入 eBPF 的实践环节，通过真实的应用案例把 eBPF 技术用起来。</p><p>今天我们先来看看，怎样使用 eBPF 去跟踪内核的状态，特别是最简单的 bpftrace 的使用方法。在下一讲中，我还将介绍两种 eBPF 程序的进阶编程方法。</p><p>上一讲我提到过，跟踪类 eBPF 程序主要包含内核插桩（<code>BPF_PROG_TYPE_KPROBE</code>）、跟踪点（<code>BPF_PROG_TYPE_TRACEPOINT</code>）以及性能事件（<code>BPF_PROG_TYPE_PERF_EVENT</code>）等程序类型，而每类 eBPF 程序类型又可以挂载到不同的内核函数、内核跟踪点或性能事件上。当这些内核函数、内核跟踪点或性能事件被调用的时候，挂载到其上的 eBPF 程序就会自动执行。</p><!-- [[[read_end]]] --><p>那么，你可能想问了：当我不知道内核中都有哪些内核函数、内核跟踪点或性能事件的时候，可以在哪里查询到它们的列表呢？对于内核函数和内核跟踪点，在需要跟踪它们的传入参数和返回值的时候，又该如何查询这些数据结构的定义格式呢？别担心，接下来就跟我一起去探索下吧。</p><h2><strong>利用调试信息查询跟踪点</strong></h2><p>实际上，作为一个软件系统，内核也经常会发生各种各样的问题，比如安全漏洞、逻辑错误、性能差，等等。因此，内核本身的调试与跟踪一直都是内核提供的核心功能之一。</p><p>比如，为了方便调试，内核把所有函数以及非栈变量的地址都抽取到了&nbsp;<code>/proc/kallsyms</code>&nbsp;中，这样调试器就可以根据地址找出对应的函数和变量名称。很显然，具有实际含义的名称要比 16 进制的地址易读得多。对内核插桩类的 eBPF 程序来说，它们要挂载的内核函数就可以从&nbsp;<code>/proc/kallsyms</code>&nbsp;这个文件中查到。</p><p>注意，内核函数是一个非稳定 API，在新版本中可能会发生变化，并且内核函数的数量也在不断增长中。以 v5.13.0 为例，总的内核符号表数量已经超过了 16 万：</p><pre><code class=\"language-bash\">$ cat /proc/kallsyms | wc -l\n165694\n</code></pre><p>不过需要提醒你的是，这些符号表不仅包含了内核函数，还包含了非栈数据变量。而且，并不是所有的内核函数都是可跟踪的，只有显式导出的内核函数才可以被 eBPF 进行动态跟踪。因而，通常我们并不直接从内核符号表查询可跟踪点，而是使用我接下来介绍的方法。</p><p>为了方便内核开发者获取所需的跟踪点信息，内核<a href=\"https://www.kernel.org/doc/html/latest/filesystems/debugfs.html\">调试文件系统</a>还向用户空间提供了内核调试所需的基本信息，如内核符号列表、跟踪点、函数跟踪（ftrace）状态以及参数格式等。你可以在终端中执行&nbsp;<code>sudo ls /sys/kernel/debug</code>&nbsp;来查询内核调试文件系统的具体信息。比如，执行下面的命令，就可以查询&nbsp;<code>execve</code>&nbsp;系统调用的参数格式：</p><pre><code class=\"language-bash\">sudo cat /sys/kernel/debug/tracing/events/syscalls/sys_enter_execve/format\n</code></pre><p>如果你碰到了&nbsp;<code>/sys/kernel/debug</code>&nbsp;目录不存在的错误，说明你的系统没有自动挂载调试文件系统。只需要执行下面的 mount 命令就可以挂载它：</p><pre><code class=\"language-plain\">sudo mount -t debugfs debugfs /sys/kernel/debug\n</code></pre><p>注意，<strong>eBPF 程序的执行也依赖于调试文件系统</strong>。如果你的系统没有自动挂载它，那么我推荐你把它加入到系统开机启动脚本里面，这样机器重启后 eBPF 程序也可以正常运行。</p><p>有了调试文件系统，你就可以从&nbsp;<code>/sys/kernel/debug/tracing</code>&nbsp;中找到所有内核预定义的跟踪点，进而可以在需要时把 eBPF 程序挂载到对应的跟踪点。</p><p>除了内核函数和跟踪点之外，性能事件又该如何查询呢？你可以使用 Linux 性能工具&nbsp;<a href=\"https://man7.org/linux/man-pages/man1/perf.1.html\">perf</a>&nbsp;来查询性能事件的列表。如下面的命令所示，你可以不带参数查询所有的性能事件，也可以加入可选的事件类型参数进行过滤：</p><pre><code class=\"language-bash\">sudo perf list [hw|sw|cache|tracepoint|pmu|sdt|metric|metricgroup]\n</code></pre><h2><strong>利用 bpftrace 查询跟踪点</strong></h2><p>虽然你可以利用内核调试信息和 perf 工具查询内核函数、跟踪点以及性能事件的列表，但它们的位置比较分散，并且用这种方法也不容易查询内核函数的定义格式。所以，我再给你推荐一个更好用的 eBPF 工具&nbsp;<a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md\">bpftrace</a>。</p><p>bpftrace 在 eBPF 和 BCC 之上构建了一个简化的跟踪语言，通过简单的几行脚本，就可以实现复杂的跟踪功能。并且，多行的跟踪指令也可以放到脚本文件中执行（脚本后缀通常为&nbsp;<code>.bt</code>）。</p><p>如下图（图片来自 bpftrace<a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/internals_development.md\">文档</a>）所示，bpftrace 会把你开发的脚本借助 BCC 编译加载到内核中执行，再通过 BPF 映射获取执行的结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/17/fb/175853e38141433058e05770285ee5fb.png?wh=1500x1050\" alt=\"图片\" title=\"bpftrace 原理\"></p><p>因此，在编写简单的 eBPF 程序，特别是编写的 eBPF 程序用于临时的调试和排错时，你可以考虑直接使用 bpftrace ，而不需要用 C 或 Python 去开发一个复杂的程序。</p><p>那 bpftrace 该如何安装呢？对于 Ubuntu 19.04+、RHEL8+ 等系统，你可以直接运行下面的命令来安装 bpftrace：</p><pre><code class=\"language-bash\"># Ubuntu 19.04\nsudo apt-get install -y bpftrace\n\n# RHEL8/CentOS8\nsudo dnf install -y bpftrace\n</code></pre><p>而对于其他旧版本的系统或其他的发行版，你可以通过源代码编译的形式安装。至于具体的步骤，你可以参考它的<a href=\"https://github.com/iovisor/bpftrace/blob/master/INSTALL.md\">安装文档</a>，这里我就不展开讲了。</p><p>安装好 bpftrace 之后，你就可以执行&nbsp;<code>bpftrace -l</code>&nbsp;来查询内核插桩和跟踪点了。比如你可以通过以下几种方式来查询：</p><pre><code class=\"language-bash\"># 查询所有内核插桩和跟踪点\nsudo bpftrace -l\n\n# 使用通配符查询所有的系统调用跟踪点\nsudo bpftrace -l 'tracepoint:syscalls:*'\n\n# 使用通配符查询所有名字包含\"execve\"的跟踪点\nsudo bpftrace -l '*execve*'\n</code></pre><p>对于跟踪点来说，你还可以加上&nbsp;<code>-v</code>&nbsp;参数查询函数的入口参数或返回值。而由于内核函数属于不稳定的 API，在 bpftrace 中只能通过&nbsp;<code>arg0</code>、<code>arg1</code>&nbsp;这样的参数来访问，具体的参数格式还需要参考内核源代码。</p><p>比如，下面就是一个查询系统调用&nbsp;<code>execve</code>&nbsp;入口参数（对应系统调用<code>sys_enter_execve</code>）和返回值（对应系统调用<code>sys_exit_execve</code>）的示例：</p><pre><code class=\"language-bash\"># 查询execve入口参数格式\n$ sudo bpftrace -lv tracepoint:syscalls:sys_enter_execve\ntracepoint:syscalls:sys_enter_execve\n    int __syscall_nr\n    const char * filename\n    const char *const * argv\n    const char *const * envp\n\n# 查询execve返回值格式\n$ sudo bpftrace -lv tracepoint:syscalls:sys_exit_execve\ntracepoint:syscalls:sys_exit_execve\n    int __syscall_nr\n    long ret\n</code></pre><p>所以，你既可以通过内核调试信息和 perf 来查询内核函数、跟踪点以及性能事件的列表，也可以使用 bpftrace 工具来查询。</p><p>在这两种方法中，我更推荐使用更简单的 bpftrace 进行查询。这是因为，我们通常只需要在开发环境查询这些列表，以便去准备 eBPF 程序的挂载点。也就是说，虽然 bpftrace 依赖 BCC 和 LLVM 开发工具，但开发环境本来就需要这些库和开发工具。综合来看，用 bpftrace 工具来查询的方法显然更简单快捷。</p><p>到这里，我已经带你了解了内核函数、跟踪点以及性能事件的查询方法，你是不是迫不及待地想去开发一个内核跟踪的 eBPF 程序了？</p><p>别急，在开发 eBPF 程序之前，你还需要在这些长长的函数列表中进行选择，确定你应该挂载到哪一个上。那么，具体该如何选择呢？接下来，就进入我们的案例环节，一起看看内核跟踪点的具体使用方法。</p><h2><strong>如何利用内核跟踪点排查短时进程问题？</strong></h2><p>在排查系统 CPU 使用率高的问题时，我想你很可能遇到过这样的困惑：明明通过&nbsp;<code>top</code>&nbsp;命令发现系统的 CPU 使用率（特别是用户 CPU 使用率）特别高，但通过&nbsp;<code>ps</code>、<code>pidstat</code>&nbsp;等工具都找不出 CPU 使用率高的进程。这是什么原因导致的呢？你可以先停下来思考一下，再继续下面的内容。</p><p>你想到可能的原因了吗？在我看来，一般情况下，这类问题很可能是以下两个原因导致的：</p><ul>\n<li>第一，应用程序里面直接调用其他二进制程序，并且这些程序的运行时间很短，通过&nbsp;<code>top</code>&nbsp;工具不容易发现；</li>\n<li>第二，应用程序自身在不停地崩溃重启中，且重启间隔较短，启动过程中资源的初始化导致了高 CPU 使用率。</li>\n</ul><p>使用&nbsp;<code>top</code>、<code>ps</code>&nbsp;等性能工具很难发现这类短时进程，这是因为它们都只会按照给定的时间间隔采样，而不会实时采集到所有新创建的进程。那要如何才能采集到所有的短时进程呢？你肯定已经想到了，那就是<strong>利用 eBPF 的事件触发机制，跟踪内核每次新创建的进程</strong>，这样就可以揪出这些短时进程。</p><p>要跟踪内核新创建的进程，首先得找到要跟踪的内核函数或跟踪点。如果你了解过 Linux 编程中创建进程的过程，我想你已经知道了，创建一个新进程通常需要调用&nbsp;<code>fork()</code>&nbsp;和&nbsp;<code>execve()</code>&nbsp;这两个标准函数，它们的调用过程如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/1c/efda99288b5366ca24a00f374c6fba1c.jpg?wh=1920x2645\" alt=\"图片\" title=\"Linux进程创建流程\"></p><p>因为我们要关心的主要是新创建进程的基本信息，而像进程名称和参数等信息都在&nbsp;<code>execve()</code>&nbsp;的参数里，所以我们就要找出&nbsp;<code>execve()</code>&nbsp;所对应的内核函数或跟踪点。</p><p>借助刚才提到的&nbsp;<code>bpftrace</code>&nbsp;工具，你可以执行下面的命令，查询所有包含&nbsp;<code>execve</code>&nbsp;关键字的跟踪点：</p><pre><code class=\"language-bash\">sudo bpftrace -l '*execve*'\n</code></pre><p>命令执行后，你会得到如下的输出内容：</p><pre><code class=\"language-plain\">kprobe:__ia32_compat_sys_execve\nkprobe:__ia32_compat_sys_execveat\nkprobe:__ia32_sys_execve\nkprobe:__ia32_sys_execveat\nkprobe:__x32_compat_sys_execve\nkprobe:__x32_compat_sys_execveat\nkprobe:__x64_sys_execve\nkprobe:__x64_sys_execveat\nkprobe:audit_log_execve_info\nkprobe:bprm_execve\nkprobe:do_execveat_common.isra.0\nkprobe:kernel_execve\ntracepoint:syscalls:sys_enter_execve\ntracepoint:syscalls:sys_enter_execveat\ntracepoint:syscalls:sys_exit_execve\ntracepoint:syscalls:sys_exit_execveat\n</code></pre><p>从输出中，你可以发现这些函数可以分为内核插桩（kprobe）和跟踪点（tracepoint）两类。在上一小节中我曾提到，内核插桩属于不稳定接口，而跟踪点则是稳定接口。因而，<strong>在内核插桩和跟踪点两者都可用的情况下，应该选择更稳定的跟踪点，以保证 eBPF 程序的可移植性（即在不同版本的内核中都可以正常执行）</strong>。</p><p>排除掉&nbsp;<code>kprobe</code>&nbsp;类型之后，剩下的&nbsp;<code>tracepoint:syscalls:sys_enter_execve</code>、<code>tracepoint:syscalls:sys_enter_execveat</code>、<code>tracepoint:syscalls:sys_exit_execve</code>&nbsp;以及&nbsp;<code>tracepoint:syscalls:sys_exit_execveat</code>&nbsp;就是我们想要的 eBPF 跟踪点。其中，<code>sys_enter_</code>&nbsp;和&nbsp;<code>sys_exit_</code>&nbsp;分别表示在系统调用的入口和出口执行。</p><p>只有跟踪点的列表还不够，因为我们还想知道具体启动的进程名称、命令行选项以及返回值，而这些也都可以通过 bpftrace 来查询。在命令行中执行下面的命令，即可查询：</p><pre><code class=\"language-bash\"># 查询sys_enter_execve入口参数\n$ sudo bpftrace -lv tracepoint:syscalls:sys_enter_execve\ntracepoint:syscalls:sys_enter_execve\n    int __syscall_nr\n    const char * filename\n    const char *const * argv\n    const char *const * envp\n\n# 查询sys_exit_execve返回值\n$ sudo bpftrace -lv tracepoint:syscalls:sys_exit_execve\ntracepoint:syscalls:sys_exit_execve\n    int __syscall_nr\n    long ret\n\n# 查询sys_enter_execveat入口参数\n$ sudo bpftrace -lv tracepoint:syscalls:sys_enter_execveat\ntracepoint:syscalls:sys_enter_execveat\n    int __syscall_nr\n    int fd\n    const char * filename\n    const char *const * argv\n    const char *const * envp\n    int flags\n\n# 查询sys_exit_execveat返回值\n$ sudo bpftrace -lv tracepoint:syscalls:sys_exit_execveat\ntracepoint:syscalls:sys_exit_execveat\n    int __syscall_nr\n    long ret\n</code></pre><p>从输出中可以看到，<code>sys_enter_execveat()</code>&nbsp;比&nbsp;<code>sys_enter_execve()</code>&nbsp;多了两个参数，而文件名&nbsp;<code>filename</code>、命令行选项&nbsp;<code>argv</code>&nbsp;以及返回值&nbsp;<code>ret</code>的定义都是一样的。</p><p>到这里，我带你使用 bpftrace 查询到了 execve 相关的跟踪点，以及这些跟踪点的具体格式。接下来，为了帮你全方位掌握 eBPF 程序的开发过程，我会以 bpftrace、BCC 和 libbpf 这三种方式为例，带你开发一个跟踪短时进程的 eBPF 程序。这三种方式各有优缺点，在实际的生产环境中都有大量的应用：</p><ul>\n<li><strong>bpftrace 通常用在快速排查和定位系统上，它支持用单行脚本的方式来快速开发并执行一个 eBPF 程序。</strong>不过，bpftrace 的功能有限，不支持特别复杂的 eBPF 程序，也依赖于 BCC 和 LLVM 动态编译执行。</li>\n<li><strong>BCC 通常用在开发复杂的 eBPF 程序中，其内置的各种小工具也是目前应用最为广泛的 eBPF 小程序。</strong>不过，BCC 也不是完美的，它依赖于 LLVM 和内核头文件才可以动态编译和加载 eBPF 程序。</li>\n<li><strong>libbpf 是从内核中抽离出来的标准库，用它开发的 eBPF 程序可以直接分发执行，这样就不需要每台机器都安装 LLVM 和内核头文件了。</strong>不过，它要求内核开启 BTF 特性，需要非常新的发行版才会默认开启（如 RHEL 8.2+ 和 Ubuntu 20.10+ 等）。</li>\n</ul><p>在实际应用中，你可以根据你的内核版本、内核配置、eBPF 程序复杂度，以及是否允许安装内核头文件和 LLVM 等编译工具等，来选择最合适的方案。</p><h3><strong>bpftrace 方法</strong></h3><p>这一讲我们先来看看，如何使用 bpftrace 来跟踪短时进程。</p><p>由于&nbsp;<code>execve()</code>&nbsp;和&nbsp;<code>execveat()</code>&nbsp;这两个系统调用的入口参数文件名&nbsp;<code>filename</code>&nbsp;和命令行选项&nbsp;<code>argv</code>&nbsp;，以及返回值&nbsp;<code>ret</code>&nbsp;的定义都是一样的，因而我们可以把这两个跟踪点放到一起来处理。</p><p>首先，我们先忽略返回值，只看入口参数。打开一个终端，执行下面的 bpftrace 命令：</p><pre><code class=\"language-bash\">sudo bpftrace -e 'tracepoint:syscalls:sys_enter_execve,tracepoint:syscalls:sys_enter_execveat { printf(\"%-6d %-8s\", pid, comm); join(args-&gt;argv);}'\n</code></pre><p>这个命令中的具体内容含义如下：</p><ul>\n<li>\n<p><code>bpftrace -e</code>&nbsp;表示直接从后面的字符串参数中读入 bpftrace 程序（除此之外，它还支持从文件中读入 bpftrace 程序）；</p>\n</li>\n<li>\n<p><code>tracepoint:syscalls:sys_enter_execve,tracepoint:syscalls:sys_enter_execveat</code>&nbsp;表示用逗号分隔的多个跟踪点，其后的中括号表示跟踪点的处理函数；</p>\n</li>\n<li>\n<p><code>printf()</code>&nbsp;表示向终端中打印字符串，其用法类似于 C 语言中的&nbsp;<code>printf()</code>&nbsp;函数；</p>\n</li>\n<li>\n<p><code>pid</code>&nbsp;和&nbsp;<code>comm</code>&nbsp;是 bpftrace 内置的变量，分别表示进程 PID 和进程名称（你可以在其<a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#1-builtins\">官方文档</a>中找到其他的内置变量）；</p>\n</li>\n<li>\n<p><code>join(args-&gt;argv)</code>&nbsp;表示把字符串数组格式的参数用空格拼接起来，再打印到终端中。对于跟踪点来说，你可以使用&nbsp;<code>args-&gt;参数名</code>&nbsp;的方式直接读取参数（比如这里的&nbsp;<code>args-&gt;argv</code>&nbsp;就是读取系统调用中的&nbsp;<code>argv</code>&nbsp;参数）。</p>\n</li>\n</ul><p>在另一个终端中执行&nbsp;<code>ls</code>&nbsp;命令，然后你会在第一个终端中看到如下的输出：</p><pre><code class=\"language-plain\">Attaching 2 probes...\n157286 zsh     ls --color=tty\n157289 zsh     git rev-parse --git-dir\n</code></pre><p>你可以发现，我的系统使用了&nbsp;<code>zsh</code>&nbsp;终端，在&nbsp;<code>zsh</code>&nbsp;终端中执行了<code>ls</code>&nbsp;和&nbsp;<code>git</code>&nbsp;命令。这儿多了个&nbsp;<code>git</code>&nbsp;命令，是因为我为&nbsp;<code>zsh</code>&nbsp;配置了&nbsp;<code>git</code>&nbsp;插件，而插件是由&nbsp;<code>zsh</code>&nbsp;自动调用的。</p><p>恭喜你，现在你已经可以通过一个简单的单行命令来跟踪短时进程问题了。不过，这个程序还不够完善，因为它的返回值还没有处理。那么，如何处理返回值呢？</p><p>一个最简单的思路就是在系统调用的入口把参数保存到 BPF 映射中，然后再在系统调用出口获取返回值后一起输出。比如，你可以尝试执行下面的命令，把新进程的参数存入哈希映射中：</p><pre><code class=\"language-bash\"># 其中，tid表示线程ID，@execs[tid]表示创建一个哈希映射\nsudo bpftrace -e 'tracepoint:syscalls:sys_enter_execve,tracepoint:syscalls:sys_enter_execveat {@execs[tid] = join(args-&gt;argv);}'\n</code></pre><p>很遗憾，这条命令并不能正常运行。根据下面的错误信息，你可以发现，<code>join()</code>&nbsp;这个内置函数没有返回字符串，不能用来赋值：</p><pre><code class=\"language-plain\">stdin:1:92-108: ERROR: join() should not be used in an assignment or as a map key\n</code></pre><p>实际上，在 bpftrace 的 GitHub 页面上，已经有其他用户汇报了同样的<a href=\"https://github.com/iovisor/bpftrace/issues/1390\">问题</a>，并且到现在还是没有解决。</p><p>正如我前面提到的，bpftrace 本身并不适用于所有的 eBPF 应用。如果是复杂的应用，我还是推荐使用 BCC 或者 libbpf 开发。关于 BCC 和 libbpf 的具体使用方法，我会在下一讲，也就是“内核跟踪”的下篇中继续为你讲解。</p><h2><strong>小结</strong></h2><p>今天，我带你梳理了查询 eBPF 跟踪点的常用方法，并以短时进程的跟踪为例，通过 bpftrace 实现了短时进程的跟踪程序。</p><p>在跟踪内核时，你要记得，所有的内核跟踪都是被内核函数、内核跟踪点或性能事件等事件源触发后才执行的。所以，在跟踪内核之前，我们就需要通过调试信息、perf、bpftrace 等，找到这些事件源，然后再利用 eBPF 提供的强大功能去跟踪这些事件的执行过程。</p><p>bpftrace 是一个使用最为简单的 eBPF 工具，因此在初学 eBPF 时，建议你可以从它开始。bpftrace 提供了一个简单的脚本语言，只需要简单的几条脚本就可以实现很丰富的 eBPF 程序。它通常用在快速排查和定位系统上，并支持用单行脚本的方式来快速开发并执行一个 eBPF 程序。</p><p>虽然 bpftrace 很好用，但你也要注意，它并不适合所有的 eBPF 应用场景。我通常把它作为 eBPF 跟踪程序的原型使用，当需要在生产环境运行 eBPF 程序时，再切换到 BCC 或者 libbpf。</p><h2><strong>思考题</strong></h2><p>虽然使用 bpftrace 时还有很多的限制，但今天我带你开发的这个跟踪程序，其实已经可以用到短时进程问题的排查中了。因为通常来说，在解决短时进程引发的性能问题时，找出短时进程才是最重要的。至于短时进程的执行结果，我们一般可以通过日志看到详细的运行过程。</p><p>不过，这个跟踪程序还是有一些比较大的限制，比如：</p><ul>\n<li>没有输出时间戳，这样去大量日志里面定位问题就比较困难；</li>\n<li>没有父进程 PID，还需要一些额外的工具或经验，才可以找出父进程。</li>\n</ul><p>那么，这些问题该如何解决呢？你可以在今天的 bpftrace 脚本基础上改进，把时间戳和父进程 PID 也输出到结果中吗？欢迎在评论区留下你的思考和实践经验。</p><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"06 | 事件触发：各类 eBPF 程序的触发机制及其应用场景","id":483364},"right":{"article_title":"08｜内核跟踪（下）：开发内核跟踪程序的进阶方法","id":484372}}},{"article_id":484372,"article_title":"08｜内核跟踪（下）：开发内核跟踪程序的进阶方法","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你梳理了查询 eBPF 跟踪点的常用方法，并以短时进程的跟踪为例，通过 bpftrace 实现了内核跟踪点的跟踪程序。</p><p>bpftrace 简单易用，非常适合入门，可以带初学者轻松体验 eBPF 的各种跟踪特性。但在上一讲的案例中，你也发现 bpftrace 并不适用于所有的 eBPF 应用，它本身的限制导致我们无法在需要复杂 eBPF 程序的场景中使用它。在复杂的应用中，我还是推荐你使用 BCC 或者 libbpf 进行开发。</p><p>那么，今天我就带你看看，如何使用 BCC 和 libbpf 这两个进阶方法来开发内核跟踪程序。</p><h2><strong>BCC 方法</strong></h2><p>我们先来看看如何使用 BCC 来开发上一讲中短时进程的跟踪程序。这里先说明下，由于&nbsp;execveat&nbsp;的处理逻辑同&nbsp;execve&nbsp;基本相同，限于篇幅的长度，接下来的 BCC 和 libbpf 程序都以&nbsp;execve&nbsp;为例。</p><p>这里我们先回顾下 <a href=\"https://time.geekbang.org/column/article/481090\">03讲</a> 的内容，使用 BCC 开发的 eBPF 程序包含两部分：</p><ul>\n<li>第一部分是用 C 语言开发的 eBPF 程序。在 eBPF 程序中，你可以利用 BCC 提供的<a href=\"https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md\">库函数和宏定义</a>简化你的处理逻辑。</li>\n<li>第二部分是用 Python 语言开发的前端界面，其中包含 eBPF 程序加载、挂载到内核函数和跟踪点，以及通过 BPF 映射获取和打印执行结果等部分。在前端程序中，你同样可以利用 BCC 库来访问 BPF 映射。</li>\n</ul><!-- [[[read_end]]] --><h3><strong>数据结构定义</strong></h3><p>我们先看第一部分。为了在系统调用入口跟踪点和出口跟踪点间共享进程信息等数据，我们可以定义一个哈希映射（比如命名为&nbsp;<code>tasks</code>）；同样地，因为我们想要在用户空间实时获取跟踪信息，这就需要一个性能事件映射。对于这两种映射的创建步骤，BCC 已经提供了非常方便的宏定义，你可以直接使用。</p><p>比如，你可以用下面的方式来创建这两个映射：</p><pre><code class=\"language-c++\">struct data_t {\n    u32 pid;\n    char comm[TASK_COMM_LEN];\n    int retval;\n    unsigned int args_size;\n    char argv[FULL_MAX_ARGS_ARR];\n};\nBPF_PERF_OUTPUT(events);\nBPF_HASH(tasks, u32, struct data_t);\n</code></pre><p>代码中指令的具体作用如下：</p><ul>\n<li><code>struct data_t</code>&nbsp;定义了一个包含进程基本信息的数据结构，它将用在哈希映射的值中（其中的参数大小&nbsp;<code>args_size</code>&nbsp;会在读取参数内容的时候用到）；</li>\n<li><code>BPF_PERF_OUTPUT(events)</code>&nbsp;定义了一个性能事件映射；</li>\n<li><code>BPF_HASH(tasks, u32, struct data_t)</code>&nbsp;定义了一个哈希映射，其键为 32 位的进程 PID，而值则是进程基本信息&nbsp;<code>data_t</code>。</li>\n</ul><p>两个映射定义好之后，接下来就是<strong>定义跟踪点的处理函数</strong>。在 BCC 中，你可以通过&nbsp;<code>TRACEPOINT_PROBE(category, event)</code>&nbsp;来定义一个跟踪点处理函数。BCC 会将所有的参数放入&nbsp;<code>args</code>&nbsp;这个变量中，这样使用&nbsp;<code>args-&gt;&lt;参数名&gt;</code>&nbsp;就可以访问跟踪点的参数值。</p><p>对我们要跟踪的短时进程问题来说，也就是下面这两个跟踪点：</p><pre><code class=\"language-c++\">// 定义sys_enter_execve跟踪点处理函数.\nTRACEPOINT_PROBE(syscalls, sys_enter_execve)\n{\n    //待添加处理逻辑\n}\n\n// 定义sys_exit_execve跟踪点处理函数.\nTRACEPOINT_PROBE(syscalls, sys_exit_execve)\n{\n    //待添加处理逻辑\n}\n</code></pre><h3><strong>入口跟踪点处理</strong></h3><p>对于入口跟踪点&nbsp;<code>sys_enter_execve</code>&nbsp;的处理，还是按照上一讲中 bpftrace 的逻辑，先获取进程的 PID、进程名称和参数列表之后，再存入刚刚定义的哈希映射中。</p><p>其中，进程 PID 和进程名称都比较容易获取。如下面的代码所示，你可以调用&nbsp;<code>bpf_get_current_pid_tgid()</code>&nbsp;查询进程 PID，调用&nbsp;<code>bpf_get_current_comm()</code>&nbsp;读取进程名称：</p><pre><code class=\"language-c++\">    // 获取进程PID和进程名称\n    struct data_t data = { };\n    u32 pid = bpf_get_current_pid_tgid();  // 取低32位为进程PID\n    data.pid = pid;\n    bpf_get_current_comm(&amp;data.comm, sizeof(data.comm));\n</code></pre><p>而命令行参数的获取就没那么容易了。因为 BCC 把所有参数都放到了&nbsp;<code>args</code>&nbsp;中，你可以使用&nbsp;<code>args-&gt;argv</code>&nbsp;来访问参数列表：</p><pre><code class=\"language-c++\">const char **argv = (const char **)(args-&gt;argv);\n</code></pre><p>注意，<code>argv</code>&nbsp;是一个用户空间的字符串数组（指针数组），这就需要调用&nbsp;<code>bpf_probe_read</code>&nbsp;系列的辅助函数，去这些指针中读取数据。并且，字符串的数量（即参数的个数）和每个字符串的长度（即每个参数的长度）都是未知的，由于 eBPF 栈大小只有 512 字节，如果想要把它们读入一个临时的字符数组中，必须要保证每次读取的内容不超过栈的大小。这类问题有很多种不同的处理方法，其中一个比较简单的方式就是<strong>把多余的参数截断，使用</strong><code>...</code><strong>代替过长的参数</strong><strong>。</strong>一般来说，知道了进程的名称和前几个参数，对调试和排错来说就足够了。</p><p>你可以定义最大读取的参数个数和参数长度，然后在哈希映射的值中定义一个字符数组，代码如下所示：</p><pre><code class=\"language-c++\">// 定义参数长度和参数个数常量\n#define ARGSIZE 64\n#define TOTAL_MAX_ARGS 5\n#define FULL_MAX_ARGS_ARR (TOTAL_MAX_ARGS * ARGSIZE)\n\nstruct data_t {\n    ...\n    char argv[FULL_MAX_ARGS_ARR];\n};\n</code></pre><p>有了字符数组，接下来再定义一个辅助函数，从参数数组中读取字符串参数（限定最长&nbsp;<code>ARGSIZE</code>）：</p><pre><code class=\"language-c++\">// 从用户空间读取字符串\nstatic int __bpf_read_arg_str(struct data_t *data, const char *ptr)\n{\n    if (data-&gt;args_size &gt; LAST_ARG) {\n        return -1;\n    }\n\n    int ret = bpf_probe_read_user_str(&amp;data-&gt;argv[data-&gt;args_size], ARGSIZE, (void *)ptr);\n    if (ret &gt; ARGSIZE || ret &lt; 0) {\n        return -1;\n    }\n\n    // increase the args size. the first tailing '\\0' is not counted and hence it\n    // would be overwritten by the next call.\n    data-&gt;args_size += (ret - 1);\n\n    return 0;\n}\n</code></pre><p>在这个函数中，有几点需要你注意：</p><ul>\n<li><code>bpf_probe_read_user_str()</code>&nbsp;返回的是包含字符串结束符&nbsp;<code>\\0</code>&nbsp;的长度。为了拼接所有的字符串，在计算已读取参数长度的时候，需要把&nbsp;<code>\\0</code>&nbsp;排除在外。</li>\n<li><code>&amp;data-&gt;argv[data-&gt;args_size]</code>&nbsp;用来获取要存放参数的位置指针，这是为了把多个参数拼接到一起。</li>\n<li>在调用&nbsp;<code>bpf_probe_read_user_str()</code>&nbsp;前后，需要对指针位置和返回值进行校验，这可以帮助 eBPF 验证器获取指针读写的边界（如果你感兴趣，可以参考<a href=\"https://sysdig.com/blog/the-art-of-writing-ebpf-programs-a-primer\">这篇文章</a>，了解更多的内存访问验证细节）。</li>\n</ul><p>有了这个辅助函数之后，因为 eBPF 在老版本内核中并不支持循环（有界循环在 <strong>5.3</strong> 之后才支持），要访问字符串数组，还需要一个小技巧：使用&nbsp;<code>#pragma unroll</code>&nbsp;告诉编译器，把源码中的循环自动展开。这就避免了最终的字节码中包含循环。</p><p>完整的处理函数如下所示（具体的每一步我都加了详细的注释，你可以参考注释来加深理解）：</p><pre><code class=\"language-c++\">// 引入内核头文件\n#include &lt;uapi/linux/ptrace.h&gt;\n#include &lt;linux/sched.h&gt;\n#include &lt;linux/fs.h&gt;\n\n// 定义sys_enter_execve跟踪点处理函数.\nTRACEPOINT_PROBE(syscalls, sys_enter_execve)\n{\n    // 变量定义\n    unsigned int ret = 0;\n    const char **argv = (const char **)(args-&gt;argv);\n\n    // 获取进程PID和进程名称\n    struct data_t data = { };\n    u32 pid = bpf_get_current_pid_tgid();\n    data.pid = pid;\n    bpf_get_current_comm(&amp;data.comm, sizeof(data.comm));\n\n    // 获取第一个参数（即可执行文件的名字）\n    if (__bpf_read_arg_str(&amp;data, (const char *)argv[0]) &lt; 0) {\n        goto out;\n    }\n\n    // 获取其他参数（限定最多5个）\n    #pragma unrollfor (int i = 1; i &lt; TOTAL_MAX_ARGS; i++) {\n        if (__bpf_read_arg_str(&amp;data, (const char *)argv[i]) &lt; 0) {\n            goto out;\n        }\n    }\n\n out:\n    // 存储到哈希映射中\n    tasks.update(&amp;pid, &amp;data);\n    return 0;\n}\n</code></pre><p>注意，<strong>为了获取内核数据结构的定义，在文件的开头需要引入相关的内核头文件</strong>。此外，读取参数完成之后，不要忘记调用&nbsp;<code>tasks.update()</code>&nbsp;把进程的基本信息存储到哈希映射中。因为返回值需要等到出口跟踪点时才可以获取，这儿只需要更新哈希映射就可以了，不需要把进程信息提交到性能事件映射中去。</p><h3><strong>出口跟踪点处理</strong></h3><p>入口跟踪点&nbsp;<code>sys_enter_execve</code>&nbsp;处理好之后，我们再来看看出口跟踪点&nbsp;<code>sys_exit_execve</code>&nbsp;该如何处理。</p><p>由于进程的基本信息已经保存在了哈希映射中，所以出口事件的处理可以分为查询进程基本信息、填充返回值、最后再提交到性能事件映射这三个步骤。具体代码如下所示：</p><pre><code class=\"language-c++\">// 定义sys_exit_execve跟踪点处理函数.\nTRACEPOINT_PROBE(syscalls, sys_exit_execve)\n{\n    // 从哈希映射中查询进程基本信息\n    u32 pid = bpf_get_current_pid_tgid();\n    struct data_t *data = tasks.lookup(&amp;pid);\n\n    // 填充返回值并提交到性能事件映射中\n    if (data != NULL) {\n        data-&gt;retval = args-&gt;ret;\n        events.perf_submit(args, data, sizeof(struct data_t));\n\n        // 最后清理进程信息\n        tasks.delete(&amp;pid);\n    }\n\n    return 0;\n}\n</code></pre><p>到这里，完整的 eBPF 程序就开发好了，你可以把上述的代码保存到一个本地文件中，并命名为&nbsp;<code>execsnoop.c</code>（你也可以在&nbsp;<a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bcc-apps/python/execsnoop.c\">GitHub</a>&nbsp;上找到全部源码）。</p><h3><strong>Python前端处理</strong></h3><p>eBPF 程序开发完成后，最后一步就是为它增加一个 Python 前端。</p><p>同 <a href=\"https://time.geekbang.org/column/article/481090\">03 讲</a> 的 Hello World 类似，<strong>Python 前端逻辑需要 eBPF 程序加载、挂载到内核函数和跟踪点，以及通过 BPF 映射获取和打印执行结果等几个步骤</strong>。其中，因为我们已经使用了&nbsp;<code>TRACEPOINT_PROBE</code>&nbsp;宏定义，来定义 eBPF 跟踪点处理函数，BCC 在加载字节码的时候，会帮你自动把它挂载到正确的跟踪点上，所以挂载的步骤就可以忽略。完整的 Python 程序如下所示：</p><pre><code class=\"language-python\"># 引入库函数\nfrom bcc import BPF\nfrom bcc.utils import printb\n\n# 1) 加载eBPF代码\nb = BPF(src_file=\"execsnoop.c\")\n\n# 2) 输出头\nprint(\"%-6s %-16s %-3s %s\" % (\"PID\", \"COMM\", \"RET\", \"ARGS\"))\n\n# 3) 定义性能事件打印函数\ndef print_event(cpu, data, size):\n    # BCC自动根据\"struct data_t\"生成数据结构\n    event = b[\"events\"].event(data)\n    printb(b\"%-6d %-16s %-3d %-16s\" % (event.pid, event.comm, event.retval, event.argv))\n\n# 4) 绑定性能事件映射和输出函数，并从映射中循环读取数据\nb[\"events\"].open_perf_buffer(print_event)\nwhile 1:\n    try:\n        b.perf_buffer_poll()\n    except KeyboardInterrupt:\n        exit()\n</code></pre><p>把上述的代码保存到&nbsp;<code>execsnoop.py</code>&nbsp;中，然后通过 Python 运行，并在另一个终端中执行&nbsp;<code>ls</code>&nbsp;命令，你就可以得到如下的输出：</p><pre><code class=\"language-bash\">$ sudo python3 execsnoop.py\nPID    COMM             RET ARGS\n249134 zsh              0   ls--color=tty\n</code></pre><p>恭喜，到这里你已经开发了一个新的 eBPF 程序，并且它可以帮你排查短时进程相关的性能问题。</p><p>不过，在你想要分发这个程序到生产环境时，又会碰到一个新的难题：BCC 依赖于 LLVM 和内核头文件才可以动态编译和加载 eBPF 程序，而出于安全策略的需要，在生产环境中通常又不允许安装这些开发工具。</p><p>这个难题应该怎么克服呢？一种很容易想到的方法是把 BCC 和开发工具都安装到容器中，容器本身不提供对外服务，这样可以降低安全风险。另外一种方法就是参考内核中的&nbsp;<a href=\"https://elixir.bootlin.com/linux/v5.13/source/samples/bpf\">eBPF 示例</a>，开发一个匹配当前内核版本的 eBPF 程序，并编译为字节码，再分发到生产环境中。</p><p>除此之外，如果你的内核已经支持了 BPF 类型格式 (BTF)，我推荐你使用从内核源码中抽离出来的 libbpf 进行开发，这样可以借助 BTF 和 CO-RE 获得更好的移植性。实际上，BCC 的很多工具都在向 BTF 迁移中，相信未来 libbpf 会成为最受欢迎的 eBPF 程序开发基础库，甚至 Windows eBPF 也会支持 libbpf。</p><h2><strong>libbpf 方法</strong></h2><p>那么，如何用 libbpf 来开发一个 eBPF 程序呢？跟刚才的 BCC 程序类似，使用 libbpf 开发 eBPF 程序也是分为两部分：第一，内核态的 eBPF 程序；第二，用户态的加载、挂载、映射读取以及输出程序等。</p><p><strong>在 eBPF 程序中，由于内核已经支持了 BTF，你不再需要引入众多的内核头文件来获取内核数据结构的定义。</strong>取而代之的是一个通过 bpftool 生成的&nbsp;<strong><code>vmlinux.h</code></strong>&nbsp;头文件，其中包含了内核数据结构的定义。</p><p>这样，使用 libbpf 开发 eBPF 程序就可以通过以下四个步骤完成：</p><ol>\n<li>使用 bpftool 生成内核数据结构定义头文件。BTF 开启后，你可以在系统中找到&nbsp;<code>/sys/kernel/btf/vmlinux</code>&nbsp;这个文件，bpftool 正是从它生成了内核数据结构头文件。</li>\n<li>开发 eBPF 程序部分。为了方便后续通过统一的 Makefile 编译，eBPF 程序的源码文件一般命名为&nbsp;<code>&lt;程序名&gt;.bpf.c</code>。</li>\n<li>编译 eBPF 程序为字节码，然后再调用&nbsp;<code>bpftool gen skeleton</code>&nbsp;为 eBPF 字节码生成脚手架头文件（Skeleton Header）。这个头文件包含了 eBPF 字节码以及相关的加载、挂载和卸载函数，可在用户态程序中直接调用。</li>\n<li>最后就是用户态程序引入上一步生成的头文件，开发用户态程序，包括 eBPF 程序加载、挂载到内核函数和跟踪点，以及通过 BPF 映射获取和打印执行结果等。</li>\n</ol><p>通常，这几个步骤里面的编译、库链接、执行&nbsp;<code>bpftool</code>&nbsp;命令等，都可以放到 Makefile 中，这样就可以通过一个&nbsp;<code>make</code>&nbsp;命令去执行所有的步骤。比如，下面是一个简化版本的 Makefile：</p><pre><code class=\"language-makefile\">APPS = execsnoop\n\n.PHONY: all\nall: $(APPS)\n\n$(APPS):\n    clang -g -O2 -target bpf -D__TARGET_ARCH_x86_64 -I/usr/include/x86_64-linux-gnu -I. -c $@.bpf.c -o $@.bpf.o\n    bpftool gen skeleton $@.bpf.o &gt; $@.skel.h\n    clang -g -O2 -Wall -I . -c $@.c -o $@.o\n    clang -Wall -O2 -g $@.o -static -lbpf -lelf -lz -o $@\n\nvmlinux:\n    $(bpftool) btf dump file /sys/kernel/btf/vmlinux format c &gt; vmlinux.h\n</code></pre><p>有了这个 Makefile 之后，你执行&nbsp;<code>make vmlinux</code>&nbsp;命令就可以生成&nbsp;<code>vmlinux.h</code>&nbsp;文件，再执行&nbsp;<code>make</code>&nbsp;就可以编译&nbsp;<code>APPS</code>&nbsp;里面配置的所有 eBPF 程序（多个程序之间以空格分隔）。</p><p>接下来，我就带你一起通过上述四个步骤开发跟踪短时进程的 eBPF 程序。</p><h3><strong>内核头文件生成</strong></h3><p>首先，对于第一步，我们只需要执行下面的命令，即可生成内核数据结构的头文件：</p><pre><code class=\"language-bash\">sudo bpftool btf dump file /sys/kernel/btf/vmlinux format c &gt; vmlinux.h\n</code></pre><p>如果命令执行失败了，并且错误说 BTF 不存在，那说明当前系统内核没有开启 BTF 特性。这时候，你需要开启&nbsp;<code>CONFIG_DEBUG_INFO_BTF=y</code>&nbsp;和&nbsp;<code>CONFIG_DEBUG_INFO=y</code>&nbsp;这两个编译选项，然后重新编译和安装内核。</p><h3><strong>eBPF 程序定义</strong></h3><p>第二步就是开发 eBPF 程序，包括定义哈希映射、性能事件映射以及跟踪点的处理函数等，而对这些数据结构和跟踪函数的定义都可以通过&nbsp;<code>SEC()</code>&nbsp;宏定义来完成。在编译时，<strong>通过 <code>SEC()</code> 宏定义的数据结构和函数会放到特定的 ELF 段中，这样后续在加载 BPF 字节码时，就可以从这些段中获取所需的元数据。</strong></p><p>比如，你可以使用下面的代码来定义映射和跟踪点处理函数：</p><pre><code class=\"language-c++\">// 包含头文件\n#include \"vmlinux.h\"\n#include &lt;bpf/bpf_helpers.h&gt;\n\n// 定义进程基本信息数据结构\nstruct event {\n    char comm[TASK_COMM_LEN];\n    pid_t pid;\n    int retval;\n    int args_count;\n    unsigned int args_size;\n    char args[FULL_MAX_ARGS_ARR];\n};\n\n// 定义哈希映射\nstruct {\n    __uint(type, BPF_MAP_TYPE_HASH);\n    __uint(max_entries, 10240);\n    __type(key, pid_t);\n    __type(value, struct event);\n} execs SEC(\".maps\");\n\n// 定义性能事件映射\nstruct {\n    __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY);\n    __uint(key_size, sizeof(u32));\n    __uint(value_size, sizeof(u32));\n} events SEC(\".maps\");\n\n// sys_enter_execve跟踪点\nSEC(\"tracepoint/syscalls/sys_enter_execve\")\nint tracepoint__syscalls__sys_enter_execve(struct trace_event_raw_sys_enter *ctx)\n{\n  // 待实现处理逻辑\n}\n\n// sys_exit_execve跟踪点\nSEC(\"tracepoint/syscalls/sys_exit_execve\")\nint tracepoint__syscalls__sys_exit_execve(struct trace_event_raw_sys_exit *ctx)\n{\n  // 待实现处理逻辑\n}\n\n// 定义许可证（前述的BCC默认使用GPL）\nchar LICENSE[] SEC(\"license\") = \"Dual BSD/GPL\";\n</code></pre><p>让我们来看看这段代码的具体含义：</p><ul>\n<li>头文件&nbsp;<code>vmlinux.h</code>&nbsp;包含了内核数据结构，而&nbsp;<code>bpf/bpf_helpers.h</code>&nbsp;包含了 <a href=\"https://time.geekbang.org/column/article/482459\">05 讲</a> 提到的 BPF 辅助函数；</li>\n<li><code>struct event</code>&nbsp;定义了进程基本信息数据结构，它会用在后面的哈希映射中；</li>\n<li><code>SEC(\".maps\")</code>&nbsp;定义了哈希映射和性能事件映射；</li>\n<li><code>SEC(\"tracepoint/&lt;跟踪点名称&gt;\")</code>&nbsp;定义了跟踪点处理函数，系统调用跟踪点的格式是&nbsp;<code>tracepoint/syscalls/&lt;系统调用名称&gt;\"</code>。以后你需要定义内核插桩和用户插桩的时候，也是以类似的格式定义，比如&nbsp;<code>kprobe/do_unlinkat</code>&nbsp;或&nbsp;<code>uprobe/func</code>；</li>\n<li>最后的&nbsp;<code>SEC(\"license\")</code>&nbsp;定义了 eBPF 程序的许可证。在上述的 BCC eBPF 程序中，我们并没有定义许可证，这是因为 BCC 自动帮你使用了 GPL 许可。</li>\n</ul><p>有了基本的程序结构，接下来就是<strong>实现系统调用入口和出口跟踪点的处理函数</strong>。它们的基本过程跟上述的 BCC 程序是类似的。</p><h3><strong>入口跟踪点处理</strong></h3><p>对于入口跟踪点&nbsp;<code>sys_enter_execve</code>&nbsp;的处理，还是按照上述 BCC 程序的逻辑，先获取进程的 PID、进程名称和参数列表之后，再存入刚刚定义的哈希映射中。完整代码如下所示，具体每一步的内容我都加了详细的注释：</p><pre><code class=\"language-c++\">SEC(\"tracepoint/syscalls/sys_enter_execve\")\nint tracepoint__syscalls__sys_enter_execve(struct trace_event_raw_sys_enter\n                       *ctx)\n{\n    struct event *event;\n    const char **args = (const char **)(ctx-&gt;args[1]);\n    const char *argp;\n\n    // 查询PID\n    u64 id = bpf_get_current_pid_tgid();\n    pid_t pid = (pid_t) id;\n\n    // 保存一个空的event到哈希映射中\n    if (bpf_map_update_elem(&amp;execs, &amp;pid, &amp;empty_event, BPF_NOEXIST)) {\n        return 0;\n    }\n    event = bpf_map_lookup_elem(&amp;execs, &amp;pid);\n    if (!event) {\n        return 0;\n    }\n\n    // 初始化event变量\n    event-&gt;pid = pid;\n    event-&gt;args_count = 0;\n    event-&gt;args_size = 0;\n\n    // 查询第一个参数\n    unsigned int ret = bpf_probe_read_user_str(event-&gt;args, ARGSIZE,\n                           (const char *)ctx-&gt;args[0]);\n    if (ret &lt;= ARGSIZE) {\n        event-&gt;args_size += ret;\n    }\n\n    // 查询其他参数\n    event-&gt;args_count++;\n    #pragma unrollfor (int i = 1; i &lt; TOTAL_MAX_ARGS; i++) {\n        bpf_probe_read_user(&amp;argp, sizeof(argp), &amp;args[i]);\n        if (!argp)\n            return 0;\n\n        if (event-&gt;args_size &gt; LAST_ARG)\n            return 0;\n\n        ret =\n            bpf_probe_read_user_str(&amp;event-&gt;args[event-&gt;args_size],\n                        ARGSIZE, argp);\n        if (ret &gt; ARGSIZE)\n            return 0;\n\n        event-&gt;args_count++;\n        event-&gt;args_size += ret;\n    }\n\n    // 再尝试一次，确认是否还有未读取的参数\n    bpf_probe_read_user(&amp;argp, sizeof(argp), &amp;args[TOTAL_MAX_ARGS]);\n    if (!argp)\n        return 0;\n\n    // 如果还有未读取参数，则增加参数数量（用于输出\"...\"）\n    event-&gt;args_count++;\n\n    return 0;\n}\n</code></pre><p>其中，你需要注意这三点：</p><ul>\n<li>第一，程序使用了&nbsp;<code>bpf_probe_read_user()</code>&nbsp;来查询参数。由于它把&nbsp;<code>\\0</code>&nbsp;也算到了已读取参数的长度里面，所以最终&nbsp;<code>event-&gt;args</code>&nbsp;中保存的各个参数是以&nbsp;<code>\\0</code>&nbsp;分隔的。在用户态程序输出参数之前，需要用空格替换&nbsp;<code>\\0</code>。</li>\n<li>第二，程序在一开始的时候向哈希映射存入了一个空事件，在后续出口跟踪点处理的时候需要确保空事件也能正确清理。</li>\n<li>第三，程序在最后又尝试多读取了一次参数列表。如果还有未读取参数，参数数量增加了 1。用户态程序可以根据参数数量来决定是不是需要在参数结尾输出一个&nbsp;<code>...</code>。</li>\n</ul><h3><strong>出口跟踪点处理</strong></h3><p>入口跟踪点处理好之后，再来看看出口跟踪点的处理方法。它的步骤跟 BCC 程序也是类似的，也是查询进程基本信息、填充返回值、提交到性能事件映射这三个步骤。</p><p>除此之外，由于刚才入口跟踪点的处理中没有读取进程名称，所以在提交性能事件之前还需要先查询一下进程名称。完整的程序如下所示，具体每一步的内容我也加了详细的注释：</p><pre><code class=\"language-c++\">SEC(\"tracepoint/syscalls/sys_exit_execve\")\nint tracepoint__syscalls__sys_exit_execve(struct trace_event_raw_sys_exit *ctx)\n{\n    u64 id;\n    pid_t pid;\n    int ret;\n    struct event *event;\n\n    // 从哈希映射中查询进程基本信息\n    id = bpf_get_current_pid_tgid();\n    pid = (pid_t) id;\n    event = bpf_map_lookup_elem(&amp;execs, &amp;pid);\n    if (!event)\n        return 0;\n\n    // 更新返回值和进程名称\n    ret = ctx-&gt;ret;\n    event-&gt;retval = ret;\n    bpf_get_current_comm(&amp;event-&gt;comm, sizeof(event-&gt;comm));\n\n    // 提交性能事件\n    size_t len = EVENT_SIZE(event);\n    if (len &lt;= sizeof(*event))\n        bpf_perf_event_output(ctx, &amp;events, BPF_F_CURRENT_CPU, event,\n                      len);\n\n    // 清理哈希映射\n    bpf_map_delete_elem(&amp;execs, &amp;pid);\n    return 0;\n}\n</code></pre><p>从这些代码中你可以看到，它的处理逻辑跟上述的 BCC 程序基本上是相同的。不过，详细对比一下，你会发现它们之间还是有不同的，不同点主要在两个方面：</p><ul>\n<li>第一，函数名的定义格式不同。BCC 程序使用的是&nbsp;<code>TRACEPOINT_PROBE</code>&nbsp;宏，而 libbpf 程序用的则是&nbsp;<code>SEC</code>&nbsp;宏。</li>\n<li>第二，映射的访问方法不同。BCC 封装了很多更易用的映射访问函数（如&nbsp;<code>tasks.lookup()</code>），而 libbpf 程序则需要调用 <a href=\"https://time.geekbang.org/column/article/482459\">05 讲</a> 提到过的 BPF 辅助函数（比如查询要使用&nbsp;<code>bpf_map_lookup_elem()</code>）。</li>\n</ul><p>到这里，新建一个目录，并把上述代码存入&nbsp;<code>execsnoop.bpf.c</code>&nbsp;文件中，eBPF 部分的代码也就开发好了。</p><h3><strong>编译并生成脚手架头文件</strong></h3><p>有了 eBPF 程序，执行下面的命令，你就可以使用 clang 和 bpftool 将其编译成 BPF 字节码，然后再生成其脚手架头文件&nbsp;<code>execsnoop.skel.h</code>&nbsp;（注意，脚手架头文件的名字一般定义为&nbsp;<code>&lt;程序名&gt;.skel.h</code>）：</p><pre><code class=\"language-bash\">clang -g -O2 -target bpf -D__TARGET_ARCH_x86_64 -I/usr/include/x86_64-linux-gnu -I. -c execsnoop.bpf.c -o execsnoop.bpf.o\nbpftool gen skeleton execsnoop.bpf.o &gt; execsnoop.skel.h\n</code></pre><p>其中，clang 的参数&nbsp;<code>-target bpf</code>&nbsp;表示要生成 BPF 字节码，<code>-D__TARGET_ARCH_x86_64</code>&nbsp;表示目标的体系结构是 x86_64，而&nbsp;<code>-I</code>&nbsp;则是引入头文件路径。</p><p>命令执行后，脚手架头文件会放到&nbsp;<code>execsnoop.skel.h</code>&nbsp;中，这个头文件包含了 BPF 字节码和相关的管理函数。因而，当用户态程序引入这个头文件并编译之后，只需要分发最终用户态程序生成的二进制文件到生产环境即可（如果用户态程序使用了其他的动态库，还需要分发动态库）。</p><h3><strong>开发用户态程序</strong></h3><p>有了脚手架头文件之后，还剩下最后一步，也就是用户态程序的开发。</p><p>同 BCC 的 Python 前端程序类似，libbpf 用户态程序也需要 eBPF 程序加载、挂载到跟踪点，以及通过 BPF 映射获取和打印执行结果等几个步骤。虽然 C 语言听起来可能比 Python 语言麻烦一些，但实际上，这几个步骤都可以通过脚手架头文件中自动生成的函数来完成。</p><p>下面是忽略了错误处理逻辑之后，用户态程序的一个基本框架：</p><pre><code class=\"language-c++\">// 引入脚手架头文件\n#include \"execsnoop.skel.h\"\n\n// C语言主函数\nint main(int argc, char **argv)\n{\n    // 定义BPF程序和性能事件缓冲区\n    struct execsnoop_bpf *skel;\n    struct perf_buffer_opts pb_opts;\n    struct perf_buffer *pb = NULL;\n    int err;\n\n    // 1. 设置调试输出函数\n    libbpf_set_print(libbpf_print_fn);\n\n    // 2. 增大 RLIMIT_MEMLOCK（默认值通常太小，不足以存入BPF映射的内容）\n    bump_memlock_rlimit();\n\n    // 3. 初始化BPF程序\n    skel = execsnoop_bpf__open();\n\n    // 4. 加载BPF字节码\n    err = execsnoop_bpf__load(skel);\n\n    // 5. 挂载BPF字节码到跟踪点\n    err = execsnoop_bpf__attach(skel);\n\n    // 6. 配置性能事件回调函数\n    pb_opts.sample_cb = handle_event;\n    pb = perf_buffer__new(bpf_map__fd(skel-&gt;maps.events), 64, &amp;pb_opts);\n\n    // 7. 从缓冲区中循环读取数据\n    while ((err = perf_buffer__poll(pb, 100)) &gt;= 0) ;\n}\n</code></pre><p>其中，<code>execsnoop_</code>&nbsp;开头的数据结构和函数都包含在脚手架头文件&nbsp;<code>execsnoop.skel.h</code>&nbsp;中。而具体到每一步的含义如下：</p><ul>\n<li>第 1 步的调试输出函数中，可以调用&nbsp;<code>printf()</code>&nbsp;把调试信息输出到终端中。</li>\n<li>第 2 步增大锁定内存限制&nbsp;<code>RLIMIT_MEMLOCK</code>&nbsp;是必要的，因为系统默认的锁定内存通常过小，无法满足 BPF 映射的需要。</li>\n<li>第 3~5 步，直接调用脚手架头文件中的函数，加载 BPF 字节码并挂载到跟踪点。</li>\n<li>第 6~7 步为性能事件设置回调函数，并从缓冲区中循环读取数据。注意，性能事件映射&nbsp;<code>skel-&gt;maps.events</code>&nbsp;也是 bpftool 自动帮你生成好的。</li>\n</ul><p>接下来，在性能事件回调函数中，把数据格式转换为&nbsp;<code>struct event</code>&nbsp;格式之后，由于参数列表是使用&nbsp;<code>\\0</code>&nbsp;来分割的，并不能直接向终端打印所有参数。所以，还需要把&nbsp;<code>\\0</code>&nbsp;先替换为空格，然后再打印。完整的回调函数如下所示：</p><pre><code class=\"language-c++\">// 性能事件回调函数(向终端中打印进程名、PID、返回值以及参数)\nvoid handle_event(void *ctx, int cpu, void *data, __u32 data_sz)\n{\n    const struct event *e = data;\n    printf(\"%-16s %-6d %3d \", e-&gt;comm, e-&gt;pid, e-&gt;retval);\n    print_args(e);\n    putchar('\\n');\n}\n\n// 打印参数（替换'\\0'为空格）\nstatic void print_args(const struct event *e)\n{\n    int args_counter = 0;\n\n    for (int i = 0; i &lt; e-&gt;args_size &amp;&amp; args_counter &lt; e-&gt;args_count; i++) {\n        char c = e-&gt;args[i];\n        if (c == '\\0') {\n            // 把'\\0'替换为空格\n            args_counter++;\n            putchar(' ');\n        } else {\n            putchar(c);\n        }\n    }\n    if (e-&gt;args_count &gt; TOTAL_MAX_ARGS) {\n        // 过长的参数输出\"...\"替代\n        fputs(\" ...\", stdout);\n    }\n}\n</code></pre><p>把上面的代码保存到&nbsp;<code>execsnoop.c</code>&nbsp;文件中（你也可以在&nbsp;<a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bpf-apps/execsnoop.c\">GitHub</a>&nbsp;上找到完整的代码），然后执行下面的命令，将其编译为可执行文件：</p><pre><code class=\"language-bash\">clang -g -O2 -Wall -I . -c execsnoop.c -o execsnoop.o\nclang -Wall -O2 -g execsnoop.o -static -lbpf -lelf -lz -o execsnoop\n</code></pre><p>最后，执行&nbsp;<code>execsnoop</code>，你就可以得到如下的结果：</p><pre><code class=\"language-bash\">$ sudo ./execsnoop\nCOMM             PID    RET ARGS\nsh               276871   0 /bin/sh -c which ps\nwhich            276872   0 /usr/bin/which ps\n</code></pre><p>你还可以直接把这个文件复制到开启了 BTF 的其他机器中，无需安装额外的 LLVM 开发工具和内核头文件，也可以直接执行。</p><p>如果命令失败，并且你看到如下的错误，这说明当前机器没有开启 BTF，需要重新编译内核开启 BTF 才可以运行：</p><pre><code class=\"language-plain\">Failed to load and verify BPF skeleton\n</code></pre><p>恭喜，加上上一讲的内容，到这里你就通过 bpftrace、BCC 和 libbpf 这三种方法，实现了短时进程的跟踪。虽然这三种方法的步骤和实现代码各不相同，但实际上它们的实现逻辑都是类似的，无非就是<strong>找出跟踪点，然后在 eBPF 部分获取想要的数据并保存到 BPF 映射中，最后在用户空间程序中读取 BPF 映射的内容并输出出来</strong>。</p><h2><strong>小结</strong></h2><p>今天，我以短时进程的跟踪为例，通过 BCC 和 libbpf 这两种方法实现了短时进程的跟踪程序（你可以在 GitHub 的<a href=\"https://github.com/feiskyer/ebpf-apps\">这个链接</a>中，找到今天的案例中提到的所有源码）。加上上一讲介绍的 bpftrace 方法，我已经带你掌握了目前最常用的三种 eBPF 程序开发方法，在这里我一起总结下。</p><p>在实际的应用中，这三种方法有不同的使用场景：</p><ul>\n<li>bpftrace 通常用在<strong>快速排查和定位系统</strong>上，它支持用单行脚本的方式来快速开发并执行一个 eBPF 程序；</li>\n<li>BCC 通常用在<strong>开发复杂的 eBPF 程序</strong>中，它内置的各种小工具也是目前应用最为广泛的 eBPF 小程序；</li>\n<li>libbpf 是<strong>从内核中抽离出来的标准库</strong>，用它开发的 eBPF 程序可以直接分发执行，不再需要在每台机器上都安装 LLVM 和内核头文件。</li>\n</ul><p>通常情况下，你可以用 bpftrace 或 BCC 做一些快速原型，验证你的设计思路是不是可行，然后再切换到 libbpf ，开发完善的 eBPF 程序后再去分发执行。这样，不仅 eBPF 程序运行得更快（无需编译步骤），还避免了在运行环境中安装开发工具和内核头文件。</p><p>在不支持 BTF 的机器中，如果不想在运行 eBPF 时依赖于 LLVM 编译和内核头文件，你还可以参考内核中的&nbsp;<a href=\"https://elixir.bootlin.com/linux/v5.13/source/samples/bpf\">BPF 示例</a>，直接引用内核源码中的&nbsp;<code>tools/lib/bpf/</code>&nbsp;库，以及内核头文件中的数据结构，来开发 eBPF 程序。</p><h2><strong>思考题</strong></h2><p>今天使用 BCC 和 libbpf 开发的 eBPF 程序虽然可以正常运行，但是我相信细心的你一定发现了，它还有不少小问题，比如：</p><ul>\n<li>单个参数过长，或者总的参数数量比较多时，都会被截断，没法完整显示所有的参数列表；</li>\n<li>在调试短时进程问题时，很多情况下我们可能还需要父进程的信息，这样才能更快定位它们都是被哪些进程创建出来的。</li>\n</ul><p>学习完最近这两讲的内容，你觉得该如何解决这些问题呢？你可以在&nbsp;<a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bcc-apps/python/execsnoop.c\">execsnoop</a>&nbsp;的基础上改进，开发一个更完善的 eBPF 程序吗？欢迎在评论区和我分享你的思路和解决方法。</p><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"07 | 内核跟踪（上）：如何查询内核中的跟踪点？","id":484207},"right":{"article_title":"09 | 用户态跟踪：如何使用 eBPF 排查应用程序？","id":484458}}},{"article_id":484458,"article_title":"09 | 用户态跟踪：如何使用 eBPF 排查应用程序？","article_content":"<p>你好，我是倪朋飞。</p><p>前面两讲，我带你梳理了查询 eBPF 跟踪点的常用方法，并以短时进程的跟踪为例，通过 bpftrace、BCC 和 libbpf 等三种方法实现了短时进程的跟踪程序。学完这些内容，我想你已经可以根据自己的实际需求，查询到内核跟踪点或内核函数，并自己开发一个 eBPF 内核跟踪程序。</p><p>也许你想问：我们能不能利用与跟踪内核状态类似的方法，去跟踪用户空间的进程呢？答案是肯定的。只要把内核态跟踪使用的&nbsp;<code>kprobe</code>&nbsp;和&nbsp;<code>tracepoint</code>&nbsp;替换成&nbsp;<code>uprobe</code>&nbsp;，或者用户空间定义的静态跟踪点（User Statically Defined Tracing，简称 USDT），并找出用户进程需要跟踪的函数，作为 eBPF 程序的挂载点，你就可以去跟踪用户进程的内部状态。</p><p>那具体该怎么做呢？今天，我就带你一起来看看，如何使用 eBPF 去跟踪用户进程的执行状态。</p><h2>如何查询用户进程跟踪点？</h2><p>在 <a href=\"https://time.geekbang.org/column/article/484207\">07讲</a> 中我曾提到，在跟踪内核的状态之前，你需要利用内核提供的调试信息查询内核函数、内核跟踪点以及性能事件等。类似地，在跟踪应用进程之前，你也需要知道<strong>这个进程所对应的二进制文件中提供了哪些可用的跟踪点</strong>。那么，从哪里可以找到这些信息呢？如果你使用 GDB 之类的应用调试过程序，这时应该已经想到了，那就是<strong>应用程序二进制文件中的调试信息</strong>。</p><!-- [[[read_end]]] --><p>在静态语言的编译过程中，通常你可以加上&nbsp;<code>-g</code>&nbsp;选项保留调试信息。这样，源代码中的函数、变量以及它们对应的代码行号等信息，就以&nbsp;<a href=\"https://dwarfstd.org/\">DWARF</a>（Debugging With Attributed Record Formats，Linux 和类 Unix 平台最主流的调试信息格式）格式存储到了编译后的二进制文件中。</p><p>有了调试信息，你就可以通过&nbsp;<a href=\"https://man7.org/linux/man-pages/man1/readelf.1.html\">readelf</a>、<a href=\"https://man7.org/linux/man-pages/man1/objdump.1.html\">objdump</a>、<a href=\"https://man7.org/linux/man-pages/man1/nm.1.html\">nm</a>&nbsp;等工具，查询可用于跟踪的函数、变量等符号列表。比如，我经常使用 <code>readelf</code> 命令，查询二进制文件的基本信息。在终端中执行下面的命令，就可以查询&nbsp;<a href=\"https://www.gnu.org/software/libc/\">libc</a>&nbsp;动态链接库中的符号表：</p><pre><code class=\"language-bash\"># 查询符号表（RHEL8系统中请把动态库路径替换为/usr/lib64/libc.so.6）\nreadelf -Ws /usr/lib/x86_64-linux-gnu/libc.so.6\n\n# 查询USDT信息（USDT信息位于ELF文件的notes段）\nreadelf -n /usr/lib/x86_64-linux-gnu/libc.so.6\n</code></pre><p>当然，我们&nbsp;<a href=\"https://time.geekbang.org/column/article/484207\">07讲</a>&nbsp;中提到的 bpftrace 工具也可以用来查询 uprobe 和 USDT 跟踪点，其查询格式如下所示（同样支持&nbsp;<code>*</code>&nbsp;通配符过滤）：</p><pre><code class=\"language-bash\"># 查询uprobe（RHEL8系统中请把动态库路径替换为/usr/lib64/libc.so.6）\nbpftrace -l 'uprobe:/usr/lib/x86_64-linux-gnu/libc.so.6:*'\n\n# 查询USDT\nbpftrace -l 'usdt:/usr/lib/x86_64-linux-gnu/libc.so.6:*'\n</code></pre><p>同内核跟踪点类似，你也可以加上&nbsp;<code>-v</code>&nbsp;选项查询用户探针的参数格式。不过需要再次强调的是，<strong>想要通过二进制文件查询符号表和参数定义，必须在编译的时候保留 DWARF 调试信息</strong>。</p><p>除了符号表之外，理论上你可以把 uprobe 插桩到二进制文件的任意地址。不过这要求你对应用程序 ELF 格式的地址空间非常熟悉，并且具体的地址会随着应用的迭代更新而发生变化。所以，在需要跟踪地址的场景中，一定要记得去 ELF 二进制文件动态获取地址信息。</p><p>另外需要提醒你的是，<strong>uprobe 是基于文件的。当文件中的某个函数被跟踪时，除非对进程 PID 进行了过滤，默认所有使用到这个文件的进程都会被插桩。</strong></p><h2>编程语言会影响 eBPF 的跟踪吗？</h2><p>如果你了解过不同的编程语言，看到这里你肯定会想到，这些查询跟踪点信息的方法很可能是跟编程语言相关的。C 语言和 Python 语言是我们课程的学习基础，也是前面几讲的案例中反复使用的编程语言，我相信你至少已经对它们有了简单的了解。这里我们对比下这两种编程语言在运行方式上的区别：</p><ul>\n<li>C 语言程序需要编译为二进制文件之后再执行，编译时加上&nbsp;<code>-g</code>&nbsp;选项就可以在最终的二进制文件中保留调试信息；</li>\n<li>Python 语言程序则是一个文本文件，不需要 C 语言那样的编译过程，而是由 Python 解释器进行语法分析后执行。因而，上述<code>readelf</code>&nbsp;等工具没法从&nbsp;<code>python</code>&nbsp;二进制文件中直接读取到应用程序的调试信息。</li>\n</ul><p>当然，C 和 Python 只是我们课程中使用的两种编程语言，实际的编程语言种类要多得多。如果把常用的编程语言进行归类，按照其运行原理，我认为大致上可以分为三类：</p><ul>\n<li>第一类是 C、C++、Golang 等编译为机器码后再执行的<strong>编译型语言</strong>。这类编程语言开发的程序，通常会编译成 ELF 格式的二进制文件，包含了保存在寄存器或栈中的函数参数和返回值，因而可以直接通过二进制文件中的符号进行跟踪。</li>\n<li>第二类是 Python、Bash、Ruby 等通过解释器语法分析之后再执行的<strong>解释型语言</strong>。这类编程语言开发的程序，无法直接从语言运行时的二进制文件中获取应用程序的调试信息，通常需要跟踪解释器的函数，再从其参数中获取应用程序的运行细节。</li>\n<li>最后一类是 Java、.Net、JavaScript 等先编译为字节码，再由即时编译器（JIT）编译为机器码执行的<strong>即时编译型语言</strong>。同解释型语言类似，这类编程语言无法直接从语言运行时的二进制文件中获取应用程序的调试信息。跟踪 JIT 编程语言开发的程序是最困难的，因为 JIT 编译的状态只存在于内存中。</li>\n</ul><p>对比这几类编程语言，你可以发现，编程语言的类型对 eBPF 跟踪也有非常大的影响，不同类型编程语言开发的应用程序，其跟踪过程和难度也不相同。接下来，我就用几个案例带你一起来看看每一类编程语言的跟踪方法。</p><h2>跟踪编译型语言应用程序</h2><p>由于可以直接从调试信息中获取到符号（用于跟踪函数）和帧指针（用于跟踪调用栈），编译型语言开发的应用程序是比较容易跟踪的。</p><p>不过需要注意的是，大部分编译型语言遵循&nbsp;<a href=\"https://en.wikipedia.org/wiki/Application_binary_interface\">ABI（Application Binary Interface）</a>&nbsp;调用规范，函数的参数和返回值都存放在寄存器中。而 Go 1.17 之前使用的是&nbsp;<a href=\"https://9p.io/sys/doc/asm.html\">Plan 9</a>&nbsp;调用规范，函数的参数和返回值都存放在堆栈中；直到 1.17， Go 才从 Plan 9 切换到 ABI 调用规范。所以，在跟踪函数参数和返回值时，你需要<strong>首先区分编程语言的调用规范</strong>，然后再去寄存器或堆栈中读取函数的参数和返回值。</p><p>此外，<strong>调试信息并非一定要内置于最终分发的应用程序二进制文件中，它们也可以放到独立的调试文件存储</strong>。为了减少应用程序二进制文件的大小，通常会把调试信息从二进制文件中剥离出来，保存到&nbsp;<code>&lt;应用名&gt;.debuginfo</code>&nbsp;或者&nbsp;<code>&lt;build-id&gt;.debug</code>&nbsp;文件中，后续排查问题需要用到时再安装。</p><p>比如，在 RHEL 和 Ubuntu 等常见的 Linux 发行版中，调试信息跟应用程序通常是两个不同的软件包。而对很多开发者来说，每次编译和发布应用程序之前，通常都需要执行一下&nbsp;<code>strip</code>&nbsp;命令，把调试信息删除后才发布到生产环境中。</p><p>这里给你个小提示：ELF 符号表包含&nbsp;<code>.symtab</code>（应用本地的符号）和&nbsp;<code>.dynsym</code>（调用到外部的符号），<code>strip</code> 命令实际上只是删除了&nbsp;<code>.symtab</code>&nbsp;的内容。</p><p>接下来，我就以每个 Linux 用户都会使用的 Bash&nbsp;为例（Bash 是一个典型的 C 语言程序），带你一起看看，如何跟踪编译型语言应用程序的执行状态。</p><p>在服务器的维护过程中，系统维护人员都需要审计每个用户登录后都执行了哪些命令，以便事后排查问题时参考。由于登录后所有命令的执行都发生在 Bash 中，那么，有没有可能使用 eBPF 来跟踪 Bash 里面到底执行过什么命令呢？答案是肯定的。</p><p>在跟踪 Bash 之前，首先执行下面的命令，安装它的调试信息：</p><pre><code class=\"language-bash\"># Ubuntu\nsudo apt install bash-dbgsym\n\n# RHEL\nsudo debuginfo-install bash\n</code></pre><p>有了 Bash 调试信息之后，再执行下面的几步，查询 Bash 的符号表：</p><pre><code class=\"language-bash\"># 第一步，查询 Build ID（用于关联调试信息）\nreadelf -n /usr/bin/bash | grep 'Build ID'\n# 输出示例为：\n#     Build ID: 7b140b33fd79d0861f831bae38a0cdfdf639d8bc\n\n# 第二步，找到调试信息对应的文件（调试信息位于目录/usr/lib/debug/.build-id中，上一步中得到的Build ID前两个字母为目录名）\nls /usr/lib/debug/.build-id/7b/140b33fd79d0861f831bae38a0cdfdf639d8bc.debug\n\n# 第三步，从调试信息中查询符号表\nreadelf -Ws /usr/lib/debug/.build-id/7b/140b33fd79d0861f831bae38a0cdfdf639d8bc.debug\n</code></pre><p>参考 Bash 的<a href=\"https://git.savannah.gnu.org/cgit/bash.git/tree/lib/readline/readline.c?h=bash-5.1#n352\">源代码</a>，每条 Bash 命令在运行前，都会调用&nbsp;<code>char</code><em><code>readline (const char</code></em><code>prompt)</code>&nbsp;函数读取用户的输入，然后再去解析执行（Bash 自身是使用编译型语言 C 开发的，而 Bash 语言则是一种解释型语言）。</p><p>注意，<code>readline</code>&nbsp;函数的参数是命令行提示符（通过环境变量&nbsp;<code>PS1</code>、<code>PS2</code> 等设置），而返回值才是用户的输入。因而，我们只需要跟踪&nbsp;<code>readline</code>&nbsp;函数的返回值，也就是使用&nbsp;<code>uretprobe</code>&nbsp;跟踪。</p><p>bpftrace、BCC 以及 libbpf 等工具均支持 <code>uretprobe</code>，因而最简单的跟踪方法就是使用 bpftrace 的单行命令：</p><pre><code class=\"language-bash\">sudo bpftrace -e 'uretprobe:/usr/bin/bash:readline { printf(\"User %d executed \\\"%s\\\" command\\n\", uid, str(retval)); }'\n</code></pre><p>这个命令中具体内容的作用如下：</p><ul>\n<li><code>uretprobe:/usr/bin/bash:readline</code>&nbsp;设置跟踪类型为&nbsp;<code>uretprobe</code>，跟踪的二进制文件为&nbsp;<code>/usr/bin/bash</code>，跟踪符号为&nbsp;<code>readline</code>；</li>\n<li>中括号里的内容为 uretprobe 的处理函数；</li>\n<li>处理函数中，<code>uid</code>&nbsp;和&nbsp;<code>retval</code>&nbsp;是两个内置变量，分别表示用户 UID 以及返回值；</li>\n<li><code>str</code> 用于从指针中读取字符串， <code>str(retval)</code> 就是 Bash 中输入命令的字符串；</li>\n<li><code>printf</code> 用于向终端中打印一个字符串。</li>\n</ul><p>打开一个终端，并在新终端中执行&nbsp;<code>ps</code>&nbsp;命令，然后就会在第一个终端中看到如下的输出（即 UID 为 1000 的用户执行了 <code>ps</code> 命令）：</p><pre><code class=\"language-plain\">Attaching 1 probe...\nUser 1000 executed \"ps\" command\n</code></pre><p>和上一讲的案例类似，这里我们也可以使用 BCC 和 libbpf 实现相同的跟踪程序。由于它们的实现逻辑是类似的，并且 BCC 提供了对初学者更友好的接口，所以接下来，我就以 BCC 为例，带你一起看看如何来跟踪用户空间的 Bash。</p><p>还记得用 BCC 来开发一个 eBPF 跟踪程序的具体步骤吗？如果你不记得，也没关系，我来带你简单回顾一下。BCC 的使用可以分为两部分：</p><ul>\n<li>第一部分是用 C 语言开发的 eBPF 程序。在 eBPF 程序中，你可以利用 BCC 提供的<a href=\"https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md\">库函数和宏定义</a>简化你的处理逻辑。</li>\n<li>第二部分是用 Python 语言开发的前端界面，其中包含 eBPF 程序加载、挂载到内核函数和跟踪点，以及通过 BPF 映射获取和打印执行结果等部分。在前端程序中，你同样可以利用 BCC 库来访问 BPF 映射。</li>\n</ul><p>对于第一部分来说，我们要做的就是从 uretprobe 的处理函数中获取&nbsp;<code>readline</code>&nbsp;的返回值，然后提交到性能事件映射中。这里你可能想问：如何获取返回值呢？是不是已经有库函数可以直接拿过来用呢？对此，特别提醒你一点：<strong>当碰到不懂的问题，特别是不清楚接口调用的具体格式时，不要去互联网搜索，而是应该查询官方文档（或者源代码）<strong><strong>来</strong></strong>确认。</strong></p><p>对于 BCC 的 uretprobe 来说，其官方文档链接就是 <a href=\"https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md#5-uretprobes\">uretprobes</a>。根据这里的文档，uretprobe 处理函数的定义格式应该为&nbsp;<code>function_name(struct pt_regs *ctx)</code>，而返回值可以通过宏&nbsp;<code>PT_REGS_RC(ctx)</code>&nbsp;来获取（实际上，kretprobe 也是采用相同的格式）。有了这些文档，就可以按照这里的函数格式来完成第一部分的开发了。代码如下所示，每一步我都加了详细的注释：</p><pre><code class=\"language-c++\">// 包含头文件\n#include &lt;uapi/linux/ptrace.h&gt;\n\n// 定义数据结构和性能事件映射\nstruct data_t {\n    u32 uid;\n    char command[64];\n};\nBPF_PERF_OUTPUT(events);\n\n// 定义uretprobe处理函数\nint bash_readline(struct pt_regs *ctx)\n{\n    // 查询uid\n    struct data_t data = { };\n    data.uid = bpf_get_current_uid_gid();\n\n    // 从PT_REGS_RC(ctx)读取返回值\n    bpf_probe_read_user(&amp;data.command, sizeof(data.command), (void *)PT_REGS_RC(ctx));\n\n    // 提交性能事件\n    events.perf_submit(ctx, &amp;data, sizeof(data));\n    return 0;\n}\n</code></pre><p>从代码中你可以看到，eBPF 程序的结构跟上一讲基本类似，唯一需要注意的是函数的定义格式以及返回值的读取位置。将上述代码保存到&nbsp;<code>bashreadline.c</code>&nbsp;文件中，我们就完成了第一部分的开发。</p><p>有了 eBPF 程序之后，第二部分的 Python 前端也比较直观，代码如下所示：</p><pre><code class=\"language-python\"># 引入BCC库\nfrom bcc import BPF\nfrom time import strftime\n\n# 加载eBPF 程序\nb = BPF(src_file=\"bashreadline.c\")\n\n# 挂载uretprobe\nb.attach_uretprobe(name=\"/usr/bin/bash\", sym=\"readline\", fn_name=\"bash_readline\")\n\n# 定义性能事件回调（输出时间、UID以及Bash中执行的命令）\ndef print_event(cpu, data, size):\n    event = b[\"events\"].event(data)\n    print(\"%-9s %-6d %s\" % (strftime(\"%H:%M:%S\"), event.uid, event.command.decode(\"utf-8\")))\n\n# 打印头\nprint(\"%-9s %-6s %s\" % (\"TIME\", \"UID\", \"COMMAND\"))\n\n# 绑定性能事件映射和输出函数，并从映射中循环读取数据\nb[\"events\"].open_perf_buffer(print_event)\nwhile 1:\n    try:\n        b.perf_buffer_poll()\n    except KeyboardInterrupt:\n        exit()\n</code></pre><p>可以看到，这部分的代码跟上一讲案例的结构也是类似的。主要的不同点在于，挂载 uretprobe 的时候调用了&nbsp;<code>b.attach_uretprobe()</code>&nbsp;，并在其参数中传入了二进制文件的路径&nbsp;<code>name=\"/usr/bin/bash\"</code>&nbsp;，以及要跟踪的符号&nbsp;<code>sym=\"readline\"</code>。</p><p>在 BCC 的内部，当你挂载 uprobe 或者 uretprobe 到用户程序时，由于同一个符号可能出现多次，BCC 会首先查询该符号的所有地址，然后把 eBPF 程序挂载到所有不同的地址上。</p><p>如果你没有使用 BCC 等库，你就需要自己来实现类似的逻辑。而要实现它，还需要详细了解 ELF 文件格式，以及对应库函数的使用方法，这个难度就要大很多了。这也是我推荐你使用 BCC、libbpf 等库进行 eBPF 程序开发的原因。这些库已经帮你把很多繁杂且跟 eBPF 事件处理主要逻辑无关的部分实现好了，你只需要调用它们实现你最核心的 eBPF 处理逻辑即可。</p><p>将上述代码保存到&nbsp;<code>bashreadline.py</code>&nbsp;，然后执行&nbsp;<code>sudo python3 bashreadline.py</code>&nbsp;，我们就可以运行这个跟踪程序。打开一个新终端并运行&nbsp;<code>ls</code>&nbsp;命令，回到第一个终端，你就可以看到如下的输出：</p><pre><code class=\"language-plain\">TIME      UID    COMMAND\n07:17:07  1000   ls\n</code></pre><p>从这里的例子中，你可以发现：编译型语言应用程序的跟踪与内核的跟踪是类似的，只不过是把跟踪类型从 kprobe 换成了 uprobe 或者 USDT（USDT 的例子我会在接下来的内容中讲到）。不同的地方在于符号信息：应用程序的符号信息可以存放在 ELF 二进制文件中，也可以以单独文件的形式，放到调试文件中；而内核的符号信息除了可以存放到内核二进制文件中之外，还会以&nbsp;<code>/proc/kallsyms</code>&nbsp;和&nbsp;<code>/sys/kernel/debug</code>&nbsp;等形式暴露到用户空间。</p><h2>跟踪解释型语言应用程序</h2><p>了解了编译型语言应用程序的跟踪方法之后，接下来，我们再来看看解释型语言应用程序又该如何跟踪。</p><p>上面我已经提到，你无法从解释型语言的二进制文件中直接获取应用程序的调试信息，而只能获得解释器本身的符号信息。所以，对于这类语言开发的应用程序，通常需要跟踪解释器内的函数，再从其参数中获取应用程序的运行细节。</p><p>对于各种解释型编程语言的二进制文件（如 Python、PHP 等），你可以使用类似编译型语言应用程序的跟踪点查询方法，查询它们在解释器层面的 uprobe 和 USDT 跟踪点。比如，对于 Python3 来说，你可以执行下面的命令查询：</p><pre><code class=\"language-bash\">sudo bpftrace -l '*:/usr/bin/python3:*'\n</code></pre><p>命令执行后，你会得到 1500 多个跟踪点。接下来的难点在于，<strong>如何从这些解释器的跟踪点中找出应用程序的函数信息</strong>，所以你需要对解释器的运行原理有一定的了解。</p><p>既然我们这门课中的案例大量使用了 Python 这种解释型编程语言，那我们下面就来试试，能不能从 Python 解释器的跟踪点中，跟踪应用程序的函数执行过程。</p><p>实际上，根据 Python&nbsp;<a href=\"https://docs.python.org/zh-cn/3/howto/instrumentation.html\">文档</a>，为其开启 USDT 跟踪点（编译选项为&nbsp;<code>--with-dtrace</code>）之后，Python3 二进制文件中就会包含一系列的 USDT 跟踪点。这些跟踪点也可以通过 bpftrace 查询到：</p><pre><code class=\"language-bash\">$ bpftrace -l '*:/usr/bin/python3:*'\nusdt:/usr/bin/python3:python:audit\nusdt:/usr/bin/python3:python:function__entry\nusdt:/usr/bin/python3:python:function__return\nusdt:/usr/bin/python3:python:gc__done\nusdt:/usr/bin/python3:python:gc__start\nusdt:/usr/bin/python3:python:import__find__load__done\nusdt:/usr/bin/python3:python:import__find__load__start\nusdt:/usr/bin/python3:python:line\n</code></pre><p>其中，跟函数调用相关的正是&nbsp;<code>function__entry</code>&nbsp;和&nbsp;<code>function__return</code>，因而它们就可以用来跟踪函数的调用过程。根据 Python&nbsp;<a href=\"https://docs.python.org/zh-cn/3/howto/instrumentation.html#available-static-markers\">文档</a>，这两个函数的定义格式为：</p><pre><code class=\"language-plain\">// 三个参数分别是文件名、函数名和行号\nfunction__entry(str filename, str funcname, int lineno)\nfunction__return(str filename, str funcname, int lineno)\n</code></pre><p>有了跟踪点的定义格式，我们就可以使用 eBPF 来跟踪这些函数。比如，对 <code>function__entry</code> 来说，执行下面的 bpftrace 单行命令，就可以跟踪 Python 函数的调用信息：</p><pre><code class=\"language-bash\">sudo bpftrace -e 'usdt:/usr/bin/python3:function__entry { printf(\"%s:%d %s\\n\", str(arg0), arg2, str(arg1))}'\n</code></pre><p>在这个命令中， <code>arg0</code>、 <code>arg1</code> 、 <code>arg2</code> 表示函数的三个参数。这条命令的含义就是把这几个参数拼接成 <code>文件名:行号 函数名</code> 的格式，然后再打印到终端上。</p><p>打开一个新终端，并执行下面的 Python 命令开启一个 http 服务：</p><pre><code class=\"language-bash\">python3 -m http.server 8080\n</code></pre><p>然后，再回到第一个终端，就可以看到如下的输出：</p><pre><code class=\"language-plain\">...\n/usr/lib/python3.9/socketserver.py:254 service_actions\n/usr/lib/python3.9/selectors.py:403 select\n/usr/lib/python3.9/socketserver.py:254 service_actions\n/usr/lib/python3.9/selectors.py:403 select\n</code></pre><p>恭喜，到这里，你已经成功从 Python 解释器中跟踪到了应用程序中的函数调用。</p><p>对于其他的解释型编程语言，其跟踪过程也是类似的，只是要把跟踪函数换成该语言解释器中处理函数调用的跟踪点。如果你不了解它们底层的实现原理，可以参考 BCC 的&nbsp;<a href=\"https://github.com/iovisor/bcc/blob/b82de2db5eece171decc5205f8a426cf8790d19e/tools/lib/ucalls.py#L60-L109\">ucalls</a>&nbsp;和&nbsp;<a href=\"https://github.com/iovisor/bcc/blob/master/tools/lib/uflow.py\">uflow</a>&nbsp;，开发针对它们的跟踪程序。</p><p>刚才我们使用的是 bpftrace，如果你想使用 BCC 和 libbpf 来开发更复杂的跟踪功能也是没问题的。比如，还是以 BCC 为例，它的跟踪过程与编译型语言相比，主要有两个不同点，下面我们来具体看看。</p><p>第一，在 eBPF 程序部分，USDT 跟踪点需要调用&nbsp;<code>bpf_usdt_readarg()</code>&nbsp;函数，来读取函数参数（对于指针类数据，还需调用&nbsp;<code>bpf_probe_read_user()</code>&nbsp;读取指针指向的内容），代码如下所示：</p><pre><code class=\"language-c++\">// 头文件引用和数据结构定义...\n\nint print_functions(struct pt_regs *ctx)\n{\n    uint64_t argptr;\n    struct data_t data = { };\n\n  // 参数1是文件名\n    bpf_usdt_readarg(1, ctx, &amp;argptr);\n    bpf_probe_read_user(&amp;data.filename, sizeof(data.filename),\n                (void *)argptr);\n\n  // 参数2是函数名\n    bpf_usdt_readarg(2, ctx, &amp;argptr);\n    bpf_probe_read_user(&amp;data.funcname, sizeof(data.funcname),\n                (void *)argptr);\n\n  // 参数3是行号\n  bpf_usdt_readarg(3, ctx, &amp;data.lineno);\n\n    // 最后提交性能事件\n    events.perf_submit(ctx, &amp;data, sizeof(data));\n    return 0;\n};\n</code></pre><p>第二，在 Python 前端程序中，挂载跟踪点时需要换成 USDT 跟踪点，即：</p><pre><code class=\"language-python\">from bcc import BPF, USDT\n\nu = USDT(pid=pid)\nu.enable_probe(probe=\"function__entry\", fn_name=\"print_functions\")\nb = BPF(src_file=\"&lt;ebpf-program&gt;.c\", usdt_contexts=[u])\n\n# 其他处理逻辑\n</code></pre><p>除了这两个不同点，其他的逻辑都是类似的。那么，参考上述的编译型语言跟踪程序，你能在这两个不同点的基础上，写出完整的 BCC 跟踪程序吗？这里你可以先想一想，这也是今天留给你的思考题。欢迎你在课后跟我分享你的思路和实现方法。</p><h2><strong>跟踪即时编译型语言应用程序</strong></h2><p>除了上面两类编程语言，还有一种是 Java、.Net 等即时编译型语言。对于这类编程语言开发的应用程序，应用源代码会先编译为字节码，再由即时编译器（JIT）编译为机器码执行。以 Java 为例，Java 虚拟机（JVM）除了会执行常规的 JIT 即时编译之外，还会在执行过程中对运行流程进行剖析和优化，因而也加大了跟踪的难度。</p><p>同解释型编程语言类似，uprobe 和 USDT 跟踪只能用在即时编译器上，从即时编译器的跟踪点参数里面获取最终应用程序的函数信息。由于 USDT 跟踪点比 uprobe 更为稳定，如果编程语言提供了 USDT 跟踪功能，我推荐打开 USDT 跟踪（比如 Java 需要打开&nbsp;<code>--enable-dtrace</code>&nbsp;编译选项），再利用 USDT 而不是 uprobe 去跟踪应用的执行过程。</p><p>要找出即时编译器的跟踪点同应用程序运行之间的关系，就需要你对编程语言的底层运行原理非常熟悉，这也是跟踪即时编译型语言应用程序最难的一步。不过这一步梳理清楚之后，具体的跟踪步骤与解释型编程语言应用程序是类似的。</p><p>如果你需要跟踪这类编程语言开发的应用，可以参考 BCC 提供的一系列<a href=\"https://github.com/iovisor/bcc/tree/master/tools/lib\">用户态跟踪库</a>，这里面已经包含了常见的几种编程语言的适配，具体的跟踪步骤我在这里就不展开了。</p><h2><strong>小结</strong></h2><p>今天，我带你一起梳理了应用进程的跟踪方法，并以 Bash 和 Python 这两种编程语言为例，带你开发了它们的 uprobe 和 USDT 等不同类型的跟踪程序。</p><p>在跟踪应用进程之前，你需要先获取这个进程所对应的二进制文件的跟踪点，以及这些跟踪点同应用程序函数的对应关系。<strong>这个对应关系依赖于编程语言的类型</strong>：编译型语言开发的程序中直接包含了应用程序的符号信息，因而可以直接拿来跟踪；而解释型语言和即时编译型语言的二进制文件，只包含了解释器或即时编译器的符号信息，所以应用程序的运行状态还需要从解释器或即时编译器跟踪的参数中去获取。</p><p>在这一讲结束之前，我还想提醒你：<strong>用户进程的跟踪</strong><strong>，</strong><strong>本质上是通过断点去执行 uprobe 处理程序。</strong>虽然内核社区已经对 BPF 做了很多的性能调优，跟踪用户态函数（特别是锁争用、内存分配之类的高频函数）还是有可能带来很大的性能开销。因此，我们在使用 uprobe 时，应该尽量避免跟踪高频函数。</p><h2>思考题</h2><p>在跟踪 Python 语言程序的 BCC 案例中，我列出了它在使用 USDT 跟踪时，相对于编译型语言的两个不同点：</p><ul>\n<li>eBPF 程序中需要调用&nbsp;<code>bpf_usdt_readarg()</code>&nbsp;来读取参数；</li>\n<li>Python 前端中需要挂载 USDT 跟踪点。</li>\n</ul><p>根据这些提示，以及上面的编译型语言 BCC 跟踪程序，你能试着写出完整的 BCC 跟踪程序吗？欢迎在评论区和我分享你的思路和解决方法。</p><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"08｜内核跟踪（下）：开发内核跟踪程序的进阶方法","id":484372},"right":{"article_title":"10 | 网络跟踪：如何使用 eBPF 排查网络问题？","id":484686}}},{"article_id":484686,"article_title":"10 | 网络跟踪：如何使用 eBPF 排查网络问题？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你一起梳理了应用进程的跟踪方法。根据编程语言的不同，从应用程序二进制文件的跟踪点中可以获取的信息也不同：编译型语言程序中的符号信息可以直接拿来跟踪应用的执行状态，而对于解释型语言和即时编译型语言程序，我们只能从解释器或即时编译器的跟踪点参数中去获取应用的执行状态。</p><p>除了前面三讲提到的内核和应用程序的跟踪之外，网络不仅是 eBPF 应用最早的领域，也是目前 eBPF 应用最为广泛的一个领域。随着分布式系统、云计算和云原生应用的普及，网络已经成为了大部分应用最核心的依赖，随之而来的网络问题也是最难排查的问题之一。</p><p>那么，eBPF 能不能协助我们更好地排查和定位网络问题呢？我们又该如何利用 eBPF 来排查网络相关的问题呢？今天，我就带你一起具体看看。</p><h2>eBPF 提供了哪些网络功能？</h2><p>既然想要使用 eBPF 排查网络问题，我想进入你头脑的第一个问题就是：eBPF 到底提供了哪些网络相关的功能框架呢？</p><p>要回答这个问题，首先要理解 Linux 网络协议栈的基本原理。下面是一个简化版的内核协议栈示意图：</p><p><img src=\"https://static001.geekbang.org/resource/image/ce/a5/ce9371b7e3b696feaf4233c1595e20a5.jpg?wh=1920x2960\" alt=\"图片\" title=\"Linux内核协议栈\"></p><p>如何理解这个网络栈示意图呢？从上往下看，就是应用程序发送网络包的过程；而反过来，从下往上看，则是网络包接收的过程。比如，从上到下来看这个网络栈，你可以发现：</p><!-- [[[read_end]]] --><ul>\n<li>应用程序会通过系统调用来跟套接字接口进行交互；</li>\n<li>套接字的下面，就是内核协议栈中的传输层、网络层和网络接口层，这其中也包含了网络过滤（Netfilter）和流量控制（Traffic Control）框架；</li>\n<li>最底层，则是网卡驱动程序以及物理网卡设备。</li>\n</ul><p>理解了网络协议栈的基本流程，eBPF 提供的网络功能也就容易理解了。如下图所示，eBPF 实际上提供了贯穿整个网络协议栈的过滤、捕获以及重定向等丰富的网络功能：</p><p><img src=\"https://static001.geekbang.org/resource/image/80/50/80c2a8fe3b36ee2fb033b4332431f750.jpg?wh=2284x2197\" alt=\"\" title=\"内核协议栈 BPF 挂载点\"></p><p>一方面，网络协议栈也是内核的一部分，因而<strong>网络相关的内核函数、跟踪点以及用户程序的函数等，也都可以使用前几讲我们提到的 kprobe、uprobe、USDT 等跟踪类 eBPF 程序进行跟踪</strong>（如上图中紫色部分所示）。</p><p>另一方面，回顾一下 <a href=\"https://time.geekbang.org/column/article/483364\">06 讲</a> 的内容，<strong>eBPF 提供了大量专用于网络的 eBPF 程序类型，包括 XDP 程序、TC 程序、套接字程序以及 cgroup 程序等</strong>。这些类型的程序涵盖了从网卡（如卸载到硬件网卡中的 XDP 程序）到网卡队列（如 TC 程序）、封装路由（如轻量级隧道程序）、TCP 拥塞控制、套接字（如 sockops 程序）等内核协议栈，再到同属于一个 cgroup 的一组进程的网络过滤和控制，而这些都是内核协议栈的核心组成部分（如上图中绿色部分所示）。</p><p>从这两个方面，你可以发现 eBPF 已经涵盖了内核协议栈的很多方面，并且内核社区在网络方面的功能还在不断丰富中。</p><p>接下来，我就以最常见的网络丢包为例，带你看看如何使用 eBPF 来排查网络问题。</p><h2>如何跟踪内核网络协议栈？</h2><p>即使理解了内核协议栈的基本原理，以及各种类型 eBPF 程序的基本功能，在想要跟踪网络相关的问题时，你可能还是觉得无从下手，这是为什么呢？</p><p>究其原因，我认为最主要是因为<strong>不清楚内核中都有哪些函数和跟踪点可以拿来跟踪</strong>。而即使通过源码查询到了一系列的内核函数，还是没有一个清晰的思路把这些内核函数与所碰到的网络问题关联起来。</p><p>不过，当你碰到这类困惑的时候不要有畏难心理。要知道，所有非内核开发者都会碰到类似的问题，而解决这类问题也并非难事，下面我们就来看解决方法。</p><p>如何把内核函数跟相关的网络问题关联起来呢？看到本小节的标题，你应该已经想到了：跟踪调用栈，<strong>根据调用栈回溯路径</strong><strong>，</strong><strong>找出导致某个网络事件发生的整个流程，进而就可以再根据这些流程中的内核函数进一步跟踪</strong>。</p><p>既然是调用栈的<strong>回溯</strong>，只有我们知道了最接近整个执行逻辑结尾的函数，才有可能开始这个回溯过程。对 Linux 网络丢包问题来说，内核协议栈执行的结尾，当然就是释放最核心的 SKB（Socket Buffer）数据结构。查询内核 <a href=\"https://www.kernel.org/doc/htmldocs/networking/ch01s02.html\">SKB 文档</a>，你可以发现，内核中释放 SKB 相关的函数有两个：</p><ul>\n<li>第一个，<a href=\"https://www.kernel.org/doc/htmldocs/networking/API-kfree-skb.html\">kfree_skb</a> ，它经常在网络异常丢包时调用；</li>\n<li>第二个，<a href=\"https://www.kernel.org/doc/htmldocs/networking/API-consume-skb.html\">consume_skb</a> ，它在正常网络连接完成时调用。</li>\n</ul><p>这两个函数除了使用场景的不同，其功能和实现流程都是一样的，即都是检查 SKB 的引用计数，当引用计数为 0 时释放其内核内存。所以，要跟踪网络丢包的执行过程，也就可以跟踪 <code>kfree_skb</code> 的内核调用栈。</p><p><strong>接下来，我就以访问极客时间的网站 <strong><code>time.geekbang.org</code></strong> 为例，来带你一起看看</strong><strong>，</strong><strong>如何使用 bpftrace 来进行调用栈的跟踪。</strong></p><p>为了方便调用栈的跟踪，bpftrace 提供了 <a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#7-kstack-stack-traces-kernel\">kstack</a> 和 <a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#8-ustack-stack-traces-user\">ustack</a> 这两个内置变量，分别用于获取内核和进程的调用栈。打开一个终端，执行下面的命令就可以跟踪 <code>kfree_skb</code> 的内核调用栈了：</p><pre><code class=\"language-plain\">sudo bpftrace -e 'kprobe:kfree_skb /comm==\"curl\"/ {printf(\"kstack: %s\\n\", kstack);}'\n</code></pre><p>这个命令中的具体内容含义如下：</p><ul>\n<li><code>kprobe:kfree_skb</code> 指定跟踪的内核函数为 <code>kfree_skb</code>；</li>\n<li>紧随其后的 <code>/comm==\"curl\"/</code> ，表示只跟踪 <code>curl</code> 进程，这是为了过滤掉其他不相关的进程操作；</li>\n<li>最后的 <code>printf()</code> 函数就是把内核协议栈打印到终端中。</li>\n</ul><p>打开一个新终端，并在终端中执行 <code>curl time.geekbang.org</code> 命令，然后回到第一个终端，就可以看到如下的输出：</p><pre><code class=\"language-c++\">kstack:\n        kfree_skb+1\n        udpv6_destroy_sock+66\n        sk_common_release+34\n        udp_lib_close+9\n        inet_release+75\n        inet6_release+49\n        __sock_release+66\n        sock_close+21\n        __fput+159\n        ____fput+14\n        task_work_run+103\n        exit_to_user_mode_loop+411\n        exit_to_user_mode_prepare+187\n        syscall_exit_to_user_mode+23\n        do_syscall_64+110\n        entry_SYSCALL_64_after_hwframe+68\n\nkstack:\n        kfree_skb+1\n        udpv6_destroy_sock+66\n        sk_common_release+34\n        udp_lib_close+9\n        inet_release+75\n        inet6_release+49\n        __sock_release+66\n        sock_close+21\n        __fput+159\n        ____fput+14\n        task_work_run+103\n        exit_to_user_mode_loop+411\n        exit_to_user_mode_prepare+187\n        syscall_exit_to_user_mode+23\n        do_syscall_64+110\n        entry_SYSCALL_64_after_hwframe+68\n\nkstack:\n        kfree_skb+1\n        unix_release+29\n        __sock_release+66\n        sock_close+21\n        __fput+159\n        ____fput+14\n        task_work_run+103\n        exit_to_user_mode_loop+411\n        exit_to_user_mode_prepare+187\n        syscall_exit_to_user_mode+23\n        do_syscall_64+110\n        entry_SYSCALL_64_after_hwframe+68\n\nkstack:\n        kfree_skb+1\n        __sys_connect_file+95\n        __sys_connect+162\n        __x64_sys_connect+24\n        do_syscall_64+97\n        entry_SYSCALL_64_after_hwframe+68\n\nkstack:\n        kfree_skb+1\n        __sys_connect_file+95\n        __sys_connect+162\n        __x64_sys_connect+24\n        do_syscall_64+97\n        entry_SYSCALL_64_after_hwframe+68\n</code></pre><p>这个输出包含了多个调用栈，每个调用栈从下往上就是 <code>kfree_skb</code> 被调用过程中的各个函数（函数名后的数字表示调用点相对函数地址的偏移），它们都是从系统调用（<code>entry_SYSCALL_64</code>）开始，通过一系列的内核函数之后，最终调用到了跟踪函数。</p><p>输出中包含多个调用栈，是因为同一个内核函数是有可能在多个地方调用的。因此，我们需要对它进一步改进，加上网络信息的过滤，并把源 IP 和目的 IP 等基本信息也打印出来。比如，我们访问一个网址，只需要关心 TCP 协议，而其他协议相关的内核栈就可以忽略掉。</p><p><code>kfree_skb</code> 函数的定义格式如下所示，它包含一个 <code>struct sk_buff</code> 类型的参数，这样我们就可以从中获取协议、源 IP 和目的 IP 等基本信息：</p><pre><code class=\"language-c++\">void kfree_skb(struct sk_buff * skb);\n</code></pre><p>由于我们需要添加数据结构读取的过程，为了更好的可读性，你可以把这些过程放入一个脚本文件中，通常后缀为 <code>.bt</code>。下面就是一个改进了的跟踪程序：</p><pre><code class=\"language-c++\">kprobe:kfree_skb /comm==\"curl\"/\n{\n  // 1. 第一个参数是 struct sk_buff\n  $skb = (struct sk_buff *)arg0;\n\n  // 2. 从网络头中获取源IP和目的IP\n  $iph = (struct iphdr *)($skb-&gt;head + $skb-&gt;network_header);\n  $sip = ntop(AF_INET, $iph-&gt;saddr);\n  $dip = ntop(AF_INET, $iph-&gt;daddr);\n\n  // 3. 只处理TCP协议\n  if ($iph-&gt;protocol == IPPROTO_TCP)\n  {\n    // 4. 打印源IP、目的IP和内核调用栈\n    printf(\"SKB dropped: %s-&gt;%s, kstack: %s\\n\", $sip, $dip, kstack);\n  }\n}\n</code></pre><p>让我们来看看这个脚本中每一处的具体含义：</p><ul>\n<li>第1处是把 bpftrace 的内置参数 <code>arg0</code> 转换成 SKB 数据结构 <code>struct sk_buff *</code>（注意使用指针）。</li>\n<li>第2处是从 SKB 数据结构中获取网络头之后，再从中拿到源IP和目的IP，最后再调用内置函数 <code>ntop()</code> ，把整数型的 IP 数据结构转换为可读性更好的字符串格式。</li>\n<li>第3处是对网络协议进行了过滤，只保留TCP协议。</li>\n<li>第4处是向终端中打印刚才获取的源IP和目的IP，同时也打印内核调用栈。</li>\n</ul><p>直接把这个脚本保存到文件中，bpftrace 并不能直接运行。你会看到如下的类型未知错误：</p><pre><code class=\"language-plain\">./dropwatch.bt:9:10-28: ERROR: Unknown struct/union: 'struct sk_buff'\n  $skb = (struct sk_buff *)arg0;\n         ~~~~~~~~~~~~~~~~~~\n./dropwatch.bt:12:10-26: ERROR: Unknown struct/union: 'struct iphdr'\n  $iph = (struct iphdr *)($skb-&gt;head + $skb-&gt;network_header);\n         ~~~~~~~~~~~~~~~~\n./dropwatch.bt:13:10-22: ERROR: Unknown identifier: 'AF_INET'\n  $sip = ntop(AF_INET, $iph-&gt;saddr);\n         ~~~~~~~~~~~~\n</code></pre><p>这是因为，bpftrace 在将上述脚本编译为 BPF 字节码的过程中，找不到这些类型的定义。在内核支持 BTF 之前，这其实是所有 eBPF 程序开发过程中都会遇到的一个问题。要解决这个问题，我们就需要把所需的内核头文件引入进来。</p><p>这里给你一个小提示：v0.9.3 或更新版本的 bpftrace 已经支持 BTF 了，但需要新版本的 libbpf，且还有很多的<a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#btf-support\">限制</a>。</p><p>那么，如何找出这些数据结构的头文件呢？我通常使用下面的两种方法：</p><ul>\n<li>第一种方法是在内核源码目录中，通过查找的方式，找出定义了这些数据结构的头文件（后缀为 <code>.h</code>）。</li>\n<li>另外一种方法是到 <a href=\"https://elixir.bootlin.com/\">https://elixir.bootlin.com/</a> 上选择正确的内核版本后，再搜索数据结构的名字。我在下面还会详细介绍这个网站的使用方法。</li>\n</ul><p>我们在脚本文件中加入这些类型定义的头文件：</p><pre><code class=\"language-c++\">#include &lt;linux/skbuff.h&gt;\n#include &lt;linux/ip.h&gt;\n#include &lt;linux/socket.h&gt;\n#include &lt;linux/netdevice.h&gt;\n</code></pre><p>然后，保存到文件 <code>dropwatch.bt</code>中（你也可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bpftrace/dropwatch.bt\">GitHub</a> 中找到完整的程序），就可以通过 <code>sudo bpftrace dropwatch.bt</code> 来运行了。</p><h2>如何排查网络丢包问题？</h2><p>有了 eBPF 跟踪脚本之后，它能不能用来排查网络丢包问题呢？我们来验证一下。</p><p>最常见的丢包是由系统防火墙阻止了相应的 IP 或端口导致的，你可以执行下面的 <code>nslookup</code>命令，查询到极客时间的 IP 地址，然后再执行<code>iptables</code> 命令，禁止访问极客时间的 80 端口：</p><pre><code class=\"language-plain\"># 首先查询极客时间的IP\n$ nslookup time.geekbang.org\nServer:        127.0.0.53\nAddress:    127.0.0.53#53\n\nNon-authoritative answer:\nName:    time.geekbang.org\nAddress: 39.106.233.176\n\n# 然后增加防火墙规则阻止80端口\n$ sudo iptables -I OUTPUT -d 39.106.233.176/32 -p tcp -m tcp --dport 80 -j DROP\n</code></pre><p>防火墙规则加好之后，在终端一中启动跟踪脚本：</p><pre><code class=\"language-plain\">sudo bpftrace dropwatch.bt\n</code></pre><p>然后，新建一个终端，访问极客时间，你应该会看到超时的错误：</p><pre><code class=\"language-plain\">$ curl --connect-timeout 1 39.106.233.176\ncurl: (28) Connection timed out after 1000 milliseconds\n</code></pre><p>返回第一个终端，你就可以看到 eBPF 程序已经成功跟踪到了内核丢包的调用栈信息，如下所示：</p><pre><code class=\"language-plain\">SKB dropped: 192.168.1.129-&gt;39.106.233.176, kstack:\n        kfree_skb+1\n        __ip_local_out+219\n        ip_local_out+29\n        __ip_queue_xmit+367\n        ip_queue_xmit+21\n        __tcp_transmit_skb+2237\n        tcp_connect+1009\n        tcp_v4_connect+951\n        __inet_stream_connect+206\n        inet_stream_connect+59\n        __sys_connect_file+95\n        __sys_connect+162\n        __x64_sys_connect+24\n        do_syscall_64+97\n        entry_SYSCALL_64_after_hwframe+68\n</code></pre><p>从这个输出中，我们可以看到，第一行输出中我们成功拿到了源 IP 和目的 IP，而接下来的每一行中都包含了指令地址、函数名以及函数地址偏移。</p><p>从下往上看这个调用栈，最后调用 <code>kfree_skb</code> 函数的是 <code>__ip_local_out</code>，那么 <code>__ip_local_out</code> 这个函数又是干什么的呢？根据函数名，你可以大致猜测出，它是用于向外发送网络包的，但具体的步骤我们就不太确定了。所以，这时候就需要去参考一下内核源代码。</p><p>这里推荐你使用 <a href=\"https://elixir.bootlin.com/\">https://elixir.bootlin.com/</a> 这个网站来查看内核源码，因为它不仅列出了所有内核版本的源代码，还提供了交叉引用的功能。在源码文件中点击任意函数或类型，它就可以自动跳转到其定义和引用的位置。</p><p>比如，对于 <code>__ip_local_out</code> 函数的定义和引用，就可以通过 <a href=\"https://elixir.bootlin.com/linux/v5.13/A/ident/__ip_local_out\">https://elixir.bootlin.com/linux/v5.13/A/ident/__ip_local_out</a> （请注意把 v5.13 替换成你的内核版本）这个网址来查看。点击<a href=\"https://elixir.bootlin.com/linux/v5.13/A/ident/__ip_local_out\">链接</a>，你会看到如下的界面：</p><p><img src=\"https://static001.geekbang.org/resource/image/98/cc/98891ed6bf8f2d6e273299bba6a3edcc.png?wh=856x614\" alt=\"图片\" title=\"交叉引用搜索结果示意图\"></p><p>查询的结果分为三个部分，分别是头文件中的函数声明、源码文件中的函数定义，以及其他文件的引用。点击中间部分（即上图红框中的第一个链接），就可以跳转到源码的定义位置。</p><p>打开<a href=\"https://elixir.bootlin.com/linux/v5.13/source/net/ipv4/ip_output.c#L99\">代码</a>之后，你会发现，其实并不需要因为不懂内核而担心自己看不懂内核源码，内核中的源码还是很简洁的。这里我把原始代码复制了过来，并且加入了详细的注释，如下所示：</p><pre><code class=\"language-c++\">int __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n    struct iphdr *iph = ip_hdr(skb);\n\n  /* 计算总长度 */\n    iph-&gt;tot_len = htons(skb-&gt;len);\n  /* 计算校验和 */\n    ip_send_check(iph);\n\n    /* L3主设备处理 */\n    skb = l3mdev_ip_out(sk, skb);\n    if (unlikely(!skb))\n        return 0;\n\n  /* 设置IP协议 */\n    skb-&gt;protocol = htons(ETH_P_IP);\n\n  /* 调用NF_INET_LOCAL_OUT钩子 */\n    return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT,\n               net, sk, skb, NULL, skb_dst(skb)-&gt;dev,\n               dst_output);\n}\n</code></pre><p>从这个代码来看，<code>__ip_local_out</code> 函数的主要流程就是计算总长度和校验和，再设置 L3 主设备和协议等属性后，最终调用 <code>nf_hook</code>。</p><p>而 <code>nf</code> 就是 netfilter 的缩写，所以你就可以将其理解为调用 iptables 规则。再根据 <code>NF_INET_LOCAL_OUT</code>参数，你就可以知道接下来调用了 OUTPUT 链（chain）的钩子。</p><p>知道了发生丢包的问题来源，接下来再去定位 iptables 就比较容易了。在终端中执行下面的 iptables 命令，就可以查询 OUTPUT 链的过滤规则：</p><pre><code class=\"language-plain\">sudo iptables -nvL OUTPUT\n</code></pre><p>命令执行后，你应该可以看到类似下面的输出。可以看到，正是我们之前加入的 iptables 规则导致了丢包：</p><pre><code class=\"language-plain\">Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    1   180 DROP       tcp  --  *      *       0.0.0.0/0            39.106.233.176       tcp dpt:80\n</code></pre><p>恭喜，到这里，通过简单的几行 bpftrace 脚本，你就成功使用 eBPF 精确定位了一个常见的网络丢包问题。</p><p>清楚了问题的根源，要解决它当然就很简单了。只要执行下面的命令，把导致丢包的 iptables 规则删除即可：</p><pre><code class=\"language-plain\">sudo iptables -D OUTPUT -d 39.106.233.176/32 -p tcp -m tcp --dport 80 -j DROP\n</code></pre><h2>如何根据函数偏移快速定位源码？</h2><p>在内核栈的输出中，我想你一定注意到每一个函数的输出格式都是<code>函数名+偏移量</code>，而这儿的偏移就是调用下一个函数的位置。那么，能不能根据<code>函数名+偏移量</code>直接定位源码的位置呢？</p><p>答案是肯定的。这是因为，不仅是我们这些 eBPF 学习者想要这种工具，内核开发者为了方便问题的排查，也经常需要根据内核栈，快速定位导致问题发生的代码位置。所以，Linux 内核维护了一个 <a href=\"https://github.com/torvalds/linux/blob/master/scripts/faddr2line\">faddr2line</a> 脚本，根据<code>函数名+偏移量</code>输出源码文件名和行号。你可以点击<a href=\"https://github.com/torvalds/linux/blob/master/scripts/faddr2line\">这里</a>，把它下载到本地，然后执行下面的命令加上可执行权限：</p><pre><code class=\"language-plain\">chmod +x faddr2line\n</code></pre><p>在使用这个脚本之前，你还需要注意两个前提条件：</p><ul>\n<li>第一，带有调试信息的内核文件，一般名字为 vmlinux（注意，/boot 目录下面的 vmlinz 是压缩后的内核，不可以直接拿来使用）。</li>\n<li>第二，系统中需要安装 <code>awk</code>、<code>readelf</code>、<code>addr2line</code>、<code>size</code>、<code>nm</code> 等命令。</li>\n</ul><p>对于第二个条件，这些命令都包含在 <a href=\"https://www.gnu.org/software/binutils/\">binutils</a> 软件包中，只需要执行 <code>apt</code> 或者 <code>dnf</code> 命令安装即可。</p><p>而对第一个条件中的内核调试信息，各个主要的发行版也都提供了相应的软件仓库，你可以根据文档进行安装。比如，对于 Ubuntu 来说，你可以执行下面的命令安装调试信息：</p><pre><code class=\"language-plain\">codename=$(lsb_release -cs)\nsudo tee /etc/apt/sources.list.d/ddebs.list &lt;&lt; EOF\ndeb http://ddebs.ubuntu.com/ ${codename}      main restricted universe multiverse\ndeb http://ddebs.ubuntu.com/ ${codename}-updates  main restricted universe multiverse\nEOF\nsudo apt-get install -y ubuntu-dbgsym-keyring\nsudo apt-get update\nsudo apt-get install -y linux-image-$(uname -r)-dbgsym\n</code></pre><p>而对于 RHEL8 等系统，则可以执行下面的命令：</p><pre><code class=\"language-plain\">sudo debuginfo-install kernel-$(uname -r)\n</code></pre><p>调试信息安装好之后，相关的调试文件会放到 <code>/usr/lib/debug</code> 目录下。不同发行版的目录结构是不同的，但你可以使用下面的命令来搜索 <code>vmlinux</code> 开头的文件：</p><pre><code class=\"language-plain\">find /usr/lib/debug/ -name 'vmlinux*'\n</code></pre><p>以我使用的 Ubuntu 21.10 为例，查找到的文件路径为 <code>/usr/lib/debug/boot/vmlinux-5.13.0-22-generic</code>。所以，接下来，就可以执行下面的命令，对刚才内核栈中的 <code>__ip_local_out+219</code> 进行定位：</p><pre><code class=\"language-plain\">faddr2line /usr/lib/debug/boot/vmlinux-5.13.0-22-generic __ip_local_out+219\n</code></pre><p>命令执行后，可以得到下面的输出：</p><pre><code class=\"language-plain\">__ip_local_out+219/0x150:\nnf_hook at include/linux/netfilter.h:256\n(inlined by) __ip_local_out at net/ipv4/ip_output.c:115\n</code></pre><p>这个输出中的具体内容含义如下：</p><ul>\n<li>第二行表示 <code>nf_hook</code>的定义位置在 <code>netfilter.h</code> 的<code>156</code>行。</li>\n<li>第三行表示 <code>net/ipv4/ip_output.c</code> 的 <code>115</code>行调用了 <code>kfree_skb</code> 函数。但是，由于 <code>nf_hook</code> 是一个内联函数，所以行号<code>115</code>实际上是内联函数 <code>nf_hook</code> 的调用位置。</li>\n</ul><p>对比一下上一个模块查找的<a href=\"https://elixir.bootlin.com/linux/v5.13/source/net/ipv4/ip_output.c#L115\">内核源码</a>，<code>net/ipv4/ip_output.c</code> 的 115 号也刚好是调用 <code>nf_hook</code> 的位置：</p><p><img src=\"https://static001.geekbang.org/resource/image/31/6a/311b02beafc800c2bba434d24aa7b56a.png?wh=703x352\" alt=\"图片\" title=\"__ip_local_out 源码\"></p><p>而再点击 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/netfilter.h#L205\">nf_hook</a> 继续去看它的定义，你可以发现，这的确是个内联函数：</p><pre><code class=\"language-c++\">static inline int nf_hook(...)\n</code></pre><blockquote>\n<p>小提示：内联是一种常用的编程优化技术，它告诉编译器把指定函数展开之后再进行编译，这样就省去了函数调用的开销。对频繁调用的小函数来说，这就可以大大提高程序的运行效率。</p>\n</blockquote><p>有了 faddr2line 工具，在以后排查内核协议栈时，你就可以根据栈中<code>函数名+偏移量</code>，直接定位到源代码的位置。这样，你就可以直接去内核源码或 <a href=\"https://elixir.bootlin.com/\">elixir.bootlin.com</a> 网站中查找相关函数的实现逻辑，进而更深层次地理解内核的实现原理。</p><h2>小结</h2><p>今天，我带你一起梳理了 eBPF 的网络功能，并以最常见的网络丢包问题为例，使用 bpftrace 开发了一个跟踪内核网络协议栈的 eBPF 程序。</p><p>eBPF 不仅诞生于网络过滤，它在网络方面的应用也是最为广泛和活跃的。由于内核协议栈也是内核的核心组成部分，前几讲我们讲到的 kprobe 跟踪、uprobe/USDT 跟踪等，也都可以应用到网络问题的跟踪和排查上来。由于内核协议栈相对比较复杂，在排查网络问题时，我们可以从内核调用栈入手。根据调用栈的执行过程，再配合 faddr2line 这样的工具，你就可以快速定位到问题发生所在的内核源码，进而找出问题的根源。</p><p>实际上，我们今天讲到的调用栈跟踪也可以用到其他内核功能和用户应用的跟踪上，并且也特别适用于性能优化领域经常需要的热点函数跟踪。在后续的课程中，我还将为你介绍更多的应用案例。</p><h2>思考题</h2><p>由于加入了进程信息和网络协议的限制，今天我们使用 bpftrace 开发的 eBPF 程序，实际上只能跟踪到 <code>curl</code>命令发出的 TCP 请求丢包问题。而在实际的应用中，很可能是其他的进程发生了丢包问题，并且丢包的也不一定都是 TCP 协议。</p><p>那么，根据这一讲的内容和 bpftrace 的文档，你可以对今天的跟踪程序进行改进，并把进程信息（如 PID 和进程名）加到输出中吗？欢迎在评论区和我分享你的思路和解决方法。</p><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"09 | 用户态跟踪：如何使用 eBPF 排查应用程序？","id":484458},"right":{"article_title":"11 | 容器安全：如何使用 eBPF 增强容器安全？","id":485101}}},{"article_id":485101,"article_title":"11 | 容器安全：如何使用 eBPF 增强容器安全？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我以最常见的网络丢包为例，带你一起梳理了 eBPF 所提供的网络功能特性，并教你使用 bpftrace 开发了一个跟踪内核网络协议栈的 eBPF 程序。虽然 eBPF 起源于网络过滤，并且网络过滤也是 eBPF 应用最为广泛的一个领域，但其实 eBPF 的应用已经远远超出了这一范畴。故障诊断、网络优化、安全控制、性能监控等，都已是 eBPF 的主战场。</p><p>随着容器和云原生技术的普及，由于容器天生共享内核的特性，容器的安全和隔离就是所有容器平台头上的“紧箍咒”。因此，如何快速定位容器安全问题，如何确保容器的隔离，以及如何预防容器安全漏洞等，是每个容器平台都需要解决的头号问题。</p><p>既然容器是共享内核的，这些安全问题的解决自然就可以从内核的角度进行考虑。除了容器自身所强依赖的命名空间、cgroups、Linux 权限控制 Capabilities 之外，可以动态跟踪和扩展内核的 eBPF 就成为了安全监控和安全控制的主要手段之一。 Sysdig、Aqua Security、Datadog 等业内知名的容器安全解决方案，都基于 eBPF 构建了丰富的安全特性。</p><p>那么，到底如何使用 eBPF 来监控容器的安全问题，又如何使用 eBPF 阻止容器中的恶意行为呢？今天，我就带你一起来看看如何借助 eBPF 来增强容器安全。</p><!-- [[[read_end]]] --><h2>eBPF 都有哪些安全能力？</h2><p>安全这个词通常包含了非常广泛的任务，包括安全问题的分析与诊断、安全事件的检测，以及安全策略的执行等。针对这些任务，eBPF 又提供了哪些安全能力呢？</p><p>首先，对于<strong>安全问题的分析与诊断</strong>，eBPF 无需修改并重新编译内核和应用就可以动态分析内核及应用的行为。这在很多需要保留安全问题现场的情况下非常有用。特别是在紧急安全事件的处理过程中，eBPF 可以实时探测进程或内核中的可疑行为，进而帮你更快地定位安全问题的根源。</p><p>比如，Aqua Security 开源的 <a href=\"https://aquasecurity.github.io/tracee/dev/\">Tracee</a> 项目就利用 eBPF，动态跟踪系统和应用的可疑行为模式，再与不断丰富的特征检测库进行匹配，就可以分析出容器应用中的安全问题。下面是 Tracee 的工作原理示意图（图片来自 <a href=\"https://aquasecurity.github.io/tracee/dev/architecture/\">Tracee 文档</a>）：</p><p><img src=\"https://static001.geekbang.org/resource/image/a9/6f/a97a6b3e5077c13fbcb346f217c0a96f.png?wh=1340x853\" alt=\"图片\" title=\"Tracee 原理示意图\"></p><p>其次，对于<strong>安全事件的监测</strong>，eBPF 的事件触发机制不仅可以用极低的开销，动态监测内核和应用程序中的各类安全事件，更可以避免其他基于采样或统计机制的安全检测系统中经常发生的安全事件漏检问题。</p><p>如下图（图片来自<a href=\"https://www.brendangregg.com/Slides/BSidesSF2017_BPF_security_monitoring.pdf\">brendangregg.com</a>）所示，kprobe、uprobe、tracepoint、USDT 等各类探针已经非常全面地涵盖了从应用到内核中的各类安全问题相关的跟踪点：</p><p><img src=\"https://static001.geekbang.org/resource/image/5e/6e/5e178ebbc7928a0cd2aff7243f24e96e.png?wh=1089x693\" alt=\"图片\" title=\"eBPF 安全跟踪点\"></p><p>比如，Sysdig 贡献给 CNCF 基金会的 <a href=\"https://falco.org/\">Falco</a> 项目，就利用 eBPF 在运行时监测内核和应用中是否发生了诸如特权提升、SHELL 命令执行、系统文件（比如 <code>/etc/passwd</code>）修改、SSH 登录等异常行为，再通过告警系统实时将这些安全事件及时推送给你。</p><p>最后，对于<strong>安全策略的执行</strong>，安全计算（seccomp）、Linux 安全模块（LSM）等 Linux 已有的安全机制，均可以通过 eBPF 来进行安全审计和决策执行，阻止系统和进程中的非法操作，甚至可以通过辅助函数 <code>bpf_send_signal()</code> 直接把有安全隐患的进程杀死。在网络安全方面，eBPF 不仅可以用来实时探测和跟踪网络请求，更可以在探测到网络攻击等危害行为时直接在网络协议栈之前丢弃数据包，从而实现极高的网络性能。</p><p>如下图所示（图片来自 KubeArmor <a href=\"https://docs.kubearmor.com/kubearmor/kubearmor-design\">文档</a>），容器运行时安全系统 <a href=\"https://kubearmor.com/\">KubeArmor</a> 就利用 LSM 和 eBPF，限制容器中进程执行、文件访问、网络连接等各种违反安全策略的操作。</p><p><img src=\"https://static001.geekbang.org/resource/image/49/6d/49bd8dbce937eff08bcc40c12559df6d.png?wh=932x518\" alt=\"图片\" title=\"KubeArmor 工作原理\"></p><h2>如何使用 eBPF 分析容器的安全问题？</h2><p>既然容器还是共享内核的，运行在内核中的 eBPF 程序自然也能够跟踪和分析容器中的应用程序。但由于容器利用 Linux 的 namespace 机制进行了隔离，其跟踪和分析方法又跟直接运行在主机内的进程有些不同。</p><p>以跟踪恶意程序的执行为例，为了躲避安全监控，很多恶意程序并不是在容器一开始启动的时候就运行了恶意进程，而是先启动一个正常程序，之后再创建新的恶意进程。这种场景特别容易出现在容器安全漏洞被恶意软件侵入的场景。</p><p>那么，如何用 eBPF 来分析这类安全问题呢？我想，你可能已经想起来了，我们课程的 <a href=\"https://time.geekbang.org/column/article/484207\">07 讲</a> 其实已经实现了对新进程创建的跟踪程序，即跟踪系统调用 <code>execve</code>。比如，执行下面的 bpftrace 命令，就可以跟踪新创建的进程：</p><pre><code class=\"language-bash\">sudo bpftrace -e 'tracepoint:syscalls:sys_enter_execve { printf(\"%-6d %-8s\", pid, comm); join(args-&gt;argv);}'\n</code></pre><p>打开一个新终端，执行一条 <code>ls</code> 命令，然后你就会看到如下的输出：</p><pre><code class=\"language-plain\">8964   bash    ls --color=auto\n</code></pre><p>接下来，参考 <a href=\"https://docs.docker.com/engine/install/\">Docker 官方文档</a>安装 Docker 之后，再执行下面的命令，启动一个 Ubuntu 容器：</p><pre><code class=\"language-bash\"># -it表示进入容器终端，--rm表示终端关闭后自动清理容器\ndocker run -it --rm --name bash --hostname bash ubuntu:impish\n</code></pre><p>在容器中执行 <code>ls</code> 命令，忽略容器启动过程中的进程跟踪信息（Docker在启动容器过程中也会执行大量的命令），你会看到跟刚才类似的输出：</p><pre><code class=\"language-plain\">9018   bash    ls --color=auto\n</code></pre><p>这个输出跟刚才在主机中执行 <code>ls</code> 后的结果是一样的，只根据这个输出，我们显然没法区分 <code>ls</code> 是不是运行在容器中。</p><p>实际上，虽然所有容器都是共享内核的，但不同的容器之间还是通过命名空间进行了隔离。你可以使用 <code>lsns</code> 命令来查询容器或者主机的命名空间。比如，在刚才的容器终端中执行 <code>lsns</code> 命令，就可以看到如下的输出：</p><pre><code class=\"language-plain\">        NS TYPE   NPROCS PID USER COMMAND\n4026531834 time        2   1 root bash\n4026531835 cgroup      2   1 root bash\n4026531837 user        2   1 root bash\n4026532530 mnt         2   1 root bash\n4026532531 uts         2   1 root bash\n4026532532 ipc         2   1 root bash\n4026532533 pid         2   1 root bash\n4026532535 net         2   1 root bash\n</code></pre><p>关于这些命名空间的含义，如果你还不了解，可以参考<a href=\"https://man7.org/linux/man-pages/man7/namespaces.7.html\">这里</a>的 Linux 手册。</p><p>在内核中，进程的基本信息都保存在 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/sched.h#L657\">task_struct</a> 结构体中，其中也包括了包含命名空间信息的  <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/nsproxy.h#L31\">nsproxy</a> 结构体。<code>nsproxy</code> 结构体的定义如下所示：</p><pre><code class=\"language-c++\">struct nsproxy {\n\tatomic_t count;\n\tstruct uts_namespace *uts_ns;\n\tstruct ipc_namespace *ipc_ns;\n\tstruct mnt_namespace *mnt_ns;\n\tstruct pid_namespace *pid_ns_for_children;\n\tstruct net \t     *net_ns;\n\tstruct time_namespace *time_ns;\n\tstruct time_namespace *time_ns_for_children;\n\tstruct cgroup_namespace *cgroup_ns;\n};\n</code></pre><p>为了区分一个进程是属于容器还是主机，我们可以在跟踪结果中输出 PID 命名空间和 UTS 命名空间中的主机名。</p><p>bpftrace 内置了表示进程结构体的 <a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md#1-builtins\">curtask</a>，因而对前面的 bpftrace 脚本，我们可以进行下面的改进：</p><pre><code class=\"language-c++\">tracepoint:syscalls:sys_enter_execve {\n  /* 1. 获取task_struct结构体 */\n  $task = (struct task_struct *)curtask;\n  /* 2. 获取PID命名空间 */\n  $pidns = $task-&gt;nsproxy-&gt;pid_ns_for_children-&gt;ns.inum;\n  /* 3. 获取主机名 */\n  $cname = $task-&gt;nsproxy-&gt;uts_ns-&gt;name.nodename;\n  /* 4. 输出PID命名空间、主机名和进程基本信息 */\n  printf(\"%-12ld %-8s %-6d %-6d %-8s\", (uint64)$pidns, $cname, curtask-&gt;parent-&gt;pid, pid, comm); join(args-&gt;argv);\n}\n</code></pre><p>这段代码中的具体内容含义如下：</p><ul>\n<li>第 1 处，把内置变量 <code>curtask</code> 转换为我们想要的 <code>task_struct</code> 结构体；</li>\n<li>第 2 处，从进程信息的 nsproxy 中读取 PID 命名空间编号；</li>\n<li>第 3 处，从进程信息的 nsproxy 中读取 UTS 命名空间的主机名（也就是在容器中执行 <code>hostname</code> 命令后的输出）；</li>\n<li>第 4 处你已经非常熟悉了，就是把刚才获取的信息输出，以便我们观察。</li>\n</ul><p>由于这儿用到了很多内核数据结构，在运行之前，还需要给它引入相关数据结构定义的头文件：</p><pre><code class=\"language-c++\">#include &lt;linux/sched.h&gt;\n#include &lt;linux/nsproxy.h&gt;\n#include &lt;linux/utsname.h&gt;\n#include &lt;linux/pid_namespace.h&gt;\n</code></pre><p>同时，由于输出的内容比较多，为了便于理解，你还可以在脚本运行开始的时候输出一个表头，表示每个输出的含义：</p><pre><code class=\"language-plain\">BEGIN {\n  printf(\"%-12s %-8s %-6s %-6s %-8s %s\\n\", \"PIDNS\", \"CONTAINER\", \"PPID\", \"PID\", \"COMM\", \"ARGS\");\n}\n</code></pre><p>把头文件引入和改进后的 bpftrace 脚本保存到 <code>execsnoop-container.bt</code> 文件中（你也可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bpftrace/execsnoop-container.bt\">GitHub</a> 上找到完整代码），然后打开一个新终端，运行下面的命令来执行：</p><pre><code class=\"language-bash\">sudo bpftrace execsnoop-container.bt\n</code></pre><p>接下来，分别在容器终端和主机终端中执行一个 <code>ls</code> 命令，就可以得到如下的输出：</p><pre><code class=\"language-bash\">PIDNS        CONTAINER PPID   PID    COMM     ARGS\n# 容器ls命令跟踪结果\n4026532533   bash     41046  41335  bash    ls --color=auto\n\n# 主机ls命令跟踪结果\n4026531836   ubuntu.localdomain 40958  41356  bash    ls --color=auto\n</code></pre><p>在输出中，容器 <code>ls</code> 命令跟踪结果中的 PID 命名空间 <code>4026532533</code> 跟上述容器中 <code>lsns</code> 结果是一致的，而主机名 <code>bash</code> 也跟运行容器时设置的 <code>--hostname name</code> 一致，因而我们很容易区分这条 <code>ls</code> 命令的来源。</p><p>你可以发现，<strong>只要理解了容器的基本原理，在跟踪过程中加入容器的基本信息，容器内外进程的跟踪和分析并没有本质的区别</strong>。</p><p>也许看到这里你会有疑问：这儿讲到的是内核态的跟踪，容器内外没有区别很正常。但如果是用户态的进程跟踪呢？你可以先思考一下，再继续下面的内容。</p><p>实际上，用户态进程的跟踪也是一样的，唯一需要注意的就是找到容器内二进制文件的正确路径。虽然容器文件系统在不同的 mount 命令空间中，但对于每个进程来说，Linux 都在 <code>/proc/[pid]/root</code> 处创建了一个链接（详细信息请参考 <a href=\"https://man7.org/linux/man-pages/man5/proc.5.html\">proc 文件系统手册</a>）。因而，容器内的文件就可以通过 <code>/proc/[pid]/root</code> 在主机中访问。</p><p>你可以执行下面的命令，查询容器的 PID，进而再查询 bash 的 uprobe 列表：</p><pre><code class=\"language-bash\"># 查询容器进程在主机命名空间中的PID\nPID=$(docker inspect -f '{{.State.Pid}}' bash)\n\n# 查询uprobe\nsudo bpftrace -l \"uprobe:/proc/$PID/root/usr/bin/bash:*\"\n\n# 跟踪bash:readline的结果\nsudo bpftrace -e \"uretprobe:/proc/$PID/root/usr/bin/bash:readline { printf(\\\"User %d executed %s in container\\n\\\", uid, str(retval)); }\"\n</code></pre><h2>如何使用 eBPF 阻止容器的异常行为？</h2><p>了解了如何分析容器中的安全问题之后，我们再来看看如何阻止容器中的异常行为。</p><p>在 eBPF 之前，其实已经有很多的机制来阻止容器的异常行为，比如 <a href=\"https://docs.docker.com/engine/security/apparmor/\">AppArmor</a>、<a href=\"https://docs.docker.com/engine/security/seccomp/\">Seccomp</a>、<a href=\"https://man7.org/linux/man-pages/man7/capabilities.7.html\">Capabilities</a> 等。这些机制虽然好用，但它们的缺点也很明显，那就是它们都需要预先配置好相关的安全策略。在检测到安全隐患之后，更新策略到策略生效中间总有一定的延迟。</p><p>而 eBPF 则可以在检测到安全隐患时，实时去触发安全策略的执行。比如，Linux 5.3 版本中新增的辅助函数 <code>bpf_send_signal()</code> 可以直接把有安全隐患的进程杀死，而 Linux 5.7 新增的 LSM 插桩则可以扩展 Linux 安全模块的审计和策略执行（想了解 LSM 钩子的详细信息，你可以参考内核头文件 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/lsm_hooks.h\">include/linux/lsm_hooks.h</a>）。在网络安全方面，XDP 程序还可以在网络协议栈之前丢弃数据包，减少非法网络请求对系统资源的消耗。</p><p>以上一小节的二进制命令执行为例，你可以在 bpftrace 中调用 <code>signal()</code> 函数，给当前进程发送特定信号：</p><pre><code class=\"language-c++\">tracepoint:syscalls:sys_enter_execve\n/comm == \"bash\"/ {\n  $task = (struct task_struct *)curtask;\n  $cname = $task-&gt;nsproxy-&gt;uts_ns-&gt;name.nodename;\n  printf(\"Killing shell command in container %s: %s \", $cname, $pidns, comm); join(args-&gt;argv);\n signal(9); /* SIGKILL */\n}\n</code></pre><p>这段代码中的具体内容含义如下：</p><ul>\n<li><code>/comm == \"bash\"/</code> 表示对进程名称进行过滤，只处理 Bash 进程；</li>\n<li><code>signal(9)</code> 表示向进程发送 SIGKILL 信号，即杀死进程。</li>\n</ul><p>加入与上一小节中相同的头文件，然后把它保存到 <code>block-container-shell.bt</code> 文件（你也可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bpftrace/block-container-shell.bt\">GitHub</a> 上找到完整代码）中，然后运行下面的命令来执行：</p><pre><code class=\"language-bash\">sudo bpftrace --unsafe block-container-shell.bt\n</code></pre><p>接着，回到容器终端，执行任意命令都会失败，并且失败信息都是 <code>Killed</code> 。</p><p>需要注意，这种直接杀死进程的方法实际上比较危险，如果出现误杀，可能会导致大量进程都无法正常启动。所以 bpftrace 要求你加上 <code>--unsafe</code>选项后才可以正常运行。</p><h2>小结</h2><p>今天，我带你一起梳理了 eBPF 的安全能力，并以容器应用为例，带你用 eBPF 分析并阻止了容器中的命令执行。</p><p>eBPF 所支持的 kprobe、uprobe、tracepoint 等各类探针已经非常全面地涵盖了从应用到内核中的各类安全问题相关的跟踪点，再配合其开销低、实时性好，且不需要修改并重新编译内核和应用等特性，使 eBPF 特别适合用于安全事件的动态监测和实时分析诊断。对于安全策略的执行，eBPF 也已经支持了 LSM 钩子、向进程发送信号、丢弃网络数据包等各类操作。</p><p>在这一讲的最后，我想提醒你：既然 eBPF 提供了这么强大的功能，在用好 eBPF 这些丰富特性的同时，你也要特别留意 eBPF 程序自身的安全性。比如，禁止普通用户和普通容器应用运行 eBPF 程序，而只允许管理员和系统服务运行。对容器来说，你可以利用 Capabilities 禁止普通容器的 <code>CAP_PERFMON</code>、<code>CAP_BPF</code>和 <code>CAP_SYS_ADMIN</code>等权限。</p><h2>思考题</h2><p>最后，我想邀请你来聊一聊：</p><ol>\n<li>在了解 eBPF 之前，你是如何监测、分析和阻止容器的安全问题的？</li>\n<li>在阻止容器 Bash 命令执行的案例中，除了杀死进程之外，还有哪些其他的方法？</li>\n</ol><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"10 | 网络跟踪：如何使用 eBPF 排查网络问题？","id":484686},"right":{"article_title":"12｜高性能网络实战（上）：如何开发一个负载均衡器？","id":485702}}},{"article_id":485702,"article_title":"12｜高性能网络实战（上）：如何开发一个负载均衡器？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你一起梳理了 eBPF 的安全能力，并教你使用 eBPF 分析和阻止了容器进程的安全问题。在开篇词中我就提到，eBPF 的主要应用场景涵盖了故障诊断、性能监控、安全控制以及网络优化等。从<a href=\"https://time.geekbang.org/column/article/484207\"> 07 讲</a> 进入实战进阶篇开始，我已经为你介绍了应用于前三个场景中的内核跟踪、用户态跟踪、网络跟踪以及安全控制等。那么，对于最后一个场景，网络性能优化，eBPF 是如何发挥作用的呢？</p><p>今天，我就以最常用的负载均衡器为例，带你一起来看看如何借助 eBPF 来优化网络的性能。</p><h2>Nginx 负载均衡器</h2><p>既然要优化负载均衡器的网络性能，那么首先就需要有一个优化的目标，即初始版的负载均衡器。在今天的案例中，我们使用最常用的反向代理和 Web 服务器 Nginx 作为初始版的负载均衡器，同时也使用自定义的 Nginx 作为后端的 Web 服务器。</p><p>为了方便环境的重现，负载均衡器、 Web 服务器以及客户端都运行在容器中，它们的 IP 和 MAC 等基本信息如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/15/e923026f577f7b991be2610734f9e415.jpg?wh=1920x1706\" alt=\"图片\"></p><p>参考 Nginx 官方文档中 <a href=\"https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/\">HTTP 负载均衡</a>的配置方法，你可以通过以下几步来搭建上述的案例环境。</p><p>1）执行下面的命令，创建上图中的4个容器：</p><pre><code class=\"language-bash\"># Webserver (响应是hostname，如 http1 或 http2)\ndocker run -itd --name=http1 --hostname=http1 feisky/webserver\ndocker run -itd --name=http2 --hostname=http2 feisky/webserver\n\n# Client\ndocker run -itd --name=client alpine\n\n# Nginx\ndocker run -itd --name=nginx nginx\n</code></pre><!-- [[[read_end]]] --><p>注意，这儿启动的 Nginx 容器使用的还是官方镜像，还需要额外的步骤更新它的负载均衡配置。</p><blockquote>\n<p>小提示：在默认安装的 Docker 环境中，假如你没有运行其他容器，运行上述命令后得到的 IP 地址跟图中是相同的。</p>\n</blockquote><p>2）执行下面的命令，查询两个 Web 服务器的 IP 地址：</p><pre><code class=\"language-bash\">IP1=$(docker inspect http1 -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}')\nIP2=$(docker inspect http2 -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}')\necho \"Webserver1's IP: $IP1\"\necho \"Webserver2's IP: $IP2\"\n</code></pre><p>命令执行后，你将会看到如下的输出：</p><pre><code class=\"language-plain\">Webserver1's IP: 172.17.0.2\nWebserver2's IP: 172.17.0.3\n</code></pre><p>3）执行下面的命令，生成并更新 Nginx 配置：</p><pre><code class=\"language-c++\"># 生成nginx.conf文件\ncat&gt;nginx.conf &lt;&lt;EOF\nuser  nginx;\nworker_processes  auto;\n\nerror_log  /var/log/nginx/error.log notice;\npid        /var/run/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n   include       /etc/nginx/mime.types;\n   default_type  application/octet-stream;\n\n    upstream webservers {\n        server $IP1;\n        server $IP2;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://webservers;\n        }\n    }\n}\nEOF\n\n# 更新Nginx配置\ndocker cp nginx.conf nginx:/etc/nginx/nginx.conf\ndocker exec nginx nginx -s reload\n</code></pre><p>配置完成后，再执行下面的命令，验证负载均衡器是不是生效了（<code>/ #</code> 表示在容器终端中执行命令）：</p><pre><code class=\"language-bash\"># 查询Nginx容器IP（输出为172.17.0.5）\ndocker inspect nginx -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'\n\n# 进入client容器终端，安装curl之后访问Nginx\ndocker exec -it client sh\n\n# (以下命令运行在client容器中)\n/ # apk add curl wrk --update\n/ # curl \"http://172.17.0.5\"\n</code></pre><p>如果一切正常，多次执行 curl 命令后，你会看到如下的输出，即通过 Nginx 成功获得了两个 Web 服务器的输出，说明负载均衡器配置成功了：</p><pre><code class=\"language-bash\">/ # curl \"http://172.17.0.5\"\nHostname: http1\n\n/ # curl \"http://172.17.0.5\"\nHostname: http2\n</code></pre><p>负载均衡器配置成功后，它的性能怎么样呢？进入 client 容器终端中，执行下面的命令，就可以使用 <a href=\"https://github.com/wg/wrk\">wrk</a> 给它做个性能测试：</p><pre><code class=\"language-bash\">/ # apk add wrk --update\n/ # wrk -c100 \"http://172.17.0.5\"\n</code></pre><p>稍等一会，你可以看到如下的性能测试报告：</p><pre><code class=\"language-bash\">Running 10s test @ http://172.17.0.5\n  2 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     7.53ms    4.96ms  39.33ms   70.78%\n    Req/Sec     6.96k   514.59     8.88k    74.00%\n  138711 requests in 10.05s, 21.83MB read\nRequests/sec:  13798.11\nTransfer/sec:      2.17MB\n</code></pre><p>从报告中你可以发现，默认情况下，总的平均每秒请求数是 13798，而每个线程的平均请求数和请求延迟是 6.96k 和 7.53 毫秒（在你的环境下可能看到不同数值，具体的性能指标取决于运行环境和配置）。你可以记录一下这些数值，以便后面跟 eBPF 进行比较。</p><h2>如何使用 eBPF 优化负载均衡性能？</h2><p>有了待优化的 eBPF 负载均衡器之后，接下来就是使用 eBPF 进行优化了。在开始接下来的内容之前，你可以先回顾一下我们之前课程的内容，思考有哪些可能的方案可以用在负载均衡的性能优化场景中。</p><p>在<a href=\"https://time.geekbang.org/column/article/483364\"> 06 讲</a> 中我曾提到过，每个 eBPF 程序都属于特定的类型，不同类型 eBPF 程序的触发事件是不同的。既然是网络的性能优化，自然应该去考虑网络类的 eBPF 程序。根据触发事件的不同，网络类 eBPF 程序可以分为 XDP程序、TC程序、套接字程序以及 cgroup 程序。这几类程序的触发事件和常用场景分别为：</p><ul>\n<li>XDP 程序在网络驱动程序刚刚收到数据包的时候触发执行，支持卸载到网卡硬件，常用于防火墙和四层负载均衡；</li>\n<li>TC 程序在网卡队列接收或发送的时候触发执行，运行在内核协议栈中，常用于流量控制；</li>\n<li>套接字程序在套接字发生创建、修改、收发数据等变化的时候触发执行，运行在内核协议栈中，常用于过滤、观测或重定向套接字网络包。其中，BPF_PROG_TYPE_SOCK_OPS、BPF_PROG_TYPE_SK_SKB、BPF_PROG_TYPE_SK_MSG 等都可以用于套接字重定向；</li>\n<li>cgroup 程序在 cgroup 内所有进程的套接字创建、修改选项、连接等情况下触发执行，常用于过滤和控制 cgroup 内多个进程的套接字。</li>\n</ul><p>根据这些触发事件，你可以发现这几类网络程序都有可能用在网络性能优化上。其中，由于支持卸载到硬件，XDP 的性能应该是最好的；而由于直接作用在套接字上，套接字程序和 cgroup 程序是最接近应用的。</p><p>既然有多种不同的性能优化方式，我就以套接字和 XDP 这两种方式为例，带你优化负载均衡的性能。由于内容比较多，接下来我们先看套接字 eBPF 程序的优化方法，而 XDP 方法我会在下一讲中为你介绍。</p><h2>使用套接字 eBPF 程序优化网络性能</h2><p>根据原理的不同，套接字 eBPF 程序又分为很多不同的类型。其中，BPF_PROG_TYPE_SOCK_OPS、BPF_PROG_TYPE_SK_SKB、BPF_PROG_TYPE_SK_MSG 等类型的 eBPF 程序可以与套接字映射（如 BPF_MAP_TYPE_SOCKMAP 或 BPF_MAP_TYPE_SOCKHASH）配合，实现套接字的转发。</p><p>套接字 eBPF 程序工作在内核空间中，无需把网络数据发送到用户空间就能完成转发。因此，我们可以先猜测，它应该是可以提升网络转发的性能（当然，具体能不能提升，还需要接下来的测试验证）。</p><p>具体来说，使用套接字映射转发网络包需要以下几个步骤：</p><ol>\n<li>创建套接字映射；</li>\n<li>在 BPF_PROG_TYPE_SOCK_OPS 类型的 eBPF 程序中，将新创建的套接字存入套接字映射中；</li>\n<li>在流解析类的 eBPF 程序（如 BPF_PROG_TYPE_SK_SKB 或 BPF_PROG_TYPE_SK_MSG ）中，从套接字映射中提取套接字信息，并调用 BPF 辅助函数转发网络包；</li>\n<li>加载并挂载 eBPF 程序到套接字事件。</li>\n</ol><p>接下来，我们一起看看具体每一步该如何操作。</p><h3>创建套接字映射</h3><p>首先，第一步是创建一个套接字类型的映射。以 BPF_MAP_TYPE_SOCKHASH 类型的套接字映射为例，它的值总是套接字文件描述符，而键则需要我们去定义。比如，可以定义一个包含 IP 协议五元组的结构体，作为套接字映射的键类型：</p><pre><code class=\"language-c++\">struct sock_key\n{\n    __u32 sip;    //源IP\n    __u32 dip;    //目的IP\n    __u32 sport;  //源端口\n    __u32 dport;  //目的端口\n    __u32 family; //协议\n};\n</code></pre><p>有了键类型之后，就可以使用 <code>SEC</code> 关键字来定义套接字映射了，如下所示：</p><pre><code class=\"language-c++\">#include &lt;linux/bpf.h&gt;\n\nstruct bpf_map_def SEC(\"maps\") sock_ops_map = {\n    .type = BPF_MAP_TYPE_SOCKHASH,\n    .key_size = sizeof(struct sock_key),\n    .value_size = sizeof(int),\n    .max_entries = 65535,\n    .map_flags = 0,\n};\n</code></pre><p>为了方便后续在 eBPF 程序中引用这两个数据结构，你可以把它们保存到一个头文件 <code>sockops.h</code> 中（你还可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/sockops/sockops.h\">GitHub</a> 中找到完整的代码）。</p><h3>更新套接字映射</h3><p>套接字映射准备好之后，第二步就是在 BPF_PROG_TYPE_SOCK_OPS 类型的 eBPF 程序中跟踪套接字事件，并把套接字信息保存到 SOCKHASH 映射中。</p><p>参考内核中 BPF_PROG_TYPE_SOCK_OPS 程序类型的<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/bpf_types.h#L29\">定义格式</a>，它的参数格式为 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5506\">struct bpf_sock_ops</a>：</p><pre><code class=\"language-c++\">#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)\n\nBPF_PROG_TYPE(BPF_PROG_TYPE_SOCK_OPS, sock_ops,\n    struct bpf_sock_ops, struct bpf_sock_ops_kern)\n</code></pre><p>因此，你就可以使用如下的格式来定义这个 eBPF 程序：</p><pre><code class=\"language-c++\">SEC(\"sockops\")\nint bpf_sockmap(struct bpf_sock_ops *skops)\n{\n  // TODO: 添加套接字映射更新操作\n}\n</code></pre><p>在添加具体的套接字映射更新逻辑之前，还需要你先从 <code>struct bpf_sock_ops</code>中获取作为键类型的五元组。参考内核中 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5506\">struct bpf_sock_ops</a> 的定义，如下的几个字段刚好可以满足我们的需要：</p><pre><code class=\"language-c++\">struct bpf_sock_ops {\n  __u32 family;\n  __u32 remote_ip4;\t/* Stored in network byte order */\n  __u32 local_ip4;\t/* Stored in network byte order */\n  __u32 remote_port;/* Stored in network byte order */\n  __u32 local_port;\t/* stored in host byte order */\n  ...\n}\n</code></pre><p>因此，你就可以直接使用它们来定义映射中所需要的键。下面就是 <code>sock_key</code> 的定义方法，注意这里把 <code>local_port</code> 转换为了同其他字段一样的网络字节序：</p><pre><code class=\"language-c++\">struct sock_key key = {\n  .dip = skops-&gt;remote_ip4,\n  .sip = skops-&gt;local_ip4,\n  .sport = bpf_htonl(skops-&gt;local_port),\n  .dport = skops-&gt;remote_port,\n  .family = skops-&gt;family,\n};\n</code></pre><p>有了键之后，还不能立刻就去更新套接字映射。这是因为 BPF_PROG_TYPE_SOCK_OPS 程序跟踪了所有类型的套接字操作，而我们只需要把新创建的套接字更新到映射中。</p><p><code>struct bpf_sock_ops</code> 中包含的 <code>op</code> 字段可用于判断套接字操作类型，其定义格式可以参考<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5638\">这里的内核头文件</a>。内核头文件中已经为每种操作的具体含义加了详细的注释，对于新创建的连接，我们就可以使用以下两个状态（即主动连接和被动连接）作为判断条件：</p><pre><code class=\"language-c++\">/* skip if it is not established op */\nif (skops-&gt;op != BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB &amp;&amp; skops-&gt;op != BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB) {\n  return BPF_OK;\n}\n</code></pre><p>到这里，说明套接字已经属于新创建的连接了，所以接下来就是调用 BPF 辅助函数去更新套接字映射，如下所示：</p><pre><code class=\"language-c++\">bpf_sock_hash_update(skops, &amp;sock_ops_map, &amp;key, BPF_NOEXIST);\n</code></pre><p>其中，<code>BPF_NOEXIST</code>  表示键不存在的时候才添加新元素。</p><p>再加上必要的头文件，完整的 eBPF 程序如下所示：</p><pre><code class=\"language-c++\">#include &lt;linux/bpf.h&gt;\n#include &lt;bpf/bpf_endian.h&gt;\n#include &lt;bpf/bpf_helpers.h&gt;\n#include &lt;sys/socket.h&gt;\n#include \"sockops.h\"\n\nSEC(\"sockops\")\nint bpf_sockmap(struct bpf_sock_ops *skops)\n{\n    /* skip if the packet is not ipv4 */\n    if (skops-&gt;family != AF_INET)\n    {\n        return BPF_OK;\n    }\n\n    /* skip if it is not established op */\n    if (skops-&gt;op != BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB &amp;&amp; skops-&gt;op != BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB) {\n        return BPF_OK;\n    }\n\n    struct sock_key key = {\n        .dip = skops-&gt;remote_ip4,\n        .sip = skops-&gt;local_ip4,\n        /* convert to network byte order */\n        .sport = (bpf_htonl(skops-&gt;local_port)),\n        .dport = skops-&gt;remote_port,\n        .family = skops-&gt;family,\n    };\n\n    bpf_sock_hash_update(skops, &amp;sock_ops_map, &amp;key, BPF_NOEXIST);\n    return BPF_OK;\n}\n\nchar LICENSE[] SEC(\"license\") = \"Dual BSD/GPL\";\n</code></pre><p>把上述代码保存到 <code>sockops.bpf.c</code> 文件中（你还可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/sockops/sockops.bpf.c\">GitHub</a> 中找到完整的代码），然后执行下面的命令，将其编译为 BPF 字节码：</p><pre><code class=\"language-bash\">clang -g -O2 -target bpf -D__TARGET_ARCH_x86 -I/usr/include/x86_64-linux-gnu -I. -c sockops.bpf.c -o sockops.bpf.o\n</code></pre><p>到这里，套接字更新的 eBPF 程序就准备好了，接下来我们来看看如何转发套接字。</p><h3>套接字转发</h3><p>第三步的套接字转发可以使用 BPF_PROG_TYPE_SK_MSG 类型的 eBPF 程序，捕获套接字中的发送数据包，并根据上述的套接字映射进行转发。根据内核头文件中的<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/bpf_types.h#L33\">定义格式</a>，它的参数格式为 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5328\">struct sk_msg_md</a>。<code>struct sk_msg_md</code> 的定义格式如下所示，也已经包含了套接字映射所需的五元组信息：</p><pre><code class=\"language-c++\">struct sk_msg_md {\n  ...\n  __u32 family;\n  __u32 remote_ip4;  /* Stored in network byte order */\n  __u32 local_ip4;   /* Stored in network byte order */\n  __u32 remote_port; /* Stored in network byte order */\n  __u32 local_port;  /* stored in host byte order */\n  ...\n};\n</code></pre><p>了解清楚数据结构的定义格式之后，还需要你注意一点：BPF_PROG_TYPE_SK_MSG 跟 BPF_PROG_TYPE_SOCK_OPS 属于不同的 eBPF 程序。虽然你可以把多个 eBPF 程序放入同一个源码文件，并编译到同一个字节码文件(即 <code>文件名.o</code>）中，但由于它们的加载和挂载格式都是不同的，我推荐你把不同的 eBPF 程序放入不同的文件中，这样管理起来更为方便。</p><p>因此，接下来创建一个新的文件（如 <code>sockredir.bpf.c</code>），用于保存 BPF_PROG_TYPE_SK_MSG 程序。添加如下的代码，就定义了一个名为 <code>bpf_redir</code> 的 eBPF 程序：</p><pre><code class=\"language-c++\">SEC(\"sk_msg\")\nint bpf_redir(struct sk_msg_md *msg)\n{\n    //TODO: 添加套接字转发逻辑\n}\n</code></pre><p>在这个 eBPF 程序中，既然还要访问相同的套接字映射，也就需要从参数 <code>struct sk_msg_md</code> 中提取五元组信息，并存入套接字映射所需要的键 <code>struct sock_key</code> 中。如下所示，我们就定义了一个新的 <code>struct sock_key</code>（注意，这里同样需要把 <code>local_port</code> 转换为网络字节序）：</p><pre><code class=\"language-c++\">struct sock_key key = {\n  .sip = msg-&gt;remote_ip4,\n  .dip = msg-&gt;local_ip4,\n  .dport = bpf_htonl(msg-&gt;local_port),\n  .sport = msg-&gt;remote_port,\n  .family = msg-&gt;family,\n};\n</code></pre><p>需要你注意的是，这儿的源 IP 和源端口对应上述 eBPF 程序的目的 IP 和目的端口，也就是说，发送方向刚好是相反的。为什么是相反的呢？来看看下面这张图，原因就很清楚了：</p><p><img src=\"https://static001.geekbang.org/resource/image/3c/70/3c2419d6a357eca924fe6b94ed5b0970.jpg?wh=1920x1809\" alt=\"图片\"></p><p>图中，灰色箭头是套接字转发之前的网络流向，而绿色箭头则是套接字转发后的网络流向。从这张图中你可以发现：</p><ul>\n<li>在套接字转发之前，即便是在同一台机器的两个容器中，负载均衡器和 Web 服务器的两个套接字通信还是需要通过完整的内核协议栈进行处理的；</li>\n<li>而在套接字转发之后，来自发送端套接字 1 的网络包在套接字层就交给了接收端的套接字 2，从而避免了额外的内核协议栈处理过程。</li>\n</ul><p>由于这两个套接字一个是发送，一个是接收，因而它们的方向是相反的，所以在构造转发套接字的键时，就需要把源和目的交换。</p><p>有了套接字映射所需要的键之后，最后还剩下添加套接字转发逻辑的步骤。参考 BPF 辅助函数文档（你可以执行 <code>man bpf-helpers</code> 查询），<code>bpf_msg_redirect_hash()</code> 正好跟我们的需求完全匹配。为了方便你理解，我把它的使用文档也贴一下：</p><pre><code class=\"language-c++\">long bpf_msg_redirect_hash(struct sk_msg_buff *msg, struct bpf_map *map, void *key, u64 flags)\n\nDescription\nThis helper is used in programs implementing policies at the socket  level.  If  the\nmessage  msg  is allowed to pass (i.e. if the verdict eBPF program returns SK_PASS),\nredirect it to the socket referenced by map (of  type  BPF_MAP_TYPE_SOCKHASH)  using\nhash  key.  Both  ingress  and  egress  interfaces  can be used for redirection. The\nBPF_F_INGRESS value in flags is used to make the distinction (ingress  path  is  se‐\nlected  if  the  flag is present, egress path otherwise). This is the only flag sup‐\nported for now.\n\nReturn SK_PASS on success, or SK_DROP on error.\n</code></pre><p>概括来说，<code>bpf_msg_redirect_hash()</code> 的作用就是把当前套接字转发给套接字映射中的套接字。而参数 <code>key</code> 用于从套接字映射中查询待转发的套接字，<code>flags</code> 用于区分入口或出口路径。</p><p>根据每个参数的具体格式，你就可以通过下面的方式进行套接字转发。注意，对于负载均衡的场景来说，只需要对入口路径进行处理，因而这儿设置了 <code>BPF_F_INGRESS</code>。</p><pre><code class=\"language-c++\">bpf_msg_redirect_hash(msg, &amp;sock_ops_map, &amp;key, BPF_F_INGRESS);\n</code></pre><p>再加上必要的头文件之后，完整的 eBPF 程序如下所示：</p><pre><code class=\"language-c++\">#include &lt;linux/bpf.h&gt;\n#include &lt;bpf/bpf_endian.h&gt;\n#include &lt;bpf/bpf_helpers.h&gt;\n#include &lt;sys/socket.h&gt;\n#include \"sockops.h\"\n\n\n\nSEC(\"sk_msg\")\nint bpf_redir(struct sk_msg_md *msg)\n{\n    struct sock_key key = {\n        .sip = msg-&gt;remote_ip4,\n        .dip = msg-&gt;local_ip4,\n        .dport = bpf_htonl(msg-&gt;local_port),\n        .sport = msg-&gt;remote_port,\n        .family = msg-&gt;family,\n    };\n\n    bpf_msg_redirect_hash(msg, &amp;sock_ops_map, &amp;key, BPF_F_INGRESS);\n    return SK_PASS;\n}\n\nchar LICENSE[] SEC(\"license\") = \"Dual BSD/GPL\";\n</code></pre><p>把上述代码保存到文件 <code>sockredir.bpf.c</code> 中（你还可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/sockops/sockredir.bpf.c\">GitHub</a> 中找到完整的代码），然后执行下面的命令，将其编译为 BPF 字节码：</p><pre><code class=\"language-bash\">clang -g -O2 -target bpf -D__TARGET_ARCH_x86 -I/usr/include/x86_64-linux-gnu -I. -c sockredir.bpf.c -o sockredir.bpf.o\n</code></pre><h3>加载 eBPF 程序</h3><p>得到套接字映射更新和转发这两个 BPF 字节码之后，还需要把它们加载到内核之中，再挂载到特定的内核事件之后才会生效。在之前的案例中，我介绍的方法是利用 BCC、libbpf 等提供的库函数。今天，我再为你介绍另外一种方法，即通过命令行工具 bpftool 加载和挂载 eBPF 程序。</p><p>首先，对于 sockops 程序 <code>sockops.bpf.o</code> 来说，你可以执行下面的命令，将其加载到内核中：</p><pre><code class=\"language-bash\">sudo bpftool prog load sockops.bpf.o /sys/fs/bpf/sockops type sockops pinmaps /sys/fs/bpf\n</code></pre><p>这条命令将 <code>sockops.bpf.o</code> 中的 eBPF 程序和映射加载到内核中，并固定到 BPF 文件系统中。固定到 BPF 文件系统的好处是，即便 bpftool 命令已经执行结束，eBPF 程序还会继续在内核中运行，并且 eBPF 映射也会继续存在内核内存中。</p><p>加载成功后，你还可以执行 <code>bpftool prog show</code> 和 <code>bpftool map show</code> 命令确认它们的加载结果。执行成功后，你会看到类似下面的输出：</p><pre><code class=\"language-bash\">$ sudo bpftool prog show name bpf_sockmap\n1062: sock_ops  name bpf_sockmap  tag e37ef726a3a85a2e  gpl\n\tloaded_at 2022-02-04T13:07:28+0000  uid 0\n\txlated 256B  jited 140B  memlock 4096B  map_ids 90\n\tbtf_id 234\n\n$ sudo bpftool map show name sock_ops_map\n90: sockhash  name sock_ops_map  flags 0x0\n\tkey 20B  value 4B  max_entries 65535  memlock 1572864B\n</code></pre><p>BPF 字节码加载成功之后，其中的 eBPF 程序还不会自动运行，因为这时候它还没有与内核事件挂载。</p><p>对 sockops 程序来说，它支持挂载到 cgroups，从而对 cgroups 所拥有的所有进程生效，这跟我们案例的容器场景也是匹配的。</p><p>虽然 Docker 支持把新容器挂载到 cgroups 子系统中，但在案例开始的时候，我们并没有指定 cgroups 子系统。此时，Docker 会自动把所有容器都添加到系统 cgroups 子系统中。所以，对 sockops 程序来说，就可以把它挂载到系统 cgroups 中，从而对包括容器应用在内的所有进程生效。</p><p>你可以执行下面的 <code>mount</code> 命令，查询当前系统的 cgroups 的挂载路径：</p><pre><code class=\"language-bash\">$ mount | grep cgroup\ncgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)\n</code></pre><p>通常情况下，主流的发行版都会把 cgroups 挂载到 <code>/sys/fs/cgroup</code> 路径下。接着，再执行下面的 <code>bpftool cgroup attach</code> 命令，把 sockops 程序挂载到 cgroups 路径中：</p><pre><code class=\"language-bash\">sudo bpftool cgroup attach /sys/fs/cgroup/ sock_ops pinned /sys/fs/bpf/sockops\n</code></pre><p>到这里，sockops 程序的加载和挂载就完成了。</p><p>接下来，再执行下面的命令，加载并挂载 sk_msg 程序 <code>sockredir.bpf.o</code>：</p><pre><code class=\"language-bash\">sudo bpftool prog load sockredir.bpf.o /sys/fs/bpf/sockredir type sk_msg map name sock_ops_map pinned /sys/fs/bpf/sock_ops_map\nsudo bpftool prog attach pinned /sys/fs/bpf/sockredir msg_verdict pinned /sys/fs/bpf/sock_ops_map\n</code></pre><p>从这两条命令中你可以看到，sk_msg 程序的加载和挂载过程跟 sockops 程序是类似的，区别只在于它们的程序类型和挂载类型不同：</p><ul>\n<li>sockops 程序的类型是 <code>sock_ops</code>，sk_msg 程序的类型是 <code>sk_msg</code>；</li>\n<li>sockops 程序的挂载类型是 <code>cgroup</code> （对应 <code>bpftool cgroup attach</code> 命令），sk_msg 程序的挂载类型是 <code>msg_verdict</code>（对应 <code>bpftool prog attach</code> 命令）。</li>\n</ul><p>由于 sk_msg 程序需要访问 sockops 程序创建的套接字映射，所以上述命令通过 BPF 文件系统路径 <code>/sys/fs/bpf/sock_ops_map</code> 对套接字映射进行了绑定。</p><p>到这里，两个 eBPF 程序的加载和挂载就都完成了。</p><p>那么，它们是不是真的可以提升网络转发的性能呢？回想一下 Nginx 负载均衡的测试步骤，我们使用相同的方法再做个性能测试，就可以知道了。</p><h3>性能测试</h3><p>执行下面的命令进入 client 容器终端，并在容器终端中执行 <code>wrk</code> 命令：</p><pre><code class=\"language-bash\">docker exec -it client sh\n/ # wrk -c100 \"http://172.17.0.5\"\n</code></pre><p>稍等一会，你会看到如下的输出（在你的环境下可能看到不同数值，具体的性能指标取决于运行环境和配置）：</p><pre><code class=\"language-bash\">Running 10s test @ http://172.17.0.5\n  2 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     6.88ms    4.71ms  46.08ms   70.77%\n    Req/Sec     7.70k   548.11     9.10k    66.50%\n  153466 requests in 10.03s, 24.15MB read\nRequests/sec:  15300.71\nTransfer/sec:      2.41MB\n</code></pre><p>你可以看到，新的平均每秒请求数是 15300，相比优化之前的 13798 提升了 10.8%；而每个线程的平均延迟 6.88ms 也比之前的 7.53ms 降低了 8.6%。这说明，eBPF 真的优化了负载均衡器的转发性能，这跟我们一开始的猜想是一致的。</p><h3>案例清理</h3><p>案例的最后，不要忘记清理今天所加载的 eBPF 程序以及容器环境。</p><p>对于 eBPF 程序来说，清理过程需要卸载（detach）和删除（unload）两个步骤。执行下面的命令，就可以卸载和删除 skops 和 sk_msg 这两个程序：</p><pre><code class=\"language-bash\"># cleanup skops prog and sock_ops_map\nsudo bpftool cgroup detach /sys/fs/cgroup/ sock_ops name bpf_sockmap\nsudo rm -f /sys/fs/bpf/sockops /sys/fs/bpf/sock_ops_map\n\n# cleanup sk_msg prog\nsudo bpftool prog detach pinned /sys/fs/bpf/sockredir msg_verdict pinned /sys/fs/bpf/sock_ops_map\nsudo rm -f /sys/fs/bpf/sockredir\n</code></pre><p>你可能已经注意到了，与 <code>attach</code> 相对应的清理操作为 <code>detach</code>，但 bpftool 并没有一个与 <code>load</code> 相对应的 <code>unload</code> 子命令。这是因为，eBPF 程序和映射都是与 BPF 文件系统绑定的，文件删除后，它们引用计数降为 0 ，就会被系统自动清理了，所以删除过程只需要把 BPF 文件系统中的文件删除即可。</p><p>而对于容器的清理就更容易了，只需要执行下面的 <code>docker rm</code> 命令即可：</p><pre><code class=\"language-bash\">docker rm -f http1 http2 client nginx\n</code></pre><h2>小结</h2><p>今天，我带你搭建了一个最简单的负载均衡程序，并借助套接字 eBPF 程序对它的性能进行了优化。</p><p>借助 sockops 和 sk_msg 等套接字 eBPF 程序，你可以在内核态中直接将网络包转发给目的应用的套接字，跳过复杂的内核协议栈，从而提升网络转发的性能。</p><p>在需要加载和挂载 eBPF 程序或映射时，除了可以利用 BCC、libbpf 等提供的库函数之外，你还可以使用 bpftool 这个工具来实现。由于 eBPF 程序及相关的工具还在快速进化中，在碰到不确定的疑问时，我推荐你参考跟当前内核版本匹配的内核头文件定义、man 手册等，去查询关于它们的详细文档，而不要单纯依赖于网络搜索。</p><p>当然，对于网络优化来说，除了套接字 eBPF 程序之外，XDP 程序和 TC 程序也是最常用的性能优化方法，我将在下一讲中为你介绍 XDP 程序的详细使用方法。</p><h2>思考题</h2><p>最后的思考题环节，我们来探讨关于排查套接字映射的问题。</p><p>由于今天的案例中没有添加日志，所以要想观察 eBPF 程序的运行状态，只能通过套接字映射，而 <code>bpftool map dump</code> 命令则提供了查看套接字映射内容的功能。</p><p>在终端中执行下面的 <code>nc</code> 命令，新建一个到 Nginx 容器的连接：</p><pre><code class=\"language-bash\">nc 172.17.0.5 80\n</code></pre><p>然后打开一个新终端，执行下面的 <code>bpftool map dump</code> 命令：</p><pre><code class=\"language-bash\">sudo bpftool map dump name sock_ops_map\n</code></pre><p>接着，你就会看到类似下面的输出：</p><pre><code class=\"language-plain\">key:\nac 11 00 01 ac 11 00 05  00 00 be 12 00 00 00 50\n02 00 00 00\nvalue:\nNo space left on device\nkey:\nac 11 00 05 ac 11 00 01  00 00 00 50 00 00 be 12\n02 00 00 00\nvalue:\nNo space left on device\n</code></pre><p>由于 value 对应的是套接字文件描述符，bpftool 无法显示它的内容，所以你可以忽略 <code>No space left on device</code> 的错误（实际上这个错误信息是不准确的，新版本中已经修复了错误信息）。</p><p>而对于 key 来说，则是一长串的十六进制数值。这里我想请你思考两个问题：</p><ol>\n<li>这些十六进制数值是什么意思？它们是如何跟 eBPF 程序中定义的 <code>struct sock_key</code> 关联的？</li>\n<li>你可以尝试把这些十六进制数值转换成 <code>struct sock_key</code> 数据结构，并计算出每个属性的值吗？</li>\n</ol><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"11 | 容器安全：如何使用 eBPF 增强容器安全？","id":485101},"right":{"article_title":"13｜高性能网络实战（下）：如何完善负载均衡器？","id":486353}}},{"article_id":486353,"article_title":"13｜高性能网络实战（下）：如何完善负载均衡器？","article_content":"<p>你好，我是倪朋飞。</p><p>上一讲，我带你使用 sockops 和 sk_msg 等套接字 eBPF 程序，在内核态对套接字进行转发，提升了负载均衡的性能。</p><p>对于网络优化来说，除了套接字 eBPF 程序，XDP 程序和 TC 程序也可以用来优化网络的性能。特别是 XDP 程序，由于它在 Linux 内核协议栈之前就可以处理网络包，在负载均衡、防火墙等需要高性能网络的场景中已经得到大量的应用。</p><p>XDP 程序在内核协议栈初始化之前运行，这也就意味着在 XDP 程序中，你并不能像在 sockops 等程序中那样直接获得套接字的详细信息。使用 XDP 程序加速负载均衡，通常也就意味着需要从头开发一个负载均衡程序。这是不是说 XDP 的使用就特别复杂，需要重新实现内核协议栈的很多逻辑呢？不要担心，<strong>XDP 处理过的数据包还可以正常通过内核协议栈继续处理，所以你只需要在 XDP 程序中实现最核心的网络逻辑就可以了</strong>。</p><p>今天，我就以 XDP 程序为例，带你继续优化和完善负载均衡器的性能。</p><h2>案例准备</h2><p>跟上一讲类似，为了方便环境的重现，负载均衡器、Web 服务器以及客户端都还是运行在容器中，它们的 IP 和 MAC 等基本信息如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/15/e923026f577f7b991be2610734f9e415.jpg?wh=1920x1706\" alt=\"图片\"><br>\n执行下面的命令，启动这几个容器：</p><pre><code class=\"language-bash\"># Webserver\ndocker run -itd --name=http1 --hostname=http1 feisky/webserver\ndocker run -itd --name=http2 --hostname=http2 feisky/webserver\n\n# Client\ndocker run -itd --name=client alpine\n\n# LB\ndocker run -itd --name=lb --privileged -v /sys/kernel/debug:/sys/kernel/debug alpine\n</code></pre><!-- [[[read_end]]] --><blockquote>\n<p>小提示：在默认安装的 Docker 环境中，假如你没有运行其他容器，运行上述命令后得到的 IP 地址跟图中是相同的。</p>\n</blockquote><p>注意，我们把作为负载均衡器的 Nginx 换成了基于 alpine 镜像的 SHELL 容器，并且以特权容器的方式运行，以便有足够的权限加载并运行 XDP 程序。</p><p>把 XDP 程序放入容器中，除了容易复现案例环境之外，在开发和调试 XDP 程序的过程中也不会影响主机的网络（否则，错误的 XDP 程序可能导致主机网络中断，进而也会影响远程 SSH 连接）。</p><p>由于负载均衡容器只启动了一个 SHELL 环境，并没有运行真正的负载均衡服务。此时，访问负载均衡器的 TCP 80 端口会直接失败。你可以运行下面的命令到客户端容器中验证（<code>/ #</code> 后的命令表示在容器终端中运行）：</p><pre><code class=\"language-bash\">docker exec -it client sh\n/ # apk add curl --update\n/ # curl \"http://172.17.0.5\"\ncurl: (7) Failed to connect to 172.17.0.5 port 80 after 1 ms: Connection refused\n</code></pre><p>案例所需的容器环境启动完毕后，接下来我们再来看看，如何使用 XDP 开发一个负载均衡服务。由于需要把 XDP 字节码放到容器中运行，本着最小依赖的原则，我们将使用 libbpf 作为 XDP 的基础库，这样只需要把编译后的二进制文件放入容器中即可运行。</p><h2>如何用 XDP 开发一个负载均衡器？</h2><p>还记得我在<a href=\"https://time.geekbang.org/column/article/484372\"> 08 讲</a> 中提到过的基于 libbpf 开发 eBPF 程序的基本步骤吗？不记得也没关系，我们再来回顾一下。 libbpf 的使用通常分为以下几步：</p><ol>\n<li>开发 eBPF 程序，并把源文件命名为 <code>&lt;程序名&gt;.bpf.c</code>；</li>\n<li>编译 eBPF 程序为字节码，然后再调用&nbsp;<code>bpftool gen skeleton</code>&nbsp;为 eBPF 字节码生成脚手架头文件；</li>\n<li>开发用户态程序，引入生成的脚手架头文件后，加载 eBPF 程序并挂载到相应的内核事件中。</li>\n</ol><p>接下来，我们就按这几个步骤来开发 XDP 负载均衡程序。</p><h3>开发 XDP eBPF 程序</h3><p>第一步是开发一个运行在内核态的 eBPF 程序。参考内核中 BPF_PROG_TYPE_XDP 程序类型的<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/bpf_types.h#L11\">定义格式</a>，它的参数类型为 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5283\">struct xdp_md</a>：</p><pre><code class=\"language-c++\">#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)\n\nBPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp,\n       struct xdp_md, struct xdp_buff)\n</code></pre><p>因而，你就可以使用如下的格式来定义这个 XDP 程序：</p><pre><code class=\"language-c++\">SEC(\"xdp\")\nint xdp_proxy(struct xdp_md *ctx)\n{\n  // TODO: 添加XDP负载均衡逻辑\n}\n</code></pre><p>这段代码中，<code>SEC(\"xdp\")</code> 表示程序的类型为 XDP 程序。你可以在 libbpf 中 <a href=\"https://github.com/libbpf/libbpf/blob/master/src/libbpf.c#L8599-L8675\">section_defs</a>找到所有 eBPF 程序类型对应的段名称格式。</p><p>参考 <code>linux/bpf.h</code> 头文件中 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5283\">struct xdp_md</a> 的定义格式，你可以发现，它比上一讲用到的 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5506\">struct bpf_sock_ops</a> 简单多了，只包含了如下的几个字段：</p><pre><code class=\"language-c++\">struct xdp_md {\n  __u32 data;\n  __u32 data_end;\n  __u32 data_meta;\n  /* Below access go through struct xdp_rxq_info */\n  __u32 ingress_ifindex; /* rxq-&gt;dev-&gt;ifindex */\n  __u32 rx_queue_index;  /* rxq-&gt;queue_index  */\n  __u32 egress_ifindex;  /* txq-&gt;dev-&gt;ifindex */\n};\n</code></pre><p>从 <code>struct xdp_md</code> 的定义中你可以看到，所有的字段都是整型数值，其中前三个表示数据指针信息（包括开始位置、结束位置、元数据位置），而后三个表示关联网卡的信息（包括入口网卡、入口网卡队列以及出口网卡的编号）。</p><p>由于 <code>struct xdp_md</code> 中并不包含 skb 数据结构，在 XDP 程序中，你只能通过 <code>data</code> 和 <code>data_end</code> 这两个指针去访问网络报文数据。而要想利用原始网络数据指针来访问网络数据，就需要你了解 TCP/IP 网络报文的基本格式。</p><p>为了方便你理解，我画了一张图，标记了以太网头、IP 头以及 TCP 头等相对于 <code>struct xdp_md</code> 中数据指针的位置关系：</p><p><img src=\"https://static001.geekbang.org/resource/image/yy/91/yy7887570f06c1d075eb31701924e791.jpg?wh=1920x577\" alt=\"图片\"><br>\n有了这些对应关系，要访问 TCP/IP 协议某一层的头结构，就可以使用开始指针 <code>data</code> 再加上它之前所有头结构大小的偏移。</p><p>比如，对于以太网头，它的位置跟开始位置 <code>data</code> 是相同的，因而你就可以使用下面的方式，把它转换为指针格式进行访问：</p><pre><code class=\"language-c++\">void *data = (void *)(long)ctx-&gt;data;\nvoid *data_end = (void *)(long)ctx-&gt;data_end;\n\nstruct ethhdr *eth = data;\nif (data + sizeof(struct ethhdr) &gt; data_end)\n{\n  return XDP_ABORTED;\n}\n</code></pre><p>为了帮助 eBPF 校验器验证数据访问的合法性，在访问以太网头数据结构 <code>struct ethhdr</code> 之前，你需要检查数据指针是否越界。如果检查失败，就要返回一个错误（这儿返回的 <code>XDP_ABORTED</code> 表示丢弃数据包并记录错误行为以便排错）。</p><p>了解了太网头的访问格式之后，IP 头的访问也是类似的。在开始指针 <code>data</code> 之后加上太网头数据结构的长度偏移，就是 IP 头所指向的位置。拿到 IP 头之后，你还可以对网络数据进行初步的校验，比如忽略 IPv6、UDP 等我们不关心的数据，只处理 TCP 数据包，代码如下：</p><pre><code class=\"language-c++\">struct iphdr *iph = data + sizeof(struct ethhdr);\nif (data + sizeof(struct ethhdr) + sizeof(struct iphdr) &gt; data_end)\n{\n  return XDP_ABORTED;\n}\n\nif (eth-&gt;h_proto != bpf_htons(ETH_P_IP))\n{\n  return XDP_PASS;\n}\n\nif (iph-&gt;protocol != IPPROTO_TCP)\n{\n  return XDP_PASS;\n}\n</code></pre><p>这段代码中返回的 <code>XDP_PASS</code> 表示把网络包传递给内核协议栈，内核协议栈接收到网络包后，按正常流程继续处理。</p><p>进行了基本的校验之后，再接下来就是实现负载均衡的逻辑了。由于我们想要实现的是一个四层负载均衡，试想一下，负载均衡器收到客户端的请求之后，需要把目的地址（包括 IP 和 MAC）替换成后端 Webserver 的地址，再重新发到内核协议栈中继续处理。</p><p>参考内核中<a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/if_ether.h#L165\">以太网头</a>和 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/ip.h#L86\">IP 头</a>的定义格式， IP 地址都是以 <code>__be16</code> 类型的大端格式存储，而 MAC 地址则是以字符数组 <code>unsigned char h_dest[6]</code> 的形式存储。因而，MAC 地址可以直接通过数据下标进行访问，而我们案例开始时列出的 IP 地址，还需要先转换为 <code>__be16</code> 格式的大端存储格式。</p><p>如果你还不熟悉 IP 地址的转换方法，那可以参考下面的程序，调用 <code>inet_addr()</code> 库函数帮你完成转换：</p><pre><code class=\"language-c++\">#include &lt;stdio.h&gt;\n#include &lt;arpa/inet.h&gt;\n\nint main() {\n  unsigned int a1 = inet_addr(\"172.17.0.2\");\n  unsigned int a2 = inet_addr(\"172.17.0.3\");\n  unsigned int a3 = inet_addr(\"172.17.0.4\");\n  unsigned int a4 = inet_addr(\"172.17.0.5\");\n  printf(\"0x%x 0x%x 0x%x 0x%x\\n\", a1, a2, a3, a4);\n}\n</code></pre><p>为了方便你理解接下来的程序，我把转换后的容器地址信息整理成了一个表格，你可以在后续的开发和问题排查过程中参考：</p><p><img src=\"https://static001.geekbang.org/resource/image/d3/bb/d31389c380dc3dddf2128495d35b6ebb.jpg?wh=2284x1510\" alt=\"\"></p><p>接下来，就是负载均衡的实现过程了，也就是根据请求的来源，把目的地址修改为 Webserver 的地址。下面的代码展示的就是一个最简单的负载均衡实现逻辑：</p><pre><code class=\"language-c++\">/* 1. 常量定义 */\n#define CLIENT_IP 0x40011ac\n#define LOADBALANCER_IP 0x50011ac\n#define ENDPOINT1_IP 0x20011ac\n#define ENDPOINT2_IP 0x30011ac\n#define CLIENT_MAC_SUFFIX 0x04\n#define LOADBALANCER_MAC_SUFFIX 0x05\n#define ENDPOINT1_MAC_SUFFIX 0x02\n#define ENDPOINT2_MAC_SUFFIX 0x03\n\n/* 2. 从客户端发送过来的请求，目的地址改为后端 Webserver 的地址 */\nif (iph-&gt;saddr == CLIENT_IP)\n{\n  iph-&gt;daddr = ENDPOINT1_IP;\n  eth-&gt;h_dest[5] = ENDPOINT1_MAC_SUFFIX; /* Only need to update the last byte */\n\n  /* 模拟从两个Webserver随机选择 */\n  if ((bpf_ktime_get_ns() &amp; 0x1) == 0x1)\n  {\n    iph-&gt;daddr = ENDPOINT2_IP;\n    eth-&gt;h_dest[5] = ENDPOINT2_MAC_SUFFIX;\n  }\n}\nelse /* 3. 反之，目的地址改为客户端 */\n{\n  iph-&gt;daddr = CLIENT_IP;\n  eth-&gt;h_dest[5] = CLIENT_MAC_SUFFIX;\n}\n\n/* 4. 修改原地址为LB地址 */\niph-&gt;saddr = LOADBALANCER_IP;\neth-&gt;h_source[5] = LOADBALANCER_MAC_SUFFIX;\n</code></pre><p>这段代码中各部分的含义如下：</p><ul>\n<li>第 1 部分，将容器地址信息定义为常量，方便后续引用和理解。注意，MAC 地址是一个包含6 个元素的数组，而前 5 个元素的值都是相同的。因而，在更新目的 MAC 地址时，只需要更新最后一个元素即可。所以，常量定义里面也只包含了最后一个字节的值。</li>\n<li>第 2 部分，对于从客户端发送过来的请求，将目的地址改为后端 Webserver 的地址。由于只有两个后端 Webserver，这儿使用时间戳最后一位的值模拟它们的随机选择过程。</li>\n<li>第 3 部分，对于从 Webserver 发送过来的响应，目的地址改为客户端地址。</li>\n<li>第 4 部分，将原地址都改为负载均衡器的地址。</li>\n</ul><p>到这里， eBPF 程序是不是已经开发好了呢？其实，如果你了解过 IP 协议的基本原理，你就知道还有一个步骤也是非常重要的：由于修改了 IP 头的数据，IP 头的校验和（checksum）就需要重新计算，否则网络包会被内核直接丢弃。</p><p>如果你不熟悉 IP 头校验和的计算方法也没关系，你可以很容易从成熟的开源项目中查找到相关的计算方法。比如，参考 Facebook 开源的 <a href=\"https://github.com/facebookincubator/katran/blob/main/katran/lib/bpf/csum_helpers.h#L30\">Katran</a>，你可以定义如下的 <code>ipv4_csum()</code> 函数来计算校验和：</p><pre><code class=\"language-c++\">static __always_inline __u16 csum_fold_helper(__u64 csum)\n{\n  int i;\n#pragma unroll\n  for (i = 0; i &lt; 4; i++)\n  {\n  if (csum &gt;&gt; 16)\n    csum = (csum &amp; 0xffff) + (csum &gt;&gt; 16);\n  }\n  return ~csum;\n}\n\nstatic __always_inline __u16 ipv4_csum(struct iphdr *iph)\n{\n  iph-&gt;check = 0;\n  unsigned long long csum = bpf_csum_diff(0, 0, (unsigned int *)iph, sizeof(struct iphdr), 0);\n  return csum_fold_helper(csum);\n}\n</code></pre><p>关于校验和的具体算法，你可以参考 TCP/IP 协议相关的原理书籍（如《TCP/IP详解》）来理解，这里我就不详细展开了。</p><p>有了校验和的计算方法之后，最后更新 IP 头的 checksum，再返回 <code>XDP_TX</code> 把数据包从原网卡发送出去，交给内核去转发就可以了。下面展示的就是更新校验和的实现方法：</p><pre><code class=\"language-c++\">SEC(\"xdp\")\nint xdp_proxy(struct xdp_md *ctx)\n{\n  ...\n  /* 重新计算校验和 */\n  iph-&gt;check = ipv4_csum(iph);\n\n  /* 把数据包从原网卡发送出去 */\n  return XDP_TX;\n}\n</code></pre><p>把上述代码保存到一个文件 <code>xdp-proxy.bpf.c</code> 中，就完成了 XDP eBPF 程序的开发（你还可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/xdp/xdp-proxy.bpf.c\">GitHub</a> 中找到完整的代码）。</p><h3>编译并生成脚手架头文件</h3><p>有了 XDP 程序之后，接下来的第二步就比较简单了。我们只需要执行下面的 <code>clang</code> 命令，把 XDP 程序编译成字节码，再执行 <code>bpftool gen skeleton</code> 命令生成脚手架头文件即可：</p><pre><code class=\"language-bash\">clang -g -O2 -target bpf -D__TARGET_ARCH_x86 -I/usr/include/x86_64-linux-gnu -I. -c xdp-proxy.bpf.c -o xdp-proxy.bpf.o\nbpftool gen skeleton xdp-proxy.bpf.o &gt; xdp-proxy.skel.h\n</code></pre><h3>开发用户态程序</h3><p>对于第三步用户态程序的开发，它的基本流程跟 <a href=\"https://time.geekbang.org/column/article/484372\">08 讲</a> 中的内核跟踪案例是类似的，也是需要引入脚手架头文件、增大 RLIMIT_MEMLOCK、初始化并加载 BPF 字节码，最后再挂载 XDP 程序这几个步骤。忽略错误处理步骤，最核心的实现步骤如下所示：</p><pre><code class=\"language-c++\">// 1. 引入脚手架头文件\n#include \"xdp-proxy.skel.h\"\n\n// C语言主函数\nint main(int argc, char **argv)\n{\n    // 2. 增大 RLIMIT_MEMLOCK（默认值通常太小，不足以存入BPF映射的内容）\n    struct rlimit rlim_new = {\n      .rlim_cur = RLIM_INFINITY,\n      .rlim_max = RLIM_INFINITY,\n    };\n    err = setrlimit(RLIMIT_MEMLOCK, &amp;rlim_new);\n\n    // 3. 初始化BPF程序\n    struct xdp_proxy_bpf *obj = xdp_proxy_bpf__open();\n\n    // 4. 加载BPF字节码\n    err = xdp_proxy_bpf__load(obj);\n\n    // 5. TODO: 挂载XDP程序到eth0网卡\n}\n</code></pre><p>这段代码中，前面 4 个步骤跟<a href=\"https://time.geekbang.org/column/article/484372\"> 08 讲</a> 中的<a href=\"https://time.geekbang.org/column/article/484372\">内核跟踪案例</a>是一样的，这儿不再详细展开。</p><p>而对于第 5 步的挂载过程，我在<a href=\"https://time.geekbang.org/column/article/483364\"> 06 讲</a> 中曾经提到，你可以使用 <code>ip link</code> 命令来挂载 XDP 程序。当时讲到的是在主机中挂载 XDP 的步骤，而在容器中的步骤其实也是一样的（注意，在容器中的虚拟网卡上，只支持以通用模式挂载）。</p><p>下面的代码展示的就是把 XDP 字节码复制到负载均衡容器并挂载到 eth0 网卡的过程：</p><pre><code class=\"language-bash\"># 复制字节码到容器中\ndocker cp xdp-proxy.bpf.o lb:/\n\n# 在容器中安装iproute2命令行工具\ndocker exec -it lb apk add iproute2 --update\n\n# 在容器中挂载XDP程序到eth0网卡\ndocker exec -it lb ip link set dev eth0 xdpgeneric object xdp-proxy.bpf.o sec xdp\n</code></pre><p>除了使用命令行工具之外，你还可以在用户态程序中调用库函数来挂载 XDP 程序，从而避免引入额外的命令行工具依赖（比如，不再需要安装 iproute2 系统工具）。</p><p>libbpf 提供了一个 <code>bpf_set_link_xdp_fd(int ifindex, int fd, __u32 flags)</code> 函数，可用于把 XDP 程序挂载到网卡。这个函数需要网卡序号和 XDP 程序文件描述符作为参数，查询这些参数并挂载 XDP 的过程如下所示：</p><pre><code class=\"language-c++\">    unsigned int ifindex = if_nametoindex(\"eth0\");\n    int prog_id = bpf_program__fd(obj-&gt;progs.xdp_proxy);\n    err = bpf_set_link_xdp_fd(ifindex, prog_id, XDP_FLAGS_UPDATE_IF_NOEXIST|XDP_FLAGS_SKB_MODE);\n</code></pre><p>这段代码中，挂载参数标志 <code>XDP_FLAGS_SKB_MODE</code> 等同于 <code>ip link</code> 命令中的 <code>xdpgeneric</code>，表示以通用模式挂载。</p><p>把上述代码保存到 <code>xdp-proxy.c</code> 文件中（你还可以在 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/xdp/xdp-proxy.c\">GitHub</a> 中找到完整的代码），再执行下面的编译命令，就可以将其编译为静态链接的可执行文件。采用静态链接的一个好处是容易在容器中分发，只需要把最终的二进制文件放入容器中即可运行，不再需要安装额外的依赖环境。</p><pre><code class=\"language-bash\">clang -g -O2 -Wall -I. -c xdp-proxy.c -o xdp-proxy.o\nclang -Wall -O2 -g xdp-proxy.o -static -lbpf -lelf -lz -o xdp-proxy\n</code></pre><p>到这里，完整的 eBPF 程序就开发好了。它是不是可以正常工作呢？如果可以正常工作，性能又会怎么样？接下来，我们把它放到容器中测试一下看看。</p><h3>性能测试</h3><p>在终端中执行下面的 docker 命令，把 XDP 程序复制到负载均衡容器中，并执行 XDP 程序：</p><pre><code class=\"language-bash\"># 复制XDP程序到容器\ndocker cp xdp-proxy lb:/\n\n# 在容器中加载XDP程序\ndocker exec -it lb /xdp-proxy\n</code></pre><p>然后，进入客户端容器终端中，执行 <code>apk</code> 命令安装 <code>curl</code> 和 <code>wrk</code> 工具，接着再使用 <code>curl</code> 访问负载均衡器：</p><pre><code class=\"language-bash\">docker exec -it client sh\n\n# (以下命令运行在client容器中)\n/ # curl \"http://172.17.0.5\"\n</code></pre><p>如果你看到 <code>Hostname: http1</code> 或者 <code>Hostname: http12</code> 的输出，说明 XDP 已经成功运行，并且它的负载均衡功能也是正常的。</p><p>接下来，继续在客户端容器终端中执行 <code>wrk</code> 命令，给负载均衡器做个性能测试：</p><pre><code class=\"language-bash\"># (以下命令运行在client容器中)\n\n# 安装wrk工具\n/ # apk add curl wrk --update\n\n# 执行性能测试\n/ # wrk -c100 \"http://172.17.0.5\"\n</code></pre><p>稍等一会，你会看到如下的输出（在你的环境下可能看到不同数值，具体的性能指标取决于运行环境和配置）：</p><pre><code class=\"language-bash\">Running 10s test @ http://172.17.0.5\n  2 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     6.37ms   11.17ms 295.43ms   98.97%\n    Req/Sec     9.09k   422.06    10.09k    75.00%\n  180889 requests in 10.02s, 31.39MB read\nRequests/sec:  18048.65\nTransfer/sec:      3.13MB\n</code></pre><p>从输出中你可以看到，平均每秒请求数是 18048，每个线程的平均延迟是 6.37ms。回顾一下 <a href=\"https://time.geekbang.org/column/article/485702?cid=100104501\">12 讲</a> 中的套接字 eBPF 程序的性能测试结果，它的平均每秒请求数是 15300，而每个线程的平均延迟是 6.88ms。这说明，相比套接字程序，XDP 程序在平均每秒请求数上提升了 18%。</p><p>最后，不要忘记清理今天的案例环境。由于所有服务都运行在容器中，我们只需要执行下面的命令，删除今天创建的所有容器，即可完成清理工作：</p><pre><code class=\"language-bash\">docker rm -f lb client http1 http2\n</code></pre><h2>小结</h2><p>今天，我带你使用 libbpf 开发了一个基于 XDP 的负载均衡服务。</p><p>XDP 程序在网络驱动程序刚刚收到数据包的时候触发执行。由于还未分配内核 SKB 数据结构，XDP 程序只能根据 TCP/IP 协议的封包格式，从原始网络包中提取所需协议层的数据，进而再按照需要进行改写或转发。XDP 程序修改过的数据包可以转发给相同或不同的网卡，再交给内核协议栈继续处理；或者，跟处理逻辑无关的包不做任何处理，直接交给内核协议栈进行处理。</p><p>今天的案例把 XDP 程序挂载到了容器的虚拟网卡上，由于采用了通用模式挂载，它的执行过程其实还是在内核中运行的。在实际生产环境中，你可以以原生模式或卸载模式把 XDP 程序挂载到支持 XDP 的网卡上，从而获得最优的网络性能。</p><h2>思考题</h2><p>最后，我想邀请你来聊一聊：</p><ol>\n<li>在今天的案例中，我们把 XDP 程序挂载到了负载均衡容器中的网卡上。如果把它直接挂载到主机的 eth0 网卡，会有什么样的现象？</li>\n<li>针对今天的负载均衡场景，还有哪些方法可以进一步优化 XDP 程序的性能？</li>\n</ol><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"12｜高性能网络实战（上）：如何开发一个负载均衡器？","id":485702},"right":{"article_title":"阶段总结｜实用 eBPF 工具及最新开源项目总结","id":487227}}},{"article_id":487227,"article_title":"阶段总结｜实用 eBPF 工具及最新开源项目总结","article_content":"<p>你好，我是倪朋飞。</p><p>到上一讲的高性能网络实战为止，我们就完成了“实战进阶篇”的学习。在这个模块中，我带你从实战出发，利用 BCC、libbpf、bpftrace 等常用的 eBPF 开发工具，开发了应用于内核跟踪、用户态跟踪、网络跟踪、容器安全、高性能网络等各个场景的 eBPF 程序。通过这个模块的学习，我想你已经掌握了 eBPF 在不同场景的应用方法，并能够举一反三，利用类似的步骤把 eBPF 应用到实际的工作当中去。</p><p>“实战进阶篇”结束后，我们课程的常规更新阶段的正文内容就基本更新完了。在之前的课程内容中，除了用实践帮你更好地理解原理，我也在案例中穿插介绍了很多开源项目和 eBPF 工具，帮你更好地利用 eBPF 去解决实际的问题。今天，我就基于现阶段的 eBPF 最新技术发展，为你汇总最实用的 eBPF 工具以及最新的开源项目状态。这样，在后续的学习和实践过程中，你就可以按图索骥，根据应用场景选择最合适的方案。今天的内容将分为开发工具集、实用工具集和最新开源项目三个部分。</p><h2>开发工具集</h2><p>首先来看第一个部分，开发工具集，也就是在开发 eBPF 程序时常用的开发库以及开发工具。</p><p>在这门课里，我已经在不同案例中为你反复介绍了 BCC、libbpf、bpftrace 等常用的开发库，以及把 eBPF 源代码编译为字节码的 LLVM 工具。除了这些方法，你还可以直接在内核源码库中，参考已有的示例（示例路径为 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/samples/bpf\">samples/bpf</a> ）进行 eBPF 程序的开发。</p><!-- [[[read_end]]] --><p>你还记得这些不同开发方法的主要应用场景和使用步骤吗？为了帮助你理解，我把它们的主要应用场景和使用方法总结成了一个表格，方便你在需要的时候参考。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/9f/8e91575ba3221d6fc66fb726a433149f.jpg?wh=1920x1140\" alt=\"图片\"></p><p>BCC、libbpf 以及内核源码，都主要使用 C 语言开发 eBPF 程序，而实际的应用程序可能会以多种多样的编程语言进行开发。所以，开源社区也开发和维护了很多不同语言的接口，方便这些高级语言跟 eBPF 系统进行交互。比如，我们课程多次使用的 BCC 就提供了 Python、C++ 等多种语言的接口，而使用 BCC 的 Python 接口去加载 eBPF 程序，要比 libbpf 和内核源码的方法简单得多。</p><p>在这些开发库的基础上，得益于 eBPF 在动态跟踪、网络、安全以及云原生等领域的广泛应用，开源社区中也诞生了各种编程语言的开发库，特别是 Go 和 Rust 这两种语言，其开发库尤为丰富。</p><p>下面的表格列出了常见的 Go 语言开发库，以及它们的使用场景：</p><p><img src=\"https://static001.geekbang.org/resource/image/c6/74/c6f49efdb9095fe35603b902d53fca74.jpg?wh=1920x959\" alt=\"图片\"></p><p>在使用这些 Go 语言开发库时需要注意，<strong>Go 开发库只适用于用户态程序中</strong>，可以完成 eBPF 程序编译、加载、事件挂载，以及 BPF 映射交互等用户态的功能，而内核态的 eBPF 程序还是需要使用 C 语言来开发的。</p><p>而对于 Rust 来说，由于其出色的安全和性能，也诞生了很多不同的开发库。下面的表格列出了常见的 Rust 语言开发库，以及它们的使用场景：</p><p><img src=\"https://static001.geekbang.org/resource/image/92/47/921a22f9d9198532ff811ebec3169747.jpg?wh=2284x1126\" alt=\"\"></p><p>从 Go 和 Rust 语言的开发库中你可以发现，纯编程语言实现的开发库（即不依赖于 libbpf 和BCC 的库）由于很难及时通过内核进行同步，通常都有一定的功能限制；而 libbpf 和 libbcc 的开发语言绑定通常功能更为完善，但开发和运行环境就需要安装 libbpf 和 libbcc（部分开发库支持静态链接，在运行环境不再需要 libbpf 或 libbcc）。因为底层的实现是类似的，所以掌握了 libbpf 和 BCC 的使用方法，在学习其他语言的绑定时会更容易理解。</p><p>了解常见的开发工具和不同编程语言的开发库之后，我们再来看看，有哪些开箱即用的 eBPF 实用工具。</p><h2>实用工具集</h2><p>作为最常用的开发工具集，BCC 和 bpftrace 除了可以帮你简化 eBPF 程序的开发和运行之外，它们其实也都提供了丰富的实用工具。在你安装 BCC 和 bpftrace 时，这些实用工具也都会默认安装到你的系统中。如果它们刚好已经满足了你的使用场景，那你就可以直接使用，而不再需要额外去开发新的程序。</p><p>比如，安装好 BCC 和 bpftrace 之后，你可以直接执行下面的命令，来跟踪 <code>execve</code> 系统调用。</p><ul>\n<li>使用 bpftrace：<code>execsnoop.bt</code></li>\n<li>使用 BCC：<code>execsnoop-bpfcc</code></li>\n</ul><p>BCC 提供的工具非常丰富，很多工具在动态跟踪、性能优化、调试排错等场景都可以直接使用。下面的图片（图片来自 <a href=\"https://www.brendangregg.com/Perf/bcc_tracing_tools.png\">www.brendangregg.com</a>）列出了常用 BCC 工具在内核中的位置，你可以根据需要选择使用：</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/8a/a1b4ef8276aabb32e0663185f372b08a.png?wh=963x674\" alt=\"图片\"></p><p>在开发自己的 eBPF 工具时，假如你碰到了难题，不妨来看看 BCC 是否提供了类似的工具，这些工具也是开发 eBPF 程序最好的参考资料之一。</p><p>同 BCC 类似，bpftrace 也提供了大量的<a href=\"https://github.com/iovisor/bpftrace/tree/master/tools\">工具</a>。下面的图片（图片来自 <a href=\"https://www.brendangregg.com/BPF/bpftrace_tools_early2019.png\">brendangregg.com</a>）列出了一些常用的 bpftrace 工具集。虽然这些工具不如 BCC 工具集完善，但得益于 bfptrace 提供的高级语言，你可以用脚本的方式更快地编写 eBPF 程序。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/2c/5f00790c151af829bbac6ef9c0f9e92c.png?wh=1920x1089\" alt=\"图片\"></p><p>除了 BCC 和 bpftrace 工具集之外，内核自带的 bpftool 也是最常用的 eBPF 工具之一。在<a href=\"https://time.geekbang.org/column/article/482459\"> 05 讲</a> 和 <a href=\"https://time.geekbang.org/column/article/485702\">12 讲</a> 中，我曾介绍过 bpftool 的一些使用方法，包括查询内核支持的 eBPF 特性、查看和操作 BPF 映射、加载和挂载 BPF 字节码，以及生成 eBPF 程序的脚手架头文件等。你可以执行 <code>bpftool help</code> 或 <code>bpftool &lt;子命令&gt; help</code> 查询更多的使用方法。</p><p>了解过常见的 eBPF 实用工具，最后我们再来看一些最新的 eBPF 开源项目，以及它们的最新发展状态。</p><h2>最新开源项目进展</h2><p>根据功能和使用场景的不同，我把 eBPF 相关的开源项目分为以下几个类别：</p><ul>\n<li>第一类是 eBPF 在操作系统内核的支持，包括 Linux 内核和 <a href=\"https://github.com/microsoft/ebpf-for-windows\">eBPF for Windows</a>。</li>\n<li>第二类是辅助 eBPF 程序开发和运行的开源项目，上述两个部分中提到的所有开源项目都属于这个类别。</li>\n<li>第三类是 eBPF 在跟踪监控、网络、安全等各个领域应用的开源项目，Cilium、Falco、Karan 等都属于这个类别。</li>\n</ul><p>接下来，我带你一起来看看每个类别都有哪些开源项目，以及它们最新的发展状态是什么样的。</p><h3><strong>操作系统内核类</strong></h3><p>首先来看第一个类别，eBPF 在操作系统中的支持。之前在 <a href=\"https://time.geekbang.org/column/article/479384\">01讲</a> 中，我为你梳理了 eBPF 的发展历程。其中，2021 年微软发布 eBPF for Windows ，以及 eBPF 基金会的成立代表着 eBPF 生态的空前活跃。</p><p>一方面，<strong>在 Linux 内核中，eBPF 是内核社区最活跃的子模块之一，并且还处在一个快速发展的阶段</strong>。在未来，eBPF 不仅会支持更多的内核特性，也有可能会作为最底层的基础，替换掉很多性能不佳的内核模块（如 iptables 防火墙等）。</p><p>另一方面，在 Windows 系统中，<a href=\"https://github.com/microsoft/ebpf-for-windows\">eBPF for Windows</a><strong>正在努力把 eBPF 技术带入 Windows 操作系统</strong>。同 Linux 类似，eBPF for Windows 也提供了 libbpf API，这样未来你开发的一套 eBPF 程序就可以在两种不同的操作系统中运行。eBPF for Windows 同样处在快速迭代的过程中，目前还没有发布稳定版本。在这门课之后的动态更新阶段，也就是“技术雷达篇”中，我会为你持续跟踪它的发展状态。</p><h3><strong>开发工具类</strong></h3><p>说完操作系统类的开源项目，接下来的第二个类别是辅助 eBPF 程序开发和运行的开源项目，包括编译工具 LLVM 和 GCC，开发工具集中的各类开发库，以及实用工具集中的各类 eBPF 工具等。其中，后两个类别的开源项目已经在前面讲过了，这儿不再展开。</p><p>而对编译工具来说，自从 3.7 版本支持 BPF 编译以来，LLVM 一直都是默认的 eBPF 编译工具。我们课程提到的所有开源项目，除 eBPF for Windows 之外，都是借助 LLVM 把 eBPF 程序编译为字节码的。GCC 虽然也已经支持了 <a href=\"https://gcc.gnu.org/onlinedocs/gcc/eBPF-Options.html#eBPF-Options\">eBPF 编译选项</a>，但目前来说还有很多的限制，也没有大型开源项目在使用它。所以，我推荐你在编译 eBPF 程序时，还是继续使用 LLVM 工具。</p><h3><strong>eBPF 应用类</strong></h3><p>最后一个类别是 eBPF 在跟踪监控、网络、安全等各个领域应用的开源项目。这类开源项目的数量是最多的，其中比较典型的是 Cilium、Falco、Karan、Calico 以及 Tracee 等已经拥有众多用户的知名开源项目，代表了 eBPF 在网络和安全方面的典型应用。</p><p>由于这类开源项目的数量比较多，我把几个典型的开源项目做成了一个表格，方便你在需要时参考。如果它们刚好满足你的应用场景，那么在开发新的 eBPF 应用之前，不妨先去尝试一下这些开源项目。我相信，即便你最终没有直接使用这些开源项目，了解它们的设计思路和实现方法也会给你带来意想不到的启发。</p><p><img src=\"https://static001.geekbang.org/resource/image/06/ca/067cc6447107b090a1aecce53eb4b4ca.jpg?wh=1920x1249\" alt=\"图片\"></p><h2>小结</h2><p>今天，我对课程前几个模块的学习内容做了个阶段总结，汇总了在 eBPF 开发和实践过程中最常用的开发工具集和实用工具集，并给你分享了最新的开源项目以及它们的发展状态。</p><p>我希望你在了解了这些工具和项目之后，可以按图索骥，根据实际需要选择最合适的方案。在 eBPF 的学习和实践过程中，如果你碰到了难题，也可以回过头来参考一下这些开源项目，看看它们是如何解决你的难题的。</p><p>最后，我还想提醒你一下：虽然今天分享的工具和开源项目种类繁多，但这并不意味着它们可以帮你解决所有的问题。所以，在实际的应用中，除了学会它们的使用方法，更重要的是<strong>理解它们背后的设计思想和运行原理</strong>。因为只有这样，你才能做到举一反三，真正借鉴开源社区的最佳实践去解决你的问题。</p><h2>思考题</h2><p>在这一讲的最后，我想邀请你来聊一聊：</p><p>在今天分享的工具和开源项目中，有没有一些是你已经在工作中使用过的呢？如果有使用过，它们帮你解决了哪些实际的问题？或者，还有哪些其他的工具和开源项目是你经常使用的呢？</p><p>欢迎在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p><h2>参考链接</h2><p>由于文中表格不支持链接，今天内容中所提到的开源项目及相关文档链接我都放在了这里，你可以自行查阅。</p><ul>\n<li><a href=\"https://github.com/aya-rs/aya\">Aya</a></li>\n<li><a href=\"https://github.com/iovisor/bcc/blob/master/docs/tutorial_bcc_python_developer.md\">BCC文档</a></li>\n<li><a href=\"https://github.com/solo-io/bumblebee\">Bumblebee</a></li>\n<li><a href=\"https://github.com/projectcalico/calico\">Calico</a></li>\n<li><a href=\"https://github.com/cilium/cilium\">Cilium</a></li>\n<li><a href=\"https://github.com/falcosecurity/falco\">Falco</a></li>\n<li><a href=\"https://github.com/cilium/hubble\">Hubble</a></li>\n<li><a href=\"https://github.com/facebookincubator/katran\">Katran</a></li>\n<li><a href=\"https://github.com/kubearmor/KubeArmor\">KubeArmor</a></li>\n<li><a href=\"https://elixir.bootlin.com/linux/v5.13/source/samples/bpf\">Kernel BPF 示例</a></li>\n<li><a href=\"https://elixir.bootlin.com/linux/v5.13/source/tools/lib/bpf\">Kernel libbpf</a></li>\n<li><a href=\"https://l3af.io\">L3AF</a></li>\n<li><a href=\"https://github.com/pixie-io/pixie\">Pixie</a></li>\n<li><a href=\"https://github.com/aquasecurity/tracee\">Tracee</a></li>\n<li><a href=\"https://github.com/aquasecurity/libbpfgo\">aquasecurity/libbpfgo</a></li>\n<li><a href=\"https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md\">bpftrace文档</a></li>\n<li><a href=\"https://github.com/cilium/ebpf\">cilium/ebpf</a></li>\n<li><a href=\"https://github.com/dropbox/goebpf\">dropbox/goebpf</a></li>\n<li><a href=\"https://github.com/microsoft/ebpf-for-windows\">eBPF for Windows</a></li>\n<li><a href=\"https://gcc.gnu.org/onlinedocs/gcc/eBPF-Options.html#eBPF-Options\">GCC eBPF 编译选项</a></li>\n<li><a href=\"https://github.com/iovisor/gobpf\">iovisor/gobpf</a></li>\n<li><a href=\"https://github.com/iovisor/kubectl-trace\">kubectl-trace</a></li>\n<li><a href=\"https://github.com/libbpf/libbpf-bootstrap\">libbpf-bootstrap</a></li>\n<li><a href=\"https://github.com/libbpf/libbpf-rs\">libbpf-rs</a></li>\n<li><a href=\"https://github.com/iovisor/ply\">ply</a></li>\n<li><a href=\"https://github.com/redsift/redbpf\">redbpf</a></li>\n<li><a href=\"https://github.com/rust-bpf/rust-bcc\">rust-bcc</a></li>\n</ul>","neighbors":{"left":{"article_title":"13｜高性能网络实战（下）：如何完善负载均衡器？","id":486353},"right":{"article_title":"未来可期｜邀你与 eBPF 共赴一场技术革新之约","id":487921}}},{"article_id":487921,"article_title":"未来可期｜邀你与 eBPF 共赴一场技术革新之约","article_content":"<p>你好，我是倪朋飞。</p><p>不知不觉间，这门课程已经上线一个月了，第一阶段的正文更新到今天就暂告一段落了。在这里，首先恭喜你完成了这一阶段的学习，掌握了关于 eBPF 的基本知识，也获得了理解 eBPF 机制、把握 eBPF 进化方向的抓手。接下来的动态更新阶段，我会带着你持续跟进 eBPF 技术的最新进展、发展趋势，相信之后四年持续学习的你，在结课时会有更多的收获。</p><p>在准备这门课的过程中，我有了很多感想和收获，接下来就把它们分享给你。</p><h2>我为什么要做这门动态更新的eBPF课程？</h2><p>去年10月的时候，极客时间团队就联系到了我，商量要一起筹备平台上的第一个动态专栏。我第一时间就想到了 eBPF 这个主题。一方面，是因为我觉得 eBPF 会是我的第一季专栏中涉及的动态追踪技术的完美补充；另一方面，动态更新的形式也很适合 eBPF 这样还在快速发展、变更频繁的技术。</p><p>早在几年前开设<a href=\"https://time.geekbang.org/column/intro/100020901?tab=catalog\">《Linux 性能优化实战》</a>专栏的时候，我就发现了一个问题：在讨论相对简单的单指标性能时，同学们都很热情；但在综合多个指标之后，有些同学就掉队了。特别是在涉及系统底层知识时，很多同学虽然也可以利用课程中的工具解决一些性能问题，但由于对内核原理的潜在恐惧，在分析多性能指标之间的相互关系时，还是不能利用底层知识把它们全部贯穿起来。</p><!-- [[[read_end]]] --><p><strong>而有了 eBPF 之后，系统底层的运行原理就不再是一个黑盒子，内核变得完全开放了。</strong>你可以根据自己的需要，去观测或定制内核和应用底层的运行状况，这样，我们就不再需要恐惧内核的底层知识了。</p><p>并且，因为 eBPF 是一个还在快速发展中的新技术，动态更新的交付方式显然可以避免知识快速过期的问题。就这样，经过近三个月的筹备，《eBPF 核心技术与实战》专栏诞生了，也成为极客时间上第一个动态交付的专栏。</p><p>不过，如你所见，这门课并没有把所有的内容都放到动态更新的过程中交付，而是根据内容和时间划分成了常规更新和动态更新两大阶段。我之所以这样设计，是想让你通过已经更新的十多讲内容，快速掌握 eBPF 的核心原理，以及最基本的使用方法。我相信你在看到这篇内容时，已经完成了第一阶段的学习，并开始尝试把 eBPF 应用到你的工作中了。</p><h2>第二阶段的课程，邀请你与我共建</h2><p>在课程更新的这一个月里，我非常高兴地看到，很多同学在留言区分享了自己的学习心得和实践经验，并针对一些知识盲点提出了很有价值的问题。非常感谢同学们的积极分享和讨论，这不仅帮助我完善了课程的现有内容，还给了我很多对于后续更新内容的启发。</p><p>这门课的第一阶段侧重于 eBPF 最核心的原理，以及这些原理的基本应用方法，课程中案例的应用场景也相对简单。而在接下来的动态更新阶段，也就是“技术雷达篇”中，我不仅会<strong>带你持续追踪 eBPF 的最新发展和最新应用，也会以最有代表性的开源项目为例，给你带来 eBPF 在复杂场景中的综合应用方法</strong>。正如开篇词中向你承诺的那样，我将会在每个季度交付一篇文章，每年年末也会交付一篇年终盘点。第二阶段的正式更新将从<strong> 2022 年 4 月</strong>开始，记得到时候回来继续学习呀！</p><p>在这里，我还想说的是：虽然这门课的更新暂时告一段落了，但还是非常希望你能继续跟我保持交流，积极参与到课程之后内容的共建中来：</p><ul>\n<li>我给你准备了一个<a href=\"https://jinshuju.net/f/tm8Ggu\">调查问卷</a>。题目不多，大概几分钟就可以填完，主要是想听一下你对这门课的看法和建议。也欢迎你在问卷里跟我分享你感兴趣的、未来想听到的 eBPF 相关话题，十分期待你的反馈！</li>\n<li>欢迎随时在留言区分享你的学习心得和实践经验。我在这门课中一直强调，大量实践 + 深入思考，就是掌握所有新技术的法宝。而想进行深入思考，最有效的路径之一就是把它分享出来，同他人进行交流和碰撞。因此，如果你对课程的内容有什么疑问，或者有一些经验想要分享，那么不要犹豫，发到留言区里和我们一起讨论吧！</li>\n<li>为了方便同学们互相交流、答疑，这门课还建立了微信群，进群入口在<a href=\"https://jinshuju.net/f/wmBy9q\">这里</a>，欢迎你加入。只要是跟这门课相关的话题，在里面都可以畅所欲言。</li>\n</ul><p>总之，如果你有对我们课程未来内容的建议，欢迎随时提出来。希望我们不仅是课程教与学的关系，更可以共同参与到课程内容的建设中，一起完善和构建一个最贴近实践的 eBPF 知识体系！</p><p>最后，非常感谢你对这门课，以及对动态更新这个新形式的支持。希望第一阶段的课程已经帮你掌握了 eBPF 的核心原理和基本的应用方法。在接下来的四年里，我邀请你和我一起继续深入 eBPF 的实践应用，紧跟 eBPF 的发展趋势，共赴一场技术革新之约。</p>","neighbors":{"left":{"article_title":"阶段总结｜实用 eBPF 工具及最新开源项目总结","id":487227},"right":{"article_title":"难点解析｜eBPF 开发环境搭建及内核编译详解","id":510472}}},{"article_id":510472,"article_title":"难点解析｜eBPF 开发环境搭建及内核编译详解","article_content":"<p>你好，我是倪朋飞。</p><p>转眼间，距离这门课的常规更新结束已经过去了两个月的时间。非常高兴看到很多同学都坚持学习到了最后，并对课程中的各个案例进行了实践操作，甚至把它们扩展到了更多的场景中。</p><p>从今天开始，我们的课程就进入了动态更新阶段——“技术雷达篇”。我会根据同学们反馈的热点问题，以及 eBPF 最新的发展状况和实践案例，动态调整这一阶段的内容。在为你解惑的同时，这一动态模块也会交付更深入的 eBPF 内核原理，以及它在实际生产环境中的综合应用方法。</p><p>这一讲是动态更新阶段的第一篇，也是我对很多同学留言反馈的统一解答。我会带你重新梳理一下 eBPF 开发环境的搭建方法，以及内核的配置和编译方法。</p><h2>关于 eBPF 开发环境搭建的三个典型问题</h2><p>在 <a href=\"https://time.geekbang.org/column/article/480094\">02讲</a> 中我曾提到，学习 eBPF 技术需要你具备一定的 Linux 操作系统基础，并掌握一些基础知识，包括常见 Linux 操作命令、软件包安装管理方法、C 语言程序的基本语法及编译运行步骤等。</p><p>在查看课程的留言和反馈时，我发现很多同学的疑惑是有共性的——这些疑惑正是源于对上面这些基础知识的掌握不够深入。其中，最典型的几个问题如下：</p><ul>\n<li>不熟悉 Linux 系统软件包的安装管理方法，比如找不到软件包 <code>linux-head-$(uname -r)</code>，无法定位软件包 <code>libbpf-dev</code> 等。</li>\n<li>不熟悉内置软件包版本过老之后的升级方法。比如，在 <code>bpftrace</code>、<code>bpftool</code> 等工具报错之后不知道该如何升级，而对于这两个工具，在很多发行版中都需要先升级到新版本，才可以体验最新的特性。</li>\n<li>不熟悉内核的编译和升级方法，比如不清楚内核编译开关的打开方法，不知道如何编译安装内核等。</li>\n</ul><!-- [[[read_end]]] --><p>接下来，我就带你一起来看看如何解决这些问题。</p><h2>如何配置 eBPF 开发环境？</h2><p>在 <a href=\"https://time.geekbang.org/column/article/481090\">03讲</a> 中我曾提到，为了体验最新的 eBPF 特性，推荐你使用自带 5.0 版本以上内核的发行版，并开启 <code>CONFIG_DEBUG_INFO_BTF=y</code> 和 <code>CONFIG_DEBUG_INFO=y</code> 这两个编译选项。而要满足这些条件，最简单的方法就是去公有云，或者借助 Vagrant 等工具，创建一个基于 Ubuntu 20.10、Fedora 31、RHEL 8.2、Debian 11 或者更新版本的虚拟机。</p><p>比如，使用 <a href=\"https://www.vagrantup.com\">Vagrant</a> 创建并登录 Ubuntu 21.10 虚拟机的步骤如下：</p><pre><code class=\"language-bash\"># 创建Ubuntu 21.10虚拟机\nvagrant init ubuntu/impish64\nvagrant up\n\n# 登录到虚拟机\nvagrant ssh\n</code></pre><p>这样登录虚拟机之后，再根据我们课程中的开发环境配置步骤或者 <a href=\"https://github.com/feiskyer/ebpf-apps#pre-requisites\">GitHub</a> 文档来安装依赖包就可以了。不过，在安装依赖包之前，不要忘记先执行一下 <code>sudo apt-get update</code> 等命令，刷新软件包列表，这样就可以避免软件包找不到的问题出现。</p><p>当然，你可能会说：最新的发行版和全新的虚拟机虽然很好用，但跟生产环境的差别太大了。比如，很多同学所在的公司还在继续使用着 CentOS、Ubuntu 18.04 等相对较老的系统。如果不使用新版本 Linux 内核引入的新特性，只要内核版本 &gt;=4.9，这些较老的系统其实也是可以稳定运行 eBPF 程序的（各个内核版本中支持的特性可以参考 <a href=\"https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md\">BCC 文档</a>）。</p><p>接下来，我就以 CentOS Stream 和 Ubuntu 18.04 为例，带你一起看看在旧版本的发行版中 eBPF 开发环境的详细配置方法。</p><h3>CentOS Stream</h3><p>首先，对于 CentOS 来说，根据<a href=\"https://centos.org/news-and-events/1321-reminder-centos-linux-is-eol-on-december-31st/\">官方文档</a>，CentOS 8 已在 2021 年底被放弃，所以不再推荐将其作为生产环境继续使用。对于已有的用户来说，可以升级到 CentOS Stream 或 Rocky Linux 继续获得开源社区的支持。比如，你可以执行下面的命令，把 CentOS 8 升级为 CentOS Stream 8：</p><pre><code class=\"language-bash\">sudo dnf --disablerepo '*' --enablerepo extras swap centos-linux-repos centos-stream-repos -y\nsudo dnf distro-sync -y\n</code></pre><p>打开一个终端，SSH 连接到 CentOS Stream 8 系统后，执行 <code>dnf info bcc-tools</code> 查询 BCC 的版本，你会看到如下的输出：</p><pre><code class=\"language-bash\">Available Packages\nName         : bcc-tools\nVersion      : 0.19.0\nRelease      : 5.el8\nArchitecture : x86_64\nSize         : 448 k\nSource       : bcc-0.19.0-5.el8.src.rpm\nRepository   : appstream\nSummary      : Command line tools for BPF Compiler Collection (BCC)\nURL          : https://github.com/iovisor/bcc\nLicense      : ASL 2.0\nDescription  : Command line tools for BPF Compiler Collection (BCC)\n</code></pre><p>从输出中你可以发现，它自带的 BCC 版本是 0.19.0，而根据 BCC 的<a href=\"https://github.com/iovisor/bcc/releases\">发布列表</a>，其最新的版本已经到了 0.24.0。所以，为了使用较新的 BCC，从源码编译安装就是比直接使用 dnf 安装更好的选择。</p><p>在终端中执行下面的命令，我们就可以从源码编译和安装 BCC 0.24.0 版本：</p><pre><code class=\"language-bash\"># 第一步，安装必要的开发工具和开发库\nsudo dnf makecache --refresh\nsudo dnf groupinstall -y \"Development tools\"\nsudo dnf install -y git bison flex cmake3 clang llvm bpftool elfutils-libelf-devel clang-devel llvm-devel ncurses-devel\n\n# 第二步，从源码编译安装BCC\ngit clone -b v0.24.0 https://github.com/iovisor/bcc.git\nmkdir bcc/build; cd bcc/build\ncmake -DENABLE_LLVM_SHARED=1 ..\nmake\nsudo make install\ncmake -DPYTHON_CMD=python3 .. # build python3 binding\npushd src/python/\nmake\nsudo make install\npopd\n</code></pre><p>命令执行成功后，所有的 BCC 工具都会安装到 <code>/usr/share/bcc/tools</code> 目录下。比如，你可以执行 <code>sudo python3 /usr/share/bcc/tools/execsnoop</code> 命令来运行 BCC 自带的 <a href=\"https://github.com/iovisor/bcc/blob/master/tools/execsnoop.py\">execsnoop</a> 工具。</p><p>而对于另外一个常用的 <a href=\"https://github.com/iovisor/bpftrace\">bpftrace</a> 来说，虽然也可以使用源码编译的方式安装，但实际上还有另外一个更简单的方式，那就是从 bpftrace 预先编译好的容器镜像中复制二进制文件。</p><p>我们执行下面的命令，安装容器工具 <a href=\"https://podman.io\">podman</a> 之后，借助 podman 拉取 bpftrace 容器镜像，再将其中的 bpftrace 二进制文件复制出来，就可以把 bpftrace 安装到当前目录了：</p><pre><code class=\"language-bash\"># 第一步，安装podman\nsudo dnf install -y podman\n\n# 第二步，下载镜像后从中复制bpftrace二进制文件\npodman pull quay.io/iovisor/bpftrace:master-vanilla_llvm_clang_glibc2.23\npodman run --security-opt label=disable -v $(pwd):/output quay.io/iovisor/bpftrace:master-vanilla_llvm_clang_glibc2.23 /bin/bash -c \"cp /usr/bin/bpftrace /output\"\n</code></pre><p>这里需要你留意一点：在上面的命令中，我们使用了 <a href=\"https://podman.io\">podman</a> 工具来拉取镜像并运行容器，这是因为 CentOS Stream 自带的软件包中不包含 Docker。</p><p>安装成功后，你可以执行下面的命令验证 bpftrace 的功能：</p><pre><code class=\"language-bash\">sudo ./bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(\"%s %s\\n\", comm, str(args-&gt;filename)); }'\n</code></pre><p>如果一切正常，你将会看到类似下面的输出：</p><pre><code class=\"language-bash\">Attaching 1 probe...\nvmstats /proc/meminfo\nvmstats /proc/stat\nvminfo /var/run/utmp\n...\n</code></pre><p>到这里，我们就完成了 CentOS Stream 开发环境的配置。接下来要讲的 Ubuntu 18.04 的安装配置方法也是类似的，只是相应的软件包管理工具要换成 apt 系列工具。</p><h3>Ubuntu 18.04</h3><p>首先，对于 BCC 的安装来说，由于 Ubuntu 系统中软件包的名字跟 CentOS 略有不同，所以在第一步安装开发工具和开发库时，我们需要做适当的调整。下面我们来看详细的安装步骤。</p><p>第一步，安装必要的开发工具和开发库：</p><pre><code class=\"language-bash\"># 第一步，安装必要的开发工具和开发库\nsudo apt update\nsudo apt install -y bison build-essential cmake flex git libedit-dev libllvm6.0 llvm-6.0-dev libclang-6.0-dev python zlib1g-dev libelf-dev libfl-dev python3-distutils linux-tools-$(uname -r)\n</code></pre><p>接下来的第二步是从源码编译安装 BCC，步骤跟上面的 CentOS Stream 是一样的，代码如下所示：</p><pre><code class=\"language-bash\"># 第二步，从源码编译安装BCC\ngit clone -b v0.24.0 https://github.com/iovisor/bcc.git\nmkdir bcc/build; cd bcc/build\ncmake -DENABLE_LLVM_SHARED=1 ..\nmake\nsudo make install\ncmake -DPYTHON_CMD=python3 .. # build python3 binding\npushd src/python/\nmake\nsudo make install\npopd\n</code></pre><p>同 CentOS Stream 系统一样，上述命令执行成功后，所有的 BCC 工具也会安装到 <code>/usr/share/bcc/tools</code> 目录下，你可以执行 <code>sudo python3 /usr/share/bcc/tools/execsnoop</code> 命令来验证 BCC 的安装。</p><p>BCC 安装成功后，我们再来安装 bpftrace。由于 Ubuntu 已经自带了 Docker 软件包，因此你可以使用 Docker，通过 bpftrace 容器镜像来完成类似 podman 的安装步骤。具体的安装命令如下所示：</p><pre><code class=\"language-bash\"># 第一步，安装docker\nsudo apt install -y docker.io\n\n# 第二步，下载镜像后从中复制bpftrace二进制文件\nsudo docker pull quay.io/iovisor/bpftrace:master-vanilla_llvm_clang_glibc2.23\nsudo docker run -v $(pwd):/output quay.io/iovisor/bpftrace:master-vanilla_llvm_clang_glibc2.23 /bin/bash -c \"cp /usr/bin/bpftrace /output\"\n</code></pre><p>安装成功后，你可以执行同样的 <code>sudo ./bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(\"%s %s\\n\", comm, str(args-&gt;filename)); }'</code> 命令验证 bpftrace 的功能。</p><p>到这里，基本的开发环境就配置好了。不过环境的配置并没有完全结束，在使用 bpftool 时（比如执行命令 <code>sudo bpftool prog dump jited id 2</code>），你很可能会碰到 <code>Error: No libbfd support</code> 的错误。这说明发行版自带的 bpftool 默认不支持 libbfd，这时就需要我们下载内核源码重新编译安装。</p><p>那么，该如何下载内核源码，又该如何编译 bpftool 呢？接下来，我就带你一起来看下。</p><h2>如何从内核源码编译升级 bpftool？</h2><p>从内核源码编译安装 bpftool 的第一步是下载内核的源码。根据发行版的不同，内核源码的下载方法可以分为三种：</p><ul>\n<li>利用发行版自带的工具，下载安装发行版提供的内核源码包。比如 RHEL、CentOS、Ubuntu 等，都可以使用这种方法。</li>\n<li>直接从内核网站 <a href=\"https://kernel.org\">kernel.org</a> 下载内核源码，注意下载前要先执行 <code>uname -r</code> 查询系统的内核版本。</li>\n<li>从发行版提供的代码仓库下载内核源码，比如对于 WSL2，就可以到 <a href=\"https://github.com/microsoft/WSL2-Linux-Kernel\">GitHub</a> 下载。</li>\n</ul><p>后两种方法比较简单，只要从相关的网站中找到链接就可以直接下载了；而对于第一种方法，你可以执行下面的步骤，借助 <code>yumdownloader</code> 或 <code>apt</code> 工具下载发行版提供的内核源码包。</p><p>比如，在 CentOS Stream 8 系统中，你可以执行下面的命令，下载内核源码并安装内核编译所需的开发工具和开发库：</p><pre><code class=\"language-bash\"># 第一步，开启必需的软件包仓库\nsudo dnf -y install dnf-plugins-core\nsudo dnf config-manager --set-enabled powertools\n\n# 第二步，下载内核源码\nyumdownloader --source kernel\nrpm -ivh kernel-*.src.rpm\n\n# 第三步，安装依赖包\ncd rpmbuild/SPECS/\nsudo dnf builddep kernel.spec\n\n# 第四步，解压内核源码并切换到解压后的内核源码目录（注意替换为你的内核版本）\nrpmbuild -bp --target=x86_64 kernel.spec\ncd ../BUILD/kernel-4.18.0-373.el8/linux-4.18.0-373.el8.x86_64/\n</code></pre><p>而在 Ubuntu 18.04 系统中，你则可以执行下面的命令，下载内核源码并安装内核编译所需的开发工具和开发库：</p><pre><code class=\"language-bash\"># 第一步，下载内核源码\nsudo apt install -y linux-source\n\n# 第二步，安装依赖包\nsudo apt install build-essential libncurses-dev bison flex libssl-dev libelf-dev dwarves libcap-dev -y\n\n# 第三步，解压并切换到内核源码目录（注意替换为你的内核版本）\ntar -jxf /usr/src/linux-source-4.15.0.tar.bz2\ncd linux-source-4.15.0/\n</code></pre><p>内核源码下载成功后，它的 <code>tools/bpf/bpftool</code> 目录就包含了 bpftool 工具的源代码，因而你就可以从这个目录重新编译和安装 bpftool。</p><p>不过在编译之前要注意，libbfd 库包含在 binutils 开发库中，因而你还需要先安装 binutils 开发包。具体的安装和编译步骤如下所示：</p><pre><code class=\"language-bash\"># 第一步，安装binutils开发库\n## CentOS执行dnf命令\nsudo dnf install -y binutils-devel\n## Ubuntu执行apt命令\nsudo apt install -y binutils-dev\n\n# 第二步，从源码编译并安装bpftool\nmake -C tools/bpf/bpftool\nsudo make install -C tools/bpf/bpftool/\n</code></pre><p>上述命令执行成功后，再次执行 <code>sudo bpftool prog dump jited id 2</code> 之后，你可以发现，现在已经可以正常看到 eBPF 程序的指令了。</p><p>到这里，bpftool 也就升级成功了。接下来，如果当前内核没有开启 <code>CONFIG_DEBUG_INFO</code> 等 eBPF 必需的内核选项，那就还需要进一步开启这些缺少的选项，并重新编译安装内核。</p><h2>如何配置和编译内核？</h2><p>在更改内核配置之前，由于我们希望保留当前内核的默认选项，而只开启 eBPF 相关的选项，这就需要我们把当前内核的配置选项复制到内核源码目录的 <code>.config</code> 中，即执行下面的复制命令：</p><pre><code class=\"language-bash\">cp -v /boot/config-$(uname -r) .config\n</code></pre><p>接着，继续执行 <code>make menuconfig</code> 就可以进入如下图所示的内核配置选项修改界面：</p><p><img src=\"https://static001.geekbang.org/resource/image/e1/85/e1d2db7fcca483ffb75b4dfe5388e985.png?wh=999x636\" alt=\"图片\"></p><p>在这个界面中，输入 <code>/</code> 将进入配置搜索界面：</p><p><img src=\"https://static001.geekbang.org/resource/image/0f/ee/0fa138d7b4e4f1726db410e7395cebee.png?wh=548x196\" alt=\"图片\"></p><p>在搜索框中输入要搜索的配置名 <code>DEBUG_INFO</code>（<code>CONFIG_</code> 前缀可有可无），然后敲回车，就会进入搜索结果界面，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/0e/12/0e184e5a9e4012ab5e5790b68a72f612.png?wh=963x577\" alt=\"图片\"></p><p>在搜索结果中，你可以发现：</p><ul>\n<li>Symbol 行显示了配置名称以及当前配置值；</li>\n<li>Type 行显示了配置的数据类型；</li>\n<li>Prompt 行显示了配置的含义、配置路径、定义位置以及依赖配置。</li>\n</ul><p>当配置值需要修改时，我们就可以通过配置路径，按键盘上的方向键找到具体的配置，然后根据界面提示修改。比如，导航到上图提示的“Compile-time checks and comipler options”菜单之后，你就可以找到“Compile the kernel with debug info”选项。如果它还没有开启，按一下键盘上的空格键就可以打开这个配置，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/0b/f7/0b54c72d6e9f4f15c5e3140dc00d4cf7.png?wh=912x214\" alt=\"图片\"></p><p>这里提醒下你，通过相同的方法修改完成所有配置之后，不要忘记通过 TAB 键切换到 <code>Save</code> 按钮保存配置。</p><p>配置更新之后，最后一步就是编译和安装内核了。执行下面的 make 命令，就可以编译并安装内核：</p><pre><code class=\"language-bash\"># 第一步，多线程编译内核\nmake -j $(getconf _NPROCESSORS_ONLN)\n\n# 第二步，安装内核模块、内核头文件以及内核二进制文件\nsudo make modules_install\nsudo make headers_install\nsudo make install\n</code></pre><p>当新的内核安装成功后，重启系统。恭喜，现在你就可以进入新的内核，然后使用上述步骤中开启的新特性了。</p><h2>小结</h2><p>今天，我带你一起梳理了 eBPF 开发环境的详细配置方法。如果你没有现成的 Linux 开发机器，那么借助于公有云平台、Windows WSL2、Vagrant 等各种方法，都可以使用最新的发行版创建一个全新的 Linux 虚拟机环境。而对于已有的 Linux 环境来说，我们也可以使用源码编译安装的方法，安装配置 BCC、bpftrace、内核等在内的各种依赖环境。</p><p>虽然今天的内容适用于还在运行 4.x 版本内核的发行版，我也要提醒你：使用旧版本的内核时，你是没法体验最新的 eBPF 特性的。当你需要体验最新的 eBPF 特性时，不妨考虑配置一个内核较新的发行版作为开发环境，同时在 eBPF 代码中设法兼容旧版本的内核。</p><p>今天这一讲就到这里了，下一次的动态更新预计会在6月份。如果你有对我们课程未来内容的建议，欢迎在评论区提出来，期待你与我一起完善和构建一个最贴近实践的 eBPF 知识体系。</p><h2>思考题</h2><p>先假设这样一个场景：我们想让 eBPF 跟踪程序可以兼容新旧版本的内核，以便在新版本内核中使用 CO-RE 等新的特性，而在旧版本内核中也可以正常运行。</p><p>那么，你知道哪些保持这种兼容性的方法呢？在这些方法中，你又使用过哪些呢？欢迎在评论区分享你的思考和实践经验。</p><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>","neighbors":{"left":{"article_title":"未来可期｜邀你与 eBPF 共赴一场技术革新之约","id":487921},"right":{"article_title":"难点解析｜ eBPF 多内核版本兼容详解","id":534577}}},{"article_id":534577,"article_title":"难点解析｜ eBPF 多内核版本兼容详解","article_content":"<p>你好，我是倪朋飞。</p><p>在上一讲中，我带你详细梳理了 eBPF 开发环境的配置方法，特别是 eBPF 相关开发软件包的安装和升级方法，以及内核的配置和编译方法。熟悉了 eBPF 的开发环境和内核编译之后，在留言区和微信群中我还看到很多同学依然在使用较旧版本的内核。而为了学习 eBPF，很多同学已经配置了一个非常新的内核版本作为开发环境，但却发现在新内核中开发的 eBPF 程序有时却没法直接在旧版本的内核中运行。</p><p>今天，我就带你一起来看看如何让 eBPF 程序兼容新旧版本的内核，以便在新版本内核中使用诸如 CO-RE 等新特性的同时，还可以在旧版本内核中正常运行。</p><h2>为什么需要考虑 eBPF 程序的内核兼容性？</h2><p>在开始正式的内容之前，我想你肯定有一个问题，那就是什么时候需要考虑新旧内核版本的兼容性，以及为什么会出现新旧内核版本兼容性的问题。</p><p>在理想情况下，开发测试环境和生产环境应该都是一致的，包括使用相同的内核版本。如果你已经满足了这个条件，那么自然也就不需要考虑内核兼容的问题。但注意这只是理想情况，实际情况下内核版本不一致的问题是不可避免的，比如：</p><ul>\n<li>为了获取更好的稳定性和社区支持，内核版本（甚至是 Linux&nbsp;发行版版本）需要持续跟随上游社区进行升级；</li>\n<li>为了采纳新技术，新的产品架构可能一开始就会采纳较新的内核，而使用旧内核的遗留系统还需要很长时间的迭代过程；</li>\n<li>为了获得更广的用户，很多商业或开源项目不仅要支持最新的内核版本，还需要兼容各种各样的用户环境，而这些用户所使用的内核版本也是千差万别的。</li>\n</ul><!-- [[[read_end]]] --><p>那么，在新旧版本的内核中运行 eBPF 程序时到底会碰到哪些问题呢？最典型的有以下这三个。</p><p><strong>第一，新内核引入的 eBPF 新特性无法在旧内核中运行。</strong>与绝大部分的技术产品类似，Linux 内核新版本中开发的特性绝大部分都不会迁移到旧版本的内核中，并且新特性的默认开启通常也需要一个比较长的过程（你可以在<a href=\"https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md\">这里</a>找到不同内核版本支持的 eBPF 特性）。</p><p><strong>第二，新内核中的数据结构、函数签名以及跟踪点等有可能跟旧版本内核不同。</strong>比如，在我们课程<a href=\"https://time.geekbang.org/column/article/481090\">第03讲</a>的跟踪案例中提到的 <code>openat2()</code> 系统调用是在内核 5.6 中才新增的，而旧版本内核需要换成 <code>openat()</code> 系统调用。</p><p><strong>第三，即便是相同的内核版本，不同的编译选项也可能会导致内核数据结构的不同。</strong>比如，<code>CONFIG_THREAD_INFO_IN_TASK</code> 的开关会直接影响内核 <a href=\"https://elixir.bootlin.com/linux/v5.13/source/include/linux/sched.h#L657\">task_struct</a> 数据结构所有成员变量的偏移地址。</p><pre><code class=\"language-c++\">  struct task_struct {\n  #ifdef CONFIG_THREAD_INFO_IN_TASK\n  \t/*\n  \t * For reasons of header soup (see current_thread_info()), this\n  \t * must be the first element of task_struct.\n  \t */\n  \tstruct thread_info\t\tthread_info;\n  #endif\n  \t/* -1 unrunnable, 0 runnable, &gt;0 stopped: */\n  \tvolatile long\t\t\tstate;\n    ...\n  }\n</code></pre><p>由于这些兼容性问题都是由内核版本不同而导致的，所以我们很容易想到的一个笨方法就是给所有不兼容的内核版本分别开发不同的 eBPF 程序。但这种方法的缺点太明显了，维护大量功能重复的 eBPF 程序成本太高，所以我并不推荐你使用这种方法。</p><p>那么还有哪些更好的方法呢？接下来，我就带你一起来看看如何更好地解决内核版本不同带来的兼容性问题。</p><h2>BCC 是如何兼容多内核版本的？</h2><p>BCC 和 bpftrace 作为使用最广泛的 eBPF 项目，自然也最容易碰到内核兼容性的问题。那么，它们是怎么解决这些兼容性问题的呢？其实也很简单，主要就是下面两个方法：</p><ul>\n<li>第一，在运行 eBPF 程序的时候使用当前系统安装的内核头文件进行就地编译，这样就可以确保 eBPF 程序中所引用的内核数据结构和函数签名等，跟运行中的内核是完全匹配的。</li>\n<li>第二，在 eBPF 程序编译前事先探测内核支持的函数签名和数据结构，进而为 eBPF 程序生成适配当前内核的版本。比如，在块设备 I/O 延迟跟踪程序 <a href=\"https://github.com/iovisor/bcc/blob/master/tools/biolatency.py#L209\">biolantecy</a> 中，BCC 借助库函数 <code>BPF.get_kprobe_functions()</code> 来判断内核是不是支持特定的探针函数，进而再根据结果去选择挂载点：</li>\n</ul><pre><code class=\"language-python\">  if BPF.get_kprobe_functions(b'__blk_account_io_start'):\n    b.attach_kprobe(event=\"__blk_account_io_start\", fn_name=\"trace_req_start\")\n  else:\n    b.attach_kprobe(event=\"blk_account_io_start\", fn_name=\"trace_req_start\")\n  \n  if BPF.get_kprobe_functions(b'__blk_account_io_done'):\n      b.attach_kprobe(event=\"__blk_account_io_done\", fn_name=\"trace_req_done\")\n  else:\n      b.attach_kprobe(event=\"blk_account_io_done\", fn_name=\"trace_req_done\")\n</code></pre><p>当然了，BCC 采用的这些方法虽然解决了内核版本兼容的问题，但同时也存在很多的缺点，包括需要在所有目标机器安装开发工具和内核头文件、编译消耗额外资源、eBPF 程序启动慢以及编译时错误难以排查等。</p><p>那么，有没有更好的方法呢？答案是肯定的。Linux 内核维护者提供了一个更好的方案，那就是一次编译到处执行（Compile Once Run Everywhere，简称 CO-RE）。接下来，我们来看看 CO-RE 是如何解决这些问题的。</p><h2>一次编译到处执行（CO-RE）</h2><p>eBPF 的一次编译到处执行（简称 CO-RE）项目借助了 BPF 类型格式（BPF Type Format, 简称 BTF）提供的调试信息，再通过下面的四个步骤，使得 eBPF 程序可以适配不同版本的内核：</p><ul>\n<li>第一，在 bpftool 工具中提供了从 BTF 生成头文件的工具，从而摆脱了对内核头文件的依赖。</li>\n<li>第二，通过对 BPF 代码中的访问偏移量进行重写，解决了不同内核版本中数据结构偏移量不同的问题。</li>\n<li>第三，在 libbpf 中预定义不同内核版本中数据结构的修改，解决了不同内核中数据结构不兼容的问题。</li>\n<li>第四，在 libbpf 中提供一系列的内核特性探测库函数，解决了 eBPF 程序在不同内核内版本中需要执行不同行为的问题。比如，你可以用 <code>bpf_core_type_exists()</code> 和<code>bpf_core_field_exists()</code> 分别检查内核数据类型和成员变量是否存在，也可以用类似 <code>extern int LINUX_KERNEL_VERSION __kconfig</code> 的方式查询内核的配置选项。</li>\n</ul><p>采用这些方法之后，CO-RE 就使得 eBPF 程序可以在开发环境编译完成之后，分发到不同版本内核的机器中运行，并且也不再需要目标机器安装各种开发工具和内核头文件。所以，<strong>Linux 内核社区更推荐所有开发者使用 CO-RE 和 libbpf 来构建 eBPF 程序。</strong>实际上，如果你看过 BCC 的源代码，你会发现 BCC 已经把很多工具都<a href=\"https://github.com/iovisor/bcc/tree/master/libbpf-tools\">迁移</a>到了 CO-RE。</p><p>需要注意的是，CO-RE 需要比较新的内核版本（大于等于5.2）并且需要打开 <code>CONFIG_DEBUG_INFO_BTF</code> 配置选项。所以，实际上采用 CO-RE 技术的 eBPF 程序还是只能运行在满足这两个条件的内核版本中。那么，不支持 BTF 的内核怎么办呢？根据开源社区的实践经验，有两种不同的解决办法。</p><p>第一种，采用条件编译的方式，根据是否支持 CO-RE，生成两个不同的 eBPF 字节码文件。而到程序运行时，再根据内核是否支持 CO-RE 选择对应的字节码文件加载运行。</p><p>第二种，采用 Aqua Security 开源的 <a href=\"https://github.com/aquasecurity/btfhub-archive\">btfhub</a> ，为目标机器匹配的内核版本下载独立的 BTF 信息库，最后再通过如下的方法借助 libbpf 进行加载：</p><pre><code class=\"language-c++\">\tstruct bpf_object_open_opts openopts = {\n\t\t.sz = sizeof(struct bpf_object_open_opts),\n         // 从BPF_CUSTOM_BTF环境变量读取BTF文件路径\n\t\t.btf_custom_path = getenv(\"BPF_CUSTOM_BTF\"),\n\t};\n\n\tobj = hello_btf_bpf__open_opts(&amp;openopts);\n\tif (!obj) {\n\t\tfprintf(stderr, \"failed to open and/or load BPF object\\n\");\n\t\treturn 1;\n\t}\n</code></pre><p>除此之外，eBPF 程序在运行时一般不需要内核的所有 BTF 信息，而只是访问其中的几个少数类型。因而，从内核 5.18 开始，bpftool 还新增了 <code>bpftool gen min_core_btf</code> 命令，帮你精简 BTF 信息，进一步减轻了 eBPF 程序（包括 BTF 信息）的分发交付。</p><p>以我们课程中的 <a href=\"https://github.com/feiskyer/ebpf-apps/blob/main/bpf-apps/execsnoop.bpf.c\">execsnoop</a> 案例为例，精简后的 BTF 只有几百个字节：</p><pre><code class=\"language-bash\">$ ls -lh /sys/kernel/btf/vmlinux\n-r--r--r-- 1 root root 4.8M Jun 20 19:23 /sys/kernel/btf/vmlinux\n\n$ bpftool gen min_core_btf /sys/kernel/btf/vmlinux execsnoop.btf execsnoop.bpf.o\n$ ls -lh execsnoop.btf\n-rw-r--r-- 1 root root 236 Jun 20 20:15 execsnoop.btf\n</code></pre><h2>小结</h2><p>今天，我带你一起梳理了在开发和运行 eBPF 程序过程中可能会碰到的内核版本兼容性问题，并总结了解决这些问题的实践经验。由于不同版本内核的数据结构、函数签名、内核特性以及内核配置等都有可能不同，针对一个版本开发的 eBPF 程序有可能没法直接运行在其他不同版本的内核上。</p><p>为了解决这个问题，BCC 采用了运行时编译的方式，直接从目标机器的内核头文件中获取数据结构，但同时也导致了需要为所有目标机器安装开发工具和内核头文件、编译消耗额外资源、eBPF 程序启动慢以及编译时错误难以排查等额外的问题。</p><p>相比于 BCC，CO-RE 提供了一种更优雅的方式，不仅借助 BTF 解耦了内核头文件的依赖，避免了运行时编译，还通过 libbpf 提供了一系列的辅助函数，方便 eBPF 程序动态探测内核所支持的特性，进而有针对性地处理。对于不支持 BTF 的内核，libbpf 还支持加载自定义的 BTF 文件，所以多内核版本的分发也不再是一个问题。对于开篇中提到的，在新内核中开发的 eBPF 程序有时没法直接在旧版本内核中运行的问题，通过这个方式也就可以解决了。</p><p>今天这一讲就到这里了，下一次的动态更新预计会在8月份。如果你有对我们课程未来内容的建议，欢迎在评论区提出来，期待你与我一起完善和构建一个最贴近实践的 eBPF 知识体系。</p><h2>思考题</h2><p>最后，我想请你聊一聊这两个问题：</p><ol>\n<li>通过前面一段时间课程的学习，你已经把 eBPF 应用到了哪些场景？又有哪些经验和收获？</li>\n<li>在分发 eBPF 程序的过程中，你有没有碰到今天讲到的内核版本兼容性的问题？结合今天的内容，你是怎么解决这些问题的？<br>\n欢迎在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。</li>\n</ol>","neighbors":{"left":{"article_title":"难点解析｜eBPF 开发环境搭建及内核编译详解","id":510472},"right":{"article_title":"加餐｜李程远：谈谈 eBPF 在云原生中的纵与横","id":492456}}},{"article_id":492456,"article_title":"加餐｜李程远：谈谈 eBPF 在云原生中的纵与横","article_content":"<p>你好，我是李程远。很高兴受邀来到这门课做一期分享。如果你之前学过极客时间上的另一个专栏<a href=\"https://time.geekbang.org/column/intro/100063801?tab=catalog\">《容器实战高手课》</a>，应该会对我比较熟悉。</p><p>今天想跟你聊的，是一些我自己关于用 eBPF 进行系统黑盒诊断的思考，特别是在云原生平台上的应用。从 2014 年进入到 Linux 内核以来，eBPF 一直是 Linux 内核中最火的领域。作为 eBPF 的三大应用领域之一，在 Linux 内核的追踪/调试中，特别是在云平台来定位一些复杂问题时，eBPF 已经处于不可替代的地位了。</p><p>在《容器实战高手课》的一篇<a href=\"https://time.geekbang.org/column/article/341820\">加餐</a>里，我也简单介绍过 eBPF 这个技术。当时我给了同学们一个例子，通过它看了如何用 eBPF 来定位我们生产环境中的数据包网络延时偶尔增大的原因。最近，我又碰到一个生产环境中的网络问题，仍然还是依靠 eBPF 程序的帮助，定位到了原因。今天，我就先跟你分享下这个问题的具体情况，以及用 eBPF 定位原因的过程。然后，我会从这个例子出发，聊聊 eBPF 程序可以怎样更好地在云原生平台上应用。</p><h2>一个例子：用eBPF解决生产环境中的网络问题</h2><p>关于遇到的这个问题，先来说一下我看到的现象。</p><p>把线上的问题简化之后，我看到 Client 向一个 Server Pod 里的服务上传数据的时候，偶尔连接会发生中断。通过对 Server Pod 所在的宿主机节点上 tcpdump 数据包的抓取，我们会看到，从 Server Pod 向 Client 发送了一个 TCP RST(reset) 数据包之后，上传数据的连接就中断了。</p><!-- [[[read_end]]] --><p>显然，我们下一步要做的就是<strong>找到这个</strong> <strong>TCP RST</strong> <strong>发送的原因</strong>。不过 TCP RST 发送的原因有很多种，同时，在我们的 Server Pod 里已经注入了 Envoy sidecar 容器，用了 service mesh之后，这样Pod network namespace 里的网络拓扑的复杂度也提高了。这个问题大致的示意图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/90/1b/90430723216f76d2dcd56b7a838a7f1b.jpg?wh=1920x783\" alt=\"图片\"></p><p>在这种情况下，我们想快速找到这个 TCP RST 发送的原因，就需要借助 eBPF 的诊断程序。一个最直接的思路就是找到内核中的发送 TCP RST 数据包的函数，也就是tcp_v4_send_reset() 和 tcp_send_active_reset() 这两个函数，然后用 eBPF 的 kprobe hook来追踪这两个函数的调用。</p><p>通过这些函数的输入参数，我们可以知道 TCP RST 发送前收到的数据包的内容、对应 socket 的状态。我们还可以知道这些被追踪函数的调用关系。</p><p>在运行 eBPF 的诊断程序之后，我们发现了两点：</p><ul>\n<li>第一，发出 TCP RST 数据包所对应的 socket 是处于 TCP_LISTEN 状态的，并且 socket 的监听端口是服务端口 srv-port , 那么这个显然是 Server 程序的 LISTEN socket。</li>\n<li>第二，内核在发出 TCP RST 数据包之前收到的数据包，其目标地址是 <pod-ip>: <srv-port> 。</srv-port></pod-ip></li>\n</ul><p>整个场景的数据包流程图如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/04/d818d719a1f5b92f17eaedb60cb87e04.jpg?wh=1920x825\" alt=\"图片\"></p><p>这里，我们结合数据在 Server Pod 里的流向来看下：当 Envoy sidecar 容器注入到 Server Pod 后，会通过 iptables(DNAT) 方式将数据流先重定向到 Envoy, 然后再由 Envoy 将数据发送给 Server。</p><p>结合上面 eBPF 诊断程序的两点发现，我们就会一下子意识到问题所在：某一个数据包没有经过 iptables 的 DNAT 操作，绕过了 Envoy 直接发送到了 Sever，从而触发了一个 TCP RST 被Client 接收到。</p><p>分析到这里，其实我们基本上已经知道产生 TCP RST 的原因了。那么，iptables 为什么没有对某个数据包做 DNAT 呢？这是因为，在 Linux 内核的 conntrack 机制里，如果收到了乱序的包，在缺省配置的情况下（这里提示下，可以去了解一下内核 ip_conntrack_tcp_be_liberal 这个参数），就是会放过这个包而不去做 NAT 的，这是一个很常见的问题了。</p><h1></h1><h2>如何在云原生平台上更好地使用 eBPF 程序？</h2><p>从上面这个问题的调试诊断过程，我们可以看到 eBPF 在云原生环境中进行黑盒调试的威力。不过，如何让 eBPF 程序在云原生平台上更好地被使用呢？按我目前的理解，要从两个维度来考虑，一个是纵向的，另一个是横向的。</p><h3>纵向的深入</h3><p>那什么是纵向呢？简单来说，就是需要不断地加深对内核的理解。</p><p>为什么这么说呢？如果我们来看 eBPF 程序的编写方式，其实它看起来是非常非常固定的。eBPF 程序的核心概念，也就是 eBPF program、eBPF map 和 helper function这些。如果去看 BCC 下面的工具代码，你也会发现，每个工具的程序代码结构几乎都是差不多的。这一切都说明，eBPF 其实是很“简单”的。</p><p>但是，真的是很简单吗？如王阳明所说，“不行不足谓之知”，如果你动手实践一下，让自己从零开始写一个 eBPF 的程序，可能一上来就会发现事实并非如此。光是弄清楚如何写好它的Makefile，就需要花费一番功夫。至于后面的代码编写，你会发现代码量不大，很多代码还可以依葫芦画瓢，参考网上已有的代码，运气好的话可能会编译执行通过了。但是，一旦编译或者代码运行中出现问题，需要解决的问题其实就只有几行代码，可花费的功夫会远远超过编写整个代码的过程。</p><p>为什么会这样呢？因为在编写 eBPF 代码的过程中，你需要运用到 Linux 内核的基本实现原理。我在这里举几个例子吧。</p><p>比如，如果我们需要追踪一个内核函数，就会发现有好几种 eBPF program 的类型可以选择，每一种 eBPF program 的输入参数还都是不一样的。如果我们需要追踪内核函数 __set_task_comm（) ，想看看哪个进程的进程名字被修改了，可以用 kprobe 的方式来追踪它，也可以用 tracepoint 的方式，还能通过 raw_tracepoint 的方式来追踪。</p><p>在使用 kprobe eBPF program 的时候，它的输入参数是 struct pt_regs；在使用 tracepoint eBPF prgoram 的时候，它的输入参数是一个自定义的结构 struct task_rename；而到使用raw tracepoint 方式的时候，输入参数又变成了 struct bpf_raw_tracepoint_args。不知道你在使用 eBPF program 的时候，有没有想过这个问题：为什么这些参数是不一样的？</p><pre><code class=\"language-plain\">void __set_task_comm(struct task_struct *tsk, const char *buf, bool exec)\n{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task_lock(tsk);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trace_task_rename(tsk, buf);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strlcpy(tsk-&gt;comm, buf, sizeof(tsk-&gt;comm));\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task_unlock(tsk);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;perf_event_comm(tsk, exec);\n}\n</code></pre><pre><code class=\"language-plain\">SEC(\"kprobe/__set_task_comm\")\nint prog(struct pt_regs *ctx)\n{\n  return 0;\n}&nbsp;\n&nbsp;\n/* from /sys/kernel/debug/tracing/events/task/task_rename/format */\nstruct task_rename {\n__u64 pad;\n__u32 pid;\nchar oldcomm[16];\nchar newcomm[16];\n__u16 oom_score_adj;\n};\nSEC(\"tracepoint/task/task_rename\")\nint prog(struct task_rename *ctx)\n{\nreturn 0;\n}\n&nbsp;\n&nbsp;\nSEC(\"raw_tracepoint/task_rename\")\nint prog(struct bpf_raw_tracepoint_args *ctx)\n{\nreturn 0;\n}\n</code></pre><p>这个问题的答案，其实还是要从内核中 kprobe、tracepoint 等内核追踪技术里得到。比如 kprobe ，是通过把当前地址上的指令替换成 int3 中断指令来实现的。而 Linux 内核会在中断发生的时候，把当前各个寄存器的内容写入到栈内存中，而结构 pt_regs 就是用来描述这块栈内存里的内容的。因此，kprobe eBPF program 的输出参数就是 struct pt_regs，根据 Linux ABI，我们就可以知道，被追踪函数的第一个参数的值可以从 pt_regs-&gt;di 里得到了。</p><p>再来看 tracepoint。对于每一个 tracepoint，它都是内核中的一个特别的函数。比如上面例子中的函数 trace_task_rename()， 函数的<a href=\"https://elixir.bootlin.com/linux/v5.15.15/source/include/trace/events/task.h#L34\">实现</a>是通过 tracepoint 的一个固定模板（宏）定义的。通过 bpf tracepoint 的 <a href=\"https://github.com/torvalds/linux/commit/98b5c2c65c2951772a8fc661f50d675e450e8bce\">commit</a>，我们看到，在 tracepoint 被调用的时候，TP_STRUCT__entry 里的变量会被赋值，tracepoint eBPF program 的输入参数就是每个 tracepoint 中 TP_STRUCT__entry 的内容。tracepoint 宏定义如下所示：</p><pre><code class=\"language-plain\">TRACE_EVENT(task_rename,\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP_PROTO(struct task_struct *task, const char *comm),\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP_ARGS(task, comm),\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP_STRUCT__entry(\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__field(&nbsp; &nbsp; &nbsp; &nbsp; pid_t,&nbsp; pid)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__array(&nbsp; &nbsp; &nbsp; &nbsp; char, oldcomm,&nbsp; TASK_COMM_LEN)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__array(&nbsp; &nbsp; &nbsp; &nbsp; char, newcomm,&nbsp; TASK_COMM_LEN)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__field(&nbsp; &nbsp; &nbsp; &nbsp; short,&nbsp; oom_score_adj)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP_fast_assign(\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__entry-&gt;pid = task-&gt;pid;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memcpy(entry-&gt;oldcomm, task-&gt;comm, TASK_COMM_LEN);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strlcpy(entry-&gt;newcomm, comm, TASK_COMM_LEN);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__entry-&gt;oom_score_adj = task-&gt;signal-&gt;oom_score_adj;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP_printk(\"pid=%d oldcomm=%s newcomm=%s oom_score_adj=%hd\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__entry-&gt;pid, __entry-&gt;oldcomm,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;__entry-&gt;newcomm, __entry-&gt;oom_score_adj)\n);\n</code></pre><p>最后看 raw tracepoint。这是 eBPF 引入的 <a href=\"https://github.com/torvalds/linux/commit/c4f6699dfcb8558d138fe838f741b2c10f416cf9\">raw tracepoint program</a> ，其目的就是访问 tracepoint 定义里的 TP_PROTO 参数。比如在我们当前的这个例子里，用了 raw tracepoint 就可以访问到 task_struct *task 参数了，可以得到 task_struct 里的所有信息，而不是 TP_STRUCT__entry 里的那几个变量了。</p><p>上面的几个例子说明，编写 eBPF 程序首先需要理解 Linux 内核的追踪机制的实现。而在使用 eBPF 定位具体内核问题的时候，就更需要理解内核对应部分的代码了。比如，如果我们要诊断网络问题，很自然地，我们需要知道一个网络数据包从网卡驱动进入到内核网络协议栈的主要函数，然后对这些函数进行追踪。下面是内核网络协议栈数据包的接收和发送主要函数图：</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/f9/8b5cd025f2d77yy73f727a50f024bdf9.jpg?wh=1920x939\" alt=\"图片\"></p><p>再举一个例子：在云原生平台上，对内核函数的追踪往往需要我们知道这个函数的调用是在哪个 namespace 里，这样就可以知道是哪个容器触发的操作。大部分的 namespace 都可以通过进程控制块 task_struct{} 里的 nsproxy 结构定义得到，不过 pid namespace 有点特殊了： nsproxy 里的 pid_ns_for_children 并不是代表了进程当前所属的 pid namespace，进程真正的 pid namespace 的值需要从进程控制块的 pid-&gt;numbers[pid-&gt;level].ns 中得到。</p><pre><code class=\"language-plain\">struct nsproxy {\n  atomic_t count;\n  struct uts_namespace *uts_ns;\n  struct ipc_namespace *ipc_ns;\n  struct mnt_namespace *mnt_ns;\n  struct pid_namespace *pid_ns_for_children;\n  struct net &nbsp; &nbsp; *net_ns;\n  struct time_namespace *time_ns;\n  struct time_namespace *time_ns_for_children;\n  struct cgroup_namespace *cgroup_ns;\n};\n</code></pre><p>从上面的这些例子中，我们可以看到：如果以一棵树来比喻的话，各种 eBPF 的工具程序只是树的枝叶，而树的根本还是 Linux 内核各种实现机制。因此，在学习 eBPF 的时候，还是要在根本上多花功夫，这也是我这里说的“纵向”功夫。</p><h3>横向的应用</h3><p>说完了纵向的问题，那么你肯定想问是不是还有横向的问题。那么什么是横向的问题呢？</p><p>纵向的问题是在一个节点上的深入，而横向的问题是在云原生平台上，如何对几千几万台机器使用 eBPF。在生产环境的云原生平台上使用 eBPF，一般有两种场景：</p><ul>\n<li>第一种是针对生产环境中的特定问题做诊断。我们在这一讲开头介绍的 Client 收到异常 TCP RST 的那个问题，就是属于这种场景。</li>\n<li>第二种是通过 eBPF 来采集内核相关的参数，作为云平台常规监控数据的一部分，进行长期的收集。</li>\n</ul><p>对于第一种场景，在几千几万台机器中使用 eBPF 来定位问题，和只在一台机器上运行 eBPF 程序还是有很大不同的，它需要考虑到下面两个问题。</p><p>第一个问题是<strong>权限的问题</strong>。在生产环境的云平台上，有 root 权限登录宿主机的同学很少，用户的 pod/container 绝大多数都是 non-privileged，而运行 eBPF 程序又需要有 privileged 的权限。这样就产生了一个很大的矛盾： 如果用户的 pod 出现网络问题，希望使用 eBPF 程序来做深入的诊断，而用户自己根本就没有权限，这样一旦发生问题，只能依靠有 root 权限的同学登录到宿主机上去执行 eBPF 程序。而有 root 权限的同学寥寥无几，这也会成为瓶颈，这样就把 eBPF 在云平台上的使用门槛不必要地拉高了。</p><p>第二个问题是<strong>多节点操作的问题</strong>。在生产环境的云平台上，要诊断一个问题，往往需要同时在多个节点上执行程序或者收集数据。比如要解决网络延时的问题，就需要在 client pod 宿主机、software LB 宿主机、server pod 宿主机上同时对一个流的数据包进行追踪。类似的多节点诊断操作在生产环境的云平台上常常会发生，如果每次都是手动到各个节点上去执行 eBPF 程序，然后汇总数据，那么工作效率也是很低下的。</p><p>要解决上面的两个问题，我们需要在云平台上建立一个运行 eBPF 诊断程序的框架。这个框架至少要包含三个部分：</p><ul>\n<li>第一部分是在每个宿主机上的 agent。这个 agent 可以用来运行各种 eBPF 诊断程序；</li>\n<li>第二部分是一个 controller。这个 controller 用于接收到用户诊断的命令，然后协调相关宿主机里的 agent 来运行 eBPF 程序并且汇总结果；</li>\n<li>第三部分是一个用户界面，包含用户认证、权限控制，让用户输入诊断指令并且输出结果。Brendan Gregg 在他的一篇文章<a href=\"https://www.brendangregg.com/blog/2021-07-03/how-to-add-bpf-observability.html\"> eBPF observability blog</a> 里也提到了在 Netflix 使用的 eBPF 诊断框架。</li>\n</ul><p>在云平台上有了 eBPF 诊断的框架之后，它不仅可以帮助解决我们前面说的第一种场景中碰到的问题，其实也可以为第二种场景，也就是监控方面提供服务。在每个宿主机上运行的 agent 不但可以接收指令来执行特定的 eBPF 程序，也可以不断输出 metrics 与现有的监控平台相结合。比如，可以使用 <a href=\"https://github.com/cloudflare/ebpf_exporter\">eBPF_exporter</a> 把一些内核相关的 metrics 不断地输出。开源项目 <a href=\"https://github.com/pixie-io/pixie\">pixie</a> 也提供了在云原生平台上的一个比较完整的 observability 框架和工具，底层的 agent 也是基于 eBPF。</p><h2>总结</h2><p>今天的分享就到这里了，我们最后来小结一下。</p><p>在云原生平台上，eBPF 的诊断程序可以帮助我们发现很多深层次 Linux 内核相关的问题，是云原生环境中黑盒调试的利器。要写好 eBPF 的诊断程序，我们需要花更多的功夫在 Linux 内核各种实现机制的研究上。而要把 eBPF 诊断程序更好地运用到云平台上，我们需要设计和实现一个框架。</p><p>感谢你看到这里，如果今天的内容让你有所收获，欢迎把它分享给你的朋友。</p>","neighbors":{"left":{"article_title":"难点解析｜ eBPF 多内核版本兼容详解","id":534577},"right":{"article_title":"用户故事｜eBPF从入门到放弃？在实践中找到突破口","id":513963}}},{"article_id":513963,"article_title":"用户故事｜eBPF从入门到放弃？在实践中找到突破口","article_content":"<p>你好，我是小李同学，坐标深圳，是一名嵌入式开发工程师，已经工作 4 年了，目前主要从事维护系统稳定性的相关工作。今天我主要想跟你分享下我学习 eBPF 的“心路历程”，以及在这门课中的一些收获。</p><h2>多次试图“入门”，始终不得其法</h2><p>我最早接触到 eBPF 是在一篇公众号文章上，上面介绍说它是内核调试的一把利器。当时我就想了：这不是跟我平时的工作联系很密切吗？在调试死机、分析代码路径的时候应该都能用到！eBPF 的强大功能让我很激动，当时就想上手试试，看看能不能把这门技术用起来。</p><p>想象很美好，但是真正开始学习的时候却发现有些棘手。我先是找了一堆资料。关于eBPF 的资料网上倒是有很多，但是不够系统，很多资料讲解也不够细致深入，总觉着看起来不太明白。特别是有一些文章，上来直接把整个 ebpf 的原理图一贴，然后就直接懵了。</p><p>我当时想，既然看原理看不懂，那就先跑起来看看吧！我尝试了下大家说的一些适合入门的 eBPF 工具集，结果发现它们都是基于服务器使用的。而我的工作环境基本都是嵌入式平台，像 BCC 这样的工具集没法直接使用。自己折腾了下，环境没有搭建起来，第一次的尝试“入门”就这么宣告失败了。</p><p>再次看到 eBPF，是见有人推荐《BPF 之巅》和《Linux 内核观测技术 BPF》这两本书。推荐的同学说自己收获很大，我就第一时间下单了，希望能从书中找到进入 eBPF 世界的法门。</p><!-- [[[read_end]]] --><p>这次呢，不自己折腾开发环境了，直接照着书上说的，依葫芦画瓢地在 Ubuntu 虚拟机上运行 BPF 工具。简单执行一些命令就可以得到一些内核参数，刚开始还觉得挺有趣，跟了几个章节之后就觉得索然无味。因为在正常系统上无非是看一看参数，没有深入的理解，再加上我平时的工作中没有使用 eBPF 的场景，也就没有什么正反馈和长期坚持的动力。就这样，想起来了才玩玩看，想不起来或者比较忙的时候就搁置，过了一段时间，这次的尝试也就不了了之了。</p><p>虽然前两次的尝试入门都失败了，但我还是觉得不甘心：既然是一门技术，怎么能搞不会呢？相信自己应该是可以掌握它的，只是还没有找到合适的学习方法和关键的突破口。</p><p>这时，我刚好发现极客时间出了 eBPF 的课程，而且还是倪朋飞老师讲的。我是倪老师第一季专栏<a href=\"https://time.geekbang.org/column/intro/100020901\">《Linux性能优化实战》</a>的老用户，在那门课里收获很大。于是我瞬间来了兴趣，第一时间订阅了这个专栏，希望这次能跟着倪老师走进 eBPF 的世界。</p><h2>反复折腾，找到一个突破口</h2><p>就这样，我开始了第三次“eBPF入门”尝试。跟着倪老师的课程，一步步地从搭建环境到原理剖析，再到实战应用，这比自己折腾要系统得多。因为有了大佬的指导，这次我也少走了很多弯路。</p><p>比如，之前我自己折腾的时候踩过一个坑：一看到不懂的就去网上查，有时候一查就发现了更多不理解的地方，往往越查越乱，最后陷入一个“泥塘”中。而在这门课里，倪老师为我们指出了哪些是可以暂时不用深究的，这就让我得以快速掌握最重要的核心原理，而避免陷入一些细节中。</p><p>再说说我最近的学习体会吧。在反复“折腾”的过程中，有时看上去是失败了，但实际上是自己实实在在地踩过了一次坑，为以后的实践积累了经验。至少呢，也能说明“此路不通”。我在运行 eBPF 程序的时候，有时遇到编译问题，有时又遇到跑不起来的问题。但正是在这个不断踩坑又不断地去修复、实践的尝试过程中，往往在某个点上就找到突破口了。总之，学 eBPF 这种实践性很强的技术，就得“折腾”，要多实践、多思考。</p><p>比如，我第一次看到 BCC、bpftrace、libbpf 等概念的时候，根本搞不清楚它们都是啥东东。经历过之前的自行研究后，我准备先跟着课程把它们用起来。基于 bpftrace、BCC 进行编程，简单地把它们当作一套工具链，将代码转换成对应的格式，然后交给内核处理，我们就可以获取到想要的信息。而对于代码格式和里面的函数，刚开始可以先记下来，先不管是什么原理，记住是怎么用的就行。实际上，在专栏的<a href=\"https://time.geekbang.org/column/article/484207\"> 07</a>、<a href=\"https://time.geekbang.org/column/article/484372\">08</a> 两讲中，倪老师已经详细介绍了如何使用 BCC、bpftrace、libbpf 三种方式开发跟踪函数，掌握了这些实践应用，我们也就搞清楚了它们之间的关系。我想，这就是倪老师在这门课里一直强调，要在实践中加深对 eBPF 原理理解的原因吧！</p><p>从开始学这门课的时候，我就希望能够把 eBPF 用在实际工作中。遗憾的是，我平时的工作环境都是在嵌入式系统中，没有 LLVM、Python 等环境，所以 BCC 和 bpftrace 是没法玩了。只能是使用 libbpf 编程方式，但课程中的 libbpf 例子是跑在宿主机上的，似乎也不能用。</p><p>我找了一堆资料后发现，要想在嵌入式系统中跑起来，需要交叉编译生成 libbpf.a，而 libbpf.a 又依赖于 zlib 和 elfutils 两个库。一通折腾之后，我终于生成了 libbpf.a，最后在嵌入式系统中把 “hello world” eBPF 程序跑起来了。基于 libbpf.a，我们又可以对 Linux 内核 tools/bpf/bpftool 进行编译，生成 bpftool，这样在嵌入式系统中也可以使用。</p><p>总之，在学习 eBPF 的道路上，只要找对方向，并且踏出了第一步，就可以慢慢地走下去。这个过程需要自己不断地去折腾、实践。对于课程中讲到的知识点，最好是要能用起来，所以我之前是用 ftrace 的，现在试着改成使用 eBPF 来获取一些内核信息等等。只要从一个小的突破点开始，不断地获取正反馈，就有更大的动力和兴趣走下去了。</p><p>最后要说的是：感谢倪老师，这次总算是入门了。现在我已经能自己进行一些简单的开发，后面需要的就是不断进行实践了。期待倪老师在动态更新阶段的精彩内容，也感谢极客时间，提供一个良好的学习平台。</p>","neighbors":{"left":{"article_title":"加餐｜李程远：谈谈 eBPF 在云原生中的纵与横","id":492456},"right":[]}}]