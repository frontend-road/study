{"id":84086,"title":"35丨AdaBoost（下）：如何使用AdaBoost对房价进行预测？","content":"<p>今天我带你用AdaBoost算法做一个实战项目。AdaBoost不仅可以用于分类问题，还可以用于回归分析。</p><p>我们先做个简单回忆，什么是分类，什么是回归呢？实际上分类和回归的本质是一样的，都是对未知事物做预测。不同之处在于输出结果的类型，分类输出的是一个离散值，因为物体的分类数有限的，而回归输出的是连续值，也就是在一个区间范围内任何取值都有可能。</p><p>这次我们的主要目标是使用AdaBoost预测房价，这是一个回归问题。除了对项目进行编码实战外，我希望你能掌握：</p><ol>\n<li>\n<p>AdaBoost工具的使用，包括使用AdaBoost进行分类，以及回归分析。</p>\n</li>\n<li>\n<p>使用其他的回归工具，比如决策树回归，对比AdaBoost回归和决策树回归的结果。</p>\n</li>\n</ol><h2>如何使用AdaBoost工具</h2><p>我们可以直接在sklearn中使用AdaBoost。如果我们要用AdaBoost进行分类，需要在使用前引用代码：</p><pre><code>from sklearn.ensemble import AdaBoostClassifier\n</code></pre><p>我们之前讲到过，如果你看到了Classifier这个类，一般都会对应着Regressor类。AdaBoost也不例外，回归工具包的引用代码如下：</p><pre><code>from sklearn.ensemble import AdaBoostRegressor\n</code></pre><p>我们先看下如何在sklearn中创建AdaBoost分类器。</p><!-- [[[read_end]]] --><p>我们需要使用AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)这个函数，其中有几个比较主要的参数，我分别来讲解下：</p><ol>\n<li>\n<p>base_estimator：代表的是弱分类器。在AdaBoost的分类器和回归器中都有这个参数，在AdaBoost中默认使用的是决策树，一般我们不需要修改这个参数，当然你也可以指定具体的分类器。</p>\n</li>\n<li>\n<p>n_estimators：算法的最大迭代次数，也是分类器的个数，每一次迭代都会引入一个新的弱分类器来增加原有的分类器的组合能力。默认是50。</p>\n</li>\n<li>\n<p>learning_rate：代表学习率，取值在0-1之间，默认是1.0。如果学习率较小，就需要比较多的迭代次数才能收敛，也就是说学习率和迭代次数是有相关性的。当你调整learning_rate的时候，往往也需要调整n_estimators这个参数。</p>\n</li>\n<li>\n<p>algorithm：代表我们要采用哪种boosting算法，一共有两种选择：SAMME 和SAMME.R。默认是SAMME.R。这两者之间的区别在于对弱分类权重的计算方式不同。</p>\n</li>\n<li>\n<p>random_state：代表随机数种子的设置，默认是None。随机种子是用来控制随机模式的，当随机种子取了一个值，也就确定了一种随机规则，其他人取这个值可以得到同样的结果。如果不设置随机种子，每次得到的随机数也就不同。</p>\n</li>\n</ol><p>那么如何创建AdaBoost回归呢？</p><p>我们可以使用AdaBoostRegressor(base_estimator=None, n_estimators=50, learning_rate=1.0, loss=‘linear’, random_state=None)这个函数。</p><p>你能看出来回归和分类的参数基本是一致的，不同点在于回归算法里没有algorithm这个参数，但多了一个loss参数。</p><p>loss代表损失函数的设置，一共有3种选择，分别为linear、square和exponential，它们的含义分别是线性、平方和指数。默认是线性。一般采用线性就可以得到不错的效果。</p><p>创建好AdaBoost分类器或回归器之后，我们就可以输入训练集对它进行训练。我们使用fit函数，传入训练集中的样本特征值train_X和结果train_y，模型会自动拟合。使用predict函数进行预测，传入测试集中的样本特征值test_X，然后就可以得到预测结果。</p><h2>如何用AdaBoost对房价进行预测</h2><p>了解了AdaBoost工具包之后，我们看下sklearn中自带的波士顿房价数据集。</p><p>这个数据集一共包括了506条房屋信息数据，每一条数据都包括了13个指标，以及一个房屋价位。</p><p>13个指标的含义，可以参考下面的表格：</p><p><img src=\"https://static001.geekbang.org/resource/image/42/b7/426dec532f34d7f458e36ee59a6617b7.png?wh=468*447\" alt=\"\"><br>\n这些指标分析得还是挺细的，但实际上，我们不用关心具体的含义，要做的就是如何通过这13个指标推导出最终的房价结果。</p><p>如果你学习了之前的算法实战，这个数据集的预测并不复杂。</p><p>首先加载数据，将数据分割成训练集和测试集，然后创建AdaBoost回归模型，传入训练集数据进行拟合，再传入测试集数据进行预测，就可以得到预测结果。最后将预测的结果与实际结果进行对比，得到两者之间的误差。具体代码如下：</p><pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import AdaBoostRegressor\n# 加载数据\ndata=load_boston()\n# 分割数据\ntrain_x, test_x, train_y, test_y = train_test_split(data.data, data.target, test_size=0.25, random_state=33)\n# 使用AdaBoost回归模型\nregressor=AdaBoostRegressor()\nregressor.fit(train_x,train_y)\npred_y = regressor.predict(test_x)\nmse = mean_squared_error(test_y, pred_y)\nprint(&quot;房价预测结果 &quot;, pred_y)\nprint(&quot;均方误差 = &quot;,round(mse,2))\n</code></pre><p>运行结果：</p><pre><code>房价预测结果  [20.2        10.4137931  14.63820225 17.80322581 24.58931298 21.25076923\n 27.52222222 17.8372093  31.79642857 20.86428571 27.87431694 31.09142857\n 12.81666667 24.13131313 12.81666667 24.58931298 17.80322581 17.66333333\n 27.83       24.58931298 17.66333333 20.90823529 20.10555556 20.90823529\n 28.20877193 20.10555556 21.16882129 24.58931298 13.27619048 31.09142857\n 17.08095238 26.19217391  9.975      21.03404255 26.74583333 31.09142857\n 25.83960396 11.859375   13.38235294 24.58931298 14.97931034 14.46699029\n 30.12777778 17.66333333 26.19217391 20.10206186 17.70540541 18.45909091\n 26.19217391 20.10555556 17.66333333 33.31025641 14.97931034 17.70540541\n 24.64421053 20.90823529 25.83960396 17.08095238 24.58931298 21.43571429\n 19.31617647 16.33733333 46.04888889 21.25076923 17.08095238 25.83960396\n 24.64421053 11.81470588 17.80322581 27.63636364 23.59731183 17.94444444\n 17.66333333 27.7253886  20.21465517 46.04888889 14.97931034  9.975\n 17.08095238 24.13131313 21.03404255 13.4        11.859375   26.19214286\n 21.25076923 21.03404255 47.11395349 16.33733333 43.21111111 31.65730337\n 30.12777778 20.10555556 17.8372093  18.40833333 14.97931034 33.31025641\n 24.58931298 22.88813559 18.27179487 17.80322581 14.63820225 21.16882129\n 26.91538462 24.64421053 13.05       14.97931034  9.975      26.19217391\n 12.81666667 26.19214286 49.46511628 13.27619048 17.70540541 25.83960396\n 31.09142857 24.13131313 21.25076923 21.03404255 26.91538462 21.03404255\n 21.16882129 17.8372093  12.81666667 21.03404255 21.03404255 17.08095238\n 45.16666667]\n均方误差 =  18.05\n</code></pre><p>这个数据集是比较规范的，我们并不需要在数据清洗，数据规范化上花太多精力，代码编写起来比较简单。</p><p>同样，我们可以使用不同的回归分析模型分析这个数据集，比如使用决策树回归和KNN回归。</p><p>编写代码如下：</p><pre><code># 使用决策树回归模型\ndec_regressor=DecisionTreeRegressor()\ndec_regressor.fit(train_x,train_y)\npred_y = dec_regressor.predict(test_x)\nmse = mean_squared_error(test_y, pred_y)\nprint(&quot;决策树均方误差 = &quot;,round(mse,2))\n# 使用KNN回归模型\nknn_regressor=KNeighborsRegressor()\nknn_regressor.fit(train_x,train_y)\npred_y = knn_regressor.predict(test_x)\nmse = mean_squared_error(test_y, pred_y)\nprint(&quot;KNN均方误差 = &quot;,round(mse,2))\n</code></pre><p>运行结果：</p><pre><code>决策树均方误差 =  23.84\nKNN均方误差 =  27.87\n</code></pre><p>你能看到相比之下，AdaBoost的均方误差更小，也就是结果更优。虽然AdaBoost使用了弱分类器，但是通过50个甚至更多的弱分类器组合起来而形成的强分类器，在很多情况下结果都优于其他算法。因此AdaBoost也是常用的分类和回归算法之一。</p><h2>AdaBoost与决策树模型的比较</h2><p>在sklearn中AdaBoost默认采用的是决策树模型，我们可以随机生成一些数据，然后对比下AdaBoost中的弱分类器（也就是决策树弱分类器）、决策树分类器和AdaBoost模型在分类准确率上的表现。</p><p>如果想要随机生成数据，我们可以使用sklearn中的make_hastie_10_2函数生成二分类数据。假设我们生成12000个数据，取前2000个作为测试集，其余作为训练集。</p><p>有了数据和训练模型后，我们就可以编写代码。我设置了AdaBoost的迭代次数为200，代表AdaBoost由200个弱分类器组成。针对训练集，我们用三种模型分别进行训练，然后用测试集进行预测，并将三个分类器的错误率进行可视化对比，可以看到这三者之间的区别：</p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.metrics import zero_one_loss\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import  AdaBoostClassifier\n# 设置AdaBoost迭代次数\nn_estimators=200\n# 使用\nX,y=datasets.make_hastie_10_2(n_samples=12000,random_state=1)\n# 从12000个数据中取前2000行作为测试集，其余作为训练集\ntrain_x, train_y = X[2000:],y[2000:]\ntest_x, test_y = X[:2000],y[:2000]\n# 弱分类器\ndt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)\ndt_stump.fit(train_x, train_y)\ndt_stump_err = 1.0-dt_stump.score(test_x, test_y)\n# 决策树分类器\ndt = DecisionTreeClassifier()\ndt.fit(train_x,  train_y)\ndt_err = 1.0-dt.score(test_x, test_y)\n# AdaBoost分类器\nada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)\nada.fit(train_x,  train_y)\n# 三个分类器的错误率可视化\nfig = plt.figure()\n# 设置plt正确显示中文\nplt.rcParams['font.sans-serif'] = ['SimHei']\nax = fig.add_subplot(111)\nax.plot([1,n_estimators],[dt_stump_err]*2, 'k-', label=u'决策树弱分类器 错误率')\nax.plot([1,n_estimators],[dt_err]*2,'k--', label=u'决策树模型 错误率')\nada_err = np.zeros((n_estimators,))\n# 遍历每次迭代的结果 i为迭代次数, pred_y为预测结果\nfor i,pred_y in enumerate(ada.staged_predict(test_x)):\n     # 统计错误率\n    ada_err[i]=zero_one_loss(pred_y, test_y)\n# 绘制每次迭代的AdaBoost错误率 \nax.plot(np.arange(n_estimators)+1, ada_err, label='AdaBoost Test 错误率', color='orange')\nax.set_xlabel('迭代次数')\nax.set_ylabel('错误率')\nleg=ax.legend(loc='upper right',fancybox=True)\nplt.show()\n</code></pre><p>运行结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/35/8ad4bb6a8c6848f2061ff6f442568735.png?wh=865*659\" alt=\"\"><br>\n从图中你能看出来，弱分类器的错误率最高，只比随机分类结果略好，准确率稍微大于50%。决策树模型的错误率明显要低很多。而AdaBoost模型在迭代次数超过25次之后，错误率有了明显下降，经过125次迭代之后错误率的变化形势趋于平缓。</p><p>因此我们能看出，虽然单独的一个决策树弱分类器效果不好，但是多个决策树弱分类器组合起来形成的AdaBoost分类器，分类效果要好于决策树模型。</p><h2>总结</h2><p>今天我带你用AdaBoost回归分析对波士顿房价进行了预测。因为这是个回归分析的问题，我们直接使用sklearn中的AdaBoostRegressor即可。如果是分类，我们使用AdaBoostClassifier。</p><p>另外我们将AdaBoost分类器、弱分类器和决策树分类器做了对比，可以看出经过多个弱分类器组合形成的AdaBoost强分类器，准确率要明显高于决策树算法。所以AdaBoost的优势在于框架本身，它通过一种迭代机制让原本性能不强的分类器组合起来，形成一个强分类器。</p><p>其实在现实工作中，我们也能找到类似的案例。IBM服务器追求的是单个服务器性能的强大，比如打造超级服务器。而Google在创建集群的时候，利用了很多PC级的服务器，将它们组成集群，整体性能远比一个超级服务器的性能强大。</p><p>再比如我们讲的“三个臭皮匠，顶个诸葛亮”，也就是AdaBoost的价值所在。</p><p><img src=\"https://static001.geekbang.org/resource/image/6c/17/6c4fcd75a65dc354bc65590c18e77d17.png?wh=1638*822\" alt=\"\"><br>\n今天我们用AdaBoost分类器与决策树分类做对比的时候，使用到了sklearn中的make_hastie_10_2函数生成数据。实际上在<a href=\"http://time.geekbang.org/column/article/79072\">第19篇</a>，我们对泰坦尼克号的乘客做生存预测的时候，也讲到了决策树工具的使用。你能不能编写代码，使用AdaBoost算法对泰坦尼克号乘客的生存做预测，看看它和决策树模型，谁的准确率更高？</p><p>你也可以把这篇文章分享给你的朋友或者同事，一起切磋一下。</p><p></p>","neighbors":{"left":{"article_title":"34丨AdaBoost（上）：如何使用AdaBoost提升分类器性能？","id":83915},"right":{"article_title":"36丨数据分析算法篇答疑","id":84499}},"comments":[{"had_liked":false,"id":72970,"user_name":"TKbook","can_delete":false,"product_type":"c1","uid":1073829,"ip_address":"","ucode":"F6E0E99CC79059","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg","comment_is_top":false,"comment_ctime":1551767912,"is_pvip":false,"replies":[{"id":"40557","content":"您好，文章已进行更正，谢谢您的反馈。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562555011,"ip_address":"","comment_id":72970,"utype":2}],"discussion_count":1,"race_medal":0,"score":"70271244648","product_id":100021701,"comment_content":"源代码中：<br># 从 12000 个数据中取前 2000 行作为测试集，其余作为训练集<br>test_x, test_y = X[2000:],y[2000:]<br>train_x, train_y = X[:2000],y[:2000]<br><br>这个部分的代码写错了吧<br>应该是：<br>test_x, test_y = x[: 2000], y[: 2000]<br>train_x, train_y = x[2000:], y[2000:]","like_count":17,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441855,"discussion_content":"您好，文章已进行更正，谢谢您的反馈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562555011,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72669,"user_name":"third","can_delete":false,"product_type":"c1","uid":1025114,"ip_address":"","ucode":"9A37408A834F0B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg","comment_is_top":false,"comment_ctime":1551689335,"is_pvip":false,"replies":[{"id":"40558","content":"结果正确，一般来说AdaBoost的结果会比决策树分类器略好一些。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562555041,"ip_address":"","comment_id":72669,"utype":2}],"discussion_count":1,"race_medal":0,"score":"27321493111","product_id":100021701,"comment_content":"结果仍然为AdaBoost算法最优。<br>个人发现，前两个分类器出结果很快<br>分析最优：<br>1.AdaBoost算法经过了更多运算，特别是在迭代弱分类器和组合上<br>2.良好组合起来的个体，能够创造更大的价值。<br><br>决策树弱分类器准确率为 0.7867<br>决策树分类器准确率为 0.7891<br>AdaBoost 分类器准确率为 0.8138<br><br>import numpy as np<br>import pandas as pd<br>from sklearn.model_selection import cross_val_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.ensemble import AdaBoostClassifier<br>from sklearn.feature_extraction import DictVectorizer<br><br># 1.数据加载<br>train_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;train.csv&#39;)<br>test_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;test.csv&#39;)<br><br># 2.数据清洗<br># 使用平均年龄来填充年龄中的 NaN 值<br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)<br># 均价填充<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(),inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)<br># 使用登陆最多的港口来填充<br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br><br># 特征选择<br>features=[&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Embarked&#39;]<br>train_features=train_data[features]<br>train_labels=train_data[&#39;Survived&#39;]<br>test_features=test_data[features]<br><br># 将符号化的Embarked对象抽象处理成0&#47;1进行表示<br>dvec=DictVectorizer(sparse=False)<br>train_features=dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))<br><br># 决策树弱分类器<br>dt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)<br>dt_stump.fit(train_features, train_labels)<br><br>print(u&#39;决策树弱分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(dt_stump, train_features, train_labels, cv=10)))<br><br># 决策树分类器<br>dt = DecisionTreeClassifier()<br>dt.fit(train_features, train_labels)<br><br>print(u&#39;决策树分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(dt, train_features, train_labels, cv=10)))<br><br># AdaBoost 分类器<br>ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=200)<br>ada.fit(train_features, train_labels)<br><br>print(u&#39;AdaBoost 分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(ada, train_features, train_labels, cv=10)))","like_count":7,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441680,"discussion_content":"结果正确，一般来说AdaBoost的结果会比决策树分类器略好一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562555041,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72605,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1551676894,"is_pvip":false,"replies":[{"id":"64468","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619623,"ip_address":"","comment_id":72605,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27321480670","product_id":100021701,"comment_content":"由于乘客测试集缺失真实值，采用 K 折交叉验证准确率<br>--------------------<br>运行结果：<br>决策树弱分类器准确率为 0.7867<br>决策树分类器准确率为 0.7813<br>AdaBoost 分类器准确率为 0.8138<br>-------------------------<br>代码：<br>import numpy as np<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.ensemble import  AdaBoostClassifier<br>import pandas as pd<br>from sklearn.feature_extraction import DictVectorizer<br>from sklearn.model_selection import cross_val_score<br><br># 设置 AdaBoost 迭代次数<br>n_estimators=200<br><br># 数据加载<br>train_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;train.csv&#39;)<br>test_data=pd.read_csv(&#39;.&#47;Titanic_Data&#47;test.csv&#39;)<br><br># 模块 2：数据清洗<br># 使用平均年龄来填充年龄中的 NaN 值<br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)<br># 使用票价的均值填充票价中的 nan 值<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(),inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)<br># 使用登录最多的港口来填充登录港口Embarked的 nan 值<br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br><br># 特征选择<br>features=[&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Fare&#39;,&#39;Embarked&#39;]<br>train_features=train_data[features]<br>train_labels=train_data[&#39;Survived&#39;]<br>test_features=test_data[features]<br><br># 将符号化的Embarked对象处理成0&#47;1进行表示<br>dvec=DictVectorizer(sparse=False)<br>train_features=dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))<br><br># 决策树弱分类器<br>dt_stump = DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)<br>dt_stump.fit(train_features, train_labels)<br><br>print(u&#39;决策树弱分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(dt_stump, train_features, train_labels, cv=10)))<br><br># 决策树分类器<br>dt = DecisionTreeClassifier()<br>dt.fit(train_features, train_labels)<br><br>print(u&#39;决策树分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(dt, train_features, train_labels, cv=10)))<br><br># AdaBoost 分类器<br>ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)<br>ada.fit(train_features, train_labels)<br><br>print(u&#39;AdaBoost 分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(ada, train_features, train_labels, cv=10)))","like_count":6,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441654,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619623,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72558,"user_name":"梁林松","can_delete":false,"product_type":"c1","uid":1144766,"ip_address":"","ucode":"FA032C3B4E245E","user_header":"https://static001.geekbang.org/account/avatar/00/11/77/be/1f2409e8.jpg","comment_is_top":false,"comment_ctime":1551667731,"is_pvip":false,"replies":[{"id":"40559","content":"对的 需要引入相应的回归类库。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562555070,"ip_address":"","comment_id":72558,"utype":2}],"discussion_count":1,"race_medal":0,"score":"14436569619","product_id":100021701,"comment_content":"跑第二块代码是需要引入两个模块<br>from sklearn.tree import DecisionTreeRegressor<br>from sklearn.neighbors import KNeighborsRegressor<br>","like_count":3,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441640,"discussion_content":"对的 需要引入相应的回归类库。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562555070,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88112,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555851774,"is_pvip":false,"replies":[{"id":"40560","content":"准确率一般不会这么低，所以你可以查下代码中是否有错误。<br>这里需要注意的是，应该是用DecisionTreeClassifier和AdaBoostClassifier，因为泰坦尼克生存预测是个分类问题（离散值），不是回归问题（连续值）。<br>另外在我们在做K折交叉验证的时候，应该使用：cross_val_score<br>cross_val_score 用来返回评测的准确率<br>cross_val_predict 用来返回预测的分类结果<br>这两处地方你调整下，再跑跑代码","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1562555182,"ip_address":"","comment_id":88112,"utype":2}],"discussion_count":1,"race_medal":0,"score":"5850819070","product_id":100021701,"comment_content":"得到结果：<br>CART决策树K折交叉验证准确率: 0.39480897860892333<br>AdaBoostK折交叉验证准确率: 0.4376641797318339<br><br>from sklearn.tree import DecisionTreeRegressor<br>from sklearn.ensemble import AdaBoostRegressor<br>from sklearn.feature_extraction import DictVectorizer<br>from sklearn.model_selection import cross_val_predict<br>import pandas as pd<br>import numpy as np<br><br>#读取数据<br>path = &#39;&#47;Users&#47;apple&#47;Desktop&#47;GitHubProject&#47;Read mark&#47;数据分析&#47;geekTime&#47;data&#47;&#39;<br>train_data = pd.read_csv(path + &#39;Titannic_Data_train.csv&#39;)<br>test_data = pd.read_csv(path + &#39;Titannic_Data_test.csv&#39;)<br><br>#数据清洗<br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)<br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br><br>#特征选择<br>features = [&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Embarked&#39;]<br>train_features = train_data[features]<br>train_result = train_data[&#39;Survived&#39;]<br>test_features = test_data[features]<br>devc = DictVectorizer(sparse=False)<br>train_features = devc.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>test_features = devc.fit_transform(test_features.to_dict(orient=&#39;record&#39;))<br><br>#构造决策树，进行预测<br>tree_regressor = DecisionTreeRegressor()<br>tree_regressor.fit(train_features,train_result)<br>predict_tree = tree_regressor.predict(test_features)<br>#交叉验证准确率<br>print(&#39;CART决策树K折交叉验证准确率:&#39;, np.mean(cross_val_predict(tree_regressor,train_features,train_result,cv=10)))<br><br>#构造AdaBoost<br>ada_regressor = AdaBoostRegressor()<br>ada_regressor.fit(train_features,train_result)<br>predict_ada = ada_regressor.predict(test_features)<br>#交叉验证准确率<br>print(&#39;AdaBoostK折交叉验证准确率:&#39;,np.mean(cross_val_predict(ada_regressor,train_features,train_result,cv=10)))<br>","like_count":1,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447734,"discussion_content":"准确率一般不会这么低，所以你可以查下代码中是否有错误。\n这里需要注意的是，应该是用DecisionTreeClassifier和AdaBoostClassifier，因为泰坦尼克生存预测是个分类问题（离散值），不是回归问题（连续值）。\n另外在我们在做K折交叉验证的时候，应该使用：cross_val_score\ncross_val_score 用来返回评测的准确率\ncross_val_predict 用来返回预测的分类结果\n这两处地方你调整下，再跑跑代码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562555182,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294034,"user_name":"KokutoDa","can_delete":false,"product_type":"c1","uid":1189534,"ip_address":"","ucode":"2561207E84840E","user_header":"https://static001.geekbang.org/account/avatar/00/12/26/9e/836f603b.jpg","comment_is_top":false,"comment_ctime":1621691967,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1621691967","product_id":100021701,"comment_content":"准确率：<br>adaboost 交叉验证：0.81147315855181<br>决策树交叉验证：0.7812484394506866<br>adaboost（accuracy_score）：0.8484848484848485<br>决策树（accuracy_score）0.9820426487093153<br>老师，为什么交叉验证的准确率和accuracy_score的准确率计算结果相反？<br><br>import pandas as pd<br>from sklearn.ensemble import AdaBoostClassifier<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.feature_extraction import DictVectorizer<br>from sklearn.model_selection import cross_val_score<br>from sklearn.metrics import accuracy_score<br>import numpy as np<br><br># adaboost 预测泰坦尼克号生存<br># 数据加载<br>train_data = pd.read_csv(&#39;..&#47;Titanic_Data&#47;train.csv&#39;)<br>test_data = pd.read_csv(&#39;..&#47;Titanic_Data&#47;test.csv&#39;)<br><br># 数据清洗<br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)<br># 使用登录最多的港口来填充登录港口的 nan 值<br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br><br># 特征选择<br>features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]<br>X_train = train_data[features]<br>y_train = train_data[&#39;Survived&#39;]<br>X_test = test_data[features]<br># 替换成计算机能理解的<br>dvec=DictVectorizer(sparse=False)<br>X_train = dvec.fit_transform(X_train.to_dict(orient=&#39;record&#39;))<br><br># Adaboost<br>ada = AdaBoostClassifier(n_estimators=200)<br>ada.fit(X_train, y_train)<br>ada_pred = ada.predict(X_train)<br><br># 决策树<br>clf = DecisionTreeClassifier(criterion=&#39;entropy&#39;)<br>clf.fit(X_train, y_train)<br>clf_pred = clf.predict(X_train)<br><br>print(np.mean(cross_val_score(ada, X_train, y_train, cv=10)))<br>print(np.mean(cross_val_score(clf, X_train, y_train, cv=10)))<br>print(accuracy_score(y_train, ada_pred))<br>print(accuracy_score(y_train, clf_pred))","like_count":0},{"had_liked":false,"id":285288,"user_name":"Liam","can_delete":false,"product_type":"c1","uid":1629141,"ip_address":"","ucode":"8AC3F3AB9BB6BF","user_header":"","comment_is_top":false,"comment_ctime":1616724404,"is_pvip":false,"replies":[{"id":"103702","content":"print([0.8] * 2)<br>你会看到打印结果为：[0.8, 0.8]<br>列表 * n 代表列表被复制扩展n倍长。<br>乘号*常被用于快速初始化list，但有一个隐患：被乘号复制的对象都指向同一个空间，所以如果你的列表中的元素要用来存储不同值时，建议用for循环。<br>老师这里只是为了可视化，所以才使用这种方式。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617028810,"ip_address":"","comment_id":285288,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616724404","product_id":100021701,"comment_content":"ax = fig.add_subplot(111)ax.plot([1,n_estimators],[dt_stump_err]*2, &#39;k-&#39;, label=u&#39;决策树弱分类器 错误率&#39;)ax.plot([1,n_estimators],[dt_err]*2,&#39;k--&#39;, label=u&#39;决策树模型 错误率&#39;)ada_err = np.zeros((n_estimators,)).  疑问：这里*2是什么意思，能解析下代码吗？","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517636,"discussion_content":"print([0.8] * 2)\n你会看到打印结果为：[0.8, 0.8]\n列表 * n 代表列表被复制扩展n倍长。\n乘号*常被用于快速初始化list，但有一个隐患：被乘号复制的对象都指向同一个空间，所以如果你的列表中的元素要用来存储不同值时，建议用for循环。\n老师这里只是为了可视化，所以才使用这种方式。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617028810,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282626,"user_name":"小晨","can_delete":false,"product_type":"c1","uid":1935063,"ip_address":"","ucode":"B7ADDC309C0D19","user_header":"https://static001.geekbang.org/account/avatar/00/1d/86/d7/46842f90.jpg","comment_is_top":false,"comment_ctime":1615343691,"is_pvip":false,"replies":[{"id":"102996","content":"结果正确，一般来说AdaBoost的结果会比决策树分类器略好一些","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1615990249,"ip_address":"","comment_id":282626,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1615343691","product_id":100021701,"comment_content":"弱分类器准确率为 0.7868<br>决策树分类器准确率为 0.7823<br>AdaBoost分类器准确率为:0.8115<br><br>#!&#47;usr&#47;bin&#47;env python<br># -*- coding:utf-8 -*-<br># Author:Peter<br><br>import numpy as np<br>import pandas as pd<br>from sklearn.ensemble import AdaBoostClassifier<br>from sklearn.feature_extraction import DictVectorizer<br>from sklearn.model_selection import cross_val_score<br>from sklearn.tree import DecisionTreeClassifier<br><br># 迭代次数<br>n_estimators = 200<br>train_data = pd.read_csv(r&#39;data&#47;Titanic_Data_train.csv&#39;)<br>test_data = pd.read_csv(r&#39;data&#47;Titanic_Data_Test.csv&#39;)<br><br># 用平均年龄将缺失的年龄补齐<br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)<br><br># 用平均票价将缺失的票价补齐<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)<br><br># 用登船港口最多的S补齐缺失<br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br><br># 将可用来分类的数据放到训练集中<br>features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]<br>train_features = train_data[features]<br>train_labels = train_data[&#39;Survived&#39;]<br>test_features = test_data[features]<br><br># 字符串数据规范化，转为int型<br>dvec = DictVectorizer(sparse=False)<br>train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>test_features = dvec.transform(test_features.to_dict(orient=&#39;record&#39;))<br><br># 弱分类器<br>dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)<br>dt_stump.fit(train_features, train_labels)<br>print(u&#39;弱分类器准确率为 %.4lf&#39; % dt_stump.score(train_features, train_labels))<br># 决策树分类器<br>dt = DecisionTreeClassifier()<br>dt.fit(train_features, train_labels)<br>print(u&#39;决策树分类器准确率为 %.4lf&#39; % np.mean(cross_val_score(dt, train_features, train_labels, cv=10)))<br><br># AdaBoost分类器<br>ada = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=n_estimators)<br>ada.fit(train_features, train_labels)<br>ada_score = np.mean(cross_val_score(ada, train_features, train_labels, cv=10))<br>print(&quot;AdaBoost分类器准确率为:%.4lf&quot; % ada_score)<br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516794,"discussion_content":"结果正确，一般来说AdaBoost的结果会比决策树分类器略好一些","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615990249,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":264252,"user_name":"非同凡想","can_delete":false,"product_type":"c1","uid":1934969,"ip_address":"","ucode":"713FD449A49D5A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg","comment_is_top":false,"comment_ctime":1606391026,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1606391026","product_id":100021701,"comment_content":"交作业：<br>import numpy as np<br>import pandas as pd<br>from sklearn import tree<br>from sklearn import feature_extraction<br>from sklearn.model_selection import cross_val_score<br>from sklearn.ensemble import  AdaBoostClassifier<br><br># load dataset<br>train_data = pd.DataFrame(pd.read_csv(&#39;~&#47;Documents&#47;titanic_data&#47;train.csv&#39;))<br>test_data = pd.DataFrame(pd.read_csv(&#39;~&#47;Documents&#47;titanic_data&#47;test.csv&#39;))<br><br># data cleaning<br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)<br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)<br># select features<br>features = [&#39;Pclass&#39;, &#39;Sex&#39;, &quot;Age&quot;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]<br>train_x = train_data[features]<br>train_y = train_data[&#39;Survived&#39;]<br>test_x = test_data[features]<br># one-hot<br>dict_vec = feature_extraction.DictVectorizer(sparse=False)<br>train_x = dict_vec.fit_transform(train_x.to_dict(orient=&#39;record&#39;))<br>test_x = dict_vec.transform(test_x.to_dict(orient=&#39;record&#39;))<br>print(dict_vec.feature_names_)<br><br># decision tree<br>dtc = tree.DecisionTreeClassifier()<br>dtc.fit(train_x, train_y)<br><br>print(&quot;决策树准确率&quot;, dtc.score(train_x, train_y))<br>print(&quot;决策树：k折交叉验证准确率：&quot;, np.mean(cross_val_score(dtc, train_x, train_y, cv= 10)))<br><br># adaboost<br>ada = AdaBoostClassifier(n_estimators=50)<br>ada.fit(train_x, train_y)<br>print(&quot;AdaBoost准确率&quot;, ada.score(train_x, train_y))<br>print(&quot;AdaBoost k折交叉验证准确率：&quot;, np.mean(cross_val_score(ada, train_x, train_y, cv= 10)))<br><br>决策树准确率 0.9820426487093153<br>决策树：k折交叉验证准确率： 0.7744943820224719<br>AdaBoost准确率 0.8338945005611672<br>AdaBoost k折交叉验证准确率： 0.8070037453183521","like_count":0},{"had_liked":false,"id":232310,"user_name":"萌辰","can_delete":false,"product_type":"c1","uid":2022518,"ip_address":"","ucode":"8CFC75B7290E50","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/J9YKHoKKd1LlzuCuKFGhnKBD1GtS9qiclsibY6vviacpCOB7uR4iaibIpdQKTzwiaVwJiaiaicB99rNx23JPQYV5wjThOWQ/132","comment_is_top":false,"comment_ctime":1593953200,"is_pvip":false,"replies":[{"id":"103704","content":"DecisionTreeRegressor的参数random_state随机数种子，用来控制估算器的随机性。 即使分割器设置为“best”，每个分割中的特征也始终是随机排列的。<br> 当max_features &lt;n_features时，算法将在每个分割处随机选择max_features，然后再在其中找到最佳分割。 <br>但是，即使max_features = n_features，找到的最佳分割也可能因不同的运行而有所不同。 就是这种情况，如果对于几个分割而言标准的改进是相同的，并且必须随机选择一个分割。 为了在拟合过程中获得确定性的行为，random_state必须固定为整数。 <br>使用相同random_state，则每次使用相同的分割策略。所以不同随机数种子参数，得到的结果不同。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617030491,"ip_address":"","comment_id":232310,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1593953200","product_id":100021701,"comment_content":"在AdaBoost、决策树回归、KNN房价预测对比中发现，随机种子对决策树的预测结果有影响。<br>分别测试了三种不同的随机种子：<br>dec_regressor=DecisionTreeRegressor(random_state=1)<br>dec_regressor=DecisionTreeRegressor(random_state=20)<br>dec_regressor=DecisionTreeRegressor(random_state=30)<br>测试结果为：<br>决策树均方误差1 =  36.65<br>决策树均方误差20 =  25.54<br>决策树均方误差30 =  37.19<br>思考：<br>此处考虑这里没有限制种子的随机性，对比的结果可能过于随机了，无法真实反映算法效果，两种算法原理中随机种子的应用情况不同。思考是不是采用多次随机MSE结果求平均的方法作为【比较项】更为合适<br>KNN算法无随机种子影响。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500599,"discussion_content":"DecisionTreeRegressor的参数random_state随机数种子，用来控制估算器的随机性。 即使分割器设置为“best”，每个分割中的特征也始终是随机排列的。\n 当max_features &amp;lt;n_features时，算法将在每个分割处随机选择max_features，然后再在其中找到最佳分割。 \n但是，即使max_features = n_features，找到的最佳分割也可能因不同的运行而有所不同。 就是这种情况，如果对于几个分割而言标准的改进是相同的，并且必须随机选择一个分割。 为了在拟合过程中获得确定性的行为，random_state必须固定为整数。 \n使用相同random_state，则每次使用相同的分割策略。所以不同随机数种子参数，得到的结果不同。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617030491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":230849,"user_name":"even","can_delete":false,"product_type":"c1","uid":1526343,"ip_address":"","ucode":"2EDE76B1F80F33","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIfCY2mvbZ2Po4efYBhMJPacb9mlOicNI6Us4ph3ianrkGlUcop8ZlzN6QiaDrnvFcNeaAfwP7XAv5fw/132","comment_is_top":false,"comment_ctime":1593509934,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1593509934","product_id":100021701,"comment_content":"不同的算法有不同的特点，老师是否可以做个总结和对比。比如在实际的工作或者项目中，根据经验和不同的算法特点，如何选择算法，为什么选择这种算法。希望老师能分享这一块实际应用场景的经验。","like_count":0},{"had_liked":false,"id":225301,"user_name":"§mc²ompleXWr","can_delete":false,"product_type":"c1","uid":1932586,"ip_address":"","ucode":"8D2527DE0F760B","user_header":"https://static001.geekbang.org/account/avatar/00/1d/7d/2a/4c7e2e2f.jpg","comment_is_top":false,"comment_ctime":1591710313,"is_pvip":false,"replies":[{"id":"103734","content":"是否需要进行数据规范化，取决于所使用的模型和特征的数据范围。比如：树模型和朴素贝叶斯模型不需要进行规范化；如果数据集特征的数据已经都在0-1之间，或者已经符合标准化，则无需规范化。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617039073,"ip_address":"","comment_id":225301,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1591710313","product_id":100021701,"comment_content":"使用自带的数据集就不用做数据规范化么？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":497802,"discussion_content":"是否需要进行数据规范化，取决于所使用的模型和特征的数据范围。比如：树模型和朴素贝叶斯模型不需要进行规范化；如果数据集特征的数据已经都在0-1之间，或者已经符合标准化，则无需规范化。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617039073,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":220996,"user_name":"鲨鱼鲸鱼鳄鱼","can_delete":false,"product_type":"c1","uid":1462680,"ip_address":"","ucode":"9976838627352F","user_header":"https://static001.geekbang.org/account/avatar/00/16/51/98/3b8de985.jpg","comment_is_top":false,"comment_ctime":1590374498,"is_pvip":false,"replies":[{"id":"103949","content":"AdaBoost模型默认使用的弱分类器是决策树模型，树模型只看点之间的相对位置，不计算二者之间的距离，因此不需要进行数据规范化（包括标准化或归一化等）","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617286426,"ip_address":"","comment_id":220996,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1590374498","product_id":100021701,"comment_content":"老师，请问AdaBoost模型在预测前需不需要对数据进行标准化或者归一化，做有什么好处，不做有什么好处呢","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496292,"discussion_content":"AdaBoost模型默认使用的弱分类器是决策树模型，树模型只看点之间的相对位置，不计算二者之间的距离，因此不需要进行数据规范化（包括标准化或归一化等）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617286426,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":196656,"user_name":"张贺","can_delete":false,"product_type":"c1","uid":1283181,"ip_address":"","ucode":"0254E40FB3EB5F","user_header":"https://static001.geekbang.org/account/avatar/00/13/94/6d/5cd6e8c7.jpg","comment_is_top":false,"comment_ctime":1585300430,"is_pvip":true,"replies":[{"id":"75463","content":"谢谢张贺同学","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1585875039,"ip_address":"","comment_id":196656,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1585300430","product_id":100021701,"comment_content":"老师讲的很清晰","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":489351,"discussion_content":"谢谢张贺同学","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585875039,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":186463,"user_name":"热水泡面不会做","can_delete":false,"product_type":"c1","uid":1903652,"ip_address":"","ucode":"299C305C8ACDA0","user_header":"https://static001.geekbang.org/account/avatar/00/1d/0c/24/8ce78297.jpg","comment_is_top":false,"comment_ctime":1583848099,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583848099","product_id":100021701,"comment_content":"可不可以解释一下这里的学习率体现在哪里呢？之前的原理讲解里好像没有用到学习率？","like_count":0},{"had_liked":false,"id":185694,"user_name":"Untitled","can_delete":false,"product_type":"c1","uid":1039464,"ip_address":"","ucode":"8DD6ABA3E81A2E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dc/68/006ba72c.jpg","comment_is_top":false,"comment_ctime":1583665477,"is_pvip":false,"discussion_count":0,"race_medal":1,"score":"1583665477","product_id":100021701,"comment_content":"结果：<br>ada train precision =  0.8338945005611672<br>ada 10k precison =  0.8070037453183521<br>clf train precision =  0.9820426487093153<br>clf 10k precision =  0.7767041198501872<br>#代码<br>import pandas as pd<br>import numpy as np<br>from sklearn.feature_extraction import DictVectorizer<br>from sklearn.ensemble import AdaBoostClassifier<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.model_selection import cross_val_score<br>train_data = pd.read_csv(&#39;train.csv&#39;)<br>test_data = pd.read_csv(&#39;test.csv&#39;)<br><br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br>test_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br>test_data[&#39;Fare&#39;].fillna(train_data[&#39;Age&#39;].mean(),inplace=True)<br><br>train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br>test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)<br><br>features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]<br>train_features = train_data[features]<br>train_labels = train_data[&#39;Survived&#39;]<br>test_features = test_data[features]<br><br>dvec=DictVectorizer(sparse=False)<br>train_features=dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))<br><br>ada = AdaBoostClassifier()<br>ada.fit(train_features, train_labels)<br>print(&quot;ada train precision = &quot;,ada.score(train_features, train_labels))<br>print(&quot;ada 10k precison = &quot;, np.mean(cross_val_score(ada,train_features,train_labels,cv=10)))<br>clf=DecisionTreeClassifier(criterion=&#39;entropy&#39;)<br>clf.fit(train_features,train_labels)<br>print(&quot;clf train precision = &quot;, clf.score(train_features, train_labels))<br>print(&quot;clf 10k precision = &quot;, np.mean(cross_val_score(clf,train_features,train_labels,cv=10)))<br>","like_count":0},{"had_liked":false,"id":124005,"user_name":"骑行的掌柜J","can_delete":false,"product_type":"c1","uid":1474214,"ip_address":"","ucode":"3163102651C653","user_header":"https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg","comment_is_top":false,"comment_ctime":1565790095,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565790095","product_id":100021701,"comment_content":"打错了 陈老师是对的 是回归算法😂里没有分类算法的algorithm 参数。","like_count":0},{"had_liked":false,"id":88097,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1555847459,"is_pvip":false,"replies":[{"id":"64233","content":"对的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577615003,"ip_address":"","comment_id":88097,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555847459","product_id":100021701,"comment_content":"分类和回归都是做预测，分类是离散值，回归是连续值","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447726,"discussion_content":"对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615003,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":80662,"user_name":"hlz-123","can_delete":false,"product_type":"c1","uid":1433586,"ip_address":"","ucode":"B7E5EF0C260BD2","user_header":"","comment_is_top":false,"comment_ctime":1553695910,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1553695910","product_id":100021701,"comment_content":"老师，在AdaBoost 与决策树模型的比较的例子中，弱分类器<br>dt_stump = DecisionTreeClassfier(max_depth=1,min_samples_leaf=1)<br>为什么两个参数都设置为1，相当于只有1个根节点，2个叶节点？<br>而普通的决策树分类器，没有设置参数，这是什么原因？","like_count":0},{"had_liked":false,"id":77709,"user_name":"叮当猫","can_delete":false,"product_type":"c1","uid":1360159,"ip_address":"","ucode":"175BB66517E21B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg","comment_is_top":false,"comment_ctime":1552985780,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1552985780","product_id":100021701,"comment_content":"fit_transform数据统一处理，求问什么时候需要？<br>在我同时没有进行fit_transform的情况下，准确率：<br>决策树弱分类器的准确率是0.7867<br>决策树分类器的准确率是0.7734<br>AdaBoost分类器的准确率是0.8161<br>在我对数据同时进行fit_transform的情况下，准确率：<br>决策树弱分类器的准确率是0.7867<br>决策树分类器的准确率是0.7745<br>AdaBoost分类器的准确率是0.8138<br><br>以下是第一种情况：<br>train_data[&#39;Embarked&#39;] = train_data[&#39;Embarked&#39;].map({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:2})<br>test_data[&#39;Embarked&#39;] = test_data[&#39;Embarked&#39;].map({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:2})<br>train_data[&#39;Sex&#39;] = train_data[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1})<br>test_data[&#39;Sex&#39;] = test_data[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1})<br><br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)<br><br>features = [&#39;Pclass&#39;, &#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]<br>train_features = train_data[features]<br>train_labels = train_data[&#39;Survived&#39;]<br>test_features = test_data[features]<br><br>#train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>#test_features = dvec.transform(test_features.to_dict(orient=&#39;record&#39;))<br><br>以下是第二种情况：<br>#train_data[&#39;Embarked&#39;] = train_data[&#39;Embarked&#39;].map({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:2})<br>#test_data[&#39;Embarked&#39;] = test_data[&#39;Embarked&#39;].map({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:2})<br>#train_data[&#39;Sex&#39;] = train_data[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1})<br>#test_data[&#39;Sex&#39;] = test_data[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1})<br><br>train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)<br>test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)<br>train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)<br>test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)<br><br>features = [&#39;Pclass&#39;, &#39;Sex&#39;,&#39;Age&#39;,&#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]<br>train_features = train_data[features]<br>train_labels = train_data[&#39;Survived&#39;]<br>test_features = test_data[features]<br><br>train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))<br>test_features = dvec.transform(test_features.to_dict(orient=&#39;record&#39;))","like_count":0},{"had_liked":false,"id":72985,"user_name":"JingZ","can_delete":false,"product_type":"c1","uid":1023464,"ip_address":"","ucode":"6F97895B2CC375","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132","comment_is_top":false,"comment_ctime":1551770725,"is_pvip":false,"replies":[{"id":"64463","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619591,"ip_address":"","comment_id":72985,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551770725","product_id":100021701,"comment_content":"# AdaBoost<br>一开始竟然蓦然惯性用了AdaBoostRegressor，得到0.33的准确率，最后看了小伙伴代码，立马修正<br><br>感觉算法代码不复杂，关键要自己从空白开始写，还需多实战<br><br>from sklearn.ensemble import AdaBoostClassifier<br><br># 使用 Adaboost 分类模型<br>ada = AdaBoostClassifier()<br>ada.fit(train_features, train_labels)<br><br>pred_labels = ada.predict(test_features)<br><br>acc_ada_classifier = round(ada.score(train_features, train_labels), 6)<br>print(u&#39;Adaboost score 准确率为 %.4lf&#39; % acc_ada_classifier)<br>print(u&#39;Adaboost cross_val_score 准确率为 %.4lf&#39; % np.mean(cross_val_score(ada, train_features, train_labels, cv=10)))<br><br>运行<br>Adaboost score 准确率为 0.8339<br>Adaboost cross_val_score 准确率为 0.8104","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441862,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72863,"user_name":"FORWARD―MOUNT","can_delete":false,"product_type":"c1","uid":1357857,"ip_address":"","ucode":"CD8E9ECF882980","user_header":"https://static001.geekbang.org/account/avatar/00/14/b8/21/c03839f1.jpg","comment_is_top":false,"comment_ctime":1551748442,"is_pvip":false,"replies":[{"id":"64466","content":"AdaBoost自己计算的","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619613,"ip_address":"","comment_id":72863,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551748442","product_id":100021701,"comment_content":"老师，房价预测这个算法，50个弱分类器是怎么来的？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441796,"discussion_content":"AdaBoost自己计算的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619613,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72512,"user_name":"佳佳的爸","can_delete":false,"product_type":"c1","uid":1082338,"ip_address":"","ucode":"9D4FE7C3552087","user_header":"https://static001.geekbang.org/account/avatar/00/10/83/e2/297518ab.jpg","comment_is_top":false,"comment_ctime":1551662583,"is_pvip":false,"replies":[{"id":"64469","content":"https:&#47;&#47;github.com&#47;cystanford","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577619637,"ip_address":"","comment_id":72512,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551662583","product_id":100021701,"comment_content":"你好老师，完整的源代码在哪里可以下载到?  我说的是每节课里边的源代码。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441625,"discussion_content":"https://github.com/cystanford","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619637,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}