{"id":134098,"title":"42 | Kafka Streams在金融领域的应用","content":"<p>你好，我是胡夕。今天我要和你分享的主题是：Kafka Streams在金融领域的应用。</p><h2>背景</h2><p>金融领域囊括的内容有很多，我今天分享的主要是，如何利用大数据技术，特别是Kafka Streams实时计算框架，来帮助我们更好地做企业用户洞察。</p><p>众所周知，金融领域内的获客成本是相当高的，一线城市高净值白领的获客成本通常可达上千元。面对如此巨大的成本压力，金融企业一方面要降低广告投放的获客成本，另一方面要做好精细化运营，实现客户生命周期内价值（Custom Lifecycle Value, CLV）的最大化。</p><p><strong>实现价值最大化的一个重要途径就是做好用户洞察，而用户洞察要求你要更深度地了解你的客户</strong>，即所谓的Know Your Customer（KYC），真正做到以客户为中心，不断地满足客户需求。</p><p>为了实现KYC，传统的做法是花费大量的时间与客户见面，做面对面的沟通以了解客户的情况。但是，用这种方式得到的数据往往是不真实的，毕竟客户内心是有潜在的自我保护意识的，短时间内的面对面交流很难真正洞察到客户的真实诉求。</p><p>相反地，渗透到每个人日常生活方方面面的大数据信息则代表了客户的实际需求。比如客户经常浏览哪些网站、都买过什么东西、最喜欢的视频类型是什么。这些数据看似很随意，但都表征了客户最真实的想法。将这些数据汇总在一起，我们就能完整地构造出客户的画像，这就是所谓的用户画像（User Profile）技术。</p><!-- [[[read_end]]] --><h2>用户画像</h2><p>用户画像听起来很玄妙，但实际上你应该是很熟悉的。你的很多基本信息，比如性别、年龄、所属行业、工资收入和爱好等，都是用户画像的一部分。举个例子，我们可以这样描述一个人：某某某，男性，28岁，未婚，工资水平大致在15000到20000元之间，是一名大数据开发工程师，居住在北京天通苑小区，平时加班很多，喜欢动漫或游戏。</p><p>其实，这一连串的描述就是典型的用户画像。通俗点来说，构建用户画像的核心工作就是给客户或用户打标签（Tagging）。刚刚那一连串的描述就是用户系统中的典型标签。用户画像系统通过打标签的形式，把客户提供给业务人员，从而实现精准营销。</p><h2>ID映射（ID Mapping）</h2><p>用户画像的好处不言而喻，而且标签打得越多越丰富，就越能精确地表征一个人的方方面面。不过，在打一个个具体的标签之前，弄清楚“你是谁”是所有用户画像系统首要考虑的问题，这个问题也被称为ID识别问题。</p><p>所谓的ID即Identification，表示用户身份。在网络上，能够标识用户身份信息的常见ID有5种。</p><ul>\n<li>身份证号：这是最能表征身份的ID信息，每个身份证号只会对应一个人。</li>\n<li>手机号：手机号通常能较好地表征身份。虽然会出现同一个人有多个手机号或一个手机号在不同时期被多个人使用的情形，但大部分互联网应用使用手机号表征用户身份的做法是很流行的。</li>\n<li>设备ID：在移动互联网时代，这主要是指手机的设备ID或Mac、iPad等移动终端设备的设备ID。特别是手机的设备ID，在很多场景下具备定位和识别用户的功能。常见的设备ID有iOS端的IDFA和Android端的IMEI。</li>\n<li>应用注册账号：这属于比较弱的一类ID。每个人在不同的应用上可能会注册不同的账号，但依然有很多人使用通用的注册账号名称，因此具有一定的关联性和识别性。</li>\n<li>Cookie：在PC时代，浏览器端的Cookie信息是很重要的数据，它是网络上表征用户信息的重要手段之一。只不过随着移动互联网时代的来临，Cookie早已江河日下，如今作为ID数据的价值也越来越小了。我个人甚至认为，在构建基于移动互联网的新一代用户画像时，Cookie可能要被抛弃了。</li>\n</ul><p>在构建用户画像系统时，我们会从多个数据源上源源不断地收集各种个人用户数据。通常情况下，这些数据不会全部携带以上这些ID信息。比如在读取浏览器的浏览历史时，你获取的是Cookie数据，而读取用户在某个App上的访问行为数据时，你拿到的是用户的设备ID和注册账号信息。</p><p>倘若这些数据表征的都是一个用户的信息，我们的用户画像系统如何识别出来呢？换句话说，你需要一种手段或技术帮你做各个ID的打通或映射。这就是用户画像领域的ID映射问题。</p><h2>实时ID Mapping</h2><p>我举个简单的例子。假设有一个金融理财用户张三，他首先在苹果手机上访问了某理财产品，然后在安卓手机上注册了该理财产品的账号，最后在电脑上登录该账号，并购买了该理财产品。ID Mapping 就是要将这些不同端或设备上的用户信息聚合起来，然后找出并打通用户所关联的所有ID信息。</p><p>实时ID Mapping的要求就更高了，它要求我们能够实时地分析从各个设备收集来的数据，并在很短的时间内完成ID Mapping。打通用户ID身份的时间越短，我们就能越快地为其打上更多的标签，从而让用户画像发挥更大的价值。</p><p>从实时计算或流处理的角度来看，实时ID Mapping能够转换成一个<strong>流-表连接问题</strong>（Stream-Table Join），即我们实时地将一个流和一个表进行连接。</p><p>消息流中的每个事件或每条消息包含的是一个未知用户的某种信息，它可以是用户在页面的访问记录数据，也可以是用户的购买行为数据。这些消息中可能会包含我们刚才提到的若干种ID信息，比如页面访问信息中可能包含设备ID，也可能包含注册账号，而购买行为信息中可能包含身份证信息和手机号等。</p><p>连接的另一方表保存的是<strong>用户所有的ID信息</strong>，随着连接的不断深入，表中保存的ID品类会越来越丰富，也就是说，流中的数据会被不断地补充进表中，最终实现对用户所有ID的打通。</p><h2>Kafka Streams实现</h2><p>好了，现在我们就来看看如何使用Kafka Streams来实现一个特定场景下的实时ID Mapping。为了方便理解，我们假设ID Mapping只关心身份证号、手机号以及设备ID。下面是用Avro写成的Schema格式：</p><pre><code>{\n  &quot;namespace&quot;: &quot;kafkalearn.userprofile.idmapping&quot;,\n  &quot;type&quot;: &quot;record&quot;,\n  &quot;name&quot;: &quot;IDMapping&quot;,\n  &quot;fields&quot;: [\n    {&quot;name&quot;: &quot;deviceId&quot;, &quot;type&quot;: &quot;string&quot;},\n    {&quot;name&quot;: &quot;idCard&quot;, &quot;type&quot;: &quot;string&quot;},\n    {&quot;name&quot;: &quot;phone&quot;, &quot;type&quot;: &quot;string&quot;}\n  ]\n}\n</code></pre><p>顺便说一下，<strong>Avro是Java或大数据生态圈常用的序列化编码机制</strong>，比如直接使用JSON或XML保存对象。Avro能极大地节省磁盘占用空间或网络I/O传输量，因此普遍应用于大数据量下的数据传输。</p><p>在这个场景下，我们需要两个Kafka主题，一个用于构造表，另一个用于构建流。这两个主题的消息格式都是上面的IDMapping对象。</p><p>新用户在填写手机号注册App时，会向第一个主题发送一条消息，该用户后续在App上的所有访问记录，也都会以消息的形式发送到第二个主题。值得注意的是，发送到第二个主题上的消息有可能携带其他的ID信息，比如手机号或设备ID等。就像我刚刚所说的，这是一个典型的流-表实时连接场景，连接之后，我们就能够将用户的所有数据补齐，实现ID Mapping的打通。</p><p>基于这个设计思路，我先给出完整的Kafka Streams代码，稍后我会对重点部分进行详细解释：</p><pre><code>package kafkalearn.userprofile.idmapping;\n\n// omit imports……\n\npublic class IDMappingStreams {\n\n\n    public static void main(String[] args) throws Exception {\n\n        if (args.length &lt; 1) {\n            throw new IllegalArgumentException(&quot;Must specify the path for a configuration file.&quot;);\n        }\n\n        IDMappingStreams instance = new IDMappingStreams();\n        Properties envProps = instance.loadProperties(args[0]);\n        Properties streamProps = instance.buildStreamsProperties(envProps);\n        Topology topology = instance.buildTopology(envProps);\n\n        instance.createTopics(envProps);\n\n        final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n        final CountDownLatch latch = new CountDownLatch(1);\n\n        // Attach shutdown handler to catch Control-C.\n        Runtime.getRuntime().addShutdownHook(new Thread(&quot;streams-shutdown-hook&quot;) {\n            @Override\n            public void run() {\n                streams.close();\n                latch.countDown();\n            }\n        });\n\n        try {\n            streams.start();\n            latch.await();\n        } catch (Throwable e) {\n            System.exit(1);\n        }\n        System.exit(0);\n    }\n\n    private Properties loadProperties(String propertyFilePath) throws IOException {\n        Properties envProps = new Properties();\n        try (FileInputStream input = new FileInputStream(propertyFilePath)) {\n            envProps.load(input);\n            return envProps;\n        }\n    }\n\n    private Properties buildStreamsProperties(Properties envProps) {\n        Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(&quot;application.id&quot;));\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(&quot;bootstrap.servers&quot;));\n        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n        return props;\n    }\n\n    private void createTopics(Properties envProps) {\n        Map&lt;String, Object&gt; config = new HashMap&lt;&gt;();\n        config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(&quot;bootstrap.servers&quot;));\n        try (AdminClient client = AdminClient.create(config)) {\n            List&lt;NewTopic&gt; topics = new ArrayList&lt;&gt;();\n            topics.add(new NewTopic(\n                    envProps.getProperty(&quot;stream.topic.name&quot;),\n                    Integer.parseInt(envProps.getProperty(&quot;stream.topic.partitions&quot;)),\n                    Short.parseShort(envProps.getProperty(&quot;stream.topic.replication.factor&quot;))));\n\n            topics.add(new NewTopic(\n                    envProps.getProperty(&quot;table.topic.name&quot;),\n                    Integer.parseInt(envProps.getProperty(&quot;table.topic.partitions&quot;)),\n                    Short.parseShort(envProps.getProperty(&quot;table.topic.replication.factor&quot;))));\n\n            client.createTopics(topics);\n        }\n    }\n\n    private Topology buildTopology(Properties envProps) {\n        final StreamsBuilder builder = new StreamsBuilder();\n        final String streamTopic = envProps.getProperty(&quot;stream.topic.name&quot;);\n        final String rekeyedTopic = envProps.getProperty(&quot;rekeyed.topic.name&quot;);\n        final String tableTopic = envProps.getProperty(&quot;table.topic.name&quot;);\n        final String outputTopic = envProps.getProperty(&quot;output.topic.name&quot;);\n        final Gson gson = new Gson();\n\n        // 1. 构造表\n        KStream&lt;String, IDMapping&gt; rekeyed = builder.&lt;String, String&gt;stream(tableTopic)\n                .mapValues(json -&gt; gson.fromJson(json, IDMapping.class))\n                .filter((noKey, idMapping) -&gt; !Objects.isNull(idMapping.getPhone()))\n                .map((noKey, idMapping) -&gt; new KeyValue&lt;&gt;(idMapping.getPhone(), idMapping));\n        rekeyed.to(rekeyedTopic);\n        KTable&lt;String, IDMapping&gt; table = builder.table(rekeyedTopic);\n\n        // 2. 流-表连接\n        KStream&lt;String, String&gt; joinedStream = builder.&lt;String, String&gt;stream(streamTopic)\n                .mapValues(json -&gt; gson.fromJson(json, IDMapping.class))\n                .map((noKey, idMapping) -&gt; new KeyValue&lt;&gt;(idMapping.getPhone(), idMapping))\n                .leftJoin(table, (value1, value2) -&gt; IDMapping.newBuilder()\n                        .setPhone(value2.getPhone() == null ? value1.getPhone() : value2.getPhone())\n                        .setDeviceId(value2.getDeviceId() == null ? value1.getDeviceId() : value2.getDeviceId())\n                        .setIdCard(value2.getIdCard() == null ? value1.getIdCard() : value2.getIdCard())\n                        .build())\n                .mapValues(v -&gt; gson.toJson(v));\n\n        joinedStream.to(outputTopic);\n\n        return builder.build();\n    }\n}\n\n</code></pre><p>这个Java类代码中最重要的方法是<strong>buildTopology函数</strong>，它构造了我们打通ID Mapping的所有逻辑。</p><p>在该方法中，我们首先构造了StreamsBuilder对象实例，这是构造任何Kafka Streams应用的第一步。之后我们读取配置文件，获取了要读写的所有Kafka主题名。在这个例子中，我们需要用到4个主题，它们的作用如下：</p><ul>\n<li>streamTopic：保存用户登录App后发生的各种行为数据，格式是IDMapping对象的JSON串。你可能会问，前面不是都创建Avro Schema文件了吗，怎么这里又用回JSON了呢？原因是这样的：社区版的Kafka没有提供Avro的序列化/反序列化类支持，如果我要使用Avro，必须改用Confluent公司提供的Kafka，但这会偏离我们专栏想要介绍Apache Kafka的初衷。所以，我还是使用JSON进行说明。这里我只是用了Avro Code Generator帮我们提供IDMapping对象各个字段的set/get方法，你使用Lombok也是可以的。</li>\n<li>rekeyedTopic：这个主题是一个中间主题，它将streamTopic中的手机号提取出来作为消息的Key，同时维持消息体不变。</li>\n<li>tableTopic：保存用户注册App时填写的手机号。我们要使用这个主题构造连接时要用到的表数据。</li>\n<li>outputTopic：保存连接后的输出信息，即打通了用户所有ID数据的IDMapping对象，将其转换成JSON后输出。</li>\n</ul><p>buildTopology的第一步是构造表，即KTable对象。我们修改初始的消息流，以用户注册的手机号作为Key，构造了一个中间流，之后将这个流写入到rekeyedTopic，最后直接使用builder.table方法构造出KTable。这样每当有新用户注册时，该KTable都会新增一条数据。</p><p>有了表之后，我们继续构造消息流来封装用户登录App之后的行为数据，我们同样提取出手机号作为要连接的Key，之后使用KStream的<strong>leftJoin方法</strong>将其与上一步的KTable对象进行关联。</p><p>在关联的过程中，我们同时提取两边的信息，尽可能地补充到最后生成的IDMapping对象中，然后将这个生成的IDMapping实例返回到新生成的流中。最后，我们将它写入到outputTopic中保存。</p><p>至此，我们使用了不到200行的Java代码，就简单实现了一个真实场景下的实时ID Mapping任务。理论上，你可以将这个例子继续扩充，扩展到任意多个ID Mapping，甚至是含有其他标签的数据，连接原理是相通的。在我自己的项目中，我借助于Kafka Streams帮助我实现了用户画像系统的部分功能，而ID Mapping就是其中的一个。</p><h2>小结</h2><p>好了，我们小结一下。今天，我展示了Kafka Streams在金融领域的一个应用案例，重点演示了如何利用连接函数来实时关联流和表。其实，Kafka Streams提供的功能远不止这些，我推荐你阅读一下<a href=\"https://kafka.apache.org/23/documentation/streams/developer-guide/\">官网</a>的教程，然后把自己的一些轻量级的实时计算线上任务改为使用Kafka Streams来实现。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/e7/75df06c2b75c3886ca3496a774730de7.jpg?wh=2069*2560\" alt=\"\"></p><h2>开放讨论</h2><p>最后，我们来讨论一个问题。在刚刚的这个例子中，你觉得我为什么使用leftJoin方法而不是join方法呢？（小提示：可以对比一下SQL中的left join和inner join。）</p><p>欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":132433,"user_name":"兔2🐰🍃","can_delete":false,"product_type":"c1","uid":1096984,"ip_address":"","ucode":"1FEDA044BB6CBD","user_header":"https://static001.geekbang.org/account/avatar/00/10/bd/18/2af6bf4b.jpg","comment_is_top":false,"comment_ctime":1568121265,"is_pvip":false,"discussion_count":4,"race_medal":0,"score":"44517794225","product_id":100029201,"comment_content":"App上发现该栏目更新了最后一节，目前才学到22节，完成了前三章的了解。我从上个月20日开始的，有20天时间了，也算是从0开始的，对Kafka有了很多了解，每听完一节，遍跟着写笔记，结构化文章内容，感觉有点慢，实践还没开始。明天起换个方式试试效果，搭个环境，实践下前22节内容，接着后面的章节先每章节内容听一遍后梳理整个章节的内容，再到实践。<br>感谢胡老师的教课，节日快乐~","like_count":11,"discussions":[{"author":{"id":1488020,"avatar":"https://static001.geekbang.org/account/avatar/00/16/b4/94/2796de72.jpg","nickname":"追风筝的人","note":"","ucode":"2993D60F94C396","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":551579,"discussion_content":"厉害了  兔哥","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645064918,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1096984,"avatar":"https://static001.geekbang.org/account/avatar/00/10/bd/18/2af6bf4b.jpg","nickname":"兔2🐰🍃","note":"","ucode":"1FEDA044BB6CBD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":45391,"discussion_content":"欣慰啊，终于看完了要，好多还要回顾熟悉下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573032901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1032331,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c0/8b/0371baee.jpg","nickname":"张丽娜","note":"","ucode":"D70CFF68E72DAF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1096984,"avatar":"https://static001.geekbang.org/account/avatar/00/10/bd/18/2af6bf4b.jpg","nickname":"兔2🐰🍃","note":"","ucode":"1FEDA044BB6CBD","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":197010,"discussion_content":"搭建环境了么？理论知识如果没有实现的话，有没有感觉很难落地？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583395007,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":45391,"ip_address":""},"score":197010,"extra":""},{"author":{"id":1096984,"avatar":"https://static001.geekbang.org/account/avatar/00/10/bd/18/2af6bf4b.jpg","nickname":"兔2🐰🍃","note":"","ucode":"1FEDA044BB6CBD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1032331,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c0/8b/0371baee.jpg","nickname":"张丽娜","note":"","ucode":"D70CFF68E72DAF","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":198068,"discussion_content":"嗯，环境搭建了，对于windows用户来说还是有点难度，配合网上资料才明白怎么搭建集群，没基础的光看理论很难弄懂，上手操作时一大堆问题，坑走过了才能学到东西。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1583468632,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":197010,"ip_address":""},"score":198068,"extra":""}]}]},{"had_liked":false,"id":132541,"user_name":"曾轼麟","can_delete":false,"product_type":"c1","uid":1451391,"ip_address":"","ucode":"D418371AC11270","user_header":"https://static001.geekbang.org/account/avatar/00/16/25/7f/473d5a77.jpg","comment_is_top":false,"comment_ctime":1568163462,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23042999942","product_id":100029201,"comment_content":"left join 可以关联上不存在的数据行，而join其实是inner join 需要两张表数据都匹配上结果才会有，left join的好处就是，如果其中有一张表没有匹配数据，也不会导致结果集没有这条记录","like_count":4},{"had_liked":false,"id":290709,"user_name":"Minze","can_delete":false,"product_type":"c1","uid":1534718,"ip_address":"","ucode":"2367DA6C09BE7C","user_header":"https://static001.geekbang.org/account/avatar/00/17/6a/fe/446c0282.jpg","comment_is_top":false,"comment_ctime":1619696543,"is_pvip":false,"replies":[{"id":"105701","content":"没有什么特别的含义，源码就是这么命名的，如下：<br>static final String OUTEROTHER_NAME = &quot;KSTREAM-OUTEROTHER-&quot;;","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1620614426,"ip_address":"","comment_id":290709,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5914663839","product_id":100029201,"comment_content":"胡老师您好，最近我在学习使用Kafka streams，有一点让我困惑的是在使用join的时候，生成的topology中会有一些名字包含KSTREAM–JOINTHIS和KSTREAM–OUTEROTHER的processor和store创建出来，这些本地的store应该是用来存储join要用到的数据，这我能理解，但是这些processor的作用和JOINTHIS、OUTEROTHER的命名含义却百思不得其解，老师可否给些建议或者提示？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519268,"discussion_content":"没有什么特别的含义，源码就是这么命名的，如下：\nstatic final String OUTEROTHER_NAME = &amp;quot;KSTREAM-OUTEROTHER-&amp;quot;;","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620614426,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":191474,"user_name":"King","can_delete":false,"product_type":"c1","uid":1109772,"ip_address":"","ucode":"955CDA858B1073","user_header":"https://static001.geekbang.org/account/avatar/00/10/ef/0c/e2169f41.jpg","comment_is_top":false,"comment_ctime":1584776871,"is_pvip":false,"replies":[{"id":"73520","content":"append的方式。不能保证，因为最后是将所有状态转换成stream了","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1584846924,"ip_address":"","comment_id":191474,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5879744167","product_id":100029201,"comment_content":"最后写入outputTopic是以append还是update？可以保证用户在outputTopic中只有一条记录吗？","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":488184,"discussion_content":"append的方式。不能保证，因为最后是将所有状态转换成stream了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584846924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":176126,"user_name":"IT小僧","can_delete":false,"product_type":"c1","uid":1036906,"ip_address":"","ucode":"4DC9B291AAD748","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d2/6a/a9039139.jpg","comment_is_top":false,"comment_ctime":1580964740,"is_pvip":true,"replies":[{"id":"68706","content":"也没关系啊，保存在底层的topic中","user_name":"作者回复","user_name_real":"huxi_2b","uid":"1288090","ctime":1581296160,"ip_address":"","comment_id":176126,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5875932036","product_id":100029201,"comment_content":"老师好，这个构造IDMapping中的KTable对象肯定会越来越大吧，这个怎么处理呢","like_count":1,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":483027,"discussion_content":"也没关系啊，保存在底层的topic中","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1581296160,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1635021,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/MIbA6MRiblftUawVWvKt6jvgOvTwibKsTCJhh5y5vKEuURtcEZDtylwGFfekZBanmwIgNSJTm9YMmZlPPicDQ14Iw/132","nickname":"Geek_4254d8","note":"","ucode":"806A8116A3177F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":313329,"discussion_content":"另外想请教老师，底层Topic数据量大了之后（即使分区应该也避免不了膨胀，而分区消息应该也无法像数据库一样通过key快速访问中间记录吧），怎么保证流表left join的性能？compact存储反过来也会消耗读取的性能","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603032048,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1635021,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/MIbA6MRiblftUawVWvKt6jvgOvTwibKsTCJhh5y5vKEuURtcEZDtylwGFfekZBanmwIgNSJTm9YMmZlPPicDQ14Iw/132","nickname":"Geek_4254d8","note":"","ucode":"806A8116A3177F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":313324,"discussion_content":"但是topic不是也会消息过期吗？这块怎么控制消息什么时候可以丢弃呢（两个场景1.用户信息很快已经匹配完成输出到仓存储KTable中对应的数据已经完成使命，2.用户信息一直没完成填报，并且不确定用户是否还会填报）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603031763,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335637,"user_name":"Kaine","can_delete":false,"product_type":"c1","uid":2799936,"ip_address":"","ucode":"970FBF356F3F59","user_header":"https://static001.geekbang.org/account/avatar/00/2a/b9/40/e350862c.jpg","comment_is_top":false,"comment_ctime":1645609671,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1645609671","product_id":100029201,"comment_content":"老师在&lt;VT, VR&gt; KStream&lt;K, VR&gt; leftJoin(final KTable&lt;K, VT&gt; table,<br>                                     final ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner);<br>有这么一段注释，上面说输入流的分区数必须与Ktable对应的topic的分区数一致这是为什么呢<br>Both input streams (or to be more precise, their underlying source topics) need to have the same number of partitions. If this is not the case, you would need to call through(String) for this KStream before doing the join, using a pre-created topic with the same number of partitions as the given KTable. Furthermore, both input streams need to be co-partitioned on the join key (i.e., use the same partitioner); cf. join(GlobalKTable, KeyValueMapper, ValueJoiner). If this requirement is not met, Kafka Streams will automatically repartition the data, i.e., it will create an internal repartitioning topic in Kafka and write and re-read the data via this topic before the actual join. The repartitioning topic will be named &quot;${applicationId}-&lt;name&gt;-repartition&quot;, where &quot;applicationId&quot; is user-specified in StreamsConfig via parameter APPLICATION_ID_CONFIG, &quot;&lt;name&gt;&quot; is an internally generated name, and &quot;-repartition&quot; is a fixed suffix.","like_count":0},{"had_liked":false,"id":331212,"user_name":"小仙","can_delete":false,"product_type":"c1","uid":1866677,"ip_address":"","ucode":"9F94043DFCEC3A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq6pWvKsV4rzQ62z5MDEjaEU5MbDfmzbA62kUgoqia2tgKIIxw4ibkDhF7W48iat5dT8UB9Adky2NuzQ/132","comment_is_top":false,"comment_ctime":1642487257,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1642487257","product_id":100029201,"comment_content":"这里用到的 4 个 topic 就是普通的 topic 吗？也是默认 7 天消息过期等等设定，并非永久保存","like_count":0},{"had_liked":false,"id":260868,"user_name":"朝花西落","can_delete":false,"product_type":"c1","uid":1055199,"ip_address":"","ucode":"CBD4A0D129500D","user_header":"https://static001.geekbang.org/account/avatar/00/10/19/df/5486f00d.jpg","comment_is_top":false,"comment_ctime":1605145990,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1605145990","product_id":100029201,"comment_content":"代码的99行是不是没有必要呢，我感觉既然是按手机号key来做join的，那不管value2是否为null，最终都应该是value1的值。因为如果不为null，value1和value2也是相等的吧。","like_count":0},{"had_liked":false,"id":251623,"user_name":"timmy21","can_delete":false,"product_type":"c1","uid":1174860,"ip_address":"","ucode":"9D6DED247B1F38","user_header":"https://static001.geekbang.org/account/avatar/00/11/ed/4c/8674b6ad.jpg","comment_is_top":false,"comment_ctime":1601732673,"is_pvip":false,"replies":[{"id":"92148","content":"保存在底层的Topic中，这类topic一定是启用了compact了的","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1602206503,"ip_address":"","comment_id":251623,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1601732673","product_id":100029201,"comment_content":"KTable是全部保存在内存吗？具体实现是怎么样的？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":506569,"discussion_content":"保存在底层的Topic中，这类topic一定是启用了compact了的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602206503,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":236025,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1595293981,"is_pvip":false,"replies":[{"id":"87309","content":"如果是Kafka Streams，consumer的数量由num.stream.threads直接决定，当然也受订阅总分区数的制约","user_name":"作者回复","user_name_real":"胡夕","uid":"1288090","ctime":1595386629,"ip_address":"","comment_id":236025,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1595293981","product_id":100029201,"comment_content":"老师，我看到有使用stream加迭代器的方式去消费消息的，貌似建立连接时传入了一个topicCount的map，也就是说这个stream其实可以消费到多个topic的数据，总觉得有点奇怪，因为跟老师之前课程里那种poll的方式差别很大。我不太能明白这种形式消费数据，到底消费者对应是某一个stream？还是topicCount的每个topic？是哪个决定了consumer的数量？","like_count":0,"discussions":[{"author":{"id":1288090,"avatar":"https://static001.geekbang.org/account/avatar/00/13/a7/9a/495cb99a.jpg","nickname":"胡夕","note":"","ucode":"5709A689B6683B","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501920,"discussion_content":"如果是Kafka Streams，consumer的数量由num.stream.threads直接决定，当然也受订阅总分区数的制约","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595386629,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":233414,"user_name":"布吉岛上的不科学君","can_delete":false,"product_type":"c1","uid":1162178,"ip_address":"","ucode":"A796610CAF5904","user_header":"https://static001.geekbang.org/account/avatar/00/11/bb/c2/cbd652ab.jpg","comment_is_top":false,"comment_ctime":1594309995,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1594309995","product_id":100029201,"comment_content":"老师你好，我有一个疑惑：<br>首先，假设用户注册的时候只有【手机号】信息，那么KTable里是没有【DeviceId】和【IdCard】的。<br>然后用户发生的第一次操作，携带了【DeviceId】，第二次操作携带了【IdCard】，这些数据从代码上看是不会更新【KTable】的，那么【outputTopic】两次输出的内容都是不全面，也就是说【outputTopic】没法同时输出携带了【DeviceId和IdCard】信息的吧？希望胡老师能够抽空解答一下，谢谢。<br>","like_count":0}]}