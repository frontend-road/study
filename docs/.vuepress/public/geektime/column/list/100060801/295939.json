{"id":295939,"title":"06 | Embedding基础：所有人都在谈的Embedding技术到底是什么？","content":"<p>你好，我是王喆。今天我们聊聊Embedding。</p><p>说起Embedding，我想你肯定不会陌生，至少经常听说。事实上，Embedding技术不仅名气大，而且用Embedding方法进行相似物品推荐，几乎成了业界最流行的做法，无论是国外的Facebook、Airbnb，还是在国内的阿里、美团，我们都可以看到Embedding的成功应用。因此，自从深度学习流行起来之后，Embedding就成为了深度学习推荐系统方向最火热的话题之一。</p><p>但是Embedding这个词又不是很好理解，你甚至很难给它找出一个准确的中文翻译，如果硬是翻译成“嵌入”“向量映射”，感觉也不知所谓。所以索性我们就还是用Embedding这个叫法吧。</p><p>那这项技术到底是什么，为什么它在推荐系统领域这么重要？最经典的Embedding方法Word2vec的原理细节到底啥样？这节课，我们就一起来聊聊这几个问题。</p><h2>什么是Embedding？</h2><p>简单来说，<strong>Embedding就是用一个数值向量“表示”一个对象（Object）的方法</strong>，我这里说的对象可以是一个词、一个物品，也可以是一部电影等等。但是“表示”这个词是什么意思呢？用一个向量表示一个物品，这句话感觉还是有点让人费解。</p><!-- [[[read_end]]] --><p>这里，我先尝试着解释一下：一个物品能被向量表示，是因为这个向量跟其他物品向量之间的距离反映了这些物品的相似性。更进一步来说，两个向量间的距离向量甚至能够反映它们之间的关系。这个解释听上去可能还是有点抽象，那我们再用两个具体的例子解释一下。</p><p>图1是Google著名的论文Word2vec中的例子，它利用Word2vec这个模型把单词映射到了高维空间中，每个单词在这个高维空间中的位置都非常有意思，你看图1左边的例子，从king到queen的向量和从man到woman的向量，无论从方向还是尺度来说它们都异常接近。这说明什么？这说明词Embedding向量间的运算居然能够揭示词之间的性别关系！比如woman这个词的词向量可以用下面的运算得出：</p><p>Embedding(<strong>woman</strong>)=Embedding(<strong>man</strong>)+[Embedding(<strong>queen</strong>)-Embedding(<strong>king</strong>)]</p><p>同样，图1右的例子也很典型，从walking到walked和从swimming到swam的向量基本一致，这说明词向量揭示了词之间的时态关系！这就是Embedding技术的神奇之处。</p><p><img src=\"https://static001.geekbang.org/resource/image/19/0a/19245b8bc3ebd987625e36881ca4f50a.jpeg?wh=960*540\" alt=\"\" title=\"图1 词向量例子\"></p><p>你可能会觉得词向量技术离推荐系统领域还是有一点远，那Netflix应用的电影Embedding向量方法，就是一个非常直接的推荐系统应用。从Netflix利用矩阵分解方法生成的电影和用户的Embedding向量示意图中，我们可以看出不同的电影和用户分布在一个二维的空间内，由于Embedding向量保存了它们之间的相似性关系，因此有了这个Embedding空间之后，我们再进行电影推荐就非常容易了。具体来说就是，我们直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了。这就是Embedding技术在推荐系统中最直接的应用。</p><p><img src=\"https://static001.geekbang.org/resource/image/da/4c/da7e73faacc5e6ea1c02345386bf6f4c.jpeg?wh=1920*1080\" alt=\"\" title=\"图2 电影-用户向量例子\"></p><h2>Embedding技术对深度学习推荐系统的重要性</h2><p>事实上，我一直把Embedding技术称作深度学习的“基础核心操作”。在推荐系统领域进入深度学习时代之后，Embedding技术更是“如鱼得水”。那为什么Embedding技术对于推荐系统如此重要，Embedding技术又在特征工程中发挥了怎样的作用呢？针对这两个问题，我主要有两点想和你深入聊聊。</p><p><strong>首先，Embedding是处理稀疏特征的利器。</strong> 上节课我们学习了One-hot编码，因为推荐场景中的类别、ID型特征非常多，大量使用One-hot编码会导致样本特征向量极度稀疏，而深度学习的结构特点又不利于稀疏特征向量的处理，因此几乎所有深度学习推荐模型都会由Embedding层负责将稀疏高维特征向量转换成稠密低维特征向量。所以说各类Embedding技术是构建深度学习推荐模型的基础性操作。</p><p><strong>其次，Embedding可以融合大量有价值信息，本身就是极其重要的特征向量 。</strong> 相比由原始信息直接处理得来的特征向量，Embedding的表达能力更强，特别是Graph Embedding技术被提出后，Embedding几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息，所以通过预训练得到的Embedding向量本身就是极其重要的特征向量。</p><p>因此我们才说，Embedding技术在深度学习推荐系统中占有极其重要的位置，熟悉并掌握各类流行的Embedding方法是构建一个成功的深度学习推荐系统的有力武器。<strong>这两个特点也是我们为什么把Embedding的相关内容放到特征工程篇的原因，因为它不仅是一种处理稀疏特征的方法，也是融合大量基本特征，生成高阶特征向量的有效手段。</strong></p><h2>经典的Embedding方法，Word2vec</h2><p>提到Embedding，就一定要深入讲解一下Word2vec。它不仅让词向量在自然语言处理领域再度流行，更关键的是，自从2013年谷歌提出Word2vec以来，Embedding技术从自然语言处理领域推广到广告、搜索、图像、推荐等几乎所有深度学习的领域，成了深度学习知识框架中不可或缺的技术点。Word2vec作为经典的Embedding方法，熟悉它对于我们理解之后所有的Embedding相关技术和概念都是至关重要的。下面，我就给你详细讲一讲Word2vec的原理。</p><h3>什么是Word2vec？</h3><p>Word2vec是“word to vector”的简称，顾名思义，它是一个生成对“词”的向量表达的模型。</p><p>想要训练Word2vec模型，我们需要准备由一组句子组成的语料库。假设其中一个长度为T的句子包含的词有w<sub>1</sub>,w<sub>2</sub>……w<sub>t</sub>，并且我们假定每个词都跟其相邻词的关系最密切。</p><p>根据模型假设的不同，Word2vec模型分为两种形式，CBOW模型（图3左）和Skip-gram模型（图3右）。其中，CBOW模型假设句子中每个词的选取都由相邻的词决定，因此我们就看到CBOW模型的输入是w<sub>t</sub>周边的词，预测的输出是w<sub>t</sub>。Skip-gram模型则正好相反，它假设句子中的每个词都决定了相邻词的选取，所以你可以看到Skip-gram模型的输入是w<sub>t</sub>，预测的输出是w<sub>t</sub>周边的词。按照一般的经验，Skip-gram模型的效果会更好一些，所以我接下来也会以Skip-gram作为框架，来给你讲讲Word2vec的模型细节。</p><p><img src=\"https://static001.geekbang.org/resource/image/f2/8a/f28a06f57e4aeb5f826df466cbe6288a.jpeg?wh=960*540\" alt=\"\" title=\"图3 Word2vec的两种模型结构CBOW和Skip-gram\"></p><h3>Word2vec的样本是怎么生成的？</h3><p>我们先来看看<strong>训练Word2vec的样本是怎么生成的。</strong> 作为一个自然语言处理的模型，训练Word2vec的样本当然来自于语料库，比如我们想训练一个电商网站中关键词的Embedding模型，那么电商网站中所有物品的描述文字就是很好的语料库。</p><p>我们从语料库中抽取一个句子，选取一个长度为2c+1（目标词前后各选c个词）的滑动窗口，将滑动窗口由左至右滑动，每移动一次，窗口中的词组就形成了一个训练样本。根据Skip-gram模型的理念，中心词决定了它的相邻词，我们就可以根据这个训练样本定义出Word2vec模型的输入和输出，输入是样本的中心词，输出是所有的相邻词。</p><p>为了方便你理解，我再举一个例子。这里我们选取了“Embedding技术对深度学习推荐系统的重要性”作为句子样本。首先，我们对它进行分词、去除停用词的过程，生成词序列，再选取大小为3的滑动窗口从头到尾依次滑动生成训练样本，然后我们把中心词当输入，边缘词做输出，就得到了训练Word2vec模型可用的训练样本。</p><p><img src=\"https://static001.geekbang.org/resource/image/e8/1f/e84e1bd1f7c5950fb70ed63dda0yy21f.jpeg?wh=960*540\" alt=\"\" title=\"图4 生成Word2vec训练样本的例子\"></p><h3>Word2vec模型的结构是什么样的？</h3><p>有了训练样本之后，我们最关心的当然是Word2vec这个模型的结构是什么样的。我相信，通过第3节课的学习，你已经掌握了神经网络的基础知识，那再理解Word2vec的结构就容易多了，它的结构本质上就是一个三层的神经网络（如图5）。</p><p><img src=\"https://static001.geekbang.org/resource/image/99/39/9997c61588223af2e8c0b9b2b8e77139.jpeg?wh=960*540\" alt=\"\" title=\"图5 Word2vec模型的结构\n\"></p><p>它的输入层和输出层的维度都是V，这个V其实就是语料库词典的大小。假设语料库一共使用了10000个词，那么V就等于10000。根据图4生成的训练样本，这里的输入向量自然就是由输入词转换而来的One-hot编码向量，输出向量则是由多个输出词转换而来的Multi-hot编码向量，显然，基于Skip-gram框架的Word2vec模型解决的是一个多分类问题。</p><p>隐层的维度是N，N的选择就需要一定的调参能力了，我们需要对模型的效果和模型的复杂度进行权衡，来决定最后N的取值，并且最终每个词的Embedding向量维度也由N来决定。</p><p>最后是激活函数的问题，这里我们需要注意的是，隐层神经元是没有激活函数的，或者说采用了输入即输出的恒等函数作为激活函数，而输出层神经元采用了softmax作为激活函数。</p><p>你可能会问为什么要这样设置Word2vec的神经网络，以及我们为什么要这样选择激活函数呢？因为这个神经网络其实是为了表达从输入向量到输出向量的这样的一个条件概率关系，我们看下面的式子：</p><p>$$p\\left(w_{O} \\mid w_{I}\\right)=\\frac{\\exp \\left(v_{w_{O}}^{\\prime}{v}_{w_{I}}\\right)}{\\sum_{i=1}^{V} \\exp \\left(v_{w_{i}}^{\\prime}{ }^{\\top} v_{w_{I}}\\right)}$$</p><p>这个由输入词WI预测输出词WO的条件概率，其实就是Word2vec神经网络要表达的东西。我们通过极大似然的方法去最大化这个条件概率，就能够让相似的词的内积距离更接近，这就是我们希望Word2vec神经网络学到的。</p><p>当然，如果你对数学和机器学习的底层理论没那么感兴趣的话，也不用太深入了解这个公式的由来，因为现在大多数深度学习平台都把它们封装好了，你不需要去实现损失函数、梯度下降的细节，你只要大概清楚他们的概念就可以了。</p><p>如果你是一个理论派，其实Word2vec还有很多值得挖掘的东西，比如，为了节约训练时间，Word2vec经常会采用负采样（Negative Sampling）或者分层softmax（Hierarchical Softmax）的训练方法。关于这一点，我推荐你去阅读<a href=\"https://github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BWord2Vec%5D%20Word2vec%20Parameter%20Learning%20Explained%20%28UMich%202016%29.pdf\">《Word2vec Parameter Learning Explained》</a>这篇文章，相信你会找到最详细和准确的解释。</p><h3>怎样把词向量从Word2vec模型中提取出来？</h3><p>在训练完Word2vec的神经网络之后，可能你还会有疑问，我们不是想得到每个词对应的Embedding向量嘛，这个Embedding在哪呢？其实，它就藏在输入层到隐层的权重矩阵WVxN中。我想看了下面的图你一下就明白了。</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/72/0de188f4b564de8076cf13ba6ff87872.jpeg?wh=960*540\" alt=\"\" title=\"图6 词向量藏在Word2vec的权重矩阵中\"></p><p>你可以看到，输入向量矩阵WVxN的每一个行向量对应的就是我们要找的“词向量”。比如我们要找词典里第i个词对应的Embedding，因为输入向量是采用One-hot编码的，所以输入向量的第i维就应该是1，那么输入向量矩阵WVxN中第i行的行向量自然就是该词的Embedding啦。</p><p>细心的你可能也发现了，输出向量矩阵$W'$也遵循这个道理，确实是这样的，但一般来说，我们还是习惯于使用输入向量矩阵作为词向量矩阵。</p><p>在实际的使用过程中，我们往往会把输入向量矩阵转换成词向量查找表（Lookup table，如图7所示）。例如，输入向量是10000个词组成的One-hot向量，隐层维度是300维，那么输入层到隐层的权重矩阵为10000x300维。在转换为词向量Lookup table后，每行的权重即成了对应词的Embedding向量。如果我们把这个查找表存储到线上的数据库中，就可以轻松地在推荐物品的过程中使用Embedding去计算相似性等重要的特征了。</p><p><img src=\"https://static001.geekbang.org/resource/image/1e/96/1e6b464b25210c76a665fd4c34800c96.jpeg?wh=960*540\" alt=\"\" title=\"图7 Word2vec的Lookup table\"></p><h3>Word2vec对Embedding技术的奠基性意义</h3><p>Word2vec是由谷歌于2013年正式提出的，其实它并不完全是原创性的，学术界对词向量的研究可以追溯到2003年，甚至更早的时期。但正是谷歌对Word2vec的成功应用，让词向量的技术得以在业界迅速推广，进而使Embedding这一研究话题成为热点。毫不夸张地说，Word2vec对深度学习时代Embedding方向的研究具有奠基性的意义。</p><p>从另一个角度来看，Word2vec的研究中提出的模型结构、目标函数、负采样方法、负采样中的目标函数在后续的研究中被重复使用并被屡次优化。掌握Word2vec中的每一个细节成了研究Embedding的基础。从这个意义上讲，熟练掌握本节课的内容是非常重要的。</p><h2>Item2Vec：Word2vec方法的推广</h2><p>在Word2vec诞生之后，Embedding的思想迅速从自然语言处理领域扩散到几乎所有机器学习领域，推荐系统也不例外。既然Word2vec可以对词“序列”中的词进行Embedding，那么对于用户购买“序列”中的一个商品，用户观看“序列”中的一个电影，也应该存在相应的Embedding方法。</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/07/d8e3cd26a9ded7e79776dd31cc8f4807.jpeg?wh=960*334\" alt=\"\" title=\"图8 不同场景下的序列数据\"></p><p>于是，微软于2015年提出了Item2Vec方法，它是对Word2vec方法的推广，使Embedding方法适用于几乎所有的序列数据。Item2Vec模型的技术细节几乎和Word2vec完全一致，只要能够用序列数据的形式把我们要表达的对象表示出来，再把序列数据“喂”给Word2vec模型，我们就能够得到任意物品的Embedding了。</p><p>Item2vec的提出对于推荐系统来说当然是至关重要的，因为它使得“万物皆Embedding”成为了可能。对于推荐系统来说，Item2vec可以利用物品的Embedding直接求得它们的相似性，或者作为重要的特征输入推荐模型进行训练，这些都有助于提升推荐系统的效果。</p><h2>小结</h2><p>这节课，我们一起学习了深度学习推荐系统中非常重要的知识点，Embedding。Embedding就是用一个数值向量“表示”一个对象的方法。通过Embedding，我们又引出了Word2vec，Word2vec是生成对“词”的向量表达的模型。其中，Word2vec的训练样本是通过滑动窗口一一截取词组生成的。在训练完成后，模型输入向量矩阵的行向量，就是我们要提取的词向量。最后，我们还学习了Item2vec，它是Word2vec在任意序列数据上的推广。</p><p>我把这些重点的内容以表格的形式，总结了出来，方便你随时回顾。</p><p><img src=\"https://static001.geekbang.org/resource/image/0f/7b/0f0f9ffefa0c610dd691b51c251b567b.jpeg?wh=1920*777\" alt=\"\"></p><p>这节课，我们主要对序列数据进行了Embedding化，那如果是图结构的数据怎么办呢？另外，有没有什么好用的工具能实现Embedding技术呢？接下来的两节课，我就会一一讲解图结构数据的Embedding方法Graph Embedding，并基于Spark对它们进行实现。</p><h2>课后思考</h2><p>在我们通过Word2vec训练得到词向量，或者通过Item2vec得到物品向量之后，我们应该用什么方法计算他们的相似性呢？你知道几种计算相似性的方法？</p><p>如果你身边的朋友正对Embedding技术感到疑惑，也欢迎你把这节课分享给TA，我们下节课再见！</p>","neighbors":{"left":{"article_title":"05 | 特征处理：如何利用Spark解决特征处理问题？","id":295300},"right":{"article_title":"07 | Embedding进阶：如何利用图结构数据生成Graph Embedding？","id":296672}},"comments":[{"had_liked":false,"id":265991,"user_name":"神经蛙","can_delete":false,"product_type":"c1","uid":2242137,"ip_address":"","ucode":"9375BA98E03A8D","user_header":"https://static001.geekbang.org/account/avatar/00/22/36/59/010b3e60.jpg","comment_is_top":false,"comment_ctime":1607092139,"is_pvip":false,"replies":[{"id":"96663","content":"太棒了，很好的资料总结。<br><br>W1，W2那个问题，我觉得不会有影响，如果负采样对W2有影响的话，对W1也会有影响。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1607193646,"ip_address":"","comment_id":265991,"utype":1}],"discussion_count":1,"race_medal":0,"score":"203470555051","product_id":100060801,"comment_content":"最近刚好看了看Word2Vec,先列一下看了的资料：<br>    1.Numpy实现的基础版 Word2vec https:&#47;&#47;github.com&#47;YelZhang&#47;word2vec_numpy&#47;blob&#47;495c2bce99fcdfe281bce0918a6765efd3179b07&#47;wordtovec.py<br>    2.公式推导 https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;136247620    https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;108987941<br>    3.Google Word2vec c源码详解  https:&#47;&#47;blog.csdn.net&#47;jeryjeryjery&#47;article&#47;details&#47;80245924    https:&#47;&#47;blog.csdn.net&#47;google19890102&#47;article&#47;details&#47;51887344   https:&#47;&#47;github.com&#47;zhaozhiyong19890102&#47;OpenSourceReading&#47;blob&#47;master&#47;word2vec&#47;word2vec.c<br><br>**理解有问题的话，麻烦大家指出来，互相进步~**<br><br>说一下看了的理解，Google的源码中Skip-gram,中间词预测周围词是一个循环，每次的优化目标词只有一个周围词中的一个。CBOW是将周围词的向量加和求平均作为上下文词向量，来预测中心词。<br><br>为什么会出现层次Softmax和负采样的优化方法？ <br>需要先摆一下前向传播和反向传播公式：<br>-- 略去了下标，以及矩阵乘法的先后。<br>词表长度V，Embedding长度N<br>输入：X(shape:(1,V))，输入到隐层矩阵W1(shape: V,N), 隐层到输出矩阵W2(shape: N,V)<br>前向传播：<br>H=X * W1    (shape:1,N)<br>U=H * W2    (shape:1,V)<br>Y=softmax(U)<br><br>这里计算Softmax，参与计算的是V个元素。<br><br>反向传播：<br>Loss= -sum(y * logY)<br><br>Loss对W2偏导=(Y-1)*H<br>Loss对W1偏导=(Y-1)*W2*x    (由于X是one-hot向量,相乘后实际只有一行是非0)<br><br>W1更新时只更新一行,W2更新时更新整个矩阵。<br><br>原因：<br>    1.前向传播时计算softmax开销大<br>    2.反向传播时更新W2矩阵开销大<br><br>于是就有了对Sofmax这里的优化。最近主要看了负采样。<br><br>负采样：<br>每次训练时，需要预测的目标词给分成2类。一类是目标词，另一类是非目标词（个数可人工指定）（负采样而来，采样词频高的词，TensorFlow里面是这样，与原论文不同）。此时就是个二分类问题，Loss就变了。变成了Sigmod的形式。这样在前向传播不用计算Softmax，反向传播时将每次更新的向量变成了很少的几个，而不是原始的V。降低开销。<br><br>关于W1和W2哪被用来做词向量，一般是W1。<br>这里我有点疑惑，用层次Softmax或者负采样优化方法会不会对W2的效果产生影响？因为更新时没有用到所有数据，所以用W1作为词向量？","like_count":48,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511116,"discussion_content":"太棒了，很好的资料总结。\n\nW1，W2那个问题，我觉得不会有影响，如果负采样对W2有影响的话，对W1也会有影响。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607193646,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253268,"user_name":"wolong","can_delete":false,"product_type":"c1","uid":1641946,"ip_address":"","ucode":"EFF191F56230FF","user_header":"https://static001.geekbang.org/account/avatar/00/19/0d/da/8a744899.jpg","comment_is_top":false,"comment_ctime":1602666967,"is_pvip":false,"replies":[{"id":"92641","content":"非常好的冷启动问题，你有什么想法，想先听听你自己的思考。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602827220,"ip_address":"","comment_id":253268,"utype":1}],"discussion_count":9,"race_medal":0,"score":"108976849367","product_id":100060801,"comment_content":"老师您好，我这边有个问题。假如我们是做商品推荐，假如商品频繁上新，我们的物品库会是一个动态的，Embedding技术如何应对？","like_count":26,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507027,"discussion_content":"非常好的冷启动问题，你有什么想法，想先听听你自己的思考。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602827220,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":317867,"discussion_content":"基本是这样。利用一些metadata信息找到一些相似物品，然后利用相似物品的embedding进行冷启动物品的初始化。\n如果遇到大量冷启动物品，那就必须重新训练emb做到批量更新。","likes_number":25,"is_delete":false,"is_hidden":false,"ctime":1603609657,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1641946,"avatar":"https://static001.geekbang.org/account/avatar/00/19/0d/da/8a744899.jpg","nickname":"wolong","note":"","ucode":"EFF191F56230FF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":315400,"discussion_content":"我这边的思考大致是借助一些额外的先验信息，如商品分类、实体类别等已知的物品特征为新商品初始化Embedding向量。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1603269249,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2406292,"avatar":"https://static001.geekbang.org/account/avatar/00/24/b7/94/22121c60.jpg","nickname":"Kendrick","note":"","ucode":"5DF1269295D24E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":344074,"discussion_content":"正好也遇到类似的问题，做广告推荐排序，广告的素材每天变化很大，一天就有一半的素材发生了变化，3-4天就基本全换了，这方面让我最近焦头烂额的，痛苦..... ","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1611283075,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2405529,"avatar":"https://static001.geekbang.org/account/avatar/00/24/b4/99/79a21147.jpg","nickname":"轩","note":"","ucode":"454CC1A0734A4C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340140,"discussion_content":"这也是我遇到的问题，我的场景是新闻类短视频类信息流。想到的解决方案是，把标题和描述信息喂进rnn得到embedding，训练的话，拿历史item2vec得到的embedding算cosine similarity，然后pairwise的训练。\n\n不过是否靠谱我不知道，没机会完成并上线，因为种种原因没招辞职跑路了😓","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1609915642,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1948686,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqaR85x4Q7sTApbG845yscBAnW7451VJamXe00icuiavc5WWYDYlgmUjTmjV73xDicqmEpPB1nrTyORg/132","nickname":"Geek266","note":"","ucode":"DC41321653627B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":376359,"discussion_content":"EGES","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622094511,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2405529,"avatar":"https://static001.geekbang.org/account/avatar/00/24/b4/99/79a21147.jpg","nickname":"轩","note":"","ucode":"454CC1A0734A4C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340146,"discussion_content":"哦，还有一个思路：\n同样是基于标题和描述信息的，同样是训练一个rnn吃标题和描述信息，但是通过历史5个点击去预测下一次的点击，作为一个整体end2end去预测。训练完成之后将rnn拿出来，吃冷启动item的信息得到冷启动item的embedding。\n\n同样不知道思路是否靠谱，这个是我离职之后想到的…","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609916208,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2199357,"avatar":"https://static001.geekbang.org/account/avatar/00/21/8f/3d/0c2f97b0.jpg","nickname":"蓝蓝蓝","note":"","ucode":"C2B9E08C8C9871","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":320083,"discussion_content":"新物品和旧物品有啥区别，新物品难道不能像旧物品一样embedding吗？为啥会涉及到冷启动呢？🙏","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604238664,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2202320,"avatar":"https://static001.geekbang.org/account/avatar/00/21/9a/d0/b54a96f2.jpg","nickname":"因子分解机","note":"","ucode":"B605322D225B7F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2199357,"avatar":"https://static001.geekbang.org/account/avatar/00/21/8f/3d/0c2f97b0.jpg","nickname":"蓝蓝蓝","note":"","ucode":"C2B9E08C8C9871","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":323300,"discussion_content":"新物品没有交互记录，没法训练，所以涉及冷启动，我们可以利用新物品的各种已有信息比如类别等取出一个已经训练好的物品embedding集合做平均来作为新物品的初始化向量","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1604913045,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":320083,"ip_address":""},"score":323300,"extra":""}]}]},{"had_liked":false,"id":253526,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1602764859,"is_pvip":false,"replies":[{"id":"92637","content":"非常好的文章，很全面了，推荐大家学习。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602826859,"ip_address":"","comment_id":253526,"utype":1}],"discussion_count":2,"race_medal":0,"score":"100387012667","product_id":100060801,"comment_content":"计算向量间相似度的常用方法：https:&#47;&#47;cloud.tencent.com&#47;developer&#47;article&#47;1668762","like_count":24,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507097,"discussion_content":"非常好的文章，很全面了，推荐大家学习。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602826859,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1258462,"avatar":"https://static001.geekbang.org/account/avatar/00/13/33/de/31c77fdc.jpg","nickname":"yonghao","note":"","ucode":"AE987A02AE2833","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":353231,"discussion_content":"cy","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615087009,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":255754,"user_name":"Geek_3c29c3","can_delete":false,"product_type":"c1","uid":2203358,"ip_address":"","ucode":"3D2E73AB1D08FA","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLaoiaerNMy7eoSA5yfibPNhta51jkhPTTL1dD1HGlnjaGnFQ6Uzbbce82Kpnic3g1JlD7rtm41Y83PA/132","comment_is_top":false,"comment_ctime":1603422274,"is_pvip":false,"replies":[{"id":"93212","content":"1. 一般用前者<br>2.业界一般直接用线上AB test的结果来衡量embedding召回的优劣。离线指标可以做参考。如果做离线测试，一般采用时间划分，要避免引入未来信息。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603493566,"ip_address":"","comment_id":255754,"utype":1}],"discussion_count":1,"race_medal":0,"score":"53143029826","product_id":100060801,"comment_content":"老师，想问一下，业界利用embedding召回时：<br>1、是用用户embedding与item embedding的相似性召回还是先计算用户之间的相似性TOPN，然后生成一个user-item矩阵，看看最相似的用户买的多的item得分就更高？；<br>2、业界用embedding召回如何评价优劣？数据集会划分训练集和验证集吗，来验证购买率，召回率等指标；如果划分，是按照时间划分还是按照用户来划分啊？","like_count":12,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507828,"discussion_content":"1. 一般用前者\n2.业界一般直接用线上AB test的结果来衡量embedding召回的优劣。离线指标可以做参考。如果做离线测试，一般采用时间划分，要避免引入未来信息。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1603493566,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253187,"user_name":"杜军","can_delete":false,"product_type":"c1","uid":1147142,"ip_address":"","ucode":"878C542E1D17AD","user_header":"https://static001.geekbang.org/account/avatar/00/11/81/06/d7c6d511.jpg","comment_is_top":false,"comment_ctime":1602644959,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"53142252511","product_id":100060801,"comment_content":"期待王老师的 Graph Embedding 讲解，能否配合一个 notebook 的示例就更完美了","like_count":13},{"had_liked":false,"id":274970,"user_name":"夜枭","can_delete":false,"product_type":"c1","uid":2414668,"ip_address":"","ucode":"7A09EC9E003379","user_header":"","comment_is_top":false,"comment_ctime":1611238408,"is_pvip":false,"replies":[{"id":"99766","content":"这是个好问题。但我觉得你自己思考一下就应该清楚这个问题根本没有什么标准答案。<br><br>简单说一下自己的经验，确实embedding相关的模型训练起来都是比较慢的，所以item达到上百万之后，天级别更新是很多公司的选择。<br><br>至于如何提高更新频率。这当然是我们希望的，那就得做一些取舍了。做数据的sample？降低embedding的复杂度？或者对于新的物品想起一些规则性的手段生成embedding？只要是你自己想到的方法，都可以尝试。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611279970,"ip_address":"","comment_id":274970,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35970976776","product_id":100060801,"comment_content":"王老师，关于item2vec有一些业务上的疑问，比如用户的点击item序列，这个item的候选集大概得是一个什么规模才能够线上推荐使用呢，目前在做的item量级比较大的话利用spark处理时耗时也会时间长，导致召回的文章并不能很快的更新，几乎是天级别的，不知道您做业务时是怎么权衡更新的频率和数据量这样一个关系的","like_count":8,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514176,"discussion_content":"这是个好问题。但我觉得你自己思考一下就应该清楚这个问题根本没有什么标准答案。\n\n简单说一下自己的经验，确实embedding相关的模型训练起来都是比较慢的，所以item达到上百万之后，天级别更新是很多公司的选择。\n\n至于如何提高更新频率。这当然是我们希望的，那就得做一些取舍了。做数据的sample？降低embedding的复杂度？或者对于新的物品想起一些规则性的手段生成embedding？只要是你自己想到的方法，都可以尝试。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611279970,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265472,"user_name":"Vinny","can_delete":false,"product_type":"c1","uid":1676038,"ip_address":"","ucode":"4341AFB464775D","user_header":"https://static001.geekbang.org/account/avatar/00/19/93/06/0cb4257e.jpg","comment_is_top":false,"comment_ctime":1606907532,"is_pvip":false,"replies":[{"id":"96516","content":"用hash 让多个id对应一个embdding之类的（但是没有解释这么做的合理性）其实没有任何合理性，就是单纯为了减少问题规模。<br><br>这是一个工程问题，应该不会有论文专门探讨这个问题。本质上是一个权衡利弊的过程，你想损失一些精度，就做一些截断和丢弃的处理，你想尽量为更多user生成embedding，不在乎训练时间和数据库空间，就保留他们。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606940080,"ip_address":"","comment_id":265472,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35966645900","product_id":100060801,"comment_content":"老师你好，想请教您个可能与这张内容关联不太大的问题，我是搞nlp的，但是之前在知乎上面看推荐的 user embedding lookup表问题。<br>像 user id 可能有很多 上千万个，lookup 表的维度就会特别大，而且一些长尾的id 出现交互的次数过少，可能学不到什么好的embedding。<br>那么工业界是怎么解决这个问题的呢？<br>之前在知乎上面看一些笼统的说法，比如用hash 让多个id对应一个embdding之类的（但是没有解释这么做的合理性），想请教一下这方面有没有什么推荐的好论文，想去研究下<br>谢谢！","like_count":9,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510944,"discussion_content":"用hash 让多个id对应一个embdding之类的（但是没有解释这么做的合理性）其实没有任何合理性，就是单纯为了减少问题规模。\n\n这是一个工程问题，应该不会有论文专门探讨这个问题。本质上是一个权衡利弊的过程，你想损失一些精度，就做一些截断和丢弃的处理，你想尽量为更多user生成embedding，不在乎训练时间和数据库空间，就保留他们。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606940080,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253524,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1602763715,"is_pvip":false,"replies":[{"id":"92646","content":"第一个问题非常好，我想你能够好好想一下梯度反向传播的过程，再回头看看这个问题。我相信你应该能得出自己的答案。<br><br>第二个也非常好，在训练的时候，确实要把最终的label做归一化，比如这样[0, 0.5, 0, 0.5, 0]。这是训练多分类模型的标准做法。另一种做法是把上一个样本拆成两个独立的onehot样本[0, 1, 0, 0, 0] 和 [0, 0, 0, 1, 0]，这样训练也可以，就不存在你说的信息丢失的问题。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602827972,"ip_address":"","comment_id":253524,"utype":1}],"discussion_count":7,"race_medal":0,"score":"35962502083","product_id":100060801,"comment_content":"老师，有两个问题想请教一下：<br>1.为什么深度学习的结构特点不利于稀疏特征向量的处理呢？<br>2.既然输出向量是Multi-hot，那用softmax这种激活函数是否不太好呢？Softmax有输出相加和为一的限制，对于一对多的任务是不是不太合适呢？","like_count":9,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507095,"discussion_content":"第一个问题非常好，我想你能够好好想一下梯度反向传播的过程，再回头看看这个问题。我相信你应该能得出自己的答案。\n\n第二个也非常好，在训练的时候，确实要把最终的label做归一化，比如这样[0, 0.5, 0, 0.5, 0]。这是训练多分类模型的标准做法。另一种做法是把上一个样本拆成两个独立的onehot样本[0, 1, 0, 0, 0] 和 [0, 0, 0, 1, 0]，这样训练也可以，就不存在你说的信息丢失的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602827972,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2208459,"avatar":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","nickname":"张弛 Conor","note":"","ucode":"193EBA4A64BAB3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":312882,"discussion_content":"问题1：受老师启发，这个问题是因为深度学习通过反向传播算法来更新参数的值，损失函数对参数w的梯度与其对应的a有关，对于稀疏向量而言，会有大量的a为0，根据链式法则，那么相应的权重w的更新梯度也会为0，从而出现梯度消失的问题。","likes_number":13,"is_delete":false,"is_hidden":false,"ctime":1602843209,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2208459,"avatar":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","nickname":"张弛 Conor","note":"","ucode":"193EBA4A64BAB3","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":314990,"discussion_content":"非常非常好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603220513,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":312882,"ip_address":""},"score":314990,"extra":""}]},{"author":{"id":2288623,"avatar":"","nickname":"Geek_1aa52e","note":"","ucode":"7015D0CB6E4B8C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":343068,"discussion_content":"深度学习不能处理稀疏OneHot编码，因为除了输入为1的神经元，其他神经元的连接权重会出现梯度消失问题，无法更新权重，而输入为1的样本数量很少 ，所以要采用embding避免梯度消失问题，充分利用所有输入数据；另一方面这也验证了Word2Vec为什么可以选择输入层与隐含层之间连接权重的某一行作为某个词emb表示，因为这个词的输入为1，只有这个词为这一行贡献了梯度，其他位置为零，梯度均消失。","likes_number":10,"is_delete":false,"is_hidden":false,"ctime":1610937128,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1064358,"avatar":"https://static001.geekbang.org/account/avatar/00/10/3d/a6/fa638f6d.jpg","nickname":"GB","note":"","ucode":"8732315FCB6CC7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2288623,"avatar":"","nickname":"Geek_1aa52e","note":"","ucode":"7015D0CB6E4B8C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":381740,"discussion_content":"没太理解，意思是输入为0，反向传回来的梯度就是0？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625196430,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":343068,"ip_address":""},"score":381740,"extra":""},{"author":{"id":1489525,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI9X140JXPuaDB8PibXpwFWds6mZvg1w7THkyB6NjBkP7x4HqSk2wuUvcmDb9O2l0fCkxvB3ibL0L2A/132","nickname":"科学养牛","note":"","ucode":"B205209A814AC8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1064358,"avatar":"https://static001.geekbang.org/account/avatar/00/10/3d/a6/fa638f6d.jpg","nickname":"GB","note":"","ucode":"8732315FCB6CC7","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399191,"discussion_content":"在算子是乘法的时候，确实是这样的，不信你推一下矩阵乘法的反向传播就清楚了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632919557,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":381740,"ip_address":""},"score":399191,"extra":""}]},{"author":{"id":2403705,"avatar":"","nickname":"Wa","note":"","ucode":"96E28E50FCA96C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340337,"discussion_content":"想追问一下第二点，multi hot应该不算多分类，而是多标签分类吧，所以softmax的确似乎不太妥","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1609982236,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253338,"user_name":"Dikiwi","can_delete":false,"product_type":"c1","uid":2198644,"ip_address":"","ucode":"F6E4C2008C7F7B","user_header":"https://static001.geekbang.org/account/avatar/00/21/8c/74/2bbd132d.jpg","comment_is_top":false,"comment_ctime":1602688010,"is_pvip":false,"replies":[{"id":"92643","content":"赞，ANN的方法也是后续的课程内容。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602827331,"ip_address":"","comment_id":253338,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35962426378","product_id":100060801,"comment_content":"相似性一般用欧式距离，cosine距离; <br>线上快速召回一般有用ANN，比如LSH算法进行近似召回。","like_count":8,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507052,"discussion_content":"赞，ANN的方法也是后续的课程内容。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602827331,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":273109,"user_name":"阳光明媚","can_delete":false,"product_type":"c1","uid":2394856,"ip_address":"","ucode":"38475D9DB40BC1","user_header":"https://static001.geekbang.org/account/avatar/00/24/8a/e8/a7ff6230.jpg","comment_is_top":false,"comment_ctime":1610441866,"is_pvip":false,"replies":[{"id":"98994","content":"很好","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1610503053,"ip_address":"","comment_id":273109,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31675212938","product_id":100060801,"comment_content":"常用的相似度度量有余弦距离和欧氏距离，余弦距离可以体现数值的相对差异，欧式距离体现数值的绝对差异；例如，衡量用户点击次数的相似度，欧氏距离更好，衡量用户对各类电影的喜好的相似度，用余弦距离更好。","like_count":7,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513495,"discussion_content":"很好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610503053,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":257284,"user_name":"浣熊当家","can_delete":false,"product_type":"c1","uid":1952248,"ip_address":"","ucode":"939F06050423E4","user_header":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","comment_is_top":false,"comment_ctime":1603907650,"is_pvip":false,"replies":[{"id":"93663","content":"从直观上可以这么理解。但是显然Embedding的优化空间要大得多，因为我们可以使用任何复杂网络生成这个embedding，所以embedding本身可以包含非常多的信息，不仅仅是降维这么简单的目的。<br><br>当然，降维肯定是Embedding的一个很重要的目的和理解的方式，但要清楚更多的embedding手段，灵活运用。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603930341,"ip_address":"","comment_id":257284,"utype":1}],"discussion_count":3,"race_medal":0,"score":"31668678722","product_id":100060801,"comment_content":"请问下老师，同样作为降维的手段，embedding和PCA的本质区别，可不可以理解为， embedding可以得到更为稠密的向量，并且降维后的各维度没有主次排序之分， 而PCA降维后的向量稠密度并不会增加， 而是得到主成分的排序。 所以如果为了提高模型计算的聚合的速度，就要选择embedding， 如果想降低模型的复杂度和防止过拟合，应该选择PCA。","like_count":7,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":508283,"discussion_content":"从直观上可以这么理解。但是显然Embedding的优化空间要大得多，因为我们可以使用任何复杂网络生成这个embedding，所以embedding本身可以包含非常多的信息，不仅仅是降维这么简单的目的。\n\n当然，降维肯定是Embedding的一个很重要的目的和理解的方式，但要清楚更多的embedding手段，灵活运用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603930341,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1067846,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4b/46/717d5cb9.jpg","nickname":"惜心（伟祺）","note":"","ucode":"393DF1A9E81332","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":327583,"discussion_content":"不是简单降维\npca和emb都是在构造联结矩阵\n差别在于连接矩阵原生 和每个格子里填的值\npca元素是个item全局粒度 格子里的值是全局出现次数有无关联\nemb 元素更多样邻居n词或者属性 格子里值可以是有无关系 几度关系 比较多样 扩展了维度","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1605866617,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1952248,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","nickname":"浣熊当家","note":"","ucode":"939F06050423E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1067846,"avatar":"https://static001.geekbang.org/account/avatar/00/10/4b/46/717d5cb9.jpg","nickname":"惜心（伟祺）","note":"","ucode":"393DF1A9E81332","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":328876,"discussion_content":"谢谢解答！我不太懂什么是粒度，有没有什么好的资料推荐可以学习？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606266363,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":327583,"ip_address":""},"score":328876,"extra":""}]}]},{"had_liked":false,"id":282429,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1615250749,"is_pvip":false,"replies":[{"id":"102596","content":"非常好的总结，推荐其他同学参考","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615336994,"ip_address":"","comment_id":282429,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23090087229","product_id":100060801,"comment_content":"业界常用 余弦相似度方式，文本相似度TD-IDF方式，<br>想了解更多：https:&#47;&#47;cloud.tencent.com&#47;developer&#47;article&#47;1668762<br><br>对word2vec的学习<br>Word2Vec是词嵌入模型之一，它是一种浅层的神经网络模型，它有两种网络结构：CBOW和Skip-gram。<br>CBOW：根据上下文出现的词语预测当前词的生成概率；<br>Skip-gram：根据当前词预测上下文各个词的概率。<br>注意：在CBOW中，还需要将各个输入词所计算出的隐含单元求和。<br>Softmax激活函数<br>输出层使用Softmax激活函数，可以计算出每个单词的生成概率。具体公式f(x) = e^x &#47; sum( e^x_i )，它运用的是极大似然。<br>由于Softmax激活函数种存在归一化项的缘故，从公式可以看出，需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，时间复杂度为O(V)，由此产生了Hierarchical Softmax（层次Softmax） 和 Negative Sampling（负采样）两种改进方法。<br>Negative Sampling（负采样）：<br>每次训练时，需要预测的目标词给分成2类。一类是目标词，另一类是非目标词（个数可人工指定）（负采样而来，采样词频高的词，TensorFlow里面是这样，与原论文不同）。此时就是个二分类问题，Loss就变了。变成了Sigmod的形式。这样在前向传播不用计算Softmax，反向传播时将每次更新的向量变成了很少的几个，而不是原始的V，降低开销。（详细可见评论区）<br>Hierarchical Softmax（分层Softmax）：<br>它不需要去遍历所有的节点信息，时间复杂度变为O(log(V))。<br>基本原理：根据标签（label）和频率建立霍夫曼树；（label出现的频率越高，Huffman树的路径越短）；Huffman树中每一叶子结点代表一个label；<br>参考：https:&#47;&#47;blog.csdn.net&#47;qq_35290785&#47;article&#47;details&#47;96706599<br>embedding和PCA的本质区别：<br>引用老师的留言：我觉得理解非常正确，embedding虽然有降维的目的，但初衷是希望在最终的vector中保存物品相关性的信息，甚至依次进行运算。而PCA就是单纯找出主成分进行降维，二者的设计动机差别是比较大的。","like_count":6,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516743,"discussion_content":"非常好的总结，推荐其他同学参考","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615336994,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":355389,"discussion_content":"感谢老师的鼓励^_^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615426220,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":281753,"user_name":"Jay Kay","can_delete":false,"product_type":"c1","uid":1165258,"ip_address":"","ucode":"582F2613E6C1AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/c7/ca/6c4010cc.jpg","comment_is_top":false,"comment_ctime":1614870203,"is_pvip":false,"replies":[{"id":"102389","content":"我觉得你理解的非常正确，其实就是一种共现性。本质上甚至和协同过滤有些相像。<br><br>语义这个事情其实不好讲，对于一个物品什么是语义呢？所以在NLP之外，我们也很少提这个概念。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615024669,"ip_address":"","comment_id":281753,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23089706683","product_id":100060801,"comment_content":"老师有个问题，item2vec所谓的相似性是拟合的一种隐式的共现么？毕竟感觉通过行为序列得到的embedding没有办法反应语义上的相似性。不知道这样的理解对不对。","like_count":5,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516495,"discussion_content":"我觉得你理解的非常正确，其实就是一种共现性。本质上甚至和协同过滤有些相像。\n\n语义这个事情其实不好讲，对于一个物品什么是语义呢？所以在NLP之外，我们也很少提这个概念。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615024669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":255302,"user_name":"大龄小学生","can_delete":false,"product_type":"c1","uid":1057111,"ip_address":"","ucode":"AA7860B88FB0F1","user_header":"https://static001.geekbang.org/account/avatar/00/10/21/57/ee02ef41.jpg","comment_is_top":false,"comment_ctime":1603297720,"is_pvip":false,"replies":[{"id":"93036","content":"本质上embedding就是降维的一种方式。比如经典的降维方法PCA，其实也就是用主成分分量来表示一个高维向量。<br><br>但是现在embedding的使用方式灵活多了，生成embedding的神经网络可以非常灵活，所以深度学习里面还是embedding用的更广泛一些。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603347103,"ip_address":"","comment_id":255302,"utype":1}],"discussion_count":3,"race_medal":0,"score":"23078134200","product_id":100060801,"comment_content":"老师，维度过高为什么不用降维，而用embedding","like_count":6,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507678,"discussion_content":"本质上embedding就是降维的一种方式。比如经典的降维方法PCA，其实也就是用主成分分量来表示一个高维向量。\n\n但是现在embedding的使用方式灵活多了，生成embedding的神经网络可以非常灵活，所以深度学习里面还是embedding用的更广泛一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603347103,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1952248,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","nickname":"浣熊当家","note":"","ucode":"939F06050423E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":318745,"discussion_content":"也想请问老师， embedding和PCA， t-SNE都是降维，他们最本质的区别是什么？ 我理解是PCA得出的维度有主次的关系， 而embedding的得到的词向量不存在主次，而是强调所以最终维度加起来整体的还原有词向量的信息。不知道理解的对不对。或者是说embedding属于深度学习，PCA属于传统线性模型。恳求王老师的解答！","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1603842866,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1952248,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","nickname":"浣熊当家","note":"","ucode":"939F06050423E4","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":327683,"discussion_content":"我觉得理解非常正确，embedding虽然有降维的目的，但初衷是希望在最终的vector中保存物品相关性的信息，甚至依次进行运算。而PCA就是单纯找出主成分进行降维，二者的设计动机差别是比较大的。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1605913351,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":318745,"ip_address":""},"score":327683,"extra":""}]}]},{"had_liked":false,"id":264022,"user_name":"AstrHan","can_delete":false,"product_type":"c1","uid":1944884,"ip_address":"","ucode":"14C5F3323A472D","user_header":"","comment_is_top":false,"comment_ctime":1606313259,"is_pvip":false,"replies":[{"id":"95798","content":"两个方法，一是item embedding生成好后，在利用item embedding生成user embedding。二是使用矩阵分解这类方法生成item和user embedding。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606376128,"ip_address":"","comment_id":264022,"utype":1}],"discussion_count":4,"race_medal":0,"score":"18786182443","product_id":100060801,"comment_content":"物品embedding和用户embedding如何一起训练，让他们在同一个空间里呢，我理解这样才可以进行物品和用户的相似度计算。","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510389,"discussion_content":"两个方法，一是item embedding生成好后，在利用item embedding生成user embedding。二是使用矩阵分解这类方法生成item和user embedding。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606376128,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1371804,"avatar":"https://static001.geekbang.org/account/avatar/00/14/ee/9c/abb7bfe3.jpg","nickname":"abc-web","note":"","ucode":"DE3B873863EFF9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533257,"discussion_content":"老师请问一下，itemId用的是雪花算法产生的id长整型怎么来做embedding；（数字太大，hash分桶造成内存溢出），谢谢老师！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637825951,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2656211,"avatar":"","nickname":"Geek_742481","note":"","ucode":"AB33CEC5C25A46","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":386394,"discussion_content":"有一个小问题，最终生成的item和user embedding的维度需要是相同的吗？只有相同，后面才可能去计算相似度？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627561864,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2532699,"avatar":"","nickname":"Geek_fdb832","note":"","ucode":"0BA1479D33DC07","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369546,"discussion_content":"请问item embedding是会比user embedding更容易生成吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619074465,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263549,"user_name":"follow-fate","can_delete":false,"product_type":"c1","uid":2302995,"ip_address":"","ucode":"EF5C9BA5A77979","user_header":"https://static001.geekbang.org/account/avatar/00/23/24/13/8fb0424b.jpg","comment_is_top":false,"comment_ctime":1606182201,"is_pvip":false,"replies":[{"id":"95683","content":"embedding本质上是一个无监督学习的方法，所以不需要所谓的标签。对于item2vec方法，我们只是希望embedding能够学习到序列中靠近物品的相似性，所以物品的序列本身就是全部的训练样本，不需要再输入label。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606246389,"ip_address":"","comment_id":263549,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18786051385","product_id":100060801,"comment_content":"老是您好，在对物品序列做embeding的时候，物品序列对应的标签序列是什么呢？word2vec预测的输出,skip-gram是根据中心词预测周边词，那物品序列做embeding时也是预测序列中的某个特征吗？","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510233,"discussion_content":"embedding本质上是一个无监督学习的方法，所以不需要所谓的标签。对于item2vec方法，我们只是希望embedding能够学习到序列中靠近物品的相似性，所以物品的序列本身就是全部的训练样本，不需要再输入label。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606246389,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":254823,"user_name":"wanlong","can_delete":false,"product_type":"c1","uid":1668387,"ip_address":"","ucode":"B7E67E406D5D88","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/plh3lengbMHg0qdGHxX3lJzKaJhh0BAXOqBe1mdmoBVk00NsQfC7HNlhTPRuvlXNDblibcwK3fPZjNicRO73Oibtg/132","comment_is_top":false,"comment_ctime":1603194727,"is_pvip":false,"replies":[{"id":"92938","content":"是的，就是这两个方向，理论细节和实践经验。<br><br>特别是实践经验，在数据量大了之后，会遇到很多工程上的问题，需要不断的积累。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603220183,"ip_address":"","comment_id":254823,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18783063911","product_id":100060801,"comment_content":"老师您好，非常棒的文章，请教一个问题：<br>如果要往更深部分的学习，除了朝您说的算法细节，还有对应各个环节实现遇到的工程挑战，并设计相对的优化方向，这样算一个靠谱的掌握了吧？","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507526,"discussion_content":"是的，就是这两个方向，理论细节和实践经验。\n\n特别是实践经验，在数据量大了之后，会遇到很多工程上的问题，需要不断的积累。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603220183,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282050,"user_name":"时不时充充电","can_delete":false,"product_type":"c1","uid":2453902,"ip_address":"","ucode":"F45712325DAE50","user_header":"https://static001.geekbang.org/account/avatar/00/25/71/8e/31458837.jpg","comment_is_top":false,"comment_ctime":1615032766,"is_pvip":false,"replies":[{"id":"102458","content":"可以从14课中看到我的一些思路。总的来说没有统一的方法，最好是能够结合应用场景进行一些效果测试。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615126636,"ip_address":"","comment_id":282050,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14499934654","product_id":100060801,"comment_content":"老师，想问个问题，工程实践中，如何评价一个embedding向量的好坏？有没有统一的方法论指导？","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516598,"discussion_content":"可以从14课中看到我的一些思路。总的来说没有统一的方法，最好是能够结合应用场景进行一些效果测试。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615126636,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280778,"user_name":"Geek_1d26c5","can_delete":false,"product_type":"c1","uid":2451171,"ip_address":"","ucode":"45B62C1BA3145D","user_header":"","comment_is_top":false,"comment_ctime":1614365577,"is_pvip":false,"replies":[{"id":"101936","content":"停用词是最好要跳过的，因为本身不包含具体语义，学出相似度意义也不大。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1614390359,"ip_address":"","comment_id":280778,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14499267465","product_id":100060801,"comment_content":"老师您好，我注意到word2vec模型在生成训练样本的时候跳过了&quot;的&quot;和&quot;了&quot;这样的停用词。请问这样做的原因是因为停用词和附近单词的语义关系不大吗?","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516204,"discussion_content":"停用词是最好要跳过的，因为本身不包含具体语义，学出相似度意义也不大。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614390359,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272224,"user_name":"轩","can_delete":false,"product_type":"c1","uid":2405529,"ip_address":"","ucode":"454CC1A0734A4C","user_header":"https://static001.geekbang.org/account/avatar/00/24/b4/99/79a21147.jpg","comment_is_top":false,"comment_ctime":1609994460,"is_pvip":false,"replies":[{"id":"98737","content":"这个做法当然是可行的，就是基于新闻内容的embedding生成，nlp里面很多应用。<br><br>但很多实践都证明仅基于内容进行推荐，精确度往往比较低，用于冷启动是可以的，但在新闻出来以后，要尽快想办法把interaction信息融入到模型里面。<br><br>不用管业界有没有做过什么的，你管别人干嘛，自己去试，你自己的问题，有谁比你更了解。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1610046735,"ip_address":"","comment_id":272224,"utype":1}],"discussion_count":3,"race_medal":0,"score":"14494896348","product_id":100060801,"comment_content":"老师您好，昨天的提出的一些问题都得到了您的回复，非常感谢。<br>关于冷启动物品的embedding，我有一些思路，不知是否可行？可业界中也没看到类似做法？<br><br>具体而言，拿冷启动物品的标题描述等文字信息喂进rnn吐出embedding，训练好这个rnn之后，就可以冷启动物品入库之时，通过模型获得embedding了，不依赖于用户行为。<br>至于论据，搜索等都是基于标签标题描述所做，这些文字信息是人类语言关于物品的编码，转换成embedding编码也是可行的？<br>训练的话，end2end训练，拿点击序列去预测下一个点击。或是拿有充分历史行为的物品训练，预测cosine similarity，应该都可以train这个rnn吧？<br><br>之前我负责的业务场景是新闻视频类信息流推荐，内容都很新，大量的冷启动，这是一直以来的一个想法。<br>由于种种原因，短时间内我没有实践机会了，可经过一段时间的学习，发现对于冷启动好像业界并没有这样做？或是这个想法有些问题？","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513204,"discussion_content":"这个做法当然是可行的，就是基于新闻内容的embedding生成，nlp里面很多应用。\n\n但很多实践都证明仅基于内容进行推荐，精确度往往比较低，用于冷启动是可以的，但在新闻出来以后，要尽快想办法把interaction信息融入到模型里面。\n\n不用管业界有没有做过什么的，你管别人干嘛，自己去试，你自己的问题，有谁比你更了解。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610046735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1489525,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI9X140JXPuaDB8PibXpwFWds6mZvg1w7THkyB6NjBkP7x4HqSk2wuUvcmDb9O2l0fCkxvB3ibL0L2A/132","nickname":"科学养牛","note":"","ucode":"B205209A814AC8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399194,"discussion_content":"其实也可以考虑直接用预训练语言模型的embedding，而不是从自己的数据从头开始训练","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632919991,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1162286,"avatar":"https://static001.geekbang.org/account/avatar/00/11/bc/2e/08079c3c.jpg","nickname":"Simon","note":"","ucode":"80146B97EB5CDE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":351872,"discussion_content":"冷启动物品没有点击序列可以用来训练。\n我理解老师的意思是由描述item的词的embedding融合得到item的embedding","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614507649,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":254636,"user_name":"小眼无神霹雳霹雳猪","can_delete":false,"product_type":"c1","uid":1653145,"ip_address":"","ucode":"99D405803DA169","user_header":"https://static001.geekbang.org/account/avatar/00/19/39/99/d6f45cc6.jpg","comment_is_top":false,"comment_ctime":1603161013,"is_pvip":false,"replies":[{"id":"92937","content":"是的，V如果变化的话必须重新训练。除非采取其他item emb冷启动的方法。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603220113,"ip_address":"","comment_id":254636,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14488062901","product_id":100060801,"comment_content":"老师你好！我想不太明白的地方是word2vec的输入层维度是词汇库的大小V。word2vec里词汇库的大小V是不怎么会变的。但如果是经常会加新物品进来的item2vec V会动态的变大。item2vec要随着V变大不断变动神经网络的结构并重新训练么？","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507466,"discussion_content":"是的，V如果变化的话必须重新训练。除非采取其他item emb冷启动的方法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603220113,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253300,"user_name":"傻","can_delete":false,"product_type":"c1","uid":1136426,"ip_address":"","ucode":"099D3EFDAF12EF","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Y5U2ADUvruWhziaB4tSyiaAN7h9OcHMGj6X6nAeqJyJvrqWs8JmyO6yOTBziatAEIG6gHRic0jvT3d0hxNhiaAUVYkw/132","comment_is_top":false,"comment_ctime":1602678573,"is_pvip":false,"replies":[{"id":"92642","content":"不满足，embedding需要离线训练。除非是通过一些average pooling，sum pooling的方法在线处理，否则不可能实时生成。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602827296,"ip_address":"","comment_id":253300,"utype":1}],"discussion_count":5,"race_medal":0,"score":"14487580461","product_id":100060801,"comment_content":"如果是线上用到的实时特征，重新计算embedding的话，响应时间是否能满足要求呢？","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507041,"discussion_content":"不满足，embedding需要离线训练。除非是通过一些average pooling，sum pooling的方法在线处理，否则不可能实时生成。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602827296,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1136426,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Y5U2ADUvruWhziaB4tSyiaAN7h9OcHMGj6X6nAeqJyJvrqWs8JmyO6yOTBziatAEIG6gHRic0jvT3d0hxNhiaAUVYkw/132","nickname":"傻","note":"","ucode":"099D3EFDAF12EF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":312846,"discussion_content":"理解了，谢谢王喆老师～\n可以这么理解嘛，就是这种embedding的特征一般是用来生成离线特征的对吧～","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1602832097,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1136426,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Y5U2ADUvruWhziaB4tSyiaAN7h9OcHMGj6X6nAeqJyJvrqWs8JmyO6yOTBziatAEIG6gHRic0jvT3d0hxNhiaAUVYkw/132","nickname":"傻","note":"","ucode":"099D3EFDAF12EF","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":314992,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603220561,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":312846,"ip_address":""},"score":314992,"extra":""}]},{"author":{"id":1809935,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/l3RGUX8aLnPLmsQsra0yU5d8m7Se5jdVpaC3bkb99FuY11BPQNAsH4MPXbZjCTia9VVwn8lnBnKLkdfSiabOgxKg/132","nickname":"Geek_e0d66a","note":"","ucode":"5078900C9CF936","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":313095,"discussion_content":"离线重新训练了模型，那同一个物品训练的embedding会保持一致吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602939985,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1809935,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/l3RGUX8aLnPLmsQsra0yU5d8m7Se5jdVpaC3bkb99FuY11BPQNAsH4MPXbZjCTia9VVwn8lnBnKLkdfSiabOgxKg/132","nickname":"Geek_e0d66a","note":"","ucode":"5078900C9CF936","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":314991,"discussion_content":"不会保持一致。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1603220544,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":313095,"ip_address":""},"score":314991,"extra":""}]}]},{"had_liked":false,"id":274242,"user_name":"Soulmate","can_delete":false,"product_type":"c1","uid":1197216,"ip_address":"","ucode":"DD3A47C1B35827","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI0LeXiaibibS5ENlib4ibibDUZ1DDPqTojcvlmdhCqdZEiareLw3SJzPcVPN1LPqr13WrZ9REG1tZSEQcIQ/132","comment_is_top":false,"comment_ctime":1610937327,"is_pvip":false,"replies":[{"id":"99565","content":"只要是ratings数据里面包含的item，都应该存在embedding，只是稳定不稳定的问题。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1610947515,"ip_address":"","comment_id":274242,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10200871919","product_id":100060801,"comment_content":"老是您好，基于 word2vec 跑出来物品向量太少，导致召回率低，会是哪些原因造成的，基础的ratings数据在1600W左右。","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513950,"discussion_content":"只要是ratings数据里面包含的item，都应该存在embedding，只是稳定不稳定的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610947515,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":267117,"user_name":"At sei ni","can_delete":false,"product_type":"c1","uid":2302928,"ip_address":"","ucode":"F6C82FB028618E","user_header":"https://static001.geekbang.org/account/avatar/00/23/23/d0/b72a900b.jpg","comment_is_top":false,"comment_ctime":1607601069,"is_pvip":false,"replies":[{"id":"96980","content":"自己理解了autoencoder和word2vec，就知道答案了。问我也是让我重新解释一遍autoencoder和word2vec，没有意义。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1607649672,"ip_address":"","comment_id":267117,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10197535661","product_id":100060801,"comment_content":"老师您好，我看了一些论文，有很多论文是通过autoencoder来extract每一个user和item的特征的，这跟您本节将的word2vec等方法有何优劣或者不同？","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511512,"discussion_content":"自己理解了autoencoder和word2vec，就知道答案了。问我也是让我重新解释一遍autoencoder和word2vec，没有意义。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607649672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253415,"user_name":"WiFeng","can_delete":false,"product_type":"c1","uid":1043466,"ip_address":"","ucode":"D7F0FD3EC815B4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/0a/c35f9000.jpg","comment_is_top":false,"comment_ctime":1602732076,"is_pvip":false,"replies":[{"id":"92638","content":"关注图中橙色的部分，Embedding Matrix","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602826987,"ip_address":"","comment_id":253415,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10192666668","product_id":100060801,"comment_content":"其实，它就藏在输入层到隐层的权重矩阵 WVxN 中。我想看了下面的图你一下就明白了。<br><br>这个愣是没明白。","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507065,"discussion_content":"关注图中橙色的部分，Embedding Matrix","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602826987,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2223053,"avatar":"https://static001.geekbang.org/account/avatar/00/21/eb/cd/3f0f9d8b.jpg","nickname":"覔","note":"","ucode":"7519BB45AD992E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":312684,"discussion_content":"这个WVxN的意思是W权重矩阵，下角标是（VxN），也就是输入层和隐藏层的权重矩阵吧！ 然后我们一直在说embedding，那么embedding在哪儿呢？ 如果看一下维度的话，我们输入是一个10000维的词的one-hot编码，那么这里的V就是10000，我们的输入应该是VxV的，那么我们的隐藏层有N个神经元，那么我们的权重矩阵不就是VxN的咯？而我们在python代码里运行torch.nn.Embedding()时候，第一个参数是输入维度，第二个参数是隐藏层维度，所以也就是说 我们习惯取这样的输入和隐藏层之间的权重矩阵为我们的Embedding矩阵。（我也是小白，尝试解答一下，有问题欢迎指正~）","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1602767991,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2223053,"avatar":"https://static001.geekbang.org/account/avatar/00/21/eb/cd/3f0f9d8b.jpg","nickname":"覔","note":"","ucode":"7519BB45AD992E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":317868,"discussion_content":"非常好的补充，谢谢。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603609735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":312684,"ip_address":""},"score":317868,"extra":""}]}]},{"had_liked":false,"id":253211,"user_name":"鱼_XueTr","can_delete":false,"product_type":"c1","uid":1506691,"ip_address":"","ucode":"A164D2540F251D","user_header":"https://static001.geekbang.org/account/avatar/00/16/fd/83/b432b125.jpg","comment_is_top":false,"comment_ctime":1602651399,"is_pvip":false,"replies":[{"id":"92639","content":"很全面了","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1602827053,"ip_address":"","comment_id":253211,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10192585991","product_id":100060801,"comment_content":"1.向量投影，即余弦相似度，2.计算模比较大小，各种距离，比如欧式距离，3.KNN","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507006,"discussion_content":"很全面了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602827053,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287078,"user_name":"Mobs","can_delete":false,"product_type":"c1","uid":2549684,"ip_address":"","ucode":"0AB3789D90F84F","user_header":"https://static001.geekbang.org/account/avatar/00/26/e7/b4/79ea288d.jpg","comment_is_top":false,"comment_ctime":1617765993,"is_pvip":false,"replies":[{"id":"104473","content":"赞","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1618113216,"ip_address":"","comment_id":287078,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5912733289","product_id":100060801,"comment_content":"我自己总结了四大相似性计算公式：1. Cosine vector similarity 2.Pearson correlation coefficient ( PCC )<br>3. Euclidean distance similarity 4.Tanimoto coefficient  (  Jaccard similarity coefficient 雅卡尔指数  ) <br>本人倾向于使用cosine公式进行计算","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518185,"discussion_content":"赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618113216,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282998,"user_name":"Geek_f676f3","can_delete":false,"product_type":"c1","uid":1434591,"ip_address":"","ucode":"E229F71BFA5F96","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ3q0LkadjOv4zicr7xQufgXyh8o1Usno8RZdeBPOqzsoH8DRiaMdYjs0OyEuTknHwHxfQ4AnBHdBCA/132","comment_is_top":false,"comment_ctime":1615514181,"is_pvip":false,"replies":[{"id":"102830","content":"第一次了解到这个开源项目，赞👍","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615793281,"ip_address":"","comment_id":282998,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5910481477","product_id":100060801,"comment_content":"向量计算相似度：https:&#47;&#47;www.milvus.io&#47;cn&#47; 工业界实践","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516904,"discussion_content":"第一次了解到这个开源项目，赞👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615793281,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":281118,"user_name":"Alan","can_delete":false,"product_type":"c1","uid":2115316,"ip_address":"","ucode":"591A28E310A8F5","user_header":"https://static001.geekbang.org/account/avatar/00/20/46/f4/93b1275b.jpg","comment_is_top":false,"comment_ctime":1614587371,"is_pvip":false,"replies":[{"id":"102378","content":"其实并不是所有人都知道，还是欢迎所有人提出疑问，多交流。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615023830,"ip_address":"","comment_id":281118,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5909554667","product_id":100060801,"comment_content":"课后思考：<br>1、其实很多，大家也都知道！业界常用计算Cosine Similarity余弦相似度方式，文本相似度TD-IDF方式","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516299,"discussion_content":"其实并不是所有人都知道，还是欢迎所有人提出疑问，多交流。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615023830,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280564,"user_name":"DUO2.0","can_delete":false,"product_type":"c1","uid":1565172,"ip_address":"","ucode":"B8C1A267F959F1","user_header":"https://static001.geekbang.org/account/avatar/00/17/e1/f4/456752ac.jpg","comment_is_top":false,"comment_ctime":1614247559,"is_pvip":false,"replies":[{"id":"101879","content":"这是一个nlp的问题，中文分词一般有比较成熟的包，直接调用就好。至于能不能把深度学习分在一起，就看分词包自己的能力了。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1614305287,"ip_address":"","comment_id":280564,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5909214855","product_id":100060801,"comment_content":"在你觉得skip gram的中文例子里面，怎么知道“深度学习”是一个词，而不是四个词呢？在英文下 deep learning因为有空格作为自然的separator，我们一般是把deep 跟learning分开处理的。","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516135,"discussion_content":"这是一个nlp的问题，中文分词一般有比较成熟的包，直接调用就好。至于能不能把深度学习分在一起，就看分词包自己的能力了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614305287,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":279986,"user_name":"wangyunyy","can_delete":false,"product_type":"c1","uid":1140229,"ip_address":"","ucode":"5075C436951C64","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK8hCUT5psspwxiaAiarTRZ4icOg4KpzFnw68ib2VqN6F5sra4GnBiaT3aSptvVKQe9sFNhzjCQG1ib1tPQ/132","comment_is_top":false,"comment_ctime":1614048219,"is_pvip":false,"replies":[{"id":"101870","content":"这是一个非常非常好的问题。<br><br>简单回答就是我当时介绍netflix电影的时候，没有那么严谨，就是让大家大概理解这个概念。<br><br>具体的相似度计算当然需要跟你训练模型时候定义的相似度一致，所以你理解的正确的。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1614304505,"ip_address":"","comment_id":279986,"utype":1}],"discussion_count":4,"race_medal":0,"score":"5909015515","product_id":100060801,"comment_content":"老师你好，您在Netflix的电影推荐中提到 ： “具体来说就是，我们直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了”<br>按照这个说法应该是通过用户向量和电影向量的相似度来推荐，但是矩阵分解中最后的推荐排序是通过用户向量和电影向量的内积，这两种方式应该是不一样的，而且用户向量和电影向量应该是没有可比性的，不知道我理解是否正确？谢谢","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515952,"discussion_content":"这是一个非常非常好的问题。\n\n简单回答就是我当时介绍netflix电影的时候，没有那么严谨，就是让大家大概理解这个概念。\n\n具体的相似度计算当然需要跟你训练模型时候定义的相似度一致，所以你理解的正确的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614304505,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1516796,"avatar":"","nickname":"时间小偷","note":"","ucode":"0EBC79DDD822C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":550353,"discussion_content":"老师您好！“具体的相似度计算当然需要跟你训练模型时候定义的相似度一致”，这句话怎么理解？那矩阵分解算是定义的什么相似度？还有就是，用矩阵分解的用户向量，计算用户的相似度，工业里是否也可以这样计算？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644492171,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":515952,"ip_address":""},"score":550353,"extra":""}]},{"author":{"id":1598977,"avatar":"https://static001.geekbang.org/account/avatar/00/18/66/01/48549335.jpg","nickname":"Allén","note":"","ucode":"0ED33442C97A5F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":353196,"discussion_content":"我咋觉得这位同学提到的两种描述并不矛盾呢？因为内积不也是一种相似度度量方式吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615069668,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1877078,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/a4/56/29a05cb2.jpg","nickname":"Kepler","note":"","ucode":"C69A3248FB1AF6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":351272,"discussion_content":"我理解是通过相似用户推荐和通过相似电影推荐，还是通过计算用户向量间和电影向量间的相似度得到的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614223225,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":277287,"user_name":"Sanders","can_delete":false,"product_type":"c1","uid":1697075,"ip_address":"","ucode":"3D460FEEDCDF34","user_header":"","comment_is_top":false,"comment_ctime":1612345557,"is_pvip":false,"replies":[{"id":"100811","content":"我还是不太理解你的问题，我感觉你这里说的跟item2vec的原理不在一个频道上。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1612595833,"ip_address":"","comment_id":277287,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5907312853","product_id":100060801,"comment_content":"在训练Item2Vec时，skip-N-gram的滑动窗口N怎么来选取，用动态N是否会影响Softmax输出？<br>输出label：w(t-2), w(t-1), w(t), w(t+1), w(t+2)的值为什么都是1？可不可以将N选为1，用one-out的方式分别训练邻居N个目标的Embedding，然后concat这几个Embedding？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514986,"discussion_content":"我还是不太理解你的问题，我感觉你这里说的跟item2vec的原理不在一个频道上。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612595833,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":277165,"user_name":"黄佳恒","can_delete":false,"product_type":"c1","uid":2199659,"ip_address":"","ucode":"1E3812DB22CC42","user_header":"https://static001.geekbang.org/account/avatar/00/21/90/6b/24ca47da.jpg","comment_is_top":false,"comment_ctime":1612283811,"is_pvip":false,"replies":[{"id":"100807","content":"我其实不建议问这么细的问题，因为在实际工作中这类问题太多太多，都是需要自己不断尝试和调参，希望你能在实践中得出答案。<br><br>当然，如果是我，我会倾向于把连续重复的item合并掉。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1612595197,"ip_address":"","comment_id":277165,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5907251107","product_id":100060801,"comment_content":"老师，我现在面对的场景是工作台推荐。就是用户每天会点击一批应用，我想猜出用户每次会点击哪些应用。 用户会连续多次点击相同的应用，这样当我训练item2vec的时候，这个点击序列应该把连续相同的给只保留一个吗？还是都保留着","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514944,"discussion_content":"我其实不建议问这么细的问题，因为在实际工作中这类问题太多太多，都是需要自己不断尝试和调参，希望你能在实践中得出答案。\n\n当然，如果是我，我会倾向于把连续重复的item合并掉。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612595197,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2458257,"avatar":"https://static001.geekbang.org/account/avatar/00/25/82/91/5c49ca76.jpg","nickname":"哄哄","note":"","ucode":"6DA0FE52DA4F84","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":351571,"discussion_content":"这个你用lru缓存策略不好吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614328926,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":274714,"user_name":"叫我大预言家","can_delete":false,"product_type":"c1","uid":2406100,"ip_address":"","ucode":"909B197802F38E","user_header":"https://static001.geekbang.org/account/avatar/00/24/b6/d4/e5bbb73b.jpg","comment_is_top":false,"comment_ctime":1611134645,"is_pvip":false,"replies":[{"id":"99714","content":"是的，后续有相关章节，可以继续学习。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611191822,"ip_address":"","comment_id":274714,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5906101941","product_id":100060801,"comment_content":"实际应用的时候，大规模的user-user, item-item, user-item 两两相似性计算，计算量应该很难满足应用需求，是不是一般都会部署 faiss 来实现呢？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514095,"discussion_content":"是的，后续有相关章节，可以继续学习。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611191822,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":274358,"user_name":"夜枭","can_delete":false,"product_type":"c1","uid":2414668,"ip_address":"","ucode":"7A09EC9E003379","user_header":"","comment_is_top":false,"comment_ctime":1610977889,"is_pvip":false,"replies":[{"id":"99653","content":"是这样","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611096334,"ip_address":"","comment_id":274358,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5905945185","product_id":100060801,"comment_content":"计算embedding相似度ANN应该是常用方法吧，利用Facebook开源的faiss应该较多","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513989,"discussion_content":"是这样","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611096334,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265475,"user_name":"Vinny","can_delete":false,"product_type":"c1","uid":1676038,"ip_address":"","ucode":"4341AFB464775D","user_header":"https://static001.geekbang.org/account/avatar/00/19/93/06/0cb4257e.jpg","comment_is_top":false,"comment_ctime":1606908322,"is_pvip":false,"replies":[{"id":"96515","content":"我们应该在几处评论中都讨论过这个问题了。可以查看学习。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606939939,"ip_address":"","comment_id":265475,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5901875618","product_id":100060801,"comment_content":"老师我还想请问一下，为什么深度学习结构不利于 处理稀疏数据？<br>有什么intuitive的解释吗？<br>谢谢","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510946,"discussion_content":"我们应该在几处评论中都讨论过这个问题了。可以查看学习。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606939939,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265205,"user_name":"Geek_b4af04","can_delete":false,"product_type":"c1","uid":2345393,"ip_address":"","ucode":"BB2A926CAC5390","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/uzvibJnyK9kXJC0p6hv1tyDk5naXzNwOrjibfMAKAb4dadJ608Gd3FW5PCqh0YCXQxMIRkmP6mpEzvraTCJzGvhA/132","comment_is_top":false,"comment_ctime":1606813675,"is_pvip":false,"replies":[{"id":"96406","content":"赞，理解了就好。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606847219,"ip_address":"","comment_id":265205,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5901780971","product_id":100060801,"comment_content":"老师讲的太棒了，之前看了Word2Vec的论文好几遍，数学推导也理解的七七八八，但一直不太理解在工业届应用形式，老师深入简出讲解让我完全理解了word2vec。<br>回答一下问题，可以用cosine和欧氏距离来计算相似度","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510834,"discussion_content":"赞，理解了就好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606847219,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263085,"user_name":"icecreamerr🍦","can_delete":false,"product_type":"c1","uid":2328224,"ip_address":"","ucode":"F3C43BCC505887","user_header":"https://static001.geekbang.org/account/avatar/00/23/86/a0/7bb97234.jpg","comment_is_top":false,"comment_ctime":1605975144,"is_pvip":false,"replies":[{"id":"95604","content":"采用softmax作为激活函数的神经元的输出往往是一个在概率分布向量而不是一个概率数字。<br><br>比如我们的词表里有四个词，那么最终的输出将是[0.1,0.2,0.5,0.2]，代表着模型预测输入样本跟词表中对应词相关的概率为0.1，0.2，0.5，0.2。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606168208,"ip_address":"","comment_id":263085,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5900942440","product_id":100060801,"comment_content":"老师您好，我有一个问题，根据图 4 生成的训练样本，这里的输入向量自然就是由输入词转换而来的 One-hot 编码向量，输出向量则是由多个输出词转换而来的 Multi-hot 编码向量，您下面说的输出层神经元采用了 softmax 作为激活函数，这样的话输出不是一个（0,1）之间的数吗，为啥能直接得到输出是一个multi-hot向量呢","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510090,"discussion_content":"采用softmax作为激活函数的神经元的输出往往是一个在概率分布向量而不是一个概率数字。\n\n比如我们的词表里有四个词，那么最终的输出将是[0.1,0.2,0.5,0.2]，代表着模型预测输入样本跟词表中对应词相关的概率为0.1，0.2，0.5，0.2。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606168208,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1529270,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epW1MiaFR2RTu5YTIBLzZLVm8GJNphfRltWWgRPnKMQJsrpvCXZQa5K0U9jozbfLNWKkTSPlSNkiaSg/132","nickname":"kakaymi","note":"","ucode":"FC675F3796FECF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390894,"discussion_content":"一个中性词可能推出多个周围词，每次softmax之后只能取一个最大值，那如何才能一次性计算多个周围词的损失呢？还是说每次计算一个，然后加起来?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630121636,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":347351,"user_name":"Geek_d389c9","can_delete":false,"product_type":"c1","uid":2645198,"ip_address":"","ucode":"CEABD38D05A030","user_header":"","comment_is_top":false,"comment_ctime":1653965458,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1653965458","product_id":100060801,"comment_content":"老师，想请教下您，为什么推荐中的Embedding用word2vec多，而不是使用bert呀？","like_count":0},{"had_liked":false,"id":334552,"user_name":"高绥凯","can_delete":false,"product_type":"c1","uid":1758269,"ip_address":"","ucode":"A35420C3A48FA8","user_header":"https://static001.geekbang.org/account/avatar/00/1a/d4/3d/068c9298.jpg","comment_is_top":false,"comment_ctime":1644998615,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1644998615","product_id":100060801,"comment_content":"cosine similarity","like_count":0},{"had_liked":false,"id":327201,"user_name":"杰克马","can_delete":false,"product_type":"c1","uid":2454176,"ip_address":"","ucode":"772A1811245A76","user_header":"https://static001.geekbang.org/account/avatar/00/25/72/a0/a3685c59.jpg","comment_is_top":false,"comment_ctime":1639977790,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1639977790","product_id":100060801,"comment_content":"老师您好，想请教下，对于新闻这一类实时性要求比较高的内容，在一开始并没有用户浏览记录的情况下，如何进行冷启动初始化呢？","like_count":0},{"had_liked":false,"id":323992,"user_name":"涛涛","can_delete":false,"product_type":"c1","uid":1180810,"ip_address":"","ucode":"747C4B1F20A2D3","user_header":"https://static001.geekbang.org/account/avatar/00/12/04/8a/ff94bd60.jpg","comment_is_top":false,"comment_ctime":1638252483,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1638252483","product_id":100060801,"comment_content":"好多理论和概念都不太懂","like_count":0},{"had_liked":false,"id":323097,"user_name":"吴梦Meng","can_delete":false,"product_type":"c1","uid":2825478,"ip_address":"","ucode":"394AB4DCA51811","user_header":"https://static001.geekbang.org/account/avatar/00/2b/1d/06/cb5b45d4.jpg","comment_is_top":false,"comment_ctime":1637726208,"is_pvip":false,"replies":[{"id":"117940","content":"user embedding的方法这门课应该都会覆盖到。如果试过主流的方法都不好的话，要么是你实现细节有问题，要么就是数据质量本身比较差","user_name":"作者回复","user_name_real":"编辑","uid":"1662192","ctime":1638816415,"ip_address":"","comment_id":323097,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637726208","product_id":100060801,"comment_content":"老师您好，感谢您的分享，请教下现在业界推荐系统user embedding的方法主要有哪些呐 ？之前试过谷歌的双塔模型效果并不好，如果用图模型在数据量很大的情况下效率又很低。请问老师有什么好的建议吗？非常感谢 ！","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536563,"discussion_content":"user embedding的方法这门课应该都会覆盖到。如果试过主流的方法都不好的话，要么是你实现细节有问题，要么就是数据质量本身比较差","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638816415,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313531,"user_name":"彭小鑫","can_delete":false,"product_type":"c1","uid":2407410,"ip_address":"","ucode":"9BEFBE52BC124E","user_header":"http://thirdwx.qlogo.cn/mmopen/VdM9K3h96B7usk6dk8Q2KsAJmLDuFBXKEUtcKxS2yhSt4aKgtO9FgC7ZdVztkPAT71c7vJQgafU70hvUgPXJXerJ7LIZ3Bxn/132","comment_is_top":false,"comment_ctime":1632477032,"is_pvip":false,"replies":[{"id":"113705","content":"app.diagrams.net画了一部分，有一些是编辑画的","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1632766476,"ip_address":"","comment_id":313531,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1632477032","product_id":100060801,"comment_content":"老师您好，图是用什么软件画的？感觉很棒","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527379,"discussion_content":"app.diagrams.net画了一部分，有一些是编辑画的","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632766476,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310239,"user_name":"何足道","can_delete":false,"product_type":"c1","uid":2335665,"ip_address":"","ucode":"F7498987EC0B84","user_header":"https://static001.geekbang.org/account/avatar/00/23/a3/b1/aea9f46d.jpg","comment_is_top":false,"comment_ctime":1630565906,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1630565906","product_id":100060801,"comment_content":"作者word2vec讲得很清楚，但是我有一个疑问，举的例子中是skip-gram训练，等价于一个多分类问题，这种情况下还能用softmax作激活函数吗？原论文用的是CBOW训练，是单分类问题，这种情况下用softmax比较合理。","like_count":0},{"had_liked":false,"id":308496,"user_name":"王  明","can_delete":false,"product_type":"c1","uid":1322189,"ip_address":"","ucode":"2737F5F4FD168C","user_header":"https://static001.geekbang.org/account/avatar/00/14/2c/cd/b88a0922.jpg","comment_is_top":false,"comment_ctime":1629645487,"is_pvip":false,"replies":[{"id":"111696","content":"推荐继续学习，会在接下来的学习和实践中找到答案","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1629682926,"ip_address":"","comment_id":308496,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1629645487","product_id":100060801,"comment_content":"王老师，你好。我是初入推荐系统的。有个一直不解。比如mf生成用户和物品隐向量我理解。那么embedding是要求序列化数据的。而我只有一条一条的用户行为数据如何embedding啊？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525506,"discussion_content":"推荐继续学习，会在接下来的学习和实践中找到答案","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629682926,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1489525,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI9X140JXPuaDB8PibXpwFWds6mZvg1w7THkyB6NjBkP7x4HqSk2wuUvcmDb9O2l0fCkxvB3ibL0L2A/132","nickname":"科学养牛","note":"","ucode":"B205209A814AC8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399197,"discussion_content":"你把用户一条一条的行为数据中交互的item id，聚合在一个列表里，成为一行数据就可以了吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632920497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":305996,"user_name":"Geek_8a732a","can_delete":false,"product_type":"c1","uid":2723806,"ip_address":"","ucode":"97A312D97F7B91","user_header":"","comment_is_top":false,"comment_ctime":1628264252,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1628264252","product_id":100060801,"comment_content":"余弦夹角、欧式距离计算相似性","like_count":0},{"had_liked":false,"id":295886,"user_name":"冻糕","can_delete":false,"product_type":"c1","uid":2052928,"ip_address":"","ucode":"B0A45E87795C69","user_header":"https://static001.geekbang.org/account/avatar/00/1f/53/40/d599bf28.jpg","comment_is_top":false,"comment_ctime":1622630414,"is_pvip":false,"replies":[{"id":"108158","content":"tf是end2end训练embedding层，并不是w2v实现。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1623876343,"ip_address":"","comment_id":295886,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1622630414","product_id":100060801,"comment_content":"我看了tf的emdedding-column源码，貌似底层并不是w2v实现，而是随机初始化权重矩阵后放入模型端对端训练。。。不知道我理解的对不对。。w2v更类似于预训练模型","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521252,"discussion_content":"tf是end2end训练embedding层，并不是w2v实现。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623876343,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290285,"user_name":"MutouMan","can_delete":false,"product_type":"c1","uid":2454131,"ip_address":"","ucode":"E2E78C6EE25E80","user_header":"https://static001.geekbang.org/account/avatar/00/25/72/73/d707c8be.jpg","comment_is_top":false,"comment_ctime":1619486412,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1619486412","product_id":100060801,"comment_content":"Cosine 是最常用了吧，其他的distance calculation都用的少，几乎没见过。<br>KNN 和 KMeans是不是也可以用呢<br>我觉得non parimetric method都是理论可以的","like_count":0},{"had_liked":false,"id":283072,"user_name":"许沛东","can_delete":false,"product_type":"c1","uid":2109641,"ip_address":"","ucode":"800CA796D3A27E","user_header":"","comment_is_top":false,"comment_ctime":1615538604,"is_pvip":false,"replies":[{"id":"102828","content":"onehot编码应该在05节讲过。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615793135,"ip_address":"","comment_id":283072,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1615538604","product_id":100060801,"comment_content":"老师您好，有一点没有理解，就是word2vec的输入是单词的one-hot编码，这个one-hot编码怎么得到呢？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516926,"discussion_content":"onehot编码应该在05节讲过。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615793135,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2109641,"avatar":"","nickname":"许沛东","note":"","ucode":"800CA796D3A27E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":359436,"discussion_content":"我的意思是怎么得到一个单词的one-hot编码","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616201983,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282083,"user_name":"Allén","can_delete":false,"product_type":"c1","uid":1598977,"ip_address":"","ucode":"0ED33442C97A5F","user_header":"https://static001.geekbang.org/account/avatar/00/18/66/01/48549335.jpg","comment_is_top":false,"comment_ctime":1615070768,"is_pvip":false,"replies":[{"id":"102459","content":"1. 根本原因是输入向量中每一个维度就对应一个词，那么权重矩阵中的对应向量当然就是这个词的embedding。<br>2. 不要纠结这个问题，没有太大意义，理解word2vec的原理就好，至于叫什么根本不重要。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615126816,"ip_address":"","comment_id":282083,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1615070768","product_id":100060801,"comment_content":"请问老师，<br>1，为什么最终把输入-隐藏层的权重矩阵作为词向量？是因为输入向量是one-hot表示吗？那么如果输入不是one-hot的话，还可以把输入-隐藏层权重矩阵作为词向量吗？<br>2，图6中，W是embedding matrix，W&#39;是context matrix 这句描述。W是embedding matrix是由『输入-隐藏层的权重矩阵作为词向量』定义而来吗？这两句话是否有先后因果关系？最后W&#39;是context matrix 是因为啥？<br>辛苦老师","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516613,"discussion_content":"1. 根本原因是输入向量中每一个维度就对应一个词，那么权重矩阵中的对应向量当然就是这个词的embedding。\n2. 不要纠结这个问题，没有太大意义，理解word2vec的原理就好，至于叫什么根本不重要。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615126816,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":254364,"user_name":"书豪","can_delete":false,"product_type":"c1","uid":1068091,"ip_address":"","ucode":"212FBDE59E8457","user_header":"https://static001.geekbang.org/account/avatar/00/10/4c/3b/2780fc51.jpg","comment_is_top":false,"comment_ctime":1603105790,"is_pvip":false,"replies":[{"id":"92861","content":"实战在第8讲就会到来，不要着急","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1603153567,"ip_address":"","comment_id":254364,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1603105790","product_id":100060801,"comment_content":"老师可以结合一个小小的案例实战来讲，这样就会更加直观一点！！！","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507362,"discussion_content":"实战在第8讲就会到来，不要着急","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603153567,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":253148,"user_name":"pedro","can_delete":false,"product_type":"c1","uid":1200704,"ip_address":"","ucode":"F40C839DDFD599","user_header":"https://static001.geekbang.org/account/avatar/00/12/52/40/e57a736e.jpg","comment_is_top":false,"comment_ctime":1602635556,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1602635556","product_id":100060801,"comment_content":"相似性的计算，实质是向量相似性的计算，大家都知道的有余弦相似性，街区距离等，复杂一点的有EMD，还有我也不知道😂","like_count":0}]}