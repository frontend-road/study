{"id":700699,"title":"04｜提示工程（上）：用少样本FewShotTemplate和ExampleSelector创建应景文案","content":"<p>你好，我是黄佳，欢迎来到LangChain实战课！</p><p>上节课我给你留了一个思考题：<strong>在提示模板的构建过程中加入了partial_variables，也就是输出解析器指定的format_instructions之后，为什么能够让模型生成结构化的输出？</strong></p><p>当你用print语句打印出最终传递给大模型的提示时，一切就变得非常明了。</p><pre><code class=\"language-plain\">您是一位专业的鲜花店文案撰写员。\n对于售价为 50 元的 玫瑰 ，您能提供一个吸引人的简短描述吗？\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n\n```json\n{\n&nbsp; &nbsp; &nbsp; &nbsp; \"description\": string&nbsp; // 鲜花的描述文案\n&nbsp; &nbsp; &nbsp; &nbsp; \"reason\": string&nbsp; // 问什么要这样写这个文案\n}\n</code></pre><p>秘密在于，LangChain的输出解析器偷偷的在提示中加了一段话，也就是 {format_instructions} 中的内容。这段由LangChain自动添加的文字，就清楚地指示着我们希望得到什么样的回答以及回答的具体格式。提示指出，模型需要根据一个schema来格式化输出文本，这个schema从 ```json 开始，到 ``` 结束。</p><!-- [[[read_end]]] --><p>这就是在告诉模型，你就follow这个schema（schema，可以理解为对数据结构的描述）的格式，就行啦！</p><p>这就是一个很棒、很典型的<strong>提示工程</strong>。有了这样清晰的提示，智能程度比较高的模型（比如GPT3.5及以上版本），肯定能够输出可以被解析的数据结构，如JSON格式的数据。</p><p>那么这节课我就带着你进一步深究，如何利用LangChain中的提示模板，做好提示工程。</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/fe/3b5584552720f22ac10e1ab1430f61fe.jpg?wh=4000x1536\" alt=\"\" title=\"提示工程：模型 I/O 的输入部分\"></p><p>上节课我说过，针对大模型的提示工程该如何做，吴恩达老师在他的 <a href=\"https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\">ChatGPT Prompt Engineering for Developers</a> 公开课中，给出了两个大的原则：第一条原则是写出清晰而具体的指示，第二条原则是给模型思考的时间。</p><p>无独有偶，在Open AI的官方文档 <a href=\"https://platform.openai.com/docs/guides/gpt-best-practices/gpt-best-practices\">GPT 最佳实践</a>中，也给出了和上面这两大原则一脉相承的6大策略。分别是：</p><ol>\n<li>写清晰的指示</li>\n<li>给模型提供参考（也就是示例）</li>\n<li>将复杂任务拆分成子任务</li>\n<li>给GPT时间思考</li>\n<li>使用外部工具</li>\n<li>反复迭代问题</li>\n</ol><p>怎么样，这些原则和策略是不是都是大白话？这些原则其实不仅能够指导大语言模型，也完全能够指导你的思维过程，让你处理问题时的思路更为清晰。所以说，大模型的思维过程和我们人类的思维过程，还是蛮相通的。</p><h2>提示的结构</h2><p>当然了，从大原则到实践，还是有一些具体工作需要说明，下面我们先看一个实用的提示框架。</p><p><img src=\"https://static001.geekbang.org/resource/image/b7/16/b77a15cd83b66bba55032d711bcf3c16.png?wh=1920x801\" alt=\"图片\"></p><p>在这个提示框架中：</p><ul>\n<li><strong>指令</strong>（Instuction）告诉模型这个任务大概要做什么、怎么做，比如如何使用提供的外部信息、如何处理查询以及如何构造输出。这通常是一个提示模板中比较固定的部分。一个常见用例是告诉模型“你是一个有用的XX助手”，这会让他更认真地对待自己的角色。</li>\n<li><strong>上下文</strong>（Context）则充当模型的额外知识来源。这些信息可以手动插入到提示中，通过矢量数据库检索得来，或通过其他方式（如调用API、计算器等工具）拉入。一个常见的用例时是把从向量数据库查询到的知识作为上下文传递给模型。</li>\n<li><strong>提示输入</strong>（Prompt Input）通常就是具体的问题或者需要大模型做的具体事情，这个部分和“指令”部分其实也可以合二为一。但是拆分出来成为一个独立的组件，就更加结构化，便于复用模板。这通常是作为变量，在调用模型之前传递给提示模板，以形成具体的提示。</li>\n<li><strong>输出指示器</strong>（Output Indicator）标记​​要生成的文本的开始。这就像我们小时候的数学考卷，先写一个“解”，就代表你要开始答题了。如果生成 Python 代码，可以使用 “import” 向模型表明它必须开始编写 Python 代码（因为大多数 Python 脚本以import开头）。这部分在我们和ChatGPT对话时往往是可有可无的，当然LangChain中的代理在构建提示模板时，经常性的会用一个“Thought：”（思考）作为引导词，指示模型开始输出自己的推理（Reasoning）。</li>\n</ul><p>下面，就让我们看看如何使用 LangChain中的各种提示模板做提示工程，将更优质的提示输入大模型。</p><h2>LangChain 提示模板的类型</h2><p>LangChain中提供String（StringPromptTemplate）和Chat（BaseChatPromptTemplate）两种基本类型的模板，并基于它们构建了不同类型的提示模板：</p><p><img src=\"https://static001.geekbang.org/resource/image/fe/yy/feefbb0a166f53f14f647b88e1025cyy.jpg?wh=2240x812\" alt=\"\"></p><p>这些模板的导入方式如下：</p><pre><code class=\"language-plain\">\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.prompts import FewShotPromptTemplate\nfrom langchain.prompts.pipeline import PipelinePromptTemplate\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.prompts import (\n    ChatMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n</code></pre><p>我发现有时候不指定 .prompts，直接从LangChain包也能导入模板。</p><pre><code class=\"language-plain\">from langchain import PromptTemplate\n</code></pre><p>下面我们通过示例来介绍上面这些模版，前两个我们简单了解就好，其中最典型的FewShotPromptTemplate会重点讲。至于PipelinePrompt和自定义模板，使用起来比较简单，请你参考LangChain文档自己学习。</p><h2>使用 PromptTemplate</h2><p>下面通过示例简单说明一下PromptTemplate的使用。</p><pre><code class=\"language-plain\">from langchain import PromptTemplate\n\ntemplate = \"\"\"\\\n你是业务咨询顾问。\n你给一个销售{product}的电商公司，起一个好的名字？\n\"\"\"\nprompt = PromptTemplate.from_template(template)\n\nprint(prompt.format(product=\"鲜花\"))\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">你是业务咨询顾问。\n你给一个销售鲜花的电商公司，起一个好的名字？\n</code></pre><p>这个程序的主要功能是生成适用于不同场景的提示，对用户定义的一种产品或服务提供公司命名建议。</p><p>在这里，<code>\"你是业务咨询顾问。你给一个销售{product}的电商公司，起一个好的名字？\"</code> 就是原始提示模板，其中 {product} 是占位符。</p><p>然后通过PromptTemplate的from_template方法，我们创建了一个提示模板对象，并通过prompt.format方法将模板中的 {product} 替换为 <code>\"鲜花\"</code>。</p><p>这样，就得到了一句具体的提示：<em>你是业务咨询顾问。你给一个销售鲜花的电商公司，起一个好的名字？</em>——这就要求大语言模型，要有的放矢。</p><p>在上面这个过程中，LangChain中的模板的一个方便之处是from_template方法可以从传入的字符串中自动提取变量名称（如product），而无需刻意指定。<strong>上面程序中的product自动成为了format方法中的一个参数</strong>。</p><p>当然，也可以通过提示模板类的构造函数，在创建模板时手工指定input_variables，示例如下：</p><pre><code class=\"language-plain\">prompt = PromptTemplate(\n&nbsp; &nbsp; input_variables=[\"product\", \"market\"], \n&nbsp; &nbsp; template=\"你是业务咨询顾问。对于一个面向{market}市场的，专注于销售{product}的公司，你会推荐哪个名字？\"\n)\nprint(prompt.format(product=\"鲜花\", market=\"高端\"))\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">你是业务咨询顾问。对于一个面向高端市场的，专注于销售鲜花的公司，你会推荐哪个名字？\n</code></pre><p>上面的方式直接生成了提示模板，并没有通过from_template方法从字符串模板中创建提示模板。二者效果是一样的。</p><h2>使用 ChatPromptTemplate</h2><p>对于OpenAI推出的ChatGPT这一类的聊天模型，LangChain也提供了一系列的模板，这些模板的不同之处是它们有对应的角色。</p><p>下面代码展示了OpenAI的Chat Model中的各种消息角色。</p><pre><code class=\"language-plain\">import openai\nopenai.ChatCompletion.create(\n&nbsp; model=\"gpt-3.5-turbo\",\n&nbsp; messages=[\n&nbsp; &nbsp; &nbsp; &nbsp; {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n&nbsp; &nbsp; &nbsp; &nbsp; {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n&nbsp; &nbsp; &nbsp; &nbsp; {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n&nbsp; &nbsp; &nbsp; &nbsp; {\"role\": \"user\", \"content\": \"Where was it played?\"}\n&nbsp; &nbsp; ]\n)\n</code></pre><p>OpenAI对传输到gpt-3.5-turbo和GPT-4的messsage格式说明如下：</p><blockquote>\n<p><span class=\"reference\">消息必须是消息对象的数组，其中每个对象都有一个角色（系统、用户或助理）和内容。对话可以短至一条消息，也可以来回多次。</span><br>\n&nbsp;<br>\n<span class=\"reference\">通常，对话首先由系统消息格式化，然后是交替的用户消息和助理消息。</span><br>\n&nbsp;<br>\n<span class=\"reference\">系统消息有助于设置助手的行为。例如，你可以修改助手的个性或提供有关其在整个对话过程中应如何表现的具体说明。但请注意，系统消息是可选的，并且没有系统消息的模型的行为可能类似于使用通用消息，例如“你是一个有用的助手”。</span><br>\n&nbsp;<br>\n<span class=\"reference\">用户消息提供助理响应的请求或评论。</span><br>\n&nbsp;<br>\n<span class=\"reference\">助理消息存储以前的助理响应，但也可以由你编写以给出所需行为的示例。</span></p>\n</blockquote><p>LangChain的ChatPromptTemplate这一系列的模板，就是<strong>跟着这一系列角色而设计的</strong>。</p><p>下面，我给出一个示例。</p><pre><code class=\"language-plain\"># 导入聊天消息类模板\nfrom langchain.prompts import (\n&nbsp; &nbsp; ChatPromptTemplate,\n&nbsp; &nbsp; SystemMessagePromptTemplate,\n&nbsp; &nbsp; HumanMessagePromptTemplate,\n)\n# 模板的构建\ntemplate=\"你是一位专业顾问，负责为专注于{product}的公司起名。\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template=\"公司主打产品是{product_detail}。\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nprompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\n# 格式化提示消息生成提示\nprompt = prompt_template.format_prompt(product=\"鲜花装饰\", product_detail=\"创新的鲜花设计。\").to_messages()\n\n# 下面调用模型，把提示传入模型，生成结果\nimport os\nos.environ[\"OPENAI_API_KEY\"] = '你的OpenAI Key'\nfrom langchain.chat_models import ChatOpenAI\nchat = ChatOpenAI()\nresult = chat(prompt)\nprint(result)\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">content='1. 花语创意\\n2. 花韵设计\\n3. 花艺创新\\n4. 花漾装饰\\n5. 花语装点\\n6. 花翩翩\\n7. 花语之美\\n8. 花馥馥\\n9. 花语时尚\\n10. 花之魅力' \nadditional_kwargs={} \nexample=False\n</code></pre><p>好吧，尽管模型成功地完成了任务，但是感觉没有咱“易速鲜花”响亮！</p><p>讲完上面两种简单易用的提示模板，下面开始介绍今天的重点内容，FewShotPromptTemplate。FewShot，也就是少样本这一概念，是提示工程中非常重要的部分，对应着OpenAI提示工程指南中的第2条——给模型提供参考（也就是示例）。</p><h2>FewShot的思想起源</h2><p>讲解概念之前，我先分享个事儿哈，帮助你理解。</p><p>今天我下楼跑步时，一个老爷爷教孙子学骑车，小孩总掌握不了平衡，蹬一两下就下车。</p><ul>\n<li>爷爷说：“宝贝，你得有毅力！”</li>\n<li>孙子说：“爷爷，什么是毅力？”</li>\n<li>爷爷说：“你看这个叔叔，绕着楼跑了10多圈了，这就是毅力，你也得至少蹬个10几趟才能骑起来。”</li>\n</ul><p>这老爷爷就是给孙子做了一个One-Shot学习。如果他的孙子第一次听说却上来就明白什么是毅力，那就神了，这就叫Zero-Shot，表明这孩子的语言天赋不是一般的高，从知识积累和当前语境中就能够推知新词的涵义。有时候我们把Zero-Shot翻译为“顿悟”，聪明的大模型，某些情况下也是能够做到的。</p><p>Few-Shot（少样本）、One-Shot（单样本）和与之对应的 Zero-Shot（零样本）的概念都起源于机器学习。如何让机器学习模型在极少量甚至没有示例的情况下学习到新的概念或类别，对于许多现实世界的问题是非常有价值的，因为我们往往无法获取到大量的标签化数据。</p><p>这几个重要概念并非在某一篇特定的论文中首次提出，而是在机器学习和深度学习的研究中逐渐形成和发展的。</p><ul>\n<li>对于Few-Shot Learning，一个重要的参考文献是2016年Vinyals, O.的论文《小样本学习的匹配网络》。</li>\n<li>这篇论文提出了一种新的学习模型——匹配网络（Matching Networks），专门针对单样本学习（One-Shot Learning）问题设计，<strong>而</strong> <strong>One-Shot Learning</strong> <strong>可以看作是一种最常见的</strong> <strong>Few-Shot</strong> <strong>学习的情况。</strong></li>\n<li>对于Zero-Shot Learning，一个代表性的参考文献是Palatucci, M.在2009年提出的《基于语义输出编码的零样本学习（Zero-Shot Learning with semantic output codes）》，这篇论文提出了零次学习（Zero-Shot Learning）的概念，其中的学习系统可以根据类的语义描述来识别之前未见过的类。</li>\n</ul><p>在提示工程（Prompt Engineering）中，Few-Shot 和 Zero-Shot 学习的概念也被广泛应用。</p><ul>\n<li>在Few-Shot学习设置中，模型会被给予几个示例，以帮助模型理解任务，并生成正确的响应。</li>\n<li>在Zero-Shot学习设置中，模型只根据任务的描述生成响应，不需要任何示例。</li>\n</ul><p>而OpenAI在介绍GPT-3模型的重要论文《Language models are Few-Shot learners（语言模型是少样本学习者）》中，更是直接指出：GPT-3模型，作为一个大型的自我监督学习模型，通过提升模型规模，实现了出色的Few-Shot学习性能。</p><p>这篇论文为大语言模型可以进行Few-Shot学习提供了扎实的理论基础。</p><p><img src=\"https://static001.geekbang.org/resource/image/48/bc/481yy45346cc28ec48269c752c3647bc.png?wh=659x786\" alt=\"\" title=\"GPT-3：语言模型是少样本学习者\"></p><p>下图就是OpenAI的GPT-3论文给出的GPT-3在翻译任务中，通过FewShot提示完成翻译的例子。</p><p><img src=\"https://static001.geekbang.org/resource/image/35/ca/357e9ca0ce2b4699a24e3fe512c047ca.png?wh=1632x1465\" alt=\"\"></p><p>以上，就是ZeroShot、OneShot、FewShot这些重要概念的起源。</p><h2>使用 FewShotPromptTemplate</h2><p>下面，就让我们来通过LangChain中的FewShotPromptTemplate构建出最合适的鲜花文案。</p><p><strong>1. 创建示例样本</strong></p><p>首先，创建一些示例，作为提示的样本。其中每个示例都是一个字典，其中键是输入变量，值是这些输入变量的值。</p><pre><code class=\"language-plain\"># 1. 创建一些示例\nsamples = [\n&nbsp; {\n&nbsp; &nbsp; \"flower_type\": \"玫瑰\",\n&nbsp; &nbsp; \"occasion\": \"爱情\",\n&nbsp; &nbsp; \"ad_copy\": \"玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。\"\n&nbsp; },\n&nbsp; {\n&nbsp; &nbsp; \"flower_type\": \"康乃馨\",\n&nbsp; &nbsp; \"occasion\": \"母亲节\",\n&nbsp; &nbsp; \"ad_copy\": \"康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。\"\n&nbsp; },\n&nbsp; {\n&nbsp; &nbsp; \"flower_type\": \"百合\",\n&nbsp; &nbsp; \"occasion\": \"庆祝\",\n&nbsp; &nbsp; \"ad_copy\": \"百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。\"\n&nbsp; },\n&nbsp; {\n&nbsp; &nbsp; \"flower_type\": \"向日葵\",\n&nbsp; &nbsp; \"occasion\": \"鼓励\",\n&nbsp; &nbsp; \"ad_copy\": \"向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。\"\n&nbsp; }\n]\n</code></pre><p>samples这个列表，它包含了四个字典，每个字典代表了一种花的类型、适合的场合，以及对应的广告文案。 这些示例样本，就是构建FewShotPrompt时，作为例子传递给模型的参考信息。</p><p><strong>2. 创建提示模板</strong></p><p>配置一个提示模板，将一个示例格式化为字符串。这个格式化程序应该是一个PromptTemplate对象。</p><pre><code class=\"language-plain\"># 2. 创建一个提示模板\nfrom langchain.prompts.prompt import PromptTemplate\ntemplate=\"鲜花类型: {flower_type}\\n场合: {occasion}\\n文案: {ad_copy}\"\nprompt_sample = PromptTemplate(input_variables=[\"flower_type\", \"occasion\", \"ad_copy\"], \n                               template=template)\nprint(prompt_sample.format(**samples[0]))\n</code></pre><p>提示模板的输出如下：</p><pre><code class=\"language-plain\">鲜花类型: 玫瑰\n场合: 爱情\n文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。\n</code></pre><p>在这个步骤中，我们创建了一个PromptTemplate对象。这个对象是根据指定的输入变量和模板字符串来生成提示的。在这里，输入变量包括 <code>\"flower_type\"</code>、<code>\"occasion\"</code>、<code>\"ad_copy\"</code>，模板是一个字符串，其中包含了用大括号包围的变量名，它们会被对应的变量值替换。</p><p>到这里，我们就把字典中的示例格式转换成了提示模板，可以形成一个个具体可用的LangChain提示。比如我用samples[0]中的数据替换了模板中的变量，生成了一个完整的提示。</p><p><strong>3. 创建 FewShotPromptTemplate 对象</strong></p><p>然后，通过使用上一步骤中创建的prompt_sample，以及samples列表中的所有示例， 创建一个FewShotPromptTemplate对象，生成更复杂的提示。</p><pre><code class=\"language-plain\"># 3. 创建一个FewShotPromptTemplate对象\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\nprompt = FewShotPromptTemplate(\n&nbsp; &nbsp; examples=samples,\n&nbsp; &nbsp; example_prompt=prompt_sample,\n&nbsp; &nbsp; suffix=\"鲜花类型: {flower_type}\\n场合: {occasion}\",\n&nbsp; &nbsp; input_variables=[\"flower_type\", \"occasion\"]\n)\nprint(prompt.format(flower_type=\"野玫瑰\", occasion=\"爱情\"))\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">鲜花类型: 玫瑰\n场合: 爱情\n文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。\n\n鲜花类型: 康乃馨\n场合: 母亲节\n文案: 康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。\n\n鲜花类型: 百合\n场合: 庆祝\n文案: 百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。\n\n鲜花类型: 向日葵\n场合: 鼓励\n文案: 向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。\n\n鲜花类型: 野玫瑰\n场合: 爱情\n</code></pre><p>可以看到，FewShotPromptTemplate是一个更复杂的提示模板，它包含了多个示例和一个提示。这种模板可以使用多个示例来指导模型生成对应的输出。目前我们创建一个新提示，其中包含了根据指定的花的类型“野玫瑰”和场合“爱情”。</p><p><strong>4. 调用大模型创建新文案</strong></p><p>最后，把这个对象输出给大模型，就可以根据提示，得到我们所需要的文案了！</p><pre><code class=\"language-plain\"># 4. 把提示传递给大模型\nimport os\nos.environ[\"OPENAI_API_KEY\"] = '你的Open AI Key'\nfrom langchain.llms import OpenAI\nmodel = OpenAI(model_name='gpt-3.5-turbo-instruct')\nresult = model(prompt.format(flower_type=\"野玫瑰\", occasion=\"爱情\"))\nprint(result)\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">文案: 野玫瑰代表着爱情的坚贞，是你向心爱的人表达爱意的最佳礼物。\n</code></pre><p>好！模型成功地模仿了我们的示例，写出了新文案，从结构到语气都蛮相似的。</p><h2>使用示例选择器</h2><p>如果我们的示例很多，那么一次性把所有示例发送给模型是不现实而且低效的。另外，每次都包含太多的Token也会浪费流量（OpenAI是按照Token数来收取费用）。</p><p>LangChain给我们提供了示例选择器，来选择最合适的样本。（注意，因为示例选择器使用向量相似度比较的功能，此处需要安装向量数据库，这里我使用的是开源的Chroma，你也可以选择之前用过的Qdrant。）</p><p>下面，就是使用示例选择器的示例代码。</p><pre><code class=\"language-plain\"># 5. 使用示例选择器\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 初始化示例选择器\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n&nbsp; &nbsp; samples,\n&nbsp; &nbsp; OpenAIEmbeddings(),\n&nbsp; &nbsp; Chroma,\n&nbsp; &nbsp; k=1\n)\n\n# 创建一个使用示例选择器的FewShotPromptTemplate对象\nprompt = FewShotPromptTemplate(\n&nbsp; &nbsp; example_selector=example_selector, \n&nbsp; &nbsp; example_prompt=prompt_sample, \n&nbsp; &nbsp; suffix=\"鲜花类型: {flower_type}\\n场合: {occasion}\", \n&nbsp; &nbsp; input_variables=[\"flower_type\", \"occasion\"]\n)\nprint(prompt.format(flower_type=\"红玫瑰\", occasion=\"爱情\"))\n</code></pre><p>输出：</p><pre><code class=\"language-plain\">鲜花类型: 玫瑰\n场合: 爱情\n文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。\n\n鲜花类型: 红玫瑰\n场合: 爱情\n</code></pre><p>在这个步骤中，它首先创建了一个SemanticSimilarityExampleSelector对象，这个对象可以根据语义相似性选择最相关的示例。然后，它创建了一个新的FewShotPromptTemplate对象，这个对象使用了上一步创建的选择器来选择最相关的示例生成提示。</p><p>然后，我们又用这个模板生成了一个新的提示，因为我们的提示中需要创建的是红玫瑰的文案，所以，示例选择器example_selector会根据语义的相似度（余弦相似度）找到最相似的示例，也就是“玫瑰”，并用这个示例构建了FewShot模板。</p><p>这样，我们就避免了把过多的无关模板传递给大模型，以节省Token的用量。</p><h2>总结时刻</h2><p>好的，到这里，今天这一讲就结束了。我们介绍了提示工程的原理，几种提示模板的用法，以及最重要的FewShot的思路。其实说白了，就是给模型一些示例做参考，模型才能明白你要什么。</p><p><img src=\"https://static001.geekbang.org/resource/image/f4/0d/f46817a7ed56c6fef64a6aeee4c1yy0d.png?wh=955x970\" alt=\"\" title=\"学术论文中的 Few-Shot 和 Zero-Shot 示例\"></p><p>总的来说，提供示例对于解决某些任务至关重要，通常情况下，FewShot的方式能够显著提高模型回答的质量。不过，当少样本提示的效果不佳时，这可能表示模型在任务上的学习不足。在这种情况下，我们建议对模型进行微调或尝试更高级的提示技术。</p><p>下一节课，我们将在探讨输出解析的同时，讲解另一种备受关注的提示技术，被称为“思维链提示”（Chain of Thought，简称CoT）。这种技术因其独特的应用方式和潜在的实用价值而引人注目。</p><h2>思考题</h2><ol>\n<li>如果你观察LangChain中的prompt.py中的PromptTemplate的实现代码，你会发现除了我们使用过的input_variables、template等初始化参数之外，还有template_format、validate_template等参数。举例来说，template_format可以指定除了f-string之外，其它格式的模板，比如jinja2。请你查看LangChain文档，并尝试使用这些参数。</li>\n</ol><pre><code class=\"language-plain\">template_format: str = \"f-string\"\n\"\"\"The format of the prompt template. Options are: 'f-string', 'jinja2'.\"\"\"\n\nvalidate_template: bool = True\n\"\"\"Whether or not to try validating the template.\"\"\"\n</code></pre><ol start=\"2\">\n<li>请你尝试使用PipelinePromptTemplate和自定义Template。<br>\n&nbsp;</li>\n<li>请你构想一个关于鲜花店运营场景中客户服务对话的少样本学习任务。在这个任务中，模型需要根据提供的示例，学习如何解答客户的各种问题，包括询问花的价格、推荐鲜花、了解鲜花的保养方法等。最好是用ChatModel完成这个任务。</li>\n</ol><pre><code class=\"language-plain\">from langchain.chat_models import ChatOpenAI\nfrom langchain import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    AIMessagePromptTemplate,\n    HumanMessagePromptTemplate)\n</code></pre><p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p><h2>延伸阅读</h2><ol>\n<li>论文： Open AI的GPT-3模型：<a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\">大模型是少样本学习者</a>， Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Agarwal, S. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</li>\n<li>论文：<a href=\"https://arxiv.org/abs/1606.04080\">单样本学习的匹配网络</a>，Vinyals, O., Blundell, C., Lillicrap, T., &amp; Wierstra, D. (2016). Matching networks for one shot learning. In Advances in neural information processing systems (pp. 3630-3638).</li>\n<li>论文：<a href=\"https://www.cs.toronto.edu/~hinton/absps/palatucci.pdf\">用语义输出编码做零样本学习</a>，Palatucci, M., Pomerleau, D., Hinton, G. E., &amp; Mitchell, T. M. (2009). Zero-shot learning with semantic output codes. In Advances in neural information processing systems (pp. 1410-1418).</li>\n<li>论文：<a href=\"https://doi.org/10.48550/arXiv.2202.12837\">对示例角色的重新思考：是什么使得上下文学习有效？</a>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).</li>\n<li>论文：<a href=\"https://arxiv.org/pdf/2109.01652.pdf\">微调后的语言模型是零样本学习者</a>，Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., &amp; Le, Q. V. (2022). Finetuned Language Models Are Zero-Shot Learners. Proceedings of the International Conference on Learning Representations (ICLR 2022).</li>\n</ol>","comments":[{"had_liked":false,"id":381004,"user_name":"黄振宇","can_delete":false,"product_type":"c1","uid":1686397,"ip_address":"浙江","ucode":"976BC7B46DA419","user_header":"https://static001.geekbang.org/account/avatar/00/19/bb/7d/26340713.jpg","comment_is_top":false,"comment_ctime":1694577091,"is_pvip":false,"replies":[{"id":138783,"content":"同学的前面几个问题，非常非常的细节。\n我使用prompt遇到的一些问题\n    1. 最常见的initial_agent方法直接传入prompt参数，好像无效。只能通过agent_kwargs的PREFIX和SUFFIX等传入。\n    2. 先定义个LLMChain传入prompt参数，在定义一个agent（single action或者multi action）\n    3. 有时候传值进去，最后打印agent的prompt，发现好像也没有成功。\n    4. openAI的模型还好，没怎么管输入输出的prompt的格式。但llama2的模型更难伺候，同样的代码只是换了LLM模型就运行不了，不是报没有input_variables就是输出无法parse，很困惑。\n这几个问题，我觉得同学可以加我的微信或者加群，然后把具体代码传给我，我们看看代码来解决。\n\n后面一个问题，我的想法是，把课程学完，我想你应该理解LangChain的六大组件的精髓和核心思想了。可能学到一半就可以知道 —— LangChain能干什么，不能干什么了。我认为最重要的是学Agent的思想，以及学LangChain是怎么做提示工程的。\n\n我认为你仍然应该以自己日常业务为工作学习的中心，LangChain只是工具，它可能能够应用再你的日常业务中，也可能不能。但是，它的思想，在未来几年的开发过程中，你肯定用得上。\n\n所以，不必苦钻细节了。了解思想精髓为主。LangChain太新，调试起来太耗费你的精力。\n\n把课程学完，我们了解LangChain的思想，以及它未来的可能性。等将来有了具体项目需求，你根据项目需求再来深入学那一块。这样效率比较高，你的成就感也高。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1694599764,"ip_address":"新加坡","comment_id":381004,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"我使用prompt遇到的一些问题\n    1. 最常见的initial_agent方法直接传入prompt参数，好像无效。只能通过agent_kwargs的PREFIX和SUFFIX等传入。\n    2. 先定义个LLMChain传入prompt参数，在定义一个agent（single action或者multi action）\n    3. 有时候传值进去，最后打印agent的prompt，发现好像也没有成功。\n    4. openAI的模型还好，没怎么管输入输出的prompt的格式。但llama2的模型更难伺候，同样的代码只是换了LLM模型就运行不了，不是报没有input_variables就是输出无法parse，很困惑。\n    \n    以上希望在老师的课程里都能得到解答。\n    \n    另外，最近想去解决上述问题，看了langchain的源码，一头雾水，感觉东西太多了，不知从何开始，如果花大量时间在研究langchain上面又担心本末倒置了，毕竟它只是一个工具，还有好多的应用层的东西需要学习和研究，但是不吃透langchain只能做一些简单chatbot的应用。所以也比较迷茫，希望老师也能给出解答，如何做好二者的平衡。","like_count":9,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627845,"discussion_content":"同学的前面几个问题，非常非常的细节。\n我使用prompt遇到的一些问题\n    1. 最常见的initial_agent方法直接传入prompt参数，好像无效。只能通过agent_kwargs的PREFIX和SUFFIX等传入。\n    2. 先定义个LLMChain传入prompt参数，在定义一个agent（single action或者multi action）\n    3. 有时候传值进去，最后打印agent的prompt，发现好像也没有成功。\n    4. openAI的模型还好，没怎么管输入输出的prompt的格式。但llama2的模型更难伺候，同样的代码只是换了LLM模型就运行不了，不是报没有input_variables就是输出无法parse，很困惑。\n这几个问题，我觉得同学可以加我的微信或者加群，然后把具体代码传给我，我们看看代码来解决。\n\n后面一个问题，我的想法是，把课程学完，我想你应该理解LangChain的六大组件的精髓和核心思想了。可能学到一半就可以知道 —— LangChain能干什么，不能干什么了。我认为最重要的是学Agent的思想，以及学LangChain是怎么做提示工程的。\n\n我认为你仍然应该以自己日常业务为工作学习的中心，LangChain只是工具，它可能能够应用再你的日常业务中，也可能不能。但是，它的思想，在未来几年的开发过程中，你肯定用得上。\n\n所以，不必苦钻细节了。了解思想精髓为主。LangChain太新，调试起来太耗费你的精力。\n\n把课程学完，我们了解LangChain的思想，以及它未来的可能性。等将来有了具体项目需求，你根据项目需求再来深入学那一块。这样效率比较高，你的成就感也高。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694599764,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":1686397,"avatar":"https://static001.geekbang.org/account/avatar/00/19/bb/7d/26340713.jpg","nickname":"黄振宇","note":"","ucode":"976BC7B46DA419","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":627859,"discussion_content":"感谢黄老师，请问怎么加群或者您的微信，没看到呀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694611032,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":627845,"ip_address":"浙江","group_id":0},"score":627859,"extra":""},{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1686397,"avatar":"https://static001.geekbang.org/account/avatar/00/19/bb/7d/26340713.jpg","nickname":"黄振宇","note":"","ucode":"976BC7B46DA419","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627964,"discussion_content":"我微信 jackyhuang79，可以加我","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694706425,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":627859,"ip_address":"新加坡","group_id":0},"score":627964,"extra":""}]}]},{"had_liked":false,"id":383156,"user_name":"高源","can_delete":false,"product_type":"c1","uid":1048887,"ip_address":"吉林","ucode":"751B41FD38EF7D","user_header":"https://static001.geekbang.org/account/avatar/00/10/01/37/12e4c9c9.jpg","comment_is_top":false,"comment_ctime":1698572685,"is_pvip":false,"replies":[{"id":139608,"content":"这是OpenAI和其它厂子用海量数据训练大模型，用代码训练，指令微调和对齐之后，大模型自己拥有的神奇能力啊。\n大模型见多识广，能够举一反三。因此称得上是“智能”。它绝对不是对之前见过的东西的简单存储。如果是那样，它和搜索引擎不就没区别了？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698771461,"ip_address":"瑞士","comment_id":383156,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师有个问题问下，例如大模型知道你提出问题后，是如何输出满意的答案呢，例如一道算法编程题目，大模型知道你的问题后，是根据自己以前存储类似答案输出的吗，我提个新的特殊问题，他是怎么思考输出的呢","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630680,"discussion_content":"这是OpenAI和其它厂子用海量数据训练大模型，用代码训练，指令微调和对齐之后，大模型自己拥有的神奇能力啊。\n大模型见多识广，能够举一反三。因此称得上是“智能”。它绝对不是对之前见过的东西的简单存储。如果是那样，它和搜索引擎不就没区别了？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698771461,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382569,"user_name":"抽象派","can_delete":false,"product_type":"c1","uid":2599971,"ip_address":"广东","ucode":"6879F90CB702FC","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/YflLdCdbUAkfr9LPzF50EibDrMxBibPicQ5NNAETaPP0ytTmuR3h6QNichDMhDbR2XelSIXpOrPwbiaHgBkMJYOeULA/132","comment_is_top":false,"comment_ctime":1697597637,"is_pvip":false,"replies":[{"id":139456,"content":"“给模型思考的时间”。就是，让模型一步一步的推理，慢慢的推理，慢慢思考的意思。背后的哲学是，把模型当成“人”来对待。\n这个哲学有多大用，值得商榷。需要论文，实验，来验证，这样指导模型是否有用，或者对什么类型的问题有用。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698142992,"ip_address":"瑞士","comment_id":382569,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，请问“模型思考的时间”这个怎么理解啊？可以举个例子吗？","like_count":2,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630162,"discussion_content":"“给模型思考的时间”。就是，让模型一步一步的推理，慢慢的推理，慢慢思考的意思。背后的哲学是，把模型当成“人”来对待。\n这个哲学有多大用，值得商榷。需要论文，实验，来验证，这样指导模型是否有用，或者对什么类型的问题有用。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1698142992,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":388856,"user_name":"一路前行","can_delete":false,"product_type":"c1","uid":1288985,"ip_address":"北京","ucode":"32D3C715690783","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcFhGY0NV4kFzOSXWDHR2lrI2UbUP4Y016GOnpTH7dqSbicqJarX0pHxMsfLopRiacKEPXLx7IHHqg/132","comment_is_top":false,"comment_ctime":1711008729,"is_pvip":false,"replies":[{"id":141717,"content":"使用本地大模型，主要用HuggingFace和PyTorch，LangChain有时候也有接口，能用就一样用。参见LangChain具体文档，如果出错，那么就参考LangChain这个框架的思路，自己直接用。LangChain给出的，主要还是个思路。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1712810057,"ip_address":"新加坡","comment_id":388856,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师，发现你在讲langchain时候基本以openai为主，这没什么问题。可否讲下。比如自己的本地大模型。如何通过langchain实现调用。比如自己的本地大模型，和嵌入模型。langchain之前的调用方法，是否需要修改，如果需要该怎么修改？","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641607,"discussion_content":"使用本地大模型，主要用HuggingFace和PyTorch，LangChain有时候也有接口，能用就一样用。参见LangChain具体文档，如果出错，那么就参考LangChain这个框架的思路，自己直接用。LangChain给出的，主要还是个思路。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1712810057,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":383668,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1699429586,"is_pvip":false,"replies":[{"id":140199,"content":"新版本会要求从更具体的包中导入，而不是LangChain主包。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1700583881,"ip_address":"瑞士","comment_id":383668,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"我发现有时候不指定 .prompts，直接从 LangChain 包也能导入模板。  会有告警信息：\nlangchain&#47;__init__.py:34: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain.prompts.PromptTemplate instead. warnings.warn(\n所以，建议还是指定 .prompts，即from langchain.prompts.prompt import PromptTemplate","like_count":1,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632248,"discussion_content":"新版本会要求从更具体的包中导入，而不是LangChain主包。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700583881,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391019,"user_name":"曹胖子","can_delete":false,"product_type":"c1","uid":1374648,"ip_address":"湖南","ucode":"8911E5DA412E59","user_header":"https://static001.geekbang.org/account/avatar/00/14/f9/b8/0f3bd8ec.jpg","comment_is_top":false,"comment_ctime":1717061149,"is_pvip":false,"replies":[{"id":142232,"content":"这个似乎是你的OpenAI  Key要换成自己的","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1717218025,"ip_address":"新加坡","comment_id":391019,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"一路照着内容跑代码  一直到example_selector = SemanticSimilarityExampleSelector.from_examples   就不行了  工具报错 提示 UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters in position 7-8: ordinal not in range(128)  问了gpt也没解决问题","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646053,"discussion_content":"这个似乎是你的OpenAI  Key要换成自己的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1717218025,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":390389,"user_name":"冬瓜蔡","can_delete":false,"product_type":"c1","uid":1117673,"ip_address":"浙江","ucode":"2F04A446D28201","user_header":"https://static001.geekbang.org/account/avatar/00/11/0d/e9/2f02a383.jpg","comment_is_top":false,"comment_ctime":1715242255,"is_pvip":false,"replies":[{"id":142270,"content":"这是模型的能力问题吧","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1717648364,"ip_address":"新加坡","comment_id":390389,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"黄老师下午好，我在跑野玫瑰 文案这个case时，并没有输出文案，而是如下信息：“这些看起来像是鲜花分类及其搭配的一些文案。需要帮你做些什么呢？”","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646257,"discussion_content":"这是模型的能力问题吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1717648364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":384925,"user_name":"D.L","can_delete":false,"product_type":"c1","uid":2735432,"ip_address":"湖北","ucode":"0F6708DD81F642","user_header":"https://static001.geekbang.org/account/avatar/00/29/bd/48/5d892c11.jpg","comment_is_top":false,"comment_ctime":1701790783,"is_pvip":false,"replies":[{"id":140847,"content":"有的时候我也遇到这样情况，有时候我的Windows系统跑不了，同样的环境Linux可以。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1704817058,"ip_address":"瑞士","comment_id":384925,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"黄老师，请教一下：\n课程中有段代码\n```\n# 初始化示例选择器\nexample_selector = SemanticSimilarityExampleSelector.from_examples( samples, \n     OpenAIEmbeddings(), \n     Chroma,\n     k=1)\n```\n\n将`Chroma`换成`Qdrant`，则会报错，报错信息如下(报错太长，省略部分）\n```\nTraceback (most recent call last):\n  File &quot;&#47;python3.10&#47;site-packages&#47;langchain&#47;vectorstores&#47;qdrant.py&quot;, line 1584, in construct_instance\n    collection_info = client.get_collection(collection_name=collection_name)\n    ... ...\n    ... ...\n    ... ...\n  File &quot;&#47;python3.10&#47;site-packages&#47;qdrant_client&#47;http&#47;api_client.py&quot;, line 97, in send\n    raise UnexpectedResponse.for_response(response)\nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 502 (Bad Gateway)\nRaw response content:\nb&#39;&#39;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;&#47;langchan_learning&#47;05-demo.py&quot;, line 43, in &lt;module&gt;\n    example_selector = SemanticSimilarityExampleSelector.from_examples(\n  File &quot;&#47;python3.10&#47;site-packages&#47;langchain_core&#47;example_selectors&#47;semantic_similarity.py&quot;, line 97, in from_examples\n    vectorstore = vectorstore_cls.from_texts(\n  File &quot;&#47;python3.10&#47;site-packages&#47;langchain&#47;vectorstores&#47;qdrant.py&quot;, line 1301, in from_texts\n    qdrant = cls.construct_instance(\n    ... ...\n    ... ...\n    ... ... \n  File &quot;&#47;python3.10&#47;site-packages&#47;qdrant_client&#47;http&#47;api_client.py&quot;, line 97, in send\n    raise UnexpectedResponse.for_response(response)\nqdrant_client.http.exceptions.UnexpectedResponse: Unexpected Response: 502 (Bad Gateway)\nRaw response content:\nb&#39;&#39;\n\n```\n\n","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635430,"discussion_content":"有的时候我也遇到这样情况，有时候我的Windows系统跑不了，同样的环境Linux可以。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704817058,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382394,"user_name":"一面湖水","can_delete":false,"product_type":"c1","uid":1193473,"ip_address":"四川","ucode":"6339F033312FB2","user_header":"https://static001.geekbang.org/account/avatar/00/12/36/01/eb3ba274.jpg","comment_is_top":false,"comment_ctime":1697177571,"is_pvip":false,"replies":[{"id":139444,"content":"具体错误同学贴一下？","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1698139510,"ip_address":"瑞士","comment_id":382394,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"# 初始化示例选择器example_selector = SemanticSimilarityExampleSelector.from_examples(    samples,    OpenAIEmbeddings(),    Chroma,    k=1)\n这里改成：Qdrant会报错呢？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":630148,"discussion_content":"具体错误同学贴一下？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1698139510,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382225,"user_name":"杨松","can_delete":false,"product_type":"c1","uid":1512517,"ip_address":"辽宁","ucode":"FB3C73BC44C94D","user_header":"https://static001.geekbang.org/account/avatar/00/17/14/45/adf079ae.jpg","comment_is_top":false,"comment_ctime":1696922426,"is_pvip":false,"replies":[{"id":139275,"content":"你好，让我们逐步解析代码来回答你的问题。\n在我们的代码中，FewShotPromptTemplate 对象是基于给定示例和示例提示来生成提示的。这个对象的目标是，根据给定的输入变量，选择一个或多个最相关的示例，并结合示例提示来生成一个完整的提示。\nexample_selector 的作用是从所有的示例中选择最相关的示例。代码中，example_selector 使用了语义相似度来选择最相关的示例，只选择一个最相关的示例（k=1）。\nexample_prompt 的作用是为选择的示例提供一个格式，将它们转换为一个可读的提示部分。\n回到你的问题：玫瑰的那个文字是example_selector生成的，还是example_prompt生成的，或者是两者同事生成后去重了？\n玫瑰的那个文字（以及关于玫瑰的整个描述）是由 example_selector 从 samples 中选择出来的。具体来说，example_selector 认为“玫瑰”是与“红玫瑰”最相关的示例。然后，这个选出的示例被传递给 example_prompt 来格式化，产生“鲜花类型: 玫瑰\\n场合: 爱情\\n文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。”这部分的输出。\n接下来，suffix 提供了一个方式来结合输入变量（在这里是 &quot;红玫瑰&quot; 和 &quot;爱情&quot;）与选择的示例描述，从而得到最终的完整提示。这个过程没有去重操作。只是 example_selector 选择了最相关的示例，然后 example_prompt 将其格式化，再结合 suffix 生成最终的提示。","user_name":"作者回复","user_name_real":"作者","uid":1809833,"ctime":1697166605,"ip_address":"瑞士","comment_id":382225,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100617601,"comment_content":"老师您好，请教个问题，文中代码：\n# 创建一个使用示例选择器的FewShotPromptTemplate对象\nprompt = FewShotPromptTemplate(    \nexample_selector=example_selector,\n example_prompt=prompt_sample, \n suffix=&quot;鲜花类型: {flower_type}\\n场合: {occasion}&quot;, \n input_variables=[&quot;flower_type&quot;, &quot;occasion&quot;])\nprint(prompt.format(flower_type=&quot;红玫瑰&quot;, occasion=&quot;爱情&quot;))\n的打印结果为：\n鲜花类型: 玫瑰\n场合: 爱情\n文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。\n\n鲜花类型: 红玫瑰\n场合: 爱情\n我的问题是 玫瑰的那个文字是example_selector生成的，还是example_prompt生成的，或者是两者同事生成后去重了？","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629517,"discussion_content":"你好，让我们逐步解析代码来回答你的问题。\n在我们的代码中，FewShotPromptTemplate 对象是基于给定示例和示例提示来生成提示的。这个对象的目标是，根据给定的输入变量，选择一个或多个最相关的示例，并结合示例提示来生成一个完整的提示。\nexample_selector 的作用是从所有的示例中选择最相关的示例。代码中，example_selector 使用了语义相似度来选择最相关的示例，只选择一个最相关的示例（k=1）。\nexample_prompt 的作用是为选择的示例提供一个格式，将它们转换为一个可读的提示部分。\n回到你的问题：玫瑰的那个文字是example_selector生成的，还是example_prompt生成的，或者是两者同事生成后去重了？\n玫瑰的那个文字（以及关于玫瑰的整个描述）是由 example_selector 从 samples 中选择出来的。具体来说，example_selector 认为“玫瑰”是与“红玫瑰”最相关的示例。然后，这个选出的示例被传递给 example_prompt 来格式化，产生“鲜花类型: 玫瑰\\n场合: 爱情\\n文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。”这部分的输出。\n接下来，suffix 提供了一个方式来结合输入变量（在这里是 \"红玫瑰\" 和 \"爱情\"）与选择的示例描述，从而得到最终的完整提示。这个过程没有去重操作。只是 example_selector 选择了最相关的示例，然后 example_prompt 将其格式化，再结合 suffix 生成最终的提示。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697166605,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"瑞士","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2080112,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/bd/70/1dba9869.jpg","nickname":"Michael","note":"","ucode":"41E34E4F58144C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":632440,"discussion_content":"老师，在找出最相似的向量后，不是也要发给chatGPT吗？为什么说会省token呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1700785993,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":629517,"ip_address":"浙江","group_id":0},"score":632440,"extra":""}]}]},{"had_liked":false,"id":381396,"user_name":"Yimmy","can_delete":false,"product_type":"c1","uid":1177248,"ip_address":"北京","ucode":"E66EAF7050C74A","user_header":"https://static001.geekbang.org/account/avatar/00/11/f6/a0/8ea0bfba.jpg","comment_is_top":false,"comment_ctime":1695136416,"is_pvip":false,"replies":[{"id":138934,"content":"谢谢同学的细心，我们已调整代码！","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695179971,"ip_address":"新加坡","comment_id":381396,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"template=&quot;鲜花类型: {flower_type}\\n场合: {occasion}\\n文案: {ad_copy}&quot;)--这里多了括号)","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628307,"discussion_content":"谢谢同学的细心，我们已调整代码！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695179971,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381274,"user_name":"阿斯蒂芬","can_delete":false,"product_type":"c1","uid":1024164,"ip_address":"广东","ucode":"61D5E3BDA4EBC5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg","comment_is_top":false,"comment_ctime":1694959636,"is_pvip":false,"replies":[{"id":138895,"content":"嗯，对啊。做垂直领域的对话机器人，工程上的东西是很多的。Prompt Engineering是必须的。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1695011610,"ip_address":"新加坡","comment_id":381274,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"假设做一个垂直领域的对话机器人，是否可以将用户的query通过一定的转换方式，比如embedding去“匹配”一些 Prompt，且增加Few or One example 的参考示例，相当于将query加工后，形成更加“指令化”的请求，便于让模型往更清晰地“理解”用户的说法，并按照更“步骤化”的方式去思考并给出答案。\n也就是说，实际上用户与模型的交互，中间还是隐藏了很多“工程师加工”细节～\n\n另外 ExampleSelector 的意义看起来是将 example 的匹配进行高层抽象，屏蔽细节，应该除了 Embbdings 方式，还有其他种种获取 example 的方式。使用Selector还有好处就是可以通过examples的瘦身节省tokens。","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628152,"discussion_content":"嗯，对啊。做垂直领域的对话机器人，工程上的东西是很多的。Prompt Engineering是必须的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695011610,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381020,"user_name":"骨汤鸡蛋面","can_delete":false,"product_type":"c1","uid":1050002,"ip_address":"上海","ucode":"2AC141A523E710","user_header":"https://static001.geekbang.org/account/avatar/00/10/05/92/b609f7e3.jpg","comment_is_top":false,"comment_ctime":1694589941,"is_pvip":false,"replies":[{"id":138777,"content":"这个猜测很有意思哦。也很可能。\n当然OpenAI 在训练 GPT-3 和之前版本的 GPT 时使用了多种数据源和技术，但具体的细节并没有公开。\n关于 GPT-3 或 ChatGPT 对特定提示 (prompt) 的表现，确实，如果模型在训练数据中遇到了类似的措辞或结构，它在对此类提示做出回应时通常会有更好的效果。\n不过，我想，LangChain的愿景可能是对各种LLM都能够用通用的、一致的、清晰的标准提示词搞定所有模型。","user_name":"作者回复","user_name_real":"编辑","uid":1809833,"ctime":1694593046,"ip_address":"新加坡","comment_id":381020,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"langchain 的很多提示都是针对chatgpt的，是不是可以理解为，chatgpt在进行sft的时候，数据集就用上了langchain很多prompt的措辞方式，所以chatgpt对这些prompt的效果也很好。","like_count":0,"discussions":[{"author":{"id":1809833,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/a9/4602808f.jpg","nickname":"黄佳","note":"","ucode":"8EC41D2EAB0E3C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627834,"discussion_content":"这个猜测很有意思哦。也很可能。\n当然OpenAI 在训练 GPT-3 和之前版本的 GPT 时使用了多种数据源和技术，但具体的细节并没有公开。\n关于 GPT-3 或 ChatGPT 对特定提示 (prompt) 的表现，确实，如果模型在训练数据中遇到了类似的措辞或结构，它在对此类提示做出回应时通常会有更好的效果。\n不过，我想，LangChain的愿景可能是对各种LLM都能够用通用的、一致的、清晰的标准提示词搞定所有模型。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1694593047,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392483,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1721050441,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"用国产模型改写：03_FewShotPrompt.py\n用阿里云的通义千问模型 改写  OpenAI 模型。\n改写OpenAIEmbeddings，由于使用阿里云embeddings模型代码复杂，采用百川智能的 BaichuanTextEmbeddings \n完整代码如下：\n————\n\n# 1. 创建一些示例\n&#39;&#39;&#39;  原代码 &#39;&#39;&#39;\n\n# 2. 创建一个提示模板\n&#39;&#39;&#39;  原代码 &#39;&#39;&#39;\n\n# 3. 创建一个FewShotPromptTemplate对象\n&#39;&#39;&#39;  原代码 &#39;&#39;&#39;\n\n# 4. 把提示传递给大模型\n# import os\n# os.environ[&quot;OPENAI_API_KEY&quot;] = &#39;你的OpenAI API Key&#39;\n# from langchain.llms import OpenAI  #不采用OpenAI\n# model = OpenAI(model_name=&#39;text-davinci-003&#39;)  \n\nfrom langchain_openai import ChatOpenAI # 导入ChatOpenAI类\nModel = ChatOpenAI(\n    api_key= DASHSCOPE_API_KEY,  # 请在此处用您的阿里云API Key进行替换\n    base_url=&quot;https:&#47;&#47;dashscope.aliyuncs.com&#47;compatible-mode&#47;v1&quot;, # DashScope base_url\n    model=&quot;qwen-plus&quot; # 阿里云的模型\n    )\n# result = model(prompt.format(flower_type=&quot;野玫瑰&quot;, occasion=&quot;爱情&quot;))\nresult = Model(prompt.format(flower_type=&quot;野玫瑰&quot;, occasion=&quot;爱情&quot;))\n# print(result) \nprint(result.content) # 输出结果 content属性\n\n# 5. 使用示例选择器\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Qdrant\n# from langchain.embeddings import OpenAIEmbeddings # 不采用OpenAI\nfrom langchain_community.embeddings import BaichuanTextEmbeddings # 导入百川智能嵌入模型\n\n# 初始化百川智能嵌入模型\nembeddings = BaichuanTextEmbeddings(baichuan_api_key=  BAICHUAN_API_KEY)  # 请在此处用您的百川 API Key进行替换\n\n# 初始化示例选择器\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    samples,\n#  OpenAIEmbeddings(),  # 不采用OpenAI\n    embeddings, # 使用百川智能嵌入模型代替OpenAIEmbeddings\n    Qdrant,\n    k=1\n)\n\n# 创建一个使用示例选择器的FewShotPromptTemplate对象\n&#39;&#39;&#39;  原代码 &#39;&#39;&#39;","like_count":3,"discussions":[{"author":{"id":2011138,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/b0/02/98f8b0ee.jpg","nickname":"方华Elton","note":"","ucode":"180D03AADB5D2C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":648740,"discussion_content":"很棒的尝试","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1722079631,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江苏","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392374,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1720750491,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":2,"score":3,"product_id":100617601,"comment_content":"第4讲打卡~\n在LLM应用中，除了准确性和性能，成本也是一个重要的评价指标，SemanticSimilarityExampleSelector可以通过比较余弦相似度，从所有示例中选取出于目标问题语义上最相近的k条示例，在很大程度上可以节省token的消耗，降低成本","like_count":1},{"had_liked":false,"id":394989,"user_name":"rs勿忘初心","can_delete":false,"product_type":"c1","uid":1519200,"ip_address":"北京","ucode":"557D1ECD757195","user_header":"https://static001.geekbang.org/account/avatar/00/17/2e/60/4fa1f3bd.jpg","comment_is_top":false,"comment_ctime":1729045234,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"记住评论区老师的回复：先不用不必苦钻细节，重点先了解LangChain的思想，以及它未来的可能性，知道它能做什么，不能做什么。","like_count":0},{"had_liked":false,"id":394321,"user_name":"极客星星","can_delete":false,"product_type":"c1","uid":1010337,"ip_address":"中国香港","ucode":"73BA4E96CCB934","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6a/a1/6270eeb7.jpg","comment_is_top":false,"comment_ctime":1726481655,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"咨询下，langchain定义的fewshot模板，把fewshot的格式进行了限制，有几个疑问：\n1 这个fewshot模板，对于所有模型都适用吗，实际使用时，需不需要根据不同模型对模板进行调整\n2 如果需要调整，能否定义自己的fewshot模板\n谢谢","like_count":0},{"had_liked":false,"id":389785,"user_name":"爬行的蜗牛","can_delete":false,"product_type":"c1","uid":1033956,"ip_address":"四川","ucode":"6623B62DE63CE9","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/QD6bf8hkS5dHrabdW7M7Oo9An1Oo3QSxqoySJMDh7GTraxFRX77VZ2HZ13x3R4EVYddIGXicRRDAc7V9z5cLDlA/132","comment_is_top":false,"comment_ctime":1713447735,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"最后一段代码需要修改下：\n# 5. 使用示例选择器\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nopenai_embeddings = OpenAIEmbeddings()  # 实例化 OpenAIEmbeddings 类\n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    samples,\n    #OpenAIEmbeddings,\n    openai_embeddings,\n    Chroma,\n    k = 1\n)\n\n\n\n# 创建一个使用示例选择器的 FewShotPromptTemplate 对象\nprompt = FewShotPromptTemplate(\n    example_selector = example_selector,\n    example_prompt = prompt_sample,\n    suffix = &quot;鲜花类型:{flower_type}\\n场合：{occasion}&quot;,\n    input_variables = [&quot;flower_type&quot;,&quot;occasion&quot;]\n)\nprint(prompt.format(flower_type=&#39;红玫瑰&#39;, occasion=&#39;爱情&#39;))","like_count":0},{"had_liked":false,"id":389715,"user_name":"爬行的蜗牛","can_delete":false,"product_type":"c1","uid":1033956,"ip_address":"四川","ucode":"6623B62DE63CE9","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/QD6bf8hkS5dHrabdW7M7Oo9An1Oo3QSxqoySJMDh7GTraxFRX77VZ2HZ13x3R4EVYddIGXicRRDAc7V9z5cLDlA/132","comment_is_top":false,"comment_ctime":1713314908,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100617601,"comment_content":"from openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=&quot;gpt-3.5-turbo&quot;,\n  messages=[\n    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},\n    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}\n  ]\n)\n我的 C:\\Users\\Admin&gt;openai --version\nopenai 1.14. 运行这个会报错， 解办法：from openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=&quot;gpt-3.5-turbo&quot;,\n  messages=[\n    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},\n    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}\n  ]\n)","like_count":0}]}