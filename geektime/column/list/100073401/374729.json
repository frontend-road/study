{"id":374729,"title":"28 | 大表Join大表（一）：什么是“分而治之”的调优思路？","content":"<p>你好，我是吴磊。</p><p>上一讲，我们探讨了“大表Join小表”场景的调优思路和应对方法。那么，除了大表Join小表的场景，数据分析领域有没有“大表Join大表”的场景呢？确实是有的，它指的是参与Join的两张体量较大的事实表，尺寸相差在3倍以内，且全部无法放进广播变量。</p><p>但是通常来说，在数据分析领域，用一张大表去关联另一张大表，这种做法在业内是极其不推荐的。甚至毫不客气地说，“大表Join大表”是冒天下之大不韪，犯了数据分析的大忌。如果非要用“大表Join大表”才能实现业务逻辑、完成数据分析，这说明数据仓库在设计之初，开发者考虑得不够完善、看得不够远。</p><p>不过，你可能会说：“我刚入职的时候，公司的数仓就已经定型了，这又不是我的锅，我也只能随圆就方。”为了应对这种情况，今天这一讲我们就来说说，当你不得不面对“大表Join大表”的时候，还有哪些调优思路和技巧。</p><p>要应对“大表Join大表”的计算场景，我们主要有两种调优思路。<strong>一种叫做“分而治之”，另一种我把它统称为“负隅顽抗”。</strong>今天这一讲，我们先来说说“分而治之”，“负隅顽抗”我们留到下一讲再去展开。</p><p>值得一提的是，即便你不需要去应对“大表Join大表”这块烫手的山芋，“分而治之”与“负隅顽抗”所涉及的调优思路与方法，也非常值得我们花时间去深入了解，因为这些思路与方法的可迁移性非常强，学习过后你会发现，它们完全可以拿来去应对其他的应用场景。</p><!-- [[[read_end]]] --><p>话不多说，我们直接开始今天的课程吧！</p><h2>如何理解“分而治之”？</h2><p>“分而治之”的调优思路是把“大表Join大表”降级为“大表Join小表”，然后使用上一讲中“大表Join小表”的调优方法来解决性能问题。它的核心思想是，<strong>先把一个复杂任务拆解成多个简单任务，再合并多个简单任务的计算结果</strong>。那么，“大表Join大表”的场景是如何应用“分而治之”的计算思想的呢？</p><p>首先，我们要根据两张表的尺寸大小区分出外表和内表。一般来说，内表是尺寸较小的那一方。然后，我们人为地在内表上添加过滤条件，把内表划分为多个不重复的完整子集。接着，我们让外表依次与这些子集做关联，得到部分计算结果。最后，再用Union操作把所有的部分结果合并到一起，得到完整的计算结果，这就是端到端的关联计算。整个“分而治之”的计算过程如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/b7/36/b7f69a554c2e5745625ea1aa969e0136.jpg?wh=6069*1974\" alt=\"\" title=\"大表Join大表的“分而治之”\"></p><h2>如何保证内表拆分的粒度足够细？</h2><p>采用“分而治之”的核心目的在于，将“大表Join大表”转化为“大表Join小表”，因此<strong>“分而治之”中一个关键的环节就是内表拆分，我们要求每一个子表的尺寸相对均匀，且都小到可以放进广播变量</strong>。只有这样，原本的Shuffle Join才能转化成一个又一个的Broadcast Joins，原本的海量数据Shuffle才能被消除，我们也才能因此享受到性能调优的收益。相反，如果内表拆分不能满足上述条件，我们就“白忙活”了。</p><p><strong>拆分的关键在于拆分列的选取</strong>，为了让子表足够小，拆分列的基数（Cardinality）要足够大才行。这么说比较抽象，我们来举几个例子。假设内表的拆分列是“性别”，性别的基数是2，取值分别是“男”和“女”。我们根据过滤条件 “性别 = 男”和“性别 = 女”把内表拆分为两份，显然，这样拆出来的子表还是很大，远远超出广播阈值。</p><p>你可能会说：“既然性别的基数这么低，不如我们选择像身份证号这种基数大的数据列。”身份证号码的基数确实足够大，就是全国的人口数量。但是，身份证号码这种基数比较大的字符串充当过滤条件有两个缺点：一，不容易拆分，开发成本太高；二，过滤条件很难享受到像谓词下推这种Spark SQL的内部优化机制。</p><p>既然基数低也不行、高也不行，那到底什么样的基数合适呢？通常来说，在绝大多数的数仓场景中，事实表上都有与时间相关的字段，比如日期或是更细致的时间戳。这也是很多事实表在建表的时候，都是以日期为粒度做分区存储的原因。因此，选择日期作为拆分列往往是个不错的选择，既能享受到Spark SQL分区剪裁（Partition Pruning）的性能收益，同时开发成本又很低。</p><h2>如何避免外表的重复扫描？</h2><p>内表拆分之后，外表就要分别和所有的子表做关联，尽管每一个关联都变成了“大表Join小表”并转化为BHJ，但是在Spark的运行机制下，每一次关联计算都需要重新、重头扫描外表的全量数据。毫无疑问，这样的操作是让人无法接受的。这就是“分而治之”中另一个关键的环节：外表的重复扫描。</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/9c/9fab5a256d544ef2b1f895c4990f4e9c.jpg?wh=3549*1509\" alt=\"\" title=\"外表的重复扫描\"></p><p>我们以上图为例，内表被拆分为4份，原本两个大表的Shuffle Join，被转化为4个Broadcast Joins。外表分别与4个子表做关联，所有关联的结果集最终通过Union合并到一起，完成计算。对于这4个关联来说，每一次计算都需要重头扫描一遍外表。换句话说，外表会被重复扫描4次。显然，外表扫描的次数取决于内表拆分的份数。</p><p>我们刚刚说到，内表的拆分需要足够细致，才能享受到性能调优带来的收益，而这往往意味着，内表拆分的份数成百上千、甚至成千上万。在这样的数量级之下，重复扫描外表带来的开销是巨大的。</p><p>要解决数据重复扫描的问题，办法其实不止一种，我们最容易想到的就是Cache。确实，如果能把外表的全量数据缓存到内存中，我们就不必担心重复扫描的问题，毕竟内存的计算延迟远低于磁盘。但是，我们面临的情况是外表的数据量非常地庞大，往往都是TB级别起步，想要把TB体量的数据全部缓存到内存，这要求我们的计算集群在资源配置上要足够的强悍，再说直白一点，你要有足够的预算去配置足够大的内存。</p><p>要是集群没这么强悍，老板也不给批预算去扩容集群内存，我们该怎么办呢？</p><p>我们还是要遵循“分而治之”的思想，既然内表可以“分而治之”，外表为什么不可以呢？<strong>对于外表参与的每一个子关联，在逻辑上，我们完全可以只扫描那些与内表子表相关的外表数据，并不需要每次都扫描外表的全量数据。</strong>如此一来，在效果上，外表的全量数据仅仅被扫描了一次。你可能会说：“说得轻巧，逻辑上是没问题，但是具体怎么做到外表的“分而治之”呢？”</p><p>这事要是搁到以前还真是没什么操作空间，但是，学习过Spark 3.0的DPP机制之后，我们就可以利用DPP来对外表进行“分而治之”。</p><p><img src=\"https://static001.geekbang.org/resource/image/fa/23/fa4bfb52cb42928f15b1dc7c37c30b23.jpg?wh=5109*2034\" alt=\"\" title=\"外表的“分而治之”\"></p><p>假设外表的分区键包含Join Keys，那么，每一个内表子表都可以通过DPP机制，帮助与之关联的外表减少数据扫描量。如上图所示，步骤1、2、3、4分别代表外表与4个不同子表的关联计算。以步骤1为例，在DPP机制的帮助下，要完成关联计算，外表只需要扫描与绿色子表对应的分区数据即可，如图中的两个绿色分区所示。同理，要完成步骤4的关联计算，外表只需要扫描与紫色子表对应的分区即可，如图中左侧用紫色标记的两个数据分区。</p><p>不难发现，每个子查询只扫描外表的一部分、一个子集，所有这些子集加起来，刚好就是外表的全量数据。因此，利用“分而治之”的调优技巧，端到端的关联计算仅需对外表做一次完整的全量扫描即可。如此一来，在把原始的Shuffle Join转化为多个Broadcast Joins之后，我们并没有引入额外的性能开销。毫无疑问，查询经过这样的调优过后，执行效率一定会有较大幅度的提升。</p><p>但是，你可能会说：“说了半天，都是一些思路和理论，要实现“分而治之”，代码该怎么写呢？”接下来，我们就结合一个小例子一起去实战一下“分而治之”的优化思路。</p><h2>“分而治之”调优思路实战</h2><p>这个实战例子来自于一个跨境电商，这家电商在全球范围内交易大型组装设备，这些设备的零部件来自于全球不同地区的不同供货商，因此一个设备订单往往包含多个零部件明细。这家电商使用orders表和transactions表来分别记录订单和交易明细，两张表的关键字段如下表所示。</p><pre><code>//orders表的关键字段\norderId: Int\ncustomerId: Int\nstatus: String\ndate: Date //分区键\n \n//lineitems表的关键字段\norderId: Int //分区键\ntxId: Int\nitemId: Int\nprice: Float\nquantity: Int\n\n</code></pre><p>orders和transactions都是事实表，体量都在TB级别。基于这两张事实表，这家电商每隔一段时间，就会计算上一个季度所有订单的交易额，业务代码如下所示。</p><pre><code>//统计订单交易额的代码实现\nval txFile: String = _\nval orderFile: String = _\n \nval transactions: DataFrame = spark.read.parquent(txFile)\nval orders: DataFrame = spark.read.parquent(orderFile)\n \ntransactions.createOrReplaceTempView(&quot;transactions&quot;)\norders.createOrReplaceTempView(&quot;orders&quot;)\n \nval query: String = &quot;\nselect sum(tx.price * tx.quantity) as revenue, o.orderId\nfrom transactions as tx inner join orders as o\non tx.orderId = o.orderId\nwhere o.status = 'COMPLETE'\nand o.date between '2020-01-01' and '2020-03-31'\ngroup by o.orderId\n&quot;\n \nval outFile: String = _\nspark.sql(query).save.parquet(outFile)\n\n</code></pre><p>不难发现，在两张表的关联计算中，transactions的角色是外表，自然 orders的角色就是内表。需要指出的是，即便内表中有不少过滤条件，如订单状态为“完成”且成交日期满足一定范围，但过滤之后的内表仍然在百GB量级，难以放入广播变量。因此，这两张大表的关联计算，自然会退化到Shuffle Joins的实现机制。</p><p>那么，如果用“分而治之”的思路来做优化，代码应该怎么改呢？“分而治之”有两个关键因素，也就是内表拆分和外表重复扫描。我们不妨从这两个因素出发来调整原来的代码。</p><p>首先，内表拆分是否合理完全取决于拆分列的选取，而候选拆分列要同时满足基数适中、子表分布均匀，并且子表尺寸小于广播阈值等多个条件。纵观orders表的所有关键字段，只有date字段能够同时满足这些条件。因此，我们可以使用date字段，以天为单位对orders表做拆分，那么原代码中的查询语句需要作如下调整。</p><pre><code>//以date字段拆分内表\nval query: String = &quot;\nselect sum(tx.price * tx.quantity) as revenue, o.orderId\nfrom transactions as tx inner join orders as o\non tx.orderId = o.orderId\nwhere o.status = 'COMPLETE'\nand o.date = '2020-01-01'\ngroup by o.orderId\n&quot;\n\n</code></pre><p>你可能会说：“这不对吧，业务需求是计算一个季度的交易额，查询这么改不是只计算一天的量吗？”别着急，代码的调整还差一步：外表重复扫描。内表拆分之后，外表自然要依次与所有的子表做关联，最终把全部子关联的结果合并到一起，才算是完成“分而治之”的实现。</p><pre><code>//循环遍历dates、完成“分而治之”的计算\nval dates: Seq[String] = Seq(&quot;2020-01-01&quot;, &quot;2020-01-02&quot;, … &quot;2020-03-31&quot;)\n \nfor (date &lt;- dates) {\n \nval query: String = s&quot;\nselect sum(tx.price * tx.quantity) as revenue, o.orderId\nfrom transactions as tx inner join orders as o\non tx.orderId = o.orderId\nwhere o.status = 'COMPLETE'\nand o.date = ${date}\ngroup by o.orderId\n&quot;\n \nval file: String = s&quot;${outFile}/${date}&quot;\nspark.sql(query).save.parquet(file)\n}\n</code></pre><p>再次调整后的代码如上表所示，我们利用一个简单的for循环来遍历日期，从而让外表依次与子表做关联，并把子关联的计算结果直接写到outFile根目录下的子目录。代码的改动还是很简单的。不过，细心的你可能会发现：“这种写法，不是我们一直要极力避免的单机思维模式吗？”没错，单纯从写法上来看，这份代码的“单机思维”味道非常浓厚。</p><p>不过，对于“单机思维模式”的理解，我们不能仅仅停留在形式或是表面上。所谓单机思维模式，它指的是开发者不假思索地直入面向过程编程，忽略或无视分布式数据实体的编程模式。但在刚刚整理调优思路的过程中，我们一直把外表的重复扫描牢记于心，并想到通过利用DPP机制来避免它。因此，虽然我们使用了for循环，但并不会在运行时引入分布式数据集的重复扫描。</p><p>总的来说，在这个案例中，利用“分而治之”的调优方法，我们可以把所有“大表Join大表”的关联查询转化为“大表Join小表”，把原始的Shuffle Join转化为多个Broadcast Joins，而且Broadcast Joins又可以有效应对关联中的数据倾斜问题，可以说是一举两得。</p><h2>小结</h2><p>“大表Join大表”的第一种调优思路是“分而治之”，我们要重点掌握它的调优思路以及两个关键环节的优化处理。</p><p>“分而治之”的核心思想是通过均匀拆分内表的方式 ，把一个复杂而又庞大的Shuffle Join转化为多个Broadcast Joins，它的目的是，消除原有Shuffle Join中两张大表所引入的海量数据分发，大幅削减磁盘与网络开销的同时，从整体上提升作业端到端的执行性能。</p><p>在“分而治之”的调优过程中，内表的拆分最为关键，因为它肩负着Shuffle Join能否成功转化为Broadcast Joins的重要作用。而拆分的关键在于拆分列的选取。为了兼顾执行性能与开发效率，拆分列的基数要足够大，这样才能让子表小到足以放进广播变量，但同时，拆分列的基数也不宜过大，否则实现“分而治之”的开发成本就会陡然上升。通常来说，日期列往往是个不错的选择。</p><p>为了避免在调优的过程中引入额外的计算开销，我们要特别注意外表的重复扫描问题。针对外表的重复扫描，我们至少有两种应对方法。第一种是将外表全量缓存到内存，不过这种方法对于内存空间的要求较高，不具备普适性。第二种是利用Spark 3.0版本推出的DPP特性，在数仓设计之初，就以Join Key作为分区键，对外表做分区存储。</p><p>当我们做好了内表拆分，同时也避免了外表的重复扫描，我们就可以把原始的Shuffle Join转化为多个Broadcast Joins，在消除海量数据在全网分发的同时，避免引入额外的性能开销。那么毫无疑问，查询经过“分而治之”的调优过后，作业端到端的执行性能一定会得到大幅提升。</p><h2>每日一练</h2><p>在大表数据分布均匀的情况下，如果我们采用“分而治之”的调优技巧，要避免外表的重复扫描，除了采用缓存或是DPP机制以外，还有哪些其他办法？</p><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","comments":[{"had_liked":false,"id":327279,"user_name":"赵鹏举","can_delete":false,"product_type":"c1","uid":1260182,"ip_address":"","ucode":"B3785788D6176C","user_header":"https://static001.geekbang.org/account/avatar/00/13/3a/96/9fddfb4a.jpg","comment_is_top":false,"comment_ctime":1640047313,"is_pvip":false,"replies":[{"id":"119393","content":"1）SQL也是可以的哈~ 不过得用到spark-sql CLI，或是Beeline这样的客户端<br>2）2.x还不行，这里需要利用到3.x才有的DPP机制","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1640356344,"ip_address":"","comment_id":327279,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14524949201","product_id":100073401,"comment_content":"1 优化过程是scalla写的，纯SQL有没有实现吗？<br>2 spark版本为2.4.3有办法达到类似效果吗？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":541403,"discussion_content":"1）SQL也是可以的哈~ 不过得用到spark-sql CLI，或是Beeline这样的客户端\n2）2.x还不行，这里需要利用到3.x才有的DPP机制","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640356344,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":307215,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1628942001,"is_pvip":false,"replies":[{"id":"111314","content":"好问题，colocated joins很有意思，它其实特指的是这种情况，参与Join的两张表：<br>1. 并行度一样<br>2. 以Join Key为基准，数据分布一致<br><br>满足这两个条件的两张表参与的Join，就叫colocated joins。<br><br>其实，说白了，就是两张表的数据分布完全一致（相对Join Key来说）。因此，对他们做关联，自然不再需要Shuffle环节，因为两张表的数据分片都已经相互“配好对”了，两边Join Keys相同的数据记录，都分布在同样Executors中。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629106152,"ip_address":"","comment_id":307215,"utype":1}],"discussion_count":4,"race_medal":0,"score":"5923909297","product_id":100073401,"comment_content":"老师，对于评论区里的说到的colocated joins，可否简单说下运行机制？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525082,"discussion_content":"好问题，colocated joins很有意思，它其实特指的是这种情况，参与Join的两张表：\n1. 并行度一样\n2. 以Join Key为基准，数据分布一致\n\n满足这两个条件的两张表参与的Join，就叫colocated joins。\n\n其实，说白了，就是两张表的数据分布完全一致（相对Join Key来说）。因此，对他们做关联，自然不再需要Shuffle环节，因为两张表的数据分片都已经相互“配好对”了，两边Join Keys相同的数据记录，都分布在同样Executors中。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629106152,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1260182,"avatar":"https://static001.geekbang.org/account/avatar/00/13/3a/96/9fddfb4a.jpg","nickname":"赵鹏举","note":"","ucode":"B3785788D6176C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":540780,"discussion_content":"2.4环境下应该怎样调用该功能呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640165521,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":525082,"ip_address":""},"score":540780,"extra":""}]},{"author":{"id":1086505,"avatar":"https://static001.geekbang.org/account/avatar/00/10/94/29/2ce8ffd5.jpg","nickname":"令狐冲。🍀 🌻 🎸","note":"","ucode":"5B46B5CE1C57C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530768,"discussion_content":"如果两个表是基于相同的key进行分桶，是不是也是colocated joins？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637144448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":1,"child_discussions":[{"author":{"id":1401217,"avatar":"https://static001.geekbang.org/account/avatar/00/15/61/81/9e34165b.jpg","nickname":"rovernerd","note":"","ucode":"C8620AAF0E4920","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1086505,"avatar":"https://static001.geekbang.org/account/avatar/00/10/94/29/2ce8ffd5.jpg","nickname":"令狐冲。🍀 🌻 🎸","note":"","ucode":"5B46B5CE1C57C3","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":542612,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640792965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":530768,"ip_address":""},"score":542612,"extra":""}]}]},{"had_liked":false,"id":294273,"user_name":"威猛的小老虎","can_delete":false,"product_type":"c1","uid":1785715,"ip_address":"","ucode":"817F254E6D28E2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ2tyVlQiaqp5kEOf1LlZB6nJPicN8lzcEkAsSEDmicvib8T3xUEcDibRichLrh3Qiclo3UyuAhhKNGQxGmg/132","comment_is_top":false,"comment_ctime":1621859459,"is_pvip":false,"replies":[{"id":"106769","content":"唯一的办法是把CPJ转化为BNLJ，如果内表不够小，没法广播，必须要用CPJ来实现，坦白说，还真没什么好办法。这也是我们为什么要极力避免CPJ的原因~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621866362,"ip_address":"","comment_id":294273,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5916826755","product_id":100073401,"comment_content":"笛卡尔积 如何优化呢","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520583,"discussion_content":"唯一的办法是把CPJ转化为BNLJ，如果内表不够小，没法广播，必须要用CPJ来实现，坦白说，还真没什么好办法。这也是我们为什么要极力避免CPJ的原因~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621866362,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1785715,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ2tyVlQiaqp5kEOf1LlZB6nJPicN8lzcEkAsSEDmicvib8T3xUEcDibRichLrh3Qiclo3UyuAhhKNGQxGmg/132","nickname":"威猛的小老虎","note":"","ucode":"817F254E6D28E2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375910,"discussion_content":"嗯，也是头疼。表自关联。在业务层面进行了一定的优化。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621866501,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293118,"user_name":"jerry guo","can_delete":false,"product_type":"c1","uid":1267753,"ip_address":"","ucode":"179943AFCB93F8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eo2GMhevabZrjINs2TKvIeGC7TJkicNlLvqTticuM5KL8ZN80OC2CnrsUyzPcZXO4uptj4Q1S4jT2lQ/132","comment_is_top":false,"comment_ctime":1621227046,"is_pvip":true,"replies":[{"id":"106294","content":"对，非常好~ 满分💯！你说的这个，其实就是collocated joins，collocated joins是一种特殊的joins，就是参与Join的两表提前按照Join Key都做了重分布，那么再次关联的时候，自然不会引入Shuffle~<br><br>不过，对于两个bucket表，要求他们的bucket id是一致的，都要包含Join Key，否则关联的时候还是会有Shuffle。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621414935,"ip_address":"","comment_id":293118,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5916194342","product_id":100073401,"comment_content":"如果外表是bucket表，内表也是相同的bucket表，那么也是可以避免多次扫描外表的。这种情况说不定币shj更好，因为不需要shuffle","like_count":2,"discussions":[{"author":{"id":2304851,"avatar":"https://static001.geekbang.org/account/avatar/00/23/2b/53/1adc2037.jpg","nickname":"风向决定发型","note":"","ucode":"7EE798C3ADFC11","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":410793,"discussion_content":"这个跟hive里面的桶表解决数据倾斜的思路是一致的","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1635777182,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520083,"discussion_content":"对，非常好~ 满分💯！你说的这个，其实就是collocated joins，collocated joins是一种特殊的joins，就是参与Join的两表提前按照Join Key都做了重分布，那么再次关联的时候，自然不会引入Shuffle~\n\n不过，对于两个bucket表，要求他们的bucket id是一致的，都要包含Join Key，否则关联的时候还是会有Shuffle。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1621414935,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293115,"user_name":"licl1008","can_delete":false,"product_type":"c1","uid":1265135,"ip_address":"","ucode":"6467D6DB97A00A","user_header":"","comment_is_top":false,"comment_ctime":1621223944,"is_pvip":false,"replies":[{"id":"106274","content":"好问题，DPP确实有这方面的限制，就是要求Join Key本身是分区键，这个要求确实比较苛刻。Join Key和分区键确实本身是一对矛盾，因为通常来说，Join Key的cardinality都不小，而分区键却要求cardinality不能太大。<br><br>你说的是对的，我们这个例子其实举的不好，这里为了demo DPP，我们选择了orderID，但实际上就像你说的，orderID本身的cardinality太大了，不适合做分区键。<br><br>所以说，利用DPP机制有个很重要的前提，就是数仓的设计，也就是结合后续常用的查询，提前把表结构设计好。这个表结构的设计，就需要考虑Join Keys和分区键的博弈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621411348,"ip_address":"","comment_id":293115,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5916191240","product_id":100073401,"comment_content":"老师 现实中用orderID作为分区键 是不是分区会太多 感觉订单ID基数很大","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520080,"discussion_content":"好问题，DPP确实有这方面的限制，就是要求Join Key本身是分区键，这个要求确实比较苛刻。Join Key和分区键确实本身是一对矛盾，因为通常来说，Join Key的cardinality都不小，而分区键却要求cardinality不能太大。\n\n你说的是对的，我们这个例子其实举的不好，这里为了demo DPP，我们选择了orderID，但实际上就像你说的，orderID本身的cardinality太大了，不适合做分区键。\n\n所以说，利用DPP机制有个很重要的前提，就是数仓的设计，也就是结合后续常用的查询，提前把表结构设计好。这个表结构的设计，就需要考虑Join Keys和分区键的博弈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621411348,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":342694,"user_name":"sweet smile","can_delete":false,"product_type":"c1","uid":2956361,"ip_address":"","ucode":"B0D0B3502AA7A1","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ4VFiaGZicIG5Fx9pMd8vibntD6E91IdzKgER10wJUSas2G8zib1pl5yzMFvkIA5zLBDB8Wa21xkynIw/132","comment_is_top":false,"comment_ctime":1650422063,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1650422063","product_id":100073401,"comment_content":"老师，有个疑问，就是咱们使用DPP的机制来防止外表进行重复扫描，但是DPP机制要求joinkey是分区字段。但是日常中joinkey往往是业务主键，如果把业务主键设计成分区字段，这样是不是不太合理呀。比如说咱们案例里面的orderID字段。","like_count":1,"discussions":[{"author":{"id":1296063,"avatar":"https://static001.geekbang.org/account/avatar/00/13/c6/bf/52b3f71d.jpg","nickname":"dawn","note":"","ucode":"1757B28F1EF5C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585941,"discussion_content":"一般不是时间字段来分区的嘛","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661914714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江苏"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":302998,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1626507939,"is_pvip":false,"replies":[{"id":"109816","content":"还有一种思路，就是系统集成。<br><br>之前有一个内存分布式文件系统，叫Tachyon，后来改名叫Alluxio，可以理解为内存里面的HDFS。它提供内存级别的文件系统。<br><br>Alluxio不少公司在用，就用来加速数据访问的，他们提前把“热数据”从HDFS加载到Alluxio，然后Spark再从Alluxio访问分布式内存文件，把磁盘I&#47;O变成了内存访问。读取效率自然能提升不少。<br><br>当然，做这种系统集成的前提是“有钱”，得有足够的预算来购置大量内存，不过，还是那句话，调优最重要的是思路和手段，每种方法都有其优缺点。确实有大厂宁愿多花钱买内存来节省时间。<br><br>总之，这种系统集成的思路也是一种优化选项，在条件具备的时候，不妨加以考虑~<br><br>其实沿着这个思路，你还可以发现更多集成的可能，Alluxio这里仅仅是举个例子加以说明，其实咱们作为开发者，平时要多关注技术动态和进展，开阔眼界和思路，我觉得这一点非常重要，个人观点哈~ 不一定对~<br><br>一个新技术，不管将来用不用得上，多了解、熟悉一些，总是不吃亏的~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1626775975,"ip_address":"","comment_id":302998,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1626507939","product_id":100073401,"comment_content":"问题回答：<br>除了shuffle，想不出其他办法....哭~","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523507,"discussion_content":"还有一种思路，就是系统集成。\n\n之前有一个内存分布式文件系统，叫Tachyon，后来改名叫Alluxio，可以理解为内存里面的HDFS。它提供内存级别的文件系统。\n\nAlluxio不少公司在用，就用来加速数据访问的，他们提前把“热数据”从HDFS加载到Alluxio，然后Spark再从Alluxio访问分布式内存文件，把磁盘I/O变成了内存访问。读取效率自然能提升不少。\n\n当然，做这种系统集成的前提是“有钱”，得有足够的预算来购置大量内存，不过，还是那句话，调优最重要的是思路和手段，每种方法都有其优缺点。确实有大厂宁愿多花钱买内存来节省时间。\n\n总之，这种系统集成的思路也是一种优化选项，在条件具备的时候，不妨加以考虑~\n\n其实沿着这个思路，你还可以发现更多集成的可能，Alluxio这里仅仅是举个例子加以说明，其实咱们作为开发者，平时要多关注技术动态和进展，开阔眼界和思路，我觉得这一点非常重要，个人观点哈~ 不一定对~\n\n一个新技术，不管将来用不用得上，多了解、熟悉一些，总是不吃亏的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626775975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2264679,"avatar":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","nickname":"陈威洋","note":"","ucode":"DCF84B4D3A7354","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384878,"discussion_content":"非常赞同开拓眼界和思路的建议，我经常上Inf Q哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626779751,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301098,"user_name":"空格","can_delete":false,"product_type":"c1","uid":2654533,"ip_address":"","ucode":"7609477C8B5EBB","user_header":"https://static001.geekbang.org/account/avatar/00/28/81/45/37db494c.jpg","comment_is_top":false,"comment_ctime":1625536170,"is_pvip":false,"replies":[{"id":"109176","content":"需要结合DPP机制才行哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1625800870,"ip_address":"","comment_id":301098,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1625536170","product_id":100073401,"comment_content":"老师，demo中按照date循环查询的sql直接使用spark sql执行可以实现外边扫描一次嘛？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522897,"discussion_content":"需要结合DPP机制才行哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625800870,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2654533,"avatar":"https://static001.geekbang.org/account/avatar/00/28/81/45/37db494c.jpg","nickname":"空格","note":"","ucode":"7609477C8B5EBB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":383952,"discussion_content":"了解了，感谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626316591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":299192,"user_name":"orangelin","can_delete":false,"product_type":"c1","uid":2010843,"ip_address":"","ucode":"730CC9997C16F2","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/cojb2AA3eM620kb7hj7YoG8k56TKsdCmVletmYKYwibickH5Ced8UyxicpY9icZEM2ZTcqyUaEk2PRmH1FVLtGTggw/132","comment_is_top":false,"comment_ctime":1624504980,"is_pvip":true,"replies":[{"id":"108576","content":"这样做确实能降低分区键的Cardinality，从而缓解分区存储的压力。不过按照尾号对orderId做关联，业务逻辑上似乎不成立。因为一般来说，肯定是orderId要完全一致Join在一起才会有意义，否则，仅仅是尾号相同、但实际是不同的orderId，他们join在一起其实业务逻辑上说不通的~<br><br>不过，话说回来，如果确实关联条件就是orderId的尾号，那么在运行时也是会触发DPP的，毕竟你说的这种场景是满足DPP的各种前提条件的~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624633567,"ip_address":"","comment_id":299192,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1624504980","product_id":100073401,"comment_content":"虽然orderid作为分区键有点太多，是否可以考虑按照orderid的数据特征，比如id是字母结尾亦或者是数字结尾，按这个列去做分区是可以减少一定的分区数，举个例子，我的分区关联条件是 数字id的尾号，那么关联的分区最多就是从0-9的10个数字，用这种方式的话是可以触发dpp呢","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522359,"discussion_content":"这样做确实能降低分区键的Cardinality，从而缓解分区存储的压力。不过按照尾号对orderId做关联，业务逻辑上似乎不成立。因为一般来说，肯定是orderId要完全一致Join在一起才会有意义，否则，仅仅是尾号相同、但实际是不同的orderId，他们join在一起其实业务逻辑上说不通的~\n\n不过，话说回来，如果确实关联条件就是orderId的尾号，那么在运行时也是会触发DPP的，毕竟你说的这种场景是满足DPP的各种前提条件的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624633567,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294200,"user_name":"天翼","can_delete":false,"product_type":"c1","uid":2459638,"ip_address":"","ucode":"82153820B786E0","user_header":"https://static001.geekbang.org/account/avatar/00/25/87/f6/bc199560.jpg","comment_is_top":false,"comment_ctime":1621828187,"is_pvip":false,"replies":[{"id":"106771","content":"是的，主要是表大小，通常来说，大多数数据库引擎，都会把尺寸比较大的表，当作外表；而尺寸较小的表，当作内表。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621866714,"ip_address":"","comment_id":294200,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1621828187","product_id":100073401,"comment_content":"请问一下，老师的内表和外表的定义是怎么来的，是根据表的大小吗？还是说有什么其他的判断依据？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520555,"discussion_content":"是的，主要是表大小，通常来说，大多数数据库引擎，都会把尺寸比较大的表，当作外表；而尺寸较小的表，当作内表。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621866714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293077,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1621206101,"is_pvip":false,"replies":[{"id":"106292","content":"这里面tx表是事实表哈，它的分区键是orderId，如下所示：<br><br>&#47;&#47;lineitems表的关键字段<br>orderId: Int &#47;&#47;分区键<br>txId: Int<br>itemId: Int<br>price: Float<br>quantity: Int<br><br>Join key是orderId，也就是on tx.orderId = o.orderId，因此就这个例子来说，是可以触发DPP的，当然，tx表选orderId作为分区键，是值得商榷的。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621414516,"ip_address":"","comment_id":293077,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1621206101","product_id":100073401,"comment_content":"老师，你的例子里，tx表的join条件没有用到date，而DPP的实现必须依赖join条件里包含分区字段，也就是date才可以的，这样好像不能实现优化吧？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520060,"discussion_content":"这里面tx表是事实表哈，它的分区键是orderId，如下所示：\n\n//lineitems表的关键字段\norderId: Int //分区键\ntxId: Int\nitemId: Int\nprice: Float\nquantity: Int\n\nJoin key是orderId，也就是on tx.orderId = o.orderId，因此就这个例子来说，是可以触发DPP的，当然，tx表选orderId作为分区键，是值得商榷的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621414516,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1370659,"avatar":"https://static001.geekbang.org/account/avatar/00/14/ea/23/508f71e3.jpg","nickname":"Jefitar","note":"","ucode":"D7ED9F32ADA5B1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":374508,"discussion_content":"DPP的条件是，大表的分区键包含大小表的join keys，跟小表的分区键没关系啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621220993,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1370659,"avatar":"https://static001.geekbang.org/account/avatar/00/14/ea/23/508f71e3.jpg","nickname":"Jefitar","note":"","ucode":"D7ED9F32ADA5B1","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375928,"discussion_content":"对，没错~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621871126,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":374508,"ip_address":""},"score":375928,"extra":""}]}]}]}