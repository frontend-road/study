{"id":355662,"title":"07 | 内存管理基础：Spark如何高效利用有限的内存空间？","content":"<p>你好，我是吴磊。</p><p>对于Spark这样的内存计算引擎来说，内存的管理与利用至关重要。业务应用只有充分利用内存，才能让执行性能达到最优。</p><p>那么，你知道Spark是如何使用内存的吗？不同的内存区域之间的关系是什么，它们又是如何划分的？今天这一讲，我就结合一个有趣的小故事，来和你深入探讨一下Spark内存管理的基础知识。</p><h2>内存的管理模式</h2><p>在管理方式上，Spark会区分<strong>堆内内存</strong>（On-heap Memory）和<strong>堆外内存</strong>（Off-heap Memory）。这里的“堆”指的是JVM Heap，因此堆内内存实际上就是Executor JVM的堆内存；堆外内存指的是通过Java Unsafe API，像C++那样直接从操作系统中申请和释放内存空间。</p><p><strong>其中，堆内内存的申请与释放统一由JVM代劳。</strong>比如说，Spark需要内存来实例化对象，JVM负责从堆内分配空间并创建对象，然后把对象的引用返回，最后由Spark保存引用，同时记录内存消耗。反过来也是一样，Spark申请删除对象会同时记录可用内存，JVM负责把这样的对象标记为“待删除”，然后再通过垃圾回收（Garbage Collection，GC）机制将对象清除并真正释放内存。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/ca/6b7c27e8b2e02e2698a031ff871313ca.jpg?wh=2889*883\" alt=\"\" title=\"JVM堆内内存的申请与释放\"></p><p>在这样的管理模式下，Spark对内存的释放是有延迟的，因此，当Spark尝试估算当前可用内存时，很有可能会高估堆内的可用内存空间。</p><!-- [[[read_end]]] --><p><strong>堆外内存则不同，Spark通过调用Unsafe的allocateMemory和freeMemory方法直接在操作系统内存中申请、释放内存空间</strong>，这听上去是不是和C++管理内存的方式很像呢？这样的内存管理方式自然不再需要垃圾回收机制，也就免去了它带来的频繁扫描和回收引入的性能开销。更重要的是，空间的申请与释放可以精确计算，因此Spark对堆外可用内存的估算会更精确，对内存的利用率也更有把握。</p><p>为了帮助你更轻松地理解这个过程，我来给你讲一个小故事。</p><h3>地主招租（上）：土地划分</h3><p>很久以前，燕山脚下有一个小村庄，村里有个地主，名叫黄四郎，四郎家有良田千顷，方圆数百里都是他的田地。黄四郎养尊处优，自然不会亲自下地种田，不过这么多田地也不能就这么荒着。于是，他想了个办法，既不用亲自动手又能日进斗金：收租子！</p><p>黄四郎虽然好吃懒做，但在管理上还是相当有一套的，他把田地划分为两块，一块叫“托管田”，另一块叫“自管田”。</p><p>我们知道，庄稼<strong>丰收之后，田地需要翻土、整平、晾晒</strong>，来年才能种下一茬庄稼。那么，托管田指的就是丰收之后，由黄四郎派专人帮你搞定翻土、整平这些琐事，不用你操心。相应的，自管田的意思就是庄稼你自己种，秋收之后的田地也得你自己收拾。</p><p><strong>毫无疑问，对租户来说托管田更省心一些，自管田更麻烦。</strong>当然了，相比自管田，托管田的租金自然更高。</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/31/b1a5fbe3701051126cb4e92yyfaeea31.jpg?wh=1626*1178\" alt=\"\" title=\"托管田与自管田\"></p><p>那么，这个故事中黄四郎的托管田就是内存管理中的堆内内存，自管田类比的则是堆外内存，田地的翻土、整平这些操作实际上就是JVM中的GC。这样类比起来是不是更好理解了呢？</p><h2>内存区域的划分</h2><p>故事先讲到这儿，让我们暂时先回到Spark的内存管理上。现在，我们知道了Spark内存管理有堆内和堆外两种模式，那Spark又是怎么划分内存区域的呢？</p><p>我们先来说说堆外内存。Spark把堆外内存划分为两块区域：一块用于执行分布式任务，如Shuffle、Sort和Aggregate等操作，这部分内存叫做<strong>Execution Memory</strong>；一块用于缓存RDD和广播变量等数据，它被称为<strong>Storage Memory</strong>。</p><p>堆内内存的划分方式和堆外差不多，Spark也会划分出用于执行和缓存的两份内存空间。不仅如此，Spark在堆内还会划分出一片叫做<strong>User Memory</strong>的内存空间，它用于存储开发者自定义数据结构。</p><p><img src=\"https://static001.geekbang.org/resource/image/a4/8c/a4b793f305410ee12964740a4958ba8c.jpg?wh=2943*912\" alt=\"\" title=\"不同内存区域的划分\"></p><p>除此之外，Spark在堆内还会预留出一小部分内存空间，叫做<strong>Reserved Memory</strong>，它被用来存储各种Spark内部对象，例如存储系统中的BlockManager、DiskBlockManager等等。</p><p>对于性能调优来说，我们在前三块内存的利用率上有比较大的发挥空间，因为业务应用主要消耗的就是它们，也即Execution memory、Storage memory和User memory。而预留内存我们却动不得，因为这块内存仅服务于Spark内部对象，业务应用不会染指。</p><p>好了，不同内存区域的划分与计算，我也把它们总结到了下面的表格中，方便你随时查阅。</p><p><img src=\"https://static001.geekbang.org/resource/image/19/87/19aae02eb53ba1ec3f4141cb662b7d87.jpeg?wh=1879*866\" alt=\"\"></p><h3>执行与缓存内存</h3><p>在所有的内存区域中，最重要的无疑是缓存内存和执行内存，而内存计算的两层含义也就是数据集缓存和Stage内的流水线计算，对应的就是Storage Memory和Execution Memory。</p><p>在Spark 1.6版本之前，Execution Memory和Storage Memory内存区域的空间划分是静态的，一旦空间划分完毕，不同内存区域的用途就固定了。也就是说，即便你没有缓存任何RDD或是广播变量，Storage Memory区域的空闲内存也不能用来执行Shuffle中的映射、排序或聚合等操作，因此宝贵的内存资源就被这么白白地浪费掉了。</p><p>考虑到静态内存划分潜在的空间浪费，在1.6版本之后，Spark推出了统一内存管理模式。<strong>统一内存管理指的是Execution Memory和Storage Memory之间可以相互转化</strong>，尽管两个区域由配置项spark.memory.storageFraction划定了初始大小，但在运行时，结合任务负载的实际情况，Storage Memory区域可能被用于任务执行（如Shuffle），Execution Memory区域也有可能存储RDD缓存。</p><p>但是，我们都知道，执行任务相比缓存任务，在内存抢占上有着更高的优先级。那你有没有想过这是为什么呢？接下来，就让我们带着“打破砂锅问到底”的精神，去探索其中更深层次的原因。</p><p>首先，执行任务主要分为两类：<strong>一类是Shuffle Map阶段的数据转换、映射、排序、聚合、归并等操作；另一类是Shuffle Reduce阶段的数据排序和聚合操作。它们所涉及的数据结构，都需要消耗执行内存</strong>。</p><p>我们可以先假设，执行任务与缓存任务在内存抢占上遵循“公正、公平和公开”的三原则。也就是说，不论谁抢占了对方的内存，当对方有需要时都会立即释放。比如说，刚开始双方的预设比例是五五开，但因为缓存任务在应用中比较靠后的位置，所以执行任务先占据了80%的内存空间，当缓存任务追赶上来之后，执行任务就需要释放30%的内存空间还给缓存任务。</p><p>这种情况下会发生什么？假设集群范围内总共有80个CPU，也就是集群在任意时刻的并行计算能力是80个分布式任务。在抢占了80%内存的情况下，80个CPU可以充分利用，每个CPU的计算负载都是比较饱满的，计算完一个任务，再去计算下一个任务。</p><p>但是，由于有30%的内存要归还给缓存任务，这意味着有30个并行的执行任务没有内存可用。也就是说会有30个CPU一直处在I/O wait的状态，没法干活！宝贵的CPU计算资源就这么白白地浪费掉了，简直是暴殄天物。</p><p>因此，相比于缓存任务，执行任务的抢占优先级一定要更高。说了这么多，我们为什么要弄清楚其中的原因呢？我认为，只有弄清楚抢占优先级的背后逻辑，我们才能理解为什么要同时调节CPU和内存的相关配置，也才有可能做到不同硬件资源之间的协同与平衡，这也是我们进行性能调优要达到的最终效果。</p><p>不过，即使执行任务的抢占优先级更高，但它们在抢占内存的时候一定也要遵循某些规则。那么，这些规则具体是什么呢？下面，咱们就接着以地主招租的故事为例，来说说Execution memory和Storage memory之间有哪些有趣的规则。</p><h3>地主招租（下）：租地协议</h3><p>黄四郎招租的告示贴出去没多久，村子里就有两个年富力强的小伙子来租种田地。一个叫黄小乙，是黄四郎的远房亲戚，前不久来投奔黄四郎。另一个叫张麻子，虽是八辈贫农，小日子过得也算是蒸蒸日上。张麻子打算把田地租过来种些小麦、玉米这样的庄稼。黄小乙就不这么想，这小子挺有商业头脑，他把田地租过来准备种棉花、咖啡这类经济作物。</p><p>两个人摩拳擦掌都想干出一番事业，恨不得把黄四郎的地全都包圆！地不愁租，黄四郎自然是满心欢喜，但烦恼也接踵而至：“既要照顾小乙这孩子，又不能打击麻子的积极性，得想个万全之策”。</p><p>于是，他眼珠一转，计上心来：“按理说呢，咱们丈量土地之后，应该在你们中间划一道实线，好区分田地的归属权。不过呢，毕竟麻子你是本村的，小乙远道而来，远来即是客嘛！咱们对小乙还是得多少照顾着点”。张麻子心生不悦：“怎么照顾？”</p><p>黄四郎接着说：“<strong>很简单，把实线改为虚线，多劳者多得</strong>。原本呢，你们应该在分界线划定的那片田地里各自劳作。不过呢，你们二人的进度各不相同嘛，所以，<strong>勤奋的人，自己的田地种满了之后，可以跨过分界线，去占用对方还在空着的田地</strong>。”</p><p>黄小乙不解地问：“四舅，这不是比谁种得快吗？也没对我特殊照顾啊！”张麻子眉间也拧了个疙瘩：“如果种得慢的人后来居上，想要把被占的田地收回去，到时候该怎么办呢？”</p><p>黄四郎得意道：“刚才说了，咱们多多照顾小乙。所以<strong>如果麻子勤快、干活也快，先占了小乙的地，种上了小麦、玉米，小乙后来居上，想要收回自己的地，那么没说的，麻子得把多占的地让出来。不管庄稼熟没熟，麻子都得把地铲平，还给人家小乙种棉花、咖啡</strong>”。</p><p><img src=\"https://static001.geekbang.org/resource/image/92/cc/92a3a3b0d69935a9f9770675ed6428cc.jpg?wh=1858*1208\" alt=\"\" title=\"黄小乙与张麻子的占地协议\"></p><p>黄四郎偷眼看了看两人的反应，继续说：“<strong>反过来，如果小乙更勤快，先占了麻子的地，麻子后来居上，想要收回，这个时候，咱们就得多照顾照顾小乙。小乙有权继续占用麻子的地，直到地上种的棉花、咖啡都丰收了，再把多占的地让出来</strong>。你们二位看怎么样？”</p><p>黄小乙听了大喜。张麻子虽然心里不爽，但也清楚黄四郎和黄小乙之间的亲戚关系，也不好再多说什么，心想：“反正我勤快些，先把地种满也就是了”。于是，三方击掌为誓，就此达成协议。</p><p>好啦，地主招租的故事到这里就讲完了。不难发现，黄小乙的地类比的是Execution Memory，张麻子的地其实就是Storage Memory。他们之间的协议其实就是Execution Memory和Storage Memory之间的抢占规则，一共可以总结为3条：</p><ul>\n<li><strong>如果对方的内存空间有空闲，双方就都可以抢占；</strong></li>\n<li><strong>对于RDD缓存任务抢占的执行内存，当执行任务有内存需要时，RDD缓存任务必须立即归还抢占的内存，涉及的RDD缓存数据要么落盘、要么清除；</strong></li>\n<li><strong>对于分布式计算任务抢占的Storage Memory内存空间，即便RDD缓存任务有收回内存的需要，也要等到任务执行完毕才能释放。</strong></li>\n</ul><p>同时，我也把这个例子中的关键内容和Spark之间的对应关系总结在了下面，希望能帮助你加深印象。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/21/692d78990aa9481b56bcc28522e9df21.jpg?wh=7824*2126\" alt=\"\" title=\"地主招租与Spark的类比关系\"></p><h2>从代码看内存消耗</h2><p>说完了理论，接下来，咱们再从实战出发，用一个小例子来直观地感受一下，应用中代码的不同部分都消耗了哪些内存区域。</p><p>示例代码很简单，目的是读取words.csv文件，然后对其中指定的单词进行统计计数。</p><pre><code>val dict: List[String] = List(“spark”, “scala”)\nval words: RDD[String] = sparkContext.textFile(“~/words.csv”)\nval keywords: RDD[String] = words.filter(word =&gt; dict.contains(word))\nkeywords.cache\nkeywords.count\nkeywords.map((_, 1)).reduceByKey(_ + _).collect\n</code></pre><p>整个代码片段包含6行代码，咱们从上到下逐一分析。</p><p>首先，第一行定义了dict字典，这个字典在Driver端生成，它在后续的RDD调用中会随着任务一起分发到Executor端。第二行读取words.csv文件并生成RDD words。<strong>第三行很关键，用dict字典对words进行过滤，此时dict已分发到Executor端，Executor将其存储在堆内存中，用于对words数据分片中的字符串进行过滤。Dict字典属于开发者自定义数据结构，因此，Executor将其存储在User Memory区域。</strong></p><p>接着，第四行和第五行用cache和count对keywords RDD进行缓存，以备后续频繁访问，分布式数据集的缓存占用的正是Storage Memory内存区域。在最后一行代码中，我们在keywords上调用reduceByKey对单词分别计数。我们知道，reduceByKey算子会引入Shuffle，而Shuffle过程中所涉及的内部数据结构，如映射、排序、聚合等操作所仰仗的Buffer、Array和HashMap，都会消耗Execution Memory区域中的内存。</p><p>不同代码与其消耗的内存区域，我都整理到了下面的表格中，方便你查看。</p><p><img src=\"https://static001.geekbang.org/resource/image/a0/ff/a05f77a27aaaf21d9d064aa1ca1be3ff.jpeg?wh=1920*735\" alt=\"\"></p><h2>小结</h2><p>深入理解内存管理的机制，有助于我们充分利用应用的内存，提升其执行性能。今天，我们重点学习了内存管理的基础知识。</p><p><strong>首先是内存的管理方式。</strong>Spark区分堆内内存和堆外内存：对于堆外内存来说，Spark通过调用Java Unsafe的allocateMemory和freeMemory方法，直接在操作系统内存中申请、释放内存空间，管理成本较高；对于堆内内存来说，无需Spark亲自操刀而是由JVM代理。但频繁的JVM GC对执行性能来说是一大隐患。另外，Spark对堆内内存占用的预估往往不够精确，高估可用内存往往会为OOM埋下隐患。</p><p><strong>其次是统一内存管理，以及Execution Memory和Storage Memory之间的抢占规则</strong>。它们就像黄四郎招租故事中黄小乙和张麻子的田地，抢占规则就像他们之间的占地协议，主要可以分为3条：</p><ul>\n<li>如果对方的内存空间有空闲，那么双方都可以抢占；</li>\n<li>对RDD缓存任务抢占的执行内存，当执行任务有内存需要时，RDD缓存任务必须立即归还抢占的内存，其中涉及的RDD缓存数据要么落盘、要么清除；</li>\n<li>对分布式计算任务抢占的Storage Memory内存空间，即便RDD缓存任务有收回内存的需要，也要等到任务执行完毕才能释放。</li>\n</ul><p><strong>最后是不同代码对不同内存区域的消耗。</strong>内存区域分为Reserved Memory、User Memory、Execution Memory和Storage Memory。其中，Reserved Memory用于存储Spark内部对象，User Memory用于存储用户自定义的数据结构，Execution Memory用于分布式任务执行，而Storage Memory则用来容纳RDD缓存和广播变量。</p><p>好了，这些就是内存管理的基础知识。当然了，与内存相关的话题还有很多，比如内存溢出、RDD缓存、内存利用率，以及执行内存的并行计算等等。在性能调优篇，我还会继续从内存视角出发，去和你探讨这些话题。</p><h2>每日一练</h2><ol>\n<li>你知道启用off-heap之后，Spark有哪些计算环节可以利用到堆外内存？你能列举出一些例子吗？</li>\n<li>相比堆内内存，为什么在堆外内存中，Spark对于内存占用量的预估更准确？</li>\n<li>结合我在下面给定的配置参数，你能分别计算不同内存区域（Reserved、User、Execution、Storage）的具体大小吗？</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/fd/66/fdb2fb17120e4d047d5ccd28d1434b66.jpeg?wh=1851*692\" alt=\"\"></p><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","neighbors":{"left":{"article_title":"06 | 存储系统：空间换时间，还是时间换空间？","id":355081},"right":{"article_title":"08 | 应用开发三原则：如何拓展自己的开发边界？","id":357197}},"comments":[{"had_liked":false,"id":292397,"user_name":"-.-","can_delete":false,"product_type":"c1","uid":1357603,"ip_address":"","ucode":"3CDC606BDFDD80","user_header":"https://static001.geekbang.org/account/avatar/00/14/b7/23/ea83d6eb.jpg","comment_is_top":false,"comment_ctime":1620804704,"is_pvip":true,"replies":[{"id":"105895","content":"先来说说这个spark.executor.memoryOverhead 哈，在yarn、k8s部署模式下，container会预留一部分内存，形式是堆外，用来保证稳定性，主要存储nio buffer，函数栈等一些开销，所以你看名字：over head。这部分内存，不管堆外还是堆内，开发者用不到，spark也用不到，所以不用关心，千万不指望调这个参数去提升性能，它的目的是保持运行时的稳定性~<br><br>想利用堆外让spark去管理数据、加速执行效率，只有off heap那两个参数，一个用来enable(spark.memory.offHeap.enabled=true)、一个指定大小(spark.memory.offHeap.size)。这两个才是正儿八经的off heap key configs。<br><br>回答你的问题：<br>1. 是的，没错，它不是Spark管理的内存空间，不会用到Execution或是Storage<br>2. 不会，这部分overhead是单独划分的，它不会参与到Spark诸多内存空间的计算，是完全独立的一块区域，也就是前面说的container预留的“buffer”。所以完全不用理它，它的目的是提供稳定性，不参与Spark任务计算。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620834499,"ip_address":"","comment_id":292397,"utype":1}],"discussion_count":3,"race_medal":0,"score":"70340281440","product_id":100073401,"comment_content":"受益匪浅，开始看第二遍了！有个问题想请教下，spark.executor.memoryOverhead控制的是堆外内存的大小，官方文档解释：This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.1. 如果设置spark.memory.offHeap.enabled=false，这块内存是不是只是jvm的堆外内存而不是spark管理的堆外内存，不会被用于执行内存和缓存内存？ 2. 如果设置spark.memory.offHeap.enabled=true,这块内存中是不是会包含offHeapSize，其中一部分为JVM堆外内存一部分为offHeap的执行内存和缓存内存？","like_count":16,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519812,"discussion_content":"先来说说这个spark.executor.memoryOverhead 哈，在yarn、k8s部署模式下，container会预留一部分内存，形式是堆外，用来保证稳定性，主要存储nio buffer，函数栈等一些开销，所以你看名字：over head。这部分内存，不管堆外还是堆内，开发者用不到，spark也用不到，所以不用关心，千万不指望调这个参数去提升性能，它的目的是保持运行时的稳定性~\n\n想利用堆外让spark去管理数据、加速执行效率，只有off heap那两个参数，一个用来enable(spark.memory.offHeap.enabled=true)、一个指定大小(spark.memory.offHeap.size)。这两个才是正儿八经的off heap key configs。\n\n回答你的问题：\n1. 是的，没错，它不是Spark管理的内存空间，不会用到Execution或是Storage\n2. 不会，这部分overhead是单独划分的，它不会参与到Spark诸多内存空间的计算，是完全独立的一块区域，也就是前面说的container预留的“buffer”。所以完全不用理它，它的目的是提供稳定性，不参与Spark任务计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620834499,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1168545,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/a1/8bc8e7e1.jpg","nickname":"赌神很低调","note":"","ucode":"1066778E1EDF26","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558961,"discussion_content":"如果我设定spark.memory.offHeap.size为1G,spark.executor.memoryOverhead是否也必须设置为1G,不然yarn不会分配这部分内存？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648531942,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1745733,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/a3/45/65e7997f.jpg","nickname":"Cellophane","note":"","ucode":"4161CDAF760F4C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":557931,"discussion_content":"老师，我看到别的文章有说oom还有数据量很大的时候，调节memoryOverhead有用，老师能详细说说吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648027994,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285676,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1616972076,"is_pvip":false,"replies":[{"id":"103655","content":"满分💯","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616979060,"ip_address":"","comment_id":285676,"utype":1}],"discussion_count":6,"race_medal":0,"score":"35976710444","product_id":100073401,"comment_content":"堆内内存中：保留内存300M，用户内存为20*0.2=4GB，Storage内存为20*0.8*0.6=9.6GB，Execution内存为20*0.8*0.4=6.4GB<br>堆外内存中：Storage内存为10*0.6=6G，Execution内存为10*0.4=4G","like_count":8,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517751,"discussion_content":"满分💯","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616979060,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1014649,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/7b/79/df384bdc.jpg","nickname":"修愿三秋","note":"","ucode":"0AEE445D8DBFF4","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362846,"discussion_content":"用户内存等不应该是减了300M后再算么？比如用户内存为(20g-0. 3g) *0. 2=3.94g","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617059311,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1168504,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","nickname":"斯盖丸","note":"","ucode":"B881D14B028F14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1014649,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/7b/79/df384bdc.jpg","nickname":"修愿三秋","note":"","ucode":"0AEE445D8DBFF4","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":363198,"discussion_content":"肯定要减300M的，只不过算起来太麻烦了所以没减😆","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1617144797,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362846,"ip_address":""},"score":363198,"extra":""},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1014649,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/7b/79/df384bdc.jpg","nickname":"修愿三秋","note":"","ucode":"0AEE445D8DBFF4","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":363650,"discussion_content":"对，严格来说是要这么算才对","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1617253898,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362846,"ip_address":""},"score":363650,"extra":""}]},{"author":{"id":2536655,"avatar":"","nickname":"Geek_74be61","note":"","ucode":"F348E20A050C84","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362667,"discussion_content":"为什么堆内内存中每个区域分配的内存值合计起来是20G+300M?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617004252,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2536655,"avatar":"","nickname":"Geek_74be61","note":"","ucode":"F348E20A050C84","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363651,"discussion_content":"300M是从20G里面分出来的\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617253932,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362667,"ip_address":""},"score":363651,"extra":""}]}]},{"had_liked":false,"id":294412,"user_name":"Kendrick","can_delete":false,"product_type":"c1","uid":2406292,"ip_address":"","ucode":"5DF1269295D24E","user_header":"https://static001.geekbang.org/account/avatar/00/24/b7/94/22121c60.jpg","comment_is_top":false,"comment_ctime":1621932171,"is_pvip":false,"replies":[{"id":"106840","content":"好问题，其实spark官方建议谨慎使用堆外内存，为啥呢？<br><br>原因其实很简单，在于堆外堆内的空间互不share，也就是说，你的task最开始用堆外，用着用着发现不够了，这个时候即使堆内还有空闲，task也没法用，所以照样会oom。<br><br>内存本来就有限，再强行划分出两块隔离的区域，其实反而增加了管理难度。tungsten在堆内其实也用内存页管理内存（Tungsten的相关优化，可以参考后面Tungsten那一讲），也用压缩的二进制数据结构，因此gc效率往往可以保障，这也是为什么官方推荐就用堆内就可以了。<br><br>回答你的问题，我不觉得有什么场景一定要用堆外，就我看来，对于开发者来说，堆外更多地是一种备选项，是Optional的。不过，尽管如此，我们还是要知道堆外、堆内各自有哪些优缺点、优劣势，这样在结合应用场景做选择的时候，也能有的放矢~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621952780,"ip_address":"","comment_id":294412,"utype":1}],"discussion_count":2,"race_medal":0,"score":"31686703243","product_id":100073401,"comment_content":"有点疑惑，我想知道堆外内存存在的意义是什么，有什么场景是一定需要堆外内存么？","like_count":8,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520642,"discussion_content":"好问题，其实spark官方建议谨慎使用堆外内存，为啥呢？\n\n原因其实很简单，在于堆外堆内的空间互不share，也就是说，你的task最开始用堆外，用着用着发现不够了，这个时候即使堆内还有空闲，task也没法用，所以照样会oom。\n\n内存本来就有限，再强行划分出两块隔离的区域，其实反而增加了管理难度。tungsten在堆内其实也用内存页管理内存（Tungsten的相关优化，可以参考后面Tungsten那一讲），也用压缩的二进制数据结构，因此gc效率往往可以保障，这也是为什么官方推荐就用堆内就可以了。\n\n回答你的问题，我不觉得有什么场景一定要用堆外，就我看来，对于开发者来说，堆外更多地是一种备选项，是Optional的。不过，尽管如此，我们还是要知道堆外、堆内各自有哪些优缺点、优劣势，这样在结合应用场景做选择的时候，也能有的放矢~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621952780,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1296063,"avatar":"https://static001.geekbang.org/account/avatar/00/13/c6/bf/52b3f71d.jpg","nickname":"dawn","note":"","ucode":"1757B28F1EF5C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585488,"discussion_content":"我觉得吧，jvm需要回收的数据，之间的关系非常复杂，而如果用堆外内存，由于更多是rdd缓存，没有那么多复杂的关系链，那么gc那一套算法，未必就有自己撸一套回收算法强，这个时候开辟堆外对内，更多是spark给自己留了一条可以优化的后路。当然，另外一个优化的思路是，寻找效率更高的gc回收器，对于使用者来说，后者显然更合适","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1661590765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江苏"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286169,"user_name":"井先生","can_delete":false,"product_type":"c1","uid":2108294,"ip_address":"","ucode":"87FD735FA7BFC3","user_header":"https://static001.geekbang.org/account/avatar/00/20/2b/86/318e6ff3.jpg","comment_is_top":false,"comment_ctime":1617175761,"is_pvip":false,"replies":[{"id":"103898","content":"具体大小可以通过参数来配置哈，堆内也一样，都是用参数开调控。不过需要注意，堆内、堆外的内存，互相之间不共享。也就是一开始你的task用off heap，后来用着用着发现不够了，这个时候是不能去占有堆内内存的，所以即便堆内有空闲，也还是会oom。所以在划分堆内堆外之前，要提前计划好，如果怕麻烦，就都用堆内。tungsten对于堆内的内存管理做的也很好，大多数场景都问题不大～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617239488,"ip_address":"","comment_id":286169,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31681946833","product_id":100073401,"comment_content":"试读了几节果断订阅了。<br>开启堆外内存后，分配的内存空间是多大？这时候还会分配堆内内存吗？谢谢","like_count":7,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517891,"discussion_content":"具体大小可以通过参数来配置哈，堆内也一样，都是用参数开调控。不过需要注意，堆内、堆外的内存，互相之间不共享。也就是一开始你的task用off heap，后来用着用着发现不够了，这个时候是不能去占有堆内内存的，所以即便堆内有空闲，也还是会oom。所以在划分堆内堆外之前，要提前计划好，如果怕麻烦，就都用堆内。tungsten对于堆内的内存管理做的也很好，大多数场景都问题不大～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617239488,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288500,"user_name":"苏子浩","can_delete":false,"product_type":"c1","uid":2486715,"ip_address":"","ucode":"8A842C9C2E53E8","user_header":"https://static001.geekbang.org/account/avatar/00/25/f1/bb/547b580a.jpg","comment_is_top":false,"comment_ctime":1618493684,"is_pvip":false,"replies":[{"id":"104753","content":"“reduceByKey算子会引入 Shuffle，而 Shuffle 过程中所涉及的内部数据结构，如映射、排序、聚合等操作所仰仗的 Buffer、Array 和 HashMap，都会消耗 Execution Memory 区域中的内存。”<br><br>内存：上面说的这些操作，都会消耗内存空间，不过Map阶段的每一个计算环节，都是为了生成中间文件（data和index文件）；<br><br>磁盘：在生成中间文件的时候，就会涉及磁盘、涉及diskStore的putByes写文件。比如临时文件溢出、比如merge得到的中间文件，等等。<br><br>或者更简单地，Shuffle过程中，只有写临时文件、和Shuffle中间文件，才会涉及diskStore和相关的磁盘操作。其他的计算步骤，都是在内存中完成的，会消耗如上的数据结构。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618567608,"ip_address":"","comment_id":288500,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23093330164","product_id":100073401,"comment_content":"老师，您好！我想问一下在文中提到“reduceByKey算子会引入 Shuffle，而 Shuffle 过程中所涉及的内部数据结构，如映射、排序、聚合等操作所仰仗的 Buffer、Array 和 HashMap，都会消耗 Execution Memory 区域中的内存。”上一节说到Shuffle的中间结果会写入磁盘：Shuffle manager通过BlockManager调用DiskStore的putBytes()方法将数据块写入文件。这里的联系是什么呢？在内存和磁盘上有点不理解，不好意思，感谢解答！","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518642,"discussion_content":"“reduceByKey算子会引入 Shuffle，而 Shuffle 过程中所涉及的内部数据结构，如映射、排序、聚合等操作所仰仗的 Buffer、Array 和 HashMap，都会消耗 Execution Memory 区域中的内存。”\n\n内存：上面说的这些操作，都会消耗内存空间，不过Map阶段的每一个计算环节，都是为了生成中间文件（data和index文件）；\n\n磁盘：在生成中间文件的时候，就会涉及磁盘、涉及diskStore的putByes写文件。比如临时文件溢出、比如merge得到的中间文件，等等。\n\n或者更简单地，Shuffle过程中，只有写临时文件、和Shuffle中间文件，才会涉及diskStore和相关的磁盘操作。其他的计算步骤，都是在内存中完成的，会消耗如上的数据结构。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618567608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2486715,"avatar":"https://static001.geekbang.org/account/avatar/00/25/f1/bb/547b580a.jpg","nickname":"苏子浩","note":"","ucode":"8A842C9C2E53E8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368765,"discussion_content":"谢谢老师的解答！明白了！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618824953,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290199,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1619427123,"is_pvip":true,"replies":[{"id":"105236","content":"是的，不过要显示地（Explicitly）指定存储级别：OFF_HEAP<br><br>rdd.persist(OFF_HEAP)","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619513697,"ip_address":"","comment_id":290199,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14504329011","product_id":100073401,"comment_content":"启用off-heap以后，RDD可以直接缓存到off-heap上。","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519136,"discussion_content":"是的，不过要显示地（Explicitly）指定存储级别：OFF_HEAP\n\nrdd.persist(OFF_HEAP)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619513697,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285763,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1617005222,"is_pvip":false,"replies":[{"id":"103698","content":"和数据源存在哪儿关系不大哈，主要看它用于哪类计算，如果是纯运算，那就是execution memory；如果缓存下来，就是storage memory。取决于你的计算负载类型～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617014606,"ip_address":"","comment_id":285763,"utype":1}],"discussion_count":5,"race_medal":0,"score":"14501907110","product_id":100073401,"comment_content":"老师，stage的输入是外部数据源的情况，比如s3的parquet文件，是用哪一块内存来保存读取的数据呀","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517774,"discussion_content":"和数据源存在哪儿关系不大哈，主要看它用于哪类计算，如果是纯运算，那就是execution memory；如果缓存下来，就是storage memory。取决于你的计算负载类型～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617014606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1629630,"avatar":"","nickname":"Geeker","note":"","ucode":"DC09B404F3EBB0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":364009,"discussion_content":"老师有点疑惑，代码中第二行读取 words.csv 文件并生成 RDD words，这里涉及到的是哪种内存？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617348969,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1629630,"avatar":"","nickname":"Geeker","note":"","ucode":"DC09B404F3EBB0","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366419,"discussion_content":"执行内存，Execution memory","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618056903,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":364009,"ip_address":""},"score":366419,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362705,"discussion_content":"别客气~ 继续参与讨论哈~ 加油！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617014941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1179298,"avatar":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","nickname":"对方正在输入。。。","note":"","ucode":"7B0DEB4D9B43D2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362701,"discussion_content":"soga！谢谢老师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617014677,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":297538,"user_name":"feihui","can_delete":false,"product_type":"c1","uid":1007294,"ip_address":"","ucode":"13F1D4A82BC650","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5e/be/9ea55f46.jpg","comment_is_top":false,"comment_ctime":1623604495,"is_pvip":true,"replies":[{"id":"108253","content":"execution memory、storage memory的划分是逻辑上的~ Spark在JVM之上的内存管理，实际上是一种基于“审计”的管理，也就是Spark会记录不同内存区域的内存消耗，但实际的内存申请与释放，还是依赖JVM本身，比如对象的实际删除，是要等到GC的时候。<br><br>关于对象尺寸的预估，或者说可用内存的预估，这是个好问题。如果不使用字节数组，确实存在预估不准的问题，而这，也是诸如广播变量时而生效、时而不生效的原因所在。因此，对于内存的使用，或者划分，我们往往倾向于留出一些富裕，目的就是为了避免Spark误判可用剩余内存而导致OOM。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624008950,"ip_address":"","comment_id":297538,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10213539087","product_id":100073401,"comment_content":"老师请教下，execution memory、storage memory 的各类划分的 memory 在内存上逻辑上连续物理上不连续的吗？此外像内存量的控制，用字节数组倒是很好预估，对于那些没有系列化的数据，怎么去控制使用的内存量呢？对象的内存分配是由 JVM 去管理， spark 的删除是将强引用设置为 null 吗？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521849,"discussion_content":"execution memory、storage memory的划分是逻辑上的~ Spark在JVM之上的内存管理，实际上是一种基于“审计”的管理，也就是Spark会记录不同内存区域的内存消耗，但实际的内存申请与释放，还是依赖JVM本身，比如对象的实际删除，是要等到GC的时候。\n\n关于对象尺寸的预估，或者说可用内存的预估，这是个好问题。如果不使用字节数组，确实存在预估不准的问题，而这，也是诸如广播变量时而生效、时而不生效的原因所在。因此，对于内存的使用，或者划分，我们往往倾向于留出一些富裕，目的就是为了避免Spark误判可用剩余内存而导致OOM。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624008950,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291147,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620051252,"is_pvip":false,"replies":[{"id":"105488","content":"第二、三题完美~ 💯<br><br>第一题答得也不错~ 等到看完Shuffle那讲，可以再回过头来想想，都有哪些数据结构，可以利用到堆外内存~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620120250,"ip_address":"","comment_id":291147,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10209985844","product_id":100073401,"comment_content":"第一题：<br>缓存rdd：rdd.persist(StorageLevel.OFF_HEAP)<br><br>第二题：<br>因为堆内内存的申请和释放是由JVM来统一管理，对Spark来说是不那么透明可控的；而堆外内存需要调用Unsafe的allocateMemory和freeMemory方法来进行内存的申请和释放，完全由Spark来控制，所以估算会相对更精准。<br><br>第三题：<br>- Reserved：300M<br>- User：(20GB - 300MB) * (1 - 0.8)<br>- Execution：(20GB - 300MB) * 0.8 * (1 - 0.6) + 10GB * (1 - 0.6)<br>- Storage：(20GB - 300MB) * 0.8 * 0.6 + 10GB * 0.6","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519385,"discussion_content":"第二、三题完美~ 💯\n\n第一题答得也不错~ 等到看完Shuffle那讲，可以再回过头来想想，都有哪些数据结构，可以利用到堆外内存~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620120250,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390024,"discussion_content":"看到这个回答,突然想问下老师,既然executor memory 和 storage memory 两块内存不share,那是不是可以通过persist来指定呢,一部分rdd使用execm 一部分rdd使用storm呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629615282,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288885,"user_name":"LYL","can_delete":false,"product_type":"c1","uid":2155021,"ip_address":"","ucode":"B95850B5A3E85B","user_header":"https://static001.geekbang.org/account/avatar/00/20/e2/0d/d7a012b7.jpg","comment_is_top":false,"comment_ctime":1618751176,"is_pvip":false,"replies":[{"id":"104852","content":"1. Tungsten确实统一了内存管理，使用Page来管理内存，这样做得目的，主要在于统一内存对象（内存页）抽象。对于堆内来说，内存页本质上就是个大对象，没什么新鲜的；但对于堆外来说，那可正儿八经的是OS的内存寻址。因此，两块内存不能“同时”使用。换句话说，一个任务，不管是执行任务、还是缓存任务，你要么用堆外，要么用堆内，驴和熊猫不可兼得，不能脚踏两条船。<br><br>2. 开启堆外之后，执行任务默认会走堆外，堆外用尽了，后续的任务才会走堆内。对于缓存来说，如果你明确指定了用off heap，那就是明确走堆外，如果你不明确指定，那么默认走堆内。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618758898,"ip_address":"","comment_id":288885,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10208685768","product_id":100073401,"comment_content":"老师，有几个问题我不太明白，<br>1.tungsten中的page用于同一管理off-heap和on-heap，利用这个机制可否在spark runtime的时候shuffle同时使用堆内和堆外内存？<br>2.在cache rdd的时候是否能指定StorageLevel为off_heap在spark runtime时使用堆外内存，memory_only的情况下使用堆内内存，或者说在配置开启堆外内存的参数之后，所有内存都是走堆外内存，无法使用堆内内存","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518761,"discussion_content":"1. Tungsten确实统一了内存管理，使用Page来管理内存，这样做得目的，主要在于统一内存对象（内存页）抽象。对于堆内来说，内存页本质上就是个大对象，没什么新鲜的；但对于堆外来说，那可正儿八经的是OS的内存寻址。因此，两块内存不能“同时”使用。换句话说，一个任务，不管是执行任务、还是缓存任务，你要么用堆外，要么用堆内，驴和熊猫不可兼得，不能脚踏两条船。\n\n2. 开启堆外之后，执行任务默认会走堆外，堆外用尽了，后续的任务才会走堆内。对于缓存来说，如果你明确指定了用off heap，那就是明确走堆外，如果你不明确指定，那么默认走堆内。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618758898,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":340066,"user_name":"赌神很低调","can_delete":false,"product_type":"c1","uid":1168545,"ip_address":"","ucode":"1066778E1EDF26","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/a1/8bc8e7e1.jpg","comment_is_top":false,"comment_ctime":1648564722,"is_pvip":true,"replies":[{"id":"124386","content":"1、spark中内存划分是逻辑上的，真正的管理还是在jvm。如user memory占用内存超过设定值，还是会占用框架内存。但框架内存会根据设定值让task做一些阻塞或spill操作，所以从这个层面上说，框架内存的值得正确设置，如用户不会用到大的list、map等内存集合，就要把用户内存空间设置得够小，以保证框架内存(执行内存+存储内存)足够大，避免不必要的阻塞或spill操作？<br><br>回答：是的，Spark的内存管理，更多的是一种”审计“上的管理，底下有JVM，Spark就不可能直接管理内存。通过内存管理机制，Spark更多地是设置一些软限制，从而从应用层面来将内存划分为不同区域，这些区域，在JVM看来，是没有区别的。如你所说：”如用户不会用到大的list、map等内存集合，就要把用户内存空间设置得够小，以保证框架内存(执行内存+存储内存)足够大，避免不必要的阻塞或spill操作“，确实是这样的。<br><br>2、如果开启了堆外内存,即使堆外内存不够，堆内内存充足，task也只会用堆外内存而不会用堆内内存？<br>回答：内存是用堆外，还是堆内，是以Job为粒度的，也就是说，要设置堆外内存，我们得确保堆外大小足以应对当前的作业，作业里面所有的tasks，都只能用堆外（如果作业在内存设置上用了堆外）。那么显然，此时跑在堆外的Job，假设内存不够用了，即便堆内还有剩余，也不会给这个Job用，这个Job还是会抛OOM。<br><br>3、spark 2.x版本中如果开启了堆外内存，并设置了spark.memory.offHeap.size=500mb,在yarn上跑的话spark.executor.memoryOverhead除了默认需要的10%是否还有要加上这500mb，否则container不会分配堆外这500mb的内存？看网上说3.0以上就不用加了。<br>回答：对的，堆外就是JVM heap以外的内存，以前的话，yarn把这部分内存算在container里面，现在不算在container里面了，不过这样其实有风险，因为堆外内存大小，对于yarn来说透明了，如果在运行时，Spark Job跑着跑着，发现OS根本分配不了500mb，那这个事情yarn是不负责的。<br><br>4、task会在哪些场景申请和释放内存呢？只是shuffle的场景吗？transformer场景会吗？<br>回答：凡是利用到AppendOnlyMap，PartitionPairBuffer这两个数据结构的计算，都要申请、释放内存，跟算子没啥关系哈，主要是shuffle write阶段的计算。","user_name":"编辑回复","user_name_real":"编辑","uid":"1501385","ctime":1648636107,"ip_address":"","comment_id":340066,"utype":2}],"discussion_count":1,"race_medal":0,"score":"5943532018","product_id":100073401,"comment_content":"老师好，有几个问题不是很明白想问下:<br>1、spark中内存划分是逻辑上的，真正的管理还是在jvm。如user memory占用内存超过设定值，还是会占用框架内存。但框架内存会根据设定值让task做一些阻塞或spill操作，所以从这个层面上说，框架内存的值得正确设置，如用户不会用到大的list、map等内存集合，就要把用户内存空间设置得够小，以保证框架内存(执行内存+存储内存)足够大，避免不必要的阻塞或spill操作？\r<br>2、如果开启了堆外内存,即使堆外内存不够，堆内内存充足，task也只会用堆外内存而不会用堆内内存？\r<br>3、spark 2.x版本中如果开启了堆外内存，并设置了spark.memory.offHeap.size=500mb,在yarn上跑的话spark.executor.memoryOverhead除了默认需要的10%是否还有要加上这500mb，否则container不会分配堆外这500mb的内存？看网上说3.0以上就不用加了。\r<br>4、task会在哪些场景申请和释放内存呢？只是shuffle的场景吗？transformer场景会吗？","like_count":2,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":559176,"discussion_content":"1、spark中内存划分是逻辑上的，真正的管理还是在jvm。如user memory占用内存超过设定值，还是会占用框架内存。但框架内存会根据设定值让task做一些阻塞或spill操作，所以从这个层面上说，框架内存的值得正确设置，如用户不会用到大的list、map等内存集合，就要把用户内存空间设置得够小，以保证框架内存(执行内存+存储内存)足够大，避免不必要的阻塞或spill操作？\n\n回答：是的，Spark的内存管理，更多的是一种”审计“上的管理，底下有JVM，Spark就不可能直接管理内存。通过内存管理机制，Spark更多地是设置一些软限制，从而从应用层面来将内存划分为不同区域，这些区域，在JVM看来，是没有区别的。如你所说：”如用户不会用到大的list、map等内存集合，就要把用户内存空间设置得够小，以保证框架内存(执行内存+存储内存)足够大，避免不必要的阻塞或spill操作“，确实是这样的。\n\n2、如果开启了堆外内存,即使堆外内存不够，堆内内存充足，task也只会用堆外内存而不会用堆内内存？\n回答：内存是用堆外，还是堆内，是以Job为粒度的，也就是说，要设置堆外内存，我们得确保堆外大小足以应对当前的作业，作业里面所有的tasks，都只能用堆外（如果作业在内存设置上用了堆外）。那么显然，此时跑在堆外的Job，假设内存不够用了，即便堆内还有剩余，也不会给这个Job用，这个Job还是会抛OOM。\n\n3、spark 2.x版本中如果开启了堆外内存，并设置了spark.memory.offHeap.size=500mb,在yarn上跑的话spark.executor.memoryOverhead除了默认需要的10%是否还有要加上这500mb，否则container不会分配堆外这500mb的内存？看网上说3.0以上就不用加了。\n回答：对的，堆外就是JVM heap以外的内存，以前的话，yarn把这部分内存算在container里面，现在不算在container里面了，不过这样其实有风险，因为堆外内存大小，对于yarn来说透明了，如果在运行时，Spark Job跑着跑着，发现OS根本分配不了500mb，那这个事情yarn是不负责的。\n\n4、task会在哪些场景申请和释放内存呢？只是shuffle的场景吗？transformer场景会吗？\n回答：凡是利用到AppendOnlyMap，PartitionPairBuffer这两个数据结构的计算，都要申请、释放内存，跟算子没啥关系哈，主要是shuffle write阶段的计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648636107,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":8}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317347,"user_name":"sparkjoy","can_delete":false,"product_type":"c1","uid":1000179,"ip_address":"","ucode":"827B3116EED3B8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/f3/e945e4ac.jpg","comment_is_top":false,"comment_ctime":1634746706,"is_pvip":false,"replies":[{"id":"115140","content":"评论区对于堆内、堆外的讨论比较多，老弟可以参考下哈~<br><br>总的来说，堆内、堆外是以Job为粒度划分的，也就是说，同一个Job，要么，你全用堆外，要么，你全用堆内。堆外、堆内的内存空间，是不能在同一个Job之内共享的。<br><br>这部分，如果看源码的话，可以把org.apache.spark.memory下面的源码，通读一遍~ 如果只关注堆外、堆内消耗相关的源码，可以从这个文件开始：TaskMemoryManager.java","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634890916,"ip_address":"","comment_id":317347,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5929714002","product_id":100073401,"comment_content":"老师，怎么知道是堆外用完了才用堆内呢？能指导一下源码的出处么？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528781,"discussion_content":"评论区对于堆内、堆外的讨论比较多，老弟可以参考下哈~\n\n总的来说，堆内、堆外是以Job为粒度划分的，也就是说，同一个Job，要么，你全用堆外，要么，你全用堆内。堆外、堆内的内存空间，是不能在同一个Job之内共享的。\n\n这部分，如果看源码的话，可以把org.apache.spark.memory下面的源码，通读一遍~ 如果只关注堆外、堆内消耗相关的源码，可以从这个文件开始：TaskMemoryManager.java","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634890916,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1711247,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/1c/8f/a5e25ee4.jpg","nickname":"豪","note":"","ucode":"D0CE3C5B60C5E5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":414449,"discussion_content":"源码捕捉实力还不够强😂，看了半天没找到 一个job堆外不够用时转用堆内 的源码，老师能指点下吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636769303,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":308726,"user_name":"Sean","can_delete":false,"product_type":"c1","uid":2162751,"ip_address":"","ucode":"69234046BFD81B","user_header":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","comment_is_top":false,"comment_ctime":1629772418,"is_pvip":false,"replies":[{"id":"112654","content":"1. persist只能用来指定存储模式，memory还是disk，但不管什么缓存，都只能消耗Storage Memory<br>2. 对的，off heap必须显示开启才行。一旦开启off heap，作业会优先用off heap<br>3. 这个比较难，off heap、on heap，是以作业为控制粒度的，不是以task为控制粒度，也就是说，一个作业，要么都用off heap，要么都用on heap，不存在一个作业内部不同task，有的用堆外、有的用堆内。这个实现机制其实是有优化空间的~<br>4. 这个参数，我理解是用来blacklist executors用的，也就是当一些executors频繁失败，spark会把他们标记到blacklist黑名单，避免下次DAGScheduler把任务调度到标记到blacklist的executors上面去。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630941123,"ip_address":"","comment_id":308726,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5924739714","product_id":100073401,"comment_content":"从第一章看到了第十一章,在留言去里面学习到了很多,老师对知识的传授也很有技巧,个人也是受益匪浅,随着阅读的慢慢深入的,总结了一些自己理解和疑惑,现在又回到了第七章,总结了一些问题,希望老师可以帮忙解惑,感谢!<br>1.在缓存rdd时,既然executor memory 和 storage memory 两块内存不可互相share,那是不是可以通过persist来指定呢,一部分rdd使用execm 一部分rdd使用storm呢?<br>2.只要不开启off heap,spark就无法使用off heap,包括yarn,k8s模式利用off heap提升稳定性也无法体现出来,一旦开启了off heap,执行任务也就是executor memory优先使用off heap,storage memory还是优先堆内内存,可以这样理解吗?<br>3.例如：spark executor如果配置了堆内和堆外各4GB，executor cores配置为2。那么该executor运行的第一个task只会使用堆外内存？调度来的第二个task，哪怕堆外剩余几十MB，它也会用堆外内存，如果第二个task发现堆外不够用，就会写磁盘,或清除部分堆外内存数据呢<br>4.shuffle 阶段的稳定性参数 spark.excludeOnFailure.application.fetchFailure.enabled 从官网描述上来看,这个参数对fetch failed会切换到别的节点,结合实际情况,在Map 阶段：Shuffle writer 按照 Reducer 的分区规则将中间数据写入本地磁盘过程中,刚好写人的datanode 的数据卷故障,但是并没有触发重试机制,而是一直runing状态,是不是可以通过启用application.fetchFailure.enabled来识别,目前使用的是物理机,这种情况也是偶尔发生一次,所以很难验证","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525604,"discussion_content":"1. persist只能用来指定存储模式，memory还是disk，但不管什么缓存，都只能消耗Storage Memory\n2. 对的，off heap必须显示开启才行。一旦开启off heap，作业会优先用off heap\n3. 这个比较难，off heap、on heap，是以作业为控制粒度的，不是以task为控制粒度，也就是说，一个作业，要么都用off heap，要么都用on heap，不存在一个作业内部不同task，有的用堆外、有的用堆内。这个实现机制其实是有优化空间的~\n4. 这个参数，我理解是用来blacklist executors用的，也就是当一些executors频繁失败，spark会把他们标记到blacklist黑名单，避免下次DAGScheduler把任务调度到标记到blacklist的executors上面去。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630941123,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286002,"user_name":"🚤","can_delete":false,"product_type":"c1","uid":1460463,"ip_address":"","ucode":"95D7F773789B74","user_header":"https://static001.geekbang.org/account/avatar/00/16/48/ef/4750cb14.jpg","comment_is_top":false,"comment_ctime":1617099039,"is_pvip":false,"replies":[{"id":"103842","content":"对，没错，count来trigger action。<br><br>第二题满分💯，<br><br>第三题堆外是不是笔误，写反了？execution和storage应该掉个个儿～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617103157,"ip_address":"","comment_id":286002,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5912066335","product_id":100073401,"comment_content":"老师，cache 之后 再进行count，主要是因为cache不是action算子，所以需要一个action算子来触发缓存的生效。<br>我这样子理解对么？<br><br>回答2：<br>堆内内存：因为spark只是将无用的对象引用删除，但是无用对象真正的回收还要依赖于JVM来管理。Spark只是做了标记，但是真正什么时候删除spark并不知道，这里存在一个时间差。<br>相比较堆外内存：spark自己做管理就可以清楚的知道当前还有多少内存空间可以使用。<br><br>回答3：<br>堆内：<br>Reserved: 300MB<br>User: 4GB<br>Execution: 6.4GB<br>Storage: 9.6GB<br><br>堆外：<br>Execution：6GB<br>Storage：4GB","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517842,"discussion_content":"对，没错，count来trigger action。\n\n第二题满分💯，\n\n第三题堆外是不是笔误，写反了？execution和storage应该掉个个儿～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617103157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363652,"discussion_content":"哈哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617253954,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1460463,"avatar":"https://static001.geekbang.org/account/avatar/00/16/48/ef/4750cb14.jpg","nickname":"🚤","note":"","ucode":"95D7F773789B74","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363045,"discussion_content":"😂😂写纸上 抄反了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617104025,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285787,"user_name":"L3nvy","can_delete":false,"product_type":"c1","uid":1271157,"ip_address":"","ucode":"0B74B27C121D56","user_header":"https://static001.geekbang.org/account/avatar/00/13/65/75/f9d7e8b7.jpg","comment_is_top":false,"comment_ctime":1617012520,"is_pvip":false,"replies":[{"id":"103772","content":"前两题满分💯，第三题需要注意，堆外、堆内要分开计算。spark在runtime，堆内、堆外互相之间并不share，也就是你一个task最开始在堆外跑，跑着跑着发现内存不够了，这个时候，即使堆内有富余，task还是会oom。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617067042,"ip_address":"","comment_id":285787,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5911979816","product_id":100073401,"comment_content":"1. 堆外内存主要分为Execution Memory和Storage Memory两部分，使用场景应该是和堆内内存的使用场景几乎一致。Execution Memory用在Shuffle、Join、Sort、Aggregation计算过程中；Storage Memory用在RDD的缓存、集群中内部数据的传播<br>2. 堆内内存因为依赖JVM的GC进行内存回收，Spark不能保证内存空间已经被回收；堆外内存因为是手动进行内存的分配和释放，所以Spark能准确预估内存暂用量<br>3. <br>    1. Reserved Memory: 300 MB<br>    2. User Memory: 20 * 0.2  = 4 GB<br>    3. Execution Memory: 20 * 0.8 * 0.4 + 10 * 0.4 = 10.4 GB<br>    4. Storage Memory: 20 * 0.8 * 0.6 + 10 * 0.6  = 15.6 GB","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517781,"discussion_content":"前两题满分💯，第三题需要注意，堆外、堆内要分开计算。spark在runtime，堆内、堆外互相之间并不share，也就是你一个task最开始在堆外跑，跑着跑着发现内存不够了，这个时候，即使堆内有富余，task还是会oom。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617067042,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":354858,"user_name":"Geek_33c134","can_delete":false,"product_type":"c1","uid":1596647,"ip_address":"广东","ucode":"C5BBA83AABD2AC","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Ir1EdhB29109UhH7SGnVLEfEcKcGPDViaTpbS7BYXngqFghThYc093oFCTwsTLjVR7nHPokXJIPm7rbmPAU7TdQ/132","comment_is_top":false,"comment_ctime":1660824118,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1660824118","product_id":100073401,"comment_content":"老师你好，想问下对于某段代码driver获取后，会交给多个executor来执行。对于你的数据源：：“~&#47;words.csv”假设是一个很大的文件，存储在多个hdfs磁盘块上面，例如分步磁盘块A,B,C。那么:<br>1.executor是否也在A,B,C的机器上分别来执行呢(假设资源足够，我理解是按照上节课的移动代码但是不移动数据来分配)？<br>2. 对与在A机器上的executor，是只加载三分之一的数据到内存里吗？也就是磁盘块A的数据到内存中，executor是看不到磁盘块B,C的数据的是吗？  <br>3. words.filter(word =&gt; dict.contains(word)) 对于这段代码的执行，dict分别在A,B,C三台机器的executor上的user memory存储，这一段的filter底层是for循环进行每个word过滤，是否是A,B,C三台机器各过滤三分之一的数据（例如有30亿个单词，A，B, C分别过滤自己磁盘块的10W条数据），并且并行执行呢？","like_count":0},{"had_liked":false,"id":352369,"user_name":"李悦城","can_delete":false,"product_type":"c1","uid":2328830,"ip_address":"","ucode":"F72974986FA015","user_header":"https://static001.geekbang.org/account/avatar/00/23/88/fe/157a2d11.jpg","comment_is_top":false,"comment_ctime":1658588534,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1658588534","product_id":100073401,"comment_content":"请教老师一个问题：如果Driver的堆外内存爆了，频繁的GC，那可能是什么原因，怎么排查呢？","like_count":0},{"had_liked":false,"id":350280,"user_name":"康","can_delete":false,"product_type":"c1","uid":2621850,"ip_address":"","ucode":"C3E85292E026D5","user_header":"https://static001.geekbang.org/account/avatar/00/28/01/9a/d2831441.jpg","comment_is_top":false,"comment_ctime":1656745899,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1656745899","product_id":100073401,"comment_content":"吴老师，对于这两个参数，调大后会有什么弊端吗，其次，spill的缓冲区是位于spark内存哪个区域？谢谢老师<br>-- 增大reducer一次性read大小<br>spark.reducer.maxSizeInFlight<br>-- 增加spill缓冲<br>set spark.shuffle.file.buffer","like_count":0},{"had_liked":false,"id":350278,"user_name":"康","can_delete":false,"product_type":"c1","uid":2621850,"ip_address":"","ucode":"C3E85292E026D5","user_header":"https://static001.geekbang.org/account/avatar/00/28/01/9a/d2831441.jpg","comment_is_top":false,"comment_ctime":1656745128,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1656745128","product_id":100073401,"comment_content":"吴老师，spark.executor.memoryOverhead是不是spark3.0之后被独立开了，之前我看还有spark.executor.memoryOverhead要大于等于spark.memory.offHeap.size的建议，现在是不是可以把它当成一个参数为container预留的，预防高峰期超出container设定后导致container被kill？","like_count":0},{"had_liked":false,"id":334782,"user_name":"陌生的心酸","can_delete":false,"product_type":"c1","uid":2829738,"ip_address":"","ucode":"2F221D93403D20","user_header":"https://static001.geekbang.org/account/avatar/00/2b/2d/aa/e33e9edd.jpg","comment_is_top":false,"comment_ctime":1645110127,"is_pvip":false,"replies":[{"id":"122408","content":"满分💯~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1645370021,"ip_address":"","comment_id":334782,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1645110127","product_id":100073401,"comment_content":"2.通过java UnSafe API 进行内存分配和回收，及时、自行进行对象的引用、回收，及时进行内存的分配和管理，避免像jvm那样进行定时清理，进行标记、清除操作，效率更快，速度更高<br>3.<br>堆内内存：<br>预留内存 : (1-0.8)*20G = 4G ; 执行内存： 0.8*(1-0.6）*20 = 6.4G ,存储内存：0.8*0.6*20 = 9.2G<br>堆外内存：<br>执行内存： （1-0.6） * 10G = 4G,存储内存 0.6*10 = 6G<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552267,"discussion_content":"满分💯~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645370021,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":334781,"user_name":"陌生的心酸","can_delete":false,"product_type":"c1","uid":2829738,"ip_address":"","ucode":"2F221D93403D20","user_header":"https://static001.geekbang.org/account/avatar/00/2b/2d/aa/e33e9edd.jpg","comment_is_top":false,"comment_ctime":1645110123,"is_pvip":false,"replies":[{"id":"122409","content":"满分💯~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1645370032,"ip_address":"","comment_id":334781,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1645110123","product_id":100073401,"comment_content":"2.通过java UnSafe API 进行内存分配和回收，及时、自行进行对象的引用、回收，及时进行内存的分配和管理，避免像jvm那样进行定时清理，进行标记、清除操作，效率更快，速度更高<br>3.<br>堆内内存：<br>预留内存 : (1-0.8)*20G = 4G ; 执行内存： 0.8*(1-0.6）*20 = 6.4G ,存储内存：0.8*0.6*20 = 9.2G<br>堆外内存：<br>执行内存： （1-0.6） * 10G = 4G,存储内存 0.6*10 = 6G<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552268,"discussion_content":"满分💯~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645370032,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":326025,"user_name":"Cellophane","can_delete":false,"product_type":"c1","uid":1745733,"ip_address":"","ucode":"4161CDAF760F4C","user_header":"https://static001.geekbang.org/account/avatar/00/1a/a3/45/65e7997f.jpg","comment_is_top":false,"comment_ctime":1639315830,"is_pvip":false,"replies":[{"id":"118712","content":"老弟这是两个专栏都订阅啦，感谢老弟的反馈~<br>同一个原理，第一个类比确实往往印象会更深些，能帮到老弟更好地理解Spark内存管理就好哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639667391,"ip_address":"","comment_id":326025,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639315830","product_id":100073401,"comment_content":"感觉土地招租的例子有点绕，没有零基础入门 Spark中那个施工的例子直观","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539324,"discussion_content":"老弟这是两个专栏都订阅啦，感谢老弟的反馈~\n同一个原理，第一个类比确实往往印象会更深些，能帮到老弟更好地理解Spark内存管理就好哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639667391,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321302,"user_name":"豪","can_delete":false,"product_type":"c1","uid":1711247,"ip_address":"","ucode":"D0CE3C5B60C5E5","user_header":"https://static001.geekbang.org/account/avatar/00/1a/1c/8f/a5e25ee4.jpg","comment_is_top":false,"comment_ctime":1636771447,"is_pvip":false,"replies":[{"id":"116842","content":"堆外、堆内的使用，是以Job为粒度哈。就是说，同一个Job内的所有Stages、所有作业，要么都用堆外，要么，都用堆内~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1637058465,"ip_address":"","comment_id":321302,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1636771447","product_id":100073401,"comment_content":"源码捕捉实力还不够强😂，看了半天没找到 一个job堆外不够用时转用堆内 的源码，老师能指点下吗","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530356,"discussion_content":"堆外、堆内的使用，是以Job为粒度哈。就是说，同一个Job内的所有Stages、所有作业，要么都用堆外，要么，都用堆内~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637058465,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1711247,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/1c/8f/a5e25ee4.jpg","nickname":"豪","note":"","ucode":"D0CE3C5B60C5E5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":547365,"discussion_content":"刚看到老师的回复😂。老师说的这个点我懂，我不清楚的是，当我启用堆外内存配置时，一个job如何判断使用堆外或者堆内内存？ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642648180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":530356,"ip_address":""},"score":547365,"extra":""}]}]},{"had_liked":false,"id":320072,"user_name":"福","can_delete":false,"product_type":"c1","uid":2455712,"ip_address":"","ucode":"F2FC7AF5D433C6","user_header":"https://static001.geekbang.org/account/avatar/00/25/78/a0/7a248ddc.jpg","comment_is_top":false,"comment_ctime":1636075657,"is_pvip":true,"replies":[{"id":"116035","content":"不是User Memory，RDD封装了分布式数据集，如果没有加Cache，这些数据加载进来之后，都会走Execution Memory，也就是执行内存。<br><br>需要特别注意的是，这些个RDD，都是“虚”的、“动态”的，就是说不要一看到RDD，就觉得这些数据是静态地存在哪个地方。<br><br>这些RDD都是批量、像流一样，加载到内存并且是分阶段执行的，很多地方的RDD，在内存中甚至没有形态，因为代码生成的缘故，数据形态可能从RDD1瞬间变换到RDD10，因为中间的形态（RDD2到RDD<br>9），都通过整合的代码，一步到位地处理过了。<br><br>所以看到RDD，就要想到Spark的DAG和lineage，不能静态、孤立地看待RDD哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1636092716,"ip_address":"","comment_id":320072,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636075657","product_id":100073401,"comment_content":"老师，想请问哈您，第2行代码，val words: RDD[String] = sparkContext.textFile(“~&#47;words.csv”)，这个words的数据，是用的哪里的内存，这个也是用户定义的内存，是在userMemory中嘛","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529862,"discussion_content":"不是User Memory，RDD封装了分布式数据集，如果没有加Cache，这些数据加载进来之后，都会走Execution Memory，也就是执行内存。\n\n需要特别注意的是，这些个RDD，都是“虚”的、“动态”的，就是说不要一看到RDD，就觉得这些数据是静态地存在哪个地方。\n\n这些RDD都是批量、像流一样，加载到内存并且是分阶段执行的，很多地方的RDD，在内存中甚至没有形态，因为代码生成的缘故，数据形态可能从RDD1瞬间变换到RDD10，因为中间的形态（RDD2到RDD\n9），都通过整合的代码，一步到位地处理过了。\n\n所以看到RDD，就要想到Spark的DAG和lineage，不能静态、孤立地看待RDD哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636092716,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":309554,"user_name":"Sean","can_delete":false,"product_type":"c1","uid":2162751,"ip_address":"","ucode":"69234046BFD81B","user_header":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","comment_is_top":false,"comment_ctime":1630228924,"is_pvip":false,"replies":[{"id":"112639","content":"memoryOverhead这参数是spark被yarn调度的时候，container预留出来的堆外内存，用于避免spark executor内存溢出，导致container内存溢出从而让yarn kill掉container，甚至是整个application。所以才说它是用来保证稳定性的。这个参数，与spark本身的堆外，没有关系。<br><br>spark用到的堆外，就是offheap相关的那几个参数，如果特别想开启堆外的话，把那两个包含“offheap”关键字的配置项弄好就行~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630938278,"ip_address":"","comment_id":309554,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1630228924","product_id":100073401,"comment_content":"二刷,接上一个留言的问题2:<br>1.memoryOverhead这个参数不管是作用在堆内还是堆外,都是占用storage memory这部分内存吗,<br>2.磊哥的回复中提到 &quot;不管堆外还是堆内，开发者用不到，spark也用不到，所以不用关心，千万不指望调这个参数去提升性能，它的目的是保持运行时的稳定性~&quot;,个人不太理解这句话的不用关心,因为有出现过oom overhead的问题,可以理解为是使用到了memoryOverhead,那么就需要去调整对应的memoryOverhead大小,&quot;开发者用不到，spark也用不到&quot;,这句话我还没有get到,斗胆在问一下磊哥,是哪里用到了这个参数,来提升稳定性呢? <br>个人理解不够,给磊哥添麻烦了 o(╥﹏╥)o  o(╥﹏╥)o  o(╥﹏╥)o","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525914,"discussion_content":"memoryOverhead这参数是spark被yarn调度的时候，container预留出来的堆外内存，用于避免spark executor内存溢出，导致container内存溢出从而让yarn kill掉container，甚至是整个application。所以才说它是用来保证稳定性的。这个参数，与spark本身的堆外，没有关系。\n\nspark用到的堆外，就是offheap相关的那几个参数，如果特别想开启堆外的话，把那两个包含“offheap”关键字的配置项弄好就行~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630938278,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2203494,"avatar":"https://static001.geekbang.org/account/avatar/00/21/9f/66/9973656e.jpg","nickname":"王平","note":"","ucode":"1F47D88C64997A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":410928,"discussion_content":"&#34;用于避免spark executor内存溢出&#34;:   那么问题是啥时以及如何判断 executor内存溢出了？因为实际碰到OOM，我们用overHead这个参数似乎都能解决问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635815740,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":299101,"user_name":"Bennan","can_delete":false,"product_type":"c1","uid":2427818,"ip_address":"","ucode":"8B6EE3DCBB1356","user_header":"https://static001.geekbang.org/account/avatar/00/25/0b/aa/09c1215f.jpg","comment_is_top":false,"comment_ctime":1624457454,"is_pvip":false,"replies":[{"id":"108496","content":"是的，这么计算没问题，和咱们本讲的计算方法不冲突哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624504167,"ip_address":"","comment_id":299101,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1624457454","product_id":100073401,"comment_content":"为什么我看源码，执行内存+存储每次=(spark.executor.memory-预留内存300m)*spark.memory.fraction","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522326,"discussion_content":"是的，这么计算没问题，和咱们本讲的计算方法不冲突哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624504167,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291050,"user_name":"冷萃浮乐朵","can_delete":false,"product_type":"c1","uid":1134525,"ip_address":"","ucode":"5C3BCBD9E5973D","user_header":"https://static001.geekbang.org/account/avatar/00/11/4f/bd/e08af2e9.jpg","comment_is_top":false,"comment_ctime":1619965727,"is_pvip":false,"replies":[{"id":"105476","content":"有关系，RDD缓存的过程，实际是把数据分区迭代器物化为MemoryEntry、并存储到MemoryStore的过程。这个过程当中消耗的内存，都会记入Storage Memory的账上。<br><br>关于RDD缓存的详细过程，可以参考存储系统那一讲，或是后面的RDD Cache那一讲~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620115085,"ip_address":"","comment_id":291050,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1619965727","product_id":100073401,"comment_content":"想问下这里说的内存管理和BlockManager的MemoryStore有关系吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519353,"discussion_content":"有关系，RDD缓存的过程，实际是把数据分区迭代器物化为MemoryEntry、并存储到MemoryStore的过程。这个过程当中消耗的内存，都会记入Storage Memory的账上。\n\n关于RDD缓存的详细过程，可以参考存储系统那一讲，或是后面的RDD Cache那一讲~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620115085,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290188,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1619424344,"is_pvip":true,"replies":[{"id":"105202","content":"哈哈哈，不平等条约~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619434299,"ip_address":"","comment_id":290188,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1619424344","product_id":100073401,"comment_content":"感觉麻子干不过小艺，小艺的政策太好了，越线的地，小艺需要麻子就要给。当麻子线内的地，小艺种了就不给麻子还了。谁叫小艺的作物经济价值高呢。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519133,"discussion_content":"哈哈哈，不平等条约~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619434299,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288208,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1618365147,"is_pvip":false,"replies":[{"id":"104644","content":"count是为了trigger cache的计算，计算cache的过程中，在展开（Unroll）之前，会消耗Execution memory；展开之后，转化为MemoryEntry，消耗的就是Storage memory。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618394552,"ip_address":"","comment_id":288208,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1618365147","product_id":100073401,"comment_content":"老师cache用的storage memory，count不应该是用的execution memory吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518548,"discussion_content":"count是为了trigger cache的计算，计算cache的过程中，在展开（Unroll）之前，会消耗Execution memory；展开之后，转化为MemoryEntry，消耗的就是Storage memory。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618394552,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287403,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1617932354,"is_pvip":false,"replies":[{"id":"104395","content":"默认不会开启，不开启就不会用。只有开启off heap，spark才会尝试用堆外，否则全部用堆内。实际上，spark社区推荐用堆内，因为tungsten的优化机制，良好的数据结构，会把gc的开销降到最低，堆内也能很好地完成计算。这部分我们在tungsten部分还会再展开～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617953998,"ip_address":"","comment_id":287403,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1617932354","product_id":100073401,"comment_content":"如果没有开启堆外内存spark.memory.offHeap.enabled=false。spark会用堆内内存，那还会用到堆外内存么。怎么感觉一些地方会用到呢。否则也没必要预留堆外内存吧？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518293,"discussion_content":"默认不会开启，不开启就不会用。只有开启off heap，spark才会尝试用堆外，否则全部用堆内。实际上，spark社区推荐用堆内，因为tungsten的优化机制，良好的数据结构，会把gc的开销降到最低，堆内也能很好地完成计算。这部分我们在tungsten部分还会再展开～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617953998,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2162751,"avatar":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","nickname":"Sean","note":"","ucode":"69234046BFD81B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390027,"discussion_content":"针对老师的回复是不是就可以理解为,只要不开启off heap,spark就无法使用,包括yarn,k8s模式利用off heap提升稳定性也无法体现出来,一旦开启了off heap,执行任务也就是executor memory优先使用off heap,storage memory还是优先堆内内存,可以这样理解吗?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629616114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286588,"user_name":"Geek_50f808","can_delete":false,"product_type":"c1","uid":2089363,"ip_address":"","ucode":"A7E47D32A3B148","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/vH3NV3dCKgCDrxfbwqeaaI3XYpZ6PqAxPQByH0zt6WNMOBoRjg3IvfWDYtib6flCpwqkP5iamd74c3WlM72ydLmw/132","comment_is_top":false,"comment_ctime":1617413948,"is_pvip":false,"replies":[{"id":"104101","content":"广播那一讲会有讲哈，会讲的很详细，应该下周就会更新，稍安勿躁哈～ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617441926,"ip_address":"","comment_id":286588,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1617413948","product_id":100073401,"comment_content":"老师，我想请教一下dict会和任务一起发送到executor，这种方式方式和广播变量的方式有什么区别么？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518038,"discussion_content":"广播那一讲会有讲哈，会讲的很详细，应该下周就会更新，稍安勿躁哈～ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617441926,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365802,"discussion_content":"初步认识广播之后，executor读取变量是在executor本地获取。没有广播是从driver端获取","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617885559,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286173,"user_name":"金角大王","can_delete":false,"product_type":"c1","uid":1259375,"ip_address":"","ucode":"DFBA85FB2FD0B0","user_header":"https://static001.geekbang.org/account/avatar/00/13/37/6f/b3337e6d.jpg","comment_is_top":false,"comment_ctime":1617178237,"is_pvip":false,"replies":[{"id":"103895","content":"不太对哈，你还要考虑spark.memory.fraction","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617238916,"ip_address":"","comment_id":286173,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1617178237","product_id":100073401,"comment_content":"onHeapStorageRegionSize =<br>        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517894,"discussion_content":"不太对哈，你还要考虑spark.memory.fraction","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617238916,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1259375,"avatar":"https://static001.geekbang.org/account/avatar/00/13/37/6f/b3337e6d.jpg","nickname":"金角大王","note":"","ucode":"DFBA85FB2FD0B0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363578,"discussion_content":"好的，多谢指点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617239515,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286047,"user_name":"金角大王","can_delete":false,"product_type":"c1","uid":1259375,"ip_address":"","ucode":"DFBA85FB2FD0B0","user_header":"https://static001.geekbang.org/account/avatar/00/13/37/6f/b3337e6d.jpg","comment_is_top":false,"comment_ctime":1617118635,"is_pvip":false,"replies":[{"id":"103861","content":"好问题，其实spark官方建议谨慎使用堆外内存，为啥呢？<br><br>原因其实很简单，在于堆外堆内的空间互不share，也就是说，你的task最开始用堆外，用着用着发现不够了，这个时候即使堆内还有空闲，task也没法用，所以照样会oom。<br><br>内存本来就有限，再强行划分出两块隔离的区域，其实反而增加了管理难度。tungsten在堆内其实也用内存页管理内存，也用压缩的二进制数据结构，因此gc效率往往可以保障，这也是为什么官方推荐就用堆内。<br><br>回答你的问题，如果开辟了堆外，spark会优先用堆外，直到用光了为止，再去用堆内。所以如果堆外空间不充分，执行任务对缓存空间的抢占会更严重，oom的风险也越高。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617154654,"ip_address":"","comment_id":286047,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1617118635","product_id":100073401,"comment_content":"老师，能讲解下Spark不同任务在堆内堆外内存的使用选择上的逻辑吗？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517854,"discussion_content":"好问题，其实spark官方建议谨慎使用堆外内存，为啥呢？\n\n原因其实很简单，在于堆外堆内的空间互不share，也就是说，你的task最开始用堆外，用着用着发现不够了，这个时候即使堆内还有空闲，task也没法用，所以照样会oom。\n\n内存本来就有限，再强行划分出两块隔离的区域，其实反而增加了管理难度。tungsten在堆内其实也用内存页管理内存，也用压缩的二进制数据结构，因此gc效率往往可以保障，这也是为什么官方推荐就用堆内。\n\n回答你的问题，如果开辟了堆外，spark会优先用堆外，直到用光了为止，再去用堆内。所以如果堆外空间不充分，执行任务对缓存空间的抢占会更严重，oom的风险也越高。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617154654,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028956,"avatar":"","nickname":"勿更改任何信息","note":"","ucode":"575185C69C05A3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":364755,"discussion_content":"老师前面说堆外堆内不共享，为撒后面又说会优先使用堆外，直到用光了在去用堆内？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617593640,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1357603,"avatar":"https://static001.geekbang.org/account/avatar/00/14/b7/23/ea83d6eb.jpg","nickname":"-.-","note":"","ucode":"3CDC606BDFDD80","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":373629,"discussion_content":"如果堆外内存用光了不就oom了，这个时候任务都挂了怎么再用堆内内存呢？这一点不太理解。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620806271,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366420,"discussion_content":"对呀，这个不矛盾呀，开了堆外，就优先用堆外，堆外与堆内不共享。如果你在堆外的task跑挂了，就算堆内还有内存，task还是会fail，就是堆内的内存有盈余，它也用不上","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618057003,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285761,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1617005029,"is_pvip":false,"replies":[{"id":"103699","content":"这块是我挖了个坑哈，可能需要对shuffle有一些理解，会更好回答，shuffle那一讲会介绍计算过程中涉及的内存数据结构，到时候可以关注下哈～<br><br>这里出这道题，主要是想引发大家去深入思考～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617014885,"ip_address":"","comment_id":285761,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617005029","product_id":100073401,"comment_content":"老师，第一个问题，我的理解是涉及到io的操作会用到堆外内存，比如shuffle reduce阶段从不同的executor拉取中间数据时；或者rdd使用persist到磁盘时，其他情况我就不得而知了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517773,"discussion_content":"这块是我挖了个坑哈，可能需要对shuffle有一些理解，会更好回答，shuffle那一讲会介绍计算过程中涉及的内存数据结构，到时候可以关注下哈～\n\n这里出这道题，主要是想引发大家去深入思考～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617014885,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285738,"user_name":"Jay","can_delete":false,"product_type":"c1","uid":1066131,"ip_address":"","ucode":"488BF5D01D3CFD","user_header":"https://static001.geekbang.org/account/avatar/00/10/44/93/2d3d5868.jpg","comment_is_top":false,"comment_ctime":1616997225,"is_pvip":false,"replies":[{"id":"103697","content":"count是用了trigger缓存的计算。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617014412,"ip_address":"","comment_id":285738,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616997225","product_id":100073401,"comment_content":"&quot;第四行和第五行用 cache 和 count 对 keywords RDD 进行缓存&quot; -- keywords.cache缓存能理解，keywords.count为啥也可以缓存呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517768,"discussion_content":"count是用了trigger缓存的计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617014412,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285673,"user_name":"白音","can_delete":false,"product_type":"c1","uid":2526221,"ip_address":"","ucode":"B83B0269B8E9A0","user_header":"https://static001.geekbang.org/account/avatar/00/26/8c/0d/42e16041.jpg","comment_is_top":false,"comment_ctime":1616969441,"is_pvip":false,"replies":[{"id":"103654","content":"满分💯","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1616979054,"ip_address":"","comment_id":285673,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616969441","product_id":100073401,"comment_content":"试答3: <br>堆内 20G <br>Reserved Memory: 300MB <br>User Memory: 20 * 0.2 = 4GB  <br>Storage Memory:  20 * 0.8 * 0.6 = 9.6GB<br>Execution Memory:  20 * 0.8 * 0.4 = 6.4GB<br><br>堆外 10GB<br>Storage Memory: 10 * 0.6 : 6GB<br>Execution Memory:  10 * 0.4 : 4GB ","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517749,"discussion_content":"满分💯","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616979054,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}