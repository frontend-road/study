{"id":366265,"title":"19 | 网络视角：如何有效降低网络开销？","content":"<p>你好，我是吴磊。</p><p>在平衡不同硬件资源的时候，相比CPU、内存、磁盘，网络开销无疑是最拖后腿的那一个，这一点在处理延迟上表现得非常明显。</p><p>下图就是不同硬件资源的处理延迟对比结果，我们可以看到最小的处理单位是纳秒。你可能对纳秒没什么概念，所以为了方便对比，我把纳秒等比放大到秒。这样，其他硬件资源的处理延迟也会跟着放大。最后一对比我们会发现，网络延迟是以天为单位的！</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/a9/c1e4926d3748bdc98a5317fcf4e5b2a9.png?wh=1749*711\" alt=\"\" title=\"不同硬件资源处理延迟对比\"></p><p>因此，要想维持硬件资源之间的平衡，尽可能地降低网络开销是我们在性能调优中必须要做的。今天这一讲，我就按照数据进入系统的时间顺序，也就是数据读取、数据处理和数据传输的顺序，带你去分析和总结数据生命周期的不同阶段有效降低网络开销的方法。</p><h2>数据读写</h2><p>对于绝大多数应用来说，第一步操作都是从分布式文件系统读取数据源。Spark支持的数据源种类非常丰富，涉及的存储格式和存储系统可以说是五花八门。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/c3/2a1e6190f6e746e97661bf6f09941cc3.jpeg?wh=622*825\" alt=\"\" title=\"存储格式与存储系统\"></p><p>这么多存储格式和外部存储系统交叉在一起又会有无数种组合，并且每一种组合都有它的应用场景。那么，我们该怎么判断网络开销会出现在哪些场景下呢？其实，<strong>不管是什么文件格式，也不管是哪种存储系统，访问数据源是否会引入网络开销，取决于任务与数据的本地性关系，也就是任务的本地性级别</strong>，它一共有4种：</p><!-- [[[read_end]]] --><ul>\n<li>PROCESS_LOCAL：任务与数据同在一个JVM进程中</li>\n<li>NODE_LOCAL：任务与数据同在一个计算节点，数据可能在磁盘上或是另一个JVM进程中</li>\n<li>RACK_LOCAL：任务与数据不在同一节点，但在同一个物理机架上</li>\n<li>ANY：任务与数据是跨机架、甚至是跨DC（Data Center，数据中心）的关系</li>\n</ul><p>根据定义我们很容易判断出，不同本地性级别下的计算任务是否会引入磁盘或网络开销，结果如下表所示。从表格中我们不难发现，从PROCESS_LOCAL到ANY，数据访问效率是逐级变差的。在读取数据源阶段，数据还未加载到内存，任务没有办法调度到PROCESS_LOCAL级别。因此，这个阶段我们能够调度的最佳级别是NODE_LOCAL。</p><p><img src=\"https://static001.geekbang.org/resource/image/15/36/1532cde0f3956d77e44byy7138453736.jpeg?wh=1390*680\" alt=\"\" title=\"不同本地性级别与磁盘、网络开销的关系\"></p><p>根据NODE_LOCAL的定义，在这个级别下，调度的目标节点至少在磁盘上存有Spark计算任务所需的数据分片。这也就意味着，在集群部署上，Spark集群与外部存储系统在物理上是紧紧耦合在一起的。相反，如果Spark集群与存储集群在物理上是分开的，那么任务的本地性级别只能退化到RACK_LOCAL，甚至是ANY，来通过网络获取所需的数据分片。</p><p><strong>因此，对于Spark加HDFS和Spark加MongoDB来说，是否会引入网络开销完全取决于它们的部署模式。物理上紧耦合，在NODE_LOCAL级别下，Spark用磁盘I/O替代网络开销获取数据；物理上分离，网络开销就无法避免。</strong></p><p>除此之外，物理上的隔离与否同样会影响数据的写入效率。当数据处理完毕，需要将处理结果落盘到外部存储的时候，紧耦合模式下的数据写入会把数据分片落盘到本地节点，避免网络开销。</p><p>值得一提的是，在企业的私有化DC中更容易定制化集群的部署方式，大家通常采用紧耦合的方式来提升数据访问效率。但是在公有云环境中，计算集群在物理上往往和存储系统隔离，因此数据源的读取只能走网络。</p><p>通过上面的分析，对于数据读写占比较高的业务场景，我们就可以通过在集群的部署模式上做规划，从而在最开始部署Spark集群的时候就提前做好准备。</p><h2>数据处理</h2><p>数据读取完成后，就进入数据处理环节了。那在数据处理的过程中，都有哪些技巧能够帮助减少网络开销呢？</p><h3>能省则省</h3><p>说起数据处理中的网络开销，我猜你最先想到的操作就是Shuffle。Shuffle作为大多数计算场景的“性能瓶颈担当”，确实是网络开销的罪魁祸首。根据“能省则省”的开发原则，我们自然要想尽办法去避免Shuffle。在数据关联的场景中，省去Shuffle最好的办法，就是把Shuffle Joins转化为Broadcast Joins。关于这方面的调优技巧，我们在广播变量那几讲有过详细的讲解，你可以翻回去看一看。尽管广播变量的创建过程也会引入网络传输，但是，两害相权取其轻，相比Shuffle的网络开销，广播变量的开销算是小巫见大巫了。</p><p>遵循“能省则省”的原则，把Shuffle消除掉自然是最好的。如果实在没法避免Shuffle，我们要尽可能地在计算中多使用Map端聚合，去减少需要在网络中分发的数据量。这方面的典型做法就是用reduceByKey、aggregateByKey替换groupByKey，不过在RDD API使用频率越来越低的当下，这个调优技巧实际上早就名存实亡了。但是，Map端聚合的思想并不过时。为什么这么说呢？下面，我通过一个小例子来你详细讲一讲。</p><p>在绝大多数2C（To Consumer）的业务场景中，我们都需要刻画用户画像。我们的小例子就是“用户画像”中的一环，：给定用户表，按照用户群组统计兴趣列表，要求兴趣列表内容唯一，也就是不存在重复的兴趣项，用户表的Schema如下表所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/80/83/80ff54fe0683b8e7aeea408539998383.jpeg?wh=1738*551\" alt=\"\" title=\"用户表Schema示例\"></p><p>要获取群组兴趣列表，我们应该先按照groupId分组，收集群组内所有用户的兴趣列表，然后再把列表中的兴趣项展平，最后去重得到内容唯一的兴趣列表。应该说思路还是蛮简单的，我们先来看第一版实现代码。</p><pre><code>val filePath: String = _\nval df = spark.read.parquent(filePath)\ndf.groupBy(“groupId”)\n.agg(array_distinct(flatten(collect_list(col(“interestList”)))))\n</code></pre><p>这版实现分别用collect_list、flatten和array_distinct，来做兴趣列表的收集、展平和去重操作，它完全符合业务逻辑。不过，见到“收集”类的操作，比如groupByKey，以及这里的collect_list，我们应该本能地提高警惕。因为这类操作会把最细粒度的全量数据在全网分发。相比其他算子，这类算子引入的网络开销最大。</p><p>那<strong>我们是不是可以把它们提前到Map端，从而减少Shuffle中需要分发的数据量呢</strong>？当然可以。比如，对于案例中的收集操作，我们可以在刚开始收集兴趣列表的时候就在Map端做一次去重，然后去查找DataFrame开发API，看看有没有与collect_list对应的Map端聚合算子。</p><p><strong>因此，在数据处理环节，我们要遵循“能省则省”的开发原则，主动削减计算过程中的网络开销。对于数据关联场景，我们要尽可能地把Shuffle Joins转化为Broadcast Joins来消除Shuffle。如果确实没法避免Shuffle，我们可以在计算中多使用Map端聚合，减少需要在网络中分发的数据量。</strong></p><p>除了Shuffle之外，还有个操作也会让数据在网络中分发，这个操作很隐蔽，我们经常注意不到它，它就是多副本的RDD缓存。</p><p>比如说，在实时流处理这样的场景下，对于系统的高可用性，应用的要求比较高，因此你可能会用“_2”甚至是“_3”的存储模式，在内存和磁盘中缓存多份数据拷贝。当数据副本数大于1的时候，本地数据分片就会通过网络被拷贝到其他节点，从而产生网络开销。虽然这看上去只是存储模式字符串的一个微小改动，但在运行时，它会带来很多意想不到的开销。因此，如果你的应用对高可用特性没有严格要求，我建议你尽量不要滥用多副本的RDD缓存，</p><h2>数据传输</h2><p>最后就到了数据传输的环节。我们知道，在落盘或是在网络传输之前，数据都是需要先进行序列化的。在Spark中，有两种序列化器供开发者选择，分别是Java serializer和Kryo Serializer。Spark官方和网上的技术博客都会推荐你使用Kryo Serializer来提高效率，通常来说，Kryo Serializer相比Java serializer，在处理效率和存储效率两个方面都会胜出数倍。因此，在数据分发之前，使用Kryo Serializer对其序列化会进一步降低网络开销。</p><p>不过，经常有同学向我抱怨：“为什么我用了Kryo Serializer，序列化之后的数据尺寸反而比Java serializer的更大呢？”注意啦，这里我要提醒你：<strong>对于一些自定义的数据结构来说，如果你没有明确把这些类型向Kryo Serializer注册的话，虽然它依然会帮你做序列化的工作，但它序列化的每一条数据记录都会带一个类名字，这个类名字是通过反射机制得到的，会非常长。在上亿的样本中，存储开销自然相当可观。</strong></p><p>那该怎么向Kryo Serializer注册自定义类型呢？其实非常简单，<strong>我们只需要在SparkConf之上调用registerKryoClasses方法就好了</strong>，代码示例如下所示。</p><pre><code>//向Kryo Serializer注册类型\nval conf = new SparkConf().setMaster(“”).setAppName(“”)\nconf.registerKryoClasses(Array(\nclassOf[Array[String]],\nclassOf[HashMap[String, String]],\nclassOf[MyClass]\n))\n</code></pre><p>另外，与Kryo Serializer有关的配置项，我也把它们汇总到了下面的表格中，方便你随时查找。其中，spark.serializer可以明确指定Spark采用Kryo Serializer序列化器。而spark.kryo.registrationRequired就比较有意思了，如果我们把它设置为True，当Kryo Serializer遇到未曾注册过的自定义类型的时候，它就不会再帮你做序列化的工作，而是抛出异常，并且中断任务执行。这么做的好处在于，在开发和调试阶段，它能帮我们捕捉那些忘记注册的类型。</p><p><img src=\"https://static001.geekbang.org/resource/image/82/64/821f4c71a21bf22c30187ca7fc3be064.jpeg?wh=1920*549\" alt=\"\" title=\"与Kryo Serializer有关的配置项\"></p><p>为了方便你理解，我们不妨把Java serializer和Kryo Serializer比作是两个不同的搬家公司。</p><p>Java Serializer是老牌企业，市场占有率高，而且因为用户体验很好，所以非常受欢迎。只要你出具家庭住址，Java Serializer会派专人到家里帮你打包，你并不需要告诉他们家里都有哪些物件，他们对不同种类的物品有一套自己的打包标准，可以帮你省去很多麻烦。不过，Java Serializer那套打包标准过于刻板，不仅打包速度慢，封装出来的包裹往往个头超大、占地儿，你必须租用最大号的货车才能把家里所有的物品都装下。</p><p>Kryo Serializer属于市场新贵，在打包速度和包裹尺寸方面都远胜Java Serializer。Kryo Serializer会以最紧凑的方式打包，一寸空间也不浪费，因此，所有包裹用一辆小货车就能装下。但是，订阅Kryo Serializer的托管服务之前，用户需要提供详尽的物品明细表，因此，很多用户都嫌麻烦，Kryo Serializer市场占有率也就一直上不去。</p><p>好啦，现在你是不是对Java Serializer和Kryo Serializer有了更深的理解了？由此可见，如果你想要不遗余力去削减数据传输过程中的网络开销，就可以尝试使用Kryo Serializer来做数据的序列化。相反，要是你觉得开发成本才是核心痛点，那采用默认的Java Serializer也未尝不可。</p><h2>小结</h2><p>这一讲，面对数据处理不同阶段出现的网络开销，我带你总结出了有效降低它的办法。</p><p>首先，在数据读取阶段，要想获得NODE_LOCAL的本地级别，我们得让Spark集群与外部存储系统在物理上紧紧耦合在一起。这样，Spark就可以用磁盘I/O替代网络开销获取数据，否则，本地级别就会退化到RACK_LOCAL或者ANY，那网络开销就无法避免。</p><p>其次，在数据处理阶段，我们应当遵循“能省则省”的开发原则，在适当的场景用Broadcast Joins来避免Shuffle引入的网络开销。如果确实没法避免Shuffle，我们可以在计算中多使用Map端聚合，减少需要在网络中分发的数据量。另外，如果应用对于高可用的要求不高，那我们应该尽量避免副本数量大于1的存储模式，避免副本跨节点拷贝带来的额外开销。</p><p>最后，在数据通过网络分发之前，我们可以利用Kryo Serializer序列化器，提升序列化字节的存储效率，从而有效降低在网络中分发的数据量，整体上减少网络开销。需要注意的，为了充分利用Kryo Serializer序列化器的优势，开发者需要明确注册自定义的数据类型，否则效果可能适得其反。</p><h2>每日一练</h2><ol>\n<li>对于文中Map端聚合的示例，你知道和collect_list对应的Map端聚合算子是什么吗？</li>\n<li>你还能想到哪些Map端聚合的计算场景？</li>\n<li>对于不同的数据处理阶段，你还知道哪些降低网络开销的办法吗？</li>\n</ol><p>期待在留言区看到你的思考和答案，也欢迎你把这一讲转发出去，我们下一讲见！</p>","comments":[{"had_liked":false,"id":290427,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1619538444,"is_pvip":false,"replies":[{"id":"105297","content":"好问题。<br><br>先来说疑问1：没错，从磁盘文件系统读数据，最好的本地性级别就是NODE_LOCAL。不过，现在有一些内存文件系统，比如Alluxio。我们之前做过Alluxio和Spark集成，这个时候数据读取，最好的时候，就是PROCESS_LOCAL。其实，Alluxio的文件存储，本质上相当于OFF HEAP的cache，因此可以做到PROCESS_LOCAL。<br><br>因此，很显然，Cache是可以保证PROCESS_LOCAL的。但是，要做到PROCESS_LOCAL，却不是一定非要做cache。举个例子，Broadcast Joins机制下，小表的广播，读取的时候也是PROCESS_LOCAL。当然了，你也可以说，广播是一种特殊的缓存，这么说也没毛病。再举个栗子，Collocated joins，你不妨搜搜这个数据关联的计算过程，它的计算，也是PROCESS_LOCAL的。总之一句话，Cache可以保证PROCESS_LOCAL，但是要做到PROCESS_LOCAL，不一定非要做Cache。<br><br>疑问2：其实上面说过了，没错，Cache可以保证后面的Task就是PROCESS_LOCAL级别。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619625986,"ip_address":"","comment_id":290427,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14504440332","product_id":100073401,"comment_content":"有点疑问<br> 1.没有RDD缓存的情况下，是不是最好的任务级别是node_level，而且是最底下包含scan操作的tasks是这个级别？<br>因为:最开始数据全在磁盘，第一个stage生成的task本地性级别最好为node_level，后面的stage生成的task都是需要进行shuffle，最起码也是rack_node。<br>换句话说，是不是只有前一个action触发了RDD缓存操作，后一个action里面的任务才有可能是process_level呢？<br>2.当RDD被缓存了，由BlockManager进行管理，满足了「有个人或有个地方记录了什么数据存储在什么地方」，这样后续生成Task时就可以有process_local级别的任务出现了。<br>不知道理解的对吗？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519200,"discussion_content":"好问题。\n\n先来说疑问1：没错，从磁盘文件系统读数据，最好的本地性级别就是NODE_LOCAL。不过，现在有一些内存文件系统，比如Alluxio。我们之前做过Alluxio和Spark集成，这个时候数据读取，最好的时候，就是PROCESS_LOCAL。其实，Alluxio的文件存储，本质上相当于OFF HEAP的cache，因此可以做到PROCESS_LOCAL。\n\n因此，很显然，Cache是可以保证PROCESS_LOCAL的。但是，要做到PROCESS_LOCAL，却不是一定非要做cache。举个例子，Broadcast Joins机制下，小表的广播，读取的时候也是PROCESS_LOCAL。当然了，你也可以说，广播是一种特殊的缓存，这么说也没毛病。再举个栗子，Collocated joins，你不妨搜搜这个数据关联的计算过程，它的计算，也是PROCESS_LOCAL的。总之一句话，Cache可以保证PROCESS_LOCAL，但是要做到PROCESS_LOCAL，不一定非要做Cache。\n\n疑问2：其实上面说过了，没错，Cache可以保证后面的Task就是PROCESS_LOCAL级别。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619625986,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290312,"user_name":"kingcall","can_delete":false,"product_type":"c1","uid":1056982,"ip_address":"","ucode":"508884DC684B5B","user_header":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","comment_is_top":false,"comment_ctime":1619493754,"is_pvip":false,"replies":[{"id":"105309","content":"Nice~ aggregateByKey玩儿的很溜啊~ 不过咱们这个例子其实可以用collect_set哈~<br><br>不过你说的对，确实RDD更灵活，DataFrame抽象更高，那也就意味着灵活性变差。驴和熊猫不可兼得么，low level API比如RDD，高阶算子，开发灵活性非常高，但是优化空间有限；high level API比如DataFrame，Spark SQL优化空间大，但是就没有RDD开发起来那么灵活了。<br><br>RDD和DataFrame的恩怨情仇，第20讲可以了解一下~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619651617,"ip_address":"","comment_id":290312,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10209428346","product_id":100073401,"comment_content":"DataFrame 要想实现map 端预聚合只能靠优化器自己了吧比较抽象层次比较高了，灵活度就降低了，但是RDD 的话还是可以自己实现实现的，虽然reduceBykey可以预聚合，但是在这个例子中不合适，不能替换collect_list，对于RDD 我们呢可以使用aggregateByKey<br>    def localDistinct(set: Set[String], b: String): mutable.Set[String] = {<br>      set.add(b)<br>      set<br>    }<br>    def combineDistinct(set1: Set[String], set2: Set[String]): mutable.Set[String] = {<br>      set1 ++ set2<br>    }<br>    rdd.map(o =&gt; (o.id, o.name)).aggregateByKey(Set[String]())(localDistinct, combineDistinct).foreach(println(_))","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519169,"discussion_content":"Nice~ aggregateByKey玩儿的很溜啊~ 不过咱们这个例子其实可以用collect_set哈~\n\n不过你说的对，确实RDD更灵活，DataFrame抽象更高，那也就意味着灵活性变差。驴和熊猫不可兼得么，low level API比如RDD，高阶算子，开发灵活性非常高，但是优化空间有限；high level API比如DataFrame，Spark SQL优化空间大，但是就没有RDD开发起来那么灵活了。\n\nRDD和DataFrame的恩怨情仇，第20讲可以了解一下~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619651617,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301149,"user_name":"really_z","can_delete":false,"product_type":"c1","uid":2625120,"ip_address":"","ucode":"935AEC20813A93","user_header":"https://static001.geekbang.org/account/avatar/00/28/0e/60/9b9e28f8.jpg","comment_is_top":false,"comment_ctime":1625553942,"is_pvip":false,"replies":[{"id":"109178","content":"好问题，这个得说回Spark调度系统，可以再回顾下调度系统。不过在那一讲，我们确实没有展开说Locality具体如何确定。是这样的，DAGScheduler在解析完DAG之后，会创建Stages，在每个Stages内部，DAGScheduler会创建TaskSet，这个其实就是Task的集合。Locality实际上是Task的属性，因此，本地性级别是在这个时候确定的。<br><br>那么问题来了，具体怎么确定呢？这个可能就比较细致了，对于HadoopRDD来说，它的每个数据分片，其实都可以从HDFS的Name Node元数据中查出数据分片所在节点，因此DAGScheduler很容易根据这些信息，来判断这个数据分片对应的Task，它的本地性倾向，也就是倾向于哪个计算节点。<br><br>对于一般的RDD来说，BlockManager会记录数据分片对应的ExecutorId和HostId，因此DAGScheduler结合这些信息，也很容易判断，这些数据分片对应的Task，应该调度到哪个Executor进程、哪个节点。<br><br>不知道这么说能不能解答你的问题哈~ 有问题继续留言~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1625801489,"ip_address":"","comment_id":301149,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5920521238","product_id":100073401,"comment_content":"老师，你好，想问一下本地性级别是在哪个环节确定的呢？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522910,"discussion_content":"好问题，这个得说回Spark调度系统，可以再回顾下调度系统。不过在那一讲，我们确实没有展开说Locality具体如何确定。是这样的，DAGScheduler在解析完DAG之后，会创建Stages，在每个Stages内部，DAGScheduler会创建TaskSet，这个其实就是Task的集合。Locality实际上是Task的属性，因此，本地性级别是在这个时候确定的。\n\n那么问题来了，具体怎么确定呢？这个可能就比较细致了，对于HadoopRDD来说，它的每个数据分片，其实都可以从HDFS的Name Node元数据中查出数据分片所在节点，因此DAGScheduler很容易根据这些信息，来判断这个数据分片对应的Task，它的本地性倾向，也就是倾向于哪个计算节点。\n\n对于一般的RDD来说，BlockManager会记录数据分片对应的ExecutorId和HostId，因此DAGScheduler结合这些信息，也很容易判断，这些数据分片对应的Task，应该调度到哪个Executor进程、哪个节点。\n\n不知道这么说能不能解答你的问题哈~ 有问题继续留言~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625801489,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291654,"user_name":"静心","can_delete":false,"product_type":"c1","uid":1789481,"ip_address":"","ucode":"B80DE4B5C923D3","user_header":"https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg","comment_is_top":false,"comment_ctime":1620400657,"is_pvip":false,"replies":[{"id":"105738","content":"用collect_set没问题哈，你说的对，应该按照groupId做分组，以分组为粒度去重，但是，你想，如果在同一个Executor中，就存在同样的groupId，他们对应的兴趣列表有重复的兴趣项，那么我们是不是应该在Map端，也就是Shuffle前，就给他们去重呢？所以说，用collect_set替换collect_list可以帮助在Map端做去重，能省则省~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620657280,"ip_address":"","comment_id":291654,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5915367953","product_id":100073401,"comment_content":"老师，文中的案例是要获取各个群组的去重兴趣列表，我觉得实现该需求就必须要基于groupId进行分组后再进行兴趣列表去重。那我们应该怎样在map端使用collect_set实现该需求以减少shuffle数据量呢？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519534,"discussion_content":"用collect_set没问题哈，你说的对，应该按照groupId做分组，以分组为粒度去重，但是，你想，如果在同一个Executor中，就存在同样的groupId，他们对应的兴趣列表有重复的兴趣项，那么我们是不是应该在Map端，也就是Shuffle前，就给他们去重呢？所以说，用collect_set替换collect_list可以帮助在Map端做去重，能省则省~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620657280,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291327,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620207369,"is_pvip":false,"replies":[{"id":"105590","content":"1、2没问题~<br>collect_set是map端聚合，你说的那些操作，也都是map端聚合操作。<br><br>3的话，序列化的缓存，其实对于网络开销没什么贡献，不过倒是能节省内存开销。原因其实很简单，如果cache的副本数不大于1，Cache本身是不会涉及网络传输的。<br><br>但如果Cache的副本大于1，你说的就是对的，由于副本大于1，因此副本需要走网络upload到其他Executors或是hosts，序列化之后的数据自然更小，网络开销更小。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620315612,"ip_address":"","comment_id":291327,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5915174665","product_id":100073401,"comment_content":"1. org.apache.spark.sql.functions中用来collect就两个函数，一个collect_list可以有重复元素，一个collect_set元素唯一<br>2. 好像没怎么理解问题的意思哈哈，聚合的话无非就是计数、就和、平均值、最大值最小值这些吗<br>3. cache的时候，数据以序列化的形式进行缓存（比如，StorageLevel.MEMORY_ONLY_SER），数据变小了，是不是也可以间接减小网络传输的开销，但是反序列化也会消耗更多的cpu","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519441,"discussion_content":"1、2没问题~\ncollect_set是map端聚合，你说的那些操作，也都是map端聚合操作。\n\n3的话，序列化的缓存，其实对于网络开销没什么贡献，不过倒是能节省内存开销。原因其实很简单，如果cache的副本数不大于1，Cache本身是不会涉及网络传输的。\n\n但如果Cache的副本大于1，你说的就是对的，由于副本大于1，因此副本需要走网络upload到其他Executors或是hosts，序列化之后的数据自然更小，网络开销更小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620315612,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":329510,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1641380935,"is_pvip":false,"replies":[{"id":"120203","content":"一个个说哈<br>1）除了基本类型，凡是用户（开发者）自定义的class，都需要单独注册<br>2）如果是spark with Hive（Metastore）的话，那其实仅仅是用Hive Metastore来存元信息，执行引擎是Spark（SQL)。虽然看上去是Hive SQL，但实际是Spark SQL，从2.0开始，Spark跟Hive用一样的语法解析器ANTLR，所以Hive SQL，Spark完全能解析。所以说，看上去是Hive SQL，但实际上是被Spark SQL来解析、执行的。所以这里并不存在说hive函数与spark api的对应关系哈<br>3）老弟还需要再仔细看看答案，这里的关键，是区分资源调度和任务调度，这两个弄清楚了，其实答案就清楚了。之所以copy之前的答案，因为，在我看来，这类问题的本质，其实都是两种调度系统之间的区别、联系，没有搞清楚~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1641724186,"ip_address":"","comment_id":329510,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641380935","product_id":100073401,"comment_content":"老师您好 我有几个问题：<br>1. 关于 Kryo Serializer 在哪里可以看到它已经支持的数据类型呢（就是不需要注册可以直接序列化）？<br>2. 我们是spark with hive的部署方式，脚本是用hive sql开发的，而不是 dataframe API，那在哪里可以找到hql中的函数&#47;语句和 dataframe API 中算子的对应关系呢？比如hql中count(a)在底层会被转化为reduceByKey去执行？<br>3. 对于 @在路上 同学的问题的回答，您好像是复制的上一节的一个回答，但是好像并没有解答他的问题呢。。我也比较好奇这个问题的答案是什么<br>谢谢老师！","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":544820,"discussion_content":"一个个说哈\n1）除了基本类型，凡是用户（开发者）自定义的class，都需要单独注册\n2）如果是spark with Hive（Metastore）的话，那其实仅仅是用Hive Metastore来存元信息，执行引擎是Spark（SQL)。虽然看上去是Hive SQL，但实际是Spark SQL，从2.0开始，Spark跟Hive用一样的语法解析器ANTLR，所以Hive SQL，Spark完全能解析。所以说，看上去是Hive SQL，但实际上是被Spark SQL来解析、执行的。所以这里并不存在说hive函数与spark api的对应关系哈\n3）老弟还需要再仔细看看答案，这里的关键，是区分资源调度和任务调度，这两个弄清楚了，其实答案就清楚了。之所以copy之前的答案，因为，在我看来，这类问题的本质，其实都是两种调度系统之间的区别、联系，没有搞清楚~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641724186,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":324870,"user_name":"在路上","can_delete":false,"product_type":"c1","uid":1233398,"ip_address":"","ucode":"F4CEB45076F9B8","user_header":"https://static001.geekbang.org/account/avatar/00/12/d1/f6/d75afb79.jpg","comment_is_top":false,"comment_ctime":1638695148,"is_pvip":false,"replies":[{"id":"118143","content":"好问题~ 我们分资源调度和任务调度两种情况来说。<br><br>Spark在做任务调度之前，SchedulerBackend封装的调度器，比如Yarn、Mesos、Standalone，实际上已经完成了资源调度，换句话说，整个集群有多少个containers&#47;executors，已经是一件确定的事情了。而且，每个Executors的CPU和内存，也都是确定的了（因为你启动Spark集群的时候，使用配置项指定了每个Executors的CPU和内存分别是多少）。资源调度器在做资源调度的时候，确实是同时需要CPU和内存信息的。<br><br>资源调度完成后，Spark开始任务调度，你的问题，其实是任务调度范畴的问题。也就是TaskScheduler在准备调度任务的时候，要事先知道都有哪些Executors可用，注意，是可用。也就是TaskScheduler的核心目的，在于获取“可用”的Executors。<br><br>现在来回答你的问题，也就是：为什么ExecutorData不存储于内存相关的信息。答案是：不需要。一来，TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。二来，每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。<br><br>再来说Free Memory，首先，我们说过，Spark对于内存的预估不准，再者，每个Executors的可用内存都会随着GC的执行而动态变化，因此，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。一者是不准，二来确实没用，因为TaskScheduler拿不到数据分片大小这样的信息，TaskScheduler在Driver端，而数据分片是在目标Executors，所以TaskScheduler拿到Free Memory也没啥用，因为它也不能判断说：task要处理的数据分片，是不是超过了目标Executors的可用内存。<br><br>综上，ExecutorData的数据结构中，只保存了CPU信息，而没有记录内存消耗等信息。不知道这些能不能解答你的问题？有问题再聊哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639060791,"ip_address":"","comment_id":324870,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1638695148","product_id":100073401,"comment_content":"老师好。请教下，spark在申请executor时候有没有考虑读取数据时候的数据本地性呢，实际生产中集群特别大，在生成hadooprdd的时候大概率可能不能数据和executor在同一个机器甚至机架上吧？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537444,"discussion_content":"好问题~ 我们分资源调度和任务调度两种情况来说。\n\nSpark在做任务调度之前，SchedulerBackend封装的调度器，比如Yarn、Mesos、Standalone，实际上已经完成了资源调度，换句话说，整个集群有多少个containers/executors，已经是一件确定的事情了。而且，每个Executors的CPU和内存，也都是确定的了（因为你启动Spark集群的时候，使用配置项指定了每个Executors的CPU和内存分别是多少）。资源调度器在做资源调度的时候，确实是同时需要CPU和内存信息的。\n\n资源调度完成后，Spark开始任务调度，你的问题，其实是任务调度范畴的问题。也就是TaskScheduler在准备调度任务的时候，要事先知道都有哪些Executors可用，注意，是可用。也就是TaskScheduler的核心目的，在于获取“可用”的Executors。\n\n现在来回答你的问题，也就是：为什么ExecutorData不存储于内存相关的信息。答案是：不需要。一来，TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。二来，每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。\n\n再来说Free Memory，首先，我们说过，Spark对于内存的预估不准，再者，每个Executors的可用内存都会随着GC的执行而动态变化，因此，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。一者是不准，二来确实没用，因为TaskScheduler拿不到数据分片大小这样的信息，TaskScheduler在Driver端，而数据分片是在目标Executors，所以TaskScheduler拿到Free Memory也没啥用，因为它也不能判断说：task要处理的数据分片，是不是超过了目标Executors的可用内存。\n\n综上，ExecutorData的数据结构中，只保存了CPU信息，而没有记录内存消耗等信息。不知道这些能不能解答你的问题？有问题再聊哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639060791,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":315953,"user_name":"To_Drill","can_delete":false,"product_type":"c1","uid":1415245,"ip_address":"","ucode":"7E6FFAF1986EC8","user_header":"https://static001.geekbang.org/account/avatar/00/15/98/4d/582d24f4.jpg","comment_is_top":false,"comment_ctime":1634086545,"is_pvip":false,"replies":[{"id":"114515","content":"是的，没错~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634135889,"ip_address":"","comment_id":315953,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634086545","product_id":100073401,"comment_content":"问题三: 对数据源进行压缩在读取数据阶段可以降低网络开销，在shuffle阶段落盘的时候采用压缩可以降低shuffle阶段的网络开销，不过CPU的负担就变重了，还是得结合具体的场景来抉择","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528214,"discussion_content":"是的，没错~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634135889,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290327,"user_name":"Jay","can_delete":false,"product_type":"c1","uid":1066131,"ip_address":"","ucode":"488BF5D01D3CFD","user_header":"https://static001.geekbang.org/account/avatar/00/10/44/93/2d3d5868.jpg","comment_is_top":false,"comment_ctime":1619501117,"is_pvip":false,"replies":[{"id":"105231","content":"推荐用Spark SQL哈~ Spark SQL内置了很多优化机制（Catalyst、Tungsten），这个我们在20-23这4讲会详细展开。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619512109,"ip_address":"","comment_id":290327,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1619501117","product_id":100073401,"comment_content":"文中提到“&#39;RDD API使用频率越来越低”。<br>公司一直还用的是Rdd, 我也一直没学过spark sql。是否有必要切到spark sql呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519174,"discussion_content":"推荐用Spark SQL哈~ Spark SQL内置了很多优化机制（Catalyst、Tungsten），这个我们在20-23这4讲会详细展开。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619512109,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290239,"user_name":"zxk","can_delete":false,"product_type":"c1","uid":1221195,"ip_address":"","ucode":"4BB2BD9D2BCD04","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","comment_is_top":false,"comment_ctime":1619444782,"is_pvip":false,"replies":[{"id":"105233","content":"答得挺好~ 不过有个细节有疑问。<br><br>问题二：“预聚合不影响正确性的”，我能想到的是“依赖排序的聚合计算”，比如分位数（不同百分位、中位数等等）的计算，你指的是这种操作吗？","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619513002,"ip_address":"","comment_id":290239,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1619444782","product_id":100073401,"comment_content":"问题一: reduceBykey 可以在 map 端预聚合<br>问题二：在 map 端聚合的场景，比如求某个 key 的数量，求和等，业务方面来看凡是预聚合不影响正确性的都可以先在 map 端做聚合<br>问题三: 在读取数据源阶段，可以尽可能将 executor 落在数据同节点上，实现node local，再次就是同个机架下，实现 rack local。数据Shuffle时，可以考虑使用Broadcast代替，或者先在 map 端预聚合减少数据量，以及只传输时用到的字段。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519143,"discussion_content":"答得挺好~ 不过有个细节有疑问。\n\n问题二：“预聚合不影响正确性的”，我能想到的是“依赖排序的聚合计算”，比如分位数（不同百分位、中位数等等）的计算，你指的是这种操作吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619513002,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}