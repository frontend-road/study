{"id":734931,"title":"23｜Rust与大模型：用 Candle 做一个聊天机器人","content":"<p>你好，我是Mike。今天我们来聊一聊如何用Rust做一个基于大语言模型的聊天机器人。</p><p>大语言模型（LLM）是2023年最火的领域，没有之一。这波热潮是由OpenAI的ChatGPT在今年上半年发布后引起的，之后全世界的研究机构和科技公司都卷入了大模型的竞争中。目前业界应用大模型训练及推理的主要语言是Python和C/C++。Python一般用来实现上层框架，而C/C++一般起底层高性能执行的作用，比如著名的框架 PyTorch，它的上层业务层面是用Python写的，下层执行层面由C执行，因为GPU加速的部分只能由C/C++来调用。</p><p>看起来大语言模型好像和Rust没什么关系，必须承认，由于历史积累的原因，在AI这一块儿Rust的影响力还非常小。但从另一方面来讲呢，Rust是目前业界除Python、C++ 外，唯一有潜力在未来20年的AI 发展中发挥重要作用的语言了。为什么这么说呢？</p><p>首先Rust的性能与C/C++一致，并且在调用GPU能力方面也同样方便；其次，Rust强大的表达能力，不输于Python，这让人们使用Rust做业务并不难；然后，Rust的cargo编译成单文件的能力，以及对WebAssembly完善的支持，部署应用的时候非常方便，这比Py + C/C++组合需要安装的一堆依赖和数G的库方便太多。</p><!-- [[[read_end]]] --><p>目前Rust生态中其实已经有很多AI相关的基础设施了，你可以从我给出的<a href=\"https://www.arewelearningyet.com/\">链接</a>里找到。</p><p>世界上最大的机器学习模型仓库平台 HuggingFace（机器学习领域的Github） 推出了 Rust 机器学习框架 <a href=\"https://github.com/huggingface/candle\">Candle</a>。在这个官方代码仓库上，HuggingFace上解释了为什么要做一个Rust的机器学习框架。</p><ol>\n<li>Candle上云端的 Serverless 推理可行。PyTorch那一套体积太大，安装完得几个G，而Candle编译后的可执行文件才十几M到几十M。</li>\n<li>Candle可以让你避免Python的 <a href=\"https://www.backblaze.com/blog/the-python-gil-past-present-and-future/\">GIL</a>，从而提高性能。</li>\n<li>HuggingFace已经用Rust写了不少基础工具了，比如 <a href=\"https://github.com/huggingface/safetensors\">safetensors</a> 和 <a href=\"https://github.com/huggingface/tokenizers\">tokenizers</a>。</li>\n</ol><p>Elon Musk的 <a href=\"https://x.ai/\">x.ai</a> 发布后，页面上也有一段对Rust的溢美之词：Rust被证明是一个理想的选择，用于构建可扩展的、可靠的、可维护性的基础设施。它提供了高性能、丰富的生态系统，它能阻止大部分错误，这些错误在分布式系统中经常会碰到。由于我们的团队规模很小，基础设施的可靠性就显得至关重要，否则维护工作会浪费大量创新的时间。Rust给我们提供了信心，任何的代码修改或重构都可以产出可工作的程序，并且在最小监管下可以持续运行好几个月（而不会出问题）。</p><p>Musk甚至说Rust语言是未来<a href=\"https://zhuanlan.zhihu.com/p/648565007\">构建 AGI</a> 的语言。</p><p>在使用Rust尝试做聊天机器人之前，我们先来了解一下相关的背景知识。</p><h2>大语言模型背景知识</h2><p>这节课我们还是主要讲Rust及Rust的应用，所以相关的背景知识我们就简单概括一下。</p><p>机器学习（Machine Learning，ML）泛指用计算机对数据集按照一定的算法进行数据处理、分析、聚类、回归等。在执行前，人往往不知道结果是什么，所以叫机器学习。机器学习可以用于自动提取信息，自动或辅助人类做决策。</p><p>而神经网络（Neural Network）是机器学习的一类算法，它是模拟人的大脑神经元和连接的一种算法。目前整个业界投入资源最多的就是在这类算法上面，在这个类别中的创新也是最多的，有种观点认为神经网络算法是通向真正的AI最可能正确的路径。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/91/6bc0b61202yy14dfb11a834d0a649b91.png?wh=1525x811\" alt=\"图片\" title=\"图片来源：https://victorzhou.com/media/nn-series/network.svg\n\"></p><p>而深度学习（Deep Learning），其实就是层次很多很深的神经网络。你可以想象，层次越深，节点数（图里那些圈）越多，就越能模拟大脑。但是深度学习带来的问题就是，层次越深，节点数越多，则那些线（就是权重值）就会越多，呈指数级增长，那么计算量就会越来越大。其实之前十几、二十年AI进展不大，主要就是因为这个计算能力限制了。</p><p>通过无数人的探索，我们发现可以把深度学习的计算并行化，优化了很多工程上的算法。这样就能够在大家熟悉的主要用来玩游戏的GPU显卡上运行，大大提高了计算速度。这个视角，二十多年前Nvidia的老黄就看明白了，早早地提供好了基础设施，CUDA、CUDNN等。所以现在你就看到了，一卡难求，已经严重影响到了游戏玩家的生存。</p><h3>ChatGPT与LLaMA</h3><p>那么ChatGPT是什么呢？英文是Chat Generative Pre-trained Transformer，是OpenAI提供的在线AI对话服务，具有令人惊叹的理解能力和回答能力。你可以把ChatGPT理解成一个为对话调优的预训练转换模型。GPT 3 有 1750 亿参数，有传言GPT 4 有 1.76万亿参数。参数是什么，就是图中节点之间的连线，你可以想象一下1750亿根线的场景。</p><p>OpenAI搞出了ChatGPT，让全世界惊掉了下巴，但是它是闭源的。于是Meta公司（前Facebook）的Yann LeCun（图灵奖得主）团队，搞了一个开源版本的GPT，叫 LLaMA。它也是在巨量的源数据集上进行的训练，生成了 70亿（7B），130亿（13B），650亿（65B）三个版本参数的大模型文件，可以供业界做学习和研究使用。现在已推出LLaMA 2，正在做LLaMA 3。</p><p>LLaMA搞出来后，掀起了LLM界的狂欢。全世界的团队在LLaMA的基础上，继续调优，推出了各种各样的大模型。比如国内清华的 ChatGLM, 零一万物的 Yi。国外的 Mistral、OpenChat、Starling 等。这块儿非常卷，大家都在争相推出自己调优后的版本，每天早上睡醒起来，都发现又推出了几个新的LLM。</p><p>这些训练好后的文件一般从几个G到几十G不等，也有几百G的。要运行它们，得有非常强大的机器才行。比如7B 的 LLaMA 2 文件，每个权重为一个 f16 浮点数，占两个字节，所以可以估算出要运行 LLaMA 2 模型，起码得有 14G 的内存或显存。内存还好，显存超过14G的个人用户真不多。并且，LLaMA2的模型文件是PyTorch导出的，只能由PyTorch框架来运行。</p><h3>llama.cpp与量子化方法</h3><p>车到山前必有路，大神 Georgi Gerganov 搞了一个项目：<a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a>。它是一个用C/C++重新实现引擎的版本，不需要安装PyTorch，就可以运行LLaMA 2模型文件。最关键的是，它提出了一种量子化（quantization）方法，可以将权重从 16 位量子化到8位、6位、5位、4位，甚至2位。这样，就相当于等比缩小了占用内存的规模。比如，一个4位量子化版本的LLaMA 2 7B模型，就只需要不到4G的内存/显存就能运行。这样，就能适配大多数的个人计算机了。</p><p>这种量子化方法是个重大创新，它直接促进了LLM生态的进一步繁荣。现在HuggingFace上有大量量子化后的模型，比如 <a href=\"https://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q4_K_M.gguf\">openchat_3.5.Q4_K_M.gguf</a> 就是一个OpenChat的4位量子化的版本。我们下载的时候，直接下载这些量子化后的模型文件就可以了。</p><p>请注意，这些文件是训练后的成品，我们下载它是用来做推理（infer）的，而不是训练（train）的。当然，我们可以在这些成品模型上运行调优（fine tune）。</p><h3>大模型文件格式</h3><p>目前HuggingFace上有几种常见的LLM文件格式。</p><ul>\n<li>bin格式：Pytorch导出的模型文件格式</li>\n<li>safetensors格式：HuggingFace定义的一种新的模型文件格式，有可能成为未来的主流格式。HuggingFace用Rust实现safetensors格式的解析，并导出为Py接口，请参见<a href=\"https://huggingface.co/docs/safetensors/index\">链接</a>。</li>\n<li>ggml格式：llama.cpp 项目量子化模型的前期模型格式。</li>\n<li>gguf格式：llama.cpp项目量子化模型的后期模型格式，也是现在主流的量子化LLM格式。</li>\n</ul><h3>Rust的机器学习框架</h3><p>Rust生态现在有几个比较不错的ML框架，最好的两个是：<a href=\"https://github.com/huggingface/candle\">Candle</a> 和 <a href=\"https://github.com/Tracel-AI/burn\">burn</a>。后续，我们以Candle为例来介绍。</p><h2>Candle介绍</h2><p>据Candle官网介绍，它是一个极小主义机器学习框架，也就是没什么依赖，不像Pytorch那样装一堆东西，部署起来很麻烦。但其实它也能用来训练。</p><p>它有下面这些特性：</p><ul>\n<li>HuggingFace出品。近水楼台先得月，Candle几乎能支持HuggingFace上所有的模型（有的需要经过转换）。</li>\n<li>语法简单，跟PyTorch差不多。</li>\n<li>CPU、Cuda、Metal的支持。</li>\n<li>让serverless和快速部署成为可能。</li>\n<li>模型训练。</li>\n<li>分布式计算（通过NCCL）。</li>\n<li>开箱即用的模型支持，LLaMA、Whisper、 Falcon 等等。</li>\n</ul><p>Candle不仅仅是大模型深度学习框架，它还是一个机器学习框架，因此它也支持其他的机器学习算法和强化学习（reinforcement learning）。下面我们就来看看如何利用Candle框架做一个聊天机器人。</p><p><span class=\"reference\">注：这节课的代码适用于 candle_core v0.3 版本。</span></p><h2>使用Candle做一个聊天机器人</h2><h3>下载模型文件</h3><p>我对一些大模型进行了测试，发现OpenChat的对话效果比较好，所以下面我们用OpenChat LLM来进行展示。我们会用 <a href=\"https://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q8_0.gguf\">quantized 8bit</a> 的版本。其实 4bit 的版本也是可以的，效果也非常好。</p><p>Candle 官方的示例比较复杂，我为这个课程定制了一个更简单的独立运行的<a href=\"https://github.com/miketang84/jikeshijian/tree/master/23-candle_chat\">示例</a>。你可以将这个仓库克隆下来，进入目录。在运行代码之前，下载模型文件和tokenizer.json文件。</p><pre><code class=\"language-plain\">与代码目录同级的位置，创建一个目录\nmkdir hf_hub\n进入这个目录，请下载\nhttps://huggingface.co/TheBloke/openchat_3.5-GGUF/blob/main/openchat_3.5.Q8_0.gguf\n和\nhttps://huggingface.co/openchat/openchat_3.5/blob/main/tokenizer.json\n将这个 tokenizer.json 重命名为 openchat_3.5_tokenizer.json\n</code></pre><p>目录结构：</p><pre><code class=\"language-plain\">23-candle_chat/\n    Cargo.toml\n    src/\nhf_hub/\n    openchat_3.5_tokenizer.json\n    openchat_3.5.Q8_0.gguf\n</code></pre><h3>运行演示</h3><p>然后，进入 23-candle_chat/ 运行：</p><pre><code class=\"language-plain\">cargo run --release --bin simple\n</code></pre><p>出现如下界面，就可以聊天了。</p><p><img src=\"https://static001.geekbang.org/resource/image/18/27/18d6de5afca0b6ffb61e1bc309877127.png?wh=1531x255\" alt=\"图片\"></p><p>下面是我问的一个问题，bot的回答好像有点问题，这个模型用英文问的效果会好一些。</p><p><img src=\"https://static001.geekbang.org/resource/image/6e/c1/6e420065087c82b10813d2694a67c6c1.png?wh=1920x791\" alt=\"图片\"></p><h3>代码讲解</h3><p>你可以看一下代码。</p><pre><code class=\"language-plain\">#![allow(unused)]\n\nuse std::fs::File;\nuse std::io::Write;\nuse std::path::PathBuf;\nuse tokenizers::Tokenizer;\n\nuse candle_core::quantized::gguf_file;\nuse candle_core::utils;\nuse candle_core::{Device, Tensor};\nuse candle_transformers::generation::LogitsProcessor;\nuse candle_transformers::models::quantized_llama as quantized_model;\n\nuse anyhow::Result;\n\nmod token_output_stream;\nuse token_output_stream::TokenOutputStream;\n\nstruct Args {\n&nbsp; &nbsp; tokenizer: String,\n&nbsp; &nbsp; model: String,\n&nbsp; &nbsp; sample_len: usize,\n&nbsp; &nbsp; temperature: f64,\n&nbsp; &nbsp; seed: u64,\n&nbsp; &nbsp; repeat_penalty: f32,\n&nbsp; &nbsp; repeat_last_n: usize,\n&nbsp; &nbsp; gqa: usize,\n}\n\nimpl Args {\n&nbsp; &nbsp; fn tokenizer(&amp;self) -&gt; Result&lt;Tokenizer&gt; {\n&nbsp; &nbsp; &nbsp; &nbsp; let tokenizer_path = PathBuf::from(&amp;self.tokenizer);\n&nbsp; &nbsp; &nbsp; &nbsp; Tokenizer::from_file(tokenizer_path).map_err(anyhow::Error::msg)\n&nbsp; &nbsp; }\n\n&nbsp; &nbsp; fn model(&amp;self) -&gt; Result&lt;PathBuf&gt; {\n&nbsp; &nbsp; &nbsp; &nbsp; Ok(std::path::PathBuf::from(&amp;self.model))\n&nbsp; &nbsp; }\n}\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n&nbsp; &nbsp; println!(\n&nbsp; &nbsp; &nbsp; &nbsp; \"avx: {}, neon: {}, simd128: {}, f16c: {}\",\n&nbsp; &nbsp; &nbsp; &nbsp; utils::with_avx(),\n&nbsp; &nbsp; &nbsp; &nbsp; utils::with_neon(),\n&nbsp; &nbsp; &nbsp; &nbsp; utils::with_simd128(),\n&nbsp; &nbsp; &nbsp; &nbsp; utils::with_f16c()\n&nbsp; &nbsp; );\n\n&nbsp; &nbsp; let args = Args {\n&nbsp; &nbsp; &nbsp; &nbsp; tokenizer: String::from(\"../hf_hub/openchat_3.5_tokenizer.json\"),\n&nbsp; &nbsp; &nbsp; &nbsp; model: String::from(\"../hf_hub/openchat_3.5.Q8_0.gguf\"),\n&nbsp; &nbsp; &nbsp; &nbsp; sample_len: 1000,\n&nbsp; &nbsp; &nbsp; &nbsp; temperature: 0.8,\n&nbsp; &nbsp; &nbsp; &nbsp; seed: 299792458,\n&nbsp; &nbsp; &nbsp; &nbsp; repeat_penalty: 1.1,\n&nbsp; &nbsp; &nbsp; &nbsp; repeat_last_n: 64,\n&nbsp; &nbsp; &nbsp; &nbsp; gqa: 8,\n&nbsp; &nbsp; };\n\n&nbsp; &nbsp; // load model\n&nbsp; &nbsp; let model_path = args.model()?;\n&nbsp; &nbsp; let mut file = File::open(&amp;model_path)?;\n&nbsp; &nbsp; let start = std::time::Instant::now();\n\n&nbsp; &nbsp; // This is the model instance\n&nbsp; &nbsp; let model = gguf_file::Content::read(&amp;mut file)?;\n&nbsp; &nbsp; let mut total_size_in_bytes = 0;\n&nbsp; &nbsp; for (_, tensor) in model.tensor_infos.iter() {\n&nbsp; &nbsp; &nbsp; &nbsp; let elem_count = tensor.shape.elem_count();\n&nbsp; &nbsp; &nbsp; &nbsp; total_size_in_bytes +=\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elem_count * tensor.ggml_dtype.type_size() / tensor.ggml_dtype.blck_size();\n&nbsp; &nbsp; }\n&nbsp; &nbsp; println!(\n&nbsp; &nbsp; &nbsp; &nbsp; \"loaded {:?} tensors ({}bytes) in {:.2}s\",\n&nbsp; &nbsp; &nbsp; &nbsp; model.tensor_infos.len(),\n&nbsp; &nbsp; &nbsp; &nbsp; total_size_in_bytes,\n&nbsp; &nbsp; &nbsp; &nbsp; start.elapsed().as_secs_f32(),\n&nbsp; &nbsp; );\n&nbsp; &nbsp; let mut model = quantized_model::ModelWeights::from_gguf(model, &amp;mut file)?;\n&nbsp; &nbsp; println!(\"model built\");\n\n&nbsp; &nbsp; // load tokenizer\n&nbsp; &nbsp; let tokenizer = args.tokenizer()?;\n&nbsp; &nbsp; let mut tos = TokenOutputStream::new(tokenizer);\n&nbsp; &nbsp; // left for future improvement: interactive\n&nbsp; &nbsp; for prompt_index in 0.. {\n&nbsp; &nbsp; &nbsp; &nbsp; print!(\"&gt; \");\n&nbsp; &nbsp; &nbsp; &nbsp; std::io::stdout().flush()?;\n&nbsp; &nbsp; &nbsp; &nbsp; let mut prompt = String::new();\n&nbsp; &nbsp; &nbsp; &nbsp; std::io::stdin().read_line(&amp;mut prompt)?;\n&nbsp; &nbsp; &nbsp; &nbsp; if prompt.ends_with('\\n') {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prompt.pop();\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if prompt.ends_with('\\r') {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prompt.pop();\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; let prompt_str = format!(\"User: {prompt} &lt;|end_of_turn|&gt; Assistant: \");\n&nbsp; &nbsp; &nbsp; &nbsp; print!(\"bot: \");\n\n&nbsp; &nbsp; &nbsp; &nbsp; let tokens = tos\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .tokenizer()\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .encode(prompt_str, true)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .map_err(anyhow::Error::msg)?;\n\n&nbsp; &nbsp; &nbsp; &nbsp; let prompt_tokens = tokens.get_ids();\n&nbsp; &nbsp; &nbsp; &nbsp; let mut all_tokens = vec![];\n&nbsp; &nbsp; &nbsp; &nbsp; let mut logits_processor = LogitsProcessor::new(args.seed, Some(args.temperature), None);\n\n&nbsp; &nbsp; &nbsp; &nbsp; let start_prompt_processing = std::time::Instant::now();\n&nbsp; &nbsp; &nbsp; &nbsp; let mut next_token = {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let input = Tensor::new(prompt_tokens, &amp;Device::Cpu)?.unsqueeze(0)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let logits = model.forward(&amp;input, 0)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let logits = logits.squeeze(0)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logits_processor.sample(&amp;logits)?\n&nbsp; &nbsp; &nbsp; &nbsp; };\n&nbsp; &nbsp; &nbsp; &nbsp; let prompt_dt = start_prompt_processing.elapsed();\n&nbsp; &nbsp; &nbsp; &nbsp; all_tokens.push(next_token);\n&nbsp; &nbsp; &nbsp; &nbsp; if let Some(t) = tos.next_token(next_token)? {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print!(\"{t}\");\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; std::io::stdout().flush()?;\n&nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; let eos_token = \"&lt;|end_of_turn|&gt;\";\n&nbsp; &nbsp; &nbsp; &nbsp; let eos_token = *tos.tokenizer().get_vocab(true).get(eos_token).unwrap();\n&nbsp; &nbsp; &nbsp; &nbsp; let start_post_prompt = std::time::Instant::now();\n&nbsp; &nbsp; &nbsp; &nbsp; let to_sample = args.sample_len.saturating_sub(1);\n&nbsp; &nbsp; &nbsp; &nbsp; let mut sampled = 0;\n&nbsp; &nbsp; &nbsp; &nbsp; for index in 0..to_sample {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let input = Tensor::new(&amp;[next_token], &amp;Device::Cpu)?.unsqueeze(0)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let logits = model.forward(&amp;input, prompt_tokens.len() + index)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let logits = logits.squeeze(0)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let logits = if args.repeat_penalty == 1. {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logits\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } else {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; let start_at = all_tokens.len().saturating_sub(args.repeat_last_n);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; candle_transformers::utils::apply_repeat_penalty(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &amp;logits,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; args.repeat_penalty,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &amp;all_tokens[start_at..],\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; )?\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; };\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; next_token = logits_processor.sample(&amp;logits)?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; all_tokens.push(next_token);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if let Some(t) = tos.next_token(next_token)? {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print!(\"{t}\");\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; std::io::stdout().flush()?;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sampled += 1;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if next_token == eos_token {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; };\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; if let Some(rest) = tos.decode_rest().map_err(candle_core::Error::msg)? {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print!(\"{rest}\");\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; std::io::stdout().flush()?;\n&nbsp; &nbsp; &nbsp; &nbsp; let dt = start_post_prompt.elapsed();\n&nbsp; &nbsp; &nbsp; &nbsp; println!(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"\\n\\n{:4} prompt tokens processed: {:.2} token/s\",\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prompt_tokens.len(),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prompt_tokens.len() as f64 / prompt_dt.as_secs_f64(),\n&nbsp; &nbsp; &nbsp; &nbsp; );\n&nbsp; &nbsp; &nbsp; &nbsp; println!(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"{sampled:4} tokens generated: {:.2} token/s\",\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sampled as f64 / dt.as_secs_f64(),\n&nbsp; &nbsp; &nbsp; &nbsp; );\n&nbsp; &nbsp; }\n\n&nbsp; &nbsp; Ok(())\n}\n</code></pre><p>我们分段讲解这100多行代码。</p><p>第19行，定义了Args参数，这是模型必要的参数配置定义。第42～48行，看看有哪些CPU特性支持。第50～59行，实例化Args，这些参数都是硬编码进去的，除了两个文件的路径外，需要LLM相关知识才能理解。</p><p>第62～81行，加载大模型文件，并生成模型对象。我们这个模型是GGUF格式的，因此需要用gguf_file模块来读。Tensor是LLM中的重要概念，它是一个多维数组，可以在CPU和GPU上计算，在GPU上还可以并行计算。一个大模型由很多的Tensor组成。我们这个模型中，加载进来了 291 个 Tensor。</p><p>第84行，加载tokenizer.json文件，并生成 tokenizer 实例。tokenizer 用于将输入和输出的文本转化为Tensor，变成大模型可理解的数据。第85行创建token输出流实例。第89～99行，建立问答界面，输入提示符为一个 &gt; 号，输出为 <code>bot:</code> 开头。第101～104行将输入的问答转化为 tokens。这个tokens就是Tensor实例。</p><p>第106～122行，是用于处理输入，大模型对输入的token做一处理，你可以理解成大模型对你的输入问题先要进行一下理解，然后后面才能做出对应的回答。<br>\nLogitsProcessor 是一个用于修改模型输出概率分布的工具。我们可以看到，这个过程中使用的设备写死了，用的CPU。</p><p>第124行，<code>\"&lt;|end_of_turn|&gt;\"</code> 是 OpenChat 模式定义的一轮对话结束的标志。第125～156行，就是对问题的回答。用的设备仍然为CPU，我们可以猜测应该会很慢。它这里面还有对penalty机制的处理。细节也需要去查阅大模型NLP相关的知识。</p><p>第158～167行，是对本次处理性能的一个汇总。在我的电脑上，纯用CPU计算的话，只能达到1秒一个多token的速度，非常卡。有GPU加持的话，会快很多。</p><p>你可能发现了，第87行有个循环，它是用来实现 interactive 交互效果的，问完一句，回答完，还可以问下一句。</p><h3>添加命令行参数</h3><p>前面的simple示例，我们所有的参数都是写死在代码里面的。这样方便理解，但不方便使用。我们可以尝试为它添加命令行参数功能。</p><p>在Rust中，写一个命令行非常简单，直接用clap，改几行代码就可以了。将上面示例中的Args结构体的定义变成下面这样就可以了，然后在调用的时候使用 <code>Args::parse()</code> 生成 Args 实例。</p><pre><code class=\"language-plain\">#[derive(Parser, Debug)]\n#[command(author, version, about, long_about = None)]\nstruct Args {\n&nbsp; &nbsp; #[arg(long, default_value = \"../hf_hub/openchat_3.5_tokenizer.json\")]\n&nbsp; &nbsp; tokenizer: String,\n&nbsp; &nbsp; #[arg(long, default_value = \"../hf_hub/openchat_3.5.Q8_0.gguf\")]\n&nbsp; &nbsp; model: String,\n&nbsp; &nbsp; #[arg(short = 'n', long, default_value_t = 1000)]\n&nbsp; &nbsp; sample_len: usize,\n&nbsp; &nbsp; #[arg(long, default_value_t = 0.8)]\n&nbsp; &nbsp; temperature: f64,\n&nbsp; &nbsp; #[arg(long, default_value_t = 299792458)]\n&nbsp; &nbsp; seed: u64,\n&nbsp; &nbsp; #[arg(long, default_value_t = 1.1)]\n&nbsp; &nbsp; repeat_penalty: f32,\n&nbsp; &nbsp; #[arg(long, default_value_t = 64)]\n&nbsp; &nbsp; repeat_last_n: usize,\n&nbsp; &nbsp; #[arg(long, default_value_t = 8)]\n&nbsp; &nbsp; gqa: usize,\n}\n\nfn main() {\n    // ...\n    let args = Args::parse();  \n    // ...\n}\n</code></pre><p>经过升级的命令有了下面这些参数：</p><pre><code class=\"language-plain\">$ cargo run --release --bin cli -- --help\n    Finished release [optimized] target(s) in 0.04s\n     Running `target/release/cli --help`\navx: false, neon: false, simd128: false, f16c: false\nUsage: cli [OPTIONS]\nOptions:\n      --tokenizer &lt;TOKENIZER&gt;            [default: ../hf_hub/openchat_3.5_tokenizer.json]\n      --model &lt;MODEL&gt;                    [default: ../hf_hub/openchat_3.5.Q8_0.gguf]\n  -n, --sample-len &lt;SAMPLE_LEN&gt;          [default: 1000]\n      --temperature &lt;TEMPERATURE&gt;        [default: 0.8]\n      --seed &lt;SEED&gt;                      [default: 299792458]\n      --repeat-penalty &lt;REPEAT_PENALTY&gt;  [default: 1.1]\n      --repeat-last-n &lt;REPEAT_LAST_N&gt;    [default: 64]\n      --gqa &lt;GQA&gt;                        [default: 8]\n  -h, --help                             Print help\n  -V, --version                          Print version\n</code></pre><p>是不是很方便？谁说Rust生产力不行的！</p><p>点击<a href=\"https://github.com/miketang84/jikeshijian/tree/master/23-candle_chat\">这里</a>可以找到源文件，你可以本地试试跑起一个大模型对话机器人。另外，Candle官方仓库中的<a href=\"https://github.com/huggingface/candle/tree/main/candle-examples/examples/quantized\">示例</a>功能更强大，但也更复杂，你可以继续深入研究。</p><h2>小结</h2><p>这节课我们一起探索了使用Rust利用Candle机器学习框架开发一个大模型聊天机器人的应用。Rust目前在AI界虽然还不够有影响力，但是未来是相当有潜力的，这也是为什么HuggingFace带头出一个Rust机器学习框架的原因。</p><p>不过这节课我们只是讲了怎么用起来，而如果要深入下去的话，机器学习、深度学习的基础知识就必不可少了。如果你有时间精力的话，你可以深入下去好好补充一下这方面的学术知识，毕竟未来几十年，AI是一个主要问题，也会是一个主要机会。目前AI发展速度太快了，有种学习跟不上业界发展速度的感觉。但是不管怎样，学好基础，永远不会过时。</p><p>另外，Rust AI这块儿，虽然已经小有起色，但是作为生态来讲，空白处还很多。所以这也正是学好Rust的机会，Rust可以在AI基建这块做大量的工作，这些工作可以服务于Rust社区，也可以服务于Python乃至整个AI社区。</p><h2>思考题</h2><p>你可以在我的示例上继续捣鼓，添加GPU的支持，在Linux、Windows、macOS多种平台上测试一下。欢迎你在评论区贴出你自己的代码，也欢迎你把这节课分享给其他朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"22｜Web开发（下）：如何实现一个Todo List应用？","id":734130},"right":{"article_title":"24｜Rust图像识别：利用YOLOv8识别对象","id":734943}},"comments":[{"had_liked":false,"id":386591,"user_name":"eriklee","can_delete":false,"product_type":"c1","uid":2826132,"ip_address":"北京","ucode":"6F755DB7C29DD6","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIyhbzdkFM64Npva5ZKf4IPwhy6rDAX0L77QNESbalnXhnGKibcTbwtSaNC0hO6z0icO8DYI9Nf4xwg/132","comment_is_top":false,"comment_ctime":1705158488,"is_pvip":false,"replies":[{"id":140944,"content":"有了c接口，就等于有了rust接口，因为rust调用c接口没有性能损耗。","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1705326671,"ip_address":"重庆","comment_id":386591,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"&quot;首先 Rust 的性能与 C&#47;C++ 一致，并且在调用 GPU 能力方面也同样方便&quot;\nrust目前应该还不能直接访问cuda吧？毕竟cuda是c接口","like_count":6,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635885,"discussion_content":"有了c接口，就等于有了rust接口，因为rust调用c接口没有性能损耗。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705326671,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":385760,"user_name":"听雨","can_delete":false,"product_type":"c1","uid":3793352,"ip_address":"四川","ucode":"A95C65A31FC72F","user_header":"https://static001.geekbang.org/account/avatar/00/39/e1/c8/c7ed7336.jpg","comment_is_top":false,"comment_ctime":1703263632,"is_pvip":false,"replies":[{"id":140632,"content":"我在wsl中测过，可以的。如果想要wsl支持图形界面应用，需要update到最新版wsl。","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1703577793,"ip_address":"重庆","comment_id":385760,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"老师，我下载到了window下的子Linux中运行，报OS error。请问是不是不支持在这里面运行啊","like_count":2,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":634498,"discussion_content":"我在wsl中测过，可以的。如果想要wsl支持图形界面应用，需要update到最新版wsl。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1703577793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386660,"user_name":"tan","can_delete":false,"product_type":"c1","uid":1477463,"ip_address":"重庆","ucode":"20E176CB1EFD51","user_header":"https://static001.geekbang.org/account/avatar/00/16/8b/57/a3daeaae.jpg","comment_is_top":false,"comment_ctime":1705302969,"is_pvip":false,"replies":[{"id":140976,"content":"👍","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1705410899,"ip_address":"重庆","comment_id":386660,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"wsl: Error: No such file or directory (os error 2) . 处理：simple.rs中 tikenizer和model的参数写全[&#47;mnt&#47;xx&#47;xx&#47;openchat_3.5_tokenizer.json]","like_count":1,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635965,"discussion_content":"👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705410899,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1624977,"avatar":"https://static001.geekbang.org/account/avatar/00/18/cb/91/14398631.jpg","nickname":"王超","note":"","ucode":"3CE4B6863C28A5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":637302,"discussion_content":"或者重新ln -sf一下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1708312353,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":389009,"user_name":"zhuxiufenghust","can_delete":false,"product_type":"c1","uid":1034291,"ip_address":"广东","ucode":"A8719F8407E74F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c8/33/2d4c464b.jpg","comment_is_top":false,"comment_ctime":1711369434,"is_pvip":false,"replies":[{"id":141744,"content":"我没遇到过这种错误，你是什么环境下运行的？","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1713028423,"ip_address":"加拿大","comment_id":389009,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"avx: false, neon: false, simd128: false, f16c: false\nError: unknown magic 0x6f64213c 这个错误要怎么解决","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641825,"discussion_content":"我没遇到过这种错误，你是什么环境下运行的？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713028423,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"加拿大","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1494311,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epdUKuOV21hnfTmXPibv5ReJCCIxiamtzXkibh9p41sSJeYQ87swreLWlTNEibh5ibefsoJfFppOvR088Q/132","nickname":"Geek_05de53","note":"","ucode":"20872B89D6A782","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":644670,"discussion_content":"该报错是没正确下载gguf文件，用wget下载的是一个&lt;!doctype html&gt;文件，需要正确下载gguf文件","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1715417436,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":388864,"user_name":"不忘初心","can_delete":false,"product_type":"c1","uid":3737491,"ip_address":"四川","ucode":"8262D42405F4E2","user_header":"https://static001.geekbang.org/account/avatar/00/39/07/93/710c7ee2.jpg","comment_is_top":false,"comment_ctime":1711021853,"is_pvip":false,"replies":[{"id":141746,"content":"Voice LLM + 客服系统 + 电销系统 ","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1713028477,"ip_address":"加拿大","comment_id":388864,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"老师, AI 电销, 需要什么样的技术栈?","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641827,"discussion_content":"Voice LLM + 客服系统 + 电销系统 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713028477,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"加拿大","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":388798,"user_name":"xl000","can_delete":false,"product_type":"c1","uid":1117935,"ip_address":"山东","ucode":"6FEABE7F7D0DC0","user_header":"https://static001.geekbang.org/account/avatar/00/11/0e/ef/030e6d27.jpg","comment_is_top":false,"comment_ctime":1710907806,"is_pvip":false,"replies":[{"id":141749,"content":"👍非常棒，遇到这种版本升级的事情。一般小问题，rust编译器会指导我们改。","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1713028679,"ip_address":"加拿大","comment_id":388798,"utype":1}],"discussion_count":2,"race_medal":2,"score":2,"product_id":100626901,"comment_content":"candle-core 0.4版本, 只需要改两处\ntensor.ggml_dtype.blck_size() 改为 tensor.ggml_dtype.block_size()\nfrom_gguf(model, &amp;mut file)那行改为 \nlet device = Device::cuda_if_available(0)?;\nlet mut model = quantized_model::ModelWeights::from_gguf(model, &amp;mut file, &amp;device)?;","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641831,"discussion_content":"👍非常棒，遇到这种版本升级的事情。一般小问题，rust编译器会指导我们改。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1713028679,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"加拿大","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1117935,"avatar":"https://static001.geekbang.org/account/avatar/00/11/0e/ef/030e6d27.jpg","nickname":"xl000","note":"","ucode":"6FEABE7F7D0DC0","race_medal":2,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":639926,"discussion_content":"还有两处直接传Device::Cpu的地方改为传device","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1710993194,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"山东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":387350,"user_name":"Geek_3b58b9","can_delete":false,"product_type":"c1","uid":3196265,"ip_address":"江苏","ucode":"E9DE52FD0B5E29","user_header":"","comment_is_top":false,"comment_ctime":1707028054,"is_pvip":false,"replies":[{"id":141161,"content":"对的，可以做这方面的工作","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1707198502,"ip_address":"四川","comment_id":387350,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"Candle 要是支持 ROCm 就更好了","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":636895,"discussion_content":"对的，可以做这方面的工作","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1707198502,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386649,"user_name":"tan","can_delete":false,"product_type":"c1","uid":1477463,"ip_address":"重庆","ucode":"20E176CB1EFD51","user_header":"https://static001.geekbang.org/account/avatar/00/16/8b/57/a3daeaae.jpg","comment_is_top":false,"comment_ctime":1705288311,"is_pvip":false,"replies":[{"id":140975,"content":"👍","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1705410889,"ip_address":"重庆","comment_id":386649,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"WSL: linker `cc` not found 处理方式： sudo apt update &amp;&amp; sudo apt install build-essential","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635964,"discussion_content":"👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705410889,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386590,"user_name":"eriklee","can_delete":false,"product_type":"c1","uid":2826132,"ip_address":"北京","ucode":"6F755DB7C29DD6","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIyhbzdkFM64Npva5ZKf4IPwhy6rDAX0L77QNESbalnXhnGKibcTbwtSaNC0hO6z0icO8DYI9Nf4xwg/132","comment_is_top":false,"comment_ctime":1705158190,"is_pvip":false,"replies":[{"id":140943,"content":"你的分析中短期内一定是对的。在学术界rust一定干不过python。 burn也很优秀，但是背后资源没有candle多。两个框架都希望能快点发展起来。","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1705326636,"ip_address":"重庆","comment_id":386590,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"老师能对比下candle和burn吗？\n另外，感觉rust优势是边缘端推理，毕竟边缘侧资源紧张. 服务器端推理，毕竟还是比不过python生态","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635884,"discussion_content":"你的分析中短期内一定是对的。在学术界rust一定干不过python。 burn也很优秀，但是背后资源没有candle多。两个框架都希望能快点发展起来。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1705326636,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386316,"user_name":"鸠摩智","can_delete":false,"product_type":"c1","uid":1106201,"ip_address":"江苏","ucode":"853E584FC4CD64","user_header":"https://static001.geekbang.org/account/avatar/00/10/e1/19/c756aaed.jpg","comment_is_top":false,"comment_ctime":1704542520,"is_pvip":true,"replies":[{"id":140812,"content":"这个方向可以尝试的，需要用到code 生成的模型。这是个非常有趣的方向。","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1704684795,"ip_address":"重庆","comment_id":386316,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100626901,"comment_content":"老师，请问一下，可以下载别人训练好的模型，通过candle 来实现根据需求描述自动生成测试用例的这种功能吗？","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635273,"discussion_content":"这个方向可以尝试的，需要用到code 生成的模型。这是个非常有趣的方向。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704684796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386147,"user_name":"superggn","can_delete":false,"product_type":"c1","uid":3623568,"ip_address":"北京","ucode":"831CCD98B393FE","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/7Q403U68Oy4lXG5sFBPVKLrfwaRzBqpBZibpEBXcPf9UOO3qrnh7RELoByTLzBZLkN9Nukfsj7DibynbZjKAKgag/132","comment_is_top":false,"comment_ctime":1704192068,"is_pvip":false,"replies":[{"id":140776,"content":"对电脑要求有点高","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1704259355,"ip_address":"重庆","comment_id":386147,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"mac 2021 款 intel 芯片， 系统版本 Montery 12.5.1 会卡住， model built 之后输入一个 hello 就不动了， 要过 5 分钟以上才会有回复","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":634979,"discussion_content":"对电脑要求有点高","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1704259355,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":385456,"user_name":"Geek_54dac1","can_delete":false,"product_type":"c1","uid":3793601,"ip_address":"四川","ucode":"649A0AE325056A","user_header":"","comment_is_top":false,"comment_ctime":1702636682,"is_pvip":false,"replies":[{"id":140475,"content":"很棒的探索和分享👍","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1702734984,"ip_address":"重庆","comment_id":385456,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"该模型在 Candle 中还暂时不支持在 GPU 上运行，因为 Quantized models on Cuda 还不支持，参考：https:&#47;&#47;github.com&#47;huggingface&#47;candle&#47;issues&#47;1250；避免大家挖坑","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633899,"discussion_content":"很棒的探索和分享👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702734984,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":385428,"user_name":"无限可能","can_delete":false,"product_type":"c1","uid":1007654,"ip_address":"北京","ucode":"D3CE995904BF58","user_header":"https://static001.geekbang.org/account/avatar/00/0f/60/26/35ef9bef.jpg","comment_is_top":false,"comment_ctime":1702618807,"is_pvip":false,"replies":[{"id":140469,"content":"1。主要是预训练神经网络模型权重数据\n2。要下载到本地，要加载到显存或者内存中运行\n3。是的","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1702697921,"ip_address":"重庆","comment_id":385428,"utype":1}],"discussion_count":3,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"老师好，大模型小白有几个问题。通过 huggingface-cli scan-cache，扫描了一下 huggingface 下载过的文件，都是 7G 或者更大：\n1. 这些模型文件里主要是什么内容，是包括了数据么？为啥会这么大？\n2. 类似这么大的文件，一定要下载到本地么，是否可以云部署之类，可以用完销毁。\n3. 我理解，我想要运行本节课的demo，这 7G 的空间就是要长期规划出去了吧","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633869,"discussion_content":"1。主要是预训练神经网络模型权重数据\n2。要下载到本地，要加载到显存或者内存中运行\n3。是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702697921,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1077733,"avatar":"https://static001.geekbang.org/account/avatar/00/10/71/e5/bcdc382a.jpg","nickname":"My dream","note":"","ucode":"2FEFB344230C17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633868,"discussion_content":"老师讲这节时，感觉没讲透，能讲一下模型是怎么训练的不？教教我们自己训练模型嘛！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702697793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"四川","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1052528,"avatar":"https://static001.geekbang.org/account/avatar/00/10/0f/70/cdef7a3d.jpg","nickname":"Joe Black","note":"","ucode":"21FE222A286445","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1077733,"avatar":"https://static001.geekbang.org/account/avatar/00/10/71/e5/bcdc382a.jpg","nickname":"My dream","note":"","ucode":"2FEFB344230C17","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635021,"discussion_content":"这可就不是rust课程该教的内容了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704320417,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":633868,"ip_address":"北京","group_id":0},"score":635021,"extra":""}]}]},{"had_liked":false,"id":385415,"user_name":"学水","can_delete":false,"product_type":"c1","uid":2557688,"ip_address":"加拿大","ucode":"F8B27FD11187EC","user_header":"https://static001.geekbang.org/account/avatar/00/27/06/f8/09ad484b.jpg","comment_is_top":false,"comment_ctime":1702608725,"is_pvip":false,"replies":[{"id":140460,"content":"是的，如果讲ml的话，又是专门一门课程了。可以在后续继续深入下去。你有pytorch基础就非常有利。👍","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1702637007,"ip_address":"重庆","comment_id":385415,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"这张感觉主要是给对rust有兴趣的mle，中间一些代码如果不是因为用过pytorch，完全不知道在干啥","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633839,"discussion_content":"是的，如果讲ml的话，又是专门一门课程了。可以在后续继续深入下去。你有pytorch基础就非常有利。👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702637007,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":385408,"user_name":"刘丹","can_delete":false,"product_type":"c1","uid":1081922,"ip_address":"广东","ucode":"66594D1C957E15","user_header":"https://static001.geekbang.org/account/avatar/00/10/82/42/8b04d489.jpg","comment_is_top":false,"comment_ctime":1702600157,"is_pvip":false,"replies":[{"id":140456,"content":"有几个还不错的。\nhttps:&#47;&#47;github.com&#47;sobelio&#47;llm-chain\nhttps:&#47;&#47;github.com&#47;rustformers&#47;llm\nhttps:&#47;&#47;github.com&#47;jondot&#47;awesome-rust-llm\n\n当然最强的，还是candle啦。\nhttps:&#47;&#47;github.com&#47;huggingface&#47;candle","user_name":"作者回复","user_name_real":"编辑","uid":2186062,"ctime":1702610466,"ip_address":"重庆","comment_id":385408,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"老师，你好，请问能否推荐几个和 ChatGPT 交互的 Rust SDK ? 最好也能支持其它 LLM 。","like_count":0,"discussions":[{"author":{"id":2186062,"avatar":"https://static001.geekbang.org/account/avatar/00/21/5b/4e/8e1f699e.jpg","nickname":"Mike Tang","note":"","ucode":"55775BCEDB5937","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633774,"discussion_content":"有几个还不错的。\nhttps://github.com/sobelio/llm-chain\nhttps://github.com/rustformers/llm\nhttps://github.com/jondot/awesome-rust-llm\n\n当然最强的，还是candle啦。\nhttps://github.com/huggingface/candle","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702610466,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392757,"user_name":"Yanxiao羅","can_delete":false,"product_type":"c1","uid":1423217,"ip_address":"福建","ucode":"D3A180FC11F42D","user_header":"https://static001.geekbang.org/account/avatar/00/15/b7/71/f0d3a1af.jpg","comment_is_top":false,"comment_ctime":1721705958,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"使用GPU加速遇到问题及处理记录\n\n1、candle版本升级到0.4.0，api变化需要调整两处。参考上面评论\n\n2、candle官网提示GPU开启需使用 `--features cuda` ，结合查看`Device::cuda_if_available(0)` 内部判断条件为读取配置 `config == cuda` 。首先修改代码中CPU的设置\n\n114行、132行\n\n```rust\n&#47;&#47;使用Device::new_cuda(0)替换Device::CPU\nlet input = Tensor::new(&amp;[next_token], &amp;Device::new_cuda(0)?)?.unsqueeze(0)?;\n```\n执行后报找不到对应的命令features\n\n3、`Cargo.toml` ，查看官方文档example配置，添加对应配置\n\n```toml\n[features]\ncuda = [&quot;candle&#47;cuda&quot;, &quot;candle-nn&#47;cuda&quot;, &quot;candle-transformers&#47;cuda&quot;]\n```\n\n4、执行后包candle报错，原因为官网使用workspace，包含了`candle-core`、`candle-transformes`、`candle-nn`。这里我们需要单独配置\n\n```toml\n[dependencies]\nanyhow = &quot;1.0.75&quot;\ncandle-core = { version = &quot;0.4.0&quot; }\ncandle-transformers = &quot;0.4.0&quot;\ncandle-nn = &quot;0.4.0&quot;\nclap = &quot;4.4.10&quot;\nhf-hub = &quot;0.3.2&quot;\ntokenizers = &quot;0.15.0&quot;\n\n[features]\ncuda = [&quot;candle-core&#47;cuda&quot;, &quot;candle-nn&#47;cuda&quot;, &quot;candle-transformers&#47;cuda&quot;]\n```\n\n4、执行后报找不到CUDA环境变量，参考以下文章，记得先查看当前机器CUDA版本，去NVIDA官网下载对应的cuda\n\n[windows下安装cuda和cudnn - 苍茫误此生博客 (cangmang.xyz)](https:&#47;&#47;cangmang.xyz&#47;articles&#47;1682852371010)\n\n5、执行后报找不到`Cannot find compiler &#39;cl.exe&#39; in PATH`，根据stackoverflow上排查，配置Visual Studio的bin目录到环境变量。注意新版本路径与stackoverflow上最佳回答不同，找到对应位置，大致如下：\n\n`C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\bin\\Hostx64\\x64`\n\n6、执行后报\n\n```rust\nDriverError(CUDA_ERROR_NOT_FOUND, &quot;named symbol not found&quot;) when loading dequantize_block_q8_0\n```\n\n极客时间后续评论为candle不支持，考虑到评论时间为去年。继续google，candle github官网issue中有相应的回答，已处理并合并到主干，时间为2024年3月份。跟踪3月份后，离3月份最近版本为`0.5.0` （避免API变化大，未使用最新版本）。更新`cargo.toml` \n\n```toml\n[dependencies]\ncandle-core = { version = &quot;0.5.0&quot; }\ncandle-transformers = &quot;0.5.0&quot;\ncandle-nn = &quot;0.5.0&quot;\n```\n\n7、执行成功。","like_count":1},{"had_liked":false,"id":391136,"user_name":"HanStrong","can_delete":false,"product_type":"c1","uid":1372962,"ip_address":"浙江","ucode":"1E0E26E1237180","user_header":"https://static001.geekbang.org/account/avatar/00/14/f3/22/9090dfc8.jpg","comment_is_top":false,"comment_ctime":1717405297,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100626901,"comment_content":"不使用gpu 3秒钟生成一个token 这是正常的速度么 还是跟电脑有关系","like_count":0}]}