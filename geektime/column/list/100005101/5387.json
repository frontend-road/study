{"id":5387,"title":"14 | 一网打尽协同过滤、矩阵分解和线性模型","content":"<p>在上一篇文章中，我讲到了使用逻辑回归和梯度提升决策树组合的模型融合办法，用于CTR预估，我还满怀爱意地给这对组合起了个名字，叫做辑度组合，因为这对组合的确可以在很多地方帮到我们。</p><p>这对组合中，梯度提升决策树，也就是人们常说的GBDT，所起的作用就是对原始的特征做各种有效的组合，一棵树一个叶子节点就是一种特征组合。</p><p>这大概就是逻辑回归的宿命吧，作为一个广义线性模型，在这个由非线性组成的世界里，唯有与各种特征组合办法精诚合作，才能活下去。</p><h2>从特征组合说起</h2><p>对逻辑回归最朴素的特征组合就是二阶笛卡尔乘积，但是你有没有想过这样暴力组合的问题所在。</p><ol>\n<li>两两组合导致特征维度灾难；</li>\n<li>组合后的特征不见得都有效，事实上大部分可能无效；</li>\n<li>组合后的特征样本非常稀疏，意思就是组合容易，但是并不能在样本中找到对应的组合出现，也就没办法在训练时更新参数。</li>\n</ol><p>如果把包含了特征两两组合的逻辑回归线性部分写出来，就是：</p><p>$$\\hat{y} = \\omega_{0} + \\sum_{i=1}^{n}{\\omega_{i}x_{i}} + \\sum_{i=1}^{n}{\\sum_{j=i+1}^{n}{\\omega_{ij}x_{i}x_{j}}}$$</p><!-- [[[read_end]]] --><p>这和原始的逻辑回归相比，就多出来了后面那一大坨，特征两两组合那部分，也需要去学习对应的参数权重。</p><p>问题就是两两组合后非常有可能没有样本能够学习到&nbsp;$w_{ij}，不但没有样本可以用来学习到参数，而且在应用时，如果遇到了这样的组合，也就只能放弃，因为没有学到权重。</p><p>针对这个问题，就有了一个新的算法模型：因子分解机模型，也叫做FM，即Factorization Machine。因子分解机也常常用来做模型融合，今天就和你聊聊因子分解机的来龙去脉。</p><h2>FM模型</h2><h3>1.原理</h3><p>因子分解机模型是在2010年被提出来的。因为逻辑回归在做特征组合时样本稀疏，从而无法学到很多特征组合的权重，所以因子分解机的提出者就想，能不能对上面那个公式中的$w _{ij}$做解耦，让每一个特征学习一个隐因子向量出来。</p><p>就好像前面讲矩阵分解时，为每一个用户和每一个物品各自都学习一个隐因子向量一样，这样一来，任何两个特征不小心在实际使用时相遇了，需要组合，那么各自掏出自己随身携带的隐因子变量做一个向量点积，就是两者组合特征的权重了。</p><p>还是针对逻辑回归的线性部分，用公式写一下更清楚：</p><p>$$\\hat{y} = \\omega_{0} + \\sum_{i=1}^{n}{\\omega_{i}x_{i}} + \\sum_{i=1}^{n}{\\sum_{j=i+1}^{n}{&lt;V_{i}, V_{j}&gt;x_{i}x_{j}}}$$</p><p>这个公式和前面特征组合的公式相比，不同之处就是原来有个 $w_{ij}$，变成了这里的两个隐因子向量的点积 $&lt;V_{i}, V_{j}&gt;$。</p><p>不要小看这个变化。它其实认为两个特征之间，即使没有共同出现在一条样本中，也是有间接联系的。比如说特征A和特征B曾在一些样本中一起出现过，特征B和特征C曾在一些样本中出现过，那么特征A和特征C无论是否在样本中一起出现过，仍然是有些联系的。</p><p>如果在实际预测CTR时，特征A和特征C真的在一起出现了，如果你用的是因子分解机模型，这时候你的预测程序就不慌不忙走向数据库，从中取出早已准备好的特征A和特征C的隐因子向量，拿出来做一个点积运算，就得到了两者组合的权重。</p><p>也许逻辑回归见到这一切不禁要问：居然还有这种操作？是的，因子分解机的先进之处就在于此。</p><p>现在聪明如你，一定也想到了，既然二阶特征组合可以学到隐因子向量，那么三阶特征组合也可以加进来，四阶，五阶……想一想是不是有点小激动？不要急，组合越多，计算复杂度就会陡增，所以一般在实际使用中，因子分解机就表演到二阶特征组合就OK了。</p><h2>2模型训练</h2><p>因子分解机的参数学习并无特别之处，看目标函数，我在这里是把它当作融合模型来看的，用来做CTR预估，因此预测目标是一个二分类，因子分解机的输出还需要经过sigmoid函数变换：</p><p>$$\\sigma(\\hat{y}) = \\frac{1}{1+e^{-\\hat{y}}}$$</p><p>因此，损失目标函数也就是常用的logistic loss：</p><p>$$ loss(\\theta)= -\\frac{1}{m}\\sum_{i=1}^{m}{[y^{(i)}log(\\sigma(\\hat{y}))+(1-y^{(i)})log(1-\\sigma(\\hat{y}))]} $$</p><p>公式中 $\\sigma(\\hat{y})$ 是因子分解机的预测输出后经过sigmoid函数变换得到的预估CTR， $y^{(i)}$ 是真实样本的类别标记，正样本是1，负样本是0，m是样本总数。</p><p>对这个损失目标函数使用梯度下降或者随机梯度下降就可以得到模型的参数，和前面文章里的没有区别，注意损失函数实际上还需要加上正则项，这在上一篇专栏中已经总结过机器学习损失函数的两板斧，就是偏差和方差。</p><h2>3预测阶段</h2><p>假如现在已经得到了因子分解机的模型参数，你忍不住跃跃欲试想端着它冲上战场，且慢，因子分解机中二阶特征组合那一坨，在实际计算时，复杂度有点高，如果隐因子向量的维度是k，特征维度是n，那这个复杂度就是$O(kn^2)$，其中n方是特征要两两组合，k是每次组合都要对k维向量计算点积。需要对此稍微做一下改造，改造过程如下：</p><p>$$<br>\n\\begin{aligned}<br>\n\\sum_{i=1}^{n}\\sum_{j=i+1}^{n}{&lt;V_{i}, V_{j}&gt;x_{i}x_{j}}={}&amp; \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}{&lt;V_{i}, V_{j}&gt;x_{i}x_{j}} - \\frac{1}{2}\\sum_{i=1}^{n}{&lt;V_{i}, V_{j}&gt;x_{i}x_{i}} \\<br>\n&amp;=\\frac{1}{2}(\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{f=1}^{k}{v_{i,f}v_{j,f}x_ix_j}-\\sum_{i=1}^{n}\\sum_{f=1}^{k}{v_{i,f}v_{i,f}x_ix_i}) \\<br>\n&amp;=\\frac{1}{2}\\sum_{f=1}^{k}((\\sum_{i=1}^{n}{v_{i,f}x_i})(\\sum_{j=1}^{n}v_{j,f}x_j)-\\sum_{i=1}^{n}{v_{i,f}^2x_i^2}) \\<br>\n&amp;=\\frac{1}{2}\\sum_{f=1}^{k}{((\\sum_{i=1}^{n}{v_{i,f}x_i})^2-\\sum_{i=1}^{n}{v_{i,f}^2x_i^2})}<br>\n\\end{aligned}<br>\n$$</p><p>看上去这个有点复杂，你如果不想理解也没关系，我直接告诉你最后该怎么算。</p><pre><code>loop1 begin: 循环k次，k就是隐因子向量的维度，其中，循环到第f次时做以下事情\n\n loop2 begin:循环n个特征，第i次循环时做这样的事情\n\n  1. 从第i个特征的隐因子向量中拿出第f维的值\n  2. 计算两个值：A是特征值和f维的值相乘，B是A的平方\n\n loop2 end\n\n 把n个A累加起来，并平方得到C，把n个B也累加起来，得到D\n\n 用C减D，得到E\n\nloop1 end\n\n把k次循环得到的k个E累加起来，除以2\n</code></pre><p>这就是因子分解机中，二阶组合部分的实际计算方法，现在这样做的复杂度只是O(kn)，原来的平方复杂度不见了。</p><h2>4.一网打尽其他模型</h2><p>下面继续带你见识一些因子分解机的神奇之处。看下面这张图，这里示意了一批样本。</p><p><img src=\"https://static001.geekbang.org/resource/image/07/e5/07dad32f420ad51602302e36d7c745e5.png?wh=1696*762\" alt=\"\"></p><p>这张图中每一条样本都记录了用户对电影的评分，最右边的y是评分，也就是预测目标；左边的特征有五种，用户ID、当前评分的电影ID、曾经评过的其他分、评分时间、上一次评分的电影。</p><p>好，现在我们来看因子分解机如何一网打尽其他模型的，注意，这里说的一网打尽并不是打败，而是说模型可以变形成其他模型。</p><p>前面已经说了因子分解机可以实现带有特征组合的逻辑回归。</p><p>现在假设图中的样本特征只留下用户ID和电影ID，因子分解机模型就变成：</p><p>$$\\hat{y} = \\omega_{0} + \\omega_{u} + \\omega_{i} + &lt;V_{u}, V_{i}&gt;$$</p><p>解释一下如何变成这样的。因为用户ID和电影ID，在一条样本中，各自都只有一个维度是1，其他都是0，所以在一阶部分就没有了求和符合，直接是wu和wi，二阶部分特征乘积也只剩下了一个1，其他都为0了。你瞧，这不就是带有偏置信息的SVD吗？</p><p>现在继续，在SVD基础上把样本中的特征加上用户历史评过分的电影ID，再求隐因子向量，这就是SVD++呀！</p><p>再加上时间信息，就变成了time-SVD。</p><p>所以因子分解机是把我之前讲过的矩阵分解一网打尽了，顺便还干起了逻辑回归的工作，也正因如此，因子分解机常常用来做模型融合，在推荐系统的排序阶段肩负起对召回结果做重排序的任务。</p><h2>5.FFM</h2><p>因子分解机的故事已经讲完，但我后来在因子分解机基础上改进了一下。改进的思路是：不但认为特征和特征之间潜藏着一些不可告人的关系，还认为特征和特征类型有着千丝万缕的关系。</p><p>这个特征类型，就是某些特征实际上是来自数据的同一个字段，比如用户ID，占据了很多维度，变成了很多特征，但他们都属于同一个类型，都叫“用户ID”。这个特征类型就是字段，即Field。这种改进叫做Field-aware Factorization Machines，简称FFM。</p><p>再回顾一下，因子分解机模型的样子是这样：</p><p>$$\\hat{y} = \\omega_{0} + \\sum_{i=1}^{n}{\\omega_{i}x_{i}} + \\sum_{i=1}^{n}{\\sum_{j=i+1}^{n}{&lt;V_{i}, V_{j}&gt;x_{i}x_{j}}}$$</p><p>之前因子分解机认为每个特征有一个隐因子向量，FFM改进的是二阶组合那部分，改进的模型认为每个特征有f个隐因子向量，这里的f就是特征一共来自多少个字段（Field），二阶组合部分改进后如下：</p><p>$$ \\sum_{i=1}^{n}{\\sum_{j=i+1}^{n}{&lt;V_{i,fj}, V_{j,fi}&gt;x_{i}x_{j}}} $$</p><p>FFM模型也常用来做CTR预估。在FM和FFM事件过程中，记得要对样本和特征都做归一化。</p><h2>总结</h2><p>今天，我给你介绍了另一种常用来做CTR预估的模型，因子分解机。因子分解机最早提出在2010年，在一些数据挖掘比赛中都取得了很好的乘积，后来被引入工业界做模型融合，也表现不俗。严格来说，因子分解机也算是矩阵分解算法的一种，因为它的学习结果也是隐因子向量，也是用过隐因子向量的点积代替原来的单个权重参数。</p><p>最后，由于不断提到特征组合的重要性，前有GBDT，现有FM，都是在特征组合上花功夫，你能不能在这里分享一下，你所用过的特征组合办法有哪些呢？欢迎留言一起讨论。<br>\n<img src=\"https://static001.geekbang.org/resource/image/87/b0/873b086966136189db14874181823fb0.jpg?wh=1110*549\" alt=\"\"></p>","comments":[{"had_liked":false,"id":5326,"user_name":"qi","can_delete":false,"product_type":"c1","uid":1049353,"ip_address":"","ucode":"01DECAB2E57819","user_header":"https://static001.geekbang.org/account/avatar/00/10/03/09/cf8bb0ea.jpg","comment_is_top":false,"comment_ctime":1523145556,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"53062753108","product_id":100005101,"comment_content":"感觉越来越不理解了，只怪自己太浅了，学识不够！","like_count":13},{"had_liked":false,"id":5247,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1522939762,"is_pvip":false,"replies":[{"id":"1417","content":"如果找不到实践机会，就去kaggle刷比赛吧。如果你想实习，也可以给我发简历：chenkaijiang001@lianjia.com","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1523069817,"ip_address":"","comment_id":5247,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14407841650","product_id":100005101,"comment_content":"感觉现在周围一般的机器学习实践GBDT用的更多一点。没和实践过推荐系统的人直接交流过，不知道因子分解机除了预测点击率外，对什么场景效果优于其他的特征组合方法。现在陈老师的理论讲得通俗易懂，不过自己编程和工程实践训练不够，实践还不知道如何入手。用哪套数据，哪套来源工具包，阅读哪套源码来学习实践还没有认知。","like_count":3,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416651,"discussion_content":"如果找不到实践机会，就去kaggle刷比赛吧。如果你想实习，也可以给我发简历：chenkaijiang001@lianjia.com","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1523069817,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":5154,"user_name":"mervynlh","can_delete":false,"product_type":"c1","uid":1080345,"ip_address":"","ucode":"351A39ED4FDFA6","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eras0f8w4Z7mWrqUia1icZWIpCRXEhSwOopsAGMibQSic6ias5lSngEZNibibmjjzWg34AGEboyGibCyVyGZw/132","comment_is_top":false,"comment_ctime":1522799792,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14407701680","product_id":100005101,"comment_content":"老师，现在项目中用的gbdt还是fm,两者比较呢","like_count":3},{"had_liked":false,"id":29451,"user_name":"🐱您的好友William🐱","can_delete":false,"product_type":"c1","uid":1215456,"ip_address":"","ucode":"427786DB178965","user_header":"https://static001.geekbang.org/account/avatar/00/12/8b/e0/9a79ddac.jpg","comment_is_top":false,"comment_ctime":1538378209,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10128312801","product_id":100005101,"comment_content":"DNN虽然可以自动做一些feature engineering的工作，但是对于大型系统来讲，还是规定一些feature，将这一部分单独拿出来做之后共享给其他组，之后各个组的工作才能对接，对接之后fine-tune的可解释性也强，如果大家都用DNN，那么就是一个黑盒子加一个黑盒子，有可能输入输出还不一样，到时候融合对接都成问题。所以DNN作为一个超级function approximator在工业界还是应该比较适用于小型独立的项目，项目组之前各个组之间feature的统一提取，或者是之后作为项目最后的决策层。","like_count":2},{"had_liked":false,"id":5201,"user_name":"上个纪元的赵天师","can_delete":false,"product_type":"c1","uid":1046954,"ip_address":"","ucode":"ACEFB2264D2D05","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f9/aa/65e78697.jpg","comment_is_top":false,"comment_ctime":1522835014,"is_pvip":false,"replies":[{"id":"1391","content":"会有的。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1522841311,"ip_address":"","comment_id":5201,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10112769606","product_id":100005101,"comment_content":"跪求老师出版实体书，感觉太有收获了","like_count":2,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416633,"discussion_content":"会有的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522841311,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2048092,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/40/5c/c992fbc0.jpg","nickname":"勇敢实践","note":"","ucode":"271CC178CD5B8D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":315046,"discussion_content":"可以关注一下liugongyou这个人，写得深度广度都比开江好，而且好很多。\n你只要看一下他写的fm，对比一下，有种大学生和小学生的感觉。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603239036,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":27378,"user_name":"帅帅","can_delete":false,"product_type":"c1","uid":1062367,"ip_address":"","ucode":"4F2C2A48A39125","user_header":"https://static001.geekbang.org/account/avatar/00/10/35/df/67f91c36.jpg","comment_is_top":false,"comment_ctime":1537880058,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"5832847354","product_id":100005101,"comment_content":"目前看起来，模型从简单到强大，一次是LR、GBDT+LR、GBDT+FM、DNN；<br>那是不是直接上DNN最好呢？<br>我的理解并不是，如果数据量很小使用DNN会容易过拟合；<br>因此，简单的就选GBDT+LR、复杂的就选DNN；","like_count":1,"discussions":[{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":246044,"discussion_content":"DNN最复杂？没有这个说法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587721730,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1343949,"avatar":"","nickname":"赖春苹","note":"","ucode":"1B637D46549A21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":35880,"discussion_content":"为啥要gbdt+fm呀，gbdt不是已经做了特征组合了嘛，fm相比lr最大的优势就是特征组合吧？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571312168,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":5240,"user_name":"愚公移山","can_delete":false,"product_type":"c1","uid":1059097,"ip_address":"","ucode":"9C6DAA10F89736","user_header":"","comment_is_top":false,"comment_ctime":1522922168,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5817889464","product_id":100005101,"comment_content":"老师，使用了两两特征组合后，逻辑回归从线性模型变成了非线性模型，因此模型表现的更好，可以这样理解吗？","like_count":1,"discussions":[{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":271427,"discussion_content":"逻辑回归还是线性模型。特征组合只是让特征更丰富。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590136682,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":224075,"user_name":"sheny","can_delete":false,"product_type":"c1","uid":1487513,"ip_address":"","ucode":"9CD10E7766046C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLRZW10jLxn9kLmYcMSWlWVu4BKiabNyQpg98G1xIsibxiapt3d4KqfLDkDk4yfmQP5FtsgmfBo75b4Q/132","comment_is_top":false,"comment_ctime":1591267182,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591267182","product_id":100005101,"comment_content":"3 预测阶段的 第一个公式推导 最后应该是&lt;Vi,Vi&gt;xi*xi 不是&lt;Vi,Vj&gt;xi*xi","like_count":0},{"had_liked":false,"id":144627,"user_name":"FF","can_delete":false,"product_type":"c1","uid":1133758,"ip_address":"","ucode":"89DB3329AAEAB2","user_header":"https://static001.geekbang.org/account/avatar/00/11/4c/be/25919d4b.jpg","comment_is_top":false,"comment_ctime":1571985165,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1571985165","product_id":100005101,"comment_content":"对于只留下用户 ID 和电影 ID的公式来说，那两个隐因子不是一般的向量？而是两个隐因子矩阵？","like_count":0,"discussions":[{"author":{"id":1133758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4c/be/25919d4b.jpg","nickname":"FF","note":"","ucode":"89DB3329AAEAB2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":49627,"discussion_content":"突然想到用户id如果变成one-hot编码的话，那么把这项隐因子向量串起来就是原来的矩阵了？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1573617100,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":90723,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1556616875,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556616875","product_id":100005101,"comment_content":"老师，没有扩展开来，现在比较流行的是deepFM和deepFFM，把每个特征做embedding，老师，想问下FM有什么开源的python包吗？","like_count":0},{"had_liked":false,"id":5215,"user_name":"Duo An","can_delete":false,"product_type":"c1","uid":1058085,"ip_address":"","ucode":"901C9802511B07","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/25/fb2ba003.jpg","comment_is_top":false,"comment_ctime":1522855607,"is_pvip":false,"replies":[{"id":"1486","content":"会说到相似的模型。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1523326049,"ip_address":"","comment_id":5215,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1522855607","product_id":100005101,"comment_content":"后边会说到deepfm fnn 这些模型吗？","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416638,"discussion_content":"会说到相似的模型。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1523326049,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}