{"id":233772,"title":"17 | 存储系统：从检索技术角度剖析LevelDB的架构设计思想","content":"<p>你好，我是陈东。</p><p>LevelDB是由Google开源的存储系统的代表，在工业界中被广泛地使用。它的性能非常突出，官方公布的LevelDB的随机读性能可以达到6万条记录/秒。那这是怎么做到的呢？这就和LevelDB的具体设计和实现有关了。</p><p>LevelDB是基于LSM树优化而来的存储系统。都做了哪些优化呢？我们知道，LSM树会将索引分为内存和磁盘两部分，并在内存达到阈值时启动树合并。但是，这里面存在着大量的细节问题。比如说，数据在内存中如何高效检索？数据是如何高效地从内存转移到磁盘的？以及我们如何在磁盘中对数据进行组织管理？还有数据是如何从磁盘中高效地检索出来的？</p><p>其实，这些问题也是很有代表性的工业级系统的实现问题。LevelDB针对这些问题，使用了大量的检索技术进行优化设计。今天，我们就一起来看看，LevelDB究竟是怎么优化检索系统，提高效率的。</p><h2>如何利用读写分离设计将内存数据高效存储到磁盘？</h2><p>首先，对内存中索引的高效检索，我们可以用很多检索技术，如红黑树、跳表等，这些数据结构会比B+树更高效。因此，LevelDB对于LSM树的第一个改进，就是使用跳表代替B+树来实现内存中的C0树。</p><p>好，解决了第一个问题。那接下来的问题就是，内存数据要如何高效存储到磁盘。在第7讲中我们说过，我们是将内存中的C0树和磁盘上的C1树归并来存储的。但如果内存中的数据一边被写入修改，一边被写入磁盘，我们在归并的时候就会遇到数据的一致性管理问题。一般来说，这种情况是需要进行“加锁”处理的，但“加锁”处理又会大幅度降低检索效率。</p><!-- [[[read_end]]] --><p>为此，LevelDB做了读写分离的设计。它将内存中的数据分为两块，一块叫作<strong>MemTable</strong>，它是可读可写的。另一块叫作<strong>Immutable MemTable</strong>，它是只读的。这两块数据的数据结构完全一样，都是跳表。那它们是怎么应用的呢？</p><p>具体来说就是，当MemTable的存储数据达到上限时，我们直接将它切换为只读的Immutable MemTable，然后重新生成一个新的MemTable，来支持新数据的写入和查询。这时，将内存索引存储到磁盘的问题，就变成了将Immutable MemTable写入磁盘的问题。而且，由于Immutable MemTable是只读的，因此，它不需要加锁就可以高效地写入磁盘中。</p><p>好了，数据的一致性管理问题解决了，我们接着看C0树和C1树的归并。在原始LSM树的设计中，内存索引写入磁盘时是直接和磁盘中的C1树进行归并的。但如果工程中也这么实现的话，会有两个很严重的问题：</p><ol>\n<li>合并代价很高，因为C1树很大，而C0树很小，这会导致它们在合并时产生大量的磁盘IO；</li>\n<li>合并频率会很频繁，由于C0树很小，很容易被写满，因此系统会频繁进行C0树和C1树的合并，这样频繁合并会带来的大量磁盘IO，这更是系统无法承受的。</li>\n</ol><p>那针对这两个问题，LevelDB采用了延迟合并的设计来优化。具体来说就是，先将Immutable MemTable顺序快速写入磁盘，直接变成一个个<strong>SSTable</strong>（Sorted String Table）文件，之后再对这些SSTable文件进行合并。这样就避免了C0树和C1树昂贵的合并代价。至于SSTable文件是什么，以及多个SSTable文件怎么合并，我们一会儿再详细分析。</p><p>好了，现在你已经知道了，内存数据高效存储到磁盘上的具体方案了。那在这种方案下，数据又是如何检索的呢？在检索一个数据的时候，我们会先在MemTable中查找，如果查找不到再去Immutable  MemTable中查找。如果Immutable MemTable也查询不到，我们才会到磁盘中去查找。</p><p><img src=\"https://static001.geekbang.org/resource/image/22/1a/22cbb79dd84126a66b12e1b50c58991a.jpeg?wh=1920*1080\" alt=\"\" title=\"增加Immutable MemTable设计的示意图\"></p><p>因为磁盘中原有的C1树被多个较小的SSTable文件代替了。那现在我们要解决的问题就变成了，如何快速提高磁盘中多个SSTable文件的检索效率。</p><h2>SSTable的分层管理设计</h2><p>我们知道，SSTable文件是由Immutable MemTable将数据顺序导入生成的。尽管SSTable中的数据是有序的，但是每个SSTable覆盖的数据范围都是没有规律的，所以SSTable之间的数据很可能有重叠。</p><p>比如说，第一个SSTable中的数据从1到1000，第二个SSTable中的数据从500到1500。那么当我们要查询600这个数据时，我们并不清楚应该在第一个SSTable中查找，还是在第二个SSTable中查找。最差的情况是，我们需要查询每一个SSTable，这会带来非常巨大的磁盘访问开销。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/02/5f197f2664d0358e03989ef7ae2e7e02.jpeg?wh=1920*1080\" alt=\"\" title=\"范围重叠时，查询多个SSTable的示意图\"></p><p>因此，对于SSTable文件，我们需要将它整理一下，将SSTable文件中存的数据进行重新划分，让每个SSTable的覆盖范围不重叠。这样我们就能将SSTable按照覆盖范围来排序了。并且，由于每个SSTable覆盖范围不重叠，当我们需要查找数据的时候，我们只需要通过二分查找的方式，找到对应的一个SSTable文件，就可以在这个SSTable中完成查询了。<br>\n<img src=\"https://static001.geekbang.org/resource/image/4d/a7/4de515f8b4f7f90cc99112fe5b2b2da7.jpeg?wh=1920*1080\" alt=\"\" title=\"范围不重叠时，只需查询一个SSTable的示意图\"></p><p>但是要让所有SSTable文件的覆盖范围不重叠，不是一个很简单的事情。为什么这么说呢？我们看一下这个处理过程。系统在最开始时，只会生成一个SSTable文件，这时候我们不需要进行任何处理，当系统生成第二个SSTable的时候，为了保证覆盖范围不重合，我们需要将这两个SSTable用多路归并的方式处理，生成新的SSTable文件。</p><p>那为了方便查询，我们要保证每个SSTable文件不要太大。因此，LevelDB还控制了每个SSTable文件的容量上限（不超过2M）。这样一来，两个SSTable合并就会生成1个到2个新的SSTable。</p><p>这时，新的SSTable文件之间的覆盖范围就不重合了。当系统再新增一个SSTable时，我们还用之前的处理方式，来计算这个新的SSTable的覆盖范围，然后和已经排好序的SSTable比较，找出覆盖范围有重合的所有SSTable进行多路归并。这种多个SSTable进行多路归并，生成新的多个SSTable的过程，也叫作Compaction。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/0a/32e551a4f13d5630b7a0e43bef556b0a.jpeg?wh=1920*1080\" alt=\"\" title=\"SSTable保持有序的多路归并过程\"></p><p>随着SSTable文件的增多，多路归并的对象也会增多。那么，最差的情况会是什么呢？最差的情况是所有的SSTable都要进行多路归并。这几乎是一个不可能被接受的时间消耗，系统的读写性能都会受到很严重的影响。</p><p>那我们该怎么降低多路归并涉及的SSTable个数呢？在<a href=\"https://time.geekbang.org/column/article/222807\">第9讲</a>中，我们提到过，对于少量索引数据和大规模索引数据的合并，我们可以采用滚动合并法来避免大量数据的无效复制。因此，LevelDB也采用了这个方法，将SSTable进行分层管理，然后逐层滚动合并。这就是LevelDB的分层思想，也是LevelDB的命名原因。接下来，我们就一起来看看LevelDB具体是怎么设计的。</p><p>首先，<strong>从Immutable MemTable转成的SSTable会被放在Level 0 层。</strong>Level 0 层最多可以放4个SSTable文件。当Level 0层满了以后，我们就要将它们进行多路归并，生成新的有序的多个SSTable文件，这一层有序的SSTable文件就是Level 1 层。</p><p>接下来，如果Level 0 层又存入了新的4个SSTable文件，那么就需要和Level 1层中相关的SSTable进行多路归并了。但前面我们也分析过，如果Level 1中的SSTable数量很多，那么在大规模的文件合并时，磁盘IO代价会非常大。因此，LevelDB的解决方案就是，<strong>给Level 1中的SSTable文件的总容量设定一个上限</strong>（默认设置为10M），这样多路归并时就有了一个代价上限。</p><p>当Level 1层的SSTable文件总容量达到了上限之后，我们就需要选择一个SSTable的文件，将它并入下一层（为保证一层中每个SSTable文件都有机会并入下一层，我们选择SSTable文件的逻辑是轮流选择。也就是说第一次我们选择了文件A，下一次就选择文件A后的一个文件）。<strong>下一层会将容量上限翻10倍</strong>，这样就能容纳更多的SSTable了。依此类推，如果下一层也存满了，我们就在该层中选择一个SSTable，继续并入下一层。这就是LevelDB的分层设计了。</p><p><img src=\"https://static001.geekbang.org/resource/image/ca/5a/ca6dad0aaa0eb1303b5c1bb17241915a.jpeg?wh=1920*1080\" alt=\"\" title=\"LevelDB的层次结构示意图\"></p><p>尽管LevelDB通过限制每层的文件总容量大小，能保证做多路归并时，会有一个开销上限。但是层数越大，容量上限就越大，那发生在下层的多路归并依然会造成大量的磁盘IO开销。这该怎么办呢？</p><p>对于这个问题，LevelDB是通过加入一个限制条件解决的。在多路归并生成第n层的SSTable文件时，LevelDB会判断生成的SSTable和第n+1层的重合覆盖度，如果重合覆盖度超过了10个文件，就结束这个SSTable的生成，继续生成下一个SSTable文件。</p><p>通过这个限制，<strong>LevelDB就保证了第n层的任何一个SSTable要和第n+1层做多路归并时，最多不会有超过10个SSTable参与</strong>，从而保证了归并性能。</p><h2>如何查找对应的SSTable文件</h2><p>在理解了这样的架构之后，我们再来看看当我们想在磁盘中查找一个元素时，具体是怎么操作的。</p><p>首先，我们会在Level 0 层中进行查找。由于Level 0层的SSTable没有做过多路归并处理，它们的覆盖范围是有重合的。因此，我们需要检查Level 0层中所有符合条件的SSTable，在其中查找对应的元素。如果Level 0没有查到，那么就下沉一层继续查找。</p><p>而从Level 1开始，每一层的SSTable都做过了处理，这能保证覆盖范围不重合的。因此，对于同一层中的SSTable，我们可以使用二分查找算法快速定位唯一的一个SSTable文件。如果查到了，就返回对应的SSTable文件；如果没有查到，就继续沉入下一层，直到查到了或查询结束。</p><p><img src=\"https://static001.geekbang.org/resource/image/57/8b/57cd22fa67cba386d83686a31434e08b.jpeg?wh=1920*1080\" alt=\"\" title=\"LevelDB分层检索过程示意图\"></p><p>可以看到，通过这样的一种架构设计，我们就将SSTable进行了有序的管理，使得查询操作可以快速被限定在有限的SSTable中，从而达到了加速检索的目的。</p><h2>SSTable文件中的检索加速</h2><p>那在定位到了对应的SSTable文件后，接下来我们该怎么查询指定的元素呢？这个时候，前面我们学过的一些检索技术，现在就可以派上用场了。</p><p>首先，LevelDB使用索引与数据分离的设计思想，将SSTable分为数据存储区和数据索引区两大部分。</p><p><img src=\"https://static001.geekbang.org/resource/image/53/40/53d347e57ffee9a7ea14dde2b5f4a340.jpeg?wh=1920*1080\" alt=\"\" title=\"SSTable文件格式\"></p><p>我们在读取SSTable文件时，不需要将整个SSTable文件全部读入内存，只需要先将数据索引区中的相关数据读入内存就可以了。这样就能大幅减少磁盘IO次数。</p><p>然后，我们需要快速确定这个SSTable是否包含查询的元素。对于这种是否存在的状态查询，我们可以使用前面讲过的BloomFilter技术进行高效检索。也就是说，我们可以从数据索引区中读出BloomFilter的数据。这样，我们就可以使用O(1)的时间代价在BloomFilter中查询。如果查询结果是不存在，我们就跳过这个SSTable文件。而如果BloomFilter中查询的结果是存在，我们就继续进行精确查找。</p><p>在进行精确查找时，我们将数据索引区中的Index Block读出，Index Block中的每条记录都记录了每个Data Block的最小分隔key、起始位置，还有block的大小。由于所有的记录都是根据Key排好序的，因此，我们可以使用二分查找算法，在Index Block中找到我们想查询的Key。</p><p>那最后一步，就是将这个Key对应的Data block从SSTable文件中读出来，这样我们就完成了数据的查找和读取。</p><h2>利用缓存加速检索SSTable文件的过程</h2><p>在加速检索SSTable文件的过程中，你会发现，每次对SSTable进行二分查找时，我们都需要将Index Block和相应的Data Block分别从磁盘读入内存，这样就会造成两次磁盘I/O操作。我们知道磁盘I/O操作在性能上，和内存相比是非常慢的，这也会影响数据的检索速度。那这个环节我们该如何优化呢？常见的一种解决方案就是使用缓存。LevelDB具体是怎么做的呢？</p><p>针对这两次读磁盘操作，LevelDB分别设计了table cache和block cache两个缓存。其中，block cache是配置可选的，它是将最近使用的Data Block加载在内存中。而table cache则是将最近使用的SSTable的Index Block加载在内存中。这两个缓存都使用LRU机制进行替换管理。</p><p>那么，当我们想读取一个SSTable的Index Block时，首先要去table cache中查找。如果查到了，就可以避免一次磁盘操作，从而提高检索效率。同理，如果接下来要读取对应的Data Block数据，那么我们也先去block cache中查找。如果未命中，我们才会去真正读磁盘。</p><p>这样一来，我们就可以省去非常耗时的I/O操作，从而加速相关的检索操作了。</p><h2>重点回顾</h2><p>好了，今天我们学习了LevelDB提升检索效率的优化方案。下面，我带你总结回顾一下今天的重点内容。</p><p>首先，在内存中检索数据的环节，LevelDB使用跳表代替B+树，提高了内存检索效率。</p><p>其次，在将数据从内存写入磁盘的环节，LevelDB先是使用了<strong>读写分离</strong>的设计，增加了一个只读的Immutable MemTable结构，避免了给内存索引加锁。然后，LevelDB又采用了<strong>延迟合并</strong>设计来优化归并。具体来说就是，它先快速将C0树落盘生成SSTable文件，再使用其他异步进程对这些SSTable文件合并处理。</p><p>而在管理多个SSTable文件的环节，LevelDB使用<strong>分层和滚动合并</strong>的设计来组织多个SSTable文件，避免了C0树和C1树的合并带来的大量数据被复制的问题。</p><p>最后，在磁盘中检索数据的环节，因为SSTable文件是有序的，所以我们通过<strong>多层二分查找</strong>的方式，就能快速定位到需要查询的SSTable文件。接着，在SSTable文件内查找元素时，LevelDB先是使用<strong>索引与数据分离</strong>的设计，减少磁盘IO，又使用<strong>BloomFilter和二分查找</strong>来完成检索加速。加速检索的过程中，LevelDB又使用<strong>缓存技术</strong>，将会被反复读取的数据缓存在内存中，从而避免了磁盘开销。</p><p>总的来说，一个高性能的系统会综合使用多种检索技术。而LevelDB的实现，就可以看作是我们之前学过的各种检索技术的落地实践。因此，这一节的内容，我建议你多看几遍，这对我们之后的学习也会有非常大的帮助。</p><h2>课堂讨论</h2><ol>\n<li>\n<p>当我们查询一个key时，为什么在某一层的SSTable中查到了以后，就可以直接返回，不用再去下一层查找了呢？如果下一层也有SSTable存储了这个key呢？</p>\n</li>\n<li>\n<p>为什么从Level 1层开始，我们是限制SSTable的总容量大小，而不是像在Level 0层一样限制SSTable的数量？ （提示：SSTable的生成过程会受到约束，无法保证每一个SSTable文件的大小）</p>\n</li>\n</ol><p>欢迎在留言区畅所欲言，说出你的思考过程和最终答案。如果有收获，也欢迎把这一讲分享给你的朋友。</p>","neighbors":{"left":{"article_title":"测一测 | 高性能检索系统的实战知识，你掌握了多少？","id":232713},"right":{"article_title":"18 | 搜索引擎：输入搜索词以后，搜索引擎是怎么工作的？","id":234839}},"comments":[{"had_liked":false,"id":215212,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1588930169,"is_pvip":false,"replies":[{"id":79672,"content":"这是一个好问题！实际上，这也是levelDB的一个瓶颈。当immutable memtable还没有完全写入磁盘时，memtable如果写满了，就会被阻塞住。\n因此，Facebook基于Google的levelDB，开源了一个rocksDB，rocksDB允许创建多个memtable，这样就解决了由于写入磁盘速度太慢导致memtable阻塞的问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588949086,"ip_address":"","comment_id":215212,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"当 MemTable 的存储数据达到上限时，我们直接将它切换为只读的 Immutable MemTable，然后重新生成一个新的 MemTable\n------------------\n这样的一个机制，内存中会出现多个Immutable MemTable 吗？ 上一个Immutable MemTable 没有及时写入到磁盘","like_count":27},{"had_liked":false,"id":215469,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1588996616,"is_pvip":false,"replies":[{"id":79789,"content":"你的问题我重新整理一下，尤其是level 0层怎么处理，这其实是一个很好的问题:\n问题1:level 0层到level 1层合并的时候，level 0层是有多少个sstable参与合并？\n回答:按道理来说，我们应该是根据轮流选择的策略，选择一个level 0层的sstable进行和下层的合并，但是由于level 0层中的sstable可能范围是重叠的，因此我们需要检查每一个sstable，将有重叠部分的都加入到合并列表中。\n问题2:level n层中的一个sstable要和level n+1层中的所有sstable进行合并么？\n回答:不需要。如果level n层的sstable的最大最小值是begin和end，我们只需要在level n+1层中，找到可能包含begin到end之间的sstable即可。这个数量不会超过10个。因此不会带来太大的io。\n问题3:为什么level n层的sstable和level n+1层的合并，个数不会超过10个？\n回答:在level n层的sstable生成的时候，我们会开始判断这个sstable和level n+1层的哪些sstable有重叠。如果发现重叠个数达到十个，就要结束这个sstable文件的生成。\n举个例子，如果level n+1层的11个sstable的第一个元素分别是[100，200，300，400，……，1000，1100]，即开头都是100的整数倍。那么，如果level n层的sstable文件生成时，准备写入的数据就是[100，200，300，400，……，1000，1100]，那么在要写入1100的时候，系统会发现，如果写入1100，那么这个sstable文件就会和下一层的11个sstable文件有重叠了，会违反规则，因此，这时候会结束这个sstable，也就是说，这个sstable文件中只有100到1000十个数。然后1100会被写入到一个新的sstable中。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589015698,"ip_address":"","comment_id":215469,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"LevelDB 分层的逻辑没有理解\n当 Level0 层 有四个 SSTable 的时候，这时候把这个四个进行归并，然后放到 Level1 层，这时候 Level0 层清空，这个有个问题是 当进行归并后 后生成几个 SSTable ,这里是有什么规则吗？\n\n接下来，然后 Level0 层在满了之后，是Level0 层的每个 SSTable 分别与 Level1 所有的 SSTable 进行多路归并吗？\n\n再然后 Level1 层满了之后，是按照顺序取 Level1 层的一个 SSTable 与 Level2所有的 SSTable 进行多路归并吗？\n\n这样会有大量的 磁盘 IO,老师说利用判断重合度进行解决的？ 这个重合度是怎么计算计算判断的呢？\n\n==============================\n老师的文中的这句话没有看明白：\n在多路归并生成第 n 层的 SSTable 文件时，LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度，如果重合覆盖度超过了 10 个文件，就结束这个 SSTable 的生成，继续生成下一个 SSTable 文件\n","like_count":21,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494357,"discussion_content":"这是一个好问题！实际上，这也是levelDB的一个瓶颈。当immutable memtable还没有完全写入磁盘时，memtable如果写满了，就会被阻塞住。\n因此，Facebook基于Google的levelDB，开源了一个rocksDB，rocksDB允许创建多个memtable，这样就解决了由于写入磁盘速度太慢导致memtable阻塞的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588949086,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3077132,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/OFh3NluBCSoxHrS7FVqia5letsNII0B5Rv7uZMs33hQXbbQ7CEqia57cUjAoDmZpqO7WXWl3LZXbt9A1rx4557icw/132","nickname":"Geek_d869e2","note":"","ucode":"9259146136FF06","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":584393,"discussion_content":"好的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660803276,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"河北","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215014,"user_name":"吴小智","can_delete":false,"product_type":"c1","uid":1310798,"ip_address":"","ucode":"C7C9F58B5C9F7B","user_header":"https://static001.geekbang.org/account/avatar/00/14/00/4e/be2b206b.jpg","comment_is_top":false,"comment_ctime":1588869253,"is_pvip":false,"replies":[{"id":79606,"content":"lsm树和b+树会有许多不同的特点。但是如果从使用场景来看，最大的区别就是看读和写的需求。\n在随机读很多，但是写入很少的场合，适合使用b+树。因为b+树能快速二分找到任何数据，并且磁盘io很少；但如果是使用lsm树，对于大量的随机读，它无法在内存中命中，因此会去读磁盘，并且是一层一层地多次读磁盘，会带来很严重的读放大效应。\n但如果是大量的写操作的场景的话，lsm树进行了大量的批量写操作优化，因此效率会比b+树高许多。b+树每次写入都要去修改叶子节点，这会带来大量的磁盘io，使得效率急剧下降。这也是为什么日志系统，监控系统这类大量生成写入数据的应用会采用lsm树的原因。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588913718,"ip_address":"","comment_id":215014,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"之前看过基于 lsm 的存储系统的代码，能很好理解这篇文章。不过，还是不太理解基于 B+ 树与基于 lsm 的存储系统，两者的优缺点和使用场景有何不同，老师有时间可以解答一下。","like_count":19,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494447,"discussion_content":"你的问题我重新整理一下，尤其是level 0层怎么处理，这其实是一个很好的问题:\n问题1:level 0层到level 1层合并的时候，level 0层是有多少个sstable参与合并？\n回答:按道理来说，我们应该是根据轮流选择的策略，选择一个level 0层的sstable进行和下层的合并，但是由于level 0层中的sstable可能范围是重叠的，因此我们需要检查每一个sstable，将有重叠部分的都加入到合并列表中。\n问题2:level n层中的一个sstable要和level n+1层中的所有sstable进行合并么？\n回答:不需要。如果level n层的sstable的最大最小值是begin和end，我们只需要在level n+1层中，找到可能包含begin到end之间的sstable即可。这个数量不会超过10个。因此不会带来太大的io。\n问题3:为什么level n层的sstable和level n+1层的合并，个数不会超过10个？\n回答:在level n层的sstable生成的时候，我们会开始判断这个sstable和level n+1层的哪些sstable有重叠。如果发现重叠个数达到十个，就要结束这个sstable文件的生成。\n举个例子，如果level n+1层的11个sstable的第一个元素分别是[100，200，300，400，……，1000，1100]，即开头都是100的整数倍。那么，如果level n层的sstable文件生成时，准备写入的数据就是[100，200，300，400，……，1000，1100]，那么在要写入1100的时候，系统会发现，如果写入1100，那么这个sstable文件就会和下一层的11个sstable文件有重叠了，会违反规则，因此，这时候会结束这个sstable，也就是说，这个sstable文件中只有100到1000十个数。然后1100会被写入到一个新的sstable中。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1589015698,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1005391,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","nickname":"奕","note":"","ucode":"73CEA468CE70C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261879,"discussion_content":"第一问还有点疑问:level0层的每个sstable可能会有范围重叠，需要把重叠的部分提取到合并列表，这个这个合并列表是什么？还有就是提取之后呢，还是要遍及level0层的每个sstable与level1层的sstable进行归并吗？\n\n还有个问题就是:当某层的sstable向下层转移的时候，碰巧下层的空间也满了，这时候的处理方案是向下层递归吗？一直往下找，然后在向上处理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589018068,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":218323,"user_name":"王坤祥","can_delete":false,"product_type":"c1","uid":1003327,"ip_address":"","ucode":"FB988B9F381A33","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4f/3f/6f62f982.jpg","comment_is_top":false,"comment_ctime":1589779881,"is_pvip":false,"replies":[{"id":80755,"content":"1.结论是对的，不过有一个小细节要注意一下，下一层的数据，是更老的一个数据，不一定是“一样的”。由于它不够上一层的新，因此可以放弃。\n2.是的。你把这一个约束规则找到了！因为有着这一条约束规则，可能某一层的sstable在某个时刻数量很多，但是每个文件都很小，如果通过文件数量限制，就使得这一层可能存不了什么数据。因此用总容量进行限制更合理。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589801855,"ip_address":"","comment_id":218323,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"1. 既然要查找的数据在某一层查到了，按照LevelDB的分层管理的设计，即使下一层数据也存在，数据也是一样的，没有必要再去查找下一层的数据了。\n\n2. “在多路归并生成第 n 层的 SSTable 文件时，LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度，如果重合覆盖度超过了 10 个文件，就结束这个 SSTable 的生成，继续生成下一个 SSTable 文件。”———如果通过控制sstable文件数量来限制每层容量的话，有可能每个sstable会比较小，很快就达到数量限制，可能分层作用就不明显了。","like_count":9,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494275,"discussion_content":"lsm树和b+树会有许多不同的特点。但是如果从使用场景来看，最大的区别就是看读和写的需求。\n在随机读很多，但是写入很少的场合，适合使用b+树。因为b+树能快速二分找到任何数据，并且磁盘io很少；但如果是使用lsm树，对于大量的随机读，它无法在内存中命中，因此会去读磁盘，并且是一层一层地多次读磁盘，会带来很严重的读放大效应。\n但如果是大量的写操作的场景的话，lsm树进行了大量的批量写操作优化，因此效率会比b+树高许多。b+树每次写入都要去修改叶子节点，这会带来大量的磁盘io，使得效率急剧下降。这也是为什么日志系统，监控系统这类大量生成写入数据的应用会采用lsm树的原因。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588913718,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1905163,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/12/0b/d2335912.jpg","nickname":"kissingurami","note":"","ucode":"5399F825A4E083","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":260745,"discussion_content":"楼主有点儿本末倒置了吧。先理解lsm和B+的设计初衷再去看代码效率更高。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1588899614,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1150927,"avatar":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","nickname":"那时刻","note":"","ucode":"B0D150856C3A4A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":260743,"discussion_content":"mongodb里有B+树和LSM树两种方式来存储索引，一般来说，对于写多读少情况，适用LSM树。对于读多写少情况，适合B+树。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1588899281,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":216179,"user_name":"时隐时现","can_delete":false,"product_type":"c1","uid":1111899,"ip_address":"","ucode":"DA4D622FF84920","user_header":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","comment_is_top":false,"comment_ctime":1589201077,"is_pvip":false,"replies":[{"id":79985,"content":"1.虽然跳表和b+树在时间代价上都是一个量级的，但是跳表的插入删除都很简单，而b+树的插入删除会有节点分裂，节点合并，节点调整等问题，因此从工程效率来看，在纯内存的环境下，b+树并不比跳表和红黑树更合适。\n2.所有的sstable都是只读的，不可更改。新的sstable生成了以后，老的sstable才会被删除，读操作才会转移到新的sstable上。因此，sstable不会被同时读写，没有读写阻塞的问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589208073,"ip_address":"","comment_id":216179,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师好，有2个问题：\n1、内存中的C0树，采用跳表替换掉B+树，检索效率会有提升吗？我一直觉得两者是差不多的吧，什么场景下跳表会比B+树性能高很多？\n2、滚动合并应该是后台操作，在合并的过程中，相应的sstable应该是被写锁锁定的吧？此时如果有应用执行读，会不会被阻塞？如果不阻塞，如何保证读写一致性？","like_count":7,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495429,"discussion_content":"1.结论是对的，不过有一个小细节要注意一下，下一层的数据，是更老的一个数据，不一定是“一样的”。由于它不够上一层的新，因此可以放弃。\n2.是的。你把这一个约束规则找到了！因为有着这一条约束规则，可能某一层的sstable在某个时刻数量很多，但是每个文件都很小，如果通过文件数量限制，就使得这一层可能存不了什么数据。因此用总容量进行限制更合理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589801855,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215298,"user_name":"xaviers","can_delete":false,"product_type":"c1","uid":1879918,"ip_address":"","ucode":"58D51C4DDC5BA8","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","comment_is_top":false,"comment_ctime":1588949971,"is_pvip":false,"replies":[{"id":79702,"content":"因为对于b+树，当内存中的change buffer写满的时候，会去更新多个叶子节点，这会带来多次磁盘IO;但lsm当内存中的memtable写满时，只会去写一次sstable文件。因此它们的主要差异，还是在怎么将数据写入磁盘上。\n当然所有的系统设计都是有利有弊，要做权衡。b+树写入磁盘后，随机读性能比较好;而lsm树写磁盘一时爽，但要随机读的时候就不爽了，它可能得在多层去寻找sstable文件，因此随机读性能比b+树差。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588985758,"ip_address":"","comment_id":215298,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师，不好意思哈，再追问一下😬那为啥用change buffer + WAL优化后的MySQL的写性能还是不如LSM类的存储系统啊？原因是啥啊","like_count":7,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494384,"discussion_content":"因为对于b+树，当内存中的change buffer写满的时候，会去更新多个叶子节点，这会带来多次磁盘IO;但lsm当内存中的memtable写满时，只会去写一次sstable文件。因此它们的主要差异，还是在怎么将数据写入磁盘上。\n当然所有的系统设计都是有利有弊，要做权衡。b+树写入磁盘后，随机读性能比较好;而lsm树写磁盘一时爽，但要随机读的时候就不爽了，它可能得在多层去寻找sstable文件，因此随机读性能比b+树差。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588985758,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215599,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1589028704,"is_pvip":false,"replies":[{"id":79811,"content":"1.合并列表其实就是记录需要合并的sstable的列表。实际上，每次合并时，系统都会生成两个合并列表。\n以你提问的level 0层的情况为例，先选定一个要合并的sstable，然后将level 0层中和它范围重叠的sstable都加入到这个列表中;这就是合并列表1。\n然后，对于合并列表1中所有的sstable，我们能找到整体的范围begin和end。那么我们在下一层中，将和begin和end范围重叠的所有sstable文件加入合并列表2。\n那么，对于合并列表1和合并列表2中的所有的sstable，我们将它们一起做一次多路归并就可以了。\n2.如果下层空间满了，没关系，先合并完，这时候，下层空间就超容量了。那么，我们再针对这一层，按之前介绍的规则，选择一个sstable再和下层合并即可。\nPS:再补充一下知识点:合并的触发条件。\n系统会统计每个level的文件容量是否超过限制。超过上限比例最大的，将会被触发合并操作。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589042904,"ip_address":"","comment_id":215599,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"在评论下回复老师看不到啊，那就在评论问一下\n\n第一问还有点疑问:level0层的每个sstable可能会有范围重叠，需要把重叠的部分提取到合并列表，这个这个合并列表是什么？还有就是提取之后呢，还是要遍及level0层的每个sstable与level1层的sstable进行归并吗？\n\n还有个问题就是:当某层的sstable向下层转移的时候，碰巧下层的空间也满了，这时候的处理方案是向下层递归吗？一直往下找，然后在向上处理","like_count":6,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494384,"discussion_content":"因为对于b+树，当内存中的change buffer写满的时候，会去更新多个叶子节点，这会带来多次磁盘IO;但lsm当内存中的memtable写满时，只会去写一次sstable文件。因此它们的主要差异，还是在怎么将数据写入磁盘上。\n当然所有的系统设计都是有利有弊，要做权衡。b+树写入磁盘后，随机读性能比较好;而lsm树写磁盘一时爽，但要随机读的时候就不爽了，它可能得在多层去寻找sstable文件，因此随机读性能比b+树差。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588985758,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215096,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1588903972,"is_pvip":false,"replies":[{"id":79668,"content":"1.不用停止计算，而是算完后，判断容量是否达到上限，如果超过，就根据文中介绍的选择文件的方式，将多余的文件和下一层进行合并。\n2.如果存在多个filter block，而且每个filter都很大的话(比如说bloomfilter就有许多数据)，将所有的filter都读入内存会造成多次磁盘IO,因此需要有metaphor index block，帮助我们只读取我们需要的filter即可。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588947207,"ip_address":"","comment_id":215096,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"有两个问题，请教下老师。\n1。在多路归并生成第 n 层的 SSTable 文件时，如何控制当前层最大容量呢？如果超过当前层的容量是停止计算还是把多余的量挪到下一层？\n2。数据索引区里meta index block，当存在多个过滤器时，对过滤器进行索引。这是涉及到filter block过滤么？","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494314,"discussion_content":"1.不用停止计算，而是算完后，判断容量是否达到上限，如果超过，就根据文中介绍的选择文件的方式，将多余的文件和下一层进行合并。\n2.如果存在多个filter block，而且每个filter都很大的话(比如说bloomfilter就有许多数据)，将所有的filter都读入内存会造成多次磁盘IO,因此需要有metaphor index block，帮助我们只读取我们需要的filter即可。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588947207,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":271966,"user_name":"Bachue Zhou","can_delete":false,"product_type":"c1","uid":1494491,"ip_address":"","ucode":"3175754775CA32","user_header":"https://static001.geekbang.org/account/avatar/00/16/cd/db/7467ad23.jpg","comment_is_top":false,"comment_ctime":1609897481,"is_pvip":false,"replies":[{"id":99529,"content":"可以看出来你思考得很深入，包括分析解决方案的缺陷和局限性，这是很好的学习方法。我也说一下我对你的问题的一些解读。\nlsm类型的数据库不是用来做随机检索或范围检索的最佳选择，它更适合的是写多读少，尤其是读近期数据的场景。\n至于打开多个文件效率低，包括寻道性能低的问题，这其实可以通过缓存来部分解决。但的确当大量文件被打开的时候，磁盘读写性能是不高的。因此文件的分层，缓存的使用，其实都是为了解决这个问题。包括你的第五个问题，为什么不对多个文件进行处理，也有这一方面的原因。(以上是对1－4问题的回答)。\n至于第五个问题，为什么只选一个文件，我的理解是为了保证一次合并不要涉及过多的文件，减少io。实际上，只要知道了这个合并原理，一次选两个或三个文件来合并也不是不可以。比如说seek compaction过程，就是可以将经常查询失败的sstable放在合并列表中，进行批量处理。主要还是要注意io性能。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1610893454,"ip_address":"","comment_id":271966,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"这个数据库在海量数据的情况下真的很快吗，我总感觉一般般的样子啊。\n1. 层次没有上限 单层文件总容量却有上限，因此极端情况下需要搜索的文件依然很多，虽然每个文件有布隆过滤器预搜索，所以单个文件检索性能还不错，但需要一层层打开文件解析文件然后开始搜索，文件数量如果很多，则性能不佳\n2. 如果是范围检索，则注定所有层次都必须被查询，性能不佳\n3. 每个文件尺寸有上限，而且很小，意味着文件数量很多，文件打开数就会很多，当达到通常的 65536 的上限时（如果每个文件 2m 大小，那么也就存了 128g，实际上由于进程自身也有文件打开数开销，实际上能提供给 leveldb 的文件打开数配额会远远小于这个值），就只能被迫使用 lru 来关闭一些不常用的小文件了，如果频繁打开解析关闭小文件时，性能不佳\n4. 多路归并多个文件的数据，意味着磁盘在多个文件中来回寻道，哪怕只有最多十个文件，性能也不佳\n5. 无法理解为何只选一个文件参与和下一层的归并，选一个文件和选两个文件的区别在哪里？\n","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513104,"discussion_content":"可以看出来你思考得很深入，包括分析解决方案的缺陷和局限性，这是很好的学习方法。我也说一下我对你的问题的一些解读。\nlsm类型的数据库不是用来做随机检索或范围检索的最佳选择，它更适合的是写多读少，尤其是读近期数据的场景。\n至于打开多个文件效率低，包括寻道性能低的问题，这其实可以通过缓存来部分解决。但的确当大量文件被打开的时候，磁盘读写性能是不高的。因此文件的分层，缓存的使用，其实都是为了解决这个问题。包括你的第五个问题，为什么不对多个文件进行处理，也有这一方面的原因。(以上是对1－4问题的回答)。\n至于第五个问题，为什么只选一个文件，我的理解是为了保证一次合并不要涉及过多的文件，减少io。实际上，只要知道了这个合并原理，一次选两个或三个文件来合并也不是不可以。比如说seek compaction过程，就是可以将经常查询失败的sstable放在合并列表中，进行批量处理。主要还是要注意io性能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610893454,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1054248,"avatar":"https://static001.geekbang.org/account/avatar/00/10/16/28/c11e7aae.jpg","nickname":"张仕华","note":"","ucode":"8D41E96A5889FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":542092,"discussion_content":"关于第五个问题的回复，有两个点和老师交流下：\n1.实际上LevelDB中是有这个优化的，通过Level N层的1个文件选取Level N+1层的文件之后，此时键范围假设为[a,b],根据[a,b]这个范围会再次去Level N层选取第二个文件，选定之后会判断Level N+1层是否会继续扩大选取范围，如果没有扩大并且所有选定的文件总大小小于50MB，那么Level N层就是两个文件参与Compaction\n2.seek compaction是其中一种触发方式，似乎跟批量处理关系不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640664388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249116,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1600436207,"is_pvip":true,"replies":[{"id":91461,"content":"其实，sstable是lsm树的基本单位，大约等于b+树中的叶子节点。你会发现，b+树中的每个叶子节点其实也不大，仅仅是一个block大小而已。\n或者你也可以将两个sstable用分隔符区分，然后写到同一个文件中，这样文件个数会减少，然后每个文件也会更大。但这样和维持两个sstable文件有什么区别呢?\n其实回到磁盘读写的本质来看，多个小文件和一个大文件相比，只是多了一些文件打开的操作(读取对应inode)，后面的读取数据其实都是以block为单位进行的，并没有本质区别。\n因此，只要能高效管理多个sstable文件，那么整体性能能得到保证，那么读取性能上和一个大文件差异不大。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1600614630,"ip_address":"","comment_id":249116,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"如果把sstable换成B+树，也有bloomfilter，是不是可以不用限制文件为2m的大小？","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505837,"discussion_content":"其实，sstable是lsm树的基本单位，大约等于b+树中的叶子节点。你会发现，b+树中的每个叶子节点其实也不大，仅仅是一个block大小而已。\n或者你也可以将两个sstable用分隔符区分，然后写到同一个文件中，这样文件个数会减少，然后每个文件也会更大。但这样和维持两个sstable文件有什么区别呢?\n其实回到磁盘读写的本质来看，多个小文件和一个大文件相比，只是多了一些文件打开的操作(读取对应inode)，后面的读取数据其实都是以block为单位进行的，并没有本质区别。\n因此，只要能高效管理多个sstable文件，那么整体性能能得到保证，那么读取性能上和一个大文件差异不大。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600614630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215212,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1588930169,"is_pvip":false,"replies":[{"id":79672,"content":"这是一个好问题！实际上，这也是levelDB的一个瓶颈。当immutable memtable还没有完全写入磁盘时，memtable如果写满了，就会被阻塞住。\n因此，Facebook基于Google的levelDB，开源了一个rocksDB，rocksDB允许创建多个memtable，这样就解决了由于写入磁盘速度太慢导致memtable阻塞的问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588949086,"ip_address":"","comment_id":215212,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"当 MemTable 的存储数据达到上限时，我们直接将它切换为只读的 Immutable MemTable，然后重新生成一个新的 MemTable\n------------------\n这样的一个机制，内存中会出现多个Immutable MemTable 吗？ 上一个Immutable MemTable 没有及时写入到磁盘","like_count":27,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494357,"discussion_content":"这是一个好问题！实际上，这也是levelDB的一个瓶颈。当immutable memtable还没有完全写入磁盘时，memtable如果写满了，就会被阻塞住。\n因此，Facebook基于Google的levelDB，开源了一个rocksDB，rocksDB允许创建多个memtable，这样就解决了由于写入磁盘速度太慢导致memtable阻塞的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588949086,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3077132,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/OFh3NluBCSoxHrS7FVqia5letsNII0B5Rv7uZMs33hQXbbQ7CEqia57cUjAoDmZpqO7WXWl3LZXbt9A1rx4557icw/132","nickname":"Geek_d869e2","note":"","ucode":"9259146136FF06","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":584393,"discussion_content":"好的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660803276,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"河北","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215469,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1588996616,"is_pvip":false,"replies":[{"id":79789,"content":"你的问题我重新整理一下，尤其是level 0层怎么处理，这其实是一个很好的问题:\n问题1:level 0层到level 1层合并的时候，level 0层是有多少个sstable参与合并？\n回答:按道理来说，我们应该是根据轮流选择的策略，选择一个level 0层的sstable进行和下层的合并，但是由于level 0层中的sstable可能范围是重叠的，因此我们需要检查每一个sstable，将有重叠部分的都加入到合并列表中。\n问题2:level n层中的一个sstable要和level n+1层中的所有sstable进行合并么？\n回答:不需要。如果level n层的sstable的最大最小值是begin和end，我们只需要在level n+1层中，找到可能包含begin到end之间的sstable即可。这个数量不会超过10个。因此不会带来太大的io。\n问题3:为什么level n层的sstable和level n+1层的合并，个数不会超过10个？\n回答:在level n层的sstable生成的时候，我们会开始判断这个sstable和level n+1层的哪些sstable有重叠。如果发现重叠个数达到十个，就要结束这个sstable文件的生成。\n举个例子，如果level n+1层的11个sstable的第一个元素分别是[100，200，300，400，……，1000，1100]，即开头都是100的整数倍。那么，如果level n层的sstable文件生成时，准备写入的数据就是[100，200，300，400，……，1000，1100]，那么在要写入1100的时候，系统会发现，如果写入1100，那么这个sstable文件就会和下一层的11个sstable文件有重叠了，会违反规则，因此，这时候会结束这个sstable，也就是说，这个sstable文件中只有100到1000十个数。然后1100会被写入到一个新的sstable中。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589015698,"ip_address":"","comment_id":215469,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"LevelDB 分层的逻辑没有理解\n当 Level0 层 有四个 SSTable 的时候，这时候把这个四个进行归并，然后放到 Level1 层，这时候 Level0 层清空，这个有个问题是 当进行归并后 后生成几个 SSTable ,这里是有什么规则吗？\n\n接下来，然后 Level0 层在满了之后，是Level0 层的每个 SSTable 分别与 Level1 所有的 SSTable 进行多路归并吗？\n\n再然后 Level1 层满了之后，是按照顺序取 Level1 层的一个 SSTable 与 Level2所有的 SSTable 进行多路归并吗？\n\n这样会有大量的 磁盘 IO,老师说利用判断重合度进行解决的？ 这个重合度是怎么计算计算判断的呢？\n\n==============================\n老师的文中的这句话没有看明白：\n在多路归并生成第 n 层的 SSTable 文件时，LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度，如果重合覆盖度超过了 10 个文件，就结束这个 SSTable 的生成，继续生成下一个 SSTable 文件\n","like_count":21,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494447,"discussion_content":"你的问题我重新整理一下，尤其是level 0层怎么处理，这其实是一个很好的问题:\n问题1:level 0层到level 1层合并的时候，level 0层是有多少个sstable参与合并？\n回答:按道理来说，我们应该是根据轮流选择的策略，选择一个level 0层的sstable进行和下层的合并，但是由于level 0层中的sstable可能范围是重叠的，因此我们需要检查每一个sstable，将有重叠部分的都加入到合并列表中。\n问题2:level n层中的一个sstable要和level n+1层中的所有sstable进行合并么？\n回答:不需要。如果level n层的sstable的最大最小值是begin和end，我们只需要在level n+1层中，找到可能包含begin到end之间的sstable即可。这个数量不会超过10个。因此不会带来太大的io。\n问题3:为什么level n层的sstable和level n+1层的合并，个数不会超过10个？\n回答:在level n层的sstable生成的时候，我们会开始判断这个sstable和level n+1层的哪些sstable有重叠。如果发现重叠个数达到十个，就要结束这个sstable文件的生成。\n举个例子，如果level n+1层的11个sstable的第一个元素分别是[100，200，300，400，……，1000，1100]，即开头都是100的整数倍。那么，如果level n层的sstable文件生成时，准备写入的数据就是[100，200，300，400，……，1000，1100]，那么在要写入1100的时候，系统会发现，如果写入1100，那么这个sstable文件就会和下一层的11个sstable文件有重叠了，会违反规则，因此，这时候会结束这个sstable，也就是说，这个sstable文件中只有100到1000十个数。然后1100会被写入到一个新的sstable中。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1589015698,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1005391,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","nickname":"奕","note":"","ucode":"73CEA468CE70C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261879,"discussion_content":"第一问还有点疑问:level0层的每个sstable可能会有范围重叠，需要把重叠的部分提取到合并列表，这个这个合并列表是什么？还有就是提取之后呢，还是要遍及level0层的每个sstable与level1层的sstable进行归并吗？\n\n还有个问题就是:当某层的sstable向下层转移的时候，碰巧下层的空间也满了，这时候的处理方案是向下层递归吗？一直往下找，然后在向上处理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589018068,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215014,"user_name":"吴小智","can_delete":false,"product_type":"c1","uid":1310798,"ip_address":"","ucode":"C7C9F58B5C9F7B","user_header":"https://static001.geekbang.org/account/avatar/00/14/00/4e/be2b206b.jpg","comment_is_top":false,"comment_ctime":1588869253,"is_pvip":false,"replies":[{"id":79606,"content":"lsm树和b+树会有许多不同的特点。但是如果从使用场景来看，最大的区别就是看读和写的需求。\n在随机读很多，但是写入很少的场合，适合使用b+树。因为b+树能快速二分找到任何数据，并且磁盘io很少；但如果是使用lsm树，对于大量的随机读，它无法在内存中命中，因此会去读磁盘，并且是一层一层地多次读磁盘，会带来很严重的读放大效应。\n但如果是大量的写操作的场景的话，lsm树进行了大量的批量写操作优化，因此效率会比b+树高许多。b+树每次写入都要去修改叶子节点，这会带来大量的磁盘io，使得效率急剧下降。这也是为什么日志系统，监控系统这类大量生成写入数据的应用会采用lsm树的原因。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588913718,"ip_address":"","comment_id":215014,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"之前看过基于 lsm 的存储系统的代码，能很好理解这篇文章。不过，还是不太理解基于 B+ 树与基于 lsm 的存储系统，两者的优缺点和使用场景有何不同，老师有时间可以解答一下。","like_count":19,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494275,"discussion_content":"lsm树和b+树会有许多不同的特点。但是如果从使用场景来看，最大的区别就是看读和写的需求。\n在随机读很多，但是写入很少的场合，适合使用b+树。因为b+树能快速二分找到任何数据，并且磁盘io很少；但如果是使用lsm树，对于大量的随机读，它无法在内存中命中，因此会去读磁盘，并且是一层一层地多次读磁盘，会带来很严重的读放大效应。\n但如果是大量的写操作的场景的话，lsm树进行了大量的批量写操作优化，因此效率会比b+树高许多。b+树每次写入都要去修改叶子节点，这会带来大量的磁盘io，使得效率急剧下降。这也是为什么日志系统，监控系统这类大量生成写入数据的应用会采用lsm树的原因。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588913718,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1905163,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/12/0b/d2335912.jpg","nickname":"kissingurami","note":"","ucode":"5399F825A4E083","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":260745,"discussion_content":"楼主有点儿本末倒置了吧。先理解lsm和B+的设计初衷再去看代码效率更高。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1588899614,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1150927,"avatar":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","nickname":"那时刻","note":"","ucode":"B0D150856C3A4A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":260743,"discussion_content":"mongodb里有B+树和LSM树两种方式来存储索引，一般来说，对于写多读少情况，适用LSM树。对于读多写少情况，适合B+树。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1588899281,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":218323,"user_name":"王坤祥","can_delete":false,"product_type":"c1","uid":1003327,"ip_address":"","ucode":"FB988B9F381A33","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4f/3f/6f62f982.jpg","comment_is_top":false,"comment_ctime":1589779881,"is_pvip":false,"replies":[{"id":80755,"content":"1.结论是对的，不过有一个小细节要注意一下，下一层的数据，是更老的一个数据，不一定是“一样的”。由于它不够上一层的新，因此可以放弃。\n2.是的。你把这一个约束规则找到了！因为有着这一条约束规则，可能某一层的sstable在某个时刻数量很多，但是每个文件都很小，如果通过文件数量限制，就使得这一层可能存不了什么数据。因此用总容量进行限制更合理。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589801855,"ip_address":"","comment_id":218323,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"1. 既然要查找的数据在某一层查到了，按照LevelDB的分层管理的设计，即使下一层数据也存在，数据也是一样的，没有必要再去查找下一层的数据了。\n\n2. “在多路归并生成第 n 层的 SSTable 文件时，LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度，如果重合覆盖度超过了 10 个文件，就结束这个 SSTable 的生成，继续生成下一个 SSTable 文件。”———如果通过控制sstable文件数量来限制每层容量的话，有可能每个sstable会比较小，很快就达到数量限制，可能分层作用就不明显了。","like_count":9,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":495429,"discussion_content":"1.结论是对的，不过有一个小细节要注意一下，下一层的数据，是更老的一个数据，不一定是“一样的”。由于它不够上一层的新，因此可以放弃。\n2.是的。你把这一个约束规则找到了！因为有着这一条约束规则，可能某一层的sstable在某个时刻数量很多，但是每个文件都很小，如果通过文件数量限制，就使得这一层可能存不了什么数据。因此用总容量进行限制更合理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589801855,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":216179,"user_name":"时隐时现","can_delete":false,"product_type":"c1","uid":1111899,"ip_address":"","ucode":"DA4D622FF84920","user_header":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","comment_is_top":false,"comment_ctime":1589201077,"is_pvip":false,"replies":[{"id":79985,"content":"1.虽然跳表和b+树在时间代价上都是一个量级的，但是跳表的插入删除都很简单，而b+树的插入删除会有节点分裂，节点合并，节点调整等问题，因此从工程效率来看，在纯内存的环境下，b+树并不比跳表和红黑树更合适。\n2.所有的sstable都是只读的，不可更改。新的sstable生成了以后，老的sstable才会被删除，读操作才会转移到新的sstable上。因此，sstable不会被同时读写，没有读写阻塞的问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589208073,"ip_address":"","comment_id":216179,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师好，有2个问题：\n1、内存中的C0树，采用跳表替换掉B+树，检索效率会有提升吗？我一直觉得两者是差不多的吧，什么场景下跳表会比B+树性能高很多？\n2、滚动合并应该是后台操作，在合并的过程中，相应的sstable应该是被写锁锁定的吧？此时如果有应用执行读，会不会被阻塞？如果不阻塞，如何保证读写一致性？","like_count":7,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494704,"discussion_content":"1.虽然跳表和b+树在时间代价上都是一个量级的，但是跳表的插入删除都很简单，而b+树的插入删除会有节点分裂，节点合并，节点调整等问题，因此从工程效率来看，在纯内存的环境下，b+树并不比跳表和红黑树更合适。\n2.所有的sstable都是只读的，不可更改。新的sstable生成了以后，老的sstable才会被删除，读操作才会转移到新的sstable上。因此，sstable不会被同时读写，没有读写阻塞的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589208073,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1218937,"avatar":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","nickname":"anker","note":"","ucode":"6EDF1FB9D45238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387942,"discussion_content":"大家查找效率都差不多，跳表的插入效率高多了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1628504015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215298,"user_name":"xaviers","can_delete":false,"product_type":"c1","uid":1879918,"ip_address":"","ucode":"58D51C4DDC5BA8","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","comment_is_top":false,"comment_ctime":1588949971,"is_pvip":false,"replies":[{"id":79702,"content":"因为对于b+树，当内存中的change buffer写满的时候，会去更新多个叶子节点，这会带来多次磁盘IO;但lsm当内存中的memtable写满时，只会去写一次sstable文件。因此它们的主要差异，还是在怎么将数据写入磁盘上。\n当然所有的系统设计都是有利有弊，要做权衡。b+树写入磁盘后，随机读性能比较好;而lsm树写磁盘一时爽，但要随机读的时候就不爽了，它可能得在多层去寻找sstable文件，因此随机读性能比b+树差。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588985758,"ip_address":"","comment_id":215298,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师，不好意思哈，再追问一下😬那为啥用change buffer + WAL优化后的MySQL的写性能还是不如LSM类的存储系统啊？原因是啥啊","like_count":7,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494704,"discussion_content":"1.虽然跳表和b+树在时间代价上都是一个量级的，但是跳表的插入删除都很简单，而b+树的插入删除会有节点分裂，节点合并，节点调整等问题，因此从工程效率来看，在纯内存的环境下，b+树并不比跳表和红黑树更合适。\n2.所有的sstable都是只读的，不可更改。新的sstable生成了以后，老的sstable才会被删除，读操作才会转移到新的sstable上。因此，sstable不会被同时读写，没有读写阻塞的问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589208073,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1218937,"avatar":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","nickname":"anker","note":"","ucode":"6EDF1FB9D45238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387942,"discussion_content":"大家查找效率都差不多，跳表的插入效率高多了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1628504015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215599,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1589028704,"is_pvip":false,"replies":[{"id":79811,"content":"1.合并列表其实就是记录需要合并的sstable的列表。实际上，每次合并时，系统都会生成两个合并列表。\n以你提问的level 0层的情况为例，先选定一个要合并的sstable，然后将level 0层中和它范围重叠的sstable都加入到这个列表中;这就是合并列表1。\n然后，对于合并列表1中所有的sstable，我们能找到整体的范围begin和end。那么我们在下一层中，将和begin和end范围重叠的所有sstable文件加入合并列表2。\n那么，对于合并列表1和合并列表2中的所有的sstable，我们将它们一起做一次多路归并就可以了。\n2.如果下层空间满了，没关系，先合并完，这时候，下层空间就超容量了。那么，我们再针对这一层，按之前介绍的规则，选择一个sstable再和下层合并即可。\nPS:再补充一下知识点:合并的触发条件。\n系统会统计每个level的文件容量是否超过限制。超过上限比例最大的，将会被触发合并操作。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589042904,"ip_address":"","comment_id":215599,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"在评论下回复老师看不到啊，那就在评论问一下\n\n第一问还有点疑问:level0层的每个sstable可能会有范围重叠，需要把重叠的部分提取到合并列表，这个这个合并列表是什么？还有就是提取之后呢，还是要遍及level0层的每个sstable与level1层的sstable进行归并吗？\n\n还有个问题就是:当某层的sstable向下层转移的时候，碰巧下层的空间也满了，这时候的处理方案是向下层递归吗？一直往下找，然后在向上处理","like_count":6,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494491,"discussion_content":"1.合并列表其实就是记录需要合并的sstable的列表。实际上，每次合并时，系统都会生成两个合并列表。\n以你提问的level 0层的情况为例，先选定一个要合并的sstable，然后将level 0层中和它范围重叠的sstable都加入到这个列表中;这就是合并列表1。\n然后，对于合并列表1中所有的sstable，我们能找到整体的范围begin和end。那么我们在下一层中，将和begin和end范围重叠的所有sstable文件加入合并列表2。\n那么，对于合并列表1和合并列表2中的所有的sstable，我们将它们一起做一次多路归并就可以了。\n2.如果下层空间满了，没关系，先合并完，这时候，下层空间就超容量了。那么，我们再针对这一层，按之前介绍的规则，选择一个sstable再和下层合并即可。\nPS:再补充一下知识点:合并的触发条件。\n系统会统计每个level的文件容量是否超过限制。超过上限比例最大的，将会被触发合并操作。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589042904,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1005391,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","nickname":"奕","note":"","ucode":"73CEA468CE70C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262307,"discussion_content":"谢谢老师，结合上个评论这个合并的流程明白了。google设计leveldb 考虑了每个细节，那个细节都是为了提升性能啊。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589073496,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215096,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1588903972,"is_pvip":false,"replies":[{"id":79668,"content":"1.不用停止计算，而是算完后，判断容量是否达到上限，如果超过，就根据文中介绍的选择文件的方式，将多余的文件和下一层进行合并。\n2.如果存在多个filter block，而且每个filter都很大的话(比如说bloomfilter就有许多数据)，将所有的filter都读入内存会造成多次磁盘IO,因此需要有metaphor index block，帮助我们只读取我们需要的filter即可。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588947207,"ip_address":"","comment_id":215096,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"有两个问题，请教下老师。\n1。在多路归并生成第 n 层的 SSTable 文件时，如何控制当前层最大容量呢？如果超过当前层的容量是停止计算还是把多余的量挪到下一层？\n2。数据索引区里meta index block，当存在多个过滤器时，对过滤器进行索引。这是涉及到filter block过滤么？","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494491,"discussion_content":"1.合并列表其实就是记录需要合并的sstable的列表。实际上，每次合并时，系统都会生成两个合并列表。\n以你提问的level 0层的情况为例，先选定一个要合并的sstable，然后将level 0层中和它范围重叠的sstable都加入到这个列表中;这就是合并列表1。\n然后，对于合并列表1中所有的sstable，我们能找到整体的范围begin和end。那么我们在下一层中，将和begin和end范围重叠的所有sstable文件加入合并列表2。\n那么，对于合并列表1和合并列表2中的所有的sstable，我们将它们一起做一次多路归并就可以了。\n2.如果下层空间满了，没关系，先合并完，这时候，下层空间就超容量了。那么，我们再针对这一层，按之前介绍的规则，选择一个sstable再和下层合并即可。\nPS:再补充一下知识点:合并的触发条件。\n系统会统计每个level的文件容量是否超过限制。超过上限比例最大的，将会被触发合并操作。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589042904,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1005391,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","nickname":"奕","note":"","ucode":"73CEA468CE70C3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262307,"discussion_content":"谢谢老师，结合上个评论这个合并的流程明白了。google设计leveldb 考虑了每个细节，那个细节都是为了提升性能啊。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589073496,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":271966,"user_name":"Bachue Zhou","can_delete":false,"product_type":"c1","uid":1494491,"ip_address":"","ucode":"3175754775CA32","user_header":"https://static001.geekbang.org/account/avatar/00/16/cd/db/7467ad23.jpg","comment_is_top":false,"comment_ctime":1609897481,"is_pvip":false,"replies":[{"id":99529,"content":"可以看出来你思考得很深入，包括分析解决方案的缺陷和局限性，这是很好的学习方法。我也说一下我对你的问题的一些解读。\nlsm类型的数据库不是用来做随机检索或范围检索的最佳选择，它更适合的是写多读少，尤其是读近期数据的场景。\n至于打开多个文件效率低，包括寻道性能低的问题，这其实可以通过缓存来部分解决。但的确当大量文件被打开的时候，磁盘读写性能是不高的。因此文件的分层，缓存的使用，其实都是为了解决这个问题。包括你的第五个问题，为什么不对多个文件进行处理，也有这一方面的原因。(以上是对1－4问题的回答)。\n至于第五个问题，为什么只选一个文件，我的理解是为了保证一次合并不要涉及过多的文件，减少io。实际上，只要知道了这个合并原理，一次选两个或三个文件来合并也不是不可以。比如说seek compaction过程，就是可以将经常查询失败的sstable放在合并列表中，进行批量处理。主要还是要注意io性能。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1610893454,"ip_address":"","comment_id":271966,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"这个数据库在海量数据的情况下真的很快吗，我总感觉一般般的样子啊。\n1. 层次没有上限 单层文件总容量却有上限，因此极端情况下需要搜索的文件依然很多，虽然每个文件有布隆过滤器预搜索，所以单个文件检索性能还不错，但需要一层层打开文件解析文件然后开始搜索，文件数量如果很多，则性能不佳\n2. 如果是范围检索，则注定所有层次都必须被查询，性能不佳\n3. 每个文件尺寸有上限，而且很小，意味着文件数量很多，文件打开数就会很多，当达到通常的 65536 的上限时（如果每个文件 2m 大小，那么也就存了 128g，实际上由于进程自身也有文件打开数开销，实际上能提供给 leveldb 的文件打开数配额会远远小于这个值），就只能被迫使用 lru 来关闭一些不常用的小文件了，如果频繁打开解析关闭小文件时，性能不佳\n4. 多路归并多个文件的数据，意味着磁盘在多个文件中来回寻道，哪怕只有最多十个文件，性能也不佳\n5. 无法理解为何只选一个文件参与和下一层的归并，选一个文件和选两个文件的区别在哪里？\n","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494314,"discussion_content":"1.不用停止计算，而是算完后，判断容量是否达到上限，如果超过，就根据文中介绍的选择文件的方式，将多余的文件和下一层进行合并。\n2.如果存在多个filter block，而且每个filter都很大的话(比如说bloomfilter就有许多数据)，将所有的filter都读入内存会造成多次磁盘IO,因此需要有metaphor index block，帮助我们只读取我们需要的filter即可。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588947207,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249116,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1600436207,"is_pvip":true,"replies":[{"id":91461,"content":"其实，sstable是lsm树的基本单位，大约等于b+树中的叶子节点。你会发现，b+树中的每个叶子节点其实也不大，仅仅是一个block大小而已。\n或者你也可以将两个sstable用分隔符区分，然后写到同一个文件中，这样文件个数会减少，然后每个文件也会更大。但这样和维持两个sstable文件有什么区别呢?\n其实回到磁盘读写的本质来看，多个小文件和一个大文件相比，只是多了一些文件打开的操作(读取对应inode)，后面的读取数据其实都是以block为单位进行的，并没有本质区别。\n因此，只要能高效管理多个sstable文件，那么整体性能能得到保证，那么读取性能上和一个大文件差异不大。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1600614630,"ip_address":"","comment_id":249116,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"如果把sstable换成B+树，也有bloomfilter，是不是可以不用限制文件为2m的大小？","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513104,"discussion_content":"可以看出来你思考得很深入，包括分析解决方案的缺陷和局限性，这是很好的学习方法。我也说一下我对你的问题的一些解读。\nlsm类型的数据库不是用来做随机检索或范围检索的最佳选择，它更适合的是写多读少，尤其是读近期数据的场景。\n至于打开多个文件效率低，包括寻道性能低的问题，这其实可以通过缓存来部分解决。但的确当大量文件被打开的时候，磁盘读写性能是不高的。因此文件的分层，缓存的使用，其实都是为了解决这个问题。包括你的第五个问题，为什么不对多个文件进行处理，也有这一方面的原因。(以上是对1－4问题的回答)。\n至于第五个问题，为什么只选一个文件，我的理解是为了保证一次合并不要涉及过多的文件，减少io。实际上，只要知道了这个合并原理，一次选两个或三个文件来合并也不是不可以。比如说seek compaction过程，就是可以将经常查询失败的sstable放在合并列表中，进行批量处理。主要还是要注意io性能。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610893454,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1054248,"avatar":"https://static001.geekbang.org/account/avatar/00/10/16/28/c11e7aae.jpg","nickname":"张仕华","note":"","ucode":"8D41E96A5889FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":542092,"discussion_content":"关于第五个问题的回复，有两个点和老师交流下：\n1.实际上LevelDB中是有这个优化的，通过Level N层的1个文件选取Level N+1层的文件之后，此时键范围假设为[a,b],根据[a,b]这个范围会再次去Level N层选取第二个文件，选定之后会判断Level N+1层是否会继续扩大选取范围，如果没有扩大并且所有选定的文件总大小小于50MB，那么Level N层就是两个文件参与Compaction\n2.seek compaction是其中一种触发方式，似乎跟批量处理关系不大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640664388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249114,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1600436105,"is_pvip":true,"replies":[{"id":91460,"content":"实际上，sstable的大小是可以设置的，可以调整到2m以上。不过不建议设置太大。原因如下:\n1.对于level 0层的sstable而言，它是由内存中的memtable转过来的。如果内存中要写满大量数据才落盘，那么为了防止内存数据丢失，wal文件就要写入相应数据量的数据。但wal为了保证效率不宜过大，因此level 0层的sstable不宜过大。\n2.对于level 1层以上的sstable，文件上限可以放宽一些，不过也不宜过大。否则两个sstable合并时，合并代价就会很大，会有大量的无关数据被进行读写，反而影响整体效率。\n\n此外，多个小文件并不一定会比一个大文件读写效率低。一方面，是level db的sstable合并机制，对于小文件是更有利的(大文件合并更慢);另一方面，sstable的信息其实有缓存和meta文件进行管理，对多个文件的读写操作其实可以避免多次文件io操作，比常规的文件系统随机读写多个小文件效率会高很多。因此，level db存储和管理多个sstable文件，整体效率并不低。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1600613991,"ip_address":"","comment_id":249114,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"为什么要限制sstable为2m，感觉很小啊，如果是个很大的数据集，文件不是会很多？","like_count":3},{"had_liked":false,"id":229507,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1593042629,"is_pvip":false,"replies":[{"id":84753,"content":"因为levelDB的本质还是lsm树的优化实现，因此它的应用场景和lsm树一样，依然是适用于写多读少的kv存储场景中。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1593099491,"ip_address":"","comment_id":229507,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师 level db 的应用场景是什么呀","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505836,"discussion_content":"实际上，sstable的大小是可以设置的，可以调整到2m以上。不过不建议设置太大。原因如下:\n1.对于level 0层的sstable而言，它是由内存中的memtable转过来的。如果内存中要写满大量数据才落盘，那么为了防止内存数据丢失，wal文件就要写入相应数据量的数据。但wal为了保证效率不宜过大，因此level 0层的sstable不宜过大。\n2.对于level 1层以上的sstable，文件上限可以放宽一些，不过也不宜过大。否则两个sstable合并时，合并代价就会很大，会有大量的无关数据被进行读写，反而影响整体效率。\n\n此外，多个小文件并不一定会比一个大文件读写效率低。一方面，是level db的sstable合并机制，对于小文件是更有利的(大文件合并更慢);另一方面，sstable的信息其实有缓存和meta文件进行管理，对多个文件的读写操作其实可以避免多次文件io操作，比常规的文件系统随机读写多个小文件效率会高很多。因此，level db存储和管理多个sstable文件，整体效率并不低。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600613991,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1066752,"avatar":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","nickname":"piboye","note":"","ucode":"7CFD8712857A85","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":308080,"discussion_content":"老师，我想到一种处理消息的情况，如果要保存很久的数据，历史消息就会很大。我就想着按天去保存文件，所以会提出是否sstable可以更大的问题。不知道历史消息是不是可以大文件b+方式按天存储，再另外一个索引文件存储每个用户的seq段具体在哪一天的文件上的方式？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600841585,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227944,"user_name":"Geek_863b69","can_delete":false,"product_type":"c1","uid":1588653,"ip_address":"","ucode":"DAD05568516F07","user_header":"https://static001.geekbang.org/account/avatar/00/18/3d/ad/819a731a.jpg","comment_is_top":false,"comment_ctime":1592529071,"is_pvip":false,"replies":[{"id":84185,"content":"你思考了缓存数据和磁盘数据的一致性问题，这一点非常好。在这里我补充两个知识点:\n1.对于leveldb而言，每个sstable都是只读的，不可修改的，所谓的sstable和上一层数据合并了，结果是会生成新的sstable，然后删除旧的sstable。因此并不会出现缓存和磁盘数据不一致的问题(缓存的是一个旧的sstable，而磁盘上是另一个新的sstable，查询时会访问新的sstable文件，该文件并不在缓存中，因此系统并不会去误读缓存中旧的sstable中的数据)。\n2.对于其他的一些应用，关于如何保证内存缓存数据和磁盘数据的一致性，一种常见的解决方案是在写操作时，先删除缓存，然后再修改磁盘数据。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1592644157,"ip_address":"","comment_id":227944,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师，请教下，levelDB中，data block存的是sstable的数据，如果sstable跟上一层数据合并了，那么查找的时候如果直接从缓存找，数据不就不一致了？还是说合并的时候会顺带删除缓存？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499513,"discussion_content":"因为levelDB的本质还是lsm树的优化实现，因此它的应用场景和lsm树一样，依然是适用于写多读少的kv存储场景中。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593099491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1187478,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1e/96/c735ad6b.jpg","nickname":"滩涂曳尾","note":"","ucode":"40F650F2A419D4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":311741,"discussion_content":"这一点可以对比redis，备忘","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602478645,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215254,"user_name":"xaviers","can_delete":false,"product_type":"c1","uid":1879918,"ip_address":"","ucode":"58D51C4DDC5BA8","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","comment_is_top":false,"comment_ctime":1588942111,"is_pvip":false,"replies":[{"id":79670,"content":"你说得对，在MySQL的b+树的具体实现中，其实借鉴了许多lsm树的设计思想来提升性能，比如使用wal技术+change buffer，然后批量写叶子节点，而不是每次都随机写。这样就能减少磁盘IO。的确比原始的b+树快。\n不过在大批量写的应用场景中，这样优化后的b+树性能还是没有lsm树更好。因此日志系统这类场景还是使用lsm树更合适。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588948147,"ip_address":"","comment_id":215254,"utype":1}],"discussion_count":4,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师晚上好，请教个问题哈。\n\nMySQL在写数据的时候，是先写到change buffer内存中的，不会立刻写磁盘的，达到一定量再将change buffer落盘。这个和Memtable的设计理念类似，按理说，速度也不会太慢吧？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498852,"discussion_content":"你思考了缓存数据和磁盘数据的一致性问题，这一点非常好。在这里我补充两个知识点:\n1.对于leveldb而言，每个sstable都是只读的，不可修改的，所谓的sstable和上一层数据合并了，结果是会生成新的sstable，然后删除旧的sstable。因此并不会出现缓存和磁盘数据不一致的问题(缓存的是一个旧的sstable，而磁盘上是另一个新的sstable，查询时会访问新的sstable文件，该文件并不在缓存中，因此系统并不会去误读缓存中旧的sstable中的数据)。\n2.对于其他的一些应用，关于如何保证内存缓存数据和磁盘数据的一致性，一种常见的解决方案是在写操作时，先删除缓存，然后再修改磁盘数据。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592644157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215087,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1588902137,"is_pvip":false,"replies":[{"id":79667,"content":"问题1:你进一步去思考删除问题。这一点很好。数据删除时，是会将记录打上一个删除标记，然后写入sstable中。\nsstable和下一层合并时，对于带着删除标记的记录，levelDB会判断下层到最后一层是否还有这个key记录，有两种结果:\n1.如果还有，那么这个删除标记就不能去掉，要一直保留到最后一层遇到相同key的时候才能删除；\n2.如果没有，那么就可以删除掉这个带删除标记的记录。\n问题2:因为在进行合并时，新生成的sstable会受到一个约束:如果和下一层的sstable重叠数超过了十个，就要停止生成这个sstable，要再继续生成一个新的sstable。这个机制会导致我们不好控制文件的个数。不如限定容量更合适。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588946467,"ip_address":"","comment_id":215087,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"讨论问题1：在某一层找到了key，不需要再去下一层查找的原因是，这一层是最新的数据。即使下一层有的话，是旧数据。这引申出另外一个问题，数据删除的时候是怎么处理的呢？是另外一个删除列表来保存删除的key吗？\n问题2：如老师提示的这样，SSTable 的生成过程会受到约束，SSTable在归并的过程中，可能由于数据倾斜，导致某个分区里的数据量比较大，所以没有办法保证每个SSTable的大小。","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494372,"discussion_content":"你说得对，在MySQL的b+树的具体实现中，其实借鉴了许多lsm树的设计思想来提升性能，比如使用wal技术+change buffer，然后批量写叶子节点，而不是每次都随机写。这样就能减少磁盘IO。的确比原始的b+树快。\n不过在大批量写的应用场景中，这样优化后的b+树性能还是没有lsm树更好。因此日志系统这类场景还是使用lsm树更合适。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588948147,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1879918,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","nickname":"xaviers","note":"","ucode":"58D51C4DDC5BA8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261279,"discussion_content":"老师，那为啥优化后的b+树写性能仍然不如LSM啊？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588949725,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1111899,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","nickname":"时隐时现","note":"","ucode":"DA4D622FF84920","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1879918,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","nickname":"xaviers","note":"","ucode":"58D51C4DDC5BA8","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263768,"discussion_content":"大批量写的B+树，仍然会生成大量随机IO，change buffer只适用于非唯一二级索引，只能一定程度上缓解而不是彻底解决；另外，大量写会导致叶子节点分裂，也会额外加重系统负担。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589246164,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":261279,"ip_address":"","group_id":0},"score":263768,"extra":""},{"author":{"id":1879918,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","nickname":"xaviers","note":"","ucode":"58D51C4DDC5BA8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1111899,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","nickname":"时隐时现","note":"","ucode":"DA4D622FF84920","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263794,"discussion_content":"早上好，谢谢指导。\n\n请问下，为什么“change buffer只适用于非唯一二级索引”？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589247230,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":263768,"ip_address":"","group_id":0},"score":263794,"extra":""}]}]},{"had_liked":false,"id":289710,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1619148838,"is_pvip":false,"replies":[{"id":105517,"content":"这是一个好问题。你可以先思考一下，如果让你设计你会怎么做。这门课学到这里，相信你会有做索引的思路了，实际上，leveldb就是加了一个叫manifest的索引文件来实现的。\n在manifest文件中，所有sstable的 Key 取值范围、层级和其它元信息都被记录下来。只需要将manifest文件加载到内存中，就可以快速查到所有sstable文件的层级和 Key 取值范围。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1620221682,"ip_address":"","comment_id":289710,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"“LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度”，\n判断第N层的SSTable跟N+1层的覆盖重合度，这块逻辑是怎么实现的，需要将第N层的数据加载到内存每条记录都判断，还是有额外的索引记录着第N层的数据范围？","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494310,"discussion_content":"问题1:你进一步去思考删除问题。这一点很好。数据删除时，是会将记录打上一个删除标记，然后写入sstable中。\nsstable和下一层合并时，对于带着删除标记的记录，levelDB会判断下层到最后一层是否还有这个key记录，有两种结果:\n1.如果还有，那么这个删除标记就不能去掉，要一直保留到最后一层遇到相同key的时候才能删除；\n2.如果没有，那么就可以删除掉这个带删除标记的记录。\n问题2:因为在进行合并时，新生成的sstable会受到一个约束:如果和下一层的sstable重叠数超过了十个，就要停止生成这个sstable，要再继续生成一个新的sstable。这个机制会导致我们不好控制文件的个数。不如限定容量更合适。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588946467,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215517,"user_name":"牛牛","can_delete":false,"product_type":"c1","uid":1194626,"ip_address":"","ucode":"CFCE68B4F92209","user_header":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","comment_is_top":false,"comment_ctime":1589008647,"is_pvip":false,"replies":[{"id":79810,"content":"首先，levelDB并没有“脏缓存”的问题。因为lsm树和b+树不一样。\nb+树的缓存对应着磁盘上的叶子节点，叶子节点是可以被修改的，因此会出现缓存在内存中的数据被修改，但是磁盘对应的叶子节点还未修改的“脏缓存”问题。\n而levelDB中，data block存的是sstable的数据，而每个sstable文件是只读的，不可修改的，因此不会出现“脏缓存”问题。\n另一点，如果缓存数据被大片读入的新数据驱除，是否会有优化方案？这其实就依赖于lru的具体实现了(比如分为old和young区)，levelDB本身并没有做特殊处理。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589042010,"ip_address":"","comment_id":215517,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师、我想请教下、levelDB是怎么处理`脏缓存`(eg. 有用户突然访问了别人很久不访问的数据(假设还比较大)、导致本来应该在缓存中的数据被驱逐, Data Block的优化效果就会打折扣)的 ?\n\n------\n我是想到了Mysql 处理Buffer Poll的机制(分为Old 和 Young区)、类似的思想在jvm的gc管理中也有用到\n\n","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518997,"discussion_content":"这是一个好问题。你可以先思考一下，如果让你设计你会怎么做。这门课学到这里，相信你会有做索引的思路了，实际上，leveldb就是加了一个叫manifest的索引文件来实现的。\n在manifest文件中，所有sstable的 Key 取值范围、层级和其它元信息都被记录下来。只需要将manifest文件加载到内存中，就可以快速查到所有sstable文件的层级和 Key 取值范围。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620221682,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369755,"discussion_content":"看到文章后边发现，sstable 中是有索引的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619149230,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215229,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1588933448,"is_pvip":false,"replies":[{"id":79674,"content":"1.没错，最新的数据在上层，所以上层能找到，就不需要去下层读旧数据了。\n2.是的，由于在生成sstable文件时，有这么一个限制:新生成的sstable文件不能和下层的sstable覆盖度超过十个，因此可能会生成多个小的sstable文件。那如果只看文件数的话，多个小的sstable文件可能容量和下一层差不多，这样就没有了分层的作用了。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588949381,"ip_address":"","comment_id":215229,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"课后思考：\n1: 因为 LevelDB 天然的具有缓存的特性，最经常使用的最新的数据离用户最近，所有在上层找到数据就不会在向下找了\n2: 如果规定生成文件的个数，那么有可能当前层和下一层的存储大小相近了，起不到分层的作用了","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494464,"discussion_content":"首先，levelDB并没有“脏缓存”的问题。因为lsm树和b+树不一样。\nb+树的缓存对应着磁盘上的叶子节点，叶子节点是可以被修改的，因此会出现缓存在内存中的数据被修改，但是磁盘对应的叶子节点还未修改的“脏缓存”问题。\n而levelDB中，data block存的是sstable的数据，而每个sstable文件是只读的，不可修改的，因此不会出现“脏缓存”问题。\n另一点，如果缓存数据被大片读入的新数据驱除，是否会有优化方案？这其实就依赖于lru的具体实现了(比如分为old和young区)，levelDB本身并没有做特殊处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589042010,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194626,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","nickname":"牛牛","note":"","ucode":"CFCE68B4F92209","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262230,"discussion_content":"对啊、对啊、sstable是只读的、明白了、感谢老师啊、这么晚了还在帮忙回复问题~~~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589042415,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215037,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1588894801,"is_pvip":false,"replies":[{"id":79620,"content":"1.你提到的这个问题非常好，lsm的范围查询比较弱，需要遍历。一种优化思路是先在level 1层中找到range，然后基于start和end的位置，去下一层再找start和end的位置。这样能提高范围查询的性能。\n2.我说一下我的理解，由于有“生成的每个sstable和下一层的sstable重合度不能超过十个”这个约束，所以sstable生成过程中可能随时被截断，因此不好控制sstable的数量。不如控制容量简单。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588925272,"ip_address":"","comment_id":215037,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"问题1： 因为只是get(key), 所以上层的sstabe的数据是最新的，所以没必要再往下面查，但如果有hbase这样的scan(startkey,endkey) 那还是得全局的多路归并（当然可以通过文件元数据迅速排除掉一些hfile）\n问题2：SSTable 的生成过程会受到约束，无法保证每一个 SSTable 文件的大小。哈哈哈，我在抄答案，我其实有疑问，就算限定文件数量，那么在层次合并的时候，假设我是先合成一个整个sstable再切，面临的对这一整个sstable怎么切成文件的问题，那就顺序的2m一个算会不会文件数量超标，决定是否要滚动下一层，不过我这样想法过于理想显然假设那块就不成立，应该是多路归并式的动态生成，否则对内存压力太大，但是如果按整个层的文件大小分，就不用考虑文件量的问题，只要key连续性大点，文件大小不超过2m就生成就好了。","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494362,"discussion_content":"1.没错，最新的数据在上层，所以上层能找到，就不需要去下层读旧数据了。\n2.是的，由于在生成sstable文件时，有这么一个限制:新生成的sstable文件不能和下层的sstable覆盖度超过十个，因此可能会生成多个小的sstable文件。那如果只看文件数的话，多个小的sstable文件可能容量和下一层差不多，这样就没有了分层的作用了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588949381,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312497,"user_name":"Helios","can_delete":false,"product_type":"c1","uid":1380758,"ip_address":"","ucode":"BE6B98EE8F0D09","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKJrOl63enWXCRxN0SoucliclBme0qrRb19ATrWIOIvibKIz8UAuVgicBMibIVUznerHnjotI4dm6ibODA/132","comment_is_top":false,"comment_ctime":1631843400,"is_pvip":false,"replies":[{"id":113597,"content":"不会的。因为切换的时候，旧的memtable依然存在，其他的读写操作依然能完成，不需要阻塞。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1632580157,"ip_address":"","comment_id":312497,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"请教老师一个问题:\n将内存可读写的table切换为 Immutable MemTable的时候会不会有阻塞操作，类似于gc中的stw，如果发生频繁是不是对性能也有影响。","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494285,"discussion_content":"1.你提到的这个问题非常好，lsm的范围查询比较弱，需要遍历。一种优化思路是先在level 1层中找到range，然后基于start和end的位置，去下一层再找start和end的位置。这样能提高范围查询的性能。\n2.我说一下我的理解，由于有“生成的每个sstable和下一层的sstable重合度不能超过十个”这个约束，所以sstable生成过程中可能随时被截断，因此不好控制sstable的数量。不如控制容量简单。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588925272,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249114,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1600436105,"is_pvip":true,"replies":[{"id":91460,"content":"实际上，sstable的大小是可以设置的，可以调整到2m以上。不过不建议设置太大。原因如下:\n1.对于level 0层的sstable而言，它是由内存中的memtable转过来的。如果内存中要写满大量数据才落盘，那么为了防止内存数据丢失，wal文件就要写入相应数据量的数据。但wal为了保证效率不宜过大，因此level 0层的sstable不宜过大。\n2.对于level 1层以上的sstable，文件上限可以放宽一些，不过也不宜过大。否则两个sstable合并时，合并代价就会很大，会有大量的无关数据被进行读写，反而影响整体效率。\n\n此外，多个小文件并不一定会比一个大文件读写效率低。一方面，是level db的sstable合并机制，对于小文件是更有利的(大文件合并更慢);另一方面，sstable的信息其实有缓存和meta文件进行管理，对多个文件的读写操作其实可以避免多次文件io操作，比常规的文件系统随机读写多个小文件效率会高很多。因此，level db存储和管理多个sstable文件，整体效率并不低。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1600613991,"ip_address":"","comment_id":249114,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"为什么要限制sstable为2m，感觉很小啊，如果是个很大的数据集，文件不是会很多？","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505836,"discussion_content":"实际上，sstable的大小是可以设置的，可以调整到2m以上。不过不建议设置太大。原因如下:\n1.对于level 0层的sstable而言，它是由内存中的memtable转过来的。如果内存中要写满大量数据才落盘，那么为了防止内存数据丢失，wal文件就要写入相应数据量的数据。但wal为了保证效率不宜过大，因此level 0层的sstable不宜过大。\n2.对于level 1层以上的sstable，文件上限可以放宽一些，不过也不宜过大。否则两个sstable合并时，合并代价就会很大，会有大量的无关数据被进行读写，反而影响整体效率。\n\n此外，多个小文件并不一定会比一个大文件读写效率低。一方面，是level db的sstable合并机制，对于小文件是更有利的(大文件合并更慢);另一方面，sstable的信息其实有缓存和meta文件进行管理，对多个文件的读写操作其实可以避免多次文件io操作，比常规的文件系统随机读写多个小文件效率会高很多。因此，level db存储和管理多个sstable文件，整体效率并不低。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600613991,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1066752,"avatar":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","nickname":"piboye","note":"","ucode":"7CFD8712857A85","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":308080,"discussion_content":"老师，我想到一种处理消息的情况，如果要保存很久的数据，历史消息就会很大。我就想着按天去保存文件，所以会提出是否sstable可以更大的问题。不知道历史消息是不是可以大文件b+方式按天存储，再另外一个索引文件存储每个用户的seq段具体在哪一天的文件上的方式？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600841585,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":229507,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1593042629,"is_pvip":false,"replies":[{"id":84753,"content":"因为levelDB的本质还是lsm树的优化实现，因此它的应用场景和lsm树一样，依然是适用于写多读少的kv存储场景中。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1593099491,"ip_address":"","comment_id":229507,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师 level db 的应用场景是什么呀","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499513,"discussion_content":"因为levelDB的本质还是lsm树的优化实现，因此它的应用场景和lsm树一样，依然是适用于写多读少的kv存储场景中。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593099491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1187478,"avatar":"https://static001.geekbang.org/account/avatar/00/12/1e/96/c735ad6b.jpg","nickname":"滩涂曳尾","note":"","ucode":"40F650F2A419D4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":311741,"discussion_content":"这一点可以对比redis，备忘","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602478645,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":227944,"user_name":"Geek_863b69","can_delete":false,"product_type":"c1","uid":1588653,"ip_address":"","ucode":"DAD05568516F07","user_header":"https://static001.geekbang.org/account/avatar/00/18/3d/ad/819a731a.jpg","comment_is_top":false,"comment_ctime":1592529071,"is_pvip":false,"replies":[{"id":84185,"content":"你思考了缓存数据和磁盘数据的一致性问题，这一点非常好。在这里我补充两个知识点:\n1.对于leveldb而言，每个sstable都是只读的，不可修改的，所谓的sstable和上一层数据合并了，结果是会生成新的sstable，然后删除旧的sstable。因此并不会出现缓存和磁盘数据不一致的问题(缓存的是一个旧的sstable，而磁盘上是另一个新的sstable，查询时会访问新的sstable文件，该文件并不在缓存中，因此系统并不会去误读缓存中旧的sstable中的数据)。\n2.对于其他的一些应用，关于如何保证内存缓存数据和磁盘数据的一致性，一种常见的解决方案是在写操作时，先删除缓存，然后再修改磁盘数据。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1592644157,"ip_address":"","comment_id":227944,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师，请教下，levelDB中，data block存的是sstable的数据，如果sstable跟上一层数据合并了，那么查找的时候如果直接从缓存找，数据不就不一致了？还是说合并的时候会顺带删除缓存？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498852,"discussion_content":"你思考了缓存数据和磁盘数据的一致性问题，这一点非常好。在这里我补充两个知识点:\n1.对于leveldb而言，每个sstable都是只读的，不可修改的，所谓的sstable和上一层数据合并了，结果是会生成新的sstable，然后删除旧的sstable。因此并不会出现缓存和磁盘数据不一致的问题(缓存的是一个旧的sstable，而磁盘上是另一个新的sstable，查询时会访问新的sstable文件，该文件并不在缓存中，因此系统并不会去误读缓存中旧的sstable中的数据)。\n2.对于其他的一些应用，关于如何保证内存缓存数据和磁盘数据的一致性，一种常见的解决方案是在写操作时，先删除缓存，然后再修改磁盘数据。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592644157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215254,"user_name":"xaviers","can_delete":false,"product_type":"c1","uid":1879918,"ip_address":"","ucode":"58D51C4DDC5BA8","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","comment_is_top":false,"comment_ctime":1588942111,"is_pvip":false,"replies":[{"id":79670,"content":"你说得对，在MySQL的b+树的具体实现中，其实借鉴了许多lsm树的设计思想来提升性能，比如使用wal技术+change buffer，然后批量写叶子节点，而不是每次都随机写。这样就能减少磁盘IO。的确比原始的b+树快。\n不过在大批量写的应用场景中，这样优化后的b+树性能还是没有lsm树更好。因此日志系统这类场景还是使用lsm树更合适。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588948147,"ip_address":"","comment_id":215254,"utype":1}],"discussion_count":4,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师晚上好，请教个问题哈。\n\nMySQL在写数据的时候，是先写到change buffer内存中的，不会立刻写磁盘的，达到一定量再将change buffer落盘。这个和Memtable的设计理念类似，按理说，速度也不会太慢吧？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494372,"discussion_content":"你说得对，在MySQL的b+树的具体实现中，其实借鉴了许多lsm树的设计思想来提升性能，比如使用wal技术+change buffer，然后批量写叶子节点，而不是每次都随机写。这样就能减少磁盘IO。的确比原始的b+树快。\n不过在大批量写的应用场景中，这样优化后的b+树性能还是没有lsm树更好。因此日志系统这类场景还是使用lsm树更合适。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588948147,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1879918,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","nickname":"xaviers","note":"","ucode":"58D51C4DDC5BA8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261279,"discussion_content":"老师，那为啥优化后的b+树写性能仍然不如LSM啊？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588949725,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1111899,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","nickname":"时隐时现","note":"","ucode":"DA4D622FF84920","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1879918,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","nickname":"xaviers","note":"","ucode":"58D51C4DDC5BA8","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263768,"discussion_content":"大批量写的B+树，仍然会生成大量随机IO，change buffer只适用于非唯一二级索引，只能一定程度上缓解而不是彻底解决；另外，大量写会导致叶子节点分裂，也会额外加重系统负担。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589246164,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":261279,"ip_address":"","group_id":0},"score":263768,"extra":""},{"author":{"id":1879918,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","nickname":"xaviers","note":"","ucode":"58D51C4DDC5BA8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1111899,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f7/5b/d2e7c2c4.jpg","nickname":"时隐时现","note":"","ucode":"DA4D622FF84920","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263794,"discussion_content":"早上好，谢谢指导。\n\n请问下，为什么“change buffer只适用于非唯一二级索引”？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589247230,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":263768,"ip_address":"","group_id":0},"score":263794,"extra":""}]}]},{"had_liked":false,"id":215087,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1588902137,"is_pvip":false,"replies":[{"id":79667,"content":"问题1:你进一步去思考删除问题。这一点很好。数据删除时，是会将记录打上一个删除标记，然后写入sstable中。\nsstable和下一层合并时，对于带着删除标记的记录，levelDB会判断下层到最后一层是否还有这个key记录，有两种结果:\n1.如果还有，那么这个删除标记就不能去掉，要一直保留到最后一层遇到相同key的时候才能删除；\n2.如果没有，那么就可以删除掉这个带删除标记的记录。\n问题2:因为在进行合并时，新生成的sstable会受到一个约束:如果和下一层的sstable重叠数超过了十个，就要停止生成这个sstable，要再继续生成一个新的sstable。这个机制会导致我们不好控制文件的个数。不如限定容量更合适。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588946467,"ip_address":"","comment_id":215087,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"讨论问题1：在某一层找到了key，不需要再去下一层查找的原因是，这一层是最新的数据。即使下一层有的话，是旧数据。这引申出另外一个问题，数据删除的时候是怎么处理的呢？是另外一个删除列表来保存删除的key吗？\n问题2：如老师提示的这样，SSTable 的生成过程会受到约束，SSTable在归并的过程中，可能由于数据倾斜，导致某个分区里的数据量比较大，所以没有办法保证每个SSTable的大小。","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494310,"discussion_content":"问题1:你进一步去思考删除问题。这一点很好。数据删除时，是会将记录打上一个删除标记，然后写入sstable中。\nsstable和下一层合并时，对于带着删除标记的记录，levelDB会判断下层到最后一层是否还有这个key记录，有两种结果:\n1.如果还有，那么这个删除标记就不能去掉，要一直保留到最后一层遇到相同key的时候才能删除；\n2.如果没有，那么就可以删除掉这个带删除标记的记录。\n问题2:因为在进行合并时，新生成的sstable会受到一个约束:如果和下一层的sstable重叠数超过了十个，就要停止生成这个sstable，要再继续生成一个新的sstable。这个机制会导致我们不好控制文件的个数。不如限定容量更合适。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588946467,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289710,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1619148838,"is_pvip":false,"replies":[{"id":105517,"content":"这是一个好问题。你可以先思考一下，如果让你设计你会怎么做。这门课学到这里，相信你会有做索引的思路了，实际上，leveldb就是加了一个叫manifest的索引文件来实现的。\n在manifest文件中，所有sstable的 Key 取值范围、层级和其它元信息都被记录下来。只需要将manifest文件加载到内存中，就可以快速查到所有sstable文件的层级和 Key 取值范围。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1620221682,"ip_address":"","comment_id":289710,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"“LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度”，\n判断第N层的SSTable跟N+1层的覆盖重合度，这块逻辑是怎么实现的，需要将第N层的数据加载到内存每条记录都判断，还是有额外的索引记录着第N层的数据范围？","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518997,"discussion_content":"这是一个好问题。你可以先思考一下，如果让你设计你会怎么做。这门课学到这里，相信你会有做索引的思路了，实际上，leveldb就是加了一个叫manifest的索引文件来实现的。\n在manifest文件中，所有sstable的 Key 取值范围、层级和其它元信息都被记录下来。只需要将manifest文件加载到内存中，就可以快速查到所有sstable文件的层级和 Key 取值范围。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620221682,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369755,"discussion_content":"看到文章后边发现，sstable 中是有索引的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619149230,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215517,"user_name":"牛牛","can_delete":false,"product_type":"c1","uid":1194626,"ip_address":"","ucode":"CFCE68B4F92209","user_header":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","comment_is_top":false,"comment_ctime":1589008647,"is_pvip":false,"replies":[{"id":79810,"content":"首先，levelDB并没有“脏缓存”的问题。因为lsm树和b+树不一样。\nb+树的缓存对应着磁盘上的叶子节点，叶子节点是可以被修改的，因此会出现缓存在内存中的数据被修改，但是磁盘对应的叶子节点还未修改的“脏缓存”问题。\n而levelDB中，data block存的是sstable的数据，而每个sstable文件是只读的，不可修改的，因此不会出现“脏缓存”问题。\n另一点，如果缓存数据被大片读入的新数据驱除，是否会有优化方案？这其实就依赖于lru的具体实现了(比如分为old和young区)，levelDB本身并没有做特殊处理。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589042010,"ip_address":"","comment_id":215517,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师、我想请教下、levelDB是怎么处理`脏缓存`(eg. 有用户突然访问了别人很久不访问的数据(假设还比较大)、导致本来应该在缓存中的数据被驱逐, Data Block的优化效果就会打折扣)的 ?\n\n------\n我是想到了Mysql 处理Buffer Poll的机制(分为Old 和 Young区)、类似的思想在jvm的gc管理中也有用到\n\n","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494464,"discussion_content":"首先，levelDB并没有“脏缓存”的问题。因为lsm树和b+树不一样。\nb+树的缓存对应着磁盘上的叶子节点，叶子节点是可以被修改的，因此会出现缓存在内存中的数据被修改，但是磁盘对应的叶子节点还未修改的“脏缓存”问题。\n而levelDB中，data block存的是sstable的数据，而每个sstable文件是只读的，不可修改的，因此不会出现“脏缓存”问题。\n另一点，如果缓存数据被大片读入的新数据驱除，是否会有优化方案？这其实就依赖于lru的具体实现了(比如分为old和young区)，levelDB本身并没有做特殊处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589042010,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194626,"avatar":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","nickname":"牛牛","note":"","ucode":"CFCE68B4F92209","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262230,"discussion_content":"对啊、对啊、sstable是只读的、明白了、感谢老师啊、这么晚了还在帮忙回复问题~~~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589042415,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215229,"user_name":"奕","can_delete":false,"product_type":"c1","uid":1005391,"ip_address":"","ucode":"73CEA468CE70C3","user_header":"https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg","comment_is_top":false,"comment_ctime":1588933448,"is_pvip":false,"replies":[{"id":79674,"content":"1.没错，最新的数据在上层，所以上层能找到，就不需要去下层读旧数据了。\n2.是的，由于在生成sstable文件时，有这么一个限制:新生成的sstable文件不能和下层的sstable覆盖度超过十个，因此可能会生成多个小的sstable文件。那如果只看文件数的话，多个小的sstable文件可能容量和下一层差不多，这样就没有了分层的作用了。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588949381,"ip_address":"","comment_id":215229,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"课后思考：\n1: 因为 LevelDB 天然的具有缓存的特性，最经常使用的最新的数据离用户最近，所有在上层找到数据就不会在向下找了\n2: 如果规定生成文件的个数，那么有可能当前层和下一层的存储大小相近了，起不到分层的作用了","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494362,"discussion_content":"1.没错，最新的数据在上层，所以上层能找到，就不需要去下层读旧数据了。\n2.是的，由于在生成sstable文件时，有这么一个限制:新生成的sstable文件不能和下层的sstable覆盖度超过十个，因此可能会生成多个小的sstable文件。那如果只看文件数的话，多个小的sstable文件可能容量和下一层差不多，这样就没有了分层的作用了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588949381,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215037,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1588894801,"is_pvip":false,"replies":[{"id":79620,"content":"1.你提到的这个问题非常好，lsm的范围查询比较弱，需要遍历。一种优化思路是先在level 1层中找到range，然后基于start和end的位置，去下一层再找start和end的位置。这样能提高范围查询的性能。\n2.我说一下我的理解，由于有“生成的每个sstable和下一层的sstable重合度不能超过十个”这个约束，所以sstable生成过程中可能随时被截断，因此不好控制sstable的数量。不如控制容量简单。\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588925272,"ip_address":"","comment_id":215037,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"问题1： 因为只是get(key), 所以上层的sstabe的数据是最新的，所以没必要再往下面查，但如果有hbase这样的scan(startkey,endkey) 那还是得全局的多路归并（当然可以通过文件元数据迅速排除掉一些hfile）\n问题2：SSTable 的生成过程会受到约束，无法保证每一个 SSTable 文件的大小。哈哈哈，我在抄答案，我其实有疑问，就算限定文件数量，那么在层次合并的时候，假设我是先合成一个整个sstable再切，面临的对这一整个sstable怎么切成文件的问题，那就顺序的2m一个算会不会文件数量超标，决定是否要滚动下一层，不过我这样想法过于理想显然假设那块就不成立，应该是多路归并式的动态生成，否则对内存压力太大，但是如果按整个层的文件大小分，就不用考虑文件量的问题，只要key连续性大点，文件大小不超过2m就生成就好了。","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494285,"discussion_content":"1.你提到的这个问题非常好，lsm的范围查询比较弱，需要遍历。一种优化思路是先在level 1层中找到range，然后基于start和end的位置，去下一层再找start和end的位置。这样能提高范围查询的性能。\n2.我说一下我的理解，由于有“生成的每个sstable和下一层的sstable重合度不能超过十个”这个约束，所以sstable生成过程中可能随时被截断，因此不好控制sstable的数量。不如控制容量简单。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588925272,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312497,"user_name":"Helios","can_delete":false,"product_type":"c1","uid":1380758,"ip_address":"","ucode":"BE6B98EE8F0D09","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKJrOl63enWXCRxN0SoucliclBme0qrRb19ATrWIOIvibKIz8UAuVgicBMibIVUznerHnjotI4dm6ibODA/132","comment_is_top":false,"comment_ctime":1631843400,"is_pvip":false,"replies":[{"id":113597,"content":"不会的。因为切换的时候，旧的memtable依然存在，其他的读写操作依然能完成，不需要阻塞。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1632580157,"ip_address":"","comment_id":312497,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"请教老师一个问题:\n将内存可读写的table切换为 Immutable MemTable的时候会不会有阻塞操作，类似于gc中的stw，如果发生频繁是不是对性能也有影响。","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526988,"discussion_content":"不会的。因为切换的时候，旧的memtable依然存在，其他的读写操作依然能完成，不需要阻塞。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632580157,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282183,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1615123912,"is_pvip":false,"replies":[{"id":102783,"content":"一般来说，我们可以通过它们的文档和代码来学习这些系统。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1615714868,"ip_address":"","comment_id":282183,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"请教老师，levelDB中实现LSM的C0树是用跳表，这个是通过源码看的么？如果我想了解下其他使用LSM实现的系统，比如doris，druid之类，我怎么判断其他开源系统的C0树是用的什么结构实现？老师可以给个思路嘛","like_count":0},{"had_liked":false,"id":226269,"user_name":"扁舟","can_delete":false,"product_type":"c1","uid":1818287,"ip_address":"","ucode":"486C0D0FAB15C8","user_header":"https://static001.geekbang.org/account/avatar/00/1b/be/af/93e14e9d.jpg","comment_is_top":false,"comment_ctime":1592019412,"is_pvip":false,"replies":[{"id":83548,"content":"联想和对比可以帮助你更好地深入理解知识点。所以可以多练习一下，凡事试着多问几个为什么，这样慢慢你就能养成打破砂锅问到底的习惯了。当然，在学会多问为什么以后，你还可以试着去自己回答一些这些问题，这样就可以让你思考问题的能力更进一步。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1592221503,"ip_address":"","comment_id":226269,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"哈哈，这么多优质评论与老师一个一个问题的细心回答。看评论时，都会思考，我为什么没有进行这样的联想与对比，看来还是思考能力不强，以后看到知识点多想几个为什么。这样的文章真是看到就是赚到","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516659,"discussion_content":"一般来说，我们可以通过它们的文档和代码来学习这些系统。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615714868,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215084,"user_name":"xaviers","can_delete":false,"product_type":"c1","uid":1879918,"ip_address":"","ucode":"58D51C4DDC5BA8","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","comment_is_top":false,"comment_ctime":1588901606,"is_pvip":false,"replies":[{"id":79619,"content":"哈哈，凌晨更新是系统默认上线操作，是由极客时间的工作人员们负责的，他们辛苦了。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588924729,"ip_address":"","comment_id":215084,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师辛苦了，经常都是凌晨更新，终于等到这篇文章了😁","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494308,"discussion_content":"哈哈，凌晨更新是系统默认上线操作，是由极客时间的工作人员们负责的，他们辛苦了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588924729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215044,"user_name":"ykkk88","can_delete":false,"product_type":"c1","uid":1068585,"ip_address":"","ucode":"A1BCFC2F0D1022","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/29/1be3dd40.jpg","comment_is_top":false,"comment_ctime":1588896718,"is_pvip":false,"replies":[{"id":79617,"content":"你思考得很仔细，的确是的。sstable中，每一条记录都有一个标志位，表示是否是删除。这样就能避免误查询。\n对于有删除标志的记录，其实查询流程是一致的，就是查到数据就返回，不再往下查。然后看这个数据的状态位是有效还是删除，决定是否使用。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588924328,"ip_address":"","comment_id":215044,"utype":1}],"discussion_count":2,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师，如果是memtable有删除key的情况下，skiplist是不是设置墓碑标志，刷level 0的时候 sstable也还是有这个删除标记，只有在最下层的sstable合并时候再真的物理删除key啊，感觉不这么做，可能get时候会读出来已经被删除的key","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494290,"discussion_content":"你思考得很仔细，的确是的。sstable中，每一条记录都有一个标志位，表示是否是删除。这样就能避免误查询。\n对于有删除标志的记录，其实查询流程是一致的，就是查到数据就返回，不再往下查。然后看这个数据的状态位是有效还是删除，决定是否使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588924328,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261194,"discussion_content":"再补充一个细节，在进行合并的时候，对于带了删除key的记录，会通过一个isBaseLevelForKey函数来判断这个key是否存在于所有的下层的数据中，如果不存在，那么就可以删除","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588945939,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381715,"user_name":"yic","can_delete":false,"product_type":"c1","uid":1201577,"ip_address":"广东","ucode":"C8DC471B7C28B8","user_header":"https://static001.geekbang.org/account/avatar/00/12/55/a9/5282a560.jpg","comment_is_top":false,"comment_ctime":1695723622,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"&quot;而从 Level 1 开始，每一层的 SSTable 都做过了处理，这能保证覆盖范围不重合的。因此，对于同一层中的 SSTable，我们可以使用二分查找算法快速定位唯一的一个 SSTable 文件&quot;  -- 老师，关于这个点我有疑问还请帮忙解答：\n这句话描述的是一个终态吧？ 我理解一条比较大的数据A（预期应该放在L6层）写入了Level0层后，要到达L6层是需要一定的时间的。\n\n如果我上面的理解没有错误，那么当数据A到达了L3层时，用户过来查询，会是怎样的处理过程呢？","like_count":0},{"had_liked":false,"id":372969,"user_name":"ifelse","can_delete":false,"product_type":"c1","uid":2550743,"ip_address":"浙江","ucode":"D0565908C99695","user_header":"https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg","comment_is_top":false,"comment_ctime":1681879164,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":4,"product_id":100048401,"comment_content":"学习打卡","like_count":0},{"had_liked":false,"id":336836,"user_name":"berkin","can_delete":false,"product_type":"c1","uid":1583482,"ip_address":"","ucode":"6082E0A1470DD7","user_header":"https://static001.geekbang.org/account/avatar/00/18/29/7a/3c0fbf9c.jpg","comment_is_top":false,"comment_ctime":1646391314,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师，这里有一段话不太理解，\n“在多路归并生成第 n 层的 SSTable 文件时，LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度，如果重合覆盖度超过了 10 个文件，就结束这个 SSTable 的生成，继续生成下一个 SSTable 文件。”\n意思是说 第 n 层 新生成的 SSTable 与 n+1 层覆盖度超过 10 个文件 就会结束第 n层的 SSTable，意思是第 n 层的 SSTable 在生成的时候一直会和第 n+1 层的文件 key range 进行比较，如果覆盖超过 n+1 层\n就会开启另外一个 SSTable 写入n -1 层归并过来的数据吗？","like_count":0},{"had_liked":false,"id":306329,"user_name":"anker","can_delete":false,"product_type":"c1","uid":1218937,"ip_address":"","ucode":"6EDF1FB9D45238","user_header":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","comment_is_top":false,"comment_ctime":1628500342,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"想问一下Compaction过程中需要加锁吗？","like_count":0},{"had_liked":false,"id":304395,"user_name":"Geek_bd6gy9","can_delete":false,"product_type":"c1","uid":1236573,"ip_address":"","ucode":"39CC8E1E8EFB8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/de/5d/3a75c20b.jpg","comment_is_top":false,"comment_ctime":1627385160,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师，请教一下，“使用LSM树，对于大量的随机读，它无法在内存中命中，因此会去读磁盘，并且是一层一层地多次读磁盘，会带来很严重的读放大效应”，这里的读放大效应如何理解呢？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494290,"discussion_content":"你思考得很仔细，的确是的。sstable中，每一条记录都有一个标志位，表示是否是删除。这样就能避免误查询。\n对于有删除标志的记录，其实查询流程是一致的，就是查到数据就返回，不再往下查。然后看这个数据的状态位是有效还是删除，决定是否使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588924328,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":261194,"discussion_content":"再补充一个细节，在进行合并的时候，对于带了删除key的记录，会通过一个isBaseLevelForKey函数来判断这个key是否存在于所有的下层的数据中，如果不存在，那么就可以删除","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588945939,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301605,"user_name":"趁早","can_delete":false,"product_type":"c1","uid":1031970,"ip_address":"","ucode":"949FB3AA250D80","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bf/22/26530e66.jpg","comment_is_top":false,"comment_ctime":1625761721,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"你好，不理解为啥写入的时候sst会出现重叠？批量写入一般都是有序的吧，为啥会范围会重叠呢，那重叠之后多个sstable 在归并的时候是怎么判断哪个sst里面的数据是最新的需要被留下的","like_count":0},{"had_liked":false,"id":282183,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1615123912,"is_pvip":false,"replies":[{"id":102783,"content":"一般来说，我们可以通过它们的文档和代码来学习这些系统。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1615714868,"ip_address":"","comment_id":282183,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"请教老师，levelDB中实现LSM的C0树是用跳表，这个是通过源码看的么？如果我想了解下其他使用LSM实现的系统，比如doris，druid之类，我怎么判断其他开源系统的C0树是用的什么结构实现？老师可以给个思路嘛","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516659,"discussion_content":"一般来说，我们可以通过它们的文档和代码来学习这些系统。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615714868,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":226269,"user_name":"扁舟","can_delete":false,"product_type":"c1","uid":1818287,"ip_address":"","ucode":"486C0D0FAB15C8","user_header":"https://static001.geekbang.org/account/avatar/00/1b/be/af/93e14e9d.jpg","comment_is_top":false,"comment_ctime":1592019412,"is_pvip":false,"replies":[{"id":83548,"content":"联想和对比可以帮助你更好地深入理解知识点。所以可以多练习一下，凡事试着多问几个为什么，这样慢慢你就能养成打破砂锅问到底的习惯了。当然，在学会多问为什么以后，你还可以试着去自己回答一些这些问题，这样就可以让你思考问题的能力更进一步。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1592221503,"ip_address":"","comment_id":226269,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"哈哈，这么多优质评论与老师一个一个问题的细心回答。看评论时，都会思考，我为什么没有进行这样的联想与对比，看来还是思考能力不强，以后看到知识点多想几个为什么。这样的文章真是看到就是赚到","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498171,"discussion_content":"联想和对比可以帮助你更好地深入理解知识点。所以可以多练习一下，凡事试着多问几个为什么，这样慢慢你就能养成打破砂锅问到底的习惯了。当然，在学会多问为什么以后，你还可以试着去自己回答一些这些问题，这样就可以让你思考问题的能力更进一步。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592221503,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215084,"user_name":"xaviers","can_delete":false,"product_type":"c1","uid":1879918,"ip_address":"","ucode":"58D51C4DDC5BA8","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/6e/30fb83f1.jpg","comment_is_top":false,"comment_ctime":1588901606,"is_pvip":false,"replies":[{"id":79619,"content":"哈哈，凌晨更新是系统默认上线操作，是由极客时间的工作人员们负责的，他们辛苦了。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588924729,"ip_address":"","comment_id":215084,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师辛苦了，经常都是凌晨更新，终于等到这篇文章了😁","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498171,"discussion_content":"联想和对比可以帮助你更好地深入理解知识点。所以可以多练习一下，凡事试着多问几个为什么，这样慢慢你就能养成打破砂锅问到底的习惯了。当然，在学会多问为什么以后，你还可以试着去自己回答一些这些问题，这样就可以让你思考问题的能力更进一步。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592221503,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215044,"user_name":"ykkk88","can_delete":false,"product_type":"c1","uid":1068585,"ip_address":"","ucode":"A1BCFC2F0D1022","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/29/1be3dd40.jpg","comment_is_top":false,"comment_ctime":1588896718,"is_pvip":false,"replies":[{"id":79617,"content":"你思考得很仔细，的确是的。sstable中，每一条记录都有一个标志位，表示是否是删除。这样就能避免误查询。\n对于有删除标志的记录，其实查询流程是一致的，就是查到数据就返回，不再往下查。然后看这个数据的状态位是有效还是删除，决定是否使用。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588924328,"ip_address":"","comment_id":215044,"utype":1}],"discussion_count":2,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师，如果是memtable有删除key的情况下，skiplist是不是设置墓碑标志，刷level 0的时候 sstable也还是有这个删除标记，只有在最下层的sstable合并时候再真的物理删除key啊，感觉不这么做，可能get时候会读出来已经被删除的key","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494308,"discussion_content":"哈哈，凌晨更新是系统默认上线操作，是由极客时间的工作人员们负责的，他们辛苦了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588924729,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381715,"user_name":"yic","can_delete":false,"product_type":"c1","uid":1201577,"ip_address":"广东","ucode":"C8DC471B7C28B8","user_header":"https://static001.geekbang.org/account/avatar/00/12/55/a9/5282a560.jpg","comment_is_top":false,"comment_ctime":1695723622,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"&quot;而从 Level 1 开始，每一层的 SSTable 都做过了处理，这能保证覆盖范围不重合的。因此，对于同一层中的 SSTable，我们可以使用二分查找算法快速定位唯一的一个 SSTable 文件&quot;  -- 老师，关于这个点我有疑问还请帮忙解答：\n这句话描述的是一个终态吧？ 我理解一条比较大的数据A（预期应该放在L6层）写入了Level0层后，要到达L6层是需要一定的时间的。\n\n如果我上面的理解没有错误，那么当数据A到达了L3层时，用户过来查询，会是怎样的处理过程呢？","like_count":0},{"had_liked":false,"id":372969,"user_name":"ifelse","can_delete":false,"product_type":"c1","uid":2550743,"ip_address":"浙江","ucode":"D0565908C99695","user_header":"https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg","comment_is_top":false,"comment_ctime":1681879164,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":4,"product_id":100048401,"comment_content":"学习打卡","like_count":0},{"had_liked":false,"id":336836,"user_name":"berkin","can_delete":false,"product_type":"c1","uid":1583482,"ip_address":"","ucode":"6082E0A1470DD7","user_header":"https://static001.geekbang.org/account/avatar/00/18/29/7a/3c0fbf9c.jpg","comment_is_top":false,"comment_ctime":1646391314,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师，这里有一段话不太理解，\n“在多路归并生成第 n 层的 SSTable 文件时，LevelDB 会判断生成的 SSTable 和第 n+1 层的重合覆盖度，如果重合覆盖度超过了 10 个文件，就结束这个 SSTable 的生成，继续生成下一个 SSTable 文件。”\n意思是说 第 n 层 新生成的 SSTable 与 n+1 层覆盖度超过 10 个文件 就会结束第 n层的 SSTable，意思是第 n 层的 SSTable 在生成的时候一直会和第 n+1 层的文件 key range 进行比较，如果覆盖超过 n+1 层\n就会开启另外一个 SSTable 写入n -1 层归并过来的数据吗？","like_count":0},{"had_liked":false,"id":306329,"user_name":"anker","can_delete":false,"product_type":"c1","uid":1218937,"ip_address":"","ucode":"6EDF1FB9D45238","user_header":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","comment_is_top":false,"comment_ctime":1628500342,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"想问一下Compaction过程中需要加锁吗？","like_count":0},{"had_liked":false,"id":304395,"user_name":"Geek_bd6gy9","can_delete":false,"product_type":"c1","uid":1236573,"ip_address":"","ucode":"39CC8E1E8EFB8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/de/5d/3a75c20b.jpg","comment_is_top":false,"comment_ctime":1627385160,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"老师，请教一下，“使用LSM树，对于大量的随机读，它无法在内存中命中，因此会去读磁盘，并且是一层一层地多次读磁盘，会带来很严重的读放大效应”，这里的读放大效应如何理解呢？","like_count":0,"discussions":[{"author":{"id":1218937,"avatar":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","nickname":"anker","note":"","ucode":"6EDF1FB9D45238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387937,"discussion_content":"原本我只想读一个key，可能大概是几十个字节，但是找不到key会产生多次读的I/O，会造成可能几个KB的数据读取，此之谓读放大，写放大亦然","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628503120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":301605,"user_name":"趁早","can_delete":false,"product_type":"c1","uid":1031970,"ip_address":"","ucode":"949FB3AA250D80","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bf/22/26530e66.jpg","comment_is_top":false,"comment_ctime":1625761721,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100048401,"comment_content":"你好，不理解为啥写入的时候sst会出现重叠？批量写入一般都是有序的吧，为啥会范围会重叠呢，那重叠之后多个sstable 在归并的时候是怎么判断哪个sst里面的数据是最新的需要被留下的","like_count":0,"discussions":[{"author":{"id":1218937,"avatar":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","nickname":"anker","note":"","ucode":"6EDF1FB9D45238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":387937,"discussion_content":"原本我只想读一个key，可能大概是几十个字节，但是找不到key会产生多次读的I/O，会造成可能几个KB的数据读取，此之谓读放大，写放大亦然","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628503120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}