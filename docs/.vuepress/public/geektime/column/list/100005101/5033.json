{"id":5033,"title":"11 | Facebook是怎么为十亿人互相推荐好友的","content":"<p>上一篇中，我和你专门聊到了矩阵分解，在这篇文章的开始，我再为你回顾一下矩阵分解。</p>\n<h2>回顾矩阵分解</h2>\n<p>矩阵分解要将用户物品评分矩阵分解成两个小矩阵，一个矩阵是代表用户偏好的用户隐因子向量组成，另一个矩阵是代表物品语义主题的隐因子向量组成。</p>\n<p>这两个小矩阵相乘后得到的矩阵，维度和原来的用户物品评分矩阵一模一样。比如原来矩阵维度是m x n，其中m是用户数量，n是物品数量，再假如分解后的隐因子向量是k个，那么用户隐因子向量组成的矩阵就是m x k，物品隐因子向量组成的矩阵就是n x k。</p>\n<p>得到的这两个矩阵有这么几个特点：</p>\n<ol>\n<li>每个用户对应一个k维向量，每个物品也对应一个k维向量，就是所谓的隐因子向量，因为是无中生有变出来的，所以叫做“隐因子”；</li>\n<li>两个矩阵相乘后，就得到了任何一个用户对任何一个物品的预测评分，具体这个评分靠不靠谱，那就是看功夫了。</li>\n</ol>\n<p>所以矩阵分解，所做的事就是矩阵填充。那到底怎么填充呢，换句话也就是说两个小矩阵怎么得到呢？</p>\n<p>按照机器学习的套路，就是使用优化算法求解下面这个损失函数：</p>\n<p>$$ \\min_{q^{* },p^{* } } \\sum_{(u,i) \\in \\kappa }{(r_{ui} - p_{u}q_{i}^{T})^{2} + \\lambda (||q_{i}||^{2} + ||p_{u}||^{2})} $$</p>\n<p>这个公式依然由两部分构成：加号左边是误差平方和，加号右边是分解后参数的平方。</p>\n<p>这种模式可以套在几乎所有的机器学习训练中：就是一个负责衡量模型准不准，另一个负责衡量模型稳不稳定。行话是这样说的：一个衡量模型的偏差，一个衡量模型的方差。偏差大的模型欠拟合，方差大的模型过拟合。</p>\n<p>有了这个目标函数后，就要用到优化算法找到能使它最小的参数。优化方法常用的选择有两个，一个是随机梯度下降（SGD），另一个是交替最小二乘（ALS）。</p>\n<p>在实际应用中，交替最小二乘更常用一些，这也是社交巨头Facebook在他们的推荐系统中选择的主要矩阵分解方法，今天，我就专门聊一聊交替最小二乘求矩阵分解。</p>\n<h2>交替最小二乘原理 (ALS)</h2>\n<p>交替最小二乘的核心是交替，什么意思呢？你的任务是找到两个矩阵P和Q，让它们相乘后约等于原矩阵R：</p>\n<p>$$ R_{m \\times n} = P_{m \\times k} \\times Q^{T}_{n \\times k} $$</p>\n<p>难就难在，P和Q两个都是未知的，如果知道其中一个的话，就可以按照线性代数标准解法求得，比如如果知道了Q，那么P就可以这样算：</p>\n<p>$$ P_{m \\times k} = R_{m \\times n} \\times Q^{-1}_{n \\times k}$$</p>\n<p>也就是R矩阵乘以Q矩阵的逆矩阵就得到了结果。</p>\n<p>反之知道了P再求Q也一样。交替最小二乘通过迭代的方式解决了这个鸡生蛋蛋生鸡的难题：</p>\n<ol>\n<li>初始化随机矩阵Q里面的元素值；</li>\n<li>把Q矩阵当做已知的，直接用线性代数的方法求得矩阵P；</li>\n<li>得到了矩阵P后，把P当做已知的，故技重施，回去求解矩阵Q；</li>\n<li>上面两个过程交替进行，一直到误差可以接受为止。</li>\n</ol>\n<p>你看吧，机器就是这么单纯善良，先用一个假的结果让算法先运转起来，然后不断迭代最终得到想要的结果。这和做互联网C2C平台的思路也一样，告诉买家说：快来这里，我们是万能的，什么都能买到！</p>\n<p>买家来了后又去告诉卖家们说：快来这里开店，我这里掌握了最多的剁手党。嗯，雪球就这样滚出来了。</p>\n<p>交替最小二乘有这么几个好处：</p>\n<ol>\n<li>在交替的其中一步，也就是假设已知其中一个矩阵求解另一个时，要优化的参数是很容易并行化的；</li>\n<li>在不那么稀疏的数据集合上，交替最小二乘通常比随机梯度下降要更快地得到结果，事实上这一点就是我马上要说的，也就是关于隐式反馈的内容。</li>\n</ol>\n<!-- [[[read_end]]] -->\n<h2>隐式反馈</h2>\n<p>矩阵分解算法，是为解决评分预测问题而生的，比如说，预测用户会给商品打几颗星，然后把用户可能打高星的商品推荐给用户，然而事实上却是，用户首先必须先去浏览商品，然后是购买，最后才可能打分。</p>\n<p>相比“预测用户会打多少分”，“预测用户会不会去浏览”更加有意义，而且，用户浏览数据远远多于打分评价数据。也就是说，实际上推荐系统关注的是预测行为，行为也就是一再强调的隐式反馈。</p>\n<p>那如何从解决评分预测问题转向解决预测行为上来呢？这就是另一类问题了，行话叫做One-Class。</p>\n<p>这是什么意思呢？如果把预测用户行为看成一个二分类问题，猜用户会不会做某件事，但实际上收集到的数据只有明确的一类：用户干了某件事，而用户明确“不干”某件事的数据却没有明确表达。所以这就是One-Class的由来，One-Class数据也是隐式反馈的通常特点。</p>\n<p>对隐式反馈的矩阵分解，需要将交替最小二乘做一些改进，改进后的算法叫做加权交替最小二乘：Weighted-ALS。</p>\n<p>这个加权要从哪说起？用户对物品的隐式反馈，通常是可以多次的，你有心心念念的衣服或者电子产品，但是刚刚剁完手的你正在吃土买不起，只能每天去看一眼。</p>\n<p>这样一来，后台就记录了你查看过这件商品多少次，查看次数越多，就代表你越喜欢这个。也就是说，行为的次数是对行为的置信度反应，也就是所谓的加权。</p>\n<p>加权交替最小二乘这样对待隐式反馈：</p>\n<ol>\n<li>如果用户对物品无隐式反馈则认为评分是0；</li>\n<li>如果用户对物品有至少一次隐式反馈则认为评分是1，次数作为该评分的置信度。</li>\n</ol>\n<p>那现在的目标函数在原来的基础上变成这样：</p>\n<p>$$ \\min_{q^{* },p^{* } } \\sum_{(u,i) \\in \\kappa }{c_{ui}(r_{ui} - p_{u}q_{i}^{T})^{2} + \\lambda (||q_{i}||^{2} + ||p_{u}||^{2})} $$</p>\n<p>多出来的Cui就是置信度，在计算误差时考虑反馈次数，次数越多，就越可信。置信度一般也不是直接等于反馈次数，根据一些经验，置信度Cui这样计算：</p>\n<p>$$ c_{ui} = 1 + \\alpha C $$</p>\n<p>其中阿尔法是一个超参数，需要调教，默认值取40可以得到差不多的效果，C就是次数了。</p>\n<p>这里又引出另一个问题，那些没有反馈的缺失值，就是在我们的设定下，取值为0的评分就非常多，有两个原因导致在实际使用时要注意这个问题：</p>\n<ol>\n<li>本身隐式反馈就只有正类别是确定的，负类别是我们假设的，你要知道，One-Class并不是随便起的名字；</li>\n<li>这会导致正负类别样本非常不平衡，严重倾斜到0评分这边。</li>\n</ol>\n<p>因此，不能一股脑儿使用所有的缺失值作为负类别，矩阵分解的初心就是要填充这些值，如果都假设他们为0了，那就忘记初心了。应对这个问题的做法就是负样本采样：挑一部分缺失值作为负类别样本即可。</p>\n<p>怎么挑？有两个方法：</p>\n<ol>\n<li>随机均匀采样和正类别一样多；</li>\n<li>按照物品的热门程度采样。</li>\n</ol>\n<p>请允许我直接说结论，第一种不是很靠谱，第二种在实践中经过了检验。</p>\n<p>还是回到初心来，你想一想，在理想情况下，什么样的样本最适合做负样本？</p>\n<p>就是展示给用户了，他也知道这个物品的存在了，但就是没有对其作出任何反馈。问题就是很多时候不知道到底是用户没有意识到物品的存在呢，还是知道物品的存在而不感兴趣呢？</p>\n<p>因此按照物品热门程度采样的思想就是：一个越热门的物品，用户越可能知道它的存在。那这种情况下，用户还没对它有反馈就表明：这很可能就是真正的负样本。</p>\n<p>按照热门程度采样来构建负样本，在实际中是一个很常用的技巧，我之前和你提到的文本算法Word2Vec学习过程，也用到了类似的负样本采样技巧。</p>\n<h2>推荐计算</h2>\n<p>在得到了分解后的矩阵后，相当于每个用户得到了隐因子向量，这是一个稠密向量，用于代表他的兴趣。同时每个物品也得到了一个稠密向量，代表它的语义或主题。而且可以认为这两者是一一对应的，用户的兴趣就是表现在物品的语义维度上的。</p>\n<p>看上去，让用户和物品的隐因子向量两两相乘，计算点积就可以得到所有的推荐结果了。但是实际上复杂度还是很高，尤其对于用户数量和物品数量都巨大的应用，如Facebook，就更不现实。于是Facebook提出了两个办法得到真正的推荐结果。</p>\n<p>第一种，利用一些专门设计的数据结构存储所有物品的隐因子向量，从而实现通过一个用户向量可以返回最相似的K个物品。</p>\n<p>Facebook给出了自己的开源实现Faiss，类似的开源实现还有Annoy，KGraph，NMSLIB。</p>\n<p>其中Facebook开源的Faiss 和NMSLIB（Non-Metric Space Library）都用到了ball tree来存储物品向量。</p>\n<p>如果需要动态增加新的物品向量到索引中，推荐使用Faiss，如果不是，推荐使用NMSLIB或者KGraph。用户向量则可以存在内存数据中，这样可以在用户访问时，实时产生推荐结果。</p>\n<p>第二种，就是拿着物品的隐因子向量先做聚类，海量的物品会减少为少量的聚类。然后再逐一计算用户和每个聚类中心的推荐分数，给用户推荐物品就变成了给用户推荐物品聚类。</p>\n<p>得到给用户推荐的聚类后，再从每个聚类中挑选少许几个物品作为最终推荐结果。这样做的好处除了大大减小推荐计算量之外，还可以控制推荐结果的多样性，因为可以控制在每个类别中选择的物品数量。</p>\n<h2>总结</h2>\n<p>在真正的推荐系统的实际应用中，评分预测实际上场景很少，而且数据也很少。因此，相比预测评分，预测“用户会对物品干出什么事”，会更加有效。</p>\n<p>然而这就需要对矩阵分解做一些改进，加权交替最小二乘就是改进后的矩阵分解算法，被Facebook采用在了他们的推荐系统中，这篇文章里，我也详细地解释了这一矩阵分解算法在落地时的步骤和注意事项。</p>\n<p>其中，我和你提到了针对One-Class这种数据集合，一种常用的负样本构建方法是根据物品的热门程度采样，你能想到还有哪些负样本构建方法吗？欢迎留言一起讨论。感谢你的收听，我们下次再见。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/87/b0/873b086966136189db14874181823fb0.jpg?wh=1110*549\" alt=\"\" /></p>\n","neighbors":{"left":{"article_title":"10 | 那些在Netflix Prize中大放异彩的推荐算法","id":5030},"right":{"article_title":"12 | 如果关注排序效果，那么这个模型可以帮到你","id":5055}},"comments":[{"had_liked":false,"id":4559,"user_name":"瑞雪","can_delete":false,"product_type":"c1","uid":1064286,"ip_address":"","ucode":"136150F666EC3F","user_header":"https://static001.geekbang.org/account/avatar/00/10/3d/5e/75563f01.jpg","comment_is_top":false,"comment_ctime":1522219362,"is_pvip":false,"discussion_count":3,"race_medal":0,"score":"44471892322","product_id":100005101,"comment_content":"你好，请问如果选取一部分为负样本，其他的缺失值在矩阵分解时是直接使用NaN吗，有点对正负样本分不太清:D","like_count":10,"discussions":[{"author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7640,"discussion_content":"我觉得不是真的把用户物品矩阵建立起来了，那样矩阵太大，只保存存在用户物品连接的数据，然后负采样从没有连接的那些中选择合适的方法进行挑选，其他的缺失值也就无所谓了，因为本来就没有真的在内存中的用户物品矩阵","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1567597234,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1064286,"avatar":"https://static001.geekbang.org/account/avatar/00/10/3d/5e/75563f01.jpg","nickname":"瑞雪","note":"","ucode":"136150F666EC3F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":20159,"discussion_content":"恩，现在知道了，以前觉得矩阵分解都要用svd的，太菜了😷","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569285233,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":7640,"ip_address":""},"score":20159,"extra":""},{"author":{"id":1343949,"avatar":"","nickname":"赖春苹","note":"","ucode":"1B637D46549A21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":34956,"discussion_content":"SGD求解确实不需要填充矩阵，只需要用存在连接的用户物品计算就好了。可是ALS不是用到了矩阵相乘、求逆之类的么~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571228906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":7640,"ip_address":""},"score":34956,"extra":""}]}]},{"had_liked":false,"id":4561,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1522221547,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"22997058027","product_id":100005101,"comment_content":"1. 既然可以根据物品的热门程度选择负样本，是不是类似也可以根据用户的活跃程度选择负样本?<br>2. 是不是可以借鉴之前基于内容推荐的方法，先找出和当前用户或场景类似内容的用户或场景中的热门物品的交互作为负样本？这里用户或场景可以用各种距离度量的方式选出k个最相邻的。基于内容相似度找和正样本最相邻的物品作为负样本可能也可以。<br>3. 负样本从概率分布中采样，概率分布的参数让整体的期望值和真实值尽可能接近。这样交互次数多的有更大概率被选中，或者可以看成赋予了更大权重。<br>4. 引入一个概率参数变量，有交互的概率为p(i, j)，预测交互值为1；无交互的概率为p(i, j)，预测交互值为0。除了计算用户和物品隐变量外，把用户和物品隐变量固定后再估算这个概率参数。","like_count":5},{"had_liked":false,"id":158823,"user_name":"neohope","can_delete":false,"product_type":"c1","uid":1043475,"ip_address":"","ucode":"C0268F6E7E2B6E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/13/49e98289.jpg","comment_is_top":false,"comment_ctime":1575459243,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"10165393835","product_id":100005101,"comment_content":"负样本构建方法：<br>1、通过页面或APP曝光，用户没有反应的<br>2、告知用户有优惠活动或优惠券，用户无动于衷的<br>3、根据用户画像，以及物品标签，用户不需要的物品<br>4、用户明确标记不感兴趣的物品或不认识的人<br>5、普遍有差评的人或物品","like_count":3},{"had_liked":false,"id":48040,"user_name":"王掌柜家的小二","can_delete":false,"product_type":"c1","uid":1124321,"ip_address":"","ucode":"FBAA0760FC8AD7","user_header":"https://static001.geekbang.org/account/avatar/00/11/27/e1/cbedb320.jpg","comment_is_top":false,"comment_ctime":1544328578,"is_pvip":false,"replies":[{"id":"19169","content":"交替最小二乘，“最小二乘”就不是确定解，而是数值优化解，所以不存在你说的这种情况。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1545489497,"ip_address":"","comment_id":48040,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10134263170","product_id":100005101,"comment_content":"有个问题没想明白的，上网找了下也没明白的：在交替最小二乘法的原理中，既然已经是随机初始化了矩阵P，求得Q就是一个确定的结果了，那么这时候用Q反过来求P的意义何在呢？得到不也是同一个P吗？既然两个值是确定的，又何来迭代一说？<br>知道自己理解的肯定有问题，忘老师回复。","like_count":3,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":431998,"discussion_content":"交替最小二乘，“最小二乘”就不是确定解，而是数值优化解，所以不存在你说的这种情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545489497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2547259,"avatar":"https://static001.geekbang.org/account/avatar/00/26/de/3b/c15d2d9f.jpg","nickname":"DAMIAN","note":"","ucode":"37A7C27DBDBCC5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":383578,"discussion_content":"本质是EM算法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626166185,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7645,"discussion_content":"我在网上查的是固定一个然后，损失函数对另外那个求导，令其导数为零，得到结果，在假设得到这个是正确的，在求导","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567597989,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":5422,"user_name":"yalei","can_delete":false,"product_type":"c1","uid":1057482,"ip_address":"","ucode":"DD512C0CF990DB","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/ca/8d103e9a.jpg","comment_is_top":false,"comment_ctime":1523324219,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10113258811","product_id":100005101,"comment_content":"通常用户需要一个“入口”才能浏览商品详情，这个入口可大部分情况下是搜索结果和算法推荐。可以设置曝光埋点，再结合点击埋点来找到真正的负样本（有曝光而无点击的样本）","like_count":3},{"had_liked":false,"id":4534,"user_name":"Drxan","can_delete":false,"product_type":"c1","uid":1057325,"ip_address":"","ucode":"96FB51264DBD21","user_header":"","comment_is_top":false,"comment_ctime":1522197285,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10112131877","product_id":100005101,"comment_content":"大神，如果要对负样本进行采样的话，是不是就无法用矩阵分解了","like_count":2},{"had_liked":false,"id":90394,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1556508670,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5851475966","product_id":100005101,"comment_content":"负样本采样最最精准的应该是曝光而无行为的那些用户sku对，即对该用户来说该sku曝光了，但是用户对该sku没有行为，但是这个量会很大，可以从中抽样得到负样本","like_count":1},{"had_liked":false,"id":62055,"user_name":"cl","can_delete":false,"product_type":"c1","uid":1369841,"ip_address":"","ucode":"F6D56054EE7B3A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq7G29kbKiaJKMDClWibNjMb7BzHEVXRicfBUbW1icyx7TjZPcXFgUGrquqfzPDepWQX7YSdkuYlf9ZeA/132","comment_is_top":false,"comment_ctime":1547896861,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5842864157","product_id":100005101,"comment_content":"还是没有明白，矩阵分解前提是需要分数矩阵，针对没有评分体系的应用从隐式反馈数据中构建这个分数矩阵，就是上一章遗留的问题，能否结合实例说一下？？","like_count":1,"discussions":[{"author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7646,"discussion_content":"没有评分体系，为什么要构建分数矩阵啊，就是构建隐式反馈矩阵吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567598065,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4905,"user_name":"森林","can_delete":false,"product_type":"c1","uid":1071920,"ip_address":"","ucode":"53BD05633B77B6","user_header":"https://static001.geekbang.org/account/avatar/00/10/5b/30/9dcf35ff.jpg","comment_is_top":false,"comment_ctime":1522650203,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5817617499","product_id":100005101,"comment_content":"目标函数里置信度C是1+aC，如果我们挑负样本的话，负样本的次数是啥？","like_count":1},{"had_liked":false,"id":4529,"user_name":"曾阿牛","can_delete":false,"product_type":"c1","uid":1049468,"ip_address":"","ucode":"29A294174EF06B","user_header":"https://static001.geekbang.org/account/avatar/00/10/03/7c/39ea8a23.jpg","comment_is_top":false,"comment_ctime":1522169755,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5817137051","product_id":100005101,"comment_content":"在构建点击率预估模型时，仅将正样本附近未点击的样本视为负样本。样本量大时，剔除一段时间内没有转化行为的用户数据（包括正负样本）","like_count":1},{"had_liked":false,"id":286555,"user_name":"当下","can_delete":false,"product_type":"c1","uid":2536298,"ip_address":"","ucode":"F3A63634B1F5FB","user_header":"https://static001.geekbang.org/account/avatar/00/26/b3/6a/eb6f9010.jpg","comment_is_top":false,"comment_ctime":1617372731,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1617372731","product_id":100005101,"comment_content":"请问怎么确定负样本个数，正负样本的比例多少合适？","like_count":0},{"had_liked":false,"id":285501,"user_name":"陈威洋","can_delete":false,"product_type":"c1","uid":2264679,"ip_address":"","ucode":"DCF84B4D3A7354","user_header":"https://static001.geekbang.org/account/avatar/00/22/8e/67/afb412fb.jpg","comment_is_top":false,"comment_ctime":1616847759,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1616847759","product_id":100005101,"comment_content":"居然能够想到物品向量做聚类，然后逐一计算用户与每个类的推荐分数，这种方法，我大吃一惊！~~","like_count":0},{"had_liked":false,"id":241759,"user_name":"山药","can_delete":false,"product_type":"c1","uid":1064167,"ip_address":"","ucode":"A1EA25B081617C","user_header":"https://static001.geekbang.org/account/avatar/00/10/3c/e7/eb5aea59.jpg","comment_is_top":false,"comment_ctime":1597413337,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1597413337","product_id":100005101,"comment_content":"Faiss用的不是ball tree吧，是Product Quantization吧","like_count":0},{"had_liked":false,"id":205567,"user_name":"白木其尔_Grace","can_delete":false,"product_type":"c1","uid":1234743,"ip_address":"","ucode":"531E02B83A292F","user_header":"https://static001.geekbang.org/account/avatar/00/12/d7/37/a14bffa6.jpg","comment_is_top":false,"comment_ctime":1586675481,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1586675481","product_id":100005101,"comment_content":"矩阵初始化是否是常规的初始化方法（Q&#47;|Q|），您的初始化方法是什么？","like_count":0},{"had_liked":false,"id":154318,"user_name":"夜雨声烦","can_delete":false,"product_type":"c1","uid":1349749,"ip_address":"","ucode":"87D8DB1E32522A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6mh3xlaMoGtWjmVJh2LutdLcQcPbKNjRlVru3bx8ynPhgwuGhhdzTkwEMoXbvBtgkcDSfom1kZg/132","comment_is_top":false,"comment_ctime":1574408500,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1574408500","product_id":100005101,"comment_content":"c2c平台的例子太形象了 ..","like_count":0},{"had_liked":false,"id":150991,"user_name":"陈朋","can_delete":false,"product_type":"c1","uid":1741323,"ip_address":"","ucode":"1174816CFAF5FA","user_header":"https://static001.geekbang.org/account/avatar/00/1a/92/0b/c86a7500.jpg","comment_is_top":false,"comment_ctime":1573639195,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1573639195","product_id":100005101,"comment_content":"ALS算法没有讲好，连接https:&#47;&#47;blog.csdn.net&#47;antkillerfarm&#47;article&#47;details&#47;53734658","like_count":0},{"had_liked":false,"id":119626,"user_name":"lazytortoise","can_delete":false,"product_type":"c1","uid":1502735,"ip_address":"","ucode":"F597FB1F7C089C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL06DePazHRU24iaGubbg5XOhvYQTkHRrItic9AvbZ24cjgFqfy620u9pOIoGu8FlK61oicEh8cGfHCw/132","comment_is_top":false,"comment_ctime":1564640668,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1564640668","product_id":100005101,"comment_content":"老师，加权的最小二乘公式中 rui 是不是 pui？ 是rui下的二元变量，当rui&gt;0时候，pui=1 ,当rui=0,pui=0.","like_count":0},{"had_liked":false,"id":119310,"user_name":"赖春苹","can_delete":false,"product_type":"c1","uid":1343949,"ip_address":"","ucode":"1B637D46549A21","user_header":"","comment_is_top":false,"comment_ctime":1564558987,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564558987","product_id":100005101,"comment_content":"有个很不明白的地方就是，隐式反馈的代价函数第一项，也是均方误差么？均方误差一般用于评分预测这种回归问题吧？隐式反馈对应的“点击”、“收藏”、“加购物车”这类的操作不是有限个状态么，不应该是分类问题吗？","like_count":0},{"had_liked":false,"id":103772,"user_name":"real","can_delete":false,"product_type":"c1","uid":1339367,"ip_address":"","ucode":"972051F534E51F","user_header":"https://static001.geekbang.org/account/avatar/00/14/6f/e7/be5d09ae.jpg","comment_is_top":false,"comment_ctime":1560507208,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1560507208","product_id":100005101,"comment_content":"这个 根本就不是矩阵分解 好伐。你可以理解为 userid 和itemid的embedding。实现组织为onehot，在embedding到低纬度。两个field的embedding就对应的 p u，然后去采样。","like_count":0,"discussions":[{"author":{"id":1514305,"avatar":"https://static001.geekbang.org/account/avatar/00/17/1b/41/dbb7d785.jpg","nickname":"xk_","note":"","ucode":"DFE1AC38EA78A7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":89879,"discussion_content":"tqnb","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1576763765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7647,"discussion_content":"我觉得看你怎么理解这个问题了，按你的意思是没问题的，就是embedding用户和物品的向量，再进一步计算评分、行为等\n当然也可以看作是两个隐向量，乘起来就是用户物品矩阵","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567598477,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":92231,"user_name":"戏入蝶衣","can_delete":false,"product_type":"c1","uid":1304500,"ip_address":"","ucode":"F6378C86A90DBC","user_header":"https://static001.geekbang.org/account/avatar/00/13/e7/b4/9a6c44cd.jpg","comment_is_top":false,"comment_ctime":1557208339,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1557208339","product_id":100005101,"comment_content":"老师，上一堂课讲到的预测评分的svd模型不需要负样本，为什么行为预测套用svd就需要负样本呢？如果我们只在用户有过行为的样本上训练模型，会有什么疏忽呢？","like_count":0,"discussions":[{"author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7648,"discussion_content":"只在正样本上训练，那模型最终不就倾向于全都给正，因为训练的时候都没有负样本，模型就会认为我都给正是没问题的，但是实际上大部分应该是负的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567598611,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1667945,"avatar":"","nickname":"Geek_7eedcf","note":"","ucode":"387C8B47126DE4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1649685,"avatar":"https://static001.geekbang.org/account/avatar/00/19/2c/15/5f30a666.jpg","nickname":"看那雨","note":"","ucode":"652641FCAFFB17","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":37582,"discussion_content":"不太明白的地方就是：矩阵分解选取负样本，用户和物品的行为矩阵，有行为的为1，没有行为的为0，与选择负样本填充为0有什么区别？只是减少了矩阵中的0元素吗？如果选择负样本，矩阵中的正样本是不是也相应的减少了，只是与负样本对应的物品的正样本的元素？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571640428,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":7648,"ip_address":""},"score":37582,"extra":""}]}]},{"had_liked":false,"id":90396,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1556508854,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556508854","product_id":100005101,"comment_content":"老师，我理解矩阵分解中，应该是我们对于其中好多元素是未知的，这个未知不代表为0，而负样本其实对应的矩阵中的元素应该确认为0或者定义的负数是么？我们矩阵分解的目的是把矩阵中未知的元素计算出来","like_count":0},{"had_liked":false,"id":90395,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1556508736,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556508736","product_id":100005101,"comment_content":"老师，您好，在您介绍算法的时候是否也推荐下其对应的python或者spark的包，方便我们实践起来","like_count":0},{"had_liked":false,"id":74754,"user_name":"无隅","can_delete":false,"product_type":"c1","uid":1058801,"ip_address":"","ucode":"756E8D6E915FF0","user_header":"https://static001.geekbang.org/account/avatar/00/10/27/f1/e4fc57a3.jpg","comment_is_top":false,"comment_ctime":1552292612,"is_pvip":true,"replies":[{"id":"27312","content":"并不是。你这昵称是故意的吗？","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1552322529,"ip_address":"","comment_id":74754,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1552292612","product_id":100005101,"comment_content":"无反馈的样本评分是0，然后被采样到的负样本评分，设为-1吗？","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442667,"discussion_content":"并不是。你这昵称是故意的吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1552322529,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1058801,"avatar":"https://static001.geekbang.org/account/avatar/00/10/27/f1/e4fc57a3.jpg","nickname":"无隅","note":"","ucode":"756E8D6E915FF0","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":316566,"discussion_content":"嗯？昵称怎么了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603426793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":9595,"user_name":"易初","can_delete":false,"product_type":"c1","uid":1130484,"ip_address":"","ucode":"299EBE4F4B3AB5","user_header":"https://static001.geekbang.org/account/avatar/00/11/3f/f4/c3b50e6c.jpg","comment_is_top":false,"comment_ctime":1526976372,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1526976372","product_id":100005101,"comment_content":"用户 和 物品 是一个pair ， 用dssm 深度语意匹配网络是不是更好","like_count":0},{"had_liked":false,"id":4774,"user_name":"哎哎哎","can_delete":false,"product_type":"c1","uid":1074993,"ip_address":"","ucode":"E33B808F79979D","user_header":"https://static001.geekbang.org/account/avatar/00/10/67/31/0fbadaff.jpg","comment_is_top":false,"comment_ctime":1522430431,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1522430431","product_id":100005101,"comment_content":"用户向量存在内存中的话，如何存的下呢？还有就是这种方式对用户向量的更新如果不是实时的话，是否线上服务的用户miss会很高啊，这种情况应该怎么处理呢？","like_count":0},{"had_liked":false,"id":4535,"user_name":"Drxan","can_delete":false,"product_type":"c1","uid":1057325,"ip_address":"","ucode":"96FB51264DBD21","user_header":"","comment_is_top":false,"comment_ctime":1522197296,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1522197296","product_id":100005101,"comment_content":"大神，如果要对负样本进行采样的话，是不是就无法用矩阵分解了","like_count":0}]}