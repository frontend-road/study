{"id":94410,"title":"12 | 我们为什么需要Spark？","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“我们为什么需要Spark”。</p><p>也许你之前没有做过大规模数据处理的项目，但是Spark这个词我相信你一定有所耳闻。</p><p>Spark是当今最流行的分布式大规模数据处理引擎，被广泛应用在各类大数据处理场景。</p><p>2009年，美国加州大学伯克利分校的AMP实验室开发了Spark。2013年，Spark成为Apache软件基金会旗下的孵化项目。</p><p>而现在，Spark已经成为了该基金会管理的项目中最活跃的一个。Spark社区也是成长迅速，不仅有数以千计的个人贡献者在不断地开发维护，还有很多大公司也加入了这个开源项目，如Databricks、IBM和华为。</p><p>在技术不断高速更迭的程序圈，一个新工具的出现与流行，必然是因为它满足了很大一部分人长期未被满足的需求，或是解决了一个长期让很多人难受的痛点。</p><p>所以，在学一个新技术之前，你有必要先了解这门技术出现的意义。这样，你才能更好地理解：它是应用到什么场景的？与同类工具相比，它的优缺点是什么？什么时候用它比其它工具好（或差）？……</p><p>至少理解了这些，你才好说自己是真正掌握了这个工具，否则只能说是浅尝辄止，半生不熟。</p><p>学习Spark同样是如此。</p><!-- [[[read_end]]] --><p>我们首先要问自己，既然已经有了看似很成熟的Hadoop和MapReduce，为什么我们还需要Spark？它能帮我们解决什么实际问题？相比于MapReduce，它的优势又是什么？</p><h2>MapReduce的缺陷</h2><p>MapReduce通过简单的Map和Reduce的抽象提供了一个编程模型，可以在一个由上百台机器组成的集群上并发处理大量的数据集，而把计算细节隐藏起来。各种各样的复杂数据处理都可以分解为Map或Reduce的基本元素。</p><p>这样，复杂的数据处理可以分解为由多个Job（包含一个Mapper和一个Reducer）组成的有向无环图（DAG），然后每个Mapper和Reducer放到Hadoop集群上执行，就可以得出结果。</p><p>我们在第一讲中讲到过MapReduce被硅谷一线公司淘汰的两大主要原因：高昂的维护成本、时间性能“达不到”用户的期待。不过除此之外，MapReduce还存在诸多局限。</p><p>第一，MapReduce模型的抽象层次低，大量的底层逻辑都需要开发者手工完成。</p><p>打个比方，写MapReduce的应用就好比用汇编语言去编写一个复杂的游戏。如果你是开发者，你会习惯用汇编语言，还是使用各种高级语言如Java、C++的现有框架呢？</p><p>第二，只提供Map和Reduce两个操作。</p><p>很多现实的数据处理场景并不适合用这个模型来描述。实现复杂的操作很有技巧性，也会让整个工程变得庞大以及难以维护。</p><p>举个例子，两个数据集的Join是很基本而且常用的功能，但是在MapReduce的世界中，需要对这两个数据集做一次Map和Reduce才能得到结果。这样框架对于开发者非常不友好。正如第一讲中提到的，维护一个多任务协调的状态机成本很高，而且可扩展性非常差。</p><p>第三，在Hadoop中，每一个Job的计算结果都会存储在HDFS文件存储系统中，所以每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟。</p><p>由于这一原因，MapReduce对于迭代算法的处理性能很差，而且很耗资源。因为迭代的每一步都要对HDFS进行读写，所以每一步都需要差不多的等待时间。</p><p>第四，只支持批数据处理，欠缺对流数据处理的支持。</p><p>因此，在Hadoop推出后，有很多人想办法对Hadoop进行优化，其中发展到现在最成熟的就是Spark。</p><p>接下来，就让我们看一下Spark是如何对上述问题进行优化的。</p><h2>Spark的优势</h2><p>Spark最基本的数据抽象叫作弹性分布式数据集（Resilient Distributed Dataset, RDD），它代表一个可以被分区（partition）的只读数据集，它内部可以有很多分区，每个分区又有大量的数据记录（record）。</p><p>RDD是Spark最基本的数据结构。Spark定义了很多对RDD的操作。对RDD的任何操作都可以像函数式编程中操作内存中的集合一样直观、简便，使得实现数据处理的代码非常简短高效。这些我们会在这一模块中的后续文章中仔细阐述。</p><p>Spark提供了很多对RDD的操作，如Map、Filter、flatMap、groupByKey和Union等等，极大地提升了对各种复杂场景的支持。开发者既不用再绞尽脑汁挖掘MapReduce模型的潜力，也不用维护复杂的MapReduce状态机。</p><p>相对于Hadoop的MapReduce会将中间数据存放到硬盘中，Spark会把中间数据缓存在内存中，从而减少了很多由于硬盘读写而导致的延迟，大大加快了处理速度。</p><p>Databricks团队曾经做过一个实验，他们用Spark排序一个100TB的静态数据集仅仅用时23分钟。而之前用Hadoop做到的最快记录也用了高达72分钟。此外，Spark还只用了Hadoop所用的计算资源的1/10，耗时只有Hadoop的1/3。</p><p>这个例子充分体现出Spark数据处理的最大优势——速度。</p><p>在某些需要交互式查询内存数据的场景中，Spark的性能优势更加明显。</p><p>根据Databricks团队的结果显示，Spark的处理速度是Hadoop的100倍。即使是对硬盘上的数据进行处理，Spark的性能也达到了Hadoop的10倍。</p><p>由于Spark可以把迭代过程中每一步的计算结果都缓存在内存中，所以非常适用于各类迭代算法。</p><p>Spark第一次启动时需要把数据载入到内存，之后的迭代可以直接在内存里利用中间结果做不落地的运算。所以，后期的迭代速度快到可以忽略不计。在当今机器学习和人工智能大热的环境下，Spark无疑是更好的数据处理引擎。</p><p>下图是在Spark和Hadoop上运行逻辑回归算法的运行时间对比。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/3d/54e4df946206a4a2168a25af8814843d.png?wh=5083*3500\" alt=\"\"></p><p>可以看出，Hadoop做每一次迭代运算的时间基本相同，而Spark除了第一次载入数据到内存以外，别的迭代时间基本可以忽略。</p><p>在任务（task）级别上，Spark的并行机制是多线程模型，而MapReduce是多进程模型。</p><p>多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间。</p><p>而Spark同一节点上的任务以多线程的方式运行在一个JVM进程中，可以带来更快的启动速度、更高的CPU利用率，以及更好的内存共享。</p><p>从前文中你可以看出，Spark作为新的分布式数据处理引擎，对MapReduce进行了很多改进，使得性能大大提升，并且更加适用于新时代的数据处理场景。</p><p>但是，Spark并不是一个完全替代Hadoop的全新工具。</p><p>因为Hadoop还包含了很多组件：</p><ul>\n<li>数据存储层：分布式文件存储系统HDFS，分布式数据库存储的HBase；</li>\n<li>数据处理层：进行数据处理的MapReduce，负责集群和资源管理的YARN；</li>\n<li>数据访问层：Hive、Pig、Mahout……</li>\n</ul><p>从狭义上来看，Spark只是MapReduce的替代方案，大部分应用场景中，它还要依赖于HDFS和HBase来存储数据，依赖于YARN来管理集群和资源。</p><p>当然，Spark并不是一定要依附于Hadoop才能生存，它还可以运行在Apache Mesos、Kubernetes、standalone等其他云平台上。</p><p><img src=\"https://static001.geekbang.org/resource/image/bc/0c/bc01239280bb853ca1d00c0fb3a8150c.jpg?wh=3604*2329\" alt=\"\"></p><p>此外，作为通用的数据处理平台，Spark有五个主要的扩展库，分别是支持结构化数据的Spark SQL、处理实时数据的Spark Streaming、用于机器学习的MLlib、用于图计算的GraphX、用于统计分析的SparkR。</p><p>这些扩展库与Spark核心API高度整合在一起，使得Spark平台可以广泛地应用在不同数据处理场景中。</p><h2>小结</h2><p>通过今天的学习，我们了解了Spark相较于MapReduce的主要优势，那就是快、易于开发及维护，和更高的适用性。我们还初步掌握了Spark系统的架构。</p><p>MapReduce作为分布式数据处理的开山鼻祖，虽然有很多缺陷，但是它的设计思想不仅没有过时，而且还影响了新的数据处理系统的设计，如Spark、Storm、Presto、Impala等。</p><p>Spark并没有全新的理论基础，它是一点点地在工程和学术的结合基础上做出来的。可以说，它站在了Hadoop和MapReduce两个巨人的肩膀上。在这一模块中，我们会对Spark的架构、核心概念、API以及各个扩展库进行深入的讨论，并且结合常见的应用例子进行实战演练，从而帮助你彻底掌握这一当今最流行的数据处理平台。</p><h2>思考题</h2><p>你认为有哪些MapReduce的缺点是在Spark框架中依然存在的？用什么思路可以解决？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":94001,"user_name":"jon","can_delete":false,"product_type":"c1","uid":1253287,"ip_address":"","ucode":"5768A34E292CAA","user_header":"https://static001.geekbang.org/account/avatar/00/13/1f/a7/d379ca4f.jpg","comment_is_top":false,"comment_ctime":1557704922,"is_pvip":false,"replies":[{"id":"48503","content":"这位同学的理解很对！","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567384701,"ip_address":"","comment_id":94001,"utype":1}],"discussion_count":1,"race_medal":0,"score":"173356396762","product_id":100025301,"comment_content":"mr编程模型单一，维护成本高，多个job执行时每个都要数据落盘。而spark拥有更高的抽象级别rdd，一次读取数据后便可在内存中进行多步迭代计算，对rdd的计算是多线程并发的所有很高效。<br>但是spark依然会存在数据倾斜的情况，在shuffle时有可能导致一个成为数据热点的情况","like_count":41,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449864,"discussion_content":"这位同学的理解很对！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567384701,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94223,"user_name":"朱同学","can_delete":false,"product_type":"c1","uid":1514233,"ip_address":"","ucode":"2EF7D5A051712C","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/f9/180f347a.jpg","comment_is_top":false,"comment_ctime":1557747897,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"121816832185","product_id":100025301,"comment_content":"似乎数据倾斜是所有分布式计算引擎的通病","like_count":29},{"had_liked":false,"id":94045,"user_name":"JohnT3e","can_delete":false,"product_type":"c1","uid":1063982,"ip_address":"","ucode":"CF4AAAC933529C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132","comment_is_top":false,"comment_ctime":1557709500,"is_pvip":false,"replies":[{"id":"48510","content":"说的很好！","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567386011,"ip_address":"","comment_id":94045,"utype":1}],"discussion_count":1,"race_medal":0,"score":"78867120828","product_id":100025301,"comment_content":"1. spark本质上还是批处理，只是通过微批处理来实现近实时处理。如果需要实时处理，可以使用apache flink；<br>2. 小文件处理依然有性能问题；<br>3. 仍需要手动调优，比如如何让数据合理分区，来避免或者减轻数据倾斜","like_count":19,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449888,"discussion_content":"说的很好！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567386011,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94088,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1557717111,"is_pvip":false,"replies":[{"id":"48525","content":"很好的总结","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567389479,"ip_address":"","comment_id":94088,"utype":1}],"discussion_count":1,"race_medal":0,"score":"57392291959","product_id":100025301,"comment_content":"Spark和MR的两个区别：<br>1. MapReduce编程模型中，每一对map和reduce都会生成一个job，而且每一次都是写磁盘，这就造成老师所说的启动时间变长，而且维护起来比较复杂；而Spark是链式计算，像map、flatmap、filter等transformation算子是不会触发计算的，只有在遇到像count、collect、saveAsTable等Action算子时，才会真正触发一次计算，对应会生成一个job。<br>2. MapReduce每次的MR结果都是保存到磁盘，所以时间开销大；而Spark在执行应用程序是，中间的处理过程、数据(RDD)的缓存和shuffle操作等都是在内存允许的情况下，放在内存中的，所以耗时短。","like_count":14,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449909,"discussion_content":"很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567389479,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94053,"user_name":"锦","can_delete":false,"product_type":"c1","uid":1468298,"ip_address":"","ucode":"CB0EB4B68C468B","user_header":"https://static001.geekbang.org/account/avatar/00/16/67/8a/babd74dc.jpg","comment_is_top":false,"comment_ctime":1557710658,"is_pvip":false,"replies":[{"id":"48524","content":"看过后边讲Spark流处理的章节就明白了，Storm的优势是纯实时处理，延迟可以到毫秒级，所以试用于需要低延迟的实时场景。我认为Spark在纯流处理上唯一的优势是吞吐量要高。但是考虑使用Spark Streaming最主要的一个因素，应该是针对整个项目进行宏观的考虑，即，如果一个项目除了实时计算之外，还包括了离线批处理、交互式查询等业务功能，而且实时计算中，可能还会牵扯到高延迟批处理、交互式查询等功能，那么就应该首选Spark生态，用Spark Core开发离线批处理，用Spark SQL开发交互式查询，用Spark Streaming开发实时计算，三者可以无缝整合，给系统提供非常高的可扩展性","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567389295,"ip_address":"","comment_id":94053,"utype":1}],"discussion_count":1,"race_medal":0,"score":"40212416322","product_id":100025301,"comment_content":"Spark是在MapReduce的基础上一点点通过工程和学术相结合做出来的，那么是不是意味着背后的理论模型都是分治思想？<br>相对于MapReduce多进程模型来说，Spark基于多线程模型，启动速度更快，Cpu利用率更好和内存共享更好。<br>MapReduce只提供Map和Reduce操作，而Spark中最基本的弹性分布式数据结构RDD，提供丰富的Api，对机器学习，人工智能更友好，更适用于现代数据处理场景。<br>MapReduce计算后的中间数据需要落盘，而Spark的中间数据缓存在内存中用于后续迭代，速度更快。<br>疑问:Spark相对于Storm在流计算中有哪些优势呢？","like_count":10,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449894,"discussion_content":"看过后边讲Spark流处理的章节就明白了，Storm的优势是纯实时处理，延迟可以到毫秒级，所以试用于需要低延迟的实时场景。我认为Spark在纯流处理上唯一的优势是吞吐量要高。但是考虑使用Spark Streaming最主要的一个因素，应该是针对整个项目进行宏观的考虑，即，如果一个项目除了实时计算之外，还包括了离线批处理、交互式查询等业务功能，而且实时计算中，可能还会牵扯到高延迟批处理、交互式查询等功能，那么就应该首选Spark生态，用Spark Core开发离线批处理，用Spark SQL开发交互式查询，用Spark Streaming开发实时计算，三者可以无缝整合，给系统提供非常高的可扩展性","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567389295,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94038,"user_name":"蒙开强","can_delete":false,"product_type":"c1","uid":1317706,"ip_address":"","ucode":"61B3183781B9F7","user_header":"https://static001.geekbang.org/account/avatar/00/14/1b/4a/f9df2d06.jpg","comment_is_top":false,"comment_ctime":1557709173,"is_pvip":false,"replies":[{"id":"48518","content":"Map和Reudce的Task都是进程级别的，这也是MapReduce广为人诟病的原因","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567386991,"ip_address":"","comment_id":94038,"utype":1}],"discussion_count":2,"race_medal":0,"score":"35917447541","product_id":100025301,"comment_content":"老师，你好，我看你专栏里说到，MapReduce 是多进程模型。我有点疑惑，MapReduce的map阶段和reduce阶段都有并行task进行运行，它们的task不是线程级别么。","like_count":8,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449883,"discussion_content":"Map和Reudce的Task都是进程级别的，这也是MapReduce广为人诟病的原因","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567386991,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1134022,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4d/c6/d4e85081.jpg","nickname":"纳兰残德","note":"","ucode":"2CCEA4539748B4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":1763,"discussion_content":"每个map reduce都是一个单独的jvm，而且 一个map任务结束 这个map对应的jvm会关闭，新的map要开jvm，所以这方面的开销也是很影响mr性能的","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1562897940,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94110,"user_name":"楚翔style","can_delete":false,"product_type":"c1","uid":1174846,"ip_address":"","ucode":"E715F82C34A9AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/ed/3e/c1725237.jpg","comment_is_top":false,"comment_ctime":1557722675,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"18737591859","product_id":100025301,"comment_content":"sparkcontext不能在集群全局共享，比如我submit了100个spark任务，每个任务都要初始化自己的sc，还要销毁，每一次的创建销毁都很耗时。 这块有什么策略可以优化下吗？ 谢谢","like_count":5,"discussions":[{"author":{"id":1093120,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ae/00/58ad3a3b.jpg","nickname":"DYK","note":"","ucode":"21BDC80CCFD049","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":291079,"discussion_content":"sparklivy","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594702388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1174846,"avatar":"https://static001.geekbang.org/account/avatar/00/11/ed/3e/c1725237.jpg","nickname":"楚翔style","note":"","ucode":"E715F82C34A9AA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1093120,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ae/00/58ad3a3b.jpg","nickname":"DYK","note":"","ucode":"21BDC80CCFD049","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":291723,"discussion_content":"\nlivy我理解也是模拟spark－submit提交.只不过是用rest方式，本质还是个submit","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594917770,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":291079,"ip_address":""},"score":291723,"extra":""}]}]},{"had_liked":false,"id":94049,"user_name":"miwucc","can_delete":false,"product_type":"c1","uid":1326429,"ip_address":"","ucode":"7935BD907119AE","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/5d/ac666969.jpg","comment_is_top":false,"comment_ctime":1557710254,"is_pvip":false,"replies":[{"id":"48529","content":"你说的很好，这两个问题都是分布式计算的痛点。用Spark的RDD API没有自动优化数据分区，很依赖开发者手动调优。后边讲到的Spark SQL的执行引擎会有这方面的优化，性能要显著优于RDD。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567390669,"ip_address":"","comment_id":94049,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18737579438","product_id":100025301,"comment_content":"感觉mapreduce遗留的问题还有<br>1.数据分片分不好，导致数据过于集中在某机器导致整体处理速度慢或者无法处理问题。spark还是全靠使用者的分片函数还是自己有方法可以动态调度？<br>2.对于实际复杂业务中的多job前后依赖，与业务紧密耦合的异常捕捉，处理机制是否有更好的解决方法？","like_count":5,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449891,"discussion_content":"你说的很好，这两个问题都是分布式计算的痛点。用Spark的RDD API没有自动优化数据分区，很依赖开发者手动调优。后边讲到的Spark SQL的执行引擎会有这方面的优化，性能要显著优于RDD。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567390669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94623,"user_name":"涵","can_delete":false,"product_type":"c1","uid":1502742,"ip_address":"","ucode":"BB8575DB13F1E0","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg","comment_is_top":false,"comment_ctime":1557867232,"is_pvip":false,"replies":[{"id":"48509","content":"看过后边几讲你就明白了，Spark有Spark Streaming和Structured Streaming的流处理模块，所以它并不是只能做批量处理。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567385984,"ip_address":"","comment_id":94623,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14442769120","product_id":100025301,"comment_content":"老师讲到Hadoop的四个缺陷，前三个都讲到了Spark的解决方案，比如抽象层次更合理，易于使用，可用的方法更多，不限于map和reduce，还有将迭代的计算结果放入内存，提升迭代计算的效率，降低能耗。但是没有提到第四个只能批量处理数据的问题。这是不是Spark的局限?","like_count":4,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450143,"discussion_content":"看过后边几讲你就明白了，Spark有Spark Streaming和Structured Streaming的流处理模块，所以它并不是只能做批量处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567385984,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94280,"user_name":"挖矿的小戈","can_delete":false,"product_type":"c1","uid":1503917,"ip_address":"","ucode":"2078A85139BD5D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/9chAb6SjxFiapSeicsAsGqzziaNlhX9d5aEt8Z0gUNsZJ9dICaDHqAypGvjv4Bx3PryHnj7OFnOXFOp7Ik21CVXEA/132","comment_is_top":false,"comment_ctime":1557758798,"is_pvip":false,"replies":[{"id":"48526","content":"很好的总结","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567389576,"ip_address":"","comment_id":94280,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14442660686","product_id":100025301,"comment_content":"1. MR、Spark都有shuffle，shuffle难免出现数据倾斜<br>2.流计算上的不足：MR就不说了，SparkStreaming只能算微批处理，尽管Spark的StructStreaming已经在这方面做了改进，但是思想貌似无法改变，spark就是以批处理的思想来处理流数据，这方面感觉没法跟flink比，不过交互式查询、ML以及生态上会比flink强","like_count":4,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449997,"discussion_content":"很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567389576,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":101919,"user_name":"珅剑","can_delete":false,"product_type":"c1","uid":1504220,"ip_address":"","ucode":"4290D5C140F80F","user_header":"https://static001.geekbang.org/account/avatar/00/16/f3/dc/80b0cd23.jpg","comment_is_top":false,"comment_ctime":1560052269,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10149986861","product_id":100025301,"comment_content":"Spark在Shuffle阶段依然有可能产生大量的小文件，大量的随机读写造成磁盘IO性能下降，","like_count":3},{"had_liked":false,"id":94496,"user_name":"杨家荣","can_delete":false,"product_type":"c1","uid":1259241,"ip_address":"","ucode":"3DA65396C7F002","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132","comment_is_top":false,"comment_ctime":1557824832,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10147759424","product_id":100025301,"comment_content":"总用scala 写spring cloud项目，属于杀鸡用牛刀吗？","like_count":2},{"had_liked":false,"id":94123,"user_name":"乾鹏","can_delete":false,"product_type":"c1","uid":1046236,"ip_address":"","ucode":"D089EDA4ED8C1C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f6/dc/710a2cc7.jpg","comment_is_top":false,"comment_ctime":1557725175,"is_pvip":true,"replies":[{"id":"48506","content":"相信你看完21讲应该就知道了，Flink在流处理上有天然的优势，而Spark虽然有各种流处理的模块，但还是做批处理比较擅长。看完21讲中Spark和Flink的对比之后相信你可以做出选择","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567385716,"ip_address":"","comment_id":94123,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10147659767","product_id":100025301,"comment_content":"spark和flink，这两个都是分布式数据处理框架，项目中应该怎么选？","like_count":2,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449922,"discussion_content":"相信你看完21讲应该就知道了，Flink在流处理上有天然的优势，而Spark虽然有各种流处理的模块，但还是做批处理比较擅长。看完21讲中Spark和Flink的对比之后相信你可以做出选择","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567385716,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":115539,"user_name":"趁早","can_delete":false,"product_type":"c1","uid":1031970,"ip_address":"","ucode":"949FB3AA250D80","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bf/22/26530e66.jpg","comment_is_top":false,"comment_ctime":1563614433,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5858581729","product_id":100025301,"comment_content":"k8s,mesos都知识编排工具吧，那具体数据存哪里呢","like_count":1,"discussions":[{"author":{"id":1032932,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c2/e4/ad418d90.jpg","nickname":"风","note":"","ucode":"2DEDB586E625C4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":135504,"discussion_content":"Spark不存储数据，数据从HDFS，Mongo，ES都可以","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1579095125,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":109347,"user_name":"darren","can_delete":false,"product_type":"c1","uid":1027735,"ip_address":"","ucode":"386736C90F32CA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/97/ba512167.jpg","comment_is_top":false,"comment_ctime":1562023820,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5856991116","product_id":100025301,"comment_content":"可以为每个进程设定使用资源，对每个线程就不好控制","like_count":1},{"had_liked":false,"id":285417,"user_name":"走刀口 💰","can_delete":false,"product_type":"c1","uid":1581245,"ip_address":"","ucode":"C7E2B812A4A02C","user_header":"https://static001.geekbang.org/account/avatar/00/18/20/bd/5656b5d7.jpg","comment_is_top":false,"comment_ctime":1616802789,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1616802789","product_id":100025301,"comment_content":"老师，为啥用mr进行join操作会有问题，你文章里说需要对2个数据集做map和reduce，难道spark不是吗？","like_count":0},{"had_liked":false,"id":247344,"user_name":"reven404","can_delete":false,"product_type":"c1","uid":1022868,"ip_address":"","ucode":"F84AAF2DC1D18F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/9b/94/4977913a.jpg","comment_is_top":false,"comment_ctime":1599664061,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1599664061","product_id":100025301,"comment_content":"如果说spark是把数据加载到内存中计算，那是怎么把100TB的数据加载到内存里面的？集群规模是？","like_count":1},{"had_liked":false,"id":215372,"user_name":"北冥有鱼","can_delete":false,"product_type":"c1","uid":1592243,"ip_address":"","ucode":"1690734A1061F4","user_header":"https://static001.geekbang.org/account/avatar/00/18/4b/b3/51bb33f2.jpg","comment_is_top":false,"comment_ctime":1588983733,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1588983733","product_id":100025301,"comment_content":"如HDFS小文件问题，数据倾斜问题，OOM内存溢出问题，还有配置文件项过多。因为spark大量使用内存，更容易OOM。同时，实际开发中发现，spark不适合高并发，容易出现内存泄露。 对于流式这一块，状态管理也不太好，而Flink从搭建架构开始，就把状态管理考虑进去了…","like_count":1},{"had_liked":false,"id":201634,"user_name":"王大龙","can_delete":false,"product_type":"c1","uid":1852401,"ip_address":"","ucode":"73A8A4428C911E","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIhkl44PZasVPBEDobjeyMhhNb50BwFbHic5aVsJz4zynN9PcqaEE3WwMND7Wrw9yzjYicUlqzdzFZA/132","comment_is_top":false,"comment_ctime":1585814622,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585814622","product_id":100025301,"comment_content":"spark通过缓存内存提高数据处理效率，但是如果很多文件需要同时处理的话，应该影响其性能吧；<br>另外想请教下老师，如果同时通过spark运行数据，在内存足够的情况下，是并行处理吗？","like_count":0},{"had_liked":false,"id":197414,"user_name":"不记年","can_delete":false,"product_type":"c1","uid":1045945,"ip_address":"","ucode":"287E40C68356DC","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f5/b9/888fe350.jpg","comment_is_top":false,"comment_ctime":1585375152,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585375152","product_id":100025301,"comment_content":"当小文件过多时MR，Spark运算效率都下降的很厉害。我觉得解决方案可能在计算框架之外，通过数据质量管理系统来解决","like_count":0},{"had_liked":false,"id":194219,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1585038445,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585038445","product_id":100025301,"comment_content":"其实spark和mr就是差不多的，很多mr的优化的结果就是spark，spark限定了编程方式，其实数据处理还有很多要考虑的，比如存储均衡，最小化访问，索引，视图实时维护等等","like_count":0},{"had_liked":false,"id":157301,"user_name":"走小調的凡世林","can_delete":false,"product_type":"c1","uid":1011453,"ip_address":"","ucode":"EF3E5195D56188","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6e/fd/6d0109c0.jpg","comment_is_top":false,"comment_ctime":1575105212,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1575105212","product_id":100025301,"comment_content":"请教下老师，我们有个需求：根据各种自定义规则（比如分辨率、大小等）计算海量资源的分数（资源可以是图片、视频、音频）。总分100，图片分辨率太小或视频太大都要扣分，最后算出一个资源总分，这种需求可以用spark实现吗？主要考虑算分过程可能比较耗时，且资源数量较多。如果可以的话如何实现呢？老师是否可以提供下思路，感谢！","like_count":0},{"had_liked":false,"id":108678,"user_name":"滩涂曳尾","can_delete":false,"product_type":"c1","uid":1187478,"ip_address":"","ucode":"40F650F2A419D4","user_header":"https://static001.geekbang.org/account/avatar/00/12/1e/96/c735ad6b.jpg","comment_is_top":false,"comment_ctime":1561852764,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1561852764","product_id":100025301,"comment_content":"怎么感觉spark相比hadoop只是做了一些工程上的优化，多线程vs多进程(类比apache vs. nginx)，内存缓存磁盘数据，... 学了后面的再看","like_count":0},{"had_liked":false,"id":105112,"user_name":"13311195819","can_delete":false,"product_type":"c1","uid":1321508,"ip_address":"","ucode":"C0BAC32249A72D","user_header":"","comment_is_top":false,"comment_ctime":1560920613,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560920613","product_id":100025301,"comment_content":"老师好，您文章中提到“MapReduce 状态机“，这个词该怎么理解呢？是不是类似于flink中state的概念？<br><br>","like_count":0},{"had_liked":false,"id":97498,"user_name":"一","can_delete":false,"product_type":"c1","uid":1220750,"ip_address":"","ucode":"28E0605EA1AE88","user_header":"https://static001.geekbang.org/account/avatar/00/12/a0/8e/6e4c7509.jpg","comment_is_top":false,"comment_ctime":1558685203,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1558685203","product_id":100025301,"comment_content":"老师好，“多进程模型便于细粒度控制每个任务占用的资源”这一句不太理解，进程不是粗粒度的概念吗？为什么以进程为单位的并发反而便于细粒度的控制每个任务占用的资源呢？","like_count":1,"discussions":[{"author":{"id":1048707,"avatar":"https://static001.geekbang.org/account/avatar/00/10/00/83/9329a697.jpg","nickname":"马殿军","note":"","ucode":"97FE6251C04E6D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":24706,"discussion_content":"是因为进程层面的资源隔离更方便吧","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1570244985,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94150,"user_name":"大鹏","can_delete":false,"product_type":"c1","uid":1004481,"ip_address":"","ucode":"F56D9D1DE892D8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/53/c1/34bb9b24.jpg","comment_is_top":false,"comment_ctime":1557732400,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557732400","product_id":100025301,"comment_content":"两者共存的数据热点问题，依然需要手工介入，一个方法是减少无用的信息，第二个就是单独处理且分而治之","like_count":0},{"had_liked":false,"id":94124,"user_name":"程序设计的艺术","can_delete":false,"product_type":"c1","uid":1107847,"ip_address":"","ucode":"CA3817C11CD9E6","user_header":"https://static001.geekbang.org/account/avatar/00/10/e7/87/9ad59b98.jpg","comment_is_top":false,"comment_ctime":1557725958,"is_pvip":false,"replies":[{"id":"48527","content":"1. 如果只是单机场景下，效率确实差不多。Spark本身就是分布式得计算引擎。<br>2. 还需要更多信息来判断，不能一概而论。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567390340,"ip_address":"","comment_id":94124,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557725958","product_id":100025301,"comment_content":"您好，我有两个问题，最近在某场景选型处理方案时，对于spark处理方式下：<br>1.如果是非集群下使用spark，是不是与jvm多线程处理任务的效率差不多？<br>2.对于库存数据与订单明细的匹配和库存校验，是否适合使用spark来处理？对于库存数据来讲，是热点数据，是看成两个数据集做union还是一个订单数据集？谢谢","like_count":0,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449923,"discussion_content":"1. 如果只是单机场景下，效率确实差不多。Spark本身就是分布式得计算引擎。\n2. 还需要更多信息来判断，不能一概而论。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567390340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94037,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1557709122,"is_pvip":false,"replies":[{"id":"48514","content":"这个我们在15讲Spark SQL中有详细的讲解它的发展历程，它确实是用来让Spark摆脱对Hive的依赖而发展来的。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567386129,"ip_address":"","comment_id":94037,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1557709122","product_id":100025301,"comment_content":"Sparksql是不是用来替代hive的一个工具","like_count":1,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449882,"discussion_content":"这个我们在15讲Spark SQL中有详细的讲解它的发展历程，它确实是用来让Spark摆脱对Hive的依赖而发展来的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567386129,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1503327,"avatar":"https://static001.geekbang.org/account/avatar/00/16/f0/5f/19d1a3ff.jpg","nickname":"你赢了","note":"","ucode":"D72375656E88C0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":451,"discussion_content":"spark本质来讲是一个计算引擎，hive是数仓，替代不了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561597367,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}