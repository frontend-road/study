{"id":98537,"title":"20 | 流处理案例实战：分析纽约市出租车载客信息","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“流处理案例实战：分析纽约市出租车载客信息”。</p><p>在上一讲中，我们结合加州房屋信息的真实数据集，构建了一个基本的预测房价的线性回归模型。通过这个实例，我们不仅学习了处理大数据问题的基本流程，而且还进一步熟练了对RDD和DataFrame API的使用。</p><p>你应该已经发现，上一讲的实例是一个典型的批处理问题，因为处理的数据是静态而有边界的。今天让我们来一起通过实例，更加深入地学习用Spark去解决实际的流处理问题。</p><p>相信你还记得，在前面的章节中我们介绍过Spark两个用于流处理的组件——Spark Streaming和Structured Streaming。其中Spark Streaming是Spark 2.0版本前的的流处理库，在Spark 2.0之后，集成了DataFrame/DataSet API的Structured Streaming成为Spark流处理的主力。</p><p>今天就让我们一起用Structured Streaming对纽约市出租车的载客信息进行处理，建立一个实时流处理的pipeline，实时输出各个区域内乘客小费的平均数来帮助司机决定要去哪里接单。</p><h2>数据集介绍</h2><p>今天的数据集是纽约市2009～2015年出租车载客的信息。每一次出行包含了两个事件，一个事件代表出发，另一个事件代表到达。每个事件都有11个属性，它的schema如下所示：</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/4a/90/4ae9c7d353925f84d36bf7280f2b5b90.jpg?wh=1538*886\" alt=\"\"></p><p>这部分数据有个不太直观的地方，那就是同一次出行会有两个记录，而且代表出发的事件没有任何意义，因为到达事件已经涵盖了所有必要的信息。现实世界中的数据都是这样复杂，不可能像学校的测试数据一样简单直观，所以处理之前，我们要先对数据进行清洗，只留下必要的信息。</p><p>这个数据还包含有另外一部分信息，就是所有出租车的付费信息，它有8个属性，schema如下所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/de/8ef443617788243f4546116fffb40ede.jpg?wh=1526*660\" alt=\"\"></p><p>这个数据集可以从<a href=\"https://training.ververica.com/setup/taxiData.html\">网上</a>下载到，数据集的规模在100MB左右，它只是节选了一部分出租车的载客信息，所以在本机运行就可以了。详细的纽约出租车数据集超过了500GB，同样在<a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\">网上</a>可以下载，感兴趣的同学可以下载来实践一下。</p><h2>流数据输入</h2><p>你可能要问，这个数据同样是静态、有边界的，为什么要用流处理？</p><p>因为我们手里没有实时更新的流数据源。我也没有权限去公开世界上任何一个上线产品的数据流。所以，这里只能将有限的数据经过Kafka处理，输出为一个伪流数据，作为我们要构建的pipeline的输入。</p><p>在模块二中，我们曾经初步了解过Apache Kafka，知道它是基于Pub/Sub模式的流数据处理平台。由于我们的专栏并不涉及Apache Kafka的具体内容，所以我在这里就不讲如何把这个数据输入到Kafka并输出的细节了。你只要知道，在这个例子中，Consumer是之后要写的Spark流处理程序，这个消息队列有两个Topic，一个包含出行的地理位置信息，一个包含出行的收费信息。Kafka会<strong>按照时间顺序</strong>，向这两个Topic中发布事件，从而模拟一个实时的流数据源。</p><p>相信你还记得，写Spark程序的第一步就是创建SparkSession对象，并根据输入数据创建对应的RDD或者DataFrame。你可以看下面的代码。</p><pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder\n   .appName(&quot;Spark Structured Streaming for taxi ride info&quot;)\n   .getOrCreate()\n\nrides = spark\n   .readStream\n   .format(&quot;kafka&quot;)\n   .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:xxxx&quot;) //取决于Kafka的配置\n   .option(&quot;subscribe&quot;, &quot;taxirides&quot;)\n   .option(&quot;startingOffsets&quot;, &quot;latest&quot;)\n   .load()\n   .selectExpr(&quot;CAST(value AS STRING)&quot;)\n\nfares = spark\n   .readStream\n   .format(&quot;kafka&quot;)\n   .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:xxxx&quot;)\n   .option(&quot;subscribe&quot;, &quot;taxifares&quot;)\n   .option(&quot;startingOffsets&quot;, &quot;latest&quot;)\n   .load()\n   .selectExpr(&quot;CAST(value AS STRING)\n</code></pre><p>在这段代码里，我们创建了两个Streaming DataFrame，并订阅了对应的Kafka topic，一个代表出行位置信息，另一个代表收费信息。Kafka对数据没有做任何修改，所以流中的每一个数据都是一个长String，属性之间是用逗号分割的。</p><pre><code>417986,END,2013-01-02 00:43:52,2013-01-02  00:39:56,-73.984528,40.745377,-73.975967,40.765533,1,2013007646,2013007642\n</code></pre><h2>数据清洗</h2><p>现在，我们要开始做数据清洗了。要想分离出我们需要的位置和付费信息，我们首先要把数据分割成一个个属性，并创建对应的DataFrame中的列。为此，我们首先要根据数据类型创建对应的schema。</p><pre><code>ridesSchema = StructType([\n   StructField(&quot;rideId&quot;, LongType()), StructField(&quot;isStart&quot;, StringType()),\n   StructField(&quot;endTime&quot;, TimestampType()), StructField(&quot;startTime&quot;, TimestampType()),\n   StructField(&quot;startLon&quot;, FloatType()), StructField(&quot;startLat&quot;, FloatType()),\n   StructField(&quot;endLon&quot;, FloatType()), StructField(&quot;endLat&quot;, FloatType()),\n   StructField(&quot;passengerCnt&quot;, ShortType()), StructField(&quot;taxiId&quot;, LongType()),\n   StructField(&quot;driverId&quot;, LongType())])\n\nfaresSchema = StructType([\n   StructField(&quot;rideId&quot;, LongType()), StructField(&quot;taxiId&quot;, LongType()),\n   StructField(&quot;driverId&quot;, LongType()), StructField(&quot;startTime&quot;, TimestampType()),\n   StructField(&quot;paymentType&quot;, StringType()), StructField(&quot;tip&quot;, FloatType()),\n   StructField(&quot;tolls&quot;, FloatType()), StructField(&quot;totalFare&quot;, FloatType())])\n</code></pre><p>接下来，我们将每个数据都用逗号分割，并加入相应的列。</p><pre><code>def parse_data_from_kafka_message(sdf, schema):\n   from pyspark.sql.functions import split\n   assert sdf.isStreaming == True, &quot;DataFrame doesn't receive streaming data&quot;\n   col = split(sdf['value'], ',')\n   for idx, field in enumerate(schema):\n       sdf = sdf.withColumn(field.name, col.getItem(idx).cast(field.dataType))\n   return sdf.select([field.name for field in schema])\n\nrides = parse_data_from_kafka_message(rides, ridesSchema)\nfares = parse_data_from_kafka_message(fares, faresSchema)\n</code></pre><p>在上面的代码中，我们定义了函数parse_data_from_kafka_message，用来把Kafka发来的message根据schema拆成对应的属性，转换类型，并加入到DataFrame的表中。</p><p>正如我们之前提到的，读入的数据包含了一些无用信息。</p><p>首先，所有代表出发的事件都已被删除，因为到达事件已经包含了出发事件的所有信息，而且只有到达之后才会付费。</p><p>其次，出发地点和目的地在纽约范围外的数据，也可以被删除。因为我们的目标是找出纽约市内小费较高的地点。DataFrame的filter函数可以很容易地做到这些。</p><pre><code>MIN_LON, MAX_LON, MIN_LAT, MAX_LAT = -73.7, -74.05, 41.0, 40.5\nrides = rides.filter(\n   rides[&quot;startLon&quot;].between(MIN_LON, MAX_LON) &amp;\n   rides[&quot;startLat&quot;].between(MIN_LAT, MAX_LAT) &amp;\n   rides[&quot;endLon&quot;].between(MIN_LON, MAX_LON) &amp;\n   rides[&quot;endLat&quot;].between(MIN_LAT, MAX_LAT))\nrides = rides.filter(rides[&quot;isStart&quot;] == &quot;END&quot;)\n</code></pre><p>上面的代码中首先定义了纽约市的经纬度范围，然后把所有起点和终点在这个范围之外的数据都过滤掉了。最后，把所有代表出发事件的数据也移除掉。</p><p>当然，除了前面提到的清洗方案，可能还会有别的可以改进的地方，比如把不重要的信息去掉，例如乘客数量、过路费等，你可以自己思考一下。</p><h2>Stream-stream Join</h2><p>我们的目标是找出小费较高的地理区域，而现在收费信息和地理位置信息还在两个DataFrame中，无法放在一起分析。那么要用怎样的方式把它们联合起来呢？</p><p>你应该还记得，DataFrame本质上是把数据当成一张关系型的表。在我们这个例子中，rides所对应的表的键值（Key）是rideId，其他列里我们关心的就是起点和终点的位置；fares所对应的表键值也是rideId，其他列里我们关心的就是小费信息（tips）。</p><p>说到这里，你可能会自然而然地想到，如果可以像关系型数据表一样，根据共同的键值rideId把两个表inner join起来，就可以同时分析这两部分信息了。但是这里的DataFrame其实是两个数据流，Spark可以把两个流Join起来吗？</p><p>答案是肯定的。在Spark 2.3中，流与流的Join（Stream-stream join）被正式支持。这样的Join难点就在于，在任意一个时刻，流数据都不是完整的，流A中后面还没到的数据有可能要和流B中已经有的数据Join起来再输出。为了解决这个问题，我们就要引入<strong>数据水印</strong>（Watermark）的概念。</p><p>数据水印定义了我们可以对数据延迟的最大容忍限度。</p><p>比如说，如果定义水印是10分钟，数据A的事件时间是1:00，数据B的事件时间是1:10，由于数据传输发生了延迟，我们在1:15才收到了A和B，那么我们将只处理数据B并更新结果，A会被无视。在Join操作中，好好利用水印，我们就知道什么时候可以不用再考虑旧数据，什么时候必须把旧数据保留在内存中。不然，我们就必须把所有旧数据一直存在内存里，导致数据不断增大，最终可能会内存泄漏。</p><p>在这个例子中，为什么我们做这样的Join操作需要水印呢？</p><p>这是因为两个数据流并不保证会同时收到同一次出行的数据，因为收费系统需要额外的时间去处理，而且这两个数据流是独立的，每个都有可能产生数据延迟。所以要对时间加水印，以免出现内存中数据无限增长的情况。</p><p>那么下一个问题就是，究竟要对哪个时间加水印，出发时间还是到达时间？</p><p>前面说过了，我们其实只关心到达时间，所以对rides而言，我们只需要对到达时间加水印。但是，在fares这个DataFrame里并没有到达时间的任何信息，所以我们没法选择，只能对出发时间加水印。因此，我们还需要额外定义一个时间间隔的限制，出发时间和到达时间的间隔要在一定的范围内。具体内容你可以看下面的代码。</p><pre><code>faresWithWatermark = fares\n   .selectExpr(&quot;rideId AS rideId_fares&quot;, &quot;startTime&quot;, &quot;totalFare&quot;, &quot;tip&quot;)\n   .withWatermark(&quot;startTime&quot;, &quot;30 minutes&quot;)\n\nridesWithWatermark = rides\n .selectExpr(&quot;rideId&quot;, &quot;endTime&quot;, &quot;driverId&quot;, &quot;taxiId&quot;, &quot;startLon&quot;, &quot;startLat&quot;, &quot;endLon&quot;, &quot;endLat&quot;)\n .withWatermark(&quot;endTime&quot;, &quot;30 minutes&quot;)\n\njoinDF = faresWithWatermark\n   .join(ridesWithWatermark,\n     expr(&quot;&quot;&quot;\n      rideId_fares = rideId AND\n       endTime &gt; startTime AND\n       endTime &lt;= startTime + interval 2 hours\n       &quot;&quot;&quot;)\n</code></pre><p>在这段代码中，我们对fares和rides分别加了半小时的水印，然后把两个DataFrame根据rideId和时间间隔的限制Join起来。这样，joinDF就同时包含了地理位置和付费信息。</p><p>接下来，就让我们开始计算实时的小费最高区域。</p><h2>计算结果并输出</h2><p>到现在为止，我们还没有处理地点信息。原生的经纬度信息显然并没有很大用处。我们需要做的是把纽约市分割成几个区域，把数据中所有地点的经纬度信息转化成区域信息，这样司机们才可以知道大概哪个地区的乘客比较可能给高点的小费。</p><p>纽约市的区域信息以及坐标可以从网上找到，这部分处理比较容易。每个接收到的数据我们都可以判定它在哪个区域内，然后对joinDF增加一个列“area”来代表终点的区域。现在，让我们假设area已经加到现有的DataFrame里。接下来我们需要把得到的信息告诉司机了。</p><p>还记得第16讲和第17讲中提到的滑动窗口操作吗？这是流处理中常见的输出形式，即输出每隔一段时间内，特定时间窗口的特征值。在这个例子中，我们可以每隔10分钟，输出过去半小时内每个区域内的平均小费。这样的话，司机可以每隔10分钟查看一下数据，决定下一步去哪里接单。这个查询（Query）可以由以下代码产生。</p><pre><code>tips = joinDF\n   .groupBy(\n       window(&quot;endTime&quot;, &quot;30 minutes&quot;, &quot;10 minutes&quot;),\n       &quot;area&quot;)\n   .agg(avg(&quot;tip&quot;))\n</code></pre><p>最后，我们把tips这个流式DataFrame输出。</p><pre><code>query.writeStream\n   .outputMode(&quot;append&quot;)\n   .format(&quot;console&quot;)\n   .option(&quot;truncate&quot;, False\n   .start()\n   .awaitTermination()\n</code></pre><p>你可能会问，为什么我们不可以把输出结果按小费多少进行排序呢？</p><p>这是因为两个流的inner-join只支持附加输出模式（Append Mode），而现在Structured Streaming不支持在附加模式下进行排序操作。希望将来Structured Streaming可以提供这个功能，但是现在，司机们只能扫一眼所有的输出数据来大概判断哪个地方的小费最高了。</p><h2>小结</h2><p>流处理和批处理都是非常常见的应用场景，而且相较而言流处理更加复杂，对延迟性要求更高。今天我们再次通过一个实例帮助你了解要如何利用Structured Streaming对真实数据集进行流处理。Spark最大的好处之一就是它拥有统一的批流处理框架和API，希望你在课下要进一步加深对DataSet/DataFrame的熟练程度。</p><h2>思考题</h2><p>今天的主题是“案例实战”，不过我留的是思考题，而不是实践题。因为我不确定你是否会使用Kafka。如果你的工作中会接触到流数据，那么你可以参考今天这个案例的思路和步骤来解决问题，多加练习以便熟悉Spark的使用。如果你还没有接触过流数据，但却想往这方面发展的话，我就真的建议你去学习一下Kafka，这是个能帮助我们更好地做流处理应用开发和部署的利器。</p><p>现在，来说一下今天的思考题吧。</p><ol>\n<li>为什么流的Inner-Join不支持完全输出模式?</li>\n<li>对于Inner-Join而言，加水印是否是必须的？ Outer-Join呢？</li>\n</ol><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"19 | 综合案例实战：处理加州房屋信息，构建线性回归模型","id":98374},"right":{"article_title":"21 | 深入对比Spark与Flink：帮你系统设计两开花","id":99152}},"comments":[{"had_liked":false,"id":100348,"user_name":"never leave","can_delete":false,"product_type":"c1","uid":1241638,"ip_address":"","ucode":"62888A1446A1E8","user_header":"https://static001.geekbang.org/account/avatar/00/12/f2/26/8d42ea6f.jpg","comment_is_top":false,"comment_ctime":1559532351,"is_pvip":false,"replies":[{"id":"42768","content":"never leave同学，感谢提问。现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。<br><br>Outer join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。<br><br>由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。","user_name":"作者回复","comment_id":100348,"uid":"1386753","ip_address":"","utype":1,"ctime":1563948970,"user_name_real":"廿七"}],"discussion_count":1,"race_medal":0,"score":"96048812863","product_id":100025301,"comment_content":"官网上说inner join的watermark是可选的，outer join的watermark是必选的。但是我感觉应该都是必选的吧，就像案例中的inner join一样，如果不是必须的话，旧数据一直保存在内存中，有可能导致内存不够。","like_count":23,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452510,"discussion_content":"never leave同学，感谢提问。现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。\n\nOuter join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。\n\n由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563948970,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":100357,"user_name":"FengX","can_delete":false,"product_type":"c1","uid":1353041,"ip_address":"","ucode":"B1B0235B1D1935","user_header":"https://static001.geekbang.org/account/avatar/00/14/a5/51/7773d421.jpg","comment_is_top":false,"comment_ctime":1559534448,"is_pvip":false,"replies":[{"id":"42771","content":"Feng.X，感谢提问。<br><br>这个限制目的在于抛弃任何长于2个小时的出租车出行数据。<br><br>对于这个例子来说，这样一个对事件时间的限制确实不是必须的。加入它其实是为了告诉你，在基于事件时间来join两个流时，我们一般不考虑时间跨度过大的情况，因为它们没有普遍意义，还会影响数据分析的结果。<br><br>举例，对于一个网页广告，我们需要知道用户看到一个广告后要多长时间才会去点击它，从而评估广告的效果。这里显然有两个流：一个代表用户看到广告的事件，另一个代表用户点击广告的事件。尽管我们可以通过用户的ID来Join这两个流，但是我们需要加一个限制，就是点击广告的时间不能比看到广告的时间晚太久，否则Join的结果很可能是不准确的。比如，用户可能在1:00和2:00都看到了广告，但是只在2:01点击了它，我们应该把2:00和2:01Join起来，而不应该Join1:00和2:01，因为1:00看到的广告并没有促使他点击。","user_name":"作者回复","comment_id":100357,"uid":"1386753","ip_address":"","utype":1,"ctime":1563949128,"user_name_real":"廿七"}],"discussion_count":2,"race_medal":0,"score":"31624305520","product_id":100025301,"comment_content":"老师，请问join操作里有riderId了，为什么要加上endTime &gt; startTime AND endTime &lt;= startTime + interval 2 hours？","like_count":8,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452514,"discussion_content":"Feng.X，感谢提问。\n\n这个限制目的在于抛弃任何长于2个小时的出租车出行数据。\n\n对于这个例子来说，这样一个对事件时间的限制确实不是必须的。加入它其实是为了告诉你，在基于事件时间来join两个流时，我们一般不考虑时间跨度过大的情况，因为它们没有普遍意义，还会影响数据分析的结果。\n\n举例，对于一个网页广告，我们需要知道用户看到一个广告后要多长时间才会去点击它，从而评估广告的效果。这里显然有两个流：一个代表用户看到广告的事件，另一个代表用户点击广告的事件。尽管我们可以通过用户的ID来Join这两个流，但是我们需要加一个限制，就是点击广告的时间不能比看到广告的时间晚太久，否则Join的结果很可能是不准确的。比如，用户可能在1:00和2:00都看到了广告，但是只在2:01点击了它，我们应该把2:00和2:01Join起来，而不应该Join1:00和2:01，因为1:00看到的广告并没有促使他点击。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563949128,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1086295,"avatar":"https://static001.geekbang.org/account/avatar/00/10/93/57/3ffdfc8d.jpg","nickname":"vigo","note":"","ucode":"036CEE5F3FABE7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":211145,"discussion_content":"这个例子不太一样，出租车案例其实是剔除最大最小情况，类似投票打分。特出情况不具有代表性，我们要找到的是普罗大众的情况。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1584804834,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":100680,"user_name":"Poleness","can_delete":false,"product_type":"c1","uid":1004746,"ip_address":"","ucode":"C172FF26ABE022","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/ca/9d066f2a.jpg","comment_is_top":false,"comment_ctime":1559615437,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"23034451917","product_id":100025301,"comment_content":"请问下，这里解析kafka的value的时候，自定义了schema，但真正生产中很多数据的类型结构是很复杂的，徒手写schema的方式不一定可行。不知道有没有更优雅的方式？<br>（看了源码，如果是json等格式好像可以自动推断，但是对于kafka，他的sourceSchema好像是写死的，不知大家有没有好的建议或者经验？）","like_count":5,"discussions":[{"author":{"id":1876212,"avatar":"https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg","nickname":"之渊","note":"","ucode":"02B9299DBB4881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300221,"discussion_content":"那就需要定义好数据传输的格式规范了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1597990865,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":209962,"user_name":"谢志斌","can_delete":false,"product_type":"c1","uid":1904789,"ip_address":"","ucode":"CF095942FDF7C0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKcwxhdFicBaGzj1wodPu89dwJdtXFJlQWKJNhxEmdnpBPykCFsyh51hoo0llpPL6UyBgekqJgbqtg/132","comment_is_top":false,"comment_ctime":1587641106,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10177575698","product_id":100025301,"comment_content":"老师好，纽约市出租车第一个数据集链接，无法访问。","like_count":2},{"had_liked":false,"id":100322,"user_name":"jon","can_delete":false,"product_type":"c1","uid":1253287,"ip_address":"","ucode":"5768A34E292CAA","user_header":"https://static001.geekbang.org/account/avatar/00/13/1f/a7/d379ca4f.jpg","comment_is_top":false,"comment_ctime":1559528232,"is_pvip":false,"replies":[{"id":"42769","content":"jon，感谢提问。<br><br>现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。<br><br>Outer join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。<br><br>由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。","user_name":"作者回复","comment_id":100322,"uid":"1386753","ip_address":"","utype":1,"ctime":1563949010,"user_name_real":"廿七"}],"discussion_count":1,"race_medal":0,"score":"10149462824","product_id":100025301,"comment_content":"不支持完全输出是因为join的只是一个时间窗口内的数据<br>在这个例子中inner join使用watermark 是必须的，left joinwatermark不是必须的","like_count":2,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452499,"discussion_content":"jon，感谢提问。\n\n现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。\n\nOuter join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。\n\n由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563949010,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":133991,"user_name":"lhk","can_delete":false,"product_type":"c1","uid":1107774,"ip_address":"","ucode":"B601715BE46FF8","user_header":"https://static001.geekbang.org/account/avatar/00/10/e7/3e/a0895bbc.jpg","comment_is_top":false,"comment_ctime":1568712270,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5863679566","product_id":100025301,"comment_content":"老师你好，请教个watermark的问题：水印是为了解决数据出现延迟时，流处理程序要等待多久。那超过这个时间的数据就丢弃了吗？程序不会再处理他们了吗？比如水印设置30分钟，那31分钟到来的数据就不管了是吧？","like_count":2},{"had_liked":false,"id":100257,"user_name":"刘万里","can_delete":false,"product_type":"c1","uid":1504306,"ip_address":"","ucode":"B4E54997D1455F","user_header":"https://static001.geekbang.org/account/avatar/00/16/f4/32/c4550f66.jpg","comment_is_top":false,"comment_ctime":1559517840,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5854485136","product_id":100025301,"comment_content":"老师 您好，最近好久没用spark，有个问题请教一下，现在最新spark是否已经支持cep了","like_count":1},{"had_liked":false,"id":317058,"user_name":"YX","can_delete":false,"product_type":"c1","uid":1338489,"ip_address":"","ucode":"973F52053DDF8C","user_header":"https://static001.geekbang.org/account/avatar/00/14/6c/79/f098c11d.jpg","comment_is_top":false,"comment_ctime":1634653424,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1634653424","product_id":100025301,"comment_content":"比如说，如果定义水印是 10 分钟，数据 A 的事件时间是 1:00，数据 B 的事件时间是 1:10，由于数据传输发生了延迟，我们在 1:15 才收到了 A 和 B，那么我们将只处理数据 B 并更新结果，A 会被无视。<br>-----------------------------------<br>这里对水印的表述存在一定的不准确，应该是和具体收到的时间无关，而是「max event time seen by the engine 」系统当前最大的event time。","like_count":0},{"had_liked":false,"id":285082,"user_name":"天敌","can_delete":false,"product_type":"c1","uid":1059944,"ip_address":"","ucode":"CD29A622197197","user_header":"https://static001.geekbang.org/account/avatar/00/10/2c/68/c299bc71.jpg","comment_is_top":false,"comment_ctime":1616600875,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1616600875","product_id":100025301,"comment_content":"老师，数据集下载不了了，能再分享一下吗？","like_count":0},{"had_liked":false,"id":243370,"user_name":"之渊","can_delete":false,"product_type":"c1","uid":1876212,"ip_address":"","ucode":"02B9299DBB4881","user_header":"https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg","comment_is_top":false,"comment_ctime":1598076550,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1598076550","product_id":100025301,"comment_content":"java版本demo: 模拟的数据集。 https:&#47;&#47;gitee.com&#47;oumin12345&#47;daimademojihe&#47;tree&#47;master&#47;cloudx&#47;bigdata&#47;src&#47;main&#47;java&#47;test&#47;spark&#47;streaming<br>还是花了不少时间的。对于初学者来说还是值得写点demo的","like_count":0,"discussions":[{"author":{"id":1096652,"avatar":"https://static001.geekbang.org/account/avatar/00/10/bb/cc/fac12364.jpg","nickname":"xxx","note":"","ucode":"E79CEA70430449","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":542964,"discussion_content":"又删了？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640916478,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1934969,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg","nickname":"非同凡想","note":"","ucode":"713FD449A49D5A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":335833,"discussion_content":"403错误","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608347151,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":216325,"user_name":"北冥有鱼","can_delete":false,"product_type":"c1","uid":1592243,"ip_address":"","ucode":"1690734A1061F4","user_header":"https://static001.geekbang.org/account/avatar/00/18/4b/b3/51bb33f2.jpg","comment_is_top":false,"comment_ctime":1589242813,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1589242813","product_id":100025301,"comment_content":"老师，比如A和B表join，且A和B都是实时数据，A需要用到B表的历史全量数据，即通过A，保证能取到B中数据，要怎么处理呢？","like_count":0},{"had_liked":false,"id":102687,"user_name":"都市夜归人","can_delete":false,"product_type":"c1","uid":1071909,"ip_address":"","ucode":"DFF59BE3D80B42","user_header":"https://static001.geekbang.org/account/avatar/00/10/5b/25/d78cc1fe.jpg","comment_is_top":false,"comment_ctime":1560297301,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1560297301","product_id":100025301,"comment_content":"这部分数据有个不太直观的地方，那就是同一次出行会有两个记录，…<br>为何会出现两个记录？用一条记录也能记录出发和到达吧？","like_count":0},{"had_liked":false,"id":100527,"user_name":"Ming","can_delete":false,"product_type":"c1","uid":1516011,"ip_address":"","ucode":"69BB73B8AB7E3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/21/eb/bb2e7a3b.jpg","comment_is_top":false,"comment_ctime":1559571285,"is_pvip":false,"replies":[{"id":"42770","content":"Ming，感谢提问。<br><br>现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。<br><br>Outer join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。<br><br>由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。","user_name":"作者回复","comment_id":100527,"uid":"1386753","ip_address":"","utype":1,"ctime":1563949046,"user_name_real":"廿七"}],"discussion_count":2,"race_medal":0,"score":"1559571285","product_id":100025301,"comment_content":"我猜：<br><br>对于inner join来说，用不用watermark只是纯粹的一个性能考量，不影响单条数据的正确性，只影响最终分析的样本大小。<br>对于outer join来说，用watermark会影响单条数据正确性，所以在逻辑上看应该是不推荐的，除非会有内存泄漏的风险。<br><br>我倒是好奇为啥spark把这个特性叫水印","like_count":0,"discussions":[{"author":{"id":1386753,"avatar":"https://static001.geekbang.org/account/avatar/00/15/29/01/20caec2f.jpg","nickname":"Yeon","note":"","ucode":"ED3549F94EB36E","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452584,"discussion_content":"Ming，感谢提问。\n\n现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。\n\nOuter join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。\n\n由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563949046,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":203331,"discussion_content":"watermark的概念是一开始在dataflow model（millwheel）里面就提出来的，不过dataflow model里的定义应该是最早的还未被process的event的event-time，后来spark使用了“接收到的event中最大的event-time”，等效于“当process=ingest时，最晚的一个已经被process的event的event-time”，理论上比原概念的定义时间上向后移动（系统表现为更激进的定义过期事件）了因为已经被处理的事件可以很靠近当前时间，不过在实际中也基本是等效的，因为就像文中的例子一样，流处理因为注重时效性，一般不会存在系统已经处理的最晚事件和在队列中的最早事件相差太久远的情况，否则系统必然overload，而在事件接收延迟超过一般可接受的范围后，基本在业务逻辑中也会直接认为事件已经超过时效了。当然了，实际中因为还有late threshold，所以可以通过配置手动弥补这两个定义的offset，导致在实际情况中Impact更小","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1584023602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}