{"id":208142,"title":"17 | 神经网络基础：神经网络还是复合函数","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/NLP\">https://gitee.com/geektime-geekbang/NLP</a></p>","comments":[{"had_liked":false,"id":186099,"user_name":"qinsi","can_delete":false,"product_type":"c3","uid":1667175,"ip_address":"","ucode":"090D9C4068FF12","user_header":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","comment_is_top":false,"comment_ctime":1583757235,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"总结一下老师大致是说，神经网络本质是可微的复合函数，不要仅仅注意到激活函数而被带偏了。然而我个人的理解，激活函数带来的非线性特性也很重要，否则神经网络的表达能力与线性函数无疑","like_count":8},{"had_liked":false,"id":206503,"user_name":"余皇南","can_delete":false,"product_type":"c3","uid":1918922,"ip_address":"","ucode":"3CCED29D3D513D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/8P1a8D3WButrPQicebnKkrAiaI1lWUfZicWPtWfXHbm9Xv7qb1tkJ7eiaxVG2JfO8mLJt7AzmPXjn0MsgjKBWujFfQ/132","comment_is_top":false,"comment_ctime":1586871485,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"可以补充西瓜书的广义线性回归和Andrew Ng的链式法则推导，感觉老师的课比较精简建议给些补充材料","like_count":5},{"had_liked":false,"id":186163,"user_name":"人工智能混饭人","can_delete":false,"product_type":"c3","uid":1104354,"ip_address":"","ucode":"71CF953311BD81","user_header":"https://static001.geekbang.org/account/avatar/00/10/d9/e2/d75009ef.jpg","comment_is_top":false,"comment_ctime":1583768318,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"神经网络的本质是复合函数，因此可以通过链式法则进行求导训练网络（也叫反向传播，通过最小化损失函数，进而反向对未知参数求导）。激活函数时非线性，进而可以拟合较复杂的函数关系。","like_count":3},{"had_liked":false,"id":232862,"user_name":"Geek_02623b","can_delete":false,"product_type":"c3","uid":1940233,"ip_address":"","ucode":"42B104E9C2D45A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9b/09/994ec468.jpg","comment_is_top":false,"comment_ctime":1594132402,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"我感觉用logistic回归的思想来解释神经网络比较切合。以二分类为例：\nX表示样本${x_1,x_2,....}$；C表示类别${c_1=1,c_2=-1}$，$P$表示概率。\n已知如下：\n当$P(C_1|x_1)&#47;P(C_2|X_1)&gt;1$时$X_1$属于$C_1$类，\n当$P(C_1|x_1)&#47;P(C_2|X_1)&lt;1$时$X_1$属于$C_2$类.\n由于是二分类问题，可以将上式继续简化\n$P(C_1|x_1)&#47;(1-P(C_1|x_1))&gt;1$时$X_1$属于$C_1$类，\n$P(C_1|x_1)&#47;(1-P(C_1|x_1))&lt;1$时$X_1$属于$C_2$类，\n上述公式是分类问题的描述，但是这个公式还有一个问题需要处理，由函数$x&#47;(1-x)$图像可知，在0和1附近的值太小和太大将函数图像的波动掩盖了，所以需要加上对数，$ln{P(C_1|x_1)&#47;(1-P(C_1|x_1))}$。\n下面引入logitic regression(LR)和神经网络。\n1. LR(欢迎讨论，根据自己理解定义的)\n   经过上述的讨论我们知道了要拟合的目标即:$ln{P(C_1|x_1)&#47;(1-P(C_1|x_1))}$，使用$\\overrightarrow w \\overrightarrow x+\\overrightarrow b$来拟合的话就是LR模型，推导如下：\n   $ln{P(C_1|x_1)&#47;(1-P(C_1|x_1))}=\\overrightarrow w \\overrightarrow x+\\overrightarrow b$\n   $e^{ln{P(C_1|x_1)&#47;(1-P(C_1|x_1))}}=e^{\\overrightarrow w \\overrightarrow x+\\overrightarrow b}$\n   $P(C_1|x_1)&#47;(1-P(C_1|x_1))=e^{\\overrightarrow w \\overrightarrow x+\\overrightarrow b}$\n   $1&#47;(\\frac {1}{P(C_1|x_1)}-1)=e^{\\overrightarrow w \\overrightarrow x+\\overrightarrow b}$\n   $P(C_1|x_1)=\\frac {e^{\\overrightarrow w \\overrightarrow x+\\overrightarrow b}}{1+e^{\\overrightarrow w \\overrightarrow x+\\overrightarrow b}}$\n   由二分类可知，\n   $P(C_2|x_1)=\\frac {1}{1+e^{\\overrightarrow w \\overrightarrow x+\\overrightarrow b}}$\n   $P(C_2|x_1)+P(C_1|x_1)= 1$\n  \n   以上也可与推广到多分类。 \n\n2. 神经网络\n   神经网络也是在拟合$ln{P(C_1|x_1)&#47;(1-P(C_1|x_1))}$，只不过神经网络的需要拟合的函数${(\\overrightarrow w \\overrightarrow x+\\overrightarrow b)}^{*}$要复杂的多，加个星号表示的是广义的。\n","like_count":1},{"had_liked":false,"id":217846,"user_name":"kingsley","can_delete":false,"product_type":"c3","uid":1757369,"ip_address":"","ucode":"B5C86B601A25AA","user_header":"https://static001.geekbang.org/account/avatar/00/1a/d0/b9/f89ce558.jpg","comment_is_top":false,"comment_ctime":1589628275,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"神经网络分前向传播和反向传播两个过程。求解梯度是反向传播的过程。 矩阵求导可以参考matrix cookbook ","like_count":1},{"had_liked":false,"id":193080,"user_name":"JaneIDK","can_delete":false,"product_type":"c3","uid":1883313,"ip_address":"","ucode":"8F6E293767C62B","user_header":"https://static001.geekbang.org/account/avatar/00/1c/bc/b1/3ba14f09.jpg","comment_is_top":false,"comment_ctime":1584883593,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"这个神经网络是复合函数的角度以前没想过","like_count":1},{"had_liked":false,"id":191444,"user_name":"乾","can_delete":false,"product_type":"c3","uid":1730491,"ip_address":"","ucode":"DA2980C4202C9C","user_header":"https://static001.geekbang.org/account/avatar/00/1a/67/bb/c2d5982d.jpg","comment_is_top":false,"comment_ctime":1584773987,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"神经网络的数学本质和神经网络反响传播的本质是不同的。你讲的是反响传播，而不是网络","like_count":1},{"had_liked":false,"id":185803,"user_name":"王大伟","can_delete":false,"product_type":"c3","uid":1180955,"ip_address":"","ucode":"A3BDC4D7B94B0F","user_header":"https://static001.geekbang.org/account/avatar/00/12/05/1b/fc1aa0ac.jpg","comment_is_top":false,"comment_ctime":1583681402,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"老师，我想系统学习下向量求导，有推荐的教程吗","like_count":1},{"had_liked":false,"id":313234,"user_name":"sky","can_delete":false,"product_type":"c3","uid":2325199,"ip_address":"","ucode":"C7BA135845E1FF","user_header":"https://static001.geekbang.org/account/avatar/00/23/7a/cf/c42dd74e.jpg","comment_is_top":false,"comment_ctime":1632319550,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"神经网络数学本质是复合函数，并且可以通过链式求导来训练更新参数。","like_count":0},{"had_liked":false,"id":232838,"user_name":"Geek_02623b","can_delete":false,"product_type":"c3","uid":1940233,"ip_address":"","ucode":"42B104E9C2D45A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9b/09/994ec468.jpg","comment_is_top":false,"comment_ctime":1594127047,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"王老师，神经网络是表示的概率还是表示的函数？我看有些在用概率解释神经网络，计算损失函数的时候也会引入概率分布的概念。","like_count":0},{"had_liked":false,"id":224018,"user_name":"Geek_427d0c","can_delete":false,"product_type":"c3","uid":1876823,"ip_address":"","ucode":"D7A04138C4B8CB","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/X4ib36ADEvj76XaKD4OUY9k15KqWCAVCwibPicBxz6BBUfDrVolpYInn8zFOw3JBPtVw3L4Lkibaf2eLPemwGKzAXA/132","comment_is_top":false,"comment_ctime":1591256792,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100046401,"comment_content":"老师有没有推荐的数学、ML、AI书，已知编程、devops很厉害。","like_count":0},{"had_liked":false,"id":222844,"user_name":"皮特尔","can_delete":false,"product_type":"c3","uid":1017161,"ip_address":"","ucode":"313862C91DD325","user_header":"https://static001.geekbang.org/account/avatar/00/0f/85/49/585c69c4.jpg","comment_is_top":false,"comment_ctime":1590927838,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":3,"product_id":100046401,"comment_content":"神经网络的本质是复合函数，那么过拟合和欠拟合的数学本质是什么🤔呢？","like_count":0,"discussions":[{"author":{"id":1940233,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/9b/09/994ec468.jpg","nickname":"Geek_02623b","note":"","ucode":"42B104E9C2D45A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289514,"discussion_content":"神经网络训练过程是在假设空间中寻找一组参数的过程。可以借助于解线性方程组来理解。现在要求解AX-B=0，A是矩阵，对应与一组参数（权值），X是样本，B是偏置。当样本量远小于参数量的时候，即A的行小于列。这时方程有很多组解，过拟合就是在可行解中找到了一个泛化误差大的解，欠拟合是找到的解不在可行解空间内或者你的方程组列的不对。\n解决过拟合的方法是找到一个方法可以很好的在可行解中找到一个更优的解，比如Regularization Term。\n对于Regularization term最常见的是L1 Regularization term。它可以支持你在可行解中找到一组稀疏的参数（解）。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1594128119,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1017161,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/85/49/585c69c4.jpg","nickname":"皮特尔","note":"","ucode":"313862C91DD325","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1940233,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/9b/09/994ec468.jpg","nickname":"Geek_02623b","note":"","ucode":"42B104E9C2D45A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289935,"discussion_content":"多谢解答","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594275538,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":289514,"ip_address":"","group_id":0},"score":289935,"extra":""}]}]}]}