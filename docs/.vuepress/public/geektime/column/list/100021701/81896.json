{"id":81896,"title":"28丨EM聚类（上）：如何将一份菜等分给两个人？","content":"<p>今天我来带你学习EM聚类。EM的英文是Expectation Maximization，所以EM算法也叫最大期望算法。</p><p>我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？</p><p>很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子A中，然后再把剩余的分到碟子B中，再来观察碟子A和B里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子A和B里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。</p><p>你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是EM算法的过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/91/3c/91f617ac484a7de011108ae99bd8cb3c.jpg?wh=1842*711\" alt=\"\"></p><h2>EM算法的工作原理</h2><p>说到EM算法，我们先来看一个概念“最大似然”，英文是Maximum Likelihood，Likelihood代表可能性，所以最大似然也就是最大可能性的意思。</p><p>什么是最大似然呢？举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是最大似然的概念。</p><!-- [[[read_end]]] --><p>最大似然估计是什么呢？它指的就是一件事情已经发生了，然后反推更有可能是什么因素造成的。还是用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性。最大似然估计是一种通过已知结果，估计参数的方法。</p><p>那么EM算法是什么？它和最大似然估计又有什么关系呢？EM算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。</p><p>再回过来看下开头我给你举的分菜的这个例子，实际上最终我们想要的是碟子A和碟子B中菜的份量，你可以把它们理解为想要求得的<strong>模型参数</strong>。然后我们通过EM算法中的E步来进行观察，然后通过M步来进行调整A和B的参数，最后让碟子A和碟子B的参数不再发生变化为止。</p><p>实际我们遇到的问题，比分菜复杂。我再给你举个一个投掷硬币的例子，假设我们有A和B两枚硬币，我们做了5组实验，每组实验投掷10次，然后统计出现正面的次数，实验结果如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/c8/e4/c8b3f2489735a21ad86d05fb9e8c0de4.png?wh=471*197\" alt=\"\"><br>\n投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是A还是B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/91/0d/91eace1de7799a2d2d392908b462730d.png?wh=484*198\" alt=\"\"><br>\n我们现在想要求得硬币A和B出现正面次数的概率，可以直接求得：</p><p><img src=\"https://static001.geekbang.org/resource/image/51/d8/51ba3cc97ed9b786f4d95f937b207bd8.png?wh=320*56\" alt=\"\"></p><p>而实际情况是我不知道每次投掷的硬币是A还是B，那么如何求得硬币A和硬币B出现正面的概率呢？</p><p>这里就需要采用EM算法的思想。</p><p>1.初始化参数。我们假设硬币A和B的正面概率（随机指定）是θA=0.5和θB=0.9。</p><p>2.计算期望值。假设实验1投掷的是硬币A，那么正面次数为5的概率为：</p><p><img src=\"https://static001.geekbang.org/resource/image/09/e0/09babe7d1f543d6ff800005d556823e0.png?wh=199*44\" alt=\"\"><br>\n公式中的C(10,5)代表的是10个里面取5个的组合方式，也就是排列组合公式，0.5的5次方乘以0.5的5次方代表的是其中一次为5次为正面，5次为反面的概率，然后再乘以C(10,5)等于正面次数为5的概率。</p><p>假设实验1是投掷的硬币B ，那么正面次数为5的概率为：</p><p><img src=\"https://static001.geekbang.org/resource/image/7b/f6/7b1bab8bf4eecb0c55b34fb8049374f6.png?wh=212*39\" alt=\"\"><br>\n所以实验1更有可能投掷的是硬币A。</p><p>然后我们对实验2~5重复上面的计算过程，可以推理出来硬币顺序应该是{A，A，B，B，A}。</p><p>这个过程实际上是通过假设的参数来估计未知参数，即“每次投掷是哪枚硬币”。</p><p>3.通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA和θB。</p><p>然后一直重复第二步和第三步，直到参数不再发生变化。</p><p>简单总结下上面的步骤，你能看出EM算法中的E步骤就是通过旧的参数来计算隐藏变量。然后在M步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。</p><h2>EM聚类的工作原理</h2><p>上面你能看到EM算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是EM聚类的原理。</p><p>相比于K-Means算法，EM聚类更加灵活，比如下面这两种情况，K-Means会得到下面的聚类结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/ba/ca/bafc98deb68400100fde69a41ebc66ca.jpg?wh=2142*941\" alt=\"\"><br>\n因为K-Means是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而EM聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。</p><p>你可以把EM算法理解成为是一个框架，在这个框架中可以采用不同的模型来用EM进行求解。常用的EM聚类有GMM高斯混合模型和HMM隐马尔科夫模型。GMM（高斯混合模型）聚类就是EM聚类的一种。比如上面这两个图，可以采用GMM来进行聚类。</p><p>和K-Means一样，我们事先知道聚类的个数，但是不知道每个样本分别属于哪一类。通常，我们可以假设样本是符合高斯分布的（也就是正态分布）。每个高斯分布都属于这个模型的组成部分（component），要分成K类就相当于是K个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是E步骤。</p><p>再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即M步骤。反复EM步骤，直到每个组成部分的高斯分布参数不变为止。</p><p>这样也就相当于将样本按照GMM模型进行了EM聚类。</p><p><img src=\"https://static001.geekbang.org/resource/image/18/3b/18fe6407b90130e5e4fa74467b1d493b.jpg?wh=2035*901\" alt=\"\"></p><h2>总结</h2><p>EM算法相当于一个框架，你可以采用不同的模型来进行聚类，比如GMM（高斯混合模型），或者HMM（隐马尔科夫模型）来进行聚类。GMM是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而HMM用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM在自然语言处理和语音识别领域中有广泛的应用。</p><p>在EM这个框架中，E步骤相当于是通过初始化的参数来估计隐含变量。M步骤就是通过隐含变量反推来优化参数。最后通过EM步骤的迭代得到模型参数。</p><p>在这个过程里用到的一些数学公式这节课不进行展开。你需要重点理解EM算法的原理。通过上面举的炒菜的例子，你可以知道EM算法是一个不断观察和调整的过程。</p><p>通过求硬币正面概率的例子，你可以理解如何通过初始化参数来求隐含数据的过程，以及再通过求得的隐含数据来优化参数。</p><p>通过上面GMM图像聚类的例子，你可以知道很多K-Means解决不了的问题，EM聚类是可以解决的。在EM框架中，我们将潜在类别当做隐藏变量，样本看做观察值，把聚类问题转化为参数估计问题，最终把样本进行聚类。</p><p><img src=\"https://static001.geekbang.org/resource/image/d8/80/d839e80d911add15add41163fa03ee80.png?wh=677*307\" alt=\"\"><br>\n最后给你留两道思考题吧，你能用自己的话说一下EM算法的原理吗？EM聚类和K-Means聚类的相同和不同之处又有哪些？</p><p>欢迎你在评论区与我分享你的答案，也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事，一起来交流。</p><p></p>","neighbors":{"left":{"article_title":"27丨K-Means（下）：如何使用K-Means对图像进行分割？","id":81591},"right":{"article_title":"29丨EM聚类（下）：用EM算法对王者荣耀英雄进行划分","id":82333}},"comments":[{"had_liked":false,"id":68718,"user_name":"third","can_delete":false,"product_type":"c1","uid":1025114,"ip_address":"","ucode":"9A37408A834F0B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg","comment_is_top":false,"comment_ctime":1550585069,"is_pvip":false,"replies":[{"id":24704,"content":"例子举的不错，相同和不同之处理解也很到位，大家都可以看看。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802168,"ip_address":"","comment_id":68718,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"想起了一个故事，摘叶子\n要找到最大的叶子\n1.先心里大概有一个叶子大小的概念（初始化模型）\n2.在三分之一的的路程上，观察叶子大小，并修改对大小的评估（观察预期，并修改参数）\n3.在三分之二的路程上，验证自己对叶子大小模型的的评估（重复1,2过程）\n4.在最后的路程上，选择最大的叶子（重复1.2，直到参数不再改变）\n\n相同点\n1.EM，KMEANS，都是随机生成预期值，然后经过反复调整，获得最佳结果\n2.聚类个数清晰\n\n不同点\n1.EM是计算概率，KMeans是计算距离。\n计算概率，概率只要不为0，都有可能即样本是每一个类别都有可能\n计算距离，只有近的的票高，才有可能，即样本只能属于一个类别","like_count":52},{"had_liked":false,"id":67555,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":false,"comment_ctime":1550196572,"is_pvip":false,"replies":[{"id":24711,"content":"一软一硬这个说的很恰当！一个输出概率，一个输出明确的答案。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802620,"ip_address":"","comment_id":67555,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"em聚类和K均值的区别就是一个软一个硬，软的输出概率，硬的要给出答案。我理解的em聚类的过程是一个翻来覆去决策的过程，这种聚类方式是先确定一个初始化的参数，再反过来推算结果，看和自己期望的差距，又在翻回去调整。好就好在，你想要一个什么样的结果他都能慢慢给你调整出来","like_count":17,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439814,"discussion_content":"例子举的不错，相同和不同之处理解也很到位，大家都可以看看。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802168,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67652,"user_name":"梁林松","can_delete":false,"product_type":"c1","uid":1144766,"ip_address":"","ucode":"FA032C3B4E245E","user_header":"https://static001.geekbang.org/account/avatar/00/11/77/be/1f2409e8.jpg","comment_is_top":false,"comment_ctime":1550217393,"is_pvip":false,"replies":[{"id":64555,"content":"对的 很形象","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621678,"ip_address":"","comment_id":67652,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM 就好像炒菜，做汤，盐多了放水，味淡了再放盐，直到合适为止。然后，就能得出放盐和水的比例（参数）","like_count":13,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439246,"discussion_content":"一软一硬这个说的很恰当！一个输出概率，一个输出明确的答案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802620,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":71452,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551344768,"is_pvip":false,"replies":[{"id":64486,"content":"对的","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619974,"ip_address":"","comment_id":71452,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"文中抛硬币的例子，应该还要说明“5组实验，每组实验投掷10次，每组中只能抛同一枚硬币”。","like_count":11,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441158,"discussion_content":"对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619974,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2037505,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","nickname":"McKee Chen","note":"","ucode":"F74B76542FAB65","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340829,"discussion_content":"找到了这条评论，学习的过程中就会产生困惑，为什么每次只能是A或B中的一种。原来果然是少了限制条件。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610172563,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68472,"user_name":"松花皮蛋me","can_delete":false,"product_type":"c1","uid":1000054,"ip_address":"","ucode":"B0846CEEF6B0D1","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/76/256bbd43.jpg","comment_is_top":false,"comment_ctime":1550504566,"is_pvip":false,"replies":[{"id":24707,"content":"EM有自我更新的机制，就像K-Means一样，所以不用担心初始化参数，即使初始化参数不正确也会逐渐迭代出来结果。区别是在于迭代的次数，也就是运行的时间。这就好比把菜分到两个盘子中，一开始A盘很少，B盘非常多。这时候初始化参数并不理想，但是没有关系，EM机制通过参数估计，最终通过迭代会让两个盘子的分量一样多。只是迭代次数会略多一些。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802393,"ip_address":"","comment_id":68472,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"有同学说:核心是初始参数啊。如果一开始就错那就完了。这完全是错的，只不过增加了更新次数而已。","like_count":11,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439692,"discussion_content":"EM有自我更新的机制，就像K-Means一样，所以不用担心初始化参数，即使初始化参数不正确也会逐渐迭代出来结果。区别是在于迭代的次数，也就是运行的时间。这就好比把菜分到两个盘子中，一开始A盘很少，B盘非常多。这时候初始化参数并不理想，但是没有关系，EM机制通过参数估计，最终通过迭代会让两个盘子的分量一样多。只是迭代次数会略多一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802393,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67614,"user_name":"白夜","can_delete":false,"product_type":"c1","uid":1354449,"ip_address":"","ucode":"7AABFA7C04EA34","user_header":"https://static001.geekbang.org/account/avatar/00/14/aa/d1/076482f3.jpg","comment_is_top":false,"comment_ctime":1550210694,"is_pvip":false,"replies":[{"id":64561,"content":"对的","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621765,"ip_address":"","comment_id":67614,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM，聚类的个数是已知的，首先，预设初始化的参数，然后获得对应的结果，再通过结果计算参数，不断循环以上两步，直到收敛。属于软分类，每个样本有一定概率和一个聚类相关。\nK-Means，聚类的个数也是已知的，首先选定一个中心点，然后计算距离，获得新的中心点，重复，直到结果收敛。属于硬分类，每个样本都只有一个分类。","like_count":7,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439692,"discussion_content":"EM有自我更新的机制，就像K-Means一样，所以不用担心初始化参数，即使初始化参数不正确也会逐渐迭代出来结果。区别是在于迭代的次数，也就是运行的时间。这就好比把菜分到两个盘子中，一开始A盘很少，B盘非常多。这时候初始化参数并不理想，但是没有关系，EM机制通过参数估计，最终通过迭代会让两个盘子的分量一样多。只是迭代次数会略多一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802393,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":203228,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1586158564,"is_pvip":false,"replies":[{"id":103990,"content":"加油！！！","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1617292549,"ip_address":"","comment_id":203228,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM算法法第一次接触，要多看两遍。","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439279,"discussion_content":"对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272606,"user_name":"McKee Chen","can_delete":false,"product_type":"c1","uid":2037505,"ip_address":"","ucode":"F74B76542FAB65","user_header":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","comment_is_top":false,"comment_ctime":1610172172,"is_pvip":false,"replies":[{"id":101923,"content":"是的，抛硬币的试验，一般会假设只会出现正反2面的结果，默认不会出现硬币垂直的情况。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1614364275,"ip_address":"","comment_id":272606,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM聚类：\n对参数进行初始化，以此估计隐含变量，然后再反推初始参数，如果参数有变化，就不断重复上述过程，知道参数不变为止，此时也能得到隐含变量，即样本的聚类情况。\n\nEM聚类和K-Means聚类的区别：\nK-Means聚类是预先给定中心点个数，然后计算样本与中心点之间的距离来进行分类，通过不断迭代优化中心点，直至中心点不再发生变换，最后确定聚类情况。这种过程也称之为硬聚类算法。\n\n有一个疑问，对于掷硬币的五次实验，每次实验都是只选择A或B其中一个么？\n\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490804,"discussion_content":"加油！！！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617292549,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":143609,"user_name":"追梦","can_delete":false,"product_type":"c1","uid":1183831,"ip_address":"","ucode":"54C6E76E8FE033","user_header":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","comment_is_top":false,"comment_ctime":1571743068,"is_pvip":false,"replies":[{"id":63486,"content":"很好的总结整理","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577514186,"ip_address":"","comment_id":143609,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"想起了一个故事，摘叶子\n要找到最大的叶子\n1.先心里大概有一个叶子大小的概念（初始化模型）\n2.在三分之一的的路程上，观察叶子大小，并修改对大小的评估（观察预期，并修改参数）\n3.在三分之二的路程上，验证自己对叶子大小模型的的评估（重复1,2过程）\n4.在最后的路程上，选择最大的叶子（重复1.2，直到参数不再改变）\n\n相同点\n1.EM，KMEANS，都是随机生成预期值，然后经过反复调整，获得最佳结果\n2.聚类个数清晰\n\n不同点\n1.EM是计算概率，KMeans是计算距离。\n计算概率，概率只要不为0，都有可能即样本是每一个类别都有可能\n计算距离，只有近的的票高，才有可能，即样本只能属于一个类别\n\n\n“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“” 这个步骤是怎样的？\n\nA 5\nA 7\nB 8\nB 9\nA 4\nθA=(5+7+4)&#47;(10+10+10)\nθB=(8+9)&#47;(10+10)\n\n以留言方式暂时记录一下","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513323,"discussion_content":"是的，抛硬币的试验，一般会假设只会出现正反2面的结果，默认不会出现硬币垂直的情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614364275,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123516,"user_name":"FeiFei","can_delete":false,"product_type":"c1","uid":1045586,"ip_address":"","ucode":"01CD655DD4E56C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg","comment_is_top":false,"comment_ctime":1565690957,"is_pvip":false,"replies":[{"id":63874,"content":"对的 比喻的不错","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577601941,"ip_address":"","comment_id":123516,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM聚类算法，通过假定参数值，来推断未知隐含变量。再不断重复这个过程，至到隐含变量恒定不变时，得出假定参数的值。也就是实际的聚类分类的结果。\nK-Means：非黑即白\nEM：黑白通吃","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":471621,"discussion_content":"很好的总结整理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577514186,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68718,"user_name":"third","can_delete":false,"product_type":"c1","uid":1025114,"ip_address":"","ucode":"9A37408A834F0B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg","comment_is_top":false,"comment_ctime":1550585069,"is_pvip":false,"replies":[{"id":24704,"content":"例子举的不错，相同和不同之处理解也很到位，大家都可以看看。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802168,"ip_address":"","comment_id":68718,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"想起了一个故事，摘叶子\n要找到最大的叶子\n1.先心里大概有一个叶子大小的概念（初始化模型）\n2.在三分之一的的路程上，观察叶子大小，并修改对大小的评估（观察预期，并修改参数）\n3.在三分之二的路程上，验证自己对叶子大小模型的的评估（重复1,2过程）\n4.在最后的路程上，选择最大的叶子（重复1.2，直到参数不再改变）\n\n相同点\n1.EM，KMEANS，都是随机生成预期值，然后经过反复调整，获得最佳结果\n2.聚类个数清晰\n\n不同点\n1.EM是计算概率，KMeans是计算距离。\n计算概率，概率只要不为0，都有可能即样本是每一个类别都有可能\n计算距离，只有近的的票高，才有可能，即样本只能属于一个类别","like_count":52,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439814,"discussion_content":"例子举的不错，相同和不同之处理解也很到位，大家都可以看看。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802168,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67555,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":false,"comment_ctime":1550196572,"is_pvip":false,"replies":[{"id":24711,"content":"一软一硬这个说的很恰当！一个输出概率，一个输出明确的答案。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802620,"ip_address":"","comment_id":67555,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"em聚类和K均值的区别就是一个软一个硬，软的输出概率，硬的要给出答案。我理解的em聚类的过程是一个翻来覆去决策的过程，这种聚类方式是先确定一个初始化的参数，再反过来推算结果，看和自己期望的差距，又在翻回去调整。好就好在，你想要一个什么样的结果他都能慢慢给你调整出来","like_count":17,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439246,"discussion_content":"一软一硬这个说的很恰当！一个输出概率，一个输出明确的答案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802620,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67652,"user_name":"梁林松","can_delete":false,"product_type":"c1","uid":1144766,"ip_address":"","ucode":"FA032C3B4E245E","user_header":"https://static001.geekbang.org/account/avatar/00/11/77/be/1f2409e8.jpg","comment_is_top":false,"comment_ctime":1550217393,"is_pvip":false,"replies":[{"id":64555,"content":"对的 很形象","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621678,"ip_address":"","comment_id":67652,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM 就好像炒菜，做汤，盐多了放水，味淡了再放盐，直到合适为止。然后，就能得出放盐和水的比例（参数）","like_count":13,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439305,"discussion_content":"对的 很形象","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621678,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":71452,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551344768,"is_pvip":false,"replies":[{"id":64486,"content":"对的","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577619974,"ip_address":"","comment_id":71452,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"文中抛硬币的例子，应该还要说明“5组实验，每组实验投掷10次，每组中只能抛同一枚硬币”。","like_count":11,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439305,"discussion_content":"对的 很形象","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621678,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68472,"user_name":"松花皮蛋me","can_delete":false,"product_type":"c1","uid":1000054,"ip_address":"","ucode":"B0846CEEF6B0D1","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/76/256bbd43.jpg","comment_is_top":false,"comment_ctime":1550504566,"is_pvip":false,"replies":[{"id":24707,"content":"EM有自我更新的机制，就像K-Means一样，所以不用担心初始化参数，即使初始化参数不正确也会逐渐迭代出来结果。区别是在于迭代的次数，也就是运行的时间。这就好比把菜分到两个盘子中，一开始A盘很少，B盘非常多。这时候初始化参数并不理想，但是没有关系，EM机制通过参数估计，最终通过迭代会让两个盘子的分量一样多。只是迭代次数会略多一些。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802393,"ip_address":"","comment_id":68472,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"有同学说:核心是初始参数啊。如果一开始就错那就完了。这完全是错的，只不过增加了更新次数而已。","like_count":11,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441158,"discussion_content":"对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577619974,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2037505,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","nickname":"McKee Chen","note":"","ucode":"F74B76542FAB65","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":340829,"discussion_content":"找到了这条评论，学习的过程中就会产生困惑，为什么每次只能是A或B中的一种。原来果然是少了限制条件。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610172563,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67614,"user_name":"白夜","can_delete":false,"product_type":"c1","uid":1354449,"ip_address":"","ucode":"7AABFA7C04EA34","user_header":"https://static001.geekbang.org/account/avatar/00/14/aa/d1/076482f3.jpg","comment_is_top":false,"comment_ctime":1550210694,"is_pvip":false,"replies":[{"id":64561,"content":"对的","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621765,"ip_address":"","comment_id":67614,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM，聚类的个数是已知的，首先，预设初始化的参数，然后获得对应的结果，再通过结果计算参数，不断循环以上两步，直到收敛。属于软分类，每个样本有一定概率和一个聚类相关。\nK-Means，聚类的个数也是已知的，首先选定一个中心点，然后计算距离，获得新的中心点，重复，直到结果收敛。属于硬分类，每个样本都只有一个分类。","like_count":7,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439279,"discussion_content":"对的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":203228,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1586158564,"is_pvip":false,"replies":[{"id":103990,"content":"加油！！！","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1617292549,"ip_address":"","comment_id":203228,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM算法法第一次接触，要多看两遍。","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490804,"discussion_content":"加油！！！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617292549,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":272606,"user_name":"McKee Chen","can_delete":false,"product_type":"c1","uid":2037505,"ip_address":"","ucode":"F74B76542FAB65","user_header":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","comment_is_top":false,"comment_ctime":1610172172,"is_pvip":false,"replies":[{"id":101923,"content":"是的，抛硬币的试验，一般会假设只会出现正反2面的结果，默认不会出现硬币垂直的情况。","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1614364275,"ip_address":"","comment_id":272606,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM聚类：\n对参数进行初始化，以此估计隐含变量，然后再反推初始参数，如果参数有变化，就不断重复上述过程，知道参数不变为止，此时也能得到隐含变量，即样本的聚类情况。\n\nEM聚类和K-Means聚类的区别：\nK-Means聚类是预先给定中心点个数，然后计算样本与中心点之间的距离来进行分类，通过不断迭代优化中心点，直至中心点不再发生变换，最后确定聚类情况。这种过程也称之为硬聚类算法。\n\n有一个疑问，对于掷硬币的五次实验，每次实验都是只选择A或B其中一个么？\n\n","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513323,"discussion_content":"是的，抛硬币的试验，一般会假设只会出现正反2面的结果，默认不会出现硬币垂直的情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614364275,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":143609,"user_name":"追梦","can_delete":false,"product_type":"c1","uid":1183831,"ip_address":"","ucode":"54C6E76E8FE033","user_header":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","comment_is_top":false,"comment_ctime":1571743068,"is_pvip":false,"replies":[{"id":63486,"content":"很好的总结整理","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577514186,"ip_address":"","comment_id":143609,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"想起了一个故事，摘叶子\n要找到最大的叶子\n1.先心里大概有一个叶子大小的概念（初始化模型）\n2.在三分之一的的路程上，观察叶子大小，并修改对大小的评估（观察预期，并修改参数）\n3.在三分之二的路程上，验证自己对叶子大小模型的的评估（重复1,2过程）\n4.在最后的路程上，选择最大的叶子（重复1.2，直到参数不再改变）\n\n相同点\n1.EM，KMEANS，都是随机生成预期值，然后经过反复调整，获得最佳结果\n2.聚类个数清晰\n\n不同点\n1.EM是计算概率，KMeans是计算距离。\n计算概率，概率只要不为0，都有可能即样本是每一个类别都有可能\n计算距离，只有近的的票高，才有可能，即样本只能属于一个类别\n\n\n“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“” 这个步骤是怎样的？\n\nA 5\nA 7\nB 8\nB 9\nA 4\nθA=(5+7+4)&#47;(10+10+10)\nθB=(8+9)&#47;(10+10)\n\n以留言方式暂时记录一下","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":471621,"discussion_content":"很好的总结整理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577514186,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":123516,"user_name":"FeiFei","can_delete":false,"product_type":"c1","uid":1045586,"ip_address":"","ucode":"01CD655DD4E56C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg","comment_is_top":false,"comment_ctime":1565690957,"is_pvip":false,"replies":[{"id":63874,"content":"对的 比喻的不错","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577601941,"ip_address":"","comment_id":123516,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100021701,"comment_content":"EM聚类算法，通过假定参数值，来推断未知隐含变量。再不断重复这个过程，至到隐含变量恒定不变时，得出假定参数的值。也就是实际的聚类分类的结果。\nK-Means：非黑即白\nEM：黑白通吃","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":462706,"discussion_content":"对的 比喻的不错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577601941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89591,"user_name":"奔跑的徐胖子","can_delete":false,"product_type":"c1","uid":1233917,"ip_address":"","ucode":"CFB8A7C4F99D34","user_header":"https://static001.geekbang.org/account/avatar/00/12/d3/fd/41eb3ecc.jpg","comment_is_top":false,"comment_ctime":1556230581,"is_pvip":false,"replies":[{"id":64229,"content":"Good Sharing","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577614952,"ip_address":"","comment_id":89591,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"原理的话就拿老师的这个抛掷硬币的例子来看：\n1、初始的时候，我们并不知道1~5次试验抛掷的分别是A硬币还是B硬币，我们就先假设一下A、B正面向上的概率。\n2、通过我们假设的概率，我们根据1~5次实验中每次正面向上的频率，使用我们1中假设的A、B正面的概率来分别计算期望值。两个期望值比较哪个大，我们就觉得这次试验抛掷的是哪个硬币。\n3、我们通过2，就第一次将本来没有分类的试验（该次实验抛掷的是哪一个硬币）给分类了，但是这个结果是我们初始化一个随机的正面向上的概率来算出来的，不准确。\n4、我们把1、2、3的出来的初始的分类结果当做已知，通过全体数据来算一下此时A、B正面向上的概率（全体数据的频率），这样，我们就得到了类似2步骤中的正面向上的概率，这里就优化了A、B这面向上的概率（完善参数）。\n5、就这样一直重复2、3的过程，直到稳定为止","like_count":0},{"had_liked":false,"id":89589,"user_name":"奔跑的徐胖子","can_delete":false,"product_type":"c1","uid":1233917,"ip_address":"","ucode":"CFB8A7C4F99D34","user_header":"https://static001.geekbang.org/account/avatar/00/12/d3/fd/41eb3ecc.jpg","comment_is_top":false,"comment_ctime":1556229994,"is_pvip":false,"replies":[{"id":64228,"content":"很好的总结","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577614945,"ip_address":"","comment_id":89589,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"EM的原理，其实就拿这个老师给的硬币的例子来看。初始的时候，我们只有一堆数据，并不知道试验1~5分别抛掷的是哪一个硬币。这样，我们先随机一下A、B两枚硬币的正面出现的概率。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448185,"discussion_content":"Good Sharing","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577614952,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":69174,"user_name":"李沛欣","can_delete":false,"product_type":"c1","uid":1362695,"ip_address":"","ucode":"98874954230D95","user_header":"https://static001.geekbang.org/account/avatar/00/14/cb/07/e34220d6.jpg","comment_is_top":false,"comment_ctime":1550672893,"is_pvip":false,"replies":[{"id":64520,"content":"对的 很好的总结","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621199,"ip_address":"","comment_id":69174,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"今天的看完了。我理解的EM算法，是先估计一个大概率的可能参数，然后再根据数据不断进行调整，直到找到最终的确认参数。\n\n它主要有高斯模型和隐马尔科夫模型，前者在自然语言处理领域有很多应用。\n\n它和K-means都属于聚类算法，但是，EM属于软聚类，同一样本可能属于多个类别；而后者则属于硬聚类，一个样本只能属于一个类别。所以前者能够发现一些隐藏的数据。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439982,"discussion_content":"对的 很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621199,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67950,"user_name":"littlePerfect","can_delete":false,"product_type":"c1","uid":1243521,"ip_address":"","ucode":"99E8B9D004BE0D","user_header":"https://static001.geekbang.org/account/avatar/00/12/f9/81/54b1a5a8.jpg","comment_is_top":false,"comment_ctime":1550336273,"is_pvip":false,"replies":[{"id":24709,"content":"2月底会上线一个找工作面试的专题。在专栏的最后部分会有几节和工作面试相关的。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802518,"ip_address":"","comment_id":67950,"utype":2}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"陈老师什么时候会更新面试的内容？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439982,"discussion_content":"对的 很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621199,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67693,"user_name":"从未在此","can_delete":false,"product_type":"c1","uid":1354589,"ip_address":"","ucode":"5A4AA275D8EE9A","user_header":"https://static001.geekbang.org/account/avatar/00/14/ab/5d/430ed3b6.jpg","comment_is_top":false,"comment_ctime":1550223755,"is_pvip":false,"replies":[{"id":24710,"content":"不如担心，一个算法的强大在于它的鲁棒性，或者说它的机制价值会允许初始化参数存在误差。举个例子EM的核心是通过参数估计来完成聚类，如果你想要把菜平均分到两个盘子中，一开始盘子A的菜很少，B中的菜很多。同样没有关系，最后EM通过不断迭代会让两个盘子的菜量一样多，只是迭代的次数会多一些。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802551,"ip_address":"","comment_id":67693,"utype":2}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"核心是初始参数啊。如果一开始就错那就完了","like_count":0,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439436,"discussion_content":"2月底会上线一个找工作面试的专题。在专栏的最后部分会有几节和工作面试相关的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802518,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70159,"user_name":"黄智荣","can_delete":false,"product_type":"c1","uid":1027823,"ip_address":"","ucode":"3C84C8654CCB11","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/ef/cbb8d881.jpg","comment_is_top":false,"comment_ctime":1551017420,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“” 这个步骤是怎样的？\n\nA  5\nA  7\nB  8\nB  9\nA  4\nθA=(5+7+4)&#47;(10+10+10)\nθB=(8+9)&#47;(10+10)","like_count":21},{"had_liked":false,"id":71457,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551345670,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"to third：\n\n吴军老师说过，这种找最大叶子的问题，最优解最大概率会在37%的时候，而不是最后。","like_count":8},{"had_liked":false,"id":81020,"user_name":"FORWARD―MOUNT","can_delete":false,"product_type":"c1","uid":1357857,"ip_address":"","ucode":"CD8E9ECF882980","user_header":"https://static001.geekbang.org/account/avatar/00/14/b8/21/c03839f1.jpg","comment_is_top":false,"comment_ctime":1553777649,"is_pvip":false,"replies":null,"discussion_count":3,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"请问：\n\n通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。\n然后一直重复第二步和第三步，直到参数不再发生变化。\n\n\n怎么完善初始化参数？，急需解答。","like_count":7,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439331,"discussion_content":"不如担心，一个算法的强大在于它的鲁棒性，或者说它的机制价值会允许初始化参数存在误差。举个例子EM的核心是通过参数估计来完成聚类，如果你想要把菜平均分到两个盘子中，一开始盘子A的菜很少，B中的菜很多。同样没有关系，最后EM通过不断迭代会让两个盘子的菜量一样多，只是迭代的次数会多一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802551,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1624013,"avatar":"https://static001.geekbang.org/account/avatar/00/18/c7/cd/70c1bcb5.jpg","nickname":"goongoup","note":"","ucode":"E9C3B0BB32EC78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337153,"discussion_content":"那我一直想问，这两个盘子，相当于是一开始就分好的聚类类型吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608806174,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":83190,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1554455343,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"em算法是假定一个样本分布概率，然后根据最大似然估计进行聚类，然后根据聚类结果修正参数，直到结果不在变化，而kmeans算法则是根据随机确定初始点，根据欧式距离等算法来计算和初始点的距离，完成初始聚类，然后迭代直到聚类结果不发生变化。kmeans是计算硬聚类，em是软聚类。","like_count":5},{"had_liked":false,"id":83185,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1554454054,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"说的通俗一点啊，最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。\n例如：一个麻袋里有白球与黑球，但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现我抽到了8次黑球2次白球，我要求最有可能的黑白球之间的比例时，就采取最大似然估计法： 我假设我抽到黑球的概率为p,那得出8次黑球2次白球这个结果的概率为：\nP(黑=8)=p^8*（1-p）^2,现在我想要得出p是多少啊，很简单，使得P(黑=8)最大的p就是我要求的结果，接下来求导的的过程就是求极值的过程啦。\n可能你会有疑问，为什么要ln一下呢，这是因为ln把乘法变成加法了，且不会改变极值的位置（单调性保持一致嘛）这样求导会方便很多~","like_count":4,"discussions":[{"author":{"id":2103790,"avatar":"","nickname":"Geek_yang","note":"","ucode":"508A702B91B441","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394556,"discussion_content":"P(黑=8)=C（10,8）p^8*（1-p）^2","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631936714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89591,"user_name":"奔跑的徐胖子","can_delete":false,"product_type":"c1","uid":1233917,"ip_address":"","ucode":"CFB8A7C4F99D34","user_header":"https://static001.geekbang.org/account/avatar/00/12/d3/fd/41eb3ecc.jpg","comment_is_top":false,"comment_ctime":1556230581,"is_pvip":false,"replies":[{"id":64229,"content":"Good Sharing","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577614952,"ip_address":"","comment_id":89591,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"原理的话就拿老师的这个抛掷硬币的例子来看：\n1、初始的时候，我们并不知道1~5次试验抛掷的分别是A硬币还是B硬币，我们就先假设一下A、B正面向上的概率。\n2、通过我们假设的概率，我们根据1~5次实验中每次正面向上的频率，使用我们1中假设的A、B正面的概率来分别计算期望值。两个期望值比较哪个大，我们就觉得这次试验抛掷的是哪个硬币。\n3、我们通过2，就第一次将本来没有分类的试验（该次实验抛掷的是哪一个硬币）给分类了，但是这个结果是我们初始化一个随机的正面向上的概率来算出来的，不准确。\n4、我们把1、2、3的出来的初始的分类结果当做已知，通过全体数据来算一下此时A、B正面向上的概率（全体数据的频率），这样，我们就得到了类似2步骤中的正面向上的概率，这里就优化了A、B这面向上的概率（完善参数）。\n5、就这样一直重复2、3的过程，直到稳定为止","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448185,"discussion_content":"Good Sharing","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577614952,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89589,"user_name":"奔跑的徐胖子","can_delete":false,"product_type":"c1","uid":1233917,"ip_address":"","ucode":"CFB8A7C4F99D34","user_header":"https://static001.geekbang.org/account/avatar/00/12/d3/fd/41eb3ecc.jpg","comment_is_top":false,"comment_ctime":1556229994,"is_pvip":false,"replies":[{"id":64228,"content":"很好的总结","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577614945,"ip_address":"","comment_id":89589,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"EM的原理，其实就拿这个老师给的硬币的例子来看。初始的时候，我们只有一堆数据，并不知道试验1~5分别抛掷的是哪一个硬币。这样，我们先随机一下A、B两枚硬币的正面出现的概率。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448184,"discussion_content":"很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577614945,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":69174,"user_name":"李沛欣","can_delete":false,"product_type":"c1","uid":1362695,"ip_address":"","ucode":"98874954230D95","user_header":"https://static001.geekbang.org/account/avatar/00/14/cb/07/e34220d6.jpg","comment_is_top":false,"comment_ctime":1550672893,"is_pvip":false,"replies":[{"id":64520,"content":"对的 很好的总结","user_name":"作者回复","user_name_real":"cy","uid":1306094,"ctime":1577621199,"ip_address":"","comment_id":69174,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"今天的看完了。我理解的EM算法，是先估计一个大概率的可能参数，然后再根据数据不断进行调整，直到找到最终的确认参数。\n\n它主要有高斯模型和隐马尔科夫模型，前者在自然语言处理领域有很多应用。\n\n它和K-means都属于聚类算法，但是，EM属于软聚类，同一样本可能属于多个类别；而后者则属于硬聚类，一个样本只能属于一个类别。所以前者能够发现一些隐藏的数据。","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448184,"discussion_content":"很好的总结","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577614945,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67950,"user_name":"littlePerfect","can_delete":false,"product_type":"c1","uid":1243521,"ip_address":"","ucode":"99E8B9D004BE0D","user_header":"https://static001.geekbang.org/account/avatar/00/12/f9/81/54b1a5a8.jpg","comment_is_top":false,"comment_ctime":1550336273,"is_pvip":false,"replies":[{"id":24709,"content":"2月底会上线一个找工作面试的专题。在专栏的最后部分会有几节和工作面试相关的。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802518,"ip_address":"","comment_id":67950,"utype":2}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"陈老师什么时候会更新面试的内容？","like_count":0,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439436,"discussion_content":"2月底会上线一个找工作面试的专题。在专栏的最后部分会有几节和工作面试相关的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802518,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67693,"user_name":"从未在此","can_delete":false,"product_type":"c1","uid":1354589,"ip_address":"","ucode":"5A4AA275D8EE9A","user_header":"https://static001.geekbang.org/account/avatar/00/14/ab/5d/430ed3b6.jpg","comment_is_top":false,"comment_ctime":1550223755,"is_pvip":false,"replies":[{"id":24710,"content":"不如担心，一个算法的强大在于它的鲁棒性，或者说它的机制价值会允许初始化参数存在误差。举个例子EM的核心是通过参数估计来完成聚类，如果你想要把菜平均分到两个盘子中，一开始盘子A的菜很少，B中的菜很多。同样没有关系，最后EM通过不断迭代会让两个盘子的菜量一样多，只是迭代的次数会多一些。","user_name":"编辑回复","user_name_real":"何昌梅","uid":1165037,"ctime":1550802551,"ip_address":"","comment_id":67693,"utype":2}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"核心是初始参数啊。如果一开始就错那就完了","like_count":0,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439331,"discussion_content":"不如担心，一个算法的强大在于它的鲁棒性，或者说它的机制价值会允许初始化参数存在误差。举个例子EM的核心是通过参数估计来完成聚类，如果你想要把菜平均分到两个盘子中，一开始盘子A的菜很少，B中的菜很多。同样没有关系，最后EM通过不断迭代会让两个盘子的菜量一样多，只是迭代的次数会多一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550802551,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1624013,"avatar":"https://static001.geekbang.org/account/avatar/00/18/c7/cd/70c1bcb5.jpg","nickname":"goongoup","note":"","ucode":"E9C3B0BB32EC78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337153,"discussion_content":"那我一直想问，这两个盘子，相当于是一开始就分好的聚类类型吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608806174,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70159,"user_name":"黄智荣","can_delete":false,"product_type":"c1","uid":1027823,"ip_address":"","ucode":"3C84C8654CCB11","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/ef/cbb8d881.jpg","comment_is_top":false,"comment_ctime":1551017420,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“” 这个步骤是怎样的？\n\nA  5\nA  7\nB  8\nB  9\nA  4\nθA=(5+7+4)&#47;(10+10+10)\nθB=(8+9)&#47;(10+10)","like_count":21},{"had_liked":false,"id":71457,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1551345670,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"to third：\n\n吴军老师说过，这种找最大叶子的问题，最优解最大概率会在37%的时候，而不是最后。","like_count":8},{"had_liked":false,"id":81020,"user_name":"FORWARD―MOUNT","can_delete":false,"product_type":"c1","uid":1357857,"ip_address":"","ucode":"CD8E9ECF882980","user_header":"https://static001.geekbang.org/account/avatar/00/14/b8/21/c03839f1.jpg","comment_is_top":false,"comment_ctime":1553777649,"is_pvip":false,"replies":null,"discussion_count":3,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"请问：\n\n通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。\n然后一直重复第二步和第三步，直到参数不再发生变化。\n\n\n怎么完善初始化参数？，急需解答。","like_count":7,"discussions":[{"author":{"id":1022786,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9b/42/aeb79b35.jpg","nickname":"Ling","note":"","ucode":"EE15D1ABDB073B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":52750,"discussion_content":"猜测到结果以后，就能得到 A、B 的新概率。比如说，第一次迭代后，结果 是{A, A, B, B, A}，那么 A 面出现正面的概率就是 (5 + 7 + 4) / 30 = 8 / 15。同理，此时 B 面出现正面的概率是 (8 + 9) / 20 = 17/20。把这个新概率代入 2、3 步，继续求概率，直至两者的概率不再发生变化。此时可认为算法已经完成工作。","likes_number":11,"is_delete":false,"is_hidden":false,"ctime":1574082807,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1115232,"avatar":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","nickname":"Fan","note":"","ucode":"3BF28670FD9407","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385637,"discussion_content":"厉害，文中这个也不说清楚。好懵的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627189975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1022786,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9b/42/aeb79b35.jpg","nickname":"Ling","note":"","ucode":"EE15D1ABDB073B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":52754,"discussion_content":"可以参考“迭代法”。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574082920,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":83190,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1554455343,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"em算法是假定一个样本分布概率，然后根据最大似然估计进行聚类，然后根据聚类结果修正参数，直到结果不在变化，而kmeans算法则是根据随机确定初始点，根据欧式距离等算法来计算和初始点的距离，完成初始聚类，然后迭代直到聚类结果不发生变化。kmeans是计算硬聚类，em是软聚类。","like_count":5},{"had_liked":false,"id":83185,"user_name":"滨滨","can_delete":false,"product_type":"c1","uid":1334567,"ip_address":"","ucode":"881EFA798BEE34","user_header":"https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg","comment_is_top":false,"comment_ctime":1554454054,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100021701,"comment_content":"说的通俗一点啊，最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。\n例如：一个麻袋里有白球与黑球，但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现我抽到了8次黑球2次白球，我要求最有可能的黑白球之间的比例时，就采取最大似然估计法： 我假设我抽到黑球的概率为p,那得出8次黑球2次白球这个结果的概率为：\nP(黑=8)=p^8*（1-p）^2,现在我想要得出p是多少啊，很简单，使得P(黑=8)最大的p就是我要求的结果，接下来求导的的过程就是求极值的过程啦。\n可能你会有疑问，为什么要ln一下呢，这是因为ln把乘法变成加法了，且不会改变极值的位置（单调性保持一致嘛）这样求导会方便很多~","like_count":4,"discussions":[{"author":{"id":1022786,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9b/42/aeb79b35.jpg","nickname":"Ling","note":"","ucode":"EE15D1ABDB073B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":52750,"discussion_content":"猜测到结果以后，就能得到 A、B 的新概率。比如说，第一次迭代后，结果 是{A, A, B, B, A}，那么 A 面出现正面的概率就是 (5 + 7 + 4) / 30 = 8 / 15。同理，此时 B 面出现正面的概率是 (8 + 9) / 20 = 17/20。把这个新概率代入 2、3 步，继续求概率，直至两者的概率不再发生变化。此时可认为算法已经完成工作。","likes_number":11,"is_delete":false,"is_hidden":false,"ctime":1574082807,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1115232,"avatar":"https://static001.geekbang.org/account/avatar/00/11/04/60/64d166b6.jpg","nickname":"Fan","note":"","ucode":"3BF28670FD9407","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385637,"discussion_content":"厉害，文中这个也不说清楚。好懵的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627189975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1022786,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9b/42/aeb79b35.jpg","nickname":"Ling","note":"","ucode":"EE15D1ABDB073B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":52754,"discussion_content":"可以参考“迭代法”。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574082920,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":327836,"user_name":"烟雨平生","can_delete":false,"product_type":"c1","uid":2087212,"ip_address":"","ucode":"C34CD9E1D990E3","user_header":"https://static001.geekbang.org/account/avatar/00/1f/d9/2c/296b02b0.jpg","comment_is_top":false,"comment_ctime":1640321580,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"老师您好，请问有什么书对于数学原理讲的比较透彻的，个人人为机器学习算法就难在数学，像西瓜书数学推导少，为什么用这样的统计学方法也没有解释，求推荐，可以英文","like_count":1},{"had_liked":false,"id":95946,"user_name":"对三要不起","can_delete":false,"product_type":"c1","uid":1238261,"ip_address":"","ucode":"175BEA32A14643","user_header":"https://static001.geekbang.org/account/avatar/00/12/e4/f5/e722d833.jpg","comment_is_top":false,"comment_ctime":1558268510,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"TO FORWARD―MOUNT\n【通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。\n然后一直重复第二步和第三步，直到参数不再发生变化。】\n\n这个步骤就是通过第一次随机，我们一直知道了顺序了可能是{A A B B A}，然后就可以算出A和B投正面的概率，再通过算出来的这个新概率（之前是随即指定的），再去模拟一遍五组硬币，可能这次模拟出来的就不是{A A B B A}了，重复这个步骤直到模拟出来的五枚硬币不再改变。此时的概率就是A和B 投正面的概率。\n","like_count":1},{"had_liked":false,"id":68550,"user_name":"老师 冯","can_delete":false,"product_type":"c1","uid":1075832,"ip_address":"","ucode":"3882D6A45A0887","user_header":"https://static001.geekbang.org/account/avatar/00/10/6a/78/7ab6f411.jpg","comment_is_top":false,"comment_ctime":1550542515,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“”  这个步骤是怎样的？跪求解答\n\n\n\n","like_count":1},{"had_liked":false,"id":364734,"user_name":"明亮","can_delete":false,"product_type":"c1","uid":1071415,"ip_address":"福建","ucode":"B77A6BBAACBA03","user_header":"https://static001.geekbang.org/account/avatar/00/10/59/37/8028950c.jpg","comment_is_top":false,"comment_ctime":1671426014,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"不讲清楚具体实验规则，要读者猜，真是服了","like_count":0},{"had_liked":false,"id":296432,"user_name":"进击的矮子","can_delete":false,"product_type":"c1","uid":2218372,"ip_address":"","ucode":"39173CF9584C51","user_header":"https://static001.geekbang.org/account/avatar/00/21/d9/84/f1b10393.jpg","comment_is_top":false,"comment_ctime":1622974089,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"给2个K-means失效的例子。\n第一个：交错半圆\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nx,y=datasets.make_moons(n_samples=500, shuffle=True, noise=None, random_state=None)\nkm=KMeans(n_clusters=2,\n    init=&#39;k-means++&#39;,\n    n_init=10,\n    max_iter=300,\n    tol=0.0001,\n    precompute_distances=&#39;deprecated&#39;,\n    verbose=0,\n    random_state=None,\n    copy_x=True,\n    n_jobs=&#39;deprecated&#39;,\n    algorithm=&#39;auto&#39;,)\n\nlabel=km.fit_predict(x)\n\nfig=plt.figure()\naxes = fig.add_subplot(121)\nfor idx,lab in enumerate(y):\n    if lab==0:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nax2=fig.add_subplot(122)\nfor idx,lab in enumerate(label):\n    if lab==0:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nplt.show()\n第二个：大小圆\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nx,y=datasets.make_circles(n_samples=200, shuffle=True, noise=None, random_state=None,\n                 factor=.8)\nkm=KMeans(n_clusters=2,\n    init=&#39;k-means++&#39;,\n    n_init=10,\n    max_iter=300,\n    tol=0.0001,\n    precompute_distances=&#39;deprecated&#39;,\n    verbose=0,\n    random_state=None,\n    copy_x=True,\n    n_jobs=&#39;deprecated&#39;,\n    algorithm=&#39;auto&#39;,)\n\nlabel=km.fit_predict(x)\n\nfig=plt.figure()\naxes = fig.add_subplot(121)\nfor idx,lab in enumerate(y):\n    if lab==0:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nax2=fig.add_subplot(122)\nfor idx,lab in enumerate(label):\n    if lab==0:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nplt.show()","like_count":0},{"had_liked":false,"id":288402,"user_name":"完美坚持","can_delete":false,"product_type":"c1","uid":1919541,"ip_address":"","ucode":"AE0261D8DDEF64","user_header":"https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg","comment_is_top":false,"comment_ctime":1618455830,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"EM聚类，首先给每个样本点人为设定一个类别，设定好之后就可以计算每个类别的样本点分布的特征参数（E-step）。根据得到的特诊参数决定的分布，可以将样本点重新归类（相当于重新设定参数，M-step），至此又可以根据新的类别划分重复上述步骤，直到每个样本点的类别划分不再有大的变化","like_count":0},{"had_liked":false,"id":69966,"user_name":"Geek_hve78z","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1550919284,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"1、 EM 算法的原理？\n当我们需要从样本观察数据中，找出样本的模型参数。 但是问题含有未观察到的隐含数据，这时采用EM算法。\n在EM算法的Expectation步，先猜想隐含数据，接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数。（EM算法的Maximization步)。\n我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。\n2、EM 聚类和 K-Means 聚类的相同和不同之处又有哪些？\nk-means 计算过程：\n1）随机选择k个类簇的中心\n2）计算每一个样本点到所有类簇中心的距离，选择最小距离作为该样本的类簇\n3）重新计算所有类簇的中心坐标，直到达到某种停止条件（迭代次数&#47;簇中心收敛&#47;最小平方误差）","like_count":0},{"had_liked":false,"id":68560,"user_name":"深白浅黑","can_delete":false,"product_type":"c1","uid":1123923,"ip_address":"","ucode":"DCCAA31DE8B127","user_header":"https://static001.geekbang.org/account/avatar/00/11/26/53/60fe31fb.jpg","comment_is_top":false,"comment_ctime":1550544861,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"原理哪里都有，还是需要结合实战！\n个人觉得，如果从数学定义角度出发，会更容易对算法原理进行理解。\nEM算法是求解隐含参数的算法，依据算法推导过程，可以视为求局部最优解的方法，可以归属为求解凸函数的问题。\nhttps:&#47;&#47;www.cnblogs.com&#47;bigmoyan&#47;p&#47;4550375.html\n","like_count":0},{"had_liked":false,"id":327836,"user_name":"烟雨平生","can_delete":false,"product_type":"c1","uid":2087212,"ip_address":"","ucode":"C34CD9E1D990E3","user_header":"https://static001.geekbang.org/account/avatar/00/1f/d9/2c/296b02b0.jpg","comment_is_top":false,"comment_ctime":1640321580,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"老师您好，请问有什么书对于数学原理讲的比较透彻的，个人人为机器学习算法就难在数学，像西瓜书数学推导少，为什么用这样的统计学方法也没有解释，求推荐，可以英文","like_count":1},{"had_liked":false,"id":95946,"user_name":"对三要不起","can_delete":false,"product_type":"c1","uid":1238261,"ip_address":"","ucode":"175BEA32A14643","user_header":"https://static001.geekbang.org/account/avatar/00/12/e4/f5/e722d833.jpg","comment_is_top":false,"comment_ctime":1558268510,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"TO FORWARD―MOUNT\n【通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。\n然后一直重复第二步和第三步，直到参数不再发生变化。】\n\n这个步骤就是通过第一次随机，我们一直知道了顺序了可能是{A A B B A}，然后就可以算出A和B投正面的概率，再通过算出来的这个新概率（之前是随即指定的），再去模拟一遍五组硬币，可能这次模拟出来的就不是{A A B B A}了，重复这个步骤直到模拟出来的五枚硬币不再改变。此时的概率就是A和B 投正面的概率。\n","like_count":1},{"had_liked":false,"id":68550,"user_name":"老师 冯","can_delete":false,"product_type":"c1","uid":1075832,"ip_address":"","ucode":"3882D6A45A0887","user_header":"https://static001.geekbang.org/account/avatar/00/10/6a/78/7ab6f411.jpg","comment_is_top":false,"comment_ctime":1550542515,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“”  这个步骤是怎样的？跪求解答\n\n\n\n","like_count":1},{"had_liked":false,"id":364734,"user_name":"明亮","can_delete":false,"product_type":"c1","uid":1071415,"ip_address":"福建","ucode":"B77A6BBAACBA03","user_header":"https://static001.geekbang.org/account/avatar/00/10/59/37/8028950c.jpg","comment_is_top":false,"comment_ctime":1671426014,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"不讲清楚具体实验规则，要读者猜，真是服了","like_count":0},{"had_liked":false,"id":296432,"user_name":"进击的矮子","can_delete":false,"product_type":"c1","uid":2218372,"ip_address":"","ucode":"39173CF9584C51","user_header":"https://static001.geekbang.org/account/avatar/00/21/d9/84/f1b10393.jpg","comment_is_top":false,"comment_ctime":1622974089,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"给2个K-means失效的例子。\n第一个：交错半圆\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nx,y=datasets.make_moons(n_samples=500, shuffle=True, noise=None, random_state=None)\nkm=KMeans(n_clusters=2,\n    init=&#39;k-means++&#39;,\n    n_init=10,\n    max_iter=300,\n    tol=0.0001,\n    precompute_distances=&#39;deprecated&#39;,\n    verbose=0,\n    random_state=None,\n    copy_x=True,\n    n_jobs=&#39;deprecated&#39;,\n    algorithm=&#39;auto&#39;,)\n\nlabel=km.fit_predict(x)\n\nfig=plt.figure()\naxes = fig.add_subplot(121)\nfor idx,lab in enumerate(y):\n    if lab==0:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nax2=fig.add_subplot(122)\nfor idx,lab in enumerate(label):\n    if lab==0:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nplt.show()\n第二个：大小圆\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nx,y=datasets.make_circles(n_samples=200, shuffle=True, noise=None, random_state=None,\n                 factor=.8)\nkm=KMeans(n_clusters=2,\n    init=&#39;k-means++&#39;,\n    n_init=10,\n    max_iter=300,\n    tol=0.0001,\n    precompute_distances=&#39;deprecated&#39;,\n    verbose=0,\n    random_state=None,\n    copy_x=True,\n    n_jobs=&#39;deprecated&#39;,\n    algorithm=&#39;auto&#39;,)\n\nlabel=km.fit_predict(x)\n\nfig=plt.figure()\naxes = fig.add_subplot(121)\nfor idx,lab in enumerate(y):\n    if lab==0:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        axes.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nax2=fig.add_subplot(122)\nfor idx,lab in enumerate(label):\n    if lab==0:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)\n    else:\n        ax2.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)\nplt.show()","like_count":0},{"had_liked":false,"id":288402,"user_name":"完美坚持","can_delete":false,"product_type":"c1","uid":1919541,"ip_address":"","ucode":"AE0261D8DDEF64","user_header":"https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg","comment_is_top":false,"comment_ctime":1618455830,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"EM聚类，首先给每个样本点人为设定一个类别，设定好之后就可以计算每个类别的样本点分布的特征参数（E-step）。根据得到的特诊参数决定的分布，可以将样本点重新归类（相当于重新设定参数，M-step），至此又可以根据新的类别划分重复上述步骤，直到每个样本点的类别划分不再有大的变化","like_count":0},{"had_liked":false,"id":69966,"user_name":"Geek_hve78z","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1550919284,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"1、 EM 算法的原理？\n当我们需要从样本观察数据中，找出样本的模型参数。 但是问题含有未观察到的隐含数据，这时采用EM算法。\n在EM算法的Expectation步，先猜想隐含数据，接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数。（EM算法的Maximization步)。\n我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。\n2、EM 聚类和 K-Means 聚类的相同和不同之处又有哪些？\nk-means 计算过程：\n1）随机选择k个类簇的中心\n2）计算每一个样本点到所有类簇中心的距离，选择最小距离作为该样本的类簇\n3）重新计算所有类簇的中心坐标，直到达到某种停止条件（迭代次数&#47;簇中心收敛&#47;最小平方误差）","like_count":0},{"had_liked":false,"id":68560,"user_name":"深白浅黑","can_delete":false,"product_type":"c1","uid":1123923,"ip_address":"","ucode":"DCCAA31DE8B127","user_header":"https://static001.geekbang.org/account/avatar/00/11/26/53/60fe31fb.jpg","comment_is_top":false,"comment_ctime":1550544861,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100021701,"comment_content":"原理哪里都有，还是需要结合实战！\n个人觉得，如果从数学定义角度出发，会更容易对算法原理进行理解。\nEM算法是求解隐含参数的算法，依据算法推导过程，可以视为求局部最优解的方法，可以归属为求解凸函数的问题。\nhttps:&#47;&#47;www.cnblogs.com&#47;bigmoyan&#47;p&#47;4550375.html\n","like_count":0}]}