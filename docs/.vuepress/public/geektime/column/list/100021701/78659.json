{"id":78659,"title":"18丨决策树（中）：CART，一棵是回归树，另一棵是分类树","content":"<p>上节课我们讲了决策树，基于信息度量的不同方式，我们可以把决策树分为ID3算法、C4.5算法和CART算法。今天我来带你学习CART算法。CART算法，英文全称叫做Classification And Regression Tree，中文叫做分类回归树。ID3和C4.5算法可以生成二叉树或多叉树，而CART只支持二叉树。同时CART决策树比较特殊，既可以作分类树，又可以作回归树。</p><p>那么你首先需要了解的是，什么是分类树，什么是回归树呢？</p><p>我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。</p><p><img src=\"https://static001.geekbang.org/resource/image/af/cf/af89317aa55ac3b9f068b0f370fcb9cf.png?wh=624*259\" alt=\"\"><br>\n分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p><h2>CART分类树的工作流程</h2><p>通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。在属性选择上，我们是通过统计“不纯度”来做判断的，ID3是基于信息增益做判断，C4.5在ID3的基础上做了改进，提出了信息增益率的概念。实际上CART分类树与C4.5算法类似，只是属性选择的指标采用的是基尼系数。</p><!-- [[[read_end]]] --><p>你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于0.4的时候，说明财富差异悬殊。基尼系数在0.2-0.4之间说明分配合理，财富差距不大。</p><p>基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以CART算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。</p><p>我们接下来详解了解一下基尼系数。基尼系数不好懂，你最好跟着例子一起手动计算下。</p><p>假设t为节点，那么该节点的GINI系数的计算公式为：</p><p><img src=\"https://static001.geekbang.org/resource/image/f9/89/f9bb4cce5b895499cabc714eb372b089.png?wh=463*94\" alt=\"\"><br>\n这里p(Ck|t)表示节点t属于类别Ck的概率，节点t的基尼系数为1减去各类别Ck概率平方和。</p><p>通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：</p><p>集合1：6个都去打篮球；</p><p>集合2：3个去打篮球，3个不去打篮球。</p><p>针对集合1，所有人都去打篮球，所以p(Ck|t)=1，因此GINI(t)=1-1=0。</p><p>针对集合2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5*0.5+0.5*0.5）=0.5。</p><p>通过两个基尼系数你可以看出，集合1的基尼系数最小，也证明样本最稳定，而集合2的样本不稳定性更大。</p><p>在CART算法中，基于基尼系数对特征属性进行二元分裂，假设属性A将节点D划分成了D1和D2，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/69/9a/69a90a43146898150a0de0811c6fef9a.jpg?wh=1854*887\" alt=\"\"><br>\n节点D的基尼系数等于子节点D1和D2的归一化基尼系数之和，用公式表示为：</p><p><img src=\"https://static001.geekbang.org/resource/image/10/1e/107fed838cb75df62eb149499db20c1e.png?wh=571*88\" alt=\"\"><br>\n归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点D中的比例。</p><p>上面我们已经计算了集合D1和集合D2的GINI系数，得到：<br>\n<img src=\"https://static001.geekbang.org/resource/image/aa/0c/aa423c65b32bded13212b7e20fb65a0c.png?wh=354*91\" alt=\"\"><br>\n<img src=\"https://static001.geekbang.org/resource/image/09/77/092a0ea87aabc5da482ff8a992691b77.png?wh=366*80\" alt=\"\"></p><p>所以在属性A的划分下，节点D的基尼系数为：</p><p><img src=\"https://static001.geekbang.org/resource/image/3c/f8/3c08d5cd66a8ea098c397e14f1469ff8.png?wh=635*83\" alt=\"\"></p><p>节点D被属性A划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。</p><h2>如何使用CART算法来创建分类树</h2><p>通过上面的讲解你可以知道，CART分类树实际上是基于基尼系数来做属性划分的。在Python的sklearn中，如果我们想要创建CART分类树，可以直接使用DecisionTreeClassifier这个类。创建这个类的时候，默认情况下criterion这个参数等于gini，也就是按照基尼系数来选择属性划分，即默认采用的是CART分类树。</p><p>下面，我们来用CART分类树，给iris数据集构造一棵分类决策树。iris这个数据集，我在Python可视化中讲到过，实际上在sklearn中也自带了这个数据集。基于iris数据集，构造CART分类树的代码如下：</p><pre><code># encoding=utf-8\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\n# 准备数据集\niris=load_iris()\n# 获取特征集和分类标识\nfeatures = iris.data\nlabels = iris.target\n# 随机抽取33%的数据作为测试集，其余为训练集\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)\n# 创建CART分类树\nclf = DecisionTreeClassifier(criterion='gini')\n# 拟合构造CART分类树\nclf = clf.fit(train_features, train_labels)\n# 用CART分类树做预测\ntest_predict = clf.predict(test_features)\n# 预测结果与测试集结果作比对\nscore = accuracy_score(test_labels, test_predict)\nprint(&quot;CART分类树准确率 %.4lf&quot; % score)\n</code></pre><p>运行结果：</p><pre><code>CART分类树准确率 0.9600\n</code></pre><p>如果我们把决策树画出来，可以得到下面的图示：</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/40/c1e2f9e4a299789bb6cc23afc6fd3140.png?wh=736*755\" alt=\"\"><br>\n首先train_test_split可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。</p><p>使用clf = DecisionTreeClassifier(criterion=‘gini’)初始化一棵CART分类树。这样你就可以对CART分类树进行训练。</p><p>使用clf.fit(train_features, train_labels)函数，将训练集的特征值和分类标识作为参数进行拟合，得到CART分类树。</p><p>使用clf.predict(test_features)函数进行预测，传入测试集的特征值，可以得到测试结果test_predict。</p><p>最后使用accuracy_score(test_labels, test_predict)函数，传入测试集的预测结果与实际的结果作为参数，得到准确率score。</p><p>我们能看到sklearn帮我们做了CART分类树的使用封装，使用起来还是很方便的。</p><p><strong>CART回归树的工作流程</strong></p><p>CART回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在CART分类树中采用的是基尼系数作为标准，那么在CART回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。</p><p>样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设x为样本的个体，均值为u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。</p><p>其中差值的绝对值为样本值减去样本均值的绝对值：</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/97/6f9677a70b1edff85e9e467f3e52bd97.png?wh=208*125\" alt=\"\"><br>\n方差为每个样本值减去样本均值的平方和除以样本个数：</p><p><img src=\"https://static001.geekbang.org/resource/image/04/c1/045fd5afb7b53f17a8accd6f337f63c1.png?wh=245*87\" alt=\"\"><br>\n所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。</p><p>我们可以通过一个例子来看下如何创建一棵CART回归树来做预测。</p><h2>如何使用CART回归树做预测</h2><p>这里我们使用到sklearn自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。</p><p>根据这些指标，我们使用CART回归树对波士顿房价进行预测，代码如下：</p><pre><code># encoding=utf-8\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\n# 准备数据集\nboston=load_boston()\n# 探索数据\nprint(boston.feature_names)\n# 获取特征集和房价\nfeatures = boston.data\nprices = boston.target\n# 随机抽取33%的数据作为测试集，其余为训练集\ntrain_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)\n# 创建CART回归树\ndtr=DecisionTreeRegressor()\n# 拟合构造CART回归树\ndtr.fit(train_features, train_price)\n# 预测测试集中的房价\npredict_price = dtr.predict(test_features)\n# 测试集的结果评价\nprint('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))\nprint('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) \n</code></pre><p>运行结果（每次运行结果可能会有不同）：</p><pre><code>['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']\n回归树二乘偏差均值: 23.80784431137724\n回归树绝对值偏差均值: 3.040119760479042\n</code></pre><p>如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）：</p><p><img src=\"https://static001.geekbang.org/resource/image/65/61/65a3855aed648b32994b808296a40b61.png?wh=1861*368\" alt=\"\"></p><p>你可以在<a href=\"https://pan.baidu.com/s/1RKD6-IwAzL--cL0jt4GPiQ\">这里</a>下载完整PDF文件。</p><p>我们来看下这个例子，首先加载了波士顿房价数据集，得到特征集和房价。然后通过train_test_split帮助我们把数据集抽取一部分作为测试集，其余作为训练集。</p><p>使用dtr=DecisionTreeRegressor()初始化一棵CART回归树。</p><p>使用dtr.fit(train_features, train_price)函数，将训练集的特征值和结果作为参数进行拟合，得到CART回归树。</p><p>使用dtr.predict(test_features)函数进行预测，传入测试集的特征值，可以得到预测结果predict_price。</p><p>最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。</p><p>我们能看到CART回归树的使用和分类树类似，只是最后求得的预测值是个连续值。</p><h2>CART决策树的剪枝</h2><p>CART决策树的剪枝主要采用的是CCP方法，它是一种后剪枝的方法，英文全称叫做cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/95/6b9735123d45e58f0b0afc7c3f68cd95.png?wh=327*140\" alt=\"\"><br>\n其中Tt代表以t为根节点的子树，C(Tt)表示节点t的子树没被裁剪时子树Tt的误差，C(t)表示节点t的子树被剪枝后节点t的误差，|Tt|代子树Tt的叶子数，剪枝后，T的叶子数减少了|Tt|-1。</p><p>所以节点的表面误差率增益值等于节点t的子树被剪枝后的误差变化除以剪掉的叶子数量。</p><p>因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。</p><p>得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。</p><h2>总结</h2><p>今天我给你讲了CART决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。</p><p>最后我们来整理下三种决策树之间在属性选择标准上的差异：</p><ul>\n<li>\n<p>ID3算法，基于信息增益做判断；</p>\n</li>\n<li>\n<p>C4.5算法，基于信息增益率做判断；</p>\n</li>\n<li>\n<p>CART算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。</p>\n</li>\n</ul><p>实际上这三个指标也是计算“不纯度”的三种计算方式。</p><p>在工具使用上，我们可以使用sklearn中的DecisionTreeClassifier创建CART分类树，通过DecisionTreeRegressor创建CART回归树。</p><p>你可以用代码自己跑一遍我在文稿中举到的例子。</p><p><img src=\"https://static001.geekbang.org/resource/image/5c/84/5cfe1151f88befc1178eca3252890f84.png?wh=820*472\" alt=\"\"><br>\n最后给你留两道思考题吧，你能说下ID3，C4.5，以及CART分类树在做节点划分时的区别吗？第二个问题是，sklearn中有个手写数字数据集，调用的方法是load_digits()，你能否创建一个CART分类树，对手写数字数据集做分类？另外选取一部分测试集，统计下分类树的准确率？</p><p>欢迎你在评论下面留言，与我分享你的答案。也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事，一起交流。</p><p></p>","neighbors":{"left":{"article_title":"17 丨决策树（上）：要不要去打篮球？决策树来告诉你","id":78273},"right":{"article_title":"19丨决策树（下）：泰坦尼克乘客生存预测","id":79072}},"comments":[{"had_liked":false,"id":67649,"user_name":"rainman","can_delete":false,"product_type":"c1","uid":1281085,"ip_address":"","ucode":"A7680916222129","user_header":"https://static001.geekbang.org/account/avatar/00/13/8c/3d/77b9abbb.jpg","comment_is_top":true,"comment_ctime":1550217134,"is_pvip":false,"replies":[{"id":"38541","content":"关于决策树可视化，大家可以看看这个。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1561357783,"ip_address":"","comment_id":67649,"utype":2}],"discussion_count":1,"race_medal":0,"score":"9.2233721801388995e+18","product_id":100021701,"comment_content":"对于 CART 回归树的可视化，可以先在电脑上安装 graphviz；然后 pip install graphviz，这是安装python的库，需要依赖前面安装的 graphviz。可视化代码如下：<br><br>----<br>from sklearn.tree import export_graphviz<br>import graphviz<br><br># 参数是回归树模型名称，不输出文件。<br>dot_data = export_graphviz(dtr, out_file=None)<br>graph = graphviz.Source(dot_data)<br># render 方法会在同级目录下生成 Boston PDF文件，内容就是回归树。<br>graph.render(&#39;Boston&#39;)<br>----<br><br>具体内容可以去 sklearn(https:&#47;&#47;scikit-learn.org&#47;stable&#47;modules&#47;generated&#47;sklearn.tree.export_graphviz.html)<br>和 graphviz(https:&#47;&#47;graphviz.readthedocs.io&#47;en&#47;stable&#47;) 看看。","like_count":34,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439302,"discussion_content":"关于决策树可视化，大家可以看看这个。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357783,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68012,"user_name":"王彬成","can_delete":false,"product_type":"c1","uid":1015045,"ip_address":"","ucode":"386803B8FC2DD5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg","comment_is_top":false,"comment_ctime":1550386267,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"78859797595","product_id":100021701,"comment_content":"1、ID3，C4.5，以及 CART 分类树在做节点划分时的区别吗？<br>ID3是基于信息增益来判断，信息增益最大的，选取作为根节点。<br>C4.5采用信息增益率来判断，信息增益率最大的，选取作为根节点。<br>CART分类树采用基尼系数最小的属性作为属性划分<br>2、sklearn 中有个手写数字数据集，调用的方法是 load_digits()，你能否创建一个 CART 分类树，对手写数字数据集做分类？另外选取一部分测试集，统计下分类树的准确率？<br># 手写数据集load_digits()建立CART分类树<br># encoding= utf-8<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br>#准备数据<br>digits=load_digits()<br>#获取特征集和分类标识<br>features=digits.data<br>labels=digits.target<br>#随机抽取33%的数据作为测试集，其余为训练集<br>train_features,test_features,train_labels,test_labels=train_test_split(features,labels,test_size=0.33,random_state=0)<br>#创建cart分类树<br>clf=DecisionTreeClassifier(criterion=&#39;gini&#39;)<br>#拟合构造cart分类树<br>clf=clf.fit(train_features,train_labels)<br># 用cart分类树做预测<br>test_predict=clf.predict(test_features)<br>#预测结果与测试集结果做比对<br>score=accuracy_score(test_labels,test_predict)<br>print(&quot;CART 分类树准确率 %.4lf&quot;%score)<br>-----------<br>运算结果：CART 分类树准确率 0.8603","like_count":18},{"had_liked":false,"id":64859,"user_name":"胡","can_delete":false,"product_type":"c1","uid":1353712,"ip_address":"","ucode":"3917AFED7701CA","user_header":"https://static001.geekbang.org/account/avatar/00/14/a7/f0/e3212f18.jpg","comment_is_top":false,"comment_ctime":1548927072,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"78858338400","product_id":100021701,"comment_content":"cart分类树的决策树那副图看不懂。","like_count":18,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":4129,"discussion_content":"具体这副图的数学原理，暂时也不明白。但就图的结果来说，这是个二叉树，每个元素里有四行，第一行是二叉树分叉条件，如 X[3] <= 0.8, 就是训练集第 4 字段，是否小于 0.8 决定进入哪个决策树分支， 第二行是该样本子集的 基尼系数，第三行是样本集容量，第四行是样本结果分布，观察最终结果为 0 1 2， 因此 [0, 37, 40] 我猜测是 等于 1 的有 37个样本，等于 2 的有 40 个样本","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1565156749,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210,"discussion_content":"CART分类树，这里使用的graphviz库，具体代码如下：\nfrom sklearn import tree\nimport graphviz \n# 使用graphviz库打印出决策树图\ndot_data = tree.export_graphviz(clf,out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(&#34;tree&#34;)\n一般来说，CART决策树可视化只是用于示意，在实际使用CART决策树的时候，主要还是用DecisionTreeClassifier类的fit函数和predict函数来操作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561282769,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67762,"user_name":"小熊猫","can_delete":false,"product_type":"c1","uid":1257442,"ip_address":"","ucode":"7549BA17FFBAD4","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/e2/3640e491.jpg","comment_is_top":false,"comment_ctime":1550244443,"is_pvip":false,"replies":[{"id":"64552","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621637,"ip_address":"","comment_id":67762,"utype":1}],"discussion_count":1,"race_medal":0,"score":"61679786587","product_id":100021701,"comment_content":"ID3：以信息增益作为判断标准，计算每个特征的信息增益，选取信息增益最大的特征，但是容易选取到取值较多的特征<br>C4.5：以信息增益比作为判断标准，计算每个特征的信息增益比，选取信息增益比最大的特征<br>CART：分类树以基尼系数为标准，选取基尼系数小的的特征<br>            回归树以均方误差或绝对值误差为标准，选取均方误差或绝对值误差最小的特征<br><br>练习题：<br>from sklearn import datasets<br>from sklearn.model_selection import train_test_split<br>from sklearn import tree<br>from sklearn.metrics import accuracy_score<br>import graphviz <br><br># 准备手写数字数据集<br>digits = datasets.load_digits()<br># 获取特征和标识<br>features = digits.data<br>labels = digits.target<br># 选取数据集的33%为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33)<br># 创建CART分类树<br>clf = tree.DecisionTreeClassifier()<br># 拟合构造CART分类树<br>clf.fit(train_features, train_labels)<br># 预测测试集结果<br>test_predict = clf.predict(test_features)<br># 测试集结果评价<br>print(&#39;CART分类树准确率:&#39;, accuracy_score(test_labels, test_predict))<br># 画决策树<br>dot_data = tree.export_graphviz(clf, out_file=None)<br>graph = graphviz.Source(dot_data)<br>graph.render(&#39;CART&#47;&#47;CART_practice_digits&#39;)<br><br>CART分类树准确率: 0.8636363636363636","like_count":14,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439361,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621637,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70067,"user_name":"jake","can_delete":false,"product_type":"c1","uid":1362010,"ip_address":"","ucode":"DDC4C7F6BF5CE0","user_header":"","comment_is_top":false,"comment_ctime":1550981456,"is_pvip":false,"replies":[{"id":"38536","content":"单纯看节点D的基尼系数，采用公式：1 - [p(ck|t)]^2，也就是Gini(D)=1 - (9&#47;12 * 9&#47;12 + 3&#47;12 * 3&#47;12) = 0.375<br>同时，文章也计算了关于节点D，在属性A划分下的基尼系数的情况：<br>Gini(D, A)=|D1|&#47;|D|*Gini(D1) + |D2|&#47;|D|*Gini(D2)=6&#47;12*0+6&#47;12*0.5=0.25<br>所以Gini(D)=0.375, Gini(D, A)=0.25","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1561357334,"ip_address":"","comment_id":70067,"utype":2}],"discussion_count":2,"race_medal":0,"score":"31615752528","product_id":100021701,"comment_content":"<br>首先想问一个问题 就是在讲到基尼系数那里 有一个图那里的例子 什么D: 9个打篮球 3个不打篮球那里<br>那里的D的基尼系数用到了子节点归一化基尼系数之和这个方法求 请问D的基尼系数不能直接用 上面那个公式 也就是&quot;1 - [p(ck|t)]^2&quot;那个公式计算吗 我用这个公式计算出D的基尼系数为 1 - (9&#47;12 * 9&#47;12 + 3&#47;12 * 3&#47;12) = 6&#47;16。 我也想问一下上面那个同学提的这个问题","like_count":7,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440424,"discussion_content":"单纯看节点D的基尼系数，采用公式：1 - [p(ck|t)]^2，也就是Gini(D)=1 - (9/12 * 9/12 + 3/12 * 3/12) = 0.375\n同时，文章也计算了关于节点D，在属性A划分下的基尼系数的情况：\nGini(D, A)=|D1|/|D|*Gini(D1) + |D2|/|D|*Gini(D2)=6/12*0+6/12*0.5=0.25\n所以Gini(D)=0.375, Gini(D, A)=0.25","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357334,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1934729,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/85/89/bc2aa262.jpg","nickname":"迟茗","note":"","ucode":"6A7C5EAB91836E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":381170,"discussion_content":"所以在A的划分下，基尼系数不是变小了吗？【节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高】这句话怎么理解呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624936537,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":68624,"user_name":"xfoolin","can_delete":false,"product_type":"c1","uid":1352218,"ip_address":"","ucode":"F63BF326141F46","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erBSoNm28CNCYpvytDbhfSYpgCo6T9vuzKkSoflr3sX8VucMz8ykrichKDY9vVoMVomUIakXSQq9icw/132","comment_is_top":false,"comment_ctime":1550560474,"is_pvip":false,"replies":[{"id":"64531","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621336,"ip_address":"","comment_id":68624,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18730429658","product_id":100021701,"comment_content":"ID3 是通过信息增益，选取信息增益最大的特征；C4.5 是通过信息增益率，选取，CART 是通过基尼系数，选取基尼系数最小的特征。<br><br><br>from sklearn.model_selection import train_test_split#训练集和测试集<br>from sklearn.metrics import mean_squared_error#二乘偏差均值<br>from sklearn.metrics import mean_absolute_error#绝对值偏差均值<br>from sklearn.datasets import load_digits#引入 digits 数据集<br>from sklearn.metrics import accuracy_score#测试结果的准确性<br>from sklearn import tree<br>import graphviz<br>#准备数据集<br>digits = load_digits()<br>#获取数据集的特征集和分类标识<br>features = digits.data<br>labels = digits.target<br>#随机抽取 33% 的数据作为测试集，其余为训练集<br>train_features,test_features,train_labels,test_labels = \\<br>train_test_split(features,labels,test_size = 0.33,random_state = 0)<br>#创建 CART 分类树<br>clf = tree.DecisionTreeClassifier(criterion = &#39;gini&#39;)<br>#拟合构造分类树<br>clf = clf.fit(train_features,train_labels)<br>#用 CART 分类树做预测<br>test_predict = clf.predict(test_features)<br>#预测结果与测试集作对比<br>score = accuracy_score(test_labels,test_predict)<br>#输出准确率<br>print(&#39;准确率 %.4f&#39;%score)<br>dot_data = tree.export_graphviz(clf,out_file = None)<br>graph = graphviz.Source(dot_data)<br>#输出分类树图示<br>graph.view()<br>","like_count":4,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439760,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":62963,"user_name":"梁林松","can_delete":false,"product_type":"c1","uid":1144766,"ip_address":"","ucode":"FA032C3B4E245E","user_header":"https://static001.geekbang.org/account/avatar/00/11/77/be/1f2409e8.jpg","comment_is_top":false,"comment_ctime":1548214916,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18728084100","product_id":100021701,"comment_content":"老师 那个打篮球的例子里 D1&#47;D和D2&#47;D为什么是6&#47;9和2&#47;9呢？如果是子节点占父节点的比例不是应该是各1&#47;2吗？","like_count":4},{"had_liked":false,"id":71781,"user_name":"rm","can_delete":false,"product_type":"c1","uid":1129548,"ip_address":"","ucode":"17EAF6256277E4","user_header":"https://static001.geekbang.org/account/avatar/00/11/3c/4c/cd7c8019.jpg","comment_is_top":false,"comment_ctime":1551431351,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14436333239","product_id":100021701,"comment_content":"老师你好，决策的数是怎么生成的？","like_count":3,"discussions":[{"author":{"id":1104690,"avatar":"https://static001.geekbang.org/account/avatar/00/10/db/32/aeb274d8.jpg","nickname":"数据社","note":"","ucode":"2A41B848035D75","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":32513,"discussion_content":"参考代码：\n# encoding=utf-8\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nimport graphviz\nfrom sklearn import tree\nimport os\nos.environ[&#34;PATH&#34;] += os.pathsep + &#39;C:\\\\Users\\\\qincf\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\Library\\\\bin\\\\graphviz&#39;\n\n# 准备数据集\niris=load_iris()\n# 获取特征集和分类标识\nfeatures = iris.data\nlabels = iris.target\n# 随机抽取 33% 的数据作为测试集，其余为训练集\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)\n# 创建 CART 分类树\nclf = DecisionTreeClassifier(criterion=&#39;gini&#39;)\n# 拟合构造 CART 分类树\nclf = clf.fit(train_features, train_labels)\n# 用 CART 分类树做预测\ntest_predict = clf.predict(test_features)\nprint(test_predict)\n# 预测结果与测试集结果作比对\nscore = accuracy_score(test_labels, test_predict)\nprint(&#34;CART 分类树准确率 %.4lf&#34; % score)\n\n##打印CART\ndot_data = tree.export_graphviz(clf,out_file=None)\ngraph = graphviz.Source(dot_data)\ngraph\nprint(graph.view())\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571042900,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":63349,"user_name":"Lee","can_delete":false,"product_type":"c1","uid":1313515,"ip_address":"","ucode":"ED3D7200A948C3","user_header":"https://static001.geekbang.org/account/avatar/00/14/0a/eb/6d6a94d2.jpg","comment_is_top":false,"comment_ctime":1548331552,"is_pvip":false,"replies":[{"id":"64667","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577623734,"ip_address":"","comment_id":63349,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14433233440","product_id":100021701,"comment_content":"# encoding=utf-8<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br># 准备数据集<br>digits=load_digits()<br># 获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br># 随机抽取 33% 的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br># 创建 CART 分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造 CART 分类树<br>clf = clf.fit(train_features, train_labels)<br># 用 CART 分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART 分类树准确率 %.4lf&quot; % score)<br><br>CART 分类树准确率 0.8620","like_count":3,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437561,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577623734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":246713,"user_name":"小强","can_delete":false,"product_type":"c1","uid":1149004,"ip_address":"","ucode":"CC3D3A9E5D9A42","user_header":"https://static001.geekbang.org/account/avatar/00/11/88/4c/2c3d2c7d.jpg","comment_is_top":false,"comment_ctime":1599460128,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10189394720","product_id":100021701,"comment_content":"这三种算法的区别应该是分析这三种算法的优劣和适用场景，谁还不知道ID3是根据信息增益，c4.5根据信息增益率巴拉巴拉来做节点划分，可惜看不到想要的答案。","like_count":2},{"had_liked":false,"id":137935,"user_name":"Untitled","can_delete":false,"product_type":"c1","uid":1039464,"ip_address":"","ucode":"8DD6ABA3E81A2E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dc/68/006ba72c.jpg","comment_is_top":false,"comment_ctime":1569928693,"is_pvip":false,"discussion_count":0,"race_medal":1,"score":"10159863285","product_id":100021701,"comment_content":"1.导入数据<br><br>from sklearn.datasets import load_digits<br>digits = load_digits()<br>feature = digits.data<br>labels = digits.target<br>测试集和训练集的构建<br>from sklearn.model_selection import train_test_split<br>rain_features,test_features,train_labels,test_labels = train_test_split(feature,labels,test_size=0.3)<br>拟合<br>from sklearn.tree import DecisionTreeClassifier<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br>clf = clf.fit(train_features,train_labels)<br>预测<br>test_predict = clf.predict(test_features)<br>准确性分析<br>from sklearn.metrics import accuracy_score<br>score = accuracy_score(test_labels,test_predict)<br>print(score)<br>0.8685185185185185","like_count":2},{"had_liked":false,"id":63881,"user_name":"雨先生的晴天","can_delete":false,"product_type":"c1","uid":1246015,"ip_address":"","ucode":"71850548322A1C","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/3f/09308258.jpg","comment_is_top":false,"comment_ctime":1548575396,"is_pvip":false,"replies":[{"id":"38543","content":"评论中有人对决策树可视化做了解释，你可以看下。采用的是graphviz这个库，你需要先安装，然后使用。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1561357883,"ip_address":"","comment_id":63881,"utype":2}],"discussion_count":1,"race_medal":0,"score":"10138509988","product_id":100021701,"comment_content":"scikit learn package 确实非常好用，很简洁。推荐大家也去官网看一看，请问一下怎样可以把decision tree 可视化呀？ ","like_count":2,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437746,"discussion_content":"评论中有人对决策树可视化做了解释，你可以看下。采用的是graphviz这个库，你需要先安装，然后使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357883,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292215,"user_name":"Boom clap!!!","can_delete":false,"product_type":"c1","uid":2443427,"ip_address":"","ucode":"E9AF8ECB963239","user_header":"https://static001.geekbang.org/account/avatar/00/25/48/a3/2df11999.jpg","comment_is_top":false,"comment_ctime":1620722816,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5915690112","product_id":100021701,"comment_content":"提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天&#47;阴天&#47;雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。","like_count":1},{"had_liked":false,"id":264220,"user_name":"McKee Chen","can_delete":false,"product_type":"c1","uid":2037505,"ip_address":"","ucode":"F74B76542FAB65","user_header":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","comment_is_top":false,"comment_ctime":1606382888,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5901350184","product_id":100021701,"comment_content":"1. ID3，C4.5，以及 CART 分类树在做节点划分时的区别<br>ID3算法基于信息增益进行判断；<br>C4.5算法基于信息增益率进行判断；<br>CART算法用作分类时基于基尼系数进行判断，用作回归时基于最小绝对偏差或最小二乘偏差进行判断<br><br><br>2.对手写数字数据集进行分类并统计准确率<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.datasets import load_digits<br><br>#导入数据集<br>digits = load_digits()<br>#获取数据集的属性特征和分类结果<br>features = digits.data<br>labels = digits.target<br>#将数据集划分为测试集和训练集，且测试集为33%<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33)<br>#初始化分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br>#将训练集参数化分类树<br>clf = clf.fit(train_features, train_labels)<br>#计算预测结果<br>test_predict = clf.predict(test_features)<br>#计算准确率<br>score = accuracy_score(test_labels, test_predict)<br>print(&#39;CART决策树误差 %.4lf&#39; %score)<br><br>#输出结果<br>CART决策树误差 0.8586","like_count":1},{"had_liked":false,"id":234519,"user_name":"Jae","can_delete":false,"product_type":"c1","uid":1378731,"ip_address":"","ucode":"E4EB9F1F0A3FE8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoju1H3UuXg6LZkibtfh7ia5AljJ5mXGrkF25MrYahvjiafytVGcqQjXKNYy4gtam0jhzIKtRExv649w/132","comment_is_top":false,"comment_ctime":1594712949,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5889680245","product_id":100021701,"comment_content":"关于剪枝部分不是很理解，先 mark 一下，当然 老师如果能够提供完整的剪枝例子更好不过了","like_count":1},{"had_liked":false,"id":153045,"user_name":"Mi compaero de armas","can_delete":false,"product_type":"c1","uid":1533046,"ip_address":"","ucode":"D6F9AE0A80E717","user_header":"https://static001.geekbang.org/account/avatar/00/17/64/76/0da71882.jpg","comment_is_top":false,"comment_ctime":1574146526,"is_pvip":false,"replies":[{"id":"62663","content":"可以用CART算法，另外GBDT里面默认使用的是CART，这是个在数据分析，机器学习中常用的模型","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577081983,"ip_address":"","comment_id":153045,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5869113822","product_id":100021701,"comment_content":"老师您好，请问采用CART算法时，如果离散型属性的值不止两种还能使用CART算法吗","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":475027,"discussion_content":"可以用CART算法，另外GBDT里面默认使用的是CART，这是个在数据分析，机器学习中常用的模型","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577081983,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":150070,"user_name":"Ronnyz","can_delete":false,"product_type":"c1","uid":1488280,"ip_address":"","ucode":"9F34527B1D343D","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg","comment_is_top":false,"comment_ctime":1573457785,"is_pvip":false,"replies":[{"id":"62674","content":"代码正确","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577082797,"ip_address":"","comment_id":150070,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5868425081","product_id":100021701,"comment_content":"第二问<br>```<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br>digits = load_digits()<br><br># print(digits)<br><br>features=digits.data<br>labels = digits.target<br><br>train_features,test_features,train_labels,test_labels=train_test_split(features,labels,test_size =0.33,random_state=0)<br><br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br>clf=clf.fit(train_features,train_labels)<br><br>predict_labels=clf.predict(test_features)<br><br>print(&#39;分类准确度：&#39;,accuracy_score(test_labels,predict_labels))<br>```<br>分类准确度： 0.8619528619528619","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474056,"discussion_content":"代码正确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577082797,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":114912,"user_name":"许宇宝","can_delete":false,"product_type":"c1","uid":1107388,"ip_address":"","ucode":"3FBB9621953EF1","user_header":"https://static001.geekbang.org/account/avatar/00/10/e5/bc/e5ff2613.jpg","comment_is_top":false,"comment_ctime":1563431619,"is_pvip":true,"replies":[{"id":"63685","content":"take it easy","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577528200,"ip_address":"","comment_id":114912,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5858398915","product_id":100021701,"comment_content":"老师，看了两遍还是不明白分类的这个决策树是依据什么画出来的？","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":458881,"discussion_content":"take it easy","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577528200,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93511,"user_name":"羊小看","can_delete":false,"product_type":"c1","uid":1488453,"ip_address":"","ucode":"90F58F80A75520","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132","comment_is_top":false,"comment_ctime":1557483813,"is_pvip":false,"replies":[{"id":"64185","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577613892,"ip_address":"","comment_id":93511,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5852451109","product_id":100021701,"comment_content":"1、为什么CRAT的基尼系数比C4.5的信息增益率好呢？既然sklearn库默认用的基尼系数，应该是这个好一些吧？<br>2、from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>#from sklearn.datasets import load_iris<br>from sklearn.datasets import load_digits<br><br># 准备数据集<br>#iris=load_iris()<br>digits=load_digits()<br># 获取特征集和分类标识<br>#features = iris.data<br>#labels = iris.target<br>features = digits.data<br>labels = digits.target<br># 随机抽取 33% 的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br># 创建 CART 分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造 CART 分类树<br>clf = clf.fit(train_features, train_labels)<br># 用 CART 分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART 分类树准确率 %.4lf&quot; % score)<br><br>CART 分类树准确率 0.8636","like_count":1,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449663,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577613892,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65157,"user_name":"圆圆的大食客","can_delete":false,"product_type":"c1","uid":1361827,"ip_address":"","ucode":"B5B87E08869507","user_header":"https://static001.geekbang.org/account/avatar/00/14/c7/a3/1e2f9f5a.jpg","comment_is_top":false,"comment_ctime":1549079361,"is_pvip":false,"replies":[{"id":"38542","content":"正确，在使用决策树DecisionTreeClassifier类的时候，默认采用的就是基尼系数，即criterion=&#39;gini&#39;。","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1561357846,"ip_address":"","comment_id":65157,"utype":2}],"discussion_count":1,"race_medal":0,"score":"5844046657","product_id":100021701,"comment_content":"from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br>#准备数据集<br>digits=load_digits()<br>#获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br>#随机抽取33%的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br>#创建CART分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br>#拟合构造CART分类树<br>clf = clf.fit(train_features, train_labels)<br>#用CART分类树做预测<br>test_predict = clf.predict(test_features)<br>#预测结果与测试集结果作对比 <br>score = accuracy_score(test_labels, test_predict)<br>print (&quot;CART分类树准确率%.4lf&quot;% score)","like_count":1,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438340,"discussion_content":"正确，在使用决策树DecisionTreeClassifier类的时候，默认采用的就是基尼系数，即criterion=&amp;#39;gini&amp;#39;。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357846,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65064,"user_name":"Chino","can_delete":false,"product_type":"c1","uid":1347181,"ip_address":"","ucode":"0240D89E5F74B4","user_header":"https://static001.geekbang.org/account/avatar/00/14/8e/6d/c68e07ef.jpg","comment_is_top":false,"comment_ctime":1549016539,"is_pvip":false,"replies":[{"id":"38539","content":"单纯看节点D的基尼系数，采用公式：1 - [p(ck|t)]^2，也就是Gini(D)=1 - (9&#47;12 * 9&#47;12 + 3&#47;12 * 3&#47;12) = 0.375<br>同时，文章也计算了关于节点D，在属性A划分下的基尼系数的情况：<br>Gini(D, A)=|D1|&#47;|D|*Gini(D1) + |D2|&#47;|D|*Gini(D2)=6&#47;12*0+6&#47;12*0.5=0.25<br>所以Gini(D)=0.375, Gini(D, A)=0.25<br>手写数字数据集，采用CART分类这个正确","user_name":"编辑回复","user_name_real":"何昌梅","uid":"1165037","ctime":1561357700,"ip_address":"","comment_id":65064,"utype":2}],"discussion_count":1,"race_medal":0,"score":"5843983835","product_id":100021701,"comment_content":"首先想问一个问题 就是在讲到基尼系数那里 有一个图那里的例子 什么D: 9个打篮球 3个不打篮球那里<br>那里的D的基尼系数用到了子节点归一化基尼系数之和这个方法求 请问D的基尼系数不能直接用 上面那个公式 也就是&quot;1 - [p(ck|t)]^2&quot;那个公式计算吗  我用这个公式计算出D的基尼系数为 1 - (9&#47;12 * 9&#47;12 + 3&#47;12 * 3&#47;12) = 6&#47;16<br><br># ID3,C4.5,CART在做节点划分的区别<br># 我认为是三者的共同之处就是得出每个类别的某个属性值 然后根据这个属性值<br># 来选取哪个类型当节点 因此不同之处就是这个属性值.<br># ID3 根据 信息增益 判断 哪个节点的信息增益最大就当节点<br># C4.5 根据 信息增益率 判断 跟ID3相似<br># CART分类树 根据 基尼系数 判断 越小代表越稳定<br><br>from sklearn.datasets import load_digits<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.metrics import accuracy_score<br>from sklearn.model_selection import train_test_split<br><br><br>digit = load_digits()<br><br># 把数据集中的数据和结果拿出来<br>features = digit.data<br>target = digit.target<br><br># 把数据集中的数据分33%当作测试数据 其他用来训练<br>train_features,test_features,train_target,test_target = train_test_split(features,target,test_size=0.33)<br><br># 定义CART分类树<br>clf = DecisionTreeClassifier()<br><br># 把训练数据弄到分类数中 构造树<br>clf = clf.fit(train_features,train_target)<br><br># 把测试数据放进树中得出预测结果<br>predict_target = clf.predict(test_features)<br><br># 对比预测出来的数据和实际结果<br>score = accuracy_score(predict_target,test_target)<br><br>print(score)<br><br>","like_count":1,"discussions":[{"author":{"id":1165037,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c6/ed/89a2dc13.jpg","nickname":"丢了个丢丢丢","note":"","ucode":"BDD7E97E0E5E96","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438305,"discussion_content":"单纯看节点D的基尼系数，采用公式：1 - [p(ck|t)]^2，也就是Gini(D)=1 - (9/12 * 9/12 + 3/12 * 3/12) = 0.375\n同时，文章也计算了关于节点D，在属性A划分下的基尼系数的情况：\nGini(D, A)=|D1|/|D|*Gini(D1) + |D2|/|D|*Gini(D2)=6/12*0+6/12*0.5=0.25\n所以Gini(D)=0.375, Gini(D, A)=0.25\n手写数字数据集，采用CART分类这个正确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561357700,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":354462,"user_name":"本来是亚","can_delete":false,"product_type":"c1","uid":1073592,"ip_address":"上海","ucode":"14527E5BC3C2A8","user_header":"https://static001.geekbang.org/account/avatar/00/10/61/b8/7b23f8cb.jpg","comment_is_top":false,"comment_ctime":1660441995,"is_pvip":true,"discussion_count":0,"race_medal":1,"score":"1660441995","product_id":100021701,"comment_content":"ID3用于分类任务，节点划分时，采用了信息增益，其规则简单，可解释性强，但在选择划分属性时，倾向于选择取值较多的属性，而且由于没有剪枝策略，容易受到噪声影响，出现过拟合现象；<br>ID4.5用于分类任务，节点划分时，采用了信息增益比，避免了ID3中对于取值较多属性的倾向，而且加入了剪枝策略，缓解过拟合现象；<br>CART可用于分类，也可用于回归，用于分类时，采用基尼系数划分节点，用于回归时，采用最小绝对偏差，或者最小二乘偏差；<br>总地来说，虽然有不同的节点划分标准，但是目的都是降低划分后的不确定度。<br>用CART树来进行手写数字识别的实例：<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br># 准备数据集<br>digit=load_digits()<br># 获取特征集和分类标识<br>features = digit.data<br>labels = digit.target<br># 随机抽取33%的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br># 创建CART分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造CART分类树<br>clf = clf.fit(train_features, train_labels)<br># 用CART分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART分类树准确率 %.4lf&quot; % score)<br>CART分类树准确率 0.8670","like_count":0},{"had_liked":false,"id":353901,"user_name":"开心小毛","can_delete":false,"product_type":"c1","uid":1023762,"ip_address":"美国","ucode":"9D57A2773759F3","user_header":"","comment_is_top":false,"comment_ctime":1659920275,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1659920275","product_id":100021701,"comment_content":"陈老师您好，不知道此处是否是您的笔误：“CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分”，我猜测您是想说“最大”而不是最小，我的猜测对吗？","like_count":0},{"had_liked":false,"id":346667,"user_name":"Geek_e54f10","can_delete":false,"product_type":"c1","uid":2868155,"ip_address":"","ucode":"93C9CFA1837EBA","user_header":"","comment_is_top":false,"comment_ctime":1653351328,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1653351328","product_id":100021701,"comment_content":"如果源数据集巨大，python库就不合适了，如何在大数据平台中实现这些算法","like_count":0},{"had_liked":false,"id":337457,"user_name":"竹梦","can_delete":false,"product_type":"c1","uid":2360919,"ip_address":"","ucode":"9C016572BF2E1E","user_header":"https://static001.geekbang.org/account/avatar/00/24/06/57/11f1dffd.jpg","comment_is_top":false,"comment_ctime":1646835127,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1646835127","product_id":100021701,"comment_content":"老师可以稍微详细解释下决策树可视化后的图中的数据和指标的意思吗？","like_count":0},{"had_liked":false,"id":324288,"user_name":"静心","can_delete":false,"product_type":"c1","uid":1335457,"ip_address":"","ucode":"EB264FA6519FDA","user_header":"https://static001.geekbang.org/account/avatar/00/14/60/a1/8f003697.jpg","comment_is_top":false,"comment_ctime":1638356876,"is_pvip":true,"discussion_count":0,"race_medal":5,"score":"1638356876","product_id":100021701,"comment_content":"请问老师：关于剪枝策略是内置到DecisionTreeClassifier中的吗？有没有相应的参数可用？","like_count":0},{"had_liked":false,"id":319492,"user_name":"靖斐","can_delete":false,"product_type":"c1","uid":1054745,"ip_address":"","ucode":"7F2B570EEA4A39","user_header":"https://static001.geekbang.org/account/avatar/00/10/18/19/f686bb1b.jpg","comment_is_top":false,"comment_ctime":1635836171,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1635836171","product_id":100021701,"comment_content":"请问，CCP剪枝在上述的房价预测代码中哪个步骤完成了这个操作呢？谢谢。","like_count":0},{"had_liked":false,"id":313775,"user_name":"Geek_09192f","can_delete":false,"product_type":"c1","uid":2750733,"ip_address":"","ucode":"82583B8D954B68","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKbiaIicFAadOSVK5y1Dkk740QbIDep9XFWibyib7Keg5fbJKYaEQXTd6oRYA3oeOFicEz7LaHPs2l1Y0w/132","comment_is_top":false,"comment_ctime":1632661148,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1632661148","product_id":100021701,"comment_content":"#手写数据集分类<br>from sklearn.model_selection import train_test_split<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br>from sklearn.metrics import accuracy_score<br><br>#加载数据集<br>digit = load_digits()<br>#特征数据和标签<br>feature = digit.data<br>labels = digit.target<br><br>#划分训练集和测试集<br>train_features,test_features,train_labels,test_labels = train_test_split(feature,labels,test_size=0.33)<br><br>clf = DecisionTreeClassifier()<br>clf.fit(train_features,train_labels)<br>#预测数据<br>predit_label = clf.predict(test_features)<br>#预测精度<br>acc = accuracy_score(test_labels,predit_label)<br>print(&quot;预测精度为：{}&quot;.format(acc))","like_count":0},{"had_liked":false,"id":303478,"user_name":"zh","can_delete":false,"product_type":"c1","uid":2679806,"ip_address":"","ucode":"6E65E5070F0681","user_header":"https://static001.geekbang.org/account/avatar/00/28/e3/fe/638201ad.jpg","comment_is_top":false,"comment_ctime":1626825340,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626825340","product_id":100021701,"comment_content":"理论解释的通俗易懂","like_count":0},{"had_liked":false,"id":303473,"user_name":"zh","can_delete":false,"product_type":"c1","uid":2679806,"ip_address":"","ucode":"6E65E5070F0681","user_header":"https://static001.geekbang.org/account/avatar/00/28/e3/fe/638201ad.jpg","comment_is_top":false,"comment_ctime":1626822550,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626822550","product_id":100021701,"comment_content":"理论点基本都cover到了，可是如何通过代码实现postpruning完全没讲。。prepruning可以调参跳过可以理解，没理由跳过postpruning啊","like_count":0},{"had_liked":false,"id":301549,"user_name":"东旭","can_delete":false,"product_type":"c1","uid":2668134,"ip_address":"","ucode":"FD3EA3265FD9C0","user_header":"https://static001.geekbang.org/account/avatar/00/28/b6/66/5d2be1ee.jpg","comment_is_top":false,"comment_ctime":1625734916,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1625734916","product_id":100021701,"comment_content":"这个波士顿的回归预测效果好还是不好呢","like_count":0},{"had_liked":false,"id":295334,"user_name":"进击的矮子","can_delete":false,"product_type":"c1","uid":2218372,"ip_address":"","ucode":"39173CF9584C51","user_header":"https://static001.geekbang.org/account/avatar/00/21/d9/84/f1b10393.jpg","comment_is_top":false,"comment_ctime":1622373041,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1622373041","product_id":100021701,"comment_content":"剪枝的部分没完全理解，原理大概是懂了，即剪枝后变化最小的节点（表示不提供丰富的额外信息）可以剪枝。但是步骤上来看，自底向上的剪枝，如果要保证全局最优的话，是不是得从每个叶节点遍历到根节点为止（当然其中相同情况可以不再重复计算）","like_count":0},{"had_liked":false,"id":273543,"user_name":"Melons","can_delete":false,"product_type":"c1","uid":2259336,"ip_address":"","ucode":"DB060AFC6F8156","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/yZKYKNcLpTJDspoDPEXEJ1w6VSpziaicvXDWauo1k1qAm9Ac9eEWurheHw5AOzoYCtPfZJ53ibqJNH6zoibkq5fWXg/132","comment_is_top":false,"comment_ctime":1610632264,"is_pvip":false,"replies":[{"id":"101922","content":"当D是一个单独的集合，没有子节点时，它的基尼系数确实是1-（9&#47;12 * 9&#47;12 + 3&#47;12 * 3&#47;12）=3&#47;8；<br>现在我们已知D又被分了2个子节点出来，作为父节点的D，它的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，即父节点的基尼系数由子节点D1和D2的基尼系数加权所得，权重是该子节点下样本数占总样本数的比例。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1614363675,"ip_address":"","comment_id":273543,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1610632264","product_id":100021701,"comment_content":"如果按照第一个公式，数据集D的基尼系数计算方法为1-（9&#47;12 * 9&#47;12 + 3&#47;12 * 3&#47;12）=3&#47;8；请问为什么不能这样算呢？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513626,"discussion_content":"当D是一个单独的集合，没有子节点时，它的基尼系数确实是1-（9/12 * 9/12 + 3/12 * 3/12）=3/8；\n现在我们已知D又被分了2个子节点出来，作为父节点的D，它的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，即父节点的基尼系数由子节点D1和D2的基尼系数加权所得，权重是该子节点下样本数占总样本数的比例。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614363675,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":249515,"user_name":"lemonlxn","can_delete":false,"product_type":"c1","uid":2184921,"ip_address":"","ucode":"520B4842201018","user_header":"","comment_is_top":false,"comment_ctime":1600680173,"is_pvip":false,"replies":[{"id":"103017","content":"特征既可以是离散值也可以是连续值","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1616001865,"ip_address":"","comment_id":249515,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1600680173","product_id":100021701,"comment_content":"使用CART树，做回归或者分类，样本数据类型 有无规定，只能是 分类 或 数值 数据类型，还是两者都可以存在？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505942,"discussion_content":"特征既可以是离散值也可以是连续值","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616001865,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":233809,"user_name":"freedomwl","can_delete":false,"product_type":"c1","uid":2031687,"ip_address":"","ucode":"13639B043B1C11","user_header":"","comment_is_top":false,"comment_ctime":1594460474,"is_pvip":false,"replies":[{"id":"103514","content":"是的。可以在train_test_split()拆分数据集时，通过参数random_state传入一个整数作为随机数种子，这样每次运行得到的都是相同的训练集和测试集。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1616693941,"ip_address":"","comment_id":233809,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1594460474","product_id":100021701,"comment_content":"老师，波士顿房价代码每次运行结果不一样是因为训练集和测试集每运行一次都会重新随机选取的原因吗？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501173,"discussion_content":"是的。可以在train_test_split()拆分数据集时，通过参数random_state传入一个整数作为随机数种子，这样每次运行得到的都是相同的训练集和测试集。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616693941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2076309,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/ae/95/9fc9a38a.jpg","nickname":"🍰Hefqic","note":"","ucode":"875BD459F0E530","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":296667,"discussion_content":"是的，如果设置random_state=固定参数（如=0），那么每次“随机”选取都会选取固定的随机值","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1596619222,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2037505,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg","nickname":"McKee Chen","note":"","ucode":"F74B76542FAB65","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2076309,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/ae/95/9fc9a38a.jpg","nickname":"🍰Hefqic","note":"","ucode":"875BD459F0E530","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":329666,"discussion_content":"random_state=0时候生成的随机数不是固定的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606445638,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":296667,"ip_address":""},"score":329666,"extra":""}]}]},{"had_liked":false,"id":212539,"user_name":"哎哟哟","can_delete":false,"product_type":"c1","uid":1619213,"ip_address":"","ucode":"8E8C2899B73DF8","user_header":"https://static001.geekbang.org/account/avatar/00/18/b5/0d/df1f17b5.jpg","comment_is_top":false,"comment_ctime":1588144625,"is_pvip":false,"replies":[{"id":"103966","content":"正确！","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617288725,"ip_address":"","comment_id":212539,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1588144625","product_id":100021701,"comment_content":"1、ID3，C4.5，以及 CART 分类树划分带来信息纯度提高，信息熵下降，ID3、C4.5分别是信息增益和信息增益率越高信息纯度越高，CART是基尼系数越小差距越小<br>2、照着老师的案例改了一下<br>from sklearn.metrics import accuracy_score<br>from sklearn.model_selection import train_test_split<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br>hd = load_digits()<br># print(handWrite)<br>features = hd.data<br>labels = hd.target<br><br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br><br># 创建CART分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造CART分类树<br>clf = clf.fit(train_features, train_labels)<br># 用CART分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART分类树准确率 %.4lf&quot; % score)","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493540,"discussion_content":"正确！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617288725,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210541,"user_name":"Geek_c9fa4e","can_delete":false,"product_type":"c1","uid":1972305,"ip_address":"","ucode":"391982F33C1AAA","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/ziaN7rOONp15HJm6A9JoAYicJL8VA59x10DX4JZyvcfqmmpCnumXgAkNn37aFoALftyTaQNlUF7te54LibvVm20TQ/132","comment_is_top":false,"comment_ctime":1587785721,"is_pvip":false,"replies":[{"id":"103970","content":"正确","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617288928,"ip_address":"","comment_id":210541,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1587785721","product_id":100021701,"comment_content":"from sklearn.datasets import load_digits<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import mean_absolute_error,mean_squared_error,accuracy_score<br>from sklearn.tree import DecisionTreeRegressor<br>data=load_digits()<br>labels=data.data<br>features=data.target<br>x_train,x_test,y_train,y_test=train_test_split(labels,features,test_size=0.2)<br>dtr=DecisionTreeRegressor()<br>dtr.fit(x_train,y_train)<br>data_predict=dtr.predict(x_test)<br>print(&quot;分类准确率为:&quot;,accuracy_score(y_test,data_predict))","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493051,"discussion_content":"正确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617288928,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":207622,"user_name":"Geek_5d8f2f","can_delete":false,"product_type":"c1","uid":1951924,"ip_address":"","ucode":"2836BC5634E64F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/1ornnGqSaTxRdbxhUHibeylvYngHvK64ebMaBso6vwXD9I3OEic75dZXxypvwfoCKeKutkpK2d7Xte8Gqh0UH4QA/132","comment_is_top":false,"comment_ctime":1587120828,"is_pvip":false,"replies":[{"id":"104213","content":"把我们训练好的模型使用pickle包保存到本地。当由新的数据时，只要该数据所有特征与训练集是相同的，则可以直接从本地load该模型进行predict。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617725771,"ip_address":"","comment_id":207622,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1587120828","product_id":100021701,"comment_content":"那请问如何应用呢？比方分类树我得出了 0.9600， 那这时候我有一个新数据，如何传入去判别这个新数据呢？？一个头两个大","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492186,"discussion_content":"把我们训练好的模型使用pickle包保存到本地。当由新的数据时，只要该数据所有特征与训练集是相同的，则可以直接从本地load该模型进行predict。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617725771,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1926127,"avatar":"","nickname":"Geek_a99f54","note":"","ucode":"72F5DFF28D179B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300766,"discussion_content":"比如预测[[5.5, 3, 4, 2]]这个集合的结果，可以使用下面代码\nclf.predict([5.5, 3, 4, 2]]) # 结果是1，也就是labels集合[0，1，2]中的某个值","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598259730,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1978686,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/31/3e/65539b22.jpg","nickname":"熙","note":"","ucode":"BB97CEA9718729","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":252428,"discussion_content":"一个头两个大，too","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588164533,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":195808,"user_name":"桔子","can_delete":false,"product_type":"c1","uid":1245080,"ip_address":"","ucode":"635DB29C04FD47","user_header":"https://static001.geekbang.org/account/avatar/00/12/ff/98/6e17646a.jpg","comment_is_top":false,"comment_ctime":1585216302,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585216302","product_id":100021701,"comment_content":"CART既可以做回归，也可以用作分类<br>回归考量的是“混乱度”，比较的是和均值的偏差，通过最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）来量化。<br>分类考量的是纯度，纯的度量依靠的是gini系数。<br>CART算法同样可以针对连续变量，自动划分阈值。","like_count":0},{"had_liked":false,"id":192648,"user_name":"JustDoDT","can_delete":false,"product_type":"c1","uid":1127175,"ip_address":"","ucode":"6AF0B80F00EAEF","user_header":"https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg","comment_is_top":false,"comment_ctime":1584865620,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584865620","product_id":100021701,"comment_content":"交作业，作业地址：<br>https:&#47;&#47;github.com&#47;LearningChanging&#47;Data-analysis-in-action&#47;tree&#47;master&#47;18-%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88%E4%B8%AD%EF%BC%89%EF%BC%9ACART%EF%BC%8C%E4%B8%80%E6%A3%B5%E6%98%AF%E5%9B%9E%E5%BD%92%E6%A0%91%EF%BC%8C%E5%8F%A6%E4%B8%80%E6%A3%B5%E6%98%AF%E5%88%86%E7%B1%BB%E6%A0%91","like_count":0},{"had_liked":false,"id":181744,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1582626500,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1582626500","product_id":100021701,"comment_content":"总结：<br>1 作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；<br>2 作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果","like_count":0},{"had_liked":false,"id":176713,"user_name":"鱼非子","can_delete":false,"product_type":"c1","uid":1818595,"ip_address":"","ucode":"BB76AE2CB4D680","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg","comment_is_top":false,"comment_ctime":1581147167,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1581147167","product_id":100021701,"comment_content":"ID3：采用信息增益作为纯度的衡量<br>C4.5：采用信息增益率作为纯度的衡量<br>CART：采用基尼基数作为纯度的衡量","like_count":0},{"had_liked":false,"id":176712,"user_name":"鱼非子","can_delete":false,"product_type":"c1","uid":1818595,"ip_address":"","ucode":"BB76AE2CB4D680","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bf/e3/2aa8ec84.jpg","comment_is_top":false,"comment_ctime":1581147092,"is_pvip":false,"replies":[{"id":"104020","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1617294303,"ip_address":"","comment_id":176712,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1581147092","product_id":100021701,"comment_content":"<br># encoding=utf-8<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br># from sklearn.datasets import load_iris<br>from sklearn.datasets import load_digits<br># 准备数据集<br># iris = load_iris()<br>digits = load_digits()<br># 获取特征集和分类标识<br># features = iris.data<br>features = digits.data<br>labels = digits.target<br># labels = iris.target<br># 随机抽取33%的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br># 创建CART分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造CART分类树<br>clf = clf.fit(train_features, train_labels)<br># 用CART分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART分类树准确率 %.4lf&quot; % score)","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":483189,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617294303,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":166597,"user_name":"GodlikeJy","can_delete":false,"product_type":"c1","uid":1606706,"ip_address":"","ucode":"85291FA61A13B8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKGRpeljInKj7fUQvwrB9v6kmTOkJlG9ghUoWCXquaDDFmtXeDrEL4p8Gscfx4ZmfeSs2GbZgwwibg/132","comment_is_top":false,"comment_ctime":1577513833,"is_pvip":false,"replies":[{"id":"63757","content":"总结的不错","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577531257,"ip_address":"","comment_id":166597,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1577513833","product_id":100021701,"comment_content":"对于模型评价，这里需要学习一些统计学知识，混淆矩阵<br>1. 真阳性（True Positive，TP）：样本的真实类别是正例，并且模型预测的结果也是正例<br>2. 真阴性（True Negative，TN）：样本的真实类别是负例，并且模型将其预测成为负例<br>3. 假阳性（False Positive，FP）：样本的真实类别是负例，但是模型将其预测成为正例<br>4. 假阴性（False Negative，FN）：样本的真实类别是正例，但是模型将其预测成为负例<br>正确率（Accuracy）：被正确分类的样本比例或数量  (TP+TN)&#47;total","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":479552,"discussion_content":"总结的不错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577531257,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":165157,"user_name":"TKbook","can_delete":false,"product_type":"c1","uid":1073829,"ip_address":"","ucode":"F6E0E99CC79059","user_header":"https://static001.geekbang.org/account/avatar/00/10/62/a5/43aa0c27.jpg","comment_is_top":false,"comment_ctime":1577173369,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1577173369","product_id":100021701,"comment_content":"from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br># 准备数据<br>digits = load_digits()<br># 获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br># 随机抽取33%的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br># 创建CART分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造CART分类树<br>clf = clf.fit(train_features, train_labels)<br># 用CART分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作对比<br>score = accuracy_score(test_labels, test_predict)<br>print(&#39;CART分类树准确率 %.4lf&#39; % score)<br><br>CART分类树准确率 0.8603","like_count":0},{"had_liked":false,"id":133716,"user_name":"微澜gao","can_delete":false,"product_type":"c1","uid":1367599,"ip_address":"","ucode":"23CF3403FE424D","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIx9A2J1pCWjnsg5drDCp1zgYpfwIARUwicIWofKsy0ECMk39iaWltn4lUwJn1GK6OC7jbjicGBBx0tA/132","comment_is_top":false,"comment_ctime":1568640664,"is_pvip":false,"replies":[{"id":"63497","content":"剪枝不想降低精度，而是减少计算量，所以是希望剪枝前后误差最小","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577514724,"ip_address":"","comment_id":133716,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1568640664","product_id":100021701,"comment_content":"请问cart决策树剪枝CCP方法中为什么希望剪枝前后误差最小<br><br>","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467483,"discussion_content":"剪枝不想降低精度，而是减少计算量，所以是希望剪枝前后误差最小","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577514724,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":127440,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1566712876,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1566712876","product_id":100021701,"comment_content":"老师，能否讲一下这些决策树构造后的实际应用的例子，看了示例代码之后，感觉只是对构造后结果进行评价，那怎么运用构造出来的决策树对实际应用对象进行分类呢","like_count":0},{"had_liked":false,"id":127439,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1566712697,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1566712697","product_id":100021701,"comment_content":"思考题1：<br>ID3以节点的信息增益的大小为标准进行结点划分；<br>C4.5以节点的信息增益率为标准进行结点划分；<br>CART算法，构造分类树时，根据基尼系数划分节点；回归树根据偏差划分节点。<br><br>思考题2：<br>#CART分类决策树算法实例：手写数字数据集<br><br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br># 准备数据集<br>digits=load_digits()<br><br># 获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br><br># 随机抽取 33% 的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br><br># 创建 CART 分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br><br># 拟合构造 CART 分类树<br>clf = clf.fit(train_features, train_labels)<br><br># 用 CART 分类树做预测<br>test_predict = clf.predict(test_features)<br><br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART 分类树准确率 %.4lf&quot; % score)<br>#输出结果<br>CART 分类树准确率 0.8670","like_count":0},{"had_liked":false,"id":117141,"user_name":"内存爆了","can_delete":false,"product_type":"c1","uid":1590007,"ip_address":"","ucode":"A0825B772CC3DF","user_header":"https://static001.geekbang.org/account/avatar/00/18/42/f7/67a2e6ee.jpg","comment_is_top":false,"comment_ctime":1563976059,"is_pvip":false,"replies":[{"id":"63631","content":"赞下认真做作业的同学","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577525628,"ip_address":"","comment_id":117141,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1563976059","product_id":100021701,"comment_content":"思考题<br>1、ID3和C4.5，以及CART分类树在做结点划分的时候的区别：<br>ID3和C4.5在做结点划分的时候，构建成的数可以是多叉树或者二叉树，但是CART算法只能构成二叉树，对于取多个值的属性变量，需要将多个类别合并成两个类别，形成超类，然后计算两个“超类”下样本测试输出取得的差异性。<br>2、构建分类树<br># encoding=utf-8<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br># 准备数据集<br>digits = load_digits()<br># 获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br># 随机抽取 33% 的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br># 创建CART 分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造CART分类树<br>clf = clf.fit(train_features, train_labels)<br># 用CART 分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试结果做对比<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART 分类树准确率 %.4lf&quot; % score)<br><br># 结果：CART 分类树准确率 0.8687","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":459838,"discussion_content":"赞下认真做作业的同学","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577525628,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":115537,"user_name":"Wei_强","can_delete":false,"product_type":"c1","uid":1152887,"ip_address":"","ucode":"B0E40FB6636F9D","user_header":"https://static001.geekbang.org/account/avatar/00/11/97/77/a01ebefc.jpg","comment_is_top":false,"comment_ctime":1563614141,"is_pvip":false,"replies":[{"id":"63662","content":"正确","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577527318,"ip_address":"","comment_id":115537,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1563614141","product_id":100021701,"comment_content":"p(C1|t) 表示节点 t 属于类别 Ck 的概率。这个和条件概率的表达形式有点像，但是意思不一样。我的理解是否正确？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":459149,"discussion_content":"正确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577527318,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":109977,"user_name":"David","can_delete":false,"product_type":"c1","uid":1506673,"ip_address":"","ucode":"5154BE2B2DF689","user_header":"https://static001.geekbang.org/account/avatar/00/16/fd/71/2c66cbdd.jpg","comment_is_top":false,"comment_ctime":1562144135,"is_pvip":false,"replies":[{"id":"39943","content":"CART算法采用二分递归分割技术将当前样本集进行二元分裂，即分为两个子样本集。所以你看到的CART树，每个每个非叶子节点都有两个分支，也就是二叉树。","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1562207958,"ip_address":"","comment_id":109977,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1562144135","product_id":100021701,"comment_content":"为啥CART只支持二叉树呢？","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":456664,"discussion_content":"CART算法采用二分递归分割技术将当前样本集进行二元分裂，即分为两个子样本集。所以你看到的CART树，每个每个非叶子节点都有两个分支，也就是二叉树。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562207958,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":92869,"user_name":"张晓辉","can_delete":false,"product_type":"c1","uid":1085046,"ip_address":"","ucode":"1CD9717DE399C5","user_header":"https://static001.geekbang.org/account/avatar/00/10/8e/76/6d55e26f.jpg","comment_is_top":false,"comment_ctime":1557359326,"is_pvip":false,"replies":[{"id":"64193","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577614000,"ip_address":"","comment_id":92869,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557359326","product_id":100021701,"comment_content":"#encoding=utf-8<br>from sklearn.datasets import load_digits<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.metrics import accuracy_score<br>from sklearn.model_selection import train_test_split<br><br>digits = load_digits()<br>features = digits.data <br>targets = digits.target <br>train_features, test_features, train_digits, test_digits = train_test_split(features, targets, test_size = 0.33)<br>clf = DecisionTreeClassifier()<br>clf = clf.fit(train_features, train_digits)<br>predict_digits = clf.predict(test_features)<br>print(&quot;The predict accuracy is:&quot;, accuracy_score(test_digits, predict_digits))","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449406,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577614000,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":86881,"user_name":"滢","can_delete":false,"product_type":"c1","uid":1221511,"ip_address":"","ucode":"971A6F20AF3F9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg","comment_is_top":false,"comment_ctime":1555470013,"is_pvip":false,"replies":[{"id":"64252","content":"Good Job","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577615360,"ip_address":"","comment_id":86881,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555470013","product_id":100021701,"comment_content":"语言版本：Python 3.6 环境：IDLE<br><br>&gt;&gt;&gt;from sklearn.model_selection import train_test_split<br>&gt;&gt;&gt;from sklearn.metrics import accuracy_score<br>&gt;&gt;&gt;from sklearn.tree import DecisionTreeClassifier<br>&gt;&gt;&gt;from sklearn.datasets import load_digits<br>&gt;&gt;&gt;import ssl<br>&gt;&gt;&gt;ssl._create_default_https_context = ssl._create_unverified_context<br>&gt;&gt;&gt;#准备数据<br>&gt;&gt;&gt;digits = load_gigits()<br>&gt;&gt;&gt;#获取特征集和分类标识<br>&gt;&gt;&gt;features = digits.data<br>&gt;&gt;&gt;labels = digits.target<br>&gt;&gt;&gt; #随机抽取40%作为测试数据，其余作为训练集<br>&gt;&gt;&gt;train_features,test_features,train_labels,test_labels = train_test_split(features,labels, test_size = 0.40, random_state = 0)<br>&gt;&gt;&gt;#创建CART分类树<br>&gt;&gt;&gt;clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br>&gt;&gt;&gt;#拟合构建CART分类树<br>&gt;&gt;&gt;clf.fit(train_features, train_labels)<br>&gt;&gt;&gt;#用CART分类树做预测<br>&gt;&gt;&gt;test_predic = clf.predict(test_features)<br>&gt;&gt;&gt;#预测结果与测试结果做比对<br>&gt;&gt;&gt;score = accuracy_score(test_labels, test_predic)<br>&gt;&gt;&gt;print (&#39;CART分类树准确率%.4lf&#39;, %score)<br>CART分类树准确率0.8164","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447228,"discussion_content":"Good Job","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577615360,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":83556,"user_name":"郑志宾","can_delete":false,"product_type":"c1","uid":1243448,"ip_address":"","ucode":"D9B010C9C71276","user_header":"https://static001.geekbang.org/account/avatar/00/12/f9/38/faec874e.jpg","comment_is_top":false,"comment_ctime":1554648931,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1554648931","product_id":100021701,"comment_content":"CART 剪枝算法中，是将剪掉的子树组成子树序列，然后进行测试？<br>还是将剪去后剩下的子树组成子树序列进行测试呢？<br><br>还有最后取得的误差最小的那个子树为什么是我们想要的结果？<br>剪枝后的决策树不是我们想要的吗？为什么还要测试子树序列？<br><br>","like_count":0},{"had_liked":false,"id":83207,"user_name":"挠头侠","can_delete":false,"product_type":"c1","uid":1150474,"ip_address":"","ucode":"F96966832E2252","user_header":"https://static001.geekbang.org/account/avatar/00/11/8e/0a/31ec5392.jpg","comment_is_top":false,"comment_ctime":1554462049,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1554462049","product_id":100021701,"comment_content":"老师 能不能分析一下决策数图里面的内容分别是什么和如何算出来的呀。","like_count":0},{"had_liked":false,"id":73237,"user_name":"Linus","can_delete":false,"product_type":"c1","uid":1340107,"ip_address":"","ucode":"6B9D638DF1A3FC","user_header":"https://static001.geekbang.org/account/avatar/00/14/72/cb/5d58f190.jpg","comment_is_top":false,"comment_ctime":1551843625,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1551843625","product_id":100021701,"comment_content":"X[2] &lt;= 5.415<br>mse = 0.955<br>samples = 27<br>value = 23.785<br>老师能不能解释一下x[2]&lt;=5.415 是不是过滤了小于5.415的数字 再进行算gini系数","like_count":0},{"had_liked":false,"id":69447,"user_name":"littlePerfect","can_delete":false,"product_type":"c1","uid":1243521,"ip_address":"","ucode":"99E8B9D004BE0D","user_header":"https://static001.geekbang.org/account/avatar/00/12/f9/81/54b1a5a8.jpg","comment_is_top":false,"comment_ctime":1550747978,"is_pvip":false,"replies":[{"id":"64516","content":"看下特征名称都有哪些","user_name":"作者回复","user_name_real":"cy","uid":"1306094","ctime":1577621154,"ip_address":"","comment_id":69447,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1550747978","product_id":100021701,"comment_content":"探索数据那一行代码有什么作用?<br>print(boston.feature_names)","like_count":0,"discussions":[{"author":{"id":1306094,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ed/ee/c4779b67.jpg","nickname":"cy","note":"","ucode":"50D653399A31F6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440127,"discussion_content":"看下特征名称都有哪些","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577621154,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67755,"user_name":"柚子","can_delete":false,"product_type":"c1","uid":1382672,"ip_address":"","ucode":"721F6C7F5DD303","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI45zO9GOMquk9JymTibN9sC25Sy4WtsDGRQzIRVIoIzPnaJGKmGe3jXqxP0zKZyTYazrXHBGYjBzw/132","comment_is_top":false,"comment_ctime":1550242675,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1550242675","product_id":100021701,"comment_content":"from sklearn.model_selection import train_test_split<br>from sklearn.datasets import load_digits<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.metrics import accuracy_score<br><br>digits = load_digits()     #准备数据集<br>features = digits.data   #获取特征值<br>labels = digits.target    #获取分类类别<br><br>train_features,test_features,train_labels,test_labels = train_test_split(features,labels,test_size = 0.33,random_state = 0)     #随机抽取33%作为测试集<br><br>dig = DecisionTreeClassifier(criterion=&#39;gini&#39;)            #创建决策树<br>dig.fit(train_features,train_labels)                             #拟合构造决策树<br>test_predict = dig.predict(test_features)                  #预测<br>score = accuracy_score(test_labels,test_predict)      #测试集结果评价<br>print(&#39;决策树准确率：%.4lf&#39;% score)","like_count":0},{"had_liked":false,"id":63909,"user_name":"胖陶","can_delete":false,"product_type":"c1","uid":1372987,"ip_address":"","ucode":"EE32D91A3BC0F3","user_header":"https://static001.geekbang.org/account/avatar/00/14/f3/3b/15ffb7c3.jpg","comment_is_top":false,"comment_ctime":1548590840,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548590840","product_id":100021701,"comment_content":"# encoding=utf-8<br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br># 准备数据集<br>digits=load_digits()<br># 获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br># 随机抽取 33% 的数据作为测试集，其余为训练集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.45, random_state=0)<br># 创建 CART 分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br># 拟合构造 CART 分类树<br>clf = clf.fit(train_features, train_labels)<br># 用 CART 分类树做预测<br>test_predict = clf.predict(test_features)<br># 预测结果与测试集结果作比对<br>score = accuracy_score(test_labels, test_predict)<br>print(&quot;CART 分类树准确率 %.4lf&quot; % score)","like_count":0},{"had_liked":false,"id":63291,"user_name":"Python","can_delete":false,"product_type":"c1","uid":1276314,"ip_address":"","ucode":"969500D2A88AE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg","comment_is_top":false,"comment_ctime":1548317885,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548317885","product_id":100021701,"comment_content":"老师，剪枝是用什么函数来实现的？","like_count":0},{"had_liked":false,"id":63187,"user_name":"FengX","can_delete":false,"product_type":"c1","uid":1353041,"ip_address":"","ucode":"B1B0235B1D1935","user_header":"https://static001.geekbang.org/account/avatar/00/14/a5/51/7773d421.jpg","comment_is_top":false,"comment_ctime":1548294384,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548294384","product_id":100021701,"comment_content":"老师，这一节课里的归一化基尼系数的公式里，所乘的比例里分母D=9，只看父节点里打篮球的数；而上一节课里，归一化信息熵的公式里，所成的比例里分母D=10，把父节点里的打篮球与不打篮球的数量都计算在内。想问下，为什么归一化基尼系数里的分母不将不打篮球的数计算在内呢？","like_count":0},{"had_liked":false,"id":63140,"user_name":"从未在此","can_delete":false,"product_type":"c1","uid":1354589,"ip_address":"","ucode":"5A4AA275D8EE9A","user_header":"https://static001.geekbang.org/account/avatar/00/14/ab/5d/430ed3b6.jpg","comment_is_top":false,"comment_ctime":1548286872,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548286872","product_id":100021701,"comment_content":"那个九分之六和九分之三，计算的时候应该之考虑我们需要的正样本。比如例子中，一共12个人，但只有9个人打篮球，所以计算时总样本是9。我是这么理解的不知道对不对","like_count":0},{"had_liked":false,"id":63051,"user_name":"JingZ","can_delete":false,"product_type":"c1","uid":1023464,"ip_address":"","ucode":"6F97895B2CC375","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132","comment_is_top":false,"comment_ctime":1548237470,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1548237470","product_id":100021701,"comment_content":"#2019&#47;1&#47;23<br><br>ID3:信息增益，C4.5信息增益率，CART基尼系数，需要上手，这块很陌生<br><br>代码熟悉一下<br><br>#encoding= utf-8<br><br>from sklearn.model_selection import train_test_split<br>from sklearn.metrics import accuracy_score<br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.datasets import load_digits<br><br>#准备数据集<br>digits = load_digits()<br><br>#探索数据<br>print(digits)<br><br>#获取特征集和分类标识<br>features = digits.data<br>labels = digits.target<br><br>#随机抽取33%作为测试集<br>train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)<br><br>#创建分类树<br>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)<br><br>#拟合分类树<br>clf = clf.fit(train_features, train_labels)<br><br>#预测<br>test_predict = clf.predict(test_features)<br><br>#比对<br>score = accuracy_score(test_labels, test_predict)<br><br>print(&quot;CART 分类树准确率 %.4lf&quot; %score)","like_count":0}]}