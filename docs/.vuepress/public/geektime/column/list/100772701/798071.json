{"id":798071,"title":"24｜带你看一个完整的向量数据库Milvus","content":"<p>你好，我是彭旭。</p><p>前面几节课我们介绍了Faiss，学习了使用Faiss构建索引与向量化检索，也用Faiss来搭建了一个简单的人脸识别应用。</p><p>通过对Faiss的介绍，你应该知道，Faiss不像数据库一样持久化存储向量化数据，而是每次使用之前，都需要从硬盘等持久化设备读取数据，加载索引。并且Faiss也没有一个数据库所需要的数据管理功能。所以 <a href=\"https://db-engines.com/en/ranking/vector+dbms\">DBRanking</a> 对向量数据库的排名，并没有将Faiss包括在内。</p><p>那有没有一个具备完整的数据存储、管理功能的向量数据库产品呢？</p><p>有，就是我们这节课要介绍的Milvus。</p><h2>Milvus是什么？</h2><p>Milvus其实是基于Faiss、HNSW、DiskANN、SCANN（Scalable Approximate Nearest Neighbor）等这些向量检索库构建的，被设计用来做稠密向量的相似性检索。它可以支持十亿，甚至万亿以上向量化数据的存储检索。</p><p>Milvus支持数据分片、动态Schema、单向量检索、多向量检索、向量与标量混合检索以及许多其他高级功能。</p><p>与StarRocks类似，Milvus也支持存算分离。因为Milvus使用MinIO对象存储来存储日志文件的快照、索引文件、数据以及一些查询的中间结果，所以能够快速地部署在兼容MinIO协议的AWS S3和Asure Blob上。</p><!-- [[[read_end]]] --><p>最后，作为一个具备完整数据管理功能的向量数据库，GUI也是必不可少的一项，Milvus提供了一个attu的Web GUI工具，你可以在<a href=\"https://github.com/zilliztech/attu\">这里</a>下载部署。</p><h2>Milvus架构</h2><p><img src=\"https://static001.geekbang.org/resource/image/7d/fa/7d8a3f8201fd61b7b5d6b7105c1755fa.png?wh=2280x1280\" alt=\"图片\" title=\"图片源自网络\"></p><p>从架构上来看，Milvus服务包括四层：访问层、协调服务层、工作节点和存储。这几个分层在弹性伸缩与故障恢复上可以相互独立。</p><p><strong>访问层</strong>由一组无状态的代理服务组成，接收客户端的请求，有点类似StarRocks的FE，访问层使用负载均衡工具（比如Nginx、Kubernetes Ingress、NodePort等）来提供一个统一的服务地址。Milvus也采用了大规模并行处理（MPP）架构，请求被并发地发送到后端处理，然后在访问层做一个汇总，最终将结果返回给客户端。</p><p><strong>协调服务层</strong>是Milvus的“大脑”，又可以分为三种类型的协调服务。</p><ol>\n<li>Root coordinator：用来处理DDL和DCL请求，比如创建、删除Collection、Partition、Index等，在Milvus里面，Collection就相当于关系型数据库里面的表。</li>\n<li>Query coordinator：负责管理和协调查询操作，包括查询的拓扑结构管理、负载均衡、查询节点之间的通信和数据传输等。</li>\n<li>Data coordinator：负责管理和协调数据的存储和访问，包括数据的分片、索引构建、数据迁移等工作。 确保数据的高效存储和检索，是实现数据的快速查询的核心。</li>\n</ol><p><strong>工作节点</strong>是Milvus的手脚，由于Milvus是存储与计算分离，所以工作节点是无状态的，会无脑执行从代理传来的数据操作语言（DML）命令。同样地，工作节点也可以分为三种类型。</p><ol>\n<li>\n<p>Query Node：主要负责接收用户的查询请求，并进行向量相似度搜索和数据分析等操作。查询节点负责对数据进行检索和计算，返回查询结果给用户。</p>\n</li>\n<li>\n<p>Data Node：Milvus数据的DML操作，同样会生成类似Redo Log的预写入日志，然后写入消息中间件，Data Node就从消息中间件订阅这些日志消息，处理变更请求，并将日志数据打包成日志快照，存储在对象存储中。</p>\n</li>\n<li>\n<p>Index Node：负责构建索引，管理索引。</p>\n</li>\n</ol><p><strong>存储层</strong>是Milvus的骨架，负责数据的持久化，存储层包含三种类型的数据存储。</p><ol>\n<li>Meta Storage存储元数据的快照，比如数据库schema、日志消费checkpoint等，为了保障存储元数据需要极高的可用性、强一致性和事务支持，Milvus使用etcd存储元数据。</li>\n<li>Object Storage使用MinIO对象存储日志文件的快照、索引文件、数据以及一些查询的中间结果。</li>\n<li>Log Broker是Milvus的脊椎。它负责数据持久性和读写分离，系统数据的操作，都会以日志的形式写入Log Broker, 然后给到下游消费去更新本地数据。Milvus集群使用Pulsar作为Log Broker，单机版采用RocksDB作为Log Broker。</li>\n</ol><p>从架构上看，Milvus就是一个大数据时代常用的分布式架构模式，不过Milvus作为向量数据库，不支持SQL查询，反而提供了丰富了SDK，比如Python、Java、Go等。</p><p>接下来，我们就来看一下Milvus的逻辑模型，在这个过程中使用Python SDK来演示下与Milvus的交互过程。</p><h2>Milvus的逻辑模型</h2><p>Milvus的逻辑模型与关系型数据库类似。</p><p><strong>首先，你可以创建database</strong>，然后给特定用户分配权限，来管理这个database。</p><pre><code class=\"language-shell\">from pymilvus import connections, db\nconn = connections.connect(host=\"127.0.0.1\", port=19530) \ndatabase = db.create_database(\"RAG\")\n</code></pre><p>与其他数据库直接授权给用户的方式不同，Milvus使用RBAC（Role-Based Access Control，基于角色的权限控制）来控制资源的访问，比如database、collection，甚至是partition。你可以新建一个角色，然后授予这个角色一些权限，最后将这个角色跟用户关联起来。</p><pre><code class=\"language-shell\">from pymilvus import MilvusClient\nclient = MilvusClient(\n    uri='http://127.0.0.1:19530', \n    token='root:Milvus' # username:password\n)\nclient.create_role(\n    role_name=\"roleA\",\n)\nclient.grant_privilege(\n    role_name='roleA',\n    object_type='User',\n    object_name='SelectUser',\n    privilege='SelectUser'\n)\nclient.create_user(\n    user_name='rag_user',\n    password='P@ssw0rd'\n)\nclient.grant_role(\n    user_name='rag_user',\n    role_name='roleA'\n)\n</code></pre><p><strong>其次，database里面可以有多个collection</strong><strong>。</strong>Collcection就类似关系型数据库的表。Milvus创建collection的时候，需要指定字段field。它也同时支持动态字段，插入时，新增的动态字段会以Key-Value的形式存储在一个叫做$meta的特殊字段当中。</p><pre><code class=\"language-shell\">from pymilvus import MilvusClient, DataType\n\n# 2. Create schema\nschema = MilvusClient.create_schema(\n    auto_id=False,\n    enable_dynamic_field=True,\n)\nschema.add_field(field_name=\"pk\", datatype=DataType.INT64, is_primary=True, auto_id=True)\nschema.add_field(field_name=\"user_id\", datatype=DataType.INT64, description='user unique id')\nschema.add_field(field_name=\"file_sha1\", datatype=DataType.VARCHAR, description='file sha1', max_length=128)\nschema.add_field(field_name=\"file_size\", datatype=DataType.INT64, description='file size')\nschema.add_field(field_name=\"file_name\", datatype=DataType.VARCHAR, description='file name', max_length=512)\nschema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, description='the original content', max_length=65535)\nschema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, description='Embedding vectors', dim=1536)\n\n# 3、创建index\nindex_params = client.prepare_index_params()\n\nindex_params.add_index(\n    field_name=\"vector\",\n    index_type=\"IVF_FLAT\",\n    metric_type=\"IP\",\n    params={\"nlist\": 128}\n)\n# 4、创建collection，同时加载index\nclient.create_collection(\n    collection_name=\"RAG_COLLECTION\",\n    schema=schema,\n    index_params=index_params\n)\n</code></pre><p>注意，创建collection的时候，一般需要同时创建好索引。像关系型数据库一样，创建一个表的时候，就需要考虑怎么样去访问这个表，根据使用场景设计好索引。当然，你也可以先创建collection，然后再添加一个索引。collection在开始写入数据与查询之前，需要先load，load的过程就会将索引加载到内存。</p><pre><code class=\"language-shell\"># 5. Load the collection\nclient.load_collection(\n    collection_name=\"RAG_COLLECTION\",\n    replica_number=1\n)\n</code></pre><p>在load collection的时候，可以设置一个collection数据需要加载的副本数replica_number，在单机版本下，这个副本数最大值是1，在集群模式下，最大值则是queryNode.replicas这个配置的值。</p><p>在集群模式下启用多副本，Milvus会将同一个数据片段分布到多个queryNode，这样可以用来容灾与提升查询性能。</p><p><strong>然后，当你创建一个collection时，Milvus会为这个collection默认创建一个叫做 _default的分区 partitions。</strong>在一个collection下，最多可以创建4096个分区。</p><p>Milvus有两种创建partition的方式。</p><p>第一种是在创建完collection后，手动再创建分区。</p><pre><code class=\"language-shell\"># 4、创建分区\nclient.create_partition(\n    collection_name=\"RAG_COLLECTION\",\n    partition_name=\"partition_technology\"\n)\n</code></pre><p>第二种是在创建collection的时候，使用一个字段，作为分区键来分区。</p><pre><code class=\"language-shell\">schema = MilvusClient.create_schema(\n    auto_id=False,\n    enable_dynamic_field=True,\n    partition_key_field=\"user_id\",\n    num_partitions=16 # Number of partitions. Defaults to 16.\n)\n</code></pre><p>Milvus使用分区键来创建分区的方式跟其他分布式数据库一样。使用分区键查询的时候，同样可以用分区键来“裁剪”查询时需要扫描的数据量。</p><h2>数据写入与读取</h2><p>一个collection里面一般会定义多个标量字段，以及至少1个向量字段。创建好collection并load后，就可以开始向collection写入与读取这些标量与向量数据了。</p><pre><code class=\"language-shell\">import random\nfrom pymilvus import MilvusClient\n\n# 2. Insert randomly generated vectors\ndata = []\nfor i in range(1000):\n    user_id = i % 2 + 1\n    file_sha1 = ''.join(random.choices('abcdef0123456789', k=128))\n    file_size = random.randint(1, 10000)\n    file_name = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=20))\n    text = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=100))\n    vector = [random.random() for _ in range(1536)]\n    data.append({\n        \"user_id\": user_id,\n        \"file_sha1\": file_sha1,\n        \"file_size\": file_size,\n        \"file_name\": file_name,\n        \"text\": text,\n        \"vector\": vector\n    })\nres = client.insert(\n    collection_name=\"RAG_COLLECTION_P\",\n    data=data\n)\n</code></pre><p>上面的代码片段，为两个用户随机准备了1000条记录，然后插入了我们之前创建的以user_id作为partition key的collection。从下面的图可以看到，这两个用户的数据，就是分布到了两个partition。</p><p><img src=\"https://static001.geekbang.org/resource/image/32/3a/32c7bde103dec57654eb7bf830dac33a.png?wh=2260x1872\" alt=\"图片\"></p><p>接下来就可以开始检索数据了，在Milvus中，基于标量字段的查询被称为Query，基于向量字段的查询被称为Search。</p><p>在2.3.x版本中，同时基于标量字段与向量字段的查询被叫做Hybrid Search。</p><pre><code class=\"language-shell\">query_vectors = [[random.random() for _ in range(1536)]]\n\nresult = client.search(\n    collection_name=\"RAG_COLLECTION_P\",\n    data=query_vectors,\n    filter=\"user_id == 1 &amp;&amp; file_name like 'pmp%'\",\n    search_params={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}},\n    output_fields=[\"user_id\", \"file_name\", \"text\"],\n    limit=3\n)\n</code></pre><p>比如上面的代码片段，通过标量字段过滤了user_id=1 , 文件名以pmp开头的数据，然后再进行向量化检索，找到与向量query_vectors近似的数据行，就是一种Hybrid Search。</p><p>注意，向量化检索时，度量方式metric_type的取值必须与创建索引时指定的值一样，这里使用IP（Inner Product，内积）的度量方式。</p><p>但是在2.4.x版本里，“Hybrid Search”被用在一个collection中存在多个向量字段，并且多个向量字段被同时检索的场景。</p><p>多个向量字段同时检索时，需要选择一个Ranking的策略，用来决定结果的排序，比如WeightedRanker策略，会为每个向量字段分配一个权重，结果就基于这个权重来排序。</p><p>2.4.x版本，还增加了一个基于主键列，作为id Get数据行的方法。</p><pre><code class=\"language-shell\"># 4. Get entities by ID\nres = client.get(\n    collection_name=\"RAG_COLLECTION_P\",\n    ids=[0, 1, 2]\n)\n</code></pre><p>好了，到这里你已经对Milvus的逻辑模型有一定了解，能够使用Milvus去存储数据、查询数据了，你还可以使用各种索引来调优数据的读取性能。</p><h2>Milvus的索引</h2><p>Milvus支持给向量字段、标量字段创建索引。具体怎么索引呢？</p><p>首先，collection的数据由多个不可变的数据片段组成，这些数据片段被称为segment。</p><p>当给标量字段创建索引后，向量化检索的时候如果带上这个索引字段作为过滤条件，Milvus会检查每个segment是否包含查询的数据，然后构建一个segment是否存在数据的bitset。向量检索的时候，会把这个bitset作为查询参数传递，用来“裁剪”需要检索的segment。看到这里，是不是又想起了布隆过滤器？</p><p><strong>好，再说向量字段的索引。</strong>Milvus支持几种模式：In-Memory Index、On-Disk Index、GPU Index。分别适用不同的场景。</p><p>比如我们的人脸识别场景，正常情况下使用In-Memory Index，查询的响应时间能够控制在几十到几百毫秒。</p><p><strong>In-Memory Index是最常用的向量索引方式，向量索引基于内存存储</strong>。其实跟Faiss类似，支持多种ANNS算法索引与度量方式，两者组合就形成了一个索引类型。以浮点数向量索引为列，你可以参考下表：</p><p><img src=\"https://static001.geekbang.org/resource/image/82/89/82efe05c26fb2b9e9f660d1d7eef6389.png?wh=2916x1516\" alt=\"图片\"></p><pre><code class=\"language-shell\">index_params.add_index(\n    field_name=\"vector\",\n    index_type=\"IVF_FLAT\",\n    metric_type=\"IP\",\n    params={\"nlist\": 128}\n)\n</code></pre><p><strong>当人脸图片数量增加到一定程度，它向量化后的数据，在内存中已经放不下了，这时候就可以考虑使用On-Disk Index</strong>，但是相应地也会带来性能的下降。</p><p>创建On-Disk Index的时候，只需要指定index_type为DISKANN。</p><pre><code class=\"language-shell\">index_params.add_index(\n    field_name=\"vector\",\n    index_type=\"DISKANN\",\n    metric_type=\"IP\"\n)\n</code></pre><p><strong>如果需要进一步提升性能与吞吐量，就可以考虑GPU Index</strong>，使用GPU Index的吞吐量可以是In-Memory Index的几倍，性能上通常也能够得到提升，不过GPU机器一般比较贵，所以会带来成本的上升。</p><p>同样你可以参考下表GPU索引支持的索引类型与度量方式<strong>。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/b2/a4/b20c5cbe11c6fbc129af3821e5a320a4.png?wh=2916x1316\" alt=\"图片\"></p><p>你可以在<a href=\"https://github.com/ZHAMoonlight/referencebook/tree/master/python/ls24\">这里</a>找到完整的代码清单，帮你理解、测试Milvus。</p><h2>小结</h2><p>在DBRanking对纯向量数据库的排名中，Milvus排名第2，仅次于Pinecone。这里我认为有很多方面的考量，比如Milvus的性能突出、分布式集群架构，能够支撑的数据体量巨大、简单易用、支持多语言SDK访问、同时支持私有化与SaaS、支持GPU索引等。所以Milvus号称全球用得最广的向量数据库，被超过5000家企业使用。</p><p>此外，Milvus有多种多租户支持的方式，比如基于database隔离、基于collection隔离、基于partition隔离、基于字段隔离。如果你需要构建一个在线的SaaS系统，Milvus也为多租户、多用户的数据隔离，提供了非常灵活的支持。</p><p>同时，Milvus架构也吸取了分布式关系型数据库的优点，比如支持数据分片、多副本等等。Milvus的数据片段写入后不可变，但也支持upsert来更新数据。它也和LSM类似，也会有后台compact任务来合并segment，同时清理删除的数据。</p><p>Milvus支持内存索引、磁盘索引、GPU索引。如果使用内存索引，索引节点会在collection加载的时候，把索引数据写入内存。</p><p>说了这么多优点，再说说我的感受。上节课我用Faiss构建了人脸识别系统，对比用Milvus构建个人知识库的时候，会感觉到Milvus在响应上更快。而且SDK的支持种类比较多，API设计简单，用起来确实便捷。下节课，我们就一起上手体验Milvus。</p><h2>思考题</h2><p>Milvus的标量索引，其实是以搜索引擎的逻辑来构建的，你知道它用的是什么数据结构吗？</p><p>欢迎你在留言区和我交流。如果觉得有所收获，也可以把课程分享给更多的朋友一起学习。欢迎你加入我们的<a href=\"http://jinshuju.net/f/QX5mGO\">读者交流群</a>，我们下节课见！</p>","neighbors":{"left":{"article_title":"23｜案例：带你用Faiss手撸一个人脸识别系统","id":797673},"right":{"article_title":"25｜案例：RAG+Milvus+大模型，搭建个人知识库","id":798124}},"comments":[]}