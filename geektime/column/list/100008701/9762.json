{"id":9762,"title":"10 | 特征预处理","content":"<p>华盛顿大学教授、《终极算法》（The Master Algorithm）的作者佩德罗·多明戈斯曾在Communications of The ACM第55卷第10期上发表了一篇名为《机器学习你不得不知的那些事》（A Few Useful Things to Know about Machine Learning）的小文，介绍了12条机器学习中的“金科玉律”，其中的7/8两条说的就是对数据的作用的认识。</p>\n<p><strong>多明戈斯的观点是：数据量比算法更重要</strong>。即使算法本身并没有什么精巧的设计，但使用大量数据进行训练也能起到填鸭的效果，获得比用少量数据训练出来的聪明算法更好的性能。这也应了那句老话：<strong>数据决定了机器学习的上限，而算法只是尽可能逼近这个上限</strong>。</p>\n<p>但多明戈斯嘴里的数据可不是硬件采集或者软件抓取的原始数据，而是经过特征工程处理之后的精修数据，<strong>在他看来，特征工程（feature engineering）才是机器学习的关键</strong>。通常来说，原始数据并不直接适用于学习，而是特征筛选、构造和生成的基础。一个好的预测模型与高效的特征提取和明确的特征表示息息相关，如果通过特征工程得到很多独立的且与所属类别相关的特征，那学习过程就变成小菜一碟。</p>\n<p><strong>特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码。</strong>机器学习中的很多具体算法都可以归纳到特征工程的范畴之中，比如使用$L_1$正则化项的<strong>LASSO回归</strong>，就是通过将某些特征的权重系数缩小到0来实现特征的过滤；再比如<strong>主成分分析</strong>，将具有相关性的一组特征变换为另一组线性无关的特征。这些方法本质上完成的都是特征工程的任务。</p><!-- [[[read_end]]] -->\n<p>但是今天，我将不会讨论这些，而是把关注点放在算法之外，看一看<strong>在特征工程之前，数据的特征需要经过哪些必要的预处理（preprocessing）</strong>。</p>\n<p><strong>特征缩放</strong>（feature scaling）可能是最广为人知的预处理技巧了，它的目的是<strong>保证所有的特征数值具有相同的数量级</strong>。在有些情况下，数据中的某些特征会具有不同的尺度，比如在电商上买衣服时，身高和体重就是不同尺度的特征。</p>\n<p>假设我的身高/体重是1.85米/64公斤，而买了同款衣服的两个朋友，1.75米/80公斤的穿L号合适，1.58米/52公斤的穿S号正好。直观判断的话，L码应该更合适我。可如果把（身高，体重）的二元组看作二维空间上的点的话，代表我自己的点显然和代表S码的点之间的欧式距离更近。如果电商不开眼的话，保不齐就会把S码推荐给我。</p>\n<p>实际上，不会有电商做出这么弱智的推荐，因为他们都会进行特征缩放。在上面的例子中，由于体重数据比身高数据高出了一个数量级，因此在计算欧式距离时，身高的影响相比于体重是可以忽略不计的，起作用的相当于只有体重一个特征，这样的算法自然就会把体重相近的划分到同一个类别。</p>\n<p><strong>特征缩放的作用就是消除特征的不同尺度所造成的偏差</strong>，具体的变换方法有以下这两种：</p>\n<ul>\n<li><p><strong>标准化</strong>（standardization）：$x_{st} = \\dfrac{x - mean(x)}{stdev(x)}$</p>\n</li>\n<li><p><strong>归一化</strong>（normalization）：$x_{norm} = \\dfrac{x - \\min (x)}{\\max (x) - \\min (x)}$</p>\n</li>\n</ul>\n<p>不难看出，<strong>标准化的方法用原始数据减去均值再除以标准差，不管原始特征的取值范围有多大，得到的每组新数据都是均值为0，方差为1</strong>，这意味着所有数据被强行拉到同一个尺度之上；<strong>归一化的方法则是用每个特征的取值区间作为一把尺子，再利用这把尺将不同的数据按比例进行转换，让所有数据都落在[0, 1]这个范围之内</strong>。虽然实现方式不同，但两者都能够对数据做出重新标定，以避免不同尺度的特征产生不一致的影响，可谓殊途同归。</p>\n<p>除了尺度之外，数据的偏度也是值得关注的一个问题。<strong>偏度（skewness）是用于描述概率分布非对称性的一个指标</strong>。下图给出了两个分别具有<strong>负偏度</strong>和<strong>正偏度</strong>的概率分布示意图，从中可以看出具有偏度的分布的形状都是类似的：一侧是瘦高的形状，占据了概率分布的大部分，另一侧则是比较长的拖尾。</p>\n<p>想要理解这个图形所表示的概率分布，只要把正偏度的图形想象成你所在单位的工资分布就可以了：左侧的瘦高形状表示了拿着低工资的绝大部分普通员工，右侧的拖尾则表示了工资更高、但人数更少的中层领导和高级主管。无论机关、事业单位还是企业，工资的分布大抵都是这样。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/f3/65/f3156d2ed2acd7f32386931469916265.png?wh=446*159\" alt=\"\">\n <center><span class=\"reference\">不同偏度的概率分布（图片来自维基百科）</span></center></p>\n<p>数据服从有偏分布意味着什么呢？意味着数据当中可能存在着<strong>异常点</strong>（outlier）。30个维秘模特的体重应该近似地服从正态分布，而正态分布是无偏的对称分布。可是如果把其中一个模特的体重换成相扑运动员的体重，这个数据集的均值就会产生明显的上升，数据的直方图也会朝新均值的反方向产生明显的偏移。这时，偏度就体现为少量异常点对样本整体的拉拽作用，类似于用一个董事长和99个普通工人计算平均工资产生的喜剧效果。</p>\n<p><strong>面对偏度较大的数据，第一反应就应该是检查是否有异常点存在</strong>。一般来说，如果少量数据点和其他数据点有明显区别，就可以认为是异常点。在处理异常点时，首先要检测这些数据的<strong>可靠性</strong>，判断异常取值是不是由错误或者失误导致，比如那个混进维秘模特里的相扑选手。</p>\n<p>如果异常点本身并没有问题，需要考虑的下一个问题就是异常点和正常点是否<strong>来源于不同的生成机制</strong>，从而具有不同的概率分布。如果对异常点所在的分布的采样数据较少，就不足以体现出分布的特性，导致单个数据点看起来显得突兀。</p>\n<p>对于像决策树这类对异常点比较敏感的算法来说，不管来源如何，异常点都需要被处理。最直接的处理办法就是<strong>将异常点移除</strong>，但当数据集容量较小时，这种一刀切的方式会进一步减少可用的数据，造成信息的丢失，这时就需要采用名为“<strong>空间标识</strong>”（spatial sign）的数值处理方法。</p>\n<p>空间标识方法先对所有的数据点进行前面提到的标准化处理，再用样本向量的2范数对样本中的所有特征进行归一化，其数学表达式可以写成</p>\n<p>$$ x_{ij}^* = \\dfrac{x_{ij}}{\\sum_{j = 1}^N x_{ij}^2} $$</p>\n<p>式中的$N$是数据的维度。显然，<strong>空间标识算法将所有的数据点都映射到高维空间的球面上，这个映射和标准化或者归一化的不同之处在于它处理的对象并不是所有样本的同一个特征，而是同一个样本的所有特征，让所有样本呈现一致的尺度</strong>。</p>\n<p>当然，即使在没有异常点的情况下，数据依然可能呈现出有偏的分布，这在数字图像处理中并不罕见。有偏分布的一个明显特点是最大值和最小值之间相差较大，通常可以达到<strong>20倍</strong>或者更高。</p>\n<p>这种数据尺度的不一致即使出现在单个特征上也不是一件好事情，对它进行修正，也就是<strong>对数据进行去偏度处理的常用方法就是取对数变换（log transformation）</strong>，也就是对特征取值取对数。最大值和最小值之间的20倍差距经过对数变换后变为$\\log_2 20 = 4.3$，这就在一个可以接受的范围内了。除了对数之外，<strong>求平方根和求倒数也是移除偏度的常见处理方式</strong>。</p>\n<p>异常点也好，尺度不一致的数据也好，它们至少还都是完整的数据。可有些时候，一个样本里的某些特征会压根儿没有取值，而是一片空白，这种情况被称为<strong>缺失值</strong>（missing values）。</p>\n<p>数据缺失的可能原因多种多样，在这里就不做展开了，关键还是在于如何处理这些缺失值。最简单粗暴的办法依然是将不完整的数据全部删除，对小数据集来说这依然不是好办法。更主动的处理方式是<strong>给这些缺失值进行人为的赋值（imputation）</strong>，就像数值计算或者信号处理中的插值方法一样。</p>\n<p>人为赋值相当于在机器学习中又嵌套了一层机器学习，里层的机器学习被用于估计未知的属性值，也要使用训练数据。最常用的赋值算法是<strong>K近邻算法</strong>：选取离具有缺失值的样本最近的K个样本，并以它们对应特征的平均值为缺失值赋值。此外，<strong>线性回归</strong>也可以用来对缺失值进行拟合。但可以确定的是，不管采用什么方法，人为赋值都会引入额外的不确定性，给模型带来的性能造成影响。</p>\n<p>会做加法也要会做减法，缺失的数据需要添加，多余的数据也要删除。<strong>在模型训练之前移除一些特征有助于增强模型的可解释性，也可以降低计算中的开销</strong>。如果两个特征之间的相关性较强，或者说具有<strong>共线性</strong>（collinearity），这时就可以删除掉其中的一个，这正是<strong>主成分分析</strong>的作用。</p>\n<p>除此之外，如果某个特征在绝大多数数据中的取值都是相同的，那这个特征就没有存在的意义，因为它体现不出对于不同分类结果的区分度。这就像在学校里，老师给所有同学的出勤都打满分，这部分平时分是拉不开成绩差距的。</p>\n<p>什么样的特征不具备区分度呢？这里有两个经验性的标准：<strong>一是特征取值的总数与样本数目的比例在10%以下</strong>，这样的特征在100个样本里的取值数目不超过10个；<strong>二是出现频率最高的特征取值的出现频率应该在出现频率次高的特征取值频率的20倍以上</strong>，如果有90个样本的特征取值为1，4个样本的特征取值为2，其余取值的样本数目都在4个以下，这样的特征就可以被删除了。</p>\n<p>今天我和你分享了在模型训练之前对数据特征进行预处理的一些指导性原则，其要点如下：</p>\n<ul>\n<li><p><span class=\"orange\">特征缩放可以让不同特征的取值具有相同的尺度，方法包括标准化和归一化；</span></p>\n</li>\n<li><p><span class=\"orange\">异常点会导致数据的有偏分布，对数变换和空间标识都可以去除数据的偏度；</span></p>\n</li>\n<li><p><span class=\"orange\"> $k$近邻方法和线性回归可以用来对特征的缺失值进行人为赋值；</span></p>\n</li>\n<li><p><span class=\"orange\">删除不具备区分度的特征能够降低计算开销，增强可解释性。</span></p>\n</li>\n</ul>\n<p>这里介绍的特征预处理技巧可以说是挂一漏万。那么在实际的任务当中，你遇到过哪些不理想的特征数据，又是如何处理的呢？</p>\n<p>欢迎分享你的经历。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/ee/42/ee7773e41b2173cf2cc244be529d0c42.jpg?wh=2379*2408\" alt=\"\"></p>\n<p></p>\n","comments":[{"had_liked":false,"id":24478,"user_name":"曾珍","can_delete":false,"product_type":"c1","uid":1210847,"ip_address":"","ucode":"9ED100FFBC0882","user_header":"https://static001.geekbang.org/account/avatar/00/12/79/df/bae913b7.jpg","comment_is_top":false,"comment_ctime":1537067711,"is_pvip":false,"replies":[{"id":"9554","content":"onehot确实比较常用","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1537530697,"ip_address":"","comment_id":24478,"utype":1}],"discussion_count":2,"race_medal":0,"score":"14421969599","product_id":100008701,"comment_content":"空值我是用独热编码的方式，好想处理结果比线回归填充好一点","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":424001,"discussion_content":"onehot确实比较常用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1537530697,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":215823,"discussion_content":"One-hot 不是用来处理类别型特征？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585387740,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":26591,"user_name":"五岳寻仙","can_delete":false,"product_type":"c1","uid":1247467,"ip_address":"","ucode":"88A46A4E1832CE","user_header":"https://static001.geekbang.org/account/avatar/00/13/08/eb/594e9e6c.jpg","comment_is_top":false,"comment_ctime":1537689416,"is_pvip":false,"replies":[{"id":"9687","content":"这个问题应该这么理解：性别这个特征本来就只有2个可能的取值，所以相当于每个值都取到了。如果说某个特征可能的取值范围是所有的正整数，但数据里只有1 2 3这三个，这才是文章里所说的情况。<br>另一个角度看，在性别这个特征上，如果100个数据里98个是男的，这样的特征也没什么意义。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1537716526,"ip_address":"","comment_id":26591,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5832656712","product_id":100008701,"comment_content":"老师好！在删除不具备区分度的特征时，老师讲到：<br><br>什么样的特征不具备区分度呢？这里有两个经验性的标准：一是特征取值的总数与样本数目的比例在 10% 以下，这样的特征在 100 个样本里的取值数目不超过 10 个；二是出现频率最高的特征取值的出现频率应该在出现频率次高的特征取值频率的 20 倍以上，如果有 90 个样本的特征取值为 1，4 个样本的特征取值为 2，其余取值的样本数目都在 4 个以下，这样的特征就可以被删除了。<br><br>我不太理解，意思是如果一个特征(类别变量)的取值太少(小于样本数的10%)就该被删掉吗？可是我们平时遇到很多情况，类别变量取值都是有限的几个(比如性别：男，女)。","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":424681,"discussion_content":"这个问题应该这么理解：性别这个特征本来就只有2个可能的取值，所以相当于每个值都取到了。如果说某个特征可能的取值范围是所有的正整数，但数据里只有1 2 3这三个，这才是文章里所说的情况。\n另一个角度看，在性别这个特征上，如果100个数据里98个是男的，这样的特征也没什么意义。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1537716526,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":13873,"user_name":"我心飞扬","can_delete":false,"product_type":"c1","uid":1149164,"ip_address":"","ucode":"E41155122C9A1F","user_header":"https://static001.geekbang.org/account/avatar/00/11/88/ec/1460179b.jpg","comment_is_top":false,"comment_ctime":1530052074,"is_pvip":false,"replies":[{"id":"4666","content":"空间标识是把异常点拉成正常的，log是处理单个特征取值范围过大的，两个解决的不是一个问题。所以结合起来用原则上可以，关键还是在于要达到什么目的。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1530160730,"ip_address":"","comment_id":13873,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5825019370","product_id":100008701,"comment_content":"请问空间标识和log的方法是要一起用吗？还是说。有负数就不能用log，这时候怎么办？如果统一把他加成正数，这样合理吗？会不会对分析产生一些误导呢。","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":419794,"discussion_content":"空间标识是把异常点拉成正常的，log是处理单个特征取值范围过大的，两个解决的不是一个问题。所以结合起来用原则上可以，关键还是在于要达到什么目的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1530160730,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":13794,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1529984336,"is_pvip":false,"replies":[{"id":"4881","content":"各个学科的思想其实都是相通的。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1530499412,"ip_address":"","comment_id":13794,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5824951632","product_id":100008701,"comment_content":"特征尺度不一致还是挺常见的。用的是文中提到的标准化方法。缺失值的K近邻和插值方法以前实践中只知道信号处理里有插值的函数，其他领域还没用过。","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":419757,"discussion_content":"各个学科的思想其实都是相通的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1530499412,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":13737,"user_name":"rkq@geekbang","can_delete":false,"product_type":"c1","uid":1005982,"ip_address":"","ucode":"3F4BC94AB6FCC9","user_header":"https://static001.geekbang.org/account/avatar/00/0f/59/9e/20ba145c.jpg","comment_is_top":false,"comment_ctime":1529973272,"is_pvip":false,"replies":[{"id":"4609","content":"是的，线性回归并不直接基于距离，所以缩放与否计算出的参数和误差会有区别，但对整体趋势不会有太大的影响。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1529993025,"ip_address":"","comment_id":13737,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5824940568","product_id":100008701,"comment_content":"关于特征缩放我有一个问题：如果我的模型是普通的线性回归，需要对特征做缩放处理吗？我的理解是不需要，因为最终学得的参数就会体现出特征的缩放。不知道对不对？","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":419739,"discussion_content":"是的，线性回归并不直接基于距离，所以缩放与否计算出的参数和误差会有区别，但对整体趋势不会有太大的影响。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1529993025,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":175787,"user_name":"杨家荣","can_delete":false,"product_type":"c1","uid":1259241,"ip_address":"","ucode":"3DA65396C7F002","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132","comment_is_top":false,"comment_ctime":1580827531,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1580827531","product_id":100008701,"comment_content":"极客时间<br>21天打卡行动 48&#47;21<br>&lt;&lt;机器学习40讲&#47;10&gt;&gt;实验设计<br>今日所学:<br>0,多明戈斯的观点是：数据量比算法更重要<br>1,数据决定了机器学习的上限，而算法只是尽可能逼近这个上限<br>2,特征工程（feature engineering）才是机器学习的关键;<br>3,特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码;<br>4,特征缩放的作用就是消除特征的不同尺度所造成的偏差;<br>5,标准化的方法用原始数据减去均值再除以标准差，不管原始特征的取值范围有多大，得到的每组新数据都是均值为 0，方差为 1;<br>6,面对偏度较大的数据，第一反应就应该是检查是否有异常点存在;<br>7,空间标识算法将所有的数据点都映射到高维空间的球面上，这个映射和标准化或者归一化的不同之处在于它处理的对象并不是所有样本的同一个特征，而是同一个样本的所有特征，让所有样本呈现一致的尺度。<br>8,对数据进行去偏度处理的常用方法就是取对数变换（log transformation）;<br>9,在模型训练之前移除一些特征有助于增强模型的可解释性，也可以降低计算中的开销。<br>重点:<br>特征缩放可以让不同特征的取值具有相同的尺度，方法包括标准化和归一化；<br>异常点会导致数据的有偏分布，对数变换和空间标识都可以去除数据的偏度；<br> k 近邻方法和线性回归可以用来对特征的缺失值进行人为赋值；<br>删除不具备区分度的特征能够降低计算开销，增强可解释性。","like_count":0},{"had_liked":false,"id":164255,"user_name":"Right as rain","can_delete":false,"product_type":"c1","uid":1266580,"ip_address":"","ucode":"08CE2EA47154E5","user_header":"https://static001.geekbang.org/account/avatar/00/13/53/94/0ae2a315.jpg","comment_is_top":false,"comment_ctime":1576917148,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1576917148","product_id":100008701,"comment_content":"老师，如何判断数据正负样本不平衡，1000正100负就算，还是1000正10负，有没有一些数据样本不平衡的评价标准呢？","like_count":0,"discussions":[{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":215822,"discussion_content":"正负样本数量要相当","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1585387658,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":59498,"user_name":"暴走的carry","can_delete":false,"product_type":"c1","uid":1192952,"ip_address":"","ucode":"B73712413907CD","user_header":"https://static001.geekbang.org/account/avatar/00/12/33/f8/8c2bae4b.jpg","comment_is_top":false,"comment_ctime":1547379819,"is_pvip":false,"replies":[{"id":"42405","content":"最好还是没有缺失值","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1563778472,"ip_address":"","comment_id":59498,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1547379819","product_id":100008701,"comment_content":"对于处理缺失值，以前我只知道用平均值或众数来代替，现在学会了，还能内嵌一个机器学习算法来处理缺失值，突然高端了好多","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":436346,"discussion_content":"最好还是没有缺失值","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563778472,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":58517,"user_name":"Daryl","can_delete":false,"product_type":"c1","uid":1333757,"ip_address":"","ucode":"3757AB87702FBD","user_header":"https://static001.geekbang.org/account/avatar/00/14/59/fd/128cc75b.jpg","comment_is_top":false,"comment_ctime":1547099653,"is_pvip":false,"replies":[{"id":"42428","content":"测试集也要做同样操作，但需要使用训练集的指标。在对测试集做标准化时，需要使用\\mu_train和\\sigma_train，而不是\\mu_test和\\sigma_test。至于选择哪种方式，就没有一定之规了，需要具体情况具体分析。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1563781547,"ip_address":"","comment_id":58517,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1547099653","product_id":100008701,"comment_content":"有个入门的问题，麻烦帮我解答下。<br>1:对训练集标准化&#47;归一化&#47;pca后，是否也要对测试集执行同样操作？<br>2:如果同样的操作，是直接对测试集transferm()，还是fit_transferm()？<br>3:标准化&#47;归一化&#47;pca 怎么针对数据集选择用哪种方式？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":436105,"discussion_content":"测试集也要做同样操作，但需要使用训练集的指标。在对测试集做标准化时，需要使用\\mu_train和\\sigma_train，而不是\\mu_test和\\sigma_test。至于选择哪种方式，就没有一定之规了，需要具体情况具体分析。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563781547,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":54220,"user_name":"Kevin.zhang🌏","can_delete":false,"product_type":"c1","uid":1247039,"ip_address":"","ucode":"5C52AE51538906","user_header":"https://static001.geekbang.org/account/avatar/00/13/07/3f/53ae01f6.jpg","comment_is_top":false,"comment_ctime":1545809361,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1545809361","product_id":100008701,"comment_content":"作业：<br>       前段时间在通过爬虫程序获取了原始数据，在数据清洗的阶段，发现有很多的缺失数据，还有重复数据，重复数据之前没有使用pandas，就直接用的SQL筛选，对于缺失数据，我采用的笨办法，就是直接观察是哪个特征缺失，然后进行最原始的人工赋值替换操作，说实话，工作量大还不靠谱！边做心里还边打鼓！我不知道如何采用线性回归和K近邻算法操作！","like_count":0},{"had_liked":false,"id":42912,"user_name":"黄海娜","can_delete":false,"product_type":"c1","uid":1150292,"ip_address":"","ucode":"3BD44D5386DF7E","user_header":"https://static001.geekbang.org/account/avatar/00/11/8d/54/5a286327.jpg","comment_is_top":false,"comment_ctime":1543106692,"is_pvip":false,"replies":[{"id":"18868","content":"空值是指属性没有取值？这属于不完整数据了吧。那就丢弃或者人为估计一个值赋给它。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1545310290,"ip_address":"","comment_id":42912,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1543106692","product_id":100008701,"comment_content":"老师，空值怎么用独热编码的方式呀？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430242,"discussion_content":"空值是指属性没有取值？这属于不完整数据了吧。那就丢弃或者人为估计一个值赋给它。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545310290,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":31732,"user_name":"Y024","can_delete":false,"product_type":"c1","uid":1018056,"ip_address":"","ucode":"397126D2E17997","user_header":"https://static001.geekbang.org/account/avatar/00/0f/88/c8/ae476935.jpg","comment_is_top":false,"comment_ctime":1539273378,"is_pvip":true,"replies":[{"id":"11805","content":"感谢分享","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1539570689,"ip_address":"","comment_id":31732,"utype":1}],"discussion_count":1,"race_medal":2,"score":"1539273378","product_id":100008701,"comment_content":"https:&#47;&#47;homes.cs.washington.edu&#47;~pedrod&#47;papers&#47;cacm12.pdf<br>文中所提小文链接","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":426483,"discussion_content":"感谢分享","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1539570689,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":13915,"user_name":"Geek_405126","can_delete":false,"product_type":"c1","uid":1158804,"ip_address":"","ucode":"05437ADCD95E50","user_header":"https://static001.geekbang.org/account/avatar/00/11/ae/94/6932783f.jpg","comment_is_top":false,"comment_ctime":1530063960,"is_pvip":false,"replies":[{"id":"4669","content":"当然可以，每棵树每个节点用来分裂的特征都是随机选择的。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1530161573,"ip_address":"","comment_id":13915,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1530063960","product_id":100008701,"comment_content":"在用随机森林模型的时候，我们能知道每棵树在不同layer的具体特征变量名字吗？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":419806,"discussion_content":"当然可以，每棵树每个节点用来分裂的特征都是随机选择的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1530161573,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":13883,"user_name":"我心飞扬","can_delete":false,"product_type":"c1","uid":1149164,"ip_address":"","ucode":"E41155122C9A1F","user_header":"https://static001.geekbang.org/account/avatar/00/11/88/ec/1460179b.jpg","comment_is_top":false,"comment_ctime":1530059580,"is_pvip":false,"replies":[{"id":"4668","content":"有负数时可以通过减最小值再加1把所有数据变成正数；也可以取绝对值做对数，再对负数得到的结果乘以-1。但这些都是纯数学的处理，线性操作可能会破坏数据的统计特性，所以还是选择其他的方法吧。<br>既然已经做了标准化，数据的尺度就应该基本一致了，为什么还要做对数呢？","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1530161256,"ip_address":"","comment_id":13883,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1530059580","product_id":100008701,"comment_content":"做标准化之后有负数不能log了 是不是先log","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":419798,"discussion_content":"有负数时可以通过减最小值再加1把所有数据变成正数；也可以取绝对值做对数，再对负数得到的结果乘以-1。但这些都是纯数学的处理，线性操作可能会破坏数据的统计特性，所以还是选择其他的方法吧。\n既然已经做了标准化，数据的尺度就应该基本一致了，为什么还要做对数呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1530161256,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}