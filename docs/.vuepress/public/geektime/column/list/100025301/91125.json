{"id":91125,"title":"03 | 大规模数据处理初体验：怎样实现大型电商热销榜？","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“怎样实现大型电商热销榜”。</p><p>我在Google面试过很多优秀的候选人，应对普通的编程问题coding能力很强，算法数据结构也应用得不错。</p><p>可是当我追问数据规模变大时该怎么设计系统，他们却说不出所以然来。这说明他们缺乏必备的规模增长的技术思维（mindset of scaling）。这会限制这些候选人的职业成长。</p><p>因为产品从1万用户到1亿用户，技术团队从10个人到1000个人，你的技术规模和数据规模都会完全不一样。</p><p>今天我们就以大型电商热销榜为例，来谈一谈从1万用户到1亿用户，从GB数据到PB数据系统，技术思维需要怎样的转型升级？</p><p>同样的问题举一反三，可以应用在淘宝热卖，App排行榜，抖音热门，甚至是胡润百富榜，因为实际上他们背后都应用了相似的大规模数据处理技术。</p><p><img src=\"https://static001.geekbang.org/resource/image/46/ec/469707990cf33d24d8713efab8fe34ec.png?wh=900*1600\" alt=\"\"></p><p>真正的排序系统非常复杂，仅仅是用来排序的特征（features）就需要多年的迭代设计。</p><p>为了便于这一讲的讨论，我们来构想一个简化的玩具问题，来帮助你理解。</p><p>假设你的电商网站销售10亿件商品，已经跟踪了网站的销售记录：商品id和购买时间 {product_id, timestamp}，整个交易记录是1000亿行数据，TB级。作为技术负责人，你会怎样设计一个系统，根据销售记录统计去年销量前10的商品呢？</p><!-- [[[read_end]]] --><p>举个例子，假设我们的数据是：</p><p><img src=\"https://static001.geekbang.org/resource/image/bd/9d/bdafa7f74c568c107c38317e0a1a669d.png?wh=1031*582\" alt=\"\"></p><p>我们可以把热销榜按 product_id 排名为：1, 2, 3。</p><h2>小规模的经典算法</h2><p>如果上过极客时间的《数据结构与算法之美》，你可能一眼就看出来，这个问题的解法分为两步：</p><p><img src=\"https://static001.geekbang.org/resource/image/3e/af/3eaea261df4257f0cff4509d82f211af.png?wh=1992*638?wh=1992*638\" alt=\"\"></p><p>第一步，统计每个商品的销量。你可以用哈希表（hashtable）数据结构来解决，是一个O(n)的算法，这里n是1000亿。</p><p>第二步，找出销量前十，可以用经典的Top K算法，也是O(n)的算法。</p><p>如果你考虑到了这些，先恭喜你答对了。</p><p>在小规模系统中，我们确实完全可以用经典的算法简洁漂亮地解决。以Python编程的话可能是类似这样的：</p><pre><code>def CountSales(sale_records):\n  &quot;&quot;&quot;Calculate number of sales for each product id.  \n  \n  Args:\n    sales_records: list of SaleRecord, SaleRecord is a named tuple,     \n      e.g. {product_id: “1”, timestamp: 1553721167}.\n  Returns:\n    dict of {product_id: num_of_sales}. E.g. {“1”: 1, “2”: 1}\n  &quot;&quot;&quot;\n  sales_count = {}\n  for record in sale_records:\n    sales_count[record[product_id]] += 1\n  \n  return sales_count\n\ndef TopSellingItems(sale_records, k=10):  \n  &quot;&quot;&quot;Calculate the best selling k products.  \n  \n  Args:\n    sales_records: list of SaleRecord, SaleRecord is a named tuple,   \n      e.g. {product_id: “1”, timestamp: 1553721167}.\n    K: num of top products you want to output.\n  Returns:\n    List of k product_id, sorted by num of sales.\n  &quot;&quot;&quot;\n  sales_count = CountSales(sale_records)\n  return heapq.nlargest(k, sales_count, key=sales_count.get)\n</code></pre><p>但在一切系统中，随着尺度的变大，很多方法就不再适用。</p><p>比如，在小尺度经典物理学中适用的牛顿力学公式是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/d1/dd/d1baf9cb72b099990c0f0476a79be2dd.png?wh=236*82\" alt=\"\"></p><p>这在高速强力的物理系统中就不再适用，在狭义相对论中有另外的表达。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/f7/2c59194f8bebaecd88f5942bcccf75f7.png?wh=452*98\" alt=\"\"></p><p>在社会系统中也是一样，管理10人团队，和治理14亿人口的国家，复杂度也不可同日而语。</p><p>具体在我们这个问题中，同样的Top K算法当数据规模变大会遇到哪些问题呢？</p><p>第一，内存占用。</p><p>对于TB级的交易记录数据，很难找到单台计算机容纳那么大的哈希表了。你可能想到，那我不要用哈希表去统计商品销售量了，我把销量计数放在磁盘里完成好了。</p><p>比如，就用一个1000亿行的文件或者表，然后再把销量统计结果一行一行读进后面的堆树/优先级队列。理论上听起来不错，实际上是否真的可行呢，那我们看下一点。</p><p>第二，磁盘I/O等延时问题。</p><p>当数据规模变大，我们难以避免地需要把一些中间结果存进磁盘，以应对单步任务出错等问题。一次磁盘读取大概需要10ms的时间。</p><p>如果按照上一点提到的文件替代方法，因为我们是一个O(n * log k)的算法，就需要10ms * 10^9 = 10 ^ 7 s = 115 天的时间。你可能需要贾跃亭附体，才能忽悠老板接受这样的设计方案了。</p><p>这些问题怎么解决呢？你可能已经想到，当单台机器已经无法适应我们数据或者问题的规模，我们需要横向扩展。</p><h2>大规模分布式解决方案</h2><p>之前的思路依然没错。但是，我们需要把每一步从简单的函数算法，升级为计算集群的分布式算法。</p><p><img src=\"https://static001.geekbang.org/resource/image/3e/af/3eaea261df4257f0cff4509d82f211af.png?wh=1992*638?wh=1992*638\" alt=\"\"></p><h3>统计每个商品的销量</h3><p>我们需要的第一个计算集群，就是统计商品销量的集群。</p><p>例如，1000台机器，每台机器一次可以处理1万条销售记录。对于每台机器而言，它的单次处理又回归到了我们熟悉的传统算法，数据规模大大缩小。</p><p>下图就是一个例子，图中每台机器输入是2条销售记录，输出是对于他们的本地输入而言的产品销量计数。</p><p><img src=\"https://static001.geekbang.org/resource/image/8e/8a/8eeff3376743e886d5f2d481ca8ddb8a.jpg?wh=1626*1018\" alt=\"\"></p><h3>找出销量前K</h3><p>我们需要的第二个计算集群，则是找出销量前十的集群。</p><p>这里我们不妨把问题抽象一下，抽象出是销量前K的产品。因为你的老板随时可能把产品需求改成前20销量，而不是前10了。</p><p>在上一个统计销量集群得到的数据输出，将会是我们这个处理流程的输入。所以这里需要把分布在各个机器分散的产品销量汇总出来。例如，把所有product_id = 1的销量全部叠加。</p><p>下图示例是K = 1的情况，每台机器先把所有product_id = 1的销量叠加在了一起，再找出自己机器上销量前K = 1的商品。可以看到对于每台机器而言，他们的输出就是最终排名前K = 1的商品候选者。</p><p><img src=\"https://static001.geekbang.org/resource/image/38/fc/38933e25ca315bd56321753573d5bbfc.jpg?wh=1616*1010\" alt=\"\"></p><h3>汇总最终结果</h3><p>到了最后一步，你需要把在“销量前K集群”中的结果汇总出来。也就是说，从所有排名前K=1的商品候选者中找出真正的销量前K=1的商品。</p><p>这时候完全可以用单一机器解决了。因为实际上你汇总的就是这1000台机器的结果，规模足够小。</p><p><img src=\"https://static001.geekbang.org/resource/image/ca/ab/cab28c9e3ba9031072a4e6949328bbab.jpg?wh=2127*877\" alt=\"\"></p><p>看到这里，你已经体会到处理超大规模数据的系统是很复杂的。</p><p>当你辛辛苦苦设计了应对1亿用户的数据处理系统时，可能你就要面临另一个维度的规模化（scaling）。那就是应用场景数量从1个变成1000个。每一次都为不同的应用场景单独设计分布式集群，招募新的工程师维护变得不再“可持续发展”。</p><p>这时，你需要一个数据处理的<strong>框架</strong>。</p><h2>大规模数据处理框架的功能要求</h2><p>在第二讲“MapReduce后谁主沉浮：怎样设计现代大规模数据处理技术”中，我们对于数据处理<strong>框架</strong>已经有了基本的方案。</p><p>今天这个实际的例子其实为我们的设计增加了新的挑战。</p><p>很多人面对问题，第一个想法是找有没有开源技术可以用一下。</p><p>但我经常说服别人不要先去看什么开源技术可以用，而是从自己面对的问题出发独立思考，忘掉MapReduce，忘掉Apache Spark，忘掉Apache Beam。</p><p>如果这个世界一无所有，你会设计怎样的大规模数据处理框架？你要经常做一些思维实验，试试带领一下技术的发展，而不是永远跟随别人的技术方向。</p><p>在我看来，两个最基本的需求是：</p><ol>\n<li>\n<p>高度抽象的数据处理流程描述语言。作为小白用户，我肯定再也不想一一配置分布式系统的每台机器了。作为框架使用者，我希望框架是非常简单的，能够用几行代码把业务逻辑描述清楚。</p>\n</li>\n<li>\n<p>根据描述的数据处理流程，自动化的任务分配优化。这个框架背后的引擎需要足够智能，简单地说，要把那些本来手动配置的系统，进行自动任务分配。</p>\n</li>\n</ol><p>那么理想状况是什么？对于上面的应用场景，我作为用户只想写两行代码。</p><p>第一行代码：</p><pre><code>sales_count = sale_records.Count()\n</code></pre><p>这样简单的描述，在我们框架设计层面，就要能自动构建成上文描述的“销量统计计算集群”。</p><p>第二行代码</p><pre><code>top_k_sales = sales_count.TopK(k)\n</code></pre><p>这行代码需要自动构建成上文描述的“找出销量前K集群”。</p><p>看到这里，你能发现这并不复杂。我们到这里就已经基本上把现代大规模数据处理架构的顶层构造掌握了。而背后的具体实现，我会在后面的专栏章节中为你一一揭晓。</p><h2>小结</h2><p>这一讲中，我们粗浅地分析了一个电商排行榜的数据处理例子。</p><p>从GB数据到TB数据，我们从小规模算法升级到了分布式处理的设计方案；从单一TB数据场景到1000个应用场景，我们探索了大规模数据处理框架的设计。</p><p>这些都是为了帮助你更好地理解后面所要讲的所有知识。比如，为什么传统算法不再奏效？为什么要去借助抽象的数据处理描述语言？希望在后面的学习过程中，你能一直带着这些问题出发。</p><h2>思考题</h2><p>在你的工作中，有没有随着数据规模变大，系统出问题的情况，你又是怎么解决的？</p><p>欢迎你把自己的想法写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"02 | MapReduce后谁主沉浮：怎样设计下一代数据处理技术？","id":90533},"right":{"article_title":"04 | 分布式系统（上）：学会用服务等级协议SLA来评估你的系统","id":91166}},"comments":[{"had_liked":false,"id":88293,"user_name":"青石","can_delete":false,"product_type":"c1","uid":1215531,"ip_address":"","ucode":"B0056AD6453322","user_header":"https://static001.geekbang.org/account/avatar/00/12/8c/2b/3ab96998.jpg","comment_is_top":false,"comment_ctime":1555902193,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"177649561329","product_id":100025301,"comment_content":"好多年前还未接触大数据时，写过日志采集统计各接口请求报表及puv的脚本，经历了几个阶段。<br>1. 最初是汇总所有日志到一台服务器，在处理日志，测试环境没问题，上生产跑起来就几个小时。<br>2. 后来分到Web服务器各自处理数据，时间缩短了，但是汇总数据偶尔会有问题。<br>3. 将数据写入到数据库，解决汇总数据问题。但是单表数据量过大，统计又很慢。<br>4. 按天分表解决数据量问题，最后就这么一直运行下去了。<br><br>这段经历其实很普通，但也确实让我更轻松的学习和理解大数据。当我学到mapreduce内容的时候，回忆起这段经历，让我很容易就接受了mapreduce的分治思想。<br><br>就像看到hbase的时候，我的理解它就是在实现数据的寻址、不断的拆分&#47;合并表，但是原来的人工操作变成现在自动化操作。","like_count":42,"discussions":[{"author":{"id":1393975,"avatar":"https://static001.geekbang.org/account/avatar/00/15/45/37/7125afa6.jpg","nickname":"jeff","note":"","ucode":"DBF6BC727D4E77","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":531944,"discussion_content":"ES /CLICKGHOUSE + kibana","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637478104,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88180,"user_name":"Liu C.","can_delete":false,"product_type":"c1","uid":1504607,"ip_address":"","ucode":"29F3988DD8128A","user_header":"https://static001.geekbang.org/account/avatar/00/16/f5/5f/217c6a14.jpg","comment_is_top":false,"comment_ctime":1555888389,"is_pvip":false,"replies":[{"id":"31700","content":"谢谢你的经验分享！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555889259,"ip_address":"","comment_id":88180,"utype":1}],"discussion_count":1,"race_medal":0,"score":"138994841861","product_id":100025301,"comment_content":"有一次处理一个非常高维的feature矩阵，要给它降维，但手头的电脑cpu和内存都不够好。于是我用了非常hack的手段：先使用random projection算法降低一定维度，这是一个纯矩阵乘法，可以分块放入内存计算。之后剩余的维度还是有些大，于是我把feature拆成几组，对每组分别做pca，之后再选出每组最大的主成分拼起来，就完成了降维。<br>","like_count":33,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447759,"discussion_content":"谢谢你的经验分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555889259,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89981,"user_name":"bwv825","can_delete":false,"product_type":"c1","uid":1508445,"ip_address":"","ucode":"23447AD4E864E0","user_header":"https://static001.geekbang.org/account/avatar/00/17/04/5d/259d1768.jpg","comment_is_top":false,"comment_ctime":1556356420,"is_pvip":false,"discussion_count":9,"race_medal":0,"score":"65980865860","product_id":100025301,"comment_content":"Top 1 的情况，只统计每台机器的top 1是不是可能会不准确呢？比如数据按时间段分片，某个商品销量很大很稳定，累计总数第一但很少是top 1, 因为各个时间段都有不同的爆款... ","like_count":16,"discussions":[{"author":{"id":1204919,"avatar":"https://static001.geekbang.org/account/avatar/00/12/62/b7/1fed6f02.jpg","nickname":"醇雾","note":"","ucode":"4A4C27B530FA30","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":33039,"discussion_content":"不是直接每台机器取TOP K，而是每台机器对数据 combine 后，将结果通过 hash 映射到另外的集群，这样所有相同 id 的数据就在同一台机器，combine 后再 sort 取每台机器的 TOP K。这是我的理解","likes_number":7,"is_delete":false,"is_hidden":false,"ctime":1571073748,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1504652,"avatar":"https://static001.geekbang.org/account/avatar/00/16/f5/8c/82fb5890.jpg","nickname":"抱小星","note":"","ucode":"BA7B0DAFDA4AF5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":299595,"discussion_content":"主要是不会按时间分片，不均匀，一般按ID分片，要shuffle","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597744523,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1504652,"avatar":"https://static001.geekbang.org/account/avatar/00/16/f5/8c/82fb5890.jpg","nickname":"抱小星","note":"","ucode":"BA7B0DAFDA4AF5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":299593,"discussion_content":"总销量会统计进去的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597744231,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1191143,"avatar":"https://static001.geekbang.org/account/avatar/00/12/2c/e7/3c0eba8b.jpg","nickname":"wuhulala","note":"","ucode":"6DBF2C9E19B930","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":224628,"discussion_content":"有shuffle过程的 就是一个key的保证在一台机器上","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586320649,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1876363,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/C5DBpgTvQ6ianXxhWia1QTQEJgQ1Pg1bRm1SwxxtDut0Pkzic6ImicQH6dMT3Uu7ibUbBL97zKkianNDKfheiaLHrCbyA/132","nickname":"托尼哈根哒西","note":"","ucode":"809066A0E477A6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":192687,"discussion_content":"反证法，假设某个id是最终的topk，那它至少必定是一个local 机器的top k，不然所有机器汇总之后，不可能是topk。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583077912,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1504652,"avatar":"https://static001.geekbang.org/account/avatar/00/16/f5/8c/82fb5890.jpg","nickname":"抱小星","note":"","ucode":"BA7B0DAFDA4AF5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1876363,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/C5DBpgTvQ6ianXxhWia1QTQEJgQ1Pg1bRm1SwxxtDut0Pkzic6ImicQH6dMT3Uu7ibUbBL97zKkianNDKfheiaLHrCbyA/132","nickname":"托尼哈根哒西","note":"","ucode":"809066A0E477A6","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":299594,"discussion_content":"不一定，假设A商品稳定top2，而top1经常变，最后汇总A也可能是top1。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1597744318,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":192687,"ip_address":""},"score":299594,"extra":""}]},{"author":{"id":1640346,"avatar":"","nickname":"余晖","note":"","ucode":"9673C8C65BAA16","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7243,"discussion_content":"有同样的疑问，每台机器的TOP K 做为汇总机器的输入，这个不严谨","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567427403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1204919,"avatar":"https://static001.geekbang.org/account/avatar/00/12/62/b7/1fed6f02.jpg","nickname":"醇雾","note":"","ucode":"4A4C27B530FA30","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1640346,"avatar":"","nickname":"余晖","note":"","ucode":"9673C8C65BAA16","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":33038,"discussion_content":"不是直接每台机器取TOP K，而是每台机器对数据 combine 后，将结果通过 hash 映射到另外的集群，这样所有相同 id 的数据就在同一台机器，combine 后再 sort 取每台机器的 TOP K。这是我的理解","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1571073732,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":7243,"ip_address":""},"score":33038,"extra":""}]},{"author":{"id":1323362,"avatar":"https://static001.geekbang.org/account/avatar/00/14/31/62/103e276b.jpg","nickname":"小灰灰zyh","note":"","ucode":"47C5B1C7002FCD","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5619,"discussion_content":"有同样的疑问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566385831,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88260,"user_name":"Mr Zhuo","can_delete":false,"product_type":"c1","uid":1243980,"ip_address":"","ucode":"821A7AA3825D85","user_header":"https://static001.geekbang.org/account/avatar/00/12/fb/4c/dc35efce.jpg","comment_is_top":false,"comment_ctime":1555901455,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"57390476303","product_id":100025301,"comment_content":"老师好，我目前是做NLP落地的，本来是作为补充知识学的这个专栏，但是学了这几节后发现这个方向很有潜力，也很感兴趣。另外由于你们google的BERT横空出世，感觉NLP方向的个人发展有些迷茫，所以想请问老师，对于专栏内容和NLP的结合，在未来发展有没有好的建议呢？","like_count":14},{"had_liked":false,"id":88837,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1556010735,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"53095618287","product_id":100025301,"comment_content":"数据量一大，最常见的问题除了各种exception，就是key 值分布不均衡。电商一般都是长尾的，少量的item 占据大多数购买量，很容易发送数据倾斜，需要设计更新的hash-sharding 方法","like_count":11},{"had_liked":false,"id":88405,"user_name":"Kev1n","can_delete":false,"product_type":"c1","uid":1507624,"ip_address":"","ucode":"A066DDA82F98C4","user_header":"https://static001.geekbang.org/account/avatar/00/17/01/28/56410b04.jpg","comment_is_top":false,"comment_ctime":1555916846,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"48800557102","product_id":100025301,"comment_content":"个人经验，拆分，复制，异步，并行，是大规模数据处理和应用架构的常见手段，一致性根据业务场景适当妥协","like_count":11},{"had_liked":false,"id":88469,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1555925606,"is_pvip":false,"replies":[{"id":"31739","content":"你这比喻很屌","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555929434,"ip_address":"","comment_id":88469,"utype":1}],"discussion_count":1,"race_medal":0,"score":"44505598566","product_id":100025301,"comment_content":"分解法…像剁鱼那样，一条一口吃不下就切成块，块一口吃还大，有风险，再就再用筷子分小…<br>关键问题是怎么切，切多大？怎么不全切碎，让它完整的，让人知道是条鱼😄","like_count":10,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447806,"discussion_content":"你这比喻很屌","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555929434,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88238,"user_name":"乘坐Tornado的线程魔法师","can_delete":false,"product_type":"c1","uid":1132661,"ip_address":"","ucode":"C4C9915866E769","user_header":"https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg","comment_is_top":false,"comment_ctime":1555899363,"is_pvip":false,"replies":[{"id":"31785","content":"这个图里面是按照product_id分组了，所以所有product id =1的都归第一个机器","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555988377,"ip_address":"","comment_id":88238,"utype":1}],"discussion_count":1,"race_medal":0,"score":"40210605027","product_id":100025301,"comment_content":"作者好！找出前K个集群小节里面的第一个计算集群的第二个节点（机器），是否应该像第一个节点一样计算product_id=1的所有记录。文中图示貌似只有第一个节点计算了。请作者查证。","like_count":10,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447778,"discussion_content":"这个图里面是按照product_id分组了，所以所有product id =1的都归第一个机器","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555988377,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88840,"user_name":"孙稚昊","can_delete":false,"product_type":"c1","uid":1010660,"ip_address":"","ucode":"44283BA4A577B6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg","comment_is_top":false,"comment_ctime":1556011195,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"35915749563","product_id":100025301,"comment_content":"我们在做商品订单统计的时候，会按itemid + order year + order month 对订单做hash来做group 的 key，分割成更小块，防止popular item 堆积造成的瓶颈","like_count":9,"discussions":[{"author":{"id":2050590,"avatar":"","nickname":"梧桐还是那棵梧桐","note":"","ucode":"01BE6B75FC0C7E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289682,"discussion_content":"即便按年月分割，热销的产品的订单相比于其他产品的订单还是会多很多吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594178090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":199669,"discussion_content":"有没有考虑过用item id+ order id 来做hash? order year/month应该还是有明显的不均匀分布的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583596461,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89047,"user_name":"leeon","can_delete":false,"product_type":"c1","uid":1083723,"ip_address":"","ucode":"74C23558036552","user_header":"https://static001.geekbang.org/account/avatar/00/10/89/4b/56d290f5.jpg","comment_is_top":false,"comment_ctime":1556068913,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"31620839985","product_id":100025301,"comment_content":"大规模的topk在计算过程中很容易引发数据倾斜的问题，在实际业务里，计算的优化是一方面，有时候从数据层面去优化也会有更好的效果，以榜单为例，可以在时间维度和地域为度去拆解数据，先小聚再大聚","like_count":7},{"had_liked":false,"id":88810,"user_name":"Codelife","can_delete":false,"product_type":"c1","uid":1041421,"ip_address":"","ucode":"10458683978083","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLhMtBwGqqmyhxp5uaDTvvp18iaalQj8qHv6u8rv1FQXGozfl3alPvdPHpEsTWwFPFVOoP6EeKT4bw/132","comment_is_top":false,"comment_ctime":1556006857,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27325810633","product_id":100025301,"comment_content":"最初，GPS数据以文件形式存储在盘阵中，数据增长达到TB级别后，考虑到性能和成本以及可扩展性，系统迁移到HDFS中，离线任务用MR，在线查询采用HBSE，现在，数据PB级别后，发现热点数据hbase成本太高，系统迁移到时序数据库，专供线上实时查询，同时，实时分析采用storm，批处理用spark。其实，很多情况下，采用什么技术，成本具有决定性因素","like_count":6},{"had_liked":false,"id":88368,"user_name":"zhihai.tu","can_delete":false,"product_type":"c1","uid":1045888,"ip_address":"","ucode":"61371EA3EF6988","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f5/80/baddf03b.jpg","comment_is_top":false,"comment_ctime":1555912608,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"23030749088","product_id":100025301,"comment_content":"有一个项目，试点的时候由于用户访问量小，传统负载均衡F5下连6台应用服务器访问为啥问题。后续推广后，由于访问量出现了50倍以上的增加，前台响应慢，服务器也出现内存溢出等问题。后续采用了docker容器技术，从应用服务器上抽取出并发访问较高的服务模块，单独部署服务层，支持横向扩展以及在线扩容，较好的解决了问题。","like_count":5},{"had_liked":false,"id":88203,"user_name":"涵","can_delete":false,"product_type":"c1","uid":1502742,"ip_address":"","ucode":"BB8575DB13F1E0","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg","comment_is_top":false,"comment_ctime":1555894418,"is_pvip":false,"replies":[{"id":"31718","content":"谢谢你的经验分享！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555897370,"ip_address":"","comment_id":88203,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735763602","product_id":100025301,"comment_content":"做传统数仓时，使用oracle数据库，随着数据量增大会需要使用到分区。分区需要思考使用哪个属性来分，分成多大的区间合适。另外，当视图很大时，有时查询很慢，会使用物化视图的方法。","like_count":4,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447766,"discussion_content":"谢谢你的经验分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555897370,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88192,"user_name":"JohnT3e","can_delete":false,"product_type":"c1","uid":1063982,"ip_address":"","ucode":"CF4AAAC933529C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132","comment_is_top":false,"comment_ctime":1555892515,"is_pvip":false,"replies":[{"id":"31719","content":"谢谢你的经验分享！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1555897400,"ip_address":"","comment_id":88192,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18735761699","product_id":100025301,"comment_content":"数据倾斜，导致任务运行时间超出预期，这个时候就需要对数据做一些分析和采样，优化shuffle。任务出错后，调试周期变长，这个目前没有很好的解决。不过，之前看flumejava论文，其采用了缓存不变结果来加快调试周期。另外，就是集群规模增大，后期运维的问题了","like_count":5,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447764,"discussion_content":"谢谢你的经验分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555897400,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":90502,"user_name":"Daryl","can_delete":false,"product_type":"c1","uid":1333757,"ip_address":"","ucode":"3757AB87702FBD","user_header":"https://static001.geekbang.org/account/avatar/00/14/59/fd/128cc75b.jpg","comment_is_top":false,"comment_ctime":1556539020,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14441440908","product_id":100025301,"comment_content":"作者其实关于top k没描述清楚，虽然我明白他的意思，因为我了解这边，但是对于没有了解的同学会有点晕乎","like_count":3},{"had_liked":false,"id":88793,"user_name":"乘坐Tornado的线程魔法师","can_delete":false,"product_type":"c1","uid":1132661,"ip_address":"","ucode":"C4C9915866E769","user_header":"https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg","comment_is_top":false,"comment_ctime":1556001611,"is_pvip":false,"replies":[{"id":"31843","content":"这里K远小于n，写成O(n)没有问题","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1556037820,"ip_address":"","comment_id":88793,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440903499","product_id":100025301,"comment_content":"顺便复习了王争老师的《数据结构与算法》，看到Top算法的时间复杂度准确来讲应该是是O(nLogK)","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447902,"discussion_content":"这里K远小于n，写成O(n)没有问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556037820,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88457,"user_name":"Charles.Gast","can_delete":false,"product_type":"c1","uid":1301945,"ip_address":"","ucode":"E485B55CB1E95B","user_header":"https://static001.geekbang.org/account/avatar/00/13/dd/b9/ce84d577.jpg","comment_is_top":false,"comment_ctime":1555924050,"is_pvip":false,"replies":[{"id":"31784","content":"哈哈","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555988282,"ip_address":"","comment_id":88457,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14440825938","product_id":100025301,"comment_content":"数据不数据什么的无所谓，我就想听听那个力学公式的讲解㊣","like_count":3,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447801,"discussion_content":"哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555988282,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":93554,"user_name":"哈哈","can_delete":false,"product_type":"c1","uid":1240120,"ip_address":"","ucode":"8B6D697CE6C83D","user_header":"https://static001.geekbang.org/account/avatar/00/12/ec/38/c7819759.jpg","comment_is_top":false,"comment_ctime":1557495066,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10147429658","product_id":100025301,"comment_content":"将大规模数据拆解到多台机器处理，还应该用一定的规则哈希到每台机器吧","like_count":2},{"had_liked":false,"id":89872,"user_name":"朱同学","can_delete":false,"product_type":"c1","uid":1514233,"ip_address":"","ucode":"2EF7D5A051712C","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/f9/180f347a.jpg","comment_is_top":false,"comment_ctime":1556290889,"is_pvip":false,"replies":[{"id":"32198","content":"谢谢你的经验分享！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556328027,"ip_address":"","comment_id":89872,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10146225481","product_id":100025301,"comment_content":"实际上传统服务也是这样，业务初期我们一台物理机，后面又是三台物理机，做的反向代理小集群，到现在几个机柜做了虚拟化，数据库也做了读写分离，说到底就是集群化处理","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448297,"discussion_content":"谢谢你的经验分享！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556328027,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89084,"user_name":"hufox","can_delete":false,"product_type":"c1","uid":1022874,"ip_address":"","ucode":"94675994D8EE16","user_header":"https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg","comment_is_top":false,"comment_ctime":1556073850,"is_pvip":false,"replies":[{"id":"32091","content":"谢谢支持！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1556223819,"ip_address":"","comment_id":89084,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10146008442","product_id":100025301,"comment_content":"以前做订单系统的时候，由于数据量没有那么大，没有考虑到大规模数据处理问题，但是一旦数据量上来了，统计查询都很慢，今天阅读了老师这一讲，原来可以这样设计处理大规模数据问题，涨姿势了！继续学习！","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":448010,"discussion_content":"谢谢支持！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1556223819,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312488,"user_name":"高景洋","can_delete":false,"product_type":"c1","uid":2717072,"ip_address":"","ucode":"532188513579E4","user_header":"","comment_is_top":false,"comment_ctime":1631841266,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5926808562","product_id":100025301,"comment_content":"1、数据量千万级别时，采用的是mysql分库分表的方式，数据大小GB级别，未达到TB级别<br>2、我们是做电商数据的，当库中商品sku数量，从千万级别，膨胀到亿级的时候，分库分表是一件维护成本极高的事情。因为每个表中的数据量达到一定级别时，总要人为的，去重做散列<br><br>3、因此，我们的做法是，将所有数据按5年增长量预算，都导到了hbase中<br>4、调度程序由普通的程序脚本，换成了spark集群。<br>5、我们的调度速度，由老版本的1h，提高到12min<br><br>———————-<br>如果我来做，课程里讲的销量统计，我可能会这样做：<br>1、按订单将数据散列到hbase中<br>2、spark集群将hbase中的数据拉出来<br>3、按sku维度做count统计操作<br>4、以jd为例，sku数量级大概50亿<br>5、其中 月维度有销量的sku量级 大概30%<br>6、也就是经过第三步后，统计出来的数据量级在15亿-20亿<br>7、将这些有销量的数据按t-1的时间维度，入到hive中<br>8、按老板们的需求从hive中，拉取topN的销量sku","like_count":1},{"had_liked":false,"id":102517,"user_name":"西北偏北","can_delete":false,"product_type":"c1","uid":1043160,"ip_address":"","ucode":"64BD69C84EE6A1","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132","comment_is_top":false,"comment_ctime":1560241958,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5855209254","product_id":100025301,"comment_content":"数据量大后，数据处理思维，除了要利用数据结构，算法在单台机器上做优化之外，还要思考，怎么利用集群的性能来搞定。","like_count":1},{"had_liked":false,"id":88792,"user_name":"乘坐Tornado的线程魔法师","can_delete":false,"product_type":"c1","uid":1132661,"ip_address":"","ucode":"C4C9915866E769","user_header":"https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg","comment_is_top":false,"comment_ctime":1556001283,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5850968579","product_id":100025301,"comment_content":"我的思考是：如果K=3，而product_id = 1, product_id = 2, ......product_id = 9(product_id有9种而且每种product_id可能会重复出现，个数是动态的)，那么在统计销量集群，分配计算的方法不变，但是在找出销量前K的集群，每台机器可以按照K负责3个product_id, 如第1台机器负责product_id = 1, 2, 3， 第二台负责product_id = 4, 5, 6以此类推......  这样需要3台机器，每台机器的输出正好就是每台机器本地的最大K。但是可以进一步通用化，如果product_id有11种情况，但是只有3台机器，该怎么办？我想的方案是第1台机器分配product_id = 1~3， 第2台机器分配product_id = 4~7，第3台机器分配product_id = 8~11。然后针对于每台机器的输出，第2第3台机器均需要从4种情况找出排名前3的情况。不确定这样的理解是否在正确的道路上，请指正。","like_count":1},{"had_liked":false,"id":88578,"user_name":"孤鹜齐飞","can_delete":false,"product_type":"c1","uid":1149144,"ip_address":"","ucode":"33CF3142DAD56C","user_header":"https://static001.geekbang.org/account/avatar/00/11/88/d8/fdd2cecd.jpg","comment_is_top":false,"comment_ctime":1555945902,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5850913198","product_id":100025301,"comment_content":"大道至简，大数据处理本质上还是要化繁为简、以大化小，分布式并行处理并汇总。","like_count":1},{"had_liked":false,"id":88244,"user_name":"乘坐Tornado的线程魔法师","can_delete":false,"product_type":"c1","uid":1132661,"ip_address":"","ucode":"C4C9915866E769","user_header":"https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg","comment_is_top":false,"comment_ctime":1555900166,"is_pvip":false,"replies":[{"id":"31786","content":"我觉得你想的很仔细啊。可以有一个机器专门处理所有的product_id=2。","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1555988440,"ip_address":"","comment_id":88244,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850867462","product_id":100025301,"comment_content":"如果K=3的情况下 并且product=2 product_id=3均有多条记录 请问下关于product_id=2 product=3的记录该如何处理 也是 每台机器都要做product_id=1的聚合吗","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447780,"discussion_content":"我觉得你想的很仔细啊。可以有一个机器专门处理所有的product_id=2。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555988440,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":266950,"user_name":"Phantom01","can_delete":false,"product_type":"c1","uid":1140424,"ip_address":"","ucode":"9FA50D3B0A06DB","user_header":"https://static001.geekbang.org/account/avatar/00/11/66/c8/f598a816.jpg","comment_is_top":false,"comment_ctime":1607533197,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1607533197","product_id":100025301,"comment_content":"主要是分治。考虑怎么拆分合并，有点像外排序。然后想办法提高单次操作的效率，减少落盘或者通信次数。<br><br>也可以先分段topK然后合并","like_count":0},{"had_liked":false,"id":259350,"user_name":"技术修行者","can_delete":false,"product_type":"c1","uid":1013147,"ip_address":"","ucode":"28CA41A1214D6B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/75/9b/611e74ab.jpg","comment_is_top":false,"comment_ctime":1604711346,"is_pvip":true,"discussion_count":0,"race_medal":1,"score":"1604711346","product_id":100025301,"comment_content":"请教一下，大规模数据处理系统的设计对于传统的数据ETL系统设计有什么参考和借鉴价值吗？<br>我们目前的系统主要针对批量数据，在使用Kettle+Carte来处理，但是对于异常处理和系统监控方面做的不是很好。","like_count":0},{"had_liked":false,"id":256264,"user_name":"Geek_639183","can_delete":false,"product_type":"c1","uid":2265206,"ip_address":"","ucode":"A4356886697745","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/IVIhn9AycTlGFSPvA4FB4b8GicYgFoqBkBRnRROOAjR6IUDcYxvJzDKGbBOh7YibqaAYLICX450rcWHZhq6ic5jdg/132","comment_is_top":false,"comment_ctime":1603589168,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1603589168","product_id":100025301,"comment_content":"第一步，统计每个商品的销量。你可以用哈希表（hashtable）数据结构来解决，是一个 O(n) 的算法，这里 n 是 1000 亿。<br><br>老师 请问为什么此处用hashtable 而不用hashmap呢？","like_count":0},{"had_liked":false,"id":243884,"user_name":"布凡","can_delete":false,"product_type":"c1","uid":1202465,"ip_address":"","ucode":"346FCD332F8BFA","user_header":"https://static001.geekbang.org/account/avatar/00/12/59/21/d2efde18.jpg","comment_is_top":false,"comment_ctime":1598319287,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1598319287","product_id":100025301,"comment_content":"老师，&quot;每台机器先把所有 product_id = 1 的销量叠加在了一起，再找出自己机器上销量前 K = 1 的商品。可以看到对于每台机器而言，他们的输出就是最终排名前 K = 1 的商品候选者。&quot;这里有个疑问。<br>我理解的是因为每台机器上的product_id的值差不多都是均匀分布的，如果在每台机器上销量排前K 那么整体上也会排前K。那会不会存product_id对应的count在机器1上排前K, 机器2上排前K 以后的情况呢？那这样机器2上排前K以后的数是不是就不会被汇总了呢？","like_count":0},{"had_liked":false,"id":186073,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1583750659,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583750659","product_id":100025301,"comment_content":"大数据下，一般都会构建物化视图，一般的企业不会有这么多机器的计算集群，所以我们这边常用方法就是构建视图了，当然这一块维护起来是比较麻烦的","like_count":0},{"had_liked":false,"id":162482,"user_name":"何妨","can_delete":false,"product_type":"c1","uid":1385377,"ip_address":"","ucode":"EC3983BFF7992A","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/a1/b08f3ee7.jpg","comment_is_top":false,"comment_ctime":1576544798,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1576544798","product_id":100025301,"comment_content":"虽然理解了思想，但是具体的实现细节还是很模糊，比如分布式集群计算之后是如何汇总的呢？还有 beam 现在已经是目前主流的处理方案了么？因为网上搜教程好像蛮少的，更多的还是 mp,hadoop,spark。小白不是很懂望科普~","like_count":0},{"had_liked":false,"id":120094,"user_name":"露娜","can_delete":false,"product_type":"c1","uid":1289012,"ip_address":"","ucode":"CC3EF71EB30EE1","user_header":"https://static001.geekbang.org/account/avatar/00/13/ab/34/ef002163.jpg","comment_is_top":false,"comment_ctime":1564789994,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1564789994","product_id":100025301,"comment_content":"遇到个模型输出的项目，用Node.JS实现训练好的数据模型。这个模型要计算200多个变量。单机计算时间5秒，太久了，影响用户体验。后来拆成5个微服务，把200个不同变量的计算逻辑分配到5台机器上。同时计算，最后在汇总后统一返回给前端。5秒计算时间变1秒了。","like_count":0},{"had_liked":false,"id":114141,"user_name":"liliumss","can_delete":false,"product_type":"c1","uid":1333094,"ip_address":"","ucode":"A52AC5042F3115","user_header":"https://static001.geekbang.org/account/avatar/00/14/57/66/e57bdb18.jpg","comment_is_top":false,"comment_ctime":1563242089,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1563242089","product_id":100025301,"comment_content":"感受：抽象层次很重要，对于大数据框架来说 应用只想关心高层的调用，细节封装再大数据框架里","like_count":0},{"had_liked":false,"id":104836,"user_name":"一叶知秋","can_delete":false,"product_type":"c1","uid":1171698,"ip_address":"","ucode":"7193850DF34312","user_header":"https://static001.geekbang.org/account/avatar/00/11/e0/f2/e73b3269.jpg","comment_is_top":false,"comment_ctime":1560850834,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1560850834","product_id":100025301,"comment_content":"选TopK的算法是不是不对，分科成绩每科都是第二，总成绩第一的情况也是存在的。","like_count":0},{"had_liked":false,"id":94711,"user_name":"吴晓亮","can_delete":false,"product_type":"c1","uid":1529305,"ip_address":"","ucode":"BA8D7D31137201","user_header":"https://static001.geekbang.org/account/avatar/00/17/55/d9/326dd1fd.jpg","comment_is_top":false,"comment_ctime":1557884349,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1557884349","product_id":100025301,"comment_content":"darren<br>每台机器计算topK,汇总到master基于每台机器的topk数据，再做top,是不准确的，相当于基于top的数据，来计算top，是不准确的。<br><br>2019-05-08<br>作者回复: 没有明白你的意思。这里的算法是全局的topk肯定在局部topk中<br><br>其实问题的关键是:是不是每台机器处理的product id是不重复的,如果第一台机器处理product 1和2,第二台也是有1和2,那就是不准的了,比方第一台机器发现product1 = 1000,product2=500,第二台机器发现product1=0 ,product2=900 ,那么按你的算法就是product1胜出了,但是实际上product2=900+500=1400才是销量最大的","like_count":0,"discussions":[{"author":{"id":1014138,"avatar":"","nickname":"LGC247CG","note":"","ucode":"2193C266017057","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":4411,"discussion_content":"根据作者老师描述，我的理解是，topk之前product1和2的记录数已经是汇总后的结果，也就是只有一个product1和一个product2，你所描述的是没有汇总的时候","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1565382251,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":92413,"user_name":"darren","can_delete":false,"product_type":"c1","uid":1027735,"ip_address":"","ucode":"386736C90F32CA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/97/ba512167.jpg","comment_is_top":false,"comment_ctime":1557268686,"is_pvip":false,"replies":[{"id":"33080","content":"没有明白你的意思。这里的算法是全局的topk肯定在局部topk中","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1557289685,"ip_address":"","comment_id":92413,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557268686","product_id":100025301,"comment_content":"每台机器计算topK,汇总到master基于每台机器的topk数据，再做top,是不准确的，相当于基于top的数据，来计算top，是不准确的。","like_count":0,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":449248,"discussion_content":"没有明白你的意思。这里的算法是全局的topk肯定在局部topk中","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557289685,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":92412,"user_name":"darren","can_delete":false,"product_type":"c1","uid":1027735,"ip_address":"","ucode":"386736C90F32CA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ae/97/ba512167.jpg","comment_is_top":false,"comment_ctime":1557268009,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1557268009","product_id":100025301,"comment_content":"有没有随着数据规模变大，系统出问题的情况，你又是怎么解决的<br>日志审计分析系统，包含数据接收、归一化、统计分析、存储和索引模块，当单台规模不足以支撑时，通过不同日志源将数据切分到不同的机器（只包含数据接收和归一化处理），将结果发送到中心（包含数据接收、统计分析、存储和索引模块），将计算成本比较大的归一化的模块，分散到不同的机器；比较麻烦的要处理资产同步、配置同步、分布式id冲突问题等。","like_count":0,"discussions":[{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":199744,"discussion_content":"分布式id冲突是什么问题？id分配按range或者hash会冲突吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583602015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":89698,"user_name":"Geek_1c9f7c","can_delete":false,"product_type":"c1","uid":1110419,"ip_address":"","ucode":"9A5611F231C474","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKic4Sia2vW3FdODLrSLzGEXqq2s6wRywMXSWHNdPY9Ge1ecW57pQ29dXRMSSl6aYLpv2uXH2U3Nliaw/132","comment_is_top":false,"comment_ctime":1556246001,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556246001","product_id":100025301,"comment_content":"大规模搜索 精确的KNN，往往难以实现，通过预处理，直接ANN求解。<br>并增加多队列的设计辅助KNN","like_count":0},{"had_liked":false,"id":89469,"user_name":"青见","can_delete":false,"product_type":"c1","uid":1031271,"ip_address":"","ucode":"F6E0D3D6D1C5BE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bc/67/5b97ba24.jpg","comment_is_top":false,"comment_ctime":1556180646,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556180646","product_id":100025301,"comment_content":"数据规模变大的时候，数据库进行了分库分表；对于一下使用平凡的数据进行REDIS 存储+内存处理，并且进行服务器的横向扩展；","like_count":0},{"had_liked":false,"id":89060,"user_name":"hd900","can_delete":false,"product_type":"c1","uid":1057579,"ip_address":"","ucode":"8CCC03F3BA080A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/IjFRWDxE5DUQZ4YJHVjN862xsDjhLObf2kKibFha2vcpNoHdzoqoBsvGibdPSNGZCJI8akhVPecf8vS9xgee2Dng/132","comment_is_top":false,"comment_ctime":1556070239,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556070239","product_id":100025301,"comment_content":"曾经有过一个学生选课的系统，一个学校的选课学生数量大概2·3k人，课程数量不定，每次选课产生的数据学生数量*课程数量的数据，然后在最终统计的时候总是使用传统的sql方法去统计每个学生的选课情况（比如必修多少，选修多少），但是因为数量很多，需要花费很多时间才完成统计，请问老师有没有好的方法，能够使得快速的统计每个学生的选课情况（不是具体到选了什么课程的内容，只统计学生选了多少课，哪些学生没有选够课程），谢谢","like_count":0},{"had_liked":false,"id":88908,"user_name":"拒绝","can_delete":false,"product_type":"c1","uid":1335155,"ip_address":"","ucode":"CB0264C4D3FE17","user_header":"https://static001.geekbang.org/account/avatar/00/14/5f/73/bb3dc468.jpg","comment_is_top":false,"comment_ctime":1556025246,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556025246","product_id":100025301,"comment_content":"老师您好:<br>      对于没有经验的我来说，听起来感觉还是比较吃力的，虽然老师已尽可能的描述的很好了，但我还是没有那种恍然大悟的感觉，希望老师对我这样的学生指点一下。","like_count":0},{"had_liked":false,"id":88784,"user_name":"沈瑞","can_delete":false,"product_type":"c1","uid":1503694,"ip_address":"","ucode":"0AED2CFDA8CE28","user_header":"https://static001.geekbang.org/account/avatar/00/16/f1/ce/dab00082.jpg","comment_is_top":false,"comment_ctime":1556000175,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1556000175","product_id":100025301,"comment_content":"我们公司有个project，曾今在storm 上做了层的sql抽象，可以抽象的表达","like_count":0},{"had_liked":false,"id":88770,"user_name":"Vito","can_delete":false,"product_type":"c1","uid":1058567,"ip_address":"","ucode":"9AB7BC1AB3EF7A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersxYcRm1mxLy6RlE5p1yibk5qFYwPS0E5XnI3XrS42rQW8pWJIDeTlshEGjSvl4uOVtEzU6IuOwOA/132","comment_is_top":false,"comment_ctime":1555996589,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1555996589","product_id":100025301,"comment_content":"当时用C++处理日志，最开始也是单机单线程处理，后来算的越来越慢，然后采用单进程多线程（按照用户ID，取模分到不同的线程）处理，发挥本机CPU性能，再后来改为多台机器多线程处理，当然，我没有去实现什么分布式文件系统，而是把文件分发到每台机器，后来接触到了MR，感觉自己当时的想法和MR思路很雷同，哈哈。","like_count":0},{"had_liked":false,"id":88629,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1555976709,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555976709","product_id":100025301,"comment_content":"总结：由于数据规模很大一台机器无法处理，或者处理的时间无法接受，那就把数据切分成多块，每个机器处理一部分问题，最终在一台机器上汇总。<br>想设计一套类似系统要处理好如何切分，如何控制各个机器运行计算，如何对运行结果做监控，如何做汇总，出错如何处理？<br>当然还要设计自己的抽象语言，通过抽象语言先描绘出最终应该是什么样子，造成头脑中的创作，再来完成现实的创作。<br><br>回答：我们遇到大的问题就直接上大数据开源软件了，不过在使用过程中有的软件比如solr遇到大规模数据也有问题，以前的版本索引元数据是存在zookeeper的一个节点上，当索引多了，分区多了这个节点越来越大，超过zookeeper默认的1M，后面solr把每个collection元数据拆分到不同节点下，这也算文中类似的分治思想","like_count":0},{"had_liked":false,"id":88616,"user_name":"Bing","can_delete":false,"product_type":"c1","uid":1095367,"ip_address":"","ucode":"B0309C89022EC8","user_header":"https://static001.geekbang.org/account/avatar/00/10/b6/c7/e7c04558.jpg","comment_is_top":false,"comment_ctime":1555972899,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555972899","product_id":100025301,"comment_content":"大数据处理思维方式就是一个事情分给多人干，事情也有先后顺序，怎么协调管理好，最终快速做完，关建还是业务熟悉，数据模型了解，这样可以均匀拆分数据并行处理，通过实际案例讲非常给力","like_count":0},{"had_liked":false,"id":88573,"user_name":"宇宙","can_delete":false,"product_type":"c1","uid":1048260,"ip_address":"","ucode":"0D7B50EED8024B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/fe/c4/aeb13012.jpg","comment_is_top":false,"comment_ctime":1555944505,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1555944505","product_id":100025301,"comment_content":"从理论上来说top K 不能每个节点都找K个吧 可能最后几个会不准确?","like_count":0},{"had_liked":false,"id":88511,"user_name":"来碗绿豆汤","can_delete":false,"product_type":"c1","uid":1070051,"ip_address":"","ucode":"B0AB63B8D9729F","user_header":"https://static001.geekbang.org/account/avatar/00/10/53/e3/39dcfb11.jpg","comment_is_top":false,"comment_ctime":1555933401,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555933401","product_id":100025301,"comment_content":"感觉大数据处理本质就是， 把大数据分割成小数据，然后小数据用传统的数据结构和算法知识解决。所以大数据框架的核心就是怎么来管理和协调集群中的各个机器。","like_count":0},{"had_liked":false,"id":88356,"user_name":":)","can_delete":false,"product_type":"c1","uid":1239198,"ip_address":"","ucode":"23D505949442B6","user_header":"https://static001.geekbang.org/account/avatar/00/12/e8/9e/6550a051.jpg","comment_is_top":false,"comment_ctime":1555910006,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555910006","product_id":100025301,"comment_content":"期待更新！","like_count":0},{"had_liked":false,"id":88354,"user_name":"陈","can_delete":false,"product_type":"c1","uid":1006448,"ip_address":"","ucode":"A8E6AFF6E5775D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5b/70/6411282d.jpg","comment_is_top":false,"comment_ctime":1555909665,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555909665","product_id":100025301,"comment_content":"启发了我一个对大数据框架的优化思路，就是在框架一般做好集群方面大规模数据处理，对单机分片上的数据可以通过传统的单机算法做优化，提高单机处理数据的性能，进而提升整个集群处理数据的性能。","like_count":0},{"had_liked":false,"id":88246,"user_name":"yiwu","can_delete":false,"product_type":"c1","uid":1307909,"ip_address":"","ucode":"712056FCDFE504","user_header":"https://static001.geekbang.org/account/avatar/00/13/f5/05/09aaa06c.jpg","comment_is_top":false,"comment_ctime":1555900681,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1555900681","product_id":100025301,"comment_content":"当年做的第一套大数据应用，冠字号系统就是采用hbase解决数据大表的简单crud操作。这个东西如果放在传统数据库+单一存储，绝对是大投入。","like_count":0}]}