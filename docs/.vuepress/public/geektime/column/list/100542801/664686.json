{"id":664686,"title":"22｜YouTubeDNN：召回算法的后起之秀（下）","content":"<p>你好，我是黄鸿波。</p><p>上节课我们讲了关于YouTubeDNN的召回模型，接下来，我们来看看如何用代码来实现它。</p><p>我们在做YouTubeDNN的时候，要把代码分成两个步骤，第一个步骤是对数据的清洗和处理，第二个步骤是搭建模型然后把数据放进去进行训练和预测。</p><h2>数据的清洗和处理</h2><p>先来讲数据部分。</p><p>按照YouTubeDNN论文来看，输入的数据是用户的信息、视频的ID序列、用户搜索的特征和一些地理信息等其他信息。到了基于文章内容的信息流产品中，就变成了用户ID、年龄、性别、城市、阅读的时间戳再加上视频的ID。我们把这些内容可以组合成YouTubeDNN需要的内容，最后处理成需要的Embedding。</p><p>由于前面没有太多的用户浏览数据，所以我先造了一批数据，数据集我会放到GitHub上（后续更新），数据的形式如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/ea/0a/eaeef45b0eb7e64c3f11c4a252f8120a.png?wh=1379x1424\" alt=\"图片\"></p><p>接下来我们就把这批数据处理成YouTubeDNN需要的形式。首先在recommendation-class项目中的utils目录下建立一个preprocess.py文件，作为处理数据的文件。</p><p>我们要处理这一批数据，需要下面五个步骤。</p><ol>\n<li>加载数据集。</li>\n<li>处理数据特征。</li>\n<li>特征转化为模型输入。</li>\n<li>模型的搭建和训练。</li>\n<li>模型评估。</li>\n</ol><!-- [[[read_end]]] --><p>在正式写代码之前，需要安装几个库，如下。</p><pre><code class=\"language-plain\">deepctr\ndeepmatch\ntensorflow==2.2\npandas\n</code></pre><p>我们可以使用pip install加上库名来安装它们，也可以把它们放在一个叫requirements.txt的文件中，使用pip install -r进行安装。</p><p>安装完成之后，我们来写preprocess.py的代码。为了能够让你看得更明白，我在函数里加了一些注释，先上代码。</p><pre><code class=\"language-plain\">from tqdm import tqdm\nimport numpy as np\nimport random\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n&nbsp;\ndef gen_data_set(data, negsample=0):\n&nbsp;&nbsp;&nbsp;&nbsp;data.sort_values(\"timestamp\", inplace=True) &nbsp;#是否用排序后的数据集替换原来的数据，这里是替换\n&nbsp;&nbsp;&nbsp;&nbsp;item_ids = data['item_id'].unique() &nbsp;&nbsp;&nbsp;#item需要进行去重\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;train_set = list()\n&nbsp;&nbsp;&nbsp;&nbsp;test_set = list()\n&nbsp;&nbsp;&nbsp;&nbsp;for reviewrID, hist in tqdm(data.groupby('user_id')): &nbsp;&nbsp;#评价过, &nbsp;历史记录\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pos_list = hist['item_id'].tolist()\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rating_list = hist['rating'].tolist()\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if negsample &gt; 0: &nbsp;&nbsp;&nbsp;#负样本\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;candidate_set = list(set(item_ids) - set(pos_list)) &nbsp;&nbsp;#去掉用户看过的item项目\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True) &nbsp;#随机选择负采样样本\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for i in range(1, len(pos_list)):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if i != len(pos_list) - 1:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_set.append((reviewrID, hist[::-1], pos_list[i], 1, len(hist[:: -1]), rating_list[i])) &nbsp;#训练集和测试集划分 &nbsp;[::-1]从后玩前数\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for negi in range(negsample):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_set.append((reviewrID, hist[::-1], neg_list[i * negsample + negi], 0, len(hist[::-1])))\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_set.append((reviewrID, hist[::-1], pos_list[i], 1, len(hist[::-1]), rating_list[i]))\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;random.shuffle(train_set) &nbsp;&nbsp;&nbsp;&nbsp;#打乱数据集\n&nbsp;&nbsp;&nbsp;&nbsp;random.shuffle(test_set)\n&nbsp;&nbsp;&nbsp;&nbsp;return train_set, test_set\n&nbsp;\ndef gen_model_input(train_set, user_profile, seq_max_len):\n&nbsp;&nbsp;&nbsp;&nbsp;train_uid = np.array([line[0] for line in train_set])\n&nbsp;&nbsp;&nbsp;&nbsp;train_seq = [line[1] for line in train_set]\n&nbsp;&nbsp;&nbsp;&nbsp;train_iid = np.array([line[2] for line in train_set])\n&nbsp;&nbsp;&nbsp;&nbsp;train_label = np.array([line[3] for line in train_set])\n&nbsp;&nbsp;&nbsp;&nbsp;train_hist_len = np.array([line[4] for line in train_set])\n&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;\"\"\"\n&nbsp;&nbsp;&nbsp;&nbsp;pad_sequences数据预处理\n&nbsp;&nbsp;&nbsp;&nbsp;sequences：浮点数或整数构成的两层嵌套列表\n&nbsp;&nbsp;&nbsp;&nbsp;maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n&nbsp;&nbsp;&nbsp;&nbsp;dtype：返回的numpy array的数据类型\n&nbsp;&nbsp;&nbsp;&nbsp;padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补`\n&nbsp;&nbsp;&nbsp;&nbsp;truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n&nbsp;&nbsp;&nbsp;&nbsp;value：浮点数，此值将在填充时代替默认的填充值0\n&nbsp;&nbsp;&nbsp;&nbsp;\"\"\"\n&nbsp;&nbsp;&nbsp;&nbsp;train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n&nbsp;&nbsp;&nbsp;&nbsp;train_model_input = {\"user_id\": train_uid, \"item_id\": train_iid, \"hist_item_id\": train_seq_pad,\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"hist_len\": train_hist_len}\n&nbsp;&nbsp;&nbsp;&nbsp;for key in {\"gender\", \"age\", \"city\"}:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values &nbsp;&nbsp;#训练模型的关键字\n&nbsp;\n\treturn train_model_input, train_label\n</code></pre><p>这段代码主要用于生成训练集和测试集以及模型的输入。它看起来有点长，我来分别解释一下。</p><p>gen_data_set()函数接受一个数据集（data）和一个负采样（negsample）参数，返回一个训练集列表和一个测试集列表。该函数首先将数据集根据时间戳排序，然后从每一个用户的历史记录中选取正样本和负样本，并将它们保存到训练集和测试集中。</p><p>gen_model_input()函数接受一个训练集列表、用户画像信息和序列最大长度参数，返回训练模型的输入和标签。该函数将训练集列表拆分成train_uid、train_seq、train_iid、train_label和train_hist_len五部分。</p><ul>\n<li>train_uid和train_iid为用户ID和物品ID。</li>\n<li>train_seq为历史交互序列。</li>\n<li>train_label为正负样本标签。</li>\n<li>train_hist_len为历史交互序列的长度。</li>\n</ul><p>此外，它对历史交互序列进行了填充处理（pad_sequences），并且将用户画像信息加入到训练模型的关键字中。最终，该函数返回训练模型的输入和标签。</p><p>在gen_data_set()函数中，首先使用data.sort_values(“timestamp”, inplace=True)函数将数据集按照时间戳排序，这是为了保证数据按照时间顺序排列，便于后续处理。接下来使用data[‘item_id’].unique()函数获取数据集中所有不重复的物品ID。因为后续需要筛选出用户未曾购买过的物品，要先获取数据集中所有的物品ID以便后续处理。</p><p>接下来使用groupby()函数将用户ID（user_id）相同的数据分组。对于每一组数据，将其分成正样本和负样本。其中正样本为用户已经购买过的物品，负样本为用户未购买过的其他物品。如果negsample参数大于0，则需要进行负采样。随机选取一些未曾购买过的物品作为负样本，并将它们保存到训练集列表中。最后，将正负样本数据以及其他信息（如历史交互序列、用户ID和历史交互序列的长度）保存到训练集列表和测试集列表中。</p><p>在gen_model_input()函数中，首先将训练集列表拆分成5个列表，分别保存用户ID、物品ID、历史交互序列、正负样本标签和历史交互序列长度。然后使用pad_sequences()函数对历史交互序列进行填充处理，将其变成长度相同的序列。最后，将用户画像信息加入到训练模型的关键字中，返回训练模型的输入和标签。</p><h2>搭建模型进行训练和预测</h2><p>当数据处理完成后，接下来就可以来做YouTubeDNN的模型部分了，我们在recall目录下新建一个文件，名字叫YouTubeDNN，然后编写如下代码。</p><pre><code class=\"language-plain\">import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom models.recall.preprocess import gen_data_set, gen_model_input\nfrom deepctr.feature_column import SparseFeat, VarLenSparseFeat\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.models import Model\nimport tensorflow as tf\nfrom deepmatch.models import *\nfrom deepmatch.utils import recall_N\nfrom deepmatch.utils import sampledsoftmaxloss\nimport numpy as np\nfrom tqdm import tqdm\n\t&nbsp;\n\t&nbsp;\n\tclass YoutubeModel(object):\n\t&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, embedding_dim=32):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.SEQ_LEN = 50\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.embedding_dim = embedding_dim\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.user_feature_columns = None\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.item_feature_columns = None\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;def training_set_construct(self):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 加载数据\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data = pd.read_csv('../../data/read_history.csv')\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 负采样个数\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;negsample = 0\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 特征编码\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features = [\"user_id\", \"item_id\", \"gender\", \"age\", \"city\"]\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features_max_idx = {}\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for feature in features:\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lbe = LabelEncoder()\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data[feature] = lbe.fit_transform(data[feature]) + 1\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;features_max_idx[feature] = data[feature].max() + 1\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 抽取用户、物品特征\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_info = data[[\"user_id\", \"gender\", \"age\", \"city\"]].drop_duplicates('user_id') &nbsp;# 去重操作\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item_info = data[[\"item_id\"]].drop_duplicates('item_id')\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_info.set_index(\"user_id\", inplace=True)\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 构建输入数据\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_set, test_set = gen_data_set(data, negsample)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 转化为模型的输入\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_model_input, train_label = gen_model_input(train_set, user_info, self.SEQ_LEN)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_model_input, test_label = gen_model_input(test_set, user_info, self.SEQ_LEN)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 用户端特征输入\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.user_feature_columns = [SparseFeat('user_id', features_max_idx['user_id'], 16),\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SparseFeat('gender', features_max_idx['gender'], 16),\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SparseFeat('age', features_max_idx['age'], 16),\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SparseFeat('city', features_max_idx['city'], 16),\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VarLenSparseFeat(SparseFeat('hist_item_id', features_max_idx['item_id'],\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.embedding_dim, embedding_name='item_id'),\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.SEQ_LEN, 'mean', 'hist_len')\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 物品端的特征输入\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.item_feature_columns = [SparseFeat('item_id', features_max_idx['item_id'], self.embedding_dim)]\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return train_model_input, train_label, test_model_input, test_label, train_set, test_set, user_info, item_info\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;def training_model(self, train_model_input, train_label):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K.set_learning_phase(True)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if tf.__version__ &gt;= '2.0.0':\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tf.compat.v1.disable_eager_execution()\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 定义模型\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model = YouTubeDNN(self.user_feature_columns, self.item_feature_columns, num_sampled=100,\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_dnn_hidden_units=(128, 64, self.embedding_dim))\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 保存训练过程中的数据\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.fit(train_model_input, train_label, batch_size=512, epochs=20, verbose=1, validation_split=0.0,)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return model\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;def extract_embedding_layer(self, model, test_model_input, item_info):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;all_item_model_input = {\"item_id\": item_info['item_id'].values, }\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 获取用户、item的embedding_layer\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_embs = user_embedding_model.predict(test_model_input, batch_size=2 ** 12)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(user_embs.shape)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(item_embs.shape)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return user_embs, item_embs\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;def eval(self, user_embs, item_embs, test_model_input, item_info, test_set):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_true_label = {line[0]: line[2] for line in test_set}\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index = faiss.IndexFlagIP(self.embedding_dim)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index.add(item_embs)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D, I = index.search(np.ascontiguousarray(user_embs), 50)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s = []\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hit = 0\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 统计预测结果\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for i, uid in tqdm(enumerate(test_model_input['user_id'])):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pred = [item_info['item_id'].value[x] for x in I[i]]\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;recall_score = recall_N(test_true_label[uid], pred, N=50)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s.append(recall_score)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if test_true_label[uid] in pred:\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hit += 1\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except:\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(i)\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 计算召回率和命中率\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;recall = np.mean(s)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hit_rate = hit / len(test_model_input['user_id'])\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return recall, hit_rate\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;def scheduler(self):\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 构建训练集、测试集\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_model_input, train_label, test_model_input, test_label, \\\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_set, test_set, user_info, item_info = self.training_set_construct()\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.training_model(train_model_input, train_label)\n\t&nbsp;\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# 获取用户、item的layer\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# user_embs, item_embs = self.extract_embedding_layer(model, test_model_input, item_info)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# # 评估模型\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# recall, hit_rate = self.eval(user_embs, item_embs, test_model_input, item_info, test_set)\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# print(recall, hit_rate)\n\t&nbsp;\n\t&nbsp;\n\tif __name__ == '__main__':\n\t&nbsp;&nbsp;&nbsp;&nbsp;model = YoutubeModel()\n\t&nbsp;&nbsp;&nbsp;&nbsp;model.scheduler()\n</code></pre><p>我来详细地解释下这段代码。首先根据导入的模块，可以看出这段代码主要使用了下面表格里的几个工具和库。</p><p><img src=\"https://static001.geekbang.org/resource/image/86/22/868fa24203ced6e3e74208bf0c178c22.jpg?wh=2628x1934\" alt=\"\"></p><p>首先我们使用下面的代码加载数据。</p><pre><code class=\"language-plain\">\tdata = pd.read_csv('../../data/read_history.csv')\n</code></pre><p>这行代码使用Pandas库来读取CSV格式的历史阅读记录数据文件，将其存储到data这个DataFrame对象中。</p><p>然后我们对数据进行特征编码。</p><pre><code class=\"language-plain\">features = [\"user_id\", \"item_id\", \"gender\", \"age\", \"city\"]\nfeatures_max_idx = {}\nfor feature in features:\n    lbe = LabelEncoder()\n    data[feature] = lbe.fit_transform(data[feature]) + 1\n    features_max_idx[feature] = data[feature].max() + 1\n</code></pre><p>这段代码使用sklearn.preprocessing.LabelEncoder对原始数据的几个特征进行编码，将连续或离散的特征转化为整数类型。这里编码的特征包括user_id、item_id、gender、age、city。将特征编码后，将最大索引值保存到features_max_idx字典中。</p><p>接下来，我们使用下面的代码来构建了数据集。</p><pre><code class=\"language-plain\">train_set, test_set = gen_data_set(data, negsample)\n</code></pre><p>这行代码使用gen_data_set函数将原始数据划分为训练集和测试集，同时进行负采样操作。该函数的输入参数为原始数据和负采样个数。输出结果为经过负采样后的训练集和测试集。</p><p>然后我们就可以调用之前的gen_model_input函数将训练集和测试集转化为模型的输入格式，包括训练集/测试集的用户ID、历史物品ID序列、历史物品ID序列的长度和待预测物品ID。这些数据会作为训练模型的输入。</p><pre><code class=\"language-plain\">train_model_input, train_label = gen_model_input(train_set, user_info, self.SEQ_LEN) \ntest_model_input, test_label = gen_model_input(test_set, user_info, self.SEQ_LEN)\n</code></pre><p>接着，我们使用deepctr库中的SparseFeat和VarLenSparseFeat函数，分别构建了用户和物品的特征输入。其中SparseFeat表示离散特征，VarLenSparseFeat表示变长特征。具体地，用户特征输入由4个离散特征和一个变长特征（历史物品ID序列）组成，物品特征输入只有一个离散特征（物品ID）。</p><pre><code class=\"language-plain\"># 用户端特征输入\nself.user_feature_columns = [SparseFeat('user_id', features_max_idx['user_id'], 16),\n                             SparseFeat('gender', features_max_idx['gender'], 16),\n                             SparseFeat('age', features_max_idx['age'], 16),\n                             SparseFeat('city', features_max_idx['city'], 16),\n                             VarLenSparseFeat(SparseFeat('hist_item_id', features_max_idx['item_id'],\n                                                         self.embedding_dim, embedding_name='item_id'),\n                                              self.SEQ_LEN, 'mean', 'hist_len')\n                             ]\n# 物品端的特征输入\nself.item_feature_columns = [SparseFeat('item_id', features_max_idx['item_id'], self.embedding_dim)]\n</code></pre><p>然后我们使用deepmatch库构建了含有DNN的YouTube推荐模型。该模型的输入由上一步定义的用户和物品特征输入组成，其中num_sampled表示分类器使用的采样点的数目。在模型构建和编译后，使用fit函数进行训练。</p><pre><code class=\"language-plain\"># 定义模型\nmodel = YouTubeDNN(self.user_feature_columns, self.item_feature_columns, num_sampled=100,\n                   user_dnn_hidden_units=(128, 64, self.embedding_dim))\nmodel.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n# 保存训练过程中的数据\nmodel.fit(train_model_input, train_label, batch_size=512, epochs=20, verbose=1, validation_split=0.0,)\n</code></pre><p>最后，利用训练好的模型提取用户和物品的Embedding Layer，以便后续计算召回率和命中率。具体地，使用Model函数将模型的输入和它的用户/物品Embedding层关联起来，然后调用predict函数计算得到预测结果。</p><pre><code class=\"language-plain\">user_embs = user_embedding_model.predict(test_model_input, batch_size=2 ** 12) \nitem_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n</code></pre><p>实际上，到这里整个数据处理和训练部分的代码就已经结束了，接下来，就是要做召回率和命中率的计算。在这个部分，我们利用Faiss库计算用户和物品Embedding Layer之间的近邻关系，并根据预测的物品列表计算召回率和命中率。具体来说就是根据用户ID索引到对应的Embedding向量，然后在物品Embedding向量集合中搜索近邻，得到预测的物品列表。最后，根据预测的物品列表和真实的物品ID，计算召回率和命中率。</p><pre><code class=\"language-plain\">def eval(self, user_embs, item_embs, test_model_input, item_info, test_set):\n&nbsp;&nbsp;&nbsp;test_true_label = {line[0]: line[2] for line in test_set}\n&nbsp;&nbsp;&nbsp;index = faiss.IndexFlagIP(self.embedding_dim)\n&nbsp;&nbsp;&nbsp;index.add(item_embs)\n&nbsp;&nbsp;&nbsp;D, I = index.search(np.ascontiguousarray(user_embs), 50)\n&nbsp;&nbsp;&nbsp;s = []\n&nbsp;&nbsp;&nbsp;hit = 0\n\n&nbsp;&nbsp;&nbsp;# 统计预测结果\n&nbsp;&nbsp;&nbsp;for i, uid in tqdm(enumerate(test_model_input['user_id'])):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pred = [item_info['item_id'].value[x] for x in I[i]]\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;recall_score = recall_N(test_true_label[uid], pred, N=50)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s.append(recall_score)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if test_true_label[uid] in pred:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hit += 1\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(i)\n\n&nbsp;&nbsp;&nbsp;# 计算召回率和命中率\n&nbsp;&nbsp;&nbsp;recall = np.mean(s)\n&nbsp;&nbsp;&nbsp;hit_rate = hit / len(test_model_input['user_id'])\n\n&nbsp;&nbsp;&nbsp;return recall, hit_rate\n&nbsp;整个流程实际上到这里就结束了，那么最后，我们使用一个scheduler函数将它们串起来：\ndef scheduler(self):\n&nbsp;&nbsp;&nbsp;# 构建训练集、测试集\n&nbsp;&nbsp;&nbsp;train_model_input, train_label, test_model_input, test_label, \\\n&nbsp;&nbsp;&nbsp;train_set, test_set, user_info, item_info = self.training_set_construct()\n&nbsp;&nbsp;&nbsp;#\n&nbsp;&nbsp;&nbsp;self.training_model(train_model_input, train_label)\n\n&nbsp;&nbsp;&nbsp;# 获取用户、item的layer\n&nbsp;&nbsp;&nbsp;# user_embs, item_embs = self.extract_embedding_layer(model, test_model_input, item_info)\n&nbsp;&nbsp;&nbsp;# # 评估模型\n&nbsp;&nbsp;&nbsp;# recall, hit_rate = self.eval(user_embs, item_embs, test_model_input, item_info, test_set)\n&nbsp;&nbsp;&nbsp;# print(recall, hit_rate)\n&nbsp;\n</code></pre><p>这里有一点需要注意，Faiss库目前在Windows上无法使用，必须在Linux上才行。因此，在最后的Schedule阶段，我将这段代码进行了注释。</p><p>整个YouTubeDNN的召回层训练和预测到这里就结束了。</p><p></p><h2>总结</h2><p>到这里，今天的课程就讲完了，接下来我们来对今天的课程做一个简单的总结，学完本节课你应该知道下面三大要点。</p><ol>\n<li>在YouTubeDNN中，数据处理会经过加载数据集、处理数据特征、特征转化为模型输入、模型的搭建和训练、模型评估这5个部分。</li>\n<li>YouTubeDNN模型通过将用户历史行为序列嵌入到低维向量空间中，来学习用户和物品之间的关系。它的输入包括用户历史行为序列以及物品ID，输出包括用户和物品的嵌入向量以及它们之间的相似度得分。</li>\n<li>熟悉使用Python来搭建一整套YouTubeDNN模型代码。</li>\n</ol><h2>课后题</h2><p>本节课学完了，我来给你留两道课后题。</p><ol>\n<li>实现本节课的代码。</li>\n<li>根据我们前面的知识，自动生成数据集。</li>\n</ol><p>欢迎你在留言区与我交流讨论，如果这节课对你有帮助，也欢迎你推荐给朋友一起学习。</p>","neighbors":{"left":{"article_title":"21｜YouTubeDNN：召回算法的后起之秀（上）","id":664211},"right":{"article_title":"23｜流程串联：数据处理和协同过滤串联进行内容推荐","id":665271}},"comments":[{"had_liked":false,"id":380899,"user_name":"静心","can_delete":false,"product_type":"c1","uid":1335457,"ip_address":"北京","ucode":"EB264FA6519FDA","user_header":"https://static001.geekbang.org/account/avatar/00/14/60/a1/45ffdca3.jpg","comment_is_top":false,"comment_ctime":1694424769,"is_pvip":false,"replies":[{"id":140021,"content":"在YouTube DNN中，embedding_dim的大小是一个超参数，需要根据具体任务和数据集来确定。以下是一些指导原则：\n\n数据集规模：embedding_dim的大小通常与数据集的大小相关。较大的数据集通常需要更大的embedding_dim，以便模型可以更好地学习数据中的复杂模式。一般来说，embedding_dim的大小可以在一个相对小的范围内进行实验，例如从10到100。\n\n特征维度：embedding_dim的大小还可以根据离散特征的维度来确定。对于具有大量离散特征的数据集，embedding_dim可能需要相对较大，以确保模型可以捕捉到离散特征之间的细微差异。如果离散特征的维度较小，则可以选择较小的embedding_dim。\n\n任务复杂度：embedding_dim的大小还可以根据任务的复杂度来确定。对于较复杂的任务，例如推荐系统中的点击率预测，可能需要更大的embedding_dim来表示更多的信息。对于较简单的任务，例如二分类问题，可以选择较小的embedding_dim。\n\n关于离散特征和变长特征的选择，一般的原则如下：\n\n离散特征：离散特征是指具有有限取值或离散类别的特征，例如性别、国家、城市等。对于离散特征，可以使用embedding来将其映射到低维连续向量空间中。这使得模型能够学习离散特征之间的相关性和交互关系。通常情况下，离散特征需要经过编码（例如独热编码）并与其他特征一起输入到模型中。\n\n变长特征：变长特征是指具有可变长度的特征，例如用户的历史行为序列或商品的标签列表。对于变长特征，可以使用循环神经网络（RNN）或Transformer等模型来建模。这些模型可以处理可变长度的序列，并捕捉序列中的时序关系和上下文信息。\n\n根据任务和数据的特点，可以选择使用离散特征或变长特征，或者二者的组合。在YouTube DNN中，通常会同时使用离散特征和变长特征来丰富模型的表达能力。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860510,"ip_address":"广东","comment_id":380899,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"有两个问题没弄明白，请老师指点：\n1、embedding_dim的大小是如何确定的？\n2、什么时候用离散特征，什么时候用变长特征？","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631655,"discussion_content":"在YouTube DNN中，embedding_dim的大小是一个超参数，需要根据具体任务和数据集来确定。以下是一些指导原则：\n\n数据集规模：embedding_dim的大小通常与数据集的大小相关。较大的数据集通常需要更大的embedding_dim，以便模型可以更好地学习数据中的复杂模式。一般来说，embedding_dim的大小可以在一个相对小的范围内进行实验，例如从10到100。\n\n特征维度：embedding_dim的大小还可以根据离散特征的维度来确定。对于具有大量离散特征的数据集，embedding_dim可能需要相对较大，以确保模型可以捕捉到离散特征之间的细微差异。如果离散特征的维度较小，则可以选择较小的embedding_dim。\n\n任务复杂度：embedding_dim的大小还可以根据任务的复杂度来确定。对于较复杂的任务，例如推荐系统中的点击率预测，可能需要更大的embedding_dim来表示更多的信息。对于较简单的任务，例如二分类问题，可以选择较小的embedding_dim。\n\n关于离散特征和变长特征的选择，一般的原则如下：\n\n离散特征：离散特征是指具有有限取值或离散类别的特征，例如性别、国家、城市等。对于离散特征，可以使用embedding来将其映射到低维连续向量空间中。这使得模型能够学习离散特征之间的相关性和交互关系。通常情况下，离散特征需要经过编码（例如独热编码）并与其他特征一起输入到模型中。\n\n变长特征：变长特征是指具有可变长度的特征，例如用户的历史行为序列或商品的标签列表。对于变长特征，可以使用循环神经网络（RNN）或Transformer等模型来建模。这些模型可以处理可变长度的序列，并捕捉序列中的时序关系和上下文信息。\n\n根据任务和数据的特点，可以选择使用离散特征或变长特征，或者二者的组合。在YouTube DNN中，通常会同时使用离散特征和变长特征来丰富模型的表达能力。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860510,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":375848,"user_name":"爱极客","can_delete":false,"product_type":"c1","uid":1073528,"ip_address":"广东","ucode":"5FC59927D194D8","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIUXWqIBiadT4H3XvpcLeOkeocfmpInuhCoHviaUrX7B0N8wnOicnqHZeicKg1SlLk070EFRya1RPQIicw/132","comment_is_top":false,"comment_ctime":1686051691,"is_pvip":false,"replies":[{"id":137190,"content":"如果有需要的话，可以的，到时候我和极客时间的工作人员商量一下，看看是以文字的形式还是直播的形式比较好。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1686335820,"ip_address":"中国香港","comment_id":375848,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师，后面会出一篇课后答疑的文章吗？","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":620624,"discussion_content":"如果有需要的话，可以的，到时候我和极客时间的工作人员商量一下，看看是以文字的形式还是直播的形式比较好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1686335820,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"中国香港","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1073528,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIUXWqIBiadT4H3XvpcLeOkeocfmpInuhCoHviaUrX7B0N8wnOicnqHZeicKg1SlLk070EFRya1RPQIicw/132","nickname":"爱极客","note":"","ucode":"5FC59927D194D8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":620674,"discussion_content":"感谢老师能在百忙之中答复。个人认为出一篇解答课后答疑的图文十分必要，感觉这样才能形成闭环。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1686409592,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":620624,"ip_address":"广东","group_id":0},"score":620674,"extra":""}]}]},{"had_liked":false,"id":375879,"user_name":"alexliu","can_delete":false,"product_type":"c1","uid":1112019,"ip_address":"上海","ucode":"DD65983BBC9CD4","user_header":"https://static001.geekbang.org/account/avatar/00/10/f7/d3/2bbc62b2.jpg","comment_is_top":false,"comment_ctime":1686104074,"is_pvip":false,"replies":[{"id":140033,"content":"您是对的，faiss.IndexFlagIP是错误的，应该是faiss.IndexFlatIP。非常抱歉给您带来了困惑。\n\n至于add方法的参数，代码片段中只有item_embs一个参数的可能是因为之前的代码中定义了n的值，将其传递给了add方法。下面是一个可能的例子，用于解释这个情况：\nimport faiss\n\n# 创建索引\nindex = faiss.IndexFlatIP(d)  # 假设d是维度大小\n\n# 这里的n表示要添加的向量数量\nn = 100\nitem_embs = ... # 设置待添加的向量\nindex.add(n, item_embs)\n\n这个例子中，通过将n值传递给add方法，告诉索引要添加的向量数量。它并不是add方法的参数，而是用来限制要添加的向量数量的一个值。add方法的参数应该是表示待添加向量的item_embs。\n\n需要注意的是，代码片段中的add方法与faiss库中的实际实现有所不同，这只是一个示例。实际的add方法可以根据具体的索引类型和实现细节有所区别，您可以查阅faiss官方文档或源码以了解具体的add方法和参数。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860884,"ip_address":"广东","comment_id":375879,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师，faiss.IndexFlagIP这个应该是faiss.IndexFlatIP吧？另外index.add(n,x)有两个参数，为什么在代码里只有item_embs一个参数？\nps：add源码如下：\n    def add(self, n, x):\n        r&quot;&quot;&quot; default add uses sa_encode&quot;&quot;&quot;\n        return _swigfaiss.IndexFlatCodes_add(self, n, x)","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631668,"discussion_content":"您是对的，faiss.IndexFlagIP是错误的，应该是faiss.IndexFlatIP。非常抱歉给您带来了困惑。\n\n至于add方法的参数，代码片段中只有item_embs一个参数的可能是因为之前的代码中定义了n的值，将其传递给了add方法。下面是一个可能的例子，用于解释这个情况：\nimport faiss\n\n# 创建索引\nindex = faiss.IndexFlatIP(d)  # 假设d是维度大小\n\n# 这里的n表示要添加的向量数量\nn = 100\nitem_embs = ... # 设置待添加的向量\nindex.add(n, item_embs)\n\n这个例子中，通过将n值传递给add方法，告诉索引要添加的向量数量。它并不是add方法的参数，而是用来限制要添加的向量数量的一个值。add方法的参数应该是表示待添加向量的item_embs。\n\n需要注意的是，代码片段中的add方法与faiss库中的实际实现有所不同，这只是一个示例。实际的add方法可以根据具体的索引类型和实现细节有所区别，您可以查阅faiss官方文档或源码以了解具体的add方法和参数。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860884,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":375849,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1686053007,"is_pvip":false,"replies":[{"id":140032,"content":"需要开发，要自己组装特征。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860836,"ip_address":"广东","comment_id":375849,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"YoutubeDNN是拿来就能用吗？类似于工具软件那种，不需要开发。","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631667,"discussion_content":"需要开发，要自己组装特征。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860836,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394689,"user_name":"Emma","can_delete":false,"product_type":"c1","uid":2698428,"ip_address":"新加坡","ucode":"C93FFB7DF0B3F3","user_header":"https://static001.geekbang.org/account/avatar/00/29/2c/bc/5311e976.jpg","comment_is_top":false,"comment_ctime":1727603107,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"请问老师，youtubeDNN的排序部分的代码有吗","like_count":1},{"had_liked":false,"id":392011,"user_name":"爱极客","can_delete":false,"product_type":"c1","uid":1073528,"ip_address":"广东","ucode":"5FC59927D194D8","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIUXWqIBiadT4H3XvpcLeOkeocfmpInuhCoHviaUrX7B0N8wnOicnqHZeicKg1SlLk070EFRya1RPQIicw/132","comment_is_top":false,"comment_ctime":1719709909,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"# 定义模型\n        model = YoutubeDNN(self.user_feature_columns, self.item_feature_columns, num_sampled=100,\n                           user_dnn_hidden_units=(128, 64, self.embedding_dim))  \n\n这个参数 num_sampled=100 在新版的模型API里面是没有的，希望老师解答","like_count":0,"discussions":[{"author":{"id":2189275,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/db/6146bce8.jpg","nickname":"朱得君","note":"","ucode":"ADA07F1B1FC95F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652452,"discussion_content":"这一段模型的代码总是报错, 确实跑不通哦, 你能运行了吗?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1728967265,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}