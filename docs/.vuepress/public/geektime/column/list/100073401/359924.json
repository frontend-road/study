{"id":359924,"title":"12 | 广播变量（一）：克制Shuffle，如何一招制胜！","content":"<p>你好，我是吴磊。</p><p>在数据分析领域，数据关联（Joins）是Shuffle操作的高发区，二者如影随从。可以说，有Joins的地方，就有Shuffle。</p><p>我们说过，面对Shuffle，开发者应当“能省则省、能拖则拖”。我们已经讲过了怎么拖，拖指的就是，把应用中会引入Shuffle的操作尽可能地往后面的计算步骤去拖。那具体该怎么省呢？</p><p>在数据关联场景中，广播变量就可以轻而易举地省去Shuffle。所以今天这一讲，我们就先说一说广播变量的含义和作用，再说一说它是如何帮助开发者省去Shuffle操作的。</p><h2>如何理解广播变量？</h2><p>接下来，咱们借助一个小例子，来讲一讲广播变量的含义与作用。这个例子和Word Count有关，它可以说是分布式编程里的Hello world了，Word Count就是用来统计文件中全部单词的，你肯定已经非常熟悉了，所以，我们例子中的需求增加了一点难度，我们要对指定列表中给定的单词计数。</p><pre><code>val dict = List(“spark”, “tune”)\nval words = spark.sparkContext.textFile(“~/words.csv”)\nval keywords = words.filter(word =&gt; dict.contains(word))\nkeywords.map((_, 1)).reduceByKey(_ + _).collect\n</code></pre><p>按照这个需求，同学小A实现了如上的代码，一共有4行，我们逐一来看。第1行在Driver端给定待查单词列表dict；第2行以textFile API读取分布式文件，内容包含一列，存储的是常见的单词；第3行用列表dict中的单词过滤分布式文件内容，只保留dict中给定的单词；第4行调用reduceByKey对单词进行累加计数。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/ba/39/ba45d47a910ccb92861b1fd153b36839.jpg?wh=3540*1482\" alt=\"\" title=\"数据结构dict随着Task一起分发到Executors\"></p><p>学习过调度系统之后，我们知道，第一行代码定义的dict列表连带后面的3行代码会一同打包到Task里面去。这个时候，Task就像是一架架小飞机，携带着这些“行李”，飞往集群中不同的Executors。对于这些“行李”来说，代码的“负重”较轻，可以忽略不计，而数据的负重占了大头，成了最主要的负担。</p><p>你可能会说：“也还好吧，dict列表又不大，也没什么要紧的”。但是，如果我们假设这个例子中的并行度是10000，那么，Driver端需要通过网络分发总共10000份dict拷贝。这个时候，集群内所有的Executors需要消耗大量内存来存储这10000份的拷贝，对宝贵的网络和内存资源来说，这已经是一笔不小的浪费了。更何况，如果换做一个更大的数据结构，Task分发所引入的网络与内存开销会更可怕。</p><p>换句话说，统计计数的业务逻辑还没有开始执行，Spark就已经消耗了大量的网络和存储资源，这简直不可理喻。因此，我们需要对示例中的代码进行优化，从而跳出这样的窘境。</p><p>但是，在着手优化之前，我们不妨先来想一想，现有的问题是什么，我们要达到的目的是什么。结合刚刚的分析，我们不难发现，<strong>Word Count的核心痛点在于，数据结构的分发和存储受制于并行，并且是以Task为粒度的，因此往往频次过高。痛点明确了，调优的目的也就清晰了，我们需要降低数据结构分发的频次</strong>。</p><p>要达到这个目的，我们首先想到的就是降低并行度。不过，牵一发而动全身，并行度一旦调整，其他与CPU、内存有关的配置项都要跟着适配，这难免把调优变复杂了。实际上，要降低数据结构的分发频次，我们还可以考虑广播变量。</p><p><strong>广播变量是一种分发机制，它一次性封装目标数据结构，以Executors为粒度去做数据分发。</strong>换句话说，在广播变量的工作机制下，数据分发的频次等同于集群中的Executors个数。通常来说，集群中的Executors数量都远远小于Task数量，相差两到三个数量级是常有的事。那么，对于第一版的Word Count实现，如果我们使用广播变量的话，会有哪些变化呢？</p><p>代码的改动很简单，主要有两个改动：第一个改动是用broadcast封装dict列表，第二个改动是在访问dict列表的地方改用broadcast.value替代。</p><pre><code>val dict = List(“spark”, “tune”)\nval bc = spark.sparkContext.broadcast(dict)\nval words = spark.sparkContext.textFile(“~/words.csv”)\nval keywords = words.filter(word =&gt; bc.value.contains(word))\nkeywords.map((_, 1)).reduceByKey(_ + _).collect\n</code></pre><p>你可能会说：“这个改动看上去也没什么呀！”别着急，我们先来分析一下，改动之后的代码在运行时都有哪些变化。</p><p><strong>在广播变量的运行机制下，封装成广播变量的数据，由Driver端以Executors为粒度分发，每一个Executors接收到广播变量之后，将其交给BlockManager管理</strong>。由于广播变量携带的数据已经通过专门的途径存储到BlockManager中，因此分发到Executors的Task不需要再携带同样的数据。</p><p>这个时候，你可以把广播变量想象成一架架专用货机，专门为Task这些小飞机运送“大件行李”。Driver与每一个Executors之间都开通一条这样的专用货机航线，统一运载负重较大的“数据行李”。有了专用货机来帮忙，Task小飞机只需要携带那些负重较轻的代码就好了。等这些Task小飞机在Executors着陆，它们就可以到Executors的公用仓库BlockManager里去提取它们的“大件行李”。</p><p><img src=\"https://static001.geekbang.org/resource/image/2c/f7/2cfe084a106a01bf14a63466fa2146f7.jpg?wh=3729*1314\" alt=\"\" title=\"用广播变量封装dict列表\"></p><p>总之，在广播变量的机制下，dict列表数据需要分发和存储的次数锐减。我们假设集群中有20个Executors，不过任务并行度还是10000，那么，Driver需要通过网络分发的dict列表拷贝就会由原来的10000份减少到20份。同理，集群范围内所有Executors需要存储的dict拷贝，也由原来的10000份，减少至20份。这个时候，引入广播变量后的开销只是原来Task分发的1/500！</p><h2>广播分布式数据集</h2><p>那在刚刚的示例代码中，广播变量封装的是Driver端创建的普通变量：字符串列表。除此之外，<strong>广播变量也可以封装分布式数据集</strong>。</p><p>我们来看这样一个例子。在电子商务领域中，开发者往往用事实表来存储交易类数据，用维度表来存储像物品、用户这样的描述性数据。事实表的特点是规模庞大，数据体量随着业务的发展不断地快速增长。维度表的规模要比事实表小很多，数据体量的变化也相对稳定。</p><p>假设用户维度数据以Parquet文件格式存储在HDFS文件系统中，业务部门需要我们读取用户数据并创建广播变量以备后用，我们该怎么做呢？很简单，几行代码就可以搞定！</p><pre><code>val userFile: String = “hdfs://ip:port/rootDir/userData”\nval df: DataFrame = spark.read.parquet(userFile)\nval bc_df: Broadcast[DataFrame] = spark.sparkContext.broadcast(df)\n</code></pre><p>首先，我们用Parquet API读取HDFS分布式数据文件生成DataFrame，然后用broadcast封装DataFrame。从代码上来看，这种实现方式和封装普通变量没有太大差别，它们都调用了broadcast API，只是传入的参数不同。</p><p>但如果不从开发的视角来看，转而去观察运行时广播变量的创建过程的话，我们就会发现，分布式数据集与普通变量之间的差异非常显著。</p><p>从普通变量创建广播变量，由于数据源就在Driver端，因此，只需要Driver把数据分发到各个Executors，再让Executors把数据缓存到BlockManager就好了。</p><p>但是，从分布式数据集创建广播变量就要复杂多了，具体的过程如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/6c/8ac91a174803b97966289ff51938106c.jpg?wh=3954*1614\" alt=\"\" title=\"从分布式数据集创建广播变量的过程\"></p><p>与普通变量相比，分布式数据集的数据源不在Driver端，而是来自所有的Executors。Executors中的每个分布式任务负责生产全量数据集的一部分，也就是图中不同的数据分区。因此，步骤1就是<strong>Driver从所有的Executors拉取这些数据分区，然后在本地构建全量数据。</strong>步骤2与从普通变量创建广播变量的过程类似。  <strong>Driver把汇总好的全量数据分发给各个Executors，Executors将接收到的全量数据缓存到存储系统的BlockManager中</strong>。</p><p>不难发现，相比从普通变量创建广播变量，从分布式数据集创建广播变量的网络开销更大。原因主要有二：一是，前者比后者多了一步网络通信；二是，前者的数据体量通常比后者大很多。</p><h2>如何用广播变量克制Shuffle？</h2><p>你可能会问：“Driver从Executors拉取DataFrame的数据分片，揉成一份全量数据，然后再广播出去，抛开网络开销不说，来来回回得费这么大劲，图啥呢？”这是一个好问题，因为以广播变量的形式缓存分布式数据集，正是克制Shuffle杀手锏。</p><h3>Shuffle Joins</h3><p>为什么这么说呢？我还是拿电子商务场景举例。有了用户的数据之后，为了分析不同用户的购物习惯，业务部门要求我们对交易表和用户表进行数据关联。这样的数据关联需求在数据分析领域还是相当普遍的。</p><pre><code>val transactionsDF: DataFrame = _\nval userDF: DataFrame = _\ntransactionsDF.join(userDF, Seq(“userID”), “inner”)\n</code></pre><p>因为需求非常明确，同学小A立即调用Parquet数据源API，读取分布式文件，创建交易表和用户表的DataFrame，然后调用DataFrame的Join方法，以userID作为Join keys，用内关联（Inner Join）的方式完成了两表的数据关联。</p><p>在分布式环境中，交易表和用户表想要以userID为Join keys进行关联，就必须要确保一个前提：交易记录和与之对应的用户信息在同一个Executors内。也就是说，如果用户黄小乙的购物信息都存储在Executor 0，而个人属性信息缓存在Executor 2，那么，在分布式环境中，这两种信息必须要凑到同一个进程里才能实现关联计算。</p><p>在不进行任何调优的情况下，Spark默认采用Shuffle Join的方式来做到这一点。Shuffle Join的过程主要有两步。</p><p><strong>第一步就是对参与关联的左右表分别进行Shuffle</strong>，Shuffle的分区规则是先对Join keys计算哈希值，再把哈希值对分区数取模。由于左右表的分区数是一致的，因此Shuffle过后，一定能够保证userID相同的交易记录和用户数据坐落在同一个Executors内。</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/28/b1b2a574eb7ef33e2315f547ecdc0328.jpg?wh=5124*1599\" alt=\"\" title=\"Shuffle Join中左右表的数据分发\"></p><p>Shuffle完成之后，<strong>第二步就是在同一个Executors内，Reduce task就可以对userID一致的记录进行关联操作</strong>。但是，由于交易表是事实表，数据体量异常庞大，对TB级别的数据进行Shuffle，想想都觉得可怕！因此，上面对两个DataFrame直接关联的代码，还有很大的调优空间。我们该怎么做呢？话句话说，对于分布式环境中的数据关联来说，要想确保交易记录和与之对应的用户信息在同一个Executors中，我们有没有其他办法呢？</p><h3>克制Shuffle的方式</h3><p>还记得之前业务部门要求我们把用户表封装为广播变量，以备后用吗？现在它终于派上用场了！</p><pre><code>import org.apache.spark.sql.functions.broadcast\n \nval transactionsDF: DataFrame = _\nval userDF: DataFrame = _\n \nval bcUserDF = broadcast(userDF)\ntransactionsDF.join(bcUserDF, Seq(“userID”), “inner”)\n\n</code></pre><p>Driver从所有Executors收集userDF所属的所有数据分片，在本地汇总用户数据，然后给每一个Executors都发送一份全量数据的拷贝。既然每个Executors都有userDF的<strong>全量数据</strong>，这个时候，交易表的数据分区待在原地、保持不动，就可以轻松地关联到一致的用户数据。如此一来，我们不需要对数据体量巨大的交易表进行Shuffle，同样可以在分布式环境中，完成两张表的数据关联。</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/2a/b3c5ab392c2303bf7923488623b4022a.jpg?wh=5094*1695\" alt=\"\" title=\"Broadcast Join将小表广播，避免大表Shuffle\n\"></p><p>利用广播变量，我们成功地避免了海量数据在集群内的存储、分发，节省了原本由Shuffle引入的磁盘和网络开销，大幅提升运行时执行性能。当然，采用广播变量优化也是有成本的，毕竟广播变量的创建和分发，也是会带来网络开销的。但是，相比大表的全网分发，小表的网络开销几乎可以忽略不计。这种小投入、大产出，用极小的成本去博取高额的性能收益，真可以说是“四两拨千斤”！</p><h2>小结</h2><p>在数据关联场景中，广播变量是克制Shuffle的杀手锏。掌握了它，我们就能以极小的成本，获得高额的性能收益。关键是我们要掌握两种创建广播变量的方式。</p><p>第一种，从普通变量创建广播变量。在广播变量的运行机制下，普通变量存储的数据封装成广播变量，由Driver端以Executors为粒度进行分发，每一个Executors接收到广播变量之后，将其交由BlockManager管理。</p><p>第二种，从分布式数据集创建广播变量，这就要比第一种方式复杂一些了。第一步，Driver需要从所有的Executors拉取数据分片，然后在本地构建全量数据；第二步，Driver把汇总好的全量数据分发给各个Executors，Executors再将接收到的全量数据缓存到存储系统的BlockManager中。</p><p>结合这两种方式，我们在做数据关联的时候，把Shuffle Joins转换为Broadcast Joins，就可以用小表广播来代替大表的全网分发，真正做到克制Shuffle。</p><h2>每日一练</h2><ol>\n<li>Spark广播机制现有的实现方式是存在隐患的，在数据量较大的情况下，Driver可能会成为瓶颈，你能想到更好的方式来重新实现Spark的广播机制吗？（提示：<a href=\"https://issues.apache.org/jira/browse/SPARK-17556\">SPARK-17556</a>）</li>\n<li>在什么情况下，不适合把Shuffle Joins转换为Broadcast Joins？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","neighbors":{"left":{"article_title":"11 | 为什么说Shuffle是一时无两的性能杀手？","id":359907},"right":{"article_title":"13 | 广播变量（二）：如何让Spark SQL选择Broadcast Joins？","id":360837}},"comments":[{"had_liked":false,"id":287404,"user_name":"Bennan","can_delete":false,"product_type":"c1","uid":2427818,"ip_address":"","ucode":"8B6EE3DCBB1356","user_header":"https://static001.geekbang.org/account/avatar/00/25/0b/aa/09c1215f.jpg","comment_is_top":false,"comment_ctime":1617932642,"is_pvip":false,"replies":[{"id":"104398","content":"Perfect！满分💯，两道题答的都很好～","user_name":"作者回复","comment_id":287404,"uid":"1043100","ip_address":"","utype":1,"ctime":1617955091,"user_name_real":"吴磊"}],"discussion_count":4,"race_medal":0,"score":"83222311266","product_id":100073401,"comment_content":"1. 改成由driver获取到数据分布，然后通知各个executor之间进行拉取，这样可以利用多个executor网络，避免只有driver组装以后再一个一个发送效率过低<br><br>2.当两个需要join的数据集都很大时，使用broadcast join需要将一个很大的数据集进行网络分发多次，已经远超出了shuffle join需要传输的数据","like_count":20,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518294,"discussion_content":"Perfect！满分💯，两道题答的都很好～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617955091,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2635387,"avatar":"https://static001.geekbang.org/account/avatar/00/28/36/7b/b06aad84.jpg","nickname":"空","note":"","ucode":"01F63669AE814A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":378921,"discussion_content":"请问第一种要怎么实现呢？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1623510820,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537966,"discussion_content":"请问第一种要怎么实现呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639281940,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2273076,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eo1QZQ0eoOqvOyMyAzcsyEK8bQaZTd9aJHTTCYicicUm9gmhUFaxQe6IBc3caLVD8PhqtmhhicrNHe0w/132","nickname":"Severus4","note":"","ucode":"79BC6487B595EE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556851,"discussion_content":"没实现成功，多个executor互相广播的话网络之类的开销十分巨大，all-to-all的时候不一定吃得消，毕竟executor的配置一般都会比driver差点，原作者提交了很多次都没过，后面觉得暂时也没迫切的需求要实现executor broadcast，就给关闭了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647537129,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":537966,"ip_address":""},"score":556851,"extra":""}]}]},{"had_liked":false,"id":292464,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1620830703,"is_pvip":false,"replies":[{"id":"105936","content":"你是对的，确实会报错。这里我没有交代清楚，RDD确实不能直接用广播变量封装，它不像DataFrame，DataFrame广播的那部分源码在内部把collect这个事做了，所以你可以直接用广播封装DataFrame，但是RDD没有，确实需要先手动collect RDD数据集，然后再在driver端用广播变量封装，我的锅，没有交代清楚~<br><br>不过，歪打正着，通过这个例子，你可以更好地理解Driver在构建广播变量时的计算过程，也就是第一步都是把数据集collect到Driver端，不管是RDD、DataFrame、Dataset，区别无非是collect这件事是谁做的。RDD是开发者来做，而DataFrame、Dataset是Spark自己“偷偷”做了。","user_name":"作者回复","comment_id":292464,"uid":"1043100","ip_address":"","utype":1,"ctime":1620899739,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"66045340143","product_id":100073401,"comment_content":"磊哥，为什么我测试了广播rdd不行：<br>我写了个demo，广播rdd是报错的，代码如下：<br>    val userFile: String =&quot;spark-basic&#47;File&#47;csv_data.csv&quot;<br>    val df: DataFrame = spark.read.csv(userFile)<br>    val rdd = spark.sparkContext.textFile(&quot;userFile&quot;)<br>    val bc_df: Broadcast[RDD[String]] = spark.sparkContext.broadcast(rdd)<br>    bc_df.value.collect().foreach(println)<br><br>报错如下：Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.<br><br>然后看了一下源码：SparkContext中的broadcast方法：<br><br>def broadcast[T: ClassTag](value: T): Broadcast[T] = {<br>    assertNotStopped()<br>    require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass),<br>      &quot;Can not directly broadcast RDDs; instead, call collect() and broadcast the result.&quot;)<br>    val bc = env.broadcastManager.newBroadcast[T](value, isLocal)<br>    val callSite = getCallSite<br>    logInfo(&quot;Created broadcast &quot; + bc.id + &quot; from &quot; + callSite.shortForm)<br>    cleaner.foreach(_.registerBroadcastForCleanup(bc))<br>    bc<br>  }<br><br>第4行的代码显示的Can not directly broadcast RDDs<br>是不是我哪里不太对？","like_count":16,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519840,"discussion_content":"你是对的，确实会报错。这里我没有交代清楚，RDD确实不能直接用广播变量封装，它不像DataFrame，DataFrame广播的那部分源码在内部把collect这个事做了，所以你可以直接用广播封装DataFrame，但是RDD没有，确实需要先手动collect RDD数据集，然后再在driver端用广播变量封装，我的锅，没有交代清楚~\n\n不过，歪打正着，通过这个例子，你可以更好地理解Driver在构建广播变量时的计算过程，也就是第一步都是把数据集collect到Driver端，不管是RDD、DataFrame、Dataset，区别无非是collect这件事是谁做的。RDD是开发者来做，而DataFrame、Dataset是Spark自己“偷偷”做了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620899739,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1476427,"avatar":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","nickname":"tiankonghewo","note":"","ucode":"7A55A9C17DD9DF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532342,"discussion_content":"大赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637582041,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287506,"user_name":"Jack","can_delete":false,"product_type":"c1","uid":1297112,"ip_address":"","ucode":"F3863DAEF449D5","user_header":"https://static001.geekbang.org/account/avatar/00/13/ca/d8/b109ed85.jpg","comment_is_top":false,"comment_ctime":1617981402,"is_pvip":true,"replies":[{"id":"104431","content":"非常赞哈👍哈～ 凡是看源码的同学，都先给个赞～<br><br>这块非常值得探讨。我的理解是这样的，代码层面，spark确实已经有code在尝试用p2p的方式来分发广播变量，从而减轻driver负担。<br><br>但是，据我观察，这部分代码尚未生效。细节可以参考这个ticket：【Executor side broadcast for broadcast joins】https:&#47;&#47;issues.apache.org&#47;jira&#47;browse&#47;SPARK-17556，看上去还是进行中的状态。<br><br>另外，从代码看，目前还是先用collect拉到driver，然后再分发出去：<br><br>BroadcastExchangeExec中的relationFuture用于获取广播变量内容<br>在relationFuture内部：<br>  1. 先是调用executeCollectIterator生成内容relation；<br>     其中，executeCollectIterator调用collect把结果集收集到driver端<br>  2. 然后用sparkContext.broadcast(relation)，把生成好的内容广播到各个Executors<br>并没有看到哪里从Executors拉取数据分片、来减轻driver负载。<br><br>并且，这里还有提示driver内存不够的exception：<br>new OutOfMemoryError(&quot;Not enough memory to build and broadcast the table to all &quot; + &quot;worker nodes. As a workaround, you can either disable broadcast by setting &quot; + s&quot;${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 or increase the spark &quot; + s&quot;driver memory by setting ${SparkLauncher.DRIVER_MEMORY} to a higher value.&quot;).initCause(oe.getCause))<br><br>你贴的这段代码确实在尝试用片2片，不过，需要仔细看看，它在哪里调用，被谁调用～","user_name":"作者回复","comment_id":287506,"uid":"1043100","ip_address":"","utype":1,"ctime":1618055686,"user_name_real":"吴磊"}],"discussion_count":8,"race_medal":0,"score":"23092817882","product_id":100073401,"comment_content":"老师，对于第1题，看了下spark的源码，目前Broadcast只有一个实现类TorrentBroadcast，看代码的注释，这个类通过使用类似Bit-torrent协议的方法解决了Driver成为瓶颈的问题。目前Spark还会存在广播变量的数据太大造成Driver成为瓶颈的问题吗？<br><br>&#47;**<br> * A BitTorrent-like implementation of [[org.apache.spark.broadcast.Broadcast]].<br> *<br> * The mechanism is as follows:<br> *<br> * The driver divides the serialized object into small chunks and<br> * stores those chunks in the BlockManager of the driver.<br> *<br> * On each executor, the executor first attempts to fetch the object from its BlockManager. If<br> * it does not exist, it then uses remote fetches to fetch the small chunks from the driver and&#47;or<br> * other executors if available. Once it gets the chunks, it puts the chunks in its own<br> * BlockManager, ready for other executors to fetch from.<br> *<br> * This prevents the driver from being the bottleneck in sending out multiple copies of the<br> * broadcast data (one per executor).<br> *<br> * When initialized, TorrentBroadcast objects read SparkEnv.get.conf.<br> *<br> * @param obj object to broadcast<br> * @param id A unique identifier for the broadcast variable.<br> *&#47;<br>private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long)<br>  extends Broadcast[T](id) with Logging with Serializable {<br>","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518328,"discussion_content":"非常赞哈👍哈～ 凡是看源码的同学，都先给个赞～\n\n这块非常值得探讨。我的理解是这样的，代码层面，spark确实已经有code在尝试用p2p的方式来分发广播变量，从而减轻driver负担。\n\n但是，据我观察，这部分代码尚未生效。细节可以参考这个ticket：【Executor side broadcast for broadcast joins】https://issues.apache.org/jira/browse/SPARK-17556，看上去还是进行中的状态。\n\n另外，从代码看，目前还是先用collect拉到driver，然后再分发出去：\n\nBroadcastExchangeExec中的relationFuture用于获取广播变量内容\n在relationFuture内部：\n  1. 先是调用executeCollectIterator生成内容relation；\n     其中，executeCollectIterator调用collect把结果集收集到driver端\n  2. 然后用sparkContext.broadcast(relation)，把生成好的内容广播到各个Executors\n并没有看到哪里从Executors拉取数据分片、来减轻driver负载。\n\n并且，这里还有提示driver内存不够的exception：\nnew OutOfMemoryError(&amp;quot;Not enough memory to build and broadcast the table to all &amp;quot; + &amp;quot;worker nodes. As a workaround, you can either disable broadcast by setting &amp;quot; + s&amp;quot;${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 or increase the spark &amp;quot; + s&amp;quot;driver memory by setting ${SparkLauncher.DRIVER_MEMORY} to a higher value.&amp;quot;).initCause(oe.getCause))\n\n你贴的这段代码确实在尝试用片2片，不过，需要仔细看看，它在哪里调用，被谁调用～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618055686,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366422,"discussion_content":"额，看起来老师和层主好像不在一个频道上，层主的 BitTorrent 是 executor 从 driver fetch 广播数据的阶段，使用 p2p 的方式。而老师说的是 executor 产生需要广播的数据被 driver 端收集的阶段，这时候应该不是 p2p。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1618057173,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":5,"child_discussions":[{"author":{"id":1112182,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f8/76/3db69173.jpg","nickname":"onepieceJT2018","note":"","ucode":"C8C214C3D5D285","race_medal":3,"user_type":1,"is_pvip":true},"reply_author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":373592,"discussion_content":"层主分享的这个类就是broadcast现在的拉取的方式 老师分享的这个jira是说去中心化 不由driver去管理拉取 而是executor之间自己拉取缺失的部分","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1620794023,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":366422,"ip_address":""},"score":373592,"extra":""},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1112182,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f8/76/3db69173.jpg","nickname":"onepieceJT2018","note":"","ucode":"C8C214C3D5D285","race_medal":3,"user_type":1,"is_pvip":true},"discussion":{"id":373605,"discussion_content":"是的，大家说的都对，这里我原文没有说清楚，是我的锅哈。这里其实分两个步骤：\n1. 第一个是Driver从所有Executors收集数据，构建广播变量，也就是全量数据，这个过程Driver是瓶颈；\n2. 第二个是各个Executors再去拉取广播变量，这个过程就像你们说的，现在只有TorrentBroadcast这一种实现方式。这个时候，Executors之间用类似“P2P”的方式去拉取广播变量的每一个block。优先从本机的其他Executors拉取、其次本Rack的其他机器去拉取，这里有点类似任务本地性，总之对于target block，给定block地址列表，BlockManager会优先选取拉取成本最低的那个去尝试。之所有说是类似“P2P”，原因在于它拉取的形式是P2P，但是这里并没有什么P2P协议，而是利用BlockManager来实现定向拉取。BlockManager从BlockManagerMaster获取目标block地址列表，然后按照刚刚说的顺序，依次尝试拉取Block。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1620797510,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":373592,"ip_address":""},"score":373605,"extra":""},{"author":{"id":2485585,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","nickname":"Geek_d794f8","note":"","ucode":"1E20DA4FF8B800","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":373741,"discussion_content":"那就是说TorrentBroadcast这个实现类下的代码目前是生效的对吧。如果是由driver依次发送给各个executor，而是各个executor之间拉取，那么driver构建全量数据的意义是什么?","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1620863136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":373605,"ip_address":""},"score":373741,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366415,"discussion_content":"p2p，不是片2片，手机上打字，容易出错。。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618056631,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":297057,"user_name":"冯杰","can_delete":false,"product_type":"c1","uid":1950765,"ip_address":"","ucode":"61C92D62D49A66","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/qmdZbyxrRD5qQLKjWkmdp3PCVhwmWTcp0cs04s39pic2RcNw0nNKTDgKqedSQ54bAGWjAVSc9p4vWP8RJRKB6nA/132","comment_is_top":false,"comment_ctime":1623293288,"is_pvip":false,"replies":[{"id":"107957","content":"先来说第一个问题~<br><br>a）你说的没错，按理说，应该走广播，毕竟广播阈值默认10MB，你的数据集足够小，磁盘上1MB，内存里4MB，怎么看都比广播阈值小。我能想到的可能的原因，就是Spark SQL对于数据集大小的误判，就是它对于DIM的预估大于10MB。你不妨用下面这个方法，计算一下DIM在Spark SQL下的预判大小：<br><br>val df: DataFrame = _<br>df.cache.count<br> <br>val plan = df.queryExecution.logical<br>val estimated: BigInt = spark<br>.sessionState<br>.executePlan(plan)<br>.optimizedPlan<br>.stats<br>.sizeInBytes<br><br>看看能不能发现什么端倪。虽然咱们不能完全确认原因，不过要解决这个问题倒是蛮简单的。一来可以用broadcast函数，二来可以用join hints，总之就是各种花式强制广播。<br><br>b）这里就需要更多信息来判断了，6G是从哪里得到的？Spark UI的DAG图吗？sort阶段的6G，仅仅是DIM表的大小？可以加我微信“方块K”或是“rJunior”，把完整的DAG贴给我~","user_name":"作者回复","comment_id":297057,"uid":"1043100","ip_address":"","utype":1,"ctime":1623396857,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"14508195176","product_id":100073401,"comment_content":"老师你好，关于broacast join，遇到了一个特别的问题请教一下。<br>1、Fact(订单) 和 DIM(门店) 关联。  其中门店表量级为 3w(条数) * 10个(字段)，采用Parquet存储在Hive上，大小1M左右。<br>2、运行参数，并行度 = 200，Executor = 50，CPU核数 = 2，内存&#47;Executor = 6G，Drvier内存=2G。 PS：没有特别配置Broadcast 相关参数<br>3、执行时，有两个疑问点，不得其解<br>    a）Spark UI 显示，并没有执行BHJ，反而执行了 hash sort merge join。   照理，如此小的数据，应该走前者<br>    b）Spark UI 显示，走hash sort merge join后，shuffle阶段的内存计算大小为4MB，sort阶段的内存计算大小为6G。    为何sort完后，为膨胀的如此厉害。<br><br>","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521701,"discussion_content":"先来说第一个问题~\n\na）你说的没错，按理说，应该走广播，毕竟广播阈值默认10MB，你的数据集足够小，磁盘上1MB，内存里4MB，怎么看都比广播阈值小。我能想到的可能的原因，就是Spark SQL对于数据集大小的误判，就是它对于DIM的预估大于10MB。你不妨用下面这个方法，计算一下DIM在Spark SQL下的预判大小：\n\nval df: DataFrame = _\ndf.cache.count\n \nval plan = df.queryExecution.logical\nval estimated: BigInt = spark\n.sessionState\n.executePlan(plan)\n.optimizedPlan\n.stats\n.sizeInBytes\n\n看看能不能发现什么端倪。虽然咱们不能完全确认原因，不过要解决这个问题倒是蛮简单的。一来可以用broadcast函数，二来可以用join hints，总之就是各种花式强制广播。\n\nb）这里就需要更多信息来判断了，6G是从哪里得到的？Spark UI的DAG图吗？sort阶段的6G，仅仅是DIM表的大小？可以加我微信“方块K”或是“rJunior”，把完整的DAG贴给我~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623396857,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287605,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1618046452,"is_pvip":false,"replies":[{"id":"104547","content":"思考的很深入，👍赞一个~<br><br>1. 我理解，你问的是这句吧？“由于左右表的分区数是一致的，因此 Shuffle 过后，一定能够保证 userID 相同的交易记录和用户数据坐落在同一个 Executors 内。” HadoopRDD的分区数、或者说并行度，确实是由HDFS文件系统决定的；但是，Shuffle过后，每个分布式数据集的并行度，就由参数spark.sql.shuffle.partitions来决定了，这个咱们在配置项哪一讲说过哟~ 因此，如果你没有手工用repartition或是Coalesce去调整并行度，默认情况下，大家Shuffle过后（在Reduce阶段）都是这个并行度。<br><br>2. 默认确实是开启的，默认值确实也是10MB，但是，这个10MB太太太太太太（太 x N）小了！很多小表其实都超过了这个阈值，因此，如果你懒得去调整这个参数，可以直接用broadcast(userDF)这种强制广播的方式，省时省力，比较方便~","user_name":"作者回复","comment_id":287605,"uid":"1043100","ip_address":"","utype":1,"ctime":1618220469,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"10207981044","product_id":100073401,"comment_content":"老师有两个问题请教一下：<br>1.文中提到两个表join，两个表数据量相差很大呀，为什么他们的的分区数是一致的，而且分区数不是根据hadoop的切片规则去划分的吗？<br>2.广播join不是默认开启的吗，好像小表默认10M；还需像文中代码val bcUserDF = broadcast(userDF)这样声明吗？<br>希望得到您的指导，多谢！","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518364,"discussion_content":"思考的很深入，👍赞一个~\n\n1. 我理解，你问的是这句吧？“由于左右表的分区数是一致的，因此 Shuffle 过后，一定能够保证 userID 相同的交易记录和用户数据坐落在同一个 Executors 内。” HadoopRDD的分区数、或者说并行度，确实是由HDFS文件系统决定的；但是，Shuffle过后，每个分布式数据集的并行度，就由参数spark.sql.shuffle.partitions来决定了，这个咱们在配置项哪一讲说过哟~ 因此，如果你没有手工用repartition或是Coalesce去调整并行度，默认情况下，大家Shuffle过后（在Reduce阶段）都是这个并行度。\n\n2. 默认确实是开启的，默认值确实也是10MB，但是，这个10MB太太太太太太（太 x N）小了！很多小表其实都超过了这个阈值，因此，如果你懒得去调整这个参数，可以直接用broadcast(userDF)这种强制广播的方式，省时省力，比较方便~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618220469,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291288,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1620184984,"is_pvip":false,"replies":[{"id":"105524","content":"这里其实有两个容易混淆的概念哈~<br><br>一个是并行度，并行度其实是从数据角度出发，表示的是你的分布式数据集划分的粒度，再直白点说，它和分区数是等效的。因此，它其实跟你集群有多少Executors，每个Executors配置了多少cores，没有关系~<br><br>第二个是并发度，或者叫Executors线程池大小，也就是你用spark.executor.cores类似的参数，给Executors指定的cores资源。它限制了在同一时间，你的Executors中最多同时能跑多少个任务。Executors并发度乘以集群中的Executors数量，其实就是你集群的并发处理能力，很多地方也叫并行处理能力。其实蛋疼的地方在于，不同的作者、不同的上下文，并发和并行这两个词，总是混用。所以也就造成大家都比较困惑。<br><br>咱们在配置项第一讲，其实就在尝试厘清、约定这两个词的定义，一来方便大家理解，二来方便后续讨论。<br><br>所以，回答你的问题，其实没什么不健康的哈~ 10000并行度，意味着10000个分区的分布式数据集，这个应该不难见到。另外100个cores的集群，其实也不算小了~ 不过你说的2000任务我没有get到，不知道是2000并行度，还是2000的集群并发。如果是2000集群并发的话，这个数和100cores对不上。这意味着你的每个core需要20个超线程，哈哈，目前还没有这么给力的CPU。一般CPU也就2个超线程。","user_name":"作者回复","comment_id":291288,"uid":"1043100","ip_address":"","utype":1,"ctime":1620227396,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"5915152280","product_id":100073401,"comment_content":"老师我生产中为啥从没有遇到过10000并行度那么大的stage，可能我公司比较小吧，集群最多才100多个核，多数时才几百个任务，最多时也才2000多个任务。这健康吗？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519429,"discussion_content":"这里其实有两个容易混淆的概念哈~\n\n一个是并行度，并行度其实是从数据角度出发，表示的是你的分布式数据集划分的粒度，再直白点说，它和分区数是等效的。因此，它其实跟你集群有多少Executors，每个Executors配置了多少cores，没有关系~\n\n第二个是并发度，或者叫Executors线程池大小，也就是你用spark.executor.cores类似的参数，给Executors指定的cores资源。它限制了在同一时间，你的Executors中最多同时能跑多少个任务。Executors并发度乘以集群中的Executors数量，其实就是你集群的并发处理能力，很多地方也叫并行处理能力。其实蛋疼的地方在于，不同的作者、不同的上下文，并发和并行这两个词，总是混用。所以也就造成大家都比较困惑。\n\n咱们在配置项第一讲，其实就在尝试厘清、约定这两个词的定义，一来方便大家理解，二来方便后续讨论。\n\n所以，回答你的问题，其实没什么不健康的哈~ 10000并行度，意味着10000个分区的分布式数据集，这个应该不难见到。另外100个cores的集群，其实也不算小了~ 不过你说的2000任务我没有get到，不知道是2000并行度，还是2000的集群并发。如果是2000集群并发的话，这个数和100cores对不上。这意味着你的每个core需要20个超线程，哈哈，目前还没有这么给力的CPU。一般CPU也就2个超线程。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620227396,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388745,"discussion_content":"如果公司业务数据量大的话，实际在生产环境，task上万的比比皆是","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628932560,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1168504,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","nickname":"斯盖丸","note":"","ucode":"B881D14B028F14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372407,"discussion_content":"老师我指的是2000个task，是并行度～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620309793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335340,"user_name":"陌生的心酸","can_delete":false,"product_type":"c1","uid":2829738,"ip_address":"","ucode":"2F221D93403D20","user_header":"https://static001.geekbang.org/account/avatar/00/2b/2d/aa/e33e9edd.jpg","comment_is_top":false,"comment_ctime":1645457490,"is_pvip":false,"replies":[{"id":"122825","content":"可以参考Bennan同学的答案哈：<br><br>1. P2P思路：改成由driver获取到数据分布，然后通知各个executor之间进行拉取，这样可以利用多个executor网络，避免只有driver组装以后再一个一个发送效率过低<br><br>2.当两个需要join的数据集都很大时，使用broadcast join需要将一个很大的数据集进行网络分发多次，已经远超出了shuffle join需要传输的数据","user_name":"作者回复","comment_id":335340,"uid":"1043100","ip_address":"","utype":1,"ctime":1645889783,"user_name_real":"编辑"}],"discussion_count":1,"race_medal":0,"score":"1645457490","product_id":100073401,"comment_content":"1&gt;.当数据量比较大，对数据进行广播后，同时还要接受各个Executor的中间结果上报，状态管理，导致网络繁忙，继而会发生分发任务到Executor产生失败<br><br>2&gt; 两个大表【超过广播变量的阈值参数设置】进行join，数据需要分发多次，效率不佳","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553466,"discussion_content":"可以参考Bennan同学的答案哈：\n\n1. P2P思路：改成由driver获取到数据分布，然后通知各个executor之间进行拉取，这样可以利用多个executor网络，避免只有driver组装以后再一个一个发送效率过低\n\n2.当两个需要join的数据集都很大时，使用broadcast join需要将一个很大的数据集进行网络分发多次，已经远超出了shuffle join需要传输的数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645889783,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320862,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1636533825,"is_pvip":false,"replies":[{"id":"116385","content":"关于Cache（Persist）的部分，建议老弟关注第16讲：内存视角（二）：如何有效避免Cache滥用？这一讲，比较系统、细致地介绍了Cache的使用场景、原则，和一般注意事项，尤其是什么时候该Cache，什么时候不能滥用Cache，老弟可以先看看哈~","user_name":"作者回复","comment_id":320862,"uid":"1043100","ip_address":"","utype":1,"ctime":1636556328,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1636533825","product_id":100073401,"comment_content":"老师您好，在整个应用资源较为紧张，数据量较大的情况下：spark core计算过程中，生成一个较大的RDD , 它被引用一次，但我还是对它persist(disk)，我在用完并且动作算子后，立刻对它进行了释放unpersist，这样操作是否能加快spark 对这个rdd 的清理，加快内存的释放，缓解内存压力？如果是persist(memory and disk)，用完并且在动作算子后立即释放unpersist，是否能缓解内存压力？如果不persist，用完并且在动作算子后立即释放unpersist，是否能缓解内存压力？ 文字有些长，希望没给老师造成困扰，谢谢老师","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530146,"discussion_content":"关于Cache（Persist）的部分，建议老弟关注第16讲：内存视角（二）：如何有效避免Cache滥用？这一讲，比较系统、细致地介绍了Cache的使用场景、原则，和一般注意事项，尤其是什么时候该Cache，什么时候不能滥用Cache，老弟可以先看看哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636556328,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320857,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1636532429,"is_pvip":false,"replies":[{"id":"116381","content":"先说问题2）老弟可以加我微信，搜索“方块K”，或是“rJunior”，我把你拉进去<br><br>再来说1）从两个角度来说：<br><br>首先， driver创建完广播变量、分发给Executors之后，广播变量在Driver端其实是没有引用的，所以随着时间流逝，这些在Driver端创建的广播变量，会慢慢地被GC掉，所以说，不存在长期积压导致内存不断消耗的问题~<br><br>再一个，即便广播变量没有被GC，那么出现你说的那种情况，前提也是你在应用中创建了很多的广播变量。一般来说，广播变量确实可以改善Shuffle join，但是，滥用也不行，适得其反。就是说，我们不能可着一只羊薅羊毛，一个应用中，如果广播变量非常得多，尤其是大数据集上的广播变量，那我们应该考虑，是否应用的实现方式是合理的，有没有其他途径，实现同样的业务逻辑","user_name":"作者回复","comment_id":320857,"uid":"1043100","ip_address":"","utype":1,"ctime":1636556107,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1636532429","product_id":100073401,"comment_content":"1 老师，您好，spark core计算过程中，需要频繁的使用broadCast 操作， 这样累计几次后driver 端内存会很有压力，怎样设置参数或者手动清除之前用来broadCast 的数据？谢谢老师<br>2 老师这个课程的微信交流群怎么添加呢？期待加入学习<br><br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":530145,"discussion_content":"先说问题2）老弟可以加我微信，搜索“方块K”，或是“rJunior”，我把你拉进去\n\n再来说1）从两个角度来说：\n\n首先， driver创建完广播变量、分发给Executors之后，广播变量在Driver端其实是没有引用的，所以随着时间流逝，这些在Driver端创建的广播变量，会慢慢地被GC掉，所以说，不存在长期积压导致内存不断消耗的问题~\n\n再一个，即便广播变量没有被GC，那么出现你说的那种情况，前提也是你在应用中创建了很多的广播变量。一般来说，广播变量确实可以改善Shuffle join，但是，滥用也不行，适得其反。就是说，我们不能可着一只羊薅羊毛，一个应用中，如果广播变量非常得多，尤其是大数据集上的广播变量，那我们应该考虑，是否应用的实现方式是合理的，有没有其他途径，实现同样的业务逻辑","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636556107,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319622,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1635867716,"is_pvip":false,"replies":[{"id":"116038","content":"其实跟inner相比，left只是在形式上不同哈，实际在运行时的执行过程，都是一样的，所以凡是适合inner的优化方法，left、right、semi、anti，都适用的~<br><br>","user_name":"作者回复","comment_id":319622,"uid":"1043100","ip_address":"","utype":1,"ctime":1636094379,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1635867716","product_id":100073401,"comment_content":"老师, 课程里您讲了内连接左右左右两表的连接过程，您能否再讲一下左外连接leftoutjoin 的呢？对shuffle过程不太清晰，谢谢老师","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529691,"discussion_content":"其实跟inner相比，left只是在形式上不同哈，实际在运行时的执行过程，都是一样的，所以凡是适合inner的优化方法，left、right、semi、anti，都适用的~\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636094379,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317504,"user_name":"狗哭","can_delete":false,"product_type":"c1","uid":2725399,"ip_address":"","ucode":"56E48A6FC1298A","user_header":"https://static001.geekbang.org/account/avatar/00/29/96/17/200c21f0.jpg","comment_is_top":false,"comment_ctime":1634817598,"is_pvip":false,"replies":[{"id":"115138","content":"能否广播，取决于b表的存储大小，是否小于广播阈值，也就是：spark.sql.autoBroadcastJoinThreshold。如果小于这个阈值，就会广播，否则就不会。<br><br>如果懒得设阈值，还可以利用 API 强制广播，这里的具体细节，可以参考第13讲哈，就是后面的一讲~ 会详细说，怎么把Shuffle Join，转化为Broadcast Join","user_name":"作者回复","comment_id":317504,"uid":"1043100","ip_address":"","utype":1,"ctime":1634890022,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"1634817598","product_id":100073401,"comment_content":"select * from<br>(select id from table1) a -- 结果很大<br>left join<br>(select id from table2) b -- 结果很小<br>on t1.id = t2.id;<br>老师请教下，这种情况b表会广播吗？如果不会怎么处理能让其广播出去呢","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528840,"discussion_content":"能否广播，取决于b表的存储大小，是否小于广播阈值，也就是：spark.sql.autoBroadcastJoinThreshold。如果小于这个阈值，就会广播，否则就不会。\n\n如果懒得设阈值，还可以利用 API 强制广播，这里的具体细节，可以参考第13讲哈，就是后面的一讲~ 会详细说，怎么把Shuffle Join，转化为Broadcast Join","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634890022,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2725399,"avatar":"https://static001.geekbang.org/account/avatar/00/29/96/17/200c21f0.jpg","nickname":"狗哭","note":"","ucode":"56E48A6FC1298A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":408439,"discussion_content":"老师我们用的是spark2.0，如果我在之前对b表进行了缓存且触发计算，这里才会走broadcastjoin，否则不会走，不知道是什么原因","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635248138,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":310784,"user_name":"zhongmin","can_delete":false,"product_type":"c1","uid":1265621,"ip_address":"","ucode":"C7AB10205A4F04","user_header":"https://static001.geekbang.org/account/avatar/00/13/4f/d5/1666b7d0.jpg","comment_is_top":false,"comment_ctime":1630909620,"is_pvip":true,"replies":[{"id":"112637","content":"一般来说，广播变量封装的，都是immutable data，就是不可变数据（集）。如果封装的变量值有变化，那么就需要创建新的广播变量，之前创建的广播变量所携带的数据内容，是不会自动跟着更新的~","user_name":"作者回复","comment_id":310784,"uid":"1043100","ip_address":"","utype":1,"ctime":1630937414,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1630909620","product_id":100073401,"comment_content":"吴老师，问个问题，在广播分布式变量的时候，如果变量的内容发生改变，是怎么去做变量的同步和更新呢？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526351,"discussion_content":"一般来说，广播变量封装的，都是immutable data，就是不可变数据（集）。如果封装的变量值有变化，那么就需要创建新的广播变量，之前创建的广播变量所携带的数据内容，是不会自动跟着更新的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630937414,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":305117,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1627826813,"is_pvip":false,"replies":[{"id":"110494","content":"好问题～<br><br>1）优先放内存，空间不足放磁盘<br><br>2）一来实际上并没有节省网络开销（不妨仔细想想为什么），再者这样的应用场景太有限了。这种case只适用于那种数据刚读进来就广播，场景太有限。","user_name":"作者回复","comment_id":305117,"uid":"1043100","ip_address":"","utype":1,"ctime":1628042194,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1627826813","product_id":100073401,"comment_content":"老师对于这节有一些问题：<br>(1)executor对于待回传driver端的广播数据集是先存在内存还是落地硬盘呢？<br>(2)由executor来处理分片再回传，可能分片需要进行一定的计算再由driver汇总广播，那么如果是无需计算的原始分片呢，driver可否亲自操刀读取所有原始分片直接汇总再分发呢，感觉这样可以节省网络开销？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524281,"discussion_content":"好问题～\n\n1）优先放内存，空间不足放磁盘\n\n2）一来实际上并没有节省网络开销（不妨仔细想想为什么），再者这样的应用场景太有限了。这种case只适用于那种数据刚读进来就广播，场景太有限。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628042194,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293236,"user_name":"xuchuan","can_delete":false,"product_type":"c1","uid":1954690,"ip_address":"","ucode":"3967199AA078C7","user_header":"https://static001.geekbang.org/account/avatar/00/1d/d3/82/c3e57eb3.jpg","comment_is_top":false,"comment_ctime":1621300126,"is_pvip":false,"replies":[{"id":"106288","content":"这块其实涉及到Join的执行过程，不论查询语句中本身有多少个Join，在运行时，传统的DBMS也好，Spark SQL也好，实际上都是两张表、两张表地做Join。比如说，有个查询，a join b，b又join c，在运行时，a join b会生成临时结果，比如叫xxx，那么后续与c做关联的，不再是b，而是这个xxx。因此，与c的关联是否能做Broadcast Join的转化，取决于xxx的中间大小，而不是b的大小。<br><br>数仓这个是个好问题，通常来说，Spark需要额外的第三方组件来管理数仓元数据，因此，可以考虑用Hive构建数仓的存储层，而用Spark做计算层。","user_name":"作者回复","comment_id":293236,"uid":"1043100","ip_address":"","utype":1,"ctime":1621413985,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1621300126","product_id":100073401,"comment_content":"2.多个大表join应该就没法用广播解决，这个延伸一个问题，以电商为例，即席查询应该还要有数仓吧，这不是spark的主要覆盖范畴。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520126,"discussion_content":"这块其实涉及到Join的执行过程，不论查询语句中本身有多少个Join，在运行时，传统的DBMS也好，Spark SQL也好，实际上都是两张表、两张表地做Join。比如说，有个查询，a join b，b又join c，在运行时，a join b会生成临时结果，比如叫xxx，那么后续与c做关联的，不再是b，而是这个xxx。因此，与c的关联是否能做Broadcast Join的转化，取决于xxx的中间大小，而不是b的大小。\n\n数仓这个是个好问题，通常来说，Spark需要额外的第三方组件来管理数仓元数据，因此，可以考虑用Hive构建数仓的存储层，而用Spark做计算层。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621413985,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292241,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1620736269,"is_pvip":false,"replies":[{"id":"105816","content":"两者都可以广播哈~ 他们都是分布式数据集，你说的对，DataFrame只是多了个Schema而已，分布式数据集都是可以广播的~ <br><br>文中举例多用DataFrame，RDD用得少，原因在于DataFrame API目前是主流，它能享受到Spark SQL带来的种种优化机制。<br><br>但像广播这种优化机制，二者都可以的。","user_name":"作者回复","comment_id":292241,"uid":"1043100","ip_address":"","utype":1,"ctime":1620797809,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1620736269","product_id":100073401,"comment_content":"磊哥，为什么dataframe可以广播，而rdd不能广播。dataframe也不存数据呀，它只是比rdd 多了schema信息。这块不太理解。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519744,"discussion_content":"两者都可以广播哈~ 他们都是分布式数据集，你说的对，DataFrame只是多了个Schema而已，分布式数据集都是可以广播的~ \n\n文中举例多用DataFrame，RDD用得少，原因在于DataFrame API目前是主流，它能享受到Spark SQL带来的种种优化机制。\n\n但像广播这种优化机制，二者都可以的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620797809,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291545,"user_name":"弦断觅知音","can_delete":false,"product_type":"c1","uid":1283398,"ip_address":"","ucode":"6234916B97268C","user_header":"https://static001.geekbang.org/account/avatar/00/13/95/46/1cd5ab5f.jpg","comment_is_top":false,"comment_ctime":1620356283,"is_pvip":false,"replies":[{"id":"105815","content":"这里我原文没有说清楚，是我的锅哈。这里广播变量的创建和使用，其实分两个步骤：<br><br>1. 广播变量创建：第一个是Driver从所有Executors收集数据，构建广播变量，也就是全量数据，这个过程Driver是瓶颈；<br><br>2. 广播变量使用&#47;读取：第二个是各个Executors再去拉取广播变量，这个过程现在只有TorrentBroadcast这一种实现方式。这个时候，Executors之间用类似“P2P”的方式去拉取广播变量的每一个block。优先从本机的其他Executors拉取、其次本Rack的其他机器去拉取，这里有点类似任务本地性，总之对于target block，给定block地址列表，BlockManager会优先选取拉取成本最低的那个去尝试。之所有说是类似“P2P”，原因在于它拉取的形式是P2P，但是这里并没有什么P2P协议，而是利用BlockManager来实现定向拉取。BlockManager从BlockManagerMaster获取目标block地址列表，然后按照刚刚说的顺序，依次尝试拉取Block。","user_name":"作者回复","comment_id":291545,"uid":"1043100","ip_address":"","utype":1,"ctime":1620797634,"user_name_real":"吴磊"}],"discussion_count":3,"race_medal":0,"score":"1620356283","product_id":100073401,"comment_content":"请教老师第一个问题： 用什么方式 改为由driver获取到数据分布，然后通知各个executor之间进行拉取？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519502,"discussion_content":"这里我原文没有说清楚，是我的锅哈。这里广播变量的创建和使用，其实分两个步骤：\n\n1. 广播变量创建：第一个是Driver从所有Executors收集数据，构建广播变量，也就是全量数据，这个过程Driver是瓶颈；\n\n2. 广播变量使用/读取：第二个是各个Executors再去拉取广播变量，这个过程现在只有TorrentBroadcast这一种实现方式。这个时候，Executors之间用类似“P2P”的方式去拉取广播变量的每一个block。优先从本机的其他Executors拉取、其次本Rack的其他机器去拉取，这里有点类似任务本地性，总之对于target block，给定block地址列表，BlockManager会优先选取拉取成本最低的那个去尝试。之所有说是类似“P2P”，原因在于它拉取的形式是P2P，但是这里并没有什么P2P协议，而是利用BlockManager来实现定向拉取。BlockManager从BlockManagerMaster获取目标block地址列表，然后按照刚刚说的顺序，依次尝试拉取Block。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620797634,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388741,"discussion_content":"我觉得老师这块原理没有调查清楚，既然spark目前支持executor之间对拉，那么driver为何还要收集，它不是只要维护一份元数据就行了么，不够合理，道理说不通","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1628932276,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2821687,"avatar":"","nickname":"huangjian01","note":"","ucode":"CF834272C1C64A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536047,"discussion_content":"你得先有数据才能对拉，DF必须被解析才行","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638665674,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290746,"user_name":"小灵芝","can_delete":false,"product_type":"c1","uid":2071355,"ip_address":"","ucode":"6DCB7CF8D5A7F1","user_header":"https://static001.geekbang.org/account/avatar/00/1f/9b/3b/dc3f819f.jpg","comment_is_top":false,"comment_ctime":1619730533,"is_pvip":false,"replies":[{"id":"105416","content":"举个例子哈，比如你的分区数，是默认的200个。那么拿userId = &#39;黄小乙&#39;来说，它的哈希值假设是201，那么对200取模（数学运算：mod、modulo）就是1，那么凡是userId = &#39;黄小乙&#39;的数据记录，就都被存储到#1号数据分区，其他userId以此类推。","user_name":"作者回复","comment_id":290746,"uid":"1043100","ip_address":"","utype":1,"ctime":1619862864,"user_name_real":"吴磊"}],"discussion_count":0,"race_medal":0,"score":"1619730533","product_id":100073401,"comment_content":"“第一步就是对参与关联的左右表分别进行 Shuffle，Shuffle 的分区规则是先对 Join keys 计算哈希值，再把哈希值对分区数取模。由于左右表的分区数是一致的，因此 Shuffle 过后，一定能够保证 userID 相同的交易记录和用户数据坐落在同一个 Executors 内。”<br><br>老师请问什么叫吧哈希值对分区取模？取模是什么意思？<br><br>提前感谢~","like_count":0},{"had_liked":false,"id":288871,"user_name":"七","can_delete":false,"product_type":"c1","uid":1078874,"ip_address":"","ucode":"4C3CCC276D0B68","user_header":"https://static001.geekbang.org/account/avatar/00/10/76/5a/4393d800.jpg","comment_is_top":false,"comment_ctime":1618740899,"is_pvip":false,"replies":[{"id":"104855","content":"Data Source的并行度，取决于分布式文件本身，比如HDFS文件系统中的源数据，它本身的并行度，就是Spark构建的HadoopRDD的初始并行度。比如一个文件，在HDFS有1204个分片，那么有它构建的HadoopRDD的初始并行度，就是1024。<br><br>这一点，通过Spark UI也可以看出来哈~ 比如你在执行数据源读取的时候，发现Spark UI上显示：“238&#47;1024”，这意味着你的源数据有1024个分片，Spark总共需要1024个task来完成数据读取，其中已经完成了238个任务，也就是成功读取了238个分区数据。","user_name":"作者回复","comment_id":288871,"uid":"1043100","ip_address":"","utype":1,"ctime":1618759522,"user_name_real":"吴磊"}],"discussion_count":1,"race_medal":0,"score":"1618740899","product_id":100073401,"comment_content":"老师，问一下 从hdfs加载parquet文件时，例如spark.read.parquet(&quot;xx&quot;)，是怎么确定分区数的呢？阅读源码DataFrameReader没有找到相关配置，希望老师给指条路，解答一下。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518756,"discussion_content":"Data Source的并行度，取决于分布式文件本身，比如HDFS文件系统中的源数据，它本身的并行度，就是Spark构建的HadoopRDD的初始并行度。比如一个文件，在HDFS有1204个分片，那么有它构建的HadoopRDD的初始并行度，就是1024。\n\n这一点，通过Spark UI也可以看出来哈~ 比如你在执行数据源读取的时候，发现Spark UI上显示：“238/1024”，这意味着你的源数据有1024个分片，Spark总共需要1024个task来完成数据读取，其中已经完成了238个任务，也就是成功读取了238个分区数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618759522,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288380,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1618449673,"is_pvip":false,"replies":[{"id":"104696","content":"这些参数之间的关系和设置，其实我们在配置项那两讲有过详细的展开哈~ 关于广播阈值的问题，就我的经验，平时工作中，2GB左右，是个不错的选择。当然，所有这些参数的设置，都没有绝对值，否则咱们就不需要调优了，大家都用一套绝对值就好了。就是因为大家的场景、部署、情况各自不同，所以才需要调优，而调优的原则，就可以follow配置项那两讲去操作。<br><br>比如你现在让我给出一个最佳组合，其实这个命题是不成立的，因为你需要给定一个场景，我才能具体说一个什么样的作业，在你的部署下，最佳的配置组合是什么。所以，调优，是以场景为基础的哈，也就是咱们只能就事论事，但这不代表我们就是瞎蒙，相反，我们设置不同的参数组合是基于一些基本原则的，而这些原则，就是配置项那两讲的内容所在~","user_name":"作者回复","comment_id":288380,"uid":"1043100","ip_address":"","utype":1,"ctime":1618485689,"user_name_real":"吴磊"}],"discussion_count":2,"race_medal":0,"score":"1618449673","product_id":100073401,"comment_content":"老师，您好，问题一和问题二有没有一种矛盾在里面，问题一是数据量大，如何使用广播，问题二我知道的是，当join的表数据量都很大时就不适合广播了，一是因为网络开销，二是因为executor内存不够大不足以存储大数据量，还有老师，能不能说一下广播的阈值多少合适，这个和其他的参数也有关系，比如整体的参数如何设置，能不能讲一下，executor数量，core的数量，分区大小以及内存大小设置","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518609,"discussion_content":"这些参数之间的关系和设置，其实我们在配置项那两讲有过详细的展开哈~ 关于广播阈值的问题，就我的经验，平时工作中，2GB左右，是个不错的选择。当然，所有这些参数的设置，都没有绝对值，否则咱们就不需要调优了，大家都用一套绝对值就好了。就是因为大家的场景、部署、情况各自不同，所以才需要调优，而调优的原则，就可以follow配置项那两讲去操作。\n\n比如你现在让我给出一个最佳组合，其实这个命题是不成立的，因为你需要给定一个场景，我才能具体说一个什么样的作业，在你的部署下，最佳的配置组合是什么。所以，调优，是以场景为基础的哈，也就是咱们只能就事论事，但这不代表我们就是瞎蒙，相反，我们设置不同的参数组合是基于一些基本原则的，而这些原则，就是配置项那两讲的内容所在~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618485689,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2172635,"avatar":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","nickname":"辰","note":"","ucode":"2E898EAC5AA141","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":368001,"discussion_content":"这个我理解参数没有唯一，因为你说的广播阈值2g是不错的选择，就比如对应的运行内存应该多少呢，因为我广播用的500兆左右，再往上加容易报错，类似于内存溢出之类的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618536072,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287493,"user_name":"Jack","can_delete":false,"product_type":"c1","uid":1297112,"ip_address":"","ucode":"F3863DAEF449D5","user_header":"https://static001.geekbang.org/account/avatar/00/13/ca/d8/b109ed85.jpg","comment_is_top":false,"comment_ctime":1617977118,"is_pvip":true,"replies":[{"id":"104436","content":"思考的很深入，先赞一个～ 我的理解是这样的，shuffle分发的，是部分数据。广播分发的，是全量数据。当然，像你说的，广播的粒度大，是executor level。shuffle粒度小，是task level。如果全量数据很大，即使能塞进广播变量，即便是executor粒度，我觉得它的开销，也远大于它的收益，甚至还有driver端oom的风险，所以这种情况，还不如采用shuffle join。 当然，这么说比较笼统，并没有精确的开销和收益的计算。这里其实更多的，是想强调广播不是银弹，不要滥用广播。","user_name":"作者回复","comment_id":287493,"uid":"1043100","ip_address":"","utype":1,"ctime":1618056553,"user_name_real":"吴磊"}],"discussion_count":4,"race_medal":0,"score":"1617977118","product_id":100073401,"comment_content":"老师，第2题@Bennan的回答有疑惑。为什么当两个需要join的数据集都很大时，broadcast join会超出shuffle join需要传输的数据。<br>假设有A，B两表，各自大小都是100G，有4个executor，每个executor有4个task对应4个分区，总共16个task。，把shuffle的过程用二分图刻画，两边都各自有16个顶点，边有16*16条。<br>对于shuffle join，假设shuffle前后分区个数不变，每个task需要去其他12个task（有3个task和自己在同一个executor上）拉取数据，把shuffle的过程看做是二分图，两边都各自有16个顶点，边有16*16条（task到task）。<br>对于broadcast join，driver从各个分区获取数据，有16条从分区到driver的边（相当于16条task到task的边），然后driver广播给各个executor，总共有4条driver到executor的边。<br>broadcast join的粒度是executor，shuffle的粒度是task，感觉还是broadcast join的数据少一点。因为同一份数据，即使两个task在同一个executor，对于shuffle，还是会在同一个executor上有两份相同的数据，而broadcast，在一个executor上只有一份数据。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518324,"discussion_content":"思考的很深入，先赞一个～ 我的理解是这样的，shuffle分发的，是部分数据。广播分发的，是全量数据。当然，像你说的，广播的粒度大，是executor level。shuffle粒度小，是task level。如果全量数据很大，即使能塞进广播变量，即便是executor粒度，我觉得它的开销，也远大于它的收益，甚至还有driver端oom的风险，所以这种情况，还不如采用shuffle join。 当然，这么说比较笼统，并没有精确的开销和收益的计算。这里其实更多的，是想强调广播不是银弹，不要滥用广播。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618056553,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366418,"discussion_content":"没有两份数据之说吧，对于 shuffle join 不同的 task 拉取的就是不同的 map 分区。所以 reduce 端肯定只有一份吧。而 broadcast 是一个 executor 一份。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618056898,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366416,"discussion_content":"我同意这种说法\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618056646,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1265135,"avatar":"","nickname":"licl1008","note":"","ucode":"6467D6DB97A00A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366362,"discussion_content":"shuffle只要传一部分 而broadcast是传全量吧 这是我个人理解","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618043114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287482,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1617969637,"is_pvip":false,"replies":[{"id":"104434","content":"是的，以小博大","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618055992,"ip_address":"","comment_id":287482,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617969637","product_id":100073401,"comment_content":"原来小表和大表join是节省了大表的shuffle，不然大表只能根据join的列在所有机器上重新分布一遍，现在懂了","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518320,"discussion_content":"是的，以小博大","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618055992,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287472,"user_name":"🚤","can_delete":false,"product_type":"c1","uid":1460463,"ip_address":"","ucode":"95D7F773789B74","user_header":"https://static001.geekbang.org/account/avatar/00/16/48/ef/4750cb14.jpg","comment_is_top":false,"comment_ctime":1617965268,"is_pvip":false,"replies":[{"id":"104435","content":"code其实有，但是就我理解，出于各种原因，并没有生效，具体细节可以参考给Jack的回复哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618056087,"ip_address":"","comment_id":287472,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617965268","product_id":100073401,"comment_content":"Executor端的Broadcast这个功能，目前的spark版本是不是没有这个功能呀？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518318,"discussion_content":"code其实有，但是就我理解，出于各种原因，并没有生效，具体细节可以参考给Jack的回复哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618056087,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}