{"id":818151,"title":"08｜前端程序员应该懂的AI数学知识","content":"<p>你好，我是柳博文，欢迎和我一起学习前端工程师的AI实战课。</p><p>今天的内容是AI数学知识的选学篇，相信通过<a href=\"https://time.geekbang.org/column/article/815865\">数学知识应用篇</a>的学习后，你对AI所需要的数学知识如何应用已经有了不错的理解。</p><p>今天这节课，专为想深入学习AI数学知识的前端工程师量身定制，通过一系列精选的数学概念，我们会搭建起通往AI世界的桥梁。我们也将揭示其背后的数学原理，让你在不偏离前端开发舒适区的同时，逐步深化对AI技术的认识。<strong>这不仅是一次知识的拓展，更是技能升级的契机，让你在未来能够更加自信地在项目中融入AI元素，推动前端技术与智能科技的融合创新。</strong></p><p>现在，就让我们揭开那些支撑AI奇迹的数学面纱，开启你的AI与计算机视觉融合之旅吧。</p><h2>知识点概览</h2><p>首先是知识概览。在实践环境，我们用到的技术是目标检测。虽然对于前端来说，日常工作可能不会直接深入到算法实现细节，但了解一些基本的数学知识对于理解算法原理、优化现有库的使用或参与相关项目的开发都是非常有帮助的。</p><p>以下是几个关键数学领域的概述，它们与AI目标检测算法紧密相关，你可以参考后面的思维导图。</p><p><img src=\"https://static001.geekbang.org/resource/image/7c/8c/7cf1f3f22479ef55cb944e0841d8618c.jpg?wh=3452x2044\" alt=\"\"></p><p>掌握上述数学知识不仅能帮助你深入理解目标检测算法的原理，还能在实践中更加灵活地调整和优化模型。同时，利用现有的库和框架（如TensorFlow.js、PyTorch等），即使不深入所有数学细节，也能快速上手并实现基本的目标检测任务。</p><!-- [[[read_end]]] --><h2>科学计算库介绍</h2><p>现在我们明确了和目标检测紧密相关的知识点都有什么。为了让你更深入地理解它们，最有效的方式就是在本地运行一些简单示例。这时就需要用到一些科学计算库。</p><p>在JavaScript领域，有多个科学计算库可供我们选择，来进行数学和矩阵运算，特别适合进行数据处理、统计分析、机器学习等任务。</p><p>下面我们就来了解一下三个最为流行的科学计算库，每个库我都提供了一个矩阵乘法的运算实例，你可以通过 npm 安装对应的包，并复制运行代码查看运行结果。</p><h3>NumJS</h3><p>NumJS是一个轻量级的JavaScript库，提供了一系列类似MATLAB的数学函数，专为大规模数值计算设计的。它支持多维数组和矩阵运算，适用于浏览器和Node.js环境。</p><pre><code class=\"language-javascript\">const nj = require('numjs');\n&nbsp;\nlet A = nj.array([[1, 2], [3, 4]]);\nlet B = nj.array([[5, 6], [7, 8]]);\n&nbsp;\nlet C = nj.dot(A, B); // 矩阵乘法\nconsole.log(C.tolist()); // 输出矩阵乘法的结果\n</code></pre><p></p><h3>TensorFlow.js</h3><p>TensorFlow.js是Google开发的一个JavaScript库，它使开发者能够在浏览器和Node.js中直接运行TensorFlow模型。它不仅支持强大的矩阵和张量运算，还集成了机器学习模型构建、训练和推理功能，非常适合深度学习应用。</p><pre><code class=\"language-javascript\">import * as tf from '@tensorflow/tfjs';\n&nbsp;\nasync function run() {\n&nbsp; const a = tf.tensor2d([[1, 2], [3, 4]]);\n&nbsp; const b = tf.tensor2d([[5, 6], [7, 8]]);\n&nbsp;\n&nbsp; const c = await a.matMul(b);\n&nbsp; c.print(); // 打印矩阵乘法的结果\n}\n&nbsp;\nrun();\n</code></pre><p></p><h3>mathJS</h3><p>mathJS是一个全面的数学库，提供了从基础数学到高级数学的各种功能，包括代数、几何、概率、统计等。它支持多种数据类型，如数字、大数、分数、复数、矩阵等，非常适合进行精确的科学计算。</p><pre><code class=\"language-javascript\">const math = require('mathjs');\n&nbsp;\nconst A = math.matrix([[1, 2], [3, 4]]);\nconst B = math.matrix([[5, 6], [7, 8]]);\n&nbsp;\nconst C = math.multiply(A, B); // 矩阵乘法\nconsole.log(math.format(C)); // 格式化输出矩阵乘法的结果\n</code></pre><p></p><h3>科学计算库使用小总结</h3><p>那么这些数据计算库都适合用在什么样的场景里呢？</p><ul>\n<li>NumJS适合需要MATLAB风格API的开发者，侧重于数值计算和小型至中型规模的数据处理项目。</li>\n<li>TensorFlow.js特别适用于深度学习和大型张量运算，适合机器学习项目。</li>\n<li>mathJS提供了最广泛的数学功能，支持多种数据类型，适合需要高度精确和多样化的数学运算场景。</li>\n</ul><p>这节课我们选择<strong> mathJS</strong> 作为示例实现的库。后面所有示例代码的环境都需要提前安装并导入mathJS包。命令如下：</p><pre><code class=\"language-javascript\">const math = require('mathjs');\n</code></pre><h2>深入细节知识</h2><p>接下来我们深入知识细节，首先是向量与矩阵的运算。</p><h3>向量与矩阵及其运算</h3><p>首先是线性代数中的向量与矩阵，比较重要的分别是向量运算与矩阵运算。</p><h4>向量</h4><p>向量是数学中的一个基本概念，尤其在物理学、工程学和计算机科学中应用广泛。向量可以视为既有大小又有方向的数量，通常用箭头表示。在数学上，向量可以看作是一组有序数的集合，称为分量。</p><p><strong>向量的基本概念</strong></p><p>首先来看向量的定义，一个n维向量v可以表示为(v1, v2, …, vn)，其中vi表示向量在第i个维度上的分量。</p><p><strong>向量加法</strong></p><p>两个n维向量v和w的和v + w是另一个n维向量，其各分量为对应分量之和。</p><p>$$v+w=(v1+w1,v2+w2,…,vn+wn)$$<br>\n使用mathJS进行实例演示如下：</p><pre><code class=\"language-javascript\">const v = math.vector([3, 1, 4]);\nconst w = math.vector([1, 5, 9]);\nconst sum = math.add(v, w);\nconsole.log(sum); // 输出: [4, 6, 13]\n</code></pre><p><strong>向量减法</strong></p><p>向量v减去向量w得到的新向量v - w，其分量为对应分量之差。</p><p>$$v−w=(v1−w1,v2−w2,…,vn−wn)$$</p><p>使用mathJS进行实例演示如下：</p><pre><code class=\"language-javascript\">const diff = math.subtract(v, w);\nconsole.log(diff); // 输出: [2, -4, -5]\n</code></pre><p><strong>标量乘法</strong><br>\n向量v乘以标量k得到的新向量kv，其各分量为原分量乘以k。</p><p>$$kv=(kv1,kv2,…,kvn)$$</p><pre><code class=\"language-javascript\">const scalar = 2;\nconst scaledV = math.multiply(scalar, v);\nconsole.log(scaledV); // 输出: [6, 2, 8]\n</code></pre><p><strong>点积（内积）</strong><br>\n两个n维向量v和w的点积定义为它们对应分量乘积的和。</p><p>$$v⋅w=v1∗w1+v2∗w2+…+vn∗wn$$</p><pre><code class=\"language-javascript\">const dotProduct = math.dot(v, w);\nconsole.log(dotProduct); // 输出: 32\n</code></pre><p><strong>叉积（外积）</strong></p><p>仅限于三维空间中的向量，结果是一个新的向量，其方向垂直于v和w所在平面，大小为v和w构成的平行四边形的面积。</p><p>$$\\upsilon = （\\upsilon1, \\upsilon2, \\upsilon3）,          \\omega = (\\omega1, \\omega2, \\omega3)$$<br>\n$$\\upsilon \\times \\omega = (\\upsilon2\\omega3 - \\upsilon3\\omega2, \\upsilon3\\omega1 - \\upsilon1\\omega3, \\upsilon1\\omega2 - \\upsilon2\\omega1)$$</p><pre><code class=\"language-javascript\">const dotProduct = math.cross(v, w);\nconsole.log(dotProduct); // 输出: [−11,−23,14]\n</code></pre><h4>矩阵</h4><p>矩阵是数学中一种重要的数据结构，特别是在线性代数中扮演着核心角色。矩阵由行和列组成的数字（或其他数学对象）的矩形排列构成，通常用于表示线性变换、解决线性方程组、表示向量和进行数据分析等。</p><p><strong>矩阵的基本概念</strong></p><p><strong>定义</strong></p><p>一个m×n矩阵是一个由m行n列元素组成的矩形数组，记作A = [aij]m×n，其中aij代表矩阵第i行第j列的元素。</p><p><strong>加法与减法</strong></p><p>若A和B是同型矩阵（即行数和列数相同），则它们的和C和差D定义为：</p><p>$$C=A+B=[cij]=[aij+bij]$$<br>\n$$D=A−B=[dij]=[aij−bij]$$</p><pre><code class=\"language-javascript\">const A = math.matrix([[1, 2], [3, 4]]);\nconst B = math.matrix([[5, 6], [7, 8]]);\nconst sum = math.add(A, B);\nconsole.log(sum); // 输出: [[6, 8], [10, 12]]\nconst diff = math.subtract(A, B);\nconsole.log(diff); // 输出: [[-4, -4], [-4, -4]]\n</code></pre><p><strong>标量乘法</strong></p><p>一个矩阵A乘以一个标量k得到的新矩阵，其每个元素都是原矩阵对应元素乘以k。</p><p>$$kA=[kaij]$$</p><pre><code class=\"language-javascript\">const C = math.matrix([[2, 0], [0, 3]]);\nconst product = math.multiply(A, C);\nconsole.log(product); // 输出: [[2, 6], [6, 12]]\n</code></pre><p><strong>矩阵乘法</strong></p><p>如果矩阵A是m×n矩阵，B是n×p矩阵，则它们的乘积C是一个m×p矩阵，其元素cij通过下述公式计算：</p><p>$$cij=k=1∑n​aik⋅bkj$$<br>\n<strong>矩阵转置</strong></p><p>矩阵A的转置AT是一个n×m矩阵，其中元素的行索引与原矩阵的列索引互换，列索引与行索引互换。</p><p>$$AT=[aji]$$</p><pre><code class=\"language-javascript\">const transposeA = math.transpose(A);\nconsole.log(transposeA); // 输出: [[1, 3], [2, 4]]\n</code></pre><p><strong>矩阵的逆</strong></p><p>仅当矩阵A是方阵（即m=n）且行列式不为零时，A存在逆矩阵A⁻¹，满足AA⁻¹ = A⁻¹A = I，I为单位矩阵。</p><pre><code class=\"language-javascript\">const squareMatrix = math.matrix([[4, 1], [2, 3]]);\ntry {\n&nbsp;&nbsp; &nbsp;const inverse = math.inv(squareMatrix);\n&nbsp;&nbsp; &nbsp;console.log(inverse); // 输出: [[0.6, -0.2], [-0.4, 0.8]]\n} catch (e) {\n&nbsp;&nbsp; &nbsp;console.error(\"The matrix is not invertible.\");\n}\n</code></pre><h3>导数和链式法则</h3><h4>导数理论</h4><p>导数是微积分中的基本概念之一，表示函数在某一点处的瞬时变化率。如果函数y=f(x)在点x=a处可导，则导数定义为极限形式：</p><p>$$f′(a)=\\lim_{h \\rightarrow 0}{\\frac{f(a+h)-f(a)}{h}}$$</p><p>导数也可以理解为函数图像在某点处的切线斜率。</p><h4>链式法则</h4><p>链式法则是微积分中的一个基本规则，用于计算复合函数的导数。如果y=f(u)且u=g(x)，则y=f(g(x))的导数按照链式法则计算为：</p><p>$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$</p><p>即外函数对中间变量的导数乘以中间变量对自变量的导数。</p><h4>使用mathJS计算导数和链式法则</h4><p>虽然mathJS主要关注于矩阵和向量运算，它也提供了一些基本的微积分功能，但直接计算链式法则的实例可能需要一些间接方法，因为库本身没有直接提供链式法则的函数。不过，我们可以通过计算两个简单函数的导数来展示如何使用mathJS进行微分计算。</p><p><strong>计算简单函数的导数</strong></p><p>假设要计算函数f(x) = x²的导数。</p><pre><code class=\"language-javascript\">const f = 'x^2'; // 定义函数表达式\nconst derivativeF = math.derivative(f, 'x'); // 计算导数，'x'是自变量\nconsole.log(derivativeF.toString()); // 输出: 2*x\n</code></pre><p><strong>间接展示链式法则</strong><br>\n虽然直接应用链式法则不易直接通过math.derivative实现，但可以通过手动计算两个步骤来间接展示：</p><ul>\n<li>计算g(x)=x+1的导数</li>\n<li>计算f(u)=u²的导数，其中u是g(x)</li>\n<li>然后根据链式法则手动相乘</li>\n</ul><pre><code class=\"language-javascript\">const g = 'x + 1';\nconst derivativeG = math.derivative(g, 'x');\nconsole.log(derivativeG.toString()); // 输出: 1\n\nconst fOfU = 'u^2';\nconst dfDu = math.derivative(fOfU, 'u'); // u代替了原本的x\nconsole.log(dfDu.toString()); // 输出: 2*u\n\n// 按照链式法则手动计算f(g(x))的导数，这里简化展示，实际上`mathJS`没有直接的链式法则应用函数\nconst chainDerivativeResult = derivativeG * dfDu.substitute('u', g);\nconsole.log(chainDerivativeResult.toString()); // 替换u为g(x)后计算结果\n</code></pre><p>请注意，最后一个步骤展示了如何间接使用mathJS进行链式法则的思想应用，但实际操作中，对于更复杂的函数，可能需要更精细的处理逻辑，或使用其他专门的数学软件或库来直接实现链式法则的计算。</p><h3>概率论</h3><p>概率论是研究随机事件发生的可能性的数学分支。它为统计学、机器学习、风险评估等多个领域提供理论基础。概率论中，概率是一个介于0和1之间的数，表示某个事件发生的可能性，其中0表示不可能发生，1表示必然发生。</p><h4>高斯分布（正态分布）</h4><p>高斯分布（也称正态分布）是一种连续概率分布，广泛应用于自然界和社会科学中，描述了大量独立同分布变量之和的分布趋势。它由均值（μ）和标准差（σ）决定，图形呈钟形曲线。</p><p>$$f(x|\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}$$</p><p>其中，𝑥 是随机变量，𝜇是均值，𝜎2是方差。使用mathJS实现实例如下所示。</p><pre><code class=\"language-javascript\">// 定义均值（μ）和标准差（σ）\nconst mean = 0;\nconst stdDeviation = 1;\n\n// 计算x=1时的高斯分布概率密度函数值\nconst x = 1;\nconst gaussianPDF = (x, mean, stdDeviation) =&gt; {\n&nbsp;&nbsp; &nbsp;const exponent = -(Math.pow(x - mean, 2) / (2 * Math.pow(stdDeviation, 2)));\n&nbsp;&nbsp; &nbsp;const denominator = stdDeviation * Math.sqrt(2 * Math.PI);\n&nbsp;&nbsp; &nbsp;return Math.pow(Math.E, exponent) / denominator;\n};\n\nconsole.log(`在均值${mean}，标准差${stdDeviation}下，x=1的高斯分布概率密度为: ${gaussianPDF(x, mean, stdDeviation).toFixed(4)}`);\n</code></pre><h4>伯努利分布</h4><p>伯努利分布是离散概率分布，用于描述只有两种可能结果的独立实验，通常指成功与失败、是与否等二元结果。它是二项分布的特例，当试验次数n=1时即为伯努利分布。</p><p>$$P(X=k∣p)=pk(1−p)1−k$$</p><p>其中，𝑋 表示随机变量，取值为0或1；𝑘 是实际观察到的结果（0或1）；𝑝 是成功的概率，1−p则是失败的概率。使用mathJS实现实例如下所示。</p><pre><code class=\"language-javascript\">// 定义成功概率p\nconst p = 0.7;\n\n// 计算成功（k=1）和失败（k=0）的概率\nconst successProb = Math.pow(p, 1) * Math.pow(1 - p, 0);\nconst failureProb = Math.pow(p, 0) * Math.pow(1 - p, 1);\n\nconsole.log(`成功概率（k=1）: ${successProb.toFixed(4)}`);\nconsole.log(`失败概率（k=0）: ${failureProb.toFixed(4)}`);\n</code></pre><h3>几何视觉</h3><h4>坐标变换与投影</h4><p>在目标检测中，坐标变换与投影技术用于改变图像中对象的位置、尺寸或视角，以便于识别和分析。这主要包括两类变换。</p><p>一类是仿射变换，也就是保持直线性且平行性的变换，包括平移、缩放、旋转和剪切。仿射变换可以用一个3x3的矩阵来表示，其中前两行两列代表线性变换，最后一行为平移向量。</p><p>另一类是透视变换，也就是模拟三维空间到二维图像的投影，改变图像的视角。这类变换常用于修正图像的透视失真，或实现图像的鸟瞰图效果。透视变换需要一个3x3的单应性矩阵。</p><h4>图像处理基础</h4><p>图像处理是一门十分庞大的学科，这里列举一些我们这门课程会涉及到的点。</p><ul>\n<li>\n<p>像素：图像的基本单元，每个像素包含颜色信息，通常用RGB值表示。</p>\n</li>\n<li>\n<p>灰度图：将彩色图像转换为单一亮度值表示，每个像素只有一个强度值。</p>\n</li>\n<li>\n<p>二值图：进一步简化，每个像素非黑即白，常用于边缘检测和形状识别。</p>\n</li>\n<li>\n<p>滤波器：用于图像处理的数学运算。常见的滤波器有两类。高通滤波器可以增强图像的细节，如边缘和噪声，常用于锐化。而低通滤波器可以平滑图像，减少噪声，如均值滤波器。</p>\n</li>\n</ul><h3>优化理论</h3><h4>梯度下降理论原理</h4><p>梯度下降是一种迭代优化算法，主要用于寻找目标函数（如损失函数）最小值的问题。其核心思想是沿着目标函数梯度（函数在某一点的最陡峭方向）的反方向逐步调整参数，直到找到函数值最小的点。</p><p><strong>基本步骤：</strong></p><ul>\n<li>初始化参数：给模型参数一个初始值。</li>\n<li>计算梯度：对当前参数下的损失函数计算梯度，梯度指向损失函数增大的最快方向。</li>\n<li>更新参数：按照梯度的负方向，以一定的学习率（步长）调整参数。</li>\n<li>重复迭代：重复步骤2和3，直到满足停止条件（如梯度接近0，迭代次数达到预设值，或损失函数变化极小）。</li>\n</ul><h4>批量梯度下降（BGD）与随机梯度下降（SGD）的原理与区别</h4><p><strong>批量梯度下降（Batch Gradient Descent）</strong></p><p><strong>原理</strong>：BGD在每次迭代时，会计算整个训练数据集上的损失函数关于模型参数的梯度，然后根据这个梯度更新参数。这意味着每一步更新都是全局最优的方向，但计算成本高，尤其是数据集较大时。</p><p><strong>特点</strong>：收敛稳定，但计算效率低，不适合大规模数据集。</p><p><strong>随机梯度下降（Stochastic Gradient Descent）</strong></p><p><strong>原理</strong>：SGD在每次迭代时，仅随机选取一个样本来计算梯度并更新参数，因此每一次更新都朝着减小该样本损失的方向。虽然每一步可能不是全局最优方向，但多次迭代后，整体趋向于最小化总体损失。</p><p><strong>特点</strong>：计算速度快，适合大数据集，但收敛过程可能会更加动荡。</p><p><strong>使用mathJS实现BGD与SGD示例</strong></p><p>假设我们要最小化一个简单的线性函数的损失：$[ J(\\theta) = \\frac{1}{2N} \\sum_{i=1}^{N} (h_\\theta(x_i) - y_i)^2 ]$，其中$hθ​(x)=θ0​+θ1​x$且我们有简单的数据集。</p><p>定义数据集、损失函数、梯度计算等函数。</p><pre><code class=\"language-javascript\">// 简单数据集\nconst dataset = [\n&nbsp;&nbsp;&nbsp; {x: 1, y: 2},\n&nbsp;&nbsp;&nbsp; {x: 2, y: 3},\n&nbsp;&nbsp;&nbsp; {x: 3, y: 4},\n&nbsp;&nbsp; &nbsp;// 更多数据...\n];\n\n// 初始化参数\nlet theta0 = 0;\nlet theta1 = 0;\nconst learningRate = 0.01;\nconst numIterations = 1000;\n\n// 损失函数\nfunction loss(theta0, theta1) {\n&nbsp;&nbsp; &nbsp;let totalLoss = 0;\n&nbsp;&nbsp;&nbsp; dataset.forEach(data =&gt; {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; totalLoss += 0.5 * Math.pow((theta0 + theta1 * data.x - data.y), 2);\n&nbsp;&nbsp;&nbsp; });\n&nbsp;&nbsp; &nbsp;return totalLoss / dataset.length;\n}\n\n// 梯度计算\nfunction gradient(theta0, theta1) {\n&nbsp;&nbsp; &nbsp;let dTheta0 = 0;\n&nbsp;&nbsp; &nbsp;let dTheta1 = 0;\n&nbsp;&nbsp;&nbsp; dataset.forEach(data =&gt; {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dTheta0 += (theta0 + theta1 * data.x - data.y);\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dTheta1 += (theta0 + theta1 * data.x - data.y) * data.x;\n&nbsp;&nbsp;&nbsp; });\n&nbsp;&nbsp; &nbsp;return {theta0: dTheta0 / dataset.length, theta1: dTheta1 / dataset.length};\n}\n</code></pre><p><strong>BGD实现</strong></p><pre><code class=\"language-javascript\">// 批量梯度下降\nfor (let i = 0; i &lt; numIterations; i++) {\n&nbsp;&nbsp; &nbsp;const gradients = gradient(theta0, theta1);\n&nbsp;&nbsp;&nbsp; theta0 -= learningRate * gradients.theta0;\n&nbsp;&nbsp;&nbsp; theta1 -= learningRate * gradients.theta1;\n&nbsp;&nbsp; &nbsp;// 可以在此打印每轮迭代的损失，观察收敛情况\n}\nconsole.log(\"BGD: theta0 =\", theta0, \", theta1 =\", theta1);\n</code></pre><p><strong>SGD实现</strong></p><pre><code class=\"language-javascript\">// 随机梯度下降\nfor (let i = 0; i &lt; numIterations; i++) {\n&nbsp;&nbsp; &nbsp;// 随机选择一个样本\n&nbsp;&nbsp; &nbsp;const randomIndex = math.randomInt(0, dataset.length);\n&nbsp;&nbsp; &nbsp;const data = dataset[randomIndex];\n&nbsp;&nbsp; &nbsp;const gradientSingle = {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;theta0: (theta0 + theta1 * data.x - data.y),\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;theta1: (theta0 + theta1 * data.x - data.y) * data.x\n&nbsp;&nbsp;&nbsp; };\n&nbsp;&nbsp;&nbsp; theta0 -= learningRate * gradientSingle.theta0;\n&nbsp;&nbsp;&nbsp; theta1 -= learningRate * gradientSingle.theta1;\n&nbsp;&nbsp; &nbsp;// 同样，可以打印迭代信息\n}\nconsole.log(\"SGD: theta0 =\", theta0, \", theta1 =\", theta1);\n</code></pre><p>这个示例简化了梯度下降的过程，实际应用中还需考虑更多因素，如学习率调整策略、梯度爆炸或消失的处理等。但对于前端工程师来说，理解这些基本概念和实现过程，有助于深入理解机器学习模型训练背后的数学原理。</p><h2>总结</h2><p>这节课对学习AI所需要的基本数学知识做了深入的讲解，现在我们一起做个总结吧。</p><p>前端工程师学习目标检测算法，关键在于掌握核心数学概念。理解这些原理，能使工程师更好地运用TensorFlow.js、PyTorch等工具，灵活优化目标检测任务，无需深入全部数学细节。</p><p>为了让你更好的理解知识点细节，在对比了多个基于JavaScript的科学计算库以后，我选了mathJS 作为实操演示的工具。mathJS提供了最广泛的数学功能，支持多种数据类型，适合需要高度精确和多样化的数学运算场景。</p><p>深入细节知识，向量与矩阵操作是基础（重点要理解向量加减、点积、标量乘及矩阵运算）；微积分里面需要掌握导数和链式法则，它们用于优化模型；概率论里主要了解用于评估模型不确定性的高斯、伯努利分布；优化理论中要掌握梯度下降，特别是批量和随机梯度下降，其作用是优化模型训练，使工程师能更高效地应用和调整目标检测技术。</p><p>总之，掌握上面这些数学理论知识，能让你更加深入地理解计算机视觉和目标检测，为我们后续的学习打好理论基础。</p><h2>思考题</h2><p>请问矩阵运算以及链式求导在模型训练过程中具体作用是什么？</p><p>欢迎你在留言区发表你的看法。如果这节课对你有帮助，也推荐你分享给更多的同事、朋友。</p>","comments":[]}