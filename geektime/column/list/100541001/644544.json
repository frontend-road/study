{"id":644544,"title":"07｜文本聚类与摘要，让AI帮你做个总结","content":"<p>你好，我是徐文浩。</p><p>上一讲里，我们用上了最新的ChatGPT的API，注册好了HuggingFace的账号，也把我们的聊天机器人部署了出去。希望通过这个过程，你对实际的应用开发过程已经有了充足的体验。那么这一讲里，我们会回到OpenAI的各个接口能够提供的能力。我们分别看看怎么通过Embedding进行文本聚类，怎么利用提示语（Prompt）做文本的总结。</p><h2>基于Embedding向量进行文本聚类</h2><p>我先给不太了解技术的同学简单科普一下什么叫做文本聚类，文本聚类就是把很多没有标注过的文本，根据它们之间的相似度，自动地分成几类。基于GPT系列的模型进行文本聚类很简单，因为我们可以通过Embedding把文本变成一段向量。而对于向量我们自然可以用一些简单的聚类算法，比如我们采用最简单的K-Means算法就可以了。</p><p>这一次，我们选用的数据集，是很多老的机器学习教程里常用的20 newsgroups数据集，也就是一个带了标注分好类的英文新闻组的数据集。这个数据集，其实不是最自然的自然语言，里面的数据是经过了预处理的，比如去除了标点符号、停用词等等。我们正好可以拿来看看，面对这样其实不太“自然语言”的数据，OpenAI的GPT系列模型处理的效果怎么样。</p><!-- [[[read_end]]] --><p>首先，我们先通过scikit-learn这个Python库来拿到数据，数据集就内置在这个库里面。scikit-learn也是非常常用的一个机器学习库，我们直接把数据下载下来，存储成CSV文件。对应的代码在下面可以看到。</p><pre><code class=\"language-python\">from sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\n\ndef twenty_newsgroup_to_csv():\n    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n\n    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n    df.columns = ['text', 'target']\n\n    targets = pd.DataFrame( newsgroups_train.target_names, columns=['title'])\n\n    out = pd.merge(df, targets, left_on='target', right_index=True)\n    out.to_csv('20_newsgroup.csv', index=False)\n    \ntwenty_newsgroup_to_csv()\n</code></pre><p>接着，我们要对数据做预处理，我们需要过滤掉数据里面有些文本是空的情况。以及和我们前面进行文本分类一样，把Token数量太多的给过滤掉。</p><pre><code class=\"language-python\">from openai.embeddings_utils import get_embeddings\nimport openai, os, tiktoken, backoff\n\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\nembedding_model = \"text-embedding-ada-002\"\nembedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\nbatch_size = 2000\nmax_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n\ndf = pd.read_csv('20_newsgroup.csv')\nprint(\"Number of rows before null filtering:\", len(df))\ndf = df[df['text'].isnull() == False]\nencoding = tiktoken.get_encoding(embedding_encoding)\n\ndf[\"n_tokens\"] = df.text.apply(lambda x: len(encoding.encode(x)))\nprint(\"Number of rows before token number filtering:\", len(df))\ndf = df[df.n_tokens &lt;= max_tokens]\nprint(\"Number of rows data used:\", len(df))\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">Number of rows before null filtering: 11314\nNumber of rows before token number filtering: 11096\nNumber of rows data used: 11044\n</code></pre><p>然后，我们仍然是通过Embedding的接口，拿到文本的Embedding向量，然后把整个数据存储成parquet文件。</p><pre><code class=\"language-python\">@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef get_embeddings_with_backoff(prompts, engine):\n    embeddings = []\n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i+batch_size]\n        embeddings += get_embeddings(list_of_text=batch, engine=engine)\n    return embeddings\n\nprompts = df.text.tolist()\nprompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n\nembeddings = []\nfor batch in prompt_batches:\n    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n    embeddings += batch_embeddings\n\ndf[\"embedding\"] = embeddings\ndf.to_parquet(\"data/20_newsgroup_with_embedding.parquet\", index=False)\n</code></pre><p>这一部分代码基本和前面我们做文本分类一样，我就不再做详细讲解了。不理解代码为什么要这么写的同学，可以去看前面的<a href=\"http:/time.geekbang.org/column/article/643889\">第 05 讲</a>。</p><p>通常，在使用Jupyter Notebook或者Python去做机器学习类的任务的时候，我们往往会把一些中间步骤的数据结果给存下来。这样，我们可以避免在后面步骤写了Bug或者参数设错的时候从头开始。在这里，我们就把原始数据，以及Embedding处理完的数据都存了一份。这样，如果后面的聚类程序要做修改，我们不需要再花钱让OpenAI给我们算一次Embedding了。</p><p>接着，我们就可以用K-Means算法来进行聚类了。因为原本的数据来自20个不同的新闻组，那么我们也不妨就聚合成20个类，正好我们看看自动聚类出来的分类会不会就和原始分类差不多。</p><pre><code class=\"language-python\">import numpy as np\nfrom sklearn.cluster import KMeans\n\nembedding_df = pd.read_parquet(\"data/20_newsgroup_with_embedding.parquet\")\n\nmatrix = np.vstack(embedding_df.embedding.values)\nnum_of_clusters = 20\n\nkmeans = KMeans(n_clusters=num_of_clusters, init=\"k-means++\", n_init=10, random_state=42)\nkmeans.fit(matrix)\nlabels = kmeans.labels_\nembedding_df[\"cluster\"] = labels\n</code></pre><p>聚类的代码非常简单，我们通过NumPy的stack函数，把所有的Embedding放到一个矩阵里面，设置一下要聚合出来的类的数量，然后运行一下K-Means算法的fit函数，就好了。不过，聚类完，我们怎么去看它聚类的结果是不是合适呢？每个聚合出来的类代表什么呢？</p><p>在这里，我们的数据之前就有分组。那么我们就可以用一个取巧的思路。我们统计一下聚类之后的每个类有多少条各个newsgroups分组的数据。然后看看这些数据里面，排名第一的分组是什么。如果我们聚类聚合出来的类，都是从某一个newsgroup分组出来的文章，那么说明这个聚合出来的类其实就和那个分组的内容差不多。使用这个思路的代码，我也放在下面了，我们一起来看一看。</p><pre><code class=\"language-python\"># 统计每个cluster的数量\nnew_df = embedding_df.groupby('cluster')['cluster'].count().reset_index(name='count')\n\n# 统计这个cluster里最多的分类的数量\ntitle_count = embedding_df.groupby(['cluster', 'title']).size().reset_index(name='title_count')\nfirst_titles = title_count.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))\nfirst_titles = first_titles.reset_index(drop=True)\nnew_df = pd.merge(new_df, first_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')\nnew_df = new_df.rename(columns={'title': 'rank1', 'title_count': 'rank1_count'})\n\n# 统计这个cluster里第二多的分类的数量\nsecond_titles = title_count[~title_count['title'].isin(first_titles['title'])]\nsecond_titles = second_titles.groupby('cluster').apply(lambda x: x.nlargest(1, columns=['title_count']))\nsecond_titles = second_titles.reset_index(drop=True)\nnew_df = pd.merge(new_df, second_titles[['cluster', 'title', 'title_count']], on='cluster', how='left')\nnew_df = new_df.rename(columns={'title': 'rank2', 'title_count': 'rank2_count'})\nnew_df['first_percentage'] = (new_df['rank1_count'] / new_df['count']).map(lambda x: '{:.2%}'.format(x))\n# 将缺失值替换为 0\nnew_df.fillna(0, inplace=True)\n# 输出结果\nfrom IPython.display import display\ndisplay(new_df)\n</code></pre><p>这个代码也不难写，我们可以分成几步来做。</p><ol>\n<li>我们通过groupby可以把之前的DataFrame按照cluster进行聚合，统计每个cluster里面数据的条数。</li>\n<li>而要统计某一个cluster里面排名第一的分组名称和数量的时候，我们可以通过groupby，把数据按照 cluster + title 的方式聚合。</li>\n<li>再通过 cluster 聚合后，使用 x.nlargest 函数拿到里面数量排名第一的分组的名字和数量。</li>\n<li>为了方便分析，我还把数据里排名第一的去掉之后，又统计了一下排名第二的分组，放在一起看一下。</li>\n</ol><p>输出结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/f6/0c/f66f45d30edfb11a3f97a964684bd00c.png?wh=781x496\" alt=\"图片\"></p><p>从这个统计数据的结果来看，大部分聚类的结果，能够对应到某一个原本新闻组的分类。比如，cluster 0就有93.98%来自 comp.windows.x 这个分类。在20个聚合出来的类里面，有10个类80%来自原本newsgroup的某一个分类。剩下的分类中，比如cluster 2，前两个分组加在一起占了75%，这两个分组的名称分别是  pc.hardware  和 mac.hardware 其实都是聊电脑硬件的，不过是newsgroups里按照硬件不同做了区分而已。我们只有3个类，对应的分组比较分散，分别是cluster 6、9和19。</p><p>从这个结果来看，我们直接使用文本的Embedding来进行聚类，效果还算不错。</p><h2>使用提示语对文本进行总结</h2><p>不过啊，在真实的应用场景里，我们拿来进行文本聚类的数据，多半并没有什么分组信息。过去，我们要去给聚合出来的类取一个名字，往往只能选择看看各个类里面的文本是什么内容。靠我们的“人脑”给“电脑”做出的选择起一个我们觉得合适的名字。比如，对应到这里的20个分类的数据，往往我们只能每个挑上几篇内容，人工读一遍，再取一个名字。而如果你英文不太好，那可就太痛苦了。</p><p>不过，既然有了OpenAI的Completion接口，我们完全可以让AI给我们聚合出来的类起一个名字。我们可以随机在每个聚合出来的类里面，挑上3～5条，然后请AI总结一下该取什么名字，然后再挑一两条文本让AI给我们翻译成中文，看看名字取的是不是合理。</p><pre><code class=\"language-python\">items_per_cluster = 10\nCOMPLETIONS_MODEL = \"text-davinci-003\"\n\nfor i in range(num_of_clusters):\n    cluster_name = new_df[new_df.cluster == i].iloc[0].rank1\n    print(f\"Cluster {i}, Rank 1: {cluster_name}, Theme:\", end=\" \")\n\n    content = \"\\n\".join(\n        embedding_df[embedding_df.cluster == i].text.sample(items_per_cluster, random_state=42).values\n    )\n    response = openai.Completion.create(\n        model=COMPLETIONS_MODEL,\n        prompt=f'''我们想要给下面的内容，分组成有意义的类别，以便我们可以对其进行总结。请根据下面这些内容的共同点，总结一个50个字以内的新闻组的名称。比如 “PC硬件”\\n\\n内容:\\n\"\"\"\\n{content}\\n\"\"\"新闻组名称：''',\n        temperature=0,\n        max_tokens=100,\n        top_p=1,\n    )\n    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n</code></pre><p>我们可以用这样一段代码通过Completion接口来实现我们的需求。</p><ol>\n<li>我们随机从聚类结果里的每一个类里面，都挑上10条记录，然后分行将这些记录拼在一起。</li>\n<li>然后，我们给AI这样一段提示语，告诉AI这些内容来自新闻组，请AI根据它们的共性给这些新闻组的内容取一个50个字以内的名字。</li>\n<li>输出的内容，我们用Cluster，Cluster里原先排名第一的分组英文，以及AI给出的新闻组名称，对应的输出结果在下面。</li>\n</ol><p>输出结果：</p><pre><code class=\"language-python\">Cluster 0, Rank 1: comp.windows.x, Theme: Xlib编程\nCluster 1, Rank 1: sci.space, Theme: 太空技术与航空\nCluster 2, Rank 1: comp.sys.ibm.pc.hardware, Theme: PC硬件与系统\nCluster 3, Rank 1: rec.sport.hockey, Theme: 欧洲冰球vs北美冰球\nCluster 4, Rank 1: talk.politics.misc, Theme: 社会观点与自由\nCluster 5, Rank 1: rec.autos, Theme: 汽车硬件\nCluster 6, Rank 1: rec.motorcycles, Theme: 数学与文化冲击\nCluster 7, Rank 1: comp.os.ms-windows.misc, Theme: PC软件与硬件\nCluster 8, Rank 1: talk.politics.mideast, Theme: “穆斯林大屠杀”\nCluster 9, Rank 1: comp.os.ms-windows.misc, Theme: 科技产品\"\"\"\nCluster 10, Rank 1: talk.politics.guns, Theme: 枪支管制与安全\nCluster 11, Rank 1: comp.graphics, Theme: 计算机编程与硬件\nCluster 12, Rank 1: rec.motorcycles, Theme: 骑行安全与技巧\nCluster 13, Rank 1: soc.religion.christian, Theme: 宗教信仰与实践\nCluster 14, Rank 1: rec.sport.baseball, Theme: 棒球联盟\nCluster 15, Rank 1: misc.forsale, Theme: 购物优惠和出售\nCluster 16, Rank 1: sci.crypt, Theme: 关于加密政策的讨论\nCluster 17, Rank 1: sci.electronics, Theme: 电子设备技术\nCluster 18, Rank 1: sci.med, Theme: 药物和疾病\nCluster 19, Rank 1: sci.electronics, Theme: 电子邮件使用者研究\n</code></pre><p>可以看到，机器给出的中文分类名称，大部分是合理的。我们还可以挑一些里面的文本内容，看看它们的中文翻译是不是和上面取的名字是一致的。翻译的代码和上面类似，少数的几个差别是：</p><ol>\n<li>我们在每个分类的抽样数据里只找了1条，而不是总结时候选的10条。</li>\n<li>我们限制了这段文本的Token数量不超过100个，免得太占地方。</li>\n<li>输出的内容我们放大了字数到500字，确保翻译能提供足够的内容。</li>\n</ol><pre><code class=\"language-python\">items_per_cluster = 1\nCOMPLETIONS_MODEL = \"text-davinci-003\"\n\nfor i in range(num_of_clusters):\n    cluster_name = new_df[new_df.cluster == i].iloc[0].rank1\n    print(f\"Cluster {i}, Rank 1: {cluster_name}, 抽样翻译:\", end=\" \")\n\n    content = \"\\n\".join(\n        embedding_df[(embedding_df.cluster == i) &amp; (embedding_df.n_tokens &gt; 100)].text.sample(items_per_cluster, random_state=42).values\n    )\n    response = openai.Completion.create(\n        model=COMPLETIONS_MODEL,\n        prompt=f'''请把下面的内容翻译成中文\\n\\n内容:\\n\"\"\"\\n{content}\\n\"\"\"翻译：''',\n        temperature=0,\n        max_tokens=2000,\n        top_p=1,\n    )\n    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">Cluster 0, Rank 1: comp.windows.x, 抽样翻译: 没有实际执行它？不知怎么回事，我的一个xterminal用户使得只要点击鼠标右键，就会自动杀死所有客户端-哦，我的：-(谢谢，Fish\nCluster 1, Rank 1: sci.space, 抽样翻译: 韦恩·马森和他的团伙在阿拉巴马州发生了什么？我还听说有一个未经证实的谣言，即航空大使们已经消失了。有其他人可以证实吗？\nCluster 2, Rank 1: comp.sys.ibm.pc.hardware, 抽样翻译: 我怀疑这不是一个特定于Quadra的问题。去年我不得不放弃我“古老”的Bernoulli 20（每个磁带的价格大约是90美元，使整个事情的价值超过我的整个电脑;）。Ocean Microsystems的技术支持人员建议可以使用一些第三方驱动程序来解决这个问题 - 在我的情况下，磁带无法格式化/挂载/分区用于A / UX。\nCluster 3, Rank 1: rec.sport.hockey, 抽样翻译: 我相信那是4-1。罗德·布林道·阿莫尔在第三节19.59时分攻入一球。\nCluster 4, Rank 1: talk.politics.misc, 抽样翻译: 为了确保每个人都清楚：“它从未有过”是指“保护”，而不是“未能保护”；即，在我的一生中，我从未见过美国政府始终保护美国公民的利益，除非是意外。\nCluster 5, Rank 1: rec.autos, 抽样翻译: 噢，来吧，傻瓜，你要做的就是在你的引擎罩上割一个洞，然后把一个管子放进去，这样你就可以把机油倒进去了。你觉得那些热门车上的大空气进气装置是干什么的？它们只是为了外观，没有人知道，它们提供了进入机油填充孔的途径。\nCluster 6, Rank 1: rec.motorcycles, 抽样翻译: 你真是个失败者\nCluster 7, Rank 1: comp.os.ms-windows.misc, 抽样翻译: 偶尔你需要为表现良好的东西说句好话。我的东西桥3401没有任何问题。它在DOS和OS/2上运行得很好。对于OS/2，你不需要加载任何特殊的驱动程序。安装会检测到它是一个东西桥驱动器，然后就完成了。顺便说一句，它也很快！\nCluster 8, Rank 1: talk.politics.mideast, 抽样翻译: Avi，    供你参考，伊斯兰教允许宗教自由——在宗教上没有强制。犹太教是否也允许宗教自由（即是否认可非犹太人）？只是好奇而已。\nCluster 9, Rank 1: comp.os.ms-windows.misc, 抽样翻译: 每个人都有自己的梦想，但只有勇敢追求梦想的人才能实现它。\nCluster 10, Rank 1: talk.politics.guns, 抽样翻译: 不一定，特别是如果强奸犯被认定为此。例如，如果你有意地把手指伸进一个装满了老鼠夹的地方，然后被夹住，这是谁的错？\nCluster 11, Rank 1: comp.graphics, 抽样翻译: 帮帮我！！我需要代码/包/任何东西来处理3D数据，并将其转换为带有隐藏线的线框表面。我正在使用DOS机器，代码可以是ANSI C或C ++，ANSI Fortran或Basic。我使用的数据形成一个矩形网格。请将您的回复发布到网络上，以便其他人受益。我的个人观点是，这是一个普遍的兴趣问题。谢谢！！！！！\nCluster 12, Rank 1: rec.motorcycles, 抽样翻译: 这是一段心理学，对于任何长期骑行者来说都是必不可少的。人们不会去想“如果我这么做会有其他人受到影响吗？”他们只会评估“如果我这么做会受到影响吗？”\nCluster 13, Rank 1: soc.religion.christian, 抽样翻译: 这是一个非常薄弱的论点，因为没有独立的支持文本（关于关键事件）。至于新约最古老的现存文本的日期......如果现在只有一个关于美国内战的现存文本，你会怎么想？现在考虑一个大部分文盲的人口，每一份手稿都是手工复制的......--Hal\nCluster 14, Rank 1: rec.sport.baseball, 抽样翻译: 这个赔率意味着你下注5美元赌反败者赢8美元，或者下注9美元赌胜者赢5美元。\nCluster 15, Rank 1: misc.forsale, 抽样翻译: 嗯，标题就是这样......我正在寻找便宜的二手TG-16游戏，它们支持2个或更多玩家（同时）....请给我发送所有带有价格的报价。\nCluster 16, Rank 1: sci.crypt, 抽样翻译: 哪里？老实说，我没有看到任何……我不同意，至少有其他标准已经存在。此外，即使他们限制NREN上的加密，谁在乎呢？大部分互联网都是商业的。NREN只适用于政府和大学研究（阅读提案-它是一条“数据高速公路”，与互联网无关）。\nCluster 17, Rank 1: sci.electronics, 抽样翻译: 动态RAM不是基于翻转锁存器；基本上每个位只有一个晶体管和电容来存储！静态RAM是基于翻转锁存器，更加昂贵，密度也更低。如果忽略电子和热膨胀，两者都没有任何“移动”的部件...Chris\nCluster 18, Rank 1: sci.med, 抽样翻译: 化学品已经消失，感谢所有的回应。\nCluster 19, Rank 1: sci.electronics, 抽样翻译: 尝试lyman.pppl.gov--/pub/8051\n</code></pre><p>从输出的结果来看，我们选取的内容和AI总结的标题的确八九不离十。不知道这个过程，有没有重新让你回忆起Completion这个接口的强大之处。我们只要给出合适的指令，它就能按照我们的心意给出合理的结果。</p><h2>利用文本摘要支持无限轮数的聊天</h2><p>不知道你是否还记得上一讲里的思考题，如果不只是保留过去几轮对话，我们怎么在对话轮数越来越多的时候让AI记住尽量多的上下文？有一个办法，就是将过去几轮的上下文内容，先交给AI做一个100字的总结。然后，把这个总结也作为对话的Prompt信息，告诉AI这是聊天的背景，再重新开始新的一轮对话。</p><p>在这里我们不妨试一下。</p><ol>\n<li>我们拿上一讲的一段聊天记录，让AI先总结一下，可以看到AI总结了User和Assistant之间聊了什么。</li>\n</ol><pre><code class=\"language-python\">history = \"\"\"User : 你是谁？\nAssistant : 我是一个人工智能语言模型，可以回答各种问题，包括做菜的问题。\n\nUser : 请问鱼香肉丝怎么做？\nAssistant : 鱼香肉丝的做法：\n\n1. 猪里脊肉切丝，加入盐、淀粉、料酒搅拌均匀，腌制10分钟\n2. 胡萝卜、青椒、木耳切丝备用\n3. 热锅凉油，放入蒜末、姜末煸炒出香味，加入肉丝翻炒至变色\n4. 加入胡萝卜、青椒、木耳翻炒，加入酱油、糖、醋、盐、味精、水淀粉搅拌均匀，倒入锅中翻炒均匀即可。\n\n提示：鱼香肉丝的关键是调料的配比，酱油、糖、醋、盐、味精的比例要适合自己的口味。\n\nUser : 那蚝油牛肉呢？\nAssistant : 蚝油牛肉的做法：\n\n1. 将牛肉切成薄片，加入盐、淀粉、料酒腌制10分钟\n2. 葱姜切末，蒜切片，青红椒切丝备用\n3. 热锅凉油，放入葱姜末、蒜片煸炒出香味，加入牛肉翻炒至变色\n4. 加入蚝油、酱油、糖、水淀粉调味，加入青红椒翻炒均匀即可。\n\n提示：蚝油牛肉的关键是牛肉要切薄，翻炒时火候要快，保证牛肉口感鲜嫩。调味时，蚝油和酱油的比例也要适合自己的口味。\n\"\"\"\n\ndef summarize(text, max_tokens=200):\n    response = openai.Completion.create(\n        model=COMPLETIONS_MODEL,\n        prompt=text + \"\\n\\n请总结一下上面User和Assistant聊了些什么：\\n\",\n        max_tokens=max_tokens,\n    )\n    return response[\"choices\"][0][\"text\"]\n\nsummarized = summarize(history)\nprint(summarized)\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">User和Assistant聊了鱼香肉丝和蚝油牛肉的制作方法。User问了Assistant两个关于如何做鱼香肉丝和蚝油牛肉的问题，Assistant给出了回答并介绍了每道菜的具体制作方法，同时也提示了调料的配比和牛肉制作时要注意的细节。\n</code></pre><ol start=\"2\">\n<li>然后，我们再新建一个Conversation，这次的提示语里，我们先加上了总结的内容，然后告诉AI把对话继续下去。</li>\n</ol><pre><code class=\"language-python\">prompt = summarized + \"\\n\\n请你根据已经聊了的内容，继续对话：\"\nconversation = Conversation(prompt, 5)\n\nquestion = \"那宫保鸡丁呢？\"\nanswer = conversation.ask(question)\nprint(\"User : %s\" % question)\nprint(\"Assistant : %s\\n\" % answer)\n</code></pre><p>注意，如果你是在Notebook里面执行的话，你需要把上一讲的Conversation类的代码复制过来先执行一下。这里，我们启动了一个新的对话对象，将之前的几轮对话的内容总结了一下，放在Prompt的最前面，然后让AI根据已经聊了的内容，继续往下聊。</p><ol start=\"3\">\n<li>当我们直接问，“那宫保鸡丁呢？”，AI会正确回答出宫保鸡丁的做法。<br>\n输出结果：</li>\n</ol><pre><code class=\"language-python\">User : 那宫保鸡丁呢？\nAssistant : 宫保鸡丁的制作方法也比较简单。首先，将鸡肉切成小丁状，用料酒、盐、生抽腌制一下。然后将青椒、红椒、葱姜蒜切成丁状备用。接着，将花生米炒香备用。\n热锅凉油，油温七成热时放入鸡丁煸炒至变色，捞出备用。再将葱姜蒜爆香，加入青红椒丁翻炒一下，然后加入鸡丁，翻炒均匀。最后加入适量的糖、盐、醋、生抽、老抽、料酒、水淀粉炒匀，最后加入炒香的花生米即可。\n需要注意的是，炒鸡丁的时候要用大火，这样鸡肉会更嫩。另外，调料的配比也很关键，需要根据个人口味适量调整。\n</code></pre><p>而如果我们没有加上AI总结的之前的对话，只是让AI对话，它只能和你闲扯一些别的。</p><pre><code class=\"language-python\">conversation = Conversation(\"请你根据已经聊了的内容，继续对话：\", 5)\n\nquestion = \"那宫保鸡丁呢？\"\nanswer = conversation.ask(question)\nprint(\"User : %s\" % question)\nprint(\"Assistant : %s\\n\" % answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">User : 那宫保鸡丁呢？\nAssistant : 宫保鸡丁是一道非常有名的川菜，口感麻辣鲜香，非常美味。你喜欢吃辣的食物吗？\n</code></pre><p>如果没有给它已经总结了的内容，AI只会和你瞎扯，告诉你宫保鸡丁很好吃。</p><h2>小结</h2><p>不知道今天教的这些技巧你学会了吗？这一讲里，我们先是快速实验了一下通过Embedding拿到的向量进行文本聚类。对于聚类的结果，我们不用再像以前那样人工看成百上千条数据，然后拍个脑袋给这个类取个名字。我们直接利用了Completion接口可以帮我们总结内容的能力，给分类取了一个名字。从最终的效果来看，还算不错。</p><p>而类似的技巧，也可以用在多轮的长对话中。我们将历史对话，让AI总结成一小段文本放到提示语里面。这样既能够让AI记住过去的对话内容，又不会因为对话越来越长而超出模型可以支持的Token数量。这个技巧也是使用大语言模型的一种常见模式。</p><h2>课后练习</h2><ol>\n<li>这一讲里，我们使用了让AI概括聚类文本内容和聊天记录的提示语。你自己在体验GPT系列模型的时候，有什么觉得特别有用的提示语吗？欢迎你分享自己的体验。</li>\n<li>在文本聚类里面，有三个聚合出来的类，和原先的分组没有很明显的对应关系。你能利用现在学到的知识，写一些代码看看数据，研究一下是为什么吗？</li>\n</ol><p>期待能在评论区看到你的思考，也欢迎你把这节课分享给感兴趣的朋友，我们下一讲再见。</p>","comments":[{"had_liked":false,"id":371686,"user_name":"Geek_d8a880","can_delete":false,"product_type":"c1","uid":2908367,"ip_address":"北京","ucode":"D5C3AB47027AAA","user_header":"","comment_is_top":false,"comment_ctime":1680181456,"is_pvip":false,"replies":[{"id":135754,"content":"是的，所以实际运用中，还需要考虑保留哪些记忆信息。哪些存储在外部，可以继续往后看。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680586432,"ip_address":"上海","comment_id":371686,"utype":1}],"discussion_count":5,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"随着对话的轮数越来越多，文本摘要也会越来越抽象，能保留的上下文信息也会越来越少吧","like_count":10,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612260,"discussion_content":"是的，所以实际运用中，还需要考虑保留哪些记忆信息。哪些存储在外部，可以继续往后看。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1680586433,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1035226,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/cb/da/79080738.jpg","nickname":"椰子船长","note":"","ucode":"36A9BD5F312028","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611924,"discussion_content":"同问，每次对话都有新的内容，之前对话的摘要不用每次都生成？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680420672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1018552,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8a/b8/97d30803.jpg","nickname":"布拉格的鱼","note":"","ucode":"95145A236D6033","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611884,"discussion_content":"感觉确实如此","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680398870,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1104601,"avatar":"https://static001.geekbang.org/account/avatar/00/10/da/d9/f051962f.jpg","nickname":"浩仔是程序员","note":"","ucode":"A7E5CF9E1571A2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611762,"discussion_content":"我也有这样子的疑问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680280273,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1104601,"avatar":"https://static001.geekbang.org/account/avatar/00/10/da/d9/f051962f.jpg","nickname":"浩仔是程序员","note":"","ucode":"A7E5CF9E1571A2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611761,"discussion_content":"我也是有","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680280259,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372337,"user_name":"Geek2014","can_delete":false,"product_type":"c1","uid":2028957,"ip_address":"北京","ucode":"9EB356D8DF287E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg","comment_is_top":false,"comment_ctime":1681006780,"is_pvip":false,"replies":[{"id":135937,"content":"来自ChatGPT的回答在下面，大家都买了这个课了还是有问题可以多问ChatGPT啊。不过新时代下，很多自动评价指标其实已经不一定合适了，值得研究一下有什么新的评价方法\n\n常用的文本摘要评价指标有：\n\n1. ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：用于衡量自动文本摘要与参考摘要之间的匹配程度，在召回率（recall）的基础上计算精确率（precision）和 F1 值。\n\n2. BLEU（Bilingual Evaluation Understudy）：基于 n-gram 的统计方法，用于衡量自动文本摘要与参考摘要的相似度。\n\n3. METEOR（Metric for Evaluation of Text with Explicit ORdering）：结合了语义模型和短语表征的评价指标，能够处理同义词、语法变化等问题。\n\n4. NIST（NIST Information Retrieval）：基于语言模型的自动摘要评价指标，使用统计的方法获取参考摘要中的关键词信息，将这些信息与自动摘要进行比对。\n\n5. CIDEr（Consensus-based Image Description Evaluation）：最初是用于评价图像描述的指标，也可以应用于文本摘要评价上，能够捕捉到多样性和流畅性等方面。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681129196,"ip_address":"美国","comment_id":372337,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请问徐老师，chatgpt 给出的文本摘要，我们能够用什么方法来评估给出的摘要是好或不好呢？可以通过哪些指标来测试呢？","like_count":8,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613106,"discussion_content":"来自ChatGPT的回答在下面，大家都买了这个课了还是有问题可以多问ChatGPT啊。不过新时代下，很多自动评价指标其实已经不一定合适了，值得研究一下有什么新的评价方法\n\n常用的文本摘要评价指标有：\n\n1. ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：用于衡量自动文本摘要与参考摘要之间的匹配程度，在召回率（recall）的基础上计算精确率（precision）和 F1 值。\n\n2. BLEU（Bilingual Evaluation Understudy）：基于 n-gram 的统计方法，用于衡量自动文本摘要与参考摘要的相似度。\n\n3. METEOR（Metric for Evaluation of Text with Explicit ORdering）：结合了语义模型和短语表征的评价指标，能够处理同义词、语法变化等问题。\n\n4. NIST（NIST Information Retrieval）：基于语言模型的自动摘要评价指标，使用统计的方法获取参考摘要中的关键词信息，将这些信息与自动摘要进行比对。\n\n5. CIDEr（Consensus-based Image Description Evaluation）：最初是用于评价图像描述的指标，也可以应用于文本摘要评价上，能够捕捉到多样性和流畅性等方面。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681129196,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371827,"user_name":"Kevin","can_delete":false,"product_type":"c1","uid":2422166,"ip_address":"澳大利亚","ucode":"3BF17EEEA6DBF7","user_header":"https://static001.geekbang.org/account/avatar/00/24/f5/96/4e3dd4e1.jpg","comment_is_top":false,"comment_ctime":1680398849,"is_pvip":false,"replies":[{"id":135732,"content":"因为ChatPDF没有开源，所以具体它是怎么做的我不知道。\n\n其实最合理的方式，是既使用Embedding，也可以用其他的NLP技术，包括找一些关键字（利用POS分词），做命名实体识别等等，来做段落的搜索（召回）。\n\n然后通过ChatGPT的API再根据内容做问答。其实要做好索引还是有很多技巧的。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680582774,"ip_address":"上海","comment_id":371827,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请教一下徐老师，chatpdf的实现也是基于embedding来对上传的pdf建立一个index么？chatpdf自己回答所不是基于embedding建立的index，顾左右而言他，说是用了其他技术，比如提了part-of-speech tagging 和 named entity recognition。所以它用其他技术对pdf做了一个摘要文件对么？使用chatgpt api是为了更好的理解自然语言（把用户语言转换成可以part-of-speech或者其他专用库的操作，类似于embedding的cosine-similarity操作，然后再把答案用自然语言回复）？非常好奇如何实现，请徐老师分享一下洞见，谢谢！以下是chatpdf回复： In order to analyze the text of a PDF file, a PDF reader is used to extract the text from the file. This text is then passed to me as input, and I use natural language processing (NLP) techniques to analyze it.\n first step in my analysis involves breaking down the text into smaller units such as words or phrases. This is done using a process called tokenization, which involves splitting the text into individual tokens based on whitespace and punctuation.\n\nOnce the text has been tokenized, I use various NLP techniques to extract meaning from it. For example, I might use part-of-speech tagging to identify the grammatical structure of sentences, or named entity recognition to identify specific entities such as people, places, or organizations.","like_count":6,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612231,"discussion_content":"因为ChatPDF没有开源，所以具体它是怎么做的我不知道。\n\n其实最合理的方式，是既使用Embedding，也可以用其他的NLP技术，包括找一些关键字（利用POS分词），做命名实体识别等等，来做段落的搜索（召回）。\n\n然后通过ChatGPT的API再根据内容做问答。其实要做好索引还是有很多技巧的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680582774,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371623,"user_name":"嗯哼😬","can_delete":false,"product_type":"c1","uid":1522613,"ip_address":"广东","ucode":"43BCCCCB6820D9","user_header":"https://static001.geekbang.org/account/avatar/00/17/3b/b5/3bb54136.jpg","comment_is_top":false,"comment_ctime":1680110595,"is_pvip":false,"replies":[{"id":135759,"content":"先把代码跑起来有点体感。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680587015,"ip_address":"上海","comment_id":371623,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"😰糟糕，没有 NLP 基础的普通开发人员越看越迷糊。","like_count":6,"discussions":[{"author":{"id":1080418,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/OwlfSZBK8I7dpGtLia70p3e4cXib5ZsbY1vIpXiaWtAowoawJQFNVibGurpKlDwIBXmqZZpHx7RtSwCEyjOepicjstA/132","nickname":"ytl","note":"","ucode":"AB0D1F77CD0B7D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":611521,"discussion_content":"gpt大大降低了nlp使用的门槛，coding起来就不迷糊了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1680147841,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江西","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612265,"discussion_content":"先把代码跑起来有点体感。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680587015,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374827,"user_name":"陈鹏","can_delete":false,"product_type":"c1","uid":1574748,"ip_address":"立陶宛","ucode":"777F23D9178A6A","user_header":"https://static001.geekbang.org/account/avatar/00/18/07/5c/2b75c836.jpg","comment_is_top":false,"comment_ctime":1684467448,"is_pvip":false,"replies":[{"id":136877,"content":"这个训练，是指传统的机器学习模型训练。\n\n有额外的一组“参数”，这个参数很少，比如1537个（正好和我们的Embedding的维度数量+1），将embedding再映射到我们想要的分类问题。\n\n小模型训练的过程就是自动优化这组额外参数的问题，参数想存在哪里都可以。下次要用的时候加载这组参数就可以了。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684754448,"ip_address":"上海","comment_id":374827,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师好，我理解第5和第7节说的是：利用openai大语言模型的api，将数据embedding，用embedding的结果去训练其他的模型，比如聚类模型或分类模型。\n\n我不太理解这里的“训练”是什么意思？\n\n把embedding后的数据集给到小模型算法，小模型会得出分类或聚类的结果，课程中，老师说这样做的效果比较好。那么这里的“训练”体现在哪里呢？\n\n用embedding后的数据给到小模型，小模型就自动优化内部参数了？还是说课程中没有涉及到模型训练的内容？\n如果这样就是训练了小模型，那么训练后的参数是如何存储的？下次调用后，这次训练的参数还存在吗？\n\n谢谢老师解惑","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619067,"discussion_content":"这个训练，是指传统的机器学习模型训练。\n\n有额外的一组“参数”，这个参数很少，比如1537个（正好和我们的Embedding的维度数量+1），将embedding再映射到我们想要的分类问题。\n\n小模型训练的过程就是自动优化这组额外参数的问题，参数想存在哪里都可以。下次要用的时候加载这组参数就可以了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684754448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373806,"user_name":"gc","can_delete":false,"product_type":"c1","uid":1159933,"ip_address":"上海","ucode":"F45A536FDEF37D","user_header":"https://static001.geekbang.org/account/avatar/00/11/b2/fd/e10b39e4.jpg","comment_is_top":false,"comment_ctime":1683189445,"is_pvip":false,"replies":[{"id":136867,"content":"你想要什么样的标签？\n\n根据书的介绍，而不是全文来打标签一般就足够了吧。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684753033,"ip_address":"上海","comment_id":373806,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师，本人小白，问个别的问题，如何对书籍进行分类聚合呢，比如一本书拆出一些分类标签，能提供一些思路吗？这种大文本的好像本章节方式并不适合？","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619054,"discussion_content":"你想要什么样的标签？\n\n根据书的介绍，而不是全文来打标签一般就足够了吧。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684753033,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1401327,"avatar":"https://static001.geekbang.org/account/avatar/00/15/61/ef/ac5e914d.jpg","nickname":"麦耀锋","note":"","ucode":"077DA831185C01","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617041,"discussion_content":"对于书籍，我觉得可以借助书籍的目录、摘要、评论等来给chatgpt做embedding。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683278330,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373465,"user_name":"Geek_32772e","can_delete":false,"product_type":"c1","uid":1446158,"ip_address":"北京","ucode":"2850EF64F4775F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoribOUgcicu1sOqZZVtPqpSDSS43vicxW0GesxQeBRjUC47CzulKSzYNj2aMg9YOZDdjPdAZxS3jNcQ/132","comment_is_top":false,"comment_ctime":1682535098,"is_pvip":false,"replies":[{"id":136509,"content":"感觉是网络或者源IP在国内的问题？OpenAI目前大陆和香港应该都不让访问","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683091443,"ip_address":"上海","comment_id":373465,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"第一段代码403Forbidden了，报错如下：urllib.error.HTTPError: HTTP Error 403: Forbidden","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616749,"discussion_content":"感觉是网络或者源IP在国内的问题？OpenAI目前大陆和香港应该都不让访问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683091443,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2846589,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/6f/7d/3abf607b.jpg","nickname":"MWF","note":"","ucode":"2436883403543A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629433,"discussion_content":"开了梯子就能正常下载了，否则403","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1697070785,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1222233,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a6/59/1689ea0c.jpg","nickname":"金hb.Ryan 冷空氣駕到","note":"","ucode":"CAD363576696E4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":618762,"discussion_content":"figshare.com 在国内访问会返回403","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684510491,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372414,"user_name":"卖烧烤夫斯基","can_delete":false,"product_type":"c1","uid":2081124,"ip_address":"广东","ucode":"EC9DB8C4A3C21A","user_header":"https://static001.geekbang.org/account/avatar/00/1f/c1/64/3b994bd5.jpg","comment_is_top":false,"comment_ctime":1681118518,"is_pvip":false,"replies":[{"id":135939,"content":"一般都是试几个值然后比较一些评价指标。\n\n来自chatgpt的回答，不理解指标的定义可以进一步问他。大家一定要习惯起来有问题可以不断追问ChatGPT来回答\n\n聚类算法的聚类数量是一个非常重要的参数，对聚类结果影响很大。以下是几种测试聚类数量合适的方法：\n\n1. Elbow method（肘部法则）：随着聚类数量的增加，聚类效果不断增加，但是聚类数量达到一定程度后，聚类效果不再明显提高，这个点称为“拐点”，也称为肘部点。通过在不同的聚类数量下计算聚类误差并绘制成图表，找到聚类误差开始骤降的点作为聚类数量的参考。\n\n2. Silhouette method（轮廓系数法）：通过计算每个样本的轮廓系数，得到平均轮廓系数，通过不同聚类数量下的平均轮廓系数来确定最合适的聚类数量。\n\n3. Gap statistic method（间隔统计法）：与随机数据比较得到理论上最佳聚类数目，原理是用原始数据与随机数据比较得到一个统计值，随着聚类数量的增加，统计值先增加后减少，取的值与随机数据的差距最大的点为最佳聚类数。\n\n4. DBSCAN（基于密度的聚类算法）：该算法本身是没有聚类数量的限制，会自动根据数据样本的密度来确定聚类数量。可以通过调整聚类半径和样本密度参数来调整聚类数量。\n\n需要注意的是，不同的数据集和聚类算法可能适用不同的方法，寻找最合适的聚类数量需要进行一定的尝试和比较。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681129400,"ip_address":"美国","comment_id":372414,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请问老师，面对大量随机文本，聚类的个数如何确定呢？","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613109,"discussion_content":"一般都是试几个值然后比较一些评价指标。\n\n来自chatgpt的回答，不理解指标的定义可以进一步问他。大家一定要习惯起来有问题可以不断追问ChatGPT来回答\n\n聚类算法的聚类数量是一个非常重要的参数，对聚类结果影响很大。以下是几种测试聚类数量合适的方法：\n\n1. Elbow method（肘部法则）：随着聚类数量的增加，聚类效果不断增加，但是聚类数量达到一定程度后，聚类效果不再明显提高，这个点称为“拐点”，也称为肘部点。通过在不同的聚类数量下计算聚类误差并绘制成图表，找到聚类误差开始骤降的点作为聚类数量的参考。\n\n2. Silhouette method（轮廓系数法）：通过计算每个样本的轮廓系数，得到平均轮廓系数，通过不同聚类数量下的平均轮廓系数来确定最合适的聚类数量。\n\n3. Gap statistic method（间隔统计法）：与随机数据比较得到理论上最佳聚类数目，原理是用原始数据与随机数据比较得到一个统计值，随着聚类数量的增加，统计值先增加后减少，取的值与随机数据的差距最大的点为最佳聚类数。\n\n4. DBSCAN（基于密度的聚类算法）：该算法本身是没有聚类数量的限制，会自动根据数据样本的密度来确定聚类数量。可以通过调整聚类半径和样本密度参数来调整聚类数量。\n\n需要注意的是，不同的数据集和聚类算法可能适用不同的方法，寻找最合适的聚类数量需要进行一定的尝试和比较。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681129400,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":374633,"user_name":"Hugh","can_delete":false,"product_type":"c1","uid":1029016,"ip_address":"上海","ucode":"126A56632CB09A","user_header":"https://static001.geekbang.org/account/avatar/00/0f/b3/98/2ad8b465.jpg","comment_is_top":false,"comment_ctime":1684243190,"is_pvip":false,"replies":[{"id":136795,"content":"啊，对。因为我用 Colab 或者 VSCode 内置的默认不需要import，我和编辑同步修改一下","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1684722898,"ip_address":"新加坡","comment_id":374633,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"display(new_df) 没有引入 from IPython.display import display","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":618931,"discussion_content":"啊，对。因为我用 Colab 或者 VSCode 内置的默认不需要import，我和编辑同步修改一下","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1684722899,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"新加坡","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372580,"user_name":"suzg","can_delete":false,"product_type":"c1","uid":1001561,"ip_address":"北京","ucode":"C3883EBA834689","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/59/439a172e.jpg","comment_is_top":false,"comment_ctime":1681298693,"is_pvip":false,"replies":[{"id":136110,"content":"有可能，你也可以拿自己的数据集试一下呀。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681614182,"ip_address":"上海","comment_id":372580,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"有没有可能是这个newsgroup数据集就在openai的训练数据中，所以效果好","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613952,"discussion_content":"有可能，你也可以拿自己的数据集试一下呀。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681614182,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371680,"user_name":"卖烧烤夫斯基","can_delete":false,"product_type":"c1","uid":2081124,"ip_address":"广东","ucode":"EC9DB8C4A3C21A","user_header":"https://static001.geekbang.org/account/avatar/00/1f/c1/64/3b994bd5.jpg","comment_is_top":false,"comment_ctime":1680170249,"is_pvip":false,"replies":[{"id":135758,"content":"👍，是的，这个我在测试的时候没有考虑。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1680586999,"ip_address":"上海","comment_id":371680,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"embedding_df[embedding_df.cluster == i].text.sample(items_per_cluster, random_state=42).values\n获取的随机文本过长，可能会超出openai的token限制，导致报错。还需要计算一下conent的token长度","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":612264,"discussion_content":"👍，是的，这个我在测试的时候没有考虑。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1680586999,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":371649,"user_name":"www","can_delete":false,"product_type":"c1","uid":1898338,"ip_address":"广东","ucode":"ADC9BC655EA16C","user_header":"https://static001.geekbang.org/account/avatar/00/1c/f7/62/947004d0.jpg","comment_is_top":false,"comment_ctime":1680147644,"is_pvip":false,"replies":null,"discussion_count":7,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"不懂NLP，跟着在colab跑了一遍，有问题就问chatgpt，有种丝滑的体验\n","like_count":11,"discussions":[{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621774,"discussion_content":"merge left_on, right_index=True\nChatGPT\n当在 merge() 方法中设置参数 left_on 和 right_index=True 时，表示使用左侧 DataFrame 的指定列作为连接键，同时使用右侧 DataFrame 的索引作为连接键进行合并。\n\n以下是一个示例：\n\npython\nCopy code\nimport pandas as pd\n\n# 创建示例 DataFrame\ndf1 = pd.DataFrame({&#39;ID&#39;: [1, 2, 3, 4],\n                    &#39;Name&#39;: [&#39;Alice&#39;, &#39;Bob&#39;, &#39;Charlie&#39;, &#39;David&#39;]})\n\ndf2 = pd.DataFrame({&#39;Age&#39;: [25, 30, 35, 40]},\n                   index=[3, 4, 5, 6])\n\n# 使用 merge() 方法进行合并，left_on=&#39;ID&#39;，right_index=True\nmerged_df = df1.merge(df2, left_on=&#39;ID&#39;, right_index=True)\nprint(merged_df)\n输出结果：\n\nCopy code\n   ID     Name  Age\n2   3  Charlie   25\n3   4    David   30\n在上面的示例中，我们有两个 DataFrame：df1 和 df2。df1 包含列 &#39;ID&#39; 和 &#39;Name&#39;，而 df2 包含列 &#39;Age&#39;，并且使用索引作为连接键。\n\n通过使用 merge() 方法，并设置 left_on=&#39;ID&#39; 和 right_index=True，我们指定使用 df1 的 &#39;ID&#39; 列作为连接键，同时使用 df2 的索引作为连接键进行合并。这样，只有 &#39;ID&#39; 列中的值与 df2 的索引匹配的行才会被合并。\n\n在合并结果中，只有 &#39;ID&#39; 列中的值为 3 和 4 的行与 df2 的索引 3 和 4 匹配，因此只有这两行被合并，并包含了 &#39;Age&#39; 列的值。\n\n这种合并方式允许我们使用左侧 DataFrame 的特定列与右侧 DataFrame 的索引进行连接操作，根据具体的数据结构和连接需求，可以选择合适的连接键。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1687619073,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621769,"discussion_content":"pandas.DataFrame.T 这里 T 是啥？\nChatGPT\n在pandas中，T是DataFrame的一个属性，表示转置（Transpose）。它用于将DataFrame的行和列进行交换，即将DataFrame的行变为列，列变为行。\n\n当你调用T属性时，它会返回一个新的转置后的DataFrame，而不会修改原始的DataFrame。转置操作对于重新排列数据以满足分析需求或改变数据结构的形状非常有用。\n\n以下是一个示例：\n\npython\nCopy code\nimport pandas as pd\n\n# 创建一个DataFrame\ndata = {&#39;Name&#39;: [&#39;John&#39;, &#39;Jane&#39;, &#39;Mike&#39;],\n        &#39;Age&#39;: [25, 30, 35],\n        &#39;City&#39;: [&#39;New York&#39;, &#39;London&#39;, &#39;Paris&#39;]}\ndf = pd.DataFrame(data)\n\n# 原始DataFrame\nprint(df)\n# 输出：\n#    Name  Age      City\n# 0  John   25  New York\n# 1  Jane   30    London\n# 2  Mike   35     Paris\n\n# 转置后的DataFrame\ndf_transposed = df.T\nprint(df_transposed)\n# 输出：\n#          0       1      2\n# Name   John    Jane   Mike\n# Age      25      30     35\n# City    New York London  Paris\n在转置后的DataFrame中，原始DataFrame的行标签变为了列标签，而列标签变为了行标签。转置后的DataFrame在进行索引和操作时，行和列的角色也会相应地交换。\n\n需要注意的是，转置操作会创建一个新的DataFrame，而不是修改原始的DataFrame。如果需要在原始DataFrame上进行转置操作，可以使用df = df.T来覆盖原始DataFrame。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1687611640,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621765,"discussion_content":"fetch_20newsgroups 函数如何使用，参数的含义是什么\nChatGPT\nfetch_20newsgroups是scikit-learn库中用于下载和加载20个新闻组数据集的函数。这个数据集是一个常用的文本分类数据集，包含大约20,000篇新闻文档，涵盖了20个不同主题的新闻组。\n\n以下是fetch_20newsgroups函数的常用参数及其含义：\n\nsubset：指定要加载的数据子集。可选值为&#39;train&#39;（训练集）, &#39;test&#39;（测试集）, &#39;all&#39;（所有文档）之一。默认值为&#39;train&#39;。\n\ncategories：指定要加载的新闻组主题的列表。可以是具体的主题名称，也可以是主题名称的模式。例如，[&#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;]将只加载与计算机图形和Windows相关的文档。默认值为None，表示加载所有主题。\n\nshuffle：指定是否对数据进行随机排序。默认值为True，表示随机排序。\n\nrandom_state：用于控制数据洗牌的随机种子。如果设置了一个固定的随机种子，每次调用函数时将获得相同的数据洗牌结果。\n\nremove：指定是否移除文档中的邮件头、引文等元信息。默认值为(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)，表示移除邮件头、邮件尾和引文。如果设置为None，则不会移除任何元信息。\n\ndownload_if_missing：指定如果数据集文件不存在时是否自动下载。默认值为True，表示自动下载。\n\nreturn_X_y：指定是否只返回数据和目标变量（True），还是返回一个Bunch对象（False）。默认值为False。\n\ndata_home：指定数据集的目标文件夹路径。数据集将会被下载到该文件夹中。如果未指定，则使用默认的scikit-learn数据集目录。\n\n下面是一个使用fetch_20newsgroups函数加载训练集数据的例子：\n\npython\nCopy code\nfrom sklearn.datasets import fetch_20newsgroups\n\n# 加载训练集数据\ntrain_data = fetch_20newsgroups(subset=&#39;train&#39;)\n\n# 打印数据集信息\nprint(train_data.DESCR)\n\n# 获取数据和目标变量\nX_train = train_data.data\ny_train = train_data.target\n这样，X_train将包含训练集的文本数据，y_train将包含相应的目标变量（对应20个新闻组主题的整数标签）。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1687609992,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621768,"discussion_content":"at 和 loc 的区别\nChatGPT\nat和loc都是pandas中用于访问DataFrame和Series中单个元素的方法，但它们在用法和性能上有一些区别。\n\nat：at方法用于通过标签（行标签和列标签）来访问单个元素。它具有以下特点：\n\n使用行标签和列标签作为输入，类似于df.at[row_label, column_label]。\n只能用于访问单个元素，不支持对行或列进行切片操作。\n返回的结果是一个标量值。\nat方法的优点是它比loc更快，因为它不需要返回整个行或列的视图，而是直接返回单个元素的值。因此，如果你只需要访问单个元素，使用at会更有效率。\n\n示例使用at方法访问单个元素：\n\npython\nCopy code\nvalue = df.at[0, &#39;Name&#39;]\nloc：loc方法用于通过标签（行标签和列标签）来访问数据，包括单个元素、多个行或多个列。它具有以下特点：\n\n使用行标签和列标签作为输入，类似于df.loc[row_label, column_label]。\n支持对行和列进行切片操作，可以选择多个行和列。\n返回的结果可以是一个标量值、Series对象或DataFrame对象，具体取决于选择的数据。\nloc方法的优点是它提供了更灵活的选择和操作数据的功能，可以根据标签进行切片、筛选和分组操作。\n\n示例使用loc方法访问单个元素和多个行/列：\n\npython\nCopy code\n# 访问单个元素\nvalue = df.loc[0, &#39;Name&#39;]\n\n# 访问多个行和列\nsubset = df.loc[1:3, [&#39;Name&#39;, &#39;Age&#39;]]\n总结：\n\nat用于通过标签访问单个元素，返回标量值。\nloc用于通过标签访问单个元素、多个行或多个列，返回标量值、Series对象或DataFrame对象。\nat适用于只需要访问单个元素的场景，因为它更快。\nloc提供了更灵活的选择和操作数据的功能，支持切片、筛选和分组操作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1687611444,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621767,"discussion_content":"loc 和 iloc 区别\nChatGPT\nloc和iloc都是用于在pandas中访问DataFrame和Series中的数据的方法，但它们在索引方式和用法上有一些区别。\n\nloc：loc是基于标签的索引，用于通过标签（行索引和列名）来访问数据。\n\n使用行标签和列标签进行索引，类似于df.loc[row_label, column_label]。\n行标签可以是单个标签、标签列表或布尔条件。\n列标签可以是单个标签、标签列表或列切片。\n返回的结果是一个新的DataFrame或Series，包含满足条件的数据。\n以下是loc的一些示例：\n\npython\nCopy code\n# 通过行标签和列标签访问单个元素\ndf.loc[0, &#39;Name&#39;]\n\n# 通过行标签和列标签访问某一行或某一列\ndf.loc[0]  # 访问第一行的所有列\ndf.loc[:, &#39;Age&#39;]  # 访问 &#39;Age&#39; 列的所有行\n\n# 通过布尔条件进行筛选\ndf.loc[df[&#39;Age&#39;] &gt; 30]\n\n# 通过标签切片访问多行或多列\ndf.loc[1:3, &#39;Name&#39;:&#39;City&#39;]\niloc：iloc是基于位置的索引，用于通过整数位置来访问数据。\n\n使用整数位置进行索引，类似于df.iloc[row_index, column_index]。\n行索引和列索引都是整数，从0开始。\n行索引和列索引可以是单个整数、整数列表或整数切片。\n返回的结果是一个新的DataFrame或Series，包含满足条件的数据。\n以下是iloc的一些示例：\n\npython\nCopy code\n# 通过整数位置访问单个元素\ndf.iloc[0, 1]\n\n# 通过整数位置访问某一行或某一列\ndf.iloc[0]  # 访问第一行的所有列\ndf.iloc[:, 2]  # 访问第三列的所有行\n\n# 通过整数位置切片访问多行或多列\ndf.iloc[1:4, 1:3]\n总结：\n\nloc是基于标签的索引，使用行标签和列标签进行索引。\niloc是基于位置的索引，使用整数位置进行索引。\nloc和iloc都可以用于选择特定的行、列，以及进行条件筛选。\n选择单个元素时，loc使用标签索引，iloc使用整数位置索引。\n使用标签切片时，loc会包含结束位置的行和列，iloc会按照惯例不包含结束位置的行和列。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1687611423,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1452167,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132","nickname":"Geek_93970d","note":"","ucode":"52AC308BEC7737","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621766,"discussion_content":"pandas.DataFrame 如何使用？\nChatGPT\npandas.DataFrame是pandas库中的一个核心数据结构，用于表示二维的表格数据，类似于Excel中的电子表格或SQL中的表。它提供了一系列功能强大的方法和操作，使得数据的处理、转换和分析变得更加简单和高效。\n\n下面是一些常用的pandas.DataFrame的使用方法：\n\n创建DataFrame：可以从多种数据源创建DataFrame，如字典、列表、NumPy数组、CSV文件、数据库查询结果等。下面是一些示例：\n\npython\nCopy code\nimport pandas as pd\n\n# 从字典创建DataFrame\ndata = {&#39;Name&#39;: [&#39;John&#39;, &#39;Jane&#39;, &#39;Mike&#39;],\n        &#39;Age&#39;: [25, 30, 35],\n        &#39;City&#39;: [&#39;New York&#39;, &#39;London&#39;, &#39;Paris&#39;]}\ndf = pd.DataFrame(data)\n\n# 从CSV文件创建DataFrame\ndf = pd.read_csv(&#39;data.csv&#39;)\n查看数据：可以使用一些方法来查看DataFrame的数据和结构。\n\npython\nCopy code\n# 查看DataFrame的前几行\ndf.head()\n\n# 查看DataFrame的基本统计信息\ndf.describe()\n\n# 查看DataFrame的列名\ndf.columns\n\n# 查看DataFrame的形状（行数和列数）\ndf.shape\n访问数据：可以使用不同的方法来访问和操作DataFrame中的数据。\n\npython\nCopy code\n# 访问某一列的数据\ndf[&#39;Name&#39;]  # 或 df.Name\n\n# 访问某一行的数据\ndf.loc[0]  # 根据行索引\ndf.iloc[0]  # 根据行号\n\n# 访问某个单元格的数据\ndf.at[0, &#39;Name&#39;]  # 根据行索引和列名\ndf.iat[0, 1]  # 根据行号和列号\n\n# 条件筛选数据\ndf[df[&#39;Age&#39;] &gt; 30]\n修改和更新数据：可以对DataFrame中的数据进行修改和更新。\n\npython\nCopy code\n# 添加新的列\ndf[&#39;Gender&#39;] = [&#39;Male&#39;, &#39;Female&#39;, &#39;Male&#39;]\n\n# 修改某列的值\ndf.loc[0, &#39;Age&#39;] = 26\n\n# 根据条件更新数据\ndf.loc[df[&#39;Age&#39;] &gt; 30, &#39;City&#39;] = &#39;Unknown&#39;\n\n# 删除某列或某行\ndf.drop(&#39;Gender&#39;, axis=1, inplace=True)  # 删除列\ndf.drop(0, inplace=True)  # 删除行\n数据操作和转换：可以对DataFrame进行各种数据操作和转换。\n\npython\nCopy code\n# 排序数据\ndf.sort_values(&#39;Age&#39;, ascending=False)\n\n# 去重\ndf.drop_duplicates()\n\n# 分组和聚合\ndf.groupby(&#39;City&#39;)[&#39;Age&#39;].mean()\n\n# 处理缺失值\ndf.dropna()  # 删除包含缺失值的行\ndf.fillna(0)  # 将缺失","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1687611402,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1118159,"avatar":"https://static001.geekbang.org/account/avatar/00/11/0f/cf/bfa0dc80.jpg","nickname":"Z12","note":"","ucode":"59F5E62F67FFDC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":615583,"discussion_content":"我比你更惨，不懂python，但是利用chatgpt也是各种丝滑的体验。追问式的学习真的是太棒了！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682341114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380942,"user_name":"里咯破","can_delete":false,"product_type":"c1","uid":1224546,"ip_address":"广东","ucode":"2DA41A6D44B3C4","user_header":"https://static001.geekbang.org/account/avatar/00/12/af/62/5eeb9041.jpg","comment_is_top":false,"comment_ctime":1694490550,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"由于apikeys过期了，所以一直没跑代码，看到第5章开始就看不懂了，只知道大概意思，特别是你做识别对比的图表，完全看不到，不知道表达了什么意思。","like_count":2},{"had_liked":false,"id":378142,"user_name":"小理想。","can_delete":false,"product_type":"c1","uid":2238528,"ip_address":"北京","ucode":"EDC35A907570DB","user_header":"https://static001.geekbang.org/account/avatar/00/22/28/40/82d748e6.jpg","comment_is_top":false,"comment_ctime":1689818019,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师能不能给下这个文件 20_newsgroup_with_embedding.parquet，默认的apikey被限制住了不允许请求，","like_count":1},{"had_liked":false,"id":378089,"user_name":"小理想。","can_delete":false,"product_type":"c1","uid":2238528,"ip_address":"北京","ucode":"EDC35A907570DB","user_header":"https://static001.geekbang.org/account/avatar/00/22/28/40/82d748e6.jpg","comment_is_top":false,"comment_ctime":1689747801,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"获取数据集并处理，代码加上了注释，感谢\nChatGPTdef twenty_newsgroup_to_csv():\n    # 获取新闻数据集，subset=&quot;train&quot;只获取训练集那部分，remove=(&#39;headers&#39;,&#39;footers&#39;,&#39;quotes&#39;) 表示在获取数据集时，同时移除文本中的标题、页脚和引用部分。\n    newsgroups_train=fetch_20newsgroups(subset=&quot;train&quot;,remove=(&#39;headers&#39;,&#39;footers&#39;,&#39;quotes&#39;))\n    # 创建DataFrame对象，DataFrame对象包含两列数据，一列是newsgroups_train.data的包含新闻组数据的列表，\n    # 另一列newsgroups_train.target是一个包含新闻组标签的列表。\n    df=pd.DataFrame([newsgroups_train.data,newsgroups_train.target.tolist()]).T\n    # 将DataFrame对象的列名更改为&#39;text&#39;和&#39;target&#39;\n    df.columns=[&#39;text&#39;,&#39;target&#39;]\n\n    # 创建一个包含新闻组分类名称的DataFrame对象，将列名命名为&#39;title&#39;\n    targets=pd.DataFrame(newsgroups_train.target_names,columns=[&#39;title&#39;])\n\n    out=pd.merge(df,targets,left_on=&#39;target&#39;,right_index=True)\n    out.to_csv(&#39;20_newsgroup.csv&#39;,index=False)\n\n\ntwenty_newsgroup_to_csv()","like_count":1},{"had_liked":false,"id":375225,"user_name":"newbiner","can_delete":false,"product_type":"c1","uid":1310478,"ip_address":"北京","ucode":"04F4B4FDB79854","user_header":"","comment_is_top":false,"comment_ctime":1685089184,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师，关于第二点“利用文本摘要支持无限轮数的聊天”，如果history和多，是不是也很费token，有没有更好的方案呢？谢谢！","like_count":1},{"had_liked":false,"id":371813,"user_name":"yanyu-xin","can_delete":false,"product_type":"c1","uid":1899757,"ip_address":"广东","ucode":"3AA389F9E4C236","user_header":"","comment_is_top":false,"comment_ctime":1680361645,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"已经看不太懂。并且程序还是有不少需要根据个人环境微调的，但是调试程序，出错就问chat，还是很容易就能顺利运行的。先开始，按课程逐一调试程序，以后逐步深入了解","like_count":1},{"had_liked":false,"id":394387,"user_name":"Geek_78a551","can_delete":false,"product_type":"c1","uid":3784461,"ip_address":"北京","ucode":"CCADD2F766E193","user_header":"","comment_is_top":false,"comment_ctime":1726678318,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"重新更新下代码吧, 上面的代码所有的都不能执行","like_count":0},{"had_liked":false,"id":388177,"user_name":"Geek_95f4ae","can_delete":false,"product_type":"c1","uid":2908735,"ip_address":"广东","ucode":"202A0F99F5638B","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/mbOToKXibw8HFuW7AkqCAIJVLMxqCaOYQYZxQYHBT4VO2v92d4UCK3XtiaFz2PpDXWpBlHnN9LbsTibCzV4EKALJA/132","comment_is_top":false,"comment_ctime":1709561733,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\ndef get_embeddings_with_backoff(prompts, engine):\n    embeddings = []\n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i+batch_size]\n        embeddings += get_embeddings(list_of_text=batch, engine=engine)\n    return embeddings\n\nprompts = df.text.tolist()\nprompt_batches = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]\n\nembeddings = []\nfor batch in prompt_batches:\n    batch_embeddings = get_embeddings_with_backoff(prompts=batch, engine=embedding_model)\n    embeddings += batch_embeddings\n\ndf[&quot;embedding&quot;] = embeddings\ndf.to_parquet(&quot;data&#47;20_newsgroup_with_embedding.parquet&quot;, index=False)\n\n老师请问上面的代码有什么要注意的地方吗，在第一句就报错，问chatgpt得到的答案也跑不通，是版本更新导致的吗？请问有处理的方法吗？","like_count":0,"discussions":[{"author":{"id":1816799,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/b8/df/26c8a2e1.jpg","nickname":"华山论剑","note":"","ucode":"72B63FE285BDBB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":639133,"discussion_content":"把openai.error.RateLimitError改成openai.RateLimitError，代码里的engine全部改成model，就可以了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1710317303,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386228,"user_name":"zengraoli","can_delete":false,"product_type":"c1","uid":3203583,"ip_address":"北京","ucode":"3F5DBA914CB974","user_header":"https://static001.geekbang.org/account/avatar/00/30/e1/ff/5e538177.jpg","comment_is_top":false,"comment_ctime":1704355379,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师你好，想跟你跑文中的程序，可惜看到了05也未见到有data文件夹的内容，请问可以给个下载地址么？","like_count":0},{"had_liked":false,"id":381270,"user_name":"胡贺鹏","can_delete":false,"product_type":"c1","uid":2398372,"ip_address":"广东","ucode":"D03369FEEDCA92","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/wr3qiaXYeAaLHyPBArO8ThtgMns5icVkv7SWasTbGeic8QsIkAIC2XCG7ibTTibdicLB6WCibjj5b50kcV7rPKxqVW8Dw/132","comment_is_top":false,"comment_ctime":1694959132,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"请问这行代码有什么用？\nnp.vstack(embedding_df.embedding.values)","like_count":0},{"had_liked":false,"id":380055,"user_name":"SlgGrlGwy","can_delete":false,"product_type":"c1","uid":3193646,"ip_address":"广东","ucode":"D35076047897EC","user_header":"https://static001.geekbang.org/account/avatar/00/30/bb/2e/3e0bd0e1.jpg","comment_is_top":false,"comment_ctime":1692886338,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"可以提供下parquet 文件吗？转化一天啦，还未完成","like_count":0},{"had_liked":false,"id":376983,"user_name":"神毓逍遥","can_delete":false,"product_type":"c1","uid":2147220,"ip_address":"上海","ucode":"83351CB18B190E","user_header":"https://static001.geekbang.org/account/avatar/00/20/c3/94/e89ebc50.jpg","comment_is_top":false,"comment_ctime":1687790252,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"本地知识库分析与回答问题，有推荐的吗？调用 OPENAPI 涉及安全问题","like_count":0},{"had_liked":false,"id":376372,"user_name":"蜕变","can_delete":false,"product_type":"c1","uid":2035234,"ip_address":"山东","ucode":"F3FB212D300B11","user_header":"https://static001.geekbang.org/account/avatar/00/1f/0e/22/988879b9.jpg","comment_is_top":false,"comment_ctime":1686740028,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"Traceback (most recent call last):\n  File &quot;&#47;Users&#47;hupengfei&#47;miniconda3&#47;envs&#47;py310&#47;lib&#47;python3.10&#47;site-packages&#47;gradio&#47;routes.py&quot;, line 394, in run_predict\n    output = await app.get_blocks().process_api(\n  File &quot;&#47;Users&#47;hupengfei&#47;miniconda3&#47;envs&#47;py310&#47;lib&#47;python3.10&#47;site-packages&#47;gradio&#47;blocks.py&quot;, line 1078, in process_api\n    data = self.postprocess_data(fn_index, result[&quot;prediction&quot;], state)\n  File &quot;&#47;Users&#47;hupengfei&#47;miniconda3&#47;envs&#47;py310&#47;lib&#47;python3.10&#47;site-packages&#47;gradio&#47;blocks.py&quot;, line 1012, in postprocess_data\n    prediction_value = block.postprocess(prediction_value)\n  File &quot;&#47;Users&#47;hupengfei&#47;miniconda3&#47;envs&#47;py310&#47;lib&#47;python3.10&#47;site-packages&#47;gradio&#47;components.py&quot;, line 4181, in postprocess\n    self._process_chat_messages(message_pair[1]),\n  File &quot;&#47;Users&#47;hupengfei&#47;miniconda3&#47;envs&#47;py310&#47;lib&#47;python3.10&#47;site-packages&#47;gradio&#47;components.py&quot;, line 4154, in _process_chat_messages\n    raise ValueError(f&quot;Invalid message for Chatbot component: {chat_message}&quot;)\nValueError: Invalid message for Chatbot component: module &#39;openai&#39; has no attribute &#39;ChatCompletion&#39;\npip show openai\n\n发送的时候报了这个错误~  ","like_count":0},{"had_liked":false,"id":373771,"user_name":"Geek_378f83","can_delete":false,"product_type":"c1","uid":3196784,"ip_address":"上海","ucode":"AD2B400EED436A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/z3nIITkhzRj9WApibhic524lQSJmqUCerpuqpntEe06LE3lRGKr6rGwPpPJtZ5Xj0NBPyvTatKtIk0hfyAnl9Hsg/132","comment_is_top":false,"comment_ctime":1683131256,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"7","like_count":0},{"had_liked":false,"id":372908,"user_name":"aoe","can_delete":false,"product_type":"c1","uid":1121758,"ip_address":"浙江","ucode":"1C6201EDB4E954","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","comment_is_top":false,"comment_ctime":1681791628,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":4,"product_id":100541001,"comment_content":"虽然没看懂，但觉得很厉害！\n原来调用 openAI 的 API 不像我想象的那么简单，还是需要一些「机器学习」的基础知识\n感谢老师带我参观「机器学习入门」","like_count":0},{"had_liked":false,"id":371694,"user_name":"Ethan New","can_delete":false,"product_type":"c1","uid":2063962,"ip_address":"浙江","ucode":"9CA2EF39E58030","user_header":"https://static001.geekbang.org/account/avatar/00/1f/7e/5a/da39f489.jpg","comment_is_top":false,"comment_ctime":1680188885,"is_pvip":true,"replies":null,"discussion_count":0,"race_medal":4,"score":4,"product_id":100541001,"comment_content":"学习打卡","like_count":0}]}