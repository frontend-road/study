{"id":357885,"title":"10 | 调优一筹莫展，配置项速查手册让你事半功倍！（下）","content":"<p>你好，我是吴磊。</p><p>上一讲，我们讲了硬件资源类的配置项。这一讲，我们继续说说Shuffle类和Spark SQL大类都有哪些配置项，它们的含义和作用，以及它们能解决的问题。同时，和上一讲一样，我们今天讲到的配置项也全部会围绕Executors展开。</p><h2>Shuffle类配置项</h2><p>首先，我们来说说Shuffle类。纵观Spark官网的<a href=\"http://spark.apache.org/docs/latest/configuration.html\">Configuration页面</a>，你会发现能调节Shuffle执行性能的配置项真是寥寥无几。其实这也很好理解，因为一旦Shuffle成为应用中不可或缺的一环，想要优化Shuffle本身的性能，我们能做的微乎其微。</p><p>不过，我们也不是完全束手无策。我们知道，Shuffle的计算过程分为Map和Reduce这两个阶段。其中，Map阶段执行映射逻辑，并按照Reducer的分区规则，将中间数据写入到本地磁盘；Reduce阶段从各个节点下载数据分片，并根据需要实现聚合计算。</p><p>那么，我们就可以通过spark.shuffle.file.buffer和spark.reducer.maxSizeInFlight这两个配置项，来分别调节Map阶段和Reduce阶段读写缓冲区的大小。具体该怎么做呢？我们一一来看。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/ab/50/ab65b81ffe85e61b186b1fb3e8620750.jpeg?wh=1544*550\" alt=\"\" title=\"缓冲区相关配置项\"></p><p>首先，在Map阶段，计算结果会以中间文件的形式被写入到磁盘文件系统。同时，为了避免频繁的I/O操作，Spark会把中间文件存储到写缓冲区（Write Buffer）。<strong>这个时候，我们可以通过设置spark.shuffle.file.buffer来扩大写缓冲区的大小，缓冲区越大，能够缓存的落盘数据越多，Spark需要刷盘的次数就越少，I/O效率也就能得到整体的提升。</strong></p><p>其次，在Reduce阶段，因为Spark会通过网络从不同节点的磁盘中拉取中间文件，它们又会以数据块的形式暂存到计算节点的读缓冲区（Read Buffer）。缓冲区越大，可以暂存的数据块越多，在数据总量不变的情况下，拉取数据所需的网络请求次数越少，单次请求的网络吞吐越高，网络I/O的效率也就越高。<strong>这个时候，我们就可以通过spark.reducer.maxSizeInFlight配置项控制Reduce端缓冲区大小，来调节Shuffle过程中的网络负载。</strong></p><p>事实上，对Shuffle计算过程的优化牵扯到了全部的硬件资源，包括CPU、内存、磁盘和网络。因此，我们上一讲汇总的关于CPU、内存和硬盘的配置项，也同样可以作用在Map和Reduce阶段的内存计算过程上。</p><p>除此之外，Spark还提供了一个叫做<strong>spark.shuffle.sort.bypassMergeThreshold</strong>的配置项，去处理一种特殊的Shuffle场景。</p><p><img src=\"https://static001.geekbang.org/resource/image/49/71/49yy9eff7b5521f0d51f9451252e0a71.jpeg?wh=1628*448\" alt=\"\" title=\"Reduce端相关配置项\"></p><p>自1.6版本之后，Spark统一采用Sort shuffle manager来管理Shuffle操作，在Sort shuffle manager的管理机制下，无论计算结果本身是否需要排序，Shuffle计算过程在Map阶段和Reduce阶段都会引入排序操作。</p><p>这样的实现机制对于repartition、groupBy这些操作就不太公平了，这两个算子一个是对原始数据集重新划分分区，另一个是对数据集进行分组，压根儿就没有排序的需求。所以，Sort shuffle manager实现机制引入的排序步骤反而变成了一种额外的计算开销。</p><p>因此，<strong>在不需要聚合，也不需要排序的计算场景中，我们就可以通过设置spark.shuffle.sort.bypassMergeThreshold的参数，来改变Reduce端的并行度</strong>（默认值是200）。当Reduce端的分区数小于这个设置值的时候，我们就能避免Shuffle在计算过程引入排序。</p><h2>Spark SQL大类配置项</h2><p>接下来，我们再来说说Spark SQL的相关配置项。在官网的<a href=\"http://spark.apache.org/docs/latest/configuration.html\">Configuration页面</a>中，Spark SQL下面的配置项还是蛮多的，其中对执行性能贡献最大的，当属AQE（Adaptive query execution，自适应查询引擎）引入的那3个特性了，也就是自动分区合并、自动数据倾斜处理和Join策略调整。因此，关于Spark SQL的配置项，咱们围绕着这3个特性去汇总。</p><p>首先我们要知道，<strong>AQE功能默认是禁用的，想要使用这些特性，我们需要先通过配置项spark.sql.adaptive.enabled来开启AQE</strong>，具体的操作如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/5b/b39cdde3cd466b5fba4cdyy01386eb5b.jpeg?wh=1832*430\" alt=\"\" title=\"启用AQE的配置项\"></p><p>因为这3个特性的原理我们在开发原则那一讲说过，这里我会先带你简单回顾一下，然后我们重点来讲，这些环节对应的配置项有哪些。</p><h3>哪些配置项与自动分区合并有关？</h3><p>分区合并的场景用一句概括就是，在Shuffle过程中，因为数据分布不均衡，导致Reduce阶段存在大量的小分区，这些小分区的数据量非常小，调度成本很高。</p><p>那么问题来了，AQE是如何判断某个分区是不是足够小，到底需不需要合并的呢？另外，既然是对多个分区进行合并，自然就存在一个收敛条件的问题，如果一直不停地合并下去，整个分布式数据集最终就会合并为一个超级大的分区。简单来说，就是：“分区合并从哪里开始，又到哪里结束呢？”</p><p><img src=\"https://static001.geekbang.org/resource/image/da/4f/dae9dc8b90c2d5e0cf77180ac056a94f.jpg?wh=4359*1623\" alt=\"\" title=\"分区合并示意图\"></p><p>我们一起来看一下AQE分区合并的工作原理。如上图所示，对于所有的数据分区，无论大小，AQE按照分区编号从左到右进行扫描，边扫描边记录分区尺寸，当相邻分区的尺寸之和大于“目标尺寸”时，AQE就把这些扫描过的分区进行合并。然后，继续向右扫描，并采用同样的算法，按照目标尺寸合并剩余分区，直到所有分区都处理完毕。</p><p>总的来说就是，<strong>AQE事先并不判断哪些分区足够小，而是按照分区编号进行扫描，当扫描量超过“目标尺寸”时，就合并一次</strong>。我们发现，这个过程中的关键就是“目标尺寸”的确定，它的大小决定了合并之后分布式数据集的分散程度。</p><p>那么，“目标尺寸”由什么来决定的呢？Spark提供了两个配置项来共同决定分区合并的“目标尺寸”，它们分别是spark.sql.adaptive.advisoryPartitionSizeInBytes和spark.sql.adaptive.coalescePartitions.minPartitionNum。</p><p><img src=\"https://static001.geekbang.org/resource/image/62/95/620fde8bd55cd9e937ebc31060936395.jpeg?wh=1920*639\" alt=\"\" title=\"AQE自动分区合并相关配置项\"></p><p>其中，第一个参数advisoryPartitionSizeInBytes是开发者建议的目标尺寸，第二个参数minPartitionNum的含义是合并之后的最小分区数，假设它是200，就说明合并之后的分区数量不能小于200。这个参数的目的就是避免并行度过低导致CPU资源利用不充分。</p><p>结合Shuffle后的数据集尺寸和最小分区数限制，我们可以反推出来每个分区的平均大小，咱们暂且把它记为#partitionSize。分区合并的目标尺寸取advisoryPartitionSizeInBytes与#partitionSize之间的最小值。</p><p>这么说比较抽象，我们来举个例子。假设，Shuffle过后数据大小为20GB，minPartitionNum设置为200，反推过来，每个分区的尺寸就是20GB / 200 = 100MB。再假设，advisoryPartitionSizeInBytes设置为200MB，最终的目标分区尺寸就是取（100MB，200MB）之间的最小值，也就是100MB。因此你看，并不是你指定了advisoryPartitionSizeInBytes是多少，Spark就会完全尊重你的意见，我们还要考虑minPartitionNum的设置。</p><h3>哪些配置项与自动数据倾斜处理有关？</h3><p>再来说说数据倾斜，在数据关联（Data Joins）的场景中，当AQE检测到倾斜的数据分区时，会自动进行拆分操作，把大分区拆成多个小分区，从而避免单个任务的数据处理量过大。不过，Spark 3.0版本发布的AQE，暂时只能在Sort Merge Join中自动处理数据倾斜，其他的Join实现方式如Shuffle Join还有待支持。</p><p>那么，AQE如何判定数据分区是否倾斜呢？它又是怎么把大分区拆分成多个小分区的？</p><p><img src=\"https://static001.geekbang.org/resource/image/3c/d6/3cd86b383909ecb2577d3839edfe2dd6.jpeg?wh=1920*701\" alt=\"\" title=\"AQE数据倾斜处理相关配置项\"></p><p>首先，<strong>分区尺寸必须要大于spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes参数的设定值，才有可能被判定为倾斜分区。然后，AQE统计所有数据分区大小并排序，取中位数作为放大基数，尺寸大于中位数一定倍数的分区会被判定为倾斜分区，中位数的放大倍数也是由参数spark.sql.adaptive.skewJoin.skewedPartitionFactor控制。</strong></p><p>接下来，我们还是通过一个例子来理解。假设数据表A有3个分区，分区大小分别是80MB、100MB和512MB。显然，这些分区按大小个排序后的中位数是100MB，因为skewedPartitionFactor的默认值是5倍，所以大于100MB * 5 = 500MB的分区才有可能被判定为倾斜分区。在我们的例子中，只有最后一个尺寸是512MB的分区符合这个条件。</p><p>这个时候，Spark还不能完全判定它就是倾斜分区，还要看skewedPartitionThresholdInBytes配置项，这个参数的默认值是256MB。对于那些满足中位数条件的分区，必须要大于256MB，Spark才会把这个分区最终判定为倾斜分区。假设skewedPartitionThresholdInBytes设定为1GB，那在我们的例子中，512MB那个大分区，Spark也不会把它看成是倾斜分区，自然也就不能享受到AQE对于数据倾斜的优化处理。</p><p>检测到倾斜分区之后，接下来就是对它拆分，拆分的时候还会用到advisoryPartitionSizeInBytes参数。假设我们将这个参数的值设置为256MB，那么，刚刚那个512MB的倾斜分区会以256MB为粒度拆分成多份，因此，这个大分区会被拆成  2 个小分区（ 512MB / 256MB =2）。拆分之后，原来的数据表就由3个分区变成了4个分区，每个分区的尺寸都不大于256MB。</p><h3>哪些配置项与Join策略调整有关？</h3><p>最后，咱们再来说说数据关联（Joins）。数据关联可以说是数据分析领域中最常见的操作，Spark SQL中的Join策略调整，它实际上指的是，把会引入Shuffle的Join方式，如Hash Join、Sort Merge Join，“降级”（Demote）为Broadcast Join。</p><p><strong>Broadcast Join的精髓在于“以小博大”，它以广播的方式将小表的全量数据分发到集群中所有的Executors，大表的数据不需要以Join keys为基准去Shuffle，就可以与小表数据原地进行关联操作。</strong>Broadcast Join以小表的广播开销为杠杆，博取了因消除大表Shuffle而带来的巨大性能收益。可以说，Broadcast Join把“杠杆原理”应用到了极致。</p><p>在Spark发布AQE之前，开发者可以利用spark.sql.autoBroadcastJoinThreshold配置项对数据关联操作进行主动降级。这个参数的默认值是10MB，参与Join的两张表中只要有一张数据表的尺寸小于10MB，二者的关联操作就可以降级为Broadcast Join。为了充分利用Broadcast Join“以小博大”的优势，你可以考虑把这个参数值调大一些，2GB左右往往是个不错的选择。</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/61/d082a82b24e2f5ecc484d849abe2e361.jpeg?wh=1733*448\" alt=\"\" title=\"AQE推出之前Join策略相关配置项\"></p><p>不过，autoBroadcastJoinThreshold这个参数虽然好用，但是有两个让人头疼的短板。</p><p>一是可靠性较差。尽管开发者明确设置了广播阈值，而且小表数据量在阈值以内，但Spark对小表尺寸的误判时有发生，导致Broadcast Join降级失败。</p><p>二来，预先设置广播阈值是一种静态的优化机制，它没有办法在运行时动态对数据关联进行降级调整。一个典型的例子是，两张大表在逻辑优化阶段都不满足广播阈值，此时Spark SQL在物理计划阶段会选择Shuffle Joins。但在运行时期间，其中一张表在Filter操作之后，有可能出现剩余的数据量足够小，小到刚好可以降级为Broadcast Join。在这种情况下，静态优化机制就是无能为力的。</p><p>AQE很好地解决了这两个头疼的问题。<strong>首先，AQE的Join策略调整是一种动态优化机制，对于刚才的两张大表，AQE会在数据表完成过滤操作之后动态计算剩余数据量，当数据量满足广播条件时，AQE会重新调整逻辑执行计划，在新的逻辑计划中把Shuffle Joins降级为Broadcast Join。再者，运行时的数据量估算要比编译时准确得多，因此AQE的动态Join策略调整相比静态优化会更可靠、更稳定。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/db/5b/db78a727a8fc6fc6da0cdecaa5ba755b.jpeg?wh=1920*407\" alt=\"\" title=\"AQE推出之后Join策略相关配置项\"></p><p>不过，启用动态Join策略调整还有个前提，也就是要满足nonEmptyPartitionRatioForBroadcastJoin参数的限制。这个参数的默认值是0.2，大表过滤之后，非空的数据分区占比要小于0.2，才能成功触发Broadcast Join降级。</p><p>这么说有点绕，我们来举个例子。假设，大表过滤之前有100个分区，Filter操作之后，有85个分区内的数据因为不满足过滤条件，在过滤之后都变成了没有任何数据的空分区，另外的15个分区还保留着满足过滤条件的数据。这样一来，这张大表过滤之后的非空分区占比是 15 / 100 = 15%，因为15%小于0.2，所以这个例子中的大表会成功触发Broadcast Join降级。</p><p>相反，如果大表过滤之后，非空分区占比大于0.2，那么剩余数据量再小，AQE也不会把Shuffle Joins降级为Broadcast Join。因此，如果你想要充分利用Broadcast Join的优势，可以考虑把这个参数适当调高。</p><h2>小结</h2><p>今天这一讲，我们深入探讨了Shuffle类和Spark SQL大类两类配置项，以及每个配置项可以解决的问题。</p><p>对于Shuffle类我们要知道，在Shuffle过程中，对于不需要排序和聚合的操作，我们可以通过控制spark.shuffle.sort.bypassMergeThreshold参数，来避免Shuffle执行过程中引入的排序环节，从而避免没必要的计算开销。</p><p>对于Spark SQL大类我们首先要知道，AQE默认是禁用状态，要充分利用AQE提供的3个特性，就是自动分区合并、数据倾斜处理和Join策略调整，我们需要把spark.sql.adaptive.enabled置为true。</p><p>除此之外，AQE的3个特性各自都有相对应的配置项，需要我们单独调整。</p><ul>\n<li>\n<p>AQE中的自动分区合并过程与我们预想的不太一样。QE事先并不判断哪些分区足够小，而是按照分区编号进行扫描，当扫描量超过“目标尺寸”时就合并一次。目标尺寸由advisoryPartitionSizeInBytes和coalescePartitions.minPartitionNum两个参数共同决定。</p>\n</li>\n<li>\n<p>AQE能够自动处理Sort Merge Join场景中的数据倾斜问题。首先根据所有分区大小的中位数，以及放大倍数skewedPartitionFactor来检测倾斜分区，然后以advisoryPartitionSizeInBytes为粒度对倾斜分区进行拆分。</p>\n</li>\n<li>\n<p>AQE动态Join策略调整可以在运行时将Shuffle Joins降级为Broadcast Join，同时，运行时的数据量估算要比编译时准确得多，因此相比静态优化会更可靠。不过，需要我们注意的是，Shuffle过后非空分区占比要小于nonEmptyPartitionRatioForBroadcastJoin才能触发Broadcast Join的降级优化。</p>\n</li>\n</ul><p>好啦，经过这两讲的学习，我们一起汇总出了Spark中与性能调优息息相关的所有配置项，为了方便你快速查阅，我把它们合并在了一张文稿的表格中，希望你能在工作中好好利用起来。</p><p><img src=\"https://static001.geekbang.org/resource/image/31/6a/31356505a2c36bac10de0e06d7e4526a.jpg?wh=1920*2510\" alt=\"\"></p><h2>每日一练</h2><ol>\n<li>AQE的分区合并算法略显简单粗暴，如果让你来重新实现分区合并特性的话，你都有哪些思路呢？</li>\n<li>AQE中数据倾斜的处理机制，你认为有哪些潜在的隐患？</li>\n</ol><p>期待在留言区看到你的思考和答案，也欢迎你把这份调优手册分享给你的朋友们，我们下一讲见！</p>","comments":[{"had_liked":false,"id":287008,"user_name":"来世愿做友人 A","can_delete":false,"product_type":"c1","uid":1181606,"ip_address":"","ucode":"EF20966B0F27E1","user_header":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","comment_is_top":false,"comment_ctime":1617725043,"is_pvip":false,"replies":[{"id":"104227","content":"好问题，思考的很深入，你是第一个问这个问题的同学，先赞一个～<br><br>首先，分区合并是在reduce阶段（这块原文我可能没说清楚，我的锅），也就是每个reduce task把数据拉回来，aqe发现有些task分片小于合并的“目标大小”，这个时候，物理策略Coalesce Shuffle Partitions就开始生效。这部分细节后面的aqe还会展开，这一讲因为focus在配置项，所以没说的特别细。<br><br>然后，对于你说的问题，我们来分类讨论。<br><br>第一种，假设是单表shuffle，比如reduceByKey这种，那么合并之后不影响，即便一个分片有多个key，也不要紧，只要保证同一个key的payload都在一个分区就行。<br><br>第二种，两表join。这个时候就有意思了，你说的担心就会成立，比如，外表大、内表小，外表合并之后，两个分区，第一个分区包含key=1、2、3，第二个分区，包含key=4、5、6；因为内表小，因此内表合并之后，一个分区就包含了1-6。其实，这样不要紧，为啥呢？如果你熟悉join实现原理的话（26讲会展开），在一个进程内，不论是nlj、smj还是hash join，外表，也就是驱动表，会驱动内表的“全量扫描”，带引号是因为效率不一样，smj、hj不用全量，但意思是一样的，就是不管内表有多少分区，都会被外表驱动着去被遍历。因此，不轮内表数据分配到了哪个分区，其实都还是在一个executor进程内，所以，不影响join逻辑，也不影响效率。nlj还是慢，hash join还是快，不会因为数据“看上去”打散了，而影响功能和性能。<br><br>手机徒手打字，可能没说特别清楚，有问题再留言哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617755864,"ip_address":"","comment_id":287008,"utype":1}],"discussion_count":5,"race_medal":0,"score":"78927136371","product_id":100073401,"comment_content":"请教老师一个问题，对于小文件合并，上边说的是某个 executor 的大小排序后合并。比如两个 executorA 和 B，分别有两个 task 运行，groupby 并且各自产生了3个分区，分别是 a.0,a.1,a.2 和 b.0,b.1,b.2，在没有合并小分区的情况下，reduce端会有三个任务拉取各自的012分区。但是，打开小分区合并，在满足合并的条件下，a.0和a.1合并成 a.01，b.1和b.2合并成b.12。这时候两个 task 各有两个分区，但是他们的分区 key 相当于混在一起了。shuffle的 reduce 是怎么拉取。因为目前只看过 raw rdd的相关，目前没想到是怎么解决这个问题的？比如又会多引入一层 shuffle？或者有其它判断，最终判断这次的 reduce 只能有一个 task，然后拉取所有 map 端的分区？","like_count":19,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518163,"discussion_content":"好问题，思考的很深入，你是第一个问这个问题的同学，先赞一个～\n\n首先，分区合并是在reduce阶段（这块原文我可能没说清楚，我的锅），也就是每个reduce task把数据拉回来，aqe发现有些task分片小于合并的“目标大小”，这个时候，物理策略Coalesce Shuffle Partitions就开始生效。这部分细节后面的aqe还会展开，这一讲因为focus在配置项，所以没说的特别细。\n\n然后，对于你说的问题，我们来分类讨论。\n\n第一种，假设是单表shuffle，比如reduceByKey这种，那么合并之后不影响，即便一个分片有多个key，也不要紧，只要保证同一个key的payload都在一个分区就行。\n\n第二种，两表join。这个时候就有意思了，你说的担心就会成立，比如，外表大、内表小，外表合并之后，两个分区，第一个分区包含key=1、2、3，第二个分区，包含key=4、5、6；因为内表小，因此内表合并之后，一个分区就包含了1-6。其实，这样不要紧，为啥呢？如果你熟悉join实现原理的话（26讲会展开），在一个进程内，不论是nlj、smj还是hash join，外表，也就是驱动表，会驱动内表的“全量扫描”，带引号是因为效率不一样，smj、hj不用全量，但意思是一样的，就是不管内表有多少分区，都会被外表驱动着去被遍历。因此，不轮内表数据分配到了哪个分区，其实都还是在一个executor进程内，所以，不影响join逻辑，也不影响效率。nlj还是慢，hash join还是快，不会因为数据“看上去”打散了，而影响功能和性能。\n\n手机徒手打字，可能没说特别清楚，有问题再留言哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617755864,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062864,"avatar":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","nickname":"aof","note":"","ucode":"5815D63C4926BC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":371982,"discussion_content":"其实，就算不合并，一般也是多个key的数据分布在同一个分区的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620115150,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1062864,"avatar":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","nickname":"aof","note":"","ucode":"5815D63C4926BC","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377052,"discussion_content":"没错","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622472203,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":371982,"ip_address":""},"score":377052,"extra":""},{"author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1062864,"avatar":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","nickname":"aof","note":"","ucode":"5815D63C4926BC","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":385304,"discussion_content":"我觉得这个问题的关键是担心由于在map端的合并导致同一个key的数据没有被拉取到同一个执行reduce task的executor中。话说spark好像已经没有reduce task的概念了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626995244,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":371982,"ip_address":""},"score":385304,"extra":""},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1199213,"avatar":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","nickname":"LJK","note":"","ucode":"12B2441099FF1D","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388673,"discussion_content":"reduce task在spark所有版本中都有","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628907321,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":385304,"ip_address":""},"score":388673,"extra":""}]}]},{"had_liked":false,"id":292227,"user_name":"CRT","can_delete":false,"product_type":"c1","uid":1442864,"ip_address":"","ucode":"59F5C8C0674ADC","user_header":"https://static001.geekbang.org/account/avatar/00/16/04/30/7f2cb8e3.jpg","comment_is_top":false,"comment_ctime":1620727915,"is_pvip":false,"replies":[{"id":"105874","content":"好问题，先说为什么参数归类为Reduce端，因为这个参数spark.shuffle.sort.bypassMergeThreshold，它指定的是Reduce阶段的并行度限制，所以归类到Reduce端参数。但它实际控制的，是在Map阶段，能否byPass排序的操作。<br><br>再来说说，为啥Reduce端的并行度为什么不能太大。这个其实怪我没有在本讲交代更多bypass的实现机制。如果Shuffle走bypass的code path，那么对于每一个Task，它的执行过程是：<br>a）为每一个reduce task生成一个临时文件<br>b）为每个临时文件创建写buffer、以及一个serializer对象，用于序列化数据<br>c）保持所有的临时文件打开，将map阶段的数据，按照reduce端partitionId的不同，依次写入到这些临时文件<br>d）最后，map task计算完毕，把所有这些临时文件合并到一起，生成data文件，并同时生成index文件<br><br>因此，你看，如果你的reduce阶段并行度非常的高，那么map task的计算开销会非常大，要同时打开非常多的临时文件、建立非常多的写buffer、新建非常多个serializer，这些都是开销。<br><br>这也是为什么，要启用shuffle的bypass，spark会要求你的reduce并行度，要小于参数的设置：spark.shuffle.sort.bypassMergeThreshold，目的其实就是说：如果你的reduce并行度非常高，那么开启bypass，即便节省了排序的开销，但是因此而引入的诸多额外开销，实际上整体上算下来是划不来的。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620805253,"ip_address":"","comment_id":292227,"utype":1}],"discussion_count":1,"race_medal":0,"score":"70340204651","product_id":100073401,"comment_content":"spark.shuffle.sort.bypassMergeThreshold 这个阈值为什么是跟Reduce 端的分区数有关，Reduce 端的分区数过大的话，取消排序会有不好的影响吗？","like_count":15,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519735,"discussion_content":"好问题，先说为什么参数归类为Reduce端，因为这个参数spark.shuffle.sort.bypassMergeThreshold，它指定的是Reduce阶段的并行度限制，所以归类到Reduce端参数。但它实际控制的，是在Map阶段，能否byPass排序的操作。\n\n再来说说，为啥Reduce端的并行度为什么不能太大。这个其实怪我没有在本讲交代更多bypass的实现机制。如果Shuffle走bypass的code path，那么对于每一个Task，它的执行过程是：\na）为每一个reduce task生成一个临时文件\nb）为每个临时文件创建写buffer、以及一个serializer对象，用于序列化数据\nc）保持所有的临时文件打开，将map阶段的数据，按照reduce端partitionId的不同，依次写入到这些临时文件\nd）最后，map task计算完毕，把所有这些临时文件合并到一起，生成data文件，并同时生成index文件\n\n因此，你看，如果你的reduce阶段并行度非常的高，那么map task的计算开销会非常大，要同时打开非常多的临时文件、建立非常多的写buffer、新建非常多个serializer，这些都是开销。\n\n这也是为什么，要启用shuffle的bypass，spark会要求你的reduce并行度，要小于参数的设置：spark.shuffle.sort.bypassMergeThreshold，目的其实就是说：如果你的reduce并行度非常高，那么开启bypass，即便节省了排序的开销，但是因此而引入的诸多额外开销，实际上整体上算下来是划不来的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620805253,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287483,"user_name":"kingcall","can_delete":false,"product_type":"c1","uid":1056982,"ip_address":"","ucode":"508884DC684B5B","user_header":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","comment_is_top":false,"comment_ctime":1617970487,"is_pvip":false,"replies":[{"id":"104433","content":"没错，在超大规模的shuffle里，这几个参数都可以贡献任务执行的稳定性，不过compress我印象中默认是enabled。其他两个参数对于稳定性很有帮助～ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618055960,"ip_address":"","comment_id":287483,"utype":1}],"discussion_count":2,"race_medal":0,"score":"31682741559","product_id":100073401,"comment_content":"其实我们的调优很多都是发生在数据规模比较大的情况下,对于比较大的shuffle 可以对下面的参数进行调节，提高整个shuffle 的健壮性<br><br>spark.shuffle.compress 是否对shuffle 的中间结果进行压缩，如果压缩的话使用`spark.io.compression.codec` 的配置进行压缩<br><br>spark.shuffle.io.maxRetries  io 失败的重试次数，在大型shuffle遇到网络或者GC 问题的时候很有用。<br><br>spark.shuffle.io.retryWait io 失败的时候的等待时间","like_count":8,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518321,"discussion_content":"没错，在超大规模的shuffle里，这几个参数都可以贡献任务执行的稳定性，不过compress我印象中默认是enabled。其他两个参数对于稳定性很有帮助～ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618055960,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":563749,"discussion_content":"啥叫“超大数据集”？ 你业务真的特别大了，早就引入外部shuffle服务了，当然External shuffle service(ESS) 已经是云服务标配了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1650073415,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":297708,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1623742761,"is_pvip":false,"replies":[{"id":"108255","content":"好问题，我觉得几个渠道吧，一个是官方的release notes，一个是databricks的官方博客，再有就是源码啦~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1624009198,"ip_address":"","comment_id":297708,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18803611945","product_id":100073401,"comment_content":"老师，我看Spark 3.1.1的文档，spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin这个参数已经被去除了。。由此想到一个重大问题：像这种重大的参数配置变更老师是怎么第一时间获悉的并跟上版本更新的节奏，而不像我们这样没人告知，只能被动地获悉呢？","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521915,"discussion_content":"好问题，我觉得几个渠道吧，一个是官方的release notes，一个是databricks的官方博客，再有就是源码啦~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624009198,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294570,"user_name":"苏子浩","can_delete":false,"product_type":"c1","uid":2486715,"ip_address":"","ucode":"8A842C9C2E53E8","user_header":"https://static001.geekbang.org/account/avatar/00/25/f1/bb/547b580a.jpg","comment_is_top":false,"comment_ctime":1622001828,"is_pvip":false,"replies":[{"id":"106975","content":"好问题，这里其实有两个话题，我们分别来讨论。不过，首先感谢老弟的补充！这块是我遗漏了，非常感谢！<br><br>先说第一个话题，就是AQE判断倾斜的条件比较苛刻。这块我同意你的说法，就是AQE对于倾斜的判定，非常的谨慎，除了对“倾斜的candidates”有要求，也就是大于skewedPartitionThresholdInBytes，同时还要大于中位数的一定倍数、等等；还对非倾斜的部分也有要求，也就是你补充的那部分。我说说我的理解，AQE之所以采用这种“严进”（也就是不会轻易地断定场景就是倾斜的），根本原因恰恰是你说的第二个话题，也就是倾斜的处理是有性能开销的。比如数据复制的开销、数据分发的开销，等等。所以对于倾斜的判定，AQE采取了比较保守的策略。应该说，策略这种东西，无所谓优劣，因为这要看场景。我们其实很难说，是保守的策略好，还是激进的策略好。这其实要看倾斜处理带来的收益有多大，这就要说到第二个话题。<br><br>也就是倾斜处理的开销问题，其实不管是AQE的自动倾斜处理，还是我们开发者手工处理数据倾斜（可以参考第29讲的“两阶段Shuffle”），实际上在处理倾斜的过程中，都会引入不同程度的计算开销。核心问题在于：数据倾斜带来的负载倾斜是否构成整个关联计算的性能瓶颈？如果这个问题的答案是肯定的，那么AQE的自动倾斜处理也好、开发者的手工处理也罢，就都是值得的。相反，那就成了“费力不讨好”。<br><br>以上是我的一些想法，欢迎老弟继续讨论~ 再次感谢老弟补充完善！","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1622044833,"ip_address":"","comment_id":294570,"utype":1}],"discussion_count":2,"race_medal":0,"score":"18801871012","product_id":100073401,"comment_content":"老师好！我想讨论一下文中“自动数据倾斜处理”部分。其中我们提到“advisoryPartitionSizeInBytes”这个参数。我通过查看源代码发现：拆分的时候我们具体使用的拆分粒度(targetSize)不仅会考虑该参数的数值，同时会考虑非倾斜的分区(non-skewedPartition)的平均大小。用数学表示的话应该是“Math.max(advisortSize, nonSkewSizes.sum &#47; nonSkewSizes.length)”。其中nonSkewSizes表示“所有分区中过滤掉倾斜分区后所剩余分区，其分区大小所构成的列表”。<br>我想表达的是：在‘自动倾斜处理’中所用到的思想与‘自动分区合并’中相似！<br>并不是指定了 advisoryPartitionSizeInBytes 是多少，Spark 就会完全尊重开发者的意见，还要考虑非倾斜分区的平均大小。<br>那么这样来看的话，文中所举的例子“检测到倾斜分区之后，接下来就是对它拆分，拆分的时候还会用到 advisoryPartitionSizeInBytes 参数。假设我们将这个参数的值设置为 256MB，那么，刚刚那个 512MB 的倾斜分区会以 256MB 为粒度拆分成多份，因此，这个大分区会被拆成 2 个小分区（ 512MB &#47; 256MB =2）。拆分之后，原来的数据表就由 3 个分区变成了 4 个分区，每个分区的尺寸都不大于 256MB。“其实在这里其实是比较了Math( ((80+100) &#47; 2),  256) = 256后，我们才最终确定以 256MB 为粒度拆分存在倾斜的分区。<br>接着是我的一点看法，AQE中对于认定倾斜分区的条件看起来非常苛刻，首先要满足该分区的大小高于<br>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes 参数的设定值。同时，取所有数据分区大小排序后的中位数作为放大基数，尺寸大于中位数指定倍数的分区才会被判定为倾斜分区。那么是不是可以看到其实做倾斜分区处理这件事的成本还是很高的。因为在数据关联场景中，如果两边的表都存在数据倾斜的话，会出现笛卡尔积的显现。哪怕是只有一边的表存在数据倾斜，另外一边的分区复制也是不小的开销。在关联场景中涉及到更多的网络开销。以及需要涉及到reducer任务从一个分区中读取部分数据，其中涉及到的数据划分成本很高？<br>其实我看到的第一反应是想到了Shuffle Hash Join激活的先决条件，感觉激活的条件都非常苛刻。","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520719,"discussion_content":"好问题，这里其实有两个话题，我们分别来讨论。不过，首先感谢老弟的补充！这块是我遗漏了，非常感谢！\n\n先说第一个话题，就是AQE判断倾斜的条件比较苛刻。这块我同意你的说法，就是AQE对于倾斜的判定，非常的谨慎，除了对“倾斜的candidates”有要求，也就是大于skewedPartitionThresholdInBytes，同时还要大于中位数的一定倍数、等等；还对非倾斜的部分也有要求，也就是你补充的那部分。我说说我的理解，AQE之所以采用这种“严进”（也就是不会轻易地断定场景就是倾斜的），根本原因恰恰是你说的第二个话题，也就是倾斜的处理是有性能开销的。比如数据复制的开销、数据分发的开销，等等。所以对于倾斜的判定，AQE采取了比较保守的策略。应该说，策略这种东西，无所谓优劣，因为这要看场景。我们其实很难说，是保守的策略好，还是激进的策略好。这其实要看倾斜处理带来的收益有多大，这就要说到第二个话题。\n\n也就是倾斜处理的开销问题，其实不管是AQE的自动倾斜处理，还是我们开发者手工处理数据倾斜（可以参考第29讲的“两阶段Shuffle”），实际上在处理倾斜的过程中，都会引入不同程度的计算开销。核心问题在于：数据倾斜带来的负载倾斜是否构成整个关联计算的性能瓶颈？如果这个问题的答案是肯定的，那么AQE的自动倾斜处理也好、开发者的手工处理也罢，就都是值得的。相反，那就成了“费力不讨好”。\n\n以上是我的一些想法，欢迎老弟继续讨论~ 再次感谢老弟补充完善！","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1622044833,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2486715,"avatar":"https://static001.geekbang.org/account/avatar/00/25/f1/bb/547b580a.jpg","nickname":"苏子浩","note":"","ucode":"8A842C9C2E53E8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":376383,"discussion_content":"谢谢老师的回复！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1622102852,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291110,"user_name":"天翼","can_delete":false,"product_type":"c1","uid":2459638,"ip_address":"","ucode":"82153820B786E0","user_header":"https://static001.geekbang.org/account/avatar/00/25/87/f6/bc199560.jpg","comment_is_top":false,"comment_ctime":1620032084,"is_pvip":false,"replies":[{"id":"105478","content":"好问题，看的很细~ 你是第一个深究这个参数的同学，赞一个~ 👍<br><br>这个是我的锅，我没有交代清楚，spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin，这是个internal的配置项，Spark社区并没有把它暴露给开发者，不过我们还是可以通过调整这个参数的设置，来调整Spark SQL的优化行为。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620115400,"ip_address":"","comment_id":291110,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18799901268","product_id":100073401,"comment_content":"老师，请问一下，spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin 这个配置在官网的 Configuration 中没有找到，是改名了吗？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519376,"discussion_content":"好问题，看的很细~ 你是第一个深究这个参数的同学，赞一个~ 👍\n\n这个是我的锅，我没有交代清楚，spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin，这是个internal的配置项，Spark社区并没有把它暴露给开发者，不过我们还是可以通过调整这个参数的设置，来调整Spark SQL的优化行为。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620115400,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311190,"user_name":"木子中心","can_delete":false,"product_type":"c1","uid":1712255,"ip_address":"","ucode":"723FB2172CAA5B","user_header":"https://static001.geekbang.org/account/avatar/00/1a/20/7f/6ce7762d.jpg","comment_is_top":false,"comment_ctime":1631095864,"is_pvip":false,"replies":[{"id":"112910","content":"可以参考刚刚留言的那个预估公式：<br><br>val df: DataFrame = _<br>df.cache.count <br><br>val plan = df.queryExecution.logical<br>val estimated: BigInt = spark.sessionState.executePlan(plan).optimizedPlan.stats.sizeInBytes<br><br>这里estimated，就是数据集在内存中的预估大小，你可以参考这个数值，来设置Driver内存~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631345029,"ip_address":"","comment_id":311190,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10221030456","product_id":100073401,"comment_content":"老师好，有一个问题：文章里面说建议设置spark.sql.autoBroadcastJoinThreshold为2G，如果数据是parquet格式的话，将数据加载到内存中会膨胀比较大，这个时候，driver端内存应该配置多少才能不oom呢？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526514,"discussion_content":"可以参考刚刚留言的那个预估公式：\n\nval df: DataFrame = _\ndf.cache.count \n\nval plan = df.queryExecution.logical\nval estimated: BigInt = spark.sessionState.executePlan(plan).optimizedPlan.stats.sizeInBytes\n\n这里estimated，就是数据集在内存中的预估大小，你可以参考这个数值，来设置Driver内存~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631345029,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":296631,"user_name":"续费专用","can_delete":false,"product_type":"c1","uid":2028938,"ip_address":"","ucode":"1B585A131B64B4","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/8a/dc9a23a1.jpg","comment_is_top":false,"comment_ctime":1623073419,"is_pvip":false,"replies":[{"id":"107795","content":"并不是哈~ 对于新特性来说，社区往往会偏保守一些，默认关闭我理解主要是一种态度，就是社区推荐大家使用AQE，但是不会一上来就强制大家使用AQE，因此开关默认是关闭的，对于感兴趣的开发者和用户，大家自行打开开关就好啦~ <br><br>DPP也是一样的，虽然DPP没什么开关控制，不过呢，你知道，DPP的触发条件，其实很苛刻，这个我们在第25讲会详细展开，到时候可以关注下哈~ 简单概括呢，就是DPP至少要满足3个条件：<br>1）事实表必须是分区表，而且分区字段（可以是多个）必须包含 Join Key<br>2）DPP 仅支持等值 Joins，不支持大于、小于这种不等值关联关系<br>3）维度表过滤之后的数据集要小于广播阈值<br><br>最要命的是，DPP的分区键和Join Keys，本身就是一对矛盾。分区键要求列的Cardinality不能太高，而Join Keys往往是那些Cardinality很高的字段，比如userId、orderId等等。所以，虽然DPP没有配置项进行限制，但是想要触发DPP的优化机制，其实没有想象的那么容易~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1623145207,"ip_address":"","comment_id":296631,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10213008011","product_id":100073401,"comment_content":"老师好，动态分区裁剪和AQE哪个对spark的优化性能更大呢？AQE默认是关闭的，是不是说明spark更推荐用户使用动态分区裁剪功能呢？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":521529,"discussion_content":"并不是哈~ 对于新特性来说，社区往往会偏保守一些，默认关闭我理解主要是一种态度，就是社区推荐大家使用AQE，但是不会一上来就强制大家使用AQE，因此开关默认是关闭的，对于感兴趣的开发者和用户，大家自行打开开关就好啦~ \n\nDPP也是一样的，虽然DPP没什么开关控制，不过呢，你知道，DPP的触发条件，其实很苛刻，这个我们在第25讲会详细展开，到时候可以关注下哈~ 简单概括呢，就是DPP至少要满足3个条件：\n1）事实表必须是分区表，而且分区字段（可以是多个）必须包含 Join Key\n2）DPP 仅支持等值 Joins，不支持大于、小于这种不等值关联关系\n3）维度表过滤之后的数据集要小于广播阈值\n\n最要命的是，DPP的分区键和Join Keys，本身就是一对矛盾。分区键要求列的Cardinality不能太高，而Join Keys往往是那些Cardinality很高的字段，比如userId、orderId等等。所以，虽然DPP没有配置项进行限制，但是想要触发DPP的优化机制，其实没有想象的那么容易~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623145207,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288608,"user_name":"耳东","can_delete":false,"product_type":"c1","uid":1266059,"ip_address":"","ucode":"70F69C219EF10B","user_header":"https://static001.geekbang.org/account/avatar/00/13/51/8b/29ed1c41.jpg","comment_is_top":false,"comment_ctime":1618560308,"is_pvip":false,"replies":[{"id":"104778","content":"大小是相对的哈，参与Join的两张表，如果左右表的尺寸相差3倍以上，就可以考虑用“大表Join小表”的思路来解决这类关联问题了。我们在后面的第27讲，专门讲解“大表Join小表”的各种优化思路。<br><br>你说的情况，是和广播阈值作对比，就是能放进广播变量的，都可以算是小表。<br><br>所以，回答你的问题，10M我觉得算是小表了，和数据条目的数量没有关系，和整个数据集的内存存储大小有关系。Spark不会限制说，你的数据集条目超过了多少行，就不能做优化，主要还是看存储大小。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618637720,"ip_address":"","comment_id":288608,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10208494900","product_id":100073401,"comment_content":"问下老师，如果某个表的大小是10m，但是它的条数超过了1千万条，这种表属不属于小表","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518683,"discussion_content":"大小是相对的哈，参与Join的两张表，如果左右表的尺寸相差3倍以上，就可以考虑用“大表Join小表”的思路来解决这类关联问题了。我们在后面的第27讲，专门讲解“大表Join小表”的各种优化思路。\n\n你说的情况，是和广播阈值作对比，就是能放进广播变量的，都可以算是小表。\n\n所以，回答你的问题，10M我觉得算是小表了，和数据条目的数量没有关系，和整个数据集的内存存储大小有关系。Spark不会限制说，你的数据集条目超过了多少行，就不能做优化，主要还是看存储大小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618637720,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":563750,"discussion_content":"胡说八道，动点脑子好不好。 10BB 存储，怎么放下10M+数据的，一条数据不到一个字节？？？？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650073524,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288007,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1618273262,"is_pvip":false,"replies":[{"id":"104593","content":"爱莫能助，得升级到3.0~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618307227,"ip_address":"","comment_id":288007,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10208207854","product_id":100073401,"comment_content":"这个aqe规则是在3.0版本才有的，但是我公司目前用的版本是2.2，有什么其他的参数设置吗","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518485,"discussion_content":"爱莫能助，得升级到3.0~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618307227,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286877,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1617664012,"is_pvip":false,"replies":[{"id":"104158","content":"内存资源宝贵且有限，这里多用一点，那里就少用一点。所以这里的缓存大了，其他地方比如执行、rdd cache、user memory就少了。<br><br>另外，buffer也并不是越大越好。它的主要作用是减少落盘和网络请求（IOPS）次数，但实际上，整体的数据量还是那么大，需要写入、拉取的数据，还是那么多，因此，不能指望把这两个buffer调大来显著提升执行效率。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617671135,"ip_address":"","comment_id":286877,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10207598604","product_id":100073401,"comment_content":"spark.shuffle.file.buffer 和 spark.reducer.maxSizeInFlight 这两个配置项听起来应该越大越好，这样可以降低落盘的频次和数据shuffle的频次。请问老师这两个参数如果调太大会有什么副作用呢？","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518117,"discussion_content":"内存资源宝贵且有限，这里多用一点，那里就少用一点。所以这里的缓存大了，其他地方比如执行、rdd cache、user memory就少了。\n\n另外，buffer也并不是越大越好。它的主要作用是减少落盘和网络请求（IOPS）次数，但实际上，整体的数据量还是那么大，需要写入、拉取的数据，还是那么多，因此，不能指望把这两个buffer调大来显著提升执行效率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617671135,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1056982,"avatar":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","nickname":"kingcall","note":"","ucode":"508884DC684B5B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366144,"discussion_content":"Maximum size of map outputs to fetch simultaneously from each reduce task, in MiB unless otherwise specified. Since each output requires us to create a buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory.\n\n官网关于shuffle 的第一个参数就是 spark.reducer.maxSizeInFlight，但是明显说不建议这个值过大，因为每个output 都需要一个这样的buffer","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1617969804,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311189,"user_name":"木子中心","can_delete":false,"product_type":"c1","uid":1712255,"ip_address":"","ucode":"723FB2172CAA5B","user_header":"https://static001.geekbang.org/account/avatar/00/1a/20/7f/6ce7762d.jpg","comment_is_top":false,"comment_ctime":1631095695,"is_pvip":false,"replies":[{"id":"112909","content":"对，确实有这个问题。<br><br>这块其实后面广播那几讲会有介绍。就是如果RDD、DataFrame没有压缩，Spark判断的存储大小，是数据集对应的源文件在文件系统上面的磁盘存储大小。如果用Parquet、ORC这种压缩比比较高的数据，就容易出现你说的问题。<br><br>我们后面会有一个数据集大小预估办法，你可以参考那个办法，来预估数据集在内存中的大小~<br><br>具体来说：<br>val df: DataFrame = _<br>df.cache.count <br><br>val plan = df.queryExecution.logical<br>val estimated: BigInt = spark.sessionState.executePlan(plan).optimizedPlan.stats.sizeInBytes","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631344973,"ip_address":"","comment_id":311189,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5926062991","product_id":100073401,"comment_content":"吴老师好！有个问题想请教下：<br>    spark广播join经常造成driver oom，spark.sql.autoBroadcastJoinThreshold使用默认值，driver为4-5G左右，文件存储格式为parquet。查阅资料发现spark是直接通过扫描文件的总大小及多少列来判断是否小于阈值，进行广播。由于使用了parquet格式，在扫描少数列的情况下，由于压缩率较高，在某些情况下，上百万数据的结果集也进行广播，造成driver段oom。","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526513,"discussion_content":"对，确实有这个问题。\n\n这块其实后面广播那几讲会有介绍。就是如果RDD、DataFrame没有压缩，Spark判断的存储大小，是数据集对应的源文件在文件系统上面的磁盘存储大小。如果用Parquet、ORC这种压缩比比较高的数据，就容易出现你说的问题。\n\n我们后面会有一个数据集大小预估办法，你可以参考那个办法，来预估数据集在内存中的大小~\n\n具体来说：\nval df: DataFrame = _\ndf.cache.count \n\nval plan = df.queryExecution.logical\nval estimated: BigInt = spark.sessionState.executePlan(plan).optimizedPlan.stats.sizeInBytes","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631344973,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291496,"user_name":"Alan","can_delete":false,"product_type":"c1","uid":2115316,"ip_address":"","ucode":"591A28E310A8F5","user_header":"https://static001.geekbang.org/account/avatar/00/20/46/f4/93b1275b.jpg","comment_is_top":false,"comment_ctime":1620308747,"is_pvip":false,"replies":[{"id":"105588","content":"这里有些混乱哈~ <br><br>spark.sql.adaptive.coalescePartitions.minPartitionNum这个配置项是和自动分区合并相关的，和倾斜处理没什么关系。<br><br>这三个配置项spark.sql.adaptive.skewJoin.skewedPartitionFactor、spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes、spark.sql.adaptive.advisoryPartitionSizeInBytes 确实很重要，前面两个决定是否倾斜，后面一个决定倾斜分区split之后，目标大小是多少。<br><br>关于你说的Shuffle Reader，这个就是常规的Shuffle Reader，指代的是Shuffle的读取阶段，也就是Reduce阶段，这个阶段，不会再引入额外的二次Shuffle了，否则自动倾斜处理特性的设计和实现复杂度都会呈指数级上升。<br><br>自动倾斜的隐患或者说缺陷，主要在于倾斜的处理以Task为粒度拆分的，并没有以Executors为粒度拆分，因此，存在一个不大不小的隐患是，Executors之间，还会存在负载倾斜。当然这个隐患取决于数据本身的分布，这部分细节我们在第29讲会详细展开，到时候可以关注下哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620314671,"ip_address":"","comment_id":291496,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5915276043","product_id":100073401,"comment_content":"2、看似AQE 数据倾斜策略确实不错，但是数据倾斜优化后的sort merge join，使用skew Shuffle reader，也就是存在shuffle的操作，会影响总体的性能，所以skewedPartitionFactor、spark.sql.adaptive.advisoryPartitionSizeInBytes 和 spark.sql.adaptive.coalescePartitions.minPartitionNum两项值设置的合理性，非常重要，最好数据量刚好适用，整数倍最好！","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519485,"discussion_content":"这里有些混乱哈~ \n\nspark.sql.adaptive.coalescePartitions.minPartitionNum这个配置项是和自动分区合并相关的，和倾斜处理没什么关系。\n\n这三个配置项spark.sql.adaptive.skewJoin.skewedPartitionFactor、spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes、spark.sql.adaptive.advisoryPartitionSizeInBytes 确实很重要，前面两个决定是否倾斜，后面一个决定倾斜分区split之后，目标大小是多少。\n\n关于你说的Shuffle Reader，这个就是常规的Shuffle Reader，指代的是Shuffle的读取阶段，也就是Reduce阶段，这个阶段，不会再引入额外的二次Shuffle了，否则自动倾斜处理特性的设计和实现复杂度都会呈指数级上升。\n\n自动倾斜的隐患或者说缺陷，主要在于倾斜的处理以Task为粒度拆分的，并没有以Executors为粒度拆分，因此，存在一个不大不小的隐患是，Executors之间，还会存在负载倾斜。当然这个隐患取决于数据本身的分布，这部分细节我们在第29讲会详细展开，到时候可以关注下哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620314671,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2115316,"avatar":"https://static001.geekbang.org/account/avatar/00/20/46/f4/93b1275b.jpg","nickname":"Alan","note":"","ucode":"591A28E310A8F5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372435,"discussion_content":"感谢🙏老师解答，避免自己掉入误区了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620314734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290733,"user_name":"小灵芝","can_delete":false,"product_type":"c1","uid":2071355,"ip_address":"","ucode":"6DCB7CF8D5A7F1","user_header":"https://static001.geekbang.org/account/avatar/00/1f/9b/3b/dc3f819f.jpg","comment_is_top":false,"comment_ctime":1619708449,"is_pvip":false,"replies":[{"id":"105417","content":"分别来说哈~<br><br>1. 比如像collect_list这种，也是“不需要聚合，也不需要排序的计算场景”，再比如coalesce(1, shuffle=True)，都属于这种。这两个是我立即就能想到的，我记得我之前好像汇总过类似的操作，等我抽时间找找，给你一份完整的操作列表。<br>2. groupByKey不需要聚合哈，这个是单纯的分组操作，没有聚合。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619863273,"ip_address":"","comment_id":290733,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5914675745","product_id":100073401,"comment_content":"老师您好呀！ 请教2个问题：<br><br>“因此，在不需要聚合，也不需要排序的计算场景中，我们就可以通过设置 spark.shuffle.sort.bypassMergeThreshold 的参数，来改变 Reduce 端的并行度”<br><br>1. 请问还有哪些类似于repartition, groupby 这种“不需要聚合，也不需要排序的计算场景”呢？<br>2. groupby 不是需要聚合的吗？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519274,"discussion_content":"分别来说哈~\n\n1. 比如像collect_list这种，也是“不需要聚合，也不需要排序的计算场景”，再比如coalesce(1, shuffle=True)，都属于这种。这两个是我立即就能想到的，我记得我之前好像汇总过类似的操作，等我抽时间找找，给你一份完整的操作列表。\n2. groupByKey不需要聚合哈，这个是单纯的分组操作，没有聚合。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619863273,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537946,"discussion_content":"Spark SQL 有相关 shuffle 的参数配置吗？ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639276274,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288320,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1618406795,"is_pvip":false,"replies":[{"id":"104699","content":"好问题，因为这个参数spark.shuffle.sort.bypassMergeThreshold，它指定的是Reduce阶段的并行度限制，所以归类到Reduce端参数。<br><br>但它实际控制的，是在Map阶段，能否byPass排序的操作。两者的关系确实有点蛋疼，但是理解就好哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618486978,"ip_address":"","comment_id":288320,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5913374091","product_id":100073401,"comment_content":"文章结尾图片中spark.shuffle.sort.bypassMergeThreshold为什么归类为Reduce端的参数呢？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518589,"discussion_content":"好问题，因为这个参数spark.shuffle.sort.bypassMergeThreshold，它指定的是Reduce阶段的并行度限制，所以归类到Reduce端参数。\n\n但它实际控制的，是在Map阶段，能否byPass排序的操作。两者的关系确实有点蛋疼，但是理解就好哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618486978,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367873,"discussion_content":"Reduce 的并行度，决定了map端的排序。那bypass设置只会影响到map阶段的排序。影响的是map生成临时文件的排序还是归并后的最终文件？ 如果影响最终文件的排序，那么是不是就没有索引文件了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618489160,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1564645,"avatar":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","nickname":"快跑","note":"","ucode":"90ED7E6D40C58E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":377055,"discussion_content":"两者都会影响哈~ 其实没了排序，每个临时文件只需要拼接在一起就行了，不需要归并排序了，最终得到的，还是data和index文件，因为即便是拼接，reduce task也需要知道自己所需的数据，是在data文件中的哪个位置。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1622472712,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":367873,"ip_address":""},"score":377055,"extra":""}]}]},{"had_liked":false,"id":339934,"user_name":"十月","can_delete":false,"product_type":"c1","uid":2955154,"ip_address":"","ucode":"95877CD3753595","user_header":"https://static001.geekbang.org/account/avatar/00/2d/17/92/0af520ef.jpg","comment_is_top":false,"comment_ctime":1648471078,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1648471078","product_id":100073401,"comment_content":"老师，spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin   这个参数在https:&#47;&#47;spark.apache.org&#47;docs&#47;latest&#47;configuration.html#memory-management    lastest=3.2.1 中查询不到？这个参数是在文档中没有体现还是被删除了呢","like_count":0,"discussions":[{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":564411,"discussion_content":"被删除了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650244685,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":336946,"user_name":"欧阳硕","can_delete":false,"product_type":"c1","uid":1815279,"ip_address":"","ucode":"2D027E63EC6C15","user_header":"https://static001.geekbang.org/account/avatar/00/1b/b2/ef/a0b79b16.jpg","comment_is_top":false,"comment_ctime":1646484753,"is_pvip":true,"replies":[{"id":"123310","content":"老弟需要再看看倾斜处理的条件哈，对于你说的例子，Spark会判定：没有倾斜分区。100个分区，99个100m，1个1m，那么中位数是100m，100m*10=1g，大于1g的分区，才是倾斜分区","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1646833664,"ip_address":"","comment_id":336946,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1646484753","product_id":100073401,"comment_content":"AQE 中数据倾斜的处理机制隐患：（假设倍数10，分区大小10m）<br>有一种场景是100个分区99个分区都超过了100m一个分区是1m，按照中位数倍数和分区，那大部分的数据分区都要进行切分，这种极端情况是否会有小文件和调度压力增大的问题么？<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":555282,"discussion_content":"老弟需要再看看倾斜处理的条件哈，对于你说的例子，Spark会判定：没有倾斜分区。100个分区，99个100m，1个1m，那么中位数是100m，100m*10=1g，大于1g的分区，才是倾斜分区","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646833664,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":327177,"user_name":"Geek1185","can_delete":false,"product_type":"c1","uid":2028954,"ip_address":"","ucode":"47BEE492EF4C1A","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f5/9a/63dc81a2.jpg","comment_is_top":false,"comment_ctime":1639968898,"is_pvip":false,"replies":[{"id":"119395","content":"建议先从Cache查起，先不用管BypassMergeSortShuffleWriter这个。<br>1）试试先把Cache去掉，看看应用能不能跑通，先不管性能，起码功能上能跑下来<br>2）定位到如果是Cache问题，再来调整内存配置好了，加大Storage部分存储空间，或是退化到MEMORY_AND_DISK模式","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1640357756,"ip_address":"","comment_id":327177,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639968898","product_id":100073401,"comment_content":"请教老师个问题，我们生产环境一个版本是1.6，有个任务代码逻辑没变，数据量翻倍后一直无法结束，也不报错，UI显示在cache的时候卡住，executor jstack显示一直运行在BypassMergeSortShuffleWriter这块的代码，spark.shuffle.sort.bypassMergeThreshold这个参数并没有配置，老师有什么思路排查或者解决么，期待回复","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":541408,"discussion_content":"建议先从Cache查起，先不用管BypassMergeSortShuffleWriter这个。\n1）试试先把Cache去掉，看看应用能不能跑通，先不管性能，起码功能上能跑下来\n2）定位到如果是Cache问题，再来调整内存配置好了，加大Storage部分存储空间，或是退化到MEMORY_AND_DISK模式","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640357756,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":308724,"user_name":"Sean","can_delete":false,"product_type":"c1","uid":2162751,"ip_address":"","ucode":"69234046BFD81B","user_header":"https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg","comment_is_top":false,"comment_ctime":1629771768,"is_pvip":false,"replies":[{"id":"112505","content":"先来说第一个，并没有这样的逻辑关系哈~ 首先，map端的临时文件溢出，跟buffer没有关系，临时文件的溢出，主要由pairedbuffer和AppendOnlyMap来决定。Map和Reduce buffer主要是数据在网络交换的过程中，可以占用的内存缓存大小，对于网络的传输本身有一定的影响，不过这种影响，与buffer之间的大小，很难形成量化的对应关系。<br><br>第二个没问题，对于spark.local.dir参数，确实把它设置到SSD，甚至是内存文件系统更好~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1630765310,"ip_address":"","comment_id":308724,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1629771768","product_id":100073401,"comment_content":"<br>根据文中的内容有两个问题想要咨询下老师:<br>①老师上文中提到的map端缓冲区和reduce端缓冲区大小,理论上来说,是不是map端的值应该大于或等于reduce端缓冲区大小更优呢?如果map端缓冲区小,表示溢出的文件可能或更多,分散到更多的节点中去,导致reduce端拉取时的io更大,所以调大reduce依然解决不了map端溢写带来的io问题,不知道这样理解是否正确?<br>2.对于磁盘spark.local.dir参数,默认是在&#47;tmp目录下,主要还是依耐于硬件,如果是普通的HDD盘其实没有什么作用,如果有SSD,那可以把目录配置到SSD上才能达到优化的目的。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525602,"discussion_content":"先来说第一个，并没有这样的逻辑关系哈~ 首先，map端的临时文件溢出，跟buffer没有关系，临时文件的溢出，主要由pairedbuffer和AppendOnlyMap来决定。Map和Reduce buffer主要是数据在网络交换的过程中，可以占用的内存缓存大小，对于网络的传输本身有一定的影响，不过这种影响，与buffer之间的大小，很难形成量化的对应关系。\n\n第二个没问题，对于spark.local.dir参数，确实把它设置到SSD，甚至是内存文件系统更好~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1630765310,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294444,"user_name":"快跑","can_delete":false,"product_type":"c1","uid":1564645,"ip_address":"","ucode":"90ED7E6D40C58E","user_header":"https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg","comment_is_top":false,"comment_ctime":1621944788,"is_pvip":false,"replies":[{"id":"106842","content":"这个有两种途径，其实本质都是看Spark SQL的执行计划，一个是通过Spark UI，查看执行计划，UI比较直观，而且执行信息更丰富；另一个办法是用Explain，比如df.explain(&quot;extended&quot;)，它会打印出逻辑计划和物理计划，从中可以看到每一个环节的执行细节~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621952976,"ip_address":"","comment_id":294444,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1621944788","product_id":100073401,"comment_content":"老师你好，设置.shuffle.sort.bypassMergeThreshold参数，能避免shuffle过程中引入排序。<br>1、这个结论怎么能验证呢。主要是想了解从怎样才能看到一个spark任务执行过程中采用的是什么Shuffle。<br>2、我的场景是Hive On Spark，也想看看这个参数在Hive On Spark是否也生效","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520658,"discussion_content":"这个有两种途径，其实本质都是看Spark SQL的执行计划，一个是通过Spark UI，查看执行计划，UI比较直观，而且执行信息更丰富；另一个办法是用Explain，比如df.explain(&amp;quot;extended&amp;quot;)，它会打印出逻辑计划和物理计划，从中可以看到每一个环节的执行细节~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621952976,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537938,"discussion_content":"有解决方法吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639274805,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388681,"discussion_content":"我们也是hive on spark场景，这个场景恶心的地方在于虽然底层计算是跑在spark中，但是很多计算行为都不是spark在做控制，包括绝大多数的spark参数，原因在于它在解析sql确定物理执行计划时使用了hiveserver2来做，spark sql的所有优化特性他都用不了，这个场景是hive社区搞出来的，新特性的开发相比于spark社区是严重滞后的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628909410,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537939,"discussion_content":"有解决方法吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639274812,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":388681,"ip_address":""},"score":537939,"extra":""}]}]},{"had_liked":false,"id":291604,"user_name":"揣瞌睡的叮叮猫儿","can_delete":false,"product_type":"c1","uid":1502804,"ip_address":"","ucode":"9E829D838725D1","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/54/b5011c70.jpg","comment_is_top":false,"comment_ctime":1620379690,"is_pvip":false,"replies":[{"id":"105616","content":"本地也可以做降级测试的，没问题，只要内表小于广播阈值，就可以转换为Broadcast Join；或者过滤后的内表小于广播阈值，AQE会帮你转Broadcast Join。<br><br>spark.sql.shuffle.partitions，这个是Spark SQL中涉及的Shuffle在Reduce阶段的默认并行度，与AQE的Join降级没有关系哈。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620396911,"ip_address":"","comment_id":291604,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620379690","product_id":100073401,"comment_content":"请教老师 spark.sql.shuffle.partitions参数值对 AQE的执行有什么影响？本地构造数据测试Sort Merge Join-&gt;Broadcast Join是否有效，开启AQE后，但spark.sql.shuffle.partitions设置个数会影响是否降级join, 不懂为什么","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519519,"discussion_content":"本地也可以做降级测试的，没问题，只要内表小于广播阈值，就可以转换为Broadcast Join；或者过滤后的内表小于广播阈值，AQE会帮你转Broadcast Join。\n\nspark.sql.shuffle.partitions，这个是Spark SQL中涉及的Shuffle在Reduce阶段的默认并行度，与AQE的Join降级没有关系哈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620396911,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291206,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620116475,"is_pvip":false,"replies":[{"id":"105739","content":"1. 思路确实是这么个思路，不过，具体怎么实现呢？<br><br>2. 其实被判定的倾斜分区数量多，这个倒不打紧，打紧的是，如果倾斜分区都分布在一个、或是少数几个Executors，那么Executors之间的负载倾斜，实际上还是个性能隐患~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620657494,"ip_address":"","comment_id":291206,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1620116475","product_id":100073401,"comment_content":"1. AQE的自动分区合并算法目前的实现，可能会造成合并之后，各分区数据不均衡，从而导致后续计算出现小的数据倾斜？合并算法是不是可以在得出合并前各分区大小之后，进行均衡的组合合并？<br>2. 因为AQE数据倾斜处理机制，是取中位数，那比如有10个分区：80MB  100 100 100 100 512 512 512 512MB，skewedPartitionFactor为5，skewedPartitionThresholdInBytes为256MB。那么这样一来的话，就会造成将近一半的分区被判定为倾斜分区，这种情况下，后续的分区拆分处理是不是代价就比较大了","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519400,"discussion_content":"1. 思路确实是这么个思路，不过，具体怎么实现呢？\n\n2. 其实被判定的倾斜分区数量多，这个倒不打紧，打紧的是，如果倾斜分区都分布在一个、或是少数几个Executors，那么Executors之间的负载倾斜，实际上还是个性能隐患~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620657494,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2855459,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/icJzoUc02ECdBbFgGzVIwYfpRgL3TXuRRE5GsDqZFmAlAAm1KUQS1rHewgj5FB4TChovo3YaceicEZE2MgZJ1ftw/132","nickname":"Geek_eb29a4","note":"","ucode":"CF88D4ED5F4A25","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537943,"discussion_content":"那么Executors之间的负载倾斜，实际上还是个性能隐患   \n这个有什么解决思路吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639276022,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":288143,"user_name":"zxk","can_delete":false,"product_type":"c1","uid":1221195,"ip_address":"","ucode":"4BB2BD9D2BCD04","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","comment_is_top":false,"comment_ctime":1618320155,"is_pvip":false,"replies":[{"id":"104646","content":"Map端的分区合并是个不错的思路~ 不过，这里有很多细节需要考虑，比如，你需要在Map阶段，就提前算出Reduce阶段的分区大小，才能决定说到底要Merge哪些分区；再者，你需要修改BlockManager中维护关于Block的元数据，这样Reduce端在拉取数据的时候，才不会出错，诸如此类。<br><br>第二个问题答得很好，不过这个是“加盐”类操作的通病，就是搞不定需要全局排序的聚合操作，比如各种分位数的计算，当然，包括你说的中位数。我们来换个问法：和“手工加盐”相比，Spark的自动倾斜处理，有哪些性能隐患？不妨再思考一下哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618398003,"ip_address":"","comment_id":288143,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1618320155","product_id":100073401,"comment_content":"问题1：在用户不指定 RDD 大小推荐值或者分区数时，默认情况下我会将合并的推荐大小设置为总的数据量除以所有 executor 的核心数，然后在 map 端对将要 shuffle 到相同 partition 的分片先进行一次合并，减少需要 shuffle 的文件数量。<br>问题2：如果是那些需要某个分片全量数据才能得到正确结果的业务场景，那么自动数据倾斜可能会导致结果不符合预期。比如需要求该分片的某列数据的中位数，那么将该分片拆为两份后，即使后面再做聚合，也无法得到预期的结果。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518528,"discussion_content":"Map端的分区合并是个不错的思路~ 不过，这里有很多细节需要考虑，比如，你需要在Map阶段，就提前算出Reduce阶段的分区大小，才能决定说到底要Merge哪些分区；再者，你需要修改BlockManager中维护关于Block的元数据，这样Reduce端在拉取数据的时候，才不会出错，诸如此类。\n\n第二个问题答得很好，不过这个是“加盐”类操作的通病，就是搞不定需要全局排序的聚合操作，比如各种分位数的计算，当然，包括你说的中位数。我们来换个问法：和“手工加盐”相比，Spark的自动倾斜处理，有哪些性能隐患？不妨再思考一下哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618398003,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1221195,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","nickname":"zxk","note":"","ucode":"4BB2BD9D2BCD04","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367624,"discussion_content":"想来想去，只能想到两点：\n1. 自动倾斜处理拆分文件后，shuffle 需要拉取文件的次数增加了\n2. 拆分为多个分片后，如果 executor 的同一时间能执行的 task 数小于拆分后的数据分片，那么会出现所有 executor 执行完一轮 task 后再执行其他零星几个 task，此时会出现大量 executor 处于空闲状态。  \n不知道正不正确，还请老师指点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618409226,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287991,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1618242544,"is_pvip":false,"replies":[{"id":"104594","content":"需要在分布式的上下文去考虑这个问题哈~<br><br>比如：<br>1. 排了序之后，按照什么规则合并分区？<br>2. 如果需要合并的partitions是跨Executors进程、跨节点的，怎么实现合并？Shuffle吗？<br><br>另外，可以在不引入额外Shuffle的情况下，优化分区合并的策略吗？等等，诸如此类~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1618307481,"ip_address":"","comment_id":287991,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1618242544","product_id":100073401,"comment_content":"我来设计分区合并思路的话，会先按照每个partitionid的总体size来从大到小排序，然后按照背包算法来装包","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518479,"discussion_content":"需要在分布式的上下文去考虑这个问题哈~\n\n比如：\n1. 排了序之后，按照什么规则合并分区？\n2. 如果需要合并的partitions是跨Executors进程、跨节点的，怎么实现合并？Shuffle吗？\n\n另外，可以在不引入额外Shuffle的情况下，优化分区合并的策略吗？等等，诸如此类~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618307481,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287226,"user_name":"maben996","can_delete":false,"product_type":"c1","uid":1705386,"ip_address":"","ucode":"95390116090D23","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/usX58SweWDFqjCmtkvPOWIfjqN2GqydQYqW53bIcFI4DGBmp6O2LZxZL1UYsVPRuEP03dEJcK3d9jHdYZVn8ug/132","comment_is_top":false,"comment_ctime":1617846842,"is_pvip":false,"replies":[{"id":"104304","content":"不会的，你本身就需要排序，spark自然会利用排序机制来提升效率。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617882222,"ip_address":"","comment_id":287226,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617846842","product_id":100073401,"comment_content":"老师请教个问题，对于 spark.shuffle.sort.bypassMergeThreshold 这个参数，默认值为200。假如我的场景是需要排序的，但是reduce端的分区数又小于200，那这个参数会不会生效，触发bypass，绕过排序shuffle？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518236,"discussion_content":"不会的，你本身就需要排序，spark自然会利用排序机制来提升效率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617882222,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286917,"user_name":"坏耗子","can_delete":false,"product_type":"c1","uid":1043565,"ip_address":"","ucode":"2EC6C2893CC1A8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/6d/d93587e8.jpg","comment_is_top":false,"comment_ctime":1617678409,"is_pvip":false,"replies":[{"id":"104178","content":"主要是充分利用catalyst和tungsten，另外再针对典型场景如joins，这些在后面的spark sql那部分都会讲哈～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617684486,"ip_address":"","comment_id":286917,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617678409","product_id":100073401,"comment_content":"问下老师，目前spark版本为2.3， 主要用spark sql为主进行开发，有什么好的sql方面的调优思路？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518131,"discussion_content":"主要是充分利用catalyst和tungsten，另外再针对典型场景如joins，这些在后面的spark sql那部分都会讲哈～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617684486,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286914,"user_name":"onee","can_delete":false,"product_type":"c1","uid":1613204,"ip_address":"","ucode":"48C0A8938548C3","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIO6eRuuA6V34kREveXkNaebicNzy3oUvEM3t48ehMRJIuCnYNe9B54VuAndjo1cZZ5ykHDHL8ZlhA/132","comment_is_top":false,"comment_ctime":1617677273,"is_pvip":false,"replies":[{"id":"104179","content":"对，如果你明确repartition、或是HadoopRDD这种本身就是带分区的，spark会尊重你的意愿，那两个参数就没用了。 spark.sql.shuffle.partitions，是你在saprk sql开发框架之下，shuffle reduce阶段，默认的并行度。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617684765,"ip_address":"","comment_id":286914,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1617677273","product_id":100073401,"comment_content":"请问spark.sql.shuffle.partitions是只有sparksql时有用还是所有？如果一个数据partitionby成1024份了，那spark.default.parallelism和spark.sql.shuffle.partitions是不是都无效了？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518129,"discussion_content":"对，如果你明确repartition、或是HadoopRDD这种本身就是带分区的，spark会尊重你的意愿，那两个参数就没用了。 spark.sql.shuffle.partitions，是你在saprk sql开发框架之下，shuffle reduce阶段，默认的并行度。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617684765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1181606,"avatar":"https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg","nickname":"来世愿做友人 A","note":"","ucode":"EF20966B0F27E1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365013,"discussion_content":"spark.sql.shuffle.partitions 只有 ds 和 df 生效，就是说也对上层 spark sql 生效，对 raw rdd 不生效\nspark.default.parallelism 只对 raw rdd 生效，并且像文中说的没有指定分区器和分区数时，才会使用 default\n另外一个您说 partitionby 成1024，用的就是 raw rdd，那么和 spark.sql.shuffle.partitions 没有关系。这个方法指定了分区器 Hash 并且指定了个数 1024，那么 spark.default.parallelism 也是无法生效的","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1617685417,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388684,"discussion_content":"楼上说的很准确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628910043,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286879,"user_name":"白音","can_delete":false,"product_type":"c1","uid":2526221,"ip_address":"","ucode":"B83B0269B8E9A0","user_header":"https://static001.geekbang.org/account/avatar/00/26/8c/0d/42e16041.jpg","comment_is_top":false,"comment_ctime":1617667476,"is_pvip":false,"replies":[{"id":"104154","content":"对，要升级到3.0","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1617670624,"ip_address":"","comment_id":286879,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1617667476","product_id":100073401,"comment_content":"问下老师，spark 2.x 的版本中 AQE 的这些特性都无法使用是吧？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518118,"discussion_content":"对，要升级到3.0","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617670624,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}