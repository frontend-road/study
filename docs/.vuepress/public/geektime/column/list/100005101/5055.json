{"id":5055,"title":"12 | 如果关注排序效果，那么这个模型可以帮到你","content":"<p>矩阵分解在推荐系统中的地位非常崇高，恐怕本专栏介绍的其他算法模型都不能轻易地撼动它。</p>\n<p>它既有协同过滤的血统，又有机器学习的基因，可以说是非常优秀了；但即便如此，传统的矩阵分解无论是在处理显式反馈，还是处理隐式反馈都让人颇有微词，这一点是为什么呢？</p>\n<h2>矩阵分解的不足</h2>\n<p>前面我讲过的两种矩阵分解，本质上都是在预测用户对一个物品的偏好程度，哪怕不是预测评分， 只是预测隐式反馈，也难逃这个事实，因为算法展现出来的目标函数就出卖了这一切。</p>\n<p>得到这样的矩阵分解结果后，常常在实际使用时，又是用这个预测结果来排序。所以，从业者们口口声声宣称想要模型的预测误差最小化，结果绕了一大圈最后还是只想要一个好点的排序，让人不禁感叹：人心总是难测。</p>\n<p>这种针对单个用户对单个物品的偏好程度进行预测，得到结果后再排序的问题，在排序学习中的行话叫做point-wise，其中point意思就是：只单独考虑每个物品，每个物品像是空间中孤立的点一样。</p>\n<p>与之相对的，还有直接预测物品两两之间相对顺序的问题，就叫做pair-wise，pair，顾名思义就是成对成双，也许恐怕这类模型对单身的人士不是很友好。</p>\n<p>前面讲的矩阵分解都属于point-wise模型。这类模型的尴尬是：只能收集到正样本，没有负样本，于是认为缺失值就是负样本，再以预测误差为评判标准去使劲逼近这些样本。逼近正样本没问题，但是同时逼近的负样本只是缺失值而已，还不知道真正呈现在用户面前，到底是不喜欢还是喜欢呢？</p>\n<p>虽然这些模型采取了一些措施来规避这个问题，比如负样本采样，但是尴尬还是存在的，为了排序而绕路也是事实。</p>\n<p>既然如此，能不能直面问题，采用pair-wise来看待矩阵分解呢？当然能，不然我也不会写出这一篇专栏文章了。</p>\n<p>其实人在面对选择时，总是倾向矮子中选高个子，而不是真的在意身高到底是不是180，因此，更直接的推荐模型应该是：能够较好地为用户排列出更好的物品相对顺序，而非更精确的评分。</p>\n<p>这个问题已经有可爱的从业者们提出了方法，就是本文的主角：贝叶斯个性化排序，简称BPR模型。下面，我就带你一探这个模型的究竟。</p>\n<h2>贝叶斯个性化排序</h2>\n<p>在前面的专栏文章中，有一个词叫做均方根误差，被我提过多次，用于评价模型预测精准程度的。那么现在要关注的是相对排序，用什么指标比较好呢？答案是AUC，AUC全称是Area Under Curve，意思是曲线下的面积，这里的曲线就是 ROC 曲线。</p>\n<h3>AUC</h3>\n<p>但是，我不打算继续解释什么是 ROC 曲线了，那是它的原始定义，而我想跟你悄悄说的是另一件事，AUC这个值在数学上等价于：模型把关心的那一类样本排在其他样本前面的概率。最大是1，完美结果，而0.5就是随机排列，0就是完美地全部排错。</p>\n<p>听到这个等价的AUC解释，你是不是眼前一亮？这个非常适合用来评价模型的排序效果，比如说，得到一个推荐模型后，按照它计算的分数，能不能把用户真正想消费的物品排在前面？这在模型上线前是可以用日志完全计算出来的。</p>\n<p>AUC怎么计算呢？一般步骤如下。</p>\n<ol>\n<li>用模型给样本计算推荐分数，比如样本都是用户和物品这样一对一对的，同时还包含了有无反馈的标识；</li>\n<li>得到打过分的样本，每条样本保留两个信息，第一个是分数，第二个是0或者1，1表示用户消费过，是正样本，0表示没有，是负样本；</li>\n<li>按照分数对样本重新排序，降序排列；</li>\n<li>给每一个样本赋一个排序值，第一位 r1 = n，第二位 r2 = n-1，以此类推；其中要注意，如果几个样本分数一样，需要将其排序值调整为他们的平均值；</li>\n<li>最终按照下面这个公式计算就可以得到AUC值。</li>\n</ol>\n<p>我在文稿中放了这个公式，你可以点击查看。</p>\n<p>$$AUC = \\frac{\\sum_{i\\in(样本)}{r_{i}} - \\frac{M\\times{(M+1)}}{2}}{M\\times{N}}$$</p>\n<p>这个公式看上去复杂，其实很简单，由两部分构成：</p>\n<p>第一部分： 分母是所有我们关心的那类样本，也就是正样本，有M个，以及其他样本有N个，这两类样本相对排序总共的组合可能性，是M x N；</p>\n<p>第二部分： 分子也不复杂，原本是这样算的：第一名的排序值是r1，它在排序上不但比过了所有的负样本，而且比过了自己以外的正样本。</p>\n<p>但后者是自己人，所以组合数要排除，于是就有n - M种组合，以此类推，排序值为rM的就贡献了rM - 1，把这些加起来就是分子。</p>\n<p>关于AUC，越接近1越好是肯定的，但是并不是越接近0就越差，最差的是接近0.5，如果AUC很接近0的话，只需要把模型预测的结果加个负号就能让AUC接近1，具体的原因自行体会。</p>\n<p>好了，已经介绍完排序的评价指标了，该主角出场了，BPR模型，它提出了一个优化准则和学习框架，使得原来传统的矩阵分解放进来能够焕发第二春。</p>\n<p>那到底BPR做了什么事情呢？主要有三点：</p>\n<ol>\n<li>一个样本构造方法；</li>\n<li>一个模型目标函数；</li>\n<li>一个模型学习框架。</li>\n</ol>\n<p>通过这套三板斧，便可以脱离评分预测，来做专门优化排序的矩阵分解。下面详细说说这三板斧。</p>\n<!-- [[[read_end]]] -->\n<h3>构造样本</h3>\n<p>前面介绍的矩阵分解，在训练时候处理的样本是：用户、物品、反馈，这样的三元组形式。</p>\n<p>其中反馈又包含真实反馈和缺失值，缺失值充当的是负样本职责。BPR则不同，提出要关心的是物品之间对于用户的相对顺序，于是构造的样本是：用户、物品1、物品2、两个物品相对顺序，这样的四元组形式，其中，“两个物品的相对顺序”，取值是：</p>\n<ol>\n<li>如果物品1是消费过的，而物品2不是，那么相对顺序取值为1，是正样本；</li>\n<li>如果物品1和物品2刚好相反，则是负样本；</li>\n<li>样本中不包含其他情况：物品1和物品2都是消费过的，或者都是没消费过的。</li>\n</ol>\n<p>这样一来，学习的数据是反应用户偏好的相对顺序，而在使用时，面对的是所有用户还没消费过的物品，这些物品仍然可以在这样的模型下得到相对顺序，这就比三元组point-wise样本要直观得多。</p>\n<h3>目标函数</h3>\n<p>现在，每条样本包含的是两个物品，样本预测目标是两个物品的相对顺序。按照机器学习的套路，就该要上目标函数了。</p>\n<p>要看BPR怎么完成矩阵分解，你依然需要像交替最小二乘那样的思想。</p>\n<p>先假装矩阵分解结果已经有了，于是就计算出用户对于每个物品的推荐分数，只不过这个推荐分数可能并不满足均方根误差最小，而是满足物品相对排序最佳。</p>\n<p>得到了用户和物品的推荐分数后，就可以计算四元组的样本中，物品1和物品2的分数差，这个分数可能是正数，也可能是负数，也可能是0。</p>\n<p>你和我当然都希望的情况是：如果物品1和物品2相对顺序为1，那么希望两者分数之差是个正数，而且越大越好；如果物品1和物品2的相对顺序是0，则希望分数之差是负数，且越小越好。</p>\n<p>用个符号来表示这个差：Xu12，表示的是对用户u，物品1和物品2的矩阵分解预测分数差。然后再用 sigmoid 函数把这个分数差压缩到0到1之间。</p>\n<p>$$\\Theta = \\frac{1}{1 + e^{-(X_{u12})}}$$</p>\n<p>也其实就是用这种方式预测了物品1排在物品2前面的似然概率，所以最大化交叉熵就是目标函数了。</p>\n<p>目标函数通常还要防止过拟合，加上正则项，正则项其实认为模型参数还有个先验概率，这是贝叶斯学派的观点，也是BPR这个名字中“贝叶斯”的来历。</p>\n<p>BPR认为模型的先验概率符合正态分布，对应到正则化方法就是L2正则，这些都属于机器学习的内容，这里不展开讲。</p>\n<p>我来把目标函数写一下：</p>\n<p>$$\\prod_{u,i,j}{p(i&gt;_ {u}j | \\theta)p(\\theta)}$$</p>\n<p>所有样本都计算：模型参数先验概率p theta，和似然概率的乘积，最大化这个目标函数就能够得到分解后的矩阵参数，其中theta就是分解后的矩阵参数。</p>\n<p>最后说一句，把这个目标函数化简和变形后，和把AUC当成目标函数是非常相似的，也正因为如此，BPR模型的作者敢于宣称该模型是为AUC而生的。</p>\n<h3>训练方法</h3>\n<p>有了目标函数之后，就要有请训练方法了。显然是老当益壮的梯度下降可以承担这件事，梯度下降又有批量梯度和随机梯度下降两个选择，前者收敛慢，后者训练快却不稳定。因此BPR的作者使用了一个介于两者之间的训练方法，结合重复抽样的梯度下降。具体来说是这样做的：</p>\n<ol>\n<li>从全量样本中有放回地随机抽取一部分样本；</li>\n<li>用这部分样本，采用随机梯度下降优化目标函数，更新模型参数；</li>\n<li>重复步骤1，直到满足停止条件。</li>\n</ol>\n<p>这样，就得到了一个更符合推荐排序要求的矩阵分解模型了。</p>\n<h2>总结</h2>\n<p>今天是矩阵分解三篇的最后一篇，传统的矩阵分解，无论是隐式反馈还是显式反馈，都是希望更加精准地预测用户对单个物品的偏好，而实际上，如果能够预测用户对物品之间的相对偏好，则更加符合实际需求的直觉。</p>\n<p>BPR就是这样一整套针对排序的推荐算法，它事实上提出了一个优化准则和一个学习框架，至于其中优化的对象是不是矩阵分解并不是它的重点。</p>\n<p>但我在这里结合矩阵分解对其做了讲解，同时还介绍了排序时最常用的评价指标AUC及其计算方法。</p>\n<p>你在看了BPR算法针对矩阵分解的推荐计算过程之后，试着想一想，如果不是矩阵分解，而是近邻模型，那该怎么做？欢迎留言给我，一起聊聊。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/9f/93/9f92d45e88830b1713c10e320dca3293.jpg?wh=3560*2008\" alt=\"\" /></p>\n","comments":[{"had_liked":false,"id":4691,"user_name":"曾阿牛","can_delete":false,"product_type":"c1","uid":1049468,"ip_address":"","ucode":"29A294174EF06B","user_header":"https://static001.geekbang.org/account/avatar/00/10/03/7c/39ea8a23.jpg","comment_is_top":false,"comment_ctime":1522340870,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"147551228934","product_id":100005101,"comment_content":"算法通过短文的方式理解较费劲，有参考书籍&#47;开源代码推荐吗？","like_count":35},{"had_liked":false,"id":28800,"user_name":"lfn","can_delete":false,"product_type":"c1","uid":1026593,"ip_address":"","ucode":"2E1558C6A12A89","user_header":"https://static001.geekbang.org/account/avatar/00/0f/aa/21/6c3ba9af.jpg","comment_is_top":false,"comment_ctime":1538146966,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"104617362070","product_id":100005101,"comment_content":"看完之后不太明白，私下里找了两个讲解的比较清楚的博客，分享给看不明白的小伙伴：<br>https:&#47;&#47;www.cnblogs.com&#47;gatherstars&#47;p&#47;6084696.html ROC曲线和AUC值<br>https:&#47;&#47;www.cnblogs.com&#47;pinard&#47;p&#47;9128682.html   贝叶斯个性化排序","like_count":25},{"had_liked":false,"id":11613,"user_name":"greekzf","can_delete":false,"product_type":"c1","uid":1094710,"ip_address":"","ucode":"07C05289F6F75D","user_header":"https://static001.geekbang.org/account/avatar/00/10/b4/36/8e3e4d4e.jpg","comment_is_top":false,"comment_ctime":1528247936,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"40182953600","product_id":100005101,"comment_content":"这句话 和公式不匹配。。。    但后者是自己人，所以组合数要排除，于是就有 n - M 种组合，以此类推，排序值为 rM 的就贡献了 rM - 1，把这些加起来就是分子。   ","like_count":10,"discussions":[{"author":{"id":2827120,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/23/70/ca07b59b.jpg","nickname":"PlanB","note":"","ucode":"82A344513EB366","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":548453,"discussion_content":"可以理解为第一个人只有一种可能，但是在计算中多计算了M种，因此要减掉；从M到1，step为1的数组总和为(M*(M+1))/2","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1643201642,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":29203,"user_name":"麦子","can_delete":false,"product_type":"c1","uid":1254320,"ip_address":"","ucode":"D991A5A6A1775D","user_header":"https://static001.geekbang.org/account/avatar/00/13/23/b0/baba4968.jpg","comment_is_top":false,"comment_ctime":1538288576,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"27308092352","product_id":100005101,"comment_content":"AUC公式分子讲解的不太清楚，看不懂文字跟公式间的联系。","like_count":7},{"had_liked":false,"id":5255,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1522945658,"is_pvip":false,"replies":[{"id":"1416","content":"你理解是正确的。赞你的认真态度！","user_name":"作者回复","comment_id":5255,"uid":"1005376","ip_address":"","utype":1,"ctime":1523069525,"user_name_real":"刑无刀"}],"discussion_count":2,"race_medal":0,"score":"18702814842","product_id":100005101,"comment_content":"我看了好一会的Adaptive k-Nearest-Neighbor的英文公式，总算有点理解。也就是先用一个传统的距离算法计算每个用户曾经有过交互的物品中的相似度值或距离值，然后对于任意上述物品集合中的一对物品，仿照矩阵分解中sigmoid 函数的计算方法由相似度值或距离值的差值来推导Θ，在由Θ优化目标函数。<br><br>如果理解有误盼望老师能指出。<br><br>上次的问题我现在明白了。文稿查看是针对录音。目标函数是先验概率与似然概率的乘积，它与AUC值有相似性。似然概率值是用sigmoid函数计算出来，原文中相当于sigmoid函数值的连乘。","like_count":5,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416654,"discussion_content":"你理解是正确的。赞你的认真态度！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1523069525,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1343949,"avatar":"","nickname":"赖春苹","note":"","ucode":"1B637D46549A21","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":3542,"discussion_content":"其中的很多细节其实还是不很理解，优化似然函数*先验概率与最大化AUC值的目标是一致的吧~ 然后似然函数*先验概率这一目标函数，前面取对数log、再取负值，做最小化，就相当于带正则项的对数损失函数吧？不知道这样理解对不对……其实预测相对顺序有点像做二分类的意思","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1564563257,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4778,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1522453747,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14407355635","product_id":100005101,"comment_content":"谢谢陈老师的分享。我在手机端查看。请问AUC公式的“可以点击文稿查看”是指在电脑端可以点击，会打开参考文献的链接吗？<br><br>文中BPR pair wise在真实场景应用中优化的目标函数是(1)AUC值还是(2)先验概率与似然概率的乘积值？<br><br>似然概率值是在矩阵参数上一步的估计值&#47;初始值确认后用文中提到的sigmoid函数计算出来的吗？<br><br>最后文中的延伸问题是指BPR算法如何应用于计算KNN的场景吗？手机端搜索和查阅自己不熟悉领域的文献慢些，之后有时间用电脑检索。","like_count":3},{"had_liked":false,"id":109140,"user_name":"山药","can_delete":false,"product_type":"c1","uid":1064167,"ip_address":"","ucode":"A1EA25B081617C","user_header":"https://static001.geekbang.org/account/avatar/00/10/3c/e7/eb5aea59.jpg","comment_is_top":false,"comment_ctime":1561970677,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10151905269","product_id":100005101,"comment_content":"关于AUC不错的一种解释https:&#47;&#47;tracholar.github.io&#47;machine-learning&#47;2018&#47;01&#47;26&#47;auc.html","like_count":2,"discussions":[{"author":{"id":1133758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4c/be/25919d4b.jpg","nickname":"FF","note":"","ucode":"89DB3329AAEAB2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6248,"discussion_content":"细看的时候发现AUC公式和解释看的懵逼还好及时翻到","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566810371,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":158975,"user_name":"neohope","can_delete":false,"product_type":"c1","uid":1043475,"ip_address":"","ucode":"C0268F6E7E2B6E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/13/49e98289.jpg","comment_is_top":false,"comment_ctime":1575509552,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5870476848","product_id":100005101,"comment_content":"老师，近邻模型做排序的话，可不可以直接通过相似度排序，然后排除买过的物品就可以呢？<br>感觉每一部分都能勉强看懂，但还是前后串不起来。<br>希望能多提供一些例子，或者能提供一些代码。","like_count":1},{"had_liked":false,"id":113709,"user_name":"对白","can_delete":false,"product_type":"c1","uid":1268797,"ip_address":"","ucode":"3183E5ADBC794B","user_header":"https://static001.geekbang.org/account/avatar/00/13/5c/3d/e8325811.jpg","comment_is_top":false,"comment_ctime":1563133309,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5858100605","product_id":100005101,"comment_content":"有一个问题是BPR的优化算法为什么需要结合重复采样呢，文章中写道后面还是使用随机梯度下降法，也就是随机采样一个样本，那前面做的从全量样本中做有放回的采样不就多此一举吗，请老师指点谢谢！","like_count":2},{"had_liked":false,"id":5965,"user_name":"zgl","can_delete":false,"product_type":"c1","uid":1059430,"ip_address":"","ucode":"58DBD016E4A379","user_header":"https://static001.geekbang.org/account/avatar/00/10/2a/66/96df31f9.jpg","comment_is_top":false,"comment_ctime":1524195530,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5819162826","product_id":100005101,"comment_content":"老师，请问对于音频推荐来说，排序负样本如何构建？只有点击日志没有曝光日志","like_count":1},{"had_liked":false,"id":310552,"user_name":"宋计洋","can_delete":false,"product_type":"c1","uid":2076251,"ip_address":"","ucode":"9A34E8F71C6CBD","user_header":"https://static001.geekbang.org/account/avatar/00/1f/ae/5b/4bd42286.jpg","comment_is_top":false,"comment_ctime":1630739468,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1630739468","product_id":100005101,"comment_content":"步骤讲的太粗糙了，对于入门的人完全看不懂","like_count":0},{"had_liked":false,"id":302610,"user_name":"Kyogre","can_delete":false,"product_type":"c1","uid":2403727,"ip_address":"","ucode":"C6159ADB1525B6","user_header":"https://static001.geekbang.org/account/avatar/00/24/ad/8f/ef7d1baa.jpg","comment_is_top":false,"comment_ctime":1626280653,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1626280653","product_id":100005101,"comment_content":"auc计算公式里，i应该属于正样本","like_count":0},{"had_liked":false,"id":35506,"user_name":"陈松林","can_delete":false,"product_type":"c1","uid":1262361,"ip_address":"","ucode":"25CE5DB2A462DB","user_header":"https://static001.geekbang.org/account/avatar/00/13/43/19/20d3bb26.jpg","comment_is_top":false,"comment_ctime":1540630893,"is_pvip":false,"replies":[{"id":"12801","content":"正负样本一起。","user_name":"作者回复","comment_id":35506,"uid":"1005376","ip_address":"","utype":1,"ctime":1540881410,"user_name_real":"刑无刀"}],"discussion_count":1,"race_medal":0,"score":"1540630893","product_id":100005101,"comment_content":"auc计算的时候只选正样本？","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":427557,"discussion_content":"正负样本一起。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1540881410,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":5602,"user_name":"张飞","can_delete":false,"product_type":"c1","uid":1057341,"ip_address":"","ucode":"09136FAC409AAF","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/3d/50cab519.jpg","comment_is_top":false,"comment_ctime":1523601600,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1523601600","product_id":100005101,"comment_content":"老师想问下数据少的话，到底能做推荐系统不？","like_count":0},{"had_liked":false,"id":5420,"user_name":"吴文敏","can_delete":false,"product_type":"c1","uid":1003150,"ip_address":"","ucode":"A4303003E547D1","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4e/8e/f4297447.jpg","comment_is_top":false,"comment_ctime":1523323189,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1523323189","product_id":100005101,"comment_content":"如果仅是top 1推荐，而且既有点击数据又有曝光未点击数据，是否还有必要用pair-wise算法？","like_count":0},{"had_liked":false,"id":4704,"user_name":"刘大猫","can_delete":false,"product_type":"c1","uid":1059169,"ip_address":"","ucode":"37DC327476CFFD","user_header":"","comment_is_top":false,"comment_ctime":1522371007,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1522371007","product_id":100005101,"comment_content":"学到的是相对排序 跟全局排序还是有些不太一样","like_count":0}]}