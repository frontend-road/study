{"id":674168,"title":"11｜SQL优化：如何发现SQL中的问题？","content":"<p>你好，我是大明。今天我们来聊一聊数据库中的 SQL 优化。</p><p>一般而言，在面试过程中，我都是鼓励你尽可能为自己打造熟练掌握性能优化技巧的人设。高并发项目经验可遇不可求，但是高性能是可以勉强追求的，性能优化就是追求高性能的方法。和我在微服务里面讲到的高可用相结合，你在写简历、自我介绍和面试过程中，可以有意识地展示自己在高可用和高性能方面的知识和积累。</p><p>而 SQL 优化是性能优化中最平易近人、最好准备的点。所以今天我们就来学习一下 SQL 优化的多种方案。</p><h2>前置知识</h2><p>SQL 优化可以看作是一个更大的主题“数据库优化”下的一个子议题。数据库优化主要包含以下内容：</p><ul>\n<li>硬件资源优化：换更大更强的机器。</li>\n<li>操作系统优化：调整操作系统的某些设置。</li>\n<li>服务器/引擎优化：也就是针对数据库软件本体进行优化，比如说调整事务隔离级别。在 MySQL 里面还可以针对不同的引擎做优化，比如说调整 InnoDB 引擎的日志刷盘时机。</li>\n<li>SQL 优化：针对的就是 SQL 本身了。</li>\n</ul><p><img src=\"https://static001.geekbang.org/resource/image/ea/f7/ea9337b3727f69fb1e494af845882ff7.png?wh=2440x1310\" alt=\"\"></p><p>如果站在数据库的角度，那么 SQL 优化就是为了达到两个目标。</p><ol>\n<li><strong>减少磁盘 IO</strong>，这个又可以说是尽量避免全表扫描、尽量使用索引以及尽量使用覆盖索引。</li>\n<li><strong>减少内存 CPU 消耗</strong>，这一部分主要是尽可能减少排序、分组、去重之类的操作。</li>\n</ol><!-- [[[read_end]]] --><p>这部分知识你只需要有一个概念就可以，在面试的时候倒是不经常用到。SQL 优化是一个实践为主的面试主题，很少讨论这种纯理论的内容。</p><p>如果想要知道你优化后的效果，就需要掌握一个工具，就是EXPLAIN 命令。</p><h3>EXPLAIN 命令</h3><p>应该说，每一个后端研发都应该掌握 EXPLAIN 命令。EXPALIN 命令的大概用法是 EXPLAIN your_sql，然后数据库就会返回一个执行计划。</p><p><img src=\"https://static001.geekbang.org/resource/image/ab/e9/ab344a50a2a03332fc243b42d30901e9.png?wh=1920x553\" alt=\"图片\"></p><p>执行计划有很多字段，我把最关键的地方列出来，你在面试的时候只需要记住这几个就可以了。</p><ol>\n<li><strong>type</strong>：指的是查询到所需行的方式，从好到坏依次是 system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL。</li>\n</ol><ul>\n<li>system 和 const 都可以理解为数据库只会返回一行数据，所以查询时间是固定的。</li>\n<li>eq_ref 和 ref 字面意思是根据索引的值来查找。</li>\n<li>range：索引范围扫描。</li>\n<li>index：索引全表扫描，也就是扫描整棵索引。</li>\n<li>ALL：全表扫描，发起磁盘 IO 的那种全表扫描。</li>\n</ul><ol start=\"2\">\n<li><strong>possible_keys</strong>：候选的索引。</li>\n<li><strong>key</strong>：实际使用的索引。</li>\n<li><strong>rows</strong>：扫描的行数。数据库可能扫描了很多行之后才找到你需要的数据。</li>\n<li><strong>filtered</strong>：查找到你所需的数据占 rows 的比例。</li>\n</ol><p>在实践中，没有必要将这些东西都死记硬背下来。等你要解读 EXPLAIN 的返回结果的时候，再去搜索一下就行了。</p><p>一般优化 SQL 都是在 <strong>EXPLAIN 查看执行计划、尝试优化</strong>两个步骤之间循环往复，直到发现 SQL 性能达标。</p><p><img src=\"https://static001.geekbang.org/resource/image/3b/fa/3bc7f7d1f70494957272092cc643d1fa.png?wh=2440x860\" alt=\"\"></p><h3>选择索引列</h3><p>设计索引的时候，列的选择非常关键，但是目前来说并没有特别统一的说法。你可以参考以下规则。</p><ul>\n<li><strong>外键</strong>，一般都会用于关联、过滤数据，所以正常来说都会为表的外键创建索引。</li>\n<li>频繁出现在 <strong>WHERE 中的列</strong>，主要是为了避免全表扫描。</li>\n<li>频繁出现在 <strong>ORDER BY 的列</strong>，这是为了避免数据库在查询出来结果之后再次排序。</li>\n<li>频繁出现在关联查询的关联条件中的列。不过一般我们都不建议使用关联查询，所以几乎可以忽略这个。</li>\n<li><strong>区分度很高的列</strong>。比如每一行的数据都不同的列，并且在创建组合索引的时候，区分度很高的列应该尽可能放到左边。</li>\n</ul><h3>大表表定义变更</h3><p>优化 SQL 很多手段都是围绕索引来进行的。比如后面我给出的案例要么是修改已有的索引，要么是加索引。但在修改索引的时候，数据量大的表修改索引和数据量小的表修改索引，实施方案是完全不一样的。</p><p>修改索引或者说表定义变更的核心问题<strong>是数据库会加表锁，直到修改完成</strong>。</p><p>所以当你发现你的 MySQL 性能不行了，准备新加一个索引的时候，如果这个表的数据很多，那么在你执行加索引的命令的时候，整张表可能都会被锁住几分钟甚至几个小时。</p><p><img src=\"https://static001.geekbang.org/resource/image/80/7e/8003705c2240717b018563d97bfa6b7e.png?wh=2440x760\" alt=\"\"></p><p>可见大表表结构变更是一件很麻烦的事情，一般可以考虑的方案有3种。</p><ol>\n<li>停机变更，就是把业务停下来，然后更新表结构。如果做得更加精细一点，那么就可以说只把和这个表有关的功能下线，但不需要将整个服务或者系统下线。</li>\n<li>在业务低谷变更，比停机更新好一点，但是业务依旧受到了影响。而且万一你以为在低谷能完成变更，结果并没有，那么你就面临着业务在高峰期也不能用的问题。</li>\n<li>创建新表，这是不停机又不想业务受到影响的方案。具体来说就是创建一张新表，这张新表就是你准备用的新的表定义。然后将旧表的数据迁移过去，我们在后面会专门讨论数据迁移方案。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/fe/71/fe056a99acd3e0f248a5d98a222a4571.png?wh=1954x832\" alt=\"\"></p><h2>面试准备</h2><p>你需要准备很多个 SQL 优化的案例，这些案例可以体现你对 SQL 和数据库底层不同技术点的理解。为此你需要收集和整理好一些信息：</p><ul>\n<li>你维护的业务的所有表结构定义（包含索引定义），每张表上执行最频繁的三个 SQL是否用到了索引。</li>\n<li>公司内部曾经或者已有的慢 SQL是怎么发现、分析和优化的。然后要记住 SQL 优化前后的执行时间，以凸显优化的效果。</li>\n<li>每一个 SQL 优化案例你都要考虑清楚面试官如果要深挖，那么会朝着什么方向深挖。</li>\n</ul><p>整体上来说，SQL 优化的手段是非常多而且非常细碎的。但是在面试中，你<strong>不需要全部掌握</strong>。你只需要在面试前精心设计一下你在整个 SQL 优化里面的面试节奏就可以了。</p><p>当面试官问到这些问题的时候，你都可以将话题引导到 SQL 优化中。</p><ul>\n<li>你是否做过性能优化？</li>\n<li>接口的响应时间是多少？有没有优化的空间？</li>\n<li>你是否了解索引？是否用过索引？</li>\n</ul><p>还有一种比较罕见的面试方式是面试官会要求你手写 SQL。手写之后，面试官会进一步考察在特定的一些场景下，你的 SQL 是否会有问题。记住，但凡让你手写 SQL 的题目，你都要谨慎考虑 SQL 有没有改进的空间。</p><h2>基本思路</h2><p>面试SQL优化的最佳策略就是把它作为自己全方面优化系统性能的一个举措。你可以考虑这样说：</p><blockquote>\n<p>某某系统是一个核心系统，对性能有很高的要求。为了让服务的响应时间降低到 100ms 以内，我做了很多性能优化的事情，比如 SQL 优化。</p>\n</blockquote><p>如果你这个系统同时还要求高可用，那么你就将高可用和性能优化融合在一起。</p><blockquote>\n<p>某某系统是一个核心系统，对可用性和性能都有很高的要求。公司对可用性的要求是希望能达到三个九，平均响应时间控制在 100ms 内。</p>\n</blockquote><p>接下来面试官就会问一些具体的措施。比如说他可能会问：“你是怎么优化你的 SQL 查询的？”那么你可以这样回答。</p><blockquote>\n<p>我们公司有 SQL 的慢查询监控，当我们发现接口响应时间比较差的时候，就会去排查 SQL 的问题。我们主要是使用 EXPLAIN 命令来查看 SQL 的执行计划，看看它有没有走索引、走了什么索引、是否有内存排序、去重之类的操作。<br>\n&nbsp;<br>\n初步判定了问题所在之后，我们尝试优化，包括改写 SQL 或者修改、创建索引。之后再次运行 SQL 看看效果。如果效果不好，就继续使用 EXPLAIN 命令，再尝试修改。如此循环往复，直到 SQL 性能达到预期。</p>\n</blockquote><p>你可以用你实际接触过的案例补充说明一下。</p><p>如果案例中涉及到了变更表结构定义、增加索引或者修改索引，那么面试官就可能问你怎么找出合适的列来创建索引，以及在大表里面怎么变更表结构。</p><p>如果面试的内容比较基础，那么面试官还会问你 EXPLAIN 命令返回了什么数据。这部分内容前置知识都有，你需要记住。</p><p>此外，如果你了解怎么优化操作系统、怎么优化数据库本身，那么你可以把这些内容结合起来，将自己打造成数据库优化高手。</p><p>即便你真不会，也有一个取巧的办法。就是你去找你们公司的 DBA，问清楚公司的数据库和数据库所在的操作系统有没有设置过什么参数。你弄清楚每一个参数的含义，修改背后的逻辑，对面试也大有裨益。</p><h2>优化案例</h2><p>我这里给你一些具体的优化案例，这些案例和方法都是可以用在生产实践中的，如果你之前没有尝试过，建议你自己亲手实现一下，增强实感。</p><h3>覆盖索引</h3><p>覆盖索引是最为常见的优化。比如说你执行最多的 SELECT 语句是 <code>SELECT A, B, C 三个列</code>，而且 WHERE 里面也只有这三个列的条件，那么就可以考虑直接创建一个 <code>&lt;A, B, C&gt;</code> 组合索引。</p><p>那么你可以这么介绍你的案例，关键词是<strong>覆盖索引。</strong></p><blockquote>\n<p>原来我们有一个执行非常频繁的 SQL。这个 SQL 查询全部的列，但是业务只会用到其中的三个列 A B C，而且 WHERE 条件里面主要的过滤条件也是这三个列组成的，所以我后面就在这三个列上创建了一个组合索引。<br>\n&nbsp;<br>\n对于这个高频 SQL 来说，新的组合索引就是一个覆盖索引。所以我在创建了索引之后，将 SQL 由 <code>SELECT *</code> 改成了 <code>SELECT  A, B, C</code>，完全避免了回表。这么一来，整个查询的查询时间就直接降到了 1ms 以内。</p>\n</blockquote><p>然后再进一步总结。</p><blockquote>\n<p>正常来说，对于非常高频的 SQL，都要考虑避免回表，那么设计一个合适的覆盖索引就非常重要了。</p>\n</blockquote><p>除了覆盖索引，还可以用索引来优化排序，也就是下面的优化 ORDER BY 案例。</p><h3>优化 ORDER BY</h3><p>这算是一个非常典型的案例。在很多场景中，我们查询一些数据之后，都要求对数据进行一定的排序，比如说按照更新时间来排序。那么你就可以这样说，关键词是<strong>将排序列加入索引</strong>。</p><blockquote>\n<p>我在公司优化过一个 SQL，这个 SQL 非常简单，就是将某个人的数据搜索出来，然后按照数据的最后更新时间来排序。SQL 大概是 <code>SELECT * FROM  xxx WHERE uid = 123 ORDER BY update_time</code>。<br>\n&nbsp;<br>\n如果用户的数据比较多，那么这个语句执行的速度还是比较慢的。后来我们做了一个比较简单的优化，就是用 uid 和 update_time 创建一个新的索引。从数据库原理上说，在 uid 确认之后，索引内的 update_time 本身就是有序的，所以避免了数据库再次排序的消耗。这样一个优化之后，查询时间从秒级降到了数十毫秒。</p>\n</blockquote><p>这种优化在现实中非常容易遇到。它的底层原理就是<strong>索引本身就是有序的</strong>。如果用表格来形容 <code>&lt;uid, update_time&gt;</code> 这个索引的数据组织形式，那么看起来大概是这样的。</p><p><img src=\"https://static001.geekbang.org/resource/image/b3/b6/b372ca5bfff7e5dyyec6b364f7b6c2b6.png?wh=1922x1210\" alt=\"\"></p><p>所以在 uid 确定之后，数据库可以直接按照 update_time 在索引中的顺序来返回结果。在优化前，数据库就只能自己将对应 uid 的数据都读过来之后再次进行排序。优化后则是省了一次排序过程。</p><p>最后你也可以总结一下。</p><blockquote>\n<p>在所有的排序场景中，都应该尽量利用索引来排序，这样能够有效减轻数据库的负担，加快响应速度。进一步来说，像ORDER BY，DISTINCT 等这样的操作也可以用类似的思路。</p>\n</blockquote><h3>优化 COUNT</h3><p><code>SELECT COUNT(*)</code> 也算是一个很常用的语句了。很不幸的是，最常使用的 MySQL InnoDB 引擎并没有存储数据总数。所以类似的语句在 MySQL InnoDB 引擎上执行起来特别慢。</p><p>那么优化 COUNT 的思路也比较简单。第一种方法是用估计值取代精确值，关键词是<strong>预估</strong>。</p><blockquote>\n<p>我的这个场景对数据的准确性不是很高，所以我用了一个奇诡的方法，即用 EXPLAIN your_sql，之后用 EXPLAIN 返回的预估行数。比如说 <code>SELECT COUNT(*) FROM xxx WHERE uid= 123</code>，就可以用 <code>EXPLAIN SELECT * FROM xxx WHERE uid = 123</code> 来拿到一个预估值。</p>\n</blockquote><p>后面你还可以进一步描述其他可行的优化。</p><blockquote>\n<p>不过如果需要精确值，那么就可以考虑使用 Redis 之类的NoSQL来直接记录总数。或者直接有一个额外的表来记录总数也可以。</p>\n</blockquote><p>这种优化就跟优化 SQL 本身没什么关系了，但是也可以认为是一种性能优化，或者架构层面的优化。</p><p>在这个回答里面，还有一个地方你要小心，那就是如果你用了 Redis 来维持总数，那么就会涉及数据一致性的问题。也就是说，如果插入数据库成功，但是更新 Redis 上的总数失败了，应该怎么办？</p><p>这本质上是一个分布式事务的问题。主要思路有两个：</p><ol>\n<li>如果数据短时间不一致但是业务可以接受的话，那么就可以考虑异步刷新 Redis 上的总数。</li>\n<li>使用 Canal 之类的工具监听 MySQL binlog，然后刷新 Redis 上的总数。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/dd/c3/dd662132438dc94eed3263fc022de2c3.png?wh=2444x766\" alt=\"\"></p><h3>索引提示优化</h3><p>在上一节课我讲到了 FORCE INDEX、USE INDEX 和 IGNORE INDEX 三个索引提示，当然我也强调了这个不是什么好的实践，尽量少用。</p><p>但是在实际工作中有些时候数据库的执行就很奇怪，要么不用索引，要么用了错误的索引，那么在这种情况下你就可以考虑使用这些索引提示来纠正数据库的行为。</p><blockquote>\n<p>早期我们有一个表结构定义，上面有 A、B 两个索引。原本按照预期，我们是认为这个查询应该会走 A 这个索引。结果实际用 EXPLAIN 命令之后，MySQL 却使用了 B 索引。所以我使用了 FORCE INDEX 之后强制查询使用 A 索引，果然查询的响应时间降低到了毫秒级。</p>\n</blockquote><p>同时还是需要强调一下，这个不是很好的实践，不到逼不得已都不要使用。</p><blockquote>\n<p>类似于 FORCE INDEX 之类的索引提示，本身并不是什么好的实践，还是要谨慎使用。</p>\n</blockquote><h3>用 WHERE 替换 HAVING</h3><p>一般来说，数据库都是先根据 WHERE 条件找到候选的列，再根据 HAVING 条件进行二次过滤。</p><p>那么你应该可以意识到，如果能够将 HAVING 中部分条件提前到 WHERE里，那么就可以提前将不符合条件的数据过滤掉了。</p><blockquote>\n<p>早期我们有一个历史系统，里面有一个 SQL 是很早以前的员工写的，比较随意。他将原本可以用在 WHERE 里面的普通的相等判断写到了 HAVING 里面。后来我将这个条件挪到了 WHERE 之后查询时间降低了40%。</p>\n</blockquote><p><img src=\"https://static001.geekbang.org/resource/image/a9/4c/a9886f43ac7e8cd8bb3315cba217884c.png?wh=2444x1312\" alt=\"\"></p><p>然后你可以总结出一般的规律。</p><blockquote>\n<p>如果不是使用聚合函数来作为过滤条件，最好还是将过滤条件优先写到 WHERE 里面。</p>\n</blockquote><p>在这个案例里面因为你提到了 SQL 的执行顺序，那么面试官可能会问 SQL 执行顺序的问题，你需要有一个心理准备。</p><h3>优化分页中的偏移量</h3><p>有一些 SQL 在不断执行中会产生极大的偏移量，比如说非常简单的文章列表分页，一页50条数据，那么当你要拿第 101 页的数据，需要写成 LIMIT 5000, 50。5000 就是偏移量。实际执行中数据库就需要读出 5050 条数据，然后将前面的 5000 都丢掉，只保留 50 条。</p><p>因此优化思路就是<strong>使用小偏移量</strong>。</p><blockquote>\n<p>在我们的系统里面，最开始有一个分页查询，那时候数据量还不大，所以一直没出什么问题。后来数据量大了之后，我们发现如果往后翻页，页码越大查询越慢。问题关键就在于我们用的 LIMIT 偏移量太大了。<br>\n&nbsp;<br>\n所以后来我就在原本的查询语句的 WHERE 里面加上了一个 <code>WHERE id &gt; max_id</code> 的条件。这个 max_id 就是上一批的最大 ID。这样我就可以保证 LIMIT 的偏移量永远是 0。这样修改之后，查询的速度非常稳定，一直保持在毫秒级。</p>\n</blockquote><p>同样可以总结一下。</p><blockquote>\n<p>很多时候因为测试环境数据量太小，这种性能问题根本不会被发现。所以所有使用分页的查询都应该考虑引入类似的查询条件。</p>\n</blockquote><p>这种优化手段还可以用于分库分表中的分页查询，我们后面会再次讨论。</p><h2>面试思路总结</h2><p>今天这节课我讲了 SQL 优化里面比较容易遇到的三个点：<strong>EXPLAIN 命令、选择索引列和大表表结构变更</strong>，并提供了覆盖索引、优化 ORDER BY、优化 COUNT、优化索引提示、用 WHERE 替换 HAVING、优化分页中的偏移量六个案例。</p><p>此外还有一点你需要注意：<strong>花样繁多的东西，你并不需要全部掌握</strong>。比如说有很多 SQL 优化的手段可能我都没听过。因此在面试过程中你需要注意把控节奏，要掌握整个面试的主动权。或者说虽然看上去面试官还在频繁问你问题，但是他问的问题都是你心中有数的，所以实际上是你在暗中控制整个面试的过程。</p><p>而如果你不能控制好这种节奏，面试官就会随便问，那么问到你不知道的也毫不意外了。</p><p><img src=\"https://static001.geekbang.org/resource/image/2b/ba/2b9249715ba95eb4d8ba9870243512ba.png?wh=1748x1530\" alt=\"\"></p><h2>思考题</h2><p>听我说了这么多，你有没有其他优化 SQL 的案例呢？欢迎你分享出来。另外在计算行数上，SHOW TABLE STATUS 也能看到一个 TABLE_ROWS 列，代表表的行数，那么能不能用这个来优化 COUNT(*)？</p><p>欢迎你把你的答案分享在评论区，也欢迎你把这节课的内容分享给需要的朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"10｜数据库索引：为什么MySQL用B+树而不用B树？","id":672553},"right":{"article_title":"12｜数据库锁：明明有行锁，怎么突然就加了表锁？","id":674789}},"comments":[{"had_liked":false,"id":378100,"user_name":"子休","can_delete":false,"product_type":"c1","uid":1131592,"ip_address":"上海","ucode":"EDB61FB012C195","user_header":"https://static001.geekbang.org/account/avatar/00/11/44/48/fae317c1.jpg","comment_is_top":false,"comment_ctime":1689754930,"is_pvip":false,"replies":[{"id":137832,"content":"赞！","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689954379,"ip_address":"广东","comment_id":378100,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"这里列一下我个人之前总结的SQL优化的思路（稍微增加了一些本章节提到了，但是我也学习到了的内容）：\n1. 索引的角度。\n（1）根据执行计划优化索引，从type，keys，extra这几个主要字段去分析。\n（2）避免返回不需要的列（覆盖索引，避免回表）\n2. 架构层面。\n（1）分库分表\n（2）读写分离\n（3）为了避免like这种，以及超级大数据量查询（没办法避免多表查询），可以借助数仓，建宽表，以及引入es等方法进行实现。\n3. 分析问题角度\n（1）执行sql之前，先执行“set profiling = 1;”，然后执行完查询sql之后，执行&quot;show profiles &quot;可以展示详细的具体执行时间。\n（2）show profiles的方法已经开始被MySQL逐渐淘汰，后续会被 performance_schema代替，performance_schema是一个数据库，里面有87张表，可以通过查询这些表来查看执行情况。\n4. 常见技巧\n（1）深度分页问题：limit m,n分页越往后越慢的问题。\n      a. select * from tableA where id &gt;=(select id from tableA limit m, 1) limit n；\n这种做法有个弊端，要求主键必须自增。\n      b. select * from tableA a\n     inner join (select id from tableA limit m, n)b\n     on a.id = b.id\n(2)避免For循环里面查单条数据，改为一条sql查集合。\n(3)建表的时候考虑增加冗余字段，尽可能保持单表查询，而非多表Join.\n(4)在所有的排序场景中，都应该尽量利用索引来排序.\n(5)算count行数的时候，如果业务场景要求不高，可以有一个偏门方法，就是执行explain select * from t where xxxx，在执行计划里面会预估出来大致的行数。\n(6)where 替代 having","like_count":30,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623899,"discussion_content":"赞！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689954379,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":377807,"user_name":"徐石头","can_delete":false,"product_type":"c1","uid":1035885,"ip_address":"湖南","ucode":"D8FA8A64FB7E33","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ce/6d/530df0dd.jpg","comment_is_top":false,"comment_ctime":1689244297,"is_pvip":true,"replies":[{"id":137709,"content":"赞！这些优化手段都可以拿去面试。\n1. 可以总结为就是尽可能查询少的字段，以及按照查询频率进行垂直分表。\n3. 可以说是新手研发的常见坑了，大部分都是为了偷懒而引起的。\n4. 是，冗余都好说，就是又要解决一致性问题，也挺麻烦的","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689424253,"ip_address":"广东","comment_id":377807,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"1. 按前台业务和后台业务分表，前台业务表不包含各种审核状态，只包含纯粹的业务数据，所以表数据更少，字段更小，每次读到buffer pool 中能读更多的数据，性能更好\n2. 避免like查询，搜索功能使用专业的搜索引擎，而且还能更加为业务提供更多发展空间，比如es的分词，权重打分等\n3. 避免循环中查询单条数据的错误，写的时候要思考如果该接口高并发时会不会挂\n4. 适当冗余少量字段，做一定的反范式设计，尽量使用单表查询\n5. 索引是有代价的，在上千万的数据中，有时候索引占的空间比表数据更大，一定要避免滥用，不要为了sql建索引，而是从业务角度考虑索引","like_count":11,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623406,"discussion_content":"赞！这些优化手段都可以拿去面试。\n1. 可以总结为就是尽可能查询少的字段，以及按照查询频率进行垂直分表。\n3. 可以说是新手研发的常见坑了，大部分都是为了偷懒而引起的。\n4. 是，冗余都好说，就是又要解决一致性问题，也挺麻烦的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689424253,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":377654,"user_name":"江 Nina","can_delete":false,"product_type":"c1","uid":1655563,"ip_address":"北京","ucode":"18E41503CC43C2","user_header":"https://static001.geekbang.org/account/avatar/00/19/43/0b/7688f18c.jpg","comment_is_top":false,"comment_ctime":1688960388,"is_pvip":false,"replies":[{"id":137644,"content":"max_id 是前端继续往后传。\n理论上来说，这个优化不适合做跳页。所以这个优化适合在 APP 这种下拉刷新的环境下。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689048099,"ip_address":"广东","comment_id":377654,"utype":1}],"discussion_count":10,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"[WHERE id &gt; max_id ] 具体实操过程中，max_id是如何获取的呢？假如直接点击第1000页的内容，是不是就没法实现了？","like_count":11,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623114,"discussion_content":"max_id 是前端继续往后传。\n理论上来说，这个优化不适合做跳页。所以这个优化适合在 APP 这种下拉刷新的环境下。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1689048099,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":3,"child_discussions":[{"author":{"id":1655563,"avatar":"https://static001.geekbang.org/account/avatar/00/19/43/0b/7688f18c.jpg","nickname":"江 Nina","note":"","ucode":"18E41503CC43C2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":623115,"discussion_content":"感谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689048173,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":623114,"ip_address":"北京","group_id":0},"score":623115,"extra":""},{"author":{"id":1544578,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIYNs91AuVo6HwQ9qbqrItiabh3m84dRhibvgxBcfz72VDKDkLVsrRicLsJxc2kbp3J3dse7ms56RFibg/132","nickname":"梦’~","note":"","ucode":"942B92E91692D3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":634280,"discussion_content":"类似douyin短视频的“瀑布流”","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1703151762,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":623114,"ip_address":"广东","group_id":0},"score":634280,"extra":""},{"author":{"id":1589523,"avatar":"https://static001.geekbang.org/account/avatar/00/18/41/13/bf8e85cc.jpg","nickname":"树心","note":"","ucode":"6C329F0FF798B9","race_medal":2,"user_type":1,"is_pvip":false},"reply_author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":647535,"discussion_content":"还有适用场景也很重要～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720151054,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":623114,"ip_address":"北京","group_id":0},"score":647535,"extra":""}]},{"author":{"id":1172050,"avatar":"https://static001.geekbang.org/account/avatar/00/11/e2/52/56dbb738.jpg","nickname":"牙小木","note":"","ucode":"E5C12D37A62949","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625386,"discussion_content":"左滑，右滑，上滑，下滑这种，跳页的没办法。或者中间限制成...","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1691636789,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1065064,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eooNCNEO0vhRiagdrCnNW2LWzzV4g5tXJ9KkTu9hegCTx6lBrA06AZ3Uylb2wdKjvtrmZUWkKKHTGA/132","nickname":"TimJuly","note":"","ucode":"56FE7BF7447DEA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623071,"discussion_content":"软件开发没有银弹，一些优化也只限定在特定场景用，例如你看百度很多场景也只支持向后翻一页，不让一下翻多页。真有翻1000页的需求，就不要考虑mysql了，这不是它擅长的。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1689016938,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1655563,"avatar":"https://static001.geekbang.org/account/avatar/00/19/43/0b/7688f18c.jpg","nickname":"江 Nina","note":"","ucode":"18E41503CC43C2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1065064,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eooNCNEO0vhRiagdrCnNW2LWzzV4g5tXJ9KkTu9hegCTx6lBrA06AZ3Uylb2wdKjvtrmZUWkKKHTGA/132","nickname":"TimJuly","note":"","ucode":"56FE7BF7447DEA","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623116,"discussion_content":"感谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689048179,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":623071,"ip_address":"北京","group_id":0},"score":623116,"extra":""},{"author":{"id":3874436,"avatar":"","nickname":"Geek_933088","note":"","ucode":"D757733C3107D6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1065064,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eooNCNEO0vhRiagdrCnNW2LWzzV4g5tXJ9KkTu9hegCTx6lBrA06AZ3Uylb2wdKjvtrmZUWkKKHTGA/132","nickname":"TimJuly","note":"","ucode":"56FE7BF7447DEA","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":641320,"discussion_content":"确实，我在做这种类似需求的时候也是让前端限制往后跳的页数的，比如有几千万数据，一页放10条，如果不做任何限制是可以直接跳一百万页的，但实际上不会有人真这么去看数据的，只有测试会关注到这样实现有问题，然后让你去优化，现在统一改成只有往后一页一页翻，不允许自定义页数了，\n","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1712497708,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":623071,"ip_address":"广东","group_id":0},"score":641320,"extra":""}]},{"author":{"id":2736557,"avatar":"https://static001.geekbang.org/account/avatar/00/29/c1/ad/62d3df46.jpg","nickname":"剑存","note":"","ucode":"41BFAC1264864E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623146,"discussion_content":"感觉可以这样：\nselect id, other_columns\nfrom table\nwhere id &gt;= (select id from table order by id limit 1 offset 1000*50)\norder by id \nlimit 50","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689079066,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"澳大利亚","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1184140,"avatar":"https://static001.geekbang.org/account/avatar/00/12/11/8c/503e440e.jpg","nickname":"柴墟散人","note":"","ucode":"DCD96E17C89408","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2736557,"avatar":"https://static001.geekbang.org/account/avatar/00/29/c1/ad/62d3df46.jpg","nickname":"剑存","note":"","ucode":"41BFAC1264864E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":645909,"discussion_content":"这里面不还是用了深度分页吗？性能可能还不如原来的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1716974446,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":623146,"ip_address":"江苏","group_id":0},"score":645909,"extra":""}]}]},{"had_liked":false,"id":377698,"user_name":"第一装甲集群司令克莱斯特","can_delete":false,"product_type":"c1","uid":1265707,"ip_address":"北京","ucode":"4E8FBB23AD860B","user_header":"https://static001.geekbang.org/account/avatar/00/13/50/2b/2344cdaa.jpg","comment_is_top":false,"comment_ctime":1689054749,"is_pvip":false,"replies":[{"id":137651,"content":"学到了！居然还有专业名词！","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689068347,"ip_address":"广东","comment_id":377698,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"SQL查询分页偏移量大的问题，又名深度分页。","like_count":7,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623136,"discussion_content":"学到了！居然还有专业名词！","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1689068347,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381680,"user_name":"sheep","can_delete":false,"product_type":"c1","uid":2770150,"ip_address":"广东","ucode":"DAC2036F08CE27","user_header":"https://static001.geekbang.org/account/avatar/00/2a/44/e6/2c97171c.jpg","comment_is_top":false,"comment_ctime":1695658101,"is_pvip":false,"replies":[{"id":139030,"content":"确实，哈哈哈，不过糊弄老板的时候可以用用。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1695728036,"ip_address":"广东","comment_id":381680,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"&quot;SHOW TABLE STATUS 也能看到一个 TABLE_ROWS 列，代表表的行数，那么能不能用这个来优化 COUNT(*)？&quot;\n不太行，索引统计的值是通过采样来估算的。实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。\n《MySQL实战45讲》的第14章，也有介绍这一点","like_count":6,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628697,"discussion_content":"确实，哈哈哈，不过糊弄老板的时候可以用用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695728037,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378609,"user_name":"浩仔是程序员","can_delete":false,"product_type":"c1","uid":1104601,"ip_address":"广东","ucode":"A7E5CF9E1571A2","user_header":"https://static001.geekbang.org/account/avatar/00/10/da/d9/f051962f.jpg","comment_is_top":false,"comment_ctime":1690476278,"is_pvip":false,"replies":[{"id":137981,"content":"回表其实是很正常的事情，只是说在性能苛刻的场景下，要避免回表。目前的数据库回表一次不到 10ms，基本都能接受。\n\n我觉得你是想问这种动态筛选怎么设置索引吧？一般我建议是用 ES 比较好，如果用 MySQL，我建议你在最频繁的几个查询条件里面设置索引。又或者，可以通过业务折中来达成目标，比如说要求某些字段是必填。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1690605709,"ip_address":"广东","comment_id":378609,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"老师你好，如果一个表单管理，有很多的筛选条件，超过10个，需要怎么设置索引比较合理，好像也避免不了回表的问题","like_count":6,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":624480,"discussion_content":"回表其实是很正常的事情，只是说在性能苛刻的场景下，要避免回表。目前的数据库回表一次不到 10ms，基本都能接受。\n\n我觉得你是想问这种动态筛选怎么设置索引吧？一般我建议是用 ES 比较好，如果用 MySQL，我建议你在最频繁的几个查询条件里面设置索引。又或者，可以通过业务折中来达成目标，比如说要求某些字段是必填。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1690605709,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":390018,"user_name":"benxiong","can_delete":false,"product_type":"c1","uid":1624574,"ip_address":"湖北","ucode":"F6498059D439D9","user_header":"https://static001.geekbang.org/account/avatar/00/18/c9/fe/874b172b.jpg","comment_is_top":false,"comment_ctime":1714114491,"is_pvip":false,"replies":[{"id":141970,"content":"你正序倒序都能用。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1715230336,"ip_address":"广东","comment_id":390018,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"请问下，优化 order by 这里，SELECT * FROM  xxx WHERE uid = 123 ORDER BY update_time，新建联合索引 &lt;uid,  update_time&gt;，uid 确定之后，update_time 自然是有序的。像我们线上都是最新订单数据放在上面，也就是倒序排序，这种情况，加联合索引是无法解决的吧，还有其他好的解决方案吗","like_count":1,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":644390,"discussion_content":"你正序倒序都能用。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1715230336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381670,"user_name":"sheep","can_delete":false,"product_type":"c1","uid":2770150,"ip_address":"广东","ucode":"DAC2036F08CE27","user_header":"https://static001.geekbang.org/account/avatar/00/2a/44/e6/2c97171c.jpg","comment_is_top":false,"comment_ctime":1695635974,"is_pvip":false,"replies":[{"id":139038,"content":"也没你想象中的那么麻烦，就是业务层面上，操作数据库的时候，顺便操作一下redis。不过如果 COUNT 的 WHERE 条件很复杂，就很难处理。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1695728534,"ip_address":"广东","comment_id":381670,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"&quot;优化 COUNT&quot;这里&quot;考虑使用 Redis 之类的 NoSQL 来直接记录总数&quot;，这里不太好记录吧，像MySQL有一致性视图就很难模拟吧，市场有啥成熟的案例咩","like_count":1,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628705,"discussion_content":"也没你想象中的那么麻烦，就是业务层面上，操作数据库的时候，顺便操作一下redis。不过如果 COUNT 的 WHERE 条件很复杂，就很难处理。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695728535,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378087,"user_name":"humor","can_delete":false,"product_type":"c1","uid":1181867,"ip_address":"浙江","ucode":"9B48C4C7BEC92C","user_header":"https://static001.geekbang.org/account/avatar/00/12/08/ab/caec7bca.jpg","comment_is_top":false,"comment_ctime":1689742765,"is_pvip":false,"replies":[{"id":137831,"content":"风险还是很大的……因为虽然online DDL 不会阻塞增删改查，但是对服务器的压力还是存在的。也就是你这段时间 mysql 的负载还是很高。所以要看你能不能接受这种性能损耗。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689954356,"ip_address":"广东","comment_id":378087,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"修改索引或者说表定义变更的核心问题是数据库会加表锁，直到修改完成。\n好像mysql是支持online ddl的吧。那如果支持的话，是不是对大表也可以直接ddl了","like_count":1,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623898,"discussion_content":"风险还是很大的……因为虽然online DDL 不会阻塞增删改查，但是对服务器的压力还是存在的。也就是你这段时间 mysql 的负载还是很高。所以要看你能不能接受这种性能损耗。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689954356,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1703085,"avatar":"","nickname":"Geek_20f6bc","note":"","ucode":"AB00FE5F029840","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625931,"discussion_content":"online DDL也是会阻塞增删改查的; \n普通的DDL流程是:先拿到表的元数据锁(写锁),修改表定义,修改表数据,然后等所有工作完成后释放元数据锁,这期间会阻塞所有增删改查;\nonlineDDL的优化是, 先拿到元数据锁(写锁), 修改表定义完成(很快),然后就可以降级为元数据读锁, 增删改查就可以进来并发执行了.\n这里面有个很大的风险点,就是online ddl一开始尝试拿元数据写锁时, 如果这时候表上面有个慢sql迟迟执行不完(DML会占有元数据读锁), 会导致online ddl被阻塞,一直拿不到元数据写锁, 进而导致online ddl后面所有的增删改查都会被阻塞.\n(个人目前的认知, 有更了解的同学可以来说说)\nddl和锁很复杂,不同的ddl都会有不同的表现, 感兴趣的可以去看看mysql手册\nhttps://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-operations.html;\nhttps://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-performance.html;\n\n","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1692330317,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378052,"user_name":"ZhiguoXue_IT","can_delete":false,"product_type":"c1","uid":2639055,"ip_address":"北京","ucode":"EAA83F53B54520","user_header":"https://static001.geekbang.org/account/avatar/00/28/44/cf/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1689692030,"is_pvip":false,"replies":[{"id":137782,"content":"确实，我以前也做过这种优化，不过还是挺罕见的场景","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689742074,"ip_address":"广东","comment_id":378052,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100551601,"comment_content":"在之前的团队中用过强制force指定索引，之所以这样做是为了在mysql表的数据量特别大的时候，mysql自己的内部优化器会给推荐别的索引，现实中发现强制有另一个索引，查询的性能会很大，也就是可以理解为mysql自己的查询优化器有时候会选择错误的索引","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623677,"discussion_content":"确实，我以前也做过这种优化，不过还是挺罕见的场景","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689742074,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378051,"user_name":"ZhiguoXue_IT","can_delete":false,"product_type":"c1","uid":2639055,"ip_address":"北京","ucode":"EAA83F53B54520","user_header":"https://static001.geekbang.org/account/avatar/00/28/44/cf/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1689691889,"is_pvip":false,"replies":[{"id":137781,"content":"GROUP BY 应该不行。不过要是 name 上有单独的索引，那么查询效果还可以。\n有一种不精确的做法，就是线下计算一次，count(distinct(name)) 和 count(name)，确认两者的比率。而后线上就只需要用 count(name) 乘以比例就可以了。又或者在 Redis 里面维护总数。\n","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689742041,"ip_address":"广东","comment_id":378051,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"1）请教一下老师，select count(distinct(name)) from user 这条sql如何优化，能用group by优化吗？select name from user group by name;\n","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623676,"discussion_content":"GROUP BY 应该不行。不过要是 name 上有单独的索引，那么查询效果还可以。\n有一种不精确的做法，就是线下计算一次，count(distinct(name)) 和 count(name)，确认两者的比率。而后线上就只需要用 count(name) 乘以比例就可以了。又或者在 Redis 里面维护总数。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689742042,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":377791,"user_name":"景","can_delete":false,"product_type":"c1","uid":1022318,"ip_address":"上海","ucode":"147E386502D9C0","user_header":"https://static001.geekbang.org/account/avatar/00/0f/99/6e/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1689216351,"is_pvip":false,"replies":[{"id":137710,"content":"一个联合索引就可以了，然后把 uid 放在前面。","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689424313,"ip_address":"广东","comment_id":377791,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"「在所有的排序场景中，都应该尽量利用索引来排序，这样能够有效减轻数据库的负担，加快响应速度。进一步来说，像 ORDER BY，DISTINCT 等这样的操作也可以用类似的思路。」\n\n在这个例子中，如果按 uid, uid &amp; update_time 的查询场景都有，那是建两个索引（一个单列索引，一个联合索引）还是一个联合索引呢？","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623407,"discussion_content":"一个联合索引就可以了，然后把 uid 放在前面。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689424313,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2525135,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/n8QDxaKGrjIjnfnhGZ6huqiaYrib9eJbAh4S86QpGnKv6Dnt28UZEX7rWsDcBItic8HnhGKeLGW7IBRGWrjOStE6w/132","nickname":"周磊","note":"","ucode":"2038A2A5567632","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623341,"discussion_content":"一个联合索引，uid在前面","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689319226,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":377673,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1688996997,"is_pvip":false,"replies":[{"id":137641,"content":"\n1 有比较多可以优化的选项，我讲几个。\n第一个是优化磁盘调度策略，从 CFG 切换到 deadline，这两个调度策略你可以了解一下。这个优化我个人没有实践过，不知道它的效果如何；\n另外一个是优化 swap，核心是尽量避免触发交换，但是也不能完全禁止 swap，因为可能内存不够崩了。\n还有就是优化 TCP 连接相关的，比如说降低 fin_timeout，增大发送、接收缓冲区。\n对于大部分和网络、IO 相关的中间件，你都可以从磁盘 IO、swap、TCP 三个方向上去优化。\n2 啊，我发现我算错了，应该是 LIMIT 5000, 50。这就是分页的算法，第一页是 LIMIT 0, 50，第二页是 LIMIT 50, 50... 第 101 页就应该是 LIMIT 5000, 50","user_name":"作者回复","user_name_real":"编辑","uid":1176655,"ctime":1689047723,"ip_address":"广东","comment_id":377673,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100551601,"comment_content":"请教老师两个问题：\nQ1：操作系统的设置可以优化数据库？\n以前没有注意到这个方面。以MySQL为例，win10下，调整哪些参数可以改善MySQL性能？（也可以以Linux为例说明）。\nQ2：为什么Limit后面是5000？\n“当你要拿第101页的数据，需要写成 LIMIT 50000, 50。50000 就是偏移量”，为什么要写成“需要写成 LIMIT 50000, 50”？","like_count":0,"discussions":[{"author":{"id":1176655,"avatar":"https://static001.geekbang.org/account/avatar/00/11/f4/4f/aa916c8c.jpg","nickname":"邓小明","note":"","ucode":"02243D1F7492A6","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":623110,"discussion_content":"\n1 有比较多可以优化的选项，我讲几个。\n第一个是优化磁盘调度策略，从 CFG 切换到 deadline，这两个调度策略你可以了解一下。这个优化我个人没有实践过，不知道它的效果如何；\n另外一个是优化 swap，核心是尽量避免触发交换，但是也不能完全禁止 swap，因为可能内存不够崩了。\n还有就是优化 TCP 连接相关的，比如说降低 fin_timeout，增大发送、接收缓冲区。\n对于大部分和网络、IO 相关的中间件，你都可以从磁盘 IO、swap、TCP 三个方向上去优化。\n2 啊，我发现我算错了，应该是 LIMIT 5000, 50。这就是分页的算法，第一页是 LIMIT 0, 50，第二页是 LIMIT 50, 50... 第 101 页就应该是 LIMIT 5000, 50","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689047723,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}