{"id":428186,"title":"05 | Tensor变形记：快速掌握Tensor切分、变形等方法","content":"<p>你好，我是方远。</p><p>上节课我们一起学习了Tensor的基础概念，也熟悉了创建、转换、维度变换等操作，掌握了这些基础知识，你就可以做一些简单的Tensor相关的操作了。</p><p>不过，要想在实际的应用中更灵活地用好Tensor，Tensor的连接、切分等操作也是必不可少的。今天这节课，咱们就通过一些例子和图片来一块学习下。虽然这几个操作比较有难度，但只要你耐心听我讲解，然后上手练习，还是可以拿下的。</p><h2>Tensor的连接操作</h2><p>在项目开发中，深度学习某一层神经元的数据可能有多个不同的来源，那么就需要将数据进行组合，这个组合的操作，我们称之为<strong>连接</strong>。</p><h3>cat</h3><p>连接的操作函数如下。</p><pre><code class=\"language-python\">torch.cat(tensors, dim = 0, out = None)\n</code></pre><p>cat是concatnate的意思，也就是拼接、联系的意思。该函数有两个重要的参数需要你掌握。</p><p>第一个参数是tensors，它很好理解，就是若干个我们准备进行拼接的Tensor。</p><p>第二个参数是dim，我们回忆一下Tensor的定义，Tensor的维度（秩）是有多种情况的。比如有两个3维的Tensor，可以有几种不同的拼接方式（如下图），dim参数就可以对此作出约定。</p><p><img src=\"https://static001.geekbang.org/resource/image/61/3c/61bd88f3yy8d0ca07799f36540d3473c.jpg?wh=1285x862\" alt=\"图片\"></p><p>看到这里，你可能觉得上面画的图是三维的，看起来比较晦涩，所以咱们先从简单的二维的情况说起，我们先声明两个3x3的矩阵，代码如下：</p><!-- [[[read_end]]] --><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.ones(3,3)\n&gt;&gt;&gt; B=2*torch.ones(3,3)\n&gt;&gt;&gt; A\ntensor([[1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.]])\n&gt;&gt;&gt; B\ntensor([[2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.]])\n</code></pre><p>我们先看看dim=0的情况，拼接的结果是怎样的：</p><pre><code class=\"language-python\">&gt;&gt;&gt; C=torch.cat((A,B),0)\n&gt;&gt;&gt; C\ntensor([[1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [2., 2., 2.]])\n\n</code></pre><p>你会发现，两个矩阵是按照“行”的方向拼接的。</p><p>我们接下来再看看，dim=1的情况是怎样的：</p><pre><code class=\"language-python\">&gt;&gt;&gt; D=torch.cat((A,B),1)\n&gt;&gt;&gt; D\ntensor([[1., 1., 1., 2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 2., 2., 2.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 2., 2., 2.]])\n</code></pre><p>显然，两个矩阵，是按照“列”的方向拼接的。那如果Tensor是三维甚至更高维度的呢？其实道理也是一样的，dim的数值是多少，两个矩阵就会按照相应维度的方向链接两个Tensor。</p><p>看到这里你可能会问了，cat实际上是将多个Tensor在已有的维度上进行连接，那如果想增加新的维度进行连接，又该怎么做呢？这时候就需要stack函数登场了。</p><h3>stack</h3><p>为了让你加深理解，我们还是结合具体例子来看看。假设我们有两个二维矩阵Tensor，把它们“堆叠”放在一起，构成一个三维的Tensor，如下图：</p><p><img src=\"https://static001.geekbang.org/resource/image/9d/66/9d991a0d571e2733ba15d67566f65166.jpg?wh=1160x770\" alt=\"图片\"></p><p>这相当于原来的维度（秩）是2，现在变成了3，变成了一个立体的结构，增加了一个维度。你需要注意的是，这跟前面的cat不同，cat中示意图的例子，原来就是3维的，cat之后仍旧是3维的，而现在咱们是<strong>从2维变成了3维</strong>。</p><p>在实际图像算法开发中，咱们有时候需要将多个单通道Tensor（2维）合并，得到多通道的结果（3维）。而实现这种增加维度拼接的方法，我们把它叫做stack。</p><p>stack函数的定义如下：</p><pre><code class=\"language-python\">torch.stack(inputs, dim=0)\n</code></pre><p>其中，inputs表示需要拼接的Tensor，dim表示新建立维度的方向。</p><p>那stack如何使用呢？我们一块来看一个例子：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.arange(0,4)\n&gt;&gt;&gt; A\ntensor([0, 1, 2, 3])\n&gt;&gt;&gt; B=torch.arange(5,9)\n&gt;&gt;&gt; B\ntensor([5, 6, 7, 8])\n&gt;&gt;&gt; C=torch.stack((A,B),0)\n&gt;&gt;&gt; C\ntensor([[0, 1, 2, 3],\n        [5, 6, 7, 8]])\n&gt;&gt;&gt; D=torch.stack((A,B),1)\n&gt;&gt;&gt; D\ntensor([[0, 5],\n        [1, 6],\n        [2, 7],\n        [3, 8]])\n</code></pre><p>结合代码，我们可以看到，首先我们构建了两个4元素向量A和B，它们的维度是1。然后，我们在dim=0，也就是“行”的方向上新建一个维度，这样维度就成了2，也就得到了C。而对于D，我们则是在dim=1，也就是“列”的方向上新建维度。</p><h2>Tensor的切分操作</h2><p>学完了连接操作之后，我们再来看看连接的逆操作：<strong>切分</strong>。</p><p>切分就是连接的逆过程，有了刚才的经验，你很容易就会想到，切分的操作也应该有很多种，比如切片、切块等。没错，切分的操作主要分为三种类型：chunk、split、unbind。</p><p>乍一看有不少，其实是因为它们各有特点，适用于不同的使用情景，让我们一起看一下。</p><h3>chunk</h3><p>chunk的作用就是将Tensor按照声明的dim，进行尽可能平均的划分。</p><p>比如说，我们有一个32channel的特征，需要将其按照channel均匀分成4组，每组8个channel，这个切分就可以通过chunk函数来实现。具体函数如下：</p><pre><code class=\"language-python\">torch.chunk(input, chunks, dim=0)\n</code></pre><p>我们挨个来看看函数中涉及到的三个参数：</p><p>首先是input，它表示要做chunk操作的Tensor。</p><p>接着，我们看下chunks，它代表将要被划分的块的数量，而不是每组的数量。请注意，<strong>chunks必须是整型</strong>。</p><p>最后是dim，想想这个参数是什么意思呢？对，就是按照哪个维度来进行chunk。</p><p>还是跟之前一样，我们通过几个代码例子直观感受一下。我们从一个简单的一维向量开始：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([1,2,3,4,5,6,7,8,9,10])\n&gt;&gt;&gt; B = torch.chunk(A, 2, 0)\n&gt;&gt;&gt; B\n(tensor([1, 2, 3, 4, 5]), tensor([ 6,&nbsp; 7,&nbsp; 8,&nbsp; 9, 10]))\n</code></pre><p>这里我们通过chunk函数，将原来10位长度的Tensor A，切分成了两个一样5位长度的向量。（注意，B是两个切分结果组成的tuple）。</p><p>那如果chunk参数不能够整除的话，结果会是怎样的呢？我们接着往下看：</p><pre><code class=\"language-python\">&gt;&gt;&gt; B = torch.chunk(A, 3, 0)\n&gt;&gt;&gt; B\n(tensor([1, 2, 3, 4]), tensor([5, 6, 7, 8]), tensor([ 9, 10]))\n</code></pre><p>我们发现，10位长度的Tensor A，切分成了三个向量，长度分别是4，4，2位。这是怎么分的呢，不应该是3，3，4这样更为平均的方式么？</p><p>想要解决问题，就得找到规律。让我们再来看一个更大一点的例子，将A改为17位长度。</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])\n&gt;&gt;&gt; B = torch.chunk(A, 4, 0)\n&gt;&gt;&gt; B\n(tensor([1, 2, 3, 4, 5]), tensor([ 6,&nbsp; 7,&nbsp; 8,&nbsp; 9, 10]), tensor([11, 12, 13, 14, 15]), tensor([16, 17]))\n</code></pre><p>17位长度的Tensor A，切分成了四个分别为5，5，5，2位长度的向量。这时候你就会发现，其实在计算每个结果元素个数的时候，chunk函数是先做除法，然后再向上取整得到每组的数量。</p><p>比如上面这个例子，17/4=4.25，向上取整就是5，那就先逐个生成若干个长度为5的向量，最后不够的就放在一块，作为最后一个向量（长度2）。</p><p>那如果chunk参数大于Tensor可以切分的长度，又要怎么办呢？我们实际操作一下，代码如下：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([1,2,3])\n&gt;&gt;&gt; B = torch.chunk(A, 5, 0)\n&gt;&gt;&gt; B\n(tensor([1]), tensor([2]), tensor([3]))\n</code></pre><p>显然，被切分的Tensor只能分成若干个长度为1的向量。</p><p>由此可以推论出二维的情况，我们再举一个例子， 看看二维矩阵Tensor的情况 ：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.ones(4,4)\n&gt;&gt;&gt; A\ntensor([[1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.]])\n&gt;&gt;&gt; B = torch.chunk(A, 2, 0)\n&gt;&gt;&gt; B\n(tensor([[1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.]]), \ntensor([[1., 1., 1., 1.],\n&nbsp; &nbsp; &nbsp; &nbsp; [1., 1., 1., 1.]]))\n</code></pre><p>还是跟前面的cat一样，这里的dim参数，表示的是第dim维度方向上进行切分。</p><p>刚才介绍的chunk函数，是按照“切分成确定的份数”来进行切分的，那如果想按照“每份按照确定的大小”来进行切分，该怎样做呢？PyTorch也提供了相应的方法，叫做split。</p><h3>split</h3><p>split的函数定义如下，跟前面一样，我们还是分别看看这里涉及的参数。</p><pre><code class=\"language-python\">torch.split(tensor, split_size_or_sections, dim=0)\n</code></pre><p>首先是tensor，也就是待切分的Tensor。</p><p>然后是split_size_or_sections这个参数。当它为整数时，表示将tensor按照每块大小为这个整数的数值来切割；当这个参数为列表时，则表示将此tensor切成和列表中元素一样大小的块。</p><p>最后同样是dim，它定义了要按哪个维度切分。</p><p>同样的，我们举几个例子来看一下split的具体操作。首先是split_size_or_sections是整数的情况。</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.rand(4,4)\n&gt;&gt;&gt; A\ntensor([[0.6418, 0.4171, 0.7372, 0.0733],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.0935, 0.2372, 0.6912, 0.8677],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.5263, 0.4145, 0.9292, 0.5671],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.2284, 0.6938, 0.0956, 0.3823]])\n&gt;&gt;&gt; B=torch.split(A, 2, 0)\n&gt;&gt;&gt; B\n(tensor([[0.6418, 0.4171, 0.7372, 0.0733],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.0935, 0.2372, 0.6912, 0.8677]]), \ntensor([[0.5263, 0.4145, 0.9292, 0.5671],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.2284, 0.6938, 0.0956, 0.3823]]))\n</code></pre><p>在这个例子里，我们看到，原来4x4大小的Tensor A，沿着第0维度，也就是沿“行”的方向，按照每组2“行”的大小进行切分，得到了两个2x4大小的Tensor。</p><p>那么问题来了，如果split_size_or_sections不能整除对应方向的大小的话，会有怎样的结果呢？我们将代码稍作修改就好了：</p><pre><code class=\"language-python\">&gt;&gt;&gt; C=torch.split(A, 3, 0)\n&gt;&gt;&gt; C\n(tensor([[0.6418, 0.4171, 0.7372, 0.0733],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.0935, 0.2372, 0.6912, 0.8677],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.5263, 0.4145, 0.9292, 0.5671]]), \ntensor([[0.2284, 0.6938, 0.0956, 0.3823]]))\n</code></pre><p>根据刚才的代码我们就能发现，原来，PyTorch会尽可能凑够每一个结果，使得其对应dim的数据大小等于split_size_or_sections。如果最后剩下的不够，那就把剩下的内容放到一块，作为最后一个结果。</p><p>接下来，我们再看一下split_size_or_sections是列表时的情况。刚才提到了，当split_size_or_sections为列表的时候，表示将此tensor切成和列表中元素大小一样的大小的块，我们来看一段对应的代码：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.rand(5,4)\n&gt;&gt;&gt; A\ntensor([[0.1005, 0.9666, 0.5322, 0.6775],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.4990, 0.8725, 0.5627, 0.8360],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.3427, 0.9351, 0.7291, 0.7306],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7939, 0.3007, 0.7258, 0.9482],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7249, 0.7534, 0.0027, 0.7793]])\n&gt;&gt;&gt; B=torch.split(A,(2,3),0)\n&gt;&gt;&gt; B\n(tensor([[0.1005, 0.9666, 0.5322, 0.6775],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.4990, 0.8725, 0.5627, 0.8360]]), \ntensor([[0.3427, 0.9351, 0.7291, 0.7306],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7939, 0.3007, 0.7258, 0.9482],\n&nbsp; &nbsp; &nbsp; &nbsp; [0.7249, 0.7534, 0.0027, 0.7793]]))\n</code></pre><p>这部分代码怎么解释呢？其实也很好理解，就是将Tensor A，沿着第0维进行切分，每一个结果对应维度上的尺寸或者说大小，分别是2（行），3（行）。</p><h3>unbind</h3><p>通过学习前面的几个函数，咱们知道了怎么按固定大小做切分，或者按照索引index来进行选择。现在我们想象一个应用场景，如果我们现在有一个3 channel图像的Tensor，想要逐个获取每个channel的数据，该怎么做呢？</p><p>假如用chunk的话，我们需要将chunks设为3；如果用split的话，需要将split_size_or_sections设为1。</p><p>虽然它们都可以实现相同的目的，但是如果channel数量很大，逐个去取也比较折腾。这时候，就需要用到另一个函数：unbind，它的函数定义如下：</p><pre><code class=\"language-python\">torch.unbind(input, dim=0)\n</code></pre><p>其中，input表示待处理的Tensor，dim还是跟前面的函数一样，表示切片的方向。</p><p>我们结合例子来理解：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.arange(0,16).view(4,4)\n&gt;&gt;&gt; A\ntensor([[ 0,&nbsp; 1,&nbsp; 2,&nbsp; 3],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 4,&nbsp; 5,&nbsp; 6,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 8,&nbsp; 9, 10, 11],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 13, 14, 15]])\n&gt;&gt;&gt; b=torch.unbind(A, 0)\n&gt;&gt;&gt; b\n(tensor([0, 1, 2, 3]), \ntensor([4, 5, 6, 7]), \ntensor([ 8,&nbsp; 9, 10, 11]), \ntensor([12, 13, 14, 15]))\n</code></pre><p>在这个例子中，我们首先创建了一个4x4的二维矩阵Tensor，随后我们从第0维，也就是“行”的方向进行切分 ，因为矩阵有4行，所以就会得到4个结果。</p><p>接下来，我们看一下：如果从第1维，也就是“列”的方向进行切分，会是怎样的结果呢：</p><pre><code class=\"language-python\">&gt;&gt;&gt; b=torch.unbind(A, 1)\n&gt;&gt;&gt; b\n(tensor([ 0,&nbsp; 4,&nbsp; 8, 12]), \ntensor([ 1,&nbsp; 5,&nbsp; 9, 13]), \ntensor([ 2,&nbsp; 6, 10, 14]), \ntensor([ 3,&nbsp; 7, 11, 15]))\n</code></pre><p>不难发现，这里是按照“列”的方向进行拆解的。所以，<strong>unbind是一种降维切分的方式</strong>，相当于删除一个维度之后的结果。</p><h2>Tensor的索引操作</h2><p>你有没有发现，刚才我们讲的chunk和split操作，我们都是将数据整体进行切分，并获得全部结果。但有的时候，我们只需要其中的一部分，这要怎么做呢？一个很自然的想法就是，直接告诉Tensor我想要哪些部分，这种方法我们称为索引操作。</p><p>索引操作有很多方式，有提供好现成API的，也有用户自行定制的操作，其中最常用的两个操作就是index_select和masked_select，我们分别去看看用法。</p><h3>index_select</h3><p>这里就需要index_select这个函数了，其定义如下：</p><pre><code class=\"language-python\">torch.index_select(tensor, dim, index)\n</code></pre><p>这里的tensor、dim跟前面函数里的一样，不再赘述。我们重点看一看index，它表示从dim维度中的哪些位置选择数据，这里需要注意，index<strong>是torch.Tensor类型</strong>。</p><p>还是跟之前一样，我们来看几个示例代码：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.arange(0,16).view(4,4)\n&gt;&gt;&gt; A\ntensor([[ 0,&nbsp; 1,&nbsp; 2,&nbsp; 3],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 4,&nbsp; 5,&nbsp; 6,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 8,&nbsp; 9, 10, 11],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 13, 14, 15]])\n&gt;&gt;&gt; B=torch.index_select(A,0,torch.tensor([1,3]))\n&gt;&gt;&gt; B\ntensor([[ 4,&nbsp; 5,&nbsp; 6,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 13, 14, 15]])\n&gt;&gt;&gt; C=torch.index_select(A,1,torch.tensor([0,3]))\n&gt;&gt;&gt; C\ntensor([[ 0,&nbsp; 3],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 4,&nbsp; 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [ 8, 11],\n&nbsp; &nbsp; &nbsp; &nbsp; [12, 15]])\n</code></pre><p>在这个例子中，我们先创建了一个4x4大小的矩阵Tensor A。然后，我们从第0维选择第1（行）和3（行）的数据，并得到了最终的Tensor B，其大小为2x4。随后我们从Tensor A中选择第0（列）和3（列）的数据，得到了最终的Tensor C，其大小为4x2。</p><p>怎么样，是不是非常简单？</p><h3>masked_select</h3><p>刚才介绍的indexed_select，它是基于给定的索引来进行数据提取的。但有的时候，我们还想通过一些判断条件来进行选择，比如提取深度学习网络中某一层中数值大于0的参数。</p><p>这时候，就需要用到PyTorch提供的masked_select函数了，我们先来看它的定义：</p><pre><code class=\"language-python\">torch.masked_select(input, mask, out=None)&nbsp;\n</code></pre><p>这里我们只需要关心前两个参数，input和mask。</p><p>input表示待处理的Tensor。mask代表掩码张量，也就是满足条件的特征掩码。这里你需要注意的是，mask须跟input张量有相同数量的元素数目，但形状或维度不需要相同。</p><p>你是不是还感觉有些云里雾里？让我来举一个例子，你看了之后，一下子就能明白。</p><p>你在平时的练习中有没有想过，如果我们让Tensor和数字做比较，会有什么样的结果？比如后面这段代码，我们随机生成一个5位长度的Tensor A：</p><pre><code>&gt;&gt;&gt; A=torch.rand(5)\n&gt;&gt;&gt; A\ntensor([0.3731, 0.4826, 0.3579, 0.4215, 0.2285])\n&gt;&gt;&gt; B=A&gt;0.3\n&gt;&gt;&gt; B\ntensor([ True,  True,  True,  True, False])\n</code></pre><p>在这段代码里，我们让A跟0.3做比较，得到了一个新的Tensor，内部每一个数值表示的是A中对应数值是否大于0.3。</p><p>比如第一个数值原来是0.3731，大于0.3，所以是True；最后一个数值0.2285小于0.3，所以是False。</p><p>这个新的Tensor其实就是一个掩码张量，它的每一位表示了一个判断条件是否成立的结果。</p><p>然后，我们继续写一段代码，看看基于掩码B的选择是怎样的结果 ：</p><pre><code class=\"language-python\">&gt;&gt;&gt; C=torch.masked_select(A, B)\n&gt;&gt;&gt; C\ntensor([0.3731, 0.4826, 0.3579, 0.4215])\n</code></pre><p>你会发现，C实际上得到的就是：A中“<strong>满足B里面元素值为True的”</strong>对应位置的数据。</p><p>好了，这下你应该知道了masked_select的作用了吧？其实就是我们根据要筛选的条件，得到一个掩码张量，然后用这个张量去提取Tensor中的数据。</p><p>根据这个思路，上面的例子就可以简化为：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.rand(5)\n&gt;&gt;&gt; A\ntensor([0.3731, 0.4826, 0.3579, 0.4215, 0.2285])\n&gt;&gt;&gt; C=torch.masked_select(A, A&gt;0.3)\n&gt;&gt;&gt; C\ntensor([0.3731, 0.4826, 0.3579, 0.4215])\n</code></pre><p>是不是非常简单呢？</p><h2>小结</h2><p>恭喜你完成了这节课的学习。这节课，我们一同学习了Tensor里更加高级的操作，包括Tensor之间的连接操作，Tensor内部的切分操作，以及基于索引或者筛选条件的数据选择操作。</p><p>当然了，在使用这些函数的时候，你最需要关注的就是边界的数值大小，具体来说就是维度和大小相关的参数，一定要提前仔细计算好，要不然就会产生错误的结果。</p><p>结合众多的例子，我相信你一定可以拿下这些操作。</p><p>这里我特意给你梳理了一张表格，总结归纳了Tensor中的主要函数跟用法。不过这些参数咱们也不用死记硬背，我们在使用的时候，根据需要灵活查询相关的参数列表即可。<br>\n<img src=\"https://static001.geekbang.org/resource/image/d1/ba/d195706087f784c8e1e1c7c7b25a22ba.jpg?wh=3020x2455\" alt=\"\"><br>\n通过这两节课，我们搞懂了Tensor的一系列操作，在以后的项目中，你就可以游刃有余地对Tensor进行各种花式操作了，加油!</p><h2>每课一练</h2><p>现在有个Tensor，如下：</p><pre><code class=\"language-python\">&gt;&gt;&gt; A=torch.tensor([[4,5,7], [3,9,8],[2,3,4]])\n&gt;&gt;&gt; A\ntensor([[4, 5, 7],\n&nbsp; &nbsp; &nbsp; &nbsp; [3, 9, 8],\n&nbsp; &nbsp; &nbsp; &nbsp; [2, 3, 4]])\n</code></pre><p>我们想提取出其中第一行的第一个，第二行的第一、第二个，第三行的最后一个，该怎么做呢？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这节课分享给更多同事、朋友！</p>","neighbors":{"left":{"article_title":"04 | Tensor：PyTorch中最基础的计算单元","id":427460},"right":{"article_title":"06 | Torchvision（上）：数据读取，训练开始的第一步","id":429048}},"comments":[{"had_liked":false,"id":317169,"user_name":"lwg0452","can_delete":false,"product_type":"c1","uid":1853479,"ip_address":"","ucode":"0B58A1C80F9074","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKyEJx6dG2dMuMz6swdfjNuw3HMoEAbtxprfdBUAyRpLFayxmwEiaYLs224LuAdwWu55ENLgsI8U4w/132","comment_is_top":false,"comment_ctime":1634698897,"is_pvip":false,"replies":[{"id":"114902","content":"你好，lwg0452。👍👍👍👍👍👍回答正确^^。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1634720655,"ip_address":"","comment_id":317169,"utype":1}],"discussion_count":1,"race_medal":0,"score":"91829012113","product_id":100093301,"comment_content":"更正😀<br>mask = torch.tensor([[1, 0, 0], [1, 1, 0], [0, 0, 1]])<br>B = torch.masked_select(A, mask&gt;0)","like_count":22,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528695,"discussion_content":"你好，lwg0452。👍👍👍👍👍👍回答正确^^。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634720655,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325466,"user_name":"徐洲更","can_delete":false,"product_type":"c1","uid":1314643,"ip_address":"","ucode":"F8A323CB732D05","user_header":"https://static001.geekbang.org/account/avatar/00/14/0f/53/92a50f01.jpg","comment_is_top":false,"comment_ctime":1638965932,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"18818835116","product_id":100093301,"comment_content":"# 手动构建了一个True&#47;False<br>A = torch.tensor([[4,5,7],[4,9,8],[2,3,4]])<br>masked_index = torch.tensor( [[True,False,False], [True,True,False], [False,False,True] ])<br>torch.masked_select(A, masked_index) <br><br>#  这篇课程，个人觉得重要的几个知识点<br>1. index_select返回的结果和输入是一个维度，而masked_select返回一维输出<br>2. split获取的是原输入的视图，也就是对split的结果的操作会影响原来的数据<br>3. stack和cat的一个不同点在于，stack会升维，而cat不会。<br>","like_count":4},{"had_liked":false,"id":329475,"user_name":"GEEKBANG_9421399","can_delete":false,"product_type":"c1","uid":1702638,"ip_address":"","ucode":"1E3E1D6D6F2901","user_header":"","comment_is_top":false,"comment_ctime":1641363925,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14526265813","product_id":100093301,"comment_content":"其实如果tensor和mask的维度相同的话，直接tensor[mask]就是torch.masked_select(tensor, mask)的效果了。","like_count":3},{"had_liked":false,"id":341007,"user_name":"Geek_fc975d","can_delete":false,"product_type":"c1","uid":2459939,"ip_address":"","ucode":"05B6507FE4349B","user_header":"https://static001.geekbang.org/account/avatar/00/25/89/23/e71f180b.jpg","comment_is_top":false,"comment_ctime":1649292628,"is_pvip":false,"replies":[{"id":"124708","content":"^^ 👍🏻👍🏻👍🏻👍🏻","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1649403768,"ip_address":"","comment_id":341007,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5944259924","product_id":100093301,"comment_content":"抄作业<br><br># 手动构建了一个True&#47;False\r<br>A = torch.tensor([[4,5,7],[4,9,8],[2,3,4]])\r<br>masked_index = torch.tensor( [[True,False,False], [True,True,False], [False,False,True] ])\r<br>torch.masked_select(A, masked_index) \r<br>\r<br>#  这篇课程，个人觉得重要的几个知识点\r<br>1. index_select返回的结果和输入是一个维度，而masked_select返回一维输出\r<br>2. split获取的是原输入的视图，也就是对split的结果的操作会影响原来的数据\r<br>3. stack和cat的一个不同点在于，stack会升维，而cat不会。\r","like_count":1,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":560610,"discussion_content":"^^ 👍🏻👍🏻👍🏻👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649403768,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":339427,"user_name":"optimus","can_delete":false,"product_type":"c1","uid":1665132,"ip_address":"","ucode":"CDE90EED75EC9E","user_header":"https://static001.geekbang.org/account/avatar/00/19/68/6c/5b572baa.jpg","comment_is_top":false,"comment_ctime":1648094519,"is_pvip":true,"replies":[{"id":"124100","content":"👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1648112365,"ip_address":"","comment_id":339427,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5943061815","product_id":100093301,"comment_content":"eye  = torch.eye(3)<br>torch.masked_select(A,eye&gt;0)","like_count":2,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558169,"discussion_content":"👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648112365,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1045809,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f5/31/bbb513ba.jpg","nickname":"mtfelix","note":"","ucode":"8C838BC0FC1F58","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":585725,"discussion_content":"这个不对吧？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1661775867,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":356755,"user_name":"A🐴@伯乐","can_delete":false,"product_type":"c1","uid":3169896,"ip_address":"北京","ucode":"5410843A6A422C","user_header":"https://static001.geekbang.org/account/avatar/00/30/5e/68/2fafe647.jpg","comment_is_top":false,"comment_ctime":1662558373,"is_pvip":false,"replies":[{"id":"129859","content":"👍🏻👍🏻👍🏻","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1662624972,"ip_address":"北京","comment_id":356755,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1662558373","product_id":100093301,"comment_content":"import torch<br>A=torch.tensor([[4,5,7], [3,9,8],[2,3,4]])<br>B=torch.tensor([[1,0,0],[1,1,0],[1,0,0]])<br>C=torch.masked_select(A, A*B!=0)<br>print(C)","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":586951,"discussion_content":"👍🏻👍🏻👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1662624972,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":355798,"user_name":"mtfelix","can_delete":false,"product_type":"c1","uid":1045809,"ip_address":"北京","ucode":"8C838BC0FC1F58","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f5/31/bbb513ba.jpg","comment_is_top":false,"comment_ctime":1661756735,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1661756735","product_id":100093301,"comment_content":"cat不升维，stack升维。<br><br>cat就是拼接，stack是堆叠。<br><br>cat时要保证原来的tensors必须具备同样的维度个数(是一个空间的)，dim只是选择在哪个具体维度上拼接，以使得合二为一。那么，就要求除了dim指定的维度外，其他维度的大小必须是一致的。<br><br>stack时，要保证原来的tensors必须shape一致。dim只是选择在哪个维度上堆叠出未来的新维度。因此，如果原来维度数是n，那么dim选择范围就是[-n+1, n]。比如，原来是3维的，dim可以选择为-3,-2,-1,0,1,2。<br><br>","like_count":1},{"had_liked":false,"id":345122,"user_name":"亚林","can_delete":false,"product_type":"c1","uid":1018972,"ip_address":"","ucode":"4A5A6D24314B79","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg","comment_is_top":false,"comment_ctime":1652064355,"is_pvip":false,"replies":[{"id":"126080","content":"你好，亚林，感谢留言。<br>差不多这个意思吧。和后面的图像分割中的mask类似。","user_name":"作者回复","user_name_real":"编辑","uid":"2802608","ctime":1652249936,"ip_address":"","comment_id":345122,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1652064355","product_id":100093301,"comment_content":"这是个mask是不是就是传说中的数据打标","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":571508,"discussion_content":"你好，亚林，感谢留言。\n差不多这个意思吧。和后面的图像分割中的mask类似。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652249936,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319042,"user_name":"Geek_a95f0e","can_delete":false,"product_type":"c1","uid":2824511,"ip_address":"","ucode":"C0B7418FD1D2B5","user_header":"","comment_is_top":false,"comment_ctime":1635522009,"is_pvip":false,"replies":[{"id":"115644","content":"你好，谢谢你的留言。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻完全正确^^.","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635584774,"ip_address":"","comment_id":319042,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1635522009","product_id":100093301,"comment_content":"a=tc.tensor([[4,5,7],[3,9,8],[2,3,4]])<br>b=tc.tensor([1,0,0,1,1,0,0,0,1],dtype=tc.bool).reshape(3,3)<br>c=tc.masked_select(a,b)","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529470,"discussion_content":"你好，谢谢你的留言。👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻👍🏻完全正确^^.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635584774,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317937,"user_name":"王骥","can_delete":false,"product_type":"c1","uid":1049879,"ip_address":"","ucode":"CE53ADEA51235A","user_header":"https://static001.geekbang.org/account/avatar/00/10/05/17/63887353.jpg","comment_is_top":false,"comment_ctime":1635065628,"is_pvip":false,"replies":[{"id":"115254","content":"你好，王骥。感谢你的留言。很赞的问题，这里“形状或维度不需要相同”，我们举个二维矩阵的例子，你可以只写“一行”，或者所有“行”，但是“列”的数量必须跟原来的数量一致。如下代码：<br><br>a=torch.randn(3,5)<br>#不会报错<br>mask=torch.tensor([True, False,False,True,True])<br>torch.masked_select(a,mask)<br><br>#会报错<br>mask=torch.tensor([True, False,False,True])<br>torch.masked_select(a,mask)<br><br>此外，需要注意的是，官网强调：The shapes of the mask tensor and the input tensor don’t need to match, but they must be broadcastable。不过一般而言咱们用的变量基本多为broadcastable的。","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635124269,"ip_address":"","comment_id":317937,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1635065628","product_id":100093301,"comment_content":"input 表示待处理的 Tensor。mask 代表掩码张量，也就是满足条件的特征掩码。这里你需要注意的是，mask 须跟 input 张量有相同数量的元素数目，但形状或维度不需要相同。<br><br>老师，mask 须跟 input 张量有相同数量的元素数目，但形状或维度不需要相同 这句话该怎么理解？<br><br>  A = torch.tensor([[4, 5, 7], [3, 9, 8], [2, 3, 4]])<br>  B = torch.masked_select(A, torch.tensor([[1, 0, 0], [1, 1, 0], [0, 0, 1]]) &gt; 0)<br>  # B = torch.masked_select(A, torch.tensor([1, 0, 0, 1, 1, 0, 0, 0, 1]) &gt; 0)<br><br>我尝试下面一种就会报错。","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529037,"discussion_content":"你好，王骥。感谢你的留言。很赞的问题，这里“形状或维度不需要相同”，我们举个二维矩阵的例子，你可以只写“一行”，或者所有“行”，但是“列”的数量必须跟原来的数量一致。如下代码：\n\na=torch.randn(3,5)\n#不会报错\nmask=torch.tensor([True, False,False,True,True])\ntorch.masked_select(a,mask)\n\n#会报错\nmask=torch.tensor([True, False,False,True])\ntorch.masked_select(a,mask)\n\n此外，需要注意的是，官网强调：The shapes of the mask tensor and the input tensor don’t need to match, but they must be broadcastable。不过一般而言咱们用的变量基本多为broadcastable的。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1635124269,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1049879,"avatar":"https://static001.geekbang.org/account/avatar/00/10/05/17/63887353.jpg","nickname":"王骥","note":"","ucode":"CE53ADEA51235A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":408278,"discussion_content":"哦，懂了。所谓行就是最后一Rank。只要提供的mask元素数目和最后一Rank的数目相等就可以了。之所以需要broadcasttable，就是因为它可以实现元素的广播让缺失的维度通过广播补齐然后和input做比较。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635216406,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317805,"user_name":"李雄","can_delete":false,"product_type":"c1","uid":2805175,"ip_address":"","ucode":"ABC36CFEEEE41B","user_header":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","comment_is_top":false,"comment_ctime":1634975864,"is_pvip":false,"replies":[{"id":"115194","content":"你好，李雄。感谢你的留言，也谢谢你的认可。希望能与你一起学习进步^^","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1635078040,"ip_address":"","comment_id":317805,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1634975864","product_id":100093301,"comment_content":"喜欢这节的内容。","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528983,"discussion_content":"你好，李雄。感谢你的留言，也谢谢你的认可。希望能与你一起学习进步^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635078040,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2805175,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/cd/b7/6efa2c68.jpg","nickname":"李雄","note":"","ucode":"ABC36CFEEEE41B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":407653,"discussion_content":"谢谢老师回复。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635080893,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317488,"user_name":"vcjmhg","can_delete":false,"product_type":"c1","uid":1526461,"ip_address":"","ucode":"B508D1E9B3F974","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132","comment_is_top":false,"comment_ctime":1634812599,"is_pvip":false,"replies":[{"id":"115083","content":"你好，vcjmhg。👍🏻👍🏻👍🏻，每次的回答都很赞^^，加油~<br>不过masked_select中的参数是mask_matrix不是mask。：）","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1634863057,"ip_address":"","comment_id":317488,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634812599","product_id":100093301,"comment_content":"A = torch.tensor([[4, 5, 7], [3, 9, 8], [2, 3, 4]])<br>mask_matrix = torch.tensor([[1, 0, 0], [1, 1, 0], [0, 0, 1]])<br>B = torch.masked_select(A, mask == 1)","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528834,"discussion_content":"你好，vcjmhg。👍🏻👍🏻👍🏻，每次的回答都很赞^^，加油~\n不过masked_select中的参数是mask_matrix不是mask。：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634863057,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317231,"user_name":"栗白","can_delete":false,"product_type":"c1","uid":2810401,"ip_address":"","ucode":"8B0FBE195DA8B8","user_header":"https://static001.geekbang.org/account/avatar/00/2a/e2/21/3bb82a79.jpg","comment_is_top":false,"comment_ctime":1634717852,"is_pvip":false,"replies":[{"id":"115010","content":"你好，栗白。谢谢留言。👍👍👍👍👍👍👍👍完美。^^","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1634775847,"ip_address":"","comment_id":317231,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634717852","product_id":100093301,"comment_content":"A=torch.tensor([[4,5,7], [3,9,8],[2,3,4]])<br>B=torch.tensor([[1,0,0],[1,1,0],[0,0,1]])<br>C=torch.masked_select(A,B&gt;0)","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528726,"discussion_content":"你好，栗白。谢谢留言。👍👍👍👍👍👍👍👍完美。^^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634775847,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317183,"user_name":"narsil的梦","can_delete":false,"product_type":"c1","uid":1185187,"ip_address":"","ucode":"67110C673BD951","user_header":"https://static001.geekbang.org/account/avatar/00/12/15/a3/e67d6039.jpg","comment_is_top":false,"comment_ctime":1634701162,"is_pvip":false,"replies":[{"id":"114898","content":"你好，narsil的梦，感谢你的留言。tensor的格式呀，咱们现在做的就是tensor的操作","user_name":"作者回复","user_name_real":"方远","uid":"2802608","ctime":1634711180,"ip_address":"","comment_id":317183,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1634701162","product_id":100093301,"comment_content":"练习：<br>    A = torch.tensor([[4, 5, 7], [3, 9, 8], [2, 3, 4]])<br><br>    # 提取出其中第一行的第一个，第二行的第一、第二个，第三行的最后一个<br>    B = torch.chunk(A, 3, dim=0)<br><br>    A00 = torch.index_select(B[0], 1, torch.tensor([0]))<br>    A1011 = torch.index_select(B[1], 1, torch.tensor([0, 1]))<br>    A22 = torch.index_select(B[2], 1, torch.tensor([2]))<br><br>PS：题目应该说明输出是什么格式的，要不同学们不知道做到哪一步算完事","like_count":0,"discussions":[{"author":{"id":2802608,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK1ZkL9L7CEicI87xicpIhXAIhVdVWpJKBsD8Jpzg9iaAwFcDEhTvdRwuKItJS14mYznT2w2YQvn8QsQ/132","nickname":"方远","note":"","ucode":"248B1DE180EB4C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528705,"discussion_content":"你好，narsil的梦，感谢你的留言。tensor的格式呀，咱们现在做的就是tensor的操作","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634711180,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2823462,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/15/26/f973aa31.jpg","nickname":"平常心","note":"","ucode":"53CC9FBB44A483","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":408394,"discussion_content":"    B = torch.chunk(A, 3, dim=0)\n换成\n    B = torch.unbind(A,dim=0)，是不是就可以了\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635239711,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317164,"user_name":"lwg0452","can_delete":false,"product_type":"c1","uid":1853479,"ip_address":"","ucode":"0B58A1C80F9074","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKyEJx6dG2dMuMz6swdfjNuw3HMoEAbtxprfdBUAyRpLFayxmwEiaYLs224LuAdwWu55ENLgsI8U4w/132","comment_is_top":false,"comment_ctime":1634698544,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1634698544","product_id":100093301,"comment_content":"思考题 B = torch.masked_select(A, torch.eye(3) &gt; 0)","like_count":0,"discussions":[{"author":{"id":2156103,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7uYZvwhrLHsJICstGXaOvUNZnyq0aO7gF0OLicMyZAZFnRiaDyvM1lGxnEDP2VUMV3m6UjiazMmSNGQ/132","nickname":"Geek_8cfc4c","note":"","ucode":"83EF0B576530E2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":409094,"discussion_content":"第二行第一个也要，单位阵漏了那个","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635381486,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}