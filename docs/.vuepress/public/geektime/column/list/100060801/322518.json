{"id":322518,"title":"28 | 业界经典：YouTube深度学习推荐系统的经典架构长什么样？","content":"<p>你好，我是王喆。今天我们一起来开启前沿拓展篇的学习。</p><p>如果你是跟着课程安排学到这里的，我可以很自信地说，你几乎已经掌握了推荐系统的全部重点知识。从数据特征的处理，模型构建，到模型的评估和上线，再到推荐服务器的实现，你的知识广度已经覆盖了推荐系统领域的全部要点。但要想成为一名合格的推荐工程师，我们还需要做两件事情，一件是追踪前沿，另一件是融会贯通。</p><p>因此，在这一篇中，我会通过详细讲解几个一线大厂的推荐系统解决方案，来帮你追踪行业的热点、创新点。它们既包括一些推荐模型的业界实现，如YouTube和Pinterest的推荐模型，也包括推荐系统的工程落地方案，如Flink的经典应用和美团对于强化学习的落地方案。最后，我还会对算法工程师的所需能力做一个全面的总结。</p><p>今天，我们今天先来学习YouTube的经典深度学习推荐系统架构。YouTube这套深度学习解决方案，已经经典到可以成为一个业界标杆式的方案了，也是我在国内外和同学、同事们交流、讨论的时候经常会提到的方案。</p><p>话不多说，我们正式开始今天的学习吧！</p><h2>YouTube推荐系统架构</h2><p>提起YouTube，我想你肯定不会陌生，作为全球最大的视频分享网站，YouTube平台中几乎所有的视频都来自UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：</p><!-- [[[read_end]]] --><ul>\n<li>一是其商业模式不同于Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而YouTube的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显；</li>\n<li>二是由于YouTube的视频基数巨大，用户难以发现喜欢的内容。</li>\n</ul><p>这样的内容特点简直就是深度学习推荐系统最适合扎根的土壤，所以YouTube也是最早落地深度学习的一线公司。那YouTube的深度学习推荐系统架构长什么样呢？</p><p><img src=\"https://static001.geekbang.org/resource/image/47/05/47c0fa06ffc18912b027fe920ac30905.jpg?wh=1630*1032\" alt=\"\" title=\"图1 YouTube推荐系统整体架构\"></p><p>上图就是YouTube在2016年发布的推荐系统架构。我们可以看到，为了对海量的视频进行快速、准确的排序，YouTube也采用了经典的召回层+排序层的推荐系统架构。</p><p><strong>它的推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking Model）完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。</strong></p><p>无论是候选集生成模型还是排序模型，YouTube都采用了深度学习的解决方案。下面，就让我们详细讲讲这两个深度学习模型是如何构建起来的。</p><h2>候选集生成模型</h2><p>首先，是用于视频召回的候选集生成模型，它的模型架构如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/cf/6968873184cf93194aa476398c2e35cf.jpg?wh=1772*1538\" alt=\"\" title=\"图2 YouTube候选集生成模型架构\"></p><p>我们一起自下而上地好好看一看这个模型的结构。</p><p>最底层是它的输入层，输入的特征包括用户历史观看视频的Embedding向量，以及搜索词的Embedding向量。对于这些Embedding特征，YouTube是利用用户的观看序列和搜索序列，采用了类似Item2vec的预训练方式生成的。</p><p>当然，我们也完全可以采用Embedding跟模型在一起End2End训练的方式来训练模型。至于预训练和End2End训练这两种方式孰优孰劣，我们也探讨过很多次了，你可以自己再深入思考一下。</p><p>除了视频和搜索词Embedding向量，特征向量中还包括用户的地理位置Embedding、年龄、性别等特征。这里我们需要注意的是，对于样本年龄这个特征，YouTube不仅使用了原始特征值，还把经过平方处理的特征值也作为一个新的特征输入模型。</p><p>这个操作其实是为了挖掘特征非线性的特性，当然，这种对连续型特征的处理方式不仅限于平方，其他诸如开方、Log、指数等操作都可以用于挖掘特征的非线性特性。具体使用哪个，需要我们根据实际的效果而定。</p><p>确定好了特征，跟我们之前实践过的深度学习模型一样，这些特征会在concat层中连接起来，输入到上层的ReLU神经网络进行训练。</p><p>三层ReLU神经网络过后，YouTube又使用了softmax函数作为输出层。值得一提的是，<strong>这里的输出层不是要预测用户会不会点击这个视频，而是要预测用户会点击哪个视频</strong>，这就跟我们之前实现过的深度推荐模型不一样了。</p><p>比如说，YouTube上有100万个视频，因为输出层要预测用户会点击哪个视频，所以这里的sofmax就有100万个输出。因此，这个候选集生成模型的最终输出，就是一个在所有候选视频上的概率分布。为什么要这么做呢？它其实是为了更好、更快地进行线上服务，这一点我们等会再详细讲。</p><p>总的来讲，YouTube推荐系统的候选集生成模型，是一个标准的利用了Embedding预训练特征的深度推荐模型，它遵循我们之前实现的Embedding MLP模型的架构，只是在最后的输出层有所区别。</p><h2>候选集生成模型独特的线上服务方法</h2><p>好，现在我们就详细说一说，为什么候选集生成模型要用“视频ID”这个标签，来代替“用户会不会点击视频”这个标签作为预测目标。事实上，这跟候选集生成模型独特的线上服务方式紧密相关。</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/69/e9a20bc7260296e09078509e3f42df69.jpg?wh=1772*471\" alt=\"\" title=\"图3 模型服务部分示意图\"></p><p>细心的同学可能已经留意到，架构图左上角的模型服务（Serving）方法与模型训练方法完全不同。在候选集生成模型的线上服务过程中，YouTube并没有直接采用训练时的模型进行预测，而是采用了一种最近邻搜索的方法，我们曾经在<a href=\"https://time.geekbang.org/column/article/301739\">第12讲</a>详细讲过基于Embedding的最近邻搜索方法，不记得的同学可以先去回顾一下。</p><p>具体来说，在模型服务过程中，网络结构比较复杂，如果我们对每次推荐请求都端到端地运行一遍模型，处理一遍候选集，那模型的参数数量就会巨大，整个推断过程的开销也会非常大。</p><p><strong> 因此，在通过“候选集生成模型”得到用户和视频的Embedding后，我们再通过Embedding最近邻搜索的方法，就可以提高模型服务的效率了。这样一来，我们甚至不用把模型推断的逻辑搬上服务器，只需要将用户Embedding和视频Embedding存到特征数据库就行了。</strong>再加上可以使用局部敏感哈希这类快速Embedding查找方法，这对于百万量级规模的候选集生成过程的效率提升是巨大的。</p><p>那么问题又来了，这里的用户Embedding和视频Embedding到底是从哪里来的呢？这个问题的答案就是，候选集生成模型为什么要用视频ID作为多分类输出的答案了。我们再仔细看一下图2的架构，架构图中从softmax向模型服务模块画了个箭头，用于代表视频Embedding向量的生成。</p><p>由于最后的输出层是softmax，而这个softmax层的参数本质上就是一个m x n维的矩阵，其中m指的是最后一层红色的ReLU层的维度m，n指的是分类的总数，也就是YouTube所有视频的总数n。因此，视频Embedding就是这个m x n维矩阵的各列向量。</p><p>这样的Embedding生成方法其实和word2vec中词向量的生成方法是相同的，你也可以参考<a href=\"https://time.geekbang.org/column/article/295939\">第6讲</a>的内容来理解它。</p><p>清楚了视频Embedding的生成原理，用户Embedding的生成就非常好理解了，因为输入的特征向量全部都是用户相关的特征，一个物品和场景特征都没有，所以在使用某用户u的特征向量作为模型输入时，最后一层ReLU层的输出向量就可以当作该用户u的Embedding向量。</p><p>在模型训练完成后，逐个输入所有用户的特征向量到模型中，YouTube就可以得到所有用户的Embedding向量，之后就可以把它们预存到线上的特征数据库中了。</p><p>在预测某用户的视频候选集时，YouTube要先从特征数据库中拿到该用户的Embedding向量，再在视频Embedding向量空间中，利用局部敏感哈希等方法搜索该用户Embedding向量的K近邻，这样就可以快速得到k个候选视频集合。这就是整个候选集生成模型的训练原理和服务过程。</p><p>到这里，你一定已经体会到了咱们前沿拓展篇案例分析的作用，通过一个YouTube候选集生成模型的原理分析，我们就已经把第6讲的Embedding、<a href=\"https://time.geekbang.org/column/article/299326\">第10讲</a>的特征数据库、第12讲的局部敏感哈希，以及<a href=\"https://time.geekbang.org/column/article/309846\">第17讲</a>的Embedding MLP模型都回顾了一遍。</p><p>如果你喜欢这种通过学习业界实践方案，把知识串联起来的方式，可以给我留言反馈，我也会在之后的课程中多采用这样的方式。</p><h2>排序模型</h2><p>通过候选集生成模型，YouTube已经得到了几百个候选视频的集合了，下一步就是利用排序模型进行精排序。下图就是YouTube深度学习排序模型的架构，我们一起来看一看。</p><p><img src=\"https://static001.geekbang.org/resource/image/28/1a/28e0acbb64760670ee94d015025da81a.jpg?wh=1670*994\" alt=\"\" title=\"图3 YouTube的深度学习排序模型的架构\"></p><p>第一眼看上去，你可能会认为排序模型的网络结构与候选集生成模型没有太大区别，在模型结构上确实是这样的，它们都遵循Embedding MLP的模型架构。但是我们来看其中的细节，特别是输入层和输出层的部分，它们跟候选集生成模型还是有很大不同的，这就是我们要重点关注的。</p><p>我们先看输入层，相比于候选集生成模型需要对几百万候选集进行粗筛，排序模型只需对几百个候选视频进行排序，因此可以引入更多特征进行精排。具体来说，YouTube的输入层从左至右引入的特征依次是：</p><ol>\n<li>impression video ID embedding：当前候选视频的Embedding；</li>\n<li>watched video IDs average embedding：用户观看过的最后N个视频Embedding的平均值；</li>\n<li>language embedding：用户语言的Embedding和当前候选视频语言的Embedding；</li>\n<li>time since last watch：表示用户上次观看同频道视频距今的时间；</li>\n<li>#previous impressions：该视频已经被曝光给该用户的次数；</li>\n</ol><p>上面5个特征中，前3个Embedding特征的含义很好理解，我就不细说了。第4个特征和第5个特征，因为很好地引入了YouTube工程师对用户行为的观察，所以我来重点解释一下。</p><p>第4个特征 <strong>time since last watch</strong>说的是用户观看同类视频的间隔时间。如果从用户的角度出发，假如某用户刚看过“DOTA比赛经典回顾”这个频道的视频，那他很大概率会继续看这个频道的其他视频，该特征就可以很好地捕捉到这一用户行为。</p><p>第5个特征<strong>#previous impressions</strong> 说的是这个视频已经曝光给用户的次数。我们试想如果一个视频已经曝光给了用户10次，用户都没有点击，那我们就应该清楚，用户对这个视频很可能不感兴趣。所以<strong>#previous impressions</strong> 这个特征的引入就可以很好地捕捉到用户这样的行为习惯，避免让同一个视频对同一用户进行持续的无效曝光，尽量增加用户看到新视频的可能性。</p><p>把这5类特征连接起来之后，需要再经过三层ReLU网络进行充分的特征交叉，然后就到了输出层。这里我们要重点注意，排序模型的输出层与候选集生成模型又有所不同。不同主要有两点：<strong>一是候选集生成模型选择了softmax作为其输出层，而排序模型选择了weighted logistic regression（加权逻辑回归）作为模型输出层；二是候选集生成模型预测的是用户会点击“哪个视频”，排序模型预测的是用户“要不要点击当前视频”。</strong></p><p>那么问题来了，YouTube为什么要这么做呢？</p><p>其实，排序模型采用不同输出层的根本原因就在于，YouTube想要更精确地预测用户的观看时长，因为观看时长才是YouTube最看中的商业指标，而使用Weighted LR作为输出层，就可以实现这样的目标。</p><p>这是怎么做到的呢？在Weighted LR的训练中，我们需要为每个样本设置一个权重，权重的大小，代表了这个样本的重要程度。为了能够预估观看时长，YouTube将正样本的权重设置为用户观看这个视频的时长，然后再用Weighted LR进行训练，就可以让模型学到用户观看时长的信息。</p><p>这是因为观看时长长的样本更加重要，严格一点来说，就是观看时长长的样本被模型预测的为正样本的概率更高，这个概率与观看时长成正比，这就是使用Weighted LR来学习观看时长信息的基本原理。</p><p>最后，我们再聊一聊排序模型的模型服务方法。我刚才讲过了，候选集生成模型是可以直接利用用户Embedding和视频Embedding进行快速最近邻搜索的。那排序模型还能这样做吗？</p><p>这就不可以了，原因有两点：一是因为我们的输入向量中同时包含了用户和视频的特征，不再只是单纯的用户特征。这样一来，用户x物品特征的组合过多，就无法通过预存的方式保存所有模型结果；二是因为排序模型的输出层不再是预测视频ID，所以我们也无法拿到视频Embedding。因此对于排序模型，我们必须使用TensorFlow Serving等模型服务平台，来进行模型的线上推断。</p><p>到这里，我们就讲完了YouTube推荐模型的全部细节。如果你有任何疑惑的地方，可以在留言区提问，同时我也建议你多看几遍这节课的内容，因为这个解决方案真的是太经典了。</p><h2>小结</h2><p>好了，这节课的内容讲完了，我们再总结一下YouTube推荐系统的重点知识。</p><p>YouTube推荐系统的架构是一个典型的召回层加排序层的架构，其中候选集生成模型负责从百万候选集中召回几百个候选视频，排序模型负责几百个候选视频的精排，最终选出几十个推荐给用户。</p><p>候选集生成模型是一个典型的Embedding MLP的架构，我们要注意的是它的输出层，它是一个多分类的输出层，预测的是用户点击了“哪个”视频。在候选集生成模型的serving过程中，需要从输出层提取出视频Embedding，从最后一层ReLU层得到用户Embedding，然后利用最近邻搜索快速得到候选集。</p><p>排序模型同样是一个Embedding MLP的架构，不同的是，它的输入层包含了更多的用户和视频的特征，输出层采用了Weighted LR作为输出层，并且使用观看时长作为正样本权重，让模型能够预测出观看时长，这更接近YouTube要达成的商业目标。</p><p>好了，这些关键知识点，我也总结在了下面的表格中，希望它能帮助你加深记忆。</p><p><img src=\"https://static001.geekbang.org/resource/image/60/2e/60509d00835ac95ayya3bdeb17f0532e.jpeg?wh=1920*1080\" alt=\"\"></p><p>在这节课结束前，关于YouTube的推荐模型我还想多说几句。事实上，YouTube的推荐系统论文中还包含了更多的细节，业界真正好的论文并不多，YouTube的这篇《Deep Neural Networks for YouTube Recommendations》绝对是不可多得的一篇，我甚至推荐大家逐句来读，抓住每一个细节。</p><p>当然，你也可以在我的书《深度学习推荐系统》中的相应章节找到更多的实现细节。这些内容让我曾经受益匪浅，相信也会对你有所帮助。</p><h2>课后思考</h2><p>YouTube的排序模型和候选集生成模型，都使用了平均池化这一操作，来把用户的历史观看视频整合起来。你能想到更好的方法来改进这个操作吗？</p><p>期待在留言区看到你的思考和总结，我们下节课见！</p>","neighbors":{"left":{"article_title":"27 | 评估体系：如何解决A/B测试资源紧张的窘境？","id":321702},"right":{"article_title":"29 | 图神经网络：Pinterest是如何应用图神经网络的？","id":322487}},"comments":[{"had_liked":false,"id":268989,"user_name":"Geek_e0d66a","can_delete":false,"product_type":"c1","uid":1809935,"ip_address":"","ucode":"5078900C9CF936","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/l3RGUX8aLnPLmsQsra0yU5d8m7Se5jdVpaC3bkb99FuY11BPQNAsH4MPXbZjCTia9VVwn8lnBnKLkdfSiabOgxKg/132","comment_is_top":false,"comment_ctime":1608463374,"is_pvip":false,"replies":[{"id":"97640","content":"因为只有最后的视频embedding是跟用户embedding在一个向量空间内。预训练和embedding和最后relu层生成的user embedding没有直接关系。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608594882,"ip_address":"","comment_id":268989,"utype":1}],"discussion_count":2,"race_medal":0,"score":"108982645774","product_id":100060801,"comment_content":"老师，请问召回模型中，输入层已经有了视频的预训练的Embedding向量，最后softmax 的参数也会作为视频的embedding向量。一开始不是都有了视频的Embedding向量了吗？最后ANN的为什么只用训练视频向量，而不用预训练的呢？","like_count":26,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512107,"discussion_content":"因为只有最后的视频embedding是跟用户embedding在一个向量空间内。预训练和embedding和最后relu层生成的user embedding没有直接关系。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1608594882,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1036428,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/d0/8c/d058d6c3.jpg","nickname":"loode_","note":"","ucode":"803EEC4937818F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":359858,"discussion_content":"@王喆  老师，关于召回网络，我的理解是最后一层relu输出的是user embedding（1xm），然后与视频embedding矩阵(mxn)进行相乘，最后通过这种方式学习得到用户和视频embedding在一个向量空间内，所以可以使用LSH类似的方法进行快速召回。而对于softmax这里的mxn的视频embedding，感觉既可以用预训练的视频embedding来进行微调，也可以是随机初始化然后一起进行训练的，不知道理解是否正确？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616306285,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268627,"user_name":"fsc2016","can_delete":false,"product_type":"c1","uid":1255585,"ip_address":"","ucode":"5480F05703A974","user_header":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","comment_is_top":false,"comment_ctime":1608277810,"is_pvip":false,"replies":[{"id":"97495","content":"1、2都是非常好的回答。<br><br>提问也非常好，但我推荐你自己再好好想想他们为什么在一个向量空间，你要思考一下relu最后输出的用户emb和softmax中的物品emb是怎么运算的？在最近邻搜索中又是怎么运算的？","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608331069,"ip_address":"","comment_id":268627,"utype":1}],"discussion_count":6,"race_medal":0,"score":"74622721842","product_id":100060801,"comment_content":"思考题：<br>1，在召回层，对用户历史观看的序列，按照时间衰减因子，对用户观看emb序列进行加权求平均，加强最近观看视频的影响力<br>2，在排序层，可以加入注意力机制，类似DIN模型中，计算候选emb与用户行为序列中视频emb的权重，然后在进行加权求平均，得到用户行为序列的emb<br>提问：老师 ，之前讲emb近邻搜索，需要用户emb和物品emb在同一向量空间。那么在召回层relu中提取的用户emb和softmax提取的物品emb，是在同一向量空间的，为什么？难道是因为同一个模型训练出来，输入特征一致才允许这样操作嘛？<br>","like_count":18,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511992,"discussion_content":"1、2都是非常好的回答。\n\n提问也非常好，但我推荐你自己再好好想想他们为什么在一个向量空间，你要思考一下relu最后输出的用户emb和softmax中的物品emb是怎么运算的？在最近邻搜索中又是怎么运算的？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608331069,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1255585,"avatar":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","nickname":"fsc2016","note":"","ucode":"5480F05703A974","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":335849,"discussion_content":"relu隐藏层的输出是用户向量，正好是softmax层的输入x，根据前向计算wi*x+b算得到了物品i 节点值，这里的wi也就能代表物品向量了，也就是说由用户向量参与计算生成了最后的物品向量，跟我们前面利用电影向量 sum pooling出用户向量逻辑一致。所以他们在同一向量空间。","likes_number":19,"is_delete":false,"is_hidden":false,"ctime":1608350421,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1255585,"avatar":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","nickname":"fsc2016","note":"","ucode":"5480F05703A974","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337017,"discussion_content":"是这样，完全正确","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608771681,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":335849,"ip_address":""},"score":337017,"extra":""}]},{"author":{"id":1128515,"avatar":"https://static001.geekbang.org/account/avatar/00/11/38/43/7c0738b6.jpg","nickname":"李倩颖","note":"","ucode":"24DA92DE04AC51","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":348908,"discussion_content":" 我认为不需要纠结是否在同一个空间中，可以从数学上推导出计算softmax和直接计算用户和wi向量的内积，对物品排序是同样的结果。softmax的结果与wi*x(如果把wi看成是物品向量，那么wi*x就认为是用户和物品向量内积)的大小成正相关关系，所以只要计算出wi*x就可以知道物品的排名。","likes_number":7,"is_delete":false,"is_hidden":false,"ctime":1612773131,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2619397,"avatar":"https://static001.geekbang.org/account/avatar/00/27/f8/05/079444c2.jpg","nickname":"创之巅","note":"","ucode":"38E29FAEFB7023","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":373948,"discussion_content":"理解不了用户向量做一层wx+b怎么就得到某个视频的embedding了？有点无中生有的感觉。为什么这个视频embedding不是视频属性或idembedding得到的？老师麻烦细致讲一下视频embedding的生成吧。输出不是一维？怎么是mxn的矩阵？全文就这个视频向量生成最不好理解。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620927731,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2577469,"avatar":"https://static001.geekbang.org/account/avatar/00/27/54/3d/366462d0.jpg","nickname":"Yvonne","note":"","ucode":"85287A0AC7456C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2619397,"avatar":"https://static001.geekbang.org/account/avatar/00/27/f8/05/079444c2.jpg","nickname":"创之巅","note":"","ucode":"38E29FAEFB7023","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":374312,"discussion_content":"我的理解是：最后一层是多分类，预测哪个物品id被观看的概率最高。所以相当于先WX+b, X是用户向量，W是m*n的矩阵（m是总视频数，n是用户embedding的纬度）。然后再把结果放入softmax中正则化得每个物品的观看概率。而大矩阵W的每一行(wi)刚好对应一个物品，所以可以被当做物品embedding。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1621133270,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":373948,"ip_address":""},"score":374312,"extra":""}]}]},{"had_liked":false,"id":269501,"user_name":"SecooHR","can_delete":false,"product_type":"c1","uid":2137388,"ip_address":"","ucode":"01C559CBEE207A","user_header":"","comment_is_top":false,"comment_ctime":1608687428,"is_pvip":false,"replies":[{"id":"97757","content":"这个问题非常好，其实两种方式都可以。有一些细微的差别，但我觉得无伤大雅，选一种就行。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608714017,"ip_address":"","comment_id":269501,"utype":1}],"discussion_count":2,"race_medal":0,"score":"35968425796","product_id":100060801,"comment_content":"老师能够讲下 实际 Weighted LR  具体训练过程吗，比如 videoid1 labels=1  weights=15 , 实际中是把这个样本 重复抽样weights 次，放入训练样本吗，还是更改LR 的loss？","like_count":9,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512299,"discussion_content":"这个问题非常好，其实两种方式都可以。有一些细微的差别，但我觉得无伤大雅，选一种就行。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608714017,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2248849,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJCeic7MeX7wwK7kD8W1AeKVib6ZiagKldt3oR11aOicbXLjVuTM4XSfBzhUAvyODn2KDKXNC1pJiaOj6w/132","nickname":"Geek_938c97","note":"","ucode":"384CEC667F0337","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":352186,"discussion_content":"老师，对于线上请求，没有weights，怎么能表示出期望时长（TP)呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614643853,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":277371,"user_name":"Geek_790c43","can_delete":false,"product_type":"c1","uid":2156292,"ip_address":"","ucode":"FFFAE0636A91EF","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/4icayE3ic5IA7RWwZcrxpMZE4T1WViakEgPsDC3UnhZwU83ad65IjmxPficy0vNZz6Q6vCiclnmyBDc5IYf7soHAXrQ/132","comment_is_top":false,"comment_ctime":1612395400,"is_pvip":false,"replies":[{"id":"100814","content":"这两点都非常好，是很工业界思路的思考，推荐其他同学参考，","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1612596101,"ip_address":"","comment_id":277371,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31677166472","product_id":100060801,"comment_content":"关于id做输入再embedding vs. 预训练embedding的想法不知道对不对：<br>1. 视频id作为输入再embedding的end2end模型，受cold start影响比较大，因为每遇到新视频模型就需要重新训练。但是用pretrained的视频embedding作为输入，哪怕遇到新视频也可以仿照airbnb的做法生成一个tmp的embedding再喂给模型。<br>2. 假如有几亿候选视频，直接id做输入会导致embedding层的参数数量非常大，使用预训练embedding可以避免这一点。（用户塔的embedding可以通过平均观看过的视频的embedding得到）","like_count":8,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515025,"discussion_content":"这两点都非常好，是很工业界思路的思考，推荐其他同学参考，","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612596101,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":274333,"user_name":"骚动","can_delete":false,"product_type":"c1","uid":2280216,"ip_address":"","ucode":"EBEBF417C866D4","user_header":"https://static001.geekbang.org/account/avatar/00/22/cb/18/0139e086.jpg","comment_is_top":false,"comment_ctime":1610968630,"is_pvip":true,"replies":[{"id":"99716","content":"思考很好，非常赞。<br><br>问题有点多，建议以后还是一条一条提问，放在一起太不友好。<br><br>简单回答<br>1、这不是个问题，这是个话题，没有统一的方案，可以多看一些冷启动的解决方案的文章。<br>2、模型训练好，离线更新<br>3、实践<br>4、自己实践，不要瞎猜原因","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611192080,"ip_address":"","comment_id":274333,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31675739702","product_id":100060801,"comment_content":"思考题：平均池化，在我的理解里，是把N个历史记录平等对待，每个user可能都会有2~4个感兴趣的标签（比如会同时关注音乐和生活的两类视频），平均池化这N个emb的方式，我觉得能更好的反映user的整体趋势，一定程度上关注了EE的问题。我看回答里采用时间加权以及注意力加权的回答，我觉得就是更加关注user最近兴趣的方式，也是非常好的方案，但是就是要考量user兴趣变化的快不快的问题，我觉得视频网站的user应该兴趣变化节奏应该并不快，采用平均池化的方式反而能挖掘用户的潜在兴趣，可能更适合youtube的业务。总体来说，我认为这两种方式孰优孰劣，并不一定，需要更多的数据分析来反映整个user群体的整体状况。<br><br>另外，我有这么几个疑问：<br>1. youtube 大部分都是UGC内容的情况下，怎么进行冷启动的？<br>2. 候选集生成模型中最近邻索引会不会存在更新速度慢的问题，还是说离线更新？<br>3. 候选集生成模型中为什么是用的ReLU ？实践得出来得结果吗？有没有先验理论的支撑？<br>4. 候选集生成模型中样本年龄这种连续型特征在这个模型中不需要归一化吗？","like_count":8,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513980,"discussion_content":"思考很好，非常赞。\n\n问题有点多，建议以后还是一条一条提问，放在一起太不友好。\n\n简单回答\n1、这不是个问题，这是个话题，没有统一的方案，可以多看一些冷启动的解决方案的文章。\n2、模型训练好，离线更新\n3、实践\n4、自己实践，不要瞎猜原因","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611192080,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269314,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1608608497,"is_pvip":false,"replies":[{"id":"97718","content":"可以参考","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608665912,"ip_address":"","comment_id":269314,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23083444977","product_id":100060801,"comment_content":"“YouTube模型结构仅占了30%的价值，剩下的70%价值在于其工程实现细节。”补充王喆老师对YouTube推荐系统十大工程问题的解读文章：https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;52504407","like_count":6,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512232,"discussion_content":"可以参考","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608665912,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269291,"user_name":"Wiiki","can_delete":false,"product_type":"c1","uid":1797573,"ip_address":"","ucode":"037F2D44C087C5","user_header":"https://static001.geekbang.org/account/avatar/00/1b/6d/c5/c0665034.jpg","comment_is_top":false,"comment_ctime":1608602587,"is_pvip":false,"replies":[{"id":"97715","content":"4000多篇肯定有必要做推荐，就连极客时间只有这几百部课程，我都建议他们做推荐，否则用户很难找到心意的课程。<br><br>至于怎么做推荐，我觉得这不是个问题，可以参考这么课中的任意一种方法。相似推荐和猜你喜欢是两个最经典的功能，同样适用与文章推荐。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608665782,"ip_address":"","comment_id":269291,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23083439067","product_id":100060801,"comment_content":"王老师，您好。作为读者，我们热切希望您能够出一个专栏针对大厂推荐系统的工程实践和理论方式面进行详细解析的课程，谢谢~ 还有一个问题想请教您：我司最近也上线了一个文章推荐系统，现实情况是文章大概有4千多篇存量，每天大概会有1到2篇的增量（文章比较少~），但是用户日志比较多，百万级别，每天的活跃用户也有几十万，针对我们的目前情况，您觉得我们是否有做文章推荐的需要？或者说在基于我们现实情况下，您觉得现阶段我们怎么做文章推荐~ 谢谢 ：）","like_count":5,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512220,"discussion_content":"4000多篇肯定有必要做推荐，就连极客时间只有这几百部课程，我都建议他们做推荐，否则用户很难找到心意的课程。\n\n至于怎么做推荐，我觉得这不是个问题，可以参考这么课中的任意一种方法。相似推荐和猜你喜欢是两个最经典的功能，同样适用与文章推荐。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608665782,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":281696,"user_name":"shenhuaze","can_delete":false,"product_type":"c1","uid":1960811,"ip_address":"","ucode":"32927236CBEA0B","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/qw7rRHUPRzhibxXWLG7kc3zkhZwBn4JZaryzko2eWOjSxDlRvUathHugrIVKhcCqxhtsANUTq0140AlbDkLZmcw/132","comment_is_top":false,"comment_ctime":1614849842,"is_pvip":false,"replies":[{"id":"102388","content":"你问了一个我非常不想回答的问题，哈哈。这个example age是一个非常不好理解的feature。如果不想理解我觉得影响也不大，如果你想进一步深究，可以参照我的这篇文章下面的评论。https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;52504407","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615024582,"ip_address":"","comment_id":281696,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14499751730","product_id":100060801,"comment_content":"老师，召回模型里的样本年龄是指的什么意思？这里的样本是指的一条带特征和label的训练样本，还是指的一个视频？","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516471,"discussion_content":"你问了一个我非常不想回答的问题，哈哈。这个example age是一个非常不好理解的feature。如果不想理解我觉得影响也不大，如果你想进一步深究，可以参照我的这篇文章下面的评论。https://zhuanlan.zhihu.com/p/52504407","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615024582,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269480,"user_name":"haydenlo","can_delete":false,"product_type":"c1","uid":1594293,"ip_address":"","ucode":"D9EEF8226C3D81","user_header":"","comment_is_top":false,"comment_ctime":1608683967,"is_pvip":false,"replies":[{"id":"97755","content":"这是个好问题，在Youtube如此大规模数据集上训练，确实有可能训练时间的问题，但他们的实现细节，我们确实无法得知。<br><br>但第二个问题我觉得是不存在的，只要保证每个视频有正样本，就肯定会有对应的embedding生成。 negative sampling只是对负样本采样。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608713838,"ip_address":"","comment_id":269480,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14493585855","product_id":100060801,"comment_content":"老师好，候选集生成模型中，采用softmax获得视频embedding的方法，面对youtube几千万甚至上亿的视频量，是不是要训练很久？如果换成nce加速的方式，由于是采样的，会不会有些视频miss掉","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512289,"discussion_content":"这是个好问题，在Youtube如此大规模数据集上训练，确实有可能训练时间的问题，但他们的实现细节，我们确实无法得知。\n\n但第二个问题我觉得是不存在的，只要保证每个视频有正样本，就肯定会有对应的embedding生成。 negative sampling只是对负样本采样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608713838,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":286941,"user_name":"inkachenko","can_delete":false,"product_type":"c1","uid":2441833,"ip_address":"","ucode":"0AC06DE6387388","user_header":"https://static001.geekbang.org/account/avatar/00/25/42/69/8c56cea0.jpg","comment_is_top":false,"comment_ctime":1617694374,"is_pvip":false,"replies":[{"id":"104471","content":"1. 可以做负采样，不是必须要做<br>2. 一个用户的一次行为是一个样本，而不是一个用户就一个样本。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1618113181,"ip_address":"","comment_id":286941,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10207628966","product_id":100060801,"comment_content":"老师您好，我有两个问题想问一下<br>1.召回的时候是否要采用negative sampling进行训练呢？视频数量有百万级别，负采样的时候我是根据视频pv的0.75次方为weight进行采样的<br>2.召回的时候一个用户只生成一个训练集吗？感觉训练集有点不够用，我打算根据用户的前i次兴趣预测第i+1次，然后遍历i，这样一个用户就可以有好多个训练集。。最后预测的时候就使用最新的数据生成embedding，不知这样是否可行","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518137,"discussion_content":"1. 可以做负采样，不是必须要做\n2. 一个用户的一次行为是一个样本，而不是一个用户就一个样本。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618113181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":275428,"user_name":"AstrHan","can_delete":false,"product_type":"c1","uid":1944884,"ip_address":"","ucode":"14C5F3323A472D","user_header":"","comment_is_top":false,"comment_ctime":1611537920,"is_pvip":false,"replies":[{"id":"99972","content":"如果是基于embedding的召回层，也最少需要天级别更新，因为大量新的item需要加入进来。<br><br>至于如何提高更新速度，这是一个非常大的话题，需要多个工程和机器学习模块的优化。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611551639,"ip_address":"","comment_id":275428,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10201472512","product_id":100060801,"comment_content":"老师 看了之后有个问题，您之前说的模型基本以天作为更新，这个说的是排序层吧？那召回层的呢？召回层模型更新一次，要把候选集几百万的数据重跑一次 开销好大。这种问题如何解决呢？我现在是采用word2vec的与训练模型，这样模型就基本不用更新了。不知道业界这一块怎么做的","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514338,"discussion_content":"如果是基于embedding的召回层，也最少需要天级别更新，因为大量新的item需要加入进来。\n\n至于如何提高更新速度，这是一个非常大的话题，需要多个工程和机器学习模块的优化。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611551639,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1267685,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJk3PElN2J96PPD6IZfkenCeWqv2WNdjUNANkoAJngMx6YPy9QNIZDvW7MAQIn2PXd20EIumicy8jQ/132","nickname":"疾风","note":"","ucode":"C86D74C62AFA90","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366250,"discussion_content":"为什么使用word2vec的预训练模型，模型就不用更新了呢？w2v不支持增量训，也得从头训练啊","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618014495,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":307919,"user_name":"Van","can_delete":false,"product_type":"c1","uid":2726010,"ip_address":"","ucode":"5D88FC2DD9B868","user_header":"","comment_is_top":false,"comment_ctime":1629307775,"is_pvip":false,"replies":[{"id":"111630","content":"是的，只要user emb不是由item emb的简单计算得来的，就不在一个空间内。<br><br>如果user item是通过一些复杂点的网络做交叉的，那么肯定不能直接做ANN召回","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1629501972,"ip_address":"","comment_id":307919,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5924275071","product_id":100060801,"comment_content":"老师您好 还是对于 user embedding 和 item embedding什么时候可以直接用ANN来取近似有些Confused. 想总结一下老师您讲的看看对不对：两者可以做内积的前提是他们必须要在一个向量空间。如何定义是同一向量空间就会是一个问题。我理解的两种方式然他们在一个向量空间：<br>1. 只要是利用用户历史的item embedding生成的用户embedding，可以说都是在一个向量空间内，这些生成方式包括但不限于average pooling，sum pooling，attention等等<br>2. 类似于MF的 矩阵分解。双塔模型运用的就是这个道理  <br><br>我的问题是 基于此是不是说明只要有user 和 item feature有交叉的情况就不可以提取出他们做ANN了？我看到DeepFM有时也被拿来做召回 所以是因为他们只加入了user 和item feature 各自下的interaction嘛？ ","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525338,"discussion_content":"是的，只要user emb不是由item emb的简单计算得来的，就不在一个空间内。\n\n如果user item是通过一些复杂点的网络做交叉的，那么肯定不能直接做ANN召回","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629501972,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":304935,"user_name":"xuexiqiu","can_delete":false,"product_type":"c1","uid":1612924,"ip_address":"","ucode":"D00BD255E0E0D3","user_header":"https://static001.geekbang.org/account/avatar/00/18/9c/7c/fdb85fde.jpg","comment_is_top":false,"comment_ctime":1627707342,"is_pvip":false,"replies":[{"id":"110365","content":"中文互联网知乎就是最活跃的学术前沿。另外偏正式，学术一点的就是追踪arXiv的最新update。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1627836420,"ip_address":"","comment_id":304935,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5922674638","product_id":100060801,"comment_content":"非常赞同“要想成为一名合格的推荐工程师，要追踪前沿”这个观点。我在google上搜相关问题的时候会看到twitter或者pinterest他们有时会有一些medium post，很有启发意义。请问前辈还有没有比较好的追踪业界前沿的渠道？感谢。","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524217,"discussion_content":"中文互联网知乎就是最活跃的学术前沿。另外偏正式，学术一点的就是追踪arXiv的最新update。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627836420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":275324,"user_name":"灯灯灯","can_delete":false,"product_type":"c1","uid":2402565,"ip_address":"","ucode":"A801439F74127C","user_header":"https://static001.geekbang.org/account/avatar/00/24/a9/05/6822b8a5.jpg","comment_is_top":false,"comment_ctime":1611472220,"is_pvip":false,"replies":[{"id":"99964","content":"1、优化目标还是softmax的经典loss cross entropy。<br>2、是这样。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611551120,"ip_address":"","comment_id":275324,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5906439516","product_id":100060801,"comment_content":"老师请问，在召回层的模型训练中，被优化的目标函数是什么？对每个用户，模型得到的是Nx1的概率向量，真实的概率向量是在用户观看的k个视频处坐标为1&#47;k其他坐标为0吗？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514311,"discussion_content":"1、优化目标还是softmax的经典loss cross entropy。\n2、是这样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611551120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2410680,"avatar":"https://static001.geekbang.org/account/avatar/00/24/c8/b8/27e45560.jpg","nickname":"芝乎者也","note":"","ucode":"7ACD6053DF4E55","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":345933,"discussion_content":"那岂不是你的特征里面就包含了label的信息了？特征里面不是有用户历史观看视频的embedding吗？label还是用户历史观看视频？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611817495,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":274118,"user_name":"Geek_03b08e","can_delete":false,"product_type":"c1","uid":2406911,"ip_address":"","ucode":"E00828B32B7DA4","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/6MVSMTIeZO1ZTxDIa4bNj3mvpOEic3mZ9b8ibrWIdmOKzH2ysBIznNJyr8dh77HpstyXKiaPwQ5zdfxQnxc6Cmdqg/132","comment_is_top":false,"comment_ctime":1610862344,"is_pvip":false,"replies":[{"id":"99571","content":"我感觉确实意义不是特别大，事实上传统的GBDT等树模型已经证明在一些低维问题上表现不差。当然，一些神经网络模型也不会差，只不过提升没有那么明显罢了。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1610948314,"ip_address":"","comment_id":274118,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5905829640","product_id":100060801,"comment_content":"老师您好，我是在银行做数据挖掘的，银行产品种类没有电商那么多，大类也就十几种，细分到小类的话也不会超过100种，那用深度学习模型来推荐的的话会不会有点大材小用了？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513912,"discussion_content":"我感觉确实意义不是特别大，事实上传统的GBDT等树模型已经证明在一些低维问题上表现不差。当然，一些神经网络模型也不会差，只不过提升没有那么明显罢了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610948314,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":355445,"user_name":"Geek_075add","can_delete":false,"product_type":"c1","uid":3164060,"ip_address":"日本","ucode":"3DBA61A5528651","user_header":"","comment_is_top":false,"comment_ctime":1661395537,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1661395537","product_id":100060801,"comment_content":"老师我想问以下几个问题<br>1）召回层中召回的候选集，在排序层里的是作为输入特征来用的吗？（想LR或者FM里这样的非深度模型也可以这么用吗？）<br>2）排序层的输出一般是候选集被点击或观看的概率吗（不考虑观看时间）。","like_count":0},{"had_liked":false,"id":354900,"user_name":"LT","can_delete":false,"product_type":"c1","uid":2189183,"ip_address":"上海","ucode":"1A7AF1E780AFAA","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/7f/7aa8f1f7.jpg","comment_is_top":false,"comment_ctime":1660874517,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1660874517","product_id":100060801,"comment_content":"召回层的方式，就是为了生成更范化的用户emb和物品emb，但完全也可以用前面提到的NeuralCF（或双塔系列模型）实现呀，youtube这样做的优势在哪呢？","like_count":0},{"had_liked":false,"id":336427,"user_name":"南海长风九万里","can_delete":false,"product_type":"c1","uid":1901270,"ip_address":"","ucode":"400B3641E2E7E7","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/30yrQPYKndceNHWd3g3Njk5wqHdJnFNh28MficN6ROZtlWG911o2LE4sE7CMTibnzo54EMJRd4Z0OGByjzxVMKxA/132","comment_is_top":false,"comment_ctime":1646141705,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1646141705","product_id":100060801,"comment_content":"&quot;清楚了视频 Embedding 的生成原理，用户 Embedding 的生成就非常好理解了，因为输入的特征向量全部都是用户相关的特征，一个物品和场景特征都没有，所以在使用某用户 u 的特征向量作为模型输入时，最后一层 ReLU 层的输出向量就可以当作该用户 u 的 Embedding 向量。&quot;<br>关于用户向量的生成方式，我有点不理解，“样本年龄”这个特征不就是物品特征吗？能帮忙解答一下吗？","like_count":0},{"had_liked":false,"id":327292,"user_name":"Geek_04f3c1","can_delete":false,"product_type":"c1","uid":1335314,"ip_address":"","ucode":"0DB5CE69766BB7","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIcIiaFaIstAv2LE4ibmWBnbe4BDIbia0Gk0S50rHWsvNpaFibkCZBKxSibOLwQERaxxwQbe7NOVicDEIfQ/132","comment_is_top":false,"comment_ctime":1640050912,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1640050912","product_id":100060801,"comment_content":"感谢老师分享，请问老师后续会有多目标训练、多目标融合相关内容讲解吗？","like_count":0},{"had_liked":false,"id":315998,"user_name":"WJing","can_delete":false,"product_type":"c1","uid":2735592,"ip_address":"","ucode":"391A148418523B","user_header":"","comment_is_top":false,"comment_ctime":1634101806,"is_pvip":false,"replies":[{"id":"114524","content":"1. 没什么不科学的，就是这样做的<br>2. 模型中无法做到随时加入，需要用其他冷启动方法","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1634147825,"ip_address":"","comment_id":315998,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1634101806","product_id":100060801,"comment_content":"老师，学完本节课受益匪浅，但是同时我也产生了几个疑问：<br>1. YouTube每天增加的视频量是巨大的，在第一层模型的SoftMax是视频数量，这总感觉不科学。。。<br>2. YouTube是如何做到最新更新的视频也被加入到训练中的呢？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528231,"discussion_content":"1. 没什么不科学的，就是这样做的\n2. 模型中无法做到随时加入，需要用其他冷启动方法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634147825,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":300749,"user_name":"W","can_delete":false,"product_type":"c1","uid":2674983,"ip_address":"","ucode":"C182E9F3BAD7B9","user_header":"https://static001.geekbang.org/account/avatar/00/28/d1/27/68543b66.jpg","comment_is_top":false,"comment_ctime":1625371144,"is_pvip":false,"replies":[{"id":"109390","content":"是的，从输出层的权重矩阵中输出","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1626215726,"ip_address":"","comment_id":300749,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1625371144","product_id":100060801,"comment_content":"那么softmax对应的embedding真值是怎么获得的呢？通过训练word2vec，得到权重矩阵获得的吗？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522807,"discussion_content":"是的，从输出层的权重矩阵中输出","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626215726,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293127,"user_name":"Yvonne","can_delete":false,"product_type":"c1","uid":2577469,"ip_address":"","ucode":"85287A0AC7456C","user_header":"https://static001.geekbang.org/account/avatar/00/27/54/3d/366462d0.jpg","comment_is_top":false,"comment_ctime":1621232579,"is_pvip":false,"replies":[{"id":"106646","content":"1. 不是同一个模型，你的理解是正确的<br><br>2.可以改成双塔，但双塔的问题在于user 和item特征不能提前交差，所以一般表达能力会差一些。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1621703660,"ip_address":"","comment_id":293127,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1621232579","product_id":100060801,"comment_content":"老师，初学者对这一章有两个疑问，先提前感谢老师解答^^：<br><br>1.第十九讲里的YT双塔召回模型和这里的召回模型不是同一个模型，对吗？<br>第十九讲里面是两个塔分别生成user embeding 和video embeddings。而这里是最后一个relu层输出相当于user embdding(X)，通过预测每一个候选视频的下一个被播放概率(softmax(WX+b))，可得W，每一个wi相当于video embedding？<br>但这两个模型又都是预测下一个被播放概率最高的视频？<br><br>2.关于这里的排序层不可以直接生成用户embedding&#47;视频embedding。这里的模型可以改成双塔模型吗？就是把这个曝光视频id单独进行embedding？如果可以改成双塔模型，是不是也可以得到用户&#47;视频embedding了？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520086,"discussion_content":"1. 不是同一个模型，你的理解是正确的\n\n2.可以改成双塔，但双塔的问题在于user 和item特征不能提前交差，所以一般表达能力会差一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621703660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":292699,"user_name":"创之巅","can_delete":false,"product_type":"c1","uid":2619397,"ip_address":"","ucode":"38E29FAEFB7023","user_header":"https://static001.geekbang.org/account/avatar/00/27/f8/05/079444c2.jpg","comment_is_top":false,"comment_ctime":1620927794,"is_pvip":false,"replies":[{"id":"106643","content":"不是非常清楚你说的是哪块。如果说的是视频的输入embedding向量的话，这部分其实是预训练的，不是在模型中生成的。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1621703538,"ip_address":"","comment_id":292699,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1620927794","product_id":100060801,"comment_content":"<br>理解不了用户向量做一层wx+b怎么就得到某个视频的embedding了？有点无中生有的感觉。为什么这个视频embedding不是视频属性或idembedding得到的？老师麻烦细致讲一下视频embedding的生成吧。输出不是一维？怎么是mxn的矩阵？全文就这个视频向量生成最不好理解。","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519922,"discussion_content":"不是非常清楚你说的是哪块。如果说的是视频的输入embedding向量的话，这部分其实是预训练的，不是在模型中生成的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621703538,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284930,"user_name":"Nee","can_delete":false,"product_type":"c1","uid":2254876,"ip_address":"","ucode":"1F1A14FD4252ED","user_header":"https://static001.geekbang.org/account/avatar/00/22/68/1c/d8db6177.jpg","comment_is_top":false,"comment_ctime":1616549377,"is_pvip":false,"replies":[{"id":"103434","content":"当然也可以保存最终的推荐结果。在一些场景下这甚至是最优最简单的工程方案。但你也可以从灵活性、扩展性、工程开销方面去想想有什么劣势。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1616637984,"ip_address":"","comment_id":284930,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1616549377","product_id":100060801,"comment_content":"如果取出训练好的用户Embedding和视频Embedding用来线上推荐，那么利用相似度搜索某个用户的相似视频Embedding，同一用户得到的相似视频向量岂不是每次都相同吗，那为什么不直接保存每个用户的相似视频id呢？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517515,"discussion_content":"当然也可以保存最终的推荐结果。在一些场景下这甚至是最优最简单的工程方案。但你也可以从灵活性、扩展性、工程开销方面去想想有什么劣势。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616637984,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":282365,"user_name":"Cwift","can_delete":false,"product_type":"c1","uid":1095409,"ip_address":"","ucode":"EEA2BC38D6D2A8","user_header":"https://static001.geekbang.org/account/avatar/00/10/b6/f1/a14fbf9d.jpg","comment_is_top":false,"comment_ctime":1615206895,"is_pvip":false,"replies":[{"id":"102594","content":"你要看后续你需不需要处理不同embedding层之间的叠加，或者elementwise操作，如果不需要，那么长度可以有区别。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1615336941,"ip_address":"","comment_id":282365,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1615206895","product_id":100060801,"comment_content":"不同 Embedding 层的长度可以有区别吗，如果可以有区别，是不是较长的 Embedding 层会比较短的层对结果有更大的影响？ ","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516718,"discussion_content":"你要看后续你需不需要处理不同embedding层之间的叠加，或者elementwise操作，如果不需要，那么长度可以有区别。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615336941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1267685,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJk3PElN2J96PPD6IZfkenCeWqv2WNdjUNANkoAJngMx6YPy9QNIZDvW7MAQIn2PXd20EIumicy8jQ/132","nickname":"疾风","note":"","ucode":"C86D74C62AFA90","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366251,"discussion_content":"tf lookuptable不支持变长emb size吧，因为涉及到计算加速问题，变长emb size不方便并行计算","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618014752,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":278211,"user_name":"大包子","can_delete":false,"product_type":"c1","uid":2370914,"ip_address":"","ucode":"87D6EF33040113","user_header":"https://static001.geekbang.org/account/avatar/00/24/2d/62/916961ab.jpg","comment_is_top":false,"comment_ctime":1612824767,"is_pvip":false,"replies":[{"id":"101029","content":"毫无疑问这是一个很好的feature，可以尝试。<br>其实不用过于在意这里的ranking模型到底用了那些feature，借鉴经验即可。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1612850209,"ip_address":"","comment_id":278211,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1612824767","product_id":100060801,"comment_content":"老师想问一下，为什么在排序的模型里面没有把用户的Embedding 或者相似度作为一个input","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515314,"discussion_content":"毫无疑问这是一个很好的feature，可以尝试。\n其实不用过于在意这里的ranking模型到底用了那些feature，借鉴经验即可。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612850209,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269687,"user_name":"Geek_91c50b","can_delete":false,"product_type":"c1","uid":2281331,"ip_address":"","ucode":"AA6F9A01B84406","user_header":"","comment_is_top":false,"comment_ctime":1608737395,"is_pvip":false,"replies":[{"id":"97776","content":"1. 输入层中的输入全都是用户的特征，所以这当然是一个个性化的模型。<br>2. 可以这么理解视频embedding的生成，但是召回层要同时生成user embedding，这二者的结合才是重点。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608750892,"ip_address":"","comment_id":269687,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608737395","product_id":100060801,"comment_content":"1.YouTube模型的召回层是如何实现与每个人相关的呢?<br>2.召回层可以理解成只是为了更复杂的生成全量视频的Embedding吗，不是简单的直接对视频id进行Embedding","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512355,"discussion_content":"1. 输入层中的输入全都是用户的特征，所以这当然是一个个性化的模型。\n2. 可以这么理解视频embedding的生成，但是召回层要同时生成user embedding，这二者的结合才是重点。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608750892,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269683,"user_name":"Geek_91c50b","can_delete":false,"product_type":"c1","uid":2281331,"ip_address":"","ucode":"AA6F9A01B84406","user_header":"","comment_is_top":false,"comment_ctime":1608736980,"is_pvip":false,"replies":[{"id":"97775","content":"输入的视频embedding是预训练的embedding，跟召回模型相比，输出层不再是预测视频 ID，所以无法拿到输出层视频 Embedding，也就没办法用ANN的方法进行快速搜索。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608750772,"ip_address":"","comment_id":269683,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608736980","product_id":100060801,"comment_content":"&quot;因为排序模型的输出层不再是预测视频 ID，所以我们也无法拿到视频 Embedding&quot;,这句话没看懂，在图三中用到的第一个特征不就是视频的Embedding吗","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512354,"discussion_content":"输入的视频embedding是预训练的embedding，跟召回模型相比，输出层不再是预测视频 ID，所以无法拿到输出层视频 Embedding，也就没办法用ANN的方法进行快速搜索。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608750772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":269473,"user_name":"浩浩","can_delete":false,"product_type":"c1","uid":1905716,"ip_address":"","ucode":"247753E381B19A","user_header":"https://static001.geekbang.org/account/avatar/00/1d/14/34/9a96e8d2.jpg","comment_is_top":false,"comment_ctime":1608682182,"is_pvip":false,"replies":[{"id":"97754","content":"可以加入一些time decay的权重之类的，没问题","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608713613,"ip_address":"","comment_id":269473,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608682182","product_id":100060801,"comment_content":"其实我和一位网友的想法比较相近，既然排序可以采用最近，那找回同样可以采用最近观看视频，或者调整相应的权重来做，也有点兴趣变迁的意味","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":512286,"discussion_content":"可以加入一些time decay的权重之类的，没问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608713613,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268575,"user_name":"会飞的鱼","can_delete":false,"product_type":"c1","uid":2338530,"ip_address":"","ucode":"F224C0331B0985","user_header":"https://static001.geekbang.org/account/avatar/00/23/ae/e2/eb1ee1be.jpg","comment_is_top":false,"comment_ctime":1608262042,"is_pvip":false,"replies":[{"id":"97494","content":"做法上当然是可以的，我想youtube仅仅是把所有的模型结构都画在这里，至于怎么实现，说实话那是每个人自己的事情。<br><br>个人而言，候选集生成模型的输入用户embedding完全可以像你说的一样预训练生成，没有一点问题","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608330936,"ip_address":"","comment_id":268575,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1608262042","product_id":100060801,"comment_content":"为什么不以一开始item2vec训练视频embedding 平均为用户的embedding呢？是因为维度对不上还是因为不在同一个向量空间?而且这时候输入的视频embedding可以finetune的","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511979,"discussion_content":"做法上当然是可以的，我想youtube仅仅是把所有的模型结构都画在这里，至于怎么实现，说实话那是每个人自己的事情。\n\n个人而言，候选集生成模型的输入用户embedding完全可以像你说的一样预训练生成，没有一点问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608330936,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1255585,"avatar":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","nickname":"fsc2016","note":"","ucode":"5480F05703A974","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":335850,"discussion_content":"这样生成的用户emb没有添加额外的信息，导致生成的emb不够泛化，加入更多用户特征生成的emb更加能代表这个用户吧","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1608350875,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":268532,"user_name":"你笑起来真好看","can_delete":false,"product_type":"c1","uid":1565857,"ip_address":"","ucode":"0A51EC1CA4BC1C","user_header":"https://static001.geekbang.org/account/avatar/00/17/e4/a1/2f5b9764.jpg","comment_is_top":false,"comment_ctime":1608251042,"is_pvip":false,"replies":[{"id":"97493","content":"当然可以。任何预训练生成embedding的方法理论上都可以","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608330759,"ip_address":"","comment_id":268532,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1608251042","product_id":100060801,"comment_content":"YouTube的召回模型中，视频可以用node2vec学习embedding，然后把softmax换成负采样这样做可以吗？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511959,"discussion_content":"当然可以。任何预训练生成embedding的方法理论上都可以","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608330759,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1565857,"avatar":"https://static001.geekbang.org/account/avatar/00/17/e4/a1/2f5b9764.jpg","nickname":"你笑起来真好看","note":"","ucode":"0A51EC1CA4BC1C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":335805,"discussion_content":"那类似抖音短视频这样的场景，最开始训练排序模型用分类做还是根据回归做呢？因为用户每个视频都点击，这个产品的目标可能是停留时长吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608337337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1565857,"avatar":"https://static001.geekbang.org/account/avatar/00/17/e4/a1/2f5b9764.jpg","nickname":"你笑起来真好看","note":"","ucode":"0A51EC1CA4BC1C","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":337020,"discussion_content":"观看时长肯定是非常重要的一个指标或者feature。你可以想一想有多少种办法把这个signal引入到推荐系统中来。\n\n至于抖音到底用了哪个方法，那根本不重要。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608772022,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":335805,"ip_address":""},"score":337020,"extra":""}]}]},{"had_liked":false,"id":268510,"user_name":"小匚","can_delete":false,"product_type":"c1","uid":1611663,"ip_address":"","ucode":"78E800B23C457A","user_header":"https://static001.geekbang.org/account/avatar/00/18/97/8f/ccce7df1.jpg","comment_is_top":false,"comment_ctime":1608223899,"is_pvip":false,"replies":[{"id":"97492","content":"是的，理论上任何池化操作都可以尝试。此外还有attention，序列模型等等","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608330704,"ip_address":"","comment_id":268510,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1608223899","product_id":100060801,"comment_content":"由于平均池化会丢掉原始特征 所以最大池化可能效果更好。池化的目的也是降维，那么是否可以考虑dense等其他层？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511955,"discussion_content":"是的，理论上任何池化操作都可以尝试。此外还有attention，序列模型等等","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608330704,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}