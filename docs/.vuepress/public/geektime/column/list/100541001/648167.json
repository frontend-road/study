{"id":648167,"title":"16｜Langchain里的“记忆力”，让AI只记住有用的事儿","content":"<p>你好，我是徐文浩。</p><p>在过去的两讲里，我们深入了解了Langchain的第一个核心功能，也就是LLMChain。 LLMChain能够帮助我们链式地调用一系列命令，这里面既包含直接调用OpenAI的API，也包括调用其他外部接口，或者自己实现的Python代码。但是这一连串的调用，还只是完成一个小任务。我们很多时候还是希望用一个互动聊天的过程，来完成整个任务。</p><p>所以LangChain并不是只有链式调用这样一个核心功能，它还封装了很多其他能力，来方便我们开发AI应用。比如，让AI能够拥有“记忆力”，也就是记住我们聊天上下文的能力。不知道你还记不记得，我们在<a href=\"https://time.geekbang.org/column/article/643915\">第 6 讲</a>里做的聊天机器人。在那个里面，为了能够让ChatGPT知道整个聊天的上下文，我们需要把历史的对话记录都传给它。但是，因为能够接收的Token数量有上限，所以我们只能设定一个参数，只保留最后几轮对话。我们最后把这个功能，抽象成了一个Conversation类。</p><pre><code class=\"language-python\">import openai\nimport os\n\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n\nclass Conversation:\n    def __init__(self, prompt, num_of_round):\n        self.prompt = prompt\n        self.num_of_round = num_of_round\n        self.messages = []\n        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n\n    def ask(self, question):\n        try:\n            self.messages.append({\"role\": \"user\", \"content\": question})\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=self.messages,\n                temperature=0.5,\n                max_tokens=2048,\n                top_p=1,\n            )\n        except Exception as e:\n            print(e)\n            return e\n\n        message = response[\"choices\"][0][\"message\"][\"content\"]\n        self.messages.append({\"role\": \"assistant\", \"content\": message})\n\n        if len(self.messages) &gt; self.num_of_round*2 + 1:\n            del self.messages[1:3] //Remove the first round conversation left.\n        return message\n\n</code></pre><!-- [[[read_end]]] --><p>不知道你是否还记得这个Conversation类。</p><h2>BufferWindow，滑动窗口记忆</h2><p>这个基于一个固定长度的滑动窗口的“记忆”功能，被直接内置在LangChain里面了。在Langchain里，把对于整个对话过程的上下文叫做Memory。任何一个LLMChain，我们都可以给它加上一个Memory，来让它记住最近的对话上下文。我也把对应的代码放在了下面。</p><pre><code class=\"language-python\">from langchain.memory import ConversationBufferWindowMemory\n\ntemplate = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文\n2. 回答限制在100个字以内\n\n{chat_history}\nHuman: {human_input}\nChatbot:\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"chat_history\", \"human_input\"], \n    template=template\n)\nmemory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=3)\nllm_chain = LLMChain(\n    llm=OpenAI(), \n    prompt=prompt, \n    memory=memory,\n    verbose=True\n)\nllm_chain.predict(human_input=\"你是谁？\")\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">' 我是一个中国厨师，我可以帮助你做菜。我会根据你的口味和特殊要求，精心烹饪出独特美味的中国菜肴。'\n</code></pre><p>可以看到，我们做的事情其实和之前的Conversation类似，我们定义了一个PromptTemplate来输入我们的指示。然后，在LLMChain构造的时候，我们为它指定了一个叫做 ConversationBufferWindowMemory的memory对象，并且为这个memory对象定义了k=3，也就是只保留最近三轮的对话内容。</p><p>如果我们和<a href=\"https://time.geekbang.org/column/article/643915\">第 6 讲</a>一样，和它连续进行几轮对话，你会发现，到第四轮的时候它还是能够记得我们问它的第一个问题是“你是谁”，但是第5轮的时候，已经变成“鱼香肉丝怎么做？”了。这就是因为我们选择只保留过去3轮对话。</p><pre><code class=\"language-python\">llm_chain.predict(human_input=\"鱼香肉丝怎么做？\")\nllm_chain.predict(human_input=\"那宫保鸡丁呢？\")\nllm_chain.predict(human_input=\"我问你的第一句话是什么？\")\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">' 你是谁？'\n</code></pre><p>再次询问第一句话是什么：</p><pre><code class=\"language-python\">llm_chain.predict(human_input=\"我问你的第一句话是什么？\")\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">' 你问我的第一句话是“鱼香肉丝怎么做？”'\n</code></pre><p>事实上，你可以直接调用memory的load_memory_variables方法，它会直接返回memory里实际记住的对话内容。</p><pre><code class=\"language-python\">memory.load_memory_variables({})\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">{'chat_history': 'Human: 那宫保鸡丁呢？\\nAI:  宫保鸡丁是一道经典的中国家常菜，需要准备鸡肉、花生米、干辣椒、葱、姜、蒜、料酒、盐、糖、胡椒粉、鸡精和醋。将鸡肉切成小块，放入盐水中浸泡，把其他食材切成小块，将花生米放入油锅中炸，再加入鸡肉和其他食材，炒至入味即可。\\nHuman: 我问你的第一句话是什么？\\nAI:  你是谁？\\nHuman: 我问你的第一句话是什么？\\nAI:  你问我的第一句话是“鱼香肉丝怎么做？”'}\n</code></pre><h2>SummaryMemory，把小结作为历史记忆</h2><p>使用BufferWindow这样的滑动窗口有一个坏处，就是几轮对话之后，AI就把一开始聊的内容给忘了。所以在<a href=\"https://time.geekbang.org/column/article/644544\">第 7 讲</a>的时候我们讲过，遇到这种情况，可以让AI去总结一下前面几轮对话的内容。这样，我们就不怕对话轮数太多或者太长了。</p><p>同样的，Langchain也提供了一个ConversationSummaryMemory，可以实现这样的功能，我们还是通过一段简单的代码来看看它是怎么用的。</p><p>代码中只有两个需要注意的点。</p><p>第一个是对于我们定义的 ConversationSummaryMemory，它的构造函数也接受一个LLM对象。这个对象会专门用来生成历史对话的小结，是可以和对话本身使用的LLM对象不同的。</p><p>第二个是这次我们没有使用LLMChain这个对象，而是用了封装好的ConversationChain。用ConversationChain的话，其实我们是可以不用自己定义PromptTemplate来维护历史聊天记录的，但是为了使用中文的PromptTemplate，我们在这里还是自定义了对应的Prompt。</p><pre><code class=\"language-python\">from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationSummaryMemory\nllm = OpenAI(temperature=0)\nmemory = ConversationSummaryMemory(llm=OpenAI())\n\nprompt_template = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文\n2. 回答限制在100个字以内\n\n{history}\nHuman: {input}\nAI:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"input\"], template=prompt_template\n)\nconversation_with_summary = ConversationChain(\n    llm=llm, \n    memory=memory,\n    prompt=prompt,\n    verbose=True\n)\nconversation_with_summary.predict(input=\"你好\")\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文\n2. 回答限制在100个字以内\n\nHuman: 你好\nAI:\n&gt; Finished chain.\n' 你好，我可以帮你做菜。我会根据你的口味和喜好，结合当地的食材，制作出美味可口的菜肴。我会尽力做出最好的菜肴，让你满意。'\n</code></pre><p>在我们打开了ConversationChain的Verbose模式，然后再次询问AI第二个问题的时候，你可以看到，在Verbose的信息里面，没有历史聊天记录，而是多了一段对之前聊天内容的英文小结。</p><pre><code class=\"language-python\">conversation_with_summary.predict(input=\"鱼香肉丝怎么做？\")\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文\n2. 回答限制在100个字以内\n\nThe human greeted the AI and the AI responded that it can help cook by combining local ingredients and tailor the meal to the human's tastes and preferences. It promised to make the best dishes possible to the human's satisfaction.\nHuman: 鱼香肉丝怎么做？\nAI:\n&gt; Finished chain.\n\n' 鱼香肉丝是一道经典的家常菜，需要准备肉丝、葱姜蒜、鱼香调料、豆瓣酱、醋、糖、盐等调料，先将肉丝用盐、料酒、胡椒粉腌制，然后炒锅里放入葱姜蒜爆香，加入肉丝翻炒，加入鱼香调料、豆瓣酱、醋、糖等调料，最后放入少许水煮熟即可。'\n</code></pre><p>而如果这个时候我们调用 memory的load_memory_variables方法，可以看到记录下来的history是一小段关于对话的英文小结。而不是像上面那样，记录完整的历史对话。</p><pre><code class=\"language-python\">memory.load_memory_variables({})\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">{'history': '\\nThe human greeted the AI, to which the AI replied that it was a Chinese chef that enjoyed making Chinese dishes such as braised pork, Kung Pao chicken, and Fish-fragrant pork shreds. The AI also said that it would use fresh ingredients and carefully cook each dish to make them delicious. When the human asked about how to make Fish-fragrant pork shreds, the AI replied that it needed to prepare ingredients such as meat shreds, scallions, ginger, garlic, peppers, Sichuan pepper, soy sauce, sugar, vinegar, cooking wine, and cornstarch. The AI then explained that the meat shreds should first be marinated with cornstarch, cooking wine, salt, and pepper, and then the scallions, ginger, garlic, and peppers should be stir-fried in a wok, followed by the addition of the meat shreds. Finally, soy sauce, sugar, vinegar, and cornstarch should be added to season the dish.'}\n</code></pre><p>而如果我们进一步通过conversation_with_summary去和AI对话，就会看到英文的小结内容会随着对话内容不断变化。每一次AI都是把之前的小结和新的对话交给memory中定义的LLM再次进行小结。</p><pre><code class=\"language-python\">conversation_with_summary.predict(input=\"那蚝油牛肉呢？\")\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文\n2. 回答限制在100个字以内\n\nThe human greeted the AI and the AI responded that it can help cook by combining local ingredients and tailor the meal to the human's tastes and preferences. It promised to make the best dishes possible to the human's satisfaction. When asked how to make 鱼香肉丝, the AI responded that it requires the preparation of meat slices, scallion, ginger, garlic, fish sauce, doubanjiang, vinegar, sugar and salt. The meat slices should be marinated with salt, cooking wine and pepper, then stir-fried with scallion, ginger and garlic. The fish sauce, doubanjiang, vinegar, sugar and salt should be added in, with some water added to cook the dish.\nHuman: 那蚝油牛肉呢？\nAI:\n&gt; Finished chain.\n\n' 蚝油牛肉需要准备牛肉、蚝油、葱、姜、蒜、料酒、盐、糖、醋、淀粉和水。牛肉应先用盐、料酒和胡椒粉腌制，然后和葱、姜、蒜一起爆炒，再加入蚝油、糖、盐、醋和水，最后加入淀粉勾芡即可。'\n</code></pre><h2>两者结合，使用SummaryBufferMemory</h2><p>虽然SummaryMemory可以支持更长的对话轮数，但是它也有一个缺点，就是<strong>即使是最近几轮的对话，记录的也不是精确的内容</strong>。当你问“上一轮我问的问题是什么？”的时候，它其实没法给出准确的回答。不过，相信你也想到了，我们把BufferMemory和SummaryMemory结合一下不就好了吗？没错，LangChain里还真提供了一个这样的解决方案，就叫做ConversationSummaryBufferMemory。</p><p>下面，我们就来看看ConversationSummaryBufferMemory怎么用。</p><pre><code class=\"language-python\">from langchain import PromptTemplate\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.llms import OpenAI\n\nSUMMARIZER_TEMPLATE = \"\"\"请将以下内容逐步概括所提供的对话内容，并将新的概括添加到之前的概括中，形成新的概括。\n\nEXAMPLE\nCurrent summary:\nHuman询问AI对人工智能的看法。AI认为人工智能是一种积极的力量。\n\nNew lines of conversation:\nHuman：为什么你认为人工智能是一种积极的力量？\nAI：因为人工智能将帮助人类发挥他们的潜能。\n\nNew summary:\nHuman询问AI对人工智能的看法。AI认为人工智能是一种积极的力量，因为它将帮助人类发挥他们的潜能。\nEND OF EXAMPLE\n\nCurrent summary:\n{summary}\n\nNew lines of conversation:\n{new_lines}\n\nNew summary:\"\"\"\n\nSUMMARY_PROMPT = PromptTemplate(\n    input_variables=[\"summary\", \"new_lines\"], template=SUMMARIZER_TEMPLATE\n)\n\nmemory = ConversationSummaryBufferMemory(llm=OpenAI(), prompt=SUMMARY_PROMPT, max_token_limit=256)\n\nCHEF_TEMPLATE = \"\"\"你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文。\n2. 对于做菜步骤的回答尽量详细一些。\n\n{history}\nHuman: {input}\nAI:\"\"\"\nCHEF_PROMPT = PromptTemplate(\n    input_variables=[\"history\", \"input\"], template=CHEF_TEMPLATE\n)\n\nconversation_with_summary = ConversationChain(\n    llm=OpenAI(model_name=\"text-davinci-003\", stop=\"\\n\\n\", max_tokens=2048, temperature=0.5), \n    prompt=CHEF_PROMPT,\n    memory=memory,\n    verbose=True\n)\nanswer = conversation_with_summary.predict(input=\"你是谁？\")\nprint(answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-plain\">&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文。\n2. 对于做菜步骤的回答尽量详细一些。\n\nHuman: 你是谁？\nAI:\n&gt; Finished chain.\n 我是一个中国厨师，您有什么可以问我的关于做菜的问题吗？\n</code></pre><ol>\n<li>这个代码显得有些长，这是为了演示的时候让你看得更加清楚一些。我把Langchain原来默认的对Memory进行小结的提示语模版从英文改成中文的了，不过这个翻译工作我也是让ChatGPT帮我做的。如果你想了解原始的英文提示语是什么样的，可以去看一下它源码里面的 _DEFAULT_SUMMARIZER_TEMPLATE，对应的链接我也放在<a href=\"https://github.com/hwchase17/langchain/blob/master/langchain/memory/prompt.py\">这里</a>了。</li>\n<li>我们定义了一个 ConversationSummaryBufferMemory，在这个Memory的构造函数里面，我们指定了使用的LLM、提示语，以及一个max_token_limit参数。max_token_limit参数，其实就是告诉我们，当对话的长度到多长之后，我们就应该调用LLM去把文本内容小结一下。</li>\n<li>后面的代码其实就和前面其他的例子基本一样了。</li>\n</ol><p>因为我们在代码里面打开了Verbose模式，所以你能看到实际AI记录的整个对话历史是怎么样的。当我们连续多问AI几句话，你就会看到，随着对话轮数的增加，Token数量超过了前面的max_token_limit 。于是SummaryBufferMemory就会触发，对前面的对话进行小结，也就会出现一个 System的信息部分，里面是聊天历史的小结，而后面完整记录的实际对话轮数就变少了。</p><p>我们先问鱼香肉丝怎么做，Verbose的信息里还是显示历史的聊天记录。</p><pre><code class=\"language-plain\">answer = conversation_with_summary.predict(input=\"请问鱼香肉丝怎么做？\")\nprint(answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-plain\">\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文。\n2. 对于做菜步骤的回答尽量详细一些。\nHuman: 你是谁？\nAI:  我是一个中国厨师，您有什么可以问我的关于做菜的问题吗？\nHuman: 请问鱼香肉丝怎么做？\nAI:\n&gt; Finished chain.\n 鱼香肉丝是一道很受欢迎的中国菜，准备材料有：猪肉、木耳、胡萝卜、葱姜蒜、花椒、八角、辣椒、料酒、糖、盐、醋、麻油、香油。做法步骤如下：1. 将猪肉切成薄片，用料酒、盐、糖、醋、麻油抓匀；2. 将木耳洗净，切碎；3. 将胡萝卜切丝；4. 将葱姜蒜切碎；5. 将花椒、八角、辣椒放入油锅中炸熟；6. 将葱姜蒜炒香；7. 加入猪肉片翻炒；8. 加入木耳、胡萝卜丝、花椒、八角、辣椒翻炒；9. 加入盐、糖、醋、麻油、香油调味；10. 加入水煮熟，即可出锅。\n</code></pre><p>等到我们再问蚝油牛肉，前面的对话就被小结到System下面去了。</p><pre><code class=\"language-plain\">answer = conversation_with_summary.predict(input=\"那蚝油牛肉呢？\")\nprint(answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-plain\">\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n你是一个中国厨师，用中文回答做菜的问题。你的回答需要满足以下要求:\n1. 你的回答必须是中文。\n2. 对于做菜步骤的回答尽量详细一些。\nSystem: \nHuman询问AI是谁，AI回答自己是一个中国厨师，并问Human是否有关于做菜的问题。Human问AI如何做出鱼香肉丝，AI回答准备材料有猪肉、木耳、胡萝卜、葱姜蒜、花椒、八角、辣椒、料酒、糖、盐、醋、麻油、香油，做法步骤是将猪肉切成薄片，用料酒、盐、糖、醋、麻油抓匀，木耳\nHuman: 那蚝油牛肉呢？\nAI:\n&gt; Finished chain.\n 准备材料有牛肉、葱、姜、蒜、蚝油、料酒、醋、糖、盐、香油，做法步骤是先将牛肉切成薄片，用料酒、盐、糖、醋、麻油抓匀，然后将葱、姜、蒜切碎，加入蚝油拌匀，最后加入香油搅拌均匀即可。\n</code></pre><p>当然，在你实际使用SummaryBufferMemory的时候，并不需要把各个Prompt都改成自定义的中文版本。用默认的英文Prompt就足够了。因为在Verbose信息里出现的System信息并不会在实际的对话进行过程中显示给用户。这部分提示，只要AI自己能够理解就足够了。当然，你也可以根据实际对话的效果，来改写自己需要的提示语。</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/76/4fd464abb35dcaa62266e3fc3bf24c76.png?wh=1920x755\" alt=\"图片\"></p><p>Pinecone 在自己网站上给出了一个数据对比，不同类型的Memory，随着对话轮数的增长，占用的Token数量的变化。你可以去看一看，不同的Memory在不同的参数下，占用的Token数量是不同的。比较合理的方式，还是使用这里的ConversationSummaryBufferMemory，这样既可以在记录少数对话内容的时候，记住的东西更加精确，也可以在对话轮数增长之后，既能够记住各种信息，又不至于超出Token数量的上限。</p><p>不过，在运行程序的过程里，你应该可以感觉到现在程序跑得有点儿慢。这是因为我们使用 ConversationSummaryBufferMemory很多时候要调用多次OpenAI的API。在字数超过 max_token_limit 的时候，需要额外调用一次API来做小结。而且这样做，对应的Token数量消耗也是不少的。</p><p>所以，<strong>不是所有的任务，都适合通过调用一次ChatGPT的API来解决。</strong>很多时候，你还是可以多思考是否可以用上一讲介绍的 UtilityChain 和 TransformChain 来解决问题。</p><h2>让AI记住点有用的信息</h2><p>我们不仅可以在整个对话过程里，使用我们的Memory功能。如果你之前已经有了一系列的历史对话，我们也可以通过Memory提供的save_context接口，把历史聊天记录灌进去。然后基于这个Memory让AI接着和用户对话。比如下面我们就把一组电商客服历史对话记录给了SummaryBufferMemory。</p><pre><code class=\"language-plain\">memory = ConversationSummaryBufferMemory(llm=OpenAI(), prompt=SUMMARY_PROMPT, max_token_limit=40)\nmemory.save_context(\n    {\"input\": \"你好\"}, \n    {\"ouput\": \"你好，我是客服李四，有什么我可以帮助您的么\"}\n    )\nmemory.save_context(\n    {\"input\": \"我叫张三，在你们这里下了一张订单，订单号是 2023ABCD，我的邮箱地址是 customer@abc.com，但是这个订单十几天了还没有收到货\"}, \n    {\"ouput\": \"好的，您稍等，我先为您查询一下您的订单\"}\n    )\nmemory.load_memory_variables({})\n</code></pre><p>输出结果：</p><pre><code class=\"language-plain\">{'history': 'System: \\nHuman和AI打招呼，AI介绍自己是客服李四，问Human有什么可以帮助的。Human提供订单号和邮箱地址，AI表示会为其查询订单状态。'}\n</code></pre><p><span class=\"reference\">注：为了演示方便，我设置了一个很小的 max_token_limit，但是这个问题在大的 max_token_limit 下，面对上下文比较多的会话一样会有问题。</span></p><p>通过调用 memory.load_memory_variables 方法，我们发现AI对整段对话做了小结。但是这个小结有个问题，就是<strong>它并没有提取到我们最关注的信息</strong>，比如用户的订单号、用户的邮箱。只有有了这些信息，AI才能够去查询订单，拿到结果然后回答用户的问题。</p><p>以前在还没有ChatGPT的时代，在客服聊天机器人这样的领域，我们会通过命名实体识别的方式，把邮箱、订单号之类的关键信息提取出来。在有了ChatGPT这样的大语言模型之后，我们还是应该这样做。不过我们不是让专门的命名实体识别的算法做，而是直接让ChatGPT帮我们做。Langchain也内置了一个EntityMemory的封装，让AI自动帮我们提取这样的信息。我们来试一试。</p><pre><code class=\"language-plain\">from langchain.chains import ConversationChain\nfrom langchain.memory import ConversationEntityMemory\nfrom langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n\nentityMemory = ConversationEntityMemory(llm=llm)\nconversation = ConversationChain(\n    llm=llm, \n    verbose=True,\n    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n    memory=entityMemory\n)\n\nanswer=conversation.predict(input=\"我叫张老三，在你们这里下了一张订单，订单号是 2023ABCD，我的邮箱地址是 customer@abc.com，但是这个订单十几天了还没有收到货\")\nprint(answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-plain\">&gt; Entering new ConversationChain chain...\nPrompt after formatting:\nYou are an assistant to a human, powered by a large language model trained by OpenAI.\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\nContext:\n{'张老三': '', '2023ABCD': '', 'customer@abc.com': ''}\nCurrent conversation:\nLast line:\nHuman: 我叫张老三，在你们这里下了一张订单，订单号是 2023ABCD，我的邮箱地址是 customer@abc.com，但是这个订单十几天了还没有收到货\nYou:\n&gt; Finished chain.\n 您好，张老三，我很抱歉你没有收到货。我们会尽快核实订单信息，并尽快给您处理，请您耐心等待，如果有任何疑问，欢迎您随时联系我们。\n</code></pre><p>我们还是使用ConversationChain，只是这一次，我们指定使用EntityMemory。可以看到，在Verbose的日志里面，整个对话的提示语，多了一个叫做 Context 的部分，里面包含了刚才用户提供的姓名、订单号和邮箱。</p><p>进一步，我们把memory里面存储的东西打印出来。</p><pre><code class=\"language-plain\">print(conversation.memory.entity_store.store)\n</code></pre><p>输出结果：</p><pre><code class=\"language-plain\">{'张老三': '张老三是一位订单号为2023ABCD、邮箱地址为customer@abc.com的客户。', '2023ABCD': '2023ABCD is an order placed by customer@abc.com that has not been received after more than ten days.', 'customer@abc.com': 'Email address of Zhang Lao San, who placed an order with Order Number 2023ABCD, but has not received the goods more than ten days later.'}\n</code></pre><p>可以看到，EntityMemory里面不仅存储了这些命名实体的名字，也对应的把命名实体所关联的上下文记录了下来。这个时候，如果我们再通过对话来询问相关的问题，AI也能够答上来。</p><p>问题1：</p><pre><code class=\"language-python\">answer=conversation.predict(input=\"我刚才的订单号是多少？\")\nprint(answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\"> 您的订单号是2023ABCD。\n</code></pre><p>问题2：</p><pre><code class=\"language-python\">answer=conversation.predict(input=\"订单2023ABCD是谁的订单？\")\nprint(answer)\n</code></pre><p>输出结果：</p><pre><code class=\"language-python\">订单2023ABCD是您张老三的订单，您的邮箱地址是customer@abc.com。\n</code></pre><p>这些往往才是我们在聊天的过程中真正关注的信息。如果我们要做一个电商客服，后续的对话需要查询订单号、用户姓名的时候，这些信息是必不可少的。</p><p>事实上，我们不仅可以把这些Memory放在内存里面，还可以进一步把它们存放在Redis这样的外部存储里面。这样即使我们的服务进程消失了，这些“记忆”也不会丢失。你可以对照着<a href=\"https://python.langchain.com/en/latest/modules/memory/examples/agent_with_memory_in_db.html\">官方文档</a>尝试一下。</p><h2>小结</h2><p>最后，我们来做个小结。这一讲，我主要为你讲解了Langchain里面的Memory功能。Memory对整个对话的过程里我们希望记住的东西做了封装。我们可以通过BufferWindowMemory记住过去几轮的对话，通过SummaryMemory概括对话的历史并记下来。也可以将两者结合，使用BufferSummaryMemory来维护一个对整体对话做了小结，同时又记住最近几轮对话的“记忆”。</p><p>不过，<strong>更具有实用意义的是 EntityMemory</strong>。在实际使用AI进行对话的过程中，并不是让它不分轻重地记住一切内容，而是有一些我们要关注的核心要点。比如，如果你要搭建一个电商客服的聊天机器人，你肯定希望它记住具体的订单号、用户的邮箱等等。这个时候，我们就可以使用EntityMemory，它会帮助我们记住整个对话里面的“命名实体”（Entity），保留实际在对话中我们最关心的信息。</p><p>在过去的几讲里面，从llama-index开始，我们已经学会了将外部的资料库索引起来进行问答，也学会了通过Langchain的链式调用，实时获取外部的数据信息，或者运行Python程序。这一讲，我们又专门研究了怎样记住对话中我们关心的部分。</p><p><strong>将这些能力组合起来，我们就可以搭建一个完整的，属于自己的聊天机器人。</strong>我们可以根据用户提供的订单号，去查询订单物流信息，安抚客户；也可以根据用户想要了解的商品，查询我们的商品库，进行商品导购。而这些，也是我们下一讲要解决的问题。</p><h2>思考题</h2><p>最后，我给你留一道思考题。在这一讲里，我为你介绍了EntityMemory的使用方法，Langchain里面还提供了一个 <a href=\"https://langchain.readthedocs.io/en/latest/modules/memory/types/kg.html\">KnowledgeGraphMemory</a>，你能不能去试着用一下，看看它能在什么样的场景下帮你解决问题？</p><h2>推荐阅读</h2><p>在Pinecone提供的Langchain AI Handbook里面，专门测试了一下，从BufferWindowMemory到BufferSummaryMemory，对于上下文保持的能力，以及消耗的Token数量的统计。那个<a href=\"https://www.pinecone.io/learn/langchain-conversational-memory/\">教程</a>你也可以去看一下。</p>","comments":[{"had_liked":false,"id":372732,"user_name":"张弛","can_delete":false,"product_type":"c1","uid":1681607,"ip_address":"中国台湾","ucode":"77E0BC5D0667A8","user_header":"https://static001.geekbang.org/account/avatar/00/19/a8/c7/f57dadb9.jpg","comment_is_top":false,"comment_ctime":1681520611,"is_pvip":false,"replies":[{"id":136116,"content":"目前大家综合考虑延时，基本上都是 Azure 或者 腾讯云北美节点","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681614549,"ip_address":"上海","comment_id":372732,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师讲的太好了，通过对AI的能力探索，感觉有了跟自己业务结合的空间，但是如果需要身边的朋友使用的话，还是需要封装成网页或者app，而之前例子里面的HuggingFace国内访问还是挺慢的，能否介绍一下如果就是想把实战提高篇里的chatbot做一些定制化，封装为网页给别人在国内使用，推荐使用什么方式部署（例如腾讯云硅谷？），谢谢。","like_count":3,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613963,"discussion_content":"目前大家综合考虑延时，基本上都是 Azure 或者 腾讯云北美节点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681614550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372626,"user_name":"Geek_429477","can_delete":false,"product_type":"c1","uid":3253315,"ip_address":"北京","ucode":"CDBFE65EF4BF23","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDdkdC6WfWDyETmkUwuhMic8ytESib2fYGSssBjly3vxjdFu9xd24qDehqDKjIlktlhOAuicmHf8YvA/132","comment_is_top":false,"comment_ctime":1681358320,"is_pvip":false,"replies":[{"id":136128,"content":"不是左右问题都应该灌到FAQ里呀\n\n看一下17讲里，通过拿到Action Input是“电子产品”，问题是“询问商品类别”，通过一个Tools来解决这种，输入可能性很多的情况可能更合适。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681616182,"ip_address":"上海","comment_id":372626,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"每天第一个追更. 请问一下大佬:\n比如一个客服机器人,卖很多种类的商品,按照大佬之前的文章思路,QA问题集如下\nQ:你们卖哪些产品\nA:图书,玩具,衣服.......(很多字)\n\n一个用户问,你们卖电子产品吗?\n如果先搜索再提示,answer的字数太多,造成token不够,该怎么解决","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613980,"discussion_content":"不是左右问题都应该灌到FAQ里呀\n\n看一下17讲里，通过拿到Action Input是“电子产品”，问题是“询问商品类别”，通过一个Tools来解决这种，输入可能性很多的情况可能更合适。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1681616182,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1953765,"avatar":"","nickname":"Geek_d54869","note":"","ucode":"CF55462EF65B9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646975,"discussion_content":"但凡你这里用的是大模型 就不会返回很多内容 基于用户提问和相似的语料回答都是简短的 只有传统的基于问答对方式才会照本宣科 输出一堆A里的内容","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719198395,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1559128,"avatar":"https://static001.geekbang.org/account/avatar/00/17/ca/58/6fe1854c.jpg","nickname":"金","note":"","ucode":"48D0B5CB8E5F9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613745,"discussion_content":"用户如果看到这么多字估计都烦了。\n把token设置长一点","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681481355,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"湖南","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372620,"user_name":"Oli张帆","can_delete":false,"product_type":"c1","uid":1338098,"ip_address":"北京","ucode":"6E60A370C3C14A","user_header":"https://static001.geekbang.org/account/avatar/00/14/6a/f2/db90fa96.jpg","comment_is_top":false,"comment_ctime":1681354574,"is_pvip":false,"replies":[{"id":136111,"content":"Langchain现在也有JS版本啦，可以去看一下","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681614201,"ip_address":"上海","comment_id":372620,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"Langchain确实是非常有用的利器，虽然我目前的项目使用NodeJS，但是里面的很多思路非常值得借鉴。","like_count":2,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613954,"discussion_content":"Langchain现在也有JS版本啦，可以去看一下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681614201,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373125,"user_name":"Geek_053159","can_delete":false,"product_type":"c1","uid":3603126,"ip_address":"四川","ucode":"6435D4A6D83FAE","user_header":"","comment_is_top":false,"comment_ctime":1682068426,"is_pvip":false,"replies":[{"id":136512,"content":"本质上也是memory，就是把历史对话记录也用上了。\n\n具体OpenAI是怎么做的我们不知道，因为这个状态它是在服务端维护的。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683091634,"ip_address":"上海","comment_id":373125,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师 在chatgpt 界面使用它时 是不是也用到了memory呢 每次我们的问话都会全部发送给它 当超过token数量时 chatGPT会自动总结之前的会话内容吗 还是需要我们提示它来总结 然后再把总结和新的问话结合起来发给它","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616753,"discussion_content":"本质上也是memory，就是把历史对话记录也用上了。\n\n具体OpenAI是怎么做的我们不知道，因为这个状态它是在服务端维护的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683091634,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372631,"user_name":"hyetry","can_delete":false,"product_type":"c1","uid":1256114,"ip_address":"广东","ucode":"32CE95B95360D2","user_header":"","comment_is_top":false,"comment_ctime":1681367388,"is_pvip":false,"replies":[{"id":136120,"content":"去Huggingface上找 https:&#47;&#47;huggingface.co&#47;models?pipeline_tag=summarization&amp;sort=downloads\n\n按照需求筛选标签","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681614740,"ip_address":"上海","comment_id":372631,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师，有文本摘要对应的替换推荐模型吗？","like_count":1,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613969,"discussion_content":"去Huggingface上找 https://huggingface.co/models?pipeline_tag=summarization&amp;sort=downloads\n\n按照需求筛选标签","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681614740,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373221,"user_name":"Jelly","can_delete":false,"product_type":"c1","uid":2861619,"ip_address":"广东","ucode":"877CC61AF0A7F3","user_header":"https://static001.geekbang.org/account/avatar/00/2b/aa/33/5e063968.jpg","comment_is_top":false,"comment_ctime":1682256176,"is_pvip":false,"replies":[{"id":136510,"content":"通过用户id或者session id维护一个map类型的数据结构好了。也可以用\nhttps:&#47;&#47;python.langchain.com&#47;en&#47;latest&#47;modules&#47;memory&#47;examples&#47;redis_chat_message_history.html\n\n通过redis存储起来","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1683091513,"ip_address":"上海","comment_id":373221,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"请问老师，每个用户单独的内存会话怎么做？需要用其他数据库存储起来？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616750,"discussion_content":"通过用户id或者session id维护一个map类型的数据结构好了。也可以用\nhttps://python.langchain.com/en/latest/modules/memory/examples/redis_chat_message_history.html\n\n通过redis存储起来","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1683091513,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1023093,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9c/75/525e53b1.jpg","nickname":"Hobby","note":"","ucode":"8954830380D44F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":621305,"discussion_content":"页面更新：https://python.langchain.com/docs/modules/memory/how_to/agent_with_memory_in_db","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1687097311,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372654,"user_name":"Toni","can_delete":false,"product_type":"c1","uid":3206957,"ip_address":"瑞士","ucode":"E6B2FACCC1E000","user_header":"https://static001.geekbang.org/account/avatar/00/30/ef/2d/757bb0d3.jpg","comment_is_top":false,"comment_ctime":1681400551,"is_pvip":false,"replies":[{"id":136127,"content":"因为用了KG Memory之后，历史对话的信息没有作为Prompt的一部分给到ChatGPT\n\n这个时候，你需要一个既组合了history消息，又有KGMemory的组合Memory","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681616100,"ip_address":"上海","comment_id":372654,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"使用 LangChain 中的 KnowledgeGraphMemory，检测其处理对话信息的能力。\n\nfrom langchain.memory import ConversationKGMemory # 知识图谱记忆\nllm = OpenAI(temperature=0)\nmemory=ConversationKGMemory(llm=llm)\ntemplate = &quot;&quot;&quot;The following is a conversation between a human and an AI.\nThe AI provides lots of specific details from its context. \nAI uses all the information contained in &quot;Related Information&quot;, &quot;Current Conversation&quot; and &quot;Context&quot;\nIf the AI does not know the answer to a question, it truthfully says it does not know，and does not hallucinate.\nanswer in 20 word.\n\n对话如下\n第一轮\nQ1 (喂给了AI 这些信息，牵扯到几个方向，试AI 如何提取并处理这些信息)\ninput=&quot;I am James and I&#39;m helping Will. He&#39;s an engineer working in IBM&#39;s electronics department. He encountered a problem in the &#39;Knowledge Graph Memory&#39; project&quot;\nA1\n&gt; Finished chain.\n&quot; Hi James, I understand that Will is having difficulty with the &#39;Knowledge Graph Memory&#39; project. I&#39;m not familiar with this project, but I can help you find the resources you need to solve the problem.&quot;\n(回答中AI 分出了James 和 Will，其它的大差不差。)\n\n第二轮对话\nQ2\ninput=&quot;What do you know about Will?&quot;\nA2\n&gt; Finished chain.\n&quot; Will is an engineer working in IBM&#39;s electronics department. He is currently working on a Knowledge Graph Memory project and has encountered a problem.&quot;\n(AI 充分利用了在第一轮对话中得到的有关Will 的信息。)\n\n第三轮对话\nA3\ninput=&quot;What do you know about me?&quot;\nQ3\n&gt; Finished chain.\n&quot; I know that you are talking to me right now and that you asked me a question about Will. I also know that Will is an engineer working in IBM&#39;s electronics department and is working on the Knowledge Graph Memory project.&quot;\n(AI 没有捕捉到 me, I, James 之间的关系，尽管它用了Hi James, )\n\n第四轮对话\nA4 (继续追问上轮中出现的漏洞)\ninput=&quot;What is my name?&quot;\nQ4\n&gt; Finished chain.\n&quot;Your name is John Smith.&quot;\n(AI 彻底错了)\n\n结果有点意外，是算法有错还是参数设置有待改进? \n共指消解(coreference&#47;entity resolution)在知识图谱记忆中是最基本的方法，整合同一实体的不同称谓。","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613979,"discussion_content":"因为用了KG Memory之后，历史对话的信息没有作为Prompt的一部分给到ChatGPT\n\n这个时候，你需要一个既组合了history消息，又有KGMemory的组合Memory","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681616100,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372644,"user_name":"Evan","can_delete":false,"product_type":"c1","uid":1344281,"ip_address":"上海","ucode":"B877ABD0CF4661","user_header":"https://static001.geekbang.org/account/avatar/00/14/83/19/0a3fe8c1.jpg","comment_is_top":false,"comment_ctime":1681379134,"is_pvip":false,"replies":[{"id":136119,"content":"具体报什么错呀？","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681614693,"ip_address":"上海","comment_id":372644,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"from langchain.memory import ConversationSummaryMemory. \n引入这个的时候报错","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613968,"discussion_content":"具体报什么错呀？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681614693,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1344281,"avatar":"https://static001.geekbang.org/account/avatar/00/14/83/19/0a3fe8c1.jpg","nickname":"Evan","note":"","ucode":"B877ABD0CF4661","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":613986,"discussion_content":"已经解决了，主要langchain和openai版本冲突了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681617287,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":613968,"ip_address":"上海","group_id":0},"score":613986,"extra":""}]}]},{"had_liked":false,"id":372598,"user_name":"君为","can_delete":false,"product_type":"c1","uid":1201843,"ip_address":"北京","ucode":"A1125F0CA4E75D","user_header":"https://static001.geekbang.org/account/avatar/00/12/56/b3/29c814af.jpg","comment_is_top":false,"comment_ctime":1681346609,"is_pvip":false,"replies":[{"id":136106,"content":"可以看看下一讲，里面就有了一个简单的你需要的这些需求的示例了。","user_name":"作者回复","user_name_real":"编辑","uid":1053568,"ctime":1681614038,"ip_address":"上海","comment_id":372598,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"老师你好，客服处理用户的问题很多都是一系列任务，有时还需要客服主动给用户发消息。比如用户要退货，需要提供手机号，订单号等信息，用户找不到订单号在哪需要一步步指导用户然后再做一些列操作。\n\n请问老师识别用户意图，提取关键信息后，如何让ChatGPT进入设定的工作流，最终处理好用户的问题？","like_count":0,"discussions":[{"author":{"id":1053568,"avatar":"https://static001.geekbang.org/account/avatar/00/10/13/80/8de66543.jpg","nickname":"徐文浩","note":"","ucode":"1D39AC564172E9","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":613948,"discussion_content":"可以看看下一讲，里面就有了一个简单的你需要的这些需求的示例了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1681614038,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1953765,"avatar":"","nickname":"Geek_d54869","note":"","ucode":"CF55462EF65B9E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646974,"discussion_content":"定一些任务工具\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719197935,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379373,"user_name":"Penguin Shi","can_delete":false,"product_type":"c1","uid":3646806,"ip_address":"广东","ucode":"477A24D7ECE09A","user_header":"https://static001.geekbang.org/account/avatar/00/37/a5/56/b3cf71a9.jpg","comment_is_top":false,"comment_ctime":1691831622,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100541001,"comment_content":"原文：Langchain 里面还提供了一个 KnowledgeGraphMemory，你能不能去试着用一下，看看它能在什么样的场景下帮你解决问题？\n更新：Conversation Knowledge Graph Memory，https:&#47;&#47;python.langchain.com&#47;docs&#47;modules&#47;memory&#47;types&#47;kg","like_count":1},{"had_liked":false,"id":386281,"user_name":"杨逸林","can_delete":false,"product_type":"c1","uid":1167233,"ip_address":"","ucode":"4BF3CF3E2F1AC7","user_header":"https://static001.geekbang.org/account/avatar/00/11/cf/81/96f656ef.jpg","comment_is_top":false,"comment_ctime":1704439955,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"可不可以这样，让一些总结类的活丢给本地 RTX 4090 可以部署的 BlueLM，然后把这一轮的对话和总结的内容丢给 ChatGPT，这样是不是更省 Token。简单的活给本地的部署的完成，复杂的丢给 ChatGPT4。就是不知道能不能做到。","like_count":0},{"had_liked":false,"id":385685,"user_name":"l_j_dota_1111","can_delete":false,"product_type":"c1","uid":1490128,"ip_address":"天津","ucode":"4CA8A41F5107C7","user_header":"https://static001.geekbang.org/account/avatar/00/16/bc/d0/7a595383.jpg","comment_is_top":false,"comment_ctime":1703118176,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"你好，请问如果既要求有总结，也要求有实体提取，又要求记住上下文，是用哪个chain呢？还有就是还想通过矢量数据库查询出信息就行参考文档，怎么和这些chain结合呢","like_count":0},{"had_liked":false,"id":383887,"user_name":"陈斌","can_delete":false,"product_type":"c1","uid":1149402,"ip_address":"广东","ucode":"AD6933D125C930","user_header":"https://static001.geekbang.org/account/avatar/00/11/89/da/136cdca6.jpg","comment_is_top":false,"comment_ctime":1699871867,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"这一章节讲了如何利用外部的存储去实现上下文及关键信息的缓存，老师里面提到的一些点都是在应用 gpt 实现过程中对成本和开销需要考虑的问题，非常有实用性。\n尤其是关键信息，主要是对场景下问题回答的约束，比如在一个电商网站的客服机器人，问它怎么做饭的，是可以拒绝回答的。这里的模型是否需要重新训练，还是可以利用 gpt 判断问题与场景的相关性，再利用提示语让 gpt 给出一个委婉回答。","like_count":0},{"had_liked":false,"id":383314,"user_name":"Aurore","can_delete":false,"product_type":"c1","uid":1563911,"ip_address":"广东","ucode":"20CE92604E804A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLH5MBWgqEDFiaIpRACgwibh4DGhRgvTRCGpa713rH0c3mI26kj6Ft2jTUCFvxlGdBrtLh96EKmY7yg/132","comment_is_top":false,"comment_ctime":1698840457,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"在最后llm_chain.predict(human_input=&quot;你是谁？&quot;)并没有输入chat_history来替换对应的占位符，不会出错吗","like_count":0},{"had_liked":false,"id":383313,"user_name":"Aurore","can_delete":false,"product_type":"c1","uid":1563911,"ip_address":"广东","ucode":"20CE92604E804A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLH5MBWgqEDFiaIpRACgwibh4DGhRgvTRCGpa713rH0c3mI26kj6Ft2jTUCFvxlGdBrtLh96EKmY7yg/132","comment_is_top":false,"comment_ctime":1698840375,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"请教一下。滑窗memory中，llm_chain.predict(human_input=&quot;你是谁？&quot;)。没有输入chat_history没有问题吗？","like_count":0},{"had_liked":false,"id":377418,"user_name":"Edon du","can_delete":false,"product_type":"c1","uid":1074742,"ip_address":"河南","ucode":"1648624751AAE9","user_header":"https://static001.geekbang.org/account/avatar/00/10/66/36/b4a4e6fb.jpg","comment_is_top":false,"comment_ctime":1688465838,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"老师，搭建外部资料索引的QA回答客服，如何使用memory功能呀?\n我试了了langchain的相关memory，都不太理想，只能保存历史会话，但是真正询问时，并不能正确回答出「我第一句说的什么」类似的问题","like_count":0},{"had_liked":false,"id":377351,"user_name":"Charles","can_delete":false,"product_type":"c1","uid":2559238,"ip_address":"上海","ucode":"8ACBA423B5A505","user_header":"https://static001.geekbang.org/account/avatar/00/27/0d/06/970cc957.jpg","comment_is_top":false,"comment_ctime":1688373927,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"使用ConversationEntityMemory的进行redis缓存时报错：redis.exceptions.ResponseError: unknown command `GETEX`, with args beginning with: `memory_store:default:Zhang Laosan`, `EX`, `259200`, \n这个有人遇到过吗，怎么解决？","like_count":0,"discussions":[{"author":{"id":2559238,"avatar":"https://static001.geekbang.org/account/avatar/00/27/0d/06/970cc957.jpg","nickname":"Charles","note":"","ucode":"8ACBA423B5A505","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":622570,"discussion_content":"查看redis文档发现，我连接得redis版本是6.0版本的，getex指令是6.2版本之后才有的，应该是升级redis数据库版本到6.2之后","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1688431599,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":375119,"user_name":"王乔","can_delete":false,"product_type":"c1","uid":1037588,"ip_address":"上海","ucode":"D4A423F38BD609","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d5/14/520e2bc6.jpg","comment_is_top":false,"comment_ctime":1684926940,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"请问老师，如果要AI读很长的文档怎么写? 现在文章过长直接就报错了","like_count":0},{"had_liked":false,"id":372603,"user_name":"浩仔是程序员","can_delete":false,"product_type":"c1","uid":1104601,"ip_address":"广东","ucode":"A7E5CF9E1571A2","user_header":"https://static001.geekbang.org/account/avatar/00/10/da/d9/f051962f.jpg","comment_is_top":false,"comment_ctime":1681348298,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100541001,"comment_content":"来了","like_count":0}]}