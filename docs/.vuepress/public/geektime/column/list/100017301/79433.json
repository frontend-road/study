{"id":79433,"title":"53 | 算法实战（二）：剖析搜索引擎背后的经典数据结构和算法","content":"<p>像百度、Google这样的搜索引擎，在我们平时的工作、生活中，几乎天天都会用到。如果我们把搜索引擎也当作一个互联网产品的话，那它跟社交、电商这些类型的产品相比，有一个非常大的区别，那就是，它是一个技术驱动的产品。所谓技术驱动是指，搜索引擎实现起来，技术难度非常大，技术的好坏直接决定了这个产品的核心竞争力。</p><p>在搜索引擎的设计与实现中，会用到大量的算法。有很多针对特定问题的算法，也有很多我们专栏中讲到的基础算法。所以，百度、Google这样的搜索引擎公司，在面试的时候，会格外重视考察候选人的算法能力。</p><p><strong><span class=\"orange\">今天我就借助搜索引擎，这样一个非常有技术含量的产品，来给你展示一下，数据结构和算法是如何应用在其中的。</span></strong></p><h2>整体系统介绍</h2><p>像Google这样的大型商用搜索引擎，有成千上万的工程师，十年如一日地对它进行优化改进，所以，它所包含的技术细节非常多。我很难、也没有这个能力，通过一篇文章把所有细节都讲清楚，当然这也不是我们专栏所专注的内容。</p><p>所以，接下来的讲解，我主要给你展示，如何在一台机器上（假设这台机器的内存是8GB， 硬盘是100多GB），通过少量的代码，实现一个小型搜索引擎。不过，麻雀虽小，五脏俱全。跟大型搜索引擎相比，实现这样一个小型搜索引擎所用到的理论基础是相通的。</p><!-- [[[read_end]]] --><p>搜索引擎大致可以分为四个部分：<strong>搜集</strong>、<strong>分析</strong>、<strong>索引</strong>、<strong>查询</strong>。其中，搜集，就是我们常说的利用爬虫爬取网页。分析，主要负责网页内容抽取、分词，构建临时索引，计算PageRank值这几部分工作。索引，主要负责通过分析阶段得到的临时索引，构建倒排索引。查询，主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。</p><p>接下来，我就按照网页处理的生命周期，从这四个阶段，依次来给你讲解，一个网页从被爬取到最终展示给用户，这样一个完整的过程。与此同时，我会穿插讲解，这个过程中需要用到哪些数据结构和算法。</p><h2>搜集</h2><p>现在，互联网越来越发达，网站越来越多，对应的网页也就越来越多。对于搜索引擎来说，它事先并不知道网页都在哪里。打个比方来说就是，我们只知道海里面有很多鱼，但却并不知道鱼在哪里。那搜索引擎是如何爬取网页的呢？</p><p>搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。我们可以利用图的遍历搜索算法，来遍历整个互联网中的网页。</p><p>我们前面介绍过两种图的遍历方法，深度优先和广度优先。搜索引擎采用的是广度优先搜索策略。具体点讲的话，那就是，我们先找一些比较知名的网页（专业的叫法是权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。</p><p>基本的原理就是这么简单。但落实到实现层面，还有很多技术细节。我下面借助搜集阶段涉及的几个重要文件，来给你解释一下搜集工程都有哪些关键技术细节。</p><h3>1.待爬取网页链接文件：links.bin</h3><p>在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从links.bin文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到links.bin文件中。</p><p>这样用文件来存储网页链接的方式，还有其他好处。比如，支持断点续爬。也就是说，当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。</p><p>关于如何解析页面获取链接，我额外多说几句。我们可以把整个页面看作一个大的字符串，然后利用字符串匹配算法，在这个大字符串中，搜索<code>&lt;link&gt;</code>这样一个网页标签，然后顺序读取<code>&lt;link&gt;&lt;/link&gt;</code>之间的字符串。这其实就是网页链接。</p><h3>2.网页判重文件：bloom_filter.bin</h3><p>如何避免重复爬取相同的网页呢？这个问题我们在<a href=\"https://time.geekbang.org/column/article/76827\">位图</a>那一节已经讲过了。使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。</p><p>不过，还是刚刚那个问题，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。</p><p>这个问题该怎么解决呢？我们可以定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在bloom_filter.bin文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的bloom_filter.bin文件，将其恢复到内存中。</p><h3>3.原始网页存储文件：doc_raw.bin</h3><p>爬取到网页之后，我们需要将其存储下来，以备后面离线分析、索引之用。那如何存储海量的原始网页数据呢？</p><p>如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。具体的存储格式，如下图所示。其中，doc_id这个字段是网页的编号，我们待会儿再解释。</p><p><img src=\"https://static001.geekbang.org/resource/image/19/4d/195c9a1dceaaa9f4d2483fa91455404d.jpg?wh=1142*644\" alt=\"\"></p><p>当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过1GB的时候，我们就创建一个新的文件，用来存储新爬取的网页。</p><p>假设一台机器的硬盘大小是100GB左右，一个网页的平均大小是64KB。那在一台机器上，我们可以存储100万到200万左右的网页。假设我们的机器的带宽是10MB，那下载100GB的网页，大约需要10000秒。也就是说，爬取100多万的网页，也就是只需要花费几小时的时间。</p><h3>4.网页链接及其编号的对应文件：doc_id.bin</h3><p>刚刚我们提到了网页编号这个概念，我现在解释一下。网页编号实际上就是给每个网页分配一个唯一的ID，方便我们后续对网页进行分析、索引。那如何给网页编号呢？</p><p>我们可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个doc_id.bin文件中。</p><p><strong>爬虫在爬取网页的过程中，涉及的四个重要的文件，我就介绍完了。其中，links.bin和bloom_filter.bin这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。</strong></p><h2>分析</h2><p>网页爬取下来之后，我们需要对网页进行离线分析。分析阶段主要包括两个步骤，第一个是抽取网页文本信息，第二个是分词并创建临时索引。我们逐一来讲解。</p><h3>1.抽取网页文本信息</h3><p>网页是半结构化数据，里面夹杂着各种标签、JavaScript代码、CSS样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。我们如何从半结构化的网页中，抽取出搜索引擎关系的文本信息呢？</p><p>我们之所以把网页叫作半结构化数据，是因为它本身是按照一定的规则来书写的。这个规则就是<strong>HTML语法规范</strong>。我们依靠HTML标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。</p><p>第一步是去掉JavaScript代码、CSS格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是<code>&lt;style&gt;&lt;/style&gt;</code>，<code>&lt;script&gt;&lt;/script&gt;</code>，<code>&lt;option&gt;&lt;/option&gt;</code>这三组标签之间的内容。我们可以利用AC自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找<code>&lt;style&gt;</code>, <code>&lt;script&gt;</code>, <code>&lt;option&gt;</code>这三个关键词。当找到某个关键词出现的位置之后，我们只需要依次往后遍历，直到对应结束标签（<code>&lt;/style&gt;</code>, <code>&lt;/script&gt;</code>, <code>&lt;/option</code>）为止。而这期间遍历到的字符串连带着标签就应该从网页中删除。</p><p>第二步是去掉所有HTML标签。这一步也是通过字符串匹配算法来实现的。过程跟第一步类似，我就不重复讲了。</p><h3>2.分词并创建临时索引</h3><p>经过上面的处理之后，我们就从网页中抽取出了我们关心的文本信息。接下来，我们要对文本信息进行分词，并且创建临时索引。</p><p>对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。但是，对于中文来说，分词就复杂太多了。我这里介绍一种比较简单的思路，基于字典和规则的分词方法。</p><p>其中，字典也叫词库，里面包含大量常用的词语（我们可以直接从网上下载别人整理好的）。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。我举个例子解释一下。</p><p>比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人”划为一个词。具体到实现层面，我们可以将词库中的单词，构建成Trie树结构，然后拿网页文本在Trie树中匹配。</p><p>每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/15/1e/156ee98c0ad5763a082c1f3002d6051e.jpg?wh=1142*596\" alt=\"\"></p><p>在临时索引文件中，我们存储的是单词编号，也就是图中的term_id，而非单词本身。这样做的目的主要是为了节省存储的空间。那这些单词的编号是怎么来的呢？</p><p>给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。</p><p>在这个过程中，我们还需要使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，我们拿分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。</p><p>当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为term_id.bin。</p><p><strong>经过分析阶段，我们得到了两个重要的文件。它们分别是临时索引文件（tmp_index.bin）和单词编号文件（term_id.bin）。</strong></p><h2>索引</h2><p>索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。文字描述比较难理解，我画了一张倒排索引的结构图，你一看就明白。</p><p><img src=\"https://static001.geekbang.org/resource/image/de/34/de1f212bc669312a499bbbf2ee3a3734.jpg?wh=1142*665\" alt=\"\"></p><p>我们刚刚讲到，在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。那如何通过临时索引文件，构建出倒排索引文件呢？这是一个非常典型的算法问题，你可以先自己思考一下，再看我下面的讲解。</p><p>解决这个问题的方法有很多。考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用<strong>多路归并排序</strong>的方法来实现。</p><p>我们先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。我们可以用之前讲到的归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用MapReduce来处理。</p><p>临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。具体的处理过程，我画成了一张图。通过图，你应该更容易理解。</p><p><img src=\"https://static001.geekbang.org/resource/image/c9/e6/c91c960472d88233f60d5d4ce6538ee6.jpg?wh=1142*711\" alt=\"\"></p><p>除了倒排文件之外，我们还需要一个文件，来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为term_offset.bin。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。</p><p><img src=\"https://static001.geekbang.org/resource/image/de/54/deb2fd01ea6f7e1df9da1ad3a8da5854.jpg?wh=1142*553\" alt=\"\"></p><p><strong>经过索引阶段的处理，我们得到了两个有价值的文件，它们分别是倒排索引文件（index.bin）和记录单词编号在索引文件中的偏移位置的文件（term_offset.bin）。</strong></p><h2>查询</h2><p>前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。</p><ul>\n<li>\n<p>doc_id.bin：记录网页链接和编号之间的对应关系。</p>\n</li>\n<li>\n<p>term_id.bin：记录单词和编号之间的对应关系。</p>\n</li>\n<li>\n<p>index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。</p>\n</li>\n<li>\n<p>term_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。</p>\n</li>\n</ul><p>这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。</p><p>当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分词之后，我们得到k个单词。</p><p>我们拿这k个单词，去term_id.bin对应的散列表中，查找对应的单词编号。经过这个查询之后，我们得到了这k个单词对应的单词编号。</p><p>我们拿这k个单词编号，去term_offset.bin对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。经过这个查询之后，我们得到了k个偏移位置。</p><p>我们拿这k个偏移位置，去倒排索引（index.bin）中，查找k个单词对应的包含它的网页编号列表。经过这一步查询之后，我们得到了k个网页编号列表。</p><p>我们针对这k个网页编号列表，统计每个网页编号出现的次数。具体到实现层面，我们可以借助散列表来进行统计。统计得到的结果，我们按照出现次数的多少，从小到大排序。出现次数越多，说明包含越多的用户查询单词（用户输入的搜索文本，经过分词之后的单词）。</p><p>经过这一系列查询，我们就得到了一组排好序的网页编号。我们拿着网页编号，去doc_id.bin文件中查找对应的网页链接，分页显示给用户就可以了。</p><h2>总结引申</h2><p>今天，我给你展示了一个小型搜索引擎的设计思路。这只是一个搜索引擎设计的基本原理，有很多优化、细节我们并未涉及，比如计算网页权重的<a href=\"https://zh.wikipedia.org/wiki/PageRank\">PageRank</a>算法、计算查询结果排名的<a href=\"https://zh.wikipedia.org/wiki/Tf-idf\">tf</a><a href=\"https://zh.wikipedia.org/wiki/Tf-idf\">-</a><a href=\"https://zh.wikipedia.org/wiki/Tf-idf\">idf</a>模型等等。</p><p>在讲解的过程中，我们涉及的数据结构和算法有：图、散列表、Trie树、布隆过滤器、单模式字符串匹配算法、AC自动机、广度优先遍历、归并排序等。如果对其中哪些内容不清楚，你可以回到对应的章节进行复习。</p><p>最后，如果有时间的话，我强烈建议你，按照我的思路，自己写代码实现一个简单的搜索引擎。这样写出来的，即便只是一个demo，但对于你深入理解数据结构和算法，也是很有帮助的。</p><h2>课后思考</h2><ol>\n<li>\n<p>图的遍历方法有两种，深度优先和广度优先。我们讲到，搜索引擎中的爬虫是通过广度优先策略来爬取网页的。搜索引擎为什么选择广度优先策略，而不是深度优先策略呢？</p>\n</li>\n<li>\n<p>大部分搜索引擎在结果显示的时候，都支持摘要信息和网页快照。实际上，你只需要对我今天讲的设计思路，稍加改造，就可以支持这两项功能。你知道如何改造吗？</p>\n</li>\n</ol><p>欢迎留言和我分享，也欢迎点击“<span class=\"orange\">请朋友读</span>”，把今天的内容分享给你的好友，和他一起讨论、学习。</p>","comments":[{"had_liked":false,"id":113929,"user_name":"feifei","can_delete":false,"product_type":"c1","uid":1105431,"ip_address":"","ucode":"B1F8AE3AD82C51","user_header":"https://static001.geekbang.org/account/avatar/00/10/de/17/75e2b624.jpg","comment_is_top":false,"comment_ctime":1563187175,"is_pvip":false,"discussion_count":13,"race_medal":0,"score":"903506319335","product_id":100017301,"comment_content":"感谢争哥的分享，我按照你这个思路，使用java语言，将这个搜索引擎的代码实现了出来，现在我也分享给大家，希望对那些希望实现搜索引擎，遇到了问题，却又不知道如何解决的童鞋，有所帮助，我的github地址: https:&#47;&#47;github.com&#47;kkzfl22&#47;searchEngine.git","like_count":211,"discussions":[{"author":{"id":1204353,"avatar":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","nickname":"拉布拉多","note":"","ucode":"637A88D9F29F57","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333015,"discussion_content":"大佬都是人狠话不多+1。根据这么一页原理，就能实现出来。","likes_number":7,"is_delete":false,"is_hidden":false,"ctime":1607417043,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2017616,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/c9/50/5bd53d78.jpg","nickname":"Even He","note":"","ucode":"B665039EC1112C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375092,"discussion_content":"请收下我的膝盖","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1621480586,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1690772,"avatar":"https://static001.geekbang.org/account/avatar/00/19/cc/94/2381f962.jpg","nickname":"月下独酌","note":"","ucode":"C6DE5808ED31C6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":367218,"discussion_content":"大佬都是人狠话不多,佩服","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1618297235,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1020525,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/92/6d/becd841a.jpg","nickname":"escray","note":"","ucode":"1F4204930E47C4","race_medal":2,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":279892,"discussion_content":"这个实在是太偶像了，想了解一下大概花了多长时间完成了搜索引擎的 demo？\n\n另外，之前对搜索引擎有研究么？只靠专栏的文章写出来的么？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1591445813,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1101427,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ce/73/cded8343.jpg","nickname":"believe me","note":"","ucode":"7BF9A3C7BF6A9B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":560460,"discussion_content":"1","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649337473,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2921012,"avatar":"https://static001.geekbang.org/account/avatar/00/2c/92/34/cdff39d2.jpg","nickname":"顺子","note":"","ucode":"8E409D2B0467C6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552839,"discussion_content":"牛\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645608713,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1027207,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ac/87/8ed5880a.jpg","nickname":"大碗","note":"","ucode":"F9CDC0C5BE48AC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":540227,"discussion_content":"非常厉害，不仅有测试，代码的设计也很优雅","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639994393,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1238166,"avatar":"https://static001.geekbang.org/account/avatar/00/12/e4/96/a5d775e9.jpg","nickname":"牧凉","note":"","ucode":"1F57A16E37C668","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":386136,"discussion_content":"请收下我的膝盖","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627438598,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1573369,"avatar":"https://static001.geekbang.org/account/avatar/00/18/01/f9/85facaca.jpg","nickname":"出招吧","note":"","ucode":"4B1C9796BF01AB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":306075,"discussion_content":"大佬都是人狠话不多","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600162069,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1055854,"avatar":"https://static001.geekbang.org/account/avatar/00/10/1c/6e/6c5f5734.jpg","nickname":"终结者999号","note":"","ucode":"33ADE61580B6DD","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":300702,"discussion_content":"真的很厉害，看看","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1598238982,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1205485,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJUhJakYu4BI7eFnheKDdibDjZqz32ia2rhN0Jz5YoR1ZRlDrLcFNr4MJnPg3WiaxaocWotOANeqsBibw/132","nickname":"小白","note":"","ucode":"862EA133563634","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262546,"discussion_content":"赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589111416,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":3,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250879,"discussion_content":"mark","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588047316,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":3,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1205485,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJUhJakYu4BI7eFnheKDdibDjZqz32ia2rhN0Jz5YoR1ZRlDrLcFNr4MJnPg3WiaxaocWotOANeqsBibw/132","nickname":"小白","note":"","ucode":"862EA133563634","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262545,"discussion_content":"赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589111406,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250879,"ip_address":""},"score":262545,"extra":""}]}]},{"had_liked":false,"id":64030,"user_name":"天凉好个秋","can_delete":false,"product_type":"c1","uid":1136145,"ip_address":"","ucode":"E11B5F990A4000","user_header":"https://static001.geekbang.org/account/avatar/00/11/56/11/5d113d5c.jpg","comment_is_top":false,"comment_ctime":1548644180,"is_pvip":false,"replies":[{"id":"22762","content":"正排-》文档包含哪些单词<br>倒排-》单词被哪些文档包含","user_name":"作者回复","comment_id":64030,"uid":"1190123","ip_address":"","utype":1,"ctime":1548732136,"user_name_real":"gg"}],"discussion_count":4,"race_medal":0,"score":"456815177556","product_id":100017301,"comment_content":"倒排索引中记录了每个单词以及包含它的网页列表，想问一下“倒排索引”这个名字是怎么来的？其中的“倒排”体现在哪里呢？","like_count":107,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437819,"discussion_content":"正排-》文档包含哪些单词\n倒排-》单词被哪些文档包含","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1548732136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2052521,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/51/a9/741b5ad9.jpg","nickname":"FD","note":"","ucode":"ED396BD322489E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":357976,"discussion_content":"类似邻接表和逆邻接表","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615901767,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1267308,"avatar":"https://static001.geekbang.org/account/avatar/00/13/56/6c/0e19dcbf.jpg","nickname":"jss","note":"","ucode":"DB0D2CA916EF17","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":328404,"discussion_content":"感觉翻译成 反向索引 更好一些","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606135224,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1451364,"avatar":"https://static001.geekbang.org/account/avatar/00/16/25/64/d66ea739.jpg","nickname":"黑洞","note":"","ucode":"840A4330B1B2F0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":286731,"discussion_content":"因为搜索引擎的爬虫的目的是尽可能快速地爬取到尽可能多的网页，而不是下载最远的那个网页。离种子网页更近的网页权重更大。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593269862,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":63953,"user_name":"wei","can_delete":false,"product_type":"c1","uid":1013347,"ip_address":"","ucode":"345682FEFD1A90","user_header":"","comment_is_top":false,"comment_ctime":1548612447,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"366620832607","product_id":100017301,"comment_content":"思考题 1:<br><br>因为搜索引擎要优先爬取权重较高的页面，离种子网页越近，较大可能权重更高，广度优先更合适。<br><br>思考题 2:<br><br>摘要信息：<br>增加 summary.bin 和 summary_offset.bin。在抽取网页文本信息后，取出前 80-160 个字作为摘要，写入到 summary.bin，并将偏移位置写入到 summary_offset.bin。<br>summary.bin 格式：<br>doc_id \\t summary_size \\t summary \\r\\n\\r\\n<br>summary_offset.bin 格式：<br>doc_id \\t offset \\r\\n<br>Google 搜索结果中显示的摘要是搜索词附近的文本。如果要实现这种效果，可以保存全部网页文本，构建搜索结果时，在网页文本中查找搜索词位置，截取搜索词附近文本。<br><br>网页快照：<br>可以把 doc_raw.bin 当作快照，增加 doc_raw_offset.bin 记录 doc_id 在 doc_raw.bin 中的偏移位置。<br>doc_raw_offset.bin 格式：<br>doc_id \\t offset \\r\\n","like_count":86,"discussions":[{"author":{"id":1204353,"avatar":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","nickname":"拉布拉多","note":"","ucode":"637A88D9F29F57","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333347,"discussion_content":"赞！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607505744,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64188,"user_name":"纯洁的憎恶","can_delete":false,"product_type":"c1","uid":1130512,"ip_address":"","ucode":"5E9757DE6F45DF","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/10/b6bf3c3c.jpg","comment_is_top":false,"comment_ctime":1548689150,"is_pvip":true,"replies":[{"id":"24190","content":"赞","user_name":"作者回复","comment_id":64188,"uid":"1190123","ip_address":"","utype":1,"ctime":1550480732,"user_name_real":"gg"}],"discussion_count":2,"race_medal":0,"score":"143282609918","product_id":100017301,"comment_content":"搜集：将广度优先搜索的优先队列存储在磁盘文件links.bin（如何解析网页内的链接？），有布隆过滤器判重并定期写入磁盘文件bloom_filter.bin，将访问到的原始网页数据存入磁盘文件doc_raw.bin，计数分配网页编号并与其链接对应关系存入磁盘文件doc_id.bin。<br><br>分析：首先抽取网页文本信息，依据HTML语法规范，通过AC自动机多模式串匹配算法，去除网页中格式化部分，提取文本内容。然后分词并创建临时索引，分词的目的是找到能够标识网页文本“身份”的特征，可借助词库（通过Trie树实现）搜索文本中与词库匹配的最长词语，因为一般情况下越长信息越多，越剧有表征能力（为什么英文简单？）。分词完成后得到一组用于表征网页的单词列表，与其对应的网页编号存入磁盘文件tmp_index.bin作为临时索引，为节省空间单词是以单词编号的形式写入，单词文本与编号的对应关系写入磁盘文本term_id.bin。<br><br>索引：通过临时索引构建倒排索引文件index.bin。倒排索引其实是以单词为主键，将临时索引中的多个相同单词行合并为一行。通过以单词为主键的排序算法，可以将相同单词的行连续排列在一起，之后只要将单词相同的连续行合并为一行即可。由于数据量大，应采用分治策略。最后建立所有单词在倒排索引文件中位置的索引文件term_offset.bin，以方便快速查找。<br><br>查询：先对搜索条件文本做分词处理，然后去term_id.bin查单词们的编号，再查term_offset.bin找到单词们在倒排索引中的位置，到index.bin找到每个单词对应的网页编号，通过网页出现次数、预评权重和统计算法（如pagerank、tf-idf）计算网页的优先次序并输出。最后在doc_in.bin中找到网页链接按序输出显示给用户。<br><br>这样理解对不？","like_count":34,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437897,"discussion_content":"赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550480732,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1204353,"avatar":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","nickname":"拉布拉多","note":"","ucode":"637A88D9F29F57","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333358,"discussion_content":"赞！自我总结的非常好，这表达能力赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607506523,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128317,"user_name":"Leon📷","can_delete":false,"product_type":"c1","uid":1219496,"ip_address":"","ucode":"B9BBD1EFAAE5A2","user_header":"https://static001.geekbang.org/account/avatar/00/12/9b/a8/6a391c66.jpg","comment_is_top":false,"comment_ctime":1566870741,"is_pvip":false,"discussion_count":4,"race_medal":0,"score":"139005824213","product_id":100017301,"comment_content":"毕业设计就是做的搜索引擎，十万个本地文档构建的倒排索引，不过我的倒排索引直接用单词了，没有编号，用开源库分词，实现了tf-idf和文档之间相似度的计算，用动态规划来实现文本纠错，可以纠正用户的搜索框的错误输入，用到的数据结构不多，主要是哈希表和vector，用内存缓存查询结果，不知道算不算快照，哈，离老师讲的似乎只有分布式爬虫和临时索引的合并没有实现，<br>https:&#47;&#47;github.com&#47;chawlau&#47;search_engine，其他人看了不要喷我","like_count":33,"discussions":[{"author":{"id":2068721,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/f1/7f2b5e16.jpg","nickname":"CHN-Lee-玉米","note":"","ucode":"0A53080F38F229","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":312207,"discussion_content":"快照是指网页页面的备份","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1602611832,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1202482,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/ydFhHonicUQibGlAfsAYBibNOfSxpCG5cJNp9oRibTJm3TrxM7Hj4WPPCRE3vluZJb0TGQqpKCaBWLdmra5Su1KF5Q/132","nickname":"yudidi","note":"","ucode":"70283DE39D86F5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":411787,"discussion_content":"动态规划纠错，是编辑距离吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636005690,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1655940,"avatar":"https://static001.geekbang.org/account/avatar/00/19/44/84/4da14994.jpg","nickname":"呆瓜","note":"","ucode":"C98C7B224D0640","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":305369,"discussion_content":"������������","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599895709,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1305639,"avatar":"https://static001.geekbang.org/account/avatar/00/13/ec/27/827015c0.jpg","nickname":"追风少年","note":"","ucode":"0467648A908B18","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":278702,"discussion_content":"本科的毕设吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591228897,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":143240,"user_name":"steve","can_delete":false,"product_type":"c1","uid":1354489,"ip_address":"","ucode":"AB6D3E9FF8E77C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJETibDh9wrP19gj9VdlLRmppuG1FibI7nyUGldEXCnoqKibKIB18UMxyEHBkZNlf5vibLNeofiaN5U6Hw/132","comment_is_top":false,"comment_ctime":1571654357,"is_pvip":false,"replies":[{"id":"56000","content":"我写过一个5万行的搜索引擎，cpp实现的，还有对应的几十页的文档，等过一整子整理一下放到公号众里：小争哥","user_name":"作者回复","comment_id":143240,"uid":"1190123","ip_address":"","utype":1,"ctime":1572231536,"user_name_real":"王争"}],"discussion_count":5,"race_medal":0,"score":"113240804053","product_id":100017301,"comment_content":"老师好 看了这篇之后我也想实现一个搜索引擎 现在很多公司里应该都用的cpp吧 我也想用cpp实现一个 请问下有没有可参考的代码 怕写到一半写不下去😂","like_count":26,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":471444,"discussion_content":"我写过一个5万行的搜索引擎，cpp实现的，还有对应的几十页的文档，等过一整子整理一下放到公号众里：小争哥","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1572231536,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1741227,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/91/ab/d787992d.jpg","nickname":"他年得志","note":"","ucode":"9A9659C2F88CE7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":294692,"discussion_content":"请问有cpp源代码吗,我也想学习一下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595966146,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1665472,"avatar":"https://static001.geekbang.org/account/avatar/00/19/69/c0/bba847d0.jpg","nickname":"聪","note":"","ucode":"E0D4D3FE980534","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":281537,"discussion_content":"老师，请问有时间整理这个代码吗？还是很期待的呀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591761296,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1736297,"avatar":"https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI9zRdkKuXMKh30ibeludlAsztmR4rD9iaiclPicOfIhbC4fWxGPz7iceb3o4hKx7qgX2dKwogYvT6VQ0g/132","nickname":"Initiative Thinker","note":"","ucode":"A884396A1581EF","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1665472,"avatar":"https://static001.geekbang.org/account/avatar/00/19/69/c0/bba847d0.jpg","nickname":"聪","note":"","ucode":"E0D4D3FE980534","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":294255,"discussion_content":"好像没有找到，害","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595841121,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":281537,"ip_address":""},"score":294255,"extra":""}]},{"author":{"id":1082785,"avatar":"https://static001.geekbang.org/account/avatar/00/10/85/a1/2442332c.jpg","nickname":"郭俊杰","note":"","ucode":"D328E5738A4413","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250881,"discussion_content":"厉害","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588047741,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":63991,"user_name":"『LHCY』","can_delete":false,"product_type":"c1","uid":1188449,"ip_address":"","ucode":"A8B5E0467B5F25","user_header":"https://static001.geekbang.org/account/avatar/00/12/22/61/bbfb2d4a.jpg","comment_is_top":false,"comment_ctime":1548638018,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"113217787714","product_id":100017301,"comment_content":"作者讲的基本和elasticsearch原理查不多，可见有了算法基础以后了解一些中间件原理会容易很多，我最开始看es原理时一脸懵逼。","like_count":27},{"had_liked":false,"id":64468,"user_name":"Jerry银银","can_delete":false,"product_type":"c1","uid":1008404,"ip_address":"","ucode":"80DA1172A2360A","user_header":"https://static001.geekbang.org/account/avatar/00/0f/63/14/06eff9a4.jpg","comment_is_top":false,"comment_ctime":1548808266,"is_pvip":false,"discussion_count":4,"race_medal":0,"score":"61678350410","product_id":100017301,"comment_content":"经过深入研究了一把，第一题终于有了比较清晰的答案：<br>从时间复杂度这个维度来考虑，BFS和DFS爬取互联网上所有的内容所需的时间是一样的。但是，我们设计爬虫系统的时候，不可能想着一次性爬完所有的网页，因为「量」太大了。所以，必须有一个优先级，不难想到：每一个网站的首页优先级最高，所以，我们肯定要先爬取每个网站的首页。从这一点出发，我们肯定要选取BFS。<br>但是，这里还有另外一个问题：如果我们爬完一个网站的首页之后，再爬取另外一个网站的首页，每次和不同网站服务器都要建立网络连接(TCP三次握手、HTTPs网站还要建立SSL握手等）都要花费大量的时间。如果总是按照BFS的策略来爬取，这中间花费的时间成本又太大了。所以，我想，中间肯定也是需要用DFS的。<br>我想到，可以使用一个优先级队列来维护需要爬取的网页。剩下的问题就是：该如何评估所需要爬取的网页的优先级呢？ 这个问题想了很久，依然不知道该如何计算机网页的优先级，难道这里也用PageRank类似的算法？","like_count":15,"discussions":[{"author":{"id":1219496,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9b/a8/6a391c66.jpg","nickname":"Leon📷","note":"","ucode":"B9BBD1EFAAE5A2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6367,"discussion_content":"tf-idf算法，根据现在频率和搜索次数等因素来计算的，现在应该有人工智能算法来评估了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1566870978,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1204353,"avatar":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","nickname":"拉布拉多","note":"","ucode":"637A88D9F29F57","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333360,"discussion_content":"同感。我感觉是用域名对应的递归深度。即从首页开始往下递归搜索了多少层，比如200层。就换一个域名继续BFS。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607506680,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1844270,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJrZb9pm07aiciaNalQ0gRgMub2In4RQHYSd7VI2gvsTQMC1wSsYfcxxCYCeBmrs6Vic7GRf9jtNRqyA/132","nickname":"壮壮.java","note":"","ucode":"755C8C8A823B70","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":289960,"discussion_content":"dfs就不用三次握手了吗、、、、、、\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594282338,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1008404,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/63/14/06eff9a4.jpg","nickname":"Jerry银银","note":"","ucode":"80DA1172A2360A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1844270,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJrZb9pm07aiciaNalQ0gRgMub2In4RQHYSd7VI2gvsTQMC1wSsYfcxxCYCeBmrs6Vic7GRf9jtNRqyA/132","nickname":"壮壮.java","note":"","ucode":"755C8C8A823B70","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":292607,"discussion_content":"第一次当然要用…","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595267588,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":289960,"ip_address":""},"score":292607,"extra":""}]}]},{"had_liked":false,"id":142131,"user_name":"嘉一","can_delete":false,"product_type":"c1","uid":1196864,"ip_address":"","ucode":"8D16BD0B75B019","user_header":"https://static001.geekbang.org/account/avatar/00/12/43/40/e7ef18de.jpg","comment_is_top":false,"comment_ctime":1571298487,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"48815938743","product_id":100017301,"comment_content":"不得了，我要写搜索引擎了！","like_count":11,"discussions":[{"author":{"id":1397351,"avatar":"https://static001.geekbang.org/account/avatar/00/15/52/67/fcba0967.jpg","nickname":"zapup","note":"","ucode":"388D6BB5D7B137","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":239453,"discussion_content":"飘了飘了哈哈，支持你","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587300479,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2249227,"avatar":"https://static001.geekbang.org/account/avatar/00/22/52/0b/50bf0f05.jpg","nickname":"橙子橙","note":"","ucode":"CD51367A14D955","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":325729,"discussion_content":"醒醒","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1605416090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":76028,"user_name":"醉比","can_delete":false,"product_type":"c1","uid":1248399,"ip_address":"","ucode":"79E65F62BE7809","user_header":"https://static001.geekbang.org/account/avatar/00/13/0c/8f/4ebd303a.jpg","comment_is_top":false,"comment_ctime":1552524875,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"35912263243","product_id":100017301,"comment_content":"王老师，很惭愧在前一阵子落下了这门课程，平心而论您的课程真的是太优秀了，从我的角度来说真的极大地提升的见世面与知识基础。虽然停滞了很长一段时间没有学习，但我很相信这门课程是可以陪伴我很久然后学习两遍到三遍的，已经关注老师的公众号， 希望继续产出高质量的内容，祝好~","like_count":8,"discussions":[{"author":{"id":1202325,"avatar":"https://static001.geekbang.org/account/avatar/00/12/58/95/640b6465.jpg","nickname":"fmouse","note":"","ucode":"9A8858CFFAB858","race_medal":2,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":279866,"discussion_content":"老师的公众号是多少，谢谢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591437384,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":176956,"user_name":"注定非凡","can_delete":false,"product_type":"c1","uid":1113597,"ip_address":"","ucode":"80673056E131B7","user_header":"https://static001.geekbang.org/account/avatar/00/10/fd/fd/326be9bb.jpg","comment_is_top":false,"comment_ctime":1581235841,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"23056072321","product_id":100017301,"comment_content":"整体系统介绍搜索引擎可以分为四个部分：搜集、分析、索引、查询。<br>\t* 搜集：就是利用爬虫爬取网页<br>\t* 分析：主要负责网页内容抽取、分词，构建临时索引，计算 PageRank 值这几部分工作<br>\t* 索引：主要负责通过分析阶段得到的临时索引，构建倒排索引<br>\t* 查询：主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户<br><br>搜集<br>\t* 搜索引擎把整个互联网看作数据结构中的有向图，把每个页面看作一个顶点。如果某个页面中包含另外一个页面的链接，就在两个顶点之间连一条有向边。可以利用图的遍历搜索算法，来遍历整个互联网中的网页。<br>\t* 搜索引擎采用的是广度优先搜索策略。<br>\t* 权重比较高的链接，作为种子网页链接，放入到队列中<br>\t* 爬虫按照广度优先的策略，不停地从队列中取出链接，取爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。<br><br>1. 待爬取网页链接文件：links.bin<br>\t* 在爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中<br>\t* 队列中的链接存储在磁盘中的文件（links.bin）。爬虫从 links.bin 文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到 links.bin 文件中。<br>\t* 文件来存储支持断点续爬<br><br>2. 网页判重文件：bloom_filter.bin<br>\t* 使用布隆过滤器，可以快速且非常节省内存地实现网页的判重，避免重复爬取相同的网页<br>\t* 要定期地将布隆过滤器持久化存储到磁盘bloom_filter.bin 文件中，避免机器重启后，布隆过滤器被清空<br>\t* 当机器重启之后，重新读取磁盘中的 bloom_filter.bin 文件，将其恢复到内存中<br><br>3. 原始网页存储文件：doc_raw.bin<br>\t* 爬取到网页之后，需要将其存储下来，以备后面离线分析、索引之用<br>\t* 把多个网页存储在一个文件中，每个网页之间，通过一定的标识进行分隔，方便后续读取<br>\t* 每个文件的大小不能超过一定的值（比如 1GB），因为文件系统对文件的大小也有限制。<br><br>4. 网页链接及其编号的对应文件：doc_id.bin<br>\t* 网页编号是给每个网页分配一个唯一的 ID，方便后续对网页进行分析、索引<br>\t* 每爬取到一个网页之后，就从一个中心的计数器中拿一个号码，分配给这个网页<br>\t* 在存储网页的同时，将网页链接跟编号之间的对应关系，存储在一个 doc_id.bin 文件中<br><br>分析<br>分析阶段两个步骤（1）抽取网页文本信息，（2）分词并创建临时索引<br>1. 抽取网页文本信息<br>\t* 网页是半结构化数据，要从半结构化的网页中，抽取出搜索引擎关系的文本信息<br><br>这个抽取的过程分为两步<br>\t\t（1）去掉 JavaScript 代码、CSS 格式以及下拉框中的内容<br>\t\t\t* 利用 AC 自动机这种多模式串匹配算法将&lt;style&gt;, &lt;script&gt;, &lt;option&gt;标签包裹的字符删除\t<br>\t\t （2）去掉所有 HTML 标签，这过程跟第一步类似<br>2. 分词并创建临时索引<br>\t* 从网页中抽取出了文本信息，要对文本信息进行分词，并且创建临时索引<br>\t* 中文分词比较复杂太多了，可以基于字典和规则的分词方法<br><br><br>字典也叫词库，里面包含大量常用的词语。借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。<br><br>\t* 具体到实现层面，将词库中的单词，构建成 Trie 树结构，然后拿网页文本在 Trie 树中匹配<br>\t* 网页的文本信息在分词完成后，得到一组单词列表。把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件<br><br>         *临时索引文件中存储的是单词编号，这样做是为了节省存储的空间。给单词编号的方式，跟给网页编号类似<br>         *这个过程中使用散列表记录已经编过号的单词。对分词的先到散列表中查找，如果找到那就直接使用；如果没有找到，再去计数器中拿号码，并将这个新单词以及编号添加到散列表中<br><br>当所有的网页处理（分词及写入临时索引）完成之后，再将单词跟编号之间的对应关系写入到磁盘文件中，并命名为 term_id.bin<br><br>索引<br>索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引<br>倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表<br><br>那如何通过临时索引文件，构建出倒排索引文件？<br>考虑到临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用多路归并排序的方法来实现<br>先对临时索引文件，按照单词编号的大小进行排序<br>因为临时索引很大，可以用归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起","like_count":5,"discussions":[{"author":{"id":1243570,"avatar":"https://static001.geekbang.org/account/avatar/00/12/f9/b2/2ed800b4.jpg","nickname":"社会你强哥","note":"","ucode":"325B9005588D9B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263440,"discussion_content":"具体到实现层面，将词库中的单词，构建成 Trie 树结构，然后拿网页文本在 Trie 树中匹配。这个是使用AC自动机么？还是网页文本从头开始跟Trie树中的单词一个字符接一个字符比对？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589207038,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64016,"user_name":"alic","can_delete":false,"product_type":"c1","uid":1240077,"ip_address":"","ucode":"47E9D22030CA01","user_header":"https://static001.geekbang.org/account/avatar/00/12/ec/0d/43d46889.jpg","comment_is_top":false,"comment_ctime":1548641586,"is_pvip":false,"replies":[{"id":"22764","content":"木有。等我有空了可以写下分享出来。","user_name":"作者回复","comment_id":64016,"uid":"1190123","ip_address":"","utype":1,"ctime":1548732207,"user_name_real":"gg"}],"discussion_count":1,"race_medal":0,"score":"23023478066","product_id":100017301,"comment_content":"有没有代码实现的例子？","like_count":5,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437812,"discussion_content":"木有。等我有空了可以写下分享出来。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1548732207,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":191626,"user_name":"拉环","can_delete":false,"product_type":"c1","uid":1247277,"ip_address":"","ucode":"5F81F2746CEF61","user_header":"https://static001.geekbang.org/account/avatar/00/13/08/2d/fb0831a9.jpg","comment_is_top":false,"comment_ctime":1584785168,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14469687056","product_id":100017301,"comment_content":"按照这里的思路，我使用java语言，将其实现了，现在我把这些代码上传到GitHub，希望能帮助大家，如果大家有发现什么问题，也欢迎大家来找我^_^<br>https:&#47;&#47;github.com&#47;la-huan&#47;small_search_engine<br>qq:851127936","like_count":4},{"had_liked":false,"id":157796,"user_name":"CHON","can_delete":false,"product_type":"c1","uid":1439409,"ip_address":"","ucode":"9349119267172E","user_header":"https://static001.geekbang.org/account/avatar/00/15/f6/b1/a073d108.jpg","comment_is_top":false,"comment_ctime":1575260163,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"14460162051","product_id":100017301,"comment_content":"‘带宽是 10MB，那下载 100GB 的网页，大约需要 10000 秒’<br>要是这样采集，十分钟之后就被封IP了。之前做爬虫都是采集一个页面休眠3-5秒，再采集下一个页面","like_count":4,"discussions":[{"author":{"id":1976284,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/aM94SrGBJ0iaOZfQ1UiaVgTYN4cDAZiaGicHpLcjfXsiaDzibFjeHOicP4Nfyexk6Mm19dvaW4CByxVPlLjoPjicpEWlnQ/132","nickname":"大聪明","note":"","ucode":"6FE999C07F214A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":284239,"discussion_content":"cookies 池 ip 池  接码平台 各种策略缺一不可","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592481975,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1127799,"avatar":"https://static001.geekbang.org/account/avatar/00/11/35/77/95e95b32.jpg","nickname":"木杉","note":"","ucode":"85651CBDDF7EDC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":278123,"discussion_content":"所以说 你需要一个ip地址池","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591152929,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":134329,"user_name":"Billy","can_delete":false,"product_type":"c1","uid":1528129,"ip_address":"","ucode":"1AB0F8B9DB4647","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKKAtGwcfB0JZiaZrvhLQ0SMCPU1gAqt4jZ06J9f6THdWaKdKMnRkgJgeYa4eclRHfggYDBnOpEzkQ/132","comment_is_top":false,"comment_ctime":1568808095,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10158742687","product_id":100017301,"comment_content":"这是我写的一个轻量级的搜索引擎,https:&#47;&#47;github.com&#47;stdbilly&#47;RssSearchEnigine","like_count":3},{"had_liked":false,"id":119767,"user_name":"ub8","can_delete":false,"product_type":"c1","uid":1481811,"ip_address":"","ucode":"0D937C3EAEB781","user_header":"https://static001.geekbang.org/account/avatar/00/16/9c/53/ade0afb0.jpg","comment_is_top":false,"comment_ctime":1564676627,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10154611219","product_id":100017301,"comment_content":"elasticsearch","like_count":2},{"had_liked":false,"id":64403,"user_name":"往事随风，顺其自然","can_delete":false,"product_type":"c1","uid":1235692,"ip_address":"","ucode":"F266EC6B143E38","user_header":"https://static001.geekbang.org/account/avatar/00/12/da/ec/779c1a78.jpg","comment_is_top":false,"comment_ctime":1548768881,"is_pvip":false,"replies":[{"id":"24180","content":"啥事排序索引和普通索引呢？我文中好像没讲到呢","user_name":"作者回复","comment_id":64403,"uid":"1190123","ip_address":"","utype":1,"ctime":1550480043,"user_name_real":"gg"}],"discussion_count":1,"race_medal":0,"score":"10138703473","product_id":100017301,"comment_content":"可以讲讲到排序索引和普通索引区别？","like_count":2,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437985,"discussion_content":"啥事排序索引和普通索引呢？我文中好像没讲到呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550480043,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64298,"user_name":"miss","can_delete":false,"product_type":"c1","uid":1256839,"ip_address":"","ucode":"07831674314ECF","user_header":"https://static001.geekbang.org/account/avatar/00/13/2d/87/aadb394b.jpg","comment_is_top":false,"comment_ctime":1548737581,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10138672173","product_id":100017301,"comment_content":"问题1， 爬取网页时，如果采用深度优先算法，很有可能导致，栈溢出的现象把，所以一般不用深度优先算法","like_count":2},{"had_liked":false,"id":266838,"user_name":"拉布拉多","can_delete":false,"product_type":"c1","uid":1204353,"ip_address":"","ucode":"637A88D9F29F57","user_header":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","comment_is_top":false,"comment_ctime":1607502317,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5902469613","product_id":100017301,"comment_content":"搜索引擎，果然是技术流，一个小小的引擎，把这门课的主流算法涉及到了一半。","like_count":2},{"had_liked":false,"id":230107,"user_name":"阿尔卑斯","can_delete":false,"product_type":"c1","uid":1244998,"ip_address":"","ucode":"0C9C026D02002D","user_header":"https://static001.geekbang.org/account/avatar/00/12/ff/46/2ea2fe90.jpg","comment_is_top":false,"comment_ctime":1593263658,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5888230954","product_id":100017301,"comment_content":"顶礼膜拜的王老师，不知你还是否关注专栏<br>你说之前有实现过cpp版本搜索引擎，代码量5W+，还有几十页文档<br>请问你分享了吗？哪里？好想学习学习😄","like_count":1},{"had_liked":false,"id":221394,"user_name":"默默","can_delete":false,"product_type":"c1","uid":1006635,"ip_address":"","ucode":"AF798B9D327F24","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5c/2b/8b771383.jpg","comment_is_top":false,"comment_ctime":1590490469,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5885457765","product_id":100017301,"comment_content":"问题一：采用广度优先算法是因为。从种子网页出发，由种子网页指向的网页一般都是质量比较高的网页。这样的遍历方式有助于将质量高的网页优先爬取下来。另外广度遍历应该对分布式支持更友好。<br>问题二：构建summary.bin文件，保存网页id与其概要信息。构建summary_offset.bin，保存网页id在summary文件中的偏移量。","like_count":2},{"had_liked":false,"id":122315,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1565337228,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5860304524","product_id":100017301,"comment_content":"思考题2<br>网页的快照和摘要可以按照docId 存起来，查找的时候一块找。","like_count":1},{"had_liked":false,"id":86788,"user_name":"Zhangxuesong","can_delete":false,"product_type":"c1","uid":1396503,"ip_address":"","ucode":"F5C4C3AC9429AD","user_header":"https://static001.geekbang.org/account/avatar/00/15/4f/17/2185685f.jpg","comment_is_top":false,"comment_ctime":1555461390,"is_pvip":false,"replies":[{"id":"31221","content":"编辑，麻烦修改下错别字，多谢。","user_name":"作者回复","user_name_real":"王争","uid":"1190123","ctime":1555468993,"ip_address":"","comment_id":86788,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850428686","product_id":100017301,"comment_content":"爬虫按照广度优先的策略，不停地从队列中取出链接，然后“取“  -&gt;  “去“  爬取对应的网页，解析出网页里包含的其他网页链接，再将解析出来的链接添加到队列中。<br>上面这段话里面有个词不是很通， 不知道后面还能修正么。","like_count":1,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447171,"discussion_content":"编辑，麻烦修改下错别字，多谢。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555468993,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":73529,"user_name":"超威丶","can_delete":false,"product_type":"c1","uid":1180753,"ip_address":"","ucode":"1A18DE885D3C44","user_header":"https://static001.geekbang.org/account/avatar/00/12/04/51/da465a93.jpg","comment_is_top":false,"comment_ctime":1551920862,"is_pvip":false,"replies":[{"id":"26790","content":"是的，两个东西的应用场景不大一样的。","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1551924042,"ip_address":"","comment_id":73529,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5846888158","product_id":100017301,"comment_content":"请问倒排索引这种结构比b树快是不是依赖于它的数据结构优势？","like_count":1,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":442135,"discussion_content":"是的，两个东西的应用场景不大一样的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551924042,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64227,"user_name":"王肖武","can_delete":false,"product_type":"c1","uid":1282385,"ip_address":"","ucode":"561AF05284EBE2","user_header":"https://static001.geekbang.org/account/avatar/00/13/91/51/234f9a73.jpg","comment_is_top":false,"comment_ctime":1548723103,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5843690399","product_id":100017301,"comment_content":"思考题1:深度优化借助栈这种数据结构，网页的深度是不可预测的，如果很深，栈大小会很大，内存可能会爆掉。","like_count":1},{"had_liked":false,"id":64025,"user_name":"蚂蚁内推+v","can_delete":false,"product_type":"c1","uid":1050508,"ip_address":"","ucode":"24B10AEE54B3FD","user_header":"https://static001.geekbang.org/account/avatar/00/10/07/8c/0d886dcc.jpg","comment_is_top":false,"comment_ctime":1548642952,"is_pvip":false,"replies":[{"id":"22763","content":"在这个例子中是的。“中国人好样的”这个句子分词就可以匹配到“中国人”","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1548732178,"ip_address":"","comment_id":64025,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5843610248","product_id":100017301,"comment_content":"王老师，字典使用最长匹配？那例子中的”中国“”中国人“不就无法匹配到了吗 ","like_count":1,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437816,"discussion_content":"在这个例子中是的。“中国人好样的”这个句子分词就可以匹配到“中国人”","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1548732178,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":339764,"user_name":"恬毅","can_delete":false,"product_type":"c1","uid":1015240,"ip_address":"","ucode":"914ED16F3BE714","user_header":"https://static001.geekbang.org/account/avatar/00/0f/7d/c8/e4727683.jpg","comment_is_top":false,"comment_ctime":1648367764,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1648367764","product_id":100017301,"comment_content":"图的遍历方法有两种，深度优先和广度优先。我们讲到，搜索引擎中的爬虫是通过广度优先策略来爬取网页的。搜索引擎为什么选择广度优先策略，而不是深度优先策略呢？<br>越深的链接，应该重要性越低。<br><br>大部分搜索引擎在结果显示的时候，都支持摘要信息和网页快照。实际上，你只需要对我今天讲的设计思路，稍加改造，就可以支持这两项功能。你知道如何改造吗？<br>根据链接的id散列到不同服务器、文件中去存储","like_count":0},{"had_liked":false,"id":315855,"user_name":"拓山","can_delete":false,"product_type":"c1","uid":1545647,"ip_address":"","ucode":"11FE9CF3821898","user_header":"https://static001.geekbang.org/account/avatar/00/17/95/af/b7f8dc43.jpg","comment_is_top":false,"comment_ctime":1634022794,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1634022794","product_id":100017301,"comment_content":"截止到这里，一刷总算是全部完成了，大约花费了我20天时间。<br>后续会开始二刷，加油！","like_count":0},{"had_liked":false,"id":288098,"user_name":"月下独酌","can_delete":false,"product_type":"c1","uid":1690772,"ip_address":"","ucode":"C6DE5808ED31C6","user_header":"https://static001.geekbang.org/account/avatar/00/19/cc/94/2381f962.jpg","comment_is_top":false,"comment_ctime":1618297828,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1618297828","product_id":100017301,"comment_content":"读完此文,深刻体会到了基础数据结构与算法的用途之广,之妙,醍醐灌顶了","like_count":0},{"had_liked":false,"id":285199,"user_name":"陌","can_delete":false,"product_type":"c1","uid":1152678,"ip_address":"","ucode":"13FF1D4B3181F0","user_header":"https://static001.geekbang.org/account/avatar/00/11/96/a6/aac2a550.jpg","comment_is_top":false,"comment_ctime":1616664029,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1616664029","product_id":100017301,"comment_content":"所以，到底该怎么使用 Trie 对拼写进行纠错呢? 比如说 kello =&gt; hello，按我的理解，Trie 只有在前缀匹配的情况下纠错效率比较高。","like_count":0},{"had_liked":false,"id":281145,"user_name":"北极的大企鹅","can_delete":false,"product_type":"c1","uid":1045577,"ip_address":"","ucode":"8935346D08E109","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f4/49/2add4f6b.jpg","comment_is_top":false,"comment_ctime":1614594653,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1614594653","product_id":100017301,"comment_content":"虽然不是很明白，但感觉离着学会这个还有距离","like_count":0},{"had_liked":false,"id":266832,"user_name":"拉布拉多","can_delete":false,"product_type":"c1","uid":1204353,"ip_address":"","ucode":"637A88D9F29F57","user_header":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","comment_is_top":false,"comment_ctime":1607500677,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1607500677","product_id":100017301,"comment_content":"doc_id只是获取到id，还缺少doc_raw.bin没有利用上。应该还需要增加一个docid_raw_offset.bin 记录doc在doc_raw.bin中的offset，这样可以快速读取doc_raw.bin的网页内容。","like_count":0},{"had_liked":false,"id":252397,"user_name":"walle斌","can_delete":false,"product_type":"c1","uid":1062848,"ip_address":"","ucode":"0DB3243004951F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersGSic8ib7OguJv6CJiaXY0s4n9C7Z51sWxTTljklFpq3ZAIWXoFTPV5oLo0GMTkqW5sYJRRnibNqOJQ/132","comment_is_top":false,"comment_ctime":1602290468,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1602290468","product_id":100017301,"comment_content":"—— ——！爬虫那块。。直接把js抹掉也有问题，有些网站内容都是动态加载的，甚至需要游标挪动才会加载内容，不是直接把内容呈现在文本里边的。。直接呈现文本的话 确实是最简单的网站了","like_count":0,"discussions":[{"author":{"id":1035762,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/cd/f2/46e48722.jpg","nickname":"卡卡","note":"","ucode":"ADDD87242DAB73","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":350780,"discussion_content":"所以PWA不适合做SEO，由此前端才产生了SSR","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614003856,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":248514,"user_name":"LearnAndTry","can_delete":false,"product_type":"c1","uid":1512860,"ip_address":"","ucode":"0ECF4398C870DC","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Rr8ABLicfgJHZ3xs3bXNjHdicwVN0zBiaFtIEXw4D1licedSuia3zicxJC4lqtrNwPmCeAW5UR2Ugia0dNfcvtXYnpOwQ/132","comment_is_top":false,"comment_ctime":1600187311,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1600187311","product_id":100017301,"comment_content":"这节课非常棒，不仅把长久以来的疑惑解决了，还串联了以前学习的知识。真是用到实战的算法才是好算法，单纯为面试准备的算法格局有点低了","like_count":0},{"had_liked":false,"id":236939,"user_name":"www","can_delete":false,"product_type":"c1","uid":1898338,"ip_address":"","ucode":"ADC9BC655EA16C","user_header":"https://static001.geekbang.org/account/avatar/00/1c/f7/62/947004d0.jpg","comment_is_top":false,"comment_ctime":1595594639,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1595594639","product_id":100017301,"comment_content":"打卡，用Go写了一遍，跑通了，继续优化中","like_count":0,"discussions":[{"author":{"id":2144755,"avatar":"","nickname":"Geek_ce255a","note":"","ucode":"389D43F67516D4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":347459,"discussion_content":"可否学习一下？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612235282,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":235679,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1595147644,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1595147644","product_id":100017301,"comment_content":"思考题1：用广度优先算法效率更高，用深度优先算法是采用递归算法，网页链接较多的话，内存占用较多，算法效率不高。<br>思考题2：可以用一个单独的文件，存储网页快照和网页摘要，文件中用网页编号和快照信息，摘要信息做对应。","like_count":0},{"had_liked":false,"id":223319,"user_name":"webmin","can_delete":false,"product_type":"c1","uid":1047014,"ip_address":"","ucode":"98B0CA882454E8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f9/e6/47742988.jpg","comment_is_top":false,"comment_ctime":1591061397,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1591061397","product_id":100017301,"comment_content":"课后思考1：<br>因为爬虫种子链接是人工选择的优先级或权重高的链接，又广度优先策略是一层一层向下的，同层的优先级和权重可以认为是一样的，所以距离种子链接层次越近的，优先级或权重越高，按照这个方式就得到的网页就已经自带排序，也优先处理权重高的网页。","like_count":0},{"had_liked":false,"id":216221,"user_name":"社会你强哥","can_delete":false,"product_type":"c1","uid":1243570,"ip_address":"","ucode":"325B9005588D9B","user_header":"https://static001.geekbang.org/account/avatar/00/12/f9/b2/2ed800b4.jpg","comment_is_top":false,"comment_ctime":1589206677,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1589206677","product_id":100017301,"comment_content":"根据次数进行从小到大排列，不是从大到小排列么？","like_count":0},{"had_liked":false,"id":172543,"user_name":"梦想的优惠券","can_delete":false,"product_type":"c1","uid":1257418,"ip_address":"","ucode":"D3B44F6C618CA7","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/ca/cbce6e94.jpg","comment_is_top":false,"comment_ctime":1579221888,"is_pvip":false,"discussion_count":0,"race_medal":4,"score":"1579221888","product_id":100017301,"comment_content":"打卡","like_count":0},{"had_liked":false,"id":169682,"user_name":"失火的夏天","can_delete":false,"product_type":"c1","uid":1241770,"ip_address":"","ucode":"10C6E66EB2A65F","user_header":"https://static001.geekbang.org/account/avatar/00/12/f2/aa/32fc0d54.jpg","comment_is_top":false,"comment_ctime":1578405290,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1578405290","product_id":100017301,"comment_content":"正向索引是通过文档来找关键字，倒排索引就是构造关键字到文档的映射，通过关键字来找文档。这样搜索文档就非常高效，不用再一个个遍历过去。搜索引擎背后原理就是倒排索引。","like_count":0},{"had_liked":false,"id":168831,"user_name":"book尾汁","can_delete":false,"product_type":"c1","uid":1446375,"ip_address":"","ucode":"AE2B8DFC643ACC","user_header":"https://static001.geekbang.org/account/avatar/00/16/11/e7/044a9a6c.jpg","comment_is_top":false,"comment_ctime":1578195240,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1578195240","product_id":100017301,"comment_content":"1 深度优先的话，不知道就爬到哪里去了，可能会爬取很多没有意义的页面。<br>2 摘要信息和网页快照，都是与网页链接相关的东西，可以爬取时，将网页链接 快照 摘要信息放在一起，或者分开来存，使用同一个doc_id","like_count":0},{"had_liked":false,"id":144576,"user_name":"teddytyy","can_delete":false,"product_type":"c1","uid":1268738,"ip_address":"","ucode":"E1569D81A4154E","user_header":"https://static001.geekbang.org/account/avatar/00/13/5c/02/e7af1750.jpg","comment_is_top":false,"comment_ctime":1571972435,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1571972435","product_id":100017301,"comment_content":"构建倒排索引可以直接用散列表吗？先把不同的数据按term_id散列到多台机器各自存放的散列表里，散列表以term_id的散列值为key，doc_id为值散列，重复元素以链表存储，这样一个term_id就对应一个doc_id的列表","like_count":0},{"had_liked":false,"id":129871,"user_name":"Lukia","can_delete":false,"product_type":"c1","uid":1028698,"ip_address":"","ucode":"C19472337BCCC6","user_header":"https://static001.geekbang.org/account/avatar/00/0f/b2/5a/574f5bb0.jpg","comment_is_top":false,"comment_ctime":1567308739,"is_pvip":false,"replies":[{"id":"48477","content":"哦哦哦 你的意思是搜索引擎会用到ac自动机是吧","user_name":"作者回复","user_name_real":"王争","uid":"1190123","ctime":1567377517,"ip_address":"","comment_id":129871,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1567308739","product_id":100017301,"comment_content":"老师好，本文中好像没有看到ac自动机的应用","like_count":0,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465686,"discussion_content":"哦哦哦 你的意思是搜索引擎会用到ac自动机是吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567377517,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":122313,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1565337064,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565337064","product_id":100017301,"comment_content":"思考题1<br>深度优先有以下问题<br>重要的网站遍历不全（假设网站和主要网站的距离越近越重要）<br>域名一样的网页没有一起下载，不利于缓存。","like_count":0},{"had_liked":false,"id":78798,"user_name":"Jeson","can_delete":false,"product_type":"c1","uid":1258168,"ip_address":"","ucode":"A75A8BF2EBD9CD","user_header":"https://static001.geekbang.org/account/avatar/00/13/32/b8/6a80fb00.jpg","comment_is_top":false,"comment_ctime":1553241974,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1553241974","product_id":100017301,"comment_content":"如果可以，希望作者能整理下您那个5w+行的代码开源分享，或者推荐个不错的代码demo。","like_count":0},{"had_liked":false,"id":64312,"user_name":"Kudo","can_delete":false,"product_type":"c1","uid":1036948,"ip_address":"","ucode":"21965914B72AEB","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d2/94/8bd217f1.jpg","comment_is_top":false,"comment_ctime":1548742824,"is_pvip":false,"replies":[{"id":"24187","content":"这个写起来比较多，我有个5万+行的C++代码，估计看起来也比较费劲！你还是自己写吧：）","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1550480573,"ip_address":"","comment_id":64312,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1548742824","product_id":100017301,"comment_content":"原理是看懂了，实现起来肯定会遇到各种各样的问题，手动实现一遍是有必要的，如果老师能提供一个参考代码就更好了。","like_count":0,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437957,"discussion_content":"这个写起来比较多，我有个5万+行的C++代码，估计看起来也比较费劲！你还是自己写吧：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550480573,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64240,"user_name":"yann [扬] :曹同学","can_delete":false,"product_type":"c1","uid":1199315,"ip_address":"","ucode":"C449253263E796","user_header":"https://static001.geekbang.org/account/avatar/00/12/4c/d3/365fe5a1.jpg","comment_is_top":false,"comment_ctime":1548725535,"is_pvip":false,"replies":[{"id":"24188","content":"好像是对的呢。注意我的单位。","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1550480630,"ip_address":"","comment_id":64240,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1548725535","product_id":100017301,"comment_content":"带宽那里貌似有点问题，应该是100的","like_count":0,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437919,"discussion_content":"好像是对的呢。注意我的单位。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550480630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64187,"user_name":"纯洁的憎恶","can_delete":false,"product_type":"c1","uid":1130512,"ip_address":"","ucode":"5E9757DE6F45DF","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/10/b6bf3c3c.jpg","comment_is_top":false,"comment_ctime":1548689080,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1548689080","product_id":100017301,"comment_content":"思考题1。网络是动态变化的，所以爬虫的任务是在有限的时间里，爬到尽可能多的最重要的网页，而重要的网页一般是首页及其直接链接的网页。这样从首页开始一层层遍历更满足要求，尤其是在资源紧张的时候。所以BFS更适合。起码在大的调度次序上是这样。但在工程上要考虑网络通讯中“握手”次数过于频繁的问题，调度算法的具体实现细节会吸取DFS的优点。","like_count":0},{"had_liked":false,"id":64112,"user_name":"猫头鹰爱拿铁","can_delete":false,"product_type":"c1","uid":1105958,"ip_address":"","ucode":"24266B58968428","user_header":"https://static001.geekbang.org/account/avatar/00/10/e0/26/4942a09e.jpg","comment_is_top":false,"comment_ctime":1548669277,"is_pvip":true,"replies":[{"id":"22761","content":"维护特定数据源的。<br>","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1548732039,"ip_address":"","comment_id":64112,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1548669277","product_id":100017301,"comment_content":"老师 像那些专业搜索的例如学术搜索是怎么去限定数据源的搜索范围呢 是不是人为的维护了特定数据源列表还是用了相关算法  ","like_count":0,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437862,"discussion_content":"维护特定数据源的。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1548732039,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64105,"user_name":"在路上","can_delete":false,"product_type":"c1","uid":1205712,"ip_address":"","ucode":"18337C6DD5E618","user_header":"https://static001.geekbang.org/account/avatar/00/12/65/d0/b5b00bc2.jpg","comment_is_top":false,"comment_ctime":1548667630,"is_pvip":false,"replies":[{"id":"22760","content":"可以的。","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1548732028,"ip_address":"","comment_id":64105,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1548667630","product_id":100017301,"comment_content":"争哥，Disruptor的无锁并发、环形数组，可以讲讲吗","like_count":0,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437859,"discussion_content":"可以的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1548732028,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":64017,"user_name":"꧁小佳꧂","can_delete":false,"product_type":"c1","uid":1131422,"ip_address":"","ucode":"83868E278A1C75","user_header":"https://static001.geekbang.org/account/avatar/00/11/43/9e/3bc0ce71.jpg","comment_is_top":false,"comment_ctime":1548641713,"is_pvip":true,"replies":[{"id":"22765","content":"也是按照html标签来处理的，没有区别的。","user_name":"作者回复","user_name_real":"gg","uid":"1190123","ctime":1548732257,"ip_address":"","comment_id":64017,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1548641713","product_id":100017301,"comment_content":"老师，如果网页的内容是程序代码，而代码里有html的相应标签，字符串匹配算法是不是还需要考虑这点。","like_count":0,"discussions":[{"author":{"id":1190123,"avatar":"https://static001.geekbang.org/account/avatar/00/12/28/eb/af064421.jpg","nickname":"王争","note":"","ucode":"2B611BE0E0EDD4","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":437813,"discussion_content":"也是按照html标签来处理的，没有区别的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1548732257,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":63990,"user_name":"莫弹弹","can_delete":false,"product_type":"c1","uid":1239978,"ip_address":"","ucode":"60A25C709A665F","user_header":"https://static001.geekbang.org/account/avatar/00/12/eb/aa/db213a66.jpg","comment_is_top":false,"comment_ctime":1548637959,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1548637959","product_id":100017301,"comment_content":"思考题1<br>如果使用深度优先遍历，那相当于全站克隆工具了，一搜全是这个网站的结果，变得像该网站的站内工具了","like_count":0,"discussions":[{"author":{"id":1204353,"avatar":"https://static001.geekbang.org/account/avatar/00/12/60/81/eaf6d0ac.jpg","nickname":"拉布拉多","note":"","ucode":"637A88D9F29F57","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333361,"discussion_content":"同感。站克隆","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607506965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}