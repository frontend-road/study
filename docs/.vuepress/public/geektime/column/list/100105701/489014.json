{"id":489014,"title":"04 | 网页爬虫设计：如何下载千亿级网页？","content":"<p>你好，我是李智慧。</p><p>在互联网早期，网络爬虫仅仅应用在搜索引擎中。随着大数据时代的到来，数据存储和计算越来越廉价和高效，越来越多的企业开始利用网络爬虫来获取外部数据。例如：获取政府公开数据以进行统计分析；获取公开资讯以进行舆情和热点追踪；获取竞争对手数据以进行产品和营销优化等等。</p><p>网络爬虫有时候也被称为网络机器人，或者网络蜘蛛。我们准备开发一个全网爬虫，爬取全（中文）互联网的公开网页，以构建搜索引擎和进行数据分析。爬虫名称为“Bajie（八戒）”。</p><p>Bajie的技术挑战包括：如何不重复地获取并存储全网海量URL？如何保证爬虫可以快速爬取全网网页但又不会给目标网站带来巨大的并发压力？接下来我们就来看看Bajie的需求与技术架构。</p><h2>需求分析</h2><p>Bajie的功能比较简单，这里不再赘述。</p><h4>性能指标估算</h4><p>因为互联网网页会不断产生，所以全网爬虫Bajie也是一个持续运行的系统。根据设计目标，Bajie需要每个月从互联网爬取的网页数为20亿个，平均每个页面500KB，且网页需存储20年。</p><p>Bajie的存储量和TPS（系统吞吐量）估算如下。</p><ul>\n<li><strong>每月新增存储量</strong><br>\n估计平均每个页面500KB，那么每个月需要新增存储1PB。</li>\n</ul><p>$\\small 20亿\\times500KB=1PB$</p><!-- [[[read_end]]] --><ul>\n<li><strong>总存储空间</strong><br>\n网页存储有效期20年，那么需要总存储空间240PB。</li>\n</ul><p>$\\small 1PB\\times12个月\\times20年=240PB$</p><ul>\n<li><strong>TPS</strong><br>\nBajie的TPS应为800。</li>\n</ul><p>$\\small 20亿\\div（30\\times24\\times60\\times60）\\approx800$</p><h4>非功能需求</h4><p>Bajie需要满足的非功能需求如下。</p><ol>\n<li>伸缩性：当未来需要增加每月爬取的网页数时，Bajie可以灵活部署，扩大集群规模，增强其爬取网页的速度。也就是说，Bajie必须是一个分布式爬虫。</li>\n<li>健壮性：互联网是一个开放的世界，也是一个混乱的世界，服务器可能会宕机，网站可能失去响应，网页HTML可能是错误的，链接可能有陷阱……所以Bajie应该能够面对各种异常，正常运行。</li>\n<li>去重：一方面需要对超链接URL去重，相同的URL不需要重复下载；另一方面还要对内容去重，不同URL但是相同内容的页面也不需要重复存储。</li>\n<li>扩展性：当前只需要爬取HTML页面即可，将来可能会扩展到图片、视频、文档等内容页面。<br>\n此外，Bajie必须是“礼貌的”。爬虫爬取页面，实际上就是对目标服务器的一次访问，如果高并发地进行访问，可能会对目标服务器造成比较大的负载压力，甚至会被目标服务器判定为DoS攻击。因此Bajie要避免对同一个域名进行并发爬取，还要根据目标服务器的承载能力增加访问延迟，即在两次爬取访问之间，增加等待时间。</li>\n</ol><p>并且，Bajie还需要遵循互联网爬虫协议，即目标网站的robots.txt协议，不爬取目标网站禁止爬取的内容。比如www.zhihu.com的robots.txt内容片段如下。</p><pre><code class=\"language-plain\">User-agent: bingbot\nDisallow: /appview/\nDisallow: /login\nDisallow: /logout\nDisallow: /resetpassword\nDisallow: /terms\nDisallow: /search\nAllow: /search-special\nDisallow: /notifications\nDisallow: /settings\nDisallow: /inbox\nDisallow: /admin_inbox\nDisallow: /*?guide*\n</code></pre><p>Zhihu约定Bing爬虫可以访问和不可以访问的路径都列在robots.txt中，其他的Google爬虫等也在robots.txt中列明。<br>\nrobots.txt还可以直接禁止某个爬虫，比如淘宝就禁止了百度爬虫，淘宝的robots.txt如下。</p><pre><code class=\"language-plain\">User-agent: Baiduspider\nDisallow: /\nUser-agent: baiduspider\nDisallow: /\n</code></pre><p>淘宝禁止百度爬虫访问根目录，也就是禁止百度爬取该网站所有页面。<br>\nrobots.txt在域名根目录下，如www.taobao.com/robots.txt。Bajie应该首先获取目标网站的robots.txt，根据爬虫协议构建要爬取的URL超链接列表。</p><h2>概要设计</h2><p>Bajie的设计目标是爬取数千亿的互联网页，那么Bajie首先需要得到这千亿级网页的URL，该如何获得呢？</p><p>全世界的互联网页面事实上是一个通过超链接连接的巨大网络，其中每个页面都包含一些指向其他页面的URL链接，这些有指向的链接将全部网页构成一个有向（网络）图。如下图所示，每个节点是一个网页，每条有向的边就是一个超链接。</p><p><img src=\"https://static001.geekbang.org/resource/image/70/2f/70737d192e53fbfdbc42a4c5a6bcf12f.jpg?wh=1920x910\" alt=\"图片\"></p><p>上图中，www.a.com包含两个超链接，分别是www.b.com和www.c.com，对应图中就是节点www.a.com指向节点www.b.com和节点www.c.com的边。同样地，www.b.com节点也会指向www.d.com节点。</p><p>如果我们从这个图中的某个节点开始遍历，根据节点中包含的链接再遍历其指向的节点，再从这些新节点遍历其指向的节点，如此下去，理论上可以遍历互联网上的全部网页。而<strong>将遍历到的网页下载保存起来</strong>，就是爬虫的主要工作。</p><p>所以，Bajie不需要事先知道数千亿的URL，然后再去下载。Bajie只需要知道一小部分URL，也就是所谓的种子URL，然后从这些种子URL开始遍历，就可以得到全世界的URL，并下载全世界的网页。</p><p>Bajie的处理流程活动图如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/72/0f/7278302442830b4576bbc04d36171d0f.jpg?wh=1920x916\" alt=\"图片\"></p><p>首先Bajie需要构建种子URL，它们就是遍历整个互联网页面有向图的起点。种子URL将影响遍历的范围和效率，所以我们通常选择比较知名的网站的主要页面（比如首页）作为种子URL。</p><p>然后，URL调度器从种子URL中选择一些URL进行处理。后面将在详细介绍中说明URL调度器的算法原理。</p><p>Bajie对选择出来的URL经过域名解析后，下载得到HTML页面内容，进而解析HTML页面，分析该内容是否已经在爬虫系统中存在。因为在互联网世界中，大约有三分之一的内容是重复的，下载重复的内容就是在浪费计算和存储资源。如果内容已存在，就丢弃该重复内容，继续从URL调度器获取URL；如果不存在，就将该HTML页面写入HDFS存储系统。</p><p>然后，Bajie进一步从已存储的HTML中提取其内部包含的超链接URL，分析这些URL是否满足过滤条件，即判断URL是否在黑名单中，以及URL指向的目标文件类型是否是爬虫要爬取的类型。</p><p>如果HTML中的某些URL满足过滤条件，那么就丢弃这些URL；如果不满足过滤条件，那么，进一步判断这些URL是否已经存在，如果已经存在，就丢弃该URL，如果不存在，就记录到待下载URL集合。URL调度器从待下载URL集合中选择一批URL继续上面的处理过程。</p><p>这里需要注意，想判断URL是否已经存在，就要判断这个URL是否已经在待下载URL集合中。此外，还需要判断这个URL是否已经下载得到HTML内容了。只有既不是待下载，也没被下载过的URL才会被写入待下载URL集合。</p><p>可以看到，在爬虫的活动图里是没有结束点的，从开始启动，就不停地下载互联网的页面，永不停息。其中，<strong>URL调度器是整个爬虫系统的中枢和核心，也是整个爬虫的驱动器</strong>。爬虫就是靠着URL调度器源源不断地选择URL，然后有节奏、可控地下载了整个互联网，所以<strong>URL调度器也是爬虫的策略中心</strong>。</p><p>据此，Bajie的部署图如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/25/a613bd705567d0ba9db7d50ff2830c25.jpg?wh=1920x1005\" alt=\"图片\"></p><p>Bajie系统中主要有两类服务器，一类是URL调度器服务器；一类是URL下载处理服务器集群，它是一个分布式集群。</p><p>URL调度器从种子URL或待下载URL集合中载入URL，再根据调度算法，选择一批URL发送给URL下载处理服务器集群。这个下载处理服务器集群是由多台服务器组成的，根据需要达到的TPS，集群规模可以进行动态伸缩，以实现需求中的伸缩性要求。</p><p>每台URL下载处理服务器先得到分配给自己的一组URL，再启动多个线程，其中每个线程处理一个URL，按照前面的流程，调用域名解析组件、HTML下载组件、HTML内容解析组件、内容去重组件、URL提取组件、URL过滤组件、URL去重组件，最终将HTML内容写入HDFS，并将待下载URL写入待下载URL集合文件。</p><h4><strong>分布式爬虫</strong></h4><p>需要注意的是，URL下载处理服务器采用分布式集群部署，主要是为了提高系统的吞吐能力，使系统满足伸缩性需求。而URL调度器则只需要采用一台高性能的服务器单机部署即可。</p><p>事实上，单机URL调度器也完全能够满足目前800TPS的负载压力，以及将来的伸缩要求。因为800TPS对于URL调度器而言其实就是每秒产生800个URL而已，计算压力并不大，单台服务器完全能够满足。</p><p>同时URL调度器也不需要考虑单服务器宕机导致的可用性问题，因为爬虫并不是一个实时在线系统，如果URL调度器宕机，只需要重新启动即可，并不需要多机部署高可用集群。</p><p>相对应地，每个URL在URL下载处理服务器上的计算负载压力要大得多，需要分布式集群处理，也因此大规模爬虫被称为分布式爬虫，Bajie就是一个分布式爬虫。</p><h2>详细设计</h2><p>Bajie详细设计关注3个技术关键点：URL调度器算法、去重算法、高可用设计。</p><h4><strong>URL调度器算法</strong></h4><p>URL调度器需要从待下载URL集合中选取一部分URL进行排序，然后分发给URL下载服务器去下载。待下载URL集合中的URL是从下载的HTML页面里提取出来，然后进行过滤、去重得到的。一个HTML页面通常包含多个URL，每个URL又对应一个页面，因此，URL集合数量会随着页面不断下载而指数级增加。</p><p>待下载URL数量将远远大于系统的下载能力，<strong>URL调度器就需要决定当前先下载哪些URL</strong>。</p><p>如果调度器一段时间内选择的都是同一个域名的URL，那就意味着我们的爬虫将以800 TPS的高并发访问同一个网站。目标网站可能会把爬虫判定为DoS攻击，从而拒绝请求；更严重的是，高并发的访问压力可能导致目标网站负载过高，系统崩溃。这样的爬虫是“不礼貌”的，也不是Bajie的设计目标。</p><p>前面说过，网页之间的链接关系构成一个有向图，因此我们可以按照图的遍历算法选择URL。图的遍历算法有深度优先和广度优先两种，深度优先就是从一个URL开始，访问网页后，从里面提取第一个URL，然后再访问该URL的页面，再提取第一个URL，如此不断深入。</p><p>深度优先需要维护较为复杂的数据结构，而且太深的下载深度导致下载的页面非常分散，不利于我们构建搜索引擎和数据分析。所以我们没有使用深度优先算法。</p><p>那广度优先算法如何呢？广度优先就是从一个URL开始，访问网页后，从中得到N个URL，然后顺序访问这个N个URL的页面，然后再从这N个页面中提取URL，如此不断深入。显然，广度优先实现更加简单，获取的页面也比较有关联性。</p><p>图的广度优先算法通常采用<strong>队列</strong>来实现。首先，URL调度器从队列头出队列（dequeue）取一个URL，交给URL下载服务器，下载得到HTML，再从HTML中提取得到若干个URL入队列（enqueue）到队列尾，URL调度器再从队列头出队列（dequeue）取一个URL……如此往复，持续不断地访问全部互联网页，这就是互联网的广度优先遍历。</p><p>事实上，由于待下载URL集合存储在文件中，URL下载服务器只需要向待下载URL集合文件尾部追加URL记录，而URL调度器只需要从文件头顺序读取URL，这样就天然实现了先进先出的广度优先算法，如下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/d6/e0/d67140e0d7yyf7011f9a3a9e515e91e0.jpg?wh=1920x916\" alt=\"图片\"></p><p>但是，广度优先搜索算法可能会导致爬虫一段时间内总是访问同一个网站，因为一个HTML页面内的链接常常是指向同一个网站的，这样就会使爬虫“不礼貌”。</p><p>通常我们针对一个网站，一次只下载一个页面，所以URL调度器需要将待下载URL根据域名进行分类。此外，不同网站的信息质量也有高低之分，爬虫应该优先爬取那些高质量的网站。优先级和域名都可以使用不同队列来区分，如下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/a5/3b/a5e8d1dd4ab4b622b79c46066004ca3b.jpg?wh=1920x1544\" alt=\"图片\"></p><p>首先优先级分类器会根据网页内容质量将域名分类（后面专栏会讲PageRank质量排名算法），并为不同质量等级的域名设置不同的优先级，然后将不同优先级记录在“域名优先级表”中。</p><p>接下来，按照广度优先算法，URL列表会从待下载URL集合文件中装载进来。根据“域名优先级表”中的优先级顺序，优先级分类器会将URL写入不同的队列中。</p><p>下一步，优先级队列选择器会根据优先级使用不同的权重，从这些优先级队列中随机获取URL，这样使得高优先级的URL有更多机会被选中。而被选中的URL都会交由域名分类器进行分类处理。域名分类器的分类依据就是“域名队列映射表”，这个表中记录了不同域名对应的队列。所以域名分类器可以顺利地将不同域名的URL写入不同的域名队列中。</p><p>最后，域名队列选择器将轮询所有的域名队列，从其中获得URL并分配给不同的URL下载服务器，进而完成下载处理。</p><h4>去重算法</h4><p>爬虫的去重包括两个方面，一个是URL，相同URL不再重复下载；一个是内容，相同页面内容不再重复存储。去重一方面是提高爬虫效率，避免无效爬取；另一方面提高搜索质量，避免相同内容在搜索结果中重复出现。URL去重可以使用<strong>布隆过滤器</strong>以提高效率。</p><p>内容去重首先要判断内容是否重复，由于爬虫存储着海量的网页，如果按照字符内容对每一个下载的页面都去和现有的页面比较是否重复，显然是不可能的。</p><p>Bajie计算页面内容的MD5值，通过判断下载页面的内容MD5值是否已经存在，判断内容是否重复。</p><p>如果把整个HTML内容都计算MD5，那么HTML中的微小改变就会导致MD5不同，事实上，不同网站即使相同内容的页面，也总会改成自己的HTML模板，导致HTML内容不同。</p><p>所以，比较内容重复的时候，需要将HTML里面的有效内容提取出来，也就是提取出去除HTML标签的文本信息，针对有效内容计算MD5。更加激进的做法是从有效内容中抽取一段话（比如最长的一句话），计算这段话的MD5，进而判断重复。</p><p>而一个内容MD5是否存在，需要在千亿级的数据上查找，如果用Hash表处理，计算和内存存储压力非常大，我们将用布隆过滤器代替Hash表，以优化性能。</p><h4>高可用设计</h4><p>Bajie的可用性主要关注两个方面，一是URL调度器或URL下载处理服务器宕机，二是下载超时或内容解析错误。</p><p>由于Bajie是一个离线系统，暂时停止爬取数据的话，不会产生严重的后果，所以Bajie并不需要像一般互联网系统那样进行高可用设计。但是当服务器宕机后重启时，系统需要能够正确恢复，保证既不会丢失数据，也不会重复下载。</p><p>所以，URL调度器和URL下载处理服务器都需要记录运行时状态，即存储本服务器已经加载的URL和已经处理完成的URL，这样宕机恢复的时候，就可以立刻读取到这些状态数据，进而使服务器恢复到宕机前的状态。对于URL下载处理服务器，Bajie采用Redis记录运行时状态数据。</p><p>此外，为了防止下载超时或内容解析错误，URL下载处理服务器会采用多线程（池）设计。每个线程独立完成一个URL的下载和处理，线程也需要捕获各种异常，不会使自己因为网络超时或者解析异常而退出。</p><h2>小结</h2><p>架构设计是一个权衡的艺术，<strong>不存在最好的架构，只存在最合适的架构</strong>。架构设计的目的是解决各种业务和技术问题，而解决问题的方法有很多种，每一种方法都需要付出各自的代价，同时又会带来各种新的问题。架构师就需要在这些方法中权衡选择，寻找成本最低的、代价最小的、自己和团队最能驾驭得住的解决方案。</p><p>因此，架构师也许不是团队中技术最好的那个人，但一定是<strong>对问题和解决方案优缺点理解最透彻</strong>的那个人。很多架构师把高可用挂在嘴上。可是，你了解你的系统的高可用的目的是什么吗？你的用户能接受的不可用下限在哪里？你为高可用付出的代价是什么？这些代价换来的回报是否值得？</p><p>我们在Bajie的设计中，核心就是URL调度器。通常在这样的大规模分布式系统中，核心组件是不允许单点的，也就是不允许单机部署，因为单机宕机就意味着核心功能的故障，也就意味着整个系统无法正常运行。</p><p>但是如果URL调度器采用分布式集群架构提高可用性，多服务器共同进行URL调度，就需要解决数据一致性和数据同步问题，反而会导致系统整体处理能力下降。而Bajie采用单机部署的的方式，虽然宕机时系统无法正常运行，但是只要在运维上保证能快速重新启动，长期看，系统整体处理能力反而更高。</p><p>此外，对于一个千亿级网页的爬虫系统而言，最主要的技术挑战应该是海量文件的存储与计算，这也确实是早期搜索引擎公司们的核心技术。但是，自从Google公开自己的大数据技术论文，而Hadoop开源实现了相关技术后，这些问题就变得容易很多了。Bajie的海量文件存储就使用了Hadoop分布式文件系统HDFS，我会在后面的《常见海量数据处理技术回顾》这一讲详细讨论它。</p><h2>思考题</h2><p>一个设计良好的爬虫需要面对的情况还有很多，你还能想到哪些文中没提及的情况？最好也能和我聊聊对应的设计方案。</p><p>欢迎在评论区分享你的思考，或者提出对这个设计文档的评审意见，我们共同进步。</p>","comments":[{"had_liked":false,"id":337995,"user_name":"开心小毛","can_delete":false,"product_type":"c1","uid":1023762,"ip_address":"","ucode":"9D57A2773759F3","user_header":"","comment_is_top":true,"comment_ctime":1647225916,"is_pvip":false,"replies":[{"id":"123591","content":"非常好的问题，我们的文章都是一篇篇设计文档，用来指导开发实践的，所以阅读文章的过程，就是不断思考每个设计点如何落地实现的过程，通过这种方式强化知识水平，提高实践能力。<br><br>大家探讨这样的问题是学习这个专栏的最好方式。<br><br>回到正题，我的想法是：太多URL的队列写回到待下载URL集合。<br>还有，这又涉及到如何判断队列不平衡？大家有什么方法？","user_name":"作者回复","user_name_real":"作者","uid":"1007349","ctime":1647236714,"ip_address":"","comment_id":337995,"utype":1}],"discussion_count":5,"race_medal":0,"score":"9.2233720513869005e+18","product_id":100105701,"comment_content":"想请问一下 ，如果“下载优先级队列”之间产生负载的不平衡怎么处理，比如说优先级为2的URL太多，以至于系统里其他的队列太空，队列2却太长被覆盖。","like_count":4,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556168,"discussion_content":"非常好的问题，我们的文章都是一篇篇设计文档，用来指导开发实践的，所以阅读文章的过程，就是不断思考每个设计点如何落地实现的过程，通过这种方式强化知识水平，提高实践能力。\n\n大家探讨这样的问题是学习这个专栏的最好方式。\n\n回到正题，我的想法是：太多URL的队列写回到待下载URL集合。\n还有，这又涉及到如何判断队列不平衡？大家有什么方法？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647236714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1018132,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/89/14/71bcd25e.jpg","nickname":"风","note":"","ucode":"6FCC11027BBE7A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":576708,"discussion_content":"有个算法叫工作窃取算法，forkjoinpool里就有，可以参考解决这个问题吗","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1655742122,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":556168,"ip_address":""},"score":576708,"extra":""}]},{"author":{"id":1627631,"avatar":"https://static001.geekbang.org/account/avatar/00/18/d5/ef/83549405.jpg","nickname":"YUNGKIT","note":"","ucode":"7E49ACE4C3ED57","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":559749,"discussion_content":"其实就跟CPU调度算法解决方法类似","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1648910705,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062848,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersGSic8ib7OguJv6CJiaXY0s4n9C7Z51sWxTTljklFpq3ZAIWXoFTPV5oLo0GMTkqW5sYJRRnibNqOJQ/132","nickname":"walle斌","note":"","ucode":"0DB3243004951F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":577670,"discussion_content":"优先级 外加 过期时间。。类似与硬盘调度算法。。电梯算法 再额外一个饥饿程度","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656292324,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2549097,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoib6BjEV4KPEaIdlLEfoVFRCxCSlL2XaIVDiaakvjhWEibibym323ZeHXAY46JMO3nSHmjiaWtAY47eww/132","nickname":"dobby","note":"","ucode":"9C1992C4DD28F5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":559142,"discussion_content":"优先级队列是有限得，比较粗糙一点的就是每个队列启一个实例调度，然后再空闲得时候抢占其他队列得URL调度就能解决了，比较粗糙得做法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648624400,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335579,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1645594853,"is_pvip":true,"replies":[{"id":"122594","content":"1 是的<br>2 压测<br>3 不理会没关系，但是爬来的数据就没法用来做公开搜索引擎<br>4 不知道<br>5 可以啊，URL调度器产生的URL快慢就可以控制，也依赖下载服务器的处理能力<br>6 会删除，一般不会丢弃，将来还要分析用的<br>7 爬虫就是一个HTTP client，各种语言都可以。python多是因为Python在数据处理领域用的多，用到爬虫的地方也多。实际上大规模爬虫开发的重头是下载HTML后的处理过程，Java或者C++的处理能力更强<br>8 是个文件<br>9 用HDFS存储<br>10 域名，多个域名可以合用一个队列。集合中的域名是有限的，列举的完","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1645597734,"ip_address":"","comment_id":335579,"utype":1}],"discussion_count":1,"race_medal":0,"score":"44595267813","product_id":100105701,"comment_content":"请教老师几个问题啊：<br>Q1：“可扩展”与“可伸缩”的区别是什么？<br>“可扩展”是指软件开发方面，“可伸缩”是指部署方面，即增删机器方面，对吗？<br>Q2：怎么知道目标服务器的负载能力？<br>Q3：爬虫如果不理会robots.txt，会怎么处理？<br>   Robots.txt中禁止爬取，是从技术的角度爬虫无法爬取吗？还是说技术上爬虫仍然可以爬取，但可能会被起诉？<br>Q4：HDFS出现之前，百度将爬取的网页存在什么系统？<br>   百度比HDFS出现的早，没有HDFS的时候网页是怎么存储的？<br>Q5：爬虫的爬取速度是可以控制的吗？<br>Q6：一个URL被正常处理完后，会从“待下载 URL 集合”中删除吗？<br>   如果会删除，删除后该URL会被丢弃还是保存到某个地方？<br>Q7：爬虫用什么开发？好像python居多，用Java可以吗？<br>Q8：“待下载 URL 集合”，用Java开发的话用什么JDK的哪种集合？<br>或者，虽然名字叫“集合”，但其实只是往文件中追加记录？<br>Q9：“待下载 URL 集合”，取出一个，添加多个，那这个文件岂不是越来越大，很快就会超出文件大小？(产生的多，处理的少，积压越来愈多)<br>Q10：“域名队列映射表”，根据具体域名还是域名类型来判断？互联网域名可太多了啊，怎么也列举不完啊。","like_count":11,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552798,"discussion_content":"1 是的\n2 压测\n3 不理会没关系，但是爬来的数据就没法用来做公开搜索引擎\n4 不知道\n5 可以啊，URL调度器产生的URL快慢就可以控制，也依赖下载服务器的处理能力\n6 会删除，一般不会丢弃，将来还要分析用的\n7 爬虫就是一个HTTP client，各种语言都可以。python多是因为Python在数据处理领域用的多，用到爬虫的地方也多。实际上大规模爬虫开发的重头是下载HTML后的处理过程，Java或者C++的处理能力更强\n8 是个文件\n9 用HDFS存储\n10 域名，多个域名可以合用一个队列。集合中的域名是有限的，列举的完","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645597734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335988,"user_name":"legendcq","can_delete":false,"product_type":"c1","uid":1179864,"ip_address":"","ucode":"62A558A9982687","user_header":"https://wx.qlogo.cn/mmopen/vi_32/7F7TZwdDKVvlbGTqoH5y1h0c7DrzWVGsOia7xiaR4lxYGLyQiaaLNuFFib3aicm3xtwJA94PEKyrMj5ekglmbDzR9GQ/132","comment_is_top":false,"comment_ctime":1645821470,"is_pvip":false,"replies":[{"id":"122875","content":"是的，少量重复对性能和结果的影响都可以忽略","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1646014658,"ip_address":"","comment_id":335988,"utype":1}],"discussion_count":3,"race_medal":0,"score":"23120657950","product_id":100105701,"comment_content":"老师，如果用布隆过滤器去重，如何处理false positive的情况，是允许一定比例的重复数据吗？","like_count":6,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553653,"discussion_content":"是的，少量重复对性能和结果的影响都可以忽略","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646014658,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1182372,"avatar":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","nickname":"张申傲","note":"","ucode":"22D46BC529BA8A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":574671,"discussion_content":"是会漏掉数据，而不是重复数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654238144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2063703,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/7d/57/58017e1b.jpg","nickname":"Miao","note":"","ucode":"2334B16FA94C30","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":567261,"discussion_content":"应该是会漏掉数据吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650871044,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":338536,"user_name":"ABC","can_delete":false,"product_type":"c1","uid":1054958,"ip_address":"","ucode":"7501AD9C0C4A70","user_header":"https://static001.geekbang.org/account/avatar/00/10/18/ee/a1ed60d1.jpg","comment_is_top":false,"comment_ctime":1647563513,"is_pvip":false,"replies":[{"id":"123736","content":"确实，搜索引擎和内容网站是共生关系，互为依赖，主动推送是一种不错的合作模式。<br><br>谢谢分享，学习了~~","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1647569974,"ip_address":"","comment_id":338536,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18827432697","product_id":100105701,"comment_content":"好几年前用Python写过一些爬虫，都是比较简单的。在搜索引擎这个方面，除了搜索引擎主动爬取网页，有的网站还会主动推送新内容给搜索引擎，比如国内某些新闻网站在某度基本是分钟级收录。","like_count":5,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":556905,"discussion_content":"确实，搜索引擎和内容网站是共生关系，互为依赖，主动推送是一种不错的合作模式。\n\n谢谢分享，学习了~~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647569974,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":336354,"user_name":"Geek_7347cf","can_delete":false,"product_type":"c1","uid":2343516,"ip_address":"","ucode":"2E25574FAB1B3B","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/mmTEibMibic5ibsVpNZkR3HBlpPpZYt0gHGdIqOduLGxRHZpTWRG3q56CT1eejoLgNsdaW5aQGWXfyibN4vm9CicYb3w/132","comment_is_top":false,"comment_ctime":1646107322,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10236041914","product_id":100105701,"comment_content":"如果网页内容是动态script渲染的 该如何爬取","like_count":1,"discussions":[{"author":{"id":2927904,"avatar":"","nickname":"Geek9382","note":"","ucode":"638288F553332F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558241,"discussion_content":"我理解这个是架构课  不是具体的爬虫实战课。。爬虫抓取 是作为一个component进行设计 分成module ，每个module 如何具体实现","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1648171783,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335842,"user_name":"门窗小二","can_delete":false,"product_type":"c1","uid":1006424,"ip_address":"","ucode":"0BF3780C247F22","user_header":"","comment_is_top":false,"comment_ctime":1645715415,"is_pvip":true,"replies":[{"id":"122753","content":"域名队列就是解决这个问题的，会把特别热门的域名放在一个单独队列，这个队列即使数据特别多，每次也只会被域名队列选择器轮询一个URL，而域名队列有几百个，这个域名只占下载任务几百分之一。","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1645756154,"ip_address":"","comment_id":335842,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5940682711","product_id":100105701,"comment_content":"老师，如果是热门网站，会不会造成这个队列的数据特别多严重倾斜，极端情况下，下载服务都在处理这个网站的数据，增加目标网站的负载同时增加自身ip被目标加入黑名单的风险？换句话说可以对热点网站处理存在对应限流策略？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553174,"discussion_content":"域名队列就是解决这个问题的，会把特别热门的域名放在一个单独队列，这个队列即使数据特别多，每次也只会被域名队列选择器轮询一个URL，而域名队列有几百个，这个域名只占下载任务几百分之一。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1645756154,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":1006424,"avatar":"","nickname":"门窗小二","note":"","ucode":"0BF3780C247F22","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":553178,"discussion_content":"那可以理解为每个域名队列同时只存在一个线程处理？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645757112,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":553174,"ip_address":""},"score":553178,"extra":""},{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1006424,"avatar":"","nickname":"门窗小二","note":"","ucode":"0BF3780C247F22","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":553190,"discussion_content":"队列是在URL调度器里，产生一批URL后就交给URL处理服务器，并不能保证同时只有一个线程，但是也不会很多","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1645759227,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":553178,"ip_address":""},"score":553190,"extra":""}]}]},{"had_liked":false,"id":343958,"user_name":"singularity of space time","can_delete":false,"product_type":"c1","uid":2816371,"ip_address":"","ucode":"14502507C9F2D7","user_header":"https://static001.geekbang.org/account/avatar/00/2a/f9/73/01eafd3c.jpg","comment_is_top":false,"comment_ctime":1651143209,"is_pvip":true,"replies":[{"id":"125799","content":"TPS和平均响应时间以及并发数相关，响应时间由目标网站控制，爬虫可以大概计算一个平均时间；并发数由爬虫线程数控制，是开发者可以决定的。<br><br>限流是用来控制用户请求并发数的，爬虫的场景没有用户请求，不需要用限流。","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1651721978,"ip_address":"","comment_id":343958,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1651143209","product_id":100105701,"comment_content":"想请问一下TPS如何控制，也即如何保证TPS在预估的800左右，而不会在某些突发情况下冲破峰值（比如在系统刚刚运行时），是采用类似漏桶、令牌桶这样的限流算法嘛？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":570280,"discussion_content":"TPS和平均响应时间以及并发数相关，响应时间由目标网站控制，爬虫可以大概计算一个平均时间；并发数由爬虫线程数控制，是开发者可以决定的。\n\n限流是用来控制用户请求并发数的，爬虫的场景没有用户请求，不需要用限流。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1651721978,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":339582,"user_name":"太空牛仔","can_delete":false,"product_type":"c1","uid":1227366,"ip_address":"","ucode":"0205635C3854AF","user_header":"https://static001.geekbang.org/account/avatar/00/12/ba/66/7d9f45e7.jpg","comment_is_top":false,"comment_ctime":1648200968,"is_pvip":true,"replies":[{"id":"124214","content":"如果每个域名一个域名队列，那么轮询的时候一定会有很多空队列，事实上还是保证了优先级权重。<br>另外，系统也会定期合并空队列，以提高性能，降低资源浪费，但是最终还是能保证优先级权重。","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1648433780,"ip_address":"","comment_id":339582,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1648200968","product_id":100105701,"comment_content":"下载优先级队列的数据都会流转到域名队列中，域名队列是通过轮询的方式发放给下载服务器，那不就失去了优先队列的优先权重吗？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":558669,"discussion_content":"如果每个域名一个域名队列，那么轮询的时候一定会有很多空队列，事实上还是保证了优先级权重。\n另外，系统也会定期合并空队列，以提高性能，降低资源浪费，但是最终还是能保证优先级权重。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1648433780,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":336976,"user_name":"Leader","can_delete":false,"product_type":"c1","uid":2748925,"ip_address":"","ucode":"90E93A28D3C16D","user_header":"https://static001.geekbang.org/account/avatar/00/29/f1/fd/003cf398.jpg","comment_is_top":false,"comment_ctime":1646506735,"is_pvip":true,"replies":[{"id":"123165","content":"网站内容更新一般都会使用新的URL。<br>","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1646617784,"ip_address":"","comment_id":336976,"utype":1}],"discussion_count":4,"race_medal":0,"score":"1646506735","product_id":100105701,"comment_content":"请问老师，如果网站内容有更新怎么办？如果重新爬取，跟去重算法冲突了怎么办？感觉去重算法这一块设计里讲的还是不够完整。","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":554787,"discussion_content":"网站内容更新一般都会使用新的URL。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646617784,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":3,"child_discussions":[{"author":{"id":2748925,"avatar":"https://static001.geekbang.org/account/avatar/00/29/f1/fd/003cf398.jpg","nickname":"Leader","note":"","ucode":"90E93A28D3C16D","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":554794,"discussion_content":"这也不一定吧？比如知乎，百度百科，维基百科类似的网页内容有更新，url也不一定改了呀。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1646618893,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":554787,"ip_address":""},"score":554794,"extra":""},{"author":{"id":1161522,"avatar":"https://static001.geekbang.org/account/avatar/00/11/b9/32/84346d4a.jpg","nickname":"雪碧心拔凉","note":"","ucode":"D13EEBAA0F443B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2748925,"avatar":"https://static001.geekbang.org/account/avatar/00/29/f1/fd/003cf398.jpg","nickname":"Leader","note":"","ucode":"90E93A28D3C16D","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":571434,"discussion_content":"感觉这块确实是个问题，爬虫如何更新最新的网站内容。针对高权重url定期重爬？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1652227780,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":554794,"ip_address":""},"score":571434,"extra":""},{"author":{"id":1366632,"avatar":"https://static001.geekbang.org/account/avatar/00/14/da/68/11ab05f3.jpg","nickname":"user","note":"","ucode":"D8C53679C24B61","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":2748925,"avatar":"https://static001.geekbang.org/account/avatar/00/29/f1/fd/003cf398.jpg","nickname":"Leader","note":"","ucode":"90E93A28D3C16D","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":574628,"discussion_content":"感觉只能 手动打标 定期重爬","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654184315,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":554794,"ip_address":""},"score":574628,"extra":""}]}]},{"had_liked":false,"id":335917,"user_name":"dll","can_delete":false,"product_type":"c1","uid":1264401,"ip_address":"","ucode":"5773CBC8BFB91F","user_header":"https://static001.geekbang.org/account/avatar/00/13/4b/11/d7e08b5b.jpg","comment_is_top":false,"comment_ctime":1645769680,"is_pvip":true,"replies":[{"id":"122765","content":"这个专栏我们主要关注是如何对系统进行抽象，也就是如何设计","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1645779351,"ip_address":"","comment_id":335917,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1645769680","product_id":100105701,"comment_content":"老师，有没有一些配套的demo code能够参考借鉴一下呢","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":553237,"discussion_content":"这个专栏我们主要关注是如何对系统进行抽象，也就是如何设计","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645779351,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335666,"user_name":"hdhdh","can_delete":false,"product_type":"c1","uid":1047168,"ip_address":"","ucode":"A0F9AC53329164","user_header":"https://static001.geekbang.org/account/avatar/00/0f/fa/80/246eadec.jpg","comment_is_top":false,"comment_ctime":1645618825,"is_pvip":false,"replies":[{"id":"122661","content":"现实中，会在项目前期做高并发的架构设计与开发，但是不会做高并发的物理部署，一则用户量是逐渐增加的，开始的时候不知道需要部署多少服务器，其次服务器的并发承载能力是压测和不断优化得到的，在设计期并不知道。物理部署图在设计阶段没有意义和价值。<br><br>多机房部署也是一种设计，后面有案例专门讨论。","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1645666824,"ip_address":"","comment_id":335666,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1645618825","product_id":100105701,"comment_content":"老师，后续可否加上物理部署图，可能理解会更深刻一些，比如几个机房，每个机房怎么样的","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552955,"discussion_content":"现实中，会在项目前期做高并发的架构设计与开发，但是不会做高并发的物理部署，一则用户量是逐渐增加的，开始的时候不知道需要部署多少服务器，其次服务器的并发承载能力是压测和不断优化得到的，在设计期并不知道。物理部署图在设计阶段没有意义和价值。\n\n多机房部署也是一种设计，后面有案例专门讨论。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645666824,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}