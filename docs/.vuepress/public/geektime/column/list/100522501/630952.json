{"id":630952,"title":"18｜组件监控：Kubernetes 控制面组件的关键指标与数据采集","content":"<p>你好，我是秦晓辉。</p><p>上一讲我们介绍了 Kubernetes <strong>工作负载节点</strong>的监控，了解了 Kube-Proxy、Kubelet、容器负载监控的方法。这一讲我们介绍<strong>控制面组件</strong>的监控，包括 APIServer、Controller-manager（简称CM）、Scheduler、etcd 四个组件，我会讲解这几个组件监控数据的采集方法和关键指标，并给出监控大盘。此外，我们还会学习如何使用 kube-state-metrics（简称KSM）来监控Kubernetes 的各类对象。</p><h2>数据采集</h2><p>自行搭建 Kubernetes 控制面的朋友，大都是选择 Kubeadm 这样的工具，Kubeadm 会把控制面的组件以静态容器的方式放到容器里运行，之后我会重点给你演示在这种部署方式下如何采集监控数据。</p><p>不过很多大一些的互联网公司会选择直接使用二进制的方式来部署，因为二进制的方式对于监控数据采集来说其实更简单，直接在采集器里配置要抓取的这几个组件的目标地址就可以了。</p><p>如果想要调用 Kubernetes 服务端 APIServer、Controller-manager、Scheduler 这三个组件的 <code>/metrics</code> 接口，需要有 Token 做鉴权，我们还是通过创建 ServiceAccount 的方式拿到 Token，后面也会把采集器直接部署到 Kubernetes 容器里，这样 Token 信息就可以自动 mount 到容器里，比较方便。</p><!-- [[[read_end]]] --><h3>创建认证信息</h3><p>相比前面为 Categraf 准备的 ServiceAccount，用于访问控制面组件的 ServiceAccount 会要求更多权限，这里我重新给出一个升级后的 YAML 文件，供你参考。</p><pre><code class=\"language-yaml\">---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: categraf\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n      - nodes/metrics\n      - nodes/stats\n      - nodes/proxy\n      - services\n      - endpoints\n      - pods\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups:\n      - extensions\n      - networking.k8s.io\n    resources:\n      - ingresses\n    verbs: [\"get\", \"list\", \"watch\"]\n  - nonResourceURLs: [\"/metrics\", \"/metrics/cadvisor\"]\n    verbs: [\"get\"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: categraf\n  namespace: flashcat\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: categraf\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: categraf\nsubjects:\n- kind: ServiceAccount\n  name: categraf\n  namespace: flashcat\n</code></pre><p>使用 <code>kubectl apply</code> 一下这个YAML 文件即可。有了认证信息，后面就是选型并部署采集器了。</p><h3>部署采集器</h3><p>我们希望能够自动感知到组件实例的变化，也就是要抓取的目标地址的变化。毫无疑问要读取 Kubernetes 的元信息，就需要具备类似 Prometheus 的 Kubernetes 服务发现能力。支持这个服务发现能力的采集器，比较常用的是 Telegraf、Prometheus、Categraf、Grafana-agent、vmagent 等，这里最原汁原味的显然是 Prometheus 自身。</p><p>Prometheus 从 v2.32.0 开始支持 agent mode 模式，相当于把 Prometheus 当做一个采集 agent 来使用，只负责采集数据，这个玩法最为简便清晰，我们就使用这个方式来采集数据。</p><p>agent mode 模式的 Prometheus，重点要配置 scrape 规则和 remote write 地址，我们把配置文件做成 ConfigMap，你可以看一下具体的 YAML 文件。</p><pre><code class=\"language-yaml\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n&nbsp; name: prometheus-agent-conf\n&nbsp; labels:\n&nbsp; &nbsp; name: prometheus-agent-conf\n&nbsp; namespace: flashcat\ndata:\n&nbsp; prometheus.yml: |-\n&nbsp; &nbsp; global:\n&nbsp; &nbsp; &nbsp; scrape_interval: 15s\n&nbsp; &nbsp; &nbsp; evaluation_interval: 15s\n\n&nbsp; &nbsp; scrape_configs:\n&nbsp; &nbsp; &nbsp; - job_name: 'apiserver'\n&nbsp; &nbsp; &nbsp; &nbsp; kubernetes_sd_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - role: endpoints\n&nbsp; &nbsp; &nbsp; &nbsp; scheme: https\n&nbsp; &nbsp; &nbsp; &nbsp; tls_config:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; insecure_skip_verify: true\n&nbsp; &nbsp; &nbsp; &nbsp; authorization:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n&nbsp; &nbsp; &nbsp; &nbsp; relabel_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action: keep\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; regex: default;kubernetes;https\n\n&nbsp; &nbsp; &nbsp; - job_name: 'controller-manager'\n&nbsp; &nbsp; &nbsp; &nbsp; kubernetes_sd_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - role: endpoints\n&nbsp; &nbsp; &nbsp; &nbsp; scheme: https\n&nbsp; &nbsp; &nbsp; &nbsp; tls_config:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; insecure_skip_verify: true\n&nbsp; &nbsp; &nbsp; &nbsp; authorization:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n&nbsp; &nbsp; &nbsp; &nbsp; relabel_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action: keep\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; regex: kube-system;kube-controller-manager;https\n\n&nbsp; &nbsp; &nbsp; - job_name: 'scheduler'\n&nbsp; &nbsp; &nbsp; &nbsp; kubernetes_sd_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - role: endpoints\n&nbsp; &nbsp; &nbsp; &nbsp; scheme: https\n&nbsp; &nbsp; &nbsp; &nbsp; tls_config:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; insecure_skip_verify: true\n&nbsp; &nbsp; &nbsp; &nbsp; authorization:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n&nbsp; &nbsp; &nbsp; &nbsp; relabel_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action: keep\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; regex: kube-system;kube-scheduler;https\n\n&nbsp; &nbsp; &nbsp; - job_name: 'etcd'\n&nbsp; &nbsp; &nbsp; &nbsp; kubernetes_sd_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - role: endpoints\n&nbsp; &nbsp; &nbsp; &nbsp; scheme: http\n&nbsp; &nbsp; &nbsp; &nbsp; relabel_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action: keep\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; regex: kube-system;etcd;http\n\n&nbsp; &nbsp; &nbsp; - job_name: 'kube-state-metrics'\n&nbsp; &nbsp; &nbsp; &nbsp; kubernetes_sd_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - role: endpoints\n&nbsp; &nbsp; &nbsp; &nbsp; scheme: http\n&nbsp; &nbsp; &nbsp; &nbsp; relabel_configs:\n&nbsp; &nbsp; &nbsp; &nbsp; - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; action: keep\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; regex: kube-system;kube-state-metrics;http-metrics\n\n&nbsp; &nbsp; remote_write:\n&nbsp; &nbsp; - url: 'http://10.206.0.16:19000/prometheus/v1/write'\n</code></pre><p>我来简单介绍一下，这段代码中包含了 5 个抓取 job，分别是 APIServer、Controller-manager、Scheduler、ectd、KSM。前面 3 个走的是 HTTPS ，后面两个走的是 HTTP，重点关注 relabel 规则。keep 的规则实际就是在做过滤，只过滤自己 job 感兴趣的那些 endpoint。最后两行配置了 remote write 地址，采集到的数据通过 remote write 协议推给远端，我这里是推给了 n9e-server，n9e-server 后面是 VictoriaMetrics 集群。</p><p>准备好配置文件之后，接下来部署 Prometheus。因为只是抓取几个服务端组件，抓取量不大，不用做分片，我这里把 Prometheus agent mode 做成单副本的 Deployment，你可以看一下我给出的 YAML 文件。</p><pre><code class=\"language-yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n&nbsp; name: prometheus-agent\n&nbsp; namespace: flashcat\n&nbsp; labels:\n&nbsp; &nbsp; app: prometheus-agent\nspec:\n&nbsp; replicas: 1\n&nbsp; selector:\n&nbsp; &nbsp; matchLabels:\n&nbsp; &nbsp; &nbsp; app: prometheus-agent\n&nbsp; template:\n&nbsp; &nbsp; metadata:\n&nbsp; &nbsp; &nbsp; labels:\n&nbsp; &nbsp; &nbsp; &nbsp; app: prometheus-agent\n&nbsp; &nbsp; spec:\n&nbsp; &nbsp; &nbsp; serviceAccountName: categraf\n&nbsp; &nbsp; &nbsp; containers:\n&nbsp; &nbsp; &nbsp; &nbsp; - name: prometheus\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; image: prom/prometheus\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; args:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - \"--config.file=/etc/prometheus/prometheus.yml\"\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - \"--web.enable-lifecycle\"\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - \"--enable-feature=agent\"\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ports:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - containerPort: 9090\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; resources:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; requests:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cpu: 500m\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory: 500M\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; limits:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cpu: 1\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory: 1Gi\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; volumeMounts:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - name: prometheus-config-volume\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mountPath: /etc/prometheus/\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - name: prometheus-storage-volume\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mountPath: /prometheus/\n&nbsp; &nbsp; &nbsp; volumes:\n&nbsp; &nbsp; &nbsp; &nbsp; - name: prometheus-config-volume\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; configMap:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; defaultMode: 420\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; name: prometheus-agent-conf\n&nbsp; &nbsp; &nbsp; &nbsp; - name: prometheus-storage-volume\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; emptyDir: {}\n</code></pre><p>这里要注意的关键点，一个是 serviceAccountName，配置是 categraf，和前面创建的 ServiceAccount 对应，另一个是 args 部分，给出了相关的启动参数，<code>--enable-feature=agent</code> 就是作为 agent mode 模式运行。</p><p>最后，执行 <code>kubectl apply -f prometheus-agent-deployment.yaml</code>，让 Kubernetes 把 Deployment 拉起来就可以了。稍等片刻，去页面上查询一下 APIServer 的监控数据，理论上是可以查到的，但是其他组件的监控数据，大概率是没有的，下面我们来一一修复。</p><h3>修复Controller-manager和Scheduler</h3><p>上面的抓取规则走的都是 Kubernetes 服务发现机制，发现 endpoint，然后过滤。我们先通过下面的命令查看一下 kube-system 这个 namespace 下有哪些 endpoint。</p><pre><code class=\"language-yaml\">kubectl get endpoints -n kube-system\n</code></pre><p>如果有 kube-controller-manager、kube-scheduler 这两个 endpoint，理论上通过上面的抓取规则就可以抓到数据，如果没有的话，我们可以创建相关的 service。</p><p>你可以参考我给出的这两个 YAML 文件来创建 service。</p><ul>\n<li><a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/controller-service.yaml\">https://github.com/flashcatcloud/categraf/blob/main/k8s/controller-service.yaml</a></li>\n<li><a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/scheduler-service.yaml\">https://github.com/flashcatcloud/categraf/blob/main/k8s/scheduler-service.yaml</a></li>\n</ul><p>另外就是得确保 Controller-manager 和 Scheduler 没有监听在 127.0.0.1，否则采集器落在其他机器上就访问不通了。具体怎么做呢？</p><p>在这两个组件的启动参数里加上 <code>--bind-address=0.0.0.0</code> 就可以了。如果是 Kubeadm 安装的，可以在 /etc/kubernetes/manifests/kube-controller-manager.yaml 和 /etc/kubernetes/manifests/kube-scheduler.yaml 里调整参数。调整完之后，理论上就可以抓到数据了。</p><h3>修复 etcd</h3><p>etcd 默认端口是 2379，如果从这个端口获取监控数据，就需要有比较复杂的认证鉴权。但其实etcd的监控数据也不是什么太关键的信息，而且是内网，直接开放就可以了。etcd提供了一个启动参数，可以为暴露监控指标单独监听一个地址。</p><p>具体参数是：</p><pre><code class=\"language-yaml\">--listen-metrics-urls=http://0.0.0.0:2381\n</code></pre><p>之后创建相关的 <a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/etcd-service-http.yaml\">service</a> ，prometheus agent 就可以发现这个 HTTP 的 endpoint 了。</p><h3>修复 KSM</h3><p>KSM 是监控各类 Kubernetes 对象的组件，通过 KSM 我们可以知道 Service、Deployment、Statefulset、Node 等组件的各类元信息，比如某个 Deployment 期望有几个副本、实际有几个 Pod 在运行这种问题，就是靠 KSM 来回答的。KSM 是如何知道这些信息的呢？它需要跟 APIServer 通信，订阅各类资源对象的变更。下面我们安装一下 KSM，相关指标就有了。</p><pre><code class=\"language-yaml\">git clone https://github.com/kubernetes/kube-state-metrics\nkubectl apply -f kube-state-metrics/examples/standard/\n</code></pre><p>KSM 在代码仓库里提供了相关的 YAML 文件，我们 clone 下来直接 apply 就可以。KSM 默认暴露了两个端口，8080 用于返回各类 Kubernetes 对象信息，8081 用于返回 KSM 自身的指标，我们在抓取规则里重点抓取的是 8080 的数据。</p><p>KSM 要返回所有 Kubernetes 对象的指标，数据量比较大，从 8080 拉取监控数据可能会拉取十几秒甚至几十秒，KSM 为此支持了分片逻辑，examples/standard 下面提供的 YAML 文件是把 KSM 部署为单副本的 Deployment，分片的话使用 Daemonset，每个 Node 上都跑一个 KSM，这个 KSM 只同步与自身节点相关的数据，KSM 的官方 README 里说得很清楚了，你可以看一下Daemonset 样例。</p><pre><code class=\"language-yaml\">apiVersion: apps/v1\nkind: DaemonSet\nspec:\n  template:\n    spec:\n      containers:\n      - image: registry.k8s.io/kube-state-metrics/kube-state-metrics:IMAGE_TAG\n        name: kube-state-metrics\n        args:\n        - --resource=pods\n        - --node=$(NODE_NAME)\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n</code></pre><p>另外，KSM 提供了两种方式来过滤要 watch 的对象类型，一个是通过白名单的方式指定具体要 watch 哪类对象，通过命令行启动参数中的 <code>--resources=daemonsets,deployments</code>，表示只 watch daemonsets 和 deployments。虽然已经限制了对象资源类型，但如果采集的某些指标仍然不想要，可以采用黑名单的方式来过滤指标：<code>--metric-denylist=kube_deployment_spec_.*</code>。这个过滤规则支持正则写法，多个正则之间可以使用逗号分隔。</p><p>做完这些操作之后，我们就可以采集到这些组件的监控数据了，下面我们继续看哪些指标更为关键。</p><h2>关键指标</h2><p>Categraf 的代码仓库里已经内置了 Kubernetes 各个组件的监控大盘，只要是出现在监控大盘上的指标，理论上就是相对比较重要的，要不然也没有必要放到大盘上了。你可以看一下 <a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/apiserver-dash.json\">APIServer</a>、<a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/cm-dash.json\">Controller-manager</a>、<a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/scheduler-dash.json\">Scheduler</a>、<a href=\"https://github.com/flashcatcloud/categraf/blob/main/k8s/etcd-dash.json\">etcd</a>、<a href=\"https://github.com/flashcatcloud/categraf/tree/main/inputs/kube_state_metrics\">KSM</a> 的大盘。</p><p>如果你使用 Grafana 来做可视化，可以参考下面两个项目中提供的 Dashboard。</p><ul>\n<li><a href=\"https://github.com/kubernetes-monitoring/kubernetes-mixin\">https://github.com/kubernetes-monitoring/kubernetes-mixin</a></li>\n<li><a href=\"https://github.com/dotdc/grafana-dashboards-kubernetes\">https://github.com/dotdc/grafana-dashboards-kubernetes</a></li>\n</ul><p>因为所有组件都是 Go 实现的，都暴露了 Go 程序通用的那些 CPU、内存、Goroutine、句柄等指标，这一部分内容我们在上一讲已经介绍过，这里不再赘述。下面我们分别看一下这几个组件一些其他类型的关键指标。</p><h3>APIServer</h3><p>APIServer 的核心职能是 Kubernetes 集群的 API 总入口，Kube-Proxy、Kubelet、Controller-Manager、Scheduler 等都需要调用 APIServer，所以 APIServer 的监控，完全按照 RED 方法论来梳理即可，最核心的就是请求吞吐和延迟。</p><ul>\n<li>apiserver_request_total：请求量的指标，可以统计每秒请求数、成功率。</li>\n<li>apiserver_request_duration_seconds：请求耗时的指标。</li>\n<li>apiserver_current_inflight_requests：APIServer 当前处理的请求数，分为 mutating（非 get、list、watch的请求）和 readOnly（get、list、watch请求）两种，请求量过大就会被限流，所以这个指标对我们观察容量水位很有帮助。</li>\n</ul><h3>Controller-manager</h3><p>Controller-manager 负责监听对象状态，并与期望状态做对比。如果状态不一致则进行调谐，重点关注的是<strong>任务数量、队列深度</strong>等。</p><ul>\n<li>workqueue_adds_total：各个 controller 接收到的任务总数。</li>\n<li>workqueue_depth：各个 controller 的队列深度，表示各个 controller 中的任务的数量，数量越大表示越繁忙。</li>\n<li>workqueue_queue_duration_seconds：任务在队列中的等待耗时，按照控制器分别统计。</li>\n<li>workqueue_work_duration_seconds：任务出队到被处理完成的时间，按照控制器分别统计。</li>\n<li>workqueue_retries_total：任务进入队列的重试次数。</li>\n</ul><h3>Scheduler</h3><p>Scheduler 在 Kubernetes 架构中负责把对象调度到合适的 Node 上，在这个过程中会有一系列的规则计算和筛选，重点关注<strong>调度</strong>这个动作的相关指标。</p><ul>\n<li>leader_election_master_status：调度器的选主状态，1表示master，0表示backup。</li>\n<li>scheduler_queue_incoming_pods_total：进入调度队列的 Pod 数量。</li>\n<li>scheduler_pending_pods：Pending 的 Pod 数量。</li>\n<li>scheduler_pod_scheduling_attempts：Pod 调度成功前，调度重试的次数分布。</li>\n<li>scheduler_framework_extension_point_duration_seconds：调度框架的扩展点延迟分布，按 extension_point 统计。</li>\n<li>scheduler_schedule_attempts_total：按照调度结果统计的尝试次数，“unschedulable”表示无法调度，“error”表示调度器内部错误。</li>\n</ul><h3>etcd</h3><p>etcd在 Kubernetes 的架构中作用巨大，相对也比较稳定，不过 etcd对硬盘 IO 要求较高，因此需要着重关注 IO 相关的指标，生产环境建议至少使用 SSD 的盘做存储。</p><ul>\n<li>etcd_server_has_leader ：etcd是否有 leader。</li>\n<li>etcd_server_leader_changes_seen_total：偶尔切主问题不大，频繁切主就要关注了。</li>\n<li>etcd_server_proposals_failed_total：提案失败次数。</li>\n<li>etcd_disk_backend_commit_duration_seconds：提交花费的耗时。</li>\n<li>etcd_disk_wal_fsync_duration_seconds  ：wal日志同步耗时。</li>\n</ul><h3>KSM</h3><p>Kube-state-metrics 这个组件，采集的很多指标都只是充当元信息，单独拿出来未必那么有用，但是和其他指标做 group_left、group_right 连接的时候可能又会很有用，还记得<a href=\"https://time.geekbang.org/column/article/623851\">第 6 讲</a>介绍的那个按照 version 绘制饼图的例子吗？那就是个典型用法。下面我挑选一些相对常用的指标解释一下。</p><ul>\n<li>kube_node_status_condition：Node 节点状态，状态不正常、有磁盘压力等都可以通过这个指标发现。</li>\n<li>kube_pod_container_status_last_terminated_reason：容器停止原因。</li>\n<li>kube_pod_container_status_waiting_reason：容器处于 waiting 状态的原因。</li>\n<li>kube_pod_container_status_restarts_total：容器重启次数。</li>\n<li>kube_deployment_spec_replicas：deployment配置期望的副本数。</li>\n<li>kube_deployment_status_replicas_available：deployment 实际可用的副本数。</li>\n</ul><p>基于 KSM 数据的比较典型的告警规则，我也举个例子，让你有一个直观的认识。</p><pre><code class=\"language-yaml\"># 长时间版本不一致需要告警\nkube_deployment_status_observed_generation{job=\"kube-state-metrics\"}\n!=\nkube_deployment_metadata_generation{job=\"kube-state-metrics\"}\n\n# deployment 副本数不一致\n(\nkube_deployment_spec_replicas{job=\"kube-state-metrics\"}\n!=\nkube_deployment_status_replicas_available{job=\"kube-state-metrics\"}\n)\nand\n(\nchanges(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\"}[5m]) == 0\n)\n\n# 容器有 Error 或者 OOM 导致的退出\n(sum(kube_pod_container_status_last_terminated_reason{reason=~\"Error|OOMKilled\"}) by (namespace,pod,container) &gt; 0)\n* on(namespace,pod,container)&nbsp;&nbsp;\nsum(increase(kube_pod_container_status_restarts_total[10m]) &gt; 0) by(namespace,pod,container)\n</code></pre><p>上面我只是举了Deployment的例子，Statefulset 也是类似的。到这里控制面的核心组件以及KSM相关的知识就讲完了，下面我们做个总结。</p><h2>小结</h2><p>Kubernetes 体系确实非常庞大，这一讲我们重点介绍控制面的组件监控，包括 APIServer、Controller-manager、Scheduler、etcd等。当然，Kubernetes 对象的监控也很关键，可以使用 KSM 完成。</p><p>核心内容主要是两部分，一个是数据采集，一个是关键指标。数据采集我们引入了 prometheus agent mode，支持 Kubernetes 服务发现，非常轻量，通过 remote write 协议把数据推给后端存储。关键指标的话需要看各个模块的核心职能以及重点依赖，比如 Scheduler 是做调度的，那就要看调度相关的指标，etcd强依赖硬盘，就要多关注硬盘 IO 相关的指标。</p><p>Kubernetes 控制面的组件全部都要认证鉴权，相比之前演示的 Kubelet 数据采集，需要给更多的权限。Controller-manager、Scheduler 可能默认没有创建 Service，需要手工创建并且修改监听地址，etcd要开启 HTTP 协议的指标暴露端口，这些都是坑，需要依次修复。</p><p>关键指标部分我提供了一些最为常见的指标说明，你也可以参考监控大盘中的配置，指标既然放到大盘中了，就表示相对重要。不得不说，监控大盘是一个很好的知识沉淀的手段。</p><p><img src=\"https://static001.geekbang.org/resource/image/c1/cd/c183a43526b4bea24c42d6fec3774ccd.jpg?wh=3623x3600\" alt=\"\"></p><h2>互动时刻</h2><p>在 Prometheus 的抓取配置中，我们给出了几个抓取 Job 的配置。如果一个 Kubernetes 集群的数据只写入一个时序库，这样的配置是没问题，如果多个 Kubernetes 集群的数据都写入一个时序库，就要通过额外的标签来区分了。你知道如何为抓取的数据附加新的标签吗？欢迎在评论区留下你的思考，也欢迎你把今天的内容分享给你身边的朋友，邀他一起学习。我们下一讲再见！</p>","comments":[{"had_liked":false,"id":368701,"user_name":"Geek_1a3949","can_delete":false,"product_type":"c1","uid":2659191,"ip_address":"上海","ucode":"98113FDBBAAEBC","user_header":"","comment_is_top":false,"comment_ctime":1676600259,"is_pvip":false,"replies":[{"id":134317,"content":"量小的话sts即可，量大的话得daemonset","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1676716405,"ip_address":"北京","comment_id":368701,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"尝试回答一下课后题：\n\n可以为prometheus增加global.external_labels配置，增加cluster的标识以区分不同的集群：\nglobal:\n  external_labels:\n    cluster:  prod-bigdata-sh\n    ....\n\n另外，请教老师一个问题，ksm的分片，官网上有statefulset、daemonset的分片方式，它们各自的适用场景是什么，在生产环境下，更推荐哪种方式？\n\n","like_count":4,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":605544,"discussion_content":"量小的话sts即可，量大的话得daemonset","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1676716405,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":370480,"user_name":"胡飞","can_delete":false,"product_type":"c1","uid":3211245,"ip_address":"上海","ucode":"D8B4E504C7C9BF","user_header":"https://static001.geekbang.org/account/avatar/00/30/ff/ed/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1678867888,"is_pvip":false,"replies":[{"id":135128,"content":"是的","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1678927426,"ip_address":"北京","comment_id":370480,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"你好老师，promtheus 如果开启了remote write后，存储会用两份吗？prom本地一份数据，第三方一份数据？","like_count":1,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":609287,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1678927427,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":368763,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1676679248,"is_pvip":false,"replies":[{"id":134316,"content":"1，不是，但可以看做是伴生的重要性\n2，有些日志有政策审计要求，有些只查问题，有些用于做数据统计，日志保存时间都不一样，指标的话默认15天即可，一些业务指标可能会需要永久保存","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1676716208,"ip_address":"北京","comment_id":368763,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"请教老师几个问题：\nQ1：KSM是k8s自身的组件吗？还是一个第三方的软件？\nQ2：一般性的问题，公司的实际运营中，日志数据一般保存多长时间？指标数据一般保存多长时间？（到期后是直接删除数据吧）","like_count":1,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":605543,"discussion_content":"1，不是，但可以看做是伴生的重要性\n2，有些日志有政策审计要求，有些只查问题，有些用于做数据统计，日志保存时间都不一样，指标的话默认15天即可，一些业务指标可能会需要永久保存","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1676716209,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1261493,"avatar":"https://static001.geekbang.org/account/avatar/00/13/3f/b5/5fe77e16.jpg","nickname":"不经意间","note":"","ucode":"C39D98697ACB8B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625073,"discussion_content":"目前我们用的zabbix是保留365天历史数据。\n今年看去年同一时间的数据也挺有意思，一整年的数据都开出来看也能看出点东西的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1691285578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":378064,"user_name":"姜兵","can_delete":false,"product_type":"c1","uid":2006967,"ip_address":"北京","ucode":"E4CCFBAC5797DE","user_header":"https://static001.geekbang.org/account/avatar/00/1e/9f/b7/8b1c1b3b.jpg","comment_is_top":false,"comment_ctime":1689727405,"is_pvip":false,"replies":[{"id":138029,"content":"一般15秒就算很高了","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1690811369,"ip_address":"北京","comment_id":378064,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"老师您好，想问一下，生产上的各类指标开启秒级采集的话，一般最小设置为多少秒可以确保采集性能和告警的及时性？","like_count":0,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":624665,"discussion_content":"一般15秒就算很高了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1690811369,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":377408,"user_name":"k8s卡拉米","can_delete":false,"product_type":"c1","uid":2827661,"ip_address":"北京","ucode":"D37A1C96281420","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJiaxxRyl13SvqsqWuhtJHWMVRMeIo7byfJ0AaicwcRvibcfw0DSrGHFVz7dhwicBJNsFSFRk4kuia28jQ/132","comment_is_top":false,"comment_ctime":1688456123,"is_pvip":false,"replies":[{"id":137548,"content":"agent mode 模式的 prometheus 仅仅作为采集器，中心端时序存储建议使用 VictoriaMetrics 或 Thanos","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1688519071,"ip_address":"北京","comment_id":377408,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"老师您好，采集work组件是后，您这篇文章中使用的是把prometheus当做agent，部署的这个采集的prometheus 是仅仅做采集使用吗？，我在其他机器上已经部署了prometheus用户和n9e和这个采集的prometheus 没关系是吗？","like_count":0,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":622650,"discussion_content":"agent mode 模式的 prometheus 仅仅作为采集器，中心端时序存储建议使用 VictoriaMetrics 或 Thanos","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1688519071,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":369507,"user_name":"晴空万里","can_delete":false,"product_type":"c1","uid":1181835,"ip_address":"广东","ucode":"6470D4B84A4D8E","user_header":"https://static001.geekbang.org/account/avatar/00/12/08/8b/1b7d0463.jpg","comment_is_top":false,"comment_ctime":1677628867,"is_pvip":false,"replies":[{"id":134673,"content":"只用了一个云的话，其实直接用云的监控就可以了，未来要是有多云或者混合云的场景，再考虑自建监控。如果自己搞监控来监控K8s，主要关注工作负载节点就可以了，托管的K8s控制面组件的可靠性让云厂商来做","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1677644483,"ip_address":"北京","comment_id":369507,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"自己做监控系统 但是用了公有云产品 比如华为云的k8s 请问怎么监控哈 公司都是用公有云saas 自己公司卖paas产品","like_count":0,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":607154,"discussion_content":"只用了一个云的话，其实直接用云的监控就可以了，未来要是有多云或者混合云的场景，再考虑自建监控。如果自己搞监控来监控K8s，主要关注工作负载节点就可以了，托管的K8s控制面组件的可靠性让云厂商来做","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1677644483,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":369178,"user_name":"LiLian","can_delete":false,"product_type":"c1","uid":1509119,"ip_address":"广东","ucode":"9807877F1A9224","user_header":"https://static001.geekbang.org/account/avatar/00/17/06/ff/e4828765.jpg","comment_is_top":false,"comment_ctime":1677204733,"is_pvip":false,"replies":[{"id":134505,"content":"是","user_name":"作者回复","user_name_real":"编辑","uid":1001078,"ctime":1677220965,"ip_address":"北京","comment_id":369178,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"请问老师：&quot;prometheus agent mode，支持 Kubernetes 服务发现&quot;  本质上还是通过list &amp; watch 监听来自api server的信息吗？ ","like_count":0,"discussions":[{"author":{"id":1001078,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/46/76/67e111da.jpg","nickname":"巴辉特","note":"","ucode":"DCBB150A99548D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":606566,"discussion_content":"是","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1677220965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":376751,"user_name":"Geek_7656a8","can_delete":false,"product_type":"c1","uid":3646181,"ip_address":"北京","ucode":"34BD2F7027551A","user_header":"","comment_is_top":false,"comment_ctime":1687318420,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100522501,"comment_content":"老师好； 采集到集群调度信息，但是 scheduler (0 &#47; 8767 active targets)；这是什么情况？ 数据没有推到远端的prometheus","like_count":0}]}