{"id":101735,"title":"25 | Transform：Beam数据转换操作的抽象方法","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“Beam数据转换操作的抽象方法”。</p><p>在上一讲中，我们一起学习了Beam中数据的抽象表达——PCollection。但是仅仅有数据的表达肯定是无法构建一个数据处理框架的。那么今天，我们就来看看Beam中数据处理的最基本单元——Transform。</p><p>下图就是单个Transform的图示。</p><p><img src=\"https://static001.geekbang.org/resource/image/cc/66/cc1266a6749cdae13426dd9721f66e66.jpg?wh=1430*694\" alt=\"\"></p><p>之前我们已经讲过，Beam把数据转换抽象成了有向图。PCollection是有向图中的边，而Transform是有向图里的节点。</p><p>不少人在理解PCollection的时候都觉得这不那么符合他们的直觉。许多人都会自然地觉得PCollection才应该是节点，而Transform是边。因为数据给人的感觉是一个实体，应该用一个方框表达；而边是有方向的，更像是一种转换操作。事实上，这种想法很容易让人走入误区。</p><p>其实，<span class=\"orange\">区分节点和边的关键是看一个Transform是不是会有一个多余的输入和输出</span>。</p><p>每个Transform都可能有大于一个的输入PCollection，它也可能输出大于一个的输出PCollection。所以，我们只能把Transform放在节点的位置。因为一个节点可以连接多条边，而同一条边却只能有头和尾两端。</p><!-- [[[read_end]]] --><h2>Transform的基本使用方法</h2><p>在了解了Transform和PCollection的关系之后，我们来看一下Transform的基本使用方法。</p><p>Beam中的PCollection有一个抽象的成员函数Apply。使用任何一个Transform时候，你都需要调用这个apply方法。</p><p>Java</p><pre><code>pcollection1 = pcollection2.apply(Transform)\n</code></pre><p>Python</p><pre><code>Pcollection1 = pcollection2 | Transform\n</code></pre><p>当然，你也可以把Transform级连起来。</p><pre><code>final_collection = input_collection.apply(Transform1)\n.apply(Transform2)\n.apply(Transform3)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/41/ec/41a408a0153844036909e98dd2cefaec.jpg?wh=2160*624\" alt=\"\"></p><p>所以说，Transform的调用方法是要通过apply()的，但是Transform有很多种。</p><h2>常见的Transform</h2><p>Beam也提供了常见的Transform接口，比如ParDo、GroupByKey。最常使用的Transform就是ParDo了。</p><p>ParDo就是 Parallel Do的意思，顾名思义，表达的是很通用的并行处理数据操作。GroupByKey的意思是把一个Key/Value的数据集按Key归并，就如下面这个例子。</p><pre><code>cat, 1\ndog, 5\nand, 1\njump, 3\ntree, 2\ncat, 5\ndog, 2\nand, 2\ncat, 9\nand, 6\n\n=&gt;\n\ncat, [1,5,9]\ndog, [5,2]\nand, [1,2,6]\njump, [3]\ntree, [2]\n</code></pre><p>当然，你也可以用ParDo来实现GroupByKey，一种简单的实现方法就是放一个全局的哈希表，然后在ParDo里把一个一个元素插进这个哈希表里。但这样的实现方法并不能用，因为你的数据量可能完全无法放进一个内存哈希表。而且，你还要考虑到PCollection会把计算分发到不同机器上的情况。</p><p>当你在编写ParDo时，你的输入是一个PCollection中的单个元素，输出可以是0个、1个，或者是多个元素。你只要考虑好怎样处理一个元素。剩下的事情，Beam会在框架层面帮你做优化和并行。</p><p>使用ParDo时，你需要继承它提供的DoFn类，你可以把DoFn看作是ParDo的一部分。因为ParDo和DoFn单独拿出来都没有意义。</p><p>java</p><pre><code>static class UpperCaseFn extends DoFn&lt;String, String&gt; {\n  @ProcessElement\n  public void processElement(@Element String word, OutputReceiver&lt;String&gt; out) {\n    out.output(word.toUpperCase());\n  }\n}\n\nPCollection&lt;String&gt; upperCaseWords = words.apply(\n    ParDo\n    .of(new UpperCaseFn())); \n</code></pre><p>在上面的代码中你可以看出，每个DoFn的@ProcessElement标注的函数processElement，就是这个DoFn真正的功能模块。在上面这个DoFn中，我们把输入的一个词转化成了它的大写形式。之后在调用apply(ParDo.of(new UpperCaseFn()))的时候，Beam就会把输入的PCollection中的每个元素都使用刚才的processElement处理一遍。</p><p>看到这里，你可能会比较迷惑，transform、apply、DoFn、ParDo之间到底是什么关系啊？怎么突然冒出来一堆名词？其实，Transform是一种概念层面的说法。具体在编程上面，Transform用代码来表达的话就是这样的：</p><pre><code>pcollection.apply(ParDo.of(new DoFn()))\n</code></pre><p>这里的apply(ParDo)就是一个Transform。</p><p>我们在<a href=\"https://time.geekbang.org/column/article/92928\">第7讲</a>中讲过数据处理流程的常见设计模式。事实上很多应用场景都可以用ParDo来实现。比如过滤一个数据集、格式转化一个数据集、提取一个数据集的特定值等等。</p><p><strong>1.过滤一个数据集</strong></p><p>当我们只想要挑出符合我们需求的元素的时候，我们需要做的，就是在processElement中实现。一般来说会有一个过滤函数，如果满足我们的过滤条件，我们就把这个输入元素输出。</p><p>Java</p><pre><code>@ProcessElement\npublic void processElement(@Element T input, OutputReceiver&lt;T&gt; out) {\n    if (IsNeeded(input)) {\n      out.output(input);\n    }\n  }\n</code></pre><p><strong>2.格式转化一个数据集</strong></p><p>给数据集转化格式的场景非常常见。比如，我们想把一个来自csv文件的数据，转化成TensorFlow的输入数据tf.Example的时候，就可以用到ParDo。</p><p>Java</p><pre><code>@ProcessElement\n  public void processElement(@Element String csvLine, OutputReceiver&lt;tf.Example&gt; out) {\n    out.output(ConvertToTfExample(csvLine));\n  }\n</code></pre><p><strong>3.提取一个数据集的特定值</strong></p><p>ParDo还可以提取一个数据集中的特定值。比如，当我们想要从一个商品的数据集中提取它们的价格的时候，也可以使用ParDo。</p><p>Java</p><pre><code>@ProcessElement\n  public void processElement(@Element Item item, OutputReceiver&lt;Integer&gt; out) {\n    out.output(item.price());\n  }\n</code></pre><p>通过前面的几个例子你可以看到，ParDo和DoFn这样的抽象已经能处理非常多的应用场景问题。事实正是如此，在实际应用中，80%的数据处理流水线都是使用基本的ParDo和DoFn。</p><h2>Stateful Transform和side input/side output</h2><p>当然，还有一些Transform其实也是很有用的，比如GroupByKey，不过它远没有ParDo那么常见。所以，这一模块中暂时不会介绍别的数据转换操作，需要的话我们可以在后面用到的时候再介绍。我想先在这里介绍和ParDo同样是必用的，却在大部分教程中被人忽略的技术点——Statefullness和side input/side output。</p><p>上面我们所介绍的一些简单场景都是无状态的，也就是说，在每一个DoFn的processElement函数中，输出只依赖于输入。它们的DoFn类不需要维持一个成员变量。无状态的DoFn能保证最大的并行运算能力。因为DoFn的processElement可以分发到不同的机器，或者不同的进程也能有多个DoFn的实例。但假如我们的processElement的运行需要另外的信息，我们就不得不转而编写有状态的DoFn了。</p><p>试想这样一个场景，你的数据处理流水线需要从一个数据库中根据用户的id找到用户的名字。你可能会想到用“在DoFn中增加一个数据库的成员变量”的方法来解决。的确，实际的应用情况中我们就会写成下面这个代码的样子。</p><p>java</p><pre><code>static class FindUserNameFn extends DoFn&lt;String, String&gt; {\n  @ProcessElement\n  public void processElement(@Element String userId, OutputReceiver&lt;String&gt; out) {\n    out.output(database.FindUserName(userId));\n  }\n\n  Database database;\n}\n</code></pre><p>但是因为有了共享的状态，这里是一个共享的数据库连接。在使用有状态的DoFn时，我们需要格外注意Beam的并行特性。</p><p>如上面讲到的，Beam不仅会把我们的处理函数分发到不同线程、进程，也会分发到不同的机器上执行。当你共享这样一个数据库的读取操作时，很可能引发服务器的QPS过高。</p><p>例如，你在处理一个1万个用户id，如果beam很有效地将你的DoFn并行化了，你就可能观察到数据库的QPS增加了几千。如果你不仅是读取，还做了修改的话，就需要注意是不是有竞争风险了。这里你可以联想在操作系统中有关线程安全的相关知识。</p><p>除了这种简单的增加一个成员变量的方法。如果我们需要共享的状态来自于另外一些Beam的数据处理的中间结果呢？这时候为了实现有状态DoFn我们需要应用Beam的Side input/side output计数。</p><p>java</p><pre><code>PCollectionView&lt;Integer&gt; mediumSpending = ...;\n\nPCollection&lt;String&gt; usersBelowMediumSpending =\n  userIds.apply(ParDo\n      .of(new DoFn&lt;String, String&gt;() {\n          @ProcessElement\n          public void processElement(@Element String userId, OutputReceiver&lt;String&gt; out, ProcessContext c) {\n            int medium = c.sideInput(mediumSpending);\n            if (findSpending(userId) &lt;= medium) {\n              out.output(userId);\n            }\n          }\n      }).withSideInputs(mediumSpending)\n  );\n</code></pre><p>比如，在这个处理流程中，我们需要根据之前处理得到的结果，也就是用户的中位数消费数据，找到消费低于这个中位数的用户。那么，我们可以通过side input把这个中位数传递进DoFn中。然后你可以在ProcessElement的参数ProcessContext中拿出来这个side input。</p><h2>Transform的优化</h2><p>之前我们也提到过，Beam中的数据操作都是lazy execution的。这使得Transform和普通的函数运算很不一样。当你写下面这样一个代码的时候，真正的计算完全没有被执行。</p><pre><code>Pcollection1 = pcollection2.apply(Transform)\n</code></pre><p>这样的代码仅仅是让Beam知道了“你想对数据进行哪些操作”，需要让它来构建你的数据处理有向图。之后Beam的处理优化器会对你的处理操作进行优化。所以，千万不要觉得你写了10个Transform就会有10个Transform马上被执行了。</p><p>理解Transform的lazy execution非常重要。很多人会过度地优化自己的DoFn代码，想要在一个DoFn中把所有运算全都做了。其实完全没这个必要。</p><p><img src=\"https://static001.geekbang.org/resource/image/23/55/235e17cc1c3885a39e79217fddbabc55.png?wh=2840*1354\" alt=\"\"></p><p>你可以用分步的DoFn把自己想要的操作表达出来，然后交给Beam的优化器去合并你的操作。比如，在FlumeJava论文中提到的MSCR Fusion，它会把几个相关的GroupByKey的Transform合并。</p><h2>小结</h2><p>在这一讲中，我们学习了Transform的概念和基本的使用方法。通过文章中的几个简单的例子，你要做到的是了解怎样编写Transform的编程模型DoFn类。有状态DoFn在实际应用中尤其常见，你可以多加关注。</p><h2>思考题</h2><p>你可能会发现Beam的ParDo类似于Spark的map()或者是MapReduce的map。它们确实有很多相似之处。那你认为它们有什么不一样之处呢？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","neighbors":{"left":{"article_title":"24 | PCollection：为什么Beam要如此抽象封装数据？","id":100666},"right":{"article_title":"26 | Pipeline：Beam如何抽象多步骤的数据流水线？","id":102182}},"comments":[{"had_liked":false,"id":105125,"user_name":"常超","can_delete":false,"product_type":"c1","uid":1138665,"ip_address":"","ucode":"4AE7743B4ADF20","user_header":"https://static001.geekbang.org/account/avatar/00/11/5f/e9/95ef44f3.jpg","comment_is_top":false,"comment_ctime":1560924882,"is_pvip":false,"replies":[{"id":"38677","content":"不错的总结！","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561427774,"ip_address":"","comment_id":105125,"utype":1}],"discussion_count":2,"race_medal":0,"score":"65985434322","product_id":100025301,"comment_content":"1.ParDo支持数据输出到多个PCollection，而Spark和MapReduce的map可以说是单线的。<br>2.ParDo提供内建的状态存储机制，而Spark和MapReduce没有（Spark Streaming有mapWithState ）。","like_count":16,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454568,"discussion_content":"不错的总结！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561427774,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1059579,"avatar":"https://static001.geekbang.org/account/avatar/00/10/2a/fb/e2b29825.jpg","nickname":"Bing","note":"","ucode":"1A9745BBF0CDDB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":242992,"discussion_content":"不就是spark的mappartitions","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587516082,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":105518,"user_name":"sxpujs","can_delete":false,"product_type":"c1","uid":1503861,"ip_address":"","ucode":"F81931EC883D7C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/yYzf0yonEqKny7dHlvLibc7OrQJ6HszX3VP1fciaMD3hITFySbayL9vULch5hvicoqGA2EBzcPicss2ciaB7ibodgQ6w/132","comment_is_top":false,"comment_ctime":1561020083,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"40215725747","product_id":100025301,"comment_content":"Spark的算子和函数非常方便和灵活，这种通用的DoFn反而很别扭。","like_count":9},{"had_liked":false,"id":139823,"user_name":"vigo","can_delete":false,"product_type":"c1","uid":1086295,"ip_address":"","ucode":"036CEE5F3FABE7","user_header":"https://static001.geekbang.org/account/avatar/00/10/93/57/3ffdfc8d.jpg","comment_is_top":false,"comment_ctime":1570752631,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"35930490999","product_id":100025301,"comment_content":"推荐python,然而这章又几乎全是java事例","like_count":8},{"had_liked":false,"id":105207,"user_name":"微思","can_delete":false,"product_type":"c1","uid":1004349,"ip_address":"","ucode":"853C48AA183A7B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/53/3d/1189e48a.jpg","comment_is_top":false,"comment_ctime":1560938347,"is_pvip":true,"discussion_count":0,"race_medal":1,"score":"10150872939","product_id":100025301,"comment_content":"Statefullness、side input&#47;side output相关的例子可以再多一点。","like_count":2},{"had_liked":false,"id":105022,"user_name":"cricket1981","can_delete":false,"product_type":"c1","uid":1001715,"ip_address":"","ucode":"758262F5958DA4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/48/f3/f1034ffd.jpg","comment_is_top":false,"comment_ctime":1560906161,"is_pvip":false,"replies":[{"id":"38685","content":"谢谢你的提问！ParDo的level好像是不行的，如果对于整个数据流水线来说的话，可以指定numWorkers。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1561429616,"ip_address":"","comment_id":105022,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10150840753","product_id":100025301,"comment_content":"ParDo能指定并行度吗？","like_count":2,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":454526,"discussion_content":"谢谢你的提问！ParDo的level好像是不行的，如果对于整个数据流水线来说的话，可以指定numWorkers。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561429616,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":205532,"user_name":"Junjie.M","can_delete":false,"product_type":"c1","uid":1667133,"ip_address":"","ucode":"6E40909A02DFB1","user_header":"https://static001.geekbang.org/account/avatar/00/19/70/3d/93aa82b6.jpg","comment_is_top":false,"comment_ctime":1586665462,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5881632758","product_id":100025301,"comment_content":"老师，当一个transform有多个输入pcollection时如何调用transform，是合并pcollection后调用还是各自调用。还有一个transform如何输出多个pcollection。可以给个代码示例吗","like_count":1},{"had_liked":false,"id":126554,"user_name":"LJK","can_delete":false,"product_type":"c1","uid":1199213,"ip_address":"","ucode":"12B2441099FF1D","user_header":"https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg","comment_is_top":false,"comment_ctime":1566414218,"is_pvip":false,"replies":[{"id":"47604","content":"不是。map是一个input一个output，map是一个input可以有0个或者多个output","user_name":"作者回复","user_name_real":"Yuannan蔡元楠","uid":"1257426","ctime":1566882687,"ip_address":"","comment_id":126554,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5861381514","product_id":100025301,"comment_content":"ParDo是不是跟map一个意思？","like_count":1,"discussions":[{"author":{"id":1257426,"avatar":"https://static001.geekbang.org/account/avatar/00/13/2f/d2/0b6a8945.jpg","nickname":"Yuannan蔡元楠","note":"","ucode":"695E183CE496A8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464053,"discussion_content":"不是。map是一个input一个output，map是一个input可以有0个或者多个output","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566882687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":346035,"user_name":"阿里斯托芬","can_delete":false,"product_type":"c1","uid":1228350,"ip_address":"","ucode":"5FB99020992974","user_header":"https://static001.geekbang.org/account/avatar/00/12/be/3e/1de66fbc.jpg","comment_is_top":false,"comment_ctime":1652780836,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1652780836","product_id":100025301,"comment_content":"ParDo应该可以理解为是一个flatmap操作，不过是一个操作更加丰富的flatmap","like_count":0},{"had_liked":false,"id":223639,"user_name":"老莫mac","can_delete":false,"product_type":"c1","uid":2027290,"ip_address":"","ucode":"0606DCCD46441B","user_header":"https://static001.geekbang.org/account/avatar/00/1e/ef/1a/34ee87e6.jpg","comment_is_top":false,"comment_ctime":1591147028,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1591147028","product_id":100025301,"comment_content":"刚开始接触并行处理架构，之前看了FLINK，后来公司选型用SPARK，走回学习SPARK的路。看了SPARK 的BEAM，我只有一个感觉，和FLINK的理念何其相像，每个处理步骤或者概念FLINK都有对应的实现。BEAM要在FLINK上面加一层，会损失效率。所以我能想象到的好处只有一个，就是BEAM能同时在FLINK和SPARK上运行，汇聚两边的结果。为了将两种不同的架构当成一种来使用，把处理目标PCOLLECTION 当成流，当成KAFKA往两边分发，两边时独立的消息处理，把结果返回，在某个地方REDUCE 或者SHUFFLE，得到结果。感觉本质上就是这样。","like_count":0},{"had_liked":false,"id":205426,"user_name":"冯杰","can_delete":false,"product_type":"c1","uid":1950765,"ip_address":"","ucode":"61C92D62D49A66","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/qmdZbyxrRD5qQLKjWkmdp3PCVhwmWTcp0cs04s39pic2RcNw0nNKTDgKqedSQ54bAGWjAVSc9p4vWP8RJRKB6nA/132","comment_is_top":false,"comment_ctime":1586615688,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1586615688","product_id":100025301,"comment_content":"感觉ParDo的本意是被设计用来满足这样的场景：数据的处理可以实现并行的操作，即数据集中任意一条数据的处理不依赖于其它的数据，在这种场景下可以满足不同分区数据的并行执行。  与spark相比的话，其实等价于spark中能被分割到单个stage内的操作算子(或者说是不产生shuffle的算子)，总结一下就是ParDo = {map、filter、flatmap...}。    与MR中的map相比的话，功能上类似，但是提供了状态的语义。 不知道理解的对不对，请老师点评。","like_count":0},{"had_liked":false,"id":115583,"user_name":"柳年思水","can_delete":false,"product_type":"c1","uid":1106802,"ip_address":"","ucode":"65589C121B904A","user_header":"https://static001.geekbang.org/account/avatar/00/10/e3/72/afd1eef0.jpg","comment_is_top":false,"comment_ctime":1563636090,"is_pvip":false,"replies":[{"id":"42255","content":"谢谢你的留言！其实Beam也有非常多内置的常用Transform。","user_name":"作者回复","user_name_real":"Geek_88e0d7","uid":"1503187","ctime":1563690364,"ip_address":"","comment_id":115583,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1563636090","product_id":100025301,"comment_content":"ParDo 有点自定义 UDX 的意思，而 Spark 或 Flink 除了支持 UDX，还内置很多常用的算子","like_count":0,"discussions":[{"author":{"id":1503187,"avatar":"","nickname":"Geek_88e0d7","note":"","ucode":"3E2596F3EB9165","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":459174,"discussion_content":"谢谢你的留言！其实Beam也有非常多内置的常用Transform。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563690364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]}]}