{"id":126493,"title":"15 | Kafka如何实现高性能IO？","content":"<p>你好，我是李玥。</p><p>Apache Kafka是一个高性能的消息队列，在众多消息队列产品中，Kafka的性能绝对是处于第一梯队的。我曾经在一台配置比较好的服务器上，对Kafka做过极限的性能压测，Kafka单个节点的极限处理能力接近每秒钟2000万条消息，吞吐量达到每秒钟600MB。</p><p>你可能会问，Kafka是如何做到这么高的性能的？</p><p>我们在专栏“进阶篇”的前几节课，讲的知识点一直围绕着同一个主题：怎么开发一个高性能的网络应用程序。其中提到了像全异步化的线程模型、高性能的异步网络传输、自定义的私有传输协议和序列化、反序列化等等，这些方法和优化技巧，你都可以在Kafka的源代码中找到对应的实现。</p><p>在性能优化方面，除了这些通用的性能优化手段之外，Kafka还有哪些“独门绝技”呢？</p><p>这节课，我来为你一一揭晓这些绝技。</p><h2>使用批量消息提升服务端处理能力</h2><p>我们知道，批量处理是一种非常有效的提升系统吞吐量的方法。在Kafka内部，消息都是以“批”为单位处理的。一批消息从发送端到接收端，是如何在Kafka中流转的呢？</p><p>我们先来看发送端，也就是Producer这一端。</p><p>在Kafka的客户端SDK（软件开发工具包）中，Kafka的Producer只提供了单条发送的send()方法，并没有提供任何批量发送的接口。原因是，Kafka根本就没有提供单条发送的功能，是的，你没有看错，虽然它提供的API每次只能发送一条消息，但实际上，Kafka的客户端SDK在实现消息发送逻辑的时候，采用了异步批量发送的机制。</p><!-- [[[read_end]]] --><p>当你调用send()方法发送一条消息之后，无论你是同步发送还是异步发送，Kafka都不会立即就把这条消息发送出去。它会先把这条消息，存放在内存中缓存起来，然后选择合适的时机把缓存中的所有消息组成一批，一次性发给Broker。简单地说，就是攒一波一起发。</p><p>在Kafka的服务端，也就是Broker这一端，又是如何处理这一批一批的消息呢？</p><p>在服务端，Kafka不会把一批消息再还原成多条消息，再一条一条地处理，这样太慢了。Kafka这块儿处理的非常聪明，每批消息都会被当做一个“批消息”来处理。也就是说，在Broker整个处理流程中，无论是写入磁盘、从磁盘读出来、还是复制到其他副本这些流程中，<strong>批消息都不会被解开，一直是作为一条“批消息”来进行处理的。</strong></p><p>在消费时，消息同样是以批为单位进行传递的，Consumer从Broker拉到一批消息后，在客户端把批消息解开，再一条一条交给用户代码处理。</p><p>比如说，你在客户端发送30条消息，在业务程序看来，是发送了30条消息，而对于Kafka的Broker来说，它其实就是处理了1条包含30条消息的“批消息”而已。显然处理1次请求要比处理30次请求要快得多。</p><p>构建批消息和解开批消息分别在发送端和消费端的客户端完成，不仅减轻了Broker的压力，最重要的是减少了Broker处理请求的次数，提升了总体的处理能力。</p><p>这就是Kafka用批量消息提升性能的方法。</p><p>我们知道，相比于网络传输和内存，磁盘IO的速度是比较慢的。对于消息队列的服务端来说，性能的瓶颈主要在磁盘IO这一块。接下来我们看一下，Kafka在磁盘IO这块儿做了哪些优化。</p><h2>使用顺序读写提升磁盘IO性能</h2><p>对于磁盘来说，它有一个特性，就是顺序读写的性能要远远好于随机读写。在SSD（固态硬盘）上，顺序读写的性能要比随机读写快几倍，如果是机械硬盘，这个差距会达到几十倍。为什么呢？</p><p>操作系统每次从磁盘读写数据的时候，需要先寻址，也就是先要找到数据在磁盘上的物理位置，然后再进行数据读写。如果是机械硬盘，这个寻址需要比较长的时间，因为它要移动磁头，这是个机械运动，机械硬盘工作的时候会发出咔咔的声音，就是移动磁头发出的声音。</p><p>顺序读写相比随机读写省去了大部分的寻址时间，它只要寻址一次，就可以连续地读写下去，所以说，性能要比随机读写要好很多。</p><p>Kafka就是充分利用了磁盘的这个特性。它的存储设计非常简单，对于每个分区，它把从Producer收到的消息，顺序地写入对应的log文件中，一个文件写满了，就开启一个新的文件这样顺序写下去。消费的时候，也是从某个全局的位置开始，也就是某一个log文件中的某个位置开始，顺序地把消息读出来。</p><p>这样一个简单的设计，充分利用了顺序读写这个特性，极大提升了Kafka在使用磁盘时的IO性能。</p><p>接下来我们说一下Kafka是如何实现缓存的。</p><h2>利用PageCache加速消息读写</h2><p>在Kafka中，它会利用PageCache加速消息读写。PageCache是现代操作系统都具有的一项基本特性。通俗地说，PageCache就是操作系统在内存中给磁盘上的文件建立的缓存。无论我们使用什么语言编写的程序，在调用系统的API读写文件的时候，并不会直接去读写磁盘上的文件，应用程序实际操作的都是PageCache，也就是文件在内存中缓存的副本。</p><p>应用程序在写入文件的时候，操作系统会先把数据写入到内存中的PageCache，然后再一批一批地写到磁盘上。读取文件的时候，也是从PageCache中来读取数据，这时候会出现两种可能情况。</p><p>一种是PageCache中有数据，那就直接读取，这样就节省了从磁盘上读取数据的时间；另一种情况是，PageCache中没有数据，这时候操作系统会引发一个缺页中断，应用程序的读取线程会被阻塞，操作系统把数据从文件中复制到PageCache中，然后应用程序再从PageCache中继续把数据读出来，这时会真正读一次磁盘上的文件，这个读的过程就会比较慢。</p><p>用户的应用程序在使用完某块PageCache后，操作系统并不会立刻就清除这个PageCache，而是尽可能地利用空闲的物理内存保存这些PageCache，除非系统内存不够用，操作系统才会清理掉一部分PageCache。清理的策略一般是LRU或它的变种算法，这个算法我们不展开讲，它保留PageCache的逻辑是：优先保留最近一段时间最常使用的那些PageCache。</p><p>Kafka在读写消息文件的时候，充分利用了PageCache的特性。一般来说，消息刚刚写入到服务端就会被消费，按照LRU的“优先清除最近最少使用的页”这种策略，读取的时候，对于这种刚刚写入的PageCache，命中的几率会非常高。</p><p>也就是说，大部分情况下，消费读消息都会命中PageCache，带来的好处有两个：一个是读取的速度会非常快，另外一个是，给写入消息让出磁盘的IO资源，间接也提升了写入的性能。</p><h2>ZeroCopy：零拷贝技术</h2><p>Kafka的服务端在消费过程中，还使用了一种“零拷贝”的操作系统特性来进一步提升消费的性能。</p><p>我们知道，在服务端，处理消费的大致逻辑是这样的：</p><ul>\n<li>首先，从文件中找到消息数据，读到内存中；</li>\n<li>然后，把消息通过网络发给客户端。</li>\n</ul><p>这个过程中，数据实际上做了2次或者3次复制：</p><ol>\n<li>从文件复制数据到PageCache中，如果命中PageCache，这一步可以省掉；</li>\n<li>从PageCache复制到应用程序的内存空间中，也就是我们可以操作的对象所在的内存；</li>\n<li>从应用程序的内存空间复制到Socket的缓冲区，这个过程就是我们调用网络应用框架的API发送数据的过程。</li>\n</ol><p>Kafka使用零拷贝技术可以把这个复制次数减少一次，上面的2、3步骤两次复制合并成一次复制。直接从PageCache中把数据复制到Socket缓冲区中，这样不仅减少一次数据复制，更重要的是，由于不用把数据复制到用户内存空间，DMA控制器可以直接完成数据复制，不需要CPU参与，速度更快。</p><p>下面是这个零拷贝对应的系统调用：</p><pre><code>#include &lt;sys/socket.h&gt;\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);\n</code></pre><p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。</p><p>如果你遇到这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。</p><h2>小结</h2><p>这节课，我们总结了Kafka的高性能设计中的几个关键的技术点：</p><ul>\n<li>使用批量处理的方式来提升系统吞吐能力。</li>\n<li>基于磁盘文件高性能顺序读写的特性来设计的存储结构。</li>\n<li>利用操作系统的PageCache来缓存数据，减少IO并提升读性能。</li>\n<li>使用零拷贝技术加速消费流程。</li>\n</ul><p>以上这些，就是Kafka之所以能做到如此高性能的关键技术点。你可以看到，要真正实现一个高性能的消息队列，是非常不容易的，你需要熟练掌握非常多的编程语言和操作系统的底层技术。</p><p>这些优化的方法和技术，同样可以用在其他适合的场景和应用程序中。我希望你能充分理解这几项优化技术的原理，知道它们在什么情况下适用，什么情况下不适用。这样，当你遇到合适场景的时候，再深入去学习它的细节用法，最终就能把它真正地用到你开发的程序中。</p><h2>思考题</h2><p>课后，我希望你去读一读Kafka的源代码，从我们这节课中找一两个技术点，找到对应的代码部分，真正去看一下，我们说的这些优化技术，是如何落地到代码上的。在分析源代码的过程中，如果有任何问题，也欢迎你在留言区和我一起讨论。</p><p>感谢阅读，如果你觉得这篇文章对你有帮助的话，也欢迎把它分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":145337,"user_name":"Peter","can_delete":false,"product_type":"c1","uid":1595382,"ip_address":"","ucode":"A77322C4E07B2D","user_header":"https://static001.geekbang.org/account/avatar/00/18/57/f6/2c7ac1ad.jpg","comment_is_top":false,"comment_ctime":1572253777,"is_pvip":false,"replies":[{"id":56303,"content":"不是，这只是使用堆外内存。\n\n所谓的零拷贝，Linux的系统调用是sendfile，在java中对应的方法是FileChannel.transferTo","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1572398338,"ip_address":"","comment_id":145337,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师您好，请教个问题：在NIO中创建缓冲区时，可以创建直接缓冲区，即ByteBuffer.allocateDirect(capacity)这里的直接缓冲区是不是就是零拷贝技术？","like_count":65},{"had_liked":false,"id":128419,"user_name":"微微一笑","can_delete":false,"product_type":"c1","uid":1250327,"ip_address":"","ucode":"CFA7ABE81D0B99","user_header":"https://static001.geekbang.org/account/avatar/00/13/14/17/8763dced.jpg","comment_is_top":false,"comment_ctime":1566877723,"is_pvip":false,"replies":[{"id":48172,"content":"A1：这个过程就是随机读的过程。所有对文件的读写最终都要指定一个位置，都是按位置去读。随机读和顺序读的区别是，读取的数据是不是在文件中连续的一段。\n\nA2：是的。\n\nA3：RocketMQ的consumerQueue文件和Kafka的index file作用是差不多的，都是log文件（保存真正的消息）的索引，消费的时候，都需要先读索引，再读log，这个方面，两者并没有什么不同。它们存储设计的真正的差异的是log文件的设计，RocketMQ每个Broker只有一组log文件，而Kafka是每个分区一组log文件，你可以想一下，这两种设计各有什么优点和缺点。\n\n另外，随机读和顺序读并没有严格的区分，不是非黑即白的。即使是最理想的顺序读，那它读第一个字节也是需要寻址的，这是不是一次随机读呢？随机读的时候，只要不是每次只读一个字节，你在读第二个字节的时候不就是顺序读吗？\n\n所以，不用纠结这个概念，只要我们能做到读取数据的时候，尽量读连续的整块的数据，尽量减少寻址次数，性能就会更好。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567127105,"ip_address":"","comment_id":128419,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师好，有些疑问希望老师解答下：\n①rocketMq有consumeQueue，存储着offset，然后通过offset去commitlog找到对应的Message。通过看rocketmq的开发文档，通过offset去查询消息属于【随机读】，offset不是存储着消息在磁盘中的位置吗？为什么属于随机读呢？\n②rocketMq的某个topic下指定的消息队列数，指的是consumeQueue的数量吗？\n③性能上，顺序读优于随机读。rocketMq的实现上，在消费者与commitlog之间设计了consumeQueue的数据结构，导致不能顺序读，只能随机读。我的疑惑是，rocketMq为什么不像kafka那样设计，通过顺序读取消息，然后再根据topic、tag平均分配给不同的消费者实例,，这样消息积压的时候，直接增加消费者实例就可以了，不需要增加consumeQueue，这样也可以去除consumeQueue的存在呀？我在想consumeQueue存在的意义是什么呢？\n哈哈，我的理解可能有些问题，希望老师指点迷津~","like_count":41,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":472343,"discussion_content":"不是，这只是使用堆外内存。\n\n所谓的零拷贝，Linux的系统调用是sendfile，在java中对应的方法是FileChannel.transferTo","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1572398338,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1571460,"avatar":"https://static001.geekbang.org/account/avatar/00/17/fa/84/f01d203a.jpg","nickname":"Simple life","note":"","ucode":"1902D7F72FB43F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":349928,"discussion_content":"零拷贝只是一个定义，堆外内存也能减少一次拷贝到JVM内存的次数","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1613631260,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1744053,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/9c/b5/c936c54b.jpg","nickname":"💣","note":"","ucode":"F3B82CEB6B6934","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":574089,"discussion_content":"零拷贝是对一个过程的定义，而不是对具体对象的定义","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653831219,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128188,"user_name":"每天晒白牙","can_delete":false,"product_type":"c1","uid":1004698,"ip_address":"","ucode":"A1B102CD933DEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","comment_is_top":false,"comment_ctime":1566860359,"is_pvip":false,"replies":[{"id":47542,"content":"期待","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566869090,"ip_address":"","comment_id":128188,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"谢谢老师，今天讲到的点，我会在课下去读源码并写出文章","like_count":25,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464939,"discussion_content":"A1：这个过程就是随机读的过程。所有对文件的读写最终都要指定一个位置，都是按位置去读。随机读和顺序读的区别是，读取的数据是不是在文件中连续的一段。\n\nA2：是的。\n\nA3：RocketMQ的consumerQueue文件和Kafka的index file作用是差不多的，都是log文件（保存真正的消息）的索引，消费的时候，都需要先读索引，再读log，这个方面，两者并没有什么不同。它们存储设计的真正的差异的是log文件的设计，RocketMQ每个Broker只有一组log文件，而Kafka是每个分区一组log文件，你可以想一下，这两种设计各有什么优点和缺点。\n\n另外，随机读和顺序读并没有严格的区分，不是非黑即白的。即使是最理想的顺序读，那它读第一个字节也是需要寻址的，这是不是一次随机读呢？随机读的时候，只要不是每次只读一个字节，你在读第二个字节的时候不就是顺序读吗？\n\n所以，不用纠结这个概念，只要我们能做到读取数据的时候，尽量读连续的整块的数据，尽量减少寻址次数，性能就会更好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567127105,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1046714,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f8/ba/d28174a9.jpg","nickname":"Geek_zbvt62","note":"","ucode":"81EA27ADD9EC1A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7012,"discussion_content":"第三点rocketmq官方有过说明，他们认为在大量topic的场景下kafka退化成随机写，为了解决这个问题rocketmq采用单一commitlog\n大量topic这个场景大概指的是物联网","likes_number":13,"is_delete":false,"is_hidden":false,"ctime":1567258217,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1246199,"avatar":"https://static001.geekbang.org/account/avatar/00/13/03/f7/3a493bec.jpg","nickname":"老杨同志","note":"","ucode":"3F334F0CFD3DE6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6506,"discussion_content":"1找到offset的过程是随机读，之后消费消息是顺序读。2我理解是","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1566949374,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1508329,"avatar":"https://static001.geekbang.org/account/avatar/00/17/03/e9/6358059c.jpg","nickname":"GalaxyCreater","note":"","ucode":"C79E8A088D57CF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":305792,"discussion_content":"kafka主题、分区多了后，就有大量文件时，时间花在找文件上了；rocketmq则很稳定","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1600086059,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062848,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersGSic8ib7OguJv6CJiaXY0s4n9C7Z51sWxTTljklFpq3ZAIWXoFTPV5oLo0GMTkqW5sYJRRnibNqOJQ/132","nickname":"walle斌","note":"","ucode":"0DB3243004951F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":549772,"discussion_content":"上边都提到了 最经典的就是 kafka在多topic  其实就打开多文件数  大概在1000左右的时候性能会下降的很明显。。rocketmq这种设计 反而不会有明显的性能下降。。从设计上讲kafka 与rocketmq 可以理解为一个是 疏松性 一个是 密集型的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644231863,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1087945,"avatar":"https://static001.geekbang.org/account/avatar/00/10/99/c9/a7c77746.jpg","nickname":"冰激凌的眼泪","note":"","ucode":"5DCB974667E93A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6517,"discussion_content":"根据数组index访问元素，叫做随机访问，这里根据offset应该是类似的。\n前面好像说增加queue才能加速消费","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566955144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":132612,"user_name":"龍蝦","can_delete":false,"product_type":"c1","uid":1000076,"ip_address":"","ucode":"BE1D500833F070","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/8c/373d4027.jpg","comment_is_top":false,"comment_ctime":1568172682,"is_pvip":true,"replies":[{"id":50885,"content":"是这样的。\n在Kafka中，这个Send是一个异步方法。如果要确保发送成功，你必须在提供的回调方法中去检查发送结果。\n\n或者你也可以调用producer.send(record).get()来同步获取发送结果。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1568251374,"ip_address":"","comment_id":132612,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"Kafka Producer 调用同步 send() 成功返回，其实没法保证消息已经成功发送到 Kafka 服务器？","like_count":17,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464833,"discussion_content":"期待","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566869090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":136040,"user_name":"长期规划","can_delete":false,"product_type":"c1","uid":1019332,"ip_address":"","ucode":"5EF65E9115834B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","comment_is_top":false,"comment_ctime":1569334740,"is_pvip":false,"replies":[{"id":52149,"content":"如果进程崩溃是不会丢数据的，如果操作系统崩溃了，确实会丢失数据。但实际上，这个几率非常小。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1569376361,"ip_address":"","comment_id":136040,"utype":1}],"discussion_count":9,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师，如果Pagecahe在刷入磁盘前系统崩溃了，那数据就丢了吧？这样说来，即使写了文件，也不代表持久化了","like_count":14,"discussions":[{"author":{"id":1164531,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c4/f3/92f654f1.jpg","nickname":"Bug? Feature!","note":"","ucode":"F8FA8A0094FBA0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366107,"discussion_content":"    /**\n     * See {@link KafkaProducer#send(ProducerRecord)}\n     */\n    Future<RecordMetadata> send(ProducerRecord<K, V> record);\n\n    /**\n     * See {@link KafkaProducer#send(ProducerRecord, Callback)}\n     */\n    Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback);","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1617960603,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467037,"discussion_content":"是这样的。\n在Kafka中，这个Send是一个异步方法。如果要确保发送成功，你必须在提供的回调方法中去检查发送结果。\n\n或者你也可以调用producer.send(record).get()来同步获取发送结果。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1568251374,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1112676,"avatar":"https://static001.geekbang.org/account/avatar/00/10/fa/64/457325e6.jpg","nickname":"Sam Fu","note":"","ucode":"EA285A4943271F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":623783,"discussion_content":"那这样是不是就会丢消息。对于金融等一致性要求比较高的场景是不是就不适合用kafka了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689830242,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":467037,"ip_address":"北京","group_id":0},"score":623783,"extra":""}]}]},{"had_liked":false,"id":128477,"user_name":"linqw","can_delete":false,"product_type":"c1","uid":1134138,"ip_address":"","ucode":"09DCFE98C54DD8","user_header":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","comment_is_top":false,"comment_ctime":1566886104,"is_pvip":false,"replies":[{"id":48176,"content":"只有相同分区的消息才能组成同一个批消息。你的第三个问题太大了，改天有时间可以专题聊一下。\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567127949,"ip_address":"","comment_id":128477,"utype":1}],"discussion_count":8,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"1、老师有个疑问，kafka在发送时，都会在客户端进行攒一波，然后过一定的时间，或者达到一定的大小发送出去，批量发送的时候，是把一批同一个topic下的分区的消息进行批量发送么？还是不管是属于同一分区的消息都进行批量发送，broker端是不会对批消息进行拆分成每一条，那这样消费端消费到的消息不是有可能有不是订阅的分区么？\n2、学习到现在，有个感想，很多事情看似很简单，但是实际再做的时候都没那么简单，很多都得持之以恒，多思考、多实践、多动手，不然的话很多都是看懂，真正在使用的时候还是不知道如何下手。把很多小事、简单的事情做好本身就不是个简单的事情，目前有个想法打算把开源rocketmq读完，代码上写上注释和理解。\n3、老师我一直有个疑惑点，如何才能当上架构师了，一方面硬核实力技术过硬，有整体的大局观，老师能否以你自身的经历给我们解惑下了","like_count":13,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":468429,"discussion_content":"如果进程崩溃是不会丢数据的，如果操作系统崩溃了，确实会丢失数据。但实际上，这个几率非常小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569376361,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062848,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersGSic8ib7OguJv6CJiaXY0s4n9C7Z51sWxTTljklFpq3ZAIWXoFTPV5oLo0GMTkqW5sYJRRnibNqOJQ/132","nickname":"walle斌","note":"","ucode":"0DB3243004951F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":549773,"discussion_content":"所以 kafka 跟rocketmq的broker都可以主动配置 刷盘策略以及 其他节点的同步策略平衡这个问题。。没有银弹。。全是均衡。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644231914,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":117624,"discussion_content":"对呀，我也有同样的疑问，如果刚刚写入page cache，操作系统还没来得及写入磁盘，主机宕机了，那岂不是会丢数据？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1578131931,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":4,"child_discussions":[{"author":{"id":1802337,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/80/61/ae3bb67c.jpg","nickname":"毛毛虫大帝","note":"","ucode":"1EBB026121C060","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":185842,"discussion_content":"肯定会丢，单机这种情况避免不了，所以需要分布式一致性来解决丢数据的问题，通过集群来保存消息，利用消息复制来保证消息可靠。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1582641364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":117624,"ip_address":"","group_id":0},"score":185842,"extra":""},{"author":{"id":1130590,"avatar":"https://static001.geekbang.org/account/avatar/00/11/40/5e/b8fada94.jpg","nickname":"Ryoma","note":"","ucode":"7F692369239692","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":203989,"discussion_content":"一般可以设置提交时，消息写入了多个副本再返回成功，这样减少主机宕机的影响；比如此时 leader 宕机了有同样新的副本成为 leader","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584108502,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":117624,"ip_address":"","group_id":0},"score":203989,"extra":""},{"author":{"id":1137850,"avatar":"","nickname":"Geek_769f26","note":"","ucode":"0408C9E4FA4C0F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331389,"discussion_content":"这是这是肯定的啊，所以刷盘机制也支持同步刷盘，但效率很低。所以现在解决高可用的最好办法还是多副本啊。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1606841759,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":117624,"ip_address":"","group_id":0},"score":331389,"extra":""}]},{"author":{"id":1323052,"avatar":"https://static001.geekbang.org/account/avatar/00/14/30/2c/3feb407d.jpg","nickname":"倾听","note":"","ucode":"93755C7C78994A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":72665,"discussion_content":"如果消息量很大，服务器断电，是不是丢的消失数会更多？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1575513499,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1019332,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","nickname":"长期规划","note":"","ucode":"5EF65E9115834B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":20794,"discussion_content":"谢谢老师解答","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569376779,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":190986,"user_name":"正本·清源","can_delete":false,"product_type":"c1","uid":1913255,"ip_address":"","ucode":"1E8A0656810C4B","user_header":"https://static001.geekbang.org/account/avatar/00/1d/31/a7/7b0fc84d.jpg","comment_is_top":false,"comment_ctime":1584715954,"is_pvip":false,"replies":[{"id":73508,"content":"我怎么记得你都逃了呢？哈哈。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1584845181,"ip_address":"","comment_id":190986,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"班长确实知识扎实，其实全是大学本科课本的知识在不同技术上的组合。好在这些专业课我没有逃课，哈哈","like_count":10,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464965,"discussion_content":"只有相同分区的消息才能组成同一个批消息。你的第三个问题太大了，改天有时间可以专题聊一下。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567127949,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1004698,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","nickname":"每天晒白牙","note":"","ucode":"A1B102CD933DEA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6618,"discussion_content":"我写的KafkaProducer源码分析，可以给你解惑\nhttps://mp.weixin.qq.com/s/-s34_y16HU6HR5HDsSD4bg","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1566996810,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1134138,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","nickname":"linqw","note":"","ucode":"09DCFE98C54DD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1004698,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","nickname":"每天晒白牙","note":"","ucode":"A1B102CD933DEA","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6629,"discussion_content":"好勒，太谢谢啦，我等下学习学习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567003362,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":6618,"ip_address":"","group_id":0},"score":6629,"extra":""}]},{"author":{"id":1046172,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f6/9c/b457a937.jpg","nickname":"不能扮演天使","note":"","ucode":"9922330BFF7FFB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6523,"discussion_content":"KafkaProducer的javadoc讲的挺详细的，producer会为每个partition维持一个未发送记录的buffer,这个buffer的大小由 batch.size 配置，设置的越大就会导致越多的批处理；（每个活动的partition都有一个buffer）","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1566955989,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1134138,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","nickname":"linqw","note":"","ucode":"09DCFE98C54DD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1046172,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f6/9c/b457a937.jpg","nickname":"不能扮演天使","note":"","ucode":"9922330BFF7FFB","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6628,"discussion_content":"好勒，太谢谢啦，学习了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567003345,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":6523,"ip_address":"","group_id":0},"score":6628,"extra":""}]},{"author":{"id":2103021,"avatar":"https://static001.geekbang.org/account/avatar/00/20/16/ed/2db68084.jpg","nickname":"刘瓜瓜","note":"","ucode":"0380FA51CB9950","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365242,"discussion_content":"分区我理解是partation，不是topic吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617756375,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1012594,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ep3DicN9rHZiblia3zNpKJ3hEegicBFZqzZBS7l0oSENZicnqw340TnHVQsfNL33OtGyxEVQuS8DmqhPDQ/132","nickname":"笑傲流云","note":"","ucode":"40DF87D569C530","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6460,"discussion_content":"大佬，第一个问题，可以参考kafka produceRequest协议，在produce端，是把同一个分区的消息放在一个batch中，具体是放在一个Dqueue里，然后批量发送","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566910403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1134138,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","nickname":"linqw","note":"","ucode":"09DCFE98C54DD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1012594,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ep3DicN9rHZiblia3zNpKJ3hEegicBFZqzZBS7l0oSENZicnqw340TnHVQsfNL33OtGyxEVQuS8DmqhPDQ/132","nickname":"笑傲流云","note":"","ucode":"40DF87D569C530","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6466,"discussion_content":"学习了学习了，感谢回复哦，老师是大佬，我不是大佬，一起跟着老师学习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566912272,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":6460,"ip_address":"","group_id":0},"score":6466,"extra":""}]}]},{"had_liked":false,"id":168656,"user_name":"汤小高","can_delete":false,"product_type":"c1","uid":1555565,"ip_address":"","ucode":"D4AB7766273D52","user_header":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","comment_is_top":false,"comment_ctime":1578132322,"is_pvip":false,"replies":[{"id":65570,"content":"主机宕机了，那岂不是会丢数据？是的，是存在这样的可能。\n\nKafka的建议是通过复制而不是刷盘来保证消息可靠性，当然你也可以配置成每条消息都同步写入磁盘log.flush.interval.messages=1，但是这样会严重降低写入性能，基本上就没法用了。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1578275388,"ip_address":"","comment_id":168656,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师，如果消息是先写入page cache，再由操作系统写入磁盘的话，如果刚刚写入page cache，操作系统还没来得及写入磁盘，主机宕机了，那岂不是会丢数据？ 那这样卡夫卡服务本身岂不是也会存在丢数据的情况？ Kafka broker是在写入page cache就给producer 回复ack 还是操作系统将page cache写入磁盘后，如果是后者，就能保证不丢数据","like_count":9,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":488094,"discussion_content":"我怎么记得你都逃了呢？哈哈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584845181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128183,"user_name":"leslie","can_delete":false,"product_type":"c1","uid":1324255,"ip_address":"","ucode":"798E7C1CC98CC2","user_header":"https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg","comment_is_top":false,"comment_ctime":1566857806,"is_pvip":false,"replies":[{"id":47548,"content":"对于第一点，你的理解是没问题的。\n\n第二个问题，我的建议是，平时注重学习积累，哪怕我只是开发一个CRUD，也要认真的做好每个细节，把涉及到的知识搞清楚。而不是照葫芦画瓢跟网上抄一个能work的就行了。对于二次开发这个事儿，先解决目的的问题。不能为了二次开发而二次开发，一定是遇到一个什么问题，经过思考，二次开发是最佳的解决方案，这样才需要做二次开发。\n\n至于涉及到哪些知识，我们这门课中讲的这些基础的东西大概率你会用到，其它的可以靠日常积累和快速学习来解决。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566869536,"ip_address":"","comment_id":128183,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"   老师的课程学到现在开始越来越费力了：一堂课学完笔记量已经直线上升了；对于今天的课程读完后有些困惑之处烦劳老师可以指点迷津：\n    1.客户端发送者的发送给服务器端的时候：其实是写入一个Packge或者说一个log包，然后服务器端处理完这个包之后，作为一个批处理，处理完成后给客户端的消费者消费者解包之后依次获得处理结果；是这样么。\n    2.关于PageCache：刘超老师的课程中曾经提及其实消息队列主要运作在缓存层，常驻缓存就是为了节约查询时间；老师早先在开课的时候提过不同的消息队列其实特性不同，Kafka擅长或者说充分利用的是PageCache，其它如RockeMQ呢？我们如何扬长避短\n    主要是基于以下两方面：一方面是-其实现在大量的服务器是在云端的，无论是Amaze云、腾讯云、阿里云其实共同的特性都是CPU和IO稳定性或者使用率并非真实会引发一些看似极高的是使用率真实情况却并非有那么高，另外一方面-其实任何消息队列的推出都是基于当下，如果想基于当下的消息队列做些二次开发或者特性改进需要做些什么或者准备些什么呢?操作系统、计算机组成原理，还有什么？望老师能提点1、2.\n     跟着老师学到现在发现确实学好这门课可能比老师最初说的要求还要高：老师的课程跟到现在，觉得自己已经在最初的目标的路上了，谢谢老师的提点；期待老师的后续课程。","like_count":9,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":480285,"discussion_content":"主机宕机了，那岂不是会丢数据？是的，是存在这样的可能。\n\nKafka的建议是通过复制而不是刷盘来保证消息可靠性，当然你也可以配置成每条消息都同步写入磁盘log.flush.interval.messages=1，但是这样会严重降低写入性能，基本上就没法用了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1578275388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128168,"user_name":"timmy21","can_delete":false,"product_type":"c1","uid":1174860,"ip_address":"","ucode":"9D6DED247B1F38","user_header":"https://static001.geekbang.org/account/avatar/00/11/ed/4c/8674b6ad.jpg","comment_is_top":false,"comment_ctime":1566839942,"is_pvip":false,"replies":[{"id":47535,"content":"A1：是的。\nA2：是的，不同的编程语言API不太一样，但都提供了类似将指针移动到文件中某个位置的功能。\nA3：会被覆盖。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566868958,"ip_address":"","comment_id":128168,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师，我有两个疑问想请教一下：1. 我们平常打开文件写入数据是顺序写吗？2. 还有如何进行随机写？是seek到某个位置开始写？但这样的话文件数据不是会被覆盖吗？","like_count":7,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464830,"discussion_content":"对于第一点，你的理解是没问题的。\n\n第二个问题，我的建议是，平时注重学习积累，哪怕我只是开发一个CRUD，也要认真的做好每个细节，把涉及到的知识搞清楚。而不是照葫芦画瓢跟网上抄一个能work的就行了。对于二次开发这个事儿，先解决目的的问题。不能为了二次开发而二次开发，一定是遇到一个什么问题，经过思考，二次开发是最佳的解决方案，这样才需要做二次开发。\n\n至于涉及到哪些知识，我们这门课中讲的这些基础的东西大概率你会用到，其它的可以靠日常积累和快速学习来解决。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566869536,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":145337,"user_name":"Peter","can_delete":false,"product_type":"c1","uid":1595382,"ip_address":"","ucode":"A77322C4E07B2D","user_header":"https://static001.geekbang.org/account/avatar/00/18/57/f6/2c7ac1ad.jpg","comment_is_top":false,"comment_ctime":1572253777,"is_pvip":false,"replies":[{"id":56303,"content":"不是，这只是使用堆外内存。\n\n所谓的零拷贝，Linux的系统调用是sendfile，在java中对应的方法是FileChannel.transferTo","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1572398338,"ip_address":"","comment_id":145337,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师您好，请教个问题：在NIO中创建缓冲区时，可以创建直接缓冲区，即ByteBuffer.allocateDirect(capacity)这里的直接缓冲区是不是就是零拷贝技术？","like_count":65,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":472343,"discussion_content":"不是，这只是使用堆外内存。\n\n所谓的零拷贝，Linux的系统调用是sendfile，在java中对应的方法是FileChannel.transferTo","likes_number":4,"is_delete":false,"is_hidden":false,"ctime":1572398338,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1571460,"avatar":"https://static001.geekbang.org/account/avatar/00/17/fa/84/f01d203a.jpg","nickname":"Simple life","note":"","ucode":"1902D7F72FB43F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":349928,"discussion_content":"零拷贝只是一个定义，堆外内存也能减少一次拷贝到JVM内存的次数","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1613631260,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1744053,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/9c/b5/c936c54b.jpg","nickname":"💣","note":"","ucode":"F3B82CEB6B6934","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":574089,"discussion_content":"零拷贝是对一个过程的定义，而不是对具体对象的定义","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653831219,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128419,"user_name":"微微一笑","can_delete":false,"product_type":"c1","uid":1250327,"ip_address":"","ucode":"CFA7ABE81D0B99","user_header":"https://static001.geekbang.org/account/avatar/00/13/14/17/8763dced.jpg","comment_is_top":false,"comment_ctime":1566877723,"is_pvip":false,"replies":[{"id":48172,"content":"A1：这个过程就是随机读的过程。所有对文件的读写最终都要指定一个位置，都是按位置去读。随机读和顺序读的区别是，读取的数据是不是在文件中连续的一段。\n\nA2：是的。\n\nA3：RocketMQ的consumerQueue文件和Kafka的index file作用是差不多的，都是log文件（保存真正的消息）的索引，消费的时候，都需要先读索引，再读log，这个方面，两者并没有什么不同。它们存储设计的真正的差异的是log文件的设计，RocketMQ每个Broker只有一组log文件，而Kafka是每个分区一组log文件，你可以想一下，这两种设计各有什么优点和缺点。\n\n另外，随机读和顺序读并没有严格的区分，不是非黑即白的。即使是最理想的顺序读，那它读第一个字节也是需要寻址的，这是不是一次随机读呢？随机读的时候，只要不是每次只读一个字节，你在读第二个字节的时候不就是顺序读吗？\n\n所以，不用纠结这个概念，只要我们能做到读取数据的时候，尽量读连续的整块的数据，尽量减少寻址次数，性能就会更好。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567127105,"ip_address":"","comment_id":128419,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师好，有些疑问希望老师解答下：\n①rocketMq有consumeQueue，存储着offset，然后通过offset去commitlog找到对应的Message。通过看rocketmq的开发文档，通过offset去查询消息属于【随机读】，offset不是存储着消息在磁盘中的位置吗？为什么属于随机读呢？\n②rocketMq的某个topic下指定的消息队列数，指的是consumeQueue的数量吗？\n③性能上，顺序读优于随机读。rocketMq的实现上，在消费者与commitlog之间设计了consumeQueue的数据结构，导致不能顺序读，只能随机读。我的疑惑是，rocketMq为什么不像kafka那样设计，通过顺序读取消息，然后再根据topic、tag平均分配给不同的消费者实例,，这样消息积压的时候，直接增加消费者实例就可以了，不需要增加consumeQueue，这样也可以去除consumeQueue的存在呀？我在想consumeQueue存在的意义是什么呢？\n哈哈，我的理解可能有些问题，希望老师指点迷津~","like_count":41,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464939,"discussion_content":"A1：这个过程就是随机读的过程。所有对文件的读写最终都要指定一个位置，都是按位置去读。随机读和顺序读的区别是，读取的数据是不是在文件中连续的一段。\n\nA2：是的。\n\nA3：RocketMQ的consumerQueue文件和Kafka的index file作用是差不多的，都是log文件（保存真正的消息）的索引，消费的时候，都需要先读索引，再读log，这个方面，两者并没有什么不同。它们存储设计的真正的差异的是log文件的设计，RocketMQ每个Broker只有一组log文件，而Kafka是每个分区一组log文件，你可以想一下，这两种设计各有什么优点和缺点。\n\n另外，随机读和顺序读并没有严格的区分，不是非黑即白的。即使是最理想的顺序读，那它读第一个字节也是需要寻址的，这是不是一次随机读呢？随机读的时候，只要不是每次只读一个字节，你在读第二个字节的时候不就是顺序读吗？\n\n所以，不用纠结这个概念，只要我们能做到读取数据的时候，尽量读连续的整块的数据，尽量减少寻址次数，性能就会更好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567127105,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1046714,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f8/ba/d28174a9.jpg","nickname":"Geek_zbvt62","note":"","ucode":"81EA27ADD9EC1A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":7012,"discussion_content":"第三点rocketmq官方有过说明，他们认为在大量topic的场景下kafka退化成随机写，为了解决这个问题rocketmq采用单一commitlog\n大量topic这个场景大概指的是物联网","likes_number":13,"is_delete":false,"is_hidden":false,"ctime":1567258217,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1246199,"avatar":"https://static001.geekbang.org/account/avatar/00/13/03/f7/3a493bec.jpg","nickname":"老杨同志","note":"","ucode":"3F334F0CFD3DE6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6506,"discussion_content":"1找到offset的过程是随机读，之后消费消息是顺序读。2我理解是","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1566949374,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1508329,"avatar":"https://static001.geekbang.org/account/avatar/00/17/03/e9/6358059c.jpg","nickname":"GalaxyCreater","note":"","ucode":"C79E8A088D57CF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":305792,"discussion_content":"kafka主题、分区多了后，就有大量文件时，时间花在找文件上了；rocketmq则很稳定","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1600086059,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062848,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersGSic8ib7OguJv6CJiaXY0s4n9C7Z51sWxTTljklFpq3ZAIWXoFTPV5oLo0GMTkqW5sYJRRnibNqOJQ/132","nickname":"walle斌","note":"","ucode":"0DB3243004951F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":549772,"discussion_content":"上边都提到了 最经典的就是 kafka在多topic  其实就打开多文件数  大概在1000左右的时候性能会下降的很明显。。rocketmq这种设计 反而不会有明显的性能下降。。从设计上讲kafka 与rocketmq 可以理解为一个是 疏松性 一个是 密集型的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644231863,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1087945,"avatar":"https://static001.geekbang.org/account/avatar/00/10/99/c9/a7c77746.jpg","nickname":"冰激凌的眼泪","note":"","ucode":"5DCB974667E93A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6517,"discussion_content":"根据数组index访问元素，叫做随机访问，这里根据offset应该是类似的。\n前面好像说增加queue才能加速消费","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566955144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128188,"user_name":"每天晒白牙","can_delete":false,"product_type":"c1","uid":1004698,"ip_address":"","ucode":"A1B102CD933DEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","comment_is_top":false,"comment_ctime":1566860359,"is_pvip":false,"replies":[{"id":47542,"content":"期待","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566869090,"ip_address":"","comment_id":128188,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"谢谢老师，今天讲到的点，我会在课下去读源码并写出文章","like_count":25,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464833,"discussion_content":"期待","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566869090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":132612,"user_name":"龍蝦","can_delete":false,"product_type":"c1","uid":1000076,"ip_address":"","ucode":"BE1D500833F070","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/8c/373d4027.jpg","comment_is_top":false,"comment_ctime":1568172682,"is_pvip":true,"replies":[{"id":50885,"content":"是这样的。\n在Kafka中，这个Send是一个异步方法。如果要确保发送成功，你必须在提供的回调方法中去检查发送结果。\n\n或者你也可以调用producer.send(record).get()来同步获取发送结果。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1568251374,"ip_address":"","comment_id":132612,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"Kafka Producer 调用同步 send() 成功返回，其实没法保证消息已经成功发送到 Kafka 服务器？","like_count":17,"discussions":[{"author":{"id":1164531,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c4/f3/92f654f1.jpg","nickname":"Bug? Feature!","note":"","ucode":"F8FA8A0094FBA0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366107,"discussion_content":"    /**\n     * See {@link KafkaProducer#send(ProducerRecord)}\n     */\n    Future<RecordMetadata> send(ProducerRecord<K, V> record);\n\n    /**\n     * See {@link KafkaProducer#send(ProducerRecord, Callback)}\n     */\n    Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback);","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1617960603,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467037,"discussion_content":"是这样的。\n在Kafka中，这个Send是一个异步方法。如果要确保发送成功，你必须在提供的回调方法中去检查发送结果。\n\n或者你也可以调用producer.send(record).get()来同步获取发送结果。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1568251374,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1112676,"avatar":"https://static001.geekbang.org/account/avatar/00/10/fa/64/457325e6.jpg","nickname":"Sam Fu","note":"","ucode":"EA285A4943271F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":623783,"discussion_content":"那这样是不是就会丢消息。对于金融等一致性要求比较高的场景是不是就不适合用kafka了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1689830242,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":467037,"ip_address":"北京","group_id":0},"score":623783,"extra":""}]}]},{"had_liked":false,"id":136040,"user_name":"长期规划","can_delete":false,"product_type":"c1","uid":1019332,"ip_address":"","ucode":"5EF65E9115834B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","comment_is_top":false,"comment_ctime":1569334740,"is_pvip":false,"replies":[{"id":52149,"content":"如果进程崩溃是不会丢数据的，如果操作系统崩溃了，确实会丢失数据。但实际上，这个几率非常小。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1569376361,"ip_address":"","comment_id":136040,"utype":1}],"discussion_count":9,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师，如果Pagecahe在刷入磁盘前系统崩溃了，那数据就丢了吧？这样说来，即使写了文件，也不代表持久化了","like_count":14,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":468429,"discussion_content":"如果进程崩溃是不会丢数据的，如果操作系统崩溃了，确实会丢失数据。但实际上，这个几率非常小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569376361,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1062848,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ersGSic8ib7OguJv6CJiaXY0s4n9C7Z51sWxTTljklFpq3ZAIWXoFTPV5oLo0GMTkqW5sYJRRnibNqOJQ/132","nickname":"walle斌","note":"","ucode":"0DB3243004951F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":549773,"discussion_content":"所以 kafka 跟rocketmq的broker都可以主动配置 刷盘策略以及 其他节点的同步策略平衡这个问题。。没有银弹。。全是均衡。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644231914,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":117624,"discussion_content":"对呀，我也有同样的疑问，如果刚刚写入page cache，操作系统还没来得及写入磁盘，主机宕机了，那岂不是会丢数据？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1578131931,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":4,"child_discussions":[{"author":{"id":1802337,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/80/61/ae3bb67c.jpg","nickname":"毛毛虫大帝","note":"","ucode":"1EBB026121C060","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":185842,"discussion_content":"肯定会丢，单机这种情况避免不了，所以需要分布式一致性来解决丢数据的问题，通过集群来保存消息，利用消息复制来保证消息可靠。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1582641364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":117624,"ip_address":"","group_id":0},"score":185842,"extra":""},{"author":{"id":1130590,"avatar":"https://static001.geekbang.org/account/avatar/00/11/40/5e/b8fada94.jpg","nickname":"Ryoma","note":"","ucode":"7F692369239692","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":203989,"discussion_content":"一般可以设置提交时，消息写入了多个副本再返回成功，这样减少主机宕机的影响；比如此时 leader 宕机了有同样新的副本成为 leader","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584108502,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":117624,"ip_address":"","group_id":0},"score":203989,"extra":""},{"author":{"id":1137850,"avatar":"","nickname":"Geek_769f26","note":"","ucode":"0408C9E4FA4C0F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1555565,"avatar":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","nickname":"汤小高","note":"","ucode":"D4AB7766273D52","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331389,"discussion_content":"这是这是肯定的啊，所以刷盘机制也支持同步刷盘，但效率很低。所以现在解决高可用的最好办法还是多副本啊。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1606841759,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":117624,"ip_address":"","group_id":0},"score":331389,"extra":""}]},{"author":{"id":1323052,"avatar":"https://static001.geekbang.org/account/avatar/00/14/30/2c/3feb407d.jpg","nickname":"倾听","note":"","ucode":"93755C7C78994A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":72665,"discussion_content":"如果消息量很大，服务器断电，是不是丢的消失数会更多？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1575513499,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1019332,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","nickname":"长期规划","note":"","ucode":"5EF65E9115834B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":20794,"discussion_content":"谢谢老师解答","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569376779,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128477,"user_name":"linqw","can_delete":false,"product_type":"c1","uid":1134138,"ip_address":"","ucode":"09DCFE98C54DD8","user_header":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","comment_is_top":false,"comment_ctime":1566886104,"is_pvip":false,"replies":[{"id":48176,"content":"只有相同分区的消息才能组成同一个批消息。你的第三个问题太大了，改天有时间可以专题聊一下。\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567127949,"ip_address":"","comment_id":128477,"utype":1}],"discussion_count":8,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"1、老师有个疑问，kafka在发送时，都会在客户端进行攒一波，然后过一定的时间，或者达到一定的大小发送出去，批量发送的时候，是把一批同一个topic下的分区的消息进行批量发送么？还是不管是属于同一分区的消息都进行批量发送，broker端是不会对批消息进行拆分成每一条，那这样消费端消费到的消息不是有可能有不是订阅的分区么？\n2、学习到现在，有个感想，很多事情看似很简单，但是实际再做的时候都没那么简单，很多都得持之以恒，多思考、多实践、多动手，不然的话很多都是看懂，真正在使用的时候还是不知道如何下手。把很多小事、简单的事情做好本身就不是个简单的事情，目前有个想法打算把开源rocketmq读完，代码上写上注释和理解。\n3、老师我一直有个疑惑点，如何才能当上架构师了，一方面硬核实力技术过硬，有整体的大局观，老师能否以你自身的经历给我们解惑下了","like_count":13,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464965,"discussion_content":"只有相同分区的消息才能组成同一个批消息。你的第三个问题太大了，改天有时间可以专题聊一下。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567127949,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1004698,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","nickname":"每天晒白牙","note":"","ucode":"A1B102CD933DEA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6618,"discussion_content":"我写的KafkaProducer源码分析，可以给你解惑\nhttps://mp.weixin.qq.com/s/-s34_y16HU6HR5HDsSD4bg","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1566996810,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1134138,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","nickname":"linqw","note":"","ucode":"09DCFE98C54DD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1004698,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","nickname":"每天晒白牙","note":"","ucode":"A1B102CD933DEA","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6629,"discussion_content":"好勒，太谢谢啦，我等下学习学习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567003362,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":6618,"ip_address":"","group_id":0},"score":6629,"extra":""}]},{"author":{"id":1046172,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f6/9c/b457a937.jpg","nickname":"不能扮演天使","note":"","ucode":"9922330BFF7FFB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6523,"discussion_content":"KafkaProducer的javadoc讲的挺详细的，producer会为每个partition维持一个未发送记录的buffer,这个buffer的大小由 batch.size 配置，设置的越大就会导致越多的批处理；（每个活动的partition都有一个buffer）","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1566955989,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1134138,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","nickname":"linqw","note":"","ucode":"09DCFE98C54DD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1046172,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f6/9c/b457a937.jpg","nickname":"不能扮演天使","note":"","ucode":"9922330BFF7FFB","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6628,"discussion_content":"好勒，太谢谢啦，学习了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567003345,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":6523,"ip_address":"","group_id":0},"score":6628,"extra":""}]},{"author":{"id":2103021,"avatar":"https://static001.geekbang.org/account/avatar/00/20/16/ed/2db68084.jpg","nickname":"刘瓜瓜","note":"","ucode":"0380FA51CB9950","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365242,"discussion_content":"分区我理解是partation，不是topic吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617756375,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1012594,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ep3DicN9rHZiblia3zNpKJ3hEegicBFZqzZBS7l0oSENZicnqw340TnHVQsfNL33OtGyxEVQuS8DmqhPDQ/132","nickname":"笑傲流云","note":"","ucode":"40DF87D569C530","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6460,"discussion_content":"大佬，第一个问题，可以参考kafka produceRequest协议，在produce端，是把同一个分区的消息放在一个batch中，具体是放在一个Dqueue里，然后批量发送","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566910403,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1134138,"avatar":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","nickname":"linqw","note":"","ucode":"09DCFE98C54DD8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1012594,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ep3DicN9rHZiblia3zNpKJ3hEegicBFZqzZBS7l0oSENZicnqw340TnHVQsfNL33OtGyxEVQuS8DmqhPDQ/132","nickname":"笑傲流云","note":"","ucode":"40DF87D569C530","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6466,"discussion_content":"学习了学习了，感谢回复哦，老师是大佬，我不是大佬，一起跟着老师学习","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566912272,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":6460,"ip_address":"","group_id":0},"score":6466,"extra":""}]}]},{"had_liked":false,"id":190986,"user_name":"正本·清源","can_delete":false,"product_type":"c1","uid":1913255,"ip_address":"","ucode":"1E8A0656810C4B","user_header":"https://static001.geekbang.org/account/avatar/00/1d/31/a7/7b0fc84d.jpg","comment_is_top":false,"comment_ctime":1584715954,"is_pvip":false,"replies":[{"id":73508,"content":"我怎么记得你都逃了呢？哈哈。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1584845181,"ip_address":"","comment_id":190986,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"班长确实知识扎实，其实全是大学本科课本的知识在不同技术上的组合。好在这些专业课我没有逃课，哈哈","like_count":10,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":488094,"discussion_content":"我怎么记得你都逃了呢？哈哈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584845181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":168656,"user_name":"汤小高","can_delete":false,"product_type":"c1","uid":1555565,"ip_address":"","ucode":"D4AB7766273D52","user_header":"https://static001.geekbang.org/account/avatar/00/17/bc/6d/f6f0a442.jpg","comment_is_top":false,"comment_ctime":1578132322,"is_pvip":false,"replies":[{"id":65570,"content":"主机宕机了，那岂不是会丢数据？是的，是存在这样的可能。\n\nKafka的建议是通过复制而不是刷盘来保证消息可靠性，当然你也可以配置成每条消息都同步写入磁盘log.flush.interval.messages=1，但是这样会严重降低写入性能，基本上就没法用了。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1578275388,"ip_address":"","comment_id":168656,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师，如果消息是先写入page cache，再由操作系统写入磁盘的话，如果刚刚写入page cache，操作系统还没来得及写入磁盘，主机宕机了，那岂不是会丢数据？ 那这样卡夫卡服务本身岂不是也会存在丢数据的情况？ Kafka broker是在写入page cache就给producer 回复ack 还是操作系统将page cache写入磁盘后，如果是后者，就能保证不丢数据","like_count":9,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":480285,"discussion_content":"主机宕机了，那岂不是会丢数据？是的，是存在这样的可能。\n\nKafka的建议是通过复制而不是刷盘来保证消息可靠性，当然你也可以配置成每条消息都同步写入磁盘log.flush.interval.messages=1，但是这样会严重降低写入性能，基本上就没法用了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1578275388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128183,"user_name":"leslie","can_delete":false,"product_type":"c1","uid":1324255,"ip_address":"","ucode":"798E7C1CC98CC2","user_header":"https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg","comment_is_top":false,"comment_ctime":1566857806,"is_pvip":false,"replies":[{"id":47548,"content":"对于第一点，你的理解是没问题的。\n\n第二个问题，我的建议是，平时注重学习积累，哪怕我只是开发一个CRUD，也要认真的做好每个细节，把涉及到的知识搞清楚。而不是照葫芦画瓢跟网上抄一个能work的就行了。对于二次开发这个事儿，先解决目的的问题。不能为了二次开发而二次开发，一定是遇到一个什么问题，经过思考，二次开发是最佳的解决方案，这样才需要做二次开发。\n\n至于涉及到哪些知识，我们这门课中讲的这些基础的东西大概率你会用到，其它的可以靠日常积累和快速学习来解决。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566869536,"ip_address":"","comment_id":128183,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"   老师的课程学到现在开始越来越费力了：一堂课学完笔记量已经直线上升了；对于今天的课程读完后有些困惑之处烦劳老师可以指点迷津：\n    1.客户端发送者的发送给服务器端的时候：其实是写入一个Packge或者说一个log包，然后服务器端处理完这个包之后，作为一个批处理，处理完成后给客户端的消费者消费者解包之后依次获得处理结果；是这样么。\n    2.关于PageCache：刘超老师的课程中曾经提及其实消息队列主要运作在缓存层，常驻缓存就是为了节约查询时间；老师早先在开课的时候提过不同的消息队列其实特性不同，Kafka擅长或者说充分利用的是PageCache，其它如RockeMQ呢？我们如何扬长避短\n    主要是基于以下两方面：一方面是-其实现在大量的服务器是在云端的，无论是Amaze云、腾讯云、阿里云其实共同的特性都是CPU和IO稳定性或者使用率并非真实会引发一些看似极高的是使用率真实情况却并非有那么高，另外一方面-其实任何消息队列的推出都是基于当下，如果想基于当下的消息队列做些二次开发或者特性改进需要做些什么或者准备些什么呢?操作系统、计算机组成原理，还有什么？望老师能提点1、2.\n     跟着老师学到现在发现确实学好这门课可能比老师最初说的要求还要高：老师的课程跟到现在，觉得自己已经在最初的目标的路上了，谢谢老师的提点；期待老师的后续课程。","like_count":9,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464830,"discussion_content":"对于第一点，你的理解是没问题的。\n\n第二个问题，我的建议是，平时注重学习积累，哪怕我只是开发一个CRUD，也要认真的做好每个细节，把涉及到的知识搞清楚。而不是照葫芦画瓢跟网上抄一个能work的就行了。对于二次开发这个事儿，先解决目的的问题。不能为了二次开发而二次开发，一定是遇到一个什么问题，经过思考，二次开发是最佳的解决方案，这样才需要做二次开发。\n\n至于涉及到哪些知识，我们这门课中讲的这些基础的东西大概率你会用到，其它的可以靠日常积累和快速学习来解决。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566869536,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128168,"user_name":"timmy21","can_delete":false,"product_type":"c1","uid":1174860,"ip_address":"","ucode":"9D6DED247B1F38","user_header":"https://static001.geekbang.org/account/avatar/00/11/ed/4c/8674b6ad.jpg","comment_is_top":false,"comment_ctime":1566839942,"is_pvip":false,"replies":[{"id":47535,"content":"A1：是的。\nA2：是的，不同的编程语言API不太一样，但都提供了类似将指针移动到文件中某个位置的功能。\nA3：会被覆盖。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566868958,"ip_address":"","comment_id":128168,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100032301,"comment_content":"老师，我有两个疑问想请教一下：1. 我们平常打开文件写入数据是顺序写吗？2. 还有如何进行随机写？是seek到某个位置开始写？但这样的话文件数据不是会被覆盖吗？","like_count":7,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464820,"discussion_content":"A1：是的。\nA2：是的，不同的编程语言API不太一样，但都提供了类似将指针移动到文件中某个位置的功能。\nA3：会被覆盖。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566868958,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1044175,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJOBwR7MCVqwZbPA5RQ2mjUjd571jUXUcBCE7lY5vSMibWn8D5S4PzDZMaAhRPdnRBqYbVOBTJibhJg/132","nickname":"ヾ(◍°∇°◍)ﾉﾞ","note":"","ucode":"89545632BDA56E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6352,"discussion_content":"我觉得普通写都是顺序写，像索引这种是相当于随机写，每次替换一块，替换经常可能不在一个区域","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566866305,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":135759,"user_name":"易水寒","can_delete":false,"product_type":"c1","uid":1238961,"ip_address":"","ucode":"F5D8127ED4754E","user_header":"https://static001.geekbang.org/account/avatar/00/12/e7/b1/5c63be67.jpg","comment_is_top":false,"comment_ctime":1569253915,"is_pvip":false,"replies":[{"id":52060,"content":"这个取决于文件系统，一般来说文件系统存储数据的单位是block，一个文件包含若干个block，文件系统一般都会尽量把一个文件的block放在一起，所以顺序读依然会比随机读快非常多。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1569291796,"ip_address":"","comment_id":135759,"utype":1}],"discussion_count":4,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师好，我有个问题一直不明白，一个文件的数据应该是分散存储在磁盘上的吧（一般不太可能会是数据的位置都是连续的），读完整个文件的数据，怎么着也是需要移动磁头的吧（假如是机械盘），那么顺序读，所谓的顺序的含义是指什么？","like_count":6},{"had_liked":false,"id":130287,"user_name":"康师傅","can_delete":false,"product_type":"c1","uid":1154203,"ip_address":"","ucode":"7D71E93E8B41AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/9c/9b/eec0d41f.jpg","comment_is_top":false,"comment_ctime":1567418951,"is_pvip":false,"replies":[{"id":48734,"content":"是的，所以Kafka在分区非常多的情况下，性能是不如RocketMQ的。这种情况，除了想办法减少一些分区，确实没什么好的办法。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567493031,"ip_address":"","comment_id":130287,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"请教一下\n我的理解是：顺序写是针对某个分区而言的，那么如果单个节点上的topic数量很多，或者分区数很多，从整体来看应该还是会有很多的随机IO，因为会切换写不同的文件，这种情况下整体性能是不是就不高了？\n这种场景下，除了增加节点，将分区分布到多个节点上，是否还有其他有效提升性能的办法？","like_count":6,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":468328,"discussion_content":"这个取决于文件系统，一般来说文件系统存储数据的单位是block，一个文件包含若干个block，文件系统一般都会尽量把一个文件的block放在一起，所以顺序读依然会比随机读快非常多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569291796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1980058,"avatar":"","nickname":"tsaishichung","note":"","ucode":"AEB1762D0E1D37","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":346120,"discussion_content":"意思是取决于文件系统的格式吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611879072,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1937062,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/8e/a6/c3286b61.jpg","nickname":"Java垒墙工程师","note":"","ucode":"E76AE44A9C76AE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":306922,"discussion_content":"这样问有点转牛角尖了，哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600418669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1101314,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ce/02/9947b2c8.jpg","nickname":"楼顶凉席看星星","note":"","ucode":"8E20DFA8CDBF28","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":252628,"discussion_content":"精辟","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588172659,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131576,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567817180,"is_pvip":false,"replies":[{"id":50415,"content":"对于消息队列中的消费位置，不同的MQ实现不一样，有的是在写入磁盘之前生成的，有的是在写入之后生成的，这个位置不是文件中的偏移量，一般是一个逻辑位置，含义是：“这个分区中的第几条消息”。\n\n而数据在文件中的位置，是在写入之前就确定的，不管你用什么语言，写文件最终大多使用的是2个系统调用write或者pwrite，在调用这两个方法之前，其实位置已经是确定了。\n\nPageCache映射的是文件中的一部分数据，那这些数据必然有一个固定地址（无论数据是否已经写到磁盘上了），否则操作系统也没办法做这个映射。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567992196,"ip_address":"","comment_id":131576,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再一批一批地写到磁盘上。读取文件的时候，也是从 PageCache 中来读取数据，这时候会出现两种可能情况。\n\n客户端读取消息是，有个位置信息，这个信息在信息写入时，只在pagecache里出现还没有落盘时，位置信息就已经有了吗？这个位置信息怎么来的，原理又是什么？","like_count":5,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465891,"discussion_content":"是的，所以Kafka在分区非常多的情况下，性能是不如RocketMQ的。这种情况，除了想办法减少一些分区，确实没什么好的办法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567493031,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":134651,"user_name":"摩云神雕","can_delete":false,"product_type":"c1","uid":1109552,"ip_address":"","ucode":"446CA7AEFF2731","user_header":"https://static001.geekbang.org/account/avatar/00/10/ee/30/fc095d86.jpg","comment_is_top":false,"comment_ctime":1568886710,"is_pvip":false,"replies":[{"id":51726,"content":"是的，你可以配置batch.size和linger.ms这两个参数来调整发送时机和批量大小","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1568942894,"ip_address":"","comment_id":134651,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"请教下老师，调用send()方法发送一条消息后，无论是同步还是异步发送，这条消息都会缓存到我本地的内存吗？ 然后在合适的时间组成一批，一次发给Broker（kafka服务端）吗？ 发送时机是客户端可配的吗？ ","like_count":4,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467847,"discussion_content":"是的，你可以配置batch.size和linger.ms这两个参数来调整发送时机和批量大小","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568942894,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128523,"user_name":"海罗沃德","can_delete":false,"product_type":"c1","uid":1165364,"ip_address":"","ucode":"8704F1D6980FA0","user_header":"https://static001.geekbang.org/account/avatar/00/11/c8/34/fb871b2c.jpg","comment_is_top":false,"comment_ctime":1566895439,"is_pvip":false,"replies":[{"id":48038,"content":"这里面的批量处理和大数据中讲的“流和批”是二个不同的概念。\n\n大数据中的“批量计算”是相对于“流计算”来说的，它指的是，一个计算任务处理一批数据，这批数据处理完了，这个计算任务就结束了。\n\n我们这里的说的批量处理消息，是相对一条一条处理来说的，成批的处理会显著提升性能。\n\n即使是在Flink或Storm这种纯正的流计算平台中，它对流数据进行传输、计算也是批量处理的。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567044824,"ip_address":"","comment_id":128523,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"Kafka既然是批量處理消息，那麼是怎麼實現Kafka的實時數據流計算呢？","like_count":4,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464984,"discussion_content":"这里面的批量处理和大数据中讲的“流和批”是二个不同的概念。\n\n大数据中的“批量计算”是相对于“流计算”来说的，它指的是，一个计算任务处理一批数据，这批数据处理完了，这个计算任务就结束了。\n\n我们这里的说的批量处理消息，是相对一条一条处理来说的，成批的处理会显著提升性能。\n\n即使是在Flink或Storm这种纯正的流计算平台中，它对流数据进行传输、计算也是批量处理的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567044824,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131294,"user_name":"看不到de颜色","can_delete":false,"product_type":"c1","uid":1162714,"ip_address":"","ucode":"88348CCAE81931","user_header":"https://static001.geekbang.org/account/avatar/00/11/bd/da/3d76ea74.jpg","comment_is_top":false,"comment_ctime":1567696533,"is_pvip":false,"replies":[{"id":49811,"content":"我在之前的课中讲到过，在发送端，你需要在发送消息之后，检查发送结果，如果发送失败再进行重试或者执行其他的补偿。\n\n这个原则同样适用于Kafka，并且和是否批量发送没关系，也和同步发送还是异步发送也没关系。\n\n即使是异步发送，或者批量发送，只要你检查了发送结果（异步发送需要在回调方法中检查结果），并且发送结果是发送成功，就可以保证消息不丢。\n\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567731757,"ip_address":"","comment_id":131294,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"有一点疑惑还请老师解答一下。kafka为了保证消息丢失，客户端在发送消息时有三种acks可供选择。那如果kafka消息客户端都采用异步批量发送，那这三种参数还有意义吗？","like_count":3,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466376,"discussion_content":"我在之前的课中讲到过，在发送端，你需要在发送消息之后，检查发送结果，如果发送失败再进行重试或者执行其他的补偿。\n\n这个原则同样适用于Kafka，并且和是否批量发送没关系，也和同步发送还是异步发送也没关系。\n\n即使是异步发送，或者批量发送，只要你检查了发送结果（异步发送需要在回调方法中检查结果），并且发送结果是发送成功，就可以保证消息不丢。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567731757,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":221545,"user_name":"鲁班大师","can_delete":false,"product_type":"c1","uid":1179156,"ip_address":"","ucode":"4F9615DF87B031","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/14/f1532dec.jpg","comment_is_top":false,"comment_ctime":1590538147,"is_pvip":false,"replies":[{"id":81725,"content":"所谓零拷贝，主要指的是sendfile这个系统调用。\n在*nix上，你都可以用man sendfile来查看详细的说明。\n\n和NIO并不是一回事儿。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1590559065,"ip_address":"","comment_id":221545,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师，我理解只要使用了nio就自动使用了零拷贝","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466376,"discussion_content":"我在之前的课中讲到过，在发送端，你需要在发送消息之后，检查发送结果，如果发送失败再进行重试或者执行其他的补偿。\n\n这个原则同样适用于Kafka，并且和是否批量发送没关系，也和同步发送还是异步发送也没关系。\n\n即使是异步发送，或者批量发送，只要你检查了发送结果（异步发送需要在回调方法中检查结果），并且发送结果是发送成功，就可以保证消息不丢。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567731757,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":148310,"user_name":"又双叒叕是一年啊","can_delete":false,"product_type":"c1","uid":1000015,"ip_address":"","ucode":"E067320E537DEE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/4f/ff1ac464.jpg","comment_is_top":false,"comment_ctime":1572972372,"is_pvip":false,"replies":[{"id":57120,"content":"Kafka在消费时，对于消息体部分的数据，是不做任何处理的，直接发送给消费者，所以可以用zerocopy。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1573006291,"ip_address":"","comment_id":148310,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师请教一下，课程最后说： 这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。\n可是我理解 kafka消费则消费过程是需要读取数据的内存处理完成后再回复消费成功消息的。为什么还能用到零拷贝技术呢？","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496489,"discussion_content":"所谓零拷贝，主要指的是sendfile这个系统调用。\n在*nix上，你都可以用man sendfile来查看详细的说明。\n\n和NIO并不是一回事儿。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590559065,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129565,"user_name":"K-Li","can_delete":false,"product_type":"c1","uid":1361330,"ip_address":"","ucode":"3091322142E43E","user_header":"https://static001.geekbang.org/account/avatar/00/14/c5/b2/5b339c64.jpg","comment_is_top":false,"comment_ctime":1567154867,"is_pvip":false,"replies":[{"id":48360,"content":"实际上是无法保证的，所以有可能会有重复消息。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567213730,"ip_address":"","comment_id":129565,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师，我知道kafka是攒一波消息后进行批处理的，那么在consumer消费到一条消息后如果处理失败需要commit offset为上一条消息来重新消费的话是这么做到下一次来的就是刚刚处理失败的那条数据而不是&quot;一批&quot;里的下一条？","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":473456,"discussion_content":"Kafka在消费时，对于消息体部分的数据，是不做任何处理的，直接发送给消费者，所以可以用zerocopy。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573006291,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128727,"user_name":"linqw","can_delete":false,"product_type":"c1","uid":1134138,"ip_address":"","ucode":"09DCFE98C54DD8","user_header":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","comment_is_top":false,"comment_ctime":1566954776,"is_pvip":false,"replies":[{"id":48179,"content":"关于为什么分多个队列，我在之前的课程中提到过，和kafka分区一样，主要是为了能并行消费，提升消费性能。另外还有一个作用是，多个队列（分区）可以分布到多个节点上，提升主题整体的可用性。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567128121,"ip_address":"","comment_id":128727,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"尝试回答微微一笑的问题，老师有空帮忙看下哦\n老师好，有些疑问希望老师解答下：\n①rocketMq有consumeQueue，存储着offset，然后通过offset去commitlog找到对应的Message。通过看rocketmq的开发文档，通过offset去查询消息属于【随机读】，offset不是存储着消息在磁盘中的位置吗？为什么属于随机读呢？\n②rocketMq的某个topic下指定的消息队列数，指的是consumeQueue的数量吗？\n③性能上，顺序读优于随机读。rocketMq的实现上，在消费者与commitlog之间设计了consumeQueue的数据结构，导致不能顺序读，只能随机读。我的疑惑是，rocketMq为什么不像kafka那样设计，通过顺序读取消息，然后再根据topic、tag平均分配给不同的消费者实例,，这样消息积压的时候，直接增加消费者实例就可以了，不需要增加consumeQueue，这样也可以去除consumeQueue的存在呀？我在想consumeQueue存在的意义是什么呢？\n哈哈，我的理解可能有些问题，希望老师指点迷津~\n①顺序读写是从头开始进行读写，比随机读比，不需要进行数据的位置定位只要从头开始进行读写，随机读需要进行数据位置的定位，如果能知道位置，通过位置进行随机读也会很快，rocketmq就是这样来优化io的随机读，快速读数据不一定是顺序读，也可以根据位置的随机读。\n②rocketmq是messageQueue的数量，老师我有个好奇点rocketmq内部为什么要分读写队列，还有messageQueue内部没有存放消息，而是由消息message存放queueId和topic，消费者在消费的时候应该会有个consumerQueue才对，但是我在rocketmq代码里没有找到。\n③rocketmq在内部用到consumeQueue，因为consumeQueue内部无需存放真正的消息，只要存储消息在commitLog的offset的位置、消息的storeSize，每次要消费的时候只要拿到位置和大小，就可以读到消息，并且无需每次根据topic和tag进行平均分配。","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465543,"discussion_content":"实际上是无法保证的，所以有可能会有重复消息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567213730,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":135759,"user_name":"易水寒","can_delete":false,"product_type":"c1","uid":1238961,"ip_address":"","ucode":"F5D8127ED4754E","user_header":"https://static001.geekbang.org/account/avatar/00/12/e7/b1/5c63be67.jpg","comment_is_top":false,"comment_ctime":1569253915,"is_pvip":false,"replies":[{"id":52060,"content":"这个取决于文件系统，一般来说文件系统存储数据的单位是block，一个文件包含若干个block，文件系统一般都会尽量把一个文件的block放在一起，所以顺序读依然会比随机读快非常多。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1569291796,"ip_address":"","comment_id":135759,"utype":1}],"discussion_count":4,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师好，我有个问题一直不明白，一个文件的数据应该是分散存储在磁盘上的吧（一般不太可能会是数据的位置都是连续的），读完整个文件的数据，怎么着也是需要移动磁头的吧（假如是机械盘），那么顺序读，所谓的顺序的含义是指什么？","like_count":6,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":468328,"discussion_content":"这个取决于文件系统，一般来说文件系统存储数据的单位是block，一个文件包含若干个block，文件系统一般都会尽量把一个文件的block放在一起，所以顺序读依然会比随机读快非常多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1569291796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1980058,"avatar":"","nickname":"tsaishichung","note":"","ucode":"AEB1762D0E1D37","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":346120,"discussion_content":"意思是取决于文件系统的格式吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611879072,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1937062,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/8e/a6/c3286b61.jpg","nickname":"Java垒墙工程师","note":"","ucode":"E76AE44A9C76AE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":306922,"discussion_content":"这样问有点转牛角尖了，哈哈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600418669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1101314,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ce/02/9947b2c8.jpg","nickname":"楼顶凉席看星星","note":"","ucode":"8E20DFA8CDBF28","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":252628,"discussion_content":"精辟","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588172659,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130287,"user_name":"康师傅","can_delete":false,"product_type":"c1","uid":1154203,"ip_address":"","ucode":"7D71E93E8B41AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/9c/9b/eec0d41f.jpg","comment_is_top":false,"comment_ctime":1567418951,"is_pvip":false,"replies":[{"id":48734,"content":"是的，所以Kafka在分区非常多的情况下，性能是不如RocketMQ的。这种情况，除了想办法减少一些分区，确实没什么好的办法。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567493031,"ip_address":"","comment_id":130287,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"请教一下\n我的理解是：顺序写是针对某个分区而言的，那么如果单个节点上的topic数量很多，或者分区数很多，从整体来看应该还是会有很多的随机IO，因为会切换写不同的文件，这种情况下整体性能是不是就不高了？\n这种场景下，除了增加节点，将分区分布到多个节点上，是否还有其他有效提升性能的办法？","like_count":6,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465891,"discussion_content":"是的，所以Kafka在分区非常多的情况下，性能是不如RocketMQ的。这种情况，除了想办法减少一些分区，确实没什么好的办法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567493031,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131576,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567817180,"is_pvip":false,"replies":[{"id":50415,"content":"对于消息队列中的消费位置，不同的MQ实现不一样，有的是在写入磁盘之前生成的，有的是在写入之后生成的，这个位置不是文件中的偏移量，一般是一个逻辑位置，含义是：“这个分区中的第几条消息”。\n\n而数据在文件中的位置，是在写入之前就确定的，不管你用什么语言，写文件最终大多使用的是2个系统调用write或者pwrite，在调用这两个方法之前，其实位置已经是确定了。\n\nPageCache映射的是文件中的一部分数据，那这些数据必然有一个固定地址（无论数据是否已经写到磁盘上了），否则操作系统也没办法做这个映射。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567992196,"ip_address":"","comment_id":131576,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再一批一批地写到磁盘上。读取文件的时候，也是从 PageCache 中来读取数据，这时候会出现两种可能情况。\n\n客户端读取消息是，有个位置信息，这个信息在信息写入时，只在pagecache里出现还没有落盘时，位置信息就已经有了吗？这个位置信息怎么来的，原理又是什么？","like_count":5,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466522,"discussion_content":"对于消息队列中的消费位置，不同的MQ实现不一样，有的是在写入磁盘之前生成的，有的是在写入之后生成的，这个位置不是文件中的偏移量，一般是一个逻辑位置，含义是：“这个分区中的第几条消息”。\n\n而数据在文件中的位置，是在写入之前就确定的，不管你用什么语言，写文件最终大多使用的是2个系统调用write或者pwrite，在调用这两个方法之前，其实位置已经是确定了。\n\nPageCache映射的是文件中的一部分数据，那这些数据必然有一个固定地址（无论数据是否已经写到磁盘上了），否则操作系统也没办法做这个映射。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567992196,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":134651,"user_name":"摩云神雕","can_delete":false,"product_type":"c1","uid":1109552,"ip_address":"","ucode":"446CA7AEFF2731","user_header":"https://static001.geekbang.org/account/avatar/00/10/ee/30/fc095d86.jpg","comment_is_top":false,"comment_ctime":1568886710,"is_pvip":false,"replies":[{"id":51726,"content":"是的，你可以配置batch.size和linger.ms这两个参数来调整发送时机和批量大小","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1568942894,"ip_address":"","comment_id":134651,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"请教下老师，调用send()方法发送一条消息后，无论是同步还是异步发送，这条消息都会缓存到我本地的内存吗？ 然后在合适的时间组成一批，一次发给Broker（kafka服务端）吗？ 发送时机是客户端可配的吗？ ","like_count":4,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466522,"discussion_content":"对于消息队列中的消费位置，不同的MQ实现不一样，有的是在写入磁盘之前生成的，有的是在写入之后生成的，这个位置不是文件中的偏移量，一般是一个逻辑位置，含义是：“这个分区中的第几条消息”。\n\n而数据在文件中的位置，是在写入之前就确定的，不管你用什么语言，写文件最终大多使用的是2个系统调用write或者pwrite，在调用这两个方法之前，其实位置已经是确定了。\n\nPageCache映射的是文件中的一部分数据，那这些数据必然有一个固定地址（无论数据是否已经写到磁盘上了），否则操作系统也没办法做这个映射。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567992196,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128523,"user_name":"海罗沃德","can_delete":false,"product_type":"c1","uid":1165364,"ip_address":"","ucode":"8704F1D6980FA0","user_header":"https://static001.geekbang.org/account/avatar/00/11/c8/34/fb871b2c.jpg","comment_is_top":false,"comment_ctime":1566895439,"is_pvip":false,"replies":[{"id":48038,"content":"这里面的批量处理和大数据中讲的“流和批”是二个不同的概念。\n\n大数据中的“批量计算”是相对于“流计算”来说的，它指的是，一个计算任务处理一批数据，这批数据处理完了，这个计算任务就结束了。\n\n我们这里的说的批量处理消息，是相对一条一条处理来说的，成批的处理会显著提升性能。\n\n即使是在Flink或Storm这种纯正的流计算平台中，它对流数据进行传输、计算也是批量处理的。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567044824,"ip_address":"","comment_id":128523,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"Kafka既然是批量處理消息，那麼是怎麼實現Kafka的實時數據流計算呢？","like_count":4,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467847,"discussion_content":"是的，你可以配置batch.size和linger.ms这两个参数来调整发送时机和批量大小","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568942894,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131294,"user_name":"看不到de颜色","can_delete":false,"product_type":"c1","uid":1162714,"ip_address":"","ucode":"88348CCAE81931","user_header":"https://static001.geekbang.org/account/avatar/00/11/bd/da/3d76ea74.jpg","comment_is_top":false,"comment_ctime":1567696533,"is_pvip":false,"replies":[{"id":49811,"content":"我在之前的课中讲到过，在发送端，你需要在发送消息之后，检查发送结果，如果发送失败再进行重试或者执行其他的补偿。\n\n这个原则同样适用于Kafka，并且和是否批量发送没关系，也和同步发送还是异步发送也没关系。\n\n即使是异步发送，或者批量发送，只要你检查了发送结果（异步发送需要在回调方法中检查结果），并且发送结果是发送成功，就可以保证消息不丢。\n\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567731757,"ip_address":"","comment_id":131294,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"有一点疑惑还请老师解答一下。kafka为了保证消息丢失，客户端在发送消息时有三种acks可供选择。那如果kafka消息客户端都采用异步批量发送，那这三种参数还有意义吗？","like_count":3,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464984,"discussion_content":"这里面的批量处理和大数据中讲的“流和批”是二个不同的概念。\n\n大数据中的“批量计算”是相对于“流计算”来说的，它指的是，一个计算任务处理一批数据，这批数据处理完了，这个计算任务就结束了。\n\n我们这里的说的批量处理消息，是相对一条一条处理来说的，成批的处理会显著提升性能。\n\n即使是在Flink或Storm这种纯正的流计算平台中，它对流数据进行传输、计算也是批量处理的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567044824,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":221545,"user_name":"鲁班大师","can_delete":false,"product_type":"c1","uid":1179156,"ip_address":"","ucode":"4F9615DF87B031","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/14/f1532dec.jpg","comment_is_top":false,"comment_ctime":1590538147,"is_pvip":false,"replies":[{"id":81725,"content":"所谓零拷贝，主要指的是sendfile这个系统调用。\n在*nix上，你都可以用man sendfile来查看详细的说明。\n\n和NIO并不是一回事儿。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1590559065,"ip_address":"","comment_id":221545,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师，我理解只要使用了nio就自动使用了零拷贝","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496489,"discussion_content":"所谓零拷贝，主要指的是sendfile这个系统调用。\n在*nix上，你都可以用man sendfile来查看详细的说明。\n\n和NIO并不是一回事儿。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590559065,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":148310,"user_name":"又双叒叕是一年啊","can_delete":false,"product_type":"c1","uid":1000015,"ip_address":"","ucode":"E067320E537DEE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/4f/ff1ac464.jpg","comment_is_top":false,"comment_ctime":1572972372,"is_pvip":false,"replies":[{"id":57120,"content":"Kafka在消费时，对于消息体部分的数据，是不做任何处理的，直接发送给消费者，所以可以用zerocopy。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1573006291,"ip_address":"","comment_id":148310,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师请教一下，课程最后说： 这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。\n可是我理解 kafka消费则消费过程是需要读取数据的内存处理完成后再回复消费成功消息的。为什么还能用到零拷贝技术呢？","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":473456,"discussion_content":"Kafka在消费时，对于消息体部分的数据，是不做任何处理的，直接发送给消费者，所以可以用zerocopy。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573006291,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":129565,"user_name":"K-Li","can_delete":false,"product_type":"c1","uid":1361330,"ip_address":"","ucode":"3091322142E43E","user_header":"https://static001.geekbang.org/account/avatar/00/14/c5/b2/5b339c64.jpg","comment_is_top":false,"comment_ctime":1567154867,"is_pvip":false,"replies":[{"id":48360,"content":"实际上是无法保证的，所以有可能会有重复消息。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567213730,"ip_address":"","comment_id":129565,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"老师，我知道kafka是攒一波消息后进行批处理的，那么在consumer消费到一条消息后如果处理失败需要commit offset为上一条消息来重新消费的话是这么做到下一次来的就是刚刚处理失败的那条数据而不是&quot;一批&quot;里的下一条？","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465543,"discussion_content":"实际上是无法保证的，所以有可能会有重复消息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567213730,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128727,"user_name":"linqw","can_delete":false,"product_type":"c1","uid":1134138,"ip_address":"","ucode":"09DCFE98C54DD8","user_header":"https://static001.geekbang.org/account/avatar/00/11/4e/3a/86196508.jpg","comment_is_top":false,"comment_ctime":1566954776,"is_pvip":false,"replies":[{"id":48179,"content":"关于为什么分多个队列，我在之前的课程中提到过，和kafka分区一样，主要是为了能并行消费，提升消费性能。另外还有一个作用是，多个队列（分区）可以分布到多个节点上，提升主题整体的可用性。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567128121,"ip_address":"","comment_id":128727,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100032301,"comment_content":"尝试回答微微一笑的问题，老师有空帮忙看下哦\n老师好，有些疑问希望老师解答下：\n①rocketMq有consumeQueue，存储着offset，然后通过offset去commitlog找到对应的Message。通过看rocketmq的开发文档，通过offset去查询消息属于【随机读】，offset不是存储着消息在磁盘中的位置吗？为什么属于随机读呢？\n②rocketMq的某个topic下指定的消息队列数，指的是consumeQueue的数量吗？\n③性能上，顺序读优于随机读。rocketMq的实现上，在消费者与commitlog之间设计了consumeQueue的数据结构，导致不能顺序读，只能随机读。我的疑惑是，rocketMq为什么不像kafka那样设计，通过顺序读取消息，然后再根据topic、tag平均分配给不同的消费者实例,，这样消息积压的时候，直接增加消费者实例就可以了，不需要增加consumeQueue，这样也可以去除consumeQueue的存在呀？我在想consumeQueue存在的意义是什么呢？\n哈哈，我的理解可能有些问题，希望老师指点迷津~\n①顺序读写是从头开始进行读写，比随机读比，不需要进行数据的位置定位只要从头开始进行读写，随机读需要进行数据位置的定位，如果能知道位置，通过位置进行随机读也会很快，rocketmq就是这样来优化io的随机读，快速读数据不一定是顺序读，也可以根据位置的随机读。\n②rocketmq是messageQueue的数量，老师我有个好奇点rocketmq内部为什么要分读写队列，还有messageQueue内部没有存放消息，而是由消息message存放queueId和topic，消费者在消费的时候应该会有个consumerQueue才对，但是我在rocketmq代码里没有找到。\n③rocketmq在内部用到consumeQueue，因为consumeQueue内部无需存放真正的消息，只要存储消息在commitLog的offset的位置、消息的storeSize，每次要消费的时候只要拿到位置和大小，就可以读到消息，并且无需每次根据topic和tag进行平均分配。","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465077,"discussion_content":"关于为什么分多个队列，我在之前的课程中提到过，和kafka分区一样，主要是为了能并行消费，提升消费性能。另外还有一个作用是，多个队列（分区）可以分布到多个节点上，提升主题整体的可用性。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567128121,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128251,"user_name":"业余草","can_delete":false,"product_type":"c1","uid":1126538,"ip_address":"","ucode":"99BDC1E629049D","user_header":"https://static001.geekbang.org/account/avatar/00/11/30/8a/b5ca7286.jpg","comment_is_top":false,"comment_ctime":1566865828,"is_pvip":false,"replies":[{"id":47545,"content":"你可以分享一下，在使用Kafka的时候遇到了哪些问题。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566869205,"ip_address":"","comment_id":128251,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"只说了它的优点，其实它的缺点也很明显。把确定也顺便解释解释。","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464865,"discussion_content":"你可以分享一下，在使用Kafka的时候遇到了哪些问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566869205,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":234769,"user_name":"G.S.K","can_delete":false,"product_type":"c1","uid":1222966,"ip_address":"","ucode":"88217F9289EB48","user_header":"https://static001.geekbang.org/account/avatar/00/12/a9/36/d054c979.jpg","comment_is_top":false,"comment_ctime":1594790952,"is_pvip":false,"replies":[{"id":86778,"content":"会的。\n\n如果你用过10年的电脑，这一点感受会特别明显，在windows启动的时候，大量的程序同时启动，都需要读写磁盘，磁盘疲于奔命，这个时候系统卡的几乎动不了。可能需要好几分钟，磁盘安静下来，才能开始正常的操作。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1594901106,"ip_address":"","comment_id":234769,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"有个疑问请教老师，虽然kafka进程是顺序读写，假如主机上还部署了其他进程比如一个进程A。进程A随机读写磁盘，移动磁头，会不会影响kafka进程呢？会不会kafka进程在顺序写，但是进程切换到进程A运行，进程A把磁头给移动走了。","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501504,"discussion_content":"会的。\n\n如果你用过10年的电脑，这一点感受会特别明显，在windows启动的时候，大量的程序同时启动，都需要读写磁盘，磁盘疲于奔命，这个时候系统卡的几乎动不了。可能需要好几分钟，磁盘安静下来，才能开始正常的操作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594901106,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":185500,"user_name":"Tesla","can_delete":false,"product_type":"c1","uid":1500742,"ip_address":"","ucode":"98629AFD9861EE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKUcSLVV6ia3dibe7qvTu8Vic1PVs2EibxoUdx930MC7j2Q9A6s4eibMDZlcicMFY0D0icd3RrDorMChu0zw/132","comment_is_top":false,"comment_ctime":1583594393,"is_pvip":false,"replies":[{"id":71949,"content":"是的，文件本身就是顺序读写。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1583820889,"ip_address":"","comment_id":185500,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师好，请问kafka是怎么实现的顺序存储呢？是操作系统log文件本身就是顺序存储的吗","like_count":1},{"had_liked":false,"id":140560,"user_name":"Tesla","can_delete":false,"product_type":"c1","uid":1500742,"ip_address":"","ucode":"98629AFD9861EE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKUcSLVV6ia3dibe7qvTu8Vic1PVs2EibxoUdx930MC7j2Q9A6s4eibMDZlcicMFY0D0icd3RrDorMChu0zw/132","comment_is_top":false,"comment_ctime":1570986500,"is_pvip":false,"replies":[{"id":54354,"content":"PageCache是操作系统来控制的，对应用程序来说，就是访问文件，并不需要操作“访问PageCache”的API。\n\n比如，你提供了一个Web服务，至于每次请求的数据是从Redis（PageCache）中拿到的，还是从MySQL（文件）拿到的，使用Web服务的客户端确定不了，也并不知道。\n\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1571019630,"ip_address":"","comment_id":140560,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师好，请问一下，kafka利用pageCache减少IO。这样消费端是不是要知道Broker保存到pageCache中的对象的内存地址呢？而它们又是两个不同的进程，内存地址是怎么传递的呢？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464865,"discussion_content":"你可以分享一下，在使用Kafka的时候遇到了哪些问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566869205,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":140559,"user_name":"Tesla","can_delete":false,"product_type":"c1","uid":1500742,"ip_address":"","ucode":"98629AFD9861EE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKUcSLVV6ia3dibe7qvTu8Vic1PVs2EibxoUdx930MC7j2Q9A6s4eibMDZlcicMFY0D0icd3RrDorMChu0zw/132","comment_is_top":false,"comment_ctime":1570985978,"is_pvip":false,"replies":[{"id":54352,"content":"是的，但这个过程是自动的，不需要实现开辟好，随着写随着开辟就好，而且，实际上这片存储空间在磁盘上并不一定是连续的，具体取决于使用的文件系统。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1571019452,"ip_address":"","comment_id":140559,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师好，请问一下kafka使用顺序读写的方式，应该是在有正式数据写入之前，先在硬件上开辟一段连续的存储空间吧？等到有数据了再依次写入存储空间，就像列表需要指定对象类型和列表长度一样。那kafka是如何确定这段空间的大小呢？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":501504,"discussion_content":"会的。\n\n如果你用过10年的电脑，这一点感受会特别明显，在windows启动的时候，大量的程序同时启动，都需要读写磁盘，磁盘疲于奔命，这个时候系统卡的几乎动不了。可能需要好几分钟，磁盘安静下来，才能开始正常的操作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594901106,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131727,"user_name":"张洪阆","can_delete":false,"product_type":"c1","uid":1001954,"ip_address":"","ucode":"CFF7035D0DF059","user_header":"https://static001.geekbang.org/account/avatar/00/0f/49/e2/1fad12eb.jpg","comment_is_top":false,"comment_ctime":1567865478,"is_pvip":false,"replies":[{"id":50419,"content":"有的，你可以看一下这个配置：linger.ms ","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567992669,"ip_address":"","comment_id":131727,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"如果没攒够数据量是否就一直不发呢，有没有超时机制？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":470442,"discussion_content":"PageCache是操作系统来控制的，对应用程序来说，就是访问文件，并不需要操作“访问PageCache”的API。\n\n比如，你提供了一个Web服务，至于每次请求的数据是从Redis（PageCache）中拿到的，还是从MySQL（文件）拿到的，使用Web服务的客户端确定不了，也并不知道。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571019630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131575,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567816964,"is_pvip":false,"replies":[{"id":50416,"content":"只能由一个消费者来消费，因为在消费者收到消息解开这个“批消息”之前，这个批消息就是一个不能分割的整体。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567992270,"ip_address":"","comment_id":131575,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"构建批消息和解开批消息分别在发送端和消费端的客户端完成，不仅减轻了 Broker 的压力，最重要的是减少了 Broker 处理请求的次数，提升了总体的处理能力。\n\n这样的他，一批消息在多个消费端的情况下，这批消息这能落到一个消费端的吧？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":470441,"discussion_content":"是的，但这个过程是自动的，不需要实现开辟好，随着写随着开辟就好，而且，实际上这片存储空间在磁盘上并不一定是连续的，具体取决于使用的文件系统。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571019452,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130120,"user_name":"嘉木","can_delete":false,"product_type":"c1","uid":1317999,"ip_address":"","ucode":"AF4877693782C0","user_header":"https://static001.geekbang.org/account/avatar/00/14/1c/6f/3ea2a599.jpg","comment_is_top":false,"comment_ctime":1567387674,"is_pvip":false,"replies":[{"id":48727,"content":"是的，并不限于磁盘到socket缓冲区。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567492487,"ip_address":"","comment_id":130120,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"如果你遇到这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。\n\n这个sendfile是不是从一个fd到另外一个fd的复制都是可以用的？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466596,"discussion_content":"有的，你可以看一下这个配置：linger.ms ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567992669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128603,"user_name":"jack","can_delete":false,"product_type":"c1","uid":1612614,"ip_address":"","ucode":"5A26761299F7AC","user_header":"https://static001.geekbang.org/account/avatar/00/18/9b/46/ad3194bd.jpg","comment_is_top":false,"comment_ctime":1566912174,"is_pvip":false,"replies":[{"id":48173,"content":"一般还是推荐每个分区单线程消费，如果消费性能不行就扩容分区，这样实现简单并且可靠。你提到的第二种方法，不是说不能多线程异步（或者像你说的在while循环外）执行消费逻辑，这样是可以的，但是你必须保证“对于每条消息，只有执行完全部消费逻辑之后，才能提交消费位置”，这样才能不丢消息。只要能保证这点，无论是同步消费还是异步消费，或者自动还是手动提交消费位置都是可以的。但实际开发的时候，手动提交消费位置如果处理不好，很容易丢消息，所以不推荐。\n\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567127602,"ip_address":"","comment_id":128603,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师，关于kafka有几个问题期待您的解答：\n1、消费者是线程不安全的，多线程使用消费者，官方提供了两个方法，一是一个线程一个消费者，但是这样总的线程数受到分区数量的影响；二是一个线程或者几个线程把数据都消费到，然后将数据交给真正处理数据的线程池处理。想问您实际开发中哪种使用的更频繁，更多呢？\n2、消费者在poll时，总是处于while(true)循环中，那么逻辑处理就都放在循环中？如果想在循环外使用数据该怎么做呢？\n3、消费者为了保证“至少一次”处理，是否应该更多的采用手动提交offset？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466521,"discussion_content":"只能由一个消费者来消费，因为在消费者收到消息解开这个“批消息”之前，这个批消息就是一个不能分割的整体。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567992270,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130144,"user_name":"牛牛","can_delete":false,"product_type":"c1","uid":1194626,"ip_address":"","ucode":"CFCE68B4F92209","user_header":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","comment_is_top":false,"comment_ctime":1567390074,"is_pvip":false,"replies":[{"id":48728,"content":"生产者和消费者都没有自己的队列，这些队列在逻辑上属于主题，物理上保存在Broker上。或者你说的ReadQueue和WriteQueue是客户端在收发消息时候的缓存吗？这个是存在的，作用仅仅是在收发消息的时候缓存一部分消息而已。\n\n关于队列相关的概念，你可以再重新看一下03和08这两节课，里面有比价详细的阐述。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567492791,"ip_address":"","comment_id":130144,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师、想请教下 RocketMQ的queue\n1. 生产者和消费者都有自己的Queue吗 ？还是生产者的Queue就是消费这的Queue呢 ？\n   如果各自有独立的Queue、这两者之间有什么联系没 ？\n2. ReadQueue 和 WriteQueue 的作用是什么呢 ？这两个Queue都是针对某个topic而言还是针对Producer和Consumer而言呢 ？ReadQueue和WriteQueue的数量可以不一致么 ？\n3. 真正存储消息的地方是Queue、而Broker只是负责消息的路由？Broker的slave和master上包含相同的数据信息、对吗？\n\n有点儿乱了、希望老师或者路过的同学指点下 ^.^、先多谢啦~~~","like_count":0,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465818,"discussion_content":"是的，并不限于磁盘到socket缓冲区。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567492487,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128251,"user_name":"业余草","can_delete":false,"product_type":"c1","uid":1126538,"ip_address":"","ucode":"99BDC1E629049D","user_header":"https://static001.geekbang.org/account/avatar/00/11/30/8a/b5ca7286.jpg","comment_is_top":false,"comment_ctime":1566865828,"is_pvip":false,"replies":[{"id":47545,"content":"你可以分享一下，在使用Kafka的时候遇到了哪些问题。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566869205,"ip_address":"","comment_id":128251,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"只说了它的优点，其实它的缺点也很明显。把确定也顺便解释解释。","like_count":2,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":486382,"discussion_content":"是的，文件本身就是顺序读写。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583820889,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":234769,"user_name":"G.S.K","can_delete":false,"product_type":"c1","uid":1222966,"ip_address":"","ucode":"88217F9289EB48","user_header":"https://static001.geekbang.org/account/avatar/00/12/a9/36/d054c979.jpg","comment_is_top":false,"comment_ctime":1594790952,"is_pvip":false,"replies":[{"id":86778,"content":"会的。\n\n如果你用过10年的电脑，这一点感受会特别明显，在windows启动的时候，大量的程序同时启动，都需要读写磁盘，磁盘疲于奔命，这个时候系统卡的几乎动不了。可能需要好几分钟，磁盘安静下来，才能开始正常的操作。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1594901106,"ip_address":"","comment_id":234769,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"有个疑问请教老师，虽然kafka进程是顺序读写，假如主机上还部署了其他进程比如一个进程A。进程A随机读写磁盘，移动磁头，会不会影响kafka进程呢？会不会kafka进程在顺序写，但是进程切换到进程A运行，进程A把磁头给移动走了。","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":470442,"discussion_content":"PageCache是操作系统来控制的，对应用程序来说，就是访问文件，并不需要操作“访问PageCache”的API。\n\n比如，你提供了一个Web服务，至于每次请求的数据是从Redis（PageCache）中拿到的，还是从MySQL（文件）拿到的，使用Web服务的客户端确定不了，也并不知道。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571019630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":185500,"user_name":"Tesla","can_delete":false,"product_type":"c1","uid":1500742,"ip_address":"","ucode":"98629AFD9861EE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKUcSLVV6ia3dibe7qvTu8Vic1PVs2EibxoUdx930MC7j2Q9A6s4eibMDZlcicMFY0D0icd3RrDorMChu0zw/132","comment_is_top":false,"comment_ctime":1583594393,"is_pvip":false,"replies":[{"id":71949,"content":"是的，文件本身就是顺序读写。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1583820889,"ip_address":"","comment_id":185500,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师好，请问kafka是怎么实现的顺序存储呢？是操作系统log文件本身就是顺序存储的吗","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":486382,"discussion_content":"是的，文件本身就是顺序读写。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583820889,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":140560,"user_name":"Tesla","can_delete":false,"product_type":"c1","uid":1500742,"ip_address":"","ucode":"98629AFD9861EE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKUcSLVV6ia3dibe7qvTu8Vic1PVs2EibxoUdx930MC7j2Q9A6s4eibMDZlcicMFY0D0icd3RrDorMChu0zw/132","comment_is_top":false,"comment_ctime":1570986500,"is_pvip":false,"replies":[{"id":54354,"content":"PageCache是操作系统来控制的，对应用程序来说，就是访问文件，并不需要操作“访问PageCache”的API。\n\n比如，你提供了一个Web服务，至于每次请求的数据是从Redis（PageCache）中拿到的，还是从MySQL（文件）拿到的，使用Web服务的客户端确定不了，也并不知道。\n\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1571019630,"ip_address":"","comment_id":140560,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师好，请问一下，kafka利用pageCache减少IO。这样消费端是不是要知道Broker保存到pageCache中的对象的内存地址呢？而它们又是两个不同的进程，内存地址是怎么传递的呢？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":470441,"discussion_content":"是的，但这个过程是自动的，不需要实现开辟好，随着写随着开辟就好，而且，实际上这片存储空间在磁盘上并不一定是连续的，具体取决于使用的文件系统。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571019452,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":140559,"user_name":"Tesla","can_delete":false,"product_type":"c1","uid":1500742,"ip_address":"","ucode":"98629AFD9861EE","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKUcSLVV6ia3dibe7qvTu8Vic1PVs2EibxoUdx930MC7j2Q9A6s4eibMDZlcicMFY0D0icd3RrDorMChu0zw/132","comment_is_top":false,"comment_ctime":1570985978,"is_pvip":false,"replies":[{"id":54352,"content":"是的，但这个过程是自动的，不需要实现开辟好，随着写随着开辟就好，而且，实际上这片存储空间在磁盘上并不一定是连续的，具体取决于使用的文件系统。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1571019452,"ip_address":"","comment_id":140559,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师好，请问一下kafka使用顺序读写的方式，应该是在有正式数据写入之前，先在硬件上开辟一段连续的存储空间吧？等到有数据了再依次写入存储空间，就像列表需要指定对象类型和列表长度一样。那kafka是如何确定这段空间的大小呢？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466596,"discussion_content":"有的，你可以看一下这个配置：linger.ms ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567992669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131727,"user_name":"张洪阆","can_delete":false,"product_type":"c1","uid":1001954,"ip_address":"","ucode":"CFF7035D0DF059","user_header":"https://static001.geekbang.org/account/avatar/00/0f/49/e2/1fad12eb.jpg","comment_is_top":false,"comment_ctime":1567865478,"is_pvip":false,"replies":[{"id":50419,"content":"有的，你可以看一下这个配置：linger.ms ","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567992669,"ip_address":"","comment_id":131727,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"如果没攒够数据量是否就一直不发呢，有没有超时机制？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466521,"discussion_content":"只能由一个消费者来消费，因为在消费者收到消息解开这个“批消息”之前，这个批消息就是一个不能分割的整体。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567992270,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131575,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567816964,"is_pvip":false,"replies":[{"id":50416,"content":"只能由一个消费者来消费，因为在消费者收到消息解开这个“批消息”之前，这个批消息就是一个不能分割的整体。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567992270,"ip_address":"","comment_id":131575,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"构建批消息和解开批消息分别在发送端和消费端的客户端完成，不仅减轻了 Broker 的压力，最重要的是减少了 Broker 处理请求的次数，提升了总体的处理能力。\n\n这样的他，一批消息在多个消费端的情况下，这批消息这能落到一个消费端的吧？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465818,"discussion_content":"是的，并不限于磁盘到socket缓冲区。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567492487,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130120,"user_name":"嘉木","can_delete":false,"product_type":"c1","uid":1317999,"ip_address":"","ucode":"AF4877693782C0","user_header":"https://static001.geekbang.org/account/avatar/00/14/1c/6f/3ea2a599.jpg","comment_is_top":false,"comment_ctime":1567387674,"is_pvip":false,"replies":[{"id":48727,"content":"是的，并不限于磁盘到socket缓冲区。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567492487,"ip_address":"","comment_id":130120,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"如果你遇到这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。\n\n这个sendfile是不是从一个fd到另外一个fd的复制都是可以用的？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465017,"discussion_content":"一般还是推荐每个分区单线程消费，如果消费性能不行就扩容分区，这样实现简单并且可靠。你提到的第二种方法，不是说不能多线程异步（或者像你说的在while循环外）执行消费逻辑，这样是可以的，但是你必须保证“对于每条消息，只有执行完全部消费逻辑之后，才能提交消费位置”，这样才能不丢消息。只要能保证这点，无论是同步消费还是异步消费，或者自动还是手动提交消费位置都是可以的。但实际开发的时候，手动提交消费位置如果处理不好，很容易丢消息，所以不推荐。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567127602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128603,"user_name":"jack","can_delete":false,"product_type":"c1","uid":1612614,"ip_address":"","ucode":"5A26761299F7AC","user_header":"https://static001.geekbang.org/account/avatar/00/18/9b/46/ad3194bd.jpg","comment_is_top":false,"comment_ctime":1566912174,"is_pvip":false,"replies":[{"id":48173,"content":"一般还是推荐每个分区单线程消费，如果消费性能不行就扩容分区，这样实现简单并且可靠。你提到的第二种方法，不是说不能多线程异步（或者像你说的在while循环外）执行消费逻辑，这样是可以的，但是你必须保证“对于每条消息，只有执行完全部消费逻辑之后，才能提交消费位置”，这样才能不丢消息。只要能保证这点，无论是同步消费还是异步消费，或者自动还是手动提交消费位置都是可以的。但实际开发的时候，手动提交消费位置如果处理不好，很容易丢消息，所以不推荐。\n\n","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567127602,"ip_address":"","comment_id":128603,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师，关于kafka有几个问题期待您的解答：\n1、消费者是线程不安全的，多线程使用消费者，官方提供了两个方法，一是一个线程一个消费者，但是这样总的线程数受到分区数量的影响；二是一个线程或者几个线程把数据都消费到，然后将数据交给真正处理数据的线程池处理。想问您实际开发中哪种使用的更频繁，更多呢？\n2、消费者在poll时，总是处于while(true)循环中，那么逻辑处理就都放在循环中？如果想在循环外使用数据该怎么做呢？\n3、消费者为了保证“至少一次”处理，是否应该更多的采用手动提交offset？","like_count":1,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465017,"discussion_content":"一般还是推荐每个分区单线程消费，如果消费性能不行就扩容分区，这样实现简单并且可靠。你提到的第二种方法，不是说不能多线程异步（或者像你说的在while循环外）执行消费逻辑，这样是可以的，但是你必须保证“对于每条消息，只有执行完全部消费逻辑之后，才能提交消费位置”，这样才能不丢消息。只要能保证这点，无论是同步消费还是异步消费，或者自动还是手动提交消费位置都是可以的。但实际开发的时候，手动提交消费位置如果处理不好，很容易丢消息，所以不推荐。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567127602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":130144,"user_name":"牛牛","can_delete":false,"product_type":"c1","uid":1194626,"ip_address":"","ucode":"CFCE68B4F92209","user_header":"https://static001.geekbang.org/account/avatar/00/12/3a/82/1ff83a38.jpg","comment_is_top":false,"comment_ctime":1567390074,"is_pvip":false,"replies":[{"id":48728,"content":"生产者和消费者都没有自己的队列，这些队列在逻辑上属于主题，物理上保存在Broker上。或者你说的ReadQueue和WriteQueue是客户端在收发消息时候的缓存吗？这个是存在的，作用仅仅是在收发消息的时候缓存一部分消息而已。\n\n关于队列相关的概念，你可以再重新看一下03和08这两节课，里面有比价详细的阐述。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1567492791,"ip_address":"","comment_id":130144,"utype":1}],"discussion_count":1,"race_medal":0,"score":4,"product_id":100032301,"comment_content":"老师、想请教下 RocketMQ的queue\n1. 生产者和消费者都有自己的Queue吗 ？还是生产者的Queue就是消费这的Queue呢 ？\n   如果各自有独立的Queue、这两者之间有什么联系没 ？\n2. ReadQueue 和 WriteQueue 的作用是什么呢 ？这两个Queue都是针对某个topic而言还是针对Producer和Consumer而言呢 ？ReadQueue和WriteQueue的数量可以不一致么 ？\n3. 真正存储消息的地方是Queue、而Broker只是负责消息的路由？Broker的slave和master上包含相同的数据信息、对吗？\n\n有点儿乱了、希望老师或者路过的同学指点下 ^.^、先多谢啦~~~","like_count":0,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":465831,"discussion_content":"生产者和消费者都没有自己的队列，这些队列在逻辑上属于主题，物理上保存在Broker上。或者你说的ReadQueue和WriteQueue是客户端在收发消息时候的缓存吗？这个是存在的，作用仅仅是在收发消息的时候缓存一部分消息而已。\n\n关于队列相关的概念，你可以再重新看一下03和08这两节课，里面有比价详细的阐述。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567492791,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128546,"user_name":"游弋云端","can_delete":false,"product_type":"c1","uid":1208637,"ip_address":"","ucode":"A960E8F5AA25B9","user_header":"https://static001.geekbang.org/account/avatar/00/12/71/3d/da8dc880.jpg","comment_is_top":false,"comment_ctime":1566898798,"is_pvip":false,"replies":[{"id":47841,"content":"进程退出不会丢数据，操作系统会保证数据会被写入到磁盘中。但如果掉电了，数据是有可能会丢失的。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566972798,"ip_address":"","comment_id":128546,"utype":1}],"discussion_count":1,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"&quot;应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再一批一批地写到磁盘&quot;. 这个地方如果不调用sync()或者fsync()，如果保障进程异常退出或者节点掉电，数据不丢失的问题？","like_count":0},{"had_liked":false,"id":128340,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1566872372,"is_pvip":false,"replies":[{"id":47840,"content":"编程语言不是事儿，看不懂就学。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566972721,"ip_address":"","comment_id":128340,"utype":1}],"discussion_count":3,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"Kafka是scala写的 看不懂","like_count":0,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464990,"discussion_content":"进程退出不会丢数据，操作系统会保证数据会被写入到磁盘中。但如果掉电了，数据是有可能会丢失的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566972798,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":135966,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1569316818,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"课后思考及问题\nkafka之所以那么快的秘密\n1：消息批处理——减少网络通信开销\n2：磁盘顺序写——减少寻道移臂开销\n3：缓存页——减少磁盘IO开销\n4：零拷贝——减少数据多次拷贝的开销\n以上基本是一个快速的数据处理组件或系统的标配了，再加上池化技术、异步化技术、不可变技术、多线程并发编程、事件驱动模型、无锁化技术。","like_count":118,"discussions":[{"author":{"id":3528598,"avatar":"","nickname":"Geek_0107bf","note":"","ucode":"9CF551F2F58852","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":615682,"discussion_content":"池化技术、异步化技术、不可变技术、多线程并发编程、事件驱动模型、无锁化技术。能展开讲讲不","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682392148,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":187401,"user_name":"Ryoma","can_delete":false,"product_type":"c1","uid":1130590,"ip_address":"","ucode":"7F692369239692","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/5e/b8fada94.jpg","comment_is_top":false,"comment_ctime":1584102124,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"前面提到发消息时可以利用 key 保证分区有序，之前的想法是消息发到服务端，服务端根据 key 做 hash 来分配到对应分区；但按照批处理消息的说明，明显不会分解消息。\n又去看了使用的 node 客户端，果然这部分是在客户端做的 hash。","like_count":2},{"had_liked":false,"id":128677,"user_name":"老杨同志","can_delete":false,"product_type":"c1","uid":1246199,"ip_address":"","ucode":"3F334F0CFD3DE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/f7/3a493bec.jpg","comment_is_top":false,"comment_ctime":1566950107,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"看了老师写的kafka这么多优点，从处理过程感觉可能会有几个缺点：1攒一批处理，可能会增加单条消息的延时2批消息是客户端组织的，broker也不会是拆解批消息，多个客户端发送时不能保证有序性。3消费时有一条消息失败整个批次重试","like_count":1,"discussions":[{"author":{"id":1250327,"avatar":"https://static001.geekbang.org/account/avatar/00/13/14/17/8763dced.jpg","nickname":"微微一笑","note":"","ucode":"CFA7ABE81D0B99","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6742,"discussion_content":"谢谢您对我问题的解答哦，我好像在林晓斌老师的《Mysql实战45讲》的专栏里见过您！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567079415,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373688,"user_name":"辰星","can_delete":false,"product_type":"c1","uid":1037406,"ip_address":"广东","ucode":"C05AF875B85718","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d4/5e/b8bfa75d.jpg","comment_is_top":false,"comment_ctime":1683016686,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"Kafka高效的原因 \n1. 批量处理 \n2. 磁盘顺序读写\n3.Page Cache\n4. 零拷贝","like_count":0},{"had_liked":false,"id":369690,"user_name":"小红帽","can_delete":false,"product_type":"c1","uid":1135290,"ip_address":"广东","ucode":"876000FB67C980","user_header":"https://static001.geekbang.org/account/avatar/00/11/52/ba/440c0157.jpg","comment_is_top":false,"comment_ctime":1677812752,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"以kafka为例，本周要看下对应的源码是如何实现的：\n1、如何实现批量读写，以及顺序写是如何实现的？；\n2、如何利用pagecache和零拷贝技术加快IO性能；\n最后有个概念我们不要混了，顺序读和随机读，我们只要保证大部分的读取是连续的块少进行随机操作就可以了。","like_count":0},{"had_liked":false,"id":367126,"user_name":"谁都会变","can_delete":false,"product_type":"c1","uid":1195017,"ip_address":"上海","ucode":"9965748F7EBB57","user_header":"https://static001.geekbang.org/account/avatar/00/12/3c/09/b7f0eac6.jpg","comment_is_top":false,"comment_ctime":1674961714,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"零拷贝，这个零说的是没有CPU参与？","like_count":0},{"had_liked":false,"id":347379,"user_name":"🍉","can_delete":false,"product_type":"c1","uid":1326566,"ip_address":"","ucode":"316A485DB352E9","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/e6/3e7c82a6.jpg","comment_is_top":false,"comment_ctime":1653979797,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"老师问一下。既然kafka是这种异步批量发送。和批量消费的。那么该如何保证消息的顺序性呢？","like_count":0},{"had_liked":false,"id":346590,"user_name":"SharpBB","can_delete":false,"product_type":"c1","uid":2014573,"ip_address":"","ucode":"D30C5B798B8E8C","user_header":"https://static001.geekbang.org/account/avatar/00/1e/bd/6d/7010f98e.jpg","comment_is_top":false,"comment_ctime":1653271876,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"批处理\n\t设计中充斥着批处理的概念 一条消息不会被立刻发出去  而是放入缓存 选择合适的时机将多条消息变为一条批消息 发送 之后在consumer里再解出来\n顺序读写\n\t随机读写为什么慢 因为里面包含了内存寻址  每次读写都要寻址\n\t而顺序写入log文件 写满了就再开一个log 读的时候也通过某个全局位置开始读 只需要寻址一次即可\n缓存\n\t使用了pagecache加速读写\n\t\t这是操作系统提供的 在写文件的时候先写入pc 下次读直接读pc即可\n\t\t清除数据使用了类似的lru算法 而保留最近最常使用的数据\n\t\t当pageCache中没有缓存时 就会触发os的缺页中断 阻塞读 先将文件写入pc 再从pc读取\n\t\t\t发现不管哪里都有缓存的概念 多用内存\n\t所以当mq写入时就会被消费  并且很容易命中缓存 效率很高\n零拷贝\n\t如果直接读到缓存 那么不需要拷贝 直接用即可\n\t没有读到缓存的情况 就需要先写入pc pc复制到用户操作对象的内存空间 再从内存空间复制到socket缓冲区\n\t零拷贝则直接将数据复制到socket缓冲区 减少了一次复制\n\t\t说白了 这其中的操作不需要对数据进行什么 处理 那么就可以省去复制到用户内存空间这一步","like_count":0,"discussions":[{"author":{"id":1250327,"avatar":"https://static001.geekbang.org/account/avatar/00/13/14/17/8763dced.jpg","nickname":"微微一笑","note":"","ucode":"CFA7ABE81D0B99","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6742,"discussion_content":"谢谢您对我问题的解答哦，我好像在林晓斌老师的《Mysql实战45讲》的专栏里见过您！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567079415,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128546,"user_name":"游弋云端","can_delete":false,"product_type":"c1","uid":1208637,"ip_address":"","ucode":"A960E8F5AA25B9","user_header":"https://static001.geekbang.org/account/avatar/00/12/71/3d/da8dc880.jpg","comment_is_top":false,"comment_ctime":1566898798,"is_pvip":false,"replies":[{"id":47841,"content":"进程退出不会丢数据，操作系统会保证数据会被写入到磁盘中。但如果掉电了，数据是有可能会丢失的。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566972798,"ip_address":"","comment_id":128546,"utype":1}],"discussion_count":1,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"&quot;应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再一批一批地写到磁盘&quot;. 这个地方如果不调用sync()或者fsync()，如果保障进程异常退出或者节点掉电，数据不丢失的问题？","like_count":0,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464990,"discussion_content":"进程退出不会丢数据，操作系统会保证数据会被写入到磁盘中。但如果掉电了，数据是有可能会丢失的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566972798,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":128340,"user_name":"飞翔","can_delete":false,"product_type":"c1","uid":1068571,"ip_address":"","ucode":"65AF6AF292DAD6","user_header":"https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg","comment_is_top":false,"comment_ctime":1566872372,"is_pvip":false,"replies":[{"id":47840,"content":"编程语言不是事儿，看不懂就学。","user_name":"作者回复","user_name_real":"李玥","uid":1501046,"ctime":1566972721,"ip_address":"","comment_id":128340,"utype":1}],"discussion_count":3,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"Kafka是scala写的 看不懂","like_count":0,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464905,"discussion_content":"编程语言不是事儿，看不懂就学。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566972721,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1980201,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/37/29/b3af57a7.jpg","nickname":"凯文小猪","note":"","ucode":"36D8AD0229547F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":373149,"discussion_content":"0.9+以后用java重构了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620631907,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1530493,"avatar":"","nickname":"Geek_e7834d","note":"","ucode":"ABAF1B1B7E6490","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6454,"discussion_content":"有很多不是核心的code是java的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566907537,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":135966,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1569316818,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"课后思考及问题\nkafka之所以那么快的秘密\n1：消息批处理——减少网络通信开销\n2：磁盘顺序写——减少寻道移臂开销\n3：缓存页——减少磁盘IO开销\n4：零拷贝——减少数据多次拷贝的开销\n以上基本是一个快速的数据处理组件或系统的标配了，再加上池化技术、异步化技术、不可变技术、多线程并发编程、事件驱动模型、无锁化技术。","like_count":118,"discussions":[{"author":{"id":1501046,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e7/76/79c1f23a.jpg","nickname":"李玥","note":"","ucode":"B19E91EE248591","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":464905,"discussion_content":"编程语言不是事儿，看不懂就学。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566972721,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1980201,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/37/29/b3af57a7.jpg","nickname":"凯文小猪","note":"","ucode":"36D8AD0229547F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":373149,"discussion_content":"0.9+以后用java重构了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620631907,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1530493,"avatar":"","nickname":"Geek_e7834d","note":"","ucode":"ABAF1B1B7E6490","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":6454,"discussion_content":"有很多不是核心的code是java的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1566907537,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":187401,"user_name":"Ryoma","can_delete":false,"product_type":"c1","uid":1130590,"ip_address":"","ucode":"7F692369239692","user_header":"https://static001.geekbang.org/account/avatar/00/11/40/5e/b8fada94.jpg","comment_is_top":false,"comment_ctime":1584102124,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"前面提到发消息时可以利用 key 保证分区有序，之前的想法是消息发到服务端，服务端根据 key 做 hash 来分配到对应分区；但按照批处理消息的说明，明显不会分解消息。\n又去看了使用的 node 客户端，果然这部分是在客户端做的 hash。","like_count":2},{"had_liked":false,"id":128677,"user_name":"老杨同志","can_delete":false,"product_type":"c1","uid":1246199,"ip_address":"","ucode":"3F334F0CFD3DE6","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/f7/3a493bec.jpg","comment_is_top":false,"comment_ctime":1566950107,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"看了老师写的kafka这么多优点，从处理过程感觉可能会有几个缺点：1攒一批处理，可能会增加单条消息的延时2批消息是客户端组织的，broker也不会是拆解批消息，多个客户端发送时不能保证有序性。3消费时有一条消息失败整个批次重试","like_count":1,"discussions":[{"author":{"id":3528598,"avatar":"","nickname":"Geek_0107bf","note":"","ucode":"9CF551F2F58852","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":615682,"discussion_content":"池化技术、异步化技术、不可变技术、多线程并发编程、事件驱动模型、无锁化技术。能展开讲讲不","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682392148,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373688,"user_name":"辰星","can_delete":false,"product_type":"c1","uid":1037406,"ip_address":"广东","ucode":"C05AF875B85718","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d4/5e/b8bfa75d.jpg","comment_is_top":false,"comment_ctime":1683016686,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"Kafka高效的原因 \n1. 批量处理 \n2. 磁盘顺序读写\n3.Page Cache\n4. 零拷贝","like_count":0},{"had_liked":false,"id":369690,"user_name":"小红帽","can_delete":false,"product_type":"c1","uid":1135290,"ip_address":"广东","ucode":"876000FB67C980","user_header":"https://static001.geekbang.org/account/avatar/00/11/52/ba/440c0157.jpg","comment_is_top":false,"comment_ctime":1677812752,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"以kafka为例，本周要看下对应的源码是如何实现的：\n1、如何实现批量读写，以及顺序写是如何实现的？；\n2、如何利用pagecache和零拷贝技术加快IO性能；\n最后有个概念我们不要混了，顺序读和随机读，我们只要保证大部分的读取是连续的块少进行随机操作就可以了。","like_count":0},{"had_liked":false,"id":367126,"user_name":"谁都会变","can_delete":false,"product_type":"c1","uid":1195017,"ip_address":"上海","ucode":"9965748F7EBB57","user_header":"https://static001.geekbang.org/account/avatar/00/12/3c/09/b7f0eac6.jpg","comment_is_top":false,"comment_ctime":1674961714,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"零拷贝，这个零说的是没有CPU参与？","like_count":0},{"had_liked":false,"id":347379,"user_name":"🍉","can_delete":false,"product_type":"c1","uid":1326566,"ip_address":"","ucode":"316A485DB352E9","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/e6/3e7c82a6.jpg","comment_is_top":false,"comment_ctime":1653979797,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"老师问一下。既然kafka是这种异步批量发送。和批量消费的。那么该如何保证消息的顺序性呢？","like_count":0},{"had_liked":false,"id":346590,"user_name":"SharpBB","can_delete":false,"product_type":"c1","uid":2014573,"ip_address":"","ucode":"D30C5B798B8E8C","user_header":"https://static001.geekbang.org/account/avatar/00/1e/bd/6d/7010f98e.jpg","comment_is_top":false,"comment_ctime":1653271876,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":5,"product_id":100032301,"comment_content":"批处理\n\t设计中充斥着批处理的概念 一条消息不会被立刻发出去  而是放入缓存 选择合适的时机将多条消息变为一条批消息 发送 之后在consumer里再解出来\n顺序读写\n\t随机读写为什么慢 因为里面包含了内存寻址  每次读写都要寻址\n\t而顺序写入log文件 写满了就再开一个log 读的时候也通过某个全局位置开始读 只需要寻址一次即可\n缓存\n\t使用了pagecache加速读写\n\t\t这是操作系统提供的 在写文件的时候先写入pc 下次读直接读pc即可\n\t\t清除数据使用了类似的lru算法 而保留最近最常使用的数据\n\t\t当pageCache中没有缓存时 就会触发os的缺页中断 阻塞读 先将文件写入pc 再从pc读取\n\t\t\t发现不管哪里都有缓存的概念 多用内存\n\t所以当mq写入时就会被消费  并且很容易命中缓存 效率很高\n零拷贝\n\t如果直接读到缓存 那么不需要拷贝 直接用即可\n\t没有读到缓存的情况 就需要先写入pc pc复制到用户操作对象的内存空间 再从内存空间复制到socket缓冲区\n\t零拷贝则直接将数据复制到socket缓冲区 减少了一次复制\n\t\t说白了 这其中的操作不需要对数据进行什么 处理 那么就可以省去复制到用户内存空间这一步","like_count":0},{"had_liked":false,"id":341014,"user_name":"再见理想","can_delete":false,"product_type":"c1","uid":1245999,"ip_address":"","ucode":"FAC88B3F6F6DFD","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/2f/0a5e0751.jpg","comment_is_top":false,"comment_ctime":1649296537,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"Kafka高性能的几个关键技术点\n1.批量处理消息\n2.磁盘顺序读写＞随机读写\n3.pagecache缓存提升数据读取速度\n4.0拷贝技术 直接将数据从pagecache复制到socket缓冲区，不经过应用程序的缓冲区，减少了一次数据拷贝","like_count":0},{"had_liked":false,"id":340835,"user_name":"等风来","can_delete":false,"product_type":"c1","uid":1350677,"ip_address":"","ucode":"5B7FF74A51F534","user_header":"https://static001.geekbang.org/account/avatar/00/14/9c/15/719f1f44.jpg","comment_is_top":false,"comment_ctime":1649171723,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"有个疑问，在用户态层面，pagecache对应用来说应该是不可见的， 操作系统默认都会用到pagcache来加快io响应，我们能控制的是不是就是pagecache的大小","like_count":0},{"had_liked":false,"id":338820,"user_name":"千锤百炼领悟之极限","can_delete":false,"product_type":"c1","uid":1744257,"ip_address":"","ucode":"224B5CF2101716","user_header":"https://static001.geekbang.org/account/avatar/00/1a/9d/81/d748b7eb.jpg","comment_is_top":false,"comment_ctime":1647752462,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"Kafka如何实现高性能IO?\n\n1.使用批量处理的方式来提高系统吞吐能力。\n2.基于磁盘文件高性能顺序读写的特性，设计存储结构。\n3.利用操作系统的PageCache来缓存数据，减少IO并提升读性能。\n4.使用零拷贝技术来加速消费流程。","like_count":0},{"had_liked":false,"id":336914,"user_name":"William Ning","can_delete":false,"product_type":"c1","uid":1592279,"ip_address":"","ucode":"4DB8D05E69E5F3","user_header":"https://static001.geekbang.org/account/avatar/00/18/4b/d7/f46c6dfd.jpg","comment_is_top":false,"comment_ctime":1646466157,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"看到这儿，发现前面的知识忘得差不多了，还是需要回顾，然后继续前进，如此反复，同时思考和实践。","like_count":0},{"had_liked":false,"id":324915,"user_name":"Geek_e7c5f3","can_delete":false,"product_type":"c1","uid":2122979,"ip_address":"","ucode":"C1CBCC3655F465","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/YrTewgZu6SicWJkepbTSuhVHfBBCyEkc0VH6TOVnaCtpp7FxMdN98j6D42TLiaYjsZBiaic2GLr32JE00Tlibap95aw/132","comment_is_top":false,"comment_ctime":1638715214,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"老师您好，咨询两个问题\n1.在生产端send出去的数据实际上会攒一波才会发送到kafka服务器，这里会有一个问题，如果在攒的过程中奔溃了，那这些数据会如何？如果丢失有方式处理吗？\n2.消费端想做到数据落地后再回复ack到kafka服务器，这点如何操作呢？","like_count":0},{"had_liked":false,"id":298172,"user_name":"阿凡达","can_delete":false,"product_type":"c1","uid":1738491,"ip_address":"","ucode":"DD61CA1D55EF3C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/5fibB9p7PfndN4kU0lhHv4dmevIWxNIjU9KQjHfKsD3JSTSol2DjbYc1PunOeLn7W3Ficoxr2aJNaicN52KjRKSCQ/132","comment_is_top":false,"comment_ctime":1623933292,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"老师请教个问题，如果发送端是批量发送消息，那发送端的应用端代码如何确保数据不会丢失","like_count":0},{"had_liked":false,"id":292965,"user_name":"芋头","can_delete":false,"product_type":"c1","uid":1227492,"ip_address":"","ucode":"A9C875548E4EE5","user_header":"https://static001.geekbang.org/account/avatar/00/12/ba/e4/6df89add.jpg","comment_is_top":false,"comment_ctime":1621120290,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"Kafka是如何实现高性能IO？\n1.消息批量处理 减少网络io的开销\n2.顺序读写\n3.page cache 减少多次磁盘读写的io\n4.零拷贝 减少多次复制的开销\n","like_count":0},{"had_liked":false,"id":292463,"user_name":"张新国","can_delete":false,"product_type":"c1","uid":1154363,"ip_address":"","ucode":"75E8710FB31CC4","user_header":"https://static001.geekbang.org/account/avatar/00/11/9d/3b/5b5b3ec9.jpg","comment_is_top":false,"comment_ctime":1620830465,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"老师，如果消费端接收到的是一个“批消息”的话，那如果批消息的大小比fetch.max.bytes大或者数量比max.poll.records多，那么这两个配置有什么用呢","like_count":0},{"had_liked":false,"id":290067,"user_name":"非想","can_delete":false,"product_type":"c1","uid":1140949,"ip_address":"","ucode":"D8CA6ADC20D4AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/68/d5/567bf8ad.jpg","comment_is_top":false,"comment_ctime":1619344270,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"对于磁盘来说，它有一个特性，就是顺序读写的性能要远远好于随机读写。在 SSD（固态硬盘）上，顺序读写的性能要比随机读写快几倍，如果是机械硬盘，这个差距会达到几十倍。\n\n老师，你文章的这句话，我个人理解是不管是机械硬盘还是固态硬盘的顺序读写永远比随机读写要快，但是固态硬盘和机械硬盘两者的顺序读写性能是差不多的，固态硬盘只是比机械硬盘在随机读写方面提升了很多性能","like_count":0},{"had_liked":false,"id":283391,"user_name":"吴月月鸟","can_delete":false,"product_type":"c1","uid":1115064,"ip_address":"","ucode":"2F068EDD166B49","user_header":"https://static001.geekbang.org/account/avatar/00/11/03/b8/961a5342.jpg","comment_is_top":false,"comment_ctime":1615739892,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"总之如何解决IO性能瓶颈是问题的关键。","like_count":0},{"had_liked":false,"id":341014,"user_name":"再见理想","can_delete":false,"product_type":"c1","uid":1245999,"ip_address":"","ucode":"FAC88B3F6F6DFD","user_header":"https://static001.geekbang.org/account/avatar/00/13/03/2f/0a5e0751.jpg","comment_is_top":false,"comment_ctime":1649296537,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"Kafka高性能的几个关键技术点\n1.批量处理消息\n2.磁盘顺序读写＞随机读写\n3.pagecache缓存提升数据读取速度\n4.0拷贝技术 直接将数据从pagecache复制到socket缓冲区，不经过应用程序的缓冲区，减少了一次数据拷贝","like_count":0},{"had_liked":false,"id":340835,"user_name":"等风来","can_delete":false,"product_type":"c1","uid":1350677,"ip_address":"","ucode":"5B7FF74A51F534","user_header":"https://static001.geekbang.org/account/avatar/00/14/9c/15/719f1f44.jpg","comment_is_top":false,"comment_ctime":1649171723,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"有个疑问，在用户态层面，pagecache对应用来说应该是不可见的， 操作系统默认都会用到pagcache来加快io响应，我们能控制的是不是就是pagecache的大小","like_count":0},{"had_liked":false,"id":338820,"user_name":"千锤百炼领悟之极限","can_delete":false,"product_type":"c1","uid":1744257,"ip_address":"","ucode":"224B5CF2101716","user_header":"https://static001.geekbang.org/account/avatar/00/1a/9d/81/d748b7eb.jpg","comment_is_top":false,"comment_ctime":1647752462,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"Kafka如何实现高性能IO?\n\n1.使用批量处理的方式来提高系统吞吐能力。\n2.基于磁盘文件高性能顺序读写的特性，设计存储结构。\n3.利用操作系统的PageCache来缓存数据，减少IO并提升读性能。\n4.使用零拷贝技术来加速消费流程。","like_count":0},{"had_liked":false,"id":336914,"user_name":"William Ning","can_delete":false,"product_type":"c1","uid":1592279,"ip_address":"","ucode":"4DB8D05E69E5F3","user_header":"https://static001.geekbang.org/account/avatar/00/18/4b/d7/f46c6dfd.jpg","comment_is_top":false,"comment_ctime":1646466157,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"看到这儿，发现前面的知识忘得差不多了，还是需要回顾，然后继续前进，如此反复，同时思考和实践。","like_count":0},{"had_liked":false,"id":324915,"user_name":"Geek_e7c5f3","can_delete":false,"product_type":"c1","uid":2122979,"ip_address":"","ucode":"C1CBCC3655F465","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/YrTewgZu6SicWJkepbTSuhVHfBBCyEkc0VH6TOVnaCtpp7FxMdN98j6D42TLiaYjsZBiaic2GLr32JE00Tlibap95aw/132","comment_is_top":false,"comment_ctime":1638715214,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"老师您好，咨询两个问题\n1.在生产端send出去的数据实际上会攒一波才会发送到kafka服务器，这里会有一个问题，如果在攒的过程中奔溃了，那这些数据会如何？如果丢失有方式处理吗？\n2.消费端想做到数据落地后再回复ack到kafka服务器，这点如何操作呢？","like_count":0},{"had_liked":false,"id":298172,"user_name":"阿凡达","can_delete":false,"product_type":"c1","uid":1738491,"ip_address":"","ucode":"DD61CA1D55EF3C","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/5fibB9p7PfndN4kU0lhHv4dmevIWxNIjU9KQjHfKsD3JSTSol2DjbYc1PunOeLn7W3Ficoxr2aJNaicN52KjRKSCQ/132","comment_is_top":false,"comment_ctime":1623933292,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"老师请教个问题，如果发送端是批量发送消息，那发送端的应用端代码如何确保数据不会丢失","like_count":0},{"had_liked":false,"id":292965,"user_name":"芋头","can_delete":false,"product_type":"c1","uid":1227492,"ip_address":"","ucode":"A9C875548E4EE5","user_header":"https://static001.geekbang.org/account/avatar/00/12/ba/e4/6df89add.jpg","comment_is_top":false,"comment_ctime":1621120290,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"Kafka是如何实现高性能IO？\n1.消息批量处理 减少网络io的开销\n2.顺序读写\n3.page cache 减少多次磁盘读写的io\n4.零拷贝 减少多次复制的开销\n","like_count":0},{"had_liked":false,"id":292463,"user_name":"张新国","can_delete":false,"product_type":"c1","uid":1154363,"ip_address":"","ucode":"75E8710FB31CC4","user_header":"https://static001.geekbang.org/account/avatar/00/11/9d/3b/5b5b3ec9.jpg","comment_is_top":false,"comment_ctime":1620830465,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"老师，如果消费端接收到的是一个“批消息”的话，那如果批消息的大小比fetch.max.bytes大或者数量比max.poll.records多，那么这两个配置有什么用呢","like_count":0},{"had_liked":false,"id":290067,"user_name":"非想","can_delete":false,"product_type":"c1","uid":1140949,"ip_address":"","ucode":"D8CA6ADC20D4AA","user_header":"https://static001.geekbang.org/account/avatar/00/11/68/d5/567bf8ad.jpg","comment_is_top":false,"comment_ctime":1619344270,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"对于磁盘来说，它有一个特性，就是顺序读写的性能要远远好于随机读写。在 SSD（固态硬盘）上，顺序读写的性能要比随机读写快几倍，如果是机械硬盘，这个差距会达到几十倍。\n\n老师，你文章的这句话，我个人理解是不管是机械硬盘还是固态硬盘的顺序读写永远比随机读写要快，但是固态硬盘和机械硬盘两者的顺序读写性能是差不多的，固态硬盘只是比机械硬盘在随机读写方面提升了很多性能","like_count":0},{"had_liked":false,"id":283391,"user_name":"吴月月鸟","can_delete":false,"product_type":"c1","uid":1115064,"ip_address":"","ucode":"2F068EDD166B49","user_header":"https://static001.geekbang.org/account/avatar/00/11/03/b8/961a5342.jpg","comment_is_top":false,"comment_ctime":1615739892,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":6,"product_id":100032301,"comment_content":"总之如何解决IO性能瓶颈是问题的关键。","like_count":0},{"had_liked":false,"id":274020,"user_name":"黑曼巴ye","can_delete":false,"product_type":"c1","uid":1180877,"ip_address":"","ucode":"3ACFD4B79E46A1","user_header":"https://static001.geekbang.org/account/avatar/00/12/04/cd/3f73dead.jpg","comment_is_top":false,"comment_ctime":1610789920,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"攒着，会有可能导致消息丢失吧？这和基础篇保证消息不丢的方案是不是有冲突？","like_count":0},{"had_liked":false,"id":272488,"user_name":"毛怪","can_delete":false,"product_type":"c1","uid":1879932,"ip_address":"","ucode":"1E54CE9755AEF3","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/7c/6d90b40a.jpg","comment_is_top":false,"comment_ctime":1610098777,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"“只有相同分区的消息才能组成同一个批消息”，老师，根据这句话是不是生产消息的时候计算法消息大小按批指定分区发送消息可以提高消息的相应速度和吞吐量。","like_count":0},{"had_liked":false,"id":272482,"user_name":"毛怪","can_delete":false,"product_type":"c1","uid":1879932,"ip_address":"","ucode":"1E54CE9755AEF3","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/7c/6d90b40a.jpg","comment_is_top":false,"comment_ctime":1610097934,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"生产大于消费导致消息堆积的时候，写入到服务端的消息不能被立即消费，消费端拉取消息服务端需要从磁盘读取到缓存。PageCache缓存优先清除最近最少使用的页的特性是不是就使用不到了。","like_count":0},{"had_liked":false,"id":243385,"user_name":"翠羽香凝","can_delete":false,"product_type":"c1","uid":1119933,"ip_address":"","ucode":"54F3762F0E545F","user_header":"https://static001.geekbang.org/account/avatar/00/11/16/bd/e14ba493.jpg","comment_is_top":false,"comment_ctime":1598082498,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"关于“利用 PageCache 加速消息读写”这一点，不是特别理解。PageCache不是操作系统实现的吗？Kafka只要利用正常的IO读写，都会使用PageCache，而且这个过程是操作系统自动完成的，那么Kafka在这里难道做了什么特别的处理吗？","like_count":0},{"had_liked":false,"id":241016,"user_name":"skyun","can_delete":false,"product_type":"c1","uid":1002658,"ip_address":"","ucode":"38097F3FF1045C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4c/a2/00241866.jpg","comment_is_top":false,"comment_ctime":1597151477,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"老师，问一下，kafka使用sendfile发送数据，数据并不会被拷贝到用户态中，根据我的理解，kafka能从index文件中知道从哪个位置开始拷贝，但是并不知道需要拷贝的数据长度，数据长度是存在data文件中的，但是data中的数据不会被拷贝到用户态中，最终是怎么知道要发送的数据长度的，往老师解答一下","like_count":0},{"had_liked":false,"id":231247,"user_name":"朱月俊","can_delete":false,"product_type":"c1","uid":1017707,"ip_address":"","ucode":"4DA0728B862FBD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/87/6b/0b6cd39a.jpg","comment_is_top":false,"comment_ctime":1593613934,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"请教个问题，消费端消费完信息，ack是攒一波发还是一个一个发的哈\n\n","like_count":0},{"had_liked":false,"id":200135,"user_name":"凌空飞起的剪刀腿","can_delete":false,"product_type":"c1","uid":1243680,"ip_address":"","ucode":"16FBBF4A3B54C6","user_header":"https://static001.geekbang.org/account/avatar/00/12/fa/20/0f06b080.jpg","comment_is_top":false,"comment_ctime":1585548412,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"https:&#47;&#47;time.geekbang.org&#47;column&#47;article&#47;198947\n这篇文章中提到\n看出生产的消息越少，平均响应时间越长。可见顺序写得越多，那每次写的平均时间就会越小，所以 Kafka 在大数据量的读写中会表现得非常好！","like_count":0},{"had_liked":false,"id":150194,"user_name":"付锐涛","can_delete":false,"product_type":"c1","uid":1450711,"ip_address":"","ucode":"F5BF0571596C4B","user_header":"https://static001.geekbang.org/account/avatar/00/16/22/d7/d166f764.jpg","comment_is_top":false,"comment_ctime":1573477009,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"批量处理\n顺序操作磁盘\n缓存\n减少可减少的流程","like_count":0},{"had_liked":false,"id":148304,"user_name":"又双叒叕是一年啊","can_delete":false,"product_type":"c1","uid":1000015,"ip_address":"","ucode":"E067320E537DEE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/4f/ff1ac464.jpg","comment_is_top":false,"comment_ctime":1572971118,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"这篇文章很给力点赞","like_count":0},{"had_liked":false,"id":136036,"user_name":"长期规划","can_delete":false,"product_type":"c1","uid":1019332,"ip_address":"","ucode":"5EF65E9115834B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","comment_is_top":false,"comment_ctime":1569334273,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"Kafk的实现高性能的四个方法中，Pagecache我没明白，Pagecache我知道，但这是OS控制的啊，没明白Kafka怎么利用它的","like_count":0},{"had_liked":false,"id":274020,"user_name":"黑曼巴ye","can_delete":false,"product_type":"c1","uid":1180877,"ip_address":"","ucode":"3ACFD4B79E46A1","user_header":"https://static001.geekbang.org/account/avatar/00/12/04/cd/3f73dead.jpg","comment_is_top":false,"comment_ctime":1610789920,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"攒着，会有可能导致消息丢失吧？这和基础篇保证消息不丢的方案是不是有冲突？","like_count":0},{"had_liked":false,"id":272488,"user_name":"毛怪","can_delete":false,"product_type":"c1","uid":1879932,"ip_address":"","ucode":"1E54CE9755AEF3","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/7c/6d90b40a.jpg","comment_is_top":false,"comment_ctime":1610098777,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"“只有相同分区的消息才能组成同一个批消息”，老师，根据这句话是不是生产消息的时候计算法消息大小按批指定分区发送消息可以提高消息的相应速度和吞吐量。","like_count":0},{"had_liked":false,"id":272482,"user_name":"毛怪","can_delete":false,"product_type":"c1","uid":1879932,"ip_address":"","ucode":"1E54CE9755AEF3","user_header":"https://static001.geekbang.org/account/avatar/00/1c/af/7c/6d90b40a.jpg","comment_is_top":false,"comment_ctime":1610097934,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"生产大于消费导致消息堆积的时候，写入到服务端的消息不能被立即消费，消费端拉取消息服务端需要从磁盘读取到缓存。PageCache缓存优先清除最近最少使用的页的特性是不是就使用不到了。","like_count":0},{"had_liked":false,"id":243385,"user_name":"翠羽香凝","can_delete":false,"product_type":"c1","uid":1119933,"ip_address":"","ucode":"54F3762F0E545F","user_header":"https://static001.geekbang.org/account/avatar/00/11/16/bd/e14ba493.jpg","comment_is_top":false,"comment_ctime":1598082498,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"关于“利用 PageCache 加速消息读写”这一点，不是特别理解。PageCache不是操作系统实现的吗？Kafka只要利用正常的IO读写，都会使用PageCache，而且这个过程是操作系统自动完成的，那么Kafka在这里难道做了什么特别的处理吗？","like_count":0},{"had_liked":false,"id":241016,"user_name":"skyun","can_delete":false,"product_type":"c1","uid":1002658,"ip_address":"","ucode":"38097F3FF1045C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4c/a2/00241866.jpg","comment_is_top":false,"comment_ctime":1597151477,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"老师，问一下，kafka使用sendfile发送数据，数据并不会被拷贝到用户态中，根据我的理解，kafka能从index文件中知道从哪个位置开始拷贝，但是并不知道需要拷贝的数据长度，数据长度是存在data文件中的，但是data中的数据不会被拷贝到用户态中，最终是怎么知道要发送的数据长度的，往老师解答一下","like_count":0},{"had_liked":false,"id":231247,"user_name":"朱月俊","can_delete":false,"product_type":"c1","uid":1017707,"ip_address":"","ucode":"4DA0728B862FBD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/87/6b/0b6cd39a.jpg","comment_is_top":false,"comment_ctime":1593613934,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"请教个问题，消费端消费完信息，ack是攒一波发还是一个一个发的哈\n\n","like_count":0},{"had_liked":false,"id":200135,"user_name":"凌空飞起的剪刀腿","can_delete":false,"product_type":"c1","uid":1243680,"ip_address":"","ucode":"16FBBF4A3B54C6","user_header":"https://static001.geekbang.org/account/avatar/00/12/fa/20/0f06b080.jpg","comment_is_top":false,"comment_ctime":1585548412,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"https:&#47;&#47;time.geekbang.org&#47;column&#47;article&#47;198947\n这篇文章中提到\n看出生产的消息越少，平均响应时间越长。可见顺序写得越多，那每次写的平均时间就会越小，所以 Kafka 在大数据量的读写中会表现得非常好！","like_count":0},{"had_liked":false,"id":150194,"user_name":"付锐涛","can_delete":false,"product_type":"c1","uid":1450711,"ip_address":"","ucode":"F5BF0571596C4B","user_header":"https://static001.geekbang.org/account/avatar/00/16/22/d7/d166f764.jpg","comment_is_top":false,"comment_ctime":1573477009,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"批量处理\n顺序操作磁盘\n缓存\n减少可减少的流程","like_count":0},{"had_liked":false,"id":148304,"user_name":"又双叒叕是一年啊","can_delete":false,"product_type":"c1","uid":1000015,"ip_address":"","ucode":"E067320E537DEE","user_header":"https://static001.geekbang.org/account/avatar/00/0f/42/4f/ff1ac464.jpg","comment_is_top":false,"comment_ctime":1572971118,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"这篇文章很给力点赞","like_count":0},{"had_liked":false,"id":136036,"user_name":"长期规划","can_delete":false,"product_type":"c1","uid":1019332,"ip_address":"","ucode":"5EF65E9115834B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","comment_is_top":false,"comment_ctime":1569334273,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":7,"product_id":100032301,"comment_content":"Kafk的实现高性能的四个方法中，Pagecache我没明白，Pagecache我知道，但这是OS控制的啊，没明白Kafka怎么利用它的","like_count":0},{"had_liked":false,"id":131581,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567817497,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"Pagecache的作用和mysql里的innodb buffer作用基本一样的，都是读取时从buffer里取数据，若果数据不存在，则发生缺页，再从磁盘读取，放入buffer，都有加速客户端读取的效果，包括写操作。","like_count":0},{"had_liked":false,"id":131578,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567817234,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"建议 零复制 这款增减一个图吧，看图在理解的话就方便的多了，谢谢！","like_count":0},{"had_liked":false,"id":129563,"user_name":"K-Li","can_delete":false,"product_type":"c1","uid":1361330,"ip_address":"","ucode":"3091322142E43E","user_header":"https://static001.geekbang.org/account/avatar/00/14/c5/b2/5b339c64.jpg","comment_is_top":false,"comment_ctime":1567154739,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"老师，我知道kafka 是攒一波消息后做为一批来处理的。那消费端如果消费一条消息后处理失败要重新消费，这时候要重新commit offset","like_count":0},{"had_liked":false,"id":129339,"user_name":"渔村蓝","can_delete":false,"product_type":"c1","uid":1307497,"ip_address":"","ucode":"A29875CE15FDA3","user_header":"https://static001.geekbang.org/account/avatar/00/13/f3/69/7039d03f.jpg","comment_is_top":false,"comment_ctime":1567090857,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"说了充分利用PageCache，但是并没有说怎么利用啊。","like_count":0},{"had_liked":false,"id":128964,"user_name":"每天晒白牙","can_delete":false,"product_type":"c1","uid":1004698,"ip_address":"","ucode":"A1B102CD933DEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","comment_is_top":false,"comment_ctime":1566996773,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"我整理的KafkaProducer的源码分析，后续会写关于老师今天介绍的点的分析\nhttps:&#47;&#47;mp.weixin.qq.com&#47;s&#47;-s34_y16HU6HR5HDsSD4bg","like_count":0},{"had_liked":false,"id":128481,"user_name":"许童童","can_delete":false,"product_type":"c1","uid":1003005,"ip_address":"","ucode":"4B799C0C6BC678","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4d/fd/0aa0e39f.jpg","comment_is_top":false,"comment_ctime":1566886916,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"我也来写一下自己的总结：\nKafka高性能的原因：\n1.批处理\n2.顺序读写\n3.PageCache充分利用操作系统缓存\n4.DMA零拷贝提升性能","like_count":0},{"had_liked":false,"id":131581,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567817497,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"Pagecache的作用和mysql里的innodb buffer作用基本一样的，都是读取时从buffer里取数据，若果数据不存在，则发生缺页，再从磁盘读取，放入buffer，都有加速客户端读取的效果，包括写操作。","like_count":0},{"had_liked":false,"id":131578,"user_name":"cfanbo","can_delete":false,"product_type":"c1","uid":1043738,"ip_address":"","ucode":"39D8D71453E575","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ed/1a/269eb3d6.jpg","comment_is_top":false,"comment_ctime":1567817234,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"建议 零复制 这款增减一个图吧，看图在理解的话就方便的多了，谢谢！","like_count":0},{"had_liked":false,"id":129563,"user_name":"K-Li","can_delete":false,"product_type":"c1","uid":1361330,"ip_address":"","ucode":"3091322142E43E","user_header":"https://static001.geekbang.org/account/avatar/00/14/c5/b2/5b339c64.jpg","comment_is_top":false,"comment_ctime":1567154739,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"老师，我知道kafka 是攒一波消息后做为一批来处理的。那消费端如果消费一条消息后处理失败要重新消费，这时候要重新commit offset","like_count":0},{"had_liked":false,"id":129339,"user_name":"渔村蓝","can_delete":false,"product_type":"c1","uid":1307497,"ip_address":"","ucode":"A29875CE15FDA3","user_header":"https://static001.geekbang.org/account/avatar/00/13/f3/69/7039d03f.jpg","comment_is_top":false,"comment_ctime":1567090857,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"说了充分利用PageCache，但是并没有说怎么利用啊。","like_count":0},{"had_liked":false,"id":128964,"user_name":"每天晒白牙","can_delete":false,"product_type":"c1","uid":1004698,"ip_address":"","ucode":"A1B102CD933DEA","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg","comment_is_top":false,"comment_ctime":1566996773,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"我整理的KafkaProducer的源码分析，后续会写关于老师今天介绍的点的分析\nhttps:&#47;&#47;mp.weixin.qq.com&#47;s&#47;-s34_y16HU6HR5HDsSD4bg","like_count":0},{"had_liked":false,"id":128481,"user_name":"许童童","can_delete":false,"product_type":"c1","uid":1003005,"ip_address":"","ucode":"4B799C0C6BC678","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4d/fd/0aa0e39f.jpg","comment_is_top":false,"comment_ctime":1566886916,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":8,"product_id":100032301,"comment_content":"我也来写一下自己的总结：\nKafka高性能的原因：\n1.批处理\n2.顺序读写\n3.PageCache充分利用操作系统缓存\n4.DMA零拷贝提升性能","like_count":0}]}