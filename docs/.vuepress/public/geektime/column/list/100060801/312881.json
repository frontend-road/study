{"id":312881,"title":"20 | DeepFM：如何让你的模型更好地处理特征交叉？","content":"<p>你好，我是王喆。</p><p>前面几节课，我们学习了Embedding MLP、Wide&amp;Deep、NerualCF等几种不同的模型结构。你有没有深入思考过这样一个问题：这几种模型都是怎么处理特征交叉这个问题的？</p><p>比如说，模型的输入有性别、年龄、电影风格这几个特征，在训练样本中我们发现有25岁男生喜欢科幻电影的样本，有35岁女生喜欢看恐怖电影的样本，那你觉得模型应该怎么推测“25岁”的女生喜欢看的电影风格呢？</p><p>事实上，这类特征组合和特征交叉问题非常常见，而且在实际应用中，特征的种类还要多得多，特征交叉的复杂程度也要大得多。<strong>解决这类问题的关键，就是模型对于特征组合和特征交叉的学习能力，因为它决定了模型对于未知特征组合样本的预测能力，而这对于复杂的推荐问题来说，是决定其推荐效果的关键点之一。</strong></p><p>但无论是Embedding MLP，还是Wide&amp;Deep其实都没有对特征交叉进行特别的处理，而是直接把独立的特征扔进神经网络，让它们在网络里面进行自由组合，就算是NeuralCF也只在最后才把物品侧和用户侧的特征交叉起来。那这样的特征交叉方法是高效的吗？深度学习模型有没有更好的处理特征交叉的方法呢？</p><p>这节课，我们就一起来解决这些问题。同时，我还会基于特征交叉的思想，带你学习和实现一种新的深度学习模型DeepFM。</p><!-- [[[read_end]]] --><h2>为什么深度学习模型需要加强处理特征交叉的能力？</h2><p>不过，在正式开始今天的课程之前，我还想和你再深入聊聊，为什么深度学习需要加强处理特征交叉的能力。我们刚才说Embedding MLP和Wide&amp;Deep模型都没有针对性的处理特征交叉问题，有的同学可能就会有疑问了，我们之前不是一直说，多层神经网络有很强的拟合能力，能够在网络内部任意地组合特征吗？这两个说法是不是矛盾了？</p><p>在进入正题前，我就带你先扫清这个疑问。我们之前一直说MLP有拟合任意函数的能力，这没有错，但这是建立在MLP有任意多层网络，以及任意多个神经元的前提下的。</p><p>在训练资源有限，调参时间有限的现实情况下，MLP对于特征交叉的处理其实还比较低效。因为MLP是通过concatenate层把所有特征连接在一起成为一个特征向量的，这里面没有特征交叉，两两特征之间没有发生任何关系。</p><p>这个时候，在我们有先验知识的情况下，人为地加入一些负责特征交叉的模型结构，其实对提升模型效果会非常有帮助。比如，在我们Sparrow RecSys项目的训练样本中其实有两个这样的特征，一个是用户喜欢的电影风格，一个是电影本身的风格，这两个特征明显具有很强的相关性。如果我们能让模型利用起这样的相关性，肯定会对最后的推荐效果有正向的影响。</p><p>既然这样，那我们不如去设计一些特定的特征交叉结构，来把这些相关性强的特征，交叉组合在一起，这就是深度学习模型要加强特征交叉能力的原因了。</p><h2>善于处理特征交叉的机器学习模型FM</h2><p>扫清了这个疑问，接下来，我们就要进入具体的深度学习模型的学习了，不过，先别着急，我想先和你聊聊传统的机器学习模型是怎么解决特征交叉问题的，看看深度学习模型能不能从中汲取到“养分”。</p><p>说到解决特征交叉问题的传统机器学习模型，我们就不得不提一下，曾经红极一时的机器学习模型因子分解机模型（Factorization Machine）了，我们可以简称它为FM。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/a2/6b2868995e486943ea90cfc51c2bc0a2.jpg?wh=1268*754\" alt=\"\" title=\"图1 FM的神经网络化结构 [br]（出自论文 DeepFM: A Factorization-Machine based Neural Network for CTR Prediction）\"></p><p>首先，我们看上图中模型的最下面，它的输入是由类别型特征转换成的One-hot向量，往上就是深度学习的常规操作，也就是把One-hot特征通过Embedding层转换成稠密Embedding向量。到这里，FM跟其他深度学习模型其实并没有区别，但再往上区别就明显了。</p><p>FM会使用一个独特的层FM Layer来专门处理特征之间的交叉问题。你可以看到，FM层中有多个内积操作单元对不同特征向量进行两两组合，这些操作单元会把不同特征的内积操作的结果输入最后的输出神经元，以此来完成最后的预测。</p><p>这样一来，如果我们有两个特征是用户喜爱的风格和电影本身的风格，通过FM层的两两特征的内积操作，这两个特征就可以完成充分的组合，不至于像Embedding MLP模型一样，还要MLP内部像黑盒子一样进行低效的交叉。</p><h2>深度学习模型和FM模型的结合DeepFM</h2><p>这个时候问题又来了，FM是一个善于进行特征交叉的模型，但是我们之前也讲过，深度学习模型的拟合能力强啊，那二者之间能结合吗？</p><p>学习过Wide&amp;Deep结构之后，我们一定可以快速给出答案，我们当然可以把FM跟其他深度学习模型组合起来，生成一个全新的既有强特征组合能力，又有强拟合能力的模型。基于这样的思想，DeepFM模型就诞生了。</p><p>DeepFM是由哈工大和华为公司联合提出的深度学习模型，我把它的架构示意图放在了下面。</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/19/d0df6ed3958byyd9529efceebba64419.png?wh=1642*674\" alt=\"\" title=\"图2 DeepFM模型架构图 [br]（出自论文 DeepFM: A Factorization-Machine based Neural Network for CTR Prediction）\"></p><p>结合模型结构图，我们可以看到，DeepFM利用了Wide&amp;Deep组合模型的思想，用FM替换了Wide&amp;Deep左边的Wide部分，加强了浅层网络部分特征组合的能力，而右边的部分跟Wide&amp;Deep的Deep部分一样，主要利用多层神经网络进行所有特征的深层处理，最后的输出层是把FM部分的输出和Deep部分的输出综合起来，产生最后的预估结果。这就是DeepFM的结构。</p><h2>特征交叉新方法：元素积操作</h2><p>接下来我们再思考一个问题，FM和DeepFM中进行特征交叉的方式，都是进行Embedding向量的点积操作，那是不是说特征交叉就只能用点积操作了？</p><p>答案当然是否定的。事实上还有很多向量间的运算方式可以进行特征的交叉，比如模型NFM（Neural Factorization Machines，神经网络因子分解机），它就使用了新的特征交叉方法。下面，我们一起来看一下。</p><p>图3就是NFM的模型架构图，相信已经看了这么多模型架构图的你，一眼就能看出它跟其他模型的区别，也就是Bi-Interaction Pooling层。那这个夹在Embedding层和MLP之间的层到底做了什么呢？</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/0c/a2c0f6751f64f50e3c628bf86cd9b00c.jpg?wh=1328*866\" alt=\"\" title=\"图3 NFM的模型架构图 [br]（出自论文Neural Factorization Machines for Sparse Predictive Analytics）\"></p><p>Bi-Interaction Pooling Layer翻译成中文就是“两两特征交叉池化层”。假设Vx是所有特征域的Embedding集合，那么特征交叉池化层的具体操作如下所示。</p><p>$$<br>\nf_{\\mathrm{PI}}\\left(V_{x}\\right)=\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i} \\boldsymbol{v}_{i} \\odot \\boldsymbol{x}_{j} \\boldsymbol{v}_{j}<br>\n$$</p><p>其中$\\odot$运算代表两个向量的元素积（Element-wise Product）操作，即两个长度相同的向量对应维相乘得到元素积向量。其中，第k维的操作如下所示。</p><p>$$<br>\n\\left(V_{i} \\odot V_{j}\\right)_{K}=v_{i k} v_{j k}<br>\n$$</p><p>在进行两两特征Embedding向量的元素积操作后，再求取所有交叉特征向量之和，我们就得到了池化层的输出向量。接着，我们再把该向量输入上层的多层全连接神经网络，就能得出最后的预测得分。</p><p>总的来说，NFM并没有使用内积操作来进行特征Embedding向量的交叉，而是使用元素积的操作。在得到交叉特征向量之后，也没有使用concatenate操作把它们连接起来，而是采用了求和的池化操作，把它们叠加起来。</p><p>看到这儿，你肯定又想问，元素积操作和点积操作到底哪个更好呢？还是那句老话，我希望我们能够尽量多地储备深度学习模型的相关知识，先不去关注哪个方法的效果会更好，至于真实的效果怎么样，交给你去在具体的业务场景的实践中验证。</p><h2>DeepFM的TensorFlow实战</h2><p>接下来，又到了TensorFlow实践的时间了，今天我们将要实现DeepFM模型。有了之前实现Wide&amp;Deep模型的经验，我想你实现起DeepFM也不会困难。跟前几节课一样，实践过程中的特征处理、模型训练评估的部分都是相同的，我也就不再重复了，我们重点看模型定义的部分。我把这部分的代码也放在了下面，你可以结合它来看我的讲解。</p><pre><code>item_emb_layer = tf.keras.layers.DenseFeatures([movie_emb_col])(inputs)\nuser_emb_layer = tf.keras.layers.DenseFeatures([user_emb_col])(inputs)\nitem_genre_emb_layer = tf.keras.layers.DenseFeatures([item_genre_emb_col])(inputs)\nuser_genre_emb_layer = tf.keras.layers.DenseFeatures([user_genre_emb_col])(inputs)\n\n\n# FM part, cross different categorical feature embeddings\nproduct_layer_item_user = tf.keras.layers.Dot(axes=1)([item_emb_layer, user_emb_layer])\nproduct_layer_item_genre_user_genre = tf.keras.layers.Dot(axes=1)([item_genre_emb_layer, user_genre_emb_layer])\nproduct_layer_item_genre_user = tf.keras.layers.Dot(axes=1)([item_genre_emb_layer, user_emb_layer])\nproduct_layer_user_genre_item = tf.keras.layers.Dot(axes=1)([item_emb_layer, user_genre_emb_layer])\n\n\n# deep part, MLP to generalize all input features\ndeep = tf.keras.layers.DenseFeatures(deep_feature_columns)(inputs)\ndeep = tf.keras.layers.Dense(64, activation='relu')(deep)\ndeep = tf.keras.layers.Dense(64, activation='relu')(deep)\n\n\n# concatenate fm part and deep part\nconcat_layer = tf.keras.layers.concatenate([product_layer_item_user, product_layer_item_genre_user_genre,\n                                            product_layer_item_genre_user, product_layer_user_genre_item, deep], axis=1)\noutput_layer = tf.keras.layers.Dense(1, activation='sigmoid')(concat_layer)\n\n\nmodel = tf.keras.Model(inputs, output_lay)\n\n</code></pre><p>在整个实践的过程中，有两个地方需要我们重点注意，一个是FM部分的构建，另一个是FM部分的输出和Deep输出的连接。</p><p>在构建FM部分的时候，我们先为FM部分选择了4个用于交叉的类别型特征，分别是用户ID、电影ID、用户喜欢的风格和电影自己的风格。接着，我们使用Dot layer把用户特征和电影特征两两交叉，这就完成了FM部分的构建。</p><p>而Deep部分的实现，其实和我们之前实现过的Wide&amp;Deep模型的Deep部分完全一样。只不过，最终我们会使用concatenate层，去把FM部分的输出和Deep部分的输出连接起来，输入到输出层的sigmoid神经元，从而产生最终的预估分数。那关于DeepFM的全部代码，你可以参照SparrowRecsys项目中的DeepFM.py文件。</p><h2>小结</h2><p>DeepFM模型在解决特征交叉问题上非常有优势，它会使用一个独特的FM层来专门处理特征之间的交叉问题。具体来说，就是使用点积、元素积等操作让不同特征之间进行两两组合，再把组合后的结果输入的输出神经元中，这会大大加强模型特征组合的能力。因此，DeepFM模型相比于Embedding MLP、Wide&amp;Deep等模型，往往具有更好的推荐效果。</p><p>实现DeepFM模型的过程并不困难，我们主要记住三点就可以了：</p><ul>\n<li>它是由FM和Deep两部分组成的；</li>\n<li>在实现FM部分特征交叉层的时候，我们使用了多个Dot Product操作单元完成不同特征的两两交叉；</li>\n<li>Deep部分则与Wide&amp;Deep模型一样，负责所有输入特征的深度拟合，提高模型整体的表达能力</li>\n</ul><p>刚才说的重点知识，我都整理在了下面的表格中，你可以看一看。</p><p><img src=\"https://static001.geekbang.org/resource/image/4d/76/4dbb2c9760199311b38b32a15daba176.jpeg?wh=1920*1080\" alt=\"\"></p><p>好了，到今天这节课，我们已经在SparrowRecsys中实现了四个深度学习模型，相信你对TensorFlow的Keras接口也已经十分熟悉了。我希望你不只满足于读懂、用好SparrowRecsys中实现好的模型，而是真的在课后自己多去尝试不同的特征输入，不同的模型结构，甚至可以按照自己的理解和思考去改进这些模型。</p><p>因为深度学习模型结构没有标准答案，我们只有清楚不同模型之间的优缺点，重点汲取它们的设计思想，才能在实际的工作中结合自己遇到的问题，来优化和改造已有的模型。也只有这样，你们才能成为一名能真正解决实际问题的算法工程师。</p><h2>课后思考</h2><p>你觉得除了点积和元素积这两个操作外，还有没有其他的方法能处理两个Embedding向量间的特征交叉？</p><p>关于深度学习中特征交叉问题的处理方法，你是不是学会了？欢迎把你的思考和疑问写在留言区，如果你的朋友也对DeepFM这个模型感兴趣，那不妨也把这节课转发给他，我们下节课见！</p>","neighbors":{"left":{"article_title":"19｜NeuralCF：如何用深度学习改造协同过滤？","id":310785},"right":{"article_title":"21｜注意力机制、兴趣演化：推荐系统如何抓住用户的心？","id":313736}},"comments":[{"had_liked":false,"id":264195,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1606376919,"is_pvip":false,"replies":[{"id":"95833","content":"这是个好问题。按照DeepFM原论文，数值型特征是不参与特征交叉的，因为特征交叉的操作是在两个embedding向量间进行的。<br><br>但是如果可以把通过分桶操作把连续型特征处理成离散型特征，然后再加Embedding层，就可以让数值型特征也参与特征交叉。这是一个可行的方案。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606415602,"ip_address":"","comment_id":264195,"utype":1}],"discussion_count":1,"race_medal":0,"score":"237829578199","product_id":100060801,"comment_content":"关于DeepFM，想请教一下老师：DeepFM的图示中，输入均是类别型特征的one-hot或embedding，请问是因为特征交叉仅适用于类别型特征的交叉吗？数值型特征之间，数值型与类别型特征之间能否进行交叉呢？另外，在DeepFM的wide部分中一阶交叉项是否可以包含未参与特征交叉的数值型特征呢？","like_count":56,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510455,"discussion_content":"这是个好问题。按照DeepFM原论文，数值型特征是不参与特征交叉的，因为特征交叉的操作是在两个embedding向量间进行的。\n\n但是如果可以把通过分桶操作把连续型特征处理成离散型特征，然后再加Embedding层，就可以让数值型特征也参与特征交叉。这是一个可行的方案。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1606415602,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":264172,"user_name":"张弛 Conor","can_delete":false,"product_type":"c1","uid":2208459,"ip_address":"","ucode":"193EBA4A64BAB3","user_header":"https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg","comment_is_top":false,"comment_ctime":1606372560,"is_pvip":false,"replies":[{"id":"95835","content":"是这样，原FM中内积作为权重，然后还要乘以特征本身的值。<br>但在DeepFM中，所有的参与交叉的特征都先转换成了embedding，而且由于是one-hot，所以特征的值就是1，参不参与交叉都无所谓。所以直接使用embedding的内积作为交叉后的值就可以了。<br><br>至于数值型特征的问题在于，如何把他们转换成embedding向量，我觉得分桶后加embedding层是一个方法，但其实分桶后加embedding层也是不用加原特征值的，因为分桶后的结果还是一个one-hot向量。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606415986,"ip_address":"","comment_id":264172,"utype":1}],"discussion_count":4,"race_medal":0,"score":"70325849296","product_id":100060801,"comment_content":"老师您好，请教一个关于FM的问题。原FM中二阶交叉项中隐向量的内积仅作为权重，但从这篇课程的图示和代码来看，他们的内积直接作为了交叉项的结果，而没有了初始特征的交叉，想请问一下，这样做是因为教程里所选的特征是one-hot格式，所以维度可能不一致，从而无法进行初始特征的交叉吗？那对于数值型的特征，他们的初始特征交叉是否应该和隐向量内积相乘再作为二阶交叉项的输出呢？","like_count":17,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510450,"discussion_content":"是这样，原FM中内积作为权重，然后还要乘以特征本身的值。\n但在DeepFM中，所有的参与交叉的特征都先转换成了embedding，而且由于是one-hot，所以特征的值就是1，参不参与交叉都无所谓。所以直接使用embedding的内积作为交叉后的值就可以了。\n\n至于数值型特征的问题在于，如何把他们转换成embedding向量，我觉得分桶后加embedding层是一个方法，但其实分桶后加embedding层也是不用加原特征值的，因为分桶后的结果还是一个one-hot向量。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606415986,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2421713,"avatar":"https://static001.geekbang.org/account/avatar/00/24/f3/d1/0663e55c.jpg","nickname":"FayeChen","note":"","ucode":"72AA0C00128017","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":356935,"discussion_content":"因为要变成同一维度才能做内积，categorical feature embedding 到 embedding_dim 维， 需要数值型也映射到embedding_dim 维。数值型映射的方式可以是分箱也可以是乘以一个 embedding_dim 的向量。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1615710382,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2106574,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKHFicKDOJk2zNE09HNL5ykibFV7a9I4r8435Y7P1FJbxzTwTGDDRCfBqYrmQKuHrgLJAV3onrOReTw/132","nickname":"Geek_04634b","note":"","ucode":"A6CB8AF3E0EC41","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":351468,"discussion_content":"你没理解什么是embedding，至于你说的数值型交叉，我也一直有这个疑问，但是老师并没有回答为什么数值特征无法直接做交叉","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614301216,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2454131,"avatar":"https://static001.geekbang.org/account/avatar/00/25/72/73/d707c8be.jpg","nickname":"MutouMan","note":"","ucode":"E2E78C6EE25E80","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2106574,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKHFicKDOJk2zNE09HNL5ykibFV7a9I4r8435Y7P1FJbxzTwTGDDRCfBqYrmQKuHrgLJAV3onrOReTw/132","nickname":"Geek_04634b","note":"","ucode":"A6CB8AF3E0EC41","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":379806,"discussion_content":"数值特征和embedding直接交叉，只是对embedding线性变换，意义不大","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1624166537,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":351468,"ip_address":""},"score":379806,"extra":""}]}]},{"had_liked":false,"id":263856,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1606281419,"is_pvip":false,"replies":[{"id":"95708","content":"这两个想法都没有问题，其实深度学习中没有什么不可以的，有的只是提出思路，改进模型，和验证效果。<br><br>把点积和元素积在一起使用，交给模型自动学习权重当然也是可行的。<br><br>除此之外，还有元素减，外积等交叉操作，除此之外还有一些自定义的复杂交叉操作，比如google cross&amp;deep模型中自定义的一些cross操作。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606287269,"ip_address":"","comment_id":263856,"utype":1}],"discussion_count":1,"race_medal":0,"score":"48850921675","product_id":100060801,"comment_content":"关于课后思考题，处理两个 Embedding 向量间的特征交叉的方法。<br>1.是否可以把这两个embedding向量组合之后再做一次embedding。<br>2.对于两个Embedding向量做一次pooling层，采用average&#47;max pooling。<br><br>另外，开个脑洞，不是有木有把点积和元素积一起使用的模型呢？","like_count":12,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510334,"discussion_content":"这两个想法都没有问题，其实深度学习中没有什么不可以的，有的只是提出思路，改进模型，和验证效果。\n\n把点积和元素积在一起使用，交给模型自动学习权重当然也是可行的。\n\n除此之外，还有元素减，外积等交叉操作，除此之外还有一些自定义的复杂交叉操作，比如google cross&amp;amp;deep模型中自定义的一些cross操作。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606287269,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263807,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1606270971,"is_pvip":false,"replies":[{"id":"95696","content":"加操作是不进行特征交叉，直接把原先的特征接入输出层，相当于wide&amp;deep模型中的wide层。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606279793,"ip_address":"","comment_id":263807,"utype":1}],"discussion_count":3,"race_medal":0,"score":"48850911227","product_id":100060801,"comment_content":"请问老师，文中提到FM 和 DeepFM 中进行特征交叉的方式，都是进行 Embedding 向量的点积操作。FM层中还有个加操作，它的作用是什么？","like_count":12,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510314,"discussion_content":"加操作是不进行特征交叉，直接把原先的特征接入输出层，相当于wide&amp;amp;deep模型中的wide层。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606279793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1255585,"avatar":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","nickname":"fsc2016","note":"","ucode":"5480F05703A974","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":330877,"discussion_content":"加操作应该是FM模型中的一阶特征部分。看网上有人说，加不加一阶特征影响不大，因为DNN隐层部分，也包括了对原始特征的一阶处理","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1606725779,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1255585,"avatar":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","nickname":"fsc2016","note":"","ucode":"5480F05703A974","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331173,"discussion_content":"自己试一下就知道，没必要看别人怎么说。","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1606799711,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":330877,"ip_address":""},"score":331173,"extra":""}]}]},{"had_liked":false,"id":280972,"user_name":"厚积薄发","can_delete":false,"product_type":"c1","uid":1206674,"ip_address":"","ucode":"8640C07176C249","user_header":"https://static001.geekbang.org/account/avatar/00/12/69/92/69c2c135.jpg","comment_is_top":false,"comment_ctime":1614507634,"is_pvip":true,"replies":[{"id":"102029","content":"元素积不求和，对位操作后生成一个向量。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1614563866,"ip_address":"","comment_id":280972,"utype":1}],"discussion_count":1,"race_medal":0,"score":"40269213298","product_id":100060801,"comment_content":"内积和元素积的区别是什么？都是对应元素相乘然后求和吧","like_count":10,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516262,"discussion_content":"元素积不求和，对位操作后生成一个向量。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614563866,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265117,"user_name":"xll","can_delete":false,"product_type":"c1","uid":1510173,"ip_address":"","ucode":"F0CCB9AE0F3418","user_header":"https://static001.geekbang.org/account/avatar/00/17/0b/1d/525b5b36.jpg","comment_is_top":false,"comment_ctime":1606786111,"is_pvip":false,"replies":[{"id":"96278","content":"几乎不可以。如果一定要做的话，也要在不同embedding层上再加上一层fc layer或者embedding layer，把他们变成一致的，然后交叉。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606798989,"ip_address":"","comment_id":265117,"utype":1}],"discussion_count":1,"race_medal":0,"score":"40261491775","product_id":100060801,"comment_content":"老师您好，按FM的交叉方式，不同特征的embedding 向量维度要相同，但实际不同离散特征的维度可能相差很大，如果想用不同的embedding 维度，那应该怎样做交叉，业界有没有这样的处理方式？","like_count":10,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510809,"discussion_content":"几乎不可以。如果一定要做的话，也要在不同embedding层上再加上一层fc layer或者embedding layer，把他们变成一致的，然后交叉。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606798989,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263872,"user_name":"Sebastian","can_delete":false,"product_type":"c1","uid":1797634,"ip_address":"","ucode":"62E6FC13DB00E3","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/55lYKUdcPFgUHibRYmaRiaBdrsmnLGOHdPp4OicjBh197X0vyGa9qAwruEqicAPuUgibXO4Lz5jLudlcbtsqq2p3CpA/132","comment_is_top":false,"comment_ctime":1606286105,"is_pvip":false,"replies":[{"id":"95730","content":"这些确实都可以，但针对性不强，还是一些专门针对两个embedding特征交叉设计的操作效果好一些。比如我们提到的dot product, element-wise product 和element-wise minus. outer product等等。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606334554,"ip_address":"","comment_id":263872,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35966024473","product_id":100060801,"comment_content":"对特征embedding做concat、average pooling、sum pooling 都可以","like_count":9,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510344,"discussion_content":"这些确实都可以，但针对性不强，还是一些专门针对两个embedding特征交叉设计的操作效果好一些。比如我们提到的dot product, element-wise product 和element-wise minus. outer product等等。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606334554,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263745,"user_name":"浣熊当家","can_delete":false,"product_type":"c1","uid":1952248,"ip_address":"","ucode":"939F06050423E4","user_header":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","comment_is_top":false,"comment_ctime":1606259910,"is_pvip":false,"replies":[{"id":"95699","content":"一般是通过人工经验进行一些预处理。当然，预处理层也可以看作深度学习模型的一部分，需要一些人工的尝试。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606280048,"ip_address":"","comment_id":263745,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23081096390","product_id":100060801,"comment_content":"这一讲里是关于不同特征之间的交叉，但对于之前提到过Youtube有对单一特征进行平方，开方这样的操作得出新的特征的做法，对于这种单一特征的变换操作有没有什么深度模型可以做到？还是一般都是根据经验和理性进行人工手动的尝试？","like_count":6,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510285,"discussion_content":"一般是通过人工经验进行一些预处理。当然，预处理层也可以看作深度学习模型的一部分，需要一些人工的尝试。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606280048,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":298245,"user_name":"王志文","can_delete":false,"product_type":"c1","uid":2237986,"ip_address":"","ucode":"92F651A8DB272F","user_header":"https://static001.geekbang.org/account/avatar/00/22/26/22/2da3db6c.jpg","comment_is_top":false,"comment_ctime":1623979770,"is_pvip":false,"replies":[{"id":"108420","content":"代码中只是一个例子，仅供参考，具体哪种效果好当然是靠自己探索。<br><br>你的思路很对，一般来说物品侧和用户侧的特征交叉的作用更大，在实际应用中，还是最好有一些手动调参，和先验知识会更好一些。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1624387893,"ip_address":"","comment_id":298245,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18803848954","product_id":100060801,"comment_content":"老师好，代码中FM的交叉好像是手工指定了几种交叉，还不是全部特征的两两交叉？进一步，如果是全部特征的两两交叉，会出现user id和 user gener的交叉，这两项都是用户侧特征，不涉及物品侧特征，感觉交叉了也不会对指标提升有作用，这样考虑对吗？如果全部两两交叉不会有作用的话，是不是又得手动做交叉特征的筛选呢？谢谢老师！","like_count":5,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522071,"discussion_content":"代码中只是一个例子，仅供参考，具体哪种效果好当然是靠自己探索。\n\n你的思路很对，一般来说物品侧和用户侧的特征交叉的作用更大，在实际应用中，还是最好有一些手动调参，和先验知识会更好一些。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624387893,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280289,"user_name":"遨游","can_delete":false,"product_type":"c1","uid":1818087,"ip_address":"","ucode":"3D24DA4B30FF41","user_header":"https://static001.geekbang.org/account/avatar/00/1b/bd/e7/3cc191d6.jpg","comment_is_top":false,"comment_ctime":1614152118,"is_pvip":false,"replies":[{"id":"101876","content":"MLP当然会处理特征交叉，只不过效率低一些，不那么直接，这也是学界提出deepfm的原因。<br><br>这些想法应该课程中有阐述到，建议再好好体会一下。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1614305120,"ip_address":"","comment_id":280289,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14499054006","product_id":100060801,"comment_content":"老师您好，如果不使用FM模型，单纯使用多层神经网络能否做到两两特征交叉或高阶特征交叉呢？如果可以该怎么处理？谢谢！","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516048,"discussion_content":"MLP当然会处理特征交叉，只不过效率低一些，不那么直接，这也是学界提出deepfm的原因。\n\n这些想法应该课程中有阐述到，建议再好好体会一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614305120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265769,"user_name":"Geek_ddf8b1","can_delete":false,"product_type":"c1","uid":2309043,"ip_address":"","ucode":"70E1BB4C1CC6C4","user_header":"","comment_is_top":false,"comment_ctime":1607007356,"is_pvip":false,"replies":[{"id":"96582","content":"1. 不一定，也有大量公司采用单机多卡的形式训练。如果是分布式的环境，改变会比较大，需要深入研究parameter server相关的配置和实践，属于非常偏工程前沿的内容了，需要大家自己去踩坑填坑。<br>2. 这个问题不用我回答，你自己estimate一下就可以了，redis的容量有多大，你的数据量有多大。然后做一个poc去先插入少量数据看一下实际的内存使用量就可以了。<br><br>如果redis容量不够怎么办，参考之前存储模块的那讲。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1607020181,"ip_address":"","comment_id":265769,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14491909244","product_id":100060801,"comment_content":"老师 您好！我准备做推荐项目场景为类似抖音这种的短视频信息流推荐 想请教一下：1、实际生产环境中tensorflow训练这些深度学习模型一般是分布式训练吧？如果是分布式训练的话，您sparrowrec项目中的代码需要做哪些改动？或者能否整体说明一下如果这个项目代码在生产环境使用的话有哪些需要注意应该要修改的地方。2、我看您代码中特征是存在reids中。想请教一下几百万用户（dau几十万）和几十万的视频 用户和物品特征一共300个字段左右 这种规模的数据量适合把用户和视频的特征都存在redis中吗？","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511037,"discussion_content":"1. 不一定，也有大量公司采用单机多卡的形式训练。如果是分布式的环境，改变会比较大，需要深入研究parameter server相关的配置和实践，属于非常偏工程前沿的内容了，需要大家自己去踩坑填坑。\n2. 这个问题不用我回答，你自己estimate一下就可以了，redis的容量有多大，你的数据量有多大。然后做一个poc去先插入少量数据看一下实际的内存使用量就可以了。\n\n如果redis容量不够怎么办，参考之前存储模块的那讲。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607020181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263746,"user_name":"浣熊当家","can_delete":false,"product_type":"c1","uid":1952248,"ip_address":"","ucode":"939F06050423E4","user_header":"https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg","comment_is_top":false,"comment_ctime":1606261240,"is_pvip":false,"replies":[{"id":"95698","content":"结构上来说，因子分解机会引入除了user id和item id的其他特征，而且FM是有一阶部分的，不只是做特征交叉。<br><br>MF就是一个只利用user id和item Id的双塔模型。<br><br>当然像你说的，MF还有多种的求解方式，推荐找找相关资料，自己认真推理一下。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606279998,"ip_address":"","comment_id":263746,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14491163128","product_id":100060801,"comment_content":"想请问下老师， 能简单介绍下因子分解机模型（Factorization Machine）和矩阵分解（matrix decomposition）之间的联系和差别吗？（我对于矩阵的很多模型算法都很懵，比如还有奇异值分解（singular value decomposition，NMF（Non-Negative Matrix Factorization），没有搜好的图示解释他们之间的渊源， 也请教老师和大家有什么好的学习资料）","like_count":4,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510286,"discussion_content":"结构上来说，因子分解机会引入除了user id和item id的其他特征，而且FM是有一阶部分的，不只是做特征交叉。\n\nMF就是一个只利用user id和item Id的双塔模型。\n\n当然像你说的，MF还有多种的求解方式，推荐找找相关资料，自己认真推理一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606279998,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265405,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1606890767,"is_pvip":false,"replies":[{"id":"96519","content":"是的","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606940212,"ip_address":"","comment_id":265405,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10196825359","product_id":100060801,"comment_content":"1.两个embedding concat以后pooling (max average)都行<br>2.内积、外积","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510920,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606940212,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":264210,"user_name":"kenan","can_delete":false,"product_type":"c1","uid":1221542,"ip_address":"","ucode":"3EA7A5804FA002","user_header":"https://static001.geekbang.org/account/avatar/00/12/a3/a6/9105f762.jpg","comment_is_top":false,"comment_ctime":1606380638,"is_pvip":true,"replies":[{"id":"95831","content":"这篇讲的深度学习模型就会应用到SparrowRecsys的排序层。<br>你讲的“选集”指的是？","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606415438,"ip_address":"","comment_id":264210,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10196315230","product_id":100060801,"comment_content":"王老师，您好，我们的课程后续会讲召回后选集排序么？","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510463,"discussion_content":"这篇讲的深度学习模型就会应用到SparrowRecsys的排序层。\n你讲的“选集”指的是？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606415438,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1221542,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a3/a6/9105f762.jpg","nickname":"kenan","note":"","ucode":"3EA7A5804FA002","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":329626,"discussion_content":"召回之后的“候选集”一个list，或者是很多item的集合，这个list如何做排序？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606437427,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1221542,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a3/a6/9105f762.jpg","nickname":"kenan","note":"","ucode":"3EA7A5804FA002","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":331172,"discussion_content":"这个list的排序就是所谓的排序层的工作，就是利用我们这一篇学到的推荐模型进行精排序。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606799670,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":329626,"ip_address":""},"score":331172,"extra":""}]}]},{"had_liked":false,"id":264055,"user_name":"onepencil","can_delete":false,"product_type":"c1","uid":1346100,"ip_address":"","ucode":"078FCC302D5FC2","user_header":"","comment_is_top":false,"comment_ctime":1606320594,"is_pvip":false,"replies":[{"id":"95781","content":"是的，只是一个示例。但通过加入一阶项修改成标准的deepfm应该很容易就可以实现。<br><br>可以帮我修改一下，之后提个pull request吗？","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606352191,"ip_address":"","comment_id":264055,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10196255186","product_id":100060801,"comment_content":"老师你好，你的代码应该只是一个示例代码吧。FM部分只用到了四个特征，并不是原型FM的全部交叉，而且也没有一阶非交叉的特征，这应该只算是deep网络和手工交叉的组合，不能算是deepfm的实现吧","like_count":3,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510405,"discussion_content":"是的，只是一个示例。但通过加入一阶项修改成标准的deepfm应该很容易就可以实现。\n\n可以帮我修改一下，之后提个pull request吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606352191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314974,"user_name":"罗辑","can_delete":false,"product_type":"c1","uid":1539649,"ip_address":"","ucode":"06AB60316B51C4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/OwZuBRbVUkziazePs2xTKskNpZachRtCBZLHlv4dAUgaBC5qHI292xaxvg3atGnHlDwjIOXPKEbc7zOrtMyicSNg/132","comment_is_top":false,"comment_ctime":1633618511,"is_pvip":false,"replies":[{"id":"114117","content":"当然可以，本质上是一种bagging的做法","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1633719620,"ip_address":"","comment_id":314974,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5928585807","product_id":100060801,"comment_content":"依次看了这几个模型，有种想把 Wide&amp;Deep+NerualCF+DeepFM+NFM 融合进一个大模型的想法，取各自的优点，然后把各自的结果拼接在一起，送给最后那个神经元来训练权重。<br>具体可以理解为：<br>Wide&amp;Deep负责记忆部分，并抽取一些单个特征的高维特性<br>NerualCF：负责一些有明显关联关系的协同信息提取<br>DeepFM：负责交叉特征提取，甚至交叉以后的新特征再次进行二阶交叉。<br>NFM：负责所有特征的融合<br>把四个模型的输入都给一个神经元，让神经元来训练分配各自权重。<br>不知道理论上可行吗？请老师指教。<br><br>","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527845,"discussion_content":"当然可以，本质上是一种bagging的做法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633719620,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":311469,"user_name":"yangming","can_delete":false,"product_type":"c1","uid":2684333,"ip_address":"","ucode":"0437187779C429","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/XVkDvXQDPkUcqwg4RmwZwerBYmfibuLgKUyDUhwkqEQRUuhdZzyVialF3lXgk8E88ib2g4n5aRhR6NMia5Fwdf71Qg/132","comment_is_top":false,"comment_ctime":1631245985,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5926213281","product_id":100060801,"comment_content":"老师，对于first_order_cat_feature，它其实是一个indicator_column(不是dense column)，相当于onehot，onehot编码为什么能做特征交叉得到新的特征？这个比较疑惑。老师教师节快乐！！","like_count":1},{"had_liked":false,"id":308429,"user_name":"Geek_de83f6","can_delete":false,"product_type":"c1","uid":2170148,"ip_address":"","ucode":"3EFB4DC01C222D","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKn2fx2UTaWgMl3fSOSicJEDOibbtYicHUVSG8JsA8j6Njibc9j3YVSvHtMZb2Z20l4NmjibiaSv8m7hz9w/132","comment_is_top":false,"comment_ctime":1629618199,"is_pvip":false,"replies":[{"id":"111694","content":"内积也是可以求偏导的啊，可以继续反向传播","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1629682763,"ip_address":"","comment_id":308429,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5924585495","product_id":100060801,"comment_content":"老师 请教个问题，如果用了DeepFM模型后，输入到全连接层的是一个内积值，那还怎么使用反向梯度下降的方法更新FM下边一层的Embedding的值呢？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525489,"discussion_content":"内积也是可以求偏导的啊，可以继续反向传播","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629682763,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":300502,"user_name":"W","can_delete":false,"product_type":"c1","uid":2674983,"ip_address":"","ucode":"C182E9F3BAD7B9","user_header":"https://static001.geekbang.org/account/avatar/00/28/d1/27/68543b66.jpg","comment_is_top":false,"comment_ctime":1625198419,"is_pvip":false,"replies":[{"id":"109388","content":"embedding特征交叉有太多不同的交叉方法，直接做点积只是其中的最简单直观的一种。你说的理论上也可以尝试。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1626215524,"ip_address":"","comment_id":300502,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5920165715","product_id":100060801,"comment_content":"代码中直接用embedding 的点乘结果（也就是一个值？）作为特征交叉结果，但是理论上点乘结果应该是交叉特征的权重值，而交叉特征是由两个one-hot特征做组合得到的，老师这样理解对吗？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522748,"discussion_content":"embedding特征交叉有太多不同的交叉方法，直接做点积只是其中的最简单直观的一种。你说的理论上也可以尝试。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626215524,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":285502,"user_name":"Alan","can_delete":false,"product_type":"c1","uid":2115316,"ip_address":"","ucode":"591A28E310A8F5","user_header":"https://static001.geekbang.org/account/avatar/00/20/46/f4/93b1275b.jpg","comment_is_top":false,"comment_ctime":1616847951,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5911815247","product_id":100060801,"comment_content":"答：DCN与DCNMix。参考了Deep Crossing模型引入了Cross network残差结构的思想，使得模型能够更深。<br>这里有我学习的DCN与DCMix学习笔记：https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;352110578，欢迎大家来阅读！","like_count":1},{"had_liked":false,"id":274696,"user_name":"AstrHan","can_delete":false,"product_type":"c1","uid":1944884,"ip_address":"","ucode":"14C5F3323A472D","user_header":"","comment_is_top":false,"comment_ctime":1611128016,"is_pvip":false,"replies":[{"id":"99713","content":"这个问题很好，但我觉得学到这你自己应该能找到不少处理的方法，自己思考吧。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1611191796,"ip_address":"","comment_id":274696,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5906095312","product_id":100060801,"comment_content":"老师请问，multi-hot的变量咋处理？要用多个电影风格的字段的话，是转化为多个ont-hot，然后不够的补足，还是有什么别的方法？","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":514089,"discussion_content":"这个问题很好，但我觉得学到这你自己应该能找到不少处理的方法，自己思考吧。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611191796,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2421713,"avatar":"https://static001.geekbang.org/account/avatar/00/24/f3/d1/0663e55c.jpg","nickname":"FayeChen","note":"","ucode":"72AA0C00128017","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":356942,"discussion_content":"接着multi hot的问题，比如当前电影movie_id_A要和历史点击风格[&#39;科幻&#39;，&#39;动作&#39;]组合，movie_id_A 的embedding 就要和 embedding_look_up 查到的电影风格的的embedding做两两组合。\n[(movie_id_A_embedding, 科幻_embedding) , (movie_id_A_embedding，动作_embedding), (科幻_embedding, 动作_embedding) ], list中的embedding两两点乘，然后拼出来。 (科幻_embedding, 动作_embedding) 可加可不加，我个人倾向不要加\n这个是个一对多的例子，多对多无非再扩展\n这样做可以么，老师？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1615711300,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":267913,"user_name":"小小的天","can_delete":false,"product_type":"c1","uid":1284426,"ip_address":"","ucode":"A3EC02BA37B3B9","user_header":"https://static001.geekbang.org/account/avatar/00/13/99/4a/09ea6699.jpg","comment_is_top":false,"comment_ctime":1607994689,"is_pvip":false,"replies":[{"id":"97328","content":"基本是这样，各种pooling的做法，attention的做法，如果有序列关系，也可以用序列模型。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1608078020,"ip_address":"","comment_id":267913,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5902961985","product_id":100060801,"comment_content":"想问一下，针对多值带权重的特征怎么处理处理进去呢？是look up之后，在做average或者sum pooling那？","like_count":2,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":511787,"discussion_content":"基本是这样，各种pooling的做法，attention的做法，如果有序列关系，也可以用序列模型。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1608078020,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":264633,"user_name":"fsc2016","can_delete":false,"product_type":"c1","uid":1255585,"ip_address":"","ucode":"5480F05703A974","user_header":"https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg","comment_is_top":false,"comment_ctime":1606553926,"is_pvip":false,"replies":[{"id":"96253","content":"是的，这两个模型的大致区别是这样。但也不能说效果一定就好，还是老话，好不好看自己实践。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606787791,"ip_address":"","comment_id":264633,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5901521222","product_id":100060801,"comment_content":"前面讲的wide&amp;deep中wide主要是体现记忆性，对原始特征人为的把一些强特征做特征交叉；而deepFM，采用FM层自动的进行二阶特征交叉，并且采用了原始特征的embedding层。所以deepFM从效果上要比wide&amp;deep要好一些，老师，大概是这个原因吧","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510629,"discussion_content":"是的，这两个模型的大致区别是这样。但也不能说效果一定就好，还是老话，好不好看自己实践。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606787791,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":264123,"user_name":"金鹏","can_delete":false,"product_type":"c1","uid":1019485,"ip_address":"","ucode":"916180E1699264","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8e/5d/562e90d6.jpg","comment_is_top":false,"comment_ctime":1606356966,"is_pvip":false,"replies":[{"id":"95800","content":"这是个好问题，但没有标准答案，还是要自己去做评估和多种模型的尝试和比较。之后我会介绍一些经验，在模型评估篇也有相关的知识。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606376354,"ip_address":"","comment_id":264123,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5901324262","product_id":100060801,"comment_content":"老师好，请教个问题？讲了这么多模型，在实际的环境中，怎么做模型的选择，以及不同模型和策略之间叠加，以及不同策略实验的加入，从而增加推荐的精准度。","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510430,"discussion_content":"这是个好问题，但没有标准答案，还是要自己去做评估和多种模型的尝试和比较。之后我会介绍一些经验，在模型评估篇也有相关的知识。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606376354,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321871,"user_name":"一半","can_delete":false,"product_type":"c1","uid":2669283,"ip_address":"","ucode":"C44FE942E49B15","user_header":"https://static001.geekbang.org/account/avatar/00/28/ba/e3/7196d766.jpg","comment_is_top":false,"comment_ctime":1637074751,"is_pvip":false,"replies":[{"id":"117063","content":"思路没问题，完全赞同。","user_name":"作者回复","user_name_real":"编辑","uid":"1662192","ctime":1637366499,"ip_address":"","comment_id":321871,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1637074751","product_id":100060801,"comment_content":"老师，我有个想法不一定对，我印象中FM模型中应该是会加入数值类型特征的，deepfm中使用的虽然是embedding向量进行特征交叉，但是想把数值型特征加入其中其实也是有办法的<br>所以我猜测了一下deepfm不用数值型特征交叉的原因：<br>因为目标主要是学习到简单的特征交叉规则，所以其实是类别之间的相关性，数值特征相关的规则更容易在深度学习模型中被学习到，不知道对不对，希望老师能帮我梳理一下。","like_count":1,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":531624,"discussion_content":"思路没问题，完全赞同。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637366499,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":321869,"user_name":"一半","can_delete":false,"product_type":"c1","uid":2669283,"ip_address":"","ucode":"C44FE942E49B15","user_header":"https://static001.geekbang.org/account/avatar/00/28/ba/e3/7196d766.jpg","comment_is_top":false,"comment_ctime":1637074159,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637074159","product_id":100060801,"comment_content":"老师好，我有个想法不知道对不对，FM的作用是学习到特征交叉的简单联系，也就是不同种类的交叉由于使用了embedding，更适合处理稀疏矩阵。","like_count":0},{"had_liked":false,"id":320623,"user_name":"hope","can_delete":false,"product_type":"c1","uid":2668893,"ip_address":"","ucode":"BD20949B489383","user_header":"https://static001.geekbang.org/account/avatar/00/28/b9/5d/67d63c1c.jpg","comment_is_top":false,"comment_ctime":1636426535,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1636426535","product_id":100060801,"comment_content":"老师,你好,这个数据预处理部分要怎么做,数据要处理成什么样","like_count":0},{"had_liked":false,"id":303803,"user_name":"ぃ霓幻風ルァ","can_delete":false,"product_type":"c1","uid":2351074,"ip_address":"","ucode":"0A022D0652BCC9","user_header":"https://static001.geekbang.org/account/avatar/00/23/df/e2/3c6e8fff.jpg","comment_is_top":false,"comment_ctime":1627006621,"is_pvip":false,"replies":[{"id":"109964","content":"实验中的测试集太小了，真实应用不可能有这么高AUC，取能让测试集AUC稳定的epochs就可以。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1627017916,"ip_address":"","comment_id":303803,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1627006621","product_id":100060801,"comment_content":"我尝试修改epochs为50次，发现AUC已经到1.0了，肯定过拟合了，想请教下王老师一般怎么设置一个比较合理的值呢？工业界一般如何设置？谢谢","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523828,"discussion_content":"实验中的测试集太小了，真实应用不可能有这么高AUC，取能让测试集AUC稳定的epochs就可以。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1627017916,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":303317,"user_name":"抱小星","can_delete":false,"product_type":"c1","uid":1504652,"ip_address":"","ucode":"BA7B0DAFDA4AF5","user_header":"https://static001.geekbang.org/account/avatar/00/16/f5/8c/82fb5890.jpg","comment_is_top":false,"comment_ctime":1626716910,"is_pvip":true,"replies":[{"id":"109825","content":"跟经典的二分类问题一样。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1626818555,"ip_address":"","comment_id":303317,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1626716910","product_id":100060801,"comment_content":"我想请问一下老师，如果应用于CTR预估，DeepFM的损失函数是什么呢？","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":523628,"discussion_content":"跟经典的二分类问题一样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626818555,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":300504,"user_name":"W","can_delete":false,"product_type":"c1","uid":2674983,"ip_address":"","ucode":"C182E9F3BAD7B9","user_header":"https://static001.geekbang.org/account/avatar/00/28/d1/27/68543b66.jpg","comment_is_top":false,"comment_ctime":1625200287,"is_pvip":false,"replies":[{"id":"109389","content":"参考上条回复","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1626215578,"ip_address":"","comment_id":300504,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1625200287","product_id":100060801,"comment_content":"老师，我一直没理解特征交叉具体是怎么做的，网上查到说特征交叉是做笛卡尔积后hash。比如有两个离散的one-hot特征做交叉，两个特征维度分别是2，3，那么交叉获得的特征是不是[[1,0,1,0,0], [1,0,0,1,0],[1,0,0,0,1],[0,1,1,0,0], [0,1,0,1,0],[0,1,0,0,1]]，也就是新的特征是6维度的（这里如果特征交叉的维度过大的话还可以利用hash取模获得最终的新特征）","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":522749,"discussion_content":"参考上条回复","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1626215578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283721,"user_name":"Geek_c0fd60","can_delete":false,"product_type":"c1","uid":2311604,"ip_address":"","ucode":"CAB74A2ABCD57C","user_header":"","comment_is_top":false,"comment_ctime":1615896612,"is_pvip":false,"replies":[{"id":"103410","content":"v2版本都是大家提交的一些新的模型，不再我们课程范围内，大家自己去尝试就好，不要过于纠结。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1616636047,"ip_address":"","comment_id":283721,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1615896612","product_id":100060801,"comment_content":"老师，你好！我从git看到deepFM v2版本，看到里面的FM部分 变的复杂了，各个特征进行 运算，有的先求和 再乘得到 second_order_sum_square_feature。有的又是先乘再求和。最后又进行相减。这个过程感觉无厘头却 又有道理。相乘是为了 得到权重的连接吗？最后又为什么要相 减 呢？我看上面图二中，没有相减的运算。第一个版本，有您的文字说明和架构图，感觉简单的。第二个版本，能看懂代码怎么运算，但是不知道为啥要这样算。麻烦老师啦。","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517128,"discussion_content":"v2版本都是大家提交的一些新的模型，不再我们课程范围内，大家自己去尝试就好，不要过于纠结。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616636047,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2404154,"avatar":"","nickname":"Geek_9d589c","note":"","ucode":"4E7D6A4D62FCBC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362192,"discussion_content":"sumSquare 和squareSum是运算复杂度降维的手段，从原来的kn^2 变成了kn。读一下FM的paper你就知道为什么要这么做了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616883608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":283146,"user_name":"haolison","can_delete":false,"product_type":"c1","uid":1740391,"ip_address":"","ucode":"7BF883557C461A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/60UgZMiaYPUp1xRqRuRLCclg25KuKyL81pwwj9meQwF7ribZU3t7AhxVC2AxUia4iawcsb3fuiaJJx2BsrWeYGoDERA/132","comment_is_top":false,"comment_ctime":1615592449,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1615592449","product_id":100060801,"comment_content":"王老师您好，用pycharm运行DeepFM.py<br>此行代码model.fit(train_dataset, epochs=5)报错，信息如下：<br>tensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.<br>\t [[{{node dense_features&#47;movieGenre3_embedding&#47;hash_table_Lookup&#47;LookupTableFindV2}}]]<br>请问是bug呢？还是数据出了问题呢？已知文件加载正常！<br>网上找不到解决的办法......特此请教，该如何改进呢？","like_count":0,"discussions":[{"author":{"id":1023983,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9f/ef/c989706e.jpg","nickname":"小白","note":"","ucode":"F453F9A90743A7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":363324,"discussion_content":"import os\n\nos.environ[&#34;KMP_DUPLICATE_LIB_OK&#34;] = &#34;TRUE&#34;","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617168066,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2404065,"avatar":"","nickname":"Geek_209909","note":"","ucode":"A7CDEAA7F0DDC0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":358722,"discussion_content":"你的tensorflow是不是1.x版本","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616035963,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280977,"user_name":"厚积薄发","can_delete":false,"product_type":"c1","uid":1206674,"ip_address":"","ucode":"8640C07176C249","user_header":"https://static001.geekbang.org/account/avatar/00/12/69/92/69c2c135.jpg","comment_is_top":false,"comment_ctime":1614509045,"is_pvip":true,"replies":[{"id":"102030","content":"代码中没有实现NFM吧？这段是对NFM的描述，不同于DeepFM的特征交叉方式。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1614563974,"ip_address":"","comment_id":280977,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1614509045","product_id":100060801,"comment_content":"总的来说，NFM 并没有使用内积操作来进行特征 Embedding 向量的交叉，而是使用元素积的操作。在得到交叉特征向量之后，也没有使用 concatenate 操作把它们连接起来，而是采用了求和的池化操作，把它们叠加起来。  这一段明明用的是内积，然后做的连接，代码中也是，写这一段有什么意思吗？还一直强调元素积，是什么意思","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516263,"discussion_content":"代码中没有实现NFM吧？这段是对NFM的描述，不同于DeepFM的特征交叉方式。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614563974,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":280973,"user_name":"厚积薄发","can_delete":false,"product_type":"c1","uid":1206674,"ip_address":"","ucode":"8640C07176C249","user_header":"https://static001.geekbang.org/account/avatar/00/12/69/92/69c2c135.jpg","comment_is_top":false,"comment_ctime":1614507947,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1614507947","product_id":100060801,"comment_content":"NFM 并没有使用内积操作来进行特征 Embedding 向量的交叉，而是使用元素积的操作。在得到交叉特征向量之后，也没有使用 concatenate 操作把它们连接起来，而是采用了求和的池化操作，把它们叠加起来。 这个的意思是不是内积=元素积+池化","like_count":0,"discussions":[{"author":{"id":2311604,"avatar":"","nickname":"Geek_c0fd60","note":"","ucode":"CAB74A2ABCD57C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":355613,"discussion_content":"应该不是吧，内积的结果是一个数，而 元素积+池化的结果是 一个向量","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1615460858,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":265151,"user_name":"Wiiki","can_delete":false,"product_type":"c1","uid":1797573,"ip_address":"","ucode":"037F2D44C087C5","user_header":"https://static001.geekbang.org/account/avatar/00/1b/6d/c5/c0665034.jpg","comment_is_top":false,"comment_ctime":1606794703,"is_pvip":false,"replies":[{"id":"96280","content":"非常好，确实有问题，应该是j=i+1。会更正。","user_name":"作者回复","user_name_real":"王喆","uid":"1662192","ctime":1606799299,"ip_address":"","comment_id":265151,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1606794703","product_id":100060801,"comment_content":"王老师，那个Fpix(Vx)中j的求和公式是不是有点问题呀？是不是应该是j=n+1~","like_count":0,"discussions":[{"author":{"id":1662192,"avatar":"https://static001.geekbang.org/account/avatar/00/19/5c/f0/46214d29.jpg","nickname":"王喆","note":"","ucode":"2EDC616F905F3F","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510821,"discussion_content":"非常好，确实有问题，应该是j=i+1。会更正。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606799299,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}