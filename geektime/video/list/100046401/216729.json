{"id":216729,"title":"37 | 半自动特征构建方法：Entity Embedding","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/NLP\">https://gitee.com/geektime-geekbang/NLP</a></p>","comments":[{"had_liked":false,"id":201257,"user_name":"tt","can_delete":false,"product_type":"c3","uid":1489957,"ip_address":"","ucode":"7753B79AD5A9AC","user_header":"https://static001.geekbang.org/account/avatar/00/16/bc/25/1c92a90c.jpg","comment_is_top":false,"comment_ctime":1585741044,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"我的理解就是词的embedding也可以先把整个词汇表中的每个单词做EMBEDDING，每个EMBEDDING都是一个向量，然后把这些向量都放到一个矩阵里，这个矩阵不就是一个ENTITY么？就是把每个词的取值看做“词汇”这个ENTITY的一个LEVEL。然后对这个ENTITY做EMBEDDING，就是词向量的EMBEDDING。\n\n\n不知道这么理解对么？","like_count":2},{"had_liked":false,"id":285254,"user_name":"Jacob.C","can_delete":false,"product_type":"c3","uid":1070253,"ip_address":"","ucode":"034998E7A7CCD1","user_header":"https://static001.geekbang.org/account/avatar/00/10/54/ad/6ee2b7cb.jpg","comment_is_top":false,"comment_ctime":1616692209,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"查不到vincinal infomation的资料，听完觉得是用一个核函数做了一次变换，提高了变量非线性能力，我这样理解的","like_count":0,"discussions":[{"author":{"id":2727363,"avatar":"https://static001.geekbang.org/account/avatar/00/29/9d/c3/abcdfbaa.jpg","nickname":"Joseph","note":"","ucode":"9953C98D57C151","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":390542,"discussion_content":"我觉得可能是写错了，应该是vicinal information ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629882194,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":271575,"user_name":"小小的天","can_delete":false,"product_type":"c3","uid":1284426,"ip_address":"","ucode":"A3EC02BA37B3B9","user_header":"https://static001.geekbang.org/account/avatar/00/13/99/4a/09ea6699.jpg","comment_is_top":false,"comment_ctime":1609722522,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"这里的转置写法，表示的是一列吧，一行就不用转置了应该","like_count":0},{"had_liked":false,"id":230747,"user_name":"洪超","can_delete":false,"product_type":"c3","uid":1100405,"ip_address":"","ucode":"4B8FFE49D6D7AA","user_header":"https://static001.geekbang.org/account/avatar/00/10/ca/75/2931e4bd.jpg","comment_is_top":false,"comment_ctime":1593482466,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"词汇本就是一个离散变量，  不能应用文末最后讲的技巧来做连续变量的entity embedding， 连续变量做离散化处理后，分成100，500，1000这样的分类值后，为了不丢失其值本来的细节，将这个值和生成的各分类key和分类的embeding做加权求和，形成这个变量x的entity embeding， 是这种方式来处理连续型变量的entity embedding， 我的理解这种方式是不适用于词向量的，因为词本来就是一个离散的，并不是连续的\n","like_count":0}]}