{"id":799732,"title":"01｜ 原理：一个例子讲清楚Transformer原理","content":"<p>你好，我是金伟。</p><p>相信很多同学都看到过类似下面的GPT介绍。</p><blockquote>\n<p>GPT-3是强大的生成式语言模型，拥有1750亿个参数。它使用Transformer架构，通过大规模的无监督学习从海量文本数据中学习了语言的统计规律和语义表示。GPT-3可以应用于多种自然语言处理(NLP)任务，包括文本生成、文本分类、问答系统等……</p>\n</blockquote><p>你有没有想过，为什么这里面的概念不管在哪种介绍里都会被反复提及？它们是什么意思？每个概念之间有什么关系？如果我们想入局大模型，需要搞清楚这些概念吗？</p><p>我的答案是，需要。想学习大模型开发的朋友，只有通盘搞清楚这些问题，才能把概念落实到程序中。</p><p>接下来，我会从一个典型的例子出发，采用抽丝剥茧的方式，分析这个例子在Transformer架构下的具体程序流程以及数据结构。</p><p>相信通过这节课，你一定能达成三个目标。</p><ol>\n<li>跟着这个Transformer程序流程图，把所有Transformer里的概念串联起来，并理解清楚流程。</li>\n<li>理解Token，Embedding和Self-Attention这3个最核心的算法和数据结构，解释Transformer为何可以达到人类智力级别。</li>\n<li>从业务层看待Transformer程序流程图，理解上述所有大模型的相关概念。</li>\n</ol><!-- [[[read_end]]] --><p>这也正是我自己理解Transformer的方式，如果你也觉得不错，那就花20分钟跟着我的思路来试一试吧。</p><h2><strong>一个GPT会话例子</strong></h2><p>我先把上面说的典型例子抛出来。</p><p>先看例1，用明确的指令“翻译”让GPT做一个翻译。</p><p><img src=\"https://static001.geekbang.org/resource/image/f8/0a/f889420db5e04bd56866fd3158e0cb0a.png?wh=787x184\" alt=\"图片\"></p><p>接着是例2，如果接着输入，GPT就会继续翻译，不再需要额外的指令。</p><p><img src=\"https://static001.geekbang.org/resource/image/37/47/37ea3a1de5aa45394d9ca688c4620647.png?wh=746x170\" alt=\"图片\"></p><p>GPT的实现原理可以用一句话表述：<strong>通过输入一段文本，GPT</strong> <strong>模型会预测出最可能成为下一个字的字</strong>。在例1中，因为字符串是以“翻译：”开头的，所以，虽然没有指明翻译成什么语言，GPT模型也就能据此推测出“我们想翻译成英文”并给出结果。后续我们再输入中文，它也能准确地预测这是一个翻译任务。</p><p>把这个过程画成流程图，会更加清晰。</p><p><img src=\"https://static001.geekbang.org/resource/image/82/81/82c1375yy6e0731e4bca00bd670ba081.png?wh=3860x984\" alt=\"图片\"></p><p>接下来，我们的任务是进一步理解GPT（核心是Transformer）的具体程序流程。</p><p>当然，要一口气吃透Transformer的流程并不容易，因此我会采用“农村包围城市”的策略，通过一个经典概率模型的解析，先帮你扫盲重要的外围辅助概念，<strong>输入</strong>，<strong>输出</strong>，<strong>词表</strong><strong>。</strong>这些概念的理解可以在Transformer里直接继承，理解了它们，再集中精力搞清3个最核心的算法和数据结构就行了。</p><h2><strong>串联概念：</strong><strong>经典概率模型</strong></h2><p>这个经典概率模型比Transformer简单10倍，非常易于理解。</p><p>假设我们要做一个汪星人的大模型，汪星人只有睡觉、跑步、吃饭三个行为，汪星语也只有下面这一种句式。</p><pre><code class=\"language-plain\">我 刚才 跑步，我 现在 吃饭\n我 刚才 睡觉，我 现在 吃饭\n……\n</code></pre><p>我们的目标和GPT是一样的，都是用已有的文本预测下一个字，比如下面的例子。</p><pre><code class=\"language-plain\">我 刚才 跑步，我 现在 ___\n</code></pre><p>要预测上面例子的横线里应该填什么，只要计算“跑步”这个行为之后哪个行为的概率最大就可以了。比如：P(吃饭|跑步)表示汪星人在“跑步”之后，下一个行为是“吃饭”的概率。</p><p>从程序视角实现这个“大模型”，显然需要先建立一个<strong>词表</strong>（Vocabulary），存储所有的汪星词汇。看下面这张图，一共有六个词，我、刚才、现在、吃饭、睡觉、跑步。那这个词表的长度L就等于6。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/cf/3f8a4b6530cccb20yy610465dcc94acf.png?wh=3412x1664\" alt=\"图片\"></p><p>然后，根据汪星语料，计算每一个词的概率 P(wn|wi)，这也就是汪星大模型的模型训练过程。在模型运行时，可以根据输入的文本, 遍历词表获取每个词的概率，输出一个结果向量（长度也为L）。</p><pre><code class=\"language-plain\">[0, 0, 0, 0.6, 0.3, 0.1]\n</code></pre><p>比如上面的向量里4号词概率最高，是0.6，所以下一个字要输出“吃饭”。</p><p>整个模型的程序流程图如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/37/1b/37a17159580c208a5cdc4f09a68aa31b.png?wh=4004x1430\" alt=\"图片\"></p><p>甚至你可以用Python代码实现这个“大模型”。</p><p>接下来的 <strong>Transformer程序流程虽然比这个复杂，但是和外围辅助概念输入、输出、词表相比，结构和功能是一样的，只是概率计算方法不同</strong>。所以，我们完全可以在这个流程图的基础上进一步细化理解Transformer程序流程。</p><h2><strong>摸清流程：</strong><strong>Transformer架构及流程图</strong></h2><p>在真正开始细化这个流程之前，我们必须先搞清Transformer的整体架构。我引用了一张Transformer论文里的架构图，这张图对熟悉机器学习的朋友来说逻辑非常清晰。</p><p><img src=\"https://static001.geekbang.org/resource/image/97/8b/97a20f1f06ee28166d49f6e382dba58b.png?wh=3112x2080\" alt=\"图片\"></p><p>对普通工程师来说，我们可以用“分治法”把Transformer架构先用红框分为3大部分，输入、编解码、输出，更容易一步步理解它。</p><p><img src=\"https://static001.geekbang.org/resource/image/4a/bb/4accdfc8e0c8e94b4c4b105bb0e927bb.png?wh=3698x2110\" alt=\"图片\"></p><p>下面我从业务视角出发，结合例1的数据，代入到这个Transformer架构图里，你就能清楚输入、编解码、输出这3个部分的具体职能了。</p><h3><strong>业务视角的逻辑流程</strong></h3><p>下面是例1的数据。</p><p>You：</p><pre><code class=\"language-plain\">我爱你\n</code></pre><p>GPT：</p><pre><code class=\"language-plain\">i love you\n</code></pre><p>Transformer是怎么做到通过输入一段文本，GPT模型就能预测出最可能成为下一个字的字的呢？这个问题，我想下面的图已经表示得非常清楚了。</p><p><img src=\"https://static001.geekbang.org/resource/image/3c/7e/3cdc8da0ffe114ae2f642be9acc4d07e.png?wh=5872x2934\" alt=\"图片\"></p><p>第一步，当Transformer接收到“我爱你”这个输入，经过1-输入层，2-编解码层，输出下一个字符 i。关键是第二步，此时的输入变为了“我爱你”加上第一步的输出i，Transformer预测的输出是love。</p><p>总的来说，就是Transformer架构的输入模块接收用户输入并做内部数据转换，将结果输出给编解码模块，编解码模块做核心算法预测概率，输出模块根据计算得到的概率向量查词表得到下一个输出字符。</p><p>其中，输出模块和刚才的经典概率模型一致，后续我们重点细化理解 “1-输入模块” “2-编解码模块”就可以了。Transformer架构图里的每一个方框代表一个算法。对普通工程师而言，Transformer架构图不好理解的部分无非就是每个方框里的算法细节，这也是我们下面要学习的知识点。</p><h3><strong>程序视角的逻辑：矩阵计算</strong></h3><p>Transformer架构里的所有算法，其实都是<strong>矩阵</strong>和<strong>向量</strong>计算。</p><p>先看一个N x M的矩阵数据结构例子。可以理解为程序中的n行m列的数组。</p><p><img src=\"https://static001.geekbang.org/resource/image/aa/ff/aaa769e63b44f87c3b195e41d3a792ff.png?wh=1576x478\" alt=\"图片\"></p><p>其中，当N = 1时，就叫做M维向量。<br>\n<img src=\"https://static001.geekbang.org/resource/image/a0/e3/a0a0000216cf9ab1140bdbf823b704e3.png?wh=1986x174\" alt=\"\"></p><p>简单起见，我们可以把每一个方框里的算法统一描述为下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/fd/13/fd483aa7f648be8c2a3cb7c1b25eae13.png?wh=2640x1580\" alt=\"图片\"></p><p>这张图可能有点复杂，我来说明一下。</p><p>你看，每一个算法的输入都是N1 x M1的矩阵，每个算法都是在这个输入基础上和其他矩阵进行计算。假设有i个相关参数矩阵，那么最后都会输出矩阵N2 x M2，它也会成为下一个算法的输入。</p><p>这些在箭头上的Ni x Mi矩阵的每次计算都是动态的，而作为运算参数的Ci x Di矩阵都是模型提前训练好的。Ni x Mi矩阵是用户输入和算法的中间结果，Ci x Di里的具体参数其实就是模型<strong>参数</strong>。</p><p>编解码层数为Nx，表示同样的算法要做Nx次，但是要注意，每一层里的Ci x Di参数矩阵具体数值是不同的，也就是有Nx套这样的参数。</p><p>这样说的话，例1的的实现逻辑就是：“我爱你” 字符串通过Transformer已经训练好的一系列矩阵参数通过多层计算后，就能获得最大概率的下一字i。</p><p>OK，这样抽象出来之后，我们接下来就不用再关注非核心的算法流程，只要重点理解输入模块和编解码模块的过程就好了。</p><h2><strong>Transformer核心</strong>算法和结构</h2><p>我们集中注意力，依次细化<strong>最核心的三个算法和结构</strong>：<strong>Token 词表</strong>，<strong>Embedding 向量</strong>，<strong>Self-Attention 算法，</strong>并且在经典模型的程序流程图上进行细化。</p><p>先从相对更好理解的Token和Token词表开始说起。</p><h3><strong>Token和Token词表</strong></h3><p>Transformer中的Token词表和前述的词表作用相同，但Token是一个比词更细的单位。比如例2中的输入：“我叫金伟”，会被拆成4个Token：我/叫/金/伟。拆分成Token的目的是控制词表的大小，因为在自然语言中，类似“金伟”的长尾词占到了90%。</p><p>好，比照经典模型的词表，刚才的例子都就可以做成一张图表示Token词表。</p><p><img src=\"https://static001.geekbang.org/resource/image/fc/2e/fc620578c3ebfb6f9b2e5f2a5e018e2e.png?wh=3844x2422\" alt=\"图片\"></p><p>Token在Transformer里会作为基本运算单位，用户的输入“我爱你”和“i”转换成Token表示就是 [ 我, 爱，你，#i ]。注意，Transformer里的每个Token并不仅仅只有一个序号，而是会用一个Embedding向量表示一个Token的语义。</p><h3><strong>输入模块的核心：<strong><strong>Embedding</strong></strong>向量</strong></h3><p>Embedding向量具体形式如下。</p><pre><code class=\"language-plain\">#i --&gt; [0.1095,0.0336,...,0.1263,0.2155,....,0.1589,0.0282,0.1756] \n长度为M，则叫M维向量\n</code></pre><p>对应的，它的Token词表在逻辑上可以细化为下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/0e/84/0e55b01be44e493d890f76df8dc1a684.png?wh=2960x1538\" alt=\"图片\"></p><p>Transformer架构输入部分第一个流程就是 <strong>Embedding</strong>，以这个例子里的输入Token [我, 爱, 你, #i ]为例，你可以把这个过程理解为：Token挨个去词表抽取相应的Embedding，这个过程我用图片表示出来了。</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/72/e6d243e631ca29bc54942e7cfcc6a472.png?wh=4140x1764\" alt=\"图片\"></p><p>你看，假设词表总长度是L，比如“我”这个Token的Embedding就可以直接从词表里取出来，这个例子输入的总Token数量N = 4，Embedding向量的维度是M，此时抽取的矩阵是一个4 x M的矩阵。</p><p>在GPT-3里，Embedding的维度M = 12288，这个例子里N = 4，所以最终输入模块得到的矩阵就是下面这样的。</p><p><img src=\"https://static001.geekbang.org/resource/image/0f/7b/0fba441a346e5e993918f5a9e211757b.png?wh=1432x382\" alt=\"图片\"></p><p>这个矩阵会被传递给编解码模块用作起始输入。一个Embedding维度代表一个Token的语义属性，维度越高，训练成本就越高，GPT-3的经验是M = 12288维，就足够涌现出类似人类的智能。</p><p>好了，到此为止，我们已经把<strong>输入模块</strong>做了足够细化，下面是第一次细化后对应的程序流程图。</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/71/c56ff3d9ba79eyy3b437f181e484a971.png?wh=7010x3820\" alt=\"图片\"></p><h3><strong>编解码模块核心：Self-Attention算法</strong></h3><p>接下来，我们带着这个NxM的输入矩阵进入编解码模块。看下面例2中的数据。</p><p>You：</p><pre><code class=\"language-plain\">我叫金伟\n</code></pre><p>GPT：</p><pre><code class=\"language-plain\">I am called Jin Wei.\n</code></pre><p>为什么GPT知道我还想让它继续翻译这句话呢？那是因为GPT在后续预测的过程中也能注意到这次会话里最重要的一个词，“翻译”。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/34/f36e8d6f0b48d77db9eb8a25c0381a34.png?wh=3584x892\" alt=\"图片\"></p><p>大模型做预测的时候，会关心或者叫注意当前自己这个句子里的那些重要的词，这个思想正是 自注意Self-Attention这个算法的命名来源。</p><p>自注意力机制（Self-Attention）是编解码模块的第一步，也是最重要的一步，目的是计算输入的每个Token在当前句子里的重要性，以便后续算法做预测时更关注那些重要的Token。</p><p><strong>我们<strong><strong>分别从参数和算法两个角度来说明这个算法流程</strong></strong>。</strong></p><ul>\n<li><strong>参数视角</strong></li>\n</ul><p>模型需要训练并得到3个权重矩阵，分别叫Wq、Wk、Wv。</p><p><img src=\"https://static001.geekbang.org/resource/image/b6/73/b6f21e80edd2087341e1a983b9d1fa73.png?wh=4256x1388\" alt=\"图片\"></p><p>现在输入的Token列表是 [t1, t2, t3 … tn]，假设当前需要计算第i个Token的重要性，记为ti，那么Wq、Wk、Wv分别是什么意思呢？</p><ul>\n<li>Wq是为了生成查询向量，也就是ti拿来去向别人查询的向量。</li>\n<li>Wk是为了生成键向量，也就是ti用来回应它人查询的向量。</li>\n<li>Wv是为了生成值向量，也就是表示ti的重要性值的向量。</li>\n</ul><p>这样描述可能不好理解，我们直接看具体的算法流程。</p><ul>\n<li><strong>算法流程</strong></li>\n</ul><p>首先，我们写下ti的Embedding向量。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/a7/dfd53c57355f43f4b40ec777f03a64a7.png?wh=3584x698\" alt=\"图片\"></p><p>我来拆解一下整个过程。</p><p>第一步，生成每个Token对应的Q（查询向量），K（键向量），V（值向量）。针对ti的算法图就是下面这样的。</p><p><img src=\"https://static001.geekbang.org/resource/image/85/7f/85b2c4fccb0feaec07e99ab8c4a07d7f.png?wh=5192x3610\" alt=\"图片\"></p><p>第二步，Token ti拿着自己的Q向量去询问每个Token，并得到自己的重要性分数Score。</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/88/c4ab6ddf3034b10370afa44fdcefd088.png?wh=4718x1726\" alt=\"图片\"></p><p>再反过来，当其他Token向ti查询的时候，ti会用自己的K向量参与回应计算。</p><p>第三步，[score1, score2,… score-n] 这些分数再和ti的值向量V计算，就得到了模型内部表示重要性的向量Z。因为这里过于复杂，我们就略去计算过程。</p><p>自注意力机制（Self-Attention）是GPT拥有高智能在算法层面的核心因素，我们对编解码模块的注意也就到这个算法为止。不用有太多负担，理解即可。接下来可以把完整的程序流程图绘制出来了。</p><h2><strong>最终的Transformer程序流程图</strong></h2><p>现在可以进行第二次程序流程图细化，得到最终的Transformer程序流程图。你也可以在这张图的基础上回看前面的内容，进一步理解细节。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/d1/bbdd673a9be8666eee103d0a4febeed1.png?wh=7082x4610\" alt=\"图片\"></p><p>这里有几个流程要点，我再来强调一下。</p><ol>\n<li>N为输入Token总数，M为Embedding维度，L为词表总数。</li>\n<li>关键流程是这样的：词的Token化 -&gt; Embedding查询 -&gt; 组成NxM输入矩阵 -&gt; Self-Attention计算Q，K，V -&gt; Nk层计算 -&gt; 得到结果向量。</li>\n<li>涉及的几个关键参数分别是Token词表，每个Token的Embedding向量，Wq、Wk、Wv权重矩阵，以及其他算法层Ci x Di参数矩阵。</li>\n</ol><p>现在，对照着这个程序流程图来理解大模型里的概念就简单多了。我来总结一下。</p><ul>\n<li><strong>模型：</strong>就是图里的算法的具体程序实现+训练好的模型参数。</li>\n<li><strong>生成式</strong>：一个字一个字预测生成一段文字的方式。</li>\n<li><strong>参数</strong>：就是图里的各个矩阵在模型训练完成后的具体数值。</li>\n<li><strong>无监督</strong>：就是图里应该出现的下一个字正好就是语料里这句话的下一个字，不需要人工标注数据。</li>\n<li><strong>语义表示</strong>：就是图里的Token表示为M维的Embedding向量。</li>\n<li><strong>NLP</strong>：就是类似语言翻译这样的自然语言任务。</li>\n</ul><h2>小结</h2><p>未来想做大模型实战开发的工程师，必须先理解GPT的基础Transformer，就像我们做互联网应用开发时，只有先对数据库原理有一定的理解，才能真正把它用好。而要理解Transformer最好的方法，就是用一个程序流程图把Transformer架构梳理一遍。</p><p>我整理了一份流程图，你可以保存。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/d1/bbdd673a9be8666eee103d0a4febeed1.png?wh=7082x4610\" alt=\"图片\"></p><p>程序 = 算法 + 数据结构。如果把这个公式代入到大模型里，那么正如上述流程图所示，大模型 = 模型算法 + 模型参数。</p><p>这两个部分又可以进一步细化。具体怎么细化，这个问题交给你，需要你仔细回想这节课的内容，然后和留言区和我互动。</p><p>所以，到底为什么GPT可以拥有这么高的智能呢？核心原因就在于Transformer里最关键的三个设计Token，Embedding和Self-Attention。</p><p>其中，<strong>Embedding</strong> 把一个词分成了上万个维度表示，<strong>Self-Attention</strong> 则让大模型处理信息的时候考虑了当前会话的重要性，这两个看似简单的创新加上巨量的模型参数，最终让大模型的智能得到了涌现。</p><p>咱们这节课分析了GPT的基本架构Transformer。不过，这只是GPT成功的因素之一，更关键的因素是GPT在工程上的几个创新，我们放到下一节来讲解。</p><h2>思考题</h2><p>既然大语言模型中的Transformer算法是通过预测最高概率的下一个字来生成文本，那是不是意味着一个输入应该只有一种输出？但是在现实应用中，为什么相同的输入可能会有多种不同的输出呢？尝试说说其中的原因和背后的机制。</p><p>欢迎你在留言区和我交流。如果觉得有所收获，也可以把课程分享给更多的朋友一起学习。我们下节课见！</p><p><a href=\"https://jsj.top/f/hm26hN\">&gt;&gt;戳此加入课程交流群</a></p>","comments":[{"had_liked":false,"id":393186,"user_name":"郑大钱","can_delete":false,"product_type":"c1","uid":1503067,"ip_address":"四川","ucode":"114CF48056880E","user_header":"https://static001.geekbang.org/account/avatar/00/16/ef/5b/ff28088f.jpg","comment_is_top":false,"comment_ctime":1723086211,"is_pvip":false,"replies":[{"id":142849,"content":"👍，我自己也有搞懂它的需求，所以用自己能懂的方式写出来了","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723610915,"ip_address":"北京","comment_id":393186,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"第一次听懂Transformer了，不得不叹一句牛逼！","like_count":12,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649568,"discussion_content":"👍，我自己也有搞懂它的需求，所以用自己能懂的方式写出来了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723610915,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393542,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1724146169,"is_pvip":false,"replies":[{"id":142913,"content":"👍👍👍","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1724221915,"ip_address":"北京","comment_id":393542,"utype":1}],"discussion_count":1,"race_medal":2,"score":2,"product_id":100786301,"comment_content":"自己总结下LLM生成内容的流程：\n\n1. 输入层：对用户输入进行分词，转换成Token列表，列表长度为N；\n2. 输入层：针对每个Token，逐个在Token词表中查询Embedding向量表示，生成一个NxM维的矩阵，其中M为Embedding的维度；\n3. 输入层：将NxM维矩阵传递给编解码层处理；\n4. 编解码层：拿到NxM维矩，基于Transformer算法，采用Q、K、V权重矩阵计算，生成每个Token的注意力分数；\n5. 编解码层：根据注意力分数，计算生成下一个Token的概率，传递给输出层；\n6. 输出层：根据生成的Token概率，以及预设的随机性策略，到Token词表中获取下一个生成的Token；\n7. 输出层：将生成的Token，与用户的原始输入一起，作为下一轮的输入，传递给输入层，重复上述步骤。","like_count":11,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649989,"discussion_content":"👍👍👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1724221915,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393802,"user_name":"8000tank","can_delete":false,"product_type":"c1","uid":3838010,"ip_address":"北京","ucode":"C5DC98A0EB57CE","user_header":"","comment_is_top":false,"comment_ctime":1724925313,"is_pvip":false,"replies":[{"id":143002,"content":"👍👍👍","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1725176971,"ip_address":"北京","comment_id":393802,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"https:&#47;&#47;bbycroft.net&#47;llm，这里可以看到多个LLM内部原理的 可视化3D图像演示细节，强烈推荐","like_count":4,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650518,"discussion_content":"👍👍👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725176971,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1819176,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/c2/28/32fdc447.jpg","nickname":"newchaos","note":"","ucode":"EBC1909694A0F4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652961,"discussion_content":"Mark","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729939124,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393788,"user_name":"我爱学习","can_delete":false,"product_type":"c1","uid":1064988,"ip_address":"北京","ucode":"C4BA8FDED8C3CC","user_header":"https://static001.geekbang.org/account/avatar/00/10/40/1c/19fea99b.jpg","comment_is_top":false,"comment_ctime":1724900386,"is_pvip":false,"replies":[{"id":143004,"content":"这里的数字是示例，实际算法就是概率计算，某个词 wn 出现的情况下下一个是 wi 的概率","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1725177200,"ip_address":"北京","comment_id":393788,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"每一个词的概率 P(wn|wi)，[0, 0, 0, 0.6, 0.3, 0.1]，这个是怎么计算出来的？","like_count":1,"discussions":[{"author":{"id":2441247,"avatar":"https://static001.geekbang.org/account/avatar/00/25/40/1f/ffe812a8.jpg","nickname":"柠檬不酸","note":"","ucode":"AEDF0A18B9B471","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651606,"discussion_content":"P(A|B)——在B条件下 A 的概率.即事件A 在另外一个事件B已经发生条件下的发生概率","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1727166561,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"江苏","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650521,"discussion_content":"这里的数字是示例，实际算法就是概率计算，某个词 wn 出现的情况下下一个是 wi 的概率","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1725177200,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":1064988,"avatar":"https://static001.geekbang.org/account/avatar/00/10/40/1c/19fea99b.jpg","nickname":"我爱学习","note":"","ucode":"C4BA8FDED8C3CC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":650528,"discussion_content":"哦明白了，谢谢！我说按上面两条数据来算，怎么和这个对不上呢( •̅_•̅ )","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1725177510,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":650521,"ip_address":"北京","group_id":0},"score":650528,"extra":""}]}]},{"had_liked":false,"id":393708,"user_name":"zahi","can_delete":false,"product_type":"c1","uid":1447887,"ip_address":"北京","ucode":"F64ABEB63C6D1F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIofiaCAziajdQnbvrfpEkpCKVFgO62y6zicamhjF1BAWZSRcCVaTBXLIerLsGeZCic7XS7KOEkTN4fRg/132","comment_is_top":false,"comment_ctime":1724663322,"is_pvip":true,"replies":[{"id":143008,"content":"不是替代，这只是内部结果","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1725177266,"ip_address":"北京","comment_id":393708,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"Self-Attention 算法计算出来的向量Z, 是代替N x M矩阵计算吗？","like_count":1,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650525,"discussion_content":"不是替代，这只是内部结果","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725177266,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":3838010,"avatar":"","nickname":"8000tank","note":"","ucode":"C5DC98A0EB57CE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650390,"discussion_content":"同问，想了解这个向量Z的维度，以及后续是如何参与计算的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1724894561,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393430,"user_name":"luminais","can_delete":false,"product_type":"c1","uid":2536601,"ip_address":"广东","ucode":"2424805AF40673","user_header":"https://static001.geekbang.org/account/avatar/00/26/b4/99/353fe94e.jpg","comment_is_top":false,"comment_ctime":1723789720,"is_pvip":false,"replies":[{"id":142876,"content":"对 本次输入的 token 才是关键，其实就是提示词","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723798136,"ip_address":"北京","comment_id":393430,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"”第二步，Token ti 拿着自己的 Q 向量去询问每个 Token，并得到自己的重要性分数 Score”\n这里的“每个 Token”是指自己本次输入语句被拆分的所有Token吗，这里没有理解到为啥要询问自己输入的Token？","like_count":1,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649720,"discussion_content":"对 本次输入的 token 才是关键，其实就是提示词","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723798136,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393384,"user_name":"Geek_11d2d2","can_delete":false,"product_type":"c1","uid":3873586,"ip_address":"广东","ucode":"D45F7242617E62","user_header":"https://static001.geekbang.org/account/avatar/00/3b/1b/32/cbfdf1f4.jpg","comment_is_top":false,"comment_ctime":1723629323,"is_pvip":false,"replies":[{"id":142861,"content":"对 有一套概率机制保证随机性","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723699336,"ip_address":"北京","comment_id":393384,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"思考题: 首先不是总是从预测的最高概率取 token, 还有温度等参数, 还有就是, 是不是在一次会话中, 即使问了重复的问题, token 计算的注意力值就会变化? 跟现实中人的对话一样, 问了相同问题, 也会有可能得到不一样描述的回答.","like_count":1,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649650,"discussion_content":"对 有一套概率机制保证随机性","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723699337,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393346,"user_name":"Alex","can_delete":false,"product_type":"c1","uid":1352753,"ip_address":"江苏","ucode":"1770CA7050647A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI8qibAw4lRCic1pbnA6yzQU3UqtQm3NqV1bUJ5EiaUnJ24V1yf4rtY7n2Wx7ZVvTemqq5a61ERWrrHA/132","comment_is_top":false,"comment_ctime":1723561944,"is_pvip":false,"replies":[{"id":142854,"content":"👍👍👍，可以尝试去看大模型源码了","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723611071,"ip_address":"北京","comment_id":393346,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"最后的思考题 \n- 有部分是根据模型的初始值有关就像针对各个行业的不同模型 其实也就是模型的训练的语料不同\n- 就像文中说的注意力的机制也是动态的 下一次的注意点不一定就是上一次的值 也许也有上下文的问题 问了多次模型会调整回答\n- 随机的问题 就是openai的tempatrue 越高越随机性就越高 自然语言本身就是模糊的 模型的回答自然也会带入一定的随机性","like_count":1,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649573,"discussion_content":"👍👍👍，可以尝试去看大模型源码了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723611071,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393287,"user_name":"风格若干","can_delete":false,"product_type":"c1","uid":1241291,"ip_address":"北京","ucode":"F6C676B4823033","user_header":"https://static001.geekbang.org/account/avatar/00/12/f0/cb/27b88c1c.jpg","comment_is_top":false,"comment_ctime":1723373128,"is_pvip":true,"replies":[{"id":142840,"content":"应该是 4xM 这里写错了","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723498138,"ip_address":"北京","comment_id":393287,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"老师，我想问一下，文稿中这段话，抽取的为什么是一个 3 x M 的矩阵呀？\n\n假设词表总长度是 L，比如“我”这个 Token 的 Embedding 就可以直接从词表里取出来，这个例子输入的总 Token 数量 N = 4，Embedding 向量的维度是 M，此时抽取的矩阵是一个 3 x M 的矩阵。","like_count":1,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649506,"discussion_content":"应该是 4xM 这里写错了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723498138,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1447512,"avatar":"https://static001.geekbang.org/account/avatar/00/16/16/58/0ef55c6a.jpg","nickname":"Geek-Pax","note":"","ucode":"60F9DB1FC4B000","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649476,"discussion_content":"我觉得应该是打错了，是4 x M的矩阵","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723462218,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2485491,"avatar":"https://static001.geekbang.org/account/avatar/00/25/ec/f3/f140576e.jpg","nickname":"Jich","note":"","ucode":"7127C10EBB27F0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649465,"discussion_content":"我也想知道","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723445529,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393783,"user_name":"8000tank","can_delete":false,"product_type":"c1","uid":3838010,"ip_address":"北京","ucode":"C5DC98A0EB57CE","user_header":"","comment_is_top":false,"comment_ctime":1724894627,"is_pvip":false,"replies":[{"id":143011,"content":"确实可以","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1725177893,"ip_address":"北京","comment_id":393783,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100786301,"comment_content":"最后的2个流程图（同一个），下方的“模型参数”中，有个“M=Token词表”，是不是应该改为“M=Token维度”？","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650529,"discussion_content":"确实可以","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725177893,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393721,"user_name":"YOUNG","can_delete":false,"product_type":"c1","uid":1103191,"ip_address":"北京","ucode":"3EC9132DE43295","user_header":"https://static001.geekbang.org/account/avatar/00/10/d5/57/1cebae66.jpg","comment_is_top":false,"comment_ctime":1724722826,"is_pvip":false,"replies":[{"id":143007,"content":"👌","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1725177248,"ip_address":"北京","comment_id":393721,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"老师讲的真好，我基本看懂了","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650524,"discussion_content":"👌","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725177248,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393675,"user_name":"Chris_zhengbin","can_delete":false,"product_type":"c1","uid":2525940,"ip_address":"北京","ucode":"7047639EC3196F","user_header":"https://static001.geekbang.org/account/avatar/00/26/8a/f4/149802dd.jpg","comment_is_top":false,"comment_ctime":1724554735,"is_pvip":false,"replies":[{"id":143009,"content":"对适合工程师","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1725177284,"ip_address":"北京","comment_id":393675,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"看了很多关于Transformer的文章和视频，就这一篇讲的最透彻最实用，看完这篇再看其他的就简单多了。","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650526,"discussion_content":"对适合工程师","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725177284,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393455,"user_name":"Edon du","can_delete":false,"product_type":"c1","uid":1074742,"ip_address":"河南","ucode":"1648624751AAE9","user_header":"https://static001.geekbang.org/account/avatar/00/10/66/36/b4a4e6fb.jpg","comment_is_top":false,"comment_ctime":1723887206,"is_pvip":false,"replies":[{"id":142890,"content":"比如一个字：我 ，它应该有是否是名词，主语还是谓语 等维度，当然这里是简化的表述，实际模型中对这个字有 1 万多维度的意义，大模型只有这么深入的认知才能聪明","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1724052597,"ip_address":"北京","comment_id":393455,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"gpt3文本嵌入的默认向量维度不是1536吗，有点不太理解的是，我们拿到一个文本，先分段，不管某一段的文本多长，只要不超过最大token数，拿到的向量化都是一个指定维度的一维数组。\n\n文中的&quot;我爱你&quot; 为什么会拆字转换成向量矩阵呢，如果转化了，那向量矩阵和我们调用向量化得到的一维数组有什么关联呢，希望老师解惑","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649849,"discussion_content":"比如一个字：我 ，它应该有是否是名词，主语还是谓语 等维度，当然这里是简化的表述，实际模型中对这个字有 1 万多维度的意义，大模型只有这么深入的认知才能聪明","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1724052597,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":1074742,"avatar":"https://static001.geekbang.org/account/avatar/00/10/66/36/b4a4e6fb.jpg","nickname":"Edon du","note":"","ucode":"1648624751AAE9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":650037,"discussion_content":"我理解老师的意思，我们输入一段话先查token，再查每个token的向量，最终是一个向量矩阵作为模型的输入。\n\n之前的疑惑是各大厂商都有自己的向量化模型，用于将一段文本转换为向量，使用这种模型主要是为了实现RAG。 以openai为例，模型向量化后转化成了一个拥有1536个数字的一维数组，代表这段文本的1536个维度数据。这和文中的token向量应该没有关联，应该主要是为了做相似性检索。拿到文本后作为上下文二次投递给模型，然后基于最终投递的文本，执行上文中的一系列流程。\n\n不知道这样理解的是否正确。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1724289554,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":649849,"ip_address":"河南","group_id":0},"score":650037,"extra":""}]}]},{"had_liked":false,"id":393297,"user_name":"麦克范儿","can_delete":false,"product_type":"c1","uid":2808812,"ip_address":"加拿大","ucode":"702CAFBC2F744F","user_header":"https://static001.geekbang.org/account/avatar/00/2a/db/ec/d5638e84.jpg","comment_is_top":false,"comment_ctime":1723425570,"is_pvip":false,"replies":[{"id":142839,"content":"具体工程实现上会加入一定的随机策略，不是只取最高概率的一个，可以看下温度参数就是用于调节这个随机性的","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723497543,"ip_address":"北京","comment_id":393297,"utype":1}],"discussion_count":1,"race_medal":1,"score":3,"product_id":100786301,"comment_content":"想问下，当output部分得出parameters概率向量后，大模型是通过概率随机的方式来选择具体产生哪个token吗？还是输出最大概率的token呢？如果输出最大概率的话会不会很多次相同提问的回复都是一样的值？谢谢！","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649505,"discussion_content":"具体工程实现上会加入一定的随机策略，不是只取最高概率的一个，可以看下温度参数就是用于调节这个随机性的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723497543,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393205,"user_name":"西雨ζ๓㎕东晴","can_delete":false,"product_type":"c1","uid":1275383,"ip_address":"北京","ucode":"5AB9D14031C57B","user_header":"https://static001.geekbang.org/account/avatar/00/13/75/f7/0645cb4a.jpg","comment_is_top":false,"comment_ctime":1723119793,"is_pvip":false,"replies":[{"id":142812,"content":"先通过参数算概率，最后才去词表查询哦","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723164461,"ip_address":"北京","comment_id":393205,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"预测下一个最高概率的字是从token词表中来的吗？","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649323,"discussion_content":"先通过参数算概率，最后才去词表查询哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723164461,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649319,"discussion_content":"先通过参数去计算概率，再从词表查询","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723162661,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393203,"user_name":"黄武","can_delete":false,"product_type":"c1","uid":1255185,"ip_address":"湖南","ucode":"F90E56352530A7","user_header":"https://static001.geekbang.org/account/avatar/00/13/27/11/07d2455b.jpg","comment_is_top":false,"comment_ctime":1723117345,"is_pvip":false,"replies":[{"id":142807,"content":"每周一三五凌晨十二点更新～还有4小时更新！蹲住！","user_name":"编辑回复","user_name_real":"编辑","uid":2852467,"ctime":1723121690,"ip_address":"北京","comment_id":393203,"utype":2}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"这个是每天一讲吗\n如何学习下一节课程","like_count":0,"discussions":[{"author":{"id":2852467,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/86/73/5190bbde.jpg","nickname":"苏果果","note":"","ucode":"12A62ED032F345","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649302,"discussion_content":"每周一三五凌晨十二点更新～还有4小时更新！蹲住！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723121690,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":8,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393189,"user_name":"W-T","can_delete":false,"product_type":"c1","uid":1088973,"ip_address":"上海","ucode":"834869DA80D95B","user_header":"https://static001.geekbang.org/account/avatar/00/10/9d/cd/c21a01dd.jpg","comment_is_top":false,"comment_ctime":1723089060,"is_pvip":false,"replies":[{"id":142813,"content":"不是多套参数的方式，可以再思考调研一下","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723164521,"ip_address":"北京","comment_id":393189,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"思考题，是不是内部会包含多套模型权重参数，有时候会选择不同的权重参数，所以会产生不一样的结果","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649324,"discussion_content":"不是多套参数的方式，可以再思考调研一下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723164521,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649320,"discussion_content":"没有多套模型参数哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723162950,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393179,"user_name":"枫树_6177003","can_delete":false,"product_type":"c1","uid":1267891,"ip_address":"四川","ucode":"84B81DAD9C832E","user_header":"https://static001.geekbang.org/account/avatar/00/13/58/b3/abb3256b.jpg","comment_is_top":false,"comment_ctime":1723079765,"is_pvip":false,"replies":[{"id":142850,"content":"感谢支持，多多交流，希望和大家一起进步","user_name":"作者回复","user_name_real":"编辑","uid":1763517,"ctime":1723610941,"ip_address":"北京","comment_id":393179,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"老师讲的透彻，太棒了","like_count":0,"discussions":[{"author":{"id":1763517,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/e8/bd/62169942.jpg","nickname":"金伟","note":"","ucode":"C0393789836F21","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649569,"discussion_content":"感谢支持，多多交流，希望和大家一起进步","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723610941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395324,"user_name":"南瓜","can_delete":false,"product_type":"c1","uid":1079562,"ip_address":"四川","ucode":"53561F551857A4","user_header":"https://static001.geekbang.org/account/avatar/00/10/79/0a/a417ec1c.jpg","comment_is_top":false,"comment_ctime":1730214414,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100786301,"comment_content":"高手就是把复杂的东西弄简单","like_count":0}]}