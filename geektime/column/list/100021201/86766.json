{"id":86766,"title":"41 | 线性回归（下）：如何使用最小二乘法进行效果验证？","content":"<p>你好，我是黄申。</p><p>上一节我们已经解释了最小二乘法的核心思想和具体推导过程。今天我们就用实际的数据操练一下，这样你的印象就会更加深刻。我会使用几个具体的例子，演示一下如何使用最小二乘法的结论，通过观测到的自变量和因变量值，来推算系数，并使用这个系数来进行新的预测。</p><h2>基于最小二乘法的求解</h2><p>假想我们手头上有一个数据集，里面有3条数据记录。每条数据记录有2维特征，也就是2个自变量，和1个因变量。</p><p><img src=\"https://static001.geekbang.org/resource/image/94/b4/9427dc10b0745cb5680e911e0d0d15b4.png?wh=1270*300\" alt=\"\"></p><p>如果我们假设这些自变量和因变量都是线性的关系，那么我们就可以使用如下这种线性方程，来表示数据集中的样本：</p><p>$b_1·0+b_2·1=1.5$<br>\n$b_1·1-b_2·1=-0.5$<br>\n$b_1·2+b_2·8=14$</p><p>也就是说，我们通过观察数据已知了自变量$x_1$、$x_2$和因变量$y$的值，而要求解的是$b_1$和$b_2$这两个系数。如果我们能求出$b_1$和$b_2$，那么在处理新数据的时候，就能根据新的自变量$x_1$和$x_2$的取值，来预测$y$的值。</p><!-- [[[read_end]]] --><p>可是我们说过，由实际项目中的数据集所构成的这类方程组，在绝大多数情况下，都没有精确解。所以这个时候我们没法使用之前介绍的高斯消元法，而是要考虑最小二乘法。根据上一节的结论，我们知道对于系数矩阵$B$，有：</p><p>$B=(X’X)^{-1}X’Y$</p><p>既然有了这个公式，要求$B$就不难了，让我们从最基本的几个矩阵开始。</p><p><img src=\"https://static001.geekbang.org/resource/image/f2/9d/f2f70be34564a2e64071cd623d0e3f9d.png?wh=912*1028\" alt=\"\"></p><p>矩阵$(X’X)^{-1}$的求解稍微繁琐一点。逆矩阵的求法我还没讲解过，之前我们说过线性方程组之中，高斯消元和回代的过程，就是把系数矩阵变为单位矩阵的过程。我们可以利用这点，来求解$X^{-1}$。我们把原始的系数矩阵$X$列在左边，然后把单位矩阵列在右边，像$[X | I]$这种形式，其中$I$表示单位矩阵。</p><p>然后我们对左侧的矩阵进行高斯消元和回代，把左边矩阵X变为单位矩阵。同时，我们也把这个相应的矩阵操作运用在右侧。这样当左侧变为单位矩阵之后，那么右侧的矩阵就是原始矩阵$X$的逆矩阵$X^{-1}$，具体证明如下：</p><p>$[X | I]$<br>\n$[X^{-1}X | X^{-1}I]$<br>\n$[I | X^{-1}I]$<br>\n$[I | X^{-1}]$</p><p>好了，给定下面的$X’X$矩阵之后，我们使用上述方法来求$(X’X)^{-1}$ 。我把具体的推导过程列在了这里。</p><p><img src=\"https://static001.geekbang.org/resource/image/1a/ad/1a6f3815a9895fc80f54124a1dae7cad.png?wh=1344*696\" alt=\"\"></p><p>求出$(X’X)^{-1}$之后，我们就可以使用$B=(X’X)^{-1}X’Y$来计算矩阵B。</p><p><img src=\"https://static001.geekbang.org/resource/image/fe/3d/fe4c0b674649e5a18b5992c71316253d.png?wh=1328*534\" alt=\"\"></p><p>最终，我们求出系数矩阵为$[1 1.5]$，也就是说$b_1 = 1$, $b_2 = 1.5$。实际上，这两个数值是精确解。我们用高斯消元也是能获得同样结果的。接下来，让我稍微修改一下$y$值，让这个方程组没有精确解。</p><p>$b_1·0+b_2·1=1.4$<br>\n$b_1·1-b_2·1=-0.48$<br>\n$b_1·2+b_2·8=13.2$</p><p>你可以尝试高斯消元法对这个方程组求解，你会发现只要两个方程就能求出解，但是无论是哪两个方程求出的解，都无法满足第三个方程。</p><p>那么通过最小二乘法，我们能不能求导一个近似解，保证_ε_足够小呢？下面，让我们遵循之前求解$(X’X)^{-1}X’Y$的过程，来计算$B$。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/10/f3653f2fd1220936834777b62cced310.png?wh=1330*800\" alt=\"\"></p><p>计算完毕之后，你会发现两个系数的值分别变为$b_1 = 0.938, b_2 = 1.415$。由于这不是精确解，所以让我们看看有了这系数矩阵$B$之后，原有的观测数据中，真实值和预测值的差别。</p><p>首先我们通过系数矩阵$B$和自变量矩阵$X$计算出来预测值。</p><p><img src=\"https://static001.geekbang.org/resource/image/a3/7a/a3cf56dc09e8e53304060a418ea9707a.png?wh=864*214\" alt=\"\"></p><p>然后是样本数据中的观测值。这里我们假设这些值是真实值。</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/d9/a693f3f03a81c01d7f34e615ce473cd9.png?wh=296*238\" alt=\"\"></p><p>根据误差$ε$的定义，我们可以得到：</p><p><img src=\"https://static001.geekbang.org/resource/image/94/20/947a02d7580305c8f3a6e19d64dbf120.png?wh=1460*144\" alt=\"\"></p><p>说到这里，你可能会怀疑，通过最小二乘法所求得的系数$b_1 = 0.949$和$b_2 = 1.415$，是不是能让$ε$最小呢？这里，我们随机的修改一下这两个系数，变为$b_1 = 0.95$和$b_2 = 1.42$，然后我们再次计算预测的$y$值和$ε$。</p><p><img src=\"https://static001.geekbang.org/resource/image/36/0e/36d8fb93ae5967f2e75cabd4da43890e.png?wh=842*236\" alt=\"\"></p><p>很明显，0.064是大于之前的0.0158。</p><p>这两次计算预测值_y_的过程，其实也是我们使用线性回归，对新的数据进行预测的过程。简短地总结一下，线性回归模型根据大量的训练样本，推算出系数矩阵$B$，然后根据新数据的自变量$X$向量或者矩阵，计算出因变量的值，作为新数据的预测。</p><h2>Python代码实现</h2><p>这一部分，我们使用Python的代码，来验证一下之前的推算结果是不是正确，并看看最小二乘法和Python sklearn库中的线性回归，这两种结果的对比。</p><p>首先，我们使用Python numpy库中的矩阵操作来实现最小二乘法。主要的函数操作涉及矩阵的转置、点乘和求逆。具体的代码和注释我列在了下方。</p><pre><code>from numpy import *\n\nx = mat([[0,1],[1,-1],[2,8]])\ny = mat([[1.4],[-0.48],[13.2]])\n\n# 分别求出矩阵X'、X'X、(X'X)的逆\n# 注意，这里的I表示逆矩阵而不是单位矩阵\nprint(&quot;X矩阵的转置X'：\\n&quot;, x.transpose())\nprint(&quot;\\nX'点乘X：\\n&quot;, x.transpose().dot(x))\nprint(&quot;\\nX'X矩阵的逆\\n&quot;, (x.transpose().dot(x)).I)\n\nprint(&quot;\\nX'X矩阵的逆点乘X'\\n&quot;, (x.transpose().dot(x)).I.dot(x.transpose()))\nprint(&quot;\\n系数矩阵B：\\n&quot;, (x.transpose().dot(x)).I.dot(x.transpose()).dot(y))\n</code></pre><p>通过上述代码，你可以看到每一步的结果，以及最终的矩阵$B$。你可以把输出结果和之前手动推算的结果进行对比，看看是不是一致。</p><p>除此之外，我们还可把最小二乘法的线性拟合结果和sklearn库中的LinearRegression().fit()函数的结果相比较，具体的代码和注释我也放在了这里。</p><pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.read_csv(&quot;/Users/shenhuang/Data/test.csv&quot;)\ndf_features = df.drop(['y'], axis=1)     #Dataframe中除了最后一列，其余列都是特征，或者说自变量\ndf_targets = df['y']                     #Dataframe最后一列是目标变量，或者说因变量\n\nprint(df_features, df_targets)\nregression = LinearRegression().fit(df_features, df_targets)        #使用特征和目标数据，拟合线性回归模型\nprint(regression.score(df_features, df_targets))        #拟合程度的好坏\nprint(regression.intercept_)\nprint(regression.coef_)            #各个特征所对应的系数\n</code></pre><p>其中，test.csv文件的内容我也列在了这里。</p><p>$x_1,x_2,y$<br>\n$0,1,1.4$<br>\n$1,-1,-0.48$<br>\n$2,8,13.2$</p><p>这样写是为了方便我们使用pandas读取csv文件并加载为dataframe。</p><p>在最终的结果中，1.0表示拟合程度非常好，而-0.014545454545452863表示一个截距，[0.94909091 1.41454545]表示系数$b_1$和$b_2$的值。这个结果和我们最小二乘法的结果有所差别，主要原因是LinearRegression().fit()默认考虑了有线性函数存在截距的情况。那么我们使用最小二乘法是不是也可以考虑有截距的情况呢？答案是肯定的，不过我们首先要略微修改一下方程组和矩阵$X$。如果我们假设有截距存在，那么线性回归方程就要改写为：</p><p>$b_0+b_1·x_1+b_2·x_2+…+b_{n-1}·x_{n-1}+b_n·x_n=y$</p><p>其中，$b_0$表示截距，而我们这里的方程组用例就要改写为：</p><p>$b_0+b_1·0+b_2·1=1.4$<br>\n$b_0+b_1·1-b_2·1=-0.48$<br>\n$b_0+b_1·2+b_2·8=13.2$</p><p>而矩阵$X$要改写为：</p><p><img src=\"https://static001.geekbang.org/resource/image/41/fb/417cf726043be38e7cf0a46a9eea3bfb.png?wh=350*228\" alt=\"\"></p><p>然后我们再执行下面这段代码。</p><pre><code>from numpy import *\n\nx = mat([[1,0,1],[1,1,-1],[1,2,8]])\ny = mat([[1.4],[-0.48],[13.2]])\n\nprint(&quot;\\n系数矩阵B：\\n&quot;, (x.transpose().dot(x)).I.dot(x.transpose()).dot(y))\n</code></pre><p>你就会得到：</p><pre><code> 系数矩阵B：\n     [[-0.01454545]\n     [ 0.94909091]\n     [ 1.41454545]]\n\n</code></pre><p>这个结果和LinearRegression().fit()的结果就一致了。</p><p>需要注意的是，使用线性回归的时候，我们都有一个前提假设，那就是数据的自变量和因变量之间呈现线性关系。如果不是线性关系，那么使用线性模型来拟合的效果一定不好。比如，之前在解释欠拟合的时候，我用过下面这个例子。</p><p><img src=\"https://static001.geekbang.org/resource/image/0b/a5/0b9f8c88a846626c74a803ee645bc1a5.png?wh=630*618\" alt=\"\"></p><p>上面这张图的数据分布并没有表达线性关系，所以我们需要对原始的数据进行非线性的变换，或者是使用非线性的模型来拟合。</p><p>那么，我们如何判断一个数据集是不是能用线性模型表示呢？在线性回归中，我们可以使用决定系数R2。这个统计指标使用了回归平方和与总平方和之比，是反映模型拟合度的重要指标。它的取值在0到1之间，越接近于1表示拟合的程度越好、数据分布越接近线性关系。随着自变量个数的增加，R2将不断增大，因此我们还需要考虑方程所包含的自变量个数对R2的影响，这个时候可使用校正的决定系数Rc2。所以，在使用各种科学计算库进行线性回归时，你需要关注R2或者Rc2，来看看是不是一个好的线性拟合。在之前的代码实践中，我们提到的regression.score函数，其实就是返回了线性回归的R2。</p><h2>总结</h2><p>今天我们使用了具体的案例来推导最小二乘法的计算过程，并用Python代码进行了验证。通过最近3节的讲解，相信你对线性方程组求精确解、求近似解、以及如何在线性回归中运用这些方法，有了更加深入的理解。</p><p>实际上，从广义上来说，最小二乘法不仅可以用于线性回归，还可以用于非线性的回归。其主要思想还是要确保误差ε最小，但是由于现在的函数是非线性的，所以不能使用求多元方程求解的办法来得到参数估计值，而需要采用迭代的优化算法来求解，比如梯度下降法、随机梯度下降法和牛顿法。</p><h2>思考题</h2><p>我这里给出一个新的方程组，请通过最小二乘法推算出系数的近似解，并使用你熟悉的语言进行验证。</p><p>$b_1+b_2·3+b_3·(-7)=-7.5$<br>\n$b_1·2+b_2·5+b_3·4=5.2$<br>\n$b_1·(-3)+b_2·(-7)+b_3·(-2)=-7.5$<br>\n$b_1·1+b_2·4+b_3·(-12)=-15$</p><p>欢迎留言和我分享，也欢迎你在留言区写下今天的学习笔记。你可以点击“请朋友读”，把今天的内容分享给你的好友，和他一起精进。</p>","comments":[{"had_liked":false,"id":89065,"user_name":"余泽锋","can_delete":false,"product_type":"c1","uid":1003207,"ip_address":"","ucode":"5AB1499746C003","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg","comment_is_top":false,"comment_ctime":1556070918,"is_pvip":false,"discussion_count":0,"race_medal":1,"score":"14440972806","product_id":100021201,"comment_content":"import numpy as np<br>X = np.mat([[1, 3, -7], [2, 5, 4], [-3, -7, -2], [1, 4, -12]])<br>Y = np.mat([[-7.5], [5.2], [-7.5], [-15]])<br>B1 = X.transpose().dot(X).I<br>B2 = B1.dot(X.transpose())<br>B = B2.dot(Y)<br>&#39;&#39;&#39;<br>matrix([[12.01208791],<br>        [-4.35934066],<br>        [ 0.82527473]])<br>&#39;&#39;&#39;","like_count":3},{"had_liked":false,"id":231684,"user_name":"骑行的掌柜J","can_delete":false,"product_type":"c1","uid":1474214,"ip_address":"","ucode":"3163102651C653","user_header":"https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg","comment_is_top":false,"comment_ctime":1593747448,"is_pvip":false,"replies":[{"id":"86132","content":"你可以这么想象，如果只看一维变量，那么就是在二维平面上数据分布是不是像一根直线，而同时看多个变量，其实就是在多维空间中看数据分布是不是像一根直线，所以两者并不一样。如果在多维空间中有一条比较好的直线拟合，投影到单维空间，就不一定能很好的拟合了。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1594313791,"ip_address":"","comment_id":231684,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10183682040","product_id":100021201,"comment_content":"感谢黄老师 之前一直只知道建模，拟合，预测，但是背后的数学原理一直没有很理解！！！现在有种恍然大悟的感觉！原来是这样，当因变量和自变量呈现线性关系，建模这一系列动作都是基于上面线代的原理！😁终于get到了它在求什么了！！<br>PS：刚刚评论有个朋友也提问了如何判断一个数据集可以用线性模型？可以用R2，也就是regression.score这个方法，<br>所以请问黄老师：是只有先去用线性模型拟合数据集，再来判断模型是否合适吗？<br>还是我可以从拿到数据集一开始，直接查看每个自变量和因变量的关系是否呈线性再来决定线性模型是否合适？谢谢","like_count":2,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500407,"discussion_content":"你可以这么想象，如果只看一维变量，那么就是在二维平面上数据分布是不是像一根直线，而同时看多个变量，其实就是在多维空间中看数据分布是不是像一根直线，所以两者并不一样。如果在多维空间中有一条比较好的直线拟合，投影到单维空间，就不一定能很好的拟合了。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1594313791,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214313,"user_name":"罗耀龙@坐忘","can_delete":false,"product_type":"c1","uid":1917663,"ip_address":"","ucode":"3CEA258DE7F3C7","user_header":"https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg","comment_is_top":false,"comment_ctime":1588722546,"is_pvip":true,"discussion_count":0,"race_medal":5,"score":"10178657138","product_id":100021201,"comment_content":"茶艺师学编程<br><br>因为还不会编程，我只能用手算了……<br><br>B=[1.463916<br>      -0.403059<br>      -0.616992]<br><br>然后再算了误差，23.907，嗯，我有98％的信心相信我这次算错了。<br><br>而且我还不知道在哪里找错。<br><br>我想到如果是我在编写程序时出现这状况，很有可能会比这更惨……","like_count":2},{"had_liked":false,"id":78599,"user_name":"Joe","can_delete":false,"product_type":"c1","uid":1337998,"ip_address":"","ucode":"EC76699640B7BF","user_header":"https://static001.geekbang.org/account/avatar/00/14/6a/8e/7b6ea886.jpg","comment_is_top":false,"comment_ctime":1553181089,"is_pvip":false,"replies":[{"id":"28693","content":"写得很好，至于逆矩阵更好的求法，我要查一下资料看看有无更优的解。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1553274772,"ip_address":"","comment_id":78599,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10143115681","product_id":100021201,"comment_content":"回答与疑问：<br>1. 非线性关系的数据拟合，可以先将自变量转为非线性。如转化为多项式（sklearn的PolynomialFeatures）。再用线性回归的方法去拟合。<br>2. 请问老师对于求解逆矩阵有没有什么高效的方法？<br>附上以前写的polyfit方法，请老师指点。谢谢<br>    def oneDPolynomiaTransform(self, x_origin):<br>        &#39;&#39;&#39;<br>        @description: generate polynomial for 1D input data. rule: [x0, x0^2, x0^3,...,x0^degreee]<br>        @param {type} x_origin- data before transformed[nX1]<br>        @return: x_transformed- data after transformed[nXdegree]<br>        &#39;&#39;&#39;<br>        len_features = len(x_origin)<br>        # polynomial feature data after transformed.<br>        x_transformed = np.array([])<br>        for i in range(len_features):<br>            for j in range(self.degree+1):<br>                x_transformed = np.append(<br>                    x_transformed, [(x_origin[i])**(j)])<br>        x_transformed = x_transformed.reshape(-1, self.degree+1)<br>        return x_transformed","like_count":2,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444158,"discussion_content":"写得很好，至于逆矩阵更好的求法，我要查一下资料看看有无更优的解。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1553274772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":138268,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1570138759,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5865106055","product_id":100021201,"comment_content":"高斯消元求得是精确解。<br>线性回归求得是最好近似解，覆盖高斯消元能处理的情况，也能在没有精确解的时候找到近似解，还提供测量近似参数，是处理线性关系的利器。","like_count":1},{"had_liked":false,"id":87127,"user_name":"叮当猫","can_delete":false,"product_type":"c1","uid":1360159,"ip_address":"","ucode":"175BB66517E21B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg","comment_is_top":false,"comment_ctime":1555514522,"is_pvip":false,"replies":[{"id":"31344","content":"这是个好问题，我查了sklearn.linear_mode好像不提供这个数据。<br><br>你可以尝试一下statsmodels.api.OLS这个包，里面应该可以返回rsquared_adj","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1555533177,"ip_address":"","comment_id":87127,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5850481818","product_id":100021201,"comment_content":"文中有提到，如何判断一个数据集是否可以用线性模型来表示，可以使用决定系数R2，随着自变量个数不断增加，R2将不断增大，这时需要用Rc2，而其中R2就是regression.score，那请问Rc2是库里面的什么呢？","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447321,"discussion_content":"这是个好问题，我查了sklearn.linear_mode好像不提供这个数据。\n\n你可以尝试一下statsmodels.api.OLS这个包，里面应该可以返回rsquared_adj","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555533177,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":81575,"user_name":"qinggeouye","can_delete":false,"product_type":"c1","uid":1251536,"ip_address":"","ucode":"5B53EEDD7BEC9C","user_header":"https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg","comment_is_top":false,"comment_ctime":1553951796,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5848919092","product_id":100021201,"comment_content":"&quot;&quot;&quot;<br>思考题同理<br>&quot;&quot;&quot;<br>x = np.mat([[1, 3, -7], [2, 5, 4], [-3, -7, -2], [1, 4, -12]])<br>y = np.mat([[-7.5], [5.2], [-7.5], [-15]])<br>print(&quot;\\n 系数矩阵 B: \\n&quot;, (x.transpose().dot(x)).I.dot(x.transpose()).dot(y))<br><br> 系数矩阵 B: <br> [[12.01208791]<br> [-4.35934066]<br> [ 0.82527473]]","like_count":1},{"had_liked":false,"id":79582,"user_name":"Cest la vie","can_delete":false,"product_type":"c1","uid":1171130,"ip_address":"","ucode":"8CB3AE0DFE9ADF","user_header":"https://static001.geekbang.org/account/avatar/00/11/de/ba/f01dc860.jpg","comment_is_top":false,"comment_ctime":1553509277,"is_pvip":false,"replies":[{"id":"29213","content":"可以考虑到后面加餐的时候来一篇","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1553616120,"ip_address":"","comment_id":79582,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5848476573","product_id":100021201,"comment_content":"老师好! 后面可以来一节PLS偏最小二乘的原理讲解和应用么","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":444549,"discussion_content":"可以考虑到后面加餐的时候来一篇","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1553616120,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":357364,"user_name":"013923","can_delete":false,"product_type":"c1","uid":3035193,"ip_address":"上海","ucode":"1214DAADBCA848","user_header":"","comment_is_top":false,"comment_ctime":1663205535,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1663205535","product_id":100021201,"comment_content":"学习了！","like_count":0},{"had_liked":false,"id":256366,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1603610460,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1603610460","product_id":100021201,"comment_content":"根据最小二乘法系数计算模型，用了一个简单的Python程序计算系数矩阵，程序代码：<br>import numpy as np<br># 矩阵X<br>X = np.array([[1, 3, -7]<br>            ,[2, 5, 4]<br>            ,[-3, -7, -2]<br>            ,[1, 4, -12]<br>             ]<br>            )<br># X的转置X&#39;<br>TRANS_X = X.T<br><br># 目标值向量Y<br>Y = np.array([[-7.5, 5.2, -7.5, -15]]).T<br><br># X&#39;X<br>B = TRANS_X.dot(X)<br><br># (X&#39;X)^-1<br>B = np.matrix(B).I<br><br># [(X&#39;X)^-1]X&#39;<br>B = B.dot(TRANS_X)<br><br># [(XX&#39;)^-1]X&#39;Y<br>B = B.dot(Y)<br><br>print(&#39;系数矩阵：&#39;)<br>print(B)<br><br>print(&#39;验证：&#39;)<br>print(X.dot(B))<br><br>================程序输出结果==================<br>系数矩阵：<br>[[12.01208791]<br> [-4.35934066]<br> [ 0.82527473]]<br>验证：<br>[[ -6.84285714]<br> [  5.52857143]<br> [ -7.17142857]<br> [-15.32857143]]","like_count":0},{"had_liked":false,"id":179730,"user_name":"观望者","can_delete":false,"product_type":"c1","uid":1142107,"ip_address":"","ucode":"08F3D79D3C811D","user_header":"https://static001.geekbang.org/account/avatar/00/11/6d/5b/03763d38.jpg","comment_is_top":false,"comment_ctime":1582084781,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1582084781","product_id":100021201,"comment_content":"#用sklearn的库<br>df = pd.read_csv(&#39;data.csv&#39;)<br>regr = linear_model.LinearRegression()#产生线性拟合的对象<br>X = df.loc[:,[&#39;x1&#39;,&#39;x2&#39;,&#39;x3&#39;]]<br>Y = df[&#39;y&#39;]<br>regr.fit(X, Y)#传入拟合的数据集<br>Y_pred = regr.predict(X)##预测值<br>print(&#39;Coefficients: \\n&#39;, regr.coef_)<br><br>#如果用公式来计算需要在数据中加入 截距（也就是x=1的列），计算如下：<br>inter = pd.Series([1 for i in range(4)], name=&#39;X0&#39;)<br>X_inter = pd.concat([inter,X], axis=1)<br>B = min2mul(X_inter,Y)<br>print(&#39;Coefficients B: \\n&#39;, B)<br>def min2mul(X,Y):<br>    X_REV = np.transpose(X)##矩阵转置<br>    X2 = np.linalg.inv(np.dot(X_REV,X))## 计算(X&#39;X)^(-1)<br>    B = np.dot(np.dot(X2,X_REV),Y)<br>    return B","like_count":0},{"had_liked":false,"id":143507,"user_name":"Ronnyz","can_delete":false,"product_type":"c1","uid":1488280,"ip_address":"","ucode":"9F34527B1D343D","user_header":"https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg","comment_is_top":false,"comment_ctime":1571721095,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1571721095","product_id":100021201,"comment_content":"from numpy import *\r<br>x=mat([[1,3,-7],[2,5,4],[-3,-7,-2],[1,4,-12]])\r<br>y=mat([[-7.5],[5.2],[-7.5],[-15]])\r<br>print(&quot;\\n系数矩阵B:\\n&quot;,(x.transpose().dot(x)).I.dot(x.transpose()).dot(y))<br><br>系数矩阵B:\r<br> [[12.01208791]\r<br> [-4.35934066]\r<br> [ 0.82527473]]","like_count":0}]}