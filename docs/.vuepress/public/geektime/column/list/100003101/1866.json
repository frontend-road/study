{"id":1866,"title":"09 机器学习 | 大道至简：朴素贝叶斯方法","content":"<p>周二我和你分享了机器学习中的线性回归算法，这一算法解决的是从连续取值的输入映射为连续取值的输出的回归问题。今天我分享的算法则用于解决分类问题，即将连续取值的输入映射为离散取值的输出，算法的名字叫作“<strong>朴素贝叶斯方法</strong>”。</p>\n<p>解决分类问题的依据是数据的属性。朴素贝叶斯分类器假定样本的不同属性满足条件独立性假设，并在此基础上应用贝叶斯定理执行分类任务。<strong>其基本思想在于分析待分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出</strong>。</p>\n<p>假设训练数据的属性由n维随机向量$\\bf x$表示，其分类结果用随机变量y表示，那么x和y的统计规律就可以用联合概率分布$P(X, Y)$描述，每一个具体的样本$(x_i, y_i)$都可以通过$P(X, Y)$独立同分布地产生。</p>\n<p>朴素贝叶斯分类器的出发点就是这个联合概率分布，根据条件概率的性质可以得到</p>\n<p> $$ P(X, Y) = P(Y) \\cdot P(X|Y)$$ </p>\n<p>$$= P(X) \\cdot P(Y|X) $$ </p>\n<p>在上式中，P(Y)代表着每个类别出现的概率，也就是<strong>类先验概率</strong>；P(X|Y)代表着在给定的类别下不同属性出现的概率，也就是<strong>类似然概率</strong>。</p>\n<p>先验概率容易根据训练数据计算出来，只需要统计不同类别样本的数目即可。而似然概率受属性取值数目的影响，其估计较为困难。</p>\n<p>如果每个样本包含100个属性，每个属性的取值都可能有100种，那么对分类的每个结果，要计算的条件概率数目就是$100 ^ 2 = 10000$。在这么多参数的情况下，对似然概率的精确估计就需要庞大的数据量。</p>\n<p>要解决似然概率难以估计的问题，就需要“条件独立性假设”登台亮相。<strong>条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用</strong>。<strong>这样类条件概率就变成了属性条件概率的乘积</strong>，在数学公式上可以体现为</p>\n<p> $$ P(X = {\\bf x}|Y = c) = $$</p>\n<p>$$P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\cdots, X^{(n)} = x^{(n)}|Y = c)$$ </p>\n<p>$$= \\Pi_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c) $$</p>\n<p><strong>这正是朴素贝叶斯方法的“朴素”之处，通过必要的假设来简化计算，并回归问题的本质</strong>。</p>\n<!-- [[[read_end]]] -->\n<p>条件独立性假设对似然概率的估计无疑是个天大的好消息。没有这一假设时，每个样本的分类结果y只能刻画其所有属性$x_1, x_2, \\cdots, x_n$形成的整体，只有具有相同$x_1, x_2, \\cdots, x_n$的样本才能放在一起进行评价。当属性数目较多且数据量较少时，要让n个属性同时取到相同的特征就需要些运气了。</p>\n<p>有了条件独立性假设后，分类结果y就相当于实现了n重复用。每一个样本既可以用于刻画$x_1$，又可以用于刻画$x_n$，这无形中将训练样本的数量扩大为原来的n倍，分析属性的每个取值对分类结果的影响时，也有更多数据作为支撑。</p>\n<p>但需要说明的是，<strong>属性的条件独立性假设是个相当强的假设</strong>。</p>\n<p>一个例子是银行在发放房贷时，需要对贷款申请人的情况进行调研，以确定是否发放贷款。本质上这就是个分类问题，分类的结果是“是”与“否”。分类时则需要考虑申请人的年龄、工作岗位、婚姻状况、收入水平、负债情况等因素。这些因素显然不是相互独立的。中年人的收入通常会高于青年人的收入，已婚者的负债水平通常也会高于未婚者的负债水平。</p>\n<p>因而在实际应用中，属性条件独立性假设会导致数据的过度简化，因而会给分类性能带来些许影响。但它带来的数学上的便利却能极大简化分类问题的计算复杂度，性能上的部分折中也就并非不可接受。</p>\n<p>有了训练数据集，先验概率P(Y)和似然概率P(X|Y)就可以视为已知条件，用来求解后验概率P(Y|X)。对于给定的输入$\\bf x$，朴素贝叶斯分类器利用贝叶斯定理求解后验概率，并将后验概率最大的类作为输出。</p>\n<p>由于在所有后验概率的求解中，边界概率P(X)都是相同的，因而其影响可以忽略。将属性条件独立性假设应用于后验概率求解中，就可以得到朴素贝叶斯分类器的数学表达式</p>\n<p>$$ y = \\arg \\mathop {\\max}\\limits_{c_k} P(y = c_k) \\cdot $$</p>\n<p>$$\\Pi_j P(X^{(j)} = x^{(j)}|Y = c_k) $$</p>\n<p>应用朴素贝叶斯分类器处理连续型属性数据时，通常假定属性数据满足正态分布，再根据每个类别下的训练数据计算出正态分布的均值和方差。</p>\n<p><strong>从模型最优化的角度观察，朴素贝叶斯分类器是平均意义上预测能力最优的模型，也就是使期望风险最小化</strong>。期望风险是风险函数的数学期望，度量的是平均意义下模型预测的误差特性，可以视为单次预测误差在联合概率分布P(X, Y)上的数学期望。</p>\n<p>朴素贝叶斯分类器通过将实例分配到后验概率最大的类中，也就同时让1 - P(Y|X)取得最小值。在以分类错误的实例数作为误差时，期望风险就等于1 - P(Y|X)。这样一来，后验概率最大化就等效于期望风险最小化。</p>\n<p>受训练数据集规模的限制，某些属性的取值在训练集中可能从未与某个类同时出现，这就可能导致属性条件概率为0，此时直接使用朴素贝叶斯分类就会导致错误的结论。</p>\n<p>还是以贷款申请为例，如果在训练集中没有样本同时具有“年龄大于60”的属性和“发放贷款”的标签，那么当一个退休人员申请贷款时，即使他是坐拥百亿身家的李嘉诚，朴素贝叶斯分类器也会因为后验概率等于零而将他无情拒绝。</p>\n<p>因为训练集样本的不充分导致分类错误，显然不是理想的结果。为了避免属性携带的信息被训练集中未曾出现过的属性值所干扰，在计算属性条件概率时需要添加一个称为“<strong>拉普拉斯平滑</strong>”的步骤。</p>\n<p>所谓拉普拉斯平滑就是在计算类先验概率和属性条件概率时，在分子上添加一个较小的修正量，在分母上则添加这个修正量与分类数目的乘积。这就可以保证在满足概率基本性质的条件下，避免了零概率对分类结果的影响。当训练集的数据量较大时，修正量对先验概率的影响也就可以忽略不计了。</p>\n<p>事实上，朴素贝叶斯是一种非常高效的方法。当以分类的正确与否作为误差指标时，只要朴素贝叶斯分类器能够把最大的后验概率找到，就意味着它能实现正确的分类。至于找到的最大后验概率的估计值是否精确，反而没那么重要了。</p>\n<p>如果一个实例在两个类别上的后验概率分别是0.9和0.1，朴素贝叶斯分类器估计出的后验概率就可能是0.6和0.4。虽然数值的精度相差较大，但大小的相对关系并未改变。依据这个粗糙估计的后验概率进行分类，得到的依然是正确的结果。</p>\n<p>上面的说法固然言之成理，却不能解释另外一个疑问。虽然属性条件独立性看起来像是空中楼阁，却给朴素贝叶斯分类器带来了实实在在的优良性能，这其中的奥秘何在？为什么在基础假设几乎永远不成立的情况下，朴素贝叶斯依然能够在绝大部分分类任务中体现出优良性能呢？</p>\n<p>一种可能的解释是：在给定的训练数据集上，两个属性之间可能具有相关性，但这种相关性在每个类别上都以同样的程度体现。 这种情况显然违背了条件独立性假设，却不会破坏朴素贝叶斯分类器的最优性。</p>\n<p>即使相关性在不同类别上的分布不是均匀的也没关系，只看两个单独的属性，它们之间可能存在强烈的依赖关系，会影响分类的结果。但当所有属性之间的依赖关系一起发挥作用时，它们就可能相互抵消，不再影响分类。</p>\n<p>简而言之，决定性的因素是所有属性之间的依赖关系的组合。<strong>影响朴素贝叶斯的分类的是所有属性之间的依赖关系在不同类别上的分布，而不仅仅是依赖关系本身</strong>。可即便如此，属性条件独立性假设依然会影响分类性能。为了放宽这一假设，研究人员又提出了“<strong>半朴素贝叶斯分类器</strong>”的学习方法。</p>\n<p>半朴素贝叶斯分类器考虑了部分属性之间的依赖关系，既保留了属性之间较强的相关性，又不需要完全计算复杂的联合概率分布。<strong>常用的方法是建立独依赖关系：假设每个属性除了类别之外，最多只依赖一个其他属性</strong>。由此，根据属性间依赖关系确定方式的不同，便衍生出了多种独依赖分类器。</p>\n<p>朴素贝叶斯分类器的应用场景非常广泛。它可以根据关键词执行对一封邮件是否是垃圾邮件的二元分类，也可以用来判断社交网络上的账号到底是活跃用户还是僵尸粉。在信息检索领域，这种分类方法尤为实用。总结起来，<strong>以朴素贝叶斯分类器为代表的贝叶斯分类方法的策略是：根据训练数据计算后验概率，基于后验概率选择最佳决策</strong>。</p>\n<p>今天我和你分享了机器学习基本算法之一的朴素贝叶斯方法的基本原理，其要点如下：</p>\n<ul>\n<li>朴素贝叶斯方法利用后验概率选择最佳分类，后验概率可以通过贝叶斯定理求解；</li>\n<li>朴素贝叶斯方法假定所有属性相互独立，基于这一假设将类条件概率转化为属性条件概率的乘积；</li>\n<li>朴素贝叶斯方法可以使期望风险最小化；</li>\n<li>影响朴素贝叶斯分类的是所有属性之间的依赖关系在不同类别上的分布。</li>\n</ul>\n<p>在使用高维数据集时，每个样本都会包含大量的属性，这时属性条件概率连乘的结果会非常接近于零，导致下溢的发生。如何防止因概率过小造成的下溢呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/1d/ab/1d23a0935e1e853e21a0d6a0dab9e4ab.jpg?wh=1110*1032\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"08 机器学习 | 简约而不简单：线性回归","id":1865},"right":{"article_title":"10 机器学习 | 衍化至繁：逻辑回归","id":1867}},"comments":[{"had_liked":false,"id":1811,"user_name":"","can_delete":false,"product_type":"c1","uid":1031328,"ip_address":"","ucode":"1B73BA45ACD06C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bc/a0/97c7679b.jpg","comment_is_top":false,"comment_ctime":1514522007,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"48759162263","product_id":100003101,"comment_content":"大佬，能有简单的项目或习题，让我们实践下不？","like_count":11},{"had_liked":false,"id":2453,"user_name":"王杰","can_delete":false,"product_type":"c1","uid":1047456,"ip_address":"","ucode":"B1BF77B43CE6B8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/fb/a0/8594f5ed.jpg","comment_is_top":false,"comment_ctime":1517309027,"is_pvip":false,"replies":[{"id":"539","content":"原则上是会取得良好效果，但应用在实际问题中还要看真实表现。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1517546885,"ip_address":"","comment_id":2453,"utype":1}],"discussion_count":2,"race_medal":0,"score":"40172014691","product_id":100003101,"comment_content":"讲的简洁易懂，回家途中看完了！一个问题：如果样本先降维去除属性相关性再用朴素贝叶斯分类，效果是不是就很好？","like_count":9,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415746,"discussion_content":"原则上是会取得良好效果，但应用在实际问题中还要看真实表现。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1517546885,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210709,"discussion_content":"降维一定有信息损失，真实效果看情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584763720,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3510,"user_name":"杜浩","can_delete":false,"product_type":"c1","uid":1057388,"ip_address":"","ucode":"AB575CFC9D3417","user_header":"https://static001.geekbang.org/account/avatar/00/10/22/6c/90527684.jpg","comment_is_top":false,"comment_ctime":1519787528,"is_pvip":false,"replies":[{"id":"696","content":"后验概率最大化意味着把实例划分到最可能的类中，使分类的错误概率最小，也就是期望风险最小。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1519974881,"ip_address":"","comment_id":3510,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31584558600","product_id":100003101,"comment_content":"朴素贝叶斯为什么是期望风险最小化的 这点还是不太理解","like_count":7,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415919,"discussion_content":"后验概率最大化意味着把实例划分到最可能的类中，使分类的错误概率最小，也就是期望风险最小。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1519974881,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":165421,"user_name":"杨家荣","can_delete":false,"product_type":"c1","uid":1259241,"ip_address":"","ucode":"3DA65396C7F002","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132","comment_is_top":false,"comment_ctime":1577236348,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"14462138236","product_id":100003101,"comment_content":"打卡第七天(7&#47;21):<br>&lt;&lt;人工智能基础课09&gt;&gt;朴素贝叶斯方法<br>回答老师问题:<br>在使用高维数据集时，每个样本都会包含大量的属性，这时属性条件概率连乘的结果会非常接近于零，导致下溢的发生。如何防止因概率过小造成的下溢呢？<br>这样想到,得到&lt;&lt;吴军数学课&gt;&gt;42讲中古德-图灵折扣估计,预防黑天鹅事件.吴军老师讲古德用每一次的概率加一除总数,这样的结果不至于出现为0的情况,但最后估值肯定不准确,用朴素贝叶斯方法做分类应该没问题,我特意重读了吴军老师的课,老师还讲了吴军老师导师创的差值法,这个更接近,更精确,最后还提到备用法;等这些方法,都能最大程度防止因概率过小造成的下溢;我个人看法;<br>今日所学:朴素贝叶斯方法,将连续取值的输入映射为离散取值的输出，算法的名字叫作“朴素贝叶斯方法”。<br>其基本思想在于分析待分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出.<br>名词:类先验概率,类似然概率(条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用),这正是朴素贝叶斯方法的“朴素”之处，通过必要的假设来简化计算，并回归问题的本质;<br>1,从模型最优化的角度观察，朴素贝叶斯分类器是平均意义上预测能力最优的模型，也就是使期望风险最小化;<br>2,影响朴素贝叶斯的分类的是所有属性之间的依赖关系在不同类别上的分布，而不仅仅是依赖关系本身;<br>3,半朴素贝叶斯分类器考虑了部分属性之间的依赖关系，既保留了属性之间较强的相关性，又不需要完全计算复杂的联合概率分布。常用的方法是建立独依赖关系：假设每个属性除了类别之外，最多只依赖一个其他属性。由此，根据属性间依赖关系确定方式的不同，便衍生出了多种独依赖分类器。<br>4,朴素贝叶斯分类器的应用场景非常广泛。它可以根据关键词执行对一封邮件是否是垃圾邮件的二元分类，也可以用来判断社交网络上的账号到底是活跃用户还是僵尸粉。在信息检索领域，这种分类方法尤为实用。总结起来，以朴素贝叶斯分类器为代表的贝叶斯分类方法的策略是：根据训练数据计算后验概率，基于后验概率选择最佳决策。<br>总结:<br>1,朴素贝叶斯方法利用后验概率选择最佳分类，后验概率可以通过贝叶斯定理求解；<br>2,朴素贝叶斯方法假定所有属性相互独立，基于这一假设将类条件概率转化为属性条件概率的乘积；<br>3,朴素贝叶斯方法可以使期望风险最小化；<br>4,影响朴素贝叶斯分类的是所有属性之间的依赖关系在不同类别上的分布。","like_count":3,"discussions":[{"author":{"id":1382587,"avatar":"https://static001.geekbang.org/account/avatar/00/15/18/bb/9299fab1.jpg","nickname":"Null","note":"","ucode":"A7D4DF2A43C7D8","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":589158,"discussion_content":"兄弟首先很佩服你。 只是你这跟抄书的笔记其实用处不大啊。有看笔记的时间，文章中都搜索到了。感动自己形的学习方式。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1664454346,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":110211,"user_name":"宋不肥","can_delete":false,"product_type":"c1","uid":1240126,"ip_address":"","ucode":"32B34AF579C91C","user_header":"https://static001.geekbang.org/account/avatar/00/12/ec/3e/885ec1d2.jpg","comment_is_top":false,"comment_ctime":1562208454,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14447110342","product_id":100003101,"comment_content":"概率取对数，把连乘变成连加","like_count":3},{"had_liked":false,"id":1830,"user_name":"七月","can_delete":false,"product_type":"c1","uid":1003340,"ip_address":"","ucode":"AC4E3DC34B1A97","user_header":"","comment_is_top":false,"comment_ctime":1514648260,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"14399550148","product_id":100003101,"comment_content":"可以用对数吗","like_count":3},{"had_liked":false,"id":3383,"user_name":"吴文敏","can_delete":false,"product_type":"c1","uid":1003150,"ip_address":"","ucode":"A4303003E547D1","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4e/8e/f4297447.jpg","comment_is_top":false,"comment_ctime":1519303792,"is_pvip":false,"replies":[{"id":"663","content":"没错，应该是每个属性的取值都有2种。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1519697169,"ip_address":"","comment_id":3383,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10109238384","product_id":100003101,"comment_content":"如果每个样本包含 100 个属性，每个属性的取值都可能有 100 种，那么对分类的每个结果，要计算的条件概率数目就是 100^2=10000 感觉这里应该是100^100<br>","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415886,"discussion_content":"没错，应该是每个属性的取值都有2种。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1519697169,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1106596,"avatar":"https://static001.geekbang.org/account/avatar/00/10/e2/a4/db658021.jpg","nickname":"王刚","note":"","ucode":"68E3CA3BCB3883","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":348169,"discussion_content":"如果每个属性取值有两种，100个属性，要计算的条件概率数目应该是2的100次方吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612444678,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":1803,"user_name":"秦龙君","can_delete":false,"product_type":"c1","uid":1004181,"ip_address":"","ucode":"2085706DBABD5C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/52/95/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1514509163,"is_pvip":false,"replies":[{"id":"295","content":"会出的","user_name":"池建强回复","user_name_real":"卖桃者","uid":"1000012","ctime":1514516620,"ip_address":"","comment_id":1803,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10104443755","product_id":100003101,"comment_content":"学习了。我想问老师一个问题，所有文章更新完了，有集结出书的计划吗？我感觉平时看完后，再用书重新看一遍，效果更好。","like_count":2,"discussions":[{"author":{"id":1000012,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/42/4c/1d5c1263.jpg","nickname":"池建强","note":"","ucode":"0E359BFB3AC329","race_medal":1,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415538,"discussion_content":"会出的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1514516620,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210711,"discussion_content":"附上实例代码，更好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584763772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":355103,"user_name":"冯思鸣","can_delete":false,"product_type":"c1","uid":1010560,"ip_address":"广东","ucode":"A4D3F412D7904D","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6b/80/dd2b3b3f.jpg","comment_is_top":false,"comment_ctime":1661081231,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1661081231","product_id":100003101,"comment_content":"音量有时会很小…","like_count":0},{"had_liked":false,"id":259456,"user_name":"IT蜗壳-Tango","can_delete":false,"product_type":"c1","uid":1000612,"ip_address":"","ucode":"92330844C1EF9C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/44/a4/7a45d979.jpg","comment_is_top":false,"comment_ctime":1604731216,"is_pvip":true,"discussion_count":0,"race_medal":1,"score":"1604731216","product_id":100003101,"comment_content":"太棒了，如果有py代码就更棒啦。学习到了。","like_count":0},{"had_liked":false,"id":194657,"user_name":"oci","can_delete":false,"product_type":"c1","uid":1200077,"ip_address":"","ucode":"091547A9B22E19","user_header":"","comment_is_top":false,"comment_ctime":1585095375,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585095375","product_id":100003101,"comment_content":"谢谢。理论很好，我还得多多实践","like_count":0},{"had_liked":false,"id":146001,"user_name":"上善若水","can_delete":false,"product_type":"c1","uid":1653332,"ip_address":"","ucode":"E3F15FB8576626","user_header":"https://static001.geekbang.org/account/avatar/00/19/3a/54/72402617.jpg","comment_is_top":false,"comment_ctime":1572415827,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1572415827","product_id":100003101,"comment_content":"条件独立性，本身就是思维模式的问题，可能常识并不正确","like_count":0,"discussions":[{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210714,"discussion_content":"条件独立，是为了简化计算。现实看，确实效果很好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584763982,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":37712,"user_name":"隔壁老任","can_delete":false,"product_type":"c1","uid":1057580,"ip_address":"","ucode":"74F9FA04460D2B","user_header":"","comment_is_top":false,"comment_ctime":1541690170,"is_pvip":false,"replies":[{"id":"18932","content":"1. 这句话的本义是将输入数据转化成分类结果，和回归问题形成对比。连续属性也是可以处理的，但需要离散化的过程，也能直接计算。<br>2. 这些传统方法不像深度学习，需要一轮一轮地训练。一波数据过来，参数就确定了，要更新就得用新的数据。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1545322687,"ip_address":"","comment_id":37712,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1541690170","product_id":100003101,"comment_content":"老师你好，请教俩问题，1.第一段有句话有点懵，朴素贝叶斯是 将连续输入转化为离散输出么？我目前简单的，感觉都是离散输入到离散输出，属性的取值也多是离散的，如果是连续的，数量就太大了<br>2.同最后一个问题，因为朴素贝叶斯是用的后验概率相乘，貌似训练一次后，参数就不会变了？后续的的参数更新一般用什么方法呢？<br>谢谢","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":428459,"discussion_content":"1. 这句话的本义是将输入数据转化成分类结果，和回归问题形成对比。连续属性也是可以处理的，但需要离散化的过程，也能直接计算。\n2. 这些传统方法不像深度学习，需要一轮一轮地训练。一波数据过来，参数就确定了，要更新就得用新的数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1545322687,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210716,"discussion_content":"1. 贝叶斯用来分类，结果当然是离散的：类别1、类别2等。连续的，为什么一定数量大？你的数量，应该是样本量。\n2. 训练完成，得到最终参数，模型就确定下来了。还要更新？说明还需要训练。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584764422,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":26446,"user_name":"wdf","can_delete":false,"product_type":"c1","uid":1168358,"ip_address":"","ucode":"179C28A57D41C6","user_header":"","comment_is_top":false,"comment_ctime":1537603537,"is_pvip":false,"replies":[{"id":"10497","content":"后验概率归一化之后和应该为1，那么0.9和0.6就应该是0.6和0.4。即使不归一化，更大的0.9也说明这个样本更像正例。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1538283392,"ip_address":"","comment_id":26446,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1537603537","product_id":100003101,"comment_content":"老师如果朴素贝叶斯算法，只在乎分类是否正确。是否他给出的概率值就参考意义不大？如果给出是正立的，赵军，只有一个是0.9，一个是0.6是不是很难说，是有区别的。","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":424637,"discussion_content":"后验概率归一化之后和应该为1，那么0.9和0.6就应该是0.6和0.4。即使不归一化，更大的0.9也说明这个样本更像正例。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1538283392,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210719,"discussion_content":"贝叶斯公式，右侧分母没有参与计算。所以计算结果是用来比较相对大小的，不是绝对值。分类问题，相对大小就够了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584764589,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4378,"user_name":"夏震华(围巾)","can_delete":false,"product_type":"c1","uid":1059549,"ip_address":"","ucode":"9D2AD222CB9B1A","user_header":"https://static001.geekbang.org/account/avatar/00/10/2a/dd/aee4d7de.jpg","comment_is_top":false,"comment_ctime":1522064010,"is_pvip":false,"replies":[{"id":"1226","content":"这可以看成取概率对数的特例","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1522415919,"ip_address":"","comment_id":4378,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1522064010","product_id":100003101,"comment_content":"在使用高维数据集时，每个样本都会包含大量的属性，这时属性条件概率连乘的结果会非常接近于零，导致下溢的发生。如何防止因概率过小造成的下溢呢？<br>都乘个100，放大了，然后到了后面在统一除去?如何","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416333,"discussion_content":"这可以看成取概率对数的特例","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522415919,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3584,"user_name":"啊哈哈","can_delete":false,"product_type":"c1","uid":1045167,"ip_address":"","ucode":"A5A7A0FD4CB70F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f2/af/db328094.jpg","comment_is_top":false,"comment_ctime":1519949406,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1519949406","product_id":100003101,"comment_content":"根据训练数据计算后验概率，基于后验概率选择最佳决策。","like_count":0},{"had_liked":false,"id":2159,"user_name":"Andy","can_delete":false,"product_type":"c1","uid":1012037,"ip_address":"","ucode":"D69ED1BAF42262","user_header":"https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1516087923,"is_pvip":false,"replies":[{"id":"443","content":"其实朴素贝叶斯也是有损失函数的，它的损失函数就是分类错误数的数学期望，让这个函数最小化和后验概率最大化是等价的，因而算法本身就暗含了最优化的过程。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1516527076,"ip_address":"","comment_id":2159,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1516087923","product_id":100003101,"comment_content":"王老师您好，感觉朴素贝叶斯不像逻辑回归那样有个loss func 可以做权重的学习，那么朴素贝叶斯训练好的模型怎么才能持久化呢？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415656,"discussion_content":"其实朴素贝叶斯也是有损失函数的，它的损失函数就是分类错误数的数学期望，让这个函数最小化和后验概率最大化是等价的，因而算法本身就暗含了最优化的过程。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1516527076,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}