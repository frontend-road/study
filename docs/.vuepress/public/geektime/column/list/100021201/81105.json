{"id":81105,"title":"24 | 语言模型：如何使用链式法则和马尔科夫假设简化概率模型？","content":"<p>你好，我是黄申。</p><p>之前我给你介绍了用于分类的朴素贝叶斯算法。我们讲了，朴素贝叶斯算法可以利用贝叶斯定理和变量之间的独立性，预测一篇文章属于某个分类的概率。除了朴素贝叶斯分类，概率的知识还广泛地运用在其他机器学习算法中，例如语言模型、马尔科夫模型、决策树等等。</p><p>今天我就来说说，基于概率和统计的语言模型。语言模型在不同的领域、不同的学派都有不同的定义和实现，因此为了避免歧义，我这里先说明一下，我们谈到的语言模型，都是指基于概率和统计的模型。</p><h2>语言模型是什么？</h2><p>在解释语言模型之前，我们先来看两个重要的概念。第一个是链式法则，第二个是马尔科夫假设及其对应的多元文法模型。为什么要先说这两个概念呢？这是因为链式法则可以把联合概率转化为条件概率，而马尔科夫假设通过变量间的独立性来减少条件概率中的随机变量，两者结合就可以大幅简化计算的复杂度。</p><h3>1.链式法则</h3><p>链式法则是概率论中一个常用法则。它使用一系列条件概率和边缘概率，来推导联合概率，我用一个公式来给你看看它的具体表现形式。</p><p><img src=\"https://static001.geekbang.org/resource/image/de/0c/de5d37d9392a18225b4f9d522b4f180c.png?wh=554*42\" alt=\"\"></p><p>其中，$x_{1}$到$x_{n}$表示了n个随机变量。</p><!-- [[[read_end]]] --><p>这个公式是怎么来的呢？你还记得联合概率、条件概率和边缘概率之间的“三角”关系吗？我们用这三者的关系来推导一下，最终我们可以得到链式法则。</p><p><img src=\"https://static001.geekbang.org/resource/image/35/a2/35bbc4bd80d8fba7bd6a63c1cdbd86a2.png?wh=475*149\" alt=\"\"></p><p>推导的每一步，都是使用了三种概率之间的关系，这个应该不难理解。</p><h3>2.马尔科夫假设</h3><p>理解了链式法则，我们再来看看马尔可夫假设。这个假设的内容是：任何一个词$w_{i}$出现的概率只和它前面的1个或若干个词有关。基于这个假设，我们可以提出<strong>多元文法（Ngram）模型</strong>。Ngram中的“N”很重要，它表示任何一个词出现的概率，只和它前面的N-1个词有关。</p><p>我以二元文法模型为例，来给你解释。按照刚才的说法，二元文法表示，某个单词出现的概率只和它前面的1个单词有关。也就是说，即使某个单词出现在一个很长的句子中，我们也只需要看前面那1个单词。用公式来表示出来就是这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/b0/e9/b0bc4bb97ba4b233e0ae9db0291059e9.png?wh=263*39\" alt=\"\"></p><p>如果是三元文法，就说明某个单词出现的概率只和它前面的2个单词有关。即使某个单词出现在很长的一个句子中，它也只看相邻的前2个单词。用公式来表达就是这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/94/a619504d5c698f0c519904cd6eddf094.png?wh=302*37\" alt=\"\"></p><p>你也许会好奇，那么一元文法呢？按照字面的意思，就是每个单词出现的概率和前面0个单词有关。这其实说明，每个词的出现都是相互独立的。用公式来表达就是这样的：</p><p><img src=\"https://static001.geekbang.org/resource/image/52/25/523ff06450418d4d91d8e6d2756e1025.png?wh=400*40\" alt=\"\"></p><p>弄明白链式法则和马尔科夫假设之后，我们现在来看语言模型。</p><p>假设我们有一个统计样本文本$d$，$s$表示某个有意义的句子，由一连串按照特定顺序排列的词$w_{1}，w_{2},…,w_{n}$组成，这里$n$是句子里单词的数量。现在，我们想知道根据文档d的统计数据，$s$在文本中出现的可能性，即$P(s|d)$，那么我们可以把它表示为$P(s|d)=P(w_{1}, w_{2}, …, w_{n} | d$)。假设我们这里考虑的都是在集合d的情况下发生的概率，所以可以忽略$d$，写为$P(s)=P(w_{1}, w_{2}, …, w_{n})$。</p><p>到这里，我们碰到了第一个难题，就是如何计算$P(w_{1}, w_{2}, …, w_{n})$要在集合中找到一模一样的句子，基本是不可能的。这个时候，我们就需要使用链式法则。我们可以把这个式子改写为：</p><p><img src=\"https://static001.geekbang.org/resource/image/2f/23/2fe51ac1be8efbad2082627860aada23.png?wh=733*40\" alt=\"\"></p><p>乍一看，问题似乎是解决了。因为通过文档集合C，你可以知道$P(w_{1})$，$P(w_{2}|w_{1})$这种概率。不过，再往后看，好像$P(w_{3}|w_{1},w_{2})$出现概率很低，$P(w_{4}|w_{1},w_{2},w_{3})$出现的概率就更低了。一直到$P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$，基本上又为0了。我们可以使用上一节提到的平滑技巧，减少0概率的出现。不过，如果太多的概率都是通过平滑的方式而得到的，那么模型和真实的数据分布之间的差距就会加大，最终预测的效果也会很差，所以平滑也不是解决0概率的最终办法。</p><p>除此之外，$P(w_{1}, w_{2}, …, w_{n})和P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$还不只会导致0概率，它还会使得模型存储空间的急速增加。</p><p>为了统计现有文档集合中$P(w_{1}, w_{2}, …, w_{n})$这类值，我们就需要生成很多的计数器。我们假设文档集合中有m个不同的单词，那么从中挑出$n$个单词的可重复排列，数量就是$m^{n}$。此外，还有$m^{n-1}$, $m^{n-2}$等等。这也意味着，如果要统计并存储的所有$P(w_{1}, w_{2}, …, w_{n})$或$P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$这类概率，就需要大量的内存和磁盘空间。当然，你可以做一些简化，不考虑单词出现的顺序，那么问题就变成了可重复组合，但是数量仍然非常巨大。</p><p>如何解决0概率和高复杂度的问题呢？马尔科夫假设和多元文法模型能帮上大忙了。如果我们使用三元文法模型，上述公式可以改写为：</p><p><img src=\"https://static001.geekbang.org/resource/image/6d/b1/6dfe1f3c267787677090a611fee964b1.png?wh=651*38\" alt=\"\"></p><p>这样，系统的复杂度大致在(C(m, 1) + C(m, 2) + C(m, 3))这个数量级，而且$P(w_{n}|w_{n-2}, w_{n-1})$为0的概率也会大大低于$P(w_{n}|w_{1}, w_{2}, …, w_{n-1})$ （其中$n&gt;&gt;3$）为0的概率。当然，多元文法模型中的N还是不能太大。随着N的增大，系统复杂度仍然会快速升高，就无法体现出多元文法的优势了。</p><h2>语言模型的应用</h2><p>基于概率的语言模型，本身不是新兴的技术。它已经在机器翻译、语音识别和中文分词中得到了成功应用。近几年来，人们也开始在信息检索领域中尝试语言模型。下面我就来讲讲语言模型在信息检索和中文分词这两个方面是如何发挥作用的。</p><h3>1.信息检索</h3><p>信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？为了解决相关性问题，布尔模型和向量空间检索模型都是从查询的角度出发，观察查询和文档之间的相似程度，并以此来决定如何找出相关的文档。这里的“相似程度”，你可以理解为两者长得有多像。那么，语言模型如何来刻画查询和文档之间的相关度呢？</p><p>它不再使用相似度定义，而是采用了概率。一种常见的做法是计算$P(d|q)$，其中$q$表示一个查询，$d$表示一篇文档。$P(d|q)$表示用户输入查询$q$的情况下，文档$d$出现的概率是多少？如果这个概率越高，我们就认为$q$和$d$之间的相关性越高。</p><p>通过我们手头的文档集合，并不能直接获得$P(d|q)$。好在我们已经学习过了贝叶斯定理，通过这个定理，我们可以将$P(d|q)$重写如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/de/67/de8d794df4e2758063f5ffb26546c467.png?wh=197*66\" alt=\"\"></p><p>对于同一个查询，其出现概率$P(q)$都是相同的，同一个文档$d$的出现概率$P(d)$也是固定的。因此它们可以忽略，我们只要关注如何计算$P(q|d)$。而语言模型，为我们解决了如何计算$P(q|d)$的问题，让$k_{1}, k_{2}, …, k_{n}$表示查询q里包含的$n$个关键词。那么根据之前的链式法则公式，可以重写为这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/d7/66/d78ba0db38a575269eff4f6cb89afa66.png?wh=735*54\" alt=\"\"></p><p>为了提升效率，我们也使用马尔科夫假设和多元文法。假设是三元文法，那么我们可以写成这样：</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/13/e977fb4b682e67a8693f1b76f1febe13.png?wh=712*52\" alt=\"\"></p><p>最终，当用户输入一个查询$q$之后，对于每一篇文档$d$，我们都能获得$P(d|q)$的值。根据每篇文档所获得的$P(d|q)$这个值，由高到低对所有的文档进行排序。这就是语言模型在信息检索中的常见用法。</p><h3>2.中文分词</h3><p>和拉丁语系不同，中文存在分词的问题。如果想进行分词，你就可以使用语言模型。我举个例子给你解释一下，你就明白了。</p><p>最普遍的分词方法之一是基于常用词的词典。如果一个尚未分词的句子里发现了存在于字典里的词，我们就认为找到一个新的词，并把它切分出来。这种切分不会出现完全离谱的结果，但是无法解决某些歧义。我下面来举个例子，原句是“兵乓球拍卖完了”。我在读的时候，会有所停顿，你就能理解分词应该如何进行。可是，仅仅从书面来看，至少有以下几种分词方式：</p><pre><code>第一种，兵乓|球|拍卖|完了\n第二种，兵乓球|拍卖|完了\n第三种，兵乓|球拍|卖完|了\n第四种，兵乓|球拍|卖|完了\n</code></pre><p>上面分词的例子，从字面来看都是合理的，所以这种歧义无法通过这句话本身来解决。那么这种情况下，语言模型能为我们做什么呢？我们知道，语言模型是基于大量的语料来统计的，所以我们可以使用这个模型来估算，哪种情况更合理。</p><p>假设整个文档集合是D，要分词的句子是$s$，分词结果为$w_{1}$, … $w_{n}$，那么我们可以求$P(s)$的概率为：</p><p><img src=\"https://static001.geekbang.org/resource/image/c7/34/c74b222e54b6c2a9a6385a988896f334.png?wh=809*47\" alt=\"\"></p><p>请注意，在信息检索中，我们关心的是每篇文章产生一个句子（也就是查询）的概率，而这里可以是整个文档集合D产生一个句子的概率。</p><p>根据链式法则和三元文法模型，那么上面的式子可以重写为：</p><p><img src=\"https://static001.geekbang.org/resource/image/d4/be/d4cfe338d5aed842cd1b366008a551be.png?wh=784*44\" alt=\"\"></p><p>也就是说，语言模型可以帮我们估计某种分词结果，在文档集合中出现的概率。但是由于不同的分词方法，会导致$w_{1}$到$w_{n}$的不同，因此就会产生不同的$P(s)$。接下来，我们只要取最大的$P(s)$，并假设这种分词方式是最合理的，就可以在一定程度上解决歧义。我们可以使用这个公式来求解：</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/dc/1cfeaeb018f38f86c448b57b61525adc.png?wh=152*42\" alt=\"\"></p><p>其中，$W_{i}$表示第$i$种分词方法。</p><p>回到“兵乓球拍卖完了”这句话，如果文档集合都是讲述的有关体育用品的销售，而不是拍卖行，那么“兵乓|球拍|卖完|了”这种分词的可能性应该更高。</p><h2>小结</h2><p>这一节，我介绍了基于概率论的语言模型，以及它在信息检索和中文分词领域中的应用。这一节的公式比较多，你刚开始看可能觉得有点犯晕。不用急，我给你梳理了几个要点，你只要掌握这几个要点，依次再进行细节学习，就会事半功倍。</p><p>第一，使用联合概率，条件概率和边缘概率的“三角”关系，进行相互推导。链式法则就是很好的体现。</p><p>第二，使用马尔科夫假设，把受较多随机变量影响的条件概率，简化为受较少随机变量影响的条件概率，甚至是边缘概率。</p><p>第三，使用贝叶斯定理，通过先验概率推导后验概率。在信息检索中，给定查询的情况下推导文档的概率，就需要用到这个定理。</p><p>如果你记住了这几点，那么不仅能很快地理解本篇的内容，还能根据实际需求，设计出满足自己需要的语言模型。</p><h2>思考题</h2><p>在中文分词的时候，我们也可以考虑文章的分类。比如，这样一句话“使用纯净水源浇灌的大米”，正确的切分应该是：</p><pre><code>使用|纯净|水源|浇灌|的|大米\n</code></pre><p>如果我们知道这句描述来自于“大米”类商品，而不是“纯净水”类商品，那么就不会错误地切分为：</p><pre><code>使用|纯净水|源|浇灌|的|大米\n</code></pre><p>想想看，如何对我介绍的语言模型加以改进，把分类信息也包含进去？</p><p><span class=\"orange\">欢迎留言和我分享，也欢迎你在留言区写下今天的学习笔记。你可以点击“请朋友读”，把今天的内容分享给你的好友，和他一起精进。</span></p>","neighbors":{"left":{"article_title":"23 | 文本分类：如何区分特定类型的新闻？","id":81009},"right":{"article_title":"25 | 马尔科夫模型：从PageRank到语音识别，背后是什么模型在支撑？","id":81374}},"comments":[{"had_liked":false,"id":84964,"user_name":"枫林火山","can_delete":false,"product_type":"c1","uid":1199855,"ip_address":"","ucode":"66099C9D1AD36C","user_header":"https://static001.geekbang.org/account/avatar/00/12/4e/ef/2ad3effd.jpg","comment_is_top":false,"comment_ctime":1554949857,"is_pvip":true,"replies":[{"id":"30597","content":"联合概率是不考虑顺序的，而N元文法一般都是要考虑一点顺序的。所谓“一点”就如你所提到的，这是一个条件概率P(xn|xn-2,xn-1)，顺序是指xn-2和xn-1都是在xn的前面出现，但是我们并不关心xn-2和xn-1之间的顺序。而另一方面，我们之前已经考虑了P(xn-1|xn-2, xn-3)，你可以认为xn-1和xn-2之间的关系已经在这一步考虑了。<br>至于你说的最后一点，P(x1,x2,x3,x4….xn) 和P(xn,xn-1,xn-2,….,x4,x3,x2,x1)理论上应该是一致的。但是n稍微大点，我们就无法直接求了，所以要使用马尔科夫假设进行近似。而马尔科夫假在一定程度上考虑了文本出现的顺序，所以不同顺序的x1，x2...xn就会影响近似的结果，所以有P(x1,x2,x3,x4….xn)约等于近似结果a，P(xn,xn-1,xn-2,….,x4,x3,x2,x1)约等于近似结果b，a和b都是同一个理论值的近似，但是由于马尔科夫假设的原因，两个近似值不一致。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1554999553,"ip_address":"","comment_id":84964,"utype":1}],"discussion_count":3,"race_medal":0,"score":"87454295777","product_id":100021201,"comment_content":"黄老师，一直没想明白多元文法里的前面N个词的是否有顺序。例如: 大家好， 家大好 。 这2种情况都符合三元文法中的P(xn|xn-2,xn-1)的统计条件吗？<br>推广下 P(x1,x2,x3,x4….xn) 等于 P(xn,xn-1,xn-2,….,x4,x3,x2,x1) 吗？<br>百度-联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。我的理解联合概率的条件是可以交换顺序的。<br>所以从联合概率定义的角度理解是等于的， 但是从语法模型的角度理解，语法是有顺序的，那用联合概率表示对不对。<br>本节讲的语法模型是把一句话当作有序队列去对待的还是无序集合对待的？ 我听您的讲解是感觉是有序的，但是我理解公式从联合概率定义又感觉公式是在说一个无序的一组条件。 没法把这两者联系起来。<br>以下两组公式，我只能知道当使用一元文法时，二者时相等的。二元以上有点懵！老师能不能讲解下<br>P(x1,x2,x3,x4….xn) = P(x1)*P(x2|x1)*P(x3|x1,x2)*…….*P(xn|x1,x2,x3,x4….xn-1) <br>P(xn,xn-1,xn-2,….,x4,x3,x2,x1) = P(xn)*P(xn-1|xn)*P(xn-2|xn,xn-1)*…….*P(x1|xn,xn-1,……,x2) ","like_count":20,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":446511,"discussion_content":"联合概率是不考虑顺序的，而N元文法一般都是要考虑一点顺序的。所谓“一点”就如你所提到的，这是一个条件概率P(xn|xn-2,xn-1)，顺序是指xn-2和xn-1都是在xn的前面出现，但是我们并不关心xn-2和xn-1之间的顺序。而另一方面，我们之前已经考虑了P(xn-1|xn-2, xn-3)，你可以认为xn-1和xn-2之间的关系已经在这一步考虑了。\n至于你说的最后一点，P(x1,x2,x3,x4….xn) 和P(xn,xn-1,xn-2,….,x4,x3,x2,x1)理论上应该是一致的。但是n稍微大点，我们就无法直接求了，所以要使用马尔科夫假设进行近似。而马尔科夫假在一定程度上考虑了文本出现的顺序，所以不同顺序的x1，x2...xn就会影响近似的结果，所以有P(x1,x2,x3,x4….xn)约等于近似结果a，P(xn,xn-1,xn-2,….,x4,x3,x2,x1)约等于近似结果b，a和b都是同一个理论值的近似，但是由于马尔科夫假设的原因，两个近似值不一致。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1554999553,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1504736,"avatar":"https://static001.geekbang.org/account/avatar/00/16/f5/e0/76822dd9.jpg","nickname":"南边","note":"","ucode":"7967C01AAB4A70","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":64295,"discussion_content":"简单理解一下，由于马尔科夫假设，砍掉了一些联合概率的元素，如果不讲顺序，那砍掉的元素都是随机的，就无法预测的比较准确了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1574944516,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1044624,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/f0/90/31600c55.jpg","nickname":"childback","note":"","ucode":"19DCD2322FAC56","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":1504,"discussion_content":"条件概率P(xn|xn-2,xn-1) ，这个和顺序无关吧。应该是说，xn-2,xn-1同时出现的情况下，xn出现的概率，至于xn，xn-2，xn-1的出现顺序应该没有关系吧。这点不明白，请老师指教。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1562667646,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65702,"user_name":"","can_delete":false,"product_type":"c1","uid":1031328,"ip_address":"","ucode":"1B73BA45ACD06C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bc/a0/97c7679b.jpg","comment_is_top":false,"comment_ctime":1549595376,"is_pvip":false,"replies":[{"id":"23260","content":"这个问题很好，确实中文比较特殊，和拉丁文不太一样。<br><br>我觉得你的问题是：中文里的歧义或者分词错误，是不是会影响分类？<br>你说的这几种情况，我简单分为以下几种：<br><br>分词：如果我们能知道123或321代表一个字符串，而不是单个的数字，那么就不会切分它们。再例如“相互”也不会切为“相”和“互”。当然中文分词本身不是件容易的事情，我这里提到概率语言模型，如果语料里有相关的信息，那么可以在一定程度上提升分词效果。<br><br>同义词：如果我们能正确切分出“相互”和“互相”，那么还需要把它们关联为同义词。基本的做法是使用词典。<br><br>语义：“纳税”的问题就更复杂一些，需要计算机理解上下文关系和语义。从统计的角度而言，那还是要看语料里“纳税”这个词哪种情况的概率更高。<br><br>所以，自然语言处理，尤其是中文的处理，是件相当复杂也是相当有趣的事情。“词包”模型只是最基本的模型，如果我们想优化它在分类问题上的表现，需要解决好中文分词、消除歧义、同义词&#47;近义词等问题。每个问题都是值得研究，并且可以提升的。如果每个点都能得到优化，那么最终分类的效果也会得到优化。<br><br>总结一下，文本分类涉及的面很广，不仅受到分类算法的影响，还受到其他许多自然语言处理的影响。由于这个系列专栏的主题是数学，所以我这讲只能把概率和相关分类算的核心思想体现出来。如果你对自然语言处理有兴趣，我可以在加餐或者其他专栏中和你分享。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1549647490,"ip_address":"","comment_id":65702,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31614366448","product_id":100021201,"comment_content":"文本分类器，对给定文本进行判断。用特征词代表该文本。应该和上篇文章分类的计算有类似之处。计算每个特征词出现在该类文章的概率。然后根据权重分类？或者根据每个词的词频。<br>（我也很迷糊）那中文中有时词的顺序错乱也能表达一个意思。<br>比如，密码是123和321是密码；蹦迪坟头和坟头蹦迪。<br>比如（相互和互相；代替和替代）比<br>如，纳税可以是一个专有名词，也可以是人名，姓纳名税。还有那遇到多音字咋办？","like_count":7,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":438556,"discussion_content":"这个问题很好，确实中文比较特殊，和拉丁文不太一样。\n\n我觉得你的问题是：中文里的歧义或者分词错误，是不是会影响分类？\n你说的这几种情况，我简单分为以下几种：\n\n分词：如果我们能知道123或321代表一个字符串，而不是单个的数字，那么就不会切分它们。再例如“相互”也不会切为“相”和“互”。当然中文分词本身不是件容易的事情，我这里提到概率语言模型，如果语料里有相关的信息，那么可以在一定程度上提升分词效果。\n\n同义词：如果我们能正确切分出“相互”和“互相”，那么还需要把它们关联为同义词。基本的做法是使用词典。\n\n语义：“纳税”的问题就更复杂一些，需要计算机理解上下文关系和语义。从统计的角度而言，那还是要看语料里“纳税”这个词哪种情况的概率更高。\n\n所以，自然语言处理，尤其是中文的处理，是件相当复杂也是相当有趣的事情。“词包”模型只是最基本的模型，如果我们想优化它在分类问题上的表现，需要解决好中文分词、消除歧义、同义词/近义词等问题。每个问题都是值得研究，并且可以提升的。如果每个点都能得到优化，那么最终分类的效果也会得到优化。\n\n总结一下，文本分类涉及的面很广，不仅受到分类算法的影响，还受到其他许多自然语言处理的影响。由于这个系列专栏的主题是数学，所以我这讲只能把概率和相关分类算的核心思想体现出来。如果你对自然语言处理有兴趣，我可以在加餐或者其他专栏中和你分享。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1549647490,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72790,"user_name":"qinggeouye","can_delete":false,"product_type":"c1","uid":1251536,"ip_address":"","ucode":"5B53EEDD7BEC9C","user_header":"https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg","comment_is_top":false,"comment_ctime":1551717379,"is_pvip":false,"replies":[{"id":"26568","content":"这是一种方法👌","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1551811187,"ip_address":"","comment_id":72790,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23026553859","product_id":100021201,"comment_content":"思考题：<br><br>首先，利用语言模型进行中文分词，计算句子 S=使用纯净水源浇灌的大米，属于哪种分词结果 W（“使用|纯净|水源|浇灌|的|大米”、“使用|纯净水|源|浇灌|的|大米”）的概率最大？<br><br>然后，回到上节的文本分类，再计算 分词结果W 属于哪种分类（“大米”、“纯净水”）的概率比较大？","like_count":6,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":441746,"discussion_content":"这是一种方法👌","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551811187,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70749,"user_name":"唯她命","can_delete":false,"product_type":"c1","uid":1240398,"ip_address":"","ucode":"8F687E8D306840","user_header":"https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg","comment_is_top":false,"comment_ctime":1551174217,"is_pvip":false,"replies":[{"id":"25374","content":"在实际项目中，可以通过大量的语料来统计，比如文档d中，在k1后面出现k2的次数，除以k1出现的次数，用来近似P(k2|k1,d)","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1551202986,"ip_address":"","comment_id":70749,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23026010697","product_id":100021201,"comment_content":"已经求得p(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) ....<br>那么我们怎么求得 p(k2|k1,d) 和  p(k3|k2,k1,d)呢","like_count":5,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440808,"discussion_content":"在实际项目中，可以通过大量的语料来统计，比如文档d中，在k1后面出现k2的次数，除以k1出现的次数，用来近似P(k2|k1,d)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551202986,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":83020,"user_name":"OP_未央","can_delete":false,"product_type":"c1","uid":1036902,"ip_address":"","ucode":"571C9E0C4E84CD","user_header":"https://static001.geekbang.org/account/avatar/00/0f/d2/66/811970de.jpg","comment_is_top":false,"comment_ctime":1554374499,"is_pvip":true,"replies":[{"id":"29989","content":"嗯，是的。可以对不同分类构建分类器，或者增加条件概率","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1554396083,"ip_address":"","comment_id":83020,"utype":1}],"discussion_count":2,"race_medal":0,"score":"18734243683","product_id":100021201,"comment_content":"思考题：<br>可以增加类别的先验概率，P(w1,w2...wn|C)*P(C)；或者已知大米广告的条件下，通过得到的不同分词计算所属类别的概率，选择属于大米概率大的那种分词？","like_count":4,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":445822,"discussion_content":"嗯，是的。可以对不同分类构建分类器，或者增加条件概率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1554396083,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1336475,"avatar":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","nickname":"J.Smile","note":"","ucode":"C4D98DFDBF7584","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":186778,"discussion_content":"这种方式还是朴素贝叶斯算法的应用吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1582713199,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67782,"user_name":"seleven","can_delete":false,"product_type":"c1","uid":1338701,"ip_address":"","ucode":"B4D84E6B930448","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/SsJajvXghPMDicSuOcx54mV6L9zv4KSKM2bKY0gsUdAH3oGCWzfRv9Q9HRljic2IvHzYFpRECp8SXGWhiaqGWFTKg/132","comment_is_top":false,"comment_ctime":1550273969,"is_pvip":false,"replies":[{"id":"24119","content":"如果是词包模型，确实对语义没有太多的理解。可以加入一些基于文法甚至是领域知识的语义分析，不过这个和具体的应用有关系，可能不是语法模型本身能很好解决的。例如，评论中的情感分析（sentiment analysis），我们可以考虑表达情感的词在否定句式中的表达等等。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1550422624,"ip_address":"","comment_id":67782,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14435175857","product_id":100021201,"comment_content":"换句话说：其实我是想问，如何能更好的利用全文或者说全部训练集的语义信息？","like_count":3,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439372,"discussion_content":"如果是词包模型，确实对语义没有太多的理解。可以加入一些基于文法甚至是领域知识的语义分析，不过这个和具体的应用有关系，可能不是语法模型本身能很好解决的。例如，评论中的情感分析（sentiment analysis），我们可以考虑表达情感的词在否定句式中的表达等等。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550422624,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":67508,"user_name":"seleven","can_delete":false,"product_type":"c1","uid":1338701,"ip_address":"","ucode":"B4D84E6B930448","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/SsJajvXghPMDicSuOcx54mV6L9zv4KSKM2bKY0gsUdAH3oGCWzfRv9Q9HRljic2IvHzYFpRECp8SXGWhiaqGWFTKg/132","comment_is_top":false,"comment_ctime":1550188606,"is_pvip":false,"replies":[{"id":"23974","content":"我想你所说的是针对分类问题对吧？那么语料不充足，是指只有少数的标注数据（文章），是吧？如果是这样，一种做法是增大ngram里面的n，因为标注数据不多，增加n不会增加太多的存储空间。另外，也可以使用少量数据训练得到的分类器，对新的数据进行分类，然后获得一些分类结果后，人工再进行复查，把正确的结果再次纳入训练数据。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1550250683,"ip_address":"","comment_id":67508,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14435090494","product_id":100021201,"comment_content":"这篇文章和上篇文章中介绍的分类基本都是在假定文章中的词语相互独立的情况下进行的，虽然马尔科夫模型用到前面词的概率，这可以说是结合了部分上下文之间的语义关系，只使用了和前面词的语义关系，对文本分类其实效果已经很好，尤其是在语料充足的情况下可以使用机器学习的方法让分类模型自迭代优化，目前很多机器翻译就是这么干的。但是我想问下老师，针对语料不是很充足的文本集，如何使用全篇文章的语义（不只是前面词的上下文关系）来设计一个文本分类器？","like_count":3,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":439223,"discussion_content":"我想你所说的是针对分类问题对吧？那么语料不充足，是指只有少数的标注数据（文章），是吧？如果是这样，一种做法是增大ngram里面的n，因为标注数据不多，增加n不会增加太多的存储空间。另外，也可以使用少量数据训练得到的分类器，对新的数据进行分类，然后获得一些分类结果后，人工再进行复查，把正确的结果再次纳入训练数据。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1550250683,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":222520,"user_name":"建强","can_delete":false,"product_type":"c1","uid":1397126,"ip_address":"","ucode":"62B03D0E0C64EC","user_header":"https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg","comment_is_top":false,"comment_ctime":1590826937,"is_pvip":false,"replies":[{"id":"82234","content":"很好的思路","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1591026247,"ip_address":"","comment_id":222520,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10180761529","product_id":100021201,"comment_content":"思考题：<br>考虑分类信息的分词模式，只要把分词概率计算公式中的文档集合D，改为某一分类的文档集合T，即公式改为：arg maxP(Wi|T)，在某个文档类别下，取分词概率最大的那种切分方式，具体计算时，应在同类别下的所有文档中去计算每个分词出现的概率(即分词出现的频度)。","like_count":2,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":496827,"discussion_content":"很好的思路","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1591026247,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":231754,"user_name":"A君","can_delete":false,"product_type":"c1","uid":1940105,"ip_address":"","ucode":"FE96F089C2312C","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg","comment_is_top":false,"comment_ctime":1593765112,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5888732408","product_id":100021201,"comment_content":"👍🏻让我茅塞顿开。通过贝叶斯公式的链式法则，将由n个词组成的联合概率展开成一系列条件概率，再通过马尔科夫假设Ngram来简化条件概率的复杂性，从而计算出一篇文章中一个句子出现的概率，或者一个句子的所有分词的可能性，亦或者是在文库中输入一个关键字，最有可能检索到的文章。","like_count":1},{"had_liked":false,"id":207308,"user_name":"罗耀龙@坐忘","can_delete":false,"product_type":"c1","uid":1917663,"ip_address":"","ucode":"3CEA258DE7F3C7","user_header":"https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg","comment_is_top":false,"comment_ctime":1587045258,"is_pvip":true,"replies":[{"id":"78390","content":"两种都可以尝试","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1587744674,"ip_address":"","comment_id":207308,"utype":1}],"discussion_count":1,"race_medal":5,"score":"5882012554","product_id":100021201,"comment_content":"茶艺师学编程<br><br>思考题：如何对语言模型加以改进，把分类信息也包含进去？<br><br>1、用另一纬度的信息来帮忙减少不确定性……我想到的是用另外一组信息表达式为y2，原来的为y1，求y1＊y2=0（正交），从而敲定正确分法。<br><br>2、链式法则公式中的文档D，直接替换成“全局函数”，即本段文字的语境，接下来就是计算在本语境下合适的分法是哪个?","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492079,"discussion_content":"两种都可以尝试","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587744674,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":182127,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1582713457,"is_pvip":false,"replies":[{"id":"70604","content":"是的","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1582778000,"ip_address":"","comment_id":182127,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5877680753","product_id":100021201,"comment_content":"思考题：朴素贝叶斯算法，计算哪种分词方法分类为大米或者纯净水的概率是最大的，比如分词方法A分为大米的概率最大，分词方法B分为纯净水的概率最大，那么根据实际文章分类情况选择不同的分词方法。","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485229,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1582778000,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131075,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1567634936,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5862602232","product_id":100021201,"comment_content":"思考题<br>原来中文分词的是一个句子在所有语料条件下成句的最大概率分词方法。如果语料足够多，可以计数在特定文章分类下的条件概率，然后取最大条件概率的分词方法。","like_count":1},{"had_liked":false,"id":131074,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1567634756,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5862602052","product_id":100021201,"comment_content":"中文分词就是选用不同的分词，然后计算每个分词成句的概率大小来对分词作优劣判断。","like_count":1},{"had_liked":false,"id":70747,"user_name":"唯她命","can_delete":false,"product_type":"c1","uid":1240398,"ip_address":"","ucode":"8F687E8D306840","user_header":"https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg","comment_is_top":false,"comment_ctime":1551173759,"is_pvip":false,"replies":[{"id":"25373","content":"是第二种，在满足k1和d的条件下，出现k2的概率","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1551202885,"ip_address":"","comment_id":70747,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5846141055","product_id":100021201,"comment_content":"老师你好，p(K2 | k1,d) 指的是K2 | k1  和   d的联合概率<br>还是指的是 在满足k1和d的条件下，出现k2的概率？","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440807,"discussion_content":"是第二种，在满足k1和d的条件下，出现k2的概率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551202885,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":70704,"user_name":"唯她命","can_delete":false,"product_type":"c1","uid":1240398,"ip_address":"","ucode":"8F687E8D306840","user_header":"https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg","comment_is_top":false,"comment_ctime":1551167432,"is_pvip":false,"replies":[{"id":"25372","content":"如果是一元文法，词包模型，确实不需要一模一样。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1551202851,"ip_address":"","comment_id":70704,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5846134728","product_id":100021201,"comment_content":"老师，$P(w_{1}, w_{2}, …, w_{n})$ 要在集合中找到一模一样的句子，基本是不可能的<br><br>不一定要找到一模一样的句子吧   例如abc   难道cab 这种打乱顺序的不行吗？","like_count":1,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440784,"discussion_content":"如果是一元文法，词包模型，确实不需要一模一样。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551202851,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":352894,"user_name":"013923","can_delete":false,"product_type":"c1","uid":3035193,"ip_address":"美国","ucode":"1214DAADBCA848","user_header":"","comment_is_top":false,"comment_ctime":1658997555,"is_pvip":false,"replies":[{"id":"129190","content":"希望对你有所帮助","user_name":"作者回复","user_name_real":"编辑","uid":"1275061","ctime":1661057441,"ip_address":"美国","comment_id":352894,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1658997555","product_id":100021201,"comment_content":"学习了！谢谢！","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":584715,"discussion_content":"希望对你有所帮助","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1661057441,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"美国"},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":273306,"user_name":"A君","can_delete":false,"product_type":"c1","uid":1940105,"ip_address":"","ucode":"FE96F089C2312C","user_header":"https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg","comment_is_top":false,"comment_ctime":1610530264,"is_pvip":true,"replies":[{"id":"99186","content":"没错👍","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1610739822,"ip_address":"","comment_id":273306,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1610530264","product_id":100021201,"comment_content":"链式法则将联合概率转成了一系列条件概率相乘，而马尔科夫假设简化了多条件概率的计算复杂度，两者结合，就可以计算一个查询、一条文本在数据集&#47;文档中出现的概率，不仅可以计算查询和文档的匹配度，还可以用于进行中文分词，根据数据集特性来对文本进行分词。","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":513562,"discussion_content":"没错👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1610739822,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":263366,"user_name":"鼠里鼠气","can_delete":false,"product_type":"c1","uid":1696883,"ip_address":"","ucode":"4793476BBDB202","user_header":"https://static001.geekbang.org/account/avatar/00/19/e4/73/74dce191.jpg","comment_is_top":false,"comment_ctime":1606117808,"is_pvip":false,"replies":[{"id":"96735","content":"这个是二元文法的定义，它研究的是在前一个单词出现的情况下，后一个单词出现的概率，所以是条件概率。举个例子，我们统计了大量的文本，发现在“我”这个词后面，出现“是”的概率为0.1，出现“爱”的概率是0.05等等，那么P(是|我) = 0.1, P(爱|我)=0.05","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1607281061,"ip_address":"","comment_id":263366,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1606117808","product_id":100021201,"comment_content":"“二元文法表示，某个单词出现的概率只和它前面的 1 个单词有关。”  这里的“某个单词出现的概率”为什么是条件概率呐？怎么看的?老师","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":510164,"discussion_content":"这个是二元文法的定义，它研究的是在前一个单词出现的情况下，后一个单词出现的概率，所以是条件概率。举个例子，我们统计了大量的文本，发现在“我”这个词后面，出现“是”的概率为0.1，出现“爱”的概率是0.05等等，那么P(是|我) = 0.1, P(爱|我)=0.05","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607281061,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":203784,"user_name":"本来是亚","can_delete":false,"product_type":"c1","uid":1073592,"ip_address":"","ucode":"14527E5BC3C2A8","user_header":"https://static001.geekbang.org/account/avatar/00/10/61/b8/7b23f8cb.jpg","comment_is_top":false,"comment_ctime":1586266148,"is_pvip":true,"replies":[{"id":"76429","content":"你说的更像是如何利用分词来帮助自动分类的任务，这里咱们说如何利用分类信息来提升分词效果","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1586401997,"ip_address":"","comment_id":203784,"utype":1}],"discussion_count":1,"race_medal":1,"score":"1586266148","product_id":100021201,"comment_content":"如果将分类的信息加入，则整个过程可以看作两个阶段：<br>1、中文分词。通过语料库计算各种分词序列的概率<br>2、分词后，计算词频，从而计算使得对应类最大概率的分词方式。","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":490959,"discussion_content":"你说的更像是如何利用分词来帮助自动分类的任务，这里咱们说如何利用分类信息来提升分词效果","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586401997,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":182123,"user_name":"J.Smile","can_delete":false,"product_type":"c1","uid":1336475,"ip_address":"","ucode":"C4D98DFDBF7584","user_header":"https://static001.geekbang.org/account/avatar/00/14/64/9b/0b578b08.jpg","comment_is_top":false,"comment_ctime":1582712831,"is_pvip":false,"replies":[{"id":"71551","content":"是的，这两个是经典的应用场景","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1583555348,"ip_address":"","comment_id":182123,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1582712831","product_id":100021201,"comment_content":"要点总结：<br>利用马尔科夫假设近似和多元文法-N-Gram模型原理进行两个工程中的应用：<br>①信息检索 ②中文分词","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485227,"discussion_content":"是的，这两个是经典的应用场景","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583555348,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":163694,"user_name":"总统老唐","can_delete":false,"product_type":"c1","uid":1490070,"ip_address":"","ucode":"F2CC66E5BB4871","user_header":"https://static001.geekbang.org/account/avatar/00/16/bc/96/c679bb3d.jpg","comment_is_top":false,"comment_ctime":1576768462,"is_pvip":false,"replies":[{"id":"62461","content":"P(q|d)可以通过原有的文档集合来统计，看看每篇文字中某个单词出现几次，然后用次数来计算概率","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1576993551,"ip_address":"","comment_id":163694,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1576768462","product_id":100021201,"comment_content":"黄老师，关于信息检索，若令 p(d | q) 表示，接受到输入 q 这个查询时，用户期望检索的是文章 d 的概率。当我们针对某个具体查询，求出文章库中每一篇文章的概率，所有概率中最大值对应的文章就是用户最想检索的文章，若将 d 称为 q 的 “最相关文章”。那 p(q | d)的含义是否该这样描述，q1，q2，q3这三个查询的最相关文章都是d，现在已知针对某个查询，输出结果为d，那这个查询是q1的概率为 p(q1 | d)","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":478509,"discussion_content":"P(q|d)可以通过原有的文档集合来统计，看看每篇文字中某个单词出现几次，然后用次数来计算概率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1576993551,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":163089,"user_name":"teddytyy","can_delete":false,"product_type":"c1","uid":1268738,"ip_address":"","ucode":"E1569D81A4154E","user_header":"https://static001.geekbang.org/account/avatar/00/13/5c/02/e7af1750.jpg","comment_is_top":false,"comment_ctime":1576654985,"is_pvip":false,"replies":[{"id":"62091","content":"在实现的过程，可以直接忽略，直接比较其他的计算结果。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1576696921,"ip_address":"","comment_id":163089,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1576654985","product_id":100021201,"comment_content":"“对于同一个查询，其出现概率 P(q) 都是相同的，同一个文档 d 的出现概率 P(d) 也是固定的。因此它们可以忽略。“对于这个描述，具体的计算时如何忽略呢？","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":478243,"discussion_content":"在实现的过程，可以直接忽略，直接比较其他的计算结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1576696921,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":143632,"user_name":"炎发灼眼","can_delete":false,"product_type":"c1","uid":1425953,"ip_address":"","ucode":"7D195057C8E94C","user_header":"https://static001.geekbang.org/account/avatar/00/15/c2/21/a8ef82ac.jpg","comment_is_top":false,"comment_ctime":1571748739,"is_pvip":false,"replies":[{"id":"55475","content":"你是指p(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d)？<br><br>先看链式法则，p(k1,k2,k3,...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) * ... * p(kn|kn-1,kn-2,...,k2,k1,d)，注意这里公式的成立是基于贝叶斯法则的，你可以用贝叶斯法则验证。<br><br>然后，通过马尔科夫假设，当前词只和前若干词相关，这里假设前两个，那么上式子可以简化为<br>p(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) * p(k4|k3,k2,d) * ... * p(kn|kn-1,kn-2,d)","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1571763438,"ip_address":"","comment_id":143632,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1571748739","product_id":100021201,"comment_content":"老师你好，文中P(q|d)通过链式法则推导的公式没看懂，联合概率的推导可以看懂，这种带了d的条件概率的推导没有看懂，望老师能给予解释。<br>","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":471626,"discussion_content":"你是指p(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d)？\n\n先看链式法则，p(k1,k2,k3,...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) * ... * p(kn|kn-1,kn-2,...,k2,k1,d)，注意这里公式的成立是基于贝叶斯法则的，你可以用贝叶斯法则验证。\n\n然后，通过马尔科夫假设，当前词只和前若干词相关，这里假设前两个，那么上式子可以简化为\np(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) * p(k4|k3,k2,d) * ... * p(kn|kn-1,kn-2,d)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1571763438,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2796578,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/ac/22/ae7ea261.jpg","nickname":"希","note":"","ucode":"FD5C04170A05B6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":564948,"discussion_content":"P(k1,k2,k3,...kn|d) = P(k1,k2,k3,...kn,d)/P(d) = P(d)*P(k1|d)*P(k2|k1,d)*...*P(kn|k1,k2,...,kn-1,d)/P(d) = P(k1|d)*P(k2|k1,d)*...*P(kn|k1,k2,...,kn-1,d)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650357971,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131073,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1567634546,"is_pvip":false,"replies":[{"id":"50122","content":"查询里的关键词有时候是存在一定的先后顺序，比如“相机镜头”和“带镜头的相机”，不过确实很多查询里的关键词都可以用词袋模型处理。当然，要精细化的提升查询的准确率，词袋就不够了。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1567795377,"ip_address":"","comment_id":131073,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1567634546","product_id":100021201,"comment_content":"信息检索是建立文章和查询的条件概率来判断查询和文章的紧密关系，这里查询当中一句句子来处理，然后用近似方法计算其在文中的概率，请问老师，查询里每个关键词的前后顺序和相邻都没什么关系，但是马尔可夫假设用到了前后和相邻关系，这样会不会造成较大误差？","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466274,"discussion_content":"查询里的关键词有时候是存在一定的先后顺序，比如“相机镜头”和“带镜头的相机”，不过确实很多查询里的关键词都可以用词袋模型处理。当然，要精细化的提升查询的准确率，词袋就不够了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567795377,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131072,"user_name":"Paul Shan","can_delete":false,"product_type":"c1","uid":1593140,"ip_address":"","ucode":"32D99989028284","user_header":"","comment_is_top":false,"comment_ctime":1567633990,"is_pvip":false,"replies":[{"id":"50121","content":"是的，链式法则是等价的，马尔科夫假设用于简化","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1567795244,"ip_address":"","comment_id":131072,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1567633990","product_id":100021201,"comment_content":"链式法则和马尔可夫假设可以大大简化计算联合概率的复杂度。链式法则在数学上是等价。马尔可夫假设是不是基于这样的事实，文章中的单词和该单词前n个单词的关系较为紧密，但是和其他单词没什么关系，从另一个角度看，一个单词和紧接着的单词也有紧密关系，但这一层关系在马尔可夫假设里由后面的单词处理，请问老师是不是这样，多谢。","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":466273,"discussion_content":"是的，链式法则是等价的，马尔科夫假设用于简化","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567795244,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":124345,"user_name":"楼外楼","can_delete":false,"product_type":"c1","uid":1243180,"ip_address":"","ucode":"791F2CA15B8F1F","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/sgEfkeMSIIibeH4l0HS8uwM6PGY3DSHoW5tV9l1hDQ06tr3OnI7F545Wdxsh59rqOKnzjLUpCcEqic3P9zZbKzPQ/132","comment_is_top":false,"comment_ctime":1565863684,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1565863684","product_id":100021201,"comment_content":"看完第二遍才搞清楚 P(x1,x2,...Xn)= P(x1)*P(x2|x1)*P(x3|x2)...   前提是: 各个条件都是独立的--参考一元文法；第二前提是: P(xn|x1,x2,...) = P(xn|xn-1,xn-2) ","like_count":0},{"had_liked":false,"id":115518,"user_name":"凝神寂照","can_delete":false,"product_type":"c1","uid":1583746,"ip_address":"","ucode":"E71DA5A0C39656","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLBemRWsNxJt0WmF3rswPMnsO2ibRKok3OiakicIsFTe8xEudicnytT7zv07E26LfMBG4ic0XsZ87WNPlA/132","comment_is_top":false,"comment_ctime":1563610344,"is_pvip":false,"replies":[{"id":"42282","content":"这里是假设每篇文章都出现了1&#47;N次，N表示文章总数","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1563734430,"ip_address":"","comment_id":115518,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1563610344","product_id":100021201,"comment_content":"老师您好，对于信息检索部分，计算 P(d|q)=P(q|d)P(d)&#47;P(q)来对不同文档进行排序，P(q)是固定的我能理解，但是对于不同的文档P（d）应该是不同的吧，您为什么说P（d）是固定的呢？","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":459141,"discussion_content":"这里是假设每篇文章都出现了1/N次，N表示文章总数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563734430,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":113686,"user_name":"李斌","can_delete":false,"product_type":"c1","uid":1007369,"ip_address":"","ucode":"73D1A97F746F69","user_header":"https://static001.geekbang.org/account/avatar/00/0f/5f/09/80484e2e.jpg","comment_is_top":false,"comment_ctime":1563118035,"is_pvip":false,"replies":[{"id":"41394","content":"一般基于概率的不用TF-IDF，这主要是因为概率论有自己的理论体系，概率的估算是使用最大似然，而非TF-IDF。","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1563119620,"ip_address":"","comment_id":113686,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1563118035","product_id":100021201,"comment_content":"查询文档相关性的部分，elasticsearch 里常用 TF-IDF 以及最新的 BM25，两者都用到了 IDF 信息，但是感觉基于概率的方法没有用到 IDF 的信息啊","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":458345,"discussion_content":"一般基于概率的不用TF-IDF，这主要是因为概率论有自己的理论体系，概率的估算是使用最大似然，而非TF-IDF。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563119620,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":100130,"user_name":"jennbetty","can_delete":false,"product_type":"c1","uid":1542316,"ip_address":"","ucode":"F456B32F9E1439","user_header":"","comment_is_top":false,"comment_ctime":1559454083,"is_pvip":false,"replies":[{"id":"36021","content":"你的意思是说，如何在三元文法的情况下，求解p(s|D)？基本还是一致的，只是把P(wn|D)改为P(wn|wn-1, wn-2, D)","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1559499052,"ip_address":"","comment_id":100130,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1559454083","product_id":100021201,"comment_content":"老师我不明白中文分词求出的三元文法P(s|D)是如何求出argmax(P(Wi|D))的呢？比如说P(w2|D), P(w3|D)是怎么求出来的呢","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452421,"discussion_content":"你的意思是说，如何在三元文法的情况下，求解p(s|D)？基本还是一致的，只是把P(wn|D)改为P(wn|wn-1, wn-2, D)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1559499052,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":85225,"user_name":"枫林火山","can_delete":false,"product_type":"c1","uid":1199855,"ip_address":"","ucode":"66099C9D1AD36C","user_header":"https://static001.geekbang.org/account/avatar/00/12/4e/ef/2ad3effd.jpg","comment_is_top":false,"comment_ctime":1555021584,"is_pvip":true,"replies":[{"id":"30648","content":"应该的，你这个问题很好，对其他人也有启发","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1555085098,"ip_address":"","comment_id":85225,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1555021584","product_id":100021201,"comment_content":"明白这个顺序在哪里体现了，谢谢老师的耐心讲解👍🏻","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":446598,"discussion_content":"应该的，你这个问题很好，对其他人也有启发","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1555085098,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":71094,"user_name":"唯她命","can_delete":false,"product_type":"c1","uid":1240398,"ip_address":"","ucode":"8F687E8D306840","user_header":"https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg","comment_is_top":false,"comment_ctime":1551263828,"is_pvip":false,"replies":[{"id":"25492","content":"是的👍 ","user_name":"作者回复","user_name_real":"黄申","uid":"1275061","ctime":1551291916,"ip_address":"","comment_id":71094,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1551263828","product_id":100021201,"comment_content":"老师啊，中文分词，同一个句子，我们是不是把每种可能得分词 的ps都算出来啊，然后所有的ps求最大值，也就是这种分词的概率最大，然后我们就选择这种分词方法","like_count":0,"discussions":[{"author":{"id":1275061,"avatar":"https://static001.geekbang.org/account/avatar/00/13/74/b5/b68e3740.jpg","nickname":"黄申","note":"","ucode":"EE9AC074A322FF","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":440987,"discussion_content":"是的👍 ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1551291916,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}