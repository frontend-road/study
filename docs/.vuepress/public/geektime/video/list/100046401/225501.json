{"id":225501,"title":"54 | 神经网络的构建：Memory","content":"<p><strong>课件和Demo地址</strong><br>\n<a href=\"https://gitee.com/geektime-geekbang/NLP\">https://gitee.com/geektime-geekbang/NLP</a></p>","comments":[{"had_liked":false,"id":207575,"user_name":"LambdaC","can_delete":false,"product_type":"c3","uid":1888899,"ip_address":"","ucode":"D89CDBD6C807D0","user_header":"https://static001.geekbang.org/account/avatar/00/1c/d2/83/177e0895.jpg","comment_is_top":false,"comment_ctime":1587111785,"is_pvip":false,"replies":null,"discussion_count":2,"race_medal":0,"score":2,"product_id":100046401,"comment_content":"这种分析论文数学原理的方法很好，再也不怕看一大堆公式了。另外想问下老师，能不能之后补充分享一些实践中attention设计的思想呢？我在用transformer的时候，有时会感到搞不清该用什么和什么作为q,k,t","like_count":4,"discussions":[{"author":{"id":1341502,"avatar":"https://static001.geekbang.org/account/avatar/00/14/78/3e/f60ea472.jpg","nickname":"grok","note":"","ucode":"4744AB3FA28FE2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635091,"discussion_content":"claude2.1--&gt;\n一般来说,query代表我们想要查询或聚焦的内容,key代表输入中的内容或上下文,value则包含我们想从中提取信息的representation。举个例子,在机器翻译任务中,query可能是当前生成的翻译内容,key和value来自编码器端的输入语句表示。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704405496,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1341502,"avatar":"https://static001.geekbang.org/account/avatar/00/14/78/3e/f60ea472.jpg","nickname":"grok","note":"","ucode":"4744AB3FA28FE2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":635090,"discussion_content":"gpt4--&gt;\n在Transformer模型中，Attention机制是核心组成部分，其基本思想是使模型能够关注到输入数据中的关键信息。Attention机制通常涉及三个主要组成部分：查询（Query, Q）、键（Key, K）和值（Value, V）。在实践中，这三个部分可以由输入数据的不同表示形式构成。\n\n1. **查询（Q）**：通常代表你想要寻找的目标信息，可以理解为一种搜索条件或问题。\n\n2. **键（K）**：代表用来与查询匹配的信息，可以理解为一组可能的答案或内容。\n\n3. **值（V）**：当查询与键匹配后，相关的值就是我们想要关注的信息，可以理解为匹配成功后提供的具体内容。\n\n在不同的应用场景中，Q、K、V的选择和构成可以有所不同。例如：\n\n- **在自然语言处理中**，如果我们使用Attention机制来捕捉句子中的上下文信息，Q、K、V可能都来自于同一个句子的不同部分。\n\n- **在图像处理中**，比如图像标注，Q可能是来自文本的查询（如一个问题），而K和V可能是来自图像的特征。\n\n关于Attention设计的实际应用，以下是一些思考方向：\n\n- **注意力的范围**：全局注意力（整个输入序列）还是局部注意力（输入序列的一部分）？\n\n- **多头注意力（Multi-head Attention）**：使用多个注意力机制并行处理，每个头关注输入的不同部分。\n\n- **自注意力（Self-Attention）**：Q、K、V均来自同一输入，用于模型内部的信息交流。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1704405307,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}