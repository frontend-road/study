{"id":731299,"title":"它只是一次添加一个词","content":"<p>ChatGPT 可以自动生成类似于人类书写的文本，这非常了不起，也非常令人意外。它是如何做到的呢？这为什么会奏效呢？我在这里将概述 ChatGPT 内部的工作方式，然后探讨为什么它能够如此出色地产生我们认为有意义的文本。必须在开头说明，我会重点关注宏观的工作方式，虽然也会提到一些工程细节，但不会深入探讨。［这里提到的本质不仅适用于 ChatGPT，也同样适用于当前的其他“大语言模型”（large language model，LLM）。］</p><p>首先需要解释，ChatGPT 从根本上始终要做的是，针对它得到的任何文本产生“合理的延续”。这里所说的“合理”是指，“人们在看到诸如数十亿个网页上的内容后，可能期待别人会这样写”。</p><p>假设我们手里的文本是“The best thing about AI is its ability to”（AI 最棒的地方在于它能）。想象一下浏览人类编写的数十亿页文本（比如在互联网上和电子书中），找到该文本的所有实例，然后看看接下来出现的是什么词，以及这些词出现的概率是多少。ChatGPT 实际上做了类似的事情，只不过它不是查看字面上的文本，而是寻找在某种程度上“意义匹配”的事物（稍后将解释）。</p><p>最终的结果是，它会列出随后可能出现的词及其出现的“概率”（按“概率”从高到低排列）。</p><!-- [[[read_end]]] --><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00060.jpeg\" alt=\"{%}\"></p><p>值得注意的是，当 ChatGPT 做一些事情，比如写一篇文章时，它实质上只是在一遍又一遍地询问“根据目前的文本，下一个词应该是什么”，并且每次都添加一个词。［正如我将要解释的那样，更准确地说，它是每次都添加一个“标记”（token），而标记可能只是词的一部分。这就是它有时可以“造词”的原因。］</p><p>好吧，它在每一步都会得到一个带概率的词列表。但它应该选择将哪一个词添加到正在写作的文章中呢？有人可能认为应该选择“排名最高”的词，即分配了最高“概率”的词。然而，这里出现了一点儿玄学<span class=\"comment-number\">1</span>的意味。出于某种原因—也许有一天能用科学解释—如果我们总是选择排名最高的词，通常会得到一篇非常“平淡”的文章，完全显示不出任何“创造力”（有时甚至会一字不差地重复前文。但是，如果有时（随机）选择排名较低的词，就会得到一篇“更有趣”的文章。</p><p>这里存在随机性意味着，如果我们多次使用相同的提示（prompt），每次都有可能得到不同的文章。而且，符合玄学思想的是，有一个所谓的“温度”参数来确定低排名词的使用频率。对于文章生成来说，“温度”为 0.8 似乎最好。（值得强调的是，这里没有使用任何“理论”，“温度”参数只是在实践中被发现有效的一种方法。例如，之所以采用“温度”的概念，是因为碰巧使用了在统计物理学中很常见的某种指数分布<span class=\"comment-number\">2</span>，但它与物理学之间并没有任何实际联系，至少就我们目前所知是这样的。）</p><p>在进入下一节之前，需要解释一下，为了方便阐述，我在大多数情况下不会使用 ChatGPT 中的完整系统，而是使用更简单的 GPT-2 系统，它的优点是足够小，可以在标准的台式计算机上运行。因此，对于书中展示的所有原理，我都能附上明确的 Wolfram 语言代码，你可以立即在自己的计算机上运行。</p><p>例如，通过以下方式可以获得前页列出的概率表。首先，需要检索底层的“语言模型”。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00061.jpeg\" alt=\"{%}\"></p><p>稍后，我们将深入了解这个神经网络，并谈谈它的工作原理。现在，我们可以把这个“网络模型”当作黑盒，应用到之前的文本中，并询问模型哪 5 个词紧随其后的概率最高。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00062.jpeg\" alt=\"{%}\"></p><p>如下获取结果并将其转换为明确格式化的“数据集”。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00063.jpeg\" alt=\"{%}\"></p><p>如果反复“应用模型”，在每一步都添加概率最高的词［在此代码中指定为模型所做的“决策”（decision）］，则会发生以下情况。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00064.jpeg\" alt=\"{%}\"></p><p>如果继续下去呢？在此（“零温度”<span class=\"comment-number\">3</span>）情况下，文本很快就会变得混乱和重复。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00065.jpeg\" alt=\"{%}\"></p><p>但是，如果我们不总是选择“排名最高”的词，而是有时随机选择“非排名最高”的词（通过将“温度”参数从 0 调高到 0.8 来获得这种随机性）呢？我们同样可以构建文本：</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00066.jpeg\" alt=\"{%}\"></p><p>每次执行此操作时，都会进行不同的随机选择，文本也会不同，就像这 5 个例子一样。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00067.jpeg\" alt=\"{%}\"></p><p>值得指出的是，即使在（温度为 0.8 的）第一步，也有许多可能的“下一个词”可供选择，尽管它们的概率迅速减小（是的，如下面的对数图所示，点的连线对应于 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00068.gif\" alt=\"n^{-1}\"> 次幂律衰减，这是语言的一般统计特征）。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00069.jpeg\" alt=\"{%}\"></p><p>如果再继续下去会发生什么呢？下面是一个随机的例子。虽然比选择排名最高的词（零温度）的情况好，但还是有点奇怪。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00070.jpeg\" alt=\"{%}\"></p><p>这是使用最简单的 GPT-2 模型（发布于 2019 年）完成的。使用更新更大的 GPT-3 模型，结果会更好。下面是在提示相同但使用最大的 GPT-3 模型的情况下生成的零温度文本。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00071.jpeg\" alt=\"{%}\"></p><p>下面是一个温度为 0.8 的随机示例。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00072.jpeg\" alt=\"{%}\"></p><br style=\"page-break-after:always\">","comments":[]}