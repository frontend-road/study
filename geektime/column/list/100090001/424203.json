{"id":424203,"title":"11 | 存储系统：数据到底都存哪儿了？","content":"<p>你好，我是吴磊。</p><p>感谢你在国庆假期仍然坚持学习，今天这一讲，我们来学习存储系统，与调度系统一样，它也是Spark重要的基础设施之一。不过，你可能会好奇：“掌握Spark应用开发，需要去了解这么底层的知识吗？”坦白地说，还真需要，为什么这么说呢？</p><p>我们前面学了Shuffle管理、RDD Cache和广播变量，这些功能与特性，对Spark作业的执行性能有着至关重要的影响。而想要实现这些功能，底层的支撑系统正是Spark存储系统。</p><p>学习和熟悉存储系统，不单单是为了完善我们的知识体系，它还能直接帮你更好地利用RDD Cache和广播变量这些特性。在未来，这些知识也能为你做Shuffle的调优奠定良好的基础。</p><p>既然存储系统这么重要，那要怎样高效快速地掌握它呢？本着学以致用的原则，我们需要先了解系统的服务对象，说白了就是存储系统是用来存什么东西的。</p><h2>服务对象</h2><p>笼统地说，<strong>Spark存储系统负责维护所有暂存在内存与磁盘中的数据，这些数据包括Shuffle中间文件、RDD Cache以及广播变量</strong>。</p><p>对于上述三类数据，我们并不陌生。我们先回顾一下什么是Shuffle中间文件，在Shuffle的计算过程中，Map Task在Shuffle Write阶段生产data与index文件。接下来，根据index文件提供的分区索引，Shuffle Read阶段的Reduce Task从不同节点拉取属于自己的分区数据。而Shuffle中间文件，指的正是两个阶段为了完成数据交换所仰仗的data与index文件。</p><!-- [[[read_end]]] --><p>RDD Cache指的是分布式数据集在内存或是磁盘中的物化，它往往有利于提升计算效率。广播变量<a href=\"https://time.geekbang.org/column/article/423878\">上一讲</a>我们刚刚介绍过，它的优势在于以Executors为粒度分发共享变量，从而大幅削减数据分发引入的网络与存储开销。</p><p>我们刚才对这三类数据做了简单回顾，如果你觉得哪里不是特别清楚的话，不妨翻回前面几讲再看一看，我们在第7、8、10这3讲分别对它们做了详细讲解。好啦，了解了存储系统服务的主要对象以后，接下来，我们来细数Spark存储系统都有哪些重要组件，看看它们之间又是如何协作的。</p><h2>存储系统的构成</h2><p>理论的学习总是枯燥而又乏味，为了让你更加轻松地掌握存储系统的核心组件，咱们不妨还是用斯巴克国际建筑集团的类比，来讲解Spark存储系统。</p><p>相比调度系统复杂的人事关系（戴格、塔斯克、拜肯德），存储系统的人员构成要简单得多。在内存管理<a href=\"https://time.geekbang.org/column/article/422400\">那一讲</a>，我们把节点内存看作是施工工地，而把节点磁盘看作是临时仓库，那么显然，管理数据存储的组件，就可以看成是仓库管理员，简称库管。</p><h3>布劳克家族</h3><p>在斯巴克建筑集团，库管这个关键角色，一直以来都是由布劳克家族把持着。</p><p>布劳克家族在斯巴克集团的地位举足轻重，老布劳克（BlockManagerMaster）坐镇集团总公司（Driver），而他的子嗣们、小布劳克（BlockManager）则驻守在各个分公司（Executors）。</p><p>对集团公司建材与仓库的整体情况，老布劳克了如指掌，当然，这一切要归功于他众多的子嗣们。各家分公司的小布劳克，争先恐后地向老爸汇报分公司的建材状态与仓库状况。关于他们的父子关系，我整理到了下面的示意图中。</p><p><img src=\"https://static001.geekbang.org/resource/image/ed/13/ed719d24047909274a1dac2de9fd4313.jpg?wh=1920x926\" alt=\"图片\" title=\"存储系统：布劳克家族\"></p><p><strong>从上图我们能够看得出来，小布劳克与老布劳克之间的信息交换是双向的</strong>。不难发现，布劳克家族的家风是典型的“家长制”和“一言堂”。如果小布劳克需要获取其他分公司的状态，他必须要通过老布劳克才能拿到这些信息。</p><p>在前面的几讲中，我们把建材比作是分布式数据集，那么，BlockManagerMaster与BlockManager之间交换的信息，实际上就是Executors之上数据的状态。说到这里，你可能会问：“既然BlockManagerMaster的信息都来自于BlockManager，那么BlockManager又是从哪里获取到这些信息的呢？”要回答这个问题，我们还要从BlockManager的职责说起。</p><p>我们开头说过，存储系统的服务对象有3个：分别是Shuffle中间文件、RDD Cache以及广播变量，而BlockManager的职责，正是在Executors中管理这3类数据的存储、读写与收发。就存储介质来说，这3类数据所消耗的硬件资源各不相同。</p><p>具体来说，Shuffle中间文件消耗的是节点磁盘，而广播变量主要占用节点的内存空间，RDD Cache则是“脚踏两条船”，既可以消耗内存，也可以消耗磁盘。</p><p><img src=\"https://static001.geekbang.org/resource/image/55/c3/55ce43b2a9d6f7dee2aaa5b9e1c171c3.jpg?wh=1920x895\" alt=\"图片\" title=\"服务对象与存储介质\"></p><p>不管是在内存、还是在磁盘，这些数据都是以数据块（Blocks）为粒度进行存取与访问的。<strong>数据块的概念与RDD数据分区（Partitions）是一致的</strong>，在RDD的上下文中，说到数据划分的粒度，我们往往把一份数据称作“数据分区”。而在存储系统的上下文中，对于细分的一份数据，我们称之为数据块。</p><p>有了数据块的概念，我们就可以进一步细化BlockManager的职责。<strong>BlockManager的核心职责，在于管理数据块的元数据（Meta data），这些元数据记录并维护数据块的地址、位置、尺寸以及状态</strong>。为了让你直观地感受一下元数据，我把它的样例放到了下面的示意图里，你可以看一看。</p><p><img src=\"https://static001.geekbang.org/resource/image/5b/77/5be58ab54064cb50eb106a1eyy28c377.jpg?wh=1920x453\" alt=\"图片\" title=\"元数据样例\"></p><p>只有借助元数据，BlockManager才有可能高效地完成数据的存与取、收与发。这就回答了前面我提出的问题，BlockManager与数据状态有关的所有信息，全部来自于元数据的管理。那么接下来的问题是，结合这些元数据，BlockManager如何完成数据的存取呢？</p><p>不管是工地上，还是仓库里，这些场所都是尘土飞扬、人来人往，像存取建材这种事情，养尊处优的小布劳克自然不会亲力亲为。于是，他招募了两个帮手，来帮他打理这些脏活累活。</p><p>这两个帮手也都不是外人，一个是大表姐迈美瑞（MemoryStore），另一个是大表哥迪斯克（DiskStore）。顾名思义，<strong>MemoryStore负责内存中的数据存取，而相应地，DiskStore则负责磁盘中的数据访问</strong>。</p><p>好啦，到此为止，存储系统的重要角色已经悉数登场，我把他们整理到了下面的表格中。接下来，我们以RDD Cache和Shuffle中间文件的存取为例，分别说一说迈美瑞和迪斯克是如何帮助小布劳克来打理数据的。</p><p><img src=\"https://static001.geekbang.org/resource/image/77/8a/77dd7153fbace16ccdd2d9e43eb3838a.jpg?wh=1920x532\" alt=\"图片\" title=\"存储系统主要角色\"></p><h3>MemoryStore：内存数据访问</h3><p>大表姐迈美瑞秀外慧中，做起事情来井井有条。为了不辜负小布劳克的托付，迈美瑞随身携带着一本小册子，这本小册子密密麻麻，记满了关于数据块的详细信息。这个小册子，是一种特别的数据结构：LinkedHashMap[BlockId, MemoryEntry]。顾名思义，LinkedHashMap是一种Map，其中键值对的Key是BlockId，Value是MemoryEntry。</p><p><img src=\"https://static001.geekbang.org/resource/image/43/f2/43de4d437b70e4659ddyyfd3cyyc68f2.jpg?wh=1920x932\" alt=\"图片\" title=\"迈美瑞的小册子：LinkedHashMap\"></p><p>BlockId用于标记Block的身份，需要注意的是，BlockId不是一个仅仅记录Id的字符串，而是一种记录Block元信息的数据结构。BlockId这个数据结构记录的信息非常丰富，包括Block名字、所属RDD、Block对应的RDD数据分区、是否为广播变量、是否为Shuffle Block，等等。</p><p>MemoryEntry是对象，它用于承载数据实体，数据实体可以是某个RDD的数据分区，也可以是广播变量。存储在LinkedHashMap当中的MemoryEntry，相当于是通往数据实体的地址。</p><p>不难发现，BlockId和MemoryEntry一起，就像是居民户口簿一样，完整地记录了存取某个数据块所需的所有元信息，相当于“居民姓名”、“所属派出所”、“家庭住址”等信息。<strong>基于这些元信息，我们就可以像“查户口”一样，有的放矢、精准定向地对数据块进行存取访问</strong>。</p><pre><code class=\"language-scala\">val rdd: RDD[_] = _\nrdd.cache\nrdd.count\n</code></pre><p>以RDD Cache为例，当我们使用上述代码创建RDD缓存的时候，Spark会在后台帮我们做如下3件事情，这个过程我把它整理到了下面的示意图中，你可以看一看。</p><ol>\n<li>以数据分区为粒度，计算RDD执行结果，生成对应的数据块；</li>\n<li>将数据块封装到MemoryEntry，同时创建数据块元数据BlockId；</li>\n<li>将（BlockId，MemoryEntry）键值对添加到“小册子”LinkedHashMap。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/04/fb/04f5faa135a7d5703c14a1b5e73e7dfb.jpg?wh=1920x725\" alt=\"图片\" title=\"RDD Cache的计算过程\"></p><p>随着RDD Cache过程的推进，LinkedHashMap当中的元素会越积越多，当迈美瑞的小册子完成记录的时候，Spark就可以通过册子上的“户口簿”来访问每一个数据块，从而实现对RDD Cache的读取与访问。</p><h3>DiskStore：磁盘数据访问</h3><p>说完大表姐，接下来，我们再来说说大表哥迪斯克。迪斯克的主要职责，是通过维护数据块与磁盘文件的对应关系，实现磁盘数据的存取访问。相比大表姐的一丝不苟、亲力亲为，迪斯克要“鸡贼”得多，他跟布劳克一样，都是甩手掌柜。</p><p>看到大表姐没日没夜地盯着自己的“小册子”，迪斯克可不想无脑地给布劳克卖命，于是他招募了一个帮手：DiskBlockManager，来帮他维护元数据。</p><p>有了DiskBlockManager这个帮手给他打理各种杂事，迪斯克这个家伙就可以哼着小曲、喝着咖啡，坐在仓库门口接待来来往往的施工工人就好了。这些工人有的存货，有的取货，但不论是干什么的，迪斯克会统一把他们打发到DiskBlockManager那里去，让DiskBlockManager告诉他们货物都存在哪些货架的第几层。</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/94/bbf78d73f7f202f97487f7788b417c94.jpg?wh=1920x821\" alt=\"图片\" title=\"迪斯克的帮手：DiskBlockManager\"></p><p>帮手DiskBlockManager是类对象，它的getFile方法以BlockId为参数，返回磁盘文件。换句话说，给定数据块，要想知道它存在了哪个磁盘文件，需要调用getFile方法得到答案。<strong>有了数据块与文件之间的映射关系，我们就可以轻松地完成磁盘中的数据访问</strong>。</p><p>以Shuffle为例，在Shuffle Write阶段，每个Task都会生成一份中间文件，每一份中间文件都包括带有data后缀的数据文件，以及带着index后缀的索引文件。那么对于每一份文件来说，我们都可以通过DiskBlockManager的getFile方法，来获取到对应的磁盘文件，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/20/a1/20bd0833760yy75600d0701c9yyaeba1.jpg?wh=1920x736\" alt=\"图片\" title=\"根据BlockId获取Shuffle中间文件\"></p><p>可以看到，获取data文件与获取index文件的流程是完全一致的，他们都是使用BlockId来调用getFile方法，从而完成数据访问。</p><h2>重点回顾</h2><p>今天这一讲，我们重点讲解了Spark存储系统。关于存储系统，你首先需要知道是，RDD Cache、Shuffle中间文件与广播变量这三类数据，是存储系统最主要的服务对象。</p><p>接着，我们介绍了存储系统的核心组件，它们是坐落在Driver端的BlockManagerMaster，以及“驻守”在Executors的BlockManager、MemoryStore和DiskStore。BlockManagerMaster与众多BlockManager之间通过心跳来完成信息交换，这些信息包括数据块的地址、位置、大小和状态，等等。</p><p>在Executors中，BlockManager通过MemoryStore来完成内存的数据存取。MemoryStore通过一种特殊的数据结构：LinkedHashMap来完成BlockId到MemoryEntry的映射。其中，BlockId记录着数据块的元数据，而MemoryEntry则用于封装数据实体。</p><p>与此同时，BlockManager通过DiskStore来实现磁盘数据的存取与访问。DiskStore并不直接维护元数据列表，而是通过DiskBlockManager这个对象，来完成从数据库到磁盘文件的映射，进而完成数据访问。</p><p><img src=\"https://static001.geekbang.org/resource/image/21/a3/217f2b7493b9dbfce384c042c6574ca3.jpg?wh=1920x795\" alt=\"图片\"></p><h2>每课一练</h2><p>LinkedHashMap是一种很特殊的数据结构，在今天这一讲，我们仅介绍了它在Map方面的功用。你可以试着自己梳理一下LinkedHashMap这种数据结构的特点与特性。</p><p>期待在留言区看到你的思考。如果这一讲对你有帮助，也推荐你转发给更多的同事、朋友。我们下一讲见！</p>","comments":[{"had_liked":false,"id":315657,"user_name":"Geek_2dfa9a","can_delete":false,"product_type":"c1","uid":1435535,"ip_address":"","ucode":"B5FE7971F5E773","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epGTSTvn7r4ibk1PuaUrSvvLdviaLcne50jbvvfiaxKkM5SLibeP6jibA2bCCQBqETibvIvcsOhAZlwS8kQ/132","comment_is_top":true,"comment_ctime":1633938175,"is_pvip":false,"replies":[{"id":"114423","content":"老弟谦虚啦！到目前为止，你的回答最为精彩，每次都让我印象很深，真的很棒👍！老弟得空可以加我微信，搜索“方块K”或是“rJunior”，我把你拉进读者群~ 在那里可以跟更多的同学探讨，分享你对于技术的理解和把握~ <br><br>LinkedHashMap分析的非常到位，很深入，满分💯，置顶🔝。不过，提醒一点，LinkedHashMap的采用，很重要的一点，是用来实现RDD Cache的LRU机制，老弟不妨就这一点，再展开思考，我们评论区继续讨论~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633965741,"ip_address":"","comment_id":315657,"utype":1}],"discussion_count":2,"race_medal":0,"score":"9.2233720599635005e+18","product_id":100090001,"comment_content":"今天的题好像面试啊，LinkedHashMap是HashMap的增强版，直接继承了HashMap类，多提供了一个可以按插入顺序的遍历，这个遍历是通过<br>两方面实现的：<br>1.直接扩展HashMap的Map.Entry，增加了before,after的指针，这样就在不改变本身数组的结构下<br>（哈希表本身通过数组实现常数级查询的时间复杂度，因此会打乱插入顺序），又变成了一个双链表，双链表的顺序就是插入顺序。<br>2.记录链头，链尾，这样就可以从头&#47;尾按顺序遍历了。<br>这里LinkedHashMap是怎么组织链表的值得提一下，LinkedHashMap没有覆盖HashMap的put方法，HashMap使用了模版设计模式，<br>很好的实现了扩展和解耦，通过提供空方法扩展点afterNodeAccess，afterNodeInsertion，afterNodeRemoval，<br>LinkedHashMap是通过重写这些扩展点实现了链表的插入和删除。<br>最后回一下上节课和老师交流的打开视野，目前来说，Spark对我来说有点像盲人摸象，现在只摸到了象腿，所以说大象像根柱子，<br>但是Spark可能还有很多精彩的地方我没摸到，期待老师带我们从多角度深入了解Spark这头大象。","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528139,"discussion_content":"老弟谦虚啦！到目前为止，你的回答最为精彩，每次都让我印象很深，真的很棒👍！老弟得空可以加我微信，搜索“方块K”或是“rJunior”，我把你拉进读者群~ 在那里可以跟更多的同学探讨，分享你对于技术的理解和把握~ \n\nLinkedHashMap分析的非常到位，很深入，满分💯，置顶🔝。不过，提醒一点，LinkedHashMap的采用，很重要的一点，是用来实现RDD Cache的LRU机制，老弟不妨就这一点，再展开思考，我们评论区继续讨论~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633965741,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2455712,"avatar":"https://static001.geekbang.org/account/avatar/00/25/78/a0/7a248ddc.jpg","nickname":"福","note":"","ucode":"F2FC7AF5D433C6","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":539774,"discussion_content":"老哥真的是太强了，每次置顶的评论基本都是你","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639837494,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":315894,"user_name":"Geek_2dfa9a","can_delete":false,"product_type":"c1","uid":1435535,"ip_address":"","ucode":"B5FE7971F5E773","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epGTSTvn7r4ibk1PuaUrSvvLdviaLcne50jbvvfiaxKkM5SLibeP6jibA2bCCQBqETibvIvcsOhAZlwS8kQ/132","comment_is_top":false,"comment_ctime":1634034208,"is_pvip":false,"replies":[{"id":"114536","content":"分析得很透彻，memoryStore缓存数据、evict数据的过程说得很清楚～ 赞👍！！！","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634174291,"ip_address":"","comment_id":315894,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35993772576","product_id":100090001,"comment_content":"MemoryStore这块涉及大量的nio，看得我头皮发麻。简单点讲，里面有个核心的常量LinkedHashMap，作为LRU缓存，存储所有Block。<br>putBytes()这个方法主要用来写数据，方法入参分别是blockId（数据块的标识），size（数据块长度），memoryMode（存放在堆上还是堆外），<br>_bytes（具体内存分配的闭包），具体实现逻辑是，先检查blockId对应的数据块是否已缓存，然后通过memoryManager（1.6以前是StaticMemoryManager，<br>不能支持堆外内存，1.6以后默认UnifiedMemoryManager，可以通过spark.memory.useLegacyMode指定）确认内存是否够缓存，在后通过<br>_bytes把数据拷贝到DirectByteBuffer，如果数据本来就在堆外的话就省略这个逻辑，最后把blockId作为key，ChunkedByteBuffer<br>（就是一个DirectByteBuffer数组，里面是要缓存的数据）作为SerializedMemoryEntry存到LinkedHashMap里，这里注意，<br>为了保证线程安全，LinkedHashMap需要加锁，这里是一个细粒度锁加到少部分代码上减少开销。<br>上面的是缓存序列化块的逻辑，putIterator既可以缓存序列化的值（堆内&#47;堆外），也可以直接缓存对象（只能存放在堆内），具体逻辑和LinkedHashMap<br>不太相关了，篇幅有限就不分析了。<br>作为一个LRU缓存，Spark肯定还要有一个容量满后的清除操作，触发点在put时校验空间是否足够，具体逻辑在evictBlocksToFreeSpace，<br>入参有三个blockId（失效后需要缓存的block），space（需要缓存的block的长度），memoryMode。缓存失效这块影响比较大，需要加两个锁，<br>在外层给memoryManager加锁，之后再给LinkedHashMap加锁，因为put的时候memoryManager没加锁，如果正在put的时候清理缓存会发生数据竞争，<br>因此LinkedHashMap也需要加锁，保证同一时间LinkedHashMap只能有一个操作，之后的操作就简单了，拿到LinkedHashMap的迭代器，<br>从第一个Entry开始判断，如果可以失效（）的话就记录下blockId并在blockInfoManager针对blockId加锁，<br>一直到释放的内存达到space。接下来判断如果清理这些失效的block能否拿到需要的space，不能的话就返回0，表示不能清理出所需空间，如果能拿到的话，<br>就开始dropBlock（），有些block可能同时在磁盘和内存缓存，如果只清除一处的话blockInfo无需删除解锁即可，如果磁盘和内存都没有了则需删除blockInfo，<br>如果清理过程中发生异常，则把还没清理的blockInfo解锁（这块逻辑放在finally里，并且作为一个编程技巧注释出来）。<br>另外有一点值得注意，Spark为了避免MaxDirectMemorySize的限制，使用了反射拿到了DirectByteBuffer的私有构造方法<br>private DirectByteBuffer(long addr, int cap)，这样就避开了allocateDirect方法里面Bits.reserveMemory的限制。","like_count":9,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528194,"discussion_content":"分析得很透彻，memoryStore缓存数据、evict数据的过程说得很清楚～ 赞👍！！！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634174291,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316846,"user_name":"Alvin-L","can_delete":false,"product_type":"c1","uid":1603052,"ip_address":"","ucode":"5AC96AAB75B720","user_header":"https://static001.geekbang.org/account/avatar/00/18/75/ec/c60b29f5.jpg","comment_is_top":false,"comment_ctime":1634567213,"is_pvip":false,"replies":[{"id":"114779","content":"首先明确一点哈，Spark是纯粹的计算引擎，它本身是没有存储引擎的。当然，它有自己的存储系统（BlockManager那一套体系），但是这个存储系统跟存储引擎（比如S3、HDFS）有着本质的区别，Spark需要外部存储引擎来获取数据源，Spark自己的BlockManager仅仅是为了更好地辅助分布式计算。<br><br>老弟说的MySQL数据，我理解是物化到磁盘的数据记录。要把MySQL的数据，dump到S3，或是HDFS，其实有很多方法。这个过程，可以通过Spark来做，也可以用其他大数据组件来做，比较MapReduce或是早期的Sqoop，当然，现在Sqoop逐渐退出历史舞台。<br><br>如果要用Spark做的话，也就是把MySQL数据dump到HDFS或是S3，其实就是read API + write API这么简单，中间都没啥计算逻辑其实，就是纯粹的读 + 写，搞定~<br><br>不知道问题我理解的对不对哈，咱们评论区继续讨论~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634654935,"ip_address":"","comment_id":316846,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5929534509","product_id":100090001,"comment_content":"原本存储在mysql里的数据如何转存储到spark里？老师可否加餐讲一讲mysql转存spark的相关内容","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528534,"discussion_content":"首先明确一点哈，Spark是纯粹的计算引擎，它本身是没有存储引擎的。当然，它有自己的存储系统（BlockManager那一套体系），但是这个存储系统跟存储引擎（比如S3、HDFS）有着本质的区别，Spark需要外部存储引擎来获取数据源，Spark自己的BlockManager仅仅是为了更好地辅助分布式计算。\n\n老弟说的MySQL数据，我理解是物化到磁盘的数据记录。要把MySQL的数据，dump到S3，或是HDFS，其实有很多方法。这个过程，可以通过Spark来做，也可以用其他大数据组件来做，比较MapReduce或是早期的Sqoop，当然，现在Sqoop逐渐退出历史舞台。\n\n如果要用Spark做的话，也就是把MySQL数据dump到HDFS或是S3，其实就是read API + write API这么简单，中间都没啥计算逻辑其实，就是纯粹的读 + 写，搞定~\n\n不知道问题我理解的对不对哈，咱们评论区继续讨论~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634654935,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2611429,"avatar":"https://static001.geekbang.org/account/avatar/00/27/d8/e5/bb87e21f.jpg","nickname":"穌滂","note":"","ucode":"2FDBCA735052C9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":575195,"discussion_content":"老师，请问用Spark SQL把MySQL的数据导出到S3存成Parquet格式会消耗很多内存是怎么回事？用SizeEstimator得到的table size往往比实际上MySQL里面的table size要大好几倍。所以总是导致OOM。请问这种情况要怎么处理？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1654661902,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314756,"user_name":"Neo-dqy","can_delete":false,"product_type":"c1","uid":1924786,"ip_address":"","ucode":"9BF300EB1DDD00","user_header":"https://static001.geekbang.org/account/avatar/00/1d/5e/b2/aceb3e41.jpg","comment_is_top":false,"comment_ctime":1633404319,"is_pvip":false,"replies":[{"id":"114119","content":"好问题～ 2.0之前，spark用akka来实现不同组件之间的通信。2.0以后，spark抛弃akka，基于netty实现了自己的rpc系统，主要对象有：<br>rpcEndpoint，对标akka的actor<br>rpcEndpointRef，对标akk的actorRef<br>rpcEnv，对标akka的actorSystem<br><br>Spark RPC解耦了Spark Core和底层通信系统，让spark不同组件之间（包括存储系统，blockmanager，等）实现高效异步通信","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633741944,"ip_address":"","comment_id":314756,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5928371615","product_id":100090001,"comment_content":"【BlockManagerMaster 与众多 BlockManager 之间通过**心跳**来完成信息交换】，可以问一下老师这个心跳机制是什么呀？具体是怎么实现的呢？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527785,"discussion_content":"好问题～ 2.0之前，spark用akka来实现不同组件之间的通信。2.0以后，spark抛弃akka，基于netty实现了自己的rpc系统，主要对象有：\nrpcEndpoint，对标akka的actor\nrpcEndpointRef，对标akk的actorRef\nrpcEnv，对标akka的actorSystem\n\nSpark RPC解耦了Spark Core和底层通信系统，让spark不同组件之间（包括存储系统，blockmanager，等）实现高效异步通信","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633741944,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314687,"user_name":"qinsi","can_delete":false,"product_type":"c1","uid":1667175,"ip_address":"","ucode":"090D9C4068FF12","user_header":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","comment_is_top":false,"comment_ctime":1633326985,"is_pvip":true,"replies":[{"id":"114046","content":"正解，满分💯~ ","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633601562,"ip_address":"","comment_id":314687,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5928294281","product_id":100090001,"comment_content":"LinkedHashMap 可用来实现 LRU Cache","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527763,"discussion_content":"正解，满分💯~ ","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633601562,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":351896,"user_name":"Wangyf","can_delete":false,"product_type":"c1","uid":2226219,"ip_address":"","ucode":"349068A07CB1D4","user_header":"https://static001.geekbang.org/account/avatar/00/21/f8/2b/339660f1.jpg","comment_is_top":false,"comment_ctime":1658282629,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1658282629","product_id":100090001,"comment_content":"插个眼。<br>为嘛内存的那个 MemoryStore 是自己干活，但磁盘的 DiskStore 却要维护另一个对象来干活？那它自己又去干嘛了？","like_count":0},{"had_liked":false,"id":340587,"user_name":"qingtama","can_delete":false,"product_type":"c1","uid":1069042,"ip_address":"","ucode":"765040243ECE01","user_header":"https://static001.geekbang.org/account/avatar/00/10/4f/f2/6ac3bdcf.jpg","comment_is_top":false,"comment_ctime":1648956428,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1648956428","product_id":100090001,"comment_content":"老师有个问题想请教下，如果使用LinkedHashMap是为了LRU的话就是说数据会有被淘汰的情况，但是我理解不过是内存的数据还是磁盘的数据都不能因为达到了LinkedHashMap存储上限而被清理掉吧，尤其数据都在被使用着的时候。","like_count":0},{"had_liked":false,"id":318204,"user_name":"小强","can_delete":false,"product_type":"c1","uid":2398148,"ip_address":"","ucode":"C3D1215867302D","user_header":"https://static001.geekbang.org/account/avatar/00/24/97/c4/6c92c78a.jpg","comment_is_top":false,"comment_ctime":1635204942,"is_pvip":false,"replies":[{"id":"115462","content":"并不是哈，shuffle中间文件是物化到spark.local.dir这个配置项设置的地址，主要是磁盘中的文件系统目录，当然，也可以是Ramdisk（把内存当磁盘用，开辟内存文件系统）。总之，具体存在哪里，是由spark.local.dir决定的，而这个配置项，往往是磁盘","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635260029,"ip_address":"","comment_id":318204,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1635204942","product_id":100090001,"comment_content":"请问老师，RDD Cache 以及广播变量是存储在storage memory, 那shuffle中间文件是否是存储在execution memory里啊？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529139,"discussion_content":"并不是哈，shuffle中间文件是物化到spark.local.dir这个配置项设置的地址，主要是磁盘中的文件系统目录，当然，也可以是Ramdisk（把内存当磁盘用，开辟内存文件系统）。总之，具体存在哪里，是由spark.local.dir决定的，而这个配置项，往往是磁盘","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635260029,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314900,"user_name":"钱鹏 Allen","can_delete":false,"product_type":"c1","uid":2518863,"ip_address":"","ucode":"7E95E82C0717DA","user_header":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","comment_is_top":false,"comment_ctime":1633555902,"is_pvip":true,"replies":[{"id":"114047","content":"可以参考qinsi的回复~ LinkedHashMap特有的结构，Spark主要利用它实现LRU","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633601629,"ip_address":"","comment_id":314900,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1633555902","product_id":100090001,"comment_content":"LinkedHashMap可以类比HashMap前者有序，后者无序","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527829,"discussion_content":"可以参考qinsi的回复~ LinkedHashMap特有的结构，Spark主要利用它实现LRU","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633601629,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2518863,"avatar":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","nickname":"钱鹏 Allen","note":"","ucode":"7E95E82C0717DA","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":401244,"discussion_content":"谢谢老师的指点，发现自己的专业基础还有很大的提升空间🌹","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1633611130,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":314847,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1633501141,"is_pvip":false,"replies":[{"id":"114120","content":"blockid是对数据块的统一封装（类），可以代表rdd cache block、广播变量block、shuffle block。是否为shuffle block这个字段，就是用来判断封装的数据块是不是用于shuffle的～","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1633742754,"ip_address":"","comment_id":314847,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1633501141","product_id":100090001,"comment_content":"老师在文章倒数第二张图片里shuffle中间文件的blockid里有个属性叫 是否为shuffle block ，这里的shuffle block是什么呢？为什么shuffle中间文件不属于shuffle block?","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527809,"discussion_content":"blockid是对数据块的统一封装（类），可以代表rdd cache block、广播变量block、shuffle block。是否为shuffle block这个字段，就是用来判断封装的数据块是不是用于shuffle的～","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1633742754,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":401939,"discussion_content":"shuffle中间文件不就是用来shuffle的吗 为什么不属于shuffle block。。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1633769335,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1690884,"avatar":"https://static001.geekbang.org/account/avatar/00/19/cd/04/e27b7803.jpg","nickname":"小新","note":"","ucode":"DCAD04665E2CF8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":534994,"discussion_content":"同样疑问","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638329393,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":401939,"ip_address":""},"score":534994,"extra":""}]}]}]}