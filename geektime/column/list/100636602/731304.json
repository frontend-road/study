{"id":731304,"title":"机器学习和神经网络的训练","content":"<p>到目前为止，我们一直在讨论“已经知道”如何执行特定任务的神经网络。但神经网络之所以很有用（人脑中的神经网络大概也如此），原因不仅在于它可以执行各种任务，还在于它可以通过逐步“根据样例训练”来学习执行这些任务。</p><p>当构建一个神经网络来区分猫和狗的图像时，我们不需要编写一个程序来（比如）明确地找到胡须，只需要展示很多关于什么是猫和什么是狗的样例，然后让神经网络从中“机器学习”如何区分它们即可。</p><p>重点在于，已训练的神经网络能够对所展示的特定例子进行“泛化”。正如我们之前看到的，神经网络不仅能识别猫图像的样例的特定像素模式，还能基于我们眼中的某种“猫的典型特征”来区分图像。</p><p>神经网络的训练究竟是如何起效的呢？本质上，我们一直在尝试找到能使神经网络成功复现给定样例的权重。然后，我们依靠神经网络在这些样例“之间”进行“合理”的“插值”（或“泛化”）。</p><p>让我们看一个比“最近点”问题更简单的问题，只试着让神经网络学习如下函数。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00119.jpeg\" alt=\"{%}\"></p><p>对于这个任务，我们需要只有一个输入和一个输出的神经网络。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00120.jpeg\" alt=\"{%}\"></p><p>但是，应该使用什么样的权重呢？对于每组可能的权重，神经网络都将计算出某个函数。例如，下面是它对于几组随机选择的权重计算出的函数。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00121.jpeg\" alt=\"{%}\"></p><p>可以清楚地看到，这些函数与我们想要的函数相去甚远。那么，如何才能找到能够复现函数的权重呢？</p><!-- [[[read_end]]] --><p>基本思想是提供大量的“输入→输出”样例以供“学习”，然后尝试找到能够复现这些样例的权重。以下是逐渐增加样例后所得的结果。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00122.jpeg\" alt=\"{%}\"></p><p>在该“训练”的每个阶段，都会逐步调整神经网络的权重，我们会发现最终得到了一个能成功复现我们想要的函数的神经网络。应该如何调整权重呢？基本思想是，在每个阶段看一下我们离想要的函数“有多远”，然后朝更接近该函数的方向更新权重。</p><p>为了明白离目标“有多远”，我们计算“损失函数”（有时也称为“成本函数”）。这里使用了一个简单的（L2）损失函数，就是我们得到的值与真实值之间的差异的平方和。随着训练过程不断进行，我们看到损失函数逐渐减小（遵循特定的“学习曲线”，不同任务的学习曲线不同），直到神经网络成功地复现（或者至少很好地近似）我们想要的函数。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00123.jpeg\" alt=\"{%}\"></p><p>最后需要解释的关键是，如何调整权重以减小损失函数。正如我们所说的，损失函数给出了我们得到的值和真实值之间的“距离”。但是“我们得到的值”在每个阶段是由神经网络的当前版本和其中的权重确定的。现在假设权重是变量，比如 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00124.gif\" alt=\"w_i\">。我们想找出如何调整这些变量的值，以最小化取决于它们的损失。</p><p>让我们对实践中使用的典型神经网络进行极大的简化，想象只有两个权重 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00125.gif\" alt=\"w_1\"> 和 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00126.gif\" alt=\"w_2\">。然后，我们可能会有一个损失函数，它作为 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00125.gif\" alt=\"w_1\"> 和 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00126.gif\" alt=\"w_2\"> 的函数看起来如下所示。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00127.jpeg\" alt=\"{%}\"></p><p>数值分析提供了各种技术来帮我们找到这种情况下的最小损失。一个典型的方法就是从之前的任意 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00125.gif\" alt=\"w_1\"> 和 <img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00126.gif\" alt=\"w_2\"> 开始，逐步沿着最陡的下降路径前进。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00128.jpeg\" alt=\"{%}\"></p><p>就像水从山上流下来一样，只能保证会到达表面上的某个局部最小值（“一个山湖”），但不一定能到达最终的全局最小值。</p><p>似乎不太容易在“权重景观”中找到最陡的下降路径，但是微积分可以拯救我们。正如上面提到的，我们总是可以将神经网络视为计算出一个数学函数—取决于其输入和权重。现在考虑对这些权重进行微分。结果表明，微积分的链式法则实际上让我们解开了神经网络中连续各层所做操作的谜团。结果是，我们可以—至少在某些局部近似中—“反转”神经网络的操作，并逐步找到使与输出相关的损失最小化的权重。</p><p>上图展示了，在仅有两个权重的情况下可能需要进行的最小化工作。但是事实证明，即使有更多的权重（ChatGPT 使用了 1750 亿个权重），也仍然可以进行最小化，至少可以在某种程度上进行近似。实际上，“深度学习”在 2012 年左右的重大突破与如下发现有关：与权重相对较少时相比，在涉及许多权重时，进行最小化（至少近似）可能会更容易。</p><p>换句话说，有时候用神经网络解决复杂问题比解决简单问题更容易—这似乎有些违反直觉。大致原因在于，当有很多“权重变量”时，高维空间中有“很多不同的方向”可以引导我们到达最小值；而当变量较少时，很容易陷入局部最小值的“山湖”，无法找到“出去的方向”。</p><p>值得指出的是，在典型情况下，有许多不同的权重集合可以使神经网络具有几乎相同的性能。在实际的神经网络训练中，通常会做出许多随机选择，导致产生一些“不同但等效”的解决方案，就像下面这些一样。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00129.jpeg\" alt=\"{%}\"></p><p>但是每个这样的“不同解决方案”都会有略微不同的行为。假如在我们给出训练样例的区域之外进行“外插”（extrapolation），可能会得到截然不同的结果。</p><p class=\"pic\"><img img=\"\" src=\"https://static001.geekbang.org/files/resource/ebook/100002/Images/image00130.jpeg\" alt=\"{%}\"></p><p>哪一个是“正确”的呢？实际上没有办法确定。它们都“与观察到的数据一致”。但它们都对应着“在已知框架外”进行“思考”的不同的“固有方式”。只是有些方式对我们人类来说可能“更合理”。</p><br style=\"page-break-after:always\">","comments":[{"had_liked":false,"id":388000,"user_name":"蒋波","can_delete":false,"product_type":"c1","uid":1526325,"ip_address":"四川","ucode":"611A1F0629F08E","user_header":"https://static001.geekbang.org/account/avatar/00/17/4a/35/16861bf8.jpg","comment_is_top":false,"comment_ctime":1709122123,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100636602,"comment_content":"在实际的神经网络训练中，通常会做出许多随机选择，导致产生一些“不同但等效”的解决方案，就像下面这些一样。\n但是每个这样的“不同解决方案”都会有略微不同的行为。假如在我们给出训练样例的区域之外进行“外插”（extrapolation），可能会得到截然不同的结果。\n这段讲述没有明白是什么意思？这是在说模型在不同权重设置下有着等效的能力，但是在处理什么情况会有截然不同的结果呢？","like_count":0,"discussions":[{"author":{"id":3781787,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLyoC3cOK6vIMfDXv4jSAicrDyr0icyDRTC35WpHkxx5ExJbqW9GNuTXibSZCVaYDywh872QT15AWM1Q/132","nickname":"Geek_567f64","note":"","ucode":"D3799D08794EA9","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":644702,"discussion_content":"意思是不同的解决方案（权重）都满足给定的样例数据，即等效的，但是超出样例数据的部分，可能就不同了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1715427911,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}