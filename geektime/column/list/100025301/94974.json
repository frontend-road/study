{"id":94974,"title":"13 | 弹性分布式数据集：Spark大厦的地基（上）","content":"<p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“弹性分布式数据集”。</p><p>上一讲中提到，Spark最基本的数据抽象是弹性分布式数据集（Resilient Distributed Dataset, 下文用RDD代指）。</p><p>Spark基于RDD定义了很多数据操作，从而使得数据处理的代码十分简洁、高效。所以，要想深入学习Spark，我们必须首先理解RDD的设计思想和特性。</p><h2>为什么需要新的数据抽象模型？</h2><p>传统的MapReduce框架之所以运行速度缓慢，很重要的原因就是有向无环图的中间计算结果需要写入硬盘这样的稳定存储介质中来防止运行结果丢失。</p><p>而每次调用中间计算结果都需要要进行一次硬盘的读取，反复对硬盘进行读写操作以及潜在的数据复制和序列化操作大大提高了计算的延迟。</p><p>因此，很多研究人员试图提出一个新的分布式存储方案，不仅保持之前系统的稳定性、错误恢复和可扩展性，还要尽可能地减少硬盘I/O操作。</p><p>一个可行的设想就是在分布式内存中，存储中间计算的结果，因为对内存的读写操作速度远快于硬盘。而RDD就是一个基于分布式内存的数据抽象，它不仅支持基于工作集的应用，同时具有数据流模型的特点。</p><h2>RDD的定义</h2><p>弹性分布式数据集是英文直译的名字，乍一看这个名字相信你会不知所云。如果你去Google或者百度搜索它的定义，你会得到如下结果：</p><!-- [[[read_end]]] --><p><strong>RDD表示已被分区、不可变的，并能够被并行操作的数据集合。</strong></p><p>这个定义很不直观，我认识的很多Spark初学者在查阅了很多资料后还是对RDD一头雾水，很难理解这个抽象的概念。接下来，让我们一起来对这个晦涩的概念抽丝剥茧，见其真义。</p><p>在上述定义以及RDD的中文译名中，我们不难发现，RDD有以下基本特性：分区、不可变和并行操作。接下来让我分别讲解这些特点。</p><h3>分区</h3><p>顾名思义，分区代表同一个RDD包含的数据被存储在系统的不同节点中，这也是它可以被并行处理的前提。</p><p>逻辑上，我们可以认为RDD是一个大的数组。数组中的每个元素代表一个分区（Partition）。</p><p>在物理存储中，每个分区指向一个存放在内存或者硬盘中的数据块（Block），而这些数据块是独立的，它们可以被存放在系统中的不同节点。</p><p>所以，RDD只是抽象意义的数据集合，分区内部并不会存储具体的数据。下图很好地展示了RDD的分区逻辑结构：</p><p><img src=\"https://static001.geekbang.org/resource/image/2f/9e/2f9ec57cdedf65be382a8ec09826029e.jpg\" alt=\"\"></p><p>RDD中的每个分区存有它在该RDD中的index。通过RDD的ID和分区的index可以唯一确定对应数据块的编号，从而通过底层存储层的接口中提取到数据进行处理。</p><p>在集群中，各个节点上的数据块会尽可能地存放在内存中，只有当内存没有空间时才会存入硬盘。这样可以最大化地减少硬盘读写的开销。</p><p>虽然 RDD 内部存储的数据是只读的，但是，我们可以去修改（例如通过 repartition 转换操作）并行计算单元的划分结构，也就是分区的数量。</p><h3>不可变性</h3><p>不可变性代表每一个RDD都是只读的，它所包含的分区信息不可以被改变。既然已有的RDD不可以被改变，我们只可以对现有的RDD进行<strong>转换</strong>（Transformation）操作，得到新的RDD作为中间计算的结果。从某种程度上讲，RDD与函数式编程的Collection很相似。</p><pre><code>lines = sc.textFile(&quot;data.txt&quot;)\nlineLengths = lines.map(lambda s: len(s))\ntotalLength = lineLengths.reduce(lambda a, b: a + b)\n</code></pre><p>在上述的简单例子中，我们首先读入文本文件data.txt，创建了第一个RDD lines，它的每一个元素是一行文本。然后调用map函数去映射产生第二个RDD lineLengths，每个元素代表每一行简单文本的字数。最后调用reduce函数去得到第三个RDD totalLength，它只有一个元素，代表整个文本的总字数。</p><p>那么这样会带来什么好处呢？显然，对于代表中间结果的RDD，我们需要记录它是通过哪个RDD进行哪些转换操作得来，即<strong>依赖关系</strong>，而不用立刻去具体存储计算出的数据本身。</p><p>这样做有助于提升Spark的计算效率，并且使错误恢复更加容易。</p><p>试想，在一个有N步的计算模型中，如果记载第N步输出RDD的节点发生故障，数据丢失，我们可以从第N-1步的RDD出发，再次计算，而无需重复整个N步计算过程。这样的容错特性也是RDD为什么是一个“弹性”的数据集的原因之一。后边我们会提到RDD如何存储这样的依赖关系。</p><h3>并行操作</h3><p>由于单个RDD的分区特性，使得它天然支持并行操作，即不同节点上的数据可以被分别处理，然后产生一个新的RDD。</p><h2>RDD的结构</h2><p>通过上述讲解，我们了解了RDD的基本特性——分区、不可变和并行计算。而且，我们还提到每一个RDD里都会包括分区信息、所依赖的父RDD以及通过怎样的转换操作才能由父RDD得来等信息。</p><p>实际上RDD的结构远比你想象的要复杂，让我们来看一个RDD的简易结构示意图：</p><p><img src=\"https://static001.geekbang.org/resource/image/8c/1c/8cae25f4d16a34be77fd3e84133d6a1c.png\" alt=\"\"></p><p>SparkContext是所有Spark功能的入口，它代表了与Spark节点的连接，可以用来创建RDD对象以及在节点中的广播变量等。一个线程只有一个SparkContext。SparkConf则是一些参数配置信息。感兴趣的同学可以去阅读官方的技术文档，一些相对不重要的概念我就不再赘述了。</p><p>Partitions前文中我已经提到过，它代表RDD中数据的逻辑结构，每个Partition会映射到某个节点内存或硬盘的一个数据块。</p><p>Partitioner决定了RDD的分区方式，目前有两种主流的分区方式：Hash partitioner和Range partitioner。Hash，顾名思义就是对数据的Key进行散列分区，Range则是按照Key的排序进行均匀分区。此外我们还可以创建自定义的Partitioner。</p><h3>依赖关系</h3><p>Dependencies是RDD中最重要的组件之一。如前文所说，Spark不需要将每个中间计算结果进行数据复制以防数据丢失，因为每一步产生的RDD里都会存储它的依赖关系，即它是通过哪个RDD经过哪个转换操作得到的。</p><p>细心的读者会问这样一个问题，父RDD的分区和子RDD的分区之间是否是一对一的对应关系呢？Spark支持两种依赖关系：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）。</p><p><img src=\"https://static001.geekbang.org/resource/image/5e/e1/5eed459f5f1960e2526484dc014ed5e1.jpg\" alt=\"\"></p><p>窄依赖就是父RDD的分区可以一一对应到子RDD的分区，宽依赖就是父RDD的每个分区可以被多个子RDD的分区使用。</p><p><img src=\"https://static001.geekbang.org/resource/image/98/f9/989682681b344d31c61b02368ca227f9.jpg\" alt=\"\"></p><p>显然，窄依赖允许子RDD的每个分区可以被并行处理产生，而宽依赖则必须等父RDD的所有分区都被计算好之后才能开始处理。</p><p>如上图所示，一些转换操作如map、filter会产生窄依赖关系，而Join、groupBy则会生成宽依赖关系。</p><p>这很容易理解，因为map是将分区里的每一个元素通过计算转化为另一个元素，一个分区里的数据不会跑到两个不同的分区。而groupBy则要将拥有所有分区里有相同Key的元素放到同一个目标分区，而每一个父分区都可能包含各种Key的元素，所以它可能被任意一个子分区所依赖。</p><p>Spark之所以要区分宽依赖和窄依赖是出于以下两点考虑：</p><ul>\n<li>\n<p>窄依赖可以支持在同一个节点上链式执行多条命令，例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。</p>\n</li>\n<li>\n<p>从失败恢复的角度考虑，窄依赖的失败恢复更有效，因为它只需要重新计算丢失的父分区即可，而宽依赖牵涉到 RDD 各级的多个父分区。</p>\n</li>\n</ul><h2>小结</h2><p>弹性分布式数据集作为Spark的基本数据抽象，相较于Hadoop/MapReduce的数据模型而言，各方面都有很大的提升。</p><p>首先，它的数据可以尽可能地存在内存中，从而大大提高的数据处理的效率；其次它是分区存储，所以天然支持并行处理；而且它还存储了每一步骤计算结果之间的依赖关系，从而大大提升了数据容错性和错误恢复的正确率，使Spark更加可靠。</p><p>下一讲，我们会继续深入研究RDD的容错机制、任务执行机制，以及Spark定义在RDD上的各种转换与动作操作。</p><h2>思考题</h2><p>窄依赖是指父RDD的每一个分区都可以唯一对应子RDD中的分区，那么是否意味着子RDD中的一个分区只可以对应父RDD中的一个分区呢？如果子RDD的一个分区需要由父RDD中若干个分区计算得来，是否还算窄依赖？</p><p>最后，欢迎你把对弹性分布式数据集的疑问写在留言区，与我和其他同学一起讨论。</p><p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p><p></p>","comments":[{"had_liked":false,"id":94692,"user_name":"Milittle","can_delete":false,"product_type":"c1","uid":1045455,"ip_address":"","ucode":"80E566639A8ABB","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f3/cf/851dab01.jpg","comment_is_top":false,"comment_ctime":1557882622,"is_pvip":true,"replies":[{"id":"48548","content":"你说的很准确！点赞","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567394211,"ip_address":"","comment_id":94692,"utype":1}],"discussion_count":6,"race_medal":0,"score":"276435789566","product_id":100025301,"comment_content":"对于思考题：需要重点理解原文中这句话：<br>窄依赖就是父 RDD 的分区可以一一对应到子 RDD 的分区，宽依赖就是父 RDD 的每个分区可以被多个子 RDD 的分区使用。<br>这句话说明了，窄依赖的父RDD必须有一个对应的子RDD，也就是说父RDD的一个分区只能被子RDD一个分区使用，但是反过来子RDD的一个分区可以使用父RDD的多个分区。那就回复今天的思考题，第一个疑问窄依赖子RDD的分区不一定只对应父RDD的一个分区，只要满足被子RDD分区利用的父RDD分区不被子RDD的其他分区利用就算窄依赖。第二个疑问其实上面已经做了回答，只有当子RDD分区依赖的父RDD分区不被其他子RDD分区依赖，这样的计算就是窄依赖，否则是宽依赖。<br>最后，总结以下，就是只有父RDD的分区被多个子RDD的分区利用的时候才是宽依赖，其他的情况就是窄依赖。如果有哪里理解不对的地方，请老师指正，谢谢~","like_count":65,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450173,"discussion_content":"你说的很准确！点赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567394211,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1581245,"avatar":"https://static001.geekbang.org/account/avatar/00/18/20/bd/5656b5d7.jpg","nickname":"走刀口 💰","note":"","ucode":"C7E2B812A4A02C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362193,"discussion_content":"不对。不能从分区几对几来区分宽窄。宽窄依赖本质还是能否以pipeline的方式执行，而不是几对几。比如cartsian算子，它父rdd一个分区被子rdd多个分区使用，可它是窄依赖。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1616884627,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":10937,"discussion_content":"课代表请坐。所谓的宽窄其实是针对父RDD讲的，只要父RDD的分区不被多个子RDD使用，那就是窄依赖。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1568341092,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1710643,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/vwv0qiaibQzcTP6yIDJXOJLdh64CnBSaOceFFuWArVHQcwicIsebwYbKJ69OSJrxeXJyawtNIINcjo3V3hgg5BW1A/132","nickname":"MuBo","note":"","ucode":"46402B9B7D7B1E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1266620,"avatar":"https://static001.geekbang.org/account/avatar/00/13/53/bc/72baeee8.jpg","nickname":"林黛玉","note":"","ucode":"F8507366012881","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":325590,"discussion_content":"我也觉得这个宽依赖的定义是相对于父RDD来说的，而且很有可能是父子RDD的上下游分区应该在同一个节点上，否则这个窄依赖的特性没有价值。个人感觉","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1605358576,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":10937,"ip_address":""},"score":325590,"extra":""},{"author":{"id":1581245,"avatar":"https://static001.geekbang.org/account/avatar/00/18/20/bd/5656b5d7.jpg","nickname":"走刀口 💰","note":"","ucode":"C7E2B812A4A02C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1710643,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/vwv0qiaibQzcTP6yIDJXOJLdh64CnBSaOceFFuWArVHQcwicIsebwYbKJ69OSJrxeXJyawtNIINcjo3V3hgg5BW1A/132","nickname":"MuBo","note":"","ucode":"46402B9B7D7B1E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362194,"discussion_content":"错，很是否经过网络传输无关","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616884681,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":325590,"ip_address":""},"score":362194,"extra":""}]},{"author":{"id":2446192,"avatar":"https://static001.geekbang.org/account/avatar/00/25/53/70/f140cfb2.jpg","nickname":"李飞","note":"","ucode":"9A475482295B81","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":366552,"discussion_content":"我也觉得不对，如果一个子rdd的分区依赖多个父rdd的分区也算窄依赖，那岂不是要等到父rdd的所有分区都完成才能并行？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618112161,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":117584,"user_name":"windcaller","can_delete":false,"product_type":"c1","uid":1514157,"ip_address":"","ucode":"1CA3E849805770","user_header":"https://static001.geekbang.org/account/avatar/00/17/1a/ad/faf1bf19.jpg","comment_is_top":false,"comment_ctime":1564099590,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"130413118470","product_id":100025301,"comment_content":"窄：一子多父，一子一父<br>宽：一父多子","like_count":31,"discussions":[{"author":{"id":1519344,"avatar":"https://static001.geekbang.org/account/avatar/00/17/2e/f0/0bbb0df5.jpg","nickname":"乐天","note":"","ucode":"1DC138F7BD536E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":381343,"discussion_content":"一子多父的窄依赖场景，子RDD同样要等父RDD都执行完才可以执行，对吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625018706,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1519344,"avatar":"https://static001.geekbang.org/account/avatar/00/17/2e/f0/0bbb0df5.jpg","nickname":"乐天","note":"","ucode":"1DC138F7BD536E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382683,"discussion_content":"这肯定吧，一子一父也要等一父执行完成吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625670822,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":381343,"ip_address":""},"score":382683,"extra":""}]}]},{"had_liked":false,"id":95282,"user_name":"追梦","can_delete":false,"product_type":"c1","uid":1183831,"ip_address":"","ucode":"54C6E76E8FE033","user_header":"https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg","comment_is_top":false,"comment_ctime":1558006939,"is_pvip":false,"replies":[{"id":"48547","content":"不对，Spark的中间计算结果如果没有特别指定持久化到硬盘，都会存在内存里以方便下一次调用，这也是它运行速度比MapReduce快的主要原因。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567393780,"ip_address":"","comment_id":95282,"utype":1}],"discussion_count":4,"race_medal":0,"score":"14442908827","product_id":100025301,"comment_content":"老师，下面这句话对吗？<br>一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。<br><br>说明：spark的中间计算结果会直接落到磁盘上的？？？<br>","like_count":4,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450401,"discussion_content":"不对，Spark的中间计算结果如果没有特别指定持久化到硬盘，都会存在内存里以方便下一次调用，这也是它运行速度比MapReduce快的主要原因。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567393780,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382685,"discussion_content":"stage是什么？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625671067,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1897610,"avatar":"","nickname":"Fiery","note":"","ucode":"CDB000687A6B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":202120,"discussion_content":"shuffle直接RPC传给下面一个stage的executor吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583863838,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1475774,"avatar":"https://static001.geekbang.org/account/avatar/00/16/84/be/1531bcb5.jpg","nickname":"yang","note":"","ucode":"301717D20ABCE4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":8693,"discussion_content":"中间划分stage不是shuffle么？shuffle还不落盘？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568036779,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94942,"user_name":"一","can_delete":false,"product_type":"c1","uid":1220750,"ip_address":"","ucode":"28E0605EA1AE88","user_header":"https://static001.geekbang.org/account/avatar/00/12/a0/8e/6e4c7509.jpg","comment_is_top":false,"comment_ctime":1557927246,"is_pvip":false,"replies":[{"id":"34662","content":"你好，在第21讲我会带大家比较Spark和Flink。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1558546575,"ip_address":"","comment_id":94942,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14442829134","product_id":100025301,"comment_content":"老师好！能不能请老师讲讲Spark和Flink的对比呢？这二者谁在机器学习乃至深度学习中更有优势呢？","like_count":3,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450267,"discussion_content":"你好，在第21讲我会带大家比较Spark和Flink。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1558546575,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95150,"user_name":"吟游雪人","can_delete":false,"product_type":"c1","uid":1063805,"ip_address":"","ucode":"C432C9577B420C","user_header":"https://static001.geekbang.org/account/avatar/00/10/3b/7d/6376926b.jpg","comment_is_top":false,"comment_ctime":1557974998,"is_pvip":false,"replies":[{"id":"48505","content":"不保存计算结果指的是不写入硬盘。像MapReduce中每一步中间计算结果都要写入HDFS。新的RDD是上一步RDD计算的结果，但是并没有立刻进行计算，看过下一讲就明白了，只有碰到action操作才会开始执行，而且如果没有进行特别的缓存或者持久化操作，结果只在内存中，没有被写入硬盘，所以说不保存结果。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567385382,"ip_address":"","comment_id":95150,"utype":1}],"discussion_count":3,"race_medal":0,"score":"10147909590","product_id":100025301,"comment_content":"新的RDD不就是上一步的RDD计算出的结果么？为什么说是不保存计算结果？","like_count":3,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450347,"discussion_content":"不保存计算结果指的是不写入硬盘。像MapReduce中每一步中间计算结果都要写入HDFS。新的RDD是上一步RDD计算的结果，但是并没有立刻进行计算，看过下一讲就明白了，只有碰到action操作才会开始执行，而且如果没有进行特别的缓存或者持久化操作，结果只在内存中，没有被写入硬盘，所以说不保存结果。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567385382,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2583,"discussion_content":"这就是 RDD 的特点，惰性计算，这样一个是快，因为计算非常耗时间，但我们要的最终结果而不是中间结果，另外，我们在最后一步计算，可以总览所有的环节，可能可以通过合并、省略部分环节，更好的优化。","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1563777898,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382686,"discussion_content":"就快在不用存储吗，那为什么MapReduce非要存储呢，感觉不不存储是个很简单的想法，为什么MapReduce做下不是很容易？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625671190,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":2583,"ip_address":""},"score":382686,"extra":""}]}]},{"had_liked":false,"id":94761,"user_name":"锦","can_delete":false,"product_type":"c1","uid":1468298,"ip_address":"","ucode":"CB0EB4B68C468B","user_header":"https://static001.geekbang.org/account/avatar/00/16/67/8a/babd74dc.jpg","comment_is_top":false,"comment_ctime":1557890174,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10147824766","product_id":100025301,"comment_content":"文中提到依赖关系的区分考虑基于两点：<br>1、性能考虑，窄依赖可以使得每个数据节点并行计算结果，也可以支持链式计算；宽依赖需要父分区的中每个分区中的数据节点计算，只能串行计算。<br>2、故障恢复考虑，窄依赖可以更快、更准的恢复数据，宽依赖则相对较慢。<br>那么基于以上考虑，父rdd与子rdd是多对多的关系，则划分到宽依赖；一对一、一对多或多对一的关系都可以划分到窄依赖。<br><br>分区方式：hash分区、rang分区，以及自定义分区<br>疑问：因为分区指向某个节点中的数据块，那么分区的key是分区在RDD中的index还是其引用的数据块中的某个数据字段？我认为是后者。<br>另外，hash分区和rang分区的应用场景分别是什么呢？<br><br>RDD具有不可变性，只能通过转换来创建新的RDD，但是不代表RDD中分区指向的节点数据块也不可变，那么如何保证数据块不可变性呢？我认为可能是使用CopyOnWrite技术实现的。<br><br>Spark优于MapReduce的地方之一在于：MapReduce的中间计算结果实时落盘，而Spark使用存储依赖关系和延迟存储中间数据来提高性能。","like_count":2},{"had_liked":false,"id":96327,"user_name":"渡码","can_delete":false,"product_type":"c1","uid":1348536,"ip_address":"","ucode":"8FD8B863D1DA0C","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/b8/6510592e.jpg","comment_is_top":false,"comment_ctime":1558399665,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5853366961","product_id":100025301,"comment_content":"思考题：<br>如果子RDD依赖多个父RDD的多个完整分区，也就是说不存在父RDD一个分区数据分到子RDD多个分区的情况，那就是窄依赖。因为此时父RDD不需要对key重新做分区计算，子RDD也不需要等父RDD所有分区计算完毕。","like_count":1,"discussions":[{"author":{"id":1825218,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmNnjcib8AGd2MiagY9ek2oQysqSpvOcaQ30TReSozVibjWQydxx1FIGXDwyyP98pkbyAaI3GVctuJw/132","nickname":"不白","note":"","ucode":"8B0D683F60A405","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":305678,"discussion_content":"&#34;不需要对key重新做分区计算&#34;是怎么理解的？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600053723,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95092,"user_name":"Little Spirits","can_delete":false,"product_type":"c1","uid":1180772,"ip_address":"","ucode":"61EBC16F4FB47D","user_header":"https://static001.geekbang.org/account/avatar/00/12/04/64/b53c0cc2.jpg","comment_is_top":false,"comment_ctime":1557967733,"is_pvip":true,"replies":[{"id":"48550","content":"建议看一下高赞的回答，那位同学的理解要更加全面","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567394485,"ip_address":"","comment_id":95092,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5852935029","product_id":100025301,"comment_content":"多个父分区到一个子分区，对于任何一个父分区而言都是pipeline的所以是窄依赖，而一个父分区到多个子分区对父分区而言不是pipeline的所以是宽依赖","like_count":1,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450328,"discussion_content":"建议看一下高赞的回答，那位同学的理解要更加全面","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567394485,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94943,"user_name":"miwucc","can_delete":false,"product_type":"c1","uid":1326429,"ip_address":"","ucode":"7935BD907119AE","user_header":"https://static001.geekbang.org/account/avatar/00/14/3d/5d/ac666969.jpg","comment_is_top":false,"comment_ctime":1557927265,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"5852894561","product_id":100025301,"comment_content":"子rdd依赖多个父rdd来产出结果。明显是宽依赖。因为需要等待多个父rdd结果完毕才能开始计算。宽依赖还是窄依赖关键看是否要等待更多父rdd准备完毕。","like_count":1,"discussions":[{"author":{"id":1583886,"avatar":"https://static001.geekbang.org/account/avatar/00/18/2b/0e/af0d53af.jpg","nickname":"bigeast","note":"","ucode":"39976738E85062","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":197767,"discussion_content":"好像不对吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1583428167,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94715,"user_name":"邱从贤※klion26","can_delete":false,"product_type":"c1","uid":1027239,"ip_address":"","ucode":"36DF21F2B9E94C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ac/a7/4d41966a.jpg","comment_is_top":false,"comment_ctime":1557884523,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5852851819","product_id":100025301,"comment_content":"思考题中窄依赖应该是不关心一个子 rdd 的 partition 对应多少个 父rdd 的 partition，只要partition可以独立计算就行，比如 map 操作，子 rdd 只有 3 个 partition，父 rdd 有6 个 partition，那么父 rdd 肯定有两个 partition 的数据会落到子 rdd 的一个 partition 中，但是落到子 rdd 同一个 partition 的两个 partition 不需要等待就可以进行计算，因此是窄依赖","like_count":1},{"had_liked":false,"id":94660,"user_name":"CoderLean","can_delete":false,"product_type":"c1","uid":1518409,"ip_address":"","ucode":"DC9E25428EDB3F","user_header":"https://static001.geekbang.org/account/avatar/00/17/2b/49/e94b2a35.jpg","comment_is_top":false,"comment_ctime":1557880049,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5852847345","product_id":100025301,"comment_content":"算啊，父子rdd可以通过函数进行转换，对于转换因子是基本不变的，那也应该支持逆转换。，而一个子rdd是无法推导出父rdd的，因为父rdd的数据是由函数转换后拆分给多个子rdd的。另外，之前没学过spark，对于flink内部算子也没有那么深入的理论，学完这个后要可以回去看看flink的算子是怎么实现的","like_count":2},{"had_liked":false,"id":94641,"user_name":"Rainbow","can_delete":false,"product_type":"c1","uid":1259525,"ip_address":"","ucode":"248A7E2C05E4DE","user_header":"https://static001.geekbang.org/account/avatar/00/13/38/05/67aae6c8.jpg","comment_is_top":false,"comment_ctime":1557877962,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5852845258","product_id":100025301,"comment_content":"uion也是窄依赖","like_count":1},{"had_liked":false,"id":94625,"user_name":"涵","can_delete":false,"product_type":"c1","uid":1502742,"ip_address":"","ucode":"BB8575DB13F1E0","user_header":"https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg","comment_is_top":false,"comment_ctime":1557868340,"is_pvip":false,"replies":[{"id":"48549","content":"👍🏻","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567394320,"ip_address":"","comment_id":94625,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5852835636","product_id":100025301,"comment_content":"我想还可以算做是窄依赖，因为子RDD分区所依赖的对个父RDD分区是互斥的，所以每个子RDD分区所依赖的多个父RDD分区可以被看做一组分区。父RDD的组分区与子分区是一一对应关系，满足窄依赖可以并行计算，而无需所以父分区都计算完毕才可以开始计算的特性。","like_count":1,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450145,"discussion_content":"👍🏻","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567394320,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382691,"discussion_content":"宽的也可以并行吧，哪里需要等待父分区计算完毕？父分区计算指的是什么计算？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625673158,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":287698,"user_name":"李飞","can_delete":false,"product_type":"c1","uid":2446192,"ip_address":"","ucode":"9A475482295B81","user_header":"https://static001.geekbang.org/account/avatar/00/25/53/70/f140cfb2.jpg","comment_is_top":false,"comment_ctime":1618112239,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1618112239","product_id":100025301,"comment_content":"最高赞的理解不对吧！","like_count":0},{"had_liked":false,"id":251842,"user_name":"Geek_04168","can_delete":false,"product_type":"c1","uid":2112008,"ip_address":"","ucode":"F125ADD877952E","user_header":"","comment_is_top":false,"comment_ctime":1601904271,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1601904271","product_id":100025301,"comment_content":"一子多父属于窄依赖，但我理解这种情况下，多个父依赖分布在不同的节点上，是不支持文中“窄依赖可以支持在同一个节点上链式执行多条命令”","like_count":0},{"had_liked":false,"id":247819,"user_name":"茂杨","can_delete":false,"product_type":"c1","uid":1181344,"ip_address":"","ucode":"8D8259E905DCA3","user_header":"https://static001.geekbang.org/account/avatar/00/12/06/a0/3da0e315.jpg","comment_is_top":false,"comment_ctime":1599876670,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1599876670","product_id":100025301,"comment_content":"讲的真是清楚啊。<br>RDD是抽象的数据结构，保存在内存当中，但是要记住这个内存可不是连续的，而是分布式的。<br>这就引出了分区的概念。它是隐藏在RDD下面的真正的存储空间。","like_count":0},{"had_liked":false,"id":226655,"user_name":"bwv825","can_delete":false,"product_type":"c1","uid":1508445,"ip_address":"","ucode":"23447AD4E864E0","user_header":"https://static001.geekbang.org/account/avatar/00/17/04/5d/259d1768.jpg","comment_is_top":false,"comment_ctime":1592180330,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1592180330","product_id":100025301,"comment_content":"“而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理”。此处应为“子 RDD 的所有分区”吧。宽依赖表示一个父分区可以被多个子分区使用，但不代表子分区有多个父分区。","like_count":0},{"had_liked":false,"id":215391,"user_name":"北冥有鱼","can_delete":false,"product_type":"c1","uid":1592243,"ip_address":"","ucode":"1690734A1061F4","user_header":"https://static001.geekbang.org/account/avatar/00/18/4b/b3/51bb33f2.jpg","comment_is_top":false,"comment_ctime":1588986112,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1588986112","product_id":100025301,"comment_content":"只要父RDD的分区数据能完整的交给子RDD就算窄依赖。也就是说，一个子RDD可以依赖多个父RDD。宽窄依赖是划分stage的依据，在同一个stage中，计算逻辑是可以合并的(如map，filter)，而不用每次计算逻辑都生成数据，再遍历数据。而不同的stage间需要shuffle，shuffle会生成中间数据，下一个stage需要等待上一个stage执行完再执行，逻辑也就不能和上一个stage进行合并了。同时spark要保证任务失败，能更好更快的重算，需要把依赖的父partition保留起来，以及shuffle后的结果数据保留起来… 理解不对的地方，请老师指正:)","like_count":0},{"had_liked":false,"id":195567,"user_name":"Eden2020","can_delete":false,"product_type":"c1","uid":1899158,"ip_address":"","ucode":"0DEE62F2335237","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/96/4a7b7505.jpg","comment_is_top":false,"comment_ctime":1585197897,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585197897","product_id":100025301,"comment_content":"主要区分一个计算是否要等待依赖计算完成了，有就是宽依赖，没有就是窄依赖，针对实际的计算需求是很容易区分的","like_count":0},{"had_liked":false,"id":183851,"user_name":"刘润森","can_delete":false,"product_type":"c1","uid":1236556,"ip_address":"","ucode":"84101C670A6747","user_header":"https://static001.geekbang.org/account/avatar/00/12/de/4c/a51ece16.jpg","comment_is_top":false,"comment_ctime":1583145563,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1583145563","product_id":100025301,"comment_content":"spark集群读不了文件，spark本地读取得到，怎么办？","like_count":0},{"had_liked":false,"id":138262,"user_name":"Goku","can_delete":false,"product_type":"c1","uid":1031868,"ip_address":"","ucode":"8008F3BB10E609","user_header":"https://static001.geekbang.org/account/avatar/00/0f/be/bc/62d402da.jpg","comment_is_top":false,"comment_ctime":1570124482,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1570124482","product_id":100025301,"comment_content":"RDD的每个partition有没有replication？万一node挂了怎么办？","like_count":0,"discussions":[{"author":{"id":1031868,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/be/bc/62d402da.jpg","nickname":"Goku","note":"","ucode":"8008F3BB10E609","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":26449,"discussion_content":"自己回答一下，按照目前的理解应该是没有replication，失败了重新计算","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1570602765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":131980,"user_name":"kissrain","can_delete":false,"product_type":"c1","uid":1120583,"ip_address":"","ucode":"2177C53E3B2DCC","user_header":"https://static001.geekbang.org/account/avatar/00/11/19/47/b27f1314.jpg","comment_is_top":false,"comment_ctime":1567998463,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1567998463","product_id":100025301,"comment_content":"之前查阅了很多资料都没有一个对RDD完整解释或者说的很模糊。老师的这篇文章真是一针见血，越是厉害的人越是能把原理说的越通俗易懂。","like_count":1,"discussions":[{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382701,"discussion_content":"这些东西只有源码和官方文档说的清楚，太冷门了，就好比波音飞机的原理是什么，估计网上的资料全没用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625673501,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":114890,"user_name":"hel","can_delete":false,"product_type":"c1","uid":1359507,"ip_address":"","ucode":"E9F93F32BCD7D7","user_header":"https://static001.geekbang.org/account/avatar/00/14/be/93/247fb2c8.jpg","comment_is_top":false,"comment_ctime":1563424410,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1563424410","product_id":100025301,"comment_content":"新RDD的分区数可以不和父RDD的分区数想等吗","like_count":0,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2582,"discussion_content":"可以啊，union 会变多，join 有时会变少","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563777738,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":107875,"user_name":"心灵捕手","can_delete":false,"product_type":"c1","uid":1232527,"ip_address":"","ucode":"6D0BC463A000C8","user_header":"https://static001.geekbang.org/account/avatar/00/12/ce/8f/6daf02db.jpg","comment_is_top":false,"comment_ctime":1561627168,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1561627168","product_id":100025301,"comment_content":"老师问下，<br>lines = sc.textFile(&quot;data.txt&quot;)<br>lineLengths = lines.map(lambda s: len(s))<br>totalLength = lineLengths.reduce(lambda a, b: a + b)<br>这个代码直接在spark上运行，报错&lt;console&gt;:1: error: &#39;)&#39; expected but &#39;(&#39; found.<br>val lineLengths = lines.map(lambda s: len(s))<br>","like_count":0},{"had_liked":false,"id":104197,"user_name":"dancer","can_delete":false,"product_type":"c1","uid":1019036,"ip_address":"","ucode":"B8D5641A3AC490","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8c/9c/d48473ab.jpg","comment_is_top":false,"comment_ctime":1560679861,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"1560679861","product_id":100025301,"comment_content":"这里有个疑惑希望老师解答。spark每一个子rdd的都会记录他和父rdd分区的依赖关系，所以不需要持久化到磁盘。那么我理解所有这必须依赖于父rdd也要一直保存在内存中才可以。如果是这样的话，是不是spark需要保存所有步骤产生的rdd在内存中。 另外如果所有数据都保存在内存中，如果机器故障，该怎样恢复数据呢？感谢老师用来答疑的宝贵时间。","like_count":0,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2586,"discussion_content":"我觉得这句话强调的是惰性计算的特点，中间子 rdd 只保存计算关系（如何从父 rdd 得出），而不是实际结果。全部在内存这个不绝对，比如这个数据集比机器内存还大，显然是放不下的，spark 应该是中间计算过程中尽量优先使用内存，并且一定有硬盘的全量快照备份，这块应该是 spark 处理了这些存储细节，没看过具体源码，我猜是这样。。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1563778382,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1129610,"avatar":"https://static001.geekbang.org/account/avatar/00/11/3c/8a/900ca88a.jpg","nickname":"test","note":"","ucode":"C57A175CBC6547","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382705,"discussion_content":"万一内存坏了呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625674208,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":2586,"ip_address":""},"score":382705,"extra":""}]}]},{"had_liked":false,"id":99267,"user_name":"Geek_f406a1","can_delete":false,"product_type":"c1","uid":1453834,"ip_address":"","ucode":"62A15101047054","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/4lUeUo6LHfsHLYfKaMQXQiaZVyEsqY1nfXU6dP0wN1KCch7LDIZTCO4rJ5mq1SdqY9FibCGMsjFdknULmEQ4Octg/132","comment_is_top":false,"comment_ctime":1559183768,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1559183768","product_id":100025301,"comment_content":"窄依赖并不是完全的父子RDD一一对应，子RDD可以对应多个父RDD，父RDD只能对应一个RDD？是否可以这样理解","like_count":0,"discussions":[{"author":{"id":1058818,"avatar":"https://static001.geekbang.org/account/avatar/00/10/28/02/a6d7ece6.jpg","nickname":"refactor","note":"","ucode":"EC8FF55FE6EC8E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":2584,"discussion_content":"是的","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1563778050,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":96259,"user_name":"lwenbin","can_delete":false,"product_type":"c1","uid":1202109,"ip_address":"","ucode":"05C4CC6BE0B56C","user_header":"https://static001.geekbang.org/account/avatar/00/12/57/bd/acf40fa0.jpg","comment_is_top":false,"comment_ctime":1558366979,"is_pvip":false,"replies":[{"id":"48552","content":"说的对。查阅paper是一个很好的学习方法，给你点赞。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567394910,"ip_address":"","comment_id":96259,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1558366979","product_id":100025301,"comment_content":"思考题答案应该算narrow partition. <br>看了spark rdd论文，对老师说的这些概念有了新的认识。<br>narrow partition对应的转换例如：map, filter, join（子分区join key都在一个分区）, union. 其中join和union就对应了老师说的子分区来自于多个父分区。<br>区别在于wide partition有shuffle过程，存在对于同一个父分区中两个record, 经过转换操作后会对应到两个不同的子分区，所以这些操作例如：groupByKey, 通常的join。","like_count":0,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450798,"discussion_content":"说的对。查阅paper是一个很好的学习方法，给你点赞。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567394910,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":95572,"user_name":"一","can_delete":false,"product_type":"c1","uid":1220750,"ip_address":"","ucode":"28E0605EA1AE88","user_header":"https://static001.geekbang.org/account/avatar/00/12/a0/8e/6e4c7509.jpg","comment_is_top":false,"comment_ctime":1558084988,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1558084988","product_id":100025301,"comment_content":"“在物理存储中，每个分区指向一个存放在内存或者硬盘中的数据块”，老师，这一句不太理解，每个分区指向数据块，可是前文说到分区在逻辑上是一个数组，那数组怎么能指向数据块呢？","like_count":0},{"had_liked":false,"id":95262,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1558000616,"is_pvip":false,"replies":[{"id":"48543","content":"HDFS我认为并没有过时，现在大部分分布式计算引擎还是与HDFS存储的数据进行交互。分布式存储系统有很多比如Amazon的Dynamo DB，Facebook曾经用的HBase等等。开源的话我没有用过很多，ceph听说还不错。","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567393223,"ip_address":"","comment_id":95262,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1558000616","product_id":100025301,"comment_content":"老师，hadoop计算框架Map&#47;Reduce 过时了，那另一个存储框架HDFS，也过时了吗？<br>现在我看很多云提供商都是对象存储实现海量需求，<br>现在开源的分布式存储，能在生产环境使用的，用什么了,ceph?","like_count":0,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450390,"discussion_content":"HDFS我认为并没有过时，现在大部分分布式计算引擎还是与HDFS存储的数据进行交互。分布式存储系统有很多比如Amazon的Dynamo DB，Facebook曾经用的HBase等等。开源的话我没有用过很多，ceph听说还不错。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567393223,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94984,"user_name":"一修💤","can_delete":false,"product_type":"c1","uid":1014211,"ip_address":"","ucode":"CE3CB2E9A67DB5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/79/c3/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1557933604,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557933604","product_id":100025301,"comment_content":"请教老师一个问题，现在有个inference任务使用hadoop streaming，1000个mapper并行执行，没有reducer，需要为每个mapper载入模型 词典等文件，如果改用spark，能否实现多个RDD读取同一块内存的模型和词典，达到节省内存的效果？ 还望老师答复","like_count":0},{"had_liked":false,"id":94912,"user_name":"linuxfans","can_delete":false,"product_type":"c1","uid":1193815,"ip_address":"","ucode":"0C0BEE82F8A409","user_header":"https://static001.geekbang.org/account/avatar/00/12/37/57/750c641d.jpg","comment_is_top":false,"comment_ctime":1557921230,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557921230","product_id":100025301,"comment_content":"这个没有疑问吧？既然父子分区是一一对应，当然不存在这种情况了。如果是一子对父的情况，就变成子分区要等待所以父分区的结果，并行的效率降低。","like_count":0},{"had_liked":false,"id":94792,"user_name":"蒙开强","can_delete":false,"product_type":"c1","uid":1317706,"ip_address":"","ucode":"61B3183781B9F7","user_header":"https://static001.geekbang.org/account/avatar/00/14/1b/4a/f9df2d06.jpg","comment_is_top":false,"comment_ctime":1557898592,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1557898592","product_id":100025301,"comment_content":"老师，你好，学习了你这节讲的RDD,我有个问题咨询你一下，如果我要判断这RDD是否有数据，在生产中那种方式是最优的呢。","like_count":0,"discussions":[{"author":{"id":1140311,"avatar":"https://static001.geekbang.org/account/avatar/00/11/66/57/e3bd54bd.jpg","nickname":"笃行之","note":"","ucode":"F76B5C6D17765E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":507,"discussion_content":"rdd.isempty","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1561625983,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94668,"user_name":"木卫六","can_delete":false,"product_type":"c1","uid":1199495,"ip_address":"","ucode":"D113DF578C5BF5","user_header":"https://static001.geekbang.org/account/avatar/00/12/4d/87/57236a2d.jpg","comment_is_top":false,"comment_ctime":1557880766,"is_pvip":true,"replies":[{"id":"48551","content":"正确！","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567394530,"ip_address":"","comment_id":94668,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557880766","product_id":100025301,"comment_content":"1.窄依赖是指父 RDD 的每一个分区都可以唯一对应子 RDD 中的分区，那么是否意味着子 RDD 中的一个分区只可以对应父 RDD 中的一个分区呢？<br><br>不是的，比如coalesce这种合并分区的操作中，子rdd需要依赖父rdd的若干个分区，但它不需要全部的分区，是窄依赖<br><br>2.如果子 RDD 的一个分区需要由父 RDD 中若干个分区计算得来，是否还算窄依赖？<br><br>算。只要纪录层级没发生重新分区，全局混洗，应该都属于窄依赖吧","like_count":0,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450164,"discussion_content":"正确！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567394530,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":94655,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1557879462,"is_pvip":false,"replies":[{"id":"48542","content":"就是说，分区本身不会存储数据，它只是有一个指向真正存储数据位置的reference。<br><br>","user_name":"作者回复","user_name_real":"Xinyue Li","uid":"1502409","ctime":1567392855,"ip_address":"","comment_id":94655,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1557879462","product_id":100025301,"comment_content":"rdd的分区不保存数据什么意思？老师可以对rdd结构再深入讲解不？我认为子rdd在窄依赖中不会对应多个父rdd，才保障单向传递","like_count":1,"discussions":[{"author":{"id":1502409,"avatar":"https://static001.geekbang.org/account/avatar/00/16/ec/c9/45bfcba3.jpg","nickname":"Xinyue Li","note":"","ucode":"0CB92610C8B9FC","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":450156,"discussion_content":"就是说，分区本身不会存储数据，它只是有一个指向真正存储数据位置的reference。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1567392855,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":1}","child_discussion_number":0,"child_discussions":[]}]}]}