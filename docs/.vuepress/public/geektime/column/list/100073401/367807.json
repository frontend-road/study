{"id":367807,"title":"20 | RDD和DataFrame：既生瑜，何生亮？","content":"<p>你好，我是吴磊。</p><p>从今天开始，我们进入Spark SQL性能调优篇的学习。在这一篇中，我会先带你学习Spark SQL已有的优化机制，如Catalyst、Tungsten这些核心组件，以及AQE、DPP等新特性。深入理解这些内置的优化机制，会让你在开发应用之初就有一个比较高的起点。然后，针对数据分析中的典型场景，如数据关联，我们再去深入探讨性能调优的方法和技巧。</p><p>今天这一讲，我们先来说说RDD和DataFrame的渊源。这也是面试的时候，面试官经常会问的。比如说：“Spark 3.0大版本发布，Spark SQL的优化占比将近50%；而像PySpark、Mllib和Streaming的优化占比都不超过10%，Graph的占比几乎可以忽略不计。这是否意味着Spark社区逐渐放弃了其他计算领域，只专注于数据分析？”</p><p><a href=\"https://databricks.com/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html\"><img src=\"https://static001.geekbang.org/resource/image/47/75/479cd67e687a3a7cdc805b55b5bdef75.jpg?wh=1600*1358\" alt=\"\"></a></p><p>这个问题的标准答案是：“Spark SQL取代Spark Core，成为新一代的引擎内核，所有其他子框架如Mllib、Streaming和Graph，都可以共享Spark SQL的性能优化，都能从Spark社区对于Spark SQL的投入中受益。”不过，面试官可没有那么好对付，一旦你这么说，他/她可能会追问：“为什么需要Spark SQL这个新一代引擎内核？Spark Core有什么问题吗？Spark SQL解决了Spark Core的哪些问题？怎么解决的？”</p><!-- [[[read_end]]] --><p>面对这一连串“箭如雨发”的追问，你还能回答出来吗？接下来，我就从RDD的痛点说起，一步一步带你探讨DataFrame出现的必然性，Spark Core的局限性，以及它和Spark SQL的关系。</p><h2>RDD之痛：优化空间受限</h2><p>自从Spark社区在1.3版本发布了DataFrame，它就开始代替RDD，逐渐成为开发者的首选。我们知道，新抽象的诞生一定是为了解决老抽象不能搞定的问题。那么，这些问题都是什么呢？下面，我们就一起来分析一下。</p><p>在RDD的开发框架下，我们调用RDD算子进行适当的排列组合，就可以很轻松地实现业务逻辑。我把这些使用频繁的RDD算子总结到了下面的表格里，你可以看一看。</p><p><a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\"><img src=\"https://static001.geekbang.org/resource/image/79/d3/790eaa488a4db5950a11da89643232d3.jpeg?wh=1442*1080\" alt=\"\"></a></p><p>表格中高亮显示的就是RDD转换和聚合算子，它们都是高阶函数。高阶函数指的是形参包含函数的函数，或是返回结果包含函数的函数。为了叙述方便，我们把那些本身是高阶函数的RDD算子，简称“高阶算子”。</p><p><strong>对于这些高阶算子，开发者需要以Lambda函数的形式自行提供具体的计算逻辑。</strong>以map为例，我们需要明确对哪些字段做映射，以什么规则映射。再以filter为例，我们需要指明以什么条件在哪些字段上过滤。</p><p>但这样一来，Spark只知道开发者要做map、filter，但并不知道开发者打算怎么做map和filter。也就是说，<strong>在RDD的开发模式下，Spark Core只知道“做什么”，而不知道“怎么做”。</strong>这会让Spark Core两眼一抹黑，除了把Lambda函数用闭包的形式打发到Executors以外，实在是没有什么额外的优化空间。</p><p><strong>对于Spark Core来说，优化空间受限最主要的影响，莫过于让应用的执行性能变得低下。</strong>一个典型的例子，就是相比Java或者Scala，PySpark实现的应用在执行性能上相差悬殊。原因在于，在RDD的开发模式下，即便是同一个应用，不同语言实现的版本在运行时也会有着天壤之别。</p><p><img src=\"https://static001.geekbang.org/resource/image/09/f0/095e9191b93b911c463ca88dba6755f0.jpg?wh=3079*1016\" alt=\"\" title=\"不同语言的运行时计算过程\"></p><p>当我们使用Java或者Scala语言做开发时，所有的计算都在JVM进程内完成，如图中左侧的Spark计算节点所示。</p><p>而当我们在PySpark上做开发的时候，只能把由RDD算子构成的计算代码，一股脑地发送给Python进程。Python进程负责执行具体的脚本代码，完成计算之后，再把结果返回给Executor进程。由于每一个Task都需要一个Python进程，如果RDD的并行度为#N，那么整个集群就需要#N个这样的Python进程与Executors交互。不难发现，其中的任务调度、数据计算和数据通信等开销，正是PySpark性能低下的罪魁祸首。</p><h2>DataFrame应运而生</h2><p>针对优化空间受限这个核心问题，Spark社区痛定思痛，在2013年在1.3版本中发布了DataFrame。那么，DataFrame的特点是什么，它和RDD又有什么不同呢？</p><p>首先，用一句话来概括，DataFrame就是携带数据模式（Data Schema）的结构化分布式数据集，而RDD是不带Schema的分布式数据集。<strong>因此，从数据表示（Data Representation）的角度来看，是否携带Schema是它们唯一的区别。</strong>带Schema的数据表示形式决定了DataFrame只能封装结构化数据，而RDD则没有这个限制，所以除了结构化数据，它还可以封装半结构化和非结构化数据。</p><p>其次，从开发API上看，<strong>RDD算子多是高阶函数，这些算子允许开发者灵活地实现业务逻辑，表达能力极强</strong>。</p><p>DataFrame的表达能力却很弱。一来，它定义了一套DSL（Domain Specific Language）算子，如select、filter、agg、groupBy等等。由于DSL语言是为解决某一类任务而专门设计的计算机语言，非图灵完备，因此，语言表达能力非常有限。二来，DataFrame中的绝大多数算子都是标量函数（Scalar Functions），它们的形参往往是结构化的数据列（Columns），表达能力也很弱。</p><p>你可能会问：“相比RDD，DataFrame的表示和表达能力都变弱了，那它是怎么解决RDD优化空间受限的核心痛点呢？”</p><p>当然，仅凭DataFrame在API上的改动就想解决RDD的核心痛点，比登天还难。<strong>DataFrame API最大的意义在于，它为Spark引擎的内核优化打开了全新的空间。</strong></p><p>首先，DataFrame中Schema所携带的类型信息，让Spark可以根据明确的字段类型设计定制化的数据结构，从而大幅提升数据的存储和访问效率。其次，DataFrame中标量算子确定的计算逻辑，让Spark可以基于启发式的规则和策略，甚至是动态的运行时信息去优化DataFrame的计算过程。</p><h2>Spark SQL智能大脑</h2><p>那么问题来了，有了DataFrame API，负责引擎内核优化的那个幕后英雄是谁？为了支持DataFrame开发模式，Spark从1.3版本开始推出Spark SQL。<strong>Spark SQL的核心组件有二，其一是Catalyst优化器，其二是Tungsten。</strong>关于Catalyst和Tungsten的特性和优化过程，我们在后面的两讲再去展开，今天这一讲，咱们专注在它们和DataFrame的关系。</p><h3>Catalyst：执行过程优化</h3><p>我们先来说说Catalyst的优化过程。当开发者通过Actions算子触发DataFrame的计算请求时，Spark内部会发生一系列有趣的事情。</p><p>首先，基于DataFrame确切的计算逻辑，Spark会使用第三方的SQL解析器ANTLR来生成抽象语法树（AST，Abstract Syntax Tree）。既然是树，就会有节点和边这两个基本的构成元素。节点记录的是标量算子（如select、filter）的处理逻辑，边携带的是数据信息：关系表和数据列，如下图所示。这样的语法树描述了从源数据到DataFrame结果数据的转换过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/1c/6f/1c0a5e8c1ccdc5eb6ecc29cc45d3f96f.jpg?wh=1396*1382\" alt=\"\" title=\"AST语法树示意图\"></p><p>在Spark中，语法树还有个别名叫做“Unresolved Logical Plan”。它正是Catalyst优化过程的起点。之所以取名“Unresolved”，是因为边上记录的关系表和数据列仅仅是一些字符串，还没有和实际数据对应起来。举个例子，Filter之后的那条边，输出的数据列是joinKey和payLoad。这些字符串的来源是DataFrame的DSL查询，Catalyst并不确定这些字段名是不是有效的，更不知道每个字段都是什么类型。</p><p>因此，<strong>Catalyst做的第一步优化，就是结合DataFrame的Schema信息，确认计划中的表名、字段名、字段类型与实际数据是否一致</strong>。这个过程也叫做把“Unresolved Logical Plan”转换成“Analyzed Logical Plan”。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/72/f3ffb5fc43ae3c9bca44c1f4f8b7e872.jpg?wh=6539*1200\" alt=\"\" title=\"Spark SQL的端到端优化过程\"></p><p>基于解析过后的“Analyzed Logical Plan”，Catalyst才能继续做优化。利用启发式的规则和执行策略，Catalyst最终把逻辑计划转换为可执行的物理计划。总之，Catalyst的优化空间来源DataFrame的开发模式。</p><h3>Tungsten：数据结构优化</h3><p>说完Catalyst，我接着再来说说Tungsten。在开发原则那一讲，我们提到过Tungsten使用定制化的数据结构Unsafe Row来存储数据，Unsafe Row的优点是存储效率高、GC效率高。Tungsten之所以能够设计这样的数据结构，仰仗的也是DataFrame携带的Schema。Unsafe Row我们之前讲过，这里我再带你简单回顾一下。</p><p><img src=\"https://static001.geekbang.org/resource/image/75/23/75ab3ca00411e1aa3933f3b0b1b3de23.jpeg?wh=1162*715\" alt=\"\" title=\"结构化的二维表\"></p><p>Tungsten是用二进制字节序列来存储每一条用户数据的，因此在存储效率上完胜Java Object。比如说，如果我们要存储上表中的数据，用Java Object来存储会消耗100个字节数，而使用Tungsten仅需要不到20个字节，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/20/02/20230c764200cfde05dedec1cae6b702.jpg?wh=3497*1191\" alt=\"\" title=\"二进制字节序列\"></p><p>但是，要想实现上图中的二进制序列，Tungsten必须要知道数据条目的Schema才行。也就是说，它需要知道每一个字段的数据类型，才能决定在什么位置安放定长字段、安插Offset，以及存放变长字段的数据值。DataFrame刚好能满足这个前提条件。</p><p>我们不妨想象一下，如果数据是用RDD封装的，Tungsten还有可能做到这一点吗？当然不可能。这是因为，虽然RDD也带类型，如RDD[Int]、RDD[(Int, String)]，但如果RDD中携带的是开发者自定义的数据类型，如RDD[User]或是RDD[Product]，Tungsten就会两眼一抹黑，完全不知道你的User和Product抽象到底是什么。成也萧何、败也萧何，RDD的通用性是一柄双刃剑，在提供开发灵活性的同时，也让引擎内核的优化变得无比困难。</p><p><strong>总的来说，基于DataFrame简单的标量算子和明确的Schema定义，借助Catalyst优化器和Tungsten，Spark SQL有能力在运行时构建起一套端到端的优化机制。这套机制运用启发式的规则与策略，以及运行时的执行信息，将原本次优、甚至是低效的查询计划转换为高效的执行计划，从而提升端到端的执行性能。</strong>因此，在DataFrame的开发框架下，不论你使用哪种开发语言，开发者都能共享Spark SQL带来的性能福利。</p><p><img src=\"https://static001.geekbang.org/resource/image/50/fc/505dbb1462dbc1f927fa1f4a2daabcfc.jpg?wh=3385*1032\" alt=\"\" title=\"不同开发模式下的分布式执行过程\"></p><p>最后，我们再来回顾最开始提到的面试题：“从2.0版本至今，Spark对于其他子框架的完善与优化，相比Spark SQL占比很低。这是否意味着，Spark未来的发展重心是数据分析，其他场景如机器学习、流计算会逐渐边缘化吗？”</p><p>最初，Spark SQL确实仅仅是运行SQL和DataFrame应用的子框架，但随着优化机制的日趋完善，Spark SQL逐渐取代Spark Core，演进为新一代的引擎内核。到目前为止，所有子框架的源码实现都已从RDD切换到DataFrame。因此，和PySpark一样，像Streaming、Graph、Mllib这些子框架实际上都是通过DataFrame API运行在Spark SQL之上，它们自然可以共享Spark SQL引入的种种优化机制。</p><p>形象地说，Spark SQL就像是Spark的智能大脑，凡是通过DataFrame这双“眼睛”看到的问题，都会经由智能大脑这个指挥中心，统筹地进行分析与优化，优化得到的行动指令，最终再交由Executors这些“四肢”去执行。</p><h2>小结</h2><p>今天，我们围绕RDD的核心痛点，探讨了DataFrame出现的必然性，Spark Core的局限性，以及它和Spark SQL的关系，对Spark SQL有了更深刻的理解。</p><p>RDD的核心痛点是优化空间有限，它指的是RDD高阶算子中封装的函数对于Spark来说完全透明，因此Spark对于计算逻辑的优化无从下手。</p><p>相比RDD，DataFrame是携带Schema的分布式数据集，只能封装结构化数据。DataFrame的算子大多数都是普通的标量函数，以消费数据列为主。但是，DataFrame更弱的表示能力和表达能力，反而为Spark引擎的内核优化打开了全新的空间。</p><p>根据DataFrame简单的标量算子和明确的Schema定义，借助Catalyst优化器和Tungsten，Spark SQL有能力在运行时，构建起一套端到端的优化机制。这套机制运用启发式的规则与策略和运行时的执行信息，将原本次优、甚至是低效的查询计划转换为高效的执行计划，从而提升端到端的执行性能。</p><p>在DataFrame的开发模式下，所有子框架、以及PySpark，都运行在Spark SQL之上，都可以共享Spark SQL提供的种种优化机制，这也是为什么Spark历次发布新版本、Spark SQL占比最大的根本原因。</p><h2>每日一练</h2><ol>\n<li>Java Object在对象存储上为什么会有比较大的开销？JVM需要多少个字节才能存下字符串“abcd”？</li>\n<li>在DataFrame的开发框架下， PySpark中还有哪些操作是“顽固分子”，会导致计算与数据在JVM进程与Python进程之间频繁交互？(提示：参考RDD的局限性，那些对Spark透明的计算逻辑，Spark是没有优化空间的)</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","neighbors":{"left":{"article_title":"19 | 网络视角：如何有效降低网络开销？","id":366265},"right":{"article_title":"21 | Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）","id":367832}},"comments":[{"had_liked":false,"id":290872,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1619797508,"is_pvip":false,"replies":[{"id":"105413","content":"Perfect x 4 ！ 理解得非常到位了~<br><br>老弟的概括能力很强，确实，把握这4点，关于RDD和DataFrame的区别与联系，其实就算是吃透了。<br><br>关于你说的第二点，确实，Spark Core实际就是分布式运行时，负责分布式任务调度、执行，严格来说，并不存在什么优化机制，仅仅是分发分布式代码、运行、状态交互，等等。<br><br>第四点说得尤其到位，大赞👍","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619861899,"ip_address":"","comment_id":290872,"utype":1}],"discussion_count":2,"race_medal":0,"score":"44569470468","product_id":100073401,"comment_content":"对RDD和DataFrame的理解<br>1.首先区分这两个东西，RDD分布式数据集;DF是带数据模式的结构化分布式数据集。核心区别在于DF带数据模式(感觉像传统分布式数据库中的一张表)，RDD不带数据模式或者说是泛型的。<br>2.其次是这两者分别提供的API，RDD API优化引擎是Spark Core(没理解这句话，我理解Spark Core负责task调度计算存储，和优化的联系在哪呢？);DF API优化引擎是SparkSQL，包括Catalyst执行过程优化和Tungsten数据结构优化。两者API的区别在于一个提供标量算子一个高阶算子和两者底层优化引擎不一致。<br>3.上面两组概念应该区分开。之前子框架如Streaming，mlib,graph都是采用RDD API来编写，现在都是采用DF API来重新编写。<br>4.调用DF API生成DF，但DF 的action算子触发执行后最终还是生成RDD，通过Spark Core框架来进行调度计算。所以可不可以认为DF API+SparkSQL就是代替了之前的RDD API？目的就是为了提供更简单的API，让Spark做统一优化，在rdd计算时更高效？","like_count":11,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519303,"discussion_content":"Perfect x 4 ！ 理解得非常到位了~\n\n老弟的概括能力很强，确实，把握这4点，关于RDD和DataFrame的区别与联系，其实就算是吃透了。\n\n关于你说的第二点，确实，Spark Core实际就是分布式运行时，负责分布式任务调度、执行，严格来说，并不存在什么优化机制，仅仅是分发分布式代码、运行、状态交互，等等。\n\n第四点说得尤其到位，大赞👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619861899,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1549032,"avatar":"","nickname":"Zzz","note":"","ucode":"9323254354868B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":378724,"discussion_content":"我也是觉得，拿spark core和sparksql当做一种东西进行对比是不合适的，为什么不称之为RDD计算和DF计算（sparksql）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1623374012,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290627,"user_name":"sky_sql","can_delete":false,"product_type":"c1","uid":1099273,"ip_address":"","ucode":"397F4263C9E590","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/09/7f2bcc6e.jpg","comment_is_top":false,"comment_ctime":1619665198,"is_pvip":false,"replies":[{"id":"105321","content":"对，没错，DataFrame的code，经过Spark SQL优化之后，最终交由Tungsten生成代码和RDD[InternalRow]，代码交由DAGScheduler进行分发、运行在RDD[InternalRow]之上。<br><br>后面的21、22、23会详细展开从DataFrame到Catalyst、再到Tungsten，最终生成代码和RDD的过程，可以重点关注下哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619673593,"ip_address":"","comment_id":290627,"utype":1}],"discussion_count":1,"race_medal":0,"score":"35979403566","product_id":100073401,"comment_content":"老师好，DataFrame和RDD这两套独立的API，最终还是会转化为RDD之上的计算吧？","like_count":9,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519249,"discussion_content":"对，没错，DataFrame的code，经过Spark SQL优化之后，最终交由Tungsten生成代码和RDD[InternalRow]，代码交由DAGScheduler进行分发、运行在RDD[InternalRow]之上。\n\n后面的21、22、23会详细展开从DataFrame到Catalyst、再到Tungsten，最终生成代码和RDD的过程，可以重点关注下哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619673593,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":293061,"user_name":"CycleGAN","can_delete":false,"product_type":"c1","uid":1679661,"ip_address":"","ucode":"9FD04813911A02","user_header":"https://static001.geekbang.org/account/avatar/00/19/a1/2d/599e9051.jpg","comment_is_top":false,"comment_ctime":1621178165,"is_pvip":false,"replies":[{"id":"106287","content":"好问题，UDF本身的开销其实大家有目共睹。不过对于你的cases来说，其实没办法武断地说用Scala重写效果如何。因为就像咱们最开始讲调优方法论的时候一样，其实这取决于UDF本身是不是整体作业的瓶颈。<br><br>对于UDF的优化，我觉得不妨这样，就是从你众多的UDF中，选中一个开销最大的，或者至少是“看上去”开销最大的，然后用Scala优化，对比前后作业端到端的执行效果，然后再去决定，要不要对其他UDF做同样的优化。<br><br>另外，对于复杂的业务逻辑，如果DSL和SQL都无法实现，除了UDF，其实还可以考虑用Script Transformation，这块Facebook有个最佳实践，可以参考下，看看对你们是否有帮助：https:&#47;&#47;databricks.com&#47;session_eu19&#47;powering-custom-apps-at-facebook-using-spark-script-transformation<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1621413550,"ip_address":"","comment_id":293061,"utype":1}],"discussion_count":5,"race_medal":0,"score":"23096014645","product_id":100073401,"comment_content":"老师好，我们的业务有很复杂的计算udf，用dsl或者sql无法描述，PySpark嵌入udf，使得优化无能为力，性能也一直很低，我们对于反复使用的数学操作会编译成so执行，也在提升udf本身的执行效率，请问将python的udf改写成scala提升会大吗？老师对于我们写复杂udf有什么建议吗？","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520053,"discussion_content":"好问题，UDF本身的开销其实大家有目共睹。不过对于你的cases来说，其实没办法武断地说用Scala重写效果如何。因为就像咱们最开始讲调优方法论的时候一样，其实这取决于UDF本身是不是整体作业的瓶颈。\n\n对于UDF的优化，我觉得不妨这样，就是从你众多的UDF中，选中一个开销最大的，或者至少是“看上去”开销最大的，然后用Scala优化，对比前后作业端到端的执行效果，然后再去决定，要不要对其他UDF做同样的优化。\n\n另外，对于复杂的业务逻辑，如果DSL和SQL都无法实现，除了UDF，其实还可以考虑用Script Transformation，这块Facebook有个最佳实践，可以参考下，看看对你们是否有帮助：https://databricks.com/session_eu19/powering-custom-apps-at-facebook-using-spark-script-transformation\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621413550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1690447,"avatar":"https://static001.geekbang.org/account/avatar/00/19/cb/4f/7466a488.jpg","nickname":"Pytrick","note":"","ucode":"2759D2BB51D981","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537426,"discussion_content":"用Pandas UDF取代普通UDF， 效率提升几倍到几十倍 \n  https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html#:~:text=The%20UDF%20definitions%20are%20the,defined%20function%20takes%20a%20pandas.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639057781,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1300070,"avatar":"https://static001.geekbang.org/account/avatar/00/13/d6/66/62206d01.jpg","nickname":"超级丹","note":"","ucode":"6735EF97F10649","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532815,"discussion_content":"pyspark重度用户来说两句。\n\n1. 上面老师提到的 facebook的 transfomer 本质还是pip，不过他们的pip用c++写，而pyspark用python写。最新的spark3.2 已经完全支持了这种语法，实际就是hive 里面的 transform语法。\n\n2. pyspark的udf，本质还是通过pip调用py脚本，这个操作和在 rdd上调用 mappartition类似。经过对比，mappartition的内存使用会比 pip 更低，运行更稳。\n\n3. 性能问题。实际开发中，为了提升效率，python pip里面的核心逻辑会用c++实现为so，加速运算过程。这个在多数情况下和scala性能相当。\n我们的情况，底层核心逻辑用c++实现，上层通过pybind11把c++编译也py扩展；通过javacpp把c++编译为java扩展。业务同学使用scala或者pyspark做业务开发。pyspark除了报错比较诡异出问题不好排查，运行不那么稳定外，其他都还行。我们一般数据探索用pyspark，正式上线还是用scala，scala是真的稳定，而且更省内存（不容易报oom）。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637715477,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":1,"child_discussions":[{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1300070,"avatar":"https://static001.geekbang.org/account/avatar/00/13/d6/66/62206d01.jpg","nickname":"超级丹","note":"","ucode":"6735EF97F10649","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":565236,"discussion_content":"瞎比说， 你先搞清楚一个问题，pip是啥，pip是包管理器！！！！！\n“pip 调用python脚本”就相当于说“maven调用java程序” ，这不是扯淡么\n\npyspark之所以快了，只因为，SQL统一外部接口、统一下层优化，py4j 上性能消耗比例降低了\n\n跟C++有毛关系，那叫interpreter好吧，那个是python语言的进化，跟spark有毛关系","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650418548,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":532815,"ip_address":""},"score":565236,"extra":""}]},{"author":{"id":1529153,"avatar":"https://static001.geekbang.org/account/avatar/00/17/55/41/b68df312.jpg","nickname":"农夫三拳","note":"","ucode":"9778194A7CD44E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":379754,"discussion_content":"老师您好，上面这位同学的问题，如果使用Spark extensions 提供的函数扩展，效果会不会比UDF性能好呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1624107756,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290682,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1619685565,"is_pvip":false,"replies":[{"id":"105344","content":"都对，或者换句话说，只要是走Spark SQL的流程（DataFrame、Dataset、SQL三种API），就都能利用到Tungsten的这种二进制Unsafe Row。比如，就像你说的：<br><br>1）缓存的时候，可以利用得到，Tungsten用Unsafe Row来封装每条数据记录；<br>2）数据源读进来的时候（当然这么说不太严谨，严格来说，应该是Tungsten生成的“手写代码”交付执行的时候，在运行时执行读取数据源的时候，会把它convert成Tungsten的二进制Unsafe Row）；<br>3）Shuffle的时候也是一样，不仅如此，Shuffle还能利用文中介绍的Memory Page（一个JVM对象），来进一步提升内存的利用效率。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619695267,"ip_address":"","comment_id":290682,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23094522045","product_id":100073401,"comment_content":"磊哥，Tungsten 使用定制化的数据结构 Unsafe Row 来存储数据，是不是指的cache缓存的时候使用Unsafe Row存储的数据，或是shuffle溢写到磁盘的时候存储的结构，还是说从数据源读进来的时候会转化成Unsafe Row的结构？","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519261,"discussion_content":"都对，或者换句话说，只要是走Spark SQL的流程（DataFrame、Dataset、SQL三种API），就都能利用到Tungsten的这种二进制Unsafe Row。比如，就像你说的：\n\n1）缓存的时候，可以利用得到，Tungsten用Unsafe Row来封装每条数据记录；\n2）数据源读进来的时候（当然这么说不太严谨，严格来说，应该是Tungsten生成的“手写代码”交付执行的时候，在运行时执行读取数据源的时候，会把它convert成Tungsten的二进制Unsafe Row）；\n3）Shuffle的时候也是一样，不仅如此，Shuffle还能利用文中介绍的Memory Page（一个JVM对象），来进一步提升内存的利用效率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619695267,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291342,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620218654,"is_pvip":false,"replies":[{"id":"105523","content":"对没错，除此之外，还有万恶的UDF。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620226669,"ip_address":"","comment_id":291342,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14505120542","product_id":100073401,"comment_content":"PySpark应用中如果用到了Python里面的数据结构都会导致在JVM进程和Python进程间进行交互的吧","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519445,"discussion_content":"对没错，除此之外，还有万恶的UDF。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620226669,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291051,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1619965812,"is_pvip":true,"replies":[{"id":"105477","content":"是的，你说的这些开销都对，不过“ABCD”在JVM中实际需要48个字节才能存下哈~ 我怀疑你是不是漏打了个4，哈哈~<br><br>具体计算细节可以参考这里：https:&#47;&#47;databricks.com&#47;blog&#47;2015&#47;04&#47;28&#47;project-tungsten-bringing-spark-closer-to-bare-metal.html","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620115250,"ip_address":"","comment_id":291051,"utype":1}],"discussion_count":4,"race_medal":0,"score":"10209900404","product_id":100073401,"comment_content":"Java Object 在对象存储上为什么会有比较大的开销？<br>我查阅了了一些资料，Java对象的存储在JVM上至少需要32或64Bit的字节存储对象自身信息，哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳。<br>&quot;ABCD&quot;的存储至少需要8个字节。","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519354,"discussion_content":"是的，你说的这些开销都对，不过“ABCD”在JVM中实际需要48个字节才能存下哈~ 我怀疑你是不是漏打了个4，哈哈~\n\n具体计算细节可以参考这里：https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620115250,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2486715,"avatar":"https://static001.geekbang.org/account/avatar/00/25/f1/bb/547b580a.jpg","nickname":"苏子浩","note":"","ucode":"8A842C9C2E53E8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372957,"discussion_content":"我看了“具体计算细节可以参考链接”，可是依旧没能算出48个字节？12字节的header加8字节的hash code，再加上每个字符的2字节（4 * 2），这么算不应该是28字节吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620539833,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2486715,"avatar":"https://static001.geekbang.org/account/avatar/00/25/f1/bb/547b580a.jpg","nickname":"苏子浩","note":"","ucode":"8A842C9C2E53E8","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":565247,"discussion_content":"你再好好看看吧\n\nString是个对象啊，又不是基础类型。\n对象是24+字节，考虑类型指针压缩\nchar[] 包括 [c （char[].class）和长度，24字节","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650420441,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":372957,"ip_address":""},"score":565247,"extra":""}]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372255,"discussion_content":"好的，好的。感谢老师，我再去看看","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620261106,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":319250,"user_name":"强","can_delete":false,"product_type":"c1","uid":1035643,"ip_address":"","ucode":"D0A761E25B0740","user_header":"https://static001.geekbang.org/account/avatar/00/0f/cd/7b/31a6bf42.jpg","comment_is_top":false,"comment_ctime":1635696809,"is_pvip":false,"replies":[{"id":"115946","content":"好问题~ <br>DataFrame和Dataset的关系，非常紧密。<br><br>首先，DataFrame实际上是Dataset[Row]，不论是DataFrame、还是Dataset，他们都能享受到Spark SQL带来的性能红利，因此在执行效率上，二者没有区别。<br><br>因此，两者的区别，最主要的，还是在开发效率。DataFrame其实就是二维表，承载结构化数据。Dataset可以用来处理非结构化数据，但是Dataset需要用户明确定义数据类型，比如自定义的Person、School之类的。<br><br>所以总结下来，如果需要处理结构化数据，一般大家都会用DataFrame，因为简单直接，不需要自定义User Class，省去这一步的麻烦。但如果需要从（非结构化）日志解析一些内容，往往会用Dataset先把数据抽出来，得到结构化数据之后，再考虑转成DataFrame，方便后续处理。<br><br>一言以蔽之，没有优劣之分，只有不同的适应场景。一般来说，DataFrame用的偏多，因为大部分情况，我们处理的都是结构化数据。相应地，Dataset用的就会少一些。另外，Dataset取代DataFrame这种说法，听听就好了，不用特别当真~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635954135,"ip_address":"","comment_id":319250,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5930664105","product_id":100073401,"comment_content":"曾经看到一个视频,说是Dataset后续要取代DataFrame。Dataset和DataFrame这两者之间有什么关系？实际项目中用的哪个偏多些？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529555,"discussion_content":"好问题~ \nDataFrame和Dataset的关系，非常紧密。\n\n首先，DataFrame实际上是Dataset[Row]，不论是DataFrame、还是Dataset，他们都能享受到Spark SQL带来的性能红利，因此在执行效率上，二者没有区别。\n\n因此，两者的区别，最主要的，还是在开发效率。DataFrame其实就是二维表，承载结构化数据。Dataset可以用来处理非结构化数据，但是Dataset需要用户明确定义数据类型，比如自定义的Person、School之类的。\n\n所以总结下来，如果需要处理结构化数据，一般大家都会用DataFrame，因为简单直接，不需要自定义User Class，省去这一步的麻烦。但如果需要从（非结构化）日志解析一些内容，往往会用Dataset先把数据抽出来，得到结构化数据之后，再考虑转成DataFrame，方便后续处理。\n\n一言以蔽之，没有优劣之分，只有不同的适应场景。一般来说，DataFrame用的偏多，因为大部分情况，我们处理的都是结构化数据。相应地，Dataset用的就会少一些。另外，Dataset取代DataFrame这种说法，听听就好了，不用特别当真~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635954135,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291047,"user_name":"Z宇锤锤","can_delete":false,"product_type":"c1","uid":2188142,"ip_address":"","ucode":"7DB36E986A7A51","user_header":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","comment_is_top":false,"comment_ctime":1619964371,"is_pvip":true,"replies":[{"id":"105475","content":"DF确实就是二维表，和RDD相比，除了API的区别之外，最大的区别，还是DF可以走Spark SQL做优化~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620114827,"ip_address":"","comment_id":291047,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5914931667","product_id":100073401,"comment_content":"RDD和DataFrame的最大区别，DF像是一张二维表，可以用来写SQL。RDD只能使用算子。","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519351,"discussion_content":"DF确实就是二维表，和RDD相比，除了API的区别之外，最大的区别，还是DF可以走Spark SQL做优化~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620114827,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372254,"discussion_content":"感谢大佬翻牌子","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620261069,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290683,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1619685714,"is_pvip":false,"replies":[{"id":"105345","content":"确实有相似之处，毕竟都是用二进制的形式来存储数据，所以会有相通的地方。<br><br>不过，两者没啥联系哈，实际上，我倒是觉得，Parquet的数据结构，要比Tungsten的复杂得多。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619695476,"ip_address":"","comment_id":290683,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5914653010","product_id":100073401,"comment_content":"Tungsten的这种存储数据结构，感觉和parquet类似，他们之间有什么关联吗","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519262,"discussion_content":"确实有相似之处，毕竟都是用二进制的形式来存储数据，所以会有相通的地方。\n\n不过，两者没啥联系哈，实际上，我倒是觉得，Parquet的数据结构，要比Tungsten的复杂得多。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619695476,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":565248,"discussion_content":"Tungsten是一个计划、是一个计划、是一个计划，不是存储、不是存储、不是存储\n\n换言之，是一系列优化的合集！！！！！！！是一个特性迭代！！！！！！！！\n\n二进制存储只是其中给一个优化点而已","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650420586,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290500,"user_name":"sky_sql","can_delete":false,"product_type":"c1","uid":1099273,"ip_address":"","ucode":"397F4263C9E590","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/09/7f2bcc6e.jpg","comment_is_top":false,"comment_ctime":1619592659,"is_pvip":false,"replies":[{"id":"105296","content":"DataFrame底层不是RDD实现的，DataFrame和RDD是两套独立的API。区别在于，RDD的优化引擎是Spark Core；而DataFrame的优化引擎是Spark SQL，这个我们后面3讲：21、22、23会详细的介绍哈~<br><br>对于开发者来说，强烈推荐DataFrame开发API，放弃RDD开发API。Spark SQL的优化机制远胜于Spark Core，换句话说，同样的业务应用，仅仅是API不同，你用DataFrame开发的code，天然地就比你用RDD开发的code，在执行效率上面更高，啥都不用调优，天然就更快。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619624280,"ip_address":"","comment_id":290500,"utype":1}],"discussion_count":3,"race_medal":0,"score":"5914559955","product_id":100073401,"comment_content":"RDD是Spark对于分布式数据模型的抽象，调度、存储都是RDD维度的，DataFrame底层是使用RDD的算子实现的吧？<br>对于普通开发者后面使用DataFrame实现业务逻辑，尽量不使用RDD？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519218,"discussion_content":"DataFrame底层不是RDD实现的，DataFrame和RDD是两套独立的API。区别在于，RDD的优化引擎是Spark Core；而DataFrame的优化引擎是Spark SQL，这个我们后面3讲：21、22、23会详细的介绍哈~\n\n对于开发者来说，强烈推荐DataFrame开发API，放弃RDD开发API。Spark SQL的优化机制远胜于Spark Core，换句话说，同样的业务应用，仅仅是API不同，你用DataFrame开发的code，天然地就比你用RDD开发的code，在执行效率上面更高，啥都不用调优，天然就更快。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619624280,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375924,"discussion_content":"最终却是会转化为RDD[InternalRow]上的计算，但是在此之前Spark SQL对DataFrame做的所有优化，和RDD是没什么关系的","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1621869794,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1179297,"avatar":"","nickname":"kris37","note":"","ucode":"65555B42CFF0A1","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":371148,"discussion_content":"SparkSQL还是生成的RDD计划吧","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1619664377,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":334430,"user_name":"October","can_delete":false,"product_type":"c1","uid":1137879,"ip_address":"","ucode":"CEDA78F4A5F8B1","user_header":"https://static001.geekbang.org/account/avatar/00/11/5c/d7/e4673fde.jpg","comment_is_top":false,"comment_ctime":1644931826,"is_pvip":false,"replies":[{"id":"122403","content":"标量函数，是和“高阶函数”相对的。简单理解，标量函数，就是普通函数，计算逻辑是确定的，比如select，就是数据列投影，比如add，就是两个数值相加。高阶函数不同，比如map、filter这些，Spark并不知道map、filter里面，用的是什么函数，什么映射逻辑，什么过滤逻辑，所以没法优化。对于像select、add，这些确定的逻辑，Spark SQL可以根据AST（语法树）还有一系列的优化规则，把开发者原来的代码，一步步优化，进而提高执行性能，可以关注后面的Catalyst和Tungsten","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1645369843,"ip_address":"","comment_id":334430,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1644931826","product_id":100073401,"comment_content":"老师好，还是不是很理解什么是标量函数和标量算子。为什么标量算子就即能知道做什么，又能知道怎么做","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552262,"discussion_content":"标量函数，是和“高阶函数”相对的。简单理解，标量函数，就是普通函数，计算逻辑是确定的，比如select，就是数据列投影，比如add，就是两个数值相加。高阶函数不同，比如map、filter这些，Spark并不知道map、filter里面，用的是什么函数，什么映射逻辑，什么过滤逻辑，所以没法优化。对于像select、add，这些确定的逻辑，Spark SQL可以根据AST（语法树）还有一系列的优化规则，把开发者原来的代码，一步步优化，进而提高执行性能，可以关注后面的Catalyst和Tungsten","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645369844,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":565249,"discussion_content":"老师说的不对，你想了解，可以看一下scala函数式编程小红书\n\n具体可以了解一下：Monoid，Functor，Applicative， Monad","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650420701,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":325643,"user_name":"Pytrick","can_delete":false,"product_type":"c1","uid":1690447,"ip_address":"","ucode":"2759D2BB51D981","user_header":"https://static001.geekbang.org/account/avatar/00/19/cb/4f/7466a488.jpg","comment_is_top":false,"comment_ctime":1639057500,"is_pvip":false,"replies":[{"id":"118186","content":"没错，老弟这一点提的相当好~<br><br>确实，pandas udf可以比普通的UDF快几倍到十几倍，确实是这样，之前直播的时候有提过，正文居然忘了说了，哈哈~<br><br>加餐回头统一考虑考虑，Spark可以加餐的内容太多了，比如最近的delta lake、project hydrogen、koalas，包括3.2的新特性，等等，回头统一考虑下~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639149904,"ip_address":"","comment_id":325643,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1639057500","product_id":100073401,"comment_content":"老师， 我觉得有个重要的优化没有讲到呀， 用pyspark + data frame开发的时候，使用pandas udf可以比普通的UDF快几倍到十几倍 。  可以加餐一个pandas UDF的使用？ ","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537724,"discussion_content":"没错，老弟这一点提的相当好~\n\n确实，pandas udf可以比普通的UDF快几倍到十几倍，确实是这样，之前直播的时候有提过，正文居然忘了说了，哈哈~\n\n加餐回头统一考虑考虑，Spark可以加餐的内容太多了，比如最近的delta lake、project hydrogen、koalas，包括3.2的新特性，等等，回头统一考虑下~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1639149904,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":324871,"user_name":"wayne","can_delete":false,"product_type":"c1","uid":1811813,"ip_address":"","ucode":"988E1F2F0B419A","user_header":"https://static001.geekbang.org/account/avatar/00/1b/a5/65/898bc6c5.jpg","comment_is_top":false,"comment_ctime":1638695832,"is_pvip":true,"replies":[{"id":"118128","content":"现在已经不存在了哈（Spark 2.0+），现在底层都是Spark SQL优化引擎，Python的性能和Scala的差不多的，虽说还有些差异，但是没那么显著了~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1639046246,"ip_address":"","comment_id":324871,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1638695832","product_id":100073401,"comment_content":"老师，既然pyspark和spark(scala)性能差距巨大，那么pyspark-sql和spark-sql(sql)的性能差异也是巨大吗？目前我们公司采用的是向yarn集群中提交.py文件，然后使用pyspark =&gt; spark.sql()的方式执行sql","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":537382,"discussion_content":"现在已经不存在了哈（Spark 2.0+），现在底层都是Spark SQL优化引擎，Python的性能和Scala的差不多的，虽说还有些差异，但是没那么显著了~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1639046246,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":1811813,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/a5/65/898bc6c5.jpg","nickname":"wayne","note":"","ucode":"988E1F2F0B419A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":540915,"discussion_content":"那老师，想问一下Scala这门语言你怎么看，Scala会不会没落 以后的大数据  以致于没人用","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1640192672,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":537382,"ip_address":""},"score":540915,"extra":""},{"author":{"id":1756909,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKDe7Ep3YW87rib33lhErnnkEsdfOJUDlDgHDs9Nic88FHU8s2feX6preYqR46TDsOMW7Eib6ddUyr0A/132","nickname":"我爱搞钱","note":"","ucode":"CBB1DF3ACE1026","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1811813,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/a5/65/898bc6c5.jpg","nickname":"wayne","note":"","ucode":"988E1F2F0B419A","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":565250,"discussion_content":"你先把 java学号了，在说scala吧","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1650420772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":540915,"ip_address":""},"score":565250,"extra":""}]}]}]}