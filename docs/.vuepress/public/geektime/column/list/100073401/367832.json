{"id":367832,"title":"21 | Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）","content":"<p>你好，我是吴磊。</p><p>上一讲我们说，Spark SQL已经取代Spark Core成为了新一代的内核优化引擎，所有Spark子框架都能共享Spark SQL带来的性能红利，所以在Spark历次发布的新版本中，Spark SQL占比最大。因此，Spark SQL的优化过程是我们必须要掌握的。</p><p>Spark SQL端到端的完整优化流程主要包括两个阶段：Catalyst优化器和Tungsten。其中，Catalyst优化器又包含逻辑优化和物理优化两个阶段。为了把开发者的查询优化到极致，整个优化过程的运作机制设计得都很精密，因此我会用三讲的时间带你详细探讨。</p><p>下图就是这个过程的完整图示，你可以先通过它对优化流程有一个整体的认知。然后随着我的讲解，逐渐去夯实其中的关键环节、重要步骤和核心知识点，在深入局部优化细节的同时，把握全局优化流程，做到既见树木、也见森林。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/72/f3ffb5fc43ae3c9bca44c1f4f8b7e872.jpg?wh=6539*1200\" alt=\"\" title=\"Spark SQL的优化过程\"></p><p>今天这一讲，我们先来说说Catalyst优化器逻辑优化阶段的工作原理。</p><h2>案例：小Q变身记</h2><p>我们先来看一个例子，例子来自电子商务场景，业务需求很简单：给定交易事实表transactions和用户维度表users，统计不同用户的交易额，数据源以Parquet的格式存储在分布式文件系统。因此，我们要先用Parquet API读取源文件。</p><!-- [[[read_end]]] --><pre><code>val userFile: String = _\nval usersDf = spark.read.parquet(userFile)\nusersDf.printSchema\n/**\nroot\n|-- userId: integer (nullable = true)\n|-- name: string (nullable = true)\n|-- age: integer (nullable = true)\n|-- gender: string (nullable = true)\n|-- email: string (nullable = true)\n*/\nval users = usersDf\n.select(&quot;name&quot;, &quot;age&quot;, &quot;userId&quot;)\n.filter($&quot;age&quot; &lt; 30)\n.filter($&quot;gender&quot;.isin(&quot;M&quot;))\n\nval txFile: String = _\nval txDf = spark.read.parquet(txFile)\ntxDf.printSchema\n/**\nroot\n|-- itemId: integer (nullable = true)\n|-- userId: integer (nullable = true)\n|-- price: float (nullable = true)\n|-- quantity: integer (nullable = true)\n*/\n\nval result = txDF.select(&quot;price&quot;, &quot;volume&quot;, &quot;userId&quot;)\n.join(users, Seq(&quot;userId&quot;), &quot;inner&quot;)\n.groupBy(col(&quot;name&quot;), col(&quot;age&quot;)).agg(sum(col(&quot;price&quot;) * col(&quot;volume&quot;)).alias(&quot;revenue&quot;))\n\nresult.write.parquet(&quot;_&quot;)\n</code></pre><p>代码示例如上图所示，为了实现业务逻辑，我们对过滤之后的用户表与交易表做内关联，然后再按照用户分组去计算交易额。不难发现，这个计算逻辑实际上就是星型数仓中典型的关联查询。为了叙述方便，我们给这个关联查询起个名字：小Q。小Q的计算需要两个输入源，一个是交易表，另一个是过滤之后的用户表。今天这一讲，我们就去追随小Q，看看它在Catalyst的逻辑优化阶段都会发生哪些变化。</p><p><img src=\"https://static001.geekbang.org/resource/image/20/00/205a81e95f483ce66ed019b4120ec000.jpg?wh=1281*1556\" alt=\"\" title=\"小Q的计算逻辑\"></p><p><strong>Catalyst逻辑优化阶段分为两个环节：逻辑计划解析和逻辑计划优化。在逻辑计划解析中，Catalyst把“Unresolved Logical Plan”转换为“Analyzed Logical Plan”；在逻辑计划优化中，Catalyst基于一些既定的启发式规则（Heuristics Based Rules），把“Analyzed Logical Plan”转换为“Optimized Logical Plan”。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/0e/a3/0efa02fd8eda5a69c794871ea77030a3.jpg?wh=3436*1130\" alt=\"\" title=\"Catalyst逻辑优化阶段\"></p><p>因为“Unresolved Logical Plan”是Catalyst优化的起点，所以在进入Catalyst优化器之前，小Q先是改头换面，从代码中的查询语句，摇身变成了“Unresolved Logical Plan”。</p><p><img src=\"https://static001.geekbang.org/resource/image/ff/80/fff906004736005de9c83cbfc09d8380.png?wh=1306*390\" alt=\"\" title=\"小Q启程：Unresolved Logical Plan\"></p><h2>逻辑计划解析</h2><p>小Q成功进入Catalyst优化器之后，就要开始执行逻辑计划解析，也就是要从“Unresolved Logical Plan”转换为“Analyzed Logical Plan”。那么，具体该怎么做呢？</p><p>从“小Q启程”那张图我们不难发现，“Unresolved Logical Plan”携带的信息相当有限，它只包含查询语句从DSL语法变换成AST语法树的信息。需要说明的是，不论是逻辑计划还是物理计划，执行的次序都是自下向上。因此，图中逻辑计划的计算顺序是从全表扫描到按性别过滤，每个步骤的含义都是准备“做什么”。</p><p>例如，在计划的最底层，Relation节点“告诉”Catalyst：“你需要扫描一张表，这张表有4个字段，分别是ABCD，文件格式是Parquet”。但这些信息对于小Q的优化还远远不够，我们还需要知道这张表的Schema是啥？字段的类型都是什么？字段名是否真实存在？数据表中的字段名与计划中的字段名是一致的吗？</p><p>因此，<strong>在逻辑计划解析环节，Catalyst就是要结合DataFrame的Schema信息，来确认计划中的表名、字段名、字段类型与实际数据是否一致。</strong>完成确认之后，Catalyst会生成“Analyzed Logical Plan”。这个时候，小Q就会从“Unresolved Logical Plan”转换成“Analyzed Logical Plan”。</p><p>从下图中我们能够看到，逻辑计划已经完成了一致性检查，并且可以识别两张表的字段类型，比如userId的类型是int，price字段的类型是double等等。</p><p><img src=\"https://static001.geekbang.org/resource/image/ac/7b/ac83f9d0cdab8655a5fffc9125a93f7b.png?wh=1564*438\" alt=\"\" title=\"小Q再变身：Analyzed Logical Plan\"></p><h2>逻辑计划优化</h2><p>对于现在的小Q来说，如果我们不做任何优化，直接把它转换为物理计划也可以。但是，这种照搬开发者的计算步骤去制定物理计划的方式，它的执行效率往往不是最优的。</p><p>为什么这么说呢？在运行时，Spark会先全量扫描Parquet格式的用户表，然后遴选出userId、name、age、gender四个字段，接着分别按照年龄和性别对数据进行过滤。</p><p>对于这样的执行计划来说，最开始的全量扫描显然是一种浪费。原因主要有两方面：一方面，查询实际上只涉及4个字段，并不需要email这一列数据；另一方面，字段age和gender上带有过滤条件，我们完全可以利用这些过滤条件减少需要扫描的数据量。</p><p>由此可见，<strong>对于同样一种计算逻辑，实现方式可以有多种，按照不同的顺序对算子做排列组合，我们就可以演化出不同的实现方式。最好的方式是，我们遵循“能省则省、能拖则拖”的开发原则，去选择所有实现方式中最优的那个</strong>。</p><p>同样，在面对这种“选择题”的时候，Catalyst也有一套自己的“原则”和逻辑。因此，生成“Analyzed Logical Plan”之后，Catalyst并不会止步于此，它会基于一套启发式的规则，把“Analyzed Logical Plan”转换为“Optimized Logical Plan”。</p><p><img src=\"https://static001.geekbang.org/resource/image/a0/29/a0391155f57085266a7703b0cb788329.jpg?wh=3447*1149\" alt=\"\" title=\"逻辑优化环节\"></p><p>那么问题来了，Catalyst都有哪些既定的规则和逻辑呢？基于这些规则，Catalyst又是怎么做转换的呢？别着急，我们一个一个来解答，咱们先来说说Catalyst的优化规则，然后再去探讨逻辑计划的转换过程。</p><h3>Catalyst的优化规则</h3><p>和Catalyst相比，咱们总结出的开发原则简直就是小巫见大巫，为什么这么说呢？在新发布的Spark 3.0版本中，Catalyst总共有81条优化规则（Rules），这81条规则会分成27组（Batches），其中有些规则会被收纳到多个分组里。因此，如果不考虑规则的重复性，27组算下来总共会有129个优化规则。</p><p>对于如此多的优化规则，我们该怎么学呢？实际上，如果从优化效果的角度出发，这些规则可以归纳到以下3个范畴：</p><ul>\n<li><strong>谓词下推（Predicate Pushdown）</strong></li>\n<li><strong>列剪裁（Column Pruning）</strong></li>\n<li><strong>常量替换 （Constant Folding）</strong></li>\n</ul><p>首先，我们来说说谓词下推谓词下推主要是围绕着查询中的过滤条件做文章。其中，<strong>“谓词”指代的是像用户表上“age &lt; 30”这样的过滤条件，“下推”指代的是把这些谓词沿着执行计划向下，推到离数据源最近的地方，从而在源头就减少数据扫描量</strong>。换句话说，让这些谓词越接近数据源越好。</p><p>不过，在下推之前，Catalyst还会先对谓词本身做一些优化，比如像OptimizeIn规则，它会把“gender in ‘M’”优化成“gender = ‘M’”，也就是把谓词in替换成等值谓词。再比如，CombineFilters规则，它会把“age &lt; 30”和“gender = ‘M’”这两个谓词，捏合成一个谓词：“age != null AND gender != null AND age &lt;30 AND gender = ‘M’”。</p><p>完成谓词本身的优化之后，Catalyst再用PushDownPredicte优化规则，把谓词推到逻辑计划树最下面的数据源上。对于Parquet、ORC这类存储格式，结合文件注脚（Footer）中的统计信息，下推的谓词能够大幅减少数据扫描量，降低磁盘I/O开销。</p><p>再来说说列剪裁。<strong>列剪裁就是扫描数据源的时候，只读取那些与查询相关的字段。</strong>以小Q为例，用户表的Schema是（userId、name、age、gender、email），但是查询中压根就没有出现过email的引用，因此，Catalyst会使用 ColumnPruning规则，把email这一列“剪掉”。经过这一步优化，Spark在读取Parquet文件的时候就会跳过email这一列，从而节省I/O开销。</p><p>不难发现，谓词下推与列剪裁的优化动机，其实和“能省则省”的原则一样。核心思想都是用尽一切办法，减少需要扫描和处理的数据量，降低后续计算的负载。</p><p>最后一类优化是常量替换，它的逻辑比较简单。假设我们在年龄上加的过滤条件是“age &lt; 12 + 18”，Catalyst会使用ConstantFolding规则，自动帮我们把条件变成“age &lt; 30”。再比如，我们在select语句中，掺杂了一些常量表达式，Catalyst也会自动地用表达式的结果进行替换。</p><p>到此为止，咱们从功用和效果的角度，探讨了Catalyst逻辑优化规则的3大范畴。你可能说：“拢共就做了这么3件事，至于兴师动众地制定81条规则吗？”我们划分这3大范畴，主要是为了叙述和理解上的方便。实际上，对于开发者写出的五花八门、千奇百怪的查询语句，正是因为Catalyst不断丰富的优化规则，才让这些查询都能够享有不错的执行性能。如果没有这些优化规则的帮忙，小Q的执行性能一定会惨不忍睹。</p><p>最终，被Catalyst优化过后的小Q，就从“Analyzed Logical Plan”转换为“Optimized Logical Plan”，如下图所示。我们可以看到，谓词下推和列剪裁都体现到了Optimized Logical Plan中。</p><p><img src=\"https://static001.geekbang.org/resource/image/72/df/7223829502eeeca0fbfb721c6a3b61df.png?wh=1338*390\" alt=\"\" title=\"小Q再变身：Optimized Logical Plan\"></p><h3>Catalys的优化过程</h3><p>接下来，我继续来回答刚刚提出的第二个问题：基于这么多优化规则，Catalyst具体是怎么把“Analyzed Logical Plan”转换成“Optimized Logical Plan”的呢？其实，不管是逻辑计划（Logical Plan）还是物理计划（Physical Plan），它们都继承自QueryPlan。</p><p>QueryPlan的父类是TreeNode，TreeNode就是语法树中对于节点的抽象。TreeNode有一个名叫children的字段，类型是Seq[TreeNode]，利用TreeNode类型，Catalyst可以很容易地构建一个树结构。</p><p>除了children字段，TreeNode还定义了很多高阶函数，其中最值得关注的是一个叫做transformDown的方法。transformDown的形参，正是Catalyst定义的各种优化规则，方法的返回类型还是TreeNode。另外，transformDown是个递归函数，参数的优化规则会先作用（Apply）于当前节点，然后依次作用到children中的子节点，直到整棵树的叶子节点。</p><p><strong>总的来说，从“Analyzed Logical Plan”到“Optimized Logical Plan”的转换，就是从一个TreeNode生成另一个TreeNode的过程。</strong>Analyzed Logical Plan的根节点，通过调用transformDown方法，不停地把各种优化规则作用到整棵树，直到把所有27组规则尝试完毕，且树结构不再发生变化为止。这个时候，生成的TreeNode就是Optimized Logical Plan。</p><p>为了把复杂问题简单化，我们使用Expression，也就是表达式来解释一下这个过程。因为Expression本身也继承自TreeNode，所以明白了这个例子，TreeNode之间的转换我们也就清楚了。</p><pre><code>//Expression的转换\nimport org.apache.spark.sql.catalyst.expressions._\nval myExpr: Expression = Multiply(Subtract(Literal(6), Literal(4)), Subtract(Literal(1), Literal(9)))\nval transformed: Expression = myExpr transformDown {\n  case BinaryOperator(l, r) =&gt; Add(l, r)\n  case IntegerLiteral(i) if i &gt; 5 =&gt; Literal(1)\n  case IntegerLiteral(i) if i &lt; 5 =&gt; Literal(0)\n}\n</code></pre><p>首先，我们定义了一个表达式：（（6 - 4）*（1 - 9）），然后我们调用这个表达式的transformDown高阶函数。在高阶函数中，我们提供了一个用case定义的匿名函数。显然，这是一个偏函数（Partial Functions），你可以把这个匿名函数理解成“自定义的优化规则”。在这个优化规则中，我们仅考虑3种情况：</p><ul>\n<li>对于所有的二元操作符，我们都把它转化成加法操作</li>\n<li>对于所有大于5的数字，我们都把它变成1</li>\n<li>对于所有小于5的数字，我们都把它变成0</li>\n</ul><p>虽然我们的优化规则没有任何实质性的意义，仅仅是一种转换规则而已，但是这并不妨碍你去理解Catalyst中TreeNode之间的转换。当我们把这个规则应用到表达式（（6 - 4）*（1 - 9））之后，得到的结果是另外一个表达式（（1 + 0）+（0 + 1）），下面的示意图直观地展示了这个过程。</p><p><img src=\"https://static001.geekbang.org/resource/image/ea/1f/ea21ec9387e55e94d763d9ee0c4a4b1f.jpg?wh=3369*827\" alt=\"\" title=\"自顶向下对执行计划进行转换\"></p><p>从“Analyzed Logical Plan”到“Optimized Logical Plan”的转换，与示例中表达式的转换过程如出一辙。最主要的区别在于，Catalyst的优化规则要复杂、精密得多。</p><h3>Cache Manager优化</h3><p>从“Analyzed Logical Plan”到“Optimized Logical Plan”的转换，Catalyst除了使用启发式的规则以外，还会利用Cache Manager做进一步的优化。</p><p><strong>这里的Cache指的就是我们常说的分布式数据缓存。想要对数据进行缓存，你可以调用DataFrame的.cache或.persist，或是在SQL语句中使用“cache table”关键字</strong>。</p><p>Cache Manager其实很简单，它的主要职责是维护与缓存有关的信息。具体来说，Cache Manager维护了一个Mapping映射字典，字典的Key是逻辑计划，Value是对应的Cache元信息。</p><p>当Catalyst尝试对逻辑计划做优化时，会先尝试对Cache Manager查找，看看当前的逻辑计划或是逻辑计划分支，是否已经被记录在Cache Manager的字典里。如果在字典中可以查到当前计划或是分支，Catalyst就用InMemoryRelation节点来替换整个计划或是计划的一部分，从而充分利用已有的缓存数据做优化。</p><h2>小结</h2><p>今天这一讲，我们主要探讨了Catalyst优化器的逻辑优化阶段。这个阶段包含两个环节：逻辑计划解析和逻辑计划优化。</p><p>在逻辑计划解析环节，Catalyst结合Schema信息，对于仅仅记录语句字符串的Unresolved Logical Plan，验证表名、字段名与实际数据的一致性。解析后的执行计划称为Analyzed Logical Plan。</p><p>在逻辑计划优化环节，Catalyst会同时利用3方面的力量优化Analyzed Logical Plan，分别是AQE、Cache Manager和启发式的规则。它们当中，Catalyst最倚重的是启发式的规则。</p><p>尽管启发式的规则多达81项，但我们把它们归纳为3大范畴：谓词下推、列剪裁和常量替换。我们要重点掌握谓词下推和列剪裁，它们的优化动机和“能省则省”的开发原则一样，核心思想都是用尽一切办法，减少需要扫描和处理的数据量，降低后续计算的负载。</p><p>针对所有的优化规则，Catalyst优化器会通过调用TreeNode中的transformDown高阶函数，分别把它们作用到逻辑计划的每一个节点上，直到逻辑计划的结构不再改变为止，这个时候生成的逻辑计划就是Optimized Logical Plan。</p><p>最后，Cache Manager的作用是提供逻辑计划与数据缓存的映射关系，当现有逻辑计划或是分支出现在Cache Manager维护的映射字典的时候，Catalyst可以充分利用已有的缓存数据来优化。</p><h2>每日一练</h2><ol>\n<li>既然Catalyst在逻辑优化阶段有81条优化规则，我们还需要遵循“能省则省、能拖则拖”的开发原则吗？</li>\n<li>你能说说Spark为什么用偏函数，而不是普通函数来定义Catalyst的优化规则吗？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","neighbors":{"left":{"article_title":"20 | RDD和DataFrame：既生瑜，何生亮？","id":367807},"right":{"article_title":"22 | Catalyst物理计划：你的SQL语句是怎么被优化的（下）？","id":369519}},"comments":[{"had_liked":false,"id":290868,"user_name":"kingcall","can_delete":false,"product_type":"c1","uid":1056982,"ip_address":"","ucode":"508884DC684B5B","user_header":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","comment_is_top":false,"comment_ctime":1619794474,"is_pvip":false,"replies":[{"id":"105414","content":"Perfect x 2！标准答案了~ 💯<br><br>回答的非常好了，无可挑剔~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619862029,"ip_address":"","comment_id":290868,"utype":1}],"discussion_count":3,"race_medal":0,"score":"117583911466","product_id":100073401,"comment_content":"问题1：开发阶段要有极客精神，尽量将优化实现在自己的代码里，而不是依赖框架，因为框架是一个普世的优化，还有就是如果我们根据业务特点进行了优化再加上框架本身带来的优化能给我们的程序带来一个更好的性能提升,也就是说上层自我优化和底层框架优化。<br>问题2：与偏函数对应的一个定义就是我们数学意义上的函数，一个输入自变量x对应一个输出因变量y，也就是y 可以表示成x 的一个特定的运算，并且这个运算关系是确定的，但是scala 中的偏函数它表示的是一种匹配关系，有点类似if else if ... else ，只有匹配上了才有对应的值，否则输出的就是默认值<br>Spark 之所以用偏函数，而不是普通函数来定义 Catalyst 的优化规则，是因为规则是预先定义的，不可能满足所有的情况，所以需要一个兜底，而这正好满足偏函数的特点。<br>","like_count":28,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519302,"discussion_content":"Perfect x 2！标准答案了~ 💯\n\n回答的非常好了，无可挑剔~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619862029,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":371780,"discussion_content":"为啥大牛这么多","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1619966787,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2068627,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/90/93/5e94be87.jpg","nickname":"钝感","note":"","ucode":"50FE1DD4EAEB78","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2188142,"avatar":"https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg","nickname":"Z宇锤锤","note":"","ucode":"7DB36E986A7A51","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":404848,"discussion_content":"加油！！以后你也会是大牛","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1634436352,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":371780,"ip_address":""},"score":404848,"extra":""}]}]},{"had_liked":false,"id":290743,"user_name":"jerry guo","can_delete":false,"product_type":"c1","uid":1267753,"ip_address":"","ucode":"179943AFCB93F8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eo2GMhevabZrjINs2TKvIeGC7TJkicNlLvqTticuM5KL8ZN80OC2CnrsUyzPcZXO4uptj4Q1S4jT2lQ/132","comment_is_top":false,"comment_ctime":1619714637,"is_pvip":true,"replies":[{"id":"105418","content":"第二题分析的很到位，就是你说的这么回事。偏函数的特性，刚好符合Spark SQL优化规则的特点，也就是：<br>1. 不完备<br>2. 保持扩展的灵活性<br><br>问题三是个超级好的问题，其实我一直想找机会说说。在传统的DBMS里，CBO是主流，RBO是辅助性的。原因其实很简单，RBO毕竟是出于启发式的，都是一些“经验主义”，CBO才是真正“尊重”事实、尊重运行时的优化策略。在传统DBMS里面，CBO是非常成熟的技术了，已经沿用很多很多年了。<br><br>然而，Spark SQL的CBO相对就比较鸡肋了，原因其实我在第24讲会详细展开，提前剧透一下，就是Spark SQL的CBO有三个非常大的痛点：窄、慢、静<br><br>窄：窄指的是适用面太窄，CBO仅支持注册到Hive Metastore的数据表，但在大量的应用场景中，数据源往往是存储在分布式文件系统的各类文件，如Parquet、ORC、CSV等等。<br><br>慢：慢指的是统计信息的搜集效率比较低。对于注册到Hive Metastore的数据表，开发者需要调用ANALYZE TABLE COMPUTE STATISTICS语句收集统计信息，而各类信息的收集会消耗大量时间。<br><br>静：静指的是静态优化，这一点与RBO一样。CBO结合各类统计信息制定执行计划，一旦执行计划交付运行，CBO的使命就算完成了。这一点和传统DBMS很不同，传统DBMS的CBO是动态的，可以在运行时做适当调整。<br><br>不仅如此，目前Spark SQL的CBO，还仅仅支持Join策略，换句话说，与Join无关的查询，CBO使不上劲。<br><br>因此，综上，CBO虽然是个非常吸引人的东西，但是Spark SQL的CBO很鸡肋，当然这有很多原因，比如CBO本身的限制，社区对于Spark SQL优化方向的考虑，等等。其实AQE的推出，就表示Spark社区已经做出了选择。就我个人观察（仅代表个人观点哈），CBO的地位和角色可能会越来越尴尬，用武之地可能会越来越小，聊胜于无。当然，再次重申，这仅仅是我个人的观点哈~<br><br>另外，这两个问题kingcall同学回答的比较好，可以参考他的答案哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619864234,"ip_address":"","comment_id":290743,"utype":1}],"discussion_count":3,"race_medal":0,"score":"23094551117","product_id":100073401,"comment_content":"1. 既然 Catalyst 在逻辑优化阶段有 81 条优化规则，我们还需要遵循“能省则省、能拖则拖”的开发原则吗？你能说说 Spark 为什么用偏函数，而不是普通函数来定义 Catalyst 的优化规则吗？<br>答：要。战略上用开发原则，战术上依赖Catalyst。<br><br>2.你能说说 Spark 为什么用偏函数，而不是普通函数来定义 Catalyst 的优化规则吗？<br>答：网上搜到一篇文章<br>&quot;The pattern matching expression that is passed to transform is a partial function, meaning that it only needs to match to a subset of all possible input trees. Catalyst will tests which parts of a tree a given rule applies to, automatically skipping over and descending into subtrees that do not match. This ability means that rules only need to reason about the trees where a given optimization applies and not those that do not match. Thus, rules do not need to be modified as new types of operators are added to the system.&quot; 这段不是很懂，大概意思是因为偏函数没有包括所有的情况，所以正好，符合定义的rule就优化，不符合就不处理，这样子比较省事；另外由于一开始没有完全定义出全部的情况（可能也定义不出来），所以这也有一定的灵活性，再新加了operator之后，也不需要改rule了。望老师指点。 <br><br>3. 另外我有个问题，RDBMS的SQL优化很早之前是基于Rule，后面变成了基于Cost。根据本篇的讲解，Catalyst和Tunsgen，都是基于Rule的，网上搜索了一下，Catalyst也可以基于cost。老师可以讲讲对CBO的看法吗？比如CBO如何和Catalyst，Tusgen一起工作？或者以后CBO会变成主流吗？","like_count":6,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519278,"discussion_content":"第二题分析的很到位，就是你说的这么回事。偏函数的特性，刚好符合Spark SQL优化规则的特点，也就是：\n1. 不完备\n2. 保持扩展的灵活性\n\n问题三是个超级好的问题，其实我一直想找机会说说。在传统的DBMS里，CBO是主流，RBO是辅助性的。原因其实很简单，RBO毕竟是出于启发式的，都是一些“经验主义”，CBO才是真正“尊重”事实、尊重运行时的优化策略。在传统DBMS里面，CBO是非常成熟的技术了，已经沿用很多很多年了。\n\n然而，Spark SQL的CBO相对就比较鸡肋了，原因其实我在第24讲会详细展开，提前剧透一下，就是Spark SQL的CBO有三个非常大的痛点：窄、慢、静\n\n窄：窄指的是适用面太窄，CBO仅支持注册到Hive Metastore的数据表，但在大量的应用场景中，数据源往往是存储在分布式文件系统的各类文件，如Parquet、ORC、CSV等等。\n\n慢：慢指的是统计信息的搜集效率比较低。对于注册到Hive Metastore的数据表，开发者需要调用ANALYZE TABLE COMPUTE STATISTICS语句收集统计信息，而各类信息的收集会消耗大量时间。\n\n静：静指的是静态优化，这一点与RBO一样。CBO结合各类统计信息制定执行计划，一旦执行计划交付运行，CBO的使命就算完成了。这一点和传统DBMS很不同，传统DBMS的CBO是动态的，可以在运行时做适当调整。\n\n不仅如此，目前Spark SQL的CBO，还仅仅支持Join策略，换句话说，与Join无关的查询，CBO使不上劲。\n\n因此，综上，CBO虽然是个非常吸引人的东西，但是Spark SQL的CBO很鸡肋，当然这有很多原因，比如CBO本身的限制，社区对于Spark SQL优化方向的考虑，等等。其实AQE的推出，就表示Spark社区已经做出了选择。就我个人观察（仅代表个人观点哈），CBO的地位和角色可能会越来越尴尬，用武之地可能会越来越小，聊胜于无。当然，再次重申，这仅仅是我个人的观点哈~\n\n另外，这两个问题kingcall同学回答的比较好，可以参考他的答案哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619864234,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1476427,"avatar":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","nickname":"tiankonghewo","note":"","ucode":"7A55A9C17DD9DF","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":532449,"discussion_content":"涨知识了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637597650,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"user_type\":1}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1168504,"avatar":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","nickname":"斯盖丸","note":"","ucode":"B881D14B028F14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":371354,"discussion_content":"Spark逻辑执行后的物理执行计划，它的优化就是基于cost的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619745764,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291690,"user_name":"辰","can_delete":false,"product_type":"c1","uid":2172635,"ip_address":"","ucode":"2E898EAC5AA141","user_header":"https://static001.geekbang.org/account/avatar/00/21/26/db/27724a6f.jpg","comment_is_top":false,"comment_ctime":1620436091,"is_pvip":false,"replies":[{"id":"105656","content":"Project是投影的意思，实际上对应的就是你SQL或是DataFrame中的select语句，也就是说，在众多的数据列中，你要“投影”出哪些列，说白了，就是选出哪些列。<br><br>Project也会，Select也好，本身都不是算子哈，不管是SQL查询，还是DSL查询，都需要show、count、save等action算子来触发计算。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620468710,"ip_address":"","comment_id":291690,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14505337979","product_id":100073401,"comment_content":"老师，执行计划中的project是什么意思啊，大概知道是和映射的关系，可不可以理解成相当于action算子一样","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519543,"discussion_content":"Project是投影的意思，实际上对应的就是你SQL或是DataFrame中的select语句，也就是说，在众多的数据列中，你要“投影”出哪些列，说白了，就是选出哪些列。\n\nProject也会，Select也好，本身都不是算子哈，不管是SQL查询，还是DSL查询，都需要show、count、save等action算子来触发计算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620468710,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291557,"user_name":"mini希","can_delete":false,"product_type":"c1","uid":1043539,"ip_address":"","ucode":"54DFFE0CE0C7EF","user_header":"https://static001.geekbang.org/account/avatar/00/0f/ec/53/dcec6fdc.jpg","comment_is_top":false,"comment_ctime":1620361328,"is_pvip":true,"replies":[{"id":"105619","content":"对，你说的没错，确实只有列存才能从列剪裁受益，行存不行。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620397141,"ip_address":"","comment_id":291557,"utype":1}],"discussion_count":1,"race_medal":4,"score":"14505263216","product_id":100073401,"comment_content":"老师好，针对列剪裁是否只有列式的存储才能享受到扫描的优化效果，行存还是会扫描整行所有字段？","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519506,"discussion_content":"对，你说的没错，确实只有列存才能从列剪裁受益，行存不行。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620397141,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290916,"user_name":"对方正在输入。。。","can_delete":false,"product_type":"c1","uid":1179298,"ip_address":"","ucode":"7B0DEB4D9B43D2","user_header":"https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg","comment_is_top":false,"comment_ctime":1619858925,"is_pvip":false,"replies":[{"id":"105408","content":"哈哈哈，喜欢就好~ 兄弟五一节快乐~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619860449,"ip_address":"","comment_id":290916,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10209793517","product_id":100073401,"comment_content":"老师的课，我是越看越爽，看完就有一种“老子天下无敌了”的感觉，哈哈哈哈哈","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519322,"discussion_content":"哈哈哈，喜欢就好~ 兄弟五一节快乐~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619860449,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":306805,"user_name":"农夫三拳","can_delete":false,"product_type":"c1","uid":1529153,"ip_address":"","ucode":"9778194A7CD44E","user_header":"https://static001.geekbang.org/account/avatar/00/17/55/41/b68df312.jpg","comment_is_top":false,"comment_ctime":1628731010,"is_pvip":false,"replies":[{"id":"111185","content":"好问题~ 就Spark SQL目前的实现来看，它是在Analyzed Logical Plan之上做的这一步优化，也就是你说的“优化前”阶段。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1628837298,"ip_address":"","comment_id":306805,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5923698306","product_id":100073401,"comment_content":"老师 想问个问题，这个缓存是根据 查询树或者查询树的一部分作为key，进行缓存，匹配到了 就替换当前节点或者整棵树。  请问下 这个缓存是针对哪个阶段的查询树呢？是  解析，优化前，优化后，还是物理计划阶段？  我个人理解是优化后阶段。","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524925,"discussion_content":"好问题~ 就Spark SQL目前的实现来看，它是在Analyzed Logical Plan之上做的这一步优化，也就是你说的“优化前”阶段。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628837298,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2028277,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","nickname":"Unknown element","note":"","ucode":"34A129800D0238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":544535,"discussion_content":"不应该是优化后吗。。老师是不是打错了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641553007,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291346,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620221499,"is_pvip":false,"replies":[{"id":"105522","content":"对，没错，完美契合！老弟这进度赶得飞起啊~ 都追到21讲了~ 赞👍","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620226546,"ip_address":"","comment_id":291346,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5915188795","product_id":100073401,"comment_content":"偏函数只针对部分输入来输出结果，而每个函数对应的优化规则也是有限的，再搭配模式匹配，很完美的应用场景哈哈","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519447,"discussion_content":"对，没错，完美契合！老弟这进度赶得飞起啊~ 都追到21讲了~ 赞👍","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620226546,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290911,"user_name":"sky_sql","can_delete":false,"product_type":"c1","uid":1099273,"ip_address":"","ucode":"397F4263C9E590","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/09/7f2bcc6e.jpg","comment_is_top":false,"comment_ctime":1619854592,"is_pvip":false,"replies":[{"id":"105409","content":"对，构建语法树、Schema验证、谓词下推、列剪裁，这些其实是绝大多数的数仓都有的优化策略。<br><br>实际上，相比传统DBMS，Spark SQL的优化流程算是简化版的了，比如传统DBMS还会有Query Re-writer、CBO（基于成本的优化）等等。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619860603,"ip_address":"","comment_id":290911,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5914821888","product_id":100073401,"comment_content":"老师好，RDD api有点类似MR编程，Spark SQL有点类似hive，过程都包括使用 Antlr 实现 SQL 的词法和语法解析，后面也有Schema 信息验证，优化环节谓词下推、列剪裁等？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519319,"discussion_content":"对，构建语法树、Schema验证、谓词下推、列剪裁，这些其实是绝大多数的数仓都有的优化策略。\n\n实际上，相比传统DBMS，Spark SQL的优化流程算是简化版的了，比如传统DBMS还会有Query Re-writer、CBO（基于成本的优化）等等。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619860603,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":352962,"user_name":"Sampson","can_delete":false,"product_type":"c1","uid":1418226,"ip_address":"北京","ucode":"BA78CA29A6D898","user_header":"https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg","comment_is_top":false,"comment_ctime":1659056861,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1659056861","product_id":100073401,"comment_content":"磊哥，想请问下在逻辑计划解析阶段是否应该还有词法解析，语法解析等步骤？如果有的话，是使用的什么方式做的呢 ？ ","like_count":0,"discussions":[{"author":{"id":1591216,"avatar":"https://static001.geekbang.org/account/avatar/00/18/47/b0/cf5529a0.jpg","nickname":"ThomasG","note":"","ucode":"C05D94E0A10662","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":588368,"discussion_content":"antlr4","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1663717971,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":347153,"user_name":"liangzai","can_delete":false,"product_type":"c1","uid":1122462,"ip_address":"","ucode":"601E3F7A0442AF","user_header":"https://static001.geekbang.org/account/avatar/00/11/20/9e/145df9a3.jpg","comment_is_top":false,"comment_ctime":1653750462,"is_pvip":true,"discussion_count":1,"race_medal":0,"score":"1653750462","product_id":100073401,"comment_content":"从cache manager中查询计划，这个没太懂，请问这个缓存的计划是从哪里来的呢？","like_count":0,"discussions":[{"author":{"id":1591216,"avatar":"https://static001.geekbang.org/account/avatar/00/18/47/b0/cf5529a0.jpg","nickname":"ThomasG","note":"","ucode":"C05D94E0A10662","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":588369,"discussion_content":"这个大概是已经解析过的执行计划，比如一个sql执行两次","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1663718042,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":308634,"user_name":"Marco","can_delete":false,"product_type":"c1","uid":1138102,"ip_address":"","ucode":"78254AE72CB03A","user_header":"https://static001.geekbang.org/account/avatar/00/11/5d/b6/bedadca5.jpg","comment_is_top":false,"comment_ctime":1629712025,"is_pvip":false,"replies":[{"id":"111768","content":"有的，CBO，不过CBO有各种毛病，这块在AQE 24讲有介绍。<br><br>但是，CBO 也面临三个方面的窘境：“窄、慢、静”。窄指的是适用面太窄，CBO 仅支持注册到 Hive Metastore 的数据表，但在大量的应用场景中，数据源往往是存储在分布式文件系统的各类文件，如 Parquet、ORC、CSV 等等。慢指的是统计信息的搜集效率比较低。对于注册到 Hive Metastore 的数据表，开发者需要调用 ANALYZE TABLE COMPUTE STATISTICS 语句收集统计信息，而各类信息的收集会消耗大量时间。静指的是静态优化，这一点与 RBO 一样。CBO 结合各类统计信息制定执行计划，一旦执行计划交付运行，CBO 的使命就算完成了。换句话说，如果在运行时数据分布发生动态变化，CBO 先前制定的执行计划并不会跟着调整、适配。<br><br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629781890,"ip_address":"","comment_id":308634,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1629712025","product_id":100073401,"comment_content":"spark只用启发式的规则优化吗，有没有基于成本模型的优化？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":525567,"discussion_content":"有的，CBO，不过CBO有各种毛病，这块在AQE 24讲有介绍。\n\n但是，CBO 也面临三个方面的窘境：“窄、慢、静”。窄指的是适用面太窄，CBO 仅支持注册到 Hive Metastore 的数据表，但在大量的应用场景中，数据源往往是存储在分布式文件系统的各类文件，如 Parquet、ORC、CSV 等等。慢指的是统计信息的搜集效率比较低。对于注册到 Hive Metastore 的数据表，开发者需要调用 ANALYZE TABLE COMPUTE STATISTICS 语句收集统计信息，而各类信息的收集会消耗大量时间。静指的是静态优化，这一点与 RBO 一样。CBO 结合各类统计信息制定执行计划，一旦执行计划交付运行，CBO 的使命就算完成了。换句话说，如果在运行时数据分布发生动态变化，CBO 先前制定的执行计划并不会跟着调整、适配。\n\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629781890,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":305333,"user_name":"猿鸽君","can_delete":false,"product_type":"c1","uid":1991951,"ip_address":"","ucode":"8562D8C5AD3D1E","user_header":"https://static001.geekbang.org/account/avatar/00/1e/65/0f/7b9f27f2.jpg","comment_is_top":false,"comment_ctime":1627909937,"is_pvip":false,"replies":[{"id":"110610","content":"老弟可以提供更详细的信息吗？比如具体的SQL语句，Spark和Hive的版本号，不同的Hive版本之间的差异还是挺大的","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1628145735,"ip_address":"","comment_id":305333,"utype":1}],"discussion_count":2,"race_medal":0,"score":"1627909937","product_id":100073401,"comment_content":"请问老师知道sparksql有时在执行insert overwrite hive table（静态分区）特别慢的原因吗？我翻了内外网都只给出了解决方案，却没有原因……","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524352,"discussion_content":"老弟可以提供更详细的信息吗？比如具体的SQL语句，Spark和Hive的版本号，不同的Hive版本之间的差异还是挺大的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628145735,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1511712,"avatar":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","nickname":"wow_xiaodi","note":"","ucode":"B3FB301556A7EA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388254,"discussion_content":"是不是spark sql的并行度过高，一个分区下写太多文件，cpu和磁盘io都损耗过大了。另外一个是是否发生了数据倾斜，是计算导致的瓶颈，而不是最后sink阶段的瓶颈","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628670325,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290924,"user_name":"zxk","can_delete":false,"product_type":"c1","uid":1221195,"ip_address":"","ucode":"4BB2BD9D2BCD04","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","comment_is_top":false,"comment_ctime":1619861767,"is_pvip":false,"replies":[{"id":"105435","content":"第一个问题答得很好~<br>第二个问题可以参考kingcall同学的答案哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619946476,"ip_address":"","comment_id":290924,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1619861767","product_id":100073401,"comment_content":"问题1：Spark 里的优化只是一种普世的优化，力求在尽可能覆盖绝大部分通用场景，减少对开发者的要求。但实际开发中场景千差万别，Spark 不一定能覆盖到所有场景，因此仍需要我们尽可能遵循这些开发原则。<br>问题2：对偏函数不是很熟悉，猜测是尽可能的拆解函数粒度，给 Spark 留出更多的优化空间。比如之前老师就有个 label encoding 的例子，将函数拆分为偏函数后，Spark 自动进行优化，性能有很大提升。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519327,"discussion_content":"第一个问题答得很好~\n第二个问题可以参考kingcall同学的答案哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619946476,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}