{"id":1867,"title":"10 机器学习 | 衍化至繁：逻辑回归","content":"<p>周四我和你分享了机器学习中的朴素贝叶斯分类算法，这一算法解决的是将连续取值的输入映射为离散取值的输出的分类问题。朴素贝叶斯分类器是一类<strong>生成模型</strong>，通过构造联合概率分布$P(X, Y)$实现分类。如果换一种思路，转而用<strong>判别模型</strong>解决分类问题的话，得到的算法就是“<strong>逻辑回归</strong>”。</p>\n<p><strong>虽然顶着“回归”的名号，但逻辑回归解决的却是实打实的分类问题</strong>。之所以取了这个名字，原因在于它来源于对线性回归算法的改进。通过引入单调可微函数$g(\\cdot)$，线性回归模型就可以推广为$y = g ^ {-1} (\\mathbf{w} ^ T \\mathbf{x})$，进而将线性回归模型的连续预测值与分类任务的离散标记联系起来。当$g(\\cdot)$取成对数函数的形式时，线性回归就演变为了逻辑回归。</p>\n<!-- [[[read_end]]] -->\n<p>在最简单的二分类问题中，分类的标记可以抽象为0和1，因而线性回归中的实值输出需要映射为二进制的结果。逻辑回归中，实现这一映射是对数几率函数，也叫Sigmoid函数</p>\n<p> $$ y = \\dfrac{1}{1 + e ^ {-z}} = \\dfrac{1}{1 + e ^ {- (\\mathbf{w} ^ T \\mathbf{x})}} $$ </p>\n<p><strong>之所以选择对数几率函数，是因为它具备良好的特性</strong>。</p>\n<p>首先，对数几率函数能够将线性回归从负无穷到正无穷的输出范围压缩到(0, 1)之间，无疑更加符合对二分类任务的直观感觉。</p>\n<p>其次，当线性回归的结果$z = 0$时，逻辑回归的结果$y = 0.5$，这可以视为一个分界点：当$z &gt; 0$时，$y &gt; 0.5$，此时逻辑回归的结果就可以判为正例；当$z &lt; 0$时，$y &lt; 0.5$，逻辑回归的结果就可以判为反例。</p>\n<p>显然，对数几率函数能够在线性回归和逻辑回归之间提供更好的可解释性。这种可解释性可以从数学的角度进一步诠释。</p>\n<p>如果将对数几率函数的结果$y$视为样本$\\mathbf{x}$作为正例的可能性，则$1 - y$就是其作为反例的可能性，两者的比值$0 &lt; \\dfrac{y}{1 -y} &lt; +\\infty$称为几率，体现的是样本作为正例的相对可能性。如果对几率函数取对数，并将前文中的公式代入，可以得到 </p>\n<p>$$ \\ln \\dfrac{y}{1 -y} = \\mathbf{w} ^ T \\mathbf{x} + b $$</p>\n<p>由此可见，当利用逻辑回归模型解决分类任务时，线性回归的结果正是以对数几率的形式出现的。</p>\n<p>归根结底，逻辑回归模型由条件概率分布表示 </p>\n<p>$$ p(y = 1 | \\mathbf{x} ) = \\dfrac{e ^ {\\mathbf{w} ^ T \\mathbf{x} + b}}{1 + e ^ {\\mathbf{w} ^ T \\mathbf{x} + b}}$$</p>\n<p>$$p(y = 0 | \\mathbf{x} ) = \\dfrac{1}{1 + e ^ {\\mathbf{w} ^ T \\mathbf{x} + b}} $$</p>\n<p><strong>对于给定的实例，逻辑回归模型比较两个条件概率值的大小，并将实例划分到概率较大的分类之中</strong>。</p>\n<p><strong>学习时，逻辑回归模型在给定的训练数据集上应用最大似然估计法确定模型的参数</strong>。对给定的数据集${ (\\mathbf{x}_i, y_i)}$，逻辑回归使每个样本属于其真实标记的概率最大化，以此为依据确定$\\mathbf{w}$的最优值。由于每个样本的输出$y_i$都满足两点分布，且不同的样本之间相互独立，因而似然函数可以表示为</p>\n<p>$$ L(\\mathbf{w} | \\mathbf{x}) = \\Pi_{i = 1}^N [p(y = 1 | \\mathbf{x}_i, \\mathbf{w})] ^ {y_i} $$</p>\n<p>$$\\cdot [1 - p(y = 1 | \\mathbf{x}_i, \\mathbf{w})] ^ {1 - y_i} $$ </p>\n<p>利用对数操作将乘积转化为求和，就可以得到对数似然函数</p>\n<p> $$ \\log L(\\mathbf{w} | \\mathbf{x}) = \\sum\\limits_{i = 1}^N y_i \\log [p(y = 1 | \\mathbf{x}_i, \\mathbf{w})] $$</p>\n<p>$$+ (1 - y_i) \\log [1 - p(y = 1 | \\mathbf{x}_i, \\mathbf{w})]$$ </p>\n<p>由于单个样本的标记$y_i$只能取得0或1，因而上式中的两项中只有一个有非零的取值。将每个条件概率的对数几率函数形式代入上式，经过化简可以得到</p>\n<p>$$ \\log L(\\mathbf{w} | \\mathbf{x}) = \\sum\\limits_{i = 1}^N y_i \\cdot (\\mathbf{w} ^ T \\mathbf{x}_i) $$</p>\n<p>$$- \\log (1 + e ^ {\\mathbf{w} ^ T \\mathbf{x}_i}) $$ </p>\n<p><strong>寻找以上函数的最大值就是以对数似然函数为目标函数的最优化问题，通常通过“梯度下降法”或拟“牛顿法”求解</strong>。</p>\n<p>当训练数据集是从所有数据中均匀抽取且数量较大时，以上结果还有一种信息论角度的阐释方式：<strong>对数似然函数的最大化可以等效为待求模型与最大熵模型之间KL散度的最小化</strong>。这意味着最优的估计对参数做出的额外假设是最少的，这无疑与最大熵原理不谋而合。</p>\n<p>从数学角度看，线性回归和逻辑回归之间的渊源来源于非线性的对数似然函数；而从特征空间的角度看，两者的区别则在于数据判定边界的变化。判定边界可以类比为棋盘上的楚河汉界，边界两侧分别对应不同类型的数据。</p>\n<p>以最简单的二维平面直角坐标系为例。受模型形式的限制，利用线性回归只能得到直线形式的判定边界；逻辑回归则在线性回归的基础上，通过对数似然函数的引入使判定边界的形状不再受限于直线，而是推广为更加复杂的曲线形式，更加精细的分类也就不在话下。</p>\n<p>逻辑回归与线性回归的关系称得上系出同门，与朴素贝叶斯分类的关系则是殊途同归。两者虽然都可以利用条件概率$P(Y|X)$完成分类任务，实现的路径却截然不同。</p>\n<p>朴素贝叶斯分类器是生成模型的代表，其思想是先由训练数据集估计出输入和输出的联合概率分布，再根据联合概率分布来生成符合条件的输出，$P(Y|X)$以后验概率的形式出现。</p>\n<p>逻辑回归模型则是判别模型的代表，其思想是先由训练数据集估计出输入和输出的条件概率分布，再根据条件概率分布来判定对于给定的输入应该选择哪种输出，$P(Y|X)$以似然概率的形式出现。</p>\n<p><strong>即便原理不同，逻辑回归与朴素贝叶斯分类器在特定的条件下依然可以等效</strong>。用朴素贝叶斯分类器处理二分类任务时，假设对每个$\\mathbf{x}_i$，属性条件概率$p(\\mathbf{x}_i | Y = y_k)$都满足正态分布，且正态分布的标准差与输出标记$Y$无关，那么根据贝叶斯定理，后验概率就可以写成 </p>\n<p>$$ p(Y = 0 | X) =$$</p>\n<p>$$ \\dfrac{p(Y = 0) \\cdot p(X | Y = 0)}{p(Y = 1) \\cdot p(X | Y = 1) + p(Y = 0) \\cdot p(X | Y = 0)} $$ </p>\n<p> $$ = \\dfrac{1}{1 + \\exp (\\ln \\frac{p(Y = 1) \\cdot p(X | Y = 1)}{p(Y = 0) \\cdot p(X | Y = 0)})} $$ </p>\n<p>根据朴素贝叶斯方法的假设，类条件概率可以表示为属性条件概率的乘积，因而令$p(Y = 0) = p_0$并将满足正态分布的属性条件概率$p(\\mathbf{x}_i | Y = y_k)$代入以上表达式中，经过一番计算就可以得到</p>\n<p> $$ p(Y = 0 | X) = $$</p>\n<p>$$\\dfrac{1}{1 + \\exp(\\ln \\frac{1 - p_0}{p_0} + \\sum\\limits_i(\\frac{\\mu_{i1} - \\mu_{i0}}{\\sigma_i^2} X_i + \\frac{\\mu_{i0} ^ 2 - \\mu_{i1} ^ 2}{2\\sigma_i^2}))} $$ </p>\n<p>不难看出，上式的形式和逻辑回归中条件概率$p(y = 0 | \\mathbf{x} )$的形式是完全一致的，这表明朴素贝叶斯方法和逻辑回归模型学习到的是同一个模型。实际上，在$p(x | Y)$的分布属于指数分布族这个更一般的假设下，类似的结论都是成立的。</p>\n<p>说完了联系，再来看看区别。<strong>两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果</strong>。当训练样本数接近无穷大时，逻辑回归的渐近分类准确率要优于朴素贝叶斯方法。而且逻辑回归并不完全依赖于属性之间相互独立的假设，即使给定违反这一假设的数据，逻辑回归的条件似然最大化算法也会调整其参数以实现最大化的数据拟合。相比之下，逻辑回归的偏差更小，但方差更大。</p>\n<p>除此之外，<strong>两者的区别还在于收敛速度的不同</strong>。逻辑回归中参数估计的收敛速度要慢于朴素贝叶斯方法。当训练数据集的容量较大时，逻辑回归的性能优于朴素贝叶斯方法；但在训练数据稀缺时，两者的表现就会发生反转。</p>\n<p>二分类任务只是特例，更通用的情形是多分类的问题，例如手写数字的识别。要让逻辑回归处理多分类问题，就要做出一些改进。</p>\n<p><strong>一种改进方式是通过多次二分类实现多个类别的标记</strong>。这等效为直接将逻辑回归应用在每个类别之上，对每个类别都建立一个二分类器。如果输出的类别标记数目为$m$，就可以得到$m$个针对不同标记的二分类逻辑回归模型，而对一个实例的分类结果就是这$m$个分类函数中输出值最大的那个。在这种方式中，对一个实例执行分类需要多次使用逻辑回归算法，其效率显然比较低下。</p>\n<p><strong>另一种多分类的方式通过直接修改逻辑回归输出的似然概率，使之适应多分类问题，得到的模型就是Softmax回归</strong>。Softmax回归给出的是实例在每一种分类结果下出现的概率</p>\n<p> $$ p(Y = k | x) = \\dfrac{e ^ {\\mathbf{w_k} ^ T \\mathbf{x}}}{\\sum\\limits_{k = 1}^K e ^ {\\mathbf{w_k} ^ T \\mathbf{x}}} $$ </p>\n<p>式中的$\\mathbf{w_k}$代表和类别$k$相关的权重参数。Softmax回归模型的训练与逻辑回归模型类似，都可以转化为通过梯度下降法或者拟牛顿法解决最优化问题。</p>\n<p>虽然都能实现多分类的任务，但两种方式的适用范围略有区别。当分类问题的所有类别之间明显互斥，即输出结果只能属于一个类别时，Softmax分类器是更加高效的选择；当所有类别之间不存在互斥关系，可能有交叉的情况出现时，多个二分类逻辑回归模型就能够得到多个类别的标记。</p>\n<p>今天我和你分享了机器学习基本算法之一的逻辑回归方法的基本原理，其要点如下：</p>\n<ul>\n<li>逻辑回归模型是对线性回归的改进，用于解决分类问题；</li>\n<li>逻辑回归输出的是实例属于每个类别的似然概率，似然概率最大的类别就是分类结果；</li>\n<li>在一定条件下，逻辑回归模型与朴素贝叶斯分类器是等价的；</li>\n<li>多分类问题时可以通过多次使用二分类逻辑回归或者使用Softmax回归解决。</li>\n</ul>\n<p>前文对逻辑回归的分析都是在概率理论的基础上完成的。但在二分类任务中，逻辑回归的作用可以视为是在平面直角坐标系上划定一条数据分类的判定边界。那么逻辑回归的作用能不能从几何角度理解，并推广到高维空间呢？</p>\n<p>欢迎发表你的观点。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/d8/aa/d81794d22373b75dd79da8655adacdaa.jpg?wh=1110*1032\" alt=\"\"></p>\n<p></p>\n","neighbors":{"left":{"article_title":"09 机器学习 | 大道至简：朴素贝叶斯方法","id":1866},"right":{"article_title":"11 机器学习 | 步步为营，有章可循：决策树","id":2008}},"comments":[{"had_liked":false,"id":2328,"user_name":"dianxin556","can_delete":false,"product_type":"c1","uid":1046317,"ip_address":"","ucode":"AF6A252560347D","user_header":"","comment_is_top":false,"comment_ctime":1516784697,"is_pvip":false,"replies":[{"id":"484","content":"似然概率是由假设正推结果，后验概率是由结果倒推假设。<br>假如一种产品由不同的工厂ABC生产，每个工厂都有自己的次品率，那么A厂生产的产品占产品总数的比例就是先验概率，A厂自己的次品率就是似然概率。<br>在一堆产品中抽出一件，检验发现是次品，那这件次品可能来源于ABC中任意的一个厂。已知产品是次品，推断这件次品来自A厂的概率，这就是后验概率。<br>","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1516949223,"ip_address":"","comment_id":2328,"utype":1}],"discussion_count":4,"race_medal":0,"score":"78826196025","product_id":100003101,"comment_content":"王老师，请问似然概率和后验概率的区别和联系？能否举例说明？谢谢！","like_count":19,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415706,"discussion_content":"似然概率是由假设正推结果，后验概率是由结果倒推假设。\n假如一种产品由不同的工厂ABC生产，每个工厂都有自己的次品率，那么A厂生产的产品占产品总数的比例就是先验概率，A厂自己的次品率就是似然概率。\n在一堆产品中抽出一件，检验发现是次品，那这件次品可能来源于ABC中任意的一个厂。已知产品是次品，推断这件次品来自A厂的概率，这就是后验概率。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1516949223,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1468901,"avatar":"https://static001.geekbang.org/account/avatar/00/16/69/e5/aac48ead.jpg","nickname":"烫烫烫个喵啊","note":"","ucode":"2DE097F100929A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369930,"discussion_content":"和维基百科相反了，望解惑： 似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是说已观察到某事件后，对相关参数进行猜测。\n\nhttps://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619202757,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1468901,"avatar":"https://static001.geekbang.org/account/avatar/00/16/69/e5/aac48ead.jpg","nickname":"烫烫烫个喵啊","note":"","ucode":"2DE097F100929A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369929,"discussion_content":"为什么我的理解是刚好相反的啊？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619202603,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210746,"discussion_content":"频率学派下是似然概率，贝叶斯学派下是后验概率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584768062,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3717,"user_name":"井中月","can_delete":false,"product_type":"c1","uid":1056027,"ip_address":"","ucode":"4ADBD773A50CE2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132","comment_is_top":false,"comment_ctime":1520384834,"is_pvip":false,"replies":[{"id":"785","content":"抛开问题不说，做多分类两种思路：一是所有类别两两配对，就是1v2, 1v3,...,1vN,2v3,2v4,...2vN,3v4,...依此类推；二是每个类别和其他所有类别配对，就是1v其他，2v其他，…Nv其他。第一种模型多，每个模型需要的数据少，第二种相反。<br>具体到你的问题，我觉得类别太多了，尤其是数据有限时，给500个数据分400个类是没有价值的。所以我认为应该先对类别做优化，当然，能不能行的通还要看实际情况。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1520663914,"ip_address":"","comment_id":3717,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14405286722","product_id":100003101,"comment_content":"王老师，按文章最后一段所说，当一个样本同时属于多个类别时，是不是有几个类别就建立几个二分类模型，这样效率比较高？但是我曾经遇到过一个类似的问题，当时没有解决，想跟您请教。每个样本属于多个类别，这些类别加起来一共有将近400种，而且绝大多数类别都是严重不平衡的。这种情况下如果一个一个的建立模型是不是效率很低？这是一个文本分类的问题，文本是餐厅评论数据，类别是人工标注的评论主题。这些类别其实是可以合并的，但是合并之后意义不大。您怎么看这种情况？","like_count":4,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416023,"discussion_content":"抛开问题不说，做多分类两种思路：一是所有类别两两配对，就是1v2, 1v3,...,1vN,2v3,2v4,...2vN,3v4,...依此类推；二是每个类别和其他所有类别配对，就是1v其他，2v其他，…Nv其他。第一种模型多，每个模型需要的数据少，第二种相反。\n具体到你的问题，我觉得类别太多了，尤其是数据有限时，给500个数据分400个类是没有价值的。所以我认为应该先对类别做优化，当然，能不能行的通还要看实际情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1520663914,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":11044,"user_name":"阿里-赤壁（羊宏飞）","can_delete":false,"product_type":"c1","uid":1145410,"ip_address":"","ucode":"9B4FA44C1E722C","user_header":"https://static001.geekbang.org/account/avatar/00/11/7a/42/9681595e.jpg","comment_is_top":false,"comment_ctime":1527813511,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10117748103","product_id":100003101,"comment_content":"王老师这个课 我认为是 机器学习入门者 非常好的 图谱 ，其中一些知识点还是要结合书本和实战。问题的讨论 知乎上回答的也非常详尽","like_count":3},{"had_liked":false,"id":3552,"user_name":"Jean","can_delete":false,"product_type":"c1","uid":1020405,"ip_address":"","ucode":"BD0EBE23B8A564","user_header":"https://static001.geekbang.org/account/avatar/00/0f/91/f5/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1519883280,"is_pvip":false,"replies":[{"id":"699","content":"逻辑是logistic的音译，logistic函数就是文章里的sigmoid函数，也就是S曲线。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1519998190,"ip_address":"","comment_id":3552,"utype":1}],"discussion_count":2,"race_medal":0,"score":"10109817872","product_id":100003101,"comment_content":"问下逻辑回归中的“逻辑”是什么意思，为什么叫逻辑，是怎么来的？","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415935,"discussion_content":"逻辑是logistic的音译，logistic函数就是文章里的sigmoid函数，也就是S曲线。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1519998190,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210750,"discussion_content":"音译，无特殊含义。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584768263,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":4477,"user_name":"叶秋","can_delete":false,"product_type":"c1","uid":1069054,"ip_address":"","ucode":"F85650EC25B414","user_header":"https://static001.geekbang.org/account/avatar/00/10/4f/fe/cd166eb4.jpg","comment_is_top":false,"comment_ctime":1522132727,"is_pvip":false,"replies":[{"id":"1224","content":"在这里没法打出公式，你可以参考维基百科上逻辑回归的页面，或是推荐书目中周老师和李老师教材的相关章节。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1522415524,"ip_address":"","comment_id":4477,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5817100023","product_id":100003101,"comment_content":"可否推导一下逻辑回归的条件概率的推导过程","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416380,"discussion_content":"在这里没法打出公式，你可以参考维基百科上逻辑回归的页面，或是推荐书目中周老师和李老师教材的相关章节。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1522415524,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3825,"user_name":"井中月","can_delete":false,"product_type":"c1","uid":1056027,"ip_address":"","ucode":"4ADBD773A50CE2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132","comment_is_top":false,"comment_ctime":1520740660,"is_pvip":false,"replies":[{"id":"840","content":"这是取决于具体问题的，建议你也把类别数量作为一个变量试一试，看看类别多少时效果比较好。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1521040433,"ip_address":"","comment_id":3825,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5815707956","product_id":100003101,"comment_content":"谢谢您的回复。我当时设想的是第二种思路，每个类和其他所有类配对。当时的数据量是50多万条。按您的经验来说，一般做分类问题，类别控制在多少个容易取得较好的效果呢？","like_count":1,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":416070,"discussion_content":"这是取决于具体问题的，建议你也把类别数量作为一个变量试一试，看看类别多少时效果比较好。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1521040433,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":1928,"user_name":"MJ小朋友","can_delete":false,"product_type":"c1","uid":1031270,"ip_address":"","ucode":"9420E46C56B664","user_header":"https://static001.geekbang.org/account/avatar/00/0f/bc/66/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1515122215,"is_pvip":false,"replies":[{"id":"331","content":"感谢你的火眼金睛，这两个问题都是符号没有统一造成的。<br>第一个，没有常项b是因为把它看成w的一个分量w_0，并让它和另一个常数x_0=1相乘，但是推导时候这样写又不方便，所以就把b拿出来了；<br>第二个是个习惯问题，这要归咎于平时书写并不规范，不写底数的log就默认是ln运算。在文章中，所有的运算全是自然对数运算。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1515156553,"ip_address":"","comment_id":1928,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5810089511","product_id":100003101,"comment_content":"我好像发现了两个无伤大雅的错误，<br>S型函数b好像漏了，后面推导有了，又没了<br>对数似然函数应该取自然对数，不然没底数不行","like_count":2,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415588,"discussion_content":"感谢你的火眼金睛，这两个问题都是符号没有统一造成的。\n第一个，没有常项b是因为把它看成w的一个分量w_0，并让它和另一个常数x_0=1相乘，但是推导时候这样写又不方便，所以就把b拿出来了；\n第二个是个习惯问题，这要归咎于平时书写并不规范，不写底数的log就默认是ln运算。在文章中，所有的运算全是自然对数运算。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1515156553,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":192043,"user_name":"刘桢","can_delete":false,"product_type":"c1","uid":1482815,"ip_address":"","ucode":"3BFAB1C9772EB4","user_header":"https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg","comment_is_top":false,"comment_ctime":1584830788,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584830788","product_id":100003101,"comment_content":"打卡打卡","like_count":0},{"had_liked":false,"id":165822,"user_name":"杨家荣","can_delete":false,"product_type":"c1","uid":1259241,"ip_address":"","ucode":"3DA65396C7F002","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132","comment_is_top":false,"comment_ctime":1577327420,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1577327420","product_id":100003101,"comment_content":"极客时间<br>21天打卡行动 8&#47;21<br>&lt;&lt;人工智能基础课10&gt;&gt;逻辑回归<br>回答老师问题:<br>前文对逻辑回归的分析都是在概率理论的基础上完成的。但在二分类任务中，逻辑回归的作用可以视为是在平面直角坐标系上划定一条数据分类的判定边界。那么逻辑回归的作用能不能从几何角度理解，并推广到高维空间呢？<br>这个问题有点深奥,根据我自身的学习知识,在逻辑回归算法中,如果用几何去表达,显然平面几何是做不到的,以前我老师讲过一个升维的算法,在平面几何不能表达的回归图形中,假设这个图形不是二维的,是多维度的,在多维度的空间中,是能计算和描绘的;<br>今日所学:Logistic回归(逻辑回归),这个算法本身是解决分类问题的,逻辑回归中，实现这一映射是对数几率函数，也叫 Sigmoid 函数;(Sigmoid函数搜索较大的值,所有值将介于0和1之间),而寻找以上函数的最大值就是以对数似然函数为目标函数的最优化问题，通常通过“梯度下降法”或拟“牛顿法”求解,另外对数似然函数的最大化可以等效为待求模型与最大熵模型之间 KL 散度的最小化。这意味着最优的估计对参数做出的额外假设是最少的，这无疑与最大熵原理不谋而合。<br>逻辑回归与朴素贝叶斯区别:<br>1,逻辑回归与线性回归的关系称得上系出同门，与朴素贝叶斯分类的关系则是殊途同归。两者虽然都可以利用条件概率 P(Y|X) 完成分类任务，实现的路径却截然不同。<br>2,朴素贝叶斯分类器是生成模型的代表，其思想是先由训练数据集估计出输入和输出的联合概率分布，再根据联合概率分布来生成符合条件的输出，P(Y|X) 以后验概率的形式出现。<br>3,两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果<br>4,两者的区别还在于收敛速度的不同;<br>5,朴素贝叶斯基于少量数据集更有优势<br>逻辑回归的改进:一种改进方式是通过多次二分类实现多个类别的标记;另一种多分类的方式通过直接修改逻辑回归输出的似然概率，使之适应多分类问题，得到的模型就是 Softmax 回归,Softmax 回归模型的训练与逻辑回归模型类似，都可以转化为通过梯度下降法或者拟牛顿法解决最优化问题。<br>说明:虽然都能实现多分类的任务，但两种方式的适用范围略有区别。当分类问题的所有类别之间明显互斥，即输出结果只能属于一个类别时，Softmax 分类器是更加高效的选择；当所有类别之间不存在互斥关系，可能有交叉的情况出现时，多个二分类逻辑回归模型就能够得到多个类别的标记。<br>总结:a,逻辑回归模型是对线性回归的改进，用于解决分类问题；b,逻辑回归输出的是实例属于每个类别的似然概率，似然概率最大的类别就是分类结果；c,在一定条件下，逻辑回归模型与朴素贝叶斯分类器是等价的；d,多分类问题时可以通过多次使用二分类逻辑回归或者使用 Softmax 回归解决。<br>","like_count":0},{"had_liked":false,"id":61427,"user_name":"小老鼠","can_delete":false,"product_type":"c1","uid":1257460,"ip_address":"","ucode":"C663A0C863A515","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/f4/2dede51a.jpg","comment_is_top":false,"comment_ctime":1547695294,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1547695294","product_id":100003101,"comment_content":"各位是不是都是数学系毕业的，好难懂","like_count":1,"discussions":[{"author":{"id":1914504,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg","nickname":"Simon","note":"","ucode":"A8A2E3E57BD029","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":210752,"discussion_content":"多看几次，自然懂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584768336,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":55224,"user_name":"历尽千帆","can_delete":false,"product_type":"c1","uid":1270572,"ip_address":"","ucode":"67A48173352375","user_header":"https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg","comment_is_top":false,"comment_ctime":1546069752,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1546069752","product_id":100003101,"comment_content":"王老师，您好。我没有明白“对数似然函数的最大化可以等效为待求模型与最大熵模型之间 KL散度的最小化。这意味着最优的估计对参数做出的额外假设是最少的”这句话，您可否再解释下？","like_count":0},{"had_liked":false,"id":50203,"user_name":"jkhcw","can_delete":false,"product_type":"c1","uid":1324237,"ip_address":"","ucode":"300F1FEAEA9EB4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/EvqrQ1wMs4SzC8dSBjAfVdEZ3yDT0bDUMicfRLq6BOSzjGFhCownt3S5MERXLpOpLmyJXCuyWbeOQG3ibzy0b4ibA/132","comment_is_top":false,"comment_ctime":1544918569,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1544918569","product_id":100003101,"comment_content":"逻辑回归在高纬模型下就是超平面","like_count":0},{"had_liked":false,"id":9281,"user_name":"星运里的错","can_delete":false,"product_type":"c1","uid":1135940,"ip_address":"","ucode":"DA49E91584F0A1","user_header":"https://static001.geekbang.org/account/avatar/00/11/55/44/024204e2.jpg","comment_is_top":false,"comment_ctime":1526728135,"is_pvip":false,"replies":[{"id":"3857","content":"我认为结合起来才有效果，想办法理解例子是如何应用概念的。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1528378199,"ip_address":"","comment_id":9281,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1526728135","product_id":100003101,"comment_content":"老师。我发现很多概念当时明白，过后就忘了，您是推荐从实战例子中去加深理解，还是反复的去看概念，知道看懂","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":418017,"discussion_content":"我认为结合起来才有效果，想办法理解例子是如何应用概念的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1528378199,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":3957,"user_name":"井中月","can_delete":false,"product_type":"c1","uid":1056027,"ip_address":"","ucode":"4ADBD773A50CE2","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132","comment_is_top":false,"comment_ctime":1521074799,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1521074799","product_id":100003101,"comment_content":"好的，谢谢老师","like_count":0},{"had_liked":false,"id":1922,"user_name":"Andy","can_delete":false,"product_type":"c1","uid":1012037,"ip_address":"","ucode":"D69ED1BAF42262","user_header":"https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg","comment_is_top":false,"comment_ctime":1515114742,"is_pvip":false,"replies":[{"id":"332","content":"不要被名字欺骗，逻辑回归的输出是离散值哟，离散变量用最小二乘就没意义了。","user_name":"作者回复","user_name_real":"王天一","uid":"1027523","ctime":1515157639,"ip_address":"","comment_id":1922,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1515114742","product_id":100003101,"comment_content":"王老师您好，在逻辑回归中，代价函数为什么选用交叉熵代价函数，而不是选用最小二乘代价函数呢？","like_count":0,"discussions":[{"author":{"id":1027523,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ad/c3/a9a0450b.jpg","nickname":"王天一","note":"","ucode":"142761D44C4C64","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":415585,"discussion_content":"不要被名字欺骗，逻辑回归的输出是离散值哟，离散变量用最小二乘就没意义了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1515157639,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}