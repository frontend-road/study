{"id":104277,"title":"41 | IPC（中）：不同项目组之间抢资源，如何协调？","content":"<p>了解了如何使用共享内存和信号量集合之后，今天我们来解析一下，内核里面都做了什么。</p><p>不知道你有没有注意到，咱们讲消息队列、共享内存、信号量的机制的时候，我们其实能够从中看到一些统一的规律：<strong>它们在使用之前都要生成key，然后通过key得到唯一的id，并且都是通过xxxget函数。</strong></p><p>在内核里面，这三种进程间通信机制是使用统一的机制管理起来的，都叫ipcxxx。</p><p>为了维护这三种进程间通信进制，在内核里面，我们声明了一个有三项的数组。</p><p>我们通过这段代码，来具体看一看。</p><pre><code>struct ipc_namespace {\n......\n\tstruct ipc_ids\tids[3];\n......\n}\n\n#define IPC_SEM_IDS\t0\n#define IPC_MSG_IDS\t1\n#define IPC_SHM_IDS\t2\n\n#define sem_ids(ns)\t((ns)-&gt;ids[IPC_SEM_IDS])\n#define msg_ids(ns)\t((ns)-&gt;ids[IPC_MSG_IDS])\n#define shm_ids(ns)\t((ns)-&gt;ids[IPC_SHM_IDS])\n</code></pre><p>根据代码中的定义，第0项用于信号量，第1项用于消息队列，第2项用于共享内存，分别可以通过sem_ids、msg_ids、shm_ids来访问。</p><p>这段代码里面有ns，全称叫namespace。可能不容易理解，你现在可以将它认为是将一台Linux服务器逻辑的隔离为多台Linux服务器的机制，它背后的原理是一个相当大的话题，我们需要在容器那一章详细讲述。现在，你就可以简单的认为没有namespace，整个Linux在一个namespace下面，那这些ids也是整个Linux只有一份。</p><p>接下来，我们再来看struct ipc_ids里面保存了什么。</p><!-- [[[read_end]]] --><p>首先，in_use表示当前有多少个ipc；其次，seq和next_id用于一起生成ipc唯一的id，因为信号量，共享内存，消息队列，它们三个的id也不能重复；ipcs_idr是一棵基数树，我们又碰到它了，一旦涉及从一个整数查找一个对象，它都是最好的选择。</p><pre><code>struct ipc_ids {\n\tint in_use;\n\tunsigned short seq;\n\tstruct rw_semaphore rwsem;\n\tstruct idr ipcs_idr;\n\tint next_id;\n};\n\nstruct idr {\n\tstruct radix_tree_root\tidr_rt;\n\tunsigned int\t\tidr_next;\n};\n</code></pre><p>也就是说，对于sem_ids、msg_ids、shm_ids各有一棵基数树。那这棵树里面究竟存放了什么，能够统一管理这三类ipc对象呢？</p><p>通过下面这个函数ipc_obtain_object_idr，我们可以看出端倪。这个函数根据id，在基数树里面找出来的是struct kern_ipc_perm。</p><pre><code>struct kern_ipc_perm *ipc_obtain_object_idr(struct ipc_ids *ids, int id)\n{\n\tstruct kern_ipc_perm *out;\n\tint lid = ipcid_to_idx(id);\n\tout = idr_find(&amp;ids-&gt;ipcs_idr, lid);\n\treturn out;\n}\n</code></pre><p>如果我们看用于表示信号量、消息队列、共享内存的结构，就会发现，这三个结构的第一项都是struct kern_ipc_perm。</p><pre><code>struct sem_array {\n\tstruct kern_ipc_perm\tsem_perm;\t/* permissions .. see ipc.h */\n\ttime_t\t\t\tsem_ctime;\t/* create/last semctl() time */\n\tstruct list_head\tpending_alter;\t/* pending operations */\n\t\t\t\t\t\t                /* that alter the array */\n\tstruct list_head\tpending_const;\t/* pending complex operations */\n\t\t\t\t\t\t/* that do not alter semvals */\n\tstruct list_head\tlist_id;\t/* undo requests on this array */\n\tint\t\t\tsem_nsems;\t/* no. of semaphores in array */\n\tint\t\t\tcomplex_count;\t/* pending complex operations */\n\tunsigned int\t\tuse_global_lock;/* &gt;0: global lock required */\n\n\tstruct sem\t\tsems[];\n} __randomize_layout;\n\nstruct msg_queue {\n\tstruct kern_ipc_perm q_perm;\n\ttime_t q_stime;\t\t\t/* last msgsnd time */\n\ttime_t q_rtime;\t\t\t/* last msgrcv time */\n\ttime_t q_ctime;\t\t\t/* last change time */\n\tunsigned long q_cbytes;\t\t/* current number of bytes on queue */\n\tunsigned long q_qnum;\t\t/* number of messages in queue */\n\tunsigned long q_qbytes;\t\t/* max number of bytes on queue */\n\tpid_t q_lspid;\t\t\t/* pid of last msgsnd */\n\tpid_t q_lrpid;\t\t\t/* last receive pid */\n\n\tstruct list_head q_messages;\n\tstruct list_head q_receivers;\n\tstruct list_head q_senders;\n} __randomize_layout;\n\nstruct shmid_kernel /* private to the kernel */\n{\t\n\tstruct kern_ipc_perm\tshm_perm;\n\tstruct file\t\t*shm_file;\n\tunsigned long\t\tshm_nattch;\n\tunsigned long\t\tshm_segsz;\n\ttime_t\t\t\tshm_atim;\n\ttime_t\t\t\tshm_dtim;\n\ttime_t\t\t\tshm_ctim;\n\tpid_t\t\t\tshm_cprid;\n\tpid_t\t\t\tshm_lprid;\n\tstruct user_struct\t*mlock_user;\n\n\t/* The task created the shm object.  NULL if the task is dead. */\n\tstruct task_struct\t*shm_creator;\n\tstruct list_head\tshm_clist;\t/* list by creator */\n} __randomize_layout;\n</code></pre><p>也就是说，我们完全可以通过struct kern_ipc_perm的指针，通过进行强制类型转换后，得到整个结构。做这件事情的函数如下：</p><pre><code>static inline struct sem_array *sem_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_idr(&amp;sem_ids(ns), id);\n\treturn container_of(ipcp, struct sem_array, sem_perm);\n}\n\nstatic inline struct msg_queue *msq_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_idr(&amp;msg_ids(ns), id);\n\treturn container_of(ipcp, struct msg_queue, q_perm);\n}\n\nstatic inline struct shmid_kernel *shm_obtain_object(struct ipc_namespace *ns, int id)\n{\n\tstruct kern_ipc_perm *ipcp = ipc_obtain_object_idr(&amp;shm_ids(ns), id);\n\treturn container_of(ipcp, struct shmid_kernel, shm_perm);\n}\n</code></pre><p>通过这种机制，我们就可以将信号量、消息队列、共享内存抽象为ipc类型进行统一处理。你有没有觉得，这有点儿面向对象编程中抽象类和实现类的意思？没错，如果你试图去了解C++中类的实现机制，其实也是这么干的。</p><p><img src=\"https://static001.geekbang.org/resource/image/08/af/082b742753d862cfeae520fb02aa41af.png\" alt=\"\"></p><p>有了抽象类，接下来我们来看共享内存和信号量的具体实现。</p><h2>如何创建共享内存？</h2><p>首先，我们来看创建共享内存的的系统调用。</p><pre><code>SYSCALL_DEFINE3(shmget, key_t, key, size_t, size, int, shmflg)\n{\n\tstruct ipc_namespace *ns;\n\tstatic const struct ipc_ops shm_ops = {\n\t\t.getnew = newseg,\n\t\t.associate = shm_security,\n\t\t.more_checks = shm_more_checks,\n\t};\n\tstruct ipc_params shm_params;\n\tns = current-&gt;nsproxy-&gt;ipc_ns;\n\tshm_params.key = key;\n\tshm_params.flg = shmflg;\n\tshm_params.u.size = size;\n\treturn ipcget(ns, &amp;shm_ids(ns), &amp;shm_ops, &amp;shm_params);\n}\n</code></pre><p>这里面调用了抽象的ipcget、参数分别为共享内存对应的shm_ids、对应的操作shm_ops以及对应的参数shm_params。</p><p>如果key设置为IPC_PRIVATE则永远创建新的，如果不是的话，就会调用ipcget_public。ipcget的具体代码如下：</p><pre><code>int ipcget(struct ipc_namespace *ns, struct ipc_ids *ids,\n\t\t\tconst struct ipc_ops *ops, struct ipc_params *params)\n{\n\tif (params-&gt;key == IPC_PRIVATE)\n\t\treturn ipcget_new(ns, ids, ops, params);\n\telse\n\t\treturn ipcget_public(ns, ids, ops, params);\n}\n\nstatic int ipcget_public(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)\n{\n\tstruct kern_ipc_perm *ipcp;\n\tint flg = params-&gt;flg;\n\tint err;\n\tipcp = ipc_findkey(ids, params-&gt;key);\n\tif (ipcp == NULL) {\n\t\tif (!(flg &amp; IPC_CREAT))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\terr = ops-&gt;getnew(ns, params);\n\t} else {\n\t\tif (flg &amp; IPC_CREAT &amp;&amp; flg &amp; IPC_EXCL)\n\t\t\terr = -EEXIST;\n\t\telse {\n\t\t\terr = 0;\n\t\t\tif (ops-&gt;more_checks)\n\t\t\t\terr = ops-&gt;more_checks(ipcp, params);\n......\n\t\t}\n\t}\n\treturn err;\n}\n</code></pre><p>在ipcget_public中，我们会按照key，去查找struct kern_ipc_perm。如果没有找到，那就看是否设置了IPC_CREAT；如果设置了，就创建一个新的。如果找到了，就将对应的id返回。</p><p>我们这里重点看，如何按照参数shm_ops，创建新的共享内存，会调用newseg。</p><pre><code>static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params-&gt;key;\n\tint shmflg = params-&gt;flg;\n\tsize_t size = params-&gt;u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) &gt;&gt; PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tvm_flags_t acctflag = 0;\n......\n\tshp = kvmalloc(sizeof(*shp), GFP_KERNEL);\n......\n\tshp-&gt;shm_perm.key = key;\n\tshp-&gt;shm_perm.mode = (shmflg &amp; S_IRWXUGO);\n\tshp-&gt;mlock_user = NULL;\n\n\tshp-&gt;shm_perm.security = NULL;\n......\n\tfile = shmem_kernel_file_setup(name, size, acctflag);\n......\n\tshp-&gt;shm_cprid = task_tgid_vnr(current);\n\tshp-&gt;shm_lprid = 0;\n\tshp-&gt;shm_atim = shp-&gt;shm_dtim = 0;\n\tshp-&gt;shm_ctim = get_seconds();\n\tshp-&gt;shm_segsz = size;\n\tshp-&gt;shm_nattch = 0;\n\tshp-&gt;shm_file = file;\n\tshp-&gt;shm_creator = current;\n\n\terror = ipc_addid(&amp;shm_ids(ns), &amp;shp-&gt;shm_perm, ns-&gt;shm_ctlmni);\n......\n\tlist_add(&amp;shp-&gt;shm_clist, &amp;current-&gt;sysvshm.shm_clist);\n......\n\tfile_inode(file)-&gt;i_ino = shp-&gt;shm_perm.id;\n\n\tns-&gt;shm_tot += numpages;\n\terror = shp-&gt;shm_perm.id;\n......\n\treturn error;\n}\n</code></pre><p><strong>newseg函数的第一步，通过kvmalloc在直接映射区分配一个struct shmid_kernel结构。</strong>这个结构就是用来描述共享内存的。这个结构最开始就是上面说的struct kern_ipc_perm结构。接下来就是填充这个struct shmid_kernel结构，例如key、权限等。</p><p><strong>newseg函数的第二步，共享内存需要和文件进行关联</strong>。**为什么要做这个呢？我们在讲内存映射的时候讲过，虚拟地址空间可以和物理内存关联，但是物理内存是某个进程独享的。虚拟地址空间也可以映射到一个文件，文件是可以跨进程共享的。</p><p>咱们这里的共享内存需要跨进程共享，也应该借鉴文件映射的思路。只不过不应该映射一个硬盘上的文件，而是映射到一个内存文件系统上的文件。mm/shmem.c里面就定义了这样一个基于内存的文件系统。这里你一定要注意区分shmem和shm的区别，前者是一个文件系统，后者是进程通信机制。</p><p>在系统初始化的时候，shmem_init注册了shmem文件系统shmem_fs_type，并且挂在到了shm_mnt下面。</p><pre><code>int __init shmem_init(void)\n{\n\tint error;\n\terror = shmem_init_inodecache();\n\terror = register_filesystem(&amp;shmem_fs_type);\n\tshm_mnt = kern_mount(&amp;shmem_fs_type);\n......\n\treturn 0;\n}\n\nstatic struct file_system_type shmem_fs_type = {\n\t.owner\t\t= THIS_MODULE,\n\t.name\t\t= &quot;tmpfs&quot;,\n\t.mount\t\t= shmem_mount,\n\t.kill_sb\t= kill_litter_super,\n\t.fs_flags\t= FS_USERNS_MOUNT,\n};\n</code></pre><p>接下来，newseg函数会调用shmem_kernel_file_setup，其实就是在shmem文件系统里面创建一个文件。</p><pre><code>/**\n * shmem_kernel_file_setup - get an unlinked file living in tmpfs which must be kernel internal.  \n * @name: name for dentry (to be seen in /proc/&lt;pid&gt;/maps\n * @size: size to be set for the file\n * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size */\nstruct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned long flags)\n{\n\treturn __shmem_file_setup(name, size, flags, S_PRIVATE);\n}\n\nstatic struct file *__shmem_file_setup(const char *name, loff_t size,\n\t\t\t\t       unsigned long flags, unsigned int i_flags)\n{\n\tstruct file *res;\n\tstruct inode *inode;\n\tstruct path path;\n\tstruct super_block *sb;\n\tstruct qstr this;\n......\n\tthis.name = name;\n\tthis.len = strlen(name);\n\tthis.hash = 0; /* will go */\n\tsb = shm_mnt-&gt;mnt_sb;\n\tpath.mnt = mntget(shm_mnt);\n\tpath.dentry = d_alloc_pseudo(sb, &amp;this);\n\td_set_d_op(path.dentry, &amp;anon_ops);\n......\n\tinode = shmem_get_inode(sb, NULL, S_IFREG | S_IRWXUGO, 0, flags);\n\tinode-&gt;i_flags |= i_flags;\n\td_instantiate(path.dentry, inode);\n\tinode-&gt;i_size = size;\n......\n\tres = alloc_file(&amp;path, FMODE_WRITE | FMODE_READ,\n\t\t  &amp;shmem_file_operations);\n\treturn res;\n}\n</code></pre><p>__shmem_file_setup会创建新的shmem文件对应的dentry和inode，并将它们两个关联起来，然后分配一个struct file结构，来表示新的shmem文件，并且指向独特的shmem_file_operations。</p><pre><code>static const struct file_operations shmem_file_operations = {\n\t.mmap\t\t= shmem_mmap,\n\t.get_unmapped_area = shmem_get_unmapped_area,\n#ifdef CONFIG_TMPFS\n\t.llseek\t\t= shmem_file_llseek,\n\t.read_iter\t= shmem_file_read_iter,\n\t.write_iter\t= generic_file_write_iter,\n\t.fsync\t\t= noop_fsync,\n\t.splice_read\t= generic_file_splice_read,\n\t.splice_write\t= iter_file_splice_write,\n\t.fallocate\t= shmem_fallocate,\n#endif\n};\n</code></pre><p><strong>newseg函数的第三步，通过ipc_addid将新创建的struct shmid_kernel结构挂到shm_ids里面的基数树上，并返回相应的id，并且将struct shmid_kernel挂到当前进程的sysvshm队列中。</strong></p><p>至此，共享内存的创建就完成了。</p><h2>如何将共享内存映射到虚拟地址空间？</h2><p>从上面的代码解析中，我们知道，共享内存的数据结构struct shmid_kernel，是通过它的成员struct file *shm_file，来管理内存文件系统shmem上的内存文件的。无论这个共享内存是否被映射，shm_file都是存在的。</p><p>接下来，我们要将共享内存映射到虚拟地址空间中。调用的是shmat，对应的系统调用如下：</p><pre><code>SYSCALL_DEFINE3(shmat, int, shmid, char __user *, shmaddr, int, shmflg)\n{\n    unsigned long ret;\n    long err;\n    err = do_shmat(shmid, shmaddr, shmflg, &amp;ret, SHMLBA);\n    force_successful_syscall_return();\n    return (long)ret;\n}\n\nlong do_shmat(int shmid, char __user *shmaddr, int shmflg,\n\t      ulong *raddr, unsigned long shmlba)\n{\n\tstruct shmid_kernel *shp;\n\tunsigned long addr = (unsigned long)shmaddr;\n\tunsigned long size;\n\tstruct file *file;\n\tint    err;\n\tunsigned long flags = MAP_SHARED;\n\tunsigned long prot;\n\tint acc_mode;\n\tstruct ipc_namespace *ns;\n\tstruct shm_file_data *sfd;\n\tstruct path path;\n\tfmode_t f_mode;\n\tunsigned long populate = 0;\n......\n\tprot = PROT_READ | PROT_WRITE;\n\tacc_mode = S_IRUGO | S_IWUGO;\n\tf_mode = FMODE_READ | FMODE_WRITE;\n......\n\tns = current-&gt;nsproxy-&gt;ipc_ns;\n\tshp = shm_obtain_object_check(ns, shmid);\n......\n\tpath = shp-&gt;shm_file-&gt;f_path;\n\tpath_get(&amp;path);\n\tshp-&gt;shm_nattch++;\n\tsize = i_size_read(d_inode(path.dentry));\n......\n\tsfd = kzalloc(sizeof(*sfd), GFP_KERNEL);\n......\n\tfile = alloc_file(&amp;path, f_mode,\n\t\t\t  is_file_hugepages(shp-&gt;shm_file) ?\n\t\t\t\t&amp;shm_file_operations_huge :\n\t\t\t\t&amp;shm_file_operations);\n......\n\tfile-&gt;private_data = sfd;\n\tfile-&gt;f_mapping = shp-&gt;shm_file-&gt;f_mapping;\n\tsfd-&gt;id = shp-&gt;shm_perm.id;\n\tsfd-&gt;ns = get_ipc_ns(ns);\n\tsfd-&gt;file = shp-&gt;shm_file;\n\tsfd-&gt;vm_ops = NULL;\n......\n\taddr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &amp;populate, NULL);\n\t*raddr = addr;\n\terr = 0;\n......\n\treturn err;\n}\n</code></pre><p>在这个函数里面，shm_obtain_object_check会通过共享内存的id，在基数树中找到对应的struct shmid_kernel结构，通过它找到shmem上的内存文件。</p><p>接下来，我们要分配一个struct shm_file_data，来表示这个内存文件。将shmem中指向内存文件的shm_file赋值给struct shm_file_data中的file成员。</p><p>然后，我们创建了一个struct file，指向的也是shmem中的内存文件。</p><p>为什么要再创建一个呢？这两个的功能不同，shmem中shm_file用于管理内存文件，是一个中立的，独立于任何一个进程的角色。而新创建的struct file是专门用于做内存映射的，就像咱们在讲内存映射那一节讲过的，一个硬盘上的文件要映射到虚拟地址空间中的时候，需要在vm_area_struct里面有一个struct file *vm_file指向硬盘上的文件，现在变成内存文件了，但是这个结构还是不能少。</p><p>新创建的struct file的private_data，指向struct shm_file_data，这样内存映射那部分的数据结构，就能够通过它来访问内存文件了。</p><p>新创建的struct file的file_operations也发生了变化，变成了shm_file_operations。</p><pre><code>static const struct file_operations shm_file_operations = {\n\t.mmap\t\t= shm_mmap,\n\t.fsync\t\t= shm_fsync,\n\t.release\t= shm_release,\n\t.get_unmapped_area\t= shm_get_unmapped_area,\n\t.llseek\t\t= noop_llseek,\n\t.fallocate\t= shm_fallocate,\n};\n</code></pre><p>接下来，do_mmap_pgoff函数我们遇到过，原来映射硬盘上的文件的时候，也是调用它。这里我们不再详细解析了。它会分配一个vm_area_struct指向虚拟地址空间中没有分配的区域，它的vm_file指向这个内存文件，然后它会调用shm_file_operations的mmap函数，也即shm_mmap进行映射。</p><pre><code>static int shm_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct shm_file_data *sfd = shm_file_data(file);\n\tint ret;\n\tret = __shm_open(vma);\n\tret = call_mmap(sfd-&gt;file, vma);\n\tsfd-&gt;vm_ops = vma-&gt;vm_ops;\n\tvma-&gt;vm_ops = &amp;shm_vm_ops;\n\treturn 0;\n}\n</code></pre><p>shm_mmap中调用了shm_file_data中的file的mmap函数，这次调用的是shmem_file_operations的mmap，也即shmem_mmap。</p><pre><code>static int shmem_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tfile_accessed(file);\n\tvma-&gt;vm_ops = &amp;shmem_vm_ops;\n\treturn 0;\n}\n</code></pre><p>这里面，vm_area_struct的vm_ops指向shmem_vm_ops。等从call_mmap中返回之后，shm_file_data的vm_ops指向了shmem_vm_ops，而vm_area_struct的vm_ops改为指向shm_vm_ops。</p><p>我们来看一下，shm_vm_ops和shmem_vm_ops的定义。</p><pre><code>static const struct vm_operations_struct shm_vm_ops = {\n\t.open\t= shm_open,\t/* callback for a new vm-area open */\n\t.close\t= shm_close,\t/* callback for when the vm-area is released */\n\t.fault\t= shm_fault,\n};\n\nstatic const struct vm_operations_struct shmem_vm_ops = {\n\t.fault\t\t= shmem_fault,\n\t.map_pages\t= filemap_map_pages,\n};\n</code></pre><p>它们里面最关键的就是fault函数，也即访问虚拟内存的时候，访问不到应该怎么办。</p><p>当访问不到的时候，先调用vm_area_struct的vm_ops，也即shm_vm_ops的fault函数shm_fault。然后它会转而调用shm_file_data的vm_ops，也即shmem_vm_ops的fault函数shmem_fault。</p><pre><code>static int shm_fault(struct vm_fault *vmf)\n{\n\tstruct file *file = vmf-&gt;vma-&gt;vm_file;\n\tstruct shm_file_data *sfd = shm_file_data(file);\n\treturn sfd-&gt;vm_ops-&gt;fault(vmf);\n}\n</code></pre><p>虽然基于内存的文件系统，已经为这个内存文件分配了inode，但是内存也却是一点儿都没分配，只有在发生缺页异常的时候才进行分配。</p><pre><code>static int shmem_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf-&gt;vma;\n\tstruct inode *inode = file_inode(vma-&gt;vm_file);\n\tgfp_t gfp = mapping_gfp_mask(inode-&gt;i_mapping);\n......\n\terror = shmem_getpage_gfp(inode, vmf-&gt;pgoff, &amp;vmf-&gt;page, sgp,\n\t\t\t\t  gfp, vma, vmf, &amp;ret);\n......\n}\n\n/*\n * shmem_getpage_gfp - find page in cache, or get from swap, or allocate\n *\n * If we allocate a new one we do not mark it dirty. That's up to the\n * vm. If we swap it in we mark it dirty since we also free the swap\n * entry since a page cannot live in both the swap and page cache.\n *\n * fault_mm and fault_type are only supplied by shmem_fault:\n * otherwise they are NULL.\n */\nstatic int shmem_getpage_gfp(struct inode *inode, pgoff_t index,\n\tstruct page **pagep, enum sgp_type sgp, gfp_t gfp,\n\tstruct vm_area_struct *vma, struct vm_fault *vmf, int *fault_type)\n{\n......\n    page = shmem_alloc_and_acct_page(gfp, info, sbinfo,\n\t\t\t\t\tindex, false);\n......\n}\n</code></pre><p>shmem_fault会调用shmem_getpage_gfp在page cache和swap中找一个空闲页，如果找不到就通过shmem_alloc_and_acct_page分配一个新的页，他最终会调用内存管理系统的alloc_page_vma在物理内存中分配一个页。</p><p>至此，共享内存才真的映射到了虚拟地址空间中，进程可以像访问本地内存一样访问共享内存。</p><h2>总结时刻</h2><p>我们来总结一下共享内存的创建和映射过程。</p><ol>\n<li>调用shmget创建共享内存。</li>\n<li>先通过ipc_findkey在基数树中查找key对应的共享内存对象shmid_kernel是否已经被创建过，如果已经被创建，就会被查询出来，例如producer创建过，在consumer中就会查询出来。</li>\n<li>如果共享内存没有被创建过，则调用shm_ops的newseg方法，创建一个共享内存对象shmid_kernel。例如，在producer中就会新建。</li>\n<li>在shmem文件系统里面创建一个文件，共享内存对象shmid_kernel指向这个文件，这个文件用struct file表示，我们姑且称它为file1。</li>\n<li>调用shmat，将共享内存映射到虚拟地址空间。</li>\n<li>shm_obtain_object_check先从基数树里面找到shmid_kernel对象。</li>\n<li>创建用于内存映射到文件的file和shm_file_data，这里的struct file我们姑且称为file2。</li>\n<li>关联内存区域vm_area_struct和用于内存映射到文件的file，也即file2，调用file2的mmap函数。</li>\n<li>file2的mmap函数shm_mmap，会调用file1的mmap函数shmem_mmap，设置shm_file_data和vm_area_struct的vm_ops。</li>\n<li>内存映射完毕之后，其实并没有真的分配物理内存，当访问内存的时候，会触发缺页异常do_page_fault。</li>\n<li>vm_area_struct的vm_ops的shm_fault会调用shm_file_data的vm_ops的shmem_fault。</li>\n<li>在page cache中找一个空闲页，或者创建一个空闲页。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/20/51/20e8f4e69d47b7469f374bc9fbcf7251.png\" alt=\"\"></p><h2>课堂练习</h2><p>在这里，我们只分析了shm_ids的结构，消息队列的程序我们写过了，但是msg_ids的结构没有解析，你可以试着解析一下。</p><p>欢迎留言和我分享你的疑惑和见解 ，也欢迎可以收藏本节内容，反复研读。你也可以把今天的内容分享给你的朋友，和他一起学习和进步。</p><p></p>","neighbors":{"left":{"article_title":"40 | IPC（上）：不同项目组之间抢资源，如何协调？","id":103724},"right":{"article_title":"42 | IPC（下）：不同项目组之间抢资源，如何协调？","id":104273}},"comments":[]}