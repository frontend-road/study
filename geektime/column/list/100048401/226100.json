{"id":226100,"title":"11｜精准Top K检索：搜索结果是怎么进行打分排序的？","content":"<p>你好，我是陈东。</p><p>在搜索引擎的检索结果中，排在前面几页的检索结果往往质量更好，更符合我们的要求。一般来说，这些高质量检索结果的排名越靠前，这个搜索引擎的用户体验也就越好。可以说，检索结果的排序是否合理，往往决定了一个检索系统的质量。</p><p>所以，在搜索引擎这样的大规模检索系统中，排序是非常核心的一个环节。简单来说，排序就是搜索引擎对符合用户要求的检索结果进行打分，选出得分最高的K个检索结果的过程。这个过程也叫作Top K检索。</p><p>今天，我就和你仔细来聊一聊，搜索引擎在Top K检索中，是如何进行打分排序的。</p><h2>经典的TF-IDF算法是什么？</h2><p>在搜索引擎的应用场景中，检索结果文档和用户输入的查询词之间的相关性越强，网页排名就越靠前。所以，在搜索引擎对检索结果的打分中，查询词和结果文档的相关性是一个非常重要的判断因子。</p><p>那要计算相关性，就必须要提到经典的TF-IDF算法了，它能很好地表示一个词在一个文档中的权重。TF-IDF算法的公式是：<strong>相关性= TF<code>*</code>IDF</strong>。其中，TF是<strong>词频</strong>（Term Frequency），IDF是<strong>逆文档频率</strong>（Inverse Document Frequency）。</p><p>在利用TF-IDF算法计算相关性之前，我们还要理解几个重要概念，分别是词频、文档频率和逆文档频率。</p><!-- [[[read_end]]] --><p><strong>词频</strong>定义的就是一个词项在文档中出现的次数。换一句话说就是，如果一个词项出现了越多次，那这个词在文档中就越重要。</p><p><strong>文档频率</strong>（Document Frequency），指的是这个词项出现在了多少个文档中。你也可以理解为，如果一个词出现在越多的文档中，那这个词就越普遍，越没有区分度。一个极端的例子，比如“的”字，它基本上在每个文档中都会出现，所以它的区分度就非常低。</p><p>那为了方便理解和计算相关性，我们又引入了一个<strong>逆文档频率</strong>的概念。逆文档频率是对文档频率取倒数，它的值越大，这个词的的区分度就越大。</p><p>因此， TF<code>*</code>IDF表示了我们综合考虑了一个词项的重要性和区分度，结合这两个维度，我们就计算出了一个词项和文档的相关性。不过，在计算的过程中，我们会对TF和IDF的值都使用对数函数进行平滑处理。处理过程如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/17/8d/173efe31a43f745f33006f6e3fd54e8d.jpg?wh=1920*741\" alt=\"\"><br>\n使用“相关性 = TF<code>*</code>IDF”，我们可以计算一个词项在一个文档中的权重。但是，很多情况下，一个查询中会有多个词项。不过，这也不用担心，处理起来也很简单。我们直接把每个词项和文档的相关性累加起来，就能计算出查询词和文档的总相关性了。</p><p>这么说可能比较抽象，我列举了一些具体的数字，我们一起动手来计算一下相关性。假设查询词是“极客时间”，它被分成了两个词项“极客”和“时间”。现在有两个文档都包含了“极客”和“时间”，在文档1中，“极客”出现了10次，“时间”出现了10次。而在文档2中，“极客”出现了1次，“时间”出现了100次。</p><p>计算TF-IDF需要的数据如下表所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/5a/ff/5a582cdf2001c40b396076940848a0ff.jpg?wh=1920*518\" alt=\"\"><br>\n那两个文档的最终相关性得分如下：</p><p>文档1打分 =TF<code>*</code>IDF（极客）+ TF<code>*</code>IDF（时间）= (1+lg(10)) * 2  + (1+lg(10)) * 1 = 4 + 2 = 6</p><p>文档2打分 = TF<code>*</code>IDF（极客）+ TF<code>*</code>IDF（时间）=（1+lg(1)) * 2  + (1+lg(100)) * 1 = 2 + 3 = 5</p><p>你会发现，尽管“时间”这个词项在文档2中出现了非常多次，但是，由于“时间”这个词项的IDF值比较低，因此，文档2的打分并没有文档1高。</p><h2>如何使用概率模型中的BM25算法进行打分？</h2><p>不过，在实际使用中，我们往往不会直接使用TF-IDF来计算相关性，而是会以TF-IDF为基础，使用向量模型或者概率模型等更复杂的算法来打分。比如说，概率模型中的<strong>BM25</strong>（Best Matching 25）算法，这个经典算法就可以看作是TF-IDF算法的一种升级。接下来，我们就一起来看看，BM25算法是怎么打分的。</p><p>BM25算法的一个重要的设计思想是，<strong>它认为词频和相关性的关系并不是线性的</strong>。也就是说，随着词频的增加，相关性的增加会越来越不明显，并且还会有一个阈值上限。当词频达到阈值以后，那相关性就不会再增长了。</p><p>因此，BM25对于TF的使用，设立了一个公式，公式如下：<br>\n<img src=\"https://static001.geekbang.org/resource/image/ea/11/ea696534a736b5ca7a7d6eae21c59211.jpg?wh=1920*420\" alt=\"\"></p><p>在这个公式中，随着tf的值逐步变大，权重会趋向于k1 + 1这个固定的阈值上限（将公式的分子分母同时除以tf，就能看出这个上限）。其中，k1是可以人工调整的参数。k1越大，权重上限越大，收敛速度越慢，表示tf越重要。在极端情况下，也就是当k1 = 0时，就表示tf不重要。比如，在下图中，当k1 = 3就比k1 = 1.2时的权重上限要高很多。那按照经验来说，我们会把k1设为1.2。</p><p><img src=\"https://static001.geekbang.org/resource/image/56/63/5685db18a2d833ebd4145b88999f5b63.jpeg?wh=1920*1080\" alt=\"\"></p><p>除了考虑词频，BM25算法还考虑了文档长度的影响，也就是同样一个词项，如果在两篇文档中出现了相同的次数，但一篇文档比较长，而另一篇文档比较短，那一般来说，短文档中这个词项会更重要。这个时候，我们需要在上面的公式中，加入文档长度相关的因子。那么，整个公式就会被改造成如下的样子：</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/2b/d074a5268f9740f03603260876bc942b.jpg?wh=1920*564\" alt=\"\"></p><p>你会看到，分母中的k1部分被乘上了文档长度的权重。其中，l表示当前文档的长度，而L表示全部文档的平均长度。l越长，分母中的k1就会越大，整体的相关性权重就会越小。</p><p>这个公式中除了k1，还有一个可以人工调整的参数b。它的取值范围是0到1，它 代表了文档长度的重要性。当b取0时，我们可以完全不考虑文档长度的影响；而当b取1时，k1的重要性要按照文档长度进行等比例缩放。按照经验，我们会把b设置为0.75，这样的计算效果会比较好。</p><p>除此以外，如果查询词比较复杂，比如说一个词项会重复出现，那我们也可以把它看作是一个短文档，用类似的方法计算词项在查询词中的权重。举个例子，如果我们的查询词是“极客们的极客时间课程”，那么“极客”这个词项，其实在查询词中就出现了两次，它的权重应该比“时间”“课程”这些只出现一次的词项更重要。因此，BM25对词项在查询词中的权重计算公式如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/ae/cc/aee9601e28286c8cd1d413a4937893cc.jpg?wh=1920*426\" alt=\"\"></p><p>其中tf<sub>q</sub> 表示词项在查询词q中的词频，而k2是可以人工调整的参数，它和k1的参数作用是类似的。由于查询词一般不会太长，所以词频也不会很大，因此，我们没必要像对待文档一下，用k1 = 1.2这么小的范围对它进行控制。我们可以放大词频的作用，把k2设置在0~10之间。极端情况下，也就是当k2取0时，表示我们可以完全不考虑查询词中的词项权重。</p><p>好了，前面我们说了这么多种权重公式，有基础的权重公式、文档中词项的权重公式和查询词中词项的权重公式。那在实际使用BM25算法打分的时候，我们该怎么使用这些公式呢？其实，我们可以回顾一下标准的TF-IDF，把其中的TF进行扩展，变为“文档中词项权重”和“查询词中词项权重”的乘积。这样，我们就得到了BM25算法计算一个词项和指定文档相关性的打分公式，公式如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/82/6e/82ba995c606f11945661895df1f3036e.jpg?wh=1920*807\" alt=\"\"></p><p>你会看到，它由IDF、文档中词项权重以及查询词中词项权重这三部分共同组成。</p><p>如果一个查询词q中有多个词项t，那我们就要把每一个词项t和文档d的相关性都计算出来，最后累加。这样，我们就得到了这个查询词q和文档d的相关性打分结果，我们用score(q,d)来表示，公式如下:</p><p><img src=\"https://static001.geekbang.org/resource/image/d0/e8/d01fae777eb1d93fe83d65a8c2b637e8.jpg?wh=1920*780\" alt=\"\"></p><p>这就是完整的BM25算法的表达式了。尽管这个公式看起来比较复杂，但是经过我们刚才一步一步的拆解，你应该可以很好地理解它了，它其实就是对TF-IDF的算法中的TF做了更细致的处理而已。其实，BM25中的IDF的部分，我们也还可以优化，比如，基于二值独立模型对它进行退化处理（这是另一个分析相关性的模型，这里就不具体展开说了）之后，我们就可以得到一个和IDF相似的优化表示，公式如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/f5/83/f5b2ce4b23bd532e124c11fa56db9083.jpg?wh=1920*405\" alt=\"\"></p><p>你可以将它视为IDF的变体，用来替换公式中原有的IDF部分。</p><p>总结来说，BM25算法就是一个对查询词和文档的相关性进行打分的概率模型算法。BM25算法考虑了四个因子，分别为IDF、文档长度、文档中的词频以及查询词中的词频。并且，公式中还加入了3个可以人工调整大小的参数，分别是 ：k1、k2和b。</p><p>因此，BM25算法的效果比TF-IDF更好，应用也更广泛。比如说，在Lucene和Elastic Search这些搜索框架，以及Google这类常见的搜索引擎中，就都支持BM25排序。不过要用好它，你需要结合我们今天讲的内容，更清楚地理解它的原理。这样才能根据不同的场景，去调整相应的参数，从而取得更好的效果。</p><h2>如何使用机器学习来进行打分？</h2><p>随着搜索引擎的越来越重视搜索结果的排序和效果，我们需要考虑的因子也越来越多。比如说，官方的网站是不是会比个人网页在打分上有更高的权重？用户的历史点击行为是否也是相关性的一个衡量指标？</p><p>在当前的主流搜索引擎中，用来打分的主要因子已经有几百种了。如果我们要将这么多的相关因子都考虑进来，再加入更多的参数，那BM25算法是无法满足我们的需求的。</p><p>这个时候，机器学习就可以派上用场了。利用机器学习打分是近些年来比较热门的研究领域，也是许多搜索引擎目前正在使用的打分机制。</p><p>那机器学习具体是怎么打分的呢？原理很简单，就是把不同的打分因子进行加权求和。比如说，有n个打分因子，分别为x<sub>1</sub>到x<sub>n</sub>，而每个因子都有不同的权重，我们记为w<sub>1</sub>到w<sub>n</sub>，那打分公式就是：</p><center style=\"color:;\"> Score = w<sub>1</sub> * x<sub>1</sub> + w<sub>2</sub> * x<sub>2</sub> + w<sub>3</sub> * x<sub>3</sub> + …… + w<sub>n</sub> * x<sub>n</sub>\n </center><p>那你可能会问了，公式中的权重要如何确定呢？这就需要我们利用训练数据，让机器学习在离线阶段，自动学出最合适的权重。这样，就避免了人工制定公式和权重的问题。</p><p>当然，这个打分公式是不能直接使用的，因为它的取值范围是负无穷到正无穷。这是一个跨度很广的范围，并不好衡量和比较相关性。一般来说，我们会使用Sigmoid函数对score进行处理，让它处于(0,1)范围内。</p><p>Sigmoid函数的取值范围是（0，1），它的函数公式和图像如下所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/e8/d1/e8caac9f39afe3edd5ce5ddc62c0ced1.jpg?wh=1920*1080\" alt=\"\"></p><center><span class=\"reference\">Sigmoid函数图像（x代表score，y代表相关性）</span></center><p>Sigmoid函数的特点就是：x值越大，y值越接近于1；x值越小，y值越接近于0。并且，x值在中间一段范围内，相关性的变化最明显，而在两头会发生边际效应递减的现象，这其实也符合我们的日常经验。比方说，一个2-3人的项目要赶进度，一开始增加1、2个人进来，项目进度会提升明显。但如果我们再持续加人进来，那项目的加速就会变平缓了。</p><p>这个打分方案，就是工业界常见的<strong>逻辑回归模型</strong>（Logistic Regression）（至于为什么逻辑回归模型的表现形式是Sigmoid函数，这是另一个话题，这里就不展开说了）。当然，工业界除了逻辑回归模型的打分方案，还有支持向量机模型、梯度下降树等。并且，随着深度学习的发展，也演化出了越来越多的复杂打分算法，比如，使用<strong>深度神经网络模型</strong>（DNN）和相关的变种等。由于机器学习和深度学习是专门的领域，因此相关的打分算法我就不展开了。在这一讲中，你只要记住，机器学习打分模型可以比人工规则打分的方式处理更多的因子，能更好地调整参数就可以了。</p><h2>如何根据打分结果快速进行Top K检索？</h2><p>在给所有的文档打完分以后，接下来，我们就要完成排序的工作了。一般来说，我们可以使用任意一种高效的排序算法来完成排序，比如说，我们可以使用快速排序，它排序的时间代价是O(n log n)。但是，我们还要考虑到，搜索引擎检索出来结果的数量级可能是千万级别的。在这种情况下，即便是O(n log n)的时间代价，也会是一个非常巨大的时间损耗。</p><p>那对于这个问题，我们该怎么优化呢？</p><p>其实，你可以回想一下，我们在使用搜索引擎的时候，一般都不会翻超过100页（如果有兴趣，你可以试着翻翻，看100页以后搜索引擎会显示什么），而且，平均一页只显示10条数据。也就是说，搜索引擎其实只需要显示前1000条数据就够了。因此，在实际系统中，我们不需要返回所有结果，只需要返回Top K个结果就可以。这就是许多大规模检索系统应用的的<strong>Top K检索</strong>了。而且，我们前面的打分过程都是非常精准的，所以我们今天学习的也叫作<strong>精准Top K检索</strong>。</p><p>当然还有非精准的Top K检索，这里先卖个关子，我会在下一讲详细来讲。</p><p>那再回到优化排序上，由于只需要选取Top K个结果，因此我们可以使用堆排序来代替全排序。这样我们就能把排序的时间代价降低到O(n) + O(k log n)（即建堆时间+在堆中选择最大的k个值的时间），而不是原来的O(n log n)。举个例子，如果k是1000，n是1000万，那排序性能就提高了近6倍！这是一个非常有效的性能提升。</p><h2>重点回顾</h2><p>好了，今天的内容就先讲到这里。我们一起来回顾一下，你要掌握的重点内容。</p><p>首先，我们讲了3种打分方法，分别是经典算法TF-IDF、概率模型BM25算法以及机器学习打分。</p><p>在TF-IDF中， TF代表了词项在文档中的权重，而IDF则体现了词项的区分度。尽管TF-IDF很简单，但它是许多更复杂的打分算法的基础。比如说，在使用机器学习进行打分的时候，我们也可以直接将TF-IDF作为一个因子来处理。</p><p>BM25算法则是概率模型中最成功的相关性打分算法。它认为TF对于相关性的影响是有上限的，所以，它不仅同时考虑了IDF、文档长度、文档中的词频，以及查询词中的词频这四个因子， 还给出了3个可以人工调整的参数。这让它的打分效果得到了广泛的认可，能够应用到很多检索系统中。</p><p>不过，因为机器学习可以更大规模地引入更多的打分因子，并且可以自动学习出各个打分因子的权重。所以，利用机器学习进行相关性打分，已经成了目前大规模检索引擎的标配。</p><p>完成打分阶段之后，排序阶段我们要重视排序的效率。对于精准Top K检索，我们可以使用堆排序来代替全排序，只返回我们认为最重要的k个结果。这样，时间代价就是O(n) + O(k log n) ，在数据量级非常大的情况下，它比O(n log n)的检索性能会高得多。</p><h2>课堂讨论</h2><ol>\n<li>\n<p>在今天介绍的精准Top K检索的过程中，你觉得哪个部分是最耗时的？是打分还是排序？</p>\n</li>\n<li>\n<p>你觉得机器学习打分的优点在哪里？你是否使用过机器学习打分？可以把你的使用场景分享出来。</p>\n</li>\n</ol><p>欢迎在留言区畅所欲言，说出你的想法。如果有收获，也欢迎把这一讲分享给你的朋友。</p>","comments":[{"had_liked":false,"id":210030,"user_name":"INFRA_1","can_delete":false,"product_type":"c1","uid":1655744,"ip_address":"","ucode":"7DA033D5278E58","user_header":"","comment_is_top":false,"comment_ctime":1587654744,"is_pvip":false,"replies":[{"id":78335,"content":"作者回复：并不是typo，你其实提了一个好问题，使用堆来完成Top K检索，时间代价是什么呢？其实有两种不同的方法，对应了两个时间代价！\n一种方法，是先对前k个元素建一个堆，这时候时间代价是O(k)。然后接下来，对于每个新加入元素，判断要将堆里的哪个元素替换掉，判断和处理的时间代价是O(logk)。这样这个堆就一直是保持有k个元素。处理完了n个元素以后，时间代价就是O(n logk)。\n而另一种方法，是先对n个元素建一个堆，这时候时间代价是O(n)，然后接下来，我们在这个堆中，进行k次操作，将堆顶的最大值取出。每次取出堆顶元素，调整的时间代价是O(log n)。因此，处理完k个元素，时间代价就是O(k log n)\n你会看到，我在文章中说的O(n) + O(k log n)的方法，其实就是第二种方法，我其实也简单做了描述。至于它和第一种方法相比，哪个更快，你会发现，当n很大的时候，第二种方法，其实时间代价是接近O(n)的，会比第一种方法O(n log k)快上接近log k倍。\n此外，如果不使用堆的话，我们还可以使用快速选择排序（注意，不是快排，但是使用了快排的partition思想），也是可以在O（n）的时间代价完成top K的检索的。","user_name":"编辑回复","user_name_real":"王莹","uid":1743279,"ctime":1587694861,"ip_address":"","comment_id":210030,"utype":2}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师是否typo，使用堆排序的最终时间复杂度应该是nlogk吧","like_count":25},{"had_liked":false,"id":284544,"user_name":"青鸟飞鱼","can_delete":false,"product_type":"c1","uid":1807943,"ip_address":"","ucode":"8C64517DA556FE","user_header":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","comment_is_top":false,"comment_ctime":1616328445,"is_pvip":false,"replies":[{"id":103344,"content":"其实这道题的核心有几个个:\n1.分治处理\n2.哈希统计\n3.堆\n4.内存与磁盘利用\n首先，由于内存有限，肯定是要分批处理的。但是分批也有多种分法，比如你说的平均分为100个小文件，然后每个小文件统计其中每个url的个数。(这里其实就有细化问题:小文件怎么统计url出现次数，是做url排序，还是用hash表)。\n但是，我也可以提供其他思路，比如我顺序读取文件，直接往内存的hash表中放，统计相同的url的个数，当hash表满时(到达内存上限)，我再将hash表以key-value形式写入小文件。这样可以生成n个k-v小文件。这是另一种分法。\n然后对多个小文件合并，小文件中的key可能会重复，如果每个小文件的key都是有序的，那么可以用归并排序，快速进行key的合并。可以写成合并成一个有序大文件，文件内容是有序的key-value。\n最后使用堆，读取该文件，保留top 100。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1616509200,"ip_address":"","comment_id":284544,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"请教老师一个面试题相关的top k问题，100 GB 的 URL 文件，进程中使用最多 1 GB 内存，计算出现次数 Top 100 的 URL 和各自的出现次数，性能越快越好。我现在的想法是把大文件依次读取url求哈希，分为100个小文件。小文件多线程统计个数后，构建一个100大小的大顶堆。最后把100个大顶堆，再够成一个100大小的小顶堆。","like_count":10,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517385,"discussion_content":"其实这道题的核心有几个个:\n1.分治处理\n2.哈希统计\n3.堆\n4.内存与磁盘利用\n首先，由于内存有限，肯定是要分批处理的。但是分批也有多种分法，比如你说的平均分为100个小文件，然后每个小文件统计其中每个url的个数。(这里其实就有细化问题:小文件怎么统计url出现次数，是做url排序，还是用hash表)。\n但是，我也可以提供其他思路，比如我顺序读取文件，直接往内存的hash表中放，统计相同的url的个数，当hash表满时(到达内存上限)，我再将hash表以key-value形式写入小文件。这样可以生成n个k-v小文件。这是另一种分法。\n然后对多个小文件合并，小文件中的key可能会重复，如果每个小文件的key都是有序的，那么可以用归并排序，快速进行key的合并。可以写成合并成一个有序大文件，文件内容是有序的key-value。\n最后使用堆，读取该文件，保留top 100。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616509200,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1218937,"avatar":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","nickname":"anker","note":"","ucode":"6EDF1FB9D45238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388060,"discussion_content":"可以参考一下 mysql 实现 grace hash join 的 spill to disk 方案","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628575653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360739,"discussion_content":"还有文件的分割方法，还可以根据哈希值取模，将同一个URL分到同一个小文件中。这样就能保证每个文件中的key都是唯一的。\n但是这样分的缺点在于，如果URL不均衡，极端情况全部都是一个URL，那么这一步取模分割大文件是无效的！\n但如果相对均匀，那么针对每个文件，如果文件可以装入内存，就内存统计;如果文件不能装入内存，就再拆分或文件内利用磁盘+内存排序。\n因此你会看到，这道题还需要你全面思考和分析极端情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616510528,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1807943,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","nickname":"青鸟飞鱼","note":"","ucode":"8C64517DA556FE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":360788,"discussion_content":"老师，如果url比较分散，虽然在同一个文件中，线程读取出来，哈希统计，有可能内存不够。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616514714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":360739,"ip_address":"","group_id":0},"score":360788,"extra":""},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1807943,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","nickname":"青鸟飞鱼","note":"","ucode":"8C64517DA556FE","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362026,"discussion_content":"是的。所以才说需要进一步拆分文件。而我之前提供的第一种方案的思路可以解决这个问题。两种思路也可以结合使用。即基于hash拆分文件，和对单个文件采用分批读入内存处理(第一种方法)的方案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616829191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":360788,"ip_address":"","group_id":0},"score":362026,"extra":""},{"author":{"id":1807943,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","nickname":"青鸟飞鱼","note":"","ucode":"8C64517DA556FE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":362932,"discussion_content":"我把老师思路整理了一下，改善了算法。依次读取URL文件，每次读取10M（不按行读取，避免太多次的内核态切换），解析Url对其哈希，分配到相应的组中（分为128组），分配组的同时对其进行哈希统计个数，暂存在内存中，每次都处理完10M后，去检测缓存ur，是否超过一定值，如果超过就刷盘；如果没有，继续读取，解析，哈希分组，统计个数。当处理完之后，检查每个哈希文件大小是否超过了1G/并发个数（逻辑核8个，开了8个线程，考虑避免线程切换）。如果超过继续读取，统计哈希个数，直至文件大小小于1/并发个数。然后8个线程同时处理不同文件，统计个数，堆排序。最后128个堆，进行合并成一个100大小的堆。按照这个思路，提交答案，好像挂了，实在想不通理由。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617075519,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362026,"ip_address":"","group_id":0},"score":362932,"extra":""}]}]},{"had_liked":false,"id":208331,"user_name":"黄海峰","can_delete":false,"product_type":"c1","uid":1275357,"ip_address":"","ucode":"E9340719BC96B2","user_header":"https://static001.geekbang.org/account/avatar/00/13/75/dd/9ead6e69.jpg","comment_is_top":false,"comment_ctime":1587340667,"is_pvip":false,"replies":[{"id":77795,"content":"我当初在写这一篇的时候也很纠结，不确定是否应该保留还是删除。不过最后出于知识体系的完整性，我还是保留了这一篇，并尽可能用通俗的语言来描述。\n不过不用担心，这是唯一一篇有公式的文章，如果公式不好理解也没关系，你只要知道，打分排序的代价很大就好了。下一篇就会恢复熟悉的味道。\n还有，对于不清楚的地方，其实你可以找些时间，重新多读几次，也许哪天你就能取得意想不到的突破。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587343045,"ip_address":"","comment_id":208331,"utype":1}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"太难了，跟不上了，每次看到一堆公式里面的各种符号都是很排斥然后就放弃了阅读。。。数学果然是分水岭","like_count":9,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":517385,"discussion_content":"其实这道题的核心有几个个:\n1.分治处理\n2.哈希统计\n3.堆\n4.内存与磁盘利用\n首先，由于内存有限，肯定是要分批处理的。但是分批也有多种分法，比如你说的平均分为100个小文件，然后每个小文件统计其中每个url的个数。(这里其实就有细化问题:小文件怎么统计url出现次数，是做url排序，还是用hash表)。\n但是，我也可以提供其他思路，比如我顺序读取文件，直接往内存的hash表中放，统计相同的url的个数，当hash表满时(到达内存上限)，我再将hash表以key-value形式写入小文件。这样可以生成n个k-v小文件。这是另一种分法。\n然后对多个小文件合并，小文件中的key可能会重复，如果每个小文件的key都是有序的，那么可以用归并排序，快速进行key的合并。可以写成合并成一个有序大文件，文件内容是有序的key-value。\n最后使用堆，读取该文件，保留top 100。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616509200,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1218937,"avatar":"https://static001.geekbang.org/account/avatar/00/12/99/79/74d4f24f.jpg","nickname":"anker","note":"","ucode":"6EDF1FB9D45238","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":388060,"discussion_content":"可以参考一下 mysql 实现 grace hash join 的 spill to disk 方案","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628575653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":360739,"discussion_content":"还有文件的分割方法，还可以根据哈希值取模，将同一个URL分到同一个小文件中。这样就能保证每个文件中的key都是唯一的。\n但是这样分的缺点在于，如果URL不均衡，极端情况全部都是一个URL，那么这一步取模分割大文件是无效的！\n但如果相对均匀，那么针对每个文件，如果文件可以装入内存，就内存统计;如果文件不能装入内存，就再拆分或文件内利用磁盘+内存排序。\n因此你会看到，这道题还需要你全面思考和分析极端情况。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616510528,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":3,"child_discussions":[{"author":{"id":1807943,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","nickname":"青鸟飞鱼","note":"","ucode":"8C64517DA556FE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":360788,"discussion_content":"老师，如果url比较分散，虽然在同一个文件中，线程读取出来，哈希统计，有可能内存不够。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616514714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":360739,"ip_address":"","group_id":0},"score":360788,"extra":""},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1807943,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","nickname":"青鸟飞鱼","note":"","ucode":"8C64517DA556FE","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":362026,"discussion_content":"是的。所以才说需要进一步拆分文件。而我之前提供的第一种方案的思路可以解决这个问题。两种思路也可以结合使用。即基于hash拆分文件，和对单个文件采用分批读入内存处理(第一种方法)的方案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1616829191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":360788,"ip_address":"","group_id":0},"score":362026,"extra":""},{"author":{"id":1807943,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","nickname":"青鸟飞鱼","note":"","ucode":"8C64517DA556FE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":362932,"discussion_content":"我把老师思路整理了一下，改善了算法。依次读取URL文件，每次读取10M（不按行读取，避免太多次的内核态切换），解析Url对其哈希，分配到相应的组中（分为128组），分配组的同时对其进行哈希统计个数，暂存在内存中，每次都处理完10M后，去检测缓存ur，是否超过一定值，如果超过就刷盘；如果没有，继续读取，解析，哈希分组，统计个数。当处理完之后，检查每个哈希文件大小是否超过了1G/并发个数（逻辑核8个，开了8个线程，考虑避免线程切换）。如果超过继续读取，统计哈希个数，直至文件大小小于1/并发个数。然后8个线程同时处理不同文件，统计个数，堆排序。最后128个堆，进行合并成一个100大小的堆。按照这个思路，提交答案，好像挂了，实在想不通理由。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617075519,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":362026,"ip_address":"","group_id":0},"score":362932,"extra":""}]}]},{"had_liked":false,"id":208441,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1587359196,"is_pvip":false,"replies":[{"id":77873,"content":"如果是or的话，那么其实也可以加权再相加。你会发现，由人工来制定规则的确会头大，因此不如交给机器学习打分，直接预估每个文档的点击率就好。\n\n至于为什么写这一篇，我也补充几点:\n1.对于一个完整的检索系统，检索结果的排序是无法回避的问题，包括在前面的课程中，留言已经有人提问，说如果检索出来的结果很多该怎么办？因此，这一篇文章能补全这个知识点，让我们知道搜索引擎，广告引擎和推荐系统是如何处理检索结果的。\n2.索引构建和打分机制其实是有关系的。如果你在使用elastic search，并且使用了基于文档的索引拆分，然后又选择了系统自带的tf-idf或者bm25进行相关性打分，那么如果处理不当，相关性计算会变差。(因为idf没有在全局被更新)。\n3.在下一篇讲如何加速检索的方案，你会发现好些内容会和这篇文章有关。敬请期待。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587364067,"ip_address":"","comment_id":208441,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"我怎么感觉是构建训练集标注数据最耗时呢😂😂😂。\n正经点，我觉得打分不应该是在查询的时候实时算，每个词项的分数应该以一定的频率更新，毕竟只要文档基数上来了，加些文档，词项的分数影响不大。还有就是老师这里说的是查询项包含多个词项，然后用多个词项的分数代表文档分数，但这前提条件是词项是and且同比重，如果是or啥的，好了我不想了我头大。感觉老师这块打分排序整体上的逻辑没有和之前的搜索串连上，让我有种脱节的感觉。","like_count":6,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492421,"discussion_content":"我当初在写这一篇的时候也很纠结，不确定是否应该保留还是删除。不过最后出于知识体系的完整性，我还是保留了这一篇，并尽可能用通俗的语言来描述。\n不过不用担心，这是唯一一篇有公式的文章，如果公式不好理解也没关系，你只要知道，打分排序的代价很大就好了。下一篇就会恢复熟悉的味道。\n还有，对于不清楚的地方，其实你可以找些时间，重新多读几次，也许哪天你就能取得意想不到的突破。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587343045,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1809802,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","nickname":"一元(eudict)","note":"","ucode":"5E7A33642FC767","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262439,"discussion_content":"不然怎么说万物基于数学嘛。老师千万别因为个别人而降低了专栏的完整度和精彩程度。看不懂就反复看，有些人宁愿吃身体上的哭也不愿意吃学习的苦，没办法，还是劝退吧","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1589097891,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1743064,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/98/d8/db4cef94.jpg","nickname":"Alan","note":"","ucode":"C656A2B4FF8C50","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":307891,"discussion_content":"我倒是希望可以提升一下完整度，内容有点偏简单了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1600781695,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2438942,"avatar":"https://static001.geekbang.org/account/avatar/00/25/37/1e/9b6554b6.jpg","nickname":"Q","note":"","ucode":"1A20AD48EE2B8C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628466,"discussion_content":"就这公式还觉得复杂，那估计不适合看这种专栏","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695338550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"湖南","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294087,"user_name":"托尼斯威特","can_delete":false,"product_type":"c1","uid":1729060,"ip_address":"","ucode":"98A1035527292E","user_header":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","comment_is_top":false,"comment_ctime":1621754490,"is_pvip":false,"replies":[{"id":107522,"content":"好的。这里要注意一点:tf和idf值其实在索引构建的时候就可以统计好的。你可以看一下索引构建那一章，在完整的索引里，doc的信息会包含了tf值。然后idf的信息可以存在token里。因此，完整的流程如下:\n1.在索引构建时，在建立posting list时，在doc信息中存入tf的值，在token存入idf值。\n2.在四个posting list做了归并以后，这时候我们会得到符合检索条件的候选doc列表。在归并的时候，同时累加tf*idf的值。就能得到每个doc的打分结果。\n3.接下来，对于打好分的文档进行排序。就是最终结果。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1622902653,"ip_address":"","comment_id":294087,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师你好, 能否重新阐述一下打分是什么时候起作用的? \n根据前几节的内容, 如果我搜索&quot;极客 时间 检索 技术&quot;, 它会被拆分成4个token, 返回4个 posting list, 然后作多路归并, 得到同时含有4个token 的doc id列表. \n\n4个token在每个doc 里的 TF 都不一样, 每个token的IDF也不一样, 最后怎么影响 搜索结果的排序? ","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492457,"discussion_content":"如果是or的话，那么其实也可以加权再相加。你会发现，由人工来制定规则的确会头大，因此不如交给机器学习打分，直接预估每个文档的点击率就好。\n\n至于为什么写这一篇，我也补充几点:\n1.对于一个完整的检索系统，检索结果的排序是无法回避的问题，包括在前面的课程中，留言已经有人提问，说如果检索出来的结果很多该怎么办？因此，这一篇文章能补全这个知识点，让我们知道搜索引擎，广告引擎和推荐系统是如何处理检索结果的。\n2.索引构建和打分机制其实是有关系的。如果你在使用elastic search，并且使用了基于文档的索引拆分，然后又选择了系统自带的tf-idf或者bm25进行相关性打分，那么如果处理不当，相关性计算会变差。(因为idf没有在全局被更新)。\n3.在下一篇讲如何加速检索的方案，你会发现好些内容会和这篇文章有关。敬请期待。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587364067,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215757,"user_name":"一元(eudict)","can_delete":false,"product_type":"c1","uid":1809802,"ip_address":"","ucode":"5E7A33642FC767","user_header":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","comment_is_top":false,"comment_ctime":1589097217,"is_pvip":false,"replies":[{"id":79847,"content":"你看得很细致也很严谨。的确严格来说，log是要标明底数的。但是你也会发现，许多场合都会直接写log n，因为在时间代价分析中，我们更看重的是量级，而不是常数级的倍数差异。因此以2为底还是以10为底，不会影响结论。另一方面，其实log不写底数时，大家约定俗成有默认底数(10或2都有，看具体上下文场景)。但的确写上会更清晰。谢谢你的细心指出，我看看后续优化一下。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589111310,"ip_address":"","comment_id":215757,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师你好，我有一个问题想请教，log(10)为什么等于1呢？不应该是lg(10)才等于1吗？仅仅log(10)并未标明底数呀？其实在前面的表示时间复杂度上就有此疑问，但后面想想时间复杂度仅仅是表示的变化趋势并不是具体的值也就能说通了。但这儿确实有些不解","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520504,"discussion_content":"好的。这里要注意一点:tf和idf值其实在索引构建的时候就可以统计好的。你可以看一下索引构建那一章，在完整的索引里，doc的信息会包含了tf值。然后idf的信息可以存在token里。因此，完整的流程如下:\n1.在索引构建时，在建立posting list时，在doc信息中存入tf的值，在token存入idf值。\n2.在四个posting list做了归并以后，这时候我们会得到符合检索条件的候选doc列表。在归并的时候，同时累加tf*idf的值。就能得到每个doc的打分结果。\n3.接下来，对于打好分的文档进行排序。就是最终结果。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1622902653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1057843,"avatar":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","nickname":"阿甘","note":"","ucode":"BC93175B70E05D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":551314,"discussion_content":"对于tf/idf的可以这么做，对于机器学习的打分这个又是怎么处理的呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644982159,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":520504,"ip_address":"","group_id":0},"score":551314,"extra":""}]}]},{"had_liked":false,"id":208438,"user_name":"王坤祥","can_delete":false,"product_type":"c1","uid":1003327,"ip_address":"","ucode":"FB988B9F381A33","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4f/3f/6f62f982.jpg","comment_is_top":false,"comment_ctime":1587358397,"is_pvip":false,"replies":[{"id":77872,"content":"打分的确是非常耗时的过程，因此，我们需要有更多的手段来加速打分过程。GPU的发展，还有矩阵运算等都是可用的手段。\n\n还有，你提到了不少资料都把深度学习称为“用数据编程”。不过，我更愿意称为“用数据推导规则”。实际上，无论是机器学习还是深度学习，目前其实离我们真正期待的智能还有一段距离，从文中的简单例子你可以看到，机器学习更多是基于巨大的算力，用数据去拟合出一个有限的规则表达式，这个学习出来的规则其实还有许多可完善的地方，比如说人脸识别，有人就做过实验，给一个五官完整，但是比例和位置一看就不对的人脸图像给模型识别，结果模型识别为“真人”。因此，在人工智能方向上，我们还需要再往前走。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587363297,"ip_address":"","comment_id":208438,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"使用堆排序替换全排后，打分应该是最耗时的。个人感觉是打分的计算过程复杂，涉及到较多的特殊运算。现代一般的CPU面向的不是纯“算数”了，在当前大数据环境下，大量的纯数据计算需要使用对“算数”优化过的GPU来提高计算速度，还有就是把大量单一的数据运算变为矩阵运算（并行），也会显著提高数据的运算速度。\n\n之前读过一些资料，机器学习不同于以往的编程，它是一种逆向思维--使用数据来编程；看到不少资料都把深度学习称为「用数据编程」，这是一种更高级的编程，也是未来新一代程序员需要掌握的方法。\n\n在深度学习中，理论上我们不需要关注从A-&gt;B的推理逻辑，只要有标签化的数据，通过深度学习就能找到从A-&gt;B的计算模型。也就是说，机器学习适用性会更好，代价就是训练数据需要花费很多的时间。\n\n一点点感悟，不知道自己理解的对不对。\n\n","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492455,"discussion_content":"打分的确是非常耗时的过程，因此，我们需要有更多的手段来加速打分过程。GPU的发展，还有矩阵运算等都是可用的手段。\n\n还有，你提到了不少资料都把深度学习称为“用数据编程”。不过，我更愿意称为“用数据推导规则”。实际上，无论是机器学习还是深度学习，目前其实离我们真正期待的智能还有一段距离，从文中的简单例子你可以看到，机器学习更多是基于巨大的算力，用数据去拟合出一个有限的规则表达式，这个学习出来的规则其实还有许多可完善的地方，比如说人脸识别，有人就做过实验，给一个五官完整，但是比例和位置一看就不对的人脸图像给模型识别，结果模型识别为“真人”。因此，在人工智能方向上，我们还需要再往前走。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587363297,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208432,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1587357136,"is_pvip":false,"replies":[{"id":77868,"content":"你的问题很好！\n1.对于df，的确会随着文档的增加而变化。因此是需要更新的。在倒排索引中，由于idf是和key存在一起的，因此，我们可以在文档变化时，对增量倒排索引的key中的idf值进行更新就可以了。不过要注意:如果使用了基于文档的水平拆分，那么增量索引只会在一个分片中生效。这样如果持续久了，idf值会不准，相关性计算精度会下降。因此，需要周期性地重构全量索引。\n2.机器学习的模型也是需要频繁更新的。一般来说是每天都会更新，还有系统为了更新及时，还会使用在线学习进行更实时的模型更新。不过，我前面说的“机器学习模型更新”，指的是因子和对应权重的更新，并不是你问题中说的“重新计算分数”。因为对一个文档和当前查询词的相关性打分过程，本来就是在查询发生时实时进行的，而不是离线算好的。因此不存在“重复计算分数”这个问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587360678,"ip_address":"","comment_id":208432,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"有两个疑惑的地方，请教下老师。\n1.文档频率df，随着文档数量的增多，df应该会重新计算么？如果是需要重新计算，也需要批量更新所有文档的分数吧？\n2.机器学习计算分数，随着数量量增大，模型会越来越准确。此时是否需要对于之前已经算过分数的文档重新计算分数呢？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492454,"discussion_content":"你的问题很好！\n1.对于df，的确会随着文档的增加而变化。因此是需要更新的。在倒排索引中，由于idf是和key存在一起的，因此，我们可以在文档变化时，对增量倒排索引的key中的idf值进行更新就可以了。不过要注意:如果使用了基于文档的水平拆分，那么增量索引只会在一个分片中生效。这样如果持续久了，idf值会不准，相关性计算精度会下降。因此，需要周期性地重构全量索引。\n2.机器学习的模型也是需要频繁更新的。一般来说是每天都会更新，还有系统为了更新及时，还会使用在线学习进行更实时的模型更新。不过，我前面说的“机器学习模型更新”，指的是因子和对应权重的更新，并不是你问题中说的“重新计算分数”。因为对一个文档和当前查询词的相关性打分过程，本来就是在查询发生时实时进行的，而不是离线算好的。因此不存在“重复计算分数”这个问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587360678,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":213606,"user_name":"paulhaoyi","can_delete":false,"product_type":"c1","uid":1105619,"ip_address":"","ucode":"C972F4B459E7D6","user_header":"https://static001.geekbang.org/account/avatar/00/10/de/d3/2aa0177f.jpg","comment_is_top":false,"comment_ctime":1588489712,"is_pvip":false,"replies":[{"id":79174,"content":"你说得很对。其实我在文中也提到了，机器学习打分，会考虑更多的维度了，包括“用户的历史点击行为”，这其实就是你说的“考虑用户”了。\n而关于搜索和推荐，它们的许多底层技术是相通的，区别在于两者的约束条件和目标不同。搜索引擎由于有着明确的用户输入和意图，因此对于query相关性的要求很高。而推荐引擎由于没有明确的用户输入信号，因此自由度会更大，可以从更多的维度去考虑检索和排序问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588493431,"ip_address":"","comment_id":213606,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"关于问题2，机器学习打分，不只是可以考虑内容和检索词本身吧？还可以考虑用户，甚至环境，上下问，这里，是不是可以近似一个推荐逻辑了？感觉检索和推荐有点像EE问题的两端，一个倾向于精确些，一个倾向于扩展些？","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492454,"discussion_content":"你的问题很好！\n1.对于df，的确会随着文档的增加而变化。因此是需要更新的。在倒排索引中，由于idf是和key存在一起的，因此，我们可以在文档变化时，对增量倒排索引的key中的idf值进行更新就可以了。不过要注意:如果使用了基于文档的水平拆分，那么增量索引只会在一个分片中生效。这样如果持续久了，idf值会不准，相关性计算精度会下降。因此，需要周期性地重构全量索引。\n2.机器学习的模型也是需要频繁更新的。一般来说是每天都会更新，还有系统为了更新及时，还会使用在线学习进行更实时的模型更新。不过，我前面说的“机器学习模型更新”，指的是因子和对应权重的更新，并不是你问题中说的“重新计算分数”。因为对一个文档和当前查询词的相关性打分过程，本来就是在查询发生时实时进行的，而不是离线算好的。因此不存在“重复计算分数”这个问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587360678,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208328,"user_name":"pedro","can_delete":false,"product_type":"c1","uid":1200704,"ip_address":"","ucode":"F40C839DDFD599","user_header":"https://static001.geekbang.org/account/avatar/00/12/52/40/e57a736e.jpg","comment_is_top":false,"comment_ctime":1587340503,"is_pvip":false,"replies":[{"id":77794,"content":"1.的确，最耗时的步骤其实是打分，因此，我们如果想加快检索效率的话，如何优化打分过程就是一个很重要的方向。\n2.机器学习打分的例子，的确文中没有说得太详细。其实机器学习的关键就是寻找因子(也就是特征)，以及学习权重的过程。对于机器学习和特征工程方向，这其实是另一个领域，也许有机会可以在其他地方再具体描述一下。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587342557,"ip_address":"","comment_id":208328,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"问题1，耗时的是打分，尤其是引入了深度学习的计算量大大超过了排序。\n问题2，个人觉得机器学习打分的一个不可忽视的优点，那就是它会根据用户行为不断进行学习，不断优化自己，从而获得更好的用户体验。暂时没有使用过，老师可以后面举一个机器学习打分的具体算法和例子吗，机器学习毕竟有些笼统。","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493825,"discussion_content":"你说得很对。其实我在文中也提到了，机器学习打分，会考虑更多的维度了，包括“用户的历史点击行为”，这其实就是你说的“考虑用户”了。\n而关于搜索和推荐，它们的许多底层技术是相通的，区别在于两者的约束条件和目标不同。搜索引擎由于有着明确的用户输入和意图，因此对于query相关性的要求很高。而推荐引擎由于没有明确的用户输入信号，因此自由度会更大，可以从更多的维度去考虑检索和排序问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588493431,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1105619,"avatar":"https://static001.geekbang.org/account/avatar/00/10/de/d3/2aa0177f.jpg","nickname":"paulhaoyi","note":"","ucode":"C972F4B459E7D6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":256524,"discussion_content":"另外，通过检索词去检索文档的过程，也特别像推荐去查询召回源。只不过也是侧重不同。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588490722,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":210030,"user_name":"INFRA_1","can_delete":false,"product_type":"c1","uid":1655744,"ip_address":"","ucode":"7DA033D5278E58","user_header":"","comment_is_top":false,"comment_ctime":1587654744,"is_pvip":false,"replies":[{"id":78335,"content":"作者回复：并不是typo，你其实提了一个好问题，使用堆来完成Top K检索，时间代价是什么呢？其实有两种不同的方法，对应了两个时间代价！\n一种方法，是先对前k个元素建一个堆，这时候时间代价是O(k)。然后接下来，对于每个新加入元素，判断要将堆里的哪个元素替换掉，判断和处理的时间代价是O(logk)。这样这个堆就一直是保持有k个元素。处理完了n个元素以后，时间代价就是O(n logk)。\n而另一种方法，是先对n个元素建一个堆，这时候时间代价是O(n)，然后接下来，我们在这个堆中，进行k次操作，将堆顶的最大值取出。每次取出堆顶元素，调整的时间代价是O(log n)。因此，处理完k个元素，时间代价就是O(k log n)\n你会看到，我在文章中说的O(n) + O(k log n)的方法，其实就是第二种方法，我其实也简单做了描述。至于它和第一种方法相比，哪个更快，你会发现，当n很大的时候，第二种方法，其实时间代价是接近O(n)的，会比第一种方法O(n log k)快上接近log k倍。\n此外，如果不使用堆的话，我们还可以使用快速选择排序（注意，不是快排，但是使用了快排的partition思想），也是可以在O（n）的时间代价完成top K的检索的。","user_name":"编辑回复","user_name_real":"王莹","uid":1743279,"ctime":1587694861,"ip_address":"","comment_id":210030,"utype":2}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师是否typo，使用堆排序的最终时间复杂度应该是nlogk吧","like_count":25,"discussions":[{"author":{"id":1743279,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/99/af/e4cc7374.jpg","nickname":"啊呜","note":"","ucode":"76E24AB808868D","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492920,"discussion_content":"作者回复：并不是typo，你其实提了一个好问题，使用堆来完成Top K检索，时间代价是什么呢？其实有两种不同的方法，对应了两个时间代价！\n一种方法，是先对前k个元素建一个堆，这时候时间代价是O(k)。然后接下来，对于每个新加入元素，判断要将堆里的哪个元素替换掉，判断和处理的时间代价是O(logk)。这样这个堆就一直是保持有k个元素。处理完了n个元素以后，时间代价就是O(n logk)。\n而另一种方法，是先对n个元素建一个堆，这时候时间代价是O(n)，然后接下来，我们在这个堆中，进行k次操作，将堆顶的最大值取出。每次取出堆顶元素，调整的时间代价是O(log n)。因此，处理完k个元素，时间代价就是O(k log n)\n你会看到，我在文章中说的O(n) + O(k log n)的方法，其实就是第二种方法，我其实也简单做了描述。至于它和第一种方法相比，哪个更快，你会发现，当n很大的时候，第二种方法，其实时间代价是接近O(n)的，会比第一种方法O(n log k)快上接近log k倍。\n此外，如果不使用堆的话，我们还可以使用快速选择排序（注意，不是快排，但是使用了快排的partition思想），也是可以在O（n）的时间代价完成top K的检索的。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587694861,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2891795,"avatar":"https://static001.geekbang.org/account/avatar/00/2c/20/13/71179a6c.jpg","nickname":"墨眉无锋","note":"","ucode":"FFB1A26FE49D3E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546919,"discussion_content":"第二种建堆方法岂不是内存申请很大，这又是空间换时间的折中么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642470245,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194253,"avatar":"https://static001.geekbang.org/account/avatar/00/12/39/0d/80ae66d7.jpg","nickname":"zzz","note":"","ucode":"6A5752C924D917","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546099,"discussion_content":"topK的完整时间复杂度是O(k*n*log(k）)，老师您的公司不对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642160388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":245697,"discussion_content":"并不是typo，你其实提了一个好问题，使用堆来完成Top K检索，时间代价是什么呢？其实有两种不同的方法，对应了两个时间代价！\n一种方法，是先对前k个元素建一个堆，这时候时间代价是O(k)。然后接下来，对于每个新加入元素，判断要将堆里的哪个元素替换掉，判断和处理的时间代价是O(logk)。这样这个堆就一直是保持有k个元素。处理完了n个元素以后，时间代价就是O(n logk)。\n而另一种方法，是先对n个元素建一个堆，这时候时间代价是O(n)，然后接下来，我们在这个堆中，进行k次操作，将堆顶的最大值取出。每次取出堆顶元素，调整的时间代价是O(log n)。因此，处理完k个元素，时间代价就是O(k log n)\n你会看到，我在文章中说的O(n) + O(k log n)的方法，其实就是第二种方法，我其实也简单做了描述。至于它和第一种方法相比，哪个更快，你会发现，当n很大的时候，第二种方法，其实时间代价是接近O(n)的，会比第一种方法O(n log k)快上接近log k倍。\n此外，如果不使用堆的话，我们还可以使用快速选择排序（注意，不是快排，但是使用了快排的partition思想），也是可以在O（n）的时间代价完成top K的检索的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587692734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":284544,"user_name":"青鸟飞鱼","can_delete":false,"product_type":"c1","uid":1807943,"ip_address":"","ucode":"8C64517DA556FE","user_header":"https://static001.geekbang.org/account/avatar/00/1b/96/47/93838ff7.jpg","comment_is_top":false,"comment_ctime":1616328445,"is_pvip":false,"replies":[{"id":103344,"content":"其实这道题的核心有几个个:\n1.分治处理\n2.哈希统计\n3.堆\n4.内存与磁盘利用\n首先，由于内存有限，肯定是要分批处理的。但是分批也有多种分法，比如你说的平均分为100个小文件，然后每个小文件统计其中每个url的个数。(这里其实就有细化问题:小文件怎么统计url出现次数，是做url排序，还是用hash表)。\n但是，我也可以提供其他思路，比如我顺序读取文件，直接往内存的hash表中放，统计相同的url的个数，当hash表满时(到达内存上限)，我再将hash表以key-value形式写入小文件。这样可以生成n个k-v小文件。这是另一种分法。\n然后对多个小文件合并，小文件中的key可能会重复，如果每个小文件的key都是有序的，那么可以用归并排序，快速进行key的合并。可以写成合并成一个有序大文件，文件内容是有序的key-value。\n最后使用堆，读取该文件，保留top 100。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1616509200,"ip_address":"","comment_id":284544,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"请教老师一个面试题相关的top k问题，100 GB 的 URL 文件，进程中使用最多 1 GB 内存，计算出现次数 Top 100 的 URL 和各自的出现次数，性能越快越好。我现在的想法是把大文件依次读取url求哈希，分为100个小文件。小文件多线程统计个数后，构建一个100大小的大顶堆。最后把100个大顶堆，再够成一个100大小的小顶堆。","like_count":10,"discussions":[{"author":{"id":1743279,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/99/af/e4cc7374.jpg","nickname":"啊呜","note":"","ucode":"76E24AB808868D","race_medal":0,"user_type":4,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492920,"discussion_content":"作者回复：并不是typo，你其实提了一个好问题，使用堆来完成Top K检索，时间代价是什么呢？其实有两种不同的方法，对应了两个时间代价！\n一种方法，是先对前k个元素建一个堆，这时候时间代价是O(k)。然后接下来，对于每个新加入元素，判断要将堆里的哪个元素替换掉，判断和处理的时间代价是O(logk)。这样这个堆就一直是保持有k个元素。处理完了n个元素以后，时间代价就是O(n logk)。\n而另一种方法，是先对n个元素建一个堆，这时候时间代价是O(n)，然后接下来，我们在这个堆中，进行k次操作，将堆顶的最大值取出。每次取出堆顶元素，调整的时间代价是O(log n)。因此，处理完k个元素，时间代价就是O(k log n)\n你会看到，我在文章中说的O(n) + O(k log n)的方法，其实就是第二种方法，我其实也简单做了描述。至于它和第一种方法相比，哪个更快，你会发现，当n很大的时候，第二种方法，其实时间代价是接近O(n)的，会比第一种方法O(n log k)快上接近log k倍。\n此外，如果不使用堆的话，我们还可以使用快速选择排序（注意，不是快排，但是使用了快排的partition思想），也是可以在O（n）的时间代价完成top K的检索的。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587694861,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2891795,"avatar":"https://static001.geekbang.org/account/avatar/00/2c/20/13/71179a6c.jpg","nickname":"墨眉无锋","note":"","ucode":"FFB1A26FE49D3E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546919,"discussion_content":"第二种建堆方法岂不是内存申请很大，这又是空间换时间的折中么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642470245,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1194253,"avatar":"https://static001.geekbang.org/account/avatar/00/12/39/0d/80ae66d7.jpg","nickname":"zzz","note":"","ucode":"6A5752C924D917","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":546099,"discussion_content":"topK的完整时间复杂度是O(k*n*log(k）)，老师您的公司不对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642160388,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":245697,"discussion_content":"并不是typo，你其实提了一个好问题，使用堆来完成Top K检索，时间代价是什么呢？其实有两种不同的方法，对应了两个时间代价！\n一种方法，是先对前k个元素建一个堆，这时候时间代价是O(k)。然后接下来，对于每个新加入元素，判断要将堆里的哪个元素替换掉，判断和处理的时间代价是O(logk)。这样这个堆就一直是保持有k个元素。处理完了n个元素以后，时间代价就是O(n logk)。\n而另一种方法，是先对n个元素建一个堆，这时候时间代价是O(n)，然后接下来，我们在这个堆中，进行k次操作，将堆顶的最大值取出。每次取出堆顶元素，调整的时间代价是O(log n)。因此，处理完k个元素，时间代价就是O(k log n)\n你会看到，我在文章中说的O(n) + O(k log n)的方法，其实就是第二种方法，我其实也简单做了描述。至于它和第一种方法相比，哪个更快，你会发现，当n很大的时候，第二种方法，其实时间代价是接近O(n)的，会比第一种方法O(n log k)快上接近log k倍。\n此外，如果不使用堆的话，我们还可以使用快速选择排序（注意，不是快排，但是使用了快排的partition思想），也是可以在O（n）的时间代价完成top K的检索的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587692734,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208331,"user_name":"黄海峰","can_delete":false,"product_type":"c1","uid":1275357,"ip_address":"","ucode":"E9340719BC96B2","user_header":"https://static001.geekbang.org/account/avatar/00/13/75/dd/9ead6e69.jpg","comment_is_top":false,"comment_ctime":1587340667,"is_pvip":false,"replies":[{"id":77795,"content":"我当初在写这一篇的时候也很纠结，不确定是否应该保留还是删除。不过最后出于知识体系的完整性，我还是保留了这一篇，并尽可能用通俗的语言来描述。\n不过不用担心，这是唯一一篇有公式的文章，如果公式不好理解也没关系，你只要知道，打分排序的代价很大就好了。下一篇就会恢复熟悉的味道。\n还有，对于不清楚的地方，其实你可以找些时间，重新多读几次，也许哪天你就能取得意想不到的突破。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587343045,"ip_address":"","comment_id":208331,"utype":1}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"太难了，跟不上了，每次看到一堆公式里面的各种符号都是很排斥然后就放弃了阅读。。。数学果然是分水岭","like_count":9,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492421,"discussion_content":"我当初在写这一篇的时候也很纠结，不确定是否应该保留还是删除。不过最后出于知识体系的完整性，我还是保留了这一篇，并尽可能用通俗的语言来描述。\n不过不用担心，这是唯一一篇有公式的文章，如果公式不好理解也没关系，你只要知道，打分排序的代价很大就好了。下一篇就会恢复熟悉的味道。\n还有，对于不清楚的地方，其实你可以找些时间，重新多读几次，也许哪天你就能取得意想不到的突破。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587343045,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1809802,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","nickname":"一元(eudict)","note":"","ucode":"5E7A33642FC767","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":262439,"discussion_content":"不然怎么说万物基于数学嘛。老师千万别因为个别人而降低了专栏的完整度和精彩程度。看不懂就反复看，有些人宁愿吃身体上的哭也不愿意吃学习的苦，没办法，还是劝退吧","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1589097891,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1743064,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/98/d8/db4cef94.jpg","nickname":"Alan","note":"","ucode":"C656A2B4FF8C50","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":307891,"discussion_content":"我倒是希望可以提升一下完整度，内容有点偏简单了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1600781695,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2438942,"avatar":"https://static001.geekbang.org/account/avatar/00/25/37/1e/9b6554b6.jpg","nickname":"Q","note":"","ucode":"1A20AD48EE2B8C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628466,"discussion_content":"就这公式还觉得复杂，那估计不适合看这种专栏","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1695338550,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"湖南","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208441,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1587359196,"is_pvip":false,"replies":[{"id":77873,"content":"如果是or的话，那么其实也可以加权再相加。你会发现，由人工来制定规则的确会头大，因此不如交给机器学习打分，直接预估每个文档的点击率就好。\n\n至于为什么写这一篇，我也补充几点:\n1.对于一个完整的检索系统，检索结果的排序是无法回避的问题，包括在前面的课程中，留言已经有人提问，说如果检索出来的结果很多该怎么办？因此，这一篇文章能补全这个知识点，让我们知道搜索引擎，广告引擎和推荐系统是如何处理检索结果的。\n2.索引构建和打分机制其实是有关系的。如果你在使用elastic search，并且使用了基于文档的索引拆分，然后又选择了系统自带的tf-idf或者bm25进行相关性打分，那么如果处理不当，相关性计算会变差。(因为idf没有在全局被更新)。\n3.在下一篇讲如何加速检索的方案，你会发现好些内容会和这篇文章有关。敬请期待。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587364067,"ip_address":"","comment_id":208441,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"我怎么感觉是构建训练集标注数据最耗时呢😂😂😂。\n正经点，我觉得打分不应该是在查询的时候实时算，每个词项的分数应该以一定的频率更新，毕竟只要文档基数上来了，加些文档，词项的分数影响不大。还有就是老师这里说的是查询项包含多个词项，然后用多个词项的分数代表文档分数，但这前提条件是词项是and且同比重，如果是or啥的，好了我不想了我头大。感觉老师这块打分排序整体上的逻辑没有和之前的搜索串连上，让我有种脱节的感觉。","like_count":6,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492457,"discussion_content":"如果是or的话，那么其实也可以加权再相加。你会发现，由人工来制定规则的确会头大，因此不如交给机器学习打分，直接预估每个文档的点击率就好。\n\n至于为什么写这一篇，我也补充几点:\n1.对于一个完整的检索系统，检索结果的排序是无法回避的问题，包括在前面的课程中，留言已经有人提问，说如果检索出来的结果很多该怎么办？因此，这一篇文章能补全这个知识点，让我们知道搜索引擎，广告引擎和推荐系统是如何处理检索结果的。\n2.索引构建和打分机制其实是有关系的。如果你在使用elastic search，并且使用了基于文档的索引拆分，然后又选择了系统自带的tf-idf或者bm25进行相关性打分，那么如果处理不当，相关性计算会变差。(因为idf没有在全局被更新)。\n3.在下一篇讲如何加速检索的方案，你会发现好些内容会和这篇文章有关。敬请期待。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587364067,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":294087,"user_name":"托尼斯威特","can_delete":false,"product_type":"c1","uid":1729060,"ip_address":"","ucode":"98A1035527292E","user_header":"https://static001.geekbang.org/account/avatar/00/1a/62/24/07e2507c.jpg","comment_is_top":false,"comment_ctime":1621754490,"is_pvip":false,"replies":[{"id":107522,"content":"好的。这里要注意一点:tf和idf值其实在索引构建的时候就可以统计好的。你可以看一下索引构建那一章，在完整的索引里，doc的信息会包含了tf值。然后idf的信息可以存在token里。因此，完整的流程如下:\n1.在索引构建时，在建立posting list时，在doc信息中存入tf的值，在token存入idf值。\n2.在四个posting list做了归并以后，这时候我们会得到符合检索条件的候选doc列表。在归并的时候，同时累加tf*idf的值。就能得到每个doc的打分结果。\n3.接下来，对于打好分的文档进行排序。就是最终结果。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1622902653,"ip_address":"","comment_id":294087,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师你好, 能否重新阐述一下打分是什么时候起作用的? \n根据前几节的内容, 如果我搜索&quot;极客 时间 检索 技术&quot;, 它会被拆分成4个token, 返回4个 posting list, 然后作多路归并, 得到同时含有4个token 的doc id列表. \n\n4个token在每个doc 里的 TF 都不一样, 每个token的IDF也不一样, 最后怎么影响 搜索结果的排序? ","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":520504,"discussion_content":"好的。这里要注意一点:tf和idf值其实在索引构建的时候就可以统计好的。你可以看一下索引构建那一章，在完整的索引里，doc的信息会包含了tf值。然后idf的信息可以存在token里。因此，完整的流程如下:\n1.在索引构建时，在建立posting list时，在doc信息中存入tf的值，在token存入idf值。\n2.在四个posting list做了归并以后，这时候我们会得到符合检索条件的候选doc列表。在归并的时候，同时累加tf*idf的值。就能得到每个doc的打分结果。\n3.接下来，对于打好分的文档进行排序。就是最终结果。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1622902653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1057843,"avatar":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","nickname":"阿甘","note":"","ucode":"BC93175B70E05D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":551314,"discussion_content":"对于tf/idf的可以这么做，对于机器学习的打分这个又是怎么处理的呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644982159,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":520504,"ip_address":"","group_id":0},"score":551314,"extra":""}]}]},{"had_liked":false,"id":215757,"user_name":"一元(eudict)","can_delete":false,"product_type":"c1","uid":1809802,"ip_address":"","ucode":"5E7A33642FC767","user_header":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","comment_is_top":false,"comment_ctime":1589097217,"is_pvip":false,"replies":[{"id":79847,"content":"你看得很细致也很严谨。的确严格来说，log是要标明底数的。但是你也会发现，许多场合都会直接写log n，因为在时间代价分析中，我们更看重的是量级，而不是常数级的倍数差异。因此以2为底还是以10为底，不会影响结论。另一方面，其实log不写底数时，大家约定俗成有默认底数(10或2都有，看具体上下文场景)。但的确写上会更清晰。谢谢你的细心指出，我看看后续优化一下。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1589111310,"ip_address":"","comment_id":215757,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"老师你好，我有一个问题想请教，log(10)为什么等于1呢？不应该是lg(10)才等于1吗？仅仅log(10)并未标明底数呀？其实在前面的表示时间复杂度上就有此疑问，但后面想想时间复杂度仅仅是表示的变化趋势并不是具体的值也就能说通了。但这儿确实有些不解","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494542,"discussion_content":"你看得很细致也很严谨。的确严格来说，log是要标明底数的。但是你也会发现，许多场合都会直接写log n，因为在时间代价分析中，我们更看重的是量级，而不是常数级的倍数差异。因此以2为底还是以10为底，不会影响结论。另一方面，其实log不写底数时，大家约定俗成有默认底数(10或2都有，看具体上下文场景)。但的确写上会更清晰。谢谢你的细心指出，我看看后续优化一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589111310,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1809802,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","nickname":"一元(eudict)","note":"","ucode":"5E7A33642FC767","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263217,"discussion_content":"好的，感谢老师耐心回答","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1589189357,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1377224,"avatar":"","nickname":"fx","note":"","ucode":"52FA7DCE5C3076","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809802,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","nickname":"一元(eudict)","note":"","ucode":"5E7A33642FC767","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331539,"discussion_content":"最近应该是更新了，我看图片好像都换了不少^_^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606896760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":263217,"ip_address":"","group_id":0},"score":331539,"extra":""}]}]},{"had_liked":false,"id":208438,"user_name":"王坤祥","can_delete":false,"product_type":"c1","uid":1003327,"ip_address":"","ucode":"FB988B9F381A33","user_header":"https://static001.geekbang.org/account/avatar/00/0f/4f/3f/6f62f982.jpg","comment_is_top":false,"comment_ctime":1587358397,"is_pvip":false,"replies":[{"id":77872,"content":"打分的确是非常耗时的过程，因此，我们需要有更多的手段来加速打分过程。GPU的发展，还有矩阵运算等都是可用的手段。\n\n还有，你提到了不少资料都把深度学习称为“用数据编程”。不过，我更愿意称为“用数据推导规则”。实际上，无论是机器学习还是深度学习，目前其实离我们真正期待的智能还有一段距离，从文中的简单例子你可以看到，机器学习更多是基于巨大的算力，用数据去拟合出一个有限的规则表达式，这个学习出来的规则其实还有许多可完善的地方，比如说人脸识别，有人就做过实验，给一个五官完整，但是比例和位置一看就不对的人脸图像给模型识别，结果模型识别为“真人”。因此，在人工智能方向上，我们还需要再往前走。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587363297,"ip_address":"","comment_id":208438,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"使用堆排序替换全排后，打分应该是最耗时的。个人感觉是打分的计算过程复杂，涉及到较多的特殊运算。现代一般的CPU面向的不是纯“算数”了，在当前大数据环境下，大量的纯数据计算需要使用对“算数”优化过的GPU来提高计算速度，还有就是把大量单一的数据运算变为矩阵运算（并行），也会显著提高数据的运算速度。\n\n之前读过一些资料，机器学习不同于以往的编程，它是一种逆向思维--使用数据来编程；看到不少资料都把深度学习称为「用数据编程」，这是一种更高级的编程，也是未来新一代程序员需要掌握的方法。\n\n在深度学习中，理论上我们不需要关注从A-&gt;B的推理逻辑，只要有标签化的数据，通过深度学习就能找到从A-&gt;B的计算模型。也就是说，机器学习适用性会更好，代价就是训练数据需要花费很多的时间。\n\n一点点感悟，不知道自己理解的对不对。\n\n","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494542,"discussion_content":"你看得很细致也很严谨。的确严格来说，log是要标明底数的。但是你也会发现，许多场合都会直接写log n，因为在时间代价分析中，我们更看重的是量级，而不是常数级的倍数差异。因此以2为底还是以10为底，不会影响结论。另一方面，其实log不写底数时，大家约定俗成有默认底数(10或2都有，看具体上下文场景)。但的确写上会更清晰。谢谢你的细心指出，我看看后续优化一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589111310,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1809802,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","nickname":"一元(eudict)","note":"","ucode":"5E7A33642FC767","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":263217,"discussion_content":"好的，感谢老师耐心回答","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1589189357,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1377224,"avatar":"","nickname":"fx","note":"","ucode":"52FA7DCE5C3076","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1809802,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/9d/8a/a2d34896.jpg","nickname":"一元(eudict)","note":"","ucode":"5E7A33642FC767","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":331539,"discussion_content":"最近应该是更新了，我看图片好像都换了不少^_^","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1606896760,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":263217,"ip_address":"","group_id":0},"score":331539,"extra":""}]}]},{"had_liked":false,"id":208432,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1587357136,"is_pvip":false,"replies":[{"id":77868,"content":"你的问题很好！\n1.对于df，的确会随着文档的增加而变化。因此是需要更新的。在倒排索引中，由于idf是和key存在一起的，因此，我们可以在文档变化时，对增量倒排索引的key中的idf值进行更新就可以了。不过要注意:如果使用了基于文档的水平拆分，那么增量索引只会在一个分片中生效。这样如果持续久了，idf值会不准，相关性计算精度会下降。因此，需要周期性地重构全量索引。\n2.机器学习的模型也是需要频繁更新的。一般来说是每天都会更新，还有系统为了更新及时，还会使用在线学习进行更实时的模型更新。不过，我前面说的“机器学习模型更新”，指的是因子和对应权重的更新，并不是你问题中说的“重新计算分数”。因为对一个文档和当前查询词的相关性打分过程，本来就是在查询发生时实时进行的，而不是离线算好的。因此不存在“重复计算分数”这个问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587360678,"ip_address":"","comment_id":208432,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"有两个疑惑的地方，请教下老师。\n1.文档频率df，随着文档数量的增多，df应该会重新计算么？如果是需要重新计算，也需要批量更新所有文档的分数吧？\n2.机器学习计算分数，随着数量量增大，模型会越来越准确。此时是否需要对于之前已经算过分数的文档重新计算分数呢？","like_count":2,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492455,"discussion_content":"打分的确是非常耗时的过程，因此，我们需要有更多的手段来加速打分过程。GPU的发展，还有矩阵运算等都是可用的手段。\n\n还有，你提到了不少资料都把深度学习称为“用数据编程”。不过，我更愿意称为“用数据推导规则”。实际上，无论是机器学习还是深度学习，目前其实离我们真正期待的智能还有一段距离，从文中的简单例子你可以看到，机器学习更多是基于巨大的算力，用数据去拟合出一个有限的规则表达式，这个学习出来的规则其实还有许多可完善的地方，比如说人脸识别，有人就做过实验，给一个五官完整，但是比例和位置一看就不对的人脸图像给模型识别，结果模型识别为“真人”。因此，在人工智能方向上，我们还需要再往前走。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587363297,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":213606,"user_name":"paulhaoyi","can_delete":false,"product_type":"c1","uid":1105619,"ip_address":"","ucode":"C972F4B459E7D6","user_header":"https://static001.geekbang.org/account/avatar/00/10/de/d3/2aa0177f.jpg","comment_is_top":false,"comment_ctime":1588489712,"is_pvip":false,"replies":[{"id":79174,"content":"你说得很对。其实我在文中也提到了，机器学习打分，会考虑更多的维度了，包括“用户的历史点击行为”，这其实就是你说的“考虑用户”了。\n而关于搜索和推荐，它们的许多底层技术是相通的，区别在于两者的约束条件和目标不同。搜索引擎由于有着明确的用户输入和意图，因此对于query相关性的要求很高。而推荐引擎由于没有明确的用户输入信号，因此自由度会更大，可以从更多的维度去考虑检索和排序问题。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588493431,"ip_address":"","comment_id":213606,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"关于问题2，机器学习打分，不只是可以考虑内容和检索词本身吧？还可以考虑用户，甚至环境，上下问，这里，是不是可以近似一个推荐逻辑了？感觉检索和推荐有点像EE问题的两端，一个倾向于精确些，一个倾向于扩展些？","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493825,"discussion_content":"你说得很对。其实我在文中也提到了，机器学习打分，会考虑更多的维度了，包括“用户的历史点击行为”，这其实就是你说的“考虑用户”了。\n而关于搜索和推荐，它们的许多底层技术是相通的，区别在于两者的约束条件和目标不同。搜索引擎由于有着明确的用户输入和意图，因此对于query相关性的要求很高。而推荐引擎由于没有明确的用户输入信号，因此自由度会更大，可以从更多的维度去考虑检索和排序问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588493431,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1105619,"avatar":"https://static001.geekbang.org/account/avatar/00/10/de/d3/2aa0177f.jpg","nickname":"paulhaoyi","note":"","ucode":"C972F4B459E7D6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":256524,"discussion_content":"另外，通过检索词去检索文档的过程，也特别像推荐去查询召回源。只不过也是侧重不同。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588490722,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208328,"user_name":"pedro","can_delete":false,"product_type":"c1","uid":1200704,"ip_address":"","ucode":"F40C839DDFD599","user_header":"https://static001.geekbang.org/account/avatar/00/12/52/40/e57a736e.jpg","comment_is_top":false,"comment_ctime":1587340503,"is_pvip":false,"replies":[{"id":77794,"content":"1.的确，最耗时的步骤其实是打分，因此，我们如果想加快检索效率的话，如何优化打分过程就是一个很重要的方向。\n2.机器学习打分的例子，的确文中没有说得太详细。其实机器学习的关键就是寻找因子(也就是特征)，以及学习权重的过程。对于机器学习和特征工程方向，这其实是另一个领域，也许有机会可以在其他地方再具体描述一下。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587342557,"ip_address":"","comment_id":208328,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"问题1，耗时的是打分，尤其是引入了深度学习的计算量大大超过了排序。\n问题2，个人觉得机器学习打分的一个不可忽视的优点，那就是它会根据用户行为不断进行学习，不断优化自己，从而获得更好的用户体验。暂时没有使用过，老师可以后面举一个机器学习打分的具体算法和例子吗，机器学习毕竟有些笼统。","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492419,"discussion_content":"1.的确，最耗时的步骤其实是打分，因此，我们如果想加快检索效率的话，如何优化打分过程就是一个很重要的方向。\n2.机器学习打分的例子，的确文中没有说得太详细。其实机器学习的关键就是寻找因子(也就是特征)，以及学习权重的过程。对于机器学习和特征工程方向，这其实是另一个领域，也许有机会可以在其他地方再具体描述一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587342557,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1057843,"avatar":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","nickname":"阿甘","note":"","ucode":"BC93175B70E05D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":551315,"discussion_content":"老师机器学习打分的过程实在建索引的时候处理的吗？分值又是存储在哪里呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644982274,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":492419,"ip_address":"","group_id":0},"score":551315,"extra":""}]}]},{"had_liked":false,"id":231071,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1593574337,"is_pvip":false,"replies":[{"id":85469,"content":"其实就是要在业务需求和性能开销之间做权衡。因此我们需要掌握多种方法，以便在合适的场合使用合适的技术。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1593699702,"ip_address":"","comment_id":231071,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"我觉得Top K 检索的过程的打分和排序，需要考虑数据量的大小和打分的规则综合考虑。","like_count":0},{"had_liked":false,"id":208325,"user_name":"黄海峰","can_delete":false,"product_type":"c1","uid":1275357,"ip_address":"","ucode":"E9340719BC96B2","user_header":"https://static001.geekbang.org/account/avatar/00/13/75/dd/9ead6e69.jpg","comment_is_top":false,"comment_ctime":1587340335,"is_pvip":false,"replies":[{"id":77791,"content":"这一部分我写得有些简略。文中有一句:“我们在使用TF和IDF时，都会使用对数函数进行平滑处理”，在示意图中有具体的处理方法。因此，“极客”的出现次数是10，但是TF值是1+log(10) =2。\n同理，“时间”的出现次数是10，故TF值也是2。\n故文档1的分数是2*10+2*1=22","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587341288,"ip_address":"","comment_id":208325,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"第一个tfidf怎么算的？文档一不是10*10+10*1=110，文档二不是1*10+100*1=110吗？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492418,"discussion_content":"这一部分我写得有些简略。文中有一句:“我们在使用TF和IDF时，都会使用对数函数进行平滑处理”，在示意图中有具体的处理方法。因此，“极客”的出现次数是10，但是TF值是1+log(10) =2。\n同理，“时间”的出现次数是10，故TF值也是2。\n故文档1的分数是2*10+2*1=22","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587341288,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208315,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1587339672,"is_pvip":false,"replies":[{"id":77792,"content":"1.你说到了很有意思的一点，tf-idf和bm25现在可以用来做召回。下一讲我就会和大家介绍。\n2.的确是，机器学习的用法会很灵活，因子可以有很多，tfidf和bm25都可以当做排序因子来使用。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587341420,"ip_address":"","comment_id":208315,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"1.tf-idf和BM25现在可以用来做召回\n2.在利用机器学习排序的时代，tdidf和BM25可以作为一个排序因子","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492418,"discussion_content":"这一部分我写得有些简略。文中有一句:“我们在使用TF和IDF时，都会使用对数函数进行平滑处理”，在示意图中有具体的处理方法。因此，“极客”的出现次数是10，但是TF值是1+log(10) =2。\n同理，“时间”的出现次数是10，故TF值也是2。\n故文档1的分数是2*10+2*1=22","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587341288,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":388283,"user_name":"代码诗人","can_delete":false,"product_type":"c1","uid":3839309,"ip_address":"北京","ucode":"6AC4ABA00BCC88","user_header":"https://static001.geekbang.org/account/avatar/00/3a/95/4d/9ad560a8.jpg","comment_is_top":false,"comment_ctime":1709796874,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"“在给所有的文档打完分以后，接下来，我们就要完成排序的工作了”\n排序之前需要把一条倒排链上的所有doc都进行打分么？如果是热词，这个链可能会非常非常长，这里有什么优化方法么？预排序加截断么，可能会丢失结果","like_count":0},{"had_liked":false,"id":372552,"user_name":"ifelse","can_delete":false,"product_type":"c1","uid":2550743,"ip_address":"浙江","ucode":"D0565908C99695","user_header":"https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg","comment_is_top":false,"comment_ctime":1681275823,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":3,"product_id":100048401,"comment_content":"学习打卡","like_count":0},{"had_liked":false,"id":349965,"user_name":"阿甘","can_delete":false,"product_type":"c1","uid":1057843,"ip_address":"","ucode":"BC93175B70E05D","user_header":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","comment_is_top":false,"comment_ctime":1656473735,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师请教一下，像广告推荐很多时候打分排序是在检索的时候实时计算的，也是耗时最大的操作。对于搜索引擎，会不会也有这种情况？还是都是在构建索引的时候就打好分了？","like_count":0},{"had_liked":false,"id":323330,"user_name":"书豪","can_delete":false,"product_type":"c1","uid":1068091,"ip_address":"","ucode":"212FBDE59E8457","user_header":"https://static001.geekbang.org/account/avatar/00/10/4c/3b/47d832f4.jpg","comment_is_top":false,"comment_ctime":1637838602,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"bm25算法 我要设置一个阈值怎么设置 比如我要把70%相似度以上的找出来","like_count":0},{"had_liked":false,"id":323329,"user_name":"书豪","can_delete":false,"product_type":"c1","uid":1068091,"ip_address":"","ucode":"212FBDE59E8457","user_header":"https://static001.geekbang.org/account/avatar/00/10/4c/3b/47d832f4.jpg","comment_is_top":false,"comment_ctime":1637838390,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师，bm25算法是否可以老师，如果采用bm25算法计算相关性得分，那么能否根据最后计算的相关性数值，设定阈值来筛选相关文章呢？比如我用TFIDF向量计算文章余弦相似度，阈值大于0.7认为两篇文章相似是没问题的。但是bm25算法好像不能很好地设定一个阈值，来判定是否相似。有没有比较好的方式呢？","like_count":0},{"had_liked":false,"id":231071,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1593574337,"is_pvip":false,"replies":[{"id":85469,"content":"其实就是要在业务需求和性能开销之间做权衡。因此我们需要掌握多种方法，以便在合适的场合使用合适的技术。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1593699702,"ip_address":"","comment_id":231071,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"我觉得Top K 检索的过程的打分和排序，需要考虑数据量的大小和打分的规则综合考虑。","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500143,"discussion_content":"其实就是要在业务需求和性能开销之间做权衡。因此我们需要掌握多种方法，以便在合适的场合使用合适的技术。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593699702,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208325,"user_name":"黄海峰","can_delete":false,"product_type":"c1","uid":1275357,"ip_address":"","ucode":"E9340719BC96B2","user_header":"https://static001.geekbang.org/account/avatar/00/13/75/dd/9ead6e69.jpg","comment_is_top":false,"comment_ctime":1587340335,"is_pvip":false,"replies":[{"id":77791,"content":"这一部分我写得有些简略。文中有一句:“我们在使用TF和IDF时，都会使用对数函数进行平滑处理”，在示意图中有具体的处理方法。因此，“极客”的出现次数是10，但是TF值是1+log(10) =2。\n同理，“时间”的出现次数是10，故TF值也是2。\n故文档1的分数是2*10+2*1=22","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587341288,"ip_address":"","comment_id":208325,"utype":1}],"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"第一个tfidf怎么算的？文档一不是10*10+10*1=110，文档二不是1*10+100*1=110吗？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500143,"discussion_content":"其实就是要在业务需求和性能开销之间做权衡。因此我们需要掌握多种方法，以便在合适的场合使用合适的技术。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593699702,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":208315,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1587339672,"is_pvip":false,"replies":[{"id":77792,"content":"1.你说到了很有意思的一点，tf-idf和bm25现在可以用来做召回。下一讲我就会和大家介绍。\n2.的确是，机器学习的用法会很灵活，因子可以有很多，tfidf和bm25都可以当做排序因子来使用。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587341420,"ip_address":"","comment_id":208315,"utype":1}],"discussion_count":2,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"1.tf-idf和BM25现在可以用来做召回\n2.在利用机器学习排序的时代，tdidf和BM25可以作为一个排序因子","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492416,"discussion_content":"1.你说到了很有意思的一点，tf-idf和bm25现在可以用来做召回。下一讲我就会和大家介绍。\n2.的确是，机器学习的用法会很灵活，因子可以有很多，tfidf和bm25都可以当做排序因子来使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587341420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1057843,"avatar":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","nickname":"阿甘","note":"","ucode":"BC93175B70E05D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":551316,"discussion_content":"我理解索引就是召回（候选集，一般比较大），然后再根据模型进行打分排序截断（topK）？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644982395,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":492416,"ip_address":"","group_id":0},"score":551316,"extra":""}]}]},{"had_liked":false,"id":388283,"user_name":"代码诗人","can_delete":false,"product_type":"c1","uid":3839309,"ip_address":"北京","ucode":"6AC4ABA00BCC88","user_header":"https://static001.geekbang.org/account/avatar/00/3a/95/4d/9ad560a8.jpg","comment_is_top":false,"comment_ctime":1709796874,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"“在给所有的文档打完分以后，接下来，我们就要完成排序的工作了”\n排序之前需要把一条倒排链上的所有doc都进行打分么？如果是热词，这个链可能会非常非常长，这里有什么优化方法么？预排序加截断么，可能会丢失结果","like_count":0},{"had_liked":false,"id":372552,"user_name":"ifelse","can_delete":false,"product_type":"c1","uid":2550743,"ip_address":"浙江","ucode":"D0565908C99695","user_header":"https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg","comment_is_top":false,"comment_ctime":1681275823,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":3,"product_id":100048401,"comment_content":"学习打卡","like_count":0},{"had_liked":false,"id":349965,"user_name":"阿甘","can_delete":false,"product_type":"c1","uid":1057843,"ip_address":"","ucode":"BC93175B70E05D","user_header":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","comment_is_top":false,"comment_ctime":1656473735,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师请教一下，像广告推荐很多时候打分排序是在检索的时候实时计算的，也是耗时最大的操作。对于搜索引擎，会不会也有这种情况？还是都是在构建索引的时候就打好分了？","like_count":0},{"had_liked":false,"id":323330,"user_name":"书豪","can_delete":false,"product_type":"c1","uid":1068091,"ip_address":"","ucode":"212FBDE59E8457","user_header":"https://static001.geekbang.org/account/avatar/00/10/4c/3b/47d832f4.jpg","comment_is_top":false,"comment_ctime":1637838602,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"bm25算法 我要设置一个阈值怎么设置 比如我要把70%相似度以上的找出来","like_count":0},{"had_liked":false,"id":323329,"user_name":"书豪","can_delete":false,"product_type":"c1","uid":1068091,"ip_address":"","ucode":"212FBDE59E8457","user_header":"https://static001.geekbang.org/account/avatar/00/10/4c/3b/47d832f4.jpg","comment_is_top":false,"comment_ctime":1637838390,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师，bm25算法是否可以老师，如果采用bm25算法计算相关性得分，那么能否根据最后计算的相关性数值，设定阈值来筛选相关文章呢？比如我用TFIDF向量计算文章余弦相似度，阈值大于0.7认为两篇文章相似是没问题的。但是bm25算法好像不能很好地设定一个阈值，来判定是否相似。有没有比较好的方式呢？","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":492416,"discussion_content":"1.你说到了很有意思的一点，tf-idf和bm25现在可以用来做召回。下一讲我就会和大家介绍。\n2.的确是，机器学习的用法会很灵活，因子可以有很多，tfidf和bm25都可以当做排序因子来使用。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587341420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1057843,"avatar":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","nickname":"阿甘","note":"","ucode":"BC93175B70E05D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":551316,"discussion_content":"我理解索引就是召回（候选集，一般比较大），然后再根据模型进行打分排序截断（topK）？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1644982395,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":492416,"ip_address":"","group_id":0},"score":551316,"extra":""}]}]}]}