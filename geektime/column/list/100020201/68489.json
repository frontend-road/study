{"id":68489,"title":"08 | MapReduce如何让数据完成一次旅行？","content":"<p>上一期我们聊到MapReduce编程模型将大数据计算过程切分为Map和Reduce两个阶段，先复习一下，在Map阶段为每个数据块分配一个Map计算任务，然后将所有map输出的Key进行合并，相同的Key及其对应的Value发送给同一个Reduce任务去处理。通过这两个阶段，工程师只需要遵循MapReduce编程模型就可以开发出复杂的大数据计算程序。</p><p>那么这个程序是如何在分布式集群中运行起来的呢？MapReduce程序又是如何找到相应的数据并进行计算的呢？答案就是需要MapReduce计算框架来完成。上一期我讲了MapReduce既是编程模型又是计算框架，我们聊完编程模型，<span class=\"orange\">今天就来讨论MapReduce如何让数据完成一次旅行，也就是MapReduce计算框架是如何运作的。</span></p><p>首先我想告诉你，在实践中，这个过程有两个关键问题需要处理。</p><ul>\n<li>\n<p>如何为每个数据块分配一个Map计算任务，也就是代码是如何发送到数据块所在服务器的，发送后是如何启动的，启动以后如何知道自己需要计算的数据在文件什么位置（BlockID是什么）。</p>\n</li>\n<li>\n<p>处于不同服务器的map输出的&lt;Key, Value&gt; ，如何把相同的Key聚合在一起发送给Reduce任务进行处理。</p>\n</li>\n</ul><!-- [[[read_end]]] --><p>那么这两个关键问题对应在MapReduce计算过程的哪些步骤呢？根据我上一期所讲的，我把MapReduce计算过程的图又找出来，你可以看到图中标红的两处，这两个关键问题对应的就是图中的两处“MapReduce框架处理”，具体来说，它们分别是MapReduce作业启动和运行，以及MapReduce数据合并与连接。</p><p><img src=\"https://static001.geekbang.org/resource/image/f3/9c/f3a2faf9327fe3f086ec2c7eb4cd229c.png?wh=1434*300\" alt=\"\"></p><h2>MapReduce作业启动和运行机制</h2><p>我们以Hadoop  1为例，MapReduce运行过程涉及三类关键进程。</p><p>1.大数据应用进程。这类进程是启动MapReduce程序的主入口，主要是指定Map和Reduce类、输入输出文件路径等，并提交作业给Hadoop集群，也就是下面提到的JobTracker进程。这是由用户启动的MapReduce程序进程，比如我们上期提到的WordCount程序。</p><p>2.JobTracker进程。这类进程根据要处理的输入数据量，命令下面提到的TaskTracker进程启动相应数量的Map和Reduce进程任务，并管理整个作业生命周期的任务调度和监控。这是Hadoop集群的常驻进程，需要注意的是，JobTracker进程在整个Hadoop集群全局唯一。</p><p>3.TaskTracker进程。这个进程负责启动和管理Map进程以及Reduce进程。因为需要每个数据块都有对应的map函数，TaskTracker进程通常和HDFS的DataNode进程启动在同一个服务器。也就是说，Hadoop集群中绝大多数服务器同时运行DataNode进程和TaskTracker进程。</p><p>JobTracker进程和TaskTracker进程是主从关系，主服务器通常只有一台（或者另有一台备机提供高可用服务，但运行时只有一台服务器对外提供服务，真正起作用的只有一台），从服务器可能有几百上千台，所有的从服务器听从主服务器的控制和调度安排。主服务器负责为应用程序分配服务器资源以及作业执行的调度，而具体的计算操作则在从服务器上完成。</p><p>具体来看，MapReduce的主服务器就是JobTracker，从服务器就是TaskTracker。还记得我们讲HDFS也是主从架构吗，HDFS的主服务器是NameNode，从服务器是DataNode。后面会讲到的Yarn、Spark等也都是这样的架构，这种一主多从的服务器架构也是绝大多数大数据系统的架构方案。</p><p>可重复使用的架构方案叫作架构模式，一主多从可谓是大数据领域的最主要的架构模式。主服务器只有一台，掌控全局；从服务器有很多台，负责具体的事情。这样很多台服务器可以有效组织起来，对外表现出一个统一又强大的计算能力。</p><p>讲到这里，我们对MapReduce的启动和运行机制有了一个直观的了解。那具体的作业启动和计算过程到底是怎样的呢？我根据上面所讲的绘制成一张图，你可以从图中一步一步来看，感受一下整个流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/2d/27/2df4e1976fd8a6ac4a46047d85261027.png?wh=1368*802\" alt=\"\"></p><p>如果我们把这个计算过程看作一次小小的旅行，这个旅程可以概括如下：</p><p>1.应用进程JobClient将用户作业JAR包存储在HDFS中，将来这些JAR包会分发给Hadoop集群中的服务器执行MapReduce计算。</p><p>2.应用程序提交job作业给JobTracker。</p><p>3.JobTracker根据作业调度策略创建JobInProcess树，每个作业都会有一个自己的JobInProcess树。</p><p>4.JobInProcess根据输入数据分片数目（通常情况就是数据块的数目）和设置的Reduce数目创建相应数量的TaskInProcess。</p><p>5.TaskTracker进程和JobTracker进程进行定时通信。</p><p>6.如果TaskTracker有空闲的计算资源（有空闲CPU核心），JobTracker就会给它分配任务。分配任务的时候会根据TaskTracker的服务器名字匹配在同一台机器上的数据块计算任务给它，使启动的计算任务正好处理本机上的数据，以实现我们一开始就提到的“移动计算比移动数据更划算”。</p><p>7.TaskTracker收到任务后根据任务类型（是Map还是Reduce）和任务参数（作业JAR包路径、输入数据文件路径、要处理的数据在文件中的起始位置和偏移量、数据块多个备份的DataNode主机名等），启动相应的Map或者Reduce进程。</p><p>8.Map或者Reduce进程启动后，检查本地是否有要执行任务的JAR包文件，如果没有，就去HDFS上下载，然后加载Map或者Reduce代码开始执行。</p><p>9.如果是Map进程，从HDFS读取数据（通常要读取的数据块正好存储在本机）；如果是Reduce进程，将结果数据写出到HDFS。</p><p>通过这样一个计算旅程，MapReduce可以将大数据作业计算任务分布在整个Hadoop集群中运行，每个Map计算任务要处理的数据通常都能从本地磁盘上读取到。现在你对这个过程的理解是不是更清楚了呢？你也许会觉得，这个过程好像也不算太简单啊！</p><p>其实，你要做的仅仅是编写一个map函数和一个reduce函数就可以了，根本不用关心这两个函数是如何被分布启动到集群上的，也不用关心数据块又是如何分配给计算任务的。<strong>这一切都由MapReduce计算框架完成</strong>！是不是很激动，这也是我们反复讲到的MapReduce的强大之处。</p><h2>MapReduce数据合并与连接机制</h2><p><strong>MapReduce计算真正产生奇迹的地方是数据的合并与连接</strong>。</p><p>让我先回到上一期MapReduce编程模型的WordCount例子中，我们想要统计相同单词在所有输入数据中出现的次数，而一个Map只能处理一部分数据，一个热门单词几乎会出现在所有的Map中，这意味着同一个单词必须要合并到一起进行统计才能得到正确的结果。</p><p>事实上，几乎所有的大数据计算场景都需要处理数据关联的问题，像WordCount这种比较简单的只要对Key进行合并就可以了，对于像数据库的join操作这种比较复杂的，需要对两种类型（或者更多类型）的数据根据Key进行连接。</p><p>在map输出与reduce输入之间，MapReduce计算框架处理数据合并与连接操作，这个操作有个专门的词汇叫<strong>shuffle</strong>。那到底什么是shuffle？shuffle的具体过程又是怎样的呢？请看下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/d6/c7/d64daa9a621c1d423d4a1c13054396c7.png?wh=1316*772\" alt=\"\"></p><p>每个Map任务的计算结果都会写入到本地文件系统，等Map任务快要计算完成的时候，MapReduce计算框架会启动shuffle过程，在Map任务进程调用一个Partitioner接口，对Map产生的每个&lt;Key, Value&gt;进行Reduce分区选择，然后通过HTTP通信发送给对应的Reduce进程。这样不管Map位于哪个服务器节点，相同的Key一定会被发送给相同的Reduce进程。Reduce任务进程对收到的&lt;Key, Value&gt;进行排序和合并，相同的Key放在一起，组成一个&lt;Key, Value集合&gt;传递给Reduce执行。</p><p>map输出的&lt;Key, Value&gt;shuffle到哪个Reduce进程是这里的关键，它是由Partitioner来实现，MapReduce框架默认的Partitioner用Key的哈希值对Reduce任务数量取模，相同的Key一定会落在相同的Reduce任务ID上。从实现上来看的话，这样的Partitioner代码只需要一行。</p><pre><code> /** Use {@link Object#hashCode()} to partition. */ \npublic int getPartition(K2 key, V2 value, int numReduceTasks) { \n    return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; \n }\n</code></pre><p>讲了这么多，对shuffle的理解，你只需要记住这一点：<strong>分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是shuffle</strong>。</p><p>shuffle是大数据计算过程中最神奇的地方，不管是MapReduce还是Spark，只要是大数据批处理计算，一定都会有shuffle过程，只有<strong>让数据关联起来</strong>，数据的内在关系和价值才会呈现出来。如果你不理解shuffle，肯定会在map和reduce编程中产生困惑，不知道该如何正确设计map的输出和reduce的输入。shuffle也是整个MapReduce过程中最难、最消耗性能的地方，在MapReduce早期代码中，一半代码都是关于shuffle处理的。</p><h2>小结</h2><p>MapReduce编程相对说来是简单的，但是MapReduce框架要将一个相对简单的程序，在分布式的大规模服务器集群上并行执行起来却并不简单。理解MapReduce作业的启动和运行机制，理解shuffle过程的作用和实现原理，对你理解大数据的核心原理，做到真正意义上把握大数据、用好大数据作用巨大。</p><h2>思考题</h2><p>互联网应用中，用户从手机或者PC上发起一个请求，请问这个请求数据经历了怎样的旅程？完成了哪些计算处理后响应给用户？</p><p>欢迎你写下自己的思考或疑问，与我和其他同学一起讨论。</p>","comments":[{"had_liked":false,"id":39358,"user_name":"张贝贝","can_delete":false,"product_type":"c1","uid":1066004,"ip_address":"","ucode":"7CDBB643AD1492","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/icicSvapqLfCWmIofXILE3b20RVDicQvooGnbksVNgz7wSzEfCKtibhIVMwibf778E39fF9hAa1EFMCFyhgljkwicicXg/132","comment_is_top":false,"comment_ctime":1542253446,"is_pvip":false,"replies":[{"id":"14143","content":"是的，主要为了可靠性，spark就不写硬盘，所以快。","user_name":"作者回复","comment_id":39358,"uid":"1007349","ip_address":"","utype":1,"ctime":1542294211,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"456808786822","product_id":100020201,"comment_content":"有个问题，为什么mapper计算完的结果要放到硬盘呢？那再发送到reducer不是还有个读取再发送的过程吗？这中间不就有一个重复的写和读的过程吗？","like_count":105,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429027,"discussion_content":"是的，主要为了可靠性，spark就不写硬盘，所以快。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294211,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1131375,"avatar":"https://static001.geekbang.org/account/avatar/00/11/43/6f/610d21e3.jpg","nickname":"老周","note":"","ucode":"0E88C405072A56","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":589660,"discussion_content":"这也是一个面试经常问到的点，MR就是这点被诟病，spark某程度上也是为了解决这个问而诞生","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1665222967,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39915,"user_name":"冬冬","can_delete":false,"product_type":"c1","uid":1282677,"ip_address":"","ucode":"68B0ADBF48D92F","user_header":"https://static001.geekbang.org/account/avatar/00/13/92/75/d19e479a.jpg","comment_is_top":false,"comment_ctime":1542382620,"is_pvip":false,"replies":[{"id":"14396","content":"会的，数据倾斜，会导致任务失败。严重的数据倾斜可能是数据本身的问题，需要做好预处理","user_name":"作者回复","comment_id":39915,"uid":"1007349","ip_address":"","utype":1,"ctime":1542459867,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"405269308444","product_id":100020201,"comment_content":"老师您好，有个问题，当某个key聚集了大量数据，shuffle到同一个reduce来汇总，考虑数据量很大的情况，这个会不会把reduce所在机器节点撑爆？这样任务是不是就失败了？","like_count":95,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429289,"discussion_content":"会的，数据倾斜，会导致任务失败。严重的数据倾斜可能是数据本身的问题，需要做好预处理","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542459867,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1367048,"avatar":"https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg","nickname":"陈斌","note":"","ucode":"B639AB5F6AA03D","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":313257,"discussion_content":"你好，数据倾斜的问题你能举个实际的例子说明下吗？例如计算词频问题或者其他问题的数据倾斜极端场景。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603013542,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39315,"user_name":"格非","can_delete":false,"product_type":"c1","uid":1004569,"ip_address":"","ucode":"89FABFFC377131","user_header":"https://static001.geekbang.org/account/avatar/00/0f/54/19/95ff4cbd.jpg","comment_is_top":false,"comment_ctime":1542246517,"is_pvip":false,"replies":[{"id":"14145","content":"是的，比如MapReduce没法计算斐波那契数列，因为不能分片计算。<br>但是大数据场景几乎都是可以分片计算的。","user_name":"作者回复","comment_id":39315,"uid":"1007349","ip_address":"","utype":1,"ctime":1542294455,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"276420153461","product_id":100020201,"comment_content":"MapReduce的思想有点类似分而治之，将一个大任务分割成小任务，分发给服务器去处理，然后汇总结果，这是MapReduce的优势，但是MapReduce也就限制在了只能处理这种可以分割的任务上，比如，统计文本中的不同单词的个数，不知道我这种想法是否正确，还想请老师指教，另外，能否分享一下MapReduce这种技术的局限性呢？","like_count":65,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429009,"discussion_content":"是的，比如MapReduce没法计算斐波那契数列，因为不能分片计算。\n但是大数据场景几乎都是可以分片计算的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294455,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2236874,"avatar":"","nickname":"徐戈","note":"","ucode":"91CF1B7C2824C0","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":333413,"discussion_content":"group的结果计数是不是就是不同的单词个数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1607515018,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39373,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1542258416,"is_pvip":false,"replies":[{"id":"14140","content":"如果是SQL操作，就用hive，不用自己编程MapReduce。<br><br>如果机器故障导致某个任务很慢，MapReduce框架会启动多个任务进程在多个服务器同时计算同一个数据块，那个算完输出那个，不会一直等。<br>需要一直等的是数据偏移，某个key聚集了太多数据，大量数据shuffle到一个reduce计算，job一直等这个任务。","user_name":"作者回复","comment_id":39373,"uid":"1007349","ip_address":"","utype":1,"ctime":1542294033,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"151866113776","product_id":100020201,"comment_content":"实际操作中是不是通过hive去完成MapReduce 的？<br>如果有一台机子一直卡在那里，整个job就差它一个返回数据，是不是整个job在等待状态？这种怎么处理？","like_count":36,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429037,"discussion_content":"如果是SQL操作，就用hive，不用自己编程MapReduce。\n\n如果机器故障导致某个任务很慢，MapReduce框架会启动多个任务进程在多个服务器同时计算同一个数据块，那个算完输出那个，不会一直等。\n需要一直等的是数据偏移，某个key聚集了太多数据，大量数据shuffle到一个reduce计算，job一直等这个任务。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294033,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1019332,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","nickname":"长期规划","note":"","ucode":"5EF65E9115834B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":557169,"discussion_content":"老师，你提到在多个服务器计算同一数据块。这里的多个服务器是该数据块的多个副本所在的服务器，对吧？不然，就要跨服务器读取数据了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647681085,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39399,"user_name":"still0007","can_delete":false,"product_type":"c1","uid":1276815,"ip_address":"","ucode":"F4E0F44CA7C733","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM4op5UXa2fBNlEe9mFiaagibNUTKuYR6X8TEg11EIksR78b413jWFv8lAjCDibQ1GcJOsyoOPVS9W9JA/132","comment_is_top":false,"comment_ctime":1542265093,"is_pvip":false,"replies":[{"id":"14137","content":"移动计算主要是map阶段，reduce阶段数据还是要移动数据合并关联，不然很多计算无法完成","user_name":"作者回复","comment_id":39399,"uid":"1007349","ip_address":"","utype":1,"ctime":1542293363,"user_name_real":"李智慧"}],"discussion_count":5,"race_medal":0,"score":"147571153157","product_id":100020201,"comment_content":"有一个疑问，之前讲到“移动计算而不是移动数据”，但是在shuffle的过程中，涉及到大量的移动数据，这又是为什么呢？","like_count":35,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429048,"discussion_content":"移动计算主要是map阶段，reduce阶段数据还是要移动数据合并关联，不然很多计算无法完成","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542293363,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2095559,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/zGH2OYewI38ic6jB1jCBQG6eXY0hoFOIVm2yW7CyjZNSUV2u6fRYPkUVYjmACLfFL9Ow7fJUQZ1ZFkRtr8F4fMA/132","nickname":"海天","note":"","ucode":"45D085D542C09F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":370073,"discussion_content":"定位\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619273414,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1050004,"avatar":"https://static001.geekbang.org/account/avatar/00/10/05/94/9291c4be.jpg","nickname":"wptercel","note":"","ucode":"877F7E5444CF95","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":344666,"discussion_content":"没太明白getPartitioner中key.hashCode() &amp; Integer.MAX_VALUE的作用是什么","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1611552246,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1656804,"avatar":"https://static001.geekbang.org/account/avatar/00/19/47/e4/17cb3df1.jpg","nickname":"BBQ","note":"","ucode":"683BBF7F7AE370","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1050004,"avatar":"https://static001.geekbang.org/account/avatar/00/10/05/94/9291c4be.jpg","nickname":"wptercel","note":"","ucode":"877F7E5444CF95","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":370522,"discussion_content":"取模","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619441093,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":344666,"ip_address":""},"score":370522,"extra":""},{"author":{"id":2028948,"avatar":"","nickname":"Geek2808","note":"","ucode":"835C9691723163","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1050004,"avatar":"https://static001.geekbang.org/account/avatar/00/10/05/94/9291c4be.jpg","nickname":"wptercel","note":"","ucode":"877F7E5444CF95","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":387747,"discussion_content":"实现取绝对值","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1628397338,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":344666,"ip_address":""},"score":387747,"extra":""}]}]},{"had_liked":false,"id":138458,"user_name":"星一","can_delete":false,"product_type":"c1","uid":1103829,"ip_address":"","ucode":"EF550131FAE1BF","user_header":"https://static001.geekbang.org/account/avatar/00/10/d7/d5/6fbf1070.jpg","comment_is_top":false,"comment_ctime":1570258525,"is_pvip":false,"replies":[{"id":"53687","content":"绝对先后关系，一个reduce必须要他前置的所有map执行完才能执行。<br><br>MapReduce框架会在85%的map程序执行完成时，开始启动reduce程序，reduce程序一边shuffle已完成的map数据，一边等待未完成的map继续执行，直到全部map完成，reduce才开始执行计算。","user_name":"作者回复","comment_id":138458,"uid":"1007349","ip_address":"","utype":1,"ctime":1570603094,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"134714244701","product_id":100020201,"comment_content":"请问一下，map和reduce有绝对的先后关系吗，还是说可以一边map一边reduce","like_count":32,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":469526,"discussion_content":"绝对先后关系，一个reduce必须要他前置的所有map执行完才能执行。\n\nMapReduce框架会在85%的map程序执行完成时，开始启动reduce程序，reduce程序一边shuffle已完成的map数据，一边等待未完成的map继续执行，直到全部map完成，reduce才开始执行计算。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1570603094,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1019332,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/8d/c4/6f97daea.jpg","nickname":"长期规划","note":"","ucode":"5EF65E9115834B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":557172,"discussion_content":"Get，也就是串行执行","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647681225,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":469526,"ip_address":""},"score":557172,"extra":""}]}]},{"had_liked":false,"id":41729,"user_name":"清清","can_delete":false,"product_type":"c1","uid":1277095,"ip_address":"","ucode":"9FF193EEEB34BC","user_header":"https://static001.geekbang.org/account/avatar/00/13/7c/a7/27cbd5e2.jpg","comment_is_top":false,"comment_ctime":1542847198,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"134686833374","product_id":100020201,"comment_content":"老师讲得很好，同学问的问题也很好，有些疑问翻评论区就解决了","like_count":32},{"had_liked":false,"id":67703,"user_name":"细小软也会有梦想","can_delete":false,"product_type":"c1","uid":1401292,"ip_address":"","ucode":"DAA235248904D9","user_header":"https://static001.geekbang.org/account/avatar/00/15/61/cc/82758c26.jpg","comment_is_top":false,"comment_ctime":1550225758,"is_pvip":false,"discussion_count":2,"race_medal":0,"score":"100334473566","product_id":100020201,"comment_content":"shuffle过程中的两次排序。这个很重要吧，需要提一下啊，我就被网易的面试官问过。","like_count":24,"discussions":[{"author":{"id":1182914,"avatar":"https://static001.geekbang.org/account/avatar/00/12/0c/c2/bad34a50.jpg","nickname":"张洋","note":"","ucode":"549BE5DEEF8417","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":369209,"discussion_content":"排序 和 combine吗\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1618974623,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1125722,"avatar":"https://static001.geekbang.org/account/avatar/00/11/2d/5a/d965c147.jpg","nickname":"wangbo","note":"","ucode":"3F0C9C320F0A1D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":321307,"discussion_content":"快速排序和归并排序吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1604561772,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":40045,"user_name":"多襄丸","can_delete":false,"product_type":"c1","uid":1074310,"ip_address":"","ucode":"1AA1497C5A293C","user_header":"https://static001.geekbang.org/account/avatar/00/10/64/86/f5a9403a.jpg","comment_is_top":false,"comment_ctime":1542453224,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"87441799144","product_id":100020201,"comment_content":"1.数据从PC&#47;Mobile端发动给服务器端<br>2.服务器端收到数据后在分布式集群下会进入到某个Server端，数据经过一系列的业务操作后可能会被记录下来<br>3.这些记录下来的数据会以文件形式存放于某个固定位置<br>4.数据推送工具可将这些固定位置的文件推送到大数据平台<br>5.大数据平台的Map Reduce框架会根据程序应用主动读取数据作为Map&#47;Reduce的数据输入<br>6.大数据平台清晰完数据后以文件形式输出<br>7.服务器端去大数据平台存放文件的位置获取文件，并进行解析入库。<br>8.最终，数据以图形形式展示在报告上。","like_count":20,"discussions":[{"author":{"id":1725375,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLft6MREe5ggpYtsK2hHUFB7WSujNE7SERRV8YwlVqgs6Y24ia0ricJLjlibiaBa3Lt4PdsXO1Fvd4a7w/132","nickname":"云淡风轻","note":"","ucode":"CA4C85875DD847","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":113264,"discussion_content":"第4步应该不需要，记录可以直接到大数据平台，或者是移动计算而不是移动数据。7步也不需要，数据平台计算结果可直接展示报告。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1577892083,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":181950,"user_name":" 臣馟飞扬","can_delete":false,"product_type":"c1","uid":1116188,"ip_address":"","ucode":"F2F882B7678055","user_header":"https://static001.geekbang.org/account/avatar/00/11/08/1c/ef15e661.jpg","comment_is_top":false,"comment_ctime":1582684237,"is_pvip":false,"replies":[{"id":"70472","content":"对于单个map任务，必须要map结束才能开始shuffle，因为map计算之后，还有个combine，数据还没完全准备好，不能shuffle。<br>对于整个作业，可以在大部分map任务完成后，也就是80%的map任务完成后启动shuffle，但是能进行shuffle的必须是已经完成的map任务的输出数据，没完成的map任务不能shuffle。","user_name":"作者回复","comment_id":181950,"uid":"1007349","ip_address":"","utype":1,"ctime":1582696101,"user_name_real":"李智慧"}],"discussion_count":4,"race_medal":0,"score":"83187062861","product_id":100020201,"comment_content":"看其他资料上介绍，shuffle过程从map的输入就已经开始了，与老师介绍的好像不太一致哦，这个过程应该是什么样？","like_count":20,"discussions":[{"author":{"id":1066752,"avatar":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","nickname":"piboye","note":"","ucode":"7CFD8712857A85","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545377,"discussion_content":"是因为还没有启动reduce？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641922599,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":485175,"discussion_content":"对于单个map任务，必须要map结束才能开始shuffle，因为map计算之后，还有个combine，数据还没完全准备好，不能shuffle。\n对于整个作业，可以在大部分map任务完成后，也就是80%的map任务完成后启动shuffle，但是能进行shuffle的必须是已经完成的map任务的输出数据，没完成的map任务不能shuffle。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1582696101,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":2,"child_discussions":[{"author":{"id":1066752,"avatar":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","nickname":"piboye","note":"","ucode":"7CFD8712857A85","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":545376,"discussion_content":"为什么不是map结束后就立马shuffle呢？还要等全局的80%是为什么？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1641922573,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":485175,"ip_address":""},"score":545376,"extra":""},{"author":{"id":2166073,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/k3YD3y3BzGDSdrwRJyJY4BXsNJibfM4uzOdDVKIAlFApR2FZCLg2ibrZtJ4vuahA3LHLW9GKzH5CMGqCDhWjhZqg/132","nickname":"戒酒的李白","note":"","ucode":"744E1A22761647","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1066752,"avatar":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","nickname":"piboye","note":"","ucode":"7CFD8712857A85","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":556344,"discussion_content":"我也想问为什么，这个85%有什么讲究吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1647320286,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":545376,"ip_address":""},"score":556344,"extra":""}]}]},{"had_liked":false,"id":54842,"user_name":"Goku","can_delete":false,"product_type":"c1","uid":1031868,"ip_address":"","ucode":"8008F3BB10E609","user_header":"https://static001.geekbang.org/account/avatar/00/0f/be/bc/62d402da.jpg","comment_is_top":false,"comment_ctime":1545968850,"is_pvip":true,"replies":[{"id":"20627","content":"通常不会","user_name":"作者回复","comment_id":54842,"uid":"1007349","ip_address":"","utype":1,"ctime":1546675941,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"83150347474","product_id":100020201,"comment_content":"请问JobTracker和之前讲到的NameNode是在同一个服务器上的吗？","like_count":19,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":434582,"discussion_content":"通常不会","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1546675941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1116188,"avatar":"https://static001.geekbang.org/account/avatar/00/11/08/1c/ef15e661.jpg","nickname":" 臣馟飞扬","note":"","ucode":"F2F882B7678055","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":185377,"discussion_content":"这个也是我想问的问题，之前看的资料说的是对于每一项工作提交在系统中执行，有一个 JobTracker 驻留在 Namenode 和 多个 TaskTracker驻留在Datanode。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1582622655,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":40295,"user_name":"hunterlodge","can_delete":false,"product_type":"c1","uid":1069755,"ip_address":"","ucode":"5B83A79E784161","user_header":"https://static001.geekbang.org/account/avatar/00/10/52/bb/225e70a6.jpg","comment_is_top":false,"comment_ctime":1542590058,"is_pvip":false,"replies":[{"id":"14810","content":"不会，调用partitioner是一个job的任务分配，动态的，结束了就完成了，不存在扩展性问题。","user_name":"作者回复","comment_id":40295,"uid":"1007349","ip_address":"","utype":1,"ctime":1542766630,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"78852001386","product_id":100020201,"comment_content":"老师，您给出的Partitioner的代码所反映的算法不会影响集群的扩展性吗？为什么不是采用一致性哈希算法呢？","like_count":19,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429463,"discussion_content":"不会，调用partitioner是一个job的任务分配，动态的，结束了就完成了，不存在扩展性问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542766630,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39281,"user_name":"slam","can_delete":false,"product_type":"c1","uid":1064967,"ip_address":"","ucode":"0C5C3456F77051","user_header":"https://static001.geekbang.org/account/avatar/00/10/40/07/050a63ee.jpg","comment_is_top":false,"comment_ctime":1542243322,"is_pvip":false,"replies":[{"id":"14147","content":"没有问题，一般宕机也能计算完成，MapReduce有容错能力。计算结果不会有问题。","user_name":"作者回复","comment_id":39281,"uid":"1007349","ip_address":"","utype":1,"ctime":1542294750,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"78851654650","product_id":100020201,"comment_content":"想问下，在Hadoop上跑计算任务，在极端异常的条件下（数据机器down，网络隔离，namenode切换），能保证计算要么返回失败要么给出可信的结果吗？背景是这样的，考量在大数据平台上做资金的清算，非程序的错误，计算结果不能有错有漏，在单机db上这个肯定ok，不考虑事务前提下，Hadoop计算是否也毫无问题？可能考量数据一致性、任务状态一致性等方面，我了解太浅，想请教下老师，这种要求绝对计算准确的场景，hadoop目前胜任吗？","like_count":19,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":428994,"discussion_content":"没有问题，一般宕机也能计算完成，MapReduce有容错能力。计算结果不会有问题。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294750,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":133386,"user_name":"恐龙虾了个皮了个姜哩个哩个啷","can_delete":false,"product_type":"c1","uid":1237435,"ip_address":"","ucode":"5AFDDF65BE461F","user_header":"https://static001.geekbang.org/account/avatar/00/12/e1/bb/cbeae2fa.jpg","comment_is_top":false,"comment_ctime":1568541705,"is_pvip":false,"replies":[{"id":"51382","content":"shuffle时partition的算法不需要解决reduce机器的宕机问题，reduce是一个进程任务，可以启动在任何机器上，任何机器空闲都可以领取这个任务。代码中计算出来的只是任务分区编号，这个时候和机器还没关系呢。","user_name":"作者回复","comment_id":133386,"uid":"1007349","ip_address":"","utype":1,"ctime":1568703316,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"65993051145","product_id":100020201,"comment_content":"看到前边有同学问到shuffle过程为什么不使用一致性hash,感觉老师您的回答没有解决我的疑问，当reduce的机器宕机了的话，如果按照代码中的逻辑是有问题的吧…","like_count":16,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":467374,"discussion_content":"shuffle时partition的算法不需要解决reduce机器的宕机问题，reduce是一个进程任务，可以启动在任何机器上，任何机器空闲都可以领取这个任务。代码中计算出来的只是任务分区编号，这个时候和机器还没关系呢。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1568703316,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":40344,"user_name":"shawn","can_delete":false,"product_type":"c1","uid":1010221,"ip_address":"","ucode":"8F6A8C7920996E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6a/2d/ec4ed8ce.jpg","comment_is_top":false,"comment_ctime":1542596454,"is_pvip":false,"replies":[{"id":"14804","content":"所有服务器都启动tasktracker，等待jobtracker分配任务，跟NameNode一样","user_name":"作者回复","comment_id":40344,"uid":"1007349","ip_address":"","utype":1,"ctime":1542766161,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"48787236710","product_id":100020201,"comment_content":"JobTracker创建JobInProcess ，JobinPrcess根据分片数目和设置reduce数目创建TaskInprocess。 那么它是如何决定具体在哪些服务器创建 task tracker呢？我觉得需要了解这个过程，才能明白大数据如何分配和使用资源的。 请老师解答下，谢谢！","like_count":11,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429484,"discussion_content":"所有服务器都启动tasktracker，等待jobtracker分配任务，跟NameNode一样","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542766161,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":60977,"user_name":"金泽","can_delete":false,"product_type":"c1","uid":1324586,"ip_address":"","ucode":"0D351036053A8A","user_header":"https://static001.geekbang.org/account/avatar/00/14/36/2a/69c4c1f6.jpg","comment_is_top":false,"comment_ctime":1547600634,"is_pvip":false,"replies":[{"id":"21714","content":"任务是全局分配的，数据块所在服务器没有资源，就会分配给其他服务器计算，其他服务器远程访问数据。任务优先分配能本地得到的数据，但不是必须。","user_name":"作者回复","comment_id":60977,"uid":"1007349","ip_address":"","utype":1,"ctime":1547601717,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"44497273594","product_id":100020201,"comment_content":"“如果 TaskTracker 有空闲的计算资源（有空闲 CPU 核心），JobTracker 就会给它分配任务”，假设极端情况下，某块数据及其备份数据块所在的服务器一直没空闲，那这一块内容是不是就缺失了？","like_count":11,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":436692,"discussion_content":"任务是全局分配的，数据块所在服务器没有资源，就会分配给其他服务器计算，其他服务器远程访问数据。任务优先分配能本地得到的数据，但不是必须。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1547601717,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206520,"user_name":"lyuzh","can_delete":false,"product_type":"c1","uid":1263250,"ip_address":"","ucode":"E2203D92C70DA0","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKibhEIEcmUmz20DNlwHfS8DuqzYTnrqicicicCmHzzxQeHnMEfq4lBY6fytIuVDPQOB9MoZpBl2UuOEQ/132","comment_is_top":false,"comment_ctime":1586873725,"is_pvip":false,"replies":[{"id":"77195","content":"是的。<br><br>不过，对wordcount而言，map之后，reduce之前，其实还有个combine，在map端做一些本地数据的reduce计算，可以减少传输的数据量。","user_name":"作者回复","comment_id":206520,"uid":"1007349","ip_address":"","utype":1,"ctime":1586917541,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"40241579389","product_id":100020201,"comment_content":"老师，您好！一直想不明白一个问题，这里的shuffle过程将map结果中相同的key通过HTTP发送给reduce时这里依然是传递数据呀 以wordcount为例 每个单词map后的结果通过网络传递给reduce时依旧是个耗时操作吧？","like_count":9,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491817,"discussion_content":"是的。\n\n不过，对wordcount而言，map之后，reduce之前，其实还有个combine，在map端做一些本地数据的reduce计算，可以减少传输的数据量。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586917541,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39272,"user_name":"滴答","can_delete":false,"product_type":"c1","uid":1126947,"ip_address":"","ucode":"14143B90660178","user_header":"https://static001.geekbang.org/account/avatar/00/11/32/23/5de4b29a.jpg","comment_is_top":false,"comment_ctime":1542242247,"is_pvip":false,"replies":[{"id":"14148","content":"所谓的快完成是指很多个map进程已经完成，一小部分map进程还没完成，这个时候启动shuffle，将完成的map输出发送给reduce，直到所有map都完成","user_name":"作者回复","comment_id":39272,"uid":"1007349","ip_address":"","utype":1,"ctime":1542294977,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"40196947911","product_id":100020201,"comment_content":"map进程快要计算完成的时候将执行分区并发送给reduce进程进行排序和合并，那请问老师map完全计算完成的时候是会再次发送给reduce然后reduce再做排序合并计算吗？那这两部分的排序如何保证整体排序，如果是reduce之后再排序的话那之前排序会不会不需要？","like_count":10,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":428991,"discussion_content":"所谓的快完成是指很多个map进程已经完成，一小部分map进程还没完成，这个时候启动shuffle，将完成的map输出发送给reduce，直到所有map都完成","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":102734,"user_name":"李慢慢","can_delete":false,"product_type":"c1","uid":1514463,"ip_address":"","ucode":"AED7F0708400B4","user_header":"https://static001.geekbang.org/account/avatar/00/17/1b/df/4caad64d.jpg","comment_is_top":false,"comment_ctime":1560300605,"is_pvip":false,"replies":[{"id":"37194","content":"Hashcode只是用来做分区，冲突了分在一起区里没关系，真正对key进行合并的时候还是根据key的值。","user_name":"作者回复","comment_id":102734,"uid":"1007349","ip_address":"","utype":1,"ctime":1560320515,"user_name_real":"李智慧"}],"discussion_count":2,"race_medal":0,"score":"31625071677","product_id":100020201,"comment_content":"不同的key经过hash计算，也有可能会产生同样的hashcode啊，这种情况怎么处理呢？莫非是跟HashMap一样，搞个链表或者红黑树处理吗？","like_count":8,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":453552,"discussion_content":"Hashcode只是用来做分区，冲突了分在一起区里没关系，真正对key进行合并的时候还是根据key的值。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1560320515,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1708090,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/10/3a/053852ee.jpg","nickname":"张小喵","note":"","ucode":"2CD5003020D751","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":63522,"discussion_content":"partation函数保证相同的key一定会被分配到同一个reduce服务器上，而不是保证一个服务器上只能有一个key","likes_number":7,"is_delete":false,"is_hidden":false,"ctime":1574873259,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":48535,"user_name":"挨踢菜鸟","can_delete":false,"product_type":"c1","uid":1195409,"ip_address":"","ucode":"CC208C8749B228","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJZuwMDaoJviaf3lZ5BOgAvTzLzmbGrMrCZ22krLSRyxpKUrVicU9pSnWsyuSHjksyNldBpXrRzUqeA/132","comment_is_top":false,"comment_ctime":1544484785,"is_pvip":false,"replies":[{"id":"17334","content":"学习Hadoop1有助于学习yarn，请看后面yarn一期专栏","user_name":"作者回复","comment_id":48535,"uid":"1007349","ip_address":"","utype":1,"ctime":1544487700,"user_name_real":"李智慧"}],"discussion_count":1,"race_medal":0,"score":"31609255857","product_id":100020201,"comment_content":"老师怎么还拿hadoop1举例呢，现在都是hadoop2，以及Yarn了","like_count":7,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":432232,"discussion_content":"学习Hadoop1有助于学习yarn，请看后面yarn一期专栏","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1544487700,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39336,"user_name":"李二木","can_delete":false,"product_type":"c1","uid":1103091,"ip_address":"","ucode":"30E03BB84ADB27","user_header":"https://static001.geekbang.org/account/avatar/00/10/d4/f3/129d6dfe.jpg","comment_is_top":false,"comment_ctime":1542250585,"is_pvip":true,"replies":[{"id":"14144","content":"map输入数据在文件中的偏移量","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542294333,"ip_address":"","comment_id":39336,"utype":1}],"discussion_count":1,"race_medal":0,"score":"31607021657","product_id":100020201,"comment_content":"文中第一幅图，map输入 中 0 hello world 12 Byte world  ，这里数字代表什么意思了？是map 输入顺序下标吗？<br>","like_count":7,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429016,"discussion_content":"map输入数据在文件中的偏移量","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294333,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":40175,"user_name":"M","can_delete":false,"product_type":"c1","uid":1117633,"ip_address":"","ucode":"88F6F1433A01A3","user_header":"https://static001.geekbang.org/account/avatar/00/11/0d/c1/d36816df.jpg","comment_is_top":false,"comment_ctime":1542537649,"is_pvip":false,"replies":[{"id":"14815","content":"并不是，sparkcontext只是初始化任务dag，等待executor进程领任务才具体分配任务，分配进程本地数据，还是移动计算","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542767924,"ip_address":"","comment_id":40175,"utype":1}],"discussion_count":1,"race_medal":0,"score":"27312341425","product_id":100020201,"comment_content":"最近在学习Spark，发现Spark中似乎并没有“移动计算而不是移动数据”。因为在Spark中初始化SparkContext时就已经分配好计算资源了，随后再对用户指定的数据进行计算。所以在Spark中还是移动了数据，请问我的想法正确吗？","like_count":6,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429410,"discussion_content":"并不是，sparkcontext只是初始化任务dag，等待executor进程领任务才具体分配任务，分配进程本地数据，还是移动计算","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542767924,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":136776,"user_name":"钱","can_delete":false,"product_type":"c1","uid":1009652,"ip_address":"","ucode":"2C92A243A463D4","user_header":"https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg","comment_is_top":false,"comment_ctime":1569506623,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"23044343103","product_id":100020201,"comment_content":"听了几遍，感觉还是没完全懂。<br>分而治之的思想我是理解的，主从架构我也是理解的，不过转移计算比转移数据更划算，这个就理解的不太好。<br>HDFS主从架构，主负责管理和路由信息，从是真正存储数据的地方，本质上可以看做是对数据的水平切分，那些数据放在哪里，这个很关键，怎么存储就决定了怎么获取。这块老师还没具体讲怎么切分，不过常用的就是各种哈希算法。<br>移动计算，难道是把计算的程序移动到对应的数据所在机器上，然后计算，计算完了将结果放到一个地方，如果还需要计算再把计算的程序移动到对应的数据所在机器继续计算，最后合并计算结果。<br>那如果真是这样，数据不动，动计算程序，那计算程序必须在任何机器上对于任何数据都能启动运行，并且计算完后知道把数据放在那，还要告诉分计算程序的leader，好做下一步的任务分配。<br>我先记一笔，回头再看一遍，也许就能听明白啦！<br><br>课后题，有点意思，可以从很多维度来答，我猜一下：<br>1：这个请求在用户的电脑上，要经过浏览器应用程序的进程，从用户态内存空间写到内核态内存空间，然后经过DMA控制把信息写到网卡，网卡再把信息发送出去，信息发出去的时候就会从ISO七层网络通信协议，从上往下一层打包加信息，直到最低层编程光或电信号<br>2：电信号在传输的过程中会经过路由器，电缆通信线路，我们访问的是域名，还有经过域名解析，定位到网络中的具体机器，然后经过局域网、广域网、局域网<br>3：到了对应的局域网络后，会过网关，过负载均衡器，到应用服务器，到具体的应用程序进程中，然后进行业务逻辑的处理，可能会查询数据库或者缓存，然后进行计算，计算完了就返回，过程和来时相似","like_count":6},{"had_liked":false,"id":88778,"user_name":"方伟","can_delete":false,"product_type":"c1","uid":1177897,"ip_address":"","ucode":"71194E674DC6D7","user_header":"https://static001.geekbang.org/account/avatar/00/11/f9/29/d03c7ad5.jpg","comment_is_top":false,"comment_ctime":1555998732,"is_pvip":false,"replies":[{"id":"32709","content":"Hive在处理SQL数据倾斜的时候有很多最佳实践，网上也有不少资料，可参考。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1557020926,"ip_address":"","comment_id":88778,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23030835212","product_id":100020201,"comment_content":"老师，我想问下，如果数据中的个别key数据量很大，会造成数据倾斜，有什么方法可以解决这个数据倾斜的问题呢？","like_count":5,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447897,"discussion_content":"Hive在处理SQL数据倾斜的时候有很多最佳实践，网上也有不少资料，可参考。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557020926,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":72680,"user_name":"落叶飞逝的恋","can_delete":false,"product_type":"c1","uid":1046429,"ip_address":"","ucode":"F9A95DB28BCF1E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f7/9d/be04b331.jpg","comment_is_top":false,"comment_ctime":1551691638,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18731560822","product_id":100020201,"comment_content":"java程序用于合并mapper和reducer的代码。<br>public class WordCountApp {<br>    public static void main(String[] args) throws Exception {<br>        &#47;&#47;创建配置信息<br>        Configuration configuration = new Configuration();<br><br>        &#47;&#47;创建Job<br>        Job worldCountJob = Job.getInstance(configuration, &quot;WorldCount&quot;);<br><br>        &#47;&#47;设置Job的处理类<br>        worldCountJob.setJarByClass(WordCountApp.class);<br><br>        &#47;&#47;设置作业的输入路径<br>        FileInputFormat.setInputPaths(worldCountJob, new Path(args[0]));<br><br>        &#47;&#47;设置map相关参数<br>        worldCountJob.setMapperClass(MyMapper.class);<br>        worldCountJob.setMapOutputKeyClass(Text.class);<br>        worldCountJob.setMapOutputValueClass(LongWritable.class);<br><br>        &#47;&#47;设置reduce相关参数<br>        worldCountJob.setReducerClass(MyReducer.class);<br>        worldCountJob.setOutputKeyClass(Text.class);<br>        worldCountJob.setOutputValueClass(LongWritable.class);<br><br>        &#47;&#47;设置作业的输出路径<br>        FileOutputFormat.setOutputPath(worldCountJob, new Path(args[1]));<br><br>        System.out.println(worldCountJob.waitForCompletion(true) ? 0 : 1);<br>    }<br>}<br>","like_count":4},{"had_liked":false,"id":42464,"user_name":"不求","can_delete":false,"product_type":"c1","uid":1231745,"ip_address":"","ucode":"C807EB6F4BE515","user_header":"https://static001.geekbang.org/account/avatar/00/12/cb/81/14dd3bab.jpg","comment_is_top":false,"comment_ctime":1542957929,"is_pvip":false,"replies":[{"id":"15184","content":"因为应用程序要指定输入文件路径，jobtracker可以根据输入路径和分片大小计算分片数，这个数目就是map任务数。<br><br>用文件路径名从NameNode取文件大小，计算分片数。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542968993,"ip_address":"","comment_id":42464,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18722827113","product_id":100020201,"comment_content":"老师，您好，我有个疑问？<br>在计算过程的第4个步骤上：<br>4.JobInProcess 根据输入数据分片数目（通常情况就是数据块的数目）和设置的 Reduce 数目创建相应数量的 TaskInProcess。<br><br>问题：<br>JobInProcess 是如何根据输入数据分片数目和设置的Reduce数目创建相应数量的TaskInProcess？<br><br>JobInProcess是JobTracker进程在主服务器上创建的，这里的根据输入数据分片数目是指的从服务器上的DataNode的数据块的分片数目吗？而DataNode和TaskTracker是在从服务器上，这个JobTracker进程是什么时候知道输入数据分片数目的？是谁在什么时候确认的呢？","like_count":4,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":430104,"discussion_content":"因为应用程序要指定输入文件路径，jobtracker可以根据输入路径和分片大小计算分片数，这个数目就是map任务数。\n\n用文件路径名从NameNode取文件大小，计算分片数。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542968993,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39370,"user_name":"hua168","can_delete":false,"product_type":"c1","uid":1065255,"ip_address":"","ucode":"CFF9A7E86EBA48","user_header":"https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg","comment_is_top":false,"comment_ctime":1542258151,"is_pvip":false,"replies":[{"id":"14142","content":"可以，普通笔记本就可以，能，最新版","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542294116,"ip_address":"","comment_id":39370,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18722127335","product_id":100020201,"comment_content":"hadoop可以通过实验去练习吧？一般什么配置？官网的说明文档能当扩展阅读吗？选择那个版本？","like_count":4,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429034,"discussion_content":"可以，普通笔记本就可以，能，最新版","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542294116,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39337,"user_name":"老男孩","can_delete":false,"product_type":"c1","uid":1134514,"ip_address":"","ucode":"CEC6D47412F620","user_header":"https://static001.geekbang.org/account/avatar/00/11/4f/b2/1e8b5616.jpg","comment_is_top":false,"comment_ctime":1542250605,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"18722119789","product_id":100020201,"comment_content":"这个思考题感觉问的很开放，我只能按照我的理解回答一下。不管是手机端还是pc端发起的请求都是http或者https的请求，协议下层(tcp&#47;ip)会根据请求地址建立连接把请求的数据（字节流）传输到服务端对应处理方法中。当然服务端需要对请求的地址url和对应的响应方法做映射。响应方法从request中读取对应的请求数据然后处理，最后把结果通过response返回给用户。响应完成后，不会保留之前的状态数据，但连接不一定就会断开，下次请求的时候就不用再重新建立连接了。","like_count":4},{"had_liked":false,"id":192606,"user_name":"杰哥长得帅","can_delete":false,"product_type":"c1","uid":1241993,"ip_address":"","ucode":"5A7FD1794F62D7","user_header":"https://static001.geekbang.org/account/avatar/00/12/f3/89/fcfecb46.jpg","comment_is_top":false,"comment_ctime":1584863508,"is_pvip":false,"replies":[{"id":"73734","content":"1 不在一个机器上就通过网络传输<br>2 在JobTracker发给TaskTracker的TaskInfo里","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1584932448,"ip_address":"","comment_id":192606,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14469765396","product_id":100020201,"comment_content":"老师我有两个问题：<br>1、DataNode 和 TaskTracker 有没有不在一个机器上的情况？如果有的话，会怎么处理<br>2、TaskTracker 的任务参数，比如『要处理的数据在文件中的起始位置和偏移量、数据块多个备份的 DataNode 主机名』根源上是从哪里获取的，理论上不是jar包，因为jar包对不同进程来说应该是同一份逻辑代码","like_count":3,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":488429,"discussion_content":"1 不在一个机器上就通过网络传输\n2 在JobTracker发给TaskTracker的TaskInfo里","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1584932448,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39383,"user_name":"Jowin","can_delete":false,"product_type":"c1","uid":1114356,"ip_address":"","ucode":"19017F7D06C22A","user_header":"https://static001.geekbang.org/account/avatar/00/11/00/f4/cc5f0896.jpg","comment_is_top":false,"comment_ctime":1542260759,"is_pvip":false,"replies":[{"id":"14139","content":"依然适合，有的map输出比map输入会多很多，比如矩阵相乘，MapReduce依然适合","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542293660,"ip_address":"","comment_id":39383,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14427162647","product_id":100020201,"comment_content":"老师，有一个问题，在WordCount的例子中，经过map处理之后数据量明细减少了，之后再做shuffle和reduce就比较容易。如果有些类型的数据，map处理并不能明显减小数据量，是否就不适合用mapreduce模型处理？","like_count":3,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429039,"discussion_content":"依然适合，有的map输出比map输入会多很多，比如矩阵相乘，MapReduce依然适合","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542293660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":88566,"user_name":"雪候鸟","can_delete":false,"product_type":"c1","uid":1121047,"ip_address":"","ucode":"B7A82676DFD39F","user_header":"https://static001.geekbang.org/account/avatar/00/11/1b/17/64e18a78.jpg","comment_is_top":false,"comment_ctime":1555943910,"is_pvip":false,"replies":[{"id":"32710","content":"直接传输","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1557020971,"ip_address":"","comment_id":88566,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10145878502","product_id":100020201,"comment_content":"想问一下shuffle过程的前半程的数据流是直接从服务器节点通过hash计算传输到对应的其他服务器节点？还是通过中心服务器JobTracker统一分发？","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":447831,"discussion_content":"直接传输","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1557020971,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":65701,"user_name":"杰之7","can_delete":false,"product_type":"c1","uid":1297232,"ip_address":"","ucode":"F7DA2E21085332","user_header":"https://static001.geekbang.org/account/avatar/00/13/cb/50/66d0bd7f.jpg","comment_is_top":false,"comment_ctime":1549595340,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10139529932","product_id":100020201,"comment_content":"通过这一节的复习，这节内容主要解决MR的启动和运行，数据的合并和连接机制。在这一节中，涉及到了大数据技术的核心技术，调用JAR包，执行机制中会将HDFS中的数据块联系起来，通过HTTP通信发送给相对应的reduce。这些内容设计到了Java编程，网络通信，Linux多服务器的知识，对这些基础知识的掌握是做数据技术的前提。<br><br>在MR的启动和连接步骤中，首先通过应用程序将JAR包存储在HDFS中供将来服务器执行MR计算。<br>应用程序提交Job给JobTrack,JobTrack创建Jobinprocess,Jobinprocess根据数据快和设置reduce数目创建TaskInprocess,这部也就将HDFS和MR关联了起来。TaskTracker与JobTracker通信，分配任务，然后TaskTracker根据任务类型和参数启动相应的Maop或者redece，找到JAR包执行任务。<br><br>上述就完成了作业的启动和运行，对于数据的联合和合并，用到的是Shuffle,通过调用partiton接口，对Map产生的&lt;key,value&gt;分区选择，通过HTTP通过发送给对应的reduce执行。<br><br>通过复习，这一节的内容贯穿了整个大数据技术的核心，涉及到了计算机的JAR，HTTP，Linux， 数据库的joinw，HDFS和MR的主从架构模式，值得我多次重复复习。","like_count":2},{"had_liked":false,"id":39818,"user_name":"落叶飞逝的恋","can_delete":false,"product_type":"c1","uid":1046429,"ip_address":"","ucode":"F9A95DB28BCF1E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f7/9d/be04b331.jpg","comment_is_top":false,"comment_ctime":1542360858,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"10132295450","product_id":100020201,"comment_content":"思考题：以百度搜索关键词为例：<br>1.用户端与服务端三次握手<br>2.服务端返回相关网页<br>3.用户浏览器接受响应结果<br>","like_count":2},{"had_liked":false,"id":278834,"user_name":"eden","can_delete":false,"product_type":"c1","uid":1907540,"ip_address":"","ucode":"CF8F33380D2C4D","user_header":"https://static001.geekbang.org/account/avatar/00/1d/1b/54/c0f328ab.jpg","comment_is_top":false,"comment_ctime":1613376966,"is_pvip":false,"replies":[{"id":"101430","content":"shuffle本身没有时延要求的。<br>随着网络带宽和时延的提升，计算与存储分离是趋势，其实也就无所谓移动计算还是移动数据了。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1613634619,"ip_address":"","comment_id":278834,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5908344262","product_id":100020201,"comment_content":"目前云计算里面各大厂商都在推崇 大数据存算分离的方案： 即 数据总量用对象存储（华为云OBS带宽超级大），datanode就可以带比较小的磁盘或者云盘，实现最终计算与存储的分离。<br>但是我们再基于他们的大数据存算分离方案中，shuffle盘还是要在datanode上面，这个是因为shuffle要求的时延很低吗？  ","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":515523,"discussion_content":"shuffle本身没有时延要求的。\n随着网络带宽和时延的提升，计算与存储分离是趋势，其实也就无所谓移动计算还是移动数据了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1613634619,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":240113,"user_name":"Monday","can_delete":false,"product_type":"c1","uid":1250907,"ip_address":"","ucode":"77B9BACC783598","user_header":"https://static001.geekbang.org/account/avatar/00/13/16/5b/83a35681.jpg","comment_is_top":false,"comment_ctime":1596764511,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5891731807","product_id":100020201,"comment_content":"千言万语不如一图来得直接，虽然没完全消化","like_count":2},{"had_liked":false,"id":174802,"user_name":"俊刚","can_delete":false,"product_type":"c1","uid":1281385,"ip_address":"","ucode":"2FD87530204E73","user_header":"https://static001.geekbang.org/account/avatar/00/13/8d/69/e9886e74.jpg","comment_is_top":false,"comment_ctime":1580392006,"is_pvip":false,"replies":[{"id":"67946","content":"相同key取模结果一样，得到的相同任务ID。这两句话不是没有联系，而是说同一件事。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1580437445,"ip_address":"","comment_id":174802,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5875359302","product_id":100020201,"comment_content":"&quot;MapReduce 框架默认的 Partitioner 用 Key 的哈希值对 Reduce 任务数量取模，相同的 Key 一定会落在相同的 Reduce 任务 ID 上。&quot;没有理解这句话的意思，个人感觉前一句与后一句没有必然联系，请老师不吝赐教，万分感谢！","like_count":2,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":482525,"discussion_content":"相同key取模结果一样，得到的相同任务ID。这两句话不是没有联系，而是说同一件事。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580437445,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":151215,"user_name":"Zend","can_delete":false,"product_type":"c1","uid":1053921,"ip_address":"","ucode":"80EBB0B6772E27","user_header":"https://static001.geekbang.org/account/avatar/00/10/14/e1/ee5705a2.jpg","comment_is_top":false,"comment_ctime":1573681756,"is_pvip":false,"replies":[{"id":"58217","content":"是的","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1573702340,"ip_address":"","comment_id":151215,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5868649052","product_id":100020201,"comment_content":"老师请问 Map输出后 根据key 做partition 分区归并是通过网络发送到指定的reduce做最后的合并排序对吗？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":474401,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1573702340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":58613,"user_name":"1峰","can_delete":false,"product_type":"c1","uid":1133940,"ip_address":"","ucode":"E0087F057DF476","user_header":"https://static001.geekbang.org/account/avatar/00/11/4d/74/bc52ae32.jpg","comment_is_top":false,"comment_ctime":1547136442,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5842103738","product_id":100020201,"comment_content":"通过这2天的学习，我类比总结一下hadoop1和hadoop2的资源管理模式，如有错漏请指正。<br><br>Hadoop1：我收到任务，由我来拆解任务成子任务，并且联系下方的同学谁有空就谁做，有任务的同学单独向我汇报进度。<br><br>Hadoop2:我知道有个任务要处理，在同学中选中一位班长，由班长来拆解子任务并告诉我需要的多少同学去做，由班长收集进度并汇报给我。<br><br>这2种方式，同学们之间都系会有内容传递，把“移动的不是数据而是计算”改成“尽量少移动数据，分配计算任务”是否更合适？而如何合理分配计算是不是更值得探讨？<br><br>另外，希望老师或前辈们解答，为什么说hadoop1的资源调度和计算框架是强耦合？","like_count":1},{"had_liked":false,"id":58535,"user_name":"编程侠阿松","can_delete":false,"product_type":"c1","uid":1237822,"ip_address":"","ucode":"D318B8EC8A251B","user_header":"https://static001.geekbang.org/account/avatar/00/12/e3/3e/91454f2e.jpg","comment_is_top":false,"comment_ctime":1547106914,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5842074210","product_id":100020201,"comment_content":"为什么JobTracker分配任务给TaskTracker的时候，TaskTracker和数据块要刚好在一个机器上呢？TaskTracker输入的数据不是应该通过HDSF的API获取的吗？","like_count":1},{"had_liked":false,"id":41835,"user_name":"小千","can_delete":false,"product_type":"c1","uid":1280968,"ip_address":"","ucode":"BF12E4ABD69FA8","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKPBiaSZVibZwoUEUcvbF4JCfOghmvPdUfbFHeDd2g5m6NbuzeN3S3b7KxZCA8FmtrH9N51Z5P177iaA/132","comment_is_top":false,"comment_ctime":1542857363,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5837824659","product_id":100020201,"comment_content":"进程起的名字都很有意思，比如说shuffle，意思是洗牌，map算出来的结果，就像纸牌一样，洗一下给reduce","like_count":1},{"had_liked":false,"id":41211,"user_name":"ming","can_delete":false,"product_type":"c1","uid":1054341,"ip_address":"","ucode":"63135A86598E64","user_header":"https://static001.geekbang.org/account/avatar/00/10/16/85/e4d53282.jpg","comment_is_top":false,"comment_ctime":1542760003,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5837727299","product_id":100020201,"comment_content":"1、用户发起请求，到达硬件分发器（如F5）<br>2、硬件分发器转发给nginx<br>3、nginx转发给业务服务器，如tomcat经过，进入filter<br>4、框架层面处理请求，如spring intercepter等<br>5、进入业务处理层，处理完毕后原路返回<br>","like_count":1},{"had_liked":false,"id":40342,"user_name":"shawn","can_delete":false,"product_type":"c1","uid":1010221,"ip_address":"","ucode":"8F6A8C7920996E","user_header":"https://static001.geekbang.org/account/avatar/00/0f/6a/2d/ec4ed8ce.jpg","comment_is_top":false,"comment_ctime":1542596295,"is_pvip":false,"replies":[{"id":"14806","content":"资源估算需要开发者确定，在参数里申请。<br>任务去哪里拉数据，文中有讲述～","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542766323,"ip_address":"","comment_id":40342,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5837563591","product_id":100020201,"comment_content":"老师，你会讲hdfs存储嘛？ 比如一个任务去哪里拉如何去hdfs上拉取数据文件，一个job 如何估算它需要的资源，您在文章中也说了任务调度本质就是解决 现有多少资源需要什么资源，然后进行匹配。具体如何如果解决估算资源和哪里拉数据文件，老师可以细讲一下嘛？谢谢！","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429482,"discussion_content":"资源估算需要开发者确定，在参数里申请。\n任务去哪里拉数据，文中有讲述～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542766323,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":40290,"user_name":"hunterlodge","can_delete":false,"product_type":"c1","uid":1069755,"ip_address":"","ucode":"5B83A79E784161","user_header":"https://static001.geekbang.org/account/avatar/00/10/52/bb/225e70a6.jpg","comment_is_top":false,"comment_ctime":1542589836,"is_pvip":false,"replies":[{"id":"14811","content":"管理任务。<br>map和reduce都有关啊。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542766684,"ip_address":"","comment_id":40290,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5837557132","product_id":100020201,"comment_content":"老师，请问TaskInProcess是做什么用的呢？为什么它的创建和Reducer有关，却和Mapper无关呢？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429460,"discussion_content":"管理任务。\nmap和reduce都有关啊。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542766684,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":40140,"user_name":"嗨喽","can_delete":false,"product_type":"c1","uid":1162527,"ip_address":"","ucode":"E13573559A50D4","user_header":"https://static001.geekbang.org/account/avatar/00/11/bd/1f/e9fbc712.jpg","comment_is_top":false,"comment_ctime":1542521798,"is_pvip":false,"replies":[{"id":"14816","content":"是的，请看下期","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542767952,"ip_address":"","comment_id":40140,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5837489094","product_id":100020201,"comment_content":"现在最新版的没有jobtask了～～","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429392,"discussion_content":"是的，请看下期","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542767952,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39952,"user_name":"静静的等","can_delete":false,"product_type":"c1","uid":1266183,"ip_address":"","ucode":"93E4CE489CB025","user_header":"https://static001.geekbang.org/account/avatar/00/13/52/07/5d74b03f.jpg","comment_is_top":false,"comment_ctime":1542418850,"is_pvip":false,"replies":[{"id":"14393","content":"reduce进程数是指定的","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542459456,"ip_address":"","comment_id":39952,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5837386146","product_id":100020201,"comment_content":"假如有10和datanode节点，每个节点上都会有一个map进程，那会有几个reduce进程呢？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429307,"discussion_content":"reduce进程数是指定的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542459456,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39830,"user_name":"Jowin","can_delete":false,"product_type":"c1","uid":1114356,"ip_address":"","ucode":"19017F7D06C22A","user_header":"https://static001.geekbang.org/account/avatar/00/11/00/f4/cc5f0896.jpg","comment_is_top":false,"comment_ctime":1542363072,"is_pvip":false,"replies":[{"id":"14307","content":"MapReduce是可以跨块读记录的～","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542366965,"ip_address":"","comment_id":39830,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5837330368","product_id":100020201,"comment_content":"GFS把“记录追加的数据大小严格控制在Chunk 最大尺寸的 1&#47;4”，如果文件尾部的Chunk能容纳追加的记录，就把记录追加到这个Chunk。否则，就把这个Chunk做填充，然后新分配一个Chunk来写追加的记录，这就保证了记录存储不会跨块。<br>今天意识到这个设计对MapReduce过程是有用的，如果记录跨块，map实例就无法拿到完整的记录，计算也必然不准确。","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429246,"discussion_content":"MapReduce是可以跨块读记录的～","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542366965,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1087595,"avatar":"https://static001.geekbang.org/account/avatar/00/10/98/6b/a09c0c5d.jpg","nickname":"生栋","note":"","ucode":"BEE3AE1E23017A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":155708,"discussion_content":"你俩说的不是一个事儿","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1580276806,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39269,"user_name":"贾洵","can_delete":false,"product_type":"c1","uid":1173596,"ip_address":"","ucode":"6B66BC83615A26","user_header":"https://static001.geekbang.org/account/avatar/00/11/e8/5c/fad21603.jpg","comment_is_top":false,"comment_ctime":1542241996,"is_pvip":false,"replies":[{"id":"14149","content":"1根据key进行partition分区发送给reduce，一个数据只发给一个reduce，不会重复<br>2MapReduce框架操作","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542295153,"ip_address":"","comment_id":39269,"utype":1}],"discussion_count":1,"race_medal":1,"score":"5837209292","product_id":100020201,"comment_content":"读完上一章的时候当时有这样一个疑问就是：mapreduce是如何将计算结果汇总的？读完这章之后知道是shuffle过程。但是现在又有了几个疑问：<br>1.看到shuffle图片，描述的是map计算结束后会向每个节点都输出自己的计算结果。这样每个reduce都会接到大量的map结算结果。这样是否会造成数据的重复计算？我们这么做是不是也是体现分布式的优势。即每个输出都一样的。<br>2.整个mapreduce计算第一步是上传jar包，这个上传过程是人为操作吗？后续map和reduce计算时下载jar包，是如何找到对应计算jar包的呢？","like_count":1,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":428990,"discussion_content":"1根据key进行partition分区发送给reduce，一个数据只发给一个reduce，不会重复\n2MapReduce框架操作","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542295153,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":346976,"user_name":"L君","can_delete":false,"product_type":"c1","uid":2769903,"ip_address":"","ucode":"367C49C2742424","user_header":"https://static001.geekbang.org/account/avatar/00/2a/43/ef/da7a32df.jpg","comment_is_top":false,"comment_ctime":1653570099,"is_pvip":false,"replies":[{"id":"126490","content":"这里说得数据块是指HDFS的block吗？<br>是的<br><br>假设一个word count的文件（内容是：word1 word2 word3），有可能被切分成（word1 wor）和（d2 word3）两个块吗，<br>数据块可以使文本也可以是二进制。map函数的输入是一行，如果是二进制，要解析出换行符，按行交给map函数技术，MapReduce框架会确保map函数不会得到半个单词<br><br>数据块是指HDFS的block吗，JobTracker是怎么知道这些数据块在哪个DataNode上的<br>是的，JobTracker要获取hdfs的元数据信息，记录每个要处理的数据块的地址信息，并和TaskTracker的地址匹配。大数据有个专有名词：locality，描述这个过程。<br>","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1653613852,"ip_address":"","comment_id":346976,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1653570099","product_id":100020201,"comment_content":"对文中提到的描述有以下疑问，希望老师能帮忙解惑，谢谢。<br><br>4.JobInProcess 根据输入数据分片数目（通常情况就是数据块的数目）和设置的 Reduce 数目创建相应数量的 TaskInProcess。<br><br>这里说得数据块是指HDFS的block吗？<br>这里的数据块是指二进制数据吗？如果是二进制数据，假设一个word count的文件（内容是：word1 word2 word3），有可能被切分成（word1 wor）和（d2 word3）两个块吗，这样的话map是怎么做统计的？<br><br>6. 如果 TaskTracker 有空闲的计算资源（有空闲 CPU 核心），JobTracker 就会给它分配任务。分配任务的时候会根据 TaskTracker 的服务器名字匹配在同一台机器上的数据块计算任务给它，使启动的计算任务正好处理本机上的数据，以实现我们一开始就提到的“移动计算比移动数据更划算”。<br><br>数据块是指HDFS的block吗，JobTracker是怎么知道这些数据块在哪个DataNode上的，从而将任务分配到DataNode来达到移动计算。<br><br>","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":573695,"discussion_content":"这里说得数据块是指HDFS的block吗？\n是的\n\n假设一个word count的文件（内容是：word1 word2 word3），有可能被切分成（word1 wor）和（d2 word3）两个块吗，\n数据块可以使文本也可以是二进制。map函数的输入是一行，如果是二进制，要解析出换行符，按行交给map函数技术，MapReduce框架会确保map函数不会得到半个单词\n\n数据块是指HDFS的block吗，JobTracker是怎么知道这些数据块在哪个DataNode上的\n是的，JobTracker要获取hdfs的元数据信息，记录每个要处理的数据块的地址信息，并和TaskTracker的地址匹配。大数据有个专有名词：locality，描述这个过程。\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653613852,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":330340,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1641923174,"is_pvip":true,"replies":[{"id":"120573","content":"没有，jar包在HDFS，如果想加速，可以设置很多个副本","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1642071414,"ip_address":"","comment_id":330340,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641923174","product_id":100020201,"comment_content":"jar包的分发有p2p的优化手段吗？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545884,"discussion_content":"没有，jar包在HDFS，如果想加速，可以设置很多个副本","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642071414,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":330339,"user_name":"piboye","can_delete":false,"product_type":"c1","uid":1066752,"ip_address":"","ucode":"7CFD8712857A85","user_header":"https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg","comment_is_top":false,"comment_ctime":1641923149,"is_pvip":true,"replies":[{"id":"120572","content":"是的","user_name":"作者回复","user_name_real":"编辑","uid":"1007349","ctime":1642071364,"ip_address":"","comment_id":330339,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641923149","product_id":100020201,"comment_content":"老师jobtracker整个集群只有一个？","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":545883,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1642071364,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":323146,"user_name":"骆驼、","can_delete":false,"product_type":"c1","uid":1443660,"ip_address":"","ucode":"252ECDB1524867","user_header":"https://static001.geekbang.org/account/avatar/00/16/07/4c/f9852f8f.jpg","comment_is_top":false,"comment_ctime":1637744036,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637744036","product_id":100020201,"comment_content":"public static class ReadMapper extends Mapper&lt;Object, Text, Text, Text&gt; {<br>        @Override<br>        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {<br>            context.write(value,new Text(&quot;1&quot;));<br>        }<br>    }<br><br>    public static class WriteReducer extends Reducer&lt;Text,Text,Text,Text&gt;{<br>        @Override<br>        protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {<br>            int sum = 0;<br>            for (Text value : values) {<br>                sum+=Integer.valueOf(value.toString()).intValue();<br>            }<br>            String outKey = key.toString()+&quot;,&quot;+sum;<br>            context.write(new Text(outKey),new Text(&quot;&quot;));<br>        }<br>    }","like_count":0},{"had_liked":false,"id":304369,"user_name":"骆驼","can_delete":false,"product_type":"c1","uid":1372865,"ip_address":"","ucode":"6488F52A953A9C","user_header":"","comment_is_top":false,"comment_ctime":1627374709,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1627374709","product_id":100020201,"comment_content":"这一节课问题就多了起来，看来过程还是没讲透彻，哈哈","like_count":0},{"had_liked":false,"id":292025,"user_name":"南天","can_delete":false,"product_type":"c1","uid":1796693,"ip_address":"","ucode":"DA26E76BC95F1B","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJ4QCWqGgMN4kp615f0Dlb8Ty61iaeOfMia7dyOic5mcgiarxGv8pyra1dibiajXicDLibxqsyM6uNabia4ckw/132","comment_is_top":false,"comment_ctime":1620647610,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1620647610","product_id":100020201,"comment_content":"老师，数据倾斜除了优化map的key方法，是否还有其他方法？","like_count":0},{"had_liked":false,"id":280904,"user_name":"Geek_a852c8","can_delete":false,"product_type":"c1","uid":2206508,"ip_address":"","ucode":"D925A9D71A76BA","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/O6qftWBakkjQHrAhF5tia21GKkQxibJaPy2nWUKc9eiaouaqb67Hj60RRKgjgHhzPmaxaHkLszcNYrDSkj21lPylQ/132","comment_is_top":false,"comment_ctime":1614475580,"is_pvip":false,"replies":[{"id":"102045","content":"是的","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1614576970,"ip_address":"","comment_id":280904,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1614475580","product_id":100020201,"comment_content":"Partitioner分区选择，然后通过 HTTP 通信发送给对应的 Reduce 进程, <br>问： reduce是等待所有Partitioner发送过来，汇总一起再处理吗？<br><br>","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516237,"discussion_content":"是的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614576970,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":254008,"user_name":"陈斌","can_delete":false,"product_type":"c1","uid":1367048,"ip_address":"","ucode":"B639AB5F6AA03D","user_header":"https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg","comment_is_top":false,"comment_ctime":1603014100,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1603014100","product_id":100020201,"comment_content":"在shuffle 的框架图中。<br>老师能举个实际的例子吗（需要对两种类型（或者更多类型）的数据根据 Key 进行连接 的例子）？<br>服务器1、2、3  ... 中 Map任务输出的结果是什么（数据结构 等）<br>服务器1、2、3  ... 中 Reduce任务中 的输入数据是什么（数据结构 等）<br>这样比较好理解 shuffle 干了啥，作用是什么？","like_count":0},{"had_liked":false,"id":254006,"user_name":"陈斌","can_delete":false,"product_type":"c1","uid":1367048,"ip_address":"","ucode":"B639AB5F6AA03D","user_header":"https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg","comment_is_top":false,"comment_ctime":1603013094,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1603013094","product_id":100020201,"comment_content":"分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是 shuffle<br>这句话能不能用一个实际的例子来解释下，例如：计算计算单词词频这个问题，shuffle做了啥，这个例子合并的数据是什么形式的数据？","like_count":0},{"had_liked":false,"id":222368,"user_name":"走马","can_delete":false,"product_type":"c1","uid":1251016,"ip_address":"","ucode":"EEFE8F7590FFA4","user_header":"https://static001.geekbang.org/account/avatar/00/13/16/c8/980776fc.jpg","comment_is_top":false,"comment_ctime":1590761892,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1590761892","product_id":100020201,"comment_content":"李老师，两个表join，mapReduce实现，reduce阶段存在一个key的value集合非常大（同一key的较多且字段多）<br>这种情况reduce能全部读进内存吗","like_count":0},{"had_liked":false,"id":207876,"user_name":"胖子","can_delete":false,"product_type":"c1","uid":1037760,"ip_address":"","ucode":"E6371346288A87","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLIuRQaZX70dsBg6khub2VPM1eQAP9IWRWxgOFed3ia4kXyNJInFRicWJ0ibf2YmLsOvJa1sGygGpmJg/132","comment_is_top":false,"comment_ctime":1587201919,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1587201919","product_id":100020201,"comment_content":"老师，1、JobInProcess 树具体起什么作用？2、＂MapReduce 计算框架会启动 shuffle 过程，在 Map 任务进程调用一个 Partitioner 接口，＂从判断触发条件开始至结束的整个调用过程是怎样发生的？","like_count":0},{"had_liked":false,"id":193509,"user_name":"Anne","can_delete":false,"product_type":"c1","uid":1344945,"ip_address":"","ucode":"AC57F43BEDD65C","user_header":"https://static001.geekbang.org/account/avatar/00/14/85/b1/fe357d1a.jpg","comment_is_top":false,"comment_ctime":1584921976,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584921976","product_id":100020201,"comment_content":"为什么是采用将相同key的数据shuffle到同一个reduce来汇总？是否可以采用两两归并的方式，比如将2个map的结果先做一遍reduce，2个reduce的结果再做reduce，以此类推，直到最后的结果？","like_count":0},{"had_liked":false,"id":192575,"user_name":"杰哥长得帅","can_delete":false,"product_type":"c1","uid":1241993,"ip_address":"","ucode":"5A7FD1794F62D7","user_header":"https://static001.geekbang.org/account/avatar/00/12/f3/89/fcfecb46.jpg","comment_is_top":false,"comment_ctime":1584860876,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1584860876","product_id":100020201,"comment_content":"老师，能讲下shuffle过程中的两次排序么","like_count":0},{"had_liked":false,"id":170747,"user_name":"niuniu","can_delete":false,"product_type":"c1","uid":1254477,"ip_address":"","ucode":"F63047EB77448A","user_header":"https://static001.geekbang.org/account/avatar/00/13/24/4d/29a93491.jpg","comment_is_top":false,"comment_ctime":1578704708,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1578704708","product_id":100020201,"comment_content":"讲的真清楚！","like_count":0},{"had_liked":false,"id":101634,"user_name":"水电工٩(｡•ω•｡)﻿و","can_delete":false,"product_type":"c1","uid":1348364,"ip_address":"","ucode":"C43B6B4DDA9BE7","user_header":"https://static001.geekbang.org/account/avatar/00/14/93/0c/dd383681.jpg","comment_is_top":false,"comment_ctime":1559895846,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1559895846","product_id":100020201,"comment_content":"手机或者PC端将请求发送到主服务器上<br>主服务器根据请求内容分配给某一处理该任务请求的从服务器，从服务器进行响应，输出结果<br>结果发送回主服务器，再发送给手机或者PC端<br>（很简单也很直接的思考，不对之出请老师指出）","like_count":0},{"had_liked":false,"id":87320,"user_name":"Frank","can_delete":false,"product_type":"c1","uid":1111985,"ip_address":"","ucode":"9DADD72C193296","user_header":"https://static001.geekbang.org/account/avatar/00/10/f7/b1/982ea185.jpg","comment_is_top":false,"comment_ctime":1555567851,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1555567851","product_id":100020201,"comment_content":"MapReduce 编程相对说来是简单的，但是 MapReduce 框架要将一个相对简单的程序，在分布式的大规模服务器集群上并行执行起来却并不简单。理解 MapReduce 作业的启动和运行机制，理解 shuffle 过程的作用和实现原理，对你理解大数据的核心原理，做到真正意义上把握大数据、用好大数据作用巨大。","like_count":0},{"had_liked":false,"id":61962,"user_name":"不似旧日","can_delete":false,"product_type":"c1","uid":1161271,"ip_address":"","ucode":"DF4C5E3AB9570C","user_header":"https://static001.geekbang.org/account/avatar/00/11/b8/37/98991aeb.jpg","comment_is_top":false,"comment_ctime":1547867261,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547867261","product_id":100020201,"comment_content":"MapReduce使用了分而治之的编程思想","like_count":0},{"had_liked":false,"id":61646,"user_name":"eldon","can_delete":false,"product_type":"c1","uid":1368116,"ip_address":"","ucode":"3165FDE0FACB9A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLeKdLZTWmcxDE7AUnM90naTbDzynshqzILrQAweQXicGgvdg1gImWxeZabiay9LVLsnOCfjj2nZaBA/132","comment_is_top":false,"comment_ctime":1547745566,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1547745566","product_id":100020201,"comment_content":"研二自学hadoop spark没有大数据支撑 项目在哪找呢老师","like_count":0},{"had_liked":false,"id":57724,"user_name":"风之翼","can_delete":false,"product_type":"c1","uid":1218129,"ip_address":"","ucode":"FD524CE1530E9A","user_header":"https://static001.geekbang.org/account/avatar/00/12/96/51/dc51d2d4.jpg","comment_is_top":false,"comment_ctime":1546875538,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1546875538","product_id":100020201,"comment_content":"个人理解：MapReduce编程实际上就是对要执行的任务进行编制，通过map定义输入以及产生的中间数据，然后通过reduce定义输出数据。至于为什么一定要用MAP+REDUCE类似微服务的方式来分开执行，而不是用一个程序或其它方式完成整个IPO的过程，我想就是因为数据量过大，通过MAP完成分布式本地读取和预处理，能够最高效利用整个集群的磁盘读写能力，极大改善大规模分散数据读取效率，预处理之后的数据量大大降低；而后通过HTTP（吐槽一下，貌似RPC效率更高一些）将数据传给同样分散各地的reduce进程进行计算，这同样能够最高效利用整个集群的计算能力。而作为高效利用的代价，从预处理到reduce计算之间的网络传输的可靠性‘稳定性以及时效性 已经成为整个集群性能瓶颈，并必须为此设计出足够完善的map-reduce 数据调度机制（也就是shuffle）","like_count":0},{"had_liked":false,"id":54918,"user_name":"尼糯米","can_delete":false,"product_type":"c1","uid":1282819,"ip_address":"","ucode":"04D1B63F3801AE","user_header":"","comment_is_top":false,"comment_ctime":1545981642,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1545981642","product_id":100020201,"comment_content":"请问：MapReduce框架启动shuffle之后，会不会出现某个或某些Reduce 进程没有分到数据的情况","like_count":0},{"had_liked":false,"id":54832,"user_name":"改个名字试一试","can_delete":false,"product_type":"c1","uid":1125483,"ip_address":"","ucode":"7C4CF1183EE7B6","user_header":"https://static001.geekbang.org/account/avatar/00/11/2c/6b/33006efe.jpg","comment_is_top":false,"comment_ctime":1545967437,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1545967437","product_id":100020201,"comment_content":"一主多从，主服务器挂掉怎么搞？某台从服务器掉线怎么办？","like_count":0},{"had_liked":false,"id":45512,"user_name":"小老鼠","can_delete":false,"product_type":"c1","uid":1257460,"ip_address":"","ucode":"C663A0C863A515","user_header":"https://static001.geekbang.org/account/avatar/00/13/2f/f4/2dede51a.jpg","comment_is_top":false,"comment_ctime":1543657626,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1543657626","product_id":100020201,"comment_content":"作为一个软件测试工程师如何学习大数据？大数据产品应该如何测试？","like_count":0},{"had_liked":false,"id":41747,"user_name":"Alex","can_delete":false,"product_type":"c1","uid":1018695,"ip_address":"","ucode":"BC0876B7052CD0","user_header":"https://static001.geekbang.org/account/avatar/00/0f/8b/47/45f33f53.jpg","comment_is_top":false,"comment_ctime":1542848655,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1542848655","product_id":100020201,"comment_content":"1. 客户端（app或者browser里）接收到用户的操作；<br>2. 客户端以某种形式向服务器发送对应的请求（包含该请求对的地位数据，通常rest api或者graphql的方式）<br>3. 服务器程序（如serverless app）接收到请求后处理并返回一个response（对应请求的response数据）。这个过程可以是一个数据库操作等任何业务操作。<br>4. 客户端拿到数据以某种方式呈现给用户。","like_count":0},{"had_liked":false,"id":41743,"user_name":"sgl","can_delete":false,"product_type":"c1","uid":1252243,"ip_address":"","ucode":"D73875BA22954F","user_header":"https://static001.geekbang.org/account/avatar/00/13/1b/93/e3b44969.jpg","comment_is_top":false,"comment_ctime":1542848175,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1542848175","product_id":100020201,"comment_content":"内存当硬盘，用这是趋势","like_count":0},{"had_liked":false,"id":40133,"user_name":"Garwen","can_delete":false,"product_type":"c1","uid":1045062,"ip_address":"","ucode":"C76346E1734AB8","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f2/46/09c457eb.jpg","comment_is_top":false,"comment_ctime":1542516411,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1542516411","product_id":100020201,"comment_content":"用户从手机或PC发起请求。请求传递至后端服务器进行解析，如果访问的事web静态资源则在后端直接返回，如果请求较为复杂，则由web服务器传递给应用服务器，然后对数据库进行增删改查等操作，也可能经过复杂的算法，处理完毕后回复给web服务器，之后渲染页面返回给前端，与用户进行交互。","like_count":0},{"had_liked":false,"id":40022,"user_name":"Ahikaka","can_delete":false,"product_type":"c1","uid":1045146,"ip_address":"","ucode":"B3A678567E40A9","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/vhOPEib27xAuTycN0eQekLzsCe9zwcTTcrOb98cIfpgibgcweZBDN38tIicABibuZBwah9jnGVr02H2Zjuue1fLfEQ/132","comment_is_top":false,"comment_ctime":1542443658,"is_pvip":false,"replies":[{"id":"14435","content":"谢谢。","user_name":"作者回复","user_name_real":"李智慧","uid":"1007349","ctime":1542524941,"ip_address":"","comment_id":40022,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1542443658","product_id":100020201,"comment_content":"真的感觉好想继续看继续读，觉得更新好慢！写得很好，真希望能了解这样的知识多一些","like_count":0,"discussions":[{"author":{"id":1007349,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/5e/f5/018907ac.jpg","nickname":"李智慧","note":"","ucode":"8C9980C438AFD1","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":429334,"discussion_content":"谢谢。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1542524941,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":39279,"user_name":"拾掇拾掇","can_delete":false,"product_type":"c1","uid":1051873,"ip_address":"","ucode":"D775F374C2A1D3","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/e1/f663213e.jpg","comment_is_top":false,"comment_ctime":1542242982,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1542242982","product_id":100020201,"comment_content":"做爬虫的肯定知道这些流程，否则爬虫工程师就不合格了！","like_count":0},{"had_liked":false,"id":39242,"user_name":"伊森","can_delete":false,"product_type":"c1","uid":1142172,"ip_address":"","ucode":"270A665771F463","user_header":"https://static001.geekbang.org/account/avatar/00/11/6d/9c/beaf7642.jpg","comment_is_top":false,"comment_ctime":1542238621,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1542238621","product_id":100020201,"comment_content":"迫不及待，更新的有点慢啊","like_count":0}]}