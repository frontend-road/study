{"id":231501,"title":"23 | 缓存设计：缓存可以锦上添花也可以落井下石","content":"<p>你好，我是朱晔。今天，我从设计的角度，与你聊聊缓存。</p><p>通常我们会使用更快的介质（比如内存）作为缓存，来解决较慢介质（比如磁盘）读取数据慢的问题，缓存是用空间换时间，来解决性能问题的一种架构设计模式。更重要的是，磁盘上存储的往往是原始数据，而缓存中保存的可以是面向呈现的数据。这样一来，缓存不仅仅是加快了IO，还可以减少原始数据的计算工作。</p><p>此外，缓存系统一般设计简单，功能相对单一，所以诸如Redis这种缓存系统的整体吞吐量，能达到关系型数据库的几倍甚至几十倍，因此缓存特别适用于互联网应用的高并发场景。</p><p>使用Redis做缓存虽然简单好用，但使用和设计缓存并不是set一下这么简单，需要注意缓存的同步、雪崩、并发、穿透等问题。今天，我们就来详细聊聊。</p><h2>不要把Redis当作数据库</h2><p>通常，我们会使用Redis等分布式缓存数据库来缓存数据，但是<strong>千万别把Redis当做数据库来使用。</strong>我就见过许多案例，因为Redis中数据消失导致业务逻辑错误，并且因为没有保留原始数据，业务都无法恢复。</p><p>Redis的确具有数据持久化功能，可以实现服务重启后数据不丢失。这一点，很容易让我们误认为Redis可以作为高性能的KV数据库。</p><p>其实，从本质上来看，Redis（免费版）是一个内存数据库，所有数据保存在内存中，并且直接从内存读写数据响应操作，只不过具有数据持久化能力。所以，Redis的特点是，处理请求很快，但无法保存超过内存大小的数据。</p><!-- [[[read_end]]] --><blockquote>\n<p>备注：VM模式虽然可以保存超过内存大小的数据，但是因为性能原因从2.6开始已经被废弃。此外，Redis企业版提供了Redis on Flash可以实现Key+字典+热数据保存在内存中，冷数据保存在SSD中。</p>\n</blockquote><p>因此，把Redis用作缓存，我们需要注意两点。</p><p>第一，从客户端的角度来说，缓存数据的特点一定是有原始数据来源，且允许丢失，即使设置的缓存时间是1分钟，在30秒时缓存数据因为某种原因消失了，我们也要能接受。当数据丢失后，我们需要从原始数据重新加载数据，不能认为缓存系统是绝对可靠的，更不能认为缓存系统不会删除没有过期的数据。</p><p>第二，从Redis服务端的角度来说，缓存系统可以保存的数据量一定是小于原始数据的。首先，我们应该限制Redis对内存的使用量，也就是设置maxmemory参数；其次，我们应该根据数据特点，明确Redis应该以怎样的算法来驱逐数据。</p><p>从<a href=\"https://redis.io/topics/lru-cache\">Redis的文档</a>可以看到，常用的数据淘汰策略有：</p><ul>\n<li>allkeys-lru，针对所有Key，优先删除最近最少使用的Key；</li>\n<li>volatile-lru，针对带有过期时间的Key，优先删除最近最少使用的Key；</li>\n<li>volatile-ttl，针对带有过期时间的Key，优先删除即将过期的Key（根据TTL的值）；</li>\n<li>allkeys-lfu（Redis 4.0以上），针对所有Key，优先删除最少使用的Key；</li>\n<li>volatile-lfu（Redis 4.0以上），针对带有过期时间的Key，优先删除最少使用的Key。</li>\n</ul><p>其实，这些算法是Key范围+Key选择算法的搭配组合，其中范围有allkeys和volatile两种，算法有LRU、TTL和LFU三种。接下来，我就从Key范围和算法角度，和你说说如何选择合适的驱逐算法。</p><p>首先，从算法角度来说，Redis 4.0以后推出的LFU比LRU更“实用”。试想一下，如果一个Key访问频率是1天一次，但正好在1秒前刚访问过，那么LRU可能不会选择优先淘汰这个Key，反而可能会淘汰一个5秒访问一次但最近2秒没有访问过的Key，而LFU算法不会有这个问题。而TTL会比较“头脑简单”一点，优先删除即将过期的Key，但有可能这个Key正在被大量访问。</p><p>然后，从Key范围角度来说，allkeys可以确保即使Key没有TTL也能回收，如果使用的时候客户端总是“忘记”设置缓存的过期时间，那么可以考虑使用这个系列的算法。而volatile会更稳妥一些，万一客户端把Redis当做了长效缓存使用，只是启动时候初始化一次缓存，那么一旦删除了此类没有TTL的数据，可能就会导致客户端出错。</p><p>所以，不管是使用者还是管理者都要考虑Redis的使用方式，使用者需要考虑应该以缓存的姿势来使用Redis，管理者应该为Redis设置内存限制和合适的驱逐策略，避免出现OOM。</p><h2>注意缓存雪崩问题</h2><p>由于缓存系统的IOPS比数据库高很多，因此要特别小心短时间内大量缓存失效的情况。这种情况一旦发生，可能就会在瞬间有大量的数据需要回源到数据库查询，对数据库造成极大的压力，极限情况下甚至导致后端数据库直接崩溃。<strong>这就是我们常说的缓存失效，也叫作缓存雪崩</strong>。</p><p>从广义上说，产生缓存雪崩的原因有两种：</p><ul>\n<li>第一种是，缓存系统本身不可用，导致大量请求直接回源到数据库；</li>\n<li>第二种是，应用设计层面大量的Key在同一时间过期，导致大量的数据回源。</li>\n</ul><p>第一种原因，主要涉及缓存系统本身高可用的配置，不属于缓存设计层面的问题，所以今天我主要和你说说如何确保大量Key不在同一时间被动过期。</p><p>程序初始化的时候放入1000条城市数据到Redis缓存中，过期时间是30秒；数据过期后从数据库获取数据然后写入缓存，每次从数据库获取数据后计数器+1；在程序启动的同时，启动一个定时任务线程每隔一秒输出计数器的值，并把计数器归零。</p><p>压测一个随机查询某城市信息的接口，观察一下数据库的QPS：</p><pre><code>@Autowired\nprivate StringRedisTemplate stringRedisTemplate;\nprivate AtomicInteger atomicInteger = new AtomicInteger();\n\n@PostConstruct\npublic void wrongInit() {\n    //初始化1000个城市数据到Redis，所有缓存数据有效期30秒\n    IntStream.rangeClosed(1, 1000).forEach(i -&gt; stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, getCityFromDb(i), 30, TimeUnit.SECONDS));\n    log.info(&quot;Cache init finished&quot;);\n    \n    //每秒一次，输出数据库访问的QPS\n    \n   Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {\n        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));\n    }, 0, 1, TimeUnit.SECONDS);\n}\n\n@GetMapping(&quot;city&quot;)\npublic String city() {\n    //随机查询一个城市\n    int id = ThreadLocalRandom.current().nextInt(1000) + 1;\n    String key = &quot;city&quot; + id;\n    String data = stringRedisTemplate.opsForValue().get(key);\n    if (data == null) {\n        //回源到数据库查询\n        data = getCityFromDb(id);\n        if (!StringUtils.isEmpty(data))\n            //缓存30秒过期\n            stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS);\n    }\n    return data;\n}\n\nprivate String getCityFromDb(int cityId) {\n    //模拟查询数据库，查一次增加计数器加一\n    atomicInteger.incrementAndGet();\n    return &quot;citydata&quot; + System.currentTimeMillis();\n}\n</code></pre><p>使用wrk工具，设置10线程10连接压测city接口：</p><pre><code>wrk -c10 -t10 -d 100s http://localhost:45678/cacheinvalid/city\n</code></pre><p>启动程序30秒后缓存过期，回源的数据库QPS最高达到了700多：</p><p><img src=\"https://static001.geekbang.org/resource/image/91/6b/918a91e34725e475cdee746d5ba8aa6b.png?wh=2114*186\" alt=\"\"></p><p>解决缓存Key同时大规模失效需要回源，导致数据库压力激增问题的方式有两种。</p><p>方案一，差异化缓存过期时间，不要让大量的Key在同一时间过期。比如，在初始化缓存的时候，设置缓存的过期时间是30秒+10秒以内的随机延迟（扰动值）。这样，这些Key不会集中在30秒这个时刻过期，而是会分散在30~40秒之间过期：</p><pre><code>@PostConstruct\npublic void rightInit1() {\n    //这次缓存的过期时间是30秒+10秒内的随机延迟\n    IntStream.rangeClosed(1, 1000).forEach(i -&gt; stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, getCityFromDb(i), 30 + ThreadLocalRandom.current().nextInt(10), TimeUnit.SECONDS));\n    log.info(&quot;Cache init finished&quot;);\n    //同样1秒一次输出数据库QPS：\n   Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {\n        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));\n    }, 0, 1, TimeUnit.SECONDS);\n}\n</code></pre><p>修改后，缓存过期时的回源不会集中在同一秒，数据库的QPS从700多降到了最高100左右：</p><p><img src=\"https://static001.geekbang.org/resource/image/6f/35/6f4a666cf48c4d1373aead40afb57a35.png?wh=2124*644\" alt=\"\"></p><p>方案二，让缓存不主动过期。初始化缓存数据的时候设置缓存永不过期，然后启动一个后台线程30秒一次定时把所有数据更新到缓存，而且通过适当的休眠，控制从数据库更新数据的频率，降低数据库压力：</p><pre><code>@PostConstruct\npublic void rightInit2() throws InterruptedException {\n    CountDownLatch countDownLatch = new CountDownLatch(1);\n    //每隔30秒全量更新一次缓存 \n    Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {\n        IntStream.rangeClosed(1, 1000).forEach(i -&gt; {\n            String data = getCityFromDb(i);\n            //模拟更新缓存需要一定的时间\n            try {\n                TimeUnit.MILLISECONDS.sleep(20);\n            } catch (InterruptedException e) { }\n            if (!StringUtils.isEmpty(data)) {\n                //缓存永不过期，被动更新\n                stringRedisTemplate.opsForValue().set(&quot;city&quot; + i, data);\n            }\n        });\n        log.info(&quot;Cache update finished&quot;);\n        //启动程序的时候需要等待首次更新缓存完成\n        countDownLatch.countDown();\n    }, 0, 30, TimeUnit.SECONDS);\n\n    Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {\n        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));\n    }, 0, 1, TimeUnit.SECONDS);\n\n    countDownLatch.await();\n}\n</code></pre><p>这样修改后，虽然缓存整体更新的耗时在21秒左右，但数据库的压力会比较稳定：</p><p><img src=\"https://static001.geekbang.org/resource/image/5c/5a/5cb8bb1764998b57b63029bd5f69465a.png?wh=2292*1198\" alt=\"\"></p><p>关于这两种解决方案，<strong>我们需要特别注意以下三点</strong>：</p><ul>\n<li>方案一和方案二是截然不同的两种缓存方式，如果无法全量缓存所有数据，那么只能使用方案一；</li>\n<li>即使使用了方案二，缓存永不过期，同样需要在查询的时候，确保有回源的逻辑。正如之前所说，我们无法确保缓存系统中的数据永不丢失。</li>\n<li>不管是方案一还是方案二，在把数据从数据库加入缓存的时候，都需要判断来自数据库的数据是否合法，比如进行最基本的判空检查。</li>\n</ul><p>之前我就遇到过这样一个重大事故，某系统会在缓存中对基础数据进行长达半年的缓存，在某个时间点DBA把数据库中的原始数据进行了归档（可以认为是删除）操作。因为缓存中的数据一直在所以一开始没什么问题，但半年后的一天缓存中数据过期了，就从数据库中查询到了空数据加入缓存，爆发了大面积的事故。</p><p>这个案例说明，缓存会让我们更不容易发现原始数据的问题，所以在把数据加入缓存之前一定要校验数据，如果发现有明显异常要及时报警。</p><p>说到这里，我们再仔细看一下回源QPS超过700的截图，可以看到在并发情况下，总共1000条数据回源达到了1002次，说明有一些条目出现了并发回源。这，就是我后面要讲到的缓存并发问题。</p><h2>注意缓存击穿问题</h2><p>在某些Key属于极端热点数据，且并发量很大的情况下，如果这个Key过期，可能会在某个瞬间出现大量的并发请求同时回源，相当于大量的并发请求直接打到了数据库。<strong>这种情况，就是我们常说的缓存击穿或缓存并发问题</strong>。</p><p>我们来重现下这个问题。在程序启动的时候，初始化一个热点数据到Redis中，过期时间设置为5秒，每隔1秒输出一下回源的QPS：</p><pre><code>@PostConstruct\npublic void init() {\n    //初始化一个热点数据到Redis中，过期时间设置为5秒\n    stringRedisTemplate.opsForValue().set(&quot;hotsopt&quot;, getExpensiveData(), 5, TimeUnit.SECONDS);\n    //每隔1秒输出一下回源的QPS\n   \n   Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {\n        log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));\n    }, 0, 1, TimeUnit.SECONDS);\n}\n\n@GetMapping(&quot;wrong&quot;)\npublic String wrong() {\n    String data = stringRedisTemplate.opsForValue().get(&quot;hotsopt&quot;);\n    if (StringUtils.isEmpty(data)) {\n        data = getExpensiveData();\n        //重新加入缓存，过期时间还是5秒\n        stringRedisTemplate.opsForValue().set(&quot;hotsopt&quot;, data, 5, TimeUnit.SECONDS);\n    }\n    return data;\n}\n</code></pre><p>可以看到，每隔5秒数据库都有20左右的QPS：</p><p><img src=\"https://static001.geekbang.org/resource/image/09/99/096f2bb47939f9ca0e4bc865eb4da399.png?wh=2056*638\" alt=\"\"></p><p>如果回源操作特别昂贵，那么这种并发就不能忽略不计。这时，我们可以考虑使用锁机制来限制回源的并发。比如如下代码示例，使用Redisson来获取一个基于Redis的分布式锁，在查询数据库之前先尝试获取锁：</p><pre><code>@Autowired\nprivate RedissonClient redissonClient;\n@GetMapping(&quot;right&quot;)\npublic String right() {\n    String data = stringRedisTemplate.opsForValue().get(&quot;hotsopt&quot;);\n    if (StringUtils.isEmpty(data)) {\n        RLock locker = redissonClient.getLock(&quot;locker&quot;);\n        //获取分布式锁\n        if (locker.tryLock()) {\n            try {\n                data = stringRedisTemplate.opsForValue().get(&quot;hotsopt&quot;);\n                //双重检查，因为可能已经有一个B线程过了第一次判断，在等锁，然后A线程已经把数据写入了Redis中\n                if (StringUtils.isEmpty(data)) {\n                    //回源到数据库查询\n                    data = getExpensiveData();\n                    stringRedisTemplate.opsForValue().set(&quot;hotsopt&quot;, data, 5, TimeUnit.SECONDS);\n                }\n            } finally {\n                //别忘记释放，另外注意写法，获取锁后整段代码try+finally，确保unlock万无一失\n                locker.unlock();\n            }\n        }\n    }\n    return data;\n}\n</code></pre><p>这样，可以把回源到数据库的并发限制在1：</p><p><img src=\"https://static001.geekbang.org/resource/image/63/28/63ccde3fdf058b48431fc7c554fed828.png?wh=2018*700\" alt=\"\"></p><p>在真实的业务场景下，<strong>不一定</strong>要这么严格地使用双重检查分布式锁进行全局的并发限制，因为这样虽然可以把数据库回源并发降到最低，但也限制了缓存失效时的并发。可以考虑的方式是：</p><ul>\n<li>方案一，使用进程内的锁进行限制，这样每一个节点都可以以一个并发回源数据库；</li>\n<li>方案二，不使用锁进行限制，而是使用类似Semaphore的工具限制并发数，比如限制为10，这样既限制了回源并发数不至于太大，又能使得一定量的线程可以同时回源。</li>\n</ul><h2>注意缓存穿透问题</h2><p>在之前的例子中，缓存回源的逻辑都是当缓存中查不到需要的数据时，回源到数据库查询。这里容易出现的一个漏洞是，缓存中没有数据不一定代表数据没有缓存，还有一种可能是原始数据压根就不存在。</p><p>比如下面的例子。数据库中只保存有ID介于0（不含）和10000（包含）之间的用户，如果从数据库查询ID不在这个区间的用户，会得到空字符串，所以缓存中缓存的也是空字符串。如果使用ID=0去压接口的话，从缓存中查出了空字符串，认为是缓存中没有数据回源查询，其实相当于每次都回源：</p><pre><code>@GetMapping(&quot;wrong&quot;)\npublic String wrong(@RequestParam(&quot;id&quot;) int id) {\n    String key = &quot;user&quot; + id;\n    String data = stringRedisTemplate.opsForValue().get(key);\n    //无法区分是无效用户还是缓存失效\n    if (StringUtils.isEmpty(data)) {\n        data = getCityFromDb(id);\n        stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS);\n    }\n    return data;\n}\n\nprivate String getCityFromDb(int id) {\n    atomicInteger.incrementAndGet();\n    //注意，只有ID介于0（不含）和10000（包含）之间的用户才是有效用户，可以查询到用户信息\n    if (id &gt; 0 &amp;&amp; id &lt;= 10000) return &quot;userdata&quot;;\n    //否则返回空字符串\n    return &quot;&quot;;\n}\n</code></pre><p>压测后数据库的QPS达到了几千：</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/d2/dc2ee3259dd21d55a845dc4a8b9146d2.png?wh=2060*190\" alt=\"\"></p><p>如果这种漏洞被恶意利用的话，就会对数据库造成很大的性能压力。<strong>这就是缓存穿透</strong>。</p><p>这里需要注意，缓存穿透和缓存击穿的区别：</p><ul>\n<li>缓存穿透是指，缓存没有起到压力缓冲的作用；</li>\n<li>而缓存击穿是指，缓存失效时瞬时的并发打到数据库。</li>\n</ul><p>解决缓存穿透有以下两种方案。</p><p>方案一，对于不存在的数据，同样设置一个特殊的Value到缓存中，比如当数据库中查出的用户信息为空的时候，设置NODATA这样具有特殊含义的字符串到缓存中。这样下次请求缓存的时候还是可以命中缓存，即直接从缓存返回结果，不查询数据库：</p><pre><code>@GetMapping(&quot;right&quot;)\npublic String right(@RequestParam(&quot;id&quot;) int id) {\n    String key = &quot;user&quot; + id;\n    String data = stringRedisTemplate.opsForValue().get(key);\n    if (StringUtils.isEmpty(data)) {\n        data = getCityFromDb(id);\n        //校验从数据库返回的数据是否有效\n        if (!StringUtils.isEmpty(data)) {\n            stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS);\n        }\n        else {\n            //如果无效，直接在缓存中设置一个NODATA，这样下次查询时即使是无效用户还是可以命中缓存\n            stringRedisTemplate.opsForValue().set(key, &quot;NODATA&quot;, 30, TimeUnit.SECONDS);\n        }\n    }\n    return data;\n}\n</code></pre><p>但，这种方式可能会把大量无效的数据加入缓存中，如果担心大量无效数据占满缓存的话还可以考虑方案二，即使用布隆过滤器做前置过滤。</p><p>布隆过滤器是一种概率型数据库结构，由一个很长的二进制向量和一系列随机映射函数组成。它的原理是，当一个元素被加入集合时，通过k个散列函数将这个元素映射成一个m位bit数组中的k个点，并置为1。</p><p>检索时，我们只要看看这些点是不是都是1就（大概）知道集合中有没有它了。如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。</p><p>原理如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/c5/1f/c58cb0c65c37f4c1bf3aceba1c00d71f.png?wh=1966*1024\" alt=\"\"></p><p>布隆过滤器不保存原始值，空间效率很高，平均每一个元素占用2.4字节就可以达到万分之一的误判率。这里的误判率是指，过滤器判断值存在而实际并不存在的概率。我们可以设置布隆过滤器使用更大的存储空间，来得到更小的误判率。</p><p>你可以把所有可能的值保存在布隆过滤器中，从缓存读取数据前先过滤一次：</p><ul>\n<li>如果布隆过滤器认为值不存在，那么值一定是不存在的，无需查询缓存也无需查询数据库；</li>\n<li>对于极小概率的误判请求，才会最终让非法Key的请求走到缓存或数据库。</li>\n</ul><p>要用上布隆过滤器，我们可以使用Google的Guava工具包提供的BloomFilter类改造一下程序：启动时，初始化一个具有所有有效用户ID的、10000个元素的BloomFilter，在从缓存查询数据之前调用其mightContain方法，来检测用户ID是否可能存在；如果布隆过滤器说值不存在，那么一定是不存在的，直接返回：</p><pre><code>private BloomFilter&lt;Integer&gt; bloomFilter;\n\n@PostConstruct\npublic void init() {\n    //创建布隆过滤器，元素数量10000，期望误判率1%\n    bloomFilter = BloomFilter.create(Funnels.integerFunnel(), 10000, 0.01);\n    //填充布隆过滤器\n    IntStream.rangeClosed(1, 10000).forEach(bloomFilter::put);\n}\n\n@GetMapping(&quot;right2&quot;)\npublic String right2(@RequestParam(&quot;id&quot;) int id) {\n    String data = &quot;&quot;;\n    //通过布隆过滤器先判断\n    if (bloomFilter.mightContain(id)) {\n        String key = &quot;user&quot; + id;\n        //走缓存查询\n        data = stringRedisTemplate.opsForValue().get(key);\n        if (StringUtils.isEmpty(data)) {\n            //走数据库查询\n            data = getCityFromDb(id);\n            stringRedisTemplate.opsForValue().set(key, data, 30, TimeUnit.SECONDS);\n        }\n    }\n    return data;\n}\n</code></pre><p>对于方案二，我们需要同步所有可能存在的值并加入布隆过滤器，这是比较麻烦的地方。如果业务规则明确的话，你也可以考虑直接根据业务规则判断值是否存在。</p><p>其实，方案二可以和方案一同时使用，即将布隆过滤器前置，对于误判的情况再保存特殊值到缓存，双重保险避免无效数据查询请求打到数据库。</p><h2>注意缓存数据同步策略</h2><p>前面提到的3个案例，其实都属于缓存数据过期后的被动删除。在实际情况下，修改了原始数据后，考虑到缓存数据更新的及时性，我们可能会采用主动更新缓存的策略。这些策略可能是：</p><ul>\n<li>先更新缓存，再更新数据库；</li>\n<li>先更新数据库，再更新缓存；</li>\n<li>先删除缓存，再更新数据库，访问的时候按需加载数据到缓存；</li>\n<li>先更新数据库，再删除缓存，访问的时候按需加载数据到缓存。</li>\n</ul><p>那么，我们应该选择哪种更新策略呢？我来和你逐一分析下这4种策略：</p><p>“先更新缓存再更新数据库”策略不可行。数据库设计复杂，压力集中，数据库因为超时等原因更新操作失败的可能性较大，此外还会涉及事务，很可能因为数据库更新失败，导致缓存和数据库的数据不一致。</p><p>“先更新数据库再更新缓存”策略不可行。一是，如果线程A和B先后完成数据库更新，但更新缓存时却是B和A的顺序，那很可能会把旧数据更新到缓存中引起数据不一致；二是，我们不确定缓存中的数据是否会被访问，不一定要把所有数据都更新到缓存中去。</p><p>“先删除缓存再更新数据库，访问的时候按需加载数据到缓存”策略也不可行。在并发的情况下，很可能删除缓存后还没来得及更新数据库，就有另一个线程先读取了旧值到缓存中，如果并发量很大的话这个概率也会很大。</p><p><strong>“先更新数据库再删除缓存，访问的时候按需加载数据到缓存”策略是最好的</strong>。虽然在极端情况下，这种策略也可能出现数据不一致的问题，但概率非常低，基本可以忽略。举一个“极端情况”的例子，比如更新数据的时间节点恰好是缓存失效的瞬间，这时A先读取到了旧值，随后在B操作数据库完成更新并且删除了缓存之后，A再把旧值加入缓存。</p><p>需要注意的是，更新数据库后删除缓存的操作可能失败，如果失败则考虑把任务加入延迟队列进行延迟重试，确保数据可以删除，缓存可以及时更新。因为删除操作是幂等的，所以即使重复删问题也不是太大，这又是删除比更新好的一个原因。</p><p>因此，针对缓存更新更推荐的方式是，缓存中的数据不由数据更新操作主动触发，统一在需要使用的时候按需加载，数据更新后及时删除缓存中的数据即可。</p><h2>重点回顾</h2><p>今天，我主要是从设计的角度，和你分享了数据缓存的三大问题。</p><p>第一，我们不能把诸如Redis的缓存数据库完全当作数据库来使用。我们不能假设缓存始终可靠，也不能假设没有过期的数据必然可以被读取到，需要处理好缓存的回源逻辑；而且要显式设置Redis的最大内存使用和数据淘汰策略，避免出现OOM的问题。</p><p>第二，缓存的性能比数据库好很多，我们需要考虑大量请求绕过缓存直击数据库造成数据库瘫痪的各种情况。对于缓存瞬时大面积失效的缓存雪崩问题，可以通过差异化缓存过期时间解决；对于高并发的缓存Key回源问题，可以使用锁来限制回源并发数；对于不存在的数据穿透缓存的问题，可以通过布隆过滤器进行数据存在性的预判，或在缓存中也设置一个值来解决。</p><p>第三，当数据库中的数据有更新的时候，需要考虑如何确保缓存中数据的一致性。我们看到，“先更新数据库再删除缓存，访问的时候按需加载数据到缓存”的策略是最为妥当的，并且要尽量设置合适的缓存过期时间，这样即便真的发生不一致，也可以在缓存过期后数据得到及时同步。</p><p>最后，我要提醒你的是，在使用缓存系统的时候，要监控缓存系统的内存使用量、命中率、对象平均过期时间等重要指标，以便评估系统的有效性，并及时发现问题。</p><p>今天用到的代码，我都放在了GitHub上，你可以点击<a href=\"https://github.com/JosephZhu1983/java-common-mistakes\">这个链接</a>查看。</p><h2>思考与讨论</h2><ol>\n<li>在聊到缓存并发问题时，我们说到热点Key回源会对数据库产生的压力问题，如果Key特别热的话，可能缓存系统也无法承受，毕竟所有的访问都集中打到了一台缓存服务器。如果我们使用Redis来做缓存，那可以把一个热点Key的缓存查询压力，分散到多个Redis节点上吗？</li>\n<li>大Key也是数据缓存容易出现的一个问题。如果一个Key的Value特别大，那么可能会对Redis产生巨大的性能影响，因为Redis是单线程模型，对大Key进行查询或删除等操作，可能会引起Redis阻塞甚至是高可用切换。你知道怎么查询Redis中的大Key，以及如何在设计上实现大Key的拆分吗？</li>\n</ol><p>关于缓存设计，你还遇到过哪些坑呢？我是朱晔，欢迎在评论区与我留言分享你的想法，也欢迎你把今天的内容分享给你的朋友或同事，一起交流。</p>","comments":[{"had_liked":false,"id":214944,"user_name":"Darren","can_delete":false,"product_type":"c1","uid":1254968,"ip_address":"","ucode":"CCD2B2C492BE9A","user_header":"https://static001.geekbang.org/account/avatar/00/13/26/38/ef063dc2.jpg","comment_is_top":false,"comment_ctime":1588854265,"is_pvip":true,"replies":[{"id":"79578","content":"厉害","user_name":"作者回复","comment_id":214944,"uid":"1001470","ip_address":"","utype":1,"ctime":1588854815,"user_name_real":"朱晔"}],"discussion_count":12,"race_medal":0,"score":"379545976313","product_id":100047701,"comment_content":"第一个问题：<br>\tvivi童鞋回复的很棒，我的第一想法也是加随机后缀。<br>\t分型一个场景：假如在一个非常热点的数据，数据更新不是很频繁，但是查询非常的频繁，要保证基本保证100%的缓存命中率，该怎么处理？<br>\t\t\t   我们的做法是，空间换效率，同一个key保留2份，1个不带后缀，1个带后缀，不带的后缀的有ttl，带后缀的没有，先查询不带后缀的，查询不到，做两件事情：1、后台程序查询DB更新缓存；2查询带后缀返回给调用方。这样可以尽可能的避免缓存击穿而引起的数据库挂了。<br>第二个问题：<br>1：单个key存储的value很大<br>\tkey分为2种类型：<br> \t第一：该key需要每次都整存整取<br>\t\t可以尝试将对象分拆成几个key-value， 使用multiGet获取值，这样分拆的意义在于分拆单次操作的压力，将操作压力平摊到多个redis实例中，降低对单个redis的IO影响；    <br> \t第二：该对象每次只需要存取部分数据<br>\t\t可以像第一种做法一样，分拆成几个key-value，  也可以将这个存储在一个hash中，每个field代表一个具体的属性，使用hget,hmget来获取部分的value，使用hset，hmset来更新部分属性。<br>2、一个集群存储了上亿的key<br>\t如果key的个数过多会带来更多的内存空间占用，<br>\t\t第一：key本身的占用（每个key 都会有一个Category前缀）<br>      \t第二：集群模式中，服务端需要建立一些slot2key的映射关系，这其中的指针占用在key多的情况下也是浪费巨大空间<br>\t\t这两个方面在key个数上亿的时候消耗内存十分明显（Redis 3.2及以下版本均存在这个问题，4.0有优化）；<br>\t\t所以减少key的个数可以减少内存消耗，可以参考的方案是转Hash结构存储，即原先是直接使用Redis String 的结构存储，现在将多个key存储在一个Hash结构中，具体场景参考如下：<br>       一： key 本身就有很强的相关性，比如多个key 代表一个对象，每个key是对象的一个属性，这种可直接按照特定对象的特征来设置一个新Key——Hash结构， 原先的key则作为这个新Hash 的field。<br>\t   二： key 本身没有相关性，预估一下总量，预分一个固定的桶数量<br>\t\t比如现在预估key 的总数为 2亿，按照一个hash存储 100个field来算，需要 2亿 &#47;  100  = 200W 个桶 (200W 个key占用的空间很少，2亿可能有将近 20G )<br>\t\t现在按照200W 固定桶分就是先计算出桶的序号 hash(123456789)   % 200W ， 这里最好保证这个 hash算法的值是个正数，否则需要调整下模除的规则；<br>\t\t这样算出三个key 的桶分别是     1 ， 2， 2。   所以存储的时候调用API    hset(key,  field, value)，读取的时候使用  hget （key， field）   <br>\t\t注意两个地方：1，hash 取模对负数的处理；  2，预分桶的时候， 一个hash 中存储的值最好不要超过 512 ，100 左右较为合适","like_count":89,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494250,"discussion_content":"厉害","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588854815,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":311139,"discussion_content":"如果预先知道了热点key，对这些热点key使用guava cache或者ehcache做下本地缓存感觉会更好一些。访问热点key直接查询本地缓存，然后直接返回，不再去查询分布式缓存。\n当对这个热点key进行更新或者删除操作时，可以通过广播的形式（比如都订阅redis的某一个频道）通知其他节点更新或删除本地缓存。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1602234497,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1254968,"avatar":"https://static001.geekbang.org/account/avatar/00/13/26/38/ef063dc2.jpg","nickname":"Darren","note":"","ucode":"CCD2B2C492BE9A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":311307,"discussion_content":"本地缓存caffeine更好","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1602302561,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":311139,"ip_address":""},"score":311307,"extra":""}]},{"author":{"id":1112490,"avatar":"https://static001.geekbang.org/account/avatar/00/10/f9/aa/3e80212e.jpg","nickname":"龙猫","note":"","ucode":"FD726CC969EF9C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":288017,"discussion_content":"这是高手为你加油！！！！！！\n　☆  *　.  　☆\n　　. ∧＿∧　∩　* ☆\n*  ☆ ( ・∀・)/ .\n　.  ⊂　　 ノ* ☆\n☆ * (つ ノ  .☆\n　　 (ノ\r\n","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1593613268,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2378309,"avatar":"https://static001.geekbang.org/account/avatar/00/24/4a/45/e048531a.jpg","nickname":"迪米乌格斯","note":"","ucode":"1D4393746DCBCE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372901,"discussion_content":"Darren 老哥对于redis的第二段分析是真的厉害...","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620495326,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1325997,"avatar":"https://static001.geekbang.org/account/avatar/00/14/3b/ad/31193b83.jpg","nickname":"孙志强","note":"","ucode":"9C070F1E4EC6FF","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":346879,"discussion_content":"第一个问题，可以多存几个key，保证高并发的读请求分散到不同的redis实例上。如果不能把数据打散到不同的redis实例上，扩展性是有限制的，单实例redis的qps也是有上限的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1612090649,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2152306,"avatar":"https://static001.geekbang.org/account/avatar/00/20/d7/72/cbef720d.jpg","nickname":"鲁鸣","note":"","ucode":"974BA3C3E64630","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":308656,"discussion_content":"后缀是固定的吗？ 不是随机的？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1601021475,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1651085,"avatar":"https://static001.geekbang.org/account/avatar/00/19/31/8d/d6367617.jpg","nickname":"小钳子","note":"","ucode":"E60214AB3E7E99","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":299956,"discussion_content":"请教一下。第一个问题。后台程序查询db更新缓存指的是缓存失效的listener触发的吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1597888462,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1128491,"avatar":"https://static001.geekbang.org/account/avatar/00/11/38/2b/9db9406b.jpg","nickname":"星夜","note":"","ucode":"3B46C09D994CEB","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":294515,"discussion_content":"每一次都会先查不带后缀的，那么不带后缀的不算是热点key吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595913043,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1254968,"avatar":"https://static001.geekbang.org/account/avatar/00/13/26/38/ef063dc2.jpg","nickname":"Darren","note":"","ucode":"CCD2B2C492BE9A","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1128491,"avatar":"https://static001.geekbang.org/account/avatar/00/11/38/2b/9db9406b.jpg","nickname":"星夜","note":"","ucode":"3B46C09D994CEB","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":294604,"discussion_content":"是很多key，不是一个key","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1595940420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":294515,"ip_address":""},"score":294604,"extra":""}]},{"author":{"id":1114093,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ff/ed/b2fc0e7c.jpg","nickname":"7","note":"","ucode":"10A6E57A027D42","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":287844,"discussion_content":"疑问\n第一个问题：既然数据都是不频繁更新的，大量查询，为什么要设计两个key，只用一个不行吗？同时在更新的时候，采用版本控制，大于它的就不更新了；亦或采用单线程更新数据库和缓存\n\n第二个问题：加随机前后缀的前提是你已经知道这个key是热点key了。但如果刚开始无法预知，这种又怎么处理呢？\n我的想法是：借助监控和日志实时分析，如果是黑客攻击，可以移交给安全部门；如果是用户访问，结合压测和机器负载情况，如果评估下来可能会导致机器故障，那就紧急修改代码加随机前后缀并上线，如果评估后机器可以承受，则不变动。\n\n如果已经预先知道某个key可能会成为热点key，除了随机前后缀，还有就是可以采用多个集群，根据用户地理位置访问不同机房的数据也可以","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1593567505,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2152306,"avatar":"https://static001.geekbang.org/account/avatar/00/20/d7/72/cbef720d.jpg","nickname":"鲁鸣","note":"","ucode":"974BA3C3E64630","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1114093,"avatar":"https://static001.geekbang.org/account/avatar/00/10/ff/ed/b2fc0e7c.jpg","nickname":"7","note":"","ucode":"10A6E57A027D42","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":308657,"discussion_content":"是不是这里的版本和楼主说的后缀有点类似，也是相当于存储了多个数据快照","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1601021578,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":287844,"ip_address":""},"score":308657,"extra":""}]}]},{"had_liked":false,"id":215122,"user_name":"汝林外史","can_delete":false,"product_type":"c1","uid":1188906,"ip_address":"","ucode":"3C66C0F0537A99","user_header":"https://static001.geekbang.org/account/avatar/00/12/24/2a/33441e2b.jpg","comment_is_top":false,"comment_ctime":1588908423,"is_pvip":false,"replies":[{"id":"79605","content":"我们的目标是避免长期出现不一致（读取到了旧值进入缓存属于长期不一致，因为又需要等一个缓存周期了）。先更新数据库再删缓存，如果并发查询发生在删缓存之前更新数据库之后，查到的不都是旧数据吗？是旧数据但是这是非常短暂的，下次查询就是新数据了。<br><br>你说的如何实现绝度一致。先删除缓存，向队列中插入一个数据的修改标识，我们如何确保删除缓存后向队列插入数据修改标识之前又有请求过来读取数据了呢？绝对一致或许考虑锁的方案。<br><br>不过反过来思考，既然已经缓存了，真的需要绝对一致吗？","user_name":"作者回复","comment_id":215122,"uid":"1001470","ip_address":"","utype":1,"ctime":1588912507,"user_name_real":"朱晔"}],"discussion_count":5,"race_medal":0,"score":"91783221639","product_id":100047701,"comment_content":"先更新数据库再删缓存，如果并发查询发生在删缓存之前更新数据库之后，查到的不都是旧数据吗？<br>不是应该先删除缓存，向队列中插入一个数据的修改标识，并发查询发现缓存为空把查询数据库的标识也放入队列中，等修改的处理完了再处理查询的请求。","like_count":22,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494323,"discussion_content":"我们的目标是避免长期出现不一致（读取到了旧值进入缓存属于长期不一致，因为又需要等一个缓存周期了）。先更新数据库再删缓存，如果并发查询发生在删缓存之前更新数据库之后，查到的不都是旧数据吗？是旧数据但是这是非常短暂的，下次查询就是新数据了。\n\n你说的如何实现绝度一致。先删除缓存，向队列中插入一个数据的修改标识，我们如何确保删除缓存后向队列插入数据修改标识之前又有请求过来读取数据了呢？绝对一致或许考虑锁的方案。\n\n不过反过来思考，既然已经缓存了，真的需要绝对一致吗？","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1588912507,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1975581,"avatar":"http://thirdwx.qlogo.cn/mmopen/2kpMNDYsSfCnhAYmvAsNcYKyNLUJG5iax9BKgXAJzxypEJGRVwhibjZ53S5icgEDibn8uqrqwsKcxzEXfRRUXTdqjA/132","nickname":"岳宜波","note":"","ucode":"5043CA250FDCB1","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":277183,"discussion_content":"之前看到一个解决方案是给数据加上时间戳，修改数据时，不删除缓存而是更新缓存，更新缓存策略是，新的时间戳可以覆盖旧的时间戳，旧的不能覆盖新的","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1591013420,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1613123,"avatar":"https://static001.geekbang.org/account/avatar/00/18/9d/43/bf628743.jpg","nickname":"Cyber","note":"","ucode":"A7842494F57D0C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":382352,"discussion_content":"延迟双删","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1625541473,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2378309,"avatar":"https://static001.geekbang.org/account/avatar/00/24/4a/45/e048531a.jpg","nickname":"迪米乌格斯","note":"","ucode":"1D4393746DCBCE","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":372902,"discussion_content":"对于这个时间段之内的数据不一致我感觉是可以接受的, 就像mysql的主从延迟一样,重试一下就可以了.","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620495466,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1488356,"avatar":"https://static001.geekbang.org/account/avatar/00/16/b5/e4/e6faf686.jpg","nickname":"握了个大蚂蚱","note":"","ucode":"AD34AD4FA37371","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":276417,"discussion_content":"况且这时候更新的请求还没返回给发起请求的客户端，其他的客户端应该感知到的就是老数据","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1590858098,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":214723,"user_name":"赵宇浩","can_delete":false,"product_type":"c1","uid":1102850,"ip_address":"","ucode":"571C1B03858F67","user_header":"https://static001.geekbang.org/account/avatar/00/10/d4/02/ad82f59a.jpg","comment_is_top":false,"comment_ctime":1588809805,"is_pvip":false,"replies":[{"id":"79519","content":"测试一下 https:&#47;&#47;krisives.github.io&#47;bloom-calculator&#47;<br>10亿的数据量，期望千分之一的错误率，推荐10个Hash函数，占用内存空间不到1.8GB","user_name":"作者回复","comment_id":214723,"uid":"1001470","ip_address":"","utype":1,"ctime":1588822287,"user_name_real":"朱晔"}],"discussion_count":5,"race_medal":0,"score":"74603253837","product_id":100047701,"comment_content":"跟用户信息相关的缓存怎么处理穿透，因为用户的量很大，很难全放进布隆过滤器。","like_count":18,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494180,"discussion_content":"测试一下 https://krisives.github.io/bloom-calculator/\n10亿的数据量，期望千分之一的错误率，推荐10个Hash函数，占用内存空间不到1.8GB","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588822287,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2014573,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/bd/6d/7010f98e.jpg","nickname":"SharpBB","note":"","ucode":"D30C5B798B8E8C","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":560231,"discussion_content":"这个hash函数是在哪加的呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649234280,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":494180,"ip_address":""},"score":560231,"extra":""}]},{"author":{"id":1351066,"avatar":"https://static001.geekbang.org/account/avatar/00/14/9d/9a/7f064a9f.jpg","nickname":"龙行秀","note":"","ucode":"2DA088D199EA9D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":297333,"discussion_content":"老师你好，布隆过滤器里面存的应该是key值吧，那如果单个比较的key对象比较大呢？","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1596877376,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1217874,"avatar":"https://static001.geekbang.org/account/avatar/00/12/95/52/ad190682.jpg","nickname":"Mr wind","note":"","ucode":"484F02D1962239","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1351066,"avatar":"https://static001.geekbang.org/account/avatar/00/14/9d/9a/7f064a9f.jpg","nickname":"龙行秀","note":"","ucode":"2DA088D199EA9D","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":315356,"discussion_content":"会对对象进行hash，取决于hash算法。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1603266227,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":297333,"ip_address":""},"score":315356,"extra":""},{"author":{"id":1023101,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/9c/7d/774e07f9.jpg","nickname":"study的程序员","note":"","ucode":"E5AE9037D24429","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1351066,"avatar":"https://static001.geekbang.org/account/avatar/00/14/9d/9a/7f064a9f.jpg","nickname":"龙行秀","note":"","ucode":"2DA088D199EA9D","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":536008,"discussion_content":"布隆过滤器是一个byte数组，不是存的key，总空间取决于key数量、错误率","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638629651,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":297333,"ip_address":""},"score":536008,"extra":""}]}]},{"had_liked":false,"id":214744,"user_name":"vivi","can_delete":false,"product_type":"c1","uid":1632728,"ip_address":"","ucode":"1A02D8396F538B","user_header":"https://static001.geekbang.org/account/avatar/00/18/e9/d8/026493cc.jpg","comment_is_top":false,"comment_ctime":1588813270,"is_pvip":true,"replies":[{"id":"79518","content":"是的，加随机前缀后缀是一个办法","user_name":"作者回复","comment_id":214744,"uid":"1001470","ip_address":"","utype":1,"ctime":1588821202,"user_name_real":"朱晔"}],"discussion_count":2,"race_medal":0,"score":"44538486230","product_id":100047701,"comment_content":"第一个问题，我认为可以给hotkey加上后缀，让这些hotkey打散到不同的redis实例上。","like_count":10,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494191,"discussion_content":"是的，加随机前缀后缀是一个办法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588821202,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2030378,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/fb/2a/2822e261.jpg","nickname":"Geek_270ad1","note":"","ucode":"9861E5DA644631","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":290130,"discussion_content":"你好 想请教下hotkey加厚最后是否需要多写？如果多写怎么保证多实例内容的一致性？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594352017,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215547,"user_name":"wang","can_delete":false,"product_type":"c1","uid":1161200,"ip_address":"","ucode":"5D569805C6A923","user_header":"https://static001.geekbang.org/account/avatar/00/11/b7/f0/a570f68a.jpg","comment_is_top":false,"comment_ctime":1589014554,"is_pvip":false,"replies":[{"id":"79795","content":"可以考虑一下Event Sourcing的思想，所有数据都从缓存读取，修改直接记录AppendLog+刷缓存，异步Apply到数据库，如果丟数据重放日志即可","user_name":"作者回复","comment_id":215547,"uid":"1001470","ip_address":"","utype":1,"ctime":1589017778,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"27358818330","product_id":100047701,"comment_content":"对于一个频繁更新的热点key，有什么好的方案，先更新redis在定时同步到数据库，可能会丢数据","like_count":6,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494471,"discussion_content":"可以考虑一下Event Sourcing的思想，所有数据都从缓存读取，修改直接记录AppendLog+刷缓存，异步Apply到数据库，如果丟数据重放日志即可","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589017778,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":229334,"user_name":"Michael","can_delete":false,"product_type":"c1","uid":1274787,"ip_address":"","ucode":"C233DF1D224EC1","user_header":"https://static001.geekbang.org/account/avatar/00/13/73/a3/2b077607.jpg","comment_is_top":false,"comment_ctime":1592967553,"is_pvip":true,"replies":[{"id":"84635","content":"我不是很理解你的问题，你是说咨询修改缓存怎么办？修改后可以直接把全量分页缓存的数据全部删除，也可以全量更新数据到缓存，当然，如果修改不涉及分页的改动，比如修改单篇文章的标题更新或删除那一页的缓存即可","user_name":"作者回复","comment_id":229334,"uid":"1001470","ip_address":"","utype":1,"ctime":1592970985,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"14477869441","product_id":100047701,"comment_content":"老师，你好！ <br>目前我们项目中遇到的问题：<br>当资讯在后台管理台修改完成审核上架后，需要在客户端实时生效，但是客户端的显示涉及分页和详情查询，分页查询中的缓存key涉及分页参数pageNo，pageSIze,分页和详情缓存key还涉及用户参数，这种情况情况如何处理？","like_count":3,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":499445,"discussion_content":"我不是很理解你的问题，你是说咨询修改缓存怎么办？修改后可以直接把全量分页缓存的数据全部删除，也可以全量更新数据到缓存，当然，如果修改不涉及分页的改动，比如修改单篇文章的标题更新或删除那一页的缓存即可","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592970985,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215917,"user_name":"hellojd","can_delete":false,"product_type":"c1","uid":1341697,"ip_address":"","ucode":"4E51807AE7E472","user_header":"https://static001.geekbang.org/account/avatar/00/14/79/01/e71510dc.jpg","comment_is_top":false,"comment_ctime":1589151223,"is_pvip":false,"replies":[{"id":"79889","content":"布隆过滤器容量预估可以参考我之前的回答，如果数据保存节点内可以定时任务来加载新增数据，不同节点数据略有不同问题不大，或者也可以为redis安装布隆模块，集中保存","user_name":"作者回复","comment_id":215917,"uid":"1001470","ip_address":"","utype":1,"ctime":1589158601,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"14474053111","product_id":100047701,"comment_content":"文稿中提到的布隆过滤器，怎么保证加载用户全部数据。用户量太大会不会oom?,如果新用户注册过来，怎么同步更新所有实例的布隆过滤器数据","like_count":4,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494612,"discussion_content":"布隆过滤器容量预估可以参考我之前的回答，如果数据保存节点内可以定时任务来加载新增数据，不同节点数据略有不同问题不大，或者也可以为redis安装布隆模块，集中保存","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589158601,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215916,"user_name":"hellojd","can_delete":false,"product_type":"c1","uid":1341697,"ip_address":"","ucode":"4E51807AE7E472","user_header":"https://static001.geekbang.org/account/avatar/00/14/79/01/e71510dc.jpg","comment_is_top":false,"comment_ctime":1589150783,"is_pvip":false,"replies":[{"id":"79884","content":"mongodb是数据库，会员状态放缓存显然不合适","user_name":"作者回复","comment_id":215916,"uid":"1001470","ip_address":"","utype":1,"ctime":1589156035,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"14474052671","product_id":100047701,"comment_content":"Mongodb算缓存吗？还是数据库？我们现正在开发的一个功能，将用户会员状态放到缓存中，缓存的过期时间是会员的过期时间，如果缓存没到缓存过期时间就被驱除,那就死翘翘了。","like_count":4,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494611,"discussion_content":"mongodb是数据库，会员状态放缓存显然不合适","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1589156035,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215142,"user_name":"那时刻","can_delete":false,"product_type":"c1","uid":1150927,"ip_address":"","ucode":"B0D150856C3A4A","user_header":"https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg","comment_is_top":false,"comment_ctime":1588914927,"is_pvip":false,"replies":[{"id":"79607","content":"https:&#47;&#47;www.infoq.cn&#47;article&#47;3L3zAQ4H8xpNoM2glSyi","user_name":"作者回复","comment_id":215142,"uid":"1001470","ip_address":"","utype":1,"ctime":1588915442,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":1,"score":"14473816815","product_id":100047701,"comment_content":"关于redis热key的处理，vivi和Darren给出很好的方案。如果hot key数据量不大的话，服务器本地localcache也是个方法吧？另外想问老师，在工程上如何发现热key呢？使用redis4提供的object freq？不知是否有其他方式？","like_count":3,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494330,"discussion_content":"https://www.infoq.cn/article/3L3zAQ4H8xpNoM2glSyi","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588915442,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":247663,"user_name":"百威","can_delete":false,"product_type":"c1","uid":1074843,"ip_address":"","ucode":"758199FDD0B44F","user_header":"https://static001.geekbang.org/account/avatar/00/10/66/9b/59776420.jpg","comment_is_top":false,"comment_ctime":1599788908,"is_pvip":false,"replies":[{"id":"90941","content":"作为使用方，提供方做了缓存你也不知道，这是它内部实现逻辑，如果你觉得它的响应不够快，那么根据自己的策略再做一次缓存","user_name":"作者回复","comment_id":247663,"uid":"1001470","ip_address":"","utype":1,"ctime":1599796547,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"10189723500","product_id":100047701,"comment_content":"问个问题，在微服务中，缓存是由数据提供方做，还是使用方做，还是都可以？我都是以提供方做为原则，不缓存别人数据，也方便排查问题，但是实际项目总会因为数据侧的缓存不合理而不得不自己再加一层","like_count":2,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":505464,"discussion_content":"作为使用方，提供方做了缓存你也不知道，这是它内部实现逻辑，如果你觉得它的响应不够快，那么根据自己的策略再做一次缓存","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1599796547,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":281089,"user_name":"码哥字节","can_delete":false,"product_type":"c1","uid":1572356,"ip_address":"","ucode":"362103AD52C8E0","user_header":"https://static001.geekbang.org/account/avatar/00/17/fe/04/bb427e47.jpg","comment_is_top":false,"comment_ctime":1614580705,"is_pvip":false,"replies":[{"id":"102080","content":"ok","user_name":"作者回复","comment_id":281089,"uid":"1001470","ip_address":"","utype":1,"ctime":1614605765,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"5909548001","product_id":100047701,"comment_content":"还有一种 “延迟双删” 的说法，也就是先删除缓存，再写数据库，延迟 一下再次删除缓存","like_count":1,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":516292,"discussion_content":"ok","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1614605765,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":259538,"user_name":"水浴清风","can_delete":false,"product_type":"c1","uid":1133926,"ip_address":"","ucode":"36EF5179E6F952","user_header":"https://static001.geekbang.org/account/avatar/00/11/4d/66/28742ad4.jpg","comment_is_top":false,"comment_ctime":1604748120,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5899715416","product_id":100047701,"comment_content":"第一个问题：<br> 考虑通过两层来完成redis热点key的<br> 1层：通过监控来发现热点key（即key的QPS达到热点访问量一半时），对热点key做复制到集群的其他redis上，然后动态修改redis的客户端配置分发（这个方案感觉比较复杂，简单点方案：监控到redis的存在热点key趋势时，通知相关实例作本地缓存该key，通知实例获取该热点key，从本地获取）。<br><br>2层：增加二级本地实例缓存，在一级集中式缓存被击穿情况下，二级作为防御手段。<br><br>第2问题：<br>可以通过redis-cli --bigkeys查找，但效果不是很好，有一个go写的通过分析rdb文件查找的工具","like_count":1},{"had_liked":false,"id":247599,"user_name":"FuriousEric","can_delete":false,"product_type":"c1","uid":1138576,"ip_address":"","ucode":"0A66DA938976F7","user_header":"https://static001.geekbang.org/account/avatar/00/11/5f/90/711efc88.jpg","comment_is_top":false,"comment_ctime":1599753076,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5894720372","product_id":100047701,"comment_content":"用实际例子计算回源qps那段代码，以及wrk性能测试，真是精彩啊！感谢作者","like_count":2},{"had_liked":false,"id":240364,"user_name":"龙行秀","can_delete":false,"product_type":"c1","uid":1351066,"ip_address":"","ucode":"2DA088D199EA9D","user_header":"https://static001.geekbang.org/account/avatar/00/14/9d/9a/7f064a9f.jpg","comment_is_top":false,"comment_ctime":1596877659,"is_pvip":false,"replies":[{"id":"88743","content":"这个没关系 关键是所有数据要先加进去","user_name":"作者回复","comment_id":240364,"uid":"1001470","ip_address":"","utype":1,"ctime":1596888161,"user_name_real":"朱晔"}],"discussion_count":2,"race_medal":0,"score":"5891844955","product_id":100047701,"comment_content":"老师，你好，关于使用布隆过滤器，如果缓存查询的键值没有规律，比如使用随机生成的UUID，那是不是就不适合了。1","like_count":2,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":503417,"discussion_content":"这个没关系 关键是所有数据要先加进去","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1596888161,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1351066,"avatar":"https://static001.geekbang.org/account/avatar/00/14/9d/9a/7f064a9f.jpg","nickname":"龙行秀","note":"","ucode":"2DA088D199EA9D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":297621,"discussion_content":"好的，另外再问下老师，现实使用中，布隆过滤器中数据预热后，如果有新增或者删除通常如何处理呢？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1596987481,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":235362,"user_name":"mickey","can_delete":false,"product_type":"c1","uid":1051663,"ip_address":"","ucode":"8B490C2DDE4010","user_header":"https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg","comment_is_top":false,"comment_ctime":1594982552,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5889949848","product_id":100047701,"comment_content":"请问老师，“先更新数据库再删除缓存，访问的时候按需加载数据到缓存”策略中，可以考虑增加一个热数据更新队列加载热数据，不需要等访问再加载么？","like_count":1},{"had_liked":false,"id":232759,"user_name":"Geek_299a34","can_delete":false,"product_type":"c1","uid":1928441,"ip_address":"","ucode":"79AECE06F08E87","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/cfNtHoAbHvia1O0jIopiafYbnppEPiawgicKa8vdI2FxMMEdqORB4VLzkYTuGJGA7HibustnU0hDTOD7YSDAWuxhmrg/132","comment_is_top":false,"comment_ctime":1594111019,"is_pvip":false,"replies":[{"id":"85917","content":"一般 volatile-lru&#47;volatile-lfu","user_name":"作者回复","comment_id":232759,"uid":"1001470","ip_address":"","utype":1,"ctime":1594119537,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"5889078315","product_id":100047701,"comment_content":"老师，缓存的淘汰策略，在项目中一般是怎么使用，谢谢","like_count":1,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":500764,"discussion_content":"一般 volatile-lru/volatile-lfu","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1594119537,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":226234,"user_name":"LVM_23","can_delete":false,"product_type":"c1","uid":1039544,"ip_address":"","ucode":"5E54F9DB582E9F","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dc/b8/31c7e110.jpg","comment_is_top":false,"comment_ctime":1592011651,"is_pvip":false,"replies":[{"id":"83542","content":"看下 https:&#47;&#47;www.javaspecialists.eu&#47;archive&#47;Issue246.html","user_name":"作者回复","comment_id":226234,"uid":"1001470","ip_address":"","utype":1,"ctime":1592217623,"user_name_real":"朱晔"}],"discussion_count":2,"race_medal":0,"score":"5886978947","product_id":100047701,"comment_content":"JAVA有实现缓存LRU算法的工具包或者代码吗","like_count":1,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":498159,"discussion_content":"看下 https://www.javaspecialists.eu/archive/Issue246.html","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592217623,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1215066,"avatar":"https://static001.geekbang.org/account/avatar/00/12/8a/5a/b67a82e3.jpg","nickname":"shen","note":"","ucode":"AE5737B0C7DC4F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":283065,"discussion_content":"linkedhashmap简单封装下就可以了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1592151890,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":215075,"user_name":"花轮君","can_delete":false,"product_type":"c1","uid":1044639,"ip_address":"","ucode":"A4F27CC1C38D3B","user_header":"https://static001.geekbang.org/account/avatar/00/0f/f0/9f/6689d26e.jpg","comment_is_top":false,"comment_ctime":1588900858,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"5883868154","product_id":100047701,"comment_content":"热key为什么不落到localcache呢","like_count":1},{"had_liked":false,"id":323142,"user_name":"Will","can_delete":false,"product_type":"c1","uid":1161437,"ip_address":"","ucode":"BB78C2B9B44D36","user_header":"https://static001.geekbang.org/account/avatar/00/11/b8/dd/d729557f.jpg","comment_is_top":false,"comment_ctime":1637743043,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1637743043","product_id":100047701,"comment_content":"rightInit2方法里的CountDownLatch是不是没什么意义，是不是应该把countDownLatch.await()放在下面代码前面Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(() -&gt; {<br>            log.info(&quot;DB QPS : {}&quot;, atomicInteger.getAndSet(0));<br>        }, 0, 1, TimeUnit.SECONDS);","like_count":0},{"had_liked":false,"id":301546,"user_name":"Devil May Cry","can_delete":false,"product_type":"c1","uid":2396435,"ip_address":"","ucode":"F06B99B71AA25D","user_header":"https://static001.geekbang.org/account/avatar/00/24/91/13/009f6a74.jpg","comment_is_top":false,"comment_ctime":1625734113,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1625734113","product_id":100047701,"comment_content":"Redis预热如何实现比较好呢","like_count":0},{"had_liked":false,"id":287727,"user_name":"扎紧绷带","can_delete":false,"product_type":"c1","uid":1316667,"ip_address":"","ucode":"4EDB38E21C12BB","user_header":"https://static001.geekbang.org/account/avatar/00/14/17/3b/37bcd31e.jpg","comment_is_top":false,"comment_ctime":1618127144,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1618127144","product_id":100047701,"comment_content":"“方案二，让缓存不主动过期。初始化缓存数据的时候设置缓存永不过期，然后启动一个后台线程 30 秒一次定时把所有数据更新到缓存，而且通过适当的休眠，控制从数据库更新数据的频率”<br>老师，【而且通过适当的休眠，控制从数据库更新数据的频率】，怎么做休眠，频率不是由30s定时的定时任务控制的吗?  不太理解","like_count":0},{"had_liked":false,"id":273999,"user_name":"鲁鸣","can_delete":false,"product_type":"c1","uid":2152306,"ip_address":"","ucode":"974BA3C3E64630","user_header":"https://static001.geekbang.org/account/avatar/00/20/d7/72/cbef720d.jpg","comment_is_top":false,"comment_ctime":1610784329,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1610784329","product_id":100047701,"comment_content":"先删除缓存，再更新数据库，最后再删除一下缓存，读的时候再加载","like_count":0},{"had_liked":false,"id":215100,"user_name":"Geek_2b3614","can_delete":false,"product_type":"c1","uid":1525522,"ip_address":"","ucode":"6208AB2C5FD4C0","user_header":"https://static001.geekbang.org/account/avatar/00/17/47/12/2c47bf36.jpg","comment_is_top":false,"comment_ctime":1588904660,"is_pvip":false,"replies":[{"id":"79603","content":"：）","user_name":"作者回复","comment_id":215100,"uid":"1001470","ip_address":"","utype":1,"ctime":1588905836,"user_name_real":"朱晔"}],"discussion_count":1,"race_medal":0,"score":"1588904660","product_id":100047701,"comment_content":"山穷水尽疑无路，柳暗花明又一村。啊。","like_count":0,"discussions":[{"author":{"id":1001470,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/47/fe/d0e25d57.jpg","nickname":"朱晔","note":"","ucode":"0B7F0BADE6AAB8","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":494315,"discussion_content":"：）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588905836,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}