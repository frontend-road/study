{"id":787594,"title":"13｜深入理解Seq2Seq：让我们看看语言翻译是怎么来的","content":"<p>你好，我是独行。</p><p>上节课我们一起学习了Word2Vec，Word2Vec的主要能力是把词汇放在多维的空间里，相似的词汇会被放在邻近的位置。这节课我们将进入Seq2Seq的领域，了解这种更为复杂且功能强大的模型，它不仅能理解词汇，还能把这些词汇串联成完整的句子。</p><h2>Seq2Seq</h2><p>Seq2Seq（Sequence-to-Sequence），顾名思义是<strong>从一个序列到另一个序列的转换</strong>。它不仅仅能理解单词之间的关系，而且还能把整个句子的意思打包，并解压成另一种形式的表达。如果说Word2Vec是让我们的机器学会了理解词汇的话，那Seq2Seq则是教会了机器如何理解句子并进行相应地转化。</p><p>在这个过程中，我们会遇到两个核心的角色：<strong>编码器</strong>（Encoder）和<strong>解码器</strong>（Decoder）。编码器的任务是理解和压缩信息，就像是把一封长信函整理成一个精简的摘要；而解码器则需要将这个摘要展开，翻译成另一种语言或形式的完整信息。这个过程有一定的挑战，比如如何确保信息在这次转换中不丢失精髓，而是以新的面貌精准地呈现出来，这就是我们接下来要探索的内容之一。</p><h2>基本概念</h2><p>Seq2Seq也是一种神经网络架构，模型的核心由两部分组成：编码器（Encoder）和解码器（Decoder）。你可以看一下这个架构的示意图。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/1b/73/1b8d0b11f635f071ab7930a493054973.png?wh=2042x648\" alt=\"\"></p><h4>编码器</h4><p>编码器的任务是<strong>读取并理解输入序列，然后把它转换为一个固定长度的上下文向量</strong>，也叫作状态向量。这个向量是输入序列的一种内部表示，捕捉了序列的关键信息。编码器通常是一个循环神经网络（RNN）或其变体，比如长短期记忆网络（LSTM）或门控循环单元（GRU），它们能够处理不同长度的输入序列，并且记住序列中的长期依赖关系。</p><h4>解码器</h4><p>解码器的任务是<strong>接收编码器生成的上下文向量，并基于这个向量生成目标序列</strong>。解码过程是一步步进行的，每一步生成目标序列中的一个元素，比如一个词或字符，直到生成特殊的结束符号，表示输出序列的结束。解码器通常也是一个RNN、LSTM或GRU，它不仅依赖于编码器的上下文向量，还可能依赖于自己之前的输出，来生成下一个输出元素。</p><h4>注意力机制（可选）</h4><p>在编码器和解码器之间，可能还会有一个注意力机制（Attention Mechanism）。注意力机制使解码器能够在生成每个输出元素时“关注”输入序列中的不同部分，从而提高模型处理长序列和捕捉复杂依赖关系的能力。编码器、解码器、注意力机制之间是怎样协作的呢？你可以看一下我给出的示意图。</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/3b/ef925bd2yyec5f51836262527e5fa03b.gif?wh=800x407\" alt=\"图片\"></p><p>下面我通过一个翻译的例子，来说明Seq2Seq的工作原理。</p><h2>工作原理</h2><p>我们先从模型的训练开始，Seq2Seq的训练和Word2Vec不太一样，因为我们讲解的是中英文翻译场景，所以训练的时候，训练数据是中英文数据对。Seq2Seq的训练会比Word2Vec更加复杂一些。上节课的Word2Vec，我们使用的是gensim库提供的基础模型，直接进行训练，这节课我们完全从头写起，训练一个Seq2Seq模型。</p><h4>模型训练</h4><p>我们先准备训练数据，可以在网上找公开的翻译数据集，我们用的是 <a href=\"https://pan.baidu.com/s/113_kXXdekw5IxtinuxBGug?pwd=qvpn\">AIchallenger 2017</a>，这个数据集有1000万对中英文数据，不过因为电脑配置问题，我直接从里面中文和英文的部分各取了10000条进行训练。数据集名称是train_1w.zh和train_1w.en。</p><pre><code class=\"language-python\">cn_sentences = []\nzh_file_path = \"train_1w.zh\"\n# 使用Python的文件操作逐行读取文件，并将每一行的内容添加到列表中\nwith open(zh_file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        # 去除行末的换行符并添加到列表中\n        cn_sentences.append(line.strip())\n\nen_sentences = []\nen_file_path = \"train_1w.en\"\n# 使用Python的文件操作逐行读取文件，并将每一行的内容添加到列表中\nwith open(en_file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        # 去除行末的换行符并添加到列表中\n        en_sentences.append(line.strip())\n</code></pre><p>接下来，基于训练数据集构建中文和英文的词汇表，将每个词映射到一个唯一的索引（integer）。</p><pre><code class=\"language-python\"># cn_sentences 和 en_sentences 分别包含了所有的中文和英文句子\ncn_vocab = build_vocab(cn_sentences, tokenize_cn, max_size=10000, min_freq=2)\nen_vocab = build_vocab(en_sentences, tokenize_en, max_size=10000, min_freq=2)\n</code></pre><p>我们再来看 biild_vocab的源码。</p><pre><code class=\"language-python\">def build_vocab(sentences, tokenizer, max_size, min_freq):\n    token_freqs = Counter()\n    for sentence in sentences:\n        tokens = tokenizer(sentence)\n        token_freqs.update(tokens)\n    vocab = {token: idx + 4 for idx, (token, freq) in enumerate(token_freqs.items()) if freq &gt;= min_freq}\n    vocab['&lt;unk&gt;'] = 0\n    vocab['&lt;pad&gt;'] = 1\n    vocab['&lt;sos&gt;'] = 2\n    vocab['&lt;eos&gt;'] = 3\n    return vocab\n\n</code></pre><p>思路就是把所有的句子读进去，循环分词，放入字典，放的时候要判断一下是否大于等于min_freq，用来过滤掉出现频率较低的词汇，最后构建出来的词汇表如下：</p><pre><code class=\"language-python\">vocab = {\n&nbsp; &nbsp; '&lt;unk&gt;': 0,\n&nbsp; &nbsp; '&lt;pad&gt;': 1,\n&nbsp; &nbsp; '&lt;sos&gt;': 2,\n&nbsp; &nbsp; '&lt;eos&gt;': 3,\n&nbsp; &nbsp; 'i': 4,\n&nbsp; &nbsp; 'like': 5,\n&nbsp; &nbsp; 'learning': 6,\n&nbsp; &nbsp; 'machine': 7,\n&nbsp; &nbsp; 'is': 8,\n&nbsp; &nbsp; 'very': 9,\n&nbsp; &nbsp; 'interesting': 10,\n&nbsp; &nbsp; ...\n}\n\n</code></pre><p>我们来看一下里面比较重要的几个部分。</p><ul>\n<li><code>&lt;unk&gt;</code>：未知单词，表示在训练数据中没有出现过的单词。当模型在处理输入文本时遇到未知单词时，会用这个标记来表示。</li>\n<li><code>&lt;pad&gt;</code>：填充单词，用于将不同长度的序列填充到相同的长度。在处理批次数据时，由于不同序列的长度可能不同，因此需要用这个标记把短序列填充到与最长序列相同的长度，以便进行批次处理。</li>\n<li><code>&lt;sos&gt;</code>：句子起始标记，表示句子的开始位置。在Seq2Seq模型中，通常会在目标句子的开头添加这个标记，以指示解码器开始生成输出。</li>\n<li><code>&lt;eos&gt;</code>：句子结束标记，表示句子的结束位置。在Seq2Seq模型中，通常会在目标句子的末尾添加该标记，以指示解码器生成结束。</li>\n</ul><p>创建训练数据集，将数据处理成方便训练的格式：语言序列，比如 <code>[1,2,3,4]</code>。</p><pre><code class=\"language-python\">dataset = TranslationDataset(cn_sentences, en_sentences, cn_vocab, en_vocab, tokenize_cn, tokenize_en)\ntrain_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n</code></pre><p>然后检测是否有显卡：</p><pre><code class=\"language-plain\"># 检查是否有可用的GPU，如果没有，则使用CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"训练设备为：\", device)\n</code></pre><p>创建模型，参数的解释可以参考代码注释。</p><pre><code class=\"language-python\">\n# 定义一些超参数\nINPUT_DIM = 10000  # 输入语言的词汇量\nOUTPUT_DIM = 10000  # 输出语言的词汇量\nENC_EMB_DIM = 256  # 编码器嵌入层大小，也就是编码器词向量维度\nDEC_EMB_DIM = 256  # 解码器嵌入层大小，解码器词向量维度\nHID_DIM = 512  # 隐藏层维度\nN_LAYERS = 2  # RNN层的数量\nENC_DROPOUT = 0.5  # 编码器神经元输出的数据有50%会被随机丢掉\nDEC_DROPOUT = 0.5  # 解码器同上\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n\nmodel = Seq2Seq(enc, dec, device).to(device)\n# 假定模型已经被实例化并移到了正确的设备上\nmodel.to(device)\n# 定义优化器和损失函数\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=en_vocab['&lt;pad&gt;'])  # 忽略&lt;pad&gt;标记的损失\n</code></pre><p>开始训练：</p><pre><code class=\"language-python\">num_epochs = 10  # 训练轮数\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for src, trg in train_loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()  # 清空梯度\n        output = model(src, trg[:-1])  # 输入给模型的是除了最后一个词的目标句子\n        # Reshape输出以匹配损失函数期望的输入\n        output_dim = output.shape[-1]\n        output = output.view(-1, output_dim)\n        trg = trg[1:].view(-1)  # 从第一个词开始的目标句子\n        loss = criterion(output, trg) # 计算模型输出和实际目标序列之间的损失\n        loss.backward()  # 通过反向传播计算损失相对于模型参数的梯度\n        optimizer.step()  # 根据梯度更新模型参数，这是优化器的一个步骤\n        total_loss += loss.item()\n    avg_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}')\n    \n</code></pre><p>我拿下面的素材举例，简单解释一下训练过程。</p><pre><code class=\"language-plain\">我 喜欢 学习 机器 学习。\nI like studying machine learning\n</code></pre><p>在开始训练之前，先把原文本转化成在对应词语表里的语言序列，比如在中文词汇表中，<code>我 喜欢 学习 机器 学习</code> 分别对应的是 <code>1,2,3,4,5</code>，那么转化成的语言序列就是 <code>[1,2,3,4,5]</code>，也就是前面讲的train_loader里的格式。</p><p>编码器接收到语言序列，经过神经网络GRU单元处理后，生成一个上下文向量，这个上下文向量会作为解码器的初始状态。</p><p>解码器接收上下文向量作为输入，并根据当前上下文以及已生成的部分目标语言序列，计算目标词汇表中每个单词的概率分布。例如，在第一个时间步，解码器可能计算出目标词汇表中每个单词的概率分布，如 <code>\"I\": 0.3, \"like\": 0.1, \"studying\": 0.5, \"machine\": 0.05, \"learning\": 0.05</code>，根据解码器生成的概率分布，选择概率最高的词studying作为当前时间步的输出。</p><p>模型将解码器生成的输出词汇与目标语言句子（“I like studying machine learning.”）中当前时间步对应的词汇进行对比。这里解码器输出的 <code>\"studying\"</code> 与目标语言句子中的 <code>\"I\"</code> 进行对比，发现它们之间的差别较大。</p><p>根据解码器输出 <code>\"studying\"</code> 和目标语言句子中的真实词汇 <code>\"I\"</code> 计算损失，并通过反向传播算法计算梯度。损失值是一个衡量模型预测输出与真实目标之间差异的指标。然后，根据损失值更新模型参数，使模型能够更准确地预测下一个词汇。</p><p>重复以上步骤，直到模型达到指定的训练轮数或者满足其他停止训练的条件。在每次训练迭代中，模型都在尝试调整自己的参数，以使其预测输出更接近真实的目标语言序列，从而提高翻译质量。</p><p>所以这里就能看出，<strong>训练轮数就非常关键，不能太少，也不能太多。</strong></p><h4>模型验证</h4><pre><code class=\"language-python\">def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n    # 将输入句子进行分词并转换为索引序列\n    src_tokens = ['&lt;sos&gt;'] + tokenize_cn(sentence) + ['&lt;eos&gt;']\n    src_indices = [src_vocab[token] if token in src_vocab else src_vocab['&lt;unk&gt;'] for token in src_tokens]\n    # 将输入句子转换为张量并移动到设备上\n    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)\n    # 将输入句子传递给编码器以获取上下文张量\n    with torch.no_grad():\n        encoder_hidden = model.encoder(src_tensor)\n    # 初始化解码器输入为&lt;sos&gt;\n    trg_token = '&lt;sos&gt;'\n    trg_index = trg_vocab[trg_token]\n    # 存储翻译结果\n    translation = []\n    # 解码过程\n    for _ in range(max_len):\n        # 将解码器输入传递给解码器，并获取输出和隐藏状态\n        with torch.no_grad():\n            trg_tensor = torch.LongTensor([trg_index]).to(device)\n            output, encoder_hidden = model.decoder(trg_tensor, encoder_hidden)\n        # 获取解码器输出中概率最高的单词的索引\n        pred_token_index = output.argmax(dim=1).item()\n        # 如果预测的单词是句子结束符，则停止解码\n        if pred_token_index == trg_vocab['&lt;eos&gt;']:\n            break\n        # 否则，将预测的单词添加到翻译结果中\n        pred_token = list(trg_vocab.keys())[list(trg_vocab.values()).index(pred_token_index)]\n        translation.append(pred_token)\n        # 更新解码器输入为当前预测的单词\n        trg_index = pred_token_index\n    # 将翻译结果转换为字符串并返回\n    translation = ' '.join(translation)\n    return translation\n\nsentence = \"我喜欢学习机器学习。\"\ntranslation = translate_sentence(sentence, cn_vocab, en_vocab, model, device)\nprint(f\"Chinese: {sentence}\")\nprint(f\"Translation: {translation}\")\n\n</code></pre><p>程序输出如下：</p><pre><code class=\"language-python\">Chinese: 我喜欢学习机器学习。\nTranslation: I a a a . a . . . .\n</code></pre><p>看上去只翻译成功了“我”这个字，其他都没出来，大概率是因为训练数据太少的原因。</p><p>推理过程和训练过程很像，区别在于，训练过程中模型会记住参数，推理的时候直接根据这些参数计算下一个词的概率即可。</p><p>结尾放一下完整的代码：</p><pre><code class=\"language-python\">import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport spacy\nimport jieba\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn as nn\nimport random\nimport torch.optim as optim\n\n# 加载英文的Spacy模型\nspacy_en = spacy.load('en_core_web_sm')\n\ndef tokenize_en(text):\n    \"\"\"\n    Tokenizes English text from a string into a list of strings (tokens)\n    \"\"\"\n    return [tok.text for tok in spacy_en.tokenizer(text)]\n\ndef tokenize_cn(text):\n    \"\"\"\n    Tokenizes Chinese text from a string into a list of strings (tokens)\n    \"\"\"\n    return list(jieba.cut(text))\n\ndef build_vocab(sentences, tokenizer, max_size, min_freq):\n    token_freqs = Counter()\n    for sentence in sentences:\n        tokens = tokenizer(sentence)\n        token_freqs.update(tokens)\n    vocab = {token: idx + 4 for idx, (token, freq) in enumerate(token_freqs.items()) if freq &gt;= min_freq}\n    vocab['&lt;unk&gt;'] = 0\n    vocab['&lt;pad&gt;'] = 1\n    vocab['&lt;sos&gt;'] = 2\n    vocab['&lt;eos&gt;'] = 3\n    return vocab\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_sentences, trg_sentences, src_vocab, trg_vocab, tokenize_src, tokenize_trg):\n        self.src_sentences = src_sentences\n        self.trg_sentences = trg_sentences\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n        self.tokenize_src = tokenize_src\n        self.tokenize_trg = tokenize_trg\n    def __len__(self):\n        return len(self.src_sentences)\n    def __getitem__(self, idx):\n        src_sentence = self.src_sentences[idx]\n        trg_sentence = self.trg_sentences[idx]\n        src_indices = [self.src_vocab[token] if token in self.src_vocab else self.src_vocab['&lt;unk&gt;']\n                       for token in ['&lt;sos&gt;'] + self.tokenize_src(src_sentence) + ['&lt;eos&gt;']]\n        trg_indices = [self.trg_vocab[token] if token in self.trg_vocab else self.trg_vocab['&lt;unk&gt;']\n                       for token in ['&lt;sos&gt;'] + self.tokenize_trg(trg_sentence) + ['&lt;eos&gt;']]\n        return torch.tensor(src_indices), torch.tensor(trg_indices)\n\ndef collate_fn(batch):\n    src_batch, trg_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, padding_value=1)  # 1 is the index for &lt;pad&gt;\n    trg_batch = pad_sequence(trg_batch, padding_value=1)  # 1 is the index for &lt;pad&gt;\n    return src_batch, trg_batch\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, src):\n        # src: [src_len, batch_size]\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.rnn(embedded)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, input, hidden):\n        input = input.unsqueeze(0)  # input: [1, batch_size]\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.rnn(embedded, hidden)\n        prediction = self.fc_out(output.squeeze(0))\n        return prediction, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src: [src_len, batch_size]\n        # trg: [trg_len, batch_size]\n        # teacher_forcing_ratio是使用真实标签的概率\n        trg_len = trg.shape[0]\n        batch_size = trg.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n        # 存储解码器输出\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        # 编码器的最后一个隐藏状态用作解码器的初始隐藏状态\n        hidden = self.encoder(src)\n        # 解码器的第一个输入是&lt;sos&gt; tokens\n        input = trg[0, :]\n        for t in range(1, trg_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[t] = output\n            # 决定是否使用teacher forcing\n            teacher_force = random.random() &lt; teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[t] if teacher_force else top1\n        return outputs\n\ncn_sentences = []\nzh_file_path = \"train_1w.zh\"\n# 使用Python的文件操作逐行读取文件，并将每一行的内容添加到列表中\nwith open(zh_file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        # 去除行末的换行符并添加到列表中\n        cn_sentences.append(line.strip())\nen_sentences = []\nen_file_path = \"train_1w.en\"\n# 使用Python的文件操作逐行读取文件，并将每一行的内容添加到列表中\nwith open(en_file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        # 去除行末的换行符并添加到列表中\n        en_sentences.append(line.strip())\n# cn_sentences 和 en_sentences 分别包含了所有的中文和英文句子\ncn_vocab = build_vocab(cn_sentences, tokenize_cn, max_size=10000, min_freq=2)\nen_vocab = build_vocab(en_sentences, tokenize_en, max_size=10000, min_freq=2)\n\n# cn_vocab 和 en_vocab 是已经创建的词汇表\ndataset = TranslationDataset(cn_sentences, en_sentences, cn_vocab, en_vocab, tokenize_cn, tokenize_en)\ntrain_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n# 检查是否有可用的GPU，如果没有，则使用CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"训练设备为：\", device)\n# 定义一些超参数\nINPUT_DIM = 10000  # 输入语言的词汇量\nOUTPUT_DIM = 10000  # 输出语言的词汇量\nENC_EMB_DIM = 256  # 编码器嵌入层大小\nDEC_EMB_DIM = 256  # 解码器嵌入层大小\nHID_DIM = 512  # 隐藏层维度\nN_LAYERS = 2  # RNN层的数量\nENC_DROPOUT = 0.5  # 编码器中dropout的比例\nDEC_DROPOUT = 0.5  # 解码器中dropout的比例\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\nmodel = Seq2Seq(enc, dec, device).to(device)\n# 假定模型已经被实例化并移到了正确的设备上\nmodel.to(device)\n# 定义优化器和损失函数\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=en_vocab['&lt;pad&gt;'])  # 忽略&lt;pad&gt;标记的损失\nnum_epochs = 10  # 训练轮数\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for src, trg in train_loader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()  # 清空梯度\n        output = model(src, trg[:-1])  # 输入给模型的是除了最后一个词的目标句子\n        # Reshape输出以匹配损失函数期望的输入\n        output_dim = output.shape[-1]\n        output = output.view(-1, output_dim)\n        trg = trg[1:].view(-1)  # 从第一个词开始的目标句子\n        loss = criterion(output, trg)\n        loss.backward()  # 反向传播\n        optimizer.step()  # 更新参数\n        total_loss += loss.item()\n    avg_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}')\n    # 可以在这里添加验证步骤\n\ndef translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n    # 将输入句子进行分词并转换为索引序列\n    src_tokens = ['&lt;sos&gt;'] + tokenize_cn(sentence) + ['&lt;eos&gt;']\n    src_indices = [src_vocab[token] if token in src_vocab else src_vocab['&lt;unk&gt;'] for token in src_tokens]\n    # 将输入句子转换为张量并移动到设备上\n    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)\n    # 将输入句子传递给编码器以获取上下文张量\n    with torch.no_grad():\n        encoder_hidden = model.encoder(src_tensor)\n    # 初始化解码器输入为&lt;sos&gt;\n    trg_token = '&lt;sos&gt;'\n    trg_index = trg_vocab[trg_token]\n    # 存储翻译结果\n    translation = []\n    # 解码过程\n    for _ in range(max_len):\n        # 将解码器输入传递给解码器，并获取输出和隐藏状态\n        with torch.no_grad():\n            trg_tensor = torch.LongTensor([trg_index]).to(device)\n            output, encoder_hidden = model.decoder(trg_tensor, encoder_hidden)\n        # 获取解码器输出中概率最高的单词的索引\n        pred_token_index = output.argmax(dim=1).item()\n        # 如果预测的单词是句子结束符，则停止解码\n        if pred_token_index == trg_vocab['&lt;eos&gt;']:\n            break\n        # 否则，将预测的单词添加到翻译结果中\n        pred_token = list(trg_vocab.keys())[list(trg_vocab.values()).index(pred_token_index)]\n        translation.append(pred_token)\n        # 更新解码器输入为当前预测的单词\n        trg_index = pred_token_index\n    # 将翻译结果转换为字符串并返回\n    translation = ' '.join(translation)\n    return translation\n\nsentence = \"我喜欢学习机器学习。\"\ntranslation = translate_sentence(sentence, cn_vocab, en_vocab, model, device)\nprint(f\"Chinese: {sentence}\")\nprint(f\"Translation: {translation}\")\n\n</code></pre><h2>小结</h2><p>这节课我们自己动手训练了一个Seq2Seq模型，Seq2Seq可以算是一种高级的神经网络模型了，除了做语言翻译外，甚至可以做基本的问答系统了。但是，Seq2Seq缺点也比较明显，首先Seq2Seq使用固定上下文长度，所以长距离依赖能力较弱。此外，Seq2Seq训练和推理通常需要逐步处理输入和输出序列，所以处理长序列可能会有限制。最后Seq2Seq参数量通常较少，所以面对复杂场景，模型性能可能会受限。</p><p>带着这些问题，下一节课我将会向你介绍终极大boss：<strong>Transformer</strong>，我们学习了这么多基础概念，就是为学习Transformer做铺垫，从ML-&gt;NLP-&gt;Word2Vec-&gt;Seq2Seq-&gt;Transformer一步一步递进。</p><p>注：en_core_web_sm、train_1w.zh、train_1w.en 链接: <a href=\"https://pan.baidu.com/s/1_GG3bIAjqpPGLGugHEI5Dg?pwd=fm8j\">https://pan.baidu.com/s/1_GG3bIAjqpPGLGugHEI5Dg?pwd=fm8j</a> 提取码: fm8j</p><h2>思考题</h2><p>我刚刚讲过，推理的时候模型会使用训练过程中记住的参数来进行概率预测，你可以思考一下，模型的参数到底是什么？欢迎在评论区留言，我们一起讨论学习，如果你觉得这节课的内容对你有帮助的话，也欢迎你分享给需要的朋友，邀TA一起学习，我们下节课再见！</p>","comments":[{"had_liked":false,"id":391924,"user_name":"方梁","can_delete":false,"product_type":"c1","uid":1899249,"ip_address":"北京","ucode":"80FA42955D250E","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/f1/7d21b2b0.jpg","comment_is_top":false,"comment_ctime":1719396065,"is_pvip":false,"replies":[{"id":142475,"content":"文末已添加","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1719556633,"ip_address":"浙江","comment_id":391924,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"train_1w.zh\ntrain_1w.en\n请提供一下哈，谢谢","like_count":1,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647145,"discussion_content":"文末已添加","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719556633,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394209,"user_name":"小毛驴","can_delete":false,"product_type":"c1","uid":1655237,"ip_address":"贵州","ucode":"F67CF9C8EF1721","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epsRcQM1w3rNleqQ9990hGuRWCrrSvAibzcHyuLBic9XhfvC0KCfyjicaIvMaPDicwrepIY0TiatFRp8ag/132","comment_is_top":false,"comment_ctime":1726126542,"is_pvip":false,"replies":[{"id":143182,"content":"重新下载下：python -m spacy download en_core_web_sm，huggingface的可能版本不对","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1726812983,"ip_address":"浙江","comment_id":394209,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师补充一下：OSError: [E053] Could not read config file from external\\en_core_web_sm-2.3.0\\config.cfg\n从网盘下载的模型加载会报错，在huggingface上引用的模型每次执行pred_token_index = output.argmax(dim=1).item()返回都是0，这是为啥？\n","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651389,"discussion_content":"重新下载下：python -m spacy download en_core_web_sm，huggingface的可能版本不对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1726812983,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":394207,"user_name":"小毛驴","can_delete":false,"product_type":"c1","uid":1655237,"ip_address":"贵州","ucode":"F67CF9C8EF1721","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epsRcQM1w3rNleqQ9990hGuRWCrrSvAibzcHyuLBic9XhfvC0KCfyjicaIvMaPDicwrepIY0TiatFRp8ag/132","comment_is_top":false,"comment_ctime":1726124501,"is_pvip":false,"replies":[{"id":143181,"content":"可以debug一下看看","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1726801572,"ip_address":"浙江","comment_id":394207,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师，请教一下为什么 pred_token_index = output.argmax(dim=1).item()这段代码永远返回都是0，是我引用的模型不对嘛？\n","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651383,"discussion_content":"可以debug一下看看","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1726801572,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393929,"user_name":"石云升","can_delete":false,"product_type":"c1","uid":1024195,"ip_address":"广东","ucode":"78F1DD33EFD000","user_header":"https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg","comment_is_top":false,"comment_ctime":1725355930,"is_pvip":false,"replies":[{"id":143034,"content":"慢慢学，还是能看的懂的","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1725530820,"ip_address":"浙江","comment_id":393929,"utype":1}],"discussion_count":1,"race_medal":1,"score":2,"product_id":100770601,"comment_content":"第三章开始的技术原理部分越来越难了。","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650744,"discussion_content":"慢慢学，还是能看的懂的","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725530820,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":393664,"user_name":"王旧业","can_delete":false,"product_type":"c1","uid":1013076,"ip_address":"陕西","ucode":"A8DEC38430D007","user_header":"https://static001.geekbang.org/account/avatar/00/0f/75/54/73cc7f73.jpg","comment_is_top":false,"comment_ctime":1724509412,"is_pvip":false,"replies":[{"id":143035,"content":"动态图制作，有点复杂","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1725530859,"ip_address":"浙江","comment_id":393664,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"老师请教下文中这种动图咋做的","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":650745,"discussion_content":"动态图制作，有点复杂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1725530859,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391923,"user_name":"方梁","can_delete":false,"product_type":"c1","uid":1899249,"ip_address":"北京","ucode":"80FA42955D250E","user_header":"https://static001.geekbang.org/account/avatar/00/1c/fa/f1/7d21b2b0.jpg","comment_is_top":false,"comment_ctime":1719395977,"is_pvip":false,"replies":[{"id":142476,"content":"文末已添加","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1719556643,"ip_address":"浙江","comment_id":391923,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"en_core_web_sm\n等文件在哪里下载？\n","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647146,"discussion_content":"文末已添加","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719556643,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391911,"user_name":"Geek_7df415","can_delete":false,"product_type":"c1","uid":3904223,"ip_address":"江苏","ucode":"113D9A600629FA","user_header":"","comment_is_top":false,"comment_ctime":1719383905,"is_pvip":false,"replies":[{"id":142477,"content":"你好，链接已更新","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1719556653,"ip_address":"浙江","comment_id":391911,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"模型训练部分， AIchallenger2017 的链接，AccessDenied","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647147,"discussion_content":"你好，链接已更新","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1719556653,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]}]}