{"id":685751,"title":"12｜实战项目（二）：动手训练一个你自己的扩散模型","content":"<p>你好，我是南柯。</p><p>前面几讲，我们已经了解了扩散模型的算法原理和组成模块，学习了Stable Diffusion模型新增的CLIP和VAE模块。掌握了这些知识，相信你也一定跃跃欲试，想要训练一个属于自己的AI绘画模型。</p><p>这一讲，我们会将前几讲的知识串联起来，从全局的视角讨论扩散模型如何训练和使用。我们将通过实战的形式，一起训练一个标准扩散模型，并微调一个Stable Diffusion模型，帮你进一步加深对知识的理解。学完这一讲，我们就迈出了模型自由的关键一步。</p><h2>关键知识串联</h2><p>在实战之前，我想请你思考一个问题：想要把标准的扩散模型升级为Stable Diffusion，需要几步操作？</p><p>答案是两步。</p><p>我们通过<a href=\"https://time.geekbang.org/column/article/681276\">第6讲</a>已经知道，标准扩散模型的训练过程包含6个步骤，分别是随机选取训练图像、随机选择时间步t、随机生成高斯噪声、一步计算第t步加噪图、使用UNet预测噪声值和计算噪声数值误差。</p><p>Stable Diffusion在此基础上，增加了VAE模块和CLIP模块。VAE模块的作用是降低输入图像的维度，从而加快模型训练、给GPU腾腾地方；CLIP模块的作用则是将文本描述通过交叉注意力机制注入到UNet模块，让AI绘画模型做到言出法随。</p><!-- [[[read_end]]] --><p>我们不妨再翻出Stable Diffusion的算法框架图回忆一下。图中最左侧粉色区域便是VAE模块，最右侧的条件控制模块便可以是CLIP（也可以是其他控制条件），而中间UNet部分展示的QKV模块，便是prompt通过交叉注意力机制引导图像生成。到此为止，是不是一切都串联起来了？</p><p><img src=\"https://static001.geekbang.org/resource/image/44/5a/4431b947bb93aafba67c8f731de29b5a.jpg?wh=3805x1907\" alt=\"\" title=\"图片来源：https://arxiv.org/abs/2112.10752\"></p><p>事实上，在Stable Diffusion中，还有很多其他黑魔法，比如无条件引导控制（Classifier-Free Guidance）、引导强度（Guidance Scale）等，我们会在下一章进一步探讨。</p><p>知道了这些，我们不妨继续思考一个问题：训练一个标准扩散模型和Stable Diffusion模型，需要准备哪些“原材料”呢？</p><p>首先，我们需要GPU，显存越大越好。没有英伟达显卡的同学，可以使用Colab免费的15G T4显卡。在<a href=\"https://time.geekbang.org/column/article/684612\">第10讲</a>中我们详细讨论了Colab GPU环境的用法，不熟悉的话你可以通过超链接回顾。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/ec/2a35be37dce07657b1e4229376388bec.png?wh=2085x841\" alt=\"\"></p><p>然后，我们需要训练数据。对于标准扩散模型而言，我们只需要纯粹的图片数据即可；对于Stable Diffusion，由于我们需要文本引导，就需要用到图片数据对应的文本描述。这里的文本描述既可以是像CLIP训练数据那种对应的文本描述，也可以是使用各种图片描述（image caption）模型获取的文本描述。</p><p>如果你要训练的是Stable Diffusion，在第1讲中我们估算过，从头开始训练的成本差不多是几套海淀学区房的价格，所以我们最好是基于某个开源预训练模型进行针对性微调。事实上，开源社区里大多数模型都是微调出来的。</p><p>此外，对于Stable Diffusion，我们还需要准备好预先训练好的CLIP模型和VAE模型。</p><p>关于训练数据、开源预训练模型、CLIP和VAE，你都不必担心。后面的代码环节我会说明获取方法。现在你只需要准备好GPU资源即可。</p><p>这一讲的实战部分，所有操作你都可以通过点开我提供的<a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson12/train_diffusion_v2.ipynb\">Colab链接</a>来完成。当然，我更推荐你新建全新的Colab，对照我提供的原始Colab逐步写代码来完成。这样有助于你加深对训练过程的理解。</p><h2>训练扩散模型</h2><p>这里我们通过两种方式来训练扩散模型。</p><p>第一种是使用denoising_diffusion_pytorch这个高度集成的工具包，第二种则是基于diffusers这种更多开发者使用的工具包。对于专业的算法同学而言，我更推荐使用diffusers来训练。原因是diffusers工具包在实际的AI绘画项目中用得更多，并且也更易于我们修改代码逻辑，实现定制化功能。</p><h3>认识基础模块</h3><p>先看第一种训练方式，我们先按照下面的方式，在Colab里安装对应工具包。你可以直接点开我的 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson12/train_diffusion_v2.ipynb\">Colab链接</a>，点击播放按键逐步操作。</p><pre><code class=\"language-bash\">pip install denoising_diffusion_pytorch\n</code></pre><p>这个工具包中提供了UNet和扩散模型两个封装好的模块，你可以通过两行指令创建UNet，并基于创建好的UNet创建一个完整的扩散模型，同时指定了图像分辨率和总的加噪步数。</p><pre><code class=\"language-python\">from denoising_diffusion_pytorch import Unet, GaussianDiffusion\nimport torch \n\nmodel = Unet(\n    dim = 64,  \n    dim_mults = (1, 2, 4, 8)\n).cuda()\n\ndiffusion = GaussianDiffusion(\n    model,\n    image_size = 128,\n    timesteps = 1000   # number of steps\n).cuda()\n</code></pre><p>训练过程也非常清爽，为了帮你更好地理解一次训练的过程是怎样的。我们结合代码例子看一下，比如我们随机初始化八张图片，便可以通过后面这两行代码完成扩散模型的一次训练。</p><pre><code class=\"language-python\"># 使用随机初始化的图片进行一次训练\ntraining_images = torch.randn(8, 3, 128, 128)\nloss = diffusion(training_images.cuda())\nloss.backward()\n</code></pre><p>如果你想用自己本地的图像，而非随机初始化的图像，可以参考下面的代码。</p><pre><code class=\"language-python\">from PIL import Image\nimport torchvision.transforms as transforms\nimport torch\n# 预设一个变换操作，将PIL Image转换为PyTorch Tensor，并对其进行归一化\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n# 我们认为你有个列表包含了8张图像的路径\nimage_paths = ['path_to_your_image1', 'path_to_your_image2', 'path_to_your_image3', 'path_to_your_image4', \n               'path_to_your_image5', 'path_to_your_image6', 'path_to_your_image7', 'path_to_your_image8'] \n# 使用List comprehension读取并处理这些图片\nimages = [transform(Image.open(image_path)) for image_path in image_paths] \n# 将处理好的图像List转化为一个4D Tensor，注意torch.stack能够自动处理3D Tensor到4D Tensor的转换\ntraining_images = torch.stack(images)\n# 现在training_images应该有8张3x128x128的图像\nprint(training_images.shape)  # torch.Size([8, 3, 128, 128])\n</code></pre><p>训练完成后，可以直接使用得到的模型来生成图像。由于我们的模型只训练了一步，模型的输出也是纯粹的噪声图。这里只是为了让你找一下手感。</p><pre><code class=\"language-python\">sampled_images = diffusion.sample(batch_size = 4)\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/2a/9f/2a5b3f82bca2a0a01272f81e6b1bbd9f.jpg?wh=4409x1486\" alt=\"\"></p><h3>数据准备</h3><p>理解完基本流程，我们使用真实数据进行一次训练。我们以 <a href=\"https://huggingface.co/datasets/nelorth/oxford-flowers\">oxford-flowers</a> 这个数据集为例，首先需要安装datasets这个工具包。</p><pre><code class=\"language-bash\">pip install datasets\n</code></pre><p>我们使用后面的代码就可以下载这个数据集，并将数据集中所有的图片单独存储成png格式，用png格式更方便我们查看。全部处理完大概有8000张图片。</p><pre><code class=\"language-bash\">from PIL import Image\nfrom io import BytesIO\nfrom datasets import load_dataset\nimport os\nfrom tqdm import tqdm\n\ndataset = load_dataset(\"nelorth/oxford-flowers\")\n\n# 创建一个用于保存图片的文件夹\nimages_dir = \"./oxford-datasets/raw-images\"\nos.makedirs(images_dir, exist_ok=True)\n\n# 遍历所有图片并保存，针对oxford-flowers，整个过程要持续15分钟左右\nfor split in dataset.keys():\n    for index, item in enumerate(tqdm(dataset[split])):\n        image = item['image']\n        image.save(os.path.join(images_dir, f\"{split}_image_{index}.png\"))\n</code></pre><p>你也可以在 <a href=\"https://huggingface.co/\">Hugging Face</a> 上挑选你喜欢的图像数据集，挑选和使用方法可以参考后面的截图。</p><p><img src=\"https://static001.geekbang.org/resource/image/96/37/960f48e7fd26e3236c84a548cf1c7237.png?wh=1710x975\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/2y/5a/2yyf8bd9eebe1f8e8a2d04542d34805a.png?wh=1606x872\" alt=\"\"></p><p>点击数据集使用后，通过下面两行代码即可完成数据集的下载和读取。</p><p><img src=\"https://static001.geekbang.org/resource/image/3f/12/3f68f0e2f579aacf001c053ffcd84812.png?wh=1575x406\" alt=\"\"></p><pre><code class=\"language-python\">from datasets import load_dataset\ndataset = load_dataset(\"nelorth/oxford-flowers\")\n</code></pre><p>比方说上图展示的这个数据集，里面都是一些不同的花朵。我们课程里就选择这个花朵数据集，训练的目的就是得到一个扩散模型，这个模型可以从噪声出发，逐步去噪得到一朵花。</p><h3>模型训练</h3><p>准备工作完成，我们便可以通过以下代码来进行完整训练。如果你的GPU不够强大，可以根据实际情况调整训练的batch_size大小。</p><pre><code class=\"language-python\">import torch\nfrom denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n\nmodel = Unet(\n    dim = 64,\n    dim_mults = (1, 2, 4, 8)\n).cuda()\n\ndiffusion = GaussianDiffusion(\n    model,\n    image_size = 128,\n    timesteps = 1000   # 加噪总步数\n).cuda()\n\ntrainer = Trainer(\n    diffusion,\n    './oxford-datasets/raw-images',\n    train_batch_size = 16,\n    train_lr = 2e-5,\n    train_num_steps = 20000,          # 总共训练20000步\n    gradient_accumulate_every = 2,    # 梯度累积步数\n    ema_decay = 0.995,                # 指数滑动平均decay参数\n    amp = True,                       # 使用混合精度训练加速\n    calculate_fid = False,            # 我们关闭FID评测指标计算（比较耗时）。FID用于评测生成质量。\n    save_and_sample_every = 2000      # 每隔2000步保存一次模型\n)\n\ntrainer.train()\n</code></pre><p>这里分享一个小技巧，如果在使用GPU的时候报错提示显存不足，可以通过后面的命令手工释放不再使用的GPU显存。</p><pre><code class=\"language-python\">import gc\n\ndel old_model # 这里的old_model是指已经不会再用到的模型\ngc.collect()\ntorch.cuda.empty_cache()\n</code></pre><p>对于16G的V100显卡而言，整个任务的训练要持续3至4个小时。在整个训练过程中，每次间隔2000个训练步，我们会保存一次模型权重，并利用当前权重进行图像的生成。</p><p>你可以参考后面的图片，能看出，随着训练步数的增多，这个扩散模型的图像生成能力在逐渐变强。</p><p><img src=\"https://static001.geekbang.org/resource/image/84/2d/840c6c627e2539d79326ab7a926bee2d.gif?wh=652x702\" alt=\"图片\"></p><h3>进阶到diffusers训练</h3><p>对于我们提到的第二种扩散模型训练方式，基于diffusers工具包的训练，我们要写的代码就会多得多，并且可调节的参数也会多很多。</p><p>这里我放一个 <a href=\"https://colab.research.google.com/github/NightWalker888/ai_painting_journey/blob/main/lesson12/diffusers_training_example_annotated.ipynb\">Colab的链接</a>，包含完整的训练代码。我来带你一起拆解下其中的关键部分。</p><p>首先，我们看数据集的使用。通过datasets工具包加载数据集，与denoising_diffusion_pytorch的训练不同，在diffusers训练模式下，我们不需要将数据集再转为本地图片格式。</p><pre><code class=\"language-python\">import torch\nfrom datasets import load_dataset\n\n# 加载数据集\nconfig.dataset_name = \"huggan/smithsonian_butterflies_subset\"\ndataset = load_dataset(config.dataset_name, split=\"train\")\n\n# 封装成训练用的格式\ntrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n</code></pre><p>为了提升模型的性能，我们可以对图像数据进行数据增广。所谓数据增广，就是对图像做一些随机左右翻转、随机颜色扰动等操作，目的是增强训练数据的多样性。</p><pre><code class=\"language-python\">from torchvision import transforms\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((config.image_size, config.image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n</code></pre><p>然后我们看UNet结构。按照下图的结构搭建UNet模块，比如图中输入和输出的分辨率都是128x128，在实际UNet搭建中你可以任意指定。</p><p><img src=\"https://static001.geekbang.org/resource/image/5d/7e/5dde838e4acda1e2603e3a6dd86f127e.png?wh=2134x1046\" alt=\"\"></p><p>我们可以通过下面的代码来创建UNet结构。</p><pre><code class=\"language-python\">from diffusers import UNet2DModel\n\nmodel = UNet2DModel(\n&nbsp; &nbsp; sample_size=config.image_size,&nbsp; # 目标图像的分辨率\n&nbsp; &nbsp; in_channels=3,&nbsp; # 输入通道的数量，对于RGB图像为3\n&nbsp; &nbsp; out_channels=3,&nbsp; # 输出通道的数量\n&nbsp; &nbsp; layers_per_block=2,&nbsp; # 每个UNet块中使用的ResNet层的数量\n&nbsp; &nbsp; block_out_channels=(128, 128, 256, 256, 512, 512),&nbsp; # 每个UNet块的输出通道数量\n&nbsp; &nbsp; down_block_types=(&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"DownBlock2D\",&nbsp; # 常规的ResNet下采样块\n&nbsp; &nbsp; &nbsp; &nbsp; \"DownBlock2D\",&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"DownBlock2D\",&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"DownBlock2D\",&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"AttnDownBlock2D\",&nbsp; # 具有空间自注意力的ResNet下采样块\n&nbsp; &nbsp; &nbsp; &nbsp; \"DownBlock2D\",\n&nbsp; &nbsp; ),&nbsp;\n&nbsp; &nbsp; up_block_types=(\n&nbsp; &nbsp; &nbsp; &nbsp; \"UpBlock2D\",&nbsp; # 常规的ResNet上采样块\n&nbsp; &nbsp; &nbsp; &nbsp; \"AttnUpBlock2D\",&nbsp; # 具有空间自注意力的ResNet上采样块\n&nbsp; &nbsp; &nbsp; &nbsp; \"UpBlock2D\",&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"UpBlock2D\",&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"UpBlock2D\",&nbsp;\n&nbsp; &nbsp; &nbsp; &nbsp; \"UpBlock2D\"&nbsp;&nbsp;\n&nbsp; &nbsp; &nbsp; ),\n)\n</code></pre><p>可以看到，使用diffusers创建UNet的步骤要比denoising_diffusion_pytorch复杂很多，好处是给工程师带来了更大的灵活性。</p><p>接下来我们看采样器的用法。这里需要确定我们加噪用的采样器，帮助我们通过一步计算得到第t步的加噪结果。</p><pre><code class=\"language-python\">from diffusers import DDPMScheduler\n\nnoise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n\n# 一步加噪的计算\nnoise = torch.randn(sample_image.shape)\ntimesteps = torch.LongTensor([50])\nnoisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n</code></pre><p>接着通过模型预测噪声，并计算损失函数。</p><pre><code class=\"language-python\">import torch.nn.functional as F\n\nnoise_pred = model(noisy_image, timesteps).sample\nloss = F.mse_loss(noise_pred, noise)\n</code></pre><p>最后，我们将这些模块串联起来，便可以得到基于diffusers训练扩散模型的核心代码。</p><pre><code class=\"language-python\">for epoch in range(num_epochs):\n    for step, batch in enumerate(train_dataloader):\n        clean_images = batch['images']\n        # 对应于扩散模型训练过程：随机采样噪声\n        noise = torch.randn(clean_images.shape).to(clean_images.device)\n        bs = clean_images.shape[0]\n\n        # 对应于扩散模型训练过程：对于batch中的每张图，随机选取时间步t\n        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n\n        # 对应于扩散模型训练过程：一步计算加噪结果\n        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n        \n        with accelerator.accumulate(model):\n            # 对应于扩散模型训练过程：预测噪声值并计算损失函数\n            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n            loss = F.mse_loss(noise_pred, noise)\n            accelerator.backward(loss)\n            optimizer.step()       \n</code></pre><h2>微调Stable Diffusion</h2><p>搞定了扩散模型的训练，我们可以再挑战一下Stable Diffusion模型的微调。我们可以直接参考diffusers官方提供的<a href=\"https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\">训练代码</a>，别看这个代码有接近1100行，其实相比于上面提到的标准扩散模型训练，核心也只是多了VAE和CLIP的部分。</p><p>这里我节选了VAE和CLIP部分的代码，目的是让你了解这两个模块是如何加载使用的。</p><pre><code class=\"language-python\">tokenizer = CLIPTokenizer.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n)\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n)\n\nvae = AutoencoderKL.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n)\n\nunet = UNet2DConditionModel.from_pretrained(\n    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.non_ema_revision\n)\n\n# 将vae 和 text_encoder的参数冻结，训练过程中权重不更新\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)\n</code></pre><p>你可能在代码中发现了一个tokenizer变量。它的作用便是我们在<a href=\"https://time.geekbang.org/column/article/682762\">第7讲</a>中提到的，对我们输入的prompt进行分词后获取token_id。有了token_id，我们便可以获取模型可用的词嵌入向量。CLIP模型的文本编码器（text_encoder）基于词嵌入向量，便可以提取文本特征。VAE模块和CLIP模块都不需要权重更新，因此上面的代码中将梯度（grad）设置为False。</p><p><img src=\"https://static001.geekbang.org/resource/image/4e/6b/4e1130cf6424d94f0yyff14a97812a6b.jpg?wh=4409x1142\" alt=\"\"></p><p>这里我需要指出，在一些情况下，比如训练DreamBooth和LoRA模型时，CLIP文本编码器的参数也可以学习和更新，这能帮我们提升模型的效果。</p><p>最后，我们再看看Stable Diffusion训练的核心代码。</p><pre><code class=\"language-python\"> for epoch in range(num_train_epochs):\n     for step, batch in enumerate(train_dataloader):\n     \n         # VAE模块将图像编码到潜在空间\n         latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n         \n         # 随机噪声 &amp; 加噪到第t步\n         noise = torch.randn_like(latents)\n         timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps)\n         noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n         \n         # 使用CLIP将文本描述作为输入\n         encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n         target = noise\n         \n         # 预测噪声并计算loss\n         model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n         loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n         optimizer.step()\n</code></pre><p>相信你在上面的代码中看到了很多熟悉的名词，VAE、潜在空间、CLIP、文本描述等，这些都是Stable Diffusion比标准扩散模型多出来的东西。如果你想进一步确认文本描述如何通过交叉注意力机制起作用，我推荐你去看看 <a href=\"https://github.com/huggingface/diffusers/blob/v0.19.3/src/diffusers/models/unet_2d_condition.py#L66\">UNet2DConditionModel</a> 这个模块的代码，加深理解。</p><h2>如何调用各种SD模型？</h2><p>其实我们可以在Hugging Face中找到各种现成的模型，我们只需通过模型的model_id，便可以直接在Colab中调用这些模型，我们这就实战练习一下。</p><p>比如我们可以使用 <a href=\"https://huggingface.co/gsdf/Counterfeit-V2.5\">Counterfeit-V2.5</a> 这个模型，首先获取到它的model_id。</p><p><img src=\"https://static001.geekbang.org/resource/image/cc/d7/ccc132a8a8069879869f8e8aec42a5d7.png?wh=1722x942\" alt=\"\"></p><p>之后，我们通过后面的代码下载并加载模型。第四行的模型ID可以灵活调整，你可以切换成你心仪的模型。</p><pre><code class=\"language-python\">import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers import DDIMScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler\npipeline = DiffusionPipeline.from_pretrained(\"gsdf/Counterfeit-V2.5\")\n</code></pre><p>然后用下面代码完成切换采样器，prompt设置等操作，便可以随心所欲地创作了。</p><pre><code class=\"language-python\"># 切换为DPM采样器\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n\nprompt = \"((masterpiece,best quality)),1girl, solo, animal ears, rabbit\"\nnegative_prompt = \"EasyNegative, extra fingers,fewer fingers,\"\nimages = pipeline(prompt, width = 512, height = 512, num_inference_steps=20, guidance_scale=7.5).images\n</code></pre><p>你可以点开我的 <a href=\"https://github.com/NightWalker888/ai_painting_journey/blob/main/lesson12/%E5%BC%80%E6%BA%90AI%E7%BB%98%E7%94%BB%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8bug%E4%BF%AE%E5%A4%8D.ipynb\">Colab链接</a>进行操作。可以在Hugging Face中调一些你喜欢的AI绘画模型，试试自己动手创作一些作品。</p><h2>总结时刻</h2><p>今天我们通过实战的形式加深了对扩散模型和Stable Diffusion模型的认识。在扩散模型部分，我们从数据准备开始，使用两种不同的形式从头开始进行模型训练，最终殊途同归，都能得到“不听话”的AI画师。</p><p>为了进一步调优，我们又引入VAE和CLIP模块，在开源Stable Diffusion模型的基础上，微调属于我们自己的SD模型，并深入探讨了其中的代码细节。我们也探索了如果通过代码直接使用开源社区提供的SD模型，通过短短几行代码就能实现AI绘画。</p><p>在我看来，以扩散模型为主的AI绘画，与此前GAN时代最大的不同之处便是“不可小觑的开源社区”。2022年之前，各种有趣的GAN模型和特效更多像是企业才“玩得动”的技术，而如今的AI绘画则是在放大每一个爱好者的创造力。</p><p>当前，企业会选择当前效果最好的开源模型，比如SDXL、AnythingV5漫画模型等，进一步构造海量的高质量数据，去微调这些SD模型。技术方案和我们今天实战部分微调SD模型是一样的，只不过企业有更多的GPU、图片数据和标注员。</p><p>即便如此，为什么开源社区的模型仍旧有如此抢眼的表现呢？我个人觉得，相比很多企业的KPI驱动，开源社区兴趣驱动更容易做出垂类精品。如果你也有类似的感觉，那么期待你和我一起，去做一些有意思的AI绘画模型。</p><p>这一讲的重点，你可以点开下面的导图进行知识回顾。</p><p><img src=\"https://static001.geekbang.org/resource/image/87/aa/878d608159bf4a358245e0d1d33050aa.jpg?wh=3600x2521\" alt=\"\"></p><h2>思考题</h2><p>这一讲是我们的实战课。我们留一个实战任务。在Hugging Face中选择一个你喜欢的基础模型，通过写代码的方式生成一组你喜欢的图片。</p><p>期待你在留言区和我交流互动，也推荐你把今天的内容分享给身边的小伙伴，一起创造更有个性的AI绘画模型。</p>","comments":[{"had_liked":false,"id":380163,"user_name":"刘","can_delete":false,"product_type":"c1","uid":1060093,"ip_address":"北京","ucode":"8B335E08D32128","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83ep2B3n5OEDVZVjpkJWxyrxTq2ib20qjBBcsAIU3jXhQ5BwlRBPsdZlwOHqdiaJtn7niaLvOz2biaja6pw/132","comment_is_top":false,"comment_ctime":1693124673,"is_pvip":false,"replies":[{"id":138499,"content":"你好，这里的微调模型是指微调整个SD模型中UNet的所有权重参数，在LoRA流行之前，大家说的微调模型是指这种方式，比如我们在Civitai上能看到的各种几GB大小的模型；微调LoRA只需要采用秩因子分解的方式，优化UNet中Attention部分的部分权重矩阵（权重不变，优化对应的LoRA），这种方式得到的权重通常只有几十到几百M大小。手中没有海量训练数据的情况下，选择训练LoRA的方式即可。对于计算资源有限的情况而言，优化LoRA也是更方便的选择。希望能帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693378800,"ip_address":"北京","comment_id":380163,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"你好，我想问下，这里微调模型和训练lora模型有什么关系？","like_count":3,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626773,"discussion_content":"你好，这里的微调模型是指微调整个SD模型中UNet的所有权重参数，在LoRA流行之前，大家说的微调模型是指这种方式，比如我们在Civitai上能看到的各种几GB大小的模型；微调LoRA只需要采用秩因子分解的方式，优化UNet中Attention部分的部分权重矩阵（权重不变，优化对应的LoRA），这种方式得到的权重通常只有几十到几百M大小。手中没有海量训练数据的情况下，选择训练LoRA的方式即可。对于计算资源有限的情况而言，优化LoRA也是更方便的选择。希望能帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693378801,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":382759,"user_name":"陈东","can_delete":false,"product_type":"c1","uid":2213995,"ip_address":"广西","ucode":"FCDE6D237CC621","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Ge7uhlEVxicQT73YuomDPrVKI8UmhqxKWrhtO5GMNlFjrHWfd3HAjgaSribR4Pzorw8yalYGYqJI4VPvUyPzicSKg/132","comment_is_top":false,"comment_ctime":1697987356,"is_pvip":true,"replies":[{"id":140312,"content":"目前是有这个弊端，在Colab上没法安装。如果你有能力探索本地部署的话就可以解决。","user_name":"编辑回复","user_name_real":"编辑","uid":1501385,"ctime":1701672560,"ip_address":"北京","comment_id":382759,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"每次在Google colab中安装package，关闭后安装包被删除，如何永久性使用安装包？尝试了很多办法，不可行。请问老师有永久性安装办法吗？谢谢","like_count":1,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":632949,"discussion_content":"目前是有这个弊端，在Colab上没法安装。如果你有能力探索本地部署的话就可以解决。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1701672560,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":8}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":381996,"user_name":"失落的走地鸡","can_delete":false,"product_type":"c1","uid":3709547,"ip_address":"安徽","ucode":"809227BCDE2844","user_header":"https://static001.geekbang.org/account/avatar/00/38/9a/6b/791d0f5e.jpg","comment_is_top":false,"comment_ctime":1696476233,"is_pvip":false,"replies":[{"id":139147,"content":"你好。在transformer工具包中（https:&#47;&#47;huggingface.co&#47;transformers&#47;v4.8.0&#47;index.html），有大量的tokenizer可供选择，比如CLIPTokenizer、BertTokenizer、OpenAIGPTTokenizer等。CLIP使用的是一种基于BPE(Byte Pair Encoding)的tokenizer，这个tokenizer是预先训练得到的。原始Transformer论文中，使用的是一种基于子词的tokenizer，使用哪种tokenizer对于transformer来说不是最核心的，可以看作是模型预处理步骤的选择。希望能够帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1696585937,"ip_address":"北京","comment_id":381996,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"微调SD模型中有行代码：tokenizer = CLIPTokenizer.from_pretrained()。前面讲transformer的组成时提到token和词嵌入，这里又将它们归属到clip，请问怎么理解这两种矛盾的说法？","like_count":1,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":629045,"discussion_content":"你好。在transformer工具包中（https://huggingface.co/transformers/v4.8.0/index.html），有大量的tokenizer可供选择，比如CLIPTokenizer、BertTokenizer、OpenAIGPTTokenizer等。CLIP使用的是一种基于BPE(Byte Pair Encoding)的tokenizer，这个tokenizer是预先训练得到的。原始Transformer论文中，使用的是一种基于子词的tokenizer，使用哪种tokenizer对于transformer来说不是最核心的，可以看作是模型预处理步骤的选择。希望能够帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1696585938,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380460,"user_name":"Geek_535f73","can_delete":false,"product_type":"c1","uid":2949120,"ip_address":"上海","ucode":"9A286565D3926A","user_header":"","comment_is_top":false,"comment_ctime":1693636277,"is_pvip":false,"replies":[{"id":138676,"content":"你好，看起来是数据文件下载不完整导致的问题。和ChatGPT交流了一下，推荐了几个解决思路，可以试试看能否解决：\n\n检查文件：确定你正在读的文件是否确实是一个 Parquet 文件。通常，Parquet 文件的扩展名为 .parquet。尝试双击文件或使用专门的读取器来查看其内容。如果无法打开或内容看起来不正常，那么文件可能损坏了。\n\n尝试重新生成文件：如果你有生成该文件的脚本或源数据，可以尝试重新生成这个文件。\n\n检查生成代码：如果你手动创建了 Parquet 文件，检查你的代码以确认你是否正确地生成了此文件。PyArrow有一个简单的API可以用来生成 Parquet 文件。\n\n确保数据集路径正确：如果你使用的是datasets库加载数据，确保你的数据集路径是正确的。\n\n希望能帮助到你，如果没能解决，欢迎继续交流~","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694269855,"ip_address":"北京","comment_id":380460,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"Generating train split:   0%|                                                                                    | 0&#47;7169 [00:00&lt;?, ? examples&#47;s]\nTraceback (most recent call last):\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;builder.py&quot;, line 1925, in _prepare_split_single\n    for _, table in generator:\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;packaged_modules&#47;parquet&#47;parquet.py&quot;, line 77, in _generate_tables\n    parquet_file = pq.ParquetFile(f)\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;pyarrow&#47;parquet&#47;__init__.py&quot;, line 286, in __init__\n    self.reader.open(\n  File &quot;pyarrow&#47;_parquet.pyx&quot;, line 1227, in pyarrow._parquet.ParquetReader.open\n  File &quot;pyarrow&#47;error.pxi&quot;, line 100, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;load.py&quot;, line 2136, in load_dataset\n    builder_instance.download_and_prepare(\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;builder.py&quot;, line 954, in download_and_prepare\n    self._download_and_prepare(\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;builder.py&quot;, line 1049, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;builder.py&quot;, line 1813, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n  File &quot;&#47;cloud&#47;sd_hbo&#47;lib&#47;python3.10&#47;site-packages&#47;datasets&#47;builder.py&quot;, line 1958, in _prepare_split_single\n    raise DatasetGenerationError(&quot;An error occurred while generating the dataset&quot;) from e\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627569,"discussion_content":"你好，看起来是数据文件下载不完整导致的问题。和ChatGPT交流了一下，推荐了几个解决思路，可以试试看能否解决：\n\n检查文件：确定你正在读的文件是否确实是一个 Parquet 文件。通常，Parquet 文件的扩展名为 .parquet。尝试双击文件或使用专门的读取器来查看其内容。如果无法打开或内容看起来不正常，那么文件可能损坏了。\n\n尝试重新生成文件：如果你有生成该文件的脚本或源数据，可以尝试重新生成这个文件。\n\n检查生成代码：如果你手动创建了 Parquet 文件，检查你的代码以确认你是否正确地生成了此文件。PyArrow有一个简单的API可以用来生成 Parquet 文件。\n\n确保数据集路径正确：如果你使用的是datasets库加载数据，确保你的数据集路径是正确的。\n\n希望能帮助到你，如果没能解决，欢迎继续交流~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694269855,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380458,"user_name":"Geek_535f73","can_delete":false,"product_type":"c1","uid":2949120,"ip_address":"上海","ucode":"9A286565D3926A","user_header":"","comment_is_top":false,"comment_ctime":1693635871,"is_pvip":false,"replies":[{"id":138675,"content":"你好，这一行是下载数据集的代码，如果报错大概率是网络原因。可以把报错的日志贴一下。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1694269714,"ip_address":"北京","comment_id":380458,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师您好\n我在自己的代码中dataset = load_dataset(&quot;nelorth&#47;oxford-flowers&quot;)时遇到这个问题\n\n确实网上搜了很久都没有找到答案，所以来请教您&#47;哭","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627568,"discussion_content":"你好，这一行是下载数据集的代码，如果报错大概率是网络原因。可以把报错的日志贴一下。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1694269714,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2949120,"avatar":"","nickname":"Geek_535f73","note":"","ucode":"9A286565D3926A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":627097,"discussion_content":"已经确认是云服务器的问题，服务器厂商已修复...","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693794013,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":380117,"user_name":"Ericpoon","can_delete":false,"product_type":"c1","uid":1051587,"ip_address":"广东","ucode":"3D09FD429FE9CC","user_header":"https://static001.geekbang.org/account/avatar/00/10/0b/c3/3385cd46.jpg","comment_is_top":false,"comment_ctime":1693022618,"is_pvip":false,"replies":[{"id":138500,"content":"你好，建议使用SDXL推荐的分辨率（默认1024x1024），不要设置极端的宽高比。除去采样器、分辨率这些原因外，我建议参考下一些prompt经验贴看看，比如这个:https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;648291684，看看能否调试出对应的风格。希望能帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693379352,"ip_address":"北京","comment_id":380117,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"试过在本地运行SDXL0.9，（1.0 运行不了，内存不足），0.9得到的图象结果没有网上写的那么好，人或动物也看着很抽象，我用的只是HUGGING FACE上给的运行代码，什么参数都没有。请问这个有没有相关的参数设置的文章，介绍一下。","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626775,"discussion_content":"你好，建议使用SDXL推荐的分辨率（默认1024x1024），不要设置极端的宽高比。除去采样器、分辨率这些原因外，我建议参考下一些prompt经验贴看看，比如这个:https://zhuanlan.zhihu.com/p/648291684，看看能否调试出对应的风格。希望能帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693379352,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379895,"user_name":"昵称C","can_delete":false,"product_type":"c1","uid":1234963,"ip_address":"北京","ucode":"1BB2D537942DC2","user_header":"https://static001.geekbang.org/account/avatar/00/12/d8/13/082013bc.jpg","comment_is_top":false,"comment_ctime":1692679898,"is_pvip":false,"replies":[{"id":138509,"content":"你好。我个人的角度，换脸换衣服这类任务选择生成能力较强的真人模型，比如墨幽人造人、Realistic这类。关掉NSFW检测，可以参考这个链接：https:&#47;&#47;stackoverflow.com&#47;questions&#47;73828107&#47;how-to-fix-nsfw-error-for-stable-diffusion。 希望能够帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693382303,"ip_address":"北京","comment_id":379895,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师，您好。我想做一个换脸，换衣服的图生图小功能。有合适的开源模型推荐么？我在用的时候stable-diffusion-v1-5模型生成的内容是有不适宜检测的，怎么能去掉这个检测呢？","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626791,"discussion_content":"你好。我个人的角度，换脸换衣服这类任务选择生成能力较强的真人模型，比如墨幽人造人、Realistic这类。关掉NSFW检测，可以参考这个链接：https://stackoverflow.com/questions/73828107/how-to-fix-nsfw-error-for-stable-diffusion。 希望能够帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693382303,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379728,"user_name":"秋晨","can_delete":false,"product_type":"c1","uid":1247845,"ip_address":"日本","ucode":"5F2002818159D6","user_header":"https://static001.geekbang.org/account/avatar/00/13/0a/65/9cd6d109.jpg","comment_is_top":false,"comment_ctime":1692349398,"is_pvip":false,"replies":[{"id":138513,"content":"你好，感谢你的反馈。噪声图我们使用diffusion.sample(batch_size = 4)函数来完成生成，图像可视化部分使用的是matplotlib模块，具体代码可以参考Colab。这部分只是做了一个可视化，与AI绘画关联不大，就没有拿出来讲。再次感谢你的反馈。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1693383123,"ip_address":"北京","comment_id":379728,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"认识基础模块\n似乎缺失了讲解如何输出噪声图的代码","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626798,"discussion_content":"你好，感谢你的反馈。噪声图我们使用diffusion.sample(batch_size = 4)函数来完成生成，图像可视化部分使用的是matplotlib模块，具体代码可以参考Colab。这部分只是做了一个可视化，与AI绘画关联不大，就没有拿出来讲。再次感谢你的反馈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693383123,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379571,"user_name":"@二十一大叔","can_delete":false,"product_type":"c1","uid":1075954,"ip_address":"上海","ucode":"394A7E80C5034C","user_header":"https://static001.geekbang.org/account/avatar/00/10/6a/f2/8829a0b8.jpg","comment_is_top":false,"comment_ctime":1692115082,"is_pvip":false,"replies":[{"id":138297,"content":"你好。推荐使用load_safetensors的形式进行模型加载：https:&#47;&#47;huggingface.co&#47;docs&#47;diffusers&#47;using-diffusers&#47;using_safetensors#load-safetensors，model_id中指定自己的模型路径。希望能帮助到你。","user_name":"作者回复","user_name_real":"编辑","uid":2288210,"ctime":1692289234,"ip_address":"北京","comment_id":379571,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"老师，最后调用sd模型的代码，在本地运行时有没有办法去加载本地指定文件夹下的模型，而不是去下载huggingface中的模型，目前是会把模型仓库中的所有问价都缓存到本地，这样对于切换模型时非常的不友好","like_count":0,"discussions":[{"author":{"id":2288210,"avatar":"https://static001.geekbang.org/account/avatar/00/22/ea/52/c6f6ed5a.jpg","nickname":"南柯","note":"","ucode":"957ACF2DCCD2F7","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625892,"discussion_content":"你好。推荐使用load_safetensors的形式进行模型加载：https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors#load-safetensors，model_id中指定自己的模型路径。希望能帮助到你。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1692289234,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379383,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1691854969,"is_pvip":false,"replies":[{"id":138274,"content":"第一个问题，建议你说明一下你心目里对“垂直类模型”的理解，以及对“比较好的”定义，也可以借助谷歌搜过工具来看看。第二个问题，前面课程里确实提供过了，刚好是你复习回顾的好机会，你可以回到第一章第四节课找一下，如果找不到再提问。","user_name":"编辑回复","user_name_real":"编辑","uid":1501385,"ctime":1692167089,"ip_address":"北京","comment_id":379383,"utype":2}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100555001,"comment_content":"请教老师几个问题啊：\nQ1：有比较好的垂直类模型吗？推荐几个啊。\nQ2：开源社区网址是什么？麻烦提供一下。前面课程也许提供了，但难以逐个查找。都是在地铁上看的，当时没有记。麻烦老师了。","like_count":0,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":625761,"discussion_content":"第一个问题，建议你说明一下你心目里对“垂直类模型”的理解，以及对“比较好的”定义，也可以借助谷歌搜过工具来看看。第二个问题，前面课程里确实提供过了，刚好是你复习回顾的好机会，你可以回到第一章第四节课找一下，如果找不到再提问。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1692167090,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":8}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":379344,"user_name":"Toni","can_delete":false,"product_type":"c1","uid":3206957,"ip_address":"瑞士","ucode":"E6B2FACCC1E000","user_header":"https://static001.geekbang.org/account/avatar/00/30/ef/2d/757bb0d3.jpg","comment_is_top":false,"comment_ctime":1691758993,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"思考题: 在 Hugging Face 中选择SDXL1.0\nhttps:&#47;&#47;huggingface.co&#47;stabilityai&#47;stable-diffusion-xl-refiner-1.0\n代码如下:\n\npip install diffusers --upgrade\n\npip install invisible_watermark transformers accelerate safetensors\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load both base &amp; refiner\nbase = DiffusionPipeline.from_pretrained(\n    &quot;stabilityai&#47;stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.float16, variant=&quot;fp16&quot;, use_safetensors=True\n)\nbase.to(&quot;cuda&quot;)\nrefiner = DiffusionPipeline.from_pretrained(\n    &quot;stabilityai&#47;stable-diffusion-xl-refiner-1.0&quot;,\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant=&quot;fp16&quot;,\n)\nrefiner.to(&quot;cuda&quot;)\n\n# Define how many steps and what % of steps to be run on each experts (80&#47;20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = &quot;RAW photo,Childhood in Beijing Hutongs,70s,two little boys playing and chasing each other,boys are dressed in shorts and vests,and appear to be very happy,the background is street,several old houses,the color tone is somewhat yellowish-brown,8k,DSLR,soft light,high quality,film grain,Fujifilm XT3&quot;\nnegative_prompt=&quot;mutated hands, fused fingers, too many fingers, missing fingers, poorly drawn hands, blurry eyes, blurred iris, blurry face, poorly drawn face, mutation, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, out of frame, multiple faces, long neck, nsfw&quot;\n\n# run both experts\nimage = base(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type=&quot;latent&quot;,\n).images\nimage1 = refiner(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n\nimage1\n\n图发在[微信AI绘画专栏交流群]里了。","like_count":1},{"had_liked":false,"id":393449,"user_name":"黄尔林","can_delete":false,"product_type":"c1","uid":2837865,"ip_address":"广东","ucode":"31E17EACBB414A","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLwAlcUEico92QXNQXFHQibC8Tzfsmqv9jVAItUtL56icL7ahbRHfr3hnB40Ra5Wdibxj1LTfLyr2ZEtQ/132","comment_is_top":false,"comment_ctime":1723859419,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"请问市场那些 sd 大模型，比如麦橘，墨幽，realistic 等大模型，也是这种方式训练的吗","like_count":0},{"had_liked":false,"id":389549,"user_name":"奔跑的蚂蚁","can_delete":false,"product_type":"c1","uid":2379253,"ip_address":"浙江","ucode":"7348CA436144CB","user_header":"https://static001.geekbang.org/account/avatar/00/24/4d/f5/2e80aca6.jpg","comment_is_top":false,"comment_ctime":1712826729,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100555001,"comment_content":"sd微调 Counterfeit-V2.5这段\n\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(&quot;gsdf&#47;Counterfeit-V2.5&quot;)\n\n报错\n\nAttributeError                            Traceback (most recent call last)\n\n&lt;ipython-input-14-2f5c494bd124&gt; in &lt;cell line: 2&gt;()\n      1 import torch\n----&gt; 2 from diffusers import DiffusionPipeline\n      3 \n      4 pipeline = DiffusionPipeline.from_pretrained(&quot;gsdf&#47;Counterfeit-V2.5&quot;)\n\n7 frames\n\n&#47;usr&#47;local&#47;lib&#47;python3.10&#47;dist-packages&#47;jax&#47;_src&#47;deprecations.py in getattr(name)\n     52       warnings.warn(message, DeprecationWarning, stacklevel=2)\n     53       return fn\n---&gt; 54     raise AttributeError(f&quot;module {module!r} has no attribute {name!r}&quot;)\n     55 \n     56   return getattr\n\nAttributeError: module &#39;jax.random&#39; has no attribute &#39;KeyArray&#39;","like_count":0}]}