{"id":418079,"title":"03 | RDD常用算子（一）：RDD内部的数据转换","content":"<p>你好，我是吴磊。</p><p>在上一讲的最后，我们用一张表格整理了Spark<a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">官网</a>给出的RDD算子。想要在Spark之上快速实现业务逻辑，理解并掌握这些算子无疑是至关重要的。</p><p>因此，在接下来的几讲，我将带你一起梳理这些常见算子的用法与用途。不同的算子，就像是厨房里的炒勺、铲子、刀具和各式各样的锅碗瓢盆，只有熟悉了这些“厨具”的操作方法，才能在客人点餐的时候迅速地做出一桌好菜。</p><p>今天这一讲，我们先来学习同一个RDD内部的数据转换。掌握RDD常用算子是做好Spark应用开发的基础，而数据转换类算子则是基础中的基础，因此我们优先来学习这类RDD算子。</p><p>在这些算子中，我们重点讲解的就是map、mapPartitions、flatMap、filter。这4个算子几乎囊括了日常开发中99%的数据转换场景，剩下的mapPartitionsWithIndex，我把它留给你作为课后作业去探索。</p><p><img src=\"https://static001.geekbang.org/resource/image/a3/88/a3ec138b50456604bae8ce22cdf56788.jpg?wh=1599x1885\" alt=\"图片\" title=\"RDD算子分类表\"></p><p>俗话说，巧妇难为无米之炊，要想玩转厨房里的厨具，我们得先准备好米、面、油这些食材。学习RDD算子也是一样，要想动手操作这些算子，咱们得先有RDD才行。</p><p>所以，接下来我们就一起来看看RDD是怎么创建的。</p><h2>创建RDD</h2><p>在Spark中，创建RDD的典型方式有两种：</p><!-- [[[read_end]]] --><ul>\n<li>通过SparkContext.parallelize在<strong>内部数据</strong>之上创建RDD；</li>\n<li>通过SparkContext.textFile等API从<strong>外部数据</strong>创建RDD。</li>\n</ul><p>这里的内部、外部是相对应用程序来说的。开发者在Spark应用中自定义的各类数据结构，如数组、列表、映射等，都属于“内部数据”；而“外部数据”指代的，是Spark系统之外的所有数据形式，如本地文件系统或是分布式文件系统中的数据，再比如来自其他大数据组件（Hive、Hbase、RDBMS等）的数据。</p><p>第一种创建方式的用法非常简单，只需要用parallelize函数来封装内部数据即可，比如下面的例子：</p><pre><code class=\"language-scala\">import org.apache.spark.rdd.RDD\nval words: Array[String] = Array(\"Spark\", \"is\", \"cool\")\nval rdd: RDD[String] = sc.parallelize(words)\n</code></pre><p>你可以在spark-shell中敲入上述代码，来直观地感受parallelize创建RDD的过程。通常来说，在Spark应用内定义体量超大的数据集，其实都是不太合适的，因为数据集完全由Driver端创建，且创建完成后，还要在全网范围内跨节点、跨进程地分发到其他Executors，所以往往会带来性能问题。因此，parallelize API的典型用法，是在“小数据”之上创建RDD。</p><p>要想在真正的“大数据”之上创建RDD，我们还得依赖第二种创建方式，也就是通过SparkContext.textFile等API从<strong>外部数据</strong>创建RDD。由于textFile API比较简单，而且它在日常的开发中出现频率比较高，因此我们使用textFile API来创建RDD。在后续对各类RDD算子讲解的过程中，我们都会使用textFile API从文件系统创建RDD。</p><p>为了保持讲解的连贯性，我们还是使用第一讲中的源文件wikiOfSpark.txt来创建RDD，代码实现如下所示：</p><pre><code class=\"language-scala\">import org.apache.spark.rdd.RDD\nval rootPath: String = _\nval file: String = s\"${rootPath}/wikiOfSpark.txt\"\n// 读取文件内容\nval lineRDD: RDD[String] = spark.sparkContext.textFile(file)\n</code></pre><p>好啦，创建好了RDD，我们就有了可以下锅的食材。接下来，咱们就要正式地走进厨房，把铲子和炒勺挥起来啦。</p><h2>RDD内的数据转换</h2><p>首先，我们先来认识一下map算子。毫不夸张地说，在所有的RDD算子中，map“出场”的概率是最高的。因此，我们必须要掌握map的用法与注意事项。</p><h3>map：以元素为粒度的数据转换</h3><p>我们先来说说map算子的用法：<strong>给定映射函数f，map(f)以元素为粒度对RDD做数据转换。</strong>其中f可以是带有明确签名的带名函数，也可以是匿名函数，它的形参类型必须与RDD的元素类型保持一致，而输出类型则任由开发者自行决定。</p><p>这种照本宣科的介绍听上去难免会让你有点懵，别着急，接下来我们用些小例子来更加直观地展示map的用法。</p><p>在<a href=\"https://time.geekbang.org/column/article/415209\">第一讲</a>的Word Count示例中，我们使用如下代码，把包含单词的RDD转换成元素为（Key，Value）对的RDD，后者统称为Paired RDD。</p><pre><code class=\"language-scala\">// 把普通RDD转换为Paired RDD\nval cleanWordRDD: RDD[String] = _ // 请参考第一讲获取完整代码\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))\n</code></pre><p>在上面的代码实现中，传递给map算子的形参，即：word =&gt; （word，1），就是我们上面说的映射函数f。只不过，这里f是以匿名函数的方式进行定义的，其中左侧的word表示匿名函数f的输入形参，而右侧的（word，1）则代表函数f的输出结果。</p><p>如果我们把匿名函数变成带名函数的话，可能你会看的更清楚一些。这里我用一段代码重新定义了带名函数f。</p><pre><code class=\"language-scala\">// 把RDD元素转换为（Key，Value）的形式\n&nbsp;\n// 定义映射函数f\ndef f(word: String): (String, Int) = {\nreturn (word, 1)\n}\n&nbsp;\nval cleanWordRDD: RDD[String] = _ // 请参考第一讲获取完整代码\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(f)\n</code></pre><p>可以看到，我们使用Scala的def语法，明确定义了带名映射函数f，它的计算逻辑与刚刚的匿名函数是一致的。在做RDD数据转换的时候，我们只需把函数f传递给map算子即可。不管f是匿名函数，还是带名函数，map算子的转换逻辑都是一样的，你不妨把以上两种实现方式分别敲入到spark-shell，去验证执行结果的一致性。</p><p>到这里为止，我们就掌握了map算子的基本用法。现在你就可以定义任意复杂的映射函数f，然后在RDD之上通过调用map(f)去翻着花样地做各种各样的数据转换。</p><p>比如，通过定义如下的映射函数f，我们就可以改写Word Count的计数逻辑，也就是把“Spark”这个单词的统计计数权重提高一倍：</p><pre><code class=\"language-scala\">// 把RDD元素转换为（Key，Value）的形式\n&nbsp;\n// 定义映射函数f\ndef f(word: String): (String, Int) = {\nif (word.equals(\"Spark\")) { return (word, 2) }\nreturn (word, 1)\n}\n&nbsp;\nval cleanWordRDD: RDD[String] = _ // 请参考第一讲获取完整代码\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(f)\n</code></pre><p>尽管map算子足够灵活，允许开发者自由定义转换逻辑。不过，就像我们刚刚说的，map(f)是以元素为粒度对RDD做数据转换的，在某些计算场景下，这个特点会严重影响执行效率。为什么这么说呢？我们来看一个具体的例子。</p><p>比方说，我们把Word Count的计数需求，从原来的对单词计数，改为对单词的哈希值计数，在这种情况下，我们的代码实现需要做哪些改动呢？我来示范一下：</p><pre><code class=\"language-scala\">// 把普通RDD转换为Paired RDD\n&nbsp;\nimport java.security.MessageDigest\n&nbsp;\nval cleanWordRDD: RDD[String] = _ // 请参考第一讲获取完整代码\n&nbsp;\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map{ word =&gt;\n  // 获取MD5对象实例\n  val md5 = MessageDigest.getInstance(\"MD5\")\n  // 使用MD5计算哈希值\n  val hash = md5.digest(word.getBytes).mkString\n  // 返回哈希值与数字1的Pair\n  (hash, 1)\n}\n</code></pre><p>由于map(f)是以元素为单元做转换的，那么对于RDD中的每一条数据记录，我们都需要实例化一个MessageDigest对象来计算这个元素的哈希值。</p><p>在工业级生产系统中，一个RDD动辄包含上百万甚至是上亿级别的数据记录，如果处理每条记录都需要事先创建MessageDigest，那么实例化对象的开销就会聚沙成塔，不知不觉地成为影响执行效率的罪魁祸首。</p><p>那么问题来了，有没有什么办法，能够让Spark在更粗的数据粒度上去处理数据呢？还真有，mapPartitions和mapPartitionsWithIndex这对“孪生兄弟”就是用来解决类似的问题。相比mapPartitions，mapPartitionsWithIndex仅仅多出了一个数据分区索引，因此接下来我们把重点放在mapPartitions上面。</p><h3>mapPartitions：以数据分区为粒度的数据转换</h3><p>按照介绍算子的惯例，我们还是先来说说mapPartitions的用法。mapPartitions，顾名思义，就是<strong>以数据分区为粒度，使用映射函数f对RDD进行数据转换</strong>。对于上述单词哈希值计数的例子，我们结合后面的代码，来看看如何使用mapPartitions来改善执行性能：</p><pre><code class=\"language-scala\">// 把普通RDD转换为Paired RDD\n&nbsp;\nimport java.security.MessageDigest\n&nbsp;\nval cleanWordRDD: RDD[String] = _ // 请参考第一讲获取完整代码\n&nbsp;\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.mapPartitions( partition =&gt; {\n  // 注意！这里是以数据分区为粒度，获取MD5对象实例\n  val md5 = MessageDigest.getInstance(\"MD5\")\n  val newPartition = partition.map( word =&gt; {\n  // 在处理每一条数据记录的时候，可以复用同一个Partition内的MD5对象\n    (md5.digest(word.getBytes()).mkString,1)\n  })\n  newPartition\n})\n</code></pre><p>可以看到，在上面的改进代码中，mapPartitions以数据分区（匿名函数的形参partition）为粒度，对RDD进行数据转换。具体的数据处理逻辑，则由代表数据分区的形参partition进一步调用map(f)来完成。你可能会说：“partition. map(f)仍然是以元素为粒度做映射呀！这和前一个版本的实现，有什么本质上的区别呢？”</p><p>仔细观察，你就会发现，相比前一个版本，我们把实例化MD5对象的语句挪到了map算子之外。如此一来，以数据分区为单位，实例化对象的操作只需要执行一次，而同一个数据分区中所有的数据记录，都可以共享该MD5对象，从而完成单词到哈希值的转换。</p><p>通过下图的直观对比，你会发现，以数据分区为单位，mapPartitions只需实例化一次MD5对象，而map算子却需要实例化多次，具体的次数则由分区内数据记录的数量来决定。</p><p><img src=\"https://static001.geekbang.org/resource/image/c7/8d/c76be8ff89f1c37e52e9f17b66bf398d.jpg?wh=1920x779\" alt=\"图片\" title=\"map与mapPartitions的区别\"></p><p>对于一个有着上百万条记录的RDD来说，其数据分区的划分往往是在百这个量级，因此，相比map算子，mapPartitions可以显著降低对象实例化的计算开销，这对于Spark作业端到端的执行性能来说，无疑是非常友好的。</p><p>实际上。除了计算哈希值以外，对于数据记录来说，凡是可以共享的操作，都可以用mapPartitions算子进行优化。这样的共享操作还有很多，比如创建用于连接远端数据库的Connections对象，或是用于连接Amazon S3的文件系统句柄，再比如用于在线推理的机器学习模型，等等，不一而足。你不妨结合实际工作场景，把你遇到的共享操作整理到留言区，期待你的分享。</p><p>相比mapPartitions，mapPartitionsWithIndex仅仅多出了一个数据分区索引，这个数据分区索引可以为我们获取分区编号，当你的业务逻辑中需要使用到分区编号的时候，不妨考虑使用这个算子来实现代码。除了这个额外的分区索引以外，mapPartitionsWithIndex在其他方面与mapPartitions是完全一样的。</p><p>介绍完map与mapPartitions算子之后，接下来，我们趁热打铁，再来看一个与这两者功能类似的算子：flatMap。</p><h3><strong>flatMap：从元素到集合、再从集合到元素</strong></h3><p>flatMap其实和map与mapPartitions算子类似，在功能上，与map和mapPartitions一样，flatMap也是用来做数据映射的，在实现上，对于给定映射函数f，flatMap(f)以元素为粒度，对RDD进行数据转换。</p><p>不过，与前两者相比，flatMap的映射函数f有着显著的不同。对于map和mapPartitions来说，其映射函数f的类型，都是（元素） =&gt; （元素），即元素到元素。而flatMap映射函数f的类型，是（元素） =&gt; （集合），即元素到集合（如数组、列表等）。因此，flatMap的映射过程在逻辑上分为两步：</p><ul>\n<li>以元素为单位，创建集合；</li>\n<li>去掉集合“外包装”，提取集合元素。</li>\n</ul><p>这么说比较抽象，我们还是来举例说明。假设，我们再次改变Word Count的计算逻辑，由原来统计单词的计数，改为统计相邻单词共现的次数，如下图所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/b9/6a/b9feyy652bb60d7d30a25e3e122a9f6a.jpg?wh=1920x764\" alt=\"图片\" title=\"变更Word Count计算逻辑\"></p><p>对于这样的计算逻辑，我们该如何使用flatMap进行实现呢？这里我们先给出代码实现，然后再分阶段地分析flatMap的映射过程：</p><pre><code class=\"language-scala\">// 读取文件内容\nval lineRDD: RDD[String] = _ // 请参考第一讲获取完整代码\n// 以行为单位提取相邻单词\nval wordPairRDD: RDD[String] = lineRDD.flatMap( line =&gt; {\n  // 将行转换为单词数组\n  val words: Array[String] = line.split(\" \")\n  // 将单个单词数组，转换为相邻单词数组\n  for (i &lt;- 0 until words.length - 1) yield words(i) + \"-\" + words(i+1)\n})\n</code></pre><p>在上面的代码中，我们采用匿名函数的形式，来提供映射函数f。这里f的形参是String类型的line，也就是源文件中的一行文本，而f的返回类型是Array[String]，也就是String类型的数组。在映射函数f的函数体中，我们先用split语句把line转化为单词数组，然后再用for循环结合yield语句，依次把单个的单词，转化为相邻单词词对。</p><p>注意，for循环返回的依然是数组，也即类型为Array[String]的词对数组。由此可见，函数f的类型是（String） =&gt; （Array[String]），也就是刚刚说的第一步，<strong>从元素到集合</strong>。但如果我们去观察转换前后的两个RDD，也就是lineRDD和wordPairRDD，会发现它们的类型都是RDD[String]，换句话说，它们的元素类型都是String。</p><p>回顾map与mapPartitions这两个算子，我们会发现，转换前后RDD的元素类型，与映射函数f的类型是一致的。但在flatMap这里，却出现了RDD元素类型与函数类型不一致的情况。这是怎么回事呢？其实呢，这正是flatMap的“奥妙”所在，为了让你直观地理解flatMap的映射过程，我画了一张示意图，如下所示：</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/bd/a6bcd12fbc377405557c1aaf63cd24bd.jpg?wh=1920x840\" alt=\"图片\"></p><p>不难发现，映射函数f的计算过程，对应着图中的步骤1与步骤2，每行文本都被转化为包含相邻词对的数组。紧接着，flatMap去掉每个数组的“外包装”，提取出数组中类型为String的词对元素，然后以词对为单位，构建新的数据分区，如图中步骤3所示。这就是flatMap映射过程的第二步：<strong>去掉集合“外包装”，提取集合元素</strong>。</p><p>得到包含词对元素的wordPairRDD之后，我们就可以沿用Word Count的后续逻辑，去计算相邻词汇的共现次数。你不妨结合文稿中的代码与第一讲中Word Count的代码，去实现完整版的“相邻词汇计数统计”。</p><h3>filter：过滤RDD</h3><p>在今天的最后，我们再来学习一下，与map一样常用的算子：filter。filter，顾名思义，这个算子的作用，是对RDD进行过滤。就像是map算子依赖其映射函数一样，filter算子也需要借助一个判定函数f，才能实现对RDD的过滤转换。</p><p>所谓判定函数，它指的是类型为（RDD元素类型） =&gt; （Boolean）的函数。可以看到，判定函数f的形参类型，必须与RDD的元素类型保持一致，而f的返回结果，只能是True或者False。在任何一个RDD之上调用filter(f)，其作用是保留RDD中满足f（也就是f返回True）的数据元素，而过滤掉不满足f（也就是f返回False）的数据元素。</p><p>老规矩，我们还是结合示例来讲解filter算子与判定函数f。</p><p>在上面flatMap例子的最后，我们得到了元素为相邻词汇对的wordPairRDD，它包含的是像“Spark-is”、“is-cool”这样的字符串。为了仅保留有意义的词对元素，我们希望结合标点符号列表，对wordPairRDD进行过滤。例如，我们希望过滤掉像“Spark-&amp;”、“|-data”这样的词对。</p><p>掌握了filter算子的用法之后，要实现这样的过滤逻辑，我相信你很快就能写出如下的代码实现：</p><pre><code class=\"language-scala\">// 定义特殊字符列表\nval list: List[String] = List(\"&amp;\", \"|\", \"#\", \"^\", \"@\")\n&nbsp;\n// 定义判定函数f\ndef f(s: String): Boolean = {\nval words: Array[String] = s.split(\"-\")\nval b1: Boolean = list.contains(words(0))\nval b2: Boolean = list.contains(words(1))\nreturn !b1 &amp;&amp; !b2 // 返回不在特殊字符列表中的词汇对\n}\n&nbsp;\n// 使用filter(f)对RDD进行过滤\nval cleanedPairRDD: RDD[String] = wordPairRDD.filter(f)\n</code></pre><p>掌握了filter算子的用法之后，你就可以定义任意复杂的判定函数f，然后在RDD之上通过调用filter(f)去变着花样地做数据过滤，从而满足不同的业务需求。</p><h2>重点回顾</h2><p>好啦，到此为止，关于RDD内数据转换的几个算子，我们就讲完了，我们一起来做个总结。今天这一讲，你需要掌握map、mapPartitions、flatMap和filter这4个算子的作用和具体用法。</p><p>首先，我们讲了map算子的用法，它允许开发者自由地对RDD做各式各样的数据转换，给定映射函数f，map(f)以元素为粒度对RDD做数据转换。其中f可以是带名函数，也可以是匿名函数，它的形参类型必须与RDD的元素类型保持一致，而输出类型则任由开发者自行决定。</p><p>为了提升数据转换的效率，Spark提供了以数据分区为粒度的mapPartitions算子。mapPartitions的形参是代表数据分区的partition，它通过在partition之上再次调用map(f)来完成数据的转换。相比map，mapPartitions的优势是以数据分区为粒度初始化共享对象，这些共享对象在我们日常的开发中很常见，比如数据库连接对象、S3文件句柄、机器学习模型等等。</p><p>紧接着，我们介绍了flatMap算子。flatMap的映射函数f比较特殊，它的函数类型是（元素） =&gt; （集合），这里集合指的是像数组、列表这样的数据结构。因此，flatMap的映射过程在逻辑上分为两步，这一点需要你特别注意：</p><ul>\n<li><strong>以元素为单位，创建集合；</strong></li>\n<li><strong>去掉集合“外包装”，提取集合元素。</strong></li>\n</ul><p>最后，我们学习了filter算子，filter算子的用法与map很像，它需要借助判定函数f来完成对RDD的数据过滤。判定函数的类型必须是（RDD元素类型） =&gt; （Boolean），也就是形参类型必须与RDD的元素类型保持一致，返回结果类型则必须是布尔值。RDD中的元素是否能够得以保留，取决于判定函数f的返回值是True还是False。</p><p>虽然今天我们只学了4个算子，但这4个算子在日常开发中的出现频率非常之高。掌握了这几个简单的RDD算子，你几乎可以应对RDD中90%的数据转换场景。希望你对这几个算子多多加以练习，从而在日常的开发工作中学以致用。</p><h2><strong>每课一练</strong></h2><p>讲完了正课，我来给你留3个思考题：</p><p>1.请你结合<a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">官网</a>的介绍，自学mapPartitionsWithIndex算子。请你说一说，在哪些场景下可能会用到这个算子？</p><p>2.对于我们今天学过的4个算子，再加上没有详细解释的mapPartitionsWithIndex，你能说说，它们之间有哪些共性或是共同点吗？</p><p>3.你能说一说，在日常的工作中，还遇到过哪些可以在mapPartitions中初始化的共享对象呢？</p><p>欢迎你在评论区回答这些练习题。你也可以把这一讲分享给更多的朋友或者同事，和他们一起讨论讨论，交流是学习的催化剂。我在评论区等你。</p>","comments":[{"had_liked":false,"id":312356,"user_name":"qinsi","can_delete":false,"product_type":"c1","uid":1667175,"ip_address":"","ucode":"090D9C4068FF12","user_header":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","comment_is_top":false,"comment_ctime":1631768119,"is_pvip":true,"replies":[{"id":"113335","content":"非常好的问题~ 先赞一个 👍<br><br>先说结论，你说的没错，通过在Driver端创建全局变量，然后在map中进行引用，也可以达到mapPartitions同样的效果。原理就是你说的函数闭包，其实本质上，它就是个对象，包含了外部数据结构的函数对象。<br><br>但是，需要注意的是，通过这种方式创建的闭包，一定要保证Driver端创建的全局变量是可以序列化的，也就是实现了Serializable接口。只有满足这个前提，Spark才能在这个闭包之上创建分布式任务，否则会报“Task not serializable”错误。<br><br>这个错误其实很好理解，要把Driver创建的对象带到Executors，这个对象一定是要可序列化的才行。不过遗憾的是，很多共享对象，比如咱们本讲的MD5，都没有实现Serializable接口，所以Driver端创建的MD5实例，Spark“带不动”（没法从Driver带到Executors），这一点你不妨动手试一试~<br><br>不过这个点提得非常好，对于那些自定义的Class，我们可以让它实现Serializable接口，从而可以轻松地在Driver创建Spark可以“带的动”的全局变量。<br><br>说到这里，我追问老弟一句：在Driver创建MD5实例，map中直接引用，Spark会报“Task not Serializable”；那为什么在mapPartitions里面创建MD5实例，map引用这个对象，Spark却没有报“Task not Serializable”错误呢？<br><br>老弟不妨再想一想，咱们评论区继续讨论~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632044078,"ip_address":"","comment_id":312356,"utype":1}],"discussion_count":11,"race_medal":0,"score":"134775754295","product_id":100090001,"comment_content":"不是很懂spark。mapPartitions()那里，光从代码上来看的话，在map()的闭包里可以访问到外面mapPartitions()闭包里的同一个md5实例，从而达到共享实例的效果。那么有没有可能在最外层创建一个全局的md5实例，这样就算只用map()，在闭包里访问的也是这同一个实例？这样会有什么问题吗？或者说在这种情况下mapPartitions()相比map()还有什么优势？","like_count":32,"discussions":[{"author":{"id":1667175,"avatar":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","nickname":"qinsi","note":"","ucode":"090D9C4068FF12","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394784,"discussion_content":"感谢回复。猜测只要闭包没有捕获无法序列化的变量的话，闭包本身就是可以被序列化的。例子里传给mapPartitions的闭包就满足这样的条件。我尝试改写map的例子：\n\nval md5thunk = () => MessageDigest.getInstance(&#34;MD5&#34;)\n\nval kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word => ((md5thunk().digest(word.getBytes).mkString, 1)))\n\n这样也是可以的，证明这样的闭包可以被序列化，并被Executors使用。但这样写就跟在map的闭包里直接初始化没啥区别了。所以对于无法序列化的对象，在mapPartitions的闭包里初始化就比在map里初始化高效。","likes_number":5,"is_delete":false,"is_hidden":false,"ctime":1632052055,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":5,"child_discussions":[{"author":{"id":1780640,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/2b/a0/9ada43b9.jpg","nickname":"Pluto","note":"","ucode":"719EBEDE17FC76","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1667175,"avatar":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","nickname":"qinsi","note":"","ucode":"090D9C4068FF12","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":396164,"discussion_content":"调md5thunk方法应该也是每次返回了一个新的对象吧?","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632400166,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":394784,"ip_address":""},"score":396164,"extra":""},{"author":{"id":1667175,"avatar":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","nickname":"qinsi","note":"","ucode":"090D9C4068FF12","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1780640,"avatar":"https://static001.geekbang.org/account/avatar/00/1b/2b/a0/9ada43b9.jpg","nickname":"Pluto","note":"","ucode":"719EBEDE17FC76","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":396393,"discussion_content":"对","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632414976,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":396164,"ip_address":""},"score":396393,"extra":""},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1667175,"avatar":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","nickname":"qinsi","note":"","ucode":"090D9C4068FF12","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":399140,"discussion_content":"正解~ 分析得非常到位~ 满分💯","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632910618,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":394784,"ip_address":""},"score":399140,"extra":""}]},{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526949,"discussion_content":"非常好的问题~ 先赞一个 👍\n\n先说结论，你说的没错，通过在Driver端创建全局变量，然后在map中进行引用，也可以达到mapPartitions同样的效果。原理就是你说的函数闭包，其实本质上，它就是个对象，包含了外部数据结构的函数对象。\n\n但是，需要注意的是，通过这种方式创建的闭包，一定要保证Driver端创建的全局变量是可以序列化的，也就是实现了Serializable接口。只有满足这个前提，Spark才能在这个闭包之上创建分布式任务，否则会报“Task not serializable”错误。\n\n这个错误其实很好理解，要把Driver创建的对象带到Executors，这个对象一定是要可序列化的才行。不过遗憾的是，很多共享对象，比如咱们本讲的MD5，都没有实现Serializable接口，所以Driver端创建的MD5实例，Spark“带不动”（没法从Driver带到Executors），这一点你不妨动手试一试~\n\n不过这个点提得非常好，对于那些自定义的Class，我们可以让它实现Serializable接口，从而可以轻松地在Driver创建Spark可以“带的动”的全局变量。\n\n说到这里，我追问老弟一句：在Driver创建MD5实例，map中直接引用，Spark会报“Task not Serializable”；那为什么在mapPartitions里面创建MD5实例，map引用这个对象，Spark却没有报“Task not Serializable”错误呢？\n\n老弟不妨再想一想，咱们评论区继续讨论~","likes_number":3,"is_delete":false,"is_hidden":false,"ctime":1632044078,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":1184678,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJDgV2qia6eAL7Fb4egX3odViclRRwOlkfCBrjhU9lLeib90KGkIDjdddSibNVs47N90L36Brgnr6ppiag/132","nickname":"ddww","note":"","ucode":"2871112FC9B3F7","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":573399,"discussion_content":"map算子和mapPartitions算子里用到了map是两个不同的东西。mapPartitions本质就是一次计算一个分区中的计算，map就相当于遍历。这个两个没有可比性。还是老师想问。一个类没有序列化，在算子中直接调用。没有异常。而当做广播变量时，在算子中直接调用会异常。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1653388606,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":526949,"ip_address":""},"score":573399,"extra":""}]},{"author":{"id":2767491,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg","nickname":"火炎焱燚","note":"","ucode":"DB11784DD94059","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394740,"discussion_content":"同问，spark里面有没有这种全局共享的变量了？比如将md5实例设置为全局变量不就行了。。。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632025512,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2767491,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg","nickname":"火炎焱燚","note":"","ucode":"DB11784DD94059","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399142,"discussion_content":"广播变量","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632910728,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":394740,"ip_address":""},"score":399142,"extra":""}]},{"author":{"id":2754963,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/hSGZCTvHTKaJH5WibTcC15qDKYpGdgdMFEV6mwcslOFoADP6skWCpBiae2ykkaBFDaexEY9xXBPhZMAGmj1nW8vA/132","nickname":"进","note":"","ucode":"B971F478E3BE8E","race_medal":1,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394237,"discussion_content":"其实我也有一个疑问. 如果mapParittions是分片里面的公共调用优先，那这里是不是根本就不是闭包访问的外部变量.\n貌似这里深究的话.还是没懂","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631797503,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313550,"user_name":"Unknown element","can_delete":false,"product_type":"c1","uid":2028277,"ip_address":"","ucode":"34A129800D0238","user_header":"https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg","comment_is_top":false,"comment_ctime":1632490886,"is_pvip":false,"replies":[{"id":"113591","content":"Perfect，正解！就是这么回事~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632558144,"ip_address":"","comment_id":313550,"utype":1}],"discussion_count":1,"race_medal":0,"score":"44582163846","product_id":100090001,"comment_content":"老师您回答一楼的问题时提到的序列化意思是不是对象在不同节点间传输的时候只能序列化为字符串传输？如果是的话那我觉得 在mapPartitions里面创建MD5实例，map引用这个对象，Spark却没有报“Task not Serializable”错误 是因为driver把这段代码分发到了各个executor，而创建对象这个工作是由executor完成的，所以不会报错？","like_count":11,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527388,"discussion_content":"Perfect，正解！就是这么回事~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632558144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312972,"user_name":"Alvin-L","can_delete":false,"product_type":"c1","uid":1603052,"ip_address":"","ucode":"5AC96AAB75B720","user_header":"https://static001.geekbang.org/account/avatar/00/18/75/ec/c60b29f5.jpg","comment_is_top":false,"comment_ctime":1632159829,"is_pvip":false,"replies":[{"id":"113454","content":"两份代码都是满分💯，👍<br><br>感谢老弟分享、贡献Python代码，后面我们一起把它整理到GitHub里面去~<br><br>感谢！！！","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632327719,"ip_address":"","comment_id":312972,"utype":1}],"discussion_count":1,"race_medal":0,"score":"23106996309","product_id":100090001,"comment_content":"Python版,虽然能跑,但是不知道对不对:<br>#哈希值计数<br>``` <br>from hashlib import md5<br>from pyspark import SparkContext<br><br><br>def f(partition):<br>    m = md5()<br>    for word in partition:<br>        m.update(word.encode(&quot;utf8&quot;))<br>        yield m.hexdigest()<br><br><br>lineRDD = SparkContext().textFile(&quot;.&#47;wikiOfSpark.txt&quot;)<br>kvRDD = (<br>    lineRDD.flatMap(lambda line: line.split(&quot; &quot;))<br>    .filter(lambda word: word != &quot;&quot;)<br>    .mapPartitions(f)<br>    .map(lambda word: (word, 1))<br>)<br><br>kvRDD.foreach(print)<br><br>``` <br><br>#相邻+过滤特殊字符<br>``` <br>from pyspark import SparkContext<br><br># 定义特殊字符列表<br>special_char_list = [&quot;&amp;&quot;, &quot;|&quot;, &quot;#&quot;, &quot;^&quot;, &quot;@&quot;]<br># 定义判定函数f<br>def f(s):<br>    words = s.split(&quot;-&quot;)<br>    b1 = words[0] in special_char_list<br>    b2 = words[1] in special_char_list<br>    return (not b1) and (not b2)<br><br><br># 定义拼接函数word_pair<br>def word_pair(line):<br>    words = line.split(&quot; &quot;)<br>    for i in range(len(words) - 1):<br>        yield words[i] + &quot;-&quot; + words[i + 1]<br><br><br>lineRDD = SparkContext().textFile(&quot;.&#47;wikiOfSpark.txt&quot;)<br>cleanedPairRDD = lineRDD.flatMap(word_pair).filter(f)<br>cleanedPairRDD.foreach(print)<br><br>``` ","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527186,"discussion_content":"两份代码都是满分💯，👍\n\n感谢老弟分享、贡献Python代码，后面我们一起把它整理到GitHub里面去~\n\n感谢！！！","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632327719,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313969,"user_name":"Geek_eb2d3d","can_delete":false,"product_type":"c1","uid":1513922,"ip_address":"","ucode":"76A30ED501CA6A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKw8ictgYcqf6iaicPTOyZ2CR7iaVeeztZLZrLdrvuJib46ibZgibkhSYg85xZAglBhhsO7bY4fz5YQ8icbMA/132","comment_is_top":false,"comment_ctime":1632761672,"is_pvip":false,"replies":[{"id":"113781","content":"不可以，严禁这么做。<br><br>RDD里面，禁止嵌套定义新的RDD，这个是Spark分布式开发的大忌。<br><br>Spark的开发模式、或者是开发规范，是利用transformation，来完成分布式数据集之间的转换，从而达到处理数据的目的～ <br><br>RDD内嵌套RDD，这种属于单机思维哈，必须要跳出来～<br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632878457,"ip_address":"","comment_id":313969,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18812630856","product_id":100090001,"comment_content":"老师，我在 map 里面使用 SparkContext 或 SparkSession 创建新的 RDD，这样是可以的么？","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527532,"discussion_content":"不可以，严禁这么做。\n\nRDD里面，禁止嵌套定义新的RDD，这个是Spark分布式开发的大忌。\n\nSpark的开发模式、或者是开发规范，是利用transformation，来完成分布式数据集之间的转换，从而达到处理数据的目的～ \n\nRDD内嵌套RDD，这种属于单机思维哈，必须要跳出来～\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632878457,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":316133,"user_name":"Geek_a30533","can_delete":false,"product_type":"c1","uid":2764866,"ip_address":"","ucode":"57B31B311C763A","user_header":"","comment_is_top":false,"comment_ctime":1634175107,"is_pvip":false,"replies":[{"id":"114580","content":"对的~ <br><br>确实，Scala的语法比较简洁（偷懒），return不是必须的，如果函数体没有return，那么最后一行代码的输出，就是整个函数的返回值~<br><br>具体到这个匿名函数，它的返回结果就是你说的Array[String]，后续flatMap再把这里的Array展平","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634287823,"ip_address":"","comment_id":316133,"utype":1}],"discussion_count":1,"race_medal":0,"score":"10224109699","product_id":100090001,"comment_content":"对scala的函数定义格式不是很清楚，那边绕了好几次，有一个小疑问，在flatMap里的匿名函数f<br><br>line =&gt; {  <br>\t\t\tval words: Array[String] = line.split(&quot; &quot;)  <br>\t\t\tfor (i &lt;- 0 until words.length - 1) yield words(i) + &quot;-&quot; + words(i+1)<br>\t\t}<br><br>只定义了形参是line，那出参是整个花括号么？主要是没有return，让我一下迷了，难道是最后一个是Array[String]所以返回值就是这个？不用声明吗？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528278,"discussion_content":"对的~ \n\n确实，Scala的语法比较简洁（偷懒），return不是必须的，如果函数体没有return，那么最后一行代码的输出，就是整个函数的返回值~\n\n具体到这个匿名函数，它的返回结果就是你说的Array[String]，后续flatMap再把这里的Array展平","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634287823,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312366,"user_name":"大叮当","can_delete":false,"product_type":"c1","uid":1181578,"ip_address":"","ucode":"418EEFBD0C5A85","user_header":"https://static001.geekbang.org/account/avatar/00/12/07/8a/4bef6202.jpg","comment_is_top":false,"comment_ctime":1631775496,"is_pvip":false,"replies":[{"id":"113200","content":"Scala语法确实比较灵活，一般来说，简单表达式用()，复杂表达式用{}。<br><br>比如，简单函数体：(x =&gt; x + 1)、(_ + 1)，等等；<br>复杂函数体：{case x: String =&gt; “一大堆关于x的转换” }","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631804383,"ip_address":"","comment_id":312366,"utype":1}],"discussion_count":4,"race_medal":0,"score":"10221710088","product_id":100090001,"comment_content":"同问老师,AIK问的问题，什么时候用小括号什么时候用花括号啊，感觉scala实在有点过于灵活","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526952,"discussion_content":"Scala语法确实比较灵活，一般来说，简单表达式用()，复杂表达式用{}。\n\n比如，简单函数体：(x =&amp;gt; x + 1)、(_ + 1)，等等；\n复杂函数体：{case x: String =&amp;gt; “一大堆关于x的转换” }","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631804383,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2759990,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/1d/36/a0a283f8.jpg","nickname":"千江有水千江月","note":"","ucode":"23F5A65024CB67","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":407011,"discussion_content":"你其实都可以用花括号表示，只是比较简单的，可以用小括号。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634891608,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1690884,"avatar":"https://static001.geekbang.org/account/avatar/00/19/cd/04/e27b7803.jpg","nickname":"小新","note":"","ucode":"DCAD04665E2CF8","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2759990,"avatar":"https://static001.geekbang.org/account/avatar/00/2a/1d/36/a0a283f8.jpg","nickname":"千江有水千江月","note":"","ucode":"23F5A65024CB67","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":534422,"discussion_content":"case是啥意思？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1638185354,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":407011,"ip_address":""},"score":534422,"extra":""},{"author":{"id":2044188,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/bNQ1xdysiatMnMYpibXw70cpWPkuA2JXiaR0eDq7dia9oBic2z3SqFY84aRnEPH3QMgSklE6JSMZAPfKYukX6kPLoZQ/132","nickname":"Be_Young","note":"","ucode":"ACDBB2DD99738A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1690884,"avatar":"https://static001.geekbang.org/account/avatar/00/19/cd/04/e27b7803.jpg","nickname":"小新","note":"","ucode":"DCAD04665E2CF8","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":554097,"discussion_content":"应该是偏函数","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1646213354,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":534422,"ip_address":""},"score":554097,"extra":""}]}]},{"had_liked":false,"id":339700,"user_name":"Spoon","can_delete":false,"product_type":"c1","uid":1959822,"ip_address":"","ucode":"2FF9193AD482C2","user_header":"https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg","comment_is_top":false,"comment_ctime":1648300448,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5943267744","product_id":100090001,"comment_content":"最后filter更建议用Set这种数据结构进行contain查询，set的算法复杂度为O(1)<br>本节内容Java版本实现<br>https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;core&#47;TransformOpJob.java","like_count":2},{"had_liked":false,"id":317233,"user_name":"DMY","can_delete":false,"product_type":"c1","uid":1480819,"ip_address":"","ucode":"22C16ED954FC3A","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/iaU7yVx9WnPfnpJBwGCCLWBy9SvkVQRMxmOBxyZTqRYFgPEEYa0pfUHAek6TNEp6SdFRwumUycSeCL7DaM6FotQ/132","comment_is_top":false,"comment_ctime":1634717907,"is_pvip":false,"replies":[{"id":"115125","content":"可以参考下面的方式来实现：<br><br>import scala.collection.mutable.ArrayBuffer<br><br>def f(iter: Iterator[Int]) : Iterator[Int] = {<br>   val dataSeq: ArrayBuffer[Int] = new ArrayBuffer()<br>   while (iter.hasNext) {<br>     val cur = iter.next;<br>     dataSeq += cur<br>   }<br>   &#47;&#47; 你的处理逻辑<br>   val res = yourRPCFunction(dataSeq)<br>   &#47;&#47; 以迭代器的形式，返回计算结果<br>   res.iterator<br> }<br><br>&#47;&#47; 处理RDD的时候，用mapPartitions调用f即可<br>data.mapPartitions(f)","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1634889557,"ip_address":"","comment_id":317233,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5929685203","product_id":100090001,"comment_content":"数据做map映射是以元素为粒度，执行f函数；<br>这里业务场景中，f函数需要调用rpc，每个数据调一次rpc+数据量大就会非常耗时。<br>所以想把一组数据打包成一个list减少rpc调用，来提高效率，这里要怎么处理呢","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":528727,"discussion_content":"可以参考下面的方式来实现：\n\nimport scala.collection.mutable.ArrayBuffer\n\ndef f(iter: Iterator[Int]) : Iterator[Int] = {\n   val dataSeq: ArrayBuffer[Int] = new ArrayBuffer()\n   while (iter.hasNext) {\n     val cur = iter.next;\n     dataSeq += cur\n   }\n   // 你的处理逻辑\n   val res = yourRPCFunction(dataSeq)\n   // 以迭代器的形式，返回计算结果\n   res.iterator\n }\n\n// 处理RDD的时候，用mapPartitions调用f即可\ndata.mapPartitions(f)","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1634889557,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312328,"user_name":"Geek_390836","can_delete":false,"product_type":"c1","uid":1896889,"ip_address":"","ucode":"201E23E1E61828","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKGiaLDlazWNmLnB7nGbHdH3g4Akx1LxJAjx4zrriaS7AWdUXfCzy2pyfhibJ4Z0LNnSOgcSvA39LuOQ/132","comment_is_top":false,"comment_ctime":1631753198,"is_pvip":false,"replies":[{"id":"113201","content":"好问题~ <br><br>你说的对，这里笔误了，已经让编辑帮忙改成了“word.getbytes”哈~ <br><br>感谢纠正~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631805408,"ip_address":"","comment_id":312328,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5926720494","product_id":100090001,"comment_content":"参考map和mapPartitions，为什么mapPartitions中的map，是对record进行getbytes而不是word.getbytes操作，刚学spark，求老师解答","like_count":2,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526939,"discussion_content":"好问题~ \n\n你说的对，这里笔误了，已经让编辑帮忙改成了“word.getbytes”哈~ \n\n感谢纠正~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631805408,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":343524,"user_name":"牧一尔","can_delete":false,"product_type":"c1","uid":1333409,"ip_address":"","ucode":"562640F2130207","user_header":"https://static001.geekbang.org/account/avatar/00/14/58/a1/6e33ffc7.jpg","comment_is_top":false,"comment_ctime":1650891378,"is_pvip":true,"discussion_count":0,"race_medal":0,"score":"1650891378","product_id":100090001,"comment_content":"课后回答<br>第一题：应用为按照分区索引做定制化操作<br>第二题：共性为都是转换算子，并非行动算子<br>第三题：之前没用过 mapPartitions 算子，也不知道，看来下代码，发现了可以很多用这个算子做优化的地方","like_count":1},{"had_liked":false,"id":341781,"user_name":"我爱夜来香","can_delete":false,"product_type":"c1","uid":2609930,"ip_address":"","ucode":"10761E677EF05F","user_header":"https://static001.geekbang.org/account/avatar/00/27/d3/0a/92640aae.jpg","comment_is_top":false,"comment_ctime":1649831506,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1649831506","product_id":100090001,"comment_content":"老师,map算子针对一个个元素进行f操作,这个元素是咋定义的,一行?","like_count":0,"discussions":[{"author":{"id":2941084,"avatar":"","nickname":"Geek_ce7292","note":"","ucode":"7C89258611315E","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":583676,"discussion_content":"我觉得这边的元素应该是RDD中元素的类型~\n\n比如上文中提到的：\n\nlineRDD的类型为：RDD[String],每个元素的类型为String，即一行文本；\n\nwordRDD的类型为：RDD[String],每个元素的类型为String，是指每一个单词；\n\nkvRDD的类型为：RDD[(String，Int)],每个元素的类型为(String，Int)，是指一个键值对；","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1660286559,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"中国香港"},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":335512,"user_name":"木之上","can_delete":false,"product_type":"c1","uid":2579938,"ip_address":"","ucode":"EE9710F7FAFCCF","user_header":"https://static001.geekbang.org/account/avatar/00/27/5d/e2/1587b1b3.jpg","comment_is_top":false,"comment_ctime":1645546633,"is_pvip":true,"replies":[{"id":"122664","content":"是的，没有问题~ JAVA8开始支持函数式编程，它的stream本质上和咱们这里的算子是一样的~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1645673032,"ip_address":"","comment_id":335512,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1645546633","product_id":100090001,"comment_content":"老师，在学习这些算子的时候，像map，flatmap是否可以类比JAVA8的lambda表达式+stream流去学习","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":552974,"discussion_content":"是的，没有问题~ JAVA8开始支持函数式编程，它的stream本质上和咱们这里的算子是一样的~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1645673033,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":329220,"user_name":"艾利特-G","can_delete":false,"product_type":"c1","uid":1195204,"ip_address":"","ucode":"8C4DA0ABE77CDC","user_header":"https://static001.geekbang.org/account/avatar/00/12/3c/c4/4ee2968a.jpg","comment_is_top":false,"comment_ctime":1641217366,"is_pvip":false,"replies":[{"id":"120192","content":"赞👍，感谢老弟的整理~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1641719462,"ip_address":"","comment_id":329220,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1641217366","product_id":100090001,"comment_content":"&#47;&#47; 相邻词汇计数统计<br>package com.shouneng<br><br>import org.apache.spark.rdd.RDD<br>import org.apache.spark.{SparkConf, SparkContext}<br><br>object NeighboringWordCount {<br>  def main(args: Array[String]) {<br>    try {<br>      val conf =<br>        new SparkConf().setAppName(&quot;NeighboringWordCount&quot;).setMaster(&quot;local[1]&quot;)<br>      val sparkContext = new SparkContext(conf)<br>      val rootPath: String = &quot;file:&#47;&#47;&quot;<br>      val file: String =<br>        s&quot;${rootPath}&#47;home&#47;shouneng&#47;geektime&#47;Getting_Started_with_Spark&#47;chapter01&#47;wikiOfSpark.txt&quot;<br><br>      &#47;&#47; 读取文件内容<br>      val lineRDD: RDD[String] = sparkContext.textFile(file)<br>      &#47;&#47;   val wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(&quot; &quot;))<br>      &#47;&#47;   val cleanWordRDD: RDD[String] = wordRDD.filter(word =&gt; !word.equals(&quot;&quot;))<br><br>      val wordPairRDD: RDD[String] = lineRDD.flatMap(line =&gt; {<br>        val words: Array[String] =<br>          line.split(&quot; &quot;).filter(word =&gt; !word.equals(&quot;&quot;))<br>        for (i &lt;- 0 until words.length - 1) yield words(i) + &quot;-&quot; + words(i + 1)<br>      })<br>      val kvRDD: RDD[(String, Int)] = wordPairRDD.map(wordPair =&gt; (wordPair, 1))<br>      val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)<br><br>      wordCounts<br>        .map { case (k, v) =&gt; (v, k) }<br>        .sortByKey(false)<br>        .take(5)<br>        .foreach(println)<br>    } catch {<br>      case ex: Exception =&gt; {<br>        ex.printStackTrace() &#47;&#47; 打印到标准err<br>        System.err.println(&quot;exception===&gt;: ...&quot;) &#47;&#47; 打印到标准err<br>      }<br>    }<br>  }<br>}","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":544799,"discussion_content":"赞👍，感谢老弟的整理~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1641719462,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320157,"user_name":"孙浩","can_delete":false,"product_type":"c1","uid":1681527,"ip_address":"","ucode":"09A77A8D5CAF25","user_header":"https://static001.geekbang.org/account/avatar/00/19/a8/77/08ee51cb.jpg","comment_is_top":false,"comment_ctime":1636102356,"is_pvip":true,"replies":[{"id":"116285","content":"并没有哈，在运行时，map和mapPartitions都是以分区为粒度，创建分布式任务。换句话说，map和mapPartitions两者在运行时，Tasks的数量是一致的，所以并不存在你说的任务与任务之间对象锁的问题","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1636435236,"ip_address":"","comment_id":320157,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636102356","product_id":100090001,"comment_content":"吴老师，mapPartitions的是不是也避免了对象锁的问题，因为partitions对应的也是任务数。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529898,"discussion_content":"并没有哈，在运行时，map和mapPartitions都是以分区为粒度，创建分布式任务。换句话说，map和mapPartitions两者在运行时，Tasks的数量是一致的，所以并不存在你说的任务与任务之间对象锁的问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1636435236,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":320150,"user_name":"孙浩","can_delete":false,"product_type":"c1","uid":1681527,"ip_address":"","ucode":"09A77A8D5CAF25","user_header":"https://static001.geekbang.org/account/avatar/00/19/a8/77/08ee51cb.jpg","comment_is_top":false,"comment_ctime":1636099997,"is_pvip":true,"replies":[{"id":"116284","content":"好问题，<br>1）（K，V）可以理解成是元组，这个没有问题<br>2）关于reduceByKey为啥不支持RDD[Map[String,Int]]，这个其实老弟需要充分理解PairRDD，就RDD[Map[String,Int]]这种类型来说，本质上，他其实是RDD[KeyType]，而不是RDD[(KeyType，ValueType)]。也就是说，RDD[Map[String,Int]]不是PairRDD，所以不仅reduceByKey不支持，其他*ByKey类操作也不支持。如果把它变换一下，比如RDD[(String，Map[String,Int])]，这样是可以的，因为这个类型属于PairRDD，KeyType是String，而ValueType是Map[String,Int]~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1636435034,"ip_address":"","comment_id":320150,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1636099997","product_id":100090001,"comment_content":"有疑问，吴老师，PariedRDD中的（K，V），1.对应的数据类型应该是scala中的元组吧？2.reduceByKey为啥不支持元素是map类型？或者如果我存在一个RDD[Map[String,Int]]，我想做reduceByKey操作，应该怎么实现？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529896,"discussion_content":"好问题，\n1）（K，V）可以理解成是元组，这个没有问题\n2）关于reduceByKey为啥不支持RDD[Map[String,Int]]，这个其实老弟需要充分理解PairRDD，就RDD[Map[String,Int]]这种类型来说，本质上，他其实是RDD[KeyType]，而不是RDD[(KeyType，ValueType)]。也就是说，RDD[Map[String,Int]]不是PairRDD，所以不仅reduceByKey不支持，其他*ByKey类操作也不支持。如果把它变换一下，比如RDD[(String，Map[String,Int])]，这样是可以的，因为这个类型属于PairRDD，KeyType是String，而ValueType是Map[String,Int]~","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1636435034,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":317916,"user_name":"小强","can_delete":false,"product_type":"c1","uid":2398148,"ip_address":"","ucode":"C3D1215867302D","user_header":"https://static001.geekbang.org/account/avatar/00/24/97/c4/6c92c78a.jpg","comment_is_top":false,"comment_ctime":1635052054,"is_pvip":false,"replies":[{"id":"115468","content":"可以参考评论区“崔小豪”同学下面的回复，属于同一类问题：数组越界~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1635261396,"ip_address":"","comment_id":317916,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1635052054","product_id":100090001,"comment_content":"import org.apache.spark.rdd.RDD<br> <br>&#47;&#47; 这里的下划线&quot;_&quot;是占位符，代表数据文件的根目录<br>val rootPath: String = &quot;&#47;home&#47;gxchen&#47;Spark&#47;01-wordCount&quot;<br>val file: String = s&quot;${rootPath}&#47;wikiOfSpark.txt&quot;<br> <br>&#47;&#47; 读取文件内容<br>val lineRDD: RDD[String] = spark.sparkContext.textFile(file)<br> <br>&#47;&#47; 以行为单位做分词<br>val wordPairRDD: RDD[String] = lineRDD.flatMap(line =&gt; {<br>\tval words: Array[String] = line.split(&quot; &quot;)<br>\tfor (i &lt;- 0 until words.length - 1) yield words(i) + &quot;-&quot; + words(i+1)<br>})<br><br><br><br>&#47;&#47; 定义特殊字符列表<br>val list: List[String] = List(&quot;&amp;&quot;, &quot;|&quot;, &quot;#&quot;, &quot;^&quot;, &quot;@&quot;)<br> <br>&#47;&#47; 定义判定函数f<br>def f(s: String): Boolean = {<br>\tval words: Array[String] = s.split(&quot;-&quot;)<br>\tval b1: Boolean = list.contains(words(0))<br>\tval b2: Boolean = list.contains(words(1))<br>\treturn !b1 &amp;&amp; !b2 &#47;&#47; 返回不在特殊字符列表中的词汇对<br>}<br> <br>&#47;&#47; 使用filter(f)对RDD进行过滤<br>val cleanWordPairRDD: RDD[String] = wordPairRDD.filter(f)<br><br>&#47;&#47; 把RDD元素转换为（Key，Value）的形式<br>val kvRDD: RDD[(String, Int)] = cleanWordPairRDD.map(wordPair =&gt; (wordPair, 1))<br>&#47;&#47; 按照单词做分组计数<br>val wordPairCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)<br> <br>&#47;&#47; 打印词频最高的5个词汇<br>wordPairCounts.map{case (k, v) =&gt; (v, k)}.sortByKey(false).take(5)<br><br><br>运行了以上代码后，报错。想了半天也没弄明白，求助一下！<br>21&#47;10&#47;24 04:58:42 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 3)<br>java.lang.ArrayIndexOutOfBoundsException: 1<br>        at $line33.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.f(&lt;console&gt;:29)<br>        at $line34.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.$anonfun$cleanWordPairRDD$1(&lt;console&gt;:27)<br><br>21&#47;10&#47;24 04:58:42 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 3) (localhost executor driver): java.lang.ArrayIndexOutOfBoundsException: 1<br><br>21&#47;10&#47;24 04:58:42 ERROR TaskSetManager: Task 1 in stage 2.0 failed 1 times; aborting job<br>21&#47;10&#47;24 04:58:42 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)<br><br>21&#47;10&#47;24 04:58:42 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (localhost executor driver): java.lang.ArrayIndexOutOfBoundsException: 0","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":529028,"discussion_content":"可以参考评论区“崔小豪”同学下面的回复，属于同一类问题：数组越界~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635261396,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":313798,"user_name":"子兮","can_delete":false,"product_type":"c1","uid":2767208,"ip_address":"","ucode":"BA213EAB26DF16","user_header":"https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg","comment_is_top":false,"comment_ctime":1632668741,"is_pvip":false,"replies":[{"id":"113764","content":"可以参考Unknown Element的回答～ m在filter外，是driver创建，需要走网络、序列化。而m在filter内，则是在executor创建，不需要从driver分发过去","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632831012,"ip_address":"","comment_id":313798,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1632668741","product_id":100090001,"comment_content":"老师，请问，一个变量m被filter 算子内部计算时使用，若m定义在了filter算子所在函数内，也就是变量和算子在同一个函数里，就不会报序列化问题，若定义在了函数之外就会报序列化问题，这是为什么？看了一些解释，说是函数闭包，也不是很理解","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527477,"discussion_content":"可以参考Unknown Element的回答～ m在filter外，是driver创建，需要走网络、序列化。而m在filter内，则是在executor创建，不需要从driver分发过去","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632831012,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312625,"user_name":"钱鹏 Allen","can_delete":false,"product_type":"c1","uid":2518863,"ip_address":"","ucode":"7E95E82C0717DA","user_header":"https://static001.geekbang.org/account/avatar/00/26/6f/4f/3cf1e9c4.jpg","comment_is_top":false,"comment_ctime":1631895628,"is_pvip":true,"replies":[{"id":"113341","content":"正解~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1632051321,"ip_address":"","comment_id":312625,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1631895628","product_id":100090001,"comment_content":"mapPartitionsWithIndex  需要结合分区索引使用<br>map filter flatMap mapPartitions mapPartitionsWithIndex  通过函数，传递参数，返回新的RDD<br>mapPartitions 采集系统中，只需实例化一次电表号，可读其他各类读数","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":527052,"discussion_content":"正解~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1632051321,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312432,"user_name":"进","can_delete":false,"product_type":"c1","uid":2754963,"ip_address":"","ucode":"B971F478E3BE8E","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/hSGZCTvHTKaJH5WibTcC15qDKYpGdgdMFEV6mwcslOFoADP6skWCpBiae2ykkaBFDaexEY9xXBPhZMAGmj1nW8vA/132","comment_is_top":false,"comment_ctime":1631797974,"is_pvip":false,"replies":[{"id":"113196","content":"谢谢支持，明天就来了。也欢迎多在评论区交流。","user_name":"编辑回复","user_name_real":"赵宇新","uid":"1501385","ctime":1631798185,"ip_address":"","comment_id":312432,"utype":2}],"discussion_count":1,"race_medal":1,"score":"1631797974","product_id":100090001,"comment_content":"期待下一讲....","like_count":0,"discussions":[{"author":{"id":1501385,"avatar":"https://static001.geekbang.org/account/avatar/00/16/e8/c9/59bcd490.jpg","nickname":"听水的湖","note":"","ucode":"B1759F90165D81","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526966,"discussion_content":"谢谢支持，明天就来了。也欢迎多在评论区交流。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631798185,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":4}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312350,"user_name":"求索","can_delete":false,"product_type":"c1","uid":1243895,"ip_address":"","ucode":"26A80D3BFB3E3A","user_header":"https://static001.geekbang.org/account/avatar/00/12/fa/f7/a6f40eee.jpg","comment_is_top":false,"comment_ctime":1631763586,"is_pvip":false,"replies":[{"id":"113204","content":"这里的资源，具体指的是哪类资源呢？CPU、内存、磁盘还是网络？<br><br>我理解，听上去是CPU资源闲置，也就是利用率不高。如果是这样的话，那么大概率是因为任务时间大部分都花在I&#47;O wait上了。也就是CPU一直在“等活儿干”，CPU闲置期间，任务的主要操作很可能是Shuffle期间的磁盘I&#47;O和网络I&#47;O，这个时候，CPU确实没什么活儿可干，只能等，所以看上去是CPU闲置，实则是它在“等活儿”~<br><br>解决办法的话，可以围绕着提升Shuffle的效率去展开，这部分我们在后面的Shuffle和Broadcast Join部分会讲，敬请期待~<br><br>当然，这些分析，都是基于对资源是CPU的假设，如果是其他资源，我们再讨论~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631806969,"ip_address":"","comment_id":312350,"utype":1}],"discussion_count":3,"race_medal":0,"score":"1631763586","product_id":100090001,"comment_content":"请教个问题，spark sql执行缓慢，然而资源却闲置状态，这是何解","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526947,"discussion_content":"这里的资源，具体指的是哪类资源呢？CPU、内存、磁盘还是网络？\n\n我理解，听上去是CPU资源闲置，也就是利用率不高。如果是这样的话，那么大概率是因为任务时间大部分都花在I/O wait上了。也就是CPU一直在“等活儿干”，CPU闲置期间，任务的主要操作很可能是Shuffle期间的磁盘I/O和网络I/O，这个时候，CPU确实没什么活儿可干，只能等，所以看上去是CPU闲置，实则是它在“等活儿”~\n\n解决办法的话，可以围绕着提升Shuffle的效率去展开，这部分我们在后面的Shuffle和Broadcast Join部分会讲，敬请期待~\n\n当然，这些分析，都是基于对资源是CPU的假设，如果是其他资源，我们再讨论~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631806969,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1243895,"avatar":"https://static001.geekbang.org/account/avatar/00/12/fa/f7/a6f40eee.jpg","nickname":"求索","note":"","ucode":"26A80D3BFB3E3A","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394378,"discussion_content":"您好，非常高兴看到您的回复，我讲的资源不仅仅是cpu还包括内存，io等，从资源监控上看都非常空闲，job配置资源无论增加减少也没有起到作用，依然执行很慢。有点迷糊，不清楚哪里有问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631862560,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":1243895,"avatar":"https://static001.geekbang.org/account/avatar/00/12/fa/f7/a6f40eee.jpg","nickname":"求索","note":"","ucode":"26A80D3BFB3E3A","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":399145,"discussion_content":"可以把SQL贴出来，一起分析分析，包括集群资源配置，等等细节","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1632911054,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":394378,"ip_address":""},"score":399145,"extra":""}]}]},{"had_liked":false,"id":312233,"user_name":"GAC·DU","can_delete":false,"product_type":"c1","uid":1385403,"ip_address":"","ucode":"7847FBE1C13740","user_header":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","comment_is_top":false,"comment_ctime":1631696848,"is_pvip":true,"replies":[{"id":"113203","content":"1）、2）、3）都没问题~<br><br>关于Scala的语法，确实比较灵活，一般来说，简单表达式用()，复杂表达式用{}。<br><br>比如，简单函数体：(x =&gt; x + 1)、(_ + 1)，等等；<br>复杂函数体：{case x: String =&gt; “一大堆关于x的转换” }","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631806114,"ip_address":"","comment_id":312233,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1631696848","product_id":100090001,"comment_content":"1、mapPartitionsWithIndex的应用场景：获取数据所在的分区并对特定分区数据进行特定处理。<br>2、这些算子首先都是惰性算子，又都使用函数作为参数传入，达到一对一的效果。<br>3、在mapPatitions中只创建过连接数据库的共享对象，其他的没有遇到过，还请老师能不能多给几个实际案例，提高一下对mapPatitions的认知。<br>还想向老师提个问题：scala编程spark什么时候用小括号,什么时候用用花括号？<br>wordCount.map{case (k, v) =&gt; (v, k)}.sortByKey(false).take(5)<br>这段代码对于为什么使用花括号，不是很清晰，尝试过用小括号，但是无法调用sortByKey。","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526905,"discussion_content":"1）、2）、3）都没问题~\n\n关于Scala的语法，确实比较灵活，一般来说，简单表达式用()，复杂表达式用{}。\n\n比如，简单函数体：(x =&amp;gt; x + 1)、(_ + 1)，等等；\n复杂函数体：{case x: String =&amp;gt; “一大堆关于x的转换” }","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631806114,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":312145,"user_name":"崔小豪","can_delete":false,"product_type":"c1","uid":2604437,"ip_address":"","ucode":"ABB59CE5209E67","user_header":"https://static001.geekbang.org/account/avatar/00/27/bd/95/e923a332.jpg","comment_is_top":false,"comment_ctime":1631664351,"is_pvip":false,"replies":[{"id":"113202","content":"这个列表List(&quot;&amp;&quot;, &quot;|&quot;, &quot;#&quot;, &quot;^&quot;, &quot;@&quot;, &quot;\\&quot;&quot;,&quot;-&quot;)，里面多了一个“-”，在后面分割字符串的时候：val words: Array[String] = s.split(&quot;-&quot;)，同样用到了“-”。所以这里分割出来的字符串数组，它的大小可能会出现不一致。<br><br>把分隔符换成其他字符就好了~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1631806038,"ip_address":"","comment_id":312145,"utype":1}],"discussion_count":5,"race_medal":0,"score":"1631664351","product_id":100090001,"comment_content":"遇到一个关于filter 函数的 bug on reduceByKey。<br>我运行第一个例子代码如下可以得到正确的结果：<br>...<br> val cleanedPairRDD: RDD[String] = wordPairRDD.filter(word =&gt; !word.equals(&quot;&quot;)) <br>...<br>可以输出结果：<br>scala&gt; :load WordCount03.scala<br>Loading WordCount03.scala...<br>import org.apache.spark.rdd.RDD<br>rootPath: String = .<br>file: String = .&#47;wikiOfSpark.txt<br>lineRDD: org.apache.spark.rdd.RDD[String] = .&#47;wikiOfSpark.txt MapPartitionsRDD[274] at textFile at WordCount03.scala:80<br>wordPairRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[275] at flatMap at WordCount03.scala:80<br>cleanedPairRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[276] at filter at WordCount03.scala:80<br>kvRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[277] at map at WordCount03.scala:80<br>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[278] at reduceByKey at WordCount03.scala:80<br>res66: Array[(Int, String)] = Array((10,Apache-Software), (8,Apache-Spark), (7,can-be), (7,-&quot;Spark), (7,of-the))<br><br>修改代码，添加filter函数后，代码报错：<br>代码：<br>...<br>&#47;&#47; 定义特殊字符列表<br>val list: List[String] = List(&quot;&amp;&quot;, &quot;|&quot;, &quot;#&quot;, &quot;^&quot;, &quot;@&quot;, &quot;\\&quot;&quot;,&quot;-&quot;)<br>def f(s: String): Boolean = {<br>    val words: Array[String] = s.split(&quot;-&quot;)<br>    val b1: Boolean = list.contains(words(0))<br>    val b2: Boolean = list.contains(words(1))<br>    return !b1 &amp;&amp; !b2}  <br>val cleanedPairRDD: RDD[String] = wordPairRDD.filter(f)  <br>...<br>输出报错：<br>scala&gt; :load WordCountDebug.scala<br>Loading WordCountDebug.scala...<br>...<br>list: List[String] = List(&amp;, |, #, ^, @, &quot;, -)<br>f: (s: String)Boolean<br>cleanedPairRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[286] at filter at WordCountDebug.scala:84<br>kvRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[287] at map at WordCountDebug.scala:82<br>wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[288] at reduceByKey at WordCountDebug.scala:82<br>21&#47;09&#47;15 09:51:42 ERROR Executor: Exception in task 1.0 in stage 108.0 (TID 141)<br>java.lang.ArrayIndexOutOfBoundsException: 1<br>...<br>","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":526877,"discussion_content":"这个列表List(&amp;quot;&amp;amp;&amp;quot;, &amp;quot;|&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;^&amp;quot;, &amp;quot;@&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;,&amp;quot;-&amp;quot;)，里面多了一个“-”，在后面分割字符串的时候：val words: Array[String] = s.split(&amp;quot;-&amp;quot;)，同样用到了“-”。所以这里分割出来的字符串数组，它的大小可能会出现不一致。\n\n把分隔符换成其他字符就好了~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631806038,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1385403,"avatar":"https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg","nickname":"GAC·DU","note":"","ucode":"7847FBE1C13740","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394034,"discussion_content":"先过滤出切片数组长度大于1的单词对\ndef filterLength(s: String): Boolean = {\n    val words = s.split(&#34;-&#34;)\n    return words.length > 1\n}","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1631694003,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2604437,"avatar":"https://static001.geekbang.org/account/avatar/00/27/bd/95/e923a332.jpg","nickname":"崔小豪","note":"","ucode":"ABB59CE5209E67","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394304,"discussion_content":"谢谢 AIK的提示。先看修改后的结果 Top 10：\nArray[(Int, String)] = Array((10,Apache-Software), (8,Apache-Spark), (7,can-be), (7,of-the), (6,Software-Foundation), (6,to-the), (6,in-a), (5,machine-learning), (5,such-as), (5,the-Apache))\n\n修改后的代码如下，增加了一个filter来保证是2个单词而且要求，同时增强了特殊字符列表。\n注意： 可以对比观察一下不同特殊字符过滤后结果的区别：\n// &#34;&amp;&#34;, &#34;|&#34;, &#34;#&#34;, &#34;^&#34;, &#34;@&#34;\nArray((10,Apache-Software), (8,Apache-Spark), (7,can-be), (7,-&#34;Spark), (7,of-the), (6,Software-Foundation), (6,to-the), (6,in-a), (5,machine-learning), (5,such-as), (5,the-Apache), (5,-), (5,in-the), (4,Malak,-Michael), (4,Spark-SQL), (4,an-RDD), (4,as-a), (4,support-for), (4,of-Spark), (4,Spark-Core))\n// &#34;&amp;&#34;, &#34;|&#34;, &#34;#&#34;, &#34;^&#34;, &#34;@&#34;, &#34;\\&#34;&#34;, &#34;-&#34;, &#34;&#34;\nArray((10,Apache-Software), (8,Apache-Spark), (7,can-be), (7,of-the), (6,Software-Foundation), (6,to-the), (6,in-a), (5,machine-learning), (5,such-as), (5,the-Apache), (5,in-the), (4,Malak,-Michael), (4,Spark-SQL), (4,an-RDD), (4,as-a), (4,support-for), (4,of-Spark), (4,Spark-Core), (4,on-a), (4,top-of))\n\n=================\n \ndef filterLength(s: String): Boolean = {\n    val words = s.split(&#34;-&#34;)\n    return words.length==2\n}\nval wordsPairRDD: RDD[String] = wordPairRDD.filter(filterLength)\nval charlist: List[String] = List(&#34;&amp;&#34;, &#34;|&#34;, &#34;#&#34;, &#34;^&#34;, &#34;@&#34;, &#34;\\&#34;&#34;, &#34;-&#34;, &#34;&#34;) // 添加了&#34;, -, &#34;&#34; 消除 （-&#34;Spark）（-）之类的词对\ndef f(s: String): Boolean = {\n    val words: Array[String] = s.split(&#34;-&#34;)\n    val b1: Boolean = charlist.contains(words(0)) // special words 1\n    val b2: Boolean = charlist.contains(words(1)) // \n    return !b1 &amp;&amp; !b2 \n}  \nval cleanedPairRDD: RDD[String] = wordsPairRDD.filter(f)\n// 把RDD元素转换为（Key，Value）的形式\nval kvRDD: RDD[(String, Int)] = cleanedPairRDD.map(word => (word, 1))\n// 按照单词做分组计数\nval wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) => x + y)\n// 打印词频最高的5个词汇\nwordCounts.map{ca","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1631838615,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2457953,"avatar":"https://static001.geekbang.org/account/avatar/00/25/81/61/97d95637.jpg","nickname":"你午睡了吗","note":"","ucode":"1D48E86004D945","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":394092,"discussion_content":"使用mapPartitions优化的代码，是不是少了(hash, 1)的1","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631717307,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2177406,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/ia89MwaRSP6iaeQB4789roUntH7tia9EXyoDOnlibE8ABibAzFPiamP0SAU54NNTRiaVtkOtmLaWRH5OXbTOhUZl46scw/132","nickname":"Geek_01fccd","note":"","ucode":"6D5D9C2882513F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":393949,"discussion_content":"数组越界了words ，有些只有一项 words(1) 这个地方","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1631669410,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}