{"id":365252,"title":"18 | 磁盘视角：如果内存无限大，磁盘还有用武之地吗？","content":"<p>你好，我是吴磊。</p><p>我们都知道，Spark的优势在于内存计算。一提到“内存计算”，我们的第一反应都是：执行效率高！但如果听到“基于磁盘的计算”，就会觉得性能肯定好不到哪儿去。甚至有的人会想，如果Spark的内存无限大就好了，这样我们就可以把磁盘完全抛弃掉。当然，这个假设大概率不会成真，而且这种一刀切的思维也不正确。</p><p>如果内存无限大，我们确实可以通过一些手段，让Spark作业在执行的过程中免去所有的落盘动作。但是，无限大内存引入的大量Full GC停顿（Stop The World），很有可能让应用的执行性能，相比有磁盘操作的时候更差。这就不符合我们一再强调的，<strong>调优的最终目的是在不同的硬件资源之间寻求平衡了</strong>。</p><p>所以今天这一讲，我们就来说说磁盘在Spark任务执行的过程中都扮演哪些重要角色，它功能方面的作用，以及性能方面的价值。掌握它们可以帮助我们更合理地利用磁盘，以成本优势平衡不同硬件资源的计算负载。</p><h2>磁盘在功能上的作用</h2><p>在Spark当中，磁盘都用在哪些地方呢？在Shuffle那一讲我们说过，在Map阶段，Spark根据计算是否需要聚合，分别采用PartitionedPairBuffer和PartitionedAppendOnlyMap两种不同的内存数据结构来缓存分片中的数据记录。分布式计算往往涉及海量数据，因此这些数据结构通常都没办法装满分区中的所有数据。在内存受限的情况下，溢出机制可以保证任务的顺利执行，不会因为内存空间不足就立即报OOM异常。</p><!-- [[[read_end]]] --><p><img src=\"https://static001.geekbang.org/resource/image/68/4e/688d5453a9431c53d0a75c7a4yy3e44e.jpg?wh=7212*2328\" alt=\"\" title=\"溢出数据到磁盘，避免频繁的OOM\"></p><p>以“仙女散花”的游戏为例，我们用groupByKey去收集不同花色的花朵。在PartitionedPairBuffer大小为4的情况下，当小红拿到的花朵数量超过4朵的时候，其余花朵要想进入内存，Spark就必须把PartitionedPairBuffer中的内容暂时溢出到临时文件，把内存空间让出来才行。<strong>这就是磁盘在功能上的第一个作用：溢出临时文件。</strong></p><p>当分区中的最后一批数据加载到PartitionedPairBuffer之后，它会和之前溢出到磁盘的临时文件一起做归并计算，最终得到Shuffle的数据文件和索引文件也会存储到磁盘上，也就是我们常说的Shuffle中间文件。<strong>这就是磁盘的在功能上的第二个作用：存储Shuffle中间文件。</strong></p><p>除此之外，<strong>磁盘的第三个作用就是缓存分布式数据集。也就是说，凡是带<em>DISK</em>字样的存储模式，都会把内存中放不下的数据缓存到磁盘</strong>。这些缓存数据还有刚刚讲的临时文件、中间文件，都会存储到spark.local.dir参数对应的文件系统目录中。</p><h2>性能上的价值</h2><p>在配置项那一讲我们说过，把spark.local.dir这个参数配置到SDD或者其他访问效率更高的存储系统中可以提供更好的 I/O 性能。除此之外，磁盘复用还能给执行性能带来更好的提升。所谓<strong>磁盘复用，它指的是Shuffle Write阶段产生的中间文件被多次计算重复利用的过程</strong>。下面，我就通过两个例子给你详细讲讲，磁盘复用的常见应用和它的收益。</p><h3>失败重试中的磁盘复用</h3><p>我们经常说，在没有RDD Cache的情况下，一旦某个计算环节出错，就会触发整条DAG从头至尾重新计算，这个过程又叫失败重试。严格来说，这种说法是不准确的。因为，失败重试的计算源头并不是整条DAG的“头”，而是与触发点距离最新的Shuffle的中间文件。</p><p><img src=\"https://static001.geekbang.org/resource/image/35/86/35c13d9f2eba5d23dabe05249ccb9486.jpg?wh=2626*1190\" alt=\"\" title=\"磁盘复用与蓄水池\"></p><p>我们以文稿示意图中的DAG为例子，HDFS源数据经过两次转换之后，分别得到RDD1和RDD2。RDD2在Shuffle之后再进行两次计算，分成得到RDD3和RDD4。</p><p>不幸的是，在计算RDD4的过程中有些任务失败了。在失败重试的时候，Spark确实会从RDD4向前回溯，但是有了磁盘复用机制的存在，它并不会一直回溯到HDFS源数据，而是直接回溯到已经物化到节点的RDD3的“数据源”，也就是RDD2在Shuffle Write阶段输出到磁盘的中间文件。因此，<strong>磁盘复用的收益之一就是缩短失败重试的路径，在保障作业稳定性的同时提升执行性能</strong>。</p><p>为了方便你理解，我们不妨把DAG中的流水线计算想象成是干渠灌溉，黄土高坡上的麦田一年到头也喝不到几滴雨水，完全依靠人工干渠进行灌溉。当水电站开闸放水的时候，水会沿着干渠一路向东流进支渠去滋养如饥似渴的麦苗。</p><p>一个水电站往往服务方圆百里大大小小的村子，如果每次灌溉都等着水电站开闸放水，遇上大旱的年头，水还没流到支渠，麦苗就都旱死了。要是能沿着干渠，每隔一段距离就修建一座蓄水池，那么附近的村民就能就近灌溉了。在这个干渠灌溉的类比中，水电站的水是HDFS数据源头，蓄水池就是Shuffle中间文件，就近取水、就近灌溉就是磁盘复用机制。</p><h3>ReuseExchange机制下的磁盘复用</h3><p>你可能会说：“磁盘复用也没什么嘛，无非是在失败重试的时候，抄个近道、少走些弯路。在任务不出错的情况下是利用不到这项优势的。”没错，所以我们再来说说磁盘复用的另一种形式：ReuseExchange机制。ReuseExchange是Spark SQL众多优化策略中的一种，它指的是<strong>相同或是相似的物理计划可以共享Shuffle计算的中间结果</strong>，也就是我们常说的Shuffle中间文件。ReuseExchange机制可以帮我们削减I/O开销，甚至节省Shuffle，来大幅提升执行性能。</p><p>那我们该怎么有效利用ReuseExchange机制呢？在数据仓库场景下，为了得到数据报表或是可视化图表，用户往往需要执行多个相似的查询，甚至会把同样的查询语句执行多次。在这种情况下，ReuseExchange策略在执行效率方面会带来非常大的收益。</p><p><img src=\"https://static001.geekbang.org/resource/image/5f/98/5f390cdb366edc25329956c11e773f98.jpg?wh=3023*1384\" alt=\"\" title=\"同样或相似的查询利用ReuseExchange缩短执行路径\"></p><p>即便是在没有DataFrame Cache的情况下，相同或是相似的查询也可以利用ReuseExchange策略，在缩短执行路径的同时，消除额外的Shuffle计算。从数据复用的角度来说，ReuseExchange和DISK_ONLY模式的DataFrame Cache能起到的作用完全等价。</p><p>咱们来举个例子。现在有这样一个业务需求：给定用户访问日志，分别统计不同用户的PV（Page Views，页面浏览量）、UV（Unique Views，网站独立访客），然后再把两项统计结果合并起来，以备后用。其中，用户日志包含用户ID、访问时间、页面地址等主要字段。业务需求不仅明确也很简单，我们很快就能把代码写出来。</p><pre><code>//版本1：分别计算PV、UV，然后合并\n// Data schema (userId: String, accessTime: Timestamp, page: String)\n \nval filePath: String = _\nval df: DataFrame = spark.read.parquet(filePath)\n \nval dfPV: DataFrame = df.groupBy(&quot;userId&quot;).agg(count(&quot;page&quot;).alias(&quot;value&quot;)).withColumn(&quot;metrics&quot;, lit(&quot;PV&quot;))\nval dfUV: DataFrame = df.groupBy(&quot;userId&quot;).agg(countDistinct(&quot;page&quot;).alias(&quot;value&quot;)).withColumn(&quot;metrics &quot;, lit(&quot;UV&quot;))\n \nval resultDF: DataFrame = dfPV.Union(dfUV)\n \n// Result样例\n| userId | metrics | value |\n| user0  | PV      | 25 |\n| user0  | UV      | 12 |\n</code></pre><p>代码逻辑是先读取用户日志，然后在同一个DataFrame之上分别调用count和countDistinct计算PV、UV，最后把PU、UV对应的两个DataFrame合并在一起。</p><p>虽然代码实现起来简单直接，但是，如果我们在resultDF之上调用explain或是通过Spark UI去查看物理计划就会发现，尽管count和countDistinct是基于同一份数据源计算的，但这两个操作的执行路径是完全独立的。它们各自扫描Parquet源文件，并且通过Shuffle完成计算，在Shuffle之前会先在Map端做本地聚合，Shuffle之后会在Reduce端再进行全局聚合。</p><p><img src=\"https://static001.geekbang.org/resource/image/dd/28/dd150e0863812522a6f2ee9102678928.jpg?wh=2663*1333\" alt=\"\" title=\"Parquet文件扫描两次、Shuffle两次\"></p><p>对于绝大多数的合并场景来说，计算流程大抵如此。显然，这样的做法是极其低效的，尤其是在需要合并多个数据集的时候，重复的数据扫描和分发就会引入更多的性能开销。那么，有没有什么办法，让同一份数据源的多个算子只读取一次Parquet文件，且只做一次Shuffle呢？</p><p>做了这么半天铺垫，答案自然是“有”。针对版本1中的代码，我们稍作调整就可以充分利用ReuseExchange策略来做优化。</p><pre><code>//版本2：分别计算PV、UV，然后合并\n// Data schema (userId: String, accessTime: Timestamp, page: String)\n \nval filePath: String = _\nval df: DataFrame = spark.read.parquet(filePath).repartition($&quot;userId&quot;)\n \nval dfPV: DataFrame = df.groupBy(&quot;userId&quot;).agg(count(&quot;page&quot;).alias(&quot;value&quot;)).withColumn(&quot;metrics&quot;, lit(&quot;PV&quot;))\nval dfUV: DataFrame = df.groupBy(&quot;userId&quot;).agg(countDistinct(&quot;page&quot;).alias(&quot;value&quot;)).withColumn(&quot;metrics &quot;, lit(&quot;UV&quot;))\n \nval resultDF: DataFrame = dfPV.Union(dfUV)\n \n// Result样例\n| userId | metrics | value |\n| user0  | PV      | 25 |\n| user0  | UV      | 12 |\n</code></pre><p>需要调整的部分仅仅是数据源读取，其他部分的代码保持不变。在用Parquet API读取用户日志之后，我们追加一步重分区操作，也就是以userId为分区键调用repartition算子。</p><p>经过这个微小的改动之后，我们重新在resultDF之上调用explain或是查看Spark UI会发现，在新的物理计划中，count或是countDistinct分支出现了ReuseExchange字样，也就是其中一方复用了另一方的Exchange结果。</p><p><img src=\"https://static001.geekbang.org/resource/image/00/b2/008e691de73eefc6daa4886017fa33b2.jpg?wh=2868*1210\" alt=\"\" title=\"ReuseExchange\"></p><p>通过观察执行计划不难发现，ReuseExchange带来的收益相当可观，不仅是<strong>数据源只需要扫描一遍，而且作为“性能瓶颈担当”的Shuffle也只发生了一次</strong>。</p><p>另外，你可能也会发现，复用Shuffle中间结果的是两个不完全相同的查询，一个是用count做统计计数，另一个是用countDistinct做去重计数。你看，两个相似的查询，通过ReuseExchange数据复用，达到了使用DISK_ONLY缓存的等价效果。换句话说，你不需要手动调用persist(DISK_ONLY)，也不需要忍受磁盘缓存的计算过程，就可以享受它带来的收益。这惊不惊喜、意不意外？</p><p>你可能会问：“既然ReuseExchange机制这么好用，满足什么条件才能触发Spark SQL去选择这个执行策略呢？”事实上，触发条件至少有2个：</p><ul>\n<li><strong>多个查询所依赖的分区规则要与Shuffle中间数据的分区规则保持一致</strong></li>\n<li><strong>多个查询所涉及的字段（Attributes）要保持一致</strong></li>\n</ul><p>对于第一个条件，我们在案例中已经演示过了，两个查询都用userId分组，这就要求所依赖的数据必须要按照userId做分区。这也是为什么我们在版本2的代码中，会添加以userId为分区键的repartition算子，只有这样，Shuffle中间结果的分区规则才能和查询所需的分区规则保持一致。</p><p>仔细观察count和countDistinct两个查询所涉及的字段，我们会发现它们完全一致。实际上，如果我们把count语句中的<code>count(\"page\")</code>改为<code>count(\"*\")</code>也并不影响PV的计算，但是，看似无关痛痒的改动会导致第二个条件不能满足，从而无法利用ReuseExchange机制来提升执行性能。版本2中的<code>count(\"page\")</code>改为<code>count(\"*\")</code>之后，物理计划会回退到版本1，我把其中的变化留给你作为课后作业去对比。</p><h2>小结</h2><p>磁盘虽然在处理延迟上远不如内存，但在性能调优中依然不可或缺。理解磁盘在功能上和性能上的价值，可以帮助我们更合理地利用磁盘，以成本优势平衡不同硬件资源的计算负载。</p><p>从功能上看，磁盘在Spark中主要有3方面的作用，分别是溢出临时文件、缓存分布式数据集和存储Shuffle中间文件。这3方面功能在提升作业稳定性的同时，也为执行效率的提升打下了基础。</p><p>从性能上看，利用好磁盘复用机制，可以极大地提高应用的执行性能。磁盘复用指的是Shuffle Write阶段产生的中间文件被多次计算重复利用的过程。磁盘复用有两种用途，一个是失败重试，另一个是ReuseExchange机制。其中，失败重试指的就是任务失败之后尝试重头计算。这个过程中，磁盘复用缩短了失败重试的路径，在保障作业稳定性的同时，提升执行性能。</p><p>ReuseExchange策略指的是，相同或是相似的物理计划可以共享Shuffle计算的中间结果。ReuseExchange对于执行性能的贡献相当可观，它可以让基于同一份数据源的多个算子只读取一次Parquet文件，并且只做一次Shuffle，来大幅削减磁盘与网络开销。</p><p>不过，要想让Spark SQL在优化阶段选择ReuseExchange，业务应用必须要满足2个条件：</p><ul>\n<li>多个查询所依赖的分区规则要与Shuffle中间数据的分区规则保持一致</li>\n<li>多个查询所涉及的字段要保持一致</li>\n</ul><h2>每日一练</h2><ol>\n<li>请你把count计算中的<code>count(\"page\")</code>改为<code>count(\"*\")</code>，以此来观察物理计划的变化，并在留言区说出你的观察</li>\n<li>为了触发ReuseExchange机制生效，我们按照userId对数据集做重分区，结合这一点，你不妨想一想，在哪些情况下，不适合采用ReuseExchange机制？为什么？</li>\n</ol><p>期待在留言区看到你的思考和答案，我们下一讲见！</p>","comments":[{"had_liked":false,"id":289782,"user_name":"sky_sql","can_delete":false,"product_type":"c1","uid":1099273,"ip_address":"","ucode":"397F4263C9E590","user_header":"https://static001.geekbang.org/account/avatar/00/10/c6/09/7f2bcc6e.jpg","comment_is_top":false,"comment_ctime":1619173069,"is_pvip":false,"replies":[{"id":"105163","content":"好问题~ 我们还是分资源调度和任务调度两种情况来说。<br><br>Spark在做任务调度之前，SchedulerBackend封装的调度器，比如Yarn、Mesos、Standalone，实际上已经完成了资源调度，换句话说，整个集群有多少个containers&#47;executors，已经是一件确定的事情了。而且，每个Executors的CPU和内存，也都是确定的了（因为你启动Spark集群的时候，使用配置项指定了每个Executors的CPU和内存分别是多少）。资源调度器在做资源调度的时候，确实是同时需要CPU和内存信息的。<br><br>资源调度完成后，Spark开始任务调度，你的问题，其实是任务调度范畴的问题。也就是TaskScheduler在准备调度任务的时候，要事先知道都有哪些Executors可用，注意，是可用。也就是TaskScheduler的核心目的，在于获取“可用”的Executors。<br><br>现在来回答你的问题，也就是：为什么ExecutorData不存储于内存相关的信息。答案是：不需要。一来，TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。二来，每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。<br><br>再来说Free Memory，首先，我们说过，Spark对于内存的预估不准，再者，每个Executors的可用内存都会随着GC的执行而动态变化，因此，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。一者是不准，二来确实没用，因为TaskScheduler拿不到数据分片大小这样的信息，TaskScheduler在Driver端，而数据分片是在目标Executors，所以TaskScheduler拿到Free Memory也没啥用，因为它也不能判断说：task要处理的数据分片，是不是超过了目标Executors的可用内存。<br><br>综上，ExecutorData的数据结构中，只保存了CPU信息，而没有记录内存消耗等信息。不知道这些能不能解答你的问题？有问题再聊哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619339198,"ip_address":"","comment_id":289782,"utype":1}],"discussion_count":1,"race_medal":0,"score":"70338649805","product_id":100073401,"comment_content":"老师好！咨询个题外话，在调度系统中讲到SchedulerBackend 用 ExecutorData 对 Executor 进行资源画像，ExecutorData中有RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等，但是没有内存相关的，是spark的Executor 内存这块比较复杂？<br>对比hadoop在调度时会考虑内存是否满足吗？","like_count":16,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519018,"discussion_content":"好问题~ 我们还是分资源调度和任务调度两种情况来说。\n\nSpark在做任务调度之前，SchedulerBackend封装的调度器，比如Yarn、Mesos、Standalone，实际上已经完成了资源调度，换句话说，整个集群有多少个containers/executors，已经是一件确定的事情了。而且，每个Executors的CPU和内存，也都是确定的了（因为你启动Spark集群的时候，使用配置项指定了每个Executors的CPU和内存分别是多少）。资源调度器在做资源调度的时候，确实是同时需要CPU和内存信息的。\n\n资源调度完成后，Spark开始任务调度，你的问题，其实是任务调度范畴的问题。也就是TaskScheduler在准备调度任务的时候，要事先知道都有哪些Executors可用，注意，是可用。也就是TaskScheduler的核心目的，在于获取“可用”的Executors。\n\n现在来回答你的问题，也就是：为什么ExecutorData不存储于内存相关的信息。答案是：不需要。一来，TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。二来，每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。\n\n再来说Free Memory，首先，我们说过，Spark对于内存的预估不准，再者，每个Executors的可用内存都会随着GC的执行而动态变化，因此，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。一者是不准，二来确实没用，因为TaskScheduler拿不到数据分片大小这样的信息，TaskScheduler在Driver端，而数据分片是在目标Executors，所以TaskScheduler拿到Free Memory也没啥用，因为它也不能判断说：task要处理的数据分片，是不是超过了目标Executors的可用内存。\n\n综上，ExecutorData的数据结构中，只保存了CPU信息，而没有记录内存消耗等信息。不知道这些能不能解答你的问题？有问题再聊哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619339198,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289728,"user_name":"zxk","can_delete":false,"product_type":"c1","uid":1221195,"ip_address":"","ucode":"4BB2BD9D2BCD04","user_header":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","comment_is_top":false,"comment_ctime":1619156652,"is_pvip":false,"replies":[{"id":"105146","content":"对，如果用磁盘Cache的话，只要把“最小公共子集”都Cache住就行了~ ReuseExchange虽然效果相同，但是前提条件确实比较苛刻，不容易命中。所以稳妥的话，还是使用Disk Cache。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619314535,"ip_address":"","comment_id":289728,"utype":1}],"discussion_count":2,"race_medal":0,"score":"23093993132","product_id":100073401,"comment_content":"老师，如果我在 df.groupBy(&quot;userId&quot;) 后做磁盘 cache ，后续再做两个 agg 操作，是否跟 ReuseExchange 效果一样？如果一样的话，那么这时候是不是可以不用管 agg 里涉及的字段了？","like_count":5,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519005,"discussion_content":"对，如果用磁盘Cache的话，只要把“最小公共子集”都Cache住就行了~ ReuseExchange虽然效果相同，但是前提条件确实比较苛刻，不容易命中。所以稳妥的话，还是使用Disk Cache。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619314535,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1056982,"avatar":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","nickname":"kingcall","note":"","ucode":"508884DC684B5B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":370634,"discussion_content":"哈哈 RelationalGroupedDataset 也就是你groupBy后的DataFrame 不支持cache ","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1619488582,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291320,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620204920,"is_pvip":false,"replies":[{"id":"105591","content":"说得好~ 👍，数据倾斜确实是个隐患。<br><br>有一类场景，其实不适合用ReuseExchange，就是“一个大Shuffle，替代两个小Shuffle”。也就是两个带Filter的Shuffle，Filter之后数据集比较小，后面即便带Shuffle，其实开销也还好。这个时候，为了利用ReuseExchange，如果强行在最前面Repartition，那么就把两个小Shuffle，变成了一个大Shuffle，如果这个大Shuffle的开销，大于两个小Shuffle的开销，那么ReuseExchange就是不值得的。<br><br>当然，这么说可能比较抽象，等以后有机会，我们可以找个例子讲一讲，你也可以在平时多关注下这种“两个相似的小Shuffle，与一个大Shuffle”之间的开销对比，或者回想一下，之前工作里面有没有遇到类似的Case。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620315893,"ip_address":"","comment_id":291320,"utype":1}],"discussion_count":1,"race_medal":0,"score":"18800074104","product_id":100073401,"comment_content":"第二题，想到有一种情况：假如原始日志数据中，某些用户的访问日志比较多，而且在调用repartition()哈希分区之后，恰好都落到了一个分区，那后续的计算可能会出现数据倾斜。其他的没想到哈哈","like_count":4,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519439,"discussion_content":"说得好~ 👍，数据倾斜确实是个隐患。\n\n有一类场景，其实不适合用ReuseExchange，就是“一个大Shuffle，替代两个小Shuffle”。也就是两个带Filter的Shuffle，Filter之后数据集比较小，后面即便带Shuffle，其实开销也还好。这个时候，为了利用ReuseExchange，如果强行在最前面Repartition，那么就把两个小Shuffle，变成了一个大Shuffle，如果这个大Shuffle的开销，大于两个小Shuffle的开销，那么ReuseExchange就是不值得的。\n\n当然，这么说可能比较抽象，等以后有机会，我们可以找个例子讲一讲，你也可以在平时多关注下这种“两个相似的小Shuffle，与一个大Shuffle”之间的开销对比，或者回想一下，之前工作里面有没有遇到类似的Case。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620315893,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":306145,"user_name":"wow_xiaodi","can_delete":false,"product_type":"c1","uid":1511712,"ip_address":"","ucode":"B3FB301556A7EA","user_header":"https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg","comment_is_top":false,"comment_ctime":1628405836,"is_pvip":false,"replies":[{"id":"111351","content":"好问题，pv、uv的聚合逻辑，确实不一样，但是他们reuse的，是repartition的shuffle结果，也就是这条语句产生的DataFrame：<br><br>val df: DataFrame = spark.read.parquet(filePath).repartition($&quot;userId&quot;)<br><br>这里的Shuffle，还没有任何的聚合逻辑，纯粹是数据的重分发而已。这样Shuffle过后，实际上提前按照userId做了分组，也就是你说的，纯粹按照&lt;partitionid, key&gt;做数据分发。因此，后面的语句里面的groupBy实际上就被“bypass”了。也就是说，尽管语法上是在做分组，但是分组这件事，已经提前被做了。<br><br>val dfPV: DataFrame = df.groupBy(&quot;userId&quot;).agg(count(&quot;page&quot;).alias(&quot;value&quot;)).withColumn(&quot;metrics&quot;, lit(&quot;PV&quot;))<br><br>val dfUV: DataFrame = df.groupBy(&quot;userId&quot;).agg(countDistinct(&quot;page&quot;).alias(&quot;value&quot;)).withColumn(&quot;metrics &quot;, lit(&quot;UV&quot;))<br><br>所以，总结下来，后面聚合逻辑的不同，并不影响pv、uv共享（reuse）分组之后的那份数据集。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1629185879,"ip_address":"","comment_id":306145,"utype":1}],"discussion_count":1,"race_medal":0,"score":"14513307724","product_id":100073401,"comment_content":"老师您好，关于ReuseExchange的例子，由于计算pv uv的聚合算法是不一样的，那么在spill阶段就不可能进行相同的map combine操作。这个ReuseExchange里发生的shuffle是不是单纯只做了按&lt;partitionid, key&gt;来排序的操作，而没有进行combine操作呢？而且对于map端聚合发生在shuffle阶段后面，还紧接着reduce聚合操作，不是很符合常规mr过程，有点难理解，望老师指点迷津。","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":524646,"discussion_content":"好问题，pv、uv的聚合逻辑，确实不一样，但是他们reuse的，是repartition的shuffle结果，也就是这条语句产生的DataFrame：\n\nval df: DataFrame = spark.read.parquet(filePath).repartition($&amp;quot;userId&amp;quot;)\n\n这里的Shuffle，还没有任何的聚合逻辑，纯粹是数据的重分发而已。这样Shuffle过后，实际上提前按照userId做了分组，也就是你说的，纯粹按照&amp;lt;partitionid, key&amp;gt;做数据分发。因此，后面的语句里面的groupBy实际上就被“bypass”了。也就是说，尽管语法上是在做分组，但是分组这件事，已经提前被做了。\n\nval dfPV: DataFrame = df.groupBy(&amp;quot;userId&amp;quot;).agg(count(&amp;quot;page&amp;quot;).alias(&amp;quot;value&amp;quot;)).withColumn(&amp;quot;metrics&amp;quot;, lit(&amp;quot;PV&amp;quot;))\n\nval dfUV: DataFrame = df.groupBy(&amp;quot;userId&amp;quot;).agg(countDistinct(&amp;quot;page&amp;quot;).alias(&amp;quot;value&amp;quot;)).withColumn(&amp;quot;metrics &amp;quot;, lit(&amp;quot;UV&amp;quot;))\n\n所以，总结下来，后面聚合逻辑的不同，并不影响pv、uv共享（reuse）分组之后的那份数据集。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1629185879,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290211,"user_name":"Geek_d794f8","can_delete":false,"product_type":"c1","uid":2485585,"ip_address":"","ucode":"1E20DA4FF8B800","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132","comment_is_top":false,"comment_ctime":1619431242,"is_pvip":false,"replies":[{"id":"105341","content":"好问题，这里确实有点Tricky。<br><br>这部分其实需要一些“前置引用”的知识，这些知识其实是在22讲Physical Planning才会涉及，所以在这一讲这里会比较难理解。<br><br>是这样的，Spark SQL执行计划中的每一个节点，都有4个重要的属性，分别是：<br>节点要求的输入：<br>1）requiredChildDistribution<br>2）requiredChildOrdering<br>节点的输出：<br>3）outputPartitioning<br>4）outputOrdering<br><br>每个节点都有输出的分区情况、排序情况，也就是3）、4）；同时，每个节点对于自己的子节点，都有关于 分区和排序的要求，也就是1）、2）。<br><br>当子节点的分区与排序情况，不满足当前节点的输入要求时，Spark SQL就会在Physical planning阶段，强行插入一些中间节点，比如Exchange（Shuffle）。<br><br>回答你的问题：咱们的例子中，两个groupBy，都是从Parquet读源数据，这个时候，读取Parquet的Scan节点，它的outputPartitioning，和groupBy这个节点对于输入的要求，两者是不match的，因此，Spark SQL会强行插入Exchange操作符。<br><br>尽管我们肉眼能看出来，两个groupBy需要的shuffle是一样的，但是，Spark SQL可不知道，这件事对它来说，是透明的。它没有任何渠道，去知道说：“OK，他俩是一样的，我可以复用”<br><br>repartition的意义在于，它一下变成了两个groupBy共同的依赖，repartition之后的DataFrame，它的outputPartitioning，刚好都是groupBy节点所需要的，因此Spark SQL回溯到这里的时候，就会发现：“我去，之前的计算已经满足我的requiredChildDistribution，我直接拿来用就好了。”<br><br>逻辑就是这么个逻辑，可能一两句话没说清楚，可以重点关注22讲的EnsureRequirements那部分，那部分会咱们这里交代的逻辑做详细展开哈~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619693616,"ip_address":"","comment_id":290211,"utype":1}],"discussion_count":2,"race_medal":0,"score":"14504333130","product_id":100073401,"comment_content":"老师，df.groupBy(&quot;userId&quot;).agg(count(&quot;page&quot;)和df.groupBy(&quot;userId&quot;).agg(countDistinct(&quot;page&quot;)都会按照userId的hashcode值进行Hash Partitioner。而repartition($&quot;userId&quot;)，只不过是在这之前进行了Hash Partitioner的shuffle操作。本质上是一样的呀？为什么最后一个会reuseExchange，一个不会呢？","like_count":3,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519139,"discussion_content":"好问题，这里确实有点Tricky。\n\n这部分其实需要一些“前置引用”的知识，这些知识其实是在22讲Physical Planning才会涉及，所以在这一讲这里会比较难理解。\n\n是这样的，Spark SQL执行计划中的每一个节点，都有4个重要的属性，分别是：\n节点要求的输入：\n1）requiredChildDistribution\n2）requiredChildOrdering\n节点的输出：\n3）outputPartitioning\n4）outputOrdering\n\n每个节点都有输出的分区情况、排序情况，也就是3）、4）；同时，每个节点对于自己的子节点，都有关于 分区和排序的要求，也就是1）、2）。\n\n当子节点的分区与排序情况，不满足当前节点的输入要求时，Spark SQL就会在Physical planning阶段，强行插入一些中间节点，比如Exchange（Shuffle）。\n\n回答你的问题：咱们的例子中，两个groupBy，都是从Parquet读源数据，这个时候，读取Parquet的Scan节点，它的outputPartitioning，和groupBy这个节点对于输入的要求，两者是不match的，因此，Spark SQL会强行插入Exchange操作符。\n\n尽管我们肉眼能看出来，两个groupBy需要的shuffle是一样的，但是，Spark SQL可不知道，这件事对它来说，是透明的。它没有任何渠道，去知道说：“OK，他俩是一样的，我可以复用”\n\nrepartition的意义在于，它一下变成了两个groupBy共同的依赖，repartition之后的DataFrame，它的outputPartitioning，刚好都是groupBy节点所需要的，因此Spark SQL回溯到这里的时候，就会发现：“我去，之前的计算已经满足我的requiredChildDistribution，我直接拿来用就好了。”\n\n逻辑就是这么个逻辑，可能一两句话没说清楚，可以重点关注22讲的EnsureRequirements那部分，那部分会咱们这里交代的逻辑做详细展开哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619693616,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1056982,"avatar":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","nickname":"kingcall","note":"","ucode":"508884DC684B5B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":370538,"discussion_content":"为了满足文中提到的第一个条件，查询的分区规则和shuffle 的分区规则一致，所以你看到我们这里的groupBy 的key 是userId，repartition的key 也是userId","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1619446159,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290012,"user_name":"RespectM","can_delete":false,"product_type":"c1","uid":2536840,"ip_address":"","ucode":"7FA4F794D2D859","user_header":"https://static001.geekbang.org/account/avatar/00/26/b5/88/477e7812.jpg","comment_is_top":false,"comment_ctime":1619320702,"is_pvip":false,"replies":[{"id":"105232","content":"确实是看需求、看场景哈，没有一个统一的标准，不过，还是有一些General的Guidelines可以遵循，核心思路还是“消除瓶颈”。硬件的选型，取决于你的计算负载类型，也就是计算密集型？内存密集型？还是磁盘、网络密集型？<br><br>我们来分类讨论：<br>1. 如果你的计算场景涉及到大量的聚合、排序、哈希计算、数值计算，等等，那么你的机器配置就要加强CPU<br>2. 如果你的计算场景需要反复消耗同一份或是同一批数据集，比如机器学习、数据分析、图计算，那么，为了把需要频繁访问的数据缓存进内存，你自然需要加大内存配置。<br>3. 如果你的计算场景会引入大量shuffle，又不能通过广播来消除Shuffle，那么你就需要配置足够的SSD以及高吞吐网络。<br><br>解答得比较粗糙，并没有具体可操作的建议，都是一些Guidelines，希望能帮到你~<br><br>","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619512708,"ip_address":"","comment_id":290012,"utype":1}],"discussion_count":2,"race_medal":0,"score":"14504222590","product_id":100073401,"comment_content":"老师好，可以讲讲企业级Spark的机器硬件该如何选行吗，以及应该考虑哪些问题。网上都是说根据需求，有什么根据吗，能不能举个例子。","like_count":3,"discussions":[{"author":{"id":2536840,"avatar":"https://static001.geekbang.org/account/avatar/00/26/b5/88/477e7812.jpg","nickname":"RespectM","note":"","ucode":"7FA4F794D2D859","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":371018,"discussion_content":"谢谢老师，我现在遇到两个问题始终找不到原因。一个是比较大的表(parquet snappy)中有大map类型，当join的时候，shuffle时候总ion ，即便分了很大的内存和堆外内存，yarn还是会oom 。然而用hive跑，分很少内存就可以跑过。还有一个就是yarn 的 resourcemanager 总被spark程序跑挂，怎么能提高它的稳定性呢","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619612378,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":2536840,"avatar":"https://static001.geekbang.org/account/avatar/00/26/b5/88/477e7812.jpg","nickname":"RespectM","note":"","ucode":"7FA4F794D2D859","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":375917,"discussion_content":"OOM的问题，可以参考17讲内存视角三：OOM，那里面OOM讲的比较多","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1621867977,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":371018,"ip_address":""},"score":375917,"extra":""}]}]},{"had_liked":false,"id":322801,"user_name":"tiankonghewo","can_delete":false,"product_type":"c1","uid":1476427,"ip_address":"","ucode":"7A55A9C17DD9DF","user_header":"https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg","comment_is_top":false,"comment_ctime":1637593841,"is_pvip":false,"replies":[{"id":"117414","content":"确实，例子确实有些牵强了，这里主要是为了介绍ReuseExchange策略，方便大家容易理解，老弟不妨推而广之、举一反三哈~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1637987092,"ip_address":"","comment_id":322801,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5932561137","product_id":100073401,"comment_content":"这种例子很牵强, 正常人都是 agg中直接count和countDistinct了, 还有人分开统计吗","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":533826,"discussion_content":"确实，例子确实有些牵强了，这里主要是为了介绍ReuseExchange策略，方便大家容易理解，老弟不妨推而广之、举一反三哈~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1637987092,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":291446,"user_name":"aof","can_delete":false,"product_type":"c1","uid":1062864,"ip_address":"","ucode":"5815D63C4926BC","user_header":"https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg","comment_is_top":false,"comment_ctime":1620290445,"is_pvip":false,"replies":[{"id":"105585","content":"确实比较苛刻~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1620312164,"ip_address":"","comment_id":291446,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5915257741","product_id":100073401,"comment_content":"验证了一下第一题，改成count(*)之后再打印Physical Plan发现ReuseExchange失效了，这也印证了老师说的ReuseExchange的第二个触发条件“多个查询所涉及的字段要保持一致”，这个触发条件真的有点严格哈哈","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519472,"discussion_content":"确实比较苛刻~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1620312164,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290293,"user_name":"kingcall","can_delete":false,"product_type":"c1","uid":1056982,"ip_address":"","ucode":"508884DC684B5B","user_header":"https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg","comment_is_top":false,"comment_ctime":1619489007,"is_pvip":false,"replies":[{"id":"105342","content":"第一个问题，可以参考我留给@ Geek_d794f8 同学的答复哈，应该就在你这条留言的下面~<br><br>关于第二个问题，我理解你的问题是，为什么groupBy之后，不能加cache，或者说，没有cache API。其实你已经给出答案啦~ 也就是你说的1、2。我同意你的说法，分组这种操作，仅仅是聚合的前奏，换句话说，如果没有聚合，很多场景是根本不需要分组的。所以把使用频率极低的数据做缓存，没有意义。","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619694340,"ip_address":"","comment_id":290293,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5914456303","product_id":100073401,"comment_content":"感觉我们只要能让算子复用shuffle 后的数据就可以做到减少shuffle,但是文中提到ReuseExchange条件太苛刻，而评论中说的缓存groupBy后的结果也就是shuffle 后的结果，供后续多个聚合统计又做不到，哈哈，这是不是spark 后续可以优化的地方<br>我觉得想想为啥spark 没有给RelationalGroupedDataset 添加cache操作，理论上来说shuffle 后的数据和shuffle 前的数据量是一致的，只要shuffle前的数据可以缓存下来，shuffle 后的数据也可以缓存下来,是不是考虑到了 1. shuffle 后的数据可能分布不均，导致个别partition 过大不能cache, 2. 认为shuffle 后的数据使用频率比较低，我们更多关注的是shuffle 聚合后的结果。<br>希望老师解答一二","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519163,"discussion_content":"第一个问题，可以参考我留给@ Geek_d794f8 同学的答复哈，应该就在你这条留言的下面~\n\n关于第二个问题，我理解你的问题是，为什么groupBy之后，不能加cache，或者说，没有cache API。其实你已经给出答案啦~ 也就是你说的1、2。我同意你的说法，分组这种操作，仅仅是聚合的前奏，换句话说，如果没有聚合，很多场景是根本不需要分组的。所以把使用频率极低的数据做缓存，没有意义。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619694340,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":290066,"user_name":"蠟筆小噺","can_delete":false,"product_type":"c1","uid":1264412,"ip_address":"","ucode":"694ABA92BC48C7","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJN8s3YnzyDRCeg73yzglRgQgk581uIY1FRFO01GibMro4Mbxk58rRgulZTKrSGnd8ZD6RHY8uQj2A/132","comment_is_top":false,"comment_ctime":1619343473,"is_pvip":false,"replies":[{"id":"105203","content":"内存无限大，JVM的配置参数xmx、xms是有界的呀 😃","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619434401,"ip_address":"","comment_id":290066,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5914310769","product_id":100073401,"comment_content":"内存无限大的情况下，对象不会遇到放不下的问题，为什么会有GC呢。。。。","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519100,"discussion_content":"内存无限大，JVM的配置参数xmx、xms是有界的呀 😃","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619434401,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1221195,"avatar":"https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg","nickname":"zxk","note":"","ucode":"4BB2BD9D2BCD04","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":370367,"discussion_content":"目前除了最新的 ZGC 跟 Shandoah GC 外，其他GC收集器的GC时间都会随着堆内存的增大而增加。\n这里也想问下老师目前最新的这两个 GC 收集器在超大堆表现如何，是否能投入生产环境使用？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619393621,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289802,"user_name":"Fendora范东_","can_delete":false,"product_type":"c1","uid":1187106,"ip_address":"","ucode":"63EE9DBEE08D70","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg","comment_is_top":false,"comment_ctime":1619180385,"is_pvip":false,"replies":[{"id":"105147","content":"ReuseExchange指的是，后面相似的计算逻辑复用Shuffle计算的中间文件，也就是Shuffle map阶段输出到本地磁盘的中间文件，这和Shuffle的Map端聚合并不冲突，实际上，直白地说，是没什么关系","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619315293,"ip_address":"","comment_id":289802,"utype":1}],"discussion_count":1,"race_medal":0,"score":"5914147681","product_id":100073401,"comment_content":"有个疑问<br>有reuseexchange算子的执行计划中，既然reuseexchange阶段是shuffle，shuffle后面直接就应该是reduce聚合了，为啥在这两者之间还存在map端聚合？","like_count":1,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":519022,"discussion_content":"ReuseExchange指的是，后面相似的计算逻辑复用Shuffle计算的中间文件，也就是Shuffle map阶段输出到本地磁盘的中间文件，这和Shuffle的Map端聚合并不冲突，实际上，直白地说，是没什么关系","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619315293,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":360344,"user_name":"组织灵魂 王子健","can_delete":false,"product_type":"c1","uid":2957218,"ip_address":"法国","ucode":"94DB462CEAFAD3","user_header":"https://static001.geekbang.org/account/avatar/00/2d/1f/a2/0ac2dc38.jpg","comment_is_top":false,"comment_ctime":1666461055,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666461055","product_id":100073401,"comment_content":"不太理解为什么ReuseExchange没办法做到在已经满足分区的DataFrame上执行不同的操作，真的还不如直接使用persist MEMORY_AND_DISK。不过逻辑其实是一样的。","like_count":0},{"had_liked":false,"id":360343,"user_name":"组织灵魂 王子健","can_delete":false,"product_type":"c1","uid":2957218,"ip_address":"法国","ucode":"94DB462CEAFAD3","user_header":"https://static001.geekbang.org/account/avatar/00/2d/1f/a2/0ac2dc38.jpg","comment_is_top":false,"comment_ctime":1666459872,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1666459872","product_id":100073401,"comment_content":"“一旦一个transformation失败就会触发整条 DAG 从头至尾重新计算，这个说法不准确，因为，失败重复的计算源头并不是整条 DAG 的“头”，而是与触发点距离最新的 Shuffle 的中间文件“。也就是说，失败重试是从失败所在的stage的源头重新开始的，因为每个wide transformation (shuffle)和最后的action是一个Job被分为大于等于一个stages的标准","like_count":0},{"had_liked":false,"id":350936,"user_name":"康","can_delete":false,"product_type":"c1","uid":2621850,"ip_address":"","ucode":"C3E85292E026D5","user_header":"https://static001.geekbang.org/account/avatar/00/28/01/9a/d2831441.jpg","comment_is_top":false,"comment_ctime":1657363588,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1657363588","product_id":100073401,"comment_content":"老师，我想闻一下，ReuseExchange 机制会利用在hive on spark场景嘛？感谢回复","like_count":0},{"had_liked":false,"id":342786,"user_name":"罗盖羽","can_delete":false,"product_type":"c1","uid":2802508,"ip_address":"","ucode":"E68FE072DDF87F","user_header":"https://static001.geekbang.org/account/avatar/00/2a/c3/4c/82b7df02.jpg","comment_is_top":false,"comment_ctime":1650462716,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1650462716","product_id":100073401,"comment_content":"老师，关于ReuseExchange的两个例子，最后一个会reuseExchange，第一个例子为什么需要扫描两次数据源？val df: DataFrame = spark.read.parquet(filePath)不是已经扫描过了一次，后面的两次聚合不都是基于df的操作吗？<br>难道是 spark.read.parquet(filePath)这个操作没有action类的算子触发？","like_count":0},{"had_liked":false,"id":332149,"user_name":"Sampson","can_delete":false,"product_type":"c1","uid":1418226,"ip_address":"","ucode":"BA78CA29A6D898","user_header":"https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg","comment_is_top":false,"comment_ctime":1643070039,"is_pvip":true,"replies":[{"id":"121409","content":"可以的，没问题，可以把df缓存到磁盘，这样效果其实和ReuseExchange是一样的，而且比ReuseExchange复用率更高，因为ReuseExchange机制的条件会比较苛刻，就像文中说的那样~","user_name":"作者回复","user_name_real":"编辑","uid":"1043100","ctime":1643191622,"ip_address":"","comment_id":332149,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1643070039","product_id":100073401,"comment_content":"你好，磊哥，关于ReuseExchange机制，我想问的是，在这个案例中，是否可以将读取文件之后的dataframe cache 下来，后续直接针对这个cache的dataframe操作呢？还是说2者有什么差别？","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":548439,"discussion_content":"可以的，没问题，可以把df缓存到磁盘，这样效果其实和ReuseExchange是一样的，而且比ReuseExchange复用率更高，因为ReuseExchange机制的条件会比较苛刻，就像文中说的那样~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1643191622,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":289688,"user_name":"斯盖丸","can_delete":false,"product_type":"c1","uid":1168504,"ip_address":"","ucode":"B881D14B028F14","user_header":"https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg","comment_is_top":false,"comment_ctime":1619144840,"is_pvip":false,"replies":[{"id":"105145","content":"确实比较苛刻~","user_name":"作者回复","user_name_real":"吴磊","uid":"1043100","ctime":1619314267,"ip_address":"","comment_id":289688,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1619144840","product_id":100073401,"comment_content":"ReuseExchange的第二个条件也太苛刻了吧，难道多选级列就不行了，实用性一下子下来了…","like_count":0,"discussions":[{"author":{"id":1043100,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/ea/9c/230061e7.jpg","nickname":"吴磊","note":"","ucode":"136DC8CF1B10DC","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":518992,"discussion_content":"确实比较苛刻~","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1619314267,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]}]}