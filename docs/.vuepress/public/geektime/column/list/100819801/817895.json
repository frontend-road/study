{"id":817895,"title":"12｜为什么企业级的微调不是炼丹是熬酱？","content":"<p>你好，我是产品二姐。</p><p>上节课我们留了两个问题：</p><ul>\n<li>\n<p>问题一，如何让Agent正确地思考，让Agent学会工作流程。</p>\n</li>\n<li>\n<p>问题二，如何让Agent正确地行动，让Agent学会 function calling。</p>\n</li>\n</ul><p>为了带着清晰的思路上手操作，我们先来讨论解题思路。</p><p>这两个问题本质上是一种“程序性”学习，就像给入职员工做企业级的培训一样，<strong>用短时间培养一个熟悉业务操作流程的员工</strong>。具体到自助工单小助手这个场景里，就是客服人员需要学会标准化操作流程和步骤，即SOP（Standard Operation Process）。</p><p>比如：</p><ul>\n<li>\n<p>第一步，收到客户问题，在故障解决知识库里查询解决方案。</p>\n</li>\n<li>\n<p>第二步，如果客户的问题需要上门，则去查询师傅可用时间。</p>\n</li>\n<li>\n<p>第三步，和客户确认上门时间、地点等信息。</p>\n</li>\n<li>\n<p>第四步，确认后提交工单，完毕。</p>\n</li>\n</ul><p>我们把这些问题和处理步骤改写为Think-Act-Observation或者Plan-and-Solve类的提示词，把它们当做few shots放在Agent的提示词里。下次遇到类似问题时，Agent大概率会做出类似的动作。</p><p>这个提示词会是这样：</p><pre><code class=\"language-markdown\">##Few shots\n**User**: 我家充电桩坏了，需要报修\n\n**Think**: 首先，我需要在故障解决知识库中查找关于充电桩故障的解决方案。这可能需要上门维修，所以我还需要准备查询技术人员的可用时间。\n**Action**: 查询故障解决知识库中关于充电桩故障的信息。\n**Observation**: 知识库显示，充电桩故障通常需要专业技术人员现场检查和维修，无法通过远程指导解决。\n\n**Think**: 既然需要上门维修，我现在需要查询技术人员的可用时间。\n**Action**: 查询技术人员的可用时间。\n**Observation**: 系统显示最近的可用时间是明天上午9:00-11:00，后天下午2:00-4:00。\n\n**Think**: 我现在有了可用的时间段，需要与客户确认具体的上门时间和地点。\n**Action**: 询问客户的偏好时间和确切地址。\n**Observation**: 北京市昌平区西二旗软件园2号楼302\n\n**Think**: 客户提供了地址，但还没有选择上门时间。我需要向客户提供可选的时间段，并确认他们的选择。\n**Action**: 向客户提供可选的上门时间段并请求确认。\n**Observation**: 明天上午9:00\n\n**Think**: 我已经收集了所有必要的信息：问题描述、上门时间和地址。现在需要总结这些信息并提交工单。\n**Action**: 总结信息并提交工单。\n**Observation**: 工单已成功提交到系统中。工单号为：CHB20241010001。\n\n**Think**: 工单已提交，现在应该向客户确认所有细节并提供工单号。\n**Action**: 向客户确认服务细节并提供工单号。\n\"非常感谢您的配合。我已经成功为您提交了维修工单。以下是您的维修服务详情：\n工单号：CHB20241010001\n问题：充电桩故障\n上门时间：明天（10月11日）上午9:00\n地址：北京市昌平区西二旗软件园2号楼302\n技术人员将会在约定时间到达您提供的地址。如果您有任何问题或需要变更预约，请随时联系我们，并提供您的工单号。\"\n\n**User**: [另外一个user request的示例]\n...[略]类似上述的Think Observation Action过程\n\n**User**:&lt;User request&gt;\n\n</code></pre><!-- [[[read_end]]] --><p>在这段提示词里，前31行用Think-Act-Observation的模式描述了客服解决工单提交问题的过程。我们会有很多个类似的实例，对应33、34行的内容。在36行的时候，比如用户问，“充电桩有问题了，怎么解决”，那么，模型就会有一定概率按照你给的few shots来执行。</p><p>然后，我们将所有的SOP都按照这种方式来写，就会有很多的few shots。这里的SOP改写可以借助大语言模型提升效率。比如我把下面的提示词给到GPT4o，就会得到相应的Think-Act-Observation few shots。</p><pre><code class=\"language-markdown\">给你一个汽车售后客服，依据流程按照ReAct中的Think， action ， observation，写出3个few shots\n第一步，收到客户问题，比如充电桩问题，或者汽车故障，在故障解决知识库里查询解决方案。\n第二步，如果客户的问题需要上门，则去查询师傅可用时间。\n第三步，和客户确认上门时间、地点等信息。\n第四步，确认后提交工单，完毕。\n\n请在每轮Think-Act-Observation用分割线分割， Think-Act-Observation之间不要有空行。\n</code></pre><p>然后得到如下回复，这个过程加速了few shots 的生成。</p><pre><code class=\"language-markdown\">### 示例 1\n\n**客户问题**：我的电动车充电桩总是充不进电，该怎么办？\n\n---\n\n**Think**：首先需要在故障解决知识库中查询关于充电桩充电问题的解决方案。\n**Action**：在知识库中搜索“充电桩充不进电”问题的解决方法。\n**Observation**：找到了解决方案：1. 检查充电线连接是否稳固；2. 确保充电桩电源正常；3. 将充电桩重启后再试。\n\n---\n\n**Think**：这些步骤可以让客户自行排查。如果问题依旧，可能需要上门进一步检查。\n**Action**：向客户询问是否尝试上述步骤，问题是否解决。\n**Observation**：客户反馈问题依旧存在。\n\n---\n\n**Think**：查询师傅可用时间，以安排时间上门查看问题。\n**Action**：查看系统中师傅的可用时间。\n**Observation**：师傅在明天下午有空。\n\n---\n\n**Think**：与客户确认是否可以安排在明天下午上门服务。\n**Action**：与客户确认明天下午的上门服务时间和地址。\n**Observation**：客户确认可以。\n\n---\n\n**Think**：提交工单以安排上门服务，完成服务请求。\n**Action**：在系统中提交工单。\n\n---\n\n### 示例 2\n\n**客户问题**：我的车子引擎有不正常的噪音。\n\n---\n\n**Think**：需要先在故障解决知识库中查询关于引擎噪音问题的解决方案。\n**Action**：在知识库中搜索“引擎不正常噪音”问题的解决方法。\n**Observation**：获得解决方案：1. 检查引擎是否有异物；2. 核实机油水平是否正常；3. 检查皮带松紧度。\n\n---\n\n**Think**：可以让客户先进行简单检查，如果问题无法解决，则需要上门检查。\n**Action**：告知客户自行检查步骤，并询问检查结果。\n**Observation**：客户尝试后，问题仍未解决。\n\n---\n\n**Think**：需要安排师傅到客户处进行进一步检查。\n**Action**：查询师傅可用时间。\n**Observation**：师傅周六全天有空。\n\n---\n\n**Think**：与客户确认是否可以安排在周六上门检查。\n**Action**：与客户确认周六的上门服务时间和地址。\n**Observation**：客户确认周六可以。\n\n---\n\n**Think**：提交工单以安排上门服务，完成服务请求。\n**Action**：在系统中提交工单。\n\n---\n\n### 示例 3\n\n**客户问题**：我的汽车空调不制冷。\n\n---\n\n**Think**：在故障解决知识库中查询关于空调不制冷的问题解决方案。\n**Action**：在知识库中搜索“空调不制冷”问题的解决方法。\n**Observation**：找到解决方案：1. 检查空调制冷剂是否充足；2. 确保空调滤网没有堵塞。\n\n---\n\n**Think**：询问客户是否可以按照这些步骤检查一下，以便初步解决问题。\n**Action**：与客户沟通自行检查步骤。\n**Observation**：客户自行检查后，发现问题未解决。\n\n---\n\n**Think**：需要安排师傅上门进行专业检修。\n**Action**：查询师傅的可用时间。\n**Observation**：师傅在周三上午有空。\n\n---\n\n**Think**：询问客户是否可以安排在周三上午上门服务。\n**Action**：与客户确认周三上午的上门服务时间和地址。\n**Observation**：客户确认周三上午可以。\n\n---\n\n**Think**：提交工单以安排上门服务，完成服务请求。\n**Action**：在系统中提交工单。\n</code></pre><p>这样，我们就可以通过提示词中的few shots让Agent学会SOP，这种通过few shots学习SOP的过程，其实就是在复现GPT-3论文中提到的《language models are few shots learners》，是不是瞬间把自己拉到了研究员的高度？</p><p>接下来，为了解决第二个问题：让Agent正确地行动，也就是上述提示词中的Action对应的动作要正确被执行，我们也可以在函数调用里加很多few shots。</p><pre><code class=\"language-markdown\">**User request：** 在知识库中搜索“空调不制冷”问题的解决方法。\n**API schema：**此处略 [知识库搜索接口SearchSolution的schema]\n**API request：**&nbsp; SearchSolution(Error = \"空调不制冷\")\n\n---\n\n**User request：**查询师傅的可用时间。&nbsp;\n**API schema：**此处略 [师傅可用时间查询接口SearchAvailableTime的schema]\n**API request：**SearchAvailableTime()\n\n---\n[此处略，很多few shots]\n\n**User request：** &lt;用户的实际问题&gt;\n</code></pre><p>但到这里很多企业就开始犹豫了：函数调用的提示词需要把企业内部的API schema（提示词中的第2、8行） 也发给通用大模型，这～不等于把自己家的接口设计逻辑放网络上“裸奔”了么？ 而且，这样做相当于把自己的业务依赖于别人的服务上，这可不行！那有没有什么其他办法呢？</p><p>[二姐看了一下百宝箱]，当然会有，那就是：微调自己的模型，然后私有化部署。这样数据、模型都专属企业自己，不用担心“裸奔”了。</p><h2>为什么企业需要微调？</h2><p>除了安全之外，微调并私有化企业专属的模型还有两个好处。</p><p><strong>一是在企业专属的领域，微调会让Agent的输出更准确。</strong></p><p>对大多数业务型企业来说，它们并不需要一个能力很强的通用模型，而是一个熟悉自身业务的专属模型。从理论上讲，使用企业专属数据微调过的模型肯定会更专业；从实践中讲，我个人以及身边人的经验，还有诸多论文的实验结果，都表明微调过的模型比不微调的模型在企业专属数据集的测试当中，准确率会有10%左右的提升。</p><p><strong>二是长期来看，在专属业务使用私有化部署微调模型的成本会降低。</strong></p><p>你可能会奇怪了，大家都说私有化部署模型的成本会高，二姐怎么会说低呢？我来给你展示这样一张图：</p><p><img src=\"https://static001.geekbang.org/resource/image/32/a1/32ba03fbf3883a6ce01746f8029c50a1.png?wh=1151x688\" alt=\"图片\"></p><p>这张图里，红线是使用通用大模型（比如GPT4o）的成本，绿线是使用私有化部署模型的成本。私有化部署模型的成本起点会比较高（用线段OA表示），在我们的案例里大概是几千-几万元，但部署后的使用成本会比通用模型低很多。当我们的业务量（token 消耗量）增长到B点的时候，采用私有模型的成本就会跑赢通用模型。</p><p>为了进一步降低私有化模型的启动成本（线段OA的高度），我们可以选择一个比通用模型参数量更小的模型。二姐通过一些论文和自身实践看到：<strong>在专属非推理类的领域，参数量较小的模型经过微调后可能会达到或接近通用大模型的能力。</strong>这就像狗狗的大脑在通用能力上确实比不过人类大脑，但狗狗大脑在嗅觉这个专属领域却可以超过人脑。</p><p>关于成本的详细计算我们会在第16、17节课细细讲述，在这里二姐希望通过这种对比来消除大家对微调成本的忧虑，千万不要“谈虎（微调）色变”。</p><p>既然企业确实有微调模型的需求，我们就来看看如何开启微调过程。在这之前，我从一个产品经理的视角和你说说微调的原理，这部分只需要我们理解，并不需要实践。</p><h2>用千层饼加酱来学习微调过程</h2><p>在我看来，微调模型就像给一个“千层饼加酱”的过程，看到这里，你会不会咽了一下口水呢？ 接下来我就和你聊聊这张“千层饼”。</p><p>第一章里，我用了水流网络图来讲述神经网络原理。这张图的每一层神经元之间有M x N个管道。其中，M是上一层的神经元个数，N 是下一层的神经元个数。这M x N的管宽度值就是模型的参数。</p><p>我们把这M x N个数值放在数学概念里，就是一个矩阵，我把它比作千层饼中的一层饼（图中红色所示）。</p><p><img src=\"https://static001.geekbang.org/resource/image/42/f8/421b869bac80d3df02fc6bcc707b68f8.png?wh=1920x1490\" alt=\"图片\"></p><p>而巨大的神经网络就是由很多层这样的矩阵组成，就构成了我所说的“千层饼”。这层饼越大、越厚，意味着模型的规模也就越大，制作成本也就越高。</p><p><img src=\"https://static001.geekbang.org/resource/image/93/4f/93b15593458e4feee6a7ac3d98612e4f.png?wh=4992x3513\" alt=\"\"></p><p><strong>微调就是要改变这张饼</strong><strong>。</strong>我们之前所说的RAG、提示词、Agent设计方式是不改变饼本身的，这是微调与其他技术的本质区别。</p><p>改变饼的方法有很多，我继续和大家类比。</p><ul>\n<li>方法一：全参微调。</li>\n</ul><p>顾名思义，就是改变网络中所有的参数。这就好像在做完饼之后，再来烤制或者蒸一下来改变千层饼的味道，这种方法会影响饼的每一层。</p><ul>\n<li>方法二：参数高效微调。</li>\n</ul><p>这是指通过调整神经网络中的少数参数来改变饼的味道。就好像做好饼之后，在某些层之间抹上酱，花的功夫较少，但是效果会很明显。因为这种方法很“高效” ，所以给他起了这个好听的名字，你也可以把它称为部分参数微调。而参数高效微调又有很多细分方法，你可以参考我放在文末的链接详细了解。</p><p>在这里我们最常用的就是LoRA微调，我把LoRA微调的原理比作给千层饼的某些特定层中间涂一层“十字酱”后，然后抹开。别急，让我们一步步来拆解这个过程。</p><h3>第一步：找到要涂酱的地方</h3><p>在开源社区Huggingface上，有些开源模型开放了自己的模型架构，比如Mistral-7B，我们可以在登录（有的模型必须在登录后才能看到）后搜索模型Mistral-7B，在模型详情页通过点击“↗️”这个按钮来查看模型。</p><p>比如在Mistral这个开源模型里，我们可以看到Mistral这张千层饼，表面一层是embedding层，底下一层是lm_head层 ， 中间总共有32个Layer， 每个Layer里又有9层。</p><p><img src=\"https://static001.geekbang.org/resource/image/1f/85/1f4e86877eede81e65264ef463e2b285.png?wh=1920x1538\" alt=\"图片\"></p><p>我用不同颜色表示千层饼里不同类型的层，就得到了下面这张图。</p><p><img src=\"https://static001.geekbang.org/resource/image/7a/3a/7ab16d8540fe569c4245985e63af7e3a.png?wh=1920x1289\" alt=\"图片\" title=\"注：这个层级架构就是通常所说的Transformer架构，关于Transformer架构在这里不详细展开，有兴趣的同学可以参考文末链接学习。\"></p><p>LoRA微调就是选定其中的某些层，用涂酱的方式来改变这些层。</p><h3>第二步：如何涂酱？</h3><p>之所以把LoRA训练称为涂酱，是因为LoRA微调过程中，我们会保持基座模型的参数值不变，训练出一个参数矩阵的增量矩阵，也就是在原有参数的基础上增加或者减少的数值。在使用时我们把这个“增量”加到原有的基座参数模型上就可以了，这就像给千层饼涂酱时，我们并不会改变原有的那一层饼。</p><p><img src=\"https://static001.geekbang.org/resource/image/e0/c2/e0be7f7b4a734f4e05891984b5ea9fc2.png?wh=1920x1148\" alt=\"图片\"></p><p>这样做的好处是，这个酱料可以独立保存，而且可以和其他酱料灵活混搭，收获不同口味的千层饼。在实际中，这意味着通过LoRA训练出来的模型只需要保存增量矩阵，降低了存储空间，且可以和其他模型灵活地合并，产生不同的模型效果。</p><p>接下来，我们来说说这层酱是怎么涂到指定的饼层上的。</p><p>我们首先要理解，这层酱的“面积”并不像千层饼的面积那么大，而是被浓缩了的两道十字形其中的一横一纵大小分别是M x R和 R x N，这里的R通常远远小于M、N。</p><p><img src=\"https://static001.geekbang.org/resource/image/ec/17/ec3yy7feb0bed865eda7855fd98b3e17.png?wh=3036x1930\" alt=\"\"></p><p>这里R是Rank，由算法工程师指定的。不过这里的Rank可不是排名的意思，而是线性代数中的秩的概念。你可千万别听到陌生词儿就吓跑哈，我来举个例子你就明白了，请看下图。</p><p><img src=\"https://static001.geekbang.org/resource/image/54/d6/54922b4be0fb60466c27c5ea557eacd6.png?wh=1920x790\" alt=\"图片\"></p><p>观察这张图中的矩阵：左边矩阵第一和第二行对应的元素是等比例的（1:2:3=2:4:6），如果矩阵的每一行代表一个特征的话，这个矩阵里第一行和第二行的特征是一样的，算一个特征。另一个特征是第三行的[3，6，5]。所以这个矩阵总共就有2个行特征向量，它的行秩就是2。</p><p>对矩阵秩的更多解读大家可以进一步学习，但这里，了解到这一步已经足够理解LoRA的本质。</p><p>直观理解，秩就是矩阵中可以抽取、压缩出来的特征数量。特征向量，顾名思义就是特征的数学表示，它可能是一些风格，比如LoRA微调放在图像生成里可以有动漫风格、写实风格；也可能是一些模式，比如不同公司的办事流程模式，也可以被当做特征抽取出来，这也是为什么我们会在企业级应用中会采用LoRA微调的原因。</p><p>而在大语言模型里，每一层的矩阵都是几千 x 几千的超大矩阵，可以想象，这种高维空间的矩阵应该有很多重复的特征向量。如果我们直接训练一个几千 x 几千增量的参数矩阵，既耗时间、耗算力，还得到了很多重复的特征向量，性价比不高。</p><p>于是研究者们就想，不如直接训练出两个低秩矩阵，横向的低秩矩阵是矩阵行特征的浓缩，纵向的低秩矩阵列向量特征的浓缩；然后把这两个矩阵做点积（特征扩散）（如下图红框第二步所示）；最后将这个扩散后的特征叠加到原有的参数矩阵上，就得到了一个全新的训练参数矩阵。这就是LoRA微调的原理。</p><p><img src=\"https://static001.geekbang.org/resource/image/f9/ed/f955b752cfd1cb0ba597ce8b2cf98eed.png?wh=1920x1057\" alt=\"图片\"></p><p>LoRA训练在保留特征的前提下，大大降低了被训练的参数量。比如原来要训练的参数量是M x N，现在要训练的参数量是R x M + R x N = R x（M+N），因为R远小于M、N，因此LoRA训练的参数量有大幅降低（类似下图中的面积对比），这真是个一举两得的聪明做法！</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/4d/6bfcbe0922e8890c3735a289aeea834d.png?wh=1920x1065\" alt=\"图片\"></p><p>那么R的值设定多少合理呢？ 在Lora的开篇论文（参考3）中，作者使用不同的Rank值来实验，发现它对模型效果的影响并不是很大。</p><p><img src=\"https://static001.geekbang.org/resource/image/c6/14/c668728bcdb9a92a1758yyeacacab614.png?wh=1098x416\" alt=\"\"></p><p>现在来看，R是一个经验数值，通常为8、16、32、64，R的大小对模型效果的影响需要通过实验来确定，也就是在固定的数据集上进行评估来确定，这样才能选择到适合自身业务的R值。</p><p>好啦，这下你知道我们要怎么涂这个酱了吧。总结一下，就是横着抹一道（得到矩阵的行特征向量），竖着抹一道（得到矩阵的列特征向量），然后均匀抹开（两个矩阵点积），均匀的敷在对应的那层饼上！</p><p>你可能还会好奇，那这两层酱到底是怎么熬制而成的呢？哈哈，接下来带你解密这份秘制酱料的制作过程，这一步一般由炼丹师（算法工程师）完成，也许下次你可以把他们称为“熬酱师”。</p><h3>第三步：如何制作酱料</h3><p>制作酱料一要了解熬制酱料的“炉子” ，二要了解“原料”。原料准备是下节课的内容，这节课我们先了解一下“炉子”。</p><p>这里炉子就是模型训练的框架，现在发展得已经比较成熟了，算法工程师可以直接拿来使用。对于产品经理，简略了解一下即可。我们拿Huggingface上LoRA训练的代码来简略解读一下，重点部分我都标记出来了，很简单。</p><p><img src=\"https://static001.geekbang.org/resource/image/14/56/140d2aa927a229826d975e55d3430c56.png?wh=1920x911\" alt=\"图片\"></p><p>在模型训练的框架里，有一个loraConfig函数，函数的参数中有target_module和rank。</p><ul>\n<li>\n<p>target_module 就是我们在开头讲的指定涂酱的地方。</p>\n</li>\n<li>\n<p>rank就是在第二步里讲的秩，你可以理解酱料熬制的过程就是原料进行“特征提取”的过程。rank越小，代表特征压缩程度越大，酱就越浓；rank 越大，代表特征压缩程度越小，酱就越淡。</p>\n</li>\n</ul><p>到这里，我们知道了3件事。</p><ol>\n<li>\n<p><strong>千层饼中每层的结构：</strong>借助Huggingface开源模型的示例，拆解大语言模型的架构，从而理解LoRA微调是通过在指定的层上训练参数的。</p>\n</li>\n<li>\n<p><strong>涂酱的过程：</strong>在LoRA训练中，通过两个低秩矩阵的点积，就能够既能保证训练质量，又能降低训练参数量，达到“参数高效”微调的效果。</p>\n</li>\n<li>\n<p><strong>熬制酱料的工具：</strong>如何通过模型训练框架来指定模型特征的压缩程度Rank，和要训练的参数层（Target_module）。</p>\n</li>\n</ol><p>下节课我就来准备熬制酱料的原料，即数据了。</p><h2>小结</h2><p>我们来小结一下，在这节课的开头，我们尝试用few shots让Agent能够正确地思考、行动，然而考虑到安全性，我们引入了微调并私有化企业专属的模型的方法来解决这一问题，进而和大家一起学习了微调最常见的方法：LoRA微调，也就是给大语言模型这张千层饼涂酱的过程，相信大家一定记住了二姐这节课的特色“比喻”。</p><p>这节课产品经理们更多的是理解，并非实践，而在下节课我们就要结合“自助工单小助手”这个案例来准备训练数据、评估模型了，相当于准备熬酱原料，并在熬制之后还要学会品尝酱料。我们下节课继续吧。</p><h2>课后题</h2><p>这节课，我们提到LoRA 中被训练的参数量大幅降低，那么到底降幅有多大呢？ 大家一起和我做一道加减乘除运算题来体会一下。</p><p>观察Huggingface 上训练 <a href=\"https://huggingface.co/spaces/PEFT/causal-language-modeling/blob/main/lora_clm_with_additional_tokens.ipynb\">Mistral 7B的案例</a>：</p><p><img src=\"https://static001.geekbang.org/resource/image/07/01/0735c3c78e7a271ec83c4aa279a80a01.png?wh=1920x905\" alt=\"图片\"></p><p>在这个案例里，</p><ul>\n<li>\n<p>指定Target Module 是\"embed_tokens\", “lm_head”, “q_proj”, “v_proj”，结合<a href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1?show_file_info=model.safetensors.index.json\">Mistral-7B这个模型的架构</a>，这几层对应的参数矩阵大小分别是：32000 x 4096， 32000 x 4096，4096 x 4096，1024 x 4096。</p>\n</li>\n<li>\n<p>指定Rank=64。</p>\n</li>\n</ul><p>根据这些数值，计算LoRA微调总共需要的训练参数，看看和Mistral 7B的总共参数量7,273,849,000相比，有多大幅度的降低呢？</p><h2>参考</h2><ol>\n<li>\n<p>参数高效微调： <a href=\"https://huggingface.co/docs/peft/index\">https://huggingface.co/docs/peft/index</a></p>\n</li>\n<li>\n<p>Transformer：<a href=\"https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.788.recommend_more_video.0\">https://www.bilibili.com/video/BV1TZ421j7Ke/</a></p>\n</li>\n<li>\n<p>Lora开篇论文：<a href=\"http://arxiv.org/pdf/2106.09685\">https://arxiv.org/pdf/2106.09685</a></p>\n</li>\n</ol>","neighbors":{"left":{"article_title":"11｜Why：从Chat走向Act，我们还缺什么？","id":817254},"right":{"article_title":"13｜通过微调提升自助工单小助手的可靠性","id":818518}},"comments":[{"had_liked":false,"id":395119,"user_name":"苏果果","can_delete":false,"product_type":"c1","uid":2852467,"ip_address":"北京","ucode":"12A62ED032F345","user_header":"https://static001.geekbang.org/account/avatar/00/2b/86/73/5190bbde.jpg","comment_is_top":false,"comment_ctime":1729496635,"is_pvip":false,"replies":[{"id":143471,"content":"哈哈，给你画一张大饼","user_name":"作者回复","user_name_real":"编辑","uid":3885064,"ctime":1729498579,"ip_address":"内蒙古","comment_id":395119,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100819801,"comment_content":"技术小白也能读懂的LoRA微调过程😢泪目了","like_count":4,"discussions":[{"author":{"id":3885064,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/48/08/e18b1a00.jpg","nickname":"哄哄","note":"","ucode":"F66093BE6A2953","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652704,"discussion_content":"哈哈，给你画一张大饼","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729498579,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"内蒙古","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":1,"child_discussions":[{"author":{"id":2852467,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/86/73/5190bbde.jpg","nickname":"苏果果","note":"","ucode":"12A62ED032F345","race_medal":0,"user_type":8,"is_pvip":false},"reply_author":{"id":3885064,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/48/08/e18b1a00.jpg","nickname":"哄哄","note":"","ucode":"F66093BE6A2953","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":652705,"discussion_content":"多涂点酱，谢谢😗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729498619,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":652704,"ip_address":"北京","group_id":0},"score":652705,"extra":""}]}]},{"had_liked":false,"id":395598,"user_name":"叶绘落","can_delete":false,"product_type":"c1","uid":1652234,"ip_address":"广东","ucode":"81A249CC67B9A9","user_header":"https://static001.geekbang.org/account/avatar/00/19/36/0a/a14b6af4.jpg","comment_is_top":false,"comment_ctime":1731371694,"is_pvip":false,"replies":[{"id":143620,"content":"🤗🤗","user_name":"作者回复","user_name_real":"编辑","uid":3885064,"ctime":1731389914,"ip_address":"北京","comment_id":395598,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100819801,"comment_content":"将模型比喻为千层饼，将微调比喻为抹酱。\n\n将复杂的专业概念简化为易懂的日常用语。\n\n作者用心了，点赞。","like_count":0,"discussions":[{"author":{"id":3885064,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/48/08/e18b1a00.jpg","nickname":"哄哄","note":"","ucode":"F66093BE6A2953","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653669,"discussion_content":"🤗🤗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1731389914,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395436,"user_name":"阿呆ଳ","can_delete":false,"product_type":"c1","uid":3964295,"ip_address":"北京","ucode":"35C5449DA2743A","user_header":"https://static001.geekbang.org/account/avatar/00/3c/7d/87/fbe507d7.jpg","comment_is_top":false,"comment_ctime":1730795325,"is_pvip":false,"replies":[{"id":143580,"content":"🙌","user_name":"作者回复","user_name_real":"编辑","uid":3885064,"ctime":1731243214,"ip_address":"北京","comment_id":395436,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100819801,"comment_content":"1. 计算LoRA参数量：LoRA参数量=embed_tokens应用LoRA后的参数量+lm_head应用LoRA后的参数量+q_proj应用LoRA后的参数量+v_proj应用LoRA后的参数量=64*(32000+4096)+64*(32000+4096)+64*(4096+4096)*32+64*(1024+4096)*32=31,883,264;\n2. 计算降幅：降幅=下降量&#47;原始参数量=(原始参数量-LoRA参数量)&#47;原始参数量=(7,273,849,000-31,883,264)&#47;7,273,849,000*100%≈99.56%","like_count":0,"discussions":[{"author":{"id":3885064,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/48/08/e18b1a00.jpg","nickname":"哄哄","note":"","ucode":"F66093BE6A2953","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653581,"discussion_content":"🙌","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1731243214,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395371,"user_name":"kevin","can_delete":false,"product_type":"c1","uid":3920101,"ip_address":"广东","ucode":"19E83C631DF25E","user_header":"https://static001.geekbang.org/account/avatar/00/3b/d0/e5/0a3ee17c.jpg","comment_is_top":false,"comment_ctime":1730458809,"is_pvip":false,"replies":[{"id":143543,"content":"哈哈，谢谢，从看论文那天就开始想着怎么能说明白。--费曼学习法","user_name":"作者回复","user_name_real":"编辑","uid":3885064,"ctime":1730697552,"ip_address":"内蒙古","comment_id":395371,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100819801,"comment_content":"通俗易懂，把微调是如何节约资源，形象化的说明白了。","like_count":0,"discussions":[{"author":{"id":3885064,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/48/08/e18b1a00.jpg","nickname":"哄哄","note":"","ucode":"F66093BE6A2953","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653330,"discussion_content":"哈哈，谢谢，从看论文那天就开始想着怎么能说明白。--费曼学习法","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1730697552,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"内蒙古","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395142,"user_name":"蝈大虾","can_delete":false,"product_type":"c1","uid":1177205,"ip_address":"上海","ucode":"BF6F671E6F53E7","user_header":"https://static001.geekbang.org/account/avatar/00/11/f6/75/f221dbbf.jpg","comment_is_top":false,"comment_ctime":1729585964,"is_pvip":false,"replies":[{"id":143478,"content":"思路是对的，不过q v层是有32个，所以这两个地方还需要乘以32哦","user_name":"作者回复","user_name_real":"编辑","uid":3885064,"ctime":1729603963,"ip_address":"北京","comment_id":395142,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100819801,"comment_content":"使用 LoRA 微调时，会在指定的层中引入低秩适配（Low-Rank Adaptation）矩阵。假设Rank=64，可以计算在每个相关层中需要的额外参数。\n\n### 1. “LoRA 参数计算“\n\n对于每个需要应用 LoRA 的层，假设有一个原始参数矩阵 (W)，维度为 (d × k)，LoRA 将引入两个低秩矩阵 (A) 和 (B)：\n- (A) 的维度为 (d × {Rank})\n- (B) 的维度为 ({Rank} × k)\n\n### 2. “计算每层的参数“\n\n根据提供的矩阵：\n- “embed_tokens (32000 x 4096)“：\n  - (A: 32000 × 64) → (2,048,000)\n  - (B: 64 × 4096) → (262,144)\n  - 总计：(2,048,000 + 262,144 = 2,310,144)\n\n- “lm_head (32000 x 4096)“：\n  - 总计同上：(2,310,144)\n\n- “q_proj (4096 x 4096)“：\n  - (A: 4096 × 64) → (262,144)\n  - (B: 64 × 4096) → (262,144)\n  - 总计：(262,144 + 262,144 = 524,288)\n\n- “v_proj (1024 x 4096)“：\n  - (A: 1024 × 64) → (65,536)\n  - (B: 64 × 4096) → (262,144)\n  - 总计：(65,536 + 262,144 = 327,680)\n\n### 3. “总结 LoRA 参数量“\n\n将所有层的 LoRA 参数量相加：\n- “embed_tokens“：(2,310,144)\n- “lm_head“：(2,310,144)\n- “q_proj“：(524,288)\n- “v_proj“：(327,680)\n\n总计：  [ 2,310,144 + 2,310,144 + 524,288 + 327,680 = 5,472,256 ]\n\n### 4. “与 Mistral 7B 的参数量对比“\n\nMistral 7B 的总参数量为 “7,273,849,000“。与 LoRA 微调所需的参数量相比：\n[text{节省比例} = frac{7,273,849,000 - 5,472,256}{7,273,849,000} approx 99.925%]\n\n### 结论\n\n通过 LoRA 微调，总共需要的训练参数仅为 “5,472,256“，与 Mistral 7B 的参数量相比，大幅度降低了约 “99.925%“，使得微调更加高效。\n\n","like_count":0,"discussions":[{"author":{"id":3885064,"avatar":"https://static001.geekbang.org/account/avatar/00/3b/48/08/e18b1a00.jpg","nickname":"哄哄","note":"","ucode":"F66093BE6A2953","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":652772,"discussion_content":"思路是对的，不过q v层是有32个，所以这两个地方还需要乘以32哦","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1729603963,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]}]}