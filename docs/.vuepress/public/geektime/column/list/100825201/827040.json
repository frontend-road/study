{"id":827040,"title":"21｜文本大模型：chatGLM2-6B的本地部署与前端集成","content":"<p>你好，我是柳博文，欢迎和我一起学习前端工程师的AI实战课。</p><p>通过前面的学习，相信你已经逐渐熟悉了AI+前端的开发新范式，对于如何把AI引入到前端工作中有更深的认识。</p><p>自从2022年10月ChatGPT问世，已经过去了600多天。在此期间，大模型不断发展，从文本大模型到文生图，图生图大模型，再到现在的文生视频大模型。这些大模型在持续优化进步，让我们切身实际地感受到了AI的力量，未来已来。</p><p>接下来的课程，我们就通过动手实验，把这些模型部署到本地，体验一下这些模型的效果。</p><p>为了方便国内环境的部署与使用，文本大模型我们选择清华开源的ChatGLM-6B，文生图片大模型选择开源的 StableDiffusion，视频生成大模型则选择腾讯开源的大模型。</p><p>那么，这节课我们先来本地部署ChatGLM-6B模型，并实现一个网页来与模型进行问答交互。</p><h2>初识ChatGLM-6B 模型</h2><p>ChatGLM-6B 是基于 <strong>GLM（General Language Model）</strong> 架构的一个针对中英文双语优化的对话生成模型。它与 GPT 模型类似，但针对中文进行了特别的优化，因此在处理中文任务时表现更加友好。</p><p>该模型有 60 亿参数，虽然规模比不上 GPT-3 的 1750 亿参数，但它在对话生成任务中具有优秀的平衡性，既能保证较高的生成效果，又不需要过于庞大的计算资源，非常适合在本地部署和运行。</p><!-- [[[read_end]]] --><p><strong>ChatGLM-6B 的核心特点</strong></p><p>首先，ChatGLM-6B 经过大规模中英文语料库的训练，因此在双语对话场景下表现得非常流畅。无论是中文用户还是英文用户，都可以通过与模型对话获得自然、连贯的答案。</p><p>其次，ChatGLM-6B 在普通设备上（例如 CPU 或较小的 GPU）也能高效运行。对于许多开发者来说，这意味着他们可以在本地部署 ChatGLM-6B，而不必依赖云计算资源，降低了成本，也保证了个人数据的安全。</p><h2>如何在本地部署 ChatGLM-6B 项目</h2><p>你可能认为部署大语言模型需要强大的计算资源或云端支持，然而 ChatGLM-6B 的一大优势就是可以在本地运行，即使使用消费级硬件也可以高效推理。</p><p>接下来我们就进入动手环节，学习如何将 ChatGLM-6B 模型部署到本地环境，并进行推理操作。</p><h3><strong>环境准备</strong></h3><p>在本地运行 ChatGLM-6B 之前，我们需要先准备好相应的运行环境。为了保证推理性能，建议使用具备一定计算能力的设备，以下是基本的硬件和软件需求：</p><ul>\n<li>操作系统：Linux、Windows 或 MacOS。</li>\n<li>Python 版本：Python 3.8 或以上。</li>\n<li>硬件要求：如果有 GPU，推理速度会大大提升，特别是使用 CUDA 支持的显卡（如 Nvidia）。</li>\n<li>软件依赖：Anaconda（方便管理依赖包）、PyTorch（用于模型推理）。</li>\n</ul><h3>安装 ChatGLM-6B</h3><p>这里我们仍然使用conda来进行ChatGLM-6B的环境创建和部署。使用以下命令创建一个虚拟环境并激活它。</p><pre><code class=\"language-powershell\">conda create -n chatglm python=3.8\nconda activate chatglm\n</code></pre><p>进入虚拟环境后，使用 pip 命令安装 PyTorch 和 Hugging Face 的 transformers 库， 同时根据 requirements.txt 中的内容安装依赖库。</p><pre><code class=\"language-powershell\">pip install torch transformers huggingface_hub\npip install -r requirement.txt\n</code></pre><p>之后我们通过 Hugging Face Hub 下载 ChatGLM-6B 模型。这一步骤只需要运行一次，模型下载后就可以在本地使用，考虑到网络和下载问题，我已经将模型下载完成并放在了代码库中，可以直接下载运行即可。</p><pre><code class=\"language-powershell\">python -c \"from transformers import AutoTokenizer, AutoModel; tokenizer = AutoTokenizer.from_pretrained('THUDM/chatglm-6b'); model = AutoModel.from_pretrained('THUDM/chatglm-6b')\"\n</code></pre><p><strong>启动模型并进行本地推理</strong></p><p>在模型安装完成后，接下来我们可以编写一个简单的 Python 脚本来启动 ChatGLM-6B 模型，并与其进行对话。以下是一个示例代码：</p><pre><code class=\"language-python\">from transformers import AutoTokenizer, AutoModel\nimport torch\n\n# 加载模型和 tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\")\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm-6b\").half().cuda()\n\n# 定义推理函数\ndef ask_chatglm(question):\n    inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 测试模型\nprint(ask_chatglm(\"你好，今天的天气如何？\"))\n</code></pre><p>这段代码会调用 ChatGLM-6B 模型，并生成回答。如果你的设备支持 GPU，可以使用 half().cuda() 来加速推理过程。如果没有 GPU的话，可以将 .half().cuda() 替换为 .float() ，这样就会使用 CPU 进行推理。</p><h3>优化和调试</h3><p>在本地运行模型时，我们有时可能会遇到显存不足或者计算资源限制等问题。对于 GPU 用户，可以通过使用 half() 方法来减少模型占用的显存，同时也可以通过调整 max_new_tokens 来控制生成文本的长度，从而减少推理时间。</p><p>对于 CPU 用户，虽然速度较慢，但依然可以通过多线程或批量处理请求来提高效率。你还可以在实际应用中，结合 Web API 接口，将 ChatGLM-6B 模型封装为服务，以便前端调用。</p><h3>在前端集成 ChatGLM-6B 模型</h3><p>前端工程师在 AI 项目中的角色越来越重要，尤其是在实现与用户交互的场景中。要在前端集成 AI 模型，最常见的方式是通过 API 与后端模型进行通信。我们这就来看看如何通过 API 在前端与 ChatGLM-6B 进行交互。</p><h3>后端 API 的设计</h3><p>通常，ChatGLM-6B 会部署在后端服务器上，通过 HTTP 请求与前端进行通信。在开源的ChatGLM-6B的源码中已经集成了进行接口请求的服务，代码如下：</p><pre><code class=\"language-javascript\">from fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom transformers import AutoTokenizer, AutoModel\nimport uvicorn, json, datetime\nimport torch\n\nDEVICE = \"cuda\"\nDEVICE_ID = \"0\"\nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n\ndef torch_gc():\n&nbsp; &nbsp; if torch.cuda.is_available():\n&nbsp; &nbsp; &nbsp; &nbsp; with torch.cuda.device(CUDA_DEVICE):\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; torch.cuda.empty_cache()\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; torch.cuda.ipc_collect()\n\napp = FastAPI()\n\n# 配置 CORS 中间件\napp.add_middleware(\n&nbsp; &nbsp; CORSMiddleware,\n&nbsp; &nbsp; allow_origins=[\"*\"], &nbsp;# 允许所有源进行访问，生产环境中应指定允许的域\n&nbsp; &nbsp; allow_credentials=True,\n&nbsp; &nbsp; allow_methods=[\"*\"], &nbsp;# 允许所有 HTTP 方法\n&nbsp; &nbsp; allow_headers=[\"*\"], &nbsp;# 允许所有请求头\n)\n\n@app.post(\"/\")\nasync def create_item(request: Request):\n&nbsp; &nbsp; global model, tokenizer\n&nbsp; &nbsp; json_post_raw = await request.json()\n&nbsp; &nbsp; json_post = json.dumps(json_post_raw)\n&nbsp; &nbsp; json_post_list = json.loads(json_post)\n&nbsp; &nbsp; prompt = json_post_list.get('prompt')\n&nbsp; &nbsp; history = json_post_list.get('history')\n&nbsp; &nbsp; max_length = json_post_list.get('max_length')\n&nbsp; &nbsp; top_p = json_post_list.get('top_p')\n&nbsp; &nbsp; temperature = json_post_list.get('temperature')\n&nbsp; &nbsp; response, history = model.chat(tokenizer,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;prompt,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;history=history,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;max_length=max_length if max_length else 2048,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;top_p=top_p if top_p else 0.7,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;temperature=temperature if temperature else 0.95)\n&nbsp; &nbsp; now = datetime.datetime.now()\n&nbsp; &nbsp; time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n&nbsp; &nbsp; answer = {\n&nbsp; &nbsp; &nbsp; &nbsp; \"response\": response,\n&nbsp; &nbsp; &nbsp; &nbsp; \"history\": history,\n&nbsp; &nbsp; &nbsp; &nbsp; \"status\": 200,\n&nbsp; &nbsp; &nbsp; &nbsp; \"time\": time\n&nbsp; &nbsp; }\n&nbsp; &nbsp; log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n&nbsp; &nbsp; print(log)\n&nbsp; &nbsp; torch_gc()\n&nbsp; &nbsp; return answer\n\nif __name__ == '__main__':\n&nbsp; &nbsp; tokenizer = AutoTokenizer.from_pretrained(\"models\", trust_remote_code=True)\n&nbsp; &nbsp; model = AutoModel.from_pretrained(\"models\", trust_remote_code=True).cuda()\n&nbsp; &nbsp; model.eval()\n&nbsp; &nbsp; uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n</code></pre><p>这段代码实现了一个基于 FastAPI 的 API 服务，使用预训练的 Transformer 模型处理自然语言生成任务。它允许客户端发送包含&nbsp;prompt&nbsp;和其他参数的 POST 请求，生成相应的文本响应，并返回给客户端。</p><p>代码还配置了 CORS 中间件以允许跨域请求，并在 GPU 上运行 PyTorch 模型以加速计算，同时将服务运行在了本机的8000端口上。</p><h3>前端调用 API</h3><p>在前端部分，我们可以编写一个前端页面来完成与API的交互，这里实现了一个网页端的聊天窗口来实现与模型的对话，部分核心代码如下所示。完整代码放在了<a href=\"https://github.com/IrvingBB/geektime_AIFE/blob/main/Chapter_5/lesson_20/test.html\">代码库</a>中，如有需要请下载。</p><pre><code class=\"language-xml\">&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n\n&lt;head&gt;\n&nbsp; &nbsp; &lt;meta charset=\"UTF-8\"&gt;\n&nbsp; &nbsp; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n&nbsp; &nbsp; &lt;title&gt;ChatGLM2 Personal Assistant&lt;/title&gt;\n&nbsp; &nbsp; &lt;style&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; body {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; font-family: Arial, sans-serif;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; background-color: #e5e5e5;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; margin: 0;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; padding: 20px;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; display: flex;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; justify-content: center;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; align-items: center;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; height: 100vh;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; overflow: hidden;\n&nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; #chat-container {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; width: 70%;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; height: 80%;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; display: flex;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; flex-direction: column;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; background-color: #ffffff;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; border-radius: 8px;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; overflow: hidden;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; position: relative;\n&nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n&nbsp; &nbsp; &lt;div id=\"chat-container\"&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;h2 style=\"text-align: center;\"&gt;ChatGLM2 Personal Assistant&lt;/h2&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;div id=\"messages\"&gt;&lt;/div&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;div id=\"input-container\"&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;input type=\"text\" id=\"prompt\" placeholder=\"Type your message here...\" /&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;button id=\"send\"&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;!-- Airplane icon --&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"currentColor\"&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;path\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; d=\"M21.992 2.22a.75.75 0 0 0-.755-.122L2.993 9.978a.75.75 0 0 0-.045 1.388l7.468 2.9 2.9 7.468a.75.75 0 0 0 1.387-.045l7.88-18.244a.75.75 0 0 0-.11-.765Zm-5.583 2.905-8.835 8.835 5.608-2.178 3.227-6.657ZM12.04 17.96l-1.883-4.85 8.835-8.835-6.657 3.227-2.178 5.608-4.85-1.883 15.733-6.113-8.835 8.835 1.883 4.85Z\" /&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/svg&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/button&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/div&gt;\n&nbsp; &nbsp; &lt;/div&gt;\n\n&nbsp; &nbsp; &lt;script&gt;\n&nbsp; &nbsp; &nbsp; &nbsp; // List of random avatar images\n&nbsp; &nbsp; &nbsp; &nbsp; const avatarUrls = [\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'https://randomuser.me/api/portraits/lego/1.jpg',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'https://randomuser.me/api/portraits/lego/2.jpg'\n&nbsp; &nbsp; &nbsp; &nbsp; ];\n\n&nbsp; &nbsp; &nbsp; &nbsp; // Randomly assign avatars to the user and bot\n&nbsp; &nbsp; &nbsp; &nbsp; const userAvatar = avatarUrls[Math.floor(Math.random() * avatarUrls.length)];\n&nbsp; &nbsp; &nbsp; &nbsp; const botAvatar = avatarUrls[Math.floor(Math.random() * avatarUrls.length)];\n\n&nbsp; &nbsp; &nbsp; &nbsp; function formatMessage(text) {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return text.replace(/\\n/g, '&lt;br&gt;'); // Replace newline characters with &lt;br&gt; tags\n&nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; function addMessage(text, className, isUser) {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const messageContainer = document.getElementById('messages');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const message = document.createElement('div');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; message.className = `message ${className}`;\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const avatar = document.createElement('div');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; avatar.className = 'avatar';\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; avatar.style.backgroundImage = `url(${isUser ? userAvatar : botAvatar})`;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; message.appendChild(avatar);\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const bubble = document.createElement('div');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bubble.className = `bubble ${className}`;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bubble.innerHTML = formatMessage(text); // Use innerHTML to render HTML\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; message.appendChild(bubble);\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (isUser) {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; message.insertBefore(bubble, avatar);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; messageContainer.appendChild(message);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; messageContainer.scrollTop = messageContainer.scrollHeight;\n&nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; async function sendMessage() {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const promptElement = document.getElementById('prompt');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const prompt = promptElement.value;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (!prompt) return;\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; addMessage(prompt, 'user', true);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; promptElement.value = '';\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const responseMessage = document.createElement('div');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; responseMessage.className = 'message bot';\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const avatar = document.createElement('div');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; avatar.className = 'avatar';\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; avatar.style.backgroundImage = `url(${botAvatar})`;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; responseMessage.appendChild(avatar);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const bubble = document.createElement('div');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bubble.className = 'bubble bot';\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bubble.innerHTML = '&lt;span class=\"loading\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;';\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; responseMessage.appendChild(bubble);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; document.getElementById('messages').appendChild(responseMessage);\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const response = await fetch('http://localhost:8000/', {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; method: 'POST',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; headers: {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'Content-Type': 'application/json',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; body: JSON.stringify({\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; prompt: prompt,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; history: [],\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; max_length: 2048,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; top_p: 0.7,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; temperature: 0.95,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }),\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const data = await response.json();\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bubble.innerHTML = formatMessage(data.response); // Use innerHTML to render HTML\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (error) {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bubble.innerHTML = `Error: ${error.message}`; // Use innerHTML for error messages\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const messageContainer = document.getElementById('messages');\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; messageContainer.scrollTop = messageContainer.scrollHeight;\n&nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; document.getElementById('send').addEventListener('click', sendMessage);\n\n&nbsp; &nbsp; &nbsp; &nbsp; // Support Enter key to send message\n&nbsp; &nbsp; &nbsp; &nbsp; document.getElementById('prompt').addEventListener('keydown', (event) =&gt; {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (event.key === 'Enter') {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sendMessage();\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; });\n&nbsp; &nbsp; &lt;/script&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre><p>通过这种方式，前端应用可以轻松与 ChatGLM-6B 模型集成，实现自然语言的交互。集成以后的页面大概是这个样子，可以尝试向模型提问，例如：让它生成一段用于StableDiffusion生成电商活动H5页面设计图的提示词。</p><p><img src=\"https://static001.geekbang.org/resource/image/aa/7f/aa220b8ea8eb67c159aae204f79a677f.jpg?wh=3000x1742\" alt=\"\"></p><h2>总结</h2><p>那么，接下来我们来一起做个总结吧。</p><p>在这节课中，我们一起了解了chatGLM-6B文本大模型，chatGLM-6B大模型凭借平衡的参数量，很方便我们个人在本地部署和使用，同时它的推理能力和效果也不错。</p><p>随后，我们在本地使用conda完成了运行chatGLM-6B的环境搭建。在这一步中，除了需要下载chatGLM-6B的源代码，还需要下载ChatGLM-6B 模型。</p><p>然后我们学习了如何在前端集成这个模型，ChatGLM-6B的源码中已经集成了接口请求的服务。通过分析相关代码，我们知道了API提供了一个基于 FastAPI 的 API 服务，它允许客户端发送包含&nbsp;prompt&nbsp;和其他参数的 POST 请求。最后，我们还实现了一个前端聊天页面和大模型做语言交互，它调用了chatGLM-6B的接口。</p><p>推荐你课后按照今天的讲解，自己动手练习一下，这样你就能轻松拥有一个自己专属的本地文本大模型助手了。</p><h2>课后思考</h2><p>这节课我们学习部署使用了chatGLM-6B模型，我相信你对个人本地化部署大模型有了一些新的思路，那么，除了chatGLM-6B，还有哪些文本大模型可以在PC上进行本地部署和使用呢？</p><p>欢迎你在留言区和我交流互动，如果这节课对你有启发，也推荐分享给身边更多朋友。</p>","neighbors":{"left":{"article_title":"20｜精细化运营：数据的“横向”与“纵向”以及人群运营模对比","id":826639},"right":{"article_title":"22｜图像生成大模型：使用Stable Diffusion生成电商活动原型设计稿","id":827666}},"comments":[]}