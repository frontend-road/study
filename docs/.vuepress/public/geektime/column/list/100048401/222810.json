{"id":222810,"title":"08 | 索引构建：搜索引擎如何为万亿级别网站生成索引？","content":"<p>你好，我是陈东。</p><p>对基于内容或者属性的检索场景，我们可以使用倒排索引完成高效的检索。但是，在一些超大规模的数据应用场景中，比如搜索引擎，它会对万亿级别的网站进行索引，生成的倒排索引会非常庞大，根本无法存储在内存中。这种情况下，我们能否像B+树或者LSM树那样，将数据存入磁盘呢？今天，我们就来聊一聊这个问题。</p><h2>如何生成大于内存容量的倒排索引？</h2><p>我们先来回顾一下，对于能够在内存中处理的小规模的文档集合，我们是如何生成基于哈希表的倒排索引的。步骤如下：</p><ol>\n<li>给每个文档编号，作为它们的唯一标识，并且排好序；</li>\n<li>顺序扫描每一个文档，将当前扫描的文档中的所有内容生成&lt;关键字，文档ID，关键字位置&gt;数据对，并将所有的&lt;关键字，文档ID，关键字位置&gt;这样的数据对，都以关键字为key存入倒排表（位置信息如果不需要可以省略）；</li>\n<li>重复第2步，直到处理完所有文档。这样就生成一个基于内存的倒排索引。<br>\n<img src=\"https://static001.geekbang.org/resource/image/2c/0d/2ccc78df6ebbd4d716318d5113fa090d.jpg?wh=1920*679\" alt=\"\"></li>\n</ol><center><span class=\"reference\">内存中生成倒排索引</span></center><p>对于大规模的文档集合，如果我们能将它分割成多个小规模文档集合，是不是就可以在内存中建立倒排索引了呢？这些存储在内存中的小规模文档的倒排索引，最终又是怎样变成一个完整的大规模的倒排索引存储在磁盘中的呢？这两个问题，你可以先思考一下，然后我们一起来看工业界是怎么做的。</p><!-- [[[read_end]]] --><p>首先，搜索引擎这种工业级的倒排索引表的实现，会比我们之前学习过的更复杂一些。比如说，如果文档中出现了“极客时间”四个字，那除了这四个字本身可能被作为关键词加入词典以外，“极客”和“时间”还有“极客时间”这三个词也可能会被加入词典。因此，完整的词典中词的数量会非常大，可能会达到几百万甚至是几千万的级别。并且，每个词因为长度不一样，所占据的存储空间也会不同。</p><p>所以，为了方便后续的处理，我们不仅会为词典中的每个词编号，还会把每个词对应的字符串存储在词典中。此外，在posting list中，除了记录文档ID，我们还会记录该词在该文档中出现的每个位置、出现次数等信息。因此，posting list中的每一个节点都是一个复杂的结构体，每个结构体以文档ID为唯一标识。完整的倒排索引表结构如下图所示：<br>\n<img src=\"https://static001.geekbang.org/resource/image/c6/6e/c6039f816ba83e0845a87129b128106e.jpg?wh=1920*743\" alt=\"\"></p><center><span class=\"reference\">倒排索引（哈希表实现）</span></center><p>那么，我们怎样才能生成这样一个工业级的倒排索引呢？</p><p>首先，我们可以将大规模文档均匀划分为多个小的文档集合，并按照之前的方法，为每个小的文档集合在内存中生成倒排索引。</p><p>接下来，我们需要将内存中的倒排索引存入磁盘，生成一个临时倒排文件。我们先将内存中的文档列表按照关键词的字符串大小进行排序，然后从小到大，将关键词以及对应的文档列表作为一条记录写入临时倒排文件。这样一来，临时文件中的每条记录就都是有序的了。</p><p>而且，在临时文件中，我们并不需要存储关键词的编号。原因在于每个临时文件的编号都是局部的，并不是全局唯一的，不能作为最终的唯一编号，所以无需保存。<br>\n<img src=\"https://static001.geekbang.org/resource/image/83/06/833a6a1aa057ff3c91bf24a14deb1d06.jpg?wh=1920*681\" alt=\"\"></p><center><span class=\"reference\">生成磁盘中的临时文件</span></center><p>我们依次处理每一批小规模的文档集合，为每一批小规模文档集合生成一份对应的临时文件。等文档全部处理完以后，我们就得到了磁盘上的多个临时文件。</p><p>那磁盘上的多个临时文件该如何合并呢？这又要用到我们熟悉的多路归并技术了。每个临时文件里的每一条记录都是根据关键词有序排列的，因此我们在做多路归并的时候，需要先将所有临时文件当前记录的关键词取出。如果关键词相同的，我们就可以将对应的posting list读出，并且合并了。</p><p>如果posting list可以完全读入内存，那我们就可以直接在内存中完成合并，然后把合并结果作为一条完整的记录写入最终的倒排文件中；如果posting list过大无法装入内存，但posting list里面的元素本身又是有序的，我们也可以将posting list从前往后分段读入内存进行处理，直到处理完所有分段。这样我们就完成了一条完整记录的归并。</p><p>每完成一条完整记录的归并，我们就可以为这一条记录的关键词赋上一个编号，这样每个关键词就有了全局唯一的编号。重复这个过程，直到多个临时文件归并结束，这样我们就可以得到最终完整的倒排文件。<br>\n<img src=\"https://static001.geekbang.org/resource/image/00/f1/00f9769908311fc598a3abc49fb71bf1.jpg?wh=1885*944\" alt=\"\"></p><center><span class=\"reference\">多个临时文件归并生成完整的倒排文件</span></center><p>这种将大任务分解为多个小任务，最终根据key来归并的思路，其实和分布式计算Map Reduce的思路是十分相似的。因此，这种将大规模文档拆分成多个小规模文档集合，再生成倒排文件的方案，可以非常方便地迁移到Map Reduce的框架上，在多台机器上同时运行，大幅度提升倒排文件的生成效率。那如果你想了解更多的内容，你可以看看Google在2004年发表的经典的map reduce论文，论文里面就说了使用map reduce来构建倒排索引是当时最成功的一个应用。</p><h2>如何使用磁盘上的倒排文件进行检索？</h2><p>那对于这样一个大规模的倒排文件，我们在检索的时候是怎么使用的呢？其实，使用的时候有一条核心原则，那就是<strong>内存的检索效率比磁盘高许多，因此，能加载到内存中的数据，我们要尽可能加载到内存中</strong>。</p><p>我们知道，一个倒排索引由两部分构成，一部分是key集合的词典，另一部分是key对应的文档列表。在许多应用中，词典这一部分数据量不会很大，可以在内存中加载。因此，我们完全可以将倒排文件中的所有key读出，在内存中使用哈希表建立词典。<br>\n<img src=\"https://static001.geekbang.org/resource/image/94/75/94c7d76248febf1dda83b03b20493d75.jpg?wh=1920*684\" alt=\"\"></p><center><span class=\"reference\">词典加载在内存中，文档列表存在磁盘</span></center><p>那么，当有查询发生时，通过检索内存中的哈希表，我们就能找到对应的key，然后将磁盘中key对应的postling list读到内存中进行处理了。</p><p>说到这里，你可能会有疑问，如果词典本身也很大，只能存储在磁盘，无法加载到内存中该怎么办呢？其实，你可以试着将词典看作一个有序的key的序列，那这个场景是不是就变得很熟悉了？是的，我们完全可以用B+树来完成词典的检索。</p><p>这样一来，我们就可以把检索过程总结成两个步骤。第一步，我们使用B+树或类似的技术，查询到对应的词典中的关键字。第二步，我们将这个关键字对应的posting list读出，在内存中进行处理。<br>\n<img src=\"https://static001.geekbang.org/resource/image/b3/ad/b38d7575d90ac7b56e1c3c828bd5cfad.jpg?wh=1920*651\" alt=\"\"></p><center><span class=\"reference\">词典文件+倒排文件</span></center><p>到这里，检索过程我们就说完了。不过，还有一种情况你需要考虑，那就是如果posting list非常长，它是很有可能无法加载到内存中进行处理的。比如说，在搜索引擎中，一些热门的关键词可能会出现在上亿个页面中，这些热门关键词对应的posting list就会非常大。那这样的情况下，我们该怎么办呢？</p><p>其实，这个问题在本质上和词典无法加载到内存中是一样的。而且，posting list中的数据也是有序的。因此，我们完全可以对长度过大的posting list也进行类似B+树的索引，只读取有用的数据块到内存中，从而降低磁盘访问次数。包括在Lucene中，也是使用类似的思想，用分层跳表来实现posting list，从而能将posting list分层加载到内存中。而对于长度不大的posting list，我们仍然可以直接加载到内存中。</p><p>此外，如果内存空间足够大，我们还能使用缓存技术，比如LRU缓存，它会将频繁使用的posting list长期保存在内存中。这样一来，当需要频繁使用该posting list的时候，我们可以直接从内存中获取，而不需要重复读取磁盘，也就减少了磁盘IO，从而提升了系统的检索效率。</p><p>总之，对于大规模倒排索引文件的使用，本质上还是我们之前学过的检索技术之间的组合应用。因为倒排文件分为词典和文档列表两部分，所以，检索过程其实就是分别对词典和文档列表的访问过程。因此，只要你知道如何对磁盘上的词典和文档列表进行索引和检索，你就能很好地掌握大规模倒排文件的检索过程。</p><h2>重点回顾</h2><p>今天，我们学习了使用多文件归并的方式对万亿级别的网页生成倒排索引，还学习了针对这样大规模倒排索引文件的检索，可以通过查询词典和查询文档列表这两个阶段来实现。</p><p>除此之外，我们接触了两个很基础但也很重要的设计思想。</p><p>一个是尽可能地将数据加载到内存中，因为内存的检索效率大大高于磁盘。那为了将数据更多地加载到内存中，索引压缩是一个重要的研究方向，目前有很多成熟的技术可以实现对词典和对文档列表的压缩。比如说在Lucene中，就使用了类似于前缀树的技术FST，来对词典进行前后缀的压缩，使得词典可以加载到内存中。</p><p>另一个是将大数据集合拆成多个小数据集合来处理。这其实就是分布式系统的核心思想。在大规模系统中，使用分布式技术进行加速是很重要的一个方向。不过，今天我们只是学习了利用分布式的思想来构建索引，在后面的课程中，我们还会进一步地学习，如何利用分布式技术优化检索效率。</p><h2>课堂讨论</h2><p>词典如果能加载在内存中，就会大幅提升检索效率。在哈希表过大无法存入内存的情况下，我们是否还有可能使用其他占用内存空间更小的数据结构，来将词典完全加载在内存中？有序数组和二叉树是否可行？为什么？</p><p>欢迎在留言区畅所欲言，说出你的思考过程和最终答案。如果有收获，也欢迎把这篇文章分享给你的朋友。</p>","neighbors":{"left":{"article_title":"07 | NoSQL检索：为什么日志系统主要用LSM树而非B+树？","id":222768},"right":{"article_title":"09 | 索引更新：刚发布的文章就能被搜到，这是怎么做到的？","id":222807}},"comments":[{"had_liked":false,"id":205896,"user_name":"无形","can_delete":false,"product_type":"c1","uid":1016889,"ip_address":"","ucode":"B740E2A68A17A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","comment_is_top":false,"comment_ctime":1586752179,"is_pvip":true,"replies":[{"id":76929,"content":"其实你想的已经很接近了。\n首先，词典的查询，是以字符串为key的(因为应用入口并不知道每个字符串对应的ID是什么，它只能以字符串为key来查询，看看这个key是否存在)。\n那如果是将字符串按照字典序在数组中排序好，并且是紧凑存储的(存string|length)，那么就可以和你说的一样，使用遍历的方式查找。\n但是遍历的效率不高，因此我们需要加上一个索引数组来进行二分查找。\n索引数组很简单，就一个元素，存每个词项在字符串数组中的偏移量。比如[0，5，18]这样。\n二分查找时，从数组中间开始，读出偏移量，然后从str数组中取出这个词项，和查询的词对比，看看是否相等。如果不等，那么就继续二分查找，往左或往右，取出下一个字符串比较。\n因此，我们使用两个数组，就能实现所有数据的紧凑存储。从而提升了内存的使用率。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586754633,"ip_address":"","comment_id":205896,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"评论里提到用数组来存储字典，不太清楚这个字典的key和value是什么，这个涉及怎么存储，如果是ID到key，我想到用连续空间来存储，按这种结构id|length|value，先对ID排好序，length是key的长度，value是key的值，这样存储紧凑，没有浪费空间，这样查找key就可以根据ID找key，不匹配可以跳过length的长度，提高效率，如果对这片连续空间创建索引，用数组实现，数组里存储的是ID|偏移量，偏移量是相对连续空间地址的距离，可以实现二分查找；如果是key到ID的映射，类似上面的结构，按照length、key字典序排序好……有点复杂还没完全想好","like_count":13},{"had_liked":false,"id":205764,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1586736785,"is_pvip":false,"replies":[{"id":76912,"content":"你的思考过程非常好！这也是我这次课后题出这个问题的初衷，希望大家能从具体实现的角度出发，去推演系统的实现方案和后续演化。\n我来根据你的思路补充一些细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586745909,"ip_address":"","comment_id":205764,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"存储效率优化想象中就两条路，第一是压缩，像老师说到的fst，对关键字集合的压缩。第二就是除了要存的数据，尽量别存些有的没的，比如我就用连续内存空间存词典中的每一项，是不是最省空间的，是但是变长怎么找，那再加个数组存词典中每一项的地址(\n当然注意有序，当然稀疏索引也不是不能接受)。那你更新代价很高呀，那就把上述两个结构分成块存。这查找效率又不行了，那我多加几层索引，树就来啦，得出结论tradeoff真可怕。。。。","like_count":11,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491578,"discussion_content":"你的思考过程非常好！这也是我这次课后题出这个问题的初衷，希望大家能从具体实现的角度出发，去推演系统的实现方案和后续演化。\n我来根据你的思路补充一些细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586745909,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1101234,"avatar":"https://static001.geekbang.org/account/avatar/00/10/cd/b2/807137b9.jpg","nickname":"北方易初","note":"","ucode":"C57FDBD37F43E6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384352,"discussion_content":"「题出这个问题的初衷」这里有错别字。哈哈","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1626513379,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211632,"user_name":"aoe","can_delete":false,"product_type":"c1","uid":1121758,"ip_address":"","ucode":"1C6201EDB4E954","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","comment_is_top":false,"comment_ctime":1587991286,"is_pvip":false,"replies":[{"id":78676,"content":"和哈希表这种o(1)级别的检索技术相比，前缀树的检索效率不算高，因为它需要逐个字符比较，但是它有自己的特点，可以用在不同的场合中。\n这篇文章中的用法，是利用了前缀树的压缩特性，将它当做压缩算法。\n此外，在没有分词的场景下，前缀树其实还可以完成前缀匹配的检索功能。\n还有在第14讲中，我也介绍了前缀树的另一种用法，你可以去看看。\n\n至于你说的布隆过滤器的方案，这的确是压缩方案，不过在第四讲中，我们也说了它适合用来检索一个对象“是否存在”，而不适合检索具体的值是什么。因此在这一讲的场景中，布隆过滤器无法发挥作用。毕竟我们不仅仅要判断关键词是否存在，而是要读出关键词对应的posting list。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587998428,"ip_address":"","comment_id":211632,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"今天才知道“前缀树”是一种压缩算法，原来只知道有这样一个数据结构……有兴趣小伙伴可以自己实现一下https:&#47;&#47;leetcode-cn.com&#47;problems&#47;implement-trie-prefix-tree&#47;\n\n占用内存空间更小的数据结构：可以采用类似“布隆过滤器”的设计，创建一个bit型的数组，1GB内存大约可以存放85亿的key。\n\n实现思路：\n1. 创建一个bit数组\n2. 需要搜索的关键字通过Hash算法映射到数组的索引位置\n 2.1. 索引位置 已使用值 = 1，未使用值 = 0 （缺点：根据“鸽巢原理”未能解决Hash冲突）\n 2.2. Hash算法简单实现（使用余数定理）：（搜索关键字 转 数字）% 数组长度（约85亿）\n\n1GB存85亿的key计算推导：\n\nbit: 计算机中的最小存储单元 \n1byte包含8bits  \n1KB=1024Byte    KB是千字节\n1MB=1024KB      MB是兆  \n1GB=1024MB      GB是千兆 \n1TB=1024GB      TB是千千兆\n\n1GB = 1024MB \n    = 1024 * 1024KB \n    = 1024 * 1024 * 1024Byte\n    = 1024 * 1024 * 1024 * 8bit\n    = 8,589,934,592\n    ≈ 85亿\n    \n也可以使用工具直接转换：https:&#47;&#47;www.bejson.com&#47;convert&#47;filesize&#47;","like_count":5,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493319,"discussion_content":"和哈希表这种o(1)级别的检索技术相比，前缀树的检索效率不算高，因为它需要逐个字符比较，但是它有自己的特点，可以用在不同的场合中。\n这篇文章中的用法，是利用了前缀树的压缩特性，将它当做压缩算法。\n此外，在没有分词的场景下，前缀树其实还可以完成前缀匹配的检索功能。\n还有在第14讲中，我也介绍了前缀树的另一种用法，你可以去看看。\n\n至于你说的布隆过滤器的方案，这的确是压缩方案，不过在第四讲中，我们也说了它适合用来检索一个对象“是否存在”，而不适合检索具体的值是什么。因此在这一讲的场景中，布隆过滤器无法发挥作用。毕竟我们不仅仅要判断关键词是否存在，而是要读出关键词对应的posting list。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587998428,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1219172,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9a/64/ad837224.jpg","nickname":"Christmas","note":"","ucode":"F48F6BE4A7595B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":354800,"discussion_content":"这里的布隆过滤器可以代替数组的二分查找探测方式，至于效率是否比二分查找高就不一定了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615343611,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1219172,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9a/64/ad837224.jpg","nickname":"Christmas","note":"","ucode":"F48F6BE4A7595B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":354828,"discussion_content":"布隆过滤器时间复杂度O（1），二分法时间复杂度O（logN）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615346818,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":354800,"ip_address":"","group_id":0},"score":354828,"extra":""}]}]},{"had_liked":false,"id":206287,"user_name":"天草二十六","can_delete":false,"product_type":"c1","uid":1360712,"ip_address":"","ucode":"3165EE3007527B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c3/48/3a739da6.jpg","comment_is_top":false,"comment_ctime":1586834858,"is_pvip":false,"replies":[{"id":77042,"content":"看得出来你有对每一篇的知识点进行比较和组织，这一点非常好！\n“铺垫”这个词说到了我写这个专栏的一个核心思路:我希望能从场景出发，一步一步升级知识体系，让你能明白“为什么”，而不仅仅是“怎么做”。\n尽管我没有专门去写es具体是怎么建立倒排索引的，但是相信你只要知道了“为什么”，那么再去看es的实现就会轻松许多，甚至你也可以根据自己的需求，写一个自己的es出来😀","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586839172,"ip_address":"","comment_id":206287,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"es倒排索引铺垫到现在差不多讲完了，真棒的梳理思路","like_count":5,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493319,"discussion_content":"和哈希表这种o(1)级别的检索技术相比，前缀树的检索效率不算高，因为它需要逐个字符比较，但是它有自己的特点，可以用在不同的场合中。\n这篇文章中的用法，是利用了前缀树的压缩特性，将它当做压缩算法。\n此外，在没有分词的场景下，前缀树其实还可以完成前缀匹配的检索功能。\n还有在第14讲中，我也介绍了前缀树的另一种用法，你可以去看看。\n\n至于你说的布隆过滤器的方案，这的确是压缩方案，不过在第四讲中，我们也说了它适合用来检索一个对象“是否存在”，而不适合检索具体的值是什么。因此在这一讲的场景中，布隆过滤器无法发挥作用。毕竟我们不仅仅要判断关键词是否存在，而是要读出关键词对应的posting list。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587998428,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1219172,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9a/64/ad837224.jpg","nickname":"Christmas","note":"","ucode":"F48F6BE4A7595B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":354800,"discussion_content":"这里的布隆过滤器可以代替数组的二分查找探测方式，至于效率是否比二分查找高就不一定了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615343611,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1219172,"avatar":"https://static001.geekbang.org/account/avatar/00/12/9a/64/ad837224.jpg","nickname":"Christmas","note":"","ucode":"F48F6BE4A7595B","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":354828,"discussion_content":"布隆过滤器时间复杂度O（1），二分法时间复杂度O（logN）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1615346818,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":354800,"ip_address":"","group_id":0},"score":354828,"extra":""}]}]},{"had_liked":false,"id":213160,"user_name":"KL3","can_delete":false,"product_type":"c1","uid":1537889,"ip_address":"","ucode":"866F01619D295F","user_header":"https://static001.geekbang.org/account/avatar/00/17/77/61/adf1c799.jpg","comment_is_top":false,"comment_ctime":1588319026,"is_pvip":false,"replies":[{"id":79107,"content":"这个细节我在文章中没有详细描述，这里可以补充一下。\n对于字典的检索，我们想查询的对象其实是关键词，因此，哈希表的key，应该是关键词的哈希值，value则是字符串。当然，由于我们还需要得到pos，因此，value其实应该是一个结构体，即保存了字符串，又保存了pos。具体结构看我下面的表示:\nkey: 字符串的哈希值\nvalue:[字符串，pos]\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588335514,"ip_address":"","comment_id":213160,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"请问老师，对倒排文件的检索，为了提高效率将词典加载到内存，并用哈希表组织，那哈希表的key是String，value是pos（文档列表在硬盘中的位置）吗？","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491742,"discussion_content":"看得出来你有对每一篇的知识点进行比较和组织，这一点非常好！\n“铺垫”这个词说到了我写这个专栏的一个核心思路:我希望能从场景出发，一步一步升级知识体系，让你能明白“为什么”，而不仅仅是“怎么做”。\n尽管我没有专门去写es具体是怎么建立倒排索引的，但是相信你只要知道了“为什么”，那么再去看es的实现就会轻松许多，甚至你也可以根据自己的需求，写一个自己的es出来😀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586839172,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206129,"user_name":"qinsi","can_delete":false,"product_type":"c1","uid":1667175,"ip_address":"","ucode":"090D9C4068FF12","user_header":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","comment_is_top":false,"comment_ctime":1586790612,"is_pvip":true,"replies":[{"id":76974,"content":"这里可能是我没有强调清楚。word指的是关键词的ID，也就是关键词的唯一标识。而string指的是关键词的具体内容。比如说一个关键词是“重启系统”，它的string有四个字符，但是Word ID只需要一个int32就够了。\n这样做的好处，是用统一长度的ID在后续处理中(比如说去查posting list时，以及后面进行相关性打分时)来代替关键词本身，这样无论是存储代价还是处理代价都会更小。\n而string才是关键词本身字符串，因此在“字典查询”这个过程中，string才是主体。\n\n解释清楚了这些以后，再来看你的第二个问题和第三个问题，应该就清楚了。\n由于Word ID需要全局唯一编号，因此临时文件不需要存；而如果不存入string，那么“词典查询”就无法执行，因为用户输入的原始查询就是字符串，而不是Word ID。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586822906,"ip_address":"","comment_id":206129,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"不是很明白存储关键词对应的字符串的作用。\n\n1. 配图里的word是指关键字，string是指关键字所在的字符串吗？string是否可以理解为word所处的context？能否有些具体的例子呢？\n2. 存入磁盘上的临时文件时只存了string而没有存word？那么在合并文件时word的信息是从哪来的呢？\n3. 如果不存string，而只是根据word来做排序与合并的话，会有什么不好的后果吗？\n","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491685,"discussion_content":"这里可能是我没有强调清楚。word指的是关键词的ID，也就是关键词的唯一标识。而string指的是关键词的具体内容。比如说一个关键词是“重启系统”，它的string有四个字符，但是Word ID只需要一个int32就够了。\n这样做的好处，是用统一长度的ID在后续处理中(比如说去查posting list时，以及后面进行相关性打分时)来代替关键词本身，这样无论是存储代价还是处理代价都会更小。\n而string才是关键词本身字符串，因此在“字典查询”这个过程中，string才是主体。\n\n解释清楚了这些以后，再来看你的第二个问题和第三个问题，应该就清楚了。\n由于Word ID需要全局唯一编号，因此临时文件不需要存；而如果不存入string，那么“词典查询”就无法执行，因为用户输入的原始查询就是字符串，而不是Word ID。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1586822906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":232502,"discussion_content":"是的。这里我联系编辑修改一下吧。也感谢你指出这个容易让读者疑惑的地方","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586871281,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1667175,"avatar":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","nickname":"qinsi","note":"","ucode":"090D9C4068FF12","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":231857,"discussion_content":"这样就说得通了，感谢解答🙏原来一直以来理解都有偏差。因为第5讲里“正排”的键值也是用的word，没有留意用作倒排key的word上已经标注了“唯一标示”...然后文中又有这么一句 “为了方便后续的处理，我们不仅会为词典中的每个词编号，还会把每个词对应的字符串存储在词典中”，就以为string是其他什么东西。其实string就是关键词本身（的字符串表示）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586833239,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":205802,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1586741880,"is_pvip":false,"replies":[{"id":76914,"content":"有一点你说得很好，数据量大的是value，也就是词项字符串，因此，使用数组存储，最大的问题是如何存储这些字符串。\n我来补充一些使用数组存储字符串的细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n针对你说的char数组可能无法申请连续空间的事情，那么我们可以将char数组分段即可。\n\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。这就是用非连续的空间，用树来组织和压缩的方案。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586746181,"ip_address":"","comment_id":205802,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"在无压缩的情况下:\n对于Hash表的存储而言，数据量大的是value，是内容。\n\n数组当然可以直接存储，但是内容太大的情况下，占用的连续内存太大了，可能会导致内存申请失败。\n\n对于二叉树而言，内容的内存占用并没有减少，但是求交集的操作比链表复杂些。","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491586,"discussion_content":"有一点你说得很好，数据量大的是value，也就是词项字符串，因此，使用数组存储，最大的问题是如何存储这些字符串。\n我来补充一些使用数组存储字符串的细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n针对你说的char数组可能无法申请连续空间的事情，那么我们可以将char数组分段即可。\n\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。这就是用非连续的空间，用树来组织和压缩的方案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586746181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":205883,"user_name":"无形","can_delete":false,"product_type":"c1","uid":1016889,"ip_address":"","ucode":"B740E2A68A17A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","comment_is_top":false,"comment_ctime":1586750430,"is_pvip":true,"replies":[{"id":76924,"content":"你的这几个问题都很好。\n首先，倒排索引中的key，是经过筛选的，在索引构建的过程中，会使用分词等技术，将有意义的关键词提取出来，作为key。因此不会过于无意义。\n然后，即使key是中英文混合带符号的，其实都是字符串，使用前缀树依然有效。比如说“重启2020”，“重启系统”，这可以是两个key，可以用前缀树。\n至于你的问题:\n1.如何快速排序，我在11篇和12篇会讲，在课程上线前，你可以自己先想想，然后个课程内容对比一下。\n2.索引更新问题，下一课就马上讲，敬请期待。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586753701,"ip_address":"","comment_id":205883,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"看到评论里有提到前缀树，我有点疑问，如果字典的key只英文字母，如abcd可以用0123来表示，但是key如果还包含各种中英文符号、中文、韩文以及特殊字符等，怎么处理？\n\n我还有两个问题，\n1.对于根据关键词检索出来的文档，假如结果集达到百万千万级，怎么实现快速对结果集的排序？\n2.对于已有的文档已经生成了key及对应的全量的文档列表，现在又有了新的文档，生成新的文档列表，全量的和新的文档列表怎么去合并？什么时候去合并？","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491604,"discussion_content":"你的这几个问题都很好。\n首先，倒排索引中的key，是经过筛选的，在索引构建的过程中，会使用分词等技术，将有意义的关键词提取出来，作为key。因此不会过于无意义。\n然后，即使key是中英文混合带符号的，其实都是字符串，使用前缀树依然有效。比如说“重启2020”，“重启系统”，这可以是两个key，可以用前缀树。\n至于你的问题:\n1.如何快速排序，我在11篇和12篇会讲，在课程上线前，你可以自己先想想，然后个课程内容对比一下。\n2.索引更新问题，下一课就马上讲，敬请期待。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586753701,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250060,"discussion_content":"想的好远","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587988507,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":4,"child_discussions":[{"author":{"id":1016889,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","nickname":"无形","note":"","ucode":"B740E2A68A17A5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250097,"discussion_content":"提到的两个问题是我实际中遇到的，第一个是写爬虫，爬了百万级数据，分词实现全文检索，实际排序数据量确实达到了百万级，第二个问题是我对历史数据做了分词、构建倒排列表、生成倒排索引等，新的数据不知道怎么处理，每次全量处理耗时太久，头发都掉光了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587991229,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250060,"ip_address":"","group_id":0},"score":250097,"extra":""},{"author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1016889,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","nickname":"无形","note":"","ucode":"B740E2A68A17A5","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":250536,"discussion_content":"原来如此，我还以为你未卜先知","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588010362,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250097,"ip_address":"","group_id":0},"score":250536,"extra":""},{"author":{"id":1016889,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","nickname":"无形","note":"","ucode":"B740E2A68A17A5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250550,"discussion_content":"要是有这能力好了，我还有点小期待呢😄","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588026961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250536,"ip_address":"","group_id":0},"score":250550,"extra":""}]}]},{"had_liked":false,"id":206091,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1586785979,"is_pvip":false,"replies":[{"id":76959,"content":"哈哈，你这样改变以后，使用了二分查找，自然不是哈希表了。\n不过你要注意一点，我们现在先不考虑posting list，而是仅仅考虑“词典太大，能否紧凑地装入内存中”。\n这个时候，哈希表中的key应该是每个词项的hash值，而value是这个词项的字符串，而不是list。你可以再想想怎么做。\n这个课后题的目的，其实是想让大家思考，为了将内存空间充分利用，能有哪些紧凑存储甚至压缩存储的方案。实际上，许多真实的系统中就是这么实现的。\n因此，你思考过了这些问题以后，以后如果看到某些系统的索引实现很折腾，那么不要奇怪，它可能只是想把数据都放入内存中而已。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586789660,"ip_address":"","comment_id":206091,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"首先老师说hash表比较大，我们无法加入到内存中，hash表一般来说key比较小，那么假如我们hash表是按照拉链法来解决冲突，在持久化磁盘中，按照槽位的顺序保存 ，每一行即一个 [ 槽位，key，list(value)]\n，由于不是每个槽位都有值，没有值的槽位key空着。我们可以在内存中，保存key和槽位的对应，以一个和槽位一样大小的数组，这样槽位和大hash表里面的槽位对应，值保存这个槽位的行在文件中的位置，来查询的时候，对要查询的数据计算hash值%槽位，得到的结果去内存中的数组中对应的位去找，找到再到文件找，找不到，就没有；如果key仍然很多，也无法用内存保存起来，可以只保存有值的key，数组中为结构体，里面含有有值的槽位，还有这个槽位在文件中的位置，查找的时候，用二分法查找；如果保留key也保存不下，那就试试用B+树，可以只保留几层，那就不用文件了，非叶子节点保存的是有值的槽位，用二分法很容易搜索，叶子节点为真正的槽位对应的值的list，不过这样的性能要差些，这样一变还是哈希表吗，哈哈","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491667,"discussion_content":"哈哈，你这样改变以后，使用了二分查找，自然不是哈希表了。\n不过你要注意一点，我们现在先不考虑posting list，而是仅仅考虑“词典太大，能否紧凑地装入内存中”。\n这个时候，哈希表中的key应该是每个词项的hash值，而value是这个词项的字符串，而不是list。你可以再想想怎么做。\n这个课后题的目的，其实是想让大家思考，为了将内存空间充分利用，能有哪些紧凑存储甚至压缩存储的方案。实际上，许多真实的系统中就是这么实现的。\n因此，你思考过了这些问题以后，以后如果看到某些系统的索引实现很折腾，那么不要奇怪，它可能只是想把数据都放入内存中而已。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586789660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":372409,"user_name":"ifelse","can_delete":false,"product_type":"c1","uid":2550743,"ip_address":"浙江","ucode":"D0565908C99695","user_header":"https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg","comment_is_top":false,"comment_ctime":1681117113,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":2,"product_id":100048401,"comment_content":"学习了，干货满满","like_count":1},{"had_liked":false,"id":205896,"user_name":"无形","can_delete":false,"product_type":"c1","uid":1016889,"ip_address":"","ucode":"B740E2A68A17A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","comment_is_top":false,"comment_ctime":1586752179,"is_pvip":true,"replies":[{"id":76929,"content":"其实你想的已经很接近了。\n首先，词典的查询，是以字符串为key的(因为应用入口并不知道每个字符串对应的ID是什么，它只能以字符串为key来查询，看看这个key是否存在)。\n那如果是将字符串按照字典序在数组中排序好，并且是紧凑存储的(存string|length)，那么就可以和你说的一样，使用遍历的方式查找。\n但是遍历的效率不高，因此我们需要加上一个索引数组来进行二分查找。\n索引数组很简单，就一个元素，存每个词项在字符串数组中的偏移量。比如[0，5，18]这样。\n二分查找时，从数组中间开始，读出偏移量，然后从str数组中取出这个词项，和查询的词对比，看看是否相等。如果不等，那么就继续二分查找，往左或往右，取出下一个字符串比较。\n因此，我们使用两个数组，就能实现所有数据的紧凑存储。从而提升了内存的使用率。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586754633,"ip_address":"","comment_id":205896,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"评论里提到用数组来存储字典，不太清楚这个字典的key和value是什么，这个涉及怎么存储，如果是ID到key，我想到用连续空间来存储，按这种结构id|length|value，先对ID排好序，length是key的长度，value是key的值，这样存储紧凑，没有浪费空间，这样查找key就可以根据ID找key，不匹配可以跳过length的长度，提高效率，如果对这片连续空间创建索引，用数组实现，数组里存储的是ID|偏移量，偏移量是相对连续空间地址的距离，可以实现二分查找；如果是key到ID的映射，类似上面的结构，按照length、key字典序排序好……有点复杂还没完全想好","like_count":13,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491608,"discussion_content":"其实你想的已经很接近了。\n首先，词典的查询，是以字符串为key的(因为应用入口并不知道每个字符串对应的ID是什么，它只能以字符串为key来查询，看看这个key是否存在)。\n那如果是将字符串按照字典序在数组中排序好，并且是紧凑存储的(存string|length)，那么就可以和你说的一样，使用遍历的方式查找。\n但是遍历的效率不高，因此我们需要加上一个索引数组来进行二分查找。\n索引数组很简单，就一个元素，存每个词项在字符串数组中的偏移量。比如[0，5，18]这样。\n二分查找时，从数组中间开始，读出偏移量，然后从str数组中取出这个词项，和查询的词对比，看看是否相等。如果不等，那么就继续二分查找，往左或往右，取出下一个字符串比较。\n因此，我们使用两个数组，就能实现所有数据的紧凑存储。从而提升了内存的使用率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586754633,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":205764,"user_name":"峰","can_delete":false,"product_type":"c1","uid":1056019,"ip_address":"","ucode":"C53CB64E8E7D19","user_header":"https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg","comment_is_top":false,"comment_ctime":1586736785,"is_pvip":false,"replies":[{"id":76912,"content":"你的思考过程非常好！这也是我这次课后题出这个问题的初衷，希望大家能从具体实现的角度出发，去推演系统的实现方案和后续演化。\n我来根据你的思路补充一些细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586745909,"ip_address":"","comment_id":205764,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"存储效率优化想象中就两条路，第一是压缩，像老师说到的fst，对关键字集合的压缩。第二就是除了要存的数据，尽量别存些有的没的，比如我就用连续内存空间存词典中的每一项，是不是最省空间的，是但是变长怎么找，那再加个数组存词典中每一项的地址(\n当然注意有序，当然稀疏索引也不是不能接受)。那你更新代价很高呀，那就把上述两个结构分成块存。这查找效率又不行了，那我多加几层索引，树就来啦，得出结论tradeoff真可怕。。。。","like_count":11,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491608,"discussion_content":"其实你想的已经很接近了。\n首先，词典的查询，是以字符串为key的(因为应用入口并不知道每个字符串对应的ID是什么，它只能以字符串为key来查询，看看这个key是否存在)。\n那如果是将字符串按照字典序在数组中排序好，并且是紧凑存储的(存string|length)，那么就可以和你说的一样，使用遍历的方式查找。\n但是遍历的效率不高，因此我们需要加上一个索引数组来进行二分查找。\n索引数组很简单，就一个元素，存每个词项在字符串数组中的偏移量。比如[0，5，18]这样。\n二分查找时，从数组中间开始，读出偏移量，然后从str数组中取出这个词项，和查询的词对比，看看是否相等。如果不等，那么就继续二分查找，往左或往右，取出下一个字符串比较。\n因此，我们使用两个数组，就能实现所有数据的紧凑存储。从而提升了内存的使用率。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586754633,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":211632,"user_name":"aoe","can_delete":false,"product_type":"c1","uid":1121758,"ip_address":"","ucode":"1C6201EDB4E954","user_header":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","comment_is_top":false,"comment_ctime":1587991286,"is_pvip":false,"replies":[{"id":78676,"content":"和哈希表这种o(1)级别的检索技术相比，前缀树的检索效率不算高，因为它需要逐个字符比较，但是它有自己的特点，可以用在不同的场合中。\n这篇文章中的用法，是利用了前缀树的压缩特性，将它当做压缩算法。\n此外，在没有分词的场景下，前缀树其实还可以完成前缀匹配的检索功能。\n还有在第14讲中，我也介绍了前缀树的另一种用法，你可以去看看。\n\n至于你说的布隆过滤器的方案，这的确是压缩方案，不过在第四讲中，我们也说了它适合用来检索一个对象“是否存在”，而不适合检索具体的值是什么。因此在这一讲的场景中，布隆过滤器无法发挥作用。毕竟我们不仅仅要判断关键词是否存在，而是要读出关键词对应的posting list。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1587998428,"ip_address":"","comment_id":211632,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"今天才知道“前缀树”是一种压缩算法，原来只知道有这样一个数据结构……有兴趣小伙伴可以自己实现一下https:&#47;&#47;leetcode-cn.com&#47;problems&#47;implement-trie-prefix-tree&#47;\n\n占用内存空间更小的数据结构：可以采用类似“布隆过滤器”的设计，创建一个bit型的数组，1GB内存大约可以存放85亿的key。\n\n实现思路：\n1. 创建一个bit数组\n2. 需要搜索的关键字通过Hash算法映射到数组的索引位置\n 2.1. 索引位置 已使用值 = 1，未使用值 = 0 （缺点：根据“鸽巢原理”未能解决Hash冲突）\n 2.2. Hash算法简单实现（使用余数定理）：（搜索关键字 转 数字）% 数组长度（约85亿）\n\n1GB存85亿的key计算推导：\n\nbit: 计算机中的最小存储单元 \n1byte包含8bits  \n1KB=1024Byte    KB是千字节\n1MB=1024KB      MB是兆  \n1GB=1024MB      GB是千兆 \n1TB=1024GB      TB是千千兆\n\n1GB = 1024MB \n    = 1024 * 1024KB \n    = 1024 * 1024 * 1024Byte\n    = 1024 * 1024 * 1024 * 8bit\n    = 8,589,934,592\n    ≈ 85亿\n    \n也可以使用工具直接转换：https:&#47;&#47;www.bejson.com&#47;convert&#47;filesize&#47;","like_count":5,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491578,"discussion_content":"你的思考过程非常好！这也是我这次课后题出这个问题的初衷，希望大家能从具体实现的角度出发，去推演系统的实现方案和后续演化。\n我来根据你的思路补充一些细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586745909,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1101234,"avatar":"https://static001.geekbang.org/account/avatar/00/10/cd/b2/807137b9.jpg","nickname":"北方易初","note":"","ucode":"C57FDBD37F43E6","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":384352,"discussion_content":"「题出这个问题的初衷」这里有错别字。哈哈","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1626513379,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206287,"user_name":"天草二十六","can_delete":false,"product_type":"c1","uid":1360712,"ip_address":"","ucode":"3165EE3007527B","user_header":"https://static001.geekbang.org/account/avatar/00/14/c3/48/3a739da6.jpg","comment_is_top":false,"comment_ctime":1586834858,"is_pvip":false,"replies":[{"id":77042,"content":"看得出来你有对每一篇的知识点进行比较和组织，这一点非常好！\n“铺垫”这个词说到了我写这个专栏的一个核心思路:我希望能从场景出发，一步一步升级知识体系，让你能明白“为什么”，而不仅仅是“怎么做”。\n尽管我没有专门去写es具体是怎么建立倒排索引的，但是相信你只要知道了“为什么”，那么再去看es的实现就会轻松许多，甚至你也可以根据自己的需求，写一个自己的es出来😀","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586839172,"ip_address":"","comment_id":206287,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"es倒排索引铺垫到现在差不多讲完了，真棒的梳理思路","like_count":5,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491742,"discussion_content":"看得出来你有对每一篇的知识点进行比较和组织，这一点非常好！\n“铺垫”这个词说到了我写这个专栏的一个核心思路:我希望能从场景出发，一步一步升级知识体系，让你能明白“为什么”，而不仅仅是“怎么做”。\n尽管我没有专门去写es具体是怎么建立倒排索引的，但是相信你只要知道了“为什么”，那么再去看es的实现就会轻松许多，甚至你也可以根据自己的需求，写一个自己的es出来😀","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586839172,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":213160,"user_name":"KL3","can_delete":false,"product_type":"c1","uid":1537889,"ip_address":"","ucode":"866F01619D295F","user_header":"https://static001.geekbang.org/account/avatar/00/17/77/61/adf1c799.jpg","comment_is_top":false,"comment_ctime":1588319026,"is_pvip":false,"replies":[{"id":79107,"content":"这个细节我在文章中没有详细描述，这里可以补充一下。\n对于字典的检索，我们想查询的对象其实是关键词，因此，哈希表的key，应该是关键词的哈希值，value则是字符串。当然，由于我们还需要得到pos，因此，value其实应该是一个结构体，即保存了字符串，又保存了pos。具体结构看我下面的表示:\nkey: 字符串的哈希值\nvalue:[字符串，pos]\n","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1588335514,"ip_address":"","comment_id":213160,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"请问老师，对倒排文件的检索，为了提高效率将词典加载到内存，并用哈希表组织，那哈希表的key是String，value是pos（文档列表在硬盘中的位置）吗？","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493695,"discussion_content":"这个细节我在文章中没有详细描述，这里可以补充一下。\n对于字典的检索，我们想查询的对象其实是关键词，因此，哈希表的key，应该是关键词的哈希值，value则是字符串。当然，由于我们还需要得到pos，因此，value其实应该是一个结构体，即保存了字符串，又保存了pos。具体结构看我下面的表示:\nkey: 字符串的哈希值\nvalue:[字符串，pos]\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588335514,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206129,"user_name":"qinsi","can_delete":false,"product_type":"c1","uid":1667175,"ip_address":"","ucode":"090D9C4068FF12","user_header":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","comment_is_top":false,"comment_ctime":1586790612,"is_pvip":true,"replies":[{"id":76974,"content":"这里可能是我没有强调清楚。word指的是关键词的ID，也就是关键词的唯一标识。而string指的是关键词的具体内容。比如说一个关键词是“重启系统”，它的string有四个字符，但是Word ID只需要一个int32就够了。\n这样做的好处，是用统一长度的ID在后续处理中(比如说去查posting list时，以及后面进行相关性打分时)来代替关键词本身，这样无论是存储代价还是处理代价都会更小。\n而string才是关键词本身字符串，因此在“字典查询”这个过程中，string才是主体。\n\n解释清楚了这些以后，再来看你的第二个问题和第三个问题，应该就清楚了。\n由于Word ID需要全局唯一编号，因此临时文件不需要存；而如果不存入string，那么“词典查询”就无法执行，因为用户输入的原始查询就是字符串，而不是Word ID。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586822906,"ip_address":"","comment_id":206129,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"不是很明白存储关键词对应的字符串的作用。\n\n1. 配图里的word是指关键字，string是指关键字所在的字符串吗？string是否可以理解为word所处的context？能否有些具体的例子呢？\n2. 存入磁盘上的临时文件时只存了string而没有存word？那么在合并文件时word的信息是从哪来的呢？\n3. 如果不存string，而只是根据word来做排序与合并的话，会有什么不好的后果吗？\n","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":493695,"discussion_content":"这个细节我在文章中没有详细描述，这里可以补充一下。\n对于字典的检索，我们想查询的对象其实是关键词，因此，哈希表的key，应该是关键词的哈希值，value则是字符串。当然，由于我们还需要得到pos，因此，value其实应该是一个结构体，即保存了字符串，又保存了pos。具体结构看我下面的表示:\nkey: 字符串的哈希值\nvalue:[字符串，pos]\n","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588335514,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":205802,"user_name":"范闲","can_delete":false,"product_type":"c1","uid":1073125,"ip_address":"","ucode":"F21FD7DF6BA53C","user_header":"https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg","comment_is_top":false,"comment_ctime":1586741880,"is_pvip":false,"replies":[{"id":76914,"content":"有一点你说得很好，数据量大的是value，也就是词项字符串，因此，使用数组存储，最大的问题是如何存储这些字符串。\n我来补充一些使用数组存储字符串的细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n针对你说的char数组可能无法申请连续空间的事情，那么我们可以将char数组分段即可。\n\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。这就是用非连续的空间，用树来组织和压缩的方案。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586746181,"ip_address":"","comment_id":205802,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"在无压缩的情况下:\n对于Hash表的存储而言，数据量大的是value，是内容。\n\n数组当然可以直接存储，但是内容太大的情况下，占用的连续内存太大了，可能会导致内存申请失败。\n\n对于二叉树而言，内容的内存占用并没有减少，但是求交集的操作比链表复杂些。","like_count":4,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491685,"discussion_content":"这里可能是我没有强调清楚。word指的是关键词的ID，也就是关键词的唯一标识。而string指的是关键词的具体内容。比如说一个关键词是“重启系统”，它的string有四个字符，但是Word ID只需要一个int32就够了。\n这样做的好处，是用统一长度的ID在后续处理中(比如说去查posting list时，以及后面进行相关性打分时)来代替关键词本身，这样无论是存储代价还是处理代价都会更小。\n而string才是关键词本身字符串，因此在“字典查询”这个过程中，string才是主体。\n\n解释清楚了这些以后，再来看你的第二个问题和第三个问题，应该就清楚了。\n由于Word ID需要全局唯一编号，因此临时文件不需要存；而如果不存入string，那么“词典查询”就无法执行，因为用户输入的原始查询就是字符串，而不是Word ID。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1586822906,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":232502,"discussion_content":"是的。这里我联系编辑修改一下吧。也感谢你指出这个容易让读者疑惑的地方","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586871281,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1667175,"avatar":"https://static001.geekbang.org/account/avatar/00/19/70/67/0c1359c2.jpg","nickname":"qinsi","note":"","ucode":"090D9C4068FF12","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":231857,"discussion_content":"这样就说得通了，感谢解答🙏原来一直以来理解都有偏差。因为第5讲里“正排”的键值也是用的word，没有留意用作倒排key的word上已经标注了“唯一标示”...然后文中又有这么一句 “为了方便后续的处理，我们不仅会为词典中的每个词编号，还会把每个词对应的字符串存储在词典中”，就以为string是其他什么东西。其实string就是关键词本身（的字符串表示）","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586833239,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":205883,"user_name":"无形","can_delete":false,"product_type":"c1","uid":1016889,"ip_address":"","ucode":"B740E2A68A17A5","user_header":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","comment_is_top":false,"comment_ctime":1586750430,"is_pvip":true,"replies":[{"id":76924,"content":"你的这几个问题都很好。\n首先，倒排索引中的key，是经过筛选的，在索引构建的过程中，会使用分词等技术，将有意义的关键词提取出来，作为key。因此不会过于无意义。\n然后，即使key是中英文混合带符号的，其实都是字符串，使用前缀树依然有效。比如说“重启2020”，“重启系统”，这可以是两个key，可以用前缀树。\n至于你的问题:\n1.如何快速排序，我在11篇和12篇会讲，在课程上线前，你可以自己先想想，然后个课程内容对比一下。\n2.索引更新问题，下一课就马上讲，敬请期待。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586753701,"ip_address":"","comment_id":205883,"utype":1}],"discussion_count":6,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"看到评论里有提到前缀树，我有点疑问，如果字典的key只英文字母，如abcd可以用0123来表示，但是key如果还包含各种中英文符号、中文、韩文以及特殊字符等，怎么处理？\n\n我还有两个问题，\n1.对于根据关键词检索出来的文档，假如结果集达到百万千万级，怎么实现快速对结果集的排序？\n2.对于已有的文档已经生成了key及对应的全量的文档列表，现在又有了新的文档，生成新的文档列表，全量的和新的文档列表怎么去合并？什么时候去合并？","like_count":3,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491586,"discussion_content":"有一点你说得很好，数据量大的是value，也就是词项字符串，因此，使用数组存储，最大的问题是如何存储这些字符串。\n我来补充一些使用数组存储字符串的细节:\n1.使用数组存每个词项，这个需要解决每个词项长度不同的问题，一个思路是使用最长的词项作为数组每个元素的大小(比如说每个元素都是20个字节)。这样就可以用数组存储和查找了。\n2.第一种方法空间会浪费，因此，改进方案可以另外开一个char数组，将所有字符串挨个紧凑存入；然后索引数组每个元素都是int 32类型，指向char数组中对应词项的初始位置。这样空间就都是紧凑的了。\n这就是使用数组的方案。\n针对你说的char数组可能无法申请连续空间的事情，那么我们可以将char数组分段即可。\n\n其实如果再深入思考，你会发现char数组中好多字符都是重复的，这时候压缩重复字符的前缀树就出来了。这就是用非连续的空间，用树来组织和压缩的方案。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586746181,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":206091,"user_name":"明翼","can_delete":false,"product_type":"c1","uid":1068361,"ip_address":"","ucode":"E77F86BEB3D5C1","user_header":"https://static001.geekbang.org/account/avatar/00/10/4d/49/28e73b9c.jpg","comment_is_top":false,"comment_ctime":1586785979,"is_pvip":false,"replies":[{"id":76959,"content":"哈哈，你这样改变以后，使用了二分查找，自然不是哈希表了。\n不过你要注意一点，我们现在先不考虑posting list，而是仅仅考虑“词典太大，能否紧凑地装入内存中”。\n这个时候，哈希表中的key应该是每个词项的hash值，而value是这个词项的字符串，而不是list。你可以再想想怎么做。\n这个课后题的目的，其实是想让大家思考，为了将内存空间充分利用，能有哪些紧凑存储甚至压缩存储的方案。实际上，许多真实的系统中就是这么实现的。\n因此，你思考过了这些问题以后，以后如果看到某些系统的索引实现很折腾，那么不要奇怪，它可能只是想把数据都放入内存中而已。","user_name":"作者回复","user_name_real":"陈东","uid":1165703,"ctime":1586789660,"ip_address":"","comment_id":206091,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100048401,"comment_content":"首先老师说hash表比较大，我们无法加入到内存中，hash表一般来说key比较小，那么假如我们hash表是按照拉链法来解决冲突，在持久化磁盘中，按照槽位的顺序保存 ，每一行即一个 [ 槽位，key，list(value)]\n，由于不是每个槽位都有值，没有值的槽位key空着。我们可以在内存中，保存key和槽位的对应，以一个和槽位一样大小的数组，这样槽位和大hash表里面的槽位对应，值保存这个槽位的行在文件中的位置，来查询的时候，对要查询的数据计算hash值%槽位，得到的结果去内存中的数组中对应的位去找，找到再到文件找，找不到，就没有；如果key仍然很多，也无法用内存保存起来，可以只保存有值的key，数组中为结构体，里面含有有值的槽位，还有这个槽位在文件中的位置，查找的时候，用二分法查找；如果保留key也保存不下，那就试试用B+树，可以只保留几层，那就不用文件了，非叶子节点保存的是有值的槽位，用二分法很容易搜索，叶子节点为真正的槽位对应的值的list，不过这样的性能要差些，这样一变还是哈希表吗，哈哈","like_count":0,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491604,"discussion_content":"你的这几个问题都很好。\n首先，倒排索引中的key，是经过筛选的，在索引构建的过程中，会使用分词等技术，将有意义的关键词提取出来，作为key。因此不会过于无意义。\n然后，即使key是中英文混合带符号的，其实都是字符串，使用前缀树依然有效。比如说“重启2020”，“重启系统”，这可以是两个key，可以用前缀树。\n至于你的问题:\n1.如何快速排序，我在11篇和12篇会讲，在课程上线前，你可以自己先想想，然后个课程内容对比一下。\n2.索引更新问题，下一课就马上讲，敬请期待。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586753701,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250060,"discussion_content":"想的好远","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1587988507,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":4,"child_discussions":[{"author":{"id":1016889,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","nickname":"无形","note":"","ucode":"B740E2A68A17A5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250097,"discussion_content":"提到的两个问题是我实际中遇到的，第一个是写爬虫，爬了百万级数据，分词实现全文检索，实际排序数据量确实达到了百万级，第二个问题是我对历史数据做了分词、构建倒排列表、生成倒排索引等，新的数据不知道怎么处理，每次全量处理耗时太久，头发都掉光了","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1587991229,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250060,"ip_address":"","group_id":0},"score":250097,"extra":""},{"author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1016889,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","nickname":"无形","note":"","ucode":"B740E2A68A17A5","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":250536,"discussion_content":"原来如此，我还以为你未卜先知","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588010362,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250097,"ip_address":"","group_id":0},"score":250536,"extra":""},{"author":{"id":1016889,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/84/39/c8772466.jpg","nickname":"无形","note":"","ucode":"B740E2A68A17A5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":1121758,"avatar":"https://static001.geekbang.org/account/avatar/00/11/1d/de/62bfa83f.jpg","nickname":"aoe","note":"","ucode":"1C6201EDB4E954","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":250550,"discussion_content":"要是有这能力好了，我还有点小期待呢😄","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1588026961,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":250536,"ip_address":"","group_id":0},"score":250550,"extra":""}]}]},{"had_liked":false,"id":372409,"user_name":"ifelse","can_delete":false,"product_type":"c1","uid":2550743,"ip_address":"浙江","ucode":"D0565908C99695","user_header":"https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg","comment_is_top":false,"comment_ctime":1681117113,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":3,"score":2,"product_id":100048401,"comment_content":"学习了，干货满满","like_count":1,"discussions":[{"author":{"id":1165703,"avatar":"https://static001.geekbang.org/account/avatar/00/11/c9/87/7a96366d.jpg","nickname":"陈东","note":"","ucode":"97CF7C67D83851","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":491667,"discussion_content":"哈哈，你这样改变以后，使用了二分查找，自然不是哈希表了。\n不过你要注意一点，我们现在先不考虑posting list，而是仅仅考虑“词典太大，能否紧凑地装入内存中”。\n这个时候，哈希表中的key应该是每个词项的hash值，而value是这个词项的字符串，而不是list。你可以再想想怎么做。\n这个课后题的目的，其实是想让大家思考，为了将内存空间充分利用，能有哪些紧凑存储甚至压缩存储的方案。实际上，许多真实的系统中就是这么实现的。\n因此，你思考过了这些问题以后，以后如果看到某些系统的索引实现很折腾，那么不要奇怪，它可能只是想把数据都放入内存中而已。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1586789660,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391560,"user_name":"Geek_7b1aa5","can_delete":false,"product_type":"c1","uid":3865678,"ip_address":"江苏","ucode":"0A301F16479A6E","user_header":"","comment_is_top":false,"comment_ctime":1718528133,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"思考：倒排索引的dict如果能全部加载进内存，就会大幅提升效率，在hash表无法存入内存的情况，是否能用其他内存空间占用更小的数据结构，来将词典完全加载到内存，有序数组和二叉树是否可行？想要节省空间，哈希表中的key应该是每个词项的hash值，而value是这个词项的字符串以及postlist指针（需要一个结构体）主要是value占用的空间比较多。\n\n可以考虑数组的连续空间存储\n● 首先思路就是使用一个数组，每个数组元素存（结构体|length)，但是结构体中的string词项是变长的，因此开辟数组空间只能以最长的为准，这样导致空间的浪费。\n● 改进的思路就是使用可变长的char数组紧凑的存储（结构体），顺序查找是比较慢的，然后在使用一个int数组建立索引，每个元素都是int类型，存储结构体的hash值，指向char对应数组元素位置。","like_count":0},{"had_liked":false,"id":361939,"user_name":"迪马","can_delete":false,"product_type":"c1","uid":2764520,"ip_address":"广东","ucode":"5832F13B7788CD","user_header":"https://static001.geekbang.org/account/avatar/00/2a/2e/e8/e00c02de.jpg","comment_is_top":false,"comment_ctime":1667990911,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"倒排索引是怎么实现范围检索的？","like_count":0},{"had_liked":false,"id":350567,"user_name":"阿甘","can_delete":false,"product_type":"c1","uid":1057843,"ip_address":"","ucode":"BC93175B70E05D","user_header":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","comment_is_top":false,"comment_ctime":1657007623,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师你上面提到的最后给出的解决方案：词典文件 + 倒排文件的方式。是不是一个search要走两次IO啊。一次是根据内存的term index找到该term在磁盘中的词典文件的pos，从而捞出term对应的posting list的pos，然后再根据这个pos从磁盘中的倒排文件捞出最终的posting list？","like_count":0},{"had_liked":false,"id":333056,"user_name":"无昂","can_delete":false,"product_type":"c1","uid":1942414,"ip_address":"","ucode":"32E88B3D5B8DC0","user_header":"https://static001.geekbang.org/account/avatar/00/1d/a3/8e/518835da.jpg","comment_is_top":false,"comment_ctime":1644033058,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"对于一些&quot;黑产&quot;造出来的一些“违规词”，这次违规词在分词的时候，分词组件并不能很好的识别把这些词分成一个词，这样搜索的效果会大打折扣，这样的问题该如何解决呢？","like_count":0},{"had_liked":false,"id":391560,"user_name":"Geek_7b1aa5","can_delete":false,"product_type":"c1","uid":3865678,"ip_address":"江苏","ucode":"0A301F16479A6E","user_header":"","comment_is_top":false,"comment_ctime":1718528133,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"思考：倒排索引的dict如果能全部加载进内存，就会大幅提升效率，在hash表无法存入内存的情况，是否能用其他内存空间占用更小的数据结构，来将词典完全加载到内存，有序数组和二叉树是否可行？想要节省空间，哈希表中的key应该是每个词项的hash值，而value是这个词项的字符串以及postlist指针（需要一个结构体）主要是value占用的空间比较多。\n\n可以考虑数组的连续空间存储\n● 首先思路就是使用一个数组，每个数组元素存（结构体|length)，但是结构体中的string词项是变长的，因此开辟数组空间只能以最长的为准，这样导致空间的浪费。\n● 改进的思路就是使用可变长的char数组紧凑的存储（结构体），顺序查找是比较慢的，然后在使用一个int数组建立索引，每个元素都是int类型，存储结构体的hash值，指向char对应数组元素位置。","like_count":0},{"had_liked":false,"id":361939,"user_name":"迪马","can_delete":false,"product_type":"c1","uid":2764520,"ip_address":"广东","ucode":"5832F13B7788CD","user_header":"https://static001.geekbang.org/account/avatar/00/2a/2e/e8/e00c02de.jpg","comment_is_top":false,"comment_ctime":1667990911,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"倒排索引是怎么实现范围检索的？","like_count":0},{"had_liked":false,"id":350567,"user_name":"阿甘","can_delete":false,"product_type":"c1","uid":1057843,"ip_address":"","ucode":"BC93175B70E05D","user_header":"https://static001.geekbang.org/account/avatar/00/10/24/33/bcf37f50.jpg","comment_is_top":false,"comment_ctime":1657007623,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"老师你上面提到的最后给出的解决方案：词典文件 + 倒排文件的方式。是不是一个search要走两次IO啊。一次是根据内存的term index找到该term在磁盘中的词典文件的pos，从而捞出term对应的posting list的pos，然后再根据这个pos从磁盘中的倒排文件捞出最终的posting list？","like_count":0},{"had_liked":false,"id":333056,"user_name":"无昂","can_delete":false,"product_type":"c1","uid":1942414,"ip_address":"","ucode":"32E88B3D5B8DC0","user_header":"https://static001.geekbang.org/account/avatar/00/1d/a3/8e/518835da.jpg","comment_is_top":false,"comment_ctime":1644033058,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":3,"product_id":100048401,"comment_content":"对于一些&quot;黑产&quot;造出来的一些“违规词”，这次违规词在分词的时候，分词组件并不能很好的识别把这些词分成一个词，这样搜索的效果会大打折扣，这样的问题该如何解决呢？","like_count":0,"discussions":[{"author":{"id":2053679,"avatar":"https://static001.geekbang.org/account/avatar/00/1f/56/2f/4518f8e1.jpg","nickname":"放不下荣华富贵","note":"","ucode":"9FE29C22B9ABE3","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":578428,"discussion_content":"人工打tag然后ML","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1656735144,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}