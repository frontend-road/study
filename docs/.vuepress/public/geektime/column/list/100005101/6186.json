{"id":6186,"title":"23 | 推荐候选池的去重策略","content":"<p>今天依然要讲到两个问题，它们看似和推荐系统没有必然关系，但实际上，在你构建自己的推荐系统的时候，不可避免地会遇到这两个问题。</p>\n<h2>去重是刚需</h2>\n<p>在推荐系统中，有一个刚需就是去重，那么说在哪些地方有去重的需求呢？</p>\n<p>主要是在两个地方：一个是内容源去重，另一个是不重复给用户推荐。</p>\n<p>先说说内容源的去重，这部分以前几年的图文信息流推荐为典型的例子。</p>\n<p>如果一个平台自己不生产内容，只是做内容搬运和聚合分发，那么从大量第三方的内容生产处抓取内容，就难免遇到相似甚至重复的内容。这就需要对内容做一个重复检测了。</p>\n<p>对内容做重复检测，直观的思路是分词，然后提取关键词，再两两计算词向量之间的距离，距离小于一定阈值后就判定为重复。然而，这对于海量内容，比如几千万以上的内容来说简直就是灾难。</p>\n<p>其实，内容源去重并不是仅在推荐系统中才首次出现，这早在搜索引擎时代就是一个刚需了，搜索引擎把整个互联网的网页都下载到自己的服务器上，这时，重复冗余的内容就需要被检测出来。</p>\n<p>另一个需求是在内容阅读类推荐场景下，给用户推荐的内容不要重复，推荐过的内容就不再出现在推荐候选集中。</p>\n<p>在你刷一个信息流产品时，不断看到重复的内容，想必不是使用感很好的一件事。因为以抓取作为主要内容来源的信息流产品，不同于社交网站上用户自发产生内容，除非遇到用户恶意发送，否则后者是不容易重复的。</p>\n<p>以上两个场景，需要在你打造自己的推荐系统时予以考虑和应对。今天就介绍两种最常见的去重算法，两者有相通之处也有不同的之处，听我慢慢说来。</p>\n<!-- [[[read_end]]] -->\n<h2>Simhash</h2>\n<p>内容重复检测，是搜索引擎公司最先遇到的，所以Google在07年公开了他们内部的内容重复检测算法，这个算法简单有效，甚至造福了今天的信息流推荐产品。</p>\n<p>对于很长的内容，如果只是检测绝对重复，也就是说完全一模一样的那种情况，那么使用MD5这样的信息指纹方法非常高效，无需再去分词、提取关键词和计算关键词向量之间的距离。</p>\n<p>我们直接将原始的内容映射为一个短字符串，这个短字符串就是原始内容的指纹，虽然不是绝对保证和原始内容一一映射，但是不同内容能得到相同指纹的概率非常小。</p>\n<p>只是这种信息指纹的方法有个非常明显的坏处就是，哪怕原始内容改一个字，得到的信息指纹就会截然不同。</p>\n<p>这就没法愉快地玩耍了，你一定希望的是只要主要内容不变，就算一些不太重要的词句不同，也仍然可以得到相近甚至相同的指纹。这才能更加灵活地进行内容重复检测。是否有这样的算法？有，就是Simhash。</p>\n<p>Simhash核心思想也是为每个内容生成一个整数表示的指纹，然后用这个指纹去做重复或者相似的检测。下面这个示意图说明了Simhash如何把一个原始内容表示成一个整数指纹。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/de/4e/de7491eede4275a68a5ab8af17d6294e.png?wh=1638*1092\" alt=\"\" /></p>\n<p>好，现在详细说明一下这个过程。</p>\n<ol>\n<li>首先，对原始内容分词，并且计算每个词的权重；</li>\n<li>对每个词哈希成一个整数，并且把这个整数对应的二进制序列中的0变成-1，1还是1，得到一个1和-1组成的向量；</li>\n<li>把每个词哈希后的向量乘以词的权重，得到一个新的加权向量；</li>\n<li>把每个词的加权向量相加，得到一个最终向量，这个向量中每个元素有正有负；</li>\n<li>把最终这个向量中元素为正的替换成1，为负的替换成0，这个向量变成一个二进制位序列，也就是最终变成了一个整数。</li>\n</ol>\n<p>最终这个整数就代表了原始的内容。这个Simhash奇妙在哪呢？</p>\n<p>看这个示意图中，我故意加了一个不太重要的词“了”，它的权重是1，对应的加权向量元素不是1就是-1，在上述的第四步中，如果这个词对应的向量缺少了，其实根本不影响最终得到那个整数，因为它很难改变最终向量元素的正负。这就是为什么那些不太重要的词不影响内容之间的重复检测。</p>\n<p>Simhash为每一个内容生成一个整数指纹，其中的关键是把每个词哈希成一个整数，这一步常常采用Jenkins算法。这里简单示意的整数只有8个二进制位，实际上可能需要64个二进制位的整数，甚至范围更大。</p>\n<p>得到每个内容的Simhash指纹后，可以两两计算汉明距离，比较二进制位不同个数，其实就是计算两个指纹的异或，异或结果中如果包含3个以下的1，则认为两条内容重复。</p>\n<p>为了高效，也可以直接认为指纹相同才重复，视情况而定。</p>\n<h2>Bloomfilter</h2>\n<p>除了内容重复检测，还有一个需求是防止已经推荐的内容被重复推荐。这个刚需和上述内容重复相比，最大的不同就是过滤对象不同，上述Simhash过滤对象是内容本身，而这里则一般是内容的ID。</p>\n<p>内容的ID一般是用一个UUID表示，是一个不太长的字符串或者整数。</p>\n<p>对于这类形如模式串的去重，显然可以用单独专门的数据库来保存，为了高效，甚至可以为它建上索引。</p>\n<p>但对于用户量巨大的情况下，这个做法对存储的消耗则不可小看。实际上，解决这类看一个字符串在不在一个集合中的问题，有一个有点老但很好用的做法，就是Bloomfilter，有时候也被称为布隆过滤器。</p>\n<p>布隆过滤器的原理也要用到哈希函数。它包含两部分：一个很长的二进制位向量，和一系列哈希函数。Bloomfilter是一个很巧妙的设计，它先把原始要查询的集合映射到一个长度为m的二进制位向量上去，它映射的方法是：</p>\n<ol>\n<li>设计n个互相独立的哈希函数，准备一个长度为m的二进制向量，最开始全是0；</li>\n<li>每个哈希函数把集合内的元素映射为一个不超过m的正整数k，m就是二进制向量的长度；</li>\n<li>把这个二进制向量中的第k个位置设置为1；也就是一个元素会在二进制向量中对应n个位置为1。</li>\n</ol>\n<p>我放了一个示意图。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/ef/4f/ef000d86f72ca2341f95d8bc74ab854f.png?wh=1350*732\" alt=\"\" /></p>\n<p>这个示意图中，原始的模式串经过三个互相独立的哈希函数，映射到8位二进制向量中的三个位置了。</p>\n<p>原始的模式串集合经过这样的处理后，就得到一个很大的二进制向量。在应用阶段时，假如来了一个模式串s，需要查询是否在这个集合中，也需要经过同样的上述步骤。</p>\n<p>每个哈希函数对这个模式串s哈希后都得到一个整数，看看这个整数在二进制向量中所指示的位置是不是1，如果每个哈希函数所指示的位置都是1，就说明模式串s已经在集合中了。</p>\n<p>需要说明的是，Bloomfilter也并不是百分之百保证的，有很小的概率把原本不存在集合中的模式串判断为存在。这样就会造成那些明明还没有推荐给用户的内容ID就再也不会推荐给用户了，当然，这个小概率是可以承受的。</p>\n<h2>总结</h2>\n<p>好了，今天介绍了两种去重算法。在推荐系统中，虽然我们十分关心推荐匹配的效果，但是别忘了，对原始内容的挖掘和清洗往往更加重要。这其中就包括对重复内容的检测。</p>\n<p>两种去重策略都是牺牲一点误伤的概率换得大幅度的效率提升，具体的做法都是要借助哈希函数。只是哈希函数的结果在两个算法中有不同的处理手段，Simhash是加权，Bloomfilter则是用来做寻址。</p>\n<p>最后，留给你一个思考题，由于今天的内容比较简单，留给你思考题也简单，请你想一想，如果要从Bloomfilter中去掉一个元素，该怎么做？欢迎给我留言，我们一起讨论。</p>\n<p>感谢你的收听，我们下次再见。</p>\n<p><img src=\"https://static001.geekbang.org/resource/image/76/02/76de8928fe8206b0467b8c773d6ced02.jpg?wh=3560*2008\" alt=\"\" /></p>\n","comments":[{"had_liked":false,"id":6350,"user_name":"林彦","can_delete":false,"product_type":"c1","uid":1032615,"ip_address":"","ucode":"5094CC6ED7B40C","user_header":"https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg","comment_is_top":false,"comment_ctime":1524663230,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"48769303486","product_id":100005101,"comment_content":"Counting Bloom Filter支持删除操作，除了已有的二进制向量，向量的每一位对应一个整数计数器。每当增加一个元素时，哈希函数映射到的二进制向量对应的整数计数器加一，删除时减一。有了这个操作可以增加，查找和删除集合里的元素。","like_count":10},{"had_liked":false,"id":113985,"user_name":"chy2048","can_delete":false,"product_type":"c1","uid":1072395,"ip_address":"","ucode":"529BB2ACF75EB8","user_header":"https://static001.geekbang.org/account/avatar/00/10/5d/0b/9d4da40a.jpg","comment_is_top":false,"comment_ctime":1563198457,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"23038034937","product_id":100005101,"comment_content":"买了这个专栏只想请教下关于去重的问题，不知道还会不会有人回复😂<br>用布隆过滤器防止内容重复推荐，具体是怎么实现的？<br>1.是一个用户一个布隆过滤器吗？<br>2.如果是一人一个布隆过滤器的话，如何设置布隆过滤器的大小呢？貌似不能动态伸缩吧？<br>3.布隆过滤器持久化是依赖redis吗？<br>4.如果需要对过去24小时的内容去重，如果每隔24小时创建一个布隆过滤器，那两个过滤器如何平滑过度？<br>看到有空麻烦回复下，谢谢🙏买这个专栏只为这一篇，本来想看下面的评论，结果评论只有10条，我晕","like_count":6,"discussions":[{"author":{"id":2311670,"avatar":"https://static001.geekbang.org/account/avatar/00/23/45/f6/09aa6e38.jpg","nickname":"le yi web","note":"","ucode":"A9CDFB3621B603","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":561205,"discussion_content":"我觉得主要是看业务吧，通常与用户相关的Filter肯定是每个用户单独一个，存储的话不局限在Redis；对于bloomfilter可以封一层变成按块或者时间滚动的bloomfilter，实现动态拓展，这个好像有很多经典的资料可以参考","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1649576764,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":6318,"user_name":"EAsY","can_delete":false,"product_type":"c1","uid":1064868,"ip_address":"","ucode":"E166B6CEDD5044","user_header":"https://static001.geekbang.org/account/avatar/00/10/3f/a4/acbd2eb4.jpg","comment_is_top":false,"comment_ctime":1524645067,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"18704514251","product_id":100005101,"comment_content":"用布隆过滤来过滤用户推荐记录的话 是否需要为每个用户存一个向量 之前考虑过用bitmap 内容池经常变动 感觉比较麻烦 ","like_count":4,"discussions":[{"author":{"id":1157776,"avatar":"https://static001.geekbang.org/account/avatar/00/11/aa/90/b3eaee4a.jpg","nickname":"魅影骑士","note":"","ucode":"1BF7DA167B349B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":8409,"discussion_content":"可以考虑将用户标识和推荐内容标识拼接在一起，使用一个大的布隆过滤器来存放。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1567998482,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":7122,"user_name":"vicviz","can_delete":false,"product_type":"c1","uid":1039633,"ip_address":"","ucode":"D6B854C2AC7405","user_header":"https://static001.geekbang.org/account/avatar/00/0f/dd/11/b93ae644.jpg","comment_is_top":false,"comment_ctime":1525306111,"is_pvip":false,"discussion_count":3,"race_medal":0,"score":"10115240703","product_id":100005101,"comment_content":"Bloomfilter非常大的时候，用什么存储呢？用户数过亿，保存上千条内容不重，还得持久化","like_count":2,"discussions":[{"author":{"id":1057663,"avatar":"https://static001.geekbang.org/account/avatar/00/10/23/7f/4030c83d.jpg","nickname":"Z张明锋","note":"","ucode":"BD76823E843867","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":5394,"discussion_content":"工程化的话可以用redis自带的bloomfilter","likes_number":2,"is_delete":false,"is_hidden":false,"ctime":1566225386,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":2,"child_discussions":[{"author":{"id":1738324,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/86/54/b8b9d7d5.jpg","nickname":"游","note":"","ucode":"37927840E2A90D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1057663,"avatar":"https://static001.geekbang.org/account/avatar/00/10/23/7f/4030c83d.jpg","nickname":"Z张明锋","note":"","ucode":"BD76823E843867","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":284944,"discussion_content":"redis有自带的吗","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1592677695,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":5394,"ip_address":""},"score":284944,"extra":""},{"author":{"id":1008582,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/63/c6/d6ea3df3.jpg","nickname":"林肯","note":"","ucode":"D2C97220230DE5","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1738324,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/86/54/b8b9d7d5.jpg","nickname":"游","note":"","ucode":"37927840E2A90D","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":306129,"discussion_content":"有 插件方式","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1600177872,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":284944,"ip_address":""},"score":306129,"extra":""}]}]},{"had_liked":false,"id":6377,"user_name":"曾阿牛","can_delete":false,"product_type":"c1","uid":1049468,"ip_address":"","ucode":"29A294174EF06B","user_header":"https://static001.geekbang.org/account/avatar/00/10/03/7c/39ea8a23.jpg","comment_is_top":false,"comment_ctime":1524675586,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"10114610178","product_id":100005101,"comment_content":"对于分页展示的推荐列表，有更快速的方法保证前后几页不重复吗？","like_count":2,"discussions":[{"author":{"id":1161584,"avatar":"https://static001.geekbang.org/account/avatar/00/11/b9/70/c454312c.jpg","nickname":"早早凡","note":"","ucode":"4AA7CAB2CAF20D","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":409222,"discussion_content":"Not in 几天的去重列表？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1635394810,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":121240,"user_name":"随心而至","can_delete":false,"product_type":"c1","uid":1021732,"ip_address":"","ucode":"2713FD5AEF2B84","user_header":"","comment_is_top":false,"comment_ctime":1565083058,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5860050354","product_id":100005101,"comment_content":"如果每一节能把参考的资料给出来就好了，虽然有的通过Google可以找到类似的。","like_count":1},{"had_liked":false,"id":99323,"user_name":"yyy","can_delete":false,"product_type":"c1","uid":1498124,"ip_address":"","ucode":"E62835A2E9B917","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eogXpJz4QXkrIamNmh2DiahxHguM4o9eluFMK2Cic2PcCH03VhSUibBKhEECkFic3ZMJGW1x6El5zNBqg/132","comment_is_top":false,"comment_ctime":1559193155,"is_pvip":false,"replies":[{"id":"36014","content":"已经完成，还在编辑中。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1559490083,"ip_address":"","comment_id":99323,"utype":1}],"discussion_count":2,"race_medal":0,"score":"5854160451","product_id":100005101,"comment_content":"信息流页面，调用api接口获取到推荐的数据，一般情况下会在页面进行瀑布流加载更多。那么每次请求数据和整个瀑布流批次数据如何统一？防止重复推荐？如何处理整个批次的推荐和单次的存储、缓存、以及统一呢？ 作者回复: 关于这个问题，在我的图书中有详细介绍。<br>我：哪本书呢","like_count":1,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":452061,"discussion_content":"已经完成，还在编辑中。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1559490083,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1034087,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/c7/67/0077314b.jpg","nickname":"田佳伟","note":"","ucode":"D31C9799F383D2","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":78363,"discussion_content":"我现在也在考虑怎么解决这个问题，就是如何把feed流数据进行过滤之后保证返回给客户端的数据条数以及如何保证下一页不重复，你的问题解决了吗？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1575988441,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":6376,"user_name":"曾阿牛","can_delete":false,"product_type":"c1","uid":1049468,"ip_address":"","ucode":"29A294174EF06B","user_header":"https://static001.geekbang.org/account/avatar/00/10/03/7c/39ea8a23.jpg","comment_is_top":false,"comment_ctime":1524675461,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"5819642757","product_id":100005101,"comment_content":"业界一般是不对布隆过滤器剔除元素，原因是剔除已有元素有可能导致整体数据错误。想到一种方法：使用一个同样长度的向量，记录对于位置1的个数，剔除是先hash6映射，对于1的位置，个数大于的话不变，等于1的话设为0；不过，缺点是这个向量占空间，存储成稀疏向量吧","like_count":1},{"had_liked":false,"id":318693,"user_name":"早早凡","can_delete":false,"product_type":"c1","uid":1161584,"ip_address":"","ucode":"4AA7CAB2CAF20D","user_header":"https://static001.geekbang.org/account/avatar/00/11/b9/70/c454312c.jpg","comment_is_top":false,"comment_ctime":1635394707,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1635394707","product_id":100005101,"comment_content":"布隆过滤器是先查询推荐的分页内容，再使用布隆过滤器过滤。<br><br>如果分页查询是基于mysql或者es，会不会查询出来被去重完了，做很多无效分页查询？","like_count":0},{"had_liked":false,"id":198847,"user_name":"晨晓","can_delete":false,"product_type":"c1","uid":1811062,"ip_address":"","ucode":"8AC664799E5CDD","user_header":"https://static001.geekbang.org/account/avatar/00/1b/a2/76/bdea7aa1.jpg","comment_is_top":false,"comment_ctime":1585470415,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1585470415","product_id":100005101,"comment_content":"少说了一个simhash抽屉原理，这个在大量pairwise计算中是很有用的","like_count":0},{"had_liked":false,"id":130599,"user_name":"luis","can_delete":false,"product_type":"c1","uid":1600246,"ip_address":"","ucode":"6B2727FFE87F01","user_header":"https://static001.geekbang.org/account/avatar/00/18/6a/f6/e1ad3e30.jpg","comment_is_top":false,"comment_ctime":1567492136,"is_pvip":false,"discussion_count":1,"race_medal":0,"score":"1567492136","product_id":100005101,"comment_content":"如bloomfilter要存储的过滤数据很大 每个用户至少需要20mb 全放在内存 100万用户就要2000g的内存 这要怎么解决","like_count":0,"discussions":[{"author":{"id":1132314,"avatar":"https://static001.geekbang.org/account/avatar/00/11/47/1a/645ab65b.jpg","nickname":"全","note":"","ucode":"C478EAFACC4E3B","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":365883,"discussion_content":"数据再大应该也不需要20M","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1617895645,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":""},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":91895,"user_name":"shangqiu86","can_delete":false,"product_type":"c1","uid":1514817,"ip_address":"","ucode":"07D376EEC21BE4","user_header":"http://thirdwx.qlogo.cn/mmopen/vi_32/qRjoqWIGC6tpmKZBGTxjQKC9cbz9XLhw2nF1c74R4icFOYOdVO4iaeQEQDqEvmbicxn6HEc4SU8kpkwVaO5nABMug/132","comment_is_top":false,"comment_ctime":1557129905,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1557129905","product_id":100005101,"comment_content":"感觉布隆过滤不错，可以考虑把我们这目前的累计用户的点击sku串改成布隆过滤这种方式，来增加保存的用户历史行为数据量","like_count":0},{"had_liked":false,"id":77126,"user_name":"chon","can_delete":false,"product_type":"c1","uid":1068925,"ip_address":"","ucode":"1C32170972F726","user_header":"https://static001.geekbang.org/account/avatar/00/10/4f/7d/dd852b04.jpg","comment_is_top":false,"comment_ctime":1552841106,"is_pvip":true,"replies":[{"id":"28800","content":"自己实现很简单的。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1553339793,"ip_address":"","comment_id":77126,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1552841106","product_id":100005101,"comment_content":"老师，simhash算法有啥好用的来源项目吗？谢谢","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":443589,"discussion_content":"自己实现很简单的。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1553339793,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":60318,"user_name":"Da.du.Ma","can_delete":false,"product_type":"c1","uid":1333556,"ip_address":"","ucode":"E3FDFEE2F04E9C","user_header":"https://static001.geekbang.org/account/avatar/00/14/59/34/2df52bae.jpg","comment_is_top":false,"comment_ctime":1547457826,"is_pvip":false,"replies":[{"id":"27306","content":"关于这个问题，在我的图书中有详细介绍。","user_name":"作者回复","user_name_real":"刑无刀","uid":"1005376","ctime":1552322191,"ip_address":"","comment_id":60318,"utype":1}],"discussion_count":1,"race_medal":0,"score":"1547457826","product_id":100005101,"comment_content":"信息流页面，调用api接口获取到推荐的数据，一般情况下会在页面进行瀑布流加载更多。那么每次请求数据和整个瀑布流批次数据如何统一？防止重复推荐？如何处理整个批次的推荐和单次的存储、缓存、以及统一呢？<br>","like_count":0,"discussions":[{"author":{"id":1005376,"avatar":"https://static001.geekbang.org/account/avatar/00/0f/57/40/777bfe30.jpg","nickname":"刑无刀","note":"","ucode":"496201BF63D49D","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":436475,"discussion_content":"关于这个问题，在我的图书中有详细介绍。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1552322191,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":""},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":8238,"user_name":"wzm1990","can_delete":false,"product_type":"c1","uid":1041666,"ip_address":"","ucode":"85EFDAC8872614","user_header":"https://static001.geekbang.org/account/avatar/00/0f/e5/02/ea609428.jpg","comment_is_top":false,"comment_ctime":1526000172,"is_pvip":false,"discussion_count":0,"race_medal":0,"score":"1526000172","product_id":100005101,"comment_content":"请教个问题，我们在用 simhash 做文本去重，用一个 simhash 值跟几十万个值比对。目前是把几十万的值放到 redis，比对时加载到程序里，这样做特别耗cpu，有没有其他更好的实现","like_count":0}]}