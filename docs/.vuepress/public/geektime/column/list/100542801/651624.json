{"id":651624,"title":"07｜数据获取：什么是Scrapy框架？","content":"<p>你好，我是黄鸿波。</p><p>上一节课我们讲解了什么是爬虫以及爬虫的基本原理，从这节课开始，我们就要实际地去爬取一些网络上的内容，为后续推荐系统的使用做准备。</p><p>这节课我们来深入了解一下Python中的常见爬虫框架：Scrapy框架。我们将学习什么是Scrapy框架、它的特点是什么以及如何安装和使用它。</p><h2>Scrapy框架概览</h2><p>Scrapy是一个适用于Python的快速、高层次的屏幕抓取和Web抓取框架，用于抓取Web站点并从页面中提取结构化的数据。它也提供了多种类型爬虫的基类，如BaseSpider、Sitemap爬虫等。我们可以很方便地通过 Scrapy 框架实现一个爬虫程序，抓取指定网站的内容或图片。</p><p><img src=\"https://static001.geekbang.org/resource/image/67/87/67220e129b726a764fa62f28fb46e587.png?wh=1200x482\" alt=\"\"></p><p>下面是Scrapy框架的架构图。</p><p><img src=\"https://static001.geekbang.org/resource/image/02/b8/02ce18db3937d494e05ddcbdd60ee1b8.png?wh=1766x1039\" alt=\"\"></p><p>通过这张图我们可以看到，Scrapy框架总共分成了下面七个部分。</p><ol>\n<li>Scrapy Engine（Scrapy引擎）。</li>\n<li>Scheduler（调度器）。</li>\n<li>Downloader（下载器）。</li>\n<li>Spiders（爬虫）。</li>\n<li>Item Pipline（管道）。</li>\n<li>Downloader Middlewares（下载中间件）。</li>\n<li>Spider Middlewares（Spider中间件）。</li>\n</ol><p>接下来，我们来看看这七个部分的具体含义，以及它们是如何协作的。</p><!-- [[[read_end]]] --><p><strong>Scrapy Engine</strong><strong>：</strong>Scrapy引擎是整个Scrapy框架的核心，负责所有子模块间的调度、通信和数据传递等，我们可以把它看作整个Scrapy框架的总指挥。</p><p><strong>Spider</strong>：它负责处理所有的responses。也就是说，当爬虫程序获取到服务器端的response响应之后，接下来就会交由Spider去处理，它会根据需求提取数据和相应的URL信息，并交给引擎。值得注意的是，Spider模块虽然是Scrapy框架自带的模块，但是它仍然需要开发者自己手写代码实现。</p><p><strong>Scheduler</strong>：调度器负责接收引擎发送过来的requests请求，并将它们放入到队列当中，在后续引擎有需要的时候再将请求返回给引擎。我们可以理解为下一步要抓取的网址由它决定，同时，在调度器里还会自动去掉重复的网址。</p><p><strong>Downloader</strong>：下载器主要负责下载Scrapy Engine发送过来的所有requests请求，并将获取到的responses信息经 Scrapy Engine  返回给Spider处理。</p><p><strong>Item Pipeline</strong>：管道，主要负责处理Spider中获取到的item并进行后期处理，比如对数据进行分析、过滤、存储等。</p><p><strong>Downloader Middlewares</strong>：下载中间件，开发者可以在这里自定义扩展下载功能的组件。</p><p><strong>Spider Middlewares</strong>：Spider中间件，可以自定义request请求和进行response过滤的组件。</p><p>了解了这七个组件的功能之后，我们来看看各组件的工作流程。在上面这张图中，我们可以看到核心的位置是引擎。也就是说基本所有的流程都和引擎相关，借助Scrapy框架爬取信息大致可以分为下面六步。</p><ol>\n<li>爬虫程序工作的时候会经由引擎向Spider提出申请，索要第一个待爬取的URL。待引擎拿到URL之后，就将这个URL传入调度程序Scheduler。</li>\n<li>Scheduler会将需要下载的URL加入到队列中，形成一个URL队列。当Scheduler完成请求后，会取出队列中的第一个URL，将其返回给引擎。</li>\n<li>引擎拿到URL之后，会将这个URL经由Downloader Middlewares交给Downloader。完成下载后，我们会得到服务端返回来的response对象。</li>\n<li>当Downloader拿到了response对象之后，接下来要做的就是把它交给引擎，然后引擎再经由Spider Middlewares将response结果交给Spider文件。</li>\n<li>Spider文件处理和分析response的结果，在此基础上拿到我们想要的数据，这里一般要做的就是标签解析、内容提取等工作。</li>\n<li>解析完数据之后，我们要把这些数据经由Item Pipeline存储起来。无论是存文件还是存数据库，理论上都是由这一个组件来完成。</li>\n</ol><p>完成上面六个步骤之后，我们的第一个链接爬虫工作就结束了。紧接着，我们会循环这个操作，直至拿到我们要爬取的所有文件为止（也就是直到URL列表空了），这时我们就完成了一整套爬虫工作。</p><h2>Scrapy框架的安装和使用</h2><p>了解了Scrapy框架的工作原理，我们就可以更好地安装和使用 Scrapy  。</p><p>Scrapy在Python中被当作一个库来使用，要想在Python中使用Scrapy，首先我们要安装Scrapy。而要在Python中安装各种各样的库，我推荐使用Anaconda来进行Python环境的管理。</p><h3>安装Anaconda环境</h3><p>Anaconda是一个开源的Python发行版本，它包括Conda、Python以及一大堆安装好的工具包，比如：Numpy、Pandas等。我们可以把Anaconda看作是一个常用的扩展库的集合，我们可以使用Anaconda很轻松地管理我们的扩展库，还可以使用Anaconda创建多个虚拟环境。每个虚拟环境都是独立的，我们可以在各个环境上分别安装独立的包。</p><p>Anaconda安装包的获取方式有很多，我们首先想到的肯定是从官网下载。不过，我个人建议从清华源下载，因为在国内访问清华源速度相对较快，下载起来比较容易。我们直接百度搜索Anaconda清华源，或者输入以下网址即可。</p><pre><code>https://mirrors.bfsu.edu.cn/anaconda/archive/\n</code></pre><p>在这里，我们下载的版本为2022.05-Windows-x86_64。</p><p><img src=\"https://static001.geekbang.org/resource/image/15/72/150fd5bd54d78894bbd59d2f4595be72.png?wh=1364x1792\" alt=\"\"></p><p>下载完成之后，我们双击安装包进行安装。</p><p><img src=\"https://static001.geekbang.org/resource/image/d9/8f/d958fcc27c37788b7dbbb892d3yyc08f.png?wh=738x574\" alt=\"\"></p><p>然后一路点击Next，直到选择目录那一步，你可以根据习惯选择目录，然后点击Next，出现下面的界面。</p><p><img src=\"https://static001.geekbang.org/resource/image/49/9e/49acd833a68a154ae9b0114150d0099e.png?wh=741x575\" alt=\"\"></p><p>这里我建议把两个复选框都选中，上面的复选框是说要把Anaconda3加入到系统的环境变量中，下面的选项大意是使用Anaconda3作为默认的Python3.9，然后点击Install。安装完成后，我们在命令控制台输入conda，如果出现以下界面，说明安装完成了。</p><p><img src=\"https://static001.geekbang.org/resource/image/aa/4d/aa63fe3372dyycefb7d292baa989254d.png?wh=2129x1229\" alt=\"\"></p><p>接下来，我们要创建一个虚拟环境。因为我们这个虚拟环境主要是用来做爬虫的，所以我们可以创建一个名为scrapy_recommendation的虚拟环境，并指定我们使用的Python版本为3.7版本。具体做法是输入如下命令。</p><pre><code>conda create -n scrapy_recommendation python==3.7\n</code></pre><p>稍等片刻，会出现下面的界面。</p><p><img src=\"https://static001.geekbang.org/resource/image/98/ef/98c8df3a921312398d94f8259e7yyaef.png?wh=2556x1089\" alt=\"\"></p><p>我们输入Y，开始安装，安装结束后，会弹出如下提示。</p><p><img src=\"https://static001.geekbang.org/resource/image/cb/f3/cb437de0752bb31b4e8697e860db0cf3.png?wh=719x342\" alt=\"\"></p><p>但是这里要注意的是，它会告诉你如果要激活你的环境，需要输入conda activate scrapy_recommendation，但是实际上在Windows中，这样输入会产生报错。</p><p><img src=\"https://static001.geekbang.org/resource/image/4e/8e/4e1e4108589e70726387e53bc33cd48e.png?wh=825x415\" alt=\"\"></p><p>这个应该属于Anaconda的一个小Bug，因为这实际上是在Linux和Mac环境下的命令，在Windows环境下，我们直接输入activate scrapy_recommendation即可。这个时候会有如下显示。</p><p><img src=\"https://static001.geekbang.org/resource/image/9f/ce/9f582124d1ecd5241d460fd9799139ce.png?wh=583x110\" alt=\"\"></p><h3>安装Scrapy环境</h3><p>现在，我们的 Anaconda  环境就已经安装好了，下一步是安装Scrapy环境。实际上要在Anaconda环境中安装Scrapy很简单，只需要执行pip install scrapy即可。</p><p>等到出现如下界面时，安装就完成了。</p><p><img src=\"https://static001.geekbang.org/resource/image/7c/d9/7c67a8c7cf0c5b04244e5cf68cb618d9.png?wh=2615x988\" alt=\"\"></p><p>我们可以通过  conda list  来查看我们已有的包。</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/fa/b102327567450a48c5d7da442eec6cfa.png?wh=830x1082\" alt=\"\"></p><h3>使用Scrapy创建爬虫开发环境</h3><p>接下来，我们就尝试使用Scrapy创建一个爬虫开发环境。</p><p>首先，我们要确定好一个我们要搭建环境的目录，我选择了I盘的geekbang作为我要搭建环境的根目录。所以我首先要做的就是切换到这个目录下。具体做法是输入如下指令。</p><pre><code>cd I:\\geekbang\n</code></pre><p>切换到指定目录下之后，我们就可以创建我们的Scrapy工程了。要注意的是，我们可以直接通过Scrapy的相关命令来创建爬虫工程。因为我们准备爬取新浪新闻里的新闻数据，把它作为我们后面推荐系统的原始数据。所以我们可以创建一个工程名为sina的爬虫项目，输入如下命令。</p><pre><code>scrapy startproject sina\n</code></pre><p>创建完成项目之后，会是下图的样子。</p><p><img src=\"https://static001.geekbang.org/resource/image/0b/80/0b47ca678b7d19155c3135f5b8dfeb80.png?wh=2088x333\" alt=\"\"></p><p>这个时候，我们进入到I:\\geekbang目录下，会发现Scrapy帮我们创建了一个名为“sina”的目录。</p><p><img src=\"https://static001.geekbang.org/resource/image/20/60/20fba8d54353019e6e40bec938875960.png?wh=830x151\" alt=\"\"></p><p>我们进入到这个目录之后会发现，这个目录里，已经帮我们创建好了一个基础的Scrapy工程，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/cb/52/cb827d02fa8ayy277a8c894d2ba9b652.png?wh=830x279\" alt=\"\"></p><p>接下来我们将项目导入到IDE环境中，然后在IDE环境里查看整体的目录结构。这里我选择了PyCharm作为我们后续所有开发的IDE。我们在PyCharm中依次点击File-&gt;open，弹出一个文件目录选择框，我们选择I:\\geekbang\\sina作为我们的项目根目录，如下图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/a3/b7/a3563e93a571658b7f1d40b49ac6aeb7.png?wh=633x726\" alt=\"\"></p><p>然后点击OK，导入我们的项目。</p><p>需要注意的是，这个时候，我们PyCharm的开发环境是默认的，我们需要将其切换到我们所创建的Anaconda的“scrapy_recommendation”环境下。</p><p>因此，我们要依次点击：</p><p>File-&gt;Settings-&gt;Project:sina-&gt;Python Interpreter，然后将Python Interpreter的目录选择为我们的Anaconda创建的scrapy_recommendation环境下的python.exe文件。然后点击OK，切换环境成功。</p><p><img src=\"https://static001.geekbang.org/resource/image/a2/cb/a267bf5e85317874459e191a8297c8cb.png?wh=1193x1006\" alt=\"\"></p><p>接下来，我们在主界面的左侧把所有的目录展开，我们来分别看一下Scrapy的各个目录以及它们的功能。</p><p><img src=\"https://static001.geekbang.org/resource/image/ef/39/efee48abeea350eb5db293679ec07839.png?wh=334x514\" alt=\"\"></p><p>我们首先可以看到，在sina工程下面有一个同名的目录“sina”，和这个目录同一级的还有一个叫做scrapy.cfg的文件。你可以对照下面这个表格，看看这些文件的作用。</p><p><img src=\"https://static001.geekbang.org/resource/image/c3/8f/c313c6199779a2e3ba77ed57486e628f.jpg?wh=3210x2072\" alt=\"\"></p><p>现在我们的 Scrapy  工程就创建完了。接下来我们再来创建一个爬虫程序，爬虫主要功能是爬取网站的数据。我们可以在I:\\geekbang\\sina目录下创建它，我们还是在cmd中切换到这个目录下，然后使用如下命令创建一个名为sina_spider的爬虫程序。</p><pre><code>scrapy genspider sina_spider sina.com.cn\n</code></pre><p>这时候命令行中显示如下。</p><p><img src=\"https://static001.geekbang.org/resource/image/10/a3/10ab2b58534e9d179041cfd37d05f6a3.png?wh=826x130\" alt=\"\"></p><p>然后我们切换到IDE中，会发现在sina\\sina\\spiders下面出现了一个名为sina_spider.py的文件，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/23/dc/2313f111c2b9347550f67bf6369291dc.png?wh=519x531\" alt=\"\"></p><p>我们打开这个文件，如果出现如下代码，说明我们这个爬虫文件创建成功了。</p><pre><code>import scrapy\n \n \nclass SinaSpiderSpider(scrapy.Spider):\n    name = 'sina_spider'\n    allowed_domains = ['sina.com.cn']\n    start_urls = ['http://sina.com.cn/']\n \n    def parse(self, response):\n        pass\n</code></pre><p>进行到这里，一个基本的爬虫程序就创建完成了。但是现在离可以爬取数据还差一步，我们需要让我们的爬虫程序可以模拟人的操作来浏览网页。也就是说，我们要装一个插件来连接我们的Scrapy框架和浏览器界面，这里我推荐使用Chrome浏览器，我们需要下载的插件名称就是ChromeDriver，ChromeDriver插件的下载地址如下。</p><pre><code>http://chromedriver.storage.googleapis.com/index.html\n</code></pre><p>这个插件有很多版本，那我们是不是随便下载一个版本就可以了呢？当然不是，我们下载的版本一定要和Chrome浏览器的版本对应上。我们可以在Chrome浏览器中输入下面的命令进入设置界面。</p><pre><code>chrome://settings/help\n</code></pre><p>这时候会弹出一个关于Chrome的界面，在这个界面里可以看到我们浏览器的版本。</p><p><img src=\"https://static001.geekbang.org/resource/image/5e/b2/5e962a6d2d9b9524e4f99c45b7304eb2.png?wh=1895x1194\" alt=\"\"></p><p>可以看到，我的版本为111.0.5563.149，所以我们回到ChromeDriver的下载界面找到110.0.5563版本进行下载，如果小版本找不到的话，就找前一个级别的版本中最后一个版本即可。</p><p><img src=\"https://static001.geekbang.org/resource/image/6b/35/6b0cf59b49783fe832d4968952282235.png?wh=1485x1093\" alt=\"\"></p><p>这里要注意的是，我们可能找不到完全一样的版本。如果遇到这种情况，找到上一级版本号一样的插件中的最后一个版本即可。我这里对应的是110.0.5563.64，点击下载。这里适配Windows的插件只有32位的版本，但它其实没什么影响，我们即使是64位的操作系统也可以使用，正常下载即可。</p><p><img src=\"https://static001.geekbang.org/resource/image/4e/67/4e6b1d34c8d6755a1317dbyy94cdc367.png?wh=1304x509\" alt=\"\"></p><p>下载完成之后，解压会得到一个chromedriver.exe  的文件，要把这个文件分别复制到我们的Anaconda的scrapy_recommendation环境和Rhrome的浏览器中，如图所示。</p><p><img src=\"https://static001.geekbang.org/resource/image/97/76/97f5639b86c2c8cb821e766a3db2b276.png?wh=831x489\" alt=\"\"></p><p><img src=\"https://static001.geekbang.org/resource/image/3e/fe/3eyy124739f8e57301e3a8e9b0ce3bfe.png?wh=830x399\" alt=\"\"></p><p>到这里，我们的ChromDriver插件就安装好了，只有安装好这个插件，我们在爬取数据的时候才能连接上浏览器。</p><p>我们知道，要运行一个程序就要写一个主文件。一般来说这个主文件是main.py文件，所以我们在项目的根目录下创建一个名为main.py的文件，然后输入如下代码。</p><pre><code>from scrapy import cmdline\n \ncmdline.execute('scrapy crawl sina_spider'.split())\n</code></pre><p>简单解释一下这段程序。在这段程序里，我们首先从Scrapy的包里导入了cmdline这个库，然后使用cmdline.execute执行了scrapy crawl sina_spider。这里scrapy crawl是一个Scrapy的基础命令，表示启动爬虫程序，后面跟的是爬虫的名字。所以上面代码合起来的意思就是使用cmd命令启动名为“sina_spider”的爬虫程序。</p><p>如果出现如下界面，说明Scrapy的基础程序是正确的。</p><p><img src=\"https://static001.geekbang.org/resource/image/69/f3/69a87bf9d64bef8852321b9b38ac79f3.png?wh=3840x2098\" alt=\"\"></p><p>到这一步，虽然程序已经知道了要爬取的网站是sina.com.cn，现在运行也没有报错，但是似乎并没有什么结果，而且马上就关闭了。这是因为我们目前只是启动了这个爬虫程序，并没有写具体的爬虫代码，所以程序只是对sina.com.cn做了个链接，并返回了响应码200。接下来，我们就要开始正式踏上爬虫程序的编写之路了。</p><h2>总结</h2><p>我们来回顾一下本节课的主要知识点。</p><ol>\n<li>Scrapy是一个适用于Python的快速、高层次的屏幕抓取和Web抓取框架，它可以抓取Web站点并从页面中提取结构化的数据。</li>\n<li>你还应该知道Scrapy框架的原理和主要模块（Scrapy引擎、调度器、下载器、爬虫、管道、下载中间件、Spider中间件），以及它们是如何协作的。</li>\n<li>最后，我们应该有能力在Anaconda环境中创建一个Scrapy环境，搭建一个最简单的Scrapy框架并把它跑起来。</li>\n</ol><h2>课后题</h2><p>学完这节课，给你留两道思考题。</p><ol>\n<li>请你自己尝试搭建一个Anaconda环境，并安装Scrapy框架相关的库。</li>\n<li>请你创建一个Scrapy的框架程序并运行它。</li>\n</ol><p>欢迎你在留言区与我交流讨论，我们下节课再见。</p>","neighbors":{"left":{"article_title":"06｜网络爬虫：爬取一个网站的流程是怎样的？","id":650545},"right":{"article_title":"08｜数据获取：如何使用Scrapy框架爬取新闻数据？","id":652864}},"comments":[{"had_liked":false,"id":381303,"user_name":"Geek_79da7f","can_delete":false,"product_type":"c1","uid":2421737,"ip_address":"美国","ucode":"6329F375C13B14","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/dcW6ufIgibXKl5jwrgsIibPxRehZBqN41ZHiamWx3yWNnfCfAOabxjYzLyDKv1HyYbNJOa05dEicobfGtBbJaJmG2w/132","comment_is_top":false,"comment_ctime":1695009184,"is_pvip":false,"replies":[{"id":140020,"content":"同学你好，感谢您提交的信息。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699860464,"ip_address":"广东","comment_id":381303,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"关于安装ChromeDriver, mac上面一个命令行就解决了： brew install chromedriver","like_count":3,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631654,"discussion_content":"同学你好，感谢您提交的信息。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699860464,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2189310,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","nickname":"悟尘","note":"","ucode":"4E7E854340D3A4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633542,"discussion_content":"这种方式会报错的，selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 99\n说明 这种方式安装的chromedriver 只支持到 谷歌99版本","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702290432,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373790,"user_name":"地铁林黛玉","can_delete":false,"product_type":"c1","uid":1241183,"ip_address":"北京","ucode":"3D35D965E54649","user_header":"https://static001.geekbang.org/account/avatar/00/12/f0/5f/25942dfb.jpg","comment_is_top":false,"comment_ctime":1683170095,"is_pvip":false,"replies":[{"id":136678,"content":"同学你好。一般来讲，在settings.py文件里有一个参数叫ROBOTSTXT_OBEY，这个文件默认为True，也就是遵循网站的robots协议，如果在true的状态下请求被拒绝，说明不能去爬取。当然，你也可以设置为Fales，也就是无视它的规则进行爬取，那么这个时候，其实就有一点违规的意思了。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683878591,"ip_address":"广东","comment_id":373790,"utype":1}],"discussion_count":4,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"爬取的这些数据我们需要通过哪些方法知道是不是违法的呢？","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617797,"discussion_content":"同学你好。一般来讲，在settings.py文件里有一个参数叫ROBOTSTXT_OBEY，这个文件默认为True，也就是遵循网站的robots协议，如果在true的状态下请求被拒绝，说明不能去爬取。当然，你也可以设置为Fales，也就是无视它的规则进行爬取，那么这个时候，其实就有一点违规的意思了。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683878591,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1916685,"avatar":"https://static001.geekbang.org/account/avatar/00/1d/3f/0d/1e8dbb2c.jpg","nickname":"怀揣梦想的学渣","note":"","ucode":"2349B9F4F6FDE3","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":618633,"discussion_content":"如果使用了非代理池或者非匿名的IP运行爬虫获取数据，需要提前邮件向网站获取授权，可以规避法律风险。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1684397509,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"山东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2820504,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/09/98/397c2c81.jpg","nickname":"贾维斯Echo","note":"","ucode":"BB8C507E46A9E5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617367,"discussion_content":"使用网络爬虫本身并不违法，但是使用网络爬虫的方式和目的可能会违反法律法规。因此，使用爬虫时需要遵守相关的法律法规和道德规范。\n\n在某些情况下，使用爬虫可能会侵犯隐私权、知识产权和著作权等。此外，使用爬虫进行数据采集时，必须遵守目标网站的服务条款和使用协议。如果目标网站明确禁止爬虫或未经授权的访问，那么使用爬虫可能会触犯相关的法律法规。\n\n因此，使用网络爬虫时需要审慎考虑并遵守相关的法律法规和道德规范。如果不确定是否可以使用爬虫，建议先咨询相关专业人士或法律顾问。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683531935,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":3616464,"avatar":"","nickname":"Geek_dacbef","note":"","ucode":"F0B20AC77B645C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":2820504,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/09/98/397c2c81.jpg","nickname":"贾维斯Echo","note":"","ucode":"BB8C507E46A9E5","race_medal":0,"user_type":1,"is_pvip":true},"discussion":{"id":617503,"discussion_content":"chatgpt？","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683618834,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":617367,"ip_address":"江苏","group_id":0},"score":617503,"extra":""}]}]},{"had_liked":false,"id":373752,"user_name":"未来已来","can_delete":false,"product_type":"c1","uid":1187130,"ip_address":"广东","ucode":"3A21ACFD53CB9C","user_header":"https://static001.geekbang.org/account/avatar/00/12/1d/3a/cdf9c55f.jpg","comment_is_top":false,"comment_ctime":1683120886,"is_pvip":false,"replies":[{"id":136676,"content":"是的，如果是被robot文件给挡住了，可以通过修改ROBOTSTXT_OBEY 来改变爬虫的规则，使其不遵循robots协议。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1683878419,"ip_address":"广东","comment_id":373752,"utype":1}],"discussion_count":2,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"遇到一个报错：Failure while parsing robots.txt.\n解决：把 settings.py 文件的 `ROBOTSTXT_OBEY = True` 改为 `ROBOTSTXT_OBEY = False` 即可","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617795,"discussion_content":"是的，如果是被robot文件给挡住了，可以通过修改ROBOTSTXT_OBEY 来改变爬虫的规则，使其不遵循robots协议。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683878419,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":1,"child_discussions":[{"author":{"id":2669122,"avatar":"https://static001.geekbang.org/account/avatar/00/28/ba/42/5ca553bd.jpg","nickname":"Weitzenböck","note":"","ucode":"78C92583084ABA","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"discussion":{"id":620948,"discussion_content":"我也遇到过这个问题，我一开始以为是网站编码用的不是utf-8，后来发现是robots协议问题。","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1686672936,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":617795,"ip_address":"江苏","group_id":0},"score":620948,"extra":""}]}]},{"had_liked":false,"id":373280,"user_name":"peter","can_delete":false,"product_type":"c1","uid":1058183,"ip_address":"北京","ucode":"261C3FC001DE2D","user_header":"https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg","comment_is_top":false,"comment_ctime":1682336800,"is_pvip":false,"replies":[{"id":136416,"content":"同学，你好，我来回答你的问题：\nA1：Scrapy是Python的爬虫框架，不是Java，所以不能直接用Scrapy来抓取Java开发的网站，但是你可以看看在Java上有没有想过的框架，原理都是一样的。\nA2：因为Anaconda安装程序默认使用了Python 3.9，但在创建虚拟环境时选择了Python 3.7，所以你看到的虚拟环境版本是3.7，虚拟环境的版本可以独立于主环境的，这个没有影响。\nA3：是的，是正常的Python开发环境，可以直接使用。\nA4：是的，conda list命令列出的scrapy中的build channel中的37表示Scrapy依赖的Python版本为3.7。在安装Scrapy时需要使用与Python版本兼容的Scrapy版本。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1682612106,"ip_address":"广东","comment_id":373280,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"请教老师几个问题啊\nQ1：网站后端是用Java开发的，可以用Scrapy来抓取数据吗？相当于两种语言的混合使用了。\nQ2：Anaconda安装的最后一步提示是“python3.9”,为什么创建虚拟环境的时候python版本是3.7？\nQ3：安装的这个Anaconda，是正常的python开发环境吧。比如用来学习python，编码等。\nQ4：conda list命令列出的scrapy，其build channel是py37XXX，\n其中的37是python版本吗？","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616194,"discussion_content":"同学，你好，我来回答你的问题：\nA1：Scrapy是Python的爬虫框架，不是Java，所以不能直接用Scrapy来抓取Java开发的网站，但是你可以看看在Java上有没有想过的框架，原理都是一样的。\nA2：因为Anaconda安装程序默认使用了Python 3.9，但在创建虚拟环境时选择了Python 3.7，所以你看到的虚拟环境版本是3.7，虚拟环境的版本可以独立于主环境的，这个没有影响。\nA3：是的，是正常的Python开发环境，可以直接使用。\nA4：是的，conda list命令列出的scrapy中的build channel中的37表示Scrapy依赖的Python版本为3.7。在安装Scrapy时需要使用与Python版本兼容的Scrapy版本。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682612106,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2820504,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/09/98/397c2c81.jpg","nickname":"贾维斯Echo","note":"","ucode":"BB8C507E46A9E5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617369,"discussion_content":"java 类似爬虫框架:\nScrapy是一个基于Python的强大、灵活且高效的网络爬虫框架，可以用于抓取和提取网站数据。如果你想要一个类似于Scrapy的Java爬虫框架，可以考虑以下几个：\n\n\nJsoup：这是一个Java HTML解析器，可以用来从网页中提取信息。它可以像Scrapy一样遍历网站并抓取数据，但它缺少Scrapy中许多高级功能，如自动限速和数据处理管道。\n\nWebMagic：WebMagic是一个功能强大的Java爬虫框架，它具有类似于Scrapy的功能。它提供了简单易用的API，可以帮助你抓取网站数据、处理数据，并将其保存到数据库中。此外，WebMagic还支持分布式爬取和异步处理，使它能够更快地处理大量数据。\n\nCrawler4j：Crawler4j是一个Java爬虫框架，它基于Apache Nutch实现。它具有类似于Scrapy的功能，可以帮助你抓取网站数据并对其进行处理。Crawler4j还支持多线程爬取和分布式爬取。\n\nStormCrawler：StormCrawler是一个基于Apache Storm的开源爬虫框架，它可以用于实时爬取Web数据。它具有类似于Scrapy的功能，包括自动限速、数据处理管道和自定义扩展。此外，StormCrawler还支持分布式爬取和高度可扩展性。\n\n\n总之，如果你需要一个类似于Scrapy的Java爬虫框架，可以根据自己的需求选择适合自己的框架。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683532283,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2820504,"avatar":"https://static001.geekbang.org/account/avatar/00/2b/09/98/397c2c81.jpg","nickname":"贾维斯Echo","note":"","ucode":"BB8C507E46A9E5","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":617368,"discussion_content":"问题一:\nScrapy是一个Python编写的网络爬虫框架，可以抓取网页内容、解析网页数据等。与网站后端使用Java开发无关，只要目标网站允许爬虫访问，并且您能够确定如何抓取和解析数据，就可以使用Scrapy进行数据爬取。\n\n在抓取Java后端网站时，您需要注意以下几点：\n\nJava后端网站通常使用的是MVC框架，因此您需要查看网站的URL结构并理解其数据的传输方式。\n\nJava后端网站可能会使用一些特定的Web技术，例如AJAX、WebSocket等，这些技术可能会导致您无法直接抓取数据。您需要了解这些技术并选择合适的方法来处理它们。\n\nJava后端网站可能会使用一些安全措施，例如反爬虫机制、验证码等，这些措施可能会对爬虫造成一定的阻碍。您需要使用一些技巧来绕过这些措施，例如使用代理IP、随机User-Agent、处理Cookie等。\n\n总之，Scrapy可以用来爬取Java后端网站的数据，但是在实际应用中需要注意一些技术细节和法律法规。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1683532141,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":373252,"user_name":"GhostGuest","can_delete":false,"product_type":"c1","uid":2871611,"ip_address":"上海","ucode":"A6785E3304276B","user_header":"https://static001.geekbang.org/account/avatar/00/2b/d1/3b/a94459d2.jpg","comment_is_top":false,"comment_ctime":1682316982,"is_pvip":false,"replies":[{"id":136408,"content":"同学你好，非常感谢你对我的课程的关注和反馈。我理解你想尽快得到更多的内容，但我的更新频率已经是我的最大努力了，因为我需要花费时间和精力来研究和准备每个更新的内容。在你们学习课程的同时，我也在每天写课、改稿和回复同学的留言到深夜。\n另外，我需要时间尽量保证内容和代码的精细化，保证其质量，这样对读者才是最好的回馈。","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1682607900,"ip_address":"广东","comment_id":373252,"utype":1}],"discussion_count":3,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"更新建议改为一天一更，现在这节奏太慢了，前摇半天","like_count":1,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":616166,"discussion_content":"同学你好，非常感谢你对我的课程的关注和反馈。我理解你想尽快得到更多的内容，但我的更新频率已经是我的最大努力了，因为我需要花费时间和精力来研究和准备每个更新的内容。在你们学习课程的同时，我也在每天写课、改稿和回复同学的留言到深夜。\n另外，我需要时间尽量保证内容和代码的精细化，保证其质量，这样对读者才是最好的回馈。","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1682607901,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2189275,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/db/6146bce8.jpg","nickname":"朱得君","note":"","ucode":"ADA07F1B1FC95F","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":651566,"discussion_content":"老师负责任的态度真的非常棒, 给赞","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1727079279,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1740680,"avatar":"https://static001.geekbang.org/account/avatar/00/1a/8f/88/3814fea5.jpg","nickname":"安静点","note":"","ucode":"9598650FD71F23","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":619671,"discussion_content":"建议专家不要建议🤣","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1685416089,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":376316,"user_name":"Weitzenböck","can_delete":false,"product_type":"c1","uid":2669122,"ip_address":"江苏","ucode":"78C92583084ABA","user_header":"https://static001.geekbang.org/account/avatar/00/28/ba/42/5ca553bd.jpg","comment_is_top":false,"comment_ctime":1686672290,"is_pvip":false,"replies":[{"id":140041,"content":"应该是掺杂了特殊字符，你可以再检查下","user_name":"作者回复","user_name_real":"编辑","uid":1982950,"ctime":1699861435,"ip_address":"广东","comment_id":376316,"utype":1}],"discussion_count":5,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"我在执行main函数的时候出现了这个错误&quot;UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte&quot;，是不是https:&#47;&#47;sina.com.cn这个网站没有用utf-8的编码格式啊","like_count":0,"discussions":[{"author":{"id":1982950,"avatar":"https://static001.geekbang.org/account/avatar/00/1e/41/e6/beb42103.jpg","nickname":"黄鸿波","note":"","ucode":"5EB4E6946A363C","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":631676,"discussion_content":"应该是掺杂了特殊字符，你可以再检查下","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1699861436,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"广东","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2}","child_discussion_number":0,"child_discussions":[]},{"author":{"id":2421737,"avatar":"https://thirdwx.qlogo.cn/mmopen/vi_32/dcW6ufIgibXKl5jwrgsIibPxRehZBqN41ZHiamWx3yWNnfCfAOabxjYzLyDKv1HyYbNJOa05dEicobfGtBbJaJmG2w/132","nickname":"Geek_79da7f","note":"","ucode":"6329F375C13B14","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":628147,"discussion_content":"上面有解决方案：复制黏贴答案如下：\n```\n遇到一个报错：Failure while parsing robots.txt.\n解决：把 settings.py 文件的 `ROBOTSTXT_OBEY = True` 改为 `ROBOTSTXT_OBEY = False` 即可\n```","likes_number":1,"is_delete":false,"is_hidden":false,"ctime":1695009576,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"美国","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]},{"author":{"id":1258480,"avatar":"https://static001.geekbang.org/account/avatar/00/13/33/f0/50c773cd.jpg","nickname":"全国花式伸脖子蹬腿锦标赛冠军🏆","note":"","ucode":"77181EEE13E761","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":626943,"discussion_content":"解决了吗，兄弟","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1693491194,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"重庆","group_id":0},"score":2,"extra":"","child_discussion_number":1,"child_discussions":[{"author":{"id":2189310,"avatar":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","nickname":"悟尘","note":"","ucode":"4E7E854340D3A4","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":1258480,"avatar":"https://static001.geekbang.org/account/avatar/00/13/33/f0/50c773cd.jpg","nickname":"全国花式伸脖子蹬腿锦标赛冠军🏆","note":"","ucode":"77181EEE13E761","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":633535,"discussion_content":"我用这个方式解决了","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1702287568,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":626943,"ip_address":"北京","group_id":0},"score":633535,"extra":""}]},{"author":{"id":1073592,"avatar":"https://static001.geekbang.org/account/avatar/00/10/61/b8/7b23f8cb.jpg","nickname":"本来是亚","note":"","ucode":"14527E5BC3C2A8","race_medal":0,"user_type":1,"is_pvip":true},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":622386,"discussion_content":"我也遇到了这个问题哎","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1688179210,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"上海","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":386553,"user_name":"叶圣枫","can_delete":false,"product_type":"c1","uid":1126794,"ip_address":"上海","ucode":"6DE65AB06AD20E","user_header":"https://static001.geekbang.org/account/avatar/00/11/31/8a/be3b7ae6.jpg","comment_is_top":false,"comment_ctime":1705066906,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"我的macbook上会报这个错：\nurllib3 v2.0 only supports OpenSSL 1.1.1+, currently the &#39;ssl&#39; module is compiled with &#39;OpenSSL 1.0.2u  20 Dec 2019\n解决方案是降级urllib3:\npip install urllib3==1.26.6\n\n","like_count":2},{"had_liked":false,"id":385224,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1702290812,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"chrom 114 版本以上的 下载chromedriver在这里：https:&#47;&#47;registry.npmmirror.com&#47;binary.html?path=chrome-for-testing&#47;","like_count":2},{"had_liked":false,"id":387607,"user_name":"李","can_delete":false,"product_type":"c1","uid":1467260,"ip_address":"浙江","ucode":"C2C38D3E17652B","user_header":"https://static001.geekbang.org/account/avatar/00/16/63/7c/51d07eff.jpg","comment_is_top":false,"comment_ctime":1708241723,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100542801,"comment_content":"老师出现这个错误是什么原因","like_count":0},{"had_liked":false,"id":385216,"user_name":"悟尘","can_delete":false,"product_type":"c1","uid":2189310,"ip_address":"北京","ucode":"4E7E854340D3A4","user_header":"https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg","comment_is_top":false,"comment_ctime":1702287627,"is_pvip":false,"replies":null,"discussion_count":1,"race_medal":0,"score":2,"product_id":100542801,"comment_content":" [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; from &lt;GET https:&#47;&#47;sina.com.cn&gt;\n [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; (referer: None)\n\n这算是连上了？","like_count":0,"discussions":[{"author":{"id":3713939,"avatar":"https://static001.geekbang.org/account/avatar/00/38/ab/93/106c2e44.jpg","nickname":"弄潮儿","note":"","ucode":"FA271FE1FF664C","race_medal":0,"user_type":1,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":649348,"discussion_content":"我的也是这个结果","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1723188441,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":false,"parent_id":0,"ip_address":"北京","group_id":0},"score":2,"extra":"","child_discussion_number":0,"child_discussions":[]}]}]}