{"id":784130,"title":"10｜经典算法之RNN：开发人员绕不开的循环神经网络","content":"<p>你好，我是独行。</p><p>上一节课的最后我们介绍了神经网络，神经网络有很多种，包括前馈神经网络（FNN）、卷积神经网络（CNN）、循环神经网络（RNN）、图神经网络（GNN）、自注意力机制模型Transformer等。</p><p>这节课我们就来学习其中的RNN，它主要用来处理序列数据，为什么我们要学习RNN呢？实际上目前大部分大语言模型都是基于Transformer的，通过学习RNN，我们可以理解神经网络如何处理序列中的依赖关系、记忆过去的信息，并在此基础上生成预测的基本概念，尤其是几个关键问题，比如梯度消失和梯度爆炸等，为我们后面进一步理解Transformer打下基础。</p><h2>循环神经网络</h2><p>循环神经网络（RNN）是一类用于处理序列数据的神经网络。不同于传统的前馈神经网络，RNN能够处理序列长度变化的数据，如文本、语音等。RNN的特点是在模型中引入循环，使得网络能够保持某种状态，从而在处理序列数据时表现出更好的性能。我们看一下下面的图片。</p><p><img src=\"https://static001.geekbang.org/resource/image/8a/45/8a7b7d8b1fb825e8a3056efd58037545.png?wh=795x319\" alt=\"图片\" title=\"图片源于网络\"></p><p>左边简单描述RNN的原理，x是输入层，o是输出层，中间s是隐藏层，在s层进行一个循环，右边表示展开循环看到的逻辑，其实是和时间t相关的一个状态变化，也就是说神经网络在处理数据的时候，能看到前一时刻、后一时刻的状态，也就是我们常说的上下文。</p><!-- [[[read_end]]] --><p>举个例子，你女朋友看了小米汽车发布会，和你说她想买个21.59万的小米，你肯定知道她要买的是标准版的小米汽车，而不是21.59万粒小黄米，因为你知道上下文。如果换做模型，怎么才能准确地理解我们的语义呢？同样，需要上下文。RNN因为隐藏层有时序状态，那么在推理的时候就可以借助上下文，从而更加准确地理解语义。</p><h2>基本结构与原理</h2><p>RNN的核心在于隐藏层，隐藏层主要的逻辑在于如何随时间的变化更新隐藏状态，隐藏状态的计算公式如下：</p><p>$$h_{t}=f(W_{xh}x_{t}+W_{hh}h_{t-1}+b_{h})$$</p><p>其中，$h_{t}$ 是当前时间步的隐藏状态，$x_{t}$ 是当前时间步的输入，$h_{t-1}$ 是前一个时间步的隐藏状态，$W_{xh}$ 和 $W_{hh}$ 是权重矩阵，$b_{h}$ 是偏置项，$f$ 是激活函数，在这个例子里我们使用tanh函数。</p><p>假设我们正在处理一个文本序列任务，目的是根据已有的字符序列预测下一个字符。为了简化，我们假设字符集只包含三个字符：“A”“B”“C”，并且我们的任务是给定序列“AB”，预测下一个字符。</p><p>首先，在输入层，我们将字符转换为数值形式，例如通过One-hot编码，“A”=[1,0,0]，“B”=[0,1,0]，“C”=[0,0,1]。所以，序列“AB”的输入表示为两个向量：[1,0,0] 和 [0,1,0]。</p><p>其次，在隐藏层，假设我们只有一个隐藏单元，实际应用中可能会有多个，使用tanh作为激活函数。</p><p>时间步1-处理A：</p><ol>\n<li>输入：[1,0,0]</li>\n<li>假设 $W_{xh}$ 和 $W_{hh}$ 的值均为1，初始隐藏状态 $h_0=0$。</li>\n<li>计算新的隐藏状态 $h_1=tanh(1*[1,0,0]+1*0)=tanh(1)\\approx0.76$。</li>\n</ol><p>时间步2-处理B：</p><ol>\n<li>输入：[0,1,0]</li>\n<li>使用上一个时间步的隐藏状态 $h_1\\approx0.76$。</li>\n<li>计算新的隐藏状态 $h_2=tanh(1*[0,1,0]+1*0.76)=tanh(0.76)\\approx0.64$。</li>\n</ol><p>在这个简化的例子中，每个时间步的隐藏状态 $h_t$ 基于当前的输入 $x_t$ 和上一个时间步的隐藏状态 $h_{t-1}$ 计算得到。通过这种方式，RNN能够记住之前的输入（在这个例子中是字符A和B），并使用这些信息来影响后续的处理，比如预测序列中的下一个字符。这样，模型就具备了记忆功能。</p><p><img src=\"https://static001.geekbang.org/resource/image/a4/35/a4aac182cde2c1d5eeae31db0931d635.png?wh=2504x1316\" alt=\"图片\"></p><p><span class=\"reference\">注：One-hot 编码的内容来自《深度学习推荐系统实战》中的</span> <a href=\"https://time.geekbang.org/column/article/295300?utm_campaign=geektime_search&utm_content=geektime_search&utm_medium=geektime_search&utm_source=geektime_search&utm_term=geektime_search\">05 | 特征处理：如何利用Spark解决特征处理问题？</a></p><h2>关键挑战</h2><p>RNN通过当前的隐藏状态来记住序列中之前的信息。这种记忆一般是短期的，因为随着时间步的增加，早期输入对当前状态的影响会逐渐减弱，在标准RNN中，尤其当遇到梯度消失情况时，就会遇到短期记忆的问题，几乎无法更新权重。</p><h4>梯度消失</h4><p>我们先来看下什么是梯度？梯度是指函数在某一点的斜率，在深度学习中，该函数一般指具有多个变量的损失函数，变量就是模型的权重。损失函数衡量的是模型预测与实际数据之间的差异，一般情况下，我们要尽可能地让损失函数的值最小。如何找到这个最小值呢？需要进行梯度下降，也就是说，我们要不断调整参数（权重），使损失函数的值降到最小，这个过程就是梯度下降。</p><p>为什么会产生梯度消失呢？一般有两个原因。</p><ol>\n<li>深层网络中的连乘效应：在深层网络中，梯度是通过链式法则进行反向传播的。如果每一层的梯度都小于1，那么随着层数的增加，这些小于1的值会连乘在一起，导致最终的梯度非常小。</li>\n<li>激活函数的选择：使用某些激活函数，如tanh，函数的取值范围是-1～1，小于1的数进行连乘，也会快速降低梯度值。</li>\n</ol><p>这里我解释下反向传播，在深度学习中，训练神经网络涉及两个主要的传播阶段：前向传播和反向传播。在前向传播阶段，输入数据从网络的输入层开始，逐层向前传递至输出层。每一层都会对其输入进行计算，如加权求和，然后应用激活函数等，并将计算结果传递给下一层，直到最终产生输出。这个过程的目标是根据当前的网络参数、权重和偏置等得到预测输出。</p><p>一旦在输出层得到了预测输出，就会计算损失函数，即预测输出与实际目标输出之间的差异。接下来，这个损失会被用来计算损失函数相对于网络中每个参数的梯度，这是通过链式法则实现的。这个计算过程从输出层开始，沿着网络向后，即向输入层的方向，逐层进行，这就是“反向传播”的由来。</p><p>这些梯度表示了为了减少损失，各个参数需要如何调整。最后，这些梯度会用来更新网络的参数，通常是通过梯度下降或其变体算法实现。而在反向传播过程中，每到达一层，都会触发激活函数，这就是我们上面说的第2点原因。</p><p>由此可见，如果要解决梯度消失的问题，我们就从这两个原因入手。</p><ol>\n<li><a href=\"https://time.geekbang.org/course/detail/100077201-420627?utm_campaign=geektime_search&utm_content=geektime_search&utm_medium=geektime_search&utm_source=geektime_search&utm_term=geektime_search\">长短期记忆（LSTM）</a>和<a href=\"https://time.geekbang.org/course/detail/100077201-418603?utm_campaign=geektime_search&utm_content=geektime_search&utm_medium=geektime_search&utm_source=geektime_search&utm_term=geektime_search\">门控循环单元（GRU）</a>是专门为了避免梯度消失问题而设计的。它们通过引入门控机制来调节信息的流动，保留长期依赖信息，从而避免梯度在反向传播过程中消失。</li>\n<li>使用ReLU及其变体激活函数，在正区间内的梯度保持恒定，不会随着输入的增加而减少到0，这有助于减轻梯度消失问题。</li>\n</ol><h4>梯度爆炸</h4><p>与梯度消失相对的问题是梯度爆炸，当模型的梯度在反向传播过程中变得非常大，以至于更新后的权重偏离了最优值，导致模型无法收敛，甚至发散。</p><p>通常梯度爆炸发生原因有三个。</p><ol>\n<li>深层网络的连乘效应：在深层网络中，梯度是通过链式法则进行反向传播的。如果每一层的梯度都大于1，那么随着层数的增加，这些大于1的值会连乘在一起，导致最终的梯度非常大。</li>\n<li>权重初始化不当：如果网络的权重初始化得太大，那么在前向传播过程中信号的大小会迅速增加，同样，反向传播时梯度也会迅速增加。</li>\n<li>使用不恰当的激活函数：某些激活函数（如ReLU）在正区间的梯度为常数。如果网络架构设计不当，使用这些激活函数也可能导致梯度爆炸。</li>\n</ol><p>梯度爆炸和梯度消失基本相反，解决方法一样，要么使用长短期记忆和门控循环单元调整网络结构，要么替换激活函数，还有一种办法是进行梯度裁剪，梯度裁剪意思是在训练过程中，通过限制梯度的最小/大值来防止梯度消失/爆炸，间接地保持梯度的稳定性。</p><h4>长短期记忆（LSTM）</h4><p>无论是梯度消失还是梯度爆炸，都提到长短期记忆结构，我们来简单看一下，LSTM就像是具有类似大脑记忆功能的模型，它在处理数据，如文本、时间序列数据时，能够记住对当前任务重要的信息，并忘记不重要的信息。这是通过以下几个关键机制实现的。</p><ol>\n<li>遗忘门（Forget Gate）：决定了哪些信息是过时的、不重要的，因此应该从模型的记忆中抛弃。就像你在阅读时可能忘记某个次要角色的不重要的细节。</li>\n<li>输入门（Input Gate）：它决定哪些新的信息是重要的，应该被添加到模型的记忆中。这就像你在阅读新章节时，发现关于男主人公重要的新信息并记住它们。</li>\n<li>输出门（Output Gate）：它决定了在当前时刻，哪些记忆是相关的，应该被用来生成输出。这就像你在思考男主人公的动机和行为时，会回想起之前关于他的重要记忆。</li>\n</ol><p><img src=\"https://static001.geekbang.org/resource/image/05/d1/0573395d4b88c182c53932976909e1d1.png?wh=2378x1246\" alt=\"图片\"></p><p>通过这些机制，LSTM能够在处理序列数据时，有效地保留长期的依赖信息，就像是记住故事中的关键情节和角色，同时避免了标准RNN中常见的梯度消失问题。这使得LSTM特别适用于需要理解整个序列背景的任务，比如语言翻译，需要理解整个句子的含义，或者股票价格预测，需要考虑长期的价格变化趋势。</p><h2>RNN实际应用场景</h2><h4>文本生成</h4><p>文本生成是RNN的一个典型应用，通过学习大量的文本数据，RNN能够生成具有相似风格的文本。我们看一个简单的文本生成模型的代码示例。</p><pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\n# 数据预处理\ntext = \"Here is some sample text to demonstrate text generation with RNN. This is a simple example.\"\ntokens = text.lower().split()\ntokenizer = {word: i + 1 for i, word in enumerate(set(tokens))}\ntotal_words = len(tokenizer) + 1\n\n# 创建输入序列\nsequences = []\nfor line in text.split('.'):\n    token_list = [tokenizer[word] for word in line.lower().split() if word in tokenizer]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i + 1]\n        sequences.append(n_gram_sequence)\nmax_sequence_len = max([len(x) for x in sequences])\nsequences = [torch.tensor(seq) for seq in sequences]\nsequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n\nclass TextDataset(Dataset):\n    def __init__(self, sequences):\n        self.x = sequences[:, :-1]\n        self.y = sequences[:, -1]\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\ndataset = TextDataset(sequences)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n# 构建模型\nclass RNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.lstm(x)\n        x = self.fc(x[:, -1, :])\n        return x\n\nmodel = RNNModel(total_words, 64, 20)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 模型训练\nfor epoch in range(100):\n    for x_batch, y_batch in dataloader:\n        optimizer.zero_grad()\n        output = model(x_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n\n# 文本生成\ndef generate_text(seed_text, next_words, model, max_sequence_len):\n    model.eval()\n    for _ in range(next_words):\n        token_list = [tokenizer[word] for word in seed_text.lower().split() if word in tokenizer]\n        token_list = torch.tensor(token_list).unsqueeze(0)\n        token_list = nn.functional.pad(token_list, (max_sequence_len - 1 - token_list.size(1), 0), 'constant', 0)\n        with torch.no_grad():\n            predicted = model(token_list)\n            predicted = torch.argmax(predicted, dim=-1).item()\n        output_word = \"\"\n        for word, index in tokenizer.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return seed_text\n\nprint(generate_text(\"Here is\", 4, model, max_sequence_len))\n\n</code></pre><p>核心过程我在代码中注释过了，这只是一个简单的代码示例，实际使用的时候，你可能需要使用更大的文本数据集来训练模型，调整模型架构，比如增加层数、调整LSTM单元数量、超参数等，以及实施更复杂的数据预处理和文本生成策略，以达到更好的生成效果。</p><h2>小结</h2><p>这节课内容实际是有点难度的，我们通过简单的例子学习了RNN的基本概念，然后通过敲代码进行练习。<strong>RNN 的优势在于它的记忆能力，通过隐藏层循环结构捕捉序列的长期依赖关系，特别适合用于文本生成、语音识别等领域。</strong>同时，RNN也有局限性，比如梯度消失、梯度爆炸等，而引入LSTM可以一定程度上解决这些问题。</p><h2>思考题</h2><p>既然引入LSTM可以解决一系列问题，那目前主流的大语言模型为什么没有使用RNN架构呢？请你说说你的理解，也欢迎你在评论区和我交流讨论，如果你觉得这节课的内容对你有帮助的话，欢迎你分享给其他朋友，我们下节课再见！</p>","neighbors":{"left":{"article_title":"09｜关于机器学习，你需要了解的基本概念（二）","id":784106},"right":{"article_title":"11｜关于自然语言处理，你需要了解的基本概念","id":784141}},"comments":[{"had_liked":false,"id":391687,"user_name":"冰冻柠檬","can_delete":false,"product_type":"c1","uid":1585858,"ip_address":"安徽","ucode":"DA940109725AE5","user_header":"https://static001.geekbang.org/account/avatar/00/18/32/c2/ffa6c819.jpg","comment_is_top":false,"comment_ctime":1718842722,"is_pvip":false,"replies":[{"id":142442,"content":"可以的，近期就换","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1718965650,"ip_address":"浙江","comment_id":391687,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"独行老师，提个建议哈，示例代码可以换成pytorch吗？","like_count":4,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":646904,"discussion_content":"可以的，近期就换","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1718965650,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":392095,"user_name":"Ethan New","can_delete":false,"product_type":"c1","uid":2063962,"ip_address":"浙江","ucode":"9CA2EF39E58030","user_header":"https://static001.geekbang.org/account/avatar/00/1f/7e/5a/da39f489.jpg","comment_is_top":false,"comment_ctime":1719932846,"is_pvip":true,"replies":[{"id":142548,"content":"算一个原因","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1720074391,"ip_address":"浙江","comment_id":392095,"utype":1}],"discussion_count":1,"race_medal":4,"score":2,"product_id":100770601,"comment_content":"既然引入 LSTM 可以解决一系列问题，那目前主流的大语言模型为什么没有使用 RNN 架构呢？\n原因在于训练效率低。RNN是串行计算，当前计算的输出依赖于前一次计算的隐藏层结果","like_count":1,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":647467,"discussion_content":"算一个原因","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1720074391,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":395014,"user_name":"潇洒哥66","can_delete":false,"product_type":"c1","uid":3831984,"ip_address":"上海","ucode":"FA0D575CC5FC4E","user_header":"https://static001.geekbang.org/account/avatar/00/3a/78/b0/13b19797.jpg","comment_is_top":false,"comment_ctime":1729088515,"is_pvip":false,"replies":[{"id":143650,"content":"哪个例子？示例应该是有一些简化，为了说明问题","user_name":"作者回复","user_name_real":"编辑","uid":2083554,"ctime":1731470655,"ip_address":"浙江","comment_id":395014,"utype":1}],"discussion_count":1,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"有一个疑问哈，RNN第一个计算的例子，是不是简化了，感觉矩阵乘法有些问题。","like_count":0,"discussions":[{"author":{"id":2083554,"avatar":"http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erZP8e2vqiaACFaDxruOzUTPbPv2uRUTp9UuEg98Ib9aYddjZK2kastqf0B14Ec7uXx7CCSXr0fhAA/132","nickname":"Geek_cf2545","note":"","ucode":"B69DF1E734FBA2","race_medal":0,"user_type":2,"is_pvip":false},"reply_author":{"id":0,"avatar":"","nickname":"","note":"","ucode":"","race_medal":0,"user_type":1,"is_pvip":false},"discussion":{"id":653815,"discussion_content":"哪个例子？示例应该是有一些简化，为了说明问题","likes_number":0,"is_delete":false,"is_hidden":false,"ctime":1731470655,"is_liked":false,"can_delete":false,"is_complain":false,"is_top":true,"parent_id":0,"ip_address":"浙江","group_id":0},"score":2,"extra":"{\"reply\":true,\"user_type\":2,\"source\":0}","child_discussion_number":0,"child_discussions":[]}]},{"had_liked":false,"id":391694,"user_name":"张申傲","can_delete":false,"product_type":"c1","uid":1182372,"ip_address":"北京","ucode":"22D46BC529BA8A","user_header":"https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg","comment_is_top":false,"comment_ctime":1718856210,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":2,"score":2,"product_id":100770601,"comment_content":"第10讲打卡~\n思考题：主流的大语言模型没有采用RNN，而是采用了Transformer，主要是因为RNN没法处理长序列。由于文中提到的梯度消失和梯度爆炸等问题，RNN模型没法捕捉到距离较远的文本之间的依赖关系，但是这并不符合人类的语言习惯。比如这样一段话：&quot;我喜欢旅行，并不喜欢上班，因为这能让我感受到自由&quot;，里面的&quot;这&quot;实际上指代的是&quot;旅行&quot;，而并不是距离它更近的&quot;上班&quot;。","like_count":9},{"had_liked":false,"id":396421,"user_name":"wlih45","can_delete":false,"product_type":"c1","uid":2943061,"ip_address":"云南","ucode":"F829468C2307DA","user_header":"https://thirdwx.qlogo.cn/mmopen/vi_32/bJjBw4nJOV2VFDibH86RicG3tA92ngUH7R0PRx5yZqhGmcWv5QPjWNFPafOIpDlHZ5BMnQH9a7r0S3Xhqa9w36NA/132","comment_is_top":false,"comment_ctime":1734373719,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"大家rnn部分最终的输出是什么呀？","like_count":0},{"had_liked":false,"id":394562,"user_name":"行者无疆","can_delete":false,"product_type":"c1","uid":1069168,"ip_address":"河南","ucode":"74DA2504FA746E","user_header":"https://static001.geekbang.org/account/avatar/00/10/50/70/0b67addb.jpg","comment_is_top":false,"comment_ctime":1727163499,"is_pvip":false,"replies":null,"discussion_count":0,"race_medal":0,"score":2,"product_id":100770601,"comment_content":"自然界中的许多关系都是非线性的，若是模型只能学习线性关系，也就无法准确地拟合这些数据。RNN的输入主要是序列数据，这些数据展示的更多的就是线性关系。激活函数（例如ReLU）用于为序列中每个位置的词元（输入向量）单独添加非线性变换，增加模型的深度，从而让其学习到更复杂的特征，提高模型的性能。","like_count":0}]}